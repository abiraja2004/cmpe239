2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 29?38,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Fast Inference in Phrase Extraction Models with Belief Propagation
David Burkett and Dan Klein
Computer Science Division
University of California, Berkeley
{dburkett,klein}@cs.berkeley.edu
Abstract
Modeling overlapping phrases in an align-
ment model can improve alignment quality
but comes with a high inference cost. For
example, the model of DeNero and Klein
(2010) uses an ITG constraint and beam-based
Viterbi decoding for tractability, but is still
slow. We first show that their model can be
approximated using structured belief propaga-
tion, with a gain in alignment quality stem-
ming from the use of marginals in decoding.
We then consider a more flexible, non-ITG
matching constraint which is less efficient for
exact inference but more efficient for BP. With
this new constraint, we achieve a relative error
reduction of 40% in F5 and a 5.5x speed-up.
1 Introduction
Modern statistical machine translation (MT) sys-
tems most commonly infer their transfer rules from
word-level alignments (Koehn et al, 2007; Li and
Khudanpur, 2008; Galley et al, 2004), typically
using a deterministic heuristic to convert these to
phrase alignments (Koehn et al, 2003). There have
been many attempts over the last decade to develop
model-based approaches to the phrase alignment
problem (Marcu and Wong, 2002; Birch et al, 2006;
DeNero et al, 2008; Blunsom et al, 2009). How-
ever, most of these have met with limited success
compared to the simpler heuristic method. One key
problem with typical models of phrase alignment
is that they choose a single (latent) segmentation,
giving rise to undesirable modeling biases (DeNero
et al, 2006) and reducing coverage, which in turn
reduces translation quality (DeNeefe et al, 2007;
DeNero et al, 2008). On the other hand, the extrac-
tion heuristic identifies many overlapping options,
and achieves high coverage.
In response to these effects, the recent phrase
alignment work of DeNero and Klein (2010) mod-
els extraction sets: collections of overlapping phrase
pairs that are consistent with an underlying word
alignment. Their extraction set model is empirically
very accurate. However, the ability to model over-
lapping ? and therefore non-local ? features comes
at a high computational cost. DeNero and Klein
(2010) handle this in part by imposing a structural
ITG constraint (Wu, 1997) on the underlying word
alignments. This permits a polynomial-time algo-
rithm, but it is still O(n6), with a large constant
factor once the state space is appropriately enriched
to capture overlap. Therefore, they use a heavily
beamed Viterbi search procedure to find a reason-
able alignment within an acceptable time frame. In
this paper, we show how to use belief propagation
(BP) to improve on the model?s ITG-based struc-
tural formulation, resulting in a new model that is
simultaneously faster and more accurate.
First, given the model of DeNero and Klein
(2010), we decompose it into factors that admit
an efficient BP approximation. BP is an inference
technique that can be used to efficiently approxi-
mate posterior marginals on variables in a graphical
model; here the marginals of interest are the phrase
pair posteriors. BP has only recently come into use
in the NLP community, but it has been shown to be
effective in other complex structured classification
tasks, such as dependency parsing (Smith and Eis-
ner, 2008). There has also been some prior success
in using BP for both discriminative (Niehues and
Vogel, 2008) and generative (Cromie`res and Kuro-
hashi, 2009) word alignment models.
By aligning all phrase pairs whose posterior under
BP exceeds some fixed threshold, our BP approxi-
mation of the model of DeNero and Klein (2010) can
29
achieve a comparable phrase pair F1. Furthermore,
because we have posterior marginals rather than a
single Viterbi derivation, we can explicitly force the
aligner to choose denser extraction sets simply by
lowering the marginal threshold. Therefore, we also
show substantial improvements over DeNero and
Klein (2010) in recall-heavy objectives, such as F5.
More importantly, we also show how the BP fac-
torization allows us to relax the ITG constraint, re-
placing it with a new set of constraints that per-
mit a wider family of alignments. Compared to
ITG, the resulting model is less efficient for exact
inference (where it is exponential), but more effi-
cient for our BP approximation (where it is only
quadratic). Our new model performs even better
than the ITG-constrained model on phrase align-
ment metrics while being faster by a factor of 5.5x.
2 Extraction Set Models
Figure 1 shows part of an aligned sentence pair, in-
cluding the word-to-word alignments, and the ex-
tracted phrase pairs licensed by those alignments.
Formally, given a sentence pair (e, f), a word-level
alignment a is a collection of links between target
words ei and source words fj . Following past work,
we further divide word links into two categories:
sure and possible, shown in Figure 1 as solid and
hatched grey squares, respectively. We represent a
as a grid of ternary word link variables aij , each of
which can take the value sure to represent a sure link
between ei and fj , poss to represent a possible link,
or off to represent no link.
An extraction set pi is a set of aligned phrase pairs
to be extracted from (e, f), shown in Figure 1 as
green rounded rectangles. We represent pi as a set of
boolean variables pighk`, which each have the value
true when the target span [g, h] is phrase-aligned to
the source span [k, `]. Following previous work on
phrase extraction, we limit the size of pi by imposing
a phrase length limit d: pi only contains a variable
pighk` if h? g < d and `? k < d.
There is a deterministic mapping pi(a) from a
word alignment to the extraction set licensed by that
word alignment. We will briefly describe it here, and
then present our factorized model.
e3 e4 e5 e6 e7
f5
f6
f7
f8
f9
?f5 = [7, 7]
?f6 = [5, 6]
?f7 = [5, 6]
?f8 = [4, 4]
?f9 = [?1,?]
Figure 1: A schematic representation of part of a sen-
tence pair. Solid grey squares indicate sure links (e.g.
a48 = sure), and hatched squares possible links (e.g.
a67 = poss). Rounded green rectangles are extracted
phrase pairs (e.g. pi5667 = true). Target spans are shown
as blue vertical lines and source spans as red horizontal
lines. Because there is a sure link at a48, ?f8 = [4, 4] does
not include the possible link at a38. However, f7 only
has possible links, so ?f7 = [5, 6] is the span containing
those. f9 is null-aligned, so ?f9 = [?1,?], which blocks
all phrase pairs containing f9 from being extracted.
2.1 Extraction Sets from Word Alignments
The mapping from a word alignment to the set of
licensed phrase pairs pi(a) is based on the standard
rule extraction procedures used in most modern sta-
tistical systems (Koehn et al, 2003; Galley et al,
2006; Chiang, 2007), but extended to handle pos-
sible links (DeNero and Klein, 2010). We start by
using a to find a projection from each target word ei
onto a source span, represented as blue vertical lines
in Figure 1. Similarly, source words project onto
target spans (red horizontal lines in Figure 1). pi(a)
contains a phrase pair iff every word in the target
span projects within the source span and vice versa.
Figure 1 contains an example for d = 2.
Formally, the mapping introduces a set of spans
?. We represent the spans as variables whose values
are intervals, where ?ei = [k, `] means that the tar-
get word ei projects to the source span [k, `]. The
set of legal values for ?ei includes any interval with
0 ? k ? ` < |f| and ` ? k < d, plus the special in-
terval [?1,?] that indicates ei is null-aligned. The
span variables for source words ?fj have target spans
[g, h] as values and are defined analogously.
For a set I of positions, we define the range func-
30
tion:
range(I) =
{
[?1,?] I = ?
[mini?I i,maxi?I i] else
(1)
For a fixed word alignment a we set the target
span variable ?ei :
?ei,s = range({j : aij = sure}) (2)
?ei,p = range({j : aij 6= off}) (3)
?ei = ?ei,s ? ?ei,p (4)
As illustrated in Figure 1, this sets ?ei to the min-
imal span containing all the source words with a
sure link to ei if there are any. Otherwise, because
of the special case for range(I) when I is empty,
?ei,s = [?1,?], so ?ei is the minimal span containing
all poss-aligned words. If all word links to ei are off,
indicating that ei is null-aligned, then ?ei is [?1,?],
preventing the alignment of any phrase pairs con-
taining ei.
Finally, we specify which phrase pairs should be
included in the extraction set pi. Given the spans ?
based on a, pi(a) sets pighk` = true iff every word in
each phrasal span projects within the other:
?ei ? [k, `] ?i ? [g, h] (5)
?fj ? [g, h] ?j ? [k, `]
2.2 Formulation as a Graphical Model
We score triples (a, pi, ?) as the dot product of a
weight vector w that parameterizes our model and a
feature vector ?(a, pi, ?). The feature vector decom-
poses into word alignment features ?a, phrase pair
features ?pi and target and source null word features
?e? and ?
f
? :
1
?(a, pi, ?) =
?
i,j
?a(aij) +
?
g,h,k,`
?pi(pighk`)+
?
i
?e?(?ei ) +
?
j
?f?(?
f
j ) (6)
This feature function is exactly the same as that
used by DeNero and Klein (2010).2 However, while
1In addition to the arguments we write out explicitly, all fea-
ture functions have access to the observed sentence pair (e, f).
2Although the null word features are not described in DeN-
ero and Klein (2010), all of their reported results include these
features (DeNero, 2010).
they formulated their inference problem as a search
for the highest scoring triple (a, pi, ?) for an ob-
served sentence pair (e, f), we wish to derive a con-
ditional probability distribution p(a, pi, ?|e, f). We
do this with the standard transformation for linear
models: p(a, pi, ?|e, f) ? exp(w??(a, pi, ?)). Due to
the factorization in Eq. (6), this exponentiated form
becomes a product of local multiplicative factors,
and hence our model forms an undirected graphical
model, or Markov random field.
In addition to the scoring function, our model
also includes constraints on which triples (a, pi, ?)
have nonzero probability. DeNero and Klein (2010)
implicitly included these constraints in their repre-
sentation: instead of sets of variables, they used a
structured representation that only encodes triples
(a, pi, ?) satisfying both the mapping pi = pi(a) and
the structural constraint that a can be generated by
a block ITG grammar. However, our inference pro-
cedure, BP, requires that we represent (a, pi, ?) as an
assignment of values to a set of variables. Therefore,
we must explicitly encode all constraints into the
multiplicative factors that define the model. To ac-
complish this, in addition to the soft scoring factors
we have already mentioned, our model also includes
a set of hard constraint factors. Hard constraint fac-
tors enforce the relationships between the variables
of the model by taking a value of 0 when the con-
straints they encode are violated and a value of 1
when they are satisfied. The full factor graph rep-
resentation of our model, including both soft scor-
ing factors and hard constraint factors, is drawn
schematically in Figure 2.
2.2.1 Soft Scoring Factors
The scoring factors all take the form exp(w ? ?),
and so can be described in terms of their respective
local feature vectors, ?. Depending on the values of
the variables each factor depends on, the factor can
be active or inactive. Features are only extracted for
active factors; otherwise ? is empty and the factor
produces a value of 1.
SURELINK. Each word alignment variable aij
has a corresponding SURELINK factor Lij to incor-
porate scores from the features ?a(aij). Lij is ac-
tive whenever aij = sure. ?a(aij) includes poste-
riors from unsupervised jointly trained HMM word
alignment models (Liang et al, 2006), dictionary
31
a11 a21
Lij
L11 L21
a12
ai1
Li1
L12 L22
a22
a1j aij
L1j
A
(a) ITG factor
agk ahk
ag? ah?
ag|f| ah|f|
a|e|k
a|e|?
Pghk?
Rghk?
?ghk?
Seg Seh
NehNeg
?eg ?eh
Sfk
Sf?
Nf?
Nfk
?fk
?f?
(b) SPAN and EXTRACT factors
Figure 2: A factor graph representation of the ITG-based extraction set model. For visual clarity, we draw the graph
separated into two components: one containing the factors that only neighbor word link variables, and one containing
the remaining factors.
and identical word features, a position distortion fea-
ture, and features for numbers and punctuation.
PHRASEPAIR. For each phrase pair variable
pighk`, scores from ?pi(pighk`) come from the factor
Rghk`, which is active if pighk` = true. Most of the
model?s features are on these factors, and include
relative frequency statistics, lexical template indica-
tor features, and indicators for numbers of words and
Chinese characters. See DeNero and Klein (2010)
for a more comprehensive list.
NULLWORD. We can determine if a word is
null-aligned by looking at its corresponding span
variable. Thus, we include features from ?e?(?ei ) in
a factor N ei that is active if ?ei = [?1,?]. The
features are mostly indicators for common words.
There are also factors Nfj for source words, which
are defined analogously.
2.2.2 Hard Constraint Factors
We encode the hard constraints on relationships
between variables in our model using three fami-
lies of factors, shown graphically in Figure 2. The
SPAN and EXTRACT factors together ensure that
pi = pi(a). The ITG factor encodes the structural
constraint on a.
SPAN. First, for each target word ei we include
a factor Sei to ensure that the span variable ?ei has
a value that agrees with the projection of the word
alignment a. As shown in Figure 2b, Sei depends
on ?ei and all the word alignment variables aij in
column i of the word alignment grid. Sei has value
1 iff the equality in Eq. (4) holds. Our model also
includes a factor Sfj to enforce the analogous rela-
tionship between each ?fj and corresponding row j
of a.
EXTRACT. For each phrase pair variable pighk`
we have a factor Pghk` to ensure that pighk` = true
iff it is licensed by the span projections ?. As shown
in Figure 2b, in addition to pighk`, Pghk` depends on
the range of span variables ?ei for i ? [g, h] and ?fj
for j ? [k, `]. Pghk` is satisfied when pighk` = true
and the relations in Eq. (5) all hold, or when pighk` =
false and at least one of those relations does not hold.
ITG. Finally, to enforce the structural constraint
on a, we include a single global factor A that de-
pends on all the word link variables in a (see Fig-
ure 2a). A is satisfied iff a is in the family of
block inverse transduction grammar (ITG) align-
ments. The block ITG family permits multiple links
to be on (aij 6= off) for a particular word ei via termi-
nal block productions, but ensures that every word is
32
in at most one such terminal production, and that the
full set of terminal block productions is consistent
with ITG reordering patterns (Zhang et al, 2008).
3 Relaxing the ITG Constraint
The ITG factor can be viewed as imposing two dif-
ferent types of constraints on allowable word align-
ments a. First, it requires that each word is aligned
to at most one relatively short subspan of the other
sentence. This is a linguistically plausible con-
straint, as it is rarely the case that a single word will
translate to an extremely long phrase, or to multiple
widely separated phrases.3
The other constraint imposed by the ITG factor
is the ITG reordering constraint. This constraint
is imposed primarily for reasons of computational
tractability: the standard dynamic program for bi-
text parsing depends on ITG reordering (Wu, 1997).
While this constraint is not dramatically restric-
tive (Haghighi et al, 2009), it is plausible that re-
moving it would permit the model to produce better
alignments. We tested this hypothesis by develop-
ing a new model that enforces only the constraint
that each word align to one limited-length subspan,
which can be viewed as a generalization of the at-
most-one-to-one constraint frequently considered in
the word-alignment literature (Taskar et al, 2005;
Cromie`res and Kurohashi, 2009).
Our new model has almost exactly the same form
as the previous one. The only difference is that A is
replaced with a new family of simpler factors:
ONESPAN. For each target word ei (and each
source word fj) we include a hard constraint factor
U ei (respectively U
f
j ). U ei is satisfied iff |?ei,p| < d
(length limit) and either ?ei,p = [?1,?] or ?j ?
?ei,p, aij 6= off (no gaps), with ?ei,p as in Eq. (3). Fig-
ure 3 shows the portion of the factor graph from Fig-
ure 2a redrawn with the ONESPAN factors replacing
the ITG factor. As Figure 3 shows, there is no longer
a global factor; each U ei depends only on the word
link variables from column i.
3Short gaps can be accomodated within block ITG (and in
our model are represented as possible links) as long as the total
aligned span does not exceed the block size.
a11 a21
Lij
L11 L21
a12
ai1
Li1
L12 L22
a22
a1j aij
L1j
Uf1
Uf2
Ufj
Ue1 Ue2 Uei
Figure 3: ONESPAN factors
4 Belief Propagation
Belief propagation is a generalization of the well
known sum-product algorithm for undirected graph-
ical models. We will provide only a procedural
sketch here, but a good introduction to BP for in-
ference in structured NLP models can be found
in Smith and Eisner (2008), and Chapters 16 and 23
of MacKay (2003) contain a general introduction to
BP in the more general context of message-passing
algorithms.
At a high level, each variable maintains a local
distribution over its possible values. These local dis-
tribution are updated via messages passed between
variables and factors. For a variable V , N (V ) de-
notes the set of factors neighboring V in the fac-
tor graph. Similarly, N (F ) is the set of variables
neighboring the factor F . During each round of BP,
messages are sent from each variable to each of its
neighboring factors:
q(k+1)V?F (v) ?
?
G?N (V ),G 6=F
r(k)G?V (v) (7)
and from each factor to each of its neighboring vari-
ables:
r(k+1)F?V (v) ?
?
XF ,XF [V ]=v
F (XF )
?
U?N (F ),U 6=V
q(k)U?F (v) (8)
where XF is a partial assignment of values to just
the variables in N (F ).
33
Marginal beliefs at time k can be computed by
simply multiplying together all received messages
and normalizing:
b(k)V (v) ?
?
G?N (V )
r(k)G?V (v) (9)
Although messages can be updated according to
any schedule, generally one iteration of BP updates
each message once. The process iterates until some
stopping criterion has been met: either a fixed num-
ber of iterations or some convergence metric.
For our models, we say that BP has converged
whenever
?
V,v
(
b(k)V (v)? b
(k?1)
V (v)
)2
< ? for
some small ? > 0.4 While we have no theoretical
convergence guarantees, it usually converges within
10 iterations in practice.
5 Efficient BP for Extraction Set Models
In general, the efficiency of BP depends directly on
the arity of the factors in the model. Performed
na??vely, the sum in Eq. (8) will take time that grows
exponentially with the size of N (F ). For the soft-
scoring factors, which each depend only on a single
variable, this isn?t a problem. However, our model
also includes factors whose arity grows with the in-
put size: for example, explicitly enumerating all as-
signments to the word link variables that the ITG
factor depends on would take O(3n2) time.5
To run BP in a reasonable time frame, we need
efficient factor-specific propagators that can exploit
the structure of the factor functions to compute out-
going messages in polynomial time (Duchi et al,
2007; Smith and Eisner, 2008). Fortunately, all of
our hard constraints permit dynamic programs that
accomplish this propagation. Space does not permit
a full description of these dynamic programs, but we
will briefly sketch the intuitions behind them.
SPAN and ONESPAN. Marginal beliefs for Sei or
U ei can be computed inO(nd2) time. The key obser-
vation is that for any legal value ?ei = [k, `], Sei and
U ei require that aij = off for all j /? [k, `].6 Thus, we
start by computing the product of all the off beliefs:
4We set ? = 0.001.
5For all asymptotic analysis, we define n = max(|e|, |f|).
6For ease of exposition, we assume that all alignments are
either sure or off ; the modifications to account for the general
case are straightforward.
Factor Runtime Count Total
SURELINK O(1) O(n2) O(n2)
PHRASEPAIR O(1) O(n2d2) O(n2d2)
NULLWORD O(nd) O(n) O(n2d)
SPAN O(nd2) O(n) O(n2d2)
EXTRACT O(d3) O(n2d2) O(n2d5)
ITG O(n6) 1 O(n6)
ONESPAN O(nd2) O(n) O(n2d2)
Table 1: Asymptotic complexity for all factors.
b? =
?
j qaij (off). Then, for each of the O(nd) legal
source spans [k, `] we can efficiently find a joint be-
lief by summing over consistent assignments to the
O(d) link variables in that span.
EXTRACT. Marginal beliefs for Pghk` can be
computed inO(d3) time. For each of theO(d) target
words, we can find the total incoming belief that ?ei
is within [k, `] by summing over the O(d2) values
[k?, `?] where [k?, `?] ? [k, `]. Likewise for source
words. Multiplying together these per-word beliefs
and the belief that pighk` = true yields the joint be-
lief of a consistent assignment with pighk` = true,
which can be used to efficiently compute outgoing
messages.
ITG. To build outgoing messages, the ITG fac-
torA needs to compute marginal beliefs for all of the
word link variables aij . These can all be computed
in O(n6) time by using a standard bitext parser to
run the inside-outside algorithm. By using a normal
form grammar for block ITG with nulls (Haghighi
et al, 2009), we ensure that there is a 1-1 correspon-
dence between the ITG derivations the parser sums
over and word alignments a that satisfy A.
The asymptotic complexity for all the factors is
shown in Table 1. The total complexity for inference
in each model is simply the sum of the complexities
of its factors, so the complexity of the ITG model is
O(n2d5 + n6), while the complexity of the relaxed
model is just O(n2d5). The complexity of exact in-
ference, on the other hand, is exponential in d for the
ITG model and exponential in both d and n for the
relaxed model.
34
6 Training and Decoding
We use BP to compute marginal posteriors, which
we use at training time to get expected feature counts
and at test time for posterior decoding. For each sen-
tence pair, we continue to pass messages until either
the posteriors converge, or some maximum number
of iterations has been reached.7 After running BP,
the marginals we are interested in can all be com-
puted with Eq. (9).
6.1 Training
We train the model to maximize the log likelihood of
manually word-aligned gold training sentence pairs
(with L2 regularization). Because pi and ? are deter-
mined when a is observed, the model has no latent
variables. Therefore, the gradient takes the standard
form for loglinear models:
OLL = ?(a, pi, ?) ? (10)
?
a?,pi?,??
p(a?, pi?, ??|e, f)?(a?, pi?, ??)? ?w
The feature vector ? contains features on sure
word links, extracted phrase pairs, and null-aligned
words. Approximate expectations of these features
can be efficiently computed using the marginal be-
liefs baij (sure), bpighk`(true), and b?ei ([?1,?]) and
b?fj ([?1,?]), respectively. We learned our final
weight vectorw using AdaGrad (Duchi et al, 2010),
an adaptive subgradient version of standard stochas-
tic gradient ascent.
6.2 Testing
We evaluate our model by measuring precision and
recall on extracted phrase pairs. Thus, the decod-
ing problem takes a sentence pair (e, f) as input, and
must produce an extraction set pi as output. Our ap-
proach, posterior thresholding, is extremely simple:
we set pighk` = true iff bpighk`(true) ? ? for some
fixed threshold ? . Note that this decoding method
does not require that there be any underlying word
alignment a licensing the resulting extraction set pi,8
7See Section 7.2 for an empirical investigation of this maxi-
mum.
8This would be true even if we computed posteriors ex-
actly, but is especially true with approximate marginals from
BP, which are not necessarily consistent.
but the structure of the model is such that two con-
flicting phrase pairs are unlikely to simultaneously
have high posterior probability.
Most publicly available translation systems ex-
pect word-level alignments as input. These can
also be generated by applying posterior threshold-
ing, aligning target word i to source word j when-
ever baij (sure) ? t.9
7 Experiments
Our experiments are performed on Chinese-to-
English alignment. We trained and evaluated all
models on the NIST MT02 test set, which consists
of 150 training and 191 test sentences and has been
used previously in alignment experiments (Ayan and
Dorr, 2006; Haghighi et al, 2009; DeNero and
Klein, 2010). The unsupervised HMM word aligner
used to generate features for the model was trained
on 11.3 million words of FBIS newswire data. We
test three models: the Viterbi ITG model of DeNero
and Klein (2010), our BP ITG model that uses the
ITG factor, and our BP Relaxed model that replaces
the ITG factor with the ONESPAN factors. In all of
our experiments, the phrase length d was set to 3.10
7.1 Phrase Alignment
We tested the models by computing precision and
recall on extracted phrase pairs, relative to the gold
phrase pairs of up to length 3 induced by the gold
word alignments. For the BP models, we trade
off precision and recall by adjusting the decoding
threshold ? . The Viterbi ITG model was trained to
optimize F5, a recall-biased measure, so in addition
to F1, we also report the recall-biased F2 and F5
measures. The maximum number of BP iterations
was set to 5 for the BP ITG model and to 10 for the
BP Relaxed model.
The phrase alignment results are shown in Fig-
ure 4. The BP ITG model performs comparably to
the Viterbi ITG model. However, because posterior
decoding permits explicit tradeoffs between preci-
sion and recall, it can do much better in the recall-
biased measures, even though the Viterbi ITG model
was explicitly trained to maximize F5 (DeNero and
9For our experiments, we set t = 0.2.
10Because the runtime of the Viterbi ITG model grows expo-
nentially with d, it was not feasible to perform comparisons for
higher phrase lengths.
35
beta p r f2 0.69 0.742 0.7309823
60 
65 
70 
75 
80 
60 65 70 75 80 85 
Re
ca
ll 
Precision 
Viterbi ITG BP ITG BP Relaxed 
Model
Best Scores Sentences
F1 F2 F5 per Second
Viterbi ITG 71.6 73.1 74.0 0.21
BP ITG 71.8 74.8 83.5 0.11
BP Relaxed 72.6 75.2 84.5 1.15
Figure 4: Phrase alignment results. A portion of the Pre-
cision/Recall curve is plotted for the BP models, with the
result from the Viterbi ITG model provided for reference.
Klein, 2010). The BP Relaxed model performs the
best of all, consistently achieving higher recall for
fixed precision than either of the other models. Be-
cause of its lower asymptotic runtime, it is also much
faster: over 5 times as fast as the Viterbi ITG model
and over 10 times as fast as the BP ITG model.11
7.2 Timing
BP approximates marginal posteriors by iteratively
updating beliefs for each variable based on cur-
rent beliefs about other variables. The iterative na-
ture of the algorithm permits us to make an explicit
speed/accuracy tradeoff by limiting the number of
iterations. We tested this tradeoff by limiting both
of the BP models to run for 2, 3, 5, 10, and 20 iter-
ations. The results are shown in Figure 5. Neither
model benefits from running more iterations than
used to obtain the results in Figure 4, but each can
be sped up by a factor of almost 1.5x in exchange
for a modest (< 1 F1) drop in accuracy.
11The speed advantage of Viterbi ITG over BP ITG comes
from Viterbi ITG?s aggressive beaming.
Speed F12.08333333 61.32 67.61.58730159 71.91.14942529 72.60.96153846 72.6
67 
68 
69 
70 
71 
72 
73 
0.5 1 2 4 8 16 
Be
st 
F1
 
Time (seconds per sentence) 
Viterbi ITG BP ITG BP Relaxed 
67 
68 
69 
70 
71 
72 
73 
0.0625 0.125 0.25 0.5 1 2 
Be
st 
F1
 
Speed (sentences per second) 
Viterbi ITG BP ITG BP Relaxed 
Figure 5: Speed/accuracy tradeoff. The speed axis is on
a logarithmic scale. From fastest to slowest, data points
correspond to maximums of 2, 5, 10, and 20 BP itera-
tions. F1 for the BP Relaxed model was very low when
limited to 2 iterations, so that data point is outside the
visible area of the graph.
Model BLEU
Relative Hours to
Improve. Train/Align
Baseline 32.8 +0.0 5
Viterbi ITG 33.5 +0.7 831
BP Relaxed 33.6 +0.8 39
Table 2: Machine translation results.
7.3 Translation
We ran translation experiments using Moses (Koehn
et al, 2007), which we trained on a 22.1 mil-
lion word parallel corpus from the GALE program.
We compared alignments generated by the baseline
HMM model, the Viterbi ITG model and the Re-
laxed BP model.12 The systems were tuned and
evaluated on sentences up to length 40 from the
NIST MT04 and MT05 test sets. The results, shown
in Table 2, show that the BP Relaxed model achives
a 0.8 BLEU improvement over the HMM baseline,
comparable to that of the Viterbi ITG model, but tak-
ing a fraction of the time,13 making the BP Relaxed
model a practical alternative for real translation ap-
plications.
12Following a simplified version of the procedure described
by DeNero and Klein (2010), we added rule counts from the
HMM alignments to the extraction set algners? counts.
13Some of the speed difference between the BP Relaxed and
Viterbi ITG models comes from better parallelizability due to
drastically reduced memory overhead of the BP Relaxed model.
36
8 Conclusion
For performing inference in a state-of-the-art, but in-
efficient, alignment model, belief propagation is a
viable alternative to greedy search methods, such as
beaming. BP also results in models that are much
more scalable, by reducing the asymptotic complex-
ity of inference. Perhaps most importantly, BP per-
mits the relaxation of artificial constraints that are
generally taken for granted as being necessary for
efficient inference. In particular, a relatively mod-
est relaxation of the ITG constraint can directly be
applied to any model that uses ITG-based inference
(e.g. Zhang and Gildea, 2005; Cherry and Lin, 2007;
Haghighi et al, 2009).
Acknowledgements
This project is funded by an NSF graduate research
fellowship to the first author and by BBN under
DARPA contract HR0011-06-C-0022.
References
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going be-
yond AER: An extensive analysis of word alignments
and their impact on MT. In ACL.
Alexandra Birch, Chris Callison-Burch, and Miles Os-
borne. 2006. Constraining the phrase-based, joint
probability statistical translation model. In AMTA.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In ACL-IJCNLP.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling. In
NAACL Workshop on Syntax and Structure in Statisti-
cal Translation.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Fabien Cromie`res and Sadao Kurohashi. 2009. An
alignment algorithm using belief propagation and a
structure-based distortion model. In EACL.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In EMNLP-CoNLL.
John DeNero and Dan Klein. 2010. Discriminative mod-
eling of extraction sets for machine translation. In
ACL.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In NAACL Workshop on Statistical
Machine Translation.
John DeNero, Alexandre Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In EMNLP.
John DeNero. 2010. Personal Communication.
John Duchi, Danny Tarlow, Gal Elidan, and Daphne
Koller. 2007. Using combinatorial optimization
within max-product belief propagation. In NIPS 2006.
John Duchi, Elad Hazan, and Yoram Singer. 2010.
Adaptive subgradient methods for online learning and
stochastic optimization. In COLT.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In HLT-
NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
COLING-ACL.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In ACL-IJCNLP.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In ACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In ACL
SSST.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In HLT-NAACL.
David J.C. MacKay. 2003. Information theory, infer-
ence, and learning algorithms. Cambridge Univ Press.
Daniel Marcu and Daniel Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In EMNLP.
Jan Niehues and Stephan Vogel. 2008. Discriminative
word alignment via alignment matrix modeling. In
ACL Workshop on Statistical Machine Translation.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In EMNLP.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005.
A discriminative matching approach to word align-
ment. In EMNLP.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Hao Zhang and Daniel Gildea. 2005. Stochastic lexical-
ized inversion transduction grammar for alignment. In
ACL.
37
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
ACL:HLT.
38
