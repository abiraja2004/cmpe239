Proceedings of the 2010 Workshop on Applications of Tree Automata in Natural Language Processing, ACL 2010, pages 1?9,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Preservation of Recognizability for
Synchronous Tree Substitution Grammars
Zolta?n Fu?lo?p
Department of Computer Science
University of Szeged
Szeged, Hungary
Andreas Maletti
Departament de Filologies Roma`niques
Universitat Rovira i Virgili
Tarragona, Spain
Heiko Vogler
Faculty of Computer Science
Technische Universita?t Dresden
Dresden, Germany
Abstract
We consider synchronous tree substitution
grammars (STSG). With the help of a
characterization of the expressive power
of STSG in terms of weighted tree bimor-
phisms, we show that both the forward and
the backward application of an STSG pre-
serve recognizability of weighted tree lan-
guages in all reasonable cases. As a con-
sequence, both the domain and the range
of an STSG without chain rules are recog-
nizable weighted tree languages.
1 Introduction
The syntax-based approach to statistical machine
translation (Yamada and Knight, 2001) becomes
more and more competitive in machine transla-
tion, which is a subfield of natural language pro-
cessing (NLP). In this approach the full parse trees
of the involved sentences are available to the trans-
lation model, which can base its decisions on this
rich structure. In the competing phrase-based ap-
proach (Koehn et al, 2003) the translation model
only has access to the linear sentence structure.
There are two major classes of syntax-based
translation models: tree transducers and synchro-
nous grammars. Examples in the former class
are the top-down tree transducer (Rounds, 1970;
Thatcher, 1970), the extended top-down tree trans-
ducer (Arnold and Dauchet, 1982; Galley et al,
2004; Knight and Graehl, 2005; Graehl et al,
2008; Maletti et al, 2009), and the extended
multi bottom-up tree transducer (Lilin, 1981; En-
gelfriet et al, 2009; Maletti, 2010). The lat-
ter class contains the syntax-directed transduc-
tions of Lewis II and Stearns (1968), the gen-
eralized syntax-directed transductions (Aho and
Ullman, 1969), the synchronous tree substitu-
tion grammar (STSG) by Schabes (1990) and the
synchronous tree adjoining grammar (STAG) by
Abeille? et al (1990) and Shieber and Schabes
(1990). The first bridge between those two classes
were established in (Martin and Vere, 1970). Fur-
ther comparisons can be found in (Shieber, 2004)
for STSG and in (Shieber, 2006) for STAG.
One of the main challenges in NLP is the am-
biguity that is inherent in natural languages. For
instance, the sentence ?I saw the man with the
telescope? has several different meanings. Some
of them can be distinguished by the parse tree,
so that probabilistic parsers (Nederhof and Satta,
2006) for natural languages can (partially) achieve
the disambiguation. Such a parser returns a set
of parse trees for each input sentence, and in
addition, each returned parse tree is assigned a
likelihood. Thus, the result can be seen as a
mapping from parse trees to probabilities where
the impossible parses are assigned the probabil-
ity 0. Such mappings are called weighted tree lan-
guages, of which some can be finitely represented
by weighted regular tree grammars (Alexandrakis
and Bozapalidis, 1987). Those weighted tree
languages are recognizable and there exist algo-
rithms (Huang and Chiang, 2005) that efficiently
extract the k-best parse trees (i.e., those with the
highest probability) for further processing.
In this paper we consider synchronized tree sub-
stitution grammars (STSG). To overcome a techni-
cal difficulty we add (grammar) nonterminals to
them. Since an STSG often uses the nontermi-
nals of a context-free grammar as terminal sym-
bols (i.e., its derived trees contain both termi-
nal and nonterminal symbols of the context-free
grammar), we call the newly added (grammar)
nonterminals of the STSG states. Substitution does
no longer take place at synchronized nonterminals
(of the context-free grammar) but at synchronized
states (one for the input and one for the output
side). The states themselves will not appear in the
final derived trees, which yields that it is sufficient
to assume that only identical states are synchro-
1
nized. Under those conventions a rule of an STSG
has the form q ? (s, t, V, a) where q is a state,
a ? R?0 is the rule weight, s is an input tree that
can contain states at the leaves, and t is an output
tree that can also contain states. Finally, the syn-
chronization is defined by V , which is a bijection
between the state-labeled leaves of s and t. We
require that V only relates identical states.
The rules of an STSG are applied in a step-wise
manner. Here we use a derivation relation to define
the semantics of an STSG. It can be understood as
the synchronization of the derivation relations of
two regular tree grammars (Ge?cseg and Steinby,
1984; Ge?cseg and Steinby, 1997) where the syn-
chronization is done on nonterminals (or states) in
the spirit of syntax-directed transductions (Lewis
II and Stearns, 1968). Thus each sentential form
is a pair of (nonterminal-) connected trees.
An STSG G computes a mapping ?G , called
its weighted tree transformation, that assigns a
weight to each pair of input and output trees,
where both the input and output tree may not con-
tain any state. This transformation is obtained as
follows: We start with two copies of the initial
state that are synchronized. Given a connected tree
pair (?, ?), we can apply the rule q ? (s, t, V, a)
to each pair of synchronized states q. Such an ap-
plication replaces the selected state q in ? by s and
the corresponding state q in ? by t. All the re-
maining synchronized states and the synchronized
states of V remain synchronized. The result is
a new connected tree pair. This step charges the
weight a. The weights of successive applications
(or steps) are multiplied to obtain the weight of the
derivation. The weighted tree transformation ?G
assigns to each pair of trees the sum of all weights
of derivations that derive that pair.
Shieber (2004) showed that for every classical
unweighted STSG there exists an equivalent bi-
morphism (Arnold and Dauchet, 1982). The con-
verse result only holds up to deterministic rela-
belings (Ge?cseg and Steinby, 1984; Ge?cseg and
Steinby, 1997), which remove the state informa-
tion from the input and output tree. It is this dif-
ference that motivates us to add states to STSG.
We generalize the result of Shieber (2004) and
prove that every weighted tree transformation that
is computable by an STSG can also be computed
by a weighted bimorphism and vice versa.
Given an STSG and a recognizable weighted
tree language ? of input trees, we investigate un-
der which conditions the weighted tree language
obtained by applying G to ? is again recognizable.
In other words, we investigate under which condi-
tions the forward application of G preserves rec-
ognizability. The same question is investigated for
backward application, which is the corresponding
operation given a recognizable weighted tree lan-
guage of output trees. Since STSG are symmet-
ric (i.e., input and output can be exchanged), the
results for backward application can be obtained
easily from the results for forward application.
Our main result is that forward application pre-
serves recognizability if the STSG G is output-
productive, which means that each rule of G con-
tains at least one output symbol that is not a state.
Dually, backward application preserves recogniz-
ability if G is input-productive, which is the anal-
ogous property for the input side. In fact, those re-
sults hold for weights taken from an arbitrary com-
mutative semiring (Hebisch and Weinert, 1998;
Golan, 1999), but we present the results only for
probabilities.
2 Preliminary definitions
In this contribution we will work with ranked
trees. Each symbol that occurs in such a tree
has a fixed rank that determines the number of
children of nodes with this label. Formally, let
? be a ranked alphabet, which is a finite set ?
together with a mapping rk? : ? ? N that asso-
ciates a rank rk?(?) with every ? ? ?. We let
?k = {? ? ? | rk?(?) = k} be the set contain-
ing all symbols in ? that have rank k. A ?-tree
indexed by a set Q is a tree with nodes labeled by
elements of ? ? Q, where the nodes labeled by
some ? ? ? have exactly rk?(?) children and the
nodes with labels ofQ have no children. Formally,
the set T?(Q) of (term representations of) ?-trees
indexed by a set Q is the smallest set T such that
? Q ? T and
? ?(t1, . . . , tk) ? T for every ? ? ?k and
t1, . . . , tk ? T .
We generally write ? instead of ?() for all ? ? ?0.
We frequently work with the set pos(t) of po-
sitions of a ?-tree t, which is defined as fol-
lows. If t ? Q, then pos(t) = {?}, and if
t = ?(t1, . . . , tk), then
pos(t) = {?} ? {iw | 1 ? i ? k,w ? pos(ti)} .
Thus, each position is a finite (possibly empty) se-
quence of natural numbers. Clearly, each position
2
designates a node of the tree, and vice versa. Thus
we identify nodes with positions. As usual, a leaf
is a node that has no children. The set of all leaves
of t is denoted by lv(t). Clearly, lv(t) ? pos(t).
The label of a position w ? pos(t) is denoted
by t(w). Moreover, for every A ? ? ? Q, let
posA(t) = {w ? pos(t) | t(w) ? A} and
lvA(t) = posA(t) ? lv(t) be the sets of po-
sitions and leaves that are labeled with an ele-
ment of A, respectively. Let t ? T?(Q) and
w1, . . . , wk ? lvQ(t) be k (pairwise) different
leaves. We write t[w1 ? t1, . . . , wk ? tk] or just
t[wi ? ti | 1 ? i ? k] with t1, . . . , tk ? T?(Q)
for the tree obtained from t by replacing, for every
1 ? i ? k, the leaf wi with the tree ti.
For the rest of this paper, let ? and ? be two
arbitrary ranked alphabets. To avoid consistency
issues, we assume that a symbol ? that occurs in
both ? and ? has the same rank in ? and ?; i.e.,
rk?(?) = rk?(?). A deterministic relabeling is
a mapping r : ? ? ? such that r(?) ? ?k for
every ? ? ?k. For a tree s ? T?, the relabeled
tree r(s) ? T? is such that pos(r(s)) = pos(s)
and
(
r(s)
)
(w) = r(s(w)) for every w ? pos(s).
The class of tree transformations computed by de-
terministic relabelings is denoted by dREL.
A tree language (over ?) is a subset of T?. Cor-
respondingly, a weighted tree language (over ?)
is a mapping ? : T? ? R?0. A weighted tree
transformation (over ? and ?) is a mapping
? : T? ? T? ? R?0. Its inverse is the weighted
tree transformation ??1 : T??T? ? R?0, which
is defined by ??1(t, s) = ?(s, t) for every t ? T?
and s ? T?.
3 Synchronous tree substitution
grammars with states
Let Q be a finite set of states with a distinguished
initial state qS ? Q. A connected tree pair is a
tuple (s, t, V, a) where s ? T?(Q), t ? T?(Q),
and a ? R?0. Moreover, V : lvQ(s) ? lvQ(t) is
a bijective mapping such that s(u) = t(v) for ev-
ery (u, v) ? V . We will often identify V with its
graph. Intuitively, a connected tree pair (s, t, V, a)
is a pair of trees (s, t) with a weight a such that
each node labeled by a state in s has a correspond-
ing node in t, and vice versa. Such a connected
tree pair (s, t, V, a) is input-productive and output-
productive if s /? Q and t /? Q, respectively. Let
Conn denote the set of all connected tree pairs that
use the index setQ. Moreover, let Connp ? Conn
contain all connected tree pairs that are input- or
output-productive.
A synchronous tree substitution grammar G
(with states) over ?, ?, and Q (for short: STSG),
is a finite set of rules of the form q ? (s, t, V, a)
where q ? Q and (s, t, V, a) ? Connp. We call
a rule q ? (s, t, V, a) a q-rule, of which q and
(s, t, V, a) are the left-hand and right-hand side,
respectively, and a is its weight. The STSG G is
input-productive (respectively, output-productive)
if each of its rules is so. To simplify the following
development, we assume (without loss of general-
ity) that two different q-rules differ on more than
just their weight.1
To make sure that we do not account essentially
the same derivation twice, we have to use a deter-
ministic derivation mode. Since the choice is im-
material, we use the leftmost derivation mode for
the output component t of a connected tree pair
(s, t, V, a). For every (s, t, V, a) ? Conn such
that V 6= ?, the leftmost output position is the
pair (w,w?) ? V , where w? is the leftmost (i.e.,
the lexicographically smallest) position of lvQ(t).
Next we define derivations. The derivation re-
lation induced by G is the binary relation ?G
over Conn such that
? = (s1, t1, V1, a1)?G (s2, t2, V2, a2) = ?
if and only if the leftmost output position of ? is
(w,w?) ? V1 and there exists a rule
s1(w)? (s, t, V, a) ? G
such that
? s2 = s1[w ? s] and t2 = t1[w? ? t],
? V2 = (V1 \ {(w,w?)}) ? V ? where
V ? = {(ww1, w?w2) | (w1, w2) ? V }, and
? a2 = a1 ? a.
A sequence D = (?1, . . . , ?n) ? Connn is a
derivation of (s, t, V, a) ? Conn from q ? Q if
? ?1 = (q, q, {(?, ?)}, 1),
? ?n = (s, t, V, a), and
? ?i ?G ?i+1 for every 1 ? i ? n? 1.
The set of all such derivations is denoted by
DqG(s, t, V, a).
For every q ? Q, s ? T?(Q), t ? T?(Q), and
bijection V : lvQ(s)? lvQ(t), let
? qG(s, t, V ) =
?
a?R?0,D?D
q
G(s,t,V,a)
a .
1Formally, q ? (s, t, V, a) ? G and q ? (s, t, V, b) ? G
implies a = b.
3
o 1? o ?G,lo
?
e o
6?1
?
?
o e
?G,lo
?
e ?
18?1
?
?
? e
?G,lo
?
?
o o
? 36?1?
?
? ?
o o
?G,lo
?
?
o ?
? 108?1?
?
? ?
? o
?G,lo
?
?
? ?
? 324?1?
?
? ?
? ?
Figure 1: Example derivation with the STSG G of Example 1.
Finally, the weighted tree transformation com-
puted by G is the weighted tree transformation
?G : T? ? T? ? R?0 with ?G(s, t) = ?
qS
G (s, t, ?)
for every s ? T? and t ? T?. As usual, we
call two STSG equivalent if they compute the same
weighted tree transformation. We observe that
every STSG is essentially a linear, nondeleting
weighted extended top-down (or bottom-up) tree
transducer (Arnold and Dauchet, 1982; Graehl et
al., 2008; Engelfriet et al, 2009) without (both-
sided) epsilon rules, and vice versa.
Example 1. Let us consider the STSG G over
? = ? = {?, ?} and Q = {e, o} where qS = o,
rk(?) = 2, and rk(?) = 0. The STSG G consists
of the following rules where V = {(1, 2), (2, 1)}
and id = {(1, 1), (2, 2)}:
o? (?(o, e), ?(e, o), V, 1/3) (?1)
o? (?(e, o), ?(o, e), V, 1/6) (?2)
o? (?(e, o), ?(e, o), id, 1/6) (?3)
o? (?, ?, ?, 1/3) (?4)
e? (?(e, e), ?(e, e), V, 1/2) (?5)
e? (?(o, o), ?(o, o), V, 1/2) (?6)
Figure 1 shows a derivation induced by G. It can
easily be checked that ?G(s, t) = 16?3?2?3?3 where
s = ?(?(?, ?), ?) and t = ?(?, ?(?, ?)). More-
over, ?G(s, s) = ?G(s, t). If ?
q
G(s, t, ?) 6= 0 with
q ? {e, o}, then s and t have the same number
of ?-labeled leaves. This number is odd if q = o,
otherwise it is even. Moreover, at every position
w ? pos(s), the left and right subtrees s1 and s2
are interchanged in s and t (due to V in the rules
?1, ?2, ?5, ?6) except if s1 and s2 contain an even
and odd number, respectively, of ?-labeled leaves.
In the latter case, the subtrees can be interchanged
or left unchanged (both with probability 1/6).
4 Recognizable weighted tree languages
Next, we recall weighted regular tree grammars
(Alexandrakis and Bozapalidis, 1987). To keep
the presentation simple, we identify WRTG with
particular STSG, in which the input and the out-
put components are identical. More precisely, a
weighted regular tree grammar over ? and Q (for
short: WRTG) is an STSG G over ?, ?, and Q
where each rule has the form q ? (s, s, id, a)
where id is the suitable (partial) identity mapping.
It follows that s /? Q, which yields that we do not
have chain rules. In the rest of this paper, we will
specify a rule q ? (s, s, id, a) of a WRTG sim-
ply by q
a
? s. For every q ? Q, we define the
weighted tree language ?qG : T?(Q) ? R?0 gen-
erated by G from q by ?qG(s) = ?
q
G(s, s, idlvQ(s))
for every s ? T?(Q), where idlvQ(s) is the iden-
tity on lvQ(s). Moreover, the weighted tree lan-
guage ?G : T? ? R?0 generated by G is defined
by ?G(s) = ?
qS
G (s) for every s ? T?.
A weighted tree language ? : T? ? R?0 is
recognizable if there exists a WRTG G such that
? = ?G . We note that our notion of recognizabil-
ity coincides with the classical one (Alexandrakis
and Bozapalidis, 1987; Fu?lo?p and Vogler, 2009).
Example 2. We consider the WRTGK over the in-
put alphabet ? = {?, ?} and P = {p, q} with
qS = q, rk(?) = 2, and rk(?) = 0. The WRTG K
contains the following rules:
q
0.4
? ?(p, ?) q
0.6
? ? p
1
? ?(?, q) (?1??3)
Let s ? T? be such that ?K(s) 6= 0. Then s is a
thin tree with zig-zag shape; i.e., there exists n ? 1
such that pos(s) contains exactly the positions:
? (12)i for every 0 ? i ? bn?12 c, and
? (12)i1, (12)i2, and (12)i11 for every integer
0 ? i ? bn?32 c.
The integer n can be understood as the length of
a derivation that derives s from q. Some example
4
??
?
? ?
?
?
?
? ?
?
? ?
?
?
weight: 0.6 weight: 0.24 weight: 0.096
Figure 2: Example trees and their weight in ?G
where G is the WRTG of Example 2.
trees with their weights are displayed in Figure 2.
Proposition 3. For every WRTG G there is an
equivalent WRTG G? in normal form, in which the
right-hand side of every rule contains exactly one
symbol of ?.
Proof. We can obtain the statement by a trivial ex-
tension to the weighted case of the approach used
in Lemma II.3.4 of (Ge?cseg and Steinby, 1984)
and Section 6 of (Ge?cseg and Steinby, 1997).
5 STSG and weighted bimorphisms
In this section, we characterize the expressive
power of STSG in terms of weighted bimorphisms.
This will provide a conceptually clear pattern for
the construction in our main result (see Theo-
rem 6) concerning the closure of recognizable
weighted tree languages under forward and back-
ward application. For this we first recall tree ho-
momorphisms. Let ? and ? be two ranked al-
phabets. Moreover, let h : ? ? T? ? (N?)?
be a mapping such that h(?) = (s, u1, . . . , uk)
for every ? ? ?k where s ? T? and all leaves
u1, . . . , uk ? lv(s) are pairwise different. The
mapping h induces the (linear and complete) tree
homomorphism h? : T? ? T?, which is defined by
h?(?(d1, . . . , dk)) = s[u1 ? d?1, . . . , uk ? d?k]
for every ? ? ?k and d1, . . . , dk ? T? with
h(?) = (s, u1, . . . , uk) and d?i = h?(di) for ev-
ery 1 ? i ? k. Moreover, every (linear and
complete) tree homomorphism is induced in this
way. In the rest of this paper we will not distin-
guish between h and h? and simply write h instead
of h?. The homomorphism h is order-preserving
if u1 < ? ? ? < uk for every ? ? ?k where
h(?) = (s, u1, . . . , uk). Finally, we note that
every ? ? dREL can be computed by a order-
preserving tree homomorphism.
A weighted bimorphism B over ? and ? con-
sists of a WRTG K over ? and P and two tree ho-
T? R?0
T? ? T?
(hin, hout)
?K
?B
Figure 3: Illustration of the semantics of the bi-
morphism B.
momorphisms
hin : T? ? T? and hout : T? ? T? .
The bimorphism B computes the weighted tree
transformation ?B : T? ? T? ? R?0 with
?B(s, t) =
?
d?h?1in (s)?h
?1
out(t)
?K(d)
for every s ? T? and t ? T?.
Without loss of generality, we assume that ev-
ery bimorphism B is presented by an WRTG K in
normal form and an order-preserving output ho-
momorphism hout. Next, we prepare the relation
between STSG and weighted bimorphisms. Let
G be an STSG over ?, ?, and Q. Moreover, let
B be a weighted bimorphism over ? and ? con-
sisting of (i) K over ? and P in normal form,
(ii) hin, and (iii) order-preserving hout. We say
that G and B are related if Q = P and there
is a bijection ? : G ? K such that, for every
rule ? ? G with ? = (q ? (s, t, V, a)) and
?(?) = (p
a
? ?(p1, . . . , pk)) we have
? p = q,
? hin(?) = (s, u1, . . . , uk),
? hout(?) = (t, v1, . . . , vk),
? V = {(u1, v1), . . . , (uk, vk)}, and
? s(ui) = pi = t(vi) for every 1 ? i ? k.
Let G and B be related. The following three easy
statements can be used to prove that G and B are
equivalent:
1. For every derivation D ? DqG(s, t, ?, a) with
q ? Q, s ? T?, t ? T?, a ? R?0, there exists
d ? T? and a derivation D? ? D
q
K(d, d, ?, a)
such that hin(d) = s and hout(d) = t.
2. For every d ? T? and D? ? D
q
K(d, d, ?, a)
with q ? Q and a ? R?0, there exists a
derivation D ? DqG(hin(d), hout(d), ?, a).
3. The mentioned correspondence on deriva-
tions is a bijection.
Given an STSG G, we can easily construct a
weighted bimorphism B such that G and B are re-
lated, and vice versa. Hence, STSG and weighted
5
bimorphisms are equally expressive, which gener-
alizes the corresponding characterization result in
the unweighted case by Shieber (2004), which we
will state after the introduction of STSG?.
Classical synchronous tree substitution gram-
mars (STSG?) do not have states. An STSG? can
be seen as an STSG by considering every substitu-
tion site (i.e., each pair of synchronised nontermi-
nals) as a state.2 We illustrate this by means of an
example here. Let us consider the STSG? G with
the following rules:
? (S(?,B?), S(D?, ?)) with weight 0.2
? (B(?,B?), D(?,D?)) with weight 0.3
? (B(?), D(?)) with weight 0.4.
The substitution sites are marked with ?. Any
rule with root A can be applied to a substitution
site A?. An equivalent STSG G? has the rules:
?S, S? ? (S(?, ?B,D?), S(?B,D?, ?), V, 0.2)
?B,D? ? (B(?, ?B,D?), D(?, ?B,D?), V ?, 0.3)
?B,D? ? (B(?), D(?), ?, 0.4) ,
where V = {(2, 1)} and V ? = {(2, 2)}. It is easy
to see that G and G? are equivalent.
Let ? = {?, ??, ???, ?, ?} where ?, ??, ??? ? ?1
and ?, ? ? ?0 (and ?? 6= ??? and ? 6= ?). We write
?m(t) with t ? T? for the tree ?(? ? ? ?(t) ? ? ? ) con-
taining m occurrences of ? above t. STSG? have a
certain locality property, which yields that STSG?
cannot compute transformations like
?(s, t) =
?
??
??
1 if s = ??(?m(?)) = t
or s = ???(?m(?)) = t
0 otherwise
for every s, t ? T?. The non-local feature is the
correspondence between the symbols ?? and ? (in
the first alternative) and the symbols ??? and ? (in
the second alternative). An STSG that computes ?
is presented in Figure 4.
Theorem 4. Let ? be a weighted tree transforma-
tion. Then the following are equivalent.
1. ? is computable by an STSG.
2. ? is computable by a weighted bimorphism.
3. There exists a STSG? G and deterministic re-
labelings r1 and r2 such that
?(s, t) =
?
s??r?11 (s),t
??r?12 (t)
?G(s
?, t?) .
2To avoid a severe expressivity restriction, several initial
states are allowed for an STSG?.
The inverse of an STSG computable weighted
tree transformation can be computed by an STSG.
Formally, the inverse of the STSG G is the STSG
G?1 = {(t, s, V ?1, a) | (s, t, V, a) ? G}
where V ?1 is the inverse of V . Then ?G?1 = ?
?1
G .
6 Forward and backward application
Let us start this section with the definition of the
concepts of forward and backward application of a
weighted tree transformation ? : T? ? T? ? R?0
to weighted tree languages ? : T? ? R?0 and
? : T? ? R?0. We will give general definitions
first and deal with the potentially infinite sums
later. The forward application of ? to ? is the
weighted tree language ?(?) : T? ? R?0, which
is defined for every t ? T? by
(
?(?)
)
(t) =
?
s?T?
?(s) ? ?(s, t) . (1)
Dually, the backward application of ? to ? is
the weighted tree language ??1(?) : T? ? R?0,
which is defined for every s ? T? by
(
??1(?)
)
(s) =
?
t?T?
?(s, t) ? ?(t) . (2)
In general, the sums in Equations (1) and (2) can
be infinite. Let us recall the important property
that makes them finite in our theorems.
Proposition 5. For every input-productive (resp.,
output-productive) STSG G and every tree s ? T?
(resp., t ? T?), there exist only finitely many
trees t ? T? (respectively, s ? T?) such that
?G(s, t) 6= 0.
Proof sketch. If G is input-productive, then each
derivation step creates at least one input symbol.
Consequently, any derivation for the input tree s
can contain at most as many steps as there are
nodes (or positions) in s. Clearly, there are only
finitely many such derivations, which proves the
statement. Dually, we can obtain the statement for
output-productive STSG.
In the following, we will consider forward ap-
plications ?G(?) where G is an output-productive
STSG and ? is recognizable, which yields that (1)
is well-defined by Proposition 5. Similarly, we
consider backward applications ??1G (?) where G
is input-productive and ? is recognizable, which
again yields that (2) is well-defined by Proposi-
tion 5. The question is whether ?G(?) and ?
?1
G (?)
6
q0 ?
??
q1
1
?
??
q1
q0 ?
???
q2
1
?
???
q2
q1 ?
?
q1
1
?
?
q1
q2 ?
?
q2
1
?
?
q2
q1 ? ?
1
? ?
q2 ? ?
1
? ?
Figure 4: STSG computing the weighted tree transformation ? with initial state q0.
are again recognizable. To avoid confusion, we
occasionally use angled parentheses as in ?p, q?
instead of standard parentheses as in (p, q). More-
over, for ease of presentation, we identify the ini-
tial state qS with ?qS, qS?.
Theorem 6. Let G be an STSG over ?, ?, and Q.
Moreover, let ? : T? ? R?0 and ? : T? ? R?0
be recognizable weighted tree languages.
1. If G is output-productive, then ?G(?) is rec-
ognizable.
2. If G is input-productive, then ??1G (?) is rec-
ognizable.
Proof. For the first item, let K be a WRTG over
? and P such that ? = ?K. Without loss of gen-
erality, we suppose that K is in normal form.
Intuitively, we take each rule q ? (s, t, V, a)
of G and run the WRTG K with every start state p
on the input side s of the rule. In this way, we
obtain a weight b. The WRTG will reach the state
leaves of s in certain states, which we then trans-
fer to the linked states in t to obtain t?. Finally, we
remove the input side and obtain a rule ?p, q?
ab
? t?
for the WRTG L that represents the forward ap-
plication. We note that the same rule of L might
be constructed several times. If this happens, then
we replace the several copies by one rule whose
weight is the sum of the weights of all copies.
As already mentioned the initial state is ?qS, qS?.
Clearly, this approach is inspired (and made rea-
sonable) by the bimorphism characterization. We
can take the HADAMARD product of the WRTG of
the bimorphism with the inverse image of ?K un-
der its input homomorphism. Then we can simply
project to the output side. Our construction per-
forms those three steps at once. The whole process
is illustrated in Figure 5.
Formally, we construct the WRTG L over ? and
P?Qwith the following rules. Let p ? P , q ? Q,
and t? ? T?(P ? Q). Then ?p, q?
c
? t? is a rule
in L?, where
c =
?
(q?(s,t,V,a))?G
V={(u1,v1),...,(uk,vk)}
p1,...,pk?P
t?=t[vi??pi,t(vi)?|1?i?k]
b=?pK(s[ui?pi|1?i?k])
ab .
This might create infinitely many rules in L?, but
clearly only finitely many will have a weight dif-
ferent from 0. Thus, we can obtain the finite rule
set L by removing all rules with weight 0.
The main statement to prove is the following:
for every t ? T?(Q) with lvQ(t) = {v1, . . . , vk},
p, p1, . . . , pk ? P , and q ? Q
?
s?T?(Q)
u1,...,uk?lvQ(s)
?pK(s
?) ? ? qG(s, t, V ) = ?
?p,q?
L (t
?) ,
where
? V = {(u1, v1), . . . , (uk, vk)},
? s? = s[ui ? pi | 1 ? i ? k], and
? t? = t[vi ? ?pi, t(vi)? | 1 ? i ? k].
In particular, for t ? T? we obtain
?
s?T?
?pK(s) ? ?
q
G(s, t, ?) = ?
?p,q?
L (t) ,
which yields
(
?G(?K)
)
(t) =
?
s?T?
?K(s) ? ?G(s, t)
=
?
s?T?
?qSK (s) ? ?
qS
G (s, t, ?)
= ??qS,qS?L (t) = ?L(t) .
In the second item G is input-productive. Then
G?1 is output-productive and ??1G (?) = ?G?1(?).
Hence the first statement proves that ??1G (?) is
recognizable.
Example 7. As an illustration of the construction
in Theorem 6, let us apply the STSG G of Exam-
ple 1 to the WRTG K over ? and P = {p, qS, q?}
and the following rules:
qS
2
5? ?(p, q?) qS
3
5? ?
p
1
? ?(q?, qS) q?
1
? ? .
In fact, K is in normal form and is equivalent to
the WRTG of Example 2. Using the construction
in the proof of Theorem 6 we obtain the WRTG L
over ? and P ?Q with Q = {e, o}. We will only
7
o
?
?
?
o o
? 136?
?
? ?
o o
?q, o?
?
?
?
?q?, o? ?q, o?
? 136 ? 25?
?
? ?
o o
?q, o?
1
36 ?
2
5????
?
? ?
?q, o? ?q?, o?
Figure 5: Illustration of the construction in the proof of Theorem 6 using the WRTG K of Example 7:
some example rule (left), run of K on the input side of the rule (middle), and resulting rule (right).
q1
1
15??
?
q2 q3
q1
1
15??
?
q3 q2
q1
1
5?? ?
q2
1
3?? ? q3
1
5??
?
q1 q2
Figure 6: WRTG constructed in Example 7. We
renamed the states and calculated the weights.
show rules of L that contribute to ?L. To the right
of each rule we indicate from which state ofK and
which rule of G the rule was constructed.
?qS, o?
1
6 ?
2
5?? ?(?q?, o?, ?p, e?) qS, ?2
?qS, o?
1
6 ?
2
5?? ?(?p, e?, ?q?, o?) qS, ?3
?qS, o?
1
3 ?
3
5?? ? qS, ?4
?q?, o?
1
3 ?1?? ? q?, ?4
?p, e?
1
2 ?
2
5?? ?(?qS, o?, ?q?, o?) p, ?6
The initial state ofL is ?qS, o?. It is easy to see that
every t ? T? such that ?L(t) 6= 0 is thin, which
means that |pos(t) ? Nn| ? 2 for every n ? N.
7 Domain and range
Finally, let us consider the domain and range of a
weighted tree transformation ? : T??T? ? R?0.
Again, we first give general definitions and deal
with the infinite sums that might occur in them
later. The domain dom(?) of ? and the range
range(?) of ? are defined by
(
dom(?)
)
(s) =
?
u?T?
?(s, u) (3)
(
range(?)
)
(t) =
?
u?T?
?(u, t) (4)
for every s ? T? and t ? T?. Obviously,
the domain dom(?) is the range range(??1) of
the inverse of ? . Moreover, we can express the
domain dom(?) of ? as the backward applica-
tion ??1(1) where 1 is the weighted tree language
that assigns the weight 1 to each tree. Note that 1
is recognizable for every ranked alphabet.
We note that the sums in Equations (3) and (4)
might be infinite, but for input-productive (re-
spectively, output-productive) STSG G the do-
main dom(?G) (respectively, the range range(?G))
are well-defined by Proposition 5. Using those ob-
servations and Theorem 6 we can obtain the fol-
lowing statement.
Corollary 8. Let G be an STSG. If G is input-
productive, then dom(?G) is recognizable. More-
over, if G is output-productive, then range(?G) is
recognizable.
Proof. These statements follow directly from The-
orem 6 with the help of the observation that
dom(?G) = ?
?1
G (1) and range(?G) = ?G(1).
Conclusion
We showed that every output-productive STSG
preserves recognizability under forward applica-
tion. Dually, every input-productive STSG pre-
serves recognizability under backward applica-
tion. We presented direct and effective construc-
tions for these operations. Special cases of those
constructions can be used to compute the domain
of an input-productive STSG and the range of an
output-productive STSG. Finally, we presented a
characterization of the power of STSG in terms of
weighted bimorphisms.
Acknowledgements
ZOLTA?N FU?LO?P and HEIKO VOGLER were finan-
cially supported by the TA?MOP-4.2.2/08/1/2008-
0008 program of the Hungarian National Devel-
opment Agency. ANDREAS MALETTI was finan-
cially supported by the Ministerio de Educacio?n y
Ciencia (MEC) grant JDCI-2007-760.
8
References
Anne Abeille?, Yves Schabes, and Aravind K. Joshi.
1990. Using lexicalized TAGs for machine trans-
lation. In Proc. 13th CoLing, volume 3, pages 1?6.
University of Helsinki, Finland.
Alfred V. Aho and Jeffrey D. Ullman. 1969. Transla-
tions on a context-free grammar. In Proc. 1st STOC,
pages 93?112. ACM.
Athanasios Alexandrakis and Symeon Bozapalidis.
1987. Weighted grammars and Kleene?s theorem.
Inf. Process. Lett., 24(1):1?4.
Andre? Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d?arbres. Theoret. Comput. Sci.,
20(1):33?93.
Joost Engelfriet, Eric Lilin, and Andreas Maletti.
2009. Extended multi bottom-up tree transducers
? composition and decomposition. Acta Inform.,
46(8):561?590.
Zolta?n Fu?lo?p and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Handbook
of Weighted Automata, chapter 9, pages 313?403.
Springer.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. HLT-NAACL 2004, pages 273?280. ACL.
Ferenc Ge?cseg and Magnus Steinby. 1984. Tree Au-
tomata. Akade?miai Kiado?, Budapest, Hungary.
Ferenc Ge?cseg and Magnus Steinby. 1997. Tree lan-
guages. In Grzegorz Rozenberg and Arto Salomaa,
editors, Handbook of Formal Languages, chapter 1,
pages 1?68. Springer.
Jonathan S. Golan. 1999. Semirings and their Appli-
cations. Kluwer Academic.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, 34(3):391?427.
Udo Hebisch and Hanns J. Weinert. 1998. Semirings
? Algebraic Theory and Applications in Computer
Science. World Scientific.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. 9th IWPT, pages 53?64. ACL.
Kevin Knight and Jonathan Graehl. 2005. An
overview of probabilistic tree transducers for natural
language processing. In Proc. 6th CICLing, volume
3406 of LNCS, pages 1?24. Springer.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003, pages 48?54. ACL.
Philip M. Lewis II and Richard Edwin Stearns. 1968.
Syntax-directed transductions. J. ACM, 15(3):465?
488.
Eric Lilin. 1981. Proprie?te?s de clo?ture d?une extension
de transducteurs d?arbres de?terministes. In Proc.
6th CAAP, volume 112 of LNCS, pages 280?289.
Springer.
Andreas Maletti, Jonathan Graehl, Mark Hopkins,
and Kevin Knight. 2009. The power of ex-
tended top-down tree transducers. SIAM J. Comput.,
39(2):410?430.
Andreas Maletti. 2010. Why synchronous tree substi-
tution grammars? In Proc. HLT-NAACL 2010. ACL.
to appear.
David F. Martin and Steven A. Vere. 1970. On syntax-
directed transduction and tree transducers. In Proc.
2nd STOC, pages 129?135. ACM.
Mark-Jan Nederhof and Giorgio Satta. 2006. Proba-
bilistic parsing strategies. J. ACM, 53(3):406?436.
William C. Rounds. 1970. Mappings and grammars
on trees. Math. Systems Theory, 4(3):257?287.
Yves Schabes. 1990. Mathematical and computa-
tional aspects of lexicalized grammars. Ph.D. thesis,
University of Pennsylvania.
Stuart M. Shieber and Yves Schabes. 1990. Syn-
chronous tree-adjoining grammars. In Proc. 13th
CoLing, pages 253?258. ACL.
Stuart M. Shieber. 2004. Synchronous grammars as
tree transducers. In Proc. TAG+7, pages 88?95. Si-
mon Fraser University.
Stuart M. Shieber. 2006. Unifying synchronous tree
adjoining grammars and tree transducers via bimor-
phisms. In Proc. 11th EACL, pages 377?384. ACL.
James W. Thatcher. 1970. Generalized2 sequential
machine maps. J. Comput. System Sci., 4(4):339?
367.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. 39th
ACL, pages 523?530. ACL.
9
