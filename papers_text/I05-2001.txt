A Classification-based Algorithm for Consistency Check of
Part-of-Speech Tagging for Chinese Corpora
Hu Zhang Jia-heng Zheng
School of Computer
& Information Technology
Shanxi University
Taiyuan, Shanxi 030006, China
Ying Zhao 
Department of Computer Science
University of Minnesota
Minneapolis, MN 55455, USA
yzhao@cs.umn.edu
Abstract
Ensuring consistency of Part-of-Speech
(POS) tagging plays an important role
in constructing high-quality Chinese
corpora. After analyzing the POS tag-
ging of multi-category words in large-
scale corpora, we propose a novel con-
sistency check method of POS tagging
in this paper. Our method builds a
vector model of the context of multi-
category words, and uses the k-NN al-
gorithm to classify context vectors con-
structed from POS tagging sequences
and judge their consistency. The ex-
perimental results indicate that the pro-
posed method is feasible and effective.
1 Introduction
Constructing high-quality and large-scale corpora
has always been a fundamental research area in
the field of Chinese natural language process-
ing. In recent years, the rapid development in
the fields of machine translation (MT), phonetic
recognition (PR), information retrieval (IR), web
text mining, and etc., is demanding more Chinese
corpora of higher quality and larger scale. En-
suring consistency of Part-of-Speech (POS) tag-
ging plays an important role in constructing high-
quality Chinese corpora. In particular, we fo-
cus on consistency check of the POS tagging
of multi-tagging words, which consist of same
Chinese characters and are near-synonymous, but
To whom correspondence should be addressed.
have different grammatical functions. No mat-
ter how many different POS tags a multi-category
words may be tagged, ensuring consistency of
POS tagging means to assign the multi-category
word with the same POS tag when it appears in
similar context.
Novel approaches and techniques have been
proposed for automatic rule-based and statistics-
based POS tagging, and the ?state-of-the-art? ap-
proaches achieve a tagging precision of 89% and
96%, respectively. A great portion of the words
appearing in Chinese corpora are multi-category
words. We have studied the text data from the
2M-word Chinese corpus published by Peking
University, and statistics show that multi-category
words cover 11% of the words, while the percent-
age of the occurrence of multi-category words is
as high as 47%. When checking the POS tags,
human experts may have disagreements or make
mistakes in some cases. After analyzing 1,042
sentences containing the word ?????, which are
extracted from the 2M-word Chinese corpus of
Peking University, the number of incorrect tags
for the word ????? is 15, which accounts for around
1.3%.
So far in the field of POS tagging, most of the
works have focused on novel algorithms or tech-
niques for POS tagging. There are only a lim-
ited number of studies has focused on consistency
check of POS tagging. Xing (Xing, 1999) ana-
lyzed the inconsistency phenomena of word seg-
mentation (WS) and POS tagging. Qu and Chen
(Qu and Chen, 2003) improved the corpus quality
by obtaining POS tagging knowledge from pro-
cessed corpora, preprocessing, and checking con-
1
sistency with methods based on rules and statis-
tics. Qian and Zheng (Qian and Zheng, 2003;
Qian and Zheng, 2004) introduced a rule-based
consistency check method that obtained POS tag-
ging knowledge automatically from processed
corpora by machine learning (ML) and rough set
(RS) methods. For real corpora, Du and Zheng
(Du and Zheng, 2001) proposed a rule-based con-
sistency check method and strategy to identify the
inconsistency phenomena of POS tagging. How-
ever, the algorithms and techniques for automatic
consistency check of POS tagging proposed in
(Qu and Chen, 2003; Qian and Zheng, 2003; Qian
and Zheng, 2004; Du and Zheng, 2001) still have
some insufficiencies. For example, the assign-
ment of POS tags of the inconsistent POS tagging
that are not included in the instance set needs to
be conducted manually.
In this paper, we propose a novel classification-
based method to check the consistency of POS
tagging. Compared to Zhang et al (Zhang et
al., 2004), the proposed method fully considers
the mutual relation of the POS in POS tagging
sequence, and adopts transition probability and
emission probability to describe the mutual de-
pendencies and k-NN algorithm to weigh the sim-
ilarity. We evaluated our proposed algorithm on
our 1.5M-word corpus. In open test, our method
achieved a precision of 85.24% and a recall of
85.84%.
The rest of the paper is organized as follows.
Section 2 introduces the context vector model of
POS tagging sequences. Section 3 describes the
proposed classification-based consistency check
algorithm. Section 4 discusses the experimental
results. Finally, the concluding remarks are given
in Section 5.
2 Describing the Context of
Multi-category Words
The basic idea of our approach is to use the
context information of multi-category words to
judge whether they are tagged consistently or
not. In other words, if a multi-category word ap-
pears in two locations and the surrounding words
in those two locations are tagged similarly, the
multi-category word should be assigned with the
same POS tag in those two locations as well.
Hence, our approach is based on the context of
multi-category words and we model the context
by looking at a window around a multi-category
word and the tagging sequence of this window. In
the rest of this section, we describe our vector rep-
resentation of the context of multi-category words
and how to determine various parameters in our
vector representations.
2.1 Vector Representation of the Context of
Multi-category Words
Our vector representation of context consists of
three key components: the POS tags of each word
in a context window (POS attribute), the impor-
tance of each word to the center multi-category
word based on distance (position attribute), and
the dependency of POS tags of the center multi-
category word and its surrounding words (Depen-
dency Attribute).
Given a multi-category word and its context
window of size l, we represent the words in se-
quential order as (w
1
; w
2
; :::; w
l
) and the POS
tags of each word as (t
1
; t
2
; :::; t
l
). We also re-
fer to the latter vector as POS tagging sequence.
In practise, we choose a proper value of l so
that the context window contains sufficient num-
ber of words and the complexity of our algorithm
remains relatively low. We will discuss this mat-
ter in detail later. In this study, we set the value of
l to be 7.
2.1.1 POS Attribute
The POS tagging sequence contains informa-
tion of the POS of each preceding (following)
word in a POS tagging sequence as well as the
position of each POS tag. The POS of surround-
ing words may have different effect on determin-
ing the POS of the multi-category word, which we
refer to as POS attribute and represent it using a
matrix as follows.
Suppose we have a tag set of size m
(
1
; 
2
; :::; 
m
), given a multi-category word with
a context window of size l (w
1
; w
2
; :::; w
l
) and its
POS tagging sequence, the POS attribute matrix
Y is an l by m matrix, where the rows indicate the
POS tags of the preceding words, multi-category
word, and the following words in the context win-
dow, while the columns present tags in the tag set.
Y
i;j
= 1 iff the POS tag of w
i
is 
j
.
For example, consider the the POS attribute
matrix of ????? in the following sentence:
??? ?/v ?/a ??/n ?/u ?/a ?/n ?/d ?/v ??/n ,/w 
2
As we let l = 7, we look at the word ????? and
its 3 preceding and following words. Hence, the
POS tagging sequence is ( a, n, u, a, n, d, v ). In
our study, we used a standard tag set that consists
of 25 tags. Suppose the tag set is ( n, v, a, d, u, p,
r, m, q, c, w, I, f, s, t, b, z, e, o, l, j, h, k, g, y), then
the POS attribute matrix of ????? in this example
is:
Y
=
0
B
B
B
B
B
B
B
B
B
B

0; 0; 1; 0; 0 : : : : : :
1; 0; 0; 0; 0 : : : : : :
0; 0; 0; 0; 1 : : : : : :
0; 0; 1; 0; 0 : : : : : :
1; 0; 0; 0; 0 : : : : : :
0; 0; 0; 1; 0 : : : : : :
0; 1; 0; 0; 0 : : : : : :
1
C
C
C
C
C
C
C
C
C
C
A
2.1.2 Position Attribute
Due to the different distances from the multi-
category word, the POS of the word before (after)
the multi-category word may in a POS tagging se-
quence have a different influence on the POS tag-
ging of the multi-category word, which we refer
to as position attribute.
Given a multi-category word with a con-
text window of size l, suppose the number of
preceding (following) words is n (i.e., l =
2n + 1), the position attribute vector V
X
of
the multi-category word is given by V
X
=
(d
1
; :::; d
n
; d
n+1
; d
n+2
; :::; d
l
), where d
n+1
is the
value of the position attribute of the multi-
category word and d
n+1 i
(d
n+1+i
) is the value
of the position attribute of the ith preceding
(following) word. We further require that 8i
d
n+1 i
= d
n+1+i
and d
n+1
+
P
n
i=1
(d
n+1 i
+
d
n+1+i
) = 1.
We choose a proper position attribute vector so
that the multi-category word itself has the high-
est weight, and the closer the surrounding word ,
the higher its weight is. If we consider a context
window of size 7, based on our preliminary exper-
iments, we chose the following position attribute
values: d
1
= d
7
= 1=22; d
2
= d
6
= 1=11;
d
3
= d
5
= 2=11; and d
4
= 4=11. Hence, the fi-
nal position attribute vector used in our study can
be written as follows:
V
X
= (
1
22
;
1
11
;
2
11
;
4
11
;
2
11
;
1
11
;
2
22
):
Note that if the POS tag in the POS tagging se-
quence is incorrect, the position attribute value of
the corresponding position should be turned into
a negative value, so that when the incorrect POS
tag appears in a POS tagging sequence, this at-
tribute can correctly show that the incorrect POS
tag has negative effect on generating the final con-
text vector.
2.1.3 Dependency Attribute
The last attribute we focus on is dependency
attribute, which corresponds to the fact that there
are mutual dependencies on the appearance of ev-
ery POS in POS tagging sequences. In particular,
we use transition probability and emission prob-
ability in Hidden Markov Model (HMM) (Leek,
1997) to capture this dependency.
Given a tag set of size m (
1
; 
2
; :::; 
m
), the
transition probability table T is an m by m ma-
trix and given by:
T
i;j
= P
T
(
i
; 
j
) =
f(
i
; 
j
)
f(
i
)
;
where f(
i
; 
j
) is the frequency of the POS tag 
j
appears after the POS tag 
i
in the entire corpus;
f(
i
) is the frequency of the POS tag 
i
appears in
the entire corpus; and P T is the transition proba-
bility.
Given a tag set of size m (
1
; 
2
; :::; 
m
), the
emission probability table E is an m by m matrix
and given by:
E
i;j
= P
E
(
i
; 
j
) =
f(
i
; 
j
)
f(
j
)
;
where f(
i
; 
j
) is the frequency of the POS tag 
i
appears before the POS tag 
j
in the entire corpus;
f(
j
) is the frequency of the POS tag 
j
appears
in the entire corpus; and PE is the emission prob-
ability.
Note that both T and E are constructed from
the entire corpus and we can look up these two ta-
bles easily when we consider the POS tags appear
in POS tagging sequences.
Now, when we look at a context window of size
7 (w
1
; w
2
; :::; w
7
) and its POS tagging sequence
(t
1
; t
2
; :::; t
7
), there are three types of probabili-
ties we need to take into account.
The first one is the probability of the appear-
ance of the POS tag t
4
of the multi-category word,
which we can write as follows:
P
CX
(t
4
) = f(w
4
is tagged as t
4
)=f(w
4
); (1)
3
where f(w
4
) is the frequency of the appearance
of the multi-category word w
4
in the entire corpus
and f(w
4
istaggedast
4
) is the frequency of the
appearance where the word w
4
is tagged as t
4
in
the entire corpus.
The second one is transition probability, which
is the probability of the appearance of the POS tag
t
i+1
in the i + 1 position after the POS tag t
i
in
the i position and shown in Eqn. 2:
P
T
(i;i+1)
= P
T
(t
i
; t
i+1
) = f(t
i
; t
i+1
)=f(t
i
):
(2)
The last last is emission probability, which is
the probability of the appearance of the POS tag
t
i 1
in the i  1 position before the POS tag t
i
in
the i position and shown in Eqn. 3:
P
E
(i 1;i)
= P
E
(t
i 1
; t
i
) = f(t
i 1
; t
i
)=f(t
i
):
(3)
According to the above three probability for-
mulas we can build a seven- dimensional vector,
where each dimension corresponds to one POS
tag, respectively.
Given a multi-category word with a context
window of size 7 and its POS tagging sequence,
the dependency attribute vector V
P
of the multi-
category word is defined as follows:
V
P
= (P
1
; P
2
; P
3
; P
4
; P
5
; P
6
; P
7
);
where
P
1
= P
T
(1;2)
 P
2
= P
T
(1;2)
 P
T
2;3
 P
T
(3;4)
 P
CX
(t
4
);
P
2
= P
T
(2;3)
 P
3
= P
T
(2;3)
 P
T
(3;4)
 P
CX
(t
4
);
P
3
= P
T
(3;4)
 P
4
= P
T
(3;4)
 P
CX
(t
4
);
P
4
= P
CX
(t
4
);
P
5
= P
T
(4;5)
 P
4
= P
E
(4;5)
 P
E
(4;5)
 P
CX
(t
4
);
P
6
= P
T
(5;6)
 P
5
= P
E
(5;6)
 P
E
(4;5)
 P
CX
(t
4
);
P
7
= P
T
(6;7)
 P
6
= P
E
(6;7)
 P
E
(5;6)
 P
(4;5)
 P
CX
(t
4
):
2.1.4 Context Vector of Multi-category
Words
Now we are ready to define the context vector
of multi-category words.
Given a multi-category word with a context
window of size l and its POS attribute matrix Y ,
position attribute vector V
X
, and dependency at-
tribute vector V
P
, the context vector V
S
of the
multi-category word is defined as follows:
V
S
= (V
X
+ V
P
) Y; (4)
where  and  are the weights of the position at-
tribute and the dependency attribute, respectively.
Note that we require +  = 1, and their opti-
mal values are determined by experiments in our
study.
2.2 Experiment on the Size of the Context
Window
Context vectors can be extended by using 4 to 7
preceding (following) words to substitute 3 pre-
ceding (following) words in context windows and
POS tagging sequences. We conducted experi-
ments with a context window of size 3 to 7 on our
sampled 1M-word training corpus and performed
closed test. The experimental results are evalu-
ated in terms of both the precision of consistency
check and algorithm complexity simultaneously.
We plot the effect on precision in Figure 1.
 0.87
 0.872
 0.874
 0.876
 0.878
 0.88
 0.882
 7 6 5 4 3
Pr
ec
is
io
n
Number of the preceding (following) words
Figure 1: Effect on precision of the number of
preceding (following) words.
As shown in Figure 1, the precision of consis-
tency check increases as we include more preced-
ing (following) words. In particular, the precision
is improved by 1% when we use 7 preceding (fol-
lowing) words. However, the increase of com-
plexity is much higher than that of precision, be-
cause the dimensionality of the position attribute
vector, POS attribute vector, and dependency at-
tribute vector doubles. Hence, we chose 3 as the
number of preceding (following) words to form
context windows and calculate context vectors.
4
2.3 Effect on consistency check precision of
 and 
When using our sampled 1M-word training cor-
pus to conduct closed test, we found that consis-
tency check precision changes significantly with
the different values of  and . Figure 2 shows
the trend when  varies from 0.1 to 0.9. We used
 = 0:4 and  = 0:6 in our experiments.
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
Pr
ec
is
io
n
?
Figure 2: Effect on consistency check precision
of  and .
3 Consistency Check of POS Tagging
Our consistency check algorithm is based on clas-
sification of context vectors of multi-category
words. In particular, we first classify context
vectors of each multi-category word in the train-
ing corpus, and then we conduct the consistency
check of POS tagging based on classification re-
sults.
3.1 Similarity between Context Cectors of
Multi-category Words
After constructing context vectors for all multi-
category words from their context windows and
POS tagging sequences, the similarity of two con-
text vectors is defined as the Euclidean Distance
between the two vectors.
d(x; y) = kx yk =
"
n
X
i=1
(x
i
  y
i
)
2
#
(1=2)
; (5)
where x and y are two arbitrary context vectors of
n dimensions.
3.2 k-NN Classification Algorithm
Classification is a process to assign objects that
need to be classified to a certain class. In this pa-
per, we used a popular classification method: the
k-NN algorithm.
Suppose we have  classes and a class
(!
i
(i = 1; 2; :::; )) has N
i
samples (x(i)
j
(j =
1; 2; :::; N
i
)). The idea of the k-NN algorithm
is that for each unlabeled object x, compute the
distances between x and all samples whose class
is known, and select k samples (k nearest neigh-
bors) with the smallest distance. This object x
will be assigned to the class that contains the most
samples in the k nearest neighbors.
We now formally define the discriminant func-
tion and discriminant rule. Suppose k
1
; k
2
; :::; k

are the numbers of samples in the k nearest neigh-
bors of the object x that belong to the classes
!
1
; !
2
; :::; !

, respectively. Define the discrimi-
nant function of the class !
i
as d
i
(x) = k
i
; i =
1; 2; :::; : Then, the discriminant rule of deter-
mining the class of the object x can be defined
as follows:
d
m
x = max
i=1;2;:::;
d
i
(x) ) x 2 !
m
3.3 Consistency Check Algorithm
In this section, we describe the steps of our
classification-based consistency check algorithm
in detail.
Step1: Randomly sampling sentences containing multi-
category words and checking their POS tagging manually.
For each multi-category word, classifying the context vec-
tors of the sampled POS tagging sequences, so that the con-
text vectors that have the same POS for the multi-category
word belong to the same class.
Step2: Given a context vector x of a multi-category word
, calculating the distances between x and all the context
vectors that contains the multi-category word  in the train-
ing corpus, and selecting k context vectors with smallest dis-
tances.
Step3: According to the k-NN algorithm, checking the
classes of the k nearest context vectors and classifying the
vector x.
Step4: Comparing the POS of the multi-category word 
in the class that the k-NN algorithm assigns x to and the POS
tag of . If they are the same, the POS tagging of the multi-
category word  is considered to be consistent, otherwise it
is inconsistent.
The major disadvantage of this algorithm is the
difficulty in selecting the value of k. If k is too
small, the classification result is unstable. On the
other hand, if k is too big, the classification devi-
ation increases.
3.4 Selecting k in Classification Algorithm
Figure 3 shows the consistency check precision
values obtained with various k values in the k-
NN algorithm. The precision values are closed
5
Table 1: Experimental Results
Number of Number of Number of
Test Test multi-category the true the identified Recall Precision
corpora type words inconsistencies inconsistencies (%) (%)
1M-word closed 127,210 1,147 1,219 (156) 92.67 87.20
500K-word open 64,467 579 583 (86) 85.84 85.24
test results on our 1M-word training corpus, and
were obtained by using  = 0:4 and  = 0:6 in
the context vector model.
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 10 9 8 7 6 5 4 3 2 1
Av
er
ag
e 
pr
ec
isi
on
Number of nearest neighbors (k)
Figure 3: Effect on precision of k in the k-NN
algorithm.
As shown in Figure 3, when k continues to in-
crease from 6, the precision remains the same.
When when k reaches to 9, the precision starts
declining. Our experiment with other  and 
values also show similar trends. Hence, we chose
k = 6 in this paper.
4 Experimental Results
We evaluated our consistency check algorithm on
our 1.5M-word corpus (including 1M-word train-
ing corpus) and conducted open and closed tests.
The results are showed in Table 1.
The experimental results show two interest-
ing trends. First, the precision and recall of
our consistency check algorithm are 87.20% and
92.67% in closed test, respectively, and 85.24%
and 85.84% in open test, respectively. Compared
to Zhang et al (Zhang et al, 2004), the precision
of consistency check is improved by 23%, and
the recall is improved by 10%. The experimental
results indicate that the context vector model has
great improvements over the one used in Zhang
et al (Zhang et al, 2004). Second, thanks to the
great improvement of the recall, to some extent,
our consistency check algorithm prevents the hap-
pening of events with small probabilities in POS
tagging.
5 Conclusion and Future Research
In this paper, we propose a new classification-
based method to check consistency of POS tag-
ging, and evaluated our method on our 1.5M-
word corpus (including 1M-word training corpus)
with both open and closed tests.
In the future, we plan to investigate more types
of word attributes and incorporate linguistic and
mathematical knowledge to develop better con-
sistency check models, which ultimately provide
a better means of building high-quality Chinese
corpora.
Acknowledgements
This research was partially supported by the Na-
tional Natural Science Foundation of China No.
60473139 and the Natural Science Foundation of
Shanxi Province No. 20051034.
References
Y. Du and J. Zheng. 2001. The proofreading method study
on consistence of segment and part-of-speech. Computer
Development and Application, 14(10):16?18.
T. R. Leek. 1997. Information extraction using hidden
Markov models. Master?s thesis, UC San Diego.
Y. Qian and J. Zheng. 2003. Research on the method of
automatic correction of chinese pos tagging. Journal of
Chinese Information Processing, 18(2):30?35.
Y. Qian and J. Zheng. 2004. An approach to improving the
quality of part-of-speech tagging of chinese text. In Pro-
ceedings of the 2004 IEEE International Conference on
Information Technology: Coding and Computing (ITCC
2004).
W. Qu and X. Chen. 2003. Analysing on the words classi-
fied hard in pos tagging. In Proceedings of the 9th Na-
tional Computational Linguistics (JSCL?03).
H. Xing. 1999. Analysing on the words classified hard in
pos tagging. In Proceedings of the 5th National Compu-
tational Linguistics (JSCL?99).
H. Zhang, J. Zheng, and J. Liu. 2004. The inspecting
method study on consistency of pos tagging of corpus.
Journal of Chinese Information Processing, 18(5):11?16.
6
