Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 130?141,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
An Empirical Evaluation of a Statistical Dialog System in Public Use
Jason D. Williams
AT&T Labs - Research, Shannon Laboratory, 180 Park Ave., Florham Park, NJ 07932, USA
jdw@research.att.com
Abstract
This paper provides a first assessment of a sta-
tistical dialog system in public use. In our di-
alog system there are four main recognition
tasks, or slots ? bus route names, bus-stop lo-
cations, dates, and times. Whereas a conven-
tional system tracks a single value for each slot
? i.e., the speech recognizer?s top hypothesis
? our statistical system tracks a distribution
of many possible values over each slot. Past
work in lab studies has showed that this distri-
bution improves robustness to speech recog-
nition errors; but to our surprise, we found
the distribution yielded an increase in accu-
racy for only two of the four slots, and actu-
ally decreased accuracy in the other two. In
this paper, we identify root causes for these
differences in performance, including intrin-
sic properties of N-best lists, parameter set-
tings, and the quality of statistical models. We
synthesize our findings into a set of guidelines
which aim to assist researchers and practition-
ers employing statistical techniques in future
dialog systems.
1 Introduction
Over the past decade, researchers have worked to ap-
ply statistical techniques to spoken dialog systems,
and in controlled laboratory studies, statistical di-
alog systems have been shown to improve robust-
ness to errors compared to conventional approaches
(Henderson and Lemon, 2008; Young et al, 2010;
Thomson and Young, 2010). However, statistical
techniques have not yet been evaluated in a publicly
deployed system, and real users behave very differ-
ently to usability subjects (Raux et al, 2005; Ai et
al., 2008). So there is an important open question
whether statistical dialog systems improve perfor-
mance with real users.
This paper provides a first evaluation of a publi-
cally deployed statistical dialog system, AT&T Let?s
Go (Williams et al, 2010). AT&T Let?s Go pro-
vides bus times for Pittsburgh, and received approx-
imately 750 calls from real bus riders during the
2010 Spoken Dialog Challenge (Black et al, 2010).
AT&T Let?s Go is based on a publicly available
toolkit (Williams, 2010a) and achieved the highest
rates of successful task completion on real callers in
the challenge, so it provides a relevant exercise from
which to draw inferences.
AT&T Let?s Go collected four types of informa-
tion, or slots: bus route names, bus-stop names,
dates, and times. For each slot, we measured turn-
level accuracy of the deployed statistical system and
compared it to accuracy without application of the
statistical techniques (i.e., the top speech recogni-
tion result).
To our surprise, we found that statistical tech-
niques appeared to improve accuracy for only two of
the four slots, and decreased accuracy for the other
two. To investigate this, we considered four mech-
anisms by which statistical methods can differ from
the top speech recognition result. Analyzing the ef-
fects of each mechanism on each slot enables un-
derlying causes to be identified: for example, one
mechanism performed exceptionally well when its
statistical models was well matched to usage data,
but rather poorly when its model diverged from real
usage. We believe this analysis ? the focus of this
paper ? is relevant to researchers as well as practi-
130
tioners applying statistical techniques to production
systems.
In this paper, Section 2 reviews the operation of
statistical spoken dialog systems. Section 3 then
describes the AT&T Let?s Go dialog system. Sec-
tion 4 reports on overall accuracy, then analyzes the
underlying reasons for accuracy gains and losses.
Section 5 tackles how well error in the belief state
can be identified compared to speech recognition er-
rors. Section 6 concludes by summarizing lessons
learned.
2 Statistical dialog systems
Statistical dialog systems maintain a distribution
over a set of hidden dialog states. A dialog state
includes information not directly observable to the
dialog system, such as the user?s overall goal in the
dialog or the user?s true action (e.g., the user?s true
dialog act). For each dialog state s, a posterior prob-
ability of correctness called a belief is maintained
b(s). The set of hidden dialog states and their be-
liefs is collectively called the belief state, and up-
dating the belief state is called belief tracking. Here
we will present belief tracking at a level sufficient
for our purposes; for a more general treatment, see
(Williams and Young, 2007).
At the start of the dialog, the belief state is initial-
ized to a prior distribution b0(s). The system then
takes an action a, and the user takes an action in
response. The automatic speech recognizer (ASR)
then produces a ranked list of N hypotheses for the
user?s action, u = (u1, . . . , uN ), called an N-best
list. For each N-best list the ASR also produces a
distribution Pasr(u) which assigns a local, context-
independent probability of correctness to each item,
often called a confidence score. The belief state is
then updated:
b?(s) = k ?
?
u
Pasr(u)Pact(u|s, a)b(s) (1)
where Pact(u|s, a) is the probability of the user tak-
ing action u given the dialog is in hidden state s and
the system takes action a. k is a normalizing con-
stant.
In practice specialized techniques must be used to
compute Eq 1 in real-time. The system in this paper
uses incremental partition recombination (Williams,
2010b); alternatives include the Hidden Information
State (Young et al, 2010), Bayesian Update of Dia-
log States (Thomson and Young, 2010), and particle
filters (Williams, 2007). The details are not impor-
tant for this paper ? the key idea is that Eq 1 synthe-
sizes a prior distribution over dialog states together
with all of the ASR N-best lists and local confidence
scores to form a cumulative, whole-dialog poste-
rior probability distribution over all possible dialog
states, b(s).
In the system studied in this paper, slots are
queried separately, and an independent belief state is
maintained for each. Consequently, within each slot
user actions u and hidden states s are drawn from
the same set of slot values. Thus the top ASR result
u1 represents the ASR?s best hypothesis for the slot
value in the current utterance, whereas the top dia-
log state argmaxs b(s) = s? represents the belief
state?s best hypothesis for the slot value given all of
the ASR results so far, a prior over the slot values,
and models of user action likelihoods. The promise
of statistical dialog systems is that s? will (we hope!)
be correct more often than u1. In the next section,
we measure this in real dialogs.
3 AT&T Let?s Go
AT&T Let?s Go is a statistical dialog system that
provides bus timetable information for Pittsburgh,
USA. This system was created to demonstrate a
production-grade system built following practices
common in industry, but which incorporates two sta-
tistical techniques: belief tracking with the AT&T
Statistical Dialog Toolkit (Williams, 2010a), and
regression-based ASR confidence scores (Williams
and Balakrishnan, 2009).
As with most commercial dialog systems, AT&T
Let?s Go follows a highly directed flow, collecting
one slot at a time. There are four types of slots:
ROUTE, LOCATION, DATE, and TIME. The sys-
tem can only recognize values for the slot being
queried, plus a handful of global commands (?re-
peat?, ?go back?, ?start over?, ?goodbye?, etc.) ?
mixed initiative and over-completion were not sup-
ported. As mentioned above, an independent belief
state is maintained for each slot: this was an inten-
tional design decision made in order to use statistical
techniques within current commercial practices.
131
The system opens by asking the user to say a bus
ROUTE, or to say ?I?m not sure.? The system next
asks for the origin and destination LOCATIONs. The
system then asks if the caller wants times for the
?next few buses?; if not, the system asks for the
DATE then TIME in two separate questions. Finally
bus times are read out.
After requesting the value of a slot, the system re-
ceives an N-best list, assigns each item a confidence
score Pasr(u), and updates the belief in (only) that
slot using Eq 1. The top dialog hypothesis s? and
its belief b(s?) are used to determine which action
to take next, following a hand-crafted policy. This is
in contrast to a conventional dialog system, in which
the top ASR result and its confidence govern dialog
flow. Figure 6 shows the design of AT&T Let?s Go.
In the period July 16 ? August 16 2010, AT&T
Let?s Go received 742 calls, of which 670 had one
or more user utterances. These calls contained a
total of 8269 user utterances, of which 4085 were
in response to requests for one of the four slots.
(The remainder were responses to yes/no questions,
timetable navigation commands like ?next bus?,
etc.)
Our goal in this paper is to determine whether
tracking a distribution over multiple dialog states
improved turn-level accuracy compared to the top
ASR result. To measure this, we compare the accu-
racy of the top belief state and the top ASR result. A
transcriber listened to each utterance and marked the
top ASR hypothesis as correct if it was an exact lex-
ical or semantic match, or incorrect otherwise. The
same was then done for the top dialog hypothesis in
each turn.
Accuracy of the top ASR hypothesis and the top
belief state are shown in Table 1, which indicates
that belief monitoring improved accuracy for ROUTE
and DATE, but degraded accuracy for LOCATION and
TIME. We had hoped that belief tracking would im-
prove accuracy for all slots; seeing that it hadn?t
prompted us to investigate the underlying causes.
4 Belief tracking analysis
When an ASR result is provided to Eq 1 and a new
belief state is computed, the top dialog state hypoth-
esis s? may differ from top ASR result u1. For-
mally, these differences are simply the result of eval-
Slot ROUTE LOCATION DATE TIME
Utts 1520 2235 173 157
ASR 769 1326 124 80
correct 50.6% 59.3% 71.7% 51.0 %
Belief 799 1246 139 63
correct 52.6% 55.7% 80.3% 40.1%
Belief +30 -80 +15 -17
? ASR +2.0% -3.6% +8.7% -10.8%
Table 1: Accuracy of the top ASR result and top be-
lief state. LOCATION includes both origin and des-
tination utterances. Most callers requested the next
bus so few were asked for DATE and TIME.
uating this equation. However, intuitively there are
four mechanisms which cause differences, and each
difference can be explained by the action of one or
more mechanisms. These mechanisms are summa-
rized here; the appendix provides graphical illustra-
tions.1
? ASR re-ranking: When computing a con-
fidence score Pasr(u), it is possible that the
entry with the highest confidence u? =
argmaxu Pasr(u) will not be the first ASR re-
sult, u1 6= u?. In other words, if the confidence
score re-ranks the N-best list, this may cause s?
to differ from u1 (Figure 7).
? Prior re-ranking: Statistical techniques use a
prior probability for each possible dialog state
? in our system, each slot value ? b0(s). If an
item recognized lower-down on the N-best list
has a high prior, it can obtain the most belief,
causing s? to differ from u1 (Figure 8).
? Confidence aggregation: If the top belief
state s? has high belief, then subsequent low-
confidence recognitions which do not contain
s? will not dislodge s? from the top position,
causing s? to differ from u1 (Figure 9).
? N-best synthesis: If an item appears in two N-
best lists, but is not in the top ASR N-best posi-
tion in the latter recognition, it may still obtain
the highest belief, causing s? to differ from u1
(Figure 10).
1This taxonomy was developed for belief tracking over a
single slot. For systems which track joint beliefs over multiple
slots, additional mechanisms could be identified.
132
 

 

 

		

		
		



	


fffi
ffi


 

 

 

 

		

		
		



	


fffi
ffi


 



!
 
!

" 
"

		

		
		



	


fffi
ffi


 

#
 
#

 

 

		

		
		



	


fffi
ffi


 

LOCATION
DATE
ROUTE
TIME
Ac
cu
ra
cy
Ac
cu
ra
cy
Figure 1: Differences in accuracy between ASR and belief monitoring. ?Baseline? indicates accuracy among
utterances where belief monitoring had no effect ? where ASR and belief monitoring are both correct, or
both incorrect. Blue bars show cases where the top belief state s? is correct and the top ASR result u1 is
not; red bars show cases where u1 is correct and s? is not. The plot is arranged to show a running total
where blue bars increase the total and red bars decrease the total. Percentages under blue and red bars show
the change in accuracy due to each mechanism. The black bar on the right shows the resulting accuracy in
deployment.
We selected utterances where the correctness of the
top ASR result and top dialog hypothesis differed ?
where one was correct and the other was not ? and
labeled these by hand to indicate which of the four
mechanisms was responsible for the difference. In
a few cases multiple mechanisms were responsible;
these were labeled with the first contributing mech-
anism in the order listed above.
Figures 1 shows results. Of the four mechanisms,
prior re-ranking occurred most often, and confidence
aggregation occurred least often. Interestingly, some
mechanisms provided a performance gain for certain
slots and a degradation for others. This led us to look
at each mechanism in detail.
4.1 Evaluation of ASR Re-ranking
The recognizer used by AT&T Let?s Go produced an
N-best list ordered by decoder cost. After decoding,
a confidence score was assigned to each item on the
N-best list using a regression model that operated on
features of the recognition (Williams and Balakrish-
nan, 2009). The purpose of this regression was to
assign a probability of correctness to each item on
the N-best list; while it was not designed to re-rank
the N-best list, the design of this model did allow it
to assign a higher score to the n = 2 hypothesis than
the n = 1 hypothesis. When this happens, we say
the N-best list was re-ranked. Table 2 shows how
often ASR re-ranking occurred, and how often the
133
fl$fl
fl$%
fl$&
fl$'
fl$(
fl$)
fl$*
fl$+
fl$,
fl$-
%$fl
fl$fl fl$% fl$& fl$' fl$( fl$) fl$* fl$+ fl$, fl$- %$fl
.
/
0
/
1
2
3
4
5
6
7
4
8
3
9
4
:
/
3
4
;
<
=>?@A BC DBEE>D@ F@>G BH IJK>L@ MFL@N OL O CEOD@FBH BC MFL@ M>HP@A
QRSTU VWXTYZTS
Figure 2: Cumulative distribution of the position
of the correct item on N-Best lists for the ROUTE
when the correct item is in position 2 . . . N . Depth
is shown as a fraction of the N-Best list length.
ASR re-ranking helped and hurt ASR accuracy. We
found that re-ranking degraded ASR accuracy for all
slots, except DATE where it had a trivial positive im-
pact. This suggested a problem with our confidence
score; examining ROUTE, LOCATION, and TIME we
found that the distributions used by the confidence
score that apportions mass to items 2 . . . N were far
more concentrated on the N=2 entry than observed
in deployment (Figure 2). Investigation revealed a
bug in the model estimation code for these slots.
Where ASR re-ranking decreased ASR accuracy,
we?d expect to see it also decrease belief state ac-
curacy. Indeed, for the TIME slot, ASR re-ranking
causes a substantial decrease in belief state accu-
racy, highlighting the importance of an accurate con-
fidence score to statistical techniques. However, for
the ROUTE slot, we see an increase in belief state ac-
curacy attributed to ASR re-ranking. This can be ex-
plained by interaction between ASR re-ranking and
prior re-ranking, discussed next.
4.2 Evaluation of prior re-ranking
Whereas N-best re-ranking affects b?(s) via Pasr,
prior re-ranking affects b?(s) via the prior proba-
bility in a slot b0(s) ? i.e., the initial belief, at the
start of the dialog, for each value the slot may take.
If the slot?s prior is uniform (non-informative), we
expect to see no effect on accuracy due to the prior
? indeed, Figure 1 shows that priors had no effect
on belief accuracy for DATE and TIME, which used
uniform priors.
ROUTE and LOCATION employed a non-uniform
prior, and here we?d expect to see a gain in perfor-
mance if the prior matches actual use. Both priors
were computed using a simple heuristic in which the
prior was proportional to the number of distinct bus-
stops on the route or covered by the location expres-
sion, smoothed with a smoothing factor. For exam-
ple, the phrase ?downtown? covered 17 stops and its
prior was 0.018; the phrase ?airport? covered 1 stop
and its prior was 0.00079. Even though historical us-
age data was available to Spoken Dialog Challenge
2010 participants (Parent and Eskenazi, 2010), we
instead chose to base priors on bus-stop counts as a
test of whether effective priors could be constructed
without access to usage data.
Overall the prior for ROUTE fit actual usage data
well (Figure 3), and we see a corresponding net gain
in belief accuracy of 3.7% = 4.0% ? 0.3% in Fig-
ure 1. However the prior for LOCATION was a poor
match with actual usage (Figure 4), and this caused
a net degradation in belief accuracy of ?0.9% =
0.5% ? 1.4%. The key problem is that the heuris-
tic wrongly assumed all stops are equally popular:
for example, although the airport contained a sin-
gle stop (and thus received a very low prior), it was
very popular. This suggests that it would be better
to estimate priors based on usage data rather than
the bus-stop count heuristic. More broadly, it also
underscores the importance of accurate priors to sta-
tistical dialog techniques.
In the previous section, for ROUTE, it was ob-
served that ASR re-ranking degraded ASR accuracy,
yet caused an improvement in belief accuracy. The
effects of the prior explain this: the prior was often
stronger, such that an error introduced by ASR re-
ranking was cancelled by prior re-ranking. Exam-
ining cases where ASR re-ranking occurred but the
belief state was still correct confirmed this. Where
ASR re-ranking and prior re-ranking agreed, the
ASR re-ranking received credit. Looking at LOCA-
TION, the prior was essentially noise, so ASR re-
ranking errors could not be systematically canceled
by prior re-ranking in the same way ? indeed, LO-
CATION belief accuracy was degraded by both ASR
re-ranking and prior re-ranking. More broadly, this
provides a nice illustration of how statistical tech-
134
Slot ROUTE LOCATION DATE TIME
All utterances 1520 2235 173 157
Utterances with 505 305 3 40
ASR re-ranking 33.2% 13.6% 1.7% 25.5%
ASR re-ranked; N=2 correct 36 11 1 3
(ASR re-ranking helped) +2.4% +0.5 % +0.6 % +1.9 %
ASR re-ranked; N=1 correct 63 33 0 9
(ASR re-ranking hurt) -4.1% -1.5 % 0 % -5.7 %
Net gain from -27 -22 +1 -6
ASR re-ranking -1.8 % -1.0% +0.6% -3.8%
Table 2: ASR re-ranking.
[\[[]
[\[[^
[\[[_
[\[[`
[\[]
a
[\[
b
^
[\[
a
_
[\]^`
[\^ca
[\c]^
d
e
f
g
h
g
i
j
i
k
l
f
e
m
e
n
o
p
n
q
r
l
s
j
f
t
u
r
h
j
n
v
wxy z{x|} ~}}z? ??? ?|}? y?{???
???????? ?????
Figure 3: Modeled prior for ROUTE vs. observed
usage. The modeled prior was a relatively good pre-
dictor of actual usage.
niques can combine conflicting evidence ? in this
case, from the prior and ASR.
4.3 Evaluation of confidence score aggregation
The conditions for confidence score aggregation oc-
cur somewhat rarely: for no slot did it have the great-
est effect on belief accuracy. It had the largest effect
on DATE; investigation revealed that belief scores for
DATE were relatively lower than for other slots (Ta-
ble 3). Since all slots used the same thresholds to
make accept/reject decisions, DATE had proportion-
ally more retries in which the top belief hypothesis
was correct, yielding more opportunities for confi-
dence aggregation to have an effect.
But why were belief values for DATE lower than
for other slots? Investigation revealed that a bug
??????
?????
?
????
??
????
??
????
?
?
???
???
??????
??????
??????
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
???? ??????????? ?????? ???? ???? ??????
???????? ?????
Figure 4: Modeled prior for LOCATION vs. ob-
served usage. The modeled prior was essentially
noise compared to actual usage.
Slot ROUTE LOCATION DATE TIME
Correct 0.90 0.89 0.60 0.73
Incorrect 0.52 0.59 0.34 0.53
Table 3: Average belief in the top dialog state hy-
pothesis when that hypothesis was correct or incor-
rect.
was causing priors for DATE to be nearly an or-
der of magnitude too small, so that each recognized
date was artificially improbable. As a result, DATE
effectively had a more stringent threshold for ac-
cept/reject decisions. Although caused by a bug, this
case study provides a more general illustration: ob-
taining sufficient belief to meet higher thresholds re-
quires more ASR evidence in the form of more re-
135
Slot ROUTE LOCATION DATE TIME
Average N-best list length 5.0 2.8 2.1 4.3
N-best accuracy 27.9% 10.6% 46.0% 34.7%
Average position of correct item (n > 1) 3.3 3.2 2.6 2.9
Table 4: Descriptive statistics for N-best lists. Average N-best list length indicates the average length of all
N-best lists, regardless of accuracy. N-best accuracy indicates how often the correct item appeared in any
position n > 1 among cases where the top ASR result n = 1 was not correct. Average position of correct
item refers to the average n among cases where the correct item appeared with n > 1.
tries.
4.4 Evaluation of N-best synthesis
For DATE, N-best synthesis had a large positive ef-
fect, TIME and LOCATION a small positive effect
(or no effect), and ROUTE a small negative effect.
N-best synthesis occurs when commonality exists
across N-best lists, so we next examined the N-best
lists for each slot.
Table 4 shows three key properties of the N-best
lists. ROUTE and DATE had the most extreme values:
ROUTE had the longest N-best lists, comparatively
poor N-best accuracy, and the correct item appeared
furthest down the N-best list. By contrast, DATE had
the shortest N-best lists, the best N-best accuracy,
and the correct item appeared closest to the top. LO-
CATION and TIME were between the two. This rela-
tive ordering aligns with the observed effect that N-
best synthesis had on belief accuracy, where DATE
enjoyed a large improvement and ROUTE suffered a
small degradation.
This correlation suggests that basic properties of
the N-best list govern the effectiveness of N-best
synthesis: when N-best lists are shorter, more of-
ten contain the correct answer, and when the correct
answer is closer to the top position, N-best synthesis
can lead to large gains. When N-best lists are longer,
less often contain the correct answer, and when the
correct answer is farther from the top position, N-
best synthesis can lead to small gains or even degra-
dations.
5 Identifying belief state errors
The analysis in the preceding section assessed the
accuracy of the belief state. In practice, a system
must decide whether to accept or reject a hypoth-
esis, so it is also important to evaluate the ability
of the belief state to discriminate between correct
and incorrect hypotheses. We studied this by plot-
ting receiver operating characteristic (ROC) curves
for each slot, in Figure 5.
Where the belief state has higher accuracy
(ROUTE, DATE), the belief state shows somewhat
better ROC results, especially at higher false-accept
rates. However, gains in ROC performance appear
to be due entirely to gains in accuracy: In LOCA-
TION, belief tracking made nearly no difference to
accuracy, and the belief state shows virtually no dif-
ference to ASR in ROC performance. TIME suf-
fered degradations in both accuracy and ROC perfor-
mance. The trend appears to be that if belief tracking
does not improve over ASR 1-best, then it seems that
belief tracking does not enable better accept/reject
decision to made. Perhaps addressing the model de-
ficiencies mentioned above will improve discrimina-
tion ? this is left to future work.
6 Conclusions
This paper has provided a first assessment of sta-
tistical techniques in a spoken dialog system under
real use. We have found that belief tracking is not
guaranteed to improve accuracy ? its effects vary de-
pending on the operating conditions:
? Overall the effects of prior re-ranking and N-
best synthesis are largest; confidence aggrega-
tion has the smallest effect.
? When N-best lists are useful, N-best synthesis
can have a large positive effect (DATE); when
N-best lists are more noisy, N-best synthesis
has a small or even negative effect (ROUTE).
? In the presence of more rejection, confidence
aggregation can have a positive effect (DATE),
136
Location
Date
Route
Time
Figure 5: ROC curves. Red curves show the top-scored ASR hypothesis u? with accept/reject decisions
made using the confidence score Pasr(u); blue curves show the top belief state s? with accept/reject decisions
made using its belief b(s?).
but otherwise plays a small role.
? When there exists an informative prior and it is
estimated correctly, prior re-ranking produces
an accuracy gain (ROUTE); when estimated
poorly, it degrades accuracy (LOCATION).
? The belief state, at least when using our current
models, improves accept/reject decisions only
when belief tracking produces a gain in accu-
racy over ASR. Absent an accuracy increase,
the belief state is no more informative than a
good confidence score for making accept/reject
decisions.
We believe these findings validate that statistical
techniques ? properly employed ? have the capabil-
ity to improve ASR robustness under real use. This
paper has focused on descriptive results; in future
work, we plan to test whether correcting the model
deficiencies and re-running belief tracking does in-
deed improve performance. For now, we hope that
this work serves as a guide to practitioners building
statistical dialog systems, providing some instruc-
tion on the importance of accurate model building,
and examples of the effects of different design deci-
sions.
Acknowledgments
Thanks to Barbara Hollister and the AT&T labeling
lab for their excellent work on this project.
137
?? ??? ???? ????? ??? ???
????
???
?
????
?
?
?
????? ??
??
?
??
?
????? ?? ? ?
?
?
???? ?????
?
?? ??
?
?
????? ?? ????????
?
?????????????
?
??
?
?
?
?
????
?
?? ?? ??
?
?????
??
????? ???
??? ???? ???
?
?? ???
????
?
??? ???? ??? ???? ???
????
?
??
?
??
?
??? *???? ??? ????
?
???? ???? ??? ?
?
? ????
??? ???????????? ??
???
?
?????
?  
??

???
?
?
??? ?

?? ????
?
 
? ?????
? 
???
 
? ???
?
???
?
???? ?? ?

????
??? ????
.
???????
?
??
?
?
.
???
	
???
? 
??????
?
????? ?
?
?
??
? 
?

?? ??????
?
?

??? ?????
?
?? ??????

???? ?
?
??? ???
?
0 ???? ??? ???
?????? ?????
 ? 

*??? ???? ???  ???
??????????
??? ? ?? ??? ??
?????? ?????
????? ?????? ???
??? ? ???? ???
???	 ? ?? ????
?????????????
0 ???? ??? ???
????? ??
?

?
 

*??? ???? ???  ???
?
????
?
????
??
? ? ?? ???
 
??
????? ??
?

????
? +
?

????
?
?
??
?
??
 
? ???? ???
?????? ???? ??? ????
?
??????? ??
+
?

????
?
??? ? ??? ?????? ?? ???  ?
??? ?????
??????? ??? ???? ???
??? ??? ? ????
???????? ??? ???? ??????
3 ?
	
???
?
???
? 	
???????

?
 

?????
  ??????	 ??? ??
?????????????????????
  ff
????? ?????
fiflffiffi
????? ?????
 
ff
ffi!
ff
ffi
?
?

??? ?????
 !
ff

?
?

??? ?????
"ffi#
?
?

??? ?????
  $% ??????	 ??? ??
????????? ????? ??????
Figure 6: Flowchart of AT&T Let?s Go. The system asks for the bus route, then the origin bus stop, then
the destination bus stop. If the user does not want the next few buses, the system also asks for the date and
time. Prompts shown are paraphrases; actual system prompts include example responses and are tailored to
dialog context. Different language models are used for each slot, and separate belief states are maintained
over each of these 5 slots. In the analysis in this paper, results for the origin and destination slots have been
combined to form the LOCATION slot.
138
References
H Ai, A Raux, D Bohus, M Eskenzai, and D Litman.
2008. Comparing spoken dialog corpora collected
with recruited subjects versus real users. In Proc SIG-
dial, Columbus, Ohio, USA.
AW Black, S Burger, B Langner, G Parent, and M Eske-
nazi. 2010. Spoken dialog challenge 2010. In Proc
SLT, Berkeley, CA.
J Henderson and O Lemon. 2008. Mixture model
POMDPs for efficient handling of uncertainty in di-
alogue management. In Proc ACL-HLT, Columbus,
Ohio.
G Parent and M Eskenazi. 2010. Toward better crowd-
sourced transcription: Transcription of a year of the
let?s go bus information system data. In Proc SLT,
Berkeley, CA.
A Raux, B Langner, D Bohus, A Black, and M Eskenazi.
2005. Let?s go public! Taking a spoken dialog system
to the real world. In Proc INTERSPEECH, Lisbon.
B Thomson and SJ Young. 2010. Bayesian update
of dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech and Language,
24:562?588.
JD Williams and S Balakrishnan. 2009. Estimating prob-
ability of correctness for ASR N-best lists. In Proc
SIGdial, London, UK.
JD Williams and SJ Young. 2007. Partially observable
Markov decision processes for spoken dialog systems.
Computer Speech and Language, 21(2):393?422.
JD Williams, I Arizmendi, and A Conkie. 2010. Demon-
stration of AT&T ?Let?s Go?: A production-grade sta-
tistical spoken dialog system. In Proc SLT, Berkeley,
CA.
JD Williams. 2007. Using particle filters to track di-
alogue state. In Proc IEEE Workshop on Automatic
Speech Recognition and Understanding (ASRU), Ky-
oto, Japan.
JD Williams, 2010a. AT&T Statistical Dialog
Toolkit. http://www.research.att.com/
people/Williams_Jason_D.
JD Williams. 2010b. Incremental partition recombina-
tion for efficient tracking of multiple dialog states. In
Proc Intl Conf on Acoustics, Speech, and Signal Pro-
cessing (ICASSP), Dallas, USA.
SJ Young, M Gas?ic?, S Keizer, F Mairesse, J Schatzmann,
B Thomson, and K Yu. 2010. The hidden information
state model: a practical framework for POMDP-based
spoken dialogue management. Computer Speech and
Language, 24(2):150?174, April.
139
Appendix: Mechanism illustrations
This appendix provides graphical illustrations of
each of the four mechanisms that can cause the top
ASR hypothesis to be different from the top belief
state hypothesis. These examples were taken from
logs of calls with real users, although some surface
forms have been simplified for space.
At the top of each panel is the system action taken.
The user?s true response is shown in italics in the
left-most column. The second column shows the
top 7 entries from the ASR N-best list, displayed
in the order produced by the speech recognition en-
gine. The third column shows the confidence score ?
the local probability of correctness assigned to each
ASR N-best entry. The last column shows the re-
sulting belief state, sorted by the magnitude of the
belief. Correct entries are shown in bold red.
ASR re-ranking and prior re-ranking occur within
one turn, and confidence aggregation and N-best
synthesis occur across two turns. These examples
all show cases where the belief state is correct and
the ASR is incorrect; however, the opposite also oc-
curs of course.
ASR 
Result
Conf
Score
Belief 
State
seven PM
seven AM
ten AM
--
--
--
--
seven AM
seven PM
ten AM
--
--
--
--
1
2
3
4
5
6
7
User 
action
"seven AM"
System : "What time are you leaving?"
Figure 7: Illustration of ASR re-ranking: The correct ASR hypothesis (?seven AM?) is in the n = 2
position, but it is assigned a higher confidence score than the misrecognized n = 1 entry ?seven PM?.
TIME uses a flat prior, so the higher confidence score results in ?seven AM? attaining the highest belief.
ASR 
Result
Conf
Score
Belief 
State
84C
54C
--
--
--
--
--
54C
84C
--
--
--
--
--
1
2
3
4
5
6
7
User 
action
"54C"
System : "Say a bus route, or say I'm not sure."
Figure 8: Illustration of Prior re-ranking: The correct ASR hypothesis (?54C?) is in the n = 2 position,
and it is assigned less confidence than the mis-recognized n = 1 entry, ?84C?. However, the prior on 54C
is much higher than on 84C, so 54C obtains the highest belief.
140
ASR 
Result
Conf
Score
Belief 
State
tomorrow
--
--
--
--
--
--
tomorrow
--
--
--
--
--
--
1
2
3
4
5
6
7
User 
action
"tomorrow"
ASR 
Result
Conf
Score
Belief 
State
july 8th
july 3rd
tuesday
sunday
july 5th
july 6th
--
tomorrow
&
july 8th
july 3rd
tuesday
sunday
july 5th
july 6th
1
2
3
4
5
6
7
User 
action
"tomorrow"
System : "Say the day you want, like today." System : "Sorry, say the day you want, like Tuesday."
Figure 9: Illustration of Confidence aggregation: In the first turn, ?tomorrow? is recognized with medium
confidence. In the second turn, ?tomorrow? does not appear on the N-best list; however the recognition
result has very low confidence, so this misrecognition is unable to dislodge ?tomorrow? from the top belief
position. At the end of the second update, the belief state?s top hypothesis of ?tomorrow? is correct even
though it didn?t appear on the second N-best list.
ASR 
Result
Conf
Score
Belief 
State
ridge ave
dallas ave
vernon ave
linden ave
highland ave
kelly ave
--
ridge ave
kelly ave
dallas ave
linden ave
highland ave
vernon ave
--
1
2
3
4
5
6
7
User 
action
"highland 
ave"
heron ave
herman ave
highland ave
--
--
--
--
highland ave
'
ridge ave
kelly ave
heron ave
dallas ave
herman ave
linden ave
ASR 
Result
Conf
Score
Belief 
State
1
2
3
4
5
6
7
User 
action
"highland 
ave"
System : "Where are you leaving from?" System : "Sorry, where are you leaving from?"
Figure 10: Illustration of N-best synthesis: In the first turn, the correct item ?highland ave? is on the
ASR N-best list but not in the top position. It appears in the belief state but not in the top position. In
the second turn, the correct item ?highland ave? is again on the ASR N-best list but again not in the top
position. However, because it appeared in the previous belief state, it obtains the highest belief after the
second update. Even though ?highland ave? was mis-recognized twice in a row, the commonality across the
two N-best lists causes it to have the highest belief after the second update.
141
