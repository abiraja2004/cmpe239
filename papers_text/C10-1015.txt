Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 125?133,
Beijing, August 2010
A Utility-Driven Approach to Question Ranking in Social QA
Razvan Bunescu
School of EECS
Ohio University
bunescu@ohio.edu
Yunfeng Huang
School of EECS
Ohio University
yh324906@ohio.edu
Abstract
We generalize the task of finding question
paraphrases in a question repository to a
novel formulation in which known ques-
tions are ranked based on their utility to
a new, reference question. We manually
annotate a dataset of 60 groups of ques-
tions with a partial order relation reflect-
ing the relative utility of questions inside
each group, and use it to evaluate mean-
ing and structure aware utility functions.
Experimental evaluation demonstrates the
importance of using structural informa-
tion in estimating the relative usefulness
of questions, holding the promise of in-
creased usability for social QA sites.
1 Introduction
Open domain Question Answering (QA) is one
of the most complex and challenging tasks in
natural language processing. While building on
ideas from Information Retrieval (IR), question
answering is generally seen as a more difficult
task due to constraints on both the input represen-
tation (natural language questions vs. keyword-
based queries) and the form of the output (fo-
cused answers vs. entire documents). Recently,
community-driven QA sites such as Yahoo! An-
swers and WikiAnswers have established a new
approach to question answering in which the bur-
den of dealing with the inherent complexity of
open domain QA is shifted from the computer
system to volunteer contributors. The computer
is no longer required to perform a deep linguis-
tic analysis of questions and generate correspond-
ing answers, and instead acts as a mediator be-
tween users submitting questions and volunteers
providing the answers. In most implementations
of community-driven QA, the mediator system
has a well defined strategy for enticing volun-
teers to post high quality answers on the website.
In general, the overall objective is to minimize
the response time and maximize the accuracy of
the answers, measures that are highly correlated
with user satisfaction. For any submitted ques-
tion, one useful strategy is to search the QA repos-
itory for similar questions that have already been
answered, and provide the corresponding ranked
list of answers, if such a question is found. The
success of this approach depends on the definition
and implementation of the question-to-question
similarity function. In the simplest solution, the
system searches for previously answered ques-
tions based on exact string matching with the
reference question. Alternatively, sites such as
WikiAnswers allow the users to mark questions
they think are rephrasings (?alternate wordings?,
or paraphrases) of existing questions. These ques-
tion clusters are then taken into account when per-
forming exact string matching, therefore increas-
ing the likelihood of finding previously answered
questions that are semantically equivalent to the
reference question. Like the original question an-
swering task, the solution to question rephrasing is
also based on volunteer contributions. In order to
lessen the amount of work required from the con-
tributors, an alternative solution is to build a sys-
tem that automatically finds rephrasings of ques-
tions, especially since question rephrasing seems
to be computationally less demanding than ques-
tion answering. The question rephrasing subtask
has spawned a diverse set of approaches. (Herm-
125
jakob et al, 2002) derive a set of phrasal patterns
for question reformulation by generalizing surface
patterns acquired automatically from a large cor-
pus of web documents. The focus of the work in
(Tomuro, 2003) is on deriving reformulation pat-
terns for the interrogative part of a question. In
(Jeon et al, 2005), word translation probabilities
are trained on pairs of semantically similar ques-
tions that are automatically extracted from an FAQ
archive, and then used in a language model that
retrieves question reformulations. (Jijkoun and de
Rijke, 2005) describe an FAQ question retrieval
system in which weighted combinations of simi-
larity functions corresponding to questions, exist-
ing answers, FAQ titles and pages are computed
using a vector space model. (Zhao et al, 2007)
exploit the Encarta logs to automatically extract
clusters containing question paraphrases and fur-
ther train a perceptron to recognize question para-
phrases inside each cluster based on a combina-
tion of lexical, syntactic and semantic similarity
features. More recently, (Bernhard and Gurevych,
2008) evaluated various string similarity measures
and vector space based similarity measures on the
task of retrieving question paraphrases from the
WikiAnswers repository.
According to previous work in this domain, a
question is considered a rephrasing of a reference
question Q0 if it uses an alternate wording to ex-
press an identical information need. For example,
Q0 and Q1 below may be considered rephrasings
of each other, and consequently they are expected
to have the same answer.
Q0 What should I feed my turtle?
Q1 What do I feed my pet turtle?
Community-driven QA sites are bound to face sit-
uations in which paraphrasings of a new ques-
tion cannot be found in the QA repository. We
believe that computing a ranked list of existing
questions that partially address the original infor-
mation need could be useful to the user, at least
until other users volunteer to give an exact an-
swer to the original, unanswered reference ques-
tion. For example, in the absence of any additional
information about the reference question Q0, the
expected answers to questions Q2 and Q3 above
may be seen as partially overlapping in informa-
tion content with the expected answer for the ref-
erence question. An answer to questionQ4, on the
other hand, is less likely to benefit the user, even
though it has a significant lexical overlap with the
reference question.
Q2 What kind of fish should I feed my turtle?
Q3 What do you feed a turtle that is the size of a
quarter?
Q4 What kind of food should I feed a turtle dove?
In this paper, we propose a generalization of
the question paraphrasing problem to a question
ranking problem, in which questions are ranked
in a partial order based on the relative information
overlap between their expected answers and the
expected answer of the reference question. The
expectation in this approach is that the user who
submits a reference question will find the answers
of the highly ranked question to be more useful
than the answers associated with the lower ranked
questions. For the reference question Q0 above,
the system is expected to produce a partial order
in whichQ1 is ranked higher thanQ2, Q3 andQ4,
whereas Q2 and Q3 are ranked higher than Q4. In
Section 2 we give further details on the question
ranking task and describe a dataset of questions
that have been manually annotated with partial or-
der information. Section 3 presents a set of initial
approaches to question ranking, followed by their
experimental evaluation in Section 4. The paper
ends with a discussion of future work, and con-
clusion.
2 A Partially Ordered Dataset for
Question Ranking
In order to enable the evaluation of question rank-
ing approaches, we created a dataset of 60 groups
of questions. Each group consists of a reference
question (e.g. Q0 above) that is associated with
a partially ordered set of questions (e.g. Q1 to
Q4 above). The 60 reference questions have been
selected to represent a diverse set of question cat-
egories from Yahoo! Answers. For each refer-
ence questions, its corresponding partially ordered
set is created from questions in Yahoo! Answers
126
REFERENCE QUESTION (Qr)
Q5 What?s a good summer camp to go to in FL?
PARAPHRASING QUESTIONS (P )
Q6 What camps are good for a vacation during the summer in FL?
Q7 What summer camps in FL do you recommend?
USEFUL QUESTIONS (U )
Q8 Does anyone know a good art summer camp to go to in FL?
Q9 Are there any good artsy camps for girls in FL?
Q10 What are some summer camps for like singing in Florida?
Q11 What is a good cooking summer camp in FL?
Q12 Do you know of any summer camps in Tampa, FL?
Q13 What is a good summer camp in Sarasota FL for a 12 year old?
Q14 Can you please help me find a surfing summer camp for beginners in Treasure Coast, FL?
Q15 Are there any acting summer camps and/or workshops in the Orlando, FL area?
Q16 Does anyone know any volleyball camps in Miramar, FL?
Q17 Does anyone know about any cool science camps in Miami?
Q18 What?s a good summer camp you?ve ever been to?
NEUTRAL QUESTIONS (N )
Q19 What?s a good summer camp in Canada?
Q20 What?s the summer like in Florida?
Table 1: A question group.
and other online repositories that have a high co-
sine similarity with the reference question. Due to
the significant lexical overlap between the ques-
tions, this is a rather difficult dataset, especially
for ranking methods that rely exclusively on bag-
of-words measures. Inside each group, the ques-
tions are manually annotated with a partial order
relation, according to their utility with respect to
the reference question. We shall use the notation
?Qi ? Qj |Qr? to encode the fact that question Qi
is more useful than question Qj with respect to
the reference question Qr. Similarly, ?Qi = Qj?
will be used to express the fact that questions Qi
andQj are reformulations of each other (the refor-
mulation relation is independent of the reference
question). The partial ordering among the ques-
tions Q0 to Q4 above can therefore be expressed
concisely as follows: ?Q0 = Q1?, ?Q1 ? Q2|Q0?,
?Q1 ? Q3|Q0?, ?Q2 ? Q4|Q0?, ?Q3 ? Q4|Q0?.
Note that we do not explicitly annotate the rela-
tion ?Q1 ? Q4|Q0?, since it can be inferred based
on the transitivity of the more useful than relation:
?Q1 ? Q2|Q0? ? ?Q2 ? Q4|Q0? ? ?Q1 ?
Q4|Q0?. Also note that no relation is specified
between Q2 and Q3, and similarly no relation can
be inferred between these two questions. This re-
flects our belief that, in the absence of any addi-
tional information regarding the user or the ?tur-
tle? referenced in Q0, we cannot compare ques-
tions Q2 and Q3 in terms of their usefulness with
respect to Q0.
Table 1 shows another reference question Q5
from our dataset, together with its annotated group
of questionsQ6 toQ20. In order to make the anno-
tation process easier and reproducible, we divide
it into two levels of annotation. During the first
annotation stage (L1), each question group is par-
titioned manually into 3 subgroups of questions:
? P is the set of paraphrasing questions.
? U is the set of useful questions.
? N is the set of neutral questions.
A question is deemed useful if its expected answer
may overlap in information content with the ex-
pected answer of the reference question. The ex-
pected answer of a neutral question, on the other
127
hand, should be irrelevant with respect to the ref-
erence question. LetQr be the reference question,
Qp ? P a paraphrasing question, Qu ? U a useful
question, and Qn ? N a neutral question. Then
the following relations are assumed to hold among
these questions:
1. ?Qp ? Qu|Qr?: a paraphrasing question is
more useful than a useful question.
2. ?Qu ? Qn|Qr?: a useful question is more
useful than a neutral question.
We also assume that, by transitivity, the following
ternary relations also hold: ?Qp ? Qn|Qr?, i.e. a
paraphrasing question is more useful than a neu-
tral question. Furthermore, if Qp1 , Qp2 ? P are
two paraphrasing questions, this implies ?Qp1 =
Qp2 |Qr?.
For the vast majority of questions, the first
annotation stage is straightforward and non-
controversial. In the second annotation stage (L2),
we perform a finer annotation of relations between
questions in the middle group U . Table 1 shows
two such relations (using indentation): ?Q8 ?
Q9|Q5? and ?Q8 ? Q10|Q5?. Question Q8 would
have been a rephrasing of the reference question,
were it not for the noun ?art? modifying the focus
noun phrase ?summer camp?. Therefore, the in-
formation content of the answer to Q8 is strictly
subsumed in the information content associated
with the answer to Q5. Similarly, in Q9 the fo-
cus noun phrase is further specialized through the
prepositional phrase ?for girls?. Therefore, (an
answer to) Q9 is less useful to Q5 than (an an-
swer to) Q8, i.e. ?Q8 ? Q9|Q5?. Furthermore,
the focus ?art summer camp? in Q8 conceptually
subsumes the focus ?summer camps for singing?
in Q10, therefore ?Q8 ? Q10|Q5?.
Table 2 below presents the following statistics
on the annotated dataset: the number of reference
questions (Qr), the total number of paraphrasings
(P), the total number of useful questions (U), the
total number of neutral questions (N ), and the to-
tal number of more useful than ordered pairs en-
coded in the dataset, either explicitly or through
transitivity, in the two annotation levels L1 and
L2.
Qr P U N L1 L2
60 177 847 427 7,378 7,639
Table 2: Dataset statistics.
3 Question Ranking Methods
An ideal question ranking method would take an
arbitrary triplet of questions Qr, Qi and Qj as
input, and output an ordering between Qi and
Qj with respect to the reference question Qr,
i.e. one of ?Qi ? Qj |Qr?, ?Qi = Qj |Qr?, or
?Qj ? Qi|Qr?. One approach is to design a
usefulness function u(Qi, Qr) that measures how
useful question Qi is for the reference question
Qr, and define the more useful than (?) relation
as follows:
?Qi ? Qj |Qr? ? u(Qi, Qr) > u(Qj , Qr)
If we define I(Q) to be the information need as-
sociated with question Q, then u(Qi, Qr) could
be defined as a measure of the relative overlap be-
tween I(Qi) and I(Qr). Unfortunately, the infor-
mation need is a concept that, in general, is de-
fined only intensionally and therefore it is diffi-
cult to measure. For lack of an operational def-
inition of the information need, we will approxi-
mate u(Qi, Qr) directly as a measure of the simi-
larity between Qi and Qr. The similarity between
two questions can be seen as a special case of
text-to-text similarity, consequently one possibil-
ity is to use a general text-to-text similarity func-
tion such as cosine similarity in the vector space
model (Baeza-Yates and Ribeiro-Neto, 1999):
cos(Qi, Qr) =
QTi Qr
?Qi??Qr?
Here, Qi and Qr denote the corresponding tf?idf
vectors. As a measure of question-to-question
similarity, cosine has two major drawbacks:
1. As an exclusively lexical measure, it is obliv-
ious to the meanings of words in each ques-
tion.
2. Questions are treated as bags-of-words,
and thus important structural information is
missed.
128
3.1 Meaning Aware Measures
The three questions below illustrate the first prob-
lem associated with cosine similarity. Q22 and
Q23 have the same cosine similarity with Q21,
they are therefore indistinguishable in terms of
their usefulness to the reference question Q21,
even though we expectQ22 to be more useful than
Q23 (a place that sells hydrangea often sells other
types of plants too, possibly including cacti).
Q21 Where can I buy a hydrangea?
Q22 Where can I buy a cactus?
Q23 Where can I buy an iPad?
To alleviate the lexical chasm, we can redefine
u(Qi, Qr) to be the similarity measure proposed
by (Mihalcea et al, 2006) as follows:
mcs(Qi, Qr) =
X
w?{Qi}
(maxSim(w,Qr) ? idf(w))
X
w?{Qi}
idf(w)
+
X
w?{Qr}
(maxSim(w,Qi) ? idf(w))
X
w?{Qr}
idf(w)
Since scaling factors are immaterial for ranking,
we have ignored the normalization constant con-
tained in the original measure. For each word
w ? Qi, maxSim(w,Qr) computes the maxi-
mum semantic similarity betweenw and any word
wr ? Qr. The similarity scores are then weighted
by the corresponding idf?s, and normalized. A
similar score is computed for each word w ? Qr.
The score computed by maxSim depends on the
actual function used to compute the word-to-word
semantic similarity. In this paper, we evaluated
four of the knowledge-based measures explored
in (Mihalcea et al, 2006): wup (Wu and Palmer,
1994), res (Resnik, 1995), lin (Lin, 1998), and
jcn (Jiang and Conrath, 1997). Since all these
measures are defined on pairs of WordNet con-
cepts, their analogues on word pairs (wi, wr) are
computed by selecting pairs of WordNet synsets
(ci, cr) such that wi belongs to concept ci, wr be-
longs to concept cr, and (ci, cr) maximizes the
similarity function. The measure introduced in
(Wu and Palmer, 1994) finds the least common
subsumer (LCS) of the two input concepts in the
WordNet hierarchy, and computes the ratio be-
tween its depth and the sum of the depths of the
two concepts:
wup(ci, cr) =
2 ? depth(lcs(ci, cr))
depth(ci) + depth(cr)
Resnik?s measure is based on the Information
Content (IC) of a concept c defined as the negative
log probability ? logP (c) of finding that concept
in a large corpus:
res(ci, cr) = IC(lcs(ci, cr))
Lin?s similarity measure can be seen as a normal-
ized version of Resnik?s information content:
lin(ci, cr) =
2 ? IC(lcs(ci, cr))
IC(ci) + IC(cr)
Jiang & Conrath?s measure is closely related to
lin and is computed as follows:
jcn(ci, cr) = [IC(ci) + IC(cr) ? 2 ? IC(lcs(ci, cr))]?1
3.2 Structure Aware Measures
Cosine similarity, henceforth referred as cos,
treats questions as bags-of-words. The meta-
measure proposed in (Mihalcea et al, 2006),
henceforth called mcs, treats questions as bags-
of-concepts. Consequently, both cos and mcs may
miss important structural information. If we con-
sider the question Q24 below as reference, ques-
tion Q26 will be deemed more useful than Q25
when using cos or mcs because of the higher rel-
ative lexical and conceptual overlap with Q24.
However, this is contrary to the actual ordering
?Q25 ? Q26|Q24?, which reflects that fact that
Q25, which expects the same answer type as Q24,
should be deemed more useful than Q26, which
has a different answer type.
Q24 What are some good thriller movies?
Q25 What are some thriller movies with happy
ending?
Q26 What are some good songs from a thriller
movie?
129
The analysis above shows the importance of us-
ing the answer type when computing the simi-
larity between two questions. However, instead
of relying exclusively on a predefined hierarchy
of answer types, we have decided to identify the
question focus of a question, defined as the set of
maximal noun phrases in the question that corefer
with the expected answer. Focus nouns such as
movies and songs provide more discriminative in-
formation than general answer types such as prod-
ucts. We use answer types only for questions such
as Q27 or Q28 below that lack an explicit question
focus. In such cases, an artificial question focus
is created from the answer type (e.g. location for
Q27, or method for Q28) and added to the set of
question words.
Q27 Where can I buy a good coffee maker?
Q28 How do I make a pizza?
Let qsim be a general bag-of-words question sim-
ilarity measure (e.g. cos or mcs). Furthermore, let
wsim by a generic word meaning similarity mea-
sure (e.g. wup, res, lin or jcn). The equation be-
low describes a modification of qsim that makes it
aware of the questions focus:
qsimf (Qi, Qr) = wsim(fi, fr) ?
qsim(Qi?{fi}, Qr?{fr})
Here, Qi and Qr refer both to the questions and
their sets of words, while fi and fr stand for the
corresponding focus words. We define qsim to
return 1 if one of its arguments is an empty set,
i.e. qsim(?, ) = qsim( , ?) = 1. The new
similarity measure qsimf multiplies the seman-
tic similarity between the two focus words with
the bag-of-words similarity between the remain-
ing words in the two questions. Consequently, the
word ?movie? in Q26 will not be compared with
the word ?movies? in Q24, and therefore Q26 will
receive a lower utility score than Q25.
In addition to the question focus, the main verb
of a question can also provide key information
in estimating question-to-question similarity. We
define the main verb to be the content verb that
is highest in the dependency tree of the question,
e.g. buy for Q27, or make for Q28. If the question
does not contain a content verb, the main verb is
defined to be the highest verb in the dependency
tree, as for example are in Q24 to Q26. The utility
of a question?s main verb in judging its similarity
to other questions can be seen more clearly in the
questions below, where Q29 is the reference:
Q29 How can I transfer music from iTunes to my
iPod?
Q30 How can I upload music to my iPod?
Q31 How can I play music in iTunes?
The fact that upload, as the main verb of Q30, is
more semantically related to transfer (upload is a
hyponym of transfer in WordNet) is essential in
deciding that ?Q30 ? Q31|Q29?, i.e. Q30 is more
useful than Q31 to Q29.
Like the focus word, the main verb can be in-
corporated in the question similarity function as
follows:
qsimfv(Qi, Qr) = wsim(fi, fr) ? wsim(vi, vr) ?
qsim(Qi?{fi, vi}, Qr?{fr, vr})
The new measure qsimfv takes into account
both the focus words and the main verbs when
estimating the semantic similarity between ques-
tions. When decomposing the questions into focus
words, main verbs and the remaining words, we
have chosen to multiply the corresponding sim-
ilarities instead of, for example, summing them.
Consequently, a close to zero score in each of
them would drive the entire similarity to zero.
This reflects the belief that question similarity is
sensitive to each component of a question.
4 Experimental Evaluation
We use the question ranking dataset described in
Section 2 to evaluate the two similarity measures
cos and mcs, as well as their structured versions
cosf , cosfv, mcsf , and mcsfv. We report one
set of results for each of the four word similarity
measures wup, res, lin or jcn. Each question simi-
larity measure is evaluated in terms of its accuracy
on the set of ordered pairs for each of the two an-
notation levels described in Section 2. Thus, for
the first annotation level (L1) , we evaluate only
over the set of relations defined across the three
130
Question Word similarity (wsim)
similarity wup res lin jcn
(qsim) L1 L2 L1 L2 L1 L2 L1 L2
cos 69.1 69.3 69.1 69.3 69.1 69.3 69.1 69.3
cosf 69.9 70.1 72.5 72.7 71.0 71.2 69.6 69.8
cosfv 69.9 70.1 72.5 72.6 71.0 71.2 69.6 69.8
mcs 62.6 62.5 65.0 65.0 65.6 65.7 66.8 66.9
mcsf 64.2 64.4 68.5 68.5 68.8 68.9 67.2 67.4
mcsfv 65.8 66.0 68.8 68.8 69.7 69.8 67.7 67.8
Table 3: Accuracy results, with and without meaning and structure information.
sets R, U , and N . If ?Qi ? Qj |Qr? is a rela-
tion specified in the annotation, we consider the
tuple ?Qi, Qj , Qr? correctly classified if and only
if u(Qi, Qr) > u(Qj , Qr), where u is the ques-
tion similarity measure (Section 3). For the sec-
ond annotation level (L2), we also consider the re-
lations annotated between useful questions inside
the group U .
We used the NLTK 1 implementation of the four
similarity measures wup, res, lin or jcn. The idf
values for each word were computed from fre-
quency counts over the entire Wikipedia. For each
question, the focus is identified automatically by
an SVM tagger trained on a separate corpus of
2,000 questions manually annotated with focus in-
formation. The SVM tagger uses a combination
of lexico-syntactic features and a quadratic ker-
nel to achieve a 93.5% accuracy in a 10-fold cross
validation evaluation on the 2,000 questions. The
main verb of a question is identified deterministi-
cally using a breadth first traversal of the depen-
dency tree.
The overall accuracy results presented in Ta-
ble 3 show that using the focus word improves the
performance across all 8 combinations of question
and word similarity measures. For cosine simi-
larity, the best performing system uses the focus
words and Resnik?s similarity function to obtain a
3.4% increase in accuracy. For the meaning aware
similarity mcs, the best performing system uses
the focus words, the main verb and Lin?s word
similarity to achieve a 4.1% increase in accu-
racy. The improvement due to accounting for fo-
cus words is consistent, whereas adding the main
1http://www.nltk.org
verb seems to improve the performance only for
mcs, although not by a large margin. The second
level of annotation brings 261 more relations in
the dataset, some of them more difficult to anno-
tate when compared with the three groups in the
first level. Nevertheless, the performance either
remains the same (somewhat expected due to the
relatively small number of additional relations), or
is marginally better. The random baseline ? as-
signing a random similarity value to each pair of
questions ? results in 50% accuracy. A somewhat
unexpected result is that mcs does not perform
better than cos on this dataset. After analysing
the result in more detail, we have noticed that mcs
seems to be less resilient than cos to variations in
the length of the questions. The Microsoft para-
phrase corpus was specifically designed such that
?the length of the shorter of the two sentences, in
words, is at least 66% that of the longer? (Dolan
and Brockett, 2005), whereas in our dataset the
two questions in a pair can have significantly dif-
ferent lengths 2.
The questions in each of the 60 groups have a
high degree of lexical overlap, making the dataset
especially difficult. In this context, we believe the
results are encouraging. We expect to obtain fur-
ther improvements in accuracy by allowing rela-
tions between all the words in a question to in-
fluence the overall similarity measure. For exam-
ple, question Q19 has the same focus word as the
reference question Q5 (repeated below), yet the
difference between the focus word prepositional
modifiers makes it a neutral question.
2Our implementation of mcs did performed better than
cos on the Microsoft dataset.
131
Q5 What?s a good summer camp to go to in FL?
Q19 What?s a good summer camp in Canada?
Some of the questions in our dataset illustrate the
need to design a word similarity function specif-
ically tailored to reflect how words change the
relative usefulness of a question. In the set of
questions below, in deciding that Q33 and Q34
are more useful than Q36 for the reference ques-
tion Q32, an ideal question ranker needs to know
that the ?Mayflower Hotel? and the ?Queensboro
Bridge? are in the proximity of ?Midtown Man-
hattan?, and that proximity relations are relevant
when asking for directions. A coarse measure
of proximity can be obtained for the pair (?Man-
hattan?, ?Queensboro Bridge?) by following the
meronymy links connecting the two entities in
WordNet. However, a different strategy needs to
be devised for entities such as ?Mayflower Hotel?,
?JFK?, or ?La Guardia? which are not covered in
WordNet.
Q32 What is the best way to get to MidtownMan-
hattan from JFK?
Q33 What?s the best way from JFK to Mayflower
Hotel?
Q34 What?s the best way from JFK to Queens-
boro Bridge?
Q35 How do I get from Manhattan to JFK airport
by train?
Q36 What is the best way to get to LaGuardia
from JFK?
Finally, to realize why question Q35 is useful one
needs to know that, once directions on how to get
by train from location X to location Y are known,
then normally it suffices to reverse the list of stops
in order to obtain directions on how to get from Y
back to X.
5 Future Work
We plan to integrate the entire dependency struc-
ture of the question in the overall similarity mea-
sure, possibly by defining kernels between ques-
tions in a maximum margin model for ranking.
We also plan to extend the word similarity func-
tions to better reflect the types of relations that
are relevant when measuring question utility, such
as proximity relations between locations. Further-
more, we intend to take advantage of databases of
interrogative paraphrases and paraphrase patterns
that were created in previous research on question
reformulation.
6 Conclusion
We presented a novel question ranking task in
which previously known questions are ordered
based on their relative utility with respect to a new,
reference question. We created a dataset of 60
groups of questions 3 annotated with a partial or-
der relation reflecting the relative utility of ques-
tions inside each group, and used it to evaluate
the ranking performance of several meaning and
structure aware utility functions. Experimental re-
sults demonstrate the importance of using struc-
tural information in judging the relative usefulness
of questions. We believe that the new perspective
on ranking questions has the potential to signifi-
cantly improve the usability of social QA sites.
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful suggestions.
References
Baeza-Yates, Ricardo and Berthier Ribeiro-Neto.
1999. Modern Information Retrieval. ACM Press,
New York.
Bernhard, Delphine and Iryna Gurevych. 2008. An-
swering learners? questions by retrieving question
paraphrases from social Q&A sites. In EANL ?08:
Proceedings of the Third Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 44?52, Morristown, NJ, USA. Association for
Computational Linguistics.
Dolan, William B. and Chris Brockett. 2005. Auto-
matically constructing a corpus of sentential para-
phrases. In Proceedings of the Third International
Workshop on Paraphrasing (IWP2005), pages 9?16.
3The dataset will be made publicly available.
132
Hermjakob, Ulf, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural language based reformula-
tion resource and web exploitation for question an-
swering. In Proceedings of TREC-2002.
Jeon, Jiwoon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of the 14th ACM in-
ternational conference on Information and knowl-
edge management (CIKM?05), pages 84?90, New
York, NY, USA. ACM.
Jiang, J.J. and D.W. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proceedings of the International Conference on
Research in Computational Linguistics, pages 19?
33.
Jijkoun, Valentin and Maarten de Rijke. 2005. Re-
trieving answers from frequently asked questions
pages on the Web. In Proceedings of the 14th ACM
international conference on Information and knowl-
edge management (CIKM?05), pages 76?83, New
York, NY, USA. ACM.
Lin, Dekang. 1998. An information-theoretic def-
inition of similarity. In Proceedings of the Fif-
teenth International Conference on Machine Learn-
ing (ICML ?98), pages 296?304, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Mihalcea, Rada, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceed-
ings of the 21st national conference on Artificial in-
telligence (AAAI?06), pages 775?780. AAAI Press.
Resnik, Philip. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In IJ-
CAI?95: Proceedings of the 14th international joint
conference on Artificial intelligence, pages 448?
453, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Tomuro, Noriko. 2003. Interrogative reformulation
patterns and acquisition of question paraphrases. In
Proceedings of the Second International Workshop
on Paraphrasing, pages 33?40, Morristown, NJ,
USA. Association for Computational Linguistics.
Wu, Zhibiao and Martha Palmer. 1994. Verbs se-
mantics and lexical selection. In Proceedings of the
32nd annual meeting on Association for Computa-
tional Linguistics, pages 133?138, Morristown, NJ,
USA. Association for Computational Linguistics.
Zhao, Shiqi, Ming Zhou, and Ting Liu. 2007. Learn-
ing question paraphrases for QA from Encarta logs.
In Proceedings of the 20th international joint con-
ference on Artifical intelligence (IJCAI?07), pages
1795?1800, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
133
