A Statistical NLP Approach for 
Feature and Sentiment Identification from Chinese Reviews 
Zhen Hai1 Kuiyu Chang1 Qinbao Song2 Jung-jae Kim1 
1School of Computer Engineering, Nanyang Technological University, Singapore 639798 
{haiz0001, askychang, jungjae.kim}@ntu.edu.sg 
2Department of Computer Science and Technology, Xi?an Jiaotong University, Xi?an 710049, China 
qbsong@mail.xjtu.edu.cn 
 
Abstract 
Existing methods for extracting features 
from Chinese reviews only use 
simplistic syntactic knowledge, while 
those for identifying sentiments rely 
heavily on a semantic dictionary. In this 
paper, we present a systematic technique 
for identifying features and sentiments, 
using both syntactic and statistical anal-
ysis. We firstly identify candidate fea-
tures using a proposed set of common 
syntactic rules. We then prune irrelevant 
candidates with topical relevance scores 
below a cut-off point. We also propose 
an association analysis method based on 
likelihood ratio test to infer the polarity 
of opinion word. The sentiment of a fea-
ture is finally adjusted by analyzing the 
negative modifiers in the local context 
of the opinion word. Experimental re-
sults show that our system performs sig-
nificantly better than a well-known opi-
nion mining system. 
1 Introduction 
There were 420 million Internet users in China 
by the end of June 2010. As a result, online so-
cial media in China has accumulated massive 
amount of valuable peer reviews on almost any-
thing. Mining this pool of Chinese reviews to 
detect features (e.g. ???? mobile phone) and 
identify the corresponding sentiment (e.g. posi-
tive, negative) has recently become a hot re-
search area. However, the vast majority of pre-
vious work on feature detection only uses sim-
plistic syntactic natural language processing 
(NLP) approaches, while those on sentiment 
identification depend heavily on a semantic dic-
tionary. Syntactic approaches are often prone to 
errors due to the informal nature of online re-
views. Dictionary-based approaches are more 
robust than syntactic approaches, but must be 
constantly updated with new terms and expres-
sions, which are constantly evolving in online 
reviews.  
To overcome these limitations, we propose a 
statistical NLP approach for Chinese feature and 
sentiment identification. The technique is in fact 
the core of our Chinese review mining system, 
called Idea Miner or iMiner. Figure 1 shows the 
architectural overview of iMiner, which com-
prises five modules, of which Module ? (Opi-
nion Feature Detection) and ? (Contextual Sen-
timent Identification) are the main focus of this 
paper. 
? Segmentation, POS Tagging, 
and Syntactic Parsing
Feature
? Candidate
 Feature Pruning
? Candidate
Feature Extraction
? Contextual
Sentiment Identification
? Polarity Inference
for Opinion Word
Sentiment
? Review Crawling
? Feature-Sentiment Summary  
Figure 1:  Overview of the iMiner system. 
2 Related Work 
Qiu et al (2007) used syntactic analysis to iden-
tify features 1  in Chinese sentences, which is 
similar to the methods proposed by Zhuang et al 
(2006) and Xia et al (2007). However, syntactic 
analysis alone tends to extract many invalid fea-
tures due to the colloquial nature of online re-
views, which are often abruptly concise or 
                                                 
1 A feature refers to the subject of an opinion. 
grammatically incorrect. To address the issue, 
our approach employs an additional step to 
prune candidates with low topical relevance, 
which is a statistical measure of how frequently 
a term appears in one review and across differ-
ent reviews.  
Pang et al (2002) examined the effectiveness 
of using supervised learning methods to identify 
document level sentiments. But the technique 
requires a large amount of training data, and 
must be re-trained whenever it is applied to a 
new domain. Furthermore, it does not perform 
well at the sentence level. Zhou et al (2008) 
and Qiu et al (2008) proposed dictionary-based 
approaches to infer contextual sentiments from 
Chinese sentences. However, it is difficult to 
maintain an up-to-date dictionary, as new ex-
pressions emerge frequently online. In contrast, 
to identify the sentiment expressed in a review 
region2, our method first infers the polarity of 
an opinion word by using statistical association 
analysis, and subsequently analyzes the local 
context of the opinion word. Our method is do-
main independent and uses only a small set of 
80 polarized words instead of a huge dictionary. 
2.1 Topic Detection and Tracking 
The task of Topic Detection and Tracking is to 
find and follow new events in a stream of news 
stories. Fukumoto and Suzuki (2000) proposed 
a domain dependence criterion to discriminate a 
topic from an event, and find all subsequent 
similar news stories. Our idea of topical relev-
ance is related but different; we only focus on 
the relevance of a candidate feature with respect 
to a review topic, so as to extract the features on 
which sentiments are expressed. 
2.2 Polarity Inference for Opinion Word 
Turney (2002) used point-wise mutual informa-
tion (PMI) to predict the polarity of an opinion 
word O, which is calculated as MI1-MI2, where 
MI1 is the mutual information between word O 
and positive word ?excellent?, and MI2 denotes 
the mutual information between O and negative 
word ?poor?. Instead of PMI, our method uses 
the likelihood ratio test (LRT) to compute the 
semantic association between an opinion word 
and each seed word, since LRT leads to better 
                                                 
2A review region is a sentence or clause which con-
tains one and only feature. 
results in practice. Finally, the polarity is calcu-
lated as the weighted sum of the polarity values 
of all seed words, where the weights are deter-
mined by the semantic association. 
2.3 Feature-Sentiment Pair Identification 
Turney (2002) proposed an unsupervised learn-
ing algorithm to identify the overall sentiments 
of reviews. However, his method does not 
detect features to associate with the sentiments. 
Shi and Chang (2006) proposed to build a huge 
Chinese semantic lexicon to extract both fea-
tures and sentiments. Other lexicon-based work 
for identifying feature-sentiment pair was pro-
posed by Yi et al (2003) and Xia et al (2007). 
We propose a new statistical NLP approach to 
identify feature-sentiment pairs, which uses not 
only syntactic analysis but also data-centric sta-
tistical analysis. Most importantly, our approach 
requires no semantic lexicon to be maintained. 
3 Feature Detection 
Module ? in iMiner aims to detect opinion 
features, which are subjects of reviews, such as 
the product itself like ???? (mobile phone) or 
specific attributes like ???? (screen). 
Example 1: ???????????? (I like 
the color of this mobile phone). 
In example 1, the noun ???? (color) indi-
cates a feature. Some features are expressed 
implicitly in review sentences, as shown below. 
Example 2: ?????????? (Too expen-
sive, I cannot afford it). 
In example 2, the noun ???? (price) is the 
opinion feature of this sentence, but it does not 
occur explicitly. In this paper, we do not deal 
with implicit features, but instead focus on the 
extraction of explicit features only. 
3.1 Candidate Feature Extraction 
According to our observation, features are gen-
erally expressed as nouns and occur in certain 
patterns in Chinese reviews. Typically, a noun 
acting as the object or subject of a verb is a po-
tential feature. In addition, when a clause con-
tains only a noun phrase without any verbs, the 
headword of the noun phrase is also a candidate. 
Due to the colloquial nature of online reviews, it 
is complicated and nearly impossible to collect 
all possible syntactic roles of features. Thus, we 
Table 1: Dependence relations and syntactic rules for candidate feature extraction. 
Relation Rule 
only use the aforementioned three primary pat-
terns to extract an initial set of candidates. 
Dependence Grammar (Tesniere, 1959) expl- 
ores asymmetric governor-dependent relation-
ship between words, which are then combined 
into the dependency structure of sentences. The 
three dependency relations SBV, VOB, and 
HED correspond to the three aforementioned 
patterns. For each relation, we define a rule with 
additional restrictions for candidate feature ex-
traction, as shown in Table 1. 
Candidate features are extracted in the fol-
lowing manner: for each word, we first deter-
mine if it is a noun; if so, we apply the VOB, 
SBV, and HED rules sequentially. A noun 
matching any of the rules is extracted as a can-
didate feature. 
3.2 Candidate Feature Pruning 
Due to the informal nature of online reviews, a 
large number of irrelevant candidates are ex-
tracted by the three syntactic rules. Thus, we 
need to further prune them by using additional 
techniques. 
Intuitively, candidates that are found in many 
reviews should be more representative com-
pared to candidates that occur in only a few re-
views. This characteristic of candidates can be 
captured by the topical relevance (TR) score. 
TR can be used to measure how strongly a can-
didate feature is relevant to a review topic. The 
TR of a candidate is described by two indica-
tors, i.e., dispersion and deviation. Dispersion 
indicates how frequently a candidate occurs 
across different reviews, while deviation de-
notes how many times a candidate appears in 
one review. The topical relevance score (TRS) 
is calculated by combining both dispersion and 
deviation. Candidate features with high TRS are 
supposed to be highly relevant, while those with 
TRS lower than a specified threshold are re-
jected. 
Formally, let the i-th candidate feature be de-
noted by Ti, and the j-th review document3 by 
Dj. The weight of feature Ti in document Dj is 
denoted by Wij, which could be computed based 
on TF.IDF (Luhn, 1957) shown in formula (1): 
(1 log ) * log 0
0
ij ij
iij
N
TF if TF
DFW
otherwise
+      >=
                                    
?????
 (1)
TFij denotes the term frequency of Ti in Dj, and 
DFi denotes the document frequency of Ti; N 
indicates the number of documents in the cor-
pus. We compute the standard deviation Si: 
2
1
( )
1
N
iij
j
i
W W
S
N
=
?
= ?
?
 
(2)
where the average weight of Ti across all docu-
ments is calculated as follows: 
1
1 N
i i
j
W W
N =
= j? . 
The dispersion Dispi of Ti is then calculated: 
i
i
i
W
Disp
S
=  (3)
The deviation Deviij of Ti in Dj is computed: 
                                                 
3 A review document refers to a forum review, which 
tends to be shorter than full length editorial articles. 
Interpretation Example (3-5) 
VOB ( , ) ( , )N VOB N C ?   
If term is noun (N) and 
depends on another com-
ponent with relation VOB, 
extract as candidate (C). 
????????? (I like the mobile 
phone). The noun ???? relies on the 
word ???? with relation VOB, thus, 
???? is extracted as candidate. 
SBV ( , ) ( , )N SBV N C ?   
If term is noun (N) and 
depends on another com-
ponent with relation SBV, 
extract as candidate (C). 
??????? (The screen is too 
small). The noun ???? depends on 
the word ??? with relation SBV, thus 
???? is extracted as candidate. 
HED ( , ) ( , )N HED N C ?   
If term is noun (N) and 
governs another compo-
nent with relation HED, 
extract as candidate (C). 
??????? (beautiful exterior). 
The noun ???? governs the word 
???? with relation HED, thus, ??
?? is extracted as candidate. 
'
ij ij jDevi W W= ?  (4)
The average scalar weight of all candidate fea-
tures in Dj is calculated as follows: 
1
' 1 M
j
i
ijW WM =
= ?  
where M is the vocabulary size of Dj. 
We can obtain the topical relevance score 
TRSij of Ti in Dj finally as follows: 
*ij i ijTRS Disp Devi=  (5)
By combining the dispersion and deviation, 
the quantity TRSij thus captures the topical re-
levance strength of Ti with respect to the topic of 
document Dj. 
All candidates of a document are then sorted 
in descending order of TRS, and those with 
TRS above a pre-specified threshold are ex-
tracted as opinion features. In fact, we can ex-
tract candidates at the document, paragraph, or 
sentence resolution. In practice, we observe no 
significant performance differences at the vari-
ous resolutions. 
3.3 Experimental Evaluation 
We collected 2,986 real-life review documents 
about mobile phones from major online Chinese 
forums. Each document corresponds to a forum 
topic, where each paragraph in a document 
matches a thread under the topic. Of these, we 
manually annotated the features and sentiment 
orientations expressed in 219 randomly selected 
documents, which include 600 review sentences. 
To evaluate the performance of our approach, 
we first conducted an experiment for extracting 
candidate features. We then performed three 
other experiments for pruning the candidates at 
the document, paragraph, and sentence levels, 
respectively. For each experiment, we tried sev-
eral different thresholds, i.e., percentage of TRS 
mean (TRSM) of all candidates. The average F-
measure (F), precision (P), and recall (R) of the 
results at the three levels are shown in Figure 2. 
The highest F-measure results of feature detec-
tion with and without pruning are shown in Ta-
ble 2 for easy comparison. 
Table 2: Feature detection results. 
Feature Detection P (%) R (%) F (%) 
No Pruning 71.61 90.69 80.03 
Pruning  (33% TRSM) 81.56 85.22 83.35 
As line 2 of Table 2 shows, feature detection 
without pruning achieves 90.69% recall, which 
shows that the proposed syntactic rules have 
excellent coverage. However, its precision is not 
so promising, achieving only 71.61%, which 
means that many irrelevant candidates are also 
extracted by our rules. Thus, relying on syntac-
tic analysis alone is not good enough, and we 
need to take one more step to prune the candi-
date features. 
As line 3 of Table 2 shows, after pruning the 
candidate set, precision improved remarkably 
by 10% to 81.56%, while recall dropped slightly 
to 85.22%. For online review mining, precision 
is much more important than recall, because 
users? confidence in iMiner rely heavily on the 
accuracy of the results they see (precision), and 
not on what they don?t see (recall). 
 
Figure 2: iMiner feature pruning results. 
Figure 2 plots the results of pruning at vari-
ous TRSM thresholds. The best F-measure of 
83.35% was achieved with a 33% TRSM. If we 
increase the threshold to 43%, the precision in-
creases to 83.19%, while the recall drops to 
81.57%. By exploring the distribution of a can-
didate in corpus, its topical relevance with re-
spect to the review topic can be measured statis-
tically, which allows the noisy candidates to be 
pruned effectively. From the results in Figure 2, 
our idea of topical relevance is shown to be 
highly effective in detecting features. 
Table 3: Characteristics of FBS and iMiner. 
Aspects FBS iMiner 
Candidates Nouns from POS tagger 
Nouns from syn-
thetic analysis 
Pruning Association Mining 
Topical 
Relevance 
Opinion word Adjectives Adjectives, verbs 
Polarity 
inference 
Dictionary 
based 
LRT association 
based 
Sentiment 
Resolution Sentence Sentence, clause 
Negation Single Single, double 
We compared our results with that of the as-
sociation mining-based method in Feature-based 
Summarization (FBS) (Hu and Liu, 2004) on 
the same dataset. Table 3 summarizes the dif-
ferences between FBS and iMiner, parts of 
which are elaborated in Section 4. The results of 
FBS with various support thresholds are shown 
in Figure 3. The support corresponds to the per-
centage of total number of review sentences. 
FBS attained the highest F-measure of 76.35% 
at a support of 0.4% with 79.6% precision and 
73.36% recall. As the support increases, the 
precision also increases from 62.99% to 86.92%, 
while the recall decreases from 91.61% to 
61.86%. Comparing the best results of the two 
systems, iMiner beats FBS by 7% in F-Measure, 
1.96% in precision, and 11.86% in recall. 
 
Figure 3: FBS feature extraction results. 
We find that FBS suffers from the following 
limitations: (1) FBS extracted an additional 
14.11% noisy candidate features due to the lack 
of syntactic analysis, which requires more ex-
tensive pruning; and (2) FBS only considers 
sentence frequency in computing the support to 
identify frequent candidate features, ignoring 
the candidate frequency within the sentence. 
3.4 Feature Extraction Error Analysis 
We categorize our feature extraction errors 
into 4 main types, FE1 to FE4, as follows. 
FE1: When more than one candidate exists in 
a review region, our algorithm may pick the 
wrong features due to misplaced priorities. Note 
that we assume only one (dominant) feature per 
region in both our algorithm and the labeled 
dataset. A total of 43% errors were due to pick-
ing the wrong dominant candidate. 
Example 6: ????????????? (The 
 sound is too weak, people cannot listen clear-
ly). 
In example 6, both ???? and ??? are ex-
tracted as features. However, the noun ??? is 
an incorrect feature detected by our algorithm.  
FE2: The proposed set of common syntactic 
rules is not comprehensive, missing out 23% of 
true features.  
Example 7: ???????????? (I am 
sick about this phone). 
In example 7, the noun ???? is a missed 
feature. This is in fact a POB dependence rela-
tion, which is outside the scope of our three 
rules. 
FE3: About 22% errors are due to irrelevant 
features possessing high TR scores, and there-
fore which are not pruned subsequently. 
Example 8: ???????????? (I like it 
very much, but I have no money to buy it). 
In example 8, the noun ??? is incorrectly 
confirmed as a feature due to its high TR score. 
FE4: About 9% errors are due to incorrect 
POS tags.  
Example 9: ????????? (Consistent 
interruption during phone calls). 
In example 9, the verb ??? is extracted in-
correctly as a feature, since it is incorrectly 
tagged as a noun. The remaining 3% of the er-
rors are due to the system incorrectly extracting 
features from sentences that contain no opi-
nions. 
4 Contextual Sentiment Identification 
The main task of module ? in iMiner is to iden-
tify the contextual sentiment of a feature. A 
two-step approach is proposed: (1) The polarity 
of an opinion word within a review region is 
inferred via association analysis based on the 
likelihood ratio test; and (2) the sentiment is 
validated against the contextual information of 
the opinion word in the region and finalized. 
4.1 Polarity Inference for Opinion Word 
To infer polarity, an opinion word is first identi-
fied in a review region, as described in Figure 4. 
Note that we consider not only adjectives but 
also verbs as opinion words. We then measure 
the association between the opinion word and 
each seed word. We calculate the polarity value 
of the opinion word as the association weighted 
sum of polarities of all seed words. 
Example 10: ??????????? (The 
price of this mobile phone is very cheap). 
Example 10 contains an adjective ??? ? 
(cheap) that governs the feature ???? (price); 
thus ???? is extracted as an opinion word. 
 
1.  feature Ti and word Wj in the same region 
2.     if (Wj = adjective and depends on Ti) 
3.        extract Wj as opinion word; 
4.     else if (Wj = adjective and governs Ti) 
5.        extract Wj as opinion word; 
6.     else if (Wj = verb and governs Ti) 
7.        extract Wj as opinion word; 
Figure 4: Extracting Opinion Word 
A set of polarized words were collected from 
corpus as seed words, including 35 positive 
words, 36 negative words, and 9 neutral words. 
Each seed word is assigned a polarity weight 
from -10 to 10. For example, ???? (lovely) 
has a score of 10, ???? (common) has a score 
of 0, and ???? (lousy) has a score of -10. 
To measure the semantic association Aij be-
tween an opinion word Oi and each seed word Sj, 
we propose a formula based on the likelihood 
ratio test (Dunning, 1993), as follows: 
1 1 1 2 2 2
1 1 2 2
2[ log ( , , ) log ( , , )
log ( , , ) log ( , , ) ]
ijA L p k n L p k n
L p k n L p k n
 +
                    ? ?  
=
 (6) 
where 
( , , ) (1 )k nL p k n p p ?= ? k ; 
1 2
1 2
k k
p
n n
+= + , 
1
1
1
k
p
n
= , 22
2
k
p
n
= ; 
1 1n k k= + 3 2 2n k k= +, . 4
The variable k1(O, S) in Table 4 refers to the 
count of documents containing both opinion 
word O and seed word S, k2(O,?S) indicates the 
number of documents containing O but not S, 
k3(?O, S) counts the number of documents con-
taining S but not O, while k4(?O,? S) tallies the 
count of documents containing neither O nor S. 
Table 4: Document counts. 
 S ? S 
O k1 (O, S) k2 (O,? S) 
?O k3 (?O, S) k4 (?O,? S) 
The higher the quantity Aij, the stronger the 
semantic association is between the opinion 
word and the seed word. 
The polarity value OVi of the opinion word Oi 
is computed as the association weighted average 
of all seed word polarity values: 
1
*
L
ij
i j
j i
A
OV SV
A=
= ?  (7)
The sum Ai of all association strength is calcu-
lated as follows: 
1
L
i i
j
A A
=
= j? ; 
where Aij denotes the association between Oi 
and Sj, SVj  indicates the polarity value of Sj, and 
L is the size of the seed word list. 
After performing association analysis, we 
then classify the polarity value OVi using an 
upper bound V+ and lower bound V-, such that 
if Vi is larger than V+, then the polarity is in-
ferred as positive; conversely if Vi is smaller 
than V-, then the polarity is inferred as negative; 
otherwise, it is neutral. Here, the V+ and V- 
boundaries refer to thresholds that can be de-
termined experimentally. 
4.2 Contextual Sentiment Identification 
Apart from inferring the polarity of opinion 
words, we also examine additional contextual 
information around the opinion words. In fact, 
the final sentiment is determined by combining 
the polarity with the contextual information. In 
this work, we focus on negative modifiers, as 
shown in the examples below. 
Example 11: ?????????? (I do not 
like this mobile phone). 
In example 11, the polarity of the opinion 
word ???? (like) is inferred as positive, but 
the review region expresses a negative orienta-
tion to the feature ????, because a negation 
word ??? (not) modifies ????. Thus, it is 
important to locate negative modifiers. 
Example 12: ??????????? (The 
screen of this mobile phone is not unlovely). 
In example 12, the polarity of opinion word 
???? (lovely) is inferred as positive. By ex-
amining its direct modifier, i.e., ??? (un-), we 
identify the sentiment of ????? (unlovely) as 
negative. However, the final sentiment about the 
feature ???? (screen) is actually positive due 
to the earlier negation ???? (not), which mod-
ifies the latter ????? (unlovely). This is what 
we call a double negation sentence, which is not 
uncommon in reviews. Therefore, it is necessary 
to take two additional steps to capture the 
double negation as follows. 
Figure 5 shows the main steps of identifying 
contextual sentiment. For an opinion word Oi in 
the review region, we first determine if there 
exists an adverb modifying it. If so, we extract 
the adverb as the direct modifier. If the modifier 
has a negative meaning, then we reverse the 
prior polarity of Oi. Similarly, we can take one 
additional step to locate the double negation 
modifier and finally identify the contextual sen-
timent orientation. 
1.  for each opinion word Oi 
2.     if (a word Wj = adverb and depends on Oi) 
3.        extract Wj as direct modifier; 
4.        if (word Wj = negation word) 
5.           reverse the prior polarity of Oi; 
6.        if (word Wk = adverb and relies on Wj) 
7.           extract Wk as indirect modifier; 
8.           if (word Wk = negation word) 
9.              reverse the current polarity of Oi; 
10.   output the current polarity of Oi; 
Figure 5: Identifying the Contextual Sentiment 
4.3 Experimental Evaluation 
Since features are detected prior to the senti-
ments, there is a possibility for an erroneous 
feature (i.e., a false positive feature) to be asso-
ciated with a sentiment. We thus conducted two 
different experiments. In the first case, we enu-
merate all extracted feature-sentiment pairs, 
including the wrong features. In the second sce-
nario, we enumerate the feature-sentiment pairs 
only for those correctly extracted features. For 
each experiment, we further evaluated the result 
with (C) and without (N.C.) contextual informa-
tion. 
We select the best case of feature detection 
and then run our sentiment identification algo-
rithm on the review dataset described in section 
3.3; the polarity thresholds V- and V+ are set to 
0.45 and 0.5, respectively. 
Table 5: Results for all features. 
Systems P (%) R (%) F (%) 
iMiner 
N.C. 57.07 58.21 57.63 
C. 70.3 71.72 71 
FBS 49.70 45.80 47.67 
Table 5 shows the results for all detected fea-
tures (correct and incorrect). As shown in line 2, 
our method achieved an F-measure of 57.63% 
without considering contextual information, 
while precision and recall are 57% and 58.21%, 
respectively. Adding contextual information, as 
line 3 shows, boosts the F-measure to 71%, a 
remarkable 13.37% improvement. 
Table 6 shows the results for just the correct-
ly extracted features. As shown in line 2, in the 
case of not considering contextual information, 
our method achieved an F-measure of 63.17%, 
while precision and recall were 69.05% and 
58.21%, respectively. By considering contextual 
information, line 3 shows that the F-measure 
improved to 77.82% which is 14.65% better, 
with precision and recall at 85.06% and 71.72%, 
respectively. The above results show that local 
contextual analysis of double and single nega-
tion can significantly improve the accuracy of 
sentiment orientation identification. 
Table 6: Results for correctly detected features. 
Systems P (%) R (%) F (%) 
iMiner 
N.C. 69.05 58.21 63.17 
C. 85.06 71.72 77.82 
FBS 62.45 45.80 52.84 
By examining the results shown in line 3 (in 
bold) of both Tables 5 and 6, the F-measure on 
correctly identified features increases from 71% 
to 77.82%, while the precision increases drasti-
cally from 70.3% to 85.06%. The results show 
that our two-step approach of identifying senti-
ment orientation is reasonable and effective and 
that a great many of sentiments can be identified 
correctly for related features, especially for 
those correctly detected one. However, in prac-
tice there is no way to tell the correctly identi-
fied features from the incorrect ones, thus Table 
5 is a more realistic gauge of our approach.. 
Lastly, we compared our approach to senti-
ment identification with FBS (see Table 3). The 
best results are used, as shown in the last rows 
of Table 5 and 6. When considering all features 
extracted, the F-measure of FBS is only 47.67%, 
which is 23.33% lower than that of iMiner, 
where both precision and recall are 49.70% and 
45.80%, respectively. Considering only the cor-
rectly detected features, iMiner widens its lead 
over FBS to 25% in terms of F-measure. 
There are several explanations for the poor 
results of FBS: (1) The inferior results of feature 
detection affect the subsequent task of sentiment 
identification; and (2) the polarity inference de-
pends heavily on a semantic dictionary Word-
Net. In our experiments for FBS, we used an 
extended version of the ??????? Thesau-
rus containing 77,492 words, and a sentiment 
lexicon with 8,856 words that is part of mini 
(free) HowNet, and lastly our seed word list 
containing 80 words. 
4.4 Sentiment Identification Error Analysis 
We classify our sentiment identification er-
rors into 5 main types, SE1 to SE5, as follows. 
SE1: Sentiment identification relies heavily 
on feature extraction, which means that if fea-
tures are detected wrongly, it is impossible for 
the sentiment identified to be correct. About 
49% of false sentiments are due to incorrectly 
extracted features. 
Even for the correctly extracted features, 
there are still several errors as listed below. 
SE2: Incorrectly identified opinion words can 
lead to mistakes in inferring sentiments, ac-
counting for 14% of the errors. 
SE3: Errors in detecting contextual informa-
tion about opinion words led to 12% of the 
wrong sentiment identification results. 
SE4: Both the quality and quantity of seed 
words influence sentiment identification. 
SE5: The threshold choices for V+ and V- di-
rectly impact the polarity inference of opinion 
words, affecting the sentiment identification. 
SE4 and SE5 errors account for the remaining 
25% of the erroneous sentiment results. 
5 Conclusion 
The main contribution of this paper is the pro-
posed systematic technique of identifying both 
features and sentiments for Chinese reviews. 
Our proposed approach compares very favora-
bly against the well-known FBS system on a 
small-scale dataset. Our feature detection is 7% 
better than FBS in terms of F-measure, with 
significantly higher recall. Meanwhile, our ap-
proach of identifying contextual sentiment 
achieved around 23% better F-measure than 
FBS. 
We plan to further explore effective methods 
to deal with the various feature and sentiment 
errors. In addition, we plan to explore the ex-
traction of implicit features, since a significant 
number of reviews express opinion via implicit 
features. Lastly, we plan to test out these im-
provements on a large-scale dataset. 
Acknowledgement 
We thank Harbin Institute of Technology?s Cen-
ter for Information Retrieval in providing their 
Language Technology Platform (LTP) software. 
This research was supported in part by Singa-
pore Ministry of Education?s Academic Re-
search Fund Tier 1 grant RG 30/09.  
References 
Dunning, T. E. 1993. Accurate methods for the statistics of 
surprise and coincidence. Computational Linguistics 
19(1). 
Fukumoto, Fumiyo, and Yoshimi Suzuki. 2000. Event 
Tracking based on Domain Dependence, SIGIR. 
Hu, Minqing, and Bing Liu. 2004. Mining and summariz-
ing customer reviews, SIGKDD, Seattle, WA, USA. 
Luhn, Hans Peter. 1957. A statistical approach tomecha-
nized encoding and searching of literary information. 
IBM Journal of Research and Development 1 (4):309-
17. 
Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment Classification using Ma-
chine Learning Techniques, EMNLP. 
Qiu, Guang, Kangmiao Liu, Jiajun Bu, Chun Chen, and 
Zhiming Kang. 2007. Extracting opinion topics for 
Chinese opinions using dependence grammar, 
ADKDD, California, USA. 
Qiu, Guang, Can Wang, Jiajun Bu, Kangmiao Liu, and 
Chun Chen. 2008. Incorporate the Syntactic Knowledge 
in Opinion Mining in User-generated Content, WWW, 
Beijing, China. 
Shi, Bin, and Kuiyu Chang. 2006. Mining Chinese Re-
views, ICDM Data Mining on Design and Marketing 
Workshop. 
Tesniere, L. 1959. Elements de Syntaxe Structurale: Li-
brairie C. Klincksieck, Paris. 
Turney, Peter D. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised Classi-
fication of Reviews, ACL, Philadelphia. 
Xia, Yunqing, Ruifeng Xu, Kam-Fai Wong, and Fang 
Zheng. 2007. The Unified Collocation Framework for 
Opinion Mining. International Conference on Machine 
Learning and Cybernetics. 
Yi, Jeonghee, Tetsuya Nasukawa, Razvan Bunescu, and 
Wayne Niblack. 2003. Sentiment Analyzer: Extracting 
Sentiments about a Given Topic using Natural Lan-
guage Processing Techniques, ICDM. 
Zhou, Chao, Guang Qiu, Kangmiao Liu, Jiajun Bu, Ming-
cheng Qu, and Chun Chen. 2008. SOPING : a Chinese 
customer review mining system, SIGIR, Singapore. 
Zhuang, Li, Feng Jing, and Xiaoyan Zhu. 2006. Movie 
Review Mining and Summarization, CIKM. 
