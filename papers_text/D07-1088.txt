Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 837?846, Prague, June 2007. c?2007 Association for Computational Linguistics 
Extracting Data Records from Unstructured Biomedical Full Text 
Donghui Feng       Gully Burns       Eduard Hovy 
Information Sciences Institute 
University of Southern California 
Marina del Rey, CA, 90292 
{donghui, burns, hovy}@isi.edu 
 
 
Abstract 
In this paper, we address the problem of 
extracting data records and their attributes 
from unstructured biomedical full text. 
There has been little effort reported on this 
in the research community. We argue that 
semantics is important for record extraction 
or finer-grained language processing tasks. 
We derive a data record template including 
semantic language models from unstruc-
tured text and represent them with a dis-
course level Conditional Random Fields 
(CRF) model. We evaluate the approach 
from the perspective of Information Extrac-
tion and achieve significant improvements 
on system performance compared with 
other baseline systems. 
1 Introduction 
The discovery and extraction of specific types of 
information, and its (re)structuring and storage into 
databases, are critical tasks for data mining, 
knowledge acquisition, and information integration 
from large corpora or heterogeneous resources 
(e.g., Muslea et al, 2001; Arasu and Garcia-
Molina, 2003). For example, webpages of products 
on Amazon may contain a list of data records such 
as books, watches, and electronics. Automatic 
extraction of individual records will facilitate the 
access and management of data resources. 
Most current approaches address this problem 
for structured or semi-structured text, for instance, 
from XML format files or lists and/or tabular data 
records on webpages (e.g., Liu et al, 2003; Zhu et 
al., 2006). The techniques applied rely strongly on 
the analysis of document structure derived from 
the webpage?s html tags (e.g., the DOM tree 
model). 
Regarding unstructured text, most Information 
Extraction (IE) work has focused on named entities 
(people, organizations, places, etc.). Such IE treats 
each extracted element as a separate record. Much 
less work has focused on the case where several 
related pieces of information have to be extracted 
to jointly comprise a single data record. In this 
work, it is usually assumed that there is only one 
record for each document (e.g., Kristjannson et al, 
2004). Almost no work tries to extract multiple 
data records from a single document. Multiple data 
records can be scattered across the narrative in free 
text. The problem becomes much harder as there 
are no explicit boundaries between data records 
and no heavily indicative format features (like html 
tags) to utilize. 
With the exponential increase of unstructured 
text resources (e.g., digitalized publications, papers 
and/or technical reports), knowledge needs have 
made it a necessity to explore this problem. For 
example, biomedical papers contain numerous ex-
periments and findings. But the large volume and 
rate of publication have made it infeasible to read 
through the articles and manually identify data re-
cords and attributes. 
We present a study to extract data records and 
attributes from the biomedical research literature. 
This is part of an effort to develop a Knowledge 
Base Management System to benefit neuroscience 
research. Specifically we are interested in knowl-
edge of various aspects (attributes) of Tract-tracing 
Experiments (TTE) (data records) in neuroscience. 
The goal of TTE experiments is to chart the inter-
connectivity of the brain by injecting tracer chemi-
cals into a region of the brain and identifying cor-
responding labeled regions where the tracer is 
837
  
Figure 1. An example of data records and attributes in a research article. 
taken up and transported to (Burns et al, 2007). 
To extract data records from the research litera-
ture, we need to solve two sub-problems: discover-
ing individual attributes of records and grouping 
them into one or more individual records, each re-
cord representing one TTE experiment. Each at-
tribute may contain a list of words or phrases and 
each record may contain a list of attributes.  
Listing each sentence from top to bottom, we 
call the first problem the Horizontal Problem (HP) 
and the second the Vertical Problem (VP). Figure 
1 provides an example of a TTE research article 
with colored fragments representing attributes and 
dashed frames representing data records. For in-
stance, the third dashed frame represents one ex-
periment record having three attributes with corre-
sponding biological interpretations: ?no labeled 
cells?, ?the DCN?, and ?the contralateral AVCN?. 
We view the HP and VP problems as two se-
quential labeling problems and describe our ap-
proach using two-level Conditional Random Fields 
(CRF) (Lafferty et al, 2001) models to extract data 
records and their attributes.  
The HP problem (finding individual attribute 
values) is solved using a sentence-level CRF label-
ing model that integrates a rich set of linguistic 
features. For the VP problem, we apply a dis-
course-level CRF model to identify individual ex-
periments (data records). This model utilizes deep 
semantic knowledge from the HP results (attribute 
labels within sentences) together with semantic 
language models and achieves significant im-
provements over baseline systems.  
This paper mainly focuses on the VP problem, 
since linguistic features for the HP problem is the 
general IE topic of much past research (e.g., Peng 
and McCallum, 2004). We apply various feature 
combinations to learn the most suitable and indica-
tive linguistic features. 
The remainder of this paper is organized as fol-
lows: in the next section we discuss related work. 
Following that, we present the approach to extract 
data records in Section 3. We give extensive ex-
perimental evaluations in Section 4 and conclude 
in Section 5. 
2 Related Work 
As mentioned, data record extraction has been 
extensively studied for structured and semi-
structured resources (e.g., Muslea et al, 2001; 
Arasu and Garcia-Molina, 2003; Liu et al, 2003; 
Zhu et al, 2006). Most of those approaches rely on 
the analysis of document structure (reflected in, for 
example, html tags), from which record templates 
are derived. However, this approach does not apply 
to unstructured text. The reason lies in the 
difficulty of representing a data record template in 
free text without formatting tags and integrating it 
838
 into a learning system. We show how to address 
this problem by deriving data record templates 
through language analysis and representing them 
with a discourse level CRF model. 
Given the problem of identifying one or more 
records in free text, it is natural to turn toward text 
segmentation. The Natural Language Processing 
(NLP) community has come up with various 
solutions towards topic-based text segmentation 
(e.g., Hearst, 1994; Choi, 2000; Malioutov and 
Barzilay, 2006). Most unsupervised text 
segmentation approaches work under optimization 
criteria to maximize the intra-segment similarity 
and minimize the inter-segment similarity based on 
word distribution statistics. However, this 
approach cannot be applied directly to data record 
extraction. A careful study of our corpus shows 
that data records share many words and phrases 
and are not distinguishable based on word 
similairties. In other words, different experiments 
(records) always belong to the same topic and there 
is no way to segment them using standard topic 
segmentation techniques (even if one views the 
problem as a finer-level segmentation than 
traditional text segmentation). In addition, most 
text segmentation approaches require a 
prespecified number of segments, which in our 
domain cannot be provided. 
(Wick et al, 2006) report extracting database re-
cords by learning record field compatibility. How-
ever, in our case, the field compatibility is hard to 
distinguish even by a human expert. Cluster-based 
or pairwise field similarity measures do not apply 
to our corpora without complex knowledge reason-
ing. Most of Wick et al?s data (faculty and stu-
dent?s homepages) contains one record. 
In addition, as explained below, we have found 
that surface word statistics alone are not sufficient 
to derive data record templates for extraction. 
Some (limited) form of semantic understanding of 
text is necessary. We therefore first perform some  
sentence level extraction (following the HP 
problem) and then integrate semantic labels and 
semantic language model features into a discourse 
level CRF model to represent the template for 
extracting data records in the future. 
Recently an increasing number of research ef-
forts on text mining and IE have used CRF models 
(e.g., Peng and McCallum, 2004). The CRF model 
provides a compact way to integrate different types 
of features when sequential labeling is important. 
Recent work includes improved model variants 
(e.g., Jiao et al, 2006; Okanohara et al, 2006) and 
applications such as web data extraction (Pinto et 
al., 2003), scientific citation extraction (Peng and 
McCallum, 2004), and word alignment (Blunsom 
and Cohn, 2006). But none of them have used 
CRFs for discourse level data record extraction. 
We use a CRF model to represent a data record 
template and integrate various knowledge as CRF 
features. Instead of traditional work on the sen-
tence level, our focus here is on the discourse level. 
As this has not been carefully explored, we ex-
periment with various selected features. 
For the biomedical domain, our work will facili-
tate biomedical research by supporting the con-
struction of Knowledge Base Management Sys-
tems (e.g., Stephan et al, 2001; Hahn et al, 2002; 
Burns and Cheng, 2006). Unlike the well-studied 
problem of relation extraction from biomedical 
text, our work focuses on grouping extracted at-
tributes across sentences into meaningful data re-
cords. TTE experiment is only one of many ex-
perimental types in biology. Our work can be gen-
eralized to many different types of data records to 
facilitate biology research. 
In the next section, we present our approach to 
extracting data records. 
3 Extracting Data Records 
Inspired by the idea of Noun Phrase (NP) chunking 
in a single sentence, we view the data records 
extraction problem as discourse chunking from a 
sequence of sentences using a sequential labeling 
CRF model. 
3.1 Sequential Labeling Model: CRF 
The CRF model addresses the problem of labeling 
sequential tokens while relaxing the strong 
independence assumptions of Hidden Markov 
Models (HMMs) and avoiding the presence of 
label bias from having few successor states. For 
each current state, we obtain the conditional 
probability of its output states given previously 
assigned values of input states. For most language 
processing tasks, this model is simply a linear-
chain Markov Random Fields model. 
In typical labeling processes using CRFs each 
token is viewed as a labeling unit. For our prob-
lem, we process each input document 
),...,,( 21 nsssD =  as a sequence of individual sen-
839
 tences, with a corresponding labeling sequence of 
labels, ),...,,( 21 nlllL = , so that each sentence corre-
sponds to only one label. In our problem, each data 
record corresponds to a distinct TTE experiment. 
Similar to NP chunking, we define three labels for 
sentences, ?B_REC? (beginning of record), 
?I_REC? (inside record), and ?O? (other). The de-
fault label ?O? indicates that this sentence is be-
yond our concern. 
The CRF model is trained to maximize the 
probability of )|( DLP , that is, given an input 
document D, we find the most probable labeling 
sequence L. The decision rule for this procedure is: 
)|(maxarg? DLPL
L
=                                        (1) 
A CRF model of the two sequences is character-
ized by a set of feature functions kf and their corre-
sponding weights k? . As in Markov fields, the 
conditional probability )|( DLP  can be computed 
using Equation 2. 
??
???
???=
= ?
T
t k
ttkk
S
tDllf
Z
DLP
1
1 ),,,(*exp
1
)|( ?        (2) 
where ),,,( 1 tDllf ttk ? is a feature function, represent-
ing either the state transition feature ),,( 1 Dllf ttk ?  or 
the feature of output state ),( Dlf tk given the input 
sequence. All these feature functions are user-
defined boolean functions. 
CRF works under the framework of supervised 
learning, which requires a pre-labeled training set 
to learn and optimize system parameters to maxi-
mize the probability or its log format. Equipped 
with this model, we investigate how to apply it and 
prepare features accordingly. 
3.2 Feature Preparation 
The CRF model provides a compact, unified 
framework to integrate features. However, unlike 
sentence-level processing, where features are very 
intuitive and circumscribed, it is not obvious what 
features are most indicative for our problem. We 
therefore explore three categories of features for 
discourse level chunking. 
3.2.1 Semantic Attribute Labels 
Most text segmentation approaches compute 
surface word similarity scores in given corpora 
without semantic analysis. However, in our case, 
data records have very similar characteristics and 
share most of the words. They are not 
distinguishable just from an analysis of surface 
word statistics. We have to understand the 
semantics before we can make decisions about data 
record extraction.  
In our case, we care about the four types of at-
tributes of each data record (one TTE experiment). 
Table 1 gives the definitions of the four attributes 
for each data record. 
Name Description 
injectionLocation the named brain region where the injection was made. 
tracerChemical the tracer chemical used. 
labelingLocation the region/location where the labeling was found. 
labelingDescription 
a description of labeling, in-
cluding label density or label 
type. 
Table 1. Attributes of data records (a TTE experiment). 
To obtain this semantic attributes information of 
individual sentences (the HP problem), we first 
apply another sentence-level CRF model to label 
each sentence. We consider five categories of fea-
tures based on language analysis. Table 2 shows 
the features for each category. 
Name Feature Description 
TOPOGRAPHY Is word topog-
raphic? 
BRAIN_REGION Is word a region 
name? 
TRACER Is word a tracer 
chemical? 
DENSITY Is word a den-
sity term? 
Lexicon 
Knowledge 
LABELING_TYPE Does word de-
note a labeling 
type? 
Surface 
Word 
Word Current word 
Context    
Window 
CONT-INJ If current word 
is within a win-
dow of injection 
context 
Prev-word Previous word Window 
Words Next-word Next word 
Root-form Root form of 
the word if dif-
ferent 
Gov-verb The governing 
verb 
Subject The sentence 
subject  
Dependency 
Features 
Object The sentence 
object 
Table 2. The features for labeling words. 
840
 a. Lexicon knowledge. We used names of brain 
structures taken from brain atlases (Swanson, 
2004), standard terms to denote neuro-
anatomical topographical relationships (e.g., 
?rostral?), the name or abbreviation of the 
tracer chemical used (e.g., ?PHAL?), and 
commonsense descriptions for descriptions of 
the labeling (e.g., ?dense?, ?light?).  
b. Surface and window word. The current 
word and the words around are important in-
dicators of the most probable label. 
c. Context window. The TTE is a description of 
the inject-label-findings process. Whenever a 
word having a root form of ?injection? or 
?deposit? appears, we generate a context 
window and all the words falling into this 
window are assigned a feature of ?CONT-
INJ?.  
d. Dependency features. We apply a depend-
ency parser MiniPar (Lin, 1998) to parse each 
sentence, and then derive four types of fea-
tures from the parsing result. These features 
are (a) root form of every word, (b) the sub-
ject within the sentence, (c) the object within 
the sentence, and (d) the governing verbs. 
The labeling system assigns a label for every to-
ken in each sentence. We achieved the best per-
formance with an F-score of 0.79 (based on a pre-
cision of 0.80 and a recall of 0.78). This is not the 
focus of this paper. Please refer to our previous 
work (Burns et al, 2007) for details. 
 
 
 
 
 
 
 
Figure 2. An example of semantic attribute labels. 
With the sentence-level understanding of each 
sentence, we obtain the semantic attribute labels 
for the data records. Figure 2 gives an example 
sentence with semantic attribute labels. Here 
<tracerChemical>, <labelingLocation>, and <la-
belingDescription> are recognized by the system, 
and the attribute names will be used as features for 
this sentence. 
3.2.2 Semantic Language Model 
Since text narratives might adhere to logical ways 
of expressing facts, language models for each sen-
tence will also provide good features to extract 
data records. However, in biomedical research arti-
cles many of the technical words/phrases used in 
the narrative are repeated across experiments, mak-
ing the surface word language model of little use in 
deriving generalized data record templates. Con-
sidering this, we replace in each sentence the la-
beled fragments with their attribute labels and then 
derive semantic language models from that format. 
By ?semantic language model? we therefore mean 
a combination of semantic labels and surface 
words.  
For example, in the sentence shown in Figure 2, 
we have the semantic language model trigrams 
location-of-<tracerChemical>, sites-in-
<injectionLocation>, and <labelingDescription>-
followed-the. In addition, we also query WordNet 
for the root form of each word to generalize the 
semantic language models. This for example pro-
duces the semantic language model trigrams site-
in-<injectionLocation> and <labelingDescription>-
follow-the. 
We believe the collected semantic language 
models represent an inherent structure of unstruc-
tured data records. By integrating them as features 
with a CRF model, we expect to represent data re-
cord templates and use the learned model to extract 
new data records.  
However, it is not clear what semantic language 
models are most indicative and useful. A bag-of-
words (language models) approach may bring 
much noise in. We show below a comparison of 
regular language models and semantic language 
models in evaluations.  
3.2.3 Layout and Word Heuristics 
The previous two categories of features come from 
the discovery of semantic components of sentences 
and their narrative form word analysis. When in-
terviewing the neuroscience expert annotator, we 
learned that some layout and word level heuristics 
may also help to delineate individual data records. 
Table 3 gives the two types of heuristic features. 
When a sentence contains heuristic words, it 
will be assigned to a word heuristic feature. If the 
sentence is at the boundary of a paragraph, it will 
be assigned a layout heuristic feature, namely the 
first or the last sentence in the paragraph.  
<SENT FILE="1995-360-213-ns.xml" INDEX= "63"> 
Regardless of the precise location of <tracerChemical> 
PHAL </tracerChemical> injection sites in <injectionLo-
cation> the MEA </injectionLocation> , <labelingDe-
scription> labeled axons </labelingDescription> followed 
the same basic routes . 
</SENT> 
841
 Name Feature Descrip-tion 
EXP_B_WORD 
INJECT 
CASE 
EXPERIMENT 
APPLICATION 
DEPOSIT 
PLACEMENT 
INTRODUCTION 
Heuristic 
words for 
beginning 
of an ex-
periment 
descrip-
tion 
POS_IN_PARA FIRST_IN_PARA 
LAST_IN_PARA 
Position of 
the sen-
tence in 
the para-
graph 
Table 3. The heuristic features. 
4 Empirical Evaluation 
To evaluate the effectiveness and performance of 
our technique, we conducted extensive experi-
ments to measure the data record extraction ap-
proach. 
4.1 Experimental Setup 
We used the machine learning package MALLET 
(McCallum, 2002) to conduct the CRF model 
training and labeling. 
We have obtained the digital publications of 
9474 Journal of Comparative Neurology (JCN)1 
articles from 1982 to 2005. We have converted the 
PDF format into plain text, maintaining paragraph 
breaks (some errors still occur though).  A simple 
heuristic based approach identifies semantic sec-
tions of the paper (e.g, Introduction, Results, Dis-
cussion). As most experimental descriptions appear 
in the Results section, we only process the Results 
section. A neuroscience expert manually annotated 
the data records in the Results section of 58 re-
search articles. The total number of sentences in 
the Results section of the 58 files is 6630 (averag-
ing 114.3 sentences per article). 
 Training Set Testing Set 
Docs 39 19 
Data Records 249 133 
Table 4. Experiment configuration. 
We randomly divided this material into training 
and testing sets under a 2:1 ratio, giving 39 docu-
ments in the training set and 19 in the testing set. 
                                                 
1 http://www3.interscience.wiley.com/cgi-bin/jhome/31248 
Table 4 gives the numbers of documents and data 
records in the training and the testing set. 
4.2 Evaluation Metrics 
To evaluate data record extraction, we notice it is 
not fair to strictly evaluate the boundaries of data 
records because this does not penalize the near-
miss and false positive of data records in a reason-
able way; sentences near a boundary that contain 
no relevant record information can be included or 
omitted without affecting the results. Hence the 
standard Pk (Beeferman et al, 1997) and WinDiff 
(Pevzner and Hearst, 2002) measures for text seg-
mentation are not so suitable for our task. 
As we are concerned with the usefulness of 
knowledge in extracted data records, we instead 
evaluate from the perspective of IE. We measure 
system performance on the quality of the extracted 
data records. For each extracted data record, it will 
be aligned to one of the data records in the gold 
standard using the ?dominance rule? (if the data 
record can be aligned to multiple records in the 
gold standard, it will be aligned to the one with 
highest overlap). Then we evaluate the precision, 
recall, and F1 scores of extracted units of the data 
record. The units are the attributes in data records. 
system by the units extracted  theof #
unitscorrect   # of
precision =   (3) 
standard gold in the units  theof #
 unitscorrect   # of
recall =                (4) 
ecallrprecision
recall*precision
F +=
*2
1                                    (5) 
These measures provide an indication of the 
completeness and correctness of each extracted 
record (experiment). We also measure the number 
of distinct records extracted, compared with the 
gold standard as appearing in the document. 
4.3 Experiment Results 
To fully compare the effectiveness of our semantic 
analysis functionality, we evaluated system per-
formance for all the following systems:  
TextTiling (TT): To compare with text segmen-
tation techniques, we use TextTiling (Hearst, 1994) 
with default parameters as the first baseline sys-
tem. 
Random Guess (RG): In order to demonstrate 
the data balance of all the possible labels in the 
testing set, we also use another baseline system 
with random decisions for each sentence.  
842
 Domain Heuristics (DH): In a regular TTE ex-
periment, only one tracer chemical will typically 
be used. Given this heuristic, we assume each data 
record contains one tracer chemical. In this system, 
we first locate sentences with identified trace 
chemicals, and then we greedily expand backward 
and forward until another new tracer chemical ap-
pears or no other attribute is included. 
Surface Text (ST): To measure the effective-
ness of the semantic analysis (attribute labels and 
semantic language models), the ST system utilizes 
only standard surface word language models and 
heuristic features. 
Semantic Analysis (SEM): The SEM system 
uses all the semantic features available (including 
identified attributes and semantic language models) 
and two heuristic features. 
Table 5 shows the final performance of these 
different systems. The second column provides the 
numbers of extracted data records. In this task, a 
larger number does not necessarily mean a better 
system, as a system might produce too many false 
positives. The remaining three columns represent 
the precision, recall, and F1 scores, averaged over 
all data records. With our approach, the system 
performance is significantly improved compared 
with other systems. System TT fails in this task as 
it only outputs the full document as one single re-
cord. 
 # of     
Records 
Prec. Rec. F1 
TT 19 0.3861 1.0 0.5571 
RG 758 0.6331 0.0913 0.1595 
DH 162 0.6703 0.4902 0.5663 
ST 82 0.8182 0.8339 0.8260 
SEM 72 0.8505 0.9258 0.8865 
Table 5. System performance. 
To investigate how plain text language models 
and semantic language models affect system per-
formance, we also experimented with all the lan-
guage models. Table 6 shows comparisons of three 
types of language models. Systems with semantic 
analysis always work better than those with only 
surface text analysis. Without semantic analysis, 
unigram features work better than bigram and tri-
gram features. This matches our intuition: without 
generalizing to semantic language models, higher 
order language models will be relatively sparse and 
contain much noise. However, when taking into 
account the semantic features, we found that bi-
gram and trigram semantic language model fea-
tures outperformed unigrams. They are especially 
important in boosting the recall scores as they cap-
ture more generalized information when derived. 
Unigram (%) Bigram (%) Trigram (%)  
Prec/Rec/F1 Prec/Rec/F1 Prec/Rec/F1 
ST 81.8/83.4/82.6 69.1/88.4/77.6 57.9/88.8/70.1 
SEM 85.1/86.6/85.6 85.1/92.6/88.7 82.2/92.7/87.1 
Table 6. Language model comparisons. 
As an example, Table 7 gives a list of high qual-
ity bigram semantic language models ranked by 
their information gains based on the training data. 
through_<labelingLocation> rat_no 
<labelingDescription>_be of_<tracerChemical> 
<labelingLocation>_( <tracerChemical>_be 
<tracerChemical>_injection be_inject 
into_<injectionLocation> be_center 
<labelingDescription>_from inject_with 
<tracerChemical>_in injection_of 
in_<labelingLocation> in_experiment 
Table 7. An example list of top-ranked bigrams. 
The main difficulty for data record extraction 
from unstructured text lies in deriving and repre-
senting a template for future extraction. We actu-
ally take advantage of CRF and represent the tem-
plate with a CRF model.  
Each data record is measured with precision, re-
call, and F1 scores. Figure 3 depicts the distribu-
tion of extracted data records according to these 
measures in the best system. 
Distribution
0
5
10
15
20
25
30
35
40
45
50
55
60
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Performance
# 
of
 e
xt
ra
ct
ed
 r
ec
or
ds
Prec
Rec
F1
 
Figure 3. Data records performance distribution. 
The results are encouraging, especially given the 
complexity and flexibility of data record descrip-
tions in the unstructured text. In Figure 3, Axis X 
843
 represents the value interval for precision, recall, 
and F1, and Axis Y represents the number of ex-
tracted records with their corresponding values. 
For example, 57 records have recall scores falling 
into [0.9, 1.0].  
Figure 4 gives an example alignment between 
system result and the gold standard. Each record is 
represented by a range of sentences. The numbers 
following each record in the system result are indi-
vidual data record?s precision and recall scores. 
          System                                   Gold 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4. An example of record extraction in one doc. 
This is a real example from the testing set. For 
records R1, R3, and R6, the system can extract the 
exact sentences contained. For record R2 and R5, 
although they do not exactly match at the sentence 
level, the extracted record contains the entire re-
quired set of attributes as in the gold standard.  
4.4 Error Analysis and Discussion 
When we investigated the errors, we found that 
sometimes the extracted data records combined 
two or more smaller gold standard records, or vice 
versa. As shown in Figure 4, extracted records R4 
and R7 are both combinations of records in the 
gold standard. This is partially due to the granular-
ity definition problem. Authors may mention sev-
eral approaches/symptoms to one type of experi-
ment for a single purpose. In this case, it is almost 
infeasible to have annotators strictly agree on 
granularity and thus to teach the system to acquire 
this knowledge. For example, in the gold standard, 
the annotator annotated three successive sentences 
as three separate records but the system output 
those as only one data record. In this extreme case, 
it is too hard to expect the system to perform well. 
In our approach, the semantic attribute labels 
and semantic language models require the result of 
the initial sentence-level labeling, which has an F-
score of 0.79. The error may propagate into the 
data record extraction procedure and lower overall 
system performance. 
In our current experiments, we also assume all 
the attributes within one segment belong to one 
record. However, the situation of embedded data 
records will make this problem harder. For exam-
ple, authors sometimes compare the current ex-
periment with other approaches in referenced pa-
pers. In this case, those attributes should be ex-
cluded from the records. We need to invent rules or 
constraints to filter them out. When such reference 
occurs at experiment boundaries, it brings higher 
risk for correct results.  
It is a very hard problem to extract from unstruc-
tured text neat structured records. The annotators 
sometimes employ background knowledge or rea-
soning when performing manual extraction; such 
knowledge cannot today be easily modeled and 
integrated into learning systems.  
In our study, we also compared some feature se-
lection approaches. Similar to (Yang and Pedersen, 
1997), we tried Feature Instance Frequency, Mu-
tual Information, Information Gain, and CHI-
square test. But we eventually found that the sys-
tem including all the features worked best, and 
with all the other configurations unchanged, fea-
ture instance frequency worked at almost the same 
level as other complex measures such as mutual 
information and information gain.  
5 Conclusion and Future Work 
In this paper, we explored the problem of extract-
ing data records from unstructured text. The lack 
of structure makes it difficult to derive meaningful 
objects and their values without resorting to deeper 
language analysis techniques. We derived indica-
tive linguistic features to represent data record 
templates in free text, using a two-pass approach in 
which the second pass used the IE labels derived 
from the first to compose attributes into coherent 
data records. We evaluated the results from an IE 
perspective and reported potential problems of er-
ror generation. 
? 
R1:S12~S29 (1.0/1.0) 
? 
R2: S31~S41 (1.0/1.0) 
 
R3: S42~S52 (1.0/1.0) 
? 
R4: S56~S73 
(0.517/1.0) 
? 
R5: S75~S88 (1.0/1.0) 
? 
R6: S91~S106(1.0/1.0) 
? 
R7: S108~S118 
(0.523/1.0)  
? 
? 
R1': S12~S29 
? 
R2': S31~S40  
? 
R3': S42~S52 
? 
R4': S56~S63 
? 
R5': S65~S73 
R6': S74~S88 
.. 
R7': S91~S106 
? 
R8': S108~S114 
R9': S115~S118 
? 
844
 For the future, we plan to explore additional fea-
ture types and feature selection strategies to deter-
mine what is ?good? for unstructured record tem-
plates to improve our results. More effort will also 
be put into the sentence-level analysis to reduce 
error propagations. In addition, ontology based 
knowledge inference strategies might be useful to 
validate attributes in single record and in turn help 
data record extraction. The last thing under our 
direction is to explore new models if applicable.  
We hope this thought-provoking problem will 
attract more attention from the community. In the 
future, we plan to make our corpus available to the 
community. The solution to this problem will 
highly affect the access of knowledge in large scale 
unstructured text corpora. 
Acknowledgements 
The work was supported in part by an ISI seed 
funding, and in part by a grant from the National 
Library of Medicine (RO1 LM07061). The authors 
want to thank Feng Pan for his helpful suggestions 
with the manuscript. We would also like to thank 
the anonymous reviewers for their valuable com-
ments. 
References 
Arasu, A., and Garcia-Molina, H. 2003. Extracting 
structured data from web pages. In Proc. of SIMOD-
2003.  
Beeferman, D., Berger, A., and Lafferty, J. 1997. Text 
segmentation using exponential models. In Proc. of 
EMNLP-1997.  
Blunsom, P. and Cohn, T. 2006. Discriminative word 
alignment with conditional random fields. In Proc. of 
ACL-2006.  
Brazma, A., et al, 2001. Minimum information about a 
microarray experiment (MIAME)-toward standards 
for microarray data. Nat Genet, 29(4): p. 365-71.  
Burns, G.A. and Cheng, W.-C. 2006. Tools for knowl-
edge acquisition within the NeuroScholar system and 
their application to anatomical tract-tracing data. In 
Journal of Biomedical Discovery and Collaboration.  
Burns, G., Feng, D., and Hovy, E.H. 2007. Intelligent 
Approaches to Mining the Primary Research Litera-
ture: Techniques, Systems, and Examples. Book 
Chapter in Computational Intelligence in Bioinfor-
matics, Springer-Verlag, Germany. 
Choi, F. Y. Y. 2000. Advances in domain independent 
linear text segmentation. In Proc. of NAACL-2000.  
Hahn, U., Romacher, M., and Schulz, S. 2002. Creating 
knowledge repositories from biomedical reports the 
MEDSYNDIKATE text mining system. In Proc. of 
PSB-2002. 
Hearst, M. 1994. Multi-paragraph segmentation of ex-
pository text. In Proc. of ACL-1994.  
Jiao, F., Wang, S., Lee, C., Greiner, R., and 
Schuurmans, D. 2006. Semi-supervised conditional 
random fields for improved sequence segmentation 
and labeling. In Proc. of ACL-2006.  
Kristjannson, T., Culotta, A. Viola, P., and McCallum, 
2004. A. Interactive information extraction with con-
strained conditional random fields. In Proc. of AAAI-
2004. 
Lafferty, J., McCallum, A. and Pereira, F. 2001 Condi-
tional Random Fields: probabilistic models for seg-
menting and labeling Sequence Data. In Proc. of 
ICML-2001. 
Lin, D. 1998. Dependency-based evaluation of MINI-
PAR. In Proc. of Workshop on the Evaluation of 
Parsing Systems.  
Liu, B., Grossman, R., and Zhai, Y. 2003. Mining data 
records in web pages. In Proc. of SIGKDD-2003.  
Malioutov, I. and Barzilay, R. 2006. Minimum cut 
model for spoken lecture segmentation. In Proc. of 
ACL-2006.  
McCallum, A.K. 2002. MALLET: A Machine Learning 
for Language Toolkit. http://mallet.cs.umass.edu. 
Muslea, I., Minton, S., and Knoblock, C.A. 2001. 
Hierarchical wrapper induction for semistructured 
information sources. Autonomous Agents and Multi-
Agent Systems 4:93-114. 
Okanohara, D., Miyao, Y., Tsuruoka, Y., and Tsujii, J. 
2006. Improving the scalability of semi-markov con-
ditional random fields for named entity recognition. 
In Proc. of ACL-2006.  
Peng, F. and McCallum, A. 2004. Accurate information 
extraction from research papers using conditional 
random fields. In Proc. of HLT-NAACL-2004.  
Pevzner, L., and Hearst, M. 2002. A Critique and Im-
provement of an Evaluation Metric for Text Segmen-
tation. Computational Linguistics. 
Pinto, D., A. McCallum, X. Wei, and W.B. Croft. 2003. 
Table Extraction Using Conditional Random Fields. 
In Proc. of SIGIR-2003.  
845
 Stephan, K.E. et al, 2001. Advanced database method-
ology for the Collation of Connectivity data on the 
Macaque brain (CoCoMac). Philos Trans R Soc Lond 
B Biol Sci, 356(1412).  
Swanson, L.W. 2004. Brain Maps: Structure of the Rat 
Brain. 3rd edition, Elsevier Academic Press.  
Wick, M., Culotta, A., and McCallum, A. 2006. Learn-
ing field compatibilities to extract database records 
from unstructured text. In Proc. of EMNLP-2006. 
Yang, Y., and Pedersen, J. 1997. A comparative study 
on feature selection in text categorization. In Proc. of 
ICML-1997, pp. 412-420.  
Zhu, J., Nie, Z., Wen, J., Zhang, B., and Ma, W. 2006. 
Simultaneous record detection and attribute labeling 
in web data extraction. In Proc. of KDD-2006. 
846
