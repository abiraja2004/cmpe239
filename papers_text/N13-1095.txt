Proceedings of NAACL-HLT 2013, pages 777?782,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Distant Supervision for Relation Extraction
with an Incomplete Knowledge Base
Bonan Min, Ralph Grishman, Li Wan
New York University
New York, NY 10003
{min,grishman,wanli}
@cs.nyu.edu
Chang Wang, David Gondek
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598
{wangchan,dgondek}
@us.ibm.com
Abstract
Distant supervision, heuristically labeling a
corpus using a knowledge base, has emerged
as a popular choice for training relation ex-
tractors. In this paper, we show that a sig-
nificant number of ?negative? examples gen-
erated by the labeling process are false neg-
atives because the knowledge base is incom-
plete. Therefore the heuristic for generating
negative examples has a serious flaw. Building
on a state-of-the-art distantly-supervised ex-
traction algorithm, we proposed an algorithm
that learns from only positive and unlabeled
labels at the pair-of-entity level. Experimental
results demonstrate its advantage over existing
algorithms.
1 Introduction
Relation Extraction is a well-studied problem
(Miller et al, 2000; Zhou et al, 2005; Kambhatla,
2004; Min et al, 2012a). Recently, Distant Super-
vision (DS) (Craven and Kumlien, 1999; Mintz et
al., 2009) has emerged to be a popular choice for
training relation extractors without using manually
labeled data. It automatically generates training ex-
amples by labeling relation mentions1 in the source
corpus according to whether the argument pair is
listed in the target relational tables in a knowledge
base (KB). This method significantly reduces human
efforts for relation extraction.
The labeling heuristic has a serious flaw. Knowl-
edge bases are usually highly incomplete. For exam-
1An occurrence of a pair of entities with the source sentence.
ple, 93.8% of persons from Freebase2 have no place
of birth, and 78.5% have no nationality (section 3).
Previous work typically assumes that if the argument
entity pair is not listed in the KB as having a re-
lation, all the corresponding relation mentions are
considered negative examples.3 This crude assump-
tion labeled many entity pairs as negative when in
fact some of their mentions express a relation. The
number of such false negative matches even exceeds
the number of positive pairs, by 3 to 10 times, lead-
ing to a significant problem for training. Previous
approaches (Riedel et al, 2010; Hoffmann et al,
2011; Surdeanu et al, 2012) bypassed this problem
by heavily under-sampling the ?negative? class.
We instead deal with a learning scenario where we
only have entity-pair level labels that are either posi-
tive or unlabeled. We proposed an extension to Sur-
deanu et al (2012) that can train on this dataset. Our
contribution also includes an analysis on the incom-
pleteness of Freebase and the false negative match
rate in two datasets of labeled examples generated
by DS. Experimental results on a realistic and chal-
lenging dataset demonstrate the advantage of the al-
gorithm over existing solutions.
2 Related Work
Distant supervision was first proposed by Craven
and Kumlien (1999) in the biomedical domain.
2Freebase is a large collaboratively-edited KB. It is available
at http://www.freebase.com.
3There are variants of labeling heuristics. For example, Sur-
deanu et al (2011) and Sun et al (2011) use a pair < e, v >
as a negative example, when it is not listed in Freebase, but e is
listed with a different v?. These assumptions are also problem-
atic in cases where the relation is not functional.
777
Since then, it has gain popularity (Mintz et al, 2009;
Bunescu and Mooney, 2007; Wu and Weld, 2007;
Riedel et al, 2010; Hoffmann et al, 2011; Sur-
deanu et al, 2012; Nguyen and Moschitti, 2011).
To tolerate noisy labels in positive examples, Riedel
et al (2010) use Multiple Instance Learning (MIL),
which assumes only at-least-one of the relation men-
tions in each ?bag? of mentions sharing a pair of ar-
gument entities which bears a relation, indeed ex-
presses the target relation. MultiR (Hoffmann et
al., 2011) and Multi-Instance Multi-Label (MIML)
learning (Surdeanu et al, 2012) further improve it
to support multiple relations expressed by different
sentences in a bag. Takamatsu et al (2012) mod-
els the probabilities of a pattern showing relations,
estimated from the heuristically labeled dataset.
Their algorithm removes mentions that match low-
probability patterns. Sun et al (2011) and Min et
al. (2012b) also estimate the probablities of patterns
showing relations, but instead use them to relabel ex-
amples to their most likely classes. Their approach
can correct highly-confident false negative matches.
3 Problem Definition
Distant Supervision: Given a KB D (a collection
of relational tables r(e1, e2), in which r?R (R is the
set of relation labels), and < e1, e2 > is a pair of
entities that is known to have relation r) and a cor-
pus C, the key idea of distant supervision is that we
align D to C, label each bag4 of relation mentions
that share argument pair < e1, e2 > with r, other-
wise OTHER. This generates a dataset that has labels
on entity-pair (bag) level. Then a relation extractor
is trained with single-instance learning (by assum-
ing all mentions have the same label as the bag), or
Multiple-Instance Learning (by assuming at-least-
one of the mentions expresses the bag-level label),
or Multi-Instance Multi-Label learning (further as-
suming a bag can have multiple labels) algorithms.
All of these works treat the OTHER class as exam-
ples that are labeled as negative.
The incomplete KB problem: KBs are usually
incomplete because they are manually constructed,
and it is not possible to cover all human knowledge
4A bag is defined as a set of relation mentions sharing the
same entity pair as relation arguments. We will use the terms
bag and entity pair interchangeably in this paper.
nor stay current. We took frequent relations, which
involve an entity of type PERSON, from Freebase
for analysis. We define the incompleteness ?(r) of a
relation r as follows:
?(r) = |{e}|?|{e|?e?,s.t.r(e,e?)?D}||{e}|
?(r) is the percentage of all persons {e} that do
not have an attribute e? (with which r(e, e?) holds).
Table 1 shows that 93.8% of persons have no place
of birth, and 78.5% of them have no nationality.
These are must-have attributes for a person. This
shows that Freebase is highly incomplete.
Freebase relation types Incompleteness
/people/person/education 0.792
/people/person/employment history 0.923
/people/person/nationality* 0.785
/people/person/parents* 0.988
/people/person/place of birth* 0.938
/people/person/places lived* 0.966
Table 1: The incompleteness of Freebase (* are must-
have attributes for a person).
We further investigate the rate of false negative
matches, as the percentage of entity-pairs that are
not listed in Freebase but one of its mentions gen-
erated by DS does express a relation in the tar-
get set of types. We randomly picked 200 unla-
beled bags5 from each of the two datasets (Riedel
et al, 2010; Surdeanu et al, 2012) generated by DS,
and we manually annotate all relation mentions in
these bags. The result is shown in Table 2, along
with a few examples that indicate a relation holds in
the set of false negative matches (bag-level). Both
datasets have around 10% false negative matches in
the unlabeled set of bags. Taking into considera-
tion that the number of positive bags and unlabeled
bags are highly imbalanced (1:134 and 1:37 in the
Riedel and KBP dataset respectively, before under-
sampling the unlabeled class), the number of false
negative matches are 11 and 4 times the number
of positive bags in Reidel and KBP dataset, respec-
tively. Such a large ratio shows false negatives do
have a significant impact on the learning process.
4 A semi-supervised MIML algorithm
Our goal is to model the bag-level label noise,
caused by the incomplete KB problem, in addition
585% and 95.7% of the bags in the Riedel and KBP datasets
have only one relation mention.
778
Dataset
(train-
ing)
# pos-
itive
bags
# positive :
# unlabeled
% are
false
negatives
# positive
: # false
negative
has human
assessment
Examples of false negative mentions
Riedel 4,700 1:134(BD*) 8.5% 1:11.4 no
(/location/location/contains)... in Brooklyn ?s Williamsburg.
(/people/person/place lived) Cheryl Rogowski , a farmer from
Orange County ...
KBP 183,062 1:37(BD*) 11.5% 1:4 yes
(per:city of birth) Juan Martn Maldacena (born September
10, 1968) is a theoretical physicist born in Buenos Aires
(per:employee of)Dave Matthews, from the ABC News, ...
Table 2: False negative matches on the Riedel (Riedel et al, 2010) and KBP dataset (Surdeanu et al, 2012). All
numbers are on bag (pairs of entities) level. BD* are the numbers before downsampling the negative set to 10% and
5% in Riedel and KBP dataset, respectively.
to modeling the instance-level noise using a 3-layer
MIL or MIML model (e.g., Surdeanu et al (2012)).
We propose a 4-layer model as shown in Figure 1.
The input to the model is a list of n bags with a
vector of binary labels, either Positive (P), or Un-
labled (U) for each relation r. Our model can be
viewed as a semi-supervised6 framework that ex-
tends a state-of-the-art Multi-Instance Multi-Label
(MIML) model (Surdeanu et al, 2012). Since the
input to previous MIML models are bags with per-
relation binary labels of either Positive (P) or Neg-
ative (N), we add a set of latent variables ? which
models the true bag-level labels, to bridge the ob-
served bag labels y and the MIML layers. We con-
sider this as our main contribution to the model. Our
hierarchical model is shown in Figure 1.
Figure 1: Plate diagram of our model.
Let i, j be the index in the bag and mention level,
respectively. Following Surdeanu et al (2012), we
model mention-level extraction p(zrij |xij ;wz) and
multi-instance multi-label aggregation p(?ri |zi;wr?)
in the bottom 3 layers. We define:
? r is a relation label. r?R ? {OTHER}, in
which OTHER denotes no relation expressed.
? yri ?{P,U}: r holds for ith bag or the bag is
unlabeled.
6We use the term semi-supervised because the algorithm
uses unlabeled bags but existing solutions requires bags to be
labeled either positive or negative.
? ?ri ?{P,N}: a hidden variable that denotes
whether r holds for the ith bag.
? ? is an observed constant controlling the total
number of bags whose latent label is positive.
We define the following conditional probabilities:
? p(yri |?ri ) =
?
?
?
?
?
?
?
1/2 if yri = P ? ?ri = P ;
1/2 if yri = U ? ?ri = P ;
1 if yri = U ? ?ri = N ;
0 otherwise ;
It encodes the constraints between true bag-
level labels and the entity pair labels in the KB.
? p(?|?) ? N (
?n
i=1
?
r?R ?(?ri ,P )
n , 1k ) where
?(x, y) = 1 if x = y, 0 otherwise. k is a large
number. ? is the fraction of the bags that are
positive. It is an observed parameter that de-
pends on both the source corpus and the KB
used.
Similar to Surdeanu et al (2012), we also define
the following parameters and conditional probabili-
ties (details are in Surdeanu et al (2012)):
? zij?R ? {OTHER}: a latent variable that de-
notes the relation type of the jth mention in the
ith bag.
? xij is the feature representation of the jth rela-
tion mention in the ith bag. We use the set of
features in Surdeanu et al (2012).
? wz is the weight vector for the multi-class rela-
tion mention-level classifier.
? wr? is the weight vector for the rth binary top-
level aggregation classifier (from mention la-
bels to bag-level prediction). We usew? to rep-
resent w1? ,w2? , ...w
|R|
? .
? p(?ri |zi;wr?) ? Bern(f?(wr? , zi)) where f? is
probability produced by the rth top-level clas-
sifier, from the mention-label level to the bag-
label level.
? p(zrij |xij ;wz) ? Multi(fz(wz,xij)) where fz
779
is probability produced by the mention-level
classifier, from the mentions to the mention-
label level.7
4.1 Training
We use hard Expectation-Maximization (EM) algo-
rithm for training the model. Our objective function
is to maximize log-likelihood:
L(wz,w?) = logp(y, ?|x;wz,w?)
= log
?
?
p(y, ?, ?|x;wz,w?)
Since solving it exactly involves exploring an expo-
nential assignment space for ?, we approximate and
iteratively set ?? = arg? max p(?|y, ?,x;wz,w?)
p(?|y, ?,x;wz,w?) ? p(y, ?, ?|x;wz,w?)
= p(y, ?|?,x)p(?|x;wz,w?)
= p(y|?)p(?|?)p(?|x;wz,w?)
Rewriting in log form:
logp(?|y, ?,x;wz,w?)
= logp(y|?) + logp(?|?) + logp(?|x;wz,w?)
=
n
?
i=1
?
r?R
logp(yri |?ri )? k(
n
?
i=1
?
r?R
?(?ri , P )
n ? ?)
2
+
n
?
i=1
?
r?R
logp(?ri |xi;wz,w?) + const
Algorithm 1 Training (E-step:2-11; M-step:12-15)
1: for i = 1, 2 to T do
2: ?ri ? N for all yri = U and r?R
3: ?ri ? P for all yri = P and r?R
4: I = {< i, r > |?ri = N}; I ? = {< i, r > |?ri = P}
5: for k = 0, 1 to ?n? |I ?| do
6: < i?, r? >= argmax<i,r>?I p(?ri |xi;wz,w?)
7: ?r?i? ? P ; I = I\{< i?, r? >}
8: end for
9: for i = 1, 2 to n do
10: z?i = argmaxzi p(zi|?i,xi;wz,w?)
11: end for
12: w?z = argmaxwz
?n
i=1
?|xi|
j=1 logp(zij |xij ,wz)
13: for all r?R do
14: w
r(?)
? = argmaxwr?
?n
i=1 p(?ri |zi,wr?)
15: end for
16: end for
17: return wz,w?
7All classifiers are implemented with L2-regularized logistic
regression with Stanford CoreNLP package.
In the E-step, we do a greedy search (steps 5-8
in algorithm 1) in all p(?ri |xi;wz,w?) and update ?ri
until the second term is maximized. wz , w? are the
model weights learned from the previous iteration.
After fixed ?, we seek to maximize:
logp(?|xi;wz,w?) =
n
?
i=1
logp(?i|xi;wz,w?)
=
n
?
i=1
log
?
zi
p(?i, zi|xi;wz,w?)
which can be solved with an approxi-
mate solution in Surdeanu et al (2012)
(step 9-11): update zi independently with:
z?i = argmaxzi p(zi|?i,xi;wz,w?). More details
can be found in Surdeanu et al (2012).
In the M-step, we retrain both of the mention-
level and the aggregation level classifiers.
The full EM algorithm is shown in algorithm 1.
4.2 Inference
Inference on a bag xi is trivial. For each mention:
z?ij = argzij?R?{OTHER} max p(zij |xij ,wz)
Followed by the aggregation (directly with w?):
yr(?)i = argyri ?{P,N} max p(y
r
i |zi;wr?)
4.3 Implementation details
We implement our model on top of the
MIML(Surdeanu et al, 2012) code base.8 We
use the same mention-level and aggregate-level
feature sets as Surdeanu et al (2012). We adopt
the same idea of using cross validation for the E
and M steps to avoid overfitting. We initialize our
algorithm by sampling 5% unlabeled examples as
negative, in essence using 1 epoch of MIML to
initialize. Empirically it performs well.
5 Experiments
Data set: We use the KBP (Ji et al, 2011)
dataset9 prepared and publicly released by Surdeanu
et al (2012) for our experiment since it is 1) large
and realistic, 2) publicly available, 3) most im-
portantly, it is the only dataset that has associated
human-labeled ground truth. Any KB held-out eval-
uation without manual assessment will be signif-
icantly affected by KB incompleteness. In KBP
8Available at http://nlp.stanford.edu/software/mimlre.shtml
9Available from Linguistic Data Consortium (LDC).
http://projects.ldc.upenn.edu/kbp/data/
780
Figure 2: Performance on the KBP dataset. The figures on the left, middle and right show MIML, Hoffmann, and
Mintz++ compared to the same MIML-Semi curve, respectively. MIML-Semi is shown in red curves (lighter curves in
black and white) while other algorithms are shown in black curves (darker curves in black and white).
dataset, the training bags are generated by mapping
Wikipedia (http://en.wikipedia.org) infoboxes (after
merging similar types following the KBP 2011 task
definition) into a large unlabeled corpus (consisting
of 1.5M documents from the KBP source corpus and
a complete snapshot of Wikipedia). The KBP shared
task provided 200 query named entities with their as-
sociated slot values (in total several thousand pairs).
We use 40 queries as development dataset (dev), and
the rest (160 queries) as evaluation dataset. We set
? = 0.25 by tuning on the dev set and use it in the
experiments. For a fair comparison, we follow Sur-
deanu et al (2012) and begin by downsampling the
?negative? class to 5%. We also set T=8 and use
the following noisy-or (for ith bag) of mention-level
probability to rank predicted types (r) of pairs and
plot the precision-recall curves for all experiments.
Probi(r) = 1?
?
j
(1? p(zij = r|xij ;wz))
Evaluation: We compare our algorithm (MIML-
semi) to three algorithms: 1) MIML (Surdeanu et
al., 2012), the Multiple-Instance Multiple Label al-
gorithm which labels the bags directly with the KB
(y = ?). 2) MultiR (denoted as Hoffmann) (Hoff-
mann et al, 2011), a Multiple-Instance algorithm
that supports overlapping relations. It also imposes
y = ?. 3) Mintz++ (Surdeanu et al, 2012), a vari-
ant of the single-instance learning algorithm (section
3). The first two are stat-of-the-art Multi-Instance
Multi-Label algorithms. Mintz++ is a strong base-
line (Surdeanu et al, 2012) and an improved ver-
sion of Mintz et al (2009). Figure 2 shows that
our algorithm consistently outperforms all three al-
gorithms at almost all recall levels (with the excep-
tion of a very small region in the PR-curve). This
demonstrates that by treating unla-beled data set dif-
ferently and leveraging the missing positive bags,
MIML-semi is able to learn a more accurate model
for extraction. Although the proposed solution is a
specific algorithm, we believe the idea of treating
unlabeled data differently can be incorporated into
any of these algorithms that only use unlabeled data
as negative examples.
6 Conclusion
We show that the distant-supervision labeling pro-
cess generates a significant number of false nega-
tives because the knowledge base is incomplete. We
proposed an algorithm that learns from only positive
and unlabeled bags. Experimental results demon-
strate its advantage over existing algorithms.
Acknowledgments
Supported in part by the Intelligence Advanced Re-
search Projects Activity (IARPA) via Department of
Interior National Business Center contract number
D11PC20154. The U.S. Government is authorized
to reproduce and distribute reprints for Governmen-
tal purposes notwithstanding any copyright annota-
tion thereon. The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoI/NBC, or the U.S. Government.
781
References
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal super-
vision. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics.
Mark Craven and Johan Kumlien. 1999. Constructing bi-
ological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh Inter-
national Conference on Intelligent Systems for Molec-
ular Biology.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics.
Heng Ji, Ralph Grishman, and Hoa T. Dang. 2011.
Overview of the TAC 2011 knowledge base popula-
tion track. In Proceedings of the Text Analytics Con-
ference.
Jing Jiang and ChengXiang Zhai. 2007. A systematic ex-
ploration of the feature space for relation extraction. In
Proceedings of HLT-NAACL-2007.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy mod-
els for information extraction. In Proceedings of ACL-
2004.
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing
to extract information from text. In Proceedings of
NAACL-2000.
Bonan Min, Shuming Shi, Ralph Grishman and Chin-
Yew Lin. 2012a. Ensemble Semantics for Large-scale
Unsupervised Relation Extraction. In Proceedings of
EMNLP-CoNLL 2012.
Bonan Min, Xiang Li, Ralph Grishman and Ang Sun.
2012b. New York University 2012 System for KBP
Slot Filling. In Proceedings of the Text Analysis Con-
ference (TAC) 2012.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of the 47th An-
nual Meeting of the Association for Computational
Linguistics.
Truc Vien T. Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant supervi-
sion from external semantic repositories. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD 10).
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New York University 2011 system for KBP slot
filling. In Proceedings of the Text Analytics Confer-
ence.
Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-
Closky, Angel X. Chang, Valentin I. Spitkovsky, and
Christopher D. Manning. 2011. Stanfords distantly-
supervised slot-filling system. In Proceedings of the
Text Analytics Conference.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
Christopher D. Manning. 2012. Multi-instance Multi-
label Learning for Relation Extraction. In Proceed-
ings of the 2012 Conference on Empirical Methods in
Natural Language Processing and Natural Language
Learning.
TAC KBP 2011 task definition. 2011. http://nlp
.cs.qc.cuny.edu/kbp/2011/KBP2011 TaskDefinition.pdf
Shingo Takamatsu, Issei Sato, Hiroshi Nakagawa. 2012.
ReducingWrong Labels in Distant Supervision for Re-
lation Extraction. In Proceedings of 50th Annual Meet-
ing of the Association for Computational Linguistics.
Fei Wu and Daniel S. Weld. 2007. Autonomously seman-
tifying wikipedia. In Proceedings of the International
Conference on Information and Knowledge Manage-
ment (CIKM-2007).
Guodong Zhou, Jian Su, Jie Zhang and Min Zhang. 2005.
Exploring various knowledge in relation extraction. In
Proceedings of ACL-2005.
782
