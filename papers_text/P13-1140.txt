Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1425?1434,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Learning a Phrase-based Translation Model from Mon-
olingual Data with Application to Domain Adaptation 
 
Jiajun Zhang and Chengqing Zong 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Sciences, Beijing, China 
{jjzhang, cqzong}@nlpr.ia.ac.cn 
 
 
 
Abstract 
Currently, almost all of the statistical ma-
chine translation (SMT) models are trained 
with the parallel corpora in some specific 
domains. However, when it comes to a lan-
guage pair or a different domain without 
any bilingual resources, the traditional SMT 
loses its power. Recently, some research 
works study the unsupervised SMT for in-
ducing a simple word-based translation 
model from the monolingual corpora. It 
successfully bypasses the constraint of 
bitext for SMT and obtains a relatively 
promising result. In this paper, we take a 
step forward and propose a simple but effec-
tive method to induce a phrase-based model 
from the monolingual corpora given an au-
tomatically-induced translation lexicon or a 
manually-edited translation dictionary. We 
apply our method for the domain adaptation 
task and the extensive experiments show 
that our proposed method can substantially 
improve the translation quality. 
1 Introduction 
During the last decade, statistical machine trans-
lation has made great progress. Novel translation 
models, such as phrase-based models (Koehn et 
a., 2007), hierarchical phrase-based models 
(Chiang, 2007) and linguistically syntax-based 
models (Liu et a., 2006; Huang et al, 2006; Gal-
ley, 2006; Zhang et al 2008; Chiang, 2010; 
Zhang et al, 2011; Zhai et al, 2011, 2012) have 
been proposed and achieved higher and higher 
translation performance. However, all of these 
state-of-the-art translation models rely on the 
parallel corpora to induce translation rules and 
estimate the corresponding parameters.  
It is unfortunate that the parallel corpora are 
very expensive to collect and are usually not 
available for resource-poor languages and for 
many specific domains even in a resource-rich 
language pair. 
Recently, more and more researchers concen-
trated on taking full advantage of the monolin-
gual corpora in both source and target languages, 
and proposed methods for bilingual lexicon in-
duction from non-parallel data (Rapp, 1995, 
1999; Koehn and Knight, 2002; Haghighi et al, 
2008; Daum? III and Jagarlamudi, 2011) and 
proposed unsupervised statistical machine trans-
lation (bilingual lexicon is a byproduct) with 
only monolingual corpora (Ravi and Knight, 
2011; Nuhn et al, 2012; Dou and Knight, 2012). 
In the bilingual lexicon induction (Koehn and 
Knight, 2002; Haghighi et al, 2008; Daum? III 
and Jagarlamudi, 2011), with the help of the or-
thographic and context features, researchers 
adopted an unsupervised method, such as canon-
ical correlation analysis (CCA) model, to auto-
matically induce the word translation pairs be-
tween two languages from non-parallel data only 
requiring that the monolingual data in each lan-
guage are from a fairly comparable domain. 
The unsupervised statistical machine transla-
tion method (Ravi and Knight, 2011; Nuhn et al, 
2012; Dou and Knight, 2012) viewed the trans-
lation task as a decipherment problem and de-
signed a generative model with the objective 
function to maximize the likelihood of the 
source language monolingual data. To tackle the 
large-scale vocabulary, they mainly considered 
the word-based model (e.g. IBM Model 3) and 
applied the Bayesian method with Gibbs sam-
pling or slice sampling. Finally, they used the 
learned translation model directly to translate 
unseen data (Ravi and Knight, 2011; Nuhn et al, 
2012) or incorporated the learned bilingual lexi-
con as a new in-domain translation resource into 
the phrase-based model which is trained with 
out-of-domain data to improve the domain adap-
tation performance in machine translation (Dou 
and Knight, 2012).  
We can easily see that these unsupervised 
methods can only induce the word-based transla-
tion rules (bilingual lexicon) at present. It is a 
big challenge that whether we can induce phrase 
1425
1, word reordering example:
?   ??   ?  ??   ?? ||| the purpose of the invention is to ||| 0-0 0-3 1-4 2-2 3-1 4-5 4-6
2, idiom example:
??   ??   ? ||| distinguish the true from the false ||| 0-0 1-2 1-5 2-1 2-4
3, unknown word translation:
??   ???   ??   ? ||| of the light-emitting diode chip ||| 0-2 1-2 2-4 3-0 3-1
 
Table 1: Examples of new translation knowledge learned with the proposed phrase pair induction method. For 
the three fields separated by ?|||?, the first two are respectively Chinese and English phrase, and the last one is 
the word alignment between these two phrases. 
 
level translation rules and learn a phrase-based 
model from the monolingual corpora. 
In this paper, we focus on exploring this di-
rection and propose a simple but effective meth-
od to induce the phrase-level translation rules 
from monolingual data. The main idea of our 
method is to divide the phrase-level translation 
rule induction into two steps: bilingual lexicon 
induction and phrase pair induction.  
Since many researchers have studied the bi-
lingual lexicon induction, in this paper, we 
mainly concentrate ourselves on phrase pair in-
duction given a probabilistic bilingual lexicon 
and two in-domain large monolingual data 
(source and target language). In addition, we 
will further introduce how to refine the induced 
phrase pairs and estimate the parameters of the 
induced phrase pairs, such as four standard 
translation features and phrase reordering feature 
used in the conventional phrase-based models 
(Koehn et al, 2007). The induced phrase-based 
model will be used to help domain adaptation 
for machine translation. 
In the rest of this paper, we first explain with 
examples to show what new translation 
knowledge can be learned with our proposed 
phrase pair induction method (Section 2), and 
then we introduce the approach for probabilistic 
bilingual lexicon acquisition in Section 3. In Sec-
tion 4 and 5, we respectively present our method 
for phrase pair induction and introduce an ap-
proach for phrase pair refinement and parameter 
estimation. Section 6 will show the detailed ex-
periments for the task of domain adaptation. We 
will introduce some related work in Section 7 
and conclude this paper in Section 8. 
2 What Can We Learn with Phrase 
Pair   Induction? 
Readers may doubt that if phrase pair induction 
is performed only using bilingual lexicon and 
monolingual data, what new translation 
knowledge can be learned? 
The bilingual lexicon can only express the 
translation equivalence between source- and tar-
get-side word pair and has little ability to deal 
with word reordering and idiom translation. In 
contrast, phrase pair induction can make up for 
this deficiency to some extent. Furthermore, our 
method is able to learn some unknown word 
translations. 
From the induced phrase pairs with our meth-
od, we have conducted a deep analysis and find 
that we can learn three kinds of new translation 
knowledge: 1) word reordering in a phrase pair; 
2) idioms; and 3) unknown word translations. 
Table 1 gives examples for each of the three 
kinds. For the first example, the source and tar-
get phrase are extracted respectively from mono-
lingual data, each word in the source phrase has 
a translation in the target phrase, but the word 
order is different. The word order encoded in a 
phrase pair is difficult to learn in a word-based 
SMT.  In the second example, the italic source 
word corresponds to two target words (in italic), 
and the phrase pair is an idiom which cannot be 
learned from word-based SMT. In the third ex-
ample, as we learn from the source and target 
monolingual text that the words around the italic 
ones are translations with each other, thus we 
cannot only extract a new phrase pair but also 
learn a translation pair of unknown words in 
italic. 
3 Probabilistic Bilingual Lexicon Ac-
quisition 
In order to induce the phrase pairs from the in-
domain monolingual data for domain adaptation, 
the probabilistic bilingual lexicon is essential. 
In this paper, we acquire the probabilistic bi-
lingual lexicon from two approaches: 1) build a 
bilingual lexicon from large-scale out-of-domain 
parallel data; 2) adopt a manually collected in-
domain lexicon. This paper uses Chinese-to-
English translation as a case study and electronic 
data is the in-domain data we focus on.  
1426
In Chinese-to-English translation, there are 
lots of parallel data on News. Here, we utilize 
about 2.08 million sentence pairs1 in News do-
main to learn a probabilistic bilingual lexicon. 
Basically, we can use GIZA++ (Och, 2003) to 
get the probabilistic lexicon. However, the prob-
lem is that each source-side word associates too 
many possible translations which contain much 
noise. For instance, in the lexicon obtained with 
GIZA++, each source-side word has about 13 
translations on average. The noise of the lexicon 
can influence the accuracy of the induced phrase 
pairs to a large extent. To learn a lexicon with a 
high precision, we follow Munteanu and Marcu 
(2006) to apply Log-Likelihood-Ratios (Dunning, 
1993; Melamed, 2000; Moore, 2004a, 2004b) to 
estimate how strong the association is between a 
source-side word and its aligned target-side word. 
We employ the same algorithm used in (Munte-
anu and Marcu, 2006) which first use the GI-
ZA++ (with grow-diag-final-and heuristic) to 
obtain the word alignment between source and 
target words, and then calculate the association 
strength between the aligned words. After using 
the log-likelihood-ratios algorithm2, we obtain a 
probabilistic bilingual lexicon with bidirectional 
translation probabilities from the out-of-domain 
data. In the final lexicon, the number of average 
translations is only 5. We call this lexicon LLR-
lex. 
   In the electronic domain, we manually collect-
ed a lexicon which contains about 140k entries. 
It should be noted that there is no translation 
probability in this lexicon. In order to assign 
probabilities to each entry, we apply the Corpus 
Translation Probability which used in (Wu et al, 
2008): given an in-domain source language 
monolingual data, we translate this data with the 
phrase-based model trained on the out-of-domain 
News data, the in-domain lexicon and the in-
domain target language monolingual data (for 
language model estimation).  With the source 
language data and its translation, we estimate the 
bidirectional translation probabilities for each 
entry in the original lexicon. For the entries 
whose translation probabilities are not estimated, 
we just assign a uniform probability. That is if a 
source word has n translations, then the transla-
tion probability of target word given the source 
word is 1/n. We call this lexicon Domain-lex. 
                                                 
1 LDC category numbers are: LDC2000T50, LDC2003E14, 
LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27, 
LDC2005T10 and LDC2005T34. 
2  Following Moore (2004b), we use the threshold 10 on 
LLR to filter out unlikely translations. 
  We combine LLR-lex and Domain-lex to obtain 
the final probabilistic bilingual lexicon for phrase 
pair induction. 
4 Phrase Pair Induction Method 
Given a probabilistic bilingual lexicon and two 
monolingual data, we present a simple but effec-
tive method for phrase pair induction in this sec-
tion. 
 
 
Figure 1: a na?ve algorithm for phrase pair induction. 
4.1 A Na?ve Method 
We first introduce a relatively na?ve way to ex-
tract phrase pairs from the given resources. For a 
source phrase (word sequence), we can reorder 
the words in the phrase (permutation) first, and 
then obtain the target phrases with the bilingual 
lexicon (translation), and finally check if the tar-
get phrase is in the target monolingual data. The 
algorithm is given in Figure 1. 
Figure 1 shows that the na?ve algorithm is very 
easy to implement. However, the time complexi-
ty is too high. For each source phrase jis  (with 
? ?1 !j i? ?  permutations), suppose a source word 
has C translations on average and checking 
whether the target phrase ''jit  in T needs time 
? ?O T , then, phrase pair induction for a single 
source phrase needs time ? ?? ?1 1 !j iO C T j i? ? ? ?. 
It is very time consuming. One may design 
smarter algorithms. For example, one can collect 
distinct n-grams from source and target monolin-
gual data. Then, for a source-side phrase with 
length L, one can find the best translation candi-
date using the probabilistic bilingual lexicon 
from the target-side phrases with the same length 
L. The biggest disadvantage of these algorithms 
is that they can only induce phrase pair (with the 
Input:   Probabilistic bilingual lexicon V (each source word 
s maps a translation set V[s]) 
            Source language monolingual data S={sn} n=1...N 
            Target language monolingual data T={tm} m=1...M 
Output: Phrase pairs  P 
 
1: For each distinct source-side phrase jis  in S:  
2:       If each jk is s? in V: 
3: Collect [ ] jk k iV s ?  
4: For each permutation ''jis  of jis :  
5:        If ''jit  in T:     ' '[ ] ' [ , ]k kt V s k i j? ?  
6:  Add phrase pair ? ?'',j ji is t into P 
1427
same length) encoding word reordering, but can-
not learn phrase pairs in different length. Fur-
thermore, they cannot learn idioms and unknown 
word translations from monolingual data. Obvi-
ously, these kind of approaches is not optimal. 
4.2 Phrase Pair Induction with Inverted 
Index 
In order to make the phrase pair induction both 
effective and efficient, we propose a method 
using inverted index data structure which is usu-
ally a central component of a typical search en-
gine.  
The inverted index is employed to represent 
the target language monolingual data. For a tar-
get language word, the inverted index not only 
records the sentence position in monolingual 
data, but also records the word position in a sen-
tence. Some examples are shown in Table 2. By 
doing this, we do not need to iterate all the per-
mutations of source language phrase jis  to ex-
plore possible phrase pairs encoding word reor-
dering. Furthermore, it is possible to learn idiom 
translation and unknown word translations. We 
will elaborate how to induce phrase pairs with 
the help of inverted index. 
Target Language 
Word 
Position 
communication (2,5), (106,20), ?, (23022, 12) 
? ? 
zoom (90,2), (280,21), ?, (90239,15) 
Table 2: Some examples of inverted index for tar-
get language words, (2,5) means that ?communica-
tion? occurs at the 5th word of the 2nd sentence in the 
target monolingual data. 
The new algorithm for phrase pair induction is 
presented in Figure 2. Line 1 iterates all the dis-
tinct phrases in the source-side monolingual data. 
It can be implemented by collecting all the dis-
tinct n-grams in which n is the phrase length we 
are interested in (3 to 7 in this paper). For each 
distinct source-side phrase, Line 2-5 efficiently 
collects all the positions in the target monolin-
gual data for the translations of each word in the 
source phrase. Line 6 sorts the positions so that 
we can easily find the position sequence belong-
ing to a same sentence. Line 8-9 discards all the 
position sub-sequences that lack translations for 
more than one source-side words. That is to say 
we allow at most one unknown word in an in-
duced phrase pair in order to make the induction 
more accurate. Line 10 and Line 12 is the core 
of this algorithm. We first define a constraint 
before detailing the algorithm. 
Figure 2: Phrase pair induction using inverted index. 
Constraint: we require that there exists at 
most one phrase in a target sentence that is the 
translation of the source-side phrase. 
According to our analysis, it is not often to 
find that two phrases (length larger than 2) in a 
same sentence have the same meaning. Even if it 
happens, it is reasonable to keep the one with the 
highest probability. Given a position sequence 
belonging to a same sentence, Line 10 smoothes 
the probability of the single word gap according 
to the probabilities of the around words. Single 
word gap means that this word is not aligned but 
its left and right words are aligned with the 
words of the source-side phrase. Suppose the 
target sub-sequence is 
i i r jt t t?
 and 
i rt ?  is the 
only word that is not aligned with source-side 
words. We smooth the probability ? ?|i rp t null?  
as follows: 
? ?
? ? ? ?? ?
? ? ? ?
1 11 1
min | , |
, 1 1
2|
| |
,
2
i j
i r i r
i t j t
i r
i r t i r t
p t s p t s
if r or r j
p t null
p t s p t s
otherwise? ? ? ?
?
? ? ? ?
?
? ? ? ??
? ?
? ?
?
?
        (1) 
The above formula means that if the left or the 
right side only has one word, then the smoothed 
probability is one half of the minimum of the 
probabilities of the two neighbors, otherwise the 
smoothed probability is the average of the prob-
abilities of the two neighbors. This smoothing 
strategy encourages that if more words around 
the un-aligned word are translations of the 
source-side phrase, then the gap word is more 
likely to belong to the translations of the source-
side phrase. 
Input:   Probabilistic bilingual lexicon V (each source word s 
maps a translation set V[s]) 
            Source language monolingual data S={sn} n=1...N 
            Inverted index representing target language monolin-
gual data IMap 
Output: Phrase pairs P 
1: For each distinct source-side phrase jis  in S:  
2:      positionArray = [] 
3:      For each jk is s? : 
4:            For each [ ]kt V s? : 
5:       add  IMap[ t ]  into positionArray 
6:      Sort  positionArray 
7:      For each sequence in a same sentence in positionArray:  
8:              If more than 1 word in jis has no trans in the seq: 
9:                    Discard this seq and continue 
10:             Probability smoothing for single word gap 
11:             For all continuous position sub-sequence: 
12:                  Find the one kht  with maximum probability 
13:                 Add phrase pair ? ?,j ki hs t into P 
1428
After probability smoothing of the single gap 
word, we are ready to extract the candidate 
translation of the source-side phrase. Similar 
with Line 9 in Figure 2, we further filter the tar-
get continuous phrase if more than one word in 
source-side phrase has no translation in this tar-
get phrase. After that, we just choose the contin-
uous target phrase with the largest probability if 
two or more continuous target phrases exist in 
the same target sentence. The probability of a 
target-side phrase given the source-side phrase is 
computed similar to that of (Koehn et al, 2003) 
except that we impose length normalization: 
? ? ? ?? ? ? ?? ?
1
,1
1| , |
| ,
nn
lex i j
i j ai
p t s a p t s
j i j a ? ??
? ?
? ?? ? ??? ?
??
         (2) 
where the alignment a is produced using 
probabilistic bilingual lexicon. If a target word 
in t is a gap word, we suppose there is a word 
alignment between the target gap word and the 
source-side null.  
Similarly, we can compute the probability of 
source-side phrase given the target-side phrase 
? ?| ,lexp s t a . Then, we find the target-side phrase 
which has the biggest value of 
? ? ? ?| , | ,lex lexp t s a p s t a? . Line 13 in Figure 2 col-
lects the induced phrase pairs. 
For the time complexity, it depends on the 
length of positionArray, since the time complex-
ity of the core algorithm (Line 7-13) is propor-
tional to the length of positionArray. If posi-
tionArray contains almost all the positions in the 
target monolingual data T, then the worst time 
complexity will be ? ?logO T T  (for array sort). 
However, we find in the target monolingual data 
(1 million sentences) that each distinct word 
happens 110 times on average. Then, for a 
sources-side phrase with 7 words, the average 
length of positionArray will be 3850, since each 
source word has averagely 5 target translations 
(mentioned in Section 3). Therefore, the algo-
rithm is relatively efficient in the average case. 
5 Phrase Pair Refinement and Parame-
terization 
5.1 Phrase Pair Refinement 
Some of the phrase pairs induced in Section 4 
may contain noise. According to our analysis, 
we find that the biggest problem is that in the 
target-side of the phrase pair, there are two or 
more identical words aligned to the same source-
side word. For example, we extract a phrase pair 
as follows: 
?  ??  ??
of  business information of 
In the above phrase pair, there are two words 
?of? in the target side and the first one is redun-
dant. The phrase pair induction algorithm pre-
sented in Section 4 cannot deal with this situa-
tion. In this section, we propose a simple ap-
proach to handle this problem. For each entry in 
LLR-lex, such as (?, of), we can learn two kinds 
of information from the out-of-domain word-
aligned sentence pairs: one is whether the target 
translation is before or after the translation of the 
preceding source-side word (Order); the other is 
whether the target translation is adjacent with 
the translation of the preceding source-side word 
(Adjacency). If the source-side word is the be-
ginning of the phrase, we calculate the corre-
sponding information with the succeeding word 
instead of the preceding word. For the entries in 
Domain-lex, we constrain that the target transla-
tion should be adjacent with the translations of 
its source-side neighbors and translation order is 
the same with the source-side words. 
With the Order and Adjacency information, 
we first check the order information, and then 
check the adjacency information if the dupli-
cates cannot be handled using order information. 
For example, since (?, of) is an entry in LLR-
lex and we have learned that ?of? is much more 
likely to be behind the translation of the suc-
ceeding word. Thus, the first word ?of? can be 
discarded. This refinement can be applied before 
finding the phrase pair with maximum probabil-
ity (Line 12 in Figure 2) so that the duplicate 
words do not affect the calculation of translation 
probability of phrase pair. 
5.2 Translation Probability Estimation 
It is well known that in the phrase-based SMT 
there are four translation probabilities and the 
reordering probability for each phrase pair. 
   The translation probabilities in the traditional 
phrase-based SMT include bidirectional phrase 
translation probabilities and bidirectional lexical 
weights. For the lexical weights, we can use the 
? ?| ,lexp s t a  and ? ?| ,lexp t s a computed in the 
above section without length normalization. 
However, for the phrase-level probability, we 
cannot use maximum likelihood estimation since 
the phrase pairs are not extracted from parallel 
sentences. 
1429
 In this paper, we borrow and extend the idea of 
(Klementiev et al, 2012) to calculate the phrase-
level translation probability with context infor-
mation in source and target monolingual corpus. 
The value is calculated using a vector space 
model. With source and target vocabularies 
? ?1 2, , , Ns s s  and ? ?1 2, , , Mt t t , the source-side 
phrase s and target-side phrase t can be respec-
tively represented in an N- and M-dimensional 
vector. The k-th component of s?s contextual 
vector is computed using the method of (Fung 
and Yee, 1998) as follows: 
? ?? ?, maxlog / 1k s k kw n n n? ? ?               (3) 
where 
,s kn
and 
kn denotes the number of times ks  
occurs in the context of s and in the entire source 
language monolingual data, and 
maxn is the max-
imum number of occurrence of any source-side 
word in the source language monolingual data. 
The k-th element of t?s vector can be computed 
with the same method. We finally normalize 
these vectors with L2-norm. 
   With the s?s and t?s contextual vector represen-
tations, we calculate two similarities: 1) project 
s?s vector into target side t  with the lexical 
mapping p(t|s), and then get the similarity by 
computing the cosine of two angles between t?s 
and t ?s vectors; 2) project t?s vector into source 
side s  with the lexical mapping p(s|t), and then 
obtain the similarity between s?s and s ?s vectors. 
These two contextual similarities will serve as 
two phrase-level translation probabilities. 
5.3 Reordering Probability Estimation 
For the reordering probabilities of newly induced 
phrase pairs, we can also follow Klementiev et al 
(2012) to estimate these probabilities using 
source and target monolingual data. The method 
is to calculate six probabilities for monotone, 
swap or discontinuous cases. For the phrase pair 
(? ?? ?? , business information of), we 
find a source sentence containing ? ?? ??, 
and find a target sentence containing business 
information of. If there is another phrase pair 
? ?,s t ,  t  exactly follows business information of 
and s  occurs in the same source sentence with 
? ?? ??, then we compare the position 
relationship between s  and ? ?? ??. We 
increment the swap count if s  is just before ? 
?? ??. After counting, we finally use max-
imum likelihood estimation method to compute 
the reordering probabilities. 
6 Related Work 
As far as we know, few researchers study phrase 
pair induction from only monolingual data. 
   There are three research works that are most 
related with ours. One is using an in-domain 
probabilistic bilingual lexicon to extract sub-
sentential parallel fragments from comparable 
corpora (Munteanu and Marcu, 2006; Quirk et al, 
2007; Cettolo et al, 2010). Munteanu and Marcu 
(2006) first extract the candidate parallel sen-
tences from the comparable corpora and further 
extract the accurate sub-sentential bilingual 
fragments from the candidate parallel sentences 
using the in-domain probabilistic bilingual lexi-
con. Compared with their work, our focus is to 
induce phrase pairs directly from monolingual 
data rather than comparable data. Thus, finding 
the candidate parallel sentences is not possible in 
our situation. 
Another is to make full use of monolingual da-
ta with transductive learning (Ueffing et al, 2007; 
Schwenk, 2008; Wu et al, 2008; Bertoldi and 
Federico, 2009). For the target-side monolingual 
data, they just use it to train language model, and 
for the source-side monolingual data, they em-
ploy a baseline (word-based SMT or phrase-
based SMT trained with small-scale bitext) to 
first translate the source sentences, combining 
the source sentence and its target translation as a 
bilingual sentence pair, and then train a new 
phrase-base SMT with these pseudo sentence 
pairs. This method cannot learn idiom transla-
tions and unknown word translations. 
The third is to estimate the translation parame-
ters and reordering parameters using monolin-
gual data given the phrase pairs (Klementiev et 
al., 2012). Their work supposes the phrase pairs 
are already given and then corresponding param-
eters can be learned with monolingual data. Dif-
ferent from their work, we concentrate ourselves 
on inducing phrase pairs from monolingual data 
and then borrow some ideas from theirs for pa-
rameter estimation. Furthermore, we extend their 
contextual similarity between source and target 
phrases to both directions. 
7 Experiments 
7.1 Experimental Setup  
Our purpose is to induce phrase pairs to improve 
translation quality for domain adaptation. We 
have introduced the out-of-domain data and the 
electronic in-domain lexicon in Section 3. Here 
we introduce other information about the in-
1430
domain data. Besides the in-domain lexicon, we 
have collected respectively 1 million monolin-
gual sentences in electronic area from the web. 
They are neither parallel nor comparable because 
we cannot even extract a small number of paral-
lel sentence pairs from this monolingual data 
using the method of (Munteanu and Marcu, 
2006). We further employ experts to translate 
2000 Chinese electronic sentences into English. 
The first half is used as the tuning set (elec1000-
tune) and the second half is employed as the test-
ing set (elec1000-test). 
   We construct two kinds of phrase-based mod-
els using Moses (Koehn et al, 2007): one uses 
out-of-domain data and the other uses in-domain 
data. For the out-of-domain data, we build the 
phrase table and reordering table using the 2.08 
million Chinese-to-English sentence pairs, and 
we use the SRILM toolkit (Stolcke, 2002) to 
train the 5-gram English language model with 
the target part of the parallel sentences and the 
Xinhua portion of the English Gigaword. For the 
in-domain electronic data, we first consider the 
lexicon as a phrase table in which we assign a 
constant 1.0 for each of the four probabilities, 
and then we combine this initial phrase table and 
the induced phrase pairs to form the new phrase 
table. The in-domain reordering table is created 
for the induced phrase pairs. An in-domain 5-
gram English language model is trained with the 
target 1 million monolingual data. 
   We use BLEU (Papineni et al, 2002) score 
with shortest length penalty as the evaluation 
metric and apply the pairwise re-sampling ap-
proach (Koehn, 2004) to perform the signifi-
cance test. 
7.2 Experimental Results  
In this section, we first conduct experiments to 
figure out how the translation performance de-
grades when the domain changes. To better illus-
trate the comparison, we first use News data to 
evaluate the NIST evaluation tests and then use 
the same News data to evaluate the electronic 
test sets. For the NIST evaluation, we employ 
Chinese-to-English NIST MT03 as the tuning set 
and NIST MT05 as the test set. Table 3 gives the 
results. It is obvious that, it is relatively high 
when using the News training data to evaluate 
the same News test set. However, when the test 
domain is changed, the translation performance 
decreases to a large extent. 
Given the in-domain bilingual lexicon and two 
monolingual data, previous works also proposed 
some good methods to explore the potential of 
the given data to improve the translation quality. 
Here, we implement their approaches and use 
them as our strong baseline. Wu et al (2008) 
regards the in-domain lexicon with corpus trans-
lation probability as another phrase table and 
further use the in-domain language model be-
sides the out-of-domain language model. Table 4 
gives the results. We can see from the table that 
the domain lexicon is much helpful and signifi-
cantly outperforms the baseline with more than 
4.0 BLEU points. When it is enhanced with the 
in-domain language model, it can further im-
prove the translation performance by more than 
2.5 BLEU points. This method has made good 
use of in-domain lexicon and the target-side in-
domain monolingual data, but it does not take 
full advantage of the in-domain source-side 
monolingual data. 
In order to use source-side monolingual data, 
Ueffing et al (2007), Schwenk (2008), Wu et al 
(2008) and Bertoldi and Federico (2009) em-
ployed the transductive learning to first translate 
the source-side monolingual data using the best 
configuration (baseline+in-domain lexicon+in-
domain language model) and obtain 1-best trans-
lation for each source-side sentence. With the 
source-side sentences and their translations, the 
new phrase table and reordering table are built. 
Then, these resources are added into the best 
configuration. The experimental results are pre-
sented in the last low of Table 4. From the results, 
we see that transductive learning can further im-
prove the translation performance significantly 
by 0.6 BLEU points. 
In tranductive learning, in-domain lexicon and 
both-side monolingual data have been explored. 
However, this method does not take full ad-
vantage of both-side monolingual data because it 
uses source and target monolingual data individ-
ually. In our method, we explore fully the source 
and target monolingual data to induce translation 
equivalence on the phrase level. In order to make 
the phrase pair induction more efficient, we first 
sort all the sentences in the both-side monolin-
gual data according to the word hit rate in the 
bilingual lexicon. Then, we conduct six sets of 
experiments respectively on the first 100k, 200k, 
300k, 500k and whole 1m sentences. All the ex-
periments are run based on the configuration 
with BLEU 13.41 in Table 4, and we call this 
configuration BestConfig. Note that the unknown 
words are only allowed if the source-side of a 
phrase pair has more than 3 words. Table 5 
shows the results. 
1431
 Training Data Tune Data (NIST MT03) Test Data (NIST MT05) 
2.08M sentence pairs in News 
35.79 34.26 
Tune Data (elec1000-tune) Test Data (elec1000-test) 
7.93 6.69 
Table 3: Experimental results using News training data to test NIST evaluation data and electronic data (numbers 
denote BLEU score points in percent). 
 
Method Tune (elec1000-tune) Test (elec1000-test) 
Baseline 7.93 6.69 
baseline + in-domain lexicon 10.97 10.87 
baseline + in-domain lexicon + in-
domain language model 
13.72 13.41++ 
Transductive Learning 14.13 14.01* 
Table 4: Experimental results using News training data, in-domain lexicon, language model and transductive 
learning. Bold figures mean that the results are statistically significant better than the baseline with p<0.01, and 
?++? denotes the result is statistically significant better than baseline+in-domain lexicon. ?*? means that the 
result is statistically significant better than 13.41 with p<0.05. 
 
Method Tune (BLEU %) Test (BLEU %) 
BestConfig 13.72 13.41 
+phrase pair induction (100k) 14.23 14.06 
+phrase pair induction (200k) 14.45 14.24 
+phrase pair induction (300k) 14.76 14.83++ 
+phrase pair induction (500k) 14.98 15.16++ 
+phrase pair induction (1m) 15.11 15.30++ 
Table 5: Experimental results of our phrase pair induction method. Bold figures denotes the corresponding 
method significantly outperform the BestConfig with p<0.05. Bold and Italic figures means the results are sig-
nificantly better than that of BestConfig with p<0.01. ?++? denotes that the corresponding approach performs 
significantly better than Transductive Learning with p<0.01. 
 
Method Before Filtering After Filtering 
+phrase pair induction (100k) 72,615 8,724 
+phrase pair induction (200k) 108,948 12,328 
+phrase pair induction (300k) 136,529 17,505 
+phrase pair induction (500k) 150,263 19,862 
+phrase pair induction (1m) 169,172 21,486 
Table 6: the number of phrase pairs induced with different size of monolingual data. 
 
  We can see from the table that our method ob-
tains the best translation performance. When us-
ing the first 100k sentences for phrase pair induc-
tion, it obtains a significant improvement over 
the BestConfig by 0.65 BLEU points and can 
outperform the transductive learning method.  
When we use more monolingual data, the per-
formance becomes even better.  The method of 
phrase pair induction using 300k sentences per-
forms quite well. It outperforms the BestConfig 
significantly with an improvement of 1.42 BLEU 
points and it also performs much better than 
transductive learning method with gains of 0.82 
BLEU points. With the monolingual data larger 
and larger, the gains become smaller and smaller 
because the word hit rate gets lower and lower. 
These experimental results empirically show the 
effectiveness of our proposed phrase pair induc-
tion method. 
   A question remains that how many new phrase 
pairs are induced with different size of monolin-
gual data. Here, we give respectively the statis-
tics before and after filtering with the 1000 test 
sentences. Table 6 shows the statistics. We can 
see from the table that lots of new phrase pairs 
can be induced since the source and target mono-
lingual data is in the same domain. However, 
since the source and target monolingual data is 
1432
far from parallel, most of the phrase pairs are not 
long. For example, in the 108,948 distinct phrase 
pairs, we find that the phrase pair distribution 
according to source-side length is (3:50.6%, 
4:35.6%, 5:3.3%, 6:9.8%, 7:0.7%). It is easy to 
see that the phrase pairs whose source-side 
length longer than 4 account for only a very 
small part. 
8 Conclusion and Future Work 
This paper proposes a simple but effective meth-
od to induce phrase pairs from monolingual data. 
Given the probabilistic bilingual lexicon and 
both-side monolingual data in the same domain, 
the method employs inverted index structure to 
represent the target-side monolingual data, and 
induce the translations for each distinct source-
side phrase with the help of the bilingual lexicon. 
We further propose an approach to refine the re-
sult phrase pairs to make them more accurate. 
We also introduce how to estimate the translation 
and reordering parameters for the induced phrase 
pairs with monolingual data. Extensive experi-
ments on domain adaptation have shown that our 
method can significantly outperform previous 
methods which also focus on exploring the in-
domain lexicon and monolingual data. 
However, through the analysis we find that our 
induced phrase pairs still contain some noise, 
such as the words in source- and target-side of 
the phrase pair are all aligned but the target-side 
phrase expresses the different meaning. Further-
more, our proposed method cannot learn expres-
sions which are not lexical translations but are 
semantic ones. In the future, we will study fur-
ther on these phenomena and propose new meth-
ods to handle these problems. 
Acknowledgments 
The research work has been funded by the Hi-
Tech Research and Development Program (?863? 
Program) of China under Grant No. 
2011AA01A207, 2012AA011101 and 
2012AA011102, and also supported by the Key 
Project of Knowledge Innovation of Program of 
Chinese Academy of Sciences under Grant No. 
KGZD-EW-501. We would also like to thank the 
anonymous reviewers for their valuable sugges-
tions.  
References  
Nicola Bertoldi and Marcello Federico, 2009. Domain 
adaptation for statistical machine translation with 
monolingual resources. In Proc. of the Fourth 
Workshop on Statistical Machine Translation, 
pages 182-189. 
Mauro Cettolo, Marcello Federico and Nicola 
Bertoldi, 2010. Mining parallel fragments from 
comparable texts. In Proc. of the seventh 
International Workshop on Spoken Language 
Translation (IWSLT), pages 227-234. 
David Chiang, 2007. Hierarchical phrase-based 
translation. computational linguistics, 33 (2). 
pages 201-228. 
David Chiang, 2010. Learning to translate with source 
and target syntax. In Proc. of ACL 2010, pages 
1443-1452. 
Hal Daum? III and Jagadeesh Jagarlamudi, 2011. 
Domain adaptation for machine translation by 
mining unseen words. In Proc. of ACL-HLT 
2011. 
Qing Dou and Kevin Knight, 2012. Large Scale 
Decipherment for Out-of-Domain Machine 
Translation. In Proc. of EMNLP-CONLL 2012. 
Ted Dunning, 1993. Accurate methods for the 
statistics of surprise and coincidence. 
computational linguistics, 19 (1). pages 61-74. 
Pascale Fung and Lo Yuen Yee, 1998. An IR 
approach for translating new words from 
nonparallel, comparable texts. In Proc. of ACL-
COLING 1998., pages 414-420. 
Michel Galley, Jonathan Graehl, Kevin Knight, 
Daniel Marcu, Steve DeNeefe, Wei Wang and 
Ignacio Thayer, 2006. Scalable inference and 
training of context-rich syntactic translation 
models. In Proc. of COLING-ACL 2006, pages 
961-968. 
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick 
and Dan Klein, 2008. Learning bilingual 
lexicons from monolingual corpora. In Proc. of 
ACL-08: HLT, pages 771-779. 
Liang Huang, Kevin Knight and Aravind Joshi, 2006. 
A syntax-directed translator with extended 
domain of locality. In Proc. of AMTA 2006, 
pages 1-8. 
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch and David Yarowsky, 2012. Toward 
statistical machine translation without parallel 
corpora. In Proc. of EACL 2012., pages 130-140. 
Philipp Koehn, 2004. Statistical significance tests for 
machine translation evaluation. In Proc. of 
EMNLP 2004., pages 388-395, Barcelona, Spain, 
July 25th-26th, 2004. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, 
Marcello Federico, Nicola Bertoldi, Brooke 
Cowan, Wade Shen, Christine Moran, Richard 
Zens, Chris Dyer, Ond?ej Bojar, Alexandra 
Constantin and Evan Herbst, 2007. Moses: Open 
source toolkit for statistical machine translation. 
In Proc. of ACL on Interactive Poster and 
Demonstration Sessions 2007., pages 177-180, 
Prague, Czech Republic, June 27th-30th, 2007. 
Philipp Koehn and Kevin Knight, 2002. Learning a 
translation lexicon from monolingual corpora. In 
1433
Proc. of the ACL-02 workshop on Unsupervised 
lexical acquisition, pages 9-16. 
Yang Liu, Qun Liu and Shouxun Lin, 2006. Tree-to-
string alignment template for statistical machine 
translation. In Proc. of COLING-ACL 2006, 
pages 609-616. 
I. Dan Melamed, 2000. Models of translational 
equivalence among words. computational 
linguistics, 26 (2). pages 221-249. 
Rorbert C. Moore, 2004a. Improving IBM word-
alignment model 1. In Proc. of ACL 2004. 
Rorbert C. Moore, 2004b. On log-likelihood-ratios 
and the significance of rare events. In Proc. of 
EMNLP 2004., pages 333-340. 
Dragos Stefan Munteanu and Daniel Marcu, 2006. 
Extracting parallel sub-sentential fragments from 
non-parallel corpora. In Proc. of ACL-COLING 
2006. 
Malte Nuhn, Arne Mauser and Hermann Ney, 2012. 
Deciphering Foreign Language by Combining 
Language Models and Context Vectors. In Proc. 
of ACL 2012. 
Franz Josef Och and Hermann Ney., 2003. A 
systematic comparison of various statistical 
alignment models. computational linguistics, 29 
(1). pages 19-51. 
Kishore Papineni, Salim Roukos, Todd Ward and 
Wei-Jing Zhu, 2002. BLEU: a method for 
automatic evaluation of machine translation. In 
Proc. of ACL 2002., pages 311-318. 
Chris Quirk, Raghavendra Udupa and Arul Menezes, 
2007. Generative models of noisy translations 
with applications to parallel fragment extraction. 
In Proc. of the Machine Translation Summit XI, 
pages 377-384. 
Reinhard Rapp, 1995. Identifying word translations in 
non-parallel texts. In Proc. of ACL 1995, pages 
320-322. 
Reinhard Rapp, 1999. Automatic identification of 
word translations from unrelated English and 
German corpora. In Proc. of ACL 1999, pages 
519-526. 
Sujith Ravi and Kevin Knight, 2011. Deciphering 
foreign language. In Proc. of ACL 2011., pages 
12-21. 
Holger Schwenk, 2008. Investigations on largescale 
lightly-supervised training for statistical machine 
translation. In Proc. of IWSLT 2008, pages 182-
189. 
Andreas Stolcke, 2002. SRILM-an extensible 
language modeling toolkit. In Proc. of 7th 
International Conference on Spoken Language 
Processing, pages 901-904, Denver, Colorado, 
USA, September 16th-20th, 2002. 
Nicola Ueffing, Gholamreza Haffari and Anoop 
Sarkar, 2007. Transductive learning for statistical 
machine translation. In Proc. of ACL 2007. 
Hua Wu, Haifeng Wang and Chengqing Zong, 2008. 
Domain adaptation for statistical machine 
translation with domain dictionary and 
monolingual corpora. In Proc. of COLING 2008., 
pages 993-1000. 
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing 
Zong, 2011.  Simple but effective approaches to 
improving tree-to-tree model.  In Proc. of MT 
Summit XIII 2011, pages 261-268. 
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing 
Zong, 2012. Tree-based translation without using 
parse trees. In Proc. of COLING 2012, pages 
3037-3054. 
Jiajun Zhang, Feifei Zhai and Chengqing Zong, 2011. 
Augmenting string-to-tree translation models 
with fuzzy use of the source-side syntax. In Proc. 
of EMNLP 2011, pages 204-215. 
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, 
Chew Lim Tan and Sheng Li, 2008. A tree 
sequence alignment-based tree-to-tree translation 
model. In Proc. of ACL-08: HLT, pages 559-567. 
 
1434
