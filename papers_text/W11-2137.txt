Proceedings of the 6th Workshop on Statistical Machine Translation, pages 323?329,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
RegMT System for Machine Translation, System Combination, and
Evaluation
Ergun Bic?ici
Koc? University
34450 Sariyer, Istanbul, Turkey
ebicici@ku.edu.tr
Deniz Yuret
Koc? University
34450 Sariyer, Istanbul, Turkey
dyuret@ku.edu.tr
Abstract
We present the results we obtain using our
RegMT system, which uses transductive re-
gression techniques to learn mappings be-
tween source and target features of given par-
allel corpora and use these mappings to gen-
erate machine translation outputs. Our train-
ing instance selection methods perform fea-
ture decay for proper selection of training in-
stances, which plays an important role to learn
correct feature mappings. RegMT uses L2
regularized regression as well as L1 regular-
ized regression for sparse regression estima-
tion of target features. We present transla-
tion results using our training instance selec-
tion methods, translation results using graph
decoding, system combination results with
RegMT, and performance evaluation with the
F1 measure over target features as a metric for
evaluating translation quality.
1 Introduction
Regression can be used to find mappings between
the source and target feature sets derived from given
parallel corpora. Transduction learning uses a sub-
set of the training examples that are closely related
to the test set without using the model induced by
the full training set. In the context of statistical ma-
chine translation, translations are performed at the
sentence level and this enables us to select a small
number of training instances for each test instance
to guide the translation process. This also gives us a
computational advantage when considering the high
dimensionality of the problem as each sentence can
be mapped to many features.
The goal in transductive regression based ma-
chine translation (RegMT) is both reducing the com-
putational burden of the regression approach by re-
ducing the dimensionality of the training set and the
feature set and also improving the translation quality
by using transduction.
We present translation results using our training
instance selection methods, translation results us-
ing graph decoding, system combination results with
RegMT, and performance evaluation with the F1
measure over target features as a metric for eval-
uating translation quality. RegMT work builds on
our previous regression-based machine translation
results (Bicici and Yuret, 2010) especially with in-
stance selection and additional graph decoding ca-
pability. We present our results to this year?s chal-
lenges.
Outline: Section 2 gives an overview of the
RegMT model. In section 3, we present our train-
ing instance selection techniques and WMT?11 re-
sults. In section 4, we present the graph decoding re-
sults on the Haitian Creole-English translation task.
Section 5 presents our system combination results
using reranking with the RegMT score. Section 6
evaluates the F1 measure that we use for the auto-
matic evaluation metrics challenge. The last section
present our contributions.
2 Machine Translation Using Regression
Let X and Y correspond to the sets of tokens
that can be used in the source and target strings,
then, m training instances are represented as
(x1, y1), . . . , (xm, ym) ? X
? ? Y ?, where (xi, yi)
corresponds to a pair of source and target language
323
token sequences for 1 ? i ? m. Our goal is to find
a mapping f : X? ? Y ? that can convert a source
sentence to a target sentence sharing the same mean-
ing in the target language (Figure 1).
X? Y ?-
? R ?
-FX FY
g
?X ?Y
6
??1Y
f
h
Figure 1: String-to-string mapping.
We define feature mappers ?X : X? ? FX =
RNX and ?Y : Y ? ? FY = RNY that map each
string sequence to a point in high dimensional real
number space. Let MX ? RNX?m and MY ?
RNY ?m such that MX = [?X(x1), . . . ,?X(xm)]
and MY = [?Y (y1), . . . ,?Y (ym)]. The ridge re-
gression solution usingL2 regularization is found by
minimizing the following cost:
WL2 = arg min
W?RNY ?NX
?MY ?WMX ?2F +??W?
2
F . (1)
Two main challenges of the regression based ma-
chine translation (RegMT) approach are learning
the regression function, h : FX ? FY , and
solving the pre-image problem, which, given the
features of the estimated target string sequence,
h(?X(x)) = ?Y (y?), attempts to find y ? Y ?:
y = arg miny?Y ? ||h(?X(x)) ? ?Y (y)||
2. Pre-
image calculation involves a search over possible
translations minimizing the cost function:
f(x) = arg min
y?Y ?
??Y (y)?W?X(x)?2 . (2)
2.1 L1 Regularized Regression
String kernels lead to sparse feature representations
and L1 regularized regression is effective to find the
mappings between sparsely observed features.We
would like to observe only a few nonzero target co-
efficients corresponding to a source feature in the co-
efficient matrix. L1 regularization helps us achieve
solutions close to permutation matrices by increas-
ing sparsity (Bishop, 2006) (page 145). In contrast,
L2 regularized solutions give us dense matrices.
WL2 is not a sparse solution and most of the coef-
ficients remain non-zero. We are interested in pe-
nalizing the coefficients better; zeroing the irrele-
vant ones leading to sparsification to obtain a solu-
tion that is closer to a permutation matrix. L1 norm
behaves both as a feature selection technique and a
method for reducing coefficient values.
WL1 = arg min
W?RNY ?NX
?MY ?WMX ?2F +??W?1 . (3)
Equation 3 presents the lasso (Tibshirani, 1996) so-
lution where the regularization term is now the L1
matrix norm defined as ?W?1=
?
i,j |Wi,j |. WL2
can be found by taking the derivative but since
L1 regularization cost is not differentiable, WL1 is
found by optimization or approximation techniques.
We use forward stagewise regression (FSR) (Hastie
et al, 2006), which approximates lasso for L1 regu-
larized regression.
2.2 Related Work:
Regression techniques can be used to model the
relationship between strings (Cortes et al, 2007).
Wang et al (2007) applies a string-to-string map-
ping approach to machine translation by using ordi-
nary least squares regression and n-gram string ker-
nels to a small dataset. Later they use L2 regularized
least squares regression (Wang and Shawe-Taylor,
2008). Although the translation quality they achieve
is not better than Moses (Koehn et al, 2007), which
is accepted to be the state-of-the-art, they show the
feasibility of the approach. Serrano et al (2009)
use kernel regression to find translation mappings
from source to target feature vectors and experiment
with translating hotel front desk requests. Locally
weighted regression solves separate weighted least
squares problems for each instance (Hastie et al,
2009), weighted by a kernel similarity function.
3 Instance Selection for Machine
Translation
Proper selection of training instances plays an im-
portant role for accurately learning feature mappings
with limited computational resources. Coverage of
the features is important since if we do not have the
correct features in the training matrices, we will not
be able to translate them. Coverage is measured by
the percentage of target features of the test set found
in the training set. For each test sentence, we pick
a limited number of training instances designed to
324
improve the coverage of correct features to build a
regression model.
We use two techniques for this purpose: (1)
Feature Decay Algorithm (FDA), which optimizes
source languge bigram coverage to maximize the
target coverage, (2) dice. Feature decay algorithms
(FDA) aim to maximize the coverage of the tar-
get language features (such as words, bigrams, and
phrases) for the test sentences. FDA selects training
instances one by one updating the coverage of the
features already added to the training set in contrast
to the features found in the test sentence.
We also use a technique that we call dice, which
optimizes source language bigram coverage such
that the difficulty of aligning source and target fea-
tures is minimized. We define Dice?s coefficient
score as:
dice(x, y) =
2C(x, y)
C(x)C(y)
, (4)
where C(x, y) is the number of times x and y co-
occurr and C(x) is the count of observing x in
the selected training set. Given a test source sen-
tence, SU , we can estimate the goodness of a train-
ing sentence pair, (S, T ), by the sum of the align-
ment scores:
?dice(SU , S, T ) =
?
x?X(SU )
|T |?
j=1
?
y?Y (x)
dice(y, Tj)
|T | log |S|
,
(5)
where X(SU ) stores the features of SU and Y (x)
lists the tokens in feature x. The difficulty of word
aligning a pair of training sentences, (S, T ), can be
approximated by |S||T |. We use a normalization fac-
tor proportional to |T | log |S|.
The details of both of these techniques and further
results can be found in (Bicici and Yuret, 2011).
3.1 Moses Experiments on the Translation
Task
We have used FDA and dice algorithms to select
training sets for the out-of-domain challenge test
sets used in (Callison-Burch et al, 2011). The par-
allel corpus contains about 1.9 million training sen-
tences and the test set contain 3003 sentences. We
built separate Moses systems using all of the paral-
lel corpus for the language pairs en-de, de-en, en-
es, and es-en. We created training sets using all
en-de de-en en-es es-en
BLEU
ALL .1376 .2074 .2829 .2919
FDA .1363 .2055 .2824 .2892
dice .1374 .2061 .2834 .2857
words
ALL 47.4 49.6 52.8 50.4
FDA 7.9 8.0 8.7 8.2
dice 6.9 7.0 3.9 3.6
% ALL
FDA 17 16 16 16
dice 14 14 7.4 7.1
Table 1: Performance for the out-of-domain task
of (Callison-Burch et al, 2011). ALL corresponds to the
baseline system using all of the parallel corpus. words
list the size of the target words used in millions.
of the features of the test set to select training in-
stances. The results given in Table 1 show that we
can achieve similar BLEU performance using about
7% of the parallel corpus target words (200,000 in-
stances) using dice and about 16% using FDA. In the
out-of-domain translation task, we are able to reduce
the training set size to achieve a performance close
to the baseline. We may be able to achieve better
performance in this out-of-domain task as well as
explained in (Bicici and Yuret, 2011).
4 Graph Decoding for RegMT
We perform graph-based decoding by first generat-
ing a De Bruijn graph from the estimated y? (Cortes et
al., 2007) and then finding Eulerian paths with max-
imum path weight. We use four features when scor-
ing paths: (1) estimation weight from regression, (2)
language model score, (3) brevity penalty as found
by e?(lR?|s|/|path|) for lR representing the length ra-
tio from the parallel corpus and |path| representing
the length of the current path, (4) future cost as in
Moses (Koehn et al, 2007) and weights are tuned
using MERT (Och, 2003) on the de-en dev set.
We demonstrate that sparse L1 regularized regres-
sion performs better than L2 regularized regression.
Graph based decoding can provide an alternative to
state of the art phrase-based decoding system Moses
in translation domains with small vocabulary and
training set size.
4.1 Haitian Creole to English Translation Task
with RegMT
We have trained a Moses system for the Haitian Cre-
ole to English translation task, cleaned corpus, us-
325
ing the options as described in section 3.1. Moses
achieves 0.3186 BLEU on this task. We observed
that graph decoding performs better where target
coverage is high such that the bigrams used lead
to a connected graph. To increase the connec-
tivity, we have included Moses translations in the
training set and performed graph decoding with
RegMT. RegMT with L2 regularized regression
achieves 0.2708 BLEU with graph decoding and
lasso achieves 0.26 BLEU.
Moses makes use of a number of distortion pa-
rameters and lexical weights, which are estimated
using all of the parallel corpus. Thus, our Moses
translation achieves a better performance than graph
decoding with RegMT using 100 training instances
for translating each source test sentence.
5 System Combination with RegMT
We perform experiments on the system com-
bination task for the English-German, German-
English, English-Spanish, and Spanish-English lan-
guage pairs using the training corpus provided in
WMT?11 (Callison-Burch et al, 2011). We have
tokenized and lowercased each of the system out-
puts and combined these in a single N -best file per
language pair. We use these N -best lists for rerank-
ing by RegMT to select the best translation model.
Feature mappers used are 2-spectrum counting word
kernels (Taylor and Cristianini, 2004).
We rerank N -best lists by a linear combination of
the following scoring functions:
1. RegMT: Regression based machine translation
scores as found by Equation 2.
2. CBLEU: Comparative BLEU scores we obtain
by measuring the average BLEU performance
of each translation relative to the other systems?
translations in the N -best list.
3. LM: We calculate 5-gram language model
scores for each translation using the language
model trained over the target corpus provided
in the translation task.
Since we do not have access to the reference trans-
lations nor to the translation model scores each sys-
tem obtained for each sentence, we estimate trans-
lation model performance (CBLEU) by measuring
the average BLEU performance of each translation
relative to the other translations in the N -best list.
Thus, each possible translation in the N -best list is
BLEU scored against other translations and the av-
erage of these scores is selected as the CBLEU score
for the sentence. Sentence level BLEU score calcu-
lation avoids singularities in n-gram precisions by
taking the maximum of the match count and 12|si| for
|si| denoting the length of the source sentence si as
used in (Macherey and Och, 2007).
Table 2 presents reranking results on all of the lan-
guage pairs we considered, using RegMT, CBLEU,
and LM scores with the same combination weights
as above. We also list the performance of the best
model (Max) as well as the worst (Min). We are
able to achieve close or better BLEU scores in all
of the listed systems when compared with the per-
formance of the best translation system except for
the ht-en language pair. The lower performance in
the ht-en language pair may be due to having a sin-
gle best translation system that outperforms others
significantly. This happens for instance when an un-
constrained model use external resources to achieve
a significantly better performance than the second
best model. 2nd best in Table 2 lists the second best
model?s performance to estimate how much the best
model?s performance is better than the rest.
BLEU en-de de-en en-es es-en ht-en
Min .1064 .1572 .2174 .1976 .2281
Max .1727 .2413 .3375 .3009 .3708
2nd best .1572 .2302 .3301 .2973 .3288
Average .1416 .1997 .292 .2579 .2993
Oracle .2529 .3305 .4265 .4233 .4336
RegMT .1631 .2322 .3311 .3052 .3234
Table 2: System combination results.
RegMT model may prefer sentences with lower
BLEU, which can sometimes cause it to achieve a
lower BLEU performance than the best model. This
is clearly the case for en-de with 1.6 BLEU points
difference with the second best model performance
and for de-en task with 1.11 BLEU points differ-
ence. Also this observation holds for en-es with
0.74 BLEU points difference and for ht-en with 4.2
BLEU points difference. For es-en task, there is 0.36
BLEU points difference with the second best model
and these models likely to complement each other.
326
The existence of complementing SMT models is
important for the reranking approach to achieve a
performance better than the best model, as there is
a need for the existence of a model performing bet-
ter than the best model on some test sentences. We
can use the competitive SMT model to achieve the
performance of the best with a guarantee even when
a single model is dominating the rest (Bicici and
Kozat, 2010). For competing translation systems
in an on-line machine translation setting adaptively
learning of model weights can be performed based
on the previous transaltion performance (Bicici and
Kozat, 2010).
6 Target F1 as a Performance Evaluation
Metric
We use target sentence F1 measure over the tar-
get features as a translation performance evaluation
metric. We optimize the parameters of the RegMT
model with the F1 measure comparing the target
vector with the estimate we get from the RegMT
model. F1 measure uses the 0/1-class predictions
over the target feature with the estimate vector,
?Y (y?). Let TP be the true positive, TN the true neg-
ative, FP the false positive, and FN the false negative
rates, we use the following measures for evaluation:
prec =
TP
TP + FP
, BER = ( FPTN+FP +
FN
TP+FN )/2 (6)
rec =
TP
TP + FN
, F1 =
2?prec?rec
prec+rec (7)
where BER is the balanced error rate, prec is pre-
cision, and rec is recall. The evaluation techniques
measure the effectiveness of the learning models in
identifying the features of the target sentence mak-
ing minimal error to increase the performance of the
decoder and its translation quality.
We use gapped word sequence kernels (Taylor
and Cristianini, 2004) when using F1 for evaluating
translations since a given translation system may not
be able to translate a given word but can correctly
identify the surrounding phrase. For instance, let the
reference translation be the following sentence:
a sound compromise has been reached
Some possible translations for the reference are
given in Table 3 together with their BLEU (Papineni
et al, 2001) and F1 scores for comparison. F1 score
does not have a brevity penalty but a brief transla-
tion is penalized by a low recall value. We use up
to 3 tokens as gaps. F1 measure is able to increase
the ranking of Trans4 by using a gapped sequence
kernel, which can be preferrable to Trans3.
We note that a missing token corresponds to vary-
ing decreases in the n-gram precision used in the
BLEU score. A sentence containing m tokens has
m 1-grams, m?1 2-grams, andm?n+1 n-grams.
A missing token degrades the performance more in
higher order n-gram precision values. A missing to-
ken decreases n-gram precision by 1m for 1-grams
and by nm?n+1 for n-grams. Based on this obser-
vation, we use F1 measure with gapped word se-
quence kernels to evaluate translations. Gapped fea-
tures allows us to consider the surrounding phrase
for a missing token as present in the translation.
Let the reference sentence be represented with
a b c d e f where a-f, x, y, z correspond to to-
kens in the sentence. Then, Trans3 has the form
a b x y f, and Trans4 has the form a c y f.
Then, F1 ranks Trans4 higher than Trans3 for orders
greater than 3 as there are two consecutive word er-
rors in Trans3. F1 can also prefer a missing token
rather than a word error as we see by comparing
Trans4 and Trans5 and it can still prefer contigu-
ity over a gapped sequence as we see by comparing
Trans5 and Trans6 in Table 3.
We calculate the correlation of F1 with BLEU on
the en-de development set. We use 5-grams with the
F1 measure as this increases the correlation with 4-
gram BLEU. Table 4 gives the correlation results us-
ing both Pearson?s correlation score and Spearman?s
correlation score. Spearman?s correlation score is a
better metric for comparing the relative orderings.
Metric No gaps Gaps
Pearson .8793 .7879
Spearman .9068 .8144
Table 4: F1 correlation with 4-gram BLEU using blended
5-gram gapped word sequence features on the develop-
ment set.
7 Contributions
We present the results we obtain using our RegMT
system, which uses transductive regression tech-
niques to learn mappings between source and tar-
327
Format BLEU F1
Ref: a sound compromise has been reached a b c d e f 4-grams 3-grams 4-grams 5-grams
Trans1: a sound agreement has been reached a b x d e f .2427 .6111 .5417 .5
Trans2: a compromise has reached a c d f .137 .44 .3492 .3188
Trans3: a sound agreement is reached a b x y f .1029 .2 .1558 .1429
Trans4: a compromise is reached a c y f .0758 .2 .1587 .1449
Trans5: a good compromise is reached a z c y f .0579 .1667 .1299 .119
Trans6: a good compromise is been a z c y e .0579 .2 .1558 .1429
Table 3: BLEU vs. F1 on sample sentence translation task.
get features of given parallel corpora and use these
mappings to generate machine translation outputs.
We also present translation results using our train-
ing instance selection methods, translation results
using graph decoding, system combination results
with RegMT, and performance evaluation with F1
measure over target features. RegMT work builds
on our previous regression-based machine transla-
tion results (Bicici and Yuret, 2010) especially with
instance selection and additional graph decoding ca-
pability.
References
Ergun Bicici and S. Serdar Kozat. 2010. Adaptive
model weighting and transductive regression for pre-
dicting best system combinations. In Proceedings of
the ACL 2010 Joint Fifth Workshop on Statistical Ma-
chine Translation and Metrics MATR, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Ergun Bicici and Deniz Yuret. 2010. L1 regularized re-
gression for reranking and system combination in ma-
chine translation. In Proceedings of the ACL 2010
Joint Fifth Workshop on Statistical Machine Transla-
tion and Metrics MATR, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Ergun Bicici and Deniz Yuret. 2011. Instance selec-
tion for machine translation using feature decay al-
gorithms. In Proceedings of the EMNLP 2011 Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, England, July.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, and Omar Zaidan, editors. 2011. Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation. Edinburgh, England, July.
Corinna Cortes, Mehryar Mohri, and Jason Weston.
2007. A general regression framework for learn-
ing string-to-string mappings. In Gokhan H. Bakir,
Thomas Hofmann, and Bernhard Sch editors, Predict-
ing Structured Data, pages 143?168. The MIT Press,
September.
Trevor Hastie, Jonathan Taylor, Robert Tibshirani, and
Guenther Walther. 2006. Forward stagewise regres-
sion and the monotone lasso. Electronic Journal of
Statistics, 1.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
2009. The Elements of Statistical Learning: Data
Mining, Inference and Prediction. Springer-Verlag,
2nd edition.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Annual
Meeting of the Assoc. for Computational Linguistics,
pages 177?180, Prague, Czech Republic, June.
Wolfgang Macherey and Franz Josef Och. 2007. An
empirical study on computing consensus translations
from multiple machine translation systems. In Pro-
ceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 986?995, Prague, Czech Republic,
June. Association for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. Association for Com-
putational Linguistics, 1:160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic evalu-
ation of machine translation. In ACL ?02: Proceedings
of the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Nicolas Serrano, Jesus Andres-Ferrer, and Francisco
Casacuberta. 2009. On a kernel regression approach
to machine translation. In Iberian Conference on Pat-
tern Recognition and Image Analysis, pages 394?401.
J. Shawe Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
328
Robert J. Tibshirani. 1996. Regression shrinkage and
selection via the lasso. Journal of the Royal Statistical
Society, Series B, 58(1):267?288.
Zhuoran Wang and John Shawe-Taylor. 2008. Kernel
regression framework for machine translation: UCL
system description for WMT 2008 shared translation
task. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, pages 155?158, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Zhuoran Wang, John Shawe-Taylor, and Sandor Szed-
mak. 2007. Kernel regression based machine trans-
lation. In Human Language Technologies 2007: The
Conference of the North American Chapter of the As-
sociation for Computational Linguistics; Companion
Volume, Short Papers, pages 185?188, Rochester, New
York, April. Association for Computational Linguis-
tics.
329
