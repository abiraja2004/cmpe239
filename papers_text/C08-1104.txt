Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 825?832
Manchester, August 2008
From Words to Senses: A Case Study of Subjectivity Recognition
Fangzhong Su
School of Computing
University of Leeds, UK
fzsu@comp.leeds.ac.uk
Katja Markert
School of Computing
University of Leeds, UK
markert@comp.leeds.ac.uk
Abstract
We determine the subjectivity of word
senses. To avoid costly annotation, we
evaluate how useful existing resources es-
tablished in opinion mining are for this
task. We show that results achieved with
existing resources that are not tailored to-
wards word sense subjectivity classifica-
tion can rival results achieved with super-
vision on a manually annotated training
set. However, results with different re-
sources vary substantially and are depen-
dent on the different definitions of subjec-
tivity used in the establishment of the re-
sources.
1 Introduction
In recent years, subjectivity analysis and opinion
mining have attracted considerable attention in the
NLP community. Unlike traditional information
extraction and document classification tasks which
usually focus on extracting facts or categorizing
documents into topics (e.g., ?sports?, ?politics?,
?medicine?), subjectivity analysis focuses on de-
termining whether a language unit (such as a word,
sentence or document) expresses a private state,
opinion or attitude and, if so, what polarity is ex-
pressed, i.e. a positive or negative attitude.
Inspired by Esuli and Sebastiani (2006) and
Wiebe and Mihalcea (2006), we explore the auto-
matic detection of the subjectivity of word senses,
in contrast to the more frequently explored task
of determining the subjectivity of words (see Sec-
tion 2). This is motivated by many words being
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
subjectivity-ambiguous, i.e. having both subjec-
tive and objective senses, such as the word positive
with its two example senses given below.
1
(1) positive, electropositive?having a positive electric
charge;?protons are positive? (objective)
(2) plus, positive?involving advantage or good; ?a plus
(or positive) factor? (subjective)
Subjectivity labels for senses add an additional
layer of annotation to electronic lexica and allow to
group many fine-grained senses into higher-level
classes based on subjectivity/objectivity. This can
increase the lexica?s usability. As an example,
Wiebe and Mihalcea (2006) prove that subjectiv-
ity information for WordNet senses can improve
word sense disambiguation tasks for subjectivity-
ambiguous words (such as positive). In addition,
Andreevskaia and Bergler (2006) show that the
performance of automatic annotation of subjectiv-
ity at the word level can be hurt by the presence of
subjectivity-ambiguous words in the training sets
they use. Moreover, the prevalence of different
word senses in different domains also means that
a subjective or an objective sense of a word might
be dominant in different domains; thus, in a sci-
ence text positive is likely not to have a subjective
reading. The annotation of words as subjective and
objective or positive and negative independent of
sense or domain does not capture such distinctions.
In this paper, we validate whether word sense
subjectivity labeling can be achieved with existing
resources for subjectivity analysis at the word and
sentence level without creating a dedicated, man-
ually annotated training set of WordNet senses la-
beled for subjectivity.
2
We show that such an ap-
1
All examples in this paper are from WordNet 2.0.
2
We use a subset of WordNet senses that are manually an-
notated for subjectivity as test set (see Section 3).
825
proach ? even using a simple rule-based unsu-
pervised algorithm ? can compete with a stan-
dard supervised approach and also compares well
to prior research on word sense subjectivity label-
ing. However, success depends to a large degree
on the definition of subjectivity used in the estab-
lishment of the prior resources.
The remainder of this paper is organized as fol-
lows. Section 2 discusses previous work. Sec-
tion 3 introduces our human annotation scheme
for word sense subjectivity and also shows that
subjectivity-ambiguous words are frequent. Sec-
tion 4 describes our proposed classification algo-
rithms in detail. Section 5 presents the experimen-
tal results and evaluation, followed by conclusions
and future work in Section 6.
2 Related Work
There has been extensive research in opinion min-
ing at the document level, for example on product
and movie reviews (Pang et al, 2002; Pang and
Lee, 2004; Dave et al, 2003; Popescu and Etzioni,
2005). Several other approaches focus on the
subjectivity classification of sentences (Kim and
Hovy, 2005; Kudo and Matsumoto, 2004; Riloff
and Wiebe, 2003). They often build on the pres-
ence of subjective words in the sentence to be clas-
sified.
Closer to our work is the large body of work
on the automatic, context-independent classifica-
tion of words according to their polarity, i.e as pos-
itive or negative (Hatzivassiloglou and McKeown,
1997; Turney and Littman, 2003; Kim and Hovy,
2004; Takamura et al, 2005). They use either
co-occurrence patterns in corpora or dictionary-
based methods. Many papers assume that sub-
jectivity recognition, i.e. separating subjective
from objective words, has already been achieved
prior to polarity recognition and test against word
lists containing subjective words only (Hatzivas-
siloglou and McKeown, 1997; Takamura et al,
2005). However, Kim and Hovy (2004) and An-
dreevskaia and Bergler (2006) also address the
classification into subjective/objective words and
show this to be a potentially harder task than po-
larity classification with lower human agreement
and automatic performance.
There are only two prior approaches address-
ing word sense subjectivity or polarity classifi-
cation. Esuli and Sebastiani (2006) determine
the polarity of word senses in WordNet, distin-
guishing among positive, negative and objective.
They expand a small, manually determined seed
set of strongly positive/negative WordNet senses
by following WordNet relations and use the result-
ing larger training set for supervised classification.
The resulting labeled WordNet gives three scores
for each sense, representing the positive, negative
and objective score respectively. However, there
is no evaluation as to the accuracy of their ap-
proach. They then extend their work (Esuli and
Sebastiani, 2007) by applying the Page Rank algo-
rithm to ranking the WordNet senses in terms of
how strongly a sense possesses a given semantic
property (e.g., positive or negative).
Wiebe and Mihalcea (2006) label word senses
in WordNet as subjective or objective. They use a
method relying on distributional similarity as well
as an independent, large manually annotated opin-
ion corpus (MPQA) (Wiebe et al, 2005) for deter-
mining subjectivity. One of the disadvantages of
their algorithm is that it is restricted to senses that
have distributionally similar words in the MPQA
corpus, excluding 23.2% of their test data from au-
tomatic classification.
3 Human Annotation of Word Sense
Subjectivity and Polarity
In contrast to other researchers (Hatzivassiloglou
and McKeown, 1997; Takamura et al, 2005), we
do not see polarity as a category that is depen-
dent on prior subjectivity assignment and therefore
applicable to subjective senses only. We follow
Wiebe and Mihalcea (2006) in that we see subjec-
tive expressions as private states ?that are not open
to objective observation or verification?. This in-
cludes direct references to emotions, beliefs and
judgements (such as anger, criticise) as well as ex-
pressions that let a private state be inferred, for
example by referring to a doctor as a quack. In
contrast, polarity refers to positive or negative as-
sociations of a word or sense. Whereas there is a
dependency in that most subjective senses have a
relatively clear polarity, polarity can be attached to
objective words/senses as well. For example, tu-
berculosis is not subjective ? it does not describe
a private state, is objectively verifiable and would
not cause a sentence containing it to carry an opin-
ion, but it does carry negative associations for the
vast majority of people.
We therefore annotate subjectivity of word
senses similarly to Wiebe and Mihalcea (2006),
826
distinguishing between subjective (S), objective
(O) or both (B). Both is used if a literal and
metaphoric sense of a word are collapsed into one
WordNet synset or if a WordNet synset contains
both opinionated and objective expressions (such
as bastard and illegitimate child in Ex. 3 below).
We expand their annotation scheme by also an-
notating polarity, using the labels positive (P), neg-
ative (N) and varied (V). The latter is used when a
sense?s polarity varies strongly with the context,
such as Example 8 below, where we would ex-
pect uncompromising to be a judgement but this
judgement will be positive or negative depending
on what a person is uncompromising about. To
avoid prevalence of personalised associations, an-
notators were told to only annotate polarity for
subjective senses, as well as objective senses that
carry a strong association likely to be shared by
most people at least in Western culture (such as the
negative polarity for words referring to diseases
and crime). Other objective senses would receive
the label O:NoPol.
Therefore, we have 7 sub categories in total:
O:NoPol, O:P, O:N, S:P, S:N, S:V, and B. The
notation before and after the colon represents the
subjectivity and polarity label respectively. We list
some annotated examples below.
(3) bastard, by-blow, love child, illegitimate child, illegiti-
mate, whoreson? the illegitimate offspring of unmar-
ried parents (B)
(4) atrophy?undergo atrophy; ?Muscles that are not used
will atrophy? (O:N)
(5) guard, safety, safety device?a device designed to pre-
vent injury (O:P)
(6) nasty, awful?offensive or even (of persons) mali-
cious;?in a nasty mood?;?a nasty accident?; ?a nasty
shock? (S:N)
(7) happy?enjoying or showing or marked by joy or plea-
sure or good fortune; ?a happy smile?;?spent many
happy days on the beach?; ?a happy marriage? (S:P)
(8) uncompromising, inflexible?not making concessions;
?took an uncompromising stance in the peace talks?
(S:V)
As far as we are aware, this is the first annotation
scheme for both subjectivity and polarity of word
senses. We believe both are relevant for opinion
extraction: subjectivity for finding and analysing
directly expressed opinions, and polarity for ei-
ther classifying these further or extracting objec-
tive words that, however, serve to ?colour? a text
or present bias rather than explicitly stated opin-
ions. Su and Markert (2008) describe the annota-
tion scheme and agreement study in full.
3.1 Agreement Study
We used the Micro-WNOp corpus containing 1105
WordNet synsets to test our annotation scheme.
3
The Micro-WNOp corpus is representative of the
part-of-speech distribution in WordNet.
Two annotators (both near-native English speak-
ers) independently annotated 606 synsets of the
Micro-WNOp corpus for subjectivity and polarity.
One annotator is the second author of this paper
whereas the other is not a linguist. The overall
agreement using all 7 categories is 84.6%, with a
kappa of 0.77, showing high reliability for a dif-
ficult pragmatic task. This at first seems at odds
with the notion of sentiment as a fuzzy category as
expressed in (Andreevskaia and Bergler, 2006) but
we believe is due to three factors:
? The annotation of senses instead of words
splits most subjectivity-ambiguous words
into several senses, removing one source of
annotation difficulty.
? The annotation of senses in a dictionary pro-
vided the annotators with sense descriptions
in form of Wordnet glosses as well as re-
lated senses, providing more information than
a pure word annotation task.
? The split of subjectivity and polarity annota-
tion made the task clearer and the annotation
of only very strong connotations for objective
word senses ?de-individualized? the task.
As in this paper we are only interested in subjec-
tivity recognition, we collapse S:V, S:P, and S:N
into a single label S and O:NoPol, O:P, and O:N
into a single label O. Label B remains unchanged.
For this three-way annotation overall percentage
agreement is 90.1%, with a kappa of 0.79.
3.2 Gold Standard
After cases with disagreement were negotiated be-
tween the two annotators, a gold standard annota-
tion was agreed upon. Our test set consists of this
agreed set as well as the remainder of the Micro-
WNOp corpus annotated by one of the annotators
alone after agreement was established. This set is
available for research purposes at http://www.
comp.leeds.ac.uk/markert/data.
3
The corpus has originally been annotated by the
providers (Esuli and Sebastiani, 2007) with scores for posi-
tive, negative and objective/no polarity, thus a mixture of sub-
jectivity and polarity annotation. We re-annotated the corpus
with our annotation scheme.
827
How many words are subjectivity-ambiguous?
As the number of senses increases with word fre-
quency, we expect rare words to be less likely
to be subjectivity-ambiguous than frequent words.
The Micro-WNOp corpus contains relatively fre-
quent words so we will get an overestimation of
subjective-ambiguous word types from this cor-
pus, though not necessarily of word tokens. It in-
cludes 298 different words with all their synsets
in WordNet 2.0. Of all words, 97 (32.5%) are
subjectivity-ambiguous, a substantial number.
4 Algorithms
In this section, we present experiments using five
different resources as training sets or clue sets for
this task. The first is the Micro-WNOp corpus with
our own dedicated word sense subjectivity anno-
tation which is used in a standard supervised ap-
proach as training and test set via 10-fold cross-
validation. This technique presupposes a man-
ual annotation effort tailored directly to our task
to provide training data. As it is costly to create
such training sets, we investigate whether exist-
ing resources such as two different subjective sen-
tence lists (Section 4.2) and two different subjec-
tive word lists (Section 4.3) can be adapted to pro-
vide training data or clue sets although they do not
provide any information about word senses. All
resources are used to create training data for su-
pervised approaches; the subjective word lists are
also used in a simple rule-based unsupervised ap-
proach.
All algorithms were tested on the Micro-WNOp
corpus by comparing to the human gold stan-
dard annotation. However, we excluded all senses
with the label both from Micro-WNOp for test-
ing the automatic algorithms, resulting in a final
1061 senses, with 703 objective and 358 subjec-
tive senses. We also compare all algorithms to a
baseline of always assigning the most frequent cat-
egory (objective) to each sense, which results in an
overall accuracy of 66.3%.
4.1 Standard Supervised Approach: 10-fold
Cross-validation (CV) on Micro-WNOp
We use 10-fold cross validation for training and
testing on the annotated synsets in the Micro-
WNOp corpus. We applied a Naive Bayes clas-
sifier,
4
using the following three types of features:
4
We also experimented with KNN, Maximum Entropy,
Rocchio and SVM algorithms and overall Naive Bayes per-
Lexical Features: These are unigrams in the
glosses. We use a bag-of-words approach and filter
out stop words.
As glosses are usually quite short, using a bag-
of-word feature representation will result in high-
dimensional and sparse feature vectors, which of-
ten deteriorate classification performance. In order
to address this problem to some degree, we also ex-
plored other features which are available as train-
ing and test instances are WordNet synsets.
Part-of-Speech (POS) Features: each sense
gets its POS as a feature (adjective, noun, verb or
adverb).
Relation Features: WordNet relations are good
indicators for determining subjectivity as many
of them are subjectivity-preserving. For exam-
ple, if sense A is subjective, then its antonym
sense B is likely to be subjective. We employ
8 relations here?antonym, similar-to, derived-
from, attribute, also-see, direct-hyponym, direct-
hypernym, and extended-antonym. Each relation
R leads to 2 features that describe for a sense A
how many links of that type it has to synsets in the
subjective or the objective training set respectively.
Finally, we represent the feature weights
through a TF*IDF measure.
Considering the size of WordNet (115,424
synsets inWordNet 2.0), the labeled Micro-WNOp
corpus is small. Therefore, the question arises
whether it is possible to adapt other data sources
that provide subjectivity information to our task.
4.2 Sentence Collections: Movie and MPQA
It is reasonable to cast word sense subjectivity
classification as a sentence classification task, with
the glosses that WordNet provides for each sense
as the sentences to be classified. Then we can in
theory feed any collection of annotated subjective
and objective sentences as training data into our
classifier while the annotated Micro-WNOp cor-
pus is used as test data. We experimented with two
different available data sets to test this assumption.
Movie-domain Subjectivity Data Set (Movie):
Pang and Lee (2004) used a collection of labeled
subjective and objective sentences in their work
on review classification.
5
The data set contains
5000 subjective sentences, extracted from movie
reviews collected from the Rotten Tomatoes web
formed best.
5
Available at http://www.cs.cornell.edu/
People/pabo/movie-review-data/
828
site.
6
The 5000 objective sentences were col-
lected from movie plot summaries from the In-
ternet Movie Database (IMDB). The assumption
is that all the snippets from the Rotten Tomatoes
pages are subjective (as they come from a review
site), while all the sentences from IMDB are ob-
jective (as they focus on movie plot descriptions).
The MPQA Corpus contains news articles
manually annotated at the phrase level for opin-
ions, their polarity and their strength. The cor-
pus (Version 1.2) contains 11,112 sentences. We
convert it into a corpus of subjective and objective
sentences following exactly the approach in (Riloff
et al, 2003; Riloff and Wiebe, 2003) and obtain
6127 subjective and 4985 objective sentences re-
spectively. Basically any sentence that contains at
least one strong subjective annotation at the phrase
level is seen as a subjective sentence.
We again use a Naive Bayes algorithm with lex-
ical unigram features. Note that part-of-speech
and relation features are not applicable here as the
training set consists of corpus sentences, notWord-
Net synsets.
4.3 Word Lists: General Inquirer and
Subjectivity List
Several word lists annotated for subjectivity or po-
larity such as the General Inquirer (GI)
7
or the sub-
jectivity clues list (SL) collated by Janyce Wiebe
and her colleagues
8
are available.
The General Inquirer (GI) was developed by
Philip Stone and colleagues in the 1960s. It con-
centrates on word polarity. Here we make the
simple assumption that both positive and negative
words in the GI list are subjective clues whereas
all other words are objective.
The Subjectivity Lexicon (SL) centers on
subjectivity so that it is ideally suited for our
task. It provides fine-grained information for each
clue, such as part-of-speech, subjectivity strength
(strong/weak), and prior polarity (positive, nega-
tive, or neutral). For example, object(verb) is a
subjective clue whereas object(noun) is objective.
Regarding strength, the adjective evil is marked as
strong subjective whereas the adjective exposed is
marked as a weak subjective clue.
Both lexica do not include any information
about word senses and therefore cannot be used
directly for subjectivity assignment at the sense
6
http://www.rottentomatoes.com/
7
http://www.wjh.harvard.edu/
?
inquirer/
8
http://www.cs.pitt.edu/mpqa/
level. For example, at least one sense of any
subjectivity-ambiguous word will be labeled incor-
rectly if we just adopt a word-based label. In addi-
tion, these lists are far from complete: compared to
the over 100,000 synsets in WordNet, GI contains
11,788 words marked for polarity (1915 positive,
2291 negative and 7582 no-polarity words) and the
SL list contains about 8,000 subjective words.
Still, it is a reasonable assumption that any gloss
that contains several subjective words indicates a
subjective sense overall. This intuition is strength-
ened by the characteristics of glosses. They nor-
mally are short and concise without a complex syn-
tactic structure, thus the occurrence of subjective
words in such a short string is likely to indicate
a subjective sense overall. This contrasts, for ex-
ample, with sentences in newspapers where one
clause might express an opinion, whereas other
parts of the sentence are objective.
Therefore, for the rule-based unsupervised
algorithm we lemmatized and POS-tagged the
glosses in the Micro-WNOp test set. Then we
compute a subjectivity score S for each synset by
summing up the weight values of all subjectivity
clues in its gloss. If S is equal or higher than an
agreed threshold T, then the synset is classified as
subjective, otherwise as objective. For the GI lexi-
con, all subjectivity clues have the same weight 1,
whereas for the SL list we assign a weight value
2 to strongly subjective clues and 1 to weakly
subjective clues. We experimented with several
thresholds T and report here the results for the best
thresholds, which were 2 for SL and 4 for the GI
word list. The corresponding methods are called
Rule-SL and Rule-GI.
This approach does not allow us to easily inte-
grate relational WordNet features. It might also
suffer from the incompleteness of the lexica and
the fact that it has to make decisions for bor-
derline cases (at the value of the threshold set).
We therefore explored instead to generate larger,
more reliable training data consisting of Word-
Net synsets from the word lists. To achieve this,
we assign a subjectivity score S as above to all
WordNet synsets (excluding synsets in the test set).
If S is higher or equal to a threshold T
1
it is added
to the subjective training set, if it is lower or equal
to T
2
it is added to the objective training set. This
allows us to choose quite clear thresholds so that
borderline cases with a score between T
1
and T
2
are not in the training set. It also allows to use part-
829
of-speech and relational features as the training set
then consists of WordNet synsets. In this way,
we can automatically generate (potentially noisy)
training data of WordNet senses marked for sub-
jectivity without annotating any WordNet senses
manually for subjectivity.
We experimented with several different thresh-
old sets but we found that they actually have a min-
imal impact on the final results. We report here the
best results for a threshold T
1
of 4 and T
2
of 2 for
the SL lexicon and of 3 and 1 respectively for the
GI word list.
5 Experiments and Evaluation
We measure the classification performance with
overall accuracy as well as precision, recall and
balanced F-score for both categories (objective and
subjective). All results are summarised in Table 1.
Results are compared to the baseline of majority
classification using a McNemar test at the signifi-
cance level of 5%.
5.1 Experimental Results
Table 1 shows that SL
?
performs best among all
the methodologies. All CV, Rule-SL and SL meth-
ods significantly beat the baseline. In addition, if
we compare the results of methods with and with-
out additional parts-of-speech and WordNet rela-
tion features, we see a small but consistent im-
provement when we use additional features. It is
also worthwhile to expand the rule-based unsuper-
vised method into a method for generating train-
ing data and use additional features as SL
?
signifi-
cantly outperforms Rule-SL.
5.2 Discussion
Word Lists. Surprisingly, using SL greatly outper-
forms GI, regardless of whether we use the super-
vised or unsupervised method or whether we use
lexical features only or the other features as well.
9
There are several reasons for this. First, the GI
lexicon is annotated for polarity, not subjectivity.
More specifically, words that we see as objective
but with a strong positive or negative association
(such as words for crimes) and words that we see
as subjective are annotated with the same polar-
ity label in the GI lexicon. Therefore, the GI def-
inition of subjectivity does not match ours. Also,
9
This pattern is repeated for all threshold combinations,
which are not reported here.
the GI lexicon does not operate with a clearly ex-
pressed polarity definition, leading to conflicting
annotations and casting doubt on its widespread
use in the opinion mining community as a gold
standard (Turney and Littman, 2003; Takamura et
al., 2005; Andreevskaia and Bergler, 2006). For
example, amelioration is seen as non-polar in GI
but improvement is annotated with positive polar-
ity. Second, in contrast to SL, GI does not consider
different parts-of-speech of a word and subjectiv-
ity strength (strong/weak subjectivity). Third, GI
contains many fewer subjective clues than SL.
Sentence Data. When using the Movie dataset
and MPQA corpus as training data, the results
are not satisfactory. We first checked the purity
of these two datasets to see whether they are too
noisy. For this purpose, we used a naive Bayes
algorithm with unigram features and conducted a
10-fold cross validation experiment on recognizing
subjective/objective sentences within the Movie
dataset and MPQA independently. Interestingly,
the accuracy for the Movie dataset and MPQA cor-
pus achieved 91% and 76% respectively. Consid-
ering that they are balanced datasets with a most
frequent category baseline of about 50%, this ac-
curacy is high, especially for the Movie dataset.
However, again the subjectivity definition in the
Movie corpus does not seem to match ours. Re-
call that we see a word sense or a sentence as sub-
jective if it expresses a private state (i.e., emotion,
opinion, sentiment, etc.), and objective otherwise.
Inspecting the movie data set, we found that in-
deed the sentences included in its subjective set
would mostly be seen as subjective in our sense
as well as they contain opinions about the movie
such as it desperately wants to be a wacky , screw-
ball comedy , but the most screwy thing here is how
so many talented people were convinced to waste
their time. It is also true that the sentences (plot
descriptions) in its ?objective? data set relatively
rarely contain opinions about the movie. How-
ever, they still contain other opinionated content
like opinions and emotions of the characters in the
movie such as the obsession of a character with
John Lennon in the beatles fan is a drama about
Albert, a psychotic prisoner who is a devoted fan
of John Lennon and the beatles. Since the data
set?s definition of subjective sentences is closer
to ours than the one for objective sentences, we
conducted a one-class learning approach (Li and
Liu, 2003) using Movie?s subjective sentences as
830
Table 1: Results
Method Subjective Objective Accuracy
Precision Recall F-score Precision Recall F-score
Baseline N/A 0 N/A 66.3% 100% 79.7% 66.3%
CV 65.2% 52.8% 58.3% 78.1% 85.6% 81.7% 74.6%?
CV
?
69.5% 55.3% 61.6% 79.4% 87.6% 83.3% 76.7%?
Movie 43.8% 60.1% 50.6% 74.9% 60.7% 67.1% 60.5%
MPQA 44.5% 78.5% 56.8% 82.1% 50.1% 62.2% 59.7%
GI 50.4% 39.4% 44.2% 72.2% 80.2% 76.0% 66.4%
GI
?
54.5% 33.5% 41.5% 71.7% 85.8% 78.1% 68.1%
SL 64.3% 62.8% 63.6% 81.3% 82.2% 81.8% 75.7%?
SL
?
66.2% 64.5% 65.3% 82.2% 83.2% 82.7% 76.9%?
Rule-GI 38.5% 5.6% 9.8% 66.5% 95.4% 78.4% 65.1%
Rule-SL 59.7% 70.4% 64.6% 83.4% 75.8% 79.4% 74.0%?
1
CV, GI and SL correspond to methods using lexical features only.
2
CV
?
, GI
?
and SL
?
correspond to methods using a feature combination of lexical,
part-of-speech, and WordNet relations.
3
? indicates results significantly better than the baseline.
the only training data. The algorithm
10
combines
Expectation Maximization and Naive Bayes algo-
rithms, and we used randomly extracted 50,000
unlabeled synsets in WordNet as the necessary un-
labeled data. This approach achieves an accuracy
of 69.4% on Micro-WNOp, which is significantly
better than the baseline.
The subjectivity definition in the MPQA corpus
is quite close to ours. However, our mapping from
its phrase annotation to sentence annotation might
be too coarse-grained as many sentences in the cor-
pus span several clauses containing both opinions
and factual description. We assume that this is pos-
sibly also the reason why its purity is lower than
in the Movie dataset. We therefore experimented
again with a one-class learning approach using just
the subjective phrases in MPQA as training data.
The accuracy does improve to 67.6% but is still
not significantly higher than the baseline.
5.3 Comparison to Prior Approaches
Esuli and Sebastiani (2006) make their labeled
WordNet SentiWordNet 1.0 publically available.
11
Recall that they actually use polarity classification:
however, as there is a dependency between po-
larity and subjectivity classification for subjective
senses, we map their polarity scores to our subjec-
tivity labels as follows. If the sum of positive and
10
Available at http://www.cs.uic.edu/?liub/LPU/.
11
Available at http://sentiwordnet.isti.cnr.
it/
negative scores of a sense in SentiWordNet is more
than or equal to 0.5, then it is subjective and other-
wise objective.
12
Using this mapping, it achieves
an accuracy of 75.3% on the Micro-WNOp cor-
pus, compared to our gold standard. Therefore our
methods CV
?
and SL
?
perform slightly better than
theirs, although the improvement is not significant.
The task definition in Wiebe and Mihal-
cea (2006) is much more similar to ours but they
use different annotated test data, which is not pub-
lically available, so an exact comparison is not pos-
sible. Both data sets, however, seem to include rel-
atively frequent words. One disadvantage of their
method is that it is not applicable to all WordNet
senses as it is dependent on distributionally sim-
ilar words being available in the MPQA. Thus,
23% of their test data is excluded from evaluation,
whereas our methods can be used on any WordNet
sense. They measure precision and recall for sub-
jective senses in a precision/recall curve: Precision
is about 48/9% at a recall of 60% for subjective
senses whereas our best SL
?
method has a preci-
sion of 66% at about the same recall. Although
this suggests better performance of our method, it
is not possible to draw final conclusions from this
comparison due to the data set differences.
12
We experimented with slightly different mappings but
this mapping gave SentiWordNet the best possible result.
There is a relatively large number of cases with a 0.5/0.5 split
in SentiWordNet, making it hard to decide between subjective
and objective senses.
831
6 Conclusion and Future Work
We proposed different ways of extracting training
data and clue sets for word sense subjectivity label-
ing from existing opinion mining resources. The
effectiveness of the resulting algorithms depends
greatly on the generated training data, more specif-
ically on the different definitions of subjectivity
used in resource creation. However, we were able
to show that at least one of these methods (based
on the SL word list) resulted in a classifier that per-
formed on a par with a supervised classifier that
used dedicated training data developed for this task
(CV). Thus, it is possible to avoid any manual an-
notation for the subjectivity classification of word
senses.
Our future work will explore new methodolo-
gies in feature representation by importing more
background information (e.g., syntactic informa-
tion). Furthermore, our current method of integrat-
ing the rich relation information in WordNet (us-
ing them as standard features) does not use joint
classification of several senses. Instead, we think
it will be more promising to use the relations to
construct graphs for semi-supervised graph-based
learning of word sense subjectivity. In addition, we
will also explore whether the derived sense labels
improve applications such as sentence classifica-
tion and clustering WordNet senses.
References
Andreevskaia, Alina and Sabine Bergler. 2006. Min-
ing WordNet for Fuzzy Sentiment: Sentiment Tag
Extraction from WordNet Glosses. Proceedings of
EACL?06.
Dave, Kushal, Steve Lawrence, and David Pennock.
2003. Mining the Peanut Gallery: Opinion Extrac-
tion and Semantic Classification of Product Reviews.
Proceedings of WWW?03.
Esuli, Andrea and Fabrizio Sebastiani. 2006. Senti-
WordNet: A Publicly Available Lexical Resource for
Opinion Mining. Proceedings of LREC?06.
Esuli, Andrea and Fabrizio Sebastiani. 2007. PageR-
anking WordNet Synsets: An application to Opinion
Mining. Proceedings of ACL?07.
Hatzivassiloglou, Vasileios and Kathleen McKeown.
1997. Predicting the Semantic Orientation of Ad-
jectives. Proceedings of ACL?97.
Kim, Soo-Min and Eduard Hovy. 2004. Determining
the Sentiment of Opinions. Proceedings of COL-
ING?04.
Kim, Soo-Min and Eduard Hovy. 2005. Automatic
Detection of Opinion Bearing Words and Sentences.
Proceedings of ICJNLP?05.
Kudo, Taku and Yuji Matsumoto. 2004. A Boosting
Algorithm for Classification of Semi-structured Text.
Proceedings of EMNLP?04.
Li, Xiaoli and Bing Liu. 2003. Learning to classify
text using positive and unlabeled data. Proceedings
of IJCAI?03.
Pang, Bo and Lillian Lee. 2004. A Sentiment Edu-
cation: Sentiment Analysis Using Subjectivity sum-
marization Based on Minimum Cuts. Proceedings of
ACL?04.
Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification us-
ing Machine Learning Techniques. Proceedings of
EMNLP?02.
Popescu, Ana-Maria and Oren Etzioni. 2003. Ex-
tracting Product Fatures and Opinions from Reviews
Proceedings of EMNLP?05.
Riloff, Ellen, JanyceWiebe, and TheresaWilson. 2003.
Learning Subjective Nouns using Extraction Pattern
Bootstrapping. Proceedings of CoNLL?03
Riloff, Ellen and Janyce Wiebe. 2003. Learning Ex-
traction Patterns for Subjective Expressions. Pro-
ceedings of EMNLP?03.
Su, Fangzhong and Katja Markert. 2008. Elicit-
ing Subjectivity and Polarity Judgements on Word
Senses. Proceedings of Coling?08 workshop of Hu-
man Judgements in Computational Linguistics.
Takamura, Hiroya, Takashi Inui, and Manabu Oku-
mura. 2005. Extracting Semantic Orientations of
Words using Spin Model. Proceedings of ACL?05.
Turney, Peter and Michael Littman. 2003. Measuring
Praise and Criticism: Inference of Semantic Orien-
tation from Association. ACM Transaction on Infor-
mation Systems.
Wiebe, Janyce and Rada Micalcea. 2006. Word Sense
and Subjectivity. Proceedings of ACL?06.
Wiebe, Janyce, Theresa Wilson, and Claire Cardie.
2005. Annotating Expressions of Opinions and
Emotions in Language. Language Resources and
Evaluation.
832
