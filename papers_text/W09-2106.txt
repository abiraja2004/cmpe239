Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 43?46,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
An Application of Latent Semantic Analysis to Word Sense Discrimination
for Words with Related and Unrelated Meanings
Juan Pino and Maxine Eskenazi
(jmpino, max)@cs.cmu.edu
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
Abstract
We present an application of Latent Semantic
Analysis to word sense discrimination within
a tutor for English vocabulary learning. We
attempt to match the meaning of a word in a
document with the meaning of the same word
in a fill-in-the-blank question. We compare
the performance of the Lesk algorithm to La-
tent Semantic Analysis. We also compare the
performance of Latent Semantic Analysis on a
set of words with several unrelated meanings
and on a set of words having both related and
unrelated meanings.
1 Introduction
In this paper, we present an application of Latent
Semantic Analysis (LSA) to word sense discrimi-
nation (WSD) within a tutor for English vocabu-
lary learning for non-native speakers. This tutor re-
trieves documents from the Web that contain tar-
get words a student needs to learn and that are at
an appropriate reading level (Collins-Thompson and
Callan, 2005). It presents a document to the student
and then follows the document reading with practice
questions that measure how the student?s knowledge
has evolved. It is important that the fill-in-the-blank
questions (also known as cloze questions) that we
ask to the students allow us to determine their vocab-
ulary knowledge accurately. An example of cloze
question is shown in Figure 1.
Some words have more than one meaning and so
the cloze question we give could be about a different
meaning than the one that the student learned in the
document. This is something that can lead to confu-
sion and must be avoided. To do this, we need to use
some automatic measure of semantic similarity.
Figure 1: Example of cloze question.
To define the problem formally, given a target
word w, a string r (the reading) containing w and
n strings q1, ..., qn (the sentences used for the ques-
tions) each containing w, find the strings qi where
the meaning of w is closest to its meaning in r.
We make the problem simpler by selecting only one
question.
This problem is challenging because the context
defined by cloze questions is short. Furthermore,
a word can have only slight variations in meaning
that even humans find sometimes difficult to distin-
guish. LSA was originally applied to Information
Retrieval (Dumais et al, 1988). It was shown to be
able to match short queries to relevant documents
even when there were no exact matches between the
words. Therefore LSA would seem to be an appro-
priate technique for matching a short context, such
as a question, with a whole document.
So we are looking to first discriminate between
the meanings of words, such as ?compound?, that
have several very different meanings (a chemical
compound or a set of buildings) and then to dis-
ambiguate words that have senses that are closely
related such as ?comprise? (?be composed of? or
?compose?). In the following sections, we present
43
LSA and some of its applications, then we present
some experimental results that compare a baseline to
the use of LSA for both tasks we have just described.
We expect the task to be easier on words with unre-
lated meanings. In addition, we expect that LSA will
perform better when we use context selection on the
documents.
2 Related Work
LSA was originally applied to Information Retrieval
(Dumais et al, 1988) and called Latent Semantic In-
dexing (LSI). It is based on the singular value de-
composition (SVD) theorem. A m ? n matrix X
with m ? n can be written as X = U ?S ?V T where
U is a m ? n matrix such that UT ? U = Im; S is
a n?n diagonal matrix whose diagonal coefficients
are in decreasing order; and V is a n?n matrix such
that V T ? V = In.
X is typically a term-document matrix that repre-
sents the occurrences of vocabulary words in a set of
documents. LSI uses truncated SVD, that is it con-
siders the first r columns of U (written Ur), the r
highest coefficients in S (Sr) and the first r columns
of V (Vr). Similarity between a query and a docu-
ment represented by vectors d and q is performed by
computing the cosine similarity between S?1r ?UTr ?d
and S?1r ?UTr ?q. The motivation for computing sim-
ilarity in a different space is to cope with the sparsity
of the vectors in the original space. The motivation
for truncating SVD is that only the most meaning-
ful semantic components of the document and the
query are represented after this transformation and
that noise is discarded.
LSA was subsequently applied to number of prob-
lems, such as synonym detection (Landauer et al,
1998), document clustering (Song and Park, 2007),
vocabulary acquisition simulation (Landauer and
Dumais, 1997), etc.
Levin and colleagues (2006) applied LSA to word
sense discrimination. They clustered documents
containing ambiguous words and for a test instance
of a document, they assigned the document to its
closest cluster. Our approach is to assign to a doc-
ument the question that is closest. In addition, we
examine the cases where a word has several unre-
lated meanings and where a word has several closely
related meanings.
3 Experimental Setup
We used a database of 62 manually generated cloze
questions covering 16 target words1. We manually
annotated the senses of the target words in these
questions using WordNet senses (Fellbaum, 1998).
For each word and for each sense, we manually gath-
ered documents from the Web containing the target
word with the corresponding sense. There were 84
documents in total. We added 97 documents ex-
tracted from the tutor database of documents that
contained at least one target word but we did not an-
notate their meaning.
We wanted to evaluate the performances of LSA
for WSD for words with unrelated meanings and for
words with both related and unrelated meanings. For
the first type of evaluation, we retained four target
words. For the second type of evaluation, all 16
words were included. We also wanted to evaluate
the influence of the size of the context of the tar-
get words. We therefore considered two matrices:
a term-document matrix and a term-context matrix
where context designates five sentences around the
target word in the document. In both cases each
cell of the matrix had a tf-idf weight. Finally, we
wanted to investigate the influence of the dimension
reduction on performance. In our experiments, we
explored these three directions.
4 Results
4.1 Baseline
We first used a variant of the Lesk algorithm (Lesk,
1986), which is based on word exact match. This al-
gorithm seems well suited for the unsupervised ap-
proach we took here since we were dealing with
discrimination rather than disambiguation. Given
a document d and a question q, we computed the
number of word tokens that were shared between d
and q, excluding the target word. The words were
lower cased and stemmed using the Porter stem-
mer. Stop words and punctuation were discarded;
we used the standard English stopword list. Finally,
we selected a window of nw words around the tar-
get word in the question q and a window of ns
sentences around the target word in the document
d. In order to detect sentence boundaries, we used
1available at: www.cs.cmu.edu/ jmpino/questions.xls
44
the OpenNLP toolkit (Baldridge et al, 2002). With
nw = 10 and ns = 2, we obtained an accuracy of
61% for the Lesk algorithm. This can be compared
to a random baseline of 44% accuracy.
4.2 LSA
We indexed the document database using the Lemur
toolkit (Allan et al, 2003). The database contained
both the manually annotated documents and the doc-
uments used by the tutor and containing the target
words. The Colt package (Binko et al, ) was used
to perform singular value decomposition and matrix
operations because it supports sparse matrix oper-
ations. We explored three directions in our analy-
sis. We investigated how LSA performs for words
with related meanings and for words with unrelated
meanings. We also explored the influence of the
truncation parameter r. Finally, we examined if re-
ducing the document to a selected context of the tar-
get word improved performance.
Figures 2 and 3 plot accuracy versus dimension
reduction in different cases. In all cases, LSA out-
performs the baseline for certain values of the trun-
cation parameter and when context selection was
used. This shows that LSA is well suited for measur-
ing semantic similarity between two contexts when
at least one of them is short. In general, using the full
dimension in SVD hurts the performances. Dimen-
sion reduction indeed helps discarding noise and
noise is certainly present in our experiments since
we do not perform stemming and do not use a stop-
word list. One could argue that filling the matrix
cells with tf-idf weights already gives less impor-
tance to noisy words.
Figure 2 shows that selecting context in docu-
ments does not give much improvement in accuracy.
It might be that the amount of context selected de-
pends on each document. Here we had a fixed size
context of five sentences around the target word.
In Figure 3, selecting context gives some im-
provement, although not statistically significant,
over the case with the whole document as context.
The best performance obtained for words with un-
related meanings and context selection is also better
than the performance for words with related and un-
related meanings.
 0
 0.2
 0.4
 0.6
 0.8
 1
 60  80  100  120  140  160  180
Ac
cu
ra
cy
Truncation Parameter
Accuracy vs. Dimension Reduction for Related Meanings with Different Contexts
whole document
selected context
Lesk baseline
Figure 2: Accuracy vs. r, the truncation parameter,
for words with related and unrelated meanings and with
whole document or selected context (95% confidence for
whole document: [0.59; 0.65], 95% confidence for se-
lected context: [0.52; 0.67])
 0
 0.2
 0.4
 0.6
 0.8
 1
 5  10  15  20  25  30  35
Ac
cu
ra
cy
Truncation Parameter
Accuracy vs. Dimension Reduction for Unrelated Meanings with Different Contexts
whole document
selected context
Lesk baseline
Figure 3: Accuracy vs. r, the truncation parameter, for
words with unrelated meanings only and with whole doc-
uments or selected context ((95% confidence for whole
document: [0.50; 0.59], 95% confidence for selected con-
text: [0.52; 0.71]))
45
5 Discussion
LSA helps overcome sparsity of short contexts such
as questions and gives an improvement over the ex-
act match baseline. However, reducing the context
of the documents to five sentences around the tar-
get word does not seem to give significant improve-
ment. This might be due to the fact that capturing
the right context for a meaning is a difficult task
and that a fixed size context does not always rep-
resent a relevant context. It is yet unclear how to set
the truncation parameter. Although dimension re-
duction seems to help, better results are sometimes
obtained when the truncation parameter is close to
full dimension or when the truncation parameter is
farther from the full dimension.
6 Conclusion and Future Work
We have shown that LSA, which can be considered
as a second-order representation of the documents
and question vectors, is better suited than the Lesk
algorithm, which is a first-order representation of
vectors, for measuring semantic similarity between
a short context such as a question and a longer con-
text such as a document. Dimension reduction was
shown to play an important role in the performances.
However, LSA is relatively difficult to apply to large
amounts of data because SVD is computationally in-
tensive when the vocabulary size is not limited. In
the context of tutoring systems, LSA could not be
applied on the fly, the documents would need to be
preprocessed and annotated beforehand.
We would like to further apply this promising
technique for WSD. Our tutor is able to provide def-
initions when a student is reading a document. We
currently provide all available definitions. It would
be more beneficial to present only the definitions
that are relevant to the meaning of the word in the
document or at least to order them according to their
semantic similarity with the context. We would also
like to investigate how the size of the selected con-
text in a document can affect performance. Finally,
we would like to compare LSA performance to other
second-order vector representations such as vectors
induced from co-occurrence statistics.
Acknowledgments
Thanks Mehrbod Sharifi, David Klahr and Ken
Koedinger for fruitful discussions. This research is
supported by NSF grant SBE-0354420. Any conclu-
sions expressed here are those of the authors.
References
James Allan, Jamie Callan, Kevin Collins-Thompson,
Bruce Croft, Fangfang Feng, David Fisher, John Laf-
ferty, Leah Larkey, Thi N. Truong, Paul Ogilvie, et al
2003. The lemur toolkit for language modeling and
information retrieval.
Jason Baldridge, Thomas Morton, and Gann Bierner.
2002. The opennlp maximum entropy package. Tech-
nical report, Technical report, SourceForge.
Pavel Binko, Dino Ferrero Merlino, Wolfgang Hoschek,
Tony Johnson, Andreas Pfeiffer, et al Open source
libraries for high performance scientific and technical
computing in java.
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical language
models. Journal of the American Society for Informa-
tion Science and Technology, 56(13):1448?1462.
Susane T. Dumais, George W. Furnas, Thomas K.
Landauer, Scott Deerwester, and Richard Harshman.
1988. Using latent semantic analysis to improve ac-
cess to textual information. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 281?285.
Christiane Fellbaum. 1998. WordNet: An electronic lex-
ical database. MIT press.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to plato?s problem: The latent semantic analysis
theory of acquisition, induction and representation of
knowledge. Psychological review, 104:211?240.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse processes, 25:259?284.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
5th annual international conference on Systems Docu-
mentation, pages 24?26.
Esther Levin, Mehrbod Sharifi, and Jerry Ball. 2006.
Evaluation of utility of lsa for word sense discrimina-
tion. In Proceedings of HLT/NAACL, pages 77?80.
Wei Song and Soon Cheol Park. 2007. A novel docu-
ment clustering model based on latent semantic analy-
sis. In Proceedings of the Third International Confer-
ence on Semantics, Knowledge and Grid, pages 539?
542.
46
