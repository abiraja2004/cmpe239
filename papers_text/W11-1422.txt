Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 180?189,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
High-Order Sequence Modeling for Language Learner Error Detection 
Michael Gamon 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 
mgamon@microsoft.com 
  
 
Abstract 
We address the problem of detecting Eng-
lish language learner errors by using a dis-
criminative high-order sequence model. 
Unlike most work in error-detection, this 
method is agnostic as to specific error 
types, thus potentially allowing for higher 
recall across different error types.  The ap-
proach integrates features from many 
sources into the error-detection model, 
ranging from language model-based fea-
tures to linguistic analysis features. Evalua-
tion results on a large annotated corpus of 
learner writing indicate the feasibility of 
our approach on a realistic, noisy and in-
herently skewed set of data. High-order 
models consistently outperform low-order 
models in our experiments. Error analysis 
on the output shows that the calculation of 
precision on the test set represents a lower 
bound on the real system performance. 
1. Introduction 
Systems for automatic detection and correction of 
errors in native writing have been developed for 
many decades. Early in the development of these 
systems, the approach was exclusively based on 
knowledge engineering. Hand-crafted grammars 
would analyze a sentence and would contain spe-
cial mechanisms for rule or constraint relaxation 
that allow ungrammatical sentences to produce a 
parse, while at the same time indicating that a 
grammatical error is present. More recently, data-
driven methods have assumed prominence and 
there has been an emerging area of research into 
the challenge of detecting and correcting errors in 
learner language (for an overview see Leacock et 
al. 2010). Data-driven methods offer the familiar 
set of advantages: they can be more flexible than a 
manually maintained set of rules and they tend to 
cope better with noisy input. Drawbacks include 
the inability to handle linguistically more complex 
errors that involve long distance dependencies such 
as subject-verb agreement. Learner errors as a tar-
get for error detection and correction pose a partic-
ular challenge but also offer some unique 
opportunities. The challenge lies in the density of 
errors (much higher than in native writing), the 
variety of errors (a superset of typical native er-
rors) and the generally more non-idiomatic writing. 
On the other hand, the availability of annotated 
corpora, often comprised of manually corrected 
learner essays or scripts, provides a big advantage 
for the evaluation and training of data-driven sys-
tems.  
Data-driven systems for English learner error 
detection and correction typically target a specific 
set of error types and contain a machine learned 
component for each error type. For example, such 
a system may have a classifier that determines the 
correct choice of preposition given the lexical and 
syntactic part-of-speech (POS) context and hence 
can aid the learner with the notoriously difficult 
problem of identifying an appropriate preposition. 
Similarly, a classifier can be used to predict the 
correct choice of article in a given context. Such 
targeted systems have the advantage that they often 
achieve relatively high precision at, of course, the 
cost of recall. However, while there are a few ma-
jor learner error categories, such as prepositions 
and articles, there is also a long tail of content 
word and other errors that is not amenable to a tar-
geted approach. 
In this paper, we depart from the error-specific 
paradigm and explore a sequence modeling ap-
proach to general error detection in learner writing. 
This approach is completely agnostic as to the er-
ror type. It attempts to predict the location of an 
180
error in a sentence based on observations gathered 
from a supervised training phase on an error-
annotated learner corpus. Features used here are 
based on an n-gram language model, POS tags, 
simple string features that indicate token length 
and capitalization, and linguistic analysis by a con-
stituency parser. We train and evaluate the method 
on a sizeable subset of the corpus. We show the 
contribution of the different feature types and per-
form a manual error analysis to pinpoint shortcom-
ings of the system and to get a more accurate idea 
of the system?s precision. 
2. Related work 
Error-specific approaches comprise the majority of 
recent work in learner error detection. Two of the 
most studied error types in learner English are 
preposition and article errors since they make up a 
large percentage of errors in learner writing (16% 
and 13% respectively in the Cambridge Learner 
Corpus, without considering spelling and punctua-
tion errors). The most widely used approach for 
detecting and correcting these errors is classifica-
tion, with lexical and POS features gleaned from a 
window around the potential preposition/article 
site in a sentence. Some recent work includes Cho-
dorow et al (2007), De Felice and Pulman (2008), 
Gamon (2010), Han et al (2010), Izumi et al 
(2004), Tetreault and Chodorow (2008), Ro-
zovskaya and Roth (2010a, 2010b). Gamon et al 
(2008) and Gamon (2010) used a language model 
in addition to a classifier and combined the classi-
fier output and language model scores in a meta-
classifier. These error-specific methods achieve 
high precision (up to 80-90% on some corpora) but 
only capture highly constrained error types such as 
preposition and determiner errors. 
There has also been research on error-detection 
methods that are not designed to identify a specific 
error type. The basic idea behind these error-
agnostic approaches is to identify an error where 
there is a particularly unlikely sequence compared 
to the patterns found in a large well-formed corpus. 
Atwell (1986) used low-likelihood sequences of 
POS tags as indicators for the presence of an error. 
Sj?bergh (2005) used a chunker to detect unlikely 
chunks in native Swedish writing compared to the 
chunks derived from a large corpus of well-formed 
Swedish writing. Bigert and Knutsson (2002) em-
ployed a statistical method to identify a variety of 
errors in Swedish writing as rare sequences of 
morpho-syntactic tags. They significantly reduced 
false positives by using additional methods to de-
termine whether the unexpected sequence is due to 
phrase or sentence boundaries or due to rare single 
tags. Chodorow and Leacock (2000) utilized mutu-
al information and chi-square statistics to identify 
typical contexts for a small set of targeted words 
from a large well-formed corpus. Comparing these 
statistics to the ones found in a novel sentence, 
they could identify unlikely contexts for the target-
ed words that were often good indicators of the 
presence of an error. Sun et al (2007) mined for 
patterns that consist of POS tags and function 
words. The patterns are of variable length and can 
also contain gaps. Patterns were then combined in 
a classifier to distinguish correct from erroneous 
sentences. Wagner et al (2007) combined parse 
probabilities from a set of statistical parsers and 
POS tag n-gram probabilities in a classifier to de-
tect ungrammatical sentences. Okanohara and Tsu-
jii (2007) differed from the previous approaches in 
that they directly used discriminative language 
models to distinguish correct from incorrect sen-
tences, without the direct modeling of error-
indicating patterns. Park and Levy (2011) use a 
noisy channel model with a base language model 
and a set of error-specific noise models for error 
detection and correction. 
In contrast to previous work, we cast the task as 
a sequence modeling problem. This provides a 
flexible framework in which multiple statistical 
and linguistic signals can be combined and cali-
brated by supervised learning. The approach is er-
ror-agnostic and can easily be extended with 
additional statistical or linguistic features. 
3. Error detection by sequence modeling 
Errors consist of a sub-sequence of tokens in a 
longer token sequence. They can be identified by a 
combination of internal and contextual features, 
the latter requiring a notion of Markov window (a 
window around a token in which relevant infor-
mation is likely to be found). This is similar to 
tasks such as named entity recognition (NER) or 
part-of-speech tagging, where sequence modeling 
has proven to be successful.  
We choose a Maximum Entropy Markov Model 
(MEMM, McCallum et al 2000) as the modeling 
technique. In NER, the annotation convention uses 
181
three labels for a token ?O? (outside of NE), ?B? 
(beginning of NE), and ?I? (inside of NE). For our 
purpose we reduced the set of labels to just ?O? 
and ?I? since most of the errors are relatively short. 
Conditional Random Fields (Lafferty et al 
2001) are considered to be superior to MEMMs in 
learning problems affected by label bias (Bottou 
1991). In our scheme, however, there are only two 
states ?O? and ?I?, and both states can transition to 
each other. Since there are no states with asymmet-
ric transition properties that would introduce a bias 
towards states with fewer transitions, label bias is 
not a problem for us. 
Figure 1 shows the structure of our MEMM with 
a Markov order of five (the diagram only shows 
the complete set of arcs for the last state). The in-
put sentence contains the token sequence the past 
year I was stayed ? with the error was stayed. In-
stead of using the tokens themselves as observa-
tions, we chose to use POS tags assigned by an 
automatic tagger (Toutanova et al 2003). This 
choice was motivated by data sparseness. Learning 
a model that observes individual lexical items and 
predicts a sequence of error/non-error tags would 
be ideal, but given the many different error types 
and triggering contexts for an error, such a model 
would require much more training data. A large set 
of features that serve as constraints on the state 
transition models are extracted for each state. The-
se features are described in Section 5.  
Note that the model structure would lend itself 
to a factorial conditional random field (McCallum 
et al 2003) which allows the joint labeling of POS 
tags and state labels. This would, however, require 
training data that is labeled for both errors and 
POS tags. 
 
Figure 1: MEMM model for error detection, the 
full set of dependencies is only shown for the last 
state. 
4. Detecting errors in the Cambridge 
Learner Corpus  
The learner corpus used to train and evaluate the 
system is the Cambridge Learner Corpus (CLC). It 
consists of essays (scripts) written as part of the 
University of Cambridge English for Speakers of 
Other Languages (ESOL) examinations. The cor-
pus contains about 30 million words of learner 
English. All errors are annotated and include, when 
possible, a single suggested correction. Errors are 
categorized into 87 error types. 
We performed a number of preprocessing steps 
on the data. On the assumption that learners have 
access to a spell checker, errors that were marked 
as spelling errors were corrected based on the an-
notations. Confused words (their/there) were treat-
ed in the same way, given that they are corrected 
by a modern proofing tool such as the one in Mi-
crosoft Word. In addition, British English spelling 
conventions were changed to those of American 
English. Sentences containing errors that had no 
suggested rewrite were eliminated. Finally, only 
lexical errors are covered in this work. For punctu-
ation and capitalization we removed the error an-
notations, retaining the original (erroneous) 
punctuation and capitalization. 
We grouped the remaining 60 error classifica-
tions into eight categories: Content word, Inflec-
tional morphology, Noun phrase errors, 
Preposition errors, Multiple errors, Other errors 
involving content words, Other errors involving 
function words and Derivational morphology. The 
distribution of error categories is shown in Table 1. 
Error Class Freq Pct 
Content word insertion, dele-
tion or choice 
185,201 21% 
Inflectional morphology and 
agreement of content words 
157,660 18% 
Noun phrase formation: De-
terminers and quantifiers 
130,829 15% 
Preposition error 124,902 14% 
Multiple: Adjacent and nested 
annotations 
113,615 13% 
Other content word errors 79,596 9% 
Other function word errors: 
anaphors and conjunctions 
65,034 7% 
Derivational morphology of 
content words 
39,213 4% 
Table 1: Error types in the CLC. 
182
The multiple error class includes any combination 
of error types where the error annotations are either 
nested or adjacent. The other categories are more 
focused: the errors are of a particular class and 
their adjacent context is correct, although there 
may be another error annotation a single token 
away. Content word errors involve the insertion, 
deletion and substitution of nouns, verbs, adjec-
tives and adverbs. Further analysis of this error 
category on a random sample of 200 instances re-
veals that the majority (72%) of content word er-
rors involve substitutions, while deletions account 
for 10% of the errors and insertions for 18%. Most 
substitutions (63%) involve the wrong choice of a 
word that is somewhat semantically related to the 
correct choice. Inflectional morphology includes 
all inflection errors for content words as well as 
subject-verb agreement errors. The inflectional 
errors include many cases of what might be con-
sidered spelling errors, for example *dieing/dying. 
Similarly, the derivational morphology errors in-
clude all derivational errors for content words ? 
and also include many errors that may be consid-
ered as spelling errors. Noun formation errors in-
clude all annotations involving determiners and 
quantifiers: inflection, derivation, countability, 
word form and noun-phrase-internal agreement. 
Preposition errors include all annotations that in-
volve prepositions: insertion, deletion, substitution 
and a non-preposition being used in place of a 
preposition. There are two other categories: those 
involving the remaining function words (anaphors 
and conjunctions) and those involving remaining 
content words (collocation, idiom, negative for-
mation, argument structure, word order, etc.). 
It is important to highlight the challenges inher-
ent in this data set. First of all, the problem is high-
ly skewed since only 7.3% of tokens in the test set 
are involved in an error. Second, since we included 
correct learner sentences in the development and 
test sets in the proportion they occur in the overall 
corpus, only 47% of sentences in the test set con-
tain error annotations, greatly increasing the likeli-
hood of false positives. 
5. Features 
5.1 Language model features 
The language model (LM) features comprise a 
total of 29 features. Each of these features is calcu-
lated from n-gram probabilities observed at and 
around the current token. All LM features are 
based on scores from a 7-gram language model 
with absolute discount smoothing built from the 
Gigaword corpus (Gao et al 2001, Nguyen et al 
2007). 
We group the language model features concep-
tually into five categories: basic features, ratio fea-
tures, drop features, entropy delta features and 
miscellaneous. All probabilities are log probabili-
ties, and n in the n-grams ranges from 1 to 5. All 
features are calculated for each token w of the to-
kens w0?wi in a sentence. 
Basic LM features consist of two features: the 
unigram probability of w and the average n-gram 
probability of all n-grams in the sentence that con-
tain w. 
Ratio features are based on the intuition that er-
rors can be characterized as involving tokens that 
have a very low ratio of higher order n-gram prob-
abilities to lower order n-gram probabilities. In 
other words, these are tokens that are part of an 
unlikely combination of otherwise likely smaller n-
grams. These features are calculated as the ratio of 
the average x-gram probability of all x-grams con-
taining w to the average y-gram probability of all 
y-grams containing w. The values for x and y are: 5 
and 1, 4 and 1, 3 and 1, 2 and 1, 5 and 4, 4 and 3, 3 
and 2. 
Drop features measure either the drop or in-
crease in n-gram probability across token w. For 
example, the bigram drop at wi is the delta between 
the bigram probability of the bigram starting at i-1 
to the bigram probability of the bigram starting at i. 
Drop features are calculated for n-grams with 2 ? n 
? 5. 
Entropy delta features offer another way to look 
at the changes of n-gram probability across a token 
w. Forward entropy for wi is defined as the entropy 
of the string wi?wn where n is the index of the last 
token in the sentence. We calculate the entropy of 
an n-gram as the language model probability of 
string wi?wn divided by the number of tokens in 
that string. Backward entropy is calculated analo-
gously for w0?wi. For n-grams with 1 ? n ? 5, we 
also calculate, at each index i into the token array, 
the delta between the n-gram entropy of the n-gram 
starting at i and the n-gram starting at i+1 (forward 
sliding entropy). Similarly the delta between the n-
gram entropy of the n-gram starting at i and the n-
gram starting at i-1 (backward sliding entropy) is 
calculated.  
183
There are four miscellaneous language model 
features. Three of them, minimum ratio to random, 
average ratio to random, and overall ratio to ran-
dom address the fact that a ?good? n-gram is likely 
to have a much higher probability than an n-gram 
with the same tokens in random order. For all n-
grams where 2 ? n ? 5 we calculate the ratio be-
tween the n-gram probability and the sum of the 
unigram probabilities. For a token wi we produce 
the minimum ratio to random (the minimum ratio 
of all n-grams including w) and the average ratio 
to random (the average of all ratios of the n-grams 
including w). Overall ratio to random is obtained 
by looping through each n-gram where 2 ? n ? 5 
that includes wi and summing the n-gram proba-
bilities (sum1) as well as the unigram probabilities 
of all unigrams in these n-grams (sum2). The ratio 
feature is then sum1/sum2. The final feature ad-
dresses the intuition that an erroneous word may 
cause n-grams that contain the word to be less like-
ly than adjacent but non-overlapping n-grams. 
Overlap to adjacent ratio is the sum of probabili-
ties of n-grams including wi, divided by the sum of 
probabilities of n-grams that are adjacent to wi but 
do not include it.  
Note that this use of a host of language model 
features is substantially different from using a sin-
gle language model score on hypothesized error 
and potential correction to filter out unlikely cor-
rection candidates as in Gamon et al (2008) and 
Gamon (2010).  
5.2 String features 
String features capture information about the char-
acters in a token and the tokens in a sentence. Two 
binary features indicate whether a token is capital-
ized (initial capitalization or all capitalized), one 
feature indicates the token length in characters and 
one feature measures the number of tokens in the 
sentence. 
5.3 Linguistic Analysis features 
Each sentence is linguistically analyzed by a 
PCFG-LA parser (Petrov et al, 2006) trained on 
the Penn Treebank (Marcus et al, 1993). A num-
ber of features are extracted from the constituency 
tree to assess the syntactic complexity of the whole 
sentence, the syntactic complexity of the local en-
vironment of a token, and simple constituency in-
formation for each token. These features are: label 
of the parent and grandparent node, number of sib-
ling nodes, number of siblings of the parent, pres-
ence of a governing head node, label of the 
governing head node, and length of path to the 
root. An additional feature indicates whether the 
POS tag assigned by the parser does not match the 
tag assigned by the POS tagger, which may indi-
cate a tagging error. 
6. Experiments 
6.1 Design 
For our experiments we use three different mutual-
ly exclusive random subsets of CLC. 50K sentenc-
es are used for training of the models (larger data 
sets exceeded the capabilities of our MEMM train-
er). In this set, we only include sentences that con-
tain at least one annotated error. We also 
experimented using a mix of error-free and errone-
ous sentences, but the resulting models turned out 
to be extremely skewed towards always predicting 
the majority state ?O? (no error). 20K sentences 
(including both erroneous and correct sentences) 
are used for parameter tuning and testing, respec-
tively. 
Each token in the data is annotated with one of 
the states ?O? or ?I?. Performance is measured on 
a per token basis, i.e. each mismatch between the 
predicted state and the annotated state is counted as 
an error, each match is counted as a correct predic-
tion. 
We use the development set to tune two parame-
ters: the size of the Markov window and a prior to 
prevent overfitting. The latter is a Gaussian prior 
(or quadratic regularizer) where the mean is fixed 
to zero and the variance is left as a free parameter. 
We perform a grid search to find values for the 
parameters that optimize the model?s F1 score on 
the development data. 
In order to be able to report precision and recall 
curves, we use a technique similar to the one de-
scribed in Minkov et al (2010): we introduce an 
artificial feature with a constant value at training 
time. At test time we perform multiple runs, modi-
fying the weight on the artificial feature. This 
weight variation influences the model?s prior pro-
pensity to assign each of the two states, allowing 
us to measure a precision/recall tradeoff.  
184
6.2 Performance of feature sets 
Figure 2 illustrates the performance of three differ-
ent feature sets and combinations. The baseline is 
using only language model features and standard 
POS tags, which tops out at about 20% precision. 
Adding the string features discussed in the previ-
ous section, and partially lexicalized (PL) POS 
tags, where we used POS tags for content word 
tokens and the lexicalized token for function 
words, we get a small but consistent improvement. 
We obtain the best performance when all features 
are used, including the linguistic analysis features 
(DepParse). We found that a high-order model 
with a Markov window size of 14 performed best 
for all experiments with a top F1 score. F1 at low-
er orders was significantly worse. Training time for 
the best models was less than one hour. 
6.3 Predicting error types 
In our next experiment, we tried to determine how 
the sequence modeling approach performs for in-
dividual error types. Here we trained eight differ-
ent models, one for each of the error types in Table 
1. As in the previous experiments, the development 
and test files contained error-free sentences. The 
optimal Markov window size ranged from 8 to 15. 
Note that our general sequence model described in 
the previous sections does not recognize different 
error types, so it was necessary to train one model 
per error type for the experiments in this section. 
Figure 3 shows the results from this series of 
experiments. We omit the results for other content 
word error, other function word and multiple er-
rors in this graph since these relatively ill-defined 
error classes performed rather poorly. As Figure 3 
illustrates, derivational errors and preposition er-
rors achieve by far the best results. The fact that 
the individual precision never reaches the level of 
the general sequence model (Figure 2) can be at-
tributed to the much smaller overall set of errors in 
each of the eight training sets. In Figure 4 we com-
pare the sequence modeling results for prepositions 
with results from the preposition component of the 
current version of the system described in Gamon 
(2010) on the same test set. That system consists of 
a preposition-specific classifier, a language model 
and a meta-classifier that combines evidence from 
the classifier and the language model. The se-
quence model approach outperforms the classifier 
of that system, but the full system including lan-
guage model and meta-classifier achieves much 
higher precision than the sequence modeling ap-
proach. 
6.4 Learning curve experiments 
An obvious question that arises is how much train-
ing data we need for an error detection sequence 
model, i.e. how does performance degrade as we 
decrease the amount of training data from the 50K 
error-annotated sentences that were used in the 
previous experiments. To this end we produced 
random subsets of the training data in 20% incre-
ments. For each of these training sets, we deter-
mined the resulting F1 score by first performing 
parameter tuning on the development set and then 
measuring precision and recall of the best model 
on the test set. Results are shown in Figure 5: at 
20% of training data, precision starts to increase at 
the cost of recall. At 80% of the training data, re-
call starts to trend up as well. This upward trend of 
both precision and recall indicates that increasing 
the amount of training data is likely to further im-
prove results. 
6.5 Error  analysis 
The precision values obtained in our experi-
ments are low, but they are also based on the 
strictest possible measure of accuracy: an error 
prediction is only counted as correct if it exactly 
matches a location and annotation in the CLC. A 
manual analysis of 400 randomly selected sentenc-
es containing ?false positives?, where the system 
had 29% precision and 10% recall, by the strictest 
calculation, showed that 14% of the ?false posi-
tives? identified an error that was either not anno-
tated in CLC or was an error type not covered by 
the system such as punctuation or case (recall from 
Section 4 that for these errors we removed the er-
ror annotations but retained the original string). An 
additional 16% were adjacent to an error annota-
tion. 12% had error annotations within 2-4 tokens 
from the predicted error. Foreign language and 
other unknown proper names comprised an addi-
tional 6%. Finally, 9% were due to tokenization 
problems or all-upper case input that throws off the 
POS tagger. Thus the precision reported in Figure 
2 through Figure 6 is really a lower bound. 30% of 
the ?false positives? either identify, or are adjacent 
to, an error. 
185
Sentence length has a strong influence on the 
accuracy of the sequence model. For sentences less 
than 7 tokens long, average precision is approxi-
mately 7%, whereas longer sentences average at 
29% precision. This observation fits with the fact 
that high-order models perform best in the task, i.e. 
the more context a model can access, the more re-
liable its predictions are. Shorter sentences are also 
less likely to contain an error: only 12% of short 
sentences contain an error, as opposed to 46% of 
sentences of seven tokens or longer. 
For sentences that are at least 7 tokens long, er-
ror predictions on the first and last two tokens (the 
last token typically being punctuation) have an av-
erage precision of 22% as compared to an average 
of 30% at all other positions. Other unreliable error 
predictions include those involving non-alphabetic 
characters (quotes, parentheses, symbols, numbers) 
with 1% precision and proper name tags with 10% 
precision. Many of the predictions on NNP tags 
identify, by and large, unknown or foreign names 
(Cricklewood, Cajamarca). Ignoring system flags 
on short sentences, symbols and NNP tags would 
improve precision with little cost to recall. 
We also experimented with a precision/recall 
metric that is less harsh but at the same time realis-
tic for error detection. For this ?soft metric? we 
count correct and incorrect predictions at the error 
level instead of the token level. An error is defined 
as a consecutive sequence of n error tags, where n 
? 1.  
 
 
Figure 2: Precision and recall of different feature sets. 
 
Figure 3: Precision and recall of different error models. 
0
0.1
0.2
0.3
0.4
0.5
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
p
re
ci
si
o
n
 
recall 
Precision and recall 
LM LM+ String + PL LM + String + PL + DepParse
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
p
re
ci
si
o
n
 
recall 
Precision and Recall per Error Type 
content deriv inflect nounphrase preposition
186
 
Figure 4: Preposition precision and recall. 
 
Figure 5: Learning curve. 
 
Figure 6: Precision and recall for adjacent annotated error 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
p
re
ci
si
o
n
 
recall 
Precision and Recall Prepositions 
sequence model full system Gamon (2010) classifier only Gamon (2010)
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0 10 20 30 40 50 60 70 80 90 100
percent of training data 
Precision, recall and amount of training data 
Precision Recall
0
0.2
0.4
0.6
0.8
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
p
re
ci
si
o
n
 
recall 
Precision and recall: soft metric and per sentence accuracy 
exact match soft metric
per sentence soft metric, short sentences excluded
per sentence, short sentences excluded exact match, short sentences excluded
187
A predicted error counts as being correct with re-
spect to an annotated error if the following two 
criteria are met: 
a) At least one predicted error token is part of 
an annotated error or is directly adjacent to 
an annotated error 
b) No more than two predicted error tokens 
fall outside the annotated error.  
Criterion (a) establishes that predicted and annotat-
ed error are overlapping or at least directly adja-
cent. Criterion (b) ensures that the predicted error 
is ?local? enough to the annotated error and does 
not include too much irrelevant context, but it still 
allows an annotated error to be flanked by predict-
ed error tokens. Figure 6 illustrates the preci-
sion/recall characteristics of the best model when 
using this soft metric as compared to the strict met-
ric. We also included a ?per sentence? metric in 
Figure 6, where we measure precision and recall at 
the level of identifying a sentence as containing an 
error or not, in other words when using the model 
as a detector for ungrammatical sentences. In addi-
tion we show for each of the three metrics how the 
results change if short sentences (shorter than 7 
tokens) are excluded from the evaluation. 
7. Conclusion and future work 
We have shown that a discriminative high order 
sequence model can be used to detect errors in 
English learner writing. This enables a general ap-
proach to error detection, at the cost of requiring 
annotated data. High-order models outperform 
lower order models significantly for this problem. 
It is obvious that there are several avenues to 
pursue in order to improve upon these initial re-
sults. Two possibilities that we would like to high-
light are the model structure and the feature set. As 
mentioned in Section 3, instead of using a separate 
POS tagger we could follow McCallum et al 
(2003) and design a model that jointly predicts two 
sequences: POS tags and error tags. As for feature 
sets, we conducted some preliminary additional 
experiments where we added a second set of lan-
guage model features, based on a different lan-
guage model, namely the Microsoft web n-gram 
model (Wang et al 2010). The addition of these 
features raised both precision and recall.  
Finally, an error detection system is only of 
practical use if it is combined with a component 
that suggests possible corrections. For future work, 
we envision a combination of generic error detec-
tion with a corpus-based lookup system that finds 
alternative strings that have been observed in simi-
lar contexts. All these alternatives can then be 
scored by a language model in the original context 
of the user input, allowing only those suggestions 
to be shown to the user that achieve a better lan-
guage model score than the original input. This 
combination of error detection and error correction 
has the advantage that the error detection compo-
nent can be used to provide recall, i.e. it can be 
allowed to operate at a lower precision level. The 
error correction component, on the other hand, 
then reduces the number of false flags by vetting 
potential corrections by language model scores. 
Acknowledgments 
We would like to thank Claudia Leacock for the 
manual error analysis, Michel Galley for detailed 
comments on an earlier draft and Chris Quirk for 
discussions and help around the MEMM model 
implementation. The idea of the ratio to random 
language model features is Yizheng Cai?s. We also 
greatly benefited from the comments of the anon-
ymous reviewers. 
References  
Eric Steven Atwell. 1986. How to detect grammatical 
errors in a text without parsing it. In Proceedings of 
EACL, pp. 38-45. 
L?on Bottou. 1991. Une approche th?orique de 
l?apprentissage connexionniste: Applications ? la re-
connaissance de la parole. Doctoral dissertation, 
Universit? de Paris XI. 
Johnny Bigert and Ola Knutsson. 2002. Robust error 
detection: a hybrid approach combining unsupervised 
error detection and linguistic knowledge. In Proceed-
ings of the Second Workshop on Robust Methods in 
Analysis of Natural Language Data, pp. 10-19. 
Martin Chodorow and Claudia Leacock. 2000. An un-
supervised method for detecting grammatical errors. 
In Proceedings of NAACL, pp. 140-147. 
Martin Chodorow, Joel Tetreault and Na-Rae Han. 
2007. Detection of grammatical errors involving 
prepositions. In Proceedings of the Fourth ACL-
SIGSEM Workshop on Prepositions, pp. 25-30. 
Rachele De Felice and Stephen G. Pulman. 2008. A 
classifier-based approach to preposition and deter-
188
miner error correction in L2 English. In Proceedings 
of COLING, pp. 169-176. 
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
dre Klementiev, William Dolan, Dmitriy Belenko 
and Lucy Vanderwende. 2008. Using Contextual 
Speller Techniques and Language Modeling for ESL 
Error Correction. In Proceedings of IJCNLP. 
Michael Gamon. 2010. Using mostly native data to cor-
rect errors in learners? writing. In Proceedings of 
NAACL. 
Jianfeng Gao, Joshua Goodman, and Jiangbo Miao. 
2001. The use of clustering techniques for language 
modeling--Application to Asian languages. Computa-
tional Linguistics and Chinese Language Processing, 
6(1), 27-60. 
Na-Rae Han, Joel Tetreault, Soo-Hwa Lee and Jin-
Young Ha. 2010. Using error-annotated ESL data to 
develop an ESL error correction system. In Proceed-
ings of LREC. 
Emi Izumi, Kiyotaka Uchimoto and Hitoshi Isahara. 
2004. SST speech corpus of Japanese learners? Eng-
lish and automatic detection of learners? errors. In-
ternational Computer Archive of Modern English 
Journal, 28:31-48. 
John Lafferty, Andrew McCallum and Fernando Perei-
ra. 2001. Conditional random fields: Probabilistic 
models for segmenting and labeling sequence data. In 
Proceedings of ICWSM, pp. 282-289. 
Claudia Leacock, Martin Chodorow, Michael Gamon 
and Joel Tetreault. 2010. Automated Grammatical 
Error Detection for Language Learners. Morgan and 
Claypool. 
Mitchell P. Marcus, Beatrice Santorini and Mary Ann 
Marcinkiewicz. 1993. Building a large annotated 
corpus of English: The Penn Treebank. Computa-
tional Linguistics 19:313-330. 
Andrew McCallum, Dayne Freitag and Fernando Perei-
ra. 2000. Maximum entropy Markov models for in-
formation extraction and segmentation. In 
Proceedings of ICML, pp. 591-598. 
Andrew McCallum, Khashayar Rohanimanesh and 
Charles Sutton. 2003. Dynamic Conditional Random 
Fields for jointly labeling multiple sequences. In 
Proceedings of NIPS Workshop on Syntax, Semantics 
and Statistics. 
Einat Minkov, Richard C. Wang, Anthony Tomsaic and 
William C. Cohen. 2010. NER systems that suit us-
er?s preferences: Adjusting the Recall-Precision 
trade-off for entity extraction. In Proceedings of 
NAACL, pp. 93-96. 
Patrick Nguyen, Jianfeng Gao, and Milind Mahajan. 
2007. MSRLM: A scalable language modeling 
toolkit (MSR-TR-2007-144). Redmond, WA: Mi-
crosoft. 
Daisuke Okanohara and Jun?ichi Tsujii. 2007. A dis-
criminative language model with pseudo-negative 
samples. In Proceedings of ACL, pp. 73-80. 
Y. Albert Park and Roger Levy. 2011. Automated 
whole sentence grammar correction using a Noisy 
Channel Model. In Proceedings of ACL 2011. 
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of 
COLING/ACL, pp. 443-440. 
Alla Rozovskaya and Dan Roth. 2010a. Training Para-
digms for correcting errors in grammar and usage. In 
Proceedings of NAACL-HLT. 
Alla Rozovskaya and Dan Roth. 2010b. Generating con-
fusion sets for context-sensitive error correction. In 
Proceedings of EMNLP. 
Jonas Sj?bergh. 2005. Chunking: An unsupervised 
method to find errors in text. In Proceedings of the 
15th NODALIDA conference. 
Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou, 
Zhongyang Xiong, John Lee and Chin-Yew Lin. 
2007. Detecting erroneous sentences using automati-
cally mined sequential patterns. In Proceedings of 
ACL, pp. 81-88. 
Joel Tetreault and Martin Chodorow. 2008. The ups and 
downs of preposition error detection in ESL writing. 
In Proceedings of COLING, pp. 865-872. 
Kristina Toutanova, Dan Klein, Chris Manning, and 
Yoram Singer. 2003. Feature-rich part-of-speech tag-
ging with a cyclic dependency network. In Proceed-
ings of NAACL, pp. 252-259. 
Joachim Wagner, Jennifer Foster, and Josef van 
Genabith. 2007. Judging grammaticality: Experi-
ments in sentence classification. In Proceedings of 
EMNLP & CONLL, pp 112-121. 
Kuansan Wang, Christopher Thrasher, Evelyne Viegas, 
Xialong Li, and Paul Hsu. 2010. An Overview of 
Microsoft web n-gram corpus and applications. In: 
Proceedings of NAACL 2010. 
189
