Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 49?56,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Unsupervised vs. supervised weight estimation
for semantic MT evaluation metrics
Chi-kiu LO and Dekai WU
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo,dekai}@cs.ust.hk
Abstract
We present an unsupervised approach to esti-
mate the appropriate degree of contribution of
each semantic role type for semantic transla-
tion evaluation, yielding a semantic MT eval-
uation metric whose correlation with human
adequacy judgments is comparable to that of
recent supervised approaches but without the
high cost of a human-ranked training corpus.
Our new unsupervised estimation approach
is motivated by an analysis showing that the
weights learned from supervised training are
distributed in a similar fashion to the relative
frequencies of the semantic roles. Empiri-
cal results show that even without a training
corpus of human adequacy rankings against
which to optimize correlation, using instead
our relative frequency weighting scheme to
approximate the importance of each semantic
role type leads to a semantic MT evaluation
metric that correlates comparable with human
adequacy judgments to previous metrics that
require far more expensive human rankings of
adequacy over a training corpus. As a result,
the cost of semantic MT evaluation is greatly
reduced.
1 Introduction
In this paper we investigate an unsupervised ap-
proach to estimate the degree of contribution of each
semantic role type in semantic translation evalua-
tion in low cost without using a human-ranked train-
ing corpus but still yields a evaluation metric that
correlates comparably with human adequacy judg-
ments to that of recent supervised approaches as in
Lo and Wu (2011a, b, c). The new approach is
motivated by an analysis showing that the distri-
bution of the weights learned from the supervised
training is similar to the relative frequencies of the
occurrences of each semantic role in the reference
translation. We then introduce a relative frequency
weighting scheme to approximate the importance of
each semantic role type. With such simple weight-
ing scheme, the cost of evaluating translation of lan-
guages with fewer resources available is greatly re-
duced.
For the past decade, the task of measuring the per-
formance of MT systems has relied heavily on lex-
ical n-gram based MT evaluation metrics, such as
BLEU (Papineni et al., 2002), NIST (Doddington,
2002), METEOR (Banerjee and Lavie, 2005), PER
(Tillmann et al., 1997), CDER (Leusch et al., 2006)
and WER (Nie?en et al., 2000) because of their sup-
port on fast and inexpensive evaluation. These met-
rics are good at ranking overall systems by averaging
their scores over the entire document. As MT sys-
tems improve, the focus of MT evaluation changes
from generally reflecting the quality of each system
to assisting error analysis on each MT output in de-
tail. The failure of such metrics in evaluating trans-
lation quality on sentence level are becoming more
apparent. Though containing roughly the correct
words, the MT output as a whole sentence is still
quite incomprehensible and fails to express mean-
ing that is close to the input. Lexical n-gram based
evaluation metrics are surface-oriented and do not
do so well at ranking translations according to ad-
equacy and are particularly poor at reflecting sig-
nificant translation quality improvements on more
meaningful word sense or semantic frame choices
which human judges can indicate clearly. Callison-
Burch et al. (2006) and Koehn and Monz (2006)
even reported cases where BLEU strongly disagrees
with human judgment on translation quality.
49
Liu and Gildea (2005) proposed STM, a struc-
tural approach based on syntax to addresses the fail-
ure of lexical similarity based metrics in evaluating
translation grammaticality. However, a grammatical
translation can achieve a high syntax-based score but
still contains meaning errors arising from confusion
of semantic roles. On the other hand, despite the
fact that non-automatic, manually evaluations, such
as HTER (Snover et al., 2006), are more adequacy
oriented and show a high correlation with human ad-
equacy judgment, the high labor cost prohibits their
widespread use. There was also work on explicitly
evaluating MT adequacy with aggregated linguistic
features (Gime?nez and Ma`rquez, 2007, 2008) and
textual entailment (Pado et al., 2009).
In the work of Lo and Wu (2011a), MEANT
and its human variants HMEANT were introduced
and empirical experimental results showed that
HMEANT, which can be driven by low-cost mono-
lingual semantic roles annotators with high inter-
annotator agreement, correlates as well as HTER
and far superior than BLEU and other surfaced ori-
ented evaluation metrics. Along with additional im-
provements to the MEANT family of metrics, Lo
and Wu (2011b) detailed the studies of the impact of
each individual semantic role to the metric?s corre-
lation with human adequacy judgments. Lo and Wu
(2011c) further discussed that with a proper weight-
ing scheme of semantic frame in a sentence, struc-
tured semantic role representation is more accurate
and intuitive than flattened role representation for se-
mantic MT evaluation metrics.
The recent trend of incorporating more linguistic
features into MT evaluation metrics raise the dis-
cussion on the appropriate approach in weighting
and combining them. ULC (Gime?nez and Ma`rquez,
2007, 2008) uses uniform weights to aggregate lin-
guistic features. This approach does not capture the
importance of each feature to the overall translation
quality to the MT output. One obvious example of
different semantic roles contribute differently to the
overall meaning is that readers usually accept trans-
lations with errors in adjunct arguments as a valid
translation but not those with errors in core argu-
ments. Unlike ULC, Liu and Gildea (2007); Lo and
Wu (2011a) approach the weight estimation prob-
lem by maximum correlation training which directly
optimize the correlation with human adequacy judg-
Figure 1: HMEANT structured role representation with a
weighting scheme reflecting the degree of contribution of
each semantic role type to the semantic frame. (Lo and
Wu, 2011a,b,c).
ments. However, the shortcomings of this approach
is that it requires a human-ranked training corpus
which is expensive, especially for languages with
limited resource.
We argue in this paper that for semantic MT eval-
uation, the importance of each semantic role type
can easily be estimated using a simple unsupervised
approach which leverage the relative frequencies of
the semantic roles appeared in the reference transla-
tion. Our proposed weighting scheme is motivated
by an analysis showing that the weights learned
from supervised training are distributed in a similar
fashion to the relative frequencies of the semantic
roles. Our results show that the semantic MT eval-
uation metric using the relative frequency weight-
ing scheme to approximate the importance of each
semantic role type correlates comparably with hu-
man adequacy judgments to previous metrics that
use maximum correlation training, which requires
expensive human rankings of adequacy over a train-
ing corpus. Therefore, the cost of semantic MT eval-
uation is greatly reduced.
2 Semantic MT evaluation metrics
Adopting the principle that a good translation is one
from which human readers may successfully un-
derstand at least the basic event structure-?who did
what to whom, when, where and why? (Pradhan et
al., 2004)-which represents the most essential mean-
ing of the source utterances, Lo and Wu (2011a,b,c)
50
proposed HMEANT to evaluate translation utility
based on semantic frames reconstructed by human
reader of machine translation output. Monolingual
(or bilingual) annotators must label the semantic
roles in both the reference and machine translations,
and then to align the semantic predicates and role
fillers in the MT output to the reference translations.
These annotations allow HMEANT to then look at
the aligned role fillers, and aggregate the transla-
tion accuracy for each role. In the spirit of Oc-
cam?s razor and representational transparency, the
HMEANT score is defined simply in terms of a
weighted f-score over these aligned predicates and
role fillers. More precisely, HMEANT is defined as
follows:
1. Human annotators annotate the shallow seman-
tic structures of both the references and MT
output.
2. Human judges align the semantic frames be-
tween the references and MT output by judging
the correctness of the predicates.
3. For each pair of aligned semantic frames,
(a) Human judges determine the translation
correctness of the semantic role fillers.
(b) Human judges align the semantic role
fillers between the reference and MT out-
put according to the correctness of the se-
mantic role fillers.
4. Compute the weighted f-score over the match-
ing role labels of these aligned predicates and
role fillers.
mi ?
#tokens filled in frame i of MT
total #tokens in MT
ri ?
#tokens filled in frame i of REF
total #tokens in REF
Mi, j ? total # ARG j of PRED i in MT
Ri, j ? total # ARG j of PRED i in REF
Ci, j ? # correct ARG j of PRED i in MT
Pi, j ? # partially correct ARG j of PRED i in MT
precision =
?i mi
wpred+? j w j(Ci, j+wpartialPi, j)
wpred+? j w jMi, j
?i mi
recall =
?i ri
wpred+? j w j(Ci, j+wpartialPi, j)
wpred+? j w jRi, j
?i ri
HMEANT =
2?precision? recall
precision+ recall
where mi and ri are the weights for frame,i, in the
MT/REF respectively. These weights estimate the
degree of contribution of each frame to the overall
meaning of the sentence. Mi, j and Ri, j are the to-
tal counts of argument of type j in frame i in the
MT/REF respectively. Ci, j and Pi, j are the count of
the correctly and partial correctly translated argu-
ment of type j in frame i in the MT. wpred is the
weight for the predicate and wj is the weights for the
arguments of type j. These weights estimate the de-
gree of contribution of different types of semantic
roles to the overall meaning of the semantic frame
they attached to. The frame precision/recall is the
weighted sum of the number of correctly translated
roles in a frame normalized by the weighted sum
of the total number of all roles in that frame in the
MT/REF respectively. The sentence precision/recall
is the weighted sum of the frame precision/recall for
all frames normalized by the weighted sum of the to-
tal number of frames in MT/REF respectively. Fig-
ure 1 shows the internal structure of HMEANT.
In the work of Lo and Wu (2011b), the correla-
tion of all individual roles with the human adequacy
judgments were found to be non-negative. There-
fore, grid search was used to estimate the weights
of each roles by optimizing the correlation with hu-
man adequacy judgments. This approach requires
an expensive human-ranked training corpus which
may not be available for languages with sparse re-
sources.Unlike the supervised training approach, our
proposed relative frequency weighting scheme does
not require additional resource other than the SRL
annotated reference translation.
3 Which roles contribute more in the
semantic MT evaluation metric?
We begin with an investigation that suggests that the
relative frequency of each semantic role (which can
be estimated in unsupervised fashion without human
rankings) approximates fairly closely its importance
as determined by previous supervised optimization
approaches. Since there is no ground truth on which
51
Role Deviation (GALE-A) Deviation (GALE-B) Deviation (WMT12)
Agent -0.09 -0.05 0.03
Experiencer 0.23 0.05 0.02
Benefactive 0.02 0.04 -0.01
Temporal 0.11 0.08 0.03
Locative -0.05 -0.05 -0.07
Purpose -0.01 0.03 -0.01
Manner -0.01 0.00 -0.01
Extent -0.02 0.00 -0.01
Modal ? 0.04 0.01
Negation ? 0.01 -0.01
Other -0.12 0.05 -0.01
Table 1: Deviation of relative frequency from optimized weight of each semantic role in GALE-A, GALE-B and
WMT12
semantic role contribute more to the overall meaning
in a sentence for semantic MT evaluation, we first
show that the unsupervised estimation are close to
the weights obtained from the supervised maximum
correlation training on a human-ranked MT evalua-
tion corpus. More precisely, the weight estimation
function is defined as follows:
c j ? # count of ARG j in REF of the test set
w j =
c j
? j c j
3.1 Experimental setup
For our benchmark comparison, the evaluation data
for our experiment is the same two sets of sentences,
GALE-A and GALE-B that were used in Lo and Wu
(2011b). The translation in GALE-A is SRL an-
notated with 9 semantic role types, while those in
GALE-B are SRL annotated with 11 semantic role
types (segregating the modal and the negation roles
from the other role).
To validate whether or not our hypothesis is lan-
guage independent, we also construct an evalua-
tion data set by randomly selecting 50 sentences
from WMT12 English to Czech (WMT12) transla-
tion task test corpus, in which 5 systems (out of
13 participating systems) were randomly picked for
translation adequacy ranking by human readers. In
total, 85 sets of translations (with translations from
some source sentences appear more than once in dif-
ferent sets) were ranked. The translation in WMT12
are also SRL annotated with the tag set as GALE-B,
i.e., 11 semantic role types.
The weights wpred, w j and wpartial were estimated
using grid search to optimize the correlation against
human adequacy judgments.
3.2 Results
Inspecting the distribution of the trained weights and
the relative frequencies from all three data sets, as
shown in table 1, we see that the overall pattern of
weights from unsupervised estimation has a fairly
small deviation from the those learned via super-
vised optimization. To visualize more clearly the
overall pattern of the weights from the two estima-
tion methods, we show the deviation of the unsuper-
vised estimation from the supervised estimation. A
deviation of 0 for all roles would mean that unsu-
pervised and supervised estimation produce exactly
identical weights. If the unsupervised estimation is
higher than the supervised estimation, the deviation
will be positive and vice versa.
What we see is that in almost all cases, the de-
viation between the trained weight and the relative
frequency of each role is always within the range [-
0.1, 0.1].
Closer inspection also reveals the following more
detailed patterns:
? The weight of the less frequent adjunct argu-
ments (e.g. purpose, manner, extent, modal and
negation) from the unsupervised estimation is
highly similar to that learned from the super-
52
PRED estimation Deviation (GALE-A) Deviation (GALE-B) Deviation (WMT12)
Method (i) 0.16 0.16 0.31
Method (ii) 0.02 0.01 0.01
Table 2: Deviation from optimized weight in GALE-A, GALE-B and WMT12 of the predicate?s weight as estimated
by (i) frequency of predicates in frames, relative to predicates and arguments; and (ii) one-fourth of agent?s weight.
vised maximum correlation training.
? The unsupervised estimation usually gives a
higher weight to the temporal role than the su-
pervised training would.
? The unsupervised estimation usually gives a
lower weight to the locative role than the super-
vised training would but the two weights from
the two approach are still high similar to each
other, yielding a deviation within the range of
[-0.07, 0.07].
? There is an obvious outlier found in GALE-A
where the deviation of the relative frequency
from the optimized weight is unusually high.
This suggests that the optimized weights in
GALE-A may be at the risk of over-fitting the
training data.
4 Estimating the weight for the predicate
The remaining question left to be investigated
is how we are to estimate the importance of the
predicate in an unsupervised approach. One obvious
approach is to treat the predicate the same way as
the arguments. That is, just like with arguments,
we could weight predicates by the relative fre-
quency of how often predicates occur in semantic
frames. However, this does not seem well motivated
since predicates are fundamentally different from
arguments: by definition, every semantic frame is
defined by one predicate, and arguments are defined
relative to the predicate.
On the other hand, inspecting the weights on the
predicate obtained from the supervised maximum
correlation training, we find that the weight of the
predicate is usually around one-fourth of the weight
of the agent role. More precisely, the two weight
estimation functions are defined as follows:
cpred ? # count of PRED in REF of the test set
Method (i) =
cpred
cpred +? j c j
Method (ii) = 0.25 ?wagent
We now show that the supervised estimation of
the predicate?s weight is closely approximated by
unsupervised estimation.
4.1 Experimental setup
The experimental setup is the same as that used in
section 3.
4.2 Results
The results in table 2 show that the trained weight
of the predicate and its unsupervised estimation of
one-fourth of the agent role?s weight are highly sim-
ilar to each other. In all three data sets, the devia-
tion between the trained weight and the heuristic of
one-fourth of the agent?s weight is always within the
range [0.1, 0.2].
On the other hand, treating the predicate the same
as arguments by estimating the unsupervised weight
using relative frequency largely over-estimates and
has a large deviation from the weight learned from
supervised estimation.
5 Semantic MT evaluation using
unsupervised weight estimates
Having seen that the weights of the predicate and
semantic roles estimated by the unsupervised ap-
proach fairly closely approximate those learned
from the supervised approach, we now show that the
unsupervised approach leads to a semantic MT eval-
uation metric that correlates comparably with hu-
man adequacy judgments to one that is trained on
a far more expensive human-ranked training corpus.
5.1 Experimental setup
Following the benchmark assessment in NIST Met-
ricsMaTr 2010 (Callison-Burch et al., 2010), we as-
sess the performance of the semantic MT evaluation
53
Metrics GALE-A GALE-B WMT12
HMEANT (supervised) 0.49 0.27 0.29
HMEANT (unsupervised) 0.42 0.23 0.20
NIST 0.29 0.09 0.12
METEOR 0.20 0.21 0.22
TER 0.20 0.10 0.12
PER 0.20 0.07 0.02
BLEU 0.20 0.12 0.01
CDER 0.12 0.10 0.14
WER 0.10 0.11 0.17
Table 3: Average sentence-level correlation with human adequacy judgments of HMEANT using supervised and
unsupervised weight scheme on GALE-A, GALE-B and WMT12, (with baseline comparison of commonly used
automatic MT evaluation metric.
metric at the sentence level using Kendall?s rank
correlation coefficient which evaluate the correla-
tion of the proposed metric with human judgments
on translation adequacy ranking. A higher the value
for indicates a higher similarity to the ranking by
the evaluation metric to the human judgment. The
range of possible values of correlation coefficient is
[-1,1], where 1 means the systems are ranked in the
same order as the human judgment and -1 means the
systems are ranked in the reverse order as the hu-
man judgment. For GALE-A and GALE-B, the hu-
man judgment on adequacy was obtained by show-
ing all three MT outputs together with the Chinese
source input to a human reader. The human reader
was instructed to order the sentences from the three
MT systems according to the accuracy of meaning in
the translations. For WMT12, the human adequacy
judgments are provided by the organizers.
The rest of the experimental setup is the same as
that used in section 3.
5.2 Results
Table 3 shows that HMEANT with the proposed un-
supervised semantic role weighting scheme corre-
late comparably with human adequacy judgments to
that optimized with a more expensive human-ranked
training corpus, and, outperforms all other com-
monly used automatic metrics (except for METEOR
in Czech). The results from GALE-A, GALE-B and
WMT12 are consistent. These encouraging results
show that semantic MT evaluation metric could be
widely applicable to languages other than English.
6 Conclusion
We presented a simple, easy to implement yet well-
motivated weighting scheme for HMEANT to esti-
mate the importance of each semantic role in eval-
uating the translation adequacy. Unlike the previ-
ous metrics, the proposed metric does not require
an expensive human-ranked training corpus and still
outperforms all other commonly used automatic MT
evaluation metrics. Interestingly, the distribution of
the optimal weights obtained by maximum correla-
tion training, is similar to the relative frequency of
occurrence of each semantic role type in the refer-
ence translation. HMEANT with the new weight-
ing scheme showed consistent results across differ-
ent language pairs and across different corpora in
the same language pair. With the proposed weight-
ing scheme, the semantic MT evaluation metric is
ready to be used off-the-shelf without depending on
a human-ranked training corpus. We believe that our
current work reduces the barrier for semantic MT
evaluation for resource scarce languages sufficiently
so that semantic MT evaluation can be applied to
most other languages.
Acknowledgments
We would like to thank Ondr?ej Bojar and all the
annotators from the Charles University in Prague
for participating in the experiments. This ma-
terial is based upon work supported in part by
the Defense Advanced Research Projects Agency
(DARPA) under BOLT contract no. HR0011-
12-C-0016, and GALE contract nos. HR0011-
06-C-0022 and HR0011-06-C-0023; by the Eu-
54
ropean Union under the FP7 grant agreement
no. 287658; and by the Hong Kong Research
Grants Council (RGC) research grants GRF621008,
GRF612806, DAG03/04.EG09, RGC6256/00E, and
RGC6083/99E. Any opinions, findings and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily reflect
the views of the RGC, EU, or DARPA.
References
