Proceedings of the Third Workshop on Statistical Machine Translation, pages 115?118,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Meteor, m-bleu and m-ter: Evaluation Metrics for
High-Correlation with Human Rankings of Machine Translation
Output
Abhaya Agarwal and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{abhayaa,alavie}@cs.cmu.edu
Abstract
This paper describes our submissions to the
machine translation evaluation shared task in
ACL WMT-08. Our primary submission is the
Meteor metric tuned for optimizing correla-
tion with human rankings of translation hy-
potheses. We show significant improvement
in correlation as compared to the earlier ver-
sion of metric which was tuned to optimized
correlation with traditional adequacy and flu-
ency judgments. We also describe m-bleu and
m-ter, enhanced versions of two other widely
used metrics bleu and ter respectively, which
extend the exact word matching used in these
metrics with the flexible matching based on
stemming and Wordnet in Meteor .
1 Introduction
Automatic Metrics for MT evaluation have been re-
ceiving significant attention in recent years. Evalu-
ating an MT system using such automatic metrics is
much faster, easier and cheaper compared to human
evaluations, which require trained bilingual evalua-
tors. The most commonly used MT evaluation met-
ric in recent years has been IBM?s Bleu metric (Pa-
pineni et al, 2002). Bleu is fast and easy to run,
and it can be used as a target function in parameter
optimization training procedures that are commonly
used in state-of-the-art statistical MT systems (Och,
2003). Various researchers have noted, however, var-
ious weaknesses in the metric. Most notably, Bleu
does not produce very reliable sentence-level scores.
Meteor , as well as several other proposed metrics
such as GTM (Melamed et al, 2003), TER (Snover
et al, 2006) and CDER (Leusch et al, 2006) aim to
address some of these weaknesses.
Meteor , initially proposed and released in 2004
(Lavie et al, 2004) was explicitly designed to im-
prove correlation with human judgments of MT qual-
ity at the segment level. Previous publications on
Meteor (Lavie et al, 2004; Banerjee and Lavie,
2005; Lavie and Agarwal, 2007) have described the
details underlying the metric and have extensively
compared its performance with Bleu and several
other MT evaluation metrics. In (Lavie and Agar-
wal, 2007), we described the process of tuning free
parameters within the metric to optimize the corre-
lation with human judgments and the extension of
the metric for evaluating translations in languages
other than English.
This paper provides a brief technical description of
Meteor and describes our experiments in re-tuning
the metric for improving correlation with the human
rankings of translation hypotheses corresponding to
a single source sentence. Our experiments show sig-
nificant improvement in correlation as a result of re-
tuning which shows the importance of having a met-
ric tunable to different testing conditions. Also, in
order to establish the usefulness of the flexible match-
ing based on stemming and Wordnet, we extend two
other widely used metrics bleu and ter which use
exact word matching, with the matcher module of
Meteor .
2 The Meteor Metric
Meteor evaluates a translation by computing a
score based on explicit word-to-word matches be-
tween the translation and a given reference trans-
lation. If more than one reference translation is
available, the translation is scored against each refer-
ence independently, and the best scoring pair is used.
Given a pair of strings to be compared, Meteor cre-
ates a word alignment between the two strings. An
alignment is mapping between words, such that ev-
ery word in each string maps to at most one word
in the other string. This alignment is incrementally
produced by a sequence of word-mapping modules.
The ?exact? module maps two words if they are ex-
actly the same. The ?porter stem? module maps two
words if they are the same after they are stemmed us-
115
ing the Porter stemmer. The ?WN synonymy? mod-
ule maps two words if they are considered synonyms,
based on the fact that they both belong to the same
?synset? in WordNet.
The word-mapping modules initially identify all
possible word matches between the pair of strings.
We then identify the largest subset of these word
mappings such that the resulting set constitutes an
alignment as defined above. If more than one maxi-
mal cardinality alignment is found, Meteor selects
the alignment for which the word order in the two
strings is most similar (the mapping that has the
least number of ?crossing? unigram mappings). The
order in which the modules are run reflects word-
matching preferences. The default ordering is to
first apply the ?exact? mapping module, followed by
?porter stemming? and then ?WN synonymy?.
Once a final alignment has been produced between
the system translation and the reference translation,
the Meteor score for this pairing is computed as
follows. Based on the number of mapped unigrams
found between the two strings (m), the total num-
ber of unigrams in the translation (t) and the total
number of unigrams in the reference (r), we calcu-
late unigram precision P = m/t and unigram recall
R = m/r. We then compute a parametrized har-
monic mean of P and R (van Rijsbergen, 1979):
Fmean =
P ?R
? ? P + (1? ?) ?R
Precision, recall and Fmean are based on single-
word matches. To take into account the extent to
which the matched unigrams in the two strings are
in the same word order, Meteor computes a penalty
for a given alignment as follows. First, the sequence
of matched unigrams between the two strings is di-
vided into the fewest possible number of ?chunks?
such that the matched unigrams in each chunk are
adjacent (in both strings) and in identical word or-
der. The number of chunks (ch) and the number of
matches (m) is then used to calculate a fragmenta-
tion fraction: frag = ch/m. The penalty is then
computed as:
Pen = ? ? frag?
The value of ? determines the maximum penalty
(0 ? ? ? 1). The value of ? determines the
functional relation between fragmentation and the
penalty. Finally, the Meteor score for the align-
ment between the two strings is calculated as:
score = (1 ? Pen) ? Fmean
The free parameters in the metric, ?, ? and ? are
tuned to achieve maximum correlation with the hu-
man judgments as described in (Lavie and Agarwal,
2007).
3 Extending Bleu and Ter with
Flexible Matching
Many widely used metrics like Bleu (Papineni et al,
2002) and Ter (Snover et al, 2006) are based on
measuring string level similarity between the refer-
ence translation and translation hypothesis, just like
Meteor . Most of them, however, depend on find-
ing exact matches between the words in two strings.
Many researchers (Banerjee and Lavie, 2005; Liu and
Gildea, 2006), have observed consistent gains by us-
ing more flexible matching criteria. In the following
experiments, we extend the Bleu and Ter metrics
to use the stemming and Wordnet based word map-
ping modules from Meteor .
Given a translation hypothesis and reference pair,
we first align them using the word mapping modules
from Meteor . We then rewrite the reference trans-
lation by replacing the matched words with the cor-
responding words in the translation hypothesis. We
now compute Bleu and Ter with these new refer-
ences without changing anything inside the metrics.
To get meaningful Bleu scores at segment level,
we compute smoothed Bleu as described in (Lin and
Och, 2004).
4 Re-tuning Meteor for Rankings
(Callison-Burch et al, 2007) reported that the inter-
coder agreement on the task of assigning ranks to
a given set of candidate hypotheses is much better
than the intercoder agreement on the task of assign-
ing a score to a hypothesis in isolation. Based on
that finding, in WMT-08, only ranking judgments
are being collected from the human judges.
The current version of Meteor uses parameters
optimized towards maximizing the Pearson?s corre-
lation with human judgments of adequacy scores. It
is not clear that the same parameters would be op-
timal for correlation with human rankings. So we
would like to re-tune the parameters in the metric
for maximizing the correlation with ranking judg-
ments instead. This requires computing full rankings
according to the metric and the humans and then
computing a suitable correlation measure on those
rankings.
4.1 Computing Full Rankings
Meteor assigns a score between 0 and 1 to every
translation hypothesis. This score can be converted
116
Language Judgments
Binary Sentences
English 3978 365
German 2971 334
French 1903 208
Spanish 2588 284
Table 1: Corpus Statistics for Various Languages
to rankings trivially by assuming that a higher score
indicates a better hypothesis.
In development data, human rankings are avail-
able as binary judgments indicating the preferred hy-
pothesis between a given pair. There are also cases
where both the hypotheses in the pair are judged to
be equal. In order to convert these binary judgments
into full rankings, we do the following:
1. Throw out all the equal judgments.
2. Construct a directed graph where nodes corre-
spond to the translation hypotheses and every
binary judgment is represented by a directed
edge between the corresponding nodes.
3. Do a topological sort on the resulting graph and
assign ranks in the sort order. The cycles in the
graph are broken by assigning same rank to all
the nodes in the cycle.
4.2 Measuring Correlation
Following (Ye et al, 2007), we first compute the
Spearman correlation between the human rankings
and Meteor rankings of the translation hypotheses
corresponding to a single source sentence. Let N be
the number of translation hypotheses and D be the
difference in ranks assigned to a hypothesis by two
rankings, then Spearman correlation is given by:
r = 1?
6
?
D2
N(N2 ? 1)
The final score for the metric is the average of the
Spearman correlations for individual sentences.
5 Experiments
5.1 Data
We use the human judgment data from WMT-07
which was released as development data for the eval-
uation shared task. Amount of data available for
various languages is shown in Table 1. Development
data contains the majority judgments (not every hy-
potheses pair was judged by same number of judges)
which means that in the cases where multiple judges
judged the same pair of hypotheses, the judgment
given by majority of the judges was considered.
English German French Spanish
? 0.95 0.9 0.9 0.9
? 0.5 3 0.5 0.5
? 0.45 0.15 0.55 0.55
Table 2: Optimal Values of Tuned Parameters for Various
Languages
Original Re-tuned
English 0.3813 0.4020
German 0.2166 0.2838
French 0.2992 0.3640
Spanish 0.2021 0.2186
Table 3: Average Spearman Correlation with Human
Rankings for Meteor on Development Data
5.2 Methodology
We do an exhaustive grid search in the feasible ranges
of parameter values, looking for parameters that
maximize the average Spearman correlation over the
training data. To get a fair estimate of performance,
we use 3-fold cross validation on the development
data. Final parameter values are chosen as the best
performing set on the data pooled from all the folds.
5.3 Results
5.3.1 Re-tuning Meteor for Rankings
The re-tuned parameter values are shown in Ta-
ble 2 while the average Spearman correlations for
various languages with original and re-tuned param-
eters are shown in Table 3. We get significant im-
provements for all the languages. Gains are specially
pronounced for German and French.
Interestingly, weight for recall becomes even higher
than earlier parameters where it was already high.
So it seems that ranking judgments are almost en-
tirely driven by the recall in all the languages. Also
the re-tuned parameters for all the languages except
German are quite similar.
5.3.2 m-bleu and m-ter
Table 4 shows the average Spearman correlations
of m-bleu and m-ter with human rankings. For
English, both m-bleu and m-ter show considerable
improvements. For other languages, improvements
in m-ter are smaller but consistent. m-bleu , how-
ever, doesn?t shows any improvements in this case.
A possible reason for this behavior can be the lack of
a ?WN synonymy? module for languages other than
English which results in fewer extra matches over the
exact matching baseline. Additionally, French, Ger-
man and Spanish have a richer morphology as com-
pared to English. The morphemes in these languages
117
Exact Match Flexible Match
English: Bleu 0.2486 0.2747
Ter 0.1598 0.2033
French: Bleu 0.2906 0.2889
Ter 0.2472 0.2604
German: Bleu 0.1829 0.1806
Ter 0.1509 0.1668
Spanish: Bleu 0.1804 0.1847
Ter 0.1787 0.1839
Table 4: Average Spearman Correlation with Human
Rankings for m-bleu and m-ter
carry much more information and different forms of
the same word may not be as freely replaceable as in
English. A more fine grained strategy for matching
words in these languages remains an area of further
investigation.
6 Conclusions
In this paper, we described the re-tuning of Me-
teor parameters to better correlate with human
rankings of translation hypotheses. Results on the
development data indicate that the re-tuned ver-
sion is significantly better at predicting ranking than
the earlier version. We also presented enhanced
Bleu and Ter that use the flexible word match-
ing module from Meteor and show that this re-
sults in better correlations as compared to the de-
fault exact matching versions. The new version of
Meteor will be soon available on our website at:
http://www.cs.cmu.edu/~alavie/METEOR/ . This
release will also include the flexible word matcher
module which can be used to extend any metric with
the flexible matching.
Acknowledgments
The work reported in this paper was supported by
NSF Grant IIS-0534932.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 65?72, Ann Arbor,
Michigan, June.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136?158, Prague, Czech Republic, June.
Association for Computational Linguistics.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
Automatic Metric for MT Evaluation with High Levels
of Correlation with Human Judgments. In Proceedings
of the Second ACL Workshop on Statistical Machine
Translation, pages 228?231, Prague, Czech Republic,
June.
Alon Lavie, Kenji Sagae, and Shyamsundar Jayaraman.
2004. The Significance of Recall in Automatic Metrics
for MT Evaluation. In Proceedings of the 6th Confer-
ence of the Association for Machine Translation in the
Americas (AMTA-2004), pages 134?143, Washington,
DC, September.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006.
CDER: Efficient MT Evaluation Using Block Move-
ments. In Proceedings of the Thirteenth Conference of
the European Chapter of the Association for Compu-
tational Linguistics.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics
for machine translation. In COLING ?04: Proceedings
of the 20th international conference on Computational
Linguistics, page 501, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Ding Liu and Daniel Gildea. 2006. Stochastic itera-
tive alignment for machine translation evaluation. In
Proceedings of the COLING/ACL on Main conference
poster sessions, pages 539?546, Morristown, NJ, USA.
Association for Computational Linguistics.
I. Dan Melamed, Ryan Green, and Joseph Turian. 2003.
Precision and Recall of Machine Translation. In Pro-
ceedings of the HLT-NAACL 2003 Conference: Short
Papers, pages 61?63, Edmonton, Alberta.
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 311?318, Philadelphia, PA,
July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas
(AMTA-2006), pages 223?231, Cambridge, MA, Au-
gust.
C. van Rijsbergen, 1979. Information Retrieval. Butter-
worths, London, UK, 2nd edition.
Yang Ye, Ming Zhou, and Chin-Yew Lin. 2007. Sen-
tence level machine translation evaluation as a rank-
ing. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 240?247, Prague,
Czech Republic, June. Association for Computational
Linguistics.
118
