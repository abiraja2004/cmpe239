Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1640?1649,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Joint Inference for Fine-grained Opinion Extraction
Bishan Yang
Department of Computer Science
Cornell University
bishan@cs.cornell.edu
Claire Cardie
Department of Computer Science
Cornell University
cardie@cs.cornell.edu
Abstract
This paper addresses the task of fine-
grained opinion extraction ? the identi-
fication of opinion-related entities: the
opinion expressions, the opinion hold-
ers, and the targets of the opinions, and
the relations between opinion expressions
and their targets and holders. Most ex-
isting approaches tackle the extraction
of opinion entities and opinion relations
in a pipelined manner, where the inter-
dependencies among different extraction
stages are not captured. We propose a joint
inference model that leverages knowledge
from predictors that optimize subtasks
of opinion extraction, and seeks a glob-
ally optimal solution. Experimental re-
sults demonstrate that our joint inference
approach significantly outperforms tradi-
tional pipeline methods and baselines that
tackle subtasks in isolation for the problem
of opinion extraction.
1 Introduction
Fine-grained opinion analysis is concerned with
identifying opinions in text at the expression level;
this includes identifying the subjective (i.e., opin-
ion) expression itself, the opinion holder and the
target of the opinion (Wiebe et al, 2005). The
task has received increasing attention as many nat-
ural language processing applications would ben-
efit from the ability to identify text spans that cor-
respond to these key components of opinions. In
question-answering systems, for example, users
may submit questions in the form ?What does en-
tity A think about target B??; opinion-oriented
summarization systems also need to recognize
opinions and their targets and holders.
In this paper, we address the task of identifying
opinion-related entities and opinion relations. We
consider three types of opinion entities: opinion
expressions or direct subjective expressions as de-
fined in Wiebe et al (2005) ? expressions that ex-
plicitly indicate emotions, sentiment, opinions or
other private states (Quirk et al, 1985) or speech
events expressing private states; opinion targets
? expressions that indicate what the opinion is
about; and opinion holders ? mentions of whom
or what the opinion is from. Consider the follow-
ing examples in which opinion expressions (O) are
underlined and targets (T) and holders (H) of the
opinion are bracketed.
S1: [The workers][H1,2] were irked[O1]
by [the government report][T1] and
were worried[O2] as they went about
their daily chores.
S2: From the very start it could be
predicted[O1] that on the subject of
economic globalization, [the developed
states][T1,2] were going to come across
fierce opposition[O2].
The numeric subscripts denote linking relations,
one of IS-ABOUT or IS-FROM. In S1, for in-
stance, opinion expression ?were irked? (O1) IS-
ABOUT ?the government report? (T1). Note that
the IS-ABOUT relation can contain an empty tar-
get (e.g. ?were worried? in S1); similarly for IS-
FROM w.r.t. the opinion holder (e.g. ?predicted? in
S2). We also allow an opinion entity to be involved
in multiple relations (e.g. ?the developed states? in
S2).
Not surprisingly, fine-grained opinion extrac-
tion is a challenging task due to the complexity
and variety of the language used to express opin-
ions and their components (Pang and Lee, 2008).
Nevertheless, much progress has been made in ex-
tracting opinion information from text. Sequence
labeling models have been successfully employed
to identify opinion expressions (e.g. (Breck et al,
1640
2007; Yang and Cardie, 2012)) and relation ex-
traction techniques have been proposed to extract
opinion holders and targets based on their link-
ing relations to the opinion expressions (e.g. Kim
and Hovy (2006), Kobayashi et al (2007)). How-
ever, most existing work treats the extraction of
different opinion entities and opinion relations in a
pipelined manner: the interaction between differ-
ent extraction tasks is not modeled jointly and er-
ror propagation is not considered. One exception
is Choi et al (2006), which proposed an ILP ap-
proach to jointly identify opinion holders, opinion
expressions and their IS-FROM linking relations,
and demonstrated the effectiveness of joint infer-
ence. Their ILP formulation, however, does not
handle implicit linking relations, i.e. opinion ex-
pressions with no explicit opinion holder; nor does
it consider IS-ABOUT relations.
In this paper, we present a model that jointly
identifies opinion-related entities, including opin-
ion expressions, opinion targets and opinion hold-
ers as well as the associated opinion linking rela-
tions, IS-ABOUT and IS-FROM. For each type of
opinion relation, we allow implicit (i.e. empty) ar-
guments for cases when the opinion holder or tar-
get is not explicitly expressed in text. We model
entity identification as a sequence tagging prob-
lem and relation extraction as binary classifica-
tion. A joint inference framework is proposed to
jointly optimize the predictors for different sub-
problems with constraints that enforce global con-
sistency. We hypothesize that the ambiguity in
the extraction results will be reduced and thus,
performance increased. For example, uncertainty
w.r.t. the spans of opinion entities can adversely
affect the prediction of opinion relations; and evi-
dence of opinion relations might provide clues to
guide the accurate extraction of opinion entities.
We evaluate our approach using a standard cor-
pus for fine-grained opinion analysis (the MPQA
corpus (Wiebe et al, 2005)) and demonstrate that
our model outperforms by a significant margin tra-
ditional baselines that do not employ joint infer-
ence for extracting opinion entities and different
types of opinion relations.
2 Related Work
Significant research effort has been invested into
fine-grained opinion extraction for open-domain
text such as news articles (Wiebe et al, 2005; Wil-
son et al, 2009). Many techniques were proposed
to identify the text spans for opinion expressions
(e.g. (Breck et al, 2007; Johansson and Moschitti,
2010b; Yang and Cardie, 2012)), opinion hold-
ers (e.g. (Choi et al, 2005)) and topics of opin-
ions (Stoyanov and Cardie, 2008). Some consider
extracting opinion targets/holders along with their
relation to the opinion expressions. Kim and Hovy
(2006) identifies opinion holders and targets by us-
ing their semantic roles related to opinion words.
Ruppenhofer et al (2008) argued that semantic
role labeling is not sufficient for identifying opin-
ion holders and targets. Johansson and Moschitti
(2010a) extract opinion expressions and holders
by applying reranking on top of sequence label-
ing methods. Kobayashi et al (2007) considered
extracting ?aspect-evaluation? relations (relations
between opinion expressions and targets) by iden-
tifying opinion expressions first and then search-
ing for the most likely target for each opinion ex-
pression via a binary relation classifier. All these
methods extract opinion arguments and opinion
relations in separate stages instead of extracting
them jointly.
Most similar to our method is Choi et al (2006),
which jointly extracts opinion expressions, hold-
ers and their IS-FROM relations using an ILP ap-
proach. In contrast, our approach (1) also consid-
ers the IS-ABOUT relation which is arguably more
complex due to the larger variety in the syntac-
tic structure exhibited by opinion expressions and
their targets, (2) handles implicit opinion relations
(opinion expressions without any associated argu-
ment), and (3) uses a simpler ILP formulation.
There has also been substantial interest in opin-
ion extraction from product reviews (Liu, 2012).
Most existing approaches focus on the extrac-
tion of opinion targets and their associated opin-
ion expressions and usually employ a pipeline
architecture: generate candidates of opinion ex-
pressions and opinion targets first, and then use
rule-based or machine-learning-based approaches
to identify potential relations between opinions
and targets (Hu and Liu, 2004; Wu et al, 2009;
Liu et al, 2012). In addition to pipeline ap-
proaches, bootstrapping-based approaches were
proposed (Qiu et al, 2009; Qiu et al, 2011; Zhang
et al, 2010) to identify opinion expressions and
targets iteratively; however, they suffer from the
problem of error propagation.
There is much work demonstrating the bene-
fit of performing global inference. Roth and Yih
1641
(2004) proposed a global inference approach in the
formulation of a linear program (LP) and applied
it to the task of extracting named entities and re-
lations simultaneously. Their problem is similar
to ours ? the difference is that Roth and Yih Roth
and Yih (2004) assume that named entity spans are
known a priori and only their labels need to be as-
signed. Joint inference has also been applied to
semantic role labeling (Punyakanok et al, 2008;
Srikumar and Roth, 2011; Das et al, 2012), where
the goal is to jointly identify semantic arguments
for given lexical predicates. The problem is con-
ceptually similar to identifying opinion arguments
for opinion expressions, however, we do not as-
sume prior knowledge of opinion expressions (un-
like in SRL, where predicates are given).
3 Model
As proposed in Section 1, we consider the task of
jointly identifying opinion entities and opinion re-
lations. Specifically, given a sentence, our goal is
to identify spans of opinion expressions, opinion
arguments (targets and holders) and their associ-
ated linking relations. Training data consists of
text with manually annotated opinion expression
and argument spans, each with a list of relation
ids specifying the linking relation between opin-
ion expressions and their arguments.
In this section, we will describe how we model
opinion entity identification and opinion relation
extraction, and how we combine them in a joint
inference model.
3.1 Opinion Entity Identification
We formulate the task of opinion entity identifica-
tion as a sequence labeling problem and employ
conditional random fields (CRFs) (Lafferty et al,
2001) to learn the probability of a sequence as-
signment y for a given sentence x. Through in-
ference we can find the best sequence assignment
for sentence x and recover the opinion entities ac-
cording to the standard ?IOB? encoding scheme.
We consider four entity labels: D,T,H,N , where
D denotes opinion expressions, T denotes opinion
targets, H denotes opinion holders and N denotes
?NONE? entities.
We define potential function fiz that gives the
probability of assigning a span i with entity label
z, and the probability is estimated based on the
learned parameters from CRFs. Formally, given
a within-sentence span i = (a, b), where a is the
starting position and b is the end position, and la-
bel z ? {D,T,H}, we have
fiz = p(ya = Bz,ya+1 = Iz, ...,
yb = Iz,yb+1 6= Iz|x)
fiN = p(ya = O, ...,yb = O|x)
These probabilities can be efficiently computed
using the forward-backward algorithm.
3.2 Opinion Relation Extraction
We consider extracting the IS-ABOUT and IS-
FROM opinion relations. In the following we will
not distinguish these two relations, since they can
both be characterized as relations between opinion
expressions and opinion arguments, and the meth-
ods for relation extraction are the same.
We treat the relation extraction problem as a
combination of two binary classification prob-
lems: opinion-arg classification, which decides
whether a pair consisting of an opinion candidate o
and an argument candidate a forms a relation; and
opinion-implicit-arg classification, which decides
whether an opinion candidate o is linked to an im-
plicit argument, i.e. no argument is mentioned. We
define a potential function r to capture the strength
of association between an opinion candidate o and
an argument candidate a,
roa = p(y = 1|x)? p(y = 0|x)
where p(y = 1|x) and p(y = 0|x) are the logistic
regression estimates of the positive and negative
relations. Similarly, we define potential ro? to de-
note the confidence of predicting opinion span o
associated with an implicit argument.
3.2.1 Opinion-Arg Relations
For opinion-arg classification, we construct can-
didates of opinion expressions and opinion argu-
ments and consider each pair of an opinion can-
didate and an argument candidate as a potential
opinion relation. Conceptually, all possible sub-
sequences in the sentence are candidates. To filter
out candidates that are less reasonable, we con-
sider the opinion expressions and arguments ob-
tained from the n-best predictions by CRFs1. We
also employ syntactic patterns from dependency
1We randomly split the training data into 10 parts and ob-
tained the 50-best CRF predictions on each part for the gen-
eration of candidates. We also experimented with candidates
generated from more CRF predictions, but did not find any
performance improvement for the task.
1642
trees to generate candidates. Specifically, we se-
lected the most common patterns of the shortest
dependency paths2 between an opinion candidate
o and an argument candidate a in our dataset, and
include all pairs of candidates that satisfy at least
one dependency pattern. For the IS-ABOUT rela-
tion, the top three patterns are (1) o ?dobj a, (2)
o ?ccomp x ?nsubj a (x is a word in the path that is
not covered by either o nor a), (3) o ?ccomp a; for
the IS-FROM relation, the top three patterns are (1)
o ?nsubj a, (2) o ?poss a, (3) o ?ccomp x ?nsubj a.
Note that generating candidates this way will
give us a large number of negative examples. Sim-
ilar to the preprocessing approach in (Choi et al,
2006), we filter pairs of opinion and argument can-
didates that do not overlap with any gold standard
relation in our training data.
Many features we use are common features
in the SRL tasks (Punyakanok et al, 2008)
due to the similarity of opinion relations to the
predicate-argument relations in SRL (Ruppen-
hofer et al, 2008; Choi et al, 2006). In general,
the features aim to capture (a) local properties of
the candidate opinion expressions and arguments
and (b) syntactic and semantic attributes of their
relation.
Words and POS tags: the words contained in the
candidate and their POS tags.
Lexicon: For each word in the candidate, we
include its WordNet hypernyms and its strength
of subjectivity in the Subjectivity Lexicon3
(e.g. weaksubj, strongsubj).
Phrase type: the syntactic category of the deepest
constituent that covers the candidate in the parse
tree, e.g. NP, VP.
Semantic frames: For each verb in the opinion
candidate, we include its frame types according to
FrameNet4.
Distance: the relative distance (number of words)
between the opinion and argument candidates.
Dependency Path: the shortest path in the
dependency tree between the opinion candidate
and the target candidate, e.g. ccomp?nsubj?. We
also include word types and POS types in the
paths, e.g. opinion?ccompsuffering?nsubjpatient,
2We use the Stanford Parser to generate parse trees and
dependency graphs.
3http://mpqa.cs.pitt.edu/lexicons/
subj_lexicon/
4https://framenet.icsi.berkeley.edu/
fndrupal/
NN?ccompVBG?nsubjNN. The dependency path
has been shown to be very useful in extracting
opinion expressions and opinion holders (Johans-
son and Moschitti, 2010a).
3.2.2 Opinion-Implicit-Arg Relations
When the opinion-arg relation classifier predicts
that there is no suitable argument for the opinion
expression candidate, it does not capture the possi-
bility that an opinion candidate may associate with
an implicit argument. To incorporate knowledge
of implicit relations, we build an opinion-implicit-
arg classifier to identify an opinion candidate with
an implicit argument based on its own properties
and context information.
For training, we consider all gold-standard
opinion expressions as training examples ?
including those with implicit arguments ? as
positive examples and those associated with
explicit arguments as negative examples. For
features, we use words, POS tags, phrase types,
lexicon and semantic frames (see Section 3.2.1
for details) to capture the properties of the opinion
expression, and also features that capture the
context of the opinion expression:
Neighboring constituents: The words and gram-
matical roles of neighboring constituents of the
opinion expression in the parse tree ? the left and
right sibling of the deepest constituent containing
the opinion expression in the parse tree.
Parent Constituent: The grammatical role of
the parent constituent of the deepest constituent
containing the opinion expression.
Dependency Argument: The word types and
POS types of the arguments of the dependency
patterns in which the opinion expression is
involved. We consider the same dependency
patterns that are used to generate candidates for
opinion-arg classification.
3.3 Joint Inference
The inference goal is to find the optimal prediction
for both opinion entity identification and opinion
relation extraction. For a given sentence, we de-
note O as a set of opinion candidates, Ak as a set
of argument candidates, where k denotes the type
of opinion relation ? IS-ABOUT or IS-FROM ?
and S as a set of within-sentence spans that cover
all of the opinion candidates and argument can-
1643
didates. We introduce binary variable xiz , where
xiz = 1 means span i is associated with label z.
We also introduce binary variable uij for every
pair of opinion candidate i and argument candidate
j, where uij = 1 means i forms an opinion rela-
tion with j, and binary variable vik for every opin-
ion candidate i in relation type k, where vik = 1
means i associates with an implicit argument in
relation k. Given the binary variables xiy, uij , vik,
it is easy to recover the entity and relation assign-
ment by checking which spans are labeled as opin-
ion entities, and which opinion span and argument
span form an opinion relation.
The objective function is defined as a linear
combination of the potentials from different pre-
dictors with a parameter ? to balance the contribu-
tion of two components: opinion entity identifica-
tion and opinion relation extraction.
argmax
x,u,v
?
?
i?S
?
z
fizxiz
+ (1? ?)
?
k
?
i?O
?
??
j?Ak
rijuij + ri?vik
?
?
(1)
It is subject to the following linear constraints:
Constraint 1: Uniqueness. For each span i, we
must assign one and only one label z, where z ?
{H,D, T,N}.
?
z
xiz = 1
Constraint 2: Non-overlapping. If two spans i and
j overlap, then at most one of the spans can be
assigned to a non-NONE entity label: H,D, T .
?
z 6=N
xiz +
?
z 6=N
xjz ? 1
Constraint 3: Consistency between the opinion-
arg and opinion-implicit-arg classifiers. For an
opinion candidate i, if it is predicted to have an
implicit argument in relation k, vik = 1, then no
argument candidate should form a relation with i.
If vik = 0, then there exists some argument can-
didate j ? Ak such that uij = 1. We introduce
two auxiliary binary variables aik and bik to limit
the maximum number of relations associated with
each opinion candidate to be less than or equal to
three5. When vik = 1, aik and bik have to be 0.
?
j?Ak
uij = 1? vik + aik + bik
aik ? 1? vik, bik ? 1? vik
Constraint 4: Consistency between opinion-arg
classifier and opinion entity extractor. Suppose
an argument candidate j in relation k is assigned
an argument label by the entity extractor, that is
xjz = 1 (z = T for IS-ABOUT relation and z = H
for IS-FROM relation), then there exists some opin-
ion candidates that associate with j. Similar to
constraint 3, we introduce auxiliary binary vari-
ables cj and dj to enforce that an argument j links
to at most three opinion expressions. If xjz = 0,
then no relations should be extracted for j.
?
i?O
uij = xjz + cjk + djk
cjk ? xjz, djk ? xjz
Constraint 5: Consistency between the opinion-
implicit-arg classifier and opinion entity extractor.
When an opinion candidate i is predicted to asso-
ciate with an implicit argument in relation k, that
is vik = 1, then we allow xiD to be either 1 or
0 depending on the confidence of labeling i as an
opinion expression. When vik = 0, there exisits
some opinion argument associated with the opin-
ion candidate, and we enforce xiD = 1, which
means the entity extractor agrees to label i as an
opinion expression.
vik + xiD ? 1
Note that in our ILP formulation, the label
assignment for a candidate span involves one
multiple-choice decision among different opinion
entity labels and the ?NONE? entity label. The
scores of different label assignments are compara-
ble for the same span since they come from one
entity extraction model. This makes our ILP for-
mulation advantageous over the ILP formulation
proposed in Choi et al (2006), which needs m bi-
nary decisions for a candidate span, wherem is the
number of types of opinion entities, and the score
for each possible label assignment is obtained by
5It is possible to add more auxiliary variables to allow
more than three arguments to link to an opinion expression,
but this rarely happens in our experiments. For the IS-FROM
relation, we set aik = 0, bik = 0 since an opinion expression
usually has only one holder.
1644
the sum of raw scores from m independent extrac-
tion models. This design choice also allows us
to easily deal with multiple types of opinion ar-
guments and opinion relations.
4 Experiments
For evaluation, we used version 2.0 of the MPQA
corpus (Wiebe et al, 2005; Wilson, 2008), a
widely used data set for fine-grained opinion anal-
ysis.6 We considered the subset of 482 docu-
ments7 that contain attitude and target annotations.
There are a total of 9,471 sentences with opinion-
related labels at the phrase level. We set aside 132
documents as a development set and use 350 doc-
uments as the evaluation set. All experiments em-
ploy 10-fold cross validation on the evaluation set;
the average over the 10 runs is reported.
Our gold standard opinion expressions, opinion
targets and opinion holders correspond to the di-
rect subjective annotations, target annotations and
agent annotations, respectively. The IS-FROM re-
lation is obtained from the agent attribute of each
opinion expression. The IS-ABOUT relation is ob-
tained from the attitude annotations: each opinion
expression is annotated with attitude frames and
each attitude frame is associated with a list of tar-
gets. The relations may overlap: for example, in
the following sentence, the target of relation 1 con-
tains relation 2.
[John]H1 is happyO1 because [[he]H2
lovesO2 [being at Enderly Park]T2]T1 .
We discard relations that contain sub-relations be-
cause we believe that identifying the sub-relations
usually is sufficient to recover the discarded rela-
tions. (Prediction of overlapping relations is con-
sidered as future work.) In the example above, we
will identify (loves, being at Enderly Park) as an
IS-ABOUT relation and happy as an opinion ex-
pression associated with an implicit target. Table 1
shows some statistics of the corpus.
We adopted the evaluation metrics for entity and
relation extraction from Choi et al (2006), which
include precision, recall, and F1-measure accord-
ing to overlap and exact matching metrics.8 We
6Available at http://www.cs.pitt.edu/mpqa/.
7349 news articles from the original MPQA corpus, 84
Wall Street Journal articles (Xbank), and 48 articles from the
American National Corpus.
8Overlap matching considers two spans to match if they
overlap, while exact matching requires two spans to be ex-
actly the same.
Opinion Target Holder
TotalNum 5849 4676 4244
Opinion-arg Relations Implicit Relations
IS-ABOUT 4823 1302
IS-FROM 4662 1187
Table 1: Data Statistics of the MPQA Corpus.
will focus our discussion on results obtained us-
ing overlap matching, since the exact boundaries
of opinion entities are hard to define even for hu-
man annotators (Wiebe et al, 2005).
We trained CRFs for opinion entity identifica-
tion using the following features: indicators for
words, POS tags, and lexicon features (the sub-
jectivity strength of the word in the Subjectivity
Lexicon). All features are computed for the cur-
rent token and tokens in a [?1,+1] window. We
used L2-regularization; the regularization param-
eter was tuned using the development set. We
trained the classifiers for relation extraction using
L1-regularized logistic regression with default pa-
rameters using the LIBLINEAR (Fan et al, 2008)
package. For joint inference, we used GLPK9 to
provide the optimal ILP solution. The parameter
? was tuned using the development set.
4.1 Baseline Methods
We compare our approach to several pipeline base-
lines. Each extracts opinion entities first using
the same CRF employed in our approach, and
then predicts opinion relations on the opinion en-
tity candidates obtained from the CRF prediction.
Three relation extraction techniques were used in
the baselines:
? Adj: Inspired by the adjacency rule used
in Hu and Liu (2004), it links each argu-
ment candidate to its nearest opinion candi-
date. Arguments that do not link to any opin-
ion candidate are discarded. This is also used
as a strong baseline in Choi et al (2006).
? Syn: Links pairs of opinion and argument
candidates that present prominent syntactic
patterns. (We consider the syntactic patterns
listed in Section 3.2.1.) Previous work also
demonstrates the effectiveness of syntactic
information in opinion extraction (Johansson
and Moschitti, 2012).
9http://www.gnu.org/software/glpk/
1645
Opinion Expression Opinion Target Opinion Holder
Method P R F1 P R F1 P R F1
CRF 82.21 66.15 73.31 73.22 48.58 58.41 72.32 49.09 58.48
CRF+Adj 82.21 66.15 73.31 80.87 42.31 55.56 75.24 48.48 58.97
CRF+Syn 82.21 66.15 73.31 81.87 30.36 44.29 78.97 40.20 53.28
CRF+RE 83.02 48.99 61.62 85.07 22.01 34.97 78.13 40.40 53.26
Joint-Model 71.16 77.85 74.35? 75.18 57.12 64.92?? 67.01 66.46 66.73??
CRF 66.60 52.57 58.76 44.44 29.60 35.54 65.18 44.24 52.71
CRF+Adj 66.60 52.57 58.76 49.10 25.81 33.83 68.03 43.84 53.32
CRF+Syn 66.60 52.57 58.76 50.26 18.41 26.94 74.60 37.98 50.33
CRF+RE 69.27 40.09 50.79 60.45 15.37 24.51 75 38.79 51.13
Joint-Model 57.39 62.40 59.79? 49.15 38.33 43.07?? 62.73 62.22 62.47??
Table 2: Performance on opinion entity extraction using overlap and exact matching metrics (the top table uses overlap and
the bottom table uses exact). Two-tailed t-test results are shown on F1 measure for our method compared to the other baselines
(statistical significance is indicated with ?(p < 0.05), ??(p < 0.005)).
IS-ABOUT IS-FROM
Method P R F1 P R F1
CRF+Adj 73.65 37.34 49.55 70.22 41.58 52.23
CRF+Syn 76.21 28.28 41.25 77.48 36.63 49.74
CRF+RE 78.26 20.33 32.28 74.81 37.55 50.00
CRF+Adj-merged-10-best 25.05 61.18 35.55 30.28 62.82 40.87
CRF+Syn-merged-10-best 41.60 45.66 43.53 48.08 54.03 50.88
CRF+RE-merged-10-best 51.60 33.09 40.32 47.73 54.40 50.84
Joint-Model 64.38 51.20 57.04?? 64.97 58.61 61.63??
Table 3: Performance on opinion relation extraction using the overlap metric.
? RE: Predicts opinion relations by employ-
ing the opinion-arg classifier and opinion-
implicit-arg classifier. First, the opinion-arg
classifier identifies pairs of opinion and argu-
ment candidates that form valid opinion rela-
tions, and then the opinion-implicit-arg clas-
sifier is used on the remaining opinion candi-
dates to further identify opinion expressions
without explicit arguments.
We report results using opinion entity candi-
dates from the best CRF output and from the
merged 10-best CRF output.10 The motivation of
merging the 10-best output is to increase recall for
the pipeline methods.
5 Results
Table 2 shows the results of opinion entity identi-
fication using both overlap and exact metrics. We
compare our approach with the pipeline baselines
and CRF (the first step of the pipeline). We can
see that our joint inference approach significantly
outperforms all the baselines in F1 measure on ex-
tracting all types of opinion entities. In general,
10It is similar to the merged 10-best baseline in Choi et
al. (2006). If an entity Ei extracted by the ith-best sequence
overlaps with an entity Ej extracted by the jth-best sequence,
where i ? j, then we discard Ej . If Ei and Ej do not over-
lap, then we consider both entities.
by adding the relation extraction step, the pipeline
baselines are able to improve precision over the
CRF but fail at recall. CRF+Syn and CRF+Adj
provide the same performance as CRF, since the
relation extraction step only affects the results of
opinion arguments. By incorporating syntactic
information, CRF+Syn provides better precision
than CRF+Adj on extracting arguments at the ex-
pense of recall. This indicates that using simple
syntactic rules would mistakenly filter many cor-
rect relations. By using binary classifiers to pre-
dict relations, CRF+RE produces high precision
on opinion and target extraction but also results in
very low recall. Using the exact metric, we ob-
serve the same general trend in the results as the
overlap metric. The scores are lower since the
metric is much stricter.
Table 3 shows the results of opinion relation ex-
traction using the overlap metric. We compare our
approach with pipelined baselines in two settings:
one employs relation extraction on 1-best output
of CRF (top half of table) and the other employs
the merged 10-best output of CRF (bottom half of
table). We can see that in general, using merged
10-best CRF outputs boosts the recall while sac-
rificing precision. This is expected since merging
the 10-best CRF outputs favors candidates that are
1646
IS-ABOUT Relation Extraction IS-FROM Relation Extraction
Method P R F1 P R F1
ILP-W/O-ENTITY 49.10 40.48 44.38 44.77 58.24 50.63
ILP-W-SINGLE-RE 63.88 49.35 55.68 53.64 65.02 58.78
ILP-W/O-IMPLICIT-RE 62.00 44.73 51.97 73.23 51.28 60.32
Joint-Model 64.38 51.20 57.04?? 64.97 58.61 61.63?
Table 4: Comparison between our approach and ILP baselines that omit some potentials in our approach.
believed to be more accurate by the CRF predictor.
If CRF makes mistakes, the mistakes will propa-
gate to the relation extraction step. The poor per-
formance on precision further confirms the error
propagation problem in the pipeline approaches.
In contrast, our joint-inference method success-
fully boosts the recall while maintaining reason-
able precision. This demonstrates that joint infer-
ence can effectively leverage the advantage of in-
dividual predictors and limit error propagation.
To demonstrate the effectiveness of different
potentials in our joint inference model, we con-
sider three variants of our ILP formulation that
omit some potentials in the joint inference: one
is ILP-W/O-ENTITY, which extracts opinion rela-
tions without integrating information from opin-
ion entity identification; one is ILP-W-SINGLE-RE,
which focuses on extracting a single opinion re-
lation and ignores the information from the other
relation; the third one is ILP-W/O-IMPLICIT-RE,
which omits the potential for opinion-implicit-arg
relation and assumes every opinion expression is
linked to an explicit argument. The objective func-
tion of ILP-W/O-ENTITY can be represented as
argmax
u
?
k
?
i?O
?
j?Ak
rijuij (2)
which is subject to constraints on uij to enforce
relations to not overlap and limit the maximum
number of relations that can be extracted for each
opinion expression and each argument. For ILP-
W-SINGLE-RE, we simply remove the variables as-
sociated with one opinion relation in the objective
function (1) and constraints. The formulation of
ILP-W/O-IMPLICIT-RE removes the variables as-
sociated with potential ri in the objective function
and corresponding constraints. It can be viewed
as an extension to the ILP approach in Choi et al
(2006) that includes opinion targets and uses sim-
pler ILP formulation with only one parameter and
fewer binary variables and constraints to represent
entity label assignments 11.
11We compared the proposed ILP formulation with the ILP
Table 4 shows the results of these methods on
opinion relation extraction. We can see that with-
out the knowledge of the entity extractor, ILP-
W/O-ENTITY provides poor performance on both
relation extraction tasks. This confirms the effec-
tiveness of leveraging knowledge from entity ex-
tractor and relation extractor. The improvement
yielded by our approach over ILP-W-SINGLE-RE
demonstrates the benefit of jointly optimizing dif-
ferent types of opinion relations. Our approach
also outperforms ILP-W/O-IMPLICIT-RE, which
does not take into account implicit relations. The
results demonstrate that incorporating knowledge
of implicit opinion relations is important.
6 Discussion
We note that the joint inference model yields a
clear improvement on recall but not on precision
compared to the CRF-based baselines. Analyz-
ing the errors, we found that the joint model ex-
tracts comparable number of opinion entities com-
pared to the gold standard, while the CRF-based
baselines extract significantly fewer opinion enti-
ties (around 60% of the number of entities in the
gold standard). With more extracted opinion enti-
ties, the precision is sacriced but recall is boosted
substantially, and overall we see an increase in
F-measure. We also found that a good portion
of errors were made because the generated candi-
dates failed to cover the correct solutions. Recall
that the joint model finds the global optimal solu-
tion over a set of opinion entity and relation can-
didates, which are obtained from the n-best CRF
predictions and constituents in the parse tree that
satisfy certain syntactic patterns. It is possible
that the generated candidates do not contain the
gold standard answers. For example, our model
failed to identify the IS-ABOUT relation (offers,
general aid) from the following sentence Powell
had contacted ... and received offersO1 of [gen-
formulation in Choi et al (2006) on extracting opinion hold-
ers, opinion expressions and IS-FROM relations, and showed
that the proposed ILP formulation performs better on all three
extraction tasks.
1647
eral aid]T1 ... because both the CRF predictor and
syntactic heuristics fail to capture (offers, general
aid) as a potential relation candidate. By applying
simple heuristics such as treating all verbs or verb
phrases as opinion candidates would not help be-
cause it would introduce a large number of nega-
tive candidates and lower the accuracy of relation
extraction (only 52% of the opinion expressions
are verbs or verb phrases and 64% of the opinion
targets are noun or noun phrases in the corpus we
used). Therefore a more effective candidate gen-
eration method is needed to allow more candidates
while limiting the number of negative candidates.
We also observed incorrect parsing to be a cause of
error. We hope to study ways to account for such
errors in our approach as future work.
For computational time, our ILP formulation
can be solved very efficiently using advanced ILP
solvers. In our experiment, using GLPK?s branch-
and-cut solver took 0.2 seconds to produce opti-
mal ILP solutions for 1000 sentences on a machine
with Intel Core 2 Duo CPU and 4GB RAM.
7 Conclusion
In this paper we propose a joint inference ap-
proach for extracting opinion-related entities and
opinion relations. We decompose the task into
different subproblems, and jointly optimize them
using constraints that aim to encourage their con-
sistency and reduce prediction uncertainty. We
show that our approach can effectively integrate
knowledge from different predictors and achieve
significant improvements in overall performance
for opinion extraction. For future work, we plan to
extend our model to handle more complex opinion
relations, e.g. nesting or cross-sentential relations.
This can be potentially addressed by incorporat-
ing more powerful predictors and more complex
linguistic constraints.
Acknowledgments
This work was supported in part by DARPA-BAA-
12-47 DEFT grant 12475008 and NSF grant BCS-
0904822. We thank Igor Labutov for helpful dis-
cussion and suggestions, Ainur Yessenalina for
early discussion of the work, as well as the reviews
for helpful comments.
References
E. Breck, Y. Choi, and C. Cardie. 2007. Identifying
expressions of opinion in context. In Proceedings of
the 20th international joint conference on Artifical
intelligence, pages 2683?2688. Morgan Kaufmann
Publishers Inc.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opin-
ions with conditional random fields and extraction
patterns. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 355?362.
Association for Computational Linguistics.
Y. Choi, E. Breck, and C. Cardie. 2006. Joint ex-
traction of entities and relations for opinion recog-
nition. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, pages 431?439. Association for Computational
Linguistics.
D. Das, A.F.T. Martins, and N.A. Smith. 2012. An
exact dual decomposition algorithm for shallow se-
mantic parsing with constraints. Proceedings of*
SEM.[ii, 10, 50].
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
M. Hu and B. Liu. 2004. Mining opinion features
in customer reviews. In Proceedings of the Na-
tional Conference on Artificial Intelligence, pages
755?760. Menlo Park, CA; Cambridge, MA; Lon-
don; AAAI Press; MIT Press; 1999.
Richard Johansson and Alessandro Moschitti. 2010a.
Reranking models in fine-grained opinion analysis.
In Proceedings of the 23rd International Conference
on Computational Linguistics, pages 519?527. As-
sociation for Computational Linguistics.
Richard Johansson and Alessandro Moschitti. 2010b.
Syntactic and semantic structure for opinion ex-
pression detection. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 67?76. Association for Com-
putational Linguistics.
Richard Johansson and Alessandro Moschitti. 2012.
Relational features in fine-grained opinion analysis.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed in
online news media text. In Proceedings of the Work-
shop on Sentiment and Subjectivity in Text, pages 1?
8. Association for Computational Linguistics.
N. Kobayashi, K. Inui, and Y. Matsumoto. 2007.
Extracting aspect-evaluation and aspect-of relations
in opinion mining. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
1648
Language Learning (EMNLP-CoNLL), pages 1065?
1074.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.
K. Liu, L. Xu, and J. Zhao. 2012. Opinion target
extraction using word-based translation model. In
Proceedings of the conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Now Pub.
V. Punyakanok, D. Roth, and W. Yih. 2008. The im-
portance of syntactic parsing and inference in se-
mantic role labeling. Computational Linguistics,
34(2):257?287.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In Proceedings of the 21st
international jont conference on Artifical intelli-
gence, pages 1199?1204. Morgan Kaufmann Pub-
lishers Inc.
G. Qiu, B. Liu, J. Bu, and C. Chen. 2011. Opinion
word expansion and target extraction through double
propagation. Computational linguistics, 37(1):9?
27.
R. Quirk, S. Greenbaum, G. Leech, J. Svartvik, and
D. Crystal. 1985. A comprehensive grammar of
the English language, volume 397. Cambridge Univ
Press.
D. Roth and W. Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. Defense Technical Information Center.
J. Ruppenhofer, S. Somasundaran, and J. Wiebe. 2008.
Finding the sources and targets of subjective expres-
sions. In Proceedings of LREC.
Vivek Srikumar and Dan Roth. 2011. A joint model
for extended semantic role labeling. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 129?139. Association
for Computational Linguistics.
V. Stoyanov and C. Cardie. 2008. Topic identification
for fine-grained opinion analysis. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics-Volume 1, pages 817?824. Asso-
ciation for Computational Linguistics.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language.
Language Resources and Evaluation, 39(2):165?
210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational linguistics, 35(3):399?433.
Theresa Wilson. 2008. Fine-Grained Subjectivity
Analysis. Ph.D. thesis, Ph. D. thesis, University of
Pittsburgh. Intelligent Systems Program.
Y. Wu, Q. Zhang, X. Huang, and L. Wu. 2009. Phrase
dependency parsing for opinion mining. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing: Volume 3-Volume
3, pages 1533?1541. Association for Computational
Linguistics.
B. Yang and C. Cardie. 2012. Extracting opinion
expressions with semi-markov conditional random
fields. In Proceedings of the conference on Empiri-
cal Methods in Natural Language Processing. Asso-
ciation for Computational Linguistics.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O?Brien-Strain. 2010. Extracting and ranking prod-
uct features in opinion documents. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters, pages 1462?1470. Asso-
ciation for Computational Linguistics.
1649
