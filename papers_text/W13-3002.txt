Proceedings of the 13th Meeting on the Mathematics of Language (MoL 13), pages 12?20,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Order and Optionality: Minimalist Grammars with Adjunction
Meaghan Fowlie
UCLA Linguistics
Los Angeles, California
mfowlie@ucla.edu
Abstract
Adjuncts are characteristically optional,
but many, such as adverbs and adjectives,
are strictly ordered. In Minimalist Gram-
mars (MGs), it is straightforward to ac-
count for optionality or ordering, but not
both. I present an extension of MGs, MGs
with Adjunction, which accounts for op-
tionality and ordering simply by keeping
track of two pieces of information at once:
the original category of the adjoined-to
phrase, and the category of the adjunct
most recently adjoined. By imposing a
partial order on the categories, the Adjoin
operation can require that higher adjuncts
precede lower adjuncts, but not vice versa,
deriving order.
1 Introduction
The behaviour of adverbs and adjectives has quali-
ties of both ordinary selection and something else,
something unique to that of modifiers. This makes
them difficult to model. Modifiers are generally
optional and transparent to selection while argu-
ments are required and driven by selection. In
languages with relatively strict word order, argu-
ments are strictly ordered, while modifiers may or
may not be. In particular, (Cinque, 1999) proposes
that adverbs, functional heads, and descriptive ad-
jectives are underlyingly uniformly ordered across
languages and models them by ordinary Merge or
selection. Such a model captures only the ordering
restrictions on these morphemes; it fails to cap-
ture their apparent optionality and transparency
to selection. I propose a model of these ordered
yet optional and transparent morphemes that intro-
duces a function Adjoin which operates on pairs
of categories: the original category of the modi-
fied phrase together with the category of the most
recently adjoined modifier. This allows the deriva-
tion to keep track of both the true head of the
phrase and the place in the Cinque hierarchy of
the modifier, preventing inverted modifier orders
in the absence of Move.
2 Minimalist Grammars
I formulate my model as a variant of Minimalist
Grammars (MGs), which are Stabler (1997)?s for-
malisation of Chomsky?s (1995) notion of feature-
driven derivations using the functions Merge and
Move. MGs are mildly context-sensitive, putting
them in the right general class for human lan-
guage grammars. They are also simple and intu-
itive to work with. Another useful property is that
the properties of well-formed derivations are eas-
ily separated from the properties of derived struc-
tures (Kobele et al, 2007). Minimalist Gram-
mars have been proposed in a number of vari-
ants, with the same set of well-formed derivations,
such as the string-generating grammar in Keenan
& Stabler (2003), the tree-generating grammars
in Stabler (1997) and Kobele et al(2007), and
the multidominant graph-generating grammar in
Fowlie (2011).
At the heart of each of these grammars is a
function that takes two derived structures and puts
them together, such as string concatenation or
tree/graph building. To make this presentation as
general as possible, I will simply call these func-
tions Com. I will give derived structures as strings
as (2003)?s grammar would generate them,1 but
this is just a place-holder for any derived structure
the grammar might be defined to generate.
Definition 2.1. A Minimalist Grammar is a five-
tuple G = ??, sel, lic,Lex ,M?. ? is a finite set
of symbols called the alphabet. sel?lic are finite
sets of base features. Let F={+f,-f,=X,X|f?
1Keenan & Stabler?s grammar also incorporates an addi-
tional element: lexical items are triples of string, features,
and lexical status, which allows derivation of Spec-Head-
Complement order. I will leave this out for simplicity, as it is
not relevant here.
12
lic, X? sel} be the features. For  the empty string,
Lex ? ? ? {} ? F ? is the lexicon, and M is the
set of operations Merge and Move. The language
LG is the closure of Lex under M . A set C ? F
of designated features can be added; these are the
types of complete sentences.
Minimalist Grammars are feature-driven,
meaning features of lexical items determine
which operations can occur and when. There are
two disjoint finite sets of features, selectional
features sel which drive the operation Merge
and licensing features lic which drive Move.
Merge puts two derived structures together; Move
operates on the already built structure. Each
feature has a positive and negative version, and
these features with their polarities make the set
F from which the feature stacks for Lexical
Items are drawn. In the course of the derivation
the features will be checked, or deleted, by the
operations Merge and Move.
Polarity? Pos Neg
for Merge =X X X? sel
for Move +f -f f? lic
Table 1: Features
In order for a derivation to succeed, LIs must be
in the following form:
=A =B+w +v=Y        ...X -f -g -h...
!"#$%&'()X*'$+,-'$./
0-1$23-2%)4$"#,'$35-00)#'-%%$')6&7$
89$)4-'3#)#9-2%)-#)3$0$1#3)-3)&4)1"#$%&'()Y:))89-3)-3)#9$)1&;<0$;$2#:
="19)3<$1-4-$')-3)$-#9$')6$'%$.)*=A,=B/)&')6&7$.)*+w,+v/:))>2)")#'".-#-&2"0)?@A"')4'";$5&'BC)#9$'$)-3);"D-;,;)&2$)3<$1-4-$'C)3&)#9$'$)5&,0.)A$)"#);&3#)&2$)4$"#,'$)-2)#9-3)0-3#:)Figure 1: LI template
For example, ?kick, =D=DV? takes a comple-
ment of category D, a specifier of category D, and
is itself a V. ?which, =ND-wh? takes an N as com-
plement forming a D phrase, which will move be-
cause of feature wh.
Merge and Move are defined over expres-
sions: sequences of pairs ?derived structure, fea-
ture stack?. The first pair in the sequence can be
thought of as the ?main? structure being built; the
remaining are waiting to move. An expression dis-
plays feature f just in case that feature is the first
feature in the feature stack of the first pair.
An MG essentially works as follows: Merge is a
binary operation driven by sel. It takes two expres-
sions and combines them into one just in case the
first expression displays =X and the second dis-
plays X for some X ? sel. Once the second ex-
pression is selected, it may still have features re-
maining; these are always negative licensing fea-
tures and mean that the second structure is going
to move. As such it is stored separately by the
derivation. When the matching positive licensing
feature comes up later in the derivation, the mov-
ing structure is combined again. This is Move.
Move also carries the requirement that for each
f?lic there be at most one structure waiting
to move. This is the shortest move constraint
(SMC).2
Definition 2.2 (Merge). For ?, ? sequences of
negative lic features, s, t derived structures:3
Merge(?s, =X?? ::moverss, ?t, X??::moverst) ={
(Com(s, t), ?) :: moverss ?moverst if ? = 
(s, ?) :: (t, ?) :: moverss ?moverst if ? 6= 
Definition 2.3 (Move). For ?, ?, ? sequences
of negative lic features, s, t derived structures,
suppose ?!?t, ?? ? movers such that ? =
-f?. Then: Move(?s, +f?? ::movers) ={
?Com(s, t), ?? :: movers? ?t, ?? if ? = 
?s, ?? :: ?t, ?? :: movers? ?t, ?? if ? 6= 
In this article I will make use of annotated
derivation trees, which are trees describing the
derivation. In addition to the name of the func-
tion, I (redundantly) include for clarity the derived
expressions in the form of strings and features, and
sometimes an explanation of why the function ap-
plied. For example, Figure 2 shows derivations
(unannotated and annotated) of the wolf with fea-
ture D.
Merge
the:=ND wolf:N
Merge
the wolf:D
the:=ND wolf:N
Figure 2: Unannotated and annotated derivation
trees
2The SMC is based on economy arguments in the linguis-
tic literature (Chomsky, 1995), but it is also crucial for a type
of finiteness: the valid derivation trees of an MG form a regu-
lar tree language (Kobele et al, 2007). The number of possi-
ble movers must be finite for the automaton to be finite-state.
The SMC could also be modified to allow up to a particular
(finite) number of movers for each f?lic.
3:: adds an element to a list; ? appends two lists; ? re-
moves an element from a list.
13
3 Cartography
The phenomena this model is designed to account
for are modifiers and other apparently optional
projections such as the following:
(1) a. The small ancient triangular green Irish pagan
metal artifact was lost.
b. *The metal green small artifact was lost.Adjec-
tives
c. Frankly, John probably once usually arrived
early.
d. *Usually, John early frankly once arrived prob-
ably. Adverbs
e. [DP
[DP
zhe
this
[NumP
[NumP
yi
one
[ClP
[ClP
zhi
CL
[NP
[NP
bi]]]
pen]]]
?this pen? Functional projections
These three phenomena can all display option-
ality, transparency to selection, and strict order-
ing. By transparency I mean that despite the inter-
vening modifiers, properties of the selected head
are relevant to selection. For example, in a classi-
fier language, the correct classifier selects a noun
even if adjectives intervene.
The hypothesis that despite their optionality
these projections are strictly ordered is part of syn-
tactic cartography (Rizzi, 2004). Cinque (1999,
2010) in particular proposes a universal hierar-
chy of functional heads that select adverbs in their
specifiers, yielding an order on both the heads and
the adverbs. He proposes a parallel hierarchy of
adjectives modifying nouns. These hierarchies are
very deep. The adverbs and functional heads in-
corporate 30 heads and 30 adverbs.
Cinque argues that the surprising univer-
sality of adverb order calls for explanation.
For example, Italian, English, Norwegian,
Bosnian/Serbo-Croatian, Mandarin Chinese,
and more show strong preferences for frankly
to precede (un)fortunately. These arguments
continue for a great deal more adverbs.4
(2) Italian
a. Francamente
Frankly
ho
have
purtroppo
unfortunately
una
a
pessima
bad
opinione
opinion
di
of
voi.
you
?Frankly I unfortunately have a very bad opin-
ion of you.?
b. *Purtroppo
Unfortuately
ho
have
francamente
frankly
una
a
pessima
bad
opinione
opinion
di
of
voi.
you
(3) English
a. Frankly, I unfortuately have a very bad opin-
ion of you
4Data from Cinque (1999)
b. ?Unfortunately I frankly have a very bad opin-
ion of you
(4) Norwegian
a. Per
Peter
forlater
leaves
[rerlig
[honestly
talt]
spoken]
[heldigvis]
[fortunately]
[nil]
[now]
selskapet.
the.party.
?Frankly, Peter is fortunately leaving the party
now.?
b. *Per
Peter
forlater
leaves
[heldigvis]
[fortunately]
[rerlig
[honestly
talt]
spoken]
[nil]
[now]
selskapet.
the.party.
(5) Bosnian/Serbo-Croatian
a. lskreno,
Frankly,
ja
I
naialost
unfortunately
imam
have
jako
very
lose
bad
misljenje
opinion
o
of
vama
you.
Frankly, I unfortunately have a very bad opin-
ion of you.?
b. *Naialost,
unfortunately
ja
I
iskreno
frankly
imam
have
jako
very
lose
bad
misljenje
opinion
o
of
varna.
you.
(6) Mandarin Chinese
a. laoshi-shuo
Frankly,
wo
I
buxing
unfortunately
dui
to
tamen
them
you
have
pian-jian.
prejudice
?Honestly I unfortunately have prejudice
against them.?
b. *buxing
unfortunately
wo
I
laoshi-shuo
Frankly
dui
to
tamen
them
you
have
pian-jian.
prejudice
Supposing these hierarchies are indeed univer-
sal, the grammar should account for it. Moreover,
in addition to strictly ordered adjuncts, ideally a
model of adjunction should account for unordered
adjuncts as well. For example, English PPs are
unordered:
(7) a. The alliance officer shot Kaeli in the cargo
hold with a gun.
b. The alliance officer shot Kaeli with a gun in
the cargo hold.
It is not unusual to see this kind of asymme-
try, where right adjuncts are unordered but left ad-
juncts are ordered.
4 Previous approaches to adjunction
This section provides a brief overview of four ap-
proaches to adjunction. The first two are from
a categorial grammar perspective and account for
the optionality and, more or less, transparency to
selection; however, they are designed to model un-
ordered adjuncts. The other two are MG formal-
14
isations of the cartographic approach. Since the
cartographic approach takes adjuncts to be regu-
lar selectors, unsurprisingly they account for or-
der, but not easily for optionality or transparency
to selection.
4.1 Categorial Grammar solutions
To account for the optionality and transparency, a
common solution is for a modifier to combine with
its modified phrase, and give the result the same
category as the original phrase. In traditional cate-
gorial grammars, a nominal modifier has category
N\N or N/N, meaning it combines with an N and
the result is an N.
Similarly, in MGs, an X-modifier has features
=XX: it selects an X and the resulting structure has
category feature X.
Merge
*the bad big wolf:D
the::=ND Merge
*bad big wolf:N
bad::=NN Merge
big wolf:N
big::=NN wolf::N
Figure 3: Traditional MG derivation of *the bad
big wolf
What this approach cannot account for is order-
ing. This is because the category of the new phrase
is the same regardless of the modifier?s place in the
hierarchy. That is, the very thing that accounts for
the optionality and the transparency of modifiers
(that the category does not change) is what makes
strict ordering impossible. Moreover, the modifier
is not truly transparent to selection: the modifier
in fact becomes the new head; it just happens to
share a category with the original head. This can
be seen in tree-generating grammars such as Sta-
bler (1997) (Figure 4).
Merge
? big, =NN? ?wolf, N?
<
big wolf
Figure 4: Derivation tree and derived bare tree.
The < points to the head, big.
4.1.1 Frey & Ga?rtner
Frey & Ga?rtner (2002) propose an improved ver-
sion of the categorial grammar approach, one
which keeps the modified element the head, giv-
ing true transparency to selection. They do this by
asymmetric feature checking.
To the basic MG formalism a third polarity is
added for sel, ?X. This polarity drives the added
function Adjoin. Adjoin behaves just like Merge
except that instead of cancelling both ?X and X,
it cancels only ?X, leaving the original X intact.
This allows the phrase to be selected or adjoined
to again by anything that selects or adjoins to X.
This model accounts for optionality and true trans-
parency: the modified element remains the head
(Figure 4.1.1).
Merge
?big, ?N? ?wolf, N?
>
big wolf
Figure 5: Frey & Ga?rtner: derivation tree and de-
rived bare tree. The > points to the head, wolf.
Since this grammar is designed to model un-
ordered modifiers, illicit orders are also derivable
(Figure 6).
Merge
*the bad big wolf:D
the::=ND Merge
*bad big wolf:N
bad::?N Merge
big wolf:N
big::?N wolf::N
Figure 6: F & G derivation of *the bad big wolf
4.2 Selectional approach
A third approach is to treat adjuncts just like any
other selector. This is the approach taken by syn-
tactic cartography. Such an approach accounts
straightforwardly for order, but not for optional-
ity or transparency; this is unsurprising since the
phenomena I am modelling share only ordering re-
strictions with ordinary selection.
The idea is to take the full hierarchy of modi-
fiers and functional heads, and have each select the
one below it; for example, big selects bad but not
vice versa, and bad selects wolf. However, here
we are left with the question of what to do when
bad is not present, and the phrase is just the big
wolf. big does not select wolf.
4.2.1 Silent, meaningless heads
The first solution is to give each modifier and
functional head a silent, meaningless version that
serves only to tie the higher modifier to the lower.
15
For example, we add to the lexicon a silent, mean-
ingless ?size? modifier that goes where big and
small and other LIs of category S go.
? ? the, =S D? ? , =S D?
? ? big, =G S? ? , =G S?
? ? bad, =N G? ? , =N G?
? ? wolf, N?
This solution doubles substantial portions of the
lexicon. Doubling is not computationally signif-
icant, but it does indicate a missing generalisa-
tion: somehow, it just happens that each of these
modifiers has a silent, meaningless doppelganger.
Relatedly, the ordering facts are epiphenomenal.
There is nothing forcing, say, D?s to always select
S?s. There is no universal principle predicting the
fairly robust cross-linguistic regularity.
Moreover, normally when something silent is in
the derivation, we want to say it is contributing
something semantically. Here these morphemes
are nothing more than a trick to hold the syntax
together. Surely we can do better.
4.2.2 Massive homophony
A second solution is for each morpheme in the
hierarchy to have versions that select each level
below it. For example, the has a version which
selects N directly, one that selects ?goodness? ad-
jectives like bad, one that selects ?size? adjectives
like big, and indeed one for each of the ten or so
levels of adjectives.
? ?the, =SD? ?the, =GD? ?the, =SD? ?the, =ND?
? ?big, =GS? ?big, =NatS??big, =NS?
? ?bad, =NatG? ?bad, =NG?
? ?Canadian, =NNat?
? ?wolf, N?
This second solution lacks the strangeness of
silent, meaningless elements, but computationally
it is far worse. To compute this we simply use
Gauss?s formula for adding sequences of numbers,
since an LI at level i in a hierarchy has i versions.
For example, in the model above, the is at level
4 (counting from 0), and there are 4 versions of
the. For a lexicon Lex without these duplicated
heads, and a language with k hierarchies of depths
li for each 1 ? i ? k, adding the duplicated heads
increases the size of the lexicon. The increase is
bounded below by a polynomial function of the
depths of the hierarchies as follows:5
|Lex?| ?
k?
i=1
1/2(l2i + li) + |Lex|
5 Proposal
I propose a solution with three components: sets
of categories defined to be adjuncts of particular
categories, a partial order on sel, and a new oper-
ation Adjoin. The sets of adjuncts I base on Sta-
bler (2013). The partial order models the hierar-
chies of interest (e.g. the Cinque hierarchy); Ad-
join is designed to be sensitive to the order.
Adjoin operates on pairs of selectional features.
The first element is the category of the first thing
that was adjoined to, for example N. The second
element is the category of the most recently ad-
joined element, for example Adj3. Adjoin is only
defined if the new adjunct is higher in the hierar-
chy than the last adjunct adjoined.
I call these grammars Minimalist Grammars
with Adjunction (MGAs).
Definition 5.1. A Minimalist Grammar with
Adjunction is a six-tuple
G = ??, ?sel,??, ad, lic,Lex ,M?. ? is a finite
set called the alphabet. sel?lic are finite sets of
base features, and ?sel,?? is a partial order. Let
F={+f,-f,=X,[X,Y]|f? lic, X,Y ? sel}.
ad : sel? P(sel) maps categories to their
adjuncts. Lex ? ? ? {} ? F ?, and M is the set
of operations Merge, Move, and Adjoin. The
language LG is the closure of Lex under M . A
set C ? sel of designated features can be added;
{[c, x]|c ? C, x ? sel, x ? c} are the types of
complete sentences.6
The differences between MGs defined above
and MGAs are: (1) in MGAs sel is partially or-
dered; (2) in MGs the negative polarity for X ?
sel is just X; in MGAs it is the pair [X,X]; (3)
MGAs add a function: Adjoin; (4) MGAs define
some subsets of sel to be adjuncts of certain cate-
gories; (5) Merge is redefined for the new feature
pair polarity. (Move remains unchanged.)
5I say ?bounded below? because this formula calculates
the increase to the lexicon assuming there is exactly one LI at
each level in the hierarchy. If there are more, each LI at level
i of a hierarchy has i versions as well.
6I have replaced all negative selectional features X with
pairs [X,X]. This is for ease of defining Adjoin and the new
Merge. Equivalently, LIs can start with category features X
as in a traditional MG, and Adjoin can build pairs. I chose
the formulation here because it halves the number of cases
for both Merge and Adjoin.
16
For ?A,?? a partial order, a, b ? A are incom-
parable, written a||b, iff a 6? b and b 6? a.
To shorten the definition of Adjoin, I define a
function f adj which determines the output features
under Adjoin. If the adjunct belongs to the hi-
erarchy of adjuncts being tracked by the second
element of the feature pair, that second element
changes. If not, the feature pair is unchanged.
Definition 5.2. For W, X, Y, Z ? sel, W ? ad(Y) :
f adj([W, X], [Y, Z]) =
?
??
??
[Y, W] if W ? Z
[Y, Z] if W||Z
undefined otherwise
Notice that if Z and W are incomparable, no
record is kept of the feature (W) of the adjunct.
This is just like Frey & Ga?rtner?s asymmetric fea-
ture checking, and derives adjuncts that are un-
ordered with respect to each other. In Definition
5.3, I model languages like English in which gen-
erally unordered adjuncts, like PPs, appear to the
right, while ordered adjuncts, like adjectives, ap-
pear to the left. The rules could be easily modified
for different orderings. See Section 6 for further
discussion.
Definition 5.3 (Adjoin). For s, t derived
structures, ?, ? ? {?f|f ? lic}?,
? ? {+f,= X|f ? lic, X ? sel}? ,
W, X, Y, Z ? sel, W ? ad(Y),
C = fadj([W, X], [Y, Z]):
Adjoin(?s, [W, X]???::mvrss,
?t, [Y, Z]?? :: mvrst) =?
????????????????????
????????????????????
?Com(s, t), ?C? :: mvrss ?mvrst
if ?, ? =  & W ? Z
?Com(t, s), ?C? :: mvrss ?mvrst
if ?, ? =  & W||Z
?s, ?C? :: ?t, ?? :: mvrss ?mvrst
if ? = , ? 6=  & W 6< Z
?t, ?C? :: ?s, ?? :: mvrss ?mvrst
if ? 6= , ? =  & W 6< Z
?, ?C? :: ?s, ?? :: ?t, ?? :: mvrss ?mvrst
if ?, ? 6=  & W 6< Z
The first case is for ordered adjuncts where nei-
ther the adjunct nor the adjoined-to phrase will
move (encoded in empty ?, ?). The second is the
same but for unordered adjuncts, which will ap-
pear on the right. The last three cases are for mov-
ing adjunct, moving adjoined-to phrase, and both
moving, respectively. ? is a sequence of positive
licensing features, which allows adjuncts to take
specifiers.
Merge needs a slight modification, to incorpo-
rate the paired categories. Notice that Merge is
interested only in the first element of the pair, the
?real? category.
Definition 5.4 (Merge). For ?, ? ? F ? , s, t
derived structures, X, Y ? sel:
Merge(?s,=X?? ::mvrss, ?t, [X, Y]??::mvrst) ={
(Com(s, t), ?) :: mvrss ?mvrst if ? = 
(s, ?) :: (t, ?) :: mvrss ?mvrst if ? 6= 
Move remains as in definition 2.3 above.
5.1 Examples
MGAs are most easily understood by example.
This first example demonstrates straightforward
applications of Adjoin that derive strictly-ordered
prenominal adjectives. The big bad wolf is deriv-
able because the derivation remembers that an N-
adjunct at level G in the hierarchy, ?bad, [G,G]?,
adjoined to the noun. It encodes this fact in the
second element of the pair [N,G]. Big is then able
to adjoin because it too is an N-adjunct and it is
higher in the hierarchy than bad (S>G). Finally,
the can be defined to select wolf directly.
Let sel = {D, G, M, N, P, C, T, V} and the partial
order ? on sel be such that D ? S ? G ? M ? N
and C ? T ? V
adjuncts = {?N, {S, G, M, P, C}?}
Lex = {?bad, [G,G]?, ?big, [S,S]?, ?the,
=N[D,D]?, ?wolf, [N,N]?, ?woods, [N,N]?,
?in, =D[P,P]?}
Merge
(the big bad wolf, [D,D])
(the, =N[D,D]) Adjoin
(big bad wolf, [N,S])
(since S?G and S?ad(N))
(big,[S,S]) Adjoin
(bad wolf, [N,G])
(since G?N and G?ad(N))
(bad,[G,G]) (wolf,[N,N])
Figure 7: Valid derivation of the big bad wolf
*Bad big wolf, on the other hand, is not deriv-
able without movement since the derivation re-
members that big, which is at level S in the hierar-
chy, has already been adjoined. bad, being lower
in the hierarchy, cannot adjoin.
17
Adjoin
*bad big wolf
(since G < S)
(bad, [G,G]) Adjoin
(big wolf, [N,S])
(since S?N and S?ad(N))
(big, [S,S]) (wolf, [N,N])
Figure 8: Invalid derivation of *bad big wolf
This next example shows a right adjunct, a PP,
being adjoined to an NP. Since P||N ? that is, no
hierarchical order is defined between N and P ?
the PP adjoins to the right, but does not alter the
category of the noun.
Adjoin
?wolf in the woods, [N, N]?
since P ?ad(N) and P||N
Merge
?in the woods, [P, P]?
?in, = D[P, P]? Merge
?the woods, [D, D]?
?the, =N[D, D]? ?woods, [N, N]?
?wolf, [N, N]?
Figure 9: Right adjunction
6 Discussion and extensions
This model captures both the strict ordering of the
merge-only models and the optionality and trans-
parency to selection of the categorial approaches.
Cinque?s observation that there is a hierarchy of
functional heads and adverbs is modelled directly
by defining a hierarchy in the grammar itself. The
strict linear order falls out of the order imposed on
the selectional features and the definition of Ad-
join: adjunction is only defined when the hierar-
chy is respected. Optionality is the result of the
transitivity of orders: intervening adjuncts are not
necessary for a higher one to be adjoined. Trans-
parency to selection is modelled by the pairing of
the selectional features: the original category of
the modified element is preserved, and Merge can
see only that feature. The adjuncts are literally ig-
nored.
The cross-linguistic consistency of the orders
is accounted for by the claim that all human lan-
guages have the same partial order on sel. As such,
it does not have to be learned, but rather comes
with the grammar.
Computationally, this approach has an advan-
tage over the merge-only model with homophony
as the latter increases the size of the lexicon by
a polynomial function in the depths of the hierar-
chies of adjuncts, but the former does not.
6.1 Left and right adjuncts
As mentioned, I defined Adjoin to derive the
asymmetry observed between left and right ad-
juncts in many languages: left adjuncts such as
adverbs and descriptive adjectives are strictly or-
dered, while right adjuncts such as PPs and clauses
are not. This fact is derived by letting the presence
or absence of an ordering relation between the ad-
junct and modified category determine which case
of Adjoin applies. If there is an order, the usual
linear order will be calculated by Com, and the
place in the hierarchy is tracked. Otherwise, the
linear order is switched, and there is asymmetric
feature checking.
If this is not the effect desired, there are alterna-
tives. The simplest is to make the domain of the
function ad sel ? {right, left}, specifying the sets
of right and left adjuncts. This allows for much
more flexibility, for good or ill. It does not de-
rive the asymmetry, but does allow ordered and
unordered adjuncts to appear on the same side of
the head, if such a pattern is desired. This is an
empirical question.
6.2 Selection and adjuncts
This model allows LIs that are in the set of ad-
juncts to be selected normally as arguments, since
adjuncts have categories of their own. For ex-
ample, Red Ridinghood was small is derivable by
allowing was to select ?small, [S,S]?: ?was,
=S[V,V]?. This is an improvement over models
that do not give adjuncts categories of their own,
such as Frey & Ga?rtner?s, but it is still lacking. In
this model, there will have to be massive dupli-
cation in the lexicon so that was can select every
adjective: ?was, =S[V,V]?, ?was, =G[V,V]?etc.
To solve this problem, we can take advantage
of the function ad, and define was to select any-
thing from a particular image under ad. Such a
model expands the definition of Merge to operate
not only on categories, but also on sets of cate-
gories. The model would look something like this:
Merge(?was, =ad(N)[V,V]?, ?small, [S,S]?)
is defined iff S? ad(N)
Because the set of features F is finite, allowing
Merge to be defined over subsets of F does not
change the finite properties of MGs. Merge could
in fact be allowed to be defined over any subset
18
of F . I suggest this model because it is restricted:
only sets that exist for other reasons already can
be quantified over.
MGAs also allow adjuncts to select arguments
and license Move. For example, a preposition can
select a complement before becoming an adjunct
PP. Moreover, a functional projection such as Fo-
cus can Move a focused phrase into its specifier
from the main tree, or Topic can Merge a specifier.
The latter is a result of allowing positive polarity
features to follow the category pair. Recall that in
traditional MGs, an LI must be of the following
form for the derivation to succeed, where each pi
is a positive polarity feature, X, Y ? sel and each
fi ? lic:
(= Y(p1p2...pn))X(-f1-f2...-fm)
However, in MGAs, LIs of the following form
are possible if the LI will Adjoin, the crucial dif-
ference being the presence of pn+1...pk:
(= Y(p1p2...pn))[X, Y](pn+1...pk)(-f1-f2...-fm)
Figure 10 shows the end of a derivation in which
the mover briefly is an adjunct, and so the licensor,
the null Foc head. Its positive licensing feature
+foc moves to the front of the stack of the derived
structure?s features.
Suppose Foc ? ad(T) and Foc ? T.
Move
?briefly she spoke, [T,Foc]?
Adjoin
?she spoke, +foc[T,Foc]?, ?briefly, -foc?
?, [Foc,Foc]+foc? Merge
?she spoke, [T,T]?, ?briefly, -foc?
Figure 10: Adjunct FocP with moved specifier.
6.3 Adjuncts of adjuncts
In natural language, adjuncts can also be adjoined
to, for example as in the very bad wolf. The func-
tion ad maps single categories to their adjuncts,
but it is not generally the case that, say, an adverb,
can only adjoin to certain adjectives. In order to
capture this fact without duplication in the lexi-
con, Adjoin, like Merge, can be extended to allow
subsets of F . Similarly to the Merge case, we can
restrict these subsets by requiring that they be the
image of a category under ad. For example:
?frankly, [Fr,Fr]?, ?unfortunately, [Fo,Fo]?, ?allegedly,
[Al,Al]?, ?bad, [G,G]?, ?wolf, [N,N]?? Lex
Fr ? Fo ? Al ? V, S ? G ? N, P
ad(N) = {S,G,P}
ad(V) = ad(S) = ad(G)= {Fr,Fo,Al}
Adjoin
?unfortunately bad, [G,G]?
(since Fo||G and Fo?ad(G))
?unfortunately, [Fo,Fo]? ?bad, [G,G]?
Figure 11: Adjoining to an adjunct
Notice however that we are still missing a gen-
eralisation: S,G, and indeed all adjectives have
the same adjuncts. Now, this can be modelled by
calling this set ad(ad(N)). However, such a solu-
tion assumes a special status for N over many other
categories such as G: why ad(ad(N)) rather than
ad(ad(G))? I would argue that such a status would
reflect the reality of natural language. We can see
N and V behaving in special ways: both are at the
bottom of hierarchies, for example. However, as
far as I am aware, no such status exists in any
MGs. Formalising these observations is a matter
for further research.
6.4 Islandhood
Adjuncts have another classic property: island-
hood. Movement is not possible out of certain
types of adjuncts.
(8) a. You left [because your ex showed up]Adj
b. *Who did you leave [because showed
up]Adj?
Any approach that keeps Adjoin separate from
Merge introduces the option of stipulating the Ad-
junct Island Constraint (AIC), either as a separate
constraint on Adjoin, as Frey & Ga?rtner do, or by
simply not including moverss in the definition of
Adjoin, making the function undefined when the
adjunct carries movers. This is not very satisfy-
ing, though: better perhaps would be to derive it,
as Graf (2013) does. On the other hand, perhaps
not all adjuncts are islands. If beside is an ad-
junct in (9), it is not an adjunct island.
(9) Who are you sitting [beside ]Adjunct?
As always, islands must remain a matter for fur-
ther research.
7 Conclusion
I have presented a model of adjunction that ac-
counts for both the optionality and the strict or-
19
dering of many adjuncts. MGAs accomplish this
by the simple expedience of keeping track of two
pieces of information at once: the original cate-
gory of the projecting phrase, and the category of
the most recent adjunct to adjoin. This allows Ad-
join to be defined to only apply when the next ad-
junct is not lower in a hierarchy than the last. At
the same time, Merge can see the original cate-
gory, and ignores the adjunct?s category.
I have also suggested some extensions of MGAs
to more efficiently account for adjuncts as the
second argument of Merge and Adjoin. These
involved quantification over categories, with the
added suggestion that the sets of categories in
question be restricted by the sets of adjuncts al-
ready defined.
Future directions for this research include not
only matters internal to the model, such as how
best to model adjuncts of adjuncts, but also
larger questions of the mathematical properties of
MGAs. MGAs are weakly equivalent to MGs,
since MGAs merely take existing ways to derive
certain strings and seek more efficient ways, which
capture more generalisations. If every adjunct in
the lexicon is replaced with the right set of selec-
tors, Adjoin does not need to be used. For exam-
ple, the adjectives in the MGA lexicon used in the
examples in Section 5.1 can be replaced by the ad-
jectives in either grammar from the selectional ap-
proaches in Section 4.2, and the same string set
can be generated.
Clearly MGs and MGAs are not strongly equiv-
alent: the derivation trees differ in that MGAs have
a function that is not present in MGs.
Because the possible configurations of features
remains finite, the derivation tree languages of
MGAs should prove to be regular, following Ko-
bele et al(2007)?s presentation: transition rules for
Adjoin need merely be added.
Also of interest are the subregular properties
of the derivation tree language. Although to my
knowledge such notions as tierwise strictly local
(Heinz et al, 2011) have not yet been formally
defined for tree languages, I conjecture that in
MGAs, Merge is tierwise strictly k-local, and Ad-
join is strictly k-local.
References
Noam Chomsky. 1995. The Minimalist Program. MIT
Press, Cambridge, MA.
Gugliemo Cinque. 1999. Adverbs and functional
heads: a cross-linguistic perspective. Oxford stud-
ies in comparative syntax. Oxford University Press,
Oxford.
Gugliemo Cinque. 2010. The syntax of adjectives: a
comparative study. Linguistic Inquiry monographs.
MIT Press, Cambridge, MA.
Meaghan Fowlie. 2011. Multidominant minimalist
grammars. Master?s thesis, University of California,
Los Angeles.
Werner Frey and Hans-Martin Ga?rtner. 2002. On the
treatment of scrambling and adjunction in minimal-
ist grammars. In Proceedings of the Conference on
Formal Grammar (FGTrento), pages 41?52, Trento.
Thomas Graf. 2013. The price of freedom: Why
adjuncts are islands. Slides of a talk given at the
Deutsche Gesellschaft fu?r Sprachwissenschaft 2013,
March 12?15, University of Potsdam, Potsdam, Ger-
many.
Jeffrey Heinz, Chetan Rawal, and Herbert Tanner.
2011. Tier-based strictly local constraints for
phonology. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Edward L. Keenan and Edward P. Stabler. 2003. Bare
Grammar. CSLI Publications, Stanford.
Gregory M. Kobele, Christian Retore?, and Sylvain Sal-
vati. 2007. An automata-theoretic approach to min-
imalism. In J. Rogers and S. Kepser, editors, Model
Theoretic Syntax at ESSLLI ?07. ESSLLI.
Luigi Rizzi. 2004. Locality and left periphery. In
Adriana Belletti, editor, Structures and Beyond:
The Cartography of Syntactic Structures, volume 3,
pages 223?251. Oxford University Press, Oxford.
Edward Stabler. 1997. Derivational minimalism. Log-
ical Aspects of Computational Linguistics, pages
68?95.
Edward Stabler. 2013. Bracketing paradoxes and
opacity: Parsing late adjunction in copy construc-
tions. Talk given at UCLA Mathematical Linguis-
tics Circle, April.
20
