Natural Reference to Objects in a Visual Domain
Margaret Mitchell
Computing Science Dept.
University of Aberdeen
Scotland, U.K.
Kees van Deemter
Computing Science Dept.
University of Aberdeen
Scotland, U.K.
{m.mitchell, k.vdeemter, e.reiter}@abdn.ac.uk
Ehud Reiter
Computing Science Dept.
University of Aberdeen
Scotland, U.K.
Abstract
This paper discusses the basic structures
necessary for the generation of reference
to objects in a visual scene. We construct
a study designed to elicit naturalistic re-
ferring expressions to relatively complex
objects, and find aspects of reference that
have not been accounted for in work on
Referring Expression Generation (REG).
This includes reference to object parts,
size comparisons without crisp measure-
ments, and the use of analogies. By draw-
ing on research in cognitive science, neu-
rophysiology, and psycholinguistics, we
begin developing the input structure and
background knowledge necessary for an
algorithm capable of generating the kinds
of reference we observe.
1 Introduction
One of the dominating tasks in Natural Language
Generation (NLG) is the generation of expressions
to pick out a referent. In recent years there has
been increased interest in generating referential
expressions that are natural, e.g., like those pro-
duced by people. Although research on the gener-
ation of referring expressions has examined differ-
ent aspects of how people generate reference, there
has been surprisingly little research on how people
refer to objects in a real-world setting. This paper
addresses this issue, and we begin formulating the
requirements for an REG algorithm that refers to
visible three-dimensional objects in the real world.
Reference to objects in a visual domain pro-
vides a straightforward extension of the sorts of
reference REG research already tends to consider.
Toy examples outline reference to objects, peo-
ple, and animals that are perceptually available
before the speaker begins generating an utterance
(Dale and Reiter, 1995; Krahmer et al, 2003; van
Deemter et al, 2006; Areces et al, 2008). Exam-
ple referents may be referred to by their color, size,
type (?dog? or ?cup?), whether or not they have a
beard, etc.
Typically, the reference process proceeds by
comparing the properties of the referent with the
properties of all the other items in the set. The
final expression roughly conforms to the Gricean
maxims (Grice, 1975).
However, when the goal is to generate natural
reference, this framework is too simple. The form
reference takes is profoundly affected by modality,
task, and audience (Chapanis et al, 1977; Cohen,
1984; Clark and Wilkes-Gibbs, 1986), and even
when these aspects are controlled, different people
will refer differently to the same object (Mitchell,
2008). In light of this, we isolate one kind of nat-
ural reference and begin building the algorithmic
framework necessary to generate the observed lan-
guage.
Psycholinguistic research has examined refer-
ence in a variety of settings, which may inform
research on natural REG, but it is not always clear
how to extend this work to a computational model.
This is true in part because these studies favor an
analysis of reference in the context of collabora-
tion; reference is embedded within language, and
language is often a joint activity. However, most
research on referring expression generation sup-
poses a solitary generating agent.1 This tacitly
assumes that reference will be taking place in a
monologue setting, rather than a dialogue or group
setting. Indeed, the goal of most REG algorithms
is to produce uniquely distinguishing, one-shot re-
ferring expressions.
Studies on natural reference usually use a
two person (speaker-listener) communication task
(e.g., Flavell et al, 1968; Krauss and Glucksberg,
1969; Ford and Olson, 1975). This research has
1A notable exception is Heeman and Hirst (1995).
shown that reference is more accurate and efficient
when it incorporates things like gesture and gaze
(Clark and Krych, 2004). There is a trade-off in
effort between initiating a noun phrase and refash-
ioning it so that both speakers understand the ref-
erent (Clark and Wilkes-Gibbs, 1986), and speak-
ers communicate to form lexical pacts on how
to refer to an object (Sacks and Schegloff, 1979;
Brennan and Clark, 1996). Mutual understanding
of referents is achieved in part by referring within
a subset of potential referents (Clark et al, 1983;
Beun and Cremers, 1998). A few studies have
compared monologue to dialogue reference, and
have shown that monologue references tend to be
harder for a later listener to disambiguate (Clark
and Krych, 2004) and that subsequent references
tend to be longer than those in dialogues (Krauss
and Weinheimer, 1967).
Aiming to generate natural reference in a mono-
logue setting raises questions about what an algo-
rithm should use to produce utterances like those
produced by people. In a monologue setting, the
speaker (or algorithm) gets no feedback from the
listener; the speaker?s reference is not tied to in-
teractions with other participants. The speaker
is therefore in a difficult position, attempting to
clearly convey a referent without being able to
check if the reference is understood along the way.
Recent studies that have focused on monologue
reference do so rather explicitly, which may af-
fect participant responses. These studies utilize
2D graphical depictions of simple 3D objects (van
Deemter et al, 2006; Viethen and Dale, 2008),
where a small set of properties can be used to dis-
tinguish one item from another. The expressions
are elicited in isolation, typed and then submitted,
which may hide some of the underlying referen-
tial processes. None of these studies utilize actual
objects. It is therefore difficult to use these data
to draw conclusions about how reference works in
naturalistic settings. It is unclear if these experi-
mental settings are natural enough, i.e., if they get
at reference as it may occur every day.
The study in this paper attempts to bring out in-
formation about reference in a number of ways.
First, we conduct the study in-person, using real-
world objects. This design invites referential phe-
nomena that may not have been previously ob-
served in simpler domains. Second, the refer-
ring expressions are produced orally. This allows
us access to reference as it is generated, without
the participants revising and so potentially obscur-
ing information about their reference. Third, we
use a relatively complicated task, where partici-
pants must explain how to use pieces to put to-
gether a picture of a face. The fact that we are
looking at reference is not made explicit, which
lessens any experimental effects caused by sub-
jects guessing the purpose of the study. This ap-
proach also situates reference within a larger task,
which may draw out aspects of reference not usu-
ally seen in experiments that elicit reference in iso-
lation. Fourth, the objects used display a variety
of different features: texture, material, color, size
along several dimensions, etc. This brings the data
set closer to objects that people interact with every
day. A monologue setting offers a picture of the
phenomena at play during a single individual?s re-
ferring expression generation.
The referring expressions gathered in this study
exhibit several aspects of reference that have not
yet been addressed in REG. This includes (1) part-
whole modularity; (2) size comparisons across
three dimensions; and (3) analogies. Work in cog-
nitive sciences suggests that these phenomena are
interrelated, and may be possible to represent in a
computational framework. This research also of-
fers connections to further aspects of natural refer-
ence that were not directly observed in the study,
but will need to be accounted for in future work on
naturalistic referring expression generation. Us-
ing these ideas, we begin formulating the struc-
tures that an REG algorithm would need in order
to produce reference to real-world objects in a vi-
sual setting.
Approaching REG in this way allows us to tie
research in the generation of referring expressions
to computational models of visual perception and
cognitively-motivated computer vision. Moving in
this direction offers the prospect of eventually de-
veloping an application for the generation of nat-
ural reference to objects automatically recognized
by a computer vision system.
In the next section, we describe our study. In
Section 3, we analyze the results and discuss what
they tell us about natural reference. In Section 4,
we draw on our results and cognitive models of ob-
ject recognition to begin building the framework
for a referring expression algorithm that generates
naturalistic reference to objects in a visual scene.
In Section 5, we offer concluding remarks and out-
line areas for further study.
Figure 1: Object Board.
2 Method
2.1 Subjects
The subjects were 20 residents of Aberdeen, Scot-
land, and included undergraduates, graduates, and
professionals. All were native speakers of English,
had normal or corrected vision, and had no other
known visual issues (such as color-blindness).
Subjects were paid for their participation. Two
recordings were left out of the analysis: one par-
ticipant?s session was not fully recorded due to a
software error, and one participant did not pick out
many objects in each face and so was not included.
The final set of participants included 18 people, 10
female and 8 male.
2.2 Materials
A board was prepared with 51 craft objects. The
objects were chosen from various craft sets, and
included pom-poms, pipe-cleaners, beads, and
feathers (see Table 1). The motley group of objects
had different colors, textures, shapes, patterns, and
were made of different materials. Similar objects
were grouped together on the board, with a label
placed underneath. This was done to control the
head noun used in each reference. The objects
were used to make up 5 different craft ?face? pic-
tures. Subjects sat at a desk facing the board and
the stack of pictures. A picture of the board is
shown in Figure 1.
Subjects were recorded on a head-mounted mi-
crophone, which fed directly into a laptop placed
on the left of the desk. The open-source audio-
recording program Audacity (Mazzoni, 2010) was
used to record the audio signal and export it to
wave format.
2.3 Procedure
Subjects were told to give instructions on how to
construct each face using the craft supplies on the
board. They were instructed to be clear enough for
a listener to be able to reconstruct each face with-
out the pictures, with only the board items in front
of them. A pilot study revealed that such open-
ended instructions left some subjects spending an
inordinate amount of time on the exact placement
of each piece, and so in the current study sub-
jects were told that each face should take ?a cou-
ple? minutes, and that the instructions should be
as clear as possible for a listener to use the same
objects in reconstructing the pictures without be-
ing ?overly concerned? with the details of exactly
how each piece is angled in relation to the other.
Subjects were first given a practice face to de-
scribe. This face was the same face for all subjects.
They were then allowed to voice any concerns or
ask questions, but the experimenter only repeated
portions of the original instructions; no new infor-
mation was given. The subject could then proceed
to the next four faces, which were in a random or-
der for each subject. A transcript of a single face
from a session is provided in Figure 2.
2.4 Analysis
The recordings of each monologue were tran-
scribed, including disfluencies, and each face sec-
tion (?eyes?, ?chin?, etc.) was marked. First refer-
ence to items on the board were annotated with
their corresponding item numbers, yielding 722
references.2 Initial references to single objects
were extracted, creating a final data set with 505
references to single objects.
3 Results
Each reference was annotated in terms of the prop-
erties used to pick out the referent. For exam-
ple, ?the red feather? was annotated as contain-
ing the <ATTRIBUTE:value> pairs <COLOR:red,
TYPE:feather>. Discerning properties from the
modifiers used in reference is generally straight-
forward, and all of the references produced may
be partially deconstructed using such properties.
2This corpus is available at
http://www.csd.abdn.ac.uk/?mitchema/craft corpus.
14 foam shapes 2 large red hearts 2 small red hearts 2 small neon green hearts
2 small blue hearts 1 small green heart 1 green triangle 1 red circle
1 red square 1 red rectangle 1 white rectangle
11 beads 4 large round wooden beads 2 small white plastic beads 2 brown patterned beads
1 gold patterned bead 1 shiny gold patterned heart 1 red patterned heart
9 pom poms 2 big green pom-poms 2 small neon green pom-poms 2 small silver pom-poms
1 small metallic green pom-pom 1 large white pom-pom 1 medium white pom-pom
8 pipe cleaners 1 gold pipe-cleaner 1 gold pipe-cleaner in half 1 silver pipe-cleaner
1 circular neon yellow soft pipe-cleaner 1 neon orange puffy pipe-cleaner 1 grey puffy pipe-cleaner
1 purple/yellow striped pipe-cleaner 1 brown/grey striped pipe-cleaner
5 feathers 2 purple feathers 2 red feathers 1 yellow feather
3 ribbons 1 gold sequined wavy ribbon 1 silver wavy ribbon 1 small silver wavy ribbon
1 star 1 gold star
Table 1: Board items.
<CHIN> Okay so this face again um this face has um uh
for the chin, it uses (10 a gold pipe-cleaner in a V shape)
where the bottom of the V is the chin. </CHIN>
<MOUTH> The mouth is made up of (9 a purple feather).
And the mouth is slightly squint, um as if the the person
is smiling or even smirking. So this this smile is almost
off to one side. </MOUTH>
<NOSE> The nose is uh (5 a wooden bead, a medium-
sized wooden bead with a hole in the center). </NOSE>
<EYES> And the eyes are made of (2,3 white pom-poms),
em just uh em evenly spaced in the center of the face.
</EYES>
<FOREHEAD> Em it?s see the person?s em top of the per-
son?s head is made out of (1 another, thicker pipe-cleaner
that?s uh a grey color, it?s kind of uh a knotted blue-type
pipe-cleaner). So that that acts as the top of the person?s
head. </FOREHEAD>
<HAIR> And down the side of the person?s face, there are
(7,8 two ribbons) on each side. (7,8 And those are silver
ribbons). Um and they just hang down the side of the face
and they join up the the grey pipe-cleaner and the top um
of the person?s head to the to the chin and then hang down
either side of the chin. </HAIR>
<EARS> And the person?s ears are made up of (4,6 two
beads, which are um love-heart-shaped beads), where the
points of the love-hearts are facing outwards. And those
are just placed um around same em same em horizontal
line as the nose of the person?s face is. </EARS>
Figure 2: Excerpt Transcript.
Using sets of properties to distinguish referents
is nothing new in REG. Algorithms for the genera-
tion of referring expressions commonly use this as
a starting point, proposing that properties are orga-
nized in some linear order (Dale and Reiter, 1995)
or weighted order (Krahmer et al, 2003) as input.
However, we find evidence that more is at play. A
breakdown of our findings is listed in Table 2.
3.1 Spatial Reference
In addition to properties that pick out referents,
throughout the data we see reference to objects
as they exist in space. Size is compared across
different dimensions of different objects, and ref-
erence is made to different parts of the objects,
picking out pieces within the whole. These two
phenomena ? relative size comparisons and part-
whole modularity ? point to an underlying spatial
object representation that may be utilized during
reference.
3.1.1 Relative Size Comparisons
A total of 122 (24.2%) references mention size
with a vague modifier (e.g., ?big?, ?wide?). This
includes comparative (e.g, ?larger?) and superla-
tive (e.g., ?largest?) size modifiers, which occur 40
(7.9%) times in the data set. Examples are given
below.
(1) ?the bigger pom-pom?
(2) ?the green largest pom-pom?
(3) ?the smallest long ribbon?
(4) ?the large orange pipe-cleaner?
Of the references that mention size, 35 (6.9%)
use a vague modifier that applies to one or two di-
mensions. This includes modifiers for height (?the
short silver ribbon?), width (?quite a fat rectan-
gle?), and depth (?the thick grey pipe-cleaner?).
87 (17.2%) use a modifier that applies to the over-
all size of the object (e.g., ?big? or ?small?). Table
3 lists these values. Crisp measurements (such as
?1 centimeter?) occur only twice (0.4%), with both
produced by the same participant.
Comparative/Superlative: 40 (7.9%)
Base: 82 (16.2%)
Height/Width/Depth: 35 (6.9%)
Overall size: 87 (17.2%)
Table 3: Size Modifier Breakdown.
Part-whole modularity Relative size Analogies
?a green pom-pom. . . ?a red foam-piece. . . ?a natural-looking piece
with the tinsel on the outside? which is more square of pipe-cleaner, it looks
?your gold twisty ribbon. . . in shape rather than a bit like a rope?
with sequins on it? the longer rectangle? ?a pipe-cleaner that
?a wooden bead. . . ?the grey pipe-cleaner. . . looks a bit like. . .
with a hole in the center? which is the thicker one. . . a fluffy caterpillar?
?one of the green pom-poms. . . ?the slightly larger one? ?the silver ribbon
with the sort of strands ?the smaller silver ribbon? that?s almost like
coming out from it.? ?the short silver ribbon? a big S shape.?
?the silver ribbon. . . with the chainmail ?quite a fat rectangle? ?a. . . pipe-cleaner
detail down through the middle of it.? ?thick grey pipe-cleaner? that looks like tinsel.?
11 References 122 References 16 References
Table 2: Examples of Observed Reference.
Participants produce such modifiers without
sizes or measurements explicitly given; with an
input of a visual object presentation, the output
includes size modifiers. Such data suggests that
natural reference in a visual domain utilizes pro-
cesses comparing the length, width, and height of
a target object with other objects in the set. Indeed,
5 references (1.0%) in our data set include explicit
comparison with the size of other objects.
(5) ?a red foam-piece. . . which is more square in
shape rather than the longer rectangle?
(6) ?the grey pipe-cleaner. . . which is the thicker
one. . . of the selection?
(7) ?the shorter of the two silver ribbons?
(8) ?the longer one of the ribbons?
(9) ?the longer of the two silver ribbons?
In Example (5), height and width across two
different objects are compared, distinguishing a
square from a rectangle. In (6) ?thicker? marks
the referent as having a larger circumference than
other items of the same type. (7) (8) and (9) com-
pare the height of the target referent to the height
of similar items.
The use of size modifiers in a domain without
specified measurements suggests that when peo-
ple refer to an object in a visual domain, they
are sensitive to its size and structure within a di-
mensional, real-world space. Without access to
crisp measurements, people compare relative size
across different objects, and this is reflected in the
expressions they generate. These comparisons are
not only limited to overall size, but include size
in each dimension. This suggests that objects?
structures within a real-world space are relevant
to REG in a visual domain.
3.1.2 Part-Whole Modularity
The role that a spatial object understanding has
within reference is further detailed by utterances
that pick out the target object by mentioning an ob-
ject part. 11 utterances (2.2%) in our data include
mention of an object part within reference to the
whole object. This is spread across participants,
such that half of the participants make reference
to an object part at least once.
(10) ?a green pom-pom, which is with the tinsel
on the outside?
(11) ?your gold twisty ribbon...with sequins on
it?
(12) ?a wooden bead...with a hole in the center?
In (10), pieces of tinsel are isolated from the
whole object and specified as being on the outside.
In (11), smaller pieces that lay on top of the ribbon
are picked out. And in (12), a hole within the bead
is isolated.
The use of part-whole modularity suggests an
understanding that parts of the object take up their
own space within the object. An object is not only
viewed as a whole during reference, but parts in,
on, and around it may be considered as well. For
an REG algorithm to generate these kinds of ref-
erences, it must be provided with a representation
that details the structure of each object.
3.2 ANALOGIES
The data from this study also provide information
on what can be expected from a knowledge base
in an algorithm that aims to generate naturalistic
reference. Reference is made 16 times (3.2%) to
objects not on the board, where the intended refer-
ent is compared against something it is like. Some
examples are given below.
(13) ?a gold. . . pipe-cleaner. . . completely
straight, like a ruler?
(14) ?a natural-looking piece of pipe-cleaner, it
looks a bit like a rope?
(15) ?a pipe-cleaner that looks a bit like. . . a
fluffy caterpillar. . . ?
In (13), a participant makes reference to a
SHAPE property of an object not on the board. In
(14) and (15), participants refer to objects that may
share a variety of properties with the referent, but
are also not on the board.
Reference to these other items do not pick out
single objects, but types of objects (e.g., an object
type, not token). They correspond to some pro-
totypical idea of an object with properties similar
to those of the referent. Work by Rosch (1975)
has examined this tendency, introducing the idea
of prototype theory, which proposes that there may
be some central, ?prototypical? notions of items. A
knowledge base with stored prototypes could be
utilized by an REG algorithm to compare the tar-
get referent to item prototypes. Such representa-
tions would help guide the generation of reference
to items not in the scene, but similar to the target
referent.
4 Discussion
We have discussed several different aspects of ref-
erence in a study where referring expressions are
elicited for objects in a spatial, visual scene. Ref-
erence in this domain draws on object forms as
they exist in a three-dimensional space and uti-
lizes background knowledge to describe referents
by analogy to items outside of the scene. This
is undoubtedly not an exhaustive account of the
phenomena at play in such a domain, but offers
some initial conclusions that may be drawn from
exploratory work of this kind.
Before continuing with the discussion, it is
worthwhile to consider whether some of our data
might be seen as going beyond reference. Perhaps
the participants are doing something else, which
could be called describing. How to draw the line
between a distinguishing reference and a descrip-
tion, and whether such a line can be drawn at all, is
an interesting question. If the two are clearly dis-
tinct, then both are interesting to NLG research.
If the two are one in the same, then this sheds
some light on how REG algorithms should treat
reference. We leave a more detailed discussion of
this for future work, but note recent psycholinguis-
tic work suggesting that referring establishes (1)
an individual as the referent; (2) a conceptualiza-
tion or perspective on that individual (Clark and
Bangerter, 2004). Schematically, referring = indi-
cating + describing.
We now turn to a discussion of how the ob-
served phenomena may be best represented in an
REG algorithm. We propose that an algorithm ca-
pable of generating natural reference to objects in
a visual scene should utilize (1) a spatial object
representation; (2) a non-spatial feature-based rep-
resentation; and (3) a knowledge base of object
prototypes.
4.1 Spatial and Visual Properties
It is perhaps unsurprising to find reference that ex-
hibits spatial knowledge in a study where objects
are presented in three-dimensional space. Hu-
man behavior is anchored in space, and spatial in-
formation is essential for our ability to navigate
the world we live in. However, referring expres-
sion generation algorithms geared towards spa-
tial representations have oversimplified this ten-
dency, keeping objects within the realm of two-
dimensions and only looking at the spatial rela-
tions between objects.
For example, Funakoshi et al (2004) and Gatt
(2006) focus on how objects should be clustered
together to form groups. This utilizes some of
the spatial information between objects, but does
not address the spatial, three-dimensional nature
of objects themselves. Rather, objects exist as en-
tities that may be grouped with other entities in a
set or singled out as individual objects; they do not
have their own spatial characteristics. Similarly,
one of the strengths of the Graph-Based Algorithm
(Krahmer et al, 2003) is its ability to generate ex-
pressions that involve relations between objects,
and these include spatial ones (?next to?, ?on top
of?, etc.). In all these approaches, however, ob-
jects are essentially one-dimensional, represented
as individual nodes.
Work that does look at the spatial information
of different objects is provided by Kelleher et al
(2005). In this approach, the overall volume of
each object is calculated to assign salience rank-
ings, which then allow the Incremental Algorithm
(Dale and Reiter, 1995) to produce otherwise ?un-
derspecified? reference. The spatial properties of
the objects are kept relatively simple. They are
not used in constructing the referring expression,
but one aspect of the object?s three-dimensional
shape (volume) affects the referring expression?s
final form. To the authors? knowledge, the cur-
rent work is the first to suggest that objects them-
selves should have their spatial properties repre-
sented during reference.
Research in cognitive modelling supports the
idea that we attend to the spatial properties of ob-
jects when we view them (Blaser et al, 2000), and
that we have purely spatial attentional mechanisms
operating alongside non-spatial, feature-based at-
tentional mechanisms (Treue and Trujillo, 1999).
These feature-based attentional mechanisms pick
out properties commonly utilized in REG, such as
texture, orientation, and color. They also pick out
edges and corners, contrast, and brightness. Spa-
tial attentional mechanisms provide information
about where the non-spatial features are located in
relation to one another, size, and the spatial inter-
relations between component parts.
Applying these findings to our study, an REG
algorithm that generates natural reference should
utilize a visual, feature-based representation of ob-
jects alongside a structural, spatial representation
of objects. A feature-based representation is al-
ready common to REG, and could be represented
as a series of <ATTRIBUTE:value> pairs. A spa-
tial representation is necessary to define how the
object is situated within a dimensional space, pro-
viding information about the relative distances be-
tween object components, edges, and corners.
With such information provided by a spatial
representation, the generation of part-whole ex-
pressions, such as ?the pom-pom with the tinsel on
the outside?, is possible. This also allows for the
generation of size modifiers (?big?, ?small?) with-
out the need for crisp measurements, for example,
by comparing the difference in overall height of
the target object with other objects in the scene, or
against a stored prototype (discussed below). Rel-
ative size comparisons across different dimensions
would also be possible, used to generate size mod-
ifiers such as ?wide? and ?thick? that refer to one
dimensional axis.
4.2 Analogies
A feature-based and a spatial representation may
also play a role in analogies. When we use analo-
gies, as in ?the pipe-cleaner that looks like a cater-
pillar?, we use world knowledge about items that
are not themselves visible. Such an expression
draws on similarity that does not link the referent
with a particular object, but with a general type of
object: the pipe-cleaner is caterpillar-like.
To generate these kinds of expressions, an REG
algorithm would first need a knowledge base with
prototypes listing prototypical values of attributes.
For example, a banana prototype might have a pro-
totypical COLOR of yellow. With prototypes in the
knowledge base, the REG algorithm would need
to calculate similarity of a target referent to other
known items. This would allow a piece of yellow
cloth, for example, to be described as being the
color of a banana.
Implementing such similarity measures in an
REG algorithm will be challenging. One difficulty
is that prototype values may be different depend-
ing on what is known about an item; a prototypical
unripe banana may be green, or a prototypical rot-
ten banana brown. Another difficulty will be in
determining when a referent is similar enough to
a prototype to warrant an analogy. Additional re-
search is needed to explore how these properties
can be reasoned about.
4.3 Further Implications
A knowledge base containing prototypes opens up
the possibility of generating many other kinds of
natural references. In particular, such knowledge
would allow the algorithm to compute which prop-
erties a given kind of referent may be expected
to have, and which properties may be unexpected.
Unexpected properties may therefore stand out as
particularly salient.
For example, a dog missing a leg may be de-
scribed as a ?three-legged dog? because the pro-
totypical dog has four legs. We believe that this
perspective, which hinges on the unexpectedness
of a property, suggests a new approach to at-
tribute selection. Unlike the Incremental Algo-
rithm, the Preference Order that determines the or-
der in which attributes are examined would not be
fixed, but would depend on the nature of the refer-
ent and what is known about it.
Approaching REG in this way follows work in
cognitive science and neurophysiology that sug-
gests that expectations about objects? visual and
spatial characteristics are derived from stored rep-
resentations of object ?prototypes? in the infe-
rior temporal lobe of the brain (Logothetis and
- A spatial representation (depicting size, inter-
relations between component parts)
- A non-spatial, propositional representation
(describing color, texture, orientation, etc.)
- A knowledge base with stored prototypical ob-
ject propositional and spatial representations
Table 4: Requirements for an REG algorithm that
generates natural reference to visual objects.
Sheinberg, 1996; Riesenhuber and Poggio, 2000;
Palmeri and Gauthier, 2004). Most formal theo-
ries of object perception posit some sort of cate-
gory activation system (Kosslyn, 1994), a system
that matches input properties of objects to those
of stored prototypes, which then helps guide ex-
pectations about objects in a top-down fashion.3
This appears to be a neurological correlate of the
knowledge base we propose to underlie analogies.
Such a system contains information about pro-
totypical objects? component parts and where they
are placed relative to one another, as well as rele-
vant values for material, color, etc. This suggests
that the spatial and non-spatial feature-based rep-
resentations proposed for visible objects could be
used to represent prototype objects as well. In-
deed, how we view and refer to objects appears to
be influenced by the interaction of these structures:
Expectations about an object?s spatial properties
guide our attention towards expected object parts
and non-spatial, feature-based properties through-
out the scene (Kosslyn, 1994; Itti and Koch, 2001).
This affects the kinds of things we are most likely
to generate language about (Itti and Arbib, 2005).
We can now outline some general requirements
for an algorithm capable of generating naturalis-
tic reference to objects in a visual scene: Input to
such an algorithm should include a feature-based
representation, which we will call a propositional
representation, with values for color, texture, etc.,
and a spatial representation, with symbolic infor-
mation about objects? size and the spatial relation-
ships between components. A system that gener-
ates naturalistic reference must also use a knowl-
edge base storing information about object proto-
types, which may be represented in terms of their
own propositional/spatial representations.
3Note that this is not the only proposed matching structure
in the brain ? an exemplar activation system matches input to
stored exemplars.
5 Conclusions and Future Work
We have explored the interaction between view-
ing objects in a three-dimensional, spatial domain
and referring expression generation. This has led
us to propose structures that may be used to con-
nect vision in a spatial modality to naturalistic ref-
erence. The proposed structures include a spatial
representation, a propositional representation, and
a knowledge base with representations for object
prototypes. Using structures that define the propo-
sitional and spatial content of objects fits well with
work in psycholinguistics, cognitive science and
neurophysiology, and may provide the basis to
generate a variety of natural-sounding references
from a system that recognizes objects.
It is important to note that any naturalistic ex-
perimental design limits the kinds of conclusions
that can be drawn about reference. A study that
elicits reference to objects in a visual scene pro-
vides insight into reference to objects in a visual
scene; these conclusions cannot easily be extended
to reference to other kinds of phenomena, such as
reference to people in a novel. We therefore make
no claims about reference as a whole in this paper;
generalizations from this research can provide hy-
potheses for further testing in different modalities
and with different sorts of referents.
Our data leave open many areas for further
study, and we hope to address these in future work.
Experiments designed specifically to elicit relative
size modifiers, reference to object components,
and reference to objects that are like other things
would help further detail the form our proposed
structures take.
What is clear from our data is that both a spa-
tial understanding and a non-spatial feature-based
understanding appear to play a role in reference
to objects in a visual scene, and further, refer-
ence in such a setting is bolstered by a knowl-
edge base with stored prototypical object repre-
sentations. Utilizing structures representative of
these phenomena, we may be able to extend ob-
ject recognition research into object reference re-
search, generating natural-sounding reference in
everyday settings.
Acknowledgements
Thanks to Advaith Siddarthan for thought-
provoking discussions and to the anonymous re-
viewers for useful suggestions.
References
Carlos Areces, Alexander Koller, and Kristina Strieg-
nitz. 2008. Referring expressions as formulas of
description logic. Proceedings of the Fifth Inter-
national Natural Language Generation Conference,
pages 42?29.
Robbert-Jan Beun and Anita H. M. Cremers. 1998.
Object reference in a shared domain of conversation.
Pragmatics and Cognition, 6:121?52.
Erik Blaser, Zenon W. Pylyshyn, and Alex O. Hol-
combe. 2000. Tracking an object through feature
space. Nature, 408:196?199.
Susan E. Brennan and Herbert H. Clark. 1996. Con-
ceptual pacts and lexical choice in conversation.
Journal of Experimental Psychology: Learning,
Memory, and Cognition, 22:1482?93.
Alphonse Chapanis, Robert N. Parrish, Robert B.
Ochsman, and Gerald D. Weeks. 1977. Studies
in interactive communication: II. the effects of four
communication modes on the linguistic performance
of teams during cooperative problem solving. Hu-
man Factors, 19:101?125.
Herbert H. Clark and Adrian Bangerter. 2004. Chang-
ing ideas about reference. In Ira A. Noveck and Dan
Sperber, editors, Experimental pragmatics, pages
25?49. Palgrave Macmillan, Basingstoke, England.
Herbert H. Clark and Meredyth A. Krych. 2004.
Speaking while monitoring addressees for under-
standing. Journal of Memory and Language, 50:62?
81.
Herbert H. Clark and Deanna Wilkes-Gibbs. 1986. Re-
ferring as a collaborative process. Cognition, 22:1?
39.
Herbert H. Clark, Robert Schreuder, and Samuel But-
trick. 1983. Common ground and the understand-
ing of demonstrative reference. Journal of Verbal
Learning and Verbal Behavior, 22:1?39.
Philip R. Cohen. 1984. The pragmatics of referring
and the modality of communication. Computational
Linguistics, 10(2):97?146.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
18:233?263.
J. H. Flavell, P. T. Botkin, D. L. Fry Jr., J. W. Wright,
and P. E. Jarvice. 1968. The Development of Role-
Taking and Communication Skills in Children. John
Wiley, New York.
William Ford and David Olson. 1975. The elaboration
of the noun phrase in children?s description of ob-
jects. The Journal of Experimental Child Psychol-
ogy, 19:371?382.
Kotaro Funakoshi, Satoru Watanabe, Naoko Kuriyama,
and Takenobu Tokunaga. 2004. Generating refer-
ring expressions using perceptual groups. In Pro-
ceedings of the 3rd International Conference on Nat-
ural Language Generation, pages 51?60.
Albert Gatt. 2006. Structuring knowledge for refer-
ence generation: A clustering algorithm. Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL-06), pages 321?328.
Paul H. Grice. 1975. Logic and conversation. Syntax
and Semantics, 3:41?58.
Peter A. Heeman and Graeme Hirst. 1995. Collabo-
rating on referring expressions. Computational Lin-
guistics, 21.
Laurent Itti and Michael A. Arbib. 2005. Attention and
the minimal subscene. In Michael A. Arbib, editor,
Action to Language via the Mirror Neuron System.
Cambridge University Press.
Laurent Itti and Christof Koch. 2001. Computational
modelling of visual attention. Nature Reviews Neu-
roscience.
J. Kelleher, F. Costello, and J. van Genabith. 2005.
Dynamically structuring, updating and interrelating
representations of visual and linguistic discourse
context. Artificial Intelligence, 167:62?102.
Stephen M. Kosslyn. 1994. Image and Brain: The
Resolution of the Imagery Debate. MIT Press, Cam-
bridge, MA.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53?72.
Robert M. Krauss and Sam Glucksberg. 1969. The
development of communication: Competence as a
function of age. Child Development, 40:255?266.
Robert M. Krauss and Sidney Weinheimer. 1967. Ef-
fect of referent similarity and communication mode
on verbal encoding. Journal of Verbal Learning and
Verbal Behavior, 6:359?363.
Nikos K. Logothetis and David L. Sheinberg. 1996.
Visual object recognition. Annual Review Neuro-
science, 19:577?621.
Dominic Mazzoni. 2010. Audacity.
Margaret Mitchell. 2008. Towards the generation
of natural reference. Master?s thesis, University of
Washington.
Thomas J. Palmeri and Isabel Gauthier. 2004. Vi-
sual object understanding. Nature Reviews Neuro-
science, 5:291?303.
Maximilian Riesenhuber and Tomaso Poggio. 2000.
Models of object recognition. Nature Neuroscience
Supplement, 3:1199?1204.
Eleanor Rosch. 1975. Cognitive representation of
semantic categories. Journal of Experimental Psy-
chology, 104:192?233.
Harvey Sacks and Emanuel A. Schegloff. 1979. Two
preferences in the organization of reference to per-
sons in conversation and their interaction. In George
Psathas, editor, Everyday Language: Studies in Eth-
nomethodology, pages 15?21. Irvington Publishers,
New York.
Stegan Treue and Julio C. Martinez Trujillo. 1999.
Feature-based attention influences motion process-
ing gain in macaque visual cortex. Nature, 399:575?
579.
Kees van Deemter, Ielka van der Sluis, and Albert Gatt.
2006. Building a semantically transparent corpus
for the generation of referring expressions. In Pro-
ceedings of the 4th International Conference on Nat-
ural Language Generation, Sydney, Australia. ACL.
Jette Viethen and Robert Dale. 2008. The use of spatial
descriptions in referring expressions. In Proceed-
ings of the 5th International Conference on Natural
Language Generation, INLG-08, Salt Fork, Ohio.
ACL.
