Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 10?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Scalable Global Model for Summarization
Dan Gillick1,2, Benoit Favre2
1 Computer Science Division, University of California Berkeley, USA
2 International Computer Science Institute, Berkeley, USA
{dgillick,favre}@icsi.berkeley.edu
Abstract
We present an Integer Linear Program for
exact inference under a maximum coverage
model for automatic summarization. We com-
pare our model, which operates at the sub-
sentence or ?concept?-level, to a sentence-
level model, previously solved with an ILP.
Our model scales more efficiently to larger
problems because it does not require a
quadratic number of variables to address re-
dundancy in pairs of selected sentences. We
also show how to include sentence compres-
sion in the ILP formulation, which has the
desirable property of performing compression
and sentence selection simultaneously. The
resulting system performs at least as well as
the best systems participating in the recent
Text Analysis Conference, as judged by a va-
riety of automatic and manual content-based
metrics.
1 Introduction
Automatic summarization systems are typically ex-
tractive or abstractive. Since abstraction is quite
hard, the most successful systems tested at the Text
Analysis Conference (TAC) and Document Under-
standing Conference (DUC)1, for example, are ex-
tractive. In particular, sentence selection represents
a reasonable trade-off between linguistic quality,
guaranteed by longer textual units, and summary
content, often improved with shorter units.
Whereas the majority of approaches employ a
greedy search to find a set of sentences that is
1TAC is a continuation of DUC, which ran from 2001-2007.
both relevant and non-redundant (Goldstein et al,
2000; Nenkova and Vanderwende, 2005), some re-
cent work focuses on improved search (McDonald,
2007; Yih et al, 2007). Among them, McDonald is
the first to consider a non-approximated maximiza-
tion of an objective function through Integer Linear
Programming (ILP), which improves on a greedy
search by 4-12%. His formulation assumes that the
quality of a summary is proportional to the sum of
the relevance scores of the selected sentences, penal-
ized by the sum of the redundancy scores of all pairs
of selected sentences. Under a maximum summary
length constraint, this problem can be expressed as
a quadratic knapsack (Gallo et al, 1980) and many
methods are available to solve it (Pisinger et al,
2005). However, McDonald reports that the method
is not scalable above 100 input sentences and dis-
cusses more practical approximations. Still, an ILP
formulation is appealing because it gives exact so-
lutions and lends itself well to extensions through
additional constraints.
Methods like McDonald?s, including the well-
known Maximal Marginal Relevance (MMR) algo-
rithm (Goldstein et al, 2000), are subject to an-
other problem: Summary-level redundancy is not
always well modeled by pairwise sentence-level re-
dundancy. Figure 1 shows an example where the
combination of sentences (1) and (2) overlaps com-
pletely with sentence (3), a fact not captured by pair-
wise redundancy measures. Redundancy, like con-
tent selection, is a global problem.
Here, we discuss a model for sentence selection
with a globally optimal solution that also addresses
redundancy globally. We choose to represent infor-
10
(1) The cat is in the kitchen.
(2) The cat drinks the milk.
(3) The cat drinks the milk in the kitchen.
Figure 1: Example of sentences redundant as a group.
Their redundancy is only partially captured by sentence-
level pairwise measurement.
mation at a finer granularity than sentences, with
concepts, and assume that the value of a summary is
the sum of the values of the unique concepts it con-
tains. While the concepts we use in experiments are
word n-grams, we use the generic term to emphasize
that this is just one possible definition. Only credit-
ing each concept once serves as an implicit global
constraint on redundancy. We show how the result-
ing optimization problem can be mapped to an ILP
that can be solved efficiently with standard software.
We begin by comparing our model to McDonald?s
(section 2) and detail the differences between the re-
sulting ILP formulations (section 3), showing that
ours can give competitive results (section 4) and of-
fer better scalability2 (section 5). Next we demon-
strate how our ILP formulation can be extended to
include efficient parse-tree-based sentence compres-
sion (section 6). We review related work (section 7)
and conclude with a discussion of potential improve-
ments to the model (section 8).
2 Models
The model proposed by McDonald (2007) considers
information and redundancy at the sentence level.
The score of a summary is defined as the sum of
the relevance scores of the sentences it contains mi-
nus the sum of the redundancy scores of each pair of
these sentences. If si is an indicator for the presence
of sentence i in the summary, Reli is its relevance,
and Redij is its redundancy with sentence j, then a
summary is scored according to:
?
i
Relisi ?
?
ij
Redijsisj
Generating a summary under this model involves
maximizing this objective function, subject to a
2Strictly speaking, exact inference for the models discussed
in this paper is NP-hard. Thus we use the term ?scalable? in a
purely practical sense.
length constraint. A variety of choices for Reli and
Redij are possible, from simple word overlap met-
rics to the output of feature-based classifiers trained
to perform information retrieval and textual entail-
ment.
As an alternative, we consider information and re-
dundancy at a sub-sentence, ?concept? level, model-
ing the value of a summary as a function of the con-
cepts it covers. While McDonald uses an explicit
redundancy term, we model redundancy implicitly:
a summary only benefits from including each con-
cept once. With ci an indicator for the presence of
concept i in the summary, and its weight wi, the ob-
jective function is:
?
i
wici
We generate a summary by choosing a set of sen-
tences that maximizes this objective function, sub-
ject to the usual length constraint.
In summing over concept weights, we assume that
the value of including a concept is not effected by the
presence of any other concept in the summary. That
is, concepts are assumed to be independent. Choos-
ing a suitable definition for concepts, and a map-
ping from the input documents to concept weights,
is both important and difficult. Concepts could be
words, named entities, syntactic subtrees or seman-
tic relations, for example. While deeper semantics
make more appealing concepts, their extraction and
weighting are much more error-prone. Any error in
concept extraction can result in a biased objective
function, leading to poor sentence selection.
3 Inference by ILP
Each model presented above can be formalized as an
Integer Linear Program, with a solution represent-
ing an optimal selection of sentences under the ob-
jective function, subject to a length constraint. Mc-
Donald observes that the redundancy term makes for
a quadratic objective function, which he coerces to
a linear function by introducing additional variables
sij that represent the presence of both sentence i and
sentence j in the summary. Additional constraints
ensure the consistency between the sentence vari-
ables (si, sj) and the quadratic term (sij). With li
the length of sentence i and L the length limit for
11
the whole summary, the resulting ILP is:
Maximize:
?
i
Relisi ?
?
ij
Redijsij
Subject to:
?
j
ljsj ? L
sij ? si sij ? sj ?i, j
si + sj ? sij ? 1 ?i, j
si ? {0, 1} ?i
sij ? {0, 1} ?i, j
To express our concept-based model as an ILP, we
maintain our notation from section 2, with ci an in-
dicator for the presence of concept i in the summary
and sj an indicator for the presence of sentence j in
the summary. We add Occij to indicate the occur-
rence of concept i in sentence j, resulting in a new
ILP:
Maximize:
?
i
wici
Subject to:
?
j
ljsj ? L
sjOccij ? ci, ?i, j (1)?
j
sjOccij ? ci ?i (2)
ci ? {0, 1} ?i
sj ? {0, 1} ?j
Note that Occ, like Rel and Red, is a constant pa-
rameter. The constraints formalized in equations (1)
and (2) ensure the logical consistency of the solu-
tion: selecting a sentence necessitates selecting all
the concepts it contains and selecting a concept is
only possible if it is present in at least one selected
sentence. Constraint (1) also prevents the inclusion
of concept-less sentences.
4 Performance
Here we compare both models on a common sum-
marization task. The data is part of the Text Analy-
sis Conference (TAC) multi-document summariza-
tion evaluation and involves generating 100-word
summaries from 10 newswire documents, each on
a given topic. While the 2008 edition of TAC also
includes an update task?additional summaries as-
suming some prior knowledge?we focus only on
the standard task. This includes 48 topics, averag-
ing 235 input sentences (ranging from 47 to 652).
Since the mean sentence length is around 25 words,
a typical summary consists of 4 sentences.
In order to facilitate comparison, we generate
summaries from both models using a common
pipeline:
1. Clean input documents. A simple set of rules
removes headers and formatting markup.
2. Split text into sentences. We use the unsuper-
vised Punkt system (Kiss and Strunk, 2006).
3. Prune sentences shorter than 5 words.
4. Compute parameters needed by the models.
5. Map to ILP format and solve. We use an open
source solver3.
6. Order sentences picked by the ILP for inclusion
in the summary.
The specifics of step 4 are described in detail in
(McDonald, 2007) and (Gillick et al, 2008). Mc-
Donald?s sentence relevance combines word-level
cosine similarity with the source document and the
inverse of its position (early sentences tend to be
more important). Redundancy between a pair of sen-
tences is their cosine similarity. For sentence i in
document D,
Reli = cosine(i,D) + 1/pos(i,D)
Redij = cosine(i, j)
In our concept-based model, we use word bi-
grams, weighted by the number of input documents
in which they appear. While word bigrams stretch
the notion of a concept a bit thin, they are eas-
ily extracted and matched (we use stemming to al-
low slightly more robust matching). Table 1 pro-
vides some justification for document frequency as a
weighting function. Note that bigrams gave consis-
tently better performance than unigrams or trigrams
for a variety of ROUGE measures. Normalizing
by document frequency measured over a generic set
(TFIDF weighting) degraded ROUGE performance.
3gnu.org/software/glpk
12
Bigrams consisting of two stopwords are pruned, as
are those appearing in fewer than three documents.
We largely ignore the sentence ordering problem,
sorting the resulting sentences first by source docu-
ment date, and then by position, so that the order of
two originally adjacent sentences is preserved, for
example.
Doc. Freq. (D) 1 2 3 4 5 6
In Gold Set 156 48 25 15 10 7
Not in Gold Set 5270 448 114 42 21 11
Relevant (P ) 0.03 0.10 0.18 0.26 0.33 0.39
Table 1: There is a strong relationship between the docu-
ment frequency of input bigrams and the fraction of those
bigrams that appear in the human generated ?gold? set:
Let di be document frequency i and pi be the percent of
input bigrams with di that are actually in the gold set.
Then the correlation ?(D,P ) = 0.95 for DUC 2007 and
0.97 for DUC 2006. Data here averaged over all prob-
lems in DUC 2007.
The summaries produced by the two systems
have been evaluated automatically with ROUGE and
manually with the Pyramid metric. In particular,
ROUGE-2 is the recall in bigrams with a set of
human-written abstractive summaries (Lin, 2004).
The Pyramid score arises from a manual alignment
of basic facts from the reference summaries, called
Summary Content Units (SCUs), in a hypothesis
summary (Nenkova and Passonneau, 2004). We
used the SCUs provided by the TAC evaluation.
Table 2 compares these results, alongside a base-
line that uses the first 100 words of the most re-
cent document. All the scores are significantly
different, showing that according to both human
and automatic content evaluation, the concept-
based model outperforms McDonald?s sentence-
based model, which in turn outperforms the base-
line. Of course, the relevance and redundancy func-
tions used for McDonald?s formulation in this exper-
iment are rather primitive, and results would likely
improve with better relevance features as used in
many TAC systems. Nonetheless, our system based
on word bigram concepts, similarly primitive, per-
formed at least as well as any in the TAC evaluation,
according to two-tailed t-tests comparing ROUGE,
Pyramid, and manually evaluated ?content respon-
siveness? (Dang and Owczarzak, 2008) of our sys-
tem and the highest scoring system in each category.
System ROUGE-2 Pyramid
Baseline 0.058 0.186
McDonald 0.072 0.295
Concepts 0.110 0.345
Table 2: Scores for both systems and a baseline on TAC
2008 data (Set A) for ROUGE-2 and Pyramid evalua-
tions.
5 Scalability
McDonald?s sentence-level formulation corresponds
to a quadratic knapsack, and he shows his particu-
lar variant is NP-hard by reduction to 3-D matching.
The concept-level formulation is similar in spirit to
the classical maximum coverage problem: Given a
set of items X , a set of subsets S of X , and an in-
teger k, the goal is to pick at most k subsets from
S that maximizes the size of their union. Maximum
coverage is known to be NP-hard by reduction to the
set cover problem (Hochbaum, 1996).
Perhaps the simplest way to show that our formu-
lation is NP-hard is by reduction to the knapsack
problem (Karp, 1972). Consider the special case
where sentences do not share any overlapping con-
cepts. Then, the value of each sentence to the sum-
mary is independent of every other sentence. This is
a knapsack problem: trying to maximize the value
in a container of limited size. Given a solver for our
problem, we could solve all knapsack problem in-
stances, so our problem must also be NP-hard.
With n input sentences and m concepts, both
formulations generate a quadratic number of con-
straints. However, McDonald?s has O(n2) variables
while ours has O(n + m). In practice, scalability
is largely determined by the sparsity of the redun-
dancy matrix Red and the sentence-concept matrix
Occ. Efficient solutions thus depend heavily on the
choice of redundancy measure in McDonald?s for-
mulation and the choice of concepts in ours. Prun-
ing to reduce complexity involves removing low-
relevance sentences or ignoring low redundancy val-
ues in the former, and corresponds to removing low-
weight concepts in the latter. Note that pruning con-
cepts may be more desirable: Pruned sentences are
irretrievable, but pruned concepts may well appear
in the selected sentences through co-occurrence.
Figure 2 compares ILP run-times for the two
13
formulations, using a set of 25 topics from DUC
2007, each of which have at least 500 input sen-
tences. These are very similar to the TAC 2008
topics, but more input documents are provided for
each topic, which allowed us to extend the analysis
to larger problems. While the ILP solver finds opti-
mal solutions efficiently for our concept-based for-
mulation, run-time for McDonald?s approach grows
very rapidly. The plot includes timing results for
250-word summaries as well, showing that our ap-
proach is fast even for much more complex prob-
lems: A rough estimate for the number of possible
summaries has
(500
4
) = 2.6?109 for 100-word sum-
maries and
(500
10
) = 2.5 ? 1020 for 250 words sum-
maries.
While exact solutions are theoretically appealing,
they are only useful in practice if fast approxima-
tions are inferior. A greedy approximation of our
objective function gives 10% lower ROUGE scores
than the exact solution, a gap that separates the high-
est scoring systems from the middle of the pack in
the TAC evaluation. The greedy solution (linear in
the number of sentences, assuming a constant sum-
mary length) marks an upper bound on speed and
a lower bound on performance; The ILP solution
marks an upper bound on performance but is subject
to the perils of exponential scaling. While we have
not experimented with much larger documents, ap-
proximate methods will likely be valuable in bridg-
ing the performance gap for complex problems. Pre-
liminary experiments with local search methods are
promising in this regard.
6 Extensions
Here we describe how our ILP formulation can
be extended with additional constraints to incor-
porate sentence compression. In particular, we
are interested in creating compressed alternatives
for the original sentence by manipulating its parse
tree (Knight and Marcu, 2000). This idea has been
applied with some success to summarization (Turner
and Charniak, 2005; Hovy et al, 2005; Nenkova,
2008) with the goal of removing irrelevant or redun-
dant details, thus freeing space for more relevant in-
formation. One way to achieve this end is to gen-
erate compressed candidates for each sentence, cre-
ating an expanded pool of input sentences, and em-
50 100 150 200 250 300 350 400 450 5000
1
2
3
4
5
6
7
8
Number of Sentences
Av
era
ge 
Tim
e p
er P
rob
lem
 (se
con
ds)
 
 
100 word summaries
250 word summaries
100 word summaries (McDonald)
Figure 2: A comparison of ILP run-times (on an AMD
1.8Ghz desktop machine) of McDonald?s sentence-based
formulation and our concept-based formulation with an
increasing number of input sentences.
ploy some redundancy removal on the final selec-
tion (Madnani et al, 2007).
We adapt this approach to fit the ILP formulations
so that the optimization procedure decides which
compressed alternatives to pick. Formally, each
compression candidate belongs to a group gk corre-
sponding to its original sentence. We can then craft
a constraint to ensure that at most one sentence can
be selected from group gk, which also includes the
original:
?
i?gk
si ? 1,?gk
Assuming that all the compressed candidates are
themselves well-formed, meaningful sentences, we
would expect this approach to generate higher qual-
ity summaries. In general, however, compression
algorithms can generate an exponential number of
candidates. Within McDonald?s framework, this
can increase the number of variables and constraints
tremendously. Thus, we seek a compact representa-
tion for compression in our concept framework.
Specifically, we assume that compression in-
volves some combination of three basic operations
on sentences: extraction, removal, and substitution.
In extraction, a sub-sentence (perhaps the content of
a quotation) may be used independently, and the rest
of the sentence is dropped. In removal, a substring
14
is dropped (a temporal clause, for example) that pre-
serves the grammaticality of the sentence. In sub-
stitution, one substring is replaced by another (US
replaces United States, for example).
Arbitrary combinations of these operations are
too general to be represented efficiently in an ILP.
In particular, we need to compute the length of a
sentence and the concepts it covers for all compres-
sion candidates. Thus, we insist that the operations
can only affect non-overlapping spans of text, and
end up with a tree representation of each sentence:
Nodes correspond to compression operations and
leaves map to the words. Each node holds the length
it contributes to the sentence recursively, as the sum
of the lengths of its children. Similarly, the concepts
covered by a node are the union of the concepts cov-
ered by its children. When a node is activated in the
ILP, we consider that the text attached to it is present
in the summary and update the length constraint and
concept selection accordingly. Figure 3 gives an ex-
ample of this tree representation for a sentence from
the TAC data, showing the derivations of some com-
pressed candidates.
For a given sentence j, let Nj be the set of nodes
in its compression tree, Ej ? Nj be the set of
nodes that can be extracted (used as independent
sentences), Rj ? Nj be the set of nodes that can
be removed, and Sj ? Nj be the set of substitu-
tion group nodes. Let x and y be nodes from Nj ; we
create binary variables nx and ny to represent the in-
clusion of x or y in the summary. Let x ? y denote
the fact that x ? Nj is a direct parent of y ? Nj .
The constraints corresponding to the compression
tree are:
?
x?Ej
nx ? 1 ?j (3)
?
x?y
ny = nx ?x ? Sj ?j (4)
nx ? ny ?(y ? x ? x /? {Rj ? Sj}) ?j (5)
nx ? ny ?(y ? x ? x /? {Ej ? Sj}) ?j (6)
Eq. (3) enforces that only one sub-sentence is ex-
tracted from the original sentence; eq. (4) enforces
that one child of a substitution group is selected if
and only if the substitution node is selected; eq. (5)
ensures that a child node is selected when its parent
is selected unless the child is removable (or a substi-
tution group); eq. (6) ensures that if a child node is
selected, its parent is also selected unless the child is
an extraction node (that can be used as a root).
Each node is associated with the words and the
concepts it contains directly (which are not con-
tained by a child node) in order to compute the new
length constraints and activate concepts in the ob-
jective function. We set Occix to represent the oc-
currence of concept i in node x as a direct child.
Let lx be the length contributed to node x as direct
children. The resulting ILP for performing sentence
compression jointly with sentence selection is:
Maximize:
?
i
wici
Subject to:
?
j
lxnx ? L
nxOccix ? ci, ?i, x?
x
nxOccix ? ci ?i
idem constraints (3) to (6)
ci ? {0, 1} ?i
nx ? {0, 1} ?x
While this framework can be used to imple-
ment a wide range of compression techniques, we
choose to derive the compression tree from the
sentence?s parse tree, extracted with the Berkeley
parser (Petrov and Klein, 2007), and use a set of
rules to label parse tree nodes with compression op-
erations. For example, declarative clauses contain-
ing a subject and a verb are labeled with the extract
(E) operation; adverbial clauses and non-mandatory
prepositional clauses are labeled with the remove
(R) operation; Acronyms can be replaced by their
full form by using substitution (S) operations and a
primitive form of co-reference resolution is used to
allow the substitution of noun phrases by their refer-
ent.
System R-2 Pyr. LQ
No comp. 0.110 0.345 2.479
Comp. 0.111 0.323 2.021
Table 3: Scores of the system with and without sentence
compression included in the ILP (TAC?08 Set A data).
When implemented in the system presented in
section 4, this approach gives a slight improvement
15
countries are    planning to hold the euroas part of their    reserves
the magazine quoted    chief Wilm Disenbergas sayingalready (ECB | European Central Bank)foreign currency
(1):E(2):E
(6):R (7):R(8):R
(4):R(3):S
A number of(5):R
Node Len. Concepts
(1):E 6 {the magazine, magazine quoted, chief Wilm, Wilm Disenberg}
(2):E 7 {countries are, planning to, to hold, hold the, the euro}
(3):S 0 {}
(3a) 1 {ECB}
(3b) 3 {European Central, Central Bank}
(4):R 2 {as saying}
(5):R 3 {a number, number of}
(6):R 1 {}
(7):R 5 {as part, part of, reserves}
(8):R 2 {foreign currency}
? Original: A number of Countries
are already planning to hold the
euro as part of their foreign cur-
rency reserves, the magazine quoted
European Central Bank chief Wim
Duisenberg as saying.
? [1,2,5,3a]: A number of countries are
planning to hold the euro, the maga-
zine quoted ECB chief Wim Duisen-
berg.
? [2,5,6,7,8]: A number of countries
are already planning to hold the euro
as part of their foreign currency re-
serves.
? [2,7,8]: Countries are planning to
hold the euro as part of their foreign
currency reserves.
? [2]: Countries are planning to hold
the euro.
Figure 3: A compression tree for an example sentence. E-nodes (diamonds) can be extracted and used as an indepen-
dent sentences, R-nodes (circles) can be removed, and S-nodes (squares) contain substitution alternatives. The table
shows the word bigram concepts covered by each node and the length it contributes to the summary. Examples of
resulting compression candidates are given on the right side, with the list of nodes activated in their derivations.
in ROUGE-2 score (see Table 3), but a reduction in
Pyramid score. An analysis of the resulting sum-
maries showed that the rules used for implementing
sentence compression fail to ensure that all com-
pression candidates are valid sentences, and about
60% of the summaries contain ungrammatical sen-
tences. This is confirmed by the linguistic qual-
ity4 score drop for this system. The poor quality
of the compressed sentences explains the reduction
in Pyramid scores: Human judges tend to not give
credit to ungrammatical sentences because they ob-
scure the SCUs.
We have shown in this section how sentence com-
pression can be implemented in a more scalable way
under the concept-based model, but it remains to be
shown that such a technique can improve summary
quality.
7 Related work
In addition to proposing an ILP for the sentence-
level model, McDonald (2007) discusses a kind of
summary-level model: The score of a summary is
4As measured according to the TAC?08 guidelines.
determined by its cosine similarity to the collection
of input documents. Though this idea is only imple-
mented with approximate methods, it is similar in
spirit to our concept-based model since it relies on
weights for individual summary words rather than
sentences.
Using a maximum coverage model for summa-
rization is not new. Filatova (2004) formalizes the
idea, discussing its similarity to the classical NP-
hard problem, but in the end uses a greedy approxi-
mation to generate summaries. More recently, Yih et
al. (2007) employ a similar model and uses a stack
decoder to improve on a greedy search. Globally
optimal summaries are also discussed by Liu (2006)
and Jaoua Kallel (2004) who apply genetic algo-
rithms for finding selections of sentences that maxi-
mize summary-level metrics. Hassel (2006) uses hill
climbing to build summaries that maximize a global
information criterion based on random indexing.
The general idea of concept-level scoring for
summarization is employed in the SumBasic sys-
tem (Nenkova and Vanderwende, 2005), which
chooses sentences greedily according to the sum
of their word values (values are derived from fre-
16
quency). Conroy (2006) describes a bag-of-words
model, with the goal of approximating the distribu-
tion of words from the input documents in the sum-
mary. Others, like (Yih et al, 2007) train a model to
learn the value of each word from a set of features
including frequency and position. Filatova?s model
is most theoretically similar to ours, though the con-
cepts she chooses are ?events?.
8 Conclusion and Future Work
We have synthesized a number of ideas from
the field of automatic summarization, including
concept-level weighting, a maximum coverage
model to minimize redundancy globally, and sen-
tence compression derived from parse trees. While
an ILP formulation for summarization is not novel,
ours provides reasonably scalable, efficient solutions
for practical problems, including those in recent
TAC and DUC evaluations. We have also shown
how it can be extended to perform sentence com-
pression and sentence selection jointly.
In ROUGE and Pyramid evaluation, our system
significantly outperformed McDonald?s ILP sys-
tem. However, we would note that better design of
sentence-level scoring would likely yield better re-
sults as suggested by the success of greedy sentence-
based methods at the DUC and TAC conferences
(see for instance (Toutanova et al, 2007)). Still, the
performance of our system, on par with the current
state-of-the-art, is encouraging.
There are three principal directions for future
work. First, word bigram concepts are convenient,
but semantically unappealing. We plan to explore
concepts derived from parse trees, where weights
may be a function of frequency as well as hierar-
chical relationships.
Second, our current approach relies entirely on
word frequency, a reasonable proxy for relevance,
but likely inferior to learning weights from train-
ing data. A number of systems have shown im-
provements by learning word values, though prelim-
inary attempts to improve on our frequency heuristic
by learning bigram values have not produced sig-
nificant gains. Better features may be necessary.
However, since the ILP gives optimal solutions so
quickly, we are more interested in discriminative
training where we learn weights for features that
push the resulting summaries in the right direction,
as opposed to the individual concept values.
Third, our rule-based sentence compression is
more of a proof of concept, showing that joint com-
pression and optimal selection is feasible. Better
statistical methods have been developed for produc-
ing high quality compression candidates (McDon-
ald, 2006), that maintain linguistic quality, some re-
cent work even uses ILPs for exact inference (Clarke
and Lapata, 2008). The addition of compressed sen-
tences tends to yield less coherent summaries, mak-
ing sentence ordering more important. We would
like to add constraints on sentence ordering to the
ILP formulation to address this issue.
Acknowledgments
This work is supported by the Defense Advanced
Research Projects Agency (DARPA) GALE project,
under Contract No. HR0011-06-C-0023. Any opin-
ions, findings, conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of DARPA.
References
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:273?381.
John M. Conroy, Judith D. Schlesinger, and Dianne P.
O?Leary. 2006. Topic-focused multi-document sum-
marization using an approximate oracle score. In Pro-
ceedings of COLING/ACL.
Hoa Trang Dang and Karolina Owczarzak. 2008.
Overview of the TAC 2008 Update Summarization
Task. In Proceedings of Text Analysis Conference.
E. Filatova and V. Hatzivassiloglou. 2004. Event-based
extractive summarization. In Proceedings of ACL
Workshop on Summarization, volume 111.
G. Gallo, PL Hammer, and B. Simeone. 1980. Quadratic
knapsack problems. Mathematical Programming
Study, 12:132?149.
D. Gillick, B. Favre, and D. Hakkani-Tur. 2008. The
ICSI Summarization System at TAC 2008. In Pro-
ceedings of the Text Understanding Conference.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark
Kantrowitz. 2000. Multi-document summarization by
sentence extraction. Proceedings of the ANLP/NAACL
Workshop on Automatic Summarization, pages 40?48.
17
Martin Hassel and Jonas Sjo?bergh. 2006. Towards
holistic summarization: Selecting summaries, not sen-
tences. In Proceedings of Language Resources and
Evaluation.
D.S. Hochbaum. 1996. Approximating covering and
packing problems: set cover, vertex cover, indepen-
dent set, and related problems. PWS Publishing Co.
Boston, MA, USA, pages 94?143.
E. Hovy, C.Y. Lin, and L. Zhou. 2005. A BE-based
multi-document summarizer with sentence compres-
sion. In Proceedings of Multilingual Summarization
Evaluation.
Fatma Jaoua Kallel, Maher Jaoua, Lamia Bel-
guith Hadrich, and Abdelmajid Ben Hamadou. 2004.
Summarization at LARIS Laboratory. In Proceedings
of the Document Understanding Conference.
Richard Manning Karp. 1972. Reducibility among com-
binatorial problems. Complexity of Computer Compu-
tations, 43:85?103.
Tibor Kiss and Jan Strunk. 2006. Unsupervised multi-
lingual sentence boundary detection. Computational
Linguistics, 32.
K. Knight and D. Marcu. 2000. Statistics-Based
Summarization-Step One: Sentence Compression. In
Proceedings of the National Conference on Artificial
Intelligence, pages 703?710. Menlo Park, CA; Cam-
bridge, MA; London; AAAI Press; MIT Press; 1999.
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Proceedings of the
Workshop on Text Summarization Branches Out (WAS
2004), pages 25?26.
D. Liu, Y. Wang, C. Liu, and Z. Wang. 2006. Multiple
Documents Summarization Based on Genetic Algo-
rithm. Lecture Notes in Computer Science, 4223:355.
N. Madnani, D. Zajic, B. Dorr, N.F. Ayan, and J. Lin.
2007. Multiple Alternative Sentence Compressions
for Automatic Text Summarization. In Proceed-
ings of the Document Understanding Conference at
NLT/NAACL.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proceedings of
the 11th EACL, pages 297?304.
R. McDonald. 2007. A Study of Global Inference Al-
gorithms in Multi-document Summarization. Lecture
Notes in Computer Science, 4425:557.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. In Proceedings of HLT-NAACL.
A. Nenkova and L. Vanderwende. 2005. The impact of
frequency on summarization. Technical Report MSR-
TR-2005-101, Microsoft Research, Redmond, Wash-
ington.
A. Nenkova. 2008. Entity-driven rewrite for multidocu-
ment summarization. Proceedings of IJCNLP.
Slav Petrov and Dan Klein. 2007. Learning and infer-
ence for hierarchically split PCFGs. In AAAI 2007
(Nectar Track).
D. Pisinger, A.B. Rasmussen, and R. Sandvik. 2005.
Solution of large-sized quadratic knapsack problems
through aggressive reduction. INFORMS Journal on
Computing.
K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi,
H. Suzuki, and L. Vanderwende. 2007. The Pythy
Summarization System: Microsoft Research at DUC
2007. In Proceedings of the Document Understanding
Conference.
J. Turner and E. Charniak. 2005. Supervised and Unsu-
pervised Learning for Sentence Compression. In Pro-
ceedings of ACL.
W. Yih, J. Goodman, L. Vanderwende, and H. Suzuki.
2007. Multi-document summarization by maximiz-
ing informative content-words. In International Joint
Conference on Artificial Intelligence.
18
