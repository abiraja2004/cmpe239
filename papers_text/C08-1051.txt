Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 401?408
Manchester, August 2008
Using Hidden Markov Random Fields to Combine Distributional and
Pattern-based Word Clustering
Nobuhiro Kaji and Masaru Kitsuregawa
Institute of Industrial Science, University of Tokyo
4-6-1 Komaba, Meguro-ku, Tokyo 153-8505 Japan
{kaji,kitsure}@tkl.iis.u-tokyo.ac.jp
Abstract
Word clustering is a conventional and im-
portant NLP task, and the literature has
suggested two kinds of approaches to this
problem. One is based on the distribu-
tional similarity and the other relies on
the co-occurrence of two words in lexico-
syntactic patterns. Although the two meth-
ods have been discussed separately, it is
promising to combine them since they are
complementary with each other. This pa-
per proposes to integrate them using hid-
den Markov random fields and demon-
strates its effectiveness through experi-
ments.
1 Introduction
Word clustering is a technique of grouping similar
words together, and it is important for various NLP
systems. Applications of word clustering include
language modeling (Brown et al, 1992), text clas-
sification (Baker and McCallum, 1998), thesaurus
construction (Lin, 1998) and so on. Furthermore,
recent studies revealed that word clustering is use-
ful for semi-supervised learning in NLP (Miller et
al., 2004; Li and McCallum, 2005; Kazama and
Torisawa, 2008; Koo et al, 2008).
A well-known approach to grouping similar
words is to use distribution of contexts in which
target words appear. It is founded on the hypothe-
sis that similar words tend to appear in similar con-
texts (Harris, 1968). Based on this idea, some stud-
ies proposed probabilistic models for word cluster-
ing (Pereira et al, 1993; Li and Abe, 1998; Rooth
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
et al, 1999; Torisawa, 2002). Others proposed
distributional similarity measures between words
(Hindle, 1990; Lin, 1998; Lee, 1999; Weeds et al,
2004). Once such similarity is defined, it is trivial
to perform clustering.
On the other hand, some researchers utilized
co-occurrence for word clustering. The idea be-
hind it is that similar words tend to co-occur in
certain patterns. Considerable efforts have been
devoted to measure word similarity based on co-
occurrence frequency of two words in a window
(Church and Hanks, 1989; Turney, 2001; Terra and
Clarke, 2003; Matsuo et al, 2006). In addition to
the classical window-based technique, some stud-
ies investigated the use of lexico-syntactic patterns
(e.g., X or Y) to get more accurate co-occurrence
statistics (Chilovski and Pantel, 2004; Bollegala et
al., 2007).
These two approaches are complementary with
each other, because they are founded on different
hypotheses and utilize different corpus statistics.
Consider to cluster a set of words based on the dis-
tributional similarity. It is likely that some words
are difficult to cluster due to the data sparseness or
some other problems, while we can still expect that
those words are correctly classified using patterns.
This consideration leads us to combine distribu-
tional and pattern-based word clustering. In this
paper we propose to combine them using mixture
models based on hidden Markov random fields.
This model was originally proposed by (Basu et
al., 2004) for semi-supervised clustering. In semi-
supervised clustering, the system is provided with
supervision in the form of pair-wise constraints
specifying data points that are likely to belong to
the same cluster. These constraints are directly in-
corporated into the clustering process as a prior
knowledge. Our idea is to view the co-occurrence
401
of two words in lexico-syntactic patterns as con-
straints, and incorporate them into distributional
word clustering.
In summary, this paper discusses the problem
of integrating multiple approaches for word clus-
tering. We consider that the clustering results are
improved if multiple approaches are successfully
combined and if they are complementary with each
other. Our contribution is to provide a proba-
bilistic framework for this problem. Although our
proposal aims at combining the distributional and
pattern-based approaches, it is also applicable to
combine other approaches like (Lin et al, 2003),
as we will discuss in Section 5.4.
2 Distributional Clustering
This and next section describe distributional and
pattern-based word clustering respectively. Sec-
tion 4 will explain how to combine them.
2.1 Probabilistic model
In distributional word clustering, similarity be-
tween words (= nouns) is measured by the distribu-
tion of contexts in which they appear. As a context,
verbs that appear in certain grammatical relations
with the target nouns are typically used. Using the
distribution of such verbs, we can express a noun
n by a feature vector ?(n):
?(n) = (f
nv
1
, f
nv
2
, ...f
nv
V
)
where f
nv
i
denotes the frequency of noun-verb
pair (n, v
i
), and V denotes the number of distinct
verbs. The basic idea of using the distribution for
clustering is to group n and n? together if?(n) and
?(n
?
) are similar.
Let us consider a soft clustering model. We hy-
pothesize that ?(n) is a mixture of multinomial,
and the probability of n is defined by1
p(n) =
Z
?
z=1
p(z)p(?(n)|z)
=
Z
?
z=1
?
z
f
n
!
?
v
f
nv
!
?
v
?
f
nv
vz
where Z is the number of mixture components,
?
z
is the mixing coefficient (?
z
?
z
= 1), f
n
=
?
v
f
nv
is the total number of occurrence of n, and
1We ignored p(f
n
) by assuming that it is independent of
hidden variables. See (McCallum and Nigam, 1998) for detail
discussion.
?
vz
is the parameter of the multinomial distribu-
tion (?
v
?
vz
= 1). In this model the hidden vari-
ables can be interpreted as semantic class of nouns.
Now consider a set of nouns n = {n
i
}
N
i=1
. Let
z = {z
i
}
N
i=1
be a set of hidden variables corre-
sponding to n. Assuming that the hidden variables
are independent and n
i
is also independent of other
nouns given the hidden variables, the probability of
n is defined by
p(n) =
?
z
p(z)p(n|z)
where
p(z) =
N
?
i=1
p(z
i
)
p(n|z) =
N
?
i=1
p(n
i
|z
i
).
Hereafter, we use p(n|z) instead of p(?(n)|z) to
keep the notation simple. p(n|z) is the conditional
distribution on all nouns given all the hidden vari-
ables, and p(z) is the prior distribution on the hid-
den variables. Computing the log-likelihood of the
complete data (n, z), we found
log p(n, z) =
N
?
i=1
log p(z
i
)p(n
i
|z
i
). (1)
2.2 Parameter estimation
The parameters can be estimated by the EM algo-
rithm. In the E-step, p(z
i
|n
i
) is computed based
on current parameters. It is computed by
p(z
i
= k|n
i
) =
p(z
i
= k)p(n
i
|z
i
= k)
?
z
p(z)p(n
i
|z)
=
?
k
?
v
?
f
n
i
v
vk
?
z
?
z
?
v
?
f
n
i
v
vz
.
In the M-step, the parameters are re-estimated by
using the result of the E-step:
?
?k
=
? +
?
i
f
n
i
?
p(z
i
= k|n
i
)
?V +
?
v
?
i
f
n
i
v
p(z
i
= k|n
i
)
?
k
=
? +
?
i
p(z
i
= k|n
i
)
?Z +
?
z
?
i
p(z
i
= z|n
i
)
where ? is a smoothing factor.2 Both steps are
repeated until a convergence criteria is satisfied.
The important point to note is that the E-step can
be computed using the above equation because the
hidden variables are independent.
2
?=1.0 in our experiment.
402
X ya Y X mo Y mo X to Y to X, Y nado
(X or Y) (X and Y) (X and Y) (X, Y etc.)
Table 1: Four lexico-syntactic patterns, where X
and Y are extracted as co-occurring words. Note
that ya, mo, and to are Japanese postpositions, and
they correspond to or or and in English.
3 Pattern-based Clustering
A graph-based algorithm was employed in order to
cluster words using patterns.
3.1 Graph Construction
We first construct the graph in which vertices
and edges correspond to words and their co-
occurrences in patterns respectively (Figure 1). We
employed four lexico-syntactic patterns (Table 1)
to extract co-occurrence of two words from cor-
pus. Note that we target Japanese in this paper al-
though our proposal is independent of languages.
The edges are weighted by the strength of co-
occurrence that is computed by the Point-wise Mu-
tual Information (PMI):
PMI(n
i
, n
j
) = log
f(n
i
, n
j
)f(?, ?)
f(n
i
, ?)f(?, n
j
)
where f(n
i
, n
j
) is the co-occurrence frequency
of two nouns, and ??? means summation over all
nouns. If PMI is less than zero, the edge is re-
moved.
3.2 Graph Partitioning
Assuming that similar words tend to co-occur in
the lexico-syntactic patterns, it is reasonable to
consider that a dense subgraph is a good cluster
(Figure 1). Following (Matsuo et al, 2006), we
exploit the Newman clustering (Newman, 2004) to
partition the graph into such dense subgraphs.
We start by describing Newman?s algorithm for
unweighted graphs and we will generalize it to
weighted graphs later. The Newman clustering is
an algorithm that divides a graph into subgraphs
based on connectivity. Roughly speaking, it di-
vides a graph such that there are a lot of edges be-
tween vertices in the same cluster. In the algorithm
goodness of clustering is measured by score Q:
Q =
?
i
(
e
ii
? a
2
i
)
ramen
dumpling
pasta
steak
Japan U.S.A.
Germany
China
France
Figure 1: An example of the graph consisting of
two dense subgraphs.
where
e
ij
=
# of edges between two vertices in cluster i and j
# of all edges
a
i
=
?
k
e
ik
.
The term e
ij
is the fraction of edges between clus-
ter i and j. a
i
is the sum of e
ik
over all clusters,
and a2
i
represents the expected number of fraction
of edges within the cluster i when edges are given
at random. See (Newman, 2004) for the detail.
The Newman clustering optimizes Q in an ag-
glomerative fashion. At the beginning of the algo-
rithm every vertex forms a singleton cluster, and
we repeatedly merge two clusters so that the join
results in the largest increase in Q. The change in
Q when cluster i and j are merged is given by
?Q = e
ij
+ e
ji
? 2a
i
a
j
= 2(e
ij
? a
i
a
j
).
The above procedure is repeated until Q reaches
local maximum.
The algorithm can be easily generalized to
weighted graphs by substituting ?sum of weights
of edges? for ?# of edges? in the definition of e
ij
.
The other part of the algorithm remains the same.
4 Integration based on Hidden Markov
Random Fields
This section represents how to integrate the distri-
bution and pattern for word clustering.
4.1 Background and idea
Clustering has long been discussed as an unsu-
pervised learning problem. In some applications,
however, it is possible to provide some form of
supervision by hand in order to improve the clus-
tering result. This motivated researchers to inves-
tigate semi-supervised clustering, which uses not
only unlabeled data but supervision in the form of
pair-wise constraints (Basu et al, 2004). In this
403
framework, the clustering system is provided with
a set of pair-wise constraints specifying data points
that are likely to belong to the same cluster. These
constraints are directly incorporated into the clus-
tering process as a prior knowledge.
Our idea is to view the co-occurrence of two
words in lexico-syntactic patterns as constraints,
and incorporate them into the distributional clus-
tering. The rest of this section describes how to ex-
tend the distributional clustering so as to incorpo-
rate the constraints, and how to generate the con-
straints using the patterns.
4.2 Probabilistic model
Let C be a set of pair-wise constraints, and con-
sider to incorporate the constraints into the distri-
butional clustering (Section 2). In what follows we
assume each constraint ?i, j? ? C represents that
z
i
and z
j
are likely to have the same value, and it is
associated with a weight w
ij
(> 0) corresponding
to a penalty for constraint violation.
It is easy to extend the distributional cluster-
ing algorithm so as to incorporate the constraints.
This is done by just changing the prior distribution
on hidden variables p(z). Following (Basu et al,
2004), we construct the Markov random field on
the hidden variables so as to incorporate the con-
straints. The new prior distribution is defined as
p(z) =
N
?
i=1
p(z
i
) ?
1
G
exp{?
?
?i,j??C
?(z
i
6= z
j
)w
ij
}
where ?(?) is the delta function. ?(z
i
6= z
j
) takes
one if the constraint ?i, j? is violated and otherwise
zero. G is the normalization factor of the Markov
random field (the second term).
By examining the log-likelihood of the complete
data, we can see how violation of constraints is pe-
nalized. Using the new prior distribution, we get
log p(n, z) =
N
?
i=1
log p(z
i
)p(n
i
|z
i
)
?
?
?i,j??C
?(z
i
6= z
j
)w
ij
? log G.
The first term in the right-hand side is equal to the
log-likelihood of the multinomial mixture, namely
equation (1). The second term can be interpreted
as the penalty for constraint violation. The last
term is a constant.
It is worth pointing out that the resulting algo-
rithm makes a soft assignment and polysemous
words can belong to more than one clusters.
4.3 Parameter estimation
The parameters are estimated by the EM algo-
rithm. The M-step is exactly the same as discussed
in Section 2.2. The problem is that the hidden vari-
ables are no longer independent and the E-step re-
quires the calculation of
p(z
i
|n) =
?
z
?i
p(z
?i
, z
i
|n)
?
?
z
?i
p(z
?i
, z
i
)p(n|z
?i
, z
i
)
where z
?i
means all hidden variables but z
i
. The
computation of the above equation is intractable
because the summation in it requires O(ZN?1) op-
erations.
Instead of exactly computing p(z
i
|n), we ap-
proximate it by using the mean field approximation
(Lange et al, 2005). In the mean field approxima-
tion, p(z|n) is approximated by a factorized distri-
bution q(z), in which all hidden variables are inde-
pendent:
q(z) =
N
?
i=1
q
i
(z
i
). (2)
Using q(z) instead of p(z|n), computation of the
E-step can be written as follows:
p(z
i
|n) '
?
z
?i
q(z
?i
, z
i
) = q
i
(z
i
). (3)
The parameters of q(z) are determined such that
the KL divergence between q(z) and p(z|n) is
minimized. In other words, the approximate dis-
tribution q(z) is determined by minimizing
?
z
q(z) log
q(z)
p(z|n)
(4)
under the condition that
?
k
q
i
(z
i
= k) = 1
for all i. This optimization problem can be re-
solved by introducing Lagrange multipliers. Be-
cause we cannot get the solution in closed form, an
iterative method is employed. Taking the deriva-
tive of equation (4) with respect to a parameter
q
ik
= q
i
(z
i
= k) and setting it to zero, we get
the following updating formula:
q
(t+1)
ik
? p(n
i
, k) exp{?
?
j?N
i
(1 ? q
(t)
jk
)w
ij
} (5)
404
where N
i
= {j|?i, j? ? C} and q(t)
ik
is the value of
q
ik
at t-th iteration. The derivation of this formula
is found in Appendix.
4.4 Generation of constraints
It is often pointed out that even small amounts of
misspecified constraints significantly decrease the
performance of semi-supervised clustering. This
is because the error of misspecified constraints is
propagated to the entire transitive neighborhoods
of the constrained data (Nelson and Cohen, 2007).
As an example, consider that we have two con-
straints ?i, j? and ?j, k?. If the former is misspeci-
fied one, the error propagate to k through j.
To tackle this problem, we propose a technique
to put an upper bound ? on the size of the transitive
neighborhoods. Our constraint generation process
is as follows. To begin with, we modified the New-
man clustering so that the maximum cluster size
does not exceed ?. This can be done by prohibit-
ing such merge that results in larger cluster than
?. Given the result of the modified Newman clus-
tering, it is straightforward to generate constraints.
Constraints are generated between two nouns in
the same cluster if they co-occur in the lexico-
syntactic patterns at least one time. The penalty
for constraint violation w
ij
was set to PMI(n
i
, n
j
).
This procedure obviously ensures that the size of
the transitive neighborhoods is less than ?.
5 Experiments
5.1 Data sets
We parsed 15 years of news articles by KNP3 so
as to obtain data sets for the distributional and
pattern-based word clustering (Table 2). The num-
ber of distinct nouns in total was 297,719. Note
that, due to the computational efficiency, we re-
moved such nouns that appeared less than 10 times
with verbs and did not appear at all in the patterns.
A test set was created using manually tailored
Japanese thesaurus (Ikehara et al, 1997). We ran-
domly selected 500 unambiguous nouns from 25
categories (20 words for each category).
5.2 Baselines
For comparison we implemented the following
baseline systems.
? The multinomial mixture (Section 2).
? The Newman clustering (Newman, 2004).
3http://nlp.kuee.kyoto-u.ac.jp/nl-resource/
nouns 208,934
verbs 64,954
noun-verb pairs 4,804,715
nouns 245,465
noun-noun pairs 633,302
Table 2: Data sets statistics. The first and second
row shows the number of distinct words (and word
pairs) used for the distributional and pattern-based
word clustering respectively.
? Three K-means algorithms using different
distributional similarity or dissimilarity mea-
sures: cosine, ?-skew divergence (Lee,
1999)4, and Lin?s similarity (Lin, 1998).
? The CBC algorithm (Lin and Pantel, 2002;
Pantel and Lin, 2002).
5.3 Evaluation procedure
All the nouns in the data set were clustered by the
proposed and baseline systems.5 For the mixture
models and K-means, the number of clusters was
set to 1,000. The parameter ? was set to 100.
The result was assessed by precision and recall
using the test data. The precision and recall were
computed by the B-CUBED algorithm as follows
(Bagga and Baldwin, 1998). For each noun n
i
in
the test data, precision
i
and recall
i
are defined as
precision
i
=
|S
i
? T
i
|
|S
i
|
recall
i
=
|S
i
? T
i
|
| T
i
|
where S
i
is the system generated cluster contain-
ing n
i
and T
i
is the goldstandard cluster containing
n
i
. The precision and recall are defined as an av-
erage of precision
i
and recall
i
for all the nouns in
the test data respectively. The result of soft clus-
tering models cannot be directly evaluated by the
precision and recall. In such cases, each noun is
assigned to the cluster that maximizes p(z|n).
5.4 The result and discussion
Table 3 shows the experimental results. The best
results for each statistic are shown in bold. For the
mixture models and K-means, the precision and re-
call are an average of 10 trials.
Table 3 demonstrates the impact of combining
distribution and pattern. Our method outperformed
4
? = 0.99 in our experiment.
5Our implementation is available from
http://www.tkl.iis.u-tokyo.ac.jp/?kaji/clustering.
405
P R F
1
proposed .383 .437 .408
multinomial mixture .360 .374 .367
Newman (2004) .318 .353 .334
cosine .603 .114 .192
?-skew divergence (Lee, 1999) .730 .155 .255
Lin?s similarity (Lin, 1998) .691 .096 .169
CBC (Lin and Pantel, 2002) .981 .060 .114
Table 3: Precision, recall, and F-measure.
all the baseline systems. It was statistically signif-
icantly better than the multinomial mixture (P <
0.01, Mann-Whitney U-test). Note that it is possi-
ble to improve some baseline systems, especially
CBC, by tuning the parameters. For CBC we sim-
ply used the same parameter values as reported in
(Lin and Pantel, 2002).
Compared with the multinomial mixture, one
advantage of our method is that it has broad cov-
erage. Our method can successfully handle un-
known words, which do not appear with verbs at
all (i.e., f
n
= 0 and ?(n) is zero vector), if they
co-occur with other words in the lexico-syntactic
patterns. For unknown words, the hidden variables
are determined based only on p(z) because p(n|z)
takes the same value for all hidden variables. This
means that our method clusters unknown words
using pair-wise constraints. On the other hand,
the multinomial mixture assigns all the unknown
words to the cluster that maximizes p(z).
The test set included 51 unknown words.6 We
split the test set into two parts: f
n
= 0 and f
n
6= 0,
and calculated precision and recall for each subset
(Table 4). Although the improvement is especially
significant for the unknown words, we can clearly
confirm the improvement for both subsets. For the
Newman clustering we can discuss similar things
(Table 5). Different from the Newman clustering,
our method can handle nouns that do not co-occur
with other nouns if 0 < f
n
. In this case the test set
included 64 unknown words.
It is interesting to point out that our framework
can further incorporate lexico-syntactic patterns
for dissimilar words (Lin et al, 2003). Namely,
we can use patterns so as to prevent distribution-
ally similar but semantically different words (e.g.,
ally and supporter (Lin et al, 2003)) from being as-
signed to the same cluster. This can be achieved by
using cannot-link constraints, which specify data
points that are likely to belong to different clus-
6The baseline systems assigned the unknown words to a
default cluster as the multinomial mixture does.
f
n
= 0 f
n
6= 0
P R F
1
P R F
1
proposed .320 .632 .435 .412 .450 .430
multi. .099 1.000 .181 .402 .394 .398
Table 4: Detail comparison with the multinomial
mixture.
f(n
i
, ?) = 0 f(n
i
, ?) 6= 0
P R F
1
P R F
1
proposed .600 .456 .518 .380 .479 .424
Newman .071 1.000 .133 .354 .412 .381
Table 5: Detail comparison with the Newman clus-
tering.
ters (Basu et al, 2004). The remaining problem
is which patterns to use so as to extract dissimilar
words. Although this problem has already been
discussed by (Lin et al, 2003), they mainly ad-
dressed antonyms. We believe that a more exhaus-
tive investigation is required. In addition, it is still
unclear whether dissimilar words are really useful
to improve clustering results.
One problem that we did not examine is how to
determine optimal number of clusters. In the ex-
periment, the number was decided with trial-and-
error through our initial experiment. We leave it
as our future work to test methods of automat-
ically determining the cluster number (Pedersen
and Kulkarni, 2006; Blei and Jordan, 2006).
6 Related work
As far as we know, the distributional and pattern-
based word clustering have been discussed inde-
pendently (e.g., (Pazienza et al, 2006)). One of
the most relevant work is (Bollegala et al, 2007),
which proposed to integrate various patterns in or-
der to measure semantic similarity between words.
Although they extensively discussed the use of pat-
terns, they did not address the distributional ap-
proach.
Mirkin (2006) pointed out the importance of
integrating distributional similarity and lexico-
syntactic patterns, and showed how to combine the
two approaches for textual entailment acquisition.
Although their work inspired our research, we dis-
cussed word clustering, which is related to but dif-
ferent from entailment acquisition.
Lin (2003) also proposed to use both distribu-
tional similarity and lexico-syntactic patterns for
finding synonyms. However, they present an oppo-
site viewpoint from our research. Their proposal
is to exploit patterns in order to filter dissimilar
406
words. As we have already discussed, the integra-
tion of such patterns can also be formalized using
similar probabilistic model to ours.
A variety of studies discussed determining po-
larity of words. Because this problem is ternary
(positive, negative, and neutral) classification of
words, it can be seen as one kind of word clus-
tering. The literature suggested two methods of
determining polarity, and they are analogous to the
distributional and co-occurrence-based approaches
in word clustering (Takamura et al, 2005; Hi-
gashiyama et al, 2008). We consider it is also
promising to integrate them for polarity determi-
nation.
7 Conclusion
The distributional and pattern-based word cluster-
ing have long been discussed separately despite
the potentiality for their integration. In this paper,
we provided a probabilistic framework for com-
bining the two approaches, and demonstrated that
the clustering result is significantly improved.
Our important future work is to extend current
framework so as to incorporate patterns for dissim-
ilar words using cannot-link constraints. We con-
sider such patterns further improve the clustering
result.
Combining distribution and pattern is important
for other NLP problems as well (e.g., entailment
acquisition, polarity determination). Although this
paper examined word clustering, we consider a
part of our idea can be applied to other problems.
Acknowledgement
This work was supported by the Comprehensive
Development of e-Society Foundation Software
program of the Ministry of Education, Culture,
Sports, Science and Technology, Japan.
References
Bagga, Amit and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector space
model. In Proceedings of ACL, pages 79?85.
Baker, L. Douglas and Andrew Kachites McCallum.
1998. Distributional clustering of words for text
classification. In Proceedings of SIGIR, pages 96?
103.
Basu, Sugato, Mikhail Bilenko, and Raymond J.
Mooney. 2004. A probabilistic framework for semi-
supervised clustering. In Proceedings of SIGKDD,
pages 59?68.
Blei, David M. and Michael I. Jordan. 2006. Vari-
ational inference for Dirichlet process mixtures.
Bayesian Analysis, 1(1):121?144.
Bollegala, Danushka, Yutaka Matsuo, and Mitsuru
Ishizuka. 2007. An integrated approach to mea-
suring semantic similarity between words using in-
formation available on the web. In Proceedings of
NAACL, pages 340?347.
Brown, Peter F., Vincent J. Della Pietra, Peter V. deS-
ouza, Jenifer C. Lai, and Rober L. Mercer. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467?479.
Chilovski, Timothy and Patrick Pantel. 2004. VER-
BOCEAN: Mining the web for fine-grained semantic
verb relations. In Proceedings of EMNLP, pages 33?
40.
Church, KennethWard and Patrick Hanks. 1989. Word
association norms, mutual information, and lexicog-
raphy. In Proceedings of ACL, pages 76?83.
Harris, Zellig. 1968. Mathematical Structure of Lan-
guage. New York: Wiley.
Higashiyama, Masahiko, Kentaro Inui, and Yuji Mat-
sumoto. 2008. Learning polarity of nouns by se-
lectional preferences of predicates (in Japanese). In
Proceedings of the Association for NLP, pages 584?
587.
Hindle, Donald. 1990. Noun classification from
predicate-argument structure. In Proceedings of
ACL, pages 268?275.
Ikehara, Satoru, Masahiro Miyazaki, Satoshi Shirai,
Akio Yokoo, Hiromi Nakaiwa, Kentarou Ogura,
and Yoshifumi Oyama Yoshihiko Hayashi, editors.
1997. Japanese Lexicon. Iwanami Publishing.
Kazama, Jun?ichi and Kentaro Torisawa. 2008. Induc-
ing gazetteers for named entity recognition by large-
scale clustering of dependency relations. In Pro-
ceedings of ACL, pages 407?415.
Koo, Terry, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL, pages 595?603.
Lange, Tilman, Martin H.C. Law, Anil K. Jain, and
Joachim M. Buhmann. 2005. Learning with con-
strained and unlabelled data. In Proceedings of
CVPR, pages 731?738.
Lee, Lillian. 1999. Measures of distributional similar-
ity. In Proceedings of ACL, pages 25?32.
Li, Hang and Naoki Abe. 1998. Word clustering and
disambiguation based on co-occurrence. InProceed-
ings of ACL-COLING, pages 749?755.
Li, Wei and Andrew McCallum. 2005. Semi-
supervised sequence modeling with syntactic topic
models. In Proceedings of AAAI, pages 813?818.
407
Lin, Dekang and Patrick Pantel. 2002. Concept discov-
ery from text. In Proceeodings of COLING, pages
577?583.
Lin, Dekang, Shaojun Zhao, Lijuan Qin, and Ming
Zhou. 2003. Identifying synonyms among distri-
butionally similar words. In Proceedings of IJCAI,
pages 1492?1493.
Lin, Dekang. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of ACL-COLING,
pages 768?774.
Matsuo, Yutaka, Takeshi Sakaki, Koki Uchiyama, and
Mitsuru Ishizuka. 2006. Graph-based word cluster-
ing using a web search engine. In Proceedings of
EMNLP, pages 542?550.
McCallum, Andrew and Kamal Nigam. 1998. A com-
parison of event models for naive Bayes text classifi-
cation. In Proceedings of AAAI Workshop on Learn-
ing for Text Categorization, pages 41?48.
Miller, Scott, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In Proceedings of NAACL, pages
579?586.
Mirkin, Shachar, Ido Dagan, andMaayan Geffet. 2006.
Integrating pattern-based and distributional similar-
ity methods for lexical entailment acquisition. In
Proceedings of COLING-ACL Poster Sessions, pages
579?586.
Nelson, Blaine and Ira Cohen. 2007. Revisiting prob-
abilistic models for clustering with pair-wise con-
straints. In Proceedings of ICML, pages 673?680.
Newman, Mark. 2004. Fast algorithm for detecting
community structure in networks. In Phys. Rev. E
69.
Pantel, Patrick and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of SIGKDD,
pages 613?619.
Pazienza, Maria Teresa, Marco Pennacchiotti, and
Fabio Massimo Zanzotto. 2006. Discovering
verb relations in corpora: Distributional versus
non-distributional approaches. In Proceedings of
IEA/AIE, pages 1042?1052.
Pedersen, Ted and Anagha Kulkarni. 2006. Automatic
cluster stopping with criterion functions and the gap
statistic. In Proceedings of HLT/NAACL, Compan-
ion Volume, pages 276?279.
Pereira, Fernando, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
Proceedings of ACL, pages 183?190.
Rooth, Mats, Stefan Riezler, Detlef Prescher, Glenn
Garrroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via EM-based clustering. In
Proceedings of ACL, pages 104?111.
Takamura, Hiroya, Takashi Inui, and Manabu Oku-
mura. 2005. Extracting semantic orientations of
words using spin model. In Proceedings of ACL,
pages 133?140.
Terra, Egidio and C.L.A. Clarke. 2003. Frequency es-
timates for statistical word similarity measures. In
Proceedings of NAACL, pages 165?172.
Torisawa, Kentaro. 2002. An unsupervised learning
method for associative relationships between verb
phrases. In Proceedings of COLING, pages 1009?
1015.
Turney, Peter. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
ECML, pages 491?502.
Weeds, Julie, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of COLING, pages 1015?
1021.
Appendix. Derivation of the updating
formula
We can rewrite equation (4) as follows:
(4) =
?
z
q(z) log q(z) (6)
?
?
z
q(z)
N
?
i=1
log p(n
i
, z
i
) (7)
+
?
z
q(z)
?
?i,j??C
?(z
i
6= z
j
)w
ij
(8)
+ const (9)
where we made use of the fact that log p(z|n) =
log p(n|z)p(z) + const. Taking the derivative of
equation (6), (7), and (8) with respect to q
ik
, we
found
?(6)
?q
ik
= log q
ik
+ const
?(7)
?q
ik
= ? log p(n
i
, k) + const
?(8)
?q
ik
=
?
z
?i
q(z
?i
)
?
j?N
i
?(z
j
6= k)w
ij
+ const
=
?
j?N
i
?
z
?i
q(z
?i
)?(z
j
6= k)w
ij
+ const
=
?
j?N
i
(1 ? q
jk
)w
ij
+ const
where const denotes terms independent of k. Mak-
ing use of these results, the updating formula can
be derived by taking the derivative of equation (4)
with respect to q
ik
and setting it to zero.
408
