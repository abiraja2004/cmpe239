Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 273?280
Manchester, August 2008
Measuring Topic Homogeneity and its  
Application to Dictionary-based Word Sense Disambiguation 
Ann Gledson and John Keane  
School of Computer Science, University of Manchester  
Oxford Road, Manchester, UK M13 9PL  
{ann.gledson,john.keane}@manchester.ac.uk 
 
Abstract 
The use of topical features is abundant in 
Natural Language Processing (NLP), a 
major example being in dictionary-based 
Word Sense Disambiguation (WSD). Yet 
previous research does not attempt to 
measure the level of topic cohesion in 
documents, despite assertions of its ef-
fects. This paper introduces a quantitative 
measure of Topic Homogeneity using a 
range of NLP resources and not requiring 
prior knowledge of correct senses. Eval-
uation is performed firstly by using the 
WordNet::Domains package to create 
word-sets with varying levels of homo-
geneity and comparing our results with 
those expected. Additionally, to evaluate 
each measure?s potential value, the ho-
mogeneity results are correlated against 
those of 3 co-occurrence/dictionary-
based WSD techniques, tested on 1040 
Semcor and SENSEVAL sub-documents. 
Many low-moderate correlations are 
found to exist with several in the mod-
erate range (above .40). These correla-
tions surpass polysemy and sense-
entropy, the 2 most cited factors affecting 
WSD. Finally, a combined homogeneity 
measure achieves correlations of up to 
.52. 
1 Introduction 
Topical features in NLP consist of unordered 
bags of words, often the context of a target word 
or phrase. In WSD for example, the word bank in 
the sentence: ?If you?re OK being tied to one 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
bank, you can get al your financial products 
there.? might be assigned its monetary sense, 
based on the occurrence of the term financial. 
Often referred to as ?topical features?, these are 
an important part of many NLP methods, such as 
WSD (Yarowsky 1995) and Topic Area Detec-
tion (TAD) (Hearst 1997). Furthermore, in the 
SENSEVAL WSD competitions they are in-
cluded in the highest performing systems. 
We assert that the effectiveness of topical fea-
tures in NLP depends upon the level of topic 
homogeneity in the text. To illustrate two ex-
tremes: the disambiguation of the word bank 
might be more difficult if occurring in i) a work 
of fiction describing a series of activities which 
includes the phrases: ?stroll along the river? and 
?pick up her cheque book?; than in ii) a news re-
port on a bank in financial difficulty (a topically 
homogeneous text).   
This paper contributes a set of unsupervised 
Topic Homogeneity measures requiring no 
knowledge of correct senses. A variety of NLP 
resources are utilized and a set of evaluation me-
thods devised, providing useful results. The pa-
per is structured as follows: Section 2 outlines 
related work; Section 3 describes the experi-
ments focusing on the resources used and their 
associated homogeneity measures; in Section 4 
three evaluation methods are described, includ-
ing a WSD task-based evaluation; Conclusions 
and future work are presented in Section 5. 
2 Related Work 
TAD research (Hearst 1997) has revealed that 
word patterns within texts can be used to locate 
topic areas. Salton and Allan (1994) distinguish 
between homogenous texts, where the topic of 
the text might be ascertained from a small num-
ber of paragraphs, and heterogeneous texts, 
where topic areas change considerably. Unfortu-
nately, this research only detects topic homo-
geneity at inter-paragraph level. We assert that 
273
the strength of relationships between the words 
of a text can vary from one text to another and 
that this is likely to affect the usefulness of topic 
features in NLP tasks. Caracciolo et-al (2004) 
also indicate that documents can vary in topical 
structure, mentioning text homogeneity as an 
important feature but they do not evaluate these 
findings explicitly. 
Lexical cohesion (Halliday and Hasan 1976) is 
the analysis of the way the phrases and sentences 
of a text adhere to form a unified, comprehensi-
ble whole. Morris and Hirst (1991) describe a set 
of relationships between word-pairs, which in-
clude their likelihood to co-occur. These are used 
to create Lexical chains, which are sequences of 
related words in a document. The 2 main reasons 
for creating these chains are that they are ?an aid 
in the resolution of ambiguity? and they deter-
mine ?coherence and discourse structure?. We 
propose the use of lexical chains, alongside other 
methods, to measure document homogeneity. 
Measuring the semantic relatedness between 
words is an important area in NLP, and has been 
used in areas such as WSD, Lexical Chaining 
and Malapropism Detection. Budanitsky & Hirst 
(2006) evaluate 5 such measures, based on lexi-
cally assigned similarities, and conclude that an 
area of future research should be the capture of 
?non-classical? relationships not found in dictio-
naries, such as distributional similarity. Weeds 
and Weir (2006) evaluate distributional similari-
ties using document retrieval from the BNC. 
Chen et-al (2006) and Bollegala et-al (2007) use 
web search-engine results. All of the reported 
similarity measures are between two words (or 
texts) only. We extend these to calculate the top-
ic homogeneity of groups containing up to 10 
words.   
A small number of unsupervised WSD me-
thods exist which take topic areas and/or docu-
ment types into account (eg Gliozzo et-al 2004, 
McCarthy et-al 2007), but these work at corpus 
level and do not measure the level of topical ho-
mogeneity in each text. An exception is the WSD 
work by Navigli and Velardi (2005) who report 
differing results for 3 types of text: unfocussed, 
mid-technical (eg finance articles) and overly 
technical (eg computer networks). 
3 Experiments 
We propose the creation of a quantifiable meas-
ure of text homogeneity in order to predict the 
effectiveness of using co-occurrence features in 
WSD. In this initial set of experiments, docu-
ments are divided into simple ~50 content-word 
blocks, regardless of topic boundaries, to maxim-
ize the range of homogeneity levels in the texts 
and to nullify the effects of text length. The long-
term objective is for documents to be divided 
into topic areas at the pre-processing stage, using 
a TAD algorithm. We also propose to take a sin-
gle homogeneity measure of each entire sub-text, 
as opposed to using a sliding window approach, 
as the latter would be over-complex and compa-
rable with a similarity-based WSD process. 
3.1 Input Texts 
The documents of Semcor2 and SENSEVAL (2 
& 3) are used in the experiments. Each text is 
divided into sub-documents of approximately 50 
content-words 3 . All documents are converted 
into Semcor 2.1 format. 
From the resulting sub-documents, 1040 ran-
dom Semcor texts and the entire set of 73 SEN-
SEVAL 2 and 3 texts are used in the experiments. 
3.2 Text Pre-Processing 
Non-topic words are found to have a negative 
effect on NLP methods using topic features, such 
as WSD (eg Yarowsky 1993, Leacock et-al 
1998), and are therefore excluded from our topic 
homogeneity experiments. Unfortunately, no 
precise definition of non-topic words exists. Un-
der ideal conditions, where correct senses are 
known, we define non-topic words as word-
senses appearing in over 25% of all Semcor texts 
and/or being marked as factotum in the WordNet 
Domains 3.2 package (Magnini and Cavaglia 
2000). 
As the experiments described in this work as-
sume no such prior knowledge, an approximation 
of the criteria used above is made. A J48 
(pruned) decision tree is used to decide whether 
each word is non-topical. The input attributes 
were the PoS, sense-count, Semcor distribution 
(SenseEntropy) of its possible senses, whether all 
of the word?s senses are factotum, and the per-
cent of Semcor documents containing that word. 
The training and test data was the entire set of 
Semcor and SENSEVAL 2 and 3 English all-
word task data and the minimum node size was 
set to 4000 instances to minimize the tree size 
and prevent over-training. Using a 10-fold cross-
validation test mode the tree obtains 83% accura-
                                                 
2 Using the Brown-1 and Brown-2 documents only 
3 Splits are made as near as possible to the 50 content-
word length whilst keeping sentences intact. 
274
cy. The learned filter for selecting non-topical 
nouns and verbs is: 
(All-Senses = Factotum) || (Corpus-Hit-Percent >25.0%) || 
((Sense_Count > 1) &&  (PoS = v)) ||   
((Sense_Count > 3) &&  (SenseEntropy > 0.5668))   
Upon entry into the system, each sub-
document has all such words labeled as non-
topical. The remaining words are labeled as top-
ic-content. The confusion matrix output is shown 
in Table 1. 
 
Classified As ? OTHER NON-TOPIC   
OTHER 15017 5768 
NON-TOPIC 4278 34292 
Table 1: J48 Confusion Matrix 
 
In addition, only nouns and verbs are consi-
dered in the experiments as these word types are 
considered most likely to contain topical infor-
mation. 
3.3 Homogeneity Measures 
Five homogeneity measures have been created 
that cover a broad range of techniques for embo-
dying topic-area information in natural language 
texts. This is to facilitate comparisons between 
different techniques and if such a variety of as-
pects is captured, it improves the likelihood of a 
successful combination of the methods to pro-
duce an optimised measure. Each takes a full pre-
processed sub-document as input and outputs a 
single score. 
Word Entropy 
It is possible to capture topic homogeneity by 
using simple measures that require minimal re-
liance on external resources. Word entropy is 
considered as having the potential to reflect topi-
cal cohesion.  
To measure WordEntropy, the frequency of 
each topic-content lemma of an input document d 
is obtained, and Entropy(d) is measured using this 
set of frequencies, as follows: 
 - ?i=1..n  p(xi) log2 p(xi)      [1] 
Where n is the number of different topic content 
lemmas in d, and p(xi) is calculated as 
frequency(lemmai)/?j=1..nfrequency(lemmaj)   [2] 
 
As Entropy(d) is affected by n and n varies from 
one document to another, Entropy(d) is normal-
ised by dividing it by the maximum possible En-
tropy calculation for d, that is if all lemmas had 
equal frequencies. 
WordNet Similarities 
WordNet::Similarities 1.04 (Pedersen et-al 2004) 
is publicly available software which uses aspects 
of WordNet to ?measure the semantic similarity 
and relatedness between a pair of concepts?. The 
package can measure similarities between lemma 
pairs, where no knowledge of the correct sense or 
PoS is required. These similarities can be easily 
adapted to assist with the measurement of docu-
ment homogeneity, by comparing similarities 
between sets of word-PoS pairs in the document.  
Three WordNet Similarities homogeneity 
measures (AvgSimMeasure) are calculated for each 
document as follows: Step 1: Order the topic-
content lemmas of the input text firstly in des-
cending order of frequency, and then by first ap-
pearance in the text. Step 2: Take the first n 
lemmas from this list (where n is all lemmas up 
to a maximum of 10) and add to FreqLemmas. 
Step 3: Calculate the mean of all of the similarity 
measures between each pair of lemmas in Freq-
Lemmas. AvgSim can be defined as: 
Mean(?i=1..n ? j=i+1..n   SimMeasure(lemmai, lemmaj))   [3] 
 
Where SimMeasure(lemmai, lemmaj) is the Word-
Net::Similarity calculation between lemmai and 
lemmaj, where all allowable PoS combinations 
for the two lemmas when using the selected simi-
larity measure are included. 
The WordNet Similarity measures selected for 
use are Lesk, JCN4, and Lch (see Pedersen et-al 
(2004) and Patwardhan et-al (2003) for details), 
as each measure represents one of the 3 main 
algorithm types available: WordNet Gloss over-
laps, information content of the least common 
subsumer and path lengths respectively. 
Yahoo Internet Searches 
The web as a corpus has been successfully used 
for many areas in NLP such as WSD (Mihalcea 
and Moldovan 1999), obtaining frequencies for 
bigrams (Keller and Lapata 2003) and measuring 
word similarity (Bollegala et-al 2007). Such re-
liance on Web search-engine results does come 
with caveats, the most important in this context 
being that reported hit counts are not always reli-
able, mostly due to the counting of duplicate 
documents. (Kilgarriff 2007). 
Using web-searches as part of the homogenei-
ty measure is considered important to our expe-
riments, as it provides up-to-date information on 
word co-occurrence frequencies in the largest 
available collection of English language docu-
                                                 
4 TheBNC information-content file is loaded. 
275
ments. In addition, it is a measure that does not 
rely on WordNet. It is therefore necessary to 
produce a web-based homogeneity measure that 
limits the effects of inaccurate hit counts. 
The SearchYahoo homogeneity measure is 
calculated for each document d as follows: 
Steps 1 and 2: Perform steps 1 and 2 described 
above (WordNet Similarities). Step 3: Using an 
internet search-engine, obtain the hit counts of 
each member of Freqlemmas. Step 4: Order the 
resulting Frequlemmas list of n lemma/hit-counts 
combinations in descending order of hit-counts 
and save this list to IndivHitsDesc. Step 5: For 
each lemma of IndivHitsDesc, save to Combi-
HitsDesc preserving the ordering. Step 6: For 
each member of CombiHitsDesc: CombiHits-
Desci, obtain the hit counts of the associated 
lemma, along with the concatenated lemmas of 
all preceding list members of CombiHitsDesc 
(CombiHitsDesc0 to CombiHitsDesc[i-1]). This 
list of lemmas are concatenated together using ? 
AND ? as the delimiter. Step 7: Calculate the 
gradients of the best-fit lines for the hit-counts of 
IndivHitsDesc and CombiHitsDesc: creating 
gradIndiv and gradCombi respectively. Step 8: 
SearchYahoo is calculated for d as gradIndiv 
minus gradCombi.  
As SearchYahoo is taken as the difference be-
tween the two descending gradients, the measure 
is more likely to reveal the effects of the proba-
bility of the set of lemmas co-occurring in the 
same documents, rather than by influences such 
as duplicate documents. If the decline in hit-
counts from IndivHitsDesc[i-1] to IndivHitsDesc[i] 
is high, then the decline in the number of hits 
from CombiHitsDesc[i-1] to CombiHitsDesc[i] is 
also expected to be higher, and the converse for 
lower drops is also expected. Deviations from 
these expectations are reflected in the final ho-
mogeneity measure and are assumed to be caused 
by the likelihood of lemmas co-occurring togeth-
er in internet texts. 
A web-service enabled search-engine was re-
quired to create a fully automated process. The 
Google search-engine hit-counts were less suita-
ble, as they did not always decline as the number 
of query terms increased. This is perhaps because 
of the way in which Google combines the results 
of several search-engine hubs. The Yahoo web-
services were therefore selected, as these pro-
duced the necessary declines for the measure to 
work. 
Further evaluation of the Yahoo Internet 
searching homogeneity measure is presented in 
Gledson and Keane (2008), along with compari-
sons with similar methods using the Google and 
Windows LiveSearch web-services. 
WordNet Domains  
Magnini et-al (2002) describe the WordNet Do-
mains5 (Magnini and Cavaglia 2000) package as: 
?an extension of WordNet in which synsets 
have been annotated with one or more do-
main labels, selected from a hierarchically 
organized set of about two hundred labels?. 
 (Magnini et-al 2002 p.361) 
They describe a domain (eg ?Politics?) as ?a set 
of words between which there are strong seman-
tic relations?. This resource is useful for measur-
ing topic homogeneity, as it stores topic area in-
formation for word-senses directly, and comple-
ments the other measures, thus contributing to a 
diverse set of measures. 
Two WordNet Domains homogeneity meas-
ures are calculated: DomEntropy and Dom-
Top3Percent. These are calculated for each input 
document d as follows: 
Step 1: Add each topic-content lemma of d to 
the list TopicContents. Step 2: for each WordNet 
sense of each topic-content lemma in TopicCon-
tents, find all associated domains using the Do-
mains package, and add these to a Domain-
Counts list. This list contains each distinct do-
main dom present in the document, each with its 
associated count of the number of times it occurs 
in TopicContents: freq(domi). Step 3: Calculate 
DomEntropy using the equation [1] above, where 
n is the number of items in DomainCounts, and 
p(xi) = freq(domi) / ?j=1..n freq(domj)  [4] 
 
Step 4: Calculate DomTop3Percent as follows: 
100 (?i=1..3 freq(domi) / ?j=1..n freq(domj)) [5] 
Lexical Chaining 
?Lexical chains are defined as clusters of seman-
tically related words? (Doran et-al 2004). These 
words are usually related by electronic dictiona-
ries such as WordNet or Roget?s Thesaurus, and 
chains are created using a natural language text 
as input.  
The lexical chaining method used in our expe-
riments is a greedy version of the algorithm de-
scribed in Ecran and Cicekli (2007). Their me-
thod uses the WordNet dictionary and calculates 
all possible chains in the text. We adopt a greedy 
approach to chaining, as it is only necessary to 
get an overall estimate of the levels of topic ho-
mogeneity within the text, rather than producing 
                                                 
5 We use version 3.2 released Feb 2007 
276
lists of keywords or document summaries. The 
LexChain homogeneity measure is calculated for 
the input document d as follows:   
Step 1: Add each topic-content noun occur-
rence in d to the list UnusedNouns. Step 2: For 
each item in UnusedNouns, find all other items in 
UnusedNouns that it is related to and add them to 
its corresponding RelatedNouns list. Each item of 
RelatedNouns is mapped to a score (relScore) 
using the following system (Ercan and Cicekli 
2007): 10 points are awarded if the word-senses 
have identical lemmas or belong to the same 
WordNet 2.1 synset. 7 points are awarded if it is 
a hyponymy relationship and 4 points are 
awarded if it is a holonymy relationship. Step 3: 
Create chains: Iterate through UnusedNouns re-
cursively, adding all related senses to the first 
chain, until no further linked nouns can be found. 
As each new node (UnusedNouns item) is added 
to the chain, remove it from UnusedNouns. Con-
tinue creating further chains, until no more re-
lated nouns can be found. Step 4: Calculate the 
chainScore of each chain by adding together all 
of the relScores contained for each sense, at each 
node. Step 5: Set LexChain as the ChainScore of 
the highest scoring chain. 
3.4 Adjusting for Polysemy and Skew 
Each of the homogeneity measures (except Wor-
dEntropy) has the potential to be affected by the 
average polysemy and sense skews of the docu-
ment. The effects are measured statistically using 
linear regression and the resulting line of best fit 
equation is used to reverse them.  
To calculate the adjustments for each measure, 
the effects of polysemy and skew must be ap-
proximated. This is achieved by applying linear 
regression6 over the entire result set. The homo-
geneity measure is entered as the dependant vari-
able and the appropriate7 average polysemy and 
skew measures (per doc) are input as indepen-
dent variables. If the homogeneity measure is 
affected by either (or both) of the independent 
variables and the effect is statistically significant, 
a line of best fit equation is output representing 
the gradient of the effect caused by those varia-
ble(s). The appropriate homogeneity measure for 
each input document is adjusted by subtracting 
the co-efficient of the gradient multiplied by the 
appropriate variable(s): polysemy and/or skew. 
                                                 
6 Using SPSS 13.0 statistical software package. 
7 Avg. document polysemy/skews are only calculated 
for lemmas incl. in homogeneity measures. 
4 Evaluation and Discussion 
It is anticipated that the main users of a set of 
topic homogeneity measures are other NLP tech-
niques. They are, therefore, best measured in 
terms of the actual results of the processes they 
are intended to improve. Human judgments can 
be subjective (Doran et-al 2004) and are there-
fore deemed inappropriate for the evaluation of 
this task.  
Three methods are used to evaluate the homo-
geneity measures. Firstly, each measure is com-
pared with its equivalent where only correct 
senses are used. Secondly, the Word-
Net::Domains (version 3.2) hierarchy (Magnini 
and Cavaglia 2000) is used to generate sets of 
words with varying levels of topic homogeneity. 
Each set is then tested using the proposed meas-
ures, and the results compared with those ex-
pected. Finally, the usefulness of each measure is 
tested by evaluating their ability to indicate the 
likely outcome of several co-
occurrence/dictionary-based WSD measures. In 
the WSD literature, the main non-topic related 
variables reported as affecting WSD results are 
polysemy and skew, so these two measures will 
be used as the baselines.  
4.1 All Senses vs. Correct Senses  
Table 2 show the set of measures correlated 
against their correct-sense complements, where 
only correct senses are utilized. Most of the re-
sults are in the moderate range (.40-.60) with 
AvgSimLESK and LexChains achieving correla-
tions in the high and very-high ranges. Only the 
SENSEVAL DomEntropy result falls below the 
moderate level, indicating that homogeneity in-
formation is, in general, not negated by incorrect-
sense noise. 
Measure Correlation 
(Pearson?s R) 
Semcor SENSEVAL 
AvgSimLESK 0.814** 0.834** 
AvgSimJCN 0.491** 0.610** 
AvgSimLCH 0.569** 0.474** 
DomEntropy 0.575** 0.371** 
DomTop3% 0.535** 0.444** 
LexChain 0.659** 0.967** 
**Significant at the .01 level (2-tailed) 
Table 2: Correlation with correct-sense equiva-
lents. 
4.2 WordNet::Domains 
The WordNet Domains package (Magnini and 
Cavaglia 2000) assigns domains to each sense of 
277
the WordNet electronic dictionary. Therefore, for 
each domain a relevant list of words can be ex-
tracted. The domains are arranged hierarchically, 
allowing sets of words with a varied degree of 
topic homogeneity to be selected. For example, 
for a highly heterogeneous set, 10 words can be 
selected from any domain, including factotum 
(level-0: the non-topic related category). For a 
slightly less heterogeneous set, words might be 
selected randomly from a level-1 category (eg 
?Applied_Science?), and any of the categories it 
subsumes (eg Agriculture, Architecture, Build-
ings etc). The levels range from level-0 (facto-
tum) to level-4; we merge levels 3 and 4 as level-
4 domains are relatively few and are viewed as 
similar to level-3. This combined set is hence-
forth known as level-3.  
For our experiments, we have collected 2 ran-
dom samples of 10 words for every WordNet 
domain (167 domains) and then increased the 
number of sets from level-0 to level-2 domains, 
to make the number of sets from each level more 
similar. The final level counts are: levels 0 to 2 
have 100 word-sets each and level 3 has 192 
word-sets. The sets contain 10 words each.  We 
then assign an expected score to each set, equal 
to its domain level. 
**Significant at the .01 level (2-tailed) 
*Significant at the .05 level (2-tailed) 
Table 3: Correlation with expected scores for 
WordNet::Domains selected sets 
 
 The first column of results on Table 3 repre-
sents the correlations with expected results for all 
492 word-sets. The high WordNet::Domains re-
sults (DomEntropy and DomTop3%) probably 
reflect the fact that they are produced using the 
same resource as the creation of the test sets. On 
the other hand, knowledge of correct senses is 
not required for the homogeneity measures and 
these scores indicate that they are capable none-
theless of capturing topic homogeneity. The 
SearchYahoo, AvgSimsJCN and LexChain me-
thods all produce promising results with correla-
tions in the moderate range (0.40 to 0.59) and 
again indicate that they can capture topic homo-
geneity. 
To indicate whether the measures are more 
capable of distinguishing between extreme levels 
of homogeneity, we repeated the above tests, but 
included only those sets of level-0 and level-3. 
The results displayed in the final column of Ta-
ble 3 and provide evidence that this might be the 
case for the WordNet::Domains measures and 
SearchYahoo, as the correlations are significantly 
higher for these more extreme test sets. 
4.3 Dictionary-based WSD 
The three WSD algorithms selected for evalua-
tion are the technique described in Mihalcea and 
Moldovan (2000) and two WordNet Similarities 
measures: Lesk and JCN, adapted for WSD as 
described in Patwardhan et-al (2004), in which 
they are found to achieve the best performances. 
Each WSD technique uses the entire document as 
the context for each target word. The method of 
Mihalcea and Moldovan (2000) is included as it 
incorporates several techniques, all complemen-
tary to our overall set of evaluation methods. For 
our experiments it is split into three: Mih-ALL: 
covering all 8 procedures, including one that re-
lies on co-location information; Mih-4: utilising 
?procedure-4?, which involves the use of noun 
co-occurrence and WordNet hyponymy data; and 
Mih-5-8: using procedures 5 to 8, which involves 
synonymy and hyponymy. The results are ad-
justed to remove the effects of polysemy and 
SenseEntropy (section 3.4). 
In Table 4, the fine-grained WSD accuracy re-
sults (for topic-content words) are compared to 
those of the homogeneity measures, (including 
the correct-sense measures which set the high-
standard benchmark). As a baseline, non-
adjusted WSD accuracies are compared with the 
average polysemy and average Sense Entropy of 
each document. 
All of the ?all-senses? results, except Domai-
nEntropy, are statistically significant and achieve 
at least low-moderate correlations with one or 
more of the WSD measures. All of the measures 
outperform the baseline correlations for most of 
the WSD algorithms displayed.  
A COMBINED measure is calculated for the 
all-sense and the correct-sense sets of measures 
respectively.  The measures included (based on 
their individual performances and maintaining 
maximum diversity) are AvgSimsJCN, WordEn-
tropy, DomTop3Percent, LexChain and Sear-
chYahoo. Each of these result sets are ordered by 
homogeneity score (most homogeneous  
Measure Correlation 
(Pearson?s R) 
All Extreme 
SearchYahoo 0.46** 0.80** 
AvgSimLESK 0.23** 0.15* 
AvgSimJCN 0.47** 0.45** 
AvgSimLCH 0.35** 0.25** 
DomEntropy 0.70** 0.75** 
DomTop3% 0.75** 0.83** 
LexChain 0.42** 0.40** 
278
first) and banded into 5 groups making 4 cut-
points at equal percentiles and numbering them 
from 5 down to 1 respectively. The combined 
measure for each document is the sum of all such 
scores.  These measures often outperform all of 
the individual methods and achieve correlations 
of up to .52 in Semcor, the largest of the datasets. 
5 Conclusions and Further Work 
This paper presents a first attempt to measure 
Topic Homogeneity using a variety of NLP re-
sources. A set of 5 unsupervised homogeneity 
measures are presented that require no prior 
knowledge of correct senses and which exhibit 
moderate to high degrees of correlation with their 
correct-sense-only equivalents. When used to 
measure word-sets created using the Word-
Net::Domains package and which have varying 
levels of homogeneity, they are found to corre-
late well with expected results, further supporting 
our conjecture that they represent topical homo-
geneity information. Finally, when compared 
with WSD topic-content word accuracies, the 
effect of topic homogeneity is shown to surpass 
that of polysemy and sense-entropy, which have 
been recognized previously as having an influ-
ence on such results. By combining these meas-
ures, correlations are improved further, often 
outperforming the individual methods and 
achieving up to .52 for over 1000 random Sem-
cor sub-documents, again indicating their poten-
tial importance. Correlations in SENSEVAL are 
often higher, but due to the lower number of 
documents, it is more difficult to obtain statisti-
cally significant results.  
Our results provide evidence that improve-
ments could be made to WSD and other NLP 
methods which utilise topic features, by adapting 
the algorithms used depending on the level of 
topic cohesion of the input text. For example, 
window-sizes for obtaining contextual data might 
be expanded or reduced, based on the homogene-
ity level of the target text. Furthermore, non-
topical features such as collocation and grammat-
ical cues might be given more emphasis when 
disambiguating heterogeneous documents. 
Further work includes testing the measures on 
other NLP tasks. A machine learning approach 
might also be used to further optimize the com-
bination of homogeneity measures. Finally, it is 
intended that our approach should eventually be 
combined with a TAD method to improve WSD 
results. 
References  
Bollegala, Danushka, Yutaka Matuo and Mitsuru 
Ishizuka, 2007. Measuring Semantic Similarity be-
tween Words Using Web Search Engines. In Procs  
World Wide Web Conference, Banff, Alberta. 
Caracciolo, Caterina, Willem van Hage and Maarten 
de Rijke, 2004. Towards Topic Driven Access to 
Full Text Documents, in Research and Advanced 
 SENSEVAL 2 & 3 SEMCOR 
Mih 
ALL 
Mih  
4 
Mih 
5-8 
LESK JCN Mih 
ALL 
Mih  
4 
Mih 
5-8 
LESK JCN 
ALL Senses 
Word Entropy    -.325 -.301 -.419 -.442 -.206  -.286 
AvgSimLESK    .500  .138 .132 .211 .121 .097 
AvgSimJCN .286 .276 .357 .233  .255 .231 .284  .081 
AvgSimLCH .271 .211 .224 .202  .254 .248 .285  .104 
DomEntropy -.193 -.205    -.163 -.157   -.091 
DomTop3% .308 .281    .224 .215 .145  .108 
LexChain    .344  .328 .345 .296  .095 
SearchYahoo -.232 -.290 -.317 -.295  -.105 -.116  -.085 -.089 
COMBINED .382 .421 .270 .243 .171 .521 .517 .256  .350 
CORRECT 
Senses 
AvgSimLESK .461 .467  .630 .189 .186 .174 .237 .171 .115 
AvgSimJCN .257  .272 .357 .325 .218 .162 .234  .210 
AvgSimLCH .310  .483 .187  .198 .181 .247  .122 
DomEntropy -.340 -.295    -.334 -.335 -.106  -.270 
DomTop3% .376 .353   .214 .398 .393 .144  .316 
LexChain    .372 .297 .426 .440 .164 .112 .364 
COMBINED .398 .363  .398 .379 .519 .494 .294  .434 
Baseline AvgPolysemy -.100 -.030 -.234 (.252) (.113) (.146) (.169) -.004 .032 -.144 AvgSenseEntropy .031 -.044 .143 -.307 -.019 -.073 -.064 -.083 .041 -.141 
Results in bold: significant at the 0.01 level (2-tailed); Results in non-bold: significant at the 0.05 level (2-tailed) 
Italicised results: not significant at the 0.05 level but considered to be of interest  
Bracketed results: inverse to expectations 
All WSD results are adjusted for polysemy / SenseEntropy, with the exception of where compared with baselines. 
Table 4: Topic-content word WSD accuracy vs. Homogeneity: Correlations 
279
Technology for Digital Libraries, LNCS, 3232, pp 
495-500 
Chen, Hsin-Hsi, Ming-Shun Lin and Yu-Chuan Wei, 
2006. Novel Association Measures Using Web 
Search with Double Checking. In Proc.s 21st Intl. 
Conference on Computational Linguistics and 44th 
Annual Meeting of the ACL, pp. 1009-1016 
Doran, William, Nicola Stokes, Joe Carthy and John 
Dunnion, 2004. Assessing the Impact of Lexical 
Chain Scoring Methods and Sentence Extraction 
Schemes on Summarization, LNCS 2945, pp 627-
635, CICLing 2004 
Ercan, Gonenc and Ilyas Cicekli, 2007. Using Lexical 
Chains for Keyword extraction, Information 
Processing and Management, 43(2007), pp 1705-
1714. 
Gledson, Ann and John Keane, 2008. Using web-
search results to measure word-group similarity. In 
Procs 22nd Intl Conference on Computational Lin-
guistics (COLING), Manchester.  (To Appear) 
Gliozzo, Alfio, Carlo Strapparava and Ido Dagan, 
2004. Unsupervised and Supervised Exploitation of 
Semantic Domains in Lexical Disambiguation, 
Computer Speech and Language  
Halliday, Michael and Ruqaiya Hasan, 1976. Cohe-
sion in English, Longman Group 
Hearst , Marti, 1997. Text Tiling: segmenting text into 
multi-paragraph subtopic passages, Computational 
Linguistics, 23(1), pp 33-64 
Keller, Frank and Mirella Lapata, 2003. Using the 
Web to Obtain Frequencies for Unseen Bigrams, 
Computational Linguistics, 29(3) 
Kilgarriff, Adam, 2007. Googleology is bad science, 
Computational Linguistics, 33(1), pp 147-151 
Leacock, Claudia, Martin Choderow and George A. 
Miller, 1998. Using Corpus Statistics and Wordnet  
Relations for Sense Identification, Computational 
Linguistics, 24(1), pp 147-165  
McCarthy, Diana, Rob Koeling, Julie Weeds and John 
Carroll, 2007. Unsupervised Acquisition of Pre-
dominant Word Senses, Computational Linguistics, 
33(4), pp. 553-590. 
Magnini, Bernardo and Gabriela Cavagli?, 2000. 
Integrating Subject Field Codes into WordNet. 
In Procs LREC-2000, Athens, Greece, 2000, pp 
1413-1418. 
Magnini, Bernardo, Carlo Strapparava, Giovanni Pez-
zulo and Alfio Gliozzo, 2002. The Role of Domain 
Information in Word Sense Disambiguation. Natu-
ral Language Engineering, 8(4), pp 359-373 
Mihalcea, Rada and Dan Moldovan, 1999. A method 
for word sense disambiguation of unrestricted txt. 
In Procs. 37th Meeting of ACL, pp 152-158 
Mihalcea, Rada and Dan Moldovan, 2000. An Itera-
tive Approach to Word Sense Disambiguation, In 
Proc. FLAIRS 2000, pp. 219-223, Orlando 
Morris, Jane and Graeme Hirst, 1991. Lexical Cohe-
sion Computed by Thesaural Relations as an Indi-
cator of the Structure of Text, Computational Lin-
guistics, 17(1), pp. 21-48.  
Navigli, Roberto, Paola Velardi, 2005. Structural Se-
mantic Interconnections: A Knowledge-Based Ap-
proach to Word Sense Disambiguation, IEEE 
Transactions on Pattern Analysis and Machine In-
telligence, 27(7),  pp. 1075-1086,  July. 
Patwardhan, Siddharth, Satanjeev Banerjee and Ted 
Pedersen, 2003. Using Measures of Semantic Rela-
tedness for Word Sense Disambiguation, LNCS 
2588, pp 241-257, CICLing 2003 
Pedersen, Ted, Siddharth Patwardhan and Jason Mi-
chelizzi, 2004. WordNet::Similarity ? Measuring 
the Relatedness of Concepts, In Procs 19th Nation-
al Conference on Artificial Intelligence. 
Salton, Gerard and James Allan, 1994. Automatic text 
decomposition and structuring, In Procs. RIAO In-
ternational Conference, New York. 
Weeds, Julie and, David Weir, 2006, Co-occurrence 
Retreival: A Flexible Framework for Lexical Dis-
tributional Similarity. Computational Linguistics, 
31(4), pp 433-475. 
Yarowsky, David, (1995), Unsupervised word sense 
disambiguation rivaling supervised methods. In 
Procs 33rd Annual Meeting of the Association for 
Computational Linguistics (ACL) 1995, pp 189-
196 
 
280
