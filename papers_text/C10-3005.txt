Coling 2010: Demonstration Volume, pages 17?20,
Beijing, August 2010
Have2eat: a Restaurant Finder with Review Summarization
for Mobile Phones
Giuseppe Di Fabbrizio and Narendra Gupta
AT&T Labs - Research, Inc.
{pino,ngupta}@research.att.com {sbesana,pmani}@attinteractive.com
Sveva Besana and Premkumar Mani
AT&T Interactive - Applied Research
Abstract
Have2eat is a popular mobile application
available for iPhone and Android-based de-
vices that helps users to find and assess
nearby restaurants. It lists restaurants lo-
cated around the device and provides a quick
highlight about the opinions expressed by
online reviewers. Have2eat summarizes tex-
tual reviews by extracting relevant sentences
and by automatically generating detailed rat-
ings about specific aspects of the restaurant.
A compact one-screen digest allows users to
quickly access the information they need, ex-
pand to full review pages, and report their ex-
perience online by entering ratings and com-
ments.
1 Introduction
Bloggers, professional reviewers, and consumers
continuously create opinion-rich web reviews about
products and services, with the result that textual re-
views are now abundant on the web and often con-
vey a useful overall rating. However, an overall rat-
ing cannot express the multiple or conflicting opin-
ions that might be contained in the text and screen-
ing the content of a large number of reviews could
be a daunting task. For example, a restaurant might
receive a great evaluation overall, while the service
might be rated below-average due to slow and dis-
courteous wait staff. Pinpointing opinions in doc-
uments, and the entities being referenced, would
provide a finer-grained sentiment analysis and bet-
ter summarize users? opinions. In addition, select-
ing salient sentences from the reviews to textually
summarize opinions would add useful details to con-
sumers that are not expressed by numeric ratings.
This is especially true for so-called road warriors and
mobile users ?on the run? who are often dealing with
limited time and display real estate in searching for a
restaurant to make a decision.
Have2eat1 is a popular2 mobile application avail-
able for iPhone and Android-based devices that ad-
dresses these challenges. Have2eat uses the geo-
location information either from the GPS device or
explicitly entered by the user to produce a list of
restaurants sorted by distance and located within a
specific radius from the originating location. In addi-
tion, when restaurant reviews are available, a compact
one-screen digest displays a summary of the reviews
posted on the web by other customers. Customers
can expand to read a full review page and also enter
their own ratings, comments and feedback. The re-
view summaries are visualized on the mobile screen:
? graphically by thumbs-up (positive reviews)
and thumbs-down (negative reviews) for differ-
ent aspects of the restaurant;
? textually by a few sentences selected from re-
view texts that best summarize the opinions
about various aspects of the restaurant expressed
in the reviews;
Extracting opinions from text presents many nat-
ural language processing challenges. Prior work on
sentiment analysis has been focusing on binary clas-
sification of positive and negative opinions (Turney,
2002; Pang et al, 2002; Yu and Hatzivassiloglou,
2003), while aspect rating inference (e.g., the task
of determining the opinion polarity in a multi-point
scale) has been previously analyzed in Pang and
Lee (2005); Goldberg and Zhu (2006); Leung et al
(2006). More recently, Snyder and Barzilay (2007);
Shimada and Endo (2008) extended the inference
process to multi-aspect ratings where reviews include
numerical ratings from mutually dependent aspects.
Snyder and Barzilay (2007) shows that modeling the
dependencies between aspect ratings in the same re-
views helps to reduce the rank-loss (Crammer and
Singer, 2001).
1www.have2eat.com
2More than 400,000 downloads to-date for the iPhone
version alone
17
There are similar mobile applications obtainable
either on the Apple iPhone App Store or as web-
based mobile application, such as Zagat3, UrbanS-
poon4, YP Mobile5, and Yelp6, but, to the extent of
our knowledge, most of them are only focused on
finding the restaurant location based on proximity
and some restaurant filtering criterion. When avail-
able, restaurant reviews are simply visualized as con-
tiguous list of text snippets with the overall experi-
ence rating. None of the listed applications include
extended rating predictions and reviews summariza-
tion.
2 System Description
The have2eat system architecture is composed of two
parts: 1) predictive model training ? illustrated in Fig-
ure 1 and described in section 2.1, and 2) graphical
and textual summarization ? shown in Figure 2 and
described in section 2.2.
2.1 Graphical summarization by thumbs
up/down
The majority of textual reviews available online are
accompanied by a single overall rating of the restau-
rant. To predict consistent ratings for different as-
pects, namely food, service, atmosphere, value, and
overall experience, we use machine learning tech-
niques to train predictive models, one for each as-
pect; see Figure 1. More specifically, we used ap-
proximately 6,000 restaurant reviews scraped from a
restaurant review website7. On this website, besides
textual reviews, users have also provided numerical
ratings for the five aspects mentioned above. Ratings
are given on a scale of 1 to 5, 1 being poor and 5
excellent. We experimented with different regression
and classification models using a host of syntactic and
semantic features. We evaluated these models using
rank-loss metrics which measure the average differ-
ence between predicted and actual ratings. We found
that a maximum entropy (Nigam et al, 1999) model
combined with a re-ranking method that keeps in con-
sideration the interdependence among aspect ratings,
provided the best predictive model with an average
rank-loss of 0.617 (Gupta et al, 2010). This results
is better than previous work on the same task as de-
scribed in Snyder and Barzilay (2007).
To cope with the limited real estate on mobile
phones for displaying and allowing users to input
their opinions, the predicted ratings were mapped
onto thumbs?up and thumbs?down. For each restau-
3mobile.zagat.com
4www.urbanspoon.com
5m.yp.com
6m.yelp.com
7www.we8there.com
rant the proportion of reviews with rating of 1 and 2
was considered thumbs down and ratings of 4 and 5
were mapped to thumbs up. Table 1 shows an exam-
ple of this mapping.
Reviews Thumbs
a b c Up Down
Atmosphere 3 2 4 50% 50%
Food 4 4 5 100% 0
Value 3 2 4 50% 50%
Service 5 5 5 100% 0
Overall 4 4 5 100% 0
Table 1: Mapping example between ratings and
thumbs up/down. Ratings of 3 are considered neutral
and ignored in this mapping
2.2 Textual summaries by sentence selection
Figure 2 shows how summary sentences are selected
from textual reviews. As described in the previous
section, we trained predictive models for each aspect
of the restaurant. To select summary sentences we
split the review text into sentences8. Using the pre-
dictive models and iterating over the restaurant list-
ings, sentences in the reviews are classified by aspect
ratings and confidence score. As a result, for each
sentence we get 5 ratings and confidence scores for
those ratings. We then select a few sentences that
have extreme ratings and high confidence and present
them as summary text.
We evaluated these summaries using the following
metrics.
1. Aspect Accuracy: How well selected sentences
represent the aspect they are supposed to.
2. Coverage: How many of the aspects present in
the textual reviews are represented in the se-
lected sentences.
8For this purpose we used a sentence splitter based on
statistical models which besides n-grams also uses word
part-of-speech as features. This sentence splitter was
trained on email data and is 97% accurate.
Figure 1: Predictive model training
18
Figure 2: Graphical and textual summarization
3. Rating Consistency: How consistent the se-
lected sentences with the summarizing aspect
ratings are.
4. Summary quality: Subjective human judgments
as to how good the summaries are and automatic
multi-document summarization to how good the
summaries are compared to a manually created
GOLD standard using ROUGE-based (Lin, 2004)
metrics.
A detailed description of the summarization task
evaluation will be published elsewhere.
3 Demonstration
When launching the application, users are presented
with a list of twenty nearby restaurants. The user can
browse more restaurants by tapping on a link at the
bottom of the page. For each listing we show the dis-
tance from the current location and, if available, we
provide a thumbs-up or thumbs-down, price informa-
tion and the summary sentence with the highest confi-
dence score across aspects. Figure 3 shows an exam-
ple of the List page. If users want a list of restaurants
for a different location they can tap the Change but-
ton at the top of the page. This action will bring up
the Location page where the user can enter city and
state and/or a street address.
Users can select a restaurant in the list to view the
details, see Figure 4. Details include address, phone
number and thumbs up/down for the overall, food,
service, value and atmosphere aspects. The user can
provide feedback by tapping on the thumbs-up or
thumbs-down buttons, as well as by leaving a com-
ment at the bottom of the screen. This page also in-
cludes a few summary sentences with extreme ratings
and high confidence scores. An example of selected
sentences with their polarity is shown in Table 2. By
tapping on any of the sentences the users can view
the full text of the review from which the sentence
was selected. Users can also add a new restaurant by
tapping the Add icon in the tab bar.
Figure 3: Have2eat listings screen shot on iPhone
Figure 5 displays the review selected in the Details
page along with any other reviews which exist for the
restaurant. Users can give feedback on whether they
found the review helpful or not by using a thumbs-up
or a thumbs-down respectively. Users can also add a
review by tapping on a link at the bottom of the page.
4 Conclusion
This demonstration has shown a restaurant finder ap-
plication for mobile phones, which makes use of
summarization techniques to predict aspect ratings
from review text and select salient phrases express-
ing users? opinions about specific restaurant aspects.
Users can directly contribute with their feedback by
tapping on the aspect thumbs buttons or by directly
typing comments.
19
Figure 4: Have2eat automatically predicted aspect
ratings and summary
Restaurant 1 (3 reviews)
+ The soups are GREAT! Everything that we have ever ordered has
exceeded the ex...
+ Delivery is prompt and credit cards are welcome
+ Their chicken fried rice is the second best in Southern California.
Restaurant 2 (8 reviews)
+ Great tasting burgers, friendly fast service!
+ The inside is warm and even though the chairs looked uncomfort-
able, they were not at all.
- Too many other places to try to worry about getting mediocre food
as a high price.
Restaurant 3 (4 reviews)
+ The salads are tasty, the breadsticks are to die for.
- We waited approximate 10 more minutes and then asked how
much longer.
+ A fun place to go with faimily or a date.
+ If you like salt then this is the place to go, almost everything is full
of s...
Table 2: Example of extracted summaries
Acknowledgments
We thank Jay Lieske, Kirk Boydston, Amy Li, Gwen
Christian, and Remi Zajac for their contributions and
great enthusiasm.
References
Crammer, Koby and Yoram Singer. 2001. Pranking with
ranking. In Thomas G. Dietterich, Suzanna Becker, and
Zoubin Ghahramani, editors, Neural Information Pro-
cessing Systems: Natural and Synthetic (NIPS). MIT
Press, Vancouver, British Columbia, Canada, pages
641?647.
Goldberg, Andrew B. and Jerry Zhu. 2006. Seeing
stars when there aren?t many stars: Graph-based semi-
supervised learning for sentiment categorization. In
TextGraphs: HLT/NAACL Workshop on Graph-based
Algorithms for Natural Language Processing.
Gupta, Narendra, Giuseppe Di Fabbrizio, and Patrick
Haffner. 2010. Capturing the stars: Predicting ratings
for service and product reviews. In Proceedings of the
HLT-NAACL Workshop on Semantic Search (Semantic-
Search 2010). Los Angeles, CA, USA.
Figure 5: Have2eat reviews
Leung, Cane Wing-ki, Stephen Chi-fai Chan, and Fu-lai
Chung. 2006. Integrating collaborative filtering and sen-
timent analysis: A rating inference approach. In Pro-
ceedings of The ECAI 2006 Workshop on Recommender
Systems. Riva del Garda, I, pages 62?66.
Lin, Chin-Yew. 2004. ROUGE: A package for automatic
evaluation of summaries. In Proc. ACL workshop on
Text Summarization Branches Out. page 10.
Nigam, Kamal, John Lafferty, and Andrew Mccallum.
1999. Using maximum entropy for text classification.
In IJCAI-99 Workshop on Machine Learning for Infor-
mation Filtering. pages 61?67.
Pang, Bo and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with re-
spect to rating scales. In Proceedings of the Association
for Computational Linguistics (ACL). pages 115?124.
Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proceedings of the Con-
ference on Empirical Methods in Natural Language Pro-
cessing (EMNLP). pages 79?86.
Shimada, Kazutaka and Tsutomu Endo. 2008. Seeing
several stars: A rating inference task for a document
containing several evaluation criteria. In Advances in
Knowledge Discovery and Data Mining, 12th Pacific-
Asia Conference, PAKDD 2008. Springer, Osaka, Japan,
volume 5012 of Lecture Notes in Computer Science,
pages 1006?1014.
Snyder, Benjamin and Regina Barzilay. 2007. Multiple
aspect ranking using the Good Grief algorithm. In
Proceedings of the Joint Human Language Technol-
ogy/North American Chapter of the ACL Conference
(HLT-NAACL). pages 300?307.
Turney, Peter. 2002. Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classification of
reviews. In Proceedings of the Association for Compu-
tational Linguistics (ACL). pages 417?424.
Yu, Hong and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP).
20
