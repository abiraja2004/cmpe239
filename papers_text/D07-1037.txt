Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 351?359, Prague, June 2007. c?2007 Association for Computational Linguistics
Topic Segmentation with Hybrid Document Indexing
Irina Matveeva
Department of Computer Science
University of Chicago
Chicago, IL 60637
matveeva@cs.uchicago.edu
Gina-Anne Levow
Department of Computer Science
University of Chicago
Chicago, IL 60637
levow@cs.uchicago.edu
Abstract
We present a domain-independent unsuper-
vised topic segmentation approach based on
hybrid document indexing. Lexical chains
have been successfully employed to evalu-
ate lexical cohesion of text segments and to
predict topic boundaries. Our approach is
based in the notion of semantic cohesion. It
uses spectral embedding to estimate seman-
tic association between content nouns over a
span of multiple text segments. Our method
significantly outperforms the baseline on the
topic segmentation task and achieves perfor-
mance comparable to state-of-the-art meth-
ods that incorporate domain specific infor-
mation.
1 Introduction
The goal of topic segmentation is to discover story
boundaries in the stream of text or audio recordings.
Story is broadly defined as segment of text contain-
ing topically related sentences. In particular, the
task may require segmenting a stream of broadcast
news, addressed by the Topic Detection and Track-
ing (TDT) evaluation project (Wayne, 2000; Allan,
2002). In this case topically related sentences belong
to the same news story. While we are considering
TDT data sets in this paper, we would like to pose
the problem more broadly and consider a domain-
independent approach to topic segmentation.
Previous research on topic segmentation has
shown that lexical coherence is a reliable indicator
of topical relatedness. Therefore, many approaches
have concentrated on different ways of estimating
lexical coherence of text segments, such as seman-
tic similarity between words (Kozima, 1993), sim-
ilarity between blocks of text (Hearst, 1994), and
adaptive language models (Beeferman et al, 1999).
These approaches use word repetitions to evaluate
coherence. Since the sentences covering the same
story represent a coherent discourse segment, they
typically contain the same or related words. Re-
peated words build lexical chains that are conse-
quently used to estimate lexical coherence. This can
be done either by analyzing the number of overlap-
ping lexical chains (Hearst, 1994) or by building a
short-range and long-range language model (Beefer-
man et al, 1999). More recently, topic segmentation
with lexical chains has been successfully applied to
segmentation of news stories, multi-party conversa-
tion and audio recordings (Galley et al, 2003).
When the task is to segment long streams of text
containing stories which may continue at a later
point in time, for example developing news stories,
building of lexical chains becomes intricate. In ad-
dition, the word repetitions do not account for syn-
onymy and semantic relatedness between words and
therefore may not be able to discover coherence of
segments with little word overlap.
Our approach aims at discovering semantic relat-
edness beyond word repetition. It is based on the
notion of semantic cohesion rather than lexical cohe-
sion. We propose to use a similarity metric between
segments of text that takes into account semantic as-
sociations between words spanning a number of seg-
ments. This method approximates lexical chains by
averaging the similarity to a number of previous text
351
segments and accounts for synonymy by using a hy-
brid document indexing scheme. Our text segmen-
tation experiments show a significant performance
improvement over the baseline.
The rest of the paper is organized as follows. Sec-
tion 2 discusses hybrid indexing. Section 3 describes
our segmentation algorithm. Section 5 reports the
experimental results. We conclude in section 6.
2 Hybrid Document Indexing
For the topic segmentation task we would like to de-
fine a similarity measure that accounts for synonymy
and semantic association between words. This simi-
larity measure will be used to evaluate semantic co-
hesion between text units and the decrease in seman-
tic cohesion will be used as an indicator of a story
boundary. First, we develop a document representa-
tion which supports this similarity measure.
Capturing semantic relations between words in
a document representation is difficult. Different
approaches tried to overcome the term indepen-
dence assumption of the bag-of-words representa-
tion (Salton and McGill, 1983) by using distribu-
tional term clusters (Slonim and Tishby, 2000) and
expanding the document vectors with synonyms, see
(Levow et al, 2005). Since content words can be
combined into semantic classes there has been a
considerable interest in low-dimensional representa-
tions. Latent Semantic Analysis (LSA) (Deerwester
et al, 1990) is one of the best known dimension-
ality reduction algorithms. In the LSA space doc-
uments are indexed with latent semantic concepts.
LSA maps all words to low dimensional vectors.
However, the notion of semantic relatedness is de-
fined differently for subsets of the vocabulary. In ad-
dition, the numerical information, abbreviations and
the documents? style may be very good indicators of
their topic. However, this information is no longer
available after the dimensionality reduction.
We use a hybrid approach to document indexing
to address these issues. We keep the notion of la-
tent semantic concepts and also try to preserve the
specifics of the document collection. Therefore, we
divide the vocabulary into two sets: nouns and the
rest of the vocabulary. The set of nouns does not
include proper nouns. We use a method of spec-
tral embedding, as described below and compute a
low-dimensional representation for documents using
only the nouns. We also compute a tf-idf represen-
tation for documents using the other set of words.
Since we can treat each latent semantic concept in
the low-dimensional representation as part of the vo-
cabulary, we combine the two vector representations
for each document by concatenating them.
2.1 Spectral Embedding
A vector space representation for documents and
sentences is convenient and makes the similarity
metrics such as cosine and distance readily avail-
able. However, those metrics will not work if they
don?t have a meaningful linguistic interpretation.
Spectral methods comprise a family of algo-
rithms that embed terms and documents in a low-
dimensional vector space. These methods use pair-
wise relations between the data points encoded in a
similarity matrix. The main step is to find an embed-
ding for the data that preserves the original similari-
ties.
GLSA We use Generalized Latent Semantic Anal-
ysis (GLSA) (Matveeva et al, 2005) to compute
spectral embedding for nouns. GLSA computes
term vectors and since we would like to use spectral
embedding for nouns, it is well-suited for our ap-
proach. GLSA extends the ideas of LSA by defining
different ways to obtain the similarities matrix and
has been shown to outperform LSA on a number of
applications (Matveeva and Levow, 2006).
GLSA begins with a matrix of pair-wise term sim-
ilarities S, computes its eigenvectors U and uses the
first k of them to represent terms and documents, for
details see (Matveeva et al, 2005). The justifica-
tion for this approach is the theorem by Eckart and
Young (Golub and Reinsch, 1971) stating that inner
product similarities between the term vectors based
on the eigenvectors of S represent the best element-
wise approximation to the entries in S. In other
words, the inner product similarity in the GLSA
space preserves the semantic similarities in S.
Since our representation will try to preserve se-
mantic similarities in S it is important to have a ma-
trix of similarities which is linguistically motivated.
352
Word Nearest Neighbors in GLSA Space
witness testify prosecutor trial testimony juror eyewitness
finance fund bank investment economy crisis category
broadcast television TV satellite ABC CBS radio
hearing hearing judge voice chatter sound appeal
surprise announcement disappointment stunning shock reaction astonishment
rest stay remain keep leave portion economy
Table 1: Words? nearest neighbors in the GLSA semantic space.
2.2 Distributional Term Similarity
PMI Following (Turney, 2001; Matveeva et al,
2005), we use point-wise mutual information (PMI)
to compute the matrix S. PMI between random vari-
ables representing the words wi and wj is computed
as
PMI(wi, wj) = log
P (Wi = 1,Wj = 1)
P (Wi = 1)P (Wj = 1)
. (1)
Thus, for GLSA, S(wi, wj) = PMI(wi, wj).
Co-occurrence Proximity An advantage of PMI
is the notion of proximity. The co-occurrence statis-
tics for PMI are typically computed using a sliding
window. Thus, PMI will be large only for words that
co-occur within a small context of fixed size.
Semantic Association vs. Synonymy Although
GLSA was successfully applied to synonymy in-
duction (Matveeva et al, 2005), we would like to
point out that the GLSA discovers semantic associ-
ation in a broad sense. Table 1 shows a few words
from the TDT2 corpus and their nearest neighbors
in the GLSA space. We can see that for ?witness?,
?finance? and ?broadcast? words are grouped into
corresponding semantic classes. The nearest neigh-
bors for ?hearing? and ?stay? represent their differ-
ent senses. Interestingly, even for the abstract noun
?surprise? the nearest neighbors are meaningful.
2.3 Document Indexing
We have two sets of the vocabulary terms: a set of
nouns, N , and the other words, T . We compute tf-idf
document vectors indexed with the words in T :
~di = (?i(w1), ?i(w2), ..., ?i(w|T |)), (2)
where ?i(wt) = tf(wt, di) ? idf(wt).
We also compute a k-dimensional representation
with latent concepts ci as a weighted linear combi-
nation of GLSA term vectors ~wt:
~di = (c1, ..., ck) =
?
t=1:|N |
?i(wt) ? ~wt, (3)
We concatenate these two representations to gener-
ate a hybrid indexing of documents:
~di = (?i(w1), ..., ?i(w|T |), c1, ...ck) (4)
In our experiments, we compute document
and sentence representation using three indexing
schemes: the tf-idf baseline, the GLSA represen-
tation and the hybrid indexing. The GLSA index-
ing computes term vectors for all vocabulary words;
document and sentence vectors are generated as lin-
ear combinations of term vectors, as shown above.
2.4 Document similarity
One can define document similarity at different lev-
els of semantic content. Documents can be similar
because they discuss the same people or events and
because they discuss related subjects and contain se-
mantically related words. Hybrid Indexing allows
us to combine both definitions of similarity. Each
representation supports a different similarity mea-
sure. tf-idf uses term-matching, the GLSA represen-
tation uses semantic association in the latent seman-
tic space computed for all words, and hybrid index-
ing uses a combination of both: term-matching for
named entities and content words other than nouns
combined with semantic association for nouns.
In the GLSA space, the inner product between
document vectors contains all pair-wise inner prod-
uct between their words, which allows one to detect
semantic similarity beyond term matching:
?~di, ~dj? =
?
w?di
?
v?dj
?i(w)?j(v)?~w,~v? (5)
353
If documents contain words which are different but
semantically related, the inner product between the
term vectors will contribute to the document similar-
ity, as illustrated with an example in section 5.
When we compare two documents indexed with
the hybrid indexing scheme, we compute a combi-
nation of similarity measures:
?~di, ~dj? =
?
nk?di
?
nm?dj
?i(nk)?j(nm)? ~nk, ~nm?+
?
t?T
?i(t) ? ?j(t).
(6)
Document similarity contains semantic association
between all pairs of nouns and uses term-matching
for the rest of the vocabulary.
3 Topic Segmentation with Semantic
Cohesion
Our approach to topic segmentation is based on
semantic cohesion supported by the hybrid index-
ing. Topic segmentation approaches use either sen-
tences (Galley et al, 2003) or blocks of words as
text units (Hearst, 1994). We used both variants
in our experiments. When using blocks, we com-
puted blocks of a fixed size (typically 20 words) slid-
ing over the documents in a fixed step size (10 or
5 words). The algorithm predicts a story boundary
when the semantic cohesion between two consecu-
tive units drops. Blocks can cross story boundaries,
thus many predicted boundaries will be displaced
with respect to the actual boundary.
Averaged similarity In our preliminary experi-
ments we used the largest difference in score to pre-
dict story boundary, following the TextTiling ap-
proach (Hearst, 1994). We found, however, that in
our document collection the word overlap between
sentences was often not large and pair-wise similar-
ity could drop to zero even for sentences within the
same story, as will be illustrated below. We could
not obtain satisfactory results with this approach.
Therefore, we used the average similarity by us-
ing a history of fixed size n. The semantic cohesion
score was computed for the position between two
text units, ti and tj as follows:
score(ti, tj) =
1
n
n?1
?
k=0
?ti?k, tj? (7)
Our approach predicts story boundaries at the min-
ima of the semantic cohesion score.
Approximating Lexical Chains One of the mo-
tivations for our cohesion score is that it approxi-
mates lexical chains, as for example in (Galley et al,
2003). Galley et al (Galley et al, 2003) define lex-
ical chains R1, .., RN by considering repetitions of
terms t1, .., tN and assigning larger weights to short
and compact chains. Then the lexical cohesion score
between two text units ti and tj is based on the num-
ber of chains that overlap both of them:
score(ti, tj) =
N
?
k=1
wk(ti)wk(tj), (8)
where wk(ti) = score(Rj) if the chain Rj over-
laps ti and zero otherwise. Our cohesion score takes
into account only the chains for words that occur in
tj and have another occurrence within n previous
sentences. Due to this simplification, we compute
the score based on inner products. Once we make
the transition to inner products, we can use hybrid
indexing and compute semantic cohesion score be-
yond term repetition.
4 Related Approaches
We compare our approach to the LCseg algorithm
which uses lexical chains to estimate topic bound-
aries (Galley et al, 2003). Hybrid indexing allows
us to compute semantic cohesion score rather than
the lexical cohesion score based on word repetitions.
Choi at al. used LSA for segmentation (Choi et
al., 2001). LSA (Deerwester et al, 1990) is a spe-
cial case of spectral embedding and Choi at al. (Choi
et al, 2001) used all vocabulary words to com-
pute low-dimensional document vectors. We use
GLSA (Matveeva et al, 2005) because it computes
term vectors as opposed to the dual document-term
representation with LSA and uses a different ma-
trix of pair-wise similarities. Furthermore, Choi
at al. (Choi et al, 2001) used clustering to predict
boundaries whereas we used the average similarity
scores.
354
s1: The Cuban news agency Prensa Latina called Clinton ?s announcement Friday that Cubans picked up
at sea will be taken to Guantanamo Bay naval base a ? new and dangerous element ? in U S immigration policy.
s2: The Cuban government has not yet publicly reacted to Clinton ?s announcement that Cuban rafters
will be turned away from the United States and taken to the U S base on the southeast tip of Cuba.
s5: The arrival of Cuban emigrants could be an ? extraordinary aggravation ? to the situation , Prensa Latina said.
s6: It noted that Cuba had already denounced the use of the base as a camp for Haitian refugees.
whom it had for many years encouraged to come to the United States.
s8: Cuba considers the land at the naval base , leased to the United States at the turn of the century,
to be illegally occupied.
s10: General Motors Corp said Friday it was recalling 5,600 1993-94 model Chevrolet Lumina, Pontiac
Trans Sport and Oldsmobile Silhouette minivans equipped with a power sliding door and built-in child seats.
s14: If this occurs , the shoulder belt may not properly retract , the carmaker said.
s15: GM is the only company to offer the power-sliding door.
s16: The company said it was not aware of any accidents or injuries related to the defect.
s17: To correct the problem , GM said dealers will install a modified interior trim piece that will reroute the seat belt.
Table 2: TDT. The first 17 sentences in the first file.
Existing approaches to hybrid indexing used dif-
ferent weights for proper nouns, nouns phrase heads
and use WordNet synonyms to expand the docu-
ments, for example (Hatzivassiloglou et al, 2000;
Hatzivassiloglou et al, 2001). Our approach does
not require linguistic resources and learning the
weights. The semantic associations between nouns
are estimated using spectral embedding.
5 Experiments
5.1 Data
The first TDT collection is part of the LCseg
toolkit1 (Galley et al, 2003) and we used it to com-
pare our approach to LCseg. We used the part of this
collection with 50 files with 22 documents each.
We also used the TDT2 collection2 of news arti-
cles from six news agencies in 1998. We used only
9,738 documents that are assigned to one topic and
have length more than 50 words. We used the Lemur
toolkit3 with stemming and stop words list for the
tf-idf indexing; we used Bikel?s parser4 to obtain
the POS-tags and select nouns; we used the PLA-
PACK package (Bientinesi et al, 2003) to compute
the eigenvalue decomposition.
1http://www1.cs.columbia.edu/ galley/tools.html
2http://nist.gov/speech/tests/tdt/tdt98/
3http://www.lemurproject.org/
4http://www.cis.upenn.edu/ dbikel/software.html
Evaluation For the TDT data we use the error
metric pk (Beeferman et al, 1999) and WindowD-
iff (Pevzner and Hearst, 2002) which are imple-
mented in the LCseg toolkit. We also used the
TDT cost metric Cseg5, with the default parameters
P(seg)=0.3, Cmiss=1, Cfa=0.3 and distance of 50
words. All these measures look at two units (words
or sentences) N units apart and evaluate how well
the algorithm can predict whether there is a bound-
ary between them or not. Lower values mean better
performance for all measures.
Global vs. Local GLSA Similarity To obtain the
PMI values we used the TDT2 collection, denoted as
GLSAlocal. Since co-occurrence statistics based on
larger collections give a better approximation to lin-
guistic similarities, we also used 700,000 documents
from the English GigaWord collection, denoted as
GLSA. We used a window of size 8.
5.2 Topic Segmentation
The first set of experiments was designed to evaluate
the advantage of the GLSA representation over the
baseline. We compare our approach to the LCseg
algorithm (Galley et al, 2003) and use sentences as
segmentation unit. To avoid the issue of parameters
setting when the number of boundaries is not known,
we provide each algorithm with the actual numbers
5www.nist.gov/speech/tests/tdt/tdt98/doc/
tdt2.eval.plan.98.v3.7.ps
355
10 21 27 45 52 65 73 89 99
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
10 21 27 45 52 65 73 89 99
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0 2 4 6 8 10 12 14 16 18 20
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
GLSA
tfidf
Figure 1: TDT. Pair-wise sentence similarities for tf-idf (left), GLSA (middle); x-axis shows story bound-
aries. Details for the first 20 sentences, table 2 (right).
10 21 27 45 52 65 73 89 99
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
10 21 27 45 52 65 73 89 99
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
10 21 27 45 52 65 73 89 99
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 2: TDT. Pair-wise sentence similarities for tf-idf (left), GLSA (middle) averaged over 10 preceeding
sentences; LCseg lexical cohesion scores (right). X-axis shows story boundaries.
of boundaries.
TDT We use the LCseg approach and our ap-
proach with the baseline tf-idf representation and the
GLSA representation to segment this corpus. Ta-
ble 2 shows a few sentences. Many content words
are repeated, so the lexical chains is definitely a
sound approach. As shown in Table 2, in the first
story the word ?Cuba? or ?Cuban? is repeated in ev-
ery sentence thus generating a lexical chain. On the
topic boundary, the word overlap between sentences
is very small. At the same time, the repetition of
words may also be interrupted within a story: sen-
tence 5, 6 and sentences 14, 15, 16 have little word
overlap. LCseg deals with this by defining several
parameters to control chain length and gaps. This
simple example illustrates the potential benefit of se-
mantic cohesion. Table 2 shows that ?General Mo-
tors? or ?GM? are not repeated in every sentence of
the second story. However, ?GM?, ?carmaker? and
?company? are semantically related. Making this
information available to the segmentation algorithm
allows it to establish a connection between each sen-
tence of the second story.
We computed pair-wise sentence similarities be-
tween pairs of consecutive sentences in the tf-idf and
GLSA representations. Figure 1 shows the similar-
ity values plotted for each sentence break. The pair-
wise similarities based on term-matching are very
spiky and there are many zeros within the story. The
GLSA-based similarity makes the dips in the simi-
larities at the boundaries more prominent. The last
plot gives the details for the sentences in table 2.
In the tf-idf representation sentences without word
overlap receive zero similarity but the GLSA repre-
sentation is able to use the semantic association be-
tween between ?emigrants? and ?refugees? for sen-
tences 5 and 6, and also the semantic association be-
tween ?carmaker? and ?company? for sentences 14
356
Measure tf-idf GLSA LCseg
Pmiss 0.29 0.19 N/A
Pfa 0.14 0.09 N/A
Cseg 0.18 0.08 N/A
pk 0.24 0.17 0.07
wd 0.27 0.21 0.10
Table 3: TDT segmentation results.
and 15.
This effect increases as we use the semantic cohe-
sion score as in equation 7. Figure 2 shows the simi-
larity values for tf-idf and GLSA and also the lexical
cohesion scores computed by LCseg. The GLSA-
based similarities are not quite as smooth as the LC-
seg scores, but they correctly discover the bound-
aries. LCseg parameters are fine-tuned for this doc-
ument collection. We used a general TDT2 GLSA
representation for this collection, and the only seg-
mentation parameter we used is to avoid placing
next boundary within n=3 sentences of the previ-
ous one. For this reason the predicted boundary may
be one sentence off the actual boundary. These re-
sults are summarized in Table 3. The GLSA repre-
sentation performs significantly better than the tf-idf
baseline. Its pk and WindowDiff scores with default
parameters for LCseg are worse than for LCseg. We
attribute it to the fact that we did not fine-tuned our
method to this collection and that boundaries are of-
ten placed one position off the actual boundary.
TDT2 For this collection we used three different
indexing schemes: the tf-idf baseline, the GLSA rep-
resentation and the hybrid indexing. Each represen-
tation supports a different similarity measure. Our
TDT experiments showed that the semantic cohe-
sion score based on the GLSA representation im-
proves the segmentation results. The variant of
the TDT corpus we used is rather small and well-
balanced, see (Galley et al, 2003) for details. In
the second phase of experiments we evaluate our ap-
proach on the larger TDT2 corpus. The experiments
were designed to address the following issues:
? performance comparison between GLSA and
Hybrid indexing representations. As men-
tioned before, GLSA embeds all words in
a low-dimensional space. Whereas semantic
#b known
Method Pmiss Pfa Cseg
tf-idf 0.52 0.14 0.19
GLSA 0.4 0.1 0.14
GLSA local 0.44 0.12 0.16
Hybrid 0.34 0.10 0.12
Hybrid local 0.38 0.09 0.13
LCseg 0.80 0.19 0.28
#b unknown
Method Pmiss Pfa Cseg
tf-idf 0.42 0.2 0.17
GLSA 0.37 0.13 0.14
GLSA local 0.35 0.19 0.14
Hybrid 0.26 0.16 0.11
Hybrid local 0.27 0.18 0.12
Table 4: TDT2 segmentation results. Sliding blocks
with size 20 and stepsize 10; similarity averaged
over 10 preceeding blocks.
classes for nouns have theoretical linguistic jus-
tification, it is harder to motivate a latent space
representation for example for proper nouns.
Therefore, we want to evaluate the advantage
of using spectral embedding only for nouns.
? collection dependence of similarities. The sim-
ilarity matrix S is computed using the TDT2
corpus (GLSAlocal) and using the larger Giga-
Word corpus. The larger corpus provides more
reliable co-occurrence statistics. On the other
hand, word distribution is different from that
in the TDT2 corpus. We wanted to evaluate
whether semantic similarities are collection in-
dependent.
Table 4 shows the performance evaluation. We show
the results computed using blocks containing 20
words (after preprocessing) with step size 10. We
tried other parameter values but did not achieve bet-
ter performance, which is consistent with other re-
search (Hearst, 1994; Galley et al, 2003). We show
the results for two settings: predict a known num-
ber of boundaries, and predict boundaries using a
threshold. In our experiments we used the average
of the smallest N scores as threshold, N = 4000
showing best results.
357
The spectral embedding based representations
(GLSA, Hybrid) significantly outperform the base-
line. This confirms the advantage of the semantic
cohesion score vs. term-matching. Hybrid index-
ing outperforms the GLSA representation support-
ing our intuition that semantic association is best de-
fined for nouns.
We used the GigaWord corpus to obtain the pair-
wise word associations for the GLSA and Hybrid
representations. We also computed GLSAlocal and
Hybridlocal using the TDT2 corpus to obtain the
pair-wise word associations. The co-occurrence
statistics based on the GigaWord corpus provide
more reliable estimations of semantic association
despite the difference in term distribution. The dif-
ference is larger for the GLSA case when we com-
pute the embedding for all words, GLSA performs
better than GLSAlocal. Hybridlocal performs only
slightly worse than Hybrid. This seems to support
the claim that semantic associations between nouns
are largely collection independent. On the other
hand, semantic associations for proper names are
collection dependent at least because the collections
are static but the semantic relations of proper names
may change over time. The semantic space for a
name of a president, for example, is different for the
period of time of his presidency and for the time be-
fore and after that.
Disappointingly, we could not achieve good re-
sults with LCseg. It tends to split stories into short
paragraphs. Hybrid indexing could achieve results
comparable to state-of-the art approaches, see (Fis-
cus et al, 1998) for an overview.
6 Conclusion and Future Work
We presented a topic segmentation approach based
on semantic cohesion scores. Our approach is do-
main independent, does not require training or use
of lexical resources. The scores are computed based
on the hybrid document indexing which uses spec-
tral embedding in the space of latent concepts for
nouns and keeps proper nouns and other specifics of
the documents collections unchanged. We approxi-
mate the lexical chains approach by simplifying the
definition of a chain which allows us to use inner
products as basis for the similarity score. The simi-
larity score takes into account semantic relations be-
tween nouns beyond term matching. This semantic
cohesion approach showed good results on the topic
segmentation task.
We intend to extend the hybrid indexing approach
by considering more vocabulary subsets. Syntactic
similarity is more appropriate for verbs, for exam-
ple, than co-occurrence. As a next step, we intend to
embed verbs using syntactic similarity. It would also
be interesting to use lexical chains for proper names
and learn the weights for different similarity scores.
References
J. Allan, editor. 2002. Topic Detection and Tracking:
Event-based Information Organization. Kluwer Aca-
demic Publishers.
Doug Beeferman, Adam Berger, and John D. Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34(1-3):177?210.
Paolo Bientinesi, Inderjit S. Dhilon, and Robert A. van de
Geijn. 2003. A parallel eigensolver for dense sym-
metric matrices based on multiple relatively robust
representations. UT CS Technical Report TR-03-26.
Freddy Choi, Peter Wiemer-Hastings, and Johanna
Moore. 2001. Latent Semantic Analysis for text seg-
mentation. In Proceedings of EMNLP, pages 109?
117.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
J. G. Fiscus, George Doddington, John S. Garofolo, and
Alvin Martin. 1998. NIST?s 1998 topic detection and
tracking evaluation (tdt2). In Proceedings of NIST?s
1998 Topic Detection and Tracking Evaluation.
M. Galley, K. McKeown, E. Fosler-Lussier, and H. Jing.
2003. Discourse segmentation of multi-party conver-
sation. In Proceedings of ACL.
G. Golub and C. Reinsch. 1971. Handbook for Ma-
trix Computation II, Linear Algebra. Springer-Verlag,
New York.
V. Hatzivassiloglou, Luis Gravano, and Ankineedu Mag-
anti. 2000. An investigation of linguistic features and
clustering algorithms for topical document clustering.
In Proceedings of SIGIR, pages 224?231.
V. Hatzivassiloglou, Regina Barzilay Min-Yen Kan Ju-
dith L. Klavans, Melissa L. Holcombe, and Kath-
leen R. McKeown. 2001. Simfinder: A flexible
358
clustering tool for summarization. In Proceedings of
NAACL, pages 41?49.
Marti A. Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings of ACL, pages 9?16.
Hideki Kozima. 1993. Text segmentation based on sim-
ilarity between words. In Proceedings of ACL, pages
286?288.
Gina-Anne Levow, Douglas W. Oard, and Philip Resnik.
2005. Dictionary-based techniques for cross-language
information retrieval. Information Processing and
Management: Special Issue on Cross-language Infor-
mation Retrieval.
Irina Matveeva and Gina-Anne Levow. 2006. Graph-
based Generalized Latent Semantic Analysis for docu-
ment representation. In Proc. of the TextGraphs Work-
shop at HLT/NAACL.
Irina Matveeva, Gina-Anne Levow, Ayman Farahat, and
Christian Royer. 2005. Generalized Latent Semantic
Analysis for term representation. In Proc. of RANLP.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Comput. Linguist., 28(1):19?36.
Gerard Salton and Michael J. McGill. 1983. Introduction
to Modern Information Retrieval. McGraw-Hill.
Noam Slonim and Naftali Tishby. 2000. Document clus-
tering using word clusters via the information bottle-
neck method. In Research and Development in Infor-
mation Retrieval, pages 208?215.
Peter D. Turney. 2001. Mining the web for synonyms:
PMI?IR versus LSA on TOEFL. Lecture Notes in
Computer Science, 2167:491?502.
C. Wayne. 2000. Multilingual topic detection and track-
ing: Successful research enabled by corpora and eval-
uation. In Proceedings of Language Resources and
Evaluation Conference (LREC), pages 1487?1494.
359
