ENGLISH GENERATION FROM INTERL INGUA 
BY  EXAMPI~E- I~ASI~I )  METHOI )  
Abstract 
This paper describes the experiment of the English 
generation from interlingua by the example-based 
method. The generator is implemented by using English 
Word Dictionary ,and Concept Dictionary developed in 
EDR. How to construct examples lind how to define the 
similarities are main problems. The results of experi- 
ments are shown. 
1. Introduction 
Eiji Komatsu*, Jin Cui**, lliroshi Yasuhara** 
(*) Oki Electric htdustry Co. Ltd. Meltimedia I.aboratory 
11-22, Shibaura 4-Chome, Minato-ku, Tokyo 108 Japan 
e-mail : komatsu @okilab.oki.co.jp 
(**) Japan Electronic l)ictionary Research Institute l.td. (EDR) 6th Laboratory 
Mita-Kokusai-Bldg. 4-28, Mira l-Chome, Minato-kv, Tokyo 108 Japan 
e-mail : sai@edrrr.edr.co.jp, yasuhara@edr6r.edr.co.jp 
to mean example data of the example-based method. And 
the terms "interlingua" and "syntactic tree" are used to 
mean sets, elements m~d fragments of elentents. 
2. Input anti Output 
The generator t,anslates an interlingt, a to a syntactic 
tree. Fig.2.1 shows a sample of input interlingnae and 
Fig.2.2, a sample of output syntactic trees. Both samples 
correspond to the same sentence "My brother will take 
the medicine". 
This paper describes the generator that is originally 
implemented tocorrect and evah,ate English Word Dic- 
tionary and Concept Dictionary being developed in EDR 
(El)R,1993). To evaluate Concept Dictionary, as the first 
strategy, interlingua method was introduced. As the num- 
ber o1' concepts is very large and they are elements of 
complex hierarchy, it is difficult to make roles and on the 
other hand the example-based method was expected to be 
more effective than the rule-based method. So, as the sec- 
ond strategy, the example-bused method was also intro- 
duced. 
The example-based method is usually used in MT by 
the transfer method (Nagao, 1984; Sato, 1991; Stnnita, 
1992), though one by Sadler (1989) is by the interlingua 
method. In this generator, the example-based method co- 
exists with the interlingua method because of above rea- 
sons, but the combination of the example-based method 
and tim interlingua method is not intportant, because 
l'rom another point of view, the generation from 
interlingua is recognized as a translation from one hm- 
guage i.e. interlingua to another i.e. English and the gen- 
eration from interlingua can be seen similar as transla- 
tions in above MT systems. So in this experiment, how to 
apply the example-based method to various natural hm- 
guage processing and lbr which parts the method are 
suitable are the main interests. For this purpose, the gen- 
erator is designed to execute the generation with maxi- 
mum usage of the example-based method. 
In this experiment, he coverage of the generation is
not complete, that is, some elements t, ch as articles and 
conjunctions are not generated. 
Below, section 2 describes the input and ot, tput of the 
generator, section 3, examples used in this system, sec- 
tion 4, the similarities used to retrieve xamples and to 
select words, section 5, the generation algorithm, section 
6, the experiments for verb selections and section 7, the 
conclusion. 
The examples, similarities and the generation algo- 
rithm are decided a priori then modilied in response to 
the output of the generator. 
To avoid confusions, the word "example" is used only 
(*) This work has been done when the author wits in EDR. 
"My brother will take tile medicine." 
(non-statement) 
.~e086)  --------liD-- {2dc30I) (no>statement) 
modify 
agent 
7 
{ 3b f0d2~ (statement) 
future {3bf0lx)} (non-statement) 
Fig.2.1 Input lnterlingua 
lnterlinguae consist of concepts, conceptual relations 
and attributes. Each concepts are classified as "state- 
ments" or "non-statements". Concepts are represented by 
concept identification umbers (To distinguish concepts 
easily by men, concept illustrations are also given). Inter- 
pretations of codes relating to interlinguae in this paper 
are shown in Table 2.1. In the table, as for concept iden- 
tification umbers, concept illustrations are showed as in- 
terp,'etations of codes. 
"My brother will take the medicine." 
brother(ENl)-M- my(EPl) 
M(s j )  
tak(EVE;EVED;I{CV9;EVDO0) - -S - -  wiII(EAV) 
"~S'-. e(EEV) 
M((to)..,.,....~. 
medicine(EN1) 
Fig.2.2 Output Syntactic 'Free 
363 
Information 
Concept 
Identification 
Numbers 
Concept 
Relations 
Table 2.1 Codes for Interlinguae 
Code Interpretation 
(3bfOd2) todrink something 
(3be086) brothers 
(Oe351f) sisters 
(2dc301) c#I 
(2dc304) c#1~ 
(3bf0D) a substance used on or in the body to treat a disease 
(3bdbf6) a drilled liquor named wtfiskey 
(3bd862) a drug or agent hat reduces fever 
(3cee4f) to obtain a thing which one wante~l 
(3ceae3) to become a certain condition 
(0fde5f) to accept others' opinions and wishes 
i i .. 
(0c98dc) tbo first part of the day, from the time when the sun 
agent 
rises, usually tmtil the time when the midday meal 
ts eaten 
Subject hat brings about a voluntary action. 
Conscious attd automated ntities are suclt subjects. 
"Animals eat" 
(eat) - -  agent~ (animals) 
Object affected by an action or change 
"Eat food." 
(eat ) - -  ob jec t~ (food) 
Time at which an event begins 
"Work until a sc time" 
(wake up) ~ t ime~ Gin time) 
object 
time 
modifier Other elationshlps 
past Viewpoint is in the past 
present Viewpoint is in the present 
future Viewpoint is in the future 
end The end of an action or event 
already Already occurred 
Table 2.2 
hfformatlon Code 
Part-of Speech EN I 
EP1 
EVE 
"~i;v 
EEV 
EPg. 
Grammatical EVSTM 
hffonnation I~VII 
EVED 
EVEN 
ECV9 
EVD(X) 
EVDO6 
Surface Relation: M(sj) 
M(do) 
M(adj) 
M(obpp) 
S 
Codes for Syntactic Trees 
lnteq~retatlon 
Common llOll i1 
Personal pronoun 
Verb 
Anxu!iary verb 
Verb eliding 
Preposition 
Article 
Uninflected part 
Infinitive 
Past tense 
Past participle 
Partially irregular inflections 
(%" follws) 
Takes a direct object 
Takes a direct object 
(the direct object is to-infinltiv( 
subject relation 
direct object relation 
adjective modification 
obligatory prepositional phrase 
relations between content words 
and functional words 
Syntactic trees consist of words, part-of-speeches, 
grammatic~d information and syntactic relations. 
The interpretations of codes relating to syntactic trees 
used in this paper are shown Table 2.2. 
3. Examples 
An example should be a pair of an interlingua nd a 
syntactic tree. For the flexibility of usage of examples, 
interlinguae and syntactic trees in ex:unples are divided 
into smaller parts that are small enough to use flexibly 
but have enough information for generations. 
Fig.3.1 shows the common form of interlinguae and 
syntactic trees in examples (referred as "basic unit", be- 
low). An example is a pair of fragments in this form made 
from an interlingua nd a syntactic tree. 
tip (near to tile root of Ihe tree lower n~lt: 
structure of an interlingm0 
lower arc lower node 
uppt r n(~le Upl~r arc ~ " 
attribute 
Fig.3.1 Basic Uifits 
Fig.3.2 shows the linguistic resources used by the gen- 
erator. As the results of trying to execute as many pro- 
cesses as possible by the example-based method, it be- 
came necessary for the generator to use two different 
kinds of examples (referred as "Basic Example Set" and 
"Example Set tbr Attribute", below). 
1/~~.qt  Word Dictionar~ 
I English Generator _ EDR Concept Dictionary \] 
/ \  
Examples ~_~ I ~  I 
Fig.3.2 Linguistic Resources 
Fig.3.3 shows examples in the Basic Example Set. 
Circlod nodes are "central nodes". Basic Example Set is 
supposod to be used for selecting content words for con- 
cepts. Functional words except prepositions and grmn- 
matical information for inflections are removed, since 
they are unnecessary for this purpose. In Fig.3.2, example 
(A) and (13) have 11o upper node and Example (C) and (D) 
have no lower node. Examples in this set are accessed by 
concepts in the central nodes of interlinguae; Example 
(A) and (B) are accessed by (3bf0d2) and (C), by 
(3bf0f9) and (D) by (0c98dc) . When several ex- 
amples with the same key exist, by the simih'u'ity defined 
below, only one example is finally accepted. 
Fig.3.4 shows examples in the Example Set for At- 
tributes. This example set is supposed to be used for de- 
ciding inflection (i.e. selecting the word whose inflection 
corresponds to the attributes) and adding functional 
words for attributes. Content words in lower nodes are 
364 
removed, since the upper node influences to the inflec- 
tion of the center word, but the lower nodes rarely don't. 
Functional words in lower nodes are added to the outputs. 
Concepts and spellings of words are also removed, since 
they can be decided by Basic Example Set and unneces- 
sary here. Examples are accessed by combinations of at- 
tributes in interlinguae, some grammatical information of
the upper node, those of central nodes and the surface re- 
lation of the upper arc; in Fig.3.4, Example (a) is accessed 
by (past, -, EVE; EVED, -), Example (b) by (end, already, 
-, EVE; EVEN, -), Example (c) by (present, -, EVE; 
EVSTM; ECV9, -), Example (d) by (present, , -, EVE; 
EVIl, -), Example (e) by (future, -, EVE; FNSTM; 
ECV9,- ) and Example (1) by (-, EVE; EVDO0, EN 1, 
M(do) ). Example (a), (b), (c), (d) and (e) have no upper 
node. Since examples in this set don't include concepts, 
examples are accessed eterministically and the similar- 
ity is not used. 
4. Similarities 
There are two major similarities in the example-based 
method. One is for the source language and used for se- 
lecting examples. Anotber is for the target language and 
used for creating outputs. In this generator, the lbrmer is 
the similarity between interlinguae (in tile form of basic 
t, nits) and the latter is the similarity between words. In 
the generator, the similarity is used only for Basic Ex-. 
ample Set. 
Example (A) : Brother takes the medicine in the morning. 
( 31~086} bro0~er(l!N I ) 
..,1 
agent f -'M(sj) 
object 
(3bf0f9) mcdicinc(EN1 ) 
\],Rel linguu Syntactic Wlec 
F.xampl'e (B) : Sister drinks the whiskey. 
(Oe35 lf) st~r( I';N 1 ) 
agent ~ (  . . . . . . . . . .  ) ~ , lk (  . . . . . . . . . . . . .  R I~ M(nj) 
"~'J?':'....~. ""M(.,,) 
(31+dbf6) ~hinkcy(l{N I )
Interlingua Synt~tic "l'l.oo 
Example (C) : Brothers takes medicine in tile morning, 
{ 3 b fljd2 } "~hjeet--II~ @ t ak(EVE;EV DO0) -M(do)q~.- ~
(non-slalemenl) 
Illterlingua Synt~lic Tree 
Example (D) : Brothers takes medicine in the morning. 
(.icAl-statement) SNIK 
iu(El'R) Inlerlinguz Syntactic Tree 
Fig.3.3 Examples in Basic Example Set 
Example (a) : *(EVE;EVED;EVDO0) 
(~state  m e r i t ) ~ ~  
past 
Example (b) :lmve *(EVE;EVEN;EVDO0) 
state merit) ~_(I~V E;EV EN; ECV9~S- -  have(EAV) 
end 
already 
Example (c) : *(EVE;EVSTM;ECV9;EVDO0) e 
statement) Q(r'.'W;;EVSTM;ECVg)~--S-- e(EI?V) 
present 
Example (d) : *(FNE;FVI~;;F.VI)O0) 
";littelrlC-l~tt) Q( I , ;V IZ ;  f2V I t ) .~  
pvese- ta t  
Example (e) : will *(I:VE;IiVSTM;ECV9;EVDO0) e 
Q-~*~"~ (stalctne,,t) ~---~(F.VI?;F.VS'I'M;ECV9)~.~,S-- wil I (EAV) 
c(\]2.EV) 
future 
Example (D : *(I{VI,\];F.VI)O0) *(EN1) 
1. )  *(r~vJ:4EVDO0) 
l'tltule 
Fig.3.4 Examples in Example Set for Attributes 
The simihu'ity between interlingt,ae is defined its follows; 
SiI(ILI,IL2) = (Sc(Clcent,C2cent) ? Kcent 
I E ,Rc(('li,C2i) ? K(slel(i)) X (k01um(Rl f'H~2) I I) 
i G i l l  f /  R2 
ILI,IL2 : intcrlinguae 
Clcent, C2cent : concepts in central nodes 
Kcent : weight of simihuity between central nodes 
Cli, C2i : concepts in lower nodes with arc i 
k(x) : weight of similarity between concepts in lower 
nodes, x is tim number of elements 
in tbe interjunction 
srel(i) : surface relation which corresponds tothe 
concept relation i 
R 1,R2 : set of conceptt,al relations each for ILl, 11.2 
ntun(S) : the number of elements of set S 
It is always assured in adwmce by tile generator that 1) tile 
word in tbe upper node of tile input is already selected (if 
there is im upper node); 2) arcs of imerlingt, a which corm- 
spond to obligatory relations of tile syntactic tree in the ex- 
;nnple, exist in the interjunction fP. 1 and R2; 3) upper arcs 
are same (if already decided); 4) part-of-speeches of words 
in upper nodes are same. l:,xamples that don't satisfy these 
365 
four conditions are rejected before the similarity calculation. 
The similarity between concepts used in the above simi- 
larity is defined as follows; 
Sc(Cl ,C2) = 
the ~lumber of common ancesters 
the number of ancesters of CI + the number ofancesters of C2 
Ilere, ancestors until three layers above are used. (Cut; 
1993) 
It is difficult to find the most similar interlingua in an ex- 
ample set to the input interlingua, because to find it ,  it is 
necessary to calculate all similarities between interlinguae in
the ex,-unple set and the input. To avoid this, in this genera- 
tor, some constraints are given for access keys i.e. central 
nodes. For "statements" in interlingua, central nodes of ex- 
amples should be same with that of the input and for "non- 
statements" in interlingua, central nodes of examples can be 
tile s,'u-ne concepts or sister concepts in the concept hierar- 
chy. By this constraints, the search of examples can be ex- 
ecuted fast. 
The similarity between words is defined as follow; 
k (0  < k < 1) i f  p~t -o f - t spe .eeh  and  lg ra lnmat icnt  in fo rn l~t lon  
.~w(~*t 1,W2)  ~ tJ itlrG ~ i r t~ 
1 
1 (O< I < 1) i f  Fmrt-of-~Fm.eeh are santo  
0 i f  s ix=l ing,  par t -o f - speech  mild grn inmt icml  in fo rn la t i~m are  
La l l  d l t  f, a re | l t  
k, 1 : some numbers 
5. Generat ion  A lgor i thm 
The generator generates fragments of a syntactic tree and 
tiredly combines them into a syntactic tree. 
The generation algorithm is as follows; 
Step 1 : Sets the current central node at the root node of 
the input interlingua. 
Step 2-1 : Cuts the basic unit for the current central node. 
Step 2-2 : Extracts candidate English words for concepts 
of the central node and lower nodes of the current basic unit, 
from English Word Dictionary. 
Step 3-1 : Retrieves an example from Basic Example Set. 
Step 3-2 : Selects the same word (neglecting inflection) 
from the candidate word lists and checks if there is an ex- 
ample in Example Set for Attributes, whose attributes and 
words in the central node coincide with attributes in the cur- 
rent basic unit and the selected word. 
Step 3-3 : If the word selection succeeded, accepts the 
example. Generates upper arc (if exists), lower arc (only for 
obligatory relations) central nodes ,and functional words for 
the central node, saves the results and similarity and calcu- 
lates the similarity of interlingua between the input and the 
example. Prepositions are extracted from the basic example. 
Step 3-4 : Repeat Step 3-2 to Step 3-3 until there remains 
no basic examples. 
Step 3-5 : Selects one example that is accepted in Step 3- 
3 and the simih'u'ity is largest. 
Step 3-6 : Puts the results. 
Step 4 : Move the current central node in the input 
linterlingua in depth-first order. 
Step 5 : Repeat Step 2-1 to Step 4 until the movement of 
the current central node ends or the word selection for a node 
fails. 
{2dc304} (non-statement) 
agent 
{3bf0d2} (statement) 
objoct 
p, t  
{3bd862} (non-statement) 
Figure 5.1 Inputted lnterllngua 
Suppose the interlingua such as Fig.5.1 is inputted and 
examples in Fig.3.3 are used as Basic Example Set and 
Fig.3.4 used as Example Set for Attributes. 
The list of candidate words for {3bf0d2} is as fol- 
lows; 
tak(EVE;EVSTM;ECV9;EVDO0), 
took(EVE;EVED;EVDO0), 
taken(EVE;EVEN;EVDO0), 
drink(I~VE;EVB;EVDO0), 
drank(EVE;EVED;EVDO0), 
drunk(EVE;EVEN;EVDO0). 
From Basic Example Set, Example (A) and (B) are re- 
trieve(l, since central nodes are same. 
By Example (A) and Example (a), took(EVE; EVED; 
EVDO0) is selected and by Example (B) and Example (a), 
drank(EVE; EVED; EVDO0) is selected. 
As similmity between the input and Example (A) is larger 
than that between the inpvt and Example (B), "took" is se- 
lected. This is because similarity between {3bd862} but 
(3bf0fg} is 0.876535 and one between {3bd862} and 
{3bdbf6} is 0. 
6. Exper iments  for  Verb  Se lect ions  
This chapter describes experiments oevaluate xamples, 
similarities and the generation algorithm. Experiments for 
verb selections are executed. 
The generator selects one word from candklate word list 
retrieved from EDR English Dictionary. 
The experiments are (lone by Jack-knife test method 
(Sumita; 1992) ; 1) Specify a concept; 2) Collect examples 
that include a word in candidate word list whose meaning is 
same with the specified concept ; 3) Remove one example 
from example sets; 4) Make tile input interlingua from the 
removed example; 5) Generate a sentence from this 
interlingua by using remained examples; 6) Compare the 
original word and the generated word for the verb; 7) Repeat 
3) - 6) by removing each example in turn. 
Below the results of three experiments (Experiment 1, 
Experiment 2, Experiment 3) me shown. 
'Fable 6.1 shows specified concepts for experiments and 
candidate word lists for the concepts. As for Experiment 1
and Experiment 2, words that have no examples is omitted 
from candidate word lists, since they won't never be se- 
lected. Fig.6.1, Fig.6.2 and Fig.6.3 show examples and gen- 
erated sentences for Experiment 1, Experiment 2 and Ex- 
periment 3 each. Examples in Fig.6.1 ,'rod Fig.6.2 are ex- 
tracted from EDR English Corpus and examples in Fig.6.3 
are extracted from a published printed English-Japanese dic- 
tionary, though some modifications (Tenses, aspects ,
366 
modals are all same. SI, bjects are same if possible) arc done. 
Sentences in the left hand sides of ,arrows are original sen- 
tences and those in the right hand side are generated sen- 
tences (In generated sentences, only verbs are generated 
words and others are copied from origimd sentences). Un- 
derlined words are words for the specified concepts. For sen- 
tences with a circle at the head of left hand sides, the genera- 
tor selects ame words with those in the original sentences. 
Sentences without circles include both right and wrong re- 
sults. 
In interlingua method, roughly speaking, all words corre- 
sponding to it concept are basically right its the generated 
word if it is grammatically consistent. So the evaluation of 
tire experiments i  delicate. 
The rates of coincides between original verbs and genero 
ated verbs are 85% (Experiment 1), 13% (Experiment 2) and 
16% (Experiment 3). Since some sentences without coin- 
cides can be also right, the real rates of success are lager than 
above nt, mbers. 
7. Conclusions 
The English generation by the example-based meth+?l is 
descrihed. For experiments of verb sel.'.+ctions, tile effective- 
hess of tile method is different for verbs to be generated. (In 
experiment 3,for "confirn?' and "endorse" the success rate is 
high), It also depends on concepts and the nunlber of candi- 
date words. 
Since examples are made automatically from large scale 
corpus and to make examples is easier than to make rules, 
the effort to design the generator became little. By removing 
redtmdant basic units, the efficiency of examples is not ser f  
OllS. 
In this paper, only the experiments for verb selections are 
shown. But the strategies that the generator uses should wiry 
in response to the categories of words to be generated. For 
example, to generate prepositions the semantic is more im- 
portant, bnt to generate other functiomtl words the syntax is 
more important. For verb selections, both are necessary. 
These strategies are also remained problems. 
Table 6.1 Concepts and Word List 
Expriments Specified Concept Candidate Word List 
E?l~riment I ~3cee4f)) acj?ev{e}(I~,VI!) 
get(EVl9 
tak\[E) (F.V F+) 
(others are omitted) 
Ext~riment 2 (3ceae3)) get (EVE) 
grew (EVE) 
fall (EVE) 
(others are omitted) 
Experiment 3 (Ofdc5f)) accept (EVE) 
acknowledgie } (EVF.) 
a(~fit (EVE) 
allow (EVE) 
answer (EVE) 
appmv{e} (EVE) 
confirm (EVE) 
endors{e} (FVE) 
grant (EVF+) 
receiv{e) (EVE) 
ratif{y} (EVE) 
recogniz{el (EVE) 
respond (EVE) 
homologat{e} (EVF.) 
ex. 01 : l ie had achieved a certain transquility. 
'- lie had ,gin a certain transquility. 
ex. 02 : Q)You have ~ our keys. 
--'- You have ~our  keys. 
ex. 03 : (1)lie quietly ,got a broom. 
- " lle qt,ietly ~ it broom. 
ex. 04 : (..~lle g~ the menus. 
-,- lie ,tg~.the menus. 
ex. 05 : ~)ln the storm 1 took shelter under it lree. 
" In the storm I took shelter under it tree. 
ex. 06 : ( ) l l e  takes dangerous drugs. 
-- ~ lle takes dangerous drugs. 
ex. 07 : (.~The people look our old house. 
The people took our old house. 
Fig.6.1 l~xamples and Results of Experinmnt 1
cx. 01 : Diantonds come expensive. 
? ~+ Diamonds become xpensive. 
ex. 02 : You ~rgLQw older. 
,- You become older. 
ex. 03 : A thing was bc~conairlg increasingly sure. 
A thing was gct_tir~ increasingly sure. 
ex. 04 : l{nvironment becomes individualized. 
'- Enviromnent grows individualized. 
ex. 05 : A man ~ oM anyhow. 
-- '- A man becomes old anyhow. 
ex. 06 : These letters became the center of my existence. 
" These letters went the center of my existettce. 
ex. 07 : Almost unbearable my fantasies become. 
Ahnost unbearable my fantasies go. 
ex. 0g : Sonmthing bad ~ wrong. 
+ + Something had fallen wrong. 
ex. 09 : We had become good f,iends during my stay 
at the hospital. 
-+ We bad ~ good friends during my stay 
at the hospital. 
ex. 10 : You're the kind to go_ violent. 
- '- You're tile kind to become violent. 
ex. 11 : ( ) t le r  eyes became bright. 
-"  l ler eyes became bright. 
ex. 12 : Eventually it become a movie. 
- ~ Fventually it ~ a movie. 
ex. 13 : After a while the signal became a buzz. 
.... After a while the signal wenl a buzz. 
ex. 14: It was ~ g  light. 
- " It was becoming light. 
ex. 15 : I le fell silent, its yesterday. 
- ~  lie went silent, as yesterday. 
ex. 16 : After a few jokes his speech became serious. 
-+ After a few jokes his speech went serious. 
ex. 17 : You'll gg$. even fatter. 
-'- You'll ,rgre.w even fatter. 
ex. 18 : She became stout. 
--" She ~ stout. 
ex. 19 : The fish has ~ bad. 
+ The fish has become bad. 
ex. 20 : Q)lle suddenly became we:tlthy. 
+ lie suddenly became wealthy. 
ex. 21 : She became impatient. 
- " She went impatient. 
ex. 22 ; (.)l ie became a priest. 
- "  l ie became a priest. 
Fig.6.2 l+.xmnples and Results of F+xperiment 2 
.367 
ex. 01: I ~ an invitation. 
- "  I allow an invitation. 
ex. 02 : 1 ~ an offer. 
- "  I .receive an offer. 
ex. 03 : I acknowledge a defeat. 
I acceot a defeat. 
ex. 04 : I acknowleclg? his fight. 
-+ I ~ his right. 
ex. 05 : I acknowledge the truth of an argument. 
-.- 1 ~ the truth of an argument. 
ex. 06 : I admit a claim. 
- "  I allow a claim. 
ex. 07 : I admit defeat. 
--" I acknowledge defeat. 
ex. 08 : I admit my guilt. 
-~ 1 acknowledge my guilt. 
ex. 09 : I will admit no objection. 
- "  I will ~ no objection. 
ex. 10 : I allow a claim. 
1 ~ a claim. 
ex. 11 : I allow your ,argument. 
I confirm your argument. 
ex. 12 : I answer his wish. 
- "  I receive his wish. 
ex. 13 : I ~ e  a bill. 
- "  I acceot a bi l l  
ex. 14 : I ap_prove a resolution. 
- ,  1 confirm a resolution. 
ex. 15 : 1 ,approve accounts. 
- "  I ~ accounts. 
ex. 16 : Q)I confirm a treaty. 
- "  I confirm a treaty. 
ex. 17 : Q)I confirm an appointment. 
- "  I confirm an appointment. 
ex. 18 : 1 .confirm a verbal promise. 
-~- I a_~rove a verbal promise. 
ex. 19 : I confirm a telegraphic order. 
-~- I answer a telegraphic order. 
ex. 20 : I confirm possession to him. 
- "  I ~,cknowledge possession to him. 
ex. 21 : 1 confirm a functionary in his new office. 
-~ 1 ~ a  functionary in his new office. 
ex. 22 : Q")I endorse his opinion. 
I endorse his opinion. 
ex. 23 : O I  endorse a policy. 
--~ I endorse a policy. 
ex. 24 : I ~ a  request. 
-~ I acknowledge a request. 
ex. 25 : The king granted the old woman her wish. 
- "  The king answered the old woman her wish. 
ex. 26 : Japan receive a treaty. 
- "  Japan ratifies a treaty. 
ex. 27 : QParl iament ratified the agreement. 
--, Parliament ratified the agreement. 
ex. 28 : I receive a proposal. 
--+ I ~ a  proposal. 
ex. 29 : I receive an offer. 
-~ I accepK_an offer. 
ex. 30 : I receive a petition. 
--" I answer a petition. 
ex. 31 : Q)Priest receives his confession. 
- "  Priest receives his confession. 
ex. 32 : Priest receives his oath. 
'- Priest ratifies his oath. 
ex. 33 : I recognize a claim as justified. 
--~ I allow a claim as justified. 
Fig.6.3 Examples and Results of Experiment 3 
ex. 34 : Japan recognizes the independence 
of a new state. 
Japan acknowledges the independence of ... 
ex. 35 : He ~qu ick ly  to the appeal 
for subscriptions. 
- "  He ~qu ick ly  to the appeal for ... 
Fig.6.3 Examples and Results of Experiment 3 (remainder) 
Reference 
Cui, J., Komatsu, E. and Yasnhara, tl. (1993). A Calculation 
of Similarity between Words Using EDR Electronic 
Dictionary. Reprint of ll'SJ, Vol.93, No.1 (in Japanese) 
EDR (1993a). EDR Electronic Dictionary Specification 
Guide.TR.04 l.
EDR (1993b). English Word Concept Dictionary. TR-026 
Komatsu, E., Cni, J. and Yasuhara, II. (1993). A Mono-lin- 
gual Corpus-Based Machine Translation of the Interlingua 
Method. Fifth International Conference on Theoretical and 
Methodological Issues 
Nagao, M. (1984). A Framework of A Mechanical Transla- 
tion between Japmlese and English by Analogy Principle. 
Artificial and Human Intelligence (A. Elithorn and R. 
Banerji, editors) Elsevier Science Publishers, B.V. 
Sadler, V. (1989). Working with Analogical Semantics, Dis- 
ambiguation Tect, niques in DLT, Foris Publ icat ions,  
Dordrecht Holland. 
Sato, S. (1991). Example-Based Translation Approach. 
Proc. (g'International Workshop on Fundamental Research 
for the 
Future Generation of Natural Language Processing, ATR 
Interpreting Telephony Research Laboratories, pp. 1-16. 
Sumita, g. and Iida, 11. (1992). Example-Based Transfer of 
Japanese Adnominal Particles into English. IEICE TRANS. 
INF. &SYST., VOL. E75-D, NO.4 
368 
