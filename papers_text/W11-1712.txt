Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 96?103,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Feature Selection for Sentiment Analysis
Based on Content and Syntax Models
Adnan Duric and Fei Song
School of Computer Science, University of Guelph, 50 Stone Road East,
Guelph, Ontario, N1G 2W1, Canada
{aduric,fsong}@uoguelph.ca
Abstract
Recent solutions for sentiment analysis have
relied on feature selection methods ranging
from lexicon-based approaches where the set
of features are generated by humans, to ap-
proaches that use general statistical measures
where features are selected solely on empiri-
cal evidence. The advantage of statistical ap-
proaches is that they are fully automatic, how-
ever, they often fail to separate features that
carry sentiment from those that do not. In this
paper we propose a set of new feature selec-
tion schemes that use a Content and Syntax
model to automatically learn a set of features
in a review document by separating the enti-
ties that are being reviewed from the subjec-
tive expressions that describe those entities in
terms of polarities. By focusing only on the
subjective expressions and ignoring the enti-
ties, we can choose more salient features for
document-level sentiment analysis. The re-
sults obtained from using these features in a
maximum entropy classifier are competitive
with the state-of-the-art machine learning ap-
proaches.
1 Introduction
As user generated data become more commonplace,
we seek to find better approaches to extract and clas-
sify relevant content automatically. This gives users
a richer, more informative, and more appropriate set
of information in an efficient and organized manner.
One way for organizing such data is text classifica-
tion, which involves mapping documents into topi-
cal categories based on the occurrences of particular
features. Sentiment Analysis (SA) can be framed as
a text classification task where the categories are po-
larities such as positive and negative. However, the
similarities end here. Whereas general text classi-
fication is concerned with features that distinguish
different topics, sentiment analysis deals with fea-
tures about subjectivity, affect, emotion, and points-
of-view that describe or modify the related entities.
Since user-generated review documents contain both
kinds of features, SA solutions ultimately face the
challenge of separating the factual content from the
subjective content describing it.
For example, taking a segment from a randomly
chosen document in Pang et al?s movie review cor-
pus1, we see how entities and modifiers are related
to each other:
... Of course, it helps that Kaye has an
actor as talented as Norton to play this
part. It?s astonishing how frightening
Norton looks with a shaved head and a
swastika on his chest. ... Visually, the film
is very powerful. Kaye indulges in a lot of
interesting artistic choices, and most of
them work nicely.
Indeed, most of the information about an entity
that relates it to a particular polarity comes from the
modifying words. In the example above, these words
are adjectives such as talented, frightening, interest-
ing, and powerful. They can also be verbs such as
work and adverbs such as nicely. The entities are
1http://www.cs.cornell.edu/people/pabo/movie-review-
data/
96
represented by various nouns and pronouns such as:
Kaye, Norton, actor and them.
Therefore, the task of classifying a review doc-
ument can be explored by taking into account a
mixture of entities and their modifiers. An impor-
tant characteristic of review documents is that the
reviewers tend to discuss the whole set of entities
throughout the entire document, whereas the modi-
fiers for those entities tend to be more localized at
the sentence or phrase level. In other words, each
entity can be polymorphous within the document,
with a long-range semantic relationship between its
forms while the modifiers in each case are bound
to the entity in a short-range, syntactic relationship.
Generalizing a single entity to all the entities that are
found in a document, and taking all their respective
modifiers into account, we can start to infer the po-
larity of the entire document based on the set of all
the modifiers. This reduces to finding all the syn-
tactic words in the document and disregarding the
entities.
Taking another look at the example modifiers, we
might assume that all of the relevant indicators for
SA come from specific parts of speech categories
such as adjectives and adverbs, while other parts
of speech classes such as nouns are more relevant
for general text classification, and can be discarded.
However, as demonstrated by Pang et al (2002),
Pang and Lee (2004), Hu and Liu (2004), and Riloff
et al (2003), there are some nouns and verbs that
are useful sentiment indicators as well. Therefore,
a clear distinction cannot be made along parts of
speech categories.
To address this issue, we propose a feature selec-
tion scheme in which we can obtain important senti-
ment indicators that:
1. Do not rely on specific parts of speech classes
while maintaining the focus on syntax words.
2. Separate semantic words that do not indicate
sentiment while keeping nouns that do.
3. Reflect the domain for the set of documents.
By using feature selection schemes that focus on
the outlined sentiment indicators as a basis for our
machine learning approach, we should achieve com-
petitive accuracy results when classifying document
polarities.
The rest of this paper is organized as follows. Sec-
tion 2 discusses some important work and results
for SA and outlines the modelling and classification
techniques used by our approach. Section 3 provides
details about our feature selection methods. Our ex-
periments and analyses are given in section 4, and
conclusions and future directions are presented in
section 5.
2 Related Work
2.1 Feature Selection in Sentiment Analysis
The majority of the approaches for SA involve a
two-step process:
1. Identify the parts of the document that will
likely contribute to positive or negative senti-
ments.
2. Combine these parts of the document in ways
that increase the odds of the document falling
into one of these two polar categories.
The simplest approach for (1) by Pang et al
(2002) is to use the most frequently-occurring words
in the corpus as polarity indicators. This approach
is commonly used with general text classification,
and the results achieved indicate that simple docu-
ment frequency cutoffs can be an effective feature
selection scheme. However, this scheme picks up
on many entity words that do not contain any sub-
jectivity.
The most common approach, used by researchers
such as Das and Chen (2007), starts with a manu-
ally created lexicon specific to their particular do-
main whereas others (Hurst and Nigam, 2004; Yi et
al., 2003) attempt to craft a general-purpose opin-
ion lexicon that can be used across domains. More
recent lexicon-based approaches (Ding et al, 2008;
Hu and Liu, 2004; Kim and Hovy, 2004; Riloff et
al., 2003) begin with a small set of ?seed? words
and bootstrap this set through synonym detection
or various on-line resources to obtain a larger lex-
icon. However, lexicon-based approaches have sev-
eral key difficulties. First, they take time to com-
pile. Whitelaw et al (2005) report that their feature
selection process took 20 person-hours, since it in-
volves work done by human annotators. In separate
qualitative experiments done by Pang et al (2002),
97
Wilson et al (2005) and Kim and Hovy (2004), the
agreement between human judges when given a list
of sentiment-bearing words is as low as 58% and no
higher than 76%. In addition, some words may not
be frequent enough for a classification algorithm.
2.2 Topic Modelling and HMM-LDA
Topic models such as Latent Dirichlet Allocation
(LDA) are generative models that allow documents
to be explained by a set of unobserved (latent) top-
ics. Hidden Markov Model LDA (HMM-LDA)
(Griffiths et al, 2005) is a topic model that simul-
taneously models topics and syntactic structure in a
collection of documents. The idea behind the model
is that a typical word can play different roles. It can
either be part of the content and serve in a seman-
tic (topical) purpose or it can be used as part of the
grammatical (syntactic) structure. It can also be used
in both contexts. HMM-LDA models this behavior
by inducing syntactic classes for each word based
on how they appear together in a sentence using a
Hidden Markov Model. Each word gets assigned to
a syntactic class, but one class is reserved for the se-
mantic words. Words in this class behave as they
would in a regular LDA topic model, participating
in different topics and having certain probabilities of
appearing in a document. More formally, the model
is defined in terms of three sets of variables and a
generative process. Let w = {w1, ..., wn} be a se-
quence of words where each word wi is one of V
words; z = {z1, ..., zn}, a sequence of topic as-
signments where each zi is one of K topics; and
c = {c1, ..., cn}, a sequence of class assignments
where each ci is one of C classes. One class, ci = 1
is designated as the ?semantic class?, and the rest,
the ?syntactic? classes.
Since we are dealing with a Hidden Markov
Model, we require a variable representing the tran-
sition probabilities between the classes, given by a
C ? C transition matrix ? that models transitions
between classes ci?1 and ci. The generative process
is described as follows:
1. Sample ?(d) from a Dirichlet prior Dir(?)
2. For each word wi in document d:
(a) Draw zi ? ?(d)
(b) Draw ci ? ?(ci?1)
(c) If ci = 1, then draw wi ? ?(zi), else draw
wi ? ?(ci)
where ?(zi) ? Dir(?) and ?(ci) ? Dir(?), both
from Dirichlet distributions.
2.3 Text Classification Based on Maximum
Entropy Modelling
Maximum Entropy Modelling (Manning and
Schu?tze, 1999) is a framework whereby the features
represent constraints on the overall model and the
idea is to incorporate the knowledge that we have
while preserving as much uncertainty as possible
about the knowledge we do not have. The features
fi are binary functions where there is a vector x
representing input elements (unigram features in our
case) and c, the class label for one of the possible
categories. More specifically, a feature function is
defined as follows:
fi,c?(x, c) =
{
1 if x contains wi and c = c?
0 otherwise
(2.1)
where word wi and category c? correspond to a spe-
cific feature.
Employing the feature functions described above,
a Maximum Entropy model takes the following
form:
P (x, c) = 1Z
K
?
i=1
?fi(x,c)i (2.2)
where K is the number of features, ?i is the weight
for feature fi, and Z is a normalizing constant. By
taking the logarithm on both sides, we get the log-
linear model:
logP (x, c) = ? logZ +
K
?
i=1
fi(x, c) log?i (2.3)
To classify a document, we compute P (c|x) so
that the c with the highest probability will be the cat-
egory for the given document.
98
3 Feature Selection (FS) Based on
HMM-LDA
3.1 Characteristics of Salient Features
To motivate our approach, we first describe criteria
that are useful in selecting salient features for SA:
1. Features should be expressive enough to add
useful information to the classification process.
As discussed in section 1, the most expressive
features in terms of polarity are the modifying
words that describe an entity in a certain way.
These are usually, but not restricted to, adjec-
tives, adverbs, subjective verbs and nouns.
2. All features together should form a broad and
comprehensive viewpoint of the entire corpus.
In a corpus of many documents, some features
can represent a subset of the corpus very accu-
rately, while other features may represent an-
other subset of the corpus. The problem arises
when representing the whole corpus with a spe-
cific feature set (Sebastiani, 2002).
3. Features should be as domain-dependent as
possible. Examples from Hurst and Nigam
(2004) and Das and Chen (2007) as well as
many other approaches indicate that SA is a
domain-dependant task, and the final features
should reflect the domain of the corpus that
they are representing.
4. Features must be frequent enough. Rare fea-
tures do not occur in many documents and
make it difficult to train a machine learning al-
gorithm. Experiments by Pang et al (2002) in-
dicate that having more features does not help
learning, and the best accuracy was achieved
by selecting features based on document fre-
quency.
5. Features should be discriminative enough. A
learning system needs to be able to pick up on
their presence in certain documents for one out-
come and absence in other documents for an-
other outcome in classification.
3.2 FS Based on Syntactic Classes
Our proposed FS scheme is to utilize HMM-LDA
to obtain words that, for the most part, follow the
criteria we set out in subsection 3.1. We train an
HMM-LDA model to give us the syntactic classes
that we further combine to form our final features.
Let word wi ? V where V is the vocabulary. Also
let cj ? C be a class. We define Pcj (wi) as the prob-
ability of word wi in class cj , and one class, cj = 1
indicates the semantic class. Since each class (syn-
tactic and semantic) has a probability distribution
over all words, we need to select words that offer
a good representation of the class. The representa-
tive words in each class have a much higher proba-
bility than the other words. Therefore, we can select
the representative words by the cumulative probabil-
ity. Specifically, we select the top percentage of the
words in a class whereby the sum of their probabil-
ities will be within some pre-defined range. This is
necessary since there are many words in each class
with low probabilities in which we are not interested
(Steyvers and Griffiths, 2006). The cumulative dis-
tribution function is defined as:
Fj(wi) =
?
Pcj (w)?Pcj (wi)
Pcj (w) (3.1)
Then, we can define the set of words in class cj as:
Wcj = {wi|Fj(wi) ? ?} (3.2)
where ? is a pre-defined threshold such that 0 ? ? ?
1. Next, we define the set of words in all the syntac-
tic classes Wsyn as:
Wsyn = {wi|wi ? Wcj and cj 6= 1} (3.3)
and the set of words in the semantic class Wsem as:
Wsem = {wi|wi ? Wcj and cj = 1} (3.4)
Since modifying words for sentiment typically
fall into syntactic classes, we could use words in
Wsyn as features for SA. However, as observed by
Pang et al (2002), the best classification perfor-
mance is achieved by a subset of features (typically
around 2500). As a general step, we can apply a
document frequency (DF) cutoff to select the most
frequent features. Let df(wi) denote the document
frequency of word wi, indicating the number of doc-
uments in which wi occurs in the corpus. Then the
99
resulting features selected based on df can be de-
fined as:
cut(Wsyn, ?) = {wi|wi ? Wsyn and df(wi) ? ?}
(3.5)
where ? is the minimum document frequency re-
quired for feature selection.
3.3 FS Based on Set Difference between
Syntactic and Semantic Classes
The main characteristic of using HMM-LDA classes
for feature selection is that the set of words in the
syntactic classes and the set of words in the semantic
class are not disjoint. In fact, there is quite a large
overlap. In this and the next subsections, we dis-
cuss ways to remedy and even exploit this situation
to get a higher level of accuracy. In the Pang et al
movie review data, there is about 35% overlap be-
tween words in the syntactic and semantic classes
for ? = 0.9. Our first systematic approach attempts
to gain better accuracy by lowering the ratio of se-
mantic words in the final feature set.
More formally, given the set of syntactic words
Wsyn, we can reduce the overlap with Wsem by do-
ing a set difference operation:
Wsyn ?Wsem (3.6)
This will give us all the words that are more
favoured in the syntactic classes. However, as we
shall see shortly, and also as we earlier speculated,
by subtracting all the words in the semantic class, we
are actually getting rid of some useful features. This
is because (a) it is possible for the semantic class
to contain words that are syntactic, and as a result
are useful, and (b) there exist some semantic words
that are good indicators of polarity. Therefore, we
seek to ?lessen? the influence of the semantic class
by cutting only a certain portion of it out, but not all
of them.
For the above scheme, we outline Algorithm 1
that enables us to select features from Wsyn by ap-
plying a percentage cutoff for Wsem and then doing
a set difference operation. We define top(Wsem, ?)
to be the ?% of the words with top probabilities in
Wsem.
Note that when ? = 1.0, we get the same result as
Wsyn ? Wsem. In our experiments, we try a range
of ? values for SA.
Algorithm 1 Syntactic-Semantic Set Difference
Require: Wsyn and Wsem as input
1: W ?sem = top(Wsem, ?)
2: Wdiff = Wsyn ?W ?sem
3: W ?syn = cut(Wdiff , ?)
3.4 FS Based on Max Scores of Syntactic
Features
The running theme through the HMM-LDA feature
selection schemes is that if a word is highly ranked
(has a high probability of occurring) in a syntactic
class, we should use that word in our feature set.
Moreover, if a word is highly ranked in the seman-
tic class, we usually do not want to use that word
in our feature set because the word usually indicates
a frequent noun. Therefore, the desirable words are
those that occur with high probability in the syntac-
tic classes, but do not occur with high probability in
the semantic class, or do not occur there at all.
To this end, we have formulated a scheme that
adds such words to our feature set. For each word,
we obtain its highest probability in the set of syn-
tactic classes. Comparing this probability with the
probability of the same word in the semantic class,
we disregard the word if the probability in the se-
mantic class is greater.
We define the max scores for word wi for both the
syntactic and semantic classes and describe how we
select features based on the max scores in Algorithm
2.
Algorithm 2 Max Scores of Syntactic Features
Require: cj ? C where 1 ? j ? |C|
1: for all wi ? V do
2: Ssyn(wi) = maxcj 6=1Pcj (wi)
3: Ssem(wi) = Pc1(wi)
4: Wmax = {wi|Ssyn(wi) > Ssem(wi)}
5: end for
6: W ?syn = cut(Wmax, ?)
4 Experiments
This section describes the steps taken to gener-
ate some experimental results for each scheme de-
scribed in the previous section. Before we can an-
alyze these sets of results, we take a look at some
100
baselines.
4.1 Evaluation
We use the corpus of 2000 movie reviews (Pang and
Lee, 2004) that consists of 1000 positive and 1000
negative documents selected from on-line forums.
In our experiments, we randomize the documents
and split the data into 1800 for training / testing pur-
poses and 200 as the validation set. For the 1800
documents, we run a 3-fold cross validation proce-
dure where we train on 1200 documents and test on
600. We compare the resultant feature sets after each
FS scheme using the OpenNLP2 Maximum Entropy
classifier.
Throughout these experiments, we are interested
in the classification accuracy. This is evaluated
simply by comparing the resultant class from the
classifier and the actual class annotated by Pang
and Lee (2004). The number of matches is di-
vided by the number of documents in the test
set. Thus, given an annotated test set dtestA =
{(d1, o1), (d2, o2), . . . (dS , oS)} and the classified
set, dtestB = {(d1, q1), (d2, q2), . . . (dS , qS)}, we
calculate the accuracy as follows:
?S
i=1 I(oi = qi)
S (4.1)
where I(?) is the indicator function.
4.2 Baseline Results
After replicating the results from Pang et al (2002),
we varied the number of iterations per fold by using
a held-out validation set ?eval?. The higher accu-
racy achieved suggests that the model was not fully
trained after 10 iterations.
In order to compare with our HMM-LDA based
schemes, we ran experiments to explore a basic
POS-based feature selection scheme. In this ap-
proach, we first tagged the words in each document
with POS tags and selected the most frequently-
occurring unigrams that were not tagged as ?NN?,
?NNP?, ?NNS? or ?NNPS? (the ?noun? categories).
This corresponds to POS (-NN*) in Table 1. Next,
we tagged all the words and only selected the words
that were tagged as ?JJ*?, ?RB*?, and ?VB*? cate-
gories (the ?syntactic? categories). The idea is to
2http://incubator.apache.org/opennlp/
include as part of the feature set al the words that
are not ?semantically oriented?. This corresponds to
POS (JJ* + RB* + VB*) in Table 1.
Iterations DF
cutoff
POS
(-NN*)
POS
(JJ*+RB*
+VB*)
10 0.821 0.827 0.811
25 0.836 0.831 0.824
eval 0.845 0.848 0.826
Table 1: Baseline results with a different number of iter-
ations. Each column represents a different feature selec-
tion method.
4.3 HMM-LDA Training
Our feature selection methods involve training an
HMM-LDA model on the Pang et al corpus of
movie reviews, taking the class assignments, and
combining the resultant unigrams to create features
for the MaxEnt classifier. Since HMM-LDA is an
unsupervised topic model, we can train it on the en-
tire corpus. We trained the model using the Topic
Modelling Toolbox3 MATLAB package on the 2000
movie reviews. Since the HMM-LDA model re-
quires sentences to be outlined, we used the usual
end-of-sentence markers (?.?, ?!?, ???, ?:?). The train-
ing parameters are T = 50 topics, S = 20 classes, AL-
PHA = 1.0, BETA = 0.01, and GAMMA = 0.1. We
found that 1000 iterations is sufficient as we tracked
the log-likelihood of every 10 iterations.
After training, we have both the topic assignments
z and the class assignments c for each word in each
of the samples.
4.4 Selecting Features Based on Syntactic
Classes
In this experiment we fix ? = 0.9 to get the top
words in each class having a cumulative probabil-
ity under 0.9. These are the representative words
in each class which we merge into Wsyn. Finally,
we select 2500 words by the df cutoff method. This
list of words is then used as features for the Max-
Ent classifier. We run the classifier for 10, 25 and
?eval? number of iterations in order to compare with
the baseline results.
3http://psiexp.ss.uci.edu/research/programs data/toolbox.htm
101
Iterations FS Based on
Syntactic Features
10 0.823
25 0.839
eval 0.863
Table 2: Results for FS Based on Syntactic Classes at 10,
25 and ?eval? iterations.
At ? = 0.9, there are 6,189 words in Wsyn before
we select the top 2500 using the df cutoff. From
Table 2, we see that the accuracy has increased from
0.845 to 0.863 at the ?eval? number iterations.
In all of our experiments, we use df cutoff to
get a manageable number of features for the clas-
sifier. This is partly based on Pang et al (2002)
and partly based on calculating the Pearson correla-
tion for each class between the document frequency
and word probability at ? = 0.9. Since every class
has a positive correlation in the range of [0.313938,
0.888160] where the average is 0.576, we can say
that there is a correlation between the two values.
4.5 Selecting Features Based on Set Difference
The result for Set Difference is derived by varying
the percentage of top semantic words that should be
excluded in the final feature set. For example, some
words in Wsyn?Wsem that have a higher probability
in Wsem are: ?hollywod?, ?war?, and ?fiction? while
some words that have a higher probability in Wsyn
include: ?good?, ?love? and ?funny?. The ? value is
defined by the percentage of the words in Wsem that
we exclude from Wsyn. The results for 0.0 ? ? ?
1.0 for increments of ??|Wsem|, are summarized in
Table 3.
? FS Based on
Set Difference
? FS Based on
Set Difference
0.0 0.861 0.5 0.852
0.1 0.862 0.6 0.846
0.2 0.865 0.7 0.849
0.3 0.858 0.8 0.847
0.4 0.857 0.9 0.840
1.0 0.831
Table 3: Results for FS Based on Syntactic-Semantic
set difference method. Each row represents the accuracy
achieved at a particular ? value.
From the results, we can see that as we remove
more and more words from Wsem, the accuracy level
decreases. This suggests that Wsem?Wsyn contains
some important features and if we subtract Wsem en-
tirely, we essentially eliminate them. At each cutoff
level, we are eliminating 10% until we have elimi-
nated the whole set. Clearly, a more fine-grained ap-
proach is needed, and that leads us to the Max-Score
results.
4.6 Selecting Features Based on Max Scores
For the method based on Max Scores, we may select
features that are in both Wsem and Wsyn sets as long
as their max scores in Wsyn are higher than those in
Wsem.
Iterations FS Based on
Max Scores
eval 0.875
Table 4: Result for FS Based on Max Scores.
Comparing the accuracy in Table 4 with those
in the previous subsections, we can say that using
the fine-grained Max-Score algorithm improves the
classification accuracy. This means that iteratively
removing words that have a relatively higher prob-
ability in Wsem compared to Wsyn does not elim-
inate important words occurring in both sets, but
lessens the influence of some high probability words
in Wsem.
4.7 Discussion of the Results
For our experiments, the best accuracy is achieved
by utilizing the Max-Score algorithm (outlined in
subsection 3.4) after a further selection of 2500 with
the df cutoff. As discussed in subsection 3.4, the
Max-Score algorithm enables us to select words that
have a higher score in Wsyn than in Wsem. This ap-
proach has the dual advantage of keeping the words
that are present in both Wsyn and Wsem but have
higher scores in Wsyn and ignoring the words that
are also present in both sets but have higher scores
in Wsem. Ultimately, this decreases the influence of
the frequent and overlapped words that have a high
probability in Wsem.
Finally, to quantify the significance level of our
best approach against the baseline methods in sub-
102
section 4.2, we calculated the p-values for the one-
tailed t-tests comparing our best approach based on
Max Scores with the DF and POS (-NN*) baselines,
respectively. The resulting p-values of 0.011 and
0.014 suggest that our best approach is significantly
better than the baseline approaches.
5 Conclusions and Future Directions
In this paper, we have described a method for fea-
ture selection based on long-range and short-range
dependencies given by the HMM-LDA topic model.
By modelling review documents based on the com-
binations of syntactic and semantic classes, we have
devised a method of separating the topical con-
tent that describes the entities under review from
the opinion context (given by sentiment modifiers)
about that entity in each case. By grouping all the
sentiment modifiers for each entity in a document,
we are selecting the features that are intuitively in
line with the outlined characteristics of salient fea-
tures for SA (see subsection 3.1). This is backed up
by our experiments where we achieve competitive
results for document polarity classification.
One avenue for future development of this frame-
work could include identifying and extracting as-
pects from a review document. So far, we have not
identified aspects from the entities, choosing instead
to classify a document as a whole. However, this
framework can be readily applied to extract relevant
(most probable) aspects using the LDA topic model
and then restrict the syntactic modifiers to the range
of sentences where an aspect occurs. This would
give us an unsupervised aspect extraction scheme
that we can combine with a classifier to predict po-
larities for each aspect.
References
Sanjiv R. Das and Mike Y. Chen. 2007. Yahoo! for
Amazon: Sentiment extraction from small talk on the
Web. Management Science, 53(9):1375?1388.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining. In
Proceedings of the Conference on Web Search and Web
Data Mining (WSDM).
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In In Advances in Neural Information Pro-
cessing Systems 17, pages 537?544. MIT Press.
Minqing Hu and Bing Liu. 2004. Mining opinion fea-
tures in customer reviews. In Proceedings of AAAI,
pages 755?760.
Matthew Hurst and Kamal Nigam. 2004. Retrieving top-
ical sentiments from online document collections. In
Document Recognition and Retrieval XI, pages 27?34.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of the In-
ternational Conference on Computational Linguistics
(COLING).
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of statistical natural language process-
ing. MIT Press, Cambridge, MA, USA.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the As-
sociation for Computational Linguistics (ACL), pages
271?278.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 79?86.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the Conference on
Natural Language Learning (CoNLL), pages 25?32.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Computing Surveys,
34(1):1?47.
Mark Steyvers and Tom Griffiths. 2006. Probabilistic
topic models. In T. Landauer, D. Mcnamara, S. Den-
nis, and W. Kintsch, editors, Latent Semantic Analysis:
A Road to Meaning. Laurence Erlbaum.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analy-
sis. In Proceedings of the ACM SIGIR Conference
on Information and Knowledge Management (CIKM),
pages 625?631. ACM.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the Human Lan-
guage Technology Conference and the Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 347?354.
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and
Wayne Niblack. 2003. Sentiment analyzer: Ex-
tracting sentiments about a given topic using natural
language processing techniques. In Proceedings of
the IEEE International Conference on Data Mining
(ICDM).
103
