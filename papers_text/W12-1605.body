Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 40?49,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Unsupervised Topic Modeling Approaches to Decision Summarization in
Spoken Meetings
Lu Wang
Department of Computer Science
Cornell University
Ithaca, NY 14853
luwang@cs.cornell.edu
Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
cardie@cs.cornell.edu
Abstract
We present a token-level decision summariza-
tion framework that utilizes the latent topic
structures of utterances to identify ?summary-
worthy? words. Concretely, a series of
unsupervised topic models is explored and
experimental results show that fine-grained
topic models, which discover topics at the
utterance-level rather than the document-level,
can better identify the gist of the decision-
making process. Moreover, our proposed
token-level summarization approach, which
is able to remove redundancies within utter-
ances, outperforms existing utterance ranking
based summarization methods. Finally, con-
text information is also investigated to add ad-
ditional relevant information to the summary.
1 Introduction
Meetings are an important way for information shar-
ing and collaboration, where people can discuss
problems and make concrete decisions. Not sur-
prisingly, there is an increasing interest in develop-
ing methods for extractive summarization for meet-
ings and conversations (Zechner, 2002; Maskey and
Hirschberg, 2005; Galley, 2006; Lin and Chen,
2010; Murray et al., 2010a). Carenini et al. (2011)
describe the specific need for focused summaries of
meetings, i.e., summaries of a particular aspect of a
meeting rather than of the meeting as a whole. For
example, the decisions made, the action items that
emerged and the problems arised are all important
outcomes of meetings. In particular, decision sum-
maries would allow participants to review decisions
from previous meetings and understand the related
topics quickly, which facilitates preparation for the
upcoming meetings.
A:We decided our target group is the focus on who can
afford it , (1)
B:Uh I?m kinda liking the idea of latex , if if spongy is
the in thing . (2)
B:what I?ve seen , just not related to this , but of latex
cases before , is that [vocalsound] there?s uh like a hard
plastic inside , and it?s just covered with the latex . (2)
C:Um [disfmarker] And I think if we wanna keep our costs
down , we should just go for pushbuttons , (3)
D:but if it?s gonna be in a latex type thing and that?s
gonna look cool , then that?s probably gonna have a
bigger impact than the scroll wheel . (2)
A:we?re gonna go with um type pushbuttons , (3)
A:So we?re gonna have like a menu button , (4)
C:uh volume , favourite channels , uh and menu . (4)
A:Pre-set channels (4)
Decision Abstracts (Summary)
DECISION 1: The target group comprises of individuals
who can afford the product.
DECISION 2: The remote will have a latex case.
DECISION 3: The remote will have pushbuttons.
DECISION 4: The remote will have a power button, volume
buttons, channel preset buttons, and a menu button.
Figure 1: A clip of a meeting from the AMI meeting cor-
pus (Carletta et al., 2005). A, B, C and D refer to distinct
speakers; the numbers in parentheses indicate the associated
meeting decision: DECISION 1, 2, 3 or 4. Also shown is the
gold-standard (manual) abstract (summary) for each decision.
Meeting conversation is intrinsically different
from well-written text, as meetings may not be well
organized and most utterances have low density of
salient content. Therefore, multiple problems need
to be addressed for speech summarization. Consider
the sample dialogue snippet in Figure 1 from the
AMI meeting corpus (Carletta et al., 2005). Only
decision-related dialogue acts (DRDAs) ? utter-
40
ances at least one decision made in the meeting1 ?
are listed and ordered by time. Each DRDA is la-
beled numerically according to the decision it sup-
ports; so the second and third utterances (in bold)
support DECISION 2, as do the fifth utterance in the
snippet. Manually constructed decision abstracts for
each decision are shown at the bottom of the figure.
Besides the prevalent dialogue phenomena (such
as ?Uh I?m kinda liking? in Figure 1), disfluencies
and off-topic expressions, we notice that single ut-
terance is usually not informative enough to form
a decision. For instance, no single DRDA associ-
ated with DECISION 4 corresponds all that well with
its decision abstract: ?pushbuttons?, ?menu button?
and ?Pre-set channels? are mentioned in separate
DAs. As a result, extractive summarization methods
that select individual utterance to form the summary
will perform poorly.
Furthermore, it is difficult to identify the core
topic when multiple topics are discussed in one ut-
terance. For example, all of the bold DRDAs sup-
porting DECISION 2 contain the word ?latex?. How-
ever, the last DA in bold also mentions ?bigger im-
pact? and ?the scroll wheel?, which are not specifi-
cally relevant for DECISION 2. Though this problem
can be approached by training a classifier to identify
the relevant phrases and ignore the irrelevant ones
or dialogue phenomena, it needs expensive human
annotation and is limited to the specific domain.
Note also that for DECISION 4, the ?power but-
ton? is not specified in any of the listed DRDAs
supporting it. By looking at the transcript, we find
?power button? mentioned in one of the preceding,
but not decision-related DAs. Consequently another
challenge would be to add complementary knowl-
edge when the DRDAs cannot provide complete in-
formation.
Therefore, we need a summarization approach
that is tolerant of dialogue phenomena, can deter-
mine the key semantic content and is easily trans-
ferable between domains. Recently, topic model-
ing approaches have been investigated and achieved
state-of-the-art results in multi-document summa-
rization (Haghighi and Vanderwende, 2009; Celiky-
1These DRDAs are annotated in the AMI corpus and usually
contain the decision content. They are similar, but not com-
pletely equivalent, to the decision dialogue acts (DDAs) of Bui
et al. (2009), Ferna?ndez et al. (2008), Frampton et al. (2009).
ilmaz and Hakkani-Tur, 2010). Thus, topic mod-
els appear to be a better ref for document simi-
larity w.r.t. semantic concepts than simple literal
word matching. However, very little work has in-
vestigated its role in spoken document summariza-
tion (Chen and Chen, 2008; Hazen, 2011), and much
less conducted comparisons among topic modeling
approaches for focused summarization in meetings.
In contrast to previous work, we study the un-
supervised token-level decision summarization in
meetings by identifying a concise set of key words
or phrases, which can either be output as a com-
pact summary or be a starting point to generate ab-
stractive summaries. This paper addresses problems
mentioned above and make contributions as follows:
? As a step towards creating the abstractive sum-
maries that people prefer when dealing with spo-
ken language (Murray et al., 2010b), we propose a
token-level rather than sentence-level framework
for identifying components of the summary. Ex-
perimental results show that, compared to the sen-
tence ranking based summarization algorithms,
our token-level summarization framework can bet-
ter identify the summary-worthy words and re-
move the redundancies.
? Rather than employing supervised learning meth-
ods that rely on costly manual annotation, we ex-
plore and evaluate topic modeling approaches of
different granularities for the unsupervised deci-
sion summarization at both the token-level and di-
alogue act-level. We investigate three topic mod-
els ? Local LDA (LocalLDA) (Brody and El-
hadad, 2010), Multi-grain LDA (MG-LDA) (Titov
and McDonald, 2008) and Segmented Topic
Model (STM) (Du et al., 2010) ? which can uti-
lize the latent topic structure on utterance level
instead of document level. Under our proposed
token-level summarization framework, three fine-
grained models outperform the basic LDA model
and two extractive baselines that select the longest
and the most representative utterance for each de-
cision, respectively. (ROUGE-SU4 F score of
14.82% for STM vs. 13.58% and 13.46% for
the baselines, given the perfect clusterings of DR-
DAs.)
? In line with prior research that explore the role of
context for utterance-based extractive summariza-
41
tion (Murray and Renals, 2007), we investigate the
role of context in our token-level summarization
framework. For the given clusters of DRDAs, We
study two types of context information ? the DAs
preceding and succeeding a DRDA and DAs of
high TF-IDF similarity with a DRDA. We also in-
vestigate two ways to select relevant words from
the context DA. Experimental results show that
two types of context have comparable effect, but
selecting words from the dominant topic of the
center DRDA performs better than from the dom-
inant topic of the context DA. Moreover, by lever-
aging context, the recall exceeds the provided up-
perbound?s recall (ROUGE-1 recall: 48.10% vs.
45.05% for upperbound by using DRDA only) al-
though the F scores decrease after adding context
information. Finally, we show that when the true
DRDA clusterings are not available, adding con-
text can improve both the recall and F score.
2 Related Work
Speech and dialogue summarization has become im-
portant in recent years as the number of multime-
dia resources containing speech has grown. A pri-
mary goal for most speech summarization systems
is to account for the special characteristics of di-
alogue. Early work in this area investigated su-
pervised learning methods, including maximum en-
tropy, conditional random fields (CRFs), and sup-
port vector machines (SVMs) (Buist et al., 2004;
Galley, 2006; Xie et al., 2008). For unsupervised
methods, maximal marginal relevance (MMR) is in-
vestigated in (Zechner, 2002) and (Xie and Liu,
2010). Gillick et al. (2009) introduce a concept-
based global optimization framework by using in-
teger linear programming (ILP).
Only in very recent works has decision sum-
marization been addressed in (Ferna?ndez et al.,
2008), (Bui et al., 2009) and (Wang and Cardie,
2011). (Ferna?ndez et al., 2008) and (Bui et al., 2009)
utilize semantic parser to identify candidate phrases
for decision summaries and employ SVM to rank
those phrases. They also train HMM and SVM
directly on a set of decision-related dialogue acts
on token level and use the classifiers to identify
summary-worthy words. Wang and Cardie (2011)
provide an exploration on supervised and unsuper-
vised learning for decision summarization on both
utterance- and token- level.
Our work also arises out of applying topic mod-
els to text summarization (Bhandari et al., 2008;
Haghighi and Vanderwende, 2009; Celikyilmaz and
Hakkani-Tur, 2010; Celikyilmaz and Hakkani-Tur,
2010). Mostly, the sentences are ranked according to
importance based on latent topic structures, and top
ones are selected as the summary. There are some
works for applying document-level topic models to
speech summarization (Kong and shan Leek, 2006;
Chen and Chen, 2008; Hazen, 2011). Different from
their work, we further investigate the topic models of
fine granularity on sentence level and leverage con-
text information for decision summarization task.
Most existing approaches for speech summariza-
tion result in a selection of utterances from the dia-
logue, which cannot remove the redundancy within
utterances. To eliminate the superfluous words, our
work is also inspired by keyphrase extraction of
meetings (Liu et al., 2009; Liu et al., 2011) and
keyphrase based summarization (Riedhammer et al.,
2010). However, a small set of keyphrases are not
enough to concretely display the content. Instead of
only picking up keyphrases, our work identifies all
of the summary-worthy words and phrases, and re-
moves redundancies within utterances.
3 Summarization Frameworks
In this section, we first present our proposed token-
level decision summarization framework ? Dom-
Sum ? which utilizes latent topic structure in ut-
terances to extract words from Dominant Topic (see
details in Section 3.1) to form Summaries. In Sec-
tion 3.2, we describe four existing sentence scor-
ing metrics denoted as OneTopic, MultiTopic, TMM-
Sum and KLSum which are also based on latent topic
distributions. We adopt them to the utterance-level
summarization for comparison in Section 6.
3.1 Token-level Summarization Framework
Domsum takes as input the clusters of DRDAs (with
or without additional context DAs), the topic distri-
bution for each DA and the word distribution for
each topic. The output is a set of topic-coherent
summary-worthy words which can be used directly
as the summary or to further generate abstractive
summary. We introduce DomSum in two steps ac-
cording to its input: taking clusters of DRDAs as the
input and with additional context information.
42
DRDAs Only. Given clusters of DRDAs, we use
Algorithm 1 to produce the token-level summary for
each cluster. Generally, Algorithm 1 chooses the
topic with the highest probability as the dominant
topic given the dialogue act (DA). Then it collects
the words with a high joint probability with the dom-
inant topic from that DA.
Input : Cluster C = {DAi}, P (Tj |DAi), P (wk|Tj)
Output: Summary
Summary? ? (empty set)
foreach DAi in C do
DomTopic? maxTj P (Tj |DAi) (*)Candidate? ?
foreach word wk inDAi do
SampleTopic? maxTj P (wk|Tj)P (Tj |DAi)
if DomTopic == SampleTopic then
Candidate? Union(Candidate, wk)
end
end
Summary? Union(Summary, Candidate)
end
Algorithm 1: DomSum ? The token-level sum-
marization framework. DomSum takes as input the
clusters of DRDAs and related probability distribu-
tions.
Leveraging Context. For each DRDA (denoted as
?center DA?), we study two types of context infor-
mation (denoted as ?context DAs?). One is adjacent
DAs, i.e., immediately preceding and succeeding
DAs, the other is the DAs having top TF-IDF simi-
larities with the center DA. Context DAs are added
into the cluster the corresponding center DA in.
We also study two criteria of word selection from
the context DAs. For each context DA, we can take
the words appearing in the dominant topic of ei-
ther this context DA or its center DRDA. We will
show in Section 6.1 that the latter performs better
as it produces more topic-coherent summaries. Al-
gorithm 1 can be easily modified to leverage context
DAs by updating the input clusters and assigning the
proper dominant topic for each DA accordingly ?
this changes the step (?) in Algorithm 1.
3.2 Utterance-level Summarization Metrics
We also adopt four sentence scoring metrics based
on the latent topic structure for extractive summa-
rization. Though they are developed on different
topic models, given the desired topic distributions as
input, they can rank the utterances according to their
importance and provide utterance-level summaries
for comparison.
OneTopic and MultiTopic. In (Bhandari et al.,
2008), several sentence scoring functions are intro-
duced based on Probabilistic Latent Semantic Index-
ing. We adopt two metrics, which are OneTopic
and MultiTopic. For OneTopic, topic T with high-
est probability P (T ) is picked as the central topic
per cluster C. The score for DA in C is:
P (DA|T ) =
?
w?DA P (T |DA,w)?
DA??C,w?DA? P (T |DA?, w)
,
MultiTopic modifies OneTopic by taking all of the
topics into consideration. Given a cluster C, DA in
C is scored as:
?
T
P (DA|T )P (T ) =
?
T
?
w?DA P (T |DA,w)?
DA??C,w?DA? P (T |DA?, w)
P (T )
TMMSum. Chen and Chen (2008) propose a Top-
ical Mixture Model (TMM) for speech summariza-
tion, where each dialogue act is modeled as a TMM
for generating the document. TMM is shown to
provide better utterance-level extractive summaries
for spoken documents than other conventional unsu-
pervised approaches, such as Vector Space Model
(VSM) (Gong and Liu, 2001), Latent Semantic
Analysis (LSA) (Gong and Liu, 2001) and Max-
imum Marginal Relevance (MMR) (Murray et al.,
2005). The importance of a sentence S can be mea-
sured by its generative probability P (D|S), where
D is the document S belongs to. In our experiments,
one decision is made per cluster of DAs. So we
adopt their scoring metric to compute the generative
probability of the cluster C for each DA:
P (C|DA) =
?
wi?C
?
Tj
P (wi|Tj)P (Tj |DA),
KLSum. Kullback-Lieber (KL) divergence is ex-
plored for summarization in (Haghighi and Vander-
wende, 2009) and (Lin et al., 2010), where it is used
to measure the distance of distributions between the
document and the summary. For a cluster C of DAs,
given a length limit ?, a set of DAs S is selected as:
S? = arg min
S:|S|<?
KL(PC ||PS) = arg min
S:|S|<?
?
Ti
P (Ti|C)log
P (Ti|C)
P (Ti|S)
4 Topic Models
In this section, we briefly describe the three fine-
grained topic models employed to compute the la-
tent topic distributions on utterance level in the
43
meetings. According to the input of Algorithm 1,
we are interested in estimating the topic distribution
for each DA P (T |DA) and the word distribution
for each topic P (w|T ). For MG-LDA, P (T |DA)
is computed as the expectation of local topic distri-
butions with respect to the window distribution.
4.1 Local LDA
Local LDA (LocalLDA) (Brody and Elhadad, 2010)
uses almost the same probabilistic generative model
as Latent Dirichlet Allocation (LDA) (Blei et al.,
2003), except that it treats each sentence as a sepa-
rate document2. Each DA d is generated as follows:
1. For each topic k:
(a) Choose word distribution: ?k ? Dir(?)
2. For each DA d:
(a) Choose topic distribution: ?d ? Dir(?)
(b) For each word w in DA d:
i. Choose topic: zd,w ? ?d
ii. choose word: w ? ?zd,w
4.2 Multi-grain LDA
Multi-grain LDA (MG-LDA) (Titov and McDonald,
2008) can model both the meeting specific topics
(e.g. the design of a remote control) and various con-
crete aspects (e.g. the cost or the functionality). The
generative process is:
1. Choose a global topic distribution: ?glm ? Dir(?gl)
2. For each sliding window v of size T :
(a) Choose local topic distribution: ?locm,v ? Dir(?loc)
(b) Choose granularity mixture: pim,v ? Beta(?mix)
3. For each DA d:
(a) choose window distribution: ?m,d ? Dir(?)
4. For each word w in DA d of meeting m:
(a) Choose sliding window: vm,w ? ?m,d
(b) Choose granularity: rm,w ? pim,vm,w
(c) If rm,w = gl, choose global topic: zm,w ? ?glm
(d) If rm,w = loc, choose local topic: zm,w ? ?locm,vm,w
(e) Choose word w from the word distribution: ?rm,wzm,w
4.3 Segmented Topic Model
The last model we utilize is Segmented Topic Model
(STM) (Du et al., 2010), which jointly models
document- and sentence-level latent topics using
a two-parameter Poisson Dirichlet Process (PDP).
Given parameters ?, ?,? and PDP parameters a, b,
the generative process is:
1. Choose distribution of topics: ?m ? Dir(?)
2. For each dialogue act d:
2For the generative process of LDA, the DAs in the same
meeting make up the document, so ?each DA? is changed to
?each meeting? in LocalLDA?s generative process.
(a) Choose distribution of topics: ?d ? PDP (?m, a, b)
3. For each word w in dialogue act d:
(a) Choose topic: zm,w ? ?d
(b) Choose word: w ? ?zm,w
5 Experimental Setup
The Corpus. We evaluate our approach on the
AMI meeting corpus (Carletta et al., 2005) that con-
sists of 140 multi-party meetings. The 129 scenario-
driven meetings involve four participants playing
different roles on a design team. A short (usually
one-sentence) abstract is manually constructed to
summarize each decision discussed in the meeting
and used as gold-standard summaries in our experi-
ments.
System Inputs. Our summarization system re-
quires as input a partitioning of the DRDAs accord-
ing to the decision(s) that each supports (i.e., one
cluster of DRDAs per decision). As mentioned ear-
lier, we assume for all experiments that the DRDAs
for each meeting have been identified. For evalua-
tion we consider two system input settings. In the
True Clusterings setting, we use the AMI annota-
tions to create perfect partitionings of the DRDAs
as the input; in the System Clusterings setting, we
employ a hierarchical agglomerative clustering algo-
rithm used for this task in previous work (Wang and
Cardie, 2011). The Wang and Cardie (2011) cluster-
ing method groups DRDAs according to their LDA
topic distribution similarity. As better approaches
for DRDA clustering become available, they could
be employed instead.
Evaluation Metric. To evaluate the performance
of various summarization approaches, we use the
widely accepted ROUGE (Lin and Hovy, 2003) met-
rics. We use the stemming option of the ROUGE
software at http://berouge.com/ and remove
stopwords from both the system and gold-standard
summaries, same as Riedhammer et al. (2010) do.
Inference and Hyperparameters We use the im-
plementation from (Lu et al., 2011) for the three
topic models in Section 4. The collapsed Gibbs
Sampling approach (Griffiths and Steyvers, 2004) is
exploited for inference. Hyperparameters are cho-
sen according to (Brody and Elhadad, 2010), (Titov
and McDonald, 2008) and (Du et al., 2010). In LDA
and LocalLDA, ? and ? are both set to 0.1 . For
MG-LDA, ?gl, ?loc and ?mix are set to 0.1; ? is 0.1
44
and the window size T is 3. And the number of lo-
cal topic is set as the same number of global topic as
discussed in (Titov and McDonald, 2008). In STM,
?, a and b are set to 0.5, 0.1 and 1, respectively.
5.1 Baselines and Comparisons
We compare our token-level summarization frame-
work based on the fine-grained topic models to (1)
two unsupervised baselines, (2) token-level summa-
rization by LDA, (3) utterance-level summarization
by Topical Mixture Model (TMM) (Chen and Chen,
2008), (4) utterance-level summarization based on
the fine-grained topic models using existing metrics
(Section 3.2), (5) two supervised methods, and (6)
an upperbound derived from the AMI gold standard
decision abstracts. (1) and (6) are described below,
others will be discussed in Section 6.
The LONGEST DA Baseline. As in (Riedhammer
et al., 2010) and (Wang and Cardie, 2011), this base-
line simply selects the longest DRDA in each cluster
as the summary. Thus, it performs utterance-level
decision summarization. This baseline and the next
allow us to determine summary quality when sum-
maries are restricted to a single utterance.
The PROTOTYPE DA Baseline. Following Wang
and Cardie (2011), the second baseline selects the
decision cluster prototype (i.e., the DRDA with the
largest TF-IDF similarity with the cluster centroid)
as the summary.
Upperbound. We also compute an upperbound
that reflects the gap between the best possible ex-
tractive summaries and the human-written abstracts
according to the ROUGE score: for each cluster of
DRDAs, we select the words that also appear in the
associated decision abstract.
6 Results and Discussion
6.1 True Clusterings
How do fine-grained topic models compare to ba-
sic topic models or baselines? Figure 2 demon-
strates that by using the DomSum token-level sum-
marization framework, the three fine-grained topic
models uniformly outperform the two non-trivial
baselines and TMM (Chen and Chen, 2008) (reim-
plemented by us) that generates utterance-level sum-
maries. Moreover, the fine-grained models also beat
basic LDA under the same DomSum token-level
summarization framework. This shows the fine-
2 3 4 5 6 7 8 9 106
7
8
9
10
11
12
13
14
15
#Topics
ROU
GE?S
U4 F
 (%)
Comparison with Baselines, TMM and LDA
 
 
LocalLDAMG?LDASTMLDATMMBASELINE 1BASELINE 2
Figure 2: With true clusterings of DRDAs as the input, we use
DomSum to compare the performance of LocalLDA, MGLDA
and STM against two baselines, LDA and TMM. ?# topic? in-
dicates the number of topics for the model. For MGLDA, ?#
topic? is the number of local topics.
2 3 4 5 6 7 8 9 1013
13.2
13.4
13.6
13.8
14
14.2
14.4
14.6
14.8
15
#Topic    
ROU
GE?S
U4 F 
(%)
Summarization from DRDAs by Different Metrics Based on STM (DRDA only)
 
 
DomSumOneTopicMultiTopicTMMSumKLSum
Figure 3: With true clusterings of DRDAs as the input, Dom-
Sum is compared with four DA-level summarization metrics us-
ing topic distributions from STM. Results from LocalLDA and
MGLDA are similar so they are not displayed.
grained topic models that discover topic structures
on utterance-level better identify gist information.
Can the proposed token-level summarization
framework better identify important words and
remove redundancies than utterance selection
methods? Figure 3 demonstrates the comparison
results for our DomSum token-level summarization
framework with four existing utterance scoring met-
rics discussed in Section 3.2, namely OneTopic,
MultiTopic, TMMSum and KLSum. The utterance
with highest score is extracted to form the summary.
LocalLDA and STM are utilized to compute the in-
put distributions, i.e., P (T |DA) and P (w|T ). From
Figure 3, DomSum yields the best F scores which
45
2 3 4 5 6 7 8 9 107.5
8
8.5
9
9.5
10
10.5
11
#Topic             
ROU
GE?
SU4
 F (%
)
Leveraging Context by STM
 
 Adj+DomSum(Multi)
Adj+DomSum(One)
TFIDF+DomSum(Multi)
TFIDF+DomSum(One)
Figure 4: Under DomSum framework, two types of context
information are added: Adjacent DA (?Adj?) and DAs with high
TFIDF similarities (?TFIDF?). For each context DA, selecting
words from the dominant topic of center DA (?One?) or the
current context DA (?Multi?) are investigated.
2 3 4 5 6 7 8 9 107.5
8
8.5
9
9.5
10
10.5
11
11.5
12
12.5
#Topic              
ROU
GE?
SU4
 F (%
)
Summarization by Different Metrics (adding Context)
 
 LocalLDA+DomSum(One)STM+DomSum(One)LocalLDA+OneTopicSTM+OneTopicLocalLDA+MultiTopicSTM+MultiTopic
Figure 5: By using adjacent DAs as context, DomSum is com-
pared with two DA-level summarization metrics: OneTopic and
MultiTopic. For DomSum, the words of context DA from dom-
inant topic of the center DA (?One?) is selected; For OneTopic
and MultiTopic, three top ranked DAs are selected.
shows that the token-level summarization approach
is more effective than utterance-level methods.
Which way is better for leveraging context infor-
mation? We explore two types of context infor-
mation. For adjacent content (Adj in Figure 4), 5
DAs immediately preceding and 5 DAs succeeding
the center DRDA are selected. For TF-IDF context
(TFIDF in Figure 4), 10 DAs of highest TF-IDF sim-
ilarity with the center DRDA are taken. We also
explore two ways to extract summary-worthy words
from the context DA ? selecting words from the
dominant topic of either the center DA (denoted as
?One? in parentheses in Figure 4) or the current con-
text DA (denoted as ?multi? in parentheses in Fig-
True Clusterings
R-1 R-2 R-SU4
PREC REC F1 F1 F1
Baselines
Longest DA 34.06 31.28 32.61 12.03 13.58
Prototype DA 40.72 28.21 33.32 12.18 13.46
Supervised
Methods
CRF 52.89 26.77 35.53 11.48 14.03
SVM 43.24 37.92 40.39 12.78 16.24
Our Approach
5 topics
LocalLDA 35.18 38.92 36.95 12.33 14.74
+ context 17.26 45.34 25.00 8.40 11.05
STM 34.06 41.30 37.32 12.42 14.82
+ context 15.60 48.10 23.56 8.16 9.98
10 topics
LocalLDA 36.20 36.81 36.50 12.04 14.34
+ context 21.82 41.57 28.62 9.61 12.24
STM 34.15 40.83 37.19 12.40 14.56
+ context 17.87 46.57 25.82 8.89 10.97
Upperbound 100.00 45.05 62.12 33.27 34.89
Table 1: ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-SU4
(R-SU4) scores for our proposed token-level summarization ap-
proaches along with two baselines, supervised methods and the
Upperbound (only using DRDAs). ? all use True Clusterings
ure 4). Figure 4 indicates that the two types of con-
text information do not have significant difference,
while selecting the words from the dominant topic
of the center DA results in better ROUGE-SU4 F
scores. Notice that compared with Figure 3, the re-
sults in Figure 4 have lower F scores when using the
true clusterings of DRDAs. This is because context
DAs bring in relevant words as well as noisy infor-
mation. We will show in Section 6.2 that when true
clusterings are not available, the context information
can boost both recall and F score.
How do the token-level summarization frame-
work compared to utterance selection methods
for leveraging context? We also compare the
ability of leveraging context of DomSum to utter-
ance scoring metrics, i.e., OneTopic and MultiTopic.
5 DAs preceding and 5 DAs succeeding the center
DA are added as context information. For context
DA under DomSum, we select words from the dom-
inant topic of the center DA (denoted as ?One? in
parentheses in Figure 5). For OneTopic and Mul-
tiTopic, the top 3 DAs are extracted as the sum-
mary. Figure 5 demonstrates the combination of Lo-
calLDA and STM with each of the metrics. Dom-
Sum, as a token-level summarization metrics, domi-
nates other two metrics in leveraging context.
46
System Clusterings
R-1 R-2 R-SU4
PREC REC F1 F1 F1
Baselines
Longest DA 17.06 11.64 13.84 2.76 3.34
Prototype DA 18.14 10.11 12.98 2.84 3.09
Supervised
Methods
CRF 46.97 15.25 23.02 6.09 9.11
SVM 39.05 18.45 25.06 6.11 9.82
Our Approach
5 topics
LocalLDA 25.57 16.57 20.11 4.03 5.87
+ context 20.68 25.96 23.02 3.09 4.48
STM 24.15 17.82 20.51 4.03 5.69
+ context 20.64 30.03 24.47 3.59 4.76
10 topics
LocalLDA 25.98 15.94 19.76 3.59 4.41
+ context 23.98 21.92 22.90 3.45 4.10
STM 26.32 19.14 22.16 4.07 5.88
+ context 22.50 28.40 25.11 3.43 4.15
Table 2: ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-SU4
(R-SU4) scores for our proposed token-level summarization ap-
proaches, compared with two baselines and supervised meth-
ods. ? all use System Clusterings
How do our approach perform when compared
with supervised learning approaches? For a bet-
ter comparison, we also provide summarization
results by using supervised systems along with
an upperbound. We use Support Vector Ma-
chines (Joachims, 1998) with RBF kernel and order-
1 Conditional Random Fields (Lafferty et al., 2001)
? trained with the same features as (Wang and
Cardie, 2011) to identify the summary-worthy to-
kens to include in the abstract. A three-fold cross
validation is conducted for both methods. ROUGE-
1, ROUGE-2 and ROUGE-SU4 scores are listed in
Table 1. From Table 1, our token-level summa-
rization approaches based on LocalLDA and STM
are shown to outperform the baselines and even the
CRF. Meanwhile, by adding context information,
both LocalLDA and STM can get better ROUGE-1
recall than the supervised methods, even higher than
the provided upperbound which is computed by only
using DRDAs. This shows the DomSum framework
can leverage context to compensate the summaries.
6.2 System Clusterings
Results using the System Clusterings (Table 2)
present similar findings, though all of the system and
baseline scores are lower. By adding context infor-
mation, the token-level summarization approaches
based on fine-grained topic models compare favor-
DRDA (1): I think if we can if we can include them at not too
much extra cost, then I?d put them in,
DRDA (2): Uh um we we?re definitely going in for voice
recognition as well as LCDs, mm.
DRDA (3): So we?ve basically worked out that we?re going
with a simple battery,
context DA (1):So it?s advanced integrated circuits?
context DA (2):the advanced chip
context DA (3): and a curved on one side case which is folded
in on itself , um made out of rubber
Decision Abstract: It will have voice recognition, use a simple
battery, and contain an advanced chip.
Longest DA & Prototype DA: Uh um we we?re definitely going
in for voice recognition as well as LCDs, mm.
TMM: I think if we can if we can include them at not too much
extra cost, then I?d put them in,
SVM: cost voice recognition simple battery
CRF: voice recognition battery
STM: extra cost, definitely going voice recognition LCDs,
simple battery
STM + context: cost, company, advanced integrated circuits, going
voice recognition, simple battery, advanced chip, curved case rubber
Table 3: Sample system outputs by different methods are in the
third cell (methods? names are in bold). First cell contains three
DRDAs supporting the decision in the second cell and three ad-
jacent DAs of them.
ably to the supervised methods in F scores, and also
get the best ROUGE-1 recalls.
6.3 Sample System Summaries
To better exemplify the summaries generated by
different systems, sample output for each method
is shown in Table 3. We see from the table that
utterance-level extractive summaries (Longest DA,
Prototype DA, TMM) make more coherent but still
far from concise and compact abstracts. On the other
hand, the supervised methods (SVM, CRF) that pro-
duce token-level extracts better identify the overall
content of the decision abstract. Unfortunately, they
require human annotation in the training phase. In
comparison, the output of fine-grained topic models
can cover the most useful information.
7 Conclusion
We propose a token-level summarization framework
based on topic models and show that modeling topic
structure at the utterance-level is better at identify-
ing relevant words and phrases than document-level
models. The role of context is also studied and
shown to be able to identify additional summary-
worthy words.
Acknowledgments This work was supported in part by
National Science Foundation Grants IIS-0968450 and
IIS-1111176, and by a gift from Google.
47
References
