Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 207?216,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Using Group History to Identify Character-directed Utterances in
Multi-child Interactions
Hannaneh Hajishirzi, Jill F. Lehman, and Jessica K. Hodgins
hannaneh.hajishirzi, jill.lehman, jkh@disneyresearch.com
Abstract
Addressee identification is an element of all
language-based interactions, and is critical for
turn-taking. We examine the particular prob-
lem of identifying when each child playing an
interactive game in a small group is speak-
ing to an animated character. After analyzing
child and adult behavior, we explore a family
of machine learning models to integrate au-
dio and visual features with temporal group
interactions and limited, task-independent lan-
guage. The best model performs identification
about 20% better than the model that uses the
audio-visual features of the child alone.
1 Introduction
Multi-party interaction between a group of partic-
ipants and an autonomous agent is an important
but difficult task. Key problems include identify-
ing when speech is present, who is producing it, and
to whom it is directed, as well as producing an ap-
propriate response to its intended meaning. Solving
these problems is made more difficult when some
or all of the participants are young children, who
have high variability in language, knowledge, and
behavior. Prior research has tended to look at single
children (Oviatt, 2000; Black et al., 2009) or multi-
person groups of adults (Bohus and Horvitz, 2009a).
We are interested in interactions between animated
or robotic characters and small groups of four to ten
year old children. The interaction can be brief but
should be fun.
Here we focus specifically on the question of de-
ciding whether or not a child?s utterance is directed
to the character, a binary form of the addressee
identification (AID) problem. Our broad goals in
this research are to understand how children?s be-
havior in group interaction with a character differs
from adults?, how controllable aspects of the charac-
ter and physical environment determine participants?
behavior, and how an autonomous character can take
advantage of these regularities.
We collected audio and video data of groups of up
to four children and adults playing language-based
games with animated characters that were under lim-
ited human control. An autonomous character can
make two kinds of AID mistakes: failing to detect
when it is being spoken to, and acting as if it has
been spoken to when it has not. The former can be
largely prevented by having the character use exam-
ples of the language that it can recognize as part of
the game. Such exemplification cannot prevent the
second kind of mistake, however. It occurs, for ex-
ample, when children confer to negotiate the next
choice, respond emotionally to changes in the game
state, or address each other without making eye con-
tact. As a result, models that use typical audio-
visual features to decide AID will not be adequate
in multi-child environments. By including tempo-
ral conversational interactions between group mem-
bers, however, we can both detect character-directed
utterances and ignore the remainder about 20% bet-
ter than simple audio-visual models alone, with less
than 15% failure when being spoken to, and about
20% failure when not addressed.
2 Related Work
Our models explore the use of multimodal features
that represent activities among children and adults
interacting with a character over time. Prior research
has tended to look at single children or multi-person
207
groups of adults and has typically used a less inclu-
sive set of features (albeit in decisions that go be-
yond simple AID).
Use of multimodal features rests on early work
by Duncan and Fiske who explored how gaze and
head and body orientation act as important predic-
tors of AID in human-human interactions (Duncan
and Fiske, 1977). Bakx and colleagues showed that
accuracy can be improved by augmenting facial ori-
entation with acoustic features in an agent?s interac-
tions with an adult dyad (Bakx et al., 2003). Oth-
ers have studied the cues that people use to show
their interest in engaging in a conversation (Gra-
vano and Hirschberg, 2009) and how gesture sup-
ports selection of the next speaker in turn-taking
(Bergmann et al., 2011). Researchers have also
looked at combining visual features with lexical fea-
tures like the parseability of the utterance (Katzen-
maier et al., 2004), the meaning of the utterance, flu-
ency of speech, and use of politeness terms (Terken
et al., 2007), and the dialog act (Matsusaka et al.,
2007). However, all use hand-annotated data in their
analysis without considering the difficulty of auto-
matically deriving the features. Finally, prosodic
features have been combined with visual and lexi-
cal features in managing the order of speaking and
predicting the end-of-turn in multi-party interactions
(Lunsford and Oviatt, 2006; Chen and Harper, 2009;
Clemens and Diekhaus, 2009).
Work modeling the temporal behavior of the
speaker includes the use of adjacent utterances (e.g.,
question-answer) to study the dynamics of the dialog
(Jovanovic et al., 2006), the prediction of addressee
based on the addressee and dialog acts in previous
time steps (Matsusaka et al., 2007), and the use of
the speaker?s features over time to predict the qual-
ity of an interaction between a robot and single adult
(Fasel et al., 2009).
Horvitz and Bohus have the most complete (and
deployed) model, combining multimodal features
with temporal information using a system for multi-
party dynamic interaction between adults and an
agent (Bohus and Horvitz, 2009a; Bohus and
Horvitz, 2009b). In (Bohus and Horvitz, 2009a)
the authors describe the use of automatic sensors for
voice detection, face detection, head position track-
ing, and utterance length. They do not model tem-
poral or group interactions in determining AID, al-
though they do use a temporal model for the inter-
action as a whole. In (Bohus and Horvitz, 2009b)
the authors use the speaker?s features for the cur-
rent and previous time steps, but do not jointly track
the attention or behavior of all the participants in the
group. Moreover, their model assumes that the sys-
tem is engaged with at most one participant at a time,
which may be a valid conversational expectation for
adults but is unlikely to hold for children. In (Bo-
hus and Horvitz, 2011), the authors make a similar
assumption regarding turn-taking, which is built on
top of the AID module.
3 User Study
We use a Wizard of Oz testbed and a scripted mix
of social dialog and interactive game play to explore
the relationship between controllable features of the
character and the complexity of interacting via lan-
guage with young children. The games are hosted by
two animated characters (Figure 1, left). Oliver, the
turtle, is the main focus of the social interactions and
also handles repair subdialogs when a game does not
run smoothly. Manny, the bear, provides comic re-
lief and controls the game board, making him the
focus of participants? verbal choices during game
play. The game appears on a large flat-screen dis-
play about six feet away from participants who stand
side-by-side behind a marked line. Audio and video
are captured, the former with both close-talk micro-
phones and a linear microphone array.
Oliver and Manny host two games designed to be
fun and easy to understand with little explicit in-
struction. In Madlibs, participants help create a short
movie by repeatedly choosing one everyday object
from a set of three. The objects can be seen on the
board and Oliver gives examples of appropriate re-
ferring phrases when prompting for a choice. In Fig-
ure 1, for example, he asks, ?Should our movie have
a robot, a monster, or a girl in it?? After five sets of
objects are seen, the choices appear in silly contexts
in a short animation; for instance, a robot babysit-
ter may serve a chocolate pickle cake for lunch. In
Mix-and-Match (MnM), participants choose apparel
and accessories to change a girl?s image in unusual
ways (Figure 1, right). MnM has six visually avail-
able objects and no verbal examples from Oliver, ex-
cept in repair subdialogs. It is a faster-paced game
208
Figure 1: Manny and Oliver host Madlibs and a family play Mix-and-Match
with the immediate reward of a silly change to the
babysitter?s appearance whenever a referring phrase
is accepted by the wizard.
The use of verbal examples in Madlibs is expected
to influence the children?s language, potentially in-
creasing the accuracy of speech recognition and ref-
erent resolution in an autonomous system. The cost
of exemplification is slower pacing because children
must wait while the choices are named. To compen-
sate, we offer only a small number of choices per
turn. Removing exemplification, as in MnM, creates
faster pacing and more variety of choice each turn,
which is more fun but also likely to increase three
types of problematic phenomena: out-of-vocabulary
choices (?the king hat? rather than ?the crown?),
side dialogs to establish a referring lexical item or
phrase (?Mommy, what is that thing??), and the use
of weak naming strategies based on physical fea-
tures (?that green hand?).
The two games are part of a longer scripted se-
quence of interactions that includes greetings, good-
byes, and appropriate segues. Overall, the language
that can be meaningfully directed to the characters is
constrained to a small social vocabulary, yes/no re-
sponses, and choices that refer to the objects on the
board. The wizard?s interface reflects these expec-
tations with buttons that come and go as a function
of the game state. For example, yes and no buttons
are available to the wizard after Oliver asks, ?Will
you help me?? while robot, monster, and girl but-
tons are available after he asks, ?Should our movie
have a robot, a monster, or a girl in it?? The wiz-
ard also has access to persistent buttons to indicate a
long silence, unclear speech, multiple people speak-
ing, or a clear reference to an object not on the board.
These buttons launch Oliver?s problem-specific re-
pair behaviors. The decomposition of functional-
ity in the interface anticipates replacing the wizard?s
various roles as voice activity detector, addressee
identifier, speech recognizer, referent resolver, and
dialog manager in an autonomous implementation.
Although meaningful language to the characters
is highly constrained, language to other participants
can be about anything. In particular, both games
establish an environment in which language among
participants is likely to be about negotiating the turn
(?Brad, do you want to change anything??), nego-
tiating the choice (?Billy, don?t do the boot?) or
commenting on the result (?her feet look strange?).
Lacking examples of referring phrases by Oliver,
MnM also causes side dialogs to discuss how ob-
jects should be named. Naming discussions, choice
negotiation, and comments define the essential dif-
ficulty in AID for our testbed; they are all likely to
include references to objects on the board without
the intention of changing the game state.
3.1 Data collection and annotation
Twenty-seven compensated children (14 male, 13
female) and six adult volunteers participated. Chil-
dren ranged in age from four to ten with a mean of
6.4 years. All children spoke English as a first lan-
guage. Groups consisted of up to four people and
always contained either a volunteer adult or the ex-
perimenter the first time through the activities. If the
experimenter participated, she did not make game
choices. Volunteer adults were instructed to sup-
port their children?s participation in whatever way
felt natural for their family. When time permitted,
children were given the option of playing one or both
209
games again. Those who played a second time were
allowed to play alone or in combination with others,
with or without an adult. Data was collected for 25
distinct groups, the details of which are provided in
Table 5 in the Appendix.
Data from all sessions was hand-annotated with
respect to language, gesture, and head orientation.
Labels were based on an initial review of the videos,
prior research on AID and turn-taking in adults, and
the ability to detect candidate features in our phys-
ical environment. A second person segmented and
labeled approximately one third of each session for
inter-annotator comparison. The redundant third
was assigned randomly from the beginning, middle,
or end of the session in order to balance across social
interactions, Madlibs choices, and MnM choices.
Labels were considered to correspond to the same
audio or video sequence if the segments overlapped
by at least 50%.
For language annotations, audio from the close-
talk microphones was used with the video and seg-
mented into utterances based on pauses of at least
50 msec. Typical mispronunciations for young chil-
dren (e.g., word initial /W/ for /R/) were transcribed
as normal words in plain text; non-standard errors
were transcribed phonologically. Every utterance
was also labeled as being directed to the character
(CHAR) or not to the character (NCHAR). Second
annotators segmented the audio and assigned ad-
dressee but did not re-transcribe the speech. Inter-
annotator agreement for segmentation was 95%
(? = .91), with differences resulting from only
one annotator segmenting properly around pauses
or only one being able to distinguish a given child?s
voice among the many who were talking. For seg-
ments coded by both annotators, CHAR/NCHAR
agreement was 94% (? = .89).
For gesture annotations, video segments were
marked for instances of pointing, emphasis, and
head shaking yes and no. Emphatic gestures were
defined as hand or arm movements toward the screen
that were not pointing or part of grooming motions.
Annotators agreed on the existence of gestures 74%
of the time (? = .49), but when both annotators in-
terpreted movement as a gesture, they used the same
label 98% of the time (? = .96).
For orientation, video was segmented when the
head turned away from the screen and when it turned
back. Rather than impose an a priori duration or an-
gle, annotators were told to use the turn-away label
when the turn was associated with meaningful in-
teraction with a person or object, but not for brief,
incidental head movements. Adults could also have
segments that were labeled as head-incline if they
bent to speak to children. Annotators agreed on the
existence of these orientation changes 83% of the
time (? = .62); disagreements may represent simple
differences in accuracy or differences in judgments
about whether a movement denoted a shift in atten-
tion. Orientation changes coded by both annotators
had the same label 92% of the time (? = .85).
The annotated sessions are a significant portion
of the training and test data used for our models.
Although these data reflect some idiosyncracy due
to human variability in speech perception, gesture
recognition, and, possibly, the attribution of inten-
tion to head movements, they show extremely good
agreement with regard to whether participants were
talking to the character. Even very young chil-
dren in group situations give signals in their speech
and movements that allow other people to determine
consistently to whom they are speaking.
3.2 Analysis of behavior
As intended, children did most of the talking
(1371/1895 utterances, 72%), spoke to the charac-
ters the majority of the time (967/1371, 71%), and
made most of the object choices (666/683, 98%).
Adults generally acted in support roles, with 88%
of all adult utterances (volunteers and experimenter)
directed to the children.
The majority of children?s CHAR utterances
(71%) were object choices. Although the wizard
in our study was free to accept any unambiguous
phrase as a valid choice, an automated system must
commit to a fixed lexicon. In general, the larger
the lexicon, the smaller the probability that a ref-
erence will be out-of-vocabulary, but the greater the
probability that a reference could be considered am-
biguous and require clarification. The lexical entry
for each game object contains the simple descrip-
tion given to the illustrator (?alien hands,? ?pickle?)
and related terms from WordNet (Fellbaum, 1998)
likely to be known by young children (see Table 3 in
the Appendix for examples). In anticipation of weak
naming strategies, MnM entries also contain salient
210
visual features based on the artwork (like color), as
well as the body part the object would replace, where
applicable. Entries for Madlibs objects average 2.75
words; entries for MnM average 5.8. With these
definitions, only 37/666 (6%) of character-directed
choices would have been out-of-vocabulary for a
word-spotting speech recognizer with human accu-
racy. However, Oliver?s use of exemplification has a
strong effect. In Madlibs, 98% of children?s choices
were unambiguous repetitions of example phrases.
In MnM, 92% of choices contained words in the lex-
icon, but only 28% indexed a unique object.
Recognition of referring phrases should be a fac-
tor in making AID decisions only if it helps to discri-
mate CHAR from NCHAR utterances. Object refer-
ences occurred in 62% of utterances to the charac-
ters and only 25% of utterances addressed to other
participants, but again, Oliver?s exemplification mat-
tered. About 20% of NCHAR utterances from chil-
dren in both games and from adults in Madlibs con-
tained object references. In MnM, however, a third
of adults? NCHAR utterances contained object ref-
erences as they responded to children?s requests for
naming advice.
Language is not the only source of information
available from our testbed. We know adults use both
eye gaze and gesture to modulate turn-taking and
signal addressee in advance of speech. Because non-
verbal mechanisms for establishing joint attention
occur early in language development, even children
as young as four might use such signals consistently.
Although we use head movement as an approxima-
tion of eye gaze, we positioned participants side-by-
side to make such movements necessary for eye con-
tact. Unfortunately, the game board constituted too
strong a ?situational attractor? (Bakx et al., 2003).
As in their kiosk environment, our adults oriented
toward the screen much of the time (68%) they were
talking to other participants. Children violated con-
versational convention more often, orienting toward
the screen for 82% of NCHAR utterances.
Gesture information is also available from the
video data and reveals distinct patterns of usage
for children and adults. The average number of
gestures/utterance was more than twice as high in
adults. Children were more likely to use empha-
sis gestures when they were talking to the charac-
ters; adults hardly used them at all. Children?s ges-
tures overlapped with their speech almost 80% of
the time, but adult?s gestures overlapped with their
speech only half the time. Moreover, when children
pointed while talking they were talking to the char-
acters, but when adults pointed while talking they
were talking to the children. Finally, adults shook
their heads when they were talking to children but
not when they were talking to the characters, while
children shook their heads when talking to both.
To maintain an engaging experience, object refer-
ences addressed to the character should be treated as
possible choices, while object references addressed
to other participants should not produce action. In-
teractions that violate this rule too often will be
frustrating rather than fun. While exemplification
in Madlibs virtually eliminated out-of-vocabulary
choices, it could not eliminate detectable object ref-
erences that were not directed to the characters. In
both games, such references were often accompa-
nied by other signs that the character was being ad-
dressed, like orientation toward the board and point-
ing. Using all the cues available, human annotators
were almost always able to agree on who was being
addressed. The next section looks at how well an
autonomous agent can perform AID using only the
cues it can sense, if it could sense them with human
levels of accuracy.
4 Models for Addressee Classification
We cast the problem of automatically identifying
whether an utterance is addressed to the character
(and so may result in a character action) as a binary
classification problem. We build and test a family
of models based on distinct sources of information
in order to understand where the power is coming
from and make it easier for other researchers to com-
pare to our approach. All models in the family are
constructed from Support Vector Machines (SVM)
(Cortes and Vapnik, 1995), and use the multimodal
features in Table 1 to map each 500 msec time slice
of a child?s speech to CHAR or NCHAR. This ba-
sic feature vector combines a subset of the hand-
annotated data (Audio and Visual) with automati-
cally generated data (Prosodic and System events).
We use a time slice rather than a lexical or semantic
boundary for forcing a judgment because in a real-
time interaction decisions must be made even when
211
Audio speech: presence/absence
Prosodic pitch: low/medium/highspeech power: low/medium/high
System event character prompt: presence/absence
Visual orientation: head turn away/backgesture: pointing/emphasis
Table 1: Basic features
lexical or semantic events do not occur.
We consider three additional sources of informa-
tion: group behavior, history, and lexical usage.
Group behavior ? the speech, prosody, head orien-
tation, and gestures of other participants ? is impor-
tant because most of the speech that is not directed
to the characters is directed to a specific person in
the group. History is important both because the side
conversations unfold gradually and because it allows
us to capture the changes to and continuity of the
speaker?s features across time slices. Finally, we use
lexical features to represent whether the participant?s
speech contains words from a small, predefined vo-
cabulary of question words, greetings, and discourse
markers (see Appendix). Because the behavioral
analysis showed significant use of words referring
to choice objects during both CHAR and NCHAR
utterances, we do not consider those words in deter-
mining AID. Indeed, we expect the AID decision to
simplify the task of the speech recognizer by helping
that component ignore NCHAR utterances entirely.
The full set of models is described by adding to
the basic vector zero or more of group (g), word (w),
or history (h) features. We use the notation g[+/-
]w[+/-]h[(time parameters)/-] to indicate the pres-
ence or absence of a knowledge source. The time
parameters vary and will be explained in the con-
text of particular models, below. Although we have
explored a larger portion of the total model space,
we limit our discussion here to representative mod-
els (including the best model) that will demonstrate
the effect of each kind of information on the two
main goals of AID: responding to CHAR utterances
and not responding to NCHAR utterances. There
are eight models of interest, the first four of which
isolate individual knowledge sources:
The Basic model (g-w-h-) is an SVM classifier
trained to generate binary CHAR/NCHAR values
based solely on the features in Table 1. It represents
the ability to predict whether a child is talking to the
A1,t	 ?
GN	 ?
P1	 ?
P2	 ?
P3	 ?
P4	 ?
t-??1,t,t+1	 ?
P1	 ?P1	 ?
t+K	 ?t-??M	 ?
T	 ?
t-??N-??1,t-??N,	 ?t-??N+1	 ?
P1	 ?
P1	 ?
P2	 ?
P3	 ?
P4	 ?
G1	 ?
Individual	 ?sub-??model	 ?
Par?cipants	 ?sub-??models	 ?
Figure 2: The two-layer Group-History model maps
group and individual behavior over a fixed window of
time slices to a CHAR/NCHAR decision at time t. The
decision at time t (A1,t) is based on the participant?s ba-
sic features (P1), the output of the individual?s submodel
(T ) ? which encapsulates the history of the individual
for M previous and K subsequent time slices ? and the
output of N participant submodels, each of which con-
tributes a value based on three times slices.
character independent of speech recognition and fo-
cused on only 500 msecs of that child?s behavior.
The Group model (g+w-h-) incorporates group
information, but ignores temporal and lexical be-
havior. This SVM is trained on an extended feature
vector that includes the basic features for the other
participants in the group together with the speaker?s
feature vector at each time slice.
The History model (g-w-h(N ,K)) considers only
the speaker?s basic features, but includesN previous
and K subsequent time slices surrounding the slice
for which we make the CHAR/NCHAR decision.1
The Word model (g-w+h-) extends the basic vec-
tor to include features for the presence or absence of
question words, greetings, and discourse markers.
The next three models combine pairs of knowl-
edge sources. The Group-Word (g+w+h-) and
History-Word (g-w+h(N ,K)) models are straight-
1A History model combining the speaker?s basic vector over
the previous and current time slices (N = 4 and K = 0) out-
performed a Conditional Random Fields (Lafferty et al., 2001)
model with N + 1 nodes representing consecutive time slices
where the last node is conditioned on the previous N nodes.
212
forward extensions of their respective base models,
created by adding lexical features to the basic vec-
tors. The Group-History model (g+w-h(N ,K,M ))
is more complex. It is possible to model group in-
teractions over time by defining a new feature vector
that includes all the participants? basic features over
multiple time slices. As we increase the number of
people in a group and/or the number of time slices to
explore the model space, however, the sheer size of
this simple combination of feature vectors becomes
unwieldy. Instead we make the process hierarchical
by defining the Group-History as a two-layer SVM.
Figure 2 instantiates the Group-History model for
participant P1 playing in a group of four. In the con-
figuration shown, the decision for P1?s utterance at
time t is based on behavior during N previous and
K subsequent time slices, meaning each decision is
delayed by K time slices with respect to real time.
The CHAR/NCHAR decision for time slice t de-
pends on P1?s basic feature vector at time t, the out-
put from the Individual submodel for time t, and the
outputs from the Participants submodel for each of
the time slices through t. A concrete instantiation of
the model can be seen in Figure 4 in the Appendix.
The Individual submodel is an SVM that assigns a
score to the composite of P1?s basic feature vectors
across a window of time (here,M+K+1). The Par-
ticipants submodel is an SVM that assigns a score to
the basic features of all members during each three
slice sliding subwindow in the full interval. More
intuitively: the Individual submodel finds correla-
tions among the child?s observable behaviors over
a window of time; the Participants submodel cap-
tures relationships between members? behaviors that
co-occur over small subwindows; and the Group-
History model combines the two to find regularities
that unfold among participants over time, weighted
toward P1?s own behavior.
The final model of interest, Group-History-Word
(g+w+h(N ,K,M ,Q)), incorporates the knowledge
from all sources of information. A Lexical submodel
is added to the Individual and Participants submod-
els described above. The Lexical submodel is an
SVM classifier trained on the combination of ba-
sic and word features for the current and Q previ-
ous time slices. The second layer SVM is trained on
the scores of the Individual, Participants, and Lex-
ical submodels as well as the combined basic and
Model Max f1 AUC TPR TNR
Basic features
g-w-h- 0.879 0.504 0.823 0.604
g+w-h- 0.903 0.588 0.872 0.650
g-w-h(8,1) 0.897 0.626 0.867 0.697
g+w-h(4,1,8) 0.903 0.645 0.849 0.730
Basic + Word features
g-w+h- 0.904 0.636 0.901 0.675
g+w+h- 0.906 0.655 0.863 0.728
g-w+h(8,1) 0.901 0.661 0.886 0.716
g+w+h(4,1,8,4) 0.913 0.701 0.859 0.786
Table 2: Comparison of models
word feature vector for the child.
5 Results and Discussions
We used the LibSVM implementation (Chang and
Lin, 2011) for evaluation, holding out one child?s
data at a time during training, and balancing the
data set to compensate for the uneven distribution
of CHAR and NCHAR utterances in the corpus. As
previously noted, we used a time slice of 500 msec
in all results reported here. Where history is used,
we consider only models with a single time slice of
look-ahead (K = 1) to create minimal additional de-
lay in the character?s response.
Table 2 reports average values, for each model
and over all sets of remaining children, in terms of
Max F1, true positive rate (TPR), true negative rate
(TNR), and area under the TPR-TNR curve (AUC).
TPR represents a model?s ability to recognize utter-
ances directed to the character; low TPR means chil-
dren will not be able to play the game effectively.
TNR indicates a model?s ability to ignore utterances
directed to other participants; low TNR means that
the character will consider changing the game state
when it hasn?t been addressed.
Table 2 (top) shows comparative performance
without the need for any speech recognition. F1
and TPR are generally high for all models. Using
only the basic features, however, gives a relatively
low TNR and an AUC that is almost random. The
History model, (g-w-h(8,1)), increased performance
across all measures compared to the basic features
(g-w-h-). We found that the History model?s per-
formance was best when four seconds of the past
were considered. Group information within a single
time slice also improves performance over the ba-
sic features, but the Group-History model has the
213
best overall tradeoff in missed CHAR versus ig-
nored NCHAR utterances (AUC). Group-History?s
best performance is achieved using two seconds of
group information from the past via the Participants
submodel and four seconds of the speaker?s past
from the Individual submodel.
Comparing the top and bottom halves of Table 2
shows that all models benefit from accurate recogni-
tion of a small set of task-independent words. The
table shows that word spotting improves both TPR
and TNR when added to the Basic model, but tends
to improve only TNR when added to models with
group and history features. Improved TNR probably
results from the ability to detect NCHAR utterances
when participants are facing the characters and/or
pointing during naming discussions and comments.2
Table 2 shows results averaged over each held out
child. We then recast this information to show, by
model, the percentage of children that would expe-
rience TPR and TNR higher than given thresholds.
Figure 3 shows a small portion of a complete graph
of this type; in this case the percentage of children
who would experience greater than 0.6 for TPR and
greater than 0.5 for TNR under each model. TPR
and TNR lines for a model have the same color and
share a common pattern.
Better models have higher TPR and TNR for more
children. The child who has to keep restating his or
her choice (poor TPR) will be frustrated, as will the
child who has the character pre-emptively take his
or her choice away by ?overhearing? side discus-
sions (poor TNR). While we do not know for any
child (or any age group) how high a TPR or TNR is
required to prevent frustration, Figure 3 shows that
without lexical information the Group-History and
Group models have the best balance for the thresh-
olds. Group-History gives about 85% of the children
a TPR ? 0.7 for a TNR ? 0.5. The simpler Group
model, which has no 500 msec delay for lookahead,
can give a better TPR for the same TNR but for only
75% of the children. When we add lexical knowl-
edge the case for Group-History becomes stronger,
as it gives more than 85% of children a TPR ? 0.7
for a TNR ? 0.6, while Group gives 85% of chil-
dren about the same TPR with a TNR ? 0.5.
2Results showing the affect of including object choice words
in the w+ models are given in Figure 4 in the Appendix.
Figure 3: The percentage of children experiencing dif-
ferent TPR/TNR tradeoffs in models with (bottom) and
without (top) lexical knowledge. The g-w-h- model does
not fall in the region of interest unless lexical features are
used.
6 Conclusions and Future Work
The behavior of the characters, types of games,
group make up, and physical environment all con-
tribute to how participants communicate over time
and signal addressee. We can manipulate some re-
lationships (e.g., by organizing the spatial layout to
promote head movement or having the character use
examples of recognizable language) and take ad-
vantage of others by detecting relevant features and
learning how they combine as behavior unfolds. Our
best current model uses group and history informa-
tion as well as basic audio-visual features to achieve
a max F1 of 0.91 and an AUC of 0.70. Although
this model does not yet perform as well as human
annotators, it may be possible to improve it by tak-
ing advantage of additional features that the behav-
ioral data tells us are predictive (e.g., whether the
speaker is an adult or child). Such additional sources
of information are likely to be important as we re-
place the annotated data with automatic sensors for
speech activity, orientation, and gesture recognition,
and embed addressee identification in the larger con-
text of turn-taking and full autonomous interaction.
214
References
