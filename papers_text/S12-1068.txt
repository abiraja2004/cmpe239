First Joint Conference on Lexical and Computational Semantics (*SEM), pages 487?492,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
ANNLOR: A Na??ve Notation-system for Lexical Outputs Ranking
Anne-Laure Ligozat
LIMSI-CNRS/ENSIIE
rue John von Neumann
91400 Orsay, France
annlor@limsi.fr
Cyril Grouin
LIMSI-CNRS
rue John von Neumann
91400 Orsay, France
cyril.grouin@limsi.fr
Anne Garcia-Fernandez
CEA-LIST
NANO INNOV, Bt. 861
91191 Gif-sur-Yvette cedex, France
anne.garcia-fernandez@cea.fr
Delphine Bernhard
LiLPa, Universite? de Strasbourg
22 rue Rene? Descartes, BP 80010
67084 Strasbourg cedex, France
dbernhard@unistra.fr
Abstract
This paper presents the systems we developed
while participating in the first task (English
Lexical Simplification) of SemEval 2012. Our
first system relies on n-grams frequencies
computed from the Simple English Wikipedia
version, ranking each substitution term by de-
creasing frequency of use. We experimented
with several other systems, based on term fre-
quencies, or taking into account the context in
which each substitution term occurs. On the
evaluation corpus, we achieved a 0.465 score
with the first system.
1 Introduction
In this paper, we present the methods we used while
participating to the Lexical Simplification task at Se-
mEval 2012 (Specia et al, 2012). We experimented
with several methods:
? using word frequencies or other statistical fig-
ures from the BNC corpus, Google Books
NGrams, the Simple English Wikipedia, and
results from the Bing search engine (with/with-
out lemmatization);
? using association measures for a word and its
context based on language models (with/with-
out inflection);
? making a combination of previous methods
with SVMRank.
Depending on the results obtained on the training
corpus, we chose the methods that seemed to best fit
the data.
2 Task description
2.1 Presentation
The Lexical Simplification task aimed at determin-
ing the degree of simplicity of words. The inputs
given were a short text, in which a target word was
chosen, and several substitutes for the target word
that fit the context.
An example of a short text follows; the target
word is ?outdoor?, and other words of this text will
be considered as the context of this target word.
< i n s t a n c e i d =? 270 ?>
<c o n t e x t>With t h e growing demand f o r
t h e s e f i n e g a r de n f u r n i s h i n g s ,
t h e y found i t n e c e s s a r y t o d e d i c a t e
a p o r t i o n o f t h e i r b u s i n e s s t o
<head>o u t d o o r< / head> l i v i n g and
p a t i o f u r n i s h i n g s .< / c o n t e x t>
< / i n s t a n c e>
The substitutes given for this target word were
the following: ?alfresco;outside;open-air;outdoor;?.
The objective was to order these words by descend-
ing simplicity.
2.2 Corpora
Two corpora were provided: the trial corpus with
development examples, and the test corpus for eval-
uation.
In the trial corpus, a gold standard was also given.
For the previous example, it stated that the substi-
tutes had to be in the following order: ?outdoor
open-air outside, alfresco?, ?outdoor? being consid-
ered as the simplest substitute, and ?outside? and
?alfresco? being considered as the less simple ones.
487
Three baselines have been given by the organiz-
ers: the first one is a simple randomization of the
substitute list, the second one keeps the substitute
list as it is, and the third one (called ?simple fre-
quency?) relies on the use of the Google Web 1T
corpus.
3 Preprocessing
3.1 Corpus constitution
In order to use machine-learning based approaches,
we produced two sub-corpora respectively for the
training and evaluation stages from the trial corpus.
The training sub-corpus is used to develop and tune
the systems we produced while the evaluation sub-
corpus is used to evaluate the results of these sys-
tems.
For each set from the SemEval trial corpus, if the
set is composed of at least eight lexical elements be-
longing to the same morpho-syntactic category (e.g.,
a set with at least eight instances of ?bright? as an
adjective), we extracted three instances from this
set for the evaluation sub-corpus, the remaining in-
stances being part of the training sub-corpus. If the
set is composed of less than eight instances, all in-
stances are used in the training sub-corpus. We also
kept two complete sets of lexical elements for the
evaluation sub-corpus in order to test the robustness
of our methods on new lexical elements that have not
been studied yet. This distribution allows us to bene-
fit from a repartition between training and evaluation
sub-corpora where the instances ratio is of 66/33%.
3.2 Corpus cleaning
While studying the trial corpus, we noticed that the
texts were not always in plain text, and in particular
contained HTML entities. As some of our methods
used the context of target words, we decided to cre-
ate a cleaner version of the corpora. For the dash and
quote HTML entities (&#8211; &#8220; etc.), we
replaced each entity by its refering symbol. When
replacing the apostrophe HTML entity (&apos;), we
decided to link the abbreviated token with the previ-
ous one because all n-grams methods worked better
with abbreviated terms of one token-length (don?t)
than two token-length (do n?t) (see section 5).
3.3 Inflection
In some sentences, the target words are inflected, but
the substitutes are given in their lemmatized forms.
For example, one of the texts was the following :
<c o n t e x t>In f a c t , d u r i n g a t l e a s t s i x
d i s t i n c t p e r i o d s i n Army h i s t o r y
s i n c e World War I , l a c k o f t r u s t and
c o n f i d e n c e i n s e n i o r l e a d e r s c au se d
t h e so?c a l l e d b e s t and
<head>b r i g h t e s t< / head> t o l e a v e t h e
Army i n d r o v e s .< / c o n t e x t>
For this text and target word, the proposed sub-
stitutes were ?capable; most able; motivated; in-
telligent; bright; clever; sharp; promising?, and if
we want to test the simplicity of the words in con-
text, for example with a 2-words left context, we
will obtain unlikely phrases such as ?best and capa-
ble? (which should be ?best and most capable?). We
thus used several resources to get inflected forms of
words: we used the Lingua::EN::Conjugate and Lin-
gua::EN::Inflect Perl modules, which give inflected
forms of verbs and plural forms of nouns, as well as
the English dictionary of inflected forms DELA,1 to
validate the Perl modules outputs if necessary, and
get comparatives and superlatives of adjectives, and
a list of irregular English verbs, also to validate the
Perl modules outputs.
4 Simple English Wikipedia based system
Our best system, called ANNLOR-simple, is based
on Simple English Wikipedia frequencies. As the
challenge focused on substitutions performed by
non-native English speakers, we tried to use linguis-
tic resources that best fit this kind of data. In this
way, we made the hypothesis that training our sys-
tem on documents written by or written for non-
native English speakers would be useful.
The use of the Simple English version from
Wikipedia seems to be a good solution as it is tar-
geted at people who do not have English as their
mother tongue. Our hypothesis seems to be correct
due to the results we obtained. Morevover, the Sim-
ple English Wikipedia has been used previously in
work on automatic text simplification, e.g. (Zhu et
al., 2010).
1http://infolingu.univ-mlv.fr/
DonneesLinguistiques/Dictionnaires/
telechargement.html
488
First, we produced a plain text version of the Sim-
ple English Wikipedia. We downloaded the dump
dated February 27, 2012 and extracted the textual
contents using the wikipedia2text tool.2 The
final plaintext file contains approximately 10 million
words.
We extracted word n-grams (n ranging from 1 to
3) and their frequencies from this corpus thanks to
the Text-NSP Perl module 3 and its count.pl pro-
gram, which produces the list of n-grams of a docu-
ment, with their frequencies. Table 1 gives the num-
ber of n-grams produced.
Table 1: Number of distinct n-grams extracted from the
Simple English Wikipedia
n #n-grams
1 301,718
2 2,517,394
3 6,680,906
1 to 3 9,500,018
Some of these n-grams are invalid, and result
from problems when extracting plain text from
Wikipedia, such as ?27|ufc 1?, which corresponds
to wiki syntax. As we would not find these n-grams
in our substitution lists, we did not try to clean the
n-gram data.
Then, we ranked the possible substitutes of a lex-
ical item according to these frequencies, in descend-
ing order. For example, for the substitution list (in-
telligent, bright, clever, smart), the respective fre-
quencies in the Simple English Wikipedia are (206,
475, 141, 201), and the substitutes will be ranked in
descending frequencies: (bright, intelligent, smart,
clever).
Several tests were conducted, with varying pa-
rameters. We used the plain text version of the Sim-
ple English Wikipedia, but also tried to lemmatize it,
since substitutes are lemmatized. We used the Tree-
Tagger 4 (Schmid, 1994) and applied it on the whole
2See http://www.polishmywriting.com/
download/wikipedia2text\_rsm\_mods.tgz
and http://blog.afterthedeadline.com/
2009/12/04/generating-a-plain-text-corpus
-from-wikipedia
3http://search.cpan.org/?tpederse/
Text-NSP-1.25/lib/Text/NSP.pm
4http://www.ims.uni-stuttgart.de/
corpus, before counting n-grams. Moreover, since
bigrams and trigrams increase a lot, the size of n-
gram data, we evaluated their influence on results.
These tests are summed up in table 2.
Table 2: Results obtained with the Simple English
Wikipedia based system, on the trial and test corpora
reference lemmas score on score on
n-grams trial corpus test corpus
1-grams only no 0.333 ?
1 and 2-grams no 0.371 ?
1 to 3-grams no 0.381 0.465
1 to 3-grams yes 0.380 0.462
Simple Frequency
0.398 0.471
baseline
WLV-SHEF-SimpLex
(best system ? 0.496
@SemEval2012)
With unigrams only, 158 substitutes of the trial
corpus are absent of the reference dataset, 105 when
adding bigrams, and 91 when adding trigrams. Most
of the missing n-grams (when using 1 to 3-grams)
indeed seem to be very uncommon, such as ?undo-
mesticated? or ?telling untruths?.
The small difference between the lemmatized and
inflected versions of Wikipedia is due to two rea-
sons: some substitutes are found in the lemmatized
version because substitutes are given in the lemma-
tized form (for example ?abnormal growth? is only
present in its plural form ?abnormal growths? in
the inflected Wikipedia); and some other substitutes
are missing in the lemmatized version, mostly be-
cause of errors from the TreeTagger (for example
?be scared of? becomes ?be scare of?).
We kept the system that obtained the best scores
on the trial corpus, that is with 1 to 3-grams and non-
lemmatized n-grams, with a score of 0.381. This
system obtained a score of 0.465 on the evalua-
tion corpus, thus ranking second ex-aequo at the Se-
mEval evaluation.
projekte/corplex/TreeTagger/
489
5 Other frequency-based methods
We tried several other reference corpora, always
with the idea that the more frequent a word is, the
simpler it is. We used the BNC corpus,5 as well
as the Google Books NGrams.6 These NGrams
were calculated on the books digitized by Google,
and contain for each encountered n-gram, its num-
ber of occurrences for a given year. As the Google
Books NGrams are quite voluminous, we selected a
random year (2008), and kept only alphabetical n-
grams with potential hyphens, and used n-grams for
n ranging from 1 to 4. The dataset used contains
477,543,736 n-grams.
We also used the Microsoft Web N-gram Service
(more details on this service are given in the fol-
lowing section) to rank substitutes in descending or-
der. The results of these methods on the trial corpus
are given in table 3. The result of the simple fre-
quency baseline is also given: this baseline is also
frequency-based, but words are ranked according to
the number of hits found when querying the Google
Web 1T corpus with each substitute.
Table 3: Results obtained with frequency-based methods,
on the trial corpus
reference corpus score
BNC 0.347
Google Books NGrams 0.367
Microsoft NGrams 0.383
Simple Frequency baseline 0.398
This table shows that all frequency-based meth-
ods have lower scores than the Simple Frequency
baseline, although the score obtained with the Mi-
crosoft NGrams is quite close to the baseline. The
results from Microsoft Ngrams and the Simple En-
glish are very close. We decided to submit the Sim-
ple English Wikipedia-based system because it was
more different from the simple frequency baseline.
6 Contextual methods
We also wanted to use contextual information, since,
according to the contexts of the target word, dif-
ferent substitutes can be used, or ranked differ-
5http://www.natcorp.ox.ac.uk/
6http://books.google.com/ngrams/datasets
ently. In the following two examples, the same word
?film? is targetted, and the same substitutes are pro-
posed ?film;picture;movie;?; yet, in the gold stan-
dard, ?film? is placed before ?movie? in instance 19,
and after it in instance 15.
< i n s t a n c e i d =? 15 ?>
<c o n t e x t>Film Music L i t e r a t u r e
C y b e r p l a c e ? I n c l u d e s
<head>f i l m< / head> r e v i e w s , message
b o a r d s , c h a t room , and images
from v a r i o u s f i l m s .< / c o n t e x t>
< / i n s t a n c e>
( . . . )
< i n s t a n c e i d =? 19 ?>
<c o n t e x t>A f i n e s c o r e by George Fen ton
( THE CRUCIBLE ) and b e a u t i f u l
p h o t o g r a h y by Roger P r a t t add
g r e a t l y t o t h e e f f e c t i v e n e s s o f t h e
<head>f i l m< / head> .< / c o n t e x t>
< / i n s t a n c e>
Ranking substitutes thus depends on the context
of the target word. We implemented two systems
taking the context of target words into account.
6.1 Language model probabilities
The other system submitted (called ANNLOR-
lmbing) relies on language models, which was the
method used by the organizers in their Simple Fre-
quency baseline. While the organizers used Google
n-grams to rank terms to be substituted by decreas-
ing frequency of use, we used Microsoft Web n-
grams in the same way. Nevertheless, we also added
the contexts of each term to be substituted.
We used the Microsoft Web N-gram Service7 to
obtain joint probability for text units, and more
precisely its Python library.8 We used the bing-
body/apr10/ ) N-Gram model.
We considered a text unit composed of the lexi-
cal item and a contextual window of 4 words to the
left and 4 words to the right (words being separated
by spaces). For example, in the following sentence,
we tested ?He brings an incredibly rich and diverse
background that?, and the same unit with the tar-
get word replaced by substitutes, for example ?He
brings an incredibly lush and diverse background
that?.
7http://research.microsoft.com/en-us/
collaboration/focus/cs/web-ngram.aspx
8http://web-ngram.research.microsoft.
com/info/MicrosoftNgram-1.02.zip
490
< i n s t a n c e i d =? 118 ?>
<c o n t e x t>He b r i n g s an i n c r e d i b l y
<head> r i c h< / head> and d i v e r s e
background t h a t i n c l u d e s e v e r y t h i n g
from e x e c u t i v e c o a c h i n g , l e a r n i n g
&amp ; deve lopmen t and management
c o n s u l t i n g , t o s e n i o r o p e r a t i o n s
r o l e s , mixed wi th a m a s t e r s i n
o r g a n i z a t i o n a l
deve lopmen t .< / c o n t e x t>
< / i n s t a n c e>
We performed several tests, with different N-
Gram models, and different context sizes. Some of
these results for the trial corpus are given in table 4.
Table 4: Results obtained with Microsoft Web N-gram
Service, on the trial corpus
Size of left context Size of right context Score
0 3 0.362
3 0 0.358
2 2 0.365
3 3 0.358
4 4 0.370
For the evaluation, this system was our second
run, with the parameters that obtained the best scores
on the training corpus (contexts of 4 words to the
left and to the right). This method obtained a 0.370
score on the trial corpus and a 0.396 score on the test
corpus.9
7 Combination of methods
As each method seemed to have its own benefits, we
tried to combine them using SVMRank 10(Joachims,
2006). The output of each system is converted into
a feature file. For example, the output of the Simple
English Wikipedia based system begins with:
1 bright 475 1
1 intelligent 206 2
1 smart 201 3
1 clever 141 4
2 light 3241 1
2 clear 707 2
9This result is different from the official one, because an
incorrect file was submitted at the time.
10http://www.cs.cornell.edu/people/tj/
svm_light/svm_rank.html
2 bright 475 3
2 luminous 14 4
2 well-lit 0 5
The first column represent the instance id, the sec-
ond one the considered substitute, the third one the
feature (in this case, the frequency of the substitute
in the Simple English Wikipedia), and the last one,
the substitute rank according to this method. Then,
we combined these files to include all features (after
basic query-wise feature scaling). For example, the
training file begins with:
1 qid:1 2:-0.00461061395325929
3:0.0345010535723618
#intelligent
2 qid:1 2:-0.00485010755325339
3:-0.0213467053270483 #clever
3 qid:1 2:-0.00462903653787422
3:0.092640777900771 #smart
4 qid:1 2:-0.00361947890097599
3:0.0489145618699556 #bright
1 qid:4 2:-0.00461061395325929
3:0.0345010535723618
#intelligent
The first column gives the gold standard rank for
the substitute (in training phase), the second one the
instance id, and then feature ids and values for each
substitute. Default parameters were used.
We used the division of the trial corpus into a
training corpus and a development corpus. Table 5
gives some examples of scores obtained by combin-
ing two methods. The scores are not exactly those
presented earlier, since they correspond to a part of
the trial corpus only. Even though some improve-
ment can be obtained by this combination, it was
quite small, and so we did not use it for the evalua-
tion.
Table 5: Results obtained with combination of methods
with SVMRank, on the trial corpus
Simple English Microsoft
SVM
Wikipedia NGrams
0.352 0.352 0.354
491
8 Conclusion
In this paper, we present several systems developed
for the English Lexical Simplification task of Se-
mEval 2012. The best results are obtained using fre-
quencies from the Simple English Wikipedia. We
found the task quite hard to solve, since none of
our experiments significantly outperforms the Sim-
ple Frequency baseline. On the trial corpus, our
system based upon the Simple English Wikipedia
achieved a score of 0.381 (below the 0.399 base-
line score); on the test corpus, we achieved a score
of 0.465 with the Simple English Wikipedia system
while the baseline achieved a score of 0.471 score.
All our systems using contextual information did not
achieve high scores.
References
Thorsten Joachims. 2006. Training Linear SVMs in Lin-
ear Time. In Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD).
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In Proc. of the Interna-
tional Conference on New Methods in Language Pro-
cessing, Manchester, UK.
Lucia Specia, Sujay K. Jauhar, and Rada Mihalcea. 2012.
SemEval-2012 Task 1: English Lexical Simplification.
In Proc. of the 6th International Workshop on Seman-
tic Evaluation (SemEval 2012), Montre?al, Canada.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A Monolingual Tree-based Translation Model
for Sentence Simplification. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (COLING 2010), pages 1353?1361.
492
