Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, pages 23?26,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Active Learning of Extractive Reference Summaries
for Lecture Speech Summarization
Justin Jian Zhang and Pascale Fung
Human Language Technology Center
Department of Electronic and Computer Engineering
University of Science and Technology (HKUST)
Clear Water Bay,Hong Kong
{zjustin,pascale}@ece.ust.hk
Abstract
We propose using active learning for tag-
ging extractive reference summary of lec-
ture speech. The training process of
feature-based summarization model usu-
ally requires a large amount of train-
ing data with high-quality reference sum-
maries. Human production of such sum-
maries is tedious, and since inter-labeler
agreement is low, very unreliable. Ac-
tive learning helps assuage this problem by
automatically selecting a small amount of
unlabeled documents for humans to hand
correct. Our method chooses the unla-
beled documents according to the similar-
ity score between the document and the
comparable resource?PowerPoint slides.
After manual correction, the selected doc-
uments are returned to the training pool.
Summarization results show an increasing
learning curve of ROUGE-L F-measure,
from 0.44 to 0.514, consistently higher
than that of using randomly chosen train-
ing samples.
Index Terms: active learning, summarization
1 Introduction
The need for the summarization of classroom lec-
tures, conference speeches, political speeches is
ever increasing with the advent of remote learning,
distributed collaboration and electronic archiving.
These user needs cannot be sufficiently met by
short abstracts. In recent years, virtually all sum-
marization systems are extractive - compiling bul-
let points from the document using some saliency
criteria. Reference summaries are often manu-
ally compiled by one or multiple human annota-
tors (Fujii et al, 2008; Nenkova et al, 2007). Un-
like for speech recognition where the reference
sentence is clear and unambiguous, and unlike
for machine translation where there are guidelines
for manual translating reference sentences, there
is no clear guideline for compiling a good ref-
erence summary. As a result, one of the most
important challenges in speech summarization re-
mains the difficulty to compile, evaluate and thus
to learn what a good summary is. Human judges
tend to agree on obviously good and very bad
summaries but cannot agree on borderline cases.
Consequently, annotator agreement is low. Refer-
ence summary generation is a tedious and low ef-
ficiency task. On the other hand, supervised learn-
ing of extractive summarization requires a large
amount of training data of reference summaries.
To reduce the amount of human annotation effort
and improve annotator agreement on the reference
summaries, we propose that active learning (selec-
tive sampling) is one possible solution.
Active learning has been applied to NLP tasks
such as spoken language understanding (Tur et al,
2005), information extraction (Shen et al, 2004),
and text classification (Lewis and Catlett, 1994;
McCallum and Nigam, 1998; Tong and Koller,
2002). Different from supervised learning which
needs the entire corpus with manual labeling re-
sult, active learning selects the most useful exam-
ples for labeling and requires manual labeling of
training dataset to re-train model.
In this paper, we suggest a framework of refer-
ence summary annotation with relatively high in-
ter labeler agreement based on the rhetorical struc-
ture in presentation slides. Based on this frame-
work, we further propose a certainty-based active
learning method to alleviate the burden of human
annotation of training data.
The rest of this paper is organized as follows:
Section 2 depicts the corpus for our experiments,
the extractive summarizer, and outlines the acous-
tic/prosodic, and linguistic feature sets for repre-
senting each sentence. Section 3 depicts how to
23
compile reference summaries with high inter la-
beler agreement by using the RDTW algorithm
and our active learning algorithm for tagging ex-
tractive reference summary. We describe our ex-
periments and evaluate the results in Section 4.
Our conclusion follows in Section 5.
2 Experimental Setup
2.1 The Corpus
Our lecture speech corpus (Zhang et al, 2008)
contains 111 presentations recorded from the
NCMMSC2005 and NCMMSC2007 conferences
for evaluating our approach. The man-
ual transcriptions and the comparable corpus?
PowerPoint slides are also collected. Each presen-
tation lasts for 15 minutes on average. We select
71 of the 111 presentations with well organized
PowerPoint slides that always have clear sketches
and evidently aligned with the transcriptions. We
use about 90% of the lecture corpus from the 65
presentations as original unlabeled data U and the
remaining 6 presentations as held-out test set. We
randomly select 5 presentations from U as our
seed presentations. Reference summaries of the
seed presentations and the presentations of test set
are generated from the PowerPoint slides and pre-
sentation transcriptions using RDTW followed by
manual correction, as described in Section 3.
2.2 SVM Classifier and the Feature Set
While (Ribeiro and de Matos, 2007) has shown
that MMR (maximum marginal relevance) ap-
proach is superior to feature-based classifica-
tion for summarizing Portuguese broadcast news
data, another work on Japanese lecture speech
drew the opposite conclusion (Fujii et al, 2008)
that feature-based classification method is bet-
ter. Therefore we continue to use the feature-
based method in our work. We consider the ex-
tractive summarization as a binary classification
problem, we predict whether each sentence of the
lecture transcription should be in a summary or
not. We use Radial Basis Function (RBF) ker-
nel for constructing SVM classifier, which is pro-
vided by LIBSVM, a library for support vector
machines (Chang and Lin, 2001). We represent
each sentence by a feature vector which consists of
acoustic features: duration of the sentence, aver-
age syllable Duration, F0 information features, en-
ergy information features; and linguistic features:
length of the sentence counted by word and TFIDF
information features, as shown in (Zhang et al,
2008). We then build the SVM classifier as our
summarizer based on these sentence feature vec-
tors.
3 Active Learning for Tagging Reference
Summary and Summarization
Similar to (Hayama et al, 2005; Kan, 2007), we
have previously proposed how presentation slides
are used to compile reference summaries automat-
ically (Zhang et al, 2008). The motivations be-
hind this procedure are:
? presentation slides are compiled by the au-
thors themselves and therefore provide a
good standard summary of their work;
? presentation slides contain the hierarchical
rhetorical structure of lecture speech as the ti-
tles, subtitles, page breaks, bullet points pro-
vide an enriched set of discourse information
that are otherwise not apparent in the spoken
lecture transcriptions.
We propose a Relaxed Dynamic Time Warping
(RDTW) procedure, which is identical to Dy-
namic Programming and Edit Distance, to align
sentences from the slides to those in the lecture
speech transcriptions, resulting in automatically
extracted reference summaries.
We calculate the similarity scores
matrix Sim = (sij), where sij =
similarity(Senttrans[i], Sentslides[j]), be-
tween the sentences in the transcription and
the sentences in the slides. We then obtain
the distance matrix Dist = (dij), where
dij = 1?sij . We calculate the initial warp path P:
P = (pini1 , ..., pinin , ..., piniN ) by DTW, where pinin
is represented by sentence pair(iinin , jinin ): one
from transcription, the other from slides. Con-
sidering that the lecturer often doesn?t follow the
flow of his/her slides strictly, we adopt Relaxed
Dynamic Time Warping (RDTW) for finding the
optimal warp path, by the following equation.
?
??
??
ioptn = iinin
joptn =
jinin +Cargmin
j=jinin ?C
dioptn ,j
(1)
We consider the transcription sentences on this
path as reference summary sentences. We then
obtain the optimal path (popt1 , ..., poptn , ..., poptN ),
where poptn is represented by (ioptn , joptn ) and C
24
is the capacity to relax the path. We then select
the sentences ioptn of the transcription whose sim-
ilarity scores of sentence pairs: (ioptn , joptn ), are
higher than the pre-defined threshold as the refer-
ence summary sentences. The advantage of using
these summaries as references is that it circum-
vents the disagreement between multiple human
annotators.
We have compared these reference summaries
to human-labeled summaries. When asked to ?se-
lect the most salient sentences for a summary?, we
found that inter-annotator agreement ranges from
30% to 50% only. Sometimes even a single per-
son might choose different sentences at different
times (Nenkova et al, 2007). However, when in-
structed to follow the structure and points in the
presentation slides, inter-annotator agreement in-
creased to 80%. The agreement between auto-
matically extracted reference summary and hu-
mans also reaches 75%. Based on this high degree
of agreement, we generate reference summaries
by asking a human to manually correct those ex-
tracted by the RDTW algorithm. Our reference
summaries therefore make for more reliable train-
ing and test data.
For a transcribed presentation D with a se-
quence of recognized sentences {s1, s2, ..., sN},
we want to find the sentences to be classified
as summary sentences by using the salient sen-
tence classification function c(). In a probabilis-
tic framework, the extractive summarization task
is equivalent to estimating P (c(??s n) = 1|D) of
each sentence sn, where ??s n is the feature vec-
tor with acoustic and linguistic features of the sen-
tence sn.
We propose an active learning approach where a
small set of transcriptions as seeds with reference
summaries, created by the RDTW algorithm and
human correction, are used to train the seed model
for the summarization classifier, and then the clas-
sifier is used to label data from a unlabel pool. At
each iteration, human annotators choose the unla-
beled documents whose similarity scores between
the extracted summary sentences and the Power-
Point slides sentences are top-N highest for label-
ing summary sentences. Formally, this approach
is described in Algorithm 1.
Given document D: {s1, s2, ..., sN}, we cal-
culate the similarity score between the extracted
summary sentences: {s?1, s?2, ..., s?K} and the Pow-
erPoint slide sentences: {ppts1, ppts2, ..., pptsL},
by equation 2.
Scoresim(D) = 1K
K?
n=1
L?
j=1
Sim(s?n, pptsj) (2)
4 Experimental Results and Evaluation
Algorithm 1 Active learning for tagging extrac-
tive reference summary and summarization
Initialization
For an unlabeled data set: Uall, i = 0
(1) Randomly choose a small set of data X{i}
from Uall; U{i} = Uall ?X{i}
(2) Manually label each sentence in X{i} as
summary or non-summary by RDTW and hu-
man correction and save these sentences and
their labels in L{i}
Active Learning Process
(3) X{i} = null
(4) Train the classifier M{i} using L{i}
(5) Test U{i} by M{i}
(6) Calculate similarity score of given docu-
ment D between the extracted summary sen-
tences and the PowerPoint slides sentences by
equation 2
(7) Select the documents with top-five highest
similarity scores from U{i}
(8) Save selected samples into X{i}
(9) Manually correct each sentence label in
X{i} as summary or non-summary
(10) L{i + 1} = L{i} + X{i}
(11) U{i + 1} = U{i} ?X{i}
(12) Evaluate M{i} on the testing set E
(13) i = i+1, and repeat from (3) until U{i} is
empty or M{i} obtains satisfying performance
(14) M{i} is produced and the process ends
We start our experiments by randomly choosing
six documents for manual labeling. We gradually
increase the training data pool by choosing five
more documents each time for manual correction.
We carry out two sets of experiments for compar-
ing our algorithm and random selection. We evalu-
ate the summarizer by ROUGE-L (summary-level
Longest Common Subsequence) F-measure (Lin,
2004).
The performance of our algorithm is illustrated
by the increasing ROUGE-L F-measure curve in
Figure 1. It is shown to be consistently higher than
25
Figure 1: Active learning vs. random selection
using randomly chosen samples. We also find that
by using only 51 documents for training, the per-
formance of the summarization model achieved
by our approach is better than that of the model
trained by random selection using all 65 presen-
tations (0.514 vs. 0.512 ROUGE-L F-measure).
This shows that our active learning approach re-
quires 22% less training data. Besides, acoustic
features can improve the performance of active
learning of speech summarization. Without acous-
tic features, our summarizer only performs 0.47
ROUGE-L F-measure.
5 Conclusion and Discussion
In this paper, we propose using active learning re-
duce the need for human annotation for tagging
extractive reference summary of lecture speech
summarization. We use RDTW to extract sen-
tences from transcriptions according to Power-
Point slides, and these sentences are then hand
corrected as reference summaries. The unlabeled
documents are selected whose similarity scores
between the extracted summary sentences and the
PowerPoint slides sentences are top-N highest for
labeling summary sentences. We then use an SVM
classifier to extract summary sentences. Summa-
rization results show an increasing learning curve
of F-measure, from 0.44 to 0.514, consistently
higher than that of using randomly chosen train-
ing data samples. Besides, acoustic features play
a significant role in active learning of speech sum-
marization. In our future work, we will try to ap-
ply different criteria, such as uncertainty-based or
committee-based criteria, for selecting samples to
be labeled, and compare the effectiveness of them.
6 Acknowledgements
This work is partially supported by GRF612806 of
the Hong Kong RGC.
References
C.C. Chang and C.J. Lin. 2001. LIBSVM: a library for sup-
port vector machines. Software available at http://www.
csie. ntu. edu. tw/cjlin/libsvm, 80:604?611.
Y. Fujii, K. Yamamoto, N. Kitaoka, and S. Nakagawa. 2008.
Class Lecture Summarization Taking into Account Con-
secutiveness of Important Sentences. In Proceedings of
Interspeech, pages 2438?2441.
T. Hayama, H. Nanba, and S. Kunifuji. 2005. Alignment
between a technical paper and presentation sheets using
a hidden markov model. In Active Media Technology,
2005.(AMT 2005). Proceedings of the 2005 International
Conference on, pages 102?106.
M.Y. Kan. 2007. SlideSeer: A digital library of aligned
document and presentation pairs. In Proceedings of the
7th ACM/IEEE-CS joint conference on Digital libraries,
pages 81?90. ACM New York, NY, USA.
D.D. Lewis and J. Catlett. 1994. Heterogeneous uncertainty
sampling for supervised learning. In Proceedings of the
Eleventh International Conference on Machine Learning,
pages 148?156. Morgan Kaufmann.
C.Y. Lin. 2004. Rouge: A Package for Automatic Evalua-
tion of Summaries. Proceedings of the Workshop on Text
Summarization Branches Out (WAS 2004), pages 25?26.
A. McCallum and K. Nigam. 1998. Employing EM in Pool-
based Active Learning for Text Classification. In Proceed-
ings of ICML, pages 350?358.
A. Nenkova, R. Passonneau, and K. McKeown. 2007. The
Pyramid Method: Incorporating human content selection
variation in summarization evaluation. ACM Transactions
on Speech and Language Processing (TSLP), 4(2).
R. Ribeiro and D.M. de Matos. 2007. Extractive Summa-
rization of Broadcast News: Comparing Strategies for Eu-
ropean Portuguese. Lecture Notes in Computer Science,
4629:115.
D. Shen, J. Zhang, J. Su, G. Zhou, and C.L. Tan. 2004.
Multi-criteria-based Active Learning for Named Entity
Recognition. In Proceedings of 42th Annual Meeting of
the Association for Computational Linguistics. Associa-
tion for Computational Linguistics Morristown, NJ, USA.
S. Tong and D. Koller. 2002. Support vector machine ac-
tive learning with applications to text classification. The
Journal of Machine Learning Research, 2:45?66.
G. Tur, D. Hakkani-Tr, and R. E. Schapiro. 2005. Combin-
ing Active and Semi-supervised Learning for Spoken Lan-
guage Understanding. Speech Communications, 45:171?
186.
J.J. Zhang, S. Huang, and P. Fung. 2008. RSHMM++ for
extractive lecture speech summarization. In IEEE Spoken
Language Technology Workshop, 2008. SLT 2008, pages
161?164.
26
