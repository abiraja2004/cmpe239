Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 9?16,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Re-examining Automatic Keyphrase Extraction Approaches
in Scientific Articles
Su Nam Kim
CSSE dept.
University of Melbourne
snkim@csse.unimelb.edu.au
Min-Yen Kan
Department of Computer Science
National University of Singapore
kanmy@comp.nus.edu.sg
Abstract
We tackle two major issues in automatic
keyphrase extraction using scientific arti-
cles: candidate selection and feature engi-
neering. To develop an efficient candidate
selection method, we analyze the nature
and variation of keyphrases and then se-
lect candidates using regular expressions.
Secondly, we re-examine the existing fea-
tures broadly used for the supervised ap-
proach, exploring different ways to en-
hance their performance. While most
other approaches are supervised, we also
study the optimal features for unsuper-
vised keyphrase extraction. Our research
has shown that effective candidate selec-
tion leads to better performance as evalua-
tion accounts for candidate coverage. Our
work also attests that many of existing fea-
tures are also usable in unsupervised ex-
traction.
1 Introduction
Keyphrases are simplex nouns or noun phrases
(NPs) that represent the key ideas of the document.
Keyphrases can serve as a representative summary
of the document and also serve as high quality in-
dex terms. It is thus no surprise that keyphrases
have been utilized to acquire critical information
as well as to improve the quality of natural lan-
guage processing (NLP) applications such as doc-
ument summarizer(Da?vanzo and Magnini, 2005),
information retrieval (IR)(Gutwin et al, 1999) and
document clustering(Hammouda et al, 2005).
In the past, various attempts have been made to
boost automatic keyphrase extraction performance
based primarily on statistics(Frank et al, 1999;
Turney, 2003; Park et al, 2004; Wan and Xiao,
2008) and a rich set of heuristic features(Barker
and Corrnacchia, 2000; Medelyan and Witten,
2006; Nguyen and Kan, 2007). In Section 2, we
give a more comprehensive overview of previous
attempts.
Current keyphrase technology still has much
room for improvement. First of all, although sev-
eral candidate selection methods have been pro-
posed for automatic keyphrase extraction in the
past (e.g. (Frank et al, 1999; Park et al, 2004;
Nguyen and Kan, 2007)), most of them do not ef-
fectively deal with various keyphrase forms which
results in the ignorance of some keyphrases as can-
didates. Moreover, no studies thus far have done
a detailed investigation of the nature and varia-
tion of manually-provided keyphrases. As a con-
sequence, the community lacks a standardized list
of candidate forms, which leads to difficulties in
direct comparison across techniques during evalu-
ation and hinders re-usability.
Secondly, previous studies have shown the ef-
fectiveness of their own features but not many
compared their features with other existing fea-
tures. That leads to a redundancy in studies and
hinders direct comparison. In addition, existing
features are specifically designed for supervised
approaches with few exceptions. However, this
approach involves a large amount of manual labor,
thus reducing its utility for real-world application.
Hence, unsupervised approach is inevitable in or-
der to minimize manual tasks and to encourage
utilization. It is a worthy study to attest the re-
liability and re-usability for the unsupervised ap-
proach in order to set up the tentative guideline for
applications.
This paper targets to resolve these issues of
candidate selection and feature engineering. In
our work on candidate selection, we analyze the
nature and variation of keyphrases with the pur-
pose of proposing a candidate selection method
which improves the coverage of candidates that
occur in various forms. Our second contribution
re-examines existing keyphrase extraction features
9
reported in the literature, in terms of their effec-
tiveness and re-usability. We test and compare
the usefulness of each feature for further improve-
ment. In addition, we assess how well these fea-
tures can be applied in an unsupervised approach.
In the remaining sections, we describe an
overview of related work in Section 2, our propos-
als on candidate selection and feature engineering
in Section 4 and 5, our system architecture and
data in Section 6. Then, we evaluate our propos-
als, discuss outcomes and conclude our work in
Section 7, 8 and 9, respectively.
2 Related Work
The majority of related work has been carried
out using statistical approaches, a rich set of
symbolic resources and linguistically-motivated
heuristics(Frank et al, 1999; Turney, 1999; Barker
and Corrnacchia, 2000; Matsuo and Ishizuka,
2004; Nguyen and Kan, 2007). Features used can
be categorized into three broad groups: (1) docu-
ment cohesion features (i.e. relationship between
document and keyphrases)(Frank et al, 1999;
Matsuo and Ishizuka, 2004; Medelyan and Wit-
ten, 2006; Nguyen and Kan, 2007), and to lesser,
(2) keyphrase cohesion features (i.e. relationship
among keyphrases)(Turney, 2003) and (3) term
cohesion features (i.e. relationship among compo-
nents in a keyphrase)(Park et al, 2004).
The simplest system is KEA (Frank et al,
1999; Witten et al, 1999) that uses TF*IDF (i.e.
term frequency * inverse document frequency) and
first occurrence in the document. TF*IDF mea-
sures the document cohesion and the first occur-
rence implies the importance of the abstract or
introduction which indicates the keyphrases have
a locality. Turney (2003) added the notion of
keyphrase cohesion to KEA features and Nguyen
and Kan (2007) added linguistic features such
as section information and suffix sequence. The
GenEx system(Turney, 1999) employed an inven-
tory of nine syntactic features, such as length in
words and frequency of stemming phrase as a
set of parametrized heuristic rules. Barker and
Corrnacchia (2000) introduced a method based
on head noun heuristics that took three features:
length of candidate, frequency and head noun fre-
quency. To take advantage of domain knowledge,
Hulth et al (2001) used a hierarchically-organized
domain-specific thesaurus from Swedish Parlia-
ment as a secondary knowledge source. The
Textract (Park et al, 2004) also ranks the can-
didate keyphrases by its judgment of keyphrases?
degree of domain specificity based on subject-
specific collocations(Damerau, 1993), in addi-
tion to term cohesion using Dice coefficient(Dice,
1945). Recently, Wan and Xiao (2008) extracts
automatic keyphrases from single documents, uti-
lizing document clustering information. The as-
sumption behind this work is that the documents
with the same or similar topics interact with each
other in terms of salience of words. The authors
first clustered the documents then used the graph-
based ranking algorithm to rank the candidates in
a document by making use of mutual influences of
other documents in the same cluster.
3 Keyphrase Analysis
In previous study, KEA employed the index-
ing words as candidates whereas others such as
(Park et al, 2004; Nguyen and Kan, 2007) gen-
erated handcrafted regular expression rules. How-
ever, none carefully undertook the analysis of
keyphrases. We believe there is more to be learned
from the reference keyphrases themselves by do-
ing a fine-grained, careful analysis of their form
and composition. Note that we used the articles
collected from ACM digital library for both ana-
lyzing keyphrases as well as evaluating methods.
See Section 6 for data in detail.
Syntactically, keyphrases can be formed by ei-
ther simplex nouns (e.g. algorithm, keyphrase,
multi-agent) or noun phrases (NPs) which can be a
sequence of nouns and their auxiliary words such
as adjectives and adverbs (e.g. mobile network,
fast computing, partially observable Markov de-
cision process) despite few incidences. They can
also incorporate a prepositional phrase (PP) (e.g.
quality of service, policy of distributed caching).
When keyphrases take the form of an NP with an
attached PP (i.e. NPs in of-PP form), the preposi-
tion of is most common, but others such as for, in,
via also occur (e.g. incentive for cooperation, in-
equality in welfare, agent security via approximate
policy, trade in financial instrument based on log-
ical formula). The patterns above correlate well
to part-of-speech (POS) patterns used in modern
keyphrase extraction systems.
However, our analysis uncovered additional lin-
guistic patterns and alternations which other stud-
ies may have overlooked. In our study we also
found that keyphrases also occur as a simple con-
10
Criteria Rules
Frequency (Rule1) Frequency heuristic i.e. frequency ? 2 for simplex words vs. frequency ? 1 for NPs
Length (Rule2) Length heuristic i.e. up to length 3 for NPs in non-of-PP form vs. up to length 4 for NPs in of-PP form
(e.g. synchronous concurrent program vs. model of multiagent interaction)
Alternation (Rule3) of-PP form alternation
(e.g. number of sensor = sensor number, history of past encounter = past encounter history)
(Rule4) Possessive alternation
(e.g. agent?s goal = goal of agent, security?s value = value of security)
Extraction (Rule5) Noun Phrase = (NN |NNS|NNP |NNPS|JJ |JJR|JJS)?(NN |NNS|NNP |NNPS)
(e.g. complexity, effective algorithm, grid computing, distributed web-service discovery architecture)
(Rule6) Simplex Word/NP IN Simplex Word/NP
(e.g. quality of service, sensitivity of VOIP traffic (VOIP traffic extracted),
simplified instantiation of zebroid (simplified instantiation extracted))
Table 1: Candidate Selection Rules
junctions (e.g. search and rescue, propagation and
delivery), and much more rarely, as conjunctions
of more complex NPs (e.g. history of past en-
counter and transitivity). Some keyphrases appear
to be more complex (e.g. pervasive document edit
and management system, task and resource allo-
cation in agent system). Similarly, abbreviations
and possessive forms figure as common patterns
(e.g. belief desire intention = BDI, inverse docu-
ment frequency = (IDF); Bayes? theorem, agent?s
dominant strategy).
A critical insight of our work is that keyphrases
can be morphologically and semantically altered.
Keyphrases that incorporate a PP or have an un-
derlying genitive composition are often easily var-
ied by word order alternation. Previous studies
have used the altered keyphrases when forming in
of-PP form. For example, quality of service can
be altered to service quality, sometimes with lit-
tle semantic difference. Also, as most morpho-
logical variation in English relates to noun num-
ber and verb inflection, keyphrases are subject to
these rules as well (e.g. distributed system 6= dis-
tributing system, dynamical caching 6= dynamical
cache). In addition, possessives tend to alternate
with of-PP form (e.g. agent?s goal = goal of agent,
security?s value = value of security).
4 Candidate Selection
We now describe our proposed candidate selection
process. Candidate selection is a crucial step for
automatic keyphrase extraction. This step is corre-
lated to term extraction study since top Nth ranked
terms become keyphrases in documents. In pre-
vious study, KEA employed the indexing words
as candidates whereas others such as (Park et al,
2004; Nguyen and Kan, 2007) generated hand-
crafted regular expression rules. However, none
carefully undertook the analysis of keyphrases. In
this section, before we present our method, we first
describe the detail of keyphrase analysis.
In our keyphrase analysis, we observed that
most of author assigned keyphrase and/or reader
assigned keyphrase are syntactically more of-
ten simplex words and less often NPs. When
keyphrases take an NP form, they tend to be a sim-
ple form of NPs. i.e. either without a PP or with
only a PP or with a conjunction, but few appear as
a mixture of such forms. We also noticed that the
components of NPs are normally nouns and adjec-
tives but rarely, are adverbs and verbs. As a re-
sult, we decided to ignore NPs containing adverbs
and verbs in this study as our candidates since they
tend to produce more errors and to require more
complexity.
Another observation is that keyphrases contain-
ing more than three words are rare (i.e. 6% in our
data set), validating what Paukkeri et al (2008)
observed. Hence, we apply a length heuristic. Our
candidate selection rule collects candidates up to
length 3, but also of length 4 for NPs in of-PP
form, since they may have a non-genetive alter-
nation that reduces its length to 3 (e.g. perfor-
mance of distributed system = distributed system
performance). In previous studies, words occur-
ring at least twice are selected as candidates. How-
ever, during our acquisition of reader assigned
keyphrase, we observed that readers tend to collect
NPs as keyphrases, regardless of their frequency.
Due to this, we apply different frequency thresh-
olds for simplex words (>= 2) and NPs (>= 1).
Note that 30% of NPs occurred only once in our
data.
Finally, we generated regular expression rules
to extract candidates, as presented in Table 1. Our
candidate extraction rules are based on those in
Nguyen and Kan (2007). However, our Rule6
for NPs in of-PP form broadens the coverage of
11
possible candidates. i.e. with a given NPs in of-
PP form, not only we collect simplex word(s),
but we also extract non-of-PP form of NPs from
noun phrases governing the PP and the PP. For
example, our rule extracts effective algorithm of
grid computing as well as effective algorithm and
grid computing as candidates while the previous
works? rules do not.
5 Feature Engineering
With a wider candidate selection criteria, the onus
of filtering out irrelevant candidates becomes the
responsibility of careful feature engineering. We
list 25 features that we have found useful in ex-
tracting keyphrases, comprising of 9 existing and
16 novel and/or modified features that we intro-
duce in our work (marked with ?). As one of
our goals in feature engineering is to assess the
suitability of features in the unsupervised setting,
we have also indicated which features are suitable
only for the supervised setting (S) or applicable to
both (S, U).
5.1 Document Cohesion
Document cohesion indicates how important the
candidates are for the given document. The most
popular feature for this cohesion is TF*IDF but
some works have also used context words to check
the correlation between candidates and the given
document. Other features for document cohesion
are distance, section information and so on. We
note that listed features other than TF*IDF are re-
lated to locality. That is, the intuition behind these
features is that keyphrases tend to appear in spe-
cific area such as the beginning and the end of doc-
uments.
F1 : TF*IDF (S,U) TF*IDF indicates doc-
ument cohesion by looking at the frequency of
terms in the documents and is broadly used in pre-
vious work(Frank et al, 1999; Witten et al, 1999;
Nguyen and Kan, 2007). However, a disadvan-
tage of the feature is in requiring a large corpus
to compute useful IDF. As an alternative, con-
text words(Matsuo and Ishizuka, 2004) can also
be used to measure document cohesion. From our
study of keyphrases, we saw that substrings within
longer candidates need to be properly counted, and
as such our method measures TF in substrings as
well as in exact matches. For example, grid com-
puting is often a substring of other phrases such as
grid computing algorithm and efficient grid com-
puting algorithm. We also normalize TF with re-
spect to candidate types: i.e. we separately treat
simplex words and NPs to compute TF. To make
our IDFs broadly representative, we employed the
Google n-gram counts, that were computed
over terabytes of data. Given this large, generic
source of word count, IDF can be incorporated
without corpus-dependent processing, hence such
features are useful in unsupervised approaches
as well. The following list shows variations of
TF*IDF, employed as features in our system.
? (F1a) TF*IDF
? (F1b*) TF including counts of substrings
? (F1c*) TF of substring as a separate feature
? (F1d*) normalized TF by candidate types
(i.e. simplex words vs. NPs)
? (F1e*) normalized TF by candidate types as
a separate feature
? (F1f*) IDF using Google n-gram
F2 : First Occurrence (S,U) KEA used the first
appearance of the word in the document(Frank et
al., 1999; Witten et al, 1999). The main idea
behind this feature is that keyphrases tend to oc-
cur in the beginning of documents, especially in
structured reports (e.g., in abstract and introduc-
tion sections) and newswire.
F3 : Section Information (S,U) Nguyen and
Kan (2007) used the identity of which specific
document section a candidate occurs in. This lo-
cality feature attempts to identify key sections. For
example, in their study of scientific papers, the
authors weighted candidates differently depending
on whether they occurred in the abstract, introduc-
tion, conclusion, section head, title and/or refer-
ences.
F4* : Additional Section Information (S,U)
We first added the related work or previous work
as one of section information not included in
Nguyen and Kan (2007). We also propose and test
a number of variations. We used the substrings
that occur in section headers and reference titles
as keyphrases. We counted the co-occurrence of
candidates (i.e. the section TF) across all key sec-
tions that indicates the correlation among key sec-
tions. We assign section-specific weights as in-
dividual sections exhibit different propensities for
generating keyphrases. For example, introduction
12
contains the majority of keyphrases while the ti-
tle or section head contains many fewer due to the
variation in size.
? (F4a*) section, ?related/previous work?
? (F4b*) counting substring occurring in key
sections
? (F4c*) section TF across all key sections
? (F4d*) weighting key sections according to
the portion of keyphrases found
F5* : Last Occurrence (S,U) Similar to dis-
tance in KEA , the position of the last occurrence
of a candidate may also imply the importance of
keyphrases, as keyphrases tend to appear in the
last part of document such as the conclusion and
discussion.
5.2 Keyphrase Cohesion
The intuition behind using keyphrase cohesion is
that actual keyphrases are often associated with
each other, since they are semantically related to
topic of the document. Note that this assumption
holds only when the document describes a single,
coherent topic ? a document that represents a col-
lection may be first need to be segmented into its
constituent topics.
F6* : Co-occurrence of Another Candidate
in Section (S,U) When candidates co-occur in
several key sections together, then they are more
likely keyphrases. Hence, we used the number of
sections that candidates co-occur.
F7* : Title overlap (S) In a way, titles also rep-
resent the topics of their documents. A large col-
lection of titles in the domain can act as a prob-
abilistic prior of what words could stand as con-
stituent words in keyphrases. In our work, as we
examined scientific papers from computer science,
we used a collection of titles obtained from the
large CiteSeer1 collection to create this feature.
? (F7a*) co-occurrence (Boolean) in title col-
location
? (F7b*) co-occurrence (TF) in title collection
F8 : Keyphrase Cohesion (S,U) Turney (2003)
integrated keyphrase cohesion into his system by
checking the semantic similarity between top N
ranked candidates against the remainder. In the
1It contains 1.3M titles from articles, papers and reports.
original work, a large, external web corpus was
used to obtain the similarity judgments. As we
did not have access to the same web corpus and
all candidates/keyphrases were not found in the
Google n-gram corpus, we approximated this fea-
ture using a similar notion of contextual similarity.
We simulated a latent 2-dimensional matrix (simi-
lar to latent semantic analysis) by listing all candi-
date words in rows and their neighboring words
(nouns, verbs, and adjectives only) in columns.
The cosine measure is then used to compute the
similarity among keyphrases.
5.3 Term Cohesion
Term cohesion further refines the candidacy judg-
ment, by incorporating an internal analysis of the
candidate?s constituent words. Term cohesion
posits that high values for internal word associa-
tion measures correlates indicates that the candi-
date is a keyphrase (Church and Hanks, 1989).
F9 : Term Cohesion (S,U) Park et al (2004)
used in the Dice coefficient (Dice, 1945)
to measure term cohesion particularly for multi-
word terms. In their work, as NPs are longer than
simplex words, they simply discounted simplex
word cohesion by 10%. In our work, we vary the
measure of TF used in Dice coefficient,
similar to our discussion earlier.
? (F9a) term cohesion by (Park et al, 2004),
? (F9b*) normalized TF by candidate types
(i.e. simplex words vs. NPs),
? (F9c*) applying different weight by candi-
date types,
? (F9d*) normalized TF and different weight-
ing by candidate types
5.4 Other Features
F10 : Acronym (S) Nguyen and Kan (2007) ac-
counted for the importance of acronym as a fea-
ture. We found that this feature is heavily depen-
dent on the data set. Hence, we used it only for
N&K to attest our candidate selection method.
F11 : POS sequence (S) Hulth and Megyesi
(2006) pointed out that POS sequences of
keyphrases are similar. It showed the distinctive
distribution of POS sequences of keyphrases and
use them as a feature. Like acronym, this is also
subject to the data set.
13
F12 : Suffix sequence (S) Similar to acronym,
Nguyen and Kan (2007) also used a candidate?s
suffix sequence as a feature, to capture the propen-
sity of English to use certain Latin derivational
morphology for technical keyphrases. This fea-
ture is also a data dependent features, thus used in
supervised approach only.
F13 : Length of Keyphrases (S,U) Barker and
Corrnacchia (2000) showed that candidate length
is also a useful feature in extraction as well as in
candidate selection, as the majority of keyphrases
are one or two terms in length.
6 System and Data
To assess the performance of the proposed candi-
date selection rules and features, we implemented
a keyphrase extraction pipe line. We start with
raw text of computer science articles converted
from PDF by pdftotext. Then, we parti-
tioned the into section such as title and sections
via heuristic rules and applied sentence segmenter
2, ParsCit3(Councill et al, 2008) for refer-
ence collection, part-of-speech tagger4 and lem-
matizer5(Minnen et al, 2001) of the input. Af-
ter preprocessing, we built both supervised and
unsupervised classifiers using Naive Bayes from
the WEKA machine learning toolkit(Witten and
Frank, 2005), Maximum Entropy6, and simple
weighting.
In evaluation, we collected 250 papers from
four different categories7 of the ACM digital li-
brary. Each paper was 6 to 8 pages on average.
In author assigned keyphrase, we found many
were missing or found as substrings. To rem-
edy this, we collected reader assigned keyphrase
by hiring senior year undergraduates in computer
science, each whom annotated five of the papers
with an annotation guideline and on average, took
about 15 minutes to annotate each paper. The fi-
nal statistics of keyphrases is presented in Table
2 where Combined represents the total number of
keyphrases. The numbers in () denotes the num-
ber of keyphrases in of-PP form. Found means the
2http://www.eng.ritsumei.ac.jp/asao/resources/sentseg/
3http://wing.comp.nus.edu.sg/parsCit/
4http://search.cpan.org/dist/Lingua-EN-
Tagger/Tagger.pm
5http://www.informatics.susx.ac.uk/research/groups/nlp/carroll/morph.html
6http://maxent.sourceforge.net/index.html
7C2.4 (Distributed Systems), H3.3 (Information Search
and Retrieval), I2.11 (Distributed Artificial Intelligence-
Multiagent Systems) and J4 (Social and Behavioral Sciences-
Economics)
number of author assigned keyphrase and reader
assigned keyphrase found in the documents.
Author Reader Combined
Total 1252 (53) 3110 (111) 3816 (146)
NPs 904 2537 3027
Average 3.85 (4.01) 12.44 (12.88) 15.26 (15.85)
Found 769 2509 2864
Table 2: Statistics in Keyphrases
7 Evaluation
The baseline system for both the supervised and
unsupervised approaches is modified N&K which
uses TF*IDF, distance, section information and
additional section information (i.e. F1-4). Apart
from baseline , we also implemented basic
KEA and N&K to compare. Note that N&K is con-
sidered a supervised approach, as it utilizes fea-
tures like acronym, POS sequence, and suffix se-
quence.
Table 3 and 4 shows the performance of our can-
didate selection method and features with respect
to supervised and unsupervised approaches using
the current standard evaluation method (i.e. exact
matching scheme) over top 5th, 10th, 15th candi-
dates.
BestFeatures includes F1c:TF of substring as
a separate feature, F2:first occurrence, F3:section
information, F4d:weighting key sections, F5:last
occurrence, F6:co-occurrence of another candi-
date in section, F7b:title overlap, F9a:term co-
hesion by (Park et al, 2004), F13:length of
keyphrases. Best-TF*IDF means using all best
features but TF*IDF.
In Tables 3 and 4, C denotes the classifier tech-
nique: unsupervised (U) or supervised using Max-
imum Entropy (S)8.
In Table 5, the performance of each feature is
measured using N&K system and the target fea-
ture. + indicates an improvement, - indicates a
performance decline, and ? indicates no effect
or unconfirmed due to small changes of perfor-
mances. Again, supervised denotes Maximum
Entropy training and Unsupervised is our unsu-
pervised approach.
8 Discussion
We compared the performances over our candi-
date selection and feature engineering with sim-
ple KEA , N&K and our baseline system. In eval-
uating candidate selection, we found that longer
8Due to the page limits, we present the best performance.
14
Method Features C Five Ten Fifteen
Match Precision Recall Fscore Match Precising Recall Fscore Match Precision Recall Fscore
All KEA U 0.03 0.64% 0.21% 0.32% 0.09 0.92% 0.60% 0.73% 0.13 0.88% 0.86% 0.87%
Candidates S 0.79 15.84% 5.19% 7.82% 1.39 13.88% 9.09% 10.99% 1.84 12.24% 12.03% 12.13%
N&K S 1.32 26.48% 8.67% 13.06% 2.04 20.36% 13.34% 16.12% 2.54 16.93% 16.64% 16.78%
baseline U 0.92 18.32% 6.00% 9.04% 1.57 15.68% 10.27% 12.41% 2.20 14.64% 14.39% 14.51%
S 1.15 23.04% 7.55% 11.37% 1.90 18.96% 12.42% 15.01% 2.44 16.24% 15.96% 16.10%
Length<=3 KEA U 0.03 0.64% 0.21% 0.32% 0.09 0.92% 0.60% 0.73% 0.13 0.88% 0.86% 0.87%
Candidates S 0.81 16.16% 5.29% 7.97% 1.40 14.00% 9.17% 11.08% 1.84 12.24% 12.03% 12.13%
N&K S 1.40 27.92% 9.15% 13.78% 2.10 21.04% 13.78% 16.65% 2.62 17.49% 17.19% 17.34%
baseline U 0.92 18.4% 6.03% 9.08% 1.58 15.76% 10.32% 12.47% 2.20 14.64% 14.39% 14.51%
S 1.18 23.68% 7.76% 11.69% 1.90 19.00% 12.45% 15.04% 2.40 16.00% 15.72% 15.86%
Length<=3 KEA U 0.01 0.24% 0.08% 0.12% 0.05 0.52% 0.34% 0.41% 0.07 0.48% 0.47% 0.47%
Candidates S 0.83 16.64% 5.45% 8.21% 1.42 14.24% 9.33% 11.27% 1.87 12.45% 12.24% 12.34%
+ Alternation N&K S 1.53 30.64% 10.04% 15.12% 2.31 23.08% 15.12% 18.27% 2.88 19.20% 18.87% 19.03%
baseline U 0.98 19.68% 6.45% 9.72% 1.72 17.24% 11.29% 13.64% 2.37 15.79% 15.51% 15.65%
S 1.33 26.56% 8.70% 13.11% 2.09 20.88% 13.68% 16.53% 2.69 17.92% 17.61% 17.76%
Table 3: Performance on Proposed Candidate Selection
Features C Five Ten Fifteen
Match Prec. Recall Fscore Match Prec. Recall Fscore Match Prec. Recall Fscore
Best U 1.14 .228 .747 .113 1.92 .192 .126 .152 2.61 .174 .171 .173
S 1.56 .312 .102 .154 2.50 .250 .164 .198 3.15 .210 .206 .208
Best U 1.14 .228 .74 .113 1.92 .192 .126 .152 2.61 .174 .171 .173
w/o TF*IDF S 1.56 .311 .102 .154 2.46 .246 .161 .194 3.12 .208 .204 .206
Table 4: Performance on Feature Engineering
A Method Feature
+ S F1a,F2,F3,F4a,F4d,F9a
U F1a,F1c,F2,F3,F4a,F4d,F5,F7b,F9a
- S F1b,F1c,F1d,F1f,F4b,F4c,F7a,F7b,F9b-d,F13
U F1d,F1e,F1f,F4b,F4c,F6,F7a,F9b-d
? S F1e,F10,F11,F12
U F1b
Table 5: Performance on Each Feature
length candidates play a role to be noises so de-
creased the overall performance. We also con-
firmed that candidate alternation offered the flexi-
bility of keyphrases leading higher candidate cov-
erage as well as better performance.
To re-examine features, we analyzed the impact
of existing and new features and their variations.
First of all, unlike previous studies, we found that
the performance with and without TF*IDF did not
lead to a large difference which indicates the im-
pact of TF*IDF was minor, as long as other fea-
tures are incorporated. Secondly, counting sub-
strings for TF improved performance, while ap-
plying term weighting for TF and/or IDF did not
impact on the performance. We estimated the
cause that many of keyphrases are substrings of
candidates and vice versa. Thirdly, section in-
formation was also validated to improve perfor-
mance, as in Nguyen and Kan (2007). Extend-
ing this logic, modeling additional section infor-
mation (related work) and weighting sections both
turned out to be useful features. Other locality
features were also validated as helpful: both first
occurrence and last occurrence are helpful as it
implies the locality of the key ideas. In addi-
tion, keyphrase co-occurrence with selected sec-
tions was proposed in our work and found empiri-
cally useful. Term cohesion (Park et al, 2004) is a
useful feature although it has a heuristic factor that
reduce the weight by 10% for simplex words. Nor-
mally, term cohesion is subject to NPs only, hence
it needs to be extended to work with multi-word
NPs as well. Table 5 summarizes the reflections
on each feature.
As unsupervised methods have the appeal of not
needing to be trained on expensive hand-annotated
data, we also compared the performance of super-
vised and unsupervised methods. Given the fea-
tures initially introduced for supervised learning,
unsupervised performance is surprisingly high.
While supervised classifier produced a matching
count of 3.15, the unsupervised classifier obtains a
count of 2.61. We feel this indicates that the exist-
ing features for supervised methods are also suit-
able for use in unsupervised methods, with slightly
reduced performance. In general, we observed that
the best features in both supervised and unsuper-
vised methods are the same ? section information
and candidate length. In our analysis of the im-
pact of individual features, we observed that most
features affect performance in the same way for
both supervised and unsupervised approaches, as
shown in Table 5. These findings indicate that al-
though these features may be been originally de-
signed for use in a supervised approach, they are
stable and can be expected to perform similar in
unsupervised approaches.
15
9 Conclusion
We have identified and tackled two core issues
in automatic keyphrase extraction: candidate se-
lection and feature engineering. In the area of
candidate selection, we observe variations and al-
ternations that were previously unaccounted for.
Our selection rules expand the scope of possible
keyphrase coverage, while not overly expanding
the total number candidates to consider. In our
re-examination of feature engineering, we com-
piled a comprehensive feature list from previous
works while exploring the use of substrings in de-
vising new features. Moreover, we also attested to
each feature?s fitness for use in unsupervised ap-
proaches, in order to utilize them in real-world ap-
plications with minimal cost.
10 Acknowledgement
This work was partially supported by a National Research
Foundation grant, Interactive Media Search (grant # R 252
000 325 279), while the first author was a postdoctoral fellow
at the National University of Singapore.
References
Ken Barker and Nadia Corrnacchia. Using noun phrase
heads to extract document keyphrases. In Proceedings of
the 13th Biennial Conference of the Canadian Society on
Computational Studies of Intelligence: Advances in Arti-
ficial Intelligence. 2000.
Regina Barzilay and Michael Elhadad. Using lexical chains
for text summarization. In Proceedings of the ACL/EACL
1997 Workshop on Intelligent Scalable Text Summariza-
tion. 1997, pp. 10?17.
Kenneth Church and Patrick Hanks. Word association
norms, mutual information and lexicography. In Proceed-
ings of ACL. 1989, 76?83.
Isaac Councill and C. Lee Giles and Min-Yen Kan. ParsCit:
An open-source CRF reference string parsing package. In
Proceedings of LREC. 2008, 28?30.
Ernesto DA?vanzo and Bernado Magnini. A Keyphrase-
Based Approach to Summarization:the LAKE System at
DUC-2005. In Proceedings of DUC. 2005.
F. Damerau. Generating and evaluating domain-oriented
multi-word terms from texts. Information Processing and
Management. 1993, 29, pp.43?447.
Lee Dice. Measures of the amount of ecologic associations
between species. Journal of Ecology. 1945, 2.
Eibe Frank and Gordon Paynter and Ian Witten and Carl
Gutwin and Craig Nevill-manning. Domain Specific
Keyphrase Extraction. In Proceedings of IJCAI. 1999,
pp.668?673.
Carl Gutwin and Gordon Paynter and Ian Witten and Craig
Nevill-Manning and Eibe Frank. Improving browsing in
digital libraries with keyphrase indexes. Journal of Deci-
sion Support Systems. 1999, 27, pp.81?104.
Khaled Hammouda and Diego Matute and Mohamed Kamel.
CorePhrase: keyphrase extraction for document cluster-
ing. In Proceedings of MLDM. 2005.
Annette Hulth and Jussi Karlgren and Anna Jonsson and
Henrik Bostrm and Lars Asker. Automatic Keyword Ex-
traction using Domain Knowledge. In Proceedings of CI-
CLing. 2001.
Annette Hulth and Beata Megyesi. A study on automatically
extracted keywords in text categorization. In Proceedings
of ACL/COLING. 2006, 537?544.
Mario Jarmasz and Caroline Barriere. Using semantic sim-
ilarity over tera-byte corpus, compute the performance of
keyphrase extraction. In Proceedings of CLINE. 2004.
Dawn Lawrie and W. Bruce Croft and Arnold Rosenberg.
Finding Topic Words for Hierarchical Summarization. In
Proceedings of SIGIR. 2001, pp. 349?357.
Y. Matsuo and M. Ishizuka. Keyword Extraction from a Sin-
gle Document using Word Co-occurrence Statistical Infor-
mation. In International Journal on Artificial Intelligence
Tools. 2004, 13(1), pp. 157?169.
Olena Medelyan and Ian Witten. Thesaurus based automatic
keyphrase indexing. In Proceedings of ACM/IEED-CS
joint conference on Digital libraries. 2006, pp.296?297.
Guido Minnen and John Carroll and Darren Pearce. Applied
morphological processing of English. NLE. 2001, 7(3),
pp.207?223.
Thuy Dung Nguyen and Min-Yen Kan. Key phrase Extrac-
tion in Scientific Publications. In Proceeding of ICADL.
2007, pp.317-326.
Youngja Park and Roy Byrd and Branimir Boguraev. Auto-
matic Glossary Extraction Beyond Terminology Identifi-
cation. In Proceedings of COLING. 2004, pp.48?55.
Mari-Sanna Paukkeri and Ilari Nieminen and Matti Polla
and Timo Honkela. A Language-Independent Approach
to Keyphrase Extraction and Evaluation. In Proceedings
of COLING. 2008.
Peter Turney. Learning to Extract Keyphrases from Text.
In National Research Council, Institute for Information
Technology, Technical Report ERB-1057. 1999.
Peter Turney. Coherent keyphrase extraction via Web min-
ing. In Proceedings of IJCAI. 2003, pp. 434?439.
Xiaojun Wan and Jianguo Xiao. CollabRank: towards a col-
laborative approach to single-document keyphrase extrac-
tion. In Proceedings of COLING. 2008.
Ian Witten and Gordon Paynter and Eibe Frank and Car
Gutwin and Graig Nevill-Manning. KEA:Practical Au-
tomatic Key phrase Extraction. In Proceedings of ACM
DL. 1999, pp.254?256.
Ian Witten and Eibe Frank. Data Mining: Practical Ma-
chine Learning Tools and Techniques. Morgan Kauf-
mann, 2005.
Yongzheng Zhang and Nur Zinchir-Heywood and Evange-
los Milios. Term based Clustering and Summarization of
Web Page Collections. In Proceedings of Conference of
the Canadian Society for Computational Studies of Intel-
ligence. 2004.
16
