First Joint Conference on Lexical and Computational Semantics (*SEM), pages 706?709,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
BUAP: Lexical and Semantic Similarity for Cross-lingual Textual
Entailment
Darnes Vilarin?o, David Pinto, Mireya Tovar, Saul Leo?n, Esteban Castillo
Beneme?rita Universidad Auto?noma de Puebla,
Faculty of Computer Science
14 Sur & Av. San Claudio, CU
Puebla, Puebla, Me?xico
{darnes, dpinto, mtovar}@cs.buap.mx
saul.ls@live.com, ecjbuap@gmail.com
Abstract
In this paper we present a report of the two di-
fferent runs submitted to the task 8 of Semeval
2012 for the evaluation of Cross-lingual Tex-
tual Entailment in the framework of Content
Synchronization. Both approaches are based
on textual similarity, and the entailment judg-
ment (bidirectional, forward, backward or no
entailment) is given based on a set of decision
rules. The first approach uses textual simi-
larity on the translated and original versions
of the texts, whereas the second approach ex-
pands the terms by means of synonyms. The
evaluation of both approaches show a similar
behavior which is still close to the average and
median.
1 Introduction
Cross-lingual Textual Entailment (CLTE) has been
recently proposed by (Mehdad et al., 2010; Mehdad
et al., 2011) as an extension of the Textual Entail-
ment task (Dagan and Glickman, 2004). Given a text
(T ) and an hypothesis (H) in different languages,
the CLTE task consists of determining if the mea-
ning of H can be inferred from the meaning of T .
In this paper we present a report of the obtained
results after submitting two different runs for the
Task 8 of Semeval 2012, named ?Cross-lingual Tex-
tual Entailment for Content Synchronization? (Negri
et al., 2012). In this task, the Cross-Lingual Tex-
tual Entailment addresses textual entailment recog-
nition under a new dimension (cross-linguality), and
within a new challenging application scenario (con-
tent synchronization). The task 8 of Semeval 2012
may be formally defined as follows:
Given a pair of topically related text fragments
(T1 and T2) in different languages, the task consists
of automatically annotating it with one of the follo-
wing entailment judgments:
? Bidirectional (T1 ? T2 & T1 ? T2): the two
fragments entail each other (semantic equiva-
lence)
? Forward (T1 ? T2 & T1 ! ? T2): unidirec-
tional entailment from T1 to T2
? Backward (T1 ! ? T2 & T1 ? T2): unidirec-
tional entailment from T2 to T1
? No Entailment (T1 ! ? T2 & T1 ! ? T2): there
is no entailment between T1 and T2
In this task, both T1 and T2 are assumed to be
TRUE statements; hence in the dataset there are no
contradictory pairs. Cross-lingual datasets are avai-
lable for the following language combinations:
? Spanish/English (SPA-ENG)
? German/English (DEU-ENG)
? Italian/English (ITA-ENG)
? French/English (FRA-ENG)
The remaining of this paper is structured as fo-
llows: Section 2 describes the two different approa-
ches presented in the competition. The obtained re-
sults are shown and dicussed in Section 3. Finally,
the findings of this work are given in Section 4.
706
2 Experimental setup
For this experiment we have considered to tackle the
CLTE task by means of textual similarity and textual
length. In particular, the textual similarity is used to
determine whether some kind of entailment exists or
not. We have established the threshold of 0.5 for the
similarity function as evidence of textual entailment.
Since the two sentences to be evaluated are written
in two different languages, we have translated each
sentence to the other language, so that, we have two
sentences in English, and two sentences in the origi-
nal language (Spanish, German, Italian and French).
We have used the Google translate for this purpose
1
.
The corpora used in the experiments comes from
a cross-lingual Textual Entailment dataset presented
in (Negri et al., 2011), and provided by the task orga-
nizers. We have employed the training dataset only
for adjust some parameters of the system, but the
approach is knowledge-based and, therefore, it does
not need a training corpus. Both, the training and
test corpus contain 500 sentences for each language.
The textual length is used to determine the entail-
ment judgment (bidirectional, forward, backward,
no entailment). We have basically, assumed that the
length of a text may give some evidence of the type
of entailment. The decision rules used for determi-
ning the entailment judgment are described in Sec-
tion 2.3.
In this competition we have submitted two diffe-
rent runs which differ with respect to the type of tex-
tual similarity used (lexical vs semantic). The first
one, calculates the similarity using only the trans-
lated version of the original sentences, whereas the
second approach uses text expansion by means of
synonyms and, thereafter, it calculates the similarity
between the pair of sentences.
Let T1 be the sentence in the original language,
T2 the T1 topically related text fragment (written in
English). Let T3 be the English translation of T1,
and T4 the translation of T2 to the original language
(Spanish, German, Italian and French). The formal
description of these two approaches are given as fo-
llows.
1http://translate.google.com.mx/
2.1 Approach 1: Lexical similarity
The evidence of textual entailment between T1 and
T2 is calculated using two formulae of lexical si-
milarity. Firstly, we determine the similarity bet-
ween the two texts written in the source language
(SimS). Additionally, we calculate the lexical simi-
larity between the two sentences written in the target
language (SimT ), in this case English.
Given the limited text length of the text fragments,
we have used the Jaccard coefficient as similarity
measure. Eq. (1) shows the lexical similarity for the
two texts written in the original language, whereas,
Eq. (2) presents the Jaccard coefficient for the texts
written in English.
simS = simJaccard(T1, T4) =
|T1 ? T4|
|T1 ? T4|
(1)
simT = simJaccard(T2, T3) =
|T2 ? T3|
|T2 ? T3|
(2)
2.2 Approach 2: Semantic similarity
In this case we calculate the semantic similarity bet-
ween the two texts written in the original language
(simS), and the semantic similarity between the two
text fragments written in English (simT ). The se-
mantic level of similarity is given by considering
the synonyms of each term for each sentence (in
the original and target language). For this purpose,
we have employed five dictionaries containing syno-
nyms for the five different languages considered in
the competition (English, Spanish, German, Italian,
and French)2. In Table 1 we show the number of
terms, so as the number of synonyms in average by
term considered for each language.
Let T1 = w1,1w1,2...w1,|T1|, T2 =
w2,1w2,2...w2,|T2| be the source and target
sentences, and let T3 = w3,1w3,2...w3,|T3|,
T4 = w4,1w4,2...w4,|T4| be translated version of the
original source and target sentences, respectively.
The synonyms of a given word wi,k, expressed as
synset(wi,k), are obtained from the aforementioned
dictionaries by extracting the synonyms of wi,k. In
order to obtain a better matching between the terms
contained in the text fragments and the terms in the
2http://extensions.services.openoffice.org/en/dictionaries
707
Table 1: Dictionaries of synonyms used for term expan-
sion
Language Terms synonyms per term
(average)
English 2,764 60
Spanish 9,887 45
German 21,958 115
Italian 25,724 56
French 36,207 93
dictionary, we have stemmed all the terms using the
Porter stemmer.
In order to determine the semantic similarity bet-
ween two terms of sentences written in the source
language (w1,i and w4,j) we use Eq. (3). The se-
mantic similariy between two terms of the English
sentences are calculated as shown in Eq. (4).
sim(w1,i, w4,j) =
?
?
?
?
?
?
?
?
?
1 if (w1,i == w4,j) ||
w1,i ? synset(w4,j) ||
w4,j ? synset(w1,i)
0 otherwise
(3)
sim(w2,i, w3,j) =
?
?
?
?
?
?
?
?
?
1 if (w2,i == w3,j) ||
w2,i ? synset(w3,j) ||
w3,j ? synset(w2,i)
0 otherwise
(4)
Both equations consider the existence of semantic
similarity when the two words are identical, or when
the some of the two words appear in the synonym set
of the other word.
The semantic similarity of the complete text frag-
ments T1 and T4 (simS) is calculated as shown in
Eq. (5). Whereas, the semantic similarity of the
complete text fragments T2 and T3 (simT ) is cal-
culated as shown in Eq. (6).
simS(T1, T4) =
?|T1|
i=1
?|T4|
j=1 sim(w1,i,w4,j)
|T1?T4|
(5)
simT (T2, T3) =
?|T2|
i=1
?|T3|
j=1 sim(w2,i,w3,j)
|T2?T3|
(6)
2.3 Decision rules
Both approches used the same decision rules in or-
der to determine the entailment judgment for a given
pair of text fragments (T1 and T2). The following al-
gorithm shows the decision rules used.
Algorithm 1.
If |T2| < |T3| then
If (simT > 0.5 and simS > 0.5)
then forward
ElseIf |T2| > |T3| then
If (simT > 0.5 and simS > 0.5)
then backward
ElseIf (|T1| == |T4| and |T2| == |T3|) then
If (simT > 0.5 and simS > 0.5)
then bidirectional
Else no entailment
As mentioned above, the rules employed the le-
xical or semantic textual similarity, and the textual
length for determining the textual entailment.
3 Results
In Table 2 we show the overall results obtained by
the two approaches submitted to the competition.
We also show the highest, lowest, average and me-
dian overall results obtained in the competition.
SPA-
ENG
ITA-
ENG
FRA-
ENG
DEU-
ENG
Highest 0.632 0.566 0.57 0.558
Average 0.407 0.362 0.366 0.357
Median 0.346 0.336 0.336 0.336
Lowest 0.266 0.278 0.278 0.262
BUAP run1 0.35 0.336 0.334 0.33
BUAP run2 0.366 0.344 0.342 0.268
Table 2: Overall statistics obtained in the Task 8 of Se-
meval 2012
The runs submitted perform similar, but the se-
mantic approach obtained a slightly better perfor-
mance. The two results are above the median but
below the average. We consider that better results
may be obtained if the two features used (textual si-
milarity and textual length) were introduced into a
supervised classifier, so that, the decision rules were
approximated on the basis of a training dataset, ins-
tead of the empirical setting done in this work. Fu-
ture experiments will be carried out in this direction.
708
4 Discussion and conclusion
Two different approaches for the Cross-lingual Tex-
tual Entailment for Content Synchronization task of
Semeval 2012 are reported in this paper. We used
two features for determining the textual entailment
judgment between two texts T1 and T2 (written in
two different languages). The first approach pro-
posed used lexical similarity, meanwhile the second
used semantic similarity by means of term expan-
sion with synonyms.
Even if the performance of both approaches is
above the median and slighly below the average,
we consider that we may easily improve this perfor-
mance by using syntactic features of the text frag-
ments. Additionally, we are planning to integrate
some supervised techniques based on decision rules
which may be trained in a supervised dataset. Future
experiments will be executed in this direction.
Acknowledgments
This project has been partially supported by projects
CONACYT #106625, #VIAD-ING11-II and VIEP
#PIAD-ING11-II.
References
