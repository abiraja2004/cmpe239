Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 1?6,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Finding the Right Supervisor: Expert-Finding in a University Domain
Fawaz Alarfaj, Udo Kruschwitz, David Hunter and Chris Fox
School of Computer Science and Electronic Engineering
University of Essex
Colchester, CO4 3SQ, UK
{falarf, udo, dkhunter, foxcj}@essex.ac.uk
Abstract
Effective knowledge management is a key fac-
tor in the development and success of any or-
ganisation. Many different methods have been
devised to address this need. Applying these
methods to identify the experts within an or-
ganisation has attracted a lot of attention. We
look at one such problem that arises within
universities on a daily basis but has attracted
little attention in the literature, namely the
problem of a searcher who is trying to iden-
tify a potential PhD supervisor, or, from the
perspective of the university?s research office,
to allocate a PhD application to a suitable su-
pervisor. We reduce this problem to identify-
ing a ranked list of experts for a given query
(representing a research area).
We report on experiments to find experts in a
university domain using two different meth-
ods to extract a ranked list of candidates:
a database-driven method and a data-driven
method. The first one is based on a fixed list
of experts (e.g. all members of academic staff)
while the second method is based on auto-
matic Named-Entity Recognition (NER). We
use a graded weighting based on proximity be-
tween query and candidate name to rank the
list of candidates. As a baseline, we use a
system that ranks candidates simply based on
frequency of occurrence within the top docu-
ments.
1 Introduction
The knowledge and expertise of individuals are sig-
nificant resources for organisation. Managing this
intangible resource effectively and efficiently con-
stitutes an essential and very important task (Non-
aka and Takeuchi, 1995; Law and Ngai, 2008). Ap-
proaching experts is the primary and most direct way
of utilising their knowledge (Yang and Huh, 2008;
Li et al., 2011). Therefore, it is important to have a
means of locating the right experts within organisa-
tions. The expert-finding task can be categorised as
an information retrieval task similar to a web search,
but where the results are people rather than docu-
ments. An expert-finding system allows users to in-
put a query, and it returns a ranked list of experts.
Here we look at a university context. We start
with a real-world problem which is to identify a list
of experts within an academic environment, e.g. a
university intranet. The research reported here is
based on an empirical study of a simple but effective
method in which a system that applies the concept of
expert-finding has been designed and implemented.
The proposed system will contribute to provide an
expert-search service to all of the university?s stake-
holders.
Expert-finding systems require two main re-
sources in order to function: a list of candidates and
a collection of data from which the evidence of ex-
pertise can be extracted. We present two approaches
to address this problem, a database-driven and a
data-driven method using NER. The main differ-
ence between the two methods is the way in which
the candidates? list is constructed. In the database
method, the candidates are simply selected from a
known list of experts, e.g. the university?s academic
staff. In the NER method, the candidates are ex-
tracted automatically from the pages returned by an
1
underlying search engine. This method promises to
be more useful for finding experts from a wider (and
possibly more up-to-date) range of candidates. Both
methods apply the same ranking function(s), as will
be discussed below.
This paper will survey related work in Section 2
and introduce the expert-finding task in a university
domain in Section 3. The process of ranking experts
will be discussed in Section 4. The evaluation will
be described in Section 4, followed by a discussion
of the experiment?s results in Section 5.
2 Related Work
The expert-finding task addresses the problem of re-
trieving a ranked list of people who are knowledge-
able about a given topic. This task has found its
place in the commercial environment as early as the
1980?s, as discussed in Maybury (2006); however,
there was very limited academic research on finding
and ranking experts until the introduction of the en-
terprise track at the 2005 Text REtrieval Conference
(TREC) (Craswell et al., 2005).
When expert-finding we must know the experts?
profiles, These profiles may be generated manually
or automatically. Manually created profiles may be
problematic. If, for example, experts enter their own
information, they may exaggerate or downplay their
expertise. In addition, any changes of expertise for
any expert requires a manual update to the expert?s
profile. Thus incurring high maintenance costs. An
example of manually generated profiles is the work
of Dumais and Nielsen (1992). Although their sys-
tem automatically assigns submitted manuscripts to
reviewers, the profiles of the reviewers or experts are
created manually.
The alternative is to generate the profiles automat-
ically, for example by extracting relevant informa-
tion from a document collection. The assumption is
that individuals will tend to be expert in the topics
of documents with which they are associated. Ex-
perts can be associated with the documents in which
they are mentioned (Craswell et al., 2001) or with
e-mails they have sent or received (Balog and de Ri-
jke, 2006a; Campbell et al., 2003; Dom et al., 2003).
They can also be associated with their home pages
or CVs (Maybury et al., 2001), and with documents
they have written (Maybury et al., 2001; Becerra-
Fernandez, 2000). Finally, some researchers use
search logs to associate experts with the web pages
they have visited (Wang et al., 2002; Macdonald and
White, 2009).
After associating candidate experts with one or
more of the kinds of textual evidence mentioned
above, the next step is to find and rank candidates
based on a user query. Many methods have been
proposed to perform this task. Craswell et al. (2001)
create virtual documents for each candidate (or em-
ployee). These virtual documents are simply con-
catenated texts of all documents from the corpus as-
sociated with a particular candidate. Afterwards,
the system indexes and processes queries for the
employee?s documents. The results would show a
list of experts based on the ten best matching em-
ployee documents. Liu et al. (2005) have applied
expert-search in the context of a community-based
question-answering service. Based on a virtual doc-
ument approach, their work applied three language
models: the query likelihood model, the relevance
model and the cluster-based language model. They
concluded that combining language models can en-
hance the retrieval performance.
Two principal approaches recognised for expert-
finding can be found in the literature. Both were
first proposed by Balog et al. (2006b). The mod-
els are called the candidate model and the doc-
ument model, or Model 1 and Model 2, respec-
tively. Different names have been used for the
two methods. Fang and Zhai (2007) refer to them
as ?Candidate Generation Models and Topic Gen-
eration Models?. Petkova and Croft (2006) call
them the ?Query-Dependent Model? and the ?Query-
Independent Model?. The main difference between
the models is that the candidate-based approaches
(Model 1) build a textual representation of candi-
date experts, and then rank the candidates based on
the given query, whereas the document-based ap-
proaches (Model 2) first find documents that are rel-
evant to the query, and then locate the associated ex-
perts in these documents.
Balog et al. (2006b) have compared the two mod-
els and concluded that Model 2 outperforms Model
1 on all measures (for this reason, we will adopt
Model 2).
As Model 2 proved to be more efficient, it formed
the basis of many other expert-search systems (Fang
2
and Zhai, 2007; Petkova and Croft, 2007; Yao
et al., 2008). Fang and Zhai developed a mixture
model using proximity-based document representa-
tion. This model makes it possible to put different
weights on different representations of a candidate
expert (Fang and Zhai, 2007). Another mixture of
personal and global language models was proposed
by Serdyukov and Hiemstra (2008). They combined
two criteria for personal expertise in the final rank-
ing: the probability of generation of the query by the
personal language model and a prior probability of
candidate experts that expresses their level of activ-
ity in the important discussions on the query topic.
Zhu et al. (2010) claimed that earlier language
models did not consider document features. They
proposed an approach that incorporates: internal
document structure; document URLs; page rank;
anchor texts; and multiple levels of association be-
tween experts and topics.
All of the proposed frameworks assume that the
more documents associated with a candidate that
score highly with respect to a query, the more likely
the candidate is to have relevant expertise for that
query. Macdonald and Ounis (2008) developed a
different approach, called the Voting Model. In their
model, candidate experts are ranked first by consid-
ering a ranking of documents with respect to the
users? query. Then, using the candidate profiles,
votes from the ranked documents are converted into
votes for candidates.
There have been attempts to tackle the expert-
finding problem using social networks. This has
mainly been investigated from two directions. The
first direction uses graph-based measures on social
networks to produce a ranking of experts (Campbell
et al., 2003; Dom et al., 2003). The second direc-
tion assumes similarities among the neighbours in a
social network and defines a smoothing procedure
to rank experts (Karimzadehgan et al., 2009; Zhang
et al., 2007).
Some have argued that it is not enough to find ex-
perts by looking only at the queries? without tak-
ing the users into consideration. They claim that
there are several factors that may play a role in
decisions concerning which experts to recommend.
Some of these factors are the users? expertise level,
social proximity and physical proximity (Borgatti
and Cross, 2003; McDonald and Ackerman, 1998;
Shami et al., 2008). McDonald and Ackerman
(1998) emphasised the importance of the accessi-
bility of the expert. They argued that people usu-
ally prefer to contact the experts who are physically
or organisationally close to them. Moreover, Shami
et al. (2008) found that people prefer to contact ex-
perts they know, even when they could potentially
receive more information from other experts who are
located outside their social network.
Woudstra and van den Hooff (2008) identified a
number of factors in selecting experts that are re-
lated to quality and accessibility. They argued that
the process of choosing which candidate expert to
contact might differ depending on the specific situa-
tion.
Hofmann et al. (2010) showed that many of these
factors can be modelled. They claimed that integrat-
ing them with retrieval models can improve retrieval
performance. Smirnova and Balog (2011) provided
a user-oriented model for expert-finding where they
placed an emphasis on the social distance between
the user and the expert. They considered a number
of social graphs based on organisational hierarchy,
geographical location and collaboration.
3 Expert-Finding in a University
In any higher educational institution, finding an ap-
propriate supervisor is a critical task for research
students, a task that can be very time consuming,
especially if academics describe their work using
terms that a student is not familiar with. A searcher
may build up a picture of who is likely to have
the relevant expertise by looking for university aca-
demic staff who have written numerous documents
about the general topic, who have authored docu-
ments exactly related to the topic, or who list the
topic as one of their research interest areas. Au-
tomating this process will not only help research stu-
dents find the most suitable supervisors, but it also
allow the university to allocate applications to super-
visors, and help researchers find other people inter-
ested in the particular topics.
3.1 Method
The two approaches we apply, database-driven, and
data-driven using NER1 are illustrated in Figure 1.
1We use OpenNLP to identify named entities.
3
Search API URL Filter
Page ReaderHTML Parser
Normalizer
Candidate 
Extractor
Candidate 
Evaluator
Named-Entity 
Recognition 
Candidate 
Evaluator
DB Connection
Update 
Candidate Rank
Display Experts
Query URLs Filtered URLs
Web Page
String
Parsed Html
Page
Normalized 
Web Page
String
Candidate List 
Normalized 
Web Page
String
Candidate
List
Candidate Rank 
For Page
Candidate Rank 
For Page
Named-Entity Recognition 
Method Database Method
Figure 1: System Architecture.
The main difference between the two methods is the
way in which the candidates? list is constructed. We
argue that each method has its advantages. In the
database method, the candidates are simply the uni-
versity?s academic staff. This avoids giving results
unrelated to the university. It would be appropriate if
the aim is to find the experts from among the univer-
sity academics. In the data-driven method, the can-
didates are extracted from the pages returned by the
underlying search engine. The experts found by this
method are not necessarily university staff. They
could be former academics, PhD students, visiting
professors, or newly appointed staff.
Both methods apply the same ranking functions,
one baseline function which is purely based on fre-
quency and one which takes into account proximity
of query terms with matches of potential candidates
in the retrieved set of documents.
3.2 The Baseline Approach
The baseline we chose for ranking candidates is the
frequency of appearance of names in the top twenty
retrieved documents. The system counts how many
times the candidate?s name appears in the document
d(cc). Then it calculates the candidate metric cm by
dividing the candidate count d(cc) by the number of
tokens in the document d(nt).
Equation 1 defines the metric, where cm is the
final candidate?s metric for all documents and n is
the number of documents.
cm =
n?
d=1
d(cc)
d(nt)
(1)
3.3 Our Approach
Our approach takes into account the proximity be-
tween query terms and candidate names in the
matching documents in the form of a distance
weight. This measure will adds a distance weight
value to the main candidate?s metric that was gener-
ated earlier. Similar approaches have been proposed
in the literature for different expert search applica-
tions Lu et al. (2006); Cao et al. (2005). The dis-
tance weight will be higher whenever the name ap-
pears closer to the query term, within a +/- 10 word
window.
We experiment with two different formulae. The
first formula is as follows:
cm1 =
n?
i=1
m?
j=1
(cm+
1
? ? ?ij
), ?ij =
{
dij if dij ? 10
0 Otherwise
(2)
where n is the number of times the candidate?s name
has been found in the matching documents, m is the
number of times the (full) query has been identified,
and dij is the distance between the name position
and query position (? has been set empirically to 3).
The second formula is:
cm2 =
n?
i=1
m?
j=1
(cm+
1
cm?ij
), ?ij =
{
dij if dij ? 10
0 Otherwise
(3)
This equation is designed to return a smaller value
as the distance x increases, and to give the candidate
with lower frequency a higher weight.
In both cases, candidates are ranked according to
the final score and displayed in order so that the can-
didates who are most likely to be experts are dis-
played at the top of the list.
4 Evaluation
As with any IR system, evaluation can be difficult.
In the given context one might argue that precision
4
is more important than recall. In any case, recall can
be difficult to measure precisely. To address these is-
sues we approximate a gold standard as follows. We
selected one school within the university for which
a page of research topics with corresponding aca-
demics exists In this experiment we take this map-
ping as a complete set of correct matches. In this
page, there are 371 topics (i.e. potential queries) di-
vided among 28 more general research topics. Each
topic/query is associated with one or more of the
school?s academic staff. It is presumed that those
names belong to experts on the corresponding top-
ics.
Table 1 illustrates some general topics with the
number of (sub)topic they contain. Table 2 list some
of the topics.
Topic N
Analogue and Digital Systems Architectures 2
Artificial Intelligence 26
Audio 12
Brain Computer Interface 18
Computational Finance Economics and Management 1
Computational Intelligence 10
. . . . . .
Table 1: Distribution of topics - N denotes the number of
topics for the corresponding general topic area.
High-Speed Lasers And Photodetectors
Human Behaviour And The Psychology
Human Motion Tracking
Human-Centred Robotics
Hybrid Heuristics
Hybrid Intelligent Systems Which Include Neuro-Fuzzy Systems
Hypercomplex Algebras And Fourier Transforms
Hypercomplex Fourier Transforms And Filters
Table 2: Some topics/queries
The measure used to test the system is recall at
the following values {3, 5, 7, 10, 15, 20}. We
also measure Mean Average Precision at rank 20
(MAP@20).
5 Results and Discussion
Table 3 shows the system results where BL is the
baseline result. There are two main findings. First
of all, the database-driven approach outperforms
the data-driven approach. Secondly, our approach
which applies a grading of results based on prox-
imity between queries and potential expert names
significantly outperforms the baseline approach that
only considers frequency, that is true for both formu-
lae we apply when ranking the results (using paired
t-tests applied to MAP with p<0.0001). However,
the differences between cm1 and cm2 tend not to be
significantly different.
BL cm1 cm2
NER DB NER DB NER DB
R@3 0.47 0.48 0.49 0.76 0.58 0.79
R@5 0.56 0.60 0.58 0.83 0.68 0.86
R@7 0.61 0.64 0.62 0.87 0.72 0.88
R@10 0.65 0.69 0.68 0.89 0.78 0.90
R@15 0.69 0.72 0.74 0.91 0.80 0.91
R@20 0.71 0.75 0.76 0.92 0.82 0.93
MAP 0.20 0.28 0.50 0.61 0.52 0.66
Table 3: Performance Measures
It is perhaps important to mention that our data is
fairly clean. More noise would make the creation of
relational database more difficult. In that case the
data-driven approach may become more appropri-
ate.
6 Conclusion
The main objective of this work was to explore
expert-finding in a university domain, an area that
has to the best of our knowledge so far attracted
little attention in the literature. The main finding
is that a database-driven approach (utilising a fixed
set of known experts) outperforms a data-driven ap-
proach which is based on automatic named-entity
recognition. Furthermore, exploiting proximity be-
tween query and candidate outperforms a straight
frequency measure.
There are a number of directions for future work.
For example, modelling the user background and
interests could increase the system?s effectiveness.
Some more realistic end-user studies could be used
to evaluate the systems. Consideration could be
given to term dependence and positional models as
in Metzler and Croft (2005), which might improve
our proximity-based scoring function. Finaly, our
gold standard collection penalises a data-driven ap-
proach, which might offer a broader range of ex-
perts. We will continue this line of work using
both technical evaluation measures as well as user-
focused evaluations.
5
References
