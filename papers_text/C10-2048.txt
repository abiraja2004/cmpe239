Coling 2010: Poster Volume, pages 418?426,
Beijing, August 2010
Word Sense Disambiguation-based Sentence Similarity 
Chukfong Ho1, Masrah Azrifah 
Azmi Murad2 
Department of Information System 
University Putra Malaysia 
hochukfong@yahoo.com1, 
masrah@fsktm.upm.edu.my2 
Rabiah Abdul Kadir, Shyamala 
C. Doraisamy 
Department of Multimedia 
University Putra Malaysia 
{rabiah, shya-
mala}@fsktm.upm.edu.my 
 
Abstract 
Previous works tend to compute the 
similarity between two sentences based 
on the comparison of their nearest 
meanings. However, the nearest 
meanings do not always represent their 
actual meanings. This paper presents a 
method which computes the similarity 
between two sentences based on a com-
parison of their actual meanings. This is 
achieved by transforming an existing 
most-outstanding corpus-based measure 
into a knowledge-based measure, which 
is then integrated with word sense dis-
ambiguation. The experimental results 
on a standard data set show that the pro-
posed method outperforms the baseline 
and the improvement achieved is statisti-
cally significant at 0.025 levels. 
1 Introduction 
Although measuring sentence similarity is a 
complicated task, it plays an important role in 
natural language processing applications. In text 
categorization (Yang and Wen, 2007), docu-
ments are retrieved based on similar or related 
features. In text summarization (Zhou et al, 
2006) and machine translation (Kauchak and 
Barzilay, 2006), summaries comparison based 
on sentence similarity has been applied for 
automatic evaluation. In text coherence (Lapata 
and Barzilay, 2005), different sentences are 
linked together based on the sequence of similar 
or related words.  
Two main issues are investigated in this paper: 
1) the performance between corpus-based meas-
ure and knowledge-based measure, and 2) the 
influence of word sense disambiguation (WSD) 
on measuring sentence similarity. WSD is the 
task of determining the sense of a polysemous 
word within a specific context (Wang et al, 
2006). Corpus-based methods typically compute 
sentence similarity based on the frequency of a 
word?s occurrence or the co-occurrence between 
collocated words. Although these methods bene-
fit from the statistical information derived from 
the corpus, this statistical information is closer to 
syntactic representation than to semantic repre-
sentation. In comparison, knowledge-based 
methods compute the similarity between two 
sentences based on the semantic information 
collected from knowledge bases. However, this 
semantic information is applied in a way that, 
for any two sentences, the comparison of their 
nearest meanings is taken into consideration in-
stead of the comparison of their actual meanings. 
More importantly, the nearest meaning does not 
always represent the actual meaning. In this pa-
per, a solution is proposed that seeks to address 
these two issues. Firstly, the most outstanding 
existing corpus-based sentence similarity meas-
ure is transformed into a knowledge-based 
measure. Then, its underlying concept, which is 
the comparison of the nearest meanings, is re-
placed by another underlying concept, the com-
parison of the actual meanings.         
The rest of this paper is organized into five 
sections. Section 2 presents an overview of the 
related works. Section 3 details the problem of 
the existing method and the improvement of the 
proposed method. Section 4 describes the ex-
perimental design. In Section 5, the experimental 
results are discussed. Finally, the implications 
and contributions are addressed in Section 6.   
418
2 Related Work 
In general, related works can be categorized into 
corpus-based, knowledge-based and hybrid-
based methods. Islam and Inkpen (2008) pro-
posed a corpus-based sentence similarity meas-
ure as a function of string similarity, word simi-
larity and common word order similarity (CWO). 
They claimed that a corpus-based measure has 
the advantage of large coverage when compared 
to a knowledge-based measure. However, the 
judgment of similarity is situational and time 
dependent (Feng et al, 2008). This suggests that 
the statistical information collected from the past 
corpus may not be relevant to sentences present 
in the current corpus. Apart from that, the role of 
string similarity is to identify any misspelled 
word. A malfunction may occur whenever string 
similarity deals with any error-free sentences 
because the purpose for its existence is no longer 
valid.   
For knowledge-based methods, Li et al (2009) 
adopted an existing word similarity measure to 
deal with the similarities of verbs and nouns 
while the similarities of adjectives and adverbs 
were measured only based on simple word over-
laps. However, Achananuparp et al (2008) pre-
viously showed that the word overlap-based 
method performed badly in measuring text simi-
larity. Liu et al (2007) integrated the Dynamic 
Time Warping (DTW) technique into the simi-
larity measure to identify the distance between 
words. The main drawback of DTW is that the 
computational cost and time will increase pro-
portionately with the sentence?s length. Wee and 
Hassan (2008) proposed a method that takes into 
account the directionality of similarity in which 
the similarity of any two words is treated as 
asymmetric. The asymmetric issue between a 
pair of words was resolved by considering both 
the similarity of the first word to the second 
word, and vice versa.  
Corley and Mihalcea (2005) proposed a hy-
brid method by combining six existing knowl-
edge-based methods. Mihalcea et al (2006) fur-
ther combined those six knowledge-based meth-
ods with two corpus-based methods and claimed 
that they usually achieved better performance in 
terms of precision and recall respectively. How-
ever, those methods were only combined by us-
ing simple average calculation.  
Perhaps the most closely related work is a re-
cently proposed query extension technique. 
Perez-Ag?era and Zaragoza (2008) made use of 
WSD information to map the original query 
words and the expansion words to WordNet 
senses. However, without the presence of or 
considering the surrounding words, the meaning 
of the expansion words alone tend to be repre-
sented by their most general meanings instead of 
the disambiguated meanings, which results in 
the possibility of WSD information not being 
useful for word expansions. In contrast to their 
work, which is more suitable to be applied on 
word-to-word similarity task, the method pro-
posed in this paper is more suitable for applica-
tion on sentence-to-sentence similarity tasks. 
Overall, the above-mentioned related works 
compute similarity based either on statistical 
information or on a comparison of the nearest 
meanings in terms of words. None of them com-
pute sentence similarity based on the comparison 
of actual meanings. Our proposed method, 
which is a solution to this issue, will be ex-
plained in detail in the next section. 
3 Sentence Similarity 
 
Figure 1. The proposed method 
 
Our proposed method shown in Figure 1, is the 
outcome of some modifications on an existing 
method, which is also the most outstanding 
method, the Semantic Text Similarity (STS) 
model (Islam and Inkpen, 2008). First of all, 
CWO is removed from STS as the previous 
works (Islam and Inkpen, 2007; Islam and Ink-
pen, 2008) have shown that the presence of 
CWO has no influence on the outcome. Then, 
419
the corpus-based word similarity function of 
STS is replaced by an existing knowledge-based 
word similarity measure called YP (Yang and 
Powers, 2005).  Finally, the underlying concept 
of YP is modified by the integration of WSD 
and is based on the assumption that any disam-
biguated sense of a word represents its actual 
meaning. Thus, the proposed method is also 
called WSD-STS. 
3.1 String similarity measure 
The string similarity between two words is 
measured by using the following equations:  
     )()(
)),((),(
2
1
ji
b
j
a
ib
j
a
i
wlwl
wwLCSl
wwNLCSv
?
==
        (1) 
)()(
)),((),(
2
1
12
ji
b
j
a
ib
j
a
i
wlwl
wwMCLCSl
wwNMCLCSv
?
==
  (2) 
)()(
)),((),(
2
3
ji
b
j
a
inb
j
a
in
wlwl
wwMCLCSl
wwNMCLCSv
?
==
 (3) 
321 33.033.033.0),( vvvYXSimstring ++=        (4) 
where l(x) represents the length of x; a and b 
represent the lengths of sentences X and Y re-
spectively after removing stop words; wi repre-
sents the i-th word in sequence a; wj represents 
the j-th word in sequence b; and Simstring(X,Y) 
represents the overall string similarity. The un-
derlying concept of string similarity is based on 
character matching. NLCS represents the nor-
malized version of the traditional longest com-
mon subsequence (LCS) technique in which the 
lengths of the two words are taken into consid-
eration. MCLCS1 represents the modified version 
of the traditional LCS in which the string match-
ing must start from the first character while 
MCLCSn represents the modified version of the 
traditional LCS in which the string matching 
may start from any character. NMCLCS1 and 
NMCLCSn represent the normalized versions of 
MCLCS1 and MCLCSn respectively. More de-
tailed information regarding string similarity 
measure can be found in the original paper (Is-
lam and Inkpen, 2008). 
3.2 Adopted word similarity measure 
Yang and Powers (2005) proposed YP based on 
the assumptions that every single path in the hi-
erarchical structure of WordNet 1) is identical; 
and 2) represents the shortest distance between 
any two connected words. The similarity be-
tween two words in sequence a and sequence b 
can be represented by the following equation: 
??
??
?
??
??
?
?
<?
=
?
=
?
???
l
l
wwSim it
l
i
tbj
a
iword
                  ,0      
            ,),(
1
1   (5) 
where 0 ? ),( bjaiword wwSim ? 1; d is the depth of 
LCS; l is the length of path between disambigu-
ated aiw  and bjw ; t represents the type of path 
(hypernyms/hyponym, synonym or holo-
nym/meronym) which connects them; ?t repre-
sents their path type factor; ?t represents their 
path distance factor; and ? represents an arbitrary 
threshold on the distance introduced for effi-
ciency, representing human cognitive limitations. 
The values of ?t, ?t and ? have already been em-
pirically tuned as 0.9, 0.85 and 12 respectively. 
More detailed information regarding YP can be 
found in the original paper (Yang and Powers, 
2005).  
In order to adapt a different underlying concept, 
which is the comparison of actual meanings, l 
has to be redefined as the path distance between 
disambiguated words, aiw  and bjw . Since YP 
only differs from the modified version of YP 
(MYP) in terms of the definition of l, MYP can 
also be represented by equation (5). 
3.3 The proposed measure 
The gap 
Generally, all the related works in Section 2 can 
be abstracted as a function of word similarity. 
This reflects the importance of a word similarity 
measure in measuring sentence similarity. How-
ever, measuring sentence similarity is always a 
more complicated task than measuring word 
similarity. The reason is that while a word simi-
larity measure only involves a single pair of 
words, a sentence similarity measure has to deal 
with multiple pairs of words. In addition, due to 
the presence of the surrounding words in a sen-
tence, the possible meaning of a word is always 
being restricted (Kolte and Bhirud, 2008). Thus, 
without some modifications, the traditional word 
similarity measures, which are based on the con-
cept of a comparison of the nearest meanings, 
are inapplicable in the context of sentence simi-
larity measures.  
The importance of WSD in reducing the gap 
420
Before performing the comparison of actual 
meanings, WSD has to be integrated so that the 
most suitable sense can be assigned to any 
polysemous word. The importance of WSD can 
be investigated by using a simple example. Con-
sider a pair of sentences, collected from Word-
Net 2.1, which use two words, ?dog? and ?cat?: 
X: The dog barked all night. 
Y: What a cat she is! 
Based on the definition in WordNet 2.1, the 
word ?dog? in X is annotated as the first sense 
which means ?a member of the genus Canis 
(probably descended from the common wolf) 
that has been domesticated by man since prehis-
toric times?. Meanwhile, the word ?cat? in Y is 
annotated as the third sense with the definition 
of ?a spiteful woman?s gossip?. The path dis-
tance between ?cat? and ?dog? based on their 
actual senses is equal to 7. However, their short-
est path distance (SPD), which is based on their 
nearest senses, is equal to 4. SPD is the least 
number of edges connecting two words in the 
hierarchical structure of WordNet. In other 
words, ?cat? and ?dog? in X and Y respectively, 
are not as similar as the one measured by using 
SPD. The presence of the additional path dis-
tances is significant as it is almost double the 
actual path distance between ?cat? and ?dog?. 
WSD-STS 
The adopted sentence similarity measure, STS, 
can be represented by the following equations: 
  
ab
ba
YXSim
c
i i
semantic 2
)()(),( 1? +?+= = ??         (6) 
  
2
),(),(),( YXSimYXSimYXSIM stringsmeantic += (7) 
where for equation (6): ? represents the number 
of overlapped words between the words in se-
quence a and sequence b; c represents the num-
ber of semantically matched words between the 
words in sequence a and sequence b, in which c 
= a if a < b or c = b if b < a, ?i represents the 
highest matching similarity score of i-th word in 
the shorter sequence with respect to one of the 
words in the longer sequence; and ?? represents 
the sum of the highest matching similarity score 
between the words in sequence a and sequence 
b.  
For STS, the similarity between two words is 
measured by using a corpus-based measure. For 
WSD-STS, this corpus-based measure is re-
placed by MYP. Finally, the overall sentence 
similarity is represented by equation (7). 
4 Experimental Design 
4.1 Data set 
Li et al, (2006) constructed a data set which 
consists of 65 pairs of human-rated sentences by 
applying the similar experimental design for cre-
ating the standard data set for the word similarity 
task (Rubenstein and Goodenough, 1965). These 
65 sentence pairs were the definitions collected 
from the Collin Cobuild Dictionary. Out of 
these, 30 sentence pairs with rated similarity 
scores that ranged from 0.01 to 0.96 were se-
lected as test data set. The corresponding 30 
word pairs for these 30 sentence pairs are shown 
in the second column of Table 1. A further set of 
66 sentence pairs is still under development and 
it will be combined with the existing data set in 
the future (O?Shea et al, 2008b). 
4.2 Procedure 
Firstly, Stanford parser 1  is used to parse each 
sentence and to tag each word with a part of 
speech (POS). Secondly, Structural Semantic 
Interconnections2 (SSI), which is an online WSD 
system, is used to disambiguate and to assign a 
sense for each word in the 30 sentences based on 
the assigned POS. SSI is applied based on the 
assumption that it is able to perform WSD cor-
rectly. The main reason for choosing SSI to per-
form WSD is its promising results reported in a 
study by Navigli and Verladi (2006). Thirdly, all 
the stop words which exist in these 30 pairs of 
sentences are removed. It is important to note 
that the 100 most frequent words collected from 
British National Corpus (BNC) were applied as 
the stop words list on the baseline, STS. How-
ever, due to the limited accessibility to BNC, a 
different stop words list 3 , which is available 
online, is applied in this paper. 
                                               
1
 http://nlp.stanford.edu/software/lex-parser.shtml 
2
 http://lcl.uniroma1.it/ssi 
3
 http://www.translatum.gr/forum/index.php?topic=    
  2476.0 
421
 Table 1. Data Set Results 
 
Finally, the remaining content words are 
lemmatized by using Natural Language Toolkit4 
(NLTK). Nevertheless, those words which can 
be found in WordNet and which have different 
definitions from their lemmatized form will be 
excluded from lemmatization. For instance, 
Cooking[NN] can be a great art. 
The word in the bracket represents the tagged 
POS for its corresponding word. Since based on 
the definitions provided by WordNet, ?cooking?, 
which is tagged as a noun, has a different mean-
ing from its lemmatized form ?cook?, which is 
also tagged as a noun. Therefore, ?cooking? is 
excluded from lemmatization. 
4.3 Experimental conditions 
Sentence similarity is measured under the fol-
lowing three conditions: 
                                               
4
 http://www.nltk.org/ 
? OLP-STS: A modified version of the 
baseline, STS (Islam and Inkpen, 2008), 
in which it only relies on the presence of 
overlapped words. This means that the 
component ? = ici ?1 , which represents the 
word similarity, is removed from equa-
tion (6).  
? SPD-STS: The corpus-based word simi-
larity measure of the baseline, STS, 
which is represented by ? = ici ?1  in equa-
tion (6), is replaced by a knowledge-
based word similarity measure, YP.   
? WSD-STS: A modified version of SPD-
STS in which the knowledge-based 
measure, YP, is replaced by MYP. 
 
As mentioned in Section 4.2, different stop 
words lists were applied between the baseline 
and the proposed methods under different ex-
422
perimental conditions in this paper. Since this 
issue may be questioned due to the unfair com-
parison, the performance of WSD-STS is evalu-
ated on top of a number of different stop words 
lists which are available online in order to inves-
tigate any influence which may be caused by 
stop words list. 
5 Results and Discussion 
Table 1 presents the similarity scores obtained 
from the mean of human ratings, the benchmarks, 
and different experimental conditions of the pro-
posed methods. Figure 2 presents the corre-
sponding Pearson correlation coefficients of 
various measures as listed in Table 1.  
 
 
Figure 2. Pearson Correlation Coefficient 
 
Figure 2 shows that STS appears to be the 
most outstanding measure among the existing 
works with a correlation coefficient of 0.853. 
However, Figure 2 also shows that both the pro-
posed methods in this paper, WSD-STS and 
SPD-STS, outperform STS. This result indicates 
that knowledge-based method tends to perform 
better than a corpus-based method. The reason is 
that a knowledge base is much closer to human 
representation of knowledge (WordNet is the 
knowledge base applied in this paper) than a 
corpus. A corpus only reflects the usage of lan-
guages and words while WordNet is a model of 
human knowledge constructed by many expert 
lexicographers (Li et al, 2006). In other words, a 
corpus is more likely to provide unprocessed 
raw data while a knowledge base tends to pro-
vide ready-to-use information.  
The results of the performance of the two 
proposed methods are as expected. SPD-STS 
achieved a bigger but statistically insignificant 
improvement while WSD-STS achieved a 
smaller but statistically significant improvement 
at 0.01 levels. The significance of a correlation 
is calculated by using an online calculator, Vas-
sarStats5. The reason for the variance in the out-
comes between SPD-STS and WSD-STS is ob-
vious; it is the difference in terms of their under-
lying concepts. In other words, sentence similar-
ity computation, which is based on a comparison 
of the nearest meanings, results in insignificant 
improvement while sentence similarity computa-
tion, which is based on a comparison of actual 
meanings, achieves statistically significant im-
provement. These explanations indicate that 
WSD is essential in confirming the validity of 
the task of measuring sentence similarity.  
Figure 2 also reveals that a relatively low cor-
relation is achieved by OLP-STS. This is not at 
all surprising since Achananuparp et al (2008) 
has already demonstrated that the overlapped 
word-based method tends to perform badly in 
measuring sentence similarity. However, it is 
interesting to find that the difference in perform-
ance between STS and OLP-STS is very small. 
This indirectly suggests that the presence of the 
string similarity measure and the corpus-based 
word similarity measure has only a slight im-
provement on the performance of OLP-STS. 
 
 
Figure 3. The performance of the WSD-SPD 
versus different stop words lists 
 
Next, in order to address the issue of unfair 
comparison due to the usage of different stop 
words lists, the performance of WSD-SPD has 
been evaluated on top of a number of different 
                                               
5
 http://faculty.vassar.edu/lowry/rdiff.html? 
423
stop words lists. A total of five stop words lists 
with different lengths (896, 2237, 319, 5718 and 
6599) of stop words were applied. The perform-
ances of WSD-SPD with respect to these stop 
words lists are portrayed in Figure 3. They are 
found to be in a comparable condition. This re-
sult connotes that the influence caused by the 
usage of different stop words lists is small and 
can be ignored. Hence, the unfair comparison 
between our proposed method and the baseline 
should not be treated as an issue for the bench-
marking purpose of this paper. 
On the other hand, although an assumption is 
made that SSI performs WSD correctly, we no-
ticed that not all the words were disambiguated 
confidently. The confident scores which were 
assigned to the disambiguated words by SSI 
range between 30% and 100%. These confident 
scores reflect the confidence of SSI in perform-
ing WSD. Thus, it is possible that some of those 
words which were assigned with low confident 
scores were disambiguated incorrectly. Conse-
quently, the final sentence similarity score is 
likely to be affected negatively. In order to re-
duce the negative effect which may be caused by 
incorrect WSD, any words pair which is not con-
fidently disambiguated is assigned the similarity 
score based on the concept of comparing the 
nearest meanings instead of comparing the ac-
tual meanings. In other words, WSD-STS and 
SPD-STS are combined and results in WSD-
SPD. The performance of WSD-SPD across a 
range of confident scores is essential in reveal-
ing the impact of WSD and SPD on the task of 
measuring sentence similarity.     
Figure 4 outlines the performance achieved by 
WSD-SPD across different confident scores as-
signed by SSI. The confident score of at least 0.7 
is identified as the threshold in which SSI opti-
mizes its performance. The performance of 
WSD-SPD is found to be statistically insignifi-
cant for those confident scores above the thresh-
old. The explanation for this phenomenon can be 
                                               
6
 http://msdn.microsoft.com/en-us/library/   
  bb164590.aspx 
7
 http://snowball.tartarus.org/algorithms/english/   
  stop.txt 
8
 http://truereader.com/manuals/onix/ 
    stopwords2.html 
9
 http://www.link-assistant.com/seo-stop- 
    words.html 
found in Figure 5. Figure 5 illustrates the per-
centage of the composition between WSD and 
SPD in WSD-SPD. It is obvious that once the 
portion of WSD exceeds the portion of SPD, the 
performance of WSD-SPD is found to be statis-
tically insignificant. This finding suggests that 
SPD, which reflects the application of the con-
cept of nearest meaning comparison, is likely to 
decrease the validity of sentence similarity 
measurement while WSD, which reflects the 
application of the concept of actual meaning 
comparison, is essential in confirming the valid-
ity of sentence similarity measurement.   
 
 
Figure 4. The performance of WSD-SPD versus 
confident scores 
 
 
Figure 5. The percentage of WSD/SPD versus 
confident score 
 
The trend of the performance of string simi-
larity measure and word similarity measure with 
respect to different weight assignments is de-
lineated in Figure 6. The lowest correlation of 
0.856 is obtained when only the string similarity 
function is considered while the word similarity 
424
function is excluded. A better performance is 
achieved by taking the two measures into con-
sideration where more weight is given to the 
measure of word similarity. This trend intimates 
that the string similarity measure offers a smaller 
contribution in measuring sentence similarity 
than word similarity measure. In contrast to a 
word similarity measure, a string similarity 
measure is purposely proposed to address the 
issue of misspelled words. Since the data set ap-
plied in this experiment does not contain any 
misspelled words, it is obvious that a string simi-
larity measure performs badly. In addition, the 
underlying concept of string similarity is ques-
tionable. Does it make sense to determine the 
similarity of two words based on the matching 
between their characters or the matching of the 
sequence of characters? Consider four pairs of 
words: ?play? versus ?pray?, ?plant? versus 
?plane?, ?plane? versus ?plan? and ?stationary? 
versus ?stationery?. These word pairs are highly 
similar in terms of characters but they are se-
mantically dissimilar or unrelated.  
 
 
Figure 6. The performance of the different 
measures versus the weight between string simi-
larity and word similarity 
 
Figure 6 also depicts that the combination of 
word similarity measure (70%) and string simi-
larity measure (30%) performs better than the 
measure which is solely based on word similar-
ity function. It is obvious that the difference is 
caused by the presence of string similarity 
measure. The combination assigns similarity 
scores to all word pairs while the word similarity 
measure only assigns similarity scores to those 
word pairs which fulfill two requirements: 1) 
any two words which share an identical POS, 
and 2) any two words which must either be a 
pair of nouns or a pair of verbs. In fact, adjec-
tives and adverbs do contribute to representing 
the meaning of a sentence although their contri-
bution is relatively smaller than the contribution 
of nouns and verbs (Liu et al, 2007; Li et al, 
2009). Therefore, by ignoring the presence of 
adjectives and adverbs, the performance will 
definitely be affected negatively. 
6 Conclusion 
This paper has presented a knowledge-based 
method which measures the similarity between 
two sentences based on their actual meaning 
comparison. The result shows that the proposed 
method, which is a knowledge-based measure, 
performs better than the baseline, which is a 
corpus-based measure. The improvement ob-
tained is statistically significant at 0.025 levels. 
This result also shows that the validity of the 
output of measuring the similarity of two sen-
tences can be improved by comparing their ac-
tual meanings instead of their nearest meanings. 
These are achieved by transforming the baseline 
into a knowledge-based method and then by in-
tegrating WSD into the adopted knowledge-
based measure. 
Although the proposed method significantly 
improves the quality of measuring sentence 
similarity, it has a limitation. The proposed 
method only measures the similarity between 
two words with an identical part of speech 
(POS) and these two words must either be a pair 
of nouns or a pair of verbs. By ignoring the im-
portance of adjectives and adverbs, and the rela-
tionship between any two words with different 
POS, a slight decline is observed in the obtained 
result. In future research, these two issues will 
be addressed by taking into account the related-
ness between two words instead of only consid-
ering their similarity. 
 
References 
Achananuparp, Palakorn, Xiao-Hua Hu, and Xiao-
Jiong Shen. 2008. The Evaluation of Sentence 
Similarity Measures. In Proceedings of the 10th 
International Conference on Data Warehousing 
425
and Knowledge Discovery (DaWak), pages 305-
316, Turin, Italy. 
Corley, Courtney, and Rada Mihalcea. 2005. Measur-
ing the Semantic Similarity of Texts. In Proceed-
ings of the ACL Workshop on Empirical Modeling 
of Semantic Equivalence and Entailment, pages 
48-55, Ann Arbor. 
Feng, Jin, Yi-Ming Zhou, and Trevor Martin. 2008. 
Sentence Similarity based on Relevance. In Pro-
ceedings of IPMU, pages 832-839. 
Islam, Aminul, and Diana Inkpen. 2007. Semantic 
Similarity of Short Texts. In Proceedings of 
RANLP, pages 291-297. 
Islam, Aminul, and Diana Inkpen. 2008. Semantic 
Text Similarity Using Corpus-Based Word Simi-
larity and String Similarity. ACM Transactions on 
Knowledge Discovery from Data, 2(2):10. 
Kauchak, David, and Regina Barzilay. 2006. Para-
phrasing for Automatic Evaluation. In Proceedings 
of HLT-NAACL, pages 455-462, New York. 
Kolte, Sopan Govind, and Sunil G. Bhirud. 2008. 
Word Sense Disambiguation using WordNet Do-
mains. In The First International Conference on 
Emerging Trends in Engineering and Technology, 
pages 1187-1191. 
Lapata, Mirella, and Regina Barzilay. 2005. Auto-
matic Evaluation of Text Coherence: Models and 
Representations. In Proceedings of the 19th Inter-
national Joint Conference on Artificial Intelli-
gence. 
Li, Lin, Xia Hu, Bi-Yun Hu, Jun Wang, and Yi-Ming 
Zhou. 2009. Measuring Sentence Similarity from 
Different Aspects. In Proceedings of the Eighth In-
ternational Conference on Machine Learning and 
Cybernetics, pages 2244-2249. 
Li, Yu-Hua, David McLean, Zuhair A. Bandar, James 
D.O'Shea, and Keeley Crockett. 2006. Sentence 
Similarity Based on Semantic Nets and Corpus 
Statistics. IEEE Transactions on Knowledge and 
Data Engineering, 18(8):1138-50. 
Liu, Xiao-Ying, Yi-Ming Zhou, and Ruo-Shi Zheng. 
2007. Sentence Similarity based on Dynamic Time 
Warping. In The International Conference on Se-
mantic Computing, pages 250-256. 
Mihalcea, Rada, Courtney Corley, and Carlo Strap-
parava. 2006. Corpus-based and Knowledge-based 
Measures of Text Semantic Similarity. In Proceed-
ings of the American Association for Artificial In-
telligence. 
Navigli, Roberto, and Paola Velardi. 2005. Structural 
Semantic Interconnections: A Knowledge-Based 
Approach to Word Sense Disambiguation. IEEE 
Transactions on Pattern Analysis and Machine In-
telligence 27(7):1075-86. 
O'Shea, James, Zuhair Bandar, Keeley Crockett, and 
David McLean. 2008a. A Comparative Study of 
Two Short Text Semantic Similarity Measures. In 
KES-AMSTA, LNAI: Springer Berlin / Heidelberg. 
O'Shea, James, Zuhair Bandar, Keeley Crockett, and 
David McLean. 2008b. Pilot Short Text Semantic 
Similarity Benchmark Data Set: Full Listing and 
Description. 
Perez-Aguera, Jose R., and Hugo Zaragoza. 2008. 
UCM-Y!R at Clef 2008 Robust and WSD Tasks. 
In Working Notes for CLEF Workshop. 
Rubenstein, Herbert, and John B. Goodenough. 1965. 
Contextual Correlates of Synonymy. Communica-
tions of the ACM, pages 627-633. 
Wang, Yao-Feng, Yue-Jie Zhang, Zhi-Ting Xu, and 
Tao Zhang. 2006. Research on Dual Pattern of Un-
supervised and Supervised Word Sense Disam-
biguation. In Proceedings of the Fifth Interna-
tional Conference on Machine Learning and Cy-
bernetics, pages 2665-2669. 
Wee, Leong Chee, and Samer Hassan. 2008. Exploit-
ing Wikipedia for Directional Inferential Text 
Similarity. In Proceedings of Fifth International 
Conference on Information Technology: New 
Generations, pages 686-691. 
Yang, Cha, and Jun Wen. 2007. Text Categorization 
Based on Similarity Approach. In Proceedings of 
International Conference on Intelligence Systems 
and Knowledge Engineering (ISKE). 
Yang, Dong-Qiang, and David M.W. Powers. 2005. 
Measuring Semantic Similarity in the Taxonomy 
of WordNet. In Proceedings of the 28th Austral-
asian Computer Science Conference, pages 315-
332, Australia. 
Zhou, Liang, Chin-Yew Lin, Dragos Stefan 
Munteanu, and Eduard Hovy. 2006. ParaEval: Us-
ing Paraphrases to Evaluate Summaries Automati-
cally. In Proceedings of Human Language Tech-
nology Conference of the North American Chapter 
of the ACL, pages 447-454, New York. 
 
426
