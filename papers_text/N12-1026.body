2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 253?262,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Optimized Online Rank Learning for Machine Translation
Taro Watanabe
National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289 JAPAN
{taro.watanabe}@nict.go.jp
Abstract
We present an online learning algorithm for
statistical machine translation (SMT) based on
stochastic gradient descent (SGD). Under the
online setting of rank learning, a corpus-wise
loss has to be approximated by a batch lo-
cal loss when optimizing for evaluation mea-
sures that cannot be linearly decomposed into
a sentence-wise loss, such as BLEU. We pro-
pose a variant of SGD with a larger batch size
in which the parameter update in each iteration
is further optimized by a passive-aggressive
algorithm. Learning is efficiently parallelized
and line search is performed in each round
when merging parameters across parallel jobs.
Experiments on the NIST Chinese-to-English
Open MT task indicate significantly better
translation results.
1 Introduction
The advancement of statistical machine translation
(SMT) relies on efficient tuning of several or many
parameters in a model. One of the standards for such
tuning is minimum error rate training (MERT) (Och,
2003), which directly minimize the loss of transla-
tion evaluation measures, i.e. BLEU (Papineni et al.,
2002). MERT has been successfully used in prac-
tical applications, although, it is known to be un-
stable (Clark et al., 2011). To overcome this insta-
bility, it requires multiple runs from random start-
ing points and directions (Moore and Quirk, 2008),
or a computationally expensive procedure by linear
programming and combinatorial optimization (Gal-
ley and Quirk, 2011).
Many alternative methods have been proposed
based on the algorithms in machine learning, such as
averaged perceptron (Liang et al., 2006), maximum
entropy (Och and Ney, 2002; Blunsom et al., 2008),
Margin Infused Relaxed Algorithm (MIRA) (Watan-
abe et al., 2007; Chiang et al., 2008b), or pairwise
rank optimization (PRO) (Hopkins and May, 2011).
They primarily differ in the mode of training; on-
line or MERT-like batch, and in their objectives;
max-margin (Taskar et al., 2004), conditional log-
likelihood (or softmax loss) (Berger et al., 1996),
risk (Smith and Eisner, 2006; Li and Eisner, 2009),
or ranking (Herbrich et al., 1999).
We present an online learning algorithm based
on stochastic gradient descent (SGD) with a larger
batch size (Shalev-Shwartz et al., 2007). Like Hop-
kins and May (2011), we optimize ranking in n-
best lists, but learn parameters in an online fash-
ion. As proposed by Haddow et al. (2011), BLEU
is approximately computed in the local batch, since
BLEU is not linearly decomposed into a sentence-
wise score (Chiang et al., 2008a), and optimization
for sentence-BLEU does not always achieve opti-
mal parameters for corpus-BLEU. Setting the larger
batch size implies the more accurate corpus-BLEU,
but at the cost of slower convergence of SGD. There-
fore, we propose an optimized update method in-
spired by the passive-aggressive algorithm (Cram-
mer et al., 2006), in which each parameter update is
further rescaled considering the tradeoff between the
amount of updates to the parameters and the ranking
loss. Learning is efficiently parallelized by splitting
training data among shards and by merging parame-
ters in each round (McDonald et al., 2010). Instead
253
of simple averaging, we perform an additional line
search step to find the optimal merging across paral-
lel jobs.
Experiments were carried out on the NIST 2008
Chinese-to-English OpenMT task. We found signif-
icant gains over traditional MERT and other tuning
algorithms, such as MIRA and PRO.
2 Statistical Machine Translation
SMT can be formulated as a maximization problem
of finding the most likely translation e given an input
sentence f using a set of parameters ? (Brown et al.,
1993)
e? = argmax
e
p(e|f ; ?). (1)
Under this maximization setting, we assume that
p(?) is represented by a linear combination of fea-
ture functions h(f, e) which are scaled by a set of
parameters w (Och and Ney, 2002)
e? = argmax
e
w?h(f, e). (2)
Each element ofh(?) is a feature function which cap-
tures different aspects of translations, for instance,
log of n-gram language model probability, the num-
ber of translated words or log of phrasal probability.
In this paper, we concentrate on the problem of
learning w, which is referred to as tuning. One of
the standard methods for parameter tuning is mini-
mum error rate training (Och, 2003) (MERT) which
directly minimizes the task loss ?(?), i.e. negative
BLEU (Papineni et al., 2002), given training data
D = {(f1, e1), ..., (fN , eN )}, sets of paired source
sentence f i and its reference translations ei
w? = argmin
w
?(
{
argmax
e
w?h(f i, e)
}N
i=1
,
{
ei
}N
i=1).
(3)
The objective in Equation 3 is discontinuous and
non-convex, and it requires decoding of all the train-
ing data given w. Therefore, MERT relies on a
derivative-free unconstrained optimization method,
such as Powell?s method, which repeatedly chooses
one direction to optimize using a line search pro-
cedure as in Algorithm 1. Expensive decoding is
approximated by an n-best merging technique in
which decoding is carried out in each epoch of it-
erations t and the maximization in Eq. 3 is approxi-
Algorithm 1 MERT
1: Initialize w1
2: for t = 1, ..., T do ? Or, until convergence
3: Generate n-bests using wt
4: Learn new wt+1 by Powell?s method
5: end for
6: return wT+1
mated by search over the n-bests merged across iter-
ations. The merged n-bests are also used in the line
search procedure to efficiently draw the error surface
for efficient computation of the outer minimization
of Eq. 3.
3 Online Rank Learning
3.1 Rank Learning
Instead of the direct task loss minimization of Eq.
3, we would like to find w by solving the L2-
regularized constrained minimization problem
argmin
w
?
2
?w?22 + ?(w;D) (4)
where ? > 0 is a hyperparameter controlling the fit-
ness to the data. The loss function ?(?) we consider
here is inspired by a pairwise ranking method (Hop-
kins and May, 2011) in which pairs of correct trans-
lation and incorrect translation are sampled from n-
bests and suffer a hinge loss
1
M(w;D)
?
(f,e)?D
?
e?,e?
max
{
0, 1?w??(f, e?, e?)
}
(5)
where
e? ? NBEST(w; f) \ ORACLE(w; f, e)
e? ? ORACLE(w; f, e)
?(f, e?, e?) = h(f, e?)? h(f, e?).
NBEST(?) is the n-best translations of f generated
with the parameter w, and ORACLE(?) is a set of
oracle translations chosen among NBEST(?). Note
that each e? (and e?) implicitly represents a deriva-
tion consisting of a tuple (e?, ?), where ? is a latent
structure, i.e. phrases in a phrase-based SMT, but we
omit ? for brevity. M(?) is a normalization constant
which is equal to the number of paired loss terms
?(f, e?, e?) in Equation 5. Since it is impossible
254
to enumerate all possible translations, we follow the
convention of approximating the domain of transla-
tion by n-bests. Unlike Hopkins and May (2011),
we do not randomly sample from all the pairs in the
n-best translations, but extract pairs by selecting one
oracle translation and one other translation in the n-
bests other than those in ORACLE(?). Oracle trans-
lations are selected by minimizing the task loss,
?(
{
e? ? NBEST(w; f i)
}N
i=1 ,
{
ei
}N
i=1)
i.e. negative BLEU, with respect to a set of ref-
erence translations e. In order to compute oracles
with corpus-BLEU, we apply a greedy search strat-
egy over n-bests (Venugopal, 2005). Equation 5 can
be easily interpreted as a constant loss ?1? for choos-
ing a wrong translation under current parameters w,
which is in contrast with the direct task-loss used in
max-margin approach to structured output learning
(Taskar et al., 2004).
As an alternative, we would also consider a soft-
max loss (Collins and Koo, 2005) represented by
1
N
?
(f,e)?D
? log ZO(w; f, e)
ZN(w; f)
(6)
where
ZO(w; f, e) =
?
e??ORACLE(w;f,e) exp(w?f(f, e?))
ZN(w; f) =
?
e??NBEST(w;f) exp(w?f(f, e?)).
Equation 6 is a log-linear model used in common
NLP tasks such as tagging, chunking and named en-
tity recognition, but differ slightly in that multiple
correct translations are discriminated from the oth-
ers (Charniak and Johnson, 2005).
3.2 Online Approximation
Hopkins and May (2011) applied a MERT-like pro-
cedure in Alg. 1 in which Equation 4 was solved
to obtain new parameters in each iteration. Here,
we employ stochastic gradient descent (SGD) meth-
ods as presented in Algorithm 2 motivated by Pega-
sos (Shalev-Shwartz et al., 2007). In each iteration,
we randomly permute D and choose a set of batches
Bt = {bt1, ..., btK} with each btj consisting of N/K
training data. For each batch b in Bt, we generate
n-bests from the source sentences in b and compute
oracle translations from the newly created n-bests
Algorithm 2 Stochastic Gradient Descent
1: k = 1,w1 ? 0
2: for t = 1, ..., T do
3: Choose Bt = {bt1, ..., btK} from D
4: for b ? Bt do
5: Compute n-bests and oracles of b
6: Set learning rate ?k
7: wk+ 12 ? wk ? ?k?(wk; b)
? Our proposed algorithm solve Eq. 12 or 16
8: wk+1 ? min
{
1, 1/
?
?
?wk+12
?2
}
wk+ 12
9: k ? k + 1
10: end for
11: end for
12: return wk
(line 5) using a batch local corpus-BLEU (Haddow
et al., 2011). Then, we optimize an approximated
objective function
argmin
w
?
2
?w?22 + ?(w; b) (7)
by replacing D with b in the objective of Eq. 4. The
parameters wk are updated by the sub-gradient of
Equation 7, ?(wk; b), scaled by the learning rate
?k (line 7). We use an exponential decayed learn-
ing rate ?k = ?0?k/K , which converges very fast in
practice (Tsuruoka et al., 2009)1. The sub-gradient
of Eq.7 with the hinge loss of Eq. 5 is
?wk ?
1
M(wk; b)
?
(f,e)?b
?
e?,e?
?(f, e?, e?) (8)
such that
1?w?k ?(f, e?, e?) > 0. (9)
We found that the normalization term by M(?) was
very slow in convergence, thus, instead, we used
M ?(w; b), which was the number of paired loss
terms satisfied the constraints in Equation 9. In the
case of the softmax loss objective of Eq. 6, the sub-
gradient is
?wk ?
1
|b|
?
(f,e)?b
?
?w
L(w; f, e)
?
?
?
?
w=wk
(10)
1We set ? = 0.85 and ?0 = 0.2 which converged well in
our preliminary experiments.
255
where L(w; f, e) = log (ZO(w; f, e)/ZN(w; f)).
After the parameter update, wk+ 12 is projected
within the L2-norm ball (Shalev-Shwartz et al.,
2007).
Setting smaller batch size implies frequent up-
dates to the parameters and a faster convergence.
However, as briefly mentioned in Haddow et al.
(2011), setting batch size to a smaller value, such as
|b| = 1, does not work well in practice, since BLEU
is devised for a corpus based evaluation, not for an
individual sentence-wise evaluation, and it is not lin-
early decomposed into a sentence-wise score (Chi-
ang et al., 2008a). Thus, the smaller batch size may
also imply less accurate batch-local corpus-BLEU
and incorrect oracle translation selections, which
may lead to incorrect sub-gradient estimations or
slower convergence. In the next section we propose
an optimized parameter update which works well
when setting a smaller batch size is impractical due
to its task loss setting.
4 Optimized Online Rank Learning
4.1 Optimized Parameter Update
In line 7 of Algorithm 2, parameters are updated by
the sub-gradient of each training instance in a batch
b. When the sub-gradient in Equation 8 is employed,
the update procedure can be rearranged as
wk+ 12 ? (1???k)wk+
?
(f,e)?b,e?,e?
?k
M(wk; b)
?(f, e?, e?)
(11)
in which each individual loss term?(?) is scaled uni-
formly by a constant ?k/M(?).
Instead of the uniform scaling, we propose to up-
date the parameters in two steps: First, we suffer the
sub-gradient from the L2 regularization
wk+ 14 ? (1? ??k)wk.
Second, we solve the following problem
argmin
w
1
2
?w?wk+ 14 ?
2
2+?k
?
(f,e)?b,e?,e?
?f,e?,e? (12)
such that
w??(f, e?, e?) ? 1? ?f,e?,e?
?f,e?,e? ? 0.
The problem is inspired by the passive-aggressive
algorithm (Crammer et al., 2006) in which new pa-
rameters are derived through the tradeoff between
the amount of updates to the parameters and the
margin-based loss. Note that the objective in MIRA
is represented by
argmin
w
?
2
?w ? wk?22 +
?
(f,e)?b,e?,e?
?f,e?,e? (13)
If we treat wk+ 14 as our previous parameters and set
? = 1/?k, they are very similar. Unlike MIRA, the
learning rate ?k is directly used as a tradeoff param-
eter which decays as training proceeds, and the sub-
gradient of the global L2 regularization term is also
combined in the problem through wk+ 14 .
The Lagrangian dual of Equation 12 is
argmin
?e?,e?
1
2
?
?
(f,e)?b,e?,e?
?e?,e??(f, e?, e?)?22
?
?
(f,e)?b,e?,e?
?e?,e?
{
1?w?k+ 14?(f, e
?, e?)
}
(14)
subject to
?
(f,e)?b,e?,e?
?e?,e? ? ?k.
We used a dual coordinate descent algorithm (Hsieh
et al., 2008)2 to efficiently solve the quadratic pro-
gram (QP) in Equation 14, leading to an update
wk+ 12 ? wk+ 14 +
?
(f,e)?b,e?,e?
?e?,e??(f, e?, e?). (15)
When compared with Equation 11, the update pro-
cedure in Equation 15 rescales the contribution from
each sub-gradient through the Lagrange multipliers
?e?,e? . Note that if we set ?e?,e? = ?k/M(?), we sat-
isfy the constraints in Eq. 14, and recover the update
in Eq. 11.
In the same manner as Eq. 12, we derive an opti-
mized update procedure for the softmax loss, which
replaces the update with Equation 10, by solving the
2Specifically, each parameter is bound constrained 0 ? ? ?
?k but is not summation constrained
?
? ? ?k. Thus, we re-
normalize ? after optimization.
256
following problem
argmin
w
1
2
?w ? wk+ 14 ?
2
2 + ?k
?
(f,e)?b
?f (16)
such that
w??(wk; f, e) ? ?L(wk; f, e)? ?f
?f ? 0
in which ?(w?; f, e) = ??wL(w; f, e)
?
?
w=w? . Equa-
tion 16 can be interpreted as a cutting-plane approx-
imation for the objective of Eq. 7, in which the orig-
inal objective of Eq. 7 with the softmax loss in Eq.
6 is approximated by |b| linear constraints derived
from the sub-gradients at pointwk (Teo et al., 2010).
Eq. 16 is efficiently solved by its Lagrange dual,
leading to an update
wk+ 12 ? wk+ 14 +
?
(f,e)?b
?f?(wk; f, e) (17)
subject to
?
(f,e)?b ?f ? ?k. Similar to Eq. 15, the
parameter update by?(?) is rescaled by its Lagrange
multipliers ?f in place of the uniform scale of 1/|b|
in the sub-gradient of Eq. 10.
4.2 Line Search for Parameter Mixing
For faster training, we employ an efficient paral-
lel training strategy proposed by McDonald et al.
(2010). The training data D is split into S disjoint
shards, {D1, ..., DS}. Each shard learns its own pa-
rameters in each single epoch t and performs param-
eter mixing by averaging parameters across shards.
We propose an optimized parallel training in Al-
gorithm 3 which performs better mixing with re-
spect to the task loss, i.e. negative BLEU. In line
5, wt+
1
2 is computed by averaging wt+1,s from all
the shards after local training using their own data
Ds. Then, the new parameters wt+1 are obtained by
linearly interpolating with the parameters from the
previous epoch wt. The linear interpolation weight
? is efficiently computed by a line search proce-
dure which directly minimizes the negative corpus-
BLEU. The procedure is exactly the same as the line
search strategy employed in MERT using wt as our
starting point with the direction wt+
1
2 ? wt. The
idea of using the line search procedure is to find the
optimum parameters under corpus-BLEU without a
Algorithm 3 Distributed training with line search
1: w1 ? 0
2: for t = 1, ..., T do
3: wt,s ? wt ? Distribute parameters
4: Each shard learns wt+1,s using Ds
? Line 3?10 in Alg. 2
5: wt+
1
2 ? 1/S
?
s wt+1,s ? Mixing
6: wt+1 ? (1? ?)wt + ?wt+
1
2 ? Line search
7: end for
8: return wT+1
batch-local approximation. Unlike MERT, however,
we do not memorize nor merge all the n-bests gener-
ated across iterations, but keep only n-bests in each
iteration for faster training and for memory saving.
Thus, the optimum ? obtained by the line search may
be suboptimal in terms of the training objective, but
potentially better than averaging for minimizing the
final task loss.
5 Experiments
Experiments were carried out on the NIST 2008
Chinese-to-English Open MT task. The training
data consists of nearly 5.6 million bilingual sen-
tences and additional monolingual data, English
Gigaword, for 5-gram language model estimation.
MT02 and MT06 were used as our tuning and devel-
opment testing, and MT08 as our final testing with
all data consisting of four reference translations.
We use an in-house developed hypergraph-based
toolkit for training and decoding with synchronous-
CFGs (SCFG) for hierarchical phrase-bassed SMT
(Chiang, 2007). The system employs 14 features,
consisting of standard Hiero-style features (Chiang,
2007), and a set of indicator features, such as the
number of synchronous-rules in a derivation. Two
5-gram language models are also included, one from
the English-side of bitexts and the other from En-
glish Gigaword, with features counting the number
of out-of-vocabulary words in each model (Dyer et
al., 2011). For faster experiments, we precomputed
translation forests inspired by Xiao et al. (2011). In-
stead of generating forests from bitexts in each it-
eration, we construct and save translation forests by
intersecting the source side of SCFG with input sen-
tences and by keeping the target side of the inter-
257
sected rules. n-bests are generated from the pre-
computed forests on the fly using the forest rescor-
ing framework (Huang and Chiang, 2007) with ad-
ditional non-local features, such as 5-gram language
models.
We compared four algorithms, MERT, PRO,
MIRA and our proposed online settings, online rank
optimization (ORO). Note that ORO without our op-
timization methods in Section 4 is essentially the
same as Pegasos, but differs in that we employ the
algorithm for ranking structured outputs with var-
ied objectives, hinge loss or softmax loss3. MERT
learns parameters from forests (Kumar et al., 2009)
with 4 restarts and 8 random directions in each it-
eration. We experimented on a variant of PRO4, in
which the objective in Eq. 4 with the hinge loss of
Eq. 5 was solved in each iteration in line 4 of Alg. 1
using an off-the-shelf solver5. Our MIRA solves the
problem in Equation 13 in line 7 of Alg. 2. For a sys-
tematic comparison, we used our exhaustive oracle
translation selection method in Section 3 for PRO,
MIRA and ORO. For each learning algorithm, we
ran 30 iterations and generated duplicate removed
1,000-best translations in each iteration. The hyper-
parameter ? for PRO and ORO was set to 10?5, se-
lected from among {10?3, 10?4, 10?5}, and 102 for
MIRA, chosen from {10, 102, 103} by preliminary
testing on MT06. Both decoding and learning are
parallelized and run on 8 cores. Each online learn-
ing took roughly 12 hours, and PRO took one day. It
took roughly 3 days for MERT with 20 iterations.
Translation results are measured by case sensitive
BLEU.
Table 1 presents our main results. Among the pa-
rameters from multiple iterations, we report the out-
puts that performed the best on MT06. With Moses
(Koehn et al., 2007), we achieved 30.36 and 23.64
BLEU for MT06 and MT08, respectively. We de-
note the ?O-? prefix for the optimized parameter up-
dates discussed in Section 4.1, and the ?-L? suffix
3The other major difference is the use of a simpler learning
rate, 1?k , which was very slow in our preliminary studies.
4Hopkins and May (2011) minimized logistic loss sampled
from the merged n-bests, and sentence-BLEU was used for de-
termining ranks.
5We used liblinear (Fan et al., 2008) at http://www.
csie.ntu.edu.tw/?cjlin/liblinear with the solver
type of 3.
MT06 MT08
MERT 31.45? 24.13?
PRO 31.76? 24.43?
MIRA-L 31.42? 24.15?
ORO-Lhinge 29.76 21.96
O-ORO-Lhinge 32.06 24.95
ORO-Lsoftmax 30.77 23.07
O-ORO-Lsoftmax 31.16? 23.20
Table 1: Translation results by BLEU. Results with-
out significant differences from the MERT baseline
are marked ?. The numbers in boldface are signif-
icantly better than the MERT baseline (both mea-
sured by the bootstrap resampling (Koehn, 2004)
with p > 0.05).
 0
 5
 10
 15
 20
 25
 30
 35
 40
 0  5  10  15  20  25  30
BL
EU
iteration
MIRA-L MT02MT08ORO-L MT02MT08O-ORO-L MT02MT08
Figure 1: Learning curves for three algorithms,
MIRA-L, ORO-Lhinge and O-ORO-Lhinge.
for parameter mixing by line search as described in
Section 4.2. The batch size was set to 16 for MIRA
and ORO. In general, our PRO and MIRA settings
achieved the results very comparable to MERT. The
hinge-loss and softmax objective OROs were lower
than those of the three baselines. The softmax ob-
jective with the optimized update (O-ORO-Lsoftmax)
performed better than the non-optimized version,
but it was still lower than our baselines. In the case
of the hinge-loss objective with the optimized update
(O-ORO-Lhinge), the gain in MT08 was significant,
and achieved the best BLEU.
Figure 1 presents the learning curves for three al-
gorithms MIRA-L, ORO-Lhinge and O-ORO-Lhinge,
in which the performance is measured by BLEU
258
MT06 MT08
MIRA 30.95 23.06
MIRA-L 31.42? 24.15?
OROhinge 29.09 21.93
ORO-Lhinge 29.76 21.96
OROsoftmax 30.80 23.06
ORO-Lsoftmax 30.77 23.07
O-OROhinge 31.15? 23.20
O-ORO-Lhinge 32.06 24.95
O-OROsoftmax 31.40? 23.93?
O-ORO-Lsoftmax 31.16? 23.20
Table 2: Parameter mixing by line search.
on the training data (MT02) and on the test data
(MT08). MIRA-L quickly converges and is slightly
unstable in the test set, while ORO-Lhinge is very sta-
ble and slow to converge, but with low performance
on the training and test data. The stable learning
curve in ORO-Lhinge is probably influenced by our
learning rate parameter ?0 = 0.2, which will be
investigated in future work. O-ORO-Lhinge is less
stable in several iterations, but steadily improves its
BLEU. The behavior is justified by our optimized
update procedure, in which the learning rate ?k is
used as a tradeoff parameter. Thus, it tries a very
aggressive update at the early stage of training, but
eventually becomes conservative in updating param-
eters.
Next, we compare the effect of line search for pa-
rameter mixing in Table 2. Line search was very
effective for MIRA and O-OROhinge, but less effec-
tive for the others. Since the line search procedure
directly minimizes a task loss, not objectives, this
may hurt the performance for the softmax objective,
where the margins between the correct and incorrect
translations are softly penalized.
Finally, Table 3 shows the effect of batch size se-
lected from {1, 4, 8, 16}. There seems to be no clear
trends in MIRA, and we achieved BLEU score of
24.58 by setting the batch size to 8. Clearly, set-
ting smaller batch size is better for ORO, but it is
the reverse for the optimized variants of both the
hinge and softmax objectives. Figure 2 compares
ORO-Lhinge and O-ORO-Lhinge on MT02 with dif-
ferent batch size settings. ORO-Lhinge converges
faster when the batch size is smaller and fine tun-
 20
 25
 30
 35
 40
 0  5  10  15  20  25  30
BL
EU
iteration
ORO-L batch-16batch-8batch-4O-ORO-L batch-16batch-8batch-4
Figure 2: Learning curves on MT02 for ORO-Lhinge
and O-ORO-Lhinge with different batch size.
ing of the learning rate parameter will be required
for a larger batch size. As discussed in Section 3,
the smaller batch size means frequent updates to pa-
rameters and a faster convergence, but potentially
leads to a poor performance since the corpus-BLEU
is approximately computed in a local batch. Our op-
timized update algorithms address the problem by
adjusting the tradeoff between the amount of up-
date to parameters and the loss, and perform better
for larger batch sizes with a more accurate corpus-
BLEU.
6 Related Work
Our work is largely inspired by pairwise rank op-
timization (Hopkins and May, 2011), but runs in
an online fashion similar to (Watanabe et al., 2007;
Chiang et al., 2008b). Major differences come from
the corpus-BLEU computation used to select oracle
translations. Instead of the sentence-BLEU used by
Hopkins andMay (2011) or the corpus-BLEU statis-
tics accumulated from previous translations gener-
ated by different parameters (Watanabe et al., 2007;
Chiang et al., 2008b), we used a simple batch lo-
cal corpus-BLEU (Haddow et al., 2011) in the same
way as an online approximation to the objectives.
An alternative is the use of a Taylor series approxi-
mation (Smith and Eisner, 2006; Rosti et al., 2011),
which was not investigated in this paper.
Training is performed by SGD with a parame-
ter projection method (Shalev-Shwartz et al., 2007).
Slower training incurred by the larger batch size
259
MT06 MT08
batch size 1 4 8 16 1 4 8 16
MIRA-L 31.28? 31.53? 31.63? 31.42? 23.46 23.97? 24.58 24.15?
ORO-Lhinge 31.32? 30.69 29.61 29.76 23.63 23.12 22.07 21.96
O-ORO-Lhinge 31.44? 31.54? 31.35? 32.06 23.72 24.02? 24.28? 24.95
ORO-Lsoftmax 25.10 31.66? 31.31? 30.77 19.27 23.59 23.50 23.07
O-ORO-Lsoftmax 31.15? 31.17? 30.90 31.16? 23.62 23.31 23.03 23.20
Table 3: Translation results with varied batch size.
for more accurate corpus-BLEU is addressed by
optimally scaling parameter updates in the spirit
of a passive-aggressive algorithm (Crammer et al.,
2006). The derived algorithm is very similar to
MIRA, but differs in that the learning rate is em-
ployed as a hyperparameter for controlling the fit-
ness to training data which decays when training
proceeds. The non-uniform sub-gradient based up-
date is also employed in an exponentiated gradient
(EG) algorithm (Kivinen and Warmuth, 1997; Kivi-
nen andWarmuth, 2001) in which parameter updates
are maximum-likely estimated using an exponen-
tially combined sub-gradients. In contrast, our ap-
proach relies on an ultraconservative update which
tradeoff between the amount of updates performed
to the parameters and the progress made for the ob-
jectives by solving a QP subproblem.
Unlike a complex parallelization by Chiang et
al. (2008b), in which support vectors are asyn-
chronously exchanged among parallel jobs, train-
ing is efficiently and easily carried out by distribut-
ing training data among shards and by mixing pa-
rameters in each iteration (McDonald et al., 2010).
Rather than simple averaging, new parameters are
derived by linearly interpolating with the previously
mixed parameters, and its weight is determined by
the line search algorithm employed in (Och, 2003).
7 Conclusion
We proposed a variant of an online learning al-
gorithm inspired by a batch learning algorithm of
(Hopkins and May, 2011). Training is performed by
SGDwith a parameter projection (Shalev-Shwartz et
al., 2007) using a larger batch size for a more accu-
rate batch local corpus-BLEU estimation. Parameter
updates in each iteration is further optimized using
an idea from a passive-aggressive algorithm (Cram-
mer et al., 2006). Learning is efficiently parallelized
(McDonald et al., 2010) and the locally learned pa-
rameters are mixed by an additional line search step.
Experiments indicate that better performance was
achieved by our optimized updates and by the more
sophisticated parameter mixing.
In future work, we would like to investigate other
objectives with a more direct task loss, such as max-
margin (Taskar et al., 2004), risk (Smith and Eisner,
2006) or softmax-loss (Gimpel and Smith, 2010),
and different regularizers, such as L1-norm for a
sparse solution. Instead of n-best approximations,
we may directly employ forests for a better con-
ditional log-likelihood estimation (Li and Eisner,
2009). We would also like to explore other mix-
ing strategies for parallel training which can directly
minimize the training objectives like those proposed
for a cutting-plane algorithm (Franc and Sonnen-
burg, 2008).
Acknowledgments
We would like to thank anonymous reviewers and
our colleagues for helpful comments and discussion.
References
