Representing and Integrating Linguistic Knowledge* 
Danie l  Ju ra fsky  
573 Ewms Ha l l  
Computer  Sc ience  D iv i s ion  
Un ivers i ty  of  Ca l i fo rn ia  a t  Berke ley  
Berke ley ,  CA  94720 
ju ra fsky@teak .Berke ley .EDU 
In t roduct ion  
This paper describes a theory of the representation 
and use of linguistic knowledge in a natural anguage 
understanding system. The representation system 
draws much of its insight from the linguistic the- 
ory of Fillmore el al. (1988). This models knowl- 
edge of language as a large collection of grammati- 
cal constructions, each a description of a linguistic 
regularity. I describe a representation language for 
constructions, and principles for encoding linguistic 
knowledge in this representation. The second part of 
the theory is a conceptual analyzer which is designed 
to model the on-line nature of the hunmn language 
understanding mechanism. I discuss the core of this 
analyzer, an information-combining operation called 
integration which combines constructions to produce 
complete interpretations of an utterance. 
Representat ion  
A natural way to model knowledge of language is 
as a large collect.ion of facts or regularities about 
the language. In this theory I use a single repre- 
sentational device, the grammatical construction, to 
represent hese regularities. Our entire model of lin- 
guistic knowledge consists of a database of these con- 
structions, uniforrnly representing lexieal knowledge, 
syntactic knowledge, and semantic knowledge. 
Although the notion of grammatical construction I 
use is based on that of Fillmore e! al. (1988) it differs 
in a few respects. Filhnore et al define a construe- 
tion as a structure which represents a "pairi~9 of a 
syntaclic pallcrn with, a meanin 9 sl'ruclure". Such a 
construction is a sig., in the sense of Saussure, or a 
rule-pairing in the sense of Montague. 1 use a some- 
what extended notion: a construction is a relation 
between one information structure and one or more 
others. These information structures can be seman- 
tic, syntactic or both. Thus, whereas the "sign" ex- 
pressed a relation between a set of ordered phonemes 
~md a meaning structure, the construction abstracts 
over this by replacing 'ordered sets of phonemes' with 
*I want to express my tha.nks to Peter Norvig, Nigel 
Ward, and l{obert Wilensky for many helpfld com- 
ments and discussions on these ideas. This research was 
~ponsored by the Defense Advanced Research Projects 
Agency (DoD), monitored by the Space and Naval War- 
Ikre Systems Command under N00039-88-C-0292, and 
the Office of Naval Research, under contract N00014-89.- 
,1-3205. 
abstractions over them. These abstractions can be 
syntactic or semantic ways of expressing more ab- 
stract categories to which these phoneme sequences 
belong. The construction is then a part-whole struc- 
turing relating these categories. 
An Example  Const ruct ion  
To make this idea more concrete, consider some spe- 
cific examples from the grammar, which currently 
includes about 30 constructions. In the examples I 
will be focusing on the knowledge that is necessary 
to handle the sentence: 
"IIow cart I find out how much disk space I
am using ?" 
The top-level construction for the sentence is the 
WhNonSub jeetQuest lon  construction. This con- 
struction is a sub-type of the WhQuest lon  con- 
struction. The wh-questions are those which begin 
with a wh-element - -  an element which is semanti- 
cally questioned. In the WhNonSub jeetQuest lon  
this questioned element begins the sentence but does 
not flmction as the grammatical subject of the sen- 
tence. Following are two other examples of WhNon-  
S ab ject  Q uest ions:  
Why did you run ~he wrong way? 
What did you pick up at the store? 
'/'he construction is represented in figure 1 below: 
(constr WhNonSubjectQuestion (Freq p) 
\[(a Sq 
($q AIO Question) 
($var -Queried $q) 
(\[$\v $\presup\] -Presupposed Sq))\] 
-> 
\[(a St 
(St AIO Identify) 
(Svar -Specified St) 
($presup -Presupposed St)) \] 
\[(a $v 
($V AIO SubjectSecondClause))\]) 
F igure  1 
Figure 1 can be summarized as follows: There is 
a construction in English called WhNonSub jec -  
tQuest ion .  It consists of' two constituents, $t and 
Sv, and a "whole", $% (In tile example sentence, 
the St constituent consists only of the word "How", 
1 199 
while t:he $v constituent consists of the phrase "can 
I find out how much disk space I am using.") These 
three elements consist of statements in a knowledge 
representation language based on Wilensky (1986). 
The representation language is relatively perspicu- 
ous, not differing greatly from other popular repre- 
sentation languages (such as KL-ONE (Brachman & 
Schmolze 1985)). The operator "a" creates an exis- 
tentially quantified variable, and is followed by the 
variable name and a set of statements in its scope. 
The infix operator "A IO" (An  Instance Of) estab- 
lishes one element as an instance of another, and the 
infix operator "AKO"  (A Kind Of) establishes one 
element as a sub-type of another. 
The first constituent, St, is an instance of the 
Ident i fy  concept, which will be described later. 
This concept contains two relations, Ident i fy -  
Speci f ied and Ident i fy -P resupposed .  (Note that 
they are referred to simply as -Spec i f ied  and - 
P resupposed . )  The second constituent, $v, must 
be an instance of the construction Sub jec tSecond-  
Clause.  This is a clause where the subject follows an 
initial "verbal auxiliary. The name Sub jec tSecond-  
C lause  was chosen to replace the traditional term 
Aux-Inversion in order to avoid employing terminol- 
ogy that uses the "movement" metaphor. 
The construction builds the $q element, which is 
an instance of the Quest ion  concept. Note that 
the variable which fills the -Quer ied  relation of 
the Quest ion  is the same element hat fills the - 
Speci f ied relation in St. The -P resupposed  re- 
lation is filled with the integration of the semantics 
of St and Sv. The integration process is discussed 
below. 
Each constituent (enclosed in square brackets) is 
composed of a set of semantic relations. These con- 
stituents correspond to what are cMled informational 
elements in the unification literature. Each consists 
of a group of semantic relations expressing some in- 
formation about some linguistic entity. 
The relation among these informational elements 
is represented by the right-arrow "--," symbol. In 
the current version of thc representational system, 
the right-arrow indicates the eonflation of part-whole 
and ordering relations. That is, the default is for 
constructions to be ordered. In order to indicate a 
part-whole relation without the ordering relations, 
the right-arrow is followed by the keyword "UN- 
ORDERED".  Figure 2 below provides an example 
of an unordered construction. 
Features of the Representation 
The representation language and the granm~ar of- 
fer a number of distinguishing features. First is the 
ability to define constituents of constructions seman- 
tically as well as syntactically. For example, note 
that the first constituent of the WhNonSub jec -  
tQuest ion ,  St, was defined ~s any informational 
element which is an instance of a certain Ident i fy  
concept. The Ident i fy  concept indicates ome ques- 
tion about the identity of the individual filling the 
Ident i fy -Spec l f ied  relation. The information that 
is known about the individual fills the Ident i fy -  
P resupposed  relation. The Ident i fy  concept is the 
main characteristic of the lexieal semantics of the 
wh-words. Thus this constituent is specified seman- 
tically, simply by requiring the presence of the Iden-  
t i fy semantics. In more traditional grammars, the 
constituent would be defined syntactically~ as some 
sort of WhPhrase .  Capturing the syntax of the 
WhPhrase  would entail duplicating huge parts of 
the grammar or introducing significant syntactic ap- 
paratus. Since the semantics of Ident i fy  must be 
in the grammar anyhow, using it to specify the con- 
struction simplifies the grammar at no cost 1 
A construction's constituents can be constrained 
to be instances of other constructions. For example, 
the second constituent in figure 1 is constrained to be 
an instance of Sub jec tSecondClause .  But these 
constraints are the only form of syntactic knowledge 
that this representational system allows. That is, 
constituency and ordering are the only syntactic re- 
lations in the system. All others are semantic. In 
keeping with these last two ideas, this representa- 
tion dispenses with any enrichment of surface forrn. 
Thus phenomena which might traditionally be han- 
dled by gaps, traces, or syntactic oindexing are han- 
dled with semantic relations. Thus, as we saw above, 
the first constituent of the WhNonSnb jectQues -  
t lon construction is specified semantically, and no 
gaps or traces are involved 2 
Relations Among Const ruct ions  
As with other kinds of knowledge, linguistic knowl- 
edge includes an abstraction hierarchy. All example 
of two constructions related by abstraction is given 
in figure 2. Constructions are also augmented by 
information concerning their frequency. The use of 
frequency information in comprehension (suggested 
in Bresnan 1982) is discussed in Wu (1989), and is 
not discussed further here. 
I noted above that a construction may constrain 
one of its constituents to be an instance of some other 
construction. In a similar manner, a construction 
may require that several of its constituents partici- 
pate together in a second construction. This is repre- 
sented by including specifications for additional con- 
structions after the keyword WITH.  The relevant 
constituents of these extra constructions are then 
marked with the proper constituent variables from 
the main construction. 
To illustrate the last few points, consider the 
VerbPar t l c le l  construction in figure 3. It is one 
of two constructions in which discontinuous phrasal 
verbs like "find out" or "look up" occur. These 
two constructions pecify the ordering of the verb 
and particle, differing in that VerbPar t i c le2 ,  which 
covers phrases like "find it out", includes an extra 
1This makes it more difficult for the parser to index 
constructions to consider. Because a construction's con- 
stituents may be defined by any set of semantic relations 
rather than by a small set of syntactic ategories, it is 
no longer possible to simply look for a syntactic han- 
dle in the input. Indeed, the "construction access prob- 
lent" becomes much more like the general memory access 
problem. In this sense this analyzer esembles the Direct 
Memory Access Parser of Riesbeck (1986). 
2Jur~fsky (1988) discusses how other phenomena clas- 
sically handled by transformations and redundancy rules 
can be represented as constructions. 
200  2 
constituent. But VerbPar t le le l  and VerbPar t l -  
cle2 both require that their constituents be filled by 
phrasal verbs like "find out". This fact is represented 
by requiring that these constituents be instances of 
the unordered Lex lca lVerbPar t i c le  construction, 
which is the ancestor of all these phr~al verbs. In 
the representation f the VerbPar t i c le  construction 
below, constituents Sv and Sp are constrained to 
be the Sverb and $part lc le  respectively of a Lexi- 
ca lVerbPar t l c le  construction. Since F indOut  is a 
subtype of Lex lea lVerbPart lc le ,  it meets the nec- 
essary constraints, and can integrate with VerbPar -  
tlcle a 
(constr VerbParticlel (Freq Sz) 
\[ (a $a 
($a hIO Verb))\] 
-> 
\[(a $v)\] 
\ [ (a$p) \ ]  
WITH 
(constr LexicalVerbParticle 
\[ (a Sa )\]  
-> UNOKDERED 
\[ (a ?v)\] 
\[ (a$p) \ ] ) )  
F igure  2 
(constr LexicalVerbParticle (Freq Sz) 
\[\] 
-> UNORDERED 
\[(a $v)\] 
\ [ (a$p) \ ] )  
(constr FindOut AIO LexicalVerbParticle 
(Freq Sz) 
\ [ (a $~ 
($ IA IO  FindingOut))\] 
~> UNORDERED 
\[$verb (word find)\] 
\[$particle (word out)\]) 
F igure  3 
The  Bas ic  In tegrat ion  Operat ion  
Associated with the representational system is an 
information-combining operation called integration, 
used here in two ways. The more complex one is 
discussed in the next section. The simpler version of 
integration is used to match informational elements 
to constituents of a constructions. The set of rela- 
tions which constitute a constituent act as a set of 
constraints on any candidate constituents. For ex- 
ample, consider what semantics must be prcsent in 
an informational element for it to be a constituent of 
aNote that a parse which uses WITH constraints does 
not have a parse tree, but a parse graph, q'his is because 
both VerbParticle and LexicalVerbParticle would occur 
in the parse tree above the phrase "find out". However, 
this grammar obviates the need to keep a parse tree at all, 
as the grammar itself specifics how semantic integration 
is to be (tone. 
the HowSca le  construction. Examples of this con- 
struction include: 
IIow wide/strong/aceurale ... ? 
How much~often/quickly~far ... ? 
How many ... ? 
The construction has two constituents, the lexi- 
cal item "how" and a second constituent which is 
semantically specified to be some sort of semantic 
scale. This constituent may be an adjective, an ad- 
verb, or a quantifer so long as it has the proper 
semantics. In the cases above, the scales range over 
such things as width, strength, speed, and amount. 
The construction takes a constituent with these se- 
mantics, and builds an instance of the Ident i fy  con- 
cept. The -P resupposed  relation of this Ident i fy  
concept is filled by the semantics of the constitucnt. 
The -Spec i f ied  relation is bound to the -Locat ion  
of the presupposed object on this scale. That is, the 
memfing of the construction is something like "The 
localion of objec~ Sz on scale Ss is in queslion". 
(constr HowScale 
\ [ (a $? 
($? 
($x 
($s 
($x 
=> 
\[ $h 
\[ (a 
hIO Identify) 
-Specified $i) 
-Presupposed $i) 
-Location Sz Ss)\] 
(word how) \] 
$s 
($s AI0 Scale) 
($z -On Ss)) \ ]  
F igure  4 
In order for an informational element o integrate 
with the Ss constituent of the HowSca le  construc- 
tion, its sernantics must already include an instance 
of a scale, with some object on the scale. As in stan- 
dard unification, the constraints are matched with 
the elements in a recursive fashion, binding variables 
in the process. Unlike unification, a match cannot 
succeed if a candidate constituent is merely compat- 
ible with the required information. For example, the 
element \[ (a Sx ($x A IO  ScAle))\] would u,~ify with 
the $s constituent. However, it would ~ot inlegratc, 
because it lacks the -On relation. We can summarize 
the simple integration algorithm as follows: 
S imple In tegrat ion  A lgor i thm:  
Unify the set of constituent relations with 
the relations present in the candidate, sub- 
ject to the constraint that every relation in 
the constraint must already bc instantiated 
in the candidate. 
There arc times when wc want the semantics of 
unification rather than integration. That is, we may 
want to ensure that a certain relation is in the se- 
mantics of a construction regardless of which con- 
stituents it may also occur in. We can accomplish 
this by putting information in the "whole" of the 
construction's part-whole structure. For example, in 
figure 4, the information "($x -Locat ion  Sz $s)" is 
added to the final semantics whether or not it was 
present in the $s constituent. 
Thus there is an asymmetry in the application of 
the integration operation, caused by the part-whole 
3 201  
structure of the construction. The relations in the 
constituent slots of the construction definition are 
viewed as conslraints on candidate constituents. The 
relations in the "whole" element, on the other hand, 
arc used as instructions for creating a whole semantic 
structure. 
Note that this interpretation of the construction 
is limited to its use in comprehension. When a con- 
struction is used in generation the exact opposite 
situation holds - -  the "whole" element imposes con- 
straints, while the constituents give instructions. 
More  Complex  In tegrat ion  
The algorithm above is sufficient o match candidate 
constituents to the constraints of a construction. But 
this sort of simplistic combination is not common 
in natural language when combining constructions. 
Integrating constituents usually involves modifying 
some of t:he structure of one of the constituents. A 
particularly common type of modification involves 
binding the value of one constituent o some open 
variable in another constituent 4. That is, we must 
decide whether two relations are at the same level, or 
whether one should fill some semantic gap in another. 
In some cases, deciding the level and finding an ap- 
propriate semantic gap can be quite simple. For ex- 
ample, part of the verb phrase construction is shown 
in figure 5 below. Note that the variable $ /v  is 
marked with a slash. This indicates that the verb 
$v is the matrix structure, while the complement $c 
is to be integrated to some gap inside of $v. This 
slash can be used for any variable to indicate that it 
is the matrix for some integration. 
(constr gerbPhrase (Freq Sp) 
\[ (a \ [$/v $c\]) 
-> 
\[ (a Sv 
($v AIO Verb))\] 
\[ (a $c)\]  
) 
F igure  5 
For a more complex example, consider figure 1 (du- 
plicated below), examining how the two constituents 
of the WhNonSub jectQuest ion  are integrated for 
the sentence "How can \[ f ind out how much disk 
space I am usingF'. I will not discuss the details of 
the Sv constituent except to note that its semantics 
involve an Ab i l i tyState  predicated of some person 
asking a question, and concerning some f inding-out 
action. 
The first constituent (St) of the WhNonSub-  
jec tQuest lon  is filled here by the lexical construc- 
tion how6, one of the various "how" constructions. 
How6 is concerned with the means of some action, 
asking for a specification of the means or plan by 
which some goal is accomplished. We can ignore for 
4This occurs in such common phenomena s WH- 
movement, Y-Movement, Topicalization, and other phe- 
nomenon classically analyzed as movement rules, where 
there is some long-distance link between some ele- 
ment and the valence-bearing predicate into which it 
integrates. 
(constr WhNonSubj ectQuestion (Freq p) 
\[(a $q 
($q AIO Question) 
($var -Queried Sq) 
(\[$\v $\presup\] -Presupposed $q))\] 
-> 
\[(a St 
(St AIO Identify) 
($var -Specified St) 
($presup -Presupposed St)) \] 
\[(a $v 
($v AIO SubjectSecondClause))\]) 
F igure  I 
now exactly how this sense is related to the other 
senses of "how". 
(word how6 (Freq x) 
\[(a Sh 
($h AIO Identify) 
(Sp -Specified $h) 
($X -Presupposed Sh) 
($x AIO Planfor) 
($p -Plan $x) 
($g-Goal Sx))\]) 
Figure 6 
Given the semantics of the two constituents of the 
WhNonSub jectQuest lon  construction, the final 
result of the integration will look something like fig- 
ure 7. 
\[(a $q 
($q Aio question) 
($p -Queried $q) 
($pr -Presupposed $q) 
($pr AIO Planfor) 
($p -Plan $pr) 
($g -Goal $pr) 
($g AIO AbilityState))\] 
Figure 7 
tlere in integrating $ /v  and $ /presup ,  the algo- 
r ithm finds a gap inside the second structure. That 
is, the integration algorithm integrated the Planfor of 
the first constituent with the AbilityState of the sec- 
ond, in effect unifying the variable Sg in the PlanFor 
structure with Sv, the AbilityStatc structure. Thus 
the complete algorithm might be expressed as fol- 
lows: 
Full  In tegrat ion  A lgor i thm:  Inte- 
grate the set of constituent relations with 
the relations present in the candidate by 
finding an appropriate gap (variable) in one 
of the two structures to integrate with the 
other. 
Integration is an augmentation of unification. In 
order to handle more complex constructions, the op- 
eration would need to be augmented further, adding 
more inferential power. For example, a certain 
class of inference is required to integrate construc- 
tions like DoubleNoun (Wu 1990), where the re- 
lation between the constituents can be quite indi- 
rect and contextually-influenced. Such an integra- 
tion algorithm might also need to make the kind 
202  4 
ol! metaphoric inferences tudied by Martin (1988), 
ok: the abductive inferences of Charniak & Goldman 
(11988) or Hobbs et al (1988). But rather than mak- 
ing these inferences in the pipelined fashion that 
these other mechanisms use, augmenting the inte- 
gi:ation algorithm allows these inferences to be made 
ir~ an on-line manner. 
Prev ious  Research  
The theory I have presented raws many elements 
from other theories of grammar, especially Fillmore 
el al. (1988). The use of ordered and unordered 
constructions is based on the ID/LP notation of 
Cazdar et al (1985) The theory differs from these 
and most other grammars (such as Bresnan (1982), 
Marcus (1980), Pereira (1985)) in allowing semantic 
constraints on constituents (indeed in emphasizing 
them), and in disallowing syntactic relations other 
than ordering and constituency. It also differs by 
constraining the grammar to be semantically intbr- 
mative enough to allow the analyzer to produce in- 
terpretations in an on-line fashion. 
As for the tradition which has concentrated on se- 
mantic analysis, this theory owes much to such sys- 
tems as ELI (Pdesbeck & Schank 1978) and the Word 
Expert Parser (Small & Rieger 1982). It ditfers from 
these in its commitment to representing high-level 
linguistic onstructions, to the use of declarative rep- 
resentations, and to capturing linguistic generaliza- 
tions. 
A nmnber of earlier systems have integrated lin- 
guistic knowledge with other knowledge, including 
P~I--KLONE (Bobrow & Webber 1980) and Jacobs's 
(1985) ACE/KING system, on which this theory 
draws heavily. The use of constructions here draws 
al~o on the pattern-concept pair of Wilensky & Arens 
(1980) and Becket's 1975 work on the phra.sal lexi- 
con. 
The integration operation is based on unification 
in ways discussed above. Unification was first pro- 
posed by Kay (1979), and has since been used and 
ex~ended by many other theories of grammar. 
Conc lus ion  
I draw three conclusions for the representation and 
use of linguistic knowledge: 
:1. Allowing constructions to include semantic as 
well as syntactic constraints removes a great 
deal of complexity from the syntactic omponent 
of a grammar. This is useful because no cor- 
responding complexity is added to the semantic 
component. Semantic knowledge which must be 
in the system anyway is employed. 
2. Committing to exploring semantically rich con- 
structions like HowSeale froln a semantic per- 
spective produces a grammar of sufficient rich- 
ness to .allow lexical semantics to influence the 
integration process, and thus the interpretation, 
in an on-line way. 
3. Using a single representational device, the gram- 
matical construction, avoids the proliferation of 
syntactic devices and simplifies the representa- 
tional theory. This may also simplify the torte- 
sponding learning theory. 
4. Extending unification-style approaches fi'om 
syntax to semantics requires augmenting the 
feature-based unification operation to richer, re- 
lational knowledge representation languages. I 
have shown how one such extension has been 
implemented, allowing some gap-seeking intelli- 
gence to guide the integration process. 
References  
Backer, J. (1975). The phrasal lexicon. In R. Schank 
& B. L. Nash-Webber, editors, Proceedings of 
the First Interdisciplinary Workshop on Theo- 
retical Issues in Natural Language Processing, 
Cambridge, MA. 
Bobrow, R.. J. & B. Webber (1980). Knowledge 
representation for syntactic/semantic process- 
ing. In Proceedings of the First National Con- 
ferenee on Artificial Intelligence, pp. 316-323. 
Morgan Kaufmann. 
Brachman, R. J. & J. G. Schmolze (1985). An 
overview of the KL-ONE knowledge represen- 
tation system. Cognitive Science, 9(2):171-216. 
Bresnan, J., editor (1982). The Mental Represen- 
tation of Grammatical Relations. MIT Press, 
Cambridge. 
Charniak, E. & R. Goldman (1988). A logic for se- 
mantic interpretation. In Proceedings of the 26th 
Annual Conference of the Association for Com- 
putational Linguistics. 
Fillmore, C., P. Kay, & M. C. O'Connor (1988). 
Regularity and idiomaticity in grammatical con- 
structions: The case of let alne. Language, 
64(3):501-538. 
Gazdar, G., E. Klein, G. Pullum, & I. Sag (1985). 
Generalized Phrase Structure Grammar. Basil 
Blackwell, Oxford. 
IIobbs, J. R., M. Stickel, P. Martin, & D. Edwards 
(1988). Interpretation as abduction, in Pro- 
ceedings of the 26th Annual Conference of the 
Association for Computational Linguistics, pp. 
95-103, Buffalo, NY. 
Jacobs, P. (1985). A knowledge-based approach to 
language generation. Technical Report 86/254, 
University of California at Berkeley Computer 
Science Division, Berkeley, CA. 
Jurafsky, D. (1988). Issues in relating syntax and 
semantics. In Proceedings of the Twelfth Inter- 
national Conference on Computational Linguis- 
tics, pp. 278-284, Budapest. 
Kay, M. (1979). Functional grammar. In Proceedings 
of the Fifth Annual Meeting of the Berkeley Lin- 
guistics Society, pp. 142-158, Berkeley, CA. 
Marcus, M. P. (1980). A Theory of Syntactic Recog- 
nition for Natural Language. MIT Press, Cam- 
bridge. 
Martin, J. (1988). A computational theory of 
metaphor. Technical Report 88/465, University 
of California at Berkeley, Computer Science Di- 
vision, Berkeley, CA. 
5 203 
Pereira, F. C. N. (1985). Characterization of at- 
tachment preferences. In D. R. Dowty, L. Kar- 
tunnen, & A. Zwicky, editors, Natural Lan- 
guage Parsing, pp. 307-319. Cambridge Univer- 
sity Press, New York. 
Riesbeck, C. K. (1986). From conceptual analyzer to 
direct memory access parsing: An overview. In 
Advances in Cognitive Science 1, pp. 236-258. 
Ellis Horwood, Chichester. 
Riesbeck, C. K. & R. C. Schank (1978). Comprehen- 
sion by computer: Expectation-based analysis 
of sentences in context. In W. J. M. Levelt & 
G. B. F. d'Arcais, editors, Studies in the percep- 
tion of language, pp. 247-293. Wiley, London. 
Small, S. L. & C. Rieger (1982). Parsing and compre- 
hending with word experts. In W. G. Lehnert 
&: M. H. Ringlet editors, Strategies for Natural 
Language Processing, pp. 89-147. Lawrence Erl- 
baum, New Jersey. 
Wilensky, R. (1986 i. Some problems and proposals 
for knowledge representation. In J. L. Kolodner 
& C. K. Riebeck, editors, Experience, Memory, 
and Reasoning, pp. 15-28. Lawrence Erlbaum, 
New Jersey. 
Wilensky, R. & Y. Arens (1980). Phran - a 
knowledge-based approach to natural language 
analysis. Technical Report UCB/ERL M80/34, 
University of California at Berkeley, Electronics 
Research Laboratory, Berkeley, CA. 
Wu, D. (1989). A probabalistic approach to marker 
propagation. In Proceedings of the Eleventh In- 
ternational Joint Conference on Artificial Intel- 
ligence, pp. 574-580, Detroit, MI. Morgan Kauf- 
mann. 
Wu, D. (1990). Probabalistic unification-based in- 
tegration of syntactic and semantic preferences 
for nominal compounds. In Proceedings of the 
Thirteenth International Conference on Compu- 
tational Linguistics, ttelsinki. 
204 6 
