Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 276?284,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Efficient Parsing of Well-Nested Linear Context-Free Rewriting Systems
Carlos G?mez-Rodr?guez1, Marco Kuhlmann2, and Giorgio Satta3
1Departamento de Computaci?n, Universidade da Coru?a, Spain, cgomezr@udc.es
2Department of Linguistics and Philology, Uppsala University, Sweden, marco.kuhlmann@lingfil.uu.se
3Department of Information Engineering, University of Padua, Italy, satta@dei.unipd.it
Abstract
The use of well-nested linear context-free
rewriting systems has been empirically moti-
vated for modeling of the syntax of languages
with discontinuous constituents or relatively
free word order. We present a chart-based pars-
ing algorithm that asymptotically improves the
known running time upper bound for this class
of rewriting systems. Our result is obtained
through a linear space construction of a binary
normal form for the grammar at hand.
1 Introduction
Since its earliest years, one of the main goals of
computational linguistics has been the modeling of
natural language syntax by means of formal gram-
mars. Following results by Huybregts (1984) and
Shieber (1985), special attention has been given to
formalisms that enlarge the generative power of con-
text-free grammars, but still remain below the full
generative power of context-sensitive grammars. On
this line of investigation, mildly context-sensitive
grammar formalisms have been introduced (Joshi,
1985), including, among several others, the tree ad-
joining grammars (TAGs) of Joshi et al (1975).
Linear context-free rewriting system (LCFRS), in-
troduced by Vijay-Shanker et al (1987), is a mildly
context-sensitive formalism that allows the deriva-
tion of tuples of strings, i.e., discontinuous phrases.
This feature has been used to model phrase structure
treebanks with discontinuous constituents (Maier and
S?gaard, 2008), as well as to map non-projective de-
pendency trees into discontinuous phrase structures
(Kuhlmann and Satta, 2009).
Informally, in an LCFRS G, each nonterminal can
generate string tuples with a fixed number of compo-
nents. The fan-out of G is defined as the maximum
number of tuple components generated by G. During
a derivation of an LCFRS, tuple components gener-
ated by the nonterminals in the right-hand side of
a production are concatenated to form new tuples,
possibly adding some terminal symbols. The only re-
striction applying to these generalized concatenation
operations is linearity, that is, components cannot be
duplicated or deleted.
The freedom in the rearrangement of components
has specific consequences in terms of the computa-
tional and descriptional complexity of LCFRS. Even
for grammars with bounded fan-out, the universal
recognition problem is NP-hard (Satta, 1992), and
these systems lack Chomsky-like normal forms for
fixed fan-out (Rambow and Satta, 1999) that are es-
pecially convenient in tabular parsing. This is in con-
trast with other mildly context-sensitive formalisms,
and TAG in particular: TAGs can be parsed in poly-
nomial time both with respect to grammar size and
string size, and they can be cast in normal forms
having binary derivation trees only.
It has recently been argued that LCFRS might be
too powerful for modeling languages with discontin-
uous constituents or with relatively free word order,
and that additional restrictions on the rearrangement
of components might be needed. More specifically,
analyses of both dependency and constituency tree-
banks (Kuhlmann and Nivre, 2006; Havelka, 2007;
Maier and Lichte, 2009) have shown that rearrange-
ments of argument tuples almost always satisfy the
so-called well-nestedness condition, a generalization
276
of the standard condition on balanced brackets. This
condition states that any two components x1, x2 of
some tuple will never be composed with any two
components y1, y2 of some other tuple in such a way
that a ?crossing? configuration is realized.
In this paper, we contribute to a better understand-
ing of the formal properties of well-nested LCFRS.
We show that, when fan-out is bounded by any inte-
ger ? ? 1, these systems can always be transformed,
in an efficient way, into a specific normal form with
no more than two nonterminals in their productions?
right-hand sides. On the basis of this result, we
then develop an efficient parsing algorithm for well-
nested LCFRS, running in timeO(? ? |G| ? |w|2?+2),
where G and w are the input grammar and string,
respectively. Well-nested LCFRS with fan-out ? = 2
are weakly equivalent to TAG, and our complex-
ity result reduces to the well-known upper bound
O(|G| ? |w|6) for this class. For ? > 2, our upper
bound is asymptotically better than the one obtained
from existing parsing algorithms for general LCFRS
or equivalent formalisms (Seki et al, 1991).
Well-nested LCFRS are generatively equivalent
to (among others) coupled context-free grammars
(CCFG), introduced by Hotz and Pitsch (1996).
These authors also provide a normal form and de-
velop a parsing algorithm for CCFGs. One difference
with respect to our result is that the normal form for
CCFGs allows more than two nonterminals to appear
in the right-hand side of a production, even though no
nonterminal may contribute more than two tuple com-
ponents. Also, the construction in (Hotz and Pitsch,
1996) results in a blow-up of the grammar that is ex-
ponential in its fan-out, and the parsing algorithm that
is derived runs in time O(4? ? |G| ? |w|2?+2). Our
result is therefore a considerable asymptotic improve-
ment over the CCFG result, both with respect to the
normal form construction and the parsing efficiency.
Finally, under a practical perspective, our parser is a
simple chart-based algorithm, while the algorithm in
(Hotz and Pitsch, 1996) involves two passes and is
considerably more complex to analyze and to imple-
ment than ours.
Kanazawa and Salvati (2010) mention a normal
form for well-nested multiple context-free grammars.
Structure In Section 2, we introduce LCFRS and
the class of well-nested LCFRS that is the focus of
this paper. In Section 3, we discuss the parsing com-
plexity of LCFRS, and show why grammars using
our normal form can be parsed efficiently. Section 4
presents the transformation of a well-nested LCFRS
into the normal form. Section 5 concludes the paper.
2 Linear Context-Free Rewriting Systems
We write [n] to denote the set of positive integers up
to and including n: [n] = {1, . . . , n}.
2.1 Linear, non-erasing functions
Let ? be an alphabet. For integers m ? 0 and
k1, . . . , km, k ? 1, a total function
f : (??)k1 ? ? ? ? ? (??)km ? (??)k
is called a linear, non-erasing function over ? with
type k1 ? ? ? ? ? km ? k, if it can be defined by an
equation of the form
f(?x1,1, . . . , x1,k1?, . . . , ?xm,1, . . . , xm,km?) = ~? ,
where ~? is a k-tuple of strings over the variables on
the left-hand side of the equation and ? with the
property that each variable occurs in ~? exactly once.
The values m and k are called the rank and the fan-
out of f , and denoted by ?(f) and ?(f).
2.2 Linear Context-Free Rewriting Systems
For the purposes of this paper, a linear context-free
rewriting system, henceforth LCFRS, is a construct
G = (N,T, P, S), where N is an alphabet of nonter-
minal symbols in which each symbol A is associated
with a positive integer ?(A) called its fan-out, T is
an alphabet of terminal symbols, S ? N is a distin-
guished start symbol with ?(S) = 1; and P is a finite
set of productions of the form
p = A? f(A1, . . . , Am) ,
where m ? 0, A,A1, . . . , Am ? N , and f is a linear,
non-erasing function over the terminal alphabet T
with type ?(A1)? ? ? ? ??(Am)? ?(A), called the
composition operation associated with p. The rank
of G and the fan-out of G are defined as the maximal
rank and fan-out of the composition operations of G,
and are denoted by ?(G) and ?(G).
The sets of derivation trees of G are the smallest
indexed family of sets DA, A ? N , such that, if
p = A? f(A1, . . . , Am)
277
N = {S,R} , T = {a, b, c, d} , P = { p1 = S ? f1(R), p2 = R? f2(R), p3 = R? f3 } ,
where: f1(?x1,1, x1,2?) = ?x1,1 x1,2? , f2(?x1,1, x1,2?) = ?a x1,1 b, c x1,2 d? , f3 = ??, ?? .
Figure 1: An LCFRS that generates the string language { anbncndn | n ? 0 }.
is a production of G and ti ? DAi for all i ? [m],
then t = p(t1, . . . , tm) ? DA. By interpreting pro-
ductions as their associated composition operations
in the obvious way, a derivation tree t ? DA evalu-
ates to a ?(A)-tuple of strings over T ; we denote this
tuple by val(t). The string language generated by G,
denoted by L(G), is then defined as
L(G) = {w ? T ? | t ? DS , ?w? = val(t) } .
Two LCFRS are called weakly equivalent, if they
generate the same string language.
Example Figure 1 shows a sample LCFRS G with
?(G) = 1 and ?(G) = 2. The sets of its deriva-
tion trees are DR = { pn2 (p3) | n ? 0 } and
DS = { p1(t) | t ? DR }. The string language
generated by G is { anbncndn | n ? 0 }.
2.3 Characteristic strings
In the remainder of this paper, we use the following
convenient syntax for tuples of strings. Instead of
?v1, . . . , vk? , we write v1 $ ? ? ? $ vk ,
using the $-symbol to mark the component bound-
aries. We call this the characteristic string of the tu-
ple, and an occurrence of the symbol $ a gap marker.
We also use this notation for composition operations.
For example, the characteristic string of the operation
f(?x1,1, x1,2?, ?x2,1?) = ?a x1,1 x2,1, x1,2 b?
is a x1,1 x2,1 $ x1,2 b. If we assume the variables on
the left-hand side of an equation to be named ac-
cording to the schema used in Section 2.1, then the
characteristic string of a composition operation deter-
mines that operation completely. We will therefore
freely identify the two, and write productions as
p = A? [v1 $ ? ? ? $ vk](A1, . . . , Am) ,
where the string inside the brackets is the charac-
teristic string of some composition operation. The
substrings v1, . . . , vk are called the components of
the characteristic string. Note that the character-
istic string of a composition operation with type
k1 ? ? ? ? ? km ? k is a sequence of terminal
symbols, gap markers, and variables from the set
{xi,j | i ? [m], j ? [ki] } in which the number of
gap markers is k?1, and each variable occurs exactly
once. When in the context of such a composition op-
eration we refer to ?a variable of the form xi,j?, then
it will always be the case that i ? [m] and j ? [ki].
The identification of composition operations and
their characteristic strings allows us to construct new
operations by string manipulations: if, for example,
we delete some variables from a characteristic string,
then the resulting string still defines a composition
operation (after a suitable renaming of the remaining
variables, which we leave implicit).
2.4 Canonical LCFRS
To simplify our presentation, we will assume that
LCFRS are given in a certain canonical form. Intu-
itively, this canonical form requires the variables in
the characteristic string of a composition operation
to be ordered in a certain way.
Formally, the defining equation of a composition
operation f with type k1 ? ? ? ? ? km ? k is called
canonical, if (i) the sequence obtained from f by
reading variables of the form xi,1 from left to right
has the form x1,1 ? ? ?xm,1; and (ii) for each i ? [m],
the sequence obtained from f by reading variables
of the form xi,j from left to right has the form
xi,1 ? ? ?xi,ki . An LCFRS is called canonical, if each
of its composition operations is canonical.
We omit the proof that every LCFRS can be trans-
formed into a weakly equivalent canonical LCFRS.
However, we point out that both the normal form and
the parsing algorithm that we present in this paper
can be applied also to general LCFRS. This is in con-
trast to some left-to-right parsers in the literature on
LCFRS and equivalent formalisms (de la Clergerie,
2002; Kallmeyer and Maier, 2009), which actually
depend on productions in canonical form.
2.5 Well-nested LCFRS
We now characterize the class of well-nested LCFRS
that are the focus of this paper. Well-nestedness
was first studied in the context of dependency gram-
mars (Kuhlmann and M?hl, 2007). Kanazawa (2009)
278
defines well-nested multiple context-free grammars,
which are weakly equivalent to well-nested LCFRS.
A composition operation is called well-nested, if it
does not contain a substring of the form
xi,i1 ? ? ?xj,j1 ? ? ?xi,i2 ? ? ?xj,j2 , where i 6= j .
For example, the operation x1,1 x2,1$x2,2 x1,2 is well-
nested, while x1,1 x2,1 $ x1,2 x2,2 is not. An LCFRS
is called well-nested, if it contains only well-nested
composition operations.
The class of languages generated by well-nested
LCFRS is properly included in the class of languages
generated by general LCFRS; see Kanazawa and Sal-
vati (2010) for further discussion.
3 Parsing LCFRS
We now discuss the parsing complexity of LCFRS,
and motivate our interest in a normal form for well-
nested LCFRS.
3.1 General parsing schema
A bottom-up, chart-based parsing algorithm for the
class of (not necessarily well-nested) LCFRS can be
defined by using the formalism of parsing schemata
(Sikkel, 1997). The parsing schemata approach con-
siders parsing as a deduction process (as in Shieber
et al (1995)), generating intermediate results called
items. Starting with an initial set of items obtained
from each input sentence, a parsing schema defines
a set of deduction steps that can be used to infer
new items from existing ones. Each item contains
information about the sentence?s structure, and a suc-
cessful parsing process will produce at least one final
item containing a full parse for the input.
The item set used by our bottom-up algorithm to
parse an input string w = a1 ? ? ? an with an LCFRS
G = (N,T, P, S) will be
I = {[A, (l1, r1), . . . , (lk, rk)] | A ? N ?
0 ? li ? ri ? n ?i ? [k]},
where an item [A, (l1, r1), . . . , (lk, rk)] can be inter-
preted as the set of those derivation trees t ? DA
of G for which
val(t) = al1+1 ? ? ? ar1 $ ? ? ? $ alk+1 ? ? ? ark .
The set of final items is thus F = {[S, (0, n)]}, con-
taining full derivation trees that evaluate to w.
For simplicity of definition of the sets of initial
items and deduction steps, let us assume that pro-
ductions of rank > 0 in our grammar do not contain
terminal symbols in their right-hand sides. This can
be easily achieved from a starting grammar by cre-
ating a nonterminal Aa for each terminal a ? T , a
corresponding rank-0 production pa = Aa ? [a](),
and then changing each occurrence of a in the char-
acteristic string of a production to the single variable
associated with the fan-out 1 nonterminal Aa. With
this, our initial item set for a string a1 ? ? ? an will be
H = {[Aai , (i? 1, i)] | i ? [n]} ,
and each production p = A0 ? f(A1, . . . , Am) of
G (excluding the ones we created for the terminals)
will produce a deduction step of the form given in
Figure 2a, where the indexes are subject to the fol-
lowing constraints, imposed by the semantics of f .
1. If the kth component of the characteristic string
of f starts with xi,j , then l0,k = li,j .
2. If the kth component of the characteristic string
of f ends with xi,j , then r0,k = ri,j .
3. If xi,jxi?,j? is an infix of the characteristic string
of f , then ri,j = li?,j? .
4. If the kth component of the characteristic string
of f is the empty string, then l0,k = r0,k.
3.2 General complexity
The time complexity of parsing LCFRS with respect
to the length of the input can be analyzed by counting
the maximum number of indexes that can appear in
an instance of the inference rule above. Although the
total number of indexes is
?m
i=0 2 ? ?(Ai), some of
these indexes are equated by the constraints.
To count the number of independent indexes, con-
sider all the indexes of the form l0,i (corresponding to
the left endpoints of each component of the character-
istic string of f ) and those of the form rj,k for j > 0
(corresponding to the right endpoints of each vari-
able in the characteristic string). By the constraints
above, these indexes are mutually independent, and it
is easy to check that any other index is equated to one
of these: indexes r0,i are equated to the index rj,k
corresponding to the last variable xj,k of the ith com-
ponent of the characteristic string, or to l0,i if there
is no such variable; while indexes lj,k with j > 0
are equated to an index l0,i if the variable xj,k is at
the beginning of a component of the characteristic
string, or to an index rj?,k?(j? > 1) if the variable xj,k
follows another variable xj?,k? .
279
[A1, (l1,1, r1,1), . . . , (l1,?(A1), r1,?(A1))] ? ? ? [Am, (lm,1, rm,1), . . . , (lm,?(Am), rm,?(Am))]
[A0, (l0,1, r0,1), . . . , (l0,?(A0), r0,?(A0))]
(a) The general rule for a parsing schema for LCFRS
[B, (l1, r1), . . . , (lm, rm)] [C, (l
?
1, r
?
1), . . . (l
?
n, r
?
n)]
[A, (l1, r1), . . . , (lm, r
?
1), . . . (l
?
n, r
?
n)]
rm = l?1
(b) Deduction step for concatenation
[B, (l1, r1), . . . , (lm, rm)] [C, (l
?
1, r
?
1), . . . (l
?
n, r
?
n)]
[A, (l1, r1), . . . , (li, r
?
1), . . . (l
?
n, ri+1), . . . , (lm, rm)]
ri = l?1, r
?
n = li+1
(c) Deduction step for wrapping
Figure 2: Deduction steps for parsing LCFRS.
Thus, the parsing complexity (Gildea, 2010) of a
production p = A0 ? f(A1, . . . , Am) is determined
by ?(A0) l-indexes and
?
i?[m] ?(Ai) r-indexes, for
a total complexity of
O(|w|?(A0)+
?
i?[m] ?(Ai))
where |w| is the length of the input string. The pars-
ing complexity of an LCFRS will correspond to the
maximum parsing complexity among its productions.
Note that this general complexity matches the result
given by Seki et al (1991).
In an LCFRS of rank ? and fan-out ?, the maxi-
mum possible parsing complexity is O(|w|?(?+1)),
obtained by applying the above expression to a pro-
duction of rank ? and where each nonterminal has fan-
out ?. The asymptotic time complexity of LCFRS
parsing is therefore exponential both in its rank and
its fan-out. This means that it is interesting to trans-
form LCFRS into equivalent forms that reduce their
rank while preserving the fan-out. For sets of LCFRS
that can be transformed into a binary form (i.e., such
that all its rules have rank at most 2), the ? factor in
the complexity is reduced to a constant, and complex-
ity is improved to O(|w|3?) (see G?mez-Rodr?guez
et al (2009) for further discussion). Unfortunately,
it is known by previous results (Rambow and Satta,
1999) that it is not always possible to convert an
LCFRS into such a binary form without increasing
the fan-out. However, we will show that it is always
possible to build such a binarization for well-nested
LCFRS. Combining this result with the inference
rule and complexity analysis given above, we would
obtain a parser for well-nested LCFRS running in
O(|w|3?) time. But the construction of our binary
normal form additionally restricts binary composition
operations in the binarized LCFRS to be of two spe-
cific forms, concatenation and wrapping, which fur-
ther improves the parsing complexity to O(|w|2?+2),
as we will see below.
3.3 Concatenation and wrapping
A composition operation is called a concatenation
operation, if its characteristic string has the form
x1,1 $ ? ? ? $ x1,m x2,1 $ ? ? ? $ x2,n ,
where m,n ? 1. Intuitively, such an operation corre-
sponds to the bottom-up combination of two adjacent
discontinuous constituents into one. An example of
a concatenation operation is the binary parsing rule
used by the standard CKY parser for context-free
grammars, which combines continuous constituents
(represented as 1-tuples of strings in the LCFRS nota-
tion). In the general case, a concatenation operation
will take an m-tuple and an n-tuple and return an
(m + n ? 1)-tuple, as the joined constituents may
have gaps that will also appear in the resulting tuple.
If we apply the general parsing rule given in Fig-
ure 2a to a production A? conc(B,C), where conc
is a concatenation operation, then we obtain the de-
duction step given in Figure 2b. This step uses 2m
different l- and r-indexes, and 2n? 1 different l?-
and r?-indexes (excluding l?1 which must equal rm),
for a total of 2m+2n?1 = 2(m+n?1)+1 indexes.
Since m+ n? 1 is the fan-out of the nonterminal A,
we conclude that the maximum number of indexes in
the step associated with a concatenation operation in
an LCFRS of fan-out ? is 2?+ 1.
280
before: p
? ? ?
t1 tm
after: p?
q
? ? ?
tq,1 tq,mq
r
? ? ?
tr,1 tr,mr
Figure 3: Transformation of derivation trees
A linear, non-erasing function is called a wrapping
operation, if its characteristic string has the form
x1,1 $ ? ? ? $ x1,i x2,1 $ ? ? ? $ x2,n x1,i+1 $ ? ? ? $ x1,m ,
where m,n ? 1 and i ? [m? 1]. Intuitively, such an
operation wraps the tuple derived from a nontermi-
nal B around the tuple derived from a nonterminal C,
filling the ith gap in the former. An example of a
wrapping operation is the adjunction of an auxiliary
tree in tree-adjoining grammar. In the general case, a
wrapping operation will take an m-tuple and an n-tu-
ple and return an (m + n ? 2)-tuple of strings: the
gaps of the argument tuples appear in the obtained
tuple, except for one gap in the tuple derived from B
which is filled by the tuple derived from C.
By applying the general parsing rule in Figure 2a
to a production A ? wrapi(B,C), where wrapi is
a wrapping operation, then we obtain the deduction
step given in Figure 2c. This step uses 2m different l-
and r-indexes, and 2n? 2 different l?- and r?-indexes
(discounting l?1 and r
?
n which are equal to other in-
dexes), for a total of 2m+2n?2 = 2(m+n?2)+2
indexes. Since the fan-out of A is m + n ? 2, this
means that a wrapping operation needs at most 2?+2
indexes for an LCFRS of fan-out ?.
From this, we conclude that an LCFRS of fan-
out ? in which all composition operations are ei-
ther concatenation operations, wrapping operations,
or operations of rank 0 or 1, can be parsed in time
O(|w|2?+2). In particular, nullary and unary compo-
sition operations do not affect this worst-case com-
plexity, since their associated deduction steps can
never have more than 2? indexes.
4 Transformation
We now show how to transform a well-nested LCFRS
into the normal form that we have just described.
4.1 Informal overview
Consider a production p = A ? f(A1, . . . , Am),
where m ? 2 and f is neither a concatenation nor a
wrapping operation. We will construct new produc-
tions p?, q, r such that every derivation that uses p can
be rewritten into a derivation that uses the new pro-
ductions, and the new productions do not license any
other derivations. Formally, this can be understood as
implementing a tree transformation, where the input
trees are derivations of the original grammar, and the
output trees are derivations of the new grammar. The
situation is illustrated in Figure 3. The tree on top
represents a derivation in the original grammar; this
derivation starts with the rewriting of the nontermi-
nal A using the production p, and continues with the
subderivations t1, . . . , tm. The tree at the bottom rep-
resents a derivation in the transformed grammar. This
derivation starts with the rewriting ofA using the new
production p?, and continues with two independent
subderivations that start with the new productions q
and r, respectively. The sub-derivations t1, . . . , tm
have been partitioned into two sequences
t1,1, . . . , t1,m1 and t2,1, . . . , t2,m2 .
The new production p? will be either a concatenation
or a wrapping operation, and the rank of both q and r
will be strictly smaller than the rank of p. The trans-
formation will continue with q and r, unless these
have rank one. By applying this strategy exhaustively,
we will thus eventually end up with a grammar that
only has productions with rank at most 2, and in
which all productions with rank 2 are either concate-
nation or wrapping operations.
4.2 Constructing the composition operations
To transform the production p, we first factorize the
composition operation f associated with p into three
new composition operations f ?, g, h as follows. Re-
call that we represent composition operations by their
characteristic strings.
In the following, we will assume that no charac-
teristic string starts or ends with a gap marker, or
contains immediate repetitions of gap markers. This
281
property can be ensured, without affecting the asymp-
totic complexity, by adding intermediate steps to the
transformation that we report here; we omit the de-
tails due to space reasons. When this property holds,
we are left with the following two cases. Let us call a
sequence of variables joint, if it contains all and only
variables associated with a given nonterminal.
Case 1 f = x1 f1 x2 ? ? ?xk?1 fk?1 xk f? ,
where k ? 1, x1, . . . , xk are joint variables, and the
suffix f? contains at least one variable. Let
g = x1 f1 x2 ? ? ?xk?1 fk?1 xk ,
let h = f?, and let f ? = conc. As f is well-nested,
both g and h define well-nested composition opera-
tions. By the specific segmentation of f , the ranks of
these operations are strictly smaller than the rank of f .
Furthermore, we have ?(f) = ?(g) + ?(h)? 1 .
Case 2 f = x1 f1 x2 ? ? ?xk?1 fk?1 xk ,
where k ? 2, x1, . . . , xk are joint variables, and there
exist at least one i such that the sequence fi contains
at least one variable. Choose an index j as follows:
if there is at least one i such that fi contains at least
one variable and one gap marker, let j be the minimal
such i; otherwise, let j be the minimal i such that fi
contains at least one variable. Now, let
g = x1 f1 x2 ? ? ?xj $ xj+1 ? ? ?xk?1 fk?1 xk ,
let h = fj , and let f ? = wrapj . As in Case 1, both g
and h define well-nested composition operations
whose ranks are strictly smaller than the rank of f .
Furthermore, we have ?(f) = ?(g) + ?(h)? 2 .
Note that at most one of the two cases can apply
to f . Furthermore, since f is well-nested, it is also
true that at least one of the two cases applies. This
is so because for two distinct nonterminals Ai, Ai? ,
either all variables associated with Ai? precede the
leftmost variable associated with Ai, succeed the
rightmost variable associated with Ai, or are placed
between two variables associated with Ai without an-
other variable associated with Ai intervening. (Here,
we have left out the symmetric cases.)
4.3 Constructing the new productions
Based on the composition operations, we now con-
struct three new productions p?, q, r as follows. LetB
and C be two fresh nonterminals with ?(B) = ?(g)
and ?(C) = ?(h), and let p? = A ? f ?(B,C).
The production p? rewrites A into B and C and
combines the two subderivations that originate at
these nonterminals using either a concatenation or a
wrapping operation. Now, let Aq,1, . . . , Aq,mq and
Ar,1, . . . , Ar,mr be the sequences of nonterminals
that are obtained from the sequence A1, . . . , Am by
deleting those nonterminals that are not associated
with any variable in g or h, respectively. Then, let
q = B ? g(Aq,1, . . . , Aq,mq) and
r = C ? h(?Ar,1, . . . , Ar,mr) .
4.4 Example
We now illustrate the transformation using the con-
crete production p = A? f(A1, A2, A3), where
f = x1,1 x2,1 $ x1,2 $ x3,1 .
Note that this operation has rank 3 and fan-out 3.
The composition operations are constructed as fol-
lows. The operation f matches the pattern of Case 1,
and hence induces the operations
g1 = x1,1 x2,1 $ x1,2 , h1 = $ x3,1 , f ?1 = conc .
The productions constructed from these are
p?1 = A? conc(B1, C1) ,
q1 = B1 ? g1(A1, A2) , r1 = C1 ? h1(A3) .
where B1 and C1 are fresh nonterminals with fan-
out 2. The production r1 has rank one, so it does not
require any further transformations. The transforma-
tion thus continues with q1. The operation g1 matches
the pattern of Case 2, and induces the operations
g2 = x1,1 $ x1,2 , h2 = x2,1$ , f ?2 = wrap1 .
The productions constructed from these are
p?2 = B1 ? wrap1(B2, C2) ,
q2 = B2 ? g2(A1) , r2 = C2 ? h2(A2) ,
where B2 and C2 are fresh nonterminals with fan-
out 2. At this point, the transformation terminates.
We can now delete p from the original grammar, and
replace it with the productions {p?1, r1, p
?
2, q2, r2}.
4.5 Correctness
To see that the transformation is correct, we need to
verify that each production of the original grammar
is transformed into a set of equivalent normal-form
productions, and that the fan-out of the new grammar
does not exceed the fan-out of the old grammar.
For the first point, we note that the transformation
preserves well-nestedness, decreases the rank of a
production, and is always applicable as long as the
282
rank of a production is at most 2 and the production
does not use a concatenation or wrapping operation.
That the new productions are equivalent to the old
ones in the sense of Figure 3 can be proved by induc-
tion on the length of a derivation in the original and
the new grammar, respectively.
Let us now convince ourselves that the fan-out of
the new grammar does not exceed the fan-out of the
old grammar. This is clear in Case 1, where
?(f) = ?(g) + ?(h)? 1
implies that both ?(g) ? ?(f) and ?(h) ? ?(f).
For Case 2, we reason as follows. The fan-out of the
operation h, being constructed from an infix of the
characteristic string of the original operation f , is
clearly bounded by the fan-out of f . For g, we have
?(g) = ?(f)? ?(h) + 2 ,
Now suppose that the index j was chosen according
to the first alternative. In this case, ?(h) ? 2, and
?(g) ? ?(f)? 2 + 2 = ?(f) .
For the case where j was chosen according to the
second alternative, ?(f) < k (since there are no
immediate repetitions of gap markers), ?(h) = 1,
and ?(g) ? k. If we assume that each nonterminal
is productive, then this means that the underlying
LCFRS has at least one production with fan-out k or
more; therefore, the fan-out of g does not increase
the fan-out of the original grammar.
4.6 Complexity
To conclude, we now briefly discuss the space com-
plexity of the normal-form transformation. We mea-
sure it in terms of the length of a production, defined
as the length of its string representation, that is, the
string A? [v1 $ ? ? ? $ vk](A1, . . . , Am) .
Looking at Figure 3, we note that the normal-form
transformation of a production p can be understood
as the construction of a (not necessarily complete)
binary-branching tree whose leaves correspond to the
productions obtained by splitting the characteristic
string of p and whose non-leaf nodes are labeled with
concatenation and wrapping operations. By construc-
tion, the sum of the lengths of leaf-node productions
is O(|p|). Since the number of inner nodes of a bi-
nary tree with n leaves is bounded by n ? 1, we
know that the tree hasO(?(p)) inner nodes. As these
nodes correspond to concatenation and wrapping
operations, each inner-node production has length
O(?(p)). Thus, the sum of the lengths of the produc-
tions created from |p| is O(|p|+ ?(p)?(p)). Since
the rank of a production is always smaller than its
length, this is reduced to O(|p|?(p)).
Therefore, the size of the normal-form transfor-
mation of an LCFRS G of fan-out ? is O(?|G|) in
the worst case, and linear space in practice, since
the fan-out is typically bounded by a small integer.
Taking the normal-form transformation into account,
our parser therefore runs in timeO(? ? |G| ? |w|2?+2)
where |G| is the original grammar size.
5 Conclusion
In this paper, we have presented an efficient parsing
algorithm for well-nested linear context-free rewrit-
ing systems, based on a new normal form for this
formalism. The normal form takes up linear space
with respect to grammar size, and the algorithm is
based on a bottom-up process that can be applied
to any LCFRS, achieving O(? ? |G| ? |w|2?+2) time
complexity when applied to LCFRS of fan-out ?
in our normal form. This complexity is an asymp-
totic improvement over existing results for this class,
both from parsers specifically geared to well-nested
LCFRS or equivalent formalisms (Hotz and Pitsch,
1996) and from applying general LCFRS parsing
techniques to the well-nested case (Seki et al, 1991).
The class of well-nested LCFRS is an interest-
ing syntactic formalism for languages with discon-
tinuous constituents, providing a good balance be-
tween coverage of linguistic phenomena in natu-
ral language treebanks (Kuhlmann and Nivre, 2006;
Maier and Lichte, 2009) and desirable formal prop-
erties (Kanazawa, 2009). Our results offer a further
argument in support of well-nested LCFRS: while
the complexity of parsing general LCFRS depends
on two dimensions (rank and fan-out), this bidimen-
sional hierarchy collapses into a single dimension
in the well-nested case, where complexity is only
conditioned by the fan-out.
Acknowledgments G?mez-Rodr?guez has been
supported by MEC/FEDER (HUM2007-66607-C04)
and Xunta de Galicia (PGIDIT07SIN005206PR, Re-
des Galegas de PL e RI e de Ling. de Corpus, Bolsas
Estad?as INCITE/FSE cofinanced). Kuhlmann has
been supported by the Swedish Research Council.
283
References
?ric Villemonte de la Clergerie. 2002. Parsing mildly
context-sensitive languages with thread automata. In
19th International Conference on Computational Lin-
guistics (COLING), pages 1?7, Taipei, Taiwan.
Daniel Gildea. 2010. Optimal parsing strategies for linear
context-free rewriting systems. In Human Language
Technologies: The Eleventh Annual Conference of the
North American Chapter of the Association for Compu-
tational Linguistics, Los Angeles, USA.
Carlos G?mez-Rodr?guez, Marco Kuhlmann, Giorgio
Satta, and David J. Weir. 2009. Optimal reduction
of rule length in linear context-free rewriting systems.
In Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 539?547,
Boulder, CO, USA.
Jir?? Havelka. 2007. Beyond projectivity: Multilin-
gual evaluation of constraints and measures on non-
projective structures. In 45th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
608?615.
G?nter Hotz and Gisela Pitsch. 1996. On parsing coupled-
context-free languages. Theoretical Computer Science,
161(1?2):205?233.
Riny Huybregts. 1984. The weak inadequacy of context-
free phrase structure grammars. In Ger de Haan, Mieke
Trommelen, and Wim Zonneveld, editors, Van periferie
naar kern, pages 81?99. Foris, Dordrecht, The Nether-
lands.
Aravind K. Joshi, Leon S. Levy, and Masako Takahashi.
1975. Tree Adjunct Grammars. Journal of Computer
and System Sciences, 10(2):136?163.
Aravind K. Joshi. 1985. Tree Adjoining Grammars: How
much context-sensitivity is required to provide reason-
able structural descriptions? In Natural Language
Parsing, pages 206?250. Cambridge University Press.
Laura Kallmeyer and Wolfgang Maier. 2009. An incre-
mental Earley parser for simple range concatenation
grammar. In Proceedings of the 11th International Con-
ference on Parsing Technologies (IWPT 2009), pages
61?64. Association for Computational Linguistics.
Makoto Kanazawa and Sylvain Salvati. 2010. The copy-
ing power of well-nested multiple context-free gram-
mars. In Fourth International Conference on Language
and Automata Theory and Applications, Trier, Ger-
many.
Makoto Kanazawa. 2009. The pumping lemma for well-
nested multiple context-free languages. In Develop-
ments in Language Theory. 13th International Confer-
ence, DLT 2009, Stuttgart, Germany, June 30?July 3,
2009. Proceedings, volume 5583 of Lecture Notes in
Computer Science, pages 312?325.
Marco Kuhlmann and Mathias M?hl. 2007. Mildly
context-sensitive dependency languages. In 45th An-
nual Meeting of the Association for Computational Lin-
guistics (ACL), pages 160?167.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly non-
projective dependency structures. In 21st International
Conference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational Lin-
guistics (COLING-ACL), Main Conference Poster Ses-
sions, pages 507?514, Sydney, Australia.
Marco Kuhlmann and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Twelfth Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), pages 478?486, Athens, Greece.
Wolfgang Maier and Timm Lichte. 2009. Characterizing
discontinuity in constituent treebanks. In 14th Confer-
ence on Formal Grammar, Bordeaux, France.
Wolfgang Maier and Anders S?gaard. 2008. Treebanks
and mild context-sensitivity. In 13th Conference on
Formal Grammar, pages 61?76, Hamburg, Germany.
Owen Rambow and Giorgio Satta. 1999. Independent
parallelism in finite copying parallel rewriting systems.
Theoretical Computer Science, 223(1?2):87?120.
Giorgio Satta. 1992. Recognition of Linear Context-
Free Rewriting Systems. In 30th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 89?95, Newark, DE, USA.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On Multiple Context-Free Gram-
mars. Theoretical Computer Science, 88(2):191?229.
Stuart M. Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive pars-
ing. Journal of Logic Programming, 24(1?2):3?36.
Stuart M. Shieber. 1985. Evidence against the context-
freeness of natural language. Linguistics and Philoso-
phy, 8(3):333?343.
Klaas Sikkel. 1997. Parsing Schemata: A Framework
for Specification and Analysis of Parsing Algorithms.
Springer.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions produced
by various grammatical formalisms. In 25th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 104?111, Stanford, CA, USA.
284
