Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 769?776,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Optimal Parsing Strategies for Linear Context-Free Rewriting Systems
Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
Abstract
Factorization is the operation of transforming
a production in a Linear Context-Free Rewrit-
ing System (LCFRS) into two simpler produc-
tions by factoring out a subset of the nontermi-
nals on the production?s righthand side. Fac-
torization lowers the rank of a production but
may increase its fan-out. We show how to
apply factorization in order to minimize the
parsing complexity of the resulting grammar,
and study the relationship between rank, fan-
out, and parsing complexity. We show that it
is always possible to obtain optimum parsing
complexity with rank two. However, among
transformed grammars of rank two, minimum
parsing complexity is not always possible with
minimum fan-out. Applying our factorization
algorithm to LCFRS rules extracted from de-
pendency treebanks allows us to find the most
efficient parsing strategy for the syntactic phe-
nomena found in non-projective trees.
1 Introduction
G?mez-Rodr?guez et al (2009a) recently examined
the problem of transforming arbitrary grammars in
the Linear Context-Free Rewriting System (LCFRS)
formalism (Vijay-Shankar et al, 1987) in order to
reduce the rank of a grammar to 2 while minimiz-
ing its fan-out. The work was motivated by the
desire to develop efficient chart-parsing algorithms
for non-projective dependency trees (Kuhlmann and
Nivre, 2006) that do not rely on the independence
assumptions of spanning tree algorithms (McDon-
ald et al, 2005). Efficient parsing algorithms for
general LCFRS are also relevant in the context of
Synchronous Context-Free Grammars (SCFGs) as a
formalism for machine translation, as well as the de-
sire to handle even more general synchronous gram-
mar formalisms which allow nonterminals to cover
discontinuous spans in either language (Melamed et
al., 2004; Wellington et al, 2006). LCFRS provides
a very general formalism which subsumes SCFGs,
the Multitext Grammars of Melamed et al (2004),
as well as mildly context-sensitive monolingual for-
malisms such as Tree Adjoining Grammar (Joshi
and Schabes, 1997). Thus, work on transforming
general LCFRS grammars promises to be widely ap-
plicable in both understanding how these formalisms
interrelate, and, from a more practical viewpoint, de-
riving efficient parsing algorithms for them.
In this paper, we focus on the problem of trans-
forming an LCFRS grammar into an equivalent
grammar for which straightforward application of
dynamic programming to each rule yields a tabular
parsing algorithm with minimum complexity. This
is closely related, but not equivalent, to the prob-
lem considered by G?mez-Rodr?guez et al (2009a),
who minimize the fan-out, rather than the parsing
complexity, of the resulting grammar. In Section 4,
we show that restricting our attention to factorized
grammars with rank no greater than 2 comes at no
cost in parsing complexity. This result may be sur-
prising, as G?mez-Rodr?guez et al (2009a) com-
ment that ?there may be cases in which one has to
find an optimal trade-off between rank and fan-out?
in order to minimize parsing complexity ? in fact,
no such trade-off is necessary, as rank 2 is always
sufficient for optimal parsing complexity. Given
this fact, we show how to adapt the factorization al-
gorithm of G?mez-Rodr?guez et al (2009a) to re-
turn a transformed grammar with minimal parsing
complexity and rank 2. In Section 5, we give a
769
counterexample to the conjecture that minimal pars-
ing complexity is possible among binarizations with
minimal fan-out.
2 Background
A linear context-free rewriting system (LCFRS) is
defined as a tuple G = (VN , VT , P, S), where VT is
a set of terminal symbols, VN is a set of nonterminal
symbols, P is a set of productions, and S ? VN is
a distinguished start symbol. Associated with each
nonterminal B is a fan-out ?(B), which tell how
many discontinuous spans B covers. Productions
p ? P take the form:
p : A? g(B1, B2, . . . , Br) (1)
where A,B1, . . . Br ? VN , and g is a function
g : (V ?T )?(B1) ? . . .? (V ?T )?(Br) ? (V ?T )?(A)
which specifies how to assemble the
?r
i=1 ?(Bi)
spans of the righthand side nonterminals into the
?(A) spans of the lefthand side nonterminal. The
function g must be linear, non-erasing, which
means that if we write
g(?x1,1, . . . , x1,?(B1)?, . . . , ?x1,1, . . . , x1,?(Br)?)
= ?t1, . . . , t?(A)?
the tuple of strings ?t1, . . . , t?(A)? on the righthand
side contains each variable xi,j from the lefthand
side exactly once, and may also contain terminals
from VT .
We call r, the number of nonterminals on the
righthand side of a production p, the rank of p, ?(p).
The fan-out of a production, ?(p) is the fan-out of its
lefthand side, ?(A). The rank of a grammar is the
maximum rank of its rules,
?(G) = max
p?P
?(p)
and similarly the fan-out of a grammar is the maxi-
mum fan-out of its rules, or equivalently, of its non-
terminals:
?(G) = max
B?VN
?(B)
3 Parsing LCFRS
A bottom-up dynamic programming parser can be
produced from an LCFRS grammar by generaliz-
ing the CYK algorithm for context-free grammars.
We convert each production of the LCFRS into a
deduction rule with variables for the left and right
endpoints of each of the ?(Bi) spans of each of the
nonterminals Bi, i ? [r] in the righthand side of the
production.
The computational complexity of the resulting
parser is polynomial in the length of the input string,
with the degree of the polynomial being the number
of distinct endpoints in the most complex produc-
tion. Thus, for input of length n, the complexity
is O(nc) for some constant c which depends on the
grammar.
For a given rule, each of the r nonterminals has
?(Bi) spans, and each span has a left and right end-
point, giving an upper bound of c ? 2?ri=1 ?(Bi).
However, some of these endpoints may be shared
between nonterminals on the righthand side. The
exact number of distinct variables for the dynamic
programming deduction rule can the written
c(p) = ?(A) +
r
?
i=1
?(Bi) (2)
where c(p) is the parsing complexity of a produc-
tion p of the form of eq. 1 (Seki et al, 1991). To
see this, consider counting the left endpoint of each
span on the lefthand side of the production, and the
right endpoint of each span on the righthand side of
the production. Any variable corresponding to the
left endpoint of a span of a righthand side nonter-
minal will either be shared with the right endpoint
of another span if two spans are being joined by the
production, or, alternatively, will form the left end-
point of a span of A. Thus, each distinct endpoint in
the production is counted exactly once by eq. 2.
The parsing complexity of a grammar, c(G), is
the maximum parsing complexity of its rules. From
eq. 2, we see that c(G) ? (?(G) + 1)?(G). While
we focus on the time complexity of parsing, it is in-
teresting to note the space complexity of the DP al-
gorithm is O(n2?(G)), since the DP table for each
nonterminal is indexed by at most 2?(G) positions
in the input string.
770
4 Binarization Minimizes Parsing
Complexity
An LCFRS production of rank r can be factorized
into two productions of the form:
p1 : A? g1(B1, . . . , Br?2, X)
p2 : X ? g2(Br?1, Br)
This operation results in new productions that have
lower rank, but possibly higher fan-out, than the
original production.
If we examine the DP deduction rules correspond-
ing to the original production p, and the first new
production p1 we find that
c(p1) ? c(p)
regardless of the function g of the original produc-
tion, or the fan-out of the production?s nonterminals.
This is because
?(X) ? ?(Br?1) + ?(Br)
that is, our newly created nonterminal X may join
spans from Br?1 and Br, but can never introduce
new spans. Thus,
c(p1) = ?(A) +
(r?2
?
i=1
?(Bi)
)
+ ?(X)
? ?(A) +
r
?
i=1
?(Bi)
= c(p)
As similar result holds for the second newly cre-
ated production:
c(p2) ? c(p)
In this case, the fan-out of the newly created nonter-
minal, ?(X) may be greater than ?(A). Let us con-
sider the left endpoints of the spans of X . Each left
endpoint is either also the left endpoint of a span of
A, or is the right endpoint of some nonterminal not
included in X , that is, one of B1, . . . Br?2. Thus,
?(X) ? ?(A) +
r?2
?
i=1
?(Bi)
and applying this inequality to the definition of c(p2)
we have:
c(p2) = ?(X) + ?(Br?1) + ?(Br?2)
? ?(A) +
r
?
i=1
?(Bi)
= c(p)
For notational convenience, we have defined the
factorization operation as factoring out the last two
nonterminals of a rule; however, the same operation
can be applied to factor out any subset of the orig-
inal nonterminals. The same argument that parsing
complexity cannot increase still applies.
We may apply the factorization operation repeat-
edly until all rules have rank 2; we refer to the re-
sulting grammar as a binarization of the original
LCFRS. The factorization operation may increase
the fan-out of a grammar, but never increases its
parsing complexity. This guarantees that, if we wish
to find the transformation of the original grammar
having the lowest parsing complexity, it is sufficient
to consider only binarizations. This is because any
transformed grammar having more than two nonter-
minals on the righthand side can be binarized with-
out increasing its parsing complexity.
5 The relationship between fan-out and
parsing complexity
G?mez-Rodr?guez et al (2009a) provide an algo-
rithm for finding the binarization of an LCFRS hav-
ing minimal fan-out. The key idea is to search over
ways of combining subsets of a rule?s righthand side
nonterminals such that subsets with low fan-out are
considered first; this results in an algorithm with
complexity polynomial in the rank of the input rule,
with the exponent depending on the resulting mini-
mum fan-out.
This algorithm can be adapted to find the binariza-
tion with minimum parsing complexity, rather than
minimum fan-out. We simply use c(p) rather than
?(p) as the score for new productions, controlling
both which binarizations we prefer and the order in
which they are explored.
An interesting question then arises: does the bina-
rization with minimal parsing complexity also have
minimal fan-out? A binarization into a grammar of
771
A? g(B1, B2, B3, B4)
g(?x1,1, x1,2?, ?x2,1, x2,2, x2,3?, ?x3,1, x3,2, x3,3, x3,4, x3,5?, ?x4,1, x4,2, x4,3?) =
?x4,1x3,1, x2,1, x4,2x1,1x2,2x4,3x3,2x2,3x3,3, x1,2x3,4, x3,5?
Figure 2: A production for which minimizing fan-out and minimizing parsing complexity are mutually exclusive.
{B3}
{B4}
{B3, B4}
{B3, B4}
{B1}
{B1, B3, B4}
{B1, B3, B4}
{B2}
{B1, B2, B3, B4}
Figure 3: The binarization of the rule from Figure 2 that minimizes parsing complexity. In each of the three steps,
we show the spans of each of the two subsets of the rule?s righthand-side nonterms being combined, with the spans of
their union (corresponding to a nonterminal created by the binarization) below.
772
1: function MINIMAL-BINARIZATION(p,?)
2: workingSet? ?;
3: agenda? priorityQueue(?);
4: for i from 1 to ?(p) do
5: workingSet? workingSet ?{Bi};
6: agenda? agenda ?{Bi};
7: while agenda 6= ? do
8: p? ? pop minimum from agenda;
9: if nonterms(p?) = {B1, . . . B?(p)} then
10: return p?;
11: for p1 ? workingSet do
12: p2 ? newProd(p?, p1);
13: find p?2 ? workingSet :
14: nonterms(p?2) = nonterms(p2);
15: if p2 ? p?2 then
16: workingSet? workingSet ?{p2}\{p?2};
17: push(agenda, p2);
Figure 1: Algorithm to compute best binarization accord-
ing to a user-specified ordering ? over productions.
fan-out f ? cannot have parsing complexity higher
than 3f ?, according to eq. 2. Thus, minimizing fan-
out puts an upper bound on parsing complexity, but
is not guaranteed to minimize it absolutely. Bina-
rizations with the same fan-out may in fact vary
in their parsing complexity; similarly binarizations
with the same parsing complexity may vary in their
fan-out. It is not immediately apparent whether, in
order to find a binarization of minimal parsing com-
plexity, it is sufficient to consider only binarizations
of minimal fan-out.
To test this conjecture, we adapted the algorithm
of G?mez-Rodr?guez et al (2009a) to use a prior-
ity queue as the agenda, as shown in Figure 1. The
algorithm takes as an argument an arbitrary partial
ordering relation on productions, and explores pos-
sible binarized rules in the order specified by this re-
lation. In Figure 1, ?workingSet? is a set of single-
ton nonterminals and binarized productions which
are guaranteed to be optimal for the subset of non-
terminals that they cover. The function ?nonterms?
returns, for a newly created production, the subset
of the original nonterminals B1, . . . Br that it gen-
erates, and returns subsets of singleton nonterminals
directly.
To find the binarization with the minimum fan-out
f ? and the lowest parsing complexity among bina-
rizations with fan-out f ?, we use the following com-
parison operation in the binarization algorithm:
p1 ??c p2 iff ?(p1) < ?(p2) ?
(?(p1) = ?(p2) ? c(p1) < c(p2))
guaranteeing that we explore binarizations with
lower fan-out first, and, among binarizations with
equal fan-out, those with lower parsing complexity
first. Similarly, we can search for the binarization
with the lowest parsing complexity c? and the lowest
fan-out among binarizations with complexity c?, we
use
p1 ?c? p2 iff c(p1) < c(p2) ?
(c(p1) = c(p2) ? ?(p1) < ?(p2))
We find that, in fact, it is sometimes necessary to
sacrifice minimum fan-out in order to achieve mini-
mum parsing complexity. An example of an LCFRS
rule for which this is the case is shown in Figure 2.
This production can be binarized to produce a set of
productions with parsing complexity 14 (Figure 3);
among binarizations with this complexity the mini-
mum fan-out is 6. However, an alternative binariza-
tion with fan-out 5 is also possible; among binariza-
tions with this fan-out, the minimum parsing com-
plexity is 15. This binarization (not pictured) first
joins B1 and B2, then adds B4, and finally adds B3.
Given the incompatibility of optimizing time
complexity and fan-out, which corresponds to space
complexity, which should we prefer? In some sit-
uations, it may be desirable to find some trade-off
between the two. It is important to note, however,
that if optimization of space complexity is the sole
objective, factorization is unnecessary, as one can
never improve on the fan-out required by the origi-
nal grammar nonterminals.
6 A Note on Generative Capacity
Rambow and Satta (1999) categorize the genera-
tive capacity of LCFRS grammars according to their
rank and fan-out. In particular, they show that
grammars can be arranged in a two-dimensional
grid, with languages of rank r and fan-out f having
greater generative capacity than both grammars of
rank r and fan-out f ?1 and grammars of rank r?1
773
nmod sbj root vc pp nmod np tmp
A hearing is scheduled on the issue today
nmod? g1 g1 = ?A ?
sbj? g2(nmod, pp) g2(?x1,1?, ?x2,1?) = ?x1,1 hearing , x2,1?
root? g3(sbj, vc) g3(?x1,1, x1,2?, ?x2,1, x2,2?) = ?x1,1 is x2,1x1,2x2,2?
vc? g4(tmp) g4(?x1,1?) = ? scheduled , x1,1?
pp? g5(tmp) g5(?x1,1?) = ? on x1,1?
nmod? g6 g6 = ? the ?
np? g7(nmod) g7(?x1,1?) = ?x1,1 issue ?
tmp? g8 g8 = ? today ?
Figure 4: A dependency tree with the LCFRS rules extracted for each word (Kuhlmann and Satta, 2009).
and fan-out f , with two exceptions: with fan-out 1,
all ranks greater than one are equivalent (context-
free languages), and with fan-out 2, rank 2 and rank
3 are equivalent.
This classification is somewhat unsatisfying be-
cause minor changes to a grammar can change both
its rank and fan-out. In particular, through factor-
izing rules, it is always possible to decrease rank,
potentially at the cost of increasing fan-out, until a
binarized grammar of rank 2 is achieved.
Parsing complexity, as defined above, also pro-
vides a method to compare the generative capacity
of LCFRS grammars. From Rambow and Satta?s
result that grammars of rank two and increasing
fan-out provide an infinite hierarchy of increasing
generative capacity, we see that parsing complexity
also provides such an infinite hierarchy. Compar-
ing grammars according to the parsing complexity
amounts to specifying a normalized binarization for
grammars of arbitrary rank and fan-out, and compar-
ing the resulting binarized grammars. This allows us
to arrange LCFRS grammars into total ordering over
generative capacity, that is a one-dimensional hier-
archy, rather than a two-dimensional grid. It also
gives a way of categorizing generative capacity that
is more closely tied to algorithmic complexity.
It is important to note, however, that parsing com-
plexity as calculated by our algorithm remains a
function of the grammar, rather than an intrinsic
function of the language. One can produce arbitrar-
ily complex grammars that generate the simple lan-
guage a?. Thus the parsing complexity of a gram-
mar, like its rank and fan-out, can be said to catego-
rize its strong generative capacity.
7 Experiments
A number of recent papers have examined dynamic
programming algorithms for parsing non-projective
dependency structures by exploring how well vari-
ous categories of polynomially-parsable grammars
cover the structures found in dependency treebanks
for various languages (Kuhlmann and Nivre, 2006;
G?mez-Rodr?guez et al, 2009b).
Kuhlmann and Satta (2009) give an algorithm for
extracting LCFRS rules from dependency structures.
One rule is extracted for each word in the depen-
dency tree. The rank of the rule is the number of
children that the word has in the dependency tree,
as shown by the example in Figure 4. The fan-out
of the symbol corresponding to a word is the num-
ber of continuous intervals in the sentence formed
by the word and its descendants in the tree. Projec-
774
complexity arabic czech danish dutch german port swedish
20 1
18 1
16 1
15 1
13 1
12 2 3
11 1 1 1
10 2 6 16 3
9 7 4 1
8 4 7 129 65 10
7 3 12 89 30 18
6 178 11 362 1811 492 59
5 48 1132 93 411 1848 172 201
4 250 18269 1026 6678 18124 2643 1736
3 10942 265202 18306 39362 154948 41075 41245
Table 1: Number of productions with specified parsing complexity
tive trees yield LCFRS rules of fan-out one and pars-
ing complexity three, while the fan-out and parsing
complexity from non-projective trees are in princi-
ple unbounded.
Extracting LCFRS rules from treebanks allows us
to study how many of the rules fall within certain
constraints. Kuhlmann and Satta (2009) give an al-
gorithm for binarizing LCRFS rules without increas-
ing the rules? fan-out; however, this is not always
possible, and the algorithm does not succeed even in
some cases for which such a binarization is possible.
Kuhlmann and Satta (2009) find that all but 0.02%
of productions in the CoNLL 2006 training data,
which includes various languages, can be binarized
by their algorithm, but they do not give the fan-out
or parsing complexity of the resulting rules. In re-
lated work, G?mez-Rodr?guez et al (2009b) define
the class of mildly ill-nested dependency structures
of varying gap degrees; gap degree is essentially fan-
out minus one. For a given gap degree k, this class of
grammars can be parsing in time O(n3k+4) for lexi-
calized grammars. G?mez-Rodr?guez et al (2009b)
study dependency treebanks for nine languages and
find that all dependency structures meet the mildly
ill-nested condition in the dependency treebanks for
some gap degree. However, they do not report the
maximum gap degree or parsing complexity.
We extracted LCFRS rules from dependency tree-
banks using the same procedure as Kuhlmann and
Satta (2009), and applied the algorithm of Figure 1
directly to calculate their minimum parsing com-
plexity. This allows us to characterize the pars-
ing complexity of the rules found in the treebank
without needing to define specific conditions on
the rules, such as well-nestedness (Kuhlmann and
Nivre, 2006) or mildly ill-nestedness, that may not
be necessary for all efficiently parsable grammars.
The numbers of rules of different complexities are
shown in Table 1.
As found by previous studies, the vast major-
ity of productions are context-free (projective trees,
parsable in O(n3)). Of non-projective rules, the
vast majority can be parsed in O(n6), including the
well-nested structures of gap degree one defined by
Kuhlmann and Nivre (2006). The single most com-
plex rule had parsing complexity of O(n20), and was
derived from a Swedish sentence which turns out to
be so garbled as to be incomprehensible (taken from
the high school essay portion of the Swedish tree-
bank). It is interesting to note that, while the bina-
rization algorithm is exponential in the worst case, it
is practical for real data: analyzing all the rules ex-
tracted from the various treebanks takes only a few
minutes. We did not find any cases in rules extracted
from Treebank data of rules where minimizing pars-
ing complexity is inconsistent with minimizing fan-
775
out, as is the case for the rule of Figure 2.
8 Conclusion
We give an algorithm for finding the optimum pars-
ing complexity for an LCFRS among grammars ob-
tained by binarization. We find that minimum pars-
ing complexity is always achievable with rank 2, but
is not always achievable with minimum fan-out. By
applying the binarization algorithm to productions
found in dependency treebanks, we can completely
characterize the parsing complexity of the extracted
LCFRS grammar.
Acknowledgments This work was funded by NSF
grants IIS-0546554 and IIS-0910611. We are grate-
ful to Joakim Nivre for assistance with the Swedish
treebank.
References
Carlos G?mez-Rodr?guez, Marco Kuhlmann, Giorgio
Satta, and David Weir. 2009a. Optimal reduction of
rule length in linear conext-free rewriting systems. In
Proceedings of the 2009 Meeting of the North Ameri-
can chapter of the Association for Computational Lin-
guistics (NAACL-09), pages 539?547.
Carlos G?mez-Rodr?guez, David Weir, and John Car-
roll. 2009b. Parsing mildly non-projective depen-
dency structures. In Proceedings of the 12th Confer-
ence of the European Chapter of the ACL (EACL-09),
pages 291?299.
A.K. Joshi and Y. Schabes. 1997. Tree-adjoining gram-
mars. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, volume 3, pages 69?
124. Springer, Berlin.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In Proceed-
ings of the International Conference on Computational
Linguistics/Association for Computational Linguistics
(COLING/ACL-06), pages 507?514.
Marco Kuhlmann and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL-09), pages 478?
486.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (HLT/EMNLP).
I. Dan Melamed, Giorgio Satta, and Ben Wellington.
2004. Generalized multitext grammars. In Proceed-
ings of the 42nd Annual Conference of the Association
for Computational Linguistics (ACL-04), Barcelona,
Spain.
Owen Rambow and Giorgio Satta. 1999. Independent
parallelism in finite copying parallel rewriting sys-
tems. Theor. Comput. Sci., 223(1-2):87?120.
H. Seki, T. Matsumura, M. Fujii, and T. Kasami. 1991.
On multiple context-free grammars. Theoretical Com-
puter Science, 88:191?229.
K. Vijay-Shankar, D. L. Weir, and A. K. Joshi. 1987.
Characterizing structural descriptions produced by
various grammatical formalisms. In Proceedings of
the 25th Annual Conference of the Association for
Computational Linguistics (ACL-87).
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the
complexity of translational equivalence. In Proceed-
ings of the International Conference on Computa-
tional Linguistics/Association for Computational Lin-
guistics (COLING/ACL-06), pages 977?984, Sydney,
Australia.
776
