Lexica l ized Context -F ree  Grammars  
Yves  Schabes  and  R ichard  C .  Waters  
Mi tsub ish i  E lectr ic  Research Laborator ies  
201 Broadway,  Cambr idge ,  MA 02139 
e-mail :  s( ;habes@merl .com and dick((~merl.coin 
Lexicalized context-free grammar (LCFG) is 
an attractive compromise between the parsing ef- 
ficiency of context-free grammar (CFC) and the 
elegance and lexical sensitivity of lexicalized tree- 
adjoining rammar (LTAG). LCFC is a restricted 
form of LTAG that can only generate context- 
free languages and can be parsed in cubic time. 
However, LCF(I supports much of the elegance of 
LTAG's analysis of English and shares with LTAG 
the ability to lexicalize CF(I;s without changing 
the trees generated. 
Motivation 
Context-free grammar (CFG) has been a well ac- 
cepted framework for computational linguistics for 
a long time. While it has drawbacks, including the 
inability to express ome linguistic constructions, 
it has the virtue of being computationally efficient, 
O(n3)-time in the worst case. 
Recently there has been a gain in interest in 
the so-called 'mildly' context-sensitive formalisms 
(Vijay-Shanker, 1987; Weir, 1988; Joshi, Vijay- 
Shanker, and Weir, 1991; Vijay-Shanker and Weir, 
1993a) that generate only a small superset of 
context-free languages. One such formalism is lex- 
icalized tree-adjoining grammar (LTAG) (Schabes, 
Abeill~, and Joshi, 1988; Abeillfi et al, 1990; Joshi 
and Schabes, 1992), which provides a number 
of attractive properties at the cost of decreased 
efficiency, O(n6)-time in the worst case (Vijay- 
Shanker, 1987; Schabes, 1991; Lang, 1990; Vijay- 
Shanker and Weir, 1993b). 
An LTAG lexicon consists of a set of trees each 
of which contains one or more lexical items. These 
elementary trees can be viewed as the elementary 
clauses (including their transformational variants) 
in which the lexical items participate. The trees 
are combined by substitution and adjunction. 
LTAC supports context-sensitive features that 
can capture some language constructs not cap- 
tured by CFG. However, the greatest virtue of 
LTAG is that it is lexicalized and supports ex- 
tended domains of locality. The lexical nature of 
LTAC is of linguistic interest, since it is believed 
that the descriptions of many linguistic phenom- 
ena are dependent upon lexical data. The ex- 
tended domains allow for the localization of most 
syntactic and semantic dependencies (e.g., filler- 
gap and predicate-argument relationships). 
A fllrther interesting aspect of LTAG is its 
ability to lexicalize CFCs. One can convert a CFC 
into an LTAG that preserves the original trees 
(Joshi and Schabes, 1992). 
Lexicalized context-free grammar (LCFG) is 
an attractive compromise between LTAG and 
CFG, that combines many of the virtues of LTAG 
with the efficiency of CFG. LCFC is a restricted 
form of LTAG that places further limits on the el- 
ementary trees that are possible and on the way 
adjunction can be performed. These restrictions 
limit LCFG to producing only context-free lan- 
guages and allow LCFC to be parsed in O(n3) -
time in the worst ease. However, LCFC retains 
most of the key features of LTAG enumerated 
above. 
In particular, most of the current LTAG gram- 
mar for English (Abeilld et al, 1990) follows the 
restrictions of LCFG. This is of significant practi- 
cal interest because it means that the processing 
of these analyses does not require more computa- 
tional resources than CFGs. 
In addition, any CFG can be transformed 
into an equivalent LCFC that generates the same 
trees (and therefore the same strings). This re- 
sult breaks new ground, because heretofore v- 
ery method of lexicalizing CFCs required context- 
sensitive operations (Joshi and Schabes, 1992). 
The following sections briefly, define LCFG, 
discuss its relationship to the current LTAG gram- 
mar for English, prove that LC, FC can be used to 
lexicalize CFC, and present a simple cubic-time 
parser for LCFC. These topics are discussed in 
greater detail in Schabes and Waters (1993). 
121 
Lex ica l i zed  Context -F ree  Grammars  
Like an LTAG, an LC'FG consists of two sets of 
trees: initial trees, which are combined by substi- 
tution and auxiliary trees, which are combined by 
adjunction. An LCFG is lexicalized in the sense 
that every initial and auxiliary tree is required to 
contain at least one terminal symbol on its fron- 
tier. 
More precisely, an LCFG is a five-tuple 
(Z, NT, I, A, ,5'), where ~ is a set of terminal sym- 
bols, NT is a set of non-terminal symbols, I and 
A are sets of trees labeled by terminal and non- 
terminal symbols, and ,5' is a distinguished non- 
terminal start symbol. 
Each initial tree in the set I satisfies the fol- 
lowing requirements. 
(i) Interior nodes are labeled by non- 
terminal symbols. 
(ii) The nodes on the frontier of the tree 
consist of zero or more non-terminal 
symbols and one or more terminal sym- 
bols. 
(iii) The non-terminal symbols on the 
frontier are marked for substitution. By 
convention, this is annotated in dia- 
grams using a down arrow (l). 
Each auxiliary tree in the set A satisfies the 
following requirements. 
(i) Interior nodes are labeled by non- 
terminal symbols. 
(ii) The nodes on the frontier consist of 
zero or more non-terminal symbols and 
one or more terminal symbols. 
(iii) All but one of the non-terminal sym- 
bols on the frontier are marked for sub- 
stitution. 
(iv) The remaining non-terminal on the 
frontier of the tree is called the foot. The 
label on the foot must be identical to 
the label on the root node of the tree. 
By convention, the foot is indicated in 
diagrams using an asterisk (.). 
(v) the foot must be in either the leftmost 
or the rightmost position on the frontier. 
Figure 1, shows seven elementary trees that 
might appear in an LCFG for English. The trees 
containing 'boy', 'saw', and 'left' are initial trees. 
The remainder are attxiliary trees. 
Auxiliary trees whose feet are leftrnost are 
called left recursive. Similarly, auxiliary trees 
whose feet are rightrnost are called righl recursive 
auxiliary trees. The path from the root of an aux- 
iliary tree to the foot is called the spine. 
NP VP N VP 
A /k A /X 
D$ N V VP* A N* VP* Adv 
I I I i 
boy seems pretty smoothly 
S 
S NPi,~(+wh) S S 
NPo$ VP NP o VP NPo$ VP 
A I \[ 
V SI*NA ?i V V NPI$ 
I I I 
think left saw 
Figure 1: Sample trees. 
In LCF(I, trees can be combined with substi- 
tution and adjunction. As illustrated in Figure 2, 
substitution replaces a node marked for substitu- 
tion with a copy of an initial tree. 
Adjunction inserts a copy of an auxiliary tree 
into another tree in place of an interior node that 
has the same label as the foot of the auxiliary tree. 
The subtree that was previously connected to the 
interior node is reconnected to the foot of the copy 
of the auxiliary tree. If the auxiliary tree is left re- 
cursive, this is referred to as left recursive adjunc- 
tion (see Figure 3). If the auxiliary tree is right 
recursive, this is referred to as right recursive ad- 
junction (see Figure 4). 
Crucially, adjunction is constrained by requir- 
ing that a left recursive auxiliary tree cannot be 
adjoined on any node that is on the spine of a 
right recursive auxiliary tree and a right recursive 
auxiliary tree cannot be adjoined on the spine of 
a left recursive auxiliary tree. 
An LCFG derivation must start with an initial 
tree rooted in S. After that, this tree can be re- 
peatedly extended using substitution and adjunc- 
tion. A derivation is complete when every frontier 
node is labeled with a terminal symbol. 
The difference between LCFG and LTAG is 
Figure 2: Substitution. 
122  
/ AA 
A 
Figure 3: Left recursive adjunction. 
~ A *  = "A 
% 
Figure 4: Right recursive adjunction. 
that LTAG allows the foot of an auxiliary tree 
to appear anywhere on the frontier and places no 
limitations on the interaction of auxiliary trees. 
In this unlimited situation, adjunction encodes 
string wrapping and is therefore more power- 
ful than concatenation (see Figure 5). However, 
the restrictions imposed by LCFG guarantee that 
no context-sensitive operations can be achieved. 
They limit the languages that can be generated by 
LCFGs to those that can be generated by CFGs. 
Coverage of LCFG and LTAG 
The power of LCFG is significantly less than 
LTAG. Surprisingly, it turns out that there are 
only two situations where the current LTAG gram- 
mar for English (Abeilld et al, 1990) fails to satisfy 
the restrictions imposed by LCFG. 
The first situation, concerns certain verbs that 
take more than one sentential complement. An ex- 
ample of such a verb is deduce, which is associated 
with the following auxiliary tree. 
S 
NPo$ VP 
V Sl* PP 
I A deduce P Sz,I, 
I from 
Since this tree contains a foot node in the cen- 
ter of its frontier, it is not part of an LCFG. Hav- 
ing the foot on the first sentential complement is
convenient, because it allows one to use the stan- 
dard LTAG wh-analyses, which depends on the 
w2 ~ W4 
% 
Figure 5: Adjunction in LTAG. 
existence of an initial tree where the filler and gap 
are local. This accounts nicely for the pair of sen- 
tences below. However, other analyses of wh ques- 
tions may not require the use of the auxiliary tree 
above. 
(1) John deduced that Mary watered the 
grass from seeing the hose. 
(2) What did John deduce that Mary wa- 
tered from seeing the hose. 
The second situation, concerns the way the 
current LTAG explains the ambiguous attach- 
ments of adverbial modifiers. For example, in the 
sentence: 
(3) John said Bill left yesterday. 
the attachment of yesterday is ambiguous. The 
two different LTAG derivations indicated in Fig- 
ure 6 represent his conveniently. 
Unfortunately, in LCFG the high attachment 
of yesterday is forbidden since a right auxiliary 
tree (corresponding to yesterday) is adjoined on 
the spines of a left auxiliary tree (corresponding to
John said). However, one could avoid this prob- 
lem by designing a mechanism to recover the high 
attachment reading from the low one. 
Besides the two cases presented above, the 
current LTAG for English uses only left and right 
recursive auxiliary trees and does not allow any 
S 
NP . .~  ",~ ........ 
I 
John V S* . .  . . . . . . . . . .  VP "-::. A said S ." "'? / \ 
,~t o- VP* ADV 
NP VP I 
I I yesterday 
Bill V 
I 
left 
Figure 6: Two LTAG derivations for John said Bill 
left yesterday. 
123 
interaction along the spine of these two kinds of 
trees. This agrees with the intuition that most 
English analyses do not require a context-sensitive 
operation. 
LCFG. However, as shown below, combining ex- 
tended substitution with restricted adjunction al- 
lows strong lexicalization of CFG, without intro- 
ducing greater parsing complexity than CFG. 
Lex ica l i za t ion  of  CFGs  
The lexicalization of grammar formalisms is of in- 
terest from a number of perspectives. It is of in- 
terest from a linguistic perspective, because most 
current linguistic theories give lexical accounts of a 
number of phenomena that used to be considered 
purely syntactic. It is of interest from a computa- 
tional perspective, because lexicalized grammars 
can be parsed significantly more efficiently than 
non-lexicalized ones (Schabes and Joshi, 1990). 
Formally, a grammar is said 'lexicalized' (Sch- 
abes, Abeill~., and Joshi, 1988) if it consists of: 
,, a finite set of elementary structures of finite size, 
each of which c, ontains an overt (i.e., non-empty) 
lexical item. 
? a finite set of operations for creating derived 
structures. 
The overt lexical item in an elementary struc- 
ture is referred to as its anchor. A lexicalized 
grammar can be organized as a lexicon where each 
lexical item is associated with a finite number of 
structures for which that item is the anchor. 
In general, CFGs are not lexicalized since rules 
such as ,5' --* NP VP  that do not locally introduce 
lexical items are allowed. In contrast, the well- 
known Creibach Normal Form (CNF) for CFCs 
is lexicalized, because very production rule is re- 
quired to be of the form A --+ ac~ (where a is a 
terminal symbol, A a non-terminal symbol and a 
a possibly empty string of non-terminal symbols) 
and therefore locally introduces a lexical item a. 
It can be shown that for any CFG (.7 (that does 
not derive the empty string), there is a CNF gram- 
mar (.7 ~ that derives the same language. However, 
it may be impossible for the set of trees produced 
by (7 ~ to be the same as the set of trees produced 
by G. 
Therefore, CNF achieves a kind of lexicaliza- 
tion of CFGs. However, it is only a weak lexical- 
ization, because the set of trees is not necessarily 
preserved. As discussed in the motivation section, 
strong lexicalization that preserves tree sets is pos- 
sible using LTAG. However, this is achieved at the 
cost of significant additional parsing complexity. 
Heretofore, several attempts have been made 
to lexicalize CFC with formalisms weaker than 
LTAG, but without success. In particular, it is 
not sufficient o merely extend substitution so that 
it applies to trees. Neither is it sutficient o rely 
solely on the kind restricted adjunction used by 
Theorem I f  G = (~,NT ,  P,S)  is a finitely 
ambiguous CFG which does not generate the 
empty .string (?), then there is an LCFG (7 ~ = 
(~, NT,  I, A, S) generating the same language and 
tree set as (7. Furthermore (7' can be chosen .so 
that it utilizes only lefl-recursive auxiliary trees. 
As usual in the above, a CFG (.7 is a four- 
tuple, (E, NT, P, S), where N is a set of terminal 
symbols, NT is a set of non-terminal symbols, P is 
a set of production rules that rewrite non-terminal 
symbols to strings of terminal and non-terminal 
symbols, and S is a distinguished non-terminal 
symbol that is the start symbol of any derivation. 
To prove the theorem we first prove a some- 
what weaker theorem and then extend the proof 
to the flfll theorem. In particular, we assume for 
the moment that the set of rules for (.7 does not 
contain any empty rules of the form A ~ e. 
S tep  1 We begin the construction of (7 ~ by con- 
structing a directed graph LCG that we call the 
left corner derivation graph. Paths in LCG cor- 
respond to leftmost paths from root to frontier in 
(partial) derivation trees rooted at non-terminal 
symbols in (1. 
L(TG contains a node for every symbol in E U 
NT and an arc for every rule in P as follows. 
For each terminal and non-terminal symbol 
X in G create a node in LCG labeled with 
X. For each rule X --+ Ya  in G create a 
directed arc labeled with X ~ Ya  from the 
node labeled with X to the node labeled Y. 
As an example, consider the example CFG in 
Figure 7 and the corresponding L(TG shown in 
Figure 8. 
The significance of L( ;G is that there is a one- 
to-one correspondence b tween paths in LCG end- 
ing on a non-terminal and left corner derivations in 
G. A left corner derivation in a CFG is a partial 
derivation starting from any non-terminal where 
every expanded node (other than the root) is the 
leftmost child of its parent and the left corner is a 
non-terminal. A left corner derivation is uniquely 
identified by the list of rules applied. Since G does 
not have any empty rules, every rule in (7 is rep- 
resented in L(;'G. Therefore, every path in LCG 
ending on a terminal corresponds to a left corner 
derivation in (7 and vice versa. 
124 
S---+A A 
,5' --+ B A 
A - -+B B 
B--+ A S 
B ----+ b 
Figure 7: An example grammar. 
S ---~ B A 
S -~.A A 
S ~- A B 
B--+ A S 
B--+b 
b 
Figure 8: The LC(;  created by Step 1. 
S tep  2 The set of initial trees I for G' is con- 
structed with reference to L(TG. In particular, an 
initial tree is created corresponding to each non- 
cyclic path in L(/G that starts at a non-terminal 
symbol X and ends on a terminal symbol y. (A 
non-cyclic path is a path that does not touch any 
node twice.) 
For each non-cyclic path in LCG from X to 
y, construct an initial tree T as follows. Start 
with a root labeled X. Apply the rules in the 
path one after another, always expanding the 
left corner node of T. While doing this, leave 
all the non-left corner non-terminal symbols 
in T unexpanded, and label them as substi- 
tution nodes. 
Given the previous example grammar, this 
step produces the initial trees shown in Figure 9. 
Each initial tree created is lexicalized, because 
each one has a non-terminal symbol as the left 
corner element of its frontier. There are a finite 
number of initial trees, because the number of non- 
cyclic paths in LCG must be finite. Each initial 
tree is finite in size, because ach non-cyclic path 
in LCG is finite in length. 
Most importantly, The set of initial trees is 
the set of non-recursive left corner derivations in 
(,'. 
S 
A AS 
B B$ B AS B B$ 
I I I 
b b b 
Figure 9: Initial trees created by Step 2. 
S tep  3 This step constructs a set of left- 
recursive auxiliary trees corresponding to the 
cyclic path segments in L(TG that were ignored in 
the previous step. In particular, an attxiliary tree 
is created corresponding to each minimM cyclic 
path in LCG that starts at a non-terminM sym- 
bol. 
For each minimal cycle in LCG from X to it- 
self, construct an auxiliary tree T by starting 
with a root labeled X and repeatedly expand- 
ing left, corner frontier nodes using the rules 
in the path as in Step 2. When all the rules in 
the path have been used, the left corner fron- 
tier node in T will be labeled X. Mark this 
as the foot node of T. While doing the above, 
leave all the other non-terminal symbols in T 
unexpanded, and label them all substitution 
nodes. 
The LC( ;  in Figure 8 has two minimal cyclic 
paths (one from A to A via B and one from B to 
B via A). This leads to the the two auxiliary trees 
shown in Figure 10, one for A and one for B. 
The attxiliary trees generated in this step are 
not, necessarily lexicalized. There are a finite num- 
ber of auxiliary trees, since the number of minimal 
cyclic paths in G must be finite. Each auxiliary 
tree is finite in size, because ach minimal-cycle in 
LCG is finite in length. 
The set of trees that can he created by corn- 
biding the initial trees from Step 2 with the auxil- 
iary trees from Step 3 by adjoining auxiliary trees 
along the left edge is the set of every left corner 
derivation in (,'. To see this, consider that ev- 
ery path in L( ;G can be represented as an initial 
non-cyclic path with zero or more minimal cycles 
inserted into it. 
The set of trees that can be created by corn- 
biding the initial trees from Step 2 with the auxil- 
iary trees from Step 3 using both substitution and 
adjunction is the set of every derivation in G. To 
see this, consider that every derivation in G can 
be decomposed into a set of left corner derivations 
in G that are combined with substitution. In par- 
ticular, whenever a non-terminal node is not the 
leftmost child of its parent, it is the head of a sep- 
A B 
B B$ A S$ 
A* S$ B* B$ 
Figure 10: Auxiliary trees created by Step 3. 
125 
arate left corner derivation. 
S tep  4 This step lexicalizes the set of auxiliary 
trees built in step 3, without altering the trees that 
can be derived. 
For each auxiliary tree T built in step 3, con- 
sider the frontier node A just to the right of 
the foot. If this node is a terminal do nothing. 
Otherwise, remove T from the set of auxiliary 
trees replace it with every tree that can be 
constructed by substituting one of the initial 
trees created in Step 2 for the node A in T. 
In the case of our continuing example, Step 4 
results in the set of auxiliary trees in Figure 11. 
Note that since G is finitely ambiguous, there 
must be a frontier node to the right of the foot of 
an attxiliary tree T. If not, then T would corre- 
spond to a derivation X:~X in G and 6' would be 
infinitely ambiguous. 
After Step 4, every auxiliary tree is lexicalized, 
since every tree that does not have a terminal to 
the right of its foot is replaced by one or more trees 
that do. Since there were only a finite number of 
finite initial and auxiliary trees to start with, there 
are still only a finite number of finite attxiliary 
trees. 
The change in the auxiliary trees caused by 
Step 4 does not alter the set of trees that can be 
produced in any way, because the only change that 
was made was to make substitutions that could be 
made anyway, and when a substitutable node was 
eliminated, this was only done after every possible 
substitution at that node was performed. 
Note that the initial trees are left anchored 
and the auxiliary trees are ahnost left anchored 
in the sense that the leftmost frontier node other 
than the foot is a terminal. This facilitates effi- 
cient left to right parsing. 
A 
A 
B B$ 
A* S B B$ 
A AS A* S A S$ 
B B$ B AS B* B 
I I I 
b b b 
Figure 1 l: Auxiliary trees created by Step 4. 
The procedure above creates a lexicalized 
grammar that generates exactly the same trees as 
G and therefore the same strings. The only re- 
maining issue is the additional assumption that G 
does not contain any empty rules. 
If (; contains an empty rule A ~ e one first 
uses standard methods to transform (; into an 
equivalent grammar H that does not have any 
such rule. When doing this, create a table showing 
how each new rule added is related to the empty 
rules removed. Lexicalize H producing H'  using 
the procedure above. Derivations in H '  result in 
elements of the tree set of H. By means of the ta- 
ble recording the relationship between (; and H, 
these trees can be converted to derivations in G. 
\[\] 
Add i t iona l  i ssues  
There are several places in the algorithm where 
greater freedom of choice is possible. For instance, 
when lexicalizing the auxiliary trees created in 
Step 3, you need not do anything if there is any 
frontier node that is a terminal and you can choose 
to expand any frontier node you want. For in- 
stance you might want to choose the node that 
corresponds to the smallest number of initial trees. 
Alternatively, everywhere in the procedure, 
the word 'left' can be replaced by 'r ight' and vice 
versa. This results in the creation of a set of right 
anchored initial trees and right recursive auxiliary 
trees. This can be of interest when the right cor- 
ner derivation graph has less cycles than the left 
corner one. 
The number of trees in G' is related to the 
number of non-cyclic and minimal cycle paths in 
LCG. In the worst case, this number rises very 
fast as a function of the number of arcs in LCG, 
(i.e., in the number of rules in G). (A fully con- 
nected graph of n 2 arcs  between n nodes has n! 
acyclic paths and n! minimal cycles.) However, in 
the typical case, this kind of an explosion of trees 
is unlikely. 
Just as there can be many ways for a CF(~ 
to derive a given string, there can be many ways 
for an LCFG to derive a given tree. For maximal 
efficiency, it would be desirable for the grammar 
G' produced by the procedure above to have no 
ambiguity in they way trees are derived. Unfortu- 
nately, the longer the minimal cycles in LCG, the 
greater the tree-generating ambiguity the proce- 
dure will introduce in G'. However, by modifying 
the procedure to make use of constraints on what 
attxiliary trees are allowed to adjoin on what nodes 
in which initial trees, it should be possible to re- 
duce or even eliminate this ambiguity. 
All these issues are discussed at greater length 
126 
in Schabes and Waters (1993). 
Pars ing  LCFG 
Since LCFG is a restricted case of tree-adjoining 
grammar (TAG), standard O(nG)-time TAG 
parsers (Vijay-Shanker, 1987; Schabes, 1991; 
Lang, 1990) can be used for parsing LCFG. Fur- 
ther, they can be straightforwardly modified to re- 
quire at most O(n4)-tirne when applied to LCFG. 
However, this still does not take fifll advantage of 
the context-freeness of LCFC. 
This section describes a simple I)ottom-up 
recognizer for LCFG that is in the style of the 
CKY parser for (IT(I;. The virtue of this algo- 
rithm is that it shows in a simple manner how the 
O(n3)-time worst case complexity can be achieved 
for LCFG. Schabes and Waters (1993) describes a
more practical and more elaborate (Earley-style) 
recognizer for LCFC, which achieves the same 
bounds. 
Suppose that G = (E, NT, I ,A ,S)  is an 
LCFG and that a l ' "a ,~ is an input string. We 
can assume without loss of generality 1 that every 
node in I U A has at most two children. 
Let 71 be a node in an elementary tree (identi- 
fied by the name of the tree and the position of the 
node in the tree). The central concept of the al- 
gorithrn is the concepts of spanning and covering. 
71 spans a string ai+l . . .a j  if and only if there is 
some tree derived by ('; for which it is the case that 
the fringe of the subtree rooted at 71 is ai+l "" "aj. 
In particular, a non-terminal node spans aj if and 
only if the label on the node is aj. A non-terrninal 
node spans ai+ 1 . . .a j  if and only if ai+l . . .a j  is 
the concatenation i left, to right order of strings 
spanned by the children of the node. 
? If 7 / does not subsume the foot node of an aux- 
iliary tree then: 71 covers the string ai+ 1 . . . a j  if 
and only if it spans a i+ l "  . a j .  
? If 7 / is on the spine of a right recursive auxiliary 
tree T then: 71 covers ai+l." .aj if and only if 
7 / spans some strin~ that is the concatenation 
of ai+l - . .a j  and a string spanned by the foot 
of T. (This situation is illustrated by the right 
drawing in Figure 12, in which 7 / is labeled with 
B.) 
? If 71 is on the spine of a left recursive auxiliary 
tree T then: 71 covers ai+\] " .aj if and only if 71 
spans some string that is the concatenation of a 
string spanned by the foot of T and ai+l . . .a j .  
(This situation is illustrated by the left drawing 
in Figure 12, in which 71 is labeled with B.) 
lit can be easily shown that by adding new nodes 
('4 "~ any L ,F(., can be transformed into an equivalent 
LC, FG satisfying this condition. 
A,  ai+l-.- aj ai+l.., aj A* 
Figure 12: Coverage of nodes on the spine. 
The algorithm stores pairs of the form (71, pos) 
in an n by n array C. In a pair, pos is either t (for 
top) or b (for bottom). For every node 7l in every 
elementary tree in (;, the algorithm guarantees the 
following. 
? ('l,b) e C\[i,j\] if and only if,I covers ai+l . . .a j .  
? ('l,t) E C\[i,j\] if and only if ('l,b} E C,\[i,j\] or 
ai+l . . .a j  is the concatenation (in either order) 
of a string covered by 7 / and a string covered by 
an auxiliary tree that can be adjoined on 71 .
The algorithm fills the upper diagonal portion 
of the array C\[i, j\] (0 < i < j _< n) for increasing 
values of j - i. The process tarts by placing each 
foot node in every cell C'\[i,i\] and each terminal 
node 71 in every cell C\[i, i + 1\] where 71 is labeled 
a i+ l  ? 
The algorithm then considers all possible ways 
of combining covers into longer covers. In particu- 
lar, it fills the cells C\[i, i + k\] for increasing values 
of k by combining elements from the cells C\[i, j\] 
and C\[ j , i  + k\] for all j such that i < j < i + k. 
There are three situations where combination is 
possible: sibling concatenation, left recursive con- 
catenation, and right recursive concatenation. 
Sibling concatenation is illustrated in Fig- 
ure 13. Suppose that there is a node 7/0 (labeled B) 
with two children 711 (labeled A) and 712 (labeled 
A'). If (711 , t) E C\[i, j\] and ('12, t} E (7\[j, i + k\] then 
('1o, b) E C\[i, i + k\]. 
Left recursive concatenation is illustrated in 
Figure 14. Here, the cover of a node is combined 
with the cover of a left auxiliary tree that can be 
adjoined at the node. Right recursive concatena- 
tion, which is shown in Figure 15 is analogous. 
For simplicity, the recognizer is written in 
two parts. A main procedure and a subpro- 
cedure Add(node, pos, i , j ) ,  which adds the pair 
(node, pos) into C\[i, j\]. 
a. . . .a.  ai+ 1 t+ l  J a j+ l ' "ak  "'" ak 
Figure 13: Sibling concatenation. 
127 
Procedure recogn izer  
begin 
;; foot node initialization ( ,\[z, i\]) 
for i = 0 to n 
for all foot node 71 in A call 
Add0/, b, i, i) 
;; terminal node initialization ((;\[i, i + 1\]) 
fo r i=0 ton- l  
for all node 71 in A U I labeled by ai+l 
call Add0/, t, i, i + 1) 
;; induction (G'\[i, i + k\] = (;\[i, j\] + (:\[j, i + k\]) 
for k = 2 to n 
for i = 0 to n -  k 
fo r j= i+ 1 to i+k-1  
;; sibling concatenation 
if (711 , l) 6 C,\[i, j\] 
and (712, t) e C\[j, i + k\] 
and r/1 is the left sibling of 7/2 
with common parent 71o 
then Add(710 , b, i, i + k) 
;; left recursive concatenation 
if {71, b) E C\[i, j\] 
and (p, t} e (,'\[/, i + k\] 
and p is the root node of a left recursive 
auxil iary tree that can adjoin on rl 
then Add0j , t, i, i + k) 
;; right recursive concatenation 
if {'l, b) e (;\[j, i + k\] 
and (p, t) E C\[i, j\] 
and p is the root node of a right recursive 
auxiliary tree that can adjoin on 7 I
then Add(r/, t, i, i + k) 
if (7/, z) e c\[0, 7q 
and 71 is labeled by ,5' 
and 71 is the root node of an initial tree in I 
then return acceptance 
otherwise return rejection 
end 
Note that the sole purl)ose of the codes t and b 
is to insure that only one auxiliary tree can adjoin 
on a node. The procedure could easily be mod- 
ified to account for other constraints on the way 
derivation should proceed such as those suggested 
for LTAGs (Schabes and Shieber, 1992). 
The procedure Add puts a pair into the array 
C. If the pair is already present, nothing is (lone. 
However, if it is new, it is added to (7 and other 
pairs may be added as well. These correspond to 
cases where the coverage is not increased: when 
a node is the only child of its parent, when the 
A 
/2.,+ /2... 
ai+l... ~ A*a .  ...a k J+ l  ai+l'"ak 
Figure 14: Left recursive concatenation. 
A 
/2., 
. . . .  A* ... ak aj+i.,  a k ai+ 1 aj ai+ 1 
Figure 15: Right recursive concatenation. 
node is recognized without adjunction, and when 
substitution occurs. 
Procedure Add(r/ ,  pos, i, j )  
begin 
Put (rl, pos) in C,\[i, j\] 
if pos = t and r I is the only child of a parent It 
call Add(#, b, i, j )  
if pos = t and r? is the root node of an 
initial tree, for each substitution node p 
at which 71 can substitute call Add(p, t, i, j )  
;; no adjunction 
if pos = b 
if the node 7/does not have an OA constraint 
call Add(r/, t, i, j )  
end 
The O(n 3) complexity of the recognizer fol- 
lows from the three nested induction loops on k, i 
and j.  (Although the procedure Add is defined 
recursively, the number of pairs added to (7 is 
bounded by a constant hat is independent of sen- 
tence length.) 
By recording how each pair was introduced in 
each cell of the array C, one can easily extend the 
recognizer to produce all derivations of the input. 
Conclusion 
LCFG combines much of the power of LTAG with 
tile computat ional  efficiency of CFG. It supports 
most of the same linguistic analysis supported by 
LTAC. In particular, most of the current LTAG 
for English falls into LCFG. In addition, LCFC 
can lexicalize CFG without altering the trees pro- 
duced. Finally, LCFG can be parsed in O(n3)- 
time. 
There are many directions in which the work 
on LCFG described here could be extended. In 
128 
particular, one could consider stochastic exten- 
sions, LP~ parsing, and non-deterministic LR pars- 
ing. 
Acknowledgments 
We thank John Coleman who, by question- 
ing whether the context-sensitivity of stochastic 
LTAG was actually being used for English, trig- 
gered this work. We thank Aravind Joshi, Fer- 
nando Pereira, Stuart Shieber and B. Srinivas for 
valuable discussions. 
REFERENCES 
Abeilld, Anne, Kathleen M. Bishop, Sharon Cote, 
and Yves Schabes. 1990. A lexicalized tree 
adjoining grammar for English. Technical Re- 
port MS-CIS-90-24, Department of Computer 
and Information Science, University of Penn- 
sylvania. 
Joshi, Aravind K. and Yves Schabes. 1992. Tree- 
adjoining grammars and lexicalized gram- 
mars. In Maurice Nivat and Andreas Podel- 
ski, editors, Tree Automata and Languages. 
Elsevier Science. 
Joshi, Aravind K., K. Vijay-Shanker, and David 
Weir. 1991. The convergence of mildly 
context-sensitive grammatical formalisms. In 
Peter Sells, Stuart Shieber, and Tom Wasow, 
editors, Foundational Issues in Nalural Lan- 
guage Processing. MIT Press, Cambridge MA. 
Lang, Bernard. 1990. The systematic onstruc- 
tions of Earley parsers: Application to the 
production of O(n 6) Earley parsers for Tree 
Adjoining Grammars. In Proceedings of the 
Ist International Workshop on Tree Adjoining 
Grammars, Dagstuhl C, astle, FRG, August. 
Schabes, Yves, Anne Abeill6, and Aravind K. 
Joshi. 1988. Parsing strategies with 'lexical- 
ized' grammars: Application to tree adjoining 
grammars. In Proceedings of the 12 th Interna- 
tional Conference on Computational Linguis- 
tics (COLING'88), Budapest, Hungary, An- 
gust. 
Schabes, Yves and Aravind K. Joshi. 1990. Pars- 
ing with lexicalized tree adjoining grammar. 
In Masaru Tomita, editor, C, urrent Issues 
in Parsing Technologies. Kluwer Accademic 
Publishers. 
Schabes, Yves and Stuart Shieber. 1992. An al- 
ternative conception of tree-adjoining deriva- 
tion. In 20 th Meeting of the Association for 
(,'omputational Linguistics (A CL '92). 
Schabes, Yves and Richard C. Waters. 1993. Lex- 
icalized context-free grammar: A cubic-time 
parsable formalism that strongly lexicalizes 
context-free grammar. Technical Report 93- 
04, Mitsubishi Electric Research Laboratories, 
201 Broadway. Cambridge MA 02139. 
Schabes, Yves. 1991. The valid prefix prop- 
erty and left to right parsing of tree-adjoining 
grammar. In Proceedings of the second Inter- 
national Workshop on Parsing Technologies, 
Cancan, Mexico, February. 
Vijay-Shanker, K. and David Weir. 1993a. The 
equivalence of four extensions of context-free 
grammars. To appear in Mathematical Sys- 
tems Theory. 
Vijay-Shanker, K. and \[)avid Weir. 1993b. Pars- 
ing some constrained grammar formalisms. 
To appear in Computational Linguistics. 
Vijay-Shanker, K. 1987. A Study of Tree Adjoin- 
ing Grammars. Ph.D. thesis, Department of 
Computer and Information Science, Univer- 
sity of Pennsylvania. 
Weir, David J. 1988. Character- 
izing Mildly Context-,5?nsitive Grammar For- 
malisms. Ph.D. thesis, Department of Com- 
puter and Information Science, University of 
Pennsylvania. 
129 
