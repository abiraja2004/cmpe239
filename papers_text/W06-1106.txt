Proceedings of the Workshop on Linguistic Distances, pages 35?42,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Sentence Comparison
using Robust Minimal Recursion Semantics
and an Ontology
Rebecca Dridan
}
and Francis Bond

}
rdrid@csse.unimelb.edu.au

bond@cslab.kecl.ntt.co.jp
}
The University of Melbourne

NTT Communication Science Laboratories,
Nippon Telegraph and Telephone Corporation
Abstract
We design and test a sentence com-
parison method using the framework
of Robust Minimal Recursion Seman-
tics which allows us to utilise the deep
parse information produced by Jacy, a
Japanese HPSG based parser and the
lexical information available in our on-
tology. Our method was used for both
paraphrase detection and also for an-
swer sentence selection for question an-
swering. In both tasks, results showed
an improvement over Bag-of-Words, as
well as providing extra information use-
ful to the applications.
1 Introduction
Comparison between sentences is required for
many NLP applications, including question
answering, paraphrasing, text summarization
and entailment tasks. In this paper we show
an RMRS (Robust Minimal Recursion Seman-
tics, see Section 1.1) comparison algorithm
that can be used to compare sentences in
any language that has RMRS generating tools
available. Lexical resources of any language
can be plugged in to give a more accurate and
informative comparison.
The simplest and most commonly used
methods of judging sentence similarity use
word overlap { either looking for matching
word sequences, or comparing a Bag-of-Words
representation of each sentence. Bag-of-Words
discards word order, and any structure desig-
nated by such, so that the cat snored and the
dog slept is equivalent to the dog snored and
the cat slept. Sequence matching on the other
hand requires exact word order matching and
hence the game began quietly and the game qui-
etly began are not considered a match. Neither
method allows for synonym matching.
Hirao et al (2004) showed that they could
get a much more robust comparison using
dependency information rather than Bag-of-
Words, since they could abstract away from
word order but still compare the important
elements of a sentence. Using deep parsing
information, such as dependencies, but also
deep lexical resources where available, enables
a much more informative and robust compar-
ison, which goes beyond lexical similarity. We
use the RMRS framework as our comparison
format because it has the descriptive power to
encode the full semantics, including argument
structure. It also enables easy combination of
deep and shallow information and, due to its
at structure, is easy to manage computation-
ally.
1.1 Robust Minimal Recursion
Semantics
Robust Minimal Recursion Semantics
(RMRS) is a form of at semantics which is
designed to allow deep and shallow processing
to use a compatible semantic representation,
while being rich enough to support gener-
alized quantiers (Frank, 2004). The main
component of an RMRS representation is
a bag of elementary predicates and their
arguments.
An elementary predicate always has a
unique label, a relation type, a relation name
and an ARG0 feature. The example in Fig-
ure 1 has a label of h5 which uniquely identi-
es this predicate. Relation types can either
be realpred for a predicate that relates di-
rectly to a content word from the input text, or
gpred for grammatical predicates which may
not have a direct referent in the text. For ex-
amples in this paper, a realpred is distin-
guished by an underscore ( ) before the rela-
tion name.
The gpred relation names come from a
35
"unten s
lbl h5
arg0 e6
#
Figure 1: Elementary predicate for-2 unten
\drive"
closed-set which specify common grammatical
relations, but the realpred names are formed
from the word in the text they relate to and
this is one way in which RMRS allows under-
specication. A full relation name is of the
form lemma pos sense, where the pos (part
of speech) is drawn from a small set of general
types including noun, verb and sahen (verbal
noun). The sense is a number that identies
the sense of the word within a particular gram-
mar being used. The POS and sense informa-
tion are only used when available and hence
the unten s 1 is more specic but compati-
ble with unten s or even unten.
The arg0 feature (e6 in Figure 1) is the
referential index of the predicate. Predicates
with the same arg0 are said to be referen-
tially co-indexed and therefore have the same
referent in the text.
A shallow parse might provide only the fea-
tures shown in Figure 1, but a deep parse can
also give information about other arguments
as well as scoping constraints. The features
arg1..arg4 specify the indices of the semantic
arguments of the relevant predicate, similar to
PropBank's argument annotation (Kingsbury
et al, 2002). While the RMRS specication
does not dene semantic roles for the argn
features, in practice arg1 is generally used for
the agent and arg2 for the patient. Fea-
tures arg3 and arg4 have less consistency in
their roles.
We will use (1) and (2) as examples of sim-
ilar sentences. They are denition sentences
for one sense of ),'* doraiba- \driver",
taken from two dierent lexicons.
(1) .30 & -2 !% 1
jidosha wo unten suru hito
car acc drive do person
\a person who drives a car"
(2) .30 #" $ -2 /
jidosha nado no unten sha
car etc. adn drive -er
\a driver of cars etc."
Examples of deep and shallow RMRS results
for (1) are given in Figure 2. Deep results for
(2) are given in Figure 3.
2 Algorithm
The matching algorithm is loosely based on
RMRS comparison code included in the LKB
(Copestake, 2002: hhttp://www.delph-in.
net/lkb/i), which was used in Ritchie (2004),
however that code used no outside lexical re-
sources and we have substantially changed the
matching algorithm.
The comparison algorithm is language inde-
pendent and can be used for any RMRS struc-
tures. It rst compares all elementary predi-
cates from the RMRSs to construct a list of
match records and then examines, and poten-
tially alters, the list of match records accord-
ing to constraints encoded in the argn vari-
ables. Using the list of scored matches, the
lowest scoring possible match set is found and,
after further processing on that set, a similar-
ity score is returned. The threshold for de-
ciding whether a pair of sentences should be
considered similar or not can be determined
separately for dierent applications.
2.1 Matching Predicates
The elementary predicates (EPs) of our RMRS
structures are divided into two groups - those
that have a referent in the text, hereafter
known as content EPs, and those that don't.
There are three kinds of content EP: real-
preds, which correspond to content bearing
words that the grammar knows; gpreds with
a carg (constant argument) feature, which
are used to represent proper names and num-
bers; and gpreds with a predicate name start-
ing with generic such as generic verb which
are used for unknown words that have only
been identied by their part of speech. All
other EPs have no referent and are used to
provide information about the content EPs or
about the structure of the sentence as a whole.
These non-content EPs can provide some use-
ful information, but generally only in relation
to other content EPs.
Each content EP of the rst RMRS is com-
pared to all content EPs in the second RMRS,
as shown in Figure 4.
Matches are categorised as exact, syn-
onym, hypernym, hyponym or no match
and a numerical score is assigned. The nu-
36
26
6
6
6
6
6
6
6
6
6
6
6
6
4
text ',)%&+ $*
top h1
rels
8
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
:
"
proposition m rel
lbl h1
arg0 e2
marg h3
#"
unknown rel
lbl h4
arg0 e2
arg x5
#

jidousha n
lbl h6
arg0 x7

2
6
4
udef rel
lbl h8
arg0 x7
rstr h9
body h10
3
7
5
2
6
4
unten s
lbl h11
arg0 e13
arg1 u12
arg2 x7
3
7
5

hito n
lbl h14
arg0 x5

2
6
4
udef rel
lbl h15
arg0 x5
rstr h16
body h17
3
7
5
"
proposition m rel
lbl h10001
arg0 e13
marg h18
#
2
6
4
topic rel
lbl h10002
arg0 e19
arg1 e13
arg2 x5
3
7
5
9
>
>
>
>
>
>
>
=
>
>
>
>
>
>
>
>
;
hcons fh3 qeq h4 ; h9 qeq h6 ; h16 qeq h14 ; h18 qeq h11g
ing fh11 ing h10002 ; h14 ing h10001g
3
7
7
7
7
7
7
7
7
7
7
7
7
7
5
2
6
6
6
4
text ',)%&+ $*
top h9
rels
 
jidousha n
lbl h1
arg0 x2

wo rel
lbl h3
arg0 u4

unten s
lbl h5
arg0 e6

suru rel
lbl h7
arg0 e8

hito n
lbl h9
arg0 x10
 
hcons fg
ing fg
3
7
7
7
5
Figure 2: Deep (top) and shallow (bottom) RMRS results for .30 & -2 !% 1
2
6
6
6
6
6
6
6
6
6
6
6
6
6
4
text ',) "! # &+ (
top h1
rels
8
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
:
"
proposition m rel
lbl h1
arg0 e2
marg h3
#"
unknown rel
lbl h4
arg0 e2
arg x5
#

jidousha n
lbl h6
arg0 x7

2
6
4
udef rel
lbl h8
arg0 x7
rstr h9
body h10
3
7
5
"
nado n
lbl h10001
arg0 u11
arg1 x7
#
2
6
4
udef rel
lbl h12
arg0 x5
rstr h13
body h14
3
7
5
2
6
4
unten s
lbl h15
arg0 x5
arg1 u16
arg2 x7
3
7
5
"
noun-relation
lbl h17
arg0 x5
arg1 h18
#"
proposition m rel
lbl h18
arg0 x5
marg h19
#"
sha n
lbl h10002
arg0 u20
arg1 x5
#
9
>
>
>
>
>
>
>
=
>
>
>
>
>
>
>
>
;
hcons fh3 qeq h4 ; h9 qeq h6 ; h13 qeq h17 ; h19 qeq h15g
ing fh6 ing h10001 ; h17 ing h10002g
3
7
7
7
7
7
7
7
7
7
7
7
7
7
5
Figure 3: RMRS representation for .30 #" $ -2 /
foreach ep1 in contentEPs1
foreach ep2 in contentEPs2
(score, match) = match_EPs(ep1, ep2)
if match != NO_MATCH
add_to_matches(ep1, ep2, score, match)
endif
done
done
Figure 4: Predicate match pseudo-code
merical score represents the distance between
the two EPs, and hence an exact match is
assigned a score of zero.
The level of matching possible depends on
the lexical resources available. With no extra
resources, or only a dictionary to pick up or-
thographic variants, the only match types pos-
sible are exact and no match. By adding
a thesaurus, an ontology or a gazetteer, it is
then possible to return synonym, hypernym
and hyponym match relations. In our ex-
periments we used the ontology described in
Section 3.2.2, which provides all three extra
match types. Adding a thesaurus only would
enable synonym matching, while a gazetteer
could be added to give, for example, Tokyo is
a hyponym of city.
Matches:
hito_n - sha_n : HYPERNYM (2)
jidosha_n - jidosha_n: EXACT (0)
unten_s_2 - unten_s_2: EXACT (0)
Figure 5: First pass match list for (1) and (2)
At the end of the rst pass, a list of match
records shows all EP matches with their match
type and score. Each EP can have multiple
possible matches. The output of comparing
(1) and (2), with the RMRSes in Figures 2
and 3, is shown in Figure 5. This shows hito n
(1 hito \person") tagged as a hypernym of
37
foreach match in matches
gpreds1 = get_gpreds_arg0(ep1{arg0})
gpreds2 = get_gpreds_arg0(ep2{arg0})
totalgpreds = len gpreds1 + len gpreds2
foreach ep1 in gpreds1
foreach ep2 in gpreds2
if(match_gram_eps(ep1, ep2)
remove(ep1, gpreds1)
remove(ep2, gpreds2)
endif
done
done
gpreds_left = len gpreds1 + len gpreds2
left = gpreds_left/totalgpreds
match{score}+= left*gpredWeight
done
Figure 6: Matching ARG0s
sha n (/ sha \-er" is a sux indicating a per-
son, normally the agent of a verb: it is more re-
strictive than English -er , in that it only refers
to people).
2.2 Constraints Pass
For each possible match, all the non-content
EPs that have the same arg0 value as the
content EPs in the match are examined, since
these have the same referent. If each non-
content EP related to the content EP on one
side of the match can be matched to the non-
content EPs related to the other content EP,
no change is made. If not, however, a penalty
is added to the match score, as shown in Fig-
ure 6. In our example, unten s 2 from the rst
sentence has a proposition m rel referen-
tially co-indexed, while the second unten s 2
has a proposition m rel, a noun-relation
and a udef rel, and so a small penalty is
added as shown in Figure 7.
The second check in the constraint match
pass examines the arguments (arg1, arg2,
arg3, arg4) of each of the matches. It looks
for possible matches found between the EPs
listed as argn for each match. This check can
result in three separate results: both EPs have
an argn but there is no potential match found
between the respective argn EPs, a potential
match has been found between the argn EPs,
or only one of the EPs in the match has an
argn feature.
Where both EPs have an argn feature, the
score (distance) of the match is decreased or
increased depending on whether a match be-
tween the argn variables was found. Given
that the RMRS denition does not specify a
Matches:
hito_n - sha_n : HYPERNYM (2.1)
jidosha_n - jidosha_n: EXACT (0)
unten_s_2 - unten_s_2: EXACT (0.05)
Figure 7: Match list
Slight penalty added to unten s 2 and hito n
for non-matching non-content EPs
`meaning' for the argn variables, comparing,
for example, arg1 variables from two dier-
ent predicates may not necessarily be compar-
ing the same semantic roles. However, be-
cause of the consistency found in arg1 and
arg2 meaning this is still a useful check. Of
course, if we are comparing the same relation,
the args will all have the same meaning. The
comparison method allows for dierent penal-
ties for each of arg1 to arg4, and also in-
cludes a scaling factor so that mismatches in
args when comparing exact EP matches will
have more eect on the score than in non
exact matches. If one EP does not have
the argn feature, no change is made to the
score. This allows for the use of underspeci-
ed RMRSs, in the case where the parse fails.
At the end of this pass, the scores of the
matches in the match list may have changed
but the number of matches is still the same.
2.3 Constructing the Sets
Match sets are constructed by using a branch-
and-bound decision tree. Each match is con-
sidered in order, and the tree is branched if
the next match is possible, given the proceed-
ing decisions. Any branch which is more than
two decisions away from the best score so far
is pruned. At the end of this stage, the lowest
scoring match set is returned and then this is
further processed.
If no matches were found, processing stops
and a sentinel value is returned. Otherwise,
the non matching predicates are grouped to-
gether by their arg0 value. Scoping con-
straints are checked and if any non matching
predicate outscopes a content predicate it is
added to that grouping. Hence if it outscopes
a matching EP it becomes part of the match,
otherwise it becomes part of a non-matching
EP group.
Any group of grammatical EPs that shares
an arg0 but does not contain a content pred-
icate is matched against any similar groupings
38
Best score is 0.799 for the match set:
MATCHES:
hito_n-sha_n: HYPERNYM:2.1
jidousha_n-jidousha_n:EXACT:0
unten_s_2-unten_s_2:EXACT:0.05
proposition_m_rel-proposition_m_rel:EXACT:0
UNMATCHED1:
UNMATCHED2:
u11: h10001:nado_n
Figure 8: Verbose comparison output
in the other RMRS. This type of match can
only be exact or no match and will make
only a small dierence in the nal score.
Content predicates that have not been
matched by this stage are not processed any
further, although this is an area for further
investigation. Potentially negation and other
modiers could be processed at this point.
2.4 Output
The output of the comparison algorithm is a
numeric score and also a representation of the
nal best match found.
The numerical score, using the default scor-
ing parameters, ranges between 0 (perfect
match) and 3. As well as the no match score
(-5), sentinel values are used to indicate miss-
ing input data so it is possible to fall back to
a shallow parse if the deep parse failed.
Details of the match set are also returned for
further processing or examination if the appli-
cation requires. This shows which predicates
were deemed to match, and with what score,
and also shows the unmatched predicates. Fig-
ure 8 shows the output of our example com-
parison.
3 Resources
While the comparison method is language in-
dependent, the resources required are lan-
guage specic. The resources fall in to two
dierent categories: parsing and morpholog-
ical analysis tools that produce the RMRSs,
and lexical resources such as ontologies, dictio-
naries and gazetteers for evaluating matches.
3.1 Parsing
Japanese language processing tools are freely
available. We used the Japanese grammar
Jacy (Siegel and Bender, 2002), a deep parsing
HPSG grammar that produces RMRSs for our
primary input source.
When parsing with Jacy failed, compar-
isons could still be made with RMRS produced
from shallow tools such as ChaSen (Mat-
sumoto et al, 2000), a morphological analyser
or CaboCha (Kudo and Matsumoto, 2002), a
Japanese dependency parser. Tools have been
built to produced RMRS from the standard
output of both those tools.
The CaboCha output supplies similar de-
pendency information to that of the Basic El-
ements (BE) tool used by Hovy et al (2005b)
for multi-document summarization. Even this
intermediate level of parsing gives better com-
parisons than either word or sequence overlap,
since it is easier to compare meaningful ele-
ments (Hovy et al, 2005a).
3.2 Lexical Resources
Whilst deep lexical resources are not available
for every language, where they are available,
they should be used to make comparisons more
informative. The comparison framework al-
lows for dierent lexical resources to be added
to a pipeline. The pipeline starts with a sim-
ple relation name match, but this could be fol-
lowed by a dictionary to extract orthographic
variants and then by ontologies such as Word-
Net (Fellbaum, 1998) or GoiTaikei (Ikehara
et al, 1997), gazetteers or named entity recog-
nisers to recognise names of people and places.
The sections below detail the lexical resources
we used within our experiments.
3.2.1 The Lexeed Semantic Database
The Lexeed Semantic Database of Japanese
is a machine readable dictionary that covers
the most familiar words in Japanese, based
on a series of psycholinguistic tests (Kasahara
et al, 2004). Lexeed has 28,000 words divided
into 46,000 senses and dened with 75,000 def-
inition sentences. Each entry includes a list of
orthographic variants, and the pronunciation,
in addition to the denitions.
3.2.2 Ontology
The lexicon has been sense-tagged and
parsed to give an ontology linking senses with
various relations, principally hypernym and
synonym (Nichols et al, 2005). For example,
hhypernym, ),'* doraiba \driver", (
,+ kurabu \club"i. The ontology entries for
nouns have been hand checked and corrected,
including adding hypernyms for words where
39
the genus term in the denition was very gen-
eral, e.g \a word used to refer insultingly to
men" where man is a more useful hypernym
than word for the dened term yarou.
4 Evaluation
We evaluated the performance of the RMRS
comparison method in two tasks. First it was
used to indicate whether two sentences were
possible paraphrases. In the second task, we
used the comparison scores to select the most
likely sentence to contain the answer to a ques-
tion.
4.1 Paraphrasing
In this task we compared denitions sen-
tences for the same head word from two dier-
ent Japanese dictionaries - the Lexeed dictio-
nary (x3.2.1) and the Iwanami Kokugo Jiten
(Iwanami: Nishio et al, 1994), the Japanese
dictionary used in the SENSEVAL-2 Japanese
lexical task (Shirai, 2002).
There are 60,321 headwords and 85,870
word senses in Iwanami. Each sense in the
dictionary consists of a sense ID and morpho-
logical information (word segmentation, POS
tag, base form and reading, all manually post-
edited).
The denitions in Lexeed and Iwanami were
linked by headword and three Japanese native
speakers assessed each potential pair of sense
denitions for the same head word to judge
which denitions were describing the same
sense. This annotation not only described
which sense from each dictionary matched, but
also whether the denitions were equal, equiv-
alent, or subsuming.
The examples (1) and (2) are the denitions
of sense 2 of ),'* doraiba \driver" from
Lexeed and Iwanami respectively. They were
judged to be equivalent denitions by all three
annotators.
4.1.1 Method
Test sets were built consisting of the Lexeed
and Iwanami denition pairs that had been an-
notated in the gold standard to be either non-
matching, equal or equivalent. Leaving out
those pairs annotated as having a subsump-
tion relation made it a clearer task judging
between paraphrase or not, rather than ex-
amining partial meaning overlap. Ten sets of
5,845 denition pairs were created, with each
set being equally split between matching and
non-matching pairs. This gives data that is to
some extent semantically equivalent (the same
word sense is being dened), but with no guar-
antee of syntactic equivalence.
Comparisons were made between the rst
sentence of each denition with both a Bag-
of-Words comparison method and our RMRS
based method. If RMRS output was not avail-
able from Jacy (due to a failed parse), RMRS
from CaboCha was used as a fall back shallow
parse result.
Scores were output and then the best
threshold score for each method was calculated
on one of the 10 sets. Using the calculated
threshold score, pairs were classied as either
matching or non-matching. Pairs classied as
matching were evaluated as correct if the gold
standard annotation was either equal or equiv-
alent.
4.1.2 Results
The Bag-of-Words comparison got an av-
erage accuracy over all sets of 73.9% with
100% coverage. A break down of the results
shows that this method was more accurate
(78%) in correctly classifying non-matches
than matches (70%). This is to be expected
since it won't pick up equivalences where a
word has been changed for its synonym.
The RMRS comparison had an accuracy
was 78.4% with almost 100% coverage, an im-
provement over the Bag-of-Words. The RMRS
based method was also more accurate over
non matches (79.9%) than matches (76.6%),
although the dierence is not as large. Con-
sidering only those sentences with a parse from
JACY gave an accuracy of 81.1% but with a
coverage of only 46.1%. This shows that deep
parsing improves precision, but must be used
in conjunction with a shallower fallback.
To explore what eect the ontology was hav-
ing on the results, another evaluation was per-
formed without the ontology matching. This
had an accuracy of 77.3% (78.1% using Jacy,
46.1% coverage). This shows that the infor-
mation available in the ontology denitely im-
proves scores, but that even without that sort
of deep lexical resource, the RMRS matching
can still improve on Bag-of-Words using just
surface form abstraction and argument match-
ing.
40
4.2 Answer Sentence Selection
To emulate a part of the question answering
pipeline, we used a freely available set of 2000
Japanese questions, annotated with, among
other things, answer and answer document ID
(Sekine et al, 2002). The document IDs for
the answer containing documents refer to the
Mainichi Newspaper 1995 corpus which has
been used as part of the document collection
for NTCIR's Question Answering Challenges.
The documents range in length from 2 to 83
sentences.
4.2.1 Method
For every question, we compared it to each
sentence in the answer document. The sen-
tence that has the best similarity to the ques-
tion is returned as the most likely to con-
tain the answer. For this sort of compari-
son, an entails option was added that changes
the similarity scoring method slightly so that
only non-matches in the rst sentence increase
the score. The rationale being that in Ques-
tion Answering (and also in entailment), ev-
erything present in the question (or hypoth-
esis) should be matched by something in the
answer, but having extra, unmatched informa-
tion in the answer should not be penalised.
The task is evaluated by checking if the an-
swer does exist in the sentence selected. This
means that more than one sentence can be the
correct answer for any question (if the answer
is mentioned multiple times in the article).
4.2.2 Results
The Bag-of-Words comparison correctly
found a sentence containing the answer for
62.5% of the 2000 questions. The RMRS com-
parison method gave a small improvement,
with a result of 64.3%. Examining the data
showed this to be much harder than the para-
phrase task because of the language level in-
volved. In the paraphrasing task, the sen-
tences averaged around 10 predicates each,
while the questions and sentences in this task
averaged over 3 times longer, with about 34
predicates. The words used were also less
likely to be in the lexical resources both be-
cause more formal, less familiar words were
used, and also because of the preponderance
of named entities. Adding name lists of peo-
ple, places and organisations would greatly im-
prove the matching in this instance.
5 Future Directions
5.1 Applications
Since the comparison method was written
to be language independent, the next stage
of evaluation would be to use it in a non-
Japanese task. The PASCAL Recognising
Textual Entailment (RTE) Challenge (Dagan
et al, 2005) is one recent English task where
participants used sentence comparison exten-
sively. While the task appears to call for in-
ference and reasoning, the top 5 participat-
ing groups used statistical methods and word
overlap only. Vanderwende et al (2005) did a
manual evaluation of the test data and found
that 37% could be decided on syntactic infor-
mation alone, while adding a thesaurus could
increase that coverage to 49%. This means
that RMRS comparison has the potential to
perform well. Not only does it improve on
basic word overlap, but it allows for easy ad-
dition of a thesaurus or dictionary. Further,
because of the detailed match output avail-
able, the method could be extended in post
processing to encompass some basic inference
methods.
Aside from comparing sentences, the RMRS
comparison can be used to compare the RMRS
output of dierent tools for the same sentence
so that the compatibility of the outputs can
be evaluated and improved.
5.2 Extensions
One immediate future improvement planned
is to add named entity lists to the lexical re-
sources so that names of people and places
could be looked up. This would allow partial
matches between, e.g., Clinton is a hyponym
of person, which would be particularly useful
for Question Answering.
Another idea is to add a bilingual dictio-
nary and try cross-lingual comparisons. As
the RMRS abstracts away much of the surface
specic details, this might be useful for sen-
tence alignment.
To go beyond sentence by sentence compar-
ison, we have plans to implement a method
for multi-sentence comparisons by either com-
bining the RMRS structures before compari-
son, or post-processing the sentence compari-
son outputs. This could be particularly inter-
esting for text summarization.
41
6 Conclusions
Deep parsing information is useful for com-
paring sentences and RMRS gives us a use-
ful framework for utilising this information
when it is available. Our RMRS compari-
son was more accurate then basic word over-
lap similarity measurement particularly in the
paraphrase task where synonyms were of-
ten used. Even when the ontology was not
used, abstracting away from surface form, and
matching arguments did give an improvement.
Falling back to shallow parse methods in-
creases the robustness which is often an issue
for tools that use deep processing, while still
allowing the use of the most accurate informa-
tion available.
The comparison method is language agnos-
tic and can be used for any language that has
RMRS generating tools. The output is much
more informative than Bag-of-Words, mak-
ing it useful in many applications that need
to know exactly how a sentence matched or
aligned.
Acknowledgements
This work was started when the rst author was a vis-
itor at the NTT Communication Science Laboratories,
Nippon Telegraph and Telephone Corporation. The
rst author was also supported by the Pam Todd schol-
arship from St Hilda's College. We would like to thank
the NTT Natural Language Research Group and two
anonymous reviewers for their valuable input.
References
Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI Publications.
Ido Dagan, Oren Glickman, and Bernado Magnini.
2005. The PASCAL recognising textual entailment
challenge. In Proceedings of the PASCAL Challenges
Workshop on Recognising Textual Entailment.
Christine Fellbaum. 1998. A semantic network of En-
glish verbs. In Christine Fellbaum, editor, WordNet:
An Electronic Lexical Database, chapter 3, pages 70{
104. MIT Press.
Anette Frank. 2004. Constraint-based RMRS con-
struction from shallow grammars. In 20th Inter-
national Conference on Computational Linguistics:
COLING-2004, pages 1269{1272. Geneva.
Tsutomu Hirao, Jun Suzuki, Hideki Isozaki, and
Eisaku Maeda. 2004. Dependency-based sentence
alignment for multiple document summarization. In
Proceedings of the COLING.
Eduard Hovy, Junichi Fukumoto, Chin-Yew Lin, and
Liang Zhao. 2005a. Basic elements. (http://www.
isi.edu/

cyl/BE).
Eduard Hovy, Chin-Yew Lin, and Liang Zhao. 2005b.
A BE-based multi-document summarizer with sen-
tence compression. In Proceedings of Multilingual
Summarization Evaluation.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai,
Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura,
Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997.
Goi-Taikei | A Japanese Lexicon. Iwanami Shoten,
Tokyo. 5 volumes/CDROM.
Kaname Kasahara, Hiroshi Sato, Francis Bond,
Takaaki Tanaka, Sanae Fujita, Tomoko Kanasugi,
and Shigeaki Amano. 2004. Construction of a
Japanese semantic lexicon: Lexeed. SIG NLC-159,
IPSJ, Tokyo. (in Japanese).
Paul Kingsbury, Martha Palmer, and Mitch Marcus.
2002. Adding semantic annotation to the penn tree-
bank. In Proceedings of the Human Language Tech-
nology 2002 Conference.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
CoNLL 2002: Proceedings of the 6th Conference
on Natural Language Learning 2002 (COLING 2002
Post-Conference Workshops), pages 63{69. Taipei.
Yuji Matsumoto, Kitauchi, Yamashita, Hirano, Mat-
suda, and Asahara. 2000. Nihongo Keitaiso Kaiseki
System: Chasen, version 2.2.1 manual edition.
http://chasen.aist-nara.ac.jp.
Eric Nichols, Francis Bond, and Daniel Flickinger.
2005. Robust ontology acquisition from machine-
readable dictionaries. In Proceedings of the Inter-
national Joint Conference on Articial Intelligence
IJCAI-2005, pages 1111{1116. Edinburgh.
Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizu-
tani. 1994. Iwanami Kokugo Jiten Dai Go Han
[Iwanami Japanese Dictionary Edition 5]. Iwanami
Shoten, Tokyo. (in Japanese).
Anna Ritchie. 2004. Compatible RMRS representa-
tions from RASP and the ERG. Technical Report
UCAM-CL-TR-661.
Satoshi Sekine, Kiyoshi Sudo, Yusuke Shinyama,
Chikashi Nobata, Kiyotaka Uchimoto, and Hitoshi
Isahara. 2002. NYU/CRL QA system, QAC ques-
tion analysis and CRL QA data. In Working Notes
of NTCIR Workshop 3.
Kiyoaki Shirai. 2002. Construction of a word sense
tagged corpus for SENSEVAL-2 Japanese dictio-
nary task. In Third International Conference on
Language Resources and Evaluation (LREC-2002),
pages 605{608.
Melanie Siegel and Emily M. Bender. 2002. E-
cient deep processing of Japanese. In Proceedings
of the 3rd Workshop on Asian Language Resources
and International Standardization at the 19th Inter-
national Conference on Computational Linguistics.
Taipei.
Lucy Vanderwende, Deborah Coughlin, and Bill Dolan.
2005. What syntax can contribute in entailment
task. In Proceedings of the PASCAL Challenges
Workshop on Recognising Textual Entailment.
42
