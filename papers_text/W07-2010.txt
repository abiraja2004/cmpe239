Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 54?58,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 11: English Lexical Sample Task
via English-Chinese Parallel Text
Hwee Tou Ng and Yee Seng Chan
Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
{nght, chanys}@comp.nus.edu.sg
Abstract
We made use of parallel texts to gather train-
ing and test examples for the English lexi-
cal sample task. Two tracks were organized
for our task. The first track used examples
gathered from an LDC corpus, while the
second track used examples gathered from
a Web corpus. In this paper, we describe
the process of gathering examples from the
parallel corpora, the differences with similar
tasks in previous SENSEVAL evaluations,
and present the results of participating sys-
tems.
1 Introduction
As part of the SemEval-2007 evaluation exercise, we
organized an English lexical sample task for word
sense disambiguation (WSD), where the sense-
annotated examples were semi-automatically gath-
ered from word-aligned English-Chinese parallel
texts. Two tracks were organized for this task, each
gathering data from a different corpus. In this paper,
we describe our motivation for organizing the task,
our task framework, and the results of participants.
Past research has shown that supervised learning
is one of the most successful approaches to WSD.
However, this approach involves the collection of
a large text corpus in which each ambiguous word
has been annotated with the correct sense to serve as
training data. Due to the expensive annotation pro-
cess, only a handful of manually sense-tagged cor-
pora are available.
An effort to alleviate the training data bottle-
neck is the Open Mind Word Expert (OMWE)
project (Chklovski and Mihalcea, 2002) to collect
sense-tagged data from Internet users. Data gath-
ered through the OMWE project were used in the
SENSEVAL-3 English lexical sample task. In that
task, WordNet-1.7.1 was used as the sense inven-
tory for nouns and adjectives, while Wordsmyth1
was used as the sense inventory for verbs.
Another source of potential training data is par-
allel texts. Our past research in (Ng et al, 2003;
Chan and Ng, 2005) has shown that examples gath-
ered from parallel texts are useful for WSD. Briefly,
after manually assigning appropriate Chinese trans-
lations to each sense of an English word, the English
side of a word-aligned parallel text can then serve as
the training data, as they are considered to have been
disambiguated and ?sense-tagged? by the appropri-
ate Chinese translations.
Using the above approach, we gathered the train-
ing and test examples for our task from parallel texts.
Note that our examples are collected without manu-
ally annotating each individual ambiguous word oc-
currence, allowing us to gather our examples in a
much shorter time. This contrasts with the setting of
the English lexical sample task in previous SENSE-
VAL evaluations. In the English lexical sample task
of SENSEVAL-2, the sense tagged data were cre-
ated through manual annotation by trained lexicog-
raphers. In SENSEVAL-3, the data were gathered
through manual sense annotation by Internet users.
In the next section, we describe in more detail
the process of gathering examples from parallel texts
and the two different parallel corpora we used. We
then give a brief description of each of the partici-
1http://www.wordsmyth.net
54
pating systems. In Section 4, we present the results
obtained by the participants, before concluding in
Section 5.
2 Gathering Examples from Parallel
Corpora
To gather examples from parallel corpora, we fol-
lowed the approach in (Ng et al, 2003). Briefly, af-
ter ensuring the corpora were sentence-aligned, we
tokenized the English texts and performed word seg-
mentation on the Chinese texts (Low et al, 2005).
We then made use of the GIZA++ software (Och and
Ney, 2000) to perform word alignment on the paral-
lel corpora. Then, we assigned some possible Chi-
nese translations to each sense of an English word
w. From the word alignment output of GIZA++, we
selected those occurrences of w which were aligned
to one of the Chinese translations chosen. The En-
glish side of these occurrences served as training
data for w, as they were considered to have been dis-
ambiguated and ?sense-tagged? by the appropriate
Chinese translations. The English half of the par-
allel texts (each ambiguous English word and its 3-
sentence context) were used as the training and test
material to set up our English lexical sample task.
Note that in our approach, the sense distinction
is decided by the different Chinese translations as-
signed to each sense of a word. This is thus
similar to the multilingual lexical sample task in
SENSEVAL-3 (Chklovski et al, 2004), except that
our training and test examples are collected with-
out manually annotating each individual ambiguous
word occurrence. The average time needed to assign
Chinese translations for one noun and one adjective
is 20 minutes and 25 minutes respectively. This is
a relatively short time, compared to the effort other-
wise needed to manually sense annotate individual
word occurrences. Also, once the Chinese transla-
tions are assigned, more examples can be automat-
ically gathered as more parallel texts become avail-
able.
We note that frequently occurring words are usu-
ally highly polysemous and hard to disambiguate.
To maximize the benefits of our work, we gathered
training data from parallel texts for a set of most fre-
quently occurring noun and adjective types in the
Brown Corpus. Also, similar to the SENSEVAL-3
Dataset Avg. no. Avg. no. of examples
of senses Training Test
LDC noun 5.2 197.6 98.5
LDC adjective 3.9 125.6 62.9
Web noun 3.5 182.0 91.3
Web adjective 2.8 88.8 44.6
Table 1: Average number of senses, training exam-
ples, and test examples per word.
English lexical sample task, we used WordNet-1.7.1
as our sense inventory.
2.1 LDC Corpus
We have two tracks for this task, each track using a
different corpus. The first corpus is the Chinese En-
glish News Magazine Parallel Text (LDC2005T10),
which is an English-Chinese parallel corpus avail-
able from the Linguistic Data Consortium (LDC).
From this parallel corpus, we gathered examples
for 50 English words (25 nouns and 25 adjectives)
using the method described above. From the gath-
ered examples of each word, we randomly selected
training and test examples, where the number of
training examples is about twice the number of test
examples.
The rows LDC noun and LDC adjective in Table
1 give some statistics about the examples. For in-
stance, each noun has an average of 197.6 training
and 98.5 test examples and these examples repre-
sent an average of 5.2 senses per noun.2 Participants
taking part in this track need to have access to this
LDC corpus in order to access the training and test
material in this track.
2.2 Web Corpus
Since not all interested participants may have access
to the LDC corpus described in the previous sub-
section, the second track of this task makes use of
English-Chinese documents gathered from the URL
pairs given by the STRAND Bilingual Databases.3
STRAND (Resnik and Smith, 2003) is a system that
acquires document pairs in parallel translation auto-
matically from the Web. Using this corpus, we gath-
ered examples for 40 English words (20 nouns and
2Only senses present in the examples are counted.
3http://www.umiacs.umd.edu/?resnik/strand
55
20 adjectives).
The rows Web noun and Web adjective in Table 1
show that we selected an average of 182.0 training
and 91.3 test examples for each noun and these ex-
amples represent an average of 3.5 senses per noun.
We note that the average number of senses per word
for the Web corpus is slightly lower than that of the
LDC corpus.
2.3 Annotation Accuracy
To measure the annotation accuracy of examples
gathered from the LDC corpus, we examined a ran-
dom selection of 100 examples each from 5 nouns
and 5 adjectives. From these 1,000 examples, we
measured a sense annotation accuracy of 84.7%.
These 10 words have an average of 8.6 senses per
word in the WordNet-1.7.1 sense inventory. As de-
scribed in (Ng et al, 2003), when several senses
of an English word are translated by the same Chi-
nese word, we can collapse these senses to obtain a
coarser-grained, lumped sense inventory. If we do
this and measure the sense annotation accuracy with
respect to a coarser-grained, lumped sense inventory,
these 10 words will have an average of 6.5 senses per
word and an annotation accuracy of 94.7%.
For the Web corpus, we similarly examined a ran-
dom selection of 100 examples each from 5 nouns
and 5 adjectives. These 10 words have an average of
6.5 senses per word in WordNet-1.7.1 and the 1,000
examples have an average sense annotation accuracy
of 85.0%. After sense collapsing, annotation ac-
curacy is 95.3% with an average of 4.8 senses per
word.
2.4 Training and Test Data from Different
Documents
In our previous work (Ng et al, 2003), we conducted
experiments on the nouns of SENSEVAL-2 English
lexical sample task. We found that there were cases
where the same document contributed both training
and test examples and this inflated the WSD accu-
racy figures. To avoid this, during our preparation
of the LDC and Web data, we made sure that a doc-
ument contributed only either training or test exam-
ples, but not both.
3 Participating Systems
Three teams participated in the Web corpus track
of our task, with each team employing one system.
There were no participants in the LDC corpus track,
possibly due to the licensing issues involved. All
participating systems employed supervised learning
and only used the training examples provided by us.
3.1 CITYU-HIF
The CITYU-HIF team from the City University of
Hong Kong trained a naive Bayes (NB) classifier
for each target word to be disambiguated, using
knowledge sources such as parts-of-speech (POS) of
neighboring words and single words in the surround-
ing context. They also experimented with using dif-
ferent sets of features for each target word.
3.2 HIT-IR-WSD
The system submitted by the HIT-IR-WSD team
from Harbin Institute of Technology used Support
Vector Machines (SVM) with a linear kernel func-
tion as the learning algorithm. Knowledge sources
used included POS of surrounding words, local col-
locations, single words in the surrounding context,
and syntactic relations.
3.3 PKU
The system submitted by the PKU team from Peking
University used a combination of SVM and maxi-
mum entropy classifiers. Knowledge sources used
included POS of surrounding words, local colloca-
tions, and single words in the surrounding context.
Feature selection was done by ignoring word fea-
tures with certain associated POS tags and by se-
lecting the subset of features based on their entropy
values.
4 Results
As all participating systems gave only one answer
for each test example, recall equals precision and
we will only report micro-average recall on the Web
corpus track in this section.
Table 2 gives the overall results obtained by each
of the systems when evaluated on all the test exam-
ples of the Web corpus. We note that all the par-
ticipants obtained scores which exceed the baseline
heuristic of tagging all test examples with the most
56
System ID Contact author Learning algorithm Score
HIT-IR-WSD Yuhang Guo, <astronaut@ir.hit.edu.cn> SVM 0.819
PKU Peng Jin, <jandp@pku.edu.cn> SVM and maximum entropy 0.815
CITYU-HIF Oi Yee Kwong, <rlolivia@cityu.edu.hk> NB 0.753
MFS ? Most frequent sense baseline 0.689
Table 2: Overall micro-average scores of the participants and the most frequent sense (MFS) baseline.
Noun MFS CITYU-HIF HIT-IR-WSD PKU
age 0.486 0.643 0.743 0.700
area 0.480 0.693 0.773 0.773
body 0.872 0.897 0.910 0.923
change 0.411 0.400 0.578 0.611
director 0.580 0.890 0.960 0.960
experience 0.830 0.830 0.880 0.840
future 0.889 0.889 0.990 0.990
interest 0.308 0.165 0.813 0.780
issue 0.651 0.711 0.892 0.855
life 0.820 0.830 0.860 0.740
material 0.719 0.719 0.781 0.641
need 0.907 0.907 0.918 0.918
performance 0.410 0.570 0.690 0.700
program 0.590 0.590 0.730 0.690
report 0.870 0.840 0.880 0.870
system 0.510 0.700 0.610 0.730
time 0.455 0.673 0.733 0.693
today 0.800 0.750 0.800 0.780
water 0.882 0.921 0.868 0.895
work 0.644 0.743 0.842 0.891
Micro-avg 0.656 0.719 0.813 0.802
Table 3: Micro-average scores of the most frequent
sense baseline and the various participants on each
noun.
frequent sense (MFS) in the training data. This sug-
gests that the Chinese translations assigned to senses
of the ambiguous words are appropriate and provide
sense distinctions which are clear enough for effec-
tive classifiers to be learned.
In Table 3 and Table 4, we show the scores ob-
tained by each system on each of the 20 nouns and
20 adjectives. For comparison purposes, we also
show the corresponding MFS score of each word.
Paired t-test on the results of the top two systems
show no significant difference between them.
5 Conclusion
We organized an English lexical sample task using
examples gathered from parallel texts. Unlike the
English lexical task of previous SENSEVAL evalua-
tions where each example is manually annotated, we
Adjective MFS CITYU-HIF HIT-IR-WSD PKU
ancient 0.778 0.667 0.778 0.741
bad 0.857 0.857 0.905 0.905
common 0.533 0.567 0.533 0.633
early 0.769 0.846 0.769 0.769
educational 0.911 0.911 0.911 0.911
free 0.760 0.792 0.854 0.917
high 0.630 0.926 0.815 0.852
human 0.872 0.987 0.962 0.962
little 0.450 0.750 0.650 0.650
long 0.667 0.690 0.786 0.714
major 0.870 0.902 0.880 0.913
medical 0.738 0.787 0.800 0.725
national 0.267 0.467 0.667 0.700
new 0.441 0.441 0.529 0.559
present 0.875 0.917 0.875 0.875
rare 0.727 0.818 0.727 0.909
serious 0.879 0.879 0.879 0.879
simple 0.795 0.818 0.864 0.864
small 0.714 0.929 0.893 0.929
third 0.888 0.988 0.963 0.963
Micro-avg 0.757 0.823 0.831 0.842
Table 4: Micro-average scores of the most frequent
sense baseline and the various participants on each
adjective.
only need to assign appropriate Chinese translations
to each sense of a word. Once this is done, we auto-
matically gather training and test examples from the
parallel texts. All the participating systems of our
task obtain results that are significantly better than
the most frequent sense baseline.
6 Acknowledgements
Yee Seng Chan is supported by a Singapore Millen-
nium Foundation Scholarship (ref no. SMF-2004-
1076).
References
Yee Seng Chan and Hwee Tou Ng. 2005. Scaling
up word sense disambiguation via parallel texts. In
Proceedings of AAAI05, pages 1037?1042, Pittsburgh,
Pennsylvania, USA.
57
Timothy Chklovski and Rada Mihalcea. 2002. Building
a sense tagged corpus with Open Mind Word Expert.
In Proceedings of ACL02 Workshop on Word Sense
Disambiguation: Recent Successes and Future Direc-
tions, pages 116?122, Philadelphia, USA.
Timothy Chklovski, Rada Mihalcea, Ted Pedersen, and
Amruta Purandare. 2004. The SENSEVAL-3 multi-
lingual English-Hindi lexical sample task. In Proceed-
ings of SENSEVAL-3, pages 5?8, Barcelona, Spain.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A
maximum entropy approach to Chinese word segmen-
tation. In Proceedings of the Fourth SIGHAN Work-
shop on Chinese Language Processing, pages 161?
164, Jeju Island, Korea.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Ex-
ploiting parallel texts for word sense disambiguation:
An empirical study. In Proceedings of ACL03, pages
455?462, Sapporo, Japan.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceeedings of ACL00,
pages 440?447, Hong Kong.
Philip Resnik and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380.
58
