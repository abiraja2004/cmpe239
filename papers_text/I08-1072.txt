Context Feature Selection for Distributional Similarity
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko Toyama
Graduate School of Information Science,
Nagoya University
Furo-cho, Chikusa-ku, Nagoya, JAPAN 464-8603
{hagiwara, yasuhiro, toyama}@kl.i.is.nagoya-u.ac.jp
Abstract
Distributional similarity is a widely used
concept to capture the semantic relatedness
of words in various NLP tasks. However, ac-
curate similarity calculation requires a large
number of contexts, which leads to imprac-
tically high computational complexity. To
alleviate the problem, we have investigated
the effectiveness of automatic context selec-
tion by applying feature selection methods
explored mainly for text categorization. Our
experiments on synonym acquisition have
shown that while keeping or sometimes in-
creasing the performance, we can drastically
reduce the unique contexts up to 10% of the
original size. We have also extended the
measures so that they cover context cate-
gories. The result shows a considerable cor-
relation between the measures and the per-
formance, enabling the automatic selection
of effective context categories for distribu-
tional similarity.
1 Introduction
Semantic similarity of words is one of the most im-
portant lexical knowledge for NLP tasks including
word sense disambiguation and synonym acquisi-
tion. To measure the semantic relatedness of words,
a concept called distributional similarity has been
widely used. Distributional similarity represents the
relatedness of two words by the commonality of
contexts the words share, based on the distributional
hypothesis (Harris, 1985), which states that seman-
tically similar words share similar contexts.
A wide range of contextual information, such
as surrounding words (Lowe and McDonald, 2000;
Curran and Moens, 2002a), dependency or case
structure (Hindle, 1990; Ruge, 1997; Lin, 1998),
and dependency path (Lin and Pantel, 2001; Pado
and Lapata, 2007), has been utilized for similar-
ity calculation, and achieved considerable success.
However, a major problem which arises when adopt-
ing distributional similarity is that it easily yields a
huge amount of unique contexts. This can lead to
high dimensionality of context space, often up to the
order of tens or hundreds of thousands, which makes
the calculation computationally impractical. Be-
cause not all of the contexts are useful, it is strongly
required for the efficiency to eliminate the unwanted
contexts to ease the expensive cost.
To tackle this issue, Curran and Moens (2002b)
suggest assigning an index vector of canonical at-
tributes, i.e., a small number of representative el-
ements extracted from the original vector, to each
word. When the comparison is performed, canonical
attributes of two target words are firstly consulted,
and the original vectors are referred to only if the
attributes have a match between them. However, it
is not clear whether the condition for canonical at-
tributes they adopted, i.e., that the attributes must be
the most weighted subject, direct object, or indirect
object, is optimal in terms of the performance.
There are also some existing studies which paid
attention to the comparison of context categories
for synonym acquisition (Curran and Moens, 2002a;
Hagiwara et al, 2006). However, they have con-
ducted only a posteriori comparison based on perfor-
mance evaluation, and we are afraid that these find-
553
ings are somewhat limited to their own experimental
settings which may not be applicable to completely
new settings, e.g., one with a new set of contexts
extracted from different sources. Therefore, general
quantitative measures which can be used for reduc-
tion and selection of any kind of contexts and con-
text categories are strongly required.
Shifting our attention from word similarity to
other areas, a great deal of studies on feature selec-
tion has been conducted in the literature, especially
for text categorization (Yang and Pedersen, 1997)
and gene expression classification (Ding and Peng,
2003). Whereas these methods have been successful
in reducing feature size while keeping classification
performance, the problem of distributional similar-
ity is radically different from that of classification,
and whether the same methods are applicable and
effective for automatic context selection in the simi-
larity problem is yet to be investigated.
In this paper, we firstly introduce existing quan-
titative methods for feature selection, namely, DF,
TS, MI, IG, CHI2, and show how to apply them to
the distributional similarity problem to measure the
context importance. We then extracted dependency
relations as context from the corpus, and conducted
automatic synonym acquisition experiments to eval-
uate the context selection performance, reducing the
unimportant contexts based on the feature selection
methods. Finally we extend the context importance
to cover context categories (RASP2 grammatical re-
lations), and show that the above methods are also
effective in selecting categories.
This paper is organized as follows: in Section
2, five existing context selection methods are in-
troduced, and how to apply classification-based se-
lection methods to distributional similarity is de-
scribed. In Section 3 and 4, the synonym acquisition
method and evaluation measures, AP and CC, em-
ployed in the evaluation experiments are detailed.
Section 5 includes two main experiments and their
results: context reduction and context category se-
lection, along with experimental settings and discus-
sions. Section 6 concludes this paper.
2 Context Selection Methods
In this section, context selection methods proposed
for text categorization or information retrieval are
introduced. In the following, n and m represent
the number of unique words and unique contexts,
respectively, and N(w, c) denotes the number of co-
occurrence of word w and context c.
2.1 Document Frequency (DF)
Document frequency (DF), commonly used for
weighting in information retrieval, is the number of
documents a term co-occur with. However, in the
distributional similarity settings, DF corresponds to
word frequency, i.e., the number of unique words the
context co-occurs with:
df(c) = |{w|N(w, c) > 0}|.
The motivation of adopting DF as a context selection
criterion is the assumption that the contexts shared
by many words should be informative. It is to note,
however, that the contexts with too high DF are not
always useful, since there are some exceptions in-
cluding so-called stopwords.
2.2 Term Strength (TS)
Term strength (TS), proposed by Wilbur and
Sirotkin (1992) and applied to text categorization
by Yang and Wilbur (1996), measures how likely a
term is to appear in ?similar documents,? and it is
shown to achieve a successful outcome in reducing
the amount of vocabulary for text retrieval. For dis-
tributional similarity, TS is defined as:
s(c) = P (c ? C(w2)|c ? C(w1)),
where (w1, w2) is a related word pair and C(w) is
a set of contexts co-occurring with the word w, i.e.,
C(w) = {c|N(w, c) > 0}. s(c) is calculated, let-
ting PH be a set of related word pairs, as
s(c) = |{(w1, w2) ? PH |c ? C(w1) ? C(w2)}|
|{(w1, w2) ? PH |c ? C(w1)}|
.
What makes TS different from DF is that it re-
quires a training set PH consisting of related word
pairs. We used the test set for class s = 1 as PH
described in the next section.
2.3 Formalization of Distributional Similarity
The following methods, MI, IG, and CHI2, are rad-
ically different from the above ones, in that they are
554
designed essentially for ?class classification? prob-
lems. Thus we formalize distributional similarity as
a classification problem as described below.
First of all, we deal with word pairs, instead of
words, as the targets of classification, and define fea-
tures f1, ..., fm corresponding to contexts c1, ..., cm,
for each pair. The feature fj = 1 if the two words of
the pair has the context cj in common, and fj = 0
otherwise. Then, we define target class s, so that
s = 1 when the pair is semantically related, and
s = 0 if not. These defined, distributional similar-
ity is formalized as a binary classification problem
which assigns the word pairs to the class s ? {0, 1}
based on the features c1, ..., cm. Finally, to calcu-
late the specific values of the following feature im-
portance measures, we prepare two test sets of re-
lated word pairs for class s = 1 and unrelated ones
for class s = 0. This enables us to apply existing
feature selection methods designed for classification
problems to the automatic context selection.
The two test sets, related and unrelated one, are
prepared using the reference sets described in Sec-
tion 4. More specifically, we created 5,000 related
word pairs by extracting from synonym pairs in the
reference set, and 5,000 unrelated ones by firstly cre-
ating random pairs of LDV, whose detail is described
later, and then manually making sure that no related
pairs are included in these random pairs.
2.4 Mutual Information (MI)
Mutual information (MI), commonly used for word
association and co-occurrence weighing in statisti-
cal NLP, is the measure of the degree of dependence
between two events. The pointwise MI value of fea-
ture f and class s is calculated as:
I(f, s) = log P (f, s)
P (f)P (s)
.
To obtain the final context importance, we combine
the MI value over both of the classes as Imax(cj) =
maxs?{0,1} I(fj , s). Note that, here we employed
the maximum value of pointwise MI values since
it is claimed to be the best in (Yang and Peder-
sen, 1997), although there can be other combination
ways such as weighted average.
2.5 Information Gain (IG)
Information gain (IG), often employed in the ma-
chine learning field as a criterion for feature impor-
tance, is the amount of gained information of an
event by knowing the outcome of the other event,
and is calculated as the weighted sum of the point-
wise MI values over all the event combinations:
G(cj) =
?
fj?{0,1}
?
s?{0,1}
P (fj , s) log
P (fj , s)
P (fj)P (s)
.
2.6 ?2 Statistic (CHI2)
?2 statistic (CHI2) estimates the lack of indepen-
dence between classes and features, which is equal
to the summed difference of observed and expected
frequency over the contingency table cells. More
specifically, letting F jnm(n,m ? {0, 1}) be the num-
ber of word pairs with fj = n and s = m, and the
number of all pairs be N , ?2 statistic is defined as:
?2(cj)
= N(F11F00 ? F01F10)
(F11 + F01)(F10 + F00)(F11 + F10)(F01 + F00)
.
3 Synonym Acquisition Method
This section describes the synonym acquisition
method, a major and important application of distri-
butional similarity, which we employed for the eval-
uation of automatic context selection. Here we men-
tion how to extract the original contexts from cor-
pora in detail, as well as the calculation of weight
and similarity between words.
3.1 Context Extraction
We adopted dependency structure as the context of
words since it is the most widely used and well-
performing contextual information in the past stud-
ies (Ruge, 1997; Lin, 1998). As the extraction of ac-
curate and comprehensive dependency structure is in
itself a difficult task, the sophisticated parser RASP
Toolkit 2 (Briscoe et al, 2006) was utilized to ex-
tract this kind of word relations. Take the following
sentence for example:
Shipments have been relatively level since January,
the Commerce Department noted.
555
RASP outputs the extracted dependency structure
as n-ary relations as follows, which are called gram-
matical relations. Annotations regarding suffix, part
of speech tags, offsets for individual words are omit-
ted for simplicity.
(ncsubj be Shipment _)
(aux be have)
(xcomp _ be level)
(ncmod _ be relatively)
(ccomp _ level note)
(ncmod _ note since)
(ncsubj note Department _)
(det Department the)
(ncmod _ Department Commerce)
(dobj since January)
While the RASP outputs are n-ary relations in
general, what we need here is co-occurrences of
words and contexts, so we extract the set of co-
occurrences of stemmed words and contexts by tak-
ing out the target word from the relation and replac-
ing the slot by an asterisk ?*?:
(words) - (contexts)
Shipment - ncsubj:be:*_
have - aux:be:*
be - ncsubj:*:Shipment:_
be - aux:*:have
be - xcomp:_:*:level
be - ncmod:_:*:relatively
relatively - ncmod:_:be:*
level - xcomp:_:be:*
level - ccomp:_:*:note
...
Summing all these up produces the raw co-
occurrence count N(w, c) of word w and context c.
3.2 Similarity Calculation
Although it is possible to use the raw count acquired
above for the similarity calculation, directly using
the raw count may cause performance degradation,
thus we need an appropriate weighting measure. In
response to the preliminary experiment results, we
employed pointwise mutual information as weight:
wgt(w, c) = log P (w, c)
P (w)P (c)
Here we made a small modification to bind the
weight to non-negative such that wgt(w, c) ? 0,
because negative weight values sometimes worsen
the performance (Curran and Moens, 2002b). The
weighting by PMI is applied after the pre-processing
including frequency cutoff and context selection.
As for the similarity measure, we used Jaccard co-
efficient, which is widely adopted to capture overlap
proportion of two sets:
?
c?C(w1)?C(w2) min(wgt(w1, c),wgt(w2, c))
?
c?C(w1)?C(w2) max(wgt(w1, c),wgt(w2, c))
.
4 Evaluation Measures
This section describes the two evaluation methods
we employed ? average precision (AP) and corre-
lation coefficient (CC).
4.1 Average Precision (AP)
The first evaluation measure, average precision
(AP), is a common evaluation scheme for informa-
tion retrieval, which evaluates how accurately the
methods are able to extract synonyms. We first pre-
pare a set of query words, for which synonyms are
obtained to evaluate the precision. We adopted the
Longman Defining Vocabulary (LDV) 1 as the can-
didate set of query words. For each word in LDV,
three existing thesauri are consulted: Roget?s The-
saurus (Roget, 1995), Collins COBUILD Thesaurus
(Collins, 2002), and WordNet (Fellbaum, 1998).
The union of synonyms obtained when the LDV
word is looked up as a noun is used as the refer-
ence set, except for words marked as ?idiom,? ?in-
formal,? ?slang? and phrases comprised of two or
more words. The LDV words for which no noun
synonyms are found in any of the reference thesauri
are omitted. From the remaining 771 LDV words,
100 query words are randomly extracted, and for
each of them the eleven precision values at 0%, 10%,
..., and 100% recall levels are averaged to calculate
the final AP value.
4.2 Correlation Coefficient (CC)
The second evaluation measure is correlation coef-
ficient (CC) between the target similarity and the
reference similarity, i.e., the answer value of sim-
ilarity for word pairs. The reference similarity is
calculated based on the closeness of two words in
the tree structure of WordNet. More specifically, the
similarity between word w with senses w1, ..., wm1
and word v with senses v1, ..., vm2 is obtained as fol-
lows. Let the depth of node wi and vj be di and dj ,
1http://www.cs.utexas.edu/users/kbarker/working notes/
ldoce-vocab.html
556
and the depth of the deepest common ancestors of
both nodes be ddca. The similarity is then
sim(w, v) = max
i,j
sim(wi, vj) = maxi,j
2 ? ddca
di + dj
,
which takes the value between 0.0 and 1.0. Then,
the value of CC is calculated as the correlation co-
efficient of reference similarities r = (r1, r2, ..., rn)
and target similarities s = (s1, s2, ..., sn) over the
word pairs in sample set Ps, which is created by
choosing the most similar 2,000 word pairs from
4,000 randomly created pairs from LDV. To avoid
test-set dependency, all the CC values presented in
this paper are the average values of three trials using
different test sets.
5 Experiments
Now we describe the experimental settings and the
evaluation results of context selection methods.
5.1 Experimental Settings
As for the corpus, New York Times section of En-
glish Gigaword 2, consisting of around 914 million
words and 1.3 million documents was analyzed to
obtain word-context co-occurrences. Frequency cut-
off was applied as a pre-processing in order to filter
out any words and contexts with low frequency and
to reduce computational cost. More specifically, any
words w such that
?
c tf(w, c) < ?f and any con-
texts c such that
?
w tf(w, c) < ?f , with ?f = 40,
were removed from the co-occurrence data.
Since we set our purpose here to the automatic
acquisition of synonymous nouns, only the nouns
except for proper nouns were selected. To distin-
guish nouns, using POS tags annotated by RASP2,
any words with POS tags APP, ND, NN, NP, PN, PP
were labeled as nouns. This left a total of 40,461
unique words and 139,618 unique context, which
corresponds to the number of vectors and the dimen-
sionality of semantic space, respectively.
5.2 Context Reduction
In the first experiment, we show the effectiveness of
the five contextual selection methods introduced in
Section 2 for context reduction problem. The five
2http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2003T05
measures were calculated for each context, and con-
texts were sorted by their importance. The change of
performance, AP and CC, was calculated on elimi-
nating the low-ranked contexts and varying the pro-
portion of remaining ones, until only 0.2% (279 in
number) of the unique contexts are left.
The result is displayed in Figure 1. The overall
observation is that the performance not only kept the
original level but also slightly improved even during
the ?aggressive? reduction when more than 80% of
the original contexts were eliminated and less than
20,000 contexts were left. It was not until 90% (ap-
prox. 10,000 remaining) elimination that the AP
values began to fall. The tendency of performance
change was almost the same for AP and CC, but
we observe a slight difference regarding which of
the five measures were effective. More specifically,
TS, IG and CHI2 worked well for AP, and DF, TS,
while CHI2 did for CC. On the whole, TS and CHI2
were performing the best, whereas the performance
of MI quickly worsened. Although the task is dif-
ferent, this experiment showed a very consistent re-
sult compared with the one of Yang and Pedersen?s
(1997). This means that feature selection methods
are also effective for context selection in distribu-
tional similarity, and our formalization of the prob-
lem described in Section 2 turned out to be appro-
priate for the purpose.
5.3 Context Category Selection
We are then naturally interested in what kinds of
contexts are included in these top-ranked effective
ones and how much they affect the overall perfor-
mance. To investigate this, we firstly built a set of
elite contexts, by gathering each top 10% (13,961
in number) contexts chosen by DF, TS, IG, and
CHI2, and obtaining the intersection of these four
top-ranked contexts. It was found that these four had
a great deal of overlap among them, the number of
which turned out to be 6,440.
Secondly, to measure the degree of effect a con-
text category has, we defined category importance
as the sum of all IG values of the contexts which
belong to the category. The reason is that, (a) IG
was one of the best-performing criteria as the previ-
ous experiment showed, and (b) IG value for a set of
contexts can be calculated as the sum of IG values of
individual elements, assuming that all the contexts
557
0.10
0.15
0.20
0.25
020000400006000080000100000120000
Number of Unique Context
Cor
rela
tion
 Co
effi
cien
t (C
C)
DF
TS
MI
IG
CHI2
`
(c)
6.0%
8.0%
10.0%
12.0%
14.0%
020000400006000080000100000120000
Number of Unique Context
Ave
rage
 Pre
cisi
on (
AP)
DF
TS
MI
IG
CHI2 (a)
0.10
0.15
0.20
0.25
05000100001500020000
Number of Unique Context
Cor
rela
tion
 Co
effi
cien
t (C
C)
DF
TS
IG
CHI2
`
(d)
6.0%
8.0%
10.0%
12.0%
14.0%
05000100001500020000
Number of Unique Context
Ave
rage
 Pre
cisi
on (
AP)
DF
TS
IG
CHI2 (b)
Figure 1: Performance of synonym acquisition on automatic context reduction
(a) The overall view and (b) the close-up of 0 to 20,000 unique contexts for AP,
and (c) the overall view and (b) the close-up for CC
are mutually independent, which is a naive but prac-
tical assumption because of the high independence
of acquired contexts from corpora.
For the categories: ncsubj, dobj, obj, obj2,
ncmod, xmod, cmod, ccomp, det, ta, based on the
RASP2 grammatical relations which occur fre-
quently (more than 1.0%) in the corpus, their cat-
egory importance within the elite context set was
computed and showed in Figure 2. The graph also
shows the performance of individual context cat-
egories, calculated when each category was sepa-
rately extracted from the entire corpus. The re-
sult indicates that there is a considerable correlation
(r = 0.760) between category importance and per-
formance, which means it is possible to predict the
final performance of any context categories by cal-
culating their category importance values in the lim-
ited size of selected context set.
As for the qualitative difference of category types,
the result also shows the effectiveness of modifica-
tion (ncmod) category, which is consistent with the
result (Hagiwara et al, 2006) that mod is more con-
tributing than subj and obj, which have been ex-
tensively used in the past. However, it can be seen
that the reason why the ncmod performs well may be
only because it is the largest category in size (2,515
558
0% 2% 4% 6% 8% 10% 12% 14%
ncsubj
dobj 
obj 
obj2 
ncmod 
xmod
cmod
ccomp
det
ta 
Average Precision (AP)
0 2 4 6 8 10
Category Importance (CI)
AP
CI
Figure 2: Performance of synonym acquisition vs
context category importance
in the elite contexts). The investigation of the rela-
tions between context size and performance should
be conducted in the future.
6 Conclusion
In this study, we firstly introduced feature selec-
tion methods, previously proposed for text catego-
rization, and showed how to apply them for auto-
matic context selection for distributional similarity
by formalizing the similarity problem as classifica-
tion. We then extracted dependency-based context
from the corpus, and conducted evaluation experi-
ments on automatic synonym acquisition.
The experimental results showed that while keep-
ing or even improving the original performance, it
is possible to eliminate a large proportion of con-
texts (almost up to 90%). We also extended the con-
text importance to cover context categories based on
RASP2 grammatical relations, and showed a consid-
erable correlation between the importance and the
actual performance, suggesting the possibility of au-
tomatic context category selection.
As the future works, we should further discuss
other kinds of formalization of distributional simi-
larity and their impact, because we introduced and
only briefly described a quite simple formalization
model in Section 2.3. More detailed investigations
on the contributions of sub-categories of contexts,
and other contexts than dependency structure, such
as surrounding words and dependency path, is also
the future work.
References
Ted Briscoe, John Carroll and Rebecca Watson. 2006.
The Second Release of the RASP System. Proc. of the
COLING/ACL 2006 Interactive Presentation Sessions,
77?80.
Collins. 2002. Collins Cobuild Major New Edition CD-
ROM. HarperCollins Publishers.
James R. Curran and Marc Moens. 2002. Scaling Con-
text Space. Proc. of ACL 2002, 231?238.
James R. Curran and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In Workshop
on Unsupervised Lexical Acquisition. Proc. of ACL
SIGLEX, 231?238.
Chris Ding and Hanchuan Peng. 2003. Minimum Re-
dundancy Feature Selection from Microarray Gene
Expression Data. Proc. of the IEEE Computer Soci-
ety Conference on Bioinformatics, 523?528.
Editors of the American Heritage Dictionary. 1995. Ro-
get?s II: The New Thesaurus, 3rd ed. Houghton Mif-
flin.
Christiane Fellbaum. 1998. WordNet: an electronic lexi-
cal database, MIT Press.
Masato Hagiwara, Yasuhiro Ogawa, Katsuhiko Toyama.
2006. Selection of Effective Contextual Information
for Automatic Synonym Acquisition. Proc. of COL-
ING/ACL 2006, 353?360.
Zellig Harris. 1985. Distributional Structure. Jerrold J.
Katz (ed.) The Philosophy of Linguistics. Oxford Uni-
versity Press. 26?47
Donald Hindle. 1990. Noun classification from
predicate-argument structures. Proc. of the 28th An-
nual Meeting of the ACL, 268?275.
Will Lowe and Scott McDonald. 2000. The direct route:
Mediated priming in semantic space. Proc. of the 22nd
Annual Conference of the Cognitive Science Society,
675?680.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. Proc. of COLING/ACL 1998, 786?774.
559
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, Volume 7, Issue 4, 343?360.
Seastian Pado and Mirella Lapata. 2007. Dependency-
Based Construction of Semantic Space Models Com-
putational Linguistics, Volume 33, Issue 2, 161?199.
Gerda Ruge. 1997. Automatic detection of thesaurus re-
lations for information retrieval applications. Founda-
tions of Computer Science: Potential - Theory - Cogni-
tion, LNCS, Volume 1337, 499?506, Springer Verlag,
Berlin, Germany.
Yiming Yang and John Wilbur. 1996. Using corpus
statistics to remove redundant words in text categoriza-
tion. Journal of the American Society for Information
Science, Volume 47, Issue 5, 357?369.
Yiming Yang and Jan O. Pedersen. 1997. A Compara-
tive Study on Feature Selection in Text Categorization.
Proc. of ICML 97, 412?420.
John Wilbur and Karl Sirotkin. 1992. The automatic
identification of stop words. Journal of Information
Science, 45?55.
560
