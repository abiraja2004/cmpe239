Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1?11, Prague, June 2007. c?2007 Association for Computational Linguistics
Modelling Compression with Discourse Constraints
James Clarke and Mirella Lapata
School of Informatics, University of Edinburgh
2 Bucclecuch Place, Edinburgh EH8 9LW, UK
jclarke@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
Sentence compression holds promise for
many applications ranging from summarisa-
tion to subtitle generation and subtitle gen-
eration. The task is typically performed on
isolated sentences without taking the sur-
rounding context into account, even though
most applications would operate over entire
documents. In this paper we present a dis-
course informed model which is capable of
producing document compressions that are
coherent and informative. Our model is in-
spired by theories of local coherence and
formulated within the framework of Integer
Linear Programming. Experimental results
show significant improvements over a state-
of-the-art discourse agnostic approach.
1 Introduction
The computational treatment of sentence compres-
sion has recently attracted much attention in the
literature. The task can be viewed as producing a
summary of a single sentence that retains the most
important information and remains grammatically
correct (Jing 2000). Sentence compression is com-
monly expressed as a word deletion problem: given
an input sentence of words W = w1,w2, . . . ,wn, the
aim is to produce a compression by removing any
subset of these words (Knight and Marcu 2002).
Sentence compression can potentially benefit
many applications. For example, in summarisation,
a compression mechanism could improve the con-
ciseness of the generated summaries (Jing 2000;
Lin 2003). Sentence compression could be also
used to automatically generate subtitles for tele-
vision programs; the transcripts cannot usually be
used verbatim due to the rate of speech being too
high (Vandeghinste and Pan 2004). Other applica-
tions include compressing text to be displayed on
small screens (Corston-Oliver 2001) such as mobile
phones or PDAs, and producing audio scanning de-
vices for the blind (Grefenstette 1998).
Most work to date has focused on a rather sim-
ple formulation of sentence compression that does
not allow any rewriting operations, besides word re-
moval. Moreover, compression is performed on iso-
lated sentences without taking into account their sur-
rounding context. An advantage of this simple view
is that it renders sentence compression amenable to
a variety of learning paradigms ranging from in-
stantiations of the noisy-channel model (Galley and
McKeown 2007; Knight and Marcu 2002; Turner
and Charniak 2005) to Integer Linear Programming
(Clarke and Lapata 2006a) and large-margin online
learning (McDonald 2006).
In this paper we take a closer look at one of
the simplifications associated with the compression
task, namely that sentence reduction can be realised
in isolation without making use of discourse-level
information. This is clearly not true ? professional
abstracters often rely on contextual cues while creat-
ing summaries (Endres-Niggemeyer 1998). Further-
more, determining what information is important in
a sentence is influenced by a variety of contextual
factors such as the discourse topic, whether the sen-
tence introduces new entities or events that have not
been mentioned before, and the reader?s background
knowledge.
The simplification is also at odds with most appli-
cations of sentence compression which aim to cre-
ate a shorter document rather than a single sentence.
The resulting document must not only be grammat-
1
ical but also coherent if it is to function as a re-
placement for the original. However, this cannot be
guaranteed without knowing how the discourse pro-
gresses from sentence to sentence. To give a simple
example, a contextually aware compression system
could drop a word or phrase from the current sen-
tence, simply because it is not mentioned anywhere
else in the document and is therefore deemed unim-
portant. Or it could decide to retain it for the sake of
topic continuity.
We are interested in creating a compression model
that is appropriate for documents and sentences. To
this end, we assess whether discourse-level informa-
tion is helpful. Our analysis is informed by two pop-
ular models of discourse, Centering Theory (Grosz
et al 1995) and lexical chains (Morris and Hirst
1991). Both approaches model local coherence ?
the way adjacent sentences bind together to form a
larger discourse. Our compression model is an ex-
tension of the integer programming formulation pro-
posed by Clarke and Lapata (2006a). Their approach
is conceptually simple: it consists of a scoring func-
tion coupled with a small number of syntactic and
semantic constraints. Discourse-related information
can be easily incorporated in the form of additional
constraints. We employ our model to perform sen-
tence compression throughout a whole document
(by compressing sentences sequentially) and evalu-
ate whether the resulting text is understandable and
informative using a question-answering task. Our
method yields significant improvements over a dis-
course agnostic state-of-the-art compression model
(McDonald 2006).
2 Related Work
Sentence compression has been extensively stud-
ied across different modelling paradigms and has
received both generative and discriminative formu-
lations. Most generative approaches (Galley and
McKeown 2007; Knight and Marcu 2002; Turner
and Charniak 2005) are instantiations of the noisy-
channel model, whereas discriminative formulations
include decision-tree learning (Knight and Marcu
2002), maximum entropy (Riezler et al 2003),
support vector machines (Nguyen et al 2004),
and large-margin learning (McDonald 2006). These
models are trained on a parallel corpus of long
source sentences and their target compressions. Us-
ing a rich feature set derived from parse trees, the
models learn either which constituents to delete or
which words to place adjacently in the compression
output. Relatively few approaches dispense with the
parallel corpus and generate compressions in an un-
supervised manner using either a scoring function
(Clarke and Lapata 2006a; Hori and Furui 2004) or
compression rules that are approximated from a non-
parallel corpus such as the Penn Treebank (Turner
and Charniak 2005).
Our work differs from previous approaches in two
key respects. First, we present a compression model
that is contextually aware; decisions on whether to
remove or retain a word (or phrase) are informed by
its discourse properties (e.g., whether it introduces a
new topic, whether it is semantically related to the
previous sentence). Second, we apply our compres-
sion model to entire documents rather than isolated
sentences. This is more in the spirit of real-world ap-
plications where the goal is to generate a condensed
and coherent text. Previous work on summarisation
has also utilised discourse information (e.g., Barzi-
lay and Elhadad 1997; Daume? III and Marcu 2002;
Marcu 2000; Teufel and Moens 2002). However, its
application to document compression is novel to our
knowledge.
3 Discourse Representation
Obtaining an appropriate representation of discourse
is the first step towards creating a compression
model that exploits contextual information. In this
work we focus on the role of local coherence as
this is prerequisite for maintaining global coherence.
Ideally, we would like our compressed document to
maintain the discourse flow of the original. For this
reason, we automatically annotate the source docu-
ment with discourse-level information which is sub-
sequently used to inform our compression proce-
dure. We first describe our algorithms for obtaining
discourse annotations and then present our compres-
sion model.
3.1 Centering Theory
Centering Theory (Grosz et al 1995) is an entity-
orientated theory of local coherence and salience.
Although an utterance in discourse may contain sev-
eral entities, it is assumed that a single entity is
salient or ?centered?, thereby representing the cur-
rent focus. One of the main claims underlying cen-
tering is that discourse segments in which succes-
2
sive utterances contain common centers are more
coherent than segments where the center repeatedly
changes.
Each utterance Ui in a discourse segment has a
list of forward-looking centers, C f (Ui) and a unique
backward-looking center, Cb(Ui). C f (Ui) represents
a ranking of the entities invoked by Ui according
to their salience. The Cb of the current utterance
Ui, is the highest-ranked element in C f (Ui?1) that is
also in Ui. The Cb thus links Ui to the previous dis-
course, but it does so locally since Cb(Ui) is chosen
from Ui?1.
Centering Algorithm So far we have presented
centering without explicitly stating how the con-
cepts ?utterance?, ?entities? and ?ranking? are in-
stantiated. A great deal of research has been devoted
into fleshing these out and many different instantia-
tions have been developed in the literature (see Poe-
sio et al 2004 for details). Since our aim is to iden-
tify centers in discourse automatically, our param-
eter choice is driven by two considerations, robust-
ness and ease of computation.
We therefore follow previous work (e.g., Milt-
sakaki and Kukich 2000) in assuming that the unit of
an utterance is the sentence (i.e., a main clause with
accompanying subordinate and adjunct clauses).
This is in line with our compression task which also
operates over sentences. We determine which en-
tities are invoked by a sentence using two meth-
ods. First, we perform named entity identification
and coreference resolution on each document using
LingPipe1, a publicly available system. Named en-
tities and all remaining nouns are added to the C f
list. Entity matching between sentences is required
to determine the Cb of a sentence. This is done using
the named entity?s unique identifier (as provided by
LingPipe) or by the entity?s surface form in the case
of nouns not classified as named entities.
Entities are ranked according to their grammatical
roles; subjects are ranked more highly than objects,
which are in turn ranked higher than other grammat-
ical roles (Grosz et al 1995); ties are broken using
left-to-right ordering of the grammatical roles in the
sentence (Tetreault 2001). We identify grammatical
roles with RASP (Briscoe and Carroll 2002). For-
mally, our centering algorithm is as follows (where
Ui corresponds to sentence i):
1LingPipe can be downloaded from http://www.
alias-i.com/lingpipe/.
1. Extract entities from Ui.
2. Create C f (Ui) by ranking the entities in
Ui according to their grammatical role
(subjects > objects > others).
3. Find the highest ranked entity in C f (Ui?1)
which occurs in C f (Ui), set the entity to
be Cb(Ui).
The above procedure involves several automatic
steps (named entity recognition, coreference reso-
lution, identification of grammatical roles) and will
unavoidably produce some noisy annotations. So,
there is no guarantee that the right Cb will be iden-
tified or that all sentences will be marked with a Cb.
The latter situation also occurs in passages that con-
tain abrupt changes in topic. In such cases, none of
the entities realised in Ui will occur in C f (Ui?1).
Rather than accept that discourse information may
be absent in a sentence, we turn to lexical chains
as an alternative means of capturing topical content
within a document.
3.2 Lexical Chains
Lexical cohesion refers to the degree of semantic re-
latedness observed among lexical items in a docu-
ment. The term was coined by Halliday and Hasan
(1976) who observed that coherent documents tend
to have more related terms or phrases than inco-
herent ones. A number of linguistic devices can be
used to signal cohesion; these range from repeti-
tion, to synonymy, hyponymy and meronymy. Lexi-
cal chains are a representation of lexical cohesion as
sequences of semantically related words (Morris and
Hirst 1991) and provide a useful means for describ-
ing the topic flow in discourse. For instance, a docu-
ment with many different lexical chains will prob-
ably contain several topics. And main topics will
tend to be represented by dense and long chains.
Words participating in such chains are important for
our compression task ? they reveal what the docu-
ment is about ? and in all likelihood should not be
deleted.
Lexical Chains Algorithm Barzilay and Elhadad
(1997) describe a technique for text summarisation
based on lexical chains. Their algorithm uses Word-
Net to build chains of nouns (and noun compounds).
These are ranked heuristically by a score based on
their length and homogeneity. A summary is then
produced by extracting sentences corresponding to
3
strong chains, i.e., chains whose score is two stan-
dard deviations above the average score.
Like Barzilay and Elhadad (1997), we wish to
determine which lexical chains indicate the most
prevalent discourse topics. Our assumption is that
terms belonging to these chains are indicative of the
document?s main focus and should therefore be re-
tained in the compressed output. Barzilay and El-
hadad?s scoring function aims to identify sentences
(for inclusion in a summary) that have a high con-
centration of chain members. In contrast, we are in-
terested in chains that span several sentences. We
thus score chains according to the number of sen-
tences their terms occur in. For example, the chain
{house3, home3, loft3, house5} (where wordi de-
notes word occurring in sentence i) would be given
a score of two as the terms only occur in two sen-
tences. We assume that a chain signals a prevalent
discourse topic if it occurs throughout more sen-
tences than the average chain. The scoring algorithm
is outlined more formally below:
1. Compute the lexical chains for the document.
2. Score(Chain) = Sentences(Chain).
3. Discard chains if Score(Chain) < Avg(Score).
4. Mark terms from the remaining chains as being
the focus of the document.
We use the method of Galley and McKeown (2003)
to compute lexical chains for each document.2 This
is an improved version of Barzilay and Elhadad?s
(1997) original algorithm.
Before compression takes place, all documents
are pre-processed using the centering and lexical
chain algorithms described above. In each sentence
we mark the center Cb(Ui) if one exists. Words (or
phrases) that are present in the current sentence and
function as the center in the next sentence Cb(Ui+1)
are also flagged. Finally, words are marked if they
are part of a prevalent chain. An example of our dis-
course annotation is given in Figure 1.
4 The Compression Model
Our model is an extension of the approach put for-
ward in Clarke and Lapata (2006a). Their work tack-
les sentence compression as an optimisation prob-
lem. Given a long sentence, a compression is formed
by retaining the words that maximise a scoring func-
2The software is available from http://www1.cs.
columbia.edu/?galley/.
Bad



weather dashed hopes of attempts to halt
the




flow1 during what was seen as a lull in
the lava?s momentum. Experts say that even
if the eruption stopped




today2 , the pressure of
lava piled up behind for six




miles3 would
bring debris cascading down on to the


 
town
anyway. Some estimate the volcano is pouring out
one million tons of debris a




day2 , at a




rate1
of 15




ft3 per




second2 , from a fissure that opened
in mid-December.
The Italian Army




yesterday2 detonated 400lb of
dynamite 3,500 feet up Mount Etna?s slopes.
Figure 1: Excerpt of document from our test set with
discourse annotations. Centers are in double boxes;
terms occurring in lexical chains are in oval boxes.
Words with the same subscript are members of the
same chain (e.g., today, day, second, yesterday)
tion. The latter is essentially a language model cou-
pled with a few constraints ensuring that the re-
sulting output is grammatical. The language model
and the constraints are encoded as linear inequal-
ities whose solution is found using Integer Linear
Programming (ILP, Vanderbei 2001; Winston and
Venkataramanan 2003).
We selected this model for several reasons. First
it does not require a parallel corpus and thus can be
ported across domains and text genres, whilst de-
livering state-of-the-art results (see Clarke and La-
pata 2006a for details). Second, discourse-level in-
formation can be easily incorporated by augment-
ing the constraint set. This is not the case for other
approaches (e.g., those based on the noisy channel
model) where compression is modelled by gram-
mar rules indicating which constituents to delete in a
syntactic context. Third, the ILP framework delivers
a globally optimal solution by searching over the en-
tire compression space3 without employing heuris-
tics or approximations during decoding.
We begin by recapping the formulation of Clarke
and Lapata (2006a). Let W = w1,w2, . . . ,wn denote
a sentence for which we wish to generate a com-
pression. A set of binary decision variables repre-
sent whether each word wi should be included in the
3For a sentence of length n, there are 2n compressions.
4
compression or not. Let:
yi =
{
1 if wi is in the compression
0 otherwise ?i ? [1 . . .n]
A trigram language model forms the backbone of
the compression model. The language model is for-
mulated as an integer program with the introduction
of extra decision variables indicating which word
sequences should be retained or dropped from the
compression. Let:
pi =
{
1 if wi starts the compression
0 otherwise ?i ? [1 . . .n]
qi j =
?
?
?
1 if sequence wi,w j ends
the compression ?i ? [1 . . .n?1]
0 otherwise ? j ? [i+ 1 . . .n]
xi jk =
?
?
?
1 if sequence wi,w j,wk ?i ? [1 . . .n?2]
is in the compression ? j ? [i+ 1 . . .n?1]
0 otherwise ?k ? [ j + 1 . . .n]
The objective function is expressed in Equa-
tion (1). It is the sum of all possible trigrams mul-
tiplied by the appropriate decision variable. The ob-
jective function also includes a significance score for
each word multiplied by the decision variable for
that word (see the last summation term in (1)). This
score highlights important content words in a sen-
tence and is defined in Section 4.1.
maxz =
n
?
i=1
pi ?P(wi|start)
+
n?2
?
i=1
n?1
?
j=i+1
n
?
k= j+1
xi jk ?P(wk|wi,w j)
+
n?1
?
i=0
n
?
j=i+1
qi j ?P(end|wi,w j)
+
n
?
i=1
yi ? I(wi) (1)
subject to:
yi, pi,qi j,xi jk = 0 or 1 (2)
A set of sequential constraints4 are added to the
problem to only allow results which combine valid
trigrams.
4We have omitted sequential constraints due to space limi-
tations. The full details are given in Clarke and Lapata (2006a).
4.1 Significance Score
The significance score is an attempt at capturing the
gist of a sentence. It gives more weight to content
words that appear in the deepest level of embed-
ding in the syntactic tree representing the source
sentence:
I(wi) =
l
N
? fi log FaFi (3)
The score is computed over a large corpus where wi
is a content word (i.e., a noun or verb), fi and Fi are
the frequencies of wi in the document and corpus
respectively, and Fa is the sum of all content words
in the corpus. l is the number of clause constituents
above wi, and N is the deepest level of embedding.
4.2 Sentential Constraints
The model also contains a small number of
sentence-level constraints. Their aim is to preserve
the meaning and structure of the original sentence
as much as possible. The majority of constraints
revolve around modification and argument struc-
ture and are defined over parse trees or gram-
matical relations. For example, the following con-
straint template disallows the inclusion of modifiers
(e.g., nouns, adjectives) without their head words:
yi ? y j ? 0 (4)
?i, j : w j modifies wi
Other constraints force the presence of modifiers
when the head is retained in the compression. This
way, it is ensured that negation will be preserved in
the compressed output:
yi ? y j = 0 (5)
?i, j : w j modifies wi ? w j = not
Argument structure constraints make sure that
the resulting compression has a canonical argument
structure. For instance a constraint ensures that if a
verb is present in the compression then so are its ar-
guments:
yi ? y j = 0 (6)
?i, j : w j ? subject/object of verb wi
Finally, Clarke and Lapata (2006a) propose one
discourse constraint which forces the system to pre-
serve personal pronouns in the compressed output:
yi = 1 (7)
?i : wi ? personal pronouns
5
4.3 Discourse Constraints
In addition to the constraints described above, our
model includes constraints relating to the centering
and lexical chains representations discussed in Sec-
tion 3. Recall that after some pre-processing, each
sentence is marked with: its own center Cb(Ui), the
center Cb(Ui+1) of the sentence following it and
words that are members of high scoring chains cor-
responding to the document?s focus. We introduce
two new types of constraints based on these addi-
tional knowledge sources.
The first constraint is the centering constraint
which operates over adjacent sentences. It ensures
that the Cb identified in the source sentence is re-
tained in the target compression. If present, the en-
tity realised as the Cb in the following sentence is
also retained:
yi = 1 (8)
?i : wi ? {Cb(Ui),Cb(Ui+1)}
Consider for example the discourse in Figure 1. The
constraints generated from Equation (8) will require
the compression to retain lava in the first two sen-
tences and debris in sentences two and three.
We also add a lexical chain constraint. This ap-
plies only to nouns which are members of prevalent
chains:
yi = 1 (9)
?i : wi ? document focus lexical chain
This constraint is complementary to the centering
constraint; the sentences it applies to do not have to
be adjacent and the entities under consideration are
not restricted to a specific syntactic role (e.g., sub-
ject or object). See for instance the words flow and
rate in Figure 1 which are members of the same
chain (marked with subscript one). According to
constraint (9) both words must be included in the
compressed document.
The constraints just described ensure that the
compressed document will retain the discourse flow
of the original and will preserve terms indicative
of important topics. We argue that these constraints
will additionally benefit sentence-level compres-
sion, as words which are not signalled as discourse
relevant can be dropped.
4.4 Applying the Constraints
Our compression system is given a (sentence sepa-
rated) document as input. The ILP model just pre-
sented is then applied sequentially to all sentences
to generate a compressed version of the original. We
thus create and solve an ILP for every sentence.5 In
the formulation of Clarke and Lapata (2006a) a sig-
nificance score (see Section 4.1) highlights which
nouns and verbs to include in the compression. As
far as nouns are concerned, our discourse constraints
perform a similar task. Thus, when a sentence con-
tains discourse annotations, we are inclined to trust
them more and only calculate the significance score
for verbs.
During development it was observed that apply-
ing all discourse constraints simultaneously (see
Equations (7)?(9)) results in relatively long com-
pressions. To counter this, we employ these con-
straints using a back-off strategy that relies on pro-
gressively less reliable information. Our back-off
model works as follows: if centering information is
present, we apply the appropriate constraints (Equa-
tion (8)). If no centers are present, we back-off to the
lexical chain information using Equation (9), and in
the absence of the latter we back-off to the pronoun
constraint (Equation (7)). Finally, if discourse infor-
mation is entirely absent from the sentence, we de-
fault to the significance score. Sentential constraints
(see Section 4.2) are applied throughout irrespec-
tively of discourse constraints. In our test data (see
Section 5 for details), the centering constraint was
used in 68.6% of the sentences. The model backed
off to lexical chains for 13.7% of the test sentences,
whereas the pronoun constraint was applied in 8.5%.
Finally, the noun and verb significance score was
used on the remaining 9.2%. An example of our sys-
tem?s output for the text in Figure 1 is given in Fig-
ure 2.
5 Experimental Set-up
In this section we present our experimental set-up.
We briefly introduce the model used for compar-
ison with our approach and give details regarding
our compression corpus and parameter estimation.
Finally, we describe our evaluation methodology.
5We use the publicly available lp solve solver (http://
www.geocities.com/lpsolve/).
6
Bad weather dashed hopes to halt the flow during
what was seen as lull in lava?s momentum. Ex-
perts say that even if eruption stopped, the pres-
sure of lava piled would bring debris cascading.
Some estimate volcano is pouring million tons of
debris from fissure opened in mid-December. The
Army yesterday detonated 400lb of dynamite.
Figure 2: System output on excerpt from Figure 1.
Comparison with state-of-the-art An obvious
evaluation experiment would involve comparing
the ILP model without any discourse constraints
against the discourse informed model presented in
this work. Unfortunately, the two models obtain
markedly different compression rates6 which ren-
ders the comparison of their outputs problematic. To
put the comparison on an equal footing, we evalu-
ated our approach against a state-of-the-art model
that achieves a compression rate similar to ours
without taking discourse-level information into ac-
count. McDonald (2006) formalises sentence com-
pression in a discriminative large-margin learning
framework as a classification task: pairs of words
from the source sentence are classified as being ad-
jacent or not in the target compression. A large
number of features are defined over words, parts
of speech, phrase structure trees and dependen-
cies. These are gathered over adjacent words in the
compression and the words in-between which were
dropped.
It is important to note that McDonald (2006) is not
a straw-man system. It achieves highly competitive
performance compared with Knight and Marcu?s
(2002) noisy channel and decision tree models. Due
to its discriminative nature, the model is able to use
a large feature set and to optimise compression ac-
curacy directly. In other words, McDonald?s model
has a head start against our own model which does
not utilise a parallel corpus and has only a few con-
straints. The comparison of the two systems allows
us to investigate whether discourse information is re-
dundant when using a powerful sentence compres-
sion model.
Corpus Previous work on sentence compres-
sion has used almost exclusively the Ziff-Davis,
6The discourse agnostic ILP model has a compression rate
of 81.2%; when discourse constraints are include the rate drops
to 65.4%.
a compression corpus derived automatically from
document-abstract pairs (Knight and Marcu 2002).
Unfortunately, this corpus is not suitable for our
purposes since it consists of isolated sentences. We
thus created a document-based compression corpus
manually. Following Clarke and Lapata (2006a), we
asked annotators to produce compressions for 82
stories (1,629 sentences) from the BNC and the LA
Times Washington Post.7 48 documents (962 sen-
tences) were used for training, 3 for development (63
sentences), and 31 for testing (604 sentences).
Parameter Estimation Our parameters for the
ILP model followed closely Clarke and Lapata
(2006a). We used a language model trained on
25 million tokens from the North American News
corpus. The significance score was based on 25
million tokens from the same corpus. Our re-
implementation of McDonald (2006) used an identi-
cal feature set, and a slightly modified loss function
to encourage compression on our data set.8
Evaluation Previous studies evaluate how well-
formed the automatically derived compressions are
out of context. The target sentences are typi-
cally rated by naive subjects on two dimensions,
grammaticality and importance (Knight and Marcu
2002). Automatic evaluation measures have also
been proposed. Riezler et al (2003) compare the
grammatical relations found in the system output
against those found in a gold standard using F-score
which Clarke and Lapata (2006b) show correlates
reliably with human judgements.
Following previous work, sentence-based com-
pressions were evaluated automatically using F-
score computed over grammatical relations which
we obtained by RASP (Briscoe and Carroll 2002).
Besides individual sentences, our goal was to evalu-
ate the compressed document as whole. Our evalu-
ation methodology was motivated by two questions:
(1) are the documents readable? and (2) how much
key information is preserved between the source
document and its target compression? We assume
here that the compressed document is to function as
a replacement for the original. We can thus measure
the extent to which the compressed version can be
7The corpus is available from http://homepages.inf.
ed.ac.uk/s0460084/data/.
8McDonald?s (2006) results are reported on the Ziff-Davis
corpus.
7
What is posing a threat to the town? (lava)
What hindered attempts to stop the lava flow?
(bad weather)
What did the Army do first to stop the lava flow?
(detonate explosives)
Figure 3: Example questions with answer key.
used to find answers for questions which are derived
from the original and represent its core content.
We therefore employed a question-answering
evaluation paradigm which has been previously used
for summarisation evaluation and text comprehen-
sion (Mani et al 2002; Morris et al 1992). The
overall objective of our Q&A task is to determine
how accurate each document (generated by differ-
ent compression systems) is at answering questions.
For this we require a methodology for constructing
Q&A pairs and for scoring each document.
Two annotators were independently instructed
to create Q&A pairs for the original documents
in the test set. Each annotator read the document
and then drafted no more than ten questions and
answers related to its content. Annotators were
asked to create factual-based questions which re-
quired an unambiguous answer; these were typically
who/what/where/when/how style questions. Anno-
tators then compared and revised their question-
answer pairs to create a common agreed upon set.
Revisions typically involved merging questions, re-
wording and simplifying questions, and in some
cases splitting a question into multiple questions.
Documents for which too few questions were cre-
ated or for which questions or answers were too am-
biguous were removed. This left an evaluation set
of six documents with between five to eight con-
cise questions per document. Some example ques-
tions corresponding to the document from Figure 1
are given in Figure 3; correct answers are shown in
parentheses.
Compressed documents and their accompanying
questions were presented to human subjects who
were asked to provide answers as best they could.
We elicited answers for six documents in three com-
pression conditions: gold standard, using the ILP
discourse model, and McDonald?s (2006) model.
Each participant was also asked to rate the readabil-
ity of the compressed document on a seven point
scale. A Latin Square design prevented participants
Model CompR F-Score
McDonald 60.1% 36.0%?
Discourse ILP 65.4% 39.6%
Gold Standard 70.3% ??
Table 1: Compression results: compression rate and
relation-based F-score; ? sig. diff. from Discourse
ILP (p < 0.05 using the Student t test).
Model Readability Q&A
McDonald 2.6? 53.7%??
Discourse ILP 3.0? 68.3%
Gold Standard 5.5? 80.7%
Table 2: Human Evaluation Results: average read-
ability ratings and average percentage of questions
answered correctly. ?: sig. diff. from Gold Standard;
?: sig. diff. from Discourse ILP.
from seeing two different compressions of the same
document.
The study was conducted remotely over the In-
ternet. Participants were presented with a set of in-
structions that explained the Q&A task and provided
examples. Subjects were first asked to read the com-
pressed document and rate its readability. Questions
were then presented one at a time and participants
were allowed to consult the document for the an-
swer. Once a participant had provided an answer
they were not allowed to modify it. Thirty unpaid
volunteers took part in our Q&A study. All were self
reported native English speakers.
The answers provided by the participants were
scored against the answer key. Answers were con-
sidered correct if they were identical to the answer
key or subsumed by it. For instance, Mount Etna
was considered a right answer to the first question
from Figure 3. A compressed document receives a
full score if subjects have answered all questions re-
lating to it correctly.
6 Results
As a sanity check, we first assessed the compres-
sions produced by our model and McDonald (2006)
on a sentence-by-sentence basis without taking the
documents into account. There is no hope for gener-
ating shorter documents if the compressed sentences
are either too wordy or too ungrammatical. Table 1
shows the compression rates (CompR) for the two
8
systems and evaluates the quality of their output us-
ing F-score based on grammatical relations. As can
be seen, the Discourse ILP compressions are slightly
longer than McDonald (65.4% vs. 60.1%) but closer
to the human gold standard (70.3%). This is not sur-
prising, the Discourse ILP model takes the entire
document into account, and compression decisions
will be slightly more conservative. The Discourse
ILP?s output is significantly better than McDonald in
terms of F-score, indicating that discourse-level in-
formation is generally helpful. Both systems could
use further improvement as inter-annotator agree-
ment on this data yields an F-score of 65.8%.
Let us now consider the results of our document-
based evaluation. Table 2 shows the mean readabil-
ity ratings obtained for each system and the per-
centage of questions answered correctly. We used
an Analysis of Variance (ANOVA) to examine the ef-
fect of compression type (McDonald, Discourse ILP,
Gold Standard). The ANOVA revealed a reliable ef-
fect on both readability and Q&A. Post-hoc Tukey
tests showed that McDonald and the Discourse ILP
model do not differ significantly in terms of read-
ability. However, they are significantly less read-
able than the gold standard (? < 0.05). For the Q&A
task we observe that our system is significantly bet-
ter than McDonald (? < 0.05) and not significantly
worse than the gold standard.
These results indicate that the automatic systems
lag behind the human gold standard in terms of
readability. When reading entire documents, sub-
jects are less tolerant of ungrammatical construc-
tions. We also find out that despite relatively low
readability, the documents are overall understand-
able. The discourse informed model generates more
informative documents ? the number of questions
answered correctly increases by 15% in comparison
to McDonald. This is an encouraging result suggest-
ing that there may be advantages in developing com-
pression models that exploit contextual information.
7 Conclusions and Future Work
In this paper we proposed a novel method for au-
tomatic sentence compression. Central in our ap-
proach is the use of discourse-level information
which we argue is an important prerequisite for doc-
ument (as opposed to sentence) compression. Our
model uses integer programming for inferring glob-
ally optimal compressions in the presence of lin-
guistically motivated constraints. Our discourse con-
straints aim to capture local coherence and are in-
spired by centering theory and lexical chains. We
showed that our model can be successfully em-
ployed to produce compressed documents that pre-
serve most of the original?s core content.
Our approach to document compression differs
from most summarisation work in that our sum-
maries are fairly long. However, we believe this is
the first step into understanding how compression
can help summarisation. In the future, we will in-
terface our compression model with sentence ex-
traction. The discourse annotations can help guide
the extraction method into selecting topically re-
lated sentences which can consequently be com-
pressed together. The compression rate can be tai-
lored through additional constraints which act on
the output length to ensure precise word limits are
obeyed.
We also plan to study the effect of global dis-
course structure (Daume? III and Marcu 2002) on the
compression task. In general, we will assess the im-
pact of discourse information more systematically
by incorporating it into generative and discrimina-
tive modelling paradigms.
Acknowledgements We are grateful to Ryan Mc-
Donald for his help with the re-implementation of
his system and our annotators Vasilis Karaiskos
and Sarah Luger. Thanks to Simone Teufel, Alex
Lascarides, Sebastian Riedel, and Bonnie Web-
ber for insightful comments and suggestions. La-
pata acknowledges the support of EPSRC (grant
GR/T04540/01).
References
Barzilay, R. and M. Elhadad. 1997. Using lexical
chains for text summarization. In Proceedings of
the Intelligent Scalable Text Summarization Work-
shop (ISTS), ACL-97.
Briscoe, E. J. and J. Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceed-
ings of the 3rd International Conference on Lan-
guage Resources and Evaluation (LREC?2002).
Las Palmas, Gran Canaria, pages 1499?1504.
Clarke, James and Mirella Lapata. 2006a.
Constraint-based sentence compression: An
integer programming approach. In Proceedings
of the COLING/ACL 2006 Main Conference
9
Poster Sessions. Sydney, Australia, pages
144?151.
Clarke, James and Mirella Lapata. 2006b. Models
for sentence compression: A comparison across
domains, training requirements and evaluation
measures. In Proceedings of the 21st Inter-
national Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association
for Computational Linguistics. Sydney, Australia,
pages 377?384.
Corston-Oliver, Simon. 2001. Text Compaction for
Display on Very Small Screens. In Proceedings of
the NAACL Workshop on Automatic Summariza-
tion. Pittsburgh, PA, pages 89?98.
Daume? III, Hal and Daniel Marcu. 2002. A noisy-
channel model for document compression. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL
2002). Philadelphia, PA, pages 449?456.
Endres-Niggemeyer, Brigitte. 1998. Summarising
Information. Springer, Berlin.
Galley, Michel and Kathleen McKeown. 2003.
Improving word sense disambiguation in lexi-
cal chaining. In Proceedings of 18th Interna-
tional Joint Conference on Artificial Intelligence
(IJCAI?03). pages 1486?1488.
Galley, Michel and Kathleen McKeown. 2007. Lex-
icalized markov grammars for sentence compres-
sion. In In Proceedings of the North Ameri-
can Chapter of the Association for Computational
Linguistics (NAACL-HLT?2007). Rochester, NY.
Grefenstette, Gregory. 1998. Producing Intelligent
Telegraphic Text Reduction to Provide an Audio
Scanning Service for the Blind. In Proceedings of
the AAAI Symposium on Intelligent Text Summa-
rization. Stanford, CA, pages 111?117.
Grosz, Barbara J., Scott Weinstein, and Aravind K.
Joshi. 1995. Centering: a framework for modeling
the local coherence of discourse. Computational
Linguistics 21(2):203?225.
Halliday, M. A. K. and Ruqaiya Hasan. 1976. Cohe-
sion in English. Longman, London.
Hori, Chiori and Sadaoki Furui. 2004. Speech sum-
marization: an approach through word extraction
and a method for evaluation. IEICE Transactions
on Information and Systems E87-D(1):15?25.
Jing, Hongyan. 2000. Sentence reduction for auto-
matic text summarization. In Proceedings of the
6th conference on Applied Natural Language Pro-
cessing (ANLP?2000). Seattle, WA, pages 310?
315.
Knight, Kevin and Daniel Marcu. 2002. Summa-
rization beyond sentence extraction: a probabilis-
tic approach to sentence compression. Artificial
Intelligence 139(1):91?107.
Lin, Chin-Yew. 2003. Improving summarization
performance by sentence compression ? a pilot
study. In Proceedings of the 6th International
Workshop on Information Retrieval with Asian
Languages. Sapporo, Japan, pages 1?8.
Mani, Inderjeet, Gary Klein, David House, Lynette
Hirschman, Therese Firmin, and Beth Sundheim.
2002. SUMMAC: A text summarization evalua-
tion. Natural Language Engineering 8(1):43?68.
Marcu, Daniel. 2000. The Theory and Practice of
Discourse Parsing and Summarization. The MIT
Press, Cambridge, MA.
McDonald, Ryan. 2006. Discriminative sentence
compression with soft syntactic constraints. In
Proceedings of the 11th EACL. Trento, Italy.
Miltsakaki, Eleni and Karen Kukich. 2000. The
role of centering theory?s rough-shift in the teach-
ing and evaluation of writing skills. In Proceed-
ings of the 38th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?2000).
pages 408?415.
Morris, A., G. Kasper, and D. Adams. 1992. The
effects and limitations of automated text condens-
ing on reading comprehension performance. In-
formation Systems Research 3(1):17?35.
Morris, Jane and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indi-
cator of the structure of text. Computational Lin-
guistics 17(1):21?48.
Nguyen, Minh Le, Akira Shimazu, Susumu
Horiguchi, Tu Bao Ho, and Masaru Fukushi.
2004. Probabilistic sentence reduction using
support vector machines. In Proceedings of
the 20th COLING. Geneva, Switzerland, pages
743?749.
Poesio, Massimo, Rosemary Stevenson, Barbara Di
Eugenio, and Janet Hitzeman. 2004. Centering: a
10
parametric theory and its instantiations. Compu-
tational Linguistics 30(3):309?363.
Riezler, Stefan, Tracy H. King, Richard Crouch, and
Annie Zaenen. 2003. Statistical sentence con-
densation using ambiguity packing and stochas-
tic disambiguation methods for lexical-functional
grammar. In Proceedings of the HLT/NAACL. Ed-
monton, Canada, pages 118?125.
Tetreault, Joel R. 2001. A corpus-based evaluation
of centering and pronoun resolution. Computa-
tional Linguistics 27(4):507?520.
Teufel, Simone and Marc Moens. 2002. Summa-
rizing scientific articles ? experiments with rele-
vance and rhetorical status. Computational Lin-
guistics 28(4):409?446.
Turner, Jenine and Eugene Charniak. 2005. Su-
pervised and unsupervised learning for sentence
compression. In Proceedings of the 43rd ACL.
Ann Arbor, MI, pages 290?297.
Vandeghinste, Vincent and Yi Pan. 2004. Sentence
compression for automated subtitling: A hybrid
approach. In Proceedings of the ACL Workshop
on Text Summarization. Barcelona, Spain, pages
89?95.
Vanderbei, Robert J. 2001. Linear Programming:
Foundations and Extensions. Kluwer Academic
Publishers, Boston, 2nd edition.
Winston, Wayne L. and Munirpallam Venkatara-
manan. 2003. Introduction to Mathematical Pro-
gramming. Brooks/Cole.
11
