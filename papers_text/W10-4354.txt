Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 281?288,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
F2 ? New Technique for Recognition of User Emotional States in    
Spoken Dialogue Systems 
Ram?n L?pez-C?zar 
Dept. of Languages and 
Computer Systems, CTIC-
UGR, University of Granada, 
Spain 
rlopezc@ugr.es 
Jan Silovsky 
Institute of Information 
Technology and Electronics, 
Technical University of 
Liberec, Czech Republic 
jan.silovsky@tul.cz 
David Griol 
Dept. of Computer Science 
Carlos III University of     
Madrid, Spain 
dgriol@inf.uc3m.es 
  
 
Abstract 
In this paper we propose a new technique to 
enhance emotion recognition by combining 
in different ways what we call emotion pre-
dictions. The technique is called F2 as the 
combination is based on a double fusion 
process. The input to the first fusion phase is 
the output of a number of classifiers which 
deal with different types of information re-
garding each sentence uttered by the user. 
The output of this process is the input to the 
second fusion stage, which provides as out-
put the most likely emotional category. Ex-
periments have been carried out using a pre-
viously-developed spoken dialogue system 
designed for the fast food domain. Results 
obtained considering three and two emo-
tional categories show that our technique 
outperforms the standard single fusion tech-
nique by 2.25% and 3.35% absolute, respec-
tively. 
1 Introduction 
Automatic recognition of user emotional states 
is a very challenging task that has attracted the 
attention of the research community for several 
decades. The goal is to design methods to 
make computers interact more naturally with 
human beings. This is a very complex task due 
to a variety of reasons. One is the absence of a 
generally agreed definition of emotion and of 
qualitatively different types of emotion. An-
other is that we still have an incomplete under-
standing of how humans process emotions, as 
even people have difficulty in distinguishing 
between them. Thus, in many cases a given 
emotion is perceived differently by different 
people. 
Studies in emotion recognition made by the 
research community have been applied to en-
hance the quality or efficiency of several ser-
vices provided by computers. For example, 
these have been applied to spoken dialogue 
systems (SDSs) used in automated call-centres, 
where the goal is to detect problems in the in-
teraction and, if appropriate, transfer the call 
automatically to a human operator. 
The remainder of the paper is organised as 
follows. Section  2 addresses related work on 
the application of emotion recognition to 
SDSs. Section  3 focuses on the proposed tech-
nique, describing the classifiers and fusion 
methods employed in the current implementa-
tion. Section  4 discusses our speech database 
and its emotional annotation. Section  5 pre-
sents the experiments, comparing results ob-
tained using the standard single fusion tech-
nique with the proposed double fusion. Finally, 
Section  6 presents the conclusions and outlines 
possibilities for future work. 
2 Related work 
Many studies can be found in the literature 
addressing potential improvements to SDSs by 
recognising user emotional states. A diversity 
of speech databases, features used for training 
and recognition, number of emotional catego-
ries, and recognition methods have been pro-
posed. For example, Batliner et al (2003) em-
ployed three different databases to detect trou-
bles in communication. One was collected 
from a single experienced actor who was told 
to express anger because of system malfunc-
tions. Other was collected from naive speakers 
who were asked to read neutral and emotional 
sentences. The third database was collected 
using a WOZ scenario designed to deliberately 
provoke user reactions to system malfunctions. 
The study focused on detecting two emotion 
categories: emotional (e.g. anger) and neutral, 
employing classifiers that dealt with prosodic, 
linguistic, and discourse information. 
281
Liscombe et al (2005) made experiments 
with a corpus of 5,690 dialogues collected with 
the ?How May I Help You? system, and con-
sidered seven emotional categories: posi-
tive/neutral, somewhat frustrated, very frus-
trated, somewhat angry, very angry, somewhat 
other negative, and very other negative. They 
employed standard lexical, prosodic and con-
textual features. 
Devillers and Vidrascu (2006) employed 
human-to-human dialogues on a financial task, 
and considered four emotional categories: an-
ger, fear, relief and sadness. Emotion classifi-
cation was carried out considering linguistic 
information and paralinguistic cues. 
Ai et al (2006) used a database collected 
from 100 dialogues between 20 students and a 
spoken dialogue tutor, and for classification 
employed lexical items, prosody, user gender, 
beginning and ending time of turns, user turns 
in the dialogue, and system/user performance 
features. Four emotional categories were con-
sidered: uncertain, certain, mixed and neutral. 
Morrison et al (2007) compared two emo-
tional speech data sources The former was col-
lected from a call-centre in which customers 
talked directly to a customer service represen-
tative. The second database was collected from 
12 non-professional actors and actresses who 
simulated six emotional categories: anger, dis-
gust, fear, happiness, sadness and surprise. 
3 The proposed technique 
The technique that we propose in this paper to 
enhance emotion recognition in SDSs consid-
ers that a set of classifiers ? = {C1, C2, ?, Cm} 
receive as input feature vectors f related to 
each sentence uttered by the user. As a result, 
each classifier generates one emotion predic-
tion, which is a vector of pairs (hi, pi), i = 1?S, 
where hi is an emotional category (e.g. Angry), 
pi is the probability of the utterance belonging 
to hi in accordance with the classifier, and S is 
the number of emotional categories considered, 
which forms the set E = {e1, e2, ?, eS}. 
The emotion predictions generated by the 
classifiers make up the input to the first fusion 
stage, which we call Fusion-0. This stage em-
ploys n fusion methods called F0i, i = 1?n, to 
generate other predictions: vectors of pairs 
(h0j,k , p0j,k), j = 1?n, k = 1?S, where h0j,k is an 
emotional category, and p0j,k is the probability 
of the utterance belonging to h0j,k in accordance 
with the fusion method F0j. 
The second fusion stage, called Fusion-1, 
receives the predictions provided by Fusion-0 
and generates the pair (h11,1 , p11,1), where h11,1 
is the emotional category with highest prob-
ability, p11,1. This emotional category is deter-
mined employing a fusion method called F11, 
and represents the user?s emotional state de-
duced by the technique. The best combination 
of fusion methods to be used in Fusion-0 (F01, 
F02,...,F0j, 1 ? j ? n) and the best fusion method 
to be used in Fusion-1 (F11) must be experi-
mentally determined.  
3.1 Classifiers 
In the current implementation our technique 
employs four classifiers, which deal with pros-
ody, acoustics, lexical items and dialogue acts 
regarding each utterance. 
3.1.1 Prosodic classifier 
The input to our prosodic classifier is an n-
dimensional feature vector obtained from 
global statistics of pitch and energy, and fea-
tures derived from the duration of 
voiced/unvoiced segments in each utterance. 
After carrying out experiments to find the ap-
propriate feature set for the classifier, we de-
cided to use the following 11 features: pitch 
mean, minimum and maximum, pitch deriva-
tives mean, mean and variance of absolute val-
ues of pitch derivatives, energy maximum, 
mean of absolute value of energy derivatives, 
correlation of pitch and energy derivatives, 
average length of voiced segments, and dura-
tion of longest monotonous segment. 
The classifier employs gender-dependent 
Gaussian Mixture Models (GMMs) to repre-
sent emotional categories. The likelihood for 
the n-dimensional feature vector (x), given an 
emotional category ?, is defined as: 
 
( ) ( )?
=
=
Q
l
ll xPwxP
1
?  
 
i.e., a weighted linear combination of Q uni-
modal Gaussian densities Pl(x). The density 
function Pl(x) is defined as: 
 
( ) ( ) ( ) ( )??
???
? ?????
?
= ? lll
l
n
l xxxP ???
1
2
1
exp
det2
1  
 
where the ?l?s are mean vectors and the ?l?s 
covariance matrices. The emotional category 
282
deduced by the classifier, h, is decided accord-
ing to the following expression: 
 ( )S
S
xPh ?maxarg=  (1) 
 
where ?S represents the models for the emo-
tional categories considered, and the max func-
tion is computed employing the EM (Expecta-
tion-Maximization) algorithm. To compute the 
probabilities pi for the emotion prediction of 
the classifier we use the following expression: 
 
?
=
=
S
k
kiip
1
/ ??  (2) 
 
where ?i is the log-likelihood of hi, S is the 
number of emotional categories considered, 
and the ?k?s are the log-likelihoods of these 
emotional categories. 
3.1.2 Acoustic classifier 
Prosodic features are nowadays among the 
most popular features for emotion recognition 
(Dellaert et al 1996; Luengo et al 2005). 
However, several authors have evaluated other 
features. For example, Nwe et al (2003) em-
ployed several short-term spectral features and 
observed that Logarithmic Frequency Power 
Coefficients (LFPCs) provide better perform-
ance than Mel-Frequency Cepstral Coefficient 
(MFCCs) or Linear Prediction Cepstral Coeffi-
cients (LPCCs). Experiments carried out with 
our speech database (which will be discussed 
in Section  4) have confirmed this observation. 
However, we have also noted that when we 
used the first and second derivatives, the best 
results were obtained for MFCCs. Hence, we 
decided to use 39-feature MFCCs (13 MFCCs, 
delta and delta-delta) for classification. 
The emotion patterns of the input utterances 
are modelled by gender-dependent GMMs, as 
with the prosodic classifier, but each input ut-
terance is represented employing a sequence of 
feature vectors x = {x1,?,xT} instead of one n-
dimensional vector. We assume mutual inde-
pendence of the feature vectors in x, and com-
pute the log-likelihood for an emotional cate-
gory ? as follows: 
 
( ) ( )?
=
=
T
t
txPxP
1
log ??  
 
The emotional category deduced by the classi-
fier, h, is decided employing Eq. (1), whereas 
Eq. (2) is used to compute the probabilities for 
the prediction, i.e. for the vector of pairs (hi, 
pi). 
3.1.3 Lexical classifier 
A number of previous studies on emotion rec-
ognition take into account information about 
the kinds of word uttered by the users, assum-
ing that there is a relationship between words 
and emotion categories. For example, swear 
words and insults can be considered as convey-
ing a negative emotion (Lee and Narayanan, 
2005). Analysis of our dialogue corpus (which 
will be discussed in Section  4) has shown that 
users did not utter swear words or insults dur-
ing the interaction with the Saplen system. 
Nevertheless, there were particular moments in 
the interaction at which their emotional state 
changed from Neutral to Tired or Angry. These 
moments correspond to dialogue states where 
the system had problems in recognising the 
sentences uttered by the users. 
The reasons for these problems are basically 
two. On the one hand, most users spoke with 
strong southern Spanish accents, characterised 
by the deletion of the final s of plural words, 
and an exchange of the phonemes s and c in 
many words. On the other hand, there are 
words in the system?s vocabulary that are very 
similar acoustically. 
Hence, our goal has been to automatically 
find these words by means of a study of the 
speech recognition results, and deduce the 
emotional category for each input utterance 
from the emotional information associated 
with the words in the recognition result. To do 
this we have followed the study of Lee and 
Narayanan (2005), which employs the infor-
mation-theoretic concept of emotional sali-
ence. The emotional salience of a word for a 
given emotional category can be defined as the 
mutual information between the word and the 
emotional category. Let W be a sentence 
(speech recognition result) comprised of a se-
quence of n words: W = w1 w2 ?wn, and E a 
set of emotional categories, E = {e1, e2, ? ,eS}. 
The mutual information between the word wi 
and an emotional category ej is defined as fol-
lows: 
 ( )
)(
|
log),(_
j
ij
ji eP
weP
ewnInformatiomutual =  
283
 
where P(ej | wi) is the posterior probability that 
a sentence containing the word wi implies the 
emotional category ej, and P(ej) represents the 
prior probability of the emotional category. 
Taking into account the previous definitions, 
we have defined the emotional salience of the 
word wi for an emotional category ej as fol-
lows: 
 
),(_)|(
),(
jiij
ji
ewnInformatiomutualweP
ewsalience
?
=
 
 
After the salient words for each emotional 
category have been identified employing a 
training corpus, we can carry out emotion rec-
ognition at the sentence level, considering that 
each word in the sentence is independent of the 
rest. The goal is to map the sentence W to any 
of the emotional categories in E. To do this, we 
compute an activation value ak for each emo-
tional category as follows: 
 
?
=
+=
n
m
kmkmk wwIa
1
 
 
where k = 1?S, n is the number of words in 
W, Im represents an indicator that has the value 
1 if wk is a salient word for the emotional cate-
gory (i.e. salience(wi,ej) ? 0) and the value 0 
otherwise; wmk is the connection weight be-
tween the word and the emotional category, 
and wk represents bias. We define the connec-
tion weight wmk as:  
 
),(_ kmmk ewnInformatiomutualw =  
 
whereas the bias is computed as: 
)(log kk ePw = . Finally, the emotional cate-
gory deduced by the classifier, h, is the one 
with highest activation value ak: 
 
)max(arg k
k
ah =  
 
To compute the probabilities pi?s for the emo-
tion prediction, we use the following expres-
sion: 
?
=
=
S
j
jii aap
1
/  
 
where ai represents the activation value of hi, 
and the aj?s are the activation values of the S 
emotional categories considered. 
3.1.4 Dialogue acts classifier 
A dialogue act can be defined as the function 
performed by an utterance within the context 
of a dialogue, for example, greeting, closing, 
suggestion, rejection, repeat, rephrase, confir-
mation, specification, disambiguation, or help 
(Batliner et al 2003; Lee and Narayanan, 
2005; Liscombe et al 2005). 
Our dialogue acts classifier is inspired by 
the study of Liscombe et al (2005), where the 
sequential structure of each dialogue is mod-
elled by a sequence of dialogue acts. A differ-
ence is that they assigned one or more labels 
related to dialogue acts to each user utterance, 
and did not assign labels to system prompts, 
whereas we assign just one label to each sys-
tem prompt and none to user utterances. This 
decision is made from the examination of our 
dialogue corpus. We have observed that users 
got tired or angry if the system generated the 
same prompt repeatedly (i.e. repeated the same 
dialogue act) to try to get a particular data 
item. For example, if it had difficulty in obtain-
ing a telephone number then it employed sev-
eral dialogue turns to obtain the number and 
confirm it, which annoyed the users, especially 
if they had employed other turns previously to 
correct misunderstandings. Hence, our dia-
logue act classifier aims to predict these nega-
tive emotional states by detecting successive 
repetitions of the same system?s prompt types 
(e.g. prompts to get the telephone number). 
In accordance with our approach, the emo-
tional category of a user?s dialogue turn, En, is 
that which maximises the posterior probability 
given a sequence of the most recent system 
prompts: 
 
),...,,|(maxarg 13)12*( ????= nnLnk
k
n DADADAEPE
 
where the prompt sequence is represented by a 
sequence of dialogue acts (DAi?s) and L is the 
length of the sequence, i.e. the number of sys-
tem?s dialogue turns in the sequence. Note that 
if L = 1 then the decision about En depends 
only on the previous system prompt. In other 
words, the emotional category obtained is that 
with the greatest probability given just the pre-
vious system turn in the dialogue. The prob-
ability of the considered emotional categories 
284
given a sequence of dialogue acts is obtained 
by employing a training dialogue corpus. 
By means of this equation, we decide the 
most likely emotional category for the input 
utterance, selecting the category with the high-
est probability given the sequence of dialogue 
acts of length L. This probability is used to 
create the pair (hi, pi) to be included in the 
emotion prediction. 
3.2 Fusion methods 
In the current implementation our technique 
employs the three fusion methods discussed in 
this section. When used in Fusion-0, these 
methods are employed to combine the predic-
tions provided by the classifiers. When used in 
Fusion-1, they are used to combine the predic-
tions generated by Fusion-0. 
3.2.1 Average of probabilities (AP) 
This method combines the predictions by aver-
aging their probabilities. To do this we con-
sider that each input utterance is represented 
by feature vectors x1,?,xm from feature spaces 
X1,?,Xm, where m is the number of classifiers. 
We also assume that each input utterance be-
longs to one of S emotional categories hi, i = 
1?S. In each of the m feature spaces a classi-
fier can be created that approximates the poste-
rior probability P(hi | x
k) as follows: 
 
)()|()( kki
k
i
kk
i xxhPxf ?+=  
 
where )( kki x?  is the error made by classifier 
k. We estimate P(hi | x
k) by )( kki xf  and as-
suming a zero-mean error for )( kki x? , we av-
erage all the )( kki xf ?s to obtain a less error-
sensitive estimation. In this way we obtain the 
following mean combination rule to decide the 
most likely emotional category: 
 
?
=
=
m
k
kk
i
m
i xfm
xxhP
1
1 )(
1
),...,|(  
3.2.2 Multiplication of probabilities (MP) 
Assuming that the feature spaces X1,?,Xm are 
different and independent, the probabilities can 
be written as follows: 
 
)|(...)|()|(
)|,...,(
21
1
i
m
ii
i
m
hxPhxPhxP
hxxP
???
=
 
 
Using Bayes rule we can obtain the following 
equation, which we use to decide the most 
likely emotional category for each input utter-
ance (represented as feature vectors x1,?,xm): 
 
? ?
?
??
?
??
?= ?
?
'
1
'
'
'
'
1
1
)(/)|(
)(/)|(
),...,|(
i
m
i
k
k
i
k
m
i
k
i
m
i
hPxhP
hPxhP
xxhP
 
3.2.3 Unweighted vote (UV) 
This method combines the emotion predictions 
by counting the number of classifiers (if used 
in Fusion-0) or fusion methods (if used in Fu-
sion-1) that consider an emotional category hi 
as the most likely for the input utterance. If we 
consider three emotional categories X, Y and Z, 
hi is decided as follows: 
 
?
?
?
?
?
?
?
?
?
??
??
??
=
? ? ? ?
? ? ??
? ? ??
= = = =
= = ==
= = ==
m
j
m
j
m
j
m
j
jjij
m
j
m
j
m
j
jj
m
j
jj
m
j
m
j
m
j
jj
m
j
jj
i
YZandXZifZ
ZYandXYifY
ZXandYXifX
h
1 1 1 1
1 1 11
1 1 11
 
 
where m is the number of classifiers or fusion 
methods employed (e.g., in our experiments, X 
= Neutral, Y = Tired and Z = Angry). The 
probability pi for hi to be included in the emo-
tion prediction is computed as follows: 
 
?
=
=
3
1
/),,|(
j
jii VhVhZYXhP  
 
where Vhi is the number of votes for hi, and the 
Vhj?s are the number of votes for the 3 emo-
tional categories. If we consider two emotional 
categories X and Y, the most likely emotional 
category hi and its probability pi are analo-
gously computed (e.g., in our experiments, X = 
Non-negative and Y = Negative). 
4 Emotional speech database 
Our emotional speech database has been con-
structed from a corpus of 440 telephone-based 
dialogues between students of the University 
of Granada and the Saplen system, which was 
285
previously developed in our lab for the fast 
food domain (L?pez-C?zar et al 1997; L?pez-
C?zar and Callejas, 2006). Each dialogue was 
stored in a log file in text format that includes 
each system prompt (e.g. Would you like to 
drink anything?), the type of prompt (e.g. Any-
FoodOrDrinkToOrder?), the name of the voice 
samples file (utterance) that stores the user re-
sponse to the prompt, and the speech recogni-
tion result for the utterance. The dialogue cor-
pus contains 7,923 utterances, 50.3% of which 
were recorded by male users and the remaining 
by female users. 
The utterances have been annotated by 4 la-
bellers (2 male and 2 female). The order of the 
utterances has been randomly chosen to avoid 
influencing the labellers by the situation in the 
dialogues, thus minimising the effect of dis-
course context. The labellers have initially as-
signed one label to each utterance, either 
<NEUTRAL>, <TIRED> or <ANGRY> according 
to the perceived emotional state of the user. 
One of these labels has been finally assigned to 
each utterance according to the majority opin-
ion of the labellers, so that 81% of the utter-
ances are annotated as ?Neutral?, 9.5% as 
?Tired? and 9.4% as ?Angry?. This shows that 
the database is clearly unbalanced in terms of 
emotional categories. 
To measure the amount of agreement be-
tween the labellers we employed the Kappa 
statistic (K), which is computed as follows 
(Cohen, 1960): 
 
)(1
)()(
EP
EPAP
K ?
?=  
 
where P(A) is the proportion of times that the 
labellers agree, and P(E) is the proportion of 
times we would expect the labellers to agree by 
chance. We obtained that K = 0.48 and K = 
0.45 for male and female labellers, respec-
tively, which according to Landis and Koch 
(1977) represents moderate agreement. 
5 Experiments 
The main goal of the experiments has been to 
test the proposed technique using our emo-
tional speech database, and employing: 
 
? Three emotional categories (Neutral, An-
gry and Tired) on the one hand, and two 
emotional categories (Non-negative and 
Negative) on the other. The experiments 
employing the former category set will be 
called 3-emotion experiments, whereas 
those employing the latter category will be 
called 2-emotion experiments. 
? The four classifiers described in Section 
 3.1, and the three fusion methods dis-
cussed in Section  3.2. 
 
In the 3-emotion experiments we consider that 
an input utterance is correctly classified if the 
emotional category deduced by the technique 
matches the label assigned to the utterance. In 
the 2-emotion experiments, the utterance is 
considered to be correctly classified if either 
the deduced emotional category is Non-
negative and the label is Neutral, or the cate-
gory is Negative and the label is Tired or An-
gry. 
To carry out training and testing we have 
used a script that takes as its input a set of la-
belled dialogues in a corpus, and processes 
each dialogue by locating within it, from the 
beginning to the end, each prompt of the 
Saplen system, the voice samples file that con-
tains the user?s response to the prompt, and the 
result provided by the system?s speech recog-
niser (sentence in text format). The type of 
each prompt is used to create a sequence of 
dialogue acts of length L, which is the input to 
the dialogue acts classifier. The voice samples 
file is the input to the prosodic and acoustic 
classifiers, and the speech recognition result is 
the input to the lexical classifier. This proce-
dure is repeated for all the dialogues in the cor-
pus. 
Experimental results have been obtained us-
ing 5-fold cross-validation, with each partition 
containing the utterances corresponding to 88 
different dialogues in the corpus. 
5.1 Performance of Fusion-0 
Table 1 sets out the average results obtained 
for Fusion-0 considering several combinations 
of the classifiers and employing the three fu-
sion methods. As can be observed, MP is the 
best fusion method, with average classification 
rates of 89.08% and 87.43% for the 2 and 3 
emotion experiments, respectively. The best 
classification rates (92.23% and 90.67%) are 
obtained by employing the four classifiers, 
which  means that the four types of informa-
tion considered (acoustic, prosodic, lexical and 
related to dialogue acts) are really useful to 
enhance classification rates. 
 
286
Fusion 
Method 
Classifiers 2 emot. 3 emot. 
Aco, Pro 84.15 82.46 
Lex, Pro 85.04 82.71 
DA, Pro 90.49 87.48 
Aco, Lex, Pro 89.20 86.17 
Aco, DA, Pro 90.24 88.56 
DA, Lex, Pro 90.02 88.02 
Aco, DA, Lex, Pro 90.49 88.32 
AP 
Average 88.66 86.25 
Aco, Pro 84.15 82.86 
Lex, Pro 85.16 83.71 
DA, Pro 91.49 89.78 
Aco, Lex, Pro 89.17 87.91 
Aco, DA, Pro 91.33 89.23 
DA, Lex, Pro 90.06 87.82 
Aco, DA, Lex, Pro 92.23 90.67 
MP 
Average 89.08 87.43 
Aco, Pro 88.64 85.19 
Lex, Pro 86.40 83.01 
DA, Pro 88.20 84.92 
Aco, Lex, Pro 88.76 85.54 
Aco, DA, Pro 88.91 85.89 
DA, Lex, Pro 88.47 85.61 
Aco, DA, Lex, Pro 89.04 87.56 
UV 
Average 88.35 85.39 
 
Table 1: Performance of Fusion-0 (results in %). 
5.2 Performance of Fusion-1 
Table 2 shows the average results obtained 
when Fusion-1 is used to combine the predic-
tions of Fusion-0. The three fusion methods are 
tested in Fusion-1, with Fusion-0 employing 
four combinations of these methods: AP,MP; 
AP,UV; MP,UV; and AP,MP,UV. In all cases 
Fusion-0 uses the four classifiers as this is the 
configuration that provides the highest classifi-
cation accuracy according to the previous sec-
tion. 
Comparison of both tables shows that Fu-
sion-1 clearly outperforms Fusion-0. The best 
results are attained for MP, which means that 
this method is preferable when the data contain 
small errors (emotion predictions generated by 
Fusion-0 with accuracy rates around 90%). 
To find the reasons for these enhancements 
we have analysed the confusion matrix of Fu-
sion-1 using MP. The study reveals that for the 
2-emotion experiments this fusion stage works 
very well in predicting the Non-negative cate-
gory, very slightly enhancing the classification 
rate of Fusion-0 (96.58% vs. 95.93%), whereas 
the classification rate of the Negative category 
is the same as that obtained by Fusion-0 
(88.91%). Overall, the best performance of 
Fusion-1 employing MP (94.48%) outdoes that 
of Fusion-0 employing AP (90.49%) and MP 
(92.23%). 
Regarding the 3-emotion experiments, our 
analysis shows that using MP, Fusion-1 
slightly lowers the classification rate of the 
Neutral category obtained by Fusion-0 
(97.79% vs. 97.9%), but slightly raises the rate 
of the Tired category (93.62% vs. 93.26%), 
and the Angry category (77.49% vs. 76.81%). 
Overall, the performance of Fusion-1 employ-
ing MP (94.02%) outdoes that of Fusion-0 em-
ploying AP (88.32%) and MP (90.67%). 
 
 
Fusion methods 
used in Fusion-0 
Fusion method 
used in Fusion-1 
(2 emotions) 
Fusion method 
used in Fusion-1 
(3 emotions) 
 AP MP UV AP MP UV 
AP,MP 93.68 94.48 93.53 91.77 94.02 90.96 
AP,UV 93.20 93.23 93.20 91.65 93.13 90.10 
MP,UV 93.34 94.38 93.20 91.27 93.98 89.48 
AP,MP,UV 93.23 94.36 93.17 91.57 93.97 89.06 
Average 93.40 94.11 93.28 91.57 93.78 89.90 
 
Table 2: Performance of Fusion-1 (results in %). 
 
6 Conclusions and future work 
Our experimental results show that the pro-
posed technique is useful to improve the classi-
fication rates of the standard fusion technique, 
which employs just one fusion stage. Compar-
ing results in Table 1 and Table 2 we can ob-
serve that for the 2-emotion experiments, Fu-
sion-1 enhances Fusion-0 by 2.25% absolute 
(from 92.23% to 94.48%), while for the 3-
emotion experiments, the improvement is 
3.35% absolute (from 90.67% to 94.02%). 
These improvements are obtained by employ-
ing AP and MP in Fusion-0 to combine the 
emotion predictions of the four classifiers, and 
using MP in Fusion-1 to combine the outputs 
of Fusion-0. 
The reason for these improvements is that 
the double fusion process (Fusion-0 and Fu-
sion-1) allows us to benefit from the advan-
287
tages of using different methods to combine 
information. According to our results, the best 
methods are AP and MP. The former allows 
gaining maximally from the independent data 
representation available, which are the input to 
Fusion-0 (in our study, prosody, acoustics, 
speech recognition errors, and dialogue acts). 
The latter provides better results when the data 
contain small errors, which occurs when the 
predictions provided by Fusion-0 are the input 
to Fusion-1. 
Future work will include testing the tech-
nique employing information sources not con-
sidered in this study. The sources we have 
dealt with in the experiments (prosodic, acous-
tic, lexical, and dialogue acts) are those most 
commonly employed in previous studies. 
However, there are also studies that suggest 
using other information sources, such as speak-
ing style, subject and problem identification, 
and non-verbal cues. 
Another future work is to test the technique 
employing other methods for classification and 
information fusion. For example, it is known 
that people are usually confused when they try 
to determine the emotional state of a speaker, 
given that the difference between some emo-
tions is not always clear. Hence, it would be 
interesting to investigate the performance of 
the technique employing classification algo-
rithms that deal with this vague boundary, such 
as fuzzy inference methods, and using boosting 
methods for improving the accuracy of the 
classifiers. 
Finally, in terms of application of the tech-
nique to improve the system-user interaction, 
we will evaluate different dialogue manage-
ment strategies to enable the system?s adapta-
tion to negative emotional states of users (Uni-
versity students). For example, a dialogue 
management strategy could be as follows: i) if 
the emotional state is Tired begin the following 
prompt apologising, and transfer the call to a 
human operator if this state is recognised twice 
consecutively, and ii) if the emotional state is 
Angry apologise and transfer the call to a hu-
man operator immediately. 
 
Acknowledgments 
This research has been funded by Spanish pro-
ject HADA TIN2007-64718, and the Czech 
Grant Agency project no. 102/08/0707. 
 
References  
Ai, H., Litman, D. J., Forbes-Riley, K., Rotaru, M., 
Tetreault, J., Purandare, A. 2006. Using system 
and user performance features to improve emo-
tion detection in spoken tutoring systems. Proc. 
of Interspeech, pp. 797-800. 
Batliner, A., Fischer, K., Huber, R., Spilker, J., 
N?th, E. 2003. How to find trouble in communi-
cation. Speech Communication, vol. 40, pp. 117-
143. 
Cohen, J. 1960. A coefficient of agreement for 
nominal scales. Educat. Psychol. Measurement, 
vol. 20, pp. 37-46. 
Dellaert, F., Polzin, T., Waibel, A. 1996. Recogniz-
ing emotion in speech. Proc. of ICSLP, pp. 
1970-1973. 
Devillers, L., Vidrascu, L. 2006. Real-life emotions 
detection with lexical and paralinguistic cues on 
human-human call center dialogs. Proc. of Inter-
speech, pp. 801-804. 
Landis, J. R., Koch, G. G. 1977. The measurement 
of observer agreement for categorical data. Bio-
metrics, vol. 33, pp. 159-174. 
Lee, C. M., Narayanan, S. S. 2005. Toward detect-
ing emotions in spoken dialogs. IEEE Transac-
tions on Speech and Audio Processing, vol. 
13(2), pp. 293-303. 
Liscombe, J., Riccardi, G., Hakkani-T?r, D. 2005. 
Using context to improve emotion detection in 
spoken dialogue systems. Proc. of Interspeech, 
pp. 1845-1848. 
L?pez-C?zar, R., Garc?a, P., D?az, J., Rubio, A. J. 
1997. A voice activated dialog system for fast-
food restaurant applications. Proc. of Eu-
rospeech, pp. 1783-1786. 
L?pez-C?zar, R., Callejas, Z. 2006. Combining 
Language Models in the Input Interface of a 
Spoken Dialogue System. Computer Speech and 
Language, 20, pp. 420-440. 
Luengo, I., Navas, E., Hern?ez, I, Sanchez, J. 2005. 
Automatic emotion recognition using prosodic 
parameters. Proc. of Interspeech, pp.493-496. 
Morrison, D., Wang, R., De Silva, L. C. 2007. En-
semble methods for spoken emotion recognition 
in call-centres. Speech Communication, vol. 
49(2) pp. 98-112. 
Nwe, T. L., Foo, S. V., De Silva, L. C. 2003. 
Speech emotion recognition using hidden 
Markov models. Speech Communication, vol. 
41(4), pp. 603-623. 
288
