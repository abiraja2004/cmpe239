Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1092?1100,
Beijing, August 2010
Dependency Forest for Statistical Machine Translation
Zhaopeng Tu ? Yang Liu ? Young-Sook Hwang ? Qun Liu ? Shouxun Lin ?
?Key Lab. of Intelligent Info. Processing ?HILab Convergence Technology Center
Institute of Computing Technology C&I Business
Chinese Academy of Sciences SKTelecom
{tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn yshwang@sktelecom.com
Abstract
We propose a structure called dependency
forest for statistical machine translation.
A dependency forest compactly represents
multiple dependency trees. We develop
new algorithms for extracting string-to-
dependency rules and training depen-
dency language models. Our forest-based
string-to-dependency system obtains sig-
nificant improvements ranging from 1.36
to 1.46 BLEU points over the tree-based
baseline on the NIST 2004/2005/2006
Chinese-English test sets.
1 Introduction
Dependency grammars have become increasingly
popular in syntax-based statistical machine trans-
lation (SMT). One important advantage of depen-
dency grammars is that they directly capture the
dependencies between words, which are key to re-
solving most parsing ambiguities. As a result, in-
corporating dependency trees proves to be effec-
tive in improving statistical machine translation
(Quirk et al, 2005; Ding and Palmer, 2005; Shen
et al, 2008).
However, most dependency-based translation
systems suffer from a major drawback: they only
use 1-best dependency trees for rule extraction,
dependency language model training, and decod-
ing, which potentially introduces translation mis-
takes due to the propagation of parsing errors
(Quirk and Corston-Oliver, 2006). While the
treelet system (Quirk et al, 2005) takes a de-
pendency tree as input, the string-to-dependency
system (Shen et al, 2008) decodes on a source-
language string. However, as we will show, the
string-to-dependency system still commits to us-
ing degenerate rules and dependency language
models learned from noisy 1-best trees.
To alleviate this problem, an obvious solu-
tion is to offer more alternatives. Recent studies
have shown that SMT systems can benefit from
widening the annotation pipeline: using packed
forests instead of 1-best trees (Mi and Huang,
2008), word lattices instead of 1-best segmenta-
tions (Dyer et al, 2008), and weighted alignment
matrices instead of 1-best alignments (Liu et al,
2009).
Along the same direction, we propose a struc-
ture called dependency forest, which encodes ex-
ponentially many dependency trees compactly, for
dependency-based translation systems. In this pa-
per, we develop two new algorithms for extracting
string-to-dependency rules and for training depen-
dency language models, respectively. We show
that using the rules and dependency language
models learned from dependency forests leads to
consistent and significant improvements over that
of using 1-best trees on the NIST 2004/2005/2006
Chinese-English test sets.
2 Background
Figure 1 shows a dependency tree of an English
sentence he saw a boy with a telescope. Arrows
point from the child to the parent, which is often
referred to as the head of the child. For example,
in Figure 1, saw is the head of he. A dependency
tree is more compact than its constituent counter-
part because there is no need to build a large su-
perstructure over a sentence.
Shen et al (2008) propose a novel string-to-
dependency translation model that features two
important advantages. First, they define that
a string-to-dependency rule must have a well-
formed dependency structure on the target side,
which makes efficient dynamic programming pos-
sible and manages to retain most useful non-
constituent rules. A well-formed structure can be
either fixed or floating . A fixed structure is a
1092
saw
he boy with
a telescope
a
he saw a boy with a telescope
ta kandao yige dai wangyuanjing de nanhai
Figure 1: A training example for tree-based rule
extraction.
dependency tree with all the children complete.
Floating structures consist of sibling nodes of a
common head, but the head itself is unspecified
or floating. For example, Figure 2(a) and Figure
2(b) are two fixed structures while Figure 2(c) is a
floating one.
Formally, for a given sentence w1:l = w1 . . . wl,
d1 . . . dl represent the parent word IDs for each
word. If wi is a root, we define di = 0.
Definition 1. A dependency structure di..j is fixed
on head h, where h /? [i, j], or fixed for short, if
and only if it meets the following conditions
? dh /? [i, j]
? ?k ? [i, j] and k 6= h, dk ? [i, j]
? ?k /? [i, j], dk = h or dk /? [i, j]
Definition 2. A dependency structure di..j is
floating with children C, for a non-empty set C
? {i, ..., j}, or floating for short, if and only if it
meets the following conditions
? ?h /? [i, j], s.t.?k ? C, dk = h
? ?k ? [i, j] and k /? C, dk ? [i, j]
? ?k /? [i, j], dk /? [i, j]
A dependency structure is well-formed if and
only if it is either fixed or floating.
2.1 Tree-based Rule Extraction
Figure 1 shows a training example consisting of an
English dependency tree, its Chinese translation,
boy
a
(a)
with
telescope
a
(b)
boy with
a telescope
a
(c)
Figure 2: Well-formed dependency structures cor-
responding to Figure 1. (a) and (b) are fixed and
(c) is floating.
and the word alignments between them. To facil-
itate identifying the correspondence between the
English and Chinese words, we also gives the En-
glish sentence. Extracting string-to-dependency
rules from aligned string-dependency pairs is sim-
ilar to extracting SCFG (Chiang, 2007) except that
the target side of a rule is a well-formed struc-
ture. For example, we can first extract a string-to-
dependency rule that is consistent with the word
alignment (Och and Ney, 2004):
with ((a) telescope) ? dai wangyuanjing de
Then a smaller rule
(a) telescope ? wangyuanjing
can be subtracted to obtain a rule with one non-
terminal:
with (X1) ? dai X1 de
where X is a non-terminal and the subscript indi-
cates the correspondence between non-terminals
on the source and target sides.
2.2 Tree-based Dependency Language Model
As dependency relations directly model the se-
mantics structure of a sentence, Shen et al (2008)
introduce dependency language model to better
account for the generation of target sentences.
Compared with the conventional n-gram language
models, dependency language model excels at
capturing non-local dependencies between words
(e.g., saw ... with in Figure 1). Given a depen-
dency tree, its dependency language model prob-
ability is a product of three sub-models defined
between headwords and their dependants. For ex-
ample, the probability of the tree in Figure 1 can
1093
saw0,7
he0,1 boy2,4 with4,7
a2,3 telescope5,7
a5,6
(a)
saw0,7
he0,1 boy2,7
a2,3 with4,7
telescope5,7
a5,6
(b)
saw0,7
he0,1 boy2,4 boy2,7
with4,7
e1 e2
a2,3
e3 e4
telescope5,7
e5
a5,6
e6
(c)
Figure 3: (a) the dependency tree in Figure 1, (b) another dependency tree for the same sentence, and
(c) a dependency forest compactly represents the two trees.
be calculated as:
Prob = PT (saw)
?PL(he|saw-as-head)
?PR(boy|saw-as-head)
?PR(with|boy, saw-as-head)
?PL(a|boy-as-head)
?PR(telescope|with-as-head)
?PL(a|telescope-as-head)
where PT (x) is the probability of word x being
the root of a dependency tree. PL and PR are the
generative probabilities of left and right sides re-
spectively.
As the string-to-tree system relies on 1-best
trees for parameter estimation, the quality of rule
table and dependency language model might be
affected by parsing errors and therefore ultimately
results in translation mistakes.
3 Dependency Forest
We propose to encode multiple dependency trees
in a compact representation called dependency
forest, which offers an elegant solution to the
problem of parsing error propagation.
Figures 3(a) and 3(b) show two dependency
trees for the example English sentence in Figure
1. The prepositional phrase with a telescope could
either depend on saw or boy. Figure 3(c) is a
dependency forest compactly represents the two
trees by sharing common nodes and edges.
Each node in a dependency forest is a word.
To distinguish among nodes, we attach a span to
each node. For example, in Figure 1, the span of
the first a is (2, 3) because it is the third word in
the sentence. As the fourth word boy dominates
the node a2,3, it can be referred to as boy2,4. Note
that the position of boy itself is taken into consid-
eration. Similarly, the word boy in Figure 3(b) can
be represented as boy2,7.
The nodes in a dependency forest are connected
by hyperedges. While an edge in a dependency
tree only points from a dependent to its head, a
hyperedge groups all the dependants that have a
common head. For example, in Figure 3(c), the
hyperedge
e1: ?(he0,1, boy2,4,with4,7), saw0,7?
denotes that he0,1, boy2,4, and with4,7 are depen-
dants (from left to right) of saw0,7.
More formally, a dependency forest is a pair
?V,E?, where V is a set of nodes, and E
is a set of hyperedges. For a given sentence
w1:l = w1 . . . wl, each node v ? V is in the
form of wi,j , which denotes that w dominates
the substring from positions i through j (i.e.,
wi+1 . . . wj). Each hyperedge e ? E is a pair
?tails(e), head(e)?, where head(e) ? V is the
head and tails(e) ? V are its dependants.
A dependency forest has a structure of a hy-
pergraph such as packed forest (Klein and Man-
ning, 2001; Huang and Chiang, 2005). However,
while each hyperedge in a packed forest naturally
treats the corresponding PCFG rule probability as
its weight, it is challenging to make dependency
forest to be a weighted hypergraph because depen-
dency parsers usually only output a score, which
can be either positive or negative, for each edge
in a dependency tree rather than a hyperedge in a
1094
saw0,7
he0,1 boy2,4 boy2,7
with4,7
e1 e2
a2,3
e3 e4
telescope5,7
e5
a5,6
e6
he saw a boy with a telescope
ta kandao yige dai wangyuanjing de nanhai
Figure 4: A training example for forest-based rule
extraction.
dependency forest. For example, in Figure 3(a),
the scores for the edges he ? saw, boy ? saw,
and with ? saw could be 13, 22, and -12, respec-
tively.
To assign a probability to each hyperedge, we
can first obtain a positive number for a hyperedge
using the scores of the corresponding edges:1
c(e) = exp
(?
v?tails(e) s
(
v, head(e)
)
|tails(e)|
)
(1)
where c(e) is the count of a hyperedge e, head(e)
is a head, tails(e) is a set of dependants of the
head, v is one dependant, and s(v, head(e)) is the
score of an edge from v to head(e). For example,
the count of the hyperedge e1 in Figure 3(c) is
c(e1) = exp
(
13 + 22 ? 12
3
)
(2)
Then, the probability of a hyperedge can be ob-
tained by normalizing the count among all hyper-
edges with the same head collected from a training
corpus:
p(e) = c(e)?
e?:head(e?)=head(e) c(e?)
(3)
Therefore, we obtain a weighted dependency
forest in which each hyperedge has a probability.
1It is difficult to assign a probability to each hyperedge.
The current method is arbitrary, and we will improve it in the
future.
Algorithm 1 Forest-based Initial Phrase Extrac-
tion
Input: a source sentence ?, a forest F , an alignment a,
and k
Output: minimal initial phrase setR
1: for each node v ? V in a bottom-up order do
2: for each hyperedge e ? E and head(e) = v do
3: W ? ?
4: fixs? EnumFixed(v,modifiers(e))
5: floatings? EnumFloating(modifiers(e))
6: add structures fixs, floatings to W
7: for each ? ?W do
8: if ? is consistent with a then
9: generate a rule r
10: R.append(r)
11: keep k-best dependency structures for v
4 Forest-based Rule Extraction
In tree-based rule extraction, one just needs to first
enumerate all bilingual phrases that are consis-
tent with word alignment and then check whether
the dependency structures over the target phrases
are well-formed. However, this algorithm fails to
work in the forest scenario because there are usu-
ally exponentially many well-formed structures
over a target phrase.
The GHKM algorithm (Galley et al, 2004),
which is originally developed for extracting tree-
to-string rules from 1-best trees, has been suc-
cessfully extended to packed forests recently (Mi
and Huang, 2008). The algorithm distinguishes
between minimal and composed rules. Although
there are exponentially many composed rules, the
number of minimal rules extracted from each node
is rather limited (e.g., one or zero). Therefore, one
can obtain promising composed rules by combin-
ing minimal rules.
Unfortunately, the GHKM algorithm cannot be
applied to extracting string-to-dependency rules
from dependency forests. This is because the
GHKM algorithm requires a complete subtree to
exist in a rule while neither fixed nor floating de-
pendency structures ensure that all dependants of
a head are included. For example, the floating
structure shown in Figure 2(c) actually contains
two trees.
Alternatively, our algorithm searches for well-
formed structures for each node in a bottom-up
style. Algorithm 1 shows the algorithm for ex-
tracting initial phrases, that is, rules without non-
1095
terminals from dependency forests. The algorithm
maintains k-best well-formed structures for each
node (line 11). The well-formed structures of a
head can be constructed from those of its depen-
dants. For example, in Figure 4, as the fixed struc-
ture rooted at telescope5,7 is
(a) telescope
we can obtain a fixed structure rooted for the node
with4,7 by attaching the fixed structure of its de-
pendant to the node (EnumFixed in line 4). Figure
2(b) shows the resulting fixed structure.
Similarly, the floating structure for the node
saw0,7 can be obtained by concatenating the fixed
structures of its dependants boy2,4 and with4,7
(EnumFloating in line 5). Figure 2(c) shows the
resulting fixed structure. The algorithm is similar
to Wang et al (2007), which binarize each con-
stituent node to create some intermediate nodes
that correspond to the floating structures.
Therefore, we can find k-best fixed and float-
ing structures for a node in a dependency forest
by manipulating the fixed structures of its depen-
dants. Then we can extract string-to-dependency
rules if the dependency structures are consistent
with the word alignment.
How to judge a well-formed structure extracted
from a node is better than others? We follow Mi
and Huang (2008) to assign a fractional count to
each well-formed structure. Given a tree fragment
t, we use the inside-outside algorithm to compute
its posterior probability:
??(t) = ?(root(t)) ?
?
e?t
p(e)
?
?
v?leaves(t)
?(v) (4)
where root(t) is the root of the tree, e is an edge,
leaves(t) is a set of leaves of the tree, ?(?) is out-
side probability, and ?(?) is inside probability.
For example, the subtree rooted at boy2,7 in Fig-
ure 4 has the following posterior probability:
?(boy2,7) ? p(e4) ? p(e5)
?p(e6) ? ?(a2,3) ? ?(a5,6) (5)
Now the fractional count of the subtree t is
c(t) = ??(t)??(TOP ) (6)
where TOP denotes the root node of the forest.
As a well-formed structure might be non-
constituent, we approximate the fractional count
by taking that of the minimal constituent tree frag-
ment that contains the well-formed structure. Fi-
nally, the fractional counts of well-formed struc-
tures can be used to compute the relative frequen-
cies of the rules having them on the target side (Mi
and Huang, 2008):
?(r|lhs(r)) = c(r)?
r?:lhs(r?)=lhs(r) c(r?)
(7)
?(r|rhs(r)) = c(r)?
r?:rhs(r?)=rhs(r) c(r?)
(8)
Often, our approach extracts a large amount of
rules from training corpus as we usually retain ex-
ponentially many well-formed structures over a
target phrase. To maintain a reasonable rule ta-
ble size, we discard any rule that has a fractional
count lower that a threshold t.
5 Forest-based Dependency Language
Model Training
Dependency language model plays an important
role in string-to-dependency system. Shen et
al. (2008) show that string-to-dependency system
achieves 1.48 point improvement in BLEU along
with dependency language model, while no im-
provement without it. However, the string-to-
dependency system still commits to using depen-
dency language model from noisy 1-best trees.
We now turn to dependency forest for it encodes
multiple dependency trees.
To train a dependency language model from a
dependency forest, we need to collect all heads
and their dependants. This can be easily done by
enumerating all hyperedges. Similarly, we use the
inside-outside algorithm to compute the posterior
probability of each hyperedge e,
??(e) = ?(head(e)) ? p(e)
?
?
v?tailes(e)
?(v) (9)
For example, the posterior probability of the hy-
peredge e2 in Figure 4 is calculated as
??(e2) = ?(saw0,7) ? p(e2)
??(he0,1) ? ?(boy2,7) (10)
1096
Rule DepLM NIST 2004 NIST 2005 NIST 2006 time
tree tree 33.97 30.21 30.73 19.6
tree forest 34.42? 31.06? 31.37? 24.1
forest tree 34.60? 31.16? 31.45? 21.7
forest forest 35.33?? 31.57?? 32.19?? 28.5
Table 1: BLEU scores and average decoding time (second/sentence) on the Chinese-English test sets.
The baseline system (row 2) used the rule table and dependency language model learned both from
1-best dependency trees. We use ? *? and ?**? to denote a result is better than baseline significantly at
p < 0.05 and p < 0.01, respectively.
Then, we can obtain the fractional count of a
hyperedge e,
c(e) = ??(e)??(TOP ) (11)
Each n-gram (e.g., ?boy-as-head a?) is assigned
the same fractional count of the hyperedge it be-
longs to.
We also tried training dependency language
model as in (Shen et al, 2008), which means
all hyperedges were on equal footing without re-
garding probabilities. However, the performance
is about 0.8 point lower in BLEU. One possbile
reason is that hyperedges with probabilities could
distinguish high quality structures better.
6 Experiments
6.1 Results on the Chinese-English Task
We used the FBIS corpus (6.9M Chinese words
+ 8.9M English words) as our bilingual train-
ing corpus. We ran GIZA++ (Och and Ney,
2000) to obtain word alignments. We trained a
4-gram language model on the Xinhua portion
of GIGAWORD corpus using the SRI Language
Modeling Toolkit (Stolcke, 2002) with modi-
fied Kneser-Ney smoothing (Kneser and Ney,
1995). We optimized feature weights using the
minimum error rate training algorithm (Och and
Ney, 2002) on the NIST 2002 test set. We evalu-
ated the translation quality using case-insensitive
BLEU metric (Papineni et al, 2002) on the NIST
2004/2005/2006 test sets.
To obtain dependency trees and forests, we
parsed the English sentences of the FBIS corpus
using a shift-reduce dependency parser that en-
ables beam search (Huang et al, 2009). We only
Rules Size New Rules
tree 7.2M -
forest 7.6M 16.86%
Table 2: Statistics of rules. The last column shows
the ratio of rules extracted from non 1-best parses
being used in 1-best derivations.
retained the best well-formed structure for each
node when extracting string-to-tree rules from de-
pendency forests (i.e., k = 1). We trained two
3-gram depLMs (one from trees and another from
forests) on English side of FBIS corpus plus 2M
sentence pairs from other LDC corpus.
After extracting rules and training depLMs, we
ran our replication of string-to-dependency sys-
tem (Shen et al, 2008) to translate the develop-
ment and test sets.
Table 1 shows the BLEU scores on the test
sets. The first column ?Rule? indicates where
the string-to-dependency rules are learned from:
1-best dependency trees or dependency forests.
Similarly, the second column ?DepLM? also dis-
tinguish between the two sources for training de-
pendency language models. The baseline sys-
tem used the rule table and dependency lan-
guage model both learned from 1-best depen-
dency trees. We find that adding the rule table and
dependency language models obtained from de-
pendency forests improves string-to-dependency
translation consistently and significantly, ranging
from +1.3 to +1.4 BLEU points. In addition, us-
ing the rule table and dependency language model
trained from forest only increases decoding time
insignificantly.
How many rules extracted from non 1-best
1097
Rule DepLM BLEU
tree tree 22.31
tree forest 22.73?
forest tree 22.80?
forest forest 23.12??
Table 3: BLEU scores on the Korean-Chinese test
set.
parses are used by the decoder? Table 2 shows the
number of rules filtered on the test set. We observe
that the rule table size hardly increases. One pos-
sible reason is that we only keep the best depen-
dency structure for each node. The last row shows
that 16.86% of the rules used in 1-best deriva-
tions are extracted from non 1-best parses in the
forests, indicating that some useful rules cannot
be extracted from 1-best parses.
6.2 Results on the Korean-Chinese Task
To examine the efficacy of our approach on differ-
ent language pairs, we carried out an experiment
on Korean-Chinese translation. The training cor-
pus contains about 8.2M Korean words and 7.3M
Chinese words. The Chinese sentences were used
to train a 5-gram language model as well as a 3-
gram dependency language model. Both the de-
velopment and test sets consist of 1,006 sentences
with single reference. Table 3 shows the BLEU
scores on the test set. Again, our forest-based ap-
proach achieves significant improvement over the
baseline (p < 0.01).
6.3 Effect of K-best
We investigated the effect of different k-best
structures for each node on translation quality
(BLEU scores on the NIST 2005 set) and the rule
table size (filtered for the tuning and test sets), as
shown in Figure 5. To save time, we extracted
rules just from the first 30K sentence pairs of the
FBIS corpus. We trained a language model and
depLMs on the English sentences. We used 10
different k: 1, 2, 3, 4, 5, 6, 7, 8, 9 and 10. Ob-
viously, the higher the k is, the more rules are
extracted. When k=10, the number of rules used
on the tuning and test sets was 1,299,290 and the
BLEU score was 20.88. Generally, both the num-
ber of rules and the BLEU score went up with
20.4
20.5
20.6
20.7
20.8
20.9
21.0
21.1
21.2
21.3
21.4
21.5
21.6
21.7
21.8
0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35
BL
EU
 s
co
re
rule table size(M)
k=1,2,...,10
Figure 5: Effect of k-best on rule table size and
translation quality.
20.4
20.5
20.6
20.7
20.8
20.9
21.0
21.1
21.2
21.3
21.4
21.5
21.6
21.7
21.8
0.98 1.00 1.02 1.04 1.06 1.08 1.10
BL
EU
 s
co
re
rule table size(M)
t=1.0,0.9,...,0.1
Figure 6: Effect of pruning threshold on rule table
size and translation quality.
the increase of k. However, this trend did not
hold within the range [4,10]. We conjecture that
when retaining more dependency structures for
each node, low quality structures would be intro-
duced, resulting in much rules of low quality.
An interesting finding is that the rule table grew
rapidly when k is in range [1,4], while gradually
within the range [4,10]. One possible reason is
that there are limited different dependency struc-
tures in the spans with a maximal length of 10,
which the target side of rules cover.
6.4 Effect of Pruning Threshold
Figure 6 shows the effect of pruning threshold on
translation quality and the rule table size. We
retained 10-best dependency structures for each
node in dependency forests. We used 10 different
1098
pruning thresholds: 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,
0.8, 0.9 and 1.0. Intuitively, the higher the prun-
ing threshold is, the less rules are extracted. When
t=0.1, the number of rules used on the tuning and
test sets was 1,081,841 and the BLEU score was
20.68.
Lots of rules are pruned when the pruning
threshold increases from 0.0 to 0.3 (around 20%).
After pruning away these rules, we achieved 0.6
point improvement in BLEU. However, when we
filtered more rules, the BLEU score went down.
Figures 5 and 6 show that using two parame-
ters that have to be hand-tuned achieves a small
improvement at the expense of an additional com-
plexity. To simplify the approach, we only keep
the best dependency structure for each node with-
out pruning any rule.
7 Related Works
While Mi and Huang (2008) and we both use
forests for rule extraction, there remain two ma-
jor differences. Firstly, Mi and Huang (2008) use
a packed forest, while we use a dependency forest.
Packed forest is a natural weighted hypergraph
(Klein and Manning, 2001; Huang and Chiang,
2005), for each hyperedge treats the correspond-
ing PCFG rule probability as its weight. However,
it is challenging to make dependency forest to be a
weighted hypergraph because dependency parsers
usually only output a score for each edge in a de-
pendency tree rather than a hyperedge in a depen-
dency forest. Secondly, The GHKM algorithm
(Galley et al, 2004), which is originally devel-
oped for extracting tree-to-string rules from 1-best
trees, has been successfully extended to packed
forests recently (Mi and Huang, 2008). Unfor-
tunately, the GHKM algorithm cannot be applied
to extracting string-to-dependency rules from de-
pendency forests, because the GHKM algorithm
requires a complete subtree to exist in a rule while
neither fixed nor floating dependency structures
ensure that all dependants of a head are included.
8 Conclusion and Future Work
In this paper, we have proposed to use dependency
forests instead of 1-best parses to extract string-to-
dependency tree rules and train dependency lan-
guage models. Our experiments show that our ap-
proach improves translation quality significantly
over a state-of-the-art string-to-dependency sys-
tem on various language pairs and test sets. We
believe that dependency forest can also be used to
improve the dependency treelet system (Quirk et
al., 2005) that takes 1-best trees as input.
Acknowledgement
The authors were supported by SK Telecom C&I
Business, and National Natural Science Founda-
tion of China, Contracts 60736014 and 60903138.
We thank the anonymous reviewers for their in-
sightful comments. We are also grateful to Wen-
bin Jiang for his invaluable help in dependency
forest.
References
Chiang, David. 2007. Hierarchical phrase-based
translation. Computational Linguistics, pages 201?
228.
Ding, Yuan and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proceedings of ACL.
Dyer, Christopher, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL.
Galley, Michel, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of NAACL.
Huang, Liang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Huang, Liang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP.
Klein, Dan and Christopher D. Manning. 2001. Pars-
ing and hypergraphs. In Proceedings of IWPT.
Kneser, R. and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In Proceedings of
Acoustics, Speech, and Signal.
Liu, Yang, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of EMNLP.
Mi, Haitao and Liang Huang. 2008. Forest-based
translation rule extraction. In Proceedings of
EMNLP.
1099
Och, Franz J. and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of ACL.
Och, Franz J. and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of ACL.
Och, Franz J. and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL.
Quirk, Chris and Simon Corston-Oliver. 2006. The
impact of parsing quality on syntactically-informed
statistical machine translation. In Proceedings of
EMNLP.
Quirk, Chris, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: syntactically in-
formed phrasal smt. In Proceedings of ACL.
Shen, Libin, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL.
Stolcke, Andreas. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP.
Wang, Wei, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based
machine translation accuracy. In Proceedings of
EMNLP.
1100
