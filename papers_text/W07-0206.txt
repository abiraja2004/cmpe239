TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 37?44,
Rochester, April 2007 c?2007 Association for Computational Linguistics
Transductive Structured Classification through Constrained
Min-Cuts
Kuzman Ganchev Fernando Pereira
Computer and Information Science
University of Pennsylvania
Philadelphia PA
{kuzman,pereira}@cis.upenn.edu
Abstract
We extend the Blum and Chawla
(2001) graph min-cut algorithm to
structured problems. This extension
can alternatively be viewed as a joint
inference method over a set of train-
ing and test instances where parts of
the instances interact through a pre-
specified associative network. The
method has has an efficient approxima-
tion through a linear-programming re-
laxation. On small training data sets,
the method achieves up to 34.8% rela-
tive error reduction.
1 Introduction
We describe a method for transductive classifi-
cation in structured problems. Our method ex-
tends the Blum and Chawla (2001) algorithm for
transductive classification. In that algorithm,
each training and test instance is represented
by a vertex in a graph. The algorithm finds the
min-cut that separates the positively and nega-
tively labeled instances. We give a linear pro-
gram that implements an approximation of this
algorithm and extend it in several ways. First,
our formulation can be used in cases where there
are more than two labels. Second, we can use
the output of a classifier to provide a prior pref-
erence of each instance for a particular label.
This lets us trade off the strengths of the min-
cut algorithm against those of a standard classi-
fier. Finally, we extend the algorithm further to
deal with structured output spaces, by encoding
parts of instances as well as constraints that en-
sure a consistent labeling of an entire instance.
The rest of this paper is organized as follows.
Section 2 explains what we mean by transduc-
tive classification and by structured problems.
Section 3 reviews the Blum and Chawla (2001)
algorithm, how we formulate it as a linear pro-
gram and our proposed extensions. Section 4
relates our proposal to previous work. Section 5
describes our experimental results on real and
synthetic data and Section 6 concludes the pa-
per.
2 Concepts and Notation
In this work we combine two separate ap-
proaches to learning: transductive methods, in
which classification of test instances arises from
optimizing a single objective involving both
training and test instances; and structured clas-
sification, in which instances involve several in-
terdependent classification problems. The de-
scription of structured problems also introduces
useful terminology for the rest of the paper.
2.1 Transductive Classification
In supervised classification, training instances
are used to induce a classifier that is then ap-
plied to individual test instances that need to
be classified. In transductive classification, a
single optimization problem is set up involving
all training and test instances; the solution of
the optimization problem yields labels for the
test instances. In this way, the test instances
provide evidence about the distribution of the
data, which may be useful when the labeled data
is limited and the distribution of unlabeled data
37
Figure 1: An example where unlabeled data
helps to reveal the underlying distribution of
the data points, borrowed from Sindhwani et al
(2005). The circles represent data points (unla-
beled are empty, positive have a ?+? and neg-
ative have a ?-?). The dashed lines represent
decision boundaries for a classifier. The first fig-
ure shows the labeled data and the max-margin
decision boundary (we use a linear boundary to
conform with Occam?s razor principle). The sec-
ond figure shows the unlabeled data points re-
vealing the distribution from which the training
examples were selected. This distribution sug-
gests that a linear boundary might not be ap-
propriate for this data. The final figure shows
a more appropriate decision boundary given the
distribution of the unlabeled data.
is informative about the location of the decision
boundary. Figure 1 illustrates this.
2.2 Structured Classification
The usual view of structured classification is as
follows. An instance consists of a set of classifi-
cation problems in which the labels of the differ-
ent problems are correlated according to a cer-
tain graphical structure. The collection of clas-
sification labels in the instance forms a single
structured label. A typical structured problem
is part of speech (POS) tagging. The parts of
speech of consecutive words are strongly corre-
lated, while the POS of words that are far away
do not influence each other much. In the natu-
ral language processing tasks that motivate this
work, we usually formalize this observation with
a Markov assumption, implemented by breaking
up the instance into parts consisting of pairs of
consecutive words. We assign a score for each
possible label of each part and then use a dy-
namic programming algorithm to find the high-
est scoring label of the entire instance.
In the rest of this paper, it will be sometimes
more convenient to think of all the (labeled and
unlabeled) instances of interest as forming a sin-
gle joint classification problem on a large graph.
In this joint problem, the atomic classification
problems are linked according to the graphical
structure imposed by their partition into struc-
tured classification instances. As we will see,
other links between atomic problems arise in our
setting that may cross between different struc-
tured instances.
2.3 Terminology
For structured problems, instance refers to an
entire problem (for example, an entire sentence
for POS tagging). A token refers to the smallest
unit that receives a label. In POS tagging, a to-
ken is a word. A part is one or more tokens and
is a division used by a learning algorithm. For
all our experiments, a part is a pair of consecu-
tive tokens, but extension to other types of parts
is trivial. If two parts share a token then a con-
sistent label for those parts has to have the same
label on the shared token. For example in the
sentence ?I love learning .? we have parts for
?I love? and ?love learning?. These share the
token ?love? and two labels for the two parts
has to agree on the label for the token in order
to be consistent. In all our experiments, a part
is a pair of consecutive tokens so two parts are
independent unless one immediately follows the
other.
3 Approach
We extend the min-cut formulation of Blum and
Chawla (2001) to multiple labels and structured
variables by adapting a linear-programming en-
coding of metric labeling problems. By relaxing
the linear program, we obtain an efficient ap-
proximate inference algorithm. To understand
our method, it is useful to review the min-
cut transductive classification algorithm (Sec-
tion 3.1) as well as the metric labeling prob-
lem and its linear programming relaxation (Sec-
tion 3.2). Section 3.3 describes how to encode
a multi-way min-cut problem as an instance of
metric labeling as well as a trivial extension that
lets us introduce a bias when computing the cut.
38
Section 3.4 extends this formalism to structured
classification.
3.1 Min-Cuts for Transductive
Classification
Blum and Chawla (2001) present an efficient
algorithm for semi-supervised machine learning
in the unstructured binary classification setting.
At a high level, the algorithm is as follows:
? Construct a graph where each instance cor-
responds to a vertex;
? Add weighted edges between similar ver-
tices with weight proportional to a measure
of similarity;
? Find the min-cut that separates positively
and negatively labeled training instances;
? Label all instances on the positive side of
the cut as positive and all others as nega-
tive.
For our purposes we need to consider two exten-
sions to this problem: multi-way classification
and constrained min-cut.
For multi-way classification, instead of com-
puting the binary min-cut as above, we need
to find the multi-way min-cut. Unfortunately,
doing this in general is NP-hard, but a poly-
nomial time approximation exists (Dahlhaus et
al., 1992). In Section 3.3 we describe how we
approximate this problem.
We extend this approach to structured data
by constructing a graph whose vertices corre-
spond to different parts of the instance, and add
weighted edges between similar parts. We then
find the multi-way min-cut that separates ver-
tices with different labels subject to some con-
straints: if two parts overlap then the labels have
to be consistent. Our main contribution is an al-
gorithm that approximately computes this con-
strained multi-way min-cut with a linear pro-
gramming relaxation.
3.2 Metric Labeling
Kleinberg and Tardos (1999) introduce the met-
ric labeling problem as a common inference
problem in a variety of fields. The inputs to
the problem are a weighted graph G = (V,E), a
set of labels L = {i|i ? 1 . . . k}, a cost function
c(v, i) which represents the preference of each
vertex for each possible label and a metric d(i, j)
between labels i and j. The goal is to assign a
label to each vertex l : V ? L so as to minimize
the cost given by:
c(l) =
?
v?V c(v, l(v))
+
?
(u,v)?E d(l(u), l(v)) ? w(u, v) .
(1)
Kleinberg and Tardos (1999) give a linear pro-
gramming approximation for this problem with
an approximation factor of two and explain how
this can be extended to an O(log k) approxima-
tion for arbitrary metrics by creating a hierar-
chy of labels. Chekuri et al (2001) present an
improved linear program that incorporates arbi-
trary metrics directly and provides an approxi-
mation at least as good as that of Kleinberg and
Tardos (1999). The idea in the new linear pro-
gram is to have a variable for each edge labeling
as well as one for each vertex labeling.
Following Chekuri et al (2001), we represent
the event that vertex u has label i by the vari-
able x(u, i) having the value 1; if x(u, i) = 0 then
vertex v must have some other label. Similarly,
we use the variable and value x(u, i, v, j) = 1 to
mean that the vertices u and v (which are con-
nected by an edge) have label i and j respec-
tively. The edge variables allow us to encode
the costs associated with violated edges in the
metric labeling problem. Edge variables should
agree with vertex labels, and by symmetry we
should have x(u, i, v, j) = x(v, j, u, i). If the
linear program gives an integer solution, this is
clearly the optimal solution to the original met-
ric labeling instance. Chekuri et al (2001) de-
scribe a rounding procedure to compute an in-
teger solution to the LP that is guaranteed to
be an approximation of the optimal integer so-
lution. For the problems we considered, this was
very rarely necessary. Their linear program re-
laxation is shown in Figure 2. The cost function
is the sum of the vertex costs and edge costs.
The first constraint requires that each vertex
have a total of one labeling unit distributed over
its labels, that is, we cannot assign more or less
than one label per vertex. The second constraint
39
min
X
u?V
X
i?L
c(u, i)x(u, i)
+
X
(u,v)?E
X
k,j?L
w(u, v)d(i, j)x(u, i, v, j)
subject to
X
i?L
x(u, i) = 1 ?u ? V
x(u, i)?
X
j?L
x(u, i, v, j) = 0 ?u ? V, v ? N(u), i ? L
x(u, i, v, j)? x(v, j, u, i) = 0 ?u, v ? V, i, j ? L
x(u, i, v, j), x(u, i) ? [0, 1] ?u, v ? V, i, j ? L
Figure 2: The Chekuri et al (2001) linear pro-
gram used to approximate metric labeling. See
text for discussion.
requires that vertex- and edge-label variables are
consistent: the label that vertex variables give
a vertex should agree with the labels that edge
variables give that vertex. The third constraint
imposes the edge-variable symmetry condition,
and the final constraint requires that all the vari-
ables be in the range [0, 1].
3.3 Min Cut as an Instance of Metric
Labeling
Given an instance of the (multi-way) min-cut
problem, we can translate it to an instance of
metric labeling as follows. The underlying graph
and edge weights will be the same as min-cut
problem. We add vertex costs (c(u, i) ?u ?
V, i ? L) and a label metric (d(i, j) ?i, j ? L).
For all unlabeled vertices set the vertex cost to
zero for all labels. For labeled vertices set the
cost of the correct label to zero and all other la-
bels to infinity. Finally let d(i, j) be one if i 6= j
and zero otherwise.
The optimal solution to this instance of metric
labeling will be the same as the optimal solution
of the initial min cut instance: the cost of any
labeling is the number of edges that link vertices
with different labels, which is exactly the num-
ber of cut edges. Also by the same argument,
every possible labeling will correspond to some
cut and approximations of the metric labeling
formulation will be approximations of the origi-
nal min-cut problem.
Since the metric labeling problem allows ar-
bitrary affinities between a vertex in the graph
and possible labels for that vertex, we can triv-
ially extend the algorithm by introducing a bias
at each vertex for labels more compatible with
that vertex. We use the output of a classifier to
bias the cost towards agreement with the clas-
sifier. Depending on the strength of the bias,
we can trade off our confidence in the perfor-
mance of the min-cut algorithm against the our
confidence in a fully-supervised classifier.
3.4 Extension to Structured
Classification
To extend this further to structured classifica-
tion we modify the Chekuri et al (2001) linear
program (Figure 2). In the structured case, we
construct a vertex for every part of an instance.
Since we want to find a consistent labeling for an
entire instance composed of overlapping parts,
we need to add some more constraints to the lin-
ear program. We want to ensure that if two ver-
tices correspond to two overlapping parts, then
they are assigned consistent labels, that is, the
token shared by two parts is given the same label
by both. First we add a new zero-weight edge
between every pair of vertices corresponding to
overlapping parts. Since its weight is zero, this
edge will not affect the cost. We then add a
constraint to the linear-program that the edge
variables for inconsistent labelings of the new
edges have a value of zero.
More formally, let (u, i, v, j) ? ? denote that
the part u having label i is consistent with the
part v having label j; if u and v do not share any
tokens, then any pair of labels for those parts are
consistent. Now add zero-weight edges between
overlapping parts. Then the only modification
to the linear program is that
x(u, i)?
?
j?L x(u, i, v, j) = 0
?u ? V, v ? N(u), i ? L
will become
x(u, i)?
?
j:(u,i,v,j)?? x(u, i, v, j) = 0
?u ? V, v ? N(u), i ? L .
40
min
X
u?V
X
i?L
c(u, i)x(u, i)
+
X
(u,v)?E
X
k,j?L
w(u, v)d(i, j)x(u, i, v, j)
subject to
X
i?L
x(u, i) = 1 ?u ? V
x(u, i)?
X
j:(u,i,v,j)??
x(u, i, v, j) = 0 ?u ? V, v ? N(u), i ? L
x(u, i, v, j)? x(v, j, u, i) = 0 ? (u, i, v, j) ? ?
x(u, i, v, j), x(u, i) ? [0, 1] ?u, v ? V, i, j ? L
Figure 3: The modified linear program used to
approximate metric labeling. See text for dis-
cussion.
What this modification does is to ensure that all
the mass of the edge variables between vertices
u and v lies in consistent labelings for their edge.
The modified linear program is shown in Figure
3. We can show that this can be encoded as
a larger instance of the metric labeling problem
(with roughly |V |+|E| more vertices and a label
set that is four times as large), but modifying the
linear program directly results in a more efficient
implementation. The final LP has one variable
for each labeling of each edge in the graph, so
we have O(|E||L|2) variables. Note that |L| is
the number of labelings of a pair of tokens for
us ? even so, computation of a single dataset
took on the order of minutes using the Xpress
MP package.
4 Relation to Previous work
Our work is set of extensions to the work of-
Blum and Chawla (2001), which we have already
described. Our extensions allow us to handle
multi-class and structured data, as well as to
take hints from a classifier. We can also spec-
ify a similarity metric between labels so that a
cut-edge can cost different amounts depending
on what partitions it spans.
Taskar et al (2004a) describe a class of
Markov networks with associative clique poten-
tials. That is, the clique potentials always prefer
that all the nodes in the clique have the same
label. The inference problem in these networks
is to find the assignment of labels to all nodes in
the graph that maximizes the sum of the clique
potentials. Their paper describes a linear pro-
gramming relaxation to find (or approximate)
this inference problem which is very similar to
the LP formulation of Chekuri et al (2001) when
all cliques are of size 2. They generalize this
to larger cliques and prove that their LP gives
an integral solution when the label alphabet has
size 2 (even for large cliques). For the learn-
ing problem they exploit the dual of the LP for-
mulation and use a maximum margin objective
similar to the one used by Taskar et al (2004b).
If we ignore the learning problem and focus on
inference, one could view our work as inference
over a Markov network created by combining a
set of linear chain conditional random fields with
an associative Markov network (with arbitrary
structure). A direction for future work would be
to train the associative Markov network either
independently from the chain-structured model
or jointly with it. This would be very similar to
the joint inference work described in the next
paragraph, and could be seen as a particular
instantiation of either a non-linear conditional
random field (Lafferty et al, 2001) or relational
Markov network (Taskar et al, 2002).
Sutton and McCallum (2004) consider the use
of linear chain CRFs augmented with extra skip
edges which encode a probabilistic belief that
the labels of two entities might be correlated.
They provide experimental results on named en-
tity recognition for e-mail messages announcing
seminars, and their system achieves a 13.7% rel-
ative reduction in error on the ?Speaker? field.
Their work differs from ours in that they add
skip edges only between identical capitalized
words and only within an instance, which for
them is an e-mail message. In particular, they
can never have an edge between labeled and un-
labeled parts. Their approach is useful for iden-
tification of personal names but less helpful for
other named entity tasks where the names may
not be capitalized.
Lafferty et al (2004) show a representer the-
orem allowing the use of Mercer kernels with
41
CRFs. They use a kernel CRF with a graph
kernel (Smola and Kondor, 2003) to do semi-
supervised learning. For them, the graph de-
fines an implicit representation of the data, but
inference is still performed only on the (chain)
structure of the CRF. By contrast, we perform
inference over the whole set of examples at the
same time.
Altun et al (2006) extend the use of graph-
based regularization to structured variables.
Their work is in the framework of maximum
margin learning for structured variables where
learning is framed as an optimization problem.
They modify the objective function by adding
a penalty whenever two parts that are expected
to have a similar label assign a different score to
the same label. They show improvements of up
to 5.3% on two real tasks: pitch accent predic-
tion and optical character recognition (OCR).
Unfortunately, to solve their optimization prob-
lem they have to invert an n?n matrix, where n
is the number of parts in the training and test-
ing data times the number of possible labels for
each part. Because of this they are forced to
train on an unrealistically small amount of data
(4-40 utterances for pitch accent prediction and
10 words for OCR).
5 Experiments
We performed experiments using our approach
on three different datasets using a conditional
random field as the base classifier. Unless oth-
erwise noted this was regularized using a zero-
mean Gaussian prior with a variance of 1.
The first dataset is the pitch-accent prediction
dataset used in semi-supervised learning by Al-
tun et al (2006). There are 31 real and binary
features (all are encoded as real values) and only
two labels. Instances correspond to an utterance
and each token corresponds to a word. Altun
et al (2006) perform experiments on 4 and 40
training instances using at most 200 unlabeled
instances.
The second dataset is the reference part of
the Cora information extraction dataset.1 This
1The Cora IE dataset has been used in Seymore et
al. (1999), Peng and McCallum (2004), McCallum et
al. (2000) and Han et al (2003), among others. We
consists of 500 computer science research paper
citations. Each token in a citation is labeled as
being part of the name of an author, part of the
title, part of the date or one of several other
labels that we combined into a single category
(?other?).
The third dataset is the chunking dataset
from the CoNLL 2000 (Sang and Buchholz,
2000) shared task restricted to noun phrases.
The task for this dataset is, given the words in a
sentence as well as automatically assigned parts
of speech for these words, label each word with
B-NP if it is the first word in a base noun phrase,
I-NP if it is part of a base noun phrase but not
the first word and O if it is not part of a noun
phrase.
For all experiments, we let each word be a
token and consider parts consisting of two con-
secutive tokens.
5.1 Pitch Accent Prediction
For the pitch accent prediction dataset, we used
the 5-nearest neighbors of each instance accord-
ing to the Euclidean distance in the original fea-
ture space to construct the graph for min-cut.
Table 1 shows the results of our experiments on
this data, as well as the results reported by Al-
tun et al (2006). The numbers in the table are
per-token accuracy and each entry is the mean
of 10 random train-test data selections.
For this problem, our method improves per-
formance over the base CRF classifier (except
when the training data consists of only 4 utter-
ances), but we do not see improvements as dra-
matic as those observed by Altun et al (2006).
Note that even the larger dataset here is quite
small ? 40 utterances where each token has been
annotated with a binary value.
5.2 Cora-IE
For the Cora information extraction dataset, we
used the first 100 principal components of the
feature space to find 5 nearest neighbors of each
part. This approximation is due to the cost of
comuting nearest neighbors in high dimensions.
In these experiments we trained on 40 instances
obtained the dataset from http://www.cs.umass.edu/
~mccallum/data/cora-ie.tar.gz.
42
Method 4:80 40:80 40:200
CRF 71.2 72.5 73.1
MinCut 69.4 74.4 74.3
STR 70.7 75.7 77.5
SVM 69.9 72.0 73.1
Table 1: Results on the pitch accent prediction
task. The methods we compare are as follows.
CRF is supervised CRF training. MinCut is our
method with a CRF as base classifier. STR and
SVM are the semi-supervised results reported in
Altun et al (2006). The experiments are 4 la-
beled and 80 unlabeled, 40 labeled and 80 unla-
beled and 40 labeled and 200 unlabeled respec-
tively.
Variance 10 100 1000
CRF 84.5% 84.3% 83.9%
MinCut 88.8% 89.6% 89.9%
Table 2: Accuracy on the Cora-IE dataset as
a percentage of tokens correctly classified at dif-
ferent settings for the CRF variance. Results for
training on 40 instances and testing on 80. In
all cases the scores are the mean of 10 random
selections of 120 instances from the set of 500
available.
and used 80 as testing data. In all cases we
randomly selected training and testing instances
10 times from the total set of 500. Table 2
shows the average accuracies for the 10 repe-
titions, with different values for the variance of
the Gaussian prior used to regularize the CRF.
If we choose the optimal value for each method,
our approach gives a 34.8% relative reduction
in error over the CRF, and improves over it in
each of the 10 random data selections, and all
settings of the Guassian prior variance.
5.3 CoNLL NP-Chunking
Our results are worst for the CoNLL NP-
Chunking dataset. As above, we used 10 ran-
dom selections of training and test sets, and
used the 100 principal components of the fea-
ture space to find 5 nearest neighbors of each
part. Table 3 shows the results of our experi-
ments. The numbers in the table are per-token
Method 20:40 40:80
CRF 87.6 90.6
MinCut(CRF) 88.2 89.6
Table 3: Results on the NP-chunking task. The
table compares a CRF with our method using a
CRF as a base classifier. The experiments use
20 labeled and 40 unlabeled and 40 labeled and
80 unlabeled instances.
accuracy as before. When the amount of train-
ing data is very small (20 instances) we improve
slightly over the base CRF classifier, but with
an increased amount of training data, the small
improvement is replaced with a small loss.
6 Discussion
We have presented a new transductive algorithm
for structured classification, which achieves er-
ror reductions on some real-world problems. Un-
fortunately, those gains are not always realized,
and sometimes our approach leads to an increase
in error. The main reason that our approach
does not always work seems to be that our mea-
sure of similarity between different parts is very
coarse. In general, finding all the pairs of parts
have the same label is as difficult as finding the
correct labeling of all instances, but it might be
possible to use unlabeled data to learn the sim-
ilarity measure.
References
Yasemin Altun, David McAllester, and Mikhail
Belkin. 2006. Maximum margin semi-supervised
learning for structured variables. In Y. Weiss,
B. Scho?lkopf, and J. Platt, editors, Advances in
Neural Information Processing Systems 18, pages
33?40. MIT Press, Cambridge, MA.
Avrim Blum and Shuchi Chawla. 2001. Learn-
ing from labeled and unlabeled data using graph
mincuts. In Proceedings of the 18th International
Conf. on Machine Learning, pages 19?26. Morgan
Kaufmann, San Francisco, CA.
Chandra Chekuri, Sanjeev Khanna, Joseph Naor,
and Leonid Zosin. 2001. Approximation algo-
rithms for the metric labeling problem via a new
linear programming formulation. In Symposium
on Discrete Algorithms, pages 109?118.
43
E. Dahlhaus, D. S. Johnson, C. H. Papadimitriou,
P. D. Seymour, and M. Yannakakis. 1992. The
complexity of multiway cuts (extended abstract).
In Proceedings of the twenty-fourth annual ACM
symposium on Theory of computing, pages 241?
251, New York, NY, USA. ACM Press.
H. Han, C. Giles, E. Manavoglu, H. Zha, Z. Zhang,
and E. Fox. 2003. Automatic document meta-
data extraction using support vector machines. In
Joint Conference on Digital Libraries.
Jon Kleinberg and Eva Tardos. 1999. Approx-
imation algorithms for classification problems
with pairwise relationships: Metric labeling and
markov random fields. In Proceedings of the 40th
Annual Symposium on Foundations of Computer
Science, page 14, Washington, DC, USA. IEEE
Computer Society.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the 10th Inter-
national Conference on Machine Learning, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
John Lafferty, Xiaojin Zhu, and Yan Liu. 2004.
Kernel conditional random fields: representation
and clique selection. In Proceedings of the twenty-
first international conference on Machine learn-
ing, page 64, New York, NY, USA. ACM Press.
A. McCallum, K. Nigam, J. Rennie, and K. Sey-
more. 2000. Automating the construction of in-
ternet portals with machine learning. Information
Retrieval, 3:127?163.
Fuchun Peng and Andrew McCallum. 2004.
Accurate information extraction from research
papers using conditional random fields. In
Daniel Marcu Susan Dumais and Salim Roukos,
editors, Main Proceedings of HLT-NAACL, pages
329?336, Boston, Massachusetts, USA, May 2 -
May 7. Association for Computational Linguistics.
Erik Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the CoNLL-2000 shared task:
Chunking. In Proceedings of the Fourth Confer-
ence on Computational Natural Language Learn-
ing and of the Second Learning Language in Logic
Workshop. Association for Computational Lin-
guistics.
K. Seymore, A. McCallum, and R. Rosenfeld. 1999.
Learning hidden markov model structure for in-
formation extraction. In AAAI?99 Workshop on
Machine Learning for Information Extraction.
Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin.
2005. Beyond the point cloud: from transductive
to semi-supervised learning. In Proceedings of the
22nd International Conference on Machine Learn-
ing, pages 824?831.
Alexander Smola and Risi Kondor. 2003. Kernels
and regularization on graphs. In M. Warmuth and
B. Scholkopf, editors, Proceedings of the Sixteenth
Annual Conference on Learning Theory and Ker-
nels Workshop.
Charles Sutton and Andrew McCallum. 2004. Col-
lective segmentation and labeling of distant enti-
ties in information extraction. Technical Report
TR # 04-49, University of Massachusetts, July.
Presented at ICML Workshop on Statistical Re-
lational Learning and Its Connections to Other
Fields.
Ben Taskar, Abbeel Pieter, and Daphne Koller.
2002. Discriminative probabilistic models for re-
lational data. In Proceedings of the 18th An-
nual Conference on Uncertainty in Artificial Intel-
ligence (UAI-02), pages 485?492, San Francisco,
CA. Morgan Kaufmann Publishers.
B. Taskar, V. Chatalbashev, and D. Koller. 2004a.
Learning associative markov networks. In Pro-
ceedings of the Twenty-First International Con-
ference on Machine Learning (ICML).
Ben Taskar, Carlos Guestrin, and Daphne Koller.
2004b. Max-margin markov networks. In Se-
bastian Thrun, Lawrence Saul, and Bernhard
Scho?lkopf, editors, Advances in Neural Informa-
tion Processing Systems 16. MIT Press, Cam-
bridge, MA.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large mar-
gin methods for structured and interdependent
output variables. JMLR, 6:1453?1484.
44
