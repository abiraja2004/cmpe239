Multi-l ingual Translation of Spontaneously Spoken Language 
in a Limited Domain 
Alon Lavie, Donna Gates, Marsal Gavaldh, 
Laura Mayfield, Alex Waibel and Lor i  Lev in  
Center  for Mach ine  Trans la t ion  
Carneg ie  Mel lon Un ivers i ty  
5000 Forbes  Ave. ,  P i t t sburgh ,  PA 15213 
emai l  : l av ie@cs .cmu.edu  
Abst rac t  
JANUS is a multi-lingual speech-to- 
speech translation system designed to 
facilitate communication between two 
parties engaged in a spontaneous con- 
versation in a limited domain. In an 
attempt o achieve both robustness and 
translation accuracy we use two (lifter- 
ent translation components: the GLR 
module, designed to be more accu- 
rate, and the Phoenix module, designed 
to be more robust. We analyze the 
strengths and weaknesses of each of the 
approaches and describe our work on 
combining them. Another recent focus 
has been on developing a detailed end- 
to-end evaluation procedure to measure 
the performance and effectiveness of the 
system. We present our most recent 
Spanish-to-English performance evalua- 
tion results. 
1 In t roduct ion  
JANUS is a multi-lingual speech-to-speech trans- 
lation system designed to facilitate communica- 
tion between two parties engaged in a sponta- 
neous conversation in a limited domain. In this 
paper we describe the current design and perfor- 
mance of the machine translation module of our 
system. The analysis of spontaneous speech re- 
quires dealing with problenls such as speech dis- 
fluencies, looser notions of grammaticality and the 
lack of clearly marked sentence boundaries. These 
problems are further exacerbated by errors of the 
speech recognizer. We describe how our machine 
translation system is designed to effectively han- 
dle these and other problems, hi an attempt 
to achieve both robustness and translation accu- 
racy we use two different ranslation components: 
the (JLlt. module, designed to be more accurate, 
and the Phoenix module, designed to be more ro- 
bust. Both modules follow an interlingua-based 
approach. The translation modules are designed 
to be language-independent in the sense that they 
each consist of a general processor that applies in- 
dependently specified knowledge about different 
languages. This facilitates the easy adaptation of 
the system to new languages and domains. We an- 
alyze the strengths and weaknesses of each of the 
translation approaches and describe our work on 
combining them. Our current system is designed 
to translate spontaneous dialogues in the Schedul- 
ing domain, with English, Spanish and German as 
both source and target languages. A recent focus 
has been on developing a detailed end-to-end eval- 
uation procedure to measure the performance and 
effectiveness of the system. We describe this pro- 
cedure in the latter part of the paper, and present 
our most recent Spanish-to-English performance 
evaluation results. 
2 System Overv iew 
The JANUS System is a large scale multi-lingual 
speech-to-speech translation system designed to 
facilitate communication between two parties en- 
gaged in a spontaneous conversation i  a limited 
domain. A diagram of the architecture of the sys- 
tem is shown in Figure 1. The system is com- 
posed of three main components: a speech recog- 
nizer, a machine translation (MT) module and a 
speech synthesis module. The speech recognition 
component of the system is described elsewhere 
(Woszczyna et al 1994). For speech synthesis, we 
use a commercially available speech synthesizer. 
The MT module is composed of two separate 
translation sub-modules which operate indepen- 
dently. The first is the GLR module, designed 
to be more accurate. The second is the Phoenix 
module, designed to be more robust. Both mod- 
ules follow an interlingua-based approach. The 
source language input string is first analyzed by 
a parser, which produces a language-independent 
interlingua content representation. The interlin- 
gua is then passed to a generation component, 
which produces an output string in the target lan- 
guage. 
The discourse processor is a component of the 
GLR translation module. It disaInbiguates the 
speech act of each sentence, normalizes temporal 
442  
S  .  i n  Source Language > J 
I r I 
OU ~ " 
FGenKit enerator \[ 
. . . .  
I s ?oo. I 
Figure 1: The  JANUS System 
expressions, and incorporates the sentence into a 
discourse plan tree. 'the discourse processor also 
updates a calendar which keeps track of what the 
speakers haw'~ said about their schedules. The dis- 
course processor is described in greater detail else.- 
where (R,osd et 31. 1995). 
3 The  QLR Trans lat ion  Modu le  
The (\]LR.* parser (Lavie and Tomita 11993; I,avie 
1994) is a parsing system based on Tomita's Gen- 
eralized LI~ parsing algorithm (Tomita 1987). The 
parser skips parts of the utterance that it cannot 
incorporate into a well-formed sentence structure. 
Thus it is well-suited to doinains ill which non- 
grammaticality is coal i t ion.  The  parser conducts 
a search for the maximal subset of the original 
input that is covered by the grammar. This is 
done using a beam search heuristic that limits tile 
combinations of skipped words considered by the 
parser, and ensures that it operates within feasible 
time and space bonnds. 
The GI,R* parser was implemented as an ex- 
tension to the G LR parsing system, a unification- 
I>ased practical natural language system ('lbmita 
1990). The grammars we develop for the ,IAN US 
system are designed to produce \[eature struc- 
tures that correspond to a frame-based language- 
independent representation f the meaning of the 
input utterance. For a given input utterance., the 
parser produces a set; of interlingua texts, or ILTs. 
The main components of an ILT are the speech 
act (e.g., suggest, accept, reject), the sentence 
type (e.g., s tate ,  query- i~,  fragment), and the 
main semantic frame (e.g., free, busy). An ex- 
ample of nn ILl' is shown in Figure 2. A detailed 
IUI' Specitication was designed as a formal de~ 
scription of the allowable ILTs. All parser output 
must conform to this ILl' Speeitication. The GLR 
unification based formalism allows the grammars 
to construct precise and very detailed ILTs. This 
in turn allows the G LI{ translation module to pro- 
duce highly accurate translations for well-formed 
input. 
The G LR* parser also includes everal tools de- 
signed to address the difficulties of parsing spon- 
taneous peech. To cope with high levels of am- 
biguity, the parser includes a statis|,ical disam- 
biguation module, in which probabilities are at- 
tached directly to the actions in the LR parsing 
table. The parser can identify sentence bound- 
aries within each hypothesis with the help of a 
statistical method that determines the probabil- 
ity of a boundary at; each point in the utterance. 
The parser must also determine the "best" parse 
from among tit(; diflZrent parsable subsets of an 
input. This is don(; using a collection of parse 
evaluation measures which are combined into an 
integrated heuristic for evaluating and ranking the 
parses produced by the parser. Additionally, a 
parse quality heuristic allows the parser to self- 
443  
((frame *free) 
(who ((frame*i))) 
(when ((frame *.simple-time) 
(day-of-week wednesday) 
(time-of-day morning))) 
(a-speech-act (*multiple* *suggesic *aec(,pt)) 
(sentence-type *state))) 
Sentence: 1could do it Wednesday morning too. 
Figure 2: An Example 112' 
judge the quality of tile parse chosen as best, and 
to detect cases in which important information is 
likely to have been skipt)ed. 
Target language generation in the (;LR modtde 
is clone using GenKit (Tomita and Nyberg 1988), 
a unification-based generation system. With well- 
developed generation grammars, GenKit results in 
very accurate translation for well-specified IUI%. 
4 The Phoenix Translation Module 
The ,IANUS Phoenix translation module (May- 
field et el. 1995) is an extension of the Phoenix 
Spoken Language System (Ward 1991; Ward 
1994). The translation component consists of a 
t)arsing module and a generation module. Trans- 
lation between any of the four source languages 
(English, German, SpanisIL Korean) and five tar- 
get languages (English, German, Spanish, Korean, 
Japanese) is possible, although we currently focus 
only on a few of these language pairs. 
Unlike the GI, R method which attempts to con- 
struct a detailed tur  for a given input utterance, 
the Phoenix approach attempts to only identify 
the key semantic concepts represented in the ut- 
terance and their underlying structure. Whereas 
GLR* is general enough to support both seman- 
tic and syntactic grammars (or some combination 
of both types), the Phoenix approach was specifi- 
cally designed for semantic grammars. Grammat-  
ical constraints are introduced at the phrase level 
(as opposed to the sentence level) and regulate 
semantic ategories. This allows the ungrammat- 
icalities that often occur between phrases to be 
ignored and reflects tile fact that syntactically in- 
correct spontaneous speech is often semantically 
well-formed. 
The parsing grammar specifies patterns which 
represent concepts in the domain. The patterns 
are composed of words of the input string as well 
as other tokens for constituent concepts. Elements 
(words or tokens) in a pattern may be specified as 
ol)tional or repeating (as in a Kleene star mecha- 
nisln). Each concept, irrespective of its level in the 
hierarchy, is represented by a separate grammar 
file. These grammars are compiled into Recursive 
Transition Networks (RTNs). 
The interlingua meaning representation of an 
input utterance is derived directly from the 1)arse 
tree constructed by the parse.r, by extracting the 
represented structure of concepts. This represen- 
tation is usually less detailed than tile correspond- 
ing GLR IlfF representation, and thus often re- 
suits in a somewhat less accurate translation. The 
set of semantic oncept okens for the Scheduling 
domain was initially developed from a set of 45 
example English dialogues. Top-level tokens, also 
called slots, represent speech acts, such as sugges- 
tion or agreement. Intermediate-level tokens dis- 
tingnish between points and intervals in time, for 
example; lower-level tokens cat)ture the speciiics 
of the utterance, such as days of the week, and 
represent he only words that are translated di- 
rectly via lookup tables. 
'File parser matches as much of the inl)ut ut- 
terance as it can to the patterns pecified by the 
I~TNs. Out-of-lexicon words are ignored, unless 
they occur in specific locations where open con- 
cepts are permitted. A word that is already known 
to the system, however, can cause a concept pat- 
tern not to match if it occurs in a position un- 
specified in the grammar. A failed concept does 
not cause the entire parse to fail. The parser can 
ignore any number of words in between top-level 
concepts, handling out-of-domain or otherwise un- 
expected input. Tile parser has no restrictions 
on the order in which slots ca~ occur. This can 
cause added ambiguity in the segmentation of the 
utterance into concepts. The parser uses a dis- 
ambiguation algorithm that attempts to cover the 
largest number of words using the smallest num- 
ber of concepts. 
Figure 3 shows an example of a speaker ut- 
terance and the parse that was produced using 
the Phoenix parser. The parsed speech recog- 
nizer outpnt is shown with unknown (-) and un- 
expected (*) words marked. These segments of 
the input were ignored by the parser. The rele- 
vant concepts, however, are extracted, and strung 
together they provide a general meaning represen- 
tation of what the speaker actually said. 
Generation in the Phoenix module is accom- 
plished using a sirnple strategy that sequentially 
generates target language text for each of the top 
level concepts in the parse analysis. Each con- 
cept has one or more tixed phrasings in the target 
language. Variables such as times and dates are 
extracted from the parse analysis and translated 
directly. The result is a meaningfifl translation, 
but can have a telegraphic feel. 
5 Combining the GLR and 
Phoenix Translation Modules 
5.1 S t rengths  and Weaknesses  of the  
Approaches  
As already described, both the GLR* parser and 
tile Phoenix parser were specifically designed to 
handle tile problems associated with analyzing 
spontaneous peech, llowever, each of the ap- 
444  
( ) r ig ina l  utter\[tn(.'('.: 
S\[ QIJE 'I'E \[ 'At{E('E ' \[ 'EN(IO t,;1~ MAt{TES I)IE(\]IO(:\[{O 
Y EL MII;;RCOI,ES DIE(~INIII.;VE I,IPII{.ES TOI)O El, I)\[A 
PODR(AMOS 11{ 1)E MATINI;" O SEA t.;N I,A TAI{I)E VEI{ 
EL I,A I 'EL\[ f :UI ,A 
(I{oughly " Yes what  do you th ink \[ have Tuesday  the I 
teenth and Wednesday  the nznetccnth  f~cc all day we ,-ou~d 
qo see tlze matmde so zn the a f te rnoon see the the 7novzc." )
As decoded hy  the. recogn izer :  
%NOISE% SII Q1JEI TE I 'AI{ECE %NOISE% ' I 'EN(;O EL 
MARTES I)IEC, IOCI\[O Y H ,  MIE1RCOLES I ) IEC INUI 'VE  
LIBRES TOI)O I.,L DI1A PODI{ l lAMOS Itt, I)E MATINE1 
'X:NOISI.;% O SEA I,A TAI{DE A VEI{ LA 
Parsed  : 
'~<S> sil quel tc parece lento  (:\[ mattes di,~iocht> y (~1 
miclrcoles diecinueve libres todo el d i la  podr i lamos  *11% *I)E 
-MATINF, 1 o sea la tarde a ver I,A %</S> 
Parse  Tree  (~ Sen lant i c  l~e l ) resentat ion) :  
\[rcst . . . . .  t\] ( \[yes\] ( S l l  )) 
\[yourt:urn\] ( QIII,\]I TE \['AII, I~CE ) 
\[give'info\] ( \ [myavai labi l i ty\]  ( q'ENGO \[temp'loc\] 
( \[teml . . . . .  1\] ( \[point:\] ( \[date\] ( EL \[d'o'w\] ( MAI'JI'ES )) 
\[,late\] ( \[clay'oral\] ( I ) IEC IOCI lO  ) \[,:onj\] ( Y ) El, \[d'o'w\] 
(MIEI I '~COI , I ' ;S) )  \[date\] ( \ [dayord \ ]  ( I ) IE ( : \ [NUEVE) ) ) ) )  
1,11 HIES )) 
\[givemfo\] ( \[my'availabil ity\] ( [temp'loc\] 
( \[temporal\] ( \[range\] ( \[entire\] ( TOl )O )El, \[,mit\] 
( Jr'unit\] ( I ) I IA  ) ) ) ) ) I 'ODRI1AMOS )) 
\[suggest\] ( \[suggest'meeting\] ( \[temp'loc\] ( \[temporal\] 
( O SEA \[point\] ( I,A \[t'o'd\] ( TARDE ))))A W,:Et )) 
Generat(.~d: 
English = <Yes what do you think? 1 could meet: Tuesday 
eighteenth and Wednesday the nineteenth 1 couhl meet the 
whole day do you want to try to get together m the afternoon> 
Figure 3: A Phoenix Spanish to English Transla- 
tion Examl)h: 
proaches has some clear strengths and weaknesses. 
Although designed t<> COl)e with speech disth|en- 
cies, (;LR* can graeehdly tolerate only moderate 
levels of deviation from the grammar. When the 
input is only slightly ungrammatical, nd contains 
relatively minor distluencies, (ILR* produces pre- 
cise and detailed IH's that result in high quality 
translations. The (ILl{* parser has <lifliculties in 
parsing long utterances that are highly dislluent, 
or that significantly deviate from the grammar. 
In many such cases, (I LH,* succeeds to parse only 
a small fragment of the entire utterance, and im- 
portant input segments end up being sldl)t)ed. 1 
l)hoenix is signitlcantly better suited to analyzing 
such utterances. Because Phoenix is capable of 
skipping over input segments that <1o not corre- 
spond to any top level semantic concept, it can 
far better recover from out-of-domain se.gments in
the input, and "restart" itself on an in-domain 
segment that follows. However, this sometime.s re- 
suits in the parser picking up and mis-translating 
a small parsal)le phrase within an out-of-domain 
IRccent work on a method for pre-brcaking the 
utterance at sentence boundaries prior to parsing have 
signiii(:antly reduced this l)rol)lem. 
segtnent. To handle this problem, we are. attempt- 
ing to develop methods for automatically detect- 
ing out-of-domain segments in an utterance (see 
section 7). 
Because the Phoenix approach ignores small 
fmlction words in the mt)ut , its translation results 
are by design bound to be less accurate. However, 
the ability to ignore function words is of great ben- 
ellt when working with speech recognition output, 
in which such words are often mistaken. By keying 
on high-conlidence words l>hoenix takes advan- 
tage of the strengths of the speech decoder. At the 
current time, Phoenix uses only very simple dis- 
ambiguation heuristics, does not employ any dis- 
course knowledge, and does not have a mechanism 
similar to the parse quality heuristic of GLR*, 
which allows the parser to self-assess the quality 
of the produced result. 
5.2 Combin ing  the Two Approaches  
I{ecause ach of the two translation methods ap- 
pears to perform better on different ypes of utter- 
ances, they may hopefldly be combined in a way 
that takes adwmtage of the strengths of each of 
them. One strategy that we have investigated is
to use the l'hoeIfiX module as a back-up to the 
(1 Lt{ module. The parse result of GLR* is trans- 
lated whenever it is judged by the parse quality 
heuristic to be "Good". Whenever the parse result 
t~'om GLI{* is judged as "Bad", the translation is 
generated from the corresponding output of the 
Phoenix parser. Results of using this combination 
scheme are presented in the next section. We art: 
in the process of investigating some more sophisti- 
cated methods for combining the two translation 
at)proaehes. 
6 Eva luat ion  
6.1  The Ewduat ion  P rocedure  
In order to assess the overall eflhctiveness of the 
two translation contponents, we developed a de- 
tailed end-to-end evaluation procedure (Gates el; 
hi. 1996). We evaluate the translation modules 
on both transcribed and spee.ch recognized input. 
The evMuation of transcribed inl)ut allows us to 
assess how well our translation modnles wouhl 
\[unction with "perfect" speech recognition. 'lhst- 
ing is performed on a set; of "unseen" dialogues, 
that were not used for developing the translation 
modules or training the speech recognizer. 
'\['he translation of an utterance is manually 
evaluated by assigning it a grade or a set of grades 
based on the number of sentences in the utter- 
alice. 'file utterances are broken clown into sen- 
tences for evaluation in order to give more weight 
to longer utterances, and so that utterances con- 
taining both in and out-of-domain sentences can 
be .iudged more accurately. 
Each sentence is cla,ssified first as either relevant 
to the scheduling domain (in-domain) or not rel- 
445 
evant to the scheduling domain (out-of-domain). 
Each sentence is then assigned one of four grades 
for translation quality: (1) Perfect - a fluent trans- 
lation with all information conveyed; (2) OK - 
all important information translated correctly but 
some unimportant details missing, or the transla- 
tion is awkward; (3) Bad - unacceptable transla- 
tion; (4) Recognition Error - unacceptable trans- 
lation due to a speech recognition error. These 
grades are used for both in-domain and out-of- 
domain sentences. However, if an out-of-domain 
sentence is automatically detected as such by the 
parser and is not translated at all, it is given an 
"OK" grade. The evaluations are performed by 
one or more independent graders. When more 
than one grader is used, the results are averaged 
together. 
6.2 Resu l ts  
Figure 4 shows the evaluation results for 16 un- 
seen Spanish dialogues containing 349 utterances 
translated into English. Acceptable is the sum of 
"Perfect" and "OK" sentences. For speech recog- 
nized input, we used the first-best hypotheses of 
the speech recognizer. 
Two trends have been observed from this eval- 
uation as well as other evaluations that we have 
conducted. First, The GLR translation module 
performs better than the Phoenix module on tran- 
scribed input and produces a higher percentage of 
"Perfect" translations, thus confirming the GLR 
approach is more accurate. This also indicates 
that GLR performance should improve with bet- 
ter speech recognition and improved pre-parsing 
utterance segmentation. Second, the Phoenix 
module performs better than GLR on the first- 
best hypotheses from the speech recognizer, a re- 
sult of the Phoenix approach being more robust. 
These results indicate that combining the two 
approaches has the potential to improve the trans- 
lation performance. Figure 5 shows the results of 
combining the two translation methods using the 
simple method described in the previous section. 
The GLR* parse quality judgement is used to de- 
termine whether to output the GLR translation 
or the Phoenix translation. The results were eval- 
uated only for in-domain sentences, since out-of- 
domain sentences are unlikely to benefit from this 
strategy. The combination of the two translation 
approaches resulted in a slight increase in the per- 
centage of acceptable translations on transcribed 
input (compared to both approaches separately). 
On speech recognized input, although the over- 
all percentage of acceptable translations does not 
improve, the percentage of "Perfect" translations 
was higher. 2 
2In a more recent evaluation, this combination 
method resulted in a 9.5% improvement in acceptable 
translations of speech recognized in-domain sentences. 
Although some variation between test sets is to be ex- 
7 Conc lus ions  and  Future  Work  
In this paper we described the design of the two 
translation modules used in the .JANUS system, 
outlined their strengths and weaknesses and de- 
scribed our etforts to combine the two approaches. 
A newly developed end-to-end evaluation proce- 
dure allows us to assess the overall performance 
of the system using each of the translations meth- 
ods separately or both combined. 
Our evaluations have confirmed that the GLR 
approach provides more accurate translations, 
while the Phoenix approach is more robust. Com- 
bining the two approaches using the parse qual- 
ity judgement of the (ILl{* parser results in im- 
proved performance. We are currently investigat- 
ing other methods for combining the two transla- 
tion approaches. Since (\]LR* performs much bet- 
ter when long utterances are broken into sentences 
or sub-utterances which are parsed separately, we 
are looking into the possibility of using Phoenix 
to detect such boundaries. We are also develop- 
ing a parse quality heuristic for the Phoenix parser 
using statistical and other methods. 
Another active research topic is the automatic 
detection of out-of-domain segments and utter- 
ances. Our experience has indicated that a large 
proportion of bad translations arise from the 
translation of small parsable fragments within 
out-of-domain phrases. Several approaches are 
nnder consideration. For the Phoenix parser, we 
have implemented a simple method that looks for 
small islands of parsed words among non-parsed 
words and rejects them. On a recent test set, we 
achieved a 33% detection rate of out-of-domain 
parses with no false alarms. Another approach we 
are pursuing is to use word salience measures to 
identify and reject out-of-domain segments. 
We are also working on tightening the coupling 
of the speech recognition and translation modules 
of our system. We are developing lattice parsing 
versions of both the GLR* and Phoenix parsers, so 
that multiple speech hypotheses can be efficiently 
analyzed in parallel, in search of an interpretation 
that is most likely to be correct. 
Acknowledgements  
The work reported in this paper was funded in 
part by grants from ATR - Interpreting Telecom- 
munications Research Laboratories of Japan, the 
US Department of Defense, and the Verbmobil 
Project of the Federal Republic of Germany. 
We would like to thank all members of the 
JANUS teams at the University of Karlsruhe and 
Carnegie Mellon University for their dedicated 
work on our many evaluations. 
pected, this result strengthens our belief in the poten- 
tial of this approach. 
446 
In l)omain (605 sentences) 
(;LI{* Phoenix 
transcribed speech lst-best transcribed speech lst-best 
Perfect 65.2 34.7 53.3 35.5 
OK 18,8 12.2 25.3 26.3 
Bad 16.0 29.2 21.4 17.1 
l{ecog Err ** 23.9 
Out of I)omain (d85 sentences) 
** 21.1 
Perfect 58.5 29.7 44.2 29.3 
O K 26.7 42.4 44.6 41.1 
Bad 7.5 9.1 
l{ecog Err 
14.8 11.2 
20.4 ** 
Acceptable (l'erfect + OK) 
20.5 
In l)om 84.0 46.9 78.6 61.8 
Out of Dora 85.2 72. l 88.8 70.4 
All l)om 84.5 58.2 82.9 65.5 
ol (,LI{ and l'hoenix. Cross-grading of 16 dialogues. Figure 4: September 1995 e.wduation " ' * ' ' 
In Domain (605 sentences) 
G L R* wid, Phoenix 
transcribed speech lst-best 
Perfect 65.4 39.7 
OK 20.8 21.2 
Bad 13.8 15.2 
Recog Err ** 23.9 
Acceptable (Perfect + OK) 
\[l In Do,,, I\[- 86.2 I 60.9 ll 
Figure 5: September 1995 evaluation of (ILR* combined with Phoenix. Cross-grading of 16 dialogues. 
Re ferences  
D. Gates, A. bavie, L. Levin, A. Waibel, 
M. GavaldS., L. Mayfield, M. Woszczyna and 
P. Zhan. End-to-end Evaluation in JANUS: 
a Speech-to-speech Translation System, To ap- 
pear in Proceedings of ECAI Workshop on Dia- 
logue Processing in Spoken Language Systems, 
Budapest, Hungary, August 1996. 
A. l,avie and M. ToInita. GLR* - An EJficient 
Noise ,5'kippmg Parsing Algorithm for Context 
Free Grammars, Proceedings of the third In- 
ternational Workshop on Parsing Technologies 
(IW PT-9a), Tilburg, The Netherlands, August 
1993. 
A. Lavie. An Integrated Heuristic Scheme for 
Partial Parse Evaluation, Proceedings of the 
32nd Annual Meeting of the ACL (ACL-94), 
Las Cruces, New Mexico, June 1994. 
L. Mayfield, M. (lavaldh, Y-H. Seo, B. Suhm, 
W. Ward, A. Wail)el. "Parsing Real Inl)ut in 
JANUS: a Concept-Based Al)proach." In Pro- 
eeedings of TMI 9,5. 
(:. P. l{os& B. Di Eugenio, L. S. Levin, and 
(;. Van Ess-I)ykema. Discourse processing of 
dialogues with multiple threads. In Proceedings 
of ACL'95, ftoston, MA, 1995. 
M. Tomita. An Efficient Augmented Context-free 
Parsing Algorithm. Computational Linguistics, 
13(1-2) :3 l -46, 1987. 
M. Tomita. Tile Generalized LR Parser/Compiler 
Version 8.4. In Proceedings of Interna- 
tional (:onference on Computational Linguis- 
tics (COLING'90), pages 59-63, llelsinki, Fin- 
land, 1990. 
M. Tomita and E. H. Nyberg 3rd. Genera- 
tion Kit and Transformation Kit, Version 3.2: 
User's Manual. Technical Report (\]MU-CMT- 
88-MEMO, Carnegie Mellon University, Pitts- 
burgh, PA, October \[988. 
W. Ward. "Understanding Spontaneous Speech: 
tile Phoenix System." In Proceedings of 
I(MSb'P-91, 1991. 
W. Ward. "Extracting Information in Sponta- 
neous Speech." In Proceedings of International 
CoT@rence on Spoken Language, 1994. 
M. Woszczyna, N. Aoki-Waibel, F. D. Buo, 
N. Coccaro, T. Horiguchi, K. and Kemp, 
A. Lavie, A. McNair, T. Polzin, 1. Rogina, (J. P. 
Ros6, T. Schultz, B. Suhm, M. Tomita, and 
A. Waibel. JANUS-93: Towards Spontaneous 
Speech Translation. In Proceedings of IEEE 
International Conference on Acoustics, Speech 
and Signal Processing (ICASSP'9~), 1994. 
447 
