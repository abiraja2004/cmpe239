A SYNTAX PARSER BASED ON THE CASE DEPENDENCY 
GRAMMAR AND ITS EFFICIENCY 
Toru Hitaka and Sho Yoshida 
Department of Electronics, Kyushu University, Fukuoka, Japan 
S UMMARY 
Augumented transit ion network 
grammars (ATNGs) or augumented context- 
free grammars are generally used in 
natural language processing systems. 
The advantages of ATNGs may be summa- 
rized as i) eff iciency of representa- 
tion, 2) perspicuity, 3) generative 
power, and the disadvantage of ATNGs is 
that it is diff icult to get an effi- 
cient parsing algorithm becuase of the 
flexibil ity of their complicated 
additional functions. 
In this paper, the syntax of 
Japanese sentences , based on case 
dependency relations are stated first, 
and then we give an bottom-up and 
breadth-f irst parsing algoritbxnwhich 
parses input sentence using time O(n 3) 
and memory space O(n2), where n is the 
length of input sentence. Moreover, 
it is shown that this parser requires 
time O(n2), whenever each B-phrase in 
input sentence is unambiguous in its 
grammatical structure. Therefore, the 
eff iciency of this parser is nearly 
equal to the Earley's parser which is 
the most eff icient parsing method for 
general context-free grammars. 
1. FUNDAMENTALS OF JAPANESE SENTENCE 
The Japanese sentence is ordinari ly 
written in kana (phonetic) letters and 
kanji (ideographic) characters without 
leaving a space between words. From 
the viewpoint of machine processing, 
however, it is necessary to express 
clearly the units composing the 
sentence in such a way as to leave a 
space between every word as in English. 
We have no standard way of spacing the 
units though the need for this has 
been demanded for a long time. 
We give some examples in Figure i. 
The first sentence in the figure is 
of ordinary written form. 
The second indicates a way of 
spacing (i.e. putting a space between 
every word). 
The third indicates another way of 
spacing (i.e. putting a space between 
every B-phrase). 
Nowadays, many other spacing methods 
have been tried in several institutes 
in Japan. 
In this paper, input sentences are 
given in colloquial style in which a 
spacing symbol is placed between two 
successive B-phrases. 
In Japanese sentences, BUNSETSUs(B- 
phrase) are the minimal morphological  
units of case dependency, and the syntax 
of Japanese sentences consists of (i) 
the syntax of B-phrase as a string of 
words, and (2) the syntax of a sentence 
as a string of B-phrases. 
A B-phrase usually pronounced 
without pausing consists of two parts 
- -ma in  part \[or equally an independent 
part in the conventional school gramma- 
tical term\] and an annex part which is 
post positioned. We denote the connec- 
tion of two parts in a B-phrase by a 
dot if necessary. A main part, which is 
a conceptual word \[or equally an inde- 
pendent word\] (e.g. noun, verb, 
adjective or adverb) provides mainly the 
information of the concept. On the 
other hand, an annex part, a possibly 
null string of suffix words (e.g. auxi- 
liary verbs or particles) provides the 
information concerning the kakariuke 
relation and/or the supplementary 
information (e.g. the speaker's attitude 
towards the contents of the sentence, 
tense, etc.) 
A word w has it's spell ing W, part 
of speech H and inflexion K. We call 
(W,H,K) the word structure of w. 
Suppose that a string b of length n 
be a B-phrase. Then, there exist an 
independent word w 0 and suffix words 
Wl, w z, ... , w~, and 
b=w0w I . ? ? w~ 
Cont(Hk,Kk,Hk+1) (0=k<i) ...(i) 
Termi (H?,K Z) ? ? ? (2) 
where (Wi,Hi,Ki) is the word structure 
of w i (0~i~?), Cont(Hk,Kk,Hk+1) means 
a word whose part of speech and inflexion 
are Hk, K k respectively can be followed 
by a word whose part of speech is Hk+lin 
-15-- 
B-phrases and Termi(HQ,Kz) means a word 
whose part of speech ~nd inflexion are 
H?, KZ respectively can be a right-most 
subword of B-phrases. 
(i), (2) are called the rules of B- 
phrase structure, and 
(W0,H0,K 0) (Wi,HI,K ~) "''(Wz,H~,K ~) 
? . .  (3 )  
is called B-phrase structure of b. If 
(3) satisfies the condition (i), w0wlw 
? . .w  Z is called to be a left partial 2 
B-phrase. 
The kakariuke relation is the depen- 
dency relation between two B-phrases in 
a sentence. A B-phrase has the syn- 
tactic functions of governor and 
dependent. The function of governor is 
mainly represented by the independent 
word of B-phrase. The function of 
dependent is mainly represented by the 
string of particles which is the right- 
most substring of B-phrase and by the 
word in front of it (right-most non- 
particle word). 
Every particle has the syntactic and 
partial ly semantic dependent function 
with its own degree of power. The 
particle whose power of dependent 
function is strongest of all particles 
appearing in the string of particles is 
called the representative particle. 
Therefore, the syntactic function of 
dependent of a B-phrase is mainly 
represented by the representative 
particle and by the right-most non- 
particle word. 
Let (W0,H0,K0) , (Wi,Hi,Ki) , (W~,H~,K~) 
be the word structures of independent J 
word, right-most non-particle word and 
representative particle of a B-phrase, 
respectively. Then, <W^,H^>_, <W..,Hi, u u ~ & 
Hj> d are called the inrormatlon or 
governor and the information of depen- 
dent of the B-phrase respectively, and 
the pair (<W0,H0>~,<Wi,Hi,Hj>d) is 
called dependency~informati6n of the 
B-phrase. 
There are many types of dependency 
relation such as agent, patient, 
instrument, location, time, etc. Let 
C be the set of all types of dependency 
relation. The set of all possible 
dependency relations from a B-phrase b l 
to a B-phrase b 2 is founded on the 
information of dependent of b I and the 
information of governor of b 2. There- 
fore, there is a function 6 which com- 
putes the set of all possible dependen- 
cy relations ~(a,8) between a B-phrase 
of dependency information ~ and another 
B-phrase of dependency information 8. 
The function ~ is realized by the 
dependency dictionary retrieved with 
the key of two dependency informations. 
The order of B-phrase is relatively 
free in a simple sentence, except for 
one constraint that the predicative 
B-phrase governing the whole sentence 
must be in the sentence's final posi- 
tion. Japanese is a post positional 
in this sense. 
The pattern of the dependency 
relations in a sentence has some 
structural property which is called the 
rules of dependency structure, and the 
dependency relations in a sentence are 
called the dependency structure of a 
sentence. The dependency structure 
of a sentence is shown in figure 2, 
where arrows indicate dependency rela- 
tions of various types. The rules of 
dependency structure consist of follow- 
ing three conditions. 
i Each B-phrase except one at the 
sentence final is a dependent of 
exactly one B-phrase appearing 
after it. 
ii A dependency relation between any 
two B-phrases does not cross with 
another dependency relations in a 
sentence. 
iii No two dependency relations 
depending on the same governor 
are the same. 
Let N be the number of B-phrases in 
a input sentence, and all B-phrases are 
numbered descendingly from right to 
left (see figure 2). We shall fix an 
input sentence, throughout this chapter. 
Let DI(i) be the set of all dependency 
informations of i-th B-phrase. 
Definition: A dependency file DF of 
a sentence is a finite set of 5-tuples. 
(i,j,ai,ej,c) 6 DF 
.... _~ { N=i>j=l, a i E DI (i), 
cde . ~j 6 DI(j) and c E~(ai,aj).  
Definition: If a subset of DF 
satisfies following conditions i) to 
5), it is called a dependency structure 
from the Z-th B-phrase to the m-th B- 
phrase (N~Z>m~i) and denoted by DS(?,m) 
or DS' (i,m). 
i) If (i,J,ei,~j,c) 6 DS(?,m), then 
?~i>jAm. 
2) For arbitrary i(ZAi>m), there 
exists unique j,ai,ej,c such that 
(i,j,ai,ej,c) ~ DS(Z,m). 
(Uniqueness of Dependent) 
3) If (i,j,a~,a~,c) 6 DS(?,m) and 
, , ~ o (j,k,~j,~k,C) % DS(Z,m), then ~ = ~ 
~,  O J "  (Uniqueness of B-phrase structure) 
, 4)  If (i,J,~i,~j,c) ~ DS(?,m), 
(i ,j,~f ,~j, ,c ) E DS(?,m) and i>i'>j, 
then j,hj. 
(Nest Structure of Dependency) 
16 ? 
5) If ( i , j ,@i,~,c)  e DS(?,m) 
(i',j,a i, ,~j,c') ~ D~(?,m) and ~ i ' ,  
then c ~ c'. 
(Inhibition of Duplication of a Case) 
The set of all dependency st ructur~ 
from i-th B-phrase to m-th B-phrase is 
denoted by ~(~,m). Any DS(N,i)~ ~(N,i) 
is called a dependency structure of the 
input sentence. The dependency infor- 
mation of j-th B-phrase is unique in 
DS(i,m), since 2) and 3) hold. Let 
JDiDS(Z,m) and jGDS(Z,m) be the depen- 
dency information of the j-th B-phrase 
inD~?,m)  and~set  of all the depen- 
dency relations that the j-th B-phrase 
governs in DS(?,m), respectively. 
def ~ i ,~ ,C)  JGDS(i,m) u__. {c I ( i , J~Ds(~m) } 
Definition: If the k-th B-phrase 
(i~k~_m) in DS(?,m) has the following 
property, k(the k-th B-phrase) is 
called a joint of DS(?,m): 
For any ( i , j ,ai ,~j,c)~ DS(~,m) , 
k~i or J~k. 
Let j~(=?) > j, > Jl > "'" > j (=m) be 
u , 
the descendlng sequence of ale the 
joints of DS(i,m) (see figure &). 
Then, the Jk-th B-phrase is called the 
k-th joint of DS(?,m). There is a 
dependency relation from k-th joint 
(dependent) to k+i-th joint(governor) 
in DS(?,m). Let J.DS(?,m) be a set of 
all the joints of DS(?,m). DS(?,m/i,j) 
a subset of DS(Z,m), is defined as 
follows: 
DS(?,m/i,j)-~{ (p,q,av,~o,c) I 
(p,q,ap,aq,C) ~ DS(~,my, i~p>q~j}. 
Lemma i. For any positive integer 
?, i, j, m (N~?~i>j~m), the following 
propositions hold. 
(i) DS(i,m/i, j)6 ~(i, j), if j is a 
joint of DS(i,m). 
(ii) DS(I,j) U DS(j,m)~ ~(?,m), if and 
only if JDiDS(Z,j) = JDiDS(j,m) . 
(iii) { (Z+i,j ,~, ~,c) }uDS (~,~) 6 ~(Z+i, 
m) if and only if (i+l,j,e,8,c) 
E DF,8=JDiDS(Z,m), j E J.DS(?,m) 
and c~ jGDS(Z,m). 
(iv) If (jk,Jk+1,ak,ek+1,c) ~ DS(j ,m) 
(k=0,1,2,-..), then Jk is the k- 
th joint of DS(J0,m). 
Syntax analysis of a Japanese 
sentence is defined as giving B-phrase 
structures and dependency structure of 
the sentence. 
2. THE PARSING ALGORITHM 
AND ITS EFFICIENCY 
In this chapter, we shall give a 
parsing method which will parse an 
input sentence using time O(n ~) and 
space O(n~), where n is the length of 
input sentence. Moreover, if the 
dependency information of each B-phrase 
is unambiguous, the time variation is 
quadratic. 
The essence of the parsing algorithm 
is theconst ruct ion  of B-phrase parse 
list BL and dependency parse list DL 
which are constructed essential ly by a 
"dynamic programming" method. The 
parsing algorithm consists of four 
minor algorithms that are the construc- 
tion of BL, the obtaining of B-phrase 
structure, the construction of DL and 
the obtaining of dependency structure. 
13-PHRASE PARSE LIST 
Let b be a string of n length and 
b(i) denote the i-th character from 
the left end of it. 
b=b(1)  (2) ... b(n). 
The B-phrase parse list of b 
consists of n minor lists BL(1), BL(2), 
? .. , BL(n). 
\ [ \ ]Form of items in BL(j) 
(i, WS, DI) 
where, IL_i < j~n,  WS is a word 
structure and DI is a dependency 
information. 
\ [ \ ]  Semantics (i, WS, DI)EBL(j) of 
(i, WS, DI)E BL(j), if and 
only if there exists a sequence of 
words w o, w l, ... , w? satisfying 
following two conditions: 
i) b(1)b(2) ... b(i)=w0w I .. ? w~_ I, 
b(i+l)b(i+2) ... b(j)=w?, and 
WS is the word structure of w?. 
2) The string of word w_w I ... w Z is 
? D a left parclal B-phrase of depen- 
dency information DI. 
ALGORITHM FOR THE CONSTRUCTION OF BL 
Input. An input string b=b(1)(2) 
? ? .b (n )  . 
Output. The B-phrase parse list 
BL(1), BL(2), ... , BL(n). 
Method. Step i: Find all the 
independent word which are the left- 
most subwords of b, using independent 
word dictionary and for each indepen- 
dent word w=b(1)b(2) ''. b(j), add 
(0, (W,H,K),a) to BL(j) where, (W,H,K) 
is the word structure of w and ~= 
(<W,H>K, <W,H,-> d) . Then, set the 
controI word i to 1 and repeat Step 2 
until ~ = n ? 
Step 2: Obtain all the suffix 
words which are the left-most subwords 
of B(i+l)B(i+2) ... b(n) and for each 
suffix word w=b( i+l )b( i+2)  ... b(k) 
of word structure (W' ,H' ,K') , and for 
each item (j, <W,H,K>,a) # BL(i), add 
(i,(W',H',K'), (W',H')oe) to BL(k) if 
--17 
C(H,K,K'). (W',H')0a is a dependency 
information defined as follows. 
i If H' is a auxil iary verb? then 
(W',H')o~ def (<~>g,<W,,H,,_>d) 
where? <a>g is the information of 
governor or a. 
ii Let <W",H",H"' > be the informa- 
tion of dependent of ~. When H' 
is a particle, 
(W,,H,)o a def 
. . . .  (<a>g,<W",H",H'>d) 
if the power of dependency 
function of H' is stronger than 
that of H"' , and else 
(W,,H,)o ~ def ~. 
There exists upper limit in the 
length of words and there exists upper 
limit in the number of dependency 
informations of all left partial B- 
phrase of a(1)a(2) ... a(i). Therefore, 
there exists upper limit for the 
necessary size of memory space of BL(i) 
and the theorem 1 follows. 
Theorem i. 
Algorithm for the construction of 
BL requires O(n) memory space and 
O(n) elementary operations. 
We shall now describe how to find a 
B-phrase structure of specif ied depen- 
dency information from BL. The method 
is given as follows. 
ALGORITHM FOR OBTAINING A B-PHRASE 
STRUCTURE OF AN INPUT STR ING 
Input. The specified dependency 
information ~ and BL. 
Output. A B-phrase structure of 
dependency information a or the error 
signal "error". 
Method. STEP i: Search any item 
(i,(W,H,K),a) in BL(n) such as Termi 
(H,H). If there is no such item, then 
emit "error" and halt. Otherwise, 
output the word structure (W,H,K), set 
the register R to (i,(W,H,K),a) and 
repeat the step 2 until i = 0. 
STEp 2: Let R be (i,(W,H,K),e). 
Search any item (i',(W',H',K'),a') in 
BL(i) such as C(H',K',H) and (W,H) o~=a. 
There exist at least one element which 
satisfies above conditions. "Output 
the word structure (W',H',K') and 
R? (i',(W',H',K'),a'). 
It is easy to know theorem 2 holds. 
Theorem 2. 
A B-phrase structure of specif ied 
dependency information is output by 
the above algorithm, if and only if the 
input string has at least one B-phrase 
structure of specified dependency 
information and it takes constant 
memory space and O(n) elementary 
operations to operate the above 
algorithm. 
The set of all the dependency 
informations DI of input string b is 
obtained from BL(n), since 
DI={a I (i, (W,H,K) ,a)?SL(n) , C(H,K) }. 
DEPENDENCY PARSE LIST DL 
Let s be a input sentence of N B- 
phrases. The set of all the depen- 
dency informations DI(i) of the i-th 
B-phrase is obtained by operating the 
algorithm of construction of BL on 
the string of the i-th B-phrase. 
The dependency parse list DL of s 
consists of N-i minor lists DL(2), 
DL(3) , ''- ,DL(N) . 
\ [ \ ]  of items in Form DL(i). 
(ai,J,aj,~,P) I 
(ai?J,aj, ,P) 
where, N~i  > j~ l ,  aie DI(i), ajE DI(j), 
ce ~, P~ and $ is a specially intro- 
duced symbol. 
I~  Semant ics  of (ai,J,aj,c,P)6DL(i). 
(ai,J,aj,c,P) ~ DL\]i) ? if and 
only if there is a dependency struc- 
ture DS(i,i) of s, where 
(i,J,ai,a~,c) ~ DS(i,i), 
jGDS (i,l) < P. 
~ Semantics of (ai ? j ?~,S,P)6DL(i).  
(ai?J,?j ,$,P) e Dn(1), if and 
only if there is a dependency structure 
DS(i,i) of s, where 
ai=iDiDS(i i) a. :JDiDS(i,i), ? r J 
j is a joint of DS(i,i) except 
O-th or 1st joint, 
jGDS(i,i) =P. 
ALGORITHM FOR THE CONSTRUCTION OF DL 
Input. The sequence of the sets of 
all dependency informations DI(1) , 
DI(2) , ''" ?DI(N) . 
Output. Dependency list DL(2), 
DL(3) , ''" ,DL(N). 
Method. STEP 1 (Construction of 
DL(2))~ For each a e DI(2)? a16 DI(1) 
and cE ~ such that ~ e6(c~2,c~i) , add 
(a2,l,al,c,{c}) to DL(2)? set i to 2 
and repeat the STEP 2 and the STEP 3 
until i = N. 
STEP 2 (Registration of items of 
the form (ai+l,j,aA,c,P)) : For any 
(ai,J,aj?c,P) ~ DL(i) and ~i+16 DI(i+l) , 
compute 6 (ai+ I,~i) and add every 
(ai+l,i,ai,c',{c'}) to DL(i+i) such 
that c'6 6(ei+1,~i). And, for any 
(c~i,J,aj,A,P) 6 DL(i) where A~ ~ ~'{$} 
and ai+1? DI(i+l), compute ~(c~i+1,aA) 
and add every (ai+l,j,c~j,c',PU {c'}~ 
to DL(i+i) such that c'6 ~(ai+1,a j) 
and c'} P. Go to Step 3. 
18 
STEP 3 (Registration of items of 
the form (ai+1,j,ej,$,P)): For any 
(ai+1,j,al,c,P) ~ DL(i+i) and (al,k,ek, 
A,P') # DL~j), add (ei+1,k,ak,$,~') to 
DL(i+i). Then, set i to i+; and go 
to STEP 2. 
Theorem 3. 
If there exist no ambiguity in the 
dependency information of B-phrases of 
input sentence, then the step 3 in 
the above algorithm can be replaced to 
the following step 3'. 
STEP 3': For each (~Ki+!,j,~A,A,P) 
6 DL(Ki+~), add (~i+~,j,aj,~,P) ?o 
DL(i+i), where 
de----~ max{k I (ai+l k ~k,C,P) Ki+l , , 
DL ( i+l )}. 
Then, set i to i+l and go to STEP 2. 
The efficiency of each step of 
above algorithm is as follows. 
The memory size of DL(i) is O(N). 
The step i, the step 2 and the step 
3 take constant, O(N) and O(N ~) 
elementary operations, respectively. 
The step 3' takes O(N) elementary 
operations since it takes O(N) 
elementary operations to compute Ki+ ~ . 
Therefore, the theorem 4 holds. 
Theorem 4. 
The algorithm for the construction 
of DL requires O(N ~) memory space and 
O(N ~) elementary operations. Moreover, 
if there exist no ambiguity in the 
dependency information of each B- 
phrases, the algorithm requires O(N ~) 
elementary operations by replacing the 
step 3 with the step 3' 
We shall now describe how to find a 
dependency structure of input sentence 
from DL. To begin with, we shall 
explain items of partial dependency 
structure list PDSL. 
Form of items in PDSL 
(i,j,a~,a~,P#) 
where, Nh i  ~j ~ i  a# ~ DI(i) ~ {#}, 
~ % DI(j) U {~,  P~ i~ a subset of C or 
#Oand# is specially introduced symbol. 
~ Semantics of (i,j,~#,e#.p#) .~  i j- 
The item (i,j,a~,e~,P#) % PDSL 
means to be a dependenceS- structure 
DS(i, j)~ ~(i,j) such that following 
conditions i),2) and 3) hold. 
i) If a~=~i(%#), then iDiDS (i,j) =e i ? 
2) If e#=aj(~#!,~ then JDiDS(i,j)=aj. 
3) If P~=P(~#). then JGDS(i,j)=P. 
Therefore, (N,i,#,#,#) means to be a 
dependency structure of the input 
sentence. 
ALGORITHM FOR OBTAINING A DEPENDENCY 
STRUCTURE FROM DL 
Input. DL. 
Output. A dependency structure of 
input sentence or the signal "error". 
Method. STEP i: If DL(N) is empty, 
emit the message "error", else, 
initialize PDSL to {(N,i,#,#,#)} and 
repeat step 2 until PDSL becomes empty. 
STEP 2: Take an item freely out of 
PDSL and delete it from PDSL. Accord- 
ing to the form of the item, execute 
i) or 2) or 3). 
i) If the item is (N,i,#,#,#) of 
the form and (aN,J,ej,c,P) ~ DL(N) , 
then output (N,J,eN,ej,c), add (N-i,j, 
#,aj,P/{c}) to PDSL i~ N-i ~ j and add 
(j,l,aj,#,#) to PDSL if j ~ i. 
2) If the item is (i,l,ei,#,#) of 
the form and (j,~i,ej,c,P)E DL(i), 
then output (i,J,ei,e~,c), add (i-l,j, 
#,aj,P/{c}) to PDSL i~ i-i @ j and add 
(j,l,a~,#,#) to PDSL if j @ 1 
3) aIf the item is ( i , j ,~,e~,P) of 
the form, where ~#=~= or #, anda(ai,j, 
~ ,c ,P )  E DL(i), then?output (i,J,~i, 
e~,c) and add (i-l,j,#,~j,P/{c}) to 
PDSL if i-i % j. When there is not 
such item in DL(i), searcha pair of 
items (ei,k,ak,C,P') E DL(i) and (ak,J, 
ej,A,P) ~ DL(k), then output (i,k,ei,ak, 
cy , add (i-l,k,#,~k,P'/{c}) to PDSL 
if i-i @k and add (k,j,~k,ej,P) to PDSL. 
PDSL needs O(N) memory space and 
STEP i, STEP 2 take constant, O(N) 
elementary operations, respectively. 
Theorem 5. 
A-igorithm for obtaining a dependency 
structure from DL requires O(N) memory 
space and O(N 2) elementary operations. 
PARSING ALGORITHM 
Input. A Japanese sentence in collo- 
quial style. 
Output. A dependency structure DS(N, 
i) of the input sentence and a B-phrase 
structure of the j-th B-phrase, whose 
dependency information is JDiDS(N,i), 
for every j(j=l,2, "'" ,N). 
Method. STEP i: Construct N B-phrase 
parse lists of all B-phrases of the 
input sentence and get the sets of 
dependency informations DI(1), DI(2), 
? ." , D I (N) .  
STEP 2: Construct dependency parse 
list DPL from DI(1), DI(2), ... ,DI(N). 
STEP 3: Obtain a dependency 
structure DS(N,i) of the input sentence 
from DL. 
STEP 4: Obtain a B-phrase structure 
of the j-th B-phrase, whose dependency 
information is JDiDS(N,i), for every 
j (j=l,2, ... ,N) and stop. 
-19-- 
Let n~ be the length of j-th B- 
phrase (~=i,2, ?-- ,N), and N,n denote 
the number of B-phrases and the length 
of input sentence, respectively. Then, 
n1+n2+ ... +n N =n 
N Ln  
By theorem i, theorem 2, theorem 4 
and theorem 5, next theorem holds. 
Theorem 6. 
The parsing algorithm requires 
O(n 2) memory space and O(n 3) elementary 
operations. Moreover, if the dependen- 
cy information of each B-phrase is 
unambiguous, it requires O(n 2) elemen- 
tary operations. 
3. CONCLUSION 
Syntax of Japanese sentences is 
stated and a efficient parsing 
algorithm is given. A Japanese sen- 
tence in colloquial style is parsed b Y 
the parsing algorithm, using time O(n ~) 
and memory space O(n2), where n is the 
length of input sentence. Moreover, 
it is parsed using time O(n 2) whenever 
dependency information of every B- 
phrase is unambiguous. 
REFERENCES 
i. Aho, Ullman : "The Theory of 
Parsing, Translation, and Compil- 
ing", Prentice Hall vol. 1 (1975). 
2. Woods : "Transition Network 
Grammars for Natural Language 
Analysis", Communication of the 
ACM, 13 (1970). 
3. Pratt : "LINGOL -- A Progress 
Report", Proc. IJCAI 4 (1975). 
(~) (~s) (s) (2) 0-) 
J0 =5) Jl (=4) J2 (=3) J3 (=i) 
: main part a: agent 
- - :  annex part p: patient 
J0,Jl,J~,J3 : the sequence of joint 
Figure 2. Dependency Structure 
Example: Taro read the composition 
written by Hanako. 
Figure i. Ways of Spacing 
20 
