Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 20?27,
Columbus, Ohio, USA June 2008. c?2008 Association for Computational Linguistics
Unsupervised word segmentation for Sesotho using Adaptor Grammars
Mark Johnson
Brown University
Mark Johnson@Brown.edu
Abstract
This paper describes a variety of non-
parametric Bayesian models of word segmen-
tation based on Adaptor Grammars that model
different aspects of the input and incorporate
different kinds of prior knowledge, and ap-
plies them to the Bantu language Sesotho.
While we find overall word segmentation ac-
curacies lower than these models achieve on
English, we also find some interesting dif-
ferences in which factors contribute to better
word segmentation. Specifically, we found lit-
tle improvement to word segmentation accu-
racy when we modeled contextual dependen-
cies, while modeling morphological structure
did improve segmentation accuracy.
1 Introduction
A Bayesian approach to learning (Bishop, 2006) is
especially useful for computational models of lan-
guage acquisition because we can use it to study
the effect of different kinds and amounts of prior
knowledge on the learning process. The Bayesian
approach is agnostic as to what this prior knowl-
edge might consist of; the prior could encode the
kinds of rich universal grammar hypothesised by
e.g., Chomsky (1986), or it could express a vague
non-linguistic preference for simpler as opposed to
more complex models, as in some of the grammars
discussed below. Clearly there?s a wide range of
possible priors, and one of the exciting possibilities
raised by Bayesian methods is that we may soon be
able to empirically evaluate the potential contribu-
tion of different kinds of prior knowledge to lan-
guage learning.
The Bayesian framework is surprisingly flexible.
The bulk of the work on Bayesian inference is on
parametric models, where the goal is to learn the
value of a set of parameters (much as in Chomsky?s
Principles and Parameters conception of learning).
However, recently Bayesian methods for nonpara-
metric inference have been developed, in which the
parameters themselves, as well as their values, are
learned from data. (The term ?nonparametric? is
perhaps misleading here: it does not mean that the
models have no parameters, rather it means that the
learning process considers models with different sets
of parameters). One can think of the prior as pro-
viding an infinite set of possible parameters, from
which a learner selects a subset with which to model
their language.
If one pairs each of these infinitely-many pa-
rameters with possible structures (or equivalently,
rules that generate such structures) then these non-
parametric Bayesian learning methods can learn
the structures relevant to a language. Determining
whether methods such as these can in fact learn lin-
guistic structure bears on the nature vs. nurture de-
bates in language acquisition, since one of the argu-
ments for the nativist position is that there doesn?t
seem to be a way to learn structure from the input
that children receive.
While there?s no reason why these methods can?t
be used to learn the syntax and semantics of human
languages, much of the work to date has focused on
lower-level learning problems such as morphologi-
cal structure learning (Goldwater et al, 2006b) and
word segmentation, where the learner is given un-
segmented broad-phonemic utterance transcriptions
20
and has to identify the word boundaries (Goldwater
et al, 2006a; Goldwater et al, 2007). One reason for
this is that these problems seem simpler than learn-
ing syntax, where the non-linguistic context plausi-
bly supplies important information to human learn-
ers. Virtually everyone agrees that the set of possible
morphemes and words, if not infinite, is astronom-
ically large, so it seems plausible that humans use
some kind of nonparametric procedure to learn the
lexicon.
Johnson et al (2007) introduced Adaptor Gram-
mars as a framework in which a wide variety
of linguistically-interesting nonparametric inference
problems can be formulated and evaluated, includ-
ing a number of variants of the models described by
Goldwater (2007). Johnson (2008) presented a vari-
ety of different adaptor grammar word segmentation
models and applied them to the problem of segment-
ing Brent?s phonemicized version of the Bernstein-
Ratner corpus of child-directed English (Bernstein-
Ratner, 1987; Brent, 1999). The main results of that
paper were the following:
1. it confirmed the importance of modeling con-
textual dependencies above the word level for
word segmentation (Goldwater et al, 2006a),
2. it showed a small but significant improvement
to segmentation accuracy by learning the possi-
ble syllable structures of the language together
with the lexicon, and
3. it found no significant advantage to learning
morphological structure together with the lex-
icon (indeed, that model confused morphologi-
cal and lexical structure).
Of course the last result is a null result, and it?s pos-
sible that a different model would be able to usefuly
combine morphological learning with word segmen-
tation.
This paper continues that research by applying
the same kinds of models to Sesotho, a Bantu lan-
guage spoken in Southern Africa. Bantu languages
are especially interesting for this kind of study, as
they have rich productive agglutinative morpholo-
gies and relatively transparent phonologies, as com-
pared to languages such as Finnish or Turkish which
have complex harmony processes and other phono-
logical complexities. The relative clarity of Bantu
has inspired previous computational work, such as
the algorithm for learning Swahili morphology by
Hu et al (2005). The Hu et al algorithm uses
a Minimum Description Length procedure (Rissa-
nen, 1989) that is conceptually related to the non-
parametric Bayesian procedure used here. However,
the work here is focused on determining whether the
word segmentation methods that work well for En-
glish generalize to Sesotho and whether modeling
morphological and/or syllable structure improves
Sesotho word segmentation, rather than learning
Sesotho morphological structure per se.
The rest of this paper is structured as follows.
Section 2 informally reviews adaptor grammars and
describes how they are used to specify different
Bayesian models. Section 3 describes the Sesotho
corpus we used and the specific adaptor grammars
we used for word segmentation, and section 5 sum-
marizes and concludes the paper.
2 Adaptor grammars
One reason why Probabilistic Context-Free Gram-
mars (PCFGs) are interesting is because they are
very simple and natural models of hierarchical struc-
ture. They are parametric models because each
PCFG has a fixed number of rules, each of which
has a numerical parameter associated with it. One
way to construct nonparametric Bayesian models is
to take a parametric model class and let one or more
of their components grow unboundedly.
There are two obvious ways to construct nonpara-
metric models from PCFGs. First, we can let the
number of nonterminals grow unboundedly, as in the
Infinite PCFG, where the nonterminals of the gram-
mar can be indefinitely refined versions of a base
PCFG (Liang et al, 2007). Second, we can fix the
set of nonterminals but permit the number of rules
or productions to grow unboundedly, which leads to
Adaptor Grammars (Johnson et al, 2007).
At any point in learning, an Adaptor Grammar has
a finite set of rules, but these can grow unbound-
edly (typically logarithmically) with the size of the
training data. In a word-segmentation application
these rules typically generate words or morphemes,
so the learner is effectively learning the morphemes
and words of its language.
The new rules learnt by an Adaptor Grammar are
21
compositions of old ones (that can themselves be
compositions of other rules), so it?s natural to think
of these new rules as tree fragments, where each
entire fragment is associated with its own proba-
bility. Viewed this way, an adaptor grammar can
be viewed as learning the tree fragments or con-
structions involved in a language, much as in Bod
(1998). For computational reasons adaptor gram-
mars require these fragments to consist of subtrees
(i.e., their yields are terminals).
We now provide an informal description of Adap-
tor Grammars (for a more formal description see
Johnson et al (2007)). An adaptor grammar con-
sists of terminals V , nonterminals N (including a
start symbol S), initial rules R and rule probabilities
p, just as in a PCFG. In addition, it also has a vec-
tor of concentration parameters ?, where ?A ? 0 is
called the (Dirichlet) concentration parameter asso-
ciated with nonterminal A.
The nonterminals A for which ?A > 0 are
adapted, which means that each subtree for A that
can be generated using the initial rules R is consid-
ered as a potential rule in the adaptor grammar. If
?A = 0 then A is unadapted, which means it ex-
pands just as in an ordinary PCFG.
Adaptor grammars are so-called because they
adapt both the subtrees and their probabilities to the
corpus they are generating. Formally, they are Hi-
erarchical Dirichlet Processes that generate a distri-
bution over distributions over trees that can be de-
fined in terms of stick-breaking processes (Teh et al,
2006). It?s probably easiest to understand them in
terms of their conditional or sampling distribution,
which is the probability of generating a new tree T
given the trees (T1, . . . , Tn) that the adaptor gram-
mar has already generated.
An adaptor grammar can be viewed as generating
a tree top-down, just like a PCFG. Suppose we have
a node A to expand. If A is unadapted (i.e., ?A = 0)
then A expands just as in a PCFG, i.e., we pick a
rule A ? ? ? R with probability pA?? and recur-
sively expand ?. If A is adapted and has expanded
nA times before, then:
1. A expands to a subtree ? with probability
n?/(nA+?A), where n? is the number of times
A has expanded to subtree ? before, and
2. A expands to ? where A ? ? ? R with prob-
ability ?A pA??/(nA + ?A).
Thus an adapted nonterminal A expands to a previ-
ously expanded subtree ? with probability propor-
tional to the number n? of times it was used before,
and expands just as in a PCFG (i.e., using R) with
probability proportional to the concentration param-
eter ?A. This parameter specifies how likely A is to
expand into a potentially new subtree; as nA and n?
grow this becomes increasingly unlikely.
We used the publically available adaptor gram-
mar inference software described in Johnson et al
(2007), which we modified slightly as described be-
low. The basic algorithm is a Metropolis-within-
Gibbs or Hybrid MCMC sampler (Robert and
Casella, 2004), which resamples the parse tree for
each sentence in the training data conditioned on the
parses for the other sentences. In order to produce
sample parses efficiently the algorithm constructs a
PCFG approximation to the adaptor grammar which
contains one rule for each adapted subtree ?, and
uses a Metropolis accept/reject step to correct for the
difference between the true adaptor grammar dis-
tribution and the PCFG approximation. With the
datasets described below less than 0.1% of proposal
parses from this PCFG approximation are rejected,
so it is quite a good approximation to the adaptor
grammar distribution.
On the other hand, at convergence this algorithm
produces a sequence of samples from the posterior
distribution over adaptor grammars, and this poste-
rior distribution seems quite broad. For example,
at convergence with the most stable of our models,
each time a sentence?s parse is resampled there is
an approximately 25% chance of the parse chang-
ing. Perhaps this is not surprising given the com-
paratively small amount of training data and the fact
that the models only use fairly crude distributional
information.
As just described, adaptor grammars require the
user to specify a concentration parameter ?A for
each adapted nonterminal A. It?s not obvious how
this should be done. Previous work has treated ?A
as an adjustable parameter, usually tying all of the
?A to some shared value which is adjusted to opti-
mize task performance (say, word segmentation ac-
curacy). Clearly, this is undesirable.
Teh et al (2006) describes how to learn the con-
22
centration parameters ?, and we modified their pro-
cedure for adaptor grammars. Specifically, we put
a vague Gamma(10, 0.1) prior on each ?A, and af-
ter each iteration through the training data we per-
formed 100 Metropolis-Hastings resampling steps
for each ?A from an increasingly narrow Gamma
proposal distribution. We found that the perfor-
mance of the models with automatically learned
concentration parameters ? was generally as good
as the models where ? was tuned by hand (although
admittedly we only tried three or four different val-
ues for ?).
3 Models of Sesotho word segmentation
We wanted to make our Sesotho corpus as similar
as possible to one used in previous work on word
segmentation. We extracted all of the non-child
utterances from the LI?LV files from the Sesotho
corpus of child speech (Demuth, 1992), and used
the Sesotho gloss as our gold-standard corpus (we
did not phonemicize them as Sesotho orthography
is very close to phonemic). This produced 8,503
utterances containing 21,037 word tokens, 30,200
morpheme tokens and 100,113 phonemes. By com-
parison, the Brent corpus contains 9,790 utterances,
33,399 word tokens and 95,809 phonemes. Thus
the Sesotho corpus contains approximately the same
number of utterances and phonemes as the Brent
corpus, but far fewer (and hence far longer) words.
This is not surprising as the Sesotho corpus involves
an older child and Sesotho, being an agglutinative
language, tends to have morphologically complex
words.
In the subsections that follow we describe a vari-
ety of adaptor grammar models for word segmenta-
tion. All of these models were given same Sesotho
data, which consisted of the Sesotho gold-standard
corpus described above with all word boundaries
(spaces) and morpheme boundaries (hyphens) re-
moved. We computed the f-score (geometric aver-
age of precision and recall) with which the models
recovered the words or the morphemes annotated in
the gold-standard corpus.
3.1 Unigram grammar
We begin by describing an adaptor grammar that
simulates the unigram word segmentation model
Model word f-score morpheme f-score
word 0.431 0.352
colloc 0.478 0.387
colloc2 0.467 0.389
word ? syll 0.502 0.349
colloc? syll 0.476 0.372
colloc2? syll 0.490 0.393
word ?morph 0.529 0.321
word ? smorph 0.556 0.378
colloc? smorph 0.537 0.352
Table 1: Summary of word and morpheme f-scores for
the different models discussed in this paper.
proposed by Goldwater et al (2006a). In this model
each utterance is generated as a sequence of words,
and each word is a sequence of phonemes. This
grammar contains three kinds of rules, including
rules that expand the nonterminal Phoneme to all of
the phonemes seen in the training data.
Sentence ? Word+
Word ? Phoneme+
Adapted non-terminals are indicated by underlin-
ing, so in the word grammar only the Word nonter-
minal is adapted. Our software doesn?t permit reg-
ular expressions in rules, so we expand all Kleene
stars in rules into right-recursive structures over new
unadapted nonterminals. Figure 1 shows a sample
parse tree generated by this grammar for the sen-
tence:
u-
SM-
e-
OM-
nk-
take-
il-
PERF-
e
IN
kae
where
?You took it from where??
This sentence shows a typical inflected verb, with a
subject marker (glossed SM), an object marker (OM),
perfect tense marker (PERF) and mood marker (IN).
In order to keep the trees a managable size, we only
display the root node, leaf nodes and nodes labeled
with adapted nonterminals.
The word grammar has a word segmentation f-
score of 43%, which is considerably below the 56%
f-score the same grammar achieves on the Brent cor-
pus. This difference presumably reflects the fact that
Sesotho words are longer and more complex, and so
segmentation is a harder task.
We actually ran the adaptor grammar sampler for
23
Sentence
Word
u e n k i l e
Word
k a e
Figure 1: A sample (correct) parse tree generated by the
word adaptor grammar for a Sesotho utterance.
the word grammar four times (as we did for all gram-
mars discussed in this paper). Because the sampler
is non-deterministic, each run produced a different
series of sample segmentations. However, the av-
erage segmentation f-score seems to be very stable.
The accuracies of the final sample of the four runs
ranges between 42.8% and 43.7%. Similarly, one
can compute the average f-score over the last 100
samples for each run; the average f-score ranges be-
tween 42.6% and 43.7%. Thus while there may
be considerable uncertainty as to where the word
boundaries are in any given sentence (which is re-
flected in fact that the word boundaries are very
likely to change from sample to sample), the aver-
age accuracy of such boundaries seems very stable.
The final sample grammars contained the initial
rules R, together with between 1,772 and 1,827 ad-
ditional expansions for Word, corresponding to the
cached subtrees for the adapted Word nonterminal.
3.2 Collocation grammar
Goldwater et al (2006a) showed that incorporating a
bigram model of word-to-word dependencies signif-
icantly improves word segmentation accuracy in En-
glish. While it is not possible to formulate such a bi-
gram model as an adaptor grammar, Johnson (2008)
showed that a similar improvement can be achieved
in an adaptor grammar by explicitly modeling col-
locations or sequences of words. The colloc adaptor
grammar is:
Sentence ? Colloc+
Colloc ? Word+
Word ? Phoneme+
This grammar generates a Sentence as a sequence
of Colloc(ations), where each Colloc(ation) is a se-
quence of Words. Figure 2 shows a sample parse tree
generated by the colloc grammar. In terms of word
segmentation, this grammar performs much worse
Sentence
Colloc
Word
u e
Word
n
Word
k i l e
Colloc
Word
k a
Colloc
Word
e
Figure 2: A sample parse tree generated by the colloc
grammar. The substrings generated by Word in fact tend
to be morphemes and Colloc tend to be words, which is
how they are evaluated in Table 1.
than the word grammar, with an f-score of 27%.
In fact, it seems that the Word nonterminals typ-
ically expand to morphemes and the Colloc nonter-
minals typically expand to words. It makes sense
that for a language like Sesotho, when given a gram-
mar with a hierarchy of units, the learner would use
the lower-level units as morphemes and the higher-
level units as words. If we simply interpret the Word
trees as morphemes and the Colloc trees as words
we get a better word segmentation accuracy of 48%
f-score.
3.3 Adding more levels
If two levels are better than one, perhaps three levels
would be better than two? More specifically, per-
haps adding another level of adaptation would per-
mit the model to capture the kind of interword con-
text dependencies that improved English word seg-
mentation. Our colloc2 adaptor grammar includes
the following rules:
Sentence ? Colloc+
Colloc ? Word+
Word ? Morph+
Morph ? Phoneme+
This grammar generates sequences of Words
grouped together in collocations, as in the previous
grammar, but each Word now consists of a sequence
of Morph(emes). Figure 3 shows a sample parse tree
generated by the colloc2 grammar.
Interestingly, word segmentation f-score is
46.7%, which is slightly lower than that obtained
by the simpler colloc grammar. Informally, it seems
that when given an extra level of structure the
colloc2 model uses it to describe structure internal
24
Sentence
Colloc
Word
Morph
u
Morph
e
Word
Morph
n k i
Morph
l e
Word
Morph
k a
Morph
e
Figure 3: A sample parse tree generated by the colloc2
grammar.
to the word, rather than to capture interword depen-
dencies. Perhaps this shouldn?t be surprising, since
Sesotho words in this corpus are considerably more
complex than the English words in the Brent corpus.
4 Adding syllable structure
Johnson (2008) found a small but significant im-
provement in word segmentation accuracy by using
an adaptor grammar that models English words as
a sequence of syllables. The word? syll grammar
builds in knowledge that syllables consist of an op-
tional Onset, a Nuc(leus) and an optional Coda, and
knows that Onsets and Codas are composes of con-
sonants and that Nucleii are vocalic (and that syl-
labic consonsants are possible Nucleii), and learns
the possible syllables of the language. The rules in
the adaptor grammars that expand Word are changed
to the following:
Word ? Syll+
Syll ? (Onset) Nuc (Coda)
Syll ? SC
Onset ? C+
Nuc ? V+
Coda ? C+
In this grammar C expands to any consonant and V
expands to any vowel, SC expands to the syllablic
consonants ?l?, ?m? ?n? and ?r?, and parentheses indi-
cate optionality. Figure 4 shows a sample parse tree
produced by the word ? syll adaptor grammar (i.e.,
where Words are generated by a unigram model),
while Figure 5 shows a sample parse tree generated
by the corresponding colloc? syll adaptor grammar
(where Words are generated as a part of a Colloca-
tion).
Sentence
Word
Syll
u
Syll
e
Syll
n k i
Syll
l e
Word
Syll
k a e
Figure 4: A sample parse tree generated by the
word? syll grammar, in which Words consist of se-
quences of Syll(ables).
Sentence
Colloc
Word
Syll
u
Word
Syll
e
Word
Syll
n k i
Syll
l e
Colloc
Word
Syll
k a e
Figure 5: A sample parse tree generated by the
colloc? syll grammar, in which Colloc(ations) consist of
sequences of Words, which in turn consist of sequences
of Syll(ables).
Building in this knowledge of syllable struc-
ture does improve word segmentation accuracy,
but the best performance comes from the simplest
word ? syll grammar (with a word segmentation f-
score of 50%).
4.1 Tracking morphological position
As we noted earlier, the various Colloc grammars
wound up capturing a certain amount of morpholog-
ical structure, even though they only implement a
relatively simple unigram model of morpheme word
order. Here we investigate whether we can im-
prove word segmentation accuracy with more so-
phisticated models of morphological structure.
The word?morph grammar generates a word as
a sequence of one to five morphemes. The relevant
productions are the following:
Word ? T1 (T2 (T3 (T4 (T5))))
T1 ? Phoneme+
T2 ? Phoneme+
T3 ? Phoneme+
T4 ? Phoneme+
T5 ? Phoneme+
25
Sentence
Word
T1
u e
T2
n k i l e
T3
k a e
Figure 6: A sample parse tree generated by the
word?morph grammar, in which Words consist of mor-
phemes T1?T5, each of which is associated with specific
lexical items.
While each morpheme is generated by a unigram
character model, because each of these five mor-
pheme positions is independently adapted, the gram-
mar can learn which morphemes prefer to appear in
which position. Figure 6 contains a sample parse
generated by this grammar. Modifying the gram-
mar in this way significantly improves word seg-
mentation accuracy, achieving a word segmentation
f-score of 53%.
Inspired by this, we decided to see what would
happen if we built-in some specific knowledge of
Sesotho morphology, namely that a word consists of
a stem plus an optional suffix and zero to three op-
tional prefixes. (This kind of information is often
built into morphology learning models, either ex-
plicitly or implicitly via restrictions on the search
procedure). The resulting grammar, which we call
word ? smorph, generates words as follows:
Word ? (P1 (P2 (P3))) T (S)
P1 ? Phoneme+
P2 ? Phoneme+
P3 ? Phoneme+
T ? Phoneme+
S ? Phoneme+
Figure 7 contains a sample parse tree generated
by this grammar. Perhaps not surprisingly, with this
modification the grammar achieves the highest word
segmentation f-score of any of the models examined
in this paper, namely 55.6%.
Of course, this morphological structure is per-
fectly compatible with models which posit higher-
level structure than Words. We can replace the Word
expansion in the colloc grammar with one just given;
the resulting grammar is called colloc? smorph,
and a sample parse tree is given in Figure 8. Interest-
Sentence
Word
P1
u
P2
e
T
n k
S
i l e
Word
T
k a
S
e
Figure 7: A sample parse tree generated by the
word? smorph grammar, in which Words consist of up
to five morphemes that satisfy prespecified ordering con-
straints.
Sentence
Colloc
Word
P1
u e
T
n
S
k i l e
Word
T
k a
S
e
Figure 8: A sample parse tree generated by the
colloc? smorph grammar, in which Colloc(ations) gen-
erate a sequence of Words, which in turn consist of up
to five morphemes that satisfy prespecified ordering con-
straints.
ingly, this grammar achieves a lower accuracy than
either of the two word-based morphology grammars
we considered above.
5 Conclusion
Perhaps the most important conclusion to be drawn
from this paper is that the methods developed for
unsupervised word segmentation for English also
work for Sesotho, despite its having radically dif-
ferent morphological structures to English. Just as
with English, more structured adaptor grammars can
achieve better word-segmentation accuracies than
simpler ones. While we find overall word segmen-
tation accuracies lower than these models achieve
on English, we also found some interesting differ-
ences in which factors contribute to better word seg-
mentation. Perhaps surprisingly, we found little
improvement to word segmentation accuracy when
we modeled contextual dependencies, even though
these are most important in English. But includ-
ing either morphological structure or syllable struc-
ture in the model improved word segmentation accu-
26
racy markedly, with morphological structure making
a larger impact. Given how important morphology is
in Sesotho, perhaps this is no surprise after all.
Acknowledgments
I?d like to thank Katherine Demuth for the Sesotho
data and help with Sesotho morphology, my collabo-
rators Sharon Goldwater and Tom Griffiths for their
comments and suggestions about adaptor grammars,
and the anonymous SIGMORPHON reviewers for
their careful reading and insightful comments on the
original abstract. This research was funded by NSF
awards 0544127 and 0631667.
References
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck, editors,
Children?s Language, volume 6. Erlbaum, Hillsdale,
NJ.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
Rens Bod. 1998. Beyond grammar: an experience-based
theory of language. CSLI Publications, Stanford, Cal-
ifornia.
M. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery. Ma-
chine Learning, 34:71?105.
Noam Chomsky. 1986. Knowledge of Language: Its
Nature, Origin and Use. Praeger, New York.
Katherine Demuth. 1992. Acquisition of Sesotho.
In Dan Slobin, editor, The Cross-Linguistic Study
of Language Acquisition, volume 3, pages 557?638.
Lawrence Erlbaum Associates, Hillsdale, N.J.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006a. Contextual dependencies in unsupervised
word segmentation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 673?680, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Sharon Goldwater, Tom Griffiths, and Mark Johnson.
2006b. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Scho?lkopf, and J. Platt, editors, Advances in Neural
Information Processing Systems 18, pages 459?466,
Cambridge, MA. MIT Press.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2007. Distributional cues to word boundaries:
Context is important. In David Bamman, Tatiana
Magnitskaia, and Colleen Zaller, editors, Proceedings
of the 31st Annual Boston University Conference on
Language Development, pages 239?250, Somerville,
MA. Cascadilla Press.
Sharon Goldwater. 2007. Nonparametric Bayesian Mod-
els of Lexical Acquisition. Ph.D. thesis, Brown Uni-
versity.
Yu Hu, Irina Matveeva, John Goldsmith, and Colin
Sprague. 2005. Refining the SED heuristic for mor-
pheme discovery: Another look at Swahili. In Pro-
ceedings of the Workshop on Psychocomputational
Models of Human Language Acquisition, pages 28?35,
Ann Arbor, Michigan, June. Association for Computa-
tional Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Adaptor Grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
In B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems 19,
pages 641?648. MIT Press, Cambridge, MA.
Mark Johnson. 2008. Using adaptor grammars to identi-
fying synergies in the unsupervised acquisition of lin-
guistic structure. In Proceedings of the 46th Annual
Meeting of the Association of Computational Linguis-
tics, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical Dirichlet
processes. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 688?697.
Rissanen. 1989. Stochastic Complexity in Statistical In-
quiry. World Scientific Company, Singapore.
Christian P. Robert and George Casella. 2004. Monte
Carlo Statistical Methods. Springer.
Y. W. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hier-
archical Dirichlet processes. Journal of the American
Statistical Association, 101:1566?1581.
27
