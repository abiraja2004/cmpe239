Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 81?86,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 16: Evaluation of Wide Coverage Knowledge Resources
Montse Cuadros
TALP Research Center
Universitat Polite?cnica de Catalunya
Barcelona, Spain
cuadros@lsi.upc.edu
German Rigau
IXA NLP Group
Euskal Herriko Unibersitatea
Donostia, Spain
german.rigau@ehu.es
Abstract
This task tries to establish the relative qual-
ity of available semantic resources (derived
by manual or automatic means). The qual-
ity of each large-scale knowledge resource
is indirectly evaluated on a Word Sense Dis-
ambiguation task. In particular, we use
Senseval-3 and SemEval-2007 English Lex-
ical Sample tasks as evaluation bechmarks
to evaluate the relative quality of each re-
source. Furthermore, trying to be as neu-
tral as possible with respect the knowledge
bases studied, we apply systematically the
same disambiguation method to all the re-
sources. A completely different behaviour is
observed on both lexical data sets (Senseval-
3 and SemEval-2007).
1 Introduction
Using large-scale knowledge bases, such as Word-
Net (Fellbaum, 1998), has become a usual, often
necessary, practice for most current Natural Lan-
guage Processing (NLP) systems. Even now, build-
ing large and rich enough knowledge bases for
broad?coverage semantic processing takes a great
deal of expensive manual effort involving large re-
search groups during long periods of development.
In fact, dozens of person-years have been invested in
the development of wordnets for various languages
(Vossen, 1998). For example, in more than ten years
of manual construction (from version 1.5 to 2.1),
WordNet passed from 103,445 semantic relations to
245,509 semantic relations1. That is, around one
thousand new relations per month. But this data
does not seems to be rich enough to support ad-
vanced concept-based NLP applications directly. It
seems that applications will not scale up to work-
ing in open domains without more detailed and rich
general-purpose (and also domain-specific) seman-
tic knowledge built by automatic means.
Fortunately, during the last years, the research
community has devised a large set of innovative
methods and tools for large-scale automatic acqui-
sition of lexical knowledge from structured and un-
structured corpora. Among others we can men-
tion eXtended WordNet (Mihalcea and Moldovan,
2001), large collections of semantic preferences ac-
quired from SemCor (Agirre and Martinez, 2001;
Agirre and Martinez, 2002) or acquired from British
National Corpus (BNC) (McCarthy, 2001), large-
scale Topic Signatures for each synset acquired from
the web (Agirre and de la Calle, 2004) or acquired
from the BNC (Cuadros et al, 2005). Obviously,
these semantic resources have been acquired using a
very different set of methods, tools and corpora, re-
sulting on a different set of new semantic relations
between synsets (or between synsets and words).
Many international research groups are working
on knowledge-based WSD using a wide range of ap-
proaches (Mihalcea, 2006). However, less attention
has been devoted on analysing the quality of each
semantic resource. In fact, each resource presents
different volume and accuracy figures (Cuadros et
al., 2006).
In this paper, we evaluate those resources on the
1Symmetric relations are counted only once.
81
SemEval-2007 English Lexical Sample task. For
comparison purposes, we also include the results of
the same resources on the Senseval-3 English Lex-
ical sample task. In both cases, we used only the
nominal part of both data sets and we also included
some basic baselines.
2 Evaluation Framework
In order to compare the knowledge resources, all the
resources are evaluated as Topic Signatures (TS).
That is, word vectors with weights associated to a
particular synset. Normally, these word vectors are
obtained by collecting from the resource under study
the word senses appearing as direct relatives. This
simple representation tries to be as neutral as possi-
ble with respect to the resources studied.
A common WSD method has been applied to
all knowledge resources on the test examples of
Senseval-3 and SemEval-2007 English lexical sam-
ple tasks. A simple word overlapping counting is
performed between the Topic Signature and the test
example. The synset having higher overlapping
word counts is selected. In fact, this is a very sim-
ple WSD method which only considers the topical
information around the word to be disambiguated.
Finally, we should remark that the results are not
skewed (for instance, for resolving ties) by the most
frequent sense in WN or any other statistically pre-
dicted knowledge.
As an example, table 1 shows a test example of
SemEval-2007 corresponding to the first sense of the
noun capital. In bold there are the words that appear
in its corresponding Topic Signature acquired from
the web.
Note that although there are several important
related words, the WSD process implements ex-
act word form matching (no preprocessing is per-
formed).
2.1 Basic Baselines
We have designed a number of basic baselines in
order to establish a complete evaluation framework
for comparing the performance of each semantic re-
source on the English WSD tasks.
RANDOM: For each target word, this method se-
lects a random sense. This baseline can be consid-
ered as a lower-bound.
Baselines P R F1
TRAIN 65.1 65.1 65.1
TRAIN-MFS 54.5 54.5 54.5
WN-MFS 53.0 53.0 53.0
SEMCOR-MFS 49.0 49.1 49.0
RANDOM 19.1 19.1 19.1
Table 2: P, R and F1 results for English Lexical Sam-
ple Baselines of Senseval-3
SemCor MFS (SEMCOR-MFS): This method
selects the most frequent sense of the target word
in SemCor.
WordNet MFS (WN-MFS): This method selects
the first sense in WN1.6 of the target word.
TRAIN-MFS: This method selects the most fre-
quent sense in the training corpus of the target word.
Train Topic Signatures (TRAIN): This baseline
uses the training corpus to directly build a Topic Sig-
nature using TFIDF measure for each word sense.
Note that this baseline can be considered as an
upper-bound of our evaluation.
Table 2 presents the precision (P), recall (R) and
F1 measure (harmonic mean of recall and preci-
sion) of the different baselines in the English Lexical
Sample exercise of Senseval-3. In this table, TRAIN
has been calculated with a vector size of at maxi-
mum 450 words. As expected, RANDOM baseline
obtains the poorest result. The most frequent senses
obtained from SemCor (SEMCOR-MFS) and WN
(WN-MFS) are both below the most frequent sense
of the training corpus (TRAIN-MFS). However, all
of them are far below the Topic Signatures acquired
using the training corpus (TRAIN).
Table 3 presents the precision (P), recall (R) and
F1 measure (harmonic mean of recall and preci-
sion) of the different baselines in the English Lexical
Sample exercise of SemEval-2007. Again, TRAIN
has been calculated with a vector size of at max-
imum 450 words. As before, RANDOM baseline
obtains the poorest result. The most frequent senses
obtained from SemCor (SEMCOR-MFS) and WN
(WN-MFS) are both far below the most frequent
sense of the training corpus (TRAIN-MFS), and all
of them are below the Topic Signatures acquired us-
ing the training corpus (TRAIN).
Comparing both lexical sample sets, SemEval-
2007 data appears to be more skewed and simple for
WSD systems than the data set from Senseval-3: less
82
<instance id=?19:0@11@wsj/01/wsj 0128@wsj@en@on? docsrc=?wsj?> <context>
? A sweeping restructuring of the industry is possible . ? Standard & Poor ?s Corp. says First Boston , Shearson
and Drexel Burnham Lambert Inc. , in particular , are likely to have difficulty shoring up their credit standing in
months ahead . What worries credit-rating concerns the most is that Wall Street firms are taking long-term risks
with their own <head> capital </head> via leveraged buy-out and junk bond financings . That ?s a departure from
their traditional practice of transferring almost all financing risks to investors . Whereas conventional securities
financings are structured to be sold quickly , Wall Street ?s new penchant for leveraged buy-outs and junk bonds is
resulting in long-term lending commitments that stretch out for months or years .
</context> </instance>
Table 1: Example of test id for capital#n which its correct sense is 1
Baselines P R F1
TRAIN 87.6 87.6 87.6
TRAIN-MFS 81.2 79.6 80.4
WN-MFS 66.2 59.9 62.9
SEMCOR-MFS 42.4 38.4 40.3
RANDOM 27.4 27.4 27.4
Table 3: P, R and F1 results for English Lexical Sam-
ple Baselines of SemEval-2007
polysemous (as shown by the RANDOM baseline),
less similar than SemCor word sense frequency dis-
tributions (as shown by SemCor-MFS), more simi-
lar to the first sense of WN (as shown by WN-MFS),
much more skewed to the first sense of the training
corpus (as shown by TRAIN-MFS), and much more
easy to be learned (as shown by TRAIN).
3 Large scale knowledge Resources
The evaluation presented here covers a wide range
of large-scale semantic resources: WordNet (WN)
(Fellbaum, 1998), eXtended WordNet (Mihalcea
and Moldovan, 2001), large collections of seman-
tic preferences acquired from SemCor (Agirre and
Martinez, 2001; Agirre and Martinez, 2002) or ac-
quired from the BNC (McCarthy, 2001), large-scale
Topic Signatures for each synset acquired from the
web (Agirre and de la Calle, 2004) or SemCor (Lan-
des et al, 2006).
Although these resources have been derived us-
ing different WN versions, using the technology for
the automatic alignment of wordnets (Daude? et al,
2003), most of these resources have been integrated
into a common resource called Multilingual Cen-
tral Repository (MCR) (Atserias et al, 2004) main-
taining the compatibility among all the knowledge
resources which use a particular WN version as a
sense repository. Furthermore, these mappings al-
low to port the knowledge associated to a particular
WN version to the rest of WN versions.
The current version of the MCR contains 934,771
semantic relations between synsets, most of them
acquired by automatic means. This represents al-
most four times larger than the Princeton WordNet
(245,509 unique semantic relations in WordNet 2.1).
Hereinafter we will refer to each semantic re-
source as follows:
WN (Fellbaum, 1998): This resource uses the
direct relations encoded in WN1.6 or WN2.0 (for
instance, tree#n#1?hyponym?>teak#n#2). We also
tested WN2 (using relations at distances 1 and 2),
WN3 (using relations at distances 1 to 3) and WN4
(using relations at distances 1 to 4).
XWN (Mihalcea and Moldovan, 2001): This re-
source uses the direct relations encoded in eXtended
WN (for instance, teak#n#2?gloss?>wood#n#1).
WN+XWN: This resource uses the direct rela-
tions included in WN and XWN. We also tested
(WN+XWN)2 (using either WN or XWN relations
at distances 1 and 2, for instance, tree#n#1?related?
>wood#n#1).
spBNC (McCarthy, 2001): This resource contains
707,618 selectional preferences acquired for sub-
jects and objects from BNC.
spSemCor (Agirre and Martinez, 2002): This re-
source contains the selectional preferences acquired
for subjects and objects from SemCor (for instance,
read#v#1?tobj?>book#n#1).
MCR (Atserias et al, 2004): This resource
uses the direct relations included in MCR but ex-
cluding spBNC because of its poor performance.
Thus, MCR contains the direct relations from
WN (as tree#n#1?hyponym?>teak#n#2), XWN
(as teak#n#2?gloss?>wood#n#1), and spSemCor
(as read#v#1?tobj?>book#n#1) but not the indi-
83
Source #relations
Princeton WN1.6 138,091
Selectional Preferences from SemCor 203,546
New relations from Princeton WN2.0 42,212
Gold relations from eXtended WN 17,185
Silver relations from eXtended WN 239,249
Normal relations from eXtended WN 294,488
Total 934,771
Table 4: Semantic relations uploaded in the MCR
rect relations of (WN+XWN)2 (tree#n#1?related?
>wood#n#1). We also tested MCR2 (using rela-
tions at distances 1 and 2), which also integrates
(WN+XWN)2 relations.
Table 4 shows the number of semantic relations
between synset pairs in the MCR.
3.1 Topic Signatures
Topic Signatures (TS) are word vectors related to a
particular topic (Lin and Hovy, 2000). Topic Signa-
tures are built by retrieving context words of a target
topic from large corpora. In our case, we consider
word senses as topics.
For this study, we use two different large-scale
Topic Signatures. The first constitutes one of the
largest available semantic resource with around 100
million relations (between synsets and words) ac-
quired from the web (Agirre and de la Calle, 2004).
The second has been derived directly from SemCor.
TSWEB2: Inspired by the work of (Leacock et
al., 1998), these Topic Signatures were constructed
using monosemous relatives from WordNet (syn-
onyms, hypernyms, direct and indirect hyponyms,
and siblings), querying Google and retrieving up to
one thousand snippets per query (that is, a word
sense), extracting the words with distinctive fre-
quency using TFIDF. For these experiments, we
used at maximum the first 700 words of each TS.
TSSEM: These Topic Signatures have been con-
structed using the part of SemCor having all words
tagged by PoS, lemmatized and sense tagged ac-
cording to WN1.6 totalizing 192,639 words. For
each word-sense appearing in SemCor, we gather
all sentences for that word sense, building a TS us-
ing TFIDF for all word-senses co-occurring in those
sentences.
2http://ixa.si.ehu.es/Ixa/resources/
sensecorpus
political party#n#1 2.3219
party#n#1 2.3219
election#n#1 1.0926
nominee#n#1 0.4780
candidate#n#1 0.4780
campaigner#n#1 0.4780
regime#n#1 0.3414
identification#n#1 0.3414
government#n#1 0.3414
designation#n#3 0.3414
authorities#n#1 0.3414
Table 5: Topic Signatures for party#n#1 obtained
from Semcor (11 out of 719 total word senses)
.
In table 5, there is an example of the first word-
senses we calculate from party#n#1.
The total number of relations between WN
synsets acquired from SemCor is 932,008.
4 Evaluating each resource
Table 6 presents ordered by F1 measure, the perfor-
mance of each knowledge resource on Senseval-3
and the average size of the TS per word-sense. The
average size of the TS per word-sense is the number
of words associated to a synset on average. Obvi-
ously, the best resources would be those obtaining
better performances with a smaller number of asso-
ciated words per synset. The best results for preci-
sion, recall and F1 measures are shown in bold. We
also mark in italics those resources using non-direct
relations.
Surprisingly, the best results are obtained by
TSSEM (with F1 of 52.4). The lowest result is ob-
tained by the knowledge directly gathered from WN
mainly because of its poor coverage (R of 18.4 and
F1 of 26.1). Also interesting, is that the knowledge
integrated in the MCR although partly derived by
automatic means performs much better in terms of
precision, recall and F1 measures than using them
separately (F1 with 18.4 points higher than WN, 9.1
than XWN and 3.7 than spSemCor).
Despite its small size, the resources derived from
SemCor obtain better results than its counterparts
using much larger corpora (TSSEM vs. TSWEB and
spSemCor vs. spBNC).
Regarding the basic baselines, all knowledge re-
sources surpass RANDOM, but none achieves nei-
ther WN-MFS, TRAIN-MFS nor TRAIN. Only
84
KB P R F1 Av. Size
TSSEM 52.5 52.4 52.4 103
MCR2 45.1 45.1 45.1 26,429
MCR 45.3 43.7 44.5 129
spSemCor 43.1 38.7 40.8 56
(WN+XWN)2 38.5 38.0 38.3 5,730
WN+XWN 40.0 34.2 36.8 74
TSWEB 36.1 35.9 36.0 1,721
XWN 38.8 32.5 35.4 69
WN3 35.0 34.7 34.8 503
WN4 33.2 33.1 33.2 2,346
WN2 33.1 27.5 30.0 105
spBNC 36.3 25.4 29.9 128
WN 44.9 18.4 26.1 14
Table 6: P, R and F1 fine-grained results for the
resources evaluated individually at Senseval-03 En-
glish Lexical Sample Task.
TSSEM obtains better results than SEMCOR-MFS
and is very close to the most frequent sense of WN
(WN-MFS) and the training (TRAIN-MFS).
Table 7 presents ordered by F1 measure, the per-
formance of each knowledge resource on SemEval-
2007 and its average size of the TS per word-sense3.
The best results for precision, recall and F1 mea-
sures are shown in bold. We also mark in italics
those resources using non-direct relations.
Interestingly, on SemEval-2007, all the knowl-
edge resources behave differently. Now, the best
results are obtained by (WN+XWN)2 (with F1 of
52.9), followed by TSWEB (with F1 of 51.0). The
lowest result is obtained by the knowledge encoded
in spBNC mainly because of its poor precision (P of
24.4 and F1 of 20.8).
Regarding the basic baselines, spBNC, WN (and
also WN2 and WN4) and spSemCor do not sur-
pass RANDOM, and none achieves neither WN-
MFS, TRAIN-MFS nor TRAIN. Now, WN+XWN,
XWN, TSWEB and (WN+XWN)2 obtain better re-
sults than SEMCOR-MFS but far below the most
frequent sense of WN (WN-MFS) and the training
(TRAIN-MFS).
5 Combination of Knowledge Resources
In order to evaluate deeply the contribution of each
knowledge resource, we also provide some results
of the combined outcomes of several resources. The
3The average size is different with respect Senseval-3 be-
cause the words selected for this task are different
KB P R F1 Av. Size
(WN+XWN)2 54.9 51.1 52.9 5,153
TSWEB 54.8 47.8 51.0 700
XWN 50.1 39.8 44.4 96
WN+XWN 45.4 36.8 40.7 101
MCR 40.2 35.5 37.7 149
TSSEM 35.1 32.7 33.9 428
MCR2 32.4 29.5 30.9 24,896
WN3 29.3 26.3 27.7 584
WN2 25.9 27.4 26.6 72
spSemCor 31.4 23.0 26.5 51.0
WN4 26.1 23.9 24.9 2,710
WN 36.8 16.1 22.4 13
spBNC 24.4 18.1 20.8 290
Table 7: P, R and F1 fine-grained results for the
resources evaluated individually at SemEval-2007,
English Lexical Sample Task .
KB Rank
MCR+(WN+XWN)2+TSWEB+TSSEM 55.5
Table 8: F1 fine-grained results for the 4 system-
combinations on Senseval-3
combinations are performed following a very basic
strategy (Brody et al, 2006).
Rank-Based Combination (Rank): Each se-
mantic resource provides a ranking of senses of the
word to be disambiguated. For each sense, its place-
ments according to each of the methods are summed
and the sense with the lowest total placement (clos-
est to first place) is selected.
Table 8 presents the F1 measure result with re-
spect this method when combining four different se-
mantic resources on the Senseval-3 test set.
Regarding the basic baselines, this combination
outperforms the most frequent sense of SemCor
(SEMCOR-MFS with F1 of 49.1), WN (WN-MFS
with F1 of 53.0) and, the training data (TRAIN-MFS
with F1 of 54.5).
Table 9 presents the F1 measure result with re-
spect the rank mthod when combining the same four
different semantic resources on the SemEval-2007
test set.
KB Rank
MCR+(WN+XWN)2+TSWEB+TSSEM 38.9
Table 9: F1 fine-grained results for the 4 system-
combinations on SemEval-2007
85
In this case, the combination of the four resources
obtains much lower result. Regarding the baselines,
this combination performs lower than the most fre-
quent senses from SEMCOR, WN or the training
data. This could be due to the poor individual per-
formance of the knowledge derived from SemCor
(spSemCor, TSSEM and MCR, which integrates
spSemCor). Possibly, in this case, the knowledge
comming from SemCor is counterproductive. Inter-
estingly, the knowledge derived from other sources
(XWN from WN glosses and TSWEB from the
web) seems to be more robust with respect corpus
changes.
6 Conclusions
Although this task had no participants, we provide
the performances of a large set of knowledge re-
sources on two different test sets: Senseval-3 and
SemEval-2007 English Lexical Sample task. We
also provide the results of a system combination of
four large-scale semantic resources. When evalu-
ated on Senseval-3, the combination of knowledge
sources surpass the most-frequent classifiers. How-
ever, a completely different behaviour is observed
on SemEval-2007 data test. In fact, both corpora
present very different characteristics. The results
show that some resources seems to be less depen-
dant than others to corpus changes.
Obviously, these results suggest that much more
research on acquiring, evaluating and using large-
scale semantic resources should be addressed.
7 Acknowledgements
We want to thank the valuable comments of the
anonymous reviewers. This work has been partially
supported by the projects KNOW (TIN2006-15049-
C03-01) and ADIMEN (EHU06/113).
References
E. Agirre and O. Lopez de la Calle. 2004. Publicly avail-
able topic signatures for all wordnet nominal senses.
In Proceedings of LREC, Lisbon, Portugal.
E. Agirre and D. Martinez. 2001. Learning class-to-class
selectional preferences. In Proceedings of CoNLL,
Toulouse, France.
E. Agirre and D. Martinez. 2002. Integrating selectional
preferences in wordnet. In Proceedings of GWC,
Mysore, India.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Car-
roll, B. Magnini, and Piek Vossen. 2004. The mean-
ing multilingual central repository. In Proceedings of
GWC, Brno, Czech Republic.
S. Brody, R. Navigli, and M. Lapata. 2006. Ensem-
ble methods for unsupervised wsd. In Proceedings of
COLING-ACL, pages 97?104.
M. Cuadros, L. Padro?, and G. Rigau. 2005. Comparing
methods for automatic acquisition of topic signatures.
In Proceedings of RANLP, Borovets, Bulgaria.
M. Cuadros, L. Padro?, and G. Rigau. 2006. An empirical
study for automatic acquisition of topic signatures. In
Proceedings of GWC, pages 51?59.
J. Daude?, L. Padro?, and G. Rigau. 2003. Validation and
Tuning of Wordnet Mapping Techniques. In Proceed-
ings of RANLP, Borovets, Bulgaria.
C. Fellbaum, editor. 1998. WordNet. An Electronic Lexi-
cal Database. The MIT Press.
S. Landes, C. Leacock, and R. Tengi. 2006. Build-
ing a semantic concordance of english. In WordNet:
An electronic lexical database and some applications.
MIT Press, Cambridge,MA., 1998, pages 97?104.
C. Leacock, M. Chodorow, and G. Miller. 1998. Us-
ing Corpus Statistics and WordNet Relations for Sense
Identification. Computational Linguistics, 24(1):147?
166.
C. Lin and E. Hovy. 2000. The automated acquisition of
topic signatures for text summarization. In Proceed-
ings of COLING. Strasbourg, France.
D. McCarthy. 2001. Lexical Acquisition at the Syntax-
Semantics Interface: Diathesis Aternations, Subcate-
gorization Frames and Selectional Preferences. Ph.D.
thesis, University of Sussex.
R. Mihalcea and D. Moldovan. 2001. extended wordnet:
Progress report. In Proceedings of NAACL Workshop
on WordNet and Other Lexical Resources, Pittsburgh,
PA.
R. Mihalcea. 2006. Knowledge based methods for word
sense disambiguation. In E. Agirre and P. Edmonds
(Eds.) Word Sense Disambiguation: Algorithms and
applications., volume 33 of Text, Speech and Lan-
guage Technology. Springer.
P. Vossen, editor. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks . Kluwer
Academic Publishers .
86
