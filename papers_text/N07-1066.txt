Proceedings of NAACL HLT 2007, pages 524?531,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Probabilistic Framework for Answer Selection in Question Answering
Jeongwoo Ko1, Luo Si2, Eric Nyberg1
1Language Technologies Institute, Carnegie Mellon, Pittsburgh, PA 15213
2Department of Computer Science, Purdue University, West Lafayette, IN 47907
jko@cs.cmu.edu, lsi@cs.purdue.edu, ehn@cs.cmu.edu
Abstract
This paper describes a probabilistic an-
swer selection framework for question an-
swering. In contrast with previous work
using individual resources such as ontolo-
gies and the Web to validate answer can-
didates, our work focuses on developing
a unified framework that not only uses
multiple resources for validating answer
candidates, but also considers evidence of
similarity among answer candidates in or-
der to boost the ranking of the correct an-
swer. This framework has been used to se-
lect answers from candidates generated by
four different answer extraction methods.
An extensive set of empirical results based
on TREC factoid questions demonstrates
the effectiveness of the unified framework.
1 Introduction
Question answering aims at finding exact answers
to a user?s natural language question from a large
collection of documents. Most QA systems com-
bine information retrieval with extraction techniques
to identify a set of likely candidates and then uti-
lize some selection strategy to generate the final
answers (Prager et al, 2000; Clarke et al, 2001;
Harabagiu et al, 2001). Since answer extractors
may be based on imprecise empirical methods, the
selection process can be very challenging, as it often
entails identifying correct answer(s) amongst many
incorrect ones.
Questio
n Ques
tion Analys
isQu
ery D
ocumen
t
Retriev
al Corpus
Docs
Answe
r
Extract
ionAn
swer candida
tes An
swer Selecti
on Answe
r
Shang
hai
FT94
2-20
16
0.5
Taiw
an
FBIS
3-4532
0
0.4
Shang
hai
FBIS
3-58
0.64
Shang
hai
WSJ9
2011
0-00
13
0.65
Hong
 Kon
g
AP88
0603
-026
8
0.7
Beijin
g
Docu
ment
 
extra
cted
Score
Answ
er 
cand
idate
s
Whic
h city
 in C
hina 
has t
he 
large
st nu
mber
 of fo
reign
 
finan
cial c
omp
anies
?
Figure 1: A traditional QA pipeline architecture
Figure 1 shows a traditional QA architecture with
an example question. Given the question ?Which
city in China has the largest number of foreign fi-
nancial companies??, the answer extraction com-
ponent produces a ranked list of five answer can-
didates. Due to imprecision in answer extraction,
an incorrect answer (?Beijing?) was ranked at the
top position. The correct answer (?Shanghai?) was
extracted from two documents with different confi-
dence scores and ranked at the third and the fifth po-
sitions. In order to select ?Shanghai? as the final
answer, we need to address two issues:
? Answer Validation. How do we identify correct
answer(s) amongst incorrect ones? Validating
an answer may involve searching for facts in
a knowledge base, e.g. IS-A(Shanghai,
city), IS-IN(Shanghai, China).
? Answer Similarity. How do we exploit evi-
dence of similarity among answer candidates?
524
For example, when there are redundant an-
swers (?Shanghai?, as above) or several an-
swers which represent a single instance (e.g.
?Clinton, Bill? and ?William Jefferson Clin-
ton?) in the candidate list, how much should we
boost the answer candidate scores?
To address the first issue, several answer selec-
tion approaches have used semantic resources. One
of the most common approaches relies on Word-
Net, CYC and gazetteers for answer validation or
answer reranking; answer candidates are pruned
or discounted if they are not found within a re-
source?s hierarchy corresponding to the expected an-
swer type (Xu et al, 2003; Moldovan et al, 2003;
Prager et al, 2004). In addition, the Web has been
used for answer reranking by exploiting search en-
gine results produced by queries containing the an-
swer candidate and question keywords (Magnini et
al., 2002), and Wikipedia?s structured information
has been used for answer type checking (Buscaldi
and Rosso, 2006).
To use more than one resource for answer
type checking of location questions, Schlobach
et al (2004) combined WordNet with geographi-
cal databases. However, in their experiments the
combination actually hurt performance because of
the increased semantic ambiguity that accompanies
broader coverage of location names. This demon-
strates that the method used to combine potential
answers may matter as much as the choice of re-
sources.
To address the second issue we must determine
how to detect and exploit answer similarity. As an-
swer candidates are extracted from different docu-
ments, they may contain identical, similar or com-
plementary text snippets. For example, the United
States may be represented by the strings ?U.S.?,
?United States? or ?USA? in different documents. It
is important to detect this type of similarity and ex-
ploit it to boost answer confidence, especially for list
questions that require a set of unique answers. One
approach is to incorporate answer clustering (Kwok
et al, 2001; Nyberg et al, 2003; Jijkoun et al,
2006). For example, we might merge ?April 1912?
and ?14 Apr 1912? into a cluster and then choose
one answer as the cluster head. However, clustering
raises new issues: how to choose the cluster head
and how to calculate the scores of the clustered an-
swers.
Although many QA systems individually address
these issues in answer selection, there has been lit-
tle research on generating a generalized probabilistic
framework that allows any validation and similarity
features to be easily incorporated.
In this paper we describe a probabilistic answer
selection framework to address the two issues. The
framework uses logistic regression to estimate the
probability that an answer candidate is correct given
multiple answer validation features and answer sim-
ilarity features. Experimental results on TREC
factoid questions (Voorhees, 2004) show that our
framework significantly improved answer selection
performance for four different extraction techniques,
when compared to default selection using the indi-
vidual candidate scores produced by each extractor.
This paper is organized as follows: Section 2 de-
scribes our answer selection framework and Section
3 lists the features that generate similarity and va-
lidity scores for factoid questions. In Section 4, we
describe the experimental methodology and the re-
sults. Section 5 describes how we intend to extend
our framework to handle complex questions. Finally
Section 6 concludes with suggestions for future re-
search.
2 Method
Answer validation is based on an estimate of the
probability P (correct(Ai)|Ai, Q), where Q is a
question and Ai is an answer candidate to the ques-
tion. Answer similarity is is based on an estimate
of the probability P (correct(Ai)|Ai, Aj), where Aj
is similar to Ai. Since both probabilities influ-
ence answer selection performance, it is important
to combine them in a unified framework and es-
timate the probability of an answer candidate as:
P (correct(Ai)|Q,A1, ..., An).
In this paper, we propose a proba-
bilistic framework that directly estimates
P (correct(Ai)|Q,A1, ..., An) using multiple
answer validation features and answer similarity
features. The framework was implemented with
logistic regression, which is a statistical machine
learning technique used to predict the probability
of a binary variable from input variables. Logistic
525
P (correct(Ai)|Q,A1, ..., An) (1)
? P (correct(Ai)|val1(Ai), ..., valK1(Ai), sim1(Ai), ..., simK2(Ai))
=
exp(?0 +
K1?
k=1
?kvalk(Ai) +
K2?
k=1
?ksimk(Ai))
1 + exp(?0 +
K1?
k=1
?kvalk(Ai) +
K2?
k=1
?ksimk(Ai))
where, simk(Ai) =
N?
j=1(j 6=i)
sim?k(Ai, Aj).
~?, ~?,~? = argmax
~?,~?,~?
R?
j=1
Nj?
i=1
logP (correct(Ai)|val1(Ai), ..., valK1(Ai), sim1(Ai), ..., simK2(Ai)) (2)
regression has been successfully employed in many
applications including multilingual document merg-
ing (Si and Callan, 2005). In our previous work (Ko
et al, 2006), we showed that logistic regression
performed well in merging three resources to vali-
date answers to location and proper name questions.
We extended this approach to combine multiple
similarity features with multiple answer validation
features. The extended framework estimates the
probability that an answer candidate is correct given
the degree of answer correctness and the amount
of supporting evidence provided in a set of answer
candidates (Equation 1).
In Equation 1, each valk(Ai) is a feature function
used to produce an answer validity score for an an-
swer candidate Ai. Each sim?k(Ai, Aj) is a similar-
ity function used to calculate an answer similarity
between Ai and Aj . K1 and K2 are the number of
answer validation and answer similarity features, re-
spectively. N is the number of answer candidates.
To incorporate multiple similarity features, each
simk(Ai) is obtained from an individual similarity
metric. For example, if Levenshtein distance is used
as one similarity metric, simk(Ai) is calculated by
summing N-1 Levenshtein distances between one
answer candidate and all other candidates. As some
string similarity metrics (e.g. Levenshtein distance)
produce a number between 0 and 1 (where 1 means
two strings are identical and 0 means they are differ-
ent), similarity scores less than some threshold value
are ignored.
The parameters ?, ?, ? were estimated from train-
ing data by maximizing the log likelihood as shown
in Equation 2, where R is the number of training
questions and Nj is the number of answer candidates
for each question Qj . For parameter estimation, we
used the Quasi-Newton algorithm (Minka, 2003).
To select correct answers, the initial answer candi-
date set is reranked according to the estimated prob-
ability of each candidate. For factoid questions, the
top answer is selected as the final answer to the ques-
tion. As logistic regression can be used for binary
classification with a default threshold of 0.5, we can
also use the framework to classify incorrect answers:
if the probability of an answer candidate is lower
than 0.5, it is considered to be a wrong answer and
is filtered out of the answer list. This is useful in
deciding whether or not a valid answer exists in the
corpus, an important aspect of the TREC QA evalu-
ation (Voorhees, 2004).
3 Feature Representation
This section details the features used to generate an-
swer validity scores and answer similarity scores for
our answer selection framework.
526
3.1 Answer Validation Features
Each answer validation feature produces a validity
score which predicts whether or not an answer can-
didate is a correct answer for the question. This task
can be done by exploiting external QA resources
such as the Web, databases, and ontologies. For fac-
toid questions, we used gazetteers and WordNet in a
knowledge-based approach; we also used Wikipedia
and Google in a data-driven approach.
3.1.1 Knowledge-based Features
In order to generate answer validity scores using
gazetteers and WordNet, we reused the algorithms
described in our previous work (Ko et al, 2006).
Gazetteers: Gazetteers provide geographic
information, which allows us to identify
strings as instances of countries, their cities,
continents, capitals, etc. For answer selec-
tion, we used three gazetteer resources: the
Tipster Gazetteer, the CIA World Factbook
(https://www.cia.gov/cia/publications/factbook/inde
x.html) and information about the US states pro-
vided by 50states.com (http://www.50states.com).
These resources were used to assign an answer
validity score between -1 and 1 to each candidate
(Figure 2). A score of 0 means the gazetteers did
not contribute to the answer selection process for
that candidate. For some numeric questions, range
checking was added to validate numeric questions
similarly to Prager et al (2004). For example, given
the question ?How many people live in Chile??,
if an answer candidate is within ? 10% of the
population stated in the CIA World Factbook, it
receives a score of 1.0. If it is in the range of 20%,
its score is 0.5. If it significantly differs by more
than 20%, it receives a score of -1.0. The threshold
may vary based on when the document was written
and when the census was taken1.
WordNet: The WordNet lexical database includes
English words organized in synonym sets, called
synsets (Fellbaum, 1998). We used WordNet in or-
der to produce an answer validity score between -1
and 1, following the algorithm in Figure 3. A score
1The ranges used here were found to work effectively, but
were not explicitly validated or tuned.
 
 
  
1)  If the answer candidate directly matches the gazetteer 
answer for the question, its gazetteer score is 1.0. (e.g. 
Given the question ?What continent is Togo on??, the 
candidate ?Africa? receives a score of 1.0.) 
2)  If the answer candidate occurs in the gazetteer within 
the subcategory of the expected answer type, its score 
is 0.5. (e.g., Given the question ?Which city in China 
has the largest number of foreign financial 
companies??, the candidates ?Shanghai? and ?Boston? 
receive a score of 0.5 because they are both cities.) 
3)  If the answer candidate is not the correct semantic 
type, its score is -1. (e.g., Given the question ?Which 
city in China has the largest number of foreign 
financial companies??, the candidate ?Taiwan? 
receives a score of -1 because it is not a city.) 
4) Otherwise, the score is 0.0. 
Figure 2: Validity scoring with gazetteers.
 
 
 
 
 
 
 
 
 
 
 
1)  If the answer candidate directly matches WordNet, its 
WordNet score is 1.0. (e.g. Given the question ?What is 
the capital of Uruguay??, the candidate ?Montevideo? 
receives a score of 1.0.) 
2)  If the answer candidate?s hypernyms include a 
subcategory of the expected answer type, its score is 
0.5. (e.g., Given the question ?Who wrote the book 
?Song of Solomon??", the candidate ?Mark Twain? 
receives a score of 0.5 because its hypernyms include 
?writer?.) 
3)  If the answer candidate is not the correct semantic 
type, this candidate receives a score of -1. (e.g., Given 
the question ?What state is Niagara Falls located in??, 
the candidate ?Toronto? gets a score of -1 because it is 
not a state.) 
4) Otherwise, the score is 0.0. 
Figure 3: Validity scoring with WordNet.
of 0 means that WordNet does not contribute to the
answer selection process for a candidate.
3.1.2 Data-driven Features
Wikipedia and Google were used in a data-driven
approach to generate answer validity scores.
Wikipedia: Wikipedia (http://www.wikipedia.org)
is a multilingual free on-line encyclopedia. Fig-
ure 4 shows the algorithm used to generate an
answer validity score from Wikipedia. If there
is a Wikipedia document whose title matches an
answer candidate, the document is analyzed to
obtain the term frequency (tf) and the inverse term
527
For
 eac
h an
swe
r ca
ndi
date
 A i,
1. I
niti
aliz
e th
e W
ikip
edia
 sco
re: 
ws(
A i) 
= 0
2. S
earc
h fo
r a 
Wik
iped
ia d
ocu
men
t wh
ose
 titl
e is
 A i
3. I
f a 
doc
um
ent 
is fo
und
, ca
lcul
ate 
tf.id
fsc
ore
 of 
A ii
n th
e 
retr
ieve
d W
ikip
edia
 doc
um
ent
ws(
A i) 
= (1
+lo
g(tf
)) ?
 (1+
log
(idf
))
4. I
f no
t, fo
r ea
ch q
ues
tion
 key
wor
d K
j,
4.1.
 Sea
rch
 for
 a W
ikip
edia
 doc
um
ent 
that
 inc
lud
es K
j
4.2.
 If a
 doc
um
ent 
is fo
und
, ca
lcul
ate 
tf.id
fsc
ore
 of 
A i 
ws(
A i) 
+= 
(1+
log
(tf)
) ? 
(1+
log
(idf
))
Figure 4: Validity scoring with Wikipedia
1 )
1( 2)(
)(
?
+
?
=
d
scs
scs
For
 eac
h an
swe
r ca
ndi
date
 A i,
1. I
niti
aliz
e th
e G
oog
le s
cor
e: g
s(A
i) =
 0
2. F
or e
ach
 sni
ppe
t s:
2.1.
 Ini
tial
ize 
the 
snip
pet 
co-
occ
urre
nce
 sco
re: 
cs(s
) = 
1
2.2.
 Fo
r ea
ch q
ues
tion
 key
wor
d k
in s
: 
2.2.
1 C
om
put
e di
stan
ce d
, th
e m
inim
um
 num
ber
 of 
wor
ds b
etw
een
 ka
nd t
he a
nsw
er c
and
idat
e 
2.2.
2 U
pda
te th
e sn
ipp
et c
o-o
ccu
rren
ce s
cor
e:
2.3.
 gs(
A i) 
= g
s(A
i) +
 cs(
s)
3. N
orm
aliz
e th
e G
oog
le s
cor
e (d
ivid
ing
 it b
y a 
con
stan
t C
)
Figure 5: Validity scoring with Google
frequency (idf) of the candidate, from which a
tf.idf score is calculated. When there is no matched
document, each question keyword is also processed
as a back-off strategy, and the answer validity score
is calculated by summing the tf.idf scores. To
calculate word frequency, the TREC Web Corpus
(http://ir.dcs.gla.ac.uk/test collections/wt10g.html)
was used as a large background corpus.
Google: Following Magnini et al (2002), we used
Google to generate a numeric score. A query con-
sisting of an answer candidate and question key-
words was sent to the Google search engine. To
calculate a score, the top 10 text snippets returned
by Google were then analyzed using the algorithm
in Figure 5.
3.2 Answer Similarity Features
We calculate the similarity between two answer can-
didates using multiple string distance metrics and a
list of synonyms.
3.2.1 String Distance Metrics
There are several different string distance metrics
to calculate the similarity of short strings. We used
five popular string distance metrics: Levenshtein,
Jaccard, Jaro, Jaro-Winkler, and Cosine similarity.
3.2.2 Synonyms
Synonyms can be used as another metric to calcu-
late answer similarity. We defined a binary similar-
ity score for synonyms.
sim(Ai, Aj) =
{
1, if Ai is a synonym of Aj
0, otherwise
To get a list of synonyms, we used three knowl-
edge bases: WordNet, Wikipedia and the CIA World
Factbook. WordNet includes synonyms for English
words. Wikipedia redirection is used to obtain an-
other set of synonyms. For example, ?Calif.? is redi-
rected to ?California? in Wikipedia, and ?William
Jefferson Clinton? is redirected to ?Bill Clinton?.
The CIA World Factbook includes five different
names for a country: conventional long form, con-
ventional short form, local long form, local short
form and former name. For example, the conven-
tional long form of Egypt is ?Arab Republic of
Egypt?, the conventional short form is ?Egypt?, the
local short form is ?Misr?, the local long form is
?Jumhuriyat Misr al-Arabiyah? and the former name
is ?United Arab Republic (with Syria)?. All are con-
sidered to be synonyms of ?Egypt?.
In addition, manually generated rules are used to
obtain synonyms for different types of answer can-
didates (Nyberg et al, 2003):
? Dates are converted into the ISO 8601 date for-
mat (YYYY-MM-DD) (e.g., ?April 12 1914?
and ?12th Apr. 1914? are converted into ?1914-
04-12? and considered as synonyms).
? Temporal expressions are converted into the
HH:MM:SS format (e.g., ?six thirty five p.m.?
and ?6:35 pm? are converted into ?18:35:xx?
and considered as synonyms).
? Numeric expression are converted into sci-
entific notation (e.g, ?one million? and
?1,000,000? are converted into ?1e+06? and
considered as synonyms).
528
? Representative entities are converted into the
represented entity when the expected answer
type is COUNTRY (e.g., ?the Egyptian govern-
ment? is changed to ?Egypt? and ?Clinton ad-
ministration? is changed to ?U.S.?).
4 Experiment
This section describes the experiments we used
to evaluate our answer selection framework. The
JAVELIN QA system (Nyberg et al, 2006) was used
as a testbed for the evaluation.
4.1 Experimental Setup
A total of 1760 factoid questions from the TREC8-
12 QA evaluations served as a dataset, with 5-fold
cross validation.
To better understand how the performance of our
framework varies for different extraction techniques,
we tested it with four JAVELIN answer extraction
modules: FST, LIGHTv1, LIGHTv2 and SVM (Ny-
berg et al, 2006). FST is an answer extractor based
on finite state transducers that incorporate a set of
extraction patterns (both manually-created and gen-
eralized patterns). LIGHTv1 is an extractor that se-
lects answer candidates using a non-linear distance
heuristic between the keywords and an answer can-
didate. LIGHTv2 is another extractor based on a
different distance heuristic, originally developed as
part of a multilingual QA system. SVM is an extrac-
tor that uses Support Vector Machines to discrimi-
nate between correct and incorrect answers.
Answer selection performance was measured by
average accuracy: the number of correct top answers
divided by the number of questions where at least
one correct answer exists in the candidate list pro-
vided by an extractor. The baseline was calculated
with the answer candidate scores provided by each
individual extractor; the answer with the best extrac-
tor score was chosen, and no validation or similarity
processing was performed. For Wikipedia, we used
a version downloaded in Nov. 2005, which con-
tained 1,811,554 articles.
4.2 Results and Analysis
We first analyzed the average accuracy when us-
ing individual validation features. Figure 6 shows
the effect of the individual answer validation fea-
tures on different extraction outputs. The combina-
0.00.10.20.30.40.50.60.70.80.91.0
AL
L
GL
W
IKI
W
N
GZ
Ba
se
lin
e
Average Accuracy
 FS
T
 Li
gh
tV1
 Li
gh
tV2
 SV
M
Figure 6: Average accuracy of individual answer
validation features (GZ: gazetteers, WN: WordNet,
WIKI: Wikipedia, GL: Google, ALL: combination
of all features).
tion of all features significantly improved the per-
formance when compared to answer selection using
a single feature. Comparing the data-driven features
with the knowledge-based features, the data-driven
features (such as Wikipedia and Google) increased
performance more than the knowledge-based fea-
tures (such as gazetteers and WordNet); our intuition
is that the knowledge-based features covered fewer
questions. The biggest improvement was found with
candidates produced by the SVM extractor: a 242%
improvement over the baseline. It was mostly be-
cause SVM tended to produce several answer can-
didates with the same or very similar confidence
scores, but our framework could select the correct
answer among many incorrect ones by exploiting
answer validation features.
Table 1 shows the effect of individual similarity
features on different extractors when using 0.3 and
0.5 as a similarity threshold, respectively. When
comparing five different string similarity features
(Levenshtein, Jaro, Jaro-Winkler, Jaccard and Co-
sine similarity), Levenshtein and Jaccard tended to
perform better than the others. When comparing
synonym features with string similarity features,
synonyms performed slightly better.
We also analyzed answer selection performance
when combining all six similarity features (?All? in
Table 1). Combining all similarity features did not
improve the performance except for the FST extrac-
tor, because including five string similarity features
529
Similarity FST LIGHTv1 LIGHTv2 SVM
feature 0.3 0.5 0.3 0.5 0.3 0.5 0.3 0.5
Levenshtein 0.728 0.728 0.471 0.455 0.399 0.400 0.381 0.383
Jaro 0.708 0.705 0.422 0.440 0.373 0.378 0.274 0.282
Jaro-Winkler 0.701 0.705 0.426 0.442 0.374 0.379 0.277 0.275
Jaccard 0.738 0.738 0.438 0.448 0.452 0.448 0.382 0.390
Cosine 0.738 0.738 0.436 0.435 0.418 0.422 0.380 0.378
Synonyms 0.745 0.745 0.458 0.458 0.442 0.442 0.412 0.412
Lev+Syn 0.748 0.751 0.460 0.466 0.445 0.448 0.420 0.412
Jac+Syn 0.742 0.742 0.456 0.465 0.440 0.445 0.396 0.396
All 0.755 0.755 0.405 0.425 0.435 0.431 0.303 0.302
Table 1: Average accuracy using individual similarity features under different thresholds: 0.3 and 0.5
(?Lev+Syn?: the combination of Levenshtein with synonyms, ?Jac+Syn?: the combination of Jaccard and
synonyms, ?All?: the combination of all similarity metrics)
Baseline Sim Val All
FST 0.658 0.751 0.855 0.877
LIGHTv1 0.394 0.466 0.612 0.628
LIGHTv2 0.343 0.448 0.578 0.582
SVM 0.169 0.420 0.578 0.586
Table 2: Average accuracy of individual features
(Sim: merging similarity features, Val: merging val-
idation features, ALL: combination of all features).
provided too much redundancy to the logistic regres-
sion. We also compared the combination of Leven-
shtein with synonyms and the combination of Jac-
card with synonyms, and then chose Levenshtein
and synonyms as the two best similarity features in
our framework.
We also analyzed the degree to which the average
accuracy was affected by answer similarity and val-
idation features. Table 2 compares the average ac-
curacy using the baseline, the answer similarity fea-
tures, the answer validation features and all feature
combinations. As can be seen, the similarity fea-
tures significantly improved performance, so we can
conclude that exploiting answer similarity improves
answer selection performance. The validation fea-
tures also significantly improved the performance.
When combining both sets of features together,
the answer selection performance increased for all
four extractors: an average of 102% over the base-
line, 30% over the similarity features and 1.82%
over the validation features. Adding the similarity
features to the validation features generated small
but consistent improvement in all configurations.
We expect more performance gain from similar-
ity features when merging similar answers returned
from all four extractors.
5 Extensions for Complex Questions
Although we conducted our experiments on fac-
toid questions, our framework can be easily ex-
tended to handle complex questions, which require
longer answers representing facts or relations (e.g.,
?What is the relationship between Alan Greenspan
and Robert Rubin??). As answer candidates are
long text snippets, different features should be used
for answer selection. Possible validation features
include question keyword inclusion and predicate
structure match (Nyberg et al, 2005). For exam-
ple, given the question ?Did Egypt sell Scud mis-
siles to Syria??, the key predicate from the ques-
tion is Sell(Egypt, Syria, Scud missile). If there is
a sentence which contains the predicate structure
Buy(Syria, Scud missile, Egypt), we can calculate
the predicate structure distance and use it as a val-
idation feature. For answer similarity, we intend to
explore novelty detection approaches evaluated in
Allan et al (2003).
6 Conclusion
In this paper, we described our answer selection
framework for estimating the probability that an an-
swer candidate is correct given multiple answer vali-
530
dation and similarity features. We conducted a series
of experiments to evaluate the performance of the
framework and analyzed the effect of individual val-
idation and similarity features. Empirical results on
TREC questions show that our framework improved
answer selection performance in the JAVELIN QA
system by an average of 102% over the baseline,
30% over the similarity features alone and 1.82%
over the validation features alone.
We plan to improve our framework by adding reg-
ularization and selecting the final answers among
candidates returned from all extractors. As our
current framework is based on the assumption that
each answer is independent, we are building another
probabilistic framework which does not require any
independence assumption, and uses an undirected
graphical model to estimate the joint probability of
all answer candidates.
7 Acknowledgments
This work was supported in part by ARDA/DTO
Advanced Question Answering for Intelli-
gence (AQUAINT) program award number
NBCHC040164.
References
J. Allan, C. Wade, and A. Bolivar. 2003. Retrieval and
novelty detection at the sentence level. In Proceedings
of SIGIR.
D. Buscaldi and P. Rosso. 2006. Mining Knowledge
from Wikipedia for the Question Answering task. In
Proceedings of the International Conference on Lan-
guage Resources and Evaluation.
C. Clarke, G. Cormack, and T. Lynam. 2001. Exploiting
redundancy in question answering. In Proceedings of
SIGIR.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,
M. Surdeanu, R. Bunescu, R. Grju, V. Rus, and
P. Morarescu. 2001. FALCON: Boosting knowledge
for answer engines. In Proceedings of TREC.
V. Jijkoun, J. van Rantwijk, D. Ahn, E. Tjong Kim Sang,
and M. de Rijke. 2006. The University of Amsterdam
at CLEF@QA 2006. In Working Notes CLEF.
J. Ko, L. Hiyakumoto, and E. Nyberg. 2006. Exploit-
ing semantic resources for answer selection. In Pro-
ceedings of the International Conference on Language
Resources and Evaluation.
C. Kwok, O. Etzioni, and D. S. Weld. 2001. Scal-
ing question answering to the web. In Proceedings of
WWW10 Conference.
B. Magnini, M. Negri, R. Pervete, and H. Tanev. 2002.
Comparing statistical and content-based techniques for
answer validation on the web. In Proceedings of the
VIII Convegno AI*IA.
T. Minka. 2003. A Comparison of Numerical Optimizers
for Logistic Regression. Unpublished draft.
D. Moldovan, D. Clark, S. Harabagiu, and S. Maiorano.
2003. Cogex: A logic prover for question answering.
In Proceedings of HLT-NAACL.
E. Nyberg, T. Mitamura, J. Carbonell, J. Callan,
K. Collins-Thompson, K. Czuba, M. Duggan,
L. Hiyakumoto, N. Hu, Y. Huang, J. Ko, L. Lita,
S. Murtagh, V. Pedro, and D. Svoboda. 2003. The
JAVELIN Question-Answering System at TREC 2002.
In Proceedings of the Text REtrieval Conference.
E. Nyberg, T. Mitamura, R. Frederking, M. Bilotti,
K. Hannan, L. Hiyakumoto, J. Ko, F. Lin, V. Pedro,
and A. Schlaikjer. 2006. JAVELIN I and II Systems at
TREC 2005. In Proceedings of TREC.
E. Nyberg, T. Mitamura, R. Frederking, V. Pedro,
M. Bilotti, A. Schlaikjer, and K. Hannan. 2005. Ex-
tending the javelin qa system with domain semantics.
In Proceedings of AAAI-05 Workshop on Question An-
swering in Restricted Domains.
J. Prager, E. Brown, A. Coden, and D. Radev. 2000.
Question answering by predictive annotation. In Pro-
ceedings of SIGIR.
J. Prager, J. Chu-Carroll, K. Czuba, C. Welty, A. Itty-
cheriah, and R. Mahindru. 2004 IBM?s Piquant in
Trec2003. In Proceedings of TREC.
S. Schlobach, M. Olsthoorn, and M. de Rijke. 2004.
Type checking in open-domain question answering. In
Proceedings of European Conference on Artificial In-
telligence.
L. Si and J. Callan. 2005 CLEF2005: Multilingual
retrieval by combining multiple multilingual ranked
lists. In Proceedings of Cross-Language Evaluation
Forum.
E. Voorhees. 2004. Overview of the TREC 2003 ques-
tion answering track. In Proceedings of TREC.
J. Xu, A. Licuanan, J. May, S. Miller, and R. Weischedel.
2003. TREC 2002 QA at BBN: Answer Selection and
Confidence Estimation. In Proceedings of TREC.
531
