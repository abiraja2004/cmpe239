Coling 2010: Demonstration Volume, pages 37?40,
Beijing, August 2010
PanLex and LEXTRACT: Translating all Words of all Languages of the
World
Timothy Baldwin,? Jonathan Pool? and Susan M. Colowick?
? CSSE
University of Melbourne
tb@ldwin.net
? Utilika Foundation
{pool,smc}@utilika.org
Abstract
PanLex is a lemmatic translation re-
source which combines a large number of
translation dictionaries and other translin-
gual lexical resources. It currently covers
1353 language varieties and 12M expres-
sions, but aims to cover all languages and
up to 350M expressions. This paper de-
scribes the resource and current applica-
tions of it, as well as lextract, a new
effort to expand the coverage of PanLex
via semi-automatic dictionary scraping.
1 Introduction
Translation dictionaries, multilingual thesauri,
and other translingual lexical (more precisely,
lemmatic) resources answer queries of the form
?Given lemma X in language A, what possible
translations of it into language B exist?? However,
existing resources answer only a small fraction of
the potential queries of this form. For example,
one may find attested translations of the Santiago
del Estero Quichua word unta into German, En-
glish, Spanish, Italian, French, Danish, Aymara,
and several other Quechua languages, but not into
the other (roughly 7 thousand) languages in the
world.
Answers to the vast majority of possible lem-
matic translation queries must be inferred. If unta
can be translated into Spanish as lleno, and lleno
can be translated into Hungarian as tele, e.g., per-
haps Quichua unta can be translated into Hungar-
ian as tele. But such inference is nontrivial, be-
cause lexical ambiguity degenerates the quality of
indirect translations as the paths through interme-
diate languages grow longer.
Current Goal
Resources 766 10K
Language varieties 1353 7000
Expressions 12M 350M
Expression?meaning pairs 27M 1000M
Expression?expression pairs 91M 1000M
Table 1: Current and goal PanLex coverage
Thus, it appears that the quality and range of
lemmatic translation would be supported by an
easily accessible graph combining a large (or, ide-
ally, complete) set of translations reported by the
world?s lexical resources. PanLex (http://
panlex.org) is a project developing a publicly
accessible graph of attested lemmatic translations
among all languages. As of 2010, it provides about
90 million undirected pairwise translations among
about 12 million lemmata in over 1,300 language
varieties, based on the consultation of over 750 re-
sources, as detailed in Table 1. By 2011 it is ex-
pected that the resources consulted will approxi-
mately quadruple.
2 The PanLex Project
PanLex is an attempt to generate as complete as
possible a translation graph, made up of expres-
sion nodes, meaning nodes, and undirected edges,
each of which links an expression node with a
meaning node. Each expression is uniquely de-
fined by a character string and a language. An ex-
pression ei is a translation or synonym of an ex-
pression ej iff there is at least one meaning mk
such that edges v(ei,mk) and v(ej,mk) exist. For
example, frame in English shares a meaning with
bikar in Bahasa Malay, and bikar shares a mean-
ing with beaker in English, but frame shares no
37
meaning with beaker. Whether ei and ej are syn-
onyms or translations depends on whether their
languages are identical. In Table 1, ?expres-
sion?meaning pairs? refers to edges v(e,m) and
?expression?expression pairs? refers to expres-
sions with at least one meaning in common.
2.1 Current Applications of PanLex
While lemmatic translation falls short of senten-
tial and discourse translation, it is not without
practical applications. It is particularly useful
in author?machine collaborative translation, when
authors are in a position to lemmatize expres-
sions. The prototype PanImages application
(http://www.panimages.org), based on Pan-
Dictionary, elicits a lemmatic search query
from the user and expands the query into dozens
of languages for submission to image-search ser-
vices. Hundreds of thousands of visitors have used
it to discover relevant images labeled in languages
they do not know, sometimes selecting particular
target languages for cultural specificity or to craft
less ambiguous queries than their own language
would permit (Christensen et al, 2009).
In lemmatic messaging applications developed
for user studies, users lemmatized sentences to tell
stories or send mail across language boundaries.
Evenwith context-unaware translation of lemmata
producing mostly non-optimal translations, users
were generally able to reconstruct half or more of
the originally intended sentences (Soderland et al,
2009). The PanLex database was also used in a
multilingual extension of the image-labeling game
initiated by Von Ahn and Dabbish (2004).
User and programmatic interfaces to PanLex
are under development. A lemmatic user in-
terface (http://panlex.org/u) communicates
with the user in a potentially unlimited set of
languages, with PanLex dynamically using its
own data for the localization. A primitive API
makes it possible for developers to provide, or
make infrastructural use of, lemmatic transla-
tion via PanLex. Prototype lemmatic transla-
tion services like TeraDict (http://panlex.
org/demo/treng.html), InterVorto (http://
panlex.org/demo/trepo.html), and T?mS?z
(http://panlex.org/demo/trtur.html) ex-
ploit the API.
2.2 Extraction and Normalization
The approach taken by PanLex to populate the
translation graph with nodes and edges is a combi-
nation of: (a) extraction of translation pairs from
as many translingual lexical resources as can be
found on the web and elsewhere; and (b) infer-
ence of new edges between expressions that exist
in PanLex.
To date, extraction has taken the form of hand
writing a series of regular expression-based scripts
for each individual dictionary, to generate normal-
ized PanLex database records. While this is ef-
ficient for families of resources which adhere to
a well-defined format (e.g. Freedict or Star-
dict dictionaries), it does not scale to the long
tail of one-off dictionaries constructed by lexi-
cographers using ad hoc formats, as detailed in
Section 2.2. lextract is an attempt to semi-
automate this process, as detailed in Section 3.
Inference of new translation edges is nontrivial,
because lexical ambiguity degenerates the qual-
ity of indirect translations as the paths through
intermediate languages grow longer. PanDic-
tionary is an attempt to infer a denser translation
graph fromPanLex combining translations from
many resources based on path redundancy, evi-
dence of ambiguity, and other information (Sam-
mer and Soderland, 2007; Mausam et al, 2009;
Mausam et al, 2010).
PanLex is more than a collection, or docbase,
of independent resources. Its value in translation
inference depends on its ability to combine facts
attested by multiple resources into a single graph,
in which lemmata frommultiple resources that are
substantively identical are recognized as identi-
cal. The obstacles to such integration of heteroge-
neous lexical data are substantial. They include:
(1) ad hoc formatting, including format changes
between portions of a resource; (2) erratic spacing,
punctuation, capitalization, and line wrapping; (3)
undocumented and non-standard character encod-
ings; (4) vagueness of the distinction between lem-
matic (e.g. Rana erythraea) and explanatory trans-
lations (e.g. a kind of tree frog); and (5) absence of
consensus for some languages as to the representa-
tion of lemmata, e.g. hyphenation and prefixation
in Bantu languages, and inclusion or exclusion of
tones in tonal languages.
38
#NAME "English-Hindi Dictionary"
#INDEX_LANGUAGE "English"
a
[p]det.[/p]
[m1][trn]??, ?????? ??????? ?? ??? ??? ??? ???; (??? ??) ???? ????? ???? ?? ?????[/trn][/m]
aback
[p]adv.[/p]
[m1][trn]?????, ?????; ????[/trn][/m]
...
?
2
eng-00
hin-00
ex
a
wc
detr
ex
??
...
Figure 1: A snippet of an English?Hindi dictionary, in its source form (left) and as normalizedPanLex
records (right)
3 lextract
lextract is a sub-project of PanLex, aimed
at automating the extraction and normalization
of data from arbitrary lexical resources, focusing
in the first instance on text-based resources, but
ultimately including XML, (X)HTML, PDF and
wiki markup-based resources. The approach taken
in lextract is to emulate the manual work-
flow used by the PanLex developers to scrape
data from dictionary files, namely learning of se-
ries of regular expressions to convert the source
dictionary into structured database records. In
this, we assume that the source dictionary has
been transcoded into utf-8 encoding,1 and fur-
ther that the first fivePanLex translation records
found in the source dictionary have been hand
generated as seed instances to bootstrap the ex-
traction process off, as illustrated in Figure 1.
Briefly, this provides vital data including: specifi-
cation of the source and target languages; manual
disambiguation of expression?expression vs. ex-
pression?meaning structuring; any optional fields
such as part of speech; and (implicitly) where the
records start from in the source file, and what
fields in the original dictionary should not be pre-
served in the PanLex database.
The procedure for learning regular expressions
can be broken down into 3 steps: (1) recordmatch-
ing; (2) match lattice pruning; and (3) regular ex-
pression generalization.
Record matching involves determining the set
of codepoint spans in the original dictionary where
the component strings (minimally the source and
1We have experimented with automatic character encod-
ing detection methods, but the consensus to date has been
that methods developed for web documents, such as the
chardet library, are inaccurate when applied to dictionary
files.
target language expressions, but possibly includ-
ing domain information, word class information or
other metadata) encoded in the five seed records
can be found, to use as the basis for learning
the formatting idiom employed in the dictionary.
For each record, we determine all positions in the
source dictionary file where all component strings
can be found within a fixed window width of one
another. This is returned as a match lattice, repre-
senting the possible sub-extents (?spans?) in the
source dictionary of each record, and the loca-
tion(s) of each component string within each.
Match lattice pruning takes the match lattice
from the record matching step, and prunes it based
on a combination of hard and soft constraints. The
single hard constraint currently used at present is
that the records must occur in the lexicon in se-
quence; any matches in the lattice which violate
this constraint can be pruned. Soft constraints in-
clude: each record should span the same num-
ber of lines; the fields in each record should oc-
cur in the same linear order; and the width of the
inter-field string(s) should be consistent. These
are expectations on dictionary formatting, but can
be violated (e.g. a given dictionary may have some
entries on a single line and others spanning two
lines). To avoid over-pruning the lattice, we de-
termine the coverage of each such soft constraint
in the form of: (a) type-level coverage, i.e. the pro-
portion of records for which a given constraint set-
ting (e.g. record size in terms of the number of
lines it spans) matches with at least one record
span; and (b) token-level coverage, i.e. the pro-
portion of individual spans a given constraint set-
ting matches. We apply soft constraints conser-
vatively, selecting the soft constraint setting with
full type-level coverage (i.e. it matches all records)
39
and maximum token-level coverage (i.e. it prunes
the least edges in the lattice). Soft constraints are
applied iteratively, as indicated in Algorithm 1.
Algorithm 1 Match lattice pruning algorithm
1: Initialize l . initialize record matching match lattice
2: repeat
3: change? False
4: for all hi ? H do . update hard constraint coverage
5: (htypei,htokeni)? coverage(hi, l)
6: if htokeni < 1 then . if pruneable edges
7: l? apply(hi, l) . apply constraint
8: change? True
9: end if
10: end for
11: for all si ? S do . update soft constraint coverage
12: {(stypeij,stokenij)}? coverage(ci, l)
13: end for
14: if s ? argmaxsij(?stypeij = 1.0 ? stoken < 1.0 ?
(?i? 6= i : |stypei?| > 1, ? j? : stokenij < 1.0 : stokenij >
stokeni?j?)) then
15: l? apply(s, l) . apply constraint
16: change? True
17: end if
18: until change = False
The final step is regular expression generaliza-
tion, whereby the disambiguated match lattice is
used to identify the multiline span of all records
in the source dictionary, and inter-field strings not
corresponding to any record field are generalized
across records to form a regular expression, which
is then applied to the remainder of the dictionary to
extract out normalized PanLex records. As part
of this, we build in dictionary-specific heuristics,
such as the common practice of including optional
fields in parentheses.
The lextract code is available from http:
//lextract.googlecode.com.
lextract has been developed over 10 sample
dictionaries, and record matching and match lat-
tice pruning has been found to perform with 100%
precision and recall over the seed records. We are
in the process of carrying out extensive evaluation
of the regular expression generalization over full
dictionary files.
Future plans for lextract to get closer to
true emulation of themanual extraction process in-
clude: dynamic normalization of target language
strings (e.g. normalizing capitalization or correct-
ing inconsistent pluralization) using a combina-
tion of language-specific tools for high-density
target languages such as English, and analysis of
existing PanLex expressions in that language;
elicitation of user feedback for extents of the doc-
ument where extraction has failed, fields where
the correct normalization strategy is unclear (e.g.
normalization of POS tags not seen in the seed
records, as for det.?detr in Figure 1); and extend-
ing lextract to handle (X)HTML and other file
types.
References
Christensen, Janara, Mausam, and Oren Etzioni. 2009.
A rose is a roos is a ruusu: Querying translations for
web image search. In Proc. of the Joint conference
of the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing
of the Asian Federation of Natural Language Pro-
cessing (ACL-IJCNLP 2009), pages 193?196, Sun-
tec, Singapore.
Mausam, Stephen Soderland, Oren Etzioni, Daniel
Weld, Michael Skinner, and Jeff Bilmes. 2009.
Compiling a massive, multilingual dictionary via
probabilistic inference. In Proc. of the Joint con-
ference of the 47th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 4th
International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natural
Language Processing (ACL-IJCNLP 2009), pages
262?270, Suntec, Singapore.
Mausam, Stephen Soderland, Oren Etzioni, Daniel S.
Weld, Kobi Reiter, Michael Skinner, Marcus Sam-
mer, and Jeff Bilmes. 2010. Panlingual lexical
translation via probabilistic inference. Artificial In-
telligence, 174(9?10):619?637.
Sammer, Marcus and Stephen Soderland. 2007. Build-
ing a sense-distinguished multilingual lexicon from
monolingual corpora and bilingual lexicons. In
Proc. of the Eleventh Machine Translation Summit
(MT Summit XI), pages 399?406, Copenhagen, Den-
mark.
Soderland, Stephen, Christopher Lim, Mausam,
Bo Qin, Oren Etzioni, and Jonathan Pool. 2009.
Lemmatic machine translation. In Proc. of Ma-
chine Translation Summit XII, page 2009, Ottawa,
Canada.
Von Ahn, Luis and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proc. of the SIGCHI
conference on Human factors in computing systems,
pages 319?326, Vienna, Austria.
40
