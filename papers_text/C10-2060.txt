Coling 2010: Poster Volume, pages 525?533,
Beijing, August 2010
A Comparative Study on Ranking and Selection Strategies for 
Multi-Document Summarization 
Feng Jin, Minlie Huang, Xiaoyan Zhu 
State Key Laboratory of Intelligent Technology and Systems 
Tsinghua National Laboratory for Information Science and Technology 
Dept. of Computer Science and Technology, Tsinghua University 
jinfengfeng@gmail.com,{aihuang,zxy-dcs}@tsinghua.edu.cn
Abstract 
This paper presents a comparative study 
on two key problems existing in extrac-
tive summarization: the ranking problem 
and the selection problem. To this end, 
we presented a systematic study of 
comparing different learning-to-rank al-
gorithms and comparing different selec-
tion strategies. This is the first work of 
providing systematic analysis on these 
problems. Experimental results on two 
benchmark datasets demonstrate three 
findings: (1) pairwise and listwise learn-
ing-to-rank algorithms outperform the 
baselines significantly; (2) there is no 
significant difference among the learn-
ing-to-rank algorithms; and (3) the in-
teger linear programming selection 
strategy generally outperformed Maxi-
mum Marginal Relevance and Diversity 
Penalty strategies. 
1 Introduction 
As the rapid development of the Internet, docu-
ment summarization has become an important 
task since document collections are growing 
larger and larger. Document summarization, 
which aims at producing a condensed version of 
the original document(s), helps users to acquire 
information that is both important and relevant 
to their information need.  So far, researchers 
have mainly focused on extractive methods 
which choose a set of salient textual units to 
form a summary.  Such textual units are typical-
ly sentences, sub-sentences (Gillick and Favre, 
2009), or excerpts (Sauper and Barzilay, 2009).  
Almost all extractive summarization methods 
face two key problems: the first problem is how 
to rank textual units, and the second one is how 
to select a subset of those ranked units. The 
ranking problem requires systems model the 
relevance of a textual unit to a topic or a query. 
In this paper, the ranking problem refers to ei-
ther sentence ranking or concept ranking. Con-
cepts can be unigrams, bigrams, semantic con-
tent units, etc., although in our experiment, only 
bigrams are used as concepts. The selection 
problem requires systems improve diversity or 
remove redundancy so that more relevant in-
formation can be covered by the summary as its 
length is limited. As our paper focuses on ex-
tractive summarization, the selection problem 
refers to selecting sentences. However, the se-
lection framework presented here is universal 
for selecting arbitrary textual units, as discussed 
in Section 4. 
There have been a variety of studies to ap-
proach the ranking problem. These include both 
unsupervised sentence ranking (Luhn, 1958; 
Radev and Jing, 2004, Erkan and Radev, 2004), 
and supervised methods (Ouyang et al, 2007; 
Shen et al, 2007; Li et al, 2009). Even given a 
list of ranked sentences, it is not trivial to select 
a subset of sentences to form a good summary 
which includes diverse information within a 
length limit. Three common selection strategies 
have been studied to address this problem: Max-
imum Marginal Relevance (MMR) (Carbonell 
and Goldstein, 1998), Diversity Penalty (DiP) 
(Wan, 2007), and integer linear programming 
(ILP) (McDonald, 2007; Gillick and Favre, 
2009). As different methods were often eva-
luated on different datasets, it is of great value 
to systematically compare ranking and selection 
strategies on the same dataset. However, to the 
best of our knowledge, there is still no work to 
compare different ranking strategies or compare 
different selection strategies.  
In this paper, we presented a comparative 
study on the ranking problem and the selection 
525
problem for extractive summarization. We 
compared three genres of learning-to-rank me-
thods for ranking sentences or concepts: SVR, a 
pointwise ranking algorithm; RankNet, a pair-
wise learning-to-rank algorithm; and ListNet, a 
listwise learning-to-rank algorithm. We adopted 
an ILP framework that is able to select sen-
tences based on sentence ranking or concept 
ranking. We compared it with other selection 
strategies such as MMR and Diversity Penalty. 
We conducted our comparative experiments on 
the TAC 2008 and TAC 2009 datasets, respec-
tively. Our contributions are two-fold: First, to 
the best of our knowledge, this is the first work 
of presenting systematic and in-depth analysis 
on comparing ranking strategies and comparing 
selection strategies. Second, this is the first 
work using pairwise and liswise learning-to-
rank algorithms to perform concept (word bi-
gram) ranking for extractive summarization.  
The rest of this paper is organized as follows. 
We introduce the related work in Section 2. In 
Section 3, we present three ranking algorithms, 
SVR, RankNet, and ListNet. We describe the 
sentence selection problem with an ILP frame-
work described in Section 4. We introduce fea-
tures in Section 5. Evaluation and experiments 
are presented in Section 6. Finally, we conclude 
this paper in Section 7. 
2 Related Work 
A number of extractive summarization studies 
used unsupervised methods with surface fea-
tures, linguistic features, and statistical features 
to guide sentence ranking (Edmundson, 1969; 
McKeown and Radev, 1995; Radev et al, 2004; 
Nekova et al, 2006). Recently, graph-based 
ranking methods have been proposed for sen-
tence ranking and scoring, such as LexRank 
(Erkan and Radev, 2004) and TextRank (Mihal-
cea and Tarau, 2004).  
There are also a variety of studies on su-
pervised learning methods for sentence ranking 
and selection. Kupiec et al (1995) developed a 
naive Bayes classifier to decide whether a sen-
tence is worthy to extract. Recently, Conditional 
Random Field (CRF) and Structural SVM have 
been employed for single document summariza-
tion (Shen et al, 2007; Li et al, 2009).  
Besides ranking sentences directly, there are 
some approaches that select sentences based on 
concept ranking. Radev et al (2004) used cen-
troid words whose tf*idf scores are above a 
threshold. Filatova and Hatzivassiloglou (2004) 
used atomic event as concept. Moreover, sum-
marization evaluation metrics such as Basic 
Element (Hovy et al, 2006), ROUGE (Lin and 
Hovy, 2003) and Pyramid (Passonneau et al, 
2005) are all counting the concept overlap be-
tween generated summaries and human-written 
summaries.  
Another important issue existing in extractive 
summarization is to find an optimal sentence 
subset which can cover diverse information. 
Maximal Marginal Relevance (MMR) (Carbo-
nell and Goldstein, 1998) and Diversity Penalty 
(Wan, 2007) are most widely used approaches 
to reduce redundancy. The two methods are es-
sentially based on greedy search. By contrast, 
ILP based approaches view summary generation 
as a global optimization problem. McDonald 
(2007) proposed a sentence-level ILP solution. 
Sauper and Barzilay (2009) presented an ex-
cerpt-level ILP method to generate Wikipedia 
articles. Gillick and Favre (2009) proposed a 
concept-level ILP, but they used document fre-
quency to score concepts (bigrams), without any 
learning process. Some recent studies (Gillick 
and Favre, 2009; Martins and Smith, 2009) also 
modeled sentence selection and compression 
jointly using ILP. Our ILP framework proposed 
here is based on these studies. Although various 
selection strategies have been proposed, there is 
no work to systematically compare these strate-
gies yet. 
Learning to rank attracts much attention in 
the information retrieval community recently. 
Pointwise, pairwise and listwise learning-to-
rank approaches have been extensively studied 
(Liu, 2009). Some of those have been applied to 
document summarization, such as SVR 
(Ouyang et al, 2007), classification SVM 
(Wang et al, 2007), and RankNet (Svore et al, 
2007). Again, there is no work to systematically 
compare these ranking algorithms. To the best 
of our knowledge, this is the first time that a 
listwise learning-to-rank algorithm, ListNet 
(Cao et al, 2007), is adapted to document sum-
marization in this paper. Moreover, pairwise 
and listwise learning-to-rank algorithms have 
never been used to perform concept ranking for 
extractive summarization.  
526
3 Ranking Sentences or Concepts 
Given a query and a collection of relevant doc-
uments, an extractive summarization system is 
required to generate a summary consisting of a 
set of text units (usually sentences). The first 
problem we need to consider is to determine the 
importance of these sentences according to the 
input query. We approach this ranking problem 
in two ways: the first way is to score sentences 
directly using learning-to-rank algorithms, and 
thus the goal of summarization is to select a 
subset of sentences, considering both relevance 
and redundancy. The second way is to score 
concepts within the document collection, and 
then the summarization task is to select a sen-
tence subset that can cover those important con-
cepts maximally. The problem of sentence se-
lection will be described in Section 4.  
Suppose the relevant document collection for 
a query q is Dq. From this collection, we obtain 
a set of sentences or concepts (e.g., word bi-
grams), S={s1,s2,?,sn} or C={c1,c2,?, cn}. Be-
fore training, each si or ci is associated with a 
gold standard score, yi. A feature vector, xj= 
?(sj/cj,q,Dq), is constructed for each sentence or 
concept. The learning algorithm will learn a 
ranking function f(xj) from a collection of 
query-document pairs {(qi,Dqi)|i= 1, 2,?,m}.  
We investigated three learning-to-rank me-
thods to learn f(xj). The first one is a pointwise 
ranking algorithm, support vector regression 
(SVR). This algorithm treats sentences (or con-
cepts) independently. The second method is a 
pairwise ranking algorithm, RankNet, which 
learns a ranking function from a list of sentence 
(or concept) pairs. Each pair is labeled as 1 if 
the first sentence si (or concept ci) ranks ahead 
of the second sj (or cj), and 0 otherwise. 
The listwise ranking algorithm, ListNet, 
learns the ranking function f(xj) in a different 
way. A list of sentences (or concepts) is treated 
as a whole. Both RankNet and ListNet take into 
account the dependency between sentences (or 
concepts). 
3.1  Support Vector Regression  
Support Vector Regression (SVR), a generaliza-
tion of the classical SVM formulation, attempts 
to learn a regression model. SVR has been ap-
plied to summarization in (Ouyang et al, 2007; 
Metzler and Kanungo, 2008). In our work, we 
train the SVR model to fit the gold standard 
score of each sentence or concept.  
Formally, the objective of SVR is to minim-
ize the following objective: 
2
, ,
1 1
|| || ( ( ))
2
i
i i
x
w b
w C v L y f x
N
?
?
?( ) =
? ?? ?? ?
+ ? + ?? ?? ?? ?? ?? ??
(1) 
where L(x)=|x|-? if x > ? and otherwise L(x)=0; 
yi is the gold standard score of xi; f(x) =wTx+b, 
the predicted score of x; C and v are two para-
meters; and N is the total number of training 
examples.  
3.2 RankNet  
RankNet is a pairwise learning-to-rank method 
(Burges et al, 2005). In this algorithm, training 
examples are handled pair by pair. Given a pair 
of feature vectors (xi, xj), the gold standard 
probability ijP is set to be 1 if the label of the 
pair is 1, which means xi ranks ahead of xj. The 
gold standard probability is 0 if the label of the 
pair is 0. Then the predicted probability Pij, 
which defines the probability of xi ranking 
ahead of xj by the model, is represented as a lo-
gistic function:  
exp( ( ) ( ))
1 exp( ( ) ( ))
i j
ij
i j
f x f x
P
f x f x
?
=
+ ?
            (2) 
where f(x) is the ranking function. The objective 
of the algorithm is to minimize the cross entro-
py between the gold standard probability and 
the predicted probability, which is defined as 
follows: 
( ) log (1 ) log(1 )ij ij ij ij ijC f P P P P= ? ? ? ?     (3) 
A three-layer neural network is used as the 
ranking function, as follows:  
3 32 2 21 2 3( ) ( ( ) )n ij jk nk j i
j k
f x g w g w x b b= + +? ?
 
 (4) 
where for weights w and bias b, the superscripts 
indicate the node layer while the subscripts in-
dicate the node indexes within each layer. And 
xnk is the k-th component of input feature vector 
xn. Then a gradient descent method is used to 
learn the parameters. For details, refer to 
(Burges et al, 2005). 
3.3 ListNet 
ListNet takes a list of items as input in the learn-
ing process. More specifically, suppose we have 
527
a list of feature vectors (x1, x2,?, xn) and each 
feature vector xi has an gold standard score yi, 
which has been assigned before training. Ac-
cordingly, we have a list of gold standard scores 
(y1, y2,?,yn). We also have a list of scores as-
signed by the algorithm during training, say, 
(f(x1), f(x2),?, f(xn)). Given a score list 
S={s1,s2,?,sn}, the probability that xj will rank 
the first place among the n items is defined as 
follows: 
1 1
( ) exp( )
( )
( ) exp( )
j j
s n n
k kk k
s s
P j
s s
= =
?
= =
?? ?
        (5) 
It is easy to prove that (Ps(1), Ps(2), ?, Ps(n)) is 
a probability distribution, as the sum of them 
equals to 1. Therefore, the cross entropy can be 
used to define the loss between the gold stan-
dard distribution Py(j) and the distribution Pf(j), 
as follows:  
1
( , ) ( ) log ( )
n
y f
j
L y f P j P j
=
= ??               (6) 
where y represents the gold standard score list  
(y1, y2,?,yn) and f=(f(x1), f(x2),?, f(xn)) is the 
score list output by the ranking algorithm.  
The function f is defined as a linear function, 
as follows: 
( ) Tw i if x w x=                          (7) 
Then the gradient of loss function L(y,f) with 
respect to the parameter vector w can be calcu-
lated as follows:  
1
1
1
( )( , )
( )
( )1
exp( ( ))
exp( ( ))
n
w jw
y j
j
n
w j
w jn
jw jj
f xL y f
w P x
w w
f x
f x
wf x
=
=
=
??? = = ?
? ?
?
+
?
?
??
 (8) 
 
During training, w is updated in a gradient des-
cent manner: w=w -??w and ? is the learning 
rate. For details, refer to (Cao et al, 2007). 
4 ILP-based Selection Framework 
After we have a way of ranking sentences or 
concepts, we face a sentence selection problem: 
selecting an optimal subset of sentences as the 
final summary. To integrate sentence/concept 
ranking, we adopted an integer linear program-
ming (ILP) framework to find the optimal sen-
tence subset (Filatova and Hatzivassiloglou, 
2004; McDonald, 2007; Gillick and Favre, 2009; 
Takamura and Okumura, 2009). ILP is a global 
optimization problem whose objective and con-
straints are linear in a set of integer variables.  
Formally, we define the problem of sentence 
selection as follows: 
maximize:  ( )*  xi i
i
f x z
? ?? ?? ??          (9) 
. .     * | |       uj j
j
z u Lims t ??  
       * ( , ) ,          u xj i
j
z I i j z i? ??  
      ( )* ( , )   ,    x xi j i jz z sim x x i j?+ < ?  
        , {0,1},      ,x ui jz z i j? ?  
where: 
xi ? the representation unit, such as a sentence 
or a concept. We term it representation unit be-
cause the summary quality is represented by the 
set of included xi; 
f(xi) - the ranking function given by the learn-
ing-to-rank algorithms; 
uj - the selection unit, for instance, a sentence in 
this paper. |uj| is the number of words in uj; 
x
iz - the indicator variable which denotes the 
presence or absence of xi in the summary; 
u
jz - the indicator variable which denotes inclu-
sion or exclusion of uj; 
I(i, j) - a  binary constant indicating that wheth-
er xi appears in uj. It is either 1 or 0; 
Lim - the length limit; 
sim(xi, xj) - a similarity measure for considering 
the redundancy; 
? - the redundancy threshold.  
The first constraint indicates the length limit. 
The second constraint asserts that if a represen-
tation unit xi is included in a summary, at least 
one selection unit that contains xi must be se-
lected. The third constraint considers redundan-
cy. If the representation unit is sentence, the 
similarity measure is defined as tf*idf similarity, 
and ?/2 is the similarity threshold, which was 
set to be 1 here. For concepts, the similarity 
measure can be defined as  
1,    
( , )
0,    otherwise
i j
i j
x x
sim x x
=?
= ?? .
 
However, other definition is also feasible, de-
pending on what has been selected as represen-
tation unit. 
528
Note that this framework is very general. If 
the representation unit xi is a sentence, the rank-
ing function is defined on sentence. Thus the 
ILP framework will find a set of sentences that 
can optimize the total scores of selected sen-
tences, subject to several constraints. If the re-
presentation unit is a concept, the ranking func-
tion measures the importance of a concept to be 
included in a summary. Thus the goal of ILP is 
to find a set of sentences by maximizing the 
scores of concepts covered by those selected 
sentences. 
 
Dq relevant document collection in response 
to query q 
d one single document 
wi unigram 
wiwi+1 bigram 
S sentence 
tfd(wi) the frequency of wi occurring in d 
dfD(wi) the number of documents containing wi 
in collection D 
Table 1. Notations for features. 
5 Features 
To facilitate the following description, some 
notations are defined in Table 1. In our dataset, 
each query has a title and narrative to precisely 
define an information need. The following is a 
query example from the TAC 2008 test dataset:  
<topic id = "D0801A">  
 <title> Airbus A380 </title> 
 <narrative> 
Describe developments in the production and 
launch of the Airbus A380. 
 </narrative> 
</topic> 
Features for sentence ranking and concept 
ranking are listed in the following. We use word 
bigrams as concept here. 
Sentence Features 
(1) Cluster frequency: ( )
qi
D iw S
tf w
??  
(2) Title frequency: ( )
i
d iw S
tf w
??  where d is a 
new document that consists of all the titles of 
documents in Dq.  
(3) Query frequency: ( )
i
d iw S
tf w
??  where d is 
a document consisting of the title and narrative 
fields of the current topic.  
(4) Theme frequency: ( )
qi i
D iw S w T
tf w
? ? ??  
where T is the top 10% frequent unigram words 
in Dq. 
(5) Document frequency of bigrams in the sen-
tence: 
1
1( )
i i
D i iw w S
df w w
+
+?? .  
(6) PageRank score: as described in (Mihalcea 
and Tarau, 2004), each sentence in Dq is a node 
in the graph and the cosine similarity between a 
pair of sentences is used as edge weight. 
Concept Features 
(1) Cluster frequency: 1( )qD i itf w w + , the fre-
quency of 1i iw w + occurring in Dq.  
(2) Title frequency: 1( )d i itf w w + , where d is a 
document consisting of all the titles of docu-
ments in Dq. 
(3) Query Frequency: the frequency of the bi-
gram occurring in the topic title and narrative. 
(4) Average term frequency: 
 1( )/ | |
q
d i i qd D
tf w w D+?? . |Dq| is the number of 
documents in the set. 
(5) Document frequency: the document fre-
quency of this bigram. 
(6) Minimal position: the minimal position of 
this bigram relative to the document length.  
(7) Average position: the average position of 
this bigram in collection Dq . 
6 Experimental Results 
6.1 Data Preprocessing 
We conducted experiments on the TAC 2008 
and TAC 2009 datasets. The task requires pro-
ducing a 100-word summary for each query (al-
so termed topic sometimes). There are 48 que-
ries in TAC 2008 and 44 queries in TAC 2009. 
A query example has been given in Section 5. 
Relevant documents for these queries have been 
specified. And four human-written summaries 
were supplied as reference summaries for each 
query. 
We segmented the relevant documents into 
sentences using the LingPipe toolkit 1  and 
stemmed words using the Porter Stemmer. 
Word bigrams are used as concepts in this paper. 
If the two words in a bigram are both stop-
words, the bigram will be discarded. The sen-
                                                 
1 http://alias-i.com/lingpipe/index.html 
529
tence features and bigram features are then cal-
culated. As our focus is on comparing different 
ranking strategies and selection strategies, we 
did not apply any sophisticated linguistic or se-
mantic processing techniques (as pre- or post-
processing). Thus we did not compare our re-
sults to those submitted to the TAC conferences.  
We train the learning algorithms on one data-
set and then evaluate the algorithms on the other. 
The generated summaries are evaluated using 
the ROUGE toolkit (Lin and Hovy, 2003).  
6.2 Preparing Training Samples 
As our work includes both sentence ranking and 
concept ranking, we need to establish two types 
of training data. Fortunately, we are able to do 
this based on the reference summaries and an-
notation results provided by the TAC confe-
rences.  
For the sentence ranking problem, we com-
pute the average ROUGE-1 score for each sen-
tence by comparing it to the four reference 
summaries for each query. This score is treated 
as the gold-standard score. In ListNet, these 
scores are directly used (see formula (5)). While 
in RankNet, the sentences for a query are 
grouped into 10 bins according to their 
ROUGE-1 scores, and then we extract sentences 
from different bins respectively to form a pair. 
We assume that a sentence in a higher scored 
bin should rank ahead of those sentences in 
lower scored bins.  
As for the concept ranking problem, gold-
standard scores are obtained from the human 
annotated Pyramid data. The weight of each 
semantic content unit (SCU) is the number of 
reference summaries in which the SCU appears. 
So straightforwardly, the gold-standard score of 
a bigram is the largest weight of all SCUs that 
contain the bigram. And if a bigram does not 
occur in any SCU, its score will be 0. Thus the 
bigram scores belong to the set {0,1,2,3,4} as 
there are four human-written summaries for 
each query. These scores are directly used in 
ListNet (see formula (5)). And in RankNet, bi-
gram pairs are constructed according to the 
gold-standard scores.  
6.3 Learning Parameters 
For SVR, the radial basis kernel function is em-
ployed and the optimal values for parameters C, 
v and g (for the kernel) are found using the gri-
dregression.py tool provided by LibSVM 
(Chang and Lin, 2001) with a 5-fold cross vali-
dation on the training set.  
RankNet applies a three-layer (one hidden 
layer) neural network with only one node in the 
output layer, as described in (Burges et al, 
2005). The number of hidden neurons was em-
pirically set to be 10. The learning rate was set 
to 0.001 for sentence ranking and 0.01 for bi-
gram ranking.  
As for ListNet, the learning rate for sentence 
ranking and concept ranking are both set to be 
0.1 empirically.  
6.4 Comparing Ranking Strategies 
In this section, we compared different ranking 
strategies for both sentence ranking and concept 
ranking. The sentence selection strategies were 
fixed to the ILP selection framework as shown 
in Section 4. We chose ILP as the selection 
strategy because we want to compare our sys-
tem with the following two methods (as base-
lines): 
(1) SENT_ILP: A sentence-level method pro-
posed by McDonald (2007) with ILP formula-
tion. We implemented the query-focused ver-
sion of the formulae as TAC 2008 and 2009 
required query-focused summarization. 
(2) DF_ILP: A concept-level ILP method using 
document frequency to score word bigrams 
(Gillick and Favre, 2009), without any learning 
process.  
The differences between our framework and 
SENT_ILP are: a) SENT_ILP used a redundan-
cy factor in the objective function whereas we 
modeled redundancy as constraints; b) 
SENT_ILP used tf*idf similarity to compute 
relevance scores whereas we used learning algo-
rithms.  
The ROUGE-1 and ROUGE-2 measures for 
each method are presented in Table 2 and Table 
3. Note that the performance on the TAC 2008 
dataset was obtained from the models that were 
trained on the TAC 2009 dataset. Then, the da-
tasets were interchanged for training and testing, 
respectively. Different learning-to-rank strate-
gies (SVR, RankNet, ListNet) do not show sig-
nificant differences between one and another, 
but they all outperform SENT_ILP substantially 
(p-value < 0.0001). And for concept ranking, 
RankNet and ListNet both achieve significantly 
better ROUGE-2 results (p-value < 0.005) than 
530
DF_ILP. This infers that considering more fea-
tures will have better results than using docu-
ment frequency to score concepts. The Wilcox-
on signed-rank test (Wilcoxon, 1945) is used for 
significance tests in our experiment. A good 
ranking strategy for modeling relevance is im-
portant for extractive summarization. RankNet 
which used a three-layer network (non-linear 
function) as the ranking function performs 
slightly better than ListNet which is based on a 
linear ranking function.  
 
Dataset Method ROUGE-1 ROUGE-2
TAC 
2008 
SVR 0.35086 0.08447 
RankNet 0.36025 0.09291 
ListNet 0.35365 0.09129 
SENT_ILP 0.31546 0.06500 
TAC 
2009 
SVR 0.36125 0.09659 
RankNet 0.36216 0.09778 
ListNet 0.35480 0.09126 
SENT_ILP 0.31962 0.07034 
Table 2. Results of sentence ranking strategies. 
 
Dataset Method ROUGE-1 ROUGE-2
TAC 2008 
SVR 0.36555 0.10291 
RankNet 0.37564 0.11213 
ListNet 0.36863 0.10660 
DF_ILP 0.36922 0.10373 
TAC 2009 
SVR 0.37126 0.10698 
RankNet 0.37513 0.11364 
ListNet 0.37499 0.11313 
DF_ILP 0.36347 0.10156 
Table 3. Results of concept ranking strategies. 
 
It is worth noting that Pyramid annotations 
may not cover all important bigrams, partly be-
cause SCUs in reference summaries have been 
rephrased by human annotators. Note that we 
simply extract original sentences to form a 
summary, thus it is possible that a bigram which 
is important in the original sentences does not 
appear in any rephrased SCUs at all. Such bi-
grams will have a gold-standard score of 0, 
which is erroneous supervision. For example, 
the bigrams hurricane katrina in topic D0804A 
about Katrina pet rescue and life support in 
D0806A about Terri Schiavo case are not anno-
tated in any SCUs, but these bigrams are both 
key terms for the topics.  
6.5 Comparing Selection Strategies 
In order to study the influence of different selec-
tion strategies, we compare the ILP selection 
strategy (as introduced in Section 4) with other 
popular selection strategies, based on the same 
sentence ranking algorithm (we chose sentence-
level RankNet). The baselines to be compared 
are as follows:  
(1) MMR: As shown in (Carbonell and 
Goldstein, 1998), the formula of MMR is: 
{ }1 2arg max ( , ) (1 ) max ( , )
i j
i i js R S s S
MMR D q s D s s? ?
? ? ?
= ? ?
 
where q is the given query; R is the set of all 
sentences; S is the set of already included sen-
tences; D1 is the normalized ranking score f(xi) 
of si, and D2 is the cosine similarity of the fea-
ture vectors for si  and sj. Our implementation 
was similar to the MMR strategy in the 
MEAD2summarizer. 
(2) DiP: Diversity penalty which penalizes the 
score of candidate sentences according to the 
already selected ones (Wan, 2007). 
 
Dataset Method ROUGE-1 ROUGE-2
TAC 2008
ILP 0.36025 0.09291 
MMR 0.35459 0.09086 
DiP 0.35263 0.08689 
TAC 2009
ILP 0.36216 0.09778 
MMR 0.35148 0.08881 
DiP 0.34714 0.08672 
Table 4. Comparing selection strategies. 
 
The corresponding ROUGE scores are pre-
sented in Table 4. ILP outperforms other selec-
tion strategies significantly on the TAC 2009 
dataset (both ILP vs. MMR and ILP vs. DiP). 
Although improvements are observed with ILP 
on the TAC 2008 dataset, the difference is not 
significant (using ILP vs. using MMR). MMR is 
comparable to DiP as they are both based on 
greedy search in nature.  
To investigate the difference between these 
strategies, we present in-depth analysis here. 
First, the average length of summaries generat-
ed by ILP is 97.1, while that by MMR and DiP 
are 95.5 and 92.7, respectively. Note that the 
required summary length is 100 and that more 
words can potentially cover more information. 
Thus, ILP can generate summaries with more 
information. This is because ILP is a global op-
timization algorithm, subject to the length con-
straint. Second, the average rank of sentences 
selected by ILP is 12.6, while that by MMR and 
                                                 
2 http://www.summarization.com/mead/ 
531
DiP is about 5, which is substantially different. 
ILP can search down the ranked list while the 
other two methods tend to only select the very 
top sentences. Third, there are 4.1 sentences on 
average in each ILP-generated summary, while 
the number for MMR and DiP generated sum-
maries are 2.7 and 2.5, respectively. Thus ILP 
tend to select shorter sentences than MMR and 
DiP. This may help reduce redundancy as long-
er sentences may contain more topic irrelevant 
clauses or phrases.  
6.6 Discussions 
Interestingly, although the learning-to-rank al-
gorithms combined with the ILP selection strat-
egy perform well in summarization, the perfor-
mance is still far from that of manual summari-
zation. In this study, we investigate the upper 
bound performance. We used the presented ILP 
framework to generate summaries based on the 
gold-standard scores, rather than the scores giv-
en by the learning algorithms. In other words, 
f(xi) in formula (9) is replaced by the gold-
standard scores. The ROUGE results are shown 
in Table 5. We also listed the best/worst/average 
ROUGE scores of human summaries in TAC by 
comparing one human summary (as generated 
summary) to the other three human summaries 
(as reference summaries). These results are sub-
stantially better than those by the learning algo-
rithms. Sentence- and concept- level ranking 
produces very close results to best human sum-
maries. Some ROUGE-2 scores are even higher 
than those of human summaries. This is reason-
able as human annotators may have difficulty in 
organizing content when there are many docu-
ments and sentences. The results reflect that 
there is a remarkable gap between the gold-
standard scores and the learned scores.  
 
Dataset Method ROUGE-1 ROUGE-2
TAC 
2008 
Sentence-level 0.44216 0.14842 
Concept-level 0.42222 0.16018 
Human Best 0.44220 0.13079 
Human Average 0.41417 0.11606 
Human Worst 0.38005 0.10736 
TAC 
2009 
Sentence-level 0.45500 0.15565 
Concept-level 0.43526 0.17118 
Human Best 0.45663 0.14864 
Human Average 0.44443 0.12680 
Human Worst 0.39652 0.11109 
Table 5. Upper bound performance. 
7 Conclusion and Future Work 
We presented systematic and extensive analysis 
on studying two key problems in extractive 
summarization: the ranking problem and the 
selection problem. We compared three genres of 
learning-to-rank algorithms for the ranking 
problem, and investigated ILP, MMR, and Di-
versity Penalty strategies for the selection prob-
lem. To the best of our knowledge, this is the 
first work of presenting systematic comparison 
and analysis on studying these problems. We 
also at the first time proposed to use learning-to-
rank algorithms to perform concept ranking for 
extractive summarization.  
Our future work will focus on: (1) exploiting 
more features that can reflect summary quality; 
(2) optimizing summarization evaluation me-
trics directly with new learning algorithms. 
Acknowledgments 
This work was partly supported by the Chinese 
Natural Science Foundation under grant No. 
60973104 and No. 60803075, and with the aid 
of a grant from the International Development 
Research Center, Ottawa, Canada IRCI project 
from the International Development.  
References 
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, 
Matt Deeds, Nicole Hamilton and Greg Hullender. 
2005. Learning to Rank Using Gradient Descent. 
In Proceedings of the 22nd International Confe-
rence on Machine Learning.  
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai and 
Hang Li. 2007. Learning to Rank: from Pairwise 
Approach to Listwise Approach. In Proceedings 
of ICML 2007.  
Jaime Carbonell and Jade Goldstein. 1998. The Use 
of MMR, Diversity-Based Reranking for Reorder-
ing Documents and Producing Summaries. In 
Proceedings of SIGIR, August 1998, pp. 335 - 336.  
Chih-Chung Chang and Chih-Jen Lin. 2001. 
LIBSVM: a Library for Support Vector Machines. 
Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm 
H. P. Edmundson. 1969. New Methods in Automatic 
Extracting. Journal of the ACM (JACM) Archive,  
Volume 16, Issue 2 (April 1969) Pages: 264 - 285.  
G. Erkan and Dragomir R. Radev. 2004. LexPage-
Rank: Prestige in Multi-Document Text Summa-
532
rization. In Proceedings of EMNLP 2004, Barce-
lona, Spain.  
Elena Filatova and Vasileios Hatzivassiloglou. 2004. 
Event-based Extractive Summarization. In Pro-
ceedings of ACL Workshop on Summarization, 
volume 111. 
Dan Gillick and Benoit Favre. 2009. A Scalable 
Global Model for Summarization. In Proceedings 
of the Workshop on Integer Linear Programming 
for Natural Language Processing.  
Eduard Hovy, Chin-yew Lin, Liang Zhou and Juni-
chi Fukumoto. 2006. Automated Summarization 
Evaluation with Basic Elements. In Proceedings 
of the Fifth Conference on Language Resources 
and Evaluation.  
Julian Kupiec, Jan Pedersen and Francine Chen. 
1995. A Trainable Document Summarizer. In 
Proceedings of SIGIR'95, pages 68 - 73, New 
York, USA.  
Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha 
and Yong Yu. 2009. Enhancing Diversity, Cover-
age and Balance for Summarization through 
Structure Learning. In Proceedings of the 18th In-
ternational Conference on World Wide Web.  
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
Occurrence Statistics. In Proceedings of HLT-
NAACL, pages 71-78. 
Tie-Yan Liu. 2009. Learning to Rank for Informa-
tion Retrieval, Foundation and Trends on Infor-
mation Retrieval.  Now Publishers.  
H.P. Luhn. 1958. The Automatic Creation of Litera-
ture Abstracts. In IBM Journal of Research and 
Development, Vol. 2, No. 2, pp. 159-165, April 
1958. 
Andr? F. T. Martins and Noah A. Smith. 2009. 
Summarization with a Joint Model for Sentence 
Extraction and Compression. In Proceedings of 
the Workshop on Integer Linear Programming for 
Natural Langauge Processing\. 
Ryan McDonald. 2007. A Study of Global Inference 
Algorithms in Multi-Document Summarization. In 
Proceedings of the 29th ECIR.  
Kathleen McKeown and Dragomir R. Radev. 1995. 
Generating Summaries of Multiple News Articles. 
In Proceedings of SIGIR'95, pages 74?82.  
Donald Metzler and Tapas Kanungo. 2008. Machine 
Learned Sentence Selection Strategies for Query-
Biased Summarization. SIGIR Learning to Rank 
Workshop.  
Rada Mihalcea and Paul Tarau. 2004. TextRank: 
Bringing Order into Texts. In Proceedings of 
EMNLP 2004, Barcelona, Spain, July 2004.  
Ani Nenkova, Lucy Vanderwende and Kathleen 
McKeown. 2006. A Compositional Context Sensi-
tive Multi-document Summarizer: Exploring the 
Factors that Influence Summarization. In Pro-
ceedings of SIGIR 2006.  
You Ouyang, Sujian Li, Wenjie Li. 2007. Develop-
ing Learning Strategies for Topic-based Summa-
rization. In Proceedings of the sixteenth ACM 
Conference on Information and Knowledge Man-
agement, 2007. 
Rebecca J. Passonneau, Ani Nenkova, Kathleen 
McKeown and Sergey Sigelman. 2005. Applying 
the Pyramid Method in DUC 2005. DUC 2005 
Workshop.  
Dragomir R. Radev, Hongyan Jing, Malgorzata Stys, 
and Daniel Tam. 2004. Centroid-based Summari-
zation of Multiple Documents. Information 
Processing and Management, 40:919?938.  
Christina Sauper and Regina Barzilay. 2009. Auto-
matically Generating Wikipedia Articles: A Struc-
ture-Aware Approach. In Proceedings of ACL 
2009.  
Dou Shen, Jian-Tao Sun, Hua Li, QiangYang and 
Zheng Chen. 2007. Document Summarization Us-
ing Conditional Random Fields. In IJCAI, pages 
2862 - 2867, 2007.  
Krysta Svore, Lucy Vanderwende, and Chris Burges. 
2007. Enhancing Single-Document Summariza-
tion by Combining RankNet and Third-Party 
Sources. In Proceedings of EMNLP-CoNLL 
(2007), pp. 448-457.. 
Hiroya Takamura and Manabu Okumura. Text 
Summarization Model Based on Maximum Cov-
erage Problem and its Variant. In Proceedings 
EACL, 2009.  
Xiaojun Wan and Jianguo Xiao. 2007. Towards a 
Unified Approach Based on Affinity Graph to 
Various Multi-document Summarizations. ECDL 
2007, 297-308.  
Changhu Wang, Feng Jing, Lei Zhang and Hong-
Jiang Zhang. 2007. Learning Query-Biased Web 
Page Summarization. In Proceedings of the six-
teenth ACM Conference on Information and 
Knowledge Management.  
Frank Wilcoxon. 1945. Individual comparisons by 
ranking methods. Biometrics, 1, 80-83. 
533
