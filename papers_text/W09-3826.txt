Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 172?175,
Paris, October 2009. c?2009 Association for Computational Linguistics
Grammar Error Detection with Best Approximated Parse
Jean-Philippe Prost
LIFO, Universite? d?Orle?ans
INRIA Lille - Nord Europe
Jean-Philippe.Prost@univ-orleans.fr
Abstract
In this paper, we propose that grammar er-
ror detection be disambiguated in generat-
ing the connected parse(s) of optimal merit
for the full input utterance, in overcom-
ing the cheapest error. The detected er-
ror(s) are described as violated grammat-
ical constraints in a framework for Model-
Theoretic Syntax (MTS). We present a
parsing algorithm for MTS, which only re-
lies on a grammar of well-formedness, in
that the process does not require any extra-
grammatical resources, additional rules
for constraint relaxation or error handling,
or any recovery process.
1 Introduction
Grammar error detection is a crucial part of
NLP applications such as Grammar Checking or
Computer-Assisted Language Learning (CALL).
The problem is made highly ambiguous depending
on which context is used for interpreting, and thus
pinpointing, the error. For example, a phrase may
look perfectly fine when isolated (e.g. brief inter-
view), but is erroneous in a specific context (e.g.
in *The judge grants brief interview to this plain-
tiff, or in *The judges brief interview this plain-
tiff ). Robust partial parsing is often not enough to
precisely desambiguate those cases. The solution
we prescribe is to point out the error(s) as a set
of violated (atomic) constraints of minimal cost,
along with the structural context used for measur-
ing that cost. Given an ungrammatical input string,
the aim is then to provide an approximated rooted
parse tree for it, along with a description of all the
grammatical constraints it violates. For example,
Figure 1 illustrates an approximated parse for an
ill-formed sentence in French, and the error be-
ing detected in that context. Property Grammar
(Blache, 2001) provides an elegant framework for
that purpose.
S15
NP3
D1
Le
The
N2
juge
judge
VP9
V8
octroie
grants
*NP7
AP6
A4
bref
brief
N5
entretien
interview
PP10
P11
a`
to
NP12
D13
ce
this
N14
plaignant
plaintiff
Figure 1: Approximated parse for an erroneous French sen-
tence (the Noun ?entretien? requires a Determiner).
Most of the relevant approaches to robust
knowledge-based parsing addresses the problem
as a recovery process. More specifically, we
observe three families of approaches in that re-
spect: those relying on grammar mal-rules in or-
der to specify how to correctly parse what ought
to be ungrammatical (Bender et al, 2004; Foster,
2007); those relying on constraint relaxation ac-
cording to specified relaxation rules (Douglas and
Dale, 1992); and those relying on constraint re-
laxation with no relaxation rules, along with a re-
covery process based on weighted parsing (Fou-
vry, 2003; Foth et al, 2005). The first two are
actually quite similar, in that, through their use
of extra-grammatical rules, they both extend the
grammar?s coverage with a set of ought-to-be-
ungrammatical utterances. The main drawback
of those approaches is that when faced with un-
expected input at best their outcome remains un-
known, at worst the parsing process fails. With
robust weighted parsing, on the other hand, that
problem does not occur. The recovery process
consists of filtering out structures with respect to
their weights or the weights of the constraints be-
ing relaxed. However, these strategies usually
can not discriminate between grammatical and un-
grammatical sentences. The reason for that comes
172
from the fact that grammaticality is disconnected
from grammar consistency: since the grammar
contains contradicting (universal) constraints, no
conclusion can be drawn with regard to the gram-
maticality of a syntactic structure, which violates
part of the constraint system. The same problem
occurs with Optimality Theory. In a different fash-
ion, Fouvry weighs unification constraints accord-
ing to ?how much information it contains?. How-
ever, relaxation only seems possible for those uni-
fication constraints: error patterns such as word
order, co-occurrence, uniqueness, mutual exclu-
sion, . . . can not be tackled. The same restriction is
observed in VanRullen (2005), though to a much
smaller extent in terms of unrelaxable constraints.
What we would like is (i) to detect any type
of errors, and present them as conditions of well-
formedness being violated in solely relying on the
knowledge of a grammar of well-formedness?as
opposed to an error grammar or mal-rules, and
(ii) to present, along-side the violated constraints,
an approximated parse for the full sentence, which
may explain which errors have been found and
overcome. We propose here a parsing algorithm
which meets these requirements.
2 Property Grammar
The framework we are using for knowledge rep-
resentation is Property Grammar (Blache, 2001)
(PG), whose model-theoretical semantics was for-
malised by Duchier et al (2009). Intuitively, a
PG grammar decomposes what would be rewriting
rules of a generative grammar into atomic syntac-
tic properties ? a property being represented as a
boolean constraint. Take, for instance, the rewrit-
ing rule NP ? D N. That rule implicitely informs
on different properties (for French): (1) NP has a
D child; (2) the D child is unique; (3) NP has an
N child; (4) the N child is unique; (5) the D child
precedes the N child; (6) the N child requires the
D child. PG defines a set of axioms, each axiom
corresponding to a constraint type. The proper-
ties above are then specified in the grammar as the
following constraints: (1) NP :M D; (2) NP : D!;
(3) NP :M N; (4) NP : N!; (5) NP : D ? N; (6)
NP : N ? D. These constraints can be indepen-
dently violated. A PG grammar is traditionally
presented as a collection of Categories (or Con-
structions), each of them being specified by a set
of constraints. Table 1 shows an example of a
category. The class of models we are working
NP (Noun Phrase)
Features Property Type : Properties
[AVM]
obligation : NP:M(N ? PRO)uniqueness : NP: D!: NP: N!: NP: PP!: NP: PRO!linearity : NP: D ? N: NP: D ? PRO: NP: D ? AP: NP: N ? PPrequirement : NP: N ? D: NP: AP ? Nexclusion : NP: N < PRO
dependency : NP: N?GEND 1NUM 2
? D?GEND 1NUM 2
?
Table 1: NP specification in Property Grammar
with is made up of trees labelled with categories,
whose surface realisations are the sentences ? of
language. A syntax tree of the realisation of the
well-formed sentence ? is a strong model of the
PG grammar G iff it satisfies every constraint in G.
The loose semantics also allows for constraints to
be relaxed. Informally, a syntax tree of the realisa-
tion of the ill-formed sentence ? is a loose model
of G iff it maximises the proportion of satisfied
constraints in G with respect to the total number
of evaluated ones for a given category. The set of
violated constraints provides a description of the
detected error(s).
3 Parsing Algorithm
The class of models is further restricted to con-
stituent tree structures with no pairwise intersect-
ing constituents, satisfying at least one constraint.
Since the solution parse must have a single root,
should a category not be found for a node a wild-
card (called Star) is used instead. The Star cate-
gory is not specified by any constraint in the gram-
mar.
We introduce an algorithm for Loose Satisfac-
tion Chart Parsing (LSCP), presented as Algo-
rithm 1. We have named our implementation of it
Numbat. LSCP is based on the probabilistic CKY,
augmented with a process of loose constraint sat-
isfaction. However, LSCP differs from CKY in
various respects. While CKY requires a grammar
in Chomsky Normal Form (CNF), LSCP takes an
ordinary PG grammar, since no equivalent of the
CNF exists for PG. Consequently, LSCP gener-
ates n-ary structures. LSCP also uses scores of
merit instead of probabilities for the constituents.
That score can be optimised, since it only factors
through the influence of the constituent?s immedi-
ate descendants.
Steps 1 and 2 enumerate all the possible and
173
Algorithm 1 Loose Satisfaction Chart Parsing
/? Initialisation ?/
Create and clear the chart pi: every score in pi is set to 0
/? Base case: populate pi with POS-tags for each word ?/
for i? 1 to num words
for (each POS-category T of wi)
if merit(T ) ? pi[i, 1, T ] then
Create constituent wTi , whose category is T
pi[i, 1, T ]? {wTi , merit(wTi )}
/? Recursive case ?/
/? Step 1: SELECTION of the current reference span ?/
for span? 1 to num words
for offset ? 1 to num words? span + 1
end ? offset + span? 1
K ? ?
/? Step 2: ENUMERATION of all the configurations ?/
for (every set partition P in [offset, . . . , end])
KP ? buildConfigurations(P)
K ? K ?KP
/? Step 3: CHARACTERISATION of the constraint system from the grammar ?/
for (every configurationA ? KP )
?A ? characterisation(A)/? Step 4: PROJECTION into categories ?/
/? CA is a set of candidate constituents ?/
CA ? projection(?A )checkpoint(CA)
/? Step 5: MEMOISATION of the optimal candidate constituent ?/
for (every candidate constituent x ? CA, of construction C)
if merit(x) ? pi[offset, span, C] then
pi[offset, span, C]? {x, merit(x)}
if pi[offset, span] = ? then
pi[offset, span]? preferred forest inK
legal configurations of optimal sub-structures al-
ready stored in the chart for a given span and off-
set. At this stage, a configuration is a tree with
an unlabelled root. Note that Step 2 actually does
not calculate all the set partitions, but only the le-
gal ones, i.e. those which are made up of sub-
sets of contiguous elements. Step 3 evaluates the
constraint system, using a configuration as an as-
signment. The characterisation process is imple-
mented with Algorithm 2. Step 4 consists of mak-
Algorithm 2 Characterisation Function
function characterisation(A = ?c1, . . . , cn? : assignment,
G: grammar)
returns the set of evaluated properties relevant toA,
and the set of projected categories forA.
/? For storing the result characterisation: ?/
create and clear ?A [property]: table of boolean, indexed by property/? For storing the result projected categories: ?/
create and clear CA: set of category
/? For temporarily storing the properties to be evaluated: ?/
create and clear S: set of property
for (mask ? [1 . . . 2n ? 1])
key? applyBinaryMask(A,mask)
if (key is in the set of indexes for G) then
/? Properties are retrieved from the grammar, then evaluated ?/
S ? G[key].getProperties()
?A ? evaluate(S)/? Projection Step: fetch the categories to be projected ?/
CA ? G[key].getDominantCategories()
return ?A , CA
The key is a hash-code of a combination of constructions, used for fetching the
constraints this combination is concerned with.
ing a category judgement for a configuration, on
the basis of which constraints are satisfied and vi-
olated, in order to label its root. The process is a
simple table lookup, the grammar being indexed
by properties. Step 5 then memoises the optimal
sub-structures for every possible category. Note
that the uniqueness of the solution is not guaran-
teed, and there may well be many different parses
with exact same merit for a given input utterance.
Should the current cell in the chart not being
populated with any constituents, a preferred for-
est of partial parses (= Star category) is used in-
stead. The preferred forest is constructed on the
fly (as part of buildConfigurations); a pointer
is maintained to the preferred configuration dur-
ing enumeration. The preference goes to: (i) the
constituents with the widest span; (ii) the least
overall number of constituents. This translates
heuristically into a preference score pF computed
as follows (where F is the forest, and Ci its con-
stituents): pF = span ? (merit(Ci) + span). In
that way, LSCP always delivers a parse for any
input. The technique is somehow similar to the
one of Riezler et al (2002), where fragment parses
are allowed for achieving increased robustness, al-
though their solution requires the standard gram-
mar to be augmented with a fragment grammar.
4 Evaluation
In order to measure Numbat?s ability to (i) detect
errors in an ungrammatical sentence, and (ii) build
the best approximated parse for it, Numbat should,
ideally, be evaluated on a corpus of both well-
formed and ill-formed utterances annotated with
spannnig phrase structures. Unfortunately, such
a Gold Standard is not available to us. The de-
velopment of adequate resources is central to fu-
ture works. In order to (partially) overcome that
problem we have carried out two distinct evalua-
tions: one aims to measure Numbat?s performance
on grammatical sentences, and the other one on
ungrammatical sentences. Evaluation 1, whose re-
sults are reported in Table 2, follows the proto-
col devised for the EASY evaluation campaign of
parsers of French (Paroubek et al, 2003), with a
subset of the campaign?s corpus. For comparison,
Table 3 reports the performance measured under
the same circumstances for two other parsers: a
shallow one (VanRullen, 2005) also based on PG,
and a stochastic one (VanRullen et al, 2006). The
grammar used for that evaluation was developed
by VanRullen (2005). Evaluation 2 was run on
174
Precision Recall F
Total 0.7835 0.7057 0.7416
general lemonde 0.8187 0.7515 0.7837
general mlcc 0.7175 0.6366 0.6746
general senat 0.8647 0.7069 0.7779
litteraire 0.8124 0.7651 0.788
mail 0.7193 0.6951 0.707
medical 0.8573 0.678 0.757
oral delic 0.6817 0.621 0.649
questions amaryllis 0.8081 0.7432 0.7743
questions trec 0.8208 0.7069 0.7596
Table 2: EASY scores of Numbat (Eval. 1)
Precision Recall F
shallow parser 0.7846 0.8376 0.8102
stochastic parser 0.9013 0.8978 0.8995
Table 3: Comparative EASY scores
a corpus of unannotated ungrammatical sentences
(Blache et al, 2006), where each of the ungram-
matical sentences (amounting to 94% of the cor-
pus) matches a controlled error pattern. Five ex-
pert annotators were asked whether the solution
trees were possible and acceptable syntactic parses
for their corresponding sentence. Specific instruc-
tions were given to make sure that the judgement
does not hold on the grammatical acceptability of
the surface sentence as such, but actually on the
parse associated with it. For that evaluation Van-
Rullen?s grammar was completed with nested cat-
egories (since the EASY annotation scheme only
has chunks). Given the nature of the material to
be assessed here, the Precision and Recall mea-
surements had to be modified. The total number
of input sentences is interpreted as the number of
predictions; the number of COMPLETE structures
is interpreted as the number of observations; and
the number of structures evaluated as CORRECT
by human judges is interpreted as the number of
correct solutions. Hence the following formula-
tions and scores: Precision=CORRECT/COMPLETE=0.74;
Recall=CORRECT/Total=0.68; F=0.71. 92% of the cor-
pus is analysed with a complete structure; 74% of
these complete parses were judged as syntactically
correct. The Recall score indicates that the correct
parses represent 68% of the corpus. In spite of a
lack of a real baseline, these scores compare with
those of grammatical parsers.
5 Conclusion
In this paper, we have proposed to address the
problem of grammar error detection in providing
a set of violated syntactic properties for an ill-
formed sentence, along with the best structural
context in the form of a connected syntax tree. We
have introduced an algorithm for Loose Satisfac-
tion Chart Parsing (LSCP) which meets those re-
quirements, and presented performance measures
for it. Future work includes optimisation of LSCP
and validation on more appropriate corpora.
Acknowledgement
Partly funded by ANR-07-MDCO-03 (CRoTAL).
References
E. M. Bender, D. Flickinger, S. Oepen, A. Walsh, and
T. Baldwin. 2004. Arboretum: Using a precision
grammar for grammar checking in CALL. In Proc.
of InSTIL/ICALL2004, volume 17, page 19.
P. Blache, B. Hemforth, and S. Rauzy. 2006. Ac-
ceptability Prediction by Means of Grammaticality
Quantification. In Proc. of CoLing/ACL, pages 57?
64. ACL.
P. Blache. 2001. Les Grammaires de Proprie?te?s :
des contraintes pour le traitement automatique des
langues naturelles. Herme`s Sciences.
S. Douglas and R. Dale. 1992. Towards Robust PATR.
In Proc. of CoLing, volume 2, pages 468?474. ACL.
D. Duchier, J-P. Prost, and T-B-H. Dao. 2009.
A Model-Theoretic Framework for Grammaticality
Judgements. In To appear in Proc. of FG?09, vol-
ume 5591 of LNCS. FOLLI, Springer.
J. Foster. 2007. Real bad grammar: Realistic grammat-
ical description with grammaticality. Corpus Lin-
guistics and Lingustic Theory, 3(1):73?86.
K. Foth, W. Menzel, and I. Schro?der. 2005. Robust
Parsing with Weighted Constraints. Natural Lan-
guage Engineering, 11(1):1?25.
F. Fouvry. 2003. Constraint relaxation with weighted
feature structures. pages 103?114.
P. Paroubek, I. Robba, and A. Vilnat. 2003. EASY:
An Evaluation Protocol for Syntactic Parsers.
www.limsi.fr/RS2005/chm/lir/lir11/ (08/2008).
S. Riezler, T. H. King, R. M. Kaplan, R. Crouch,
J. T. III Maxwell, and M. Johnson. 2002.
Parsing the Wall Street Journal using a Lexical-
Functional Grammar and Discriminative Estimation
Techniques. In Proc. of ACL, pages 271?278. ACL.
T. VanRullen, P. Blache, and J-M. Balfourier. 2006.
Constraint-Based Parsing as an Efficient Solution:
Results from the Parsing Evaluation Campaign
EASy. In Proc. of LREC, pages 165?170.
T. VanRullen. 2005. Vers une analyse syntaxique a`
granularite? variable. The`se de doctorat.
175
