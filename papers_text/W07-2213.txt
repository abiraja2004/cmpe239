Proceedings of the 10th Conference on Parsing Technologies, pages 94?105,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Are Very Large Context-Free Grammars Tractable?
Pierre Boullier & Beno??t Sagot
INRIA-Rocquencourt
Domaine de Voluceau, Rocquencourt BP 105
78153 Le Chesnay Cedex, France
{Pierre.Boullier,Benoit.Sagot}@inria.fr
Abstract
In this paper, we present a method which, in
practice, allows to use parsers for languages
defined by very large context-free grammars
(over a million symbol occurrences). The
idea is to split the parsing process in two
passes. A first pass computes a sub-grammar
which is a specialized part of the large gram-
mar selected by the input text and various
filtering strategies. The second pass is a tra-
ditional parser which works with the sub-
grammar and the input text. This approach
is validated by practical experiments per-
formed on a Earley-like parser running on
a test set with two large context-free gram-
mars.
1 Introduction
More and more often, in real-word natural lan-
guage processing (NLP) applications based upon
grammars, these grammars are no more written by
hand but are automatically generated, this has sev-
eral consequences. This paper will consider one of
these consequences: the generated grammars may
be very large. Indeed, we aim to deal with grammars
that have, say, over a million symbol occurrences
and several hundred thousands rules. Traditional
parsers are not usually prepared to handle them,
either because these grammars are simply too big
(the parser?s internal structures blow up) or the time
spent to analyze a sentence becomes prohibitive.
This paper will concentrate on context-free gram-
mars (CFG) and their associated parsers. However,
virtually all Tree Adjoining Grammars (TAG, see
e.g., (Schabes et al, 1988)) used in NLP applica-
tions can (almost) be seen as lexicalized Tree In-
sertion Grammars (TIG), which can be converted
into strongly equivalent CFGs (Schabes and Waters,
1995). Hence, the parsing techniques and tools de-
scribed here can be applied to most TAGs used for
NLP, with, in the worst case, a light over-generation
which can be easily and efficiently eliminated in a
complementary pass. This is indeed what we have
achieved with a TAG automatically extracted from
(Villemonte de La Clergerie, 2005)?s large-coverage
factorized French TAG, as we will see in Section 4.
Even (some kinds of) non CFGs may benefit from
the ideas described in this paper.
The reason why the run-time of context-free (CF)
parsers for large CFGs is damaged relies on a theo-
retical result. A well-known result is that CF parsers
may reach a worst-case running time ofO(|G|?n3)
where |G| is the size of the CFG and n is the length
of the source text.1 In typical NLP applications
which mainly work at the sentence level, the length
of a sentence does not often go beyond a value of
say 100, while its average length is around 20-30
words.2 In these conditions, the size of the grammar,
despite its linear impact on the complexity, may be
the prevailing factor: in (Joshi, 1997), the author re-
marks that ?the real limiting factor in practice is the
size of the grammar?.
The idea developed in this paper is to split the
parsing process in two passes. A first pass called
filtering pass computes a sub-grammar which is the
1These two notions will be defined precisely later on.
2At least for French, English and similar languages.
94
sub-part of the large input grammar selected by the
input sentence and various filtering strategies. The
second pass is a traditional parser which works with
the sub-grammar and the input sentence. The pur-
pose is to find a filtering strategy which, in typical
practical situations, minimizes on the average the
total run-time of the filtering pass followed by the
parser pass.
A filtering pass may be seen as a (filtering) func-
tion that uses the input sentence to select a sub-
grammar out of a large input CFG. Our hope, us-
ing such a filter, is that the time saved by the parser
pass which uses a (smaller) sub-grammar will not
totally be used by the filter pass to generate this sub-
grammar.
It must be clear that this method cannot improve
the worst-case parse-time because there exists gram-
mars for which the sub-grammar selected by the fil-
tering pass is the input grammar itself. In such a
case, the filtering pass is simply a waste of time. Our
purpose in this paper is to argue that this technique
may profit from typical grammars used in NLP. To
do that we put aside the theoretical view point and
we will consider instead the average behaviour of
our processors.
More precisely we will study on two large NL
CFGs the behaviour of our filtering strategies on a
set of test sentences. The purpose being to choose
the best filtering strategy, if any. By best, we mean
the one which, on the average, minimizes the total
run-time of both the filtering pass followed by the
parsing pass.
Useful formal notions and notations are recalled
in Section 2. The filtering strategies are presented
in Section 3 while the associated experiments are
reported in Section 4. This paper ends with some
concluding remarks in Section 5.
2 Preliminaries
2.1 Context-free grammars
A CFG G is a quadruple (N,T, P, S) where N is
a non-empty finite set of nonterminal symbols, T is
a finite set of terminal symbols, P is a finite set of
(context-free rewriting) rules (or productions) and
S is a distinguished nonterminal symbol called the
axiom. The sets N and T are disjoint and V = N?T
is the vocabulary. The rules in P have the form A?
?, with A ? N and ? ? V ?.
For a given string ? ? V ?, its size (length)
is noted |?|. As an example, for the input string
w = a1 ? ? ? an, ai ? T , we have |w| = n. The empty
string is denoted ? and we have |?| = 0. The size |G|
of a CFG G is defined by |G| = ?A???P |A?|.
For G, on strings of V ?, we define the binary re-
lation derive, noted ?, by ?1A?2 A???G ?1??2 if
A ? ? ? P and ?1, ?2 ? V ?. The subscript G
or even the superscript A ? ? may be omitted. As
usual, its transitive (resp. reflexive transitive) clo-
sure is noted +?
G
(resp. ??
G
). We call derivation any
sequence of the form ?1 ?G ? ? ? ?G ?2. A complete
derivation is a derivation which starts with the ax-
iom and ends with a terminal string w. In that case
we have S ??
G
? ??
G
w, and ? is a sentential form.
The string language defined (generated, recog-
nized) by G is the set of all the terminal strings that
are derived from the axiom: L(G) = {w | S +?
G
w,w ? T ?}. We say that a CFG is empty iff its
language is empty.
A nonterminal symbol A is nullable iff it can de-
rive the empty string (i.e., A +?
G
?). A CFG is ?-free
iff its nonterminal symbols are non-nullable.
A CFG is reduced iff every symbol of every pro-
duction is a symbol of at least one complete deriva-
tion. A reduced grammar is empty iff its production
set is empty (P = ?). We say that a non-empty
reduced grammar is in canonical form iff its vocab-
ulary only contains symbols that appear in the pro-
ductions of P .3,4
Two CFGs G and G? are weakly equivalent iff
they generate the same string language. They are
strongly equivalent iff they generate the same set of
structural descriptions (i.e., parse trees). It is a well
known result (See Section 3.2) that every CFG G
can be transformed in time linear w.r.t. |G| into a
strongly equivalent (canonical) reduced CFG G?.
For a given input string w ? T ?, we define its
3We may say that the canonical form of the empty reduced
grammar is ({S}, ?, ?, S) though the axiom S does not appear
in any production.
4Note that the pair (P, S) completely defines a reduced CFG
G = (N,T, P, S) in canonical form since we have N = {X0 |
X0 ? ? ? P} ? {S}, T = {Xi | X0 ? X1 ? ? ?Xp ?
P ?1 ? i ? p}?N . Thus, in the sequel, we often note simply
G = (P, S) grammars in canonical form.
95
ranges as the set Rw = {[i..j] | 1 ? i ? j ?
|w| + 1}. If w = w1tw3 ? T ? is a terminal string,
and if t ? T ? {?} is a (terminal or empty) sym-
bol, the instantiation of t in w is the triple noted
t[i..j] where [i..j] is a range with i = |w1| + 1 and
j = i + |t|. More generally, the instantiation of the
terminal string w2 in w1w2w3 is noted w2[i..j] with
i = |w1| + 1 and j = i + |w2|. Obviously, the in-
stantiation of w itself is then w[1..1 + |w|].
Let us consider an input string w = w1w2w3
and a CFG G. If we have a complete derivation
d = S ??
G
w1Aw3 A???G w1?w3
??
G
w1w2w3, we
see that A derives w2 (we have A +?G w2). More-
over, in this complete derivation, we also know a
range in Rw, namely [i..j], which covers the sub-
string w2 which is derived by A (i = |w1| + 1
and j = i + |w2|). This is represented by the in-
stantiated nonterminal symbol A[i..j]. In fact, each
symbol which appears in a complete derivation may
be transformed into its instantiated counterpart. We
thus talk of instantiated productions or (complete)
instantiated derivations. For a given input text w,
and a CFG G, let PwG be the set of instantiated pro-
ductions that appears in all complete instantiated
derivations.5 The pair (PwG , S[1..|w|+1]) is the (re-
duced) shared parse forest in canonical form.6
2.2 Finite-state automata
A finite-state automaton (FSA) is the 5-tuple A =
(Q,?, ?, q0, F ) where Q is a non empty finite set
of states, ? is a finite set of terminal symbols, ? is
the transition relation ? = {(qi, t, qj)|qi, qj ? Q ?
t ? T ? {?}}, q0 is a distinguished element of Q
called the initial state and F is a subset of Q whose
elements are called final states. The size of A is
defined by |A| = |?|.
As usual, we define both a configuration as an ele-
ment of Q?T ? and derive a binary relation between
5For example, in the previous complete derivation
d, let the right-hand side ? be the (vocabulary) string
X1 ? ? ?Xk ? ? ?Xp in which each symbol Xk derives the ter-
minal string xk ? T ? (we have Xk ??
G
xk and w2 =
x1 ? ? ?xk ? ? ?xp), then the instantiated production A[i0..ip] ?
X1[i0..i1] ? ? ?Xk[ik?1..ik] ? ? ?Xp[ip?1..ip] with i0 = |w1| +
1, i1 = i0 + |x1|, . . . , ik = ik?1 + |xk| . . . and ip = i0 + |w2|
is an element of PwG .
6The popular notion of shared forests mainly comes from
(Billot and Lang, 1989).
configurations, noted ?
A
by (q, tx) ?
A
(q?, x), iff
(q, t, q?) ? ?. If w?w?? ? T ?, we call derivation any
sequence of the form (q?, w?w??) ?
A
? ? ? ?
A
(q??, w??).
If w ? T ?, the initial configuration is noted c0 and
is the pair (q0, w). A final configuration is noted cf
and has the form (qf , ?) with qf ? F . A complete
derivation is a derivation which starts with c0 and
ends in a final configuration cf . In that case we have
c0
?
?
A
cf .
The language L(A) defined (generated, recog-
nized) by the FSA A is the set of all terminal strings
w for which there exists a complete derivation. We
say that an FSA is empty iff its language is empty.
Two FSAs A and A? are equivalent iff they defined
the same language.
An FSA is ?-free iff its transition relation has the
form ? = {(qi, t, qj)|qi, qj ? Q, t ? ?}, except per-
haps for a distinguished transition, the ?-transition
which has the form (q0, ?, qf ), qf ? F and allows
the empty string ? to be in L(A). Every FSA can be
transformed into an equivalent ?-free FSA.
An FSA A = (Q,?, ?, q0, F ) is reduced iff every
element of ? appears in a complete derivation. A
reduced FSA is empty iff we have ? = ?. We say
that a non-empty reduced FSA is in canonical form
iff its set of states Q and its set of terminal symbols
? only contain elements that appear in the transition
relation ?.7 It is a well known result that every FSA
A can be transformed in time linear with |A| into an
equivalent (canonical) reduced FSA A?.
2.3 Input strings and input DAGs
In many NLP applications8 the source text cannot
be considered as a single string of terminal symbols
but rather as a finite set of terminal strings. These
sets are finite languages which can be defined by
particular FSAs. These particular type of FSAs are
called directed-acyclic graphs (DAGs). In a DAG
w = (Q,?, ?, q0, F ), the initial state q0 is 1 and we
assume that there is a single final state f (F = {f}),
Q is a finite subset of the positive integers less than
or equal to f : Q = {i|1 ? i ? f}, ? is the set of
terminal symbols. For the transition relation ?, we
7We may say that the canonical form of the empty reduced
FSA is ({q0}, ?, ?, q0, ?) though the initial state q0 does not
appear in any transition.
8Speech processing, lexical ambiguity representation, . . .
96
require that its elements (i, t, j) are such that i < j
(there are no loops in a DAG). Without loss of gen-
erality, we will assume that DAGs are ?-free reduced
FSAs in canonical form and that any DAG w is noted
by a triple (?, ?, f) since its initial state is always 1
and its set of states is {i | 1 ? i ? f}.
For a given CFG G, the recognition of an input
DAG w is equivalent to the emptiness of its inter-
section with G. This problem can be solved in time
linear in |G| and cubic in |Q| the number of states of
w.
If the input text is a DAG, the previous notions of
range, instantiations and parse forest easily general-
ize: the indices i and j which in the string case locate
the positions of substrings are changed in the DAG
case into DAG states. For example if A[i0..ip] ?
X1[i0..i1] ? ? ?Xp[ip?1..ip] is an instantiated produc-
tion of the parse forest for G = (N,T, P, S) and
w = (?, ?, f), we have A ? X1 ? ? ?Xp ? P and
there is a path in the input DAG from state i0 to state
ip via states i1, . . . , ip?1.
Of course, any nonempty terminal string w ? T+,
may be viewed as a DAG (?, ?, f) where ? = {t |
w = w1tw2 ? t ? T}, ? = {(i, t, i + 1) | w =
w1tw2?t ? T?i = 1+|w1|} and f = 1+|w|. If the
input string w is the empty string ?, the associated
DAG is (?, ?, f) where ? = ?, ? = {(1, ?, 2)} and
f = 2. Thus, in the sequel, we will assume that the
inputs of our parsers are not strings but DAGs. As a
consequence the size (or length) of a sentence is the
size of its DAG (i.e., its number of transitions).
3 Filtering Strategies
3.1 Gold Strategy
Let G = (N,T, P, S) be a CFG, w = (?, ?, f)
be an input DAG of size n = |?| and ?Fw? =
(?Pw?, S[1..f ]) be the reduced output parse for-
est in canonical form. From ?Pw?, it is pos-
sible to extract a set of (reduced) uninstanti-
ated productions P gw = {A ? X1 ? ? ?Xp |
A[i0..ip] ? X1[i0..i1]X2[i1..i2] ? ? ?Xp[ip?1..ip] ?
?Pw?}, which, together with the axiom S, defines a
new reduced CFG Ggw = (P gw, S) in canonical form.
This grammar is called the gold grammar of G for
w, hence the superscript g. Now, if we use Ggw to
reparse the same input DAG w, we will get the same
output forest ?Fw?. But in that case, we are sure that
every production in P gw is used in at least one com-
plete derivation. Now, if this process is viewed as
a filtering strategy that computes a filtering function
as introduced in Section 1, it is clear that this strat-
egy is size-optimal in the sense that P gw is of minimal
size, we call it the gold strategy and the associated
gold filtering function is noted g. Since we do not
want that a filtering strategy looses parses, the result
Gfw = (P fw , S) of any filtering function f must be
such that, for every sentence w, P fw is a superset of
P gw. In other words the recall score of any filtering
function f must be of 100%. We can note that the
parsing pass which generates Ggw may be led by any
filtering strategy f .
As usual, the precision score (precision for short)
of a filtering strategy f (w.r.t. the gold case) is, for
a given w, defined by the quotient |P
g
w|
|P fw|
which ex-
presses the number of useful productions selected by
f on w (for some G).
However, it is clear that we are interested in strate-
gies that are time-optimal and size-optimal strategies
are not necessarily also time-optimal: the time taken
at filtering-time to get a smaller grammar will not
necessarily be won back at parse-time.
For a given CFG G, an input DAG w and a filter-
ing strategy c, we only have to plot the times taken
by the filtering pass and by the parsing pass to make
some estimations on their average (median, decile)
parse times and then to decide which is the winner.
However, it may well happens that a strategy which
has not received the award (with the sample of CFGs
and the test sets tried) would be the winner in an-
other context!
All the following filtering strategies exhibit nec-
essary conditions that any production must hold in
order to be in a parse.
3.2 The make-a-reduced-grammar Algorithm
An algorithm which takes as input any CFG
G = (N,T, P, S) and generates as output a
strongly equivalent reduced CFG G? and which
runs in O(|G|) can be found in many text books
(See (Hopcroft and Ullman, 1979) for example).
So as to eliminate from all our intermediate sub-
grammars all useless productions, each filtering
strategy will end by a call to such an algorithm
named make-a-reduced-grammar.
97
The make-a-reduced-grammar algorithm works
as follows. It first finds all productive9 symbols. Af-
terwards it finds all reachable10 symbols. A symbol
is useful (otherwise useless) if it is both productive
and reachable. A production A? X1 ? ? ?Xp is use-
ful (otherwise useless) iff all its symbols are useful.
A last scan over the grammar erases all useless pro-
duction and leaves the reduced form. The canonical
form is reached in only retaining in the nonterminal
and terminal sets of the sub-grammar the symbols
which occur in the (useful) production set.
3.3 Basic Filtering Strategy: b-filter
The basic filtering strategy (b-filter for short) which
is described in this section will always be tried the
first. Thus, its input is the couple (G,w) where
G = (N,T, P, S) is the large initial CFG and the in-
put sentence w is a reduced DAG in canonical form
w = (?, ?, f) of size n. It generates a reduced CFG
in canonical form noted Gb = (P b, S) in which the
references to both G and w are assumed. Besides
this b-filter, we will examine in Sections 3.4 and 3.5
two others filtering strategies named a and d. These
filters will always have as input a couple (Gc, w)
where Gc = (P c, S) is a reduced CFG in canonical
form which has already been filtered by a previous
sequence of strategies noted c. They generate a re-
duced CFG in canonical form noted Gcf = (P cf , S)
with f = a or f = d respectively. Of course it may
happens that Gcf is identical to Gc if the f -filter is
not effective. A filtering strategy or a combination of
filtering strategies may be applied several times and
lead to a filtered grammar of the form say Gba2da
in which the sequence ba2da explicits the order in
which the filtering strategies have been performed.
We may even repeatedly apply a until a fixed point
is reached before applying d, and thus get something
of the form Gba?d.
The idea behind the b-filter is very simple and has
largely been used in lexicalized formalisms parsing,
in particular in LTAG (Schabes et al, 1988) parsing.
The filter rejects productions of P which contain ter-
minal symbols that do not occur in ? (i.e., that are
not terminal symbols of the DAG w) and thus takes
9X ? V is productive iff we have X ??
G
w,w ? T ?.
10X ? V is reachable iff we have S ??
G
w1Xw2, w1w2 ?
T ?.
S ? AB (1)
S ? BA (2)
A ? a (3)
A ? ab (4)
B ? b (5)
B ? bc (6)
Table 1: A simple grammar
O(|G|) time if we assume that the access to the ele-
ments of the terminal set ? is performed in constant
time. Unlexicalized productions whose right-hand
sides are in N? are kept. It also rejects productions
in which several terminal symbol occurs, in an order
which is not compatible with the linear order of the
input.
Consider for example the set of productions
shown in Table 1 and assume that the source text
is the terminal string ab. It is clear that the b-filter
will erase production 6 since c is not in the source
text.
The execution of the b-filter produces a (non-
reduced) CFG G? such that |G?| ? |G|. However, it
may be the case that some productions of G? are use-
less, it will thus be the task of the make-a-reduced-
grammar algorithm to transform G? into its reduced
canonical form Gb in time O(|G?|). The worst-case
total running time of the whole b-filter pass is thus
O(|G| ? n).
We can remark that, after the execution of the b-
filter, the set of terminal symbols of Gb is a subset
of T ? ?.
3.4 Adjacent Filtering Strategy: a-filter
As explained before, we assume that the input to
the adjacent filtering strategy (a-filter for short) de-
scribed in this section is a couple (Gc, w) where
Gc = (N c, T c, P c, S) is a reduced CFG in canon-
ical form. However, the a-filter would also work
for a non-reduced CFG. As usual, we define the
symbols of Gc as the elements of the vocabulary
V c = N c ? T c.
The idea is to erase productions that cannot be
part of any parses for w in using an adjacency crite-
ria: if two symbols are adjacent in a rule, they must
98
derive terminal symbols that are also adjacent in w.
To give a (very) simple practical idea of what we
mean by adjacency criteria, let us consider again the
source string ab and the grammar defined in Table 1
in which the last production has already been erased
by the b-filter.
The fact that the B-production ends with a b and
that the A-productions all start with an a, implies
that production 2 is in a complete parse only if the
source text is such that b is immediately followed
by a. Since it is not the case, production 2 can be
erased.
More generally, consider a production of the form
A ? ? ? ?XY ? ? ? . If for each couple (a, b) ? T 2 in
which a is a terminal symbol that can terminate (the
terminal strings generated by) X and b is a terminal
symbol that can lead (the terminal strings generated
by) Y , there is no transition on b that can follow a
transition on a in the DAG w, it is clear that the pro-
duction A? ? ? ?XY ? ? ? can be safely erased.
Now assume that we have the following (left)
derivation Y ?? Y1?1 ?? Yi?i ? ? ? ?1 ??
? ? ? Yp?1??pYp?p? ?pYp?p ? ? ? ?1 ?? Yp?p ? ? ? ?1,
with ?p ?? ?. If for each couple (a, b?) in which
a has the previous definition and b? is a terminal
symbol that can lead (the terminal strings gener-
ated by) Yp, there is no transition on b? that can fol-
low a transition on a in the DAG w, the production
Yp?1 ? ?pYp?p can be erased if it is not valid in
another context.
Moreover, consider a (right) derivation of the
form X ?? ?1X1 ?? ?1 ? ? ??iXi ??
? ? ? Xp?1??pXp?p? ?1 ? ? ??pXp?p ?? ?1 ? ? ??pXp,
with ?p ?? ?. If for each couple (a?, b) in which b
has the previous definition and a? is a terminal sym-
bol that can terminate (the terminal strings gener-
ated by) Xp, there is no transition on b that can fol-
low a transition on a? in the DAG w, the production
Xp?1 ? ?pXp?p can be erased if it is not valid in
another context.
In order to formalize these notions we define sev-
eral binary relations together with their (reflexive)
transitive closure.
Within a CFG G = (N,T, P, S), we first define
left-corner noted x. Left-corner (Nederhof, 1993;
Moore, 2000), hereafter LC, is a well-known rela-
tion since many parsing strategies are based upon it.
We say that X is in the LC of A and we write A x X
iff (A,X) ? {(B,Y ) | B ? ?Y ? ? P ? ? ??
G
?}.
We can write A x
A??X?
X to enforce how the cou-
ple (A,X) may be produced.
For its dual relation, right-corner, noted y, we say
that X is in the right corner of A and we write X y A
iff (X,A) ? {(Y,B) | B ? ?Y ? ? P ? ? ??
G
?}. We can write X y
A??X?
A to enforce how the
couple (X,A) may be produced.
We also define the first (resp. last) relation noted
??t (resp. ?? t) by ??t= {(X, t) | X ? V ? t ?
T ?X ??
G
tx ? x ? T ?} (resp. ?? t= {(X, t) | X ?
V ? t ? T ?X ??
G
xt ? x ? T ?}).
We define the adjacent ternary relation on V ?
N? ? V noted ? and we write X ?? Y iff
(X,?, Y ) ? {(U, ?, V ) | A? ?U?V ? ? P ?? ??
G
?}. This means that X and Y occur in that order in
the right-hand side of some production and are sep-
arated by a nullable string ?. Note that X or Y may
or may not be nullable.
On the input DAG w = (?, ?, f), we define the
immediately precede relation noted < and we write
a < b for a, b ? ? iff w1abw3 ? L(w), w1, w3 ?
??.
We also define the precede relation noted ? and
we write a ? b for a, b ? ? iff w1aw2bw3 ?
L(w), w1, w2, w3 ? ??.We can note that ? is not
the transitive closure of <.11
For each production A ? ?X0X1 ? ? ?Xp?1Xp?
in P c and for each symbol pairs (X0,Xp) of non-
nullable symbols s.t. X1 ? ? ?Xp?1 ??Gc ?, we com-
pute two sets A1 and A2 of couples (a, b), a, b ? T c
defined by A1 = ?0<i?p = {(a, b) | a ?? t
X0
X1???Xi?1? Xi ??t b} and A2 = ?0?i<p =
{(a, b) | a ?? t Xi
Xi+1???Xp?1? Xp ??t b}. Any
11Consider the source string bcab for which we have a
+
< c,
but not a ? c.
99
pair (a, b) of A1 is such that the terminal symbol
a may terminate a phrase of X0 while the terminal
symbol b may lead a phrase of X1 ? ? ?Xp. Since
X0 and Xp are not nullable, A1 is not empty. If
none of its elements (a, b) is such that a < b, the
production A ? ?X0X1 ? ? ?Xp?1Xp? is useless
and can be erased. Analogously, any pair (a, b) of
A2 is such that the terminal symbol a may termi-
nate a phrase of X0X1 ? ? ?Xp?1 while the terminal
symbol b may lead a phrase of Xp. Since X0 and
Xp are not nullable, A2 is not empty. If none of
its elements (a, b) is such that a < b, the produc-
tion A ? ?X0X1 ? ? ?Xp?1Xp? is useless and can
be erased. Of course if X1 ? ? ?Xp?1 = ?, we have
A1 = A2.12
The previous method has checked some adjacent
properties inside the right-hand sides of productions.
The following will perform some analogous checks
but at the beginning and at the end of the right-hand
sides of productions.
Let us go back to Table 1 to illustrate our pur-
pose. Recall that, with source text ab, productions 6
and 2 have already been erased. Consider produc-
tion 4 whose left-hand side is an A, the terminal
string ab that it generates ends by b. If we look for
the occurrences of A in the right-hand sides of the
(remaining) productions, we only find production 1
which indicates that A is followed by B. Since the
phrases of B all start with b (See production 5) and
since in the source text b does not immediately fol-
low another b, production 4 can be erased.
In order to check that the input sentence w starts
and ends by valid terminal symbols, we augment
the adjacent relation with two elements ($, ?, S) and
(S, ?, $) where $ is a new terminal symbol which is
supposed to start and to end every sentence.13
Let Z ? ?U? be a production in P c in which U
is non-nullable and ? ??
Gc
?. If X is a non-nullable
symbol, we compute the set L = {(a, b) | a ?? t
X ?? Y ?x Z x
Z??U?
U ??t b}. Since Gc is reduced
and since $ < S, we are sure that the set X ?? Y ?x
12It can be shown that the previous check can be performed
on (Gc, w) in worst-case timeO(|Gc|?|?|3) (recall that |?| ?
n). This time reduces to O(|Gc| ? |?|2) if the input sentence
is not a DAG but a string.
13This is equivalent to assume the existence in the grammar
of a super-production whose right-hand side has the form $S$.
Z is non-empty, thus L is also non-empty.14
We can associate with each couple (a, b) ?
L at least one (left) derivation of the form
X?Y ??
Gc
w0aw1?Y ??Gc w0aw1w2Y
??
Gc
w0aw1w2w3Z?2
Z??U??
Gc
w0aw1w2w3?U??2 ??Gc
w0aw1w2w3w4U??2 ??Gc w0aw1w2w3w4w5b?1??2
in which w1w2w3w4w5 ? T c?. These derivations
contains all possible usages of the production Z ?
?U? in a parse. If for every couple (a, b) ? L, the
statement a? b does not hold, we can conclude that
the production Z ? ?U? is not used in any parse
and can thus be deleted.
Analogously, we can check that the order of ter-
minal symbols is compatible with both a production
and its right grammatical context.
Let Z ? ?U? be a production in P c in which U
is non-nullable and ? ??
Gc
?. If Y is a non-nullable
symbol, we compute the set R = {(a, b) | a ?? t
U y
Z??U?
Z ?y X ?? Y ??t b}. Since Gc is reduced
and since S < $, we are sure that the set Z ?y X ??
Y is non-empty, thus R is also non-empty.14
To each couple (a, b) ? R we can asso-
ciate at least one (right) derivation of the form
X?Y ??
Gc
X?w1bw0 ??Gc Xw2w1bw0
??
Gc
?1Zw3w2w1bw0
Z??U??
Gc
?1?U?w3w2w1bw0 ??Gc
?1?Uw4w3w2w1bw0 ??Gc ?1??2aw5w4w3w2w1bw0
in which w5w4w3w2w1 ? T c?. These deriva-
tions contains all possible usages of the production
Z ? ?U? in a partial parse. If for every couple
(a, b) ? L, the statement a ? b does not hold, we
can conclude that the production Z ? ?U? is not
used in any parse and can thus be deleted.
Now, a call to the make-a-reduced-grammar al-
gorithm produces a reduced CFG in canonical form
named Gca = (N ca, T ca, P ca, S).
14This statement does not hold any more if we exclude from
P c the productions that have been previously erased during the
current a-filter. In that case, an empty set indicates that the
production Z ? ?U? can be erased.
100
3.5 Dynamic Set Automaton Filtering
Strategy: d-filter
In (Boullier, 2003) the author has presented a
method that takes a CFG G and computes a FSA
that defines a regular superset of L(G). However his
method would produce intractable gigantic FSAs.
Thus he uses his method to dynamically compute
the FSA at parse time on a given source text. Based
on experimental results, he shows that his method
called dynamic set automaton (DSA) is tractable.
He uses it to guide an Earley parser (See (Ear-
ley, 1970)) and shows improvements over the non
guided version. The DSA method can directly be
used as a filtering strategy since the states of the un-
derlying FSA are in fact sets of items. For a CFG
G = (N,T, P, S), an item (or dotted production)
is an element of {[A ? ?.?] | A ? ?? ? P}.
A complete item has the form [A ? ?.], it indi-
cates that the production A ? ? has been, in some
sense, recognized. Thus, the complete items of the
DSA states gives the set of productions selected by
the DSA. This selection can be further refined if we
also use the mirror DSA which processes the source
text from right to left and if we only select complete
items that both belong to the DSA and to its mirror.
Thus, if we assume that the input to the DSA fil-
tering strategy (d-filter) is a couple (Gc, w) where
Gc = (P c, S) is a reduced CFG in canonical form,
we will eventually get a set of productions which is
a subset of P c. If it is a strict subset, we then ap-
ply the make-a-reduced-grammar algorithm which
produces a reduced CFG in canonical form named
Gcd = (P cd, S).
The Section 4 will give measures that may help to
compare the practical merits of the a and d-filtering
strategies.
4 Experiments
The measures presented in this section have been
taken on a 1.7GHz AMD Athlon PC with 1.5 Gb
of RAM running Linux. All parsers are written in C
and have been compiled with gcc 2.96 with the O2
optimization flag.
4.1 Grammars and corpus
We have performed experiments with two large
grammars described below. The first one is an auto-
matically generated CFG, the other one is the CFG
equivalent of a TIG automatically extracted from a
factorized TAG.
The first grammar, named GT>N , is a variant of
the CFG backbone of a large-coverage LFG gram-
mar for French used in the French LFG parser de-
scribed in (Boullier and Sagot, 2005). In this vari-
ant, the set T of terminal symbols is the whole set of
French inflected forms present in the Lefff , a large-
coverage syntactic lexicon for French (Sagot et al,
2006). This leads to as many as 407,863 different
terminal symbols and 520,711 lexicalized produc-
tions (hence, the average number of categories ?
which are here non-terminal symbols ? for an in-
flected form is 1.27). Moreover, this CFG entails
a non-neglectible amount of syntactic constraints
(including over-generating sub-categorization frame
checking), which implies as many as |Pu| = 19, 028
non-lexicalized productions. All in all, GT>N has
539,739 productions.
The second grammar, named GTIG, is a CFG
which represents a TIG. To achieve this, we applied
(Boullier, 2000)?s algorithm on the unfolded version
of (Villemonte de La Clergerie, 2005)?s factorized
TAG. The number of productions in GTIG is com-
parable to that of GT>N . However, these two gram-
mars are completely different. First, GTIG has much
less terminal and non-terminal symbols than GT>N .
This means that the basic filter may be less efficient
on GTIG than on GT>N . Second, the size of GTIG
is enormous (more than 10 times that of GT>N ),
which shows that right-hand sides of GTIG?s pro-
ductions are huge (the average number of right-hand
side symbols is more than 24). This may increase
the usefulness of a- and d-filtering strategies.
Global quantitative data about these grammars is
shown in Table 2.
Both grammars, as evoked in the introduction,
have not been written by hand. On the contrary, they
are automatically generated from a more abstract
and more compact level (a meta-level over LFG for
GT>N , and a metagrammar for GTIG). These gram-
mars are not artificial grammars set up only for this
experiment. On the contrary, they are automatically
generated huge real-life CFGs that are variants of
grammars used in real NLP applications.
Our test suite is a set of 3093 French journalistic
sentences. These sentences are the general lemonde
101
G |N | |T | |P | |Pu| |G|
GT>N 7,862 407,863 539,739 19,028 1,123,062
GTIG 448 173 493,408 4,338 12,455,767
Table 2: Sizes of the grammars GT>N and GTIG
used in our experiments
part of the EASy parsing evaluation campaign cor-
pus. Raw sentences have been turned into DAGs
of inflected forms known by both grammar/lexicon
couples.15 This step has been achieved by the pre-
syntactic processing chain SXPipe (Sagot and Boul-
lier, 2005). They are all recognized by both gram-
mars.16 The resulting DAGs have a median size of
28 and an average size of 31.7.
Before entering into details, let us give here the
first important result of these experiments: it was
actually possible to build parsers out of GT>N and
GTIG and to parse efficiently with the resulting
parsers (we shall detail later on efficiency results).
Given the fact that we are dealing with grammars
whose sizes are respectively over 1,000,000 and over
12,000,000, this is in itself a very satisfying result.
4.2 Precision results
Let us recall informally that the precision of a filter-
ing strategy is the proportion of productions in the
resulting sub-grammar that are in the gold grammar,
i.e., that have effectively instantiated counterparts in
the final parse forest.
We have applied different strategies so as to com-
pare their precisions. The results on GT>N and
GTIG are summed up in Table 3. These results give
several valuable results. First, as we expected, the
basic b-filter drastically reduces the size of the gram-
mar. The result is even better on GT>N thanks to its
large number of terminal symbols. Second, both the
adjacency a-filter and the DSA d-filter efficiently re-
duce the size of the grammar: on GT>N , the a-filter
eliminates 20% of the productions they receive as
input, a bit less for the d-filter. Indeed, the a-filter
performs better than the d-filter introduced in (Boul-
15As seen above, inflected forms are directly terminal sym-
bols of GT>N , while GTIG uses a lexicon to map these in-
flected forms into its own terminal symbols, thereby possibly
introducing lexical ambiguity.
16Approx. 15% of the original set of sentences were not rec-
ognized, and required error recovery techniques; we decided to
discard them for this experiment.
Strategy Average precision
GT>N GTIG
no filter 0.04% 0.03%
b 62.87% 39.43%
bd 74.53% 66.56%
ba 77.31% 66.94%
ba? 77.48% 67.48%
bad 80.27% 77.16%
ba?d 80.30% 77.41%
gold 100% 100%
Table 3: Average precision of six different filtering
strategies on our test corpus with GT>N and GTIG.
lier, 2003), at least as precision is concerned. We
shall see later that this is still the case on global
parsing times. However, applying the d-filter after
the a-filter still removes a non-neglectible amount
of productions:17 each technique is able to eliminate
productions that are kept by the other one. The result
of these filters is suprisingly good: in average, after
all filters, only approx. 20% of the productions that
have been kept will not be successfully instantiated
in the final parse forest. Third, the adjacency filter
can be used in its one-pass mode, since almost all
the benefit from the full (fix-point) mode is already
reached after the first application. This is practically
a very valuable result, since the one-pass mode is
obviously faster than the full mode.
However, all these filters do require computing
time, and it is necessary to evaluate not only the pre-
cision of these filters, but also their execution time
as well as the influence they have on the global (in-
cluding filtering) parsing time .
4.3 Parsing time and best filter
Filter execution times for the six filtering strategies
introduced in Table 3 are illustrated for GT>N in
Figure 1. These graphics show three extremely valu-
able pieces of information. First, filtering times are
extremely low: the average filtering time for the
slowest filter (ba?d, i.e., basic plus full adjacency
plus DSA) on 40-word sentences is around 20 ms.
Second, on small sentences, filtering times are virtu-
ally zero. This is important, since it means that there
17Although not reported here, applying the a before d leads
to the same conclusion.
102
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Fi
lte
r e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Median filtering time
Filtering time at percentile rank 90
Filtering time at percentile rank 10
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Fi
lte
r e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Median filtering time
Filtering time at percentile rank 90
Filtering time at percentile rank 10
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Fi
lte
r e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Median filtering time
Filtering time at percentile rank 90
Filtering time at percentile rank 10
b-filter bd-filter ba-filter
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Fi
lte
r e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Median filtering time
Filtering time at percentile rank 90
Filtering time at percentile rank 10
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Fi
lte
r e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Median filtering time
Filtering time at percentile rank 90
Filtering time at percentile rank 10
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Fi
lte
r e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Median filtering time
Filtering time at percentile rank 90
Filtering time at percentile rank 10
ba?-filter bad-filter ba?d-filter
Figure 1: Filtering times for six different strategies with GT>N
is almost no fixed cost to pay when we use these
filters (let us recall that without any filter, building
efficient parsers for such a huge grammar is highly
problematic). Third, all these filters, at least when
used with GT>N , are executed in a time which is
linear w.r.t. the size of the input sentence (i.e., the
size of the input DAG).
The results on GTIG lead to the same conclusions,
with one exception: with this extremely huge gram-
mar with so long right-hand sides, the basic filter
is not as fast as on GT>N (and not as precise, as
we will see below, which slows down the make-a-
reduced-grammar algorithm since it is applied on
a larger filtered grammars). For example, the me-
dian execution time for the basic filter on sentences
whose size is approximately 40 is 0.25 seconds,
to be compared with the 0.00 seconds reached on
GT>N (this zero value means a median time strictly
lower than 0.01 seconds, which is the granularity of
our time measurments).
Figure 2 and 3 show the global (filtering+parsing)
execution time for the 6 different filters. We only
show median times computed on classes of sen-
tences of length 10i to 10(i + 1) ? 1 and plotted
with a centered x-coordinate (10(i + 1/2)), but re-
sults with other percentiles or average times on the
same classes draw the same overall picture.
 0
 0.05
 0.1
 0.15
 0.2
 0  20  40  60  80  100
Av
er
ag
e 
gl
ob
al
 e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Basic filter only
DSA filter
One-pass adjacency filter
Full adjacency filter
One-pass adjacency filter and DSA filter
Full adjacency filter and DSA filter
Figure 2: Global (filtering+parsing) times for six
different strategies with GT>N
One can see that the results are completely differ-
ent, showing a strong dependency on the character-
istics of the grammar. In the case of GT>N , the huge
number of terminal symbols and the reasonable av-
erage size of right-hand sides of productions, the ba-
sic filtering strategy is the best strategy: although it
is fast because relatively simple, it reduces the gram-
mar extremely efficiently (it has a 60.56% precision,
to be compared with the precision of the void filter
which is 0.04%). Hence, for GT>N , our only result
103
 0
 0.5
 1
 1.5
 2
 0  20  40  60  80  100
Av
er
ag
e 
gl
ob
al
 e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Basic filter only
DSA filter
One-pass adjacency filter
Full adjacency filter
One-pass adjacency filter and DSA filter
Full adjacency filter and DSA filter
Figure 3: Global (filtering+parsing) times for six
different strategies with GTIG
is that this basic filter does allow us to build an effi-
cient parser (the most efficient one), but that refined
additionnal filtering strategies are not useful.
The picture is completely different with GTIG.
Contrary to GT>N , this grammar has comparatively
very few terminal and non-terminal symbols, and
very long right-hand sides. These two facts lead
to a lower precision of the basic filter (39.43%),
which keeps many more productions when applied
on GTIG than when applied on GT>N , and leads,
when applied alone, to the less efficient parser. This
gives to the adjacency filter much more opportunity
to improve the global execution time. However, the
complexity of the grammar makes the construction
of the DSA filter relatively costly despite its preci-
sion, leading to the following conclusion: on GTIG
(and probably on any grammar with similar charac-
teristics), the best filtering strategy is the one-pass
adjacency strategy. In particular, this leads to an im-
provement over the work of (Boullier, 2003) which
only introduced the DSA filter. Incidentally, the
extreme size of GTIG leads to much higher pars-
ing times, approximately 10 times higher than with
GT>N , which is consistent with the ratio between
the sizes of both involved grammars.
5 Conclusion
It is a well known result in optimization techniques
that the key to practically improve these processes is
to reduce their search space. This is also the case in
parsing and in particular in CF parsing.
Many parsers process their inputs from left to
right but we can find in the literature other parsing
strategies. In particular, in NLP, (van Noord, 1997)
and (Satta and Stock, 1994) propose bidirectional al-
gorithms. These parsers have the reputation to have
a better efficiency than their left-to-right counterpart.
This reputation is not only based upon experimental
results (van Noord, 1997) but also upon mathemat-
ical arguments in (Nederhof and Satta, 2000). This
is specially true when the productions of the CFG
strongly depend on lexical information. In that case
the parsing search space is reduced because the con-
straints associated to lexical elements are evaluated
as early as possible. We can note that our filtering
strategies try to reach the same purpose by a totally
different mean: we reduce the parsing search space
by eliminating as many productions as possible, in-
cluding possibly non-lexicalized productions whose
irrelevance to parse the current input can not be di-
rectly deduced from that input.
We can also remark that our results are not in con-
tradiction with the claims of (Nederhof and Satta,
2000) in which they argue that ?Earley algorithm
and related standard parsing techniques [. . . ] can-
not be directly extended to allow left-to-right and
correct-prefix-property parsing in acceptable time
bound?. First, as already noted in Section 1, our
method does not work for any large CFG. In order
to work well, the first step of our basic strategy must
filter out a great amount of (lexicalized) productions.
To do that, it is clear that the set of terminals in the
input text must select a small ratio of lexicalized pro-
ductions. To give a more concrete idea we advo-
cate that the selected productions produce roughly a
grammar of normal size out of the large grammar.
Second, our method as a whole clearly does not pro-
cess the input text from left-to-right and thus does
not enter in the categories studied in (Nederhof and
Satta, 2000). Moreover, the authors bring strong evi-
dences that in case of polynomial-time off-line com-
pilation of the grammar, left-to-right parsing cannot
be performed in polynomial time, independently of
the size of the lexicon. Once again, if our filter pass
is viewed as an off-line processing of the large input
grammar, our output is not a compilation of the large
grammar, but a (compilation of a) smaller grammar,
specialized in (some abstractions of) the source text
only. In other words their negative results do not
104
necessarily apply to our specific case.
The experiment campaign as been conducted in
using an Earley-like parser.18 We have also success-
fuly tried the coupling of our filtering strategies with
a CYK parser (Kasami, 1967; Younger, 1967) as
post-processor. However the coupling with a GLR
parser (See (Satta, 1992) for example) is perhaps
more problematic since the time taken to build up
the underlying nondeterministic LR automaton from
the sub-grammar can be prohibitive.
Though no definitive answer can be made to the
question asked in the title, we have shown that, in
some cases, the answer is certainly yes.
References
Sylvie Billot and Bernard Lang. 1989. The structure of
shared forests in ambiguous parsing. In Meeting of
the Association for Computational Linguistics, pages
143?151.
Pierre Boullier and Beno??t Sagot. 2005. Efficient and ro-
bust LFG parsing: SxLfg. In Proceedings of IWPT?05,
pages 1?10, Vancouver, Canada.
Pierre Boullier. 2000. On TAG parsing. Traitement Au-
tomatique des Langues (T.A.L.), 41(3):759?793.
Pierre Boullier. 2003. Guided Earley parsing. In Pro-
ceedings of IWPT 03, pages 43?54, Nancy, France.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communication of the ACM, 13(2):94?102.
Jeffrey D. Hopcroft and John E. Ullman. 1979. Intro-
duction to Automata Theory, Languages, and Compu-
tation. Addison-Wesley, Reading, Mass.
Aravind Joshi. 1997. Parsing techniques. In Sur-
vey of the state of the art in human language tech-
nology, pages 351?356. Cambridge University Press,
New York, NY, USA.
Tadao Kasami. 1967. An efficient recognition and syntax
algorithm for context-free languages. Scientific Re-
port AFCRL-65?758, Air Force Cambridge Research
Laboratory, Bedford, Massachusetts, USA.
Robert C. Moore. 2000. Improved left-corner
chart parsing for large context-free gram-
mars. In Proceedings of IWPT 2000, pages
18Contrarily to classical Earley parsers, its predictor phase
uses a pre-computed structure which is roughly an LC relation.
Note that this feature forces our filters to compute an LC rela-
tion on the generated sub-grammar. This also shows that LC
parsers may also benefit from our filtering techniques.
171?182, Trento, Italy. Revised version at
http://www.cogs.susx.ac.uk/lab/nlp/
carroll/cfg-resources/iwpt2000-rev2.ps.
Mark-Jan Nederhof and Giorgio Satta. 2000. Left-to-
right parsing and bilexical context-free grammars. In
Proceedings of the first conference on North American
chapter of the ACL, pages 272?279, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Mark-Jan Nederhof. 1993. Generalized left-corner pars-
ing. In Proceedings of the sixth conference on Euro-
pean chapter of the ACL, pages 305?314, Morristown,
NJ, USA. ACL.
Beno??t Sagot and Pierre Boullier. 2005. From raw cor-
pus to word lattices: robust pre-parsing processing. In
Proceedings of L&TC 2005, pages 348?351, Poznan?,
Poland.
Beno??t Sagot, Lionel Cle?ment, ?Eric Villemonte de La
Clergerie, and Pierre Boullier. 2006. The Lefff 2 syn-
tactic lexicon for french: architecture, acquisition, use.
In Proc. of LREC?06.
Giorgio Satta and Oliviero Stock. 1994. Bidirectional
context-free grammar parsing for natural language
processing. Artif. Intell., 69(1-2):123?164.
Giorgio Satta. 1992. Review of ?generalized lr parsing?
by masaru tomita. kluwer academic publishers 1991.
Comput. Linguist., 18(3):377?381.
Yves Schabes and Richard C. Waters. 1995. Tree in-
sertion grammar: Cubic-time, parsable formalism that
lexicalizes context-free grammar without changing the
trees produced. Comput. Linguist., 21(4):479?513.
Yves Schabes, Anne Abeille?, and Aravind K. Joshi.
1988. Parsing strategies with ?lexicalized? grammars:
Application to tree adjoining grammars. In Proceed-
ings of the 12th International Conference on Comput.
Linguist. (COLING?88), Budapest, Hungary.
Gertjan van Noord. 1997. An efficient implementation of
the head-corner parser. Comput. Linguist., 23(3):425?
456.
?Eric Villemonte de La Clergerie. 2005. From metagram-
mars to factorized TAG/TIG parsers. In Proceedings
of IWPT?05, pages 190?191, Vancouver, Canada.
Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189?208.
105
