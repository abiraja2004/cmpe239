Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 93?96,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Hybrid Approach for Coreference Resolution 
 
 
First Author: Sobha, Lalitha Devi., Pattabhi, RK Rao., Vijay Sundar Ram, R. 
Second Author: Malarkodi, CS., Akilandeswari, A. 
AU-KBC Research Centre, 
MIT Campus of Anna University, 
Chrompet, Chennai, India. 
sobha@au-kbc.org 
 
 
 
 
 
 
Abstract 
This paper describes our participation in 
the CoNLL-2011 shared task for closed 
task. The approach used combines refined 
salience measure based pronominal 
resolution and CRFs for non-pronominal 
resolution. In this work we also use 
machine learning based approach for 
identifying non-anaphoric pronouns. 
1 Introduction 
In this paper we describe our system, used in the 
CoNLL-2011 shared task ?Modeling Unrestricted 
Coreference in OntoNotes?. The goal of this task is 
to identify coreference chains in a document. The 
coreference chains can include names, nominal 
mentions, pronouns, verbs that are coreferenced 
with a noun phrases.  
The coreferents are classified into two types, 
pronominal and non-pronominal referents. We use 
two different approaches using machine learning 
and salience factor in the resolution of the above 
two types. Pronominal resolution is done using 
salience factors and Non-Pronominals using 
machine learning approach. Pronominal resolution 
refers to identification of a Noun phrase (NP) that 
is referred by a pronominal and Non-Pronominals 
are NP referring to another NP. In the next section 
we describe the system in detail. 
2 System Description 
In this section we give a detailed description of our 
system. The task is divided into two sub-tasks. 
They are 
    i) Pronominal resolution 
   ii) Non-pronominal resolution 
2.1 Pronominal Resolution 
Here we have identified salience factors and 
assigned weights for each factor.  Before resolving 
the pronouns we identify whether a given pronoun 
is anaphoric or not. In example, (1) below, the 
pronoun ?It?, does not refer to any entity, and it is 
a pleonastic ?it?. 
(1) ?It will rain today? 
In identifying the non-anaphoric pronouns such 
as ?it? we use a CRFs engine, a machine learning 
approach. We build a language model using the 
above ML method to identify the non-anaphoric 
pronouns and the features used in training are word 
and it?s POS in a window of five (two preceding 
and two following words to the pronoun). After the 
non-anaphoric pronoun identification, we resolve 
the anaphoric pronouns using a pronominal 
resolution system. Though we use salience factors 
based on the Lappin and Leass (1994), we have 
substantially deviated from the basic algorithm and 
have also used factors from Sobha (2008), where 
named entity and ontology are considered for 
resolution. 
For identifying an antecedent for a pronoun we 
consider all the noun phrases before the pronoun in 
93
the current sentence and in the four sentences 
preceding the current sentence. Those noun 
phrases which agree in PNG with the pronoun are 
considered as the possible candidates. The PNG is 
obtained using the gender data work of Shane 
Bergsma and Dekang Lin (2006). The possible 
candidates are scored based on the salience factors 
and ranked. The salience factors considered here 
are presented in the table 1. 
 
Salience Factors Weights 
Current Sentence 
(sentence in which 
pronoun occurs) 
100 
For the preceding 
sentences up to four 
sentences from the 
current sentence 
Reduce sentence score 
by 10 
Current Clause 
(clause in which 
pronoun occurs) 
100 ? for possessive 
pronoun 
50 ? for non-possessive 
pronouns  
Immediate Clause 
(clause preceding or 
following the current 
clause) 
50 ? for possessive  
pronoun 
100 ? for non-
possessive pronouns 
Non-immediate 
Clause (neither the 
current or immediate 
clause) 
50 
Possessive NP 65 
Existential NP 70 
Subject 80 
Direct Object 50 
Indirect Object 40 
Compliment of PP 30 
  
Table 1: Salience Factors and weights 
 
Improving pronominal resolution Using Name 
Entity (NE) and WordNet: Pronouns such as 
?He?, ?She?, ?I? and ?You? can take antecedents 
which are animate and particularly having the NE 
tag PERSON. Similarly the pronoun ?It? can never 
take an animate as the antecedent. From the 
WordNet we obtain the information of noun 
category such as ?person?, ?object?, ?artifact?, 
?location? etc. Using the NE information provided 
in the document and the category information in 
WordNet, the irrelevant candidates are filtered out 
from the possible candidates. Thus the antecedent 
and pronoun category agrees. 
The highest ranked candidate is considered as 
the antecedent for the particular pronoun. 
In TC and BC genres, the pronouns ?I? and 
?you? refer to the speakers involved in the 
conversation. For these pronouns we identify the 
antecedent using heuristic rules making use of the 
speaker information provided. 
2.2 Non-pronominal Coreference resolution 
In identifying the Non-pronominal as said earlier, 
we have used a CRFs based machine learning 
approach. CRFs are well known for label 
sequencing tasks such as Chunking, Named Entity 
tagging (Lafferty et al 2001; Taku Kudo 2005). 
Here we have CRFs for classification task, by 
using only the current state features and not the 
features related to state transition. The features 
used for training are based on Soon et al(2001). 
We have changed the method of deriving, values 
of the features such as String match, alias, from the 
Soon el al method and found that our method is 
giving more result.  The features used in our work 
are as follows. 
a) Distance feature ? same as in Soon et al
b) Definite NP - same as in Soon et al
c) Demonstrative NP ? same as in Soon et al
d) String match ? (Not as Soon et althe possible 
values are between 0 and 1. This is calculated as 
ratio of the number of words matched between the 
NPs and the total number of words of the anaphor 
NP. Here we consider the NP on the left side as 
antecedent NP and NP on the right side as anaphor 
NP. 
e) Number Agreement ? We use the gender data 
file (Bergsma and Lin, 2006) and also the POS 
information 
f) Gender agreement ? We use the gender data 
file (Bergsma and Lin, 2006) 
g) Alias feature ? (Not as in Soon et al the alias 
feature takes the value 0 or 1. This is obtained 
using three methods, 
     i) Comparing the head of the NPs, if both are 
same then scored as 1 
     ii) If both the NPs start with NNP or NNPS 
POS tags, and if they are same then scored as 1 
     iii) Looks for Acronym match, if one is an 
acronym of other it is scored as 1 
h) Both proper NPs ? same as Soon et al  
i )  NE tag information. 
94
The semantic class information (noun category) 
obtained from the WordNet is used for the filtering 
purpose. The pairs which do not have semantic 
feature match are filtered out. We have not used 
the appositive feature described in Soon et al
(2001), since we are not considering appositives 
for the coreference chains.  
The feature template for CRF is defined in such 
a way that more importance is given to the features 
such as the string match, gender agreement and 
alias feature. The data for training is prepared by 
taking all NPs between an anaphor and antecedent 
as negative NPs and the antecedent and anaphor as 
positive NP. 
The core CRFs engine for Non-pronominal 
resolution system identifies the coreferring pairs of 
NPs. The Coreferring pairs obtained from 
pronominal resolution system and Non-pronominal 
system are merged to generate the complete 
coreference chains. The merging is done as 
follows: A member of a coreference pair is 
compared with all the members of the coreference 
pairs identified and if it occurs in anyone of the 
pair, then the two pairs are grouped.  This process 
is done for all the members of the identified pairs 
and the members in each group are aligned based 
on their position in the document to form the chain. 
3 Evaluation   
In this section we present the evaluation of the 
complete system, which was developed under the 
closed task, along with the independent evaluation 
of the two sub-modules. 
a) Non-anaphoric detection modules 
b) Pronominal resolution module 
The data used for training as well as testing was 
provided CoNLL-2001 shared task (Pradhan et al, 
2011), (Pradhan et al, 2007) organizers. The 
results shown in this paper were obtained for the 
development data. 
The non-anaphoric pronoun detection module is 
trained using the training data. This module was 
evaluated using the 91files development data. The 
training data contained 1326 non-anaphoric 
pronouns. The development data used for 
evaluation had 160 non-anaphoric pronouns. The 
table 2 shows the evaluation, of the non-anaphoric 
pronoun detection module. 
The Pronominal resolution module was also 
evaluated on the development data. The filtering of 
non-anaphoric pronouns helped in the increase in 
precision of the pronoun resolution module. The 
table 3 shows the evaluation of pronoun resolution 
module on the development data. Here we show 
the results without the non-anaphor detection and 
with non-anaphor detection. 
 
Type of 
pronoun 
Actual 
(gold 
standard
) 
System 
identified 
Correctly 
Accuracy 
(%) 
Anaphoric 
Pronouns 
939 908 96.6 
Non-
anaphoric 
pronouns 
160 81 50.6 
Total 1099 989 89.9 
   Table 2: Evaluation of Non-anaphoric pronoun 
 
System 
type 
Total 
Anap
horic 
Pron
ouns 
System 
identifi
ed 
pronou
ns 
System 
correctl
y 
Resolv
ed 
Pronou
ns 
Prec
isio
n 
(%) 
Without 
non-
anaphoric 
pronoun 
detection 
939 1099 693 63.1 
With non-
anaphoric 
pronoun 
detection 
939 987 693 70.2 
  Table 3: Evaluation of Pronominal resolution    
module 
 
The output of the Non-pronominal resolution 
module, merged with the output of the pronominal 
resolution module and it was evaluated using 
scorer program of the CoNLL-2011. The 
evaluation was done on the development data, 
shown in the table 4. 
On analysis of the output we found mainly three 
types of errors. They are 
 
a) Newly invented chains ? The system identifies 
new chains that are not found in the gold standard 
annotation. This reduces the precision of the 
95
system. This is because of the string match as one 
of the features. 
 
Metri
c 
Mention 
Detection 
Coreference 
Resolution 
Rec  Prec F1 Rec Prec F1 
MUC 68.1 61.5 64.6 52.1 49.9 50.9 
BCU
BED 
68.1 61.5 64.6 66.6 67.6 67.1 
CEA
FE 
68.1 61.5 64.6 42.8 44.9 43.8 
Avg 68.1 61.5 64.6 53.8 54.1 53.9 
Table 4: Evaluation of the Complete System 
 
b) Only head nouns in the chain ? We observed 
that system while selecting pair for identifying 
coreference, the pair has only the head noun 
instead of the full phrase. In the phrase ?the letters 
sent in recent days?, the system identifies ?the 
letters? instead of the whole phrase. This affects 
both the precision and recall of the system. 
c) Incorrect merging of chains ? The output 
chains obtained from the pronominal resolution 
system and the non-pronominal resolution system 
are merged to form a complete chain. When the 
antecedents in the pronominal chain are merged 
with the non-pronominal chains, certain chains are 
wrongly merged into single chain. For example 
?the chairman of the committee? is identified as 
coreferring with another similar phrase ?the 
chairman of executive board? by the non-
pronominal resolution task. Both of these are 
actually not referring to the same person. This 
happens because of string similarity feature of the 
non-pronominal resolution. This merging leads to 
building a wrong chain. Hence this affects the 
precision and recall of the system. 
4 Conclusion 
We have presented a coreference resolution system 
which combines the pronominal resolution using 
refined salience based approach with non-
pronominal resolution using CRFs, machine 
learning approach. In the pronominal resolution, 
initially we identify the non-anaphoric pronouns 
using CRFs based technique. This helps in 
improving the precision. In non-pronominal 
resolution algorithm, the string match feature is an 
effective feature in identifying coreference. But, 
this feature is found to introduce errors. We need 
to add additional contextual and semantic feature 
to reduce above said errors.  The results on the 
development set are encouraging.  
References  
Shane Bergsma, and Dekang Lin. 2006. Bootstrapping 
Path-Based Pronoun Resolution. In Proceedings of 
the Conference on Computational Lingustics / 
Association for Computational Linguistics 
(COLING/ACL-06), Sydney, Australia, July 17-21, 
2006. 
John Lafferty, Andrew McCallum, Fernando Pereira.   
2001. Conditional Random Fields: Probabilistic  
Models for Segmenting and Labeling Sequence Data.   
In Proceedings of the Eighteenth International   
Conference on Machine Learning (ICML-2001).  
282-289. 
S. Lappin and H. Leass. 1994. An Algorithm for 
Pronominal Anaphora Resolution. Computational 
Linguistics, 20(4):535?562, 1994. 
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, 
Martha Palmer, Ralph Weischedel, Nianwen Xue. 
2011. CoNLL-2011 Shared Task: Modeling 
Unrestricted Coreference in OntoNotes. In 
Proceedings of the Fifteenth Conference on 
Computational Natural Language Learning (CoNLL 
2011). 
Sameer Pradhan and Lance Ramshaw and Ralph 
Weischedel and Jessica MacBride and Linnea 
Micciulla. 2007. Unrestricted Coreference: 
Identifying Entities and Events in OntoNotes. In 
Proceedings of the IEEE International Conference on 
Semantic Computing (ICSC)". Irvine, CA, 
September 17-19, 2007.  
Sobha, L. 2008. Anaphora Resolution Using Named 
Entity and Ontology. In Proceedings of the Second 
Workshop on Anaphora Resolution (WAR II), Ed 
Christer Johansson, NEALT Proceedings Series, Vol. 
2 (2008) Estonia. 91-96. 
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A 
Machine Learning Approach to Coreference 
Resolution of Noun Phrases. Computational 
Linguistics, 27(4):521?544. 
Taku Kudo. 2005. CRF++, an open source toolkit for   
CRF, http://crfpp.sourceforge.net . 
96
