Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 172?181,
Columbus, June 2008. c?2008 Association for Computational Linguistics
An Evaluation Understudy for Dialogue Coherence Models
Sudeep Gandhe and David Traum
Institute for Creative Technologies
University of Southern California
13274 Fiji way, Marina del Rey, CA, 90292
{gandhe,traum}@ict.usc.edu
Abstract
Evaluating a dialogue system is seen as a
major challenge within the dialogue research
community. Due to the very nature of the task,
most of the evaluation methods need a sub-
stantial amount of human involvement. Fol-
lowing the tradition in machine translation,
summarization and discourse coherence mod-
eling, we introduce the the idea of evaluation
understudy for dialogue coherence models.
Following (Lapata, 2006), we use the infor-
mation ordering task as a testbed for evaluat-
ing dialogue coherence models. This paper re-
ports findings about the reliability of the infor-
mation ordering task as applied to dialogues.
We find that simple n-gram co-occurrence
statistics similar in spirit to BLEU (Papineni
et al, 2001) correlate very well with human
judgments for dialogue coherence.
1 Introduction
In computer science or any other research field, sim-
ply building a system that accomplishes a certain
goal is not enough. It needs to be thoroughly eval-
uated. One might want to evaluate the system just
to see to what degree the goal is being accomplished
or to compare two or more systems with one another.
Evaluation can also lead to understanding the short-
comings of the system and the reasons for these. Fi-
nally the evaluation results can be used as feedback
in improving the system.
The best way to evaluate a novel algorithm or a
model for a system that is designed to aid humans
in processing natural language would be to employ
it in a real system and allow users to interact with it.
The data collected by this process can then be used
for evaluation. Sometimes this data needs further
analysis - which may include annotations, collect-
ing subjective judgments from humans, etc. Since
human judgments tend to vary, we may need to em-
ploy multiple judges. These are some of the reasons
why evaluation is time consuming, costly and some-
times prohibitively expensive.
Furthermore, if the system being developed con-
tains a machine learning component, the problem of
costly evaluation becomes even more serious. Ma-
chine learning components often optimize certain
free parameters by using evaluation results on held-
out data or by using n-fold cross-validation. Eval-
uation results can also help with feature selection.
This need for repeated evaluation can forbid the use
of data-driven machine learning components.
For these reasons, using an automatic evalua-
tion measure as an understudy is quickly becoming
a common practice in natural language processing
tasks. The general idea is to find an automatic eval-
uation metric that correlates very well with human
judgments. This allows developers to use the auto-
matic metric as a stand-in for human evaluation. Al-
though it cannot replace the finesse of human evalu-
ation, it can provide a crude idea of progress which
can later be validated. e.g. BLEU (Papineni et al,
2001) for machine translation, ROUGE (Lin, 2004)
for summarization.
Recently, the discourse coherence modeling com-
munity has started using the information ordering
task as a testbed to test their discourse coherence
models (Barzilay and Lapata, 2005; Soricut and
Marcu, 2006). Lapata (2006) has proposed an au-
172
tomatic evaluation measure for the information or-
dering task. We propose to use the same task as a
testbed for dialogue coherence modeling. We evalu-
ate the reliability of the information ordering task as
applied to dialogues and propose an evaluation un-
derstudy for dialogue coherence models.
In the next section, we look at related work in
evaluation of dialogue systems. Section 3 sum-
marizes the information ordering task and Lap-
ata?s (2006) findings. It is followed by the details
of the experiments we carried out and our observa-
tions. We conclude with a summary future work di-
rections.
2 Related Work
Most of the work on evaluating dialogue systems fo-
cuses on human-machine communication geared to-
wards a specific task. A variety of evaluation met-
rics can be reported for such task-oriented dialogue
systems. Dialogue systems can be judged based
on the performance of their components like WER
for ASR (Jurafsky and Martin, 2000), concept er-
ror rate or F-scores for NLU, understandability for
speech synthesis etc. Usually the core component,
the dialogue model - which is responsible for keep-
ing track of the dialogue progression and coming
up with an appropriate response, is evaluated indi-
rectly. Different dialogue models can be compared
with each other by keeping the rest of components
fixed and then by comparing the dialogue systems
as a whole. Dialogue systems can report subjective
measures such as user satisfaction scores and per-
ceived task completion. SASSI (Hone and Graham,
2000) prescribes a set of questions used for elicit-
ing such subjective assessments. The objective eval-
uation metrics can include dialogue efficiency and
quality measures.
PARADISE (Walker et al, 2000) was an attempt
at reducing the human involvement in evaluation. It
builds a predictive model for user satisfaction as a
linear combination of some objective measures and
perceived task completion. Even then the system
needs to train on the data gathered from user sur-
veys and objective features retrieved from logs of di-
alogue runs. It still needs to run the actual dialogue
system and collect objective features and perceived
task completeion to predict user satisfaction.
Other efforts in saving human involvement in
evaluation include using simulated users for test-
ing (Eckert et al, 1997). This has become a popu-
lar tool for systems employing reinforcement learn-
ing (Levin et al, 1997; Williams and Young, 2006).
Some of the methods involved in user simulation
are as complex as building dialogue systems them-
selves (Schatzmann et al, 2007). User simulations
also need to be evaluated as how closely they model
human behavior (Georgila et al, 2006) or as how
good a predictor they are of dialogue system perfor-
mance (Williams, 2007).
Some researchers have proposed metrics for eval-
uating a dialogue model in a task-oriented system.
(Henderson et al, 2005) used the number of slots in
a frame filled and/or confirmed. Roque et al (2006)
proposed hand-annotating information-states in a di-
alogue to evaluate the accuracy of information state
updates. Such measures make assumptions about
the underlying dialogue model being used (e.g.,
form-based or information-state based etc.).
We are more interested in evaluating types of di-
alogue systems that do not follow these task-based
assumptions: systems designed to imitate human-
human conversations. Such dialogue systems can
range from chatbots like Alice (Wallace, 2003),
Eliza (Weizenbaum, 1966) to virtual humans used
in simulation training (Traum et al, 2005). For
such systems, the notion of task completion or ef-
ficiency is not well defined and task specific objec-
tive measures are hardly suitable. Most evaluations
report the subjective evaluations for appropriateness
of responses. Traum et. al. (2004) propose a cod-
ing scheme for response appropriateness and scoring
functions for those categories. Gandhe et. al. (2006)
propose a scale for subjective assessment for appro-
priateness.
3 Information Ordering
The information ordering task consists of choos-
ing a presentation sequence for a set of information
bearing elements. This task is well suited for text-
to-text generation like in single or multi-document
summarization (Barzilay et al, 2002). Recently
there has been a lot of work in discourse coher-
ence modeling (Lapata, 2003; Barzilay and Lap-
ata, 2005; Soricut and Marcu, 2006) that has used
173
information ordering to test the coherence mod-
els. The information-bearing elements here are sen-
tences rather than high-level concepts. This frees the
models from having to depend on a hard to get train-
ing corpus which has been hand-authored for con-
cepts.
Most of the dialogue models still work at the
higher abstraction level of dialogue acts and inten-
tions. But with an increasing number of dialogue
systems finding use in non-traditional applications
such as simulation training, games, etc.; there is a
need for dialogue models which do not depend on
hand-authored corpora or rules. Recently Gandhe
and Traum (2007) proposed dialogue models that
do not need annotations for dialogue-acts, seman-
tics and hand-authored rules for information state
updates or finite state machines.
Such dialogue models focus primarily on gener-
ating an appropriate coherent response given the di-
alogue history. In certain cases the generation of
a response can be reduced to selection from a set
of available responses. For such dialogue models,
maintaining the information state can be considered
as a secondary goal. The element that is common
to the information ordering task and the task of se-
lecting next most appropriate response is the ability
to express a preference for one sequence of dialogue
turns over the other. We propose to use the informa-
tion ordering task to test dialogue coherence models.
Here the information bearing units will be dialogue
turns.1
There are certain advantages offered by using in-
formation ordering as a task to evaluate dialogue co-
herence models. First the task does not require a
dialogue model to take part in conversations in an
interactive manner. This obviates the need for hav-
ing real users engaging in the dialogue with the sys-
tem. Secondly, the task is agnostic about the under-
lying dialogue model. It can be a data-driven statis-
tical model or information-state based, form based
or even a reinforcement learning system based on
MDP or POMDP. Third, there are simple objective
measures available to evaluate the success of infor-
mation ordering task.
Recently, Purandare and Litman (2008) have used
1These can also be at the utterance level, but for this paper
we will use dialogue turns.
this task for modeling dialogue coherence. But they
only allow for a binary classification of sequences
as either coherent or incoherent. For comparing dif-
ferent dialogue coherence models, we need the abil-
ity for finer distinction between sequences of infor-
mation being put together. Lapata (2003) proposed
Kendall?s ? , a rank correlation measure, as one such
candidate. In a recent study they show that Kendall?s
? correlates well with human judgment (Lapata,
2006). They show that human judges can reliably
provide coherence ratings for various permutations
of text. (Pearson?s correlation for inter-rater agree-
ment is 0.56) and that Kendall?s ? is a good in-
dicator for human judgment (Pearson?s correlation
for Kendall?s ? with human judgment is 0.45 (p <
0.01)).
Before adapting the information ordering task for
dialogues, certain questions need to be answered.
We need to validate that humans can reliably per-
form the task of information ordering and can judge
the coherence for different sequences of dialogue
turns. We also need to find which objective mea-
sures (like Kendall?s ? ) correlate well with human
judgments.
4 Evaluating Information Ordering
One of the advantages of using information order-
ing as a testbed is that there are objective measures
available to evaluate the performance of information
ordering task. Kendall?s ? (Kendall, 1938), a rank
correlation coefficient, is one such measure. Given
a reference sequence of length n, Kendall?s ? for an
observed sequence can be defined as,
? = # concordant pairs ? # discordant pairs# total pairs
Each pair of elements in the observed sequence
is marked either as concordant - appearing in the
same order as in reference sequence or as discor-
dant otherwise. The total number of pairs is Cn2 =
n(n? 1)/2. ? ranges from -1 to 1.
Another possible measure can be defined as the
fraction of n-grams from reference sequence, that
are preserved in the observed sequence.
bn = # n-grams preserved# total n-grams
In this study we have used, b2, fraction of bigrams
and b3, fraction of trigrams preserved from the ref-
erence sequence. These values range from 0 to 1.
Table 1 gives examples of observed sequences and
174
Observed Sequence b2 b3 ?
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 1.00 1.00 1.00
[8, 9, 0, 1, 2, 3, 4, 5, 6, 7] 0.89 0.75 0.29
[4, 1, 0, 3, 2, 5, 8, 7, 6, 9] 0.00 0.00 0.60
[6, 9, 8, 5, 4, 7, 0, 3, 2, 1] 0.00 0.00 -0.64
[2, 3, 0, 1, 4, 5, 8, 9, 6, 7] 0.56 0.00 0.64
Table 1: Examples of observed sequences and their re-
spective b2, b3 & ? values. Here the reference sequence
is [0,1,2,3,4,5,6,7,8,9].
respective b2, b3 and ? values. Notice how ? al-
lows for long-distance relationships whereas b2, b3
are sensitive to local features only. 2
5 Experimental Setup
For our experiments we used segments drawn from 9
dialogues. These dialogues were two-party human-
human dialogues. To ensure applicability of our
results over different types of dialogue, we chose
these 9 dialogues from different sources. Three of
these were excerpts from role-play dialogues involv-
ing negotiations which were originally collected for
a simulation training scenario (Traum et al, 2005).
Three are from SRI?s Amex Travel Agent data which
are task-oriented dialogues about air travel plan-
ning (Bratt et al, 1995). The rest of the dialogues are
scripts from popular television shows. Fig 6 shows
an example from the air-travel domain. Each excerpt
drawn was 10 turns long with turns strictly alternat-
ing between the two speakers.
Following the experimental design of (Lapata,
2006) we created random permutations for these di-
alogue segments. We constrained our permutations
so that the permutations always start with the same
speaker as the original dialogue and turns strictly al-
ternate between the speakers. With these constraints
there are still 5!? 5! = 14400 possible permutations
per dialogue. We selected 3 random permutations
for each of the 9 dialogues. In all, we have a total
of 27 dialogue permutations. They are arranged in 3
sets, each set containing a permutation for all 9 di-
alogues. We ensured that not all permutations in a
given set are particularly very good or very bad. We
used Kendall?s ? to balance the permutations across
2For more on the relationship between b2, b3 and ? see row
3,4 of table 1 and figure 4.
the given set as well as across the given dialogue.
Unlike Lapata (2006) who chose to remove the
pronouns and discourse connectives, we decided not
do any pre-processing on the text like removing
disfluencies or removing cohesive devices such as
anaphora, ellipsis, discourse connectives, etc. One
of the reason is such pre-processing if done manu-
ally defeats the purpose of removing humans from
the evaluation procedure. Moreover it is very diffi-
cult to remove certain cohesive devices such as dis-
course deixis without affecting the coherence level
of the original dialogues.
6 Experiment 1
In our first experiment, we divided a total of 9 hu-
man judges among the 3 sets (3 judges per set). Each
judge was presented with 9 dialogue permutations.
They were asked to assign a single coherence rat-
ing for each dialogue permutation. The ratings were
on a scale of 1 to 7, with 1 being very incoherent
and 7 being perfectly coherent. We did not provide
any additional instructions or examples of scale as
we wanted to capture the intuitive idea of coherence
from our judges. Within each set the dialogue per-
mutations were presented in random order.
We compute the inter-rater agreement by using
Pearson?s correlation analysis. We correlate the rat-
ings given by each judge with the average ratings
given by the judges who were assigned the same set.
For inter-rater agreement we report the average of 9
such correlations which is 0.73 (std dev = 0.07). Art-
stein and Poesio (2008) have argued that Krippen-
dorff?s ? (Krippendorff, 2004) can be used for inter-
rater agreement with interval scales like the one we
have. In our case for the three sets ? values were
0.49, 0.58, 0.64. These moderate values of alpha in-
dicate that the task of judging coherence is indeed a
difficult task, especially when detailed instructions
or examples of scales are not given.
In order to assess whether Kendall?s ? can be used
as an automatic measure of dialogue coherence, we
perform a correlation analysis of ? values against
the average ratings by human judges. The Pearson?s
correlation coefficient is 0.35 and it is statistically
not significant (P=0.07). Fig 1(a) shows the rela-
tionship between coherence judgments and ? val-
ues. This experiment fails to support the suitability
175
(a) Kendall?s ? does not correlate well with human
judgments for dialogue coherence.
(b) Fraction of bigram & trigram counts correlate well
with human judgments for dialogue coherence.
Figure 1: Experiment 1 - single coherence rating per permutation
of Kendall?s ? as an evaluation understudy.
We also analyzed the correlation of human judg-
ments against simple n-gram statistics, specifically
(b2 + b3) /2. Fig 1(b) shows the relationship be-
tween human judgments and the average of fraction
of bigrams and fraction of trigrams that were pre-
served in the permutation. The Pearson?s correlation
coefficient is 0.62 and it is statistically significant
(P<0.01).
7 Experiment 2
Since human judges found it relatively hard to as-
sign a single rating to a dialogue permutation, we
decided to repeat experiment 1 with some modifica-
tions. In our second experiment we asked the judges
to provide coherence ratings at every turn, based on
the dialogue that preceded that turn. The dialogue
permutations were presented to the judges through a
web interface in an incremental fashion turn by turn
as they rated each turn for coherence (see Fig 5 in
the appendix for the screenshot of this interface). We
used a scale from 1 to 5 with 1 being completely in-
coherent and 5 as perfectly coherent. 3 A total of 11
judges participated in this experiment with the first
set being judged by 5 judges and the remaining two
sets by 3 judges each.
3We believe this is a less complex task than experiment 1
and hence a narrower scale is used.
For the rest of the analysis, we use the average
coherence rating from all turns as a coherence rat-
ing for the dialogue permutation. We performed
the inter-rater agreement analysis as in experiment
1. The average of 11 correlations is 0.83 (std dev =
0.09). Although the correlation has improved, Krip-
pendorff?s ? values for the three sets are 0.49, 0.35,
0.63. This shows that coherence rating is still a hard
task even when judged turn by turn.
We assessed the relationship between the aver-
age coherence rating for dialogue permutations with
Kendall?s ? (see Fig 2(a)). The Pearson?s correlation
coefficient is 0.33 and is statistically not significant
(P=0.09).
Fig 2(b) shows high correlation of average coher-
ence ratings with the fraction of bigrams and tri-
grams that were preserved in permutation. The Pear-
son?s correlation coefficient is 0.75 and is statisti-
cally significant (P<0.01).
Results of both experiments suggest that,
(b2 + b3) /2 correlates very well with human judg-
ments and can be used for evaluating information
ordering when applied to dialogues.
8 Experiment 3
We wanted to know whether information ordering as
applied to dialogues is a valid task or not. In this ex-
periment we seek to establish a higher baseline for
176
(a) Kendall?s ? does not correlate well with human
judgments for dialogue coherence.
(b) Fraction of bigram & trigram counts correlate well
with human judgments for dialogue coherence.
Figure 2: Experiment 2 - turn-by-turn coherence rating
the task of information ordering in dialogues. We
presented the dialogue permutations to our human
judges and asked them to reorder the turns so that
the resulting order is as coherent as possible. All 11
judges who participated in experiment 2 also partic-
ipated in this experiment. They were presented with
a drag and drop interface over the web that allowed
them to reorder the dialogue permutations. The re-
ordering was constrained to keep the first speaker
of the reordering same as that of the original di-
alogue and the re-orderings must have strictly al-
ternating turns. We computed the Kendall?s ? and
fraction of bigrams and trigrams (b2 + b3) /2 for
these re-orderings. There were a total of 11 ? 9
= 99 reordered dialogue permutations. Fig 3(a)
and 3(b) shows the frequency distribution of ? and
(b2 + b3) /2 values respectively.
Humans achieve high values for the reordering
task. For Kendall?s ? , the mean of the reordered dia-
logues is 0.82 (std dev = 0.25) and for (b2 + b3) /2,
the mean is 0.71 (std dev = 0.28). These values es-
tablish an upper baseline for the information order-
ing task. These can be compared against the random
baseline. For ? random performance is 0.02 4 and
4Theoretically this should be zero. The slight positive bias
is the result of the constraints imposed on the re-orderings -
like only allowing the permutations that have the correct starting
speaker.
for (b2 + b3) /2 it is 0.11. 5
9 Discussion
Results show that (b2 + b3) /2 correlates well with
human judgments for dialogue coherence better than
Kendall?s ? . ? encodes long distance relationships
in orderings where as (b2 + b3) /2 only looks at lo-
cal context. Fig 4 shows the relationship between
these two measures. Notice that most of the order-
ings have ? values around zero (i.e. in the middle
range for ? ), whereas majority of orderings will have
a low value for (b2 + b3) /2. ? seems to overesti-
mate the coherence even in the absence of immedi-
ate local coherence (See third entry in table 1). It
seems that local context is more important for dia-
logues than for discourse, which may follow from
the fact that dialogues are produced by two speakers
who must react to each other, while discourse can be
planned by one speaker from the beginning. Traum
and Allen (1994) point out that such social obliga-
tions to respond and address the contributions of the
other should be an important factor in building dia-
logue systems.
The information ordering paradigm does not take
into account the content of the information-bearing
items, e.g. the fact that turns like ?yes?, ?I agree?,
5This value is calculated by considering all 14400 permuta-
tions as equally likely.
177
(a) Histogram of Kendall?s ? for reordered se-
quences
(b) Histogram of fraction of bigrams & tri-
grams values for reordered sequences
Figure 3: Experiment 3 - upper baseline for information ordering task (human performance)
?okay? perform the same function and should be
treated as replaceable. This may suggest a need to
modify some of the objective measures to evaluate
the information ordering specially for dialogue sys-
tems that involve more of such utterances.
Human judges can find the optimal sequences
with relatively high frequency, at least for short
dialogues. It remains to be seen how this varies
with longer dialogue lengths which may contain
sub-dialogues that can be arranged independently of
each other.
10 Conclusion & Future Work
Evaluating dialogue systems has always been a ma-
jor challenge in dialogue systems research. The core
component of dialogue systems, the dialogue model,
has usually been only indirectly evaluated. Such
evaluations involve too much human effort and are a
bottleneck for the use of data-driven machine learn-
ing models for dialogue coherence. The information
ordering task, widely used in discourse coherence
modeling, can be adopted as a testbed for evaluating
dialogue coherence models as well. Here we have
shown that simple n-gram statistics that are sensi-
tive to local features correlate well with human judg-
ments for coherence and can be used as an evalua-
tion understudy for dialogue coherence models. As
with any evaluation understudy, one must be careful
while using it as the correlation with human judg-
ments is not perfect and may be inaccurate in some
cases ? it can not completely replace the need for
full evaluation with human judges in all cases (see
(Callison-Burch et al, 2006) for a critique of BLUE
along these lines).
In the future, we would like to perform more ex-
periments with larger data sets and different types
of dialogues. It will also be interesting to see the
role cohesive devices play in coherence ratings. We
would like to see if there are any other measures or
certain modifications to the current ones that corre-
late better with human judgments. We also plan to
employ this evaluation metric as feedback in build-
ing dialogue coherence models as is done in ma-
chine translation (Och, 2003).
Acknowledgments
The effort described here has been sponsored by the U.S. Army
Research, Development, and Engineering Command (RDE-
COM). Statements and opinions expressed do not necessarily
reflect the position or the policy of the United States Govern-
ment, and no official endorsement should be inferred. We would
like to thank Radu Soricut, Ron Artstein, and the anonymous
SIGdial reviewers for helpful comments.
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. In To appear
in Computational Linguistics.
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In Proc.
ACL-05.
178
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2002. Inferring strategies for sentence ordering
in multidocument summarization. JAIR, 17:35?55.
Harry Bratt, John Dowding, and Kate Hunicke-Smith.
1995. The sri telephone-based atis system. In Pro-
ceedings of the Spoken Language Systems Technology
Workshop, January.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. In proceedings of EACL-2006.
Wieland Eckert, Esther Levin, and Roberto Pieraccini.
1997. User modeling for spoken dialogue system eval-
uation. In Automatic Speech Recognition and Under-
standing, pages 80?87, Dec.
Sudeep Gandhe and David Traum. 2007. Creating spo-
ken dialogue characters from corpora without annota-
tions. In Proceedings of Interspeech-07.
Sudeep Gandhe, Andrew Gordon, and David Traum.
2006. Improving question-answering with linking di-
alogues. In International Conference on Intelligent
User Interfaces (IUI), January.
Kalliroi Georgila, James Henderson, and Oliver Lemon.
2006. User simulation for spoken dialogue systems:
Learning and evaluation. In proceedings of Inter-
speech.
James Henderson, Oliver Lemon, and Kallirroi Georgila.
2005. Hybrid reinforcement/supervised learning for
dialogue policies from communicator data. In pro-
ceedings of IJCAI workshop.
Kate S. Hone and Robert Graham. 2000. Towards a tool
for the subjective assessment of speech system inter-
faces (SASSI). Natural Language Engineering: Spe-
cial Issue on Best Practice in Spoken Dialogue Sys-
tems.
Daniel Jurafsky and James H. Martin. 2000. SPEECH
and LANGUAGE PROCESSING: An Introduction to
Natural Language Processing, Computational Lin-
guistics, and Speech Recognition. Prentice-Hall.
Maurice G. Kendall. 1938. A new measure of rank cor-
relation. Biometrika, 30:81?93.
Klaus Krippendorff. 2004. Content Analysis, An Intro-
duction to Its Methodology 2nd Edition. Sage Publi-
cations.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, Sapporo, Japan.
Mirella Lapata. 2006. Automatic evaluation of informa-
tion ordering. Computational Linguistics, 32(4):471?
484.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
1997. Learning dialogue strategies within the markov
decision process framework. In Automatic Speech
Recognition and Understanding, pages 72?79, Dec.
Chin-Yew Lin. 2004. ROUGE: a package for automatic
evaluation of summaries. In Proceedings of the Work-
shop on Text Summarization Branches Out.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In In ACL 2003: Proc.
of the 41st Annual Meeting of the Association for Com-
putational Linguistics, July.
Kishore A. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Technical Re-
port RC22176 (W0109-022), IBM Research Division,
September.
Amruta Purandare and Diane Litman. 2008. Analyz-
ing dialog coherence using transition patterns in lexi-
cal and semantic features. In Proceedings 21st Inter-
national FLAIRS Conference, May.
Antonio Roque, Hua Ai, and David Traum. 2006. Evalu-
ation of an information state-based dialogue manager.
In Brandial 2006: The 10th Workshop on the Seman-
tics and Pragmatics of Dialogue.
Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve Young. 2007. Agenda-based user
simulation for bootstrapping a pomdp dialogue sys-
tem. In proceedings of HLT/NAACL, Rochester, NY.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Proc.
ACL-06.
David R. Traum and James F. Allen. 1994. Discourse
obligations in dialogue processing. In proceedings of
the 32nd Annual Meeting of the Association for Com-
putational Linguistics (ACL-94), pages 1?8.
David R. Traum, Susan Robinson, and Jens Stephan.
2004. Evaluation of multi-party virtual reality dia-
logue interaction. In In Proceedings of Fourth Interna-
tional Conference on Language Resources and Evalu-
ation (LREC), pages 1699?1702.
David Traum, William Swartout, Jonathan Gratch, and
Stacy Marsella. 2005. Virtual humans for non-team
interaction training. In AAMAS-05 Workshop on Cre-
ating Bonds with Humanoids, July.
M. Walker, C. Kamm, and D. Litman. 2000. Towards de-
veloping general models of usability with PARADISE.
Natural Language Engineering: Special Issue on Best
Practice in Spoken Dialogue Systems.
Richard Wallace. 2003. Be Your Own Botmaster, 2nd
Edition. ALICE A. I. Foundation.
Joseph Weizenbaum. 1966. Eliza?a computer program
for the study of natural language communication be-
tween man and machine. Communications of the
ACM, 9(1):36?45, January.
Jason D. Williams and Steve Young. 2006. Partially ob-
servable markov decision processes for spoken dialog
systems. Computer Speech and Language, 21:393?
422.
Jason D. Williams. 2007. A method for evaluating and
comparing user simulations: The cramer-von mises di-
vergence. In IEEE Workshop on Automatic Speech
Recognition and Understanding (ASRU).
179
Appendix
(a) (b) (c)
Figure 4: Distributions for Kendall?s ? , (b2 + b3) /2 and the relationship between them for all possible dialogue
permutations with 10 turns and earlier mentioned constraints.
Figure 5: Screenshot of the interface used for collecting coherence rating for dialogue permutations.
180
Agent AAA at American Express may I help you?
User yeah this is BBB BBB I need to make some travel arrangements
Agent ok and what do you need to do?
User ok on June sixth from San Jose to Denver, United
Agent leaving at what time?
User I believe there?s one leaving at eleven o?clock in the morning
Agent leaves at eleven a.m. and arrives Denver at two twenty p.m. out of San Jose
User ok
Agent yeah that?s United flight four seventy
User that?s the one
Doctor hello i?m doctor perez
how can i help you
Captain uh well i?m with uh the local
i?m i?m the commander of the local company
and uh i?d like to talk to you about some options you have for relocating your clinic
Doctor uh we?re not uh planning to relocate the clinic captain
what uh what is this about
Captain well have you noticed that there?s been an awful lot of fighting in the area recently
Doctor yes yes i have
we?re very busy
we?ve had many more casual+ casualties many more patients than than uh usual in the
last month
but uh what what is this about relocating our clinic
have have uh you been instructed to move us
Captain no
but uh we just have some concerns about the increase in fighting xx
Doctor are you suggesting that we relocate the clinic
because we had no plans
we uh we uh we?re located here and we?ve been uh
we are located where the patients need us
Captain yeah but
yeah actually it is a suggestion that you would be a lot safer if you moved away from
this area
we can put you in an area where there?s n+ no insurgents
and we have the area completely under control with our troops
Doctor i see captain
is this a is this a suggestion from your commander
Captain i?m uh the company commander
Figure 6: Examples of the dialogues used to elicit human judgments for coherence
181
