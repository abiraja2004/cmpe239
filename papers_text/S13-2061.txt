Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 375?379, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
TJP: Using Twitter to Analyze the Polarity of Contexts 
 
 
Tawunrat Chalothorn Jeremy Ellman 
University of Northumbria at Newcastle University of Northumbria at Newcastle 
Pandon Building, Camden Street Pandon Building, Camden Street 
Newcastle Upon Tyne, NE2 1XE, UK Newcastle Upon Tyne, NE2 1XE, UK 
Tawunrat.chalothorn@unn.ac.uk Jeremy.ellman@unn.ac.uk 
 
 
 
 
 
 
Abstract 
This paper presents our system, TJP, which 
participated in SemEval 2013 Task 2 part A: 
Contextual Polarity Disambiguation. The goal 
of this task is to predict whether marked con-
texts are positive, neutral or negative. Howev-
er, only the scores of positive and negative 
class will be used to calculate the evaluation 
result using F-score. We chose to work as 
?constrained?, which used only the provided 
training and development data without addi-
tional sentiment annotated resources. Our ap-
proach considered unigram, bigram and 
trigram using Na?ve Bayes training model 
with the objective of establishing a simple-
approach baseline. Our system achieved F-
score 81.23% and F-score 78.16% in the re-
sults for SMS messages and Tweets respec-
tively. 
1 Introduction 
Natural language processing (NLP) is a research 
area comprising various tasks; one of which is sen-
timent analysis. The main goal of sentiment analy-
sis is to identify the polarity of natural language 
text (Shaikh et al, 2007). Sentiment analysis can 
be referred to as opinion mining, as study peoples? 
opinions, appraisals and emotions towards entities 
and events and their attributes (Pang and Lee, 
2008). Sentiment analysis has become a popular 
research area in NLP with the purpose of identify-
ing opinions or attitudes in terms of polarity.  
This paper presents TJP, a system submitted to 
SemEval 2013 for Task 2 part A: Contextual Polar-
ity Disambiguation (Wilson et al, 2013). TJP was 
focused on the ?constrained? task, which used only 
training and development data provided. This 
avoided both resource implications and potential 
advantages implied by the use of additional data 
containing sentiment annotations. The objective 
was to explore the relative success of a simple ap-
proach that could be implemented easily with 
open-source software.  
The TJP system was implemented using the Py-
thon Natural Language Toolkit (NLTK, Bird et al, 
2009). We considered several basic approaches. 
These used a preprocessing phase to expand con-
tractions, eliminate stopwords, and identify emoti-
cons. The next phase used supervised machine 
learning and n-gram features. Although we had 
two approaches that both used n-gram features, we 
were limited to submitting just one result. Conse-
quently, we chose to submit a unigram based ap-
proach followed by naive Bayes since this 
performed better on the data.  
The remainder of this paper is structured as fol-
lows: section 2 provides some discussion on the 
related work. The methodology of corpus collec-
tion and data classification are provided in section 
3. Section 4 outlines details of the experiment and 
results, followed by the conclusion and ideas for 
future work in section 5. 
2 Related Work  
The micro-blogging tool Twitter is well-known 
and increasingly popular. Twitter allows its users 
to post messages, or ?Tweets? of up to 140 charac-
ters each time, which are available for immediate 
375
download over the Internet. Tweets are extremely 
interesting to marketing since their rapid public 
interaction can either indicate customer success or 
presage public relations disasters far more quickly 
than web pages or traditional media. Consequently, 
the content of tweets and identifying their senti-
ment polarity as positive or negative is a current 
active research topic. 
Emoticons are features of both SMS texts, and 
tweets. Emoticons such as :) to represent a smile, 
allow emotions to augment the limited text in SMS 
messages using few characters. Read (2005) used 
emoticons from a training set that was downloaded 
from Usenet newsgroups as annotations (positive 
and negative). Using the machine learning tech-
niques of Na?ve Bayes and Support Vector Ma-
chines Read (2005) achieved up to 70 % accuracy 
in determining text polarity from the emoticons 
used. 
Go et al (2009) used distant supervision to clas-
sify sentiment of Twitter, as similar as in (Read, 
2005). Emoticons have been used as noisy labels in 
training data to perform distant supervised learning 
(positive and negative). Three classifiers were 
used: Na?ve Bayes, Maximum Entropy and Sup-
port Vector Machine, and they were able to obtain 
more than 80% accuracy on their testing data.  
Aisopos et al (2011) divided tweets in to three 
groups using emoticons for classification. If tweets 
contain positive emoticons, they will be classified 
as positive and vice versa. Tweets without posi-
tive/negative emoticons will be classified as neu-
tral. However, tweets that contain both positive 
and negative emoticons are ignored in their study. 
Their task focused on analyzing the contents of 
social media by using n-gram graphs, and the re-
sults showed that n-gram yielded high accuracy 
when tested with C4.5, but low accuracy with Na-
?ve Bayes Multinomial (NBM). 
3 Methodology  
3.1 Corpus 
The training data set for SemEval was built using 
Twitter messages training and development data.  
There are more than 7000 pieces of context. Users 
usually use emoticons in their tweets; therefore, 
emoticons have been manually collected and la-
beled as positive and negative to provide some 
context (Table 1), which is the same idea as in Ai-
sopos et al (2011).  
 
Negative emoticons :( :-( :d :< D: :\ /: etc. 
Positive emoticons 
:) ;) :-) ;-) :P ;P (: (; :D 
;D etc. 
 
Table 1: Emoticon labels as negative and positive 
 
Furthermore, there are often features that have 
been used in tweets, such as hashtags, URL links, 
etc. To extract those features, the following pro-
cesses have been applied to the data. 
 
1. Retweet (RT), twitter username (@panda), 
URL links (e.g. y2u.be/fiKKzdLQvFo), 
and special punctuation were removed. 
2. Hashtags have been replaced by the fol-
lowing word (e.g. # love was replaced by 
love, # exciting was replaced by exciting). 
3. English contraction of ?not? was converted 
to full form (e.g. don?t -> do not). 
4. Repeated letters have been reduced and re-
placed by 2 of the same character (e.g. 
happpppppy will be replaced by happy, 
coollllll will be replaced by cooll). 
3.2 Classifier 
Our system used the NLTK Na?ve Bayes classifier 
module. This is a classification based on Bayes?s 
rule and also known as the state-of-art of the Bayes 
rules (Cufoglu et al, 2008). The Na?ve Bayes 
model follows the assumption that attributes within 
the same case are independent given the class label 
(Hope and Korb, 2004).  
Tang et al (2009) considered that Na?ve Bayes 
assigns a context   (represented by a vector   
 ) to 
the class    that maximizes        
   by applying 
Bayes?s rule, as in (1). 
 
 (  |  
 )   
         
     
    
  
 (1) 
 
 
 
where     
   is a randomly selected context  . The 
representation of vector is   
 .      is the random 
select context that is assigned to class  . 
To classify the term     
     , features in   
  
were assumed as    from         as in (2). 
376
  (  |  
 )   
     ?         
 
   
    
  
 (2) 
 
There are many different approaches to lan-
guage analysis using syntax, semantics, and se-
mantic resources such as WordNet. That may be 
exploited using the NLTK (Bird et al 2009). How-
ever, for simplicity we opted here for the n-gram 
approach where texts are decomposed into term 
sequences. A set of single sequences is a unigram. 
The set of two word sequences (with overlapping) 
are bigrams, whilst the set of overlapping three 
term sequences are trigrams. The relative ad-
vantage of the bi-and trigram approaches are that 
coordinates terms effectively disambiguate senses 
and focus content retrieval and recognition. 
N-grams have been used many times in contents 
classification. For example, Pang et al (2002) used 
unigram and bigram to classify movie reviews. The 
results showed that unigram gave better results 
than bigram. Conversely, Dave et al (2003) re-
ported gaining better results from trigrams rather 
than bigram in classifying product reviews. Conse-
quently, we chose to evaluate unigrams, bigrams 
and trigrams to see which will give the best results 
 
 
 
 
Figure 1: Comparison of Twitter messages from two approaches 
 
 
 
 
Figure 2: Comparison of SMS messages from two approaches 
Unigram Bigram Trigram
Pos 1 84.46 82.09 80.8
Neg 1 71.08 59.53 52.91
Pos 2 84.62 83.31 83.25
Neg 2 71.70 65.00 64.34
50
55
60
65
70
75
80
85
90
F
-s
co
re
 (
%
) Pos 1
Neg 1
Pos 2
Neg 2
Unigram Bigram Trigram
Pos 1 76.23 73.89 72.02
Neg 1 82.61 76.04 71.19
Pos 2 77.81 75.69 75.42
Neg 2 84.66 79.94 79.37
50
55
60
65
70
75
80
85
90
F
-s
co
re
 (
%
) 
Pos 1
Neg 1
Pos 2
Neg 2
377
in the polarity classification. Our results are de-
scribed in the next section. 
4 Experiment and Results  
In this experiment, we used the distributed data 
from Twitter messages and the F-measure for sys-
tem evaluation. As at first approach, the corpora 
were trained directly in the system, while stop-
words (e.g. a, an, the) were removed before train-
ing using the python NLTK for the second 
approach. The approaches are demonstrated on a 
sample context in Table 2 and 3. 
After comparing both approaches (Figure 1), we 
were able to obtain an F-score 84.62% of positive 
and 71.70% of negative after removing stopwords. 
Then, the average F-score is 78.16%, which was 
increased from the first approach by 0.50%. The 
results from both approaches showed that, unigram 
achieved higher scores than either bigrams or tri-
grams.  
Moreover, these experiments have been tested 
with a set of SMS messages to assess how well our 
system trained on Twitter data can be generalized 
to other types of message data. The second ap-
proach still achieved the better scores (Figure 2), 
where we were able to obtain an F-score of 77.81% 
of positive and 84.66% of negative; thus, the aver-
age F-score is 81.23%. 
The results of unigram from the second ap-
proach submitted to SemEval 2013 can be found in 
Figure 3. After comparing them using the average 
F-score from positive and negative class, the re-
sults showed that our system works better for SMS 
messaging than for Twitter. 
 
gonna miss some of my classes. 
Unigram Bigram Trigram 
gonna 
miss 
some 
of 
my 
classes 
gonna miss 
miss some 
some of 
of my 
my classes 
gonna miss some 
miss some of 
some of my 
of my classes 
 
Table 2: Example of context from first approach 
 
 
 
 
 
 
gonna miss (some of) my classes. 
Unigram Bigram Trigram 
gonna 
miss 
my 
classes 
gonna miss 
miss my 
my classes 
gonna miss my 
miss my classes 
 
Table 3: Example of context from second approach. 
Note ?some? and ?of? are listed in NLTK stopwords. 
 
 
 
 
Figure 3: Results of unigram of Twitter and SMS in the 
second approach 
5 Conclusion and Future Work 
A system, TJP, has been described that participated 
in SemEval 2013 Task 2 part A: Contextual Polari-
ty Disambiguation (Wilson et al, 2013). The sys-
tem used the Python NLTK (Bird et al2009) Naive 
Bayes classifier trained on Twitter data. Further-
more, emoticons were collected and labeled as pos-
itive and negative in order to classify contexts with 
emoticons. After analyzing the Twitter message 
and SMS messages, we were able to obtain an av-
erage F-score of 78.16% and 81.23% respectively 
during the SemEval 2013 task. The reason that, our 
system achieved better scores with SMS message 
then Twitter message might be due to our use of 
Twitter messages as training data. However this is 
still to be verified experimentally. 
The experimental performance on the tasks 
demonstrates the advantages of simple approaches. 
This provides a baseline performance set to which 
more sophisticated or resource intensive tech-
niques may be compared. 
 
Pos Neg Average
Twitter 84.62 71.70 78.16
SMS 77.81 84.66 81.23
65
70
75
80
85
90
F
-s
co
re
 (
%
) 
378
For future work, we intend to trace back to the 
root words and work with the suffix and prefix that 
imply negative semantics, such as ?dis-?, ?un-?, ?-
ness? and ?-less?. Moreover, we would like to col-
lect more shorthand texts than that used commonly 
in microblogs, such as gr8 (great), btw (by the 
way), pov (point of view), gd (good) and ne1 (any-
one). We believe these could help to improve our 
system and achieve better accuracy when classify-
ing the sentiment of context from microblogs. 
References  
Alec Go, Richa Bhayani and Lei Huang. 2009. Twitter 
sentiment classification using distant supervision. 
CS224N Project Report, Stanford, 1-12. 
Ayse Cufoglu, Mahi Lohi and Kambiz Madani. 2008. 
Classification accuracy performance of Naive Bayes-
ian (NB), Bayesian Networks (BN), Lazy Learning of 
Bayesian Rules (LBR) and Instance-Based Learner 
(IB1) - comparative study. Paper presented at the 
Computer Engineering & Systems, 2008. ICCES 
2008. International Conference on. 
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan. 
2002. Thumbs up?: sentiment classification using 
machine learning techniques. Paper presented at the 
Proceedings of the ACL-02 conference on Empirical 
methods in natural language processing - Volume 10. 
Fotis Aisopos, George Papadakis and Theodora 
Varvarigou. 2011. Sentiment analysis of social media 
content using N-Gram graphs. Paper presented at the 
Proceedings of the 3rd ACM SIGMM international 
workshop on Social media, Scottsdale, Arizona, 
USA. 
Huifeng Tang, Songbo Tan and Xueqi Cheng. 2009. A 
survey on sentiment detection of reviews. Expert Sys-
tems with Applications, 36(7), 10760-10773. 
Jonathon. Read. 2005. Using emoticons to reduce de-
pendency in machine learning techniques for senti-
ment classification. Paper presented at the 
Proceedings of the ACL Student Research Work-
shop, Ann Arbor, Michigan. 
Kushal Dave, Steve Lawrence and David M. Pennock. 
2003. Mining the peanut gallery: opinion extraction 
and semantic classification of product reviews. Paper 
presented at the Proceedings of the 12th international 
conference on World Wide Web, Budapest, Hungary. 
Lucas R. Hope and Kevin B. Korb. 2004. A bayesian 
metric for evaluating machine learning algorithms. 
Paper presented at the Proceedings of the 17th Aus-
tralian joint conference on Advances in Artificial In-
telligence, Cairns, Australia.  
Mostafa Al Shaikh, Helmut Prendinger and Ishizuka 
Mitsuru. 2007. Assessing Sentiment of Text by Se-
mantic Dependency and Contextual Valence Analy-
sis. Paper presented at the Proceedings of the 2nd in-
ternational conference on Affective Computing and 
Intelligent Interaction, Lisbon, Portugal. 
Pang Bo and Lillian Lee. 2008. Opinion Mining and 
Sentiment Analysis. Found. Trends Inf. Retr., 2(1-2), 
1-135. 
Steven Bird, Ewan Klein and Edward Loper. 2009. Nat-
ural language processing with Python: O'Reilly.  
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, 
Sara Rosenthal, Veselin Stoyanov and Alan Ritter. 
2013. SemEval-2013 Task 2: Sentiment Analysis in 
Twitter Proceedings of the 7th International Work-
shop on Semantic Evaluation: Association for Com-
putational Linguistics. 
 
 
379
