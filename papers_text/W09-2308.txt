Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 60?68,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
On the complexity of alignment problems in two synchronous grammar
formalisms
Anders S?gaard?
Center for Language Technology
University of Copenhagen
soegaard@hum.ku.dk
Abstract
The alignment problem for synchronous
grammars in its unrestricted form, i.e. whether
for a grammar and a string pair the grammar
induces an alignment of the two strings, re-
duces to the universal recognition problem,
but restrictions may be imposed on the align-
ment sought, e.g. alignments may be 1 : 1,
island-free or sure-possible sorted. The com-
plexities of 15 restricted alignment problems
in two very different synchronous grammar
formalisms of syntax-based machine transla-
tion, inversion transduction grammars (ITGs)
(Wu, 1997) and a restricted form of range
concatenation grammars ((2,2)-BRCGs) (S?-
gaard, 2008), are investigated. The universal
recognition problems, and therefore also the
unrestricted alignment problems, of both for-
malisms can be solved in time O(n6|G|). The
complexities of the restricted alignment prob-
lems differ significantly, however.
1 Introduction
The synchronous grammar formalisms used in
syntax-based machine translation typically induce
alignments by aligning all words that are recog-
nized simultaneously (Wu, 1997; Zhang and Gildea,
?This work was done while the first author was a Senior
Researcher at the Dpt. of Linguistics, University of Potsdam,
supported by the German Research Foundation in the Emmy
Noether project Ptolemaios on grammar learning from paral-
lel corpora; and while he was a Postdoctoral Researcher at the
ISV Computational Linguistics Group, Copenhagen Business
School, supported by the Danish Research Foundation in the
project Efficient syntax- and semantics-based machine transla-
tion.
2004). On a par with weak and strong generative ca-
pacity, it is thus possible to talk about the alignment
capacity of those formalisms. In this paper, two syn-
chronous grammar formalisms are discussed, inver-
sion transduction grammars (ITGs) (Wu, 1997) and
two-variable binary bottom-up non-erasing range
concatenation grammars ((2,2)-BRCGs) (S?gaard,
2008). It is known that ITGs do not induce the class
of inside-out alignments discussed in Wu (1997).
Another class that ITGs do not induce is that of
alignments with discontinuous translation units (S?-
gaard, 2008). S?gaard (2008), on the other hand,
shows that the alignments induced by (2,2)-BRCGs
are closed under union, i.e. (2,2)-BRCGs induce all
possible alignments.
The universal recognition problems of both ITGs
and (2,2)-BRCGs can be solved in time O(n6|G|).
This may come as a surprise, as ITGs restrict the
alignment search space considerably, while (2,2)-
BRCGs do not. In the context of the NP-hardness of
decoding in statistical machine translation (Knight,
1999; Udupa and Maji, 2006), it is natural to ask
why the universal recognition problem of (2,2)-
BRCGs isn?t NP-hard? How can (2,2)-BRCGs in-
duce all possible alignments and still avoid NP-
hardness? This paper bridges the gap between
these results and shows that when alignments are
restricted to be 1 : 1, island-free or sure-possible
sorted (see below), or all combinations thereof,
the alignment problem of (2,2)-BRCGs is NP-hard.
(2,2)-BRCGs in a sense avoid NP-hardness by giv-
ing up control over global properties of alignments,
e.g. any pair of words may be aligned multiple times
in a derivation.
60
The alignment structures induced by synchronous
grammars in syntax-based machine translation have
the following property: If an alignment structure in-
cludes alignments v|v?, v|w? and w|w?, it also in-
cludes the alignment w|v?, where w,w?, v, v? are
word instances.1 This follows from the fact that
only words that are recognized simultanously, are
aligned. Otherwise alignment structures are just a
binary symmetric relation on two strings, a source
and a target string, such that two words in the source,
resp. target string, cannot be aligned. Maximally
connected subgraphs (ignoring precedence edges)
are called translation units.
The alignment problem can be formulated this
way (with s, s? source and target sentence, resp.):
INSTANCE: G, ?s, s??.
QUESTION: Does G induce an alignment
on ?s, s???
The alignment problem in its unrestricted form
reduces to the universal recognition problem (Bar-
ton et al, 1987), i.e. whether for a grammar G and
a string pair ?s, s?? it holds that ?s, s?? ? L(G)?
Of course the alignment may in this case be empty
or partial. Both ITGs and (2,2)-BRCGs permit un-
aligned nodes.
This paper investigates the complexity of re-
stricted versions of the alignment problem for ITGs
and (2,2)-BRCGs. A simple example, which can
be solved in linear time for both formalisms, is the
alignment problem wrt. alignments that consist of a
single translation unit including all source and target
words. It may be formulated this way:
INSTANCE: G, ?s, s??.
QUESTION: Does G induce an alignment that
consists of a single translation unit
with no unaligned words on ?s, s???
This can be solved for ITGs by checking if there
is a production rule that introduces all the words in
the right order such that:2
1w|w? is our short hand notation for saying that w, a word
in the source string, and w?, a word in the target string, have
been aligned. In the formal definition of alignments below, it is
said that w ? Vs (w is a word in the source string), w? ? Vt
(w? is a word in the target string) and (w,w?) ? A, i.e. w is
aligned to w?, and vice versa. Alignments are bidirectional in
what follows.
2In fact in normal form ITGs, we can simply check if there
? The LHS nonterminal symbol (possibly suf-
fixed by the empty string ?) can be derived from
the start symbol.
? The empty string ? can be derived from all RHS
nonterminal symbols.
The only difference for (2,2)-BRCGs is that pro-
duction rules are typically referred to as clauses in
the range concatenation grammar literature.
This paper considers some more complex exam-
ples; namely, the alignment problems wrt. 1 : 1-
alignments, (source-side and/or target-side) island-
free alignments and sure-possible sorted alignments.
The formal definitions of the three properties are as
follows:
Definition 1.1. An alignment structure for a string
pair ?w1 . . . wn, v1 . . . vm? is a graph D = ?V,E?
where V = Vs : {w1, . . . , wn} ? Vt : {v1, . . . , vm}
and E = Es : {wi ? wj | i < j} ? Et : {vi ? vj |
i < j} ? A where A ? Vs ? Vt. If (wi, vj) ? A,
also written wi|vj , wi is said to be aligned to vj ,
and vice versa. An alignment structure is said to
be wellformed iff for all wi, wj , vi? , vj? it holds that
if wi|vi? , wi|vj? and wj|vi? are aligned then so are
wj |vj? . An alignment structure is said to be 1 : 1 iff
no word occurs in two distinct tuples inA. An align-
ment structure is said to be island-free iff all words
in V occur in some tuple inA; it is said to be source-
side, resp. target-side, island-free if all words in Vs,
resp. Vt, occur in some tuple in A. The set of align-
ments is divided into sure and possible alignments,
i.e. A = S ? P (in most cases P = ?). An align-
ment structure is said to be sure-possible sorted iff if
it holds that (wi, vj?) ? S then for all wj , vi? neither
(wi, vi?) ? P nor (wj , vj?) ? P holds; similarly, if
it holds that (wi, vj?) ? P then for all wj , vi? neither
(wi, vi?) ? S nor (wj , vj?) ? S holds.
The precedence relations in E are not important
for any of our definitions, but are important for
meaningful interpretation of alignment structures.
Note that synchronous grammars are guaranteed to
induce wellformed alignment structures. Some brief
motivation for the properties singled out:
is a production rule with the start symbol in the LHS that in-
troduces all the words in the right order, since all production
rules with nonterminal symbols in the RHS are branching and
contain no terminal symbols.
61
Result 1 : 1 IF(s) IF(t) SP ITGs (2,2)-BRCGs
(1) X O(n6|G|) NP-complete
(2) X O(n6|G|) NP-complete
(3) X O(n6|G|) NP-complete
(4) X O(n6|G|) NP-complete
(5) X X O(n6|G|) NP-complete
(6) X X O(n6|G|) NP-complete
(7) X X O(n6|G|) NP-complete
(8) X X O(n6|G|) NP-complete
(9) X X O(n6|G|) NP-complete
(10) X X O(n6|G|) NP-complete
(11) X X X O(n6|G|) NP-complete
(12) X X X O(n6|G|) NP-complete
(13) X X X O(n6|G|) NP-complete
(14) X X X O(n6|G|) NP-complete
(15) X X X X O(n6|G|) NP-complete
Figure 1: The complexity of restricted alignment problems for ITGs and (2,2)-BRCGs.
? 1 : 1-alignments have been argued to be ad-
equate by Melamed (1999) and elsewhere, and
it may therefore be useful to know if a grammar
extracted from a parallel corpus produces 1 : 1-
alignments for a finite set of sentence pairs.
? Island-free alignments are interesting to the ex-
tent that unaligned nodes increase the chance of
translation errors. An island threshold may for
instance be used to rule out risky translations.
? The notion of sure-possible sorted alignments
is more unusual, but can, for instance, be used
to check if the use of possible alignments is
consistently triggered by words that are hard to
align.
The results for all cross-classifications of the
four properties ? 1 : 1, source-side island-free
(IF(s)), target-side island-free (IF(t)) and sure-
possible sorted (SP) ? are presented in the table in
Figure 1.3 Note that all (24 ? 1 = 15) combina-
tions of the four properties lead to NP-hard align-
ment problems for (2,2)-BRCGs. Consequently,
3One of our reviewers remarks that the Figure 1 is ?artifi-
cially blown up?, since all combinations have the same com-
plexity. It cannot really be left out, however. The numbers in
the figure?s left-most column serves as a reference in the proofs
below. Since the 15 results derive from only four proofs, it is
convenient to have a short-hand notation for the decision prob-
lems.
while the unrestricted alignment problem for (2,2)-
BRCGs can be solved in O(n6|G|), the alignment
problem turns NP-hard as soon as restrictions are put
on the alignments sought. So the extra expressivity
of (2,2)-BRCGs in a way comes at the expense of
control over the kind of alignments obtained.
On the structure of the paper: Sect. 2 and 3 briefly
introduce, resp., ITGs and (2,2)-BRCGs. Sect. 4
presents three NP-hardness proofs from which the
15 results in Figure 1 can be derived. The three
proofs are based on reconstructions of the Hamilton
circuit problem, the 3SAT problem and the vertex
cover problem (Garey and Johnson, 1979).
2 Inversion transduction grammars
Inversion transduction grammars (ITGs) (Wu, 1997)
are a notational variant of binary syntax-directed
translation schemas (Aho and Ullman, 1972) and are
usually presented with a normal form:
A ? [BC]
A ? ?BC?
A ? e | f
A ? e | ?
A ? ? | f
where A,B,C ? N and e, f ? T . The
first production rule, intuitively, says that the sub-
tree [[]B []C ]A in the source language translates into
62
a subtree [[]B []C ]A, whereas the second produc-
tion rule inverts the order in the target language,
i.e. [[]C []B ]A. The universal recognition problem of
ITGs can be solved in time O(n6|G|) by a CYK-
style parsing algorithm with two charts.
Figure 1 tells us that all the restricted alignment
problems listed can be solved in time O(n6|G|).
The explanation is simple. It can be read off from
the syntactic form of the production rules in ITGs
whether they introduce 1 : 1-alignments, island-free
alignments or sure-possible sorted alignments. Note
that normal form ITGs only induce 1 : 1-alignments.
Consider, for example, the following grammar,
not in normal form for brevity:
(1) S ? ?ASB? | ?AB?
(2) A ? a | a
(3) A ? a | ?
(4) B ? b | b
Note that this grammar recognizes the transla-
tion {?anbn, bnam | n ? m}. To check if for a
string pair ?w1 . . . wn, v1 . . . vm? this grammar in-
duces an island-free alignment, simply remove pro-
duction rule (3). It holds that only strings in the sub-
language {?anbn, bnan | n ? 1} induce island-free
alignments. Similarly, to check if the grammar in-
duces source-side island-free alignments for string
pairs, no production rules will have to be removed.
3 Two-variable binary bottom-up
non-erasing range concatenation
grammars
(2,2)-BRCGs are positive RCGs (Boullier, 1998)
with binary start predicate names, i.e. ?(S) = 2. In
RCG, predicates can be negated (for complementa-
tion), and the start predicate name is typically unary.
The definition is changed only for aesthetic rea-
sons; a positive RCG with a binary start predicate
name S is turned into a positive RCG with a unary
start predicate name S? simply by adding a clause
S?(X1X2) ? S(X1,X2).
A positive RCG is a 5-tuple G = ?N,T, V, P, S?.
N is a finite set of predicate names with an arity
function ?: N ? N, T and V are finite sets of, resp.,
terminal and variables. P is a finite set of clauses of
the form ?0 ? ?1 . . . ?m, where each of the ?i, 0 ?
i ? m, is a predicate of the form A(?1, . . . , ??(A)).
Each ?j ? (T ?V )?, 1 ? j ? ?(A), is an argument.
S ? N is the start predicate name with ?(S) = 2.
Note that the order of RHS predicates in a clause
is of no importance. Three subclasses of RCGs are
introduced for further reference: An RCG G =
?N,T, V, P, S? is simple iff for all c ? P , it holds
that no variable X occurs more than once in the
LHS of c, and if X occurs in the LHS then it oc-
curs exactly once in the RHS, and each argument
in the RHS of c contains exactly one variable. An
RCGG = ?N,T, V, P, S? is a k-RCG iff for all A ?
N, ?(A) ? k. Finally, an RCG G = ?N,T, V, P, S?
is said to be bottom-up non-erasing iff for all c ? P
all variables that occur in the RHS of c also occur in
its LHS.
A positive RCG is a (2,2)-BRCG iff it is a 2-RCG,
if an argument of the LHS predicate contains at most
two variables, and if it is bottom-up non-erasing.
The language of a (2,2)-BRCG is based
on the notion of range. For a string pair
?w1 . . . wn, vn+2 . . . vn+1+m? a range is a pair
of indices ?i, j? with 0 ? i ? j ? n or
n < i ? j ? n + 1 + m, i.e. a string span,
which denotes a substring wi+1 . . . wj in the source
string or a substring vi+1 . . . vj in the target string.
Only consequtive ranges can be concatenated into
new ranges. Terminals, variables and arguments
in a clause are bound to ranges by a substitution
mechanism. An instantiated clause is a clause in
which variables and arguments are consistently
replaced by ranges; its components are instantiated
predicates. For example A(?g . . . h?, ?i . . . j?) ?
B(?g . . . h?, ?i + 1 . . . j ? 1?) is an instantiation
of the clause A(X1, aY1b) ? B(X1, Y1) if the
target string is such that vi+1 = a and vj = b.
A derive relation =? is defined on strings of
instantiated predicates. If an instantiated predicate
is the LHS of some instantiated clause, it can be
replaced by the RHS of that instantiated clause. The
language of a (2,2)-BRCG G = ?N,T, V, P, S? is
the set L(G) = {?w1 . . . wn, vn+2 . . . vn+1+m? |
S(?0, n?, ?n + 1, n + 1 + m?) ?=? ?}, i.e. an
input string pair ?w1 . . . wn, vn+2 . . . vn+1+m? is
recognized iff the empty string can be derived from
S(?0, n?, ?n + 1, n + 1 +m?).
It is not difficult to see that ITGs are also (2,2)-
BRCGs. The left column is ITG production rules;
63
the right column their translations in simple (2,2)-
BRCGs.
A? [BC] A(X1X2, Y1Y2) ? B(X1, Y1)C(X2, Y2)
A? ?BC? A(X1X2, Y1Y2) ? B(X1, Y2)C(X2, Y1)
A? e | f A(e, f) ? ?
A? e | ? A(e, ?) ? ?
A? ? | f A(?, f) ? ?
Consequently, (2,2)-BRCGs recognize all trans-
lations recognized by ITGs. In fact the inclusion is
strict, as shown in S?gaard (2008). The universal
recognition problem of (2,2)-BRCGs can be solved
in time O(n6|G|) by the CYK-style parsing algo-
rithm presented in S?gaard (2008).
Example 3.1. Consider the (2,2)-BRCG G =
?{Ss, S0, S?0, S1, S?1, A,B,C,D}, {a, b, c, d}, {X1 ,
X2, Y1, Y2}, P, Ss? with P the following set of
clauses:
(1) Ss(X1, Y1) ? S0(X1, Y1)S?0(X1, Y1)
(2) S0(X1X2, Y1) ? S1(X1, Y1)D(X2)
(3) S1(aX1c, abY1) ? S1(X1, Y1)
(4) S1(X1, Y1Y2) ? B(X1)C(Y1)D(Y2)
(5) S?0(X1X2, Y1) ? S?1(X2, Y1)A(X1)
(6) S?1(bX1d, Y1cd) ? S?1(X1, Y1)
(7) S?1(X1, Y1Y2) ? C(X1)A(Y1)B(Y2)
(8) A(aX1) ? A(X1)
(9) A(?) ? ?
(10) B(bX1) ? B(X1)
(11) B(?) ? ?
(12) C(cX1) ? C(X1)
(13) C(?) ? ?
(14) D(dX1) ? D(X1)
(15) D(?) ? ?
The string pair ?abbcdd, abcdcd? is derived:
Ss(?0, 6?, ?0, 6?)
=? S0(?0, 6?, ?0, 6?)S?0(?0, 6?, ?0, 6?) (1)=? S1(?0, 4?, ?0, 6?)D(?4, 6?) (2)
S?0(?0, 6?, ?0, 6?)=? S1(?0, 4?, ?0, 6?)S?0(?0, 6?, ?0, 6?) (14?15)=? S1(?1, 3?, ?2, 6?)S?0(?0, 6?, ?0, 6?) (3)=? B(?1, 3?)C(?2, 4?)D(?4, 6?) (4)
S?0(?0, 6?, ?0, 6?)=? S?0(?0, 6?, ?0, 6?) (10?15)=? S?1(?1, 6?, ?0, 6?)A(?0, 1?) (5)=? S?1(?1, 6?, ?0, 6?) (8?9)=? S?1(?2, 5?, ?0, 4?) (6)=? S?1(?3, 4?, ?0, 2?) (6)=? C(?3, 4?)A(?0, 1?)B(?1, 2?) (7)
=? ? (8?13)
Note that L(G) = {?anbmcndm, (ab)n(cd)m? |
m,n ? 0}.
4 Results
4.1 Checking island-freeness and sure-possible
sortedness
One possible way to check for island-freeness and
sure-possible sortedness in the context of (2,2)-
BRCGs is to augment the CYK-style algorithm with
feature structures (Boolean vectors); all there is
needed, e.g. to check sure-possible sortedness, is to
pair up the nonterminals inserted in the cells of the
chart with a flat feature structure of the form:
?
???
SURE1 val1
.
.
.
SUREn valn
?
???
where n is the length of the source, resp. tar-
get, string in the source, resp. target, chart, and
1 ? i ? n : val i ? {+,?}. When a clause ap-
plies that induces a sure alignment between a word
wi and some word in the target, resp. source, string,
the attribute SUREi is assigned the value +; if a pos-
sible alignment is induced between wi and another
word, the attribute is assigned the value -. This can
all be done in constant time. A copying clause now
checks if the appropriate nonterminals have been in-
serted in the cells in question, but also that the as-
sociated feature structures unify. This can be done
in linear time. Feature structures can be used the
same way to record what words have been aligned to
check island-freeness. Unfortunately, this technique
does not guarantee polynomial runtime. Note that
there can be 2n many distinct feature structures for
each nonterminal symbol in a chart. Consequently,
whereas the size of a cell in the standard CYK algo-
rithm is bounded by |N |, and in synchronous parsing
by |N |? (2n? 1),4 the cells are now of exponential
size in the worst case.
The following three sections provide three NP-
hardness proofs: The first shows that the alignment
4The indices used to check that two nonterminals are derived
simultaneously (S?gaard, 2008) mean that it may be necessary
within a cell in the source, resp. target, chart to keep track of
multiple tuples with the same nonterminals. In the worst case,
there is a nonterminal for each span in the target, resp. source,
chart, i.e. 2n? 1 many.
64
problem wrt. 1 : 1-alignments is NP-hard for (2,2)-
BRCGs and goes by reduction of the Hamilton cir-
cuit problem for directed connected graphs. The sec-
ond shows that the alignment problem wrt. source-
or target-side island-free and sure-possible sorted
alignments is NP-hard for (2,2)-BRCGs and goes
by 3SAT reduction. The third proof is more general
and goes by reduction of the vertex cover problem.
All three formal decision problems are discussed in
detail in Garey and Johnson (1979). All 15 results
in Figure 1 are derived from modifications of these
proofs.
4.2 NP-hardness of the 1 : 1 restriction for
(2,2)-BRCGs
Theorem 4.1. The alignment problem wrt. 1 : 1-
alignments is NP-hard for (2,2)-BRCGs.
Proof. An instance of the Hamilton circuit problem
for directed connected graphs is simply a directed
connected graph G = ?V,E? and the problem is
whether there is a path that visits each vertex exactly
once and returns to its starting point? Consider, for
instance, the directed connected graph:
1 2
3
4 5
It is easy to see that there is no path in this case
that visits each vertex exactly once and returns to its
starting point. The intuition behind our reconstruc-
tion of the Hamilton circuit problem for directed
connected graphs is to check this via alignments be-
tween a sequence of all the vertices in the graph and
itself. The grammar permits an alignment between
two wordsw|v if there is a directed edge between the
corresponding nodes in the graph, e.g. (w, v) ? E.
The alignment structures below depict the possible
alignments induced by the grammar obtained by the
translation described below for our example graph:
1 2 3 4 5
1 2 3 4 5
1 2 3 4 5
1 2 3 4 5
1 2 3 4 5
1 2 3 4 5
1 2 3 4 5
1 2 3 4 5
Since no alignment above is 1 : 1, there is no
solution to the corresponding circuit problem. The
translation goes as follows:
? Add a rule S(X1, Y1) ? {Svi(X1, Y1) |
?vi.?vj.(vi, vj) ? E}.
? For each edge (vi, vj) ? E add
a rule Svi(X1viX2, Y1vjY2) ?
?(X1)?(X2)?(X3)?(X4).5
? For all vi ? V add a rule ?(viX1) ? ?(X1).
? Add a rule ?(?) ? ?.
The grammar ensures source-side island-freeness,
and therefore if there exists a 1 : 1-alignment of any
linearization of V and itself, by connectivity of the
input graph, there is a solution to the Hamilton cir-
cuit problem for directed connected graphs.
4.3 NP-hardness of island-freeness and
sure-possible sortedness for (2,2)-BRCGs
Theorem 4.2. The alignment problem wrt. target-
side island-free and sure-possible sorted alignments
is NP-hard for (2,2)-BRCGs.
Proof. An instance of the 3SAT problem is a propo-
sitional logic formula ? that is a conjunction of
clauses of three literals connected by disjunctions,
and the problem whether this formula is satisfiable,
i.e. has a model? Say ? = p?q?r?p??q??r?. For our
reconstruction, we use the propositional variables
in ? as source string, and ? itself with ??s omitted
and conjuncts as words as the target string. One of
the representations of a solution constructed by the
translation described below is the following align-
ment structure:
p q r
p ? q ? r p? ? q? ? r?
Solid lines are sure alignments; dotted lines are
possible alignments. The intuition is to use sure
alignments to encode true assignments, and possi-
ble alignments as false assignments. The alignment
5? is an arbitrary predicate name chosen to reflect the fact
that all possible substrings over the vocabulary are recognized
by the ? predicates.
65
above thus corresponds to the model {p, r?}, which
clearly satisfies ?.
For the translation, assume that each 3SAT in-
stance, over a set of propositional variables PROP,
consists of a set of clauses c1 . . . cm that are sets of
literals of size 3. For any literal lj , if lj = p?j then
pos(lj) = pj and lit(lj) = ?; and if lj = pj then
pos(lj) = pj and lit(lj) = +. If lj is a literal in
ci, we write lj ? ci. First add the following four
clauses:
Ss(X1, Y1) ? Ss(X1, Y1) | Sp(X1, Y1)
Sp(X1, Y1) ? Ss(X1, Y1) | Sp(X1, Y1)
? If lj ? ci and lit(lj) = ?, add
Sp(X1pos(lj)X2, Y1ciY2) ? ?(X1)?(X2)
?(Y1)?(Y2) .
? If lj ? ci and lit(lj) = +, add
Ss(X1pos(lj)X2, Y1ciY2) ? ?(X1)?(X2)
?(Y1)?(Y2) .
? For all pj , add ?(pjX1) ? ?(X1).
? For all ci, add ?(ciX1) ? ?(X1).
? Add a rule ?(?) ? ?.
It is easy to see that the first rule adds at most
7m clauses, which for the largest non-redundant
formulas equals 7((2|PROP|)3). The second rule
adds at most 2|PROP| clauses; and the third at most
m ? (2|PROP|)3 clauses. It is also easy to see that
the grammar induces a target-side island-free, sure-
possible sorted alignment if and only if the 3SAT in-
stance is satisfiable. Note that the grammar does not
guarantee that all induced alignments are target-side
island-free. Nothing, in other words, corresponds
to conjunctions in our reconstruction. This is not
necessary as long as there is at least one target-side
island-free alignment that is induced.
Note that the proof also applies in the case where
it is the source side that is required to be island-free.
All needed is to make the source string the target
string, and vice versa. Note also that the proof can
be modified for the case where both sides are island-
free: Just add a dummy symbol to the clause side
and allow (or force) all propositional variables to
be aligned to this dummy symbol. Consequently, if
there is a target-side (clause-side) island-free align-
ment there is also an island-free alignment. Re-
versely, if there is an island-free alignment there is
also a target-side island-free alignment of the string
pair in question.
Note also that a more general proof can be ob-
tained by introducing a clause, similar to the clause
introduced in the first bullet point of the Hamil-
ton circuit reduction in the proof of Theorem 4.1:
S(X1, Y1) ? {Sci(X1, Y1) | 1 ? i ? m}. The
four rules used to change between sure and pos-
sible alignments then of course need to be copied
out for all Sci predicates, and the LHS predicates,
except ?, of the other clauses must be properly
subscripted. Now the grammar enforces target-
side island-freeness, and sure-possible sortedness is
the only restriction needed on alignments. Conse-
quently, this reduction proves (4) that the alignment
problem wrt. sure-possible sortedness is NP-hard for
(2,2)-BRCGs.
4.4 NP-hardness of island-freeness for
(2,2)-BRCGs
Theorem 4.3. The alignment problem wrt. island-
free alignments is NP-hard for (2,2)-BRCGs.
Proof. An instance of the vertex problem is a graph
D = ?V,E? and an integer k, and the prob-
lem whether there exists a vertex cover of D of
size k? Say D = ?V = {a, b, c, d}, E =
{(a, c), (b, c), (b, d), (c, d)}? and k = 2. The trans-
lation described below constructs a sentence pair
??1?2?3?4uu????, aaaabbbbccccdddd?
for this instance, and a (2,2)-BRCG with the
clauses in Figure 2. Note that there are four kinds
of clauses:
? A clause with an S predicate in the LHS. In
general, there will be one such clause in the
grammar constructed for any instance of the
vertex cover problem.
? 8 clauses with ?i predicates in the LHS. In gen-
eral, there will be 2|E| many clauses of this
form in the grammars.
? 8 clauses withU i predicates in the LHS. In gen-
eral, there will be |V |? (|V |?k) many clauses
of this form in the grammars.
66
? 16 clauses with ?1 predicates in the LHS. In
general, there will be (|E| ? |V | ? |E| ? |E| ?
(|V | ? k)) ? |V | many clauses of this form in
the grammars.
For an instance ?D = ?V,E?, k?, the translation
function in general constructs the following clauses:
S(X1, Y1) ? {?i(X1, Y1) | 1 ? i ? |E|}?
{U |V |?k(X1, Y1)}?
{?|E|?|V |?|E|?|E|?(|V |?k)(X1, Y1)}
and for all 1 ? i ? |E| iff ei ? E = (e, e?):
?i(X1?iX2, Y1eY2) ? ?(X1)?(X2)?(Y1)?(Y2)
?i(X1?iX2, Y1e?Y2) ? ?(X1)?(X2)?(Y1)?(Y2)
For all 2 ? i ? |V | ? k and for all v ? V :
U i(X1UX2, Y1v . . . vY2) ? U i?1(X1, Y1)
?(X2)?(Y2)
where |v . . . v| = |E|. For the case U1, add the
clauses for all v ? V :
U1(X1UX2, Y1v . . . vY2) ? ?(X1)?(Y1)
?(X2)?(Y2)
The string pair is constructed this way:
??1 . . . ?|E|U1 . . . U|V |?k
?1 . . . ?|E|?|V |?|E|?|E|?(|V |?k), ??
Finally, for all words w in this string pair, add:
?(wX1) ? ?(X1)
Since this translation is obviously polynomial, it
follows that the alignment problem wrt. island-free
alignments for (2,2)-BRCGs is NP-hard.
Note that the proof also applies if only the source,
resp. target, side is required to be island-free, since
the grammar restricts the alignments in a way such
that if one side is island-free then so is the other side.
This gives us results (2) and (3).
It is not difficult to see either that it is possible
to convert the grammar into a grammar that induces
1 : 1-alignments. This gives us results (5), (8) and
(11). Of course by the observation that all the gram-
mars only use sure alignments, it follows that the
alignment problems in (7), (9?10) and (12?15) are
also NP-hard.
5 Conclusion
The universal recognition problems of both ITGs
and (2,2)-BRCGs can be solved in time O(n6|G|).
This may come as a surprise, as ITGs restrict the
alignment space considerably, while (2,2)-BRCGs
induce all possible alignments. In the context of
the NP-hardness of decoding in statistical machine
translation (Knight, 1999; Udupa and Maji, 2006),
it is natural to ask why the universal recognition
problem of (2,2)-BRCGs isn?t NP-hard? This pa-
per bridges the gap between these results and shows
that when alignments are restricted to be 1 : 1,
island-free or sure-possible sorted, or all combi-
nations thereof, the alignment problem of (2,2)-
BRCGs is NP-hard. Consequently, while the un-
restricted alignment problem for (2,2)-BRCGs can
be solved in O(n6|G|), the alignment problem turns
NP-hard as soon as restrictions are put on the align-
ments sought. So the extra expressivity in a way
comes at the expense of control over the kind of
alignments obtained. Note also that an alignment
of two words may be enforced multiple times in a
(2,2)-BRCGs parse, since two derivation trees that
share leaves on both sides can align the same two
words.
Our results are not intended to be qualifications of
the usefulness of (2,2)-BRCGs (S?gaard, 2008), but
rather they are attempts to bridge a gap in our under-
standing of the synchronous grammar formalisms at
hand to us in syntax-based machine translation.
67
S(X1, Y1) ? ?1(X1, Y1)?2(X1, Y1)
?3(X1, Y1)?4(X1, Y1)
U2(X1, Y1)?4(X1, Y1)
?1(X1?1X2, Y1aY2) ? ?(X1)?(X2)?(Y1)?(Y2)
?1(X1?1X2, Y1cY2) ? ?(X1)?(X2)?(Y1)?(Y2)
. . .
U2(X1UX2, aaaaY1) ? U1(X1, Y1)?(X2)
U1(X1UX2, Y1bbbbY2) ? ?(X1)?(Y1)?(X2)?(Y2)
U2(X1UX2, Y1bbbbY2) ? U1(X1, Y1)?(X2)?(Y2)
. . .
?4(X1?X2, Y1aY2) ? ?3(X1, Y1)?(X2)?(Y2)
?4(X1?X2, Y1bY2) ? ?3(X1, Y1)?(X2)?(Y2)
. . .
Figure 2: A (2,2)-BRCG for the instance of the vertex cover problem ??{a, b, c, d}, {(a, c), (b, c), (b, d), (c, d)}?, 2?.
References
Alfred Aho and Jeffrey Ullman. 1972. The theory
of parsing, translation and compiling. Prentice-Hall,
London, England.
Edward Barton, Robert Berwick, and Erik Ristad. 1987.
Computational complexity and natural language. MIT
Press, Cambridge, Massachusetts.
Pierre Boullier. 1998. Proposal for a natural language
processing syntactic backbone. Technical report, IN-
RIA, Le Chesnay, France.
Michael Garey and David Johnson. 1979. Computers
and intractability. W. H. Freeman & Co., New York,
New York.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607?615.
Dan Melamed. 1999. Bitext maps and alignment
via pattern recognition. Computational Linguistics,
25(1):107?130.
Anders S?gaard. 2008. Range concatenation gram-
mars for translation. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics, Companion Volume, pages 103?106, Manchester,
England.
Raghavendra Udupa and Hemanta Maji. 2006. Compu-
tational complexity of statistical machine translation.
In Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 25?32, Trento, Italy.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Hao Zhang and Daniel Gildea. 2004. Syntax-based
alignment: supervised or unsupervised? In Proceed-
ings of the 20th International Conference on Compu-
tational Linguistics, pages 418?424, Geneva, Switzer-
land.
68
