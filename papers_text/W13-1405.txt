Proceedings of the Second Workshop on Computational Linguistics for Literature, pages 32?40,
Atlanta, Georgia, June 14, 2013. c?2013 Association for Computational Linguistics
The desirability of a corpus of online book responses 
  
Peter Boot 
Huygens ING 
PO Box 90754 
2509 HT The Hague 
The  Netherlands 
peter.boot@huygens.knaw.nl 
 
 
 
Abstract 
This position paper argues the need for a 
comprehensive corpus of online book re-
sponses. Responses to books (in traditional 
reviews, book blogs, on booksellers? sites, 
etc.) are important for understanding how 
readers understand literature and how literary 
works become popular. A sufficiently large, 
varied and representative corpus of online re-
sponses to books will facilitate research into 
these processes. This corpus should include 
context information about the responses and 
should remain open to additional material. 
Based on a pilot study for the creation of a 
corpus of Dutch online book response, the 
paper shows how linguistic tools can find 
differences in word usage between responses 
from various sites. They can also reveal re-
sponse type by clustering responses based on 
usage of either words or their POS-tags, and 
can show the sentiments expressed in the re-
sponses. LSA-based similarity between book 
fragments and response may be able to reveal 
the book fragments that most affected read-
ers. The paper argues that a corpus of book 
responses can be an important instrument for 
research into reading behavior, reader re-
sponse, book reviewing and literary appre-
ciation.  
1 Introduction 
The literary system does not consist of authors 
and works alone. It includes readers (or listeners) 
and their responses to literary works. Research 
into reception is an important subfield of literary 
studies (e.g. Goldstein and Machor, 2008). Shared 
attention to stories may have evolved as way of 
learning to understand others and to increase 
bonding (Boyd, 2009).  Discussing literature may 
thus be something that we are wired to do, and 
that we do indeed wherever possible: today on 
Amazon, on weblogs, and on Twitter, and in ear-
lier days in newspapers and letters. These re-
sponses to books are important both as 
documentation of the ways literary works are read 
and understood, and because they help determine 
works? short- and long-term success. 
This position paper argues that what we need, 
therefore, is a large and representative corpus of 
book responses. ?Book response? in this paper 
includes any opinion that responds to a book, i.e. 
traditional book reviews, book-based discussion, 
opinions given on booksellers? sites, on Twitter, 
thoughtful blog posts, and the like. The word 
?books? here is meant to refer to all genres, in-
cluding literature as well as more popular genres 
such as fantasy, thrillers, comics, etc. Section 2 of 
the paper discusses the importance and research 
potential of book responses. Section 3 reviews 
related research. In section 4, I outline the proper-
ties that this corpus should have. Section 5 de-
scribes a Dutch pilot corpus and shows some 
aspects of this corpus that lend themselves to 
analysis with linguistic and stylometric tools. 
Section 6 presents conclusions and directions for 
future work.  
The author of this paper is not a computational 
linguist, but has a background in literary studies 
and digital humanities. The intention is to create a 
dialogue between literary studies and computa-
tional linguistics about fruitful ways to investigate 
book responses, their relations to the books they 
32
respond to and their effects on short-term or long-
term appreciation.  
2 Book responses and their importance 
Evaluating books and talking about our response 
is a very natural thing to do (Van Peer, 2008). In a 
professionalized form, the discipline of literary 
criticism has a long and distinguished tradition 
(Habib, 2005). But ?ordinary? readers too have 
always talked about their reading experiences 
(Long, 2003; Rehberg Sedo, 2003). The written 
output of these reflections and discussions has 
been an important source for reading and recep-
tion studies. Proof of this importance is e.g. the 
existence of the Reading Experience Database 
(RED) that collects experiences of reading as 
documented in letters, memoirs and other historic 
material (Crone et al, 2011). Halsey (2009) e.g. 
shows how this database can help study changes 
in stylistic preferences over time.  
One reason for the importance of written book 
responses is that they provide documentation of 
how works affect their readers: they show what 
elements of the reading experience readers con-
sider important enough to write down and share 
with friends and fellow-readers. To some extent at 
least this will be determined by the elements of 
the book that were most significant to the reader 
and that he or she is most likely to remember. 
Unlike in earlier historic periods, this sort of evi-
dence today is plentiful and researchers should 
take advantage of this. Spontaneous written re-
sponses to reading are not the only way of as-
sessing the effects of (literary) reading. 
Experimental research (Miall, 2006) and other 
approaches have an important place. Today?s 
online book responses, however, are unique in 
that they are produced spontaneously by ordinary 
readers and have an ecological validity that other 
research data lack. (Which does, of course, not 
imply we should take everything that people write 
online at face value). 
A second reason for the importance of written 
book responses is that their role as (co-)deter-
miners, or at least predictors, of literary success is 
well-documented. In the wake of a large body of 
research on movie reviews (e.g. Liu, 2006), this 
was established for reviews on booksellers? sites 
by (Chevalier and Mayzlin, 2006). For traditional 
(newspaper) reviews, their effects on long-term 
success (canonization) have been shown in e.g. 
(Ekelund and B?rjesson, 2002; Rosengren, 1987). 
If reading responses are that important for the 
study of literature and its effects, it follows we 
need to understand them better. We need tools 
that can analyze their style, rhetorical structure, 
topics, and sentiment, and these tools should be 
sensitive to the many different sorts of readers, 
responses and response sites that form part of the 
landscape of online book discussion. We also 
need tools that can help us see relationships be-
tween the responses and the works that they re-
spond to, in terms of topics and narrative (what 
characters and plot developments do reviewers 
respond to), as well as at higher (cognitive, emo-
tional and moral) levels. An important step to-
wards such tools is the creation of a representative 
corpus that can provide a test bed for tool devel-
opment. 
3 Related research 
Online book discussion is a wide field that can be 
studied from many different angles. I discuss first 
a number of studies that do not use computational 
methods. Online book reviewing has often been 
discussed negatively in its relation to traditional 
reviews (McDonald, 2007; Pool, 2007). Certainly 
problematic aspects of online reviews are the 
possibilities of plagiarism and fraud (David and 
Pinch, 2006). Verboord (2010) uses a question-
naire to investigate the perceived legitimacy of 
internet critics. Online critics? role in canonization 
was investigated in (Grafton, 2010). That online 
reviews do have an influence on books sales was      
established by (Chevalier and Mayzlin, 2006), 
and specifically for books by women and popular 
fiction in (Verboord, 2011). Many librarians have 
looked at what online book discussion sites can 
mean for the position of the library, library cata-
loguing and book recommendations (Pera and Ng, 
2011; Pirmann, 2012). Online book discussion as 
an extension of the reading group is discussed in 
e.g. (Fister, 2005). A look at the whole field, from 
a genre perspective, is given in (Boot, 2011). 
Steiner (2010) looks specifically at Swedish web-
logs; (Steiner, 2008) discusses Amazon reviews, 
as does (Domsch, 2009). Gutjahr (2002) sent out 
a survey to posters of Amazon reviews. Finally, 
(Miller, 2011) investigates how book blogs can 
33
help develop the habits of mind required for liter-
ary reading. 
 Researchers that have used more or less so-
phisticated linguistic technology to investigate 
online book responses have done so with a num-
ber of different questions in mind. (Boot et al, 
2012) sought to characterize responses from dif-
ferent site types based on word usage. Much ef-
fort has gone into the analysis of review 
sentiment, which has clear practical applications 
in marketing. (Taboada et al, 2011) use a lexicon-
based approach; (Okanohara and Tsujii, 2005) a 
machine learning approach. (De Smedt and 
Daelemans, 2012a) create a Dutch sentiment lexi-
con based on reviews at an online bookseller. The 
helpfulness of online reviews has been investigat-
ed by e.g. (Tsur and Rappoport, 2009) while 
(Mukherjee and Liu, 2012) have modeled review 
comments. From an information retrieval perspec-
tive, the INEX social book search competition has 
explored the use of online reviews from Amazon 
and LibraryThing to create book recommenda-
tions (Koolen et al, 2012). A proposal for using 
text mining and discourse analysis techniques on 
pre-internet reviews is (Taboada et al, 2006). 
(Finn, 2011) used named entity recognition in 
reviews of a single writer in order to explore the 
?ideational network? associated with her work.  
It does not seem unfair to say that most of the 
computer-based linguistic research done into 
online book responses has been motivated by 
practical, if not commercial aims. Much of it was 
published in marketing journals. Computational 
linguistic research as a tool for understanding the 
variety of online book response is still at a very 
early stage of development.  
4 A corpus of book responses  
A corpus of book responses should present re-
searchers with a varied, representative, and suffi-
ciently large collection of book responses. It 
should not be a closed corpus but continue to 
grow. It should contain not just response texts but 
also include the metadata that describes and con-
textualizes the responses.  
Varied: the responses should be taken from as 
wide a selection of sites as is possible. Sites are 
very different with regards to the active review-
ers, their audience, the books that are discussed, 
the responses? function and the explicit and im-
plicit expectations about what constitutes a proper 
response (Boot, 2011). Pragmatic aspects of the 
response (e.g. a response given on a weblog 
where the responder is the main author vs. a re-
sponse in a forum where the responder is just one 
participant in a group discussion) obviously help 
determine both content and style of the response 
and tools that analyze responses should take ac-
count of these differences in setting.  
Another respect in which variety is important 
is book genre. Much has been written about dif-
ferences in book appreciation between e.g. read-
ers of popular fiction and ?high? literature (Von 
Heydebrand and Winko, 1996). A response cor-
pus should present researchers with a large body 
of responses from readers of a wide selection of 
genres (popular fiction, literature, non-fiction, 
essays, poetry, etc.), irrespective of its medium of 
publication (paper, e-book, online). 
Representative: there is no need for this corpus 
to be strictly proportional with respect to site type 
or book genre. Still, it is important for all types 
and genres to be represented. Given the need to 
request permission from copyright holders, it will 
probably be impossible to achieve a truly repre-
sentative corpus.     
Sufficiently large: the required size of the cor-
pus will depend on the sort of analysis that one 
tries to do. It is clear that analysis that goes be-
yond the collection level, e.g. at the book genre 
level, or at the level of individual reviewers, will 
need substantial amounts of text. A rule of thumb 
might be that collections should preferably con-
tain more than a thousand responses and more 
than a million words.  
Open: As new forms of computer-mediated 
communication continue to evolve, the ways of 
responding to and talking about books will also 
change. The corpus should facilitate research into 
these changes, and be regularly updated with col-
lections from new site types.  
Metadata: book response text acquires a large 
part of its meaning from its context. To facilitate 
research into many aspects of these responses it is 
important for the corpus to store information 
about that context. That information should in-
clude at least the site that the response was taken 
from, the response date, whatever can be known 
about the author of the response, and, if available, 
the book that the response responds to. Figure 
1shows the relevant entities. 
34
We will not discuss the data model in detail. 
Sites can contain multiple collections of respons-
es, with different properties. Some sites for in-
stance contain both commissioned reviews and 
user reviews. Weblogs contains posts by the blog 
owner and responses to those posts. Book theme 
sites often carry review sections and discussion 
forums. When analyzing a response, it is im-
portant to be aware what section the response 
belongs to. Book responses can also be written in 
response to other posts, be it in a discussion fo-
rum, on Twitter, or on a book-based social net-
working site. Book responses can be tagged, and 
the tags may carry valuable information about 
book topics, book appreciation or other book in-
formation. Responses are written by persons, 
sometimes unknown, who may own a site (as with 
blogs) or be among many people active on a site, 
or perhaps on multiple sites. Reviewers some-
times write profile texts about themselves that 
also discuss their book preferences. On some sites 
(book SNS?s, Twitter) reviewers may strike up 
friendships or similar relationships. Some sites 
also allow people to list the books they own 
and/or their favorite books. Finally, meaningful 
use of book level data will often require being 
able to group multiple versions (manifestations) 
of the same work.  
 
 
Figure 1. Book response corpus data model 
 
For most collections, extracting the information 
carried by the respective entities mentioned is not 
a trivial task. Book shop review pages will proba-
bly contain an ISBN somewhere near the review, 
but forums probably will not and a tweet with an 
ISBN number is certainly unusual. And even if a 
response is ostensibly about book A, it may very 
well also discuss book B. Reviewer information 
will also be hard to obtain, as many reviews (e.g. 
on booksellers? sites) are unsigned.  
5 Pilot study 
For a pilot study that explores the research poten-
tial of online book response, I have been collect-
ing Dutch-language book responses from a 
number of sites. The size of the pilot corpus and 
its subcollections is given in table 1. The pilot 
corpus contains responses from a number of web-
logs, from online review magazine 8Weekly, 
book-based social network site watleesjij.nu 
(?whatareyoureading.now)?, book publicity, re-
views  and user reviews from thriller site Crime-
zone, a collection of print reviews (from multiple 
papers and magazines) about Dutch novelist Ar-
non Grunberg, print reviews from Dutch newspa-
per NRC and publicity from the NRC web shop. 
The collection should be extended with responses 
from other site types (e.g. forums, twitter, 
bookseller reviews) other book genres (e.g. fanta-
sy, romance, poetry) and perhaps other text genres 
(e.g. book news, interviews). 
 
Collection Article 
genre 
Res-
ponse 
count 
Word 
count 
(*1000) 
8weekly  review 2273 1512 
weblogs blog post  6952 3578 
watleesjij.nu user review 28037 2515 
crimezone book desc publicity 3698 462 
crimezone review review 3696 1622 
crimezone userrev user review 9163 1537 
grunberg print review 196 187 
NRC web shop publicity 1345 198 
NRC reviews print review 1226 1133 
Total 56586 12744 
Table 1. Present composition of pilot corpus of 
responses 
 
I have done a number of experiments in order to 
explore the potential for computational linguistic 
analysis of book responses. 
35
5.1 Measure response style and approach 
using LIWC   
As a first test, I investigated word usage in the 
book responses using LIWC (Pennebaker et al, 
2007; Zijlstra et al, 2004). Figure 2 shows the 
usage of first person pronouns on the respective 
site types. The pattern conforms to what one 
would expect: on the book SNS watleesjij.nu, 
where readers give personal opinions, ?I? predom-
inates, as it does in the Crimezone user reviews, 
and to a lesser extent in the weblogs. In the com-
missioned reviews both in print (NRC newspaper 
and Grunberg collection) and online (8Weekly) 
?we? prevails, as reviewers have to maintain an 
objective stance. Interestingly, the Crimezone 
book descriptions manage to avoid first person 
pronouns almost completely. 
 
 
Figure 2. Normalized frequencies first person singular 
and first person plural pronouns 
 
 
Figure 3. Positive and negative emotion word frequen-
cies 
 
A similar result appears when we chart positive 
and negative emotion words (Figure 3). Especial-
ly positive emotions are often expressed on wat-
leesjij.nu and in the Crimezone user reviews. In 
this case the group of informal sites does not in-
clude the weblogs, perhaps because the weblogs 
included in the pilot corpus are blogs at the intel-
lectual end of the spectrum. Also interesting is the 
high proportion of negative emotion in the Crime-
zone book descriptions, perhaps because in the 
case of thrillers emotions like fear and anxiety can 
function as recommendations.  
From these examples it is clear that word us-
age on the respective sites shows meaningful var-
iation that will profit from further research. 
Investigation into these patterns at the level of 
individual reviewers (e.g. bloggers) should begin 
to show individual styles of responding to litera-
ture.  
5.2 Site stylistic similarities  
As a second test, I looked into writing style, ask-
ing whether the styles on the respective sites are 
sufficiently recognizable to allow meaningful 
clustering. For each of the collections, except for 
the weblogs, I created five files of 20000 words 
each and used the tools for computational stylom-
etry described in (Eder and Rybicki, 2011) to 
derive a clustering, based on the 300 most fre-
quent words. Figure 4 shows the results.  
It is interesting to note that all except the wat-
leesjij.nu (book SNS) samples are stylistically 
consistent enough to be clustered by themselves. 
It is even more interesting to note that the book 
descriptions from the NRC (newspaper) shop 
cluster with the descriptions taken from the 
Crimezone site, that the reviews in online maga-
zine 8Weekly cluster with the printed reviews, 
and that the Crimezone reviews, commissioned 
and user-contributed, cluster with the wat-
leesjij.nu reviews. This may be related to the fact 
that there are a large number of thriller aficiona-
dos on watleesjij.nu, or to Crimezone reviews 
being significantly different from traditional re-
views. Again, this seems a fruitful area for further 
investigation, only possible in the context of a 
large corpus containing different text types. 
In order to exclude the possibility that this 
clustering is based on content words (e.g. words 
related to crime), I repeated the experiment using 
bi-grams of the words? POS-tags, as derived by 
the Pattern toolset (De Smedt and Daelemans, 
2012b). The resulting figure, not reproduced here, 
is very similar to Figure 4. This result leads to 
another question: what sort of syntactic construc-
0
1
2
3
4
5
6
negemo
posemo
36
tions are specific to which site types? And can we 
connect these stylistic differences to the approach 
to literature that these sites take?  
 
 
 
Figure 4. Clustering of 20000-word review texts based 
on 300 most frequent words. 
5.3 Response sentiment analysis 
In a third experiment, I applied the sentiment 
lexicon embedded in the Pattern toolset to the 
responses in those collections that include ratings. 
I predict a positive rating (i.e. above or equal to 
the collection median) when the sentiment as 
measured by Pattern is above 0.1, and compute 
precision, recall and F1-value for this prediction 
(see Figure 5). Results on the book SNS wat-
leesjij.nu are similar to the results reported by (De 
Smedt and Daelemans, 2012a) for reviews from 
bookseller bol.com, perhaps because the respons-
es on the two sites are similar. As expected, the 
results are considerably worse for the longer re-
views on 8Weekly and NRC. That precision 
should be as high as .84 for the Crimezone re-
views is somewhat of a mystery.  
While it is not unexpected that the sentiment 
prediction quality should be higher for the sites 
with simpler reviews, this does imply a challenge 
for researchers of sentiment analysis. Without 
accurately gauging response sentiment (and many 
other response properties) measuring literary im-
pact from responses will remain illusory.  
 
 
Figure 5. Prediction of positive or negative rating: 
precision, recall and F-score 
5.4 Semantic similarities between book 
fragments and responses 
A final experiment is based on the assumption 
that the semantics of book response texts to some 
extent reflect the semantics of the books they 
respond to. If that is true, it should be possible to 
determine the chapters that most impressed read-
ers by comparing the book?s and the reviews? 
semantic content. In order to test the assumption, 
I used Latent Semantic Analysis (LSA) (Landauer 
et al, 2007; ?eh??ek and Sojka, 2010) to measure 
the distances between 400-word fragments taken 
from the novel Tirza by Dutch novelist Arnon 
Grunberg and 60 reviews of the book taken from 
book SNS watleesjij.nu. In order to compensate 
for potential similarities between book fragments 
and any reviews, rather than with reviews specifi-
cally of this book, I also measured semantic dis-
tances between the book?s fragments and a set of 
random reviews from the same site, and subtract-
ed those from the distances with the Tirza re-
views. In order to test how these distances relate 
0,5
0,6
0,7
0,8
0,9
P
R
F
37
to the book?s content, I computed LIWC scores 
for the fragments and then correlations between 
these LIWC scores and the LSA distances. For 
e.g. LIWC category ?family?, a very important 
subject for this book, the correlation is positive 
and highly significant (.34, p< .0001).  
Further experimentation with other books, oth-
er review collections and other LSA models is 
clearly needed. It is too early to say whether LSA 
indeed offers a viable approach for determining 
the book fragments most closely related to review 
texts, but this is clearly a promising result. Being 
able to connect measurable aspects of books with 
impact in reviews would help us understand how 
books affect their readers. 
6 Conclusion 
This paper adopts a broad conception of the ob-
ject of literary studies, taking it to include the 
individual and social responses that literature 
elicits. I argued here that the (plentifully availa-
ble) online book responses are important to liter-
ary studies, both as evidence (because they 
document the reception of literary works) and as 
objects (because they help determine works? short 
and long term popularity). If only because of the 
numbers of these responses, we need computa-
tional linguistic tools in order to analyze and un-
derstand them. Because the responses published 
on the various response platforms are in many 
respects very different, potential tools would need 
to be developed with these differences in mind. A 
good way to ensure this is to create an appropri-
ately large and representative corpus of online 
book response. On the basis of a Dutch pilot cor-
pus we saw that existing linguistic tools can re-
veal some of the differences between the 
respective platforms. They are currently unable, 
however, to perform any deeper analysis of these 
differences, let alne a deeper analysis of the rela-
tions between responses and books. 
Naturally, written book response can only in-
form us about the reading experience of those that 
take the trouble of writing down and publishing 
their response. Even though those who provide 
book response are by no means a homogeneous 
group, it is clear that the proposed corpus would 
necessarily be selective, and should not be our 
only method of studying reader response. This is 
less of an issue when studying how books become 
popular and eventually canonized, as those who 
don?t participate in the discussions will, for that 
very reason, be less influential. 
With these caveats, there are a number of areas 
that a corpus of online book response would help 
investigate. Among these are:  
? the responses themselves and their respective 
platforms: what language is used, what topics 
are discussed, what is their structure? What 
do they reveal about the literary norms that 
(groups of) readers apply? 
? the relations between responses: we should be 
able to answer the questions about influence. 
What sort of discussions are going on about 
literature on which platforms? Which partici-
pants are most influential? Can response 
styles reveal these influences?  
? what the responses show about the reading 
experience: we?d like to know how books 
(both books in general and specific books) af-
fect people, what attracts people in books, 
what they remember from books, what they 
like about them, etc. What passages do they 
quote from the books they respond to? What 
characteristic words do they adopt? 
? what the responses show about readers: as the 
corpus should facilitate selection by respond-
er, we should be able to investigate the role of 
the reader in book response. Do responders? 
writing styles predict their ratings? Do people 
who like, say, James Joyce dislike science fic-
tion? And can their book responses tell us 
why?  
 
Many of these phenomena are interesting at mul-
tiple levels. They are interesting at the level of the 
individual reader, for whom reading in general 
and specific books are important. They are inter-
esting at a sociological level, as discussions help 
determine books? popularity or even canonization. 
Finally, at the level of the book, study of book 
responses can show what readers, individually 
and in groups, take away from a book. In this 
respect especially, study of book responses is a 
necessary complement to study of the literary 
text.  
38
References 
 
Boot, Peter. 2011. Towards a Genre Analysis of Online 
Book Discussion: socializing, participation 
and publication in the Dutch booksphere. 
Selected Papers of Internet Research IR 12.0. 
Boot, Peter, Van Erp, Marieke, Aroyo, Lora, and 
Schreiber, Guus. 2012. The changing face of 
the book review. Paper presented at Web 
Science 2012, Evanston (IL). 
Boyd, Brian. 2009. On the origin of stories: Evolution, 
cognition, and fiction. Cambridge MA: 
Harvard University Press. 
Chevalier, Judith A., and Mayzlin, Dina. 2006. The 
effect of word of mouth on sales: Online book 
reviews. Journal of Marketing Research 
43:345-354. 
Crone, Rosalind, Halsey, Katry, Hammond, Mary, and 
Towheed, Shafquat. 2011. The Reading 
Experience Database 1450-1945 (RED). In 
The history of reading. A reader, eds. 
Shafquat Towheed, Rosalind Crone and Katry 
Halsey, 427-436. Oxon: Routledge. 
David, Shay, and Pinch, Trevor. 2006. Six degrees of 
reputation: The use and abuse of online 
review and recommendation systems. First 
Monday 11. 
De Smedt, Tom, and Daelemans, Walter. 2012a. 
?Vreselijk mooi!? (terribly beautiful): A 
Subjectivity Lexicon for Dutch Adjectives. 
Paper presented at Proceedings of the 8th 
Language Resources and Evaluation 
Conference (LREC?12). 
De Smedt, Tom, and Daelemans, Walter. 2012b. 
Pattern for Python. The Journal of Machine 
Learning Research 13:2031-2035. 
Domsch, Sebastian. 2009. Critical genres. Generic 
changes of literary criticism in computer-
mediated communication. In Genres in the 
Internet: issues in the theory of genre, eds. 
Janet Giltrow and Dieter Stein, 221-238. 
Amsterdam: John Benjamins Publishing 
Company. 
Eder, Maciej, and Rybicki, Jan. 2011. Stylometry with 
R. In Digital Humanities 2011: Conference 
Abstracts, 308-311. Stanford University, 
Stanford, CA. 
Ekelund, B. G., and B?rjesson, M. 2002. The shape of 
the literary career: An analysis of publishing 
trajectories. Poetics 30:341-364. 
Finn, Edward F. 2011. The Social Lives of Books: 
Literary Networks in Contemporary American 
Fiction, Stanford University: PhD. 
Fister, Barbara. 2005. Reading as a contact sport. 
Reference & User Services Quarterly 44:303-
309. 
Goldstein, Philip, and Machor, James L. 2008. New 
directions in American reception study. New 
York: Oxford University Press, USA. 
Grafton, Kathryn. 2010. Paying attention to public 
readers of Canadian literature: popular genre 
systems, publics, and canons, University of 
British Columbia: PhD. 
Gutjahr, Paul C. 2002. No Longer Left Behind: 
Amazon.com, Reader-Response, and the 
Changing Fortunes of the Christian Novel in 
America. Book History 5:209-236. 
Habib, M. A. R. 2005. A history of literary criticism: 
from Plato to the present. Malden, MA: 
Blackwell. 
Halsey, Katie. 2009. ?Folk stylistics? and the history of 
reading: a discussion of method. Language 
and Literature 18:231-246. 
Koolen, Marijn, Kamps, Jaap, and Kazai, Gabriella. 
2012. Social Book Search: Comparing 
Topical Relevance Judgements and Book 
Suggestions for Evaluation. In CIKM?12, 
October 29?November 2, 2012. Maui, HI, 
USA. 
Landauer, T. K., McNamara, D. S., Dennis, S., and 
Kintsch, W. 2007. Handbook of latent 
semantic analysis: Lawrence Erlbaum. 
Liu, Yong. 2006. Word of Mouth for Movies: Its 
Dynamics and Impact on Box Office 
Revenue. Journal of Marketing 70:74-89. 
Long, Elizabeth. 2003. Book clubs: Women and the 
uses of reading in everyday life. Chicago: 
University of Chicago Press. 
McDonald, R?n?n. 2007. The death of the critic. 
London, New York: Continuum International 
Publishing Group. 
Miall, David S. 2006. Literary reading: empirical & 
theoretical studies. New York: Peter Lang 
Publishing. 
Miller, Donna L. 2011. Talking with Our Fingertips: 
An Analysis for Habits of Mind in Blogs 
about Young Adult Books, Arizona State 
University: PhD. 
Mukherjee, Arjun, and Liu, Bing. 2012. Modeling 
Review Comments. In Proceedings of 50th 
Anunal Meeting of Association for 
Computational Linguistics (ACL-2012). Jeju 
(Korea). 
Okanohara, Daisuke, and Tsujii, Jun?ichi. 2005. 
Assigning polarity scores to reviews using 
machine learning techniques. Natural 
Language Processing?IJCNLP 2005:314-325. 
Pennebaker, J. W., Booth, R. J., and Francis, M. E. 
2007. Linguistic Inquiry and Word Count 
(LIWC2007). Austin, TX. 
Pera, Maria Soledad, and Ng, Yiu-Kai. 2011. With a 
Little Help from My Friends: Generating 
39
Personalized Book Recommendations Using 
Data Extracted from a Social Website. Paper 
presented at Web Intelligence and Intelligent 
Agent Technology (WI-IAT), 2011. 
Pirmann, Carrie. 2012. Tags in the Catalogue: Insights 
From a Usability Study of LibraryThing for 
Libraries. Library Trends 61:234-247. 
Pool, Gail. 2007. Faint praise: the plight of book 
reviewing in America. Columbia, MO: 
University of Missouri Press. 
Rehberg Sedo, DeNel. 2003. Readers in Reading 
Groups. An Online Survey of Face-to-Face 
and Virtual Book Clubs. Convergence 9:66-
90. 
?eh??ek, Radim, and Sojka, Petr. 2010. Software 
framework for topic modelling with large 
corpora. Paper presented at Proceedings of 
LREC 2010 workshop New Challenges for 
NLP Frameworks, Valletta, Malta. 
Rosengren, Karl Erik. 1987. Literary criticism: Future 
invented. Poetics 16:295-325. 
Steiner, Ann. 2008. Private Criticism in the Public 
Space: Personal writing on literature in 
readers' reviews on Amazon. Participations 5. 
Steiner, Ann. 2010. Personal Readings and Public 
Texts: Book Blogs and Online Writing about 
Literature. Culture unbound 2:471-494. 
Taboada, Maite, Gillies, Mary Ann, and McFetridge, 
Paul. 2006. Sentiment Classification 
Techniques for Tracking Literary Reputation. 
In Proceedings of LREC 2006 Workshop 
?Towards Computational Models of Literary 
Analysis?. 
Taboada, Maite, Brooke, Julian, Tofiloski, Milan, Voll, 
Kimberly, and Stede, Manfred. 2011. 
Lexicon-based methods for sentiment 
analysis. Computational Linguistics 37:267-
307. 
Tsur, Oren, and Rappoport, Ari. 2009. Revrank: A 
fully unsupervised algorithm for selecting the 
most helpful book reviews. Paper presented at 
International AAAI Conference on Weblogs 
and Social Media. 
Van Peer, Willie. 2008. Introduction. In The quality of 
literature: linguistic studies in literary 
evaluation, 1-14. Amsterdam: John 
Benjamins Publishing Co. 
Verboord, Marc. 2010. The Legitimacy of Book Critics 
in the Age of the Internet and 
Omnivorousness: Expert Critics, Internet 
Critics and Peer Critics in Flanders and the 
Netherlands. European Sociological Review 
26:623-637. 
Verboord, Marc. 2011. Cultural products go online: 
Comparing the internet and print media on 
distributions of gender, genre and commercial 
success. Communications 36:441-462. 
Von Heydebrand, Renate, and Winko, Simone. 1996. 
Einf?hrung in die Wertung von Literatur: 
Systematik, Geschichte, Legitimation. 
Paderborn: Sch?ningh. 
Zijlstra, Hanna, van Meerveld, Tanja, van Middendorp, 
Henri?t, Pennebaker, James W., and Geenen, 
Rinie. 2004. De Nederlandse versie van de 
?Linguistic Inquiry and Word Count? (LIWC). 
Gedrag & Gezondheid 32:271-281. 
 
 
40
