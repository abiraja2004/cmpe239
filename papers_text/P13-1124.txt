Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1264?1274,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Two-Neighbor Orientation Model with Cross-Boundary Global Contexts
Hendra Setiawan, Bowen Zhou, Bing Xiang and Libin Shen
IBM T.J.Watson Research Center
1101 Kitchawan Road
Yorktown Heights, NY 10598, USA
{hendras,zhou,bxiang,lshen}@us.ibm.com
Abstract
Long distance reordering remains one of
the greatest challenges in statistical ma-
chine translation research as the key con-
textual information may well be beyond
the confine of translation units. In this
paper, we propose Two-Neighbor Orien-
tation (TNO) model that jointly models
the orientation decisions between anchors
and two neighboring multi-unit chunks
which may cross phrase or rule bound-
aries. We explicitly model the longest
span of such chunks, referred to as Max-
imal Orientation Span, to serve as a
global parameter that constrains under-
lying local decisions. We integrate our
proposed model into a state-of-the-art
string-to-dependency translation system
and demonstrate the efficacy of our pro-
posal in a large-scale Chinese-to-English
translation task. On NIST MT08 set, our
most advanced model brings around +2.0
BLEU and -1.0 TER improvement.
1 Introduction
Long distance reordering remains one of the great-
est challenges in Statistical Machine Translation
(SMT) research. The challenge stems from the
fact that an accurate reordering hinges upon the
model?s ability to make many local and global
reordering decisions accurately. Often, such
reordering decisions require contexts that span
across multiple translation units.1 Unfortunately,
previous approaches fall short in capturing such
cross-unit contextual information that could be
1We define translation units as phrases in phrase-based
SMT, and as translation rules in syntax-based SMT.
critical in reordering. Specifically, the popular dis-
tortion or lexicalized reordering models in phrase-
based SMT focus only on making good local pre-
diction (i.e. predicting the orientation of imme-
diate neighboring translation units), while transla-
tion rules in syntax-based SMT come with a strong
context-free assumption, which model only the re-
ordering within the confine of the rules. In this
paper, we argue that reordering modeling would
greatly benefit from richer cross-boundary contex-
tual information
We introduce a reordering model that incorpo-
rates such contextual information, named the Two-
Neighbor Orientation (TNO) model. We first iden-
tify anchors as regions in the source sentences
around which ambiguous reordering patterns fre-
quently occur and chunks as regions that are con-
sistent with word alignment which may span mul-
tiple translation units at decoding time. Most no-
tably, anchors and chunks in our model may not
necessarily respect the boundaries of translation
units. Then, we jointly model the orientations of
chunks that immediately precede and follow the
anchors (hence, the name ?two-neighbor?) along
with the maximal span of these chunks, to which
we refer as Maximal Orientation Span (MOS).
As we will elaborate further in next sections,
our models provide a stronger mechanism to make
more accurate global reordering decisions for the
following reasons. First of all, we consider the
orientation decisions on both sides of the anchors
simultaneously, in contrast to existing works that
only consider one-sided decisions. In this way, we
hope to upgrade the unigram formulation of exist-
ing reordering models to a higher order formula-
tion. Second of all, we capture the reordering of
chunks that may cross translation units and may
be composed of multiple units, in contrast to ex-
1264
isting works that focus on the reordering between
individual translation units. In effect, MOS acts as
a global reordering parameter that guides or con-
strains the underlying local reordering decisions.
To show the effectiveness of our model, we
integrate our TNO model into a state-of-the-
art syntax-based SMT system, which uses syn-
chronous context-free grammar (SCFG) rules to
jointly model reordering and lexical translation.
The introduction of nonterminals in the SCFG
rules provides some degree of generalization.
However as mentioned earlier, the context-free
assumption ingrained in the syntax-based for-
malism often limits the model?s ability to in-
fluence global reordering decision that involves
cross-boundary contexts. In integrating TNO, we
hope to strengthen syntax-based system?s ability
to make more accurate global reordering deci-
sions.
Our other contribution in this paper is a prac-
tical method for integrating the TNO model into
syntax-based translations. The integration is non-
trivial since the decoding of syntax-based SMT
proceeds in a bottom-up fashion, while our model
is more natural for top-down parsing, thus the
model?s full context sometimes is often available
only at the latest stage of decoding. We implement
an efficient shift-reduce algorithm that facilitates
the accumulation of partial context in a bottom-up
fashion, allowing our model to influence the trans-
lation process even in the absence of full context.
We show the efficacy of our proposal in a large-
scale Chinese-to-English translation task where
the introduction of our TNO model provides a
significant gain over a state-of-the-art string-to-
dependency SMT system (Shen et al, 2008) that
we enhance with additional state-of-the-art fea-
tures. Even though the experimental results car-
ried out in this paper employ SCFG-based SMT
systems, we would like to point out that our mod-
els is applicable to other systems including phrase-
based SMT systems.
The rest of the paper is organized as follows.
In Section 2, we introduce the formulation of our
TNO model. In Section 3, we introduce and moti-
vate the concept of Maximal Orientation Span. In
Section 4, we introduce four variants of the TNO
model with different model complexities. In Sec-
tion 5, we describe the training procedure to esti-
mate the parameters of our models. In Section 6,
we describe our shift-reduce algorithm which inte-
grates our proposed TNO model into syntax-based
SMT. In Section 7, we describe our experiments
and present our results. We wrap up with related
work in Section 8 and conclusion in Section 9.
2 Two-Neighbor Orientation Model
Given an aligned sentence pair ? = (F,E,?), let
?(?) be all possible chunks that can be extracted
from ? according to: 2
{(f j2j1/e
i2
i1) :?j1? j? j2,?i : (j, i)??, ii? i? i2 ?
?i1? i? i2,?j : (j, i)??, ji? j?j2}
Our Two-Neighbor Orientation model (TNO)
designatesA ? ?(?) as anchors and jointly mod-
els the orientation of chunks that appear immedi-
ately to the left and to the right of the anchors as
well as the identities of these chunks. We define
anchors as chunks, around which ambiguous re-
ordering patterns frequently occur. Anchors can
be learnt automatically from the training data or
identified from the linguistic analysis of the source
sentence. In our experiments, we use a simple
heuristics based on part-of-speech tags which will
be described in Section 7.
More concretely, given A ? ?(?), let a =
(f j2j1/e
i2
i1) ? A be a particular anchor. Then, let
CL(a) ? ?(?) be a?s left neighbors and let
CR(a) ? ?(?) be a?s right neighbors, iff:
?CL = (f j4j3/e
i4
i3) ? CL(a) : j4 + 1 = j1 (1)
?CR = (f j6j5/e
i6
i5) ? CR(a) : j2 + 1 = j5 (2)
Given CL(a) and CR(a), let CL = (f j4j3/ei4i3) and
CR = (f j6j5/e
i6
i5) be a particular pair of left and right
neighbors of a = (f j2j1/ei2i1). Then, the orientationof CL and CR are OL(CL, a) and OR(CR, a) re-
spectively and each may take one of the following
four orientation values (similar to (Nagata et al,
2006)):
? Monotone Adjacent (MA), if (i4 + 1) = i1
for OL and if (i2 + 1) = i5 for OR
? Reverse Adjacent (RA), if (i2 + 1) = i3 for
OL and if (i6 + 1) = i1 for OR
? Monotone Gap (MG), if (i4 + 1) < i1 for
OL and if (i2 + 1) < i5 for OR
2We represent a chunk as a source and target phrase pair
(f j2j1/ei2i1 ) where the subscript and the superscript indicate the
starting and the ending indices as such f j2j1 denotes a sourcephrase that spans from j1 to j2.
1265
Figure 1: An aligned Chinese-English sentence pair. Circles
represent alignment points. Black circle represents the an-
chor; boxes represent the anchor?s neighbors.
? Reverse Gap (RG), if (i2 + 1) < i3 for OL
and if (i6 + 1) < i1 for OR. (1)
The first clause (monotone, reverse) indicates
whether the target order of the chunks follows the
source order; the second (adjacent, gap) indicates
whether the chunks are adjacent or separated by an
intervening phrase when projected.
To be more concrete, let us consider an aligned
sentence pair in Fig. 1, which is adapted from
(Chiang, 2005). Suppose there is only one anchor,
i.e. a = (f77 /e77) which corresponds to the word
de(that). By applying Eqs. 1 and 2, we can infer
that a has three left neighbors and four right neigh-
bors, i.e. CL(a) = (f66 /e99), (f65 /e98), (f63 /e118 ) and
CR(a) = (f88 /e55), (f98 /e65), (f108 /e64), (f118 /e63)
respectively. Then, by applying Eq.
1, we can compute the orientation val-
ues of each of these neighbors, which
are OL(CL(a), a) = RG,RA,RA and
OR(CR(a), a) = RG,RA,RA,RA. As shown,
most of the neighbors have Reverse Adjacent
(RA) orientation except for the smallest left and
right neighbors (i.e. (f66 /e99) and (f88 /e55)) which
have Reverse Gap (RG) orientation.
Given the anchors together with its neighboring
chunks and their orientations, the Two-Neighbor
Orientation model takes the following form:
?
a?A
?
CL?CL(a),
CR?CR(a)
PTNO(CL, OL, CR, OR|a; ?) (2)
For conciseness, references that are clear from
context, such the reference to CL and a in
OL(CL, a), are dropped.
3 Maximal Orientation Span
As shown in Eq. 2, the TNO model has to enu-
merate all possible pairing of CL ? CL(a) and
CR ? CR(a). To make the TNO model more
tractable, we simplify the TNO model to consider
only the largest left and right neighbors, referred
to as the Maximal Orientation Span/MOS (M ).
More formally, given a = (f j2j1/ei2i1), the left andthe right MOS of a are:
ML(a) = arg max
(fj4j3 /e
i4
i3 )?CL(a)
(j4 ? j3)
MR(a) = arg max
(fj6j5 /e
i6
i5 )?CR(a)
(j6 ? j5)
Coming back to our example, the left and right
MOS of the anchor are ML(a) = (f63 /e118 ) and
MR(a) = (f118 /e63). In Fig. 1, they are denoted as
the largest boxes delineated by solid lines.
As such, we reformulate Eq. 2 into:
?
a?A
?
CL?CL(a),
CR?CR(a)
PTNO(ML, OL,MR, OR|a; ?).?CL==ML?
CR==MR
(3)
where ? returns 1 if (CL == ML ?CR == MR),
otherwise 0.
Beyond simplifying the computation, the key
benefit of modeling MOS is that it serves as a
global parameter that can guide or constrain un-
derlying local reorderings. As a case in point, let
us consider a cheating exercise where we have to
translate the Chinese sentence in Fig. 1 with the
following set of hierarchical phrases3:
Xa??Aozhou1shi2X1,Australia1 is2X1?
Xb??yu3 Beihan4X1, X1with3 North4 Korea?
Xc??you5bangjiao6, have5dipl.6 rels.?
Xd??X1de7shaoshu8 guojia9 zhi10 yi11,
one11of10the few8 countries9 that7X1?
This set of hierarchical phrases represents a trans-
lation model that has resolved all local ambiguities
(i.e. local reordering and lexical mappings) except
for the spans of the hierarchical phrases. With this
example, we want to show that accurate local de-
cisions (rather obviously) don?t always lead to ac-
curate global reordering and to demonstrate that
explicit MOS modeling can play a crucial role to
address this issue. To do so, we will again focus
on the same anchor de (that).
3We use hierarchical phrase-based translation system as a
case in point, but the merit is generalizable to other systems.
1266
d? ?X1de7shaoshu8 guojia9 zhi10 yi11?, ?one11of10the few8 countries9 that7X1?
a? ??Aozhou1shi2X1?de7shaoshu8 guojia9 zhi10 yi11?,
?one11of10the few8 countries9 that7?Australia1 is2X1??
b? ??Aozhou1shi2 ?yu3 Beihan4X1??de7shaoshu8 guojia9 zhi10 yi11?,
?one11of10the few8 countries9 that7?Australia1 is2?X1with3 North4 Korea???
c? ?d ?aAozhou1shi2 ?byu3 Beihan4 ?cyou5bangjiao6?c?b?ade7shaoshu8 guojia9 zhi10 yi11 ?d ,
?one11of10the few8 countries9 that7?Australia1 is2??have5dipl.6 rels.?with3 North4 Korea???
Table 1: Derivation of Xd ?Xa ?Xb ?Xc that leads to an incorrect translation.
a? ?Aozhou1shi2X1?, ?Australia1 is2X1?
b? ?Aozhou1shi2?yu3Beihan4X1??, ?Australia1 is2?X1with3 North4 Korea??
d? ?Aozhou1shi2?yu3Beihan4?X1de7shaoshu8 guojia9 zhi10 yi11???,
?Australia1 is2??one11of10the few8 countries9 that7X1?with3 North4 Korea??
c? ?aAozhou1shi2?byu3Beihan4 ?d ?cyou5bangjiao6?cde7shaoshu8 guojia9 zhi10 yi11 ?d ?b?a,
Australia1 is2??one11of10the few8 countries9 that7?have5dipl.6??with3 North4 Korea??
Table 2: Derivation of Xa ?Xb ?Xd ?Xc that leads to the correct translation.
As the rule?s identifier, we attach an alphabet
letter to each rule?s left hand side, as such the an-
chor de (that) appears in rule Xd. We also attach
the word indices as the superscript of the source
words and project the indices to the target words
aligned, as such ?have5? suggests that the word
?have? is aligned to the 5-th source word, i.e. you.
Note that to facilitate the projection, the rules must
come with internal word alignment in practice.
Now the indices on the target words in the rules
are different from those in Fig. 1. We will also
extensively use indices in this sense in the sub-
sequent section about decoding. In such a sense,
ML(a) = (f63 /e63) and MR(a) = (f118 /e118 ).
Given the rule set, there are three possible
derivations, i.e. Xd ?Xa ?Xb ?Xc,Xa ?Xb ?
Xd ?Xc, and Xa ?Xd ?Xb ?Xc, where ? in-
dicates that the first operand dominates the second
operand in the derivation tree. The application of
the rules would show that the first derivation will
produce an incorrect reordering while the last two
will produce the correct ones. Here, we would like
to point out that even in this simple example where
all local decisions are made accurate, this ambigu-
ity occurs and it would occur even more so in the
real translation task where local decisions may be
highly inaccurate.
Next, we will show that the MOS-related in-
formation can help to resolve this ambiguity, by
focusing more closely on the first and the second
derivations, which are detailed in Tables 1 and 2.
Particularly, we want to show that the MOS gen-
erated by the incorrect derivation does not match
the MOS learnt from Fig. 1. As shown, at the
end of the derivation, we have all the informa-
tion needed to compute the MOS (i.e. ?) which is
equivalent to that available at training time, i.e. the
source sentence, the complete translation and the
word alignment. Running the same MOS extrac-
tion procedure on both derivations would produce
the right MOS that agrees with the right MOS pre-
viously learnt from Fig. 1, i.e. (f118 /e118 ). How-
ever, that?s not the case for left MOS, which we
underline in Tables 1 and 2. As shown, the incor-
rect derivation produces a left MOS that spans six
words, i.e. (f61 /e61), while the correct derivation
produces a left MOS that spans four words, i.e.
(f63 /e63). Clearly, the MOS of the incorrect deriva-
tion doesn?t agree with the MOS we learnt from
Fig. 1, unlike the MOS of the correct translation.
This suggests that explicit MOS modeling would
provide a mechanism for resolving crucial global
reordering ambiguities that are beyond the ability
of local models.
Additionally, this illustration also shows a case
where MOS acts as a cross-boundary context
which effectively relaxes the context-free assump-
tion of hierarchical phrase-based formalism. In
Tables 1 and 2?s full derivations, we indicate rule
boundaries explicitly by indexing the angle brack-
ets, e.g. ?a indicates the beginning of rule Xa in
the derivation. As the anchor appears in Xd, we
1267
highlight its boundaries in box frames. de (that)?s
MOS respects rule boundaries if and only if all
the words come entirely from Xd?s antecedent or
?d and ?d appears outside of MOS; otherwise it
crosses the rule boundaries. As clearly shown in
Table 2, the left MOS of the correct derivation (un-
derlined) crosses the rule boundary (of Xd) since
?d appears within the MOS.
Going back to the formulation, focusing on
modeling MOS would simplify the formulation of
TNO model from Eq. 2 into:
?
a?A
PTNO(ML, OL,MR, OR|a; ?) (4)
which doesn?t require enumerating of all possible
pairs of CL and CR.
4 Model Decomposition and Variants
To make the model more tractable, we decompose
PTNO in Eq. 4 into the following four factors:
P (MR|a)? P (OR|MR, a)? P (ML|OR,MR, a)
? P (OL|ML, OR,MR, a). Subsequently, we will
refer to them as PMR , POR , PML and POL respec-
tively. Each of these factors will act as an addi-
tional feature in the log-linear framework of our
SMT system. The above decomposition follows
a generative story that starts from generating the
right neighbor first. There are other equally credi-
ble alternatives, but based on empirical results, we
settle with the above.
Next, we present four different variants of the
model (not to be confused with the four factors
above). Each variant has a different probabilistic
conditioning of the factors. We start by making
strong independence assumptions in Model 1 and
then relax them as we progress to Model 4. The
description of the models is as follow:
? Model 1. We assume PML and PMR to be
equal to 1 and POR ? P (OR|a; ?) to be in-
dependent of MR and POL ? P (OL|a; ?) to
be in independent of ML,MR and OR.
? Model 2. On top of Model 1, we
make POL dependent on POR , thus
POL?P (OL|OR, a; ?).
? Model 3. On top of Model 2, we make POR
dependent on MR and POL on MR and ML,
thus POR ? P (OR|MR, a; ?) and POL ?
P (OL|ML, OR,MR; a,?) .
? Model 4. On top of Model 3, we model PMR
and PML as multinomial distributions esti-
mated from training data.
Model 1 represents a model that focuses on
making accurate one-sided decisions, independent
of the decision on the other side. Model 2 is
designed to address the deficiency of Model 1
since Model 1 may assign non-zero probability to
improbable assignment of orientation values, e.g.
Monotone Adjacent for the left neighbor and Re-
verse Adjacent for the right neighbor. Model 2
does so by conditioning POL on OR. In Model 3,
we start incorporating MOS-related information in
predicting OL and OR. In Model 4, we explicitly
model the MOS of each anchor.
5 Training
The TNO model training consists of two differ-
ent training regimes: 1) discriminative for train-
ing POL ,POR ; and 2) generative for training PML ,
PMR . Before describing the specifics, we start by
describing the procedure to extract anchors and
their corresponding MOS from training data, from
which we collect statistics and extract features to
train the model.
For each aligned sentence pair (F,E,?) in the
training data, the training starts with the iden-
tification of the regions in the source sentences
as anchors (A). For our Chinese-English experi-
ments, we use a simple heuristic that equates as
anchors, single-word chunks whose corresponding
word class belongs to closed-word classes, bear-
ing a close resemblance to (Setiawan et al, 2007).
In total, we consider 21 part-of-speech tags; some
of which are as follow: VC (copula), DEG, DEG,
DER, DEV (de-related), PU (punctuation), AD
(adjectives) and P (prepositions).
Next we generate all possible chunks ?(?)
as previously described in Sec. 3. We then de-
fine a functionMinC(?, j1, j2) which returns the
shortest chunk that can span from j1 to j2. If
(f j2j1 /e
i2
i1) ? ?, then MinC returns (f j2j1 /ei2i1).The algorithm to extract MOS takes ? and an
anchor a = (f j2j1 /ei2i1) as input; and outputs thechunk that qualifies as MOS or none. Alg. 1
provides the algorithm to extract the right MOS;
the algorithm to extract the left MOS is identical
to Alg. 1, except that it scans for chunks to the
left of the anchor. In Alg. 1, there are two in-
termediate parameters si and ei which represent
the active search range and should initially be set
to j2 + 1 and |F | respectively. Once we obtain
a,ML(a) andMR(a), we computeOL(ML(a), a)
and OR(MR(a), a) and are ready for training.
1268
To estimate POL and POR , we train discrimi-
native classifiers that predict the orientation val-
ues and use the normalized posteriors at decoding
time as additional feature scores in SMT?s log lin-
ear framework. We train the classifiers on a rich
set of binary features ranging from lexical to part-
of-speech (POS) and to syntactic features.
Algorithm 1: Function MREx
input : a = (f j2j1 /ei2i1), si, ei: int; ?: chunks
output: (f j4j3 /ei4i3) : chunk or ?
(f j4j3 /e
i4
i3) = MinC(?, j2 + 1, ei)
if (j3 == j2 + 1 ? j4 == ei) then
? f j4j3 /e
i4
i3
else
if (j2 + 1 == ei) then
? ?
else
if (ei-2 ? si) then
?MREx(a, si, ei? 1,?)
else
m = d(si+ei)/2e
(f j4j3 /e
i4
i4) = MinC(?, j2 + 1,m)
if (j3 == j2 + 1) then
c = MREx(a,m, ei? 1,?)
if (c == ?) then
? f j4j3 /e
i4
i3
else
? c
end
else
?MREx(a, si,m? 1,?)
end
end
end
end
Suppose a = (f j2j1 /ei2i1), ML(a) = (f j4j3 /ei4i3)
and ML(a) = (f j6j5 /ei6i5), then based on the con-text?s location, the elementary features employed
in our classifiers can be categorized into:
1. anchor-related: slex (the actual word of
f j2j1 ), spos (part-of-speech (POS) tag of
slex), sparent (spos?s parent in the parse
tree), tlex (ei2i1?s actual target word)..
2. surrounding: lslex (the previous word /
f j1?1j1?1 ), rslex (the next word / f j2+1j2+1 ), lspos(lslex?s POS tag), rspos (rslex?s POS
tag), lsparent (lslex?s parent), rsparent
(rslex?s parent).
3. non-local: lanchorslex (the previous
anchor?s word) , ranchorslex (the next an-
chor?s word), lanchorspos (lanchorslex?s
POS tag), ranchorspos (ranchorslex?s
POS tag).
4. MOS-related: mosl int slex (the actual
word of f j3j3 ), mosl ext slex (the actual word
of f j3j3 ), mosl int spos (mosl int slex?sPOS tag), mosl ext spos (mosl ext spos?s
POS tag), mosr int slex (the actual word of
f j3j3 ), mosr ext slex (the actual word of f j3j3 ),
mosr int spos (mosr int slex?s POS tag),
mosr ext spos (mosr ext spos?s POS tag).
For Model 1, we train one classifier each for
POR and POL . For Model 2-4, we train four clas-
sifiers for POL for each value of OR. We use only
the MOS features for Model 3 and 4. Addition-
ally, we augment the feature set with compound
features, e.g. conjunction of the lexical of the an-
chor and the lexical of the left and the right an-
chors. Although they increase the number of fea-
tures significantly, we found that these compound
features are empirically beneficial.
We come up with > 50 types of features, which
consist of a combination of elementary and com-
pound features. In total, we generate hundreds of
millions of such features from the training data.
To keep the number features to a manageable size,
we employ the L1-regularization in training to en-
force sparse solutions, using the off-the-shelf LIB-
LINEAR toolkit (Fan et al, 2008). After training,
the number of features in our classifiers decreases
to below 5 million features for each classifier.
We train PML and PMR via the relative fre-
quency principle. To avoid the sparsity issue, we
represent ML as (mosl int spos,mosl ext spos)
and MR as (mosr int spos,mosr ext spos). We
condition PML and PMR only on spos and the ori-
entation, estimating them as follow:
P (ML|spos, OL) =
N(ML, spos, OL)
N(spos, OL)
P (MR|spos, OR) =
N(MR, spos, OR)
N(spos, OR)
where N returns the count of the events in the
training data.
1269
Target string (w/ source index) Symbol(s) read Op. Stack(s)
(1) Xc have5 dipl.6 rels. [5][6] S,S,R Xc:[5-6]
(2) Xd one11 of10 few8 countries9 [11][10] S,S,R [10-11]
that7 Xc
(3) [8][9] S,S,R,R [8-11]
(4) [7] S [8-11][7]
(5) Xc:[5,6] S Xd:[8-11][7][5,6]
(6) Xb Xd with3 North4 Korea Xd:[8-11][7][5,6] S [8-11][7][5,6]
(7) [3][4] S,S,R,R Xb:[8-11][7][3-6]
(8) Xa Australia1 is2 Xb [1][2] S,S,R [1-2]
(9) Xb:[8-11][7][3,6] S,A Xa:[1-2][8-11][7][3,6]
Table 3: The application of the shift-reduce parsing algorithm, which corresponds to Table 2?s derivation.
6 Decoding
Integrating the TNO Model into syntax-based
SMT systems is non-trivial, especially with the
MOS modeling. The method described in Sec. 3
assumes ? = (F,E,?), thus it is only applicable
at training or at the last stage of decoding. Since
many reordering decisions may have been made
at the earlier stages, the late application of TNO
model would limit the utility of the model. In this
section, we describe an algorithm that facilitates
the incremental construction of MOS and the com-
putation of TNO model on partial derivations.
The algorithm bears a close resemblance to the
shift-reduce algorithm where a stack is used to ac-
cumulate (partial) information about a, ML and
MR for each a ? A in the derivation. This al-
gorithm takes an input stream and applies either
the shift or the reduce operations starting from the
beginning until the end of the stream. The shift op-
eration advances the input stream by one symbol
and push the symbol into the stack; while the re-
duce operation applies some reduction rule to the
topmost elements of the stack. The algorithm ter-
minates at the end of the input stream where the
resulting stack will be propagated to the parent for
the later stage of decoding. In our case, the in-
put stream is the target string of the rule and the
symbol is the corresponding source index of the
elements of the target string. The reduction rule
looks at two indices and merge them if they are
adjacent (i.e. has no intervening phrase). We for-
bid the application of the reduction rule to anchors.
Table 3 shows the execution trace of the algorithm
for the derivation described in Table 2.
As shown, the algorithm starts with an empty
stack. It then projects the source index to the cor-
responding target word and then enumerates the
target string in a left to right fashion. If it finds
a target word with a source index, it applies the
shift operation, pushing the index to the stack. Un-
less the symbol corresponds to an anchor, it tries
to apply the reduce operation. Line (4) indicates
the special treatment to the anchor. If the symbol
read is a nonterminal, then we push the entire stack
that corresponds to that nonterminal. For example,
when the algorithm reads Xd at line (6), it pushes
the entire stack from line (5).
This algorithm facilitates the incremental con-
struction of MOS which may cross rule bound-
aries. For example, at the end of the application of
Xd at line (5), the current left MOS is [5-6]. How-
ever, the algorithm grows it to [3-6] after the appli-
cation of ruleXb at line (7). Furthermore, it allows
us to compute the models from partial hypothesis.
For example, at line (5), we can compute POL by
considering [5,6] as ML to be updated with [3,6]
in line (7). This way, we expect our TNO model
would play a bigger role at decoding time.
Specific to SCFG-based translation, the values
of OL and OR are identical in the partial or in
the full derivations. For example, the orientation
values of de (that)?s left neighbor is always RA.
This statement holds, even though at the end of
Section 2, we stated that de (that)?s left neigh-
bor may have other orientation values, i.e. RG
for CL(a) = (f66 /e99). The formal proof is omit-
ted, but the intuition comes from the fact that the
derivations for SCFG-based translation are sub-
set of ?(?) and that (f66 /e99) will never become
ML forMinC(CL(a), a) respectively (chunk that
spans a and CL). Consequently, for Model 1 and
Model 2, we can obtain the model score earlier in
the decoding process.
1270
7 Experiments
Our baseline systems is a state-of-the-art string-
to-dependency system (Shen et al, 2008). The
system is trained on 10 million parallel sentences
that are available to the Phase 1 of the DARPA
BOLT Chinese-English MT task. The training cor-
pora include a mixed genre of newswire, weblog,
broadcast news, broadcast conversation, discus-
sion forums and comes from various sources such
as LDC, HK Law, HK Hansard and UN data.
In total, our baseline model employs about
40 features, including four from our proposed
Two-Neighbor Orientation model. In addition to
the standard features including the rule transla-
tion probabilities, we incorporate features that are
found useful for developing a state-of-the-art base-
line, such as the provenance features (Chiang et
al., 2011). We use a large 6-gram language model,
which was trained on 10 billion English words
from multiple corpora, including the English side
of our parallel corpus plus other corpora such as
Gigaword (LDC2011T07) and Google News. We
also train a class-based language model (Chen,
2009) on two million English sentences selected
from the parallel corpus. As the backbone of
our string-to-dependency system, we train 3-gram
models for left and right dependencies and un-
igram for head using the target side of the bi-
lingual training data. To train our Two-Neighbor
Orientation model, we select a subset of 5 million
aligned sentence pairs.
For the tuning and development sets, we set
aside 1275 and 1239 sentences selected from
LDC2010E30 corpus. We tune the decoding
weights with PRO (Hopkins and May, 2011) to
maximize BLEU-TER. As for the blind test set,
we report the performance on the NIST MT08
evaluation set, which consists of 691 sentences
from newswire and 666 sentences from weblog.
We pick the weights that produce the highest de-
velopment set scores to decode the test set.
Table 4 summarizes the experimental results on
NIST MT08 newswire and weblog. In column 2,
we report the classification accuracy on a subset of
training data. Note that these numbers are for ref-
erence only and not directly comparable with each
other since the features used in these classifiers
include several gold standard information, such
as the anchors? target words, the anchors? MOS-
related features (Model 3 & 4) and the orientation
of the right MOS (Model 2-4); all of which have
Acc MT08 nw MT08 wbBLEU TER BLEU TER
S2D - 36.77 53.28 26.34 57.41
M1 72.5 37.60 52.70 27.59 56.33
M2 77.4 37.86 52.68 27.74 56.11
M3 84.5 38.02 52.42 28.22 55.82
M4 84.5 38.55 52.41 28.44 56.45
Table 4: The NIST MT08 results on newswire (nw) and we-
blog (wb) genres. S2D is the baseline string-to-dependency
system (line 1), on top of which Two-Neighbor Orientation
Model 1 to 4 are employed (line 2-5). The best TER and
BLEU results on each genre are in bold. For BLEU, higher
scores are better, while for TER, lower scores are better.
to be predicted at decoding time.
In columns 2 and 4, we report the BLEU scores,
while in columns 3 and 5, we report the TER
scores. The performance of our baseline string-
to-dependency syntax-based SMT is shown in the
first line, followed by the performance of our Two-
Neighbor Orientation model starting from Model
1 to Model 4. As shown, the empirical results
confirm our intuition that SMT can greatly benefit
from reordering model that incorporate cross-unit
contextual information.
Model 1 provides most of the gain across the
two genres of around +0.9 to +1.2 BLEU and -0.5
to -1.1 TER. Model 2 which conditions POL on
OR provides an additional +0.2 BLEU improve-
ment on BLEU score consistently across the two
genres. As shown in line 4, we see a stronger
improvement in the inclusion of MOS-related in-
formation as features in Model 3. In newswire,
Model 3 gives an additional +0.4 BLEU and -0.2
TER, while in weblog, it gives a stronger improve-
ment of an additional +0.5 BLEU and -0.3 TER.
The inclusion of explicit MOS modeling in Model
4 gives a significant BLEU score improvement of
+0.5 but no TER improvement in newswire. In
weblog, Model 4 gives a mixed results of +0.2
BLEU score improvement and a hit of +0.6 TER.
We conjecture that the weblog text has a more am-
biguous orientation span that are more challenging
to learn. In total, our TNO model gives an encour-
aging result. Our most advanced model gives sig-
nificant improvement of +1.8 BLEU/-0.8 TER in
newswire domain and +2.1 BLEU/-1.0 TER over
a strong string-to-dependency syntax-based SMT
enhanced with additional state-of-the-art features.
1271
8 Related Work
Our work intersects with existing work in many
different respects. In this section, we mainly focus
on work related to the probabilistic conditioning
of our TNO model and the MOS modeling.
Our TNO model is closely related to the Uni-
gram Orientation Model (UOM) (Tillman, 2004),
which is the de facto reordering model of phrase-
based SMT (Koehn et al, 2007). UOM views
reordering as a process of generating (b, o) in a
left-to-right fashion, where b is the current phrase
pair and o is the orientation of b with the pre-
viously generated phrase pair b?. UOM makes
strong independence assumptions and formulates
the model as P (o|b). Tillmann and Zhang (2007)
proposed a Bigram Orientation Model (BOM) to
include both phrase pairs (b and b?) into the model.
Their original intent is to model P (o, b|o?, b?), but
perhaps due to sparsity concerns, they settle with
P (o|b, b?), dropping the conditioning on the pre-
vious orientation o?. Subsequent improvements
use the P (o|b, b?) formula, for example, for in-
corporating various linguistics feature like part-of-
speech (Zens and Ney, 2006), syntactic (Chang et
al., 2009), dependency information (Bach et al,
2009) and predicate-argument structure (Xiong et
al., 2012). Our TNO model is more faithful to the
BOM?s original formulation.
Our MOS concept is also closely related to hi-
erarchical reordering model (Galley and Manning,
2008) in phrase-based decoding, which computes
o of b with respect to a multi-block unit that may
go beyond b?. They mainly use it to avoid overes-
timating ?discontiguous? orientation but fall short
in modeling the multi-block unit, perhaps due to
data sparsity issue. Our MOS is also closely re-
lated to the efforts of modeling the span of hi-
erarchical phrases in formally syntax-based SMT.
Early works reward/penalize spans that respect the
syntactic parse constituents of an input sentence
(Chiang, 2005), and (Marton and Resnik, 2008).
(Xiong et al, 2009) learn the boundaries from
parsed and aligned training data, while (Xiong et
al., 2010) learn the boundaries from aligned train-
ing data alone. Recent work couples span mod-
eling tightly with reordering decisions, either by
adding an additional feature for each hierarchical
phrase (Chiang et al, 2008; Shen et al, 2009) or
by refining the nonterminal label (Venugopal et
al., 2009; Huang et al, 2010; Zollmann and Vo-
gel, 2011). Common to this work is that the spans
modeled may not correspond to MOS, which may
be suboptimal as discussed in Sec. 3.
In equating anchors with the function word
class, our work, particularly Model 1, is closely
related to the function word-centered model of Se-
tiawan et al (2007) and Setiawan et al (2009).
However, we provide a discriminative treatment
to the model to include a richer set of features in-
cluding the MOS modeling. Our work in incorpo-
rating global context also intersects with existing
work in Preordering Model (PM), e.g. (Niehues
and Kolss, 2009; Costa-jussa` and Fonollosa, 2006;
Genzel, 2010; Visweswariah et al, 2011; Tromble
and Eisner, 2009). The goal of PM is to reorder the
input sentence F into F ? whose order is closer to
the target language order, whereas the goal of our
model is to directly reorder F into the target lan-
guage order. The crucial difference is that we have
to integrate our model into SMT decoder, which is
highly non-trivial.
9 Conclusion
We presented a novel approach to address a kind
of long-distance reordering that requires global
cross-boundary contextual information. Our ap-
proach, which we formulate as a Two-Neighbor
Orientation model, includes the joint modeling of
two orientation decisions and the modeling of the
maximal span of the reordered chunks through the
concept of Maximal Orientation Span. We de-
scribe four versions of the model and implement
an algorithm to integrate our proposed model into
a syntax-based SMT system. Empirical results
confirm our intuition that incorporating cross-
boundaries contextual information improves trans-
lation quality. In a large scale Chinese-to-English
translation task, we achieve a significant improve-
ment over a strong baseline. In the future, we hope
to continue this line of research, perhaps by learn-
ing to identify anchors automatically from training
data, incorporating a richer set of linguistics fea-
tures such as dependency structure and strength-
ening the modeling of Maximal Orientation Span.
Acknowledgements
We would like to acknowledge the support of DARPA un-
der Grant HR0011-12-C-0015 for funding part of this work.
The views, opinions, and/or findings contained in this arti-
cle/presentation are those of the author/ presenter and should
not be interpreted as representing the official views or poli-
cies, either expressed or implied, of the DARPA.
1272
References
Nguyen Bach, Qin Gao, and Stephan Vogel. 2009.
Source-side dependency tree reordering models with
subtree movements and constraints. In Proceed-
ings of the Twelfth Machine Translation Summit
(MTSummit-XII), Ottawa, Canada, August. Interna-
tional Association for Machine Translation.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with Chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation (SSST-3)
at NAACL HLT 2009, pages 51?59, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Stanley Chen. 2009. Shrinking exponential language
models. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 468?476, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the
2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 224?233, Honolulu,
Hawaii, October.
David Chiang, Steve DeNeefe, and Michael Pust.
2011. Two easy improvements to lexical weighting.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 455?460, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages
263?270, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Marta R. Costa-jussa` and Jose? A. R. Fonollosa. 2006.
Statistical machine reordering. In Proceedings of
the 2006 Conference on Empirical Methods in Nat-
ural Language Processing, pages 70?76, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848?856, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine trans-
lation. In Proceedings of the 23rd International
Conference on Computational Linguistics (Coling
2010), pages 376?384, Beijing, China, August. Col-
ing 2010 Organizing Committee.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Zhongqiang Huang, Martin Cmejrek, and Bowen
Zhou. 2010. Soft syntactic constraints for hierar-
chical phrase-based translation using latent syntac-
tic distributions. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 138?147, Cambridge, MA, Octo-
ber. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion, June.
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In Proceedings of The 46th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1003?
1011, Columbus, Ohio, June.
Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,
and Kazuteru Ohashi. 2006. A clustered global
phrase reordering model for statistical machine
translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 713?720, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 206?214, Athens, Greece,
March. Association for Computational Linguistics.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li.
2007. Ordering phrases with function words. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 712?
719, Prague, Czech Republic, June. Association for
Computational Linguistics.
Hendra Setiawan, Min Yen Kan, Haizhou Li, and Philip
Resnik. 2009. Topological ordering of function
words in hierarchical phrase-based translation. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
1273
of the AFNLP, pages 324?332, Suntec, Singapore,
August. Association for Computational Linguistics.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of ACL-08: HLT, pages 577?585,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of lin-
guistic and contextual information for statistical ma-
chine translation. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 72?80, Singapore, August. Asso-
ciation for Computational Linguistics.
Christoph Tillman. 2004. A unigram orienta-
tion model for statistical machine translation. In
HLT-NAACL 2004: Short Papers, pages 101?104,
Boston, Massachusetts, USA, May 2 - May 7. Asso-
ciation for Computational Linguistics.
Christoph Tillmann and Tong Zhang. 2007. A
block bigram prediction model for statistical ma-
chine translation. ACM Transactions on Speech and
Language Processing (TSLP), 4(3).
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 1007?1016,
Singapore, August. Association for Computational
Linguistics.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars:
Softening syntactic constraints to improve statisti-
cal machine translation. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 236?244,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for im-
proved machine translation. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 486?496, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.
2009. A syntax-driven bracketing model for phrase-
based translation. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 315?
323, Suntec, Singapore, August. Association for
Computational Linguistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010.
Learning translation boundaries for phrase-based
decoding. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 136?144, Los Angeles, California,
June. Association for Computational Linguistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Mod-
eling the translation of predicate-argument structure
for smt. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 902?911, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Richard Zens and Hermann Ney. 2006. Discrimina-
tive reordering models for statistical machine trans-
lation. In Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL):
Proceedings of the Workshop on Statistical Machine
Translation, pages 55?63, New York City, NY, June.
Association for Computational Linguistics.
Andreas Zollmann and Stephan Vogel. 2011. A word-
class approach to labeling pscfg rules for machine
translation. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1?11,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
1274
