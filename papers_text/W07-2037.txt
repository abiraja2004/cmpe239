Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 177?182,
Prague, June 2007. c?2007 Association for Computational Linguistics
I2R: Three Systems for Word Sense Discrimination, Chinese Word Sense
Disambiguation, and English Word Sense Disambiguation
Zheng-Yu Niu, Dong-Hong Ji
Institute for Infocomm Research
21 Heng Mui Keng Terrace
119613 Singapore
niu zy@hotmail.com
dhji@i2r.a-star.edu.sg
Chew-Lim Tan
Department of Computer Science
National University of Singapore
3 Science Drive 2
117543 Singapore
tancl@comp.nus.edu.sg
Abstract
This paper describes the implementation
of our three systems at SemEval-2007, for
task 2 (word sense discrimination), task 5
(Chinese word sense disambiguation), and
the first subtask in task 17 (English word
sense disambiguation). For task 2, we ap-
plied a cluster validation method to esti-
mate the number of senses of a target word
in untagged data, and then grouped the in-
stances of this target word into the esti-
mated number of clusters. For both task 5
and task 17, We used the label propagation
algorithm as the classifier for sense disam-
biguation. Our system at task 2 achieved
63.9% F-score under unsupervised evalua-
tion, and 71.9% supervised recall with su-
pervised evaluation. For task 5, our sys-
tem obtained 71.2% micro-average preci-
sion and 74.7% macro-average precision.
For the lexical sample subtask for task
17, our system achieved 86.4% coarse-
grained precision and recall.
1 Introduction
SemEval-2007 launches totally 18 tasks for evalua-
tion exercise, covering word sense disambiguation,
word sense discrimination, semantic role labeling,
and sense disambiguation for information retrieval,
and other topics in NLP. We participated three tasks
in SemEval-2007, which are task 2 (Evaluating
Word Sense Induction and Discrimination Systems),
task 5 (Multilingual Chinese-English Lexical Sam-
ple Task) and the first subtask at task 17 (English
Lexical Sample, English Semantic Role Labeling
and English All-Words Tasks).
The goal for SemEval-2007 task 2 (Evaluat-
ing Word Sense Induction and Discrimination Sys-
tems)(Agirre and Soroa, 2007) is to automatically
discriminate the senses of English target words by
the use of only untagged data. Here we address this
word sense discrimination problem by (1) estimat-
ing the number of word senses of a target word in
untagged data using a stability criterion, and then (2)
grouping the instances of this target word into the
estimated number of clusters according to the simi-
larity of contexts of the instances. No sense-tagged
data is used to help the clustering process.
The goal of task 5 (Chinese Word Sense Disam-
biguation) is to create a framework for the evaluation
of word sense disambiguation in Chinese-English
machine translation systems. Each participates of
this task will be provided with sense tagged train-
ing data and untagged test data for 40 Chinese pol-
ysemous words. The ?sense tags? for the ambigu-
ous Chinese target words are given in the form of
their English translations. Here we used a semi-
supervised classification algorithm (label propaga-
tion algorithm) (Niu, et al, 2005) to address this
Chinese word sense disambiguation problem.
The lexical sample subtask of task 17 (English
Word Sense Disambiguation) provides sense-tagged
training data and untagged test data for 35 nouns and
65 verbs. This data includes, for each target word:
OntoNotes sense tags (these are groupings of Word-
Net senses that are more coarse-grained than tradi-
177
tional WN entries), as well as the sense inventory for
these lemmas. Here we used only the training data
supplied in this subtask for sense disambiguation in
test set. The label propagation algorithm (Niu, et al,
2005) was used to perform sense disambiguation by
the use of both training data and test data.
This paper will be organized as follows. First, we
will provide the feature set used for task 2, task 5
and task 17 in section 2. Secondly, we will present
the word sense discrimination method used for task
2 in section 3. Then, we will give the label propa-
gation algorithm for task 5 and task 17 in section 4.
Section 5 will provide the description of data sets at
task 2, task 5 and task 17. Then, we will present the
experimental results of our systems at the three tasks
in section 6. Finally we will give a conclusion of our
work in section 7.
2 Feature Set
In task 2, task 5 and task 17, we used three types of
features to capture contextual information: part-of-
speech of neighboring words (no more than three-
word distance) with position information, unordered
single words in topical context (all the contextual
sentences), and local collocations (including 11 col-
locations). The feature set used here is as same as
the feature set used in (Lee and Ng, 2002) except
that we did not use syntactic relations.
3 The Word Sense Discrimination Method
for Task 2
Word sense discrimination is to automatically dis-
criminate the senses of target words by the use of
only untagged data. So we can employ clustering
algorithms to address this problem. Another prob-
lem is that there is no sense inventories for target
words. So the clustering algorithms should have the
ability to automatically estimate the sense number
of a target word.
Here we used the sequential Information Bottle-
neck algorithm (sIB) (Slonim, et al, 2002) to esti-
mate cluster structure, which measures the similarity
of contexts of instances of target words according to
the similarity of their contextual feature conditional
distribution. But sIB requires the number of clus-
ters as input. So we used a cluster validation method
to automatically estimate the sense number of a tar-
Table 1: Sense number estimation procedure for
word sense discrimination.
1 Set lower bound Kmin and upper bound Kmax
for sense number k;
2 Set k = Kmin;
3 Conduct the cluster validation process
presented in Table 2 to evaluate the merit of k;
4 Record k and the value of Mk;
5 Set k = k + 1. If k ? Kmax, go to step 3,
otherwise go to step 6;
6 Choose the value k? that maximizes Mk,
where k? is the estimated sense number.
get word before clustering analysis. Cluster valida-
tion (or stability based approach)is a commonly used
method to the problem of model order identification
(or cluster number estimation) (Lange, et al, 2002;
Levine and Domany, 2001). The assumption of this
method is that if the model order is identical with the
true value, then the cluster structure estimated from
the data is stable against resampling, otherwise, it is
more likely to be the artifact of sampled data.
3.1 The Sense Number Estimation Procedure
Table 1 presents the sense number estimation pro-
cedure. Kmin was set as 2, and Kmax was set as 5 in
our system. The evaluation function Mk (described
in Table 2) is relevant with the sense number k. q
is set as 20 here. Clustering solution which is stable
against resampling will give rise to a local optimum
of Mk, which indicates the true value of sense num-
ber. In the cluster validation procedure, we used the
sIB algorithm to perform clustering analysis (de-
scribed in section 3.2).
The function M(C?, C) in Table 2 is given by
(Levine and Domany, 2001):
M(C?, C) =
?
i,j 1{C
?
i,j = Ci,j = 1, di ? D?, dj ? D?}?
i,j 1{Ci,j = 1, di ? D?, dj ? D?}
,
(1)
where D? is a subset with size ?|D| sampled from
full data set D, C and C? are |D|? |D| connectivity
matrixes based on clustering solutions computed on
D and D? respectively, and 0 ? ? ? 1. The con-
nectivity matrix C is defined as: Ci,j = 1 if di and
dj belong to the same cluster, otherwise Ci,j = 0.
C? is calculated in the same way. ? is set as 0.90 in
this paper.
178
Table 2: The cluster validation method for evalua-
tion of values of sense number k.
Function: Cluster Validation(k, D, q)
Input: cluster number k, data set D,
and sampling frequency q;
Output: the score of the merit of k;
1 Perform clustering analysis using sIB on
data set D with k as input;
2 Construct connectivity matrix Ck based on
above clustering solution on D;
3 Use a random predictor ?k to assign
uniformly drawn labels to instances in D;
4 Construct connectivity matrix C?k
using above clustering solution on D;
5 For ? = 1 to q do
5.1 Randomly sample a subset (D?) with size
?|D| from D, 0 ? ? ? 1;
5.2 Perform clustering analysis using sIB on
(D?) with k as input;
5.3 Construct connectivity matrix C?k using
above clustering solution on (D?);
5.4 Use ?k to assign uniformly drawn labels
to instances in (D?);
5.5 Construct connectivity matrix C??kusing above clustering solution on (D?);
Endfor
6 Evaluate the merit of k using following
objective function:
Mk = 1q
?
? M(C?k , Ck) ? 1q
?
? M(C??k , C?k),
where M(C?, C) is given by equation (1);
7 Return Mk;
M(C?, C) measures the proportion of document
pairs in each cluster computed on D that are also as-
signed into the same cluster by clustering solution
on D?. Clearly, 0 ? M ? 1. Intuitively, if clus-
ter number k is identical with the true value, then
clustering results on different subsets generated by
sampling should be similar with that on full data set,
which gives rise to a local optimum of M(C?, C).
In our algorithm, we normalize M(C?F,k, CF,k)using the equation in step 6 of Table 2, which
makes our objective function different from the fig-
ure of merit (equation ( 1)) proposed in (Levine
and Domany, 2001). The reason to normalize
M(C?F,k, CF,k) is that M(C?F,k, CF,k) tends to de-
crease when increasing the value of k. Therefore for
avoiding the bias that smaller value of k is to be se-
lected as cluster number, we use the cluster validity
of a random predictor to normalize M(C?F,k, CF,k).
3.2 The sIB Clustering Algorithm
Here we used the sIB algorithm (Slonim, et al,
2002) to estimate cluster structure, which measures
the similarity of contexts of instances according to
the similarity of their feature conditional distribu-
tion. sIB is a simplified ?hard? variant of informa-
tion bottleneck method (Tishby, et al, 1999).
Let d represent a document, and w represent a fea-
ture word, d ? D, w ? F . Given the joint distri-
bution p(d,w), the document clustering problem is
formulated as looking for a compact representation
T for D, which preserves as much information as
possible about F . T is the document clustering so-
lution. For solving this optimization problem, sIB
algorithm was proposed in (Slonim, et al, 2002),
which found a local maximum of I(T, F ) by: given
an initial partition T , iteratively drawing a d ? D
out of its cluster t(d), t ? T , and merging it into
tnew such that tnew = argmaxt?Td(d, t). d(d, t) is
the change of I(T, F ) due to merging d into cluster
tnew, which is given by
d(d, t) = (p(d) + p(t))JS(p(w|d), p(w|t)). (2)
JS(p, q) is the Jensen-Shannon divergence, which
is defined as
JS(p, q) = pipDKL(p?p) + piqDKL(q?p), (3)
DKL(p?p) =
?
y
plog pp, (4)
DKL(q?p) =
?
y
qlog qp, (5)
{p, q} ? {p(w|d), p(w|t)}, (6)
{pip, piq} ? {
p(d)
p(d) + p(t) ,
p(t)
p(d) + p(t)}, (7)
p = pipp(w|d) + piqp(w|t). (8)
179
4 The Label Propagation Algorithm for
Task 5 and Task 17
In the label propagation algorithm (LP) (Zhu and
Ghahramani, 2002), label information of any ver-
tex in a graph is propagated to nearby vertices
through weighted edges until a global stable stage
is achieved. Larger edge weights allow labels to
travel through easier. Thus the closer the examples,
more likely they have similar labels (the global con-
sistency assumption).
In label propagation process, the soft label of each
initial labeled example is clamped in each iteration
to replenish label sources from these labeled data.
Thus the labeled data act like sources to push out la-
bels through unlabeled data. With this push from la-
beled examples, the class boundaries will be pushed
through edges with large weights and settle in gaps
along edges with small weights. If the data structure
fits the classification goal, then LP algorithm can use
these unlabeled data to help learning classification
plane.
Let Y 0 ? Nn?c represent initial soft labels at-
tached to vertices, where Y 0ij = 1 if yi is sj and 0
otherwise. Let Y 0L be the top l rows of Y 0 and Y 0U
be the remaining u rows. Y 0L is consistent with the
labeling in labeled data, and the initialization of Y 0U
can be arbitrary.
Optimally we expect that the value of Wij across
different classes is as small as possible and the value
of Wij within same class is as large as possible.
This will make label propagation to stay within same
class. In later experiments, we set ? as the aver-
age distance between labeled examples from differ-
ent classes.
Define n ? n probability transition matrix Tij =
P (j ? i) = Wij?n
k=1 Wkj
, where Tij is the probability
to jump from example xj to example xi.
Compute the row-normalized matrix T by T ij =
Tij/
?n
k=1 Tik. This normalization is to maintain
the class probability interpretation of Y .
Then LP algorithm is defined as follows:
1. Initially set t=0, where t is iteration index;
2. Propagate the label by Y t+1 = TY t;
3. Clamp labeled data by replacing the top l row
of Y t+1 with Y 0L . Repeat from step 2 until Y t con-
verges;
4. Assign xh(l + 1 ? h ? n) with a label sj? ,
where j? = argmaxjYhj .
This algorithm has been shown to converge to
a unique solution, which is Y?U = limt?? Y tU =
(I ? T uu)?1T ulY 0L (Zhu and Ghahramani, 2002).
We can see that this solution can be obtained with-
out iteration and the initialization of Y 0U is not im-
portant, since Y 0U does not affect the estimation of
Y?U . I is u ? u identity matrix. T uu and T ul are
acquired by splitting matrix T after the l-th row and
the l-th column into 4 sub-matrices.
For task 5 and 17, we constructed connected
graphs as follows: two instances u, v will be con-
nected by an edge if u is among v?s k nearest neigh-
bors, or if v is among u?s k nearest neighbors as mea-
sured by cosine or JS distance measure. k is set 10
in our system implementation.
5 Data Sets of Task 2, Task 5 and Task 17
The test data for task 2 includes totally 27132 un-
tagged instances for 100 ambiguous English words.
There is no training data for task 2.
There are 40 ambiguous Chinese words in task
5. The training data for this task consists of 2686
instances, while the test data includes 935 instances.
There are 100 ambiguous English words in the
first subtask of task 17. The training data for this
task consists of 22281 instances, while the test data
includes 4851 instances.
6 Experimental Results of Our Systems at
Task 2, Task 5 and Task 17
Table 3: The best/worst/average F-score of all the
systems at task 2 and the F-score of our system at
task 2 for all target words, nouns and verbs with un-
supervised evaluation.
All words Nouns Verbs
Best 78.7% 80.8% 76.3%
Worst 56.1% 65.8% 45.1%
Average 65.4% 69.0% 61.4%
Our system 63.9% 68.0% 59.3%
Table 3 lists the best/worst/average F-score of all
the systems at task 2 and the F-score of our system
at task 2 for all target words, nouns and verbs with
180
Table 4: The best/worst/average supervised recall of
all the systems at task 2 and the supervised recall of
our system at task 2 for all target words, nouns and
verbs with supervised evaluation.
All words Nouns Verbs
Best 81.6% 86.8% 75.7%
Worst 78.5% 81.4% 75.2%
Average 79.6% 83.0% 75.7%
Our system 81.6% 86.8% 75.7%
Table 5: The best/worst/average micro-average pre-
cision and macro-average precision of all the sys-
tems at task 5 and the micro-average precision and
macro-average precision of our system at task 5.
Micro-average Macro-average
Best 71.7% 74.9%
Worst 33.7% 39.6%
Average 58.5% 62.7%
Our system 71.2% 74.7%
unsupervised evaluation. Our system obtained the
fourth place among six systems with unsupervised
evaluation. Table 4 shows the best/worst/average
supervised recall of all the systems at task 2 and the
supervised recall of our system at task 2 for all tar-
get words, nouns and verbs with supervised evalu-
ation. Our system is ranked as the first among six
systems with supervised evaluation. Table 7 lists
the estimated sense numbers by our system for all
the words at task 2. The average of all the estimated
sense numbers is 3.1, while the average of all the
ground-truth sense numbers is 3.6 if we consider the
sense inventories provided in task 17 as the answer.
It seems that our estimated sense numbers are close
to the ground-truth ones.
Table 5 provides the best/worst/average micro-
average precision and macro-average precision of all
the systems at task 5 and the micro-average preci-
sion and macro-average precision of our system at
task 5. Our system obtained the second place among
six systems for task 5.
Table 6 shows the best/worst/average coarse-
grained score (precision) of all the systems the lexi-
cal sample subtask of task 17 and the coarse-grained
score (precision) of our system at the lexical sample
Table 6: The best/worst/average coarse-grained
score (precision) of all the systems at the lexical
sample subtask of task 17 and the coarse-grained
score (precision) of our system at the lexical sam-
ple subtask of task 17.
Coarse-grained score (precision)
Best 88.7%
Worst 52.1%
Average 70.0%
Our system 86.4%
subtask of task 17. The attempted rate of all the sys-
tems is 100%. So the precision value is equal to the
recall value for all the systems. Here we listed only
the precision for the 13 systems at this subtask. Our
system is ranked as the third one among 13 systems.
7 Conclusion
In this paper, we described the implementation of
our I2R systems that participated in task 2, task 5,
and task 17 at SemEval-2007. Our systems achieved
63.9% F-score and 81.6% supervised recall for task
2, 71.2% micro-average precision and 74.7% macro-
average precision for task 5, and 86.4% coarse-
grained precision and recall for the lexical sample
subtask of task 17. The performance of our system
is very good under supervised evaluation. It may
be explained by that our system has the ability to
find some minor senses so that it can outperforms
the baseline system that always uses the most fre-
quent sense as the answer.
References
Agirre E. , & Soroa A. 2007. SemEval-2007 Task 2:
Evaluating Word Sense Induction and Discrimination
Systems. Proceedings of SemEval-2007, Association
for Computational Linguistics.
Lange, T., Braun, M., Roth, V., & Buhmann, J. M. 2002.
Stability-Based Model Selection. Advances in Neural
Information Processing Systems 15.
Lee, Y.K., & Ng, H.T. 2002. An Empirical Evalua-
tion of Knowledge Sources and Learning Algorithms
for Word Sense Disambiguation. Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing, (pp. 41-48).
181
Levine, E., & Domany, E. 2001. Resampling Method for
Unsupervised Estimation of Cluster Validity. Neural
Computation, Vol. 13, 2573?2593.
Niu, Z.Y., Ji, D.H., & Tan, C.L. 2005. Word Sense
Disambiguation Using Label Propagation Based Semi-
Supervised Learning. Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics.
Slonim, N., Friedman, N., & Tishby, N. 2002. Un-
supervised Document Classification Using Sequential
Information Maximization. Proceedings of the 25th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval.
Tishby, N., Pereira, F., & Bialek, W. (1999) The Infor-
mation Bottleneck Method. Proc. of the 37th Allerton
Conference on Communication, Control and Comput-
ing.
Zhu, X. & Ghahramani, Z.. 2002. Learning from La-
beled and Unlabeled Data with Label Propagation.
CMU CALD tech report CMU-CALD-02-107.
Table 7: The estimated sense numbers by our system
for all the words at task 2.
explain 2 move 3
position 3 express 4
buy 2 begin 2
hope 3 prepare 3
feel 5 policy 2
hold 2 attempt 2
work 5 recall 3
people 4 find 2
system 2 join 2
bill 2 build 2
hour 5 base 3
value 4 management 2
job 5 turn 4
rush 2 kill 2
ask 2 area 5
approve 4 affect 4
capital 4 keep 5
purchase 2 improve 2
propose 2 do 2
see 3 drug 5
president 3 come 5
power 3 disclose 4
effect 2 avoid 3
part 5 plant 2
exchange 4 share 2
state 2 carrier 2
care 5 complete 2
promise 3 maintain 3
estimate 2 development 4
rate 2 space 5
say 2 raise 3
remove 5 future 3
grant 4 network 3
remember 3 announce 5
cause 2 start 3
point 5 order 2
occur 4 defense 5
authority 3 set 3
regard 2 chance 2
go 3 produce 2
allow 4 negotiate 2
describe 2 enjoy 4
prove 3 exist 4
claim 4 replace 3
fix 2 examine 3
end 5 lead 3
receive 3 source 2
complain 3 report 2
need 2 believe 2
condition 2 contribute 3
182
