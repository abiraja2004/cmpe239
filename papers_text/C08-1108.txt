Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 857?864
Manchester, August 2008
Experiments with Reasoning for Temporal Relations between Events
Marta Tatu and Munirathnam Srikanth
Lymba Corporation
Richardson, Texas, United States
marta,srikanth@lymba.com
Abstract
Few attempts have been made to inves-
tigate the utility of temporal reasoning
within machine learning frameworks for
temporal relation classification between
events in news articles. This paper presents
three settings where temporal reasoning
aids machine learned classifiers of tempo-
ral relations: (1) expansion of the dataset
used for learning; (2) detection of inconsis-
tencies among the automatically identified
relations; and (3) selection among multiple
temporal relations. Feature engineering is
another effort in our work to improve clas-
sification accuracy.
1 Introduction
In recent years, there has been a growing inter-
est in temporal information extraction, as more
and more operational natural language processing
(NLP) systems demand dealing with time-related
issues in natural language texts. Machine learning-
based temporal relation identification has been ex-
plored by only a few researchers, including Bogu-
raev and Ando (2005), Mani et al (2006), Cham-
bers et al (2007), and the TempEval 2007 partici-
pants (Verhagen et al, 2007).
For a given ordered pair of elements (x
1
, x
2
),
where x
1
and x
2
are events or times, temporal re-
lation resolution is the task of automatic identifi-
cation of the relation r
i
? TempRel that tem-
porally links x
1
and x
2
. For example, given
the statement Mr. Antar was charged
e
137
last
month
t
237
in a civil suit
e
138
filed
e
140
in federal
c
? Lymba Corporation 2008. Licensed under the Cre-
ative Commons Attribution-Noncommercial-Share Alike 3.0
Unported license (http://creativecommons.org/licenses/by-
nc-sa/3.0/). Some rights reserved.
court in Newark by the Securities and Exchange
Commission (wsj 07781) and the pairs (e
137
, t
137
),
(e
137
, e
138
), and (e
138
, e
140
), the task is to automat-
ically label the given pairs with the is included,
is included, and simultaneous relations, respec-
tively. We note that the granularity of the temporal
relations (TempRel) varies from TimeML?s 14 re-
lations to TempEval?s three coarse-grain relations.
While machine learning approaches attempt to
improve classification accuracy through feature
engineering, Mani et al (2006) introduced a tem-
poral reasoning component to greatly expand the
training data. By computing the temporal closure
of the training data relations, they increased the
training set by a factor of 10. They reported en-
couraging accuracy of classification on event-event
and event-time relations. According to their ex-
periments, the event-event relation accuracy goes
from 62.5% to 94.95% and the event-time rela-
tion accuracy ranges from 73.68% to 90.16%. Re-
cently, extensions of Mani et al (2006)?s research
is briefly described in (Mani et al, 2007). This
technical report addresses two problems found
in (Mani et al, 2006): (1) feature vector dupli-
cation caused by the data normalization process
(once fixed, the accuracy drops to 76.56% and
83.23%) and (2) a somewhat unrealistic evaluation
scheme (we describe Mani et al (2007)?s results in
Section 4.1).
TempEval 2007 is the first standard evaluation
arena that consists of three temporal relation clas-
sification tasks (Verhagen et al, 2007). The par-
ticipants reported F-measure scores ranging from
42% to 55% for event-event relations, and 73% to
80% for event-time relations.
Because of their different experimental settings,
1All examples shown here are taken from TimeBank 1.2.
857
the results reported in (Mani et al, 2007) cannot
be directly compared with those of TempEval 2007
participants. Among others, the three major differ-
ences are:
1. significantly different training and testing
data. Although the datasets used Time-
Bank 1.2 (Pustejovsky et al, 2003), Mani
et al (2007) added the AQUAINT Corpus
(www.timeml.org) to their experimental data;
2. different sets of temporal relations. Mani et
al. (2006; 2007) target six normalized rela-
tions (before, immediately before (ibefore),
includes, ends, begins and simultaneous). In
TempEval 2007, a set of three coarse-grain
temporal relations was used (before, after,
and overlap).
3. different relation scope. In (Mani et al, 2006;
Mani et al, 2007), event-event temporal re-
lations are discourse-wide, i.e. any pair of
events can be temporally linked. For Tem-
pEval 2007, the event-event relations are re-
stricted to events within two consecutive sen-
tences.
These two modeling frameworks for solving the
problem of temporal relation classification pro-
duce highly dissimilar results. With this in mind,
we are interested in two issues in this paper: (1)
How might temporal reasoning assist in tempo-
ral relation identification? (2) What other features
might be used to improve the performance of clas-
sification? As a byproduct of our exploration to
these two questions, we hope to find some insights
on why the same problem explored under different
environment produces highly divergent results.
In this paper, we investigate several interactions
between temporal reasoning and a machine learn-
ing approach for temporal ordering of events in
natural language texts. We continue by describing
the data used for our experiments. In Section 3, we
briefly describe the set of features we currently use
to build Support Vector Machine (SVM) (Chang
and Lin, 2001) and Maximum Entropy (ME) mod-
els for temporal relation resolution. The three in-
teractions we envision between temporal reasoning
and the learned models are presented in Section 4.
In conclusion, we present a discussion of our ex-
perimental results and future research directions.
2 Data Preparation and Analysis
2.1 TimeBank 1.2
In this paper, we use the TimeBank 1.2 data (Puste-
jovsky et al, 2003). This is the first attempt to
create a corpus with human annotated temporal re-
lations. It contains 183 news documents collected
from several news agencies.
2.2 Data normalization
Similar to (Mani et al, 2006; Mani et al, 2007),
we use a normalized version of the 14 tempo-
ral relations annotated in TimeBank where the
inverse relations are removed and simultaneous
and identity are collapsed as well as during and
is included. The distribution of the normalized
event-event temporal relations annotated in the
data we used for training our temporal resolution
models is shown in Table 2.
2.3 Experimental data
For the experiments described in this paper, we
used a random 80-20 percent split of the TimeBank
data to train and test the learned classifiers (36 ran-
domly selected documents for testing and the re-
maining 147 for training the models) and 5-fold-
cross-validation of the training data for parame-
ter tuning. We note that our experimental setup
is closer to the one used in (Mani et al, 2006;
Mani et al, 2007). Noting that we do not use the
AQUAINT Corpus in our experiments, our results
can be compared with theirs, but not with the Tem-
pEval system performances.
3 Feature Engineering
As reported by participants in TempEval
2007 (Verhagen et al, 2007), (Boguraev and
Ando, 2005), (Chambers et al, 2007), and (Mani
et al, 2007), most of the features used for learning
are syntactic attributes extracted for single event
terms. In our work, we have experimented with
semantic features and attributes which take the
event?s linguistic context into consideration (Min
et al, 2007). Our experiments show that only
few features are critical and impact the classifier?s
accuracy (Table 1). These include the basic
features available in TimeBank, e.g. event-class,
tense, aspect, polarity, modality, stem, and part of
speech of event terms (Baseline row in Table 1).
Additional features that we explored include:
858
Feature set Accuracy (%)
Baseline 46.3
Baseline with sameActor +0.4
Baseline with eventCoref +0.2
Baseline with oneSent +4.0
Baseline with relToDocDate +0.2
Baseline with tensebigram +0.8
Baseline with tensetrigram +0.6
Baseline with all 57.4
Table 1: New features impact
1. sameActor. This binary feature indicates
whether the two events share the same se-
mantic role AGENT. The motivation behind
this feature is that two event terms, especially,
verbs, which have the same agent, have a
closer semantic relationship and, accordingly,
are temporally related.
2. eventCoref. This binary attribute captures the
event co-reference information. If two events
co-refer, even though they have different sur-
face forms, they must take place simultane-
ously. For instance, the offer and deal events,
mentioned in the following sentences, refer to
the same transaction and, therefore, must be
linked by a simultaneous relation.
a) Sony Corp. completed its tender offer for Columbia
Pictures Entertainment Inc., with Columbia sharehold-
ers tendering 99.3% of all common shares outstanding
by the Tuesday deadline.
b) Sony Columbia Acquisition Corp., formed for the
Columbia deal, will formally take ownership of the
movie studio later this month, a spokesman said.
3. oneSent. This binary feature is true if the two
events are part of the same sentence. Cham-
bers et al(2007) introduced this feature in
their experiments, and our analysis shows that
this attribute has a relatively larger contribu-
tion to the overall performance. The intu-
ition behind this feature is that the closer two
events get, the closer their temporal relation-
ship is.
4. relToDocDate. This feature encodes the tem-
poral relation between each event and the
Document Date. This was one of the sub-
tasks of TempEval 2007 and we used this re-
lationship as a feature. Our motivation is that
we might be able to infer the relationship be-
tween two events e
1
and e
2
from the tem-
poral relations they have with the Document
Date. For example, if before(e
1
, DocDate)
and after(e
2
, DocDate) are true, then
before(e
1
, e
2
). There may be two reasons for
the low impact of this feature: (1) an accu-
rate computation of the temporal relation be-
tween an event and the Document Date is not
easy, as demonstrated in TempEval 2007 and
(2) if two events have the same relation with
the Document Date, there is no way to deter-
mine the event-event relation.
5. tenseBigram and tenseTrigram. Going be-
yond using the tense attribute for single event
terms, we extract bigrams and trigrams with
the tense values of the current event and im-
mediately preceding and following events.
This feature is intended to reflect the tense
shifts of sequential events as part of a larger
context of the current event.
All these features have a positive impact on the
performance of the learned classifiers (Table 1).
Further improvement is desired and we use tem-
poral reasoning in three different settings in an at-
tempt to obtain more accurate temporal relations.
4 Temporal Reasoning
Following our feature set improvements for ma-
chine learned models of temporal relations, we
turned to temporal reasoning and explored differ-
ent ways in which it can aid the resolution of tem-
poral relations. We experimented with three dif-
ferent interactions between our temporal reasoning
and temporal relation resolution modules.
Our natural language reasoning engine (Tatu
and Moldovan, 2005; Tatu and Moldovan, 2007)
makes use of (1) a first-order logical representation
of the input document which captures the concepts
mentioned in the text, their attributes including
named entity class values, event class or normal-
ized values (for times) and the syntactic as well as
the semantic dependencies between concepts2; (2)
a rich set of axioms which encode the knowledge
needed to derive meaningful information from a
document; (3) a logic prover which operates in a
proof by contradiction manner (a hypothesis H is
entailed by a text T assumed to be true, denoted
by T ? H , if and only if (T ? ?H) ? ?, where ?
is false). Given the logical transformation of a text
T , the prover uses the knowledge encoded in the
2These dependencies include the temporal relations iden-
tified either by human annotators or by the models presented
in Section 3.
859
axioms (Bk) to derive new information (T ?) about
T
3 and scores the best mapping of the hypothesis
H to T ?.
For the temporal relation resolution experiments
presented in this paper, we are interested in de-
riving additional temporal information from an in-
put document without checking the entailment be-
tween this document and a hypothesis. There-
fore, for the following tasks, the text T is a
TimeBank 1.2 document and the set of axioms
Bk used by the prover contains 94 temporal
axioms which link each temporal relation with
its inverse (R?1(x, y) ? R(y, x), for example
before(x, y) ? after(y, x)) and define the tem-
poral relation resulting from the combination of
two relations (R
1
(x, y) ? R
2
(y, z) ? R
3
(x, z),
for example, before(x, y) ? before(y, z) ?
before(x, z)). These axioms were derived from
Allen?s interval algebra (Allen, 1983).
We note that the prover computes and uses tem-
poral relations between any two temporal expres-
sions mentioned in an input TimeBank document4
(e.g. now [19891101] and last year [1988] are
linked by a before temporal relation in wsj 0324).
Within this reasoning setup, the information de-
rived by the prover (T ?) will include the temporal
closure of the input text?s relations. We note that
the temporal closure includes event-event, event-
time and time-time temporal relations. We also
note that the temporal axioms are considered 100%
accurate (if the temporal relations given as input
are correct, then the temporal relations derived us-
ing the axioms are also correct).
4.1 Training data expansion
Our first effort to create more accurate temporal re-
lation resolution classifiers given our temporal rea-
soning engine is to augment the gold training data
with new relations from the temporal closure of the
relations identified by human annotators. There-
fore, given the 3,527 temporal relations annotated
in the TimeBank data used to train our initial tem-
poral resolution models, we derived 12,270 new
relations (an increase of 3.47 times). We show in
Table 2 statistics of the normalized event-event re-
lations for both the original and the closed train-
ing data. We note that the temporal inconsisten-
cies identified in the original training data (by the
3
T
? contains all the information the prover can derive
from T given the axioms Bk.
4We restricted the time-time relations to only before, si-
multaneous, and after.
Relation Original data Closed (?) data
Freq. % Freq. %
ibefore 51 2.06 137 1.59
begins 52 2.10 119 1.38
ends 61 2.47 125 1.45
includes 434 17.59 1,161 13.47
before 885 35.88 3,165 36.73
simultaneous 983 39.86 3,909 45.36
Total 2,466 100.00 8,616 100.00
Table 2: Normalized training data (event-event re-
lations)
procedure described in Section 4.2) were resolved
manually by one of the authors of this paper.
We built SVM and ME models from the total
of 8,616 normalized temporal relations using the
set of 15 features described in Section 3. Table 3
shows the performance of the learned models on
the test data (original data as well as closed test
data). Unlike (Mani et al, 2006), the accuracy of
Accuracy for event-event relations
Training data Original test Closed test Train
(845) (4,189)
ME models
Original (2,466) 50.4 46.1 83.3
Closed (8,616) 47.0 41.0 76.1
SVM models
Original (2,466) 56.9 45.8 74.2
Closed (8,616) 52.4 52.0 77.5
Table 3: Event-event temporal resolution
the learned classifiers drops when they are trained
on the closed training dataset. By analyzing the re-
sults from Table 3, one cannot help but notice the
high accuracy on the data used for training and the
significant difference between the performance on
the training and testing datasets. This may suggest
that (1) the machine learners overfit the models on
the training data and they are not able to gener-
alize and resolve the relations in the test data5 or
(2) the two datasets are very different (in terms
of feature values) and the data split happened to
create a training data which is not (fully) charac-
teristic to the problem we are trying to solve (the
two datasets have different distributions). There-
fore, we measured the accuracy of ME models for
event-event relation resolution using 5-fold-cross-
validation of the entire TimeBank data (Table 4).
For these experiments, each TimeBank document
(with all its temporal relations) was used as part
5The accuracy of the SVM models is lower on the training
data when compared with the ME models while their perfor-
mance on the test dataset is better.
860
Test data Train data
1/5 of the data remaining 4/5s
5-fold-cross split at the document level
Original (3,311) 57.4 89.5
Closed (11,530) 58.2 85.5
5-fold-cross split at the relation level
Original (3,311) 58.3 90.0
Closed (11,530) 73.4 85.3
Table 4: Average ME accuracy for event-event re-
lations using 5-fold-cross-validation on the entire
TimeBank data
of either the training or the testing dataset. Our re-
sults for the random 5-fold-cross split of the data at
the document level are similar to the ones obtained
for the models learned on the pre-established train-
ing data (top two rows in Table 4). Thus, our ini-
tial split of the data was not an ?unfortunate? divi-
sion. The same significant difference between the
performance on the unseen data and the training
set can be seen. This suggests that some overfit-
ting occurs. Features, such as the event term stem,
with a large number of possible values mislead the
machine learning algorithms and the models they
create are not able to correctly classify event pairs
with unseen values for these high-valued features.
For instance, showdown is part of a single Time-
Bank document (AP900816-0139) and the mod-
els learned using other documents will misclassify
showdown?s temporal relations. We note that, by
expanding the training data using its temporal clo-
sure, no new events are added to the training set,
only new temporal relations between the same set
of events are added. Long-term solutions include
(1) the expansion of the annotated data or (2) the
reduction in the number of values for certain fea-
tures (for example, by generalizing the event term
stem to its WordNet hypernym). In an attempt to
homogenize the feature values for the training and
the testing datasets, we split the set of normalized
event-event temporal relations annotated in Time-
Bank into training and testing without considering
the document boundaries. The performance of the
learned classifiers increases by 1% when trained
on unclosed data and by more than 15% when the
closed data is used (Table 4).
In their most recent technical paper, Mani et
al. (2007) revise their evaluation method and report
performance values for classifiers learned by par-
titioning the data at the document level (accuracy
drops from 59.68% to 51.14% when closed train-
ing data is used). These results are consistent with
our findings. In the near future, we shall experi-
ment with the second solution we propose above.
4.2 Testing data validation
Given that almost half of the temporal relations
automatically identified for the testing data are
incorrect when compared to the gold annotation,
we decided, as our second experiment, to use
temporal reasoning to find temporal inconsisten-
cies and replace some of the relations contribut-
ing to the inconsistency by the immediate lower
confidence relation returned by the learned classi-
fiers. For this purpose, we use an additional set
of 77 temporal axioms which encode the irreflex-
ivity of temporal relations (?R(x, x), for exam-
ple, ?before(x, x)) and their empty intersections
(R
1
(x, y) ? ?R
2
(x, y) when R
1
6= R
2
, for ex-
ample before(x, y) ? ?simultaneous(x, y)).
Our process of testing data validation is itera-
tive. Once a temporal inconsistency is identified
in the test data, it is resolved and the procedure
which computes the temporal closure is re-started.
A temporal inconsistency in a TimeBank docu-
ment (T ) is detected every time ? ? T ?. The
automatically identified temporal relations (part of
the text T ) which contributed to the derivation of
? become candidates for the resolution of the in-
consistency6. These candidates are sorted based
on the confidence assigned by the machine learn-
ing algorithm7 and the lowest confidence relation
is replaced either by the temporal relation found
by the prover which directly contradicted the auto-
matically identified relation8 (Figure 1(a)) or, for
the cases where such a relation does not exist, by
the immediate lower confidence relation identified
by the learned models (Figure 1(b)).
If, for example, for the statement The US is
bolstering its military presence in the gulf, as
President Clinton discussed
e
1
the Iraq crisis with
the one ally who has backed
e
2
his threat
e
3
of
force, British prime minister Tony Blair, the ME
classifier built in Section 3 identifies the tempo-
ral relations before(e
2
, e
1
) (confidence: 0.53),
before(e
3
, e
2
) (0.47) and includes(e
3
, e
1
) (0.42),
the prover identifies the temporal inconsistency
6The temporal closure axioms are accurate and do not ?in-
troduce? incorrect temporal relations.
7For all experiments which exploit the confidence as-
signed by the machine learning algorithm, we use the learned
ME models (SVM models do not provide a confidence for
their decision).
8The confidence of a relation derived by the prover is the
average of the its ?parent?s confidence values.
861
 event1 
event3 
event4 
event2 c2 
cmin 
c3 
c1 R34 
R23 
R12 R?13 
R?14 
R14 
(a) R
14
replaced by R?
14
 
event1 
event3 
event4 
event2 cmin 
c4 
c3 
c1 R34 
R23 
R12 R?13 
R?14 
R14 
(b) R
23
replaced by the next
best relation identified for
(event
2
, event
3
)
Figure 1: Temporal inconsistency resolution
generated by these three relations and replaces the
lowest confidence relation (includes(e
3
, e
1
)) with
the relation it derives from the closure of the other
two relations (before(e
3
, e
1
), confidence: 0.50).
We note that, during the inconsistency checking
process, all types of temporal relations are used
(event-event, event-time and time-time). For this
inconsistency resolution process, we make the as-
sumption that only one of the temporal relations
which generated the inconsistency is incorrect and
should be replaced.
For the testing dataset described in Section 2.3,
the validation algorithm found inconsistencies in
only 25% of the test documents. This is not very
encouraging, given that the accuracy of the tem-
poral relations identified in the other 75% of the
documents is 50.4%. The documents marked as
inconsistent include, on average, 3.66 temporal in-
consistencies (with a maximum of 8 in a single
document). For each pair of events, we consid-
ered only the top three temporal relations (in terms
of confidence) identified by the learned classifiers.
When the third relation identified for a given pair
of events had to be removed by the inconsistency
resolution algorithm, no other temporal relation
was added to replace it. Table 5 shows the impact
of the validation step on the unclosed test data.
Precision Recall F-measure
Baseline 50.4 50.4 50.4
With test validation 50.1 49.7 49.9
Table 5: Performance change after the testing data
validation step. The baseline is the ME model
learned on the original (unclosed) training data.
Our error analysis shows that, for each discov-
ered temporal inconsistency, more than one incor-
rect relation lead to an inconsistent temporal clo-
sure. Frequently enough, replacing the lowest con-
fidence relation does not resolve the inconsistency
and the temporal relations used to replace it are,
in turn, replaced in the next iterations (the confi-
dence of the replacing relation is lower than the
confidence of the replaced relation). The ME clas-
sifier?s numerous errors and the low applicability
of this process make its contribution to the overall
temporal relation resolution process negative. Our
future work will focus on (1) experimenting with
less erroneous data for which our one-incorrect-
relation-per-inconsistency assumption holds ((per-
haps) the models learned from closed training data
using a data split at the relation level) and (2) test-
ing the existence of a consistent temporal closure
in the absence of the lowest confidence relation. If
none of the six temporal relations that we use to la-
bel an event-event relation can replace the lowest
confidence relation and lead to a consistent tempo-
ral closure, then our candidate incorrect relation is
among the other higher confidence relations. We
also note that we rely heavily on the confidences
automatically assigned by the ME classifiers.
Mani et al (2007) briefly describe a Greedy
method for ensuring global consistency of auto-
matically labeled testing data. No evaluation re-
sults are reported. As far as we can tell, Mani at
el. (2007) use this algorithm to decide whether or
not to assign the top 1 relation automatically iden-
tified by ME classifiers to a given pair of events.
No attempts are made to replace this relation. Our
validation algorithm uses lower confidence rela-
tions found by the learned models for the same pair
of events to replace the lowest confidence relation
that leads to a temporal inconsistency.
4.3 Temporal relation alignment
In the previous section, we used temporal reason-
ing to replace certain relations automatically iden-
tified by the learned temporal relation resolution
models with the next best relation (in terms of
the confidence) found for the same pair of events.
For our third experiment, we use the top n re-
lations automatically identified for a single pair
of events. Across a document, these relation-
ships can be grouped to form different tempo-
ral orderings of the events mentioned in the doc-
ument. For instance, for four pairs of events,
81 different temporal settings can be created us-
ing the top 3 temporal relations. Figure 2 shows
two of these 81 facets ({R
12
, R
?
23
, R
?
34
, R
41
} and
{R?
12
, R
23
, R?
34
, R?
41
}) for events e
1
, . . . , e
4
.
For these event temporal orderings, we pro-
862
 R12,c1 
(event1,event2) (event2,event3) (event3,event4) (event4,event1) 
R?12,c?1 
R?12,c?1 
R23,c2 
R?23,c?2 
R?23,c?2 
R34,c3 
R?34,c?3 
R?34,c?3 
R41,c4 
R?41,c?4 
R?41,c?4 
Figure 2: Two possible relation alignments
pose to use our temporal reasoning module to de-
rive, score, and rank their temporal closures. We
make the assumption that the correct document-
level event ordering, the document?s temporal co-
hesion can be identified by measuring the closure
of the document?s temporal relations and that or-
derings that use incorrect relations do not gener-
ate good closures. Thus, the relations that generate
the best temporal closure will be considered final
and will be used to label the test document?s event-
event and event-time pairs.
For the example shown in Figure 2, 81 different
temporal closures are generated depending on the
set of relations used to derive them from. In order
to find the final four temporal relations between
events e
1
, . . . , e
4
, we score and order the derived
temporal closures. The best closure decides, for
each pair of events, which relation among the top
3 should be selected as final.
Our first step is to identify the best value for
n. Table 6 shows the maximum gain in perfor-
mance when multiple relations are considered for
the same pair of events (an instance is considered
correct if the gold annotation is among the top n
relations returned by the system).
Top n relations Accuracy (%)
1 57.40
2 80.66
3 94.49
4 97.38
5 99.12
6 100.00
Table 6: Top n oracle performance using 5-fold-
cross-validation on the TimeBank data
Because there is substantial improvement in the
top 3 relation set, we use for our experiments the
first three relations identified by the ME classi-
fiers. But, if we consider the top 3 relations for
each pair of events in a document, we end up with
3
N possible alignments, where N is the number of
event-event and event-time pairs9 and the scoring
9For each time-time pair, there is a single temporal relation
with confidence equal to 1.
Accuracy
Baseline - top 1 50.4
Oracle (upper bound) - top 3 92.4
With test alignment 47.5
Table 7: Test dataset performance change after the
testing data alignment step. The top 1 and top
3 baselines were generated using the ME model
learned on the original (unclosed) training data.
and ranking of all 3N temporal closures becomes
hardly possible. Therefore, we use a more Greedy
approach. Iteratively, we score and rank temporal
closures derived from a small set of top 3 relations
between N ? event-event pairs (N ? < N ) and any
final temporal relations. The best closure is used
to decide on N ? temporal relations which will be
added to the best partial alignment and will be used
to compute all the following temporal closures.
Secondly, we must identify the temporal clo-
sure scoring function. For our experiments, this
function takes into account the size of the tempo-
ral closure (|T ?|) as well as the confidence val-
ues of the relations identified by the ME classi-
fiers in the test set (not derived by the tempo-
ral closure algorithm) (only {c
12
, c
?
23
, c
?
34
, c
41
} and
{c?
12
, c
23
, c?
34
, c?
41
} for the example shown in
Figure 2). The correlation between these param-
eters and the scoring function is not straightfor-
ward. A preference for the confidence values fa-
vors closures which use only the top relations (in
terms of confidence). However, weighing the size
of the temporal closure leads to a result dominated
by relations that close very well10, such as simulta-
neous or before (which are also very frequent in the
dataset). In the settings which produced the results
shown in Table 7, we used the score
1
function: for
T = {(R
1
, c
1
), . . . , (R
k
, c
k
)},
score
1
(T
?
) = lg(|T
?
|)?
k
?
i=1
c
i
.
The temporal relation accuracy drops by 3% after
the relation selection among the top 3 best tem-
poral relations for the testing documents. Posibile
explanation: score
1
does not promote the close-
to-gold temporal closures. The difinition of a good
scoring function is not an easy process. Machine
learning approaches might give us better coefi-
cients for the parameters we consider. Alternativ-
elly, our main assumption might prove incorrect:
10When present, these relations will quickly generate many
others in the temporal closure.
863
temporal closure is not a good indicator of a docu-
ment?s event ordering. The information conveyed
by a document need not disclose a rich total order-
ing of its events.
5 Conclusion
In this paper, we briefly described our feature en-
gineering efforts for temporal relation resolution
and we analyzed three methods that exploit tem-
poral reasoning and, more specifically, the closure
of temporal relations, for the purpose of improving
the performance of machine learned classifiers of
temporal relations between events in text.
Based on our experiments, we find that feature
engineering helps improve the classification prob-
lem, when compared with several baseline perfor-
mances. However, given our current NLP capabil-
ities, it is clear that we are faced with the perfor-
mance bottleneck problem (accuracy below 60%).
Any attempt to derive more advanced features de-
mands more sophisticated methodologies of mod-
eling temporal expressions, events and their re-
lationships as well as advanced discourse under-
standing capabilities. For instance, the temporal
duration or the start/end time points of events are
highly useful for learning temporal relations. But,
this introduces an even more challenging problem.
In terms of the utility of temporal reasoning
in classifying temporal relation, the idea of using
temporal reasoning to boost training data is cer-
tainly sound. But in order for the boosted train-
ing data to really take effect, more advanced fea-
tures need to be investigated. Certainly, the pro-
cess of dividing the data into training and testing
has its impact on the system?s performance and we
are faced with the data sparseness problem. Tem-
poral inconsistencies in our automatically labeled
test dataset occurred in just a few test documents
and the resolution process did not impact the sys-
tem?s performance. Improvements are needed in
the process of selection of the to-be-replaced re-
lations. Temporal data alignment largely depends
on the function used to score the temporal closures
and we plan to analyze the temporal closure of
the training data and to explore other scoring func-
tions.
References
Allen, J. F. 1983. Maintaining Knowledge about
Temporal Intervals. Communications of the ACM,
26(11):832?843.
Boguraev, B. and R. K. Ando. 2005. TimeML-
Compliant Text Analysis for Temporal Reasoning.
In Proceedings of Nineteenth International Joint
Conference on Artificial Intelligence (IJCAI-05),
pages 997?1003, Edinburgh, Scotland, August.
Chambers, N., S. Wang, and D. Jurafsky. 2007. Clas-
sifying Temporal Relations Between Events. In Pro-
ceedings of the ACL 2007 Demo and Poster Sessions,
pages 173?176, Prague, Czech Republic, June.
Chang, C. and C. Lin, 2001. LIBSVM: a library
for support vector machines. Software available at
www.csie.ntu.edu.tw/?cjlin/libsvm.
Mani, I., M. Verhagen, B. Wellner, C. Min Lee, and
J. Pustejovsky. 2006. Machine Learning of Tem-
poral Relations. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the ACL, pages 753?760,
Sydney, Australia, July.
Mani, I., B. Wellner, M. Verhagen, and J. Pustejovsky.
2007. Three Approaches to Learning TLINKs in
TimeML. Technical Report CS-07-268, Computer
Science Department, Brandeis University, Waltham,
USA.
Min, Congmin, Munirathnam Srikanth, and Abraham
Fowler. 2007. LCC-TE: A Hybrid Approach to
Temporal Relation Identification in News Text. In
Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), pages
219?222, Prague, Czech Republic, June.
Pustejovsky, J., P. Hanks, R. Sauri, A. See,
R. Gaizauskas, A. Setzer, D. Radev, B. Sundheim,
D. Day, L. Ferro, and M. Lazo. 2003. The TIME-
BANK Corpus. In Proceedings of Corpus Lin-
guistics, pages 647?656, Lancaster University (UK),
March.
Tatu, Marta and Dan Moldovan. 2005. A Semantic Ap-
proach to Recognizing Textual Entailment. In Pro-
ceedings of the HTL-EMNLP 2005, pages 371?378,
Vancouver, BC Canada, October.
Tatu, Marta and Dan Moldovan. 2007. COGEX at
RTE3. In Proceedings of the ACL-PASCAL Work-
shop on Textual Entailment and Paraphrasing, pages
22?27, Prague, Czech Republic, June.
Verhagen, M., R. Gaizauskas, F. Schilder, M. Hep-
ple, G. Katz, and J. Pustejovsky. 2007. SemEval-
2007 Task 15: TempEval Temporal Relation Identi-
fication. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
pages 75?80, Prague, Czech Republic, June.
864
