Intentions, Implicatures and Processing of Complex Questions
Sanda M. Harabagiu and Steven J. Maiorano and Alessandro Moschitti and Cosmin A. Bejan
University of Texas at Dallas
Human Language Technology Research Institute
Richardson, TX 75083-0688, USA
Abstract
In this paper we introduce two methods for
deriving the intentional structure of complex
questions. Techniques that enable the deriva-
tion of implied information are also presented.
We show that both the intentional structure
and the implicatures enabled by it are essen-
tial components of Q/A systems capable of suc-
cessfully processing complex questions. The
results of our evaluation support the claim that
there are multiple interactions between the pro-
cess of answer finding and the coercion of in-
tentions and implicatures.
1 Introduction
The Problem of Question Intentions.
When using a Question Answering system to find
information, the user cannot separate the intentions
and beliefs from the formulation of the question. A
direct consequence of this phenomenon is that the user
incorporates his or her intentions and beliefs into the
interrogation. For example, when asking the question:
Q1: What kind of assistance has North Korea received
from the USSR/Russia for its missile program?
the user associate with the question a number of in-
tentions, that maybe expressed a set of intended ques-
tions. Each intended question, in turn generates implied
information, that maybe expresses as implied questions.
For question Q1, a list of intended questions and implied
questions is detailed in Table1.
Most of the intended questions are similar with the
questions evaluated in TREC1. For example questions
1The Text REtrieval Conferences (TREC) are evaluation
workshops in which Information Retrieval tasks are annually
tested. Since 1999 the performance of question answering sys-
tems are measured in the TREC QA track.
Qi1, Qi2 and Qi3 are so-called definition questions, since
they ask about defining properties of an object. However
unlike the TREC definition questions, these questions ex-
press unstated intentions of the questioner and need to be
processed in the context of the original complex question
Q1. Questions Qi4 and Qi5 are factoid questions, request-
ing information about facts or events. Qi6 asks about the
source of information that enables the answers of ques-
tion Q1.
Questions Qi1, Qi2, Qi3, Qi4 and Qi5 result from the in-
tentional structure generated when processing question
Q1 or questions similar to it. When intended questions
are generated, their sequential processing (a) represents a
decomposition of the complex question and (b) generates
a scenario for finding information; thus questions like Q1
are also known as scenario questions.
Intentions and Implicatures.
As Table 1 suggests, the implied information takes the
form of alternatives that guide the answers to intended
questions. For example, question Qm11 lists alternatives
for the answer to Qi1 whereas Qm21 lists components of
the answer of Qi1. Implicatures may also involve tem-
poral inference, e.g. the implied questions pertaining
to Qi3 and Qi4. Additionally, the reliability of infor-
mation is commonly an implicature in the case of sce-
nario questions, since the causal and temporal inference
is based on the quality and correctness of the available
data sources. Neither intentions or implicatures are rec-
ognizable at syntactic or semantic level, but they both
play an important role in the question interpretation. In-
terpretations disregard the implied information or the user
intentions determine the extraction of incorrect answers,
thus influence the performance of Q/A systems.
Our solution.
In this paper we present two different mechanisms of
deriving the question implicatures. Both methods start
from the syntactic and semantic content of the interro-
gation. The first method considers only the semantic
Intended Questions Implied Questions
Qi1 :What is the USSR/Russia? Qm11 :Is this the Soviet/Russian government?
Qm12 :Does it include private firms, state-owned firms, educational
institutions, and individuals?
Qi2 :What is North Korea? Qm21 :Is this the North Korean government only?
Qm22 :Does it include private firms, state-owned firms, educational
institutions, and individuals?
Qi3 :What is assistance? Qm31 :Is it the transfer of complete missile systems, licensing agreements,
components, materials, or plans?
Qm32 :Is it the training of personnel?
Qm33 :What kind of training?
Qm34 :Does transfer include data, and, if so, what kind of data?
Qm35 :Does transfer include financial assistance, and, if so, what kind of
financial assistance?
Qi4 :What are the missiles in the North Qm41 :Are any based upon Soviet/Russian designs?
Korean inventory? Qm42 :If so, which ones?
Qm43 :What was the development timeline of the missiles?
Qm44 :Did any timeline differ significantly from others?
Qm45 :Did North Korea receive assistance from other sources besides
USSR/Russia to develop these missiles?
Qi5 :When did North Korea receive assistance Qm51 :Was any intended assistance halted, stopped or intercepted?
from the USSR/Russia?
Qi6 :What are the sources of information? Qm61 :Are the sources reliable?
Qm62 :Is some information contradictory?
Table 1: Question decomposition associated with question Q1
meaning of the words used in the question whereas the
second method considers the predicate-argument struc-
ture of the question and candidate answers as a form of
shallow semantics that enables the inference of the inten-
tional structure. Question implicatures are derived from
lexico-semantic paths retrieved from the WordNet lexico-
semantic database. These paths bring forward new con-
cepts, that may be associated with the question implica-
tures when testing the paths against the conversational
maxims introduced by Grice in (Grice, 1975a). For ex-
ample, if the user asks ?Will Prime Minister Mori survive
the crisis??, the first method detects the user?s belief that
the position of the Prime Minister is in jeopardy, since
the concept DANGER is coerced although none of the
question words directly imply it.
The second method generates the intentional structure
of the question, enabling a more structured representa-
tion of the pragmatics of question interpretation. The in-
tentional structure is based on a study that we have con-
ducted for capturing the motivations of a group of users
when asking series of questions in several scenarios. We
show how the intentional structures that we have gathered
guide the coercion of knowledge that helps to support the
acceptance of rejection of computational implicatures.
The derivation of intentional structures is made possi-
ble by predicate-argument structures that are recognized
both at the question level and at the candidate answer
level. In this paper we show how richer semantic ob-
jects can be derived around predicate-argument structures
and how inferential mechanisms can be associated with
such semantic objects for obtaining correct answers. The
rest of the paper is organized as follows. In Section 2
we describe several forms of complex questions that re-
quire the derivation of computational implicatures. Sec-
tion 3 details the models of Question Answering that we
considered and Section 4 shows our methods of deriving
predicate-argument structures and their usage in identi-
fying answers for questions. Section 5 details the inten-
tional structures whereas Section 6 summarizes the con-
clusions.
2 Question Complexity
Since 1999, the TREC QA evaluations focused on fac-
toid questions, such as ?In what year did Joe Di Maggio
compile his 56-game hitting streak?? or ?Name a film in
which Jude Law acted.?. The answers to most of these
questions belong to semantic categories associated with
each question class. For example, questions asking about
a date or a year can be answered because Named Entity
Recognizers identify a temporal expression in a candidate
text span. Similarly, names of people or organizations
are provided as answers to questions such as ?Who is the
first Russian astronaut?? or ?What is the largest software
company in the world??. Most Named Entity Recogniz-
ers detect names of PEOPLE, ORGANIZATIONS, LOCA-
TIONS, DATES, PRICES and NUMBERS. For factoid Q/A,
the list of name categories needs to be extended, as re-
ported in (Harabagiu et al, 2003) for recognizing many
What kind od assistance has North Korea received fromComplex Question:
What kind of assistance has X received from Y for Z?Question PATTERN:
X=North Korea FOCUS=Z=misile program
Intended Questions:
Definition Questions:
What is Y? "What is USSR/Russia?"
What is assistance? 
Elaboration of FOCUS:
Reliability: "What are the sources of information?"
the USSR/Russia for its missile program?
assistance from the USSR/Russia?"
(1) RESULTATIVE
(2) TEMPORAL
North Korean inventory?"
"When did North Korea receive
Y=USSR/Russia
What is X? "What is North Korea?"
"What are the missiles in the
Figure 1: Decomposition of scenario question into intended questions
more types of names, e.g. names of movies, names of
diseases, names of battles. Moreover, the semantic cat-
egories of the extended set of names need to be incor-
porated into an answer type taxonomy that enables the
recognition of (a) the expected answer type and (b) the
question class. The taxonomy of expected answer types
is useful because the answer is not always a name; it can
be a lexicalized concept or a concept that is expressed by
a paraphrase.
The TREC evaluations have also considered two more
classes of questions: (1) list questions and (2) definition
questions. The list questions have answers that are typ-
ically assembled from different documents. Such ques-
tions are harder to answer than factoid questions be-
cause the systems must detect duplications. Example of
list questions are ?Name singers performing the role of
Donna Elvira in performances of Mozart?s ?Don Gio-
vani?.? or ?What companies manufacture golf clubs??.
Definition questions require a different form of process-
ing that factoid questions because no taxonomy of answer
types needs to be used. The expected answer type is a def-
inition, which cannot be represented by a single concept.
Q/A systems assume that definitions are given by follow-
ing a set of linguistic patterns that need to be matched for
extracting the answer. Example of definition questions
are ?What is a golden parachute?? or ?What is ETA in
Spain??.
In (Echihabi and Marcu, 2003) a noisy channel model
for Q/A was introduced. This model is based on the
idea that if a given sentence SA contains an answer sub-
string A to a question Q, then SA can be re-written into
Q through a sequence of stochastic operators. Not only a
justification of the answer is produced, but the conditional
probability P(Q?SA) re-ranks all candidate answers.
A different viewpoint of Q/A was reported in (Itty-
cheriah et al, 2000). Finding the answers A to a ques-
tion Q was considered a classification problem that maxi-
mizes the conditional probability P(A?Q). This model is
not tractable currently, because (a) the search space is too
large for a text collection like the TREC or the AQUAINT
corpora; and (b) the training data is insufficient. There-
fore, Q/A is modeled by the distribution P(C?A,Q)
where C measures the ?correctness? of A to question
Q. By using a hidden variable E that represents the ex-
pected answer type, P(C?A,Q) = ?E p(C,E?Q,A) =
?E p(C?E,Q,A) * p(E?Q,A). Both distributions are
modeled by using the maximum entropy.
All three forms of questions are also useful when pro-
cessing complex questions, determined by a scenario re-
sulting from a problem-solving situation. As illustrated
in Figure 1, a scenario question may be associated with a
pattern. One of the pattern variables represents the focus
of the question. The notion of the question focus was first
introduced by (Lehnert, 1978). The focus represents the
most important concept of the question; a concept deter-
mining the domain of question. In the case of question
Q1, the focus is missile program. The identification of
the focus is based on the predicate-structure of the ques-
tion pattern and on the order of the arguments. Figure 3
shows both the question pattern associated with Q1 and
its predicate-argument structure. The argument with the
role of purpose is ranked highest, and thus it determines
the question focus.
With the exception of the focus, all arguments from
the predicate-argument structure may be used for gener-
ating definition questions. The focus is elaborated upon.
Several forms of elaborations are possible. One is a tem-
poral one, as illustrated in Figure 1. Other are resultative,
causative or manner-based. For example, the knowledge
that assistance in a missile program results in an inven-
When did North Korea receive from USSR/Russiaassistance
WRB VBD NNP NNP NNPVB NN
NPB
IN
PP
NPB
VP
NPB
SQ
WHADVP
SBARQ
OBJECTBENEFICIARY SOURCEDATE
when=DATE
Step 2(a): Binary Semantic Dependencies
receive North Korea USSR/Russia assistance
TypeExpected Answer
Step 2(b): Predicate?Argument Structures
Predicate: receive
Arguments: assistance=OBJECT
North Korea=BENEFICIARY
USSR/Russia=SOURCE
When=DATE=Expected Answer Type
Question: When did North Korea receive assistance from USSR/Russia?
Step 1: Syntactic Parse
Figure 2: Deriving the Expected Answer Type
Predicate?argument structure:
Predicate:
Arguments:
receive
Purpose: Z
Beneficiary: X Source: Y
Object: assistance
Question Pattern: What kind of assistance has X received
from Y for Z?
Figure 3: Predicate-argument structure
tory of missiles allows for resultative elaboration. Further
knowledge needs to be coerced for generating the implied
questions as possible follow-ups to intended questions.
The relationship between intended questions and im-
plied questions is marked by the presence of multiple
references, e.g. the pronouns it and this or any and
ones. The generation of implied questions is made pos-
sible by knowledge that is coerced from the intended
questions. For example, when asking Qi1 :?What is
the USSR/Russia?? the coercion process abstracts away
from the concept that needs to be defined, i.e. a coun-
try. The implied question requests confirmation of
the metonymy resolution involving USSR/Russia.This
named entity may represent a country but most likely it
refers to its government or, as Qm12 suggests, organiza-
tions or individuals acting on behalf of the country. Both
Qm11 and Qm12 , implied questions derived from the in-
tended question Qi1, refer to the metonymy by using the
pronouns this and it respectively. Different forms of coer-
cion are used for Qi3 because in this case the knowledge
is associated with the predicate. The implied questions
associated with the focus, i.e. the intended question Qi4,
coerce the design and development predicates which are
associated with the missiles as well as the timelines of
possible additional assistance.
3 Models of Question Answering
The processing of questions is typically performed as a
sequence of three processes: (1) Question Processing; (2)
Document Processing and (3) Answer Extraction. In the
case of factoid questions , question processing involves
the classification of questions with the purpose of pre-
dicting what semantic class the answer should belong
to. Thus we may have questions asking about PEOPLE,
ORGANIZATIONS, TIME or LOCATIONS. Since open-
domain Q/A systems process questions regardless of the
domain of interest, question processing must be based on
an extended ontology of answer types. The identification
of the expected answer type is based either on binary se-
mantic dependencies extracted from the syntactic parse of
the question (Harabagiu et al, 2001) or on the predicate-
argument structure of the question. In both cases, the re-
lation to the question stem (i.e. what, who, when) enables
the classification. Figure 2 illustrates a factoid question
generated as an intended question and the derivation of
its expected answer type.
However, many times the expected answer type needs
to be identified from an ontology that has high lexico-
semantic coverage. Many Q/A systems use the WordNet
database for this purpose. In contrast, definition ques-
tions do not require the identification of the expected an-
Answer Pattern:
Answer:
Question Pattern:
Question?Point, a Definition
has killed nearly 800 people since
taking up arms in 1968
for Basque Homeland and Freedom ?
ETA, a Basque language acronym
What is Question?Point in Country?
What is in SpainETA
NNP
NPB
IN
PP
NNP
NPB
NP
VBZ
SQ
WP
WHNP
SBARQ
Definition Question: What is ETA in Spain?
Question Parse:
Figure 4: Patterns for Processing Definition Questions
swer type, since they always request a definition. How-
ever, definition questions are matched against a set of pat-
terns, which enables the extraction of the definition from
the candidate answers. Figure 4 illustrates a definition
question, the pattern it matched as well as the extracted
answer.
Both factoid and definition questions can be answered
only if candidate passages are available. The retrieval of
these passages is made possible by keywords that are se-
lected from the question words. The Documents Process-
ing module implements a search engine that returns pas-
sages that are likely to contain the expected answer type
in the case of factoid questions or the definition pattern
in the case of definition questions. The answer extraction
module optimizes the extraction of the correct answer by
unifying the question information with the answer infor-
mation. The unification may be based on pattern match-
ing; on machine learning algorithms based on the ques-
tion and answer features or on abductive reasoning that
justifies the answer correctness.
Current state-of-the-art QA systems search for the can-
didate answer by assuming that the answers are single
concepts, that can be recognized from a hierarchy or by
a Named Entity Recognizer. This is a serious limitation,
but it works well for the factoid, list or definition ques-
tions evaluated in TREC.
The three modules of current Q/A systems reflect the
three functions that need to be considered by any Q/A
model: (1) understanding what the question asks; (2)
identify candidate text passage that might contain the an-
swer; and (3) the extraction of the correct answer. Cur-
rently, the expected answer type represents what ques-
tion asks about: a semantic concept, e.g. the name of a
person, location or organization, kinds of diseases, types
of animals or plants. Generally these semantic concepts
are lexicalized in a single word or in 2-word collocations.
Clearly, this represents a limitation, since often the ques-
tions ask for more than a single concept. As we have
seen in Table1, there is additional intended and implied
information that is requested. Therefore new models of
Question/Answering need to incorporate these additional
forms of knowledge.
When definition questions are processed in current
Q/A systems, they are matched against a pattern, which is
different from the question patterns associated with com-
plex questions similar to those illustrated in Figure 1. In
the case of a definition question like ?What is ETA in
Spain??, the pattern identifies the question-point (QP) as
ETA- the concept that needs to be defined and Spain as
its context. The definition question pattern also contains
several surface-form patterns that are matched in the can-
didate paragraphs. One such pattern is recognized in an
apposition, by [QP, a AP] where AP represents the an-
swer phrase. In the following passage:
?ETA, a Basque language acronym for Basque Homeland
and Freedom - has killed nearly 800 people since taking
up arms in 1968.?
the exact answer representing the definition is identified
in AP: Basque language acronym for Basque Homeland
and Freedom. The fact that Basque country is a region in
Spain allows a justification of the question context.
In this paper, by considering the intentional informa-
tion and the implied information that can be derived when
processing questions, we introduce a novel model of Q/A,
which has access to rich semantic structures and enables
the retrieval of more accurate answers as well as inference
processes that explain the validity and contextual cover-
age of answers.
Figure 5 shows the structure of the novel model of Q/A
we propose. Both Question Processing and Document
Processing have the recognition of predicate-argument
structures as a crux of their models. As reported in (Sur-
deanu et al, 2003), the recognition of predicate-argument
structures depends on features made available by full syn-
tactic parses and by Named Entity Recognizers. As we
shall show in this paper, the predicate-argument struc-
tures enable the recognition of question pattern, the ques-
tion focus and the intentional structure associated with
Question
Syntactic Parse Named EntityRecognition
Identification of
Predicate?Argument Structure
Structure
Recognition of Answer
based on extended
Indexing & Retrieval
lexico?semantic knowledge
Named EntityRecognitionSyntactic Parse
Intentional Structure
Identification ofPredicate?ArgumentStructures Question Pattern
Recognition of
Identification ofQuestion Focus
Recognition of Answer Structure
Keyword Extraction
Validation of Implied Information
Answer Structure
Recognition of
Recognition and extention
of intentional structure
Reference Resolution
Question Processing Answer ProcessingDocument Processing
Answer
Figure 5: Novel Question/Answering Architecture.
a question. When the intentions are known, the answer
structure can be identified and the keywords extracted.
For better retrieval of candidate answers, documents are
indexed and retrieved based on the predicate-argument
structures as well as on complex semantic structure asso-
ciated with different question patterns. Similarly, the in-
tentional structures are used for indexing/retrieving can-
didate passages. The Answer Processing function in-
volves the recognition of the answer structure and inten-
tional structure. Often this requires reference resolution.
The implied information coerced from both the question
and the candidate answer is also validated before decid-
ing on the answer correctness.
4 Predicate-Argument Structures
To identify predicate-argument structures in questions
and passages, we have: (1) used the Proposition Bank or
PropBank as training data; and (2) a mode for predicting
argument roles similar to the one employed by (Gildea
and Jurafsky, 2002).
PropBank is a one million word corpus annotated with
predicate-argument structures on top of the Penn Tree-
bank 2 Wall Street Journal texts. For any given predicate,
the expected arguments are labeled sequentially from Arg
0 to Arg 4. Generally, Arg 0 stands for agent, Arg 1 for
direct object or theme or patient, Arg 2 for indirect object
or benefactive or instrument or attribute or end state, Arg
3 for start point or benefactive or attribute and Arg4 for
end point. In addition to these core arguments, adjunc-
tative arguments are marked up. They include functional
tags from Treebank, e.g. ArgM-DIR indicates a direc-
tional, ArgM-LOC indicates a locative, and ArgM-TMP
stands for a temporal.
An example of PropBank markup is:
[Arg10 Analysts ] have been [predicate1 expecting ] [Arg11
a GM-Jaguar pact ] that would [predicate2 give ] [Arg22 the
U.S. car maker ] [Arg21 an eventual 30% state in the British
Company ].
The model of identifying the arguments of each pred-
icate consists of two tasks: (1) the recognition of the
boundaries of each argument in the syntactic parse tree;
(2) the identification of the argument role. Each task can
be cast as a separate classifier. Next section describes
our approach based on Support Vector Machines (SVM)
(Vapnik, 1995).
4.1 Automatic Predicate-Argument extraction
Given a sentence in natural language, all the predicates
associated with its verbs have to be identified along with
their arguments. This problem can be divided in two sub-
tasks: (a) detection of the target argument boundaries,
i.e. all its compounding words, and (b) classification of
the argument type, e.g. Arg0 or ArgM.
A direct approach to learn both detection and classifi-
cation of predicate arguments is summarized by the fol-
lowing steps:
1. Given a sentence from the training-set, generate a
full syntactic parse-tree;
2. let P and A be the set of predicates and the set of
parse-tree nodes (i.e. the potential arguments), re-
spectively;
3. for each pair <p, a> ? P ?A:
? extract the feature representation set, Fp,a;
? if the subtree rooted in a covers exactly the
words of one argument of p, put Fp,a in T+
(positive examples), otherwise put it in T?
(negative examples).
0.68
0.71
0.74
0.77
0.8
0.83
1 2 3 4 5
Polynomial Degree
(a)
F1
Arg0
Arg1
ArgM
0.65
0.68
0.71
0.74
0.77
0.8
1 2 3 4 5
Polynomial Degree
(b)
F1
Figure 6: Single classifiers and Multi-classifier performance for argument extraction.
The above T+ and T? sets can be re-organized as pos-
itive T+argi and negative T?argi examples for each argu-
ment i. In this way, an individual ONE-vs-ALL SVM
classifier for each argument i can be trained. We adopted
this solution as it is simple and effective (Pradhan et al,
2003). In the classification phase, given a sentence of the
test-set, all its Fp,a are generated and classified by each
individual SVM classifier. As a final decision, we select
the argument associated with the maximum value among
the scores provided by the SVMs2, i.e. argmaxi?S Ci,
where S is the target set of arguments.
The discovering of relevant features is a complex task.
Nevertheless there is a common consensus on the basic
features that should be adopted. These standard features,
first proposed in (Gildea and Jurafsky, 2002), are derived
from parse trees as illustrated by Table 2.
4.2 Parsing Sentence into Predicate Argument
Structures
For the experiments, we used PropBank
(www.cis.upenn.edu/?ace) along with Penn-
TreeBank3 2 (www.cis.upenn.edu/?treebank)
(Echihabi and Marcu, 2003). This corpus contains about
53,700 sentences and a fixed split between training and
testing which has been used in other researches (Gildea
and Jurafsky, 2002; Surdeanu et al, 2003; Hacioglu et
al., 2003; Chen and Rambow, 2003; Gildea and Hock-
enmaier, 2003; Gildea and Palmer, 2002; Pradhan et al,
2003). In this split, Sections from 02 to 21 are used for
training, section 23 for testing and sections 1 and 22 as
developing set. We considered all PropBank arguments
from Arg0 to Arg9, ArgA and ArgM even if only Arg0
from Arg4 and ArgM contain enough training/testing
2This is a basic method to pass from binary categorization
into a multi-class categorization problem; several optimization
have been proposed, e.g. (Goh et al, 2001).
3We point out that we removed from the Penn TreeBank the
special tags of noun phrases like Subj and TMP as parsers usu-
ally are not able to provide this information.
data to affect the global performance.
The classifier evaluations were carried out using
the SVM-light software (Joachims, 1999) available at
http://svmlight.joachims.org/ with the de-
fault polynomial kernel according to a degree d ?
{1, 2, 3, 4, 5}. The performances were evaluated using
the F1 measure for both single argument classifiers and
the multi-class classifier.
- PHRASE TYPE (pt): This feature indicates the syntactic
type of the phrase labeled as a predicate argument.
- PARSE TREE PATH (path): This feature contains the path
in the parse tree between the predicate phrase and the argu-
ment phrase, expressed as a sequence of nonterminal labels
linked by direction (up or down).
- POSITION (pos) Indicates if the constituent appears be-
fore or after the predicate in the sentence.
- VOICE (voice) This feature distinguishes between active
or passive voice for the predicate phrase.
- HEAD WORD (hw) This feature contains the head word
of the evaluated phrase. Case and morphological informa-
tion are preserved.
- GOVERNING CATEGORY (gov) This feature applies to
noun phrases only, and it indicates if the NP is dominated by
a sentence phrase (typical for subject arguments with active
voice predicates), or by a verb phrase (typical for object
arguments).
- PREDICATE WORD In our implementation this feature
consists of two components: (1) VERB: the word itself
with the case and morphological information preserved; and
(2) LEMMA which represents the verb normalized to lower
case and infinitive form.
Table 2: Standard Features used in Predicate Argument
Extraction.
Figure 6 illustrates the F1 measures for the overall ar-
gument extraction task (i.e. identification and classifica-
tion) according to different polynomial degrees. Figure
6(a) illustrates the F1-performance of single classifiers
for the arguments Arg0, Arg1 and ArgM. Figure 6(b) il-
lustrates the performance for all the arguments (i.e. the
multi-classifier). In general, we were able to recognize
predicate argument structures with an F1-score of 80%.
4.3 Using Predicate-Argument Structures in
Question Answering.
Predicate-argument structures are useful for identifying
candidate answers. Since they recognize long-distance
dependencies between a predicate and one its arguments,
they enable (1) the identification of the exact boundaries
of an answer; and (2) they unify the predicate-argument
relation sought by question with those recognized in can-
didate passages.
Moreover, they are very useful in situations when the
expected answer type of the question could not be recog-
nized. There are two causes when the expected answer
type cannot be identified:
Case1: the answer class is a name that cannot be correctly
classified by an available Named Entity Recognizer, be-
cause its class name is not encoded.
Case2: the answer class cannot be found in the Answer
Type hierarchy. The example from Figure 7 shows an in-
stance of case 1. In this figure, the TREC question Q2054
has a predicate that can be unified with PREDICATES
from the answer passage. The Arg1 of the predicate is
the expected answer, which is identified as ?the Declara-
tion of Independence?. The Arg0 in the question is But-
ton Gwinnett, whereas in the answer, it is underspecified,
and should be resolved to who. This relative pronoun has
Button Gwinnett as one of its antecedents.
In Figure 8 the second case is illustrated. The question
asked about the first argument of the predicate ?measure?,
when its Arg2 = ?a theodolite?. In the answer, Predicate
2, with its infinite form, has as Arg 2 the same ?theodo-
lite?. However, the predicates are lexicalized by different
verbs. In WordNet, the first sense of the verb ?measure?
as the verb ?determine? as a hypernym, therefore Arg1 =
?wind speeds? is the correct answer.
5 Intentional Structures
The correct interpretation of many questions requires
the inference of implicit information, that is not directly
stated in the question, but merely implied. The mecha-
nisms of recognizing the intentions of the questioner are
helpful means of identifying the implied information. For
example, in the question QI :?Will Prime Minister Mori
survive the crisis??, the user does not literally mean ?Will
Prime Minister Mori be still alive when the political cri-
sis is over ??, but rather (s)he implies her/his belief that
the current political crisis might cost the Japanese Prime
Minister his job. It is very unlikely that any expert knowl-
edge base covering Japanese politics will encode knowl-
edge covering all situations of political crisis and the pos-
sible outcomes of the prime minister. However, this prag-
matic knowledge is essential for the correct interpretation
of the question.
Q2054:
Answer: Button Gwinnett, George Walton and Lyman Hall were
Georgians who could have been hanged as traitors for
What document did Button Gwinnett sign on the upper left
hand side?
signing the Declaration of Independence on July 4, 1776.
Predicate?argument structure:
PREDICATE1: were
ARG1(PREDICATE1): Georgians
who
PREDICATE2: could have been hanged
ARG2(PREDICATE2): as traitors
PREDICATE3: signing
ARG1(PREDICATE3): The Declaration of Independence
ARGM?LOC(PREDICATE3): on July 4, 1776
ARG0: Button Gwinnett, George Walton and Lyman Hall
ARGM?LOC: on the upper left hand side
ARG0: Button Gwinnett
PREDICATE: sign
ARG1: What document Question Type
Predicate?argument structure:
Figure 7: Answer extraction from predicate-argument struc-
tures: Case1
measurePREDICATE:
What does a theodolite measure?
Predicate?argument structure:
ARG1: What
ARG2: a theodolite
Answer: The theodolite ? a 1940s gadget, no longer in production,
wind speeds.
that uses a helium balloon and trigonometry to determine
Predicate?argument structure:
a 1940s gadget
that
PREDICATE1: uses
PREDICATE2: to determine
ARG1(PREDICATE2): wind speeds
Q2145:
ARG1(PREDICATE1): The theodolite
ARG2(PREDICATE1): a helium balloon and trigonometry
Figure 8: Answer extraction from predicate-argument struc-
tures: Case 2
The design of advanced Question&Answering systems
capable of grasping the intention of a professional analyst
when (s)he poses a question depends both on the knowl-
edge of the domain referred by the question as well as on
a variety of rules and conventions that allow the commu-
nication of intentions and beliefs in addition to the literary
meaning of the question. Access to domain knowledge is
granted by a combination of retrieval mechanisms that
bring forward relevant document passages from unstruc-
tured collections of documents, specialized knowledge
Text Information RetrievalEngine
QUERY: Prime & Minister & Mori &DANGER (word)
Japanese Factual PoliticsKnowledge Base
Text Retrieval
"vote of non-confidence against 
Prime Minister Mori"
political crisis
survive crisis
adversity DANGER
WordNet 1.6
resignation
removal
strike
vote
vote of non-confidence =
   DANGER(Position)
continue in existence
Question: Will Prime Minister Mori survive the political crisis ?
DANGER ( Prime Minister Mori continues in Position)
Figure 9: Intentional Structure derived from Lexico-Semantic Knowledge.
bases and/or database access mechanisms. The research
proposed in this project focuses on the derivation and us-
age of pragmatic knowledge that supports the recognition
of question implications, also known as implicatures (cf.
(Grice, 1975b)).
5.1 Intentional structures Derived from
Lexico-Semantic Knowledge
The novel idea of this research is to link computa-
tional implicatures, similar to those defined by Grice
(Grice, 1975b), to inferences that can be drawn from
general lexico-semantic knowledge bases such as Word-
Net of FrameNet. Incipient work was described in
(Sanda Harabagiu and Yukawa, 1996), where a method
of using lexico-semantic path for recognizing textual im-
plicatures was presented. To our knowledge, this is the
only computational model of implicatures that was de-
veloped and tested on a large lexico-semantic knowledge
base (e.g. WordNet), enabling successful recognition of
implicatures.
The model proposed in (Sanda Harabagiu and Yukawa,
1996) uncovered a relationship between (a) the coherence
of a text segment; (b) its cohesion expressed by the lexical
paths and (c) the implicatures that can be drawn, mostly
to account for pragmatic knowledge. This relationship
can be extended across documents and across topics, to
learn patterns of textual and Q&A implicatures and the
methods of deriving knowledge that enables their recog-
nition.
The derivation of pragmatic knowledge combines in-
formation from three different sources:
(1) lexical knowledge bases (e.g. WordNet),
(2) expert knowledge bases that can be rapidly formatted
for many domains (e.g. Japanese political knowledge);
and
(3) knowledge supported from the textual information
available from documents. The methodology of combin-
ing these three sources of information is novel.
For question QI , the starting point is the concept iden-
tified as a cue for the expected answer type through meth-
ods described in (Harabagiu et al, 2000). This con-
cept is lexicalized by the verb-object pair survive-crisis.
Verb survive has four distinct senses in the WordNet 1.6
database, whereas noun crisis has two senses. The poly-
semy of the expected answer type increases the difficulty
of the derivation of pragmatic knowledge, but it does not
presupposes the word sense disambiguation of the ex-
pression. The information available in the glosses defin-
ing the WordNet synsets provides helpful information for
expanding the multi-word term defining the expected an-
swer type. By measuring the similarity between the two
senses of the noun crisis and the words encountered as
objects or prepositional attachments in the glosses of the
various senses of the verb survive, we distinguish the
noun adversity and the example cancer as expressing the
closest semantic orientation to the first sense of noun cri-
sis. The similarity is measured by counting the number
of common hypernyms and gloss concepts of hypernyms
of two synsets. Figure 9 illustrates the concepts related
to the question QI , as derived from WordNet lexico-
semantic knowledge base.
The fact that surviving a political crisis has a dangerous
component, indicated by the noun adversity, may also be
supported by inferences drawn from an expert knowledge
base, showing that a political crisis may be dangerous for
political figures in power. However, at this point, the ob-
ject of the dangerous situation is not specified. But sev-
eral concepts indicating dangerous political situations can
be inferred from the expert knowledge base and used in
the query for text evidence. Only when text passages in-
volving Prime Minister Mori are retrieved, clarifications
of the situation are brought to attention: a vote of non-
confidence against the prime minister is considered. This
new information helps inference from the expert knowl-
edge base. The expert knowledge base modeling the
 Intentional Structure of Questions
0* Evidence (     1-possess      (          2-Iraq,      3-biological weapons   ))
4* Means of finding (0)
a. reports 
b. inspections
c. assessments ? patterns of inspections
5* Source (0)
a. authority
b. reliability ? may, would
6* Consequence (0)
a. Enablement
b. Hiding/Presenting finding evidence 
Question: Does Iraq   have biological weapons  ?
x                           y
: have( Iraq, biological weapons )
Predicate-
Argument
Structure
Question
pattern
Does x have y ?
possess (x, y) 
Topic (3)
biological  weapons
a. Types of topic
b. Components
- chemical agents
- mustard gas, VX, sarin
c. Usage
- rockets, artillery shells
a. discover(1,2,3)
b. stockpile(2,3)
c. use(2,3)
d. 1-possess
a. develop(2,3)
b. acquire(2,3)
a. inspections( _,2,3)
b. ban( _,2,3)
Source/      fact/          reliability
reporter     evidence
5.a              0              5.b
coercion
Structure
Figure 10: Intentional structure derived from predicate-argument structures.
Japanese factional politics confirms that this is a danger-
ous situation for the Prime Minister and that in fact his
position is in jeopardy. Due to this inference from the
expert knowledge base, the concept POSITION replaces
noun existence from the gloss of the second sense of verb
survive, and the pragmatic knowledge required for the in-
terpretation of the implicature is assembled:
The interactions between the three information sources
derives the pragmatic knowledge on which relies the im-
plication of the question. The user had an inherent belief
that Prime Minister Mori might be replaced, and (s)he
queries the Q&A system not only to find information but
also to find support for his/her belief. The intentional
structure is represented as a set of concepts and the re-
lations that span them, as illustrated in Figure 9.
5.2 Coercion of Intentions
A second method of deriving the intentional structure of a
question is based on the predicate-argument structure that
is derived from the question and the candidate answers.
Figure 10 illustrates the Intentional Structure of one
such question. The structure of the intentions is deter-
mined by the predicate-argument structure of the ques-
tion and by its pattern. Generally, when asking whether
X posses Y, we want to find (1) evidence of this fact;
(2) we explore different means of finding the informa-
tion; (3) we are interested in the source of information
and (4) the enablers or inhibitors of finding the informa-
tion as well as the consequences of knowing it are of in-
terest. We assign a different index to each object from
the predicate-argument structure, and do the same for
each element of the intentional structure. For instance,
in Figure 2, source(0) is interpreted as source(index=0)
= source(evidence). Another feature of the intentional
structure is determined by the coercions that are associ-
ated with both forms of indexed objects. For example,
the coercion of evidence shows the most typical ways
of finding evidence in the context of the topic of the
question. Figure 2 lists such possibilities as (a) discov-
ering, (b) stockpiling, (c) using and even (d) possess-
ing. These possibilities are inserted in the context of the
topic, since they make use of the indexes for associat-
ing meaning to their representations. In fact, option (a)
discover(1,2,3) reads as discover(index=1, index=2, in-
dex=3) =discover(possesses(Iraq, biological weapons)).
Whereas option (b) stockpile(2,3) can be similarly inter-
preted as stockpile(Iraq, biological weapons). Note that
one of the indexed objects is the topic. The structure of
the topic is define along three semantic dimensions: (1)
hyponyms or examples of other types of the same cate-
gory as the topic; (2) the meronyms or components; and
(3) the functionality or the usage. The derivation of such
a large set of intentional structures helped us learn how
to coerce pragmatic knowledge. We have developed a
probabilistic approach extending the metonymy work of
(Lapata and Lascarides, 2003).
Lapata and Lascarides report a model of interpretation
of verbal metonymy as the point distribution P (e, o, v) of
three variables: the metonymy verb v, its object, and the
sought after interpretation i. For example a verb ? ob-
ject relation that needs to be metonymycally interpreted,
is enjoy ? movie. In this case v = enjoy, o = movie
and i ? {making, watching, directing}. The variables
of the distribution re ordered as <i, v, o> to help factor-
ing P (i, v, o) = P (i) ? P (v|i) ? P (o|i, v). Each of the
probabilities P (i), P (v|i) and P (o|i, v) can be estimated
using maximum likelihood. As it is illustrated in Fig-
ure 10, we have extended this model to account for: (1)
coercion of topic information; (2) coercion of evidence
of a fact; (3) interpretation of predicate and (4) inter-
pretation of arguments. Since the verb ? object rela-
tion translates in one of the predicate-argument relations,
we have coerced the predicate interpretations in the same
way as (Lapata and Lascarides, 2003), but we allowed
for any predicate-argument relation. Argument coercions
were produced by searching the most likely predicates
that used the same arguments. The topic model also in-
corporated topic signatures, similar to these reported in
(E.H. Hovy and Ravichandran, 2002).
6 Conclusions
In this paper we have described the problem of interpret-
ing the question intentions and proposed two methods of
generating the intentional structure of questions. The first
method is based on lexico-semantic chains between con-
cepts that are related to the question. The second method
generates intentional structures by using the predicate-
argument structures of questions and the topic represen-
tation of questions. To derive both forms of intentional
structures, we have relied on information available from
WordNet and on the parsing of questions and answers
in predicate-argument structures. Our experiments show
that the intentional structure may determine a different in-
terpretation of the question, and thus different keywords
can be used to retrieve the answers. Answer extraction
also depends on the semantic relations between the co-
erced interpretations of predicates and arguments. By
selecting a set of 100 questions for test, we have eval-
uated the correctness of the extracted answers when (1)
no intentional knowledge was coerced; (2) implicatures
were derived from lexico-semantic knowledge and (3)
intentional structures were derived based on predicate-
argument structures. An increase of 8structures and one
of 22the impact of each element of the intentional struc-
ture on the Q/A processing.
References
John Chen and Owen Rambow. 2003. Use of deep lin-
guistic features for the recognition and labeling of se-
mantic arguments. In Proceedings of the 2003 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In Pro-
ceedings of the 41st Annual Meeting of the ACL, Sap-
poro, Japan.
Chin-Yew Lin E.H. Hovy, U. Hermjakob and Deepak
Ravichandran. 2002. Using knowledge to facilitate
pinpointing of factoid answers. In Proceedings of the
19th International Conference on Computational Lin-
guistics (COLING 2002).
Daniel Gildea and Julia Hockenmaier. 2003. Identifying
semantic roles using combinatory categorial grammar.
In Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistic,
28(3):254?288.
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition. In
Proceedings of the 40th Annual Conference of the
Association for Computational Linguistics (ACL-02),
Philadelphia, PA.
King-Shy Goh, Edward Chang, and Kwang-Ting Cheng.
2001. SVM binary classifier ensembles for image clas-
sification. In Proceedings of the tenth international
conference on Information and knowledge manage-
ment, pages 395?402.
Paul H. Grice. 1975a. Logic and conversation. In P. Cole
and New York J.L. Morgan (ed.), Academic Press, edi-
tors, Syntax and Sematics Vol.3:Speech Acts, pages 41?
58.
Paul J. Grice. 1975b. Syntax and Semantics Vol.3:Speech
Acts. P. Cole and J. Morgan, editors.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, Jim Mar-
tin, and Dan Jurafsky. 2003. Shallow semantic parsing
using support vector machines. Technical report.
Sanda Harabagiu, Marius Pas?ca, and Steven Maiorano.
2000. Experiments with open-domain textual question
answering. In Proceedings of the 18th International
Conference on Computational Linguistics (COLING-
2000), pages 292?298, Saarbrucken, Germany,.
Sanda M. Harabagiu, Dan I. Moldovan, Marius Pasca,
Rada Mihalcea, Mihai Surdeanu, Razvan C. Bunescu,
Roxana Girju, Vasile Rus, and Paul Morarescu. 2001.
The role of lexico-semantic feedback in open-domain
textual question-answering. In Meeting of the ACL,
pages 274?281.
S. Harabagiu, D. Moldovan, C. Clark, M. Bodwen,
J. Williams, and J. Bensley. 2003. Answer mining by
combining extraction techniques with abductive rea-
soning. In Notebook of the Twelveth Text REtrieval
Converence (TREC-2003), pages 46?53.
A. Ittycheriah, M. Franz, W. Zhu, and A. Ratnaparkhi.
2000. IBM?s statistical question answering system.
In Proceedings of the 9th Text REtrieval Conference,
Gaithersburg, MD.
T. Joachims. 1999. T. Joachims, Making large-Scale
SVM Learning Practical. In B. Scho?lkopf and C.
Burges and A. Smola (ed.), MIT-Press., editor, Ad-
vances in Kernel Methods - Support Vector Learning.
Maria Lapata and Alex Lascarides. 2003. A probabilistic
account of logical metonymy. Computational Linguis-
tics, 29:2:263?317.
Wendy Lehnert. 1978. The process of question answer-
ing. In Lawrence Erlbaum Assoc., Hillsdale.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H.
Martin, and Daniel Jurafsky. 2003. Semantic role
parsing: Adding semantic structure to unstructured
text. In Proceedings of the International Conference
on Data Mining (ICDM-2003).
Dan Moldovan Sanda Harabagiu and Takashi Yukawa.
1996. Testing gricean constraints on a wordnet-based
coherence evaluation system. In Working Notes of the
AAAI-96 Spring Symposium on Computational Impli-
cature, Stanford, CA.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of the
41th Annual Conference of the Association for Compu-
tational Linguistics (ACL-03), pages 8?15.
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer.
