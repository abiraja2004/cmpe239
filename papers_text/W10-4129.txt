An Double Hidden HMM and an CRF for Segmentation Tasks with
Pinyin?s Finals
Huixing Jiang Zhe Dong
Center for Intelligence Science and Technology
Beijing University of Posts and Telecommunications
Beijing, China
jhx0129@163.com jimmybupt@gmail.com
Abstract
We have participated in the open tracks
and closed tracks on four corpora of Chi-
nese word segmentation tasks in CIPS-
SIGHAN-2010 Bake-offs. In our experi-
ments, we used the Chinese inner phonol-
ogy information in all tracks. For open
tracks, we proposed a double hidden lay-
ers? HMM (DHHMM) in which Chinese
inner phonology information was used as
one hidden layer and the BIO tags as an-
other hidden layer. N-best results were
firstly generated by using DHHMM, then
the best one was selected by using a new
lexical statistic measure. For close tracks,
we used CRF model in which the Chinese
inner phonology information was used as
features.
1 Introduction
Chinese language has many characteristics not
possessed by other languages. One obvious is
that the written Chinese text does not have explicit
word boundaries like western languages. So word
segmentation became very significative for Chi-
nese information processing, and is usually con-
sidered as the first step of any further processing.
Identifying words has been a basic task for many
researchers who have devoted themselves on Chi-
nese text processing.
The biggest characteristic of Chinese language
is its trinity of sound, form and meaning (Pan,
2002). Hanyu Pinyin is the form of sound for
Chinese text and the Chinese phonology informa-
tion is explicit expressed by Pinyin which is the
inner features of Chinese Characters. And it nat-
urally contributes to the identification of Out-Of-
Vacabulary words (OOV).
In our work, Chinese phonology information is
used as basic features of Chinese characters in all
models. For open tracks, we propose a new dou-
ble hidden layers HMM in which a new phonol-
ogy information is built in as a hidden layer, a
new lexical association is proposed to deal with
the OOV questions and domains? adaptation ques-
tions. And for closed tracks, CRF model has been
used , combined with Chinese inner phonology in-
formation. We used the CRF++ package Version
0.43 by Taku Kudo1.
In the rest sections of this paper, we firstly in-
troduce the Chinese phonology in Section 2. Then
in the Section 3, the models used in our tasks are
presented. And the experiments and results are
described in Section 4. Finally, we give the con-
clusions and make prospect on future work.
2 Chinese Phonology
Hanyu Pinyin is the form of sound for Chi-
nese text and the Chinese phonology informa-
tion is explicit expressed by Pinyin. It is cur-
rently the most commonly used romanization sys-
tem for Standard Mandarin. Hanyu means the
Chinese language, and Pinyin means ?phonetics?,
or more literally, ?spelling sound? or ?spelled
sound? (wikipedia, 2010). The system has been
employed to teach Mandarin as home language
or as second language by China, Malaysia, Sin-
gapore et.al. Pinyin has been the most Chinese
character?s input method for computers and other
devices.
1http://crfpp.sourceforge.net/
The romanization system was developed by a
government committee in the People?s Repub-
lic of China, and approved by the Chinese gov-
ernment on February 11, 1958. The Interna-
tional Organization for Standardization adopted
pinyin as the international standard in 1982, and
since then it has been adopted by many other
organizations(wikipedia, 2010). In this system,
pinyin is composed by initials(pinyin: shengmu),
finals(pinyin: yunmu) and tones(pinyin: sheng-
diao) instead of consonants and vowels used in
European language. For example, the Pinyin of
??? is ?zhong1? composed by ?zh?, ?ong? and
?1?. In which ?zh? is initial, ?ong? is final and
?1? is the tone.
Every language has its rhythm and rhyme, so
Chinese is no exception. The rhythm system are
the driving force from the unconscious habit of
language(Edward, 1921). And the Pinyin?s finals
contribute the Chinese rhythm system, Which is
the basic assumption our research based on.
3 Algorithms
Generally the task of segmentation can be viewed
as a sequence labeling problem. We first define a
tag set as TS = {B, I, E, S}, shown in Table 1.
Table 1: The tag set used in this paper.
Label Explanation
B beginning character of a word
I inner character of a word
E end character of a word
S a single character as a word
For the piece ??????????? of
the example described in the experiments section,
firstly, the TS tags are labeled to it. And its re-
sult is ??/S?/B?/E?/S?/B?/E?/B?/I
?/E?. Then the tags are combined sequentially to
get the finally result ?? ?? ? ?? ????.
In this section, A novel HMM solution is pre-
sented firstly for open tracks. Then the CRF solu-
tion for closed tracks is introduced.
3.1 Double hidden layers? HMM
For a given piece of Chinese sentence, X =
x1x2 . . . xT , where xi, i = 1, . . . , T is a Chinese
character. Suppose that we can give each Chinese
character xi a Pinyin?s final yi. And suppose the
label sequence of X is S = s1s2 . . . sT , where
si ? TS is the tag of xi. Then what we want to
find is an optimal tag sequence S? which is de-
fined in (1).
S? = argmax
S
P (S, Y |X)
= argmax
S
P (X|S, Y )P (S, Y ) (1)
The model is described in Fig. 1. For a given
piece of Chinese character strings, One hidden
layer is label sequence S. Another hidden layer is
Pinyin?s finals sequence Y . The observation layer
is the given piece of Chinese characters X .
Figure 1: Double Hidden Markov Model
For transition probability, second-order Markov
model is used to estimate probability of the double
hidden sequences as described in (2).
P (S, Y ) =
?
t
p(st, yt|st?1, yt?1) (2)
For emission probability, we keep the first-
order Markov assumption as shown in (5).
P (X|S, Y ) =
?
t
p(xt|st, yt) (3)
3.1.1 Nbest results
Based on the work of (Jiang, 2010), a word lat-
tice is also built firstly, then in the second step, the
backward A? algorithm is used to find the top N
results instead of using the backward viterbi al-
gorithm to find the top one. The backward A?
search algorithm is described as follow (Wang,
2002; Och, 2001).
3.1.2 Reranking with a new lexical statistic
measure
Given two random Chinese charactersX and Y
and assume that they appears in an aligned region
of the corpus. The distribution of the two random
Chinese characters could be depicted by a 2 by 2
contingency table shown in Fig. 2(Chang, 2002).
Figure 2: A 2 by 2 contingency table
In Fig. 2, a is the counts ofX and Y co-occur; b
is the counts of the cases thatX occurs but Y does
not; c is the counts of the cases that X does not
occur but Y does; d is the counts of the cases that
both X and Y do not occur. The Log-likelihood
rate is calculated by (4).
LLR(x, y) = 2(a ? log a ? N
(a + b) ? (a + c)
+ b ? log b ? N
(a + b) ? (b + d)
+ c ? log c ? N
(c + d) ? (a + c)
+ d ? log d ? N
(c + d) ? (b + d)
) (4)
For the N-best result described in sec. 3.1.1,
they can be re-ranked by (5).
S? = argmin
S
(scoreh(S)+
?
K
K
?
k=1
LLR(xk, yk))
(5)
where scoreh is the negative log value of
P (S, Y |X). K is the number of breaks in X and
xk is the left Chinese character of the k break
and yk is the right Chinese character of the k
break. ? is the regulatory factor(in our experi-
ments ? = 0.45).
Bigger value of LLR(xk, yk) means stronger
ability in combining of the two characters xk and
yk, then they should not be segmented.
3.2 CRF model for closed tracks
Conditional random field, as statistical sequence
labeling model, has been used widely in segmen-
tation(Lafferty, 2001; Zhao, 2006). In the closed
tracks of the paper, we also use it.
3.2.1 Feature templates
We adopted two main kinds of features: n-gram
features and Pinyin?s finals features. The n-gram
feature set is quite orthodox, they are, namely, C-
2, C-1, C0, C1, C2, C-2C-1, C-1C0, C0C1, C1C2.
The Pinyin?s finals feature set is the same as n-
gram feature set. They are described in Table. 2.
Table 2: Feature templates
Templates Category
C-2, C-1, C0, C1, C2 N-gram: Unigram
C-2C-1, C-1C0, C0C1, C1C2 N-gram: Bigram
P-2, P-1, P0, P1, P2 Phonetic: Unigram
P-2P-1, P-1P0, P0P1, P1P2 Phonetic: Bigram
4 Experiments and Results
4.1 Dataset
We build a basic words dictionary for DHHMM
and a Pinyin?s finals dictionary for both DHHMM
and CRF from The Grammatical Knowledge-base
of Contemporary Chinese(Yu, 2001). For the fi-
nals dictionary, we give each Chinese character a
final extracted from its Pinyin. When it comes to
a polyphone, we just combine its all finals simply
to one. For example, ??{ong}?, ??{a&ai&i}?.
The training corpus (5,769 KB) we used is the
Labeled Corpus provided by the organizer. We
firstly add the Pinyin?s finals to each Chinese
character of it, then we train the parameters of
DHHMM and CRF model on it.
And the test corpus contains four domains: Lit-
erature (A), Computer (B), Medicine (C) and Fi-
nance(D).
The LLR function?s parameters{a, b, c, d} are
counted from the current test corpus A, B, C, or
D. It?s means that for segmenting A, the LLR pa-
rameters are counted from A, so the same for seg-
menting B, C and D.
4.2 Preprocessing
The date, time, numbers and symbols information
are easily identified by rules. We propose four
regular expressions? processes, in which the reg-
ular expressions? processes are handled one after
another in order of date, time, numbers and sym-
bols. By now, a rough segmentation can be done.
For a character stream, the date, time, numbers
and symbols are firstly identified, then the whole
stream can be divided by these units to some
pieces of character strings which will be segment
by the models described in sec. 3. For example,
a character stream ?2009??8?31?????
??????12?????? will be divided
to ?2009? ? 8? 31? ? ????????
? 12 ???? ??. Then the pieces ???, ??
?????????, ?????? will be seg-
mented sequentially by the models described in
Section 3.
4.3 Results on DHHMM
We evaluate our system by Precision Rate(6), Re-
call Rate(7), F1 measure(8) and OOV(Out-Of-
Vocabulary) Recall rate(9).
P = C(correct words in segmented result)
C(words in segmented result)
(6)
R = C(correct words in segmented result)
C(words in standard result)
(7)
F1 = 2 ? P ? R
P + R
(8)
OR = C(correct OOV in segmented result)
C(OOV in standard result)
(9)
In (6-9), C(? ? ?) is the count of (? ? ?).
Table 3 are the results of the DHHMM on open
tracks.
In Table 3, OOV RR is the recall rate of OOV,
IV RR is the recall rate of IV(In Vocabulary).
4.4 Postprocessing for CRF and Results on It
Since the CRF segmenter will not always return
a valid tag sequence that can be translated into
segmentation result, some corrections should be
made if such error occurs. We devised a dynamic
programming routine to tackle this problem: first
we compute the valid tag sequence that closest to
Table 3: Results of open tracks using DHHMM:
Literature (A), Computer (B), Medicine (C) and
Finance(D)
A B C D
R 0.893 0.918 0.917 0.928
P 0.918 0.896 0.907 0.934
F1 0.905 0.907 0.912 0.931
OOV RR 0.803 0.771 0.704 0.808
IV RR 0.899 0.945 0.943 0.939
the output of CRF segmenter (by term closest, we
mean least hamming distance), if there is a tie, we
choose the one has the least ?S? tags, if the tie still
exists, we choose the one that comes lexicograph-
ically earlier (B < I < E < S, described in
Table. 1). Table 4 are the results of the CRF on
closed tracks.
Table 4: Results of closed tracks using CRF: Lit-
erature (A), Computer (B), Medicine (C) and Fi-
nance(D)
A B C D
R 0.945 0.946 0.94 0.956
P 0.946 0.914 0.928 0.952
F1 0.946 0.93 0.934 0.954
OOV RR 0.816 0.808 0.761 0.849
IV RR 0.954 0.971 0.962 0.966
From the results of Table 3 and Table 4, we
can observe that the CRF model outperforms the
DHHMM by average 2.72% in F1 measure. In the
other hand, from Table 5, we can see that the com-
putation cost in DHHMM is less than half of the
time cost and lower one-fifth memory cost than
CRF model.
Table 5: The computation cost in DHHMM and
CRF
Time cost(ms) Memory cost(MB)
DHHMM 34398 16.3
CRF 43415 35
5 Conclusions and Future works
This paper has presented a double hidden lawyers
HMM for Chinese word segmentation task in
SIGHAN bakeoff 2010. It firstly created N top
results and then select the best one from it by a
new lexical association.
Chinese phonology (specially by Pinyin?s final
in text) is very useful inner information of Chinese
language, which is the first time used in our mod-
els. We have used it in both DHHMM and CRF
model.
In future work, there are lots of improvements
can be done. Firstly, which polyphone?s finals
should be used in a given context is a visible ques-
tion. And the strategy to train the parameter ? de-
scribed in 3.1.2 can also be improved.
Acknowledgments
This research has been partially supported by
the National Science Foundation of China (NO.
NSFC90920006). We also thank Caixia Yuan for
leading our discuss, Li Sun, Peng Zhang, Yaojing
Chen, Zhixu Lin, Gan Lin, Guannan Fang for their
useful helps in this work.
References
Wenguo Pan. 2002. zibenwei yu hanyu yanjiu:120?
141. East China Normal University Press.
Sapir Edward 1921. Language: An introduction to the
study of speech:230. New York: Harcourt, Brace
and company.
wikipedia. 2010. Pinyin. http://en.wikipedia.org/wiki
/Pinyin#cite note-6.
Baobao Chang, Pernilla Danielsson, and Wolfgang
Teubert. 2002. Extraction of translation unit from
chinese-english parallel corpora, Proceedings of
the first SIGHAN workshop on Chinese language
processing:1?5.
Huixing Jiang, Xiaojie Wang, Jilei Tian. 2010.
Second-order HMM for Event Extraction from Short
Message, 15th International Conference on Ap-
plications of Natural Language to Information Sys-
tems, Cardiff, Wales, UK.
Franz Josef Och, Nicola Ueffing, Hermann Ney. 2001.
An EfficientA? Search Algorithm for Statistical Ma-
chine Translation, Proceedings of the ACL Work-
shop on Data-Driven methods in Machine Transla-
tion 14(Toulouse, France): 1-8.
Ye-Yi Wang, Alex Waibel. 2002. Decoding Algo-
rithm in Statistical Machine Translation, Proceed-
ings of the 35th Annual Meeting of the Association
for Computational Linguistics and Eighth Confer-
ence of the European Chapter of the Association for
Computational Linguistics: 366-372.
Yu Shiwen, Zhu Xuefeng, Wang Hui. 2001. New
Progress of the Grammatical Knowledge-base of
Contemporary Chinese, ZHONGWEN XINXI
XUEBAO, 2001Vol. 01.
John Lafferty, A.Mccallum, F.Pereira. 2001. Condi-
tional Random Field: Probabilitic Models for Seg-
menting and Labeling Sequence Data., Proceedings
of the Eighteenth International Conference on Ma-
chine Learning: 282?289.
Hai Zhao, Changning Huang, Mu Li. 2006. An
Improved Chinese Word Segmentation System with
Conditional Random Field, Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Process-
ing (SIGHAN-5)(Sydney, Australia):162-165.
