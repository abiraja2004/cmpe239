Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 286?289,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Spoken Tutorial Dialogue and the Feeling of Another?s Knowing
Diane Litman
University of Pittsburgh
Pittsburgh, PA 15260 USA
litman@cs.pitt.edu
Kate Forbes-Riley
University of Pittsburgh
Pittsburgh, PA 15260 USA
forbesk@cs.pitt.edu
Abstract
We hypothesize that monitoring the accu-
racy of the ?feeling of another?s know-
ing? (FOAK) is a useful predictor of tu-
torial dialogue system performance. We
test this hypothesis in the context of a
wizarded spoken dialogue tutoring system,
where student learning is the primary per-
formance metric. We first present our cor-
pus, which has been annotated with re-
spect to student correctness and uncer-
tainty. We then discuss the derivation of
FOAK measures from these annotations,
for use in building predictive performance
models. Our results show that monitoring
the accuracy of FOAK is indeed predictive
of student learning, both in isolation and in
conjunction with other predictors.
1 Introduction
Detecting and exploiting knowledge of a speaker?s
uncertainty has been studied in several research
communities. Spoken language researchers have
identified statistically significant relationships be-
tween speaker uncertainty and linguistic proper-
ties of utterances such as prosody and lexical con-
tent (Liscombe et al, 2005; Dijkstra et al, 2006;
Pon-Barry, 2008). Spoken dialogue researchers
in turn are studying whether responding to user
states such as uncertainty can improve system
performance as measured by usability and effi-
ciency (Tsukahara and Ward, 2001; Pon-Barry et
al., 2006; Forbes-Riley and Litman, 2009a). In
the psycholinguistics community, uncertainty has
been studied in the context of metacognitive abil-
ities, e.g. the ability to monitor the accuracy
of one?s own knowledge (?Feeling of Knowing?
(FOK)), and the ability to monitor the FOK of
someone else (?Feeling of Another?s Knowing?
(FOAK)) (Smith and Clark, 1993; Brennan and
Williams, 1995).
Here we take a spoken dialogue systems per-
spective on FOAK, and investigate whether mon-
itoring the accuracy of FOAK is a useful con-
struct for predictive performance modeling. Our
study uses data previously collected with a wiz-
arded spoken dialogue tutoring system, where stu-
dent learning is the primary performance metric.
Section 2 reviews several relevant constructs and
measures from the area of metacognition. Sec-
tion 3 introduces our dialogue corpus and its user
correctness and uncertainty annotations. Section 4
presents our method for measuring monitoring ac-
curacy of FOAK from these annotations, while
Section 5 shows how we use these measures to
build predictive performance models. Our results
show that monitoring the accuracy of FOAK is in-
deed a significant positive predictor of learning,
both in isolation and over and above other predic-
tors. As discussed in Section 6, increasing mon-
itoring accuracy of FOAK is thus one avenue for
also potentially increasing performance, which we
plan to explore in future versions of our system.
2 Feeling of Another?s Knowing
?Feeling of knowing? (FOK) refers to peoples?
ability to accurately monitor their own knowl-
edge, e.g. to know whether they have answered
a question correctly. Psycholinguistics research
has shown that speakers display FOK in conver-
sation using linguistic cues such as filled pauses
and prosody (Smith and Clark, 1993). Of perhaps
more relevance to dialogue systems, research has
also shown that listeners can use the same cues
to monitor the FOK of someone else, i.e. ?feel-
286
ing of another?s knowing? (FOAK) (Brennan and
Williams, 1995).
To quantify knowledge monitoring, measures of
monitoring accuracy have been proposed. For ex-
ample, consider an FOK experimental paradigm,
where subjects 1) respond to a set of general
knowledge questions, 2) take a FOK survey, judg-
ing whether or not1 they think they would rec-
ognize the answer to each question in a multiple
choice test, and 3) take such a recognition test. As
shown in Figure 1, such data can be summarized in
an array where each cell represents a mutually ex-
clusive option: the row labels represent the possi-
ble FOK judgments (Y/N), while the columns rep-
resent the possible results of the multiple choice
test (Y/N).
Recognition=Y Recognition=N
Judgment=Y a b
Judgment=N c d
Gamma = (a)(d)?(b)(c)(a)(d)+(b)(c) HC =
(a+d)?(b+c)
(a+d)+(b+c)
Figure 1: Measuring Monitoring Accuracy.
Given such an array, the relationship between
the correctness and the judgment of FOK for an-
swers can be measured using the standard formu-
las in Figure 1: Gamma and the Harmann coef-
ficient (HC) measure relative and absolute knowl-
edge monitoring accuracy, respectively. We use
both metrics in the performance modeling exper-
iments described in Section 5, as both measures
have been well studied and there is a lack of con-
sensus regarding their relative benefits (e.g. (Niet-
feld et al, 2006)). Note that both range in value
from -1 (no monitoring accuracy) to 1 (perfect
monitoring accuracy).
3 Data
This research uses a corpus of 405 dialogues be-
tween 81 students and ITSPOKE-WOZ. The cor-
pus was collected in an experiment showing that
performance improved when ITSPOKE-WOZ re-
sponded to the uncertainty in student answers,
over and above correctness (Forbes-Riley and Lit-
man, 2009a).
ITSPOKE-WOZ is a semi-automatic version
of ITSPOKE (Intelligent Tutoring SPOKEn di-
alogue system), which is a speech-enhanced ver-
sion of the Why2-Atlas qualitative physics tu-
tor (VanLehn et al, 2002). Recent versions of
1Likert scale rating schemes are also possible.
ITSPOKE have also been enhanced to detect and
respond to student uncertainty, by parameterizing
the dialogue manager so that it can adapt con-
tingently based on student correctness and/or un-
certainty (to test various ways of responding to
uncertainty). In ITSPOKE-WOZ, a few compo-
nents (speech recognition, language understand-
ing, and uncertainty detection) have been replaced
by a human ?wizard?, to test the upper-bound per-
formance of adapting to uncertainty in a spoken
dialogue system (i.e. without the noise introduced
by speech and language processing).
The experimental procedure was as follows:
Students (1) read background material, (2) took
a multiple-choice pretest, (3) worked through five
problems with ITSPOKE-WOZ (1 per dialogue),
(4) took a survey, and (5) took a posttest simi-
lar to the pretest. Each student turn in the cor-
pus was annotated2 in real-time by the wizard dur-
ing the experiment, using binary tags representing
student Uncertainty (uncertain (U), nonuncertain
(nonU)) and student Correctness (correct (C), in-
correct (I)). Figure 2 shows an annotated dialogue
excerpt.
ITSPOKE-WOZ6: Now let?s find the forces exerted on the
car in the VERTICAL direction during the collision. First,
what vertical force is always exerted on an object near the
surface of the earth?
USER6: gravity (nonU,C)
ITSPOKE-WOZ7: Fine. In addition to the downward force
of gravity, one other force is exerted on the car in the VER-
TICAL direction during the collision. We haven?t discussed
this force yet, because we only dealt with freefalling objects
before this problem. But you may recall this other vertical
force from the reading. What is it?
USER7: um ... the friction from the air ... or the wind (U,I)
ITSPOKE-WOZ8: Great try, but I think you know there?s
a mistake in your answer. Let?s work it out. We know that
gravity can?t be the only vertical force on the car, because . . .
Figure 2: Annotated Corpus Excerpt.
4 Predictors
To derive our performance models, we use
posttest score as the dependent measure, and in-
clude pretest score among the independent mea-
sures. This allows us to measure learning gain,
not just final test score. We also include addi-
tional independent measures computed from the
wizard?s uncertainty and correctness annotations,
2In prior ITSPOKE corpora, this wizard displayed in-
terannotator agreement of 0.85 and 0.62 Kappa on correct-
ness and uncertainty, respectively (Forbes-Riley and Litman,
2008).
287
as detailed below. Note that all measures are com-
puted on a per student basis (over all 5 dialogues).
Table 1 shows means and standard deviations of
all measures across all 81 students.
Measure Mean Std. Dev.
pretest .51 .15
posttest .75 .14
%C .79 .09
%U .23 .11
Gamma .77 .17
HC .59 .16
Table 1: Descriptive Corpus Statistics.
The percentage of student turns annotated as
correct (%C) and as uncertain (%U) normalize
the raw counts of the wizard?s C and U annota-
tions. Similar measures predict learning in prior
experiments by ourselves and others (e.g (Litman
et al, 2009)) and thus serve as useful baselines.
In our corpus, 79% of a student?s turns are an-
swered correctly on average, while 77% are an-
swered without uncertainty.
The monitoring accuracy measures Gamma
and HC were introduced in Section 2. To con-
struct an array like that shown in Figure 1, we
map the first and second rows to our uncertainty
annotations NonU and U, and map the columns to
our correctness annotations C and I. In (Dijkstra
et al, 2006), high and low FOK/FOAK judgments
are similarly associated with speaker certainty and
uncertainty, respectively. Note that in our annota-
tion scheme, NonU answers are either certain or
neutral.
5 Results: Predicting Student Learning
Given the above measures, our first prediction ex-
periment measures the partial Pearson?s correla-
tion between each of the independent measures
and posttest, after first controlling for pretest to
account for learning gain. Our goal here is exam-
ine the predictive utility of the correctness, uncer-
tainty, and monitoring dimensions in isolation.
Table 2 shows the statistically significant results
of the partial correlations. The table shows the in-
dependent measure, the corresponding Pearson?s
Correlation Coefficient (R), and the significance of
the correlation (p). As can be seen, both monitor-
ing measures are positively correlated with learn-
ing, with HC providing better predictive utility
than Gamma. However, %C is even more pre-
dictive of learning than either monitoring measure.
Interestingly, the uncertainty measure %U in and
of itself does not show predictive utility in this
data.
Measure R p
%C .52 .00
Gamma .36 .00
HC .42 .00
Table 2: Partial Correlations with Posttest (p <
.05).
Our second prediction experiment uses PAR-
ADISE to build a learning model that can po-
tentially include multiple independent measures.
As in prior PARADISE applications (e.g. (Mo?ller,
2005)), we train the models using stepwise mul-
tiple linear regression, which automatically deter-
mines the measures to include in the model. Our
goal here is to explore whether monitoring accu-
racy provides any added value to our correctness
and uncertainty measures.
When all measures are made available for pre-
dicting learning, we see that monitoring accuracy
as measured byHC does add value over and above
correctness: the stepwise procedure includes HC
in the model, as it significantly accounts for more
variance than just including %C and pretest. In
particular, the application of PARADISE shows
that the following performance function provides
the best significant training fit to our data (R2 =
.71, p < .01):
postest = .44?%C+ .21?pretest+ .20?HC
The equation shows each selected measure and its
(standardized) weight; larger weights indicate pa-
rameters with greater relative predictive power in
accounting for posttest variance. %C is signifi-
cant at p < .01, while pretest and HC are each
significant at p < .05, with the coefficients all pos-
itive. Like the correlations, our regression demon-
strates the predictive utility of the accuracy and
monitoring measures, but not the uncertainty mea-
sure. The model further shows that while correctly
answering the system?s questions (%C) is predic-
tive of learning, also including FOAK monitoring
accuracy (HC) significantly increases the model?s
predictive power.
6 Conclusion and Future Directions
This paper explores whether knowledge monitor-
ing accuracy is a useful construct for understand-
ing dialogue system performance. In particular,
288
we demonstrate the utility of combining previ-
ously studied correctness and uncertainty annota-
tions, using a measure of FOAK monitoring ac-
curacy. Our results show that while the correct-
ness of a user?s response predicts learning, the un-
certainty with which a user conveys a response
does not. In contrast, the ability to monitor FOAK
accuracy predicts learning, in isolation and over
and above correctness. We believe that monitor-
ing accuracy will be a relevant construct for other
dialogue applications involving knowledge asym-
metry, such as problem solving, instruction giv-
ing, and trouble shooting (e.g. (Janarthanam and
Lemon, 2008)).
In future work we plan to use our results to in-
form a modification of our system aimed at im-
proving inferred user knowledge monitoring abil-
ities; we will better measure such improvements
by incorporating FOK ratings into our testing. In
addition, we recently found interactions between
learning and both user domain expertise and gen-
der (Forbes-Riley and Litman, 2009b); we will
investigate whether similar interactions extend to
knowledge monitoring metrics. Since our corpus
contains dialogues with both uncertainty-adaptive
and non-adaptive versions of ITSPOKE-WOZ, we
also plan to examine whether differing dialogue
strategies influence the learned predictive models.
Finally, we plan to replicate our analyses in a di-
alogue corpus we recently collected using a fully
automated version of our system.
Acknowledgements
NSF #0631930 supports this work. We thank H.
Ai, P. Jordan, and the reviewers for helpful com-
ments.
References
S. E. Brennan and M. Williams. 1995. The feeling
of another?s knowing: Prosody and filled pauses as
cues to listeners about the metacognitive states of
speakers. Journal of Memory and Language.
C. Dijkstra, E. Krahmer, and M. Swerts. 2006. Ma-
nipulating uncertainty: The contribution of different
audiovisual prosodic cues to the perception of confi-
dence. In Proc. Speech Prosody.
K. Forbes-Riley and D. J. Litman. 2008. Analyz-
ing dependencies between student certainness states
and tutor responses in a spoken dialogue corpus. In
L. Dybkjaer and W. Minker, editors, Recent Trends
in Discourse and Dialogue. Springer.
K. Forbes-Riley and D. Litman. 2009a. Adapting to
student uncertainty improves tutoring dialogues. In
Proc. Intl. Conf. on Artificial Intelligence in Educa-
tion.
K. Forbes-Riley and D. Litman. 2009b. A user
modeling-based performance analysis of a wizarded
uncertainty-adaptive dialogue system corpus. In
Proc. Interspeech, Brighton, UK, September.
S. Janarthanam and O. Lemon. 2008. User simulations
for online adaptation and knowledge-alignment in
troubleshooting dialogue systems. In Proc. SEM-
dial.
J. Liscombe, J. Venditti, and J. Hirschberg. 2005. De-
tecting certainness in spoken tutorial dialogues. In
Proc. Interspeech.
D. Litman, J. Moore, M. Dzikovska, and E. Farrow.
2009. Using natural language processing to analyze
tutorial dialogue corpora across domains and modal-
ities. In Proc. Intl. Conf. on Artificial Intelligence in
Education.
S. Mo?ller. 2005. Parameters for quantifying the in-
teraction with spoken dialogue telephone services.
In Proc. SIGdial Workshop on Discourse and Dia-
logue.
J. L. Nietfeld, C. K. Enders, and G. Schraw. 2006. A
Monte Carlo comparison of measures of relative and
absolute monitoring accuracy. Educational and Psy-
chological Measurement.
H. Pon-Barry, K. Schultz, E. O. Bratt, B. Clark, and
S. Peters. 2006. Responding to student uncertainty
in spoken tutorial dialogue systems. Intl. Journal of
Artificial Intelligence in Education.
H. Pon-Barry. 2008. Prosodic manifestations of confi-
dence and uncertainty in spoken language. In Proc.
Interspeech.
V. L. Smith and H. H. Clark. 1993. On the course of
answering questions. Journal of Memory and Lan-
guage.
W. Tsukahara and N. Ward. 2001. Responding to sub-
tle, fleeting changes in the user?s internal state. In
Proc. SIG-CHI on Human factors in computing sys-
tems.
K. VanLehn, P. W. Jordan, C. Rose?, D. Bhembe,
M. Bo?ttner, A. Gaydos, M. Makatchev, U. Pap-
puswamy, M. Ringenberg, A. Roque, S. Siler, R. Sri-
vastava, and R. Wilson. 2002. The architecture of
Why2-Atlas: A coach for qualitative physics essay
writing. In Proc. Intl. Conf. on Intelligent Tutoring
Systems.
289
