Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 23?31,
Suntec, Singapore, 7 August 2009.
c
?2009 ACL and AFNLP
Random Walks for Text Semantic Similarity
Daniel Ramage, Anna N. Rafferty, and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{dramage,manning}@cs.stanford.edu
rafferty@eecs.berkeley.edu
Abstract
Many tasks in NLP stand to benefit from
robust measures of semantic similarity for
units above the level of individual words.
Rich semantic resources such as WordNet
provide local semantic information at the
lexical level. However, effectively com-
bining this information to compute scores
for phrases or sentences is an open prob-
lem. Our algorithm aggregates local re-
latedness information via a random walk
over a graph constructed from an underly-
ing lexical resource. The stationary dis-
tribution of the graph walk forms a ?se-
mantic signature? that can be compared
to another such distribution to get a relat-
edness score for texts. On a paraphrase
recognition task, the algorithm achieves an
18.5% relative reduction in error rate over
a vector-space baseline. We also show that
the graph walk similarity between texts
has complementary value as a feature for
recognizing textual entailment, improving
on a competitive baseline system.
1 Introduction
Many natural language processing applications
must directly or indirectly assess the semantic sim-
ilarity of text passages. Modern approaches to
information retrieval, summarization, and textual
entailment, among others, require robust numeric
relevance judgments when a pair of texts is pro-
vided as input. Although each task demands its
own scoring criteria, a simple lexical overlap mea-
sure such as cosine similarity of document vectors
can often serve as a surprisingly powerful base-
line. We argue that there is room to improve these
general-purpose similarity measures, particularly
for short text passages.
Most approaches fall under one of two cate-
gories. One set of approaches attempts to explic-
itly account for fine-grained structure of the two
passages, e.g. by aligning trees or constructing
logical forms for theorem proving. While these
approaches have the potential for high precision
on many examples, errors in alignment judgments
or formula construction are often insurmountable.
More broadly, it?s not always clear that there is a
correct alignment or logical form that is most ap-
propriate for a particular sentence pair. The other
approach tends to ignore structure, as canonically
represented by the vector space model, where any
lexical item in common between the two passages
contributes to their similarity score. While these
approaches often fail to capture distinctions im-
posed by, e.g. negation, they do correctly capture
a broad notion of similarity or aboutness.
This paper presents a novel variant of the vec-
tor space model of text similarity based on a ran-
dom walk algorithm. Instead of comparing two
bags-of-words directly, we compare the distribu-
tion each text induces when used as the seed of
a random walk over a graph derived from Word-
Net and corpus statistics. The walk posits the ex-
istence of a distributional particle that roams the
graph, biased toward the neighborhood surround-
ing an input bag of words. Eventually, the walk
reaches a stationary distribution over all nodes in
the graph, smoothing the peaked input distribution
over a much larger semantic space. Two such sta-
tionary distributions can be compared using con-
ventional measures of vector similarity, producing
a final relatedness score.
This paper makes the following contributions.
We present a novel random graph walk algorithm
23
Word Step 1 Step 2 Step 3 Conv.
eat 3 8 9 9
corrode 10 33 53 >100
pasta ? 2 3 5
dish ? 4 5 6
food ? ? 21 12
solid ? ? ? 26
Table 1: Ranks of sample words in the distribu-
tion for I ate a salad and spaghetti after a given
number of steps and at convergence. Words in the
vector are ordered by probability at time step t; the
word with the highest probability in the vector has
rank 1. ??? indicates that node had not yet been
reached.
for semantic similarity of texts, demonstrating its
efficiency as compared to a much slower but math-
ematically equivalent model based on summed
similarity judgments of individual words. We
show that walks effectively aggregate information
over multiple types of links and multiple input
words on an unsupervised paraphrase recognition
task. Furthermore, when used as a feature, the
walk?s semantic similarity score can improve the
performance of an existing, competitive textual
entailment system. Finally, we provide empiri-
cal results demonstrating that indeed, each step of
the random walk contributes to its ability to assess
paraphrase judgments.
2 A random walk example
To provide some intuition about the behavior of
the random walk on text passages, consider the
following example sentence: I ate a salad and
spaghetti.
No measure based solely on lexical identity
would detect overlap between this sentence and
another input consisting of only the word food.
But if each text is provided as input to the random
walk, local relatedness links from one word to an-
other allow the distributional particle to explore
nearby parts of the semantic space. The number of
non-zero elements in both vectors increases, even-
tually converging to a stationary distribution for
which both vectors have many shared non-zero en-
tries.
Table 1 ranks elements of the sentence vector
based on their relative weights. Observe that at the
beginning of the walk, corrode has a high rank due
to its association with the WordNet sense of eat
corresponding to eating away at something. How-
ever, because this concept is not closely linked
with other words in the sentence, its relative rank
drops as the distribution converges and other word
senses more related to food are pushed up. The
random walk allows the meanings of words to re-
inforce one another. If the sentence above had
ended with drank wine rather than spaghetti, the
final weight on the food node would be smaller
since fewer input words would be as closely linked
to food. This matches the intuition that the first
sentence has more to do with food than does the
second, although both walks should and do give
some weight to this node.
3 Related work
Semantic relatedness for individual words has
been thoroughly investigated in previous work.
Budanitsky and Hirst (2006) provide an overview
of many of the knowledge-based measures derived
from WordNet, although other data sources have
been used as well. Hughes and Ramage (2007) is
one such measure based on random graph walks.
Prior work has considered random walks on var-
ious text graphs, with applications to query expan-
sion (Collins-Thompson and Callan, 2005), email
address resolution (Minkov and Cohen, 2007), and
word-sense disambiguation (Agirre and Soroa,
2009), among others.
Measures of similarity have also been proposed
for sentence or paragraph length text passages.
Mihalcea et al (2006) present an algorithm for
the general problem of deciding the similarity of
meaning in two text passages, coining the name
?text semantic similarity? for the task. Corley
and Mihalcea (2005) apply this algorithm to para-
phrase recognition.
Previous work has shown that similarity mea-
sures can have some success as a measure of tex-
tual entailment. Glickman et al (2005) showed
that many entailment problems can be answered
using only a bag-of-words representation and web
co-occurrence statistics. Many systems integrate
lexical relatedness and overlap measures with
deeper semantic and syntactic features to create
improved results upon relatedness alone, as in
Montejo-R?aez et al (2007).
4 Random walks on lexical graphs
In this section, we describe the mechanics of
computing semantic relatedness for text passages
24
based on the random graph walk framework. The
algorithm underlying these computations is related
to topic-sensitive PageRank (Haveliwala, 2002);
see Berkhin (2005) for a survey of related algo-
rithms.
To compute semantic relatedness for a pair of
passages, we compare the stationary distributions
of two Markov chains, each with a state space de-
fined over all lexical items in an underlying corpus
or database. Formally, we define the probability of
finding the particle at a node n
i
at time t as:
n
(t)
i
=
?
n
j
?V
n
(t?1)
j
P (n
i
| n
j
)
where P (n
i
| n
j
) is the probability of transition-
ing from n
j
to n
i
at any time step. If those transi-
tions bias the particle to the neighborhood around
the words in a text, the particle?s distribution can
be used as a lexical signature.
To compute relatedness for a pair of texts, we
first define the graph nodes and transition proba-
bilities for the random walk Markov chain from
an underlying lexical resource. Next, we deter-
mine an initial distribution over that state space for
a particular input passage of text. Then, we sim-
ulate a random walk in the state space, biased to-
ward the initial distribution, resulting in a passage-
specific distribution over the graph. Finally, we
compare the resulting stationary distributions from
two such walks using a measure of distributional
similarity. The remainder of this section discusses
each stage in more detail.
4.1 Graph construction
We construct a graph G = (V,E) with vertices V
and edges E extracted from WordNet 3.0. Word-
Net (Fellbaum, 1998) is an annotated graph of
synsets, each representing one concept, that are
populated by one or more words. The set of ver-
tices extracted from the graph is all synsets present
in WordNet (e.g. foot#n#1 meaning the part of
the human leg below the ankle), all part-of-speech
tagged words participating in those synsets (e.g.
foot#n linking to foot#n#1 and foot#n#2 etc.), and
all untagged words (e.g. foot linking to foot#n and
foot#v). The set of edges connecting synset nodes
is all inter-synset edges contained in WordNet,
such as hyponymy, synonomy, antonymy, etc., ex-
cept for regional and usage links. All WordNet
relational edges are given uniform weight. Edges
also connect each part-of-speech tagged word to
all synsets it takes part in, and from each word to
all its part-of-speech. These edge weights are de-
rived from corpus counts as in Hughes and Ram-
age (2007). We also included a low-weight self-
loop for each node.
Our graph has 420,253 nodes connected by
1,064,464 edges. Because synset nodes do not link
outward to part-of-speech tagged nodes or word
nodes in this graph, only the 117,659 synset nodes
have non-zero probability in every random walk?
i.e. the stationary distribution will always be non-
zero for these 117,659 nodes, but will be non-zero
for only a subset of the remainder.
4.2 Initial distribution construction
The next step is to seed the random walk with an
initial distribution over lexical nodes specific to
the given sentence. To do so, we first tag the in-
put sentence with parts-of-speech and lemmatize
each word based on the finite state transducer of
Minnen et al (2001). We search over consecu-
tive words to match multi-word collocation nodes
found in the graph. If the word or its lemma is
part of a sequence that makes a complete colloca-
tion, that collocation is used. If not, the word or
its lemma with its part of speech tag is used if it
is present as a graph node. Finally, we fall back
to the surface word form or underlying lemma
form without part-of-speech information if neces-
sary. For example, the input sentence: The boy
went with his dog to the store, would result in mass
being assigned to underlying graph nodes boy#n,
go with, he, dog#n, store#n.
Term weights are set with tf.idf and then nor-
malized. Each term?s weight is proportional to the
number of occurrences in the sentence times the
log of the number of documents in some corpus
divided by the number of documents containing
that term. Our idf counts were derived from the
English Gigaword corpus 1994-1999.
4.3 Computing the stationary distribution
We use the power iteration method to compute the
stationary distribution for the Markov chain. Let
the distribution over the N states at time step t of
the random walk be denoted ~v
(t)
? R
N
, where
~v
(0)
is the initial distribution as defined above. We
denote the column-normalized state-transition ma-
trix as M ? R
N?N
. We compute the stationary
distribution of the Markov chain with probability
? of returning to the initial distribution at each
25
time step as the limit as t?? of:
~v
(t)
= ?~v
(0)
+ (1? ?)M~v
(t?1)
In practice, we test for convergence by examining
if
?
N
i=1
?v
(t)
i
? v
(t?1)
i
? < 10
?6
, which in our ex-
periments was usually after about 50 iterations.
Note that the resulting stationary distribution
can be factored as the weighted sum of the sta-
tionary distributions of each word represented in
the initial distribution. Because the initial distri-
bution ~v
(0)
is a normalized weighted sum, it can
be re-written as ~v
(0)
=
?
k
?
k
? ~w
(0)
k
for ~w
k
hav-
ing a point mass at some underlying node in the
graph and with ?
k
positive such that
?
k
?
k
= 1.
A simple proof by induction shows that the sta-
tionary distribution ~v
(?)
is itself the weighted sum
of the stationary distribution of each underlying
word, i.e. ~v
?
=
?
k
?
k
? ~w
(?)
k
.
In practice, the stationary distribution for a
passage of text can be computed from a single
specially-constructed Markov chain. The process
is equivalent to taking the weighted sum of every
word type in the passage computed independently.
Because the time needed to compute the station-
ary distribution is dominated by the sparsity pat-
tern of the walk?s transition matrix, the computa-
tion of the stationary distribution for the passage
takes a fraction of the time needed if the station-
ary distribution for each word were computed in-
dependently.
4.4 Comparing stationary distributions
In order to get a final relatedness score for a pair
of texts, we must compare the stationary distribu-
tion from the first walk with the distribution from
the second walk. There exist many measures for
computing a final similarity (or divergence) mea-
sure from a pair of distributions, including geo-
metric measures, information theoretic measures,
and probabilistic measures. See, for instance, the
overview of measures provided in Lee (2001).
In system development on training data, we
found that most measures were reasonably effec-
tive. For the rest of this paper, we report num-
bers using cosine similarity, a standard measure in
information retrieval; Jensen-Shannon divergence,
a commonly used symmetric measure based on
KL-divergence; and the dice measure extended to
weighted features (Curran, 2004). A summary of
these measures is shown in Table 2. Justification
Cosine
~x?~y
?~x?
2
?~y?
2
Jensen-Shannon
1
2
D(x?
x+y
2
) +
1
2
D(y?
x+y
2
)
Dice
2
P
i
min(x
i
,y
i
)
P
i
x
i
+
P
i
y
i
Table 2: Three measures of distributional similar-
ity between vectors ~x and ~y used to compare the
stationary distributions from passage-specific ran-
dom walks. D(p?q) is KL-divergence, defined as
?
i
p
i
log
p
i
q
i
.
for the choice of these three measures is discussed
in Section 6.
5 Evaluation
We evaluate the system on two tasks that might
benefit from semantic similarity judgments: para-
phrase recognition and recognizing textual entail-
ment. A complete solution to either task will cer-
tainly require tools more tuned to linguistic struc-
ture; the paraphrase detection evaluation argues
that the walk captures a useful notion of semantics
at the sentence level. The entailment system eval-
uation demonstrates that the walk score can im-
prove a larger system that does make use of more
fine-grained linguistic knowledge.
5.1 Paraphrase recognition
The Microsoft Research (MSR) paraphrase data
set (Dolan et al, 2004) is a collection of 5801
pairs of sentences automatically collected from
newswire over 18 months. Each pair was hand-
annotated by at least two judges with a binary
yes/no judgment as to whether one sentence was
a valid paraphrase of the other. Annotators were
asked to judge whether the meanings of each
sentence pair were reasonably equivalent. Inter-
annotator agreement was 83%. However, 67% of
the pairs were judged to be paraphrases, so the cor-
pus does not reflect the rarity of paraphrases in the
wild. The data set comes pre-split into 4076 train-
ing pairs and 1725 test pairs.
Because annotators were asked to judge if the
meanings of two sentences were equivalent, the
paraphrase corpus is a natural evaluation testbed
for measures of semantic similarity. Mihalcea et
al. (2006) defines a measure of text semantic sim-
ilarity and evaluates it in an unsupervised para-
phrase detector on this data set. We present their
26
algorithm here as a strong reference point for se-
mantic similarity between text passages, based on
similar underlying lexical resources.
The Mihalcea et al (2006) algorithm is a wrap-
per method that works with any underlying mea-
sure of lexical similarity. The similarity of a pair
of texts T
1
and T
2
, denoted as sim
m
(T
1
, T
2
), is
computed as:
sim
m
(T
1
, T
2
) =
1
2
f(T
1
, T
2
) +
1
2
f(T
2
, T
1
)
f(T
a
, T
b
) =
P
w?T
a
maxSim(w, T
b
) ? idf(w)
P
w?T
a
idf(w)
where the maxSim(w, T ) function is defined as
the maximum similarity of the word w within the
text T as determined by an underlying measure of
lexical semantic relatedness. Here, idf(w) is de-
fined as the number of documents in a background
corpus divided by the number of documents con-
taining the term. maxSim compares only within
the same WordNet part-of-speech labeling in or-
der to support evaluation with lexical relatedness
measures that cannot cross part-of-speech bound-
aries.
Mihalcea et al (2006) presents results for sev-
eral underlying measures of lexical semantic re-
latedness. These are subdivided into corpus-based
measures (using Latent Semantic Analysis (Lan-
dauer et al, 1998) and a pointwise-mutual infor-
mation measure) and knowledge-based resources
driven by WordNet. The latter include the methods
of Jiang and Conrath (1997), Lesk (1986), Resnik
(1999), and others.
In this unsupervised experimental setting, we
consider using only a thresholded similarity value
from our system and from the Mihalcea algorithm
to determine the paraphrase or non-paraphrase
judgment. For consistency with previous work, we
threshold at 0.5. Note that this threshold could be
tuned on the training data in a supervised setting.
Informally, we observed that on the training data a
threshold of near 0.5 was often a good choice for
this task.
Table 3 shows the results of our system and
a representative subset of those reported in (Mi-
halcea et al, 2006). All the reported measures
from both systems do a reasonable job of para-
phrase detection ? the majority of pairs in the cor-
pus are deemed paraphrases when the similarity
measure is thresholded at 0.5, and indeed this is
reasonable given the way in which the data were
System Acc. F
1
: c
1
F
1
: c
0
Macro F
1
Random Graph Walk
Walk (Cosine) 0.687 0.787 0.413 0.617
Walk (Dice) 0.708 0.801 0.453 0.645
Walk (JS) 0.688 0.805 0.225 0.609
Mihalcea et. al., Corpus-based
PMI-IR 0.699 0.810 0.301 0.625
LSA 0.684 0.805 0.170 0.560
Mihalcea et. al., WordNet-based
J&C 0.693 0.790 0.433 0.629
Lesk 0.693 0.789 0.439 0.629
Resnik 0.690 0.804 0.254 0.618
Baselines
Vector-based 0.654 0.753 0.420 0.591
Random 0.513 0.578 0.425 0.518
Majority (c
1
) 0.665 0.799 ? 0.399
Table 3: System performance on 1725 examples of
the MSR paraphrase detection test set. Accuracy
(micro-averaged F
1
), F
1
for c
1
?paraphrase? and
c
0
?non-paraphrase? classes, and macro-averaged
F
1
are reported.
collected. The first three rows are the perfor-
mance of the similarity judgments output by our
walk under three different distributional similar-
ity measures (cosine, dice, and Jensen-Shannon),
with the walk score using the dice measure outper-
forming all other systems on both accuracy and
macro-averaged F
1
. The output of the Mihalcea
system using a representative subset of underly-
ing lexical measures is reported in the second and
third segments. The fourth segment reports the re-
sults of baseline methods?the vector space simi-
larity measure is cosine similarity among vectors
using tf.idf weighting, and the random baseline
chooses uniformly at random, both as reported in
(Mihalcea et al, 2006). We add the additional
baseline of always guessing the majority class la-
bel because the data set is skewed toward ?para-
phrase.?
In an unbalanced data setting, it is important to
consider more than just accuracy and F
1
on the
majority class. We report accuracy, F
1
for each
class label, and the macro-averaged F
1
on all sys-
tems. F
1
: c
0
and Macro-F
1
are inferred for the sys-
tem variants reported in (Mihalcea et al, 2006).
Micro-averaged F
1
in this context is equivalent to
accuracy (Manning et al, 2008).
Mihalcea also reports a combined classifier
which thresholds on the simple average of the in-
dividual classifiers, resulting in the highest num-
bers reported in that work, with accuracy of 0.703,
?paraphrase? class F
1
: c
1
= 0.813, and inferred
Macro F
1
= 0.648. We believe that the scores
27
Data Set Cosine Dice Jensen-Shannon
RTE2 dev 55.00 51.75 55.50
RTE2 test 57.00 54.25 57.50
RTE3 dev 59.00 57.25 59.00
RTE3 test 55.75 55.75 56.75
Table 4: Accuracy of entailment detection when
thresholding the text similarity score output by the
random walk.
from the various walk measures might also im-
prove performance when in a combination clas-
sifier, but without access to the individual judg-
ments in that system we are unable to evaluate
the claim directly. However, we did create an up-
per bound reference by combining the walk scores
with easily computable simple surface statistics.
We trained a support vector classifier on the MSR
paraphrase training set with a feature space con-
sisting of the walk score under each distributional
similarity measure, the length of each text, the dif-
ference between those lengths, and the number of
unigram, bigram, trigram, and four-gram overlaps
between the two texts. The resulting classifier
achieved accuracy of 0.719 with F
1
: c
1
= 0.807
and F
1
: c
0
= 0.487 and Macro F
1
= 0.661. This
is a substantial improvement, roughly on the same
order of magnitude as from switching to the best
performing distributional similarity function.
Note that the running time of the Mihalcea et
al. algorithm for comparing texts T
1
and T
2
re-
quires |T
1
| ? |T
2
| individual similarity judgments.
By contrast, this work allows semantic profiles to
be constructed and evaluated for each text in a sin-
gle pass, independent of the number of terms in
the texts.
The performance of this unsupervised applica-
tion of walks to paraphrase recognition suggests
that the framework captures important intuitions
about similarity in text passages. In the next sec-
tion, we examine the performance of the measure
embedded in a larger system that seeks to make
fine-grained entailment judgments.
5.2 Textual entailment
The Recognizing Textual Entailment Challenge
(Dagan et al, 2005) is a task in which systems as-
sess whether a sentence is entailed by a short pas-
sage or sentence. Participants have used a variety
of strategies beyond lexical relatedness or overlap
for the task, but some have also used only rela-
tively simple similarity metrics. Many systems
Data Set Baseline Cosine Dice JS
RTE2 dev 66.00 66.75 65.75 66.25
RTE2 test 63.62 64.50 63.12 63.25
RTE3 dev 70.25 70.50 70.62 70.38
RTE3 test 65.44 65.82 65.44 65.44
Table 5: Accuracy when the random walk is
added as a feature of an existing RTE system
(left column) under various distance metrics (right
columns).
incorporate a number of these strategies, so we
experimented with using the random walk to im-
prove an existing RTE system. This addresses the
fact that using similarity alone to detect entailment
is impoverished: entailment is an asymmetric de-
cision while similarity is necessarily symmetric.
However, we also experiment with thresholding
random walk scores as a measure of entailment to
compare to other systems and provide a baseline
for whether the walk could be useful for entail-
ment detection.
We tested performance on the development and
test sets for the Second and Third PASCAL RTE
Challenges (Bar-Haim et al, 2006; Giampiccolo
et al, 2007). Each of these data sets contains 800
pairs of texts for which to determine entailment.
In some cases, no words from a passage appear
in WordNet, leading to an empty vector. In this
case, we use the Levenshtein string similarity mea-
sure between the two texts; this fallback is used in
fewer than five examples in any of our data sets
(Levenshtein, 1966).
Table 4 shows the results of using the simi-
larity measure alone to determine entailment; the
system?s ability to recognize entailment is above
chance on all data sets. Since the RTE data sets are
balanced, we used the median of the random walk
scores for each data set as the threshold rather than
using an absolute threshold. While the measure
does not outperform most RTE systems, it does
outperform some systems that used only lexical
overlap such as the Katrenko system from the sec-
ond challenge (Bar-Haim et al, 2006). These re-
sults show that the measure is somewhat sensitive
to the distance metric chosen, and that the best dis-
tance metric may vary by application.
To test the random walk?s value for improv-
ing an existing RTE system, we incorporated the
walk as a feature of the Stanford RTE system
(Chambers et al, 2007). This system computes
28
a weighted sum of a variety of features to make
an entailment decision. We added the random
walk score as one of these features and scaled it
to have a magnitude comparable to the other fea-
tures; other than scaling, there was no system-
specific engineering to add this feature.
As shown in Table 5, adding the random walk
feature improves the original RTE system. Thus,
the random walk score provides meaningful ev-
idence for detecting entailment that is not sub-
sumed by other information, even in a system with
several years of feature engineering and competi-
tive performance. In particular, this RTE system
contains features representing the alignment score
between two passages; this score is composed of a
combination of lexical relatedness scores between
words in each text. The ability of the random walk
to add value to the system even given this score,
which contains many common lexical relatedness
measures, suggests we are able to extract text sim-
ilarity information that is distinct from other mea-
sures. To put the gain we achieve in perspective,
an increase in the Stanford RTE system?s score of
the same magnitude would have moved the sys-
tem?s two challenge entries from 7th and 25th
to 6th and 17th, respectively, in the second RTE
Challenge. It is likely the gain from this feature
could be increased by closer integration with the
system and optimizing the initial distribution cre-
ation for this task.
By using the score as a feature, the system is
able to take advantage of properties of the score
distribution. While Table 4 shows performance
when a threshold is picked a priori, experiment-
ing with that threshold increases performance by
over two percent. By lowering the threshold (clas-
sifying more passages as entailments), we increase
recall of entailed pairs without losing as much pre-
cision in non-entailed pairs since many have very
low scores. As a feature, this aspect of the score
distribution can be incorporated by the system, but
it cannot be used in a simple thresholding design.
6 Discussion
The random walk framework smoothes an initial
distribution of words into a much larger lexical
space. In one sense, this is similar to the technique
of query expansion used in information retrieval.
A traditional query expansion model extends a bag
of words (usually a query) with additional related
words. In the case of pseudo-relevance feedback,
Figure 1: Impact of number of walk steps on cor-
relation with MSR paraphrase judgments. The
left column shows absolute correlation across ten
resampled runs (y-axis) versus number of steps
taken (x-axis). The right column plots the mean
ratio of performance at step t (x-axis) versus per-
formance at convergence.
29
these words come from the first documents re-
turned by the search engine, but other modes of se-
lecting additional words exist. In the random walk
framework, this expansion is analogous to taking
only a single step of the random walk. Indeed,
in the case of the translation model introduced in
(Berger and Lafferty, 1999), they are mathemati-
cally equivalent. However, we have argued that the
walk is an effective global aggregator of related-
ness information. We can formulate the question
as an empirical one?does simulating the walk un-
til convergence really improve our representation
of the text document?
To answer this question, we extracted a 200
items subset of the MSR training data and trun-
cated the walk at each time step up until our con-
vergence threshold was reached at around 50 it-
erations. We then evaluated the correlation of
the walk score with the correct label from the
MSR data for 10 random resamplings of 66 doc-
uments each. Figure 1 plots this result for dif-
ferent distributional similarity measures. We ob-
serve that as the number of steps increases, per-
formance under most of the distributional similar-
ity measures improves, with the exception of the
asymmetric skew-divergence measure introduced
in (Lee, 2001).
This plot also gives some insight into the qual-
itative nature of the stability of the various distri-
butional measures for the paraphrase task. For in-
stance, we observe that the Jensen-Shannon score
and dice score tend to be the most consistent be-
tween runs, but the dice score has a slightly higher
mean. This explains in part why the dice score was
the best performing measure for the task. In con-
trast, cosine similarity was observed to perform
poorly here, although it was found to be the best
measure when combined with our textual entail-
ment system. We believe this discrepancy is due
in part to the feature scaling issues described in
section 5.2.
7 Final remarks
Notions of similarity have many levels of gran-
ularity, from general metrics for lexical related-
ness to application-specific measures between text
passages. While lexical relatedness is well stud-
ied, it is not directly applicable to text passages
without some surrounding environment. Because
this work represents words and passages as in-
terchangeable mathematical objects (teleport vec-
tors), our approach holds promise as a general
framework for aggregating local relatedness infor-
mation between words into reliable measures be-
tween text passages.
The random walk framework can be used to
evaluate changes to lexical resources because it
covers the entire scope of a resource: the whole
graph is leveraged to construct the final distribu-
tion, so changes to any part of the graph are re-
flected in each walk. This means that the meaning-
fulness of changes in the graph can be evaluated
according to how they affect these text similarity
scores; this provides a more semantically relevant
evaluation of updates to a resource than, for ex-
ample, counting how many new words or links be-
tween words have been added. As shown in Jar-
masz and Szpakowicz (2003), an updated resource
may have many more links and concepts but still
have similar performance on applications as the
original. Evaluations of WordNet extensions, such
as those in Navigli and Velardi (2005) and Snow et
al. (2006), are easily conducted within the frame-
work of the random walk.
The presented framework for text semantic sim-
ilarity with random graph walks is more general
than the WordNet-based instantiation explored
here. Transition matrices from alternative linguis-
tic resources such as corpus co-occurrence statis-
tics or larger knowledge bases such as Wikipedia
may very well add value as a lexical resource un-
derlying the walk. One might also consider tailor-
ing the output of the walk with machine learning
techniques like those presented in (Minkov and
Cohen, 2007).
References
E. Agirre and A. Soroa. 2009. Personalizing pagerank
for word sense disambiguation. In EACL, Athens,
Greece.
R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Gi-
ampiccolo, B. Magnini, and I. Szpektor. 2006. The
2nd PASCAL recognizing textual entailment chal-
lenge. In PASCAL Challenges Workshop on RTE.
A. Berger and J. Lafferty. 1999. Information retrieval
as statistical translation. SIGIR 1999, pages 222?
229.
P. Berkhin. 2005. A survey on pagerank computing.
Internet Mathematics, 2(1):73?120.
A. Budanitsky and G. Hirst. 2006. Evaluating
wordnet-based measures of lexical semantic related-
ness. Computational Linguistics, 32(1):13?47.
30
N. Chambers, D. Cer, T. Grenager, D. Hall, C. Kiddon,
B. MacCartney, M. de Marneffe, D. Ramage, E. Yeh,
and C. D. Manning. 2007. Learning alignments and
leveraging natural logic. In ACL-PASCAL Workshop
on Textual Entailment and Paraphrasing.
K. Collins-Thompson and J. Callan. 2005. Query ex-
pansion using random walk models. In CIKM ?05,
pages 704?711, New York, NY, USA. ACM Press.
C. Corley and R. Mihalcea. 2005. Measuring the se-
mantic similarity of texts. In ACL Workshop on Em-
pirical Modeling of Semantic Equivalence and En-
tailment, pages 13?18, Ann Arbor, Michigan, June.
ACL.
J. R. Curran. 2004. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
I. Dagan, O. Glickman, and B. Magnini. 2005.
The PASCAL recognizing textual entailment chal-
lenge. In Quinonero-Candela et al, editor, MLCW
2005, LNAI Volume 3944, pages 177?190. Springer-
Verlag.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsu-
pervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Col-
ing 2004, pages 350?356, Geneva, Switzerland, Aug
23?Aug 27. COLING.
C. Fellbaum. 1998. WordNet: An electronic lexical
database. MIT Press.
D. Giampiccolo, B. Magnini, I. Dagan, and B. Dolan.
2007. The 3rd PASCAL Recognizing Textual En-
tailment Challenge. In ACL-PASCAL Workshop on
Textual Entailment and Paraphrasing, pages 1?9,
Prague, June.
O. Glickman, I. Dagan, and M. Koppel. 2005. Web
based probabilistic textual entailment. In PASCAL
Challenges Workshop on RTE.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In
WWW ?02, pages 517?526, New York, NY, USA.
ACM.
T. Hughes and D. Ramage. 2007. Lexical semantic
relatedness with random graph walks. In EMNLP-
CoNLL, pages 581?589.
M. Jarmasz and S. Szpakowicz. 2003. Roget?s the-
saurus and semantic similarity. In Proceedings of
RANLP-03, pages 212?219.
J. J. Jiang and D. W. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In ROCLING X, pages 19?33.
T.K. Landauer, P.W. Foltz, and D. Laham. 1998. An
introduction to latent semantic analysis. Discourse
Processes, 25(2-3):259?284.
L. Lee. 2001. On the effectiveness of the skew diver-
gence for statistical language analysis. In Artificial
Intelligence and Statistics 2001, pages 65?72.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. ACM SIGDOC: Pro-
ceedings of the 5th Annual International Conference
on Systems Documentation, 1986:24?26.
V. I. Levenshtein. 1966. Binary Codes Capable
of Correcting Deletions, Insertions, and Reversals.
Ph.D. thesis, Soviet Physics Doklady.
C. Manning, P. Raghavan, and H. Schutze, 2008. In-
troduction to information retrieval, pages 258?263.
Cambridge University Press.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of
text semantic similarity. AAAI 2006, 6.
E. Minkov and W. W. Cohen. 2007. Learning to rank
typed graph walks: Local and global approaches. In
WebKDD and SNA-KDD joint workshop 2007.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied
morphological processing of English. Natural Lan-
guage Engineering, 7(03):207?223.
A. Montejo-R?aez, J.M. Perea, F. Mart??nez-Santiago,
M. A. Garc??a-Cumbreras, M. M. Valdivia, and
A. Ure?na L?opez. 2007. Combining lexical-syntactic
information with machine learning for recognizing
textual entailment. In ACL-PASCAL Workshop on
Textual Entailment and Paraphrasing, pages 78?82,
Prague, June. ACL.
R. Navigli and P. Velardi. 2005. Structural seman-
tic interconnections: A knowledge-based approach
to word sense disambiguation. IEEE Trans. Pattern
Anal. Mach. Intell., 27(7):1075?1086.
P. Resnik. 1999. Semantic similarity in a taxonomy:
An information-based measure and its application to
problems of ambiguity in natural language. JAIR,
(11):95?130.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
ACL, pages 801?808.
31
