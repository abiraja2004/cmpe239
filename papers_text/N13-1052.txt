Proceedings of NAACL-HLT 2013, pages 487?496,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Approximate PCFG Parsing Using Tensor Decomposition
Shay B. Cohen
Department of Computer Science
Columbia University, USA
scohen@cs.columbia.edu
Giorgio Satta
Department of Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Michael Collins
Department of Computer Science
Columbia University, USA
mcollins@cs.columbia.edu
Abstract
We provide an approximation algorithm for
PCFG parsing, which asymptotically im-
proves time complexity with respect to the in-
put grammar size, and prove upper bounds on
the approximation quality. We test our algo-
rithm on two treebanks, and get significant im-
provements in parsing speed.
1 Introduction
The problem of speeding-up parsing algorithms
based on probabilistic context-free grammars
(PCFGs) has received considerable attention in
recent years. Several strategies have been proposed,
including beam-search, best-first and A?. In this
paper we focus on the standard approach of approx-
imating the source PCFG in such a way that parsing
accuracy is traded for efficiency.
Nederhof (2000) gives a thorough presentation
of old and novel ideas for approximating non-
probabilistic CFGs by means of finite automata,
on the basis of specialized preprocessing of self-
embedding structures. In the probabilistic domain,
approximation by means of regular grammars is also
exploited by Eisner and Smith (2005), who filter
long-distance dependencies on-the-fly.
Beyond finite automata approximation, Charniak
et al (2006) propose a coarse-to-fine approach in
which an approximated (not necessarily regular)
PCFG is used to construct a parse forest for the in-
put sentence. Some statistical parameters are then
computed on such a structure, and exploited to filter
parsing with the non-approximated grammar. The
approach can also be iterated at several levels. In
the non-probabilistic setting, a similar filtering ap-
proach was also proposed by Boullier (2003), called
?guided parsing.?
In this paper we rely on an algebraic formulation
of the inside-outside algorithm for PCFGs, based on
a tensor formulation developed for latent-variable
PCFGs in Cohen et al (2012). We combine the
method with known techniques for tensor decompo-
sition to approximate the source PCFG, and develop
a novel algorithm for approximate PCFG parsing.
We obtain improved time upper bounds with respect
to the input grammar size for PCFG parsing, and
provide error upper bounds on the PCFG approxi-
mation, in contrast with existing heuristic methods.
2 Preliminaries
This section introduces the special representation for
probabilistic context-free grammars that we adopt in
this paper, along with the decoding algorithm that
we investigate. For an integer i ? 1, we let [i] =
{1, 2, . . . , i}.
2.1 Probabilistic Context-Free Grammars
We consider context-free grammars (CFGs) in
Chomsky normal form, and denote them as
(N ,L,R) where:
? N is the finite set of nonterminal symbols, with
m = |N |, and L is the finite set of words (lexi-
cal tokens), with L?N = ? and with n = |L|.
? R is a set of rules having the form a? b c,
a, b, c ? N , or the form a? x, a ? N and
x ? L.
A probabilistic CFG (PCFG) is a CFG associated
with a set of parameters defined as follows:
? For each (a? b c) ? R, we have a parameter
p(a? b c | a).
487
? For each (a? x) ? R, we have a parameter
p(a? x | a).
? For each a ? N , we have a parameter pia,
which is the probability of a being the root
symbol of a derivation.
The parameters above satisfy the following nor-
malization conditions:
?
(a?b c)?R
p(a? b c | a) +
?
(a?x)?R
p(a? x | a) = 1,
for each a ? N , and
?
a?N pia = 1.
The probability of a tree ? deriving a sentence in
the language, written p(?), is calculated as the prod-
uct of the probabilities of all rule occurrences in ? ,
times the parameter pia where a is the symbol at the
root of ? .
2.2 Tensor Form of PCFGs
A three-dimensional tensor C ? R(m?m?m) is a
set of m3 parameters Ci,j,k for i, j, k ? [m]. In what
follows, we associate with each tensor three func-
tions, each mapping a pair of vectors in Rm into a
vector in Rm.
Definition 1 Let C ? R(m?m?m) be a tensor.
Given two vectors y1, y2 ? Rm, we let C(y1, y2)
be them-dimensional row vector with components:
[C(y1, y2)]i =
?
j?[m],k?[m]
Ci,j,ky
1
j y
2
k .
We also let C(1,2)(y1, y2) be them-dimensional col-
umn vector with components:
[C(1,2)(y
1, y2)]k =
?
i?[m],j?[m]
Ci,j,ky
1
i y
2
j .
Finally, we let C(1,3)(y1, y2) be the m-dimensional
column vector with components:
[C(1,3)(y
1, y2)]j =
?
i?[m],k?[m]
Ci,j,ky
1
i y
2
k .
For two vectors x, y ? Rm we denote by x y ?
Rm the Hadamard product of x and y, i.e., [xy]i =
xiyi. Finally, for vectors x, y, z ? Rm, xy>z> is the
tensor D ? Rm?m?m where Di,j,k = xiyjzk (this
is analogous to the outer product: [xy>]i,j = xiyj).
We extend the parameter set of our PCFG such
that p(a? b c | a) = 0 for all a? b c not in R,
and p(a? x | a) = 0 for all a? x not in R. We
also represent each a ? N by a unique index in [m],
and we represent each x ? L by a unique index in
[n]: it will always be clear from the context whether
these indices refer to a nonterminal inN or else to a
word in L.
In this paper we assume a tensor representation
for the parameters p(a? b c | a), and we denote by
T ? Rm?m?m a tensor such that:
Ta,b,c , p(a? b c | a).
Similarly, we denote by Q ? Rm?n a matrix such
that:
Qa,x , p(a? x | a).
The root probabilities are denoted using a vector pi ?
Rm?1 such that pia is defined as before.
2.3 Minimum Bayes-Risk Decoding
Let z = x1 ? ? ?xN be some input sentence; we write
T (z) to denote the set of all possible trees for z. It
is often the case that parsing aims to find the high-
est scoring tree ?? for z according to the underlying
PCFG, also called the ?Viterbi parse:?
?? = argmax
??T (z)
p(?)
Goodman (1996) noted that Viterbi parsers do not
optimize the same metric that is usually used for
parsing evaluation (Black et al, 1991). He sug-
gested an alternative algorithm, which he called the
?Labelled Recall Algorithm,? which aims to fix this
issue.
Goodman?s algorithm has two phases. In the first
phase it computes, for each a ? N and for each sub-
string xi ? ? ?xj of z, the marginal ?(a, i, j) defined
as:
?(a, i, j) =
?
??T (z) : (a,i,j)??
p(?).
Here we write (a, i, j) ? ? if nonterminal a spans
words xi ? ? ?xj in the parse tree ? .
488
Inputs: Sentence x1 ? ? ?xN , PCFG (N ,L,R), pa-
rameters T ? R(m?m?m), Q ? R(m?n), pi ?
R(m?1).
Data structures:
? Each ?(a, i, j) ? R for a ? N , i, j ? [N ],
i ? j, is a marginal probability.
? Each ?i,j ? R for i, j ? [N ], i ? j, is the high-
est score for a tree spanning substring xi ? ? ?xj .
Algorithm:
(Marginals) ?a ? N ,?i, j ? [N ], i ? j, compute
the marginals ?(a, i, j) using the inside-outside
algorithm.
(Base case) ?i ? [N ],
?i,i = max
(a?xi)?R
?(a, i, i)
(Maximize Labelled Recall) ?i, j ? [N ], i < j,
?i,j = max
a?N
?(a, i, j) + max
i?k<j
(
?i,k + ?k+1,j
)
Figure 1: The labelled recall algorithm from Goodman
(1996). The algorithm in this figure finds the highest
score for a tree which maximizes labelled recall. The ac-
tual parsing algorithm would use backtrack pointers in
the score computation to return a tree. These are omitted
for simplicity.
The second phase includes a dynamic program-
ming algorithm which finds the tree ?? that maxi-
mizes the sum over marginals in that tree:
?? = argmax
??T (z)
?
(a,i,j)??
?(a, i, j).
Goodman?s algorithm is described in Figure 1.
As Goodman notes, the complexity of the second
phase (?Maximize Labelled Recall,? which is also
referred to as ?minimum Bayes risk decoding?) is
O(N3 +mN2). There are two nested outer loops,
each of order N , and inside these, there are two sep-
arate loops, one of order m and one of order N ,
yielding this computational complexity. The reason
for the linear dependence on the number of nonter-
minals is the lack of dependence on the actual gram-
mar rules, once the marginals are computed.
In its original form, Goodman?s algorithm does
not enforce that the output parse trees are included in
the tree language of the PCFG, that is, certain com-
binations of children and parent nonterminals may
violate the rules in the grammar. In our experiments
we departed from this, and changed Goodman?s al-
gorithm by incorporating the grammar into the dy-
namic programming algorithm in Figure 1. The rea-
son this is important for our experiments is that we
binarize the grammar prior to parsing, and we need
to enforce the links between the split nonterminals
(in the binarized grammar) that refer to the same
syntactic category. See Matsuzaki et al (2005) for
more details about the binarization scheme we used.
This step changes the dynamic programming equa-
tion of Goodman to be linear in the size of the gram-
mar (figure 1). However, empirically, it is the inside-
outside algorithm which takes most of the time to
compute with Goodman?s algorithm. In this paper
we aim to asymptotically reduce the time complex-
ity of the calculation of the inside-outside probabili-
ties using an approximation algorithm.
3 Tensor Formulation of the
Inside-Outside Algorithm
At the core of our approach lies the observation that
there is a (multi)linear algebraic formulation of the
inside-outside algorithm. It can be represented as a
series of tensor, matrix and vector products. A sim-
ilar observation has been made for latent-variable
PCFGs (Cohen et al, 2012) and hidden Markov
models, where only matrix multiplication is required
(Jaeger, 2000). Cohen and Collins (2012) use this
observation together with tensor decomposition to
improve the speed of latent-variable PCFG parsing.
The representation of the inside-outside algorithm
in tensor form is given in Figure 2. For example,
if we consider the recursive equation for the inside
probabilities (where ?i,j is a vector varying over the
nonterminals in the grammar, describing the inside
probability for each nonterminal spanning words i
to j):
?i,j =
j?1?
k=i
T (?i,k, ?k+1,j)
489
Inputs: Sentence x1 ? ? ?xN , PCFG (N ,L,R), pa-
rameters T ? R(m?m?m), Q ? R(m?n), pi ?
R(m?1).
Data structures:
? Each ?i,j ? R1?m, i, j ? [N ], i ? j, is a row
vector of inside terms ranging over a ? N .
? Each ?i,j ? Rm?1, i, j ? [N ], i ? j, is a
column vector of outside terms ranging over
a ? N .
? Each ?(a, i, j) ? R for a ? N , i, j ? [N ],
i ? j, is a marginal probability.
Algorithm:
(Inside base case) ?i ? [N ], ?(a? xi) ? R,
[?i,i]a = Qa,x
(Inside recursion) ?i, j ? [N ], i < j,
?i,j =
j?1?
k=i
T (?i,k, ?k+1,j)
(Outside base case) ?a ? N ,
[?1,N ]a = pia
(Outside recursion) ?i, j ? [N ], i ? j,
?i,j =
i?1?
k=1
T(1,2)(?
k,j , ?k,i?1)+
N?
k=j+1
T(1,3)(?
i,k, ?j+1,k)
(Marginals) ?a ? N ,?i, j ? [N ], i ? j,
?(a, i, j) = [?i,j ]a ? [?i,j ]a
Figure 2: The tensor form of the inside-outside algorithm,
for calculation of marginal terms ?(a, i, j).
and then apply the tensor product from Definition 1
to this equation, we get that coordinate a in ?i,j is
defined recursively as follows:
[?i,j ]a =
j?1?
k=i
?
b,c
Ta,b,c ? ?
i,k
b ? ?
k+1,j
c
=
j?1?
k=i
?
b,c
p(a? b c | a)? ?i,kb ? ?
k+1,j
c ,
which is exactly the recursive definition of the inside
algorithm. The correctness of the outside recursive
equations follows very similarly.
The time complexity of the algorithm in this case
is O(m3N3). To see this, observe that each tensor
application takes timeO(m3). Furthermore, the ten-
sor T is applied O(N) times in the computation of
each vector ?i,j and ?i,j . Finally, we need to com-
pute a total ofO(N2) inside and outside vectors, one
for each substring of the input sentence.
4 Tensor Decomposition for the
Inside-Outside Algorithm
In this section, we detail our approach to approxi-
mate parsing using tensor decomposition.
4.1 Tensor Decomposition
In the formulation of the inside-outside algorithm
based on tensor T , each vector ?i,j and ?i,j consists
of m elements, where computation of each element
requires timeO(m2). Therefore, the algorithm has a
O(m3) multiplicative factor in its time complexity,
which we aim to reduce by means of an approximate
algorithm.
Our approximate method relies on a simple ob-
servation. Given an integer r ? 1, assume that
the tensor T has the following special form, called
?Kruskal form:?
T =
r?
i=1
?iuiv>i w
>
i . (1)
In words, T is the sum of r tensors, where each
tensor is obtained as the product of three vectors
ui, vi and wi, together with a scalar ?i. Exact
Kruskal decomposition of a tensor is not necessarily
unique. See Kolda and Bader (2009) for discussion
of uniqueness of tensor decomposition.
490
Consider now two vectors y1, y2 ? Rm, associ-
ated with the inside probabilities for the left (y1) and
right child (y2) of a given node in a parse tree. Let
us introduce auxiliary arrays U, V,W ? Rr?m, with
the i-th row being ui, vi and wi, respectively. Let
also ? = (?1, . . . , ?r). Using the decomposition in
Eq. (1) within Definition 1 we can express the array
T (y1, y2) as:
T (y1, y2) =
[
r?
i=1
?iuiv>i w
>
i
]
(y1, y2) =
r?
i=1
?iui(v>i y
1)(w>i y
2) =
(
U>(? V y1 Wy2)
)
. (2)
The total complexity of the computation in Eq. (2)
is nowO(rm). It is well-known that an exact tensor
decomposition for T can be achieved with r = m2
(Kruskal, 1989). In this case, there is no computa-
tional gain in using Eq. (2) for the inside calculation.
The minimal r required for an exact tensor decom-
position can be smaller than m2. However, identify-
ing that minimal r is NP-hard (H?astad, 1990).
In this section we focused on the computa-
tion of the inside probabilities through vectors
T (?i,k, ?k+1,j). Nonetheless, the steps above can
be easily adapted for the computation of the outside
probabilities through vectors T(1,2)(?k,j , ?k,i?1)
and T(1,3)(?i,k, ?j+1,k).
4.2 Approximate Tensor Decomposition
The PCFG tensor T will not necessarily have the ex-
act decomposed form in Eq. (1). We suggest to ap-
proximate the tensor T by finding the closest tensor
according to some norm over Rm?m?m.
An example of such an approximate decom-
position is the canonical polyadic decomposition
(CPD), also known as CANDECOMP/PARAFAC
decomposition (Carroll and Chang, 1970; Harsh-
man, 1970; Kolda and Bader, 2009). Given an in-
teger r, least squares CPD aims to find the nearest
tensor in Kruskal form, minimizing squared error.
More formally, for a given tensor D ? Rm?m?m,
let ||D||F =
??
i,j,kD
2
i,j,k. Let the set of tensors in
Kruskal form Cr be:
Cr ={C ? Rm?m?m | C =
r?
i=1
?iuiv>i w
>
i
s.t. ?i ? R, ui, vi, wi ? Rm,
||ui||2 = ||vi||2 = ||wi||2 = 1}.
The least squares CPD of C is a tensor C? such
that C? ? argminC??Cr ||C ? C?||F . Here, we treat
the argmin as a set because there could be multiple
solutions which achieve the same accuracy.
There are various algorithms to perform CPD,
such as alternating least squares, direct linear de-
composition, alternating trilinear decomposition and
pseudo alternating least squares (Faber et al, 2003)
and even algorithms designed for sparse tensors (Chi
and Kolda, 2011). Most of these algorithms treat
the problem of identifying the approximate tensor as
an optimization problem. Generally speaking, these
optimization problems are hard to solve, but they
work quite well in practice.
4.3 Parsing with Decomposed Tensors
Equipped with the notion of tensor decomposition,
we can now proceed with approximate tensor pars-
ing in two steps. The first is approximating the ten-
sor using a CPD algorithm, and the second is apply-
ing the algorithms in Figure 1 and Figure 2 to do
parsing, while substituting all tensor product com-
putations with the approximate O(rm) operation of
tensor product.
This is not sufficient to get a significant speed-up
in parsing time. Re-visiting Eq. (2) shows that there
are additional ways to speed-up the tensor applica-
tion T in the context of the inside-outside algorithm.
The first thing to note is that the projections V y1
and Wy2 in Eq. (2) can be cached, and do not have
to be re-calculated every time the tensor is applied.
Here, y1 and y2 will always refer to an outside or
an inside probability vector over the nonterminals in
the grammar. Caching these projections means that
after each computation of an inside or outside proba-
bility, we can immediately project it to the necessary
r-dimensional space, and then re-use this computa-
tion in subsequent application of the tensor.
The second thing to note is that the U projection
in T can be delayed, because of rule of distributiv-
ity. For example, the step in Figure 2 that computes
491
the inside probability ?i,j can be re-formulated as
follows (assuming an exact decomposition of T ):
?i,j =
j?1?
k=i
T (?i,k, ?k+1,j)
=
j?1?
k=1
U>(? V ?i,k W?k+1,j)
= U>
(j?1?
k=1
(? V ?i,k W?k+1,j)
)
. (3)
This means that projection through U can be done
outside of the loop over splitting points in the sen-
tence. Similar reliance on distributivity can be used
to speed-up the outside calculations as well.
The caching speed-up and the delayed projection
speed-up make the approximate inside-outside com-
putation asymptotically faster. While na??ve applica-
tion of the tensor yields an inside algorithm which
runs in time O(rmN3), the improved algorithm
runs in time O(rN3 + rmN2).
5 Quality of Approximate Tensor Parsing
In this section, we give the main approximation re-
sult, that shows that the probability distribution in-
duced by the approximate tensor is close to the orig-
inal probability distribution, if the distance between
the approximate tensor and the rule probabilities is
not too large.
Denote by T (N) the set of trees in the tree lan-
guage of the PCFG with N words (any nontermi-
nal can be the root of the tree). Let T (N) be the
set of pairs of trees ? = (?1, ?2) such that the to-
tal number of binary rules combined in ?1 and ?2 is
N ? 2 (this means that the total number of words
combined is N ). Let T? be the approximate ten-
sor for T . Denote the probability distribution in-
duced by T? by p?.1 Define the vector ?(?) such that
[?(?)]a = Ta,b,c ? p(?1 | b) ? p(?2 | c) where the root
?1 is nonterminal b and the root of ?2 is c. Similarly,
define [??(?)]a = T?a,b,c ? p?(?1 | b) ? p?(?2 | c).
Define Z(a,N) =
?
??T (N)[??(?)]a. In addition,
define D(a,N) =
?
??T (N) |[??(?)]a ? [?(?)]a|
1Here, p? does not have to be a distribution, because T? could
have negative values, in principle, and its slices do not have to
normalize to 1. However, we just treat p? as a function that maps
trees to products of values according to T? .
and define F (a,N) = D(a,N)/Z(a,N). De-
fine ? = ||T? ? T ||F . Last, define ? =
min(a?b c)?R p(a? b c | a). Then, the following
lemma holds:
Lemma 1 For any a and any N , it holds:
D(a,N) ? Z(a,N)
(
(1 + ?/?)N ? 1
)
Proof sketch: The proof is by induction on N .
Assuming that 1 + F (b, k) ? (1 + ?/?)k and
1 + F (c,N ? k ? 1) ? (1 + ?/?)N?k?1 for F
defined as above (this is the induction hypothesis), it
can be shown that the lemma holds. 
Lemma 2 The following holds for any N :
?
??T (N)
|p?(?)? p(?)| ? m
(
(1 + ?/?)N ? 1
)
Proof sketch: Using Ho?lder?s inequality and
Lemma 1 and the fact that Z(a,N) ? 1, it follows
that:
?
??T (N)
|p?(?)? p(?)| ?
?
??T (N),a
|[?(?)]a ? [??(?)]a|
?
(
?
a
Z(a,N)
)
(
(1 + ?/?)N ? 1
)
? m
(
(1 + ?/?)N ? 1
)

Then, the following is a result that explains how
accuracy changes as a function of the quality of the
tensor approximation:
Theorem 1 For anyN , and  < 1/4, it holds that if
? ?
?
2Nm
, then:
?
??T (N)
|p?(?)? p(?)| ? 
Proof sketch: This is the result of applying Lemma 2
together with the inequality (1 + y/t)t? 1 ? 2y for
any t > 0 and y ? 1/2. 
492
We note that Theorem 1 also implicitly bounds
the difference between a marginal ?(a, i, j) and its
approximate version. A marginal corresponds to a
sum over a subset of summands in Eq. (1).
A question that remains at this point is to decide
whether for a given grammar, the optimal ? that can
be achieved is large or small. We define:
??r = min
T??Cr
||T ? T? ||F (4)
The following theorem gives an upper bound on
the value of ??r based on intrinsic property of the
grammar, or more specifically T . It relies on the
fact that for three-dimensional tensors, where each
dimension is of length m, there exists an exact de-
composition of T using m2 components.
Theorem 2 Let:
T =
m2?
i=1
??iu
?
i (v
?
i )
>(w?i )
>
be an exact Kruskal decomposition of T such that
||u?i ||2 = ||v
?
i ||2 = ||w
?
i || = 1 and ?
?
i ? ?
?
i+1 for
i ? [m2 ? 1]. Then, for a given r, it holds:
??r ?
m2?
i=r+1
|??i |
Proof: Let T? be a tensor that achieves the minimum
in Eq. (4). Define:
T ?r =
r?
i=1
??iu
?
i (v
?
i )
>(w?i )
>
Then, noting that ??r is a minimizer of the norm
difference between T and T? and then applying the
triangle inequality and then Cauchy-Schwartz in-
equality leads to the following chain of inequalities:
??r = ||T ? T? ||F ? ||T ? T
?
r||F
= ||
m2?
i=r+1
??iu
?
i (v
?
i )
>(w?i )
>||F
?
m2?
i=r+1
|??i | ? ||u
?
i (v
?
i )
>(w?i )
>||F =
m2?
i=r+1
|??i |
as required. 
6 Experiments
In this section, we describe experiments that demon-
strate the trade-off between the accuracy of the ten-
sor approximation (and as a consequence, the accu-
racy of the approximate parsing algorithm) and pars-
ing time.
Experimental Setting We compare the tensor ap-
proximation parsing algorithm versus the vanilla
Goodman algorithm. Both algorithms were imple-
mented in Java, and the code for both is almost iden-
tical, except for the set of instructions which com-
putes the dynamic programming equation for prop-
agating the beliefs up in the tree. This makes the
clocktime comparison reliable for drawing conclu-
sions about the speed of the algorithms. Our im-
plementation of the vanilla parsing algorithm is lin-
ear in the size of the grammar (and not cubic in the
number of nonterminals, which would give a worse
running time).
In our experiments, we use the method described
in Chi and Kolda (2011) for tensor decomposition.2
This method is fast, even for large tensors, as long
as they are sparse. Such is the case with the tensors
for our grammars.
We use two treebanks for our comparison: the
Penn treebank (Marcus et al, 1993) and the Arabic
treebank (Maamouri et al, 2004). With the Penn
treebank, we use sections 2?21 for training a max-
imum likelihood model and section 22 for parsing,
while for the Arabic treebank we divide the data into
two sets, of size 80% and 20%, one is used for train-
ing a maximum likelihood model and the other is
used for parsing.
The number of binary rules in the treebank gram-
mar is 7,240. The number of nonterminals is 112
and the number of preterminals is 2593Unary rules
are removed by collapsing non-terminal chains. This
increased the number of preterminals. The number
of binary rules in the Arabic treebank is significantly
smaller and consists of 232 rules. We run all parsing
experiments on sentences of length ? 40. The num-
ber of nonterminals is 48 and the number of preter-
2We use the implementation given in Sandia?s Mat-
lab Tensor Toolbox, which can be downloaded at http:
//www.sandia.gov/?tgkolda/TensorToolbox/
index-2.5.html.
3.
493
rank (r) baseline 20 60 100 140 180 220 260 300 340
Ara
bic speed 0.57 0.04 0.06 0.1 0.12 0.16 0.19 0.22 0.26 0.28
F1 63.78 51.80 58.39 63.63 63.77 63.88 63.82 63.84 63.80 63.88
Eng
lish speed 3.89 0.15 0.21 0.30 0.37 0.44 0.52 0.60 0.70 0.79
F1 71.07 57.83 61.67 68.28 69.63 70.30 70.82 71.42 71.28 71.13
Table 1: Results for the Arabic and English treebank of parsing using a vanilla PCFG with and without tensor decom-
position. Speed is given in seconds per sentence.
minals is 81.
Results Table 1 describes the results of compar-
ing the tensor decomposition algorithm to the vanilla
PCFG parsing algorithm.
The first thing to note is that the running time of
the parsing algorithm is linear in r. This indeed
validates the asymptotic complexity of the inside-
outside component in Goodman?s algorithm with the
approximate tensors. It also shows that most of the
time during parsing is spent on the inside-outside al-
gorithm, and not on the dynamic programming algo-
rithm which follows it.
In addition, compared to the baseline which uses
a vanilla CKY algorithm (linear in the number of
rules), we get a speed up of a factor of 4.75 for
Arabic (r = 140) and 6.5 for English (r = 260)
while retaining similar performance. Perhaps more
surprising is that using the tensor approximation ac-
tually improves performance in several cases. We
hypothesize that the cause of this is that the tensor
decomposition requires less parameters to express
the rule probabilities in the grammar, and therefore
leads to better generalization than a vanilla maxi-
mum likelihood estimate.
We include results for a more complex model for
Arabic, which uses horizontal Markovization of or-
der 1 and vertical Markovization of order 2 (Klein
and Manning, 2003). This grammar includes 2,188
binary rules. Parsing exhaustively using this gram-
mar takes 1.30 seconds per sentence (on average)
with an F1 measure of 64.43. Parsing with tensor
decomposition for r = 280 takes 0.62 seconds per
sentence (on average) with an F1 measure of 64.05.
7 Discussion
In this section, we briefly touch on several other top-
ics related to tensor approximation.
7.1 Approximating the Probability of a String
The probability of a sentence z under a PCFG is de-
fined as p(z) =
?
??T (z) p(?), and can be approx-
imated using the algorithm in Section 4.3, running
in time O(rN3 + rmN2). Of theoretical interest,
we discuss here a time O(rN3 + r2N2) algorithm,
which is more convenient when r < m.
Observe that in Eq. (3) vector ?i,j always appears
within one of the two terms V ?i,j and W?i,j in
Rr?1, whose dimensions are independent of m.
We can therefore use Eq. (3) to compute V ?i,j as
V ?i,j = V U>
(?j?1
k=1(? V ?
i,k W?k+1,j)
)
,
where V U> is a Rr?r matrix that can be
computed off-line, i.e., independently of
z. A symmetrical relation can be used
to compute W?i,j . Finally, we can write
p(z) = pi>U
(?N?1
k=1 (? V ?
1,k W?k+1,N )
)
,
where pi>U is a R1?r vector that can again be
computed off-line. This algorithm then runs in time
O(rN3 + r2N2).
7.2 Applications to Dynamic Programming
The approximation method presented in this paper is
not limited to PCFG parsing. A similar approxima-
tion method has been used for latent-variable PCFGs
(Cohen and Collins, 2012), and in general, ten-
sor approximation can be used to speed-up inside-
outside algorithms for general dynamic program-
ming algorithms or weighted logic programs (Eisner
et al, 2004; Cohen et al, 2011). In the general case,
the dimension of the tensors will not be necessarily
just three (corresponding to binary rules), but can be
of a higher dimension, and therefore the speed gain
can be even greater. In addition, tensor approxima-
tion can be used for computing marginals of latent
variables in graphical models.
For example, the complexity of the forward-
494
backward algorithm for HMMs can be reduced to
be linear in the number of states (as opposed to
quadratic) and linear in the rank used in an approxi-
mate singular-value decomposition (instead of ten-
sor decomposition) of the transition and emission
matrices.
7.3 Tighter (but Slower) Approximation Using
Singular Value Decomposition
The accuracy of the algorithm depends on the ability
of the tensor decomposition algorithm to decompose
the tensor with a small reconstruction error. The de-
composition algorithm is performed on the tensor T
which includes all rules in the grammar.
Instead, one can approach the approximation by
doing a decomposition for each slice of T separately
using singular value decomposition. This will lead
to a more accurate approximation, but will also lead
to an extra factor of m during parsing. This factor
is added because now there is not a single U , V and
W , but instead there are such matrices for each non-
terminal in the grammar.
8 Conclusion
We described an approximation algorithm for prob-
abilistic context-free parsing. The approximation al-
gorithm is based on tensor decomposition performed
on the underlying rule table of the CFG grammar.
The approximation algorithm leads to significant
speed-up in PCFG parsing, with minimal loss in per-
formance.
References
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-
ishman, P Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Proceedings of DARPA Work-
shop on Speech and Natural Language.
P. Boullier. 2003. Guided earley parsing. In 8th In-
ternational Workshop on Parsing Technologies, pages
43?54.
J. D. Carroll and J. J. Chang. 1970. Analysis of indi-
vidual differences in multidimensional scaling via an
N-way generalization of Eckart-Young decomposition.
Psychometrika, 35:283?319.
E. Charniak, M. Johnson, M. Elsner, J. Austerweil,
D. Ellis, I. Haxton, C. Hill, R. Shrivaths, J. Moore,
M. Pozar, and T. Vu. 2006. Multilevel coarse-to-fine
pcfg parsing. In Proceedings of HLT-NAACL.
E. C. Chi and T. G. Kolda. 2011. On tensors, spar-
sity, and nonnegative factorizations. arXiv:1112.2414
[math.NA], December.
S. B. Cohen and M. Collins. 2012. Tensor decomposi-
tion for fast latent-variable PCFG parsing. In Proceed-
ings of NIPS.
S. B. Cohen, R. J. Simmons, and N. A. Smith. 2011.
Products of weighted logic programs. Theory and
Practice of Logic Programming, 11(2?3):263?296.
S. B. Cohen, K. Stratos, M. Collins, D. F. Foster, and
L. Ungar. 2012. Spectral learning of latent-variable
PCFGs. In Proceedings of ACL.
J. Eisner and N. A. Smith. 2005. Parsing with soft and
hard constraints on dependency length. In Proceed-
ings of IWPT, Parsing ?05.
J. Eisner, E. Goldlust, and N. A. Smith. 2004. Dyna: A
declarative language for implementing dynamic pro-
grams. In Proc. of ACL (companion volume).
N. M. Faber, R. Bro, and P. Hopke. 2003. Recent devel-
opments in CANDECOMP/PARAFAC algorithms: a
critical review. Chemometrics and Intelligent Labora-
tory Systems, 65(1):119?137.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of ACL.
R. A. Harshman. 1970. Foundations of the PARAFAC
procedure: Models and conditions for an ?explana-
tory? multi-modal factor analysis. UCLA working pa-
pers in phoentics, 16:1?84.
J. H?astad. 1990. Tensor rank is NP-complete. Algo-
rithms, 11:644?654.
H. Jaeger. 2000. Observable operator models for dis-
crete stochastic time series. Neural Computation,
12(6):1371?1398.
D. Klein and C. D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of ACL.
T. G. Kolda and B. W. Bader. 2009. Tensor decomposi-
tions and applications. SIAM Rev., 51:455?500.
J. B. Kruskal. 1989. Rank, decomposition, and unique-
ness for 3-way and N-way arrays. In R. Coppi and
S. Bolasco, editors, Multiway Data Analysis, pages 7?
18.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki.
2004. The Penn Arabic Treebank: Building a large-
scale annotated Arabic corpus. In Proceedings NEM-
LAR.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19(2):313?330.
495
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic CFG with latent annotations. In Proceedings
of ACL.
M.-J. Nederhof. 2000. Practical experiments with regu-
lar approximation of context-free languages. Compu-
tational Linguistics, 26(1):17?44.
496
