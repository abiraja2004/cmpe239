Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 15?22,
Prague, June 2007. c?2007 Association for Computational Linguistics
Bayesian Identiation of Cognates and Correspondenes
T. Mark Ellison
Linguistis, University of Western Australia,
and Analith Ltd
markmarkellison.net
Abstrat
This paper presents a Bayesian approah
to omparing languages: identifying og-
nates and the regular orrespondenes
that ompose them. A simple model of
language is extended to inlude these no-
tions in an aount of parent languages.
An expression is developed for the pos-
terior probability of hild language forms
given a parent language. Bayes' Theo-
rem oers a shema for evaluating hoies
of ognates and orrespondenes to ex-
plain semantially mathed data. An im-
plementation optimising this value with
gradient desent is shown to distinguish
ognates from non-ognates in data from
Polish and Russian.
Modern historial linguistis addresses ques-
tions like the following. How did language
originate? What were historially-reorded lan-
guages like? How related are languages? What
were the anestors of modern languages like?
Reently, omputation has beome a key tool in
addressing suh questions.
Kirby (2002) gives an overview of urrent ur-
rent work on how language evolved, muh of it
based on omputational models and simulations.
Ellison (1992) presents a linguistially motivated
method for lassifying onsonants as onsonants
or vowels. An unexpeted result for the dead
language Gothi provides added weight to one
of two ompeting phonologial interpretations of
the orthography of this dead language.
Other reent work has applied omputational
methods for phylogenetis to measuring linguis-
ti distanes, and/or onstruting taxonomi
trees from distanes between languages and di-
alets (Dyen et al, 1992; Ringe et al, 2002; Gray
and Atkinson, 2003; MMahon and MMahon,
2003; Nakleh et al, 2005; Ellison and Kirby,
2006).
A entral fous of historial linguistis is the
reonstrution of parent languages from the ev-
idene of their desendents. In historial lin-
guistis proper, this is done by the ompara-
tive method (Jeers and Lehiste, 1989; Hok,
1991) in whih shared arbitrary struture is as-
sumed to reet ommon origin. At the phono-
logial level, reonstrution identies ognates
and orrespondenes, and then onstruts sound
hanges whih explain them.
This paper presents a Bayesian approah to
assessing ognates and orrespondenes. Best
sets of ognates and orrespondenes an then
be identied by gradient asent on this evalua-
tion measure. While the work is motivated by
the eventual goal of oering software solutions
to historial linguistis, it also hopes to show
that Bayes' theorem applied to an expliit, sim-
ple model of language an lead to a prinipled
and tratable method for identifying ognates.
The struture of the paper is as follows. The
next setion details the notions of historial lin-
guistis needed for this paper. Setion 2 for-
mally denes a model of language and parent
language. The subsequent setion situates the
work amongst similar work in the literature,
15
making use of onepts desribed in the earlier
setions. Setion 4 desribes the alulation of
the probability of wordlist data given a hypoth-
esised parent language. This is ombined with
Bayes' theorem and gradient searh in an algo-
rithm to nd the best parent language for the
data. Setion 5 desribes the results of apply-
ing an implementation of the algorithm to data
from Polish and Russian. The nal setion sum-
marises the paper and suggests further work.
1 Cognates, Correspondenes and
Reonstrution
In the neo-Grammarian model of language
hange, a population speaking a uniform lan-
guage divides, and then the two populations un-
dergo separate language hanges.
Word forms with ontinuous histories in re-
spetive daughter languages desending whih
from a ommon word-form anestor are alled
ognate, no matter what has happened to their
semantis. Cognate word forms may have un-
dergone deformations to make them less simi-
lar to eah other, these deformations resulting
from regular, phonologial hanges. Note that
in the elds of applied linguistis, seond lan-
guage aquisition, and mahine translation, the
term ognate is used to mean any words that are
phonologially similar to eah other. This is not
the sense meant here.
Phonologial hange produes modiations
to the segmental inventory, replaing one seg-
ment by another in all or only some ontexts.
This sometimes has the eet of ollapsing seg-
ment types together. Other hanges may di-
vide one segment type into two, depending on
a ontextual ondition. The relation of parent-
language segments to daughter-language seg-
ments is, usually, a many-to-many relation.
Parent-hild segmental relations are reeted
in the orrespondenes between segment in-
ventories in the daughter languages. Cor-
respondenes are pairings of segments from
daughter languages whih have derived from
a ommon parent segment. For example, p
in Latin frequently orresponds to f in En-
glish, as in words like pater and father. Both
segments have developed from a (postulated)
Proto-IndoEuropean *p. Beause orrespon-
denes only our between ognates, identify-
ing the two is often a bootstrap proess: or-
raling ognates helps nd more orrespondenes,
and forms sharing a number orrespondenes are
probably ognate.
2 Formal Strutures
The method presented in this paper is based on
a formal model of language. This is desribed in
setion 2.1. The subsequent setion extends the
model to dene a parent language, whose seg-
mental inventory is orrespondenes and whose
lexion is ognates linking two desendent lan-
guages.
2.1 Language model
The language model is based on three assump-
tions.
Assumption 1 There is a universal, disrete
set M of meanings.
Assumption 2 A language L has its own set of
segments ?(L).
Assumption 3 The lexion ? of a language L is
a partial map of meanings to strings of segments
? : M ? ?(L)?.
On the basis of these assumptions, we an de-
ne a language L to be a triple (M,?(L), ?(L))
of meanings, segments and mappings from mean-
ings onto strings of segments.
For example, onsider written Polish. The
set of meanings ontains onepts as to take-
perfet-innitive, tree-nominative-singular,
and so on. The segmental inventory ontains
the 32 segments a a b   d e e f g h i j k l
 l m n n o o p r s s t u w y z 
z z, ignoring
apitalisation. The lexion mathes meanings to
strings of segments, to take-perfet-innitive
to wzia, tree-nominative-singular to drzewo.
2.2 Parent language model
Denition 1 A degree-(u, v) orrespondene
between L1 and L2 is a pair of strings (s, t) ?
?(L1) ? ?(L2) over the segments of L1 and L2
16
respetively, with lengths at least u and no more
than v.
As an example of a orrespondene, onsider
the pair of small strings from Polish and Russian,
(,??). This is a degree-(1, 2) orrespondene
beause its members have lengths as low as one
and as high as two. It is also a degree-(u, v)
orrespondene for any u ? 1 and v ? 2.
Any orrespondene an be mapped onto its
omponents by projetion funtions.
Denition 2 The projetions pi1 and pi2 map
a orrespondene (s, t) onto its rst pi1(s, t) = s
or seond pi2(s, t) = t omponent string respe-
tively.
The rst projetion funtion will map (,??)
onto , while the seond maps (,??) onto ??.
Correspondenes an be formed into strings.
These strings also have projetions.
Denition 3 The projetions pi1 and pi2 map
a string of orrespondenes c1..ck onto the on-
atenation of the projetions of eah orrespon-
dene.
pi1(c1..ck) = pi1(c1)pi1(c2)..pi1(ck),
pi2(c1..ck) = pi2(c1)pi2(c2)..pi2(ck)
Suppose we sequene four orrespondenes
into the string (w,?)(z,?)(ia,?)(,??). This
string has rst and seond projetions, wzia
and ?????, formed by onatenating the respe-
tive projetions of eah orrespondene.
We an now dene a parent language.
Denition 4 A degree-(u, v) parent L0 of two
languages L1, L2 is a triple (M,?(L0), ?(L0))
where ?(L0) is a set of degree-(u, v) orrespon-
denes between L1 and L2, exluding the pair of
null strings, and ?(L0) is a partial mapping from
M onto ?(L0) whih obeys
pi1 ? ?(L0) ? ?(L1), pi2 ? ?(L0) ? ?(L2)
The irle stands for funtion omposition.
Continuing our past example, we will fous
on the two meanings to take-perfet-innitive
and tree-nominative-singular. The segment in-
ventory for the parent language ontains degree-
(0, 2) orrespondenes: (,?), (,??), (d,?),
(e,?), (ia,?), (o,?), (rz,?), (w,?), (z,?). The
lexial funtion maps to take-perfet-innitive
onto the string of orrespondenes (w,?) (z,?)
(ia,?) (,??) while tree-nominative-singular
maps to (d,?) (,?) (rz,?) (e,?) (w,?) (o,?).
The parent language ondition is veried by
heking the projetions of the two orrespon-
dene strings. The rst string has proje-
tions wzia and ?????, whih are forms for
the meaning to take-perfet-innitive in Pol-
ish and Russian respetively. The seond string
has projetions drzewo and ??????, whih are
forms for the meaning tree-nominative-singular
in Polish and Russian respetively. So the pro-
jetion ondition is satised. If the lexial fun-
tion is only dened on these two meanings, then
this is a valid parent language.
It is worth emphasising that the projetion
ondition for qualifying as a parent language ap-
plies only for those meanings for whih the par-
ent lexial mapping is dened. The orrespond-
ing forms in the hild languages are said to be
ognate in this model. Where no parent form
is reonstruted, the forms are not ognate, and
are to be aounted for in some way other than
the parent language.
3 Related Work
The urrent work is, of ourse, far from the rst
to seek to identify ognates and/or orrespon-
denes. Here is an abbreviated overview of pre-
vious work in the eld
1
. More detailed surveys
an be found in hapter 3 of Kondrak's (2002)
PhD thesis or Lowe's online survey
2
of prior art
in this eld.
In perhaps the rst omputational work on
historial linguistis, Kay (1964) desribed an al-
gorithm for determining orrespondenes given
a list of ognate pairs aross two daughter lan-
guages. His method seeks to nd the smallest set
1
An anonymous reviewer suggests that the urrent
work shares features with that of Kessler (2001). I have
been unable to aess this book in time to inlude dis-
ussion of it in this paper.
2
linguistis.berkeley.edu/? jblowe/REWWW/PriorArt.html
17
of orrespondenes whih allows a degree-(1,?)
alignment for eah ognate pair. Unfortunately,
the omplexity of the problem has preluded its
appliation to signiant daa sets.
Frantz (1970) developed a PL/1 programming
whih returned numerial evaluations of orre-
spondenes and ognay, given a list of possi-
ble ognate word-pairs. Eah word pair must be
supplied as a degree-(0, 1) reonstrution, that
is, aligning single segments with eah other or
with gaps.
Guy (1984; 1994) presented a program alled
COGNATE whih nds regular orrespondenes
and identies ognates using statistial teh-
niques.
For his Master's, Broza (1998) developed
MDL-based software alled andid whih identi-
es orrespondenes from ognates and expresses
these as ontextual phonologial transformation
rules.
Kondrak's (2002) dotoral dissertation om-
bines phonologial and semanti similarity meth-
ods with orrespondane-learning. The algo-
rithms for learning orrespondenes are taken
from Melamed's (2000) probabilisti methods
for identifying word-word translation equiva-
lene. These methods, like the urrent work,
are Bayesian. Beause Melamed's problem seeks
partial rather than omplete explanation of the
inputs in terms of orrespondenes, the math-
ing problem is somewhat more diult theoret-
ially. As a result, he does not arrive at the de-
omposition of the sum of the probability of two
inputs given the set of possible orrespondenes,
approximating this with a high probability align-
ment.
4 Conditional Probability of the
Data
The ore of any Bayesian model is the ondi-
tional probability of the data given the hypoth-
esis. This setion details how probabilities as-
signed to data, and the assumptions on whih
this assignment is based.
The data is the mapping of meanings onto
forms in two daughter languages. If those two
languages are L1 and L2, we want to determine
P (?(L1), ?(L2)|h). The nature of h will be dis-
ussed in setion 4.6.
For brevity, we will write ?i for ?(Li).
4.1 Meaning independene
The rst step in dening the onditional prob-
ability of the data is to deompose it into
meaning-by-meaning probabilities. This an be
ahieved by adopting the following two assump-
tions.
Assumption 4 In a given language, the forms
for dierent meanings are seleted indepen-
dently.
This assumption states that within a single
language hoosing, for example, a form wzia
for meaning to take-perfet-innitive is no help
in prediting the form whih expresses tree-
nominative-singular.
Assumption 5 Aross dierent languages, the
forms orresponding to dierent meanings are
independent.
Aording to this assumption, the Polish word
wzia and the Russian word ????? an be
struturally dependent beause they express the
same meaning. In ontrast, we an only ex-
pet a hane relationship between the Rus-
sian word ????? meaning to take-perfet-
innitive, and the Polish word drzewo express-
ing tree-nominative-singular.
Together, these two assumptions imply that
the only dependenies possible between any four
forms expressing the two meanings m1 and m2 in
two languages L1 and L2 are between ?(m1) and
?(m1) on the one hand and ?(m2) and ?(m2) on
the other.
Consequently the probability of generating the
word forms in two languages an be deomposed
into the produt of generating the two language-
partiular forms for eah meaning.
P (?1, ?2|h) =
?
m?M
P (?1(m), ?2(m)|h)
18
4.2 Cognay and independene
The next assumption holds that strutural or-
relation between orresponding forms should be
explained as resulting from ognay.
Assumption 6 Aross dierent languages,
forms orresponding to the same meaning are
dependent only if the forms are ognate.
If the words for a partiular meaning do not
derive from a ommon anestral form, then they
are unorrelated. To return to our Polish and
Russian examples, we an expet dependenies
in struture between the ognate words drzewo
and ??????. But we should expet no suh or-
relation in the non-ognate pair pomaranza
and ???????? meaning orange-nominative-
singular.
Let us write Mi for the domain of the lexial
funtion in language Li. This is the set of mean-
ings for whih this language has dened a word
form. The set of ognates is the domain of the
lexial funtion of the parent language, M0. We
an deompose the evidential words into three
sets: M0 of ognates, M1 \M0 of meanings only
expressed in language L1, and M2 \M0 of mean-
ings only expressed in language L2. Words in the
seond and third ategories are non-ognate, and
so probabilistially independent of eah other.
The onditional probability of the data an
thus be expressed as follows.
P (?1, ?2|h) =
?
m?M0
P (?1(m), ?2(m)|h)
?
m?M1\M0
P (?1(m)|h)
?
m?M2\M0
P (?2(m)|h)
4.3 Probability of a word
We now turn to the probability of generating a
string in a language. The rst assumption de-
nes the distribution over word-length.
Assumption 7 The probability of a word hav-
ing a partiular length is negative exponential in
that length.
The seond assumption allows segment prob-
ability to depend only on the segment identity,
and not on its neighbourhood.
Assumption 8 Segment hoie is ontext-
independent.
These two assumptions together imply that
the probability of strings is determined by a xed
distribution over ?(Li) ? {#}, where # is an
end-of-word marker. For the desendent lan-
guages, this distribution an be taken as the rela-
tive frequenies of the segments and end-of-word
marker. Denote this distribution for language Li
by fi.
The probability of generating a word in a lan-
guage, given relative frequenies fi, is the prod-
ut of the relative frequenies for eah lettern in
the word, multiplied by the relative frequeny of
the end-of-word marker.
P (?i(m)|h) = fi(#)
?
a??i(m)
fi(a)
Note that this expression only holds for words
that are independent of all others, suh as om-
ponents of non-ognate pairs.
4.4 Probability of generating a ognate
pair
The probability of generating a ognate pair of
words is similar to the above, beause desen-
dent forms are deterministially derivable from
the parent forms. If (?1(m), ?2(m)) are a pair of
ognates derived from an anestral form ?0(m),
then there is unit probability that the desen-
dent forms are what they are given the parent:
P (?1(m), ?2(m)|?0(m)) = 1.
Sine a ognate pair is derivable from a par-
ent form, the probability of a ognate pair is
the sum of the probabilities of all parent forms
whih will generate the two desendents. Write
W (m) = W (?1(m), ?2(m)) for the set of pos-
sible orrespondene strings in the parent whih
projet onto wordforms ?1(m) and ?2(m). Then
the probability of the word pair is given by:
P (?1(m), ?2(m)|h) =
?
s?W (m)
P (?0(m) = s|h)
The summation poses a slight problem, however.
How do we sum over all possible strings with
given projetions? Fortunately, we an deom-
pose the summation. Start by reognising that
19
the parent language is also a language, and so
the probability of forms in the language is de-
termined by a distribution over segments  in
this ase orrespondenes  and the end-of-word
marker. For onsisteny, we all this distribution
f0.
The only parent form whih projets onto two
empty strings is the empty string, onsisting
only of the end-of-word marker. For brevity,
we will drop the lambdas, writing P (x, y|h) for
P (?1(m) = x, ?2(m) = y|h)
P (0, 0|h) = f0(#)
We assume, without loss of generality, that
the segmental inventory of the parent language
onsists of all degree-(u, v) orrespondenes be-
tween L1 and L2. Parent segments whih are
never used an be exluded by giving them zero
relative frequeny in f0.
The funtion Pre(s;u, v) returns the set of bi-
nary divisions (a, b) of the string s, suh that the
length of the rst part a is at least u and at most
v.
Pre(s;u, v) = {(a, b)|ab = s,m ? |a| ? n}
With this funtion, we an reursively dene a
funtion W (s, t;u, v) on pairs of strings (s, t)
whih returns the set of all degree-(u, v) parent
language strings whih projet onto s and t. For
brevity, we will treat all u, v arguments as im-
pliit.
W (0, 0) = {0}
By denition, the only parent language string
whih an map onto the empty string in both
desendents is the empty string.
The reursive step breaks the strings s and
t into all possible prexes a and c respetively.
The orrespondene (a, c) is then preposed on all
strings returned by W when it is applied to the
remainders of s and t.
W (s, t) =
?
(a,b)?Pre(s)
?
(c,d)?Pre(t)
(a, c)W (b, d)
Note that this is the set W (m) we dened earlier.
W (m) = W (?1(m), ?2(m);u, v)
The reursive denition of W in terms of dis-
joint unions and onatenation an be trans-
formed into a reursive denition for the proba-
bility P0(s, t|h) of onstruting a member of the
set. Disjoint union is replaed by summation,
onatenation by produt. The probability of
an individual orrespondene (a, c) is its (un-
known) relative frequeny f0(a, c) in the parent
language. One again, we hide the impliit u, v
parameters.
P0(0, 0|h) = f0(#)
P0(s, t|h) =
?
(a,b)?Pre(s)
?
(c,d)?Pre(t)
f0(a, c)P (b, d|h)
4.5 Probability of a form-pair
We now have the piees to speify the probabil-
ity of nding any partiular form as the form-
pair for the desendent languages. The prob-
ability of the pair in the ase of ognay is
P0(?1(m), ?2(m)|h). If the pair are not ognate,
then they are independent, and their probabil-
ity is P1(?1(m))P2(?2(m)|h). If we write c(m|h)
for the likelihood that the pair is ognate, we
an ombine these two values to given a total
probability of the two forms.
P0(?1(m), ?2(m)|h)c(m|h)
+P1(?1(m))P2(?2(m)|h)(1.0 ? c(m|h))
Beause the word-pairs are independent (as-
sumption 4), the produt of the above probabil-
ity for eah meaning m gives the probability of
the data given the hypothesis.
4.6 Hypothesis
One burning question remains, however. What
is the hypothesis? The simple answer is that it
is exatly those free variables in the speiation
of the probability of the data
There were two groups of unknowns in the
probability of the data. The rst is the rela-
tive frequeny f0 assigned to orrespondenes in
parent-language forms. The seond is the like-
lihood of ognay c, a vetor of values between
zero and one indexed by meanings.
A hypothesis is therefore any setting of values
for the pair of vetors (f, c).
20
Note that while the degree variables u, v were
not xed in the above derivation, they will be
held onstant for any partiular searh, and thus
do not dene a dimension in the hypothesis
spae.
4.7 Searh
In this setion, we have derived P (D|h), the like-
lihood of our data given a hypothesis.
For simpliity, we hoose a at prior over hy-
potheses, rendering the MAP Bayesian approah
an instane of maximum likelihood determina-
tion. The value for the likelihood is dierentiable
in eah of the parameters. Consequently, gradi-
ent desent an be used to nd the hypothesis
whih maximises the probability of the data.
5 Results
In onstruting the method, we made a number
of assumptions about independene of forms. It
is sensible that for testing, the method is applied
to data that onforms reasonably well to these
assumptions. The alternative is to apply it to
data whih ontradits its fundamental assump-
tions, onsequently hampering its eetiveness.
5.1 The data
Polish and Russian were hosen to provide the
data beause they approximately obey assump-
tion 6: words have dependent strutures if and
only if they are ognate. For our two lan-
guages, this means that borrowings from om-
mon soures are unommon (numbering 45 in
our data set), at least in omparison with the
number of ognates (numbering 156).
The data was harvested from two online
ditionaries (Wordgumbo, 2007a; Wordgumbo,
2007b), one English-Polish, the other English-
Russian. Multiple translations were simplied,
with the shortest translation retained. The En-
glish glosses were used as the meanings for the
words. Where the gloss ontained a apital let-
ter, indiating a proper noun, this was elimi-
nated from the data.
The data should also onform to assumption
4, that words for dierent meanings with a lan-
guage are independent. So where two meanings
in the data sets were realised with the same form,
these meanings were deemed to be struturally
dependent, and so only the rst was retained in
the wordlist.
The remaining data ontains 407 aligned
Polish-Russian word pairs.
Polish and Russian both use a great deal of
derivational and inetional morphology. The
simple language model used here does not take
this into aount, so this will be a disturbing
inuene on the results.
5.2 Evaluation
The aligned wordlists were hand-tagged as og-
nate, ommon borrowing or non-ognate. A per-
missive rule of ognay was used: if the roots
of words in the two languages were ognate,
they were ognate, even if represented with non-
ognate derivational and/or inetional mor-
phology.
Figure 1 shows the evaluation of the program's
performane on the data.
Borrowings as: ognates non-ognates
Found f 162 119
Missed m 41 37
Errant e 6 49
Auray f/(f + e) 96% 71%
Reall f/(f + m) 81% 76%
Figure 1: Evaluation of program performane
on 407 meaning-mathed pairs of Polish-Russian
words. Common borrowings are sored as og-
nates in the rst olumn, non-ognates in the
seond.
The sores show that the method works well
in identifying ognates, partiularly if ommon
borrowings are aepted as ognates, or exluded
manually. If ommon borrowings are sored as
non-ognates, then the auray falls.
Of the orrespondenes found between Polish
and Russian, 67 have a phonologial basis. The
remaining 27 result from mismath morphology
in ognates or dierenes in ommon borrowings.
6 Conlusion
This paper has presented a model of language
whih allows the alulation of the posterior
probability of forms arising in the ases where
21
they are ognate, and where they are not. Bayes'
theorem relates these probabilities to the poste-
rior likelihood of partiular orrespondenes and
ognay relationships. Gradient desent an be
used to searh this spae for the best distribution
over orrespondenes, and best ognay evalua-
tions for meaning-paired words. The appliation
to data from Polish and Russian shows remark-
able suess identifying both ognates and non-
ognates.
Future work will proeed by relaxing on-
straints on the parent language. The parent in-
ventory will be widened to inlude multisegment
orrespondenes. Multiple parent languages will
be permitted, to the end of separating borrow-
ings from ognates. Finally, riher models of
language, inorporating syllable struture, will
allow more information to identify ognates.
Referenes
Gil Broza. 1998. Inter-language regularity: the
transformation learning problem. Master's thesis,
Institute of Computer Siene, Hebrew University
of Jerusalem, Otober.
Isidore Dyen, Joseph B. Kruskal, and Paul Blak.
1992. An Indo-European lassiation: a lexio-
statistial experiment. Transations of the Amer-
ian Philosophial Soiety, 82(5).
T. Mark Ellison and Simon Kirby. 2006. Measuring
language divergene by intra-lexial omparison.
In ACL, pages 273280, Sydney.
T. Mark Ellison. 1992. The Mahine Learning of
Phonologial Struture. Ph.D. thesis, University
of Western Australia.
Donald G. Frantz. 1970. A PL/1 program to assist
the omparative linguist. Communiations of the
ACM, 13(6):353356.
Russell D. Gray and Quentin D. Atkinson. 2003.
Language-tree divergene times support the ana-
tolian theory of indo-european origin. Nature,
426:435439.
Jaques B. M. Guy. 1984. An algorithm for identi-
fying ognates between related languages. In 10th
International Conferene on Computational Lin-
guistis and 22nd Annual Meeting of the Asso-
iation for Computational Linguistis. Available
online as http://al.ld.upenn.edu/P/P84/P84-
1091.pdf.
Jaques B. M. Guy. 1994. An algorithm for identi-
fying ognates in bilingual wordlists and its appli-
ability to mahine translation. Journal of Quan-
titative Linguistis, 1(1):3542.
Hans Heinrih Hok. 1991. Priniples of Historial
Linguistis. Mouton de Gruyter, Berlin.
Robert J. Jeers and Ilse Lehiste. 1989. Prini-
ples and Methods for Historial Linguistis. MIT
Press, Cambridge, MA.
Martin Kay. 1964. The logi of ognate reognition
in historial linguistis. Tehnial Report RM-
4224-PR, The RAND Corporation, Santa Monia,
CA, September.
Brett Kessler. 2001. The Signiane of Word Lists.
CSLI Publiations, Stanford, CA.
Simon Kirby. 2002. Natural language from artiial
life. Artiial Life, 8(2):185215.
Grzegorz Kondrak. 2002. Algorithms for Lan-
guage Reonstrution. Ph.D. thesis, University of
Toronto.
April MMahon and Robert MMahon. 2003. Find-
ing families: quantitative methods in language
lassiation. Transations of the Philologial So-
iety, 101:755.
I. Dan Melamed. 2000. Models of translational
equivalene among words. Computational Lin-
guistis, 26(2):221249.
Luay Nakleh, Tandy Warnow, Don Ringe, and
Steven N. Evans. 2005. A omparison of
phylogeneti reonstrution methods on an ie
dataset. Transations of the Philologial Soiety,
103(2):171192.
D. Ringe, Tandy Warnow, and A. Taylor.
2002. Indo-European and omputational ladis-
tis. Transations of the Philologial Soiety,
100(1):59129.
Wordgumbo. 2007a. /ie/sla/pol/erengpol.htm.
Website http://www.wordgumbo.om/.
Wordgumbo. 2007b. /ie/sla/rus/erengrus.htm.
Website http://www.wordgumbo.om/.
22
