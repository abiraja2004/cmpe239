First Joint Conference on Lexical and Computational Semantics (*SEM), pages 449?453,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Soft Cardinality: A Parameterized Similarity Function for Text Comparison
Sergio Jimenez
Universidad Nacional
de Colombia, Bogota,
Ciudad Universitaria
edificio 453, oficina 220
sgjimenezv@unal.edu.co
Claudia Becerra
Universidad Nacional
de Colombia, Bogota
cjbecerrac@unal.edu.co
Alexander Gelbukh
CIC-IPN
Av. Juan Dios B?tiz,
Av. Mendiz?bal, Col.
Nueva Industrial Vallejo,
CP 07738, DF, M?xico
gelbukh@gelbukh.com
Abstract
We present an approach for the construction of text
similarity functions using a parameterized resem-
blance coefficient in combination with a softened
cardinality function called soft cardinality. Our ap-
proach provides a consistent and recursive model,
varying levels of granularity from sentences to char-
acters. Therefore, our model was used to compare
sentences divided into words, and in turn, words di-
vided into q-grams of characters. Experimentally,
we observed that a performance correlation func-
tion in a space defined by all parameters was rel-
atively smooth and had a single maximum achiev-
able by ?hill climbing.? Our approach used only sur-
face text information, a stop-word remover, and a
stemmer to tackle the semantic text similarity task
6 at SEMEVAL 2012. The proposed method ranked
3rd (average), 5th (normalized correlation), and 15th
(aggregated correlation) among 89 systems submit-
ted by 31 teams.
1 Introduction
Similarity is the intrinsic ability of humans and some
animals to balance commonalities and differences when
comparing objects that are not identical. Although there
is no direct evidence of how this process works in liv-
ing organisms, some models have been proposed from
the cognitive perspective (Sj?berg, 1972; Tversky, 1977;
Navarro and Lee, 2004). On the other hand, several simi-
larity models have been proposed in mathematics, statis-
tics, and computer science among other fields. Particu-
larly in AI, similarity measures play an important role in
the construction of intelligent systems that are required
to exhibit behavior similar to humans. For instance, in
the field of natural language processing, text similarity
functions provide estimates of the human similarity judg-
ments related to language. In this paper, we combine el-
ements from the perspective of cognitive psychology and
computer science to propose a model for building simi-
larity functions suitable for the task of semantic text sim-
ilarity.
We identify four main families of text similarity func-
tions: i) resemblance coefficients based on sets (e.g. Jac-
card?s (1901) and Dice?s (1945) coefficients) ii) functions
in metric spaces (e.g. cosine tf-idf similarity (Salton et
al., 1975)); iii) the edit distance family of measures (e.g.
Levenstein (1966) distance, LCS (Hirschberg, 1977));
and iv) hybrid approaches ((Monge and Elkan, 1996; Co-
hen et al., 2003; Corley and Mihalcea, 2005; Jimenez et
al., 2010)). All of these measures use a subdivision of
the texts in different granularity levels, such as q-grams
of words, words, q-grams of characters, syllables, and
characters. Among hybrid approaches, Monge-Elkan?s
measure and soft cardinality methods are recursive and
can be used to build similarity functions at any arbitrary
range of granularity. For instance, it is possible to con-
struct a similarity function to compare sentences based
on a function that compares words, which in turn can be
constructed based on a function that compares bigrams of
characters. Furthermore, hybrid approaches can integrate
similarity functions that are not based on the representa-
tion of the surface of text, such as semantic relatedness
measures (Pedersen et al., 2004).
Text similarity measures can be static or adaptive
whether they are binary functions using only surface in-
formation of the two texts, or are functions that suit
to a wider set of texts. For instance, measures using
tf-idf weights adapt their results to the set of texts in
which those weights were obtained. Other approaches
learn parameters of the similarity function from a set of
texts to optimize a particular task. For instance, Ris-
tad and Yianilos (1998) and Bikenko and Mooney (2003)
learned the costs of edit operations for all characters for
an edit-distance function in a name-matching task. Other
machine-learning approaches have also been proposed to
build adaptive measures in name-matching (Bilenko and
449
Mooney, 2003) and textual-entailment tasks.
However, those machine-learning-based methods for
adaptive similarity suffer from sparseness and the ?curse
of dimensionality?. For example, the method of Ristad
and Yianilos learns n2 + 2n parameters, where n is the
size of the character set. Similarly, dimensionality in the
method of Bilenko and Mooney is the size of the data
set vocabulary. This issue is addressed primarily through
machine-learning algorithms, which reduce the dimen-
sionality of the problem regularizating to achieve enough
generalization to get an acceptable performance differ-
ence between training and test data. Although machine-
learning solutions have proven effective for many appli-
cations, the principle of Occam?s razor suggests that it
should be preferable to have a model that explains the
data with a smaller number of significant parameters. In
this paper, we seek a simpler adaptive similarity model
with few meaningful parameters.
Our proposed similarity model starts with a
cardinality-based resemblance coefficient (i.e. Dice?s
coefficient 2|A?B|/|A|+|B|) and generalizes it to model
the effect of asymmetric selection of the referent. This
effect is a human factor discovered by Tversky (1977)
that affects judgments of similarity, i.e. humans tends
to select the more prominent stimulus as the referent
and the less salient stimulus as the object. Some of
Tversky?s examples are ?the son resembles the father?
rather than ?the father resembles the son?, ?an ellipse is
like a circle? not ?a circle is like an ellipse?, and ?North
Korea is like Red China? rather than ?Red China is like
North Korea?. Generally speaking, ?the variant is more
similar to the prototype than vice versa?. In the previous
example, stimulus salience is associated with the promi-
nence of the country; for text comparison we associate
word salience with tf-idf weights. At the text level, we
associate salience with a combination of word-salience,
inter-word similarity, and text length provided by soft
cardinality. Experimentally, we observed that this effect
also occurs when comparing texts, but not necessarily
in the same direction suggested by Tversky. We used
this effect to improve the performance of our similarity
model. In addition, we proposed a parameter that biases
the function to generate greater or lower similarity
scores.
Finally, in our model we used a soft cardinality func-
tion (Jimenez et al., 2010) instead of the classical set car-
dinality. Just as classical cardinality counts the number
of elements which are not identical in a set, soft cardi-
nality uses an auxiliary inter-element similarity function
to make a soft count. For instance, the soft cardinality of
a set with two very similar (but not identical) elements
should be a real number closer to 1.0 instead of 2.0.
The rest of the paper is organized as follows. In Sec-
tion 2 we briefly present soft cardinality. In Section 3 the
proposed parameterized similarity model is presented. In
Section 4 experimental validation is provided using 8 data
sets annotated with human similarity judgments from the
?Semantic-Text-Similarity? task at SEMEVAL-2012. Fi-
nally, a brief discussion is provided in Section 5 and con-
clusions are presented in Section 6.
2 Soft Cardinality
Let A =
{
a1, a2, . . . , a|A|
}
and B =
{
b1, b2, . . . , b|B|
}
be two sets being compared. When each element of ai
or bj has an associated weight wai or wbj the problem
of comparing those sets becomes a weighted similarity
problem. This means that such model has to take into
account not only the commonalities and diferences, but
also their weights. Also, if an (|A ? B|) ? (|A ? B|)
similarity matrix S is available, the problem becomes a
weighted soft similarity problem because the common-
ality between A and B has to be computed not only
with identical elements, but also with elements with a
degree of similarity. The values of S can be obtained
from an auxiliary similarity function sim(a, b) that sat-
isfies at least non-negativity (?a, b, sim(a, b) ? 0) and
reflexivity (?a, sim(a, a) = 1). Other postulates such as
symmetry (?a, b, sim(a, b) = sim(b, a)) and triangle in-
equality1 (?a, b, c, sim(a, c) ? sim(a, b) + sim(b, c)?
1) are not strictly necessary.
Jimenez et al. (2010) proposed a set-based weighted
soft-similarity model using resemblance coefficients and
the soft cardinality function instead of classical set car-
dinality. The idea of calculating the soft cardinality is
to treat elements ai in set the A as sets themselves and
to treat inter-element similarities as the intersections be-
tween the elements sim(ai, aj) = |ai ? aj |. Therefore,
the soft cardinality of set A becomes |A|
?
=
?
?
?
?|A|
i=1ai
?
?
?.
Since it is not feasible to calculate this union, they pro-
posed the following weighted approximation using |ai| =
wai :
|A|
?
sim '
|A|?
i
wai
?
?
|A|?
j
sim(ai, aj)
p
?
?
?1
(1)
Parameter p ? 0 in eq.1 controls the ?softeness? of
the cardinality, taking p = 1 its no-effect value and leav-
ing element similarities unchanged for the calculation of
soft cardinality. When p is large, all sim(?, ?) results
lower than 1 are transformed into a number approaching
0. As a result, the soft cardinality behaves like the clas-
sical cardinality, returning the addition of all the weights
of the elements, i.e |A|
?
sim '
?|A|
i wai . When p is close
to 0, all sim(?.?) results are transformed approaching
1triangle inequality postulate for similarity is derived from its coun-
terpart for dissimilarity (distance) distance(a, b) = 1? sim(a, b).
450
into a number approaching 1, making the soft cardinal-
ity returns the average of the weights of the elements, i.e.
|A|
?
sim '
1
|A|
?|A|
i wai . Jimenez et al. used p = 2 and
idf weights in the same name-matching task proposed by
Cohen et al. (Cohen et al., 2003).
3 A Parameterized Similarity Model
As we mentioned above, Tvesky proposed that humans
tends to select more salient stimulus as referent and less
salient stimulus as object when comparing two objects A
and B. Based on the idea of Tvesrky, the similarity be-
tween two objects can be measured as the ratio between
the salience of commonalities and the salience of the less
salient object. Drawing an analogy between objects as
sets and salience as the cardinality of a set, the salience
of commonalities is |A ? B|, and the salience of the less
salient object is min(|A|, |B|). This ratio is known as the
overlap coefficient Overlap(A,B) = |A?B|min(|A|,|B|) . How-
ever, whether |A| < |B| or whether |A|  |B|, the sim-
ilarity obtained by Overlap(A,B) is the same. Hence,
we propose to model the selecction of the referent using
a parameter ? that makes a weighted average between
min(|A|, |B|) and max(|A|, |B|), controling the degree
to which the asymmetric referent-selection effect is con-
sidered in the similarity measure.
SIM(A,B) =
|A ?B|+ bias
?max (|A|, |B|) + (1? ?)min (|A|, |B|)
(2)
The parameter ? controls the degree to which the
asymmetric referent-selection effect is considered in the
similarity measure. Its no-effect value is ? = 0.5, so
the eq.2 becomes the Dice coefficient. Moreover, when
? = 0 the eq.2 becomes the overlap coefficient, other-
wise when ? = 1 the opposite effect is modeled.
In addition, we introduced a bias parameter in eq. 2
that increases the commonalities of each object pair by
the same amount, and so it measures the degree to which
all of the objects have commonalities among each other.
Clearly, the non-effect value for the bias parameter is 0.
Besides, the bias parameter has the effect of biasing
SIM(A,B) by considering any pair ?A,B? more sim-
ilar if bias > 0 and their cardinalities are small. Con-
versely, the similarity between pairs with large cardinal-
ities is promoted if bias < 0. However, as higher values
of biasmay result in similarity scores outside the interval
[0, 1], additional post-procesing to limit the similarities in
this interval may be required.
The proposed parameterized text similarity measure is
constructed by combining the proposed resemblance co-
efficient in eq.2 and the soft cardinality in eq.1. The
resulting measure has three parameters: ?, bias, and p.
Weights wai can be idf weights. This measure takes two
? Asymetric referent selection at text level
bias Bias parameter at text level
p Soft cardinality exponent at word level
wai Element weights at word level
q1, q2 q1-grams or [q1 : q2]spectra word division
?sim Asymetric referent selection at q-gram level
biassim Bias parameter q-gram level
Table 1: Parameters of the proposed similarity model
texts represented as sets of words and returns their simi-
larity. The auxiliary similarity function sim(a, b) neces-
sary for calculating the soft cardinality is another param-
eter of the model. This auxiliary function is any function
that can compare two words and return a similarity score
in [0, 1].
To build this sim(a, b) function, we chose to reuse the
eq.2 but representing words as sets of q-grams or ranges
of q-grams of different sizes, i.e. [q1 : q2] spectra. Q-
grams are consecutive overlapped substrings of size q.
For instance, the word ?saturday? divided into trigrams
is {/sa, sat, atu, tur, urd, rda, day, ay.}. The character
?.? is a padding character added to differenciate q-grams
at the begining or end of the string. A [2 : 4]spectra
is the combined representation of a word using ?in this
example? bigrams, trigrams and quadgrams (Jimenez and
Gelbukh, 2011). The cardinality function for sim() was
the classical set cardinality. Clearly, the soft cardinal-
ity could be used again if an auxiliary similarity func-
tion for character comparison and a q-gram weighting
mechanism are provided to allow another level of recur-
sion. Therefore, the parameters of sim(a, b) are: ?sim,
biassim. Finally, the entire set of parameters of the pro-
posed similarity model is shown in Table 1.
4 Experimental Setup and Results
The aim of these experiments is to observe the behavior
of the parameters of our similarity model and verify if the
hypothesis that motivated these parameters can be con-
firmed experimentally. The experimental data are 8 data
sets (3 for training and 5 for test) proposed in the ?Seman-
tic Text Similarity? task at SEMEVAL-2012. Each data
set consist of a set of pairs of text annotated with human-
similarity judgments on a scale of 0 to 5. Each similarity
judgment is the average of the judgments provided by 5
human judges. For a comprehensible description of the
task see(Agirre et al., 2012).
For the experiments, all data sets were pre-processed
by converting to lowercase characters, English stop-
words removal and stemming using Porter stemmer
(Porter, 1980). The performance measure used for all ex-
periments was the Pearson correlation r.
451
4.1 Model Parameters
In order to make an initial exploration of the parame-
ters in Table 1, we set q1 = 2 (i.e. bigrams) and used
wai = idf(ai). For other parameters, we started with all
the non-effect values, i.e. ? = 0.5, bias = 0, p = 1,
?sim = 0.5 and biassim = 0. Plots in Figure 1 show
the Pearson correlation measured in each of the data sets.
For each graph, the non-effect configuration was used and
each parameter varies in the range indicated in each hor-
izontal axis. For best viewing, the non-effect values on
each graph are represented by a vertical line.
In this exploration of the parameters it was noted that
each parameter defines a function for the performance
measure that is smooth and with an unique global maxi-
mum. Therefore, we assumed that the join performance
function in the space defined by the 5 parameters also
had the same properties. The parameters for each data set
shown in Table 2 were found using a simple hill-climbing
algorithm. Different q-gram and spectra configurations
were tested manually.
5 Discussion
It is possible to observe from the results in Figure 1 and
Table 2 that the behavior of the parameters is similar in
pairs of data sets that have training and test parts. This
behavior is evident in both MSRvid and MSRpar data
sets, but it is less evident in SMTeuroparl. Furthermore,
the optimal parameters for training data sets MSRvid and
MSRpar were similar to those of their test data sets. In
conclusion, the proposed set of parameters provides a set
of features that characterize a data set for the text similar-
ity task.
Regarding the effect of asymmetry in referent selecc-
tion proposed by Tvesrky, it was observed that ?at text
level? the MSRvid data sets were the only ones that sup-
ported this hypothesis (? = 0.32, 0.42). The remaining
data sets showed the opposite effect (? > 0.5). That is,
annotators chose the most salient document (the longer)
as the referent when a pair of texts is being compared.
The Table 2 also shows that the optimal parameters
for all data sets were different from the no-effect values
combination. This result can also be seen in Figure 1,
where curves crossed the vertical line of no-effect value
?in most of the cases? in values different to the optimum.
Clearly, the proposed set of parameters is useful for ad-
justing the similarity function for a particular data set and
task.
6 Conclusions
We have proposed a new parameterized similarity func-
tion for text comparison and a method for finding the op-
timal values of the parameter set when training data is
available. In addition, the parameter ?, which was moti-
vated by the similarity model of Tversky, proved effective
in obtaining better performance, but we could not con-
firm the Tvesky?s hypothesis that humans tends to select
the object (text) with less stimulus salience (text length)
as the referent. This result might have occurred because
either the stimulus salience is not properly represented by
the length of the text, or Tversky?s hypothesis cannot be
extended to text comparison.
The proposed similarity function proved effective in
the task of ?Semantic Text Similarity? in SEMEVAL
2012. Our method obtained the third best average cor-
relation on the 5 test data sets. This result is remarkable
because our method only used data from the surface of
the texts, a stop-word remover, and a stemmer, which can
be even be considered as a baseline method.
Acknowledgments
This research was funded by the Systems and Industrial
Engineering Department, the Office of Student Welfare
of the National University of Colombia, Bogot?, and
throught a grant from the Colombian Department for
Science, Technology and Innovation Colciencias, proj.
110152128465. The second author recognizes the sup-
port from Mexican Government (SNI, COFAA-IPN, SIP
20113295, CONACYT 50206-H) and CONACYT?DST
India (proj. ?Answer Validation through Textual Entail-
ment?).
References
