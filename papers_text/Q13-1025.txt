Data-driven, PCFG-based and Pseudo-PCFG-based Models for Chinese
Dependency Parsing
Weiwei Sun and Xiaojun Wan
Institute of Computer Science and Technology, Peking University
The MOE Key Laboratory of Computational Linguistics, Peking University
{ws,wanxiaojun}@pku.edu.cn
Abstract
We present a comparative study of transition-,
graph- and PCFG-based models aimed at il-
luminating more precisely the likely contri-
bution of CFGs in improving Chinese depen-
dency parsing accuracy, especially by com-
bining heterogeneous models. Inspired by
the impact of a constituency grammar on de-
pendency parsing, we propose several strate-
gies to acquire pseudo CFGs only from de-
pendency annotations. Compared to linguistic
grammars learned from rich phrase-structure
treebanks, well designed pseudo grammars
achieve similar parsing accuracy and have
equivalent contributions to parser ensemble.
Moreover, pseudo grammars increase the di-
versity of base models; therefore, together
with all other models, further improve sys-
tem combination. Based on automatic POS
tagging, our final model achieves a UAS of
87.23%, resulting in a significant improve-
ment of the state of the art.
1 Introduction
Popular approaches to dependency parsing can
be divided into two classes: grammar-free and
grammar-based. Data-driven, grammar-free ap-
proaches make essential use of machine learning
from linguistic annotations in order to parse new
sentences. Such approaches, e.g. transition-based
(Nivre, 2008) and graph-based (McDonald, 2006;
Torres Martins et al, 2009) have attracted the most
attention in recent years. In contrast, grammar-
based approaches rely on linguistic grammars (in
either dependency or constituency formalisms) to
shape the search space for possible syntactic anal-
ysis. In particular, CFG-based dependency parsing
exploits a mapping between dependency and con-
stituency representations and reuses parsing algo-
rithms developed for CFG to produce dependency
structures. In previous work, data-driven, discrim-
inative approaches have been widely discussed for
Chinese dependency parsing. On the other hand,
various PCFG-based constituent parsing methods
have been applied to obtain phrase-structures as
well. With rich linguistic rules, phrase-structures of
Chinese sentences can be well transformed to their
corresponding dependency structures (Xue, 2007).
Therefore, PCFG parsers with such conversion rules
can be taken as another type of dependency parser.
We call them PCFG-based parsers, in this paper.
Explicitly defining linguistic rules to express
precisely generic grammatical regularities, a con-
stituency grammar can be applied to arrange sen-
tences into a hierarchy of nested phrases, which de-
termines constructions between larger phrases and
their smaller component phrases. This type of infor-
mation is different from, but highly related to, the
information captured by a dependency representa-
tion. A constituency grammar, thus, has great possi-
ble contributions to dependency parsing. In order
to pave the way for new and better methods, we
study the impact of CFGs on Chinese dependency
parsing. A series of empirical analysis of state-of-
the-art graph-, transition-and PCFG-based parsers is
presented to illuminate more precisely the properties
of heterogeneous models. We show that CFGs have
a great impact on dependency parsing and PCFG-
based models have complementary predictive pow-
ers to data-driven models.
System ensemble is an effective and important
technique to build more accurate parsers based on
multiple, diverse, weaker models. Exploiting differ-
301
Transactions of the Association for Computational Linguistics, 1 (2013) 301?314. Action Editor: Jason Eisner.
Submitted 6/2012; Revised 10/2012; Published 7/2013. c?2013 Association for Computational Linguistics.
ent data-driven models, e.g. transition- and graph-
based models, has received the most attention in
dependency parser ensemble (Nivre and McDon-
ald, 2008; Torres Martins et al, 2008; Sagae and
Lavie, 2006). Only a few works investigate inte-
grating data-driven and PCFG-based models (Mc-
Donald, 2006). We argue that grammars can signif-
icantly increase the diversity of base models, which
plays a central role in parser ensemble, and therefore
lead to better and more promising hybrid systems.
We introduce a general classifier enhancing tech-
nique, i.e. bootstrap aggregating (Bagging), to im-
prove dependency parsing accuracy. This technique
can be applied to enhance a single-view parser, or
to combine multiple heterogeneous parsers. Exper-
iments on the CoNLL 09 shared task data demon-
strate its effectiveness: (1) Bagging can improve in-
dividual single-view parsers, especially the PCFG-
based one; (2) Bagging is more effective than pre-
viously introduced ensemble methods to combine
multi-view parsers; (3) Integrating data-driven and
PCFG-based models is more useful than combining
different data-driven models.
Although PCFG-based models have a big con-
tribution to data-driven dependency parsing, they
have a serious limitation: There are no corre-
sponding constituency annotations for some depen-
dency treebanks, e.g. Chinese Dependency Tree-
bank (LDC2012T05). To overcome this limita-
tion, we propose several strategies to acquire pseudo
grammars only from dependency annotations. In
particular, dependency trees are converted to pseudo
constituency trees and PCFGs can be extracted from
such trees. Another motivation of this study is to in-
crease the diversity of candidate models for parser
ensemble. Experiments show that pseudo-PCFG-
based models are very competitive: (1) Pseudo
grammars achieve similar or even better parsing re-
sults than linguistic grammars learned from rich
constituency annotations; (2) Compared to linguistic
grammars, well designed, single-view pseudo gram-
mars have an equivalent contribution to parser en-
semble; (3) Combining different pseudo grammars
even work better for ensemble than linguistic gram-
mars; (4) Pseudo-PCFG-based models increase the
diversity of base models, and therefore lead to fur-
ther improvements for ensemble.
Based on automatic POS tagging, our final model
achieves a UAS of 87.23% on the CoNLL data and
84.65% on CTB5, which yield relative error reduc-
tions of 18-24% over the best published results in
the literature.
2 Background and related work
2.1 Data-driven dependency parsing
The mainstream work on recent dependency pars-
ing focuses on data-driven approaches that automat-
ically learn to produce dependency graphs for sen-
tences solely from a hand-crafted dependency tree-
bank. The advantage of such models is that they
are easily ported to any language in which labeled
linguistic resources exist. Practically all statisti-
cal models that have been proposed in recent years
can be mainly described as either graph-based or
transition-based (McDonald and Nivre, 2007). Both
models have been adopted to learn Chinese depen-
dency structures (Zhang and Clark, 2011; Zhang
and Nivre, 2011; Huang and Sagae, 2010; Hatori
et al, 2011; Li et al, 2011, 2012). According to
published results, graph-based and transition-based
parsers achieve similar accuracy.
In the graph-based framework, informative evalu-
ation results have been presented in (Li et al, 2011).
First, second and third order projective parsing mod-
els are well evaluated. In the transition-based frame-
work, two advanced techniques have been stud-
ied. First, developing features has been shown
crucial to advancing parsing accuracy and a very
rich feature set is carefully evaluated by Zhang and
Nivre (2011). Second, beyond deterministic greedy
search, principled dynamic programming strategies
can be employed to explore more possible hypothe-
ses (Huang and Sagae, 2010). Both techniques have
been examined and shown helpful for Chinese de-
pendency parsing. Furthermore, Hatori et al (2011)
combined both and obtained a state-of-the-art super-
vised parsing result.
2.2 PCFG-based dependency parsing
PCFG-based dependency parsing approaches are
based on the finding that projective dependency trees
can be transformed from constituency trees by ap-
plying rich linguistic rules. In such approaches, de-
pendency parsing can be resolved by a two-step pro-
cess: constituent parsing and rule-based extraction
302
of dependencies from phrase structures. The ad-
vantage of constituency-grammar-based approach is
that all the well-studied parsing methods for such
grammars can be used for dependency parsing as
well. Two language-specific properties essentially
make PCFG-based approaches easy to be applied
to Chinese dependency parsing: (1) Chinese gram-
maticians favor using projective structures;1 (2) Chi-
nese phrase-structure annotations normally contain
richer information and thus are reliable for tree con-
version.
2.2.1 Constituency parsing
Compared to many other languages, statistical
constituent parsing for Chinese has reached early
success, due to the fact that the language has rela-
tively fixed word order and extremely poor inflec-
tional morphology. Both facts allow PCFG-based
statistical modeling to perform well. For the con-
stituent parsing, the majority of the state-of-the-
art parsers are based on generative PCFG learn-
ing. For example, the well-known and success-
ful Collins and Charniak&Johnson parsers (Collins,
2003; Charniak, 2000; Charniak and Johnson, 2005)
implement generative lexicalized statistical models.
Apart from lexicalized PCFG parsing, unlex-
icalized parsing with latent variable grammars
(PCFGLA) can also produce comparable accuracy
(Matsuzaki et al, 2005; Petrov et al, 2006). Latent
variable grammars model an observed treebank of
coarse parse trees with a model over more refined,
but unobserved, derivation trees that represent much
more complex syntactic processes. Rather than
attempting to manually specify fine-grained cate-
gories, previous work shows that automatically in-
ducing the sub-categories from data can work quite
well. A PCFGLA parser leverages on an automatic
procedure to learn refined grammars and are there-
fore more robust to parse non-English languages that
are not well studied. For Chinese, such a parser
achieves the state-of-the-art performance and de-
feats many other types of parsers, including Collins
as well as Charniak parser (Che et al, 2012) and
1For example, as two popular dependency treebanks, the
CoNLL 2009 data and the Chinese Dependency Treebank both
excluede non-projective annotations. It is worth noting that the
former one is converted from a constituency treebank while the
latter one is directly annotated by lingusitics.
discriminative transition-based models (Zhang and
Clark, 2009).
2.2.2 CS to DS conversion
In the absence of dependency and constituency
structures for a particular treebank, treebank-guided
parser developers normally apply rich linguistic
rules to convert one representation formalism to an-
other to get necessary data to train parsers. Xue
(2007) examines the linguistic adequacy of depen-
dency structure annotation automatically converted
from phrase structure treebanks with rule-based ap-
proaches. A structural approach is introduced for
the constituency structure (CS) to dependency struc-
ture (DS) conversion for the Chinese Treebank data,
which is the basis of the CoNLL 2009 shared task
data. By applying this conversion procedure on the
outputs of an automatic phrase structure parser, we
can build a PCFG-based dependency parser.
2.3 Parser ensemble
NLP systems built on particular single views nor-
mally capture different properties of an original
problem, and therefore differ in predictive powers.
As a result, NLP systems can take advantage of com-
plementary strengths of multiple views. Combining
the outputs of several systems has been shown in the
past to improve parsing performance significantly,
including integrating phrase-structure parsers (Hen-
derson and Brill, 1999), dependency parsers (Nivre
and McDonald, 2008), or both (McDonald, 2006).
Several ensemble models have been proposed for
the parsing of syntactic constituents and dependen-
cies, including learning-based stacking (Nivre and
McDonald, 2008; Torres Martins et al, 2008) and
learning-free post-inference (Henderson and Brill,
1999; Sagae and Lavie, 2006). Surdeanu and Man-
ning (2010) present a systematic analysis of these
ensemble methods and find several non-obvious
facts:
? the diversity of base parsers is more important
than complex models for learning, and
? simplest scoring model for voting and repars-
ing performs essentially as well as other more
complex models.
303
3 A comparative analysis of heterogeneous
parsers
The information encoded in a dependency repre-
sentation is different from the information captured
in a constituency representation. While the depen-
dency structure represents head-dependent relations
between words, the constituency structure repre-
sents the grouping of words into phrases, classified
by structural categories. These differences concern
what is explicitly encoded in the respective represen-
tations, and affects data-driven and PCFG-based de-
pendency parsing models substantially. In this sec-
tion, we give a comparative analysis of transition-,
graph- and PCFG-based models aimed at illuminat-
ing more precisely the likely contribution of CFGs
in dependency parsing.
3.1 Experimental setup
Penn Chinese TreeBank (CTB) is a segmented,
POS tagged, and fully bracketed corpus in the con-
stituency formalism, and very popular to evaluate
fundamental NLP tasks, including word segmenta-
tion, POS tagging, constituent parsing as well as de-
pendency parsing. We use CTB 6 as our main corpus
and define the training, development and test sets ac-
cording to the CoNLL 2009 shared task. To evaluate
and analyze dependency parsers, we directly use the
CoNLL data. CTB?s syntactic annotations also in-
cludes functional information and empty categories.
Modern parsers, e.g. Collins and Berkeley parsers,
ignore these types of linguistic knowledge. To train
a constituent parser, we perform a heuristic proce-
dure on the treebank data to delete function tags and
empty categories as well as its associated redundant
ancestors. Many papers reported parsing results of
an older version CTB (namely CTB 5). To compare
with systems introduced in these papers, we evaluate
our final ensemble model on CTB5 in Section 5.4.
For dependency parsing, we choose a second
order graph-based parser2 (Bohnet, 2010) and a
transition-based parser (Hatori et al, 2011), for
experiments. For constituent parsing, we choose
Berkeley parser,3 a well known implementation of
the unlexicalized PCFGLA model and Bikel parser,4
2code.google.com/p/mate-tools/
3code.google.com/p/berkeleyparser/
4cis.upenn.edu/?dbikel/software.html
a well known implementation of Collins? lexical-
ized model, for experiments. In data-driven pars-
ing, features consisting of POS tags are very effec-
tive, so typically POS tagging is performed as a pre-
processing. We use the baseline sequential tagger
described in (Sun and Uszkoreit, 2012) to provide
such lexical information to the graph-based parser.
Note that the transition-based parser performs a joint
inference to acquire POS and dependency informa-
tion simultaneously, so there is no need to offer extra
tagging results to it.
3.2 Overall performance
Table 1 (Column 2-6) summarizes the overall accu-
racy of different parsers. Two transition-based pars-
ing results are presented: The first one employ a
simple feature set (Zhang and Clark, 2008) and a
small beam (16); the second one employ rich fea-
tures (Zhang and Nivre, 2011) and a larger beam
(32). Two graph-based parsing results are reported;
the difference between them is whether integrate re-
lation labels into the parsing procedure. Roughly
speaking, currently state-of-the-art data-driven mod-
els achieves slightly better precision than unlexical-
ized PCFG-based models with regard to unlabeled
dependency prediction.
There is a big gap between lexicalized and unlexi-
calized parsing. The same phenomenon has been ob-
served by (Che et al, 2012) and (Zhuang and Zong,
2010). In addition to dependency parsing, Zhuang
and Zong (2010) found that Berkeley parser pro-
duce much more accurate syntactic analyses to assist
a Chinese semantic role labeler than Bikel parser.
Charniak and Stanford parsers are two other well-
known and frequently used tools that can provide
lexicalized parsing results. According to (Che et al,
2012), they perform even worse than Bikel parser,
at least for Stanford dependencies. Due to the poor
parsing performance, we only concentrate on the un-
lexicalized model in the remainder of this paper.
The performance of labeled dependency predic-
tion of the unlexicalized PCFG-based parser is much
lower. We can learn that the CS to DS conversion is
not robust to assign functional categories to depen-
dencies and simple linguistic rules are not capable
to do fine-grained classification. Previous research
on English indicates that the main difficulty in de-
pendency parsing is the prediction of dependency
304
Devel. UAS LAS Compl. Fsib Fgrd
Tran[b=16,Z08] 82.80 N/A 29.00 66.55 79.74
Tran[b=32,Z11] 83.80 N/A 31.61 68.58 80.87
Graph[-lab] 83.66 N/A 29.28 67.96 80.82
Graph[+lab] 84.24 80.55 30.99 69.11 81.38
Unlex 82.86 67.44 27.98 69.07 81.22
Lex 70.38 58.10 - - - - - -
Bagging(15)
Tran[b=16,Z08] 83.25 N/A 28.66 67.17 78.89
Tran[b=32,Z11] 84.25 N/A 31.21 69.14 81.49
Graph[-lab] 83.81 N/A 29.68 68.00 80.62
Graph[+lab] 84.50 N/A 31.44 69.48 81.10
Unlex 84.92 N/A 32.35 71.08 83.66
Bagging(8)
Unlex 84.35 N/A 31.16 70.49 83.57
Table 1: Accuracy of different parsers. The first block
presents baseline parsers; the last two blocks present
Bagging-enhanced parsers, where m is respectively set to
15 and 8. Z08 and Z11 distinguish different feature sets;
b=16 and b=32 are beam sizes. +/-lab means whether to
incorporate relation labels to a model.
structures, and an extra statistical classifier can be
employed to label automatically recognized depen-
dencies with a high accuracy. Although this issue
is not well studied for Chinese dependency parsing,
previous research on function tag labeling (Sun and
Sui, 2009) and semantic role labeling (Sun, 2010a)
gives us some clues. Their research shows that both
functional and predicate-argument structural infor-
mation is relatively easy to predict if high-quality
syntactic parses are available. We mainly focus on
the UAS metric in the following experiments.
3.3 Constraints
A grammar-based model utilizes an explicitly de-
fined formal grammar to shape the search space for
possible syntactic hypotheses. Parameters of a sta-
tistical grammar-based model are related to a gram-
mar rule, and as a result specific language construc-
tions are constrained by each other. For example,
parameters are assigned to rewrite rules for a CFG-
based model. Since the PCFG-based model lever-
ages rewrite rules to locally constrain several possi-
ble dependents for one head word, it does relatively
better for locally connected dependencies. The tra-
ditional evaluation metrics, i.e. UAS and LAS, only
consider bi-lexical (first order) dependencies, which
are smallest pieces of a dependency structure. Be-
sides bi-lexical dependencies, we report the predic-
tion accuracy of grandparent and sibling dependen-
cies, i.e. second order dependencies. The metrics
are defined as follows.
? For every word d whose parent is not the root,
we consider the word triple ?d, p, g? among d
and its parent p and grandparent g. A word
triple ?d, p, g? from a predicted tree is consid-
ered as correct if it also apprears in the corre-
sponding gold tree. Based on this definition,
precison, recall and f-score of grandparent de-
pendency can be defined in a normal sense. All
punctuations are excluded for calculation.
? For every word h that governs at least two chil-
dren (d1, ..., dn), we consider every word triple
?h, di, di+1?, among h and its sibling depen-
dents di as well as di+1 (0 ? i < n). Similar
to the grandparent dependencies, we can define
evaluation metrics for sibling dependencies.
From Table 1, we can see that the grammar-based
model parses relatively better for slightly larger frag-
ments. For example, the UAS of the graph-based
model is significantly higher than the grammar-
based one, but their sibling and grandparent scores
are similar. In the next section, we will introduce
a general parser enhancement technique and present
more discussions based on enhanced parsing results
(Column 7-14).
3.4 Endocentric and exocentric constructions
<-NN<- <-NR<- <-NT<- <-PN<- <-VA <- <-VC<- <-VE<- <-VV<-
Unlex 2 7.61 19.3 17.2 5 14.0 9 39.72 45.51 49.83 41.44
G raph[+lab] 2 4.82 17.45 12 .2 12 .1 38.12 49.9 51.18 42 .14
Tran[b=32 ,Z0 8] 2 5.2 5 17.82 15.16 13.48 41.32 47.7 49.83 42 .34
10
2 0
30
40
50
Er
ro
r 
ra
te
 
Figure 1: Nominal vs. verbal constructions.
Arguments in exocentric constructions help com-
plete the meaning of a predicate and are taken to be
obligatory and selected by their heads; adjuncts in
305
endocentric constructions are structurally dispens-
able parts that provide auxiliary information and
taken to be optional and not selected by their heads.
An important annotation policy of the CTB is ?one
grammatical relation per bracket?, which means
each constituent falls into one of the three primitive
grammatical relations: (1) head-complementation,
(2) head-adjunction and (3) coordination. Addi-
tionally, the argument is attached at a level that is
?closer? to the head than the adjuncts. Due to the
linguistic properties of different dependents and the
annotation strategies, a grammar-based model can
capture more syntactic preference properties of ar-
guments via hard constraints, i.e. grammar rules,
and are therefore more suitable to analyze exocen-
tric constructions.
Figure 1 is the error rate of unlabeled dependen-
cies considering different construction. A construc-
tion ?? X ?? is considered as correctly predicted
if and only if all dependent words and head word of
X are completely correctly found. The error rate
in terms of this metric seems rather high because
the units we consider are normally much larger than
word pairs. From this figure, we can clearly see that
the data-driven parser does better for the prediction
of nominal constructions (NN/NR/NT/PN5), which
relate more on optional adjuncts or modifiers; the
grammar-based parser performs better for the pre-
diction of verbal constructions (VC/VE/VV), which
relate more on obligatory arguments. The evalua-
tion of the nominal and verbal constructions roughly
confirms the strength of grammar-based model to
predict verbal constructions.
4 Bagging parsers
The comparative analysis highlights the fundamen-
tal diversity between data-driven and PCFG-based
models. In order to exploit the diversity gain, we ad-
dress the issue of parser combination. We employ
a general ensemble learning technique, i.e. Bag-
ging, to enhance a single-view parser and to com-
bine multi-view parsers.
5For the definition and illustration of these tags, please refer
to the annotation guidelines (http://www.cis.upenn.
edu/?chinese/posguide.3rd.ch.pdf).
4.1 Applying Bagging to dependency parsing
Bagging is a machine learning ensemble meta-
algorithm to improve classification and regression
models in terms of stability and classification accu-
racy (Breiman, 1996). It also reduces variance and
helps to avoid overfitting. Given a training set D of
size n, Bagging generates m new training sets Di
of size n? ? n, by sampling examples from D. m
models are separately learned on the m new train-
ing sets and combined by voting (for classification)
or averaging the output (for regression). Hender-
son and Brill (2000) successfully applied Bagging
to enhance a constituent parser. Moreover, Bagging
has been applied to combine multiple solutions for
Chinese lexical processing (Sun, 2010b; Sun and
Uszkoreit, 2012). In this paper, we apply Bagging
to dependency parsing. Since training even one sin-
gle parser takes hours (if not days), experiments on
Bagging is time-consuming. To save time, we con-
duct data-driven parsing experiments based on sim-
ple configuration. More specifically, the beam size
of the transition-based parser is set to 16, and the
simple feature set is utilized; dependency relations
are not incorporated for the graph-based parser.
Bootrapping step. In the training phase, given a
training set D of size n, our model generates m new
training sets Di of size ?n by sampling uniformly
without replacement. Each Di can be used to train
a single-view parser or multiple parsers according
to different views. Using this strategy, we can get m
weak parsers or km parsers if multiple views are im-
plemented. In the parsing phase, for each sentence,
the (k)m models output (k)m candidate analyses
that are combined in a post-inference procedure.
Aggregating step. Different from classification
problems, simple voting scheme is not suitable for
parsing, which is a typical structured prediction
problem. To aggregate outputs of (k)m sub-models,
a structured inference procedure is needed. Sagae
and Lavie (2006) present a framework for combin-
ing the output of several different parsers to produce
results that are superior to each of the individual
parsers. We implement their method to aggregate
models. Once we have obtained multiple depen-
dency trees respectively from base parsers, we can
build a graph where each word in the sentence is a
306
node. We then create weighted directed edges be-
tween the nodes corresponding to words for which
dependencies are obtained from each of the initial
structures. The weights are the word-by-word voting
results of sub-models. Based on this graph, the sen-
tence can be reparsed by a graph-based algorithm.
Taking Chinese as a projective language, we use Eis-
ner?s algorithm (Eisner, 1996) to combine multiple
dependency parses. Surdeanu and Manning (2010)
indicates that reparsing performs essentially as well
as other simpler or more complex models.
4.2 Parameter tuning
We evaluate our combination model on the same
data set used in the last section. The two hyper-
parameters (? and m) of our Bagging model are
tuned on the development (validation) set. On one
hand, with the increase of the size of sub-samples,
i.e. ?, the performance of sub-models is improved.
However, since the sub-models overlap more, the di-
versity of base models for ensemble will decrease
and the final prediction accuracy may go down. To
evaluate the effect of ?, we separately sample 50%,
60%, 70% and 80% sentences from the original
training data 5 times, train 5 sub-models for each
parser, and combine them together. The beam size
of the transition-based parser is set to 16. Table 2
shows the influence of the choice of ??s. For all fol-
lowing experiments, we set ? = 0.7.
? 50% 60% 70% 80%
Tran+Graph[-lab]+Unlex 83.50 85.96 86.15 85.60
Table 2: UAS of Bagging(5) models with different ?.
The second parameter for Bagging is the number
of sub-models to be used for combination. Figure 2
summarizes the Bagging performance when differ-
ent models are employed and different number (i.e.
m) of subsamples are used. From this figure, we can
learn the influence of the number of sub-models.
4.3 Bagging single-view parsers
4.3.1 Results
Table 1 indicates that Bagging can improve in-
dividual single-view parsers, especially Berkeley
parser. If we take Bagging as a general parser en-
hancement technique and still consider a Bagging-
enhanced parser as a single view, we conclude
81.5
82 .5
83.5
84.5
85.5
86.5
3 4 5 6 7 8 9
G raph[-lab]
Tran
Unlex
G raph[-lab]+Unlex
Tran+Unlex
G raph[-lab]+Tran
Figure 2: Averaged UAS of different Bagging models
with different numbers of sampling data sets.
that Bagging-enhanced PCFG-based method works
best among state-of-the-art approaches. For the
transition-based parser, though the score over single
words goes up, the score over sentences goes down.
The main reason is that the reparsing algorithm is a
graph-based one, which performs worse with regard
to the prediction of a whole sentence. The improve-
ment for the graph-based parser is very modest.
We train a Bagging(8)-enhanced Berkeley parser,
which achieves equivalent overall UAS to data-
driven parsers, and compare their parsing abilities
of second order dependencies. Now we can more
clearly see that the Bagging-enhanced PCFG-based
model performs better in the prediction of second
order dependencies.
4.3.2 Related experiments on sequence models
Bagging has been applied to enhance discrimina-
tive sequence models for Chinese word segmenta-
tion (Sun, 2010b) and POS tagging (Sun and Uszko-
reit, 2012). For word segmentation, experiments
on discriminative Markov and semi-Markov tagging
models are reported. Their experiments showed that
Bagging can consistently enhance a semi-Markov
model but not the Markov one. Experiments on POS
tagging indicated that BaggingMarkov models hurts
tagging performance. It seems that the relationships
among basic processing units affect Bagging.
PCFGLA parsers are built upon generative mod-
els with latent annotations. The use of automati-
cally induced latent variables may also affect Bag-
ging. Generative sequence models with latent anno-
307
tations can also achieve good performance for Chi-
nese POS tagging. Huang et al (2009) described
and evaluated a bi-gram HMM tagger that utilizes
latent annotations. Different from negative results of
Bagging discriminative models, our auxiliary exper-
iment shows that Bagging Huang et al?s tagger can
help Chinese POS tagging. In other words, Bagging
substantially improves both HMMLA and PCFGLA
models, at least for Chinese POS tagging and con-
stituency parsing. It seems that Bagging favors the
use of latent variables.
4.4 Bagging multi-view parsers
4.4.1 Results
Figure 2 clearly shows that the Bagging model
taking both data-driven and PCFG-based models as
basic systems outperform the Bagging model taking
either model in isolation as basic systems. The com-
bination of a PCFG-based model and a data-driven
model (either graph-based or transition-based) is
more effective than the combination of two data-
driven models, which has received the most atten-
tion in dependency parser ensemble. Table 3 is
the performance of reparsing on the development
data. From this table, we can see by utilizing more
parsers, Bagging can enhance reparsing. According
to Surdeanu and Manning (2010)?s findings, repars-
ing performs as well as other combination mod-
els. Our auxiliary experiments confirm this finding:
Learning-based stacking cannot achieve better per-
formance. Limited to the document length, we do
not give descriptions of these experiments.
Devel. UAS
Reparsing(Tran[b=16,Z08]+Graph[-lab]+Unlex) 85.82
+Bagging(15) 86.37
bagging(reparse(g, t, c)) 86.09
reparse(bagging(g, t, c)) 85.86
Table 3: UAS of reparsing and Bagging.
4.4.2 Analysis
In our proposed model, Bagging has a two-fold
effect: One is as a system combination technique
and the other as a general parser enhancing tech-
nique. Two additional experiments are performed
to evaluate these two effects. To illustrate the differ-
ences between these two experiments, respectively
denote graph-based, transition-based and PCFG-
based parsers as g, t and c; denote the reparsing
procedure as reparse and the Bagging procedure as
bagging. The two experiments are as follows.
? Bagging a hybrid parser. In this experiment,
for each sub-sample Di, we first train three
parsers: gi, ti and ci. Then we combine these
three parsers by reparsing and construct a hy-
brid parser reparse(gi, ti, ci). Finally, all hy-
brid parsers are collected to build the final
parser: bagging(reparse(g, t, c)).
? Combining Bagging-enhanced parsers. In
this experiment, for each model, we first train
three Bagging-enhanced parsers: bagging(g),
bagging(t) and bagging(c). Then these
three Bagging-enhanced parsers are com-
bined by reparsing to build the final parser:
reparse(bagging(g, t, c)).
Evaluation results are presented in Table 3.
5 Pseudo-grammar-based models
Although the combination of data-driven and
grammar-based models is very effective, it has a
serious limitation: It is only applicable when con-
stituency annotations are available to learning a
grammar. However, many treebanks, e.g. Chinese
Dependency Treebank (LDC2012T05), do not have
such linguistically rich structures. Our experiments
also suggest that a constituency grammar can sig-
nificantly increase the diversity of base models for
parser ensemble, which plays a major role in boost-
ing prediction accuracy.
In order to reduce the need for phrase-structure
annotations, and to increase the diversity of candi-
date parsers, we study learning pseudo grammars
for dependency parsing. The key idea is very sim-
ple: By converting a dependency structure to a
constituency one, we can reuse the PCFGLA ap-
proach to learn pseudo grammars for dependency
parsing. Figure 3 is an example. The first tree is
an original dependency parse, while the second tree
is the corresponding CTB annotation. The next two
trees are two automatically converted pseudo con-
stituency trees. By applying DS to CS rules, we
can acquire pseudo constituency treebanks and then
learn pseudo grammars from them.
308
(1) Dependency tree (2) Linguistic constituency tree
(3) Flat constituency tree (4) Binarized constituency tree
Figure 3: An example: China encourages private entrepreneurs to invest in national infrastructure.
The basic idea of our method is to use parsing
models in one formalism for parsing in another for-
malism. In previous work, PCFGs are used to solve
parsing problems in many other formalisms, includ-
ing dependency (Collins et al, 1999), CCG (Fowler
and Penn, 2010), LFG (Cahill et al, 2004) and HPSG
(Zhang and Krieger, 2011) parsing.
5.1 Strategies for DS to CS conversion
The conversion from DS to CS is a non-trivial prob-
lem. One main issue in the conversion is the indeter-
minancy in the choice of a phrasal category given a
dependency relation, the level and position of attach-
ment of a dependent in the constituency structure, as
dependency relations typically do not encode such
information. To convert a DS to a CS, especially
for dependency parsing, we should consider (1) how
to transform between the topological structures, (2)
how to induce a syntactic category, and (3) how to
easily recover dependency trees from pseudo con-
stituency trees. From these three aspects, we present
the following strategies.
5.1.1 Topological structure
The topological structures represent the boundary
information of constituents in a given sentence. De-
pendency structures do not directly represent such
boundary information. Nevertheless, a complete
subtree in a projective dependency tree should be
considered as a constituent. We can construct a very
flat constituent tree, of which nodes are associated
with complete subtrees of a dependency parse. The
third tree in Figure 3 is an example of such conver-
sion.
Right-to-left binarization According to the study
in (Sun, 2010a), head words of most phrases in
Chinese are located at the first or the last position.
That means for binarizing most phrases, we only
need sequentially combine the right or left parts to-
gether with their head phrases. Main exceptions are
clauses, of which the head predicate locates inside,
since Chinese is an SVO language. To deal with
these exceptions, we split each phrase whose head
child is inside itself into three parts: left child(ren),
head and right child(ren). We first sequentially com-
bine the head and its right child(ren) that are usu-
ally objects as intermediate phrases, then sequen-
tially combine the left child(ren) until reach the orig-
inal parent node. For example, the first rewrite rule
in follows should be transferred into the second and
third types of rules.
1. Xp ? X1, ..., Xi, ..., Xm
309
2. X?p ? Xi, Xi+1; Xp?? ? Xp?, Xi+2; ...
3. X?p ? Xi?1, Xp?...?; X??p ? Xi?2, X?p; ...
This right-to-left binarization strategy is consistent
with most Chinese treebank annotation schemes.
The fourth tree in Figure 3 is an example of bina-
rized pseudo tree.
5.1.2 Phrasal category
Projection principle is introduced by Chomsky
to link together the levels of syntactic description.
It connects syntactic structures with lexical entries:
Lexical structure must be represented categorically
at every syntactic level, and representations at each
level of syntax are projected from the lexicon in that
they observe the subcategorisation properties of lex-
ical items. According to this principle, it is reason-
able to use the lexical category (POS) of the head
word as the phrasal category of a phrase.
5.1.3 Auxiliary symbol
We can use auxiliary symbols to denote the head
phrase position in a CFG rule. In other words, some
categories may be splitted into subcategories accord-
ing to if they are head phrases of their parent nodes
or which children are their head phrases. Auxiliary
symbols could be either assigned to one of the right
hand side or the left hand side. The first choice is to
conveniently use a H symbol to indicate that current
phrase is the head of its parent node. The second
choice is to practically use an L or R symbol to indi-
cate the head of current node is its left or right child,
in a binarized tree. The following table gives an ex-
ample of different rules with auxiliary symbols.
With head symbol With left/right symbol
Xl ? Xl#H, Xr Xl#L? Xl, Xr
Xr ? Xl, Xr#H Xr#R? Xl, Xr
5.2 Three conversions
Taking into account the above strategies, we propose
three concrete DS to CS conversions:
Flat conversion with H auxiliary symbol (FlatH).
Just as shown as the third tree in Figure 3, we can
learn a grammar from very flat constituency trees
where the auxiliary symbol H is used for extracting
dependencies.
Right-to-left binarizing with H auxiliary symbol
(BinH). Different from the flat conversion, we bi-
narize a tree according to the right-to-left principle.
Auxiliary symbol H is chosen.
Right-to-left binarizing with LR auxiliary sym-
bol (BinLR). Different from the second type of
conversion, we use auxiliary L/R symbols to denote
head phrases. See the fourth tree in Figure 3 for in-
stance.
Practically, every constituency parse that is pro-
duced by parsers trained with binarized trees exactly
maps to one dependency tree. However, the parser
trained with flat trees may produce very bad con-
stituency results. Sometimes, one parent node may
have zero child that is assigned with H or more than
one children that are are assigned H. In the first case,
we select the right most child as the head of such
parent, while in the second case, we select the right
most one from the children that are assigned H.
5.3 Evaluation
5.3.1 Equivalent parsing accuracy
Devel. Base Bagging(15)
CTB 83.49% 84.92%
FlatH 80.15% 83.53%
BinH 81.80% 84.64%
BinLR 82.46% 84.90%
Table 4: UAS of pseudo-grammar-based models.
Table 4 summarizes the performance of differ-
ent pseudo-grammar-based models. Compared to
the linguistic grammar learned from CTB, we can
see that pseudo grammars are very competitive.
Not that, the FlatH/BinH/BinLR trees are derived
from the CoNLL data, rather than the original CTB.
Among different DS to CS conversion strategies, the
BinLR conversion works best. More interestingly,
when we enhance the PCFGLA method by using
Bagging, the BinLR model performs as well as the
real-grammar-based model.
5.3.2 Better contribution to ensemble
The experiments above indicate that we can eas-
ily build good grammar-based dependency parser
without any constituency annotations. The fol-
lowing experiments on parser combination show
that compared to the linguistic grammar, binH and
310
Devel. UAS
Tran+Graph+CTB 86.37%
Tran+Graph+FlatH 86.14%
Tran+Graph+BinH 86.29%
Tran+Graph+BinLR 86.28%
Tran+Graph+flat+BinH+BinLR 87.03%
Tran+Graph+CTB+FlatH 86.96%
Tran+Graph+CTB+BinH 87.10%
Tran+Graph+CTB+BinLR 87.15%
Tran+Graph+CTB+BinH+BinLR 87.38%
Tran+Graph+CTB+FlatH+BinH+BinLR 87.35%
Table 5: UAS of different Bagging(15) models.
binLR grammars have equivalent contributions to
parser ensemble. Table 5 presents the ensem-
ble performance on the development data. By
Bagging, the data-driven models together with ei-
ther real grammar-based or pseudo-grammar-based
model reach a similar UAS.
5.3.3 Increased parser diversity
Since pseudo grammars are very different from
real grammars that are induced from large-scale lin-
guistic annotations. Pseudo-grammar-based parsing
models behave very differently with grammar-based
models. In other words, they increase the diver-
sity of model candidates for parser ensemble. As
a result, pseudo-grammar-based models lead to fur-
ther improvements for parser combination. Table 5
shows that the combination of data-driven, PCFG-
based and binarized pseudo-grammar-based models
is significantly better than the combination of data-
driven and PCFG-based models.
5.4 Comparison to the state-of-the-art
Table 6 summarizes the parsing performance on the
test data set, as well as the best published result re-
ported in Li et al (2012). To fairly compare the per-
formance of our parser and other systems which are
built without linguistic constituency trees, we only
use pseudo-PCFGs in this experiment. Based on
automatic POS tagging, our final model achieves a
UAS of 87.23%, which yields a relative error reduc-
tion of 24% over the best published result. Table
6 also presents the results evaluated on the CTB5
data that is more widely used for previous research.
Li et al (2011) and Hatori et al (2011) respec-
tively evaluated their graph-based and transition-
based parsers; Zhang and Clark (2011) evaluated
CoNLL-test UAS
(Li et al, 2012) 83.23%
Graph+Tran+FlatH+BinH+BinLR 87.23%
CTB5-test UAS
(Li et al, 2011) 80.79%
(Hatori et al, 2011) 81.33%
(Zhang and Clark, 2011) 81.21%
Graph+Tran+FlatH+BinH+BinLR 84.65%
Table 6: UAS of different models on the test data.
a hybrid data-driven parser. Our model is signifi-
cantly better than these systems: It achieves a UAS
of 84.65%, which obtains an error reduction of 18%
over the best system in the literature.
6 Conclusion and Future Work
There have been several attempts to develop high
accuracy parsers in both constituency and depen-
dency formalisms for Chinese, and many successful
parsing algorithms designed for English have been
applied. However, the state-of-the-art still falls far
short when compared to English. This paper stud-
ies data-driven and PCFG-based models for Chinese
dependency parsing. We present a comparative anal-
ysis of transition-, graph-, and PCFG-based parsers,
which highlights the systematic differences between
data-driven and PCFG-based models. Our analysis
may benefit parser ensemble, parser co-training, ac-
tive learning for treebank construction, and so on.
In order to exploit the diversity gain, we address
the issue of parser combination. To overcome the
limitation of the lack of constituency treebanks, we
study pseudo-grammar-based models. Experimental
results show that combining various data-driven and
PCFG-based models significantly advance the state-
of-the-art, and by converting parse trees, we can still
take advantages of the constituency representation
even without constituency annotations.
Acknowledgement
We would like to thank thank all anonymous review-
ers whose valuable comments led to signilicant re-
visions. The first author would like to thank Prof.
Hans Uszkoreit for discussion and feedback of an
early version of this work.
The work was supported by NSFC (61170166),
Beijing Nova Program (2008B03) and National
High-Tech R&D Program (2012AA011101).
311
References
Bernd Bohnet. 2010. Top accuracy and fast de-
pendency parsing is not a contradiction. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pages
89?97. Coling 2010 Organizing Committee, Bei-
jing, China. URL http://www.aclweb.
org/anthology/C10-1011.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Aoife Cahill, Michael Burke, Ruth O?Donovan,
Josef Van Genabith, and Andy Way. 2004. Long-
distance dependency resolution in automatically
acquired wide-coverage pcfg-based lfg approx-
imations. In Proceedings of the 42nd Meet-
ing of the Association for Computational Lin-
guistics (ACL?04), Main Volume, pages 319?
326. Barcelona, Spain. URL http://www.
aclweb.org/anthology/P04-1041.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the first con-
ference on North American chapter of the Associ-
ation for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics (ACL?05), pages 173?180. Associa-
tion for Computational Linguistics, Ann Arbor,
Michigan.
Wanxiang Che, Valentin Spitkovsky, and Ting
Liu. 2012. A comparison of chinese parsers
for stanford dependencies. In Proceedings
of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume
2: Short Papers), pages 11?16. Associa-
tion for Computational Linguistics, Jeju Island,
Korea. URL http://www.aclweb.org/
anthology/P12-2003.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational
Linguistics, 29(4):589?637.
Michael Collins, Jan Hajic, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser for
czech. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Lin-
guistics, pages 505?512. Association for Com-
putational Linguistics, College Park, Maryland,
USA. URL http://www.aclweb.org/
anthology/P99-1065.
Jason M. Eisner. 1996. Three new probabilis-
tic models for dependency parsing: an ex-
ploration. In Proceedings of the 16th con-
ference on Computational linguistics - Vol-
ume 1, COLING ?96, pages 340?345. Associa-
tion for Computational Linguistics, Stroudsburg,
PA, USA. URL http://dx.doi.org/10.
3115/992628.992688.
Timothy A. D. Fowler and Gerald Penn. 2010. Ac-
curate context-free parsing with combinatory cat-
egorial grammar. In Proceedings of the 48th
Annual Meeting of the Association for Com-
putational Linguistics, pages 335?344. Associ-
ation for Computational Linguistics, Uppsala,
Sweden. URL http://www.aclweb.org/
anthology/P10-1035.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2011. Incremental joint pos tag-
ging and dependency parsing in chinese. In Pro-
ceedings of 5th International Joint Conference on
Natural Language Processing, pages 1216?1224.
Asian Federation of Natural Language Process-
ing, Chiang Mai, Thailand. URL http://www.
aclweb.org/anthology/I11-1136.
John Henderson and Eric Brill. 1999. Exploiting di-
versity in natural language processing: Combin-
ing parsers. In In Proceedings of the Fourth Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 187?194.
John C. Henderson and Eric Brill. 2000. Bag-
ging and boosting a treebank parser. In Pro-
ceedings of the 1st North American chapter of
the Association for Computational Linguistics
conference, NAACL 2000, pages 34?41. Asso-
ciation for Computational Linguistics, Strouds-
burg, PA, USA. URL http://dl.acm.org/
citation.cfm?id=974305.974310.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
312
1077?1086. Association for Computational Lin-
guistics, Uppsala, Sweden. URL http://www.
aclweb.org/anthology/P10-1110.
Zhongqiang Huang, Vladimir Eidelman, and Mary
Harper. 2009. Improving a simple bigram hmm
part-of-speech tagger by latent annotation and
self-training. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics, Compan-
ion Volume: Short Papers, pages 213?216. As-
sociation for Computational Linguistics, Boulder,
Colorado. URL http://www.aclweb.org/
anthology/N/N09/N09-2054.
Zhenghua Li, Ting Liu, and Wanxiang Che.
2012. Exploiting multiple treebanks for pars-
ing with quasi-synchronous grammars. In Pro-
ceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 675?684. Associa-
tion for Computational Linguistics, Jeju Island,
Korea. URL http://www.aclweb.org/
anthology/P12-1071.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting
Liu, Wenliang Chen, and Haizhou Li. 2011.
Joint models for Chinese pos tagging and depen-
dency parsing. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 1180?1191. Association
for Computational Linguistics, Edinburgh, Scot-
land, UK. URL http://www.aclweb.org/
anthology/D11-1109.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi
Tsujii. 2005. Probabilistic cfg with latent an-
notations. In Proceedings of the 43rd An-
nual Meeting on Association for Computational
Linguistics, ACL ?05, pages 75?82. Associa-
tion for Computational Linguistics, Stroudsburg,
PA, USA. URL http://dx.doi.org/10.
3115/1219840.1219850.
RyanMcDonald. 2006. Discriminative learning and
spanning tree algorithms for dependency pars-
ing. Ph.D. thesis, University of Pennsylvania,
Philadelphia, PA, USA. AAI3225503.
Ryan McDonald and Joakim Nivre. 2007. Char-
acterizing the errors of data-driven dependency
parsing models. In Proceedings of the 2007
Joint Conference on Empirical Methods in
Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-
CoNLL), pages 122?131. Association for Com-
putational Linguistics, Prague, Czech Repub-
lic. URL http://www.aclweb.org/
anthology/D/D07/D07-1013.
Joakim Nivre. 2008. Algorithms for de-
terministic incremental dependency pars-
ing. Comput. Linguist., 34:513?553. URL
http://dx.doi.org/10.1162/coli.
07-056-R1-07-027.
Joakim Nivre and Ryan McDonald. 2008. In-
tegrating graph-based and transition-based
dependency parsers. In Proceedings of ACL-08:
HLT, pages 950?958. Association for Compu-
tational Linguistics, Columbus, Ohio. URL
http://www.aclweb.org/anthology/
P/P08/P08-1108.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, compact,
and interpretable tree annotation. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
433?440. Association for Computational Linguis-
tics, Sydney, Australia.
Kenji Sagae and Alon Lavie. 2006. Parser com-
bination by reparsing. In Proceedings of the
Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers,
NAACL-Short ?06, pages 129?132. Association
for Computational Linguistics, Stroudsburg, PA,
USA. URL http://portal.acm.org/
citation.cfm?id=1614049.1614082.
Weiwei Sun. 2010a. Improving Chinese se-
mantic role labeling with rich syntactic fea-
tures. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 168?172. Associ-
ation for Computational Linguistics, Uppsala,
Sweden. URL http://www.aclweb.org/
anthology/P10-2031.
Weiwei Sun. 2010b. Word-based and character-
based word segmentation models: Compari-
son and combination. In Proceedings of the
313
23rd International Conference on Computational
Linguistics (Coling 2010), pages 1211?1219.
Coling 2010 Organizing Committee, Beijing,
China. URL http://www.aclweb.org/
anthology/C10-2139.
Weiwei Sun and Zhifang Sui. 2009. Chinese func-
tion tag labeling. In Proceedings of the 23rd Pa-
cific Asia Conference on Language, Information
and Computation. Hong Kong.
Weiwei Sun and Hans Uszkoreit. 2012. Capturing
paradigmatic and syntagmatic lexical relations:
Towards accurate Chinese part-of-speech tagging.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics. Asso-
ciation for Computational Linguistics.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: Cheap
and good? In Human Language Technologies:
The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics, pages 649?652. Association
for Computational Linguistics, Los Angeles, Cal-
ifornia. URL http://www.aclweb.org/
anthology/N10-1091.
Andre Torres Martins, Noah Smith, and Eric Xing.
2009. Concise integer linear programming for-
mulations for dependency parsing. In Proceed-
ings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International
Joint Conference on Natural Language Process-
ing of the AFNLP, pages 342?350. Associa-
tion for Computational Linguistics, Suntec, Sin-
gapore. URL http://www.aclweb.org/
anthology/P/P09/P09-1039.
Andre? Filipe Torres Martins, Dipanjan Das, Noah A.
Smith, and Eric P. Xing. 2008. Stacking de-
pendency parsers. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 157?166. Associ-
ation for Computational Linguistics, Honolulu,
Hawaii. URL http://www.aclweb.org/
anthology/D08-1017.
Nianwen Xue. 2007. Tapping the implicit infor-
mation for the PS to DS conversion of the Chi-
nese treebank. In Proceedings of the Sixth Inter-
national Workshop on Treebanks and Linguistics
Theories.
Yi Zhang and Hans-Ulrich Krieger. 2011. Large-
scale corpus-driven pcfg approximation of an
hpsg. In Proceedings of the 12th International
Conference on Parsing Technologies, pages 198?
208. Association for Computational Linguistics,
Dublin, Ireland. URL http://www.aclweb.
org/anthology/W11-2923.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages
562?571. Association for Computational Linguis-
tics, Honolulu, Hawaii. URL http://www.
aclweb.org/anthology/D08-1059.
Yue Zhang and Stephen Clark. 2009. Transition-
based parsing of the Chinese treebank using a
global discriminative model. In Proceedings
of the 11th International Conference on Pars-
ing Technologies (IWPT?09), pages 162?171. As-
sociation for Computational Linguistics, Paris,
France. URL http://www.aclweb.org/
anthology/W09-3825.
Yue Zhang and Stephen Clark. 2011. Syntac-
tic processing using the generalized perceptron
and beam search. Comput. Linguist., 37(1):105?
151. URL http://dx.doi.org/10.1162/
coli_a_00037.
Yue Zhang and Joakim Nivre. 2011. Transition-
based dependency parsing with rich non-local fea-
tures. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 188?
193. Association for Computational Linguistics,
Portland, Oregon, USA. URL http://www.
aclweb.org/anthology/P11-2033.
Tao Zhuang and Chengqing Zong. 2010. A min-
imum error weighting combination strategy for
Chinese semantic role labeling. In Proceedings
of the 23rd International Conference on Compu-
tational Linguistics (Coling 2010), pages 1362?
1370. Coling 2010 Organizing Committee, Bei-
jing, China. URL http://www.aclweb.
org/anthology/C10-1153.
314
