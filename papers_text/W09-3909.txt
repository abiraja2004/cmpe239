Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 62?70,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Genre-Based Paragraph Classification for Sentiment Analysis 
 
 
Maite Taboada 
Department of Linguistics 
Simon Fraser University 
Burnaby, BC, Canada 
mtaboada@sfu.ca 
Julian Brooke 
Department of Computer Science 
University of Toronto 
Toronto, ON, Canada 
jbrooke@cs.toronto.edu 
Manfred Stede 
Institute of Linguistics 
University of Potsdam 
Potsdam, Germany 
stede@ling.uni-
potsdam.de 
 
  
 
 
Abstract 
We present a taxonomy and classification 
system for distinguishing between differ-
ent types of paragraphs in movie reviews: 
formal vs. functional paragraphs and, 
within the latter, between description and 
comment. The classification is used for 
sentiment extraction, achieving im-
provement over a baseline without para-
graph classification. 
1 Introduction 
Much of the recent explosion in sentiment-
related research has focused on finding low-level 
features that will help predict the polarity of a 
phrase, sentence or text. Features, widely unders-
tood, may be individual words that tend to ex-
press sentiment, or other features that indicate 
not only sentiment, but also polarity. The two 
main approaches to sentiment extraction, the se-
mantic or lexicon-based, and the machine learn-
ing or corpus-based approach, both attempt to 
identify low-level features that convey opinion. 
In the semantic approach, the features are lists of 
words and their prior polarity, (e.g., the adjective 
terrible will have a negative polarity, and maybe 
intensity, represented as -4; the noun masterpiece 
may be a 5). Our approach is lexicon-based, but 
we make use of information derived from ma-
chine learning classifiers. 
Beyond the prior polarity of a word, its local 
context obviously plays an important role in 
conveying sentiment. Polanyi and Zaenen (2006) 
use the term ?contextual valence shifters? to refer 
to expressions in the local context that may 
change a word?s polarity, such as intensifiers, 
modal verbs, connectives, and of course negation. 
Further beyond the local context, the overall 
structure and organization of the text, influenced 
by its genre, can help the reader determine how 
the evaluation is expressed, and where it lies. 
Polanyi and Zaenen (2006) also cite genre con-
straints as relevant factors in calculating senti-
ment.  
Among the many definitions of genre, we take 
the view of Systemic Functional Linguistics that 
genres are purposeful activities that develop in 
stages, or parts (Eggins and Martin, 1997), which 
can be identified by lexicogrammatical proper-
ties (Eggins and Slade, 1997). Our proposal is 
that, once we have identified different stages in a 
text, the stages can be factored in the calculation 
of sentiment, by weighing more heavily those 
that are more likely to contain evaluation, an ap-
proach also pursued in automatic summarization 
(Seki et al, 2006). 
To test this hypothesis, we created a taxonomy 
of stages specific to the genre of movie reviews, 
and annotated a set of texts. We then trained 
various classifiers to differentiate the stages. 
Having identified the stages, we lowered the 
weight of those that contained mostly description. 
Our results show that we can achieve improve-
ment over a baseline when classifying the polar-
ity of texts, even with a classifier that can stand 
to improve (at 71.1% accuracy). The best per-
formance comes from weights derived from the 
output of a linear regression classifier. 
We first describe our inventory of stages and 
the manual annotation (Section 2), and in Sec-
tion 3 turn to automatic stage classification. After 
describing our approach to sentiment classifica-
tion of texts in Section 4, we describe experi-
ments to improve its performance with the in-
formation on stages in Section 5. Section 6 dis-
62
cusses related work, and Section 7 provides con-
clusions.  
2 Stages in movie reviews 
Within the larger review genre, we focus on 
movie reviews. Movie reviews are particularly 
difficult to classify (Turney, 2002), because large 
portions of the review contain description of the 
plot, the characters, actors, director, etc., or 
background information about the film. 
Our approach is based on the work of Bieler et 
al. (2007), who identify formal and functional 
zones (stages) within German movie reviews. 
Formal zones are parts of the text that contribute 
factual information about the cast and the credits, 
and also about the review itself (author, date of 
publication and the reviewer?s rating of the mov-
ie). Functional zones contain the main gist of the 
review, and can be divided roughly into descrip-
tion and comment. Bieler et al showed that func-
tional zones could be identified using 5-gram 
SVM classifiers built from an annotated German 
corpus.  
2.1 Taxonomy 
In addition to the basic Describe/Comment dis-
tinction in Bieler et al, we use a De-
scribe+Comment label, as in our data it is often 
the case that both description and comment are 
present in the same paragraph. We decided that a 
paragraph could be labeled as De-
scribe+Comment when it contained at least a 
clause of each, and when the comment part could 
be assigned a polarity (i.e., it was not only sub-
jective, but also clearly positive or negative).  
Each of the three high-level tags has a subtag, 
a feature also present in Bieler et al?s manual 
annotation. The five subtags are: overall, plot, 
actors/characters, specific and general. ?Specific? 
refers to one particular aspect of the movie (not 
plot or characters), whereas ?general? refers to 
multiple topics in the same stage (special effects 
and cinematography at the same time). Outside 
the Comment/Describe scale, we also include 
tags such as Background (discussion of other 
movies or events outside the movie being 
reviewed), Interpretation (subjective but not 
opinionated or polar), and Quotes. Altogether, 
the annotation system includes 40 tags, with 22 
formal and 18 functional zones. Full lists of 
zone/stage labels are provided in Appendix A. 
2.2 Manual annotation 
We collected 100 texts from rottentomatoes.com, 
trying to include one positive and one negative 
review for the same movie. The reviews are part 
of the ?Top Critics? section of the site, all of 
them published in newspapers or on-line maga-
zines. We restricted the texts to ?Top Critics? 
because we wanted well-structured, polished 
texts, unlike those found in some on-line review 
sites. Future work will address those more in-
formal reviews. 
The 100 reviews contain 83,275 words and 
1,542 paragraphs. The annotation was performed 
at the paragraph level. Although stages may span 
across paragraphs, and paragraphs may contain 
more than one stage, there is a close relationship 
between paragraphs and stages. The restriction 
also resulted in a more reliable annotation, per-
formed with the PALinkA annotation tool (Ora-
san, 2003). 
The annotation was performed by one of the 
authors, and we carried out reliability tests with 
two other annotators, one another one of the au-
thors, who helped develop the taxonomy, and the 
third one a project member who read the annota-
tion guidelines1, and received a few hours? train-
ing in the labels and software. We used Fleiss? 
kappa (Fleiss, 1971), which extends easily to the 
case of multiple raters (Di Eugenio and Glass, 
2004). We all annotated four texts. The results of 
the reliability tests show a reasonable agreement 
level for the distinction between formal and 
functional zones (.84 for the 3-rater kappa). The 
lowest reliability was for the 3-way distinction in 
the functional zones (.68 for the first two raters, 
and .54 for the three raters). The full kappa val-
ues for all the distinctions are provided in Ap-
pendix B. After the reliability test, one of the 
authors performed the full annotation for all 100 
texts. Table 1 shows the breakdown of high-level 
stages for the 100 texts.  
 
Stage Count 
Describe 347 
Comment 237 
Describe+Comment 237 
Background 51 
Interpretation 22 
Quote 2 
Formal 646 
Table 1. Stages in 100 text RT corpus 
                                                 
1Available from http://www.sfu.ca/~mtaboada/nserc-
project.html 
63
3 Classifying stages 
Our first classification task aims at distinguishing 
the two main types of functional zones, Com-
ment and Describe, vs. Formal zones.  
3.1 Features 
We test two different sets of features. The first, 
following Bieler et al (2007), consists of 5-
grams (including unigrams, bigrams, 3-grams 
and 4-grams), although we note in our case that 
there was essentially no performance benefit 
beyond 3-grams. We limited the size of our fea-
ture set to n-grams that appeared at least 4 times 
in our training corpus. For the 2 class task (no 
formal zones), this resulted in 8,092 binary fea-
tures, and for the 3 and 4 class task there were 
9,357 binary n-gram features. 
The second set of features captures different 
aspects of genre and evaluation, and can in turn 
be divided into four different types, according to 
source. With two exceptions (features indicating 
whether a paragraph was the first or last para-
graph in text), the features were numerical (fre-
quency) and normalized to the length of the pa-
ragraph. 
The first group of genre features comes from 
Biber (1988), who attempted to characterize di-
mensions of genre. The features here include fre-
quency of first, second and third person pro-
nouns; demonstrative pronouns; place and time 
adverbials; intensifiers; and modals, among a 
number of others. 
The second category of genre features in-
cludes discourse markers, primarily from Knott 
(1996), that indicate contrast, comparison, causa-
tion, evidence, condition, and similar relations. 
The third type of genre features was a list of 
500 adjectives classified in terms of Appraisal 
(Martin and White, 2005) as indicating Apprec-
iation, Judgment or Affect. Appraisal categories 
have been shown to be useful in improving the 
performance of polarity classifiers (Whitelaw et 
al., 2005).  
Finally, we also include text statistics as fea-
tures, such as average length of words and sen-
tences and position of paragraphs in the text.  
3.2 Classifiers 
To classify paragraphs in the text, we use the 
WEKA suite (Witten and Frank, 2005), testing 
three popular machine learning algorithms: 
Na?ve Bayes, Support Vector Machine, and Li-
near Regression (preliminary testing with Deci-
sion Trees suggests that it is not appropriate for 
this task). Training parameters were set to default 
values. 
In order to use Linear Regression, which pro-
vides a numerical output based on feature values 
and derived feature weights, we have to conceive 
of Comment/Describe/Describe+Comment not as 
nominal (or ordinal) classes, but rather as corres-
ponding to a Comment/Describe ratio, with 
?pure? Describe at one end and ?pure? Comment 
at the other. For training, we assign a 0 value (a 
Comment ratio) to all paragraphs tagged De-
scribe and a 1 to all Comment paragraphs; for 
Describe+Comment, various options (including 
omission of this data) were tested. The time re-
quired to train a linear regression classifier on a 
large feature set proved to be prohibitive, and 
performance with smaller sets of features gener-
ally quite poor, so for the linear regression clas-
sifier we present results only for our compact set 
of genre features. 
3.3 Performance 
Table 2 shows the performance of classifi-
er/feature-set combinations for the 2-, 3-, and 4-
class tasks on the 100-text training set, with 10-
fold cross-validation, in terms of precision (P), 
recall (R) and F-measure 2 . SVM and Na?ve 
Bayes provide comparable performance, al-
though there is considerable variation, particular-
ly with respect to the feature set; the SVM is a 
significantly (p<0.05) better choice for our genre 
features 3 , while for the n-gram features the 
Bayes classification is generally preferred. The 
SVM-genre classifier significantly outperforms 
the other classifiers in the 2-class task; these ge-
nre features, however, are not as useful as 5-
grams at identifying Formal zones (the n-gram 
classifier, by contrast, can make use of words 
such as cast). In general, formal zone classifica-
tion is fairly straightforward, whereas identifica-
tion of Describe+Comment is quite difficult, and 
the SVM-genre classifier, which is more sensi-
tive to frequency bias, elects to (essentially) ig-
nore this category in order to boost overall accu-
racy.  
To evaluate a linear regression (LR) classifier, 
we calculate correlation coefficient ?, which re-
flects the goodness of fit of the line to the da-
ta. Table 3 shows values for the classifiers built 
from the corpus, with various Comment ratios
                                                 
2 For the 2- and 3-way classifiers, Describe+Comment pa-
ragraphs are treated as Comment. This balances the num-
bers of each class, ultimately improving performance. 
3 All significance tests use chi-square (?2). 
64
Classifier 
Comment Describe Formal Desc+Comm Overall 
Accuracy P R F P R F P R F P R F 
2-class-5-gram-Bayes .66 .79 .72 .70 .55 .62 - - - - - - 68.0 
2-class-5-gram-SVM .53 .63 .64 .68 .69 .69 - - - - - - 66.8 
2-class-genre-Bayes .66 .75 .70 .67 .57 .61 - - - - - - 66.2 
2-class-genre-SVM .71 .76 .74 .71 .65 .68 - - - - - - 71.1 
3-class-5-gram-Bayes .69 .49 .57 .66 .78 .71 .92 .97 .95 - - - 78.1 
3-class-5-gram-SVM .64 .63 .63 .68 .65 .65 .91 .97 .94 - - - 77.2 
3-class-genre-Bayes .68 .68 .66 .67 .46 .55 .84 .96 .90 - - - 74.0 
3-class-genre-SVM .66 .71 .68 .67 .56 .61 .90 .94 .92 - - - 76.8 
4-class-5-gram-Bayes .46 .35 .38 .69 .47 .56 .92 .97 .95 .42 .64 .51 69.0 
4-class-5-gram-SVM .43 .41 .44 .59 .62 .60 .91 .97 .94 .45 .41 .42 69.6 
4-class-genre-Bayes .38 .31 .34 .66 .30 .41 .86 .97 .90 .33 .60 .42 62.3 
4-class-genre-SVM .46 .32 .38 .53 .82 .65 .87 .94 .90 .26 .03 .06 67.4 
Table 2. Stage identification performance of various categorical classifiers 
 
(C) assigned to paragraphs with the De-
scribe+Comment tag, and with De-
scribe+Comment paragraphs removed from con-
sideration. 
 
Classifier ? 
LR, Des+Com C = 0 .37 
LR, Des+Com C = 0.25 .44 
LR, Des+Com C = 0.5 .47 
LR, Des+Com C = 0.75 .46 
LR, Des+Com C = 1 .43 
LR, No Des+Com .50 
Table 3. Correlation coefficients for LR 
classifiers 
The drop in correlation when more extreme 
values are assigned to Describe+Comment sug-
gests that Describe+Comment paragraphs do in-
deed belong in the middle of the Comment spec-
trum. Since there is a good deal of variation in 
the amount of comment across De-
scribe+Comment paragraphs, the best correlation 
comes with complete removal of these somewhat 
unreliable paragraphs. Overall, these numbers 
indicate that variations in relevant features are 
able to predict roughly 50% of the variation in 
Comment ratio, which is fairly good considering 
the small number and simplistic nature of the 
features involved. 
4 Sentiment detection: SO-CAL 
In this section, we outline our semantic orienta-
tion calculator, SO-CAL. SO-CAL extracts 
words from a text, and aggregates their semantic 
orientation value, which is in turn extracted from 
a set of dictionaries. SO-CAL uses five dictionar-
ies: four lexical dictionaries with 2,257 adjec-
tives, 1,142 nouns, 903 verbs, and 745 adverbs, 
and a fifth dictionary containing 177 intensifying 
expressions. Although the majority of the entries 
are single words, the calculator also allows for 
multiword entries written in regular expression-
like language.  
The SO-carrying words in these dictionaries 
were taken from a variety of sources, the three 
largest a corpus of 400 reviews from Epin-
ions.com, first used by Taboada and Grieve 
(2004), a 100 text subset of the 2,000 movie re-
views in the Polarity Dataset (Pang and Lee, 
2004), and words from the General Inquirer dic-
tionary (Stone, 1997). Each of the open-class 
words were given a hand-ranked SO value be-
tween 5 and -5 (neutral or zero-value words are 
not included in the dictionary) by a native Eng-
lish speaker. The numerical values were chosen 
to reflect both the prior polarity and strength of 
the word, averaged across likely interpretations. 
For example, the word phenomenal is a 5, nicely 
a 2, disgust a -3, and monstrosity a -5. The dic-
tionary was later reviewed by a committee of 
three other researchers in order to minimize the 
subjectivity of ranking SO by hand. 
Our calculator moves beyond simple averag-
ing of each word?s semantic orientation value, 
and implements and expands on the insights of 
Polanyi and Zaenen (2006) with respect to con-
textual valence shifters. We implement negation 
by shifting the SO value of a word towards the 
opposite polarity (not terrible, for instance, is 
calculated as -5+4 = -1). Intensification is mod-
eled using percentage modifiers (very engaging: 
4x125% = 5). We also ignore words appearing 
within the scope of irrealis markers such as cer-
tain verbs, modals, and punctuation, and de-
crease the weight of words which appear often in 
the text. In order to counter positive linguistic 
65
bias (Boucher and Osgood, 1969), a problem for 
lexicon-based sentiment classifiers (Kennedy and 
Inkpen, 2006), we increase the final SO of any 
negative expression appearing in the text. 
The performance of SO-CAL tends to be in 
the 76-81% range. We have tested on informal 
movie, book and product reviews and on the Po-
larity Dataset (Pang and Lee, 2004). The perfor-
mance on movie reviews tends to be on the lower 
end of the scale. Our baseline for movies, de-
scribed in Section 5, is 77.7%. We believe that 
we have reached a ceiling in terms of word- and 
phrase-level performance, and most future im-
provements need to come from discourse fea-
tures. The stage classification described in this 
paper is one of them.  
5 Results 
The final goal of a stage classifier is to use the 
information about different stages in sentiment 
classification. Our assumption is that descriptive 
paragraphs contain less evaluative content about 
the movie being reviewed, and they may include 
noise, such as evaluative words describing the 
plot or the characters. Once the paragraph clas-
sifier had assigned labels we used those labels to 
weigh paragraphs. 
5.1 Classification with manual tags 
Before moving on to automatic paragraph classi-
fication, we used the 100 annotated texts to see 
the general effect of weighting paragraphs with 
the ?perfect? human annotated tags on sentiment 
detection, in order to show the potential im-
provements that can be gained from this ap-
proach.  
Our baseline polarity detection performance 
on the 100 annotated texts is 65%, which is very 
low, even for movie reviews. We posit that for-
mal movie reviews might be particularly difficult 
because full plot descriptions are more common 
and the language used to express opinion less 
straightforward (metaphors are common). How-
ever, if we lower the weight on non-Comment 
and mixed Comment paragraphs (to 0, except for 
Describe+Comment, which is maximized by a 
0.1 weight), we are able to boost performance to 
77%, an improvement which is significant at the 
p<0.05 level. Most of the improvement (7%) is 
due to disregarding Describe paragraphs, but 2% 
comes from Describe+Comment, and 1% each 
from Background, Interpretation, and (all) For-
mal tags. There is no performance gain, however, 
from the use of aspect tags (e.g., by increasing 
the weight on Overall paragraphs), justifying our 
decision to ignore subtags for text-level polarity 
classification.  
5.2 Categorical classification 
We evaluated all the classifiers from Table 2, but 
we omit discussion of the worst performing. The 
evaluation was performed on the Polarity Dataset 
(Pang and Lee, 2004), a collection of 2,000 on-
line movie reviews, balanced for polarity. The 
SO performance for the categorical classifiers is 
given in Figure 1. When applicable, we always 
gave Formal Zones (which Table 2 indicates are 
fairly easy to identify) a weight of 0, however for 
Describe paragraphs we tested at 0.1 intervals 
between 0 and 1. Testing all possible values of 
Describe+Comment was not feasible, so we set 
the weights of those to a value halfway between 
the weight of Comment paragraphs (1) and the 
weight of the Describe paragraph. 
Most of the classifiers were able to improve 
performance beyond the 77.7% (unweighted) 
baseline. The best performing model (the 2-
class-genre-SVM) reached a polarity identifica-
tion accuracy of 79.05%, while the second best 
(the 3-class 5-gram-SVM) topped out at 78.9%. 
Many of the classifiers showed a similar pattern 
with respect to the weight on Describe, increas-
ing linearly as weight on Describe was decreased 
before hitting a maximum in the 0.4-0.1 range, 
and then dropping afterwards (often precipitous-
ly). Only the classifiers which were more con-
servative with respect to Describe, such as the 4-
class-5-gram-Bayes, avoided the drop, which can 
be attributed to low precision Describe identifi-
cation: At some point, the cost associated with 
disregarding paragraphs which have been mis-
tagged as Describe becomes greater that the ben-
efit of disregarding correctly-labeled ones. In-
deed, the best performing classifier for each class 
option is exactly the one that has the highest pre-
cision for identification of Describe, regardless 
of other factors. This suggests that improving 
precision is key, and, in lieu of that, weighting is 
a better strategy than simply removing parts of 
the text. 
In general, increasing the complexity of the 
task (increasing the number of classes) decreases 
performance. One clear problem is that the iden-
tification of Formal zones, which are much more 
common in our training corpus than our test cor-
pus, does not add important information, since 
most Formal zones have no SO valued words. 
The delineation of an independent De-
scribe+Comment class is mostly ineffective, 
66
 
Figure 1. SO Performance with various paragraph tagging classifiers, by weight on Describe 
 
probably because this class is not easily distin-
guishable from Describe and Comment (nor in 
fact should it be). 
We can further confirm that our classifier is 
properly distinguishing Describe and Comment 
by discounting Comment paragraphs rather than 
Describe paragraphs (following Pang and Lee 
2004). When Comment paragraphs tagged by the 
best performing classifier are ignored, SO-CAL?s 
accuracy drops to 56.65%, just barely above 
chance. 
5.3 Continuous classification 
Table 4 gives the results for the linear regression 
classifier, which assigns a Comment ratio to each 
paragraph used for weighting.  
 
Model Accuracy 
LR, Des+Com C = 0 78.75 
LR, Des+Com C = 0.25 79.35 
LR, Des+Com C = 0.5 79.00 
LR, Des+Com C = 0.75 78.90 
LR, Des+Com C = 1 78.95 
LR, No Des+Com 79.05 
Table 4. SO Performance with linear regression 
 
The linear regression model trained with a 
0.25 comment ratio on Describe+Comment para-
graphs provides the best performance of all clas-
sifiers we tested (an improvement of 1.65% from 
baseline). The correlation coefficients noted 
in Table 4 are reflected in these results, but the 
spike at C = 0.25 is most likely related to a gen-
eral preference for low (but non-zero) weights on 
Describe+Comment paragraphs also noted when 
weights were applied using the manual tags; 
these paragraphs are unreliable (as compared to 
pure Comment), but cannot be completely dis-
counted. There were some texts which had only 
Describe+Comment paragraphs.  
Almost a third of the tags assigned by the 2-
class genre feature classifier were different than 
the corresponding n-gram classifier, suggesting 
the two classifiers might have different strengths. 
However, initial attempts to integrate the various 
high performing classifiers?including collaps-
ing of feature sets, metaclassifiers, and double 
tagging of paragraphs?resulted in similar or 
worse performance. We have not tested all poss-
ible options (there are simply too many), but we 
think it unlikely that additional gains will be 
made with these simple, surface feature sets. Al-
though our testing with human annotated texts 
and the large performance gap between movie 
reviews and other consumer reviews both sug-
gest there is more potential for improvement, it 
will probably require more sophisticated and 
precise models. 
6 Related work 
The bulk of the work in sentiment analysis has 
focused on classification at either the sentence 
level, e.g., the subjectivity/polarity detection of 
Wiebe and Riloff (2005), or alternatively at the 
level of the entire text. With regards to the latter, 
two major approaches have emerged: the use of 
machine learning classifiers trained on n-grams 
77
78
79
80
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
SO
?C
al
cu
la
to
r?
A
cc
ur
ac
y
Weight?on?Describe?Paragraph
No?tagging?Baseline
2?Class?5?gram?SVM
2?Class?5?gram?Bayes
2?Class?genre?Bayes
2?Class?genre?SVM
3?Class?5?gram?Bayes
3?Class?5?gram?SVM
3?Class?genre?Bayes
4?Class?5?gram?Bayes
4?Class?5?gram?SVM
4?Class?genre?Bayes
67
or similar features (Pang et al, 2002), and the 
use of sentiment dictionaries (Esuli and Sebas-
tiani, 2006; Taboada et al, 2006). Support Vec-
tor Machine (SVM) classifiers have been shown 
to out-perform lexicon-based models within a 
single domain (Kennedy and Inkpen, 2006); 
however they have trouble with cross-domain 
tasks (Aue and Gamon, 2005), and some re-
searchers have argued for hybrid classifiers (An-
dreevskaia and Bergler, 2008). 
Pang and Lee (2004) attempted to improve the 
performance of an SVM classifier by identifying 
and removing objective sentences from the texts. 
Results were mixed: The improvement was mi-
nimal for the SVM classifier (though the perfor-
mance of a na?ve Bayes classifier was signifi-
cantly boosted), however testing with parts of the 
text classified as subjective showed that the elim-
inated parts were indeed irrelevant. In contrast to 
our findings, they reported a drop in performance 
when paragraphs were taken as the only possible 
boundary between subjective and objective text 
spans. 
Other research that has dealt with identifying 
more or less relevant parts of the text for the pur-
poses of sentiment analysis include Taboada and 
Grieve (2004), who improved the performance of 
a lexicon-based model by weighing words to-
wards the end of the text; Nigam and Hurst 
(2006), who detect polar expressions in topic 
sentences; and Voll and Taboada (2007), who 
used a topic classifier and discourse parser to 
eliminate potentially off-topic or less important 
sentences. 
7 Conclusions 
We have described a genre-based taxonomy for 
classifying paragraphs in movie reviews, with 
the main classification being a distinction be-
tween formal and functional stages, and, within 
those, between mainly descriptive vs. comment 
stages. The taxonomy was used to annotate 100 
movie reviews, as the basis for building classifi-
ers.  
We tested a number of different classifiers. 
Our results suggest that a simple, two-way or 
continuous classification using a small set of lin-
guistically-motivated features is the best for our 
purposes; a more complex system is feasible, but 
comes at the cost of precision, which seems to be 
the key variable in improving sentiment analysis. 
Ultimately, the goal of the classification was 
to improve the accuracy of SO-CAL, our seman-
tic orientation calculator. Using the manual an-
notations, we manage to boost performance by 
12% over the baseline. With the best automatic 
classifier, we still show consistent improvement 
over the baseline. Given the relatively low accu-
racy of the classifiers, the crucial factor involves 
using fine-grained weights on paragraphs, rather 
than simply ignoring Describe-labeled para-
graphs, as Pang and Lee (2004) did for objective 
sentences.  
An obvious expansion to this work would in-
volve a larger dataset on which to train, to im-
prove the performance of the classifier(s). We 
would also like to focus on the syntactic patterns 
and verb class properties of narration, aspects 
that are not captured with simply using words 
and POS labels. Connectives in particular are 
good indicators of the difference between narra-
tion (temporal connectives) and opinion (contras-
tive connectives). There may also be benefit to 
combining paragraph- and sentence-based ap-
proaches. Finally, we would like to identify 
common sequences of stages, such as plot and 
character descriptions appearing together, and 
before evaluation stages. This generic structure 
has been extensively studied for many genres 
(Eggins and Slade, 1997). 
Beyond sentiment extraction, our taxonomy 
and classifiers can be used for searching and in-
formation retrieval. One could, for instance, ex-
tract paragraphs that include mostly comment or 
description. Using the more fine-grained labels, 
searches for comment/description on actors, di-
rectors, or other aspects of the movie are possible. 
Acknowledgements 
This work was supported by SSHRC (410-2006-
1009) and NSERC (261104-2008) grants to 
Maite Taboada. 
References 
Andreevskaia, Alina & Sabine Bergler. 2008. When 
specialists and generalists work together: Domain 
dependence in sentiment tagging. Proceedings of 
46th Annual Meeting of the Association for Com-
putational Linguistics (pp. 290-298). Columbus, 
OH. 
Aue, Anthony & Michael Gamon. 2005. Customizing 
sentiment classifiers to new domains: A case study. 
Proceedings of the International Conference on 
Recent Advances in Natural Language Processing. 
Borovets, Bulgaria. 
Biber, Douglas. 1988. Variation across Speech and 
Writing. Cambridge: Cambridge University Press. 
68
Bieler, Heike, Stefanie Dipper & Manfred Stede. 
2007. Identifying formal and functional zones in 
film reviews. Proceedings of the 8th SIGdial 
Workshop on Discourse and Dialogue (pp. 75-78). 
Antwerp, Belgium. 
Boucher, Jerry D. & Charles E. Osgood. 1969. The 
Pollyanna hypothesis. Journal of Verbal Learning 
and Verbal Behaviour, 8: 1-8. 
Di Eugenio, Barbara & Michael Glass. 2004. The 
kappa statistic: A second look. Computational Lin-
guistics, 30(1): 95-101. 
Eggins, Suzanne & James R. Martin. 1997. Genres 
and registers of discourse. In Teun A. van Dijk 
(ed.), Discourse as Structure and Process. Dis-
course Studies: A Multidisciplinary Introduction 
(pp. 230-256). London: Sage. 
Eggins, Suzanne & Diana Slade. 1997. Analysing 
Casual Conversation. London: Cassell. 
Esuli, Andrea & Fabrizio Sebastiani. 2006. Senti-
WordNet: A publicly available lexical resource for 
opinion mining. Proceedings of 5th International 
Conference on Language Resources and Evaluation 
(LREC) (pp. 417-422). Genoa, Italy. 
Fleiss, Joseph L. 1971. Measuring nominal scale 
agreement among many raters. Psychological Bul-
letin, 76: 378-382. 
Kennedy, Alistair & Diana Inkpen. 2006. Sentiment 
classification of movie and product reviews using 
contextual valence shifters. Computational Intelli-
gence, 22(2): 110-125. 
Knott, Alistair. 1996. A Data-Driven Methodology for 
Motivating a Set of Coherence Relations. Edin-
burgh, UK: University of EdinburghThesis Type. 
Martin, James R. & Peter White. 2005. The Language 
of Evaluation. New York: Palgrave. 
Nigam, Kamal & Matthew Hurst. 2006. Towards a 
robust metric of polarity. In Janyce Wiebe (ed.), 
Computing Attitude and Affect in Text: Theory 
and Applications (pp. 265-279). Dordrecht: Sprin-
ger. 
Orasan, Constantin. 2003. PALinkA: A highly custo-
mizable tool for discourse annotation. Proceedings 
of 4th SIGdial Workshop on Discourse and Dialog 
(pp. 39 ? 43). Sapporo, Japan. 
Pang, Bo & Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. Proceedings of 
42nd Meeting of the Association for Computation-
al Linguistics (pp. 271-278). Barcelona, Spain. 
Pang, Bo, Lillian Lee & Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment classification using 
Machine Learning techniques. Proceedings of Con-
ference on Empirical Methods in NLP (pp. 79-86). 
Polanyi, Livia & Annie Zaenen. 2006. Contextual 
valence shifters. In James G. Shanahan, Yan Qu & 
Janyce Wiebe (eds.), Computing Attitude and Af-
fect in Text: Theory and Applications (pp. 1-10). 
Dordrecht: Springer. 
Seki, Yohei, Koji Eguchi & Noriko Kando. 2006. 
Multi-document viewpoint summarization focused 
on facts, opinion and knowledge. In Janyce Wiebe 
(ed.), Computing Attitude and Affect in Text: 
Theory and Applications (pp. 317-336). Dordrecht: 
Springer. 
Stone, Philip J. 1997. Thematic text analysis: New 
agendas for analyzing text content. In Carl Roberts 
(ed.), Text Analysis for the Social Sciences. Mah-
wah, NJ: Lawrence Erlbaum. 
Taboada, Maite, Caroline Anthony & Kimberly Voll. 
2006. Creating semantic orientation dictionaries. 
Proceedings of 5th International Conference on 
Language Resources and Evaluation (LREC) (pp. 
427-432). Genoa, Italy. 
Taboada, Maite & Jack Grieve. 2004. Analyzing ap-
praisal automatically. Proceedings of AAAI Spring 
Symposium on Exploring Attitude and Affect in 
Text (AAAI Technical Report SS-04-07) (pp. 158-
161). Stanford University, CA. 
Turney, Peter. 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised clas-
sification of reviews. Proceedings of 40th Meeting 
of the Association for Computational Linguistics 
(pp. 417-424). 
Voll, Kimberly & Maite Taboada. 2007. Not all 
words are created equal: Extracting semantic orien-
tation as a function of adjective relevance. Pro-
ceedings of the 20th Australian Joint Conference 
on Artificial Intelligence (pp. 337-346). Gold 
Coast, Australia. 
Whitelaw, Casey, Navendu Garg & Shlomo Arga-
mon. 2005. Using Appraisal groups for sentiment 
analysis. Proceedings of ACM SIGIR Conference 
on Information and Knowledge Management 
(CIKM 2005) (pp. 625-631). Bremen, Germany. 
Wiebe, Janyce & Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unan-
notated texts. Proceedings of Sixth International 
Conference on Intelligent Text Processing and 
Computational Linguistics (CICLing-2005). Mex-
ico City, Mexico. 
Witten, Ian H. & Eibe Frank. 2005. Data Mining: 
Practical Machine Learning Tools and Techniques 
(2nd edn.). San Francisco: Morgan Kaufmann. 
 
69
Appendix A: Full lists of formal and functional zones 
 
 
Figure A1. Functional zones 
 
 
Figure A2. Formal zones 
 
Describe
Comment
Plot
Character
Specific
General
Content
Plot
Actors+characters
Specific
General
Overall
Plot
Actors+characters
Specific
General
Content
Structural
elements
Information
about the
film
Tagline
Structure
Off-topic
Title, Title+year, Runtime,
Country+year, Director,
Genre, Audience-restriction,
Cast, Credits, Show-Loc+date,
Misc-Movie-Info
Source, Author, Author-Bio,
Place, Date, Legal-Notice,
Misc-Review-Info, Rating
 
 
Appendix B: Kappa values for annotation task 
 
Classes 2-rater 
kappa 
3-rater 
kappa 
Describe/Comment/Describe+Comment/Formal .82 .73 
Describe/Comment/Formal .92 .84 
Describe/Comment/Describe+Comment .68 .54 
Describe/Comment .84 .69 
Table B1. Kappa values for stage annotations 
 
 
70
