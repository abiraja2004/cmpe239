Proceedings of the 8th Workshop on Asian Language Resources, pages 30?37,
Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language Processing
Augmenting a Bilingual Lexicon with Information       
for Word Translation Disambiguation
Takashi Tsunakawa
Faculty of Informatics
Shizuoka University
tuna@inf.shizuoka.ac.jp
Hiroyuki Kaji
Faculty of Informatics
Shizuoka University
kaji@inf.shizuoka.ac.jp
Abstract
We describe a method for augmenting
a bilingual lexicon with additional in-
formation for selecting an appropriate 
translation word. For each word in the 
source language, we calculate a corre-
lation matrix of its association words 
versus its translation candidates. We 
estimate the degree of correlation by
using comparable corpora based on 
these assumptions: ?parallel word as-
sociations? and ?one sense per word 
association.? In our word translation 
disambiguation experiment, the results 
show that our method achieved 42% 
recall and 49% precision for Japa-
nese-English newspaper texts, and 45% 
recall and 76% precision for Chi-
nese-Japanese technical documents.
1 Introduction
The bilingual lexicon, or bilingual dictionary, 
is a fundamental linguistic resource for multi-
lingual natural language processing (NLP). For 
each word, multiword, or expression in the 
source language, the bilingual lexicon provides 
translation candidates representing the original 
meaning in the target language.
Selecting the right words for translation is a 
serious problem in almost all of multilingual 
NLP. One word in the source language almost 
always has two or more translation candidates 
in the target language by looking up them in 
the bilingual lexicon. Because each translation
candidate has a distinct meaning and property, 
we must be careful in selecting the appropriate 
translation candidate that has the same sense as 
the word inputted. This task is often called 
word translation disambiguation.
In this paper, we describe a method for add-
ing information for word translation disam-
biguation into the bilingual lexicon. Compara-
ble corpora can be used to determine which 
word associations suggest which translations 
of the word (Kaji and Morimoto, 2002). First, 
we extract word associations in each language 
corpus and align them by using a bilingual dic-
tionary. Then, we construct a word correlation 
matrix for each word in the source language. 
This correlation matrix works as information 
for word translation disambiguation.
We carried out word translation experiments 
on two settings: English-to-Japanese and Chi-
nese-to-Japanese. In the experiments, we tested 
Dice/Jaccard coefficients, pointwise mutual 
information, log-likelihood ratio, and Student?s
t-score as the association measures for extract-
ing word associations.
2 Constructing word correlation ma-
trices for word translation disam-
biguation
2.1 Outline of our method
In this section, we describe the method for 
calculating a word correlation matrix for each 
word in the source language. The correlation 
matrix for a word f consists of its association 
words and its translation candidates. Among 
the translation candidates, we choose the most 
acceptable one that is strongly suggested by its 
association words occurring around f.
We use two assumptions for this framework:
(i)  Parallel word associations:
Translations of words associated with 
each other in a language are also asso-
ciated with each other in another language 
30
(Rapp, 1995). For example, two English 
words ?tank? and ?soldier? are associated 
with each other and their Japanese trans-
lations ??? (sensha)? and ??? (hei-
shi)? are also associated with each other.
(ii)  One sense per word association:
A polysemous word exhibits only one 
sense of a word per word association 
(Yarowsky, 1993). For example, a poly-
semous word ?tank? exhibits the ?military
vehicle? sense of a word when it is asso-
ciated with ?soldier,? while it exhibits the 
?container for liquid or gas? sense when it 
is associated with ?gasoline.?
Under these assumptions, we determine which 
of the words associated with an input word 
suggests which of its translations by aligning 
word associations by using a bilingual dictio-
nary. Consider the associated English words 
(tank, soldier) and their Japanese translations 
(?? (sensha), ?? (heishi)). When we 
translate the word ?tank? into Japanese, the 
associated word ?soldier? helps us to translate 
it into ??? (sensha)?, not to translate it into 
???? (tanku)? which means ?a storage 
tank.?
This naive method seems to suffer from the 
following difficulties:
 A disparity in topical coverage between 
two corpora in two languages
 A shortage in the bilingual dictionary
 The existence of polysemous associated 
words that cannot determine the correct 
sense of the input word
For these difficulties, we use the tendency that 
the two words associated with a third word are 
likely to suggest the same sense of the third 
word when they are also associated with each 
other. For example, consider an English asso-
ciated word pair (tank, troop). The word ?troop?
cannot distinguish the different meanings be-
cause it can co-occur with the word ?tank? in 
both senses of the word. The third word ?sol-
dier,? which is associated with both ?tank? and 
?troop,? can suggest the translation ???
(sensha).?
The overview of our method is shown in
Figure 1. We first extract associated word pairs 
in the source and target languages from com-
parable corpora. Using a bilingual dictionary,
we obtain alignments of these word associa-
tions. Then, we iteratively calculate a correla-
tion matrix for each word in the source lan-
guage. Finally, we select the translation with 
the highest correlation from the translation 
candidates of the input word and the 
co-occurring words.
For each input word in the source language, 
we calculate correlation values between their 
translation candidates and their association 
words. The algorithm is shown in Figure 2.
In Algorithm 1, the initialization of correla-
tion values is based on word associations, 
where D is a set of word pairs in the bilingual 
dictionary, and Af and Ae are the sets of asso-
ciated word pairs. First, we retain associated 
words f?(i) when its translation e? exists and 
when e? is associated with e. In the iteration, 
the correlation values of associated words f?(i)
that suggest e(j) increase relatively by using 
association scores ((), ) and
Figure 1. Overview of our method.
31
((), ). In our experiments, we set the 
number of iterations Nr to 10.
2.2 Alternative association measures for 
extracting word associations
We extract co-occurring word pairs and calcu-
late their association scores. In this paper, we 
focus on some frequently used metrics for 
finding word associations based on their oc-
currence/co-occurrence frequencies.
Suppose that words x and y frequently 
co-occur. Let n1 and n2 be the occurrence fre-
quencies of x and y respectively, and let m be 
the frequency that x and y co-occur between w
content words. The parameter w is a window 
size that adjusts the range of co-occurrences.
Let N and M be the sum of occur-
rences/co-occurrences of all words/word pairs, 
respectively. The frequencies are summarized 
in Table 1.
The word association scores (, ) are de-
fined as follows:
 Dice coefficient (Smadja, 1993)
Dice(, ) =
2
	
 + 	
(1)
 Jaccard coefficient (Smadja et al, 1996)
Jaccard(, ) =

	
 + 	  
(2)
 Pointwise mutual information (pMI) 
(Church and Hanks, 1990)
pMI(, ) = log
/
(	
  )(	  )
(3)
 Log-likelihood ratio (LLR) (Dunning, 
1993)
LLR(, )
= 2logL(, 	
, )
+ logL(	  ,   	
, )
 logL(, 	
, 
)
 logL(	  ,   	
, ); (4)
logL(, 	, ) =
 log  + (	  ) log(1  ), (5)

 =

	

,  =
	  
  	

,  =
	

(6)
 Student?s t-score (TScore) (Church et al, 
1991)
TScore(, ) =
  	
	 

(7)
We calculate association scores for all pairs 
of words when their occurrence frequencies 
are not less than a threshold Tf and when their
x
occur
x not 
occur
Total
y
occur
m n2 ? m n2
y not 
occur
n1 ? m M ? n1
? n2 + m
N ? n2
Total n1 N ? n1 N
Table 1. Contingency matrix of occurrence 
frequencies.
Figure 2. Algorithm for calculating correlation 
matrices.
?
 ((), )  (, ())
max

 ((), )   (, ())

(, ) =                                                    
          !
1 (": (, ) # $, (, ) # %)
0 (otherwise)
&'
Algorithm 1:
Input:
f: an input word
f?(1), ?, f?(I): associated words of f
e(1), ?, e(J): translation candidates 
of f
Nr: number of iterations
A bilingual lexicon
Word association scores  for both 
languages
Output:
Cf = [Cf (f?(i), e(j))]: a correlation ma-
trix for f
1: if  *(), () - 0 then
2:   
*(), ()  .
345
6(7),8(9)<
 346(7),8()>
3: else
4:   
*(), ()  . 0
5: end
6:  . 0
7: while i < Nr
8:    .  + 1
9:   
*(), ()  .  ((), )
10: end
(?: = ?{**|(,**)#@4,(*(7),**)#@4})
32
threshold Tc.
We handle word pairs whose association 
scores are not less than a predefined value TA;
some of the thresholds were evaluated in our 
experiment. The associated word pair sets Af
and Ae in Algorithm 1 includes only word pairs 
whose scores are not less than TA in the source 
and target language, respectively.
3 Word Translation Disambiguation
Consider that a translator changes a word in an 
input sentence. Usually, two or more transla-
tion candidates are enumerated in the bilingual 
dictionary for a word. The translator should 
select a translation word that is grammatically/
syntactically correct, semantically equivalent
to the input, and pragmatically appropriate.
We assume that the translation word e for an 
input word f tends to be selected if words oc-
curring around f are strongly correlated with e.
Using the correlation matrices, we select e as a 
translation if the associated word f? occurs 
around f and the score Cf(f?,e) is large. In addi-
tion, we take distance between f and f? into 
account.
We define the score of the translation word 
e(f0) for an input word f0 as follows. Consider 
an input word f0 that occurs in the context of 
?? f-2 f-1 f0 f1 f2 ?.? The score for a trans-
lation word (A) for an input word f0 is
defined as where p is the relative position of 
the words surrounding f0, B 5C, (A)< is 
the value of the correlation matrix for f0, and E
Figure 3. An example of an input word ?home?
is the window size for word translation disam-
biguation.
A simple example is shown in Figure 3. The 
word ?home? in this context means the sense 
of ?home base? used in baseball games, not 
?house? or ?hometown.? The surrounding 
words such as ?games,? ?bases,? and ?runs?
can be clues for indicating the correct sense of 
the word. By using the correlation matrix (Ta-
ble 2) and formula (8), we calculate a score for 
each translation candidate and select the best 
translation with the largest score. In this case, 
Score(?? (honrui)) = 0.1134 was the best 
score and the correct translation was selected.
4 Experiment
4.1 Experimental settings
We carried out word translation experiments 
on two different settings. In the first experi-
ment (Experiment A), we used large-scale 
comparable corpora from English and Japanese 
newspaper texts. The second experiment (Ex-
periment B) was targeted at translating tech-
nical terms by using Chinese and Japanese 
domain-specific corpora.
We used the following linguistic resources 
for the experiments:
 Experiment A
<home> ?
(kuni)
[country]
??
(honrui)
[home base]
?
(ie)
[house]
??
(jitaku)
[my home]
??
(katei)
[homeplace]
??
(shisetsu)
[facilities]
base 0.009907 0.017649 0.005495 0.006117 0.005186 0.005597
game 0.043507 0.048358 0.025145 0.028208 0.019987 0.023014
Kansas 0.010514 0.003786 0.004280 0.007307 0.004320 0.005459
run 0.023468 0.042035 0.014430 0.015765 0.012061 0.012986
season 0.044855 0.050952 0.025406 0.028506 0.020716 0.023631
Table 2. Word correlation matrix for a word ?home,?
as information for word translation disambiguation
Score(A)
= F
1
G|H|
B 5C, (A)< ,

K|C|KN
(8)
A career .284 hitter, Beltran batted .267 
in the regular season, split between 
Kansas City and Houston, but came 
alive in the playoffs. He hit .435 in 12 
postseason games, with six stolen bases,
eight home runs and 14 runs batted in.
33
 Training comparable corpora
 The New York Times texts from 
English Gigaword Corpus 
Fourth Edition (LDC2009T13): 
1.6 billion words
 The Mainichi Shimbun Corpus 
(2000-2005): 195 million words
 Test corpus
 A part of The New York Times 
(January 2005): 157 paragraphs, 
1,420 input words
 Experiment B1
 Training comparable corpus
 In-house Chinese-Japanese pa-
rallel corpus in the environment
domain: 53,027 sentence pairs1
 Test corpus
 A part of the training data: 1,443 
sentences, 668 input words
 Experiment B2
 Training comparable corpus
 In-house Chinese-Japanese pa-
rallel corpus in the medical do-
main: 123,175 sentence pairs
 Test corpus
 A part of the training data: 940 
sentences, 3,582 input words
 Dictionaries
 Japanese-English bilingual dictiona-
ries: Total 333,656 term pairs
 EDR Electronic Dictionary
 Eijiro, Third Edition
 EDICT (Breen, 1995)
 Chinese-English bilingual dictionary
 Chinese-English Translation 
Lexicon Version 3.0 (LDC
2002L27): 54,170 term pairs
 Wanfang Data Chinese-English 
Science/Technology Bilingual 
Dictionary: 525,259 term pairs
For the Chinese-Japanese translation, we gen-
erated a Chinese-Japanese bilingual dictionary 
by merging Chinese-English and Japa-
nese-English dictionaries. The Chi-
nese-Japanese bilingual dictionary includes 
1 We could prepare only parallel corpora for Chi-
nese-Japanese language pair as training corpora. 
For our experiments, we assumed them as compa-
rable corpora and did not use the correspondence of 
sentence pairs.
every Chinese-Japanese term pair (tC, tJ) when
(tC, tE) and (tJ, tE) were present in the dictiona-
ries for one or more English terms tE. This 
merged dictionary contains about two million 
term pairs. While these Chinese-Japanese term 
pairs include wrong translations, it was not a 
serious problem in our experiments because 
wrong translations were excluded in the pro-
cedure of our method.
We applied morphological analysis and 
part-of-speech tagging by using TreeTagger 
(Schmid, 1994) for English, JUMAN for Jap-
anese, and mma (Kruengkrai et al, 2009) for 
Chinese, respectively.
In the test corpus, we manually annotated 
reference translations for each target word. 2
Experiment A:
The parameters we used were as follows:
Tf = 100 (Japanese), Tf = 1000 (English),
Tc = 4, w = 30, E = 30.
Experiment B1/B2:
Tf = 100, Tc = 4, w = 10, E = 25.
Some of the parameters were empirically ad-
justed.
In the experiments, the matrices could be 
obtained for 9103 English words (A), 674 
Chinese words (B1) and 1258 Chinese words 
(B2), respectively. In average one word had 
3.24 (A), 1.15 (B1) and 1.51 (B2) translation 
candidates3
4.2 Results of English-Japanese word 
translation
by using the best setting. Table 2 
is the resulted matrix for the word ?home? in 
the Experiment A.
Table 3 shows the results of Experiment A. We 
classified the translation results for 1,420 
target English words into four categories: True, 
False, R, and M. When the translation was 
output, the result was True if the output is 
included in the reference translations, and it 
was False otherwise. The result was R when all 
the associated words in the correlation matrix 
2 We prepared multiple references for several tar-
get words. The average numbers of reference trans-
lations for an input word are 1.84 (A), 1.50 (B1), 
and 1.48 (B2), respectively.
3 From each matrix, we cut off the columns with 
translations that do not have the best scores for any 
associated words, because such translations are 
never selected.
34
did not occur around the input word. The result 
was M when no correlation matrix existed for 
the input word. We did not select a translation
output in these cases. The recall and precision 
are shown in the parentheses below.
 Recall = (True) / (Number of input words)
 Precision = (True) / (True + False)
Among the settings, we obtained the best 
results, 42% recall and 49% precision, when 
we used the Jaccard coefficient for association 
scores and TA = 0, which means all pairs were 
taken into consideration. Among other settings, 
the Dice coefficient achieved a comparable 
performance with Jaccard.
4.3 Results of Chinese-Japanese word 
translation
Tables 4 and 5 show the results of Experiment 
B. In each domain, we tested only the settings 
on Dice, pMI, and LLR with TA = 0.
In the environmental domain, the pointwise 
mutual information score achieved the best 
performance, 45% recall and 76% precision. 
However, the Dice coefficient gave the best 
recall (55%) for the medical domain. This re-
sult indicates that Experiment B1/B2 had 
higher precision and more words without the
correlation matrix than Experiment A had.
4.4 Discussion
As a result, we could generate bilingual lex-
icons with word translation disambiguation 
information for 9103 English words and 1932 
Chinese words. Although the number of words 
might be augmented by changing the settings, 
the size does not seem to be sufficient as bi-
lingual dictionaries. The availability of larger
output should be investigated.
The experimental results show that our me-
thod selected correct translations for at least
half of the input words if a correlation matrix 
existed and if the associated words co-occur. 
Among all input words, at least 40% of the
input words can be translated. The bilingual 
dictionaries included 24.4, 38.6, and 52.0
translation candidates for one input word in 
Experiment A, B1, and B2, respectively. When 
we select the most frequent word, the preci-
sions were 7%, 1%, and 1%, respectively.
Meanwhile, the average numbers of translation
Table 3. Results of English-Japanese word 
translation (A)
Table 4. Results of Chinese-Japanese word
translation for environmental domain (B1).
Table 5. Results of Chinese-Japanese word 
translation for medical domain (B2).
candidates in the correlation matrices for one 
input word are shown in Table 6. These indi-
cate that our method effectively removed noisy 
Score TA True False R M
Dice 0 588 627 90 115
(41%/48%)
0.001 586 619 94 121
(41%/49%)
0.01 479 507 243 191
(34%/49%)
Jacc-
ard
0 594 621 90 115
(42%/49%)
0.001 584 609 105 122
(41%/49%)
0.01 348 378 374 320
(25%/48%)
pMI 0 292 309 704 115
(21%/49%)
1 293 308 703 116
(21%/49%)
LLR 10 530 747 28 115
(37%/42%)
100 529 744 32 115
(37%/42%)
T-
Score
1 486 793 26 115
(34%/38%)
4 489 787 26 118
(34%/38%)
Score TA True False R M
Dice 0 1984 895 82 621
(55%/69%)
pMI 0 1886 804 271 621
(53%/70%)
LLR 0 1652 1246 63 621
(46%/57%)
Score TA True False R M
Dice 0 277 124 14 253
(41%/69%)
pMI 0 303 95 17 253
(45%/76%)
LLR 0 269 131 15 253
(40%/67%)
35
translations from the Chinese-Japanese dictio-
nary merged Japanese-English and Chi-
nese-English dictionaries, and that the associa-
tion scores contributed word translation dis-
ambiguation.
Among the settings, the Jaccard/Dice coef-
ficients were proven to be effective, although 
pointwise mutual information (pMI) was also 
effective for technical domains and the Chi-
nese-Japanese language pair. Because the Jac-
card/Dice coefficients were originally used for 
measuring the proximity of sets, these might 
be effective for collecting related words by 
using the similarity of kinds of co-occurring 
words. However, pMI tends to emphasize
low-frequency words as associated words. The 
consequence of this tendency might be that 
low-frequency associated words do not appear 
around the input word in the newspaper text.4
In most metrics for the association score, the 
lowest threshold value TA achieved the best 
performance. This result indicates that the 
cut-off of associated words by some thresholds 
was not effective, although it requires more
time and memory space to obtain correlation 
matrices without cut-off. How to optimize oth-
er parameters in our method remains unsolved.
More words without the correlation matrix 
were present in Experiment B1/B2 than in Ex-
periment A because the input word was often a 
technical term that was not in the bilingual dic-
tionary. The better recall and precision of Ex-
periment B1/B2 came from several reasons,
including difference of test sets and language 
pairs. In addition, it might have an impact on 
this result that the fact that word translation 
disambiguation of technical terms is easier 
than word translation disambiguation of com-
mon words.
We handled only nouns as input words and
associated words in this study. Considering 
only the co-occurrence in a fixed window 
would be insufficient to apply this method to 
the translation of verbs and other parts of
speech. In future work, we will consider syn-
4 We limited the maximum number of association 
words for one word to 400 in descending order of 
their association scores because of restriction of 
computational resources. In future work, we may 
alleviate the drawback of pMI by enlarging or de-
leting this limitation.
Score TA Exp.A Exp.B1 Exp.B2
Dice 0 1.83 0.61 0.91
pMI 0 1.27 0.50 0.79
LLR 10/0 1.34 1.48 2.75
Table 6. Average numbers of translation can-
didates in the correlation matrices for one input 
word.
tactic co-occurrence, which is obtained by 
conducting dependency parsing of the results 
of a sentence. The correlation between asso-
ciated words and translation candidates also 
needs to be re-examined. Similarly, we will 
handle verbs as associated words to the input 
nouns by using syntactic co-occurrence.
5 Related Work
Statistical machine translation (Brown et al, 
1990) automatically acquires knowledge for 
word translation disambiguation from parallel 
corpora. Word translation disambiguation is 
based on probabilities calculated from the 
word alignment, phrase pair extraction, and the 
language model. However, much broad con-
text/domain information is not considered.
Carpuat and Wu (2007) proposed con-
text-dependent phrasal translation lexicons by 
introducing context-dependent features into 
statistical machine translation.
Unsupervised methods using dictionaries 
and corpora were proposed for monolingual 
WSD (Ide and Veronis, 1998). They used 
grammatical information including 
parts-of-speech, syntactically related words, 
and co-occurring words as the clues for the 
WSD. Our method uses a part of the clues for 
bilingual WSD and word translation disam-
biguation.
Li and Li (2002) constructed a classifier for 
word translation disambiguation by using a 
bilingual dictionary with bootstrapping tech-
niques. We also conducted recursive calcula-
tion by dealing with the bilingual dictionary as 
the seeds of the iteration.
Vickrey et al (2005) introduced a context as 
a feature for a statistical MT system and they 
generated word-level translations. How to in-
troduce the word-level translation disambigua-
tion into sentence-level translation is a consi-
derable problem.
36
6 Conclusion
In this paper, we described a method for add-
ing information for word translation disam-
biguation into the bilingual lexicon, by consi-
dering the associated words that co-occur with 
the input word. We based our method on the 
following two assumptions: ?parallel word 
associations? and ?one sense per word associa-
tion.? We aligned word associations by using a 
bilingual dictionary, and constructed a correla-
tion matrix for each word in the source lan-
guage for word translation disambiguation.
Experiments showed that our method was ap-
plicable for both common newspaper text and 
domain-specific text and for two language 
pairs. The Jaccard/Dice coefficients were 
proven to be more effective than the other me-
trics as word association scores. Future work 
includes extending our method to handle verbs 
as input words by introducing syntactic 
co-occurrence. The comparisons with other 
disambiguation methods and machine transla-
tion systems would strengthen the effective-
ness of our method. We consider also evalua-
tions on real NLP tasks including machine 
translation.
Acknowledgments
This work was partially supported by Japa-
nese/Chinese Machine Translation Project in 
Special Coordination Funds for Promoting 
Science and Technology (MEXT, Japan).
References
Brown, Peter F., John Cocke, Stephen A. Della 
Pietra, Vincent J. Della Pietra, Fredrick Jelinek, 
John D. Lafferty, Robert L. Mercer, and Paul S. 
Roossin. 1990. A statistical approach to machine 
translation. Computational Linguistics,
16(2):79-85.
Carpuat, Marine and Dekai Wu. 2007. Con-
text-dependent phrasal translation lexicons for 
statistical machine translation. In Proc. of Ma-
chine Translation Summit XI, pages 73-80.
Church, Kenneth W., William Gale, Patrick Hanks 
and Donald Hindle. 1991. Using statistics in 
lexical analysis. Lexical Acquisition: Using 
On-line Resources to Build a Lexicon, pages 
115-164.
Church, Kenneth W. and Patrick Hanks. 1990. Word 
association norms, mutual information, and lex-
icography. Computational Linguistics,
16(1):22-29.
Dunning, Ted. 1993. Accurate methods for the sta-
tistics of surprise and coincidence. Computa-
tional Linguistics, 19(1):61-74.
Ide, Nancy and Jean Veronis. 1998. Introduction to 
the special issue on word sense disambiguation: 
the state of the art. Computational Linguistics,
24(1):1-40.
Kaji, Hiroyuki and Yasutsugu Morimoto. 2002. 
Unsupervised word sense disambiguation using 
bilingual comparable corpora. In Proc. of the 
19th International Conference on Computational 
Linguistics, pages 411-417.
Kruengkrai, Canasai, Kiyotaka Uchimoto, Jun?ichi 
Kazama, Yiou Wang, Kentaro Torisawa, and 
Hitoshi Isahara. 2009. An error-driven 
word-character hybrid model for joint Chinese 
word segmentation and POS tagging. In Proc. of 
the Joint Conference of the 47th Annual Meeting 
of the ACL and the 4th International Joint Con-
ference on Natural Language Processing of the 
AFNLP, pages 513-521.
Li, Cong and Hang Li. 2002. Word translation 
disambiguation using bilingual bootstrapping. In 
Proc. of the 40th Annual Meeting of Association 
for Computational Linguistics, pages 343-351.
Rapp, Reinhard. 1995. Identifying word transla-
tions in non-parallel texts. In Proc. of the 33rd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 320-322.
Schmid, Helmut. 1994. Probabilistic part-of-speech 
tagging using decision trees. In Proc. of the 1st 
International Conference on New Methods in 
Natural Language Processing.
Smadja, Frank. 1993. Retrieving collocations from 
text: Xtract. Computational Linguistics,
19(1):143-177.
Smadja, Frank, Kathleen R. McKeown and Vasi-
leios Hatzivassiloglou. 1996. Translating collo-
cations or bilingual lexicons: A statistical ap-
proach. Computational Linguistics, 22(1):3-38.
Vickrey, David, Luke Biewald, Marc Teyssier, and 
Daphne Koller. 2005. Word-sense disambigua-
tion for machine translation. In Proc. of the 
Conference on HLT/EMNLP, pages 771-778.
Yarowsky, David. 1993. One sense per collocation. 
In Proc. of ARPA Human Language Technology 
Workshop, pages 266-271.
37
