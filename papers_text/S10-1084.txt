Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 375?378,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
RALI: Automatic weighting of text window distances
Bernard Brosseau-Villeneuve*#, Noriko Kando#, Jian-Yun Nie*
* Universit? de Montr?al, Email: {brosseab, nie}@iro.umontreal.ca
# National Institute of Informatics, Email: {bbrosseau, kando}@nii.ac.jp
Abstract
Systems using text windows to model
word contexts have mostly been using
fixed-sized windows and uniform weights.
The window size is often selected by trial
and error to maximize task results. We
propose a non-supervised method for se-
lecting weights for each window distance,
effectively removing the need to limit win-
dow sizes, by maximizing the mutual gen-
eration of two sets of samples of the same
word. Experiments on Semeval Word
Sense Disambiguation tasks showed con-
siderable improvements.
1 Introduction
The meaning of a word can be defined by the
words that accompany it in the text. This is the
principle often used in previous studies on Word
Sense Disambiguation (WSD) (Ide and V?ronis,
1998; Navigli, 2009). In general, the accompa-
nying words form a context vector of the target
word, or a probability distribution of the context
words. For example, under the unigram bag-of-
word assumption, this means building p(x|t) =
count(x,t)
?
x
?
count(x
?
,t)
, where count(x, t) is the count of
co-occurrences of word x with the target word t
under a certain criterion. In most studies, x and t
should co-occur within a window of up to k words
or sentences. The bounds are usually selected as
to maximize system performance. Occurrences in-
side the window usually weight the same with-
out regard to their position. This is counterintu-
itive. Indeed, a word closer to the target word usu-
ally has a greater semantic constraint on the tar-
get word than a more distant word. Some studies
have also proposed decaying factors to decrease
the importance of more distant words in the con-
text vector. However, the decaying functions are
defined manually. It is unclear that the functions
defined can capture the true impact of the con-
text words on the target word. In this paper, we
propose an unsupervised method to automatically
learn the optimal weight of a word according to its
distance to the target word. The general idea used
to determine such weight is that, if we randomly
determine two sets of texts containing the target
word, the resulting probability distributions for its
context words in the two sets should be similar.
Therefore, the weights of context words at differ-
ent distance are determined so as to maximize the
mutual generation probabilities of two sets of sam-
ples. Experimentation on Semeval-2007 English
and Semeval-2010 Japanese lexical sample task
data shows that improvements can automatically
be attained on simple Naive Bayes (NB) systems
in comparison to the best manually selected fixed
window system.
The remainder of this paper is organized as fol-
lows: example uses of text windows and related
work are presented in Section 2. Our method
is presented in Section 3. In Section 4 and
5, we show experimental results on English and
Japanese WSD. We conclude in Section 6 with
discussion and further possible extensions.
2 Uses of text windows
Modeling the distribution of words around one
target word has many uses. For instance, the
Xu&Croft co-occurrence-based stemmer (Xu and
Croft, 1998) uses window co-occurrence statis-
tics to calculate the best equivalence classes for
a group of word forms. They suggest using win-
dows of up to 100 words. Another example can be
found in WSD systems, where a shorter window is
preferred. In Semeval-2007, top performing sys-
tems on WSD tasks, such as NUS-ML (Cai et al,
2007), made use of bag-of-word features around
the target word. In this case, they found that the
best results can be achieved using a window size
of 3.
375
Both these systems limit the size of their win-
dows for different purposes. The former aims to
model the topic of the documents containing the
word rather than the word?s meaning. The latter
limits the size because bag-of-word features fur-
ther from the target word would not be sufficiently
related to its meaning (Ide and V?ronis, 1998). We
see that because of sparsity issues, there is a com-
promise between taking few, highly related words,
or taking several, lower quality words.
In most current systems, all words in a window
are given equal weight, but we can easily under-
stand that the occurrences of words should gener-
ally count less as they become farther; they form
a long tail that we should use. Previous work pro-
posed using non-linear functions of the distance
to model the relation between two words. For in-
stance, improvements can be obtained by using an
exponential function (Gao et al, 2002). Yet, there
is no evidence that the exponential ? with its man-
ually selected parameter ? is the best function.
3 Computing weights for distances
In this section, we present our method for choos-
ing howmuch a word should count according to its
distance to the target word. First, for some defini-
tions, let C be a corpus, W a set of text windows,
c
W,i,x
the count of occurrences of word x at dis-
tance i in W , c
W,i
the sum of these counts, and ?
i
the weight put on one word at distance i. Then,
P
ML,W
(x) =
?
i
?
i
c
W,i,x
?
i
?
i
c
W,i
(1)
is the maximum likelihood estimator for x. To
counter the zero-probability problem, we apply
Dirichlet smoothing with the collection language
model as a prior:
P
Dir,W
(x) =
?
i
?
i
c
W,i,x
+ ?
W
P (x|C)
?
i
?
i
c
W,i
+ ?
W
(2)
The pseudo-count ?
W
is found by using Newton?s
method via leave-one-out estimation. We follow
the procedure shown in (Zhai and Lafferty, 2002),
but since occurrences have different weights, the
log-likelihood is changed to
L
?1
(?|W, C) = (3)
?
i
?
x?V
?
i
c
W,i,x
log
?
i
c
W,i,x
??
i
+?P (x|C)
?
j
?
j
c
W,j
??
i
+?
To find the best weights for our model we pro-
pose the following:
? Let T be the set of all windows containing
the target word. We randomly split this set
into two sets A and B.
? We want to find ?
?
that maximizes the mu-
tual generation of the two sets, by minimizing
their cross-entropy:
l(?) = H(P
ML,A
, P
Dir,B
) + H(P
ML,B
, P
Dir,A
)
(4)
In other words, we want ?
i
to represent how
much an occurrence at distance i models the con-
text better than the collection language model,
whose counts are controlled by the Dirichlet
pseudo-count. We hypothesize that target words
occurs in limited contexts, and as we get farther
from them, the possibilities become greater, re-
sulting in sparse and less related counts.
3.1 Gradient descent
We propose a simple gradient descent minimiz-
ing (4) over ?. For the following experiments,
we used one single curve for all words in a task.
We used the mini-batch type of gradient descent:
the gradients of a fixed amount of target words are
summed, a gradient step is done, and the proces
is repeated while cycling the data. The starting
state was with all ?
i
to one, the batch size of 50
and a learning rate of 1. We notice that as the al-
gorithm progress, weights on close distances in-
crease and the farthest decrease. As further dis-
tances contribute less and less, middle distances
start to decay more and more, until at some point,
all distances but the closest start to decrease, head-
ing towards a degenerate solution. We therefore
suggest using the observation of several consecu-
tive decreases of all except ?
1
as an end criterion.
We used 10 consecutive steps for our experiments.
4 Experiments on Semeval-2007 English
Lexical Sample
The Semeval workshop holds WSD tasks such as
the English Lexical Sample (ELS) (Pradhan et al,
2007). It consists of a selected set of polysemous
words, contained within passages where a sense
taken from a sense inventory is manually anno-
tated. The task is to create supervised classifiers
maximizing accuracy on test data.
Since there are only 50 words and instances are
few, we judged there was not enough data to com-
pute weights. Instead, we used the AP Newswire
corpus of the TREC collection (CD 1 & 2). Words
376
were stemmed with the Porter stemmer and text
windows were grouped for all words. For sim-
plicity and efficiency, windows to the right and to
the left were considered independent, and we only
kept words with between 30 and 1000 windows.
Also, only windows with a size of 100, which was
considered big enough without any doubt, were
kept. A stop list of the top 10 frequent words was
used, but place holders were left in the windows to
preserve the distances. Multiple consecutive stop
words (ex: ?of the?) were merged, and the tar-
get word, being the same for all samples of a set,
was ignored. This results in 32,650 sets contain-
ing 5,870,604 windows. In Figure 1, we can see
the resulting weight curve.
0 20 40 60 80 100
distance
0.0
0.2
0.4
0.6
0.8
1.0
w
e
i
g
h
t
Figure 1: Weight curve for AP Newswire
Since the curve converges, words over the 100th
distance were assigned the minimumweight found
in the curve. From this we constructed NB models
whose class priors used an absolute discounting of
0.5. The collection language model used the con-
catenation of the AP collection and the Semeval
data. As the unstemmed target word is an impor-
tant feature it was added to the models. It?s weight
was chosen to be 0.7 by maximizing accuracy on
one-held-out cross-validation of the training data.
The results are listed in Table 1.
System Cross-Val (%) Test set (%)
Prior only 78.66 77.76
Best uniform 85.48 83.28
RALI-2 88.23 86.45
Table 1: WSD accuracy on Semeval-2007 ELC
We used two baselines: most frequent sense
(prior only), and the best uniform (except target
word) fixed size window found from extensive
search on the training data. The best settings were
a window of size 4, with a weight of 4.4 on the
target word and a Laplace smoothing of 2.9. The
improvements seen using our system are substan-
tial, beating most of the systems originally pro-
posed for the task (Pradhan et al, 2007). Out
of 15 systems, the best results had accuracies of
89.1*, 89.1*, 88.7, 86.9 and 86.4 (* indicates post-
competition submissions). Notice that most were
using Support Vector Machine (SVM) with bag-
of-word features in a very small window, local col-
locations and POS tags. In our future work, we
will investigate the applications of SVM with our
new term weighting scheme.
5 Experiments on Semeval-2010
Japanese WSD
The Semeval-2010 Japanese WSD task (Okumura
et al, 2010) consists of 50 polysemous words
for which examples were taken from the BC-
CWJ tagged corpus. It was manually segmented,
tagged, and annotated with senses taken from the
Iwanami Kokugo dictionary. The task is identical
to the ELS of the previous experiment.
Since the data was again insufficient to com-
pute curves, we used the Mainichi-2005 corpus of
NTCIR-8. We tried to reproduce the same kind
of segmentation as the training data by using the
Chasen parser with UniDic. For the corpus and
Semeval data, conjugations (setsuzoku-to, jod?-
shi, etc.), particles (all jo-shi), symbols (blanks,
kig?, etc.), and numbers were stripped. When a
base-form reading was present (for verbs and ad-
jectives), the token was replaced by the Kanjis
(chinese characters) in the word writing concate-
nated with the base-form reading. This treatment
is somewhat equivalent to the stemming+stop list
of the ELS tasks. The resulting curve can be seen
in Figure 2.
The NB models are the same as in the previous
experiments. Target words were again added the
same way as in the ELS task. The best fixed win-
dow model was found to have a window size of 1
with a target word weight of 0.6 and used manual
Dirichlet smoothing with a pseudo-count of 110.
We submited two systems with the following set-
tings: RALI-1 used manual Dirichlet smoothing
and 0.9 for the target word. RALI-2 used auto-
377
0 20 40 60 80 100
distance
0.0
0.2
0.4
0.6
0.8
1.0
w
e
i
g
h
t
Figure 2: Weight curve for Mainichi Shinbun 2005
matic Dirichlet smoothing and 1.7 for the target
word weight. Results are listed in Table 2.
System Cross-Val (%) Test set (%)
prior only 75.23 68.96
Best uniform 82.29 76.12
RALI-1 82.77 75.92
RALI-2 83.05 76.36
Table 2: WSD accuracy on Semeval-2010 JWSD
As we can see, the results are not significantly
different from the best uniform model. This may
be due to differences in the segmentation parame-
ters of our external corpus. Another reason could
be that the systems use almost the same weights:
the best fixed window had size 1, and the Japanese
curve is steeper than the English one.
This steeper curve can be explained by the
grammatical structure of the Japanese language.
While English can be considered a Subject-
Verb-Complement language, Japanese is consid-
ered Subject-Complement-Verb. Verbs are mostly
found at the end of the sentence, far from their sub-
ject, and vice versa. The window distance is there-
fore less useful in Japanese than in English since
it has more non-local dependencies. These results
show that the curves work as expected even in dif-
ferent languages.
6 Conclusions
This paper proposed an unsupervised method for
finding weights for counts in text windows ac-
cording to their distance to the target word. Re-
sults from the Semeval-2007 English lexical sam-
ple showed a substantial improvement in preci-
sion. Yet, as we have seen with the Japanese task,
window distance is not always a good indicator of
word relatedness. Fortunately, we can easily imag-
ine extensions to the current scheme that bins word
counts by factors other than word distance. For in-
stance, we could also bin counts by parsing tree
distance, sentence distance or POS-tags.
Acknowledgments
The authors would like to thank Florian Boudin
and Satoko Fujisawa for helpful comments on
this work. This work is partially supported
by Japanese MEXT Grant-in-Aid for Scientific
Research on Info-plosion (#21013046) and the
Japanese MEXT Research Student Scholarship
program.
References
Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh. 2007.
Nus-ml: improving word sense disambiguation us-
ing topic features. In SemEval ?07 Proceedings,
pages 249?252, Morristown, NJ, USA. Association
for Computational Linguistics.
Jianfeng Gao, Ming Zhou, Jian-Yun Nie, Hongzhao
He, and Weijun Chen. 2002. Resolving query trans-
lation ambiguity using a decaying co-occurrence
model and syntactic dependence relations. In SI-
GIR ?02 Proceedings, pages 183?190, New York,
NY, USA. ACM.
Nancy Ide and Jean V?ronis. 1998. Introduction to
the special issue on word sense disambiguation: the
state of the art. Comput. Linguist., 24(1):2?40.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Comput. Surv., 41(2):1?69.
Manabu Okumura, Kiyoaki Shirai, Kanako Komiya,
and Hikaru Yokono. 2010. Semeval-2010 task:
Japanese wsd. In SemEval ?10 Proceedings. Associ-
ation for Computational Linguistics.
Sameer S. Pradhan, Edward Loper, Dmitriy Dligach,
and Martha Palmer. 2007. Semeval-2007 task 17:
English lexical sample, srl and all words. In Se-
mEval ?07 Proceedings, pages 87?92, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Jinxi Xu and W. Bruce Croft. 1998. Corpus-
based stemming using cooccurrence of word vari-
ants. ACM Trans. Inf. Syst., 16(1):61?81.
ChengXiang Zhai and John Lafferty. 2002. Two-stage
language models for information retrieval. In SIGIR
?02 Proceedings, pages 49?56, NewYork, NY, USA.
ACM.
378
