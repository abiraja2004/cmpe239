Proceedings of the 6th Workshop on Statistical Machine Translation, pages 116?122,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
TINE: A Metric to Assess MT Adequacy
Miguel Rios, Wilker Aziz and Lucia Specia
Research Group in Computational Linguistics
University of Wolverhampton
Stafford Street, Wolverhampton, WV1 1SB, UK
{m.rios, w.aziz, l.specia}@wlv.ac.uk
Abstract
We describe TINE, a new automatic evalua-
tion metric for Machine Translation that aims
at assessing segment-level adequacy. Lexical
similarity and shallow-semantics are used as
indicators of adequacy between machine and
reference translations. The metric is based on
the combination of a lexical matching com-
ponent and an adequacy component. Lexi-
cal matching is performed comparing bags-
of-words without any linguistic annotation.
The adequacy component consists in: i) us-
ing ontologies to align predicates (verbs), ii)
using semantic roles to align predicate argu-
ments (core arguments and modifiers), and
iii) matching predicate arguments using dis-
tributional semantics. TINE?s performance
is comparable to that of previous metrics
at segment level for several language pairs,
with average Kendall?s tau correlation from
0.26 to 0.29. We show that the addition of
the shallow-semantic component improves the
performance of simple lexical matching strate-
gies and metrics such as BLEU.
1 Introduction
The automatic evaluation of Machine Translation
(MT) is a long-standing problem. A number of met-
rics have been proposed in the last two decades,
mostly measuring some form of matching between
the MT output (hypothesis) and one or more human
(reference) translations. However, most of these
metrics focus on fluency aspects, as opposed to ad-
equacy. Therefore, measuring whether the meaning
of the hypothesis and reference translation are the
same or similar is still an understudied problem.
The most commonly used metrics, BLEU (Pap-
ineni et al, 2002) and alike, perform simple exact
matching of n-grams between hypothesis and refer-
ence translations. Such a simple matching proce-
dure has well known limitations, including that the
matching of non-content words counts as much as
the matching of content words, that variations of
words with the same meaning are disregarded, and
that a perfect matching can happen even if the order
of sequences of n-grams in the hypothesis and ref-
erence translation are very different, changing com-
pletely the meaning of the translation.
A number of other metrics have been proposed
to address these limitations, for example, by allow-
ing for the matching of synonyms or paraphrases
of content words, such as in METEOR (Denkowski
and Lavie, 2010). Other attempts have been made
to capture whether the reference translation and hy-
pothesis translations share the same meaning us-
ing shallow semantics, i.e., Semantic Role Labeling
(Gime?nez and Ma?rquez, 2007). However, these are
limited to the exact matching of semantic roles and
their fillers.
We propose TINE, a new metric that comple-
ments lexical matching with a shallow semantic
component to better address adequacy. The main
contribution of such a metric is to provide a more
flexible way of measuring the overlap between shal-
low semantic representations that considers both the
semantic structure of the sentence and the content
of the semantic elements. The metric uses SRLs
such as in (Gime?nez and Ma?rquez, 2007). However,
it analyses the content of predicates and arguments
seeking for either exact or ?similar? matches. The
116
inexact matching is based on the use of ontologies
such as VerbNet (Schuler, 2006) and distributional
semantics similarity metrics, such as Dekang Lin?s
thesaurus (Lin, 1998) .
In the remainder of this paper we describe some
related work (Section 2), present our metric - TINE
- (Section 3) and its performance compared to pre-
vious work (Section 4) as well as some further im-
provements. We then provide an analysis of these
results and discuss the limitations of the metric (Sec-
tion 5) and present conclusions and future work
(Section 6).
2 Related Work
A few metrics have been proposed in recent years
to address the problem of measuring whether a hy-
pothesis and a reference translation share the same
meaning. The most well-know metric is probably
METEOR (Banerjee and Lavie, 2005; Denkowski
and Lavie, 2010). METEOR is based on a general-
ized concept of unigram matching between the hy-
pothesis and the reference translation. Alignments
are based on exact, stem, synonym, and paraphrase
matches between words and phrases. However, the
structure of the sentences is not considered.
Wong and Kit (2010) measure word choice and
word order by the matching of words based on
surface forms, stems, senses and semantic similar-
ity. The informativeness of matched and unmatched
words is also weighted.
Liu et al (2010) propose to match bags of uni-
grams, bigrams and trigrams considering both recall
and precision and F-measure giving more impor-
tance to recall, but also using WordNet synonyms.
Tratz and Hovy (2008) use transformations in or-
der to match short syntactic units defined as Ba-
sic Elements (BE). The BE are minimal-length
syntactically well defined units. For example,
nouns, verbs, adjectives and adverbs can be con-
sidered BE-Unigrams, while a BE-Bigram could be
formed from a syntactic relation (e.g. subject+verb,
verb+object). BEs can be lexically different, but se-
mantically similar.
Pado? et al (2009) uses Textual Entailment fea-
tures extracted from the Standford Entailment Rec-
ognizer (MacCartney et al, 2006). The Textual En-
tailment Recognizer computes matching and mis-
matching features over dependency parses. The met-
ric then predicts the MT quality with a regression
model. The alignment is improved using ontologies.
He et al (2010) measure the similarity between
hypothesis and reference translation in terms of
the Lexical Functional Grammar (LFG) represen-
tation. The representation uses dependency graphs
to generate unordered sets of dependency triples.
Calculating precision, recall, and F-score on the
sets of triples corresponding to the hypothesis and
reference segments allows measuring similarity at
the lexical and syntactic levels. The measure also
matches WordNet synonyms.
The closest related metric to the one proposed in
this paper is that by Gime?nez and Ma?rquez (2007)
and Gime?nez et al (2010), which also uses shallow
semantic representations. Such a metric combines a
number of components, including lexical matching
metrics like BLEU and METEOR, as well as com-
ponents that compute the matching of constituent
and dependency parses, named entities, discourse
representations and semantic roles. However, the se-
mantic role matching is based on exact matching of
roles and role fillers. Moreover, it is not clear what
the contribution of this specific information is for the
overall performance of the metric.
We propose a metric that uses a lexical similar-
ity component and a semantic component in order
to deal with both word choice and semantic struc-
ture. The semantic component is based on seman-
tic roles, but instead of simply matching the surface
forms (i.e. arguments and predicates) it is able to
match similar words.
3 Metric Description
The rationale behind TINE is that an adequacy-
oriented metric should go beyond measuring the
matching of lexical items to incorporate information
about the semantic structure of the sentence, as in
(Gime?nez et al, 2010). However, the metric should
also be flexible to consider inexact matches of se-
mantic components, similar to what is done with lex-
ical metrics like METEOR (Denkowski and Lavie,
2010). We experiment with TINE having English
as target language because of the availability of lin-
guistic processing tools for this language. The met-
ric is particularly dependent on semantic role label-
117
ing systems, which have reached satisfactory perfor-
mance for English (Carreras and Ma?rquez, 2005).
TINE uses semantic role labels (SRL) and lexical se-
mantics to fulfill two requirements by: (i) compare
both the semantic structure and its content across
matching arguments in the hypothesis and refer-
ence translations; and (ii) propose alternative ways
of measuring inexact matches for both predicates
and role fillers. Additionally, it uses an exact lexi-
cal matching component to reward hypotheses that
present the same lexical choices as the reference
translation. The overall score s is defined using the
simple weighted average model in Equation (1):
s(H,R) = max
{
?L(H,R) + ?A(H,R)
?+ ?
}
R?R
(1)
where H represents the hypothesis translation, R
represents a reference translation contained in the set
of available references R; L defines the (exact) lex-
ical match component in Equation (2), A defines the
adequacy component in Equation (3); and ? and ?
are tunable weights for these two components. If
multiple references are provided, the score of the
segment is the maximum score achieved by compar-
ing the segment to each available reference.
L(H,R) =
|H
?
R|
?
|H| ? |R|
(2)
The lexical match component measures the over-
lap between the two representations in terms of the
cosine similarity metric. A segment, either a hypoth-
esis or a reference, is represented as a bag of tokens
extracted from an unstructured representation, that
is, bag of unigrams (words or stems). Cosine sim-
ilarity was chosen, as opposed to simply checking
the percentage of overlapping words (POW) because
cosine does not penalize differences in the length of
the hypothesis and reference translation as much as
POW. Cosine similarity normalizes the cardinality
of the intersection |H?R| using the geometric mean?
|H| ? |R| instead of the union |H?R|. This is par-
ticularly important for the matching of arguments -
which is also based on cosine similarity. If an hy-
pothesized argument has the same meaning as its
reference translation, but differs from it in length,
cosine will penalize less the matching than POW.
That is specially interesting when core arguments
get merged with modifiers due to bad semantic role
labeling (e.g. [A0 I] [T bought] [A1 something to eat
yesterday] instead of [A0 I] [T bought] [A1 some-
thing to eat] [AM-TMP yesterday]).
A(H,R) =
?
v?V verb score(Hv, Rv)
|Vr|
(3)
In the adequacy component, V is the set of verbs
aligned between H and R, and |Vr| is the number of
verbs in R. Hereafter the indexes h and r stand for
hypothesis and reference translations, respectively.
Verbs are aligned using VerbNet (Schuler, 2006) and
VerbOcean (Chklovski and Pantel, 2004). A verb in
the hypothesis vh is aligned to a verb in the refer-
ence vr if they are related according to the follow-
ing heuristics: (i) the pair of verbs share at least one
class in VerbNet; or (ii) the pair of verbs holds a re-
lation in VerbOcean.
For example, in VerbNet the verbs spook and ter-
rify share the same class amuse-31.1, and in VerbO-
cean the verb dress is related to the verb wear.
verb score(Hv, Rv) =
?
a?Ar?At
arg score(Ha, Ra)
|Ar|
(4)
The similarity between the arguments of a verb
pair (vh, vr) in V is measured as defined in Equa-
tion (4), where Ah and At are the sets of labeled
arguments of the hypothesis and the reference re-
spectively and |Ar| is the number of arguments of
the verb in R. In other words, we only measure the
similarity of arguments in a pair of sentences that are
annotated with the same role. This ensures that the
structure of the sentence is taken into account (for
example, an argument in the role of agent would not
be compared against an argument in a role of experi-
encer). Additionally, by restricting the comparison
to arguments of a given verb pair, we avoid argument
confusion in sentences with multiple verbs.
The arg score(Ha, Ra) computation is based on
the cosine similarity as in Equation (2). We treat
the tokens in the argument as a bag-of-words. How-
ever, in this case we change the representation of
the segments. If the two sets do not match exactly,
we expand both of them by adding similar words.
For every mismatch in a segment, we retrieve the
118
20-most similar words from Dekang Lin?s distribu-
tional thesaurus (Lin, 1998), resulting in sets with
richer lexical variety.
The following example shows how the computa-
tion of A(H,R) is performed, considering the fol-
lowing hypothesis and reference translations:
H: The lack of snow discourages people from ordering
ski stays in hotels and boarding houses.
R: The lack of snow is putting people off booking ski
holidays in hotels and guest houses.
1. extract verbs from H: Vh = {discourages, ordering}
2. extract verbs from R: Vr = {putting, booking}
3. similar verbs aligned with VerbNet (shared class
get-13.5.1): V = {(vh = order,vr = book)}
4. compare arguments of (vh = order,vr = book):
Ah = {A0, A1, AM-LOC}
Ar = {A0, A1, AM-LOC}
5. Ah ?Ar = {A0, A1, AM-LOC}
6. exact matches:
HA0 = {people} and RA0 = {people}
argument score = 1
7. different word forms: expand the representation:
HA1 = {ski, stays} and RA1 = {ski, holidays}
expand to:
HA1 = {{ski},{stays, remain... journey...}}
RA1 = {{ski},{holidays, vacations, trips... jour-
ney...}}
argument score = 0.5
8. similarly to HAM?LOC and RAM?LOC
argument score = 0.72
9. verb score (order, book) = 1+0.5+0.723 = 0.74
10. A(H,R) = 0.742 = 0.37
Different from previous work, we have not used
WordNet to measure lexical similarity for two main
reasons: problems with lexical ambiguity and lim-
ited coverage in WordNet (instances of named enti-
ties are not in WordNet, e.g. Barack Obama). For
example, in WordNet the aligned verbs (order/book)
from the previous hypothesis and reference trans-
lations have: 9 senses - order (e.g. give instruc-
tions to or direct somebody to do something with
authority, make a request for something, etc.) - and
4 senses - book (engage for a performance, arrange
for and reserve (something for someone else) in ad-
vance, etc.). Thus, a WordNet-based similarity mea-
sure would require disambiguating segments, an ad-
ditional step and a possible source of errors. Second,
a thresholds would need to be set to determine when
a pair of verbs is aligned. In contrast, the structure of
VerbNet (i.e. clusters of verbs) allows a binary deci-
sion, although the VerbNet heuristic results in some
errors, as we discuss in Section 5.
4 Results
We set the weights ? and ? by experimental test-
ing to ? = 1 and ? = 0.25. The lexical component
weight is prioritized because it has shown a good av-
erage Kendall?s tau correlation (0.23) on a develop-
ment dataset (Callison-Burch et al, 2010). Table 1
shows the correlation of the lexical component with
human judgments for a number of language pairs.
Table 1: Kendall?s tau segment-level correlation of the
lexical component with human judgments
Metric cz-en fr-en de-en es-en avg
Lexical 0.27 0.21 0.26 0.19 0.23
We use the SENNA1 SRL system to tag the
dataset with semantic roles. SENNA has shown to
have achieved an F-measure of 75.79% for tagging
semantic roles over the CoNLL 2005 2 benchmark.
We compare our metric against standard BLEU
(Papineni et al, 2002), METEOR (Denkowski and
Lavie, 2010) and other previous metrics reported in
(Callison-Burch et al, 2010) which also claim to use
some form of semantic information (see Section 2
for their description). The comparison is made in
terms of Kendall?s tau correlation against the human
judgments at a segment-level. For our submission to
the shared evaluation task, system-level scores are
obtained by averaging the segment-level scores.
TINE achieves the same average correlation with
BLUE, but outperforms it for some language pairs.
Additionally, TINE outperforms some of the previ-
ous which use WordNet to deal with synonyms as
part of the lexical matching.
The closest metric to TINE (Gime?nez et al,
2010), which also uses semantic roles as one of its
1http://ml.nec-labs.com/senna/
2http://www.lsi.upc.edu/ srlconll/
119
Table 2: Comparison with previous semantically-
oriented metrics using segment-level Kendall?s tau cor-
relation with human judgments
Metric cz-en fr-en de-en es-en avg
(Liu et al,
2010)
0.34 0.34 0.38 0.34 0.35
(Gime?nez
et al, 2010)
0.34 0.33 0.34 0.33 0.33
(Wong and
Kit, 2010)
0.33 0.27 0.37 0.32 0.32
METEOR 0.33 0.27 0.36 0.33 0.32
TINE 0.28 0.25 0.30 0.22 0.26
BLEU 0.26 0.22 0.27 0.28 0.26
(He et al,
2010)
0.15 0.14 0.17 0.21 0.17
(Tratz
and Hovy,
2008)
0.05 0.0 0.12 0.05 0.05
components, achieves better performance. However,
this metric is a rather complex combination of a
number of other metrics to deal with different lin-
guistic phenomena.
4.1 Further Improvements
As an additional experiment, we use BLEU as the
lexical component L(H,R) in order to test if the
shallow-semantic component can contribute to the
performance of this standard evaluation metric. Ta-
ble 3 shows the results of the combination of BLEU
and the shallow-semantic component using the same
parameter configuration as in Section 4. The addi-
tion of the shallow-semantic component increased
the average correlation of BLEU from 0.26 to 0.28.
Table 3: TINE-B: Combination of BLEU and the
shallow-semantic component
Metric cz-en fr-en de-en es-en avg
TINE-B 0.27 0.25 0.30 0.30 0.28
Finally, we improve the tuning of the weights of
the components (? and ? parameters) by using a
simple genetic algorithm (Back et al, 1999) to se-
lect the weights that maximize the correlation with
human scores on a development set (we use the de-
velopment sets from WMT10 (Callison-Burch et al,
2010)). The configuration of the genetic algorithm
is as follows:
? Fitness function: Kendall?s tau correlation
? Chromosome: two real numbers, ? and ?
? Number of individuals: 80
? Number of generations: 100
? Selection method: roulette
? Crossover probability: 0.9
? Mutation probability: 0.01
Table 4 shows the parameter values obtaining
from tuning for each language pair and the corre-
lation achieved by the metric with such parameters.
With such an optimization step the average correla-
tion of the metric increases to 0.29.
Table 4: Optimized values of the parameters using a ge-
netic algorithm and Kendall?s tau and final correlation of
the metric on the test sets
Language pair Correlation ? ?
cz-en 0.28 0.62 0.02
fr-en 0.25 0.91 0.03
de-en 0.30 0.72 0.1
es-en 0.31 0.57 0.02
avg 0.29 ? ?
5 Discussion
In what follows we discuss with a few examples
some of the common errors made by TINE. Over-
all, we consider the following categories of errors:
1. Lack of coverage of the ontologies.
R: This year, women were awarded the Nobel Prize in all
fields except physics
H: This year the women received the Nobel prizes in all
categories less physical
The lack of coverage in VerbNet prevented the
detection of the similarity between receive and
award.
2. Matching of unrelated verbs.
R: If snow falls on the slopes this week, Christmas will
sell out too, says Schiefert.
H: If the roads remain snowfall during the week, the dates
of Christmas will dry up, said Schiefert.
In VerbOcean remain and say are incorrectly
120
said to be related. VerbOcean was cre-
ated by a semi-automatic extraction algorithm
(Chklovski and Pantel, 2004) with an average
accuracy of 65.5%.
3. Incorrect tagging of the semantic roles by
SENNA.
R: Colder weather is forecast for Thursday, so if anything
falls, it should be snow.
H: On Thursday , must fall temperatures and, if there is
rain, in the mountains should.
The position of the predicates affects the SRL
tagging. The predicate fall has the following
roles (A1, V, and S-A1) in the reference, and
the following roles (AM-ADV, A0, AM-MOD,
and AM-DIS) in the hypothesis. As a con-
sequence, the metric cannot attempt to match
the fillers. Also, SRL systems do not detect
phrasal verbs such as in the example of Section
3, where the action putting people off is similar
to discourages.
6 Conclusions and Future Work
We have presented an MT evaluation metric based
on the alignment of semantic roles and flexible
matching of role fillers between hypothesis and ref-
erence translations. To deal with inexact matches,
the metric uses ontologies and distributional seman-
tics, as opposed to lexical databases like WordNet,
in order to minimize ambiguity and lack of cover-
age. The metric also uses an exact lexical matching
component to reward hypotheses that present lexical
choices similar to those of the reference translation.
Given the simplicity of the metric, it has achieved
competitive results. We have shown that the addition
of the shallow-semantic component into a lexical
component yields absolute improvements in the cor-
relation of 3%-6% on average, depending on the lex-
ical component used (cosine similarity or BLEU).
In future work, in order to improve the perfor-
mance of the metric we plan to add components to
address a few other linguistic phenomena such as
in (Gime?nez and Ma?rquez, 2007; Gime?nez et al,
2010). In order to deal with the coverage problem
of an ontology, we plan to use distributional seman-
tics (i.e. word space models) also to align the pred-
icates. We consider using a backoff model for the
shallow-semantic component to deal with the very
frequent cases where there are no comparable pred-
icates between the reference and hypothesis transla-
tions, which result in a 0 score from the semantic
component. Finally, we plan to improve the lexical
component to better tackle fluency, for example, by
adding information about the word order.
References
Thomas Back, David B. Fogel, and Zbigniew
Michalewicz, editors. 1999. Evolutionary Com-
putation 1, Basic Algorithms and Operators. IOP
Publishing Ltd., Bristol, UK, 1st edition.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65?72, Ann Arbor, Michigan, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July.
Xavier Carreras and Llu??s Ma?rquez. 2005. Introduction
to the conll-2005 shared task: Semantic role labeling.
In Proceedings of the 9th Conference on Natural Lan-
guage Learning, CoNLL-2005, Ann Arbor, MI USA.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic Verb
Relations. In Dekang Lin and Dekai Wu, editors, Pro-
ceedings of EMNLP 2004, pages 33?40, Barcelona,
Spain, July.
Michael Denkowski and Alon Lavie. 2010. Meteor-next
and the meteor paraphrase tables: Improved evaluation
support for five target languages. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 339?342, July.
Jesu?s Gime?nez and Llu??s Ma?rquez. 2007. Linguistic fea-
tures for automatic evaluation of heterogenous mt sys-
tems. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, StatMT ?07, pages 256?
264, Stroudsburg, PA, USA.
Jesu?s Gime?nez, Llu??s Ma?rquez, Elisabet Comelles, Irene
Castello?n, and Victoria Arranz. 2010. Document-
level automatic mt evaluation based on discourse rep-
resentations. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metrics-
MATR, WMT ?10, pages 333?338, Stroudsburg, PA,
USA.
121
Yifan He, Jinhua Du, Andy Way, and Josef van Gen-
abith. 2010. The dcu dependency-based metric in
wmt-metricsmatr 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
MetricsMATR, WMT ?10, pages 349?353, Strouds-
burg, PA, USA.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 2, ACL ?98, pages 768?
774, Stroudsburg, PA, USA.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2010.
Tesla: translation evaluation of sentences with linear-
programming-based analysis. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, WMT ?10, pages 354?359,
Stroudsburg, PA, USA.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Manning.
2006. Learning to recognize features of valid textual
entailments. In Proceedings of the Human Language
Technology Conference of the NAACL, pages 41?48,
New York City, USA, June.
Sebastian Pado?, Daniel Cer, Michel Galley, Dan Jurafsky,
and Christopher D. Manning. 2009. Measuring ma-
chine translation quality as semantic equivalence: A
metric based on entailment features. Machine Trans-
lation, 23:181?193, September.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA.
Karin Kipper Schuler. 2006. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. thesis,
University of Pennsylvania.
Stephen Tratz and Eduard Hovy. 2008. Summarisation
evaluation using transformed basic elements. In Pro-
ceedings TAC 2008.
Billy T.-M. Wong and Chunyu Kit. 2010. The parameter-
optimized atec metric for mt evaluation. In Proceed-
ings of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, WMT ?10, pages 360?
364, Stroudsburg, PA, USA.
122
