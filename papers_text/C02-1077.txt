Improved Iterative Scaling can yield multiple globally optimal
models with radically diering performance levels
Iain Bancarz and Miles Osborne
fiainrb,osborneg@cogsci.ed.ac.uk
Division of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW
Scotland
Abstract
Log-linear models can be e?ciently estimated us-
ing algorithms such as Improved Iterative Scaling
(IIS)(Laerty et al, 1997). Under certain conditions
and for a particular class of problems, IIS is guaran-
teed to approach both the maximum-likelihood and
maximum entropy solution. This solution, in like-
lihood space, is unique. Unfortunately, in realistic
situations, multiple solutions may exist, all of which
are equivalent to each other in terms of likelihood,
but radically dierent from each other in terms of
performance. We show that this behaviour can occur
when a model contains overlapping features and the
training material is sparse. Experimental results,
from the domain of parse selection for stochastic at-
tribute value grammars, shows the wide variation
in performance that can be found when estimating
models using IIS. Further results show that the in-
uence of the initial model can be diminished by
selecting either uniform weights, or else by model
averaging.
1 Background
When statistically modelling linguistic phenomena
of one sort or another, researchers typically t log-
linear models to the data (for example (Johnson et
al., 1999)). There are (at least) three reasons for
the popularity of such models: they do not make un-
warranted independence assumptions, the maximum
likelihood solution of such models coincides with the
maximum entropy solution, and nally, they can be
e?ciently estimated (using algorithms such as Im-
proved Iterative Scaling (IIS) (Laerty et al, 1997)).
Now, the solution found by IIS is guaranteed to
approach a global maximum for both likelihood and
entropy under certain conditions. Although this is
appealing, in realistic situations it turns out that
multiple models exist, all of which are equivalent in
terms of likelihood but dierent from each other in
terms of their performance at some task. In particu-
lar, the initial weight settings can inuence the qual-
ity of the nal model, even though this nal model
is the maximum entropy solution (as found by IIS).
At rst glance, this seems very strange. The IIS
algorithm is guaranteed to converge to a globally op-
timal solution regardless of the initial parameters.
If the initial weights assigned to some of the fea-
tures are wildly inappropriate then the algorithm
may take longer to converge, but one would expect
the nal destination to remain the same. However,
as we show later, what is unique in terms of likeli-
hood need not be unique in terms of performance,
and so IIS can be sensitive to the initial weight set-
tings.
Some of the reason for this behaviour may lie
in a relatively subtle eect which we call overlap-
ping features.
1
If some features behave identically in
the training set (but not necessarily in future sam-
ples), IIS cannot distinguish between dierent sets
of weights for those features unless the sum of all
weights in each set is dierent. In fact, the nal
weights assigned to such features will be dependent
on their initial values. Under these conditions, there
will be a family of models, all of which are identical
as far as IIS is concerned, but distinguishable from
each other in terms of performance. This means that
in terms of performance space, the landscape will
contain local maxima. In terms of likelihood space,
the landscape will continue to contain just the single
(global) maximum.
This indeterminacy is clearly undesirable. How-
ever, there are (at least) two practical ways of deal-
ing with it: one either initialises IIS with weights
that are identical (0 is a good choice), or else one
takes a Bayesian approach, and averages over the
space of models. In this paper, we show that the
performance of IIS is sensitive to the initial weight
settings. We show that setting the weights to zero
yields performance that is better than a large set
of randomly initialised models. Also, we show that
model averaging can also reduce indeterminacies in-
troduced through the choice of initial weights.
The rest of this paper is as follows. Section
2 is a restatement of the theory behind IIS. Sec-
tion 3 shows how sparse training material, coupled
1
We do not claim that overlapping features are the sole
reason for our observed eects. As an anonymous reviewer
noted, sparse statistics may also yield similar ndings.
with large models can result in situations whereby
IIS produces suboptimal results. We then move
on to experimental support for our theoretical re-
sults. Our domain is parse selection for broad-
coverage stochastic attribute-value grammars. Sec-
tion 4 shows how we model the broad coverage
attribute-value grammar (mentioned in section 5
used in our experiments), and then present our re-
sults. The rst set of experiments (section 7) deals
with how well IIS, with uniform initial settings. out-
performs models with randomised initial settings.
The second set of experiments shows how model
averging can deal with the problem of initialising the
model. We conclude the paper with some comments
on our work.
2 The Duality Lemma
How can we be certain that IIS seeks out a global
maximum for both likelihood and entropy? The an-
swer is that for the class of problems under consider-
ation, there is only a single maximum - which is then
necessarily the global one. The IIS algorithm sim-
ply `hill-climbs' and seeks to increase the likelihood
of the model being trained. Its success is guaranteed
because of a result which we refer to as the Duality
Lemma
2
.
The proof of the Duality Lemma is contained in
(Laerty et al, 1997) but will be omitted here. In
order to state the lemma, we rst dene the setting
and establish some notation.
Suppose that we have a probability measure space
(
;F ;P), where as usual 
 is a set made up of ele-
ments !, F is a sigma-eld on 
, and P is a prob-
ability measure on F . Suppose further that X is a
simple random variable on 
 - that is to say, it is a
real-valued function on 
, having nite range, and
such that [! : X(!) = x] 2 F . As usual, we will omit
the argument !, so that X indicates a general value
of the function as well as the function itself, and x
denotes [! : X(!) = x]. We will also abuse notation
by letting X denote the set of possible values of x;
the meaning should be clear from context.
We now consider a stochastic process on (
;F ; P ).
We are given a sample of past outputs (data points)
from this process which make up the training data.
We again abuse notation by using ~p to refer to both
the set of data points and the distribution dened by
that set. Let  denote the set of all possible proba-
bility distributions over X ; we seek a model q

2 
which is in some sense the best possible probability
distribution over future outputs. We specically ex-
amine generalized Gibbs distributions, which are of
the form:
q
h
(x) =
1
(Z
q
(h))
exph(x)q(x) (1)
2
This result was called Proposition 4 (Laerty et al, 1997).
In this case, h is a real-valued function on X , q is an
initial probability distribution overX (which may be
the uniform distribution), and Z
q
(h) is a normalising
constant, taking a value such that
P
x2X
q
h
(x) = 1.
The function h here takes the form:
h(x) =
n
X
i=1

i
f
i
(x) = (  f)(x) (2)
where the f
i
(x) are integer feature functions. The
real numbers 
i
are adjustable parameters.
Suppose that we are given the data ~p, an initial
model q
0
, and a set of features f . Laerty et al
(1997) describe two natural sets of models. The rst
is the set P(f; ~p) of all distributions that agree with
~p as to the expected value of the feature function f :
P(f; ~p) = [p 2  : p [f ] = ~p [f ]] (3)
where, as usual, p [f ] denotes the expectation of the
function f under the distribution p.
The second is Q(f; q
0
), the set of generalized
Gibbs distributions based on q
0
and with feature set
f :
Q(f; q
0
) = [(  f) ? q
0
] (4)
Let Q denote the closure of Q in , with respect
to the topology  inherits as a subset of Euclidean
space.
These in turn determine two natural candidates
for the `best' model q

. Let D(pkq) denote the
Kullback-Leibler divergence between two distribu-
tions p and q. The suitable models are:
 Maximum Likelihood Gibbs Distribution. A dis-
tribution in Q with maximum likelihood with
respect to ~p: q
ML

= argmin
q2Q
D(~pkq).
 Maximum Entropy Constrained Distribution. A
distribution in P with maximum entropy rela-
tive to q
0
: q
ME

= argmin
q2P(f;~p)
D(pkq
0
)
The key result of Laerty et al (1997) is that
there is a unique q

satisfying q

= q
ML

= q
ME

.
In Appendix 1 of that paper, the following result is
proved:
The Duality Lemma. Suppose that D(~pkq
0
) <
1. Then there exists a unique q

2  satisfying:
1. q

2 P \ Q
2. D(pkq) = D(pkq

) +D(q

kq) 8p 2 P; q 2 Q
3. q

= argmin
q2Q
D(~pkq)
4. q

= argmin
q2P(f;~p)
D(pkq
0
)
The Duality Lemma is a very useful result, funda-
mental to the ability of IIS to converge upon the sin-
gle maximum likelihood, maximum entropy solution.
The IIS algorithm itself uses a fairly straightforward
technique to look for any maximum of the likelihood
function; because of the lemma, IIS is guaranteed to
approach the solution q

.
3 Limitations With Sparse Training
Data
3.1 Overlapping Features
In this paper, all models have the same training data
and a uniform initial distribution q
0
. This ensures
that D(pkq
0
)  1 and that, for all models, the IIS
algorithm approaches the same optimal distribution
q

. However, our experiments show that not all dis-
tributions obtained by running IIS (to convergence)
are equally good at modelling a set of test data. Per-
formance appears to depend (at least) on the start-
ing values for the weights 
i
. This may be a result
of the following situation:
Consider two features, f
i
and f
j
with i < j,
with weights 
i
and 
j
respectively. Suppose that
f
i
(x) = f
j
(x) for all values of x in ~p, but there exist
values of x outside ~p such that f
i
(x) 6= f
j
(x); that
is, the two functions take exactly the same values
on the set of training data but dier outside of it.
This phenomenon can be called overlapping.
3
Over-
lapping features are commonly found in maximum
entropy models, and are one of the main reasons for
their popularity. In fact, one could argue that all
maximum entropy models found in natural language
applications contain overlapping features. Overlap-
ping features may be present by explicit design (for
example when emulating backing-o smoothing), or
else naturally, for example when using features to
treat words co-occurring with each other.
We assign initial weights 
(0)
i
and 
(0)
j
to the fea-
tures, and after n iterations of the algorithm, they
have been adjusted to 
(n)
i
and 
(n)
j
respectively.
Now consider the target solution q

, as determined
by q
0
and p. Let us assume for convenience that q

is of the form:
q

=
1
Z

exp (
n
X
k=1


k
f
k
) (5)
where Z

is the usual normalising constant. Notice
that q

may not belong to the family of exponential
models Q, but instead may be part of the larger
set Q. Indeed, della Pietra et al state that P \
Q may be empty. This is not a serious limitation
as one can come arbitrarily close to any element of
Q while remaining inside Q. Thus, if q

=2 Q, we
may consider the above expression to be a very close
approximation to q

, such as could be obtained by
running IIS to convergence.
Because the ith and jth features are equal on the
training data, the exact values of 

i
and 

j
have
no eect on the likelihood of the model as long as
3
We can relax this denition of overlapping and allow fea-
tures to largely co-occur together. Depending upon the de-
gree of co-occurrence, we would expect to continue to nd our
results.
their sum remains the same. In this instance, q

is not a single model at all, but rather a family of
models satisfying the condition 

i
+ 

j
= 

ij
for
a particular 

ij
.
4
All models in this family assign
equal likelihood to the training data, and so the IIS
algorithm is unable to distinguish between them.
Under these circumstances we should ensure that


i
= 

j
. Since we have no way to distinguish be-
tween the two features, our model should assign
them equal importance. However, this is not guar-
anteed by the IIS algorithm. In particular, it is less
likely to occur if the initial weights 
(0)
i
and 
(0)
j
are
not equal.
3.2 IIS and Overlapping Features - A
Simple Example
Suppose that our model has a vector of parameters
 = [
i
: i = 1 : : : n] and we wish to change it to
 + ?, where ? = [?
i
: i = 1 : : : n] The IIS algorithm
considers each 
i
in turn. It chooses ?
i
to maximize
an auxiliary function B(?j), which provides a lower
bound on the change in log-likelihood of the model.
Each adjustment 
i
 
i
+ ?
i
is guaranteed to in-
crease the likelihood of the model and thus approach
our ideal solution q

. This process is repeated un-
til convergence. There are no inherent restrictions
on the value of ?
i
; it may be positive or negative,
and large or small compared to the values taken for
dierent features.
Now, suppose that the features f
i
and f
j
over-
lap and we halt the algorithm after t iterations.
Clearly, the algorithm cannot guarantee that the -
nal weights 
t
i
and 
t
j
will be equal. The following
highly simplied example should demonstrate why
this is the case.
Example 1: Imagine that our model has two
overlapping features f
1
and f
2
, and q

is the fam-
ily of models in which 
1
+ 
2
= 5. Suppose that
the initial weights are 
(0)
1
= 5; 
0
2
= 0. Recall that
at the ith step of an iteration, the IIS algorithm
considers the change in log-likelihood of the model
which can be made by only adjusting the ith pa-
rameter. In this case no change is possible as the
sum 
1
+ 
2
is already at its optimum value, so the
algorithm terminates with 
t
1
= 5 and 
t
2
= 0. We
have assigned far greater importance to f
1
than to
f
2
with no justication for doing so. Clearly, this
assumption might not be warranted.
3.3 IIS and Overlapping Features in
Practice
The situation will obviously be much more compli-
cated in practice. Most importantly, there is no such
4
The number 
ij
is not a `constant' as such, since any
vector of weights is in a sense unique only up to multiplication
by a positive constant. This will be examined in greater detail
when we consider overlapping features in practice.
thing as an absolute \optimum" vector of weights


. As a result, no one parameter can be regarded
as xed until the algorithm has converged for all pa-
rameters. In the above example, our \true" set of
target solutions is those for which 
1
+ 
2
is a con-
stant. Since there are no other features, IIS would in
fact terminate at once for any pair of initial weights.
Suppose that we added a third feature f
3
which
did not overlap the rst two. Our set of target solu-
tions would then be of the form 
1
+
2
= 

12
k; 
3
=


3
k for some xed 
12
and 
3
and for any k > 0.
The adjustments made to 
1
and 
2
will depend on
those made to 
3
, and the algorithm will not neces-
sarily terminate after the rst pass.
The following example will describe what happens
in this more realistic situation. It will also explain
the possible benets of setting all initial weights to
the same value.
Example 2: Let the model have the three fea-
tures described above, with initial weights 
0
1
=
5; 
0
2
= 0; 
0
3
= 1 and a family of target solutions q

dened by 
1
+ 
2
= 5k; 
3
= k for any k > 0. IIS
again terminates at once, because the initial model
is already part of the family q

. Again there is an
unjustied dierence between the nal weights for
f
1
and f
2
.
Now suppose that all three initial weights are
set to zero. Imagine for simplicity that we have
restricted the algorithm to adjust weights only by
zero or 1. On the rst pass, all three weights are
changed to 1. On the second, 
1
and 
2
are increased
to 2; 
3
remains at 1. In the third iteration, 
1
is set
to 3, 
2
remains at 2 and 
3
at 1, and the algorithm
terminates.
5
Notice that, although there is still a dierence be-
tween the nal weights for f
1
and f
2
, it is much less
than before. This more closely approaches the ideal
situation in which the weights are equal.
This concludes our theoretical treatment of IIS.
We now show experimentally the inuence that the
initial weight settings have, and how it can be
minimised. Our strategy is to use plausible fea-
tures, as found in a realistic domain (parse selec-
tion for stochastic attribute-value grammars), and
rstly show what happens when the initial weight
settings are set uniformly (to zero). We then show
what happens when these initial settings are ran-
domly set, and nally, what happens when we av-
erage over randomly initialised maximum likelihood
solutions.
5
The exact behaviour of the algorithm will depend on the
training data, so this is not the only imaginable outcome, but
it is certainly a plausible one.
4 Log-linear Modelling of
Attribute-Value Grammars
Here we show how attribute-value grammars may be
modelled using log-linear models. Abney gives fuller
details (Abney, 1997).
Let G be an attribute-value grammar, and D a set
of sentences within the string-set dened by L(G).
A log-linear model, M , consist of two components:
a set of features, F and a set of weights, .
The (unnormalised) total weight of a parse x,
 (x), is a function of the k features that are `active'
on a parse:
 (x) = exp(
k
X
i=1

i
f
i
(x)) (6)
The probability of a parse, P (x j M), is simply
the result of normalising the total weight associated
with that parse:
P (x jM) =
1
Z
 (x) (7)
Z =
X
y2

 (y) (8)

 is the union of the set of parses assigned to each
sentence in D by the grammar G, such that each
parse in 
 is unique in terms of the features that are
active on it. Normally a parse can be viewed as the
set of features that are active on it.
The interpretation of this probability (equation 7)
depends upon the application of the model. Here,
we use parse probabilities to reect preferences for
parses.
5 The Grammar
The grammar we model with log-linear models
(called the Tag Sequence Grammar (Briscoe and
Carroll, 1996), or TSG for short) was manually de-
veloped with regard to coverage, and when compiled
consists of 455 Denite Clause Grammar (DCG)
rules. It does not parse sequences of words directly,
but instead assigns derivations to sequences of part-
of-speech tags (using the CLAWS2 tagset). The
grammar is relatively shallow (for example, it does
not fully analyse unbounded dependencies).
6 Modelling the Grammar
Modelling the TSG with respect to the parsed Wall
Street Journal consists of two steps: creation of a
feature set and denition of a reference distribution
(the target model, ~p).
6.1 Feature Set
Modelling the TSG with respect to the parsed Wall
Street Journal consists of two steps: creation of a
AP/a1:unimpeded
A1/app1:unimpeded
a
a
a
!
!
!
unimpeded PP/p1:by
P1/pn1:by
b
b
"
"
by N1/n:tra?c
tra?c
Figure 1: TSG Parse Fragment
feature set and denition of the reference distribu-
tion.
Our feature set is created by parsing sentences
in the training set, and using each parse to instan-
tiate templates. Each template denes a family of
features. Our templates are motivated by the ob-
servations that linguistically-stipulated units (DCG
rules) are informative, and that many DCG appli-
cations in preferred parses can be predicted using
lexical information.
The rst template creates features that count
the number of times a DCG instantiationis present
within a parse.
6
For example, suppose we parsed
the Wall Street Journal AP:
1 unimpeded by tra?c
A parse tree generated by TSG might be as shown
in gure 1. Here, to save on space, we have labelled
each interior node in the parse tree with TSG rule
names, and not attribute-value bundles. Further-
more, we have annotated each node with the head
word of the phrase in question. Within our gram-
mar, heads are (usually) explicitly marked. This
means we do not have to make any guesses when
identifying the head of a local tree. With head in-
formation, we are able to lexicalise models. We have
suppressed tagging information.
For example, a feature dened using this template
might count the number of times the we saw:
AP/a1
A1/app1
in a parse. Such features record some of the context
of the rule application, in that rule applications that
6
Note, all our features suppress any terminals that appear
in a local tree. Lexical information is included when we decide
to lexicalise features.
dier in terms of how attributes are bound will be
modelled by dierent features.
Our second template creates features that are par-
tially lexicalised. For each local tree (of depth one)
that has a PP daughter, we create a feature that
counts the number of times that local tree, decorated
with the head-word of the PP, was seen in a parse.
An example of such a lexicalised feature would be:
A1/app1
PP/p1:by
These features are designed to model PP attach-
ments that can be resolved using the head of the
PP.
The third and nal template creates features that
are again partially lexicalised. This time, we create
local trees of depth one that are decorated with the
head word. For example, here is one such feature:
AP/a1:unimpeded
A1/app1
Note the second and third templates result in fea-
tures that overlap with features resulting from ap-
plications of the rst template.
6.2 Reference Distribution
We create the reference distribution R (an associa-
tion of probabilities with TSG parses of sentences,
such that the probabilities reect parse preferences)
using the following process:
1. Take the training set of parses and for each
parse, compare the structural dierences be-
tween it and a reference treebank parse.
2. Map these tree similarity scores into probabili-
ties (where the sum of all reference probabilities
for all parses sums to one).
Again, see Anon for more details.
This concludes our discussion of how we model
grammars. We now go on to present our experimen-
tal investigation of the inuence of the initial weight
settings.
7 Experiments
Here we present three sets of experiments. The
rst set shows the performance of maximum entropy
when the initial weight setting are zero. The sec-
ond set show the eects of randomised initial setting,
and so establishes (an estimate of) the variation in
performance space. The third set of experiments
showed how the inuence of the initial weight set-
tings could be minimised by averaging over manyh
models.
Throughout, we used the same training set. This
consisted of a sample of 53795 parses (produced from
sentences at most 15 tokens long, with at most 15
parses per sentence). The sentences were drawn
from the parsed Wall Street Journal, and all could
be parsed using our grammar. The motivation for
this choice of training set came from the fact that
when the sample of sentences is too small, the result-
ing model will tend to undert. Likewise, when the
training set is too large, the model will tend to over-
t. A sample of appropriate size (which can be found
using a simple search, as Osborne (2000) demon-
strated) will therefore neither signicantly undert
nor overt. Quite apart from estimation issues re-
lated to sample size, because we repeatedly estimate
models, using a sample that is just su?ciently large
(and no larger) allows us to make signicant compu-
tational savings.
We used a disjoint development set and testing
set. The development set consisted of 2620 parses,
derived from parsing sentences at most 30 tokens
long, with at most 100 parses per sentence. The test-
ing set was randomly sampled from the Wall Street
Journal, and consisted of 469 sentences, with each
sentence at most 30 tokens long, with at most 100
parses per sentence. Each sentence has on average
60:0 parses per sentence.
The model we used throughout contained 75171
features.
For all experiments, we ran IIS for the same num-
ber of iterations (75). This number was selected as
the number of iterations that produced the best per-
formance for maximum entropy (our yardstick).
Evaluation was in terms of exact match: for each
sentence in the test set, we awarded ourselves a
point if the estimated model ranked highest the same
parse that was ranked highest using the reference
probabilities.
7
Ties were randomly broken.
7.1 Maximum Entropy Results using
Uniform Initialisation
A model trained using weights all initialised to zero
yielded an exact match score of 52:0%.
7.2 Randomised Models
A pool of 10; 000 models was created by randomly
setting the initial weights to values in the range [-
0.3,0.3] and then estimating the nal weights using
the training set.
The histogram in gure 2 shows the number of
models that produced the same results on the test-
ing material. As can be seen, performance is roughly
normally distributed, with some local minima hav-
ing a much wider basin of attraction than other min-
ima. Also, note that all of these models underper-
7
We use exact match as an arbitrary evaluation metric.
The particular choice of metric is not crucial to the results of
this paper.
forms a model crated using uniform initialisation.
8
The performance of the worst model was 24:1%.
This is our lower bound, and is less than half that
of basic maxent. The best randomly selected model
had a performance of 46:5%.
0
50
100
150
200
250
300
350
400
450
20 25 30 35 40 45 50
N
um
be
r o
f M
od
el
s
Exact Match
"rand-hist04"
Figure 2: Distribution of models with randomly ini-
tialised starting conditions
7.3 Averaging over models
To see whether an ensemble of such randomised
models could cancel-out the inuence of the ini-
tial weight settings, we created a pool of 600 ran-
domised models, and then combined then together
using equation 9:
P (x jM
1
: : :M
n
) =
Q
n
i=1
P (x jM
i
)
P
y
Q
n
j=1
P (y jM
j
)
(9)
Because it is possible that some subset of the mod-
els outperforms an ensemble using all models, we
uniformly sampled, with replacement, from this pool
models for inclusion into the nal ensemble. Ran-
dom selection introduces variation, so we repeated
our results ten time and averaged the results.
Figure 3 shows our results (N is the number of
models in each ensemble, x is the mean performance
and 
2
is the standard deviation). The nal entry
(marked all) shows the performance obtained using
an ensemble consisting of all models in the model,
equally weighted.
As can be seen, increasing the number of models
in the ensemble reduces the inuence of the initial
weight settings. However, even with a large pool
of models, we still marginally underperform uniform
initialisation.
8
Running IIS until convergence did not narrow the gap,
so our ndings cannot be attributed to dierential rates of
convergence.
N x 
2
N x 
2
1 38.78 0.12 50 49.94 1.24
2 41.41 1.04 100 50.55 0.70
3 44.24 1.63 150 50.62 0.88
5 46.57 1.69 200 50.66 0.75
10 47.21 1.71 300 50.81 0.73
20 49.23 1.19 600 51.04 0.36
all 51.39 -
Figure 3: Model averaging results
Note that we have not carried out the control ex-
periment of repeating our runs using features which
do not overlap with each other. Unfortunately, doing
this would probably mean having to create models
that are unnatural. We therefore cannot be abso-
lutely certain that the sole reason for the existance
of local optima is the presence of overlapping fea-
tures. However, we can be sure that they do exist,
and that varying the initial weight settings will re-
veal them.
8 Comments
We have established that, in the presence of overlap-
ping features, the values of the initial weights can
aect the utility of the nal model. Our experi-
mental results support this nding. In general, one
might encounter a similar problem (overlapping fea-
tures) with what might be called `semi-overlapping
features': features which are very similar (but not
identical) on the training data and very dierent
outside of it.
IIS could be made more robust to the choice of
initial parameters in a number of ways:
 The simplest course of action is to set al ini-
tial weights to the same value. Although zero
is often a convenient initial value, in principle
any real number would do, since the IIS algo-
rithm can reach a given optimal solution from
any starting point in the space of initial param-
eters.
 We could also examine all the features to de-
termine which ones overlap, and force it to bal-
ance the nal weights of these features. For very
large models, this may be prohibitively di?cult
and time-consuming.
 Model averaging can also cancel out variations
caused by a particular choice of initial settings.
However, this implies a greater computational
burden as IIS will need to be run many times in
order to gain a representative sample of models.
 The number of features in the model could be
reduced using feature selection methods (for ex-
ample (Mullen and Osborne, 2000)).
Although IIS is a useful tool for estimating log-
linear models, we have since moved-on to estimating
models using limited-memory variable-metric meth-
ods (Malouf, 2002). Our ndings show that conver-
gence, for a range of problems, is faster. An inter-
esting question is seeing the extent to which other
numerical methods for estimating log-linear models
are sensitive to initial parameter values. Finally, it
should be noted that our theoretical results apply to
a more general setting than that of log-linear mod-
els trained using the IIS algorithm. The problem of
overlapping features could in principle occur in any
situation in which a model has a linear combination
of features, and a `hill-climbing' algorithm is used to
seek a maximum-likelihood solution.
Acknowledgements
We wish to thank Steve Clark for useful discussions
about IIS, Rob Malouf for supplying the IIS imple-
mentation and the two anonymous reviewers. Iain
Bancarz was supported by the EPSRC grant POEM.
References
Steven P. Abney. 1997. Stochastic Attribute-
Value Grammars. Computational Linguistics,
23(4):597{618, December.
Ted Briscoe and John Carroll. 1996. Automatic
Extraction of Subcategorization from Corpora.
In Proceedings of the 5
th
Conference on Applied
NLP, pages 356{363, Washington, DC.
Mark Johnson, Stuart Geman, Stephen Cannon,
Zhiyi Chi, and Stephan Riezler. 1999. Estimators
for Stochastic \Unication-Based" Grammars. In
37
th
Annual Meeting of the ACL.
J. Laerty, S. Della Pietra, and V. Della Pietra.
1997. Inducing features of random elds. IEEE
Transactions on Pattern Analysis and Machine
Intelligence, 19(4):380{393, April.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In
Proceedings of the joint CoNLL-WVLC Meeting,
Taipei, Taiwan. ACL. To appear.
Tony Mullen and Miles Osborne. 2000. Overtting
Avoidance for Stochastic Modeling of Attribute-
Value Grammars. In Claire Cardie, Walter Daele-
mans, Claire Nedellec, and Erik Tjong Kim Sang,
editors, Proceedings of the Computational Natural
Language learning 2000, pages 49{54. ACL, Lis-
bon, Portugal.
Kamal Nigam, John Laerty, , and Andrew Mc-
Callum. 1999. Using maximum entropy for text
classication. In IJCAI-99 Workshop on Machine
Learning for Information Filtering,.
Miles Osborne. 2000. Estimation of Stochastic
Attribute-Value Grammars using an Informative
Sample. In The 18
th
International Conference on
Computational Linguistics, Saarbrucken, August.
