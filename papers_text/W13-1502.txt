Proceedings of the 2th Workshop of Natural Language Processing for Improving Textual Accessibility (NLP4ITA), pages 11?19,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Open Book: a tool for helping ASD users? semantic comprehension
Eduard Barbu
University of Jae?n
Paraje de Las Lagunillas
Jae?n, 23071, Spain
ebarbu@ujaen.es
Maria Teresa Mart??n-Valdivia
University of Jae?n
Paraje de Las Lagunillas
Jae?n, 23071, Spain
maite@ujaen.es
Luis Alfonso Uren?a-Lo?pez
University of Jae?n
Paraje de Las Lagunillas
Jae?n, 23071, Spain
laurena@ujaen.es
Abstract
Persons affected by Autism Spectrum Dis-
orders (ASD) present impairments in so-
cial interaction. A significant percentile of
them have inadequate reading comprehension
skills. In the ongoing FIRST project we build
a multilingual tool called Open Book that
helps the ASD people to better understand the
texts. The tool applies a series of automatic
transformations to user documents to identify
and remove the reading obstacles to compre-
hension. We focus on three semantic compo-
nents: an Image component that retrieves im-
ages for the concepts in the text, an idiom de-
tection component and a topic model compo-
nent. Moreover, we present the personaliza-
tion component that adapts the system output
to user preferences.
1 Introduction
Autism Spectrum Disorders are widespread and af-
fect every 6 people in 10000 according to Autism
Europe site1. The disorder is chiefly characterized
by impairments in social interaction and by repet-
itive and stereotyped behaviour (Attwood, 2007).
People affected by ASD are not able to communi-
cate properly because they lack an adequate theory
of mind (Baron-Cohen, 2001). Therefore, they are
not able to infer the other persons? mental states:
beliefs, emotions or desires. This lack of empathy
prevents the people with ASD to have a fulfilled so-
cial life. Their inability to understand others leads
to the incapacity to communicate their wishes and
desires and to social marginalization.
1http://www.autismeurope.org/
The FIRST project seeks to make a small step
towards integration of ASD people in the informa-
tion society by addressing their reading comprehen-
sion ability. It is well known that many of the ASD
people have a wide range of language difficulties.
Psychological studies showed that they have prob-
lems understanding less common words (Gillispie,
2008), have difficulty comprehending polysemous
words (Fossett and Mirenda, 2006) and have trou-
bles dealing with figurative language (Douglas et al,
2011). The absence of good comprehension skills
impedes the ASD students to participate in curricu-
lum activities or to properly interact with their col-
leagues in chats or blogs. To enhance the reading
comprehension of ASD people we are developing a
software tool. It is built by partners in Academia and
Industry in close collaboration with teams of psy-
chologists and clinicians. It operates in a multilin-
gual setting and is able to process texts in English,
Spanish and Bulgarian languages. Based on litera-
ture research and on a series of studies performed
in the United Kingdom, Spain and Bulgaria with a
variety of autistic patients ranging from children to
adults the psychologists identified a series of obsta-
cles in reading comprehensions that the tool should
remove. From a linguistic point of view they can
be classified in syntactic obstacles (difficulty in pro-
cessing relative clauses, for example) and semantic
obstacles (difficulty in understanding rare or special-
ized terms or in comprehension of idioms, for exam-
ple). The tool applies a series of automatic transfor-
mations to user documents to identify and remove
the reading obstacles to comprehension. It also as-
sists the carers , persons that assist the ASD people
in every day life tasks, to correct the results of auto-
11
matic processing and prepare the documents for the
users. This paper will focus on three essential soft-
ware components related to semantic processing: a
software component that adds images to concepts
in the text, a software component that identifies id-
iomatic expressions and a component that computes
the topics of the document. Moreover, we present
the personalization component that adapts the sys-
tem output to user preferences. The rest of the paper
has the following structure: the next section briefly
presents other similar tools on the market. Section
3 presents a simple procedure for identifying the
obstacles ASD people have in reading comprehen-
sions. Section 4 shows the architecture of the seman-
tic processing components and the personalization
component. The last section draws the conclusions
and comments on the future work. Before present-
ing the main part of the article we make a brief note:
throughout the paper we will use whenever possible
the term ?user? instead of ASD people or patients.
2 Related Work
A number of software tools were developed to sup-
port the learning of ASD people. Probably the
most known one is Mind Reading2, a tool that
teaches human emotions using a library of 412 ba-
sic human emotions illustrated by images and video.
Other well known software is VAST-Autism3, a tool
that supports the understanding of linguistic units:
words, phrase and sentences by combining spoken
language and images. ?Stories about me? is an IPad
application4 that allows early learners to compose
stories about themselves. All these tools and others
from the same category are complementary to Open
Book. However, they are restricted to pre-stored
texts and not able to accommodate new pieces of
information. The main characteristics that sets aside
our tool is its scalability and the fact that it is the only
tool that uses NLP techniques to enhance text com-
prehension. Even if the carers correct the automatic
processing output, part of their work is automatized.
2http://www.jkp.com/mindreading/index.php
3http://a4cwsn.com/2011/03/vast-autism-1-core/
4http://www.limitedcue.com/our-apps/
3 Obstacles in text comprehension
Most of the automatic operations executed by the
Open Book tool are actually manually performed by
the carers. They simplify the parts of the text that are
difficult to understand. We compared the texts be-
fore and after the manual simplification process and
registered the main operations. The main simplifica-
tion operations ordered by frequency performed by
carers for 25 Spanish documents belonging to dif-
ferent genders: rent contracts, newspaper articles,
children literature, health care advices, are the fol-
lowing:
1. Synonymous (64 Operations). A noun or an ad-
jective is replaced by its less complex synonym.
2. Sentence Splitting (40 Operations). A long sen-
tence is split in shorter sentences or in a bullet
list.
3. Definition (34 Operations). A difficult term is
explained using Wikipedia or a dictionary.
4. Near Synonymous (33 Operations). The term
is replaced by a near synonym.
5. Image (27 Operations) A concept is illustrated
by an image.
6. Explanation (24 Operations). A sentence is
rewritten using different words.
7. Deletion (17 Operations). Parts of the sentence
are removed.
8. Coreference(17 Operations). A coreference
resolution is performed.
9. Syntactic Operation (9 Operations). A trans-
formation on the syntactic parse trees is per-
formed.
10. Figurative Language (9 Operations). An idiom
or metaphor is explained.
11. Summarization (3 Operations). The content of
a sentence or paragraph is summarized.
The most frequent operations with the exception
of Sentence Splitting are semantic in nature: replac-
ing a word with a synonym, defining the difficult
12
terms. The only obstacle that cannot be tackled au-
tomatically is Explanation. The Explanation entails
interpretation of the sentence or paragraph and can-
not be reduced to simpler operations.
A similar inventory has been done in English.
Here the most frequent operation are Sentence Split-
ting, Synonyms and Definition. The operations are
similar across English and Spanish but their ordering
differs slightly.
4 The Semantic System
In this paper we focus on three semantic compo-
nents meant to augment the reading experience of
the users. The components enhance the meaning
of documents assigning images to the representa-
tive and difficult concepts, detecting and explaining
the idiomatic expressions or computing the topics to
which the documents belong.
In addition to these components we present an-
other component called Personalization. Strictly
speaking, the personalization is not related to se-
mantic processing per se but, nevertheless, it has
an important role in the final system. Its role
is to aggregate the output of all software compo-
nents,including the three ones mentioned above, and
adapt it according to user?s needs.
All the input and output documents handled by
NLP components are GATE (Cunningham et al,
2011) documents. There are three reasons why
GATE documents are preferred: reusability, extensi-
bility and flexibility. A GATE document is reusable
because there are many software components devel-
oped both in academy and industry, most of them
collected in repositories by University of Sheffield,
that work with this format. A GATE document is
extensible because new components can add their
annotations without modifying previous annotations
or the content of the document. Moreover, in case
there is no dependence between the software com-
ponents the annotations can be added in parallel. Fi-
nally, a GATE document is flexible because it al-
lows the creation of various personalization work-
flows based on the specified attributes of the anno-
tations. The GATE document format is inspired by
TIPSTER architecture design5 and contains in ad-
dition to the text or multimedia content annotations
5http://www.itl.nist.gov/iaui/894.02/related projects/tipster/
grouped in Annotation Sets and features. The GATE
format requires that an annotation has the following
mandatory features: an id, a type and a span. The
span defines the starting and the ending offsets of
the annotation in the document text.
Each developed software component adds its an-
notations in separate name annotation sets. The
components are distributed and exposed to the out-
side world as SOAP web services. Throughout the
rest of the paper we will use interchangeably the
terms: component, software component and web
service.
For each semantic component we discuss:
? The reasons for its development. In general,
there are two reasons for the development of a
certain software component: previous studies
in the literature and studies performed by our
psychologists and clinicians. In this paper we
will give only motivations from previous stud-
ies because the discussion of our clinicians and
psychologist studies are beyond the purpose of
this paper.
? Its architecture. We present both the foreseen
characteristics of the component and what was
actually achieved at this stage but we focus on
the latter.
? The annotations it added. We discuss all the
features of the annotations added by each com-
ponent.
4.1 The Image Web Service
In her landmark book, ?Thinking in Pictures: My
Life with Autism?, Temple Grandin (1996), a scien-
tist affected by ASD, gives an inside testimony for
the importance of pictures in the life of ASD peo-
ple:
?Growing up, I learned to convert abstract ideas
into pictures as a way to understand them. I visu-
alized concepts such as peace or honesty with sym-
bolic images. I thought of peace as a dove, an Indian
peace pipe, or TV or newsreel footage of the signing
of a peace agreement. Honesty was represented by
an image of placing one?s hand on the Bible in court.
A news report describing a person returning a wallet
with all the money in it provided a picture of honest
behavior.?
13
Grandin suggests that not only the ASD people
need images to understand abstract concepts but that
most of their thought process is visual. Other studies
document the importance of images in ASD: Kana
and colleagues (2006) show that the ASD people use
mental imagery even for comprehension of low im-
agery sentences. In an autobiographic study Grandin
(2009) narrates that she uses language to retrieve
pictures from the memory in a way similar to an im-
age retrieval system.
The image component assigns images to concepts
in the text and to concepts summarizing the meaning
of the paragraphs or the meaning of the whole doc-
ument. Currently we are able to assign images to
the concepts in the text and to the topics computed
for the document. Before retrieving the images from
the database we need a procedure for identifying
the difficult concepts. The research literature helps
with this task, too. It says that our users have diffi-
culty understanding less common words (Lopez and
Leekam, 2003) and that they need word disambigua-
tion (Fossett and Mirenda, 2006).
From an architectural point of view the Image
Web Service incorporates three independent sub-
components:
? Document Indexing. The Document Index-
ing sub-component indexes the document con-
tent for fast access and stores all offsets of the
indexing units. The indexed textual units are
words or combinations of words (e.g., terms).
? Difficult Concepts Detection. The difficult
concepts are words or terms (e.g. named enti-
ties) disambiguated against comprehensive re-
sources: like Wordnet and Wikipedia. This
sub-component formalizes the notion ?difficult
to understand? for the users. It should be based
on statistical procedures for identifying rare
terms as well as on heuristics for evaluating the
term complexity from a phonological point of
view. For the time being the sub-component
searches in the document a precompiled list of
terms.
? Image Retrieval. This sub-component re-
trieves the images corresponding to difficult
concepts from image databases or from web
searching engines like Google and Bing.
The Image Web Service operates in automated
mode or in on-demand mode. In the automated
mode a document received by the Image Web Ser-
vice is processed according to the working flow in
Figure 1. In the on-demand mode the user high-
lights the concepts (s)he considers difficult and the
web service retrieves the corresponding image or set
of images. The difference between the two modes
of operations is that in the on-demand mode the dif-
ficult concept detection is performed manually.
Once the GATE document is received by the sys-
tem it is tokenized, POS (Part of Speech) tagged
and lemmatized (if these operations were not already
performed by other component) by a layer that is not
presented in Figure 1. Subsequently, the document
content is indexed by Document Indexing subcom-
ponent. For the time being the terms of the doc-
ument are disambiguated against Wordnet. The Im-
age Retrieval component retrieves the corresponding
images from the image database.
The current version uses the ImageNet Database
(Deng et al, 2009) as image database. The Ima-
geNet database pairs the synsets in Princeton Word-
net with images automatically retrieved from Web
and cleaned with the aid of Mechanical Turk. Be-
cause the wordnets for Spanish and Bulgarian are ei-
ther small or not publicly available future versions of
the Web Service will disambiguate the terms against
Wikipedia articles and retrieve the image illustrating
the article title. All annotations are added in ?Im-
ageAnnotationSet?. An annotation contains the fol-
lowing features:
? Image Disambiguation Confidence is the con-
fidence of the WSD (Word Sense Disambigua-
tion) algorithm in disambiguating a concept.
? Image URL represents the URL address of the
retrieved image
? Image Retrieval Confidence is the confidence
of assigning an image to a disambiguated con-
cept.
In the on-demand mode the images are also re-
trieved from Google and Bing Web Services and
the list of retrieved images is presented to the carer
and/or to the users. The carer or user selects the im-
age and inserts it in the appropriate place in the doc-
ument.
14
Figure 1: The Image Web Service.
4.2 The Idiom Detection Web Service
In the actual linguistic discourse and lexicographical
practice the term ?idiom? is applied to a fuzzy cat-
egory defined by prototypical examples: ?kick the
bucket?, ?keep tabs on?, etc. Because we cannot
provide definitions for idioms we venture to spec-
ify three important properties that characterize them
(Nunberg et al, 1994) :
? Conventionality.The meaning of idioms are not
compositional.
? Inflexibility. Idioms appear in a limited range
of syntactic constructions.
? Figuration. The line between idioms and
other figurative language is somewhat blurred
because other figurative constructions like
metaphors: ?take the bull by the horns? or hy-
perboles: ?not worth the paper it?s printed on?
are also considered idioms.
The figurative language in general and the id-
ioms in particular present particular problems for
our users as they are not able to grasp the meaning
of these expressions (Douglas et al, 2011). To facil-
itate the understanding of idiomatic expressions our
system identifies the expressions and provide defini-
tions for them.
The actual Idiom Web Service finds idiomatic ex-
pressions in the user submitted documents by simple
text matching. The final version of Idiom Web Ser-
vice will use a combination of trained models and
hand written rules for idiom detection. Moreover, it
is also envisaged that other types of figurative lan-
guage like metaphors could be detected. At the mo-
ment the detection is based on precompiled lists of
idioms and their definitions. Because the compo-
nent works by simple text matching, it is language
independent. Unlike the actual version of the Idiom
Web Service the final version should be both lan-
guage and domain dependent. The architecture of
this simple component is presented in Figure 2 .
Figure 2: The Idiom Web Service.
The GATE input document is indexed by the doc-
ument indexing component for providing fast ac-
cess to its content. For each language we compiled
list of idioms from web sources, dictionaries and
Wikipedia. All idiom annotations are added in the
?IdiomAnnotationSet?. An annotation contains the
following features:
? Idiom Confidence represents the confidence the
algorithm assigns to a particular idiom detec-
tion.
? Definition represents the definition for the ex-
tracted idiom.
4.3 The Topic Models Web Service
The mathematical details of the topics models are
somewhat harder to grasp but the main intuition be-
hind is easily understood. Consider an astrobiology
document. Most likely it will talk about at least three
topics: biology, computer models of life and astron-
omy. It will contain words like: cell, molecules, life
related to the biology topic; model, computer, data,
number related to computer models of life topic and
star, galaxy, universe, cluster related with astronomy
topic. The topic models are used to organize vast
15
collections of documents based on the themes or dis-
courses that permeate the collection. From a practi-
cal point of view the topics can be viewed as clus-
ters of words (those related to the three topics in the
example above are good examples) that frequently
co-occur in the collection. The main assumption be-
hind Latent Dirichlet Allocation (LDA) (Blei et al,
2003), the simplest topic model technique, is that
the documents in the collections were generated by a
random process in which the topics are drawn from
a given distribution of topics and words are drawn
from the topics themselves. The task of LDA and
other probabilistic topic models is to construct the
topic distribution and the topics (which are basically
probability distributions over words) starting with
the documents in the collection.
The Topic Models Web Service is based on an
implementation of LDA. It assigns topics to the
user submitted documents, thus informing about the
themes traversing the documents and facilitating the
browsing of the document repository. The topics
themselves perform a kind of summarization of doc-
uments showing, before actual reading experience,
what the document is about.
The architecture of the Topic Models Web Service
is presented in Figure 3.
Figure 3: The Topic Model Web Service.
Once a document is received it is first dispatched
to the Feature Extraction Module where it is POS
tagged and lemmatized and the relevant features are
extracted. As for training models, the features are
all nouns, name entities and verbs in the document.
Then the Topic Inferencer module loads the appro-
priate domain model and performs the inference and
assigns the new topics to the document. There are
three domains/genders that the users of our system
are mainly interested in: News, Health Domain and
Literature. For each of these domains we train topic
models in each of the three languages of the project.
Of course the system is easily extensible to other do-
mains. Adding a new model is simply a matter of
loading it in the system and modifying a configura-
tion file.
The output of the Web System is a document in
the GATE format containing the most important top-
ics and the most significant words in the topics. The
last two parameters can be configured (by default
they are set to 3 and 5 respectively). Unlike the an-
notations for the previous components the annota-
tion for Topic Model Web Service are not added for
span of texts in the original document. This is be-
cause the topics are not necessarily words belonging
to the original document. Strictly speaking the top-
ics are attributes of the original document and there-
fore they are added in the ?GateDocumentFeatures?
section. An example of an output document contain-
ing the section corresponding to the document topics
is given in Figure 4.
Figure 4: The GATE Document Representation of the
Computed Topic Model.
Currently we trained three topic models cor-
responding to the three above mentioned do-
mains/genres for the Spanish language:
? News. The corpus of news contains more
than 500.000 documents downloaded from the
web pages of the main Spanish newspapers (El
Mundo, El Pais, La Razon, etc. . . ). The topic
model is trained using a subset of 50.000 docu-
ments and 400 topics. The optimum number of
documents and topics will be determined when
16
the users test the component. However, one
constraint on the number of documents to use
for model training is the time required to per-
form the inference: if the stored model is too
big then the inference time can exceed the time
limit the users expect.
? Health Domain. The corpus contains 7168
Spanish documents about general health is-
sues (healthy alimentation, description of the
causes and treatments of common diseases,
etc.) downloaded from medlineplus portal. The
topic model is trained with all documents and
100 topics. In the future we will extend both
the corpus and the topic model.
? Literature. The corpus contains literature in
two genders: children literature (121 Spanish
translation of Grimm brothers stories) and 336
Spanish novels. Since for the time being the
corpus is quite small we train a topic model
with 20 topics just for the system testing pur-
poses.
For the English and the Bulgarian language we
have prepared corpora for each domain but we have
not trained a topic model yet. To create the training
model all corpora should be POS tagged, lemma-
tized and the name entities recognized. The features
for training the topic model are all nouns, name en-
tities and verbs in the corpora.
4.4 Personalization
The role of the Personalization Web Service is to
adapt the output of the system to the user?s expe-
rience. This is achieved by building both static and
dynamic user profiles. The static user profiles con-
tain a number of parameters that can be manually
set. Unlike the static profiles, the dynamic ones con-
tain a series of parameters whose values are learnt
automatically. The system registers a series of ac-
tions the users or carers perform with the text. For
example, they can accept or reject the decisions per-
formed by other software components. Based on
editing operations a dynamic user profile will be
built incrementally by the system. Because at this
stage of the project the details of the dynamic pro-
file are not yet fully specified we focus on the static
profile in this section.
The architecture of the Personalization compo-
nent is presented in Figure 5.
Figure 5: The Personalization Web Service.
In addition to the web services presented in the
previous sections (The Idiom Web Service and The
Image Web Service) the Personalization Web Ser-
vice receives input from Anaphora Web Service and
Syntax Simplification Web Service. The Anaphora
component resolves the pronominal anaphora and
the Syntax Simplification component identifies and
eliminates difficult syntactic constructions. The Per-
sonalization component aggregates the input from
all web services and based on the parameters speci-
fied in the static profile (the wheel in Figure 5) trans-
forms the aggregate document according to the user
preferences. The personalization parameters in the
static profile are the following:
1. Image Disambiguation Confidence. The image
annotation is dropped when the corresponding
concept disambiguation confidence is less than
the threshold.
2. Image Retrieval Confidence. The image an-
notation is dropped when the assigned image
is retrieved with a confidence lower than the
threshold.
3. Idiom Confidence. The idiom annotation is
dropped when the assigned idiom confidence is
less than the threshold.
4. Anaphora Confidence. The pronominal
anaphora annotations are dropped when the
anaphor is solved with a confidence less than
the threshold.
5. Anaphora Complexity. The parameter assess
the complexity of anaphors. If the anaphora
17
complexity score is less than the specified
threshold it drops the resolved pronominal
anaphora.
6. Syntactic Complexity. It drops all annotations
for which the syntactic complexity is less than
the threshold.
The user can also reject the entire output of a cer-
tain web service if he does not need the functionality.
For example, the user can require to display or not
the images, to resolve or not the anaphora, to sim-
plify the sentences or not, etc. In case the output of
a certain web service is desired the user can spec-
ify the minimum level of confidence accepted. Any
annotation that has a level of confidence lower than
the specified threshold will be dropped. In addition
to the parameters related to document content the
static profile includes parameters related to graphi-
cal appearance (e.g. fonts or user themes) that are
not discussed here.
5 Conclusions and further work
In this paper we presented three semantic compo-
nents to aid ASD people to understand the texts.
The Image Component finds, disambiguates and as-
signs Images to difficult terms in the text or re-
lated to the text. It works in two modes: auto-
mated or on-demand. In the automated mode a doc-
ument is automatically enriched with images. In
the on-demand mode the user highlights the con-
cepts (s)he considers difficult and the web service
retrieves the corresponding images. Further devel-
opment of this component will involve disambigua-
tion against Wikipedia and retrieval of images from
the corresponding articles. The Idiom Component
finds idioms and other figurative language expres-
sions in the user documents and provides definitions
for them. Further versions of the component will
go beyond simple matching and will identify other
categories of figurative language. The Topic Mod-
els component helps organizing the repository col-
lection by computing topics for the user documents.
Moreover it also offers a summarization of the doc-
ument before the actual reading experience. Finally
the Personalization component adapts the system
output to the user experience. Future versions of the
component will define dynamic user profiles in addi-
tion to the static user profiles in the current version.
Our hope is that the Open Book tool will be useful
for other parts of populations that have difficulties
with syntactic constructions or semantic processing,
too.
Acknowledgments
We want to thank the three anonymous reviewers
whose suggestions helped improve the clarity of this
paper. This work is partially funded by the European
Commission under the Seventh (FP7 - 2007-2013)
Framework Program for Research and Technologi-
cal Development through the FIRST project (FP7-
287607). This publication reflects only the views
of the authors, and the Commission cannot be held
responsible for any use which may be made of the
information contained therein.
References
Tony Attwood. 2007. The complete guide to Asperger
Syndrome. Jessica Kingsley Press.
Simon Baron-Cohen. 2001. Theory of mind and autism:
a review. Int Rev Ment Retard, 23:169?184.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan, Niraj Aswani, Ian
Roberts, Genevieve Gorrell, Adam Funk, An-
gus Roberts, Danica Damljanovic, Thomas Heitz,
Mark A. Greenwood, Horacio Saggion, Johann
Petrak, Yaoyong Li, and Wim Peters. 2011. Text
Processing with GATE (Version 6).
Jia Deng, Wei Dong, R. Socher, Li-Jia Li, Kai Li, and
Li Fei-Fei. 2009. ImageNet: A large-scale hierarchi-
cal image database. In Computer Vision and Pattern
Recognition, 2009. CVPR 2009. IEEE Conference on,
pages 248?255. IEEE, June.
K.H. Douglas, K.M. Ayres, J. Langone, and V.B. Bram-
lett. 2011. The effectiveness of electronic text and
pictorial graphic organizers to improve comprehension
related to functional skills. Journal of Special Educa-
tion Technology, 26(1):43?57.
Brenda Fossett and Pat Mirenda. 2006. Sight word
reading in children with developmental disabilities:
A comparison of paired associate and picture-to-text
matching instruction. Research in Developmental Dis-
abilities, 27(4):411?429.
William Matthew Gillispie. 2008. Semantic Process-
ing in Children with Reading Comprehension Deficits.
Ph.D. thesis, University of Kansas.
18
Temple Grandin. 1996. Thinking In Pictures: and Other
Reports from My Life with Autism. Vintage, October.
Temple Grandin. 2009. How does visual thinking work
in the mind of a person with autism? a personal ac-
count. Philosophical Transactions of the Royal So-
ciety B: Biological Sciences, 364(1522):1437?1442,
May.
Rajesh K. Kana, Timothy A. Keller, Vladimir L.
Cherkassky, Nancy J. Minshew, and Marcel Adam
Just. 2006. Sentence comprehension in autism:
Thinking in pictures with decreased functional con-
nectivity.
B. Lopez and S. R. Leekam. 2003. Do children with
autism fail to process information in context ? Journal
of child psychology and psychiatry., 44(2):285?300,
February.
Geoffrey Nunberg, Ivan Sag, and Thomas Wasow. 1994.
Idioms. Language.
19
