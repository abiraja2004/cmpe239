Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 25?33, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
Semeval-2013 Task 8:
Cross-lingual Textual Entailment for Content Synchronization
Matteo Negri
FBK-irst
Trento, Italy
negri@fbk.eu
Alessandro Marchetti
CELCT
Trento, Italy
amarchetti@celct.it
Yashar Mehdad
UBC
Vancouver, Canada
mehdad@cs.ubc.ca
Luisa Bentivogli
FBK-irst
Trento, Italy
bentivo@fbk.eu
Danilo Giampiccolo
CELCT
Trento, Italy
giampiccolo@celct.it
Abstract
This paper presents the second round of the
task on Cross-lingual Textual Entailment for
Content Synchronization, organized within
SemEval-2013. The task was designed to pro-
mote research on semantic inference over texts
written in different languages, targeting at the
same time a real application scenario. Par-
ticipants were presented with datasets for dif-
ferent language pairs, where multi-directional
entailment relations (?forward?, ?backward?,
?bidirectional?, ?no entailment?) had to be
identified. We report on the training and test
data used for evaluation, the process of their
creation, the participating systems (six teams,
61 runs), the approaches adopted and the re-
sults achieved.
1 Introduction
The cross-lingual textual entailment task (Mehdad et
al., 2010) addresses textual entailment (TE) recog-
nition (Dagan and Glickman, 2004) under the new
dimension of cross-linguality, and within the new
challenging application scenario of content synchro-
nization. Given two texts in different languages, the
cross-lingual textual entailment (CLTE) task con-
sists of deciding if the meaning of one text can be
inferred from the meaning of the other text. Cross-
linguality represents an interesting direction for re-
search on recognizing textual entailment (RTE), es-
pecially due to its possible application in a vari-
ety of tasks. Among others (e.g. question answer-
ing, information retrieval, information extraction,
and document summarization), multilingual content
synchronization represents a challenging application
scenario to evaluate CLTE recognition components
geared to the identification of sentence-level seman-
tic relations.
Given two documents about the same topic writ-
ten in different languages (e.g. Wikipedia pages),
the content synchronization task consists of au-
tomatically detecting and resolving differences in
the information they provide, in order to produce
aligned, mutually enriched versions of the two docu-
ments (Monz et al, 2011; Bronner et al, 2012). To-
wards this objective, a crucial requirement is to iden-
tify the information in one page that is either equiv-
alent or novel (more informative) with respect to the
content of the other. The task can be naturally cast
as an entailment recognition problem, where bidi-
rectional and unidirectional entailment judgements
for two text fragments are respectively mapped into
judgements about semantic equivalence and novelty.
The task can also be seen as a machine translation
evaluation problem, where judgements about se-
mantic equivalence and novelty depend on the pos-
sibility to fully or partially translate a text fragment
into the other.
The recent advances on monolingual TE on the
one hand, and the methodologies used in Statisti-
cal Machine Translation (SMT) on the other, offer
promising solutions to approach the CLTE task. In
line with a number of systems that model the RTE
task as a similarity problem (i.e. handling similar-
ity scores between T and H as features contributing
to the entailment decision), the standard sentence
and word alignment programs used in SMT offer
a strong baseline for CLTE (Mehdad et al, 2011;
25
Figure 1: Example of SP-EN CLTE pairs.
Mehdad et al, 2012). However, although repre-
senting a solid starting point to approach the prob-
lem, similarity-based techniques are just approx-
imations, open to significant improvements com-
ing from semantic inference at the multilingual
level (e.g. cross-lingual entailment rules such as
?perro???animal?). Taken in isolation, similarity-
based techniques clearly fall short of providing an
effective solution to the problem of assigning direc-
tions to the entailment relations (especially in the
complex CLTE scenario, where entailment relations
are multi-directional). Thanks to the contiguity be-
tween CLTE, TE and SMT, the proposed task pro-
vides an interesting scenario to approach the issues
outlined above from different perspectives, and of-
fers large room for mutual improvement.
Building on the success of the first CLTE evalua-
tion organized within SemEval-2012 (Negri et al,
2012a), the remainder of this paper describes the
second evaluation round organized within SemEval-
2013. The following sections provide an overview
of the datasets used, the participating systems, the
approaches adopted, the achieved results, and the
lessons learned.
2 The task
Given a pair of topically related text fragments (T1
and T2) in different languages, the CLTE task con-
sists of automatically annotating it with one of the
following entailment judgements (see Figure 1 for
Spanish/English examples of each judgement):
? bidirectional (T1?T2 & T1?T2): the two
fragments entail each other (semantic equiva-
lence);
? forward (T1?T2 & T16?T2): unidirectional
entailment from T1 to T2;
? backward (T16?T2 & T1?T2): unidirectional
entailment from T2 to T1;
? no entailment (T16?T2 & T16?T2): there is
no entailment between T1 and T2 in either di-
rection;
In this task, both T1 and T2 are assumed to be
true statements. Although contradiction is relevant
from an application-oriented perspective, contradic-
tory pairs are not present in the dataset.
3 Dataset description
The CLTE-2013 dataset is composed of four CLTE
corpora created for the following language combi-
nations: Spanish/English (SP-EN), Italian/English
(IT-EN), French/English (FR-EN), German/English
(DE-EN). Each corpus consists of 1,500 sentence
pairs (1,000 for training and 500 for test), balanced
across the four entailment judgements.
In this year?s evaluation, as training set we used
the CLTE-2012 corpus1 that was created for the
SemEval-2012 evaluation exercise2 (including both
training and test sets). The CLTE-2013 test set was
created from scratch, following the methodology de-
scribed in the next section.
3.1 Data collection and annotation
To collect the entailment pairs for the 2013 test set
we adopted a slightly modified version of the crowd-
sourcing methodology followed to create the CLTE-
2012 corpus (Negri et al, 2011). The main differ-
ence with last year?s procedure is that we did not
take advantage of crowdsourcing for the whole data
collection process, but only for part of it.
As for CLTE-2012, the collection and annotation
process consists of the following steps:
1. First, English sentences were manually ex-
tracted from Wikipedia and Wikinews. The se-
lected sentences represent one of the elements
(T1) of each entailment pair;
1http://www.celct.it/resources.php?id page=CLTE
2http://www.cs.york.ac.uk/semeval-2012/task8/
26
2. Next, each T1 was modified in various ways
in order to obtain a corresponding T2. While
in the CLTE-2012 dataset the whole T2 cre-
ation process was carried out through crowd-
sourcing, for the CLTE-2013 test set we crowd-
sourced only the first phase of T1 modification,
namely the creation of paraphrases. Focusing
on the creation of high quality paraphrases, we
followed the crowdsourcing methodology ex-
perimented in Negri et al (2012b), in which
a paraphrase is obtained through an itera-
tive modification process of an original sen-
tence, by asking workers to introduce meaning-
preserving lexical and syntactic changes. At
each round of the iteration, new workers are
presented with the output of the previous iter-
ation in order to increase divergence from the
original sentence. At the end of the process,
only the more divergent paraphrases according
to the Lesk score (Lesk, 1986) are selected. As
for the second phase of T2 creation process,
this year it was carried out by expert annota-
tors, who followed the same criteria used last
year for the crowdsourced tasks, i.e. i) remove
information from the input (paraphrased) sen-
tence and ii) add information from sentences
surrounding T1 in the source article;
3. Each T2 was then paired to the original T1, and
the resulting pairs were annotated with one of
the four entailment judgements. In order to re-
duce the correlation between the difference in
sentences? length and entailment judgements,
only the pairs where the difference between the
number of words in T1 and T2 (length diff ) was
below a fixed threshold (10 words) were re-
tained.3 The final result is a monolingual En-
glish dataset annotated with multi-directional
entailment judgements, which are well dis-
tributed over length diff values ranging from 0
to 9;
4. In order to create the cross-lingual datasets,
each English T1 was manually translated into
3Such constraint has been applied in order to focus as much
as possible on semantic aspects of the problem, by reduc-
ing the applicability of simple association rules such as IF
length(T1)>length(T2) THEN T1?T2.
four different languages (i.e. Spanish, German,
Italian and French) by expert translators;
5. By pairing the translated T1 with the cor-
responding T2 in English, four cross-lingual
datasets were obtained.
To ensure the good quality of the datasets, all the
collected pairs were cross-annotated and filtered to
retain only those pairs with full agreement in the
entailment judgement between two expert annota-
tors. The final result is a multilingual parallel en-
tailment corpus, where T1s are in 5 different lan-
guages (i.e. English, Spanish, German, Italian, and
French), and T2s are in English. It is worth men-
tioning that the monolingual English corpus, a by-
product of our data collection methodology, will be
publicly released as a further contribution to the re-
search community.
3.2 Dataset statistics
As described in section 3.1, the methodology fol-
lowed to create the training and test sets was the
same except for the crowdsourced tasks. This al-
lowed us to obtain two datasets with the same bal-
ance across the entailment judgements, and to keep
under control the distribution of the pairs for differ-
ent length diff values in each language combination.
Training Set. The training set is composed of
1,000 CLTE pairs for each language combina-
tion, balanced across the four entailment judge-
ments (bidirectional, forward, backward, and
no entailment). As shown in Table 1, our data col-
lection procedure led to a dataset where the major-
ity of the pairs falls in the +5 -5 length diff range
for each language pair (67.2% on average across the
four language pairs). This characteristic is partic-
ularly relevant as our assumption is that such data
distribution makes entailment judgements based on
mere surface features such as sentence length inef-
fective, thus encouraging the development of alter-
native, deeper processing strategies.
Test Set. The test set is composed of 500 entail-
ment pairs for each language combination, balanced
across the four entailment judgements. As shown
in Table 2, also in this dataset the majority of the
collected entailment pairs is uniformly distributed
27
(a) SP-EN (b) IT-EN
(c) FR-EN (d) DE-EN
Figure 2: Pair distribution in the 2013 test set: total number of pairs (y-axis) for different length diff values (x-axis).
SP-EN IT-EN FR-EN DE-EN
Forward 104 132 121 179
Backward 202 182 191 123
No entailment 163 173 169 174
Bidirectional 175 199 193 209
ALL 644 686 674 685
% (out of 1,000) 64.4 68.6 67.4 68.5
Table 1: Training set pair distribution within the -5/+5
length diff range.
in the [-5,+5] length diff range (68.1% on average
across the four language pairs).
However, comparing training and test set for
each language pair, it can be seen that while the
Spanish-English and Italian-English datasets are ho-
mogeneous with respect to the length diff feature,
the French-English and German-English datasets
present noticeable differences between training and
test set. These figures show that, despite the consid-
erable effort spent to produce comparable training
SP-EN IT-EN FR-EN DE-EN
backward 82 89 82 102
bidirectional 89 92 90 106
forward 69 78 76 98
no entailment 71 80 59 100
ALL 311 339 307 406
% (out of 500) 62.2 67.8 61.4 81.2
Table 2: Test set pair distribution within the -5/+5
length diff range.
and test sets, the ideal objective of a full homogene-
ity between the datasets for these two languages was
difficult to reach.
Complete details about the distribution of the
pairs in terms of length diff for the four cross-
lingual corpora in the test set are provided in Figure
2. Vertical bars represent, for each length diff value,
the proportion of pairs belonging to the four entail-
ment classes.
28
4 Evaluation metrics and baselines
Evaluation results have been automatically com-
puted by comparing the entailment judgements re-
turned by each system with those manually assigned
by human annotators in the gold standard. The met-
rics used for systems? ranking is accuracy over the
whole test set, i.e. the number of correct judge-
ments out of the total number of judgements in the
test set. Additionally, we calculated precision, re-
call, and F1 measures for each of the four entail-
ment judgement categories taken separately. These
scores aim at giving participants the possibility to
gain clearer insights into their system?s behaviour on
the entailment phenomena relevant to the task.
To allow comparison with the CLTE-2012 re-
sults, the same three baselines were calculated on the
CLTE-2013 test set for each language combination.
The first one is the 0.25 accuracy score obtained by
assigning each test pair in the balanced dataset to
one of the four classes. The other two baselines con-
sider the length difference between T1 and T2:
? Composition of binary judgements (Bi-
nary). To calculate this baseline an SVM
classifier is trained to take binary en-
tailment decisions (?YES?, ?NO?). The
classifier uses length(T1)/length(T2) and
length(T2)/length(T1) as features respectively
to check for entailment from T1 to T2 and vice-
versa. For each test pair, the unidirectional
judgements returned by the two classifiers are
composed into a single multi-directional judge-
ment (?YES-YES?=?bidirectional?, ?YES-
NO?=?forward?, ?NO-YES?=?backward?,
?NO-NO?=?no entailment?);
? Multi-class classification (Multi-class). A
single SVM classifier is trained with the same
features to directly assign to each pair one of
the four entailment judgements.
Both the baselines have been calculated with the
LIBSVM package (Chang and Lin, 2011), using de-
fault parameters. Baseline results are reported in Ta-
ble 3.
Although the four CLTE datasets are derived from
the same monolingual EN-EN corpus, baseline re-
sults present slight differences due to the effect of
translation into different languages. With respect to
last year?s evaluation, we can observe a slight drop
in the binary classification baseline results. This
might be due to the fact that the length distribution
of examples is slightly different this year. How-
ever, there are no significant differences between the
multi-class baseline results of this year in compar-
ison with the previous round results. This might
suggest that multi-class classification is a more ro-
bust approach for recognizing multi-directional en-
tailment relations. Moreover, both baselines failed
in capturing the ?no-entailment? examples in all
datasets (F1no?entailment = 0).
SP-EN IT-EN FR-EN DE-EN
1-class 0.25 0.25 0.25 0.25
Binary 0.35 0.39 0.37 0.39
Multi-class 0.43 0.44 0.42 0.42
Table 3: Baseline accuracy results.
5 Submitted runs and results
Like in the 2012 round of the CLTE task, partici-
pants were allowed to submit up to five runs for each
language combination. A total of twelve teams reg-
istered for participation and downloaded the train-
ing set. Out of them, six4 submitted valid runs.
Five teams produced submissions for all the four
language combinations, while one team participated
only in the DE-EN task. In total, 61 runs have been
submitted and evaluated (16 for DE-EN, and 15 for
each of the other language pairs).
Accuracy results are reported in Table 4. As can
be seen from the table, the performance of the best
systems is quite similar across the four language
combinations, with the best submissions achieving
results in the 43.4-45.8% accuracy interval. Simi-
larly, also average and median results are close to
each other, with a small drop on DE-EN. This drop
might be explained by the difference between the
training and test set with respect to the length diff
feature. Moreover, the performance of DE-EN auto-
matic translation might affect approaches based on
?pivoting?, (i.e. addressing CLTE by automatically
translating T1 in the same language of T2, as de-
scribed in Section 6).
4Including the task organizers.
29
System name SP-EN IT-EN FR-EN DE-EN
altn run1* 0.428 0.432 0.420 0.388
BUAP run1 0.364 0.358 0.368 0.322
BUAP run2 0.374 0.358 0.364 0.318
BUAP run3 0.380 0.358 0.362 0.316
BUAP run4 0.364 0.388 0.392 0.350
BUAP run5 0.386 0.360 0.372 0.318
celi run1 0.340 0.324 0.334 0.342
celi run2 0.342 0.324 0.340 0.342
ECNUCS run1 0.428 0.426 0.438 0.422
ECNUCS run2 0.404 0.420 0.450 0.436
ECNUCS run3 0.408 0.426 0.458 0.432
ECNUCS run4 0.422 0.416 0.436 0.452
ECNUCS run5 0.392 0.402 0.442 0.426
SoftCard run1 0.434 0.454 0.416 0.414
SoftCard run2 0.432 0.448 0.426 0.402
umelb run1 ? ? ? 0.324
Highest 0.434 0.454 0.458 0.452
Average 0.404 0.404 0.401 0.378
Median 0.428 0.426 0.420 0.369
Lowest 0.342 0.324 0.340 0.324
Table 4: CLTE-2013 accuracy results (61 runs) over the
4 language combinations. Highest, average, median and
lowest scores are calculated considering only the best run
for each team (*task organizers? system).
Compared to the results achieved last year, shown
in Table 5, a sensible decrease in the highest scores
can be observed. While in CLTE-2012 the top sys-
tems achieved an accuracy well above 0.5 (with a
maximum of 0.632 in SP-EN), the results for this
year are far below such level (the peak is now at
45,8% for FR-EN). A slight decrease with respect
to 2012 can also be noted for average performances.
However, it?s worth remarking the general increase
of the lowest and median scores, which are less sen-
sitive to isolate outstanding results achieved by sin-
gle teams. This indicates that a progress in CLTE
research has been made building on the lessons
learned after the first round of the initiative.
To better understand the behaviour of each sys-
tem, Table 6 provides separate precision, recall, and
F1 scores for each entailment judgement, calculated
over the best runs of each participating team. In
contrast to CLTE-2012, where the ?bidirectional?
and ?no entailment? categories consistently proved
to be more problematic than ?forward? and ?back-
ward? judgements, this year?s results are more ho-
mogeneous across the different classes. Neverthe-
less, on average, the classification of ?bidirectional?
pairs is still worse for three language pairs (SP-EN,
IT-EN and FR-EN), and results for ?no entailment?
are lower for two of them (SP-EN and DE-EN).
SP-EN IT-EN FR-EN DE-EN
Highest 0.632 0.566 0.570 0.558
Average 0.440 0.411 0.408 0.408
Median 0.407 0.350 0.365 0.363
Lowest 0.274 0.326 0.296 0.296
Table 5: CLTE-2012 accuracy results. Highest, average,
median and lowest scores are calculated considering only
the best run for each team.
As regards the comparison with the baselines,
this year?s results confirm that the length diff -based
baselines are hard to beat. More specifically, most
of the systems are slightly above the binary classi-
fication baseline (with the exception of the DE-EN
dataset where only two systems out of six outper-
formed it), whereas for all the language combina-
tions the multi-class baseline was beaten only by the
best participating system.
This shows that, despite the effort in keeping the
distribution of the entailment classes uniform across
different length diff values, eliminating the correla-
tion between sentence length and correct entailment
decisions is difficult. As a consequence, although
disregarding semantic aspects of the problem, fea-
tures considering length information are quite ef-
fective in terms of overall accuracy. Such features,
however, perform rather poorly when dealing with
challenging cases (e.g. ?no-entailment?), which are
better handled by participating systems.
6 Approaches
A rough classification of the approaches adopted by
participants can be made along two orthogonal di-
mensions, namely:
? Pivoting vs. Cross-lingual. Pivoting meth-
ods rely on the automatic translation of one of
the two texts (either single words or the en-
tire sentence) into the language of the other
(typically English) in order perform monolin-
gual TE recognition. Cross-lingual methods
assign entailment judgements without prelim-
inary translation.
? Composition of binary judgements vs.
Multi-class classification. Compositional ap-
proaches map unidirectional (?YES?/?NO?)
30
SP-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full spa-eng 0.509 0.464 0.485 0.440 0.264 0.330 0.464 0.416 0.439 0.357 0.568 0.438
BUAP spa-eng run5 0.446 0.360 0.398 0.521 0.296 0.378 0.385 0.456 0.418 0.300 0.432 0.354
celi spa-eng run2 0.396 0.352 0.373 0.431 0.400 0.415 0.325 0.328 0.327 0.245 0.288 0.265
ECNUCS spa-eng run1 0.458 0.432 0.444 0.533 0.320 0.400 0.406 0.416 0.411 0.380 0.544 0.447
SoftCard spa-eng run1 0.462 0.344 0.394 0.619 0.480 0.541 0.418 0.472 0.444 0.325 0.440 0.374
AVG. 0.454 0.390 0.419 0.509 0.352 0.413 0.400 0.418 0.408 0.321 0.454 0.376
IT-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full ita-eng 0.448 0.376 0.409 0.417 0.344 0.377 0.512 0.496 0.504 0.374 0.512 0.432
BUAP ita-eng run4 0.418 0.328 0.368 0.462 0.384 0.419 0.379 0.440 0.407 0.327 0.400 0.360
celi ita-eng run1 0.288 0.256 0.271 0.395 0.408 0.402 0.336 0.304 0.319 0.279 0.328 0.301
ECNUCS ita-eng run1 0.422 0.456 0.438 0.592 0.336 0.429 0.440 0.440 0.440 0.349 0.472 0.401
SoftCard ita-eng run1 0.514 0.456 0.483 0.612 0.480 0.538 0.392 0.464 0.425 0.364 0.416 0.388
AVG. 0.418 0.374 0.394 0.496 0.390 0.433 0.412 0.429 0.419 0.339 0.426 0.376
FR-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full fra-eng 0.405 0.392 0.398 0.420 0.296 0.347 0.500 0.440 0.468 0.381 0.552 0.451
BUAP fra-eng run4 0.407 0.472 0.437 0.431 0.376 0.402 0.379 0.376 0.378 0.352 0.344 0.348
celi fra-eng run2 0.394 0.344 0.368 0.364 0.376 0.370 0.352 0.352 0.352 0.263 0.288 0.275
ECNUCS fra-eng run3 0.422 0.432 0.427 0.667 0.352 0.461 0.514 0.432 0.470 0.383 0.616 0.472
SoftCard fra-eng run2 0.477 0.416 0.444 0.556 0.400 0.465 0.412 0.432 0.422 0.335 0.456 0.386
AVG. 0.421 0.411 0.415 0.488 0.360 0.409 0.431 0.406 0.418 0.343 0.451 0.386
DE-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full deu-eng 0.432 0.408 0.420 0.378 0.272 0.316 0.445 0.392 0.417 0.330 0.480 0.391
BUAP deu-eng run4 0.364 0.344 0.354 0.389 0.280 0.326 0.352 0.352 0.352 0.317 0.424 0.363
celi deu-eng run1 0.346 0.352 0.349 0.414 0.424 0.419 0.351 0.264 0.301 0.272 0.328 0.297
ECNUCS deu-eng run4 0.429 0.432 0.430 0.611 0.352 0.447 0.415 0.392 0.403 0.429 0.632 0.511
SoftCard deu-eng run1 0.511 0.368 0.428 0.527 0.384 0.444 0.417 0.400 0.408 0.317 0.504 0.389
umelb deu-eng run1 0.323 0.320 0.321 0.240 0.184 0.208 0.362 0.376 0.369 0.347 0.416 0.378
AVG. 0.401 0.371 0.384 0.426 0.316 0.360 0.390 0.363 0.375 0.335 0.464 0.389
Table 6: Precision, recall and F1 scores, calculated for each team?s best run for all the language combinations.
entailment decisions taken separately into sin-
gle judgements (similar to the Binary baseline
in Section 4). Methods based on multi-class
classification directly assign one of the four en-
tailment judgements to each test pair (similar to
our Multi-class baseline).
In contrast with CLTE-2012, where the combina-
tion of pivoting and compositional methods was the
option adopted by the majority of the approaches,
this year?s solutions do not show a clear trend. Con-
cerning the former dimension, participating systems
are equally distributed in cross-lingual and pivoting
methods relying on external automatic translation
tools. Regarding the latter dimension, in addition
to compositional and multi-class strategies, also al-
ternative solutions that leverage more sophisticated
meta-classification strategies have been proposed.
Besides the recourse to MT tools (e.g. Google
Translate), other tools and resources used by partic-
ipants include: WordNet, word alignment tools (e.g.
Giza++), part-of-speech taggers (e.g. Stanford POS
Tagger), stemmers (e.g. Snowball), machine learn-
ing libraries (e.g. Weka, SVMlight), parallel corpora
(e.g. Europarl), and stopword lists. More in detail:
ALTN [cross-lingual, compositional] (Turchi
and Negri, 2013) adopts a supervised learning
method based on features that consider word align-
ments between the two sentences obtained with
GIZA++ (Och et al, 2003). Binary entailment
judgements are taken separately, and combined into
final CLTE decisions.
BUAP [pivoting, multi-class and meta-
classifier] (Vilarin?o et al, 2013) adopts a pivoting
method based on translating T1 into the language of
31
T2 and vice versa (using Google Translate5). Sim-
ilarity measures (e.g. Jaccard index) and features
based on n-gram overlap, computed at the level of
words and part of speech categories, are used (either
alone or in combination) by different classification
strategies including: multi-class, a meta-classifier
(i.e. combining the output of 2/3/4-class classifiers),
and majority voting.
CELI [cross-lingual, meta-classifier]
(Kouylekov, 2013) uses dictionaries for word
matching, and a multilingual corpus extracted
from Wikipedia for term weighting. A variety of
distance measures implemented in the RTE system
EDITS (Kouylekov and Negri, 2010; Negri et
al., 2009) are used to extract features to train a
meta-classifier. Such classifier combines binary
decisions (?YES?/?NO?) taken separately for each
of the four CLTE judgements.
ECNUCS [pivoting, multi-class] (Jiang and
Man, 2013) uses Google Translate to obtain the En-
glish translation of each T1. After a pre-processing
step aimed at maximizing the commonalities be-
tween the two sentences (e.g. abbreviation replace-
ment), a number of features is extracted to train
a multi-class SVM classifier. Such features con-
sider information about sentence length, text sim-
ilarity/difference measures, and syntactic informa-
tion.
SoftCard [pivoting, multi-class] (Jimenez et al,
2013) after automatic translation with Google Trans-
late, uses SVMs to learn entailment decisions based
on information about the cardinality of: T1, T2, their
intersection and their union. Cardinalities are com-
puted in different ways, considering tokens in T1 and
T2, their IDF, and their similarity.
Umelb [cross-lingual, pivoting, compositional]
(Graham et al, 2013) adopts both pivoting and
cross-lingual approaches. For the latter, GIZA++
was used to compute word alignments between the
input sentences. Word alignment features are used
to train binary SVM classifiers whose decisions are
eventually composed into CLTE judgements.
7 Conclusion
Following the success of the first round of the Cross-
lingual Textual Entailment for Content Synchroniza-
5http://translate.google.com/
tion task organized within SemEval-2012, a second
evaluation task has been organized within SemEval-
2013. Despite the decrease in the number of partic-
ipants (six teams - four less than in the first round
- submitted a total of 61 runs) the new experience
is still positive. In terms of data, a new test set
has been released, extending the old one with 500
new CLTE pairs. The resulting 1,500 cross-lingual
pairs, aligned over four language combinations (in
addition to the monolingual English version), and
annotated with multiple entailment relations, repre-
sent a significant contribution to the research com-
munity and a solid starting point for further develop-
ments.6 In terms of results, in spite of a significant
decrease of the top scores, the increase of both me-
dian and lower results demonstrates some encour-
aging progress in CLTE research. Such progress is
also demonstrated by the variety of the approaches
proposed. While in the first round most of the
teams adopted more intuitive and ?simpler? solu-
tions based on pivoting (i.e. translation of T1 and
T2 in the same language) and compositional entail-
ment decision strategies, this year new ideas and
more complex solutions have emerged. Pivoting and
cross-lingual approaches are equally distributed, and
new classification methods have been proposed. Our
hope is that the large room for improvement, the in-
crease of available data, and the potential of CLTE
as a way to address complex NLP tasks and applica-
tions will motivate further research on the proposed
problem.
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-248531). The
authors would also like to acknowledge Pamela
Forner and Giovanni Moretti from CELCT, and the
volunteer translators that contributed to the creation
of the dataset: Giusi Calo, Victoria D??az, Bianca
Jeremias, Anne Kauffman, Laura Lo?pez Ortiz, Julie
Mailfait, Laura Mora?n Iglesias, Andreas Schwab.
6Together with the datasets derived from translation of the
RTE data (Negri and Mehdad, 2010), this is the only material
currently available to train and evaluate CLTE systems.
32
References
Amit Bronner, Matteo Negri, Yashar Mehdad, Angela
Fahrni, and Christof Monz. 2012. Cosyne: Synchro-
nizing multilingual wiki content. In Proceedings of
WikiSym 2012.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology (TIST),
2(3):27.
Ido Dagan and Oren Glickman. 2004. Probabilistic Tex-
tual Entailment: Generic Applied Modeling of Lan-
guage Variability. In Proceedings of the PASCAL
Workshop of Learning Methods for Text Understand-
ing and Mining.
Yvette Graham, Bahar Salehi, and Tim Baldwin. 2013.
Unimelb: Cross-lingual Textual Entailment with Word
Alignment and String Similarity Features. In Proceed-
ings of the 7th International Workshop on Semantic
Evaluation (SemEval 2013).
Zhao Jiang and Lan Man. 2013. ECNUCS: Recognizing
Cross-lingual Textual Entailment Using Multiple Fea-
ture Types. . In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013).
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013. Soft Cardinality-CLTE: Learning to Iden-
tify Directional Cross-Lingual Entailmens from Car-
dinalities and SMT. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013).
Milen Kouylekov and Matteo Negri. 2010. An open-
source package for recognizing textual entailment. In
Proceedings of the ACL 2010 System Demonstrations.
Milen Kouylekov. 2013. Celi: EDITS and Generic Text
Pair Classification. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013).
Michael Lesk. 1986. Automated Sense Disambiguation
Using Machine-readable Dictionaries: How to Tell a
Pine Cone from an Ice Cream Cone. In Proceedings
of the 5th annual international conference on Systems
documentation (SIGDOC86).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment. In
Proceedings of the 11th Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL HLT 2010).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using Bilingual Parallel Corpora for Cross-
Lingual Textual Entailment. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL HLT 2011).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012. Detecting Semantic Equivalence and Informa-
tion Disparity in Cross-lingual Documents. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL 2012).
Christof Monz, Vivi Nastase, Matteo Negri, Angela
Fahrni, Yashar Mehdad, and Michael Strube. 2011.
Cosyne: a framework for multilingual content syn-
chronization of wikis. In Proceedings of WikiSym
2011.
Matteo Negri and Yashar Mehdad. 2010. Creating a Bi-
lingual Entailment Corpus through Translations with
Mechanical Turk: $100 for a 10-day Rush. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Cre-
ating Speech and Language Data with Amazons? Me-
chanical Turk.
Matteo Negri, Milen Kouylekov, Bernardo Magnini,
Yashar Mehdad, and Elena Cabrio. 2009. Towards ex-
tensible textual entailment engines: the edits package.
In AI* IA 2009: Emergent Perspectives in Artificial In-
telligence, pages 314?323. Springer.
Matto Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and Conquer: Crowdsourcing the Creation of
Cross-Lingual Textual Entailment Corpora. Proceed-
ings of the 2011 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2011).
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012a.
Semeval-2012 Task 8: Cross-lingual Textual Entail-
ment for Content Synchronization. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval 2012).
Matteo Negri, Yashar Mehdad, Alessandro Marchetti,
Danilo Giampiccolo, and Luisa Bentivogli. 2012b.
Chinese Whispers: Cooperative Paraphrase Acqui-
sition. In Proceedings of the Eight International
Conference on Language Resources and Evaluation
(LREC12), volume 2, pages 2659?2665.
F. Och, H. Ney, F. Josef, and O. H. Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics.
Marco Turchi and Matteo Negri. 2013. ALTN: Word
Alignment Features for Cross-Lingual Textual Entail-
ment. In Proceedings of the 7th International Work-
shop on Semantic Evaluation (SemEval 2013).
Darnes Vilarin?o, David Pinto, Saul Leo?n, Yuridiana
Alema?n, and Helena Go?mez-Adorno. 2013. BUAP:
N -gram based Feature Evaluation for the Cross-
Lingual Textual Entailment Task. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion (SemEval 2013).
33
