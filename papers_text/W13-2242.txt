Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 343?351,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Referential Translation Machines for Quality Estimation
Ergun Bic?ici
Centre for Next Generation Localisation,
Dublin City University, Dublin, Ireland.
ergun.bicici@computing.dcu.ie
Abstract
We introduce referential translation ma-
chines (RTM) for quality estimation of
translation outputs. RTMs are a computa-
tional model for identifying the translation
acts between any two data sets with re-
spect to a reference corpus selected in the
same domain, which can be used for esti-
mating the quality of translation outputs,
judging the semantic similarity between
text, and evaluating the quality of student
answers. RTMs achieve top performance
in automatic, accurate, and language inde-
pendent prediction of sentence-level and
word-level statistical machine translation
(SMT) quality. RTMs remove the need to
access any SMT system specific informa-
tion or prior knowledge of the training data
or models used when generating the trans-
lations. We develop novel techniques for
solving all subtasks in the WMT13 qual-
ity estimation (QE) task (QET 2013) based
on individual RTM models. Our results
achieve improvements over last year?s QE
task results (QET 2012), as well as our
previous results, provide new features and
techniques for QE, and rank 1st or 2nd in
all of the subtasks.
1 Introduction
Quality Estimation Task (QET) (Callison-Burch et
al., 2012; Callison-Burch et al, 2013) aims to de-
velop quality indicators for translations and pre-
dictors without access to the references. Predic-
tion of translation quality is important because the
expected translation performance can help in esti-
mating the effort required for correcting the trans-
lations during post-editing by human translators.
Bicici et al (2013) develop the Machine Trans-
lation Performance Predictor (MTPP), a state-of-
the-art, language independent, and SMT system
extrinsic machine translation performance predic-
tor, which achieves better performance than the
competitive QET baseline system (Callison-Burch
et al, 2012) by just looking at the test source sen-
tences and becomes the 2nd overall after also look-
ing at the translation outputs in QET 2012.
In this work, we introduce referential translation
machines (RTM) for quality estimation of transla-
tion outputs, which is a computational model for
identifying the acts of translation for translating
between any given two data sets with respect to
a reference corpus selected in the same domain.
RTMs reduce our dependence on any task depen-
dent resource. In particular, we do not use the
baseline software or the SMT resources provided
with the QET 2013 challenge. We believe having
access to glass-box features such as the phrase ta-
ble or the n-best lists is not realistic especially for
use-cases where translations may be provided by
different MT vendors (not necessarily from SMT
products) or by human translators. Even the prior
knowledge of the training corpora used for build-
ing the SMT models or any other model used when
generating the translations diverges from the goal
of independent and unbiased prediction of trans-
lation quality. Our results show that we do not
need to use any SMT system dependent informa-
tion to achieve the top performance when predict-
ing translation output quality.
2 Referential Translation Machine
(RTM)
Referential translation machines (RTMs) provide
a computational model for quality and seman-
tic similarity judgments using retrieval of rele-
vant training data (Bic?ici and Yuret, 2011a; Bic?ici,
2011) as interpretants for reaching shared seman-
tics (Bic?ici, 2008). RTMs achieve very good per-
formance in judging the semantic similarity of
sentences (Bic?ici and van Genabith, 2013a) and
we can also use RTMs to automatically assess the
343
correctness of student answers to obtain better re-
sults (Bic?ici and van Genabith, 2013b) than the
state-of-the-art (Dzikovska et al, 2012).
RTM is a computational model for identifying
the acts of translation for translating between any
given two data sets with respect to a reference cor-
pus selected in the same domain. RTM can be
used for predicting the quality of translation out-
puts. An RTM model is based on the selection of
common training data relevant and close to both
the training set and the test set of the task where
the selected relevant set of instances are called the
interpretants. Interpretants allow shared semantics
to be possible by behaving as a reference point for
similarity judgments and providing the context. In
semiotics, an interpretant I interprets the signs
used to refer to the real objects (Bic?ici, 2008).
RTMs provide a model for computational seman-
tics using interpretants as a reference according
to which semantic judgments with translation acts
are made. Each RTM model is a data translation
model between the instances in the training set
and the test set. We use the FDA (Feature De-
cay Algorithms) instance selection model for se-
lecting the interpretants (Bic?ici and Yuret, 2011a)
from a given corpus, which can be monolingual
when modeling paraphrasing acts, in which case
the MTPP model (Section 2.1) is built using the
interpretants themselves as both the source and the
target side of the parallel corpus. RTMs map the
training and test data to a space where translation
acts can be identified. We view that acts of transla-
tion are ubiquitously used during communication:
Every act of communication is an act of
translation (Bliss, 2012).
Translation need not be between different lan-
guages and paraphrasing or communication also
contain acts of translation. When creating sen-
tences, we use our background knowledge and
translate information content according to the cur-
rent context. Given a training set train, a test
set test, and some monolingual corpus C, prefer-
ably in the same domain as the training and test
sets, the RTM steps are:
1. T = train ? test.
2. select(T, C)? I
3. MTPP(I,train)? Ftrain
4. MTPP(I,test)? Ftest
5. learn(M,Ftrain)?M
6. predict(M,Ftest)? q?
Step 2 selects the interpretants, I, relevant to the
instances in the combined training and test data.
Steps 3 and 4 use I to map train and test
to a new space where similarities between transla-
tion acts can be derived more easily. Step 5 trains
a learning model M over the training features,
Ftrain, and Step 6 obtains the predictions. RTM
relies on the representativeness of I as a medium
for building translation models for translating be-
tween train and test.
Our encouraging results in the QET challenge
provides a greater understanding of the acts of
translation we ubiquitously use when communi-
cating and how they can be used to predict the
performance of translation, judging the semantic
similarity between text, and evaluating the qual-
ity of student answers. RTM and MTPP models
are not data or language specific and their mod-
eling power and good performance are applicable
across different domains and tasks. RTM expands
the applicability of MTPP by making it feasible
when making monolingual quality and similarity
judgments and it enhances the computational scal-
ability by building models over smaller but more
relevant training data as interpretants.
2.1 The Machine Translation Performance
Predictor (MTPP)
In machine translation (MT), pairs of source and
target sentences are used for training statistical
MT (SMT) models. SMT system performance is
affected by the amount of training data used as
well as the closeness of the test set to the training
set. MTPP (Bic?ici et al, 2013) is a state-of-the-
art and top performing machine translation per-
formance predictor, which uses machine learning
models over features measuring how well the test
set matches the training set to predict the quality
of a translation without using a reference trans-
lation. MTPP measures the coverage of individ-
ual test sentence features and syntactic structures
found in the training set and derives feature func-
tions measuring the closeness of test sentences to
the available training data, the difficulty of trans-
lating the sentence, and the presence of acts of
translation for data transformation.
2.2 MTPP Features for Translation Acts
MTPP uses n-gram features defined over text or
common cover link (CCL) (Seginer, 2007) struc-
tures as the basic units of information over which
similarity calculations are made. Unsupervised
344
parsing with CCL extracts links from base words
to head words, resulting in structures represent-
ing the grammatical information instantiated in the
training and test data. Feature functions use statis-
tics involving the training set and the test sen-
tences to determine their closeness. Since they are
language independent, MTPP allows quality esti-
mation to be performed extrinsically.
We extend MTPP (Bic?ici et al, 2013) in its
learning module, the features included, and their
representations. Categories for the 308 features
(S for source, T for target) used are listed below
where the number of features are given in {#} and
the detailed descriptions for some of the features
are presented in (Bic?ici et al, 2013).
? Coverage {110}: Measures the degree to
which the test features are found in the train-
ing set for both S ({56}) and T ({54}).
? Synthetic Translation Performance {6}: Cal-
culates translation scores achievable accord-
ing to the n-gram coverage.
? Length {7}: Calculates the number of words
and characters for S and T and their average
token lengths and their ratios.
? Feature Vector Similarity {16}: Calculates
similarities between vector representations.
? Perplexity {90}: Measures the fluency of
the sentences according to language models
(LM). We use both forward ({30}) and back-
ward ({15}) LM features for S and T.
? Entropy {9}: Calculates the distributional
similarity of test sentences to the training set
over top N retrieved sentences.
? Retrieval Closeness {24}: Measures the de-
gree to which sentences close to the test set
are found in the selected training set, I, us-
ing FDA (Bic?ici and Yuret, 2011a).
? Diversity {6}: Measures the diversity of co-
occurring features in the training set.
? IBM1 Translation Probability {16}: Cal-
culates the translation probability of test
sentences using the selected training set,
I, (Brown et al, 1993).
? IBM2 Alignment Features {11}: Calculates
the sum of the entropy of the distribution of
alignment probabilities for S (?s?S ?p log p
for p = p(t|s) where s and t are tokens) and
T, their average for S and T, the number of en-
tries with p ? 0.2 and p ? 0.01, the entropy
of the word alignment between S and T and
its average, and word alignment log probabil-
ity and its value in terms of bits per word.
? Minimum Bayes Retrieval Risk {4}: Calcu-
lates the translation probability for the trans-
lation having the minimum Bayes risk among
the retrieved training instances.
? Sentence Translation Performance {3}: Cal-
culates translation scores obtained accord-
ing to q(T,R) using BLEU (Papineni et
al., 2002), NIST (Doddington, 2002), or
F1 (Bic?ici and Yuret, 2011b) for q.
? Character n-grams {4}: Calculates cosine
between character n-grams (for n=2,3,4,5)
obtained for S and T (Ba?r et al, 2012).
? LIX {2}: Calculates the LIX readability
score (Wikipedia, 2013; Bjo?rnsson, 1968) for
S and T. 1
For retrieval closeness, we use FDA instead
of dice for sentence selection. We also improve
FDA?s instance selection score by scaling with the
length of the sentence (Bic?ici and Yuret, 2011a).
IBM2 alignments and their probabilities are ob-
tained by first obtaining IBM1 alignments and
probabilities, which become the starting point for
the IBM2 model. Both models are trained for 25
to 75 iterations or until convergence.
3 Quality Estimation Task Results
We participate in all of the four challenges of the
quality estimation task (QET) (Callison-Burch
et al, 2013), which include English to Spanish
(en-es) and German to English translation direc-
tions. There are two main categories of chal-
lenges: sentence-level prediction (Task 1.*) and
word-level prediction (Task 2). Task 1.1 is about
predicting post-editing effort (PEE), Task 1.2 is
about ranking translations from different systems,
Task 1.3 is about predicting post-editing time
(PET), and Task 2 is about binary or multi-class
classification of word-level quality.
For each task, we develop RTM mod-
els using the parallel corpora and the LM
corpora distributed by the translation task
(WMT13) (Callison-Burch et al, 2013) and the
LM corpora provided by LDC for English and
Spanish 2. The parallel corpora contain 4.3M
sentences for de-en with 106M words for de and
111M words for en and 15M sentences for en-es
with 406M words for en and 455M words for
1LIX=AB + C 100A , where A is the number of words, C iswords longer than 6 characters, B is words that start or end
with any of ?.?, ?:?, ?!?, ??? similar to (Hagstro?m, 2012).
2English Gigaword 5th, Spanish Gigaword 3rd edition.
345
es. We do not use any resources provided by
QET including data, software, or baseline features
since they are SMT system dependent or language
specific. Instance selection for the training set and
the language model (LM) corpus is handled by a
parallel implementation of FDA (Bic?ici, 2013).
We tokenize and true-case all of the corpora. The
true-caser is trained on all of the training corpus
using Moses (Koehn et al, 2007). We prepare the
corpora by following this procedure: tokenize ?
train the true-caser ? true-case. Table 1 lists the
statistics of the data used in the training and test
sets for the tasks.
Task 1.1 1.2 (de-en) 1.2 (en-es) 1.3 & 2
Train
sents 2254 32730 22338 803
words 63K (en) 762K (de) 528K (en) 18K (en)67K (es) 786K (en) 559K (es) 20K (es)
Test sents 500 1810 1315 284
Table 1: Data statistics for different tasks. The
number of words is listed after tokenization.
Since we do not know the best training set
size that will maximize the performance, we rely
on previous SMT experiments (Bic?ici and Yuret,
2011a; Bic?ici and Yuret, 2011b) and quality es-
timation challenges (Bic?ici and van Genabith,
2013a; Bic?ici and van Genabith, 2013b) to select
the proper training set size. For each training and
test sentence provided in each subtask, we choose
between 65 and 600 sentences from the parallel
training corpora to be added to the training set,
which creates roughly 400K sentences for train-
ing. We add the selected training set to the 8 mil-
lion sentences selected for each LM corpus. The
statistics of the training data selected by the par-
allel FDA and used as interpretants in the RTM
models is given in Table 2.
Task 1.1 1.2 (de-en) 1.2 (en-es) 1.3 2
sents 406K 318K 299K 398K 397K
words 6.3M (en) 4.8M (de) 4.3M (en) 6.6M (en) 6.6M (en)6.9M (es) 4.9M (en) 4.6M (es) 7.2M (es) 7.2M (es)
Table 2: Statistics of the training data used as in-
terpretants in the RTM models in thousands (K) of
sentences or millions (M) of words.
3.1 Evaluation
In this section, we describe the metrics we use to
evaluate the learning performance. Let yi repre-
sent the actual target value for instance i, y? the
mean of the actual target values, y?i the value es-
timated by the learning model, and ??y the mean of
the estimated target values, then we use the fol-
lowing metrics to evaluate the learning models:
? Mean Absolute Error (MAE): |?| =
?n
i=1 |y?i?yi|
n
? Relative Absolute Error (RAE) : ??|| =
?n
i=1 |y?i?yi|?n
i=1 |y??yi|
? Root Mean Squared Error: RMSE =??n
i=1(y?i?yi)2
n
? DeltaAvg: ??(V, S) =
1
|S|/2?1
?|S|/2
n=2
(?n?1
k=1
?
s??ki=1 qi
V (s)
|?ki=1 qi|
)
? Correlation: r =
?n
i=1(y?i???y)(yi?y?)??n
i=1(y?i???y)2
??n
i=1(yi?y?)2
DeltaAvg (Callison-Burch et al, 2012) calculates
the average quality difference between the scores
for the top n ? 1 quartiles and the overall quality
for the test set. Relative absolute error measures
the error relative to the error when predicting the
actual mean. We use the coefficient of determina-
tion, R2 = 1 ??ni=1(y?i ? yi)2/
?n
i=1(y? ? yi)2,
during optimization where the models are
regression based and higher R2 values are better.
3.2 Task 1: Sentence-level Prediction of
Quality
In this subsection, we develop techniques for the
prediction of quality at the sentence-level. We first
discuss the learning models we use and how we
optimize them and then provide the results for the
individual subtasks and the settings used.
3.2.1 Learning Models and Optimization
The learning models we use for predicting the
translation quality include the ridge regression
(RR) and support vector regression (SVR) with
RBF (radial basis functions) kernel (Smola and
Scho?lkopf, 2004). Both of these models learn
a regression function using the features to esti-
mate a numerical target value such as the HTER
score, the F1 score (Bic?ici and Yuret, 2011b), or
the PET score. We also use these learning models
after a feature subset selection with recursive fea-
ture elimination (RFE) (Guyon et al, 2002) or a
dimensionality reduction and mapping step using
partial least squares (PLS) (Specia et al, 2009),
both of which are described in (Bic?ici et al, 2013).
The learning parameters that govern the behavior
of RR and SVR are the regularization ? for RR and
the C, ?, and ? parameters for SVR. We optimize
346
the learning parameters, the number of features
to select, and the number of dimensions used for
PLS. More detailed description of the optimiza-
tion process is given in (Bic?ici et al, 2013). In
our submissions, we only used the results we ob-
tained from SVR and SVR after PLS (SVRPLS)
since they perform the best during training.
Optimization can be a challenge for SVR due to
the large number of parameter settings to search.
In this work, we decrease the search space by se-
lecting ? close to the theoretically optimal values.
We select ? close to the standard deviation of the
noise in the training set since the optimal value
for ? is shown to have linear dependence to the
noise level for different noise models (Smola et al,
1998). We use RMSE of RR on the training set as
an estimate for the noise level (? of noise) and the
following formulas to obtain the ? with ? = 3:
? = ??
?
lnn
n (1)
and the C (Cherkassky and Ma, 2004; Chal-
imourda et al, 2004):
C = max(|y? + 3?y|, |y? ? 3?y|) (2)
Since the C obtained could be low (Chalimourda
et al, 2004), we use a range of C values in ad-
dition to the obtained C value including C values
with a couple of ?y values larger.
Table 3 lists the RMSE of the RR model on the
training set and the corresponding ? and C val-
ues for different subtasks. We also present the op-
timized parameter values for SVR and SVRPLS.
Table 3 shows that, empirically, Equation 1 and
Equation 2 gives results close to the best parame-
ters found after optimization.
Task 1.1 1.2 (de-en) 1.2 (en-es) 1.3
RMSE RR .1397 .1169 .1569 68.06
? .0245 .0062 .01 18.64
C .8398 .8713 1.02 371.28
C? (SVR) .8398 .5 .5 100
? (SVR) .0005 .001 .0001 .0005
C? (SVRPLS) 1.5 .8713 1.02 100
? (SVRPLS) .0001 .0001 .0001 .001
# dim (SVRPLS) 60 60 60 60
Table 3: Optimal parameters predicted by Equa-
tion 1 and Equation 2 and the optimized parame-
ter values, C? and ? for SVR and SVRPLS and the
number of dimensions (# dim) for SVRPLS.
3.2.2 Task 1.1: Scoring and Ranking for
Post-Editing Effort
Task 1.1 involves the prediction of the case insen-
sitive translation edit rate (TER) scores obtained
by TERp (Snover et al, 2009) and their ranking.
In contrast, we derive features over sentences that
are true-cased. We obtain the rankings by sorting
according to the predicted TER scores.
Task 1.1 R2 r RMSE MAE RAE
RR 0.3510 0.5965 0.1393 0.1086 0.7888
RR PLS 0.4232 0.6509 0.1313 0.1023 0.7430
SVR 0.4394 0.6647 0.1295 0.0967 0.7023
SVR PLS 0.4305 0.6569 0.1305 0.1003 0.7284
Table 4: Task1.1 results on the training set.
Table 4 presents the learning performance on
the training set using the optimized parameters.
We are able to significantly improve the results
when compared with the QET 2012 (Callison-
Burch et al, 2012) and our previous results (Bic?ici
et al, 2013) especially in terms of MAE and RAE.
The results on the test set are given in Table 5.
Rank lists the overall ranking in the task. RTMs
with SVR PLS learning is able to achieve the top
rank in this task.
Ranking DeltaAvg r Rank
CNGL SVRPLS 11.09 0.55 1
CNGL SVR 9.88 0.51 4
Scoring MAE RMSE Rank
CNGL SVRPLS 13.26 16.82 3
CNGL SVR 13.85 17.28 8
Table 5: Task1.1 results on the test set.
3.2.3 Task 1.2: Ranking Translations from
Different Systems
Task 1.2 involves the prediction of the ranking
among up to 5 translation outputs produced by dif-
ferent MT systems. Evaluation is done against
the human rankings using the Kendall?s ? corre-
lation (Callison-Burch et al, 2013): ? = (c ?
d)/n(n?1)2 = c?dc+d where a pair is concordant, c, ifthe ordering agrees, discordant, d, if their ordering
disagrees, and neither concordant nor discordant if
their rankings are equal.
We use sentence-level F1 scores (Bic?ici and
Yuret, 2011b) as the target to predict. We use
F1 because it can be easily interpreted and it cor-
relates well with human judgments (more than
TER) (Bic?ici and Yuret, 2011b; Callison-Burch et
al., 2011). We also found that the ? of the rank-
ings obtained according to the F1 score over the
347
training set (0.2040) is better than BLEU (Pap-
ineni et al, 2002) (0.1780) and NIST (Dodding-
ton, 2002) (0.1907) for de-en. Table 6 presents the
learning performance on the training set using the
optimized parameters. Learning F1 becomes an
easier task than learning TER as observed from the
results but we have significantly more training in-
stances. We use the SVR model for predicting the
F1 scores on the training set and the test set. MAE
is a more important performance metric here since
we want to be as precise as possible when predict-
ing the actual performance.
Task 1.2 R2 r RMSE MAE RAE
de-en RR 0.6320 0.7953 0.1169 0.0733 0.5535SVR 0.7528 0.8692 0.0958 0.0463 0.3494
en-es RR 0.5101 0.7146 0.1569 0.1047 0.6323SVR 0.4819 0.7018 0.1613 0.0973 0.5873
Table 6: Task1.2 results on the training set.
Our next goal is to learn a threshold for judg-
ing if two translations are equal over the predicted
F1 scores. This threshold is used to determine
whether we need to alter the ranking. We try
to mimic the human decision process when de-
termining whether two translations are equivalent.
On some occasions where the sentences are close
enough, humans give them equal ranking. This
is also related to the granularity of the differences
visible with a 1 to 5 ranking schema.
We compared different threshold formulations
and used the following condition in our submis-
sions to decide whether the ranking of item i in a
set S of translations, i ? S, should be different:
?
j 6=i
F1(j)? F1(i)
|j ? i| /|S| > t, (3)
where t is the optimized threshold minimizing the
following loss for n training instances:
n?
i=1
?(f(t, qi), ri) (4)
where f(t, qi) is a function returning rankings
based on the threshold t and the quality scores for
instance i, qi and ?(rj , ri) calculates the ? score
based on the rankings rj and ri.
For both de-en and en-es subtasks, we found the
thresholds obtained to be very similar or the same.
The optimized values are given in Table 7. On the
test set, we used the same threshold, t = 0.00275
for both de-en and en-es, which is a little higher
than the optimal t to prevent overfitting.
Task 1.2 ? t # same # all
de-en .2339 .00013 236 25644.2287 .00275 494
en-es .2801 .00073 136 17752.2764 .00275 233
Table 7: Task1.2 optimized thresholds and the
corresponding comparisons that were found to be
equal (# same) over all comparisons (# all).
We believe that human judgments of linguis-
tic equality and the corresponding thresholds we
learned in this work can be useful for developing
better automatic evaluation metrics and can im-
prove the correlation of the scores obtained with
human judgments (as we did here). The results on
the test set are given in Table 8. We are also able
to achieve the top ranking in this task.
Ties penalized model ? Rank
de-en CNGL SVRPLS F1 0.17 3CNGL SVR F1 0.17 4
en-es CNGL SVRPLS F1 0.15 1CNGL SVR F1 0.13 2
Ties ignored model ? Rank
de-en CNGL SVRPLS F1 0.17 3CNGL SVR F1 0.17 4
en-es CNGL SVRPLS F1 0.16 2CNGL SVR F1 0.13 3
Table 8: Task1.2 results on the test set.
3.2.4 Task 1.3: Predicting Post-Editing Time
Task 1.3 involves the prediction of the post-editing
time (PET) for a translator to post-edit the MT out-
put. Table 9 presents the learning performance on
the training set using the optimized parameters.
Task 1.3 R2 r RMSE MAE RAE
RR 0.4463 0.6702 68.0628 39.5250 0.6694
RR PLS 0.5917 0.7716 58.4464 35.8759 0.6076
SVR 0.4062 0.6753 70.4853 36.5132 0.6184
SVR PLS 0.5316 0.7604 62.6031 33.5490 0.5682
Table 9: Task1.3 results on the training set.
The results on the test set are given in Table 10.
We are able to become the 2nd best system accord-
ing to MAE in this task.
3.3 Task 2: Word-level Prediction of Quality
In this subsection, we develop a learning model,
global linear models with dynamic learning rate
(GLMd), for the prediction of quality at the word-
level where the word-level quality is a binary (K:
keep, C: change) or multi-class classification prob-
lem (K: keep, S: substitute, D: delete). We first
discuss the GLMd learning model, then we present
348
Task 1.3 MAE Rank
CNGL SVR 49.2121 3
CNGL SVRPLS 49.6161 4
RMSE Rank
CNGL SVRPLS 86.6175 4
CNGL SVR 90.3650 7
Table 10: Task1.3 results on the test set.
the word-level features we use, and then present
our results on the test set.
3.3.1 Global Linear Models with Dynamic
Learning (GLMd)
Collins (2002) develops global learning models
(GLM), which rely on Viterbi decoding, percep-
tron learning, and flexible feature definitions. We
extend the GLM framework by parallel percep-
tron training (McDonald et al, 2010) and dynamic
learning with adaptive weight updates in the per-
ceptron learning algorithm:
w = w + ? (?(xi, yi)? ?(xi, y?)) , (5)
where ? returns a global representation for in-
stance i and the weights are updated by ? =
exp(log10(3?1/0)) with ?1 and 0 representing
the error of the previous and first iteration respec-
tively. ? decays the amount of the change during
weight updates at later stages and prevents large
fluctuations with updates. We used both the GLM
model and the GMLd models in our submissions.
3.3.2 Word-level Features
We introduce a number of novel features for the
prediction of word-level translation quality. In
broad categories, these word-level features are:
? CCL: Uses CCL links.
? Word context: Surrounding words.
? Word alignments: Alignments, their probabili-
ties, source and target word contexts.
? Length: Word lengths, n-grams over them.
? Location: Location of the words.
? Prefix and Suffix: Word prefixes, suffixes.
? Form: Capital, contains digit or punctuation.
We found that CCL links are the most discrimi-
native feature among these. In total, we used 511K
features for binary and 637K for multi-class clas-
sification. The learning curve is given in Figure 1.
The results on the test set are given in Table 11.
P, R, and A stand for precision, recall, and accu-
racy respectively. We are able to become the 2nd
according to A in this task.
Figure 1: Learning curve with the parallel GLM
and GLMd models.
Binary A P R F1 Rank (A)
CNGL dGLM .7146 .7392 .9261 .8222 2
CNGL GLM .7010 .7554 .8581 .8035 5
Multi-class A Rank
CNGL dGLM .7162 3
CNGL GLM .7116 4
Table 11: Task 2 results on the test set.
4 Contributions
Referential translation machines achieve top per-
formance in automatic, accurate, and language in-
dependent prediction of sentence-level and word-
level statistical machine translation (SMT) qual-
ity. RTMs remove the need to access any SMT
system specific information or prior knowledge of
the training data or models used when generating
the translations. We develop novel techniques for
solving all subtasks in the quality estimation (QE)
task (QET 2013) based on individual RTM mod-
els. Our results achieve improvements over last
year?s QE task results (QET 2012), as well as our
previous results, provide new features and tech-
niques for QE, and rank 1st or 2nd in all of the
subtasks.
Acknowledgments
This work is supported in part by SFI
(07/CE/I1142) as part of the Centre for Next
Generation Localisation (www.cngl.ie) at Dublin
City University and in part by the European Com-
mission through the QTLaunchPad FP7 project
(No: 296347). We also thank the SFI/HEA Irish
Centre for High-End Computing (ICHEC) for the
provision of computational facilities and support.
349
References
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In *SEM 2012: The First Joint
Conference on Lexical and Computational Seman-
tics ? Volume 1: Proceedings of the main conference
and the shared task, and Volume 2: Proceedings of
the Sixth International Workshop on Semantic Eval-
uation (SemEval 2012), pages 435?440, Montre?al,
Canada, 7-8 June. Association for Computational
Linguistics.
Ergun Bic?ici and Josef van Genabith. 2013a. CNGL-
CORE: Referential translation machines for measur-
ing semantic similarity. In *SEM 2013: The Second
Joint Conference on Lexical and Computational Se-
mantics, Atlanta, Georgia, USA, 13-14 June. Asso-
ciation for Computational Linguistics.
Ergun Bic?ici and Josef van Genabith. 2013b. CNGL:
Grading student answers by acts of translation. In
*SEM 2013: The Second Joint Conference on Lex-
ical and Computational Semantics and Proceedings
of the Seventh International Workshop on Semantic
Evaluation (SemEval 2013), Atlanta, Georgia, USA,
14-15 June. Association for Computational Linguis-
tics.
Ergun Bic?ici and Deniz Yuret. 2011a. Instance se-
lection for machine translation using feature decay
algorithms. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 272?283,
Edinburgh, Scotland, July. Association for Compu-
tational Linguistics.
Ergun Bic?ici and Deniz Yuret. 2011b. RegMT system
for machine translation, system combination, and
evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 323?329,
Edinburgh, Scotland, July. Association for Compu-
tational Linguistics.
Ergun Bic?ici, Declan Groves, and Josef van Genabith.
2013. Predicting sentence translation quality using
extrinsic and language independent features. Ma-
chine Translation.
Ergun Bic?ici. 2011. The Regression Model of Machine
Translation. Ph.D. thesis, Koc? University. Supervi-
sor: Deniz Yuret.
Ergun Bic?ici. 2013. Feature decay algorithms for fast
deployment of accurate statistical machine transla-
tion systems. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Ergun Bic?ici. 2008. Consensus ontologies in socially
interacting multiagent systems. Journal of Multia-
gent and Grid Systems.
Carl Hugo Bjo?rnsson. 1968. La?sbarhet. Liber.
Chris Bliss. 2012. Comedy is transla-
tion, February. http://www.ted.com/talks/
chris bliss comedy is translation.html.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263?311, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omer F. Zaidan. 2011. Findings of the 2011
Workshop on Statistical Machine Translation. In
Proceedings of the Sixth Workshop on Statistical
Machine Translation, Edinburgh, England, July. As-
sociation for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2013.
Findings of the 2013 workshop on statistical ma-
chine translation. In Proceedings of the Eighth
Workshop on Statistical Machine Translation, pages
10?51. Association for Computational Linguistics,
August.
Athanassia Chalimourda, Bernhard Scho?lkopf, and
Alex J. Smola. 2004. Experimentally optimal
? in support vector regression for different noise
models and parameter settings. Neural Networks,
17(1):127?141, January.
Vladimir Cherkassky and Yunqian Ma. 2004. Practical
selection of svm parameters and noise estimation for
svm regression. Neural Netw., 17(1):113?126, Jan-
uary.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing - Volume 10, EMNLP
?02, pages 1?8, Stroudsburg, PA, USA. Association
for Computational Linguistics.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research, pages 138?145, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Myroslava O. Dzikovska, Rodney D. Nielsen, and
Chris Brew. 2012. Towards effective tutorial feed-
back for explanation questions: A dataset and base-
lines. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
350
Computational Linguistics: Human Language Tech-
nologies, pages 200?210, Montre?al, Canada, June.
Association for Computational Linguistics.
Isabelle Guyon, Jason Weston, Stephen Barnhill, and
Vladimir Vapnik. 2002. Gene selection for cancer
classification using support vector machines. Ma-
chine Learning, 46(1-3):389?422.
Kenth Hagstro?m. 2012. Swedish readabil-
ity calculator. https://github.com/keha76/Swedish-
Readability-Calculator.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Annual Meeting of the Assoc. for Compu-
tational Linguistics, pages 177?180, Prague, Czech
Republic, June.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 456?464, Los Angeles, California,
June. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Yoav Seginer. 2007. Learning Syntactic Structure.
Ph.D. thesis, Universiteit van Amsterdam.
Alex J. Smola and Bernhard Scho?lkopf. 2004. A tu-
torial on support vector regression. Statistics and
Computing, 14(3):199?222, August.
A. J. Smola, N. Murata, B. Scho?lkopf, and K.-R.
Mu?ller. 1998. Asymptotically optimal choice of
?-loss for support vector machines. In L. Niklas-
son, M. Boden, and T. Ziemke, editors, Proceedings
of the International Conference on Artificial Neural
Networks, Perspectives in Neural Computing, pages
105?110, Berlin. Springer.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, adequacy,
or hter?: exploring different human judgments
with a tunable mt metric. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, StatMT ?09, pages 259?268, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Lucia Specia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In Proceedings of the 13th Annual Con-
ference of the European Association for Machine
Translation (EAMT), pages 28?35, Barcelona, May.
EAMT.
Wikipedia. 2013. Lix.
http://en.wikipedia.org/wiki/LIX.
351
