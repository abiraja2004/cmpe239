Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 163?171,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
RankPref: Ranking Sentences Describing Relations
between Biomedical Entities with an Application
Catalina O Tudor K Vijay-Shanker
Department of Computer and Information Sciences
University of Delaware, Newark, DE, USA
tudor@cis.udel.edu vijay@cis.udel.edu
Abstract
This paper presents a machine learning ap-
proach that selects and, more generally, ranks
sentences containing clear relations between
genes and terms that are related to them. This
is treated as a binary classification task, where
preference judgments are used to learn how to
choose a sentence from a pair of sentences.
Features to capture how the relationship is de-
scribed textually, as well as how central the
relationship is in the sentence, are used in the
learning process. Simplification of complex
sentences into simple structures is also applied
for the extraction of the features. We show that
such simplification improves the results by up
to 13%. We conducted three different evalu-
ations and we found that the system signifi-
cantly outperforms the baselines.
1 Introduction
Life scientists, doctors and clinicians often search
for information relating biological concepts. For ex-
ample, a doctor might be interested to know the im-
pact of a drug on some disease. One source of infor-
mation is the knowledge bases and ontologies that
are manually curated with facts from scientific arti-
cles. However, the curation process is slow and can-
not keep up with ongoing publications. Moreover,
not all associations between biological concepts can
be found in these databases.
Another source of information is the scientific
literature itself. However, searching for biological
facts and how they might be related is often cumber-
some. The work presented in this paper tries to au-
tomate the process of finding sentences that clearly
describe relationships between biological concepts.
We rank all sentences mentioning two concepts and
pick the top one to show to the user. In this paper, we
focused on certain specific types of concepts (i.e.,
genes1 and terms believed to be related to them), al-
though our approach can be generalized.
Systems to facilitate knowledge exploration of
genes are being built for the biomedical domain.
One of them, eGIFT (Tudor et al., 2010), tries to
identify iTerms (informative terms) for a gene based
on frequency of co-occurrence (see Figure 1 for top
15 terms selected for gene Groucho). iTerms are
unigrams, bigrams, and exact matches of biomedi-
cal terms gathered from various controlled vocabu-
laries. Thus, iTerms can be of any type (e.g., pro-
cesses, domains, drugs, other genes, etc.), the types
being determined by what is being described about
the gene in the literature. The iTerms for a gene
are ranked based on a score that compares their fre-
quencies of occurrence in publications mentioning
the gene in question with their frequencies in a back-
ground set of articles about a wide variety of genes.
Previous evaluation of eGIFT by life scientists
suggested that there is almost always some kind of
relationship between a gene and its iTerms. These
relationships can be many and varied from one gene-
term pair to another. Sometimes a user might make
an erroneous assumption about a gene-term asso-
ciation if sentences supporting the association are
not immediately inspected. For example, upon see-
ing ?co-repressor? in connection to gene Groucho,
eGIFT users might correctly assume that Groucho is
1Throughout the paper, the word ?gene? will be used for
both the gene and its products.
163
Figure 1: Top iTerms for gene Groucho, and sentences picked by RankPref for various iTerms.
a co-repressor (i.e., a protein that binds to transcrip-
tion factors). However, upon seeing ?wrpw motif?,
a user might assume that this is a motif contained
within gene Groucho, as this is typically the asso-
ciation that we make between genes and informa-
tion annotated for them in knowledge bases. But
this would be a wrong assumption, since in actuality
the wrpw motif is contained within other genes that
interact with Groucho, fact which is evident from
reading sentences containing the gene and the mo-
tif. To get a quick overall understanding of a gene?s
functionalities, users of eGIFT could be presented
with terms extracted for the gene, as well as sen-
tences clearly describing how they are related.
Our method selects sentences using a model that
is trained on preference judgments provided by biol-
ogists. Example sentences chosen by our method are
shown in Figure 1. While we evaluate our approach
on sentences from eGIFT, this work could have
equally applied on other similar systems (Smal-
heiser et al., 2008; Gladki et al., 2008; Kim et al.,
2008; Kaczanowski et al., 2009). These systems
also identify ?important terms? from a set of doc-
uments retrieved for a given search (either a gene
name or other biomedical concept).
The main contributions of this work are: (1) a
method for ranking sentences by employing ma-
chine learning; (2) the use of preference judgments;
(3) features to capture whether two terms are clearly
related and in focus in a sentence; (4) another appli-
cation of sentence simplification, showing a signifi-
cant gain in performance when utilized.
We continue with a description of our approach,
which includes the use of preference judgments to
learn the models, how the features are extracted, and
how the sentence simplifier is used for this task. The
evaluation of the trained model and the system?s re-
sults are presented in the following section. Re-
lated work, conclusions, and future directions are
provided at the end of the manuscript.
2 Methods
Rather than pre-judging what is important for this
task and manually determining a weighting schema
to automatically score sentences for a gene-term
pair, we approached this task using machine learn-
ing. We asked a group of annotators to rank sen-
tences relating genes and iTerms, and we used their
annotations, together with features described in Sec-
tion 2.3, to learn how to rank sentences.
164
2.1 Preference Judgments
For the annotation task, we presented biologists with
sentences containing a gene-term pair and asked
them to specify which sentence they prefer. One
way to do this is by employing the pointwise ap-
proach, which requires absolute judgments (i.e. the
annotator scores each sentence in a list or ranks the
sentences based on their relevance to the given task).
A second approach is the pairwise approach, which
requires the iteration of preference judgments (i.e.,
the annotator is presented with two sentences at a
time, and is asked to chose one as more relevant to
the task than the other).
In order to simplify the annotator?s task, as well
as construct a more reliable training set, we used the
pairwise approach. Our decision was influenced by
Carterette et al. (2008), who showed that preference
judgments are faster and easier to make than abso-
lute judgments. Thus, we can obtain many annotated
instances in a relatively short amount of time. More-
over, since there are only two possible outcomes in
choosing one sentence, we need at most three judges
for a majority vote. This will also ensure consistency
in the annotations. We discuss the model trained on
preference judgments in Section 2.2.
2.2 Learned Models: PrefSVM and RankPref
We used the preference judgments to learn a model,
PrefSVM, that picks one sentence from a pair of sen-
tences. This model was built using SVMLight with
a linear kernel. The examples used in the learning
process correspond to pairs of sentences. For each
pair, we constructed a vector of feature values, by
subtracting the feature values corresponding to the
first sentence from the feature values corresponding
to the second sentence. We assigned a positive value
to a pair vector if the first sentence was preferred and
a negative value if the second one was preferred.
We can also use PrefSVM to design a system that
can rank all the sentences containing a gene and
an iTerm, by performing comparisons between sen-
tences in the list. We call RankPref the system that
picks one sentence from a group of sentences, and
which also ranks the entire set of sentences. This
method recursively applies PrefSVM in the following
manner: Two sentences are randomly picked from
a given list of sentences. PrefSVM chooses one sen-
tence and discards the other. A third sentence is then
randomly picked from the list, and PrefSVM makes
its choice by comparing it to the sentence kept in the
previous step. This process of picking, comparing
and discarding sentences is continued until there is
only one sentence left. We keep track of comparison
results and apply transitivity, in order to speed up the
process of ranking all the sentences.
2.3 Features
Each sentence is first chunked into base phrases. We
used Genia Tagger (Tsuruoka et al., 2005), which
provides part-of-speech tags for every word in the
sentence. We trained a chunker (i.e., shallow parser
that identifies base NPs) using the Genia corpus.
We considered typical features that are used in
machine learning approaches, such as distance be-
tween gene and iTerm, length of sentence, etc.
Moreover, we included additional groups of features
that we felt might be important for this task: one
group to capture how the relationship is described
textually, another group to capture how central the
relationship is in terms of what is being described in
the sentence, and the last to capture whether the re-
lation is stated as a conjecture or a fact. The weights
for these features will be determined automatically
during the learning process and they will be depen-
dent on whether or not the features were effective,
given the annotation set.
The first type of features is to capture how the
relationship is described textually. As an example,
consider the sentence ?Bmp2 stimulates osteoblas-
tic differentiation?2, where the gene and the iTerm
are in subject and object (direct object or otherwise)
positions, and the verb is a common biological verb.
Thus, we constructed a set of lexico-syntactic pat-
terns to capture the different kinds of argument re-
lations served by the two concepts. We grouped 25
lexico-syntactic patterns into 8 groups, correspond-
ing to different relational constructions that can ex-
ist between a gene and an iTerm. Example patterns
are shown in Table 1 for each group, and the sym-
bols used in these patterns are explained in Table 2.
When a sentence matches a pattern group, the corre-
sponding value is set to 1 for that feature.
2In our examples, the gene will be marked in italics and the
iTerm will be marked in bold.
165
Group Example Pattern
G1 G VG+ I
G2 G/I via/by/through I/G
G3 G VG+ (NP/PP)* by/in VBG I
G4 G/I by/in VBG I/G
G5 G/I VB I/G
G6 G/I of I/G
G7 G/I other preposition I/G
G8 including/such as/etc. G/I and I/G
Table 1: Examples of lexico-syntactic patterns
For example, the following sentence, in which
the gene is Lmo2 and the iTerm is ?erythropoiesis?,
matches the pattern in G1: [G VG+ I].
While Tal1 has been shown to induce ery-
throid differentiation , Lmo2 appears to sup-
press fetal erythropoiesis.
where ?Lmo2? matches G, ?appears to suppress?
matches VG+, and ?fetal erythropoiesis? matches I.
Notice how the verb plays an important role in
the patterns of groups G1, G3, G4, and G5. We also
have a verb type feature which differentiates groups
of verbs having the gene and the iTerm as arguments
(e.g., ?activates?, ?is involved in?, ?plays a role?,
etc. are treated as different types).
The second type of features captures how cen-
tral the relationship is in terms of what is being de-
scribed in the sentence. The subject feature records
whether the gene and iTerm appear in the subject
position, as this will tell us if they are in focus in
the sentence. While we do not parse the sentence,
we take a simplified sentence (see Section 2.4) and
see if the gene/term appear in a noun phrase pre-
ceding the first tensed verb. Another feature, the
gene-iTerm position, measures how close the gene
and the term are to each other and to the beginning
of the sentence, as this makes it easier for a reader
to grasp the relation between them. For this, we add
the number of words occurring to the left of the seg-
ment spanning the gene and iTerm, and half of the
number of words occurring between them. Finally,
we included a headedness feature. The idea here is
that if the gene/term are not the head of the noun
group, but rather embedded inside, then this poten-
tially makes the relation less straightforward. These
Symb Definition
NP a base noun phrase
PP a preposition followed by a base noun phrase
VG+ a series of one or more verb groups
VBG a verb group in which the head is a gerund verb
VBN a verb group in which the head is a participle verb
VB a verb group in which the head is a base verb
G, I base noun phrases, with 0 or more prepositional
phrases, containing the gene/iTerm
Table 2: Symbols used in the pattern notation
groups are denoted by G and I in the patterns shown
in Table 1.
The third type of features captures information
about the sentence itself. The sentence complexity
feature is measured in terms of the number of verbs,
conjunctions, commas, and parentheticals that oc-
cur in the sentence. We use a conjecture feature for
detecting whether the sentence involves a hypothe-
sis. We have a simple rule for this feature, by see-
ing if words such as ?may?, ?could?, ?probably?,
?potentially?, etc., appear in proximity of the gene
and iTerm. Additionally, we have a negation feature
to detect whether the relationship is mentioned in a
negative way. We look for words such as ?not?, ?nei-
ther?, etc., within proximity of the gene and iTerm.
Although the features and lexico-syntactic pat-
terns were determined by analyzing a development
set of sentences containing genes and their iTerms,
we believe that these features and patterns can be
used to rank sentences involving other biomedical
entities, not just genes.
2.4 Sentence Simplification
Notice that the lexico-syntactic patterns are written
as sequences of chunks and lexical tags. If a sen-
tence matches a pattern, then the sentence expresses
a relation between the gene and the iTerm. However,
sometimes it is not possible to match a pattern if the
sentence is complex.
For example, consider sentence A in Table 3, for
gene Cd63. Let us assume that the iTerm is ?prota-
somes?. Clearly, there is a relationship between the
gene and the iTerm, namely that Cd63 was found in
pc-3 cell-derived protasomes. However, none of the
lexico-syntactic patterns is able to capture this rela-
tion, because of all the extra information between
166
A Cd63, an integral membrane protein found
in multivesicular lysosomes and secretory
granules, was also found in pc-3 cell-
derived protasomes.
S1 Cd63 was found in pc-3 cell-derived pro-
tasomes.
S2 Cd63 is an integral membrane protein.
CS1 Cd63 is found in multivesicular lyso-
somes.
CS2 Cd63 is found in secretory granules.
Table 3: Simplified sentences for gene Cd63. Example
iTerms: ?protasomes? and ?secretory granules?.
the gene and the term. While we may have multi-
ple patterns in each group, we cannot necessarily ac-
count for each lexical variation at this level of gran-
ularity.
We are using a sentence simplifier, built in-house,
to ensure a match where applicable. The simpli-
fier identifies appositions, relative clauses, and con-
junctions/lists of different types, using regular ex-
pressions to match chunked tags. In the sentence
of Table 3, the simplifier recognizes the apposition
?an integral membrane protein?, the reduced relative
clause ?found in multivesicular bodies/lysosomes
and secretory granules? and the noun conjunction
?multivesicular bodies/lysosome and secretory gran-
ules?. It then produces several simplified sentences
containing the gene. S1 and S2, shown in Table 3,
are simplified sentences obtained from the simpli-
fier. CS1 and CS2 are additional simplified sen-
tences, which required the combination of multiple
simplifications: the appositive, the relative clause,
and the noun conjunction.
Notice how each of the simplified sentences
shown in Table 3 is now matching a pattern group.
If we are interested in the relationship between Cd63
and ?protasomes?, we can look at S1. Likewise, if
we are interested in the relationship between Cd63
and ?secretory granules?, we can look at CS2.
We have a matching feature that tells whether the
pattern was matched in the original sentence, a sim-
plified sentence, or a combined sentence, and this
feature is taken into account in the learning process.
3 Results and Discussion
We evaluated both PrefSVM and RankPref. Each re-
quired a different set of annotated data. For the
evaluation of PrefSVM, we used the preference judg-
ments and leave-one-out cross validation. And for
the evaluation of RankPref, we asked the annota-
tors to order a group of sentences mentioning gene-
iTerm pairs. Six life science researchers, with grad-
uate degrees, annotated both sets.
3.1 Evaluation of PrefSVM
First, we evaluated the performance of PrefSVM us-
ing leave-one-out cross validation.
3.1.1 Annotation of Preference Judgements
We started by selecting a group of pairs of sen-
tences. We randomly picked gene-iTerm combi-
nations, and for each combination, we randomly
picked two sentences containing both the gene and
the term. To alleviate bias, the order of the sentences
was chosen randomly before displaying them to the
annotators. In our guidelines, we asked the annota-
tors to choose sentences that clearly state the rela-
tionship between the gene and the iTerm. Because
the focus here is on the relationship between the two
terms, we also asked them to refrain from choos-
ing sentences that describe additional information or
other aspects. It is conceivable that, for other appli-
cations, extra information might be an important de-
termining factor, but for our task we wanted to focus
on the relationship only.
For each pair of sentences, we wanted to have
three opinions so that we can have a majority vote.
To alleviate the burden on the annotators, we started
by giving each pair of sentences to two annotators,
and asked for an extra opinion only when they did
not agree. Each biologist was given an initial set
of 75 pairs of sentences to annotate, and shared the
same amount of annotations (15) with each of the
other biologists. 225 unique pairs of sentences were
thus annotated, but six were discarded after the an-
notators informed us that they did not contain the
gene in question.
In 34 out of 219 pairs of sentences, the two biol-
ogists disagreed on their annotations. These cases
included pairs of similar sentences, or pairs of sen-
tences that did not describe any relationship between
167
System Performance Correct
Baseline 1 65.75% 144
Baseline 2 71.69% 157
PrefSVM without Simp 72.14% 158
PrefSVM with Simp 83.10% 182
Table 4: Results for PrefSVM
the gene and the iTerm. An example of sentences for
which the annotators could not agree is:
1. The tle proteins are the mammalian ho-
mologues of gro, a member of the drosophila
notch signaling pathway.
2. In drosophila, gro is one of the neurogenic
genes that participates in the notch signalling
pathway .
For these 34 pairs, we randomly selected another
annotator and considered the majority vote.
3.1.2 Baselines
We chose two baselines against which to com-
pare PrefSVM. The first baseline always chooses
the shortest sentence. For the second baseline, we
looked at the proximity of the gene/term to the be-
ginning of the sentence, as well as the proximity of
the two to each other, and chose the sentence that
had the lowest accumulated proximity. The reason
for this second baseline is because the proximity of
the gene/term to the beginning of the sentence could
mean that the sentence focuses on the gene/term and
their relation. Furthermore, the proximity of the
gene to the iTerm could mean a clearer relation be-
tween them.
3.1.3 Results
We evaluated PrefSVM by performing leave-one-
out cross validation on the set of 219 pairs of sen-
tences. Each pair of sentences was tested by using
the model trained on the remaining 218 pairs. The
results are shown in Table 4.
The first baseline performed at 65.75%, correctly
choosing 144 of 219 sentences. The second base-
line performed slightly better, at 71.69%. PrefSVM
outperformed both baselines, especially when the
sentence simplifier was used, as this facilitated the
match of the lexico-syntactic patterns used as fea-
tures. PrefSVM performed at 83.10%, which is
17.35% better than the first baseline, and 11.41%
better than the second baseline.
3.2 Evaluation of RankPref
The previous evaluation showed how PrefSVM per-
forms at picking a sentence from a pair of sentences.
But ultimately, for the intended eGIFT application,
the system needs to choose one sentence from many.
We evaluated RankPref for this task.
3.2.1 Annotating Data for Sentence Selection
For this evaluation, we needed to create a different
set of annotated data that reflects the selection of one
sentence from a group of sentences.
Since a gene and an iTerm can appear in many
sentences, it is too onerous a task for a human anno-
tator to choose one out of tens or hundreds of sen-
tences. For this reason, we limited the set of sen-
tences mentioning a gene and an iTerm to only 10.
We randomly picked 100 gene-term pairs and for the
pairs that contained more than ten sentences, we ran-
domly chose ten of them. On average, there were 9.4
sentences per set.
We asked the same annotators as in the previous
evaluation to participate in this annotation task. Be-
cause the task is very time consuming, and because
it is hard to decide how to combine the results from
multiple annotators, we assigned each set of sen-
tences to only one annotator. We showed the sen-
tences in a random order so that biasing them would
not be an issue.
We initially asked the annotators to order the sen-
tences in the set. However, this task proved to be im-
possible, since many sentences were alike. Instead,
we asked the annotators to assign them one of three
categories:
(Cat.1) Any sentence in this category could be
considered the ?best? among the choices provided;
(Cat.2) These sentences are good, but there are
other sentences that are slightly better;
(Cat.3) These sentences are not good or at least
there are other sentences in this set that are much
better.
Classifying the sentences into these categories
was less cumbersome, fact which was confirmed by
our evaluators after a trial annotation.
Out of the total of 936 sentences, 322 (34.4%)
were placed in the first category, 332 (35.5%) were
168
System Cat.1 Cat.2 Cat.3
Baseline 1 58 30 12
Baseline 2 61 24 15
RankPref without Simp 67 21 12
RankPref with Simp 80 17 3
Table 5: Results for RankPref
placed in the second category, and 282 (30.1%) were
placed in the third category. On average, it took
about 15 minutes for an annotator to group a set?s
sentences into these three categories. So each anno-
tator volunteered approximately 5 hours of annota-
tion time.
3.2.2 Results
Table 5 shows how the top sentences picked for
the 100 gene-term pairs by the four systems matched
with the annotations. 80 of 100 sentences that
RankPref picked were placed in Cat.1 by the anno-
tators, 17 were placed in Cat.2, and 3 sentences
were placed in Cat.3. These results compare favor-
ably with results obtained for the two baselines and
RankPref without the use of the simplifier.
Furthermore, instead of just focussing on the top
choice sentence, we also considered the ranking of
the entire set of sentences. We looked at how the
ranked lists agree with the categories assigned by
the annotators. We used the normalized discounted
cumulative gain (nDCG) (Jarvelin and Kekalainen,
2002), a standard metric used in information re-
trieval to evaluate the quality of the ranked lists.
DCG at rank p is defined as:
DCGp = rel1 +
p?
i=2
reli
log2i
where reli is the relevance of the item at position i.
We normalize DCG by dividing it by an ideal gain
(i.e., DCG of same list, when ordered from highest
to lowest relevance).
For our task, we took the relevance score to be 1
for a sentence placed in Cat.1, a relevance score of
0.5 for a sentence placed in Cat.2, and a relevance
score of 0 for a sentence placed in Cat.3. We report
a normalized discounted cumulative gain of 77.19%.
This result compares favorably with results re-
ported for the two baselines (68.36% for B1 and
Figure 2: Distribution of nDCG for different relevance
scores assigned to sentences placed in category Cat.2.
68.32% for B2) as well as for when the sentence
simplifier was removed (69.45%).
Figure 2 shows different results for nDCG when
the relevance score for Cat.2 is varied between 0
(same as sentences placed in Cat.1) and 1 (same as
sentences placed in Cat.3).
4 Related Work
To the best of our knowledge, no one has attempted
to rank sentences from the biomedical literature,
using machine learning on a set of data marked
with preference judgments. However, different ap-
proaches have been described in the literature that
use preference judgments to learn ranked lists. For
example, Radlinski and Joachims (2005) used pref-
erence judgments to learn ranked retrieval functions
for web search results. These judgments were gen-
erated automatically from search engine logs. Their
learned rankings outperformed a static ranking func-
tion. Similar approaches in IR are those of Cohen et
al. (1999) and Freund et al. (2003).
Ranking of text passages and documents has
been done previously in BioNLP for other purposes.
Suomela and Andrade (2005) proposed a way to
rank the entire PubMed database, given a large train-
ing set for a specific topic. Goldberg et al. (2008)
and Lu et al. (2009) describe in detail how they iden-
tified and ranked passages for the 2006 Trec Ge-
nomics Track (Hersh et al., 2006). Yeganova et
al. (2011) present a method for ranking positively la-
beled data within large sets of data, and this method
was applied by Neveol et al. (2011) to rank sen-
tences containing deposition relationships between
biological data and public repositories.
169
Extraction of sentences describing gene functions
has also been applied for creating gene summaries
(Ling et al., 2007; Jin et al., 2009; Yang et al., 2009).
However, these methods differ in that their goal is
not to look for sentences containing specific terms
and their relations with genes, but rather for sen-
tences that fall into some predefined categories of
sentences typically observed in gene summaries.
Sentence simplification has been used to aid pars-
ing (Chandrasekar et al., 1996; Jonnalagadda et
al., 2009). Devlin and Tait (1998) and Carroll et
al. (1998) use it to help people with aphasia. Sid-
dharthan (2004) was concerned with cohesion and
suggested some applications.
The idea of using lexico-syntactic patterns to
identify relation candidates has also been applied in
the work of Banko et al. (2007), although their pat-
terns are not used in the learning process.
5 Conclusion and Future Directions
We have developed a system which aims to identify
sentences that clearly and succinctly describe the re-
lation between two entities. We used a set of prefer-
ence judgements, as provided by biologists, to learn
an SVM model that could make a choice between
any two sentences mentioning these entities.
The model compares favorably with baselines on
both the task of choosing between two sentences, as
well as ranking a set of sentences. The performance
for choosing between two sentences was 83.10%, as
compared to 65.75% and 71.69% for the two base-
lines, respectively. For choosing one sentence from
a list of sentences, the performance was 80%, as
compared to 58% and 61%. Furthermore, when the
entire list of ranked sentences was evaluated, the
system reported a nDCG of 77.19%, compared to
68.36% and 68.32% for the two baselines.
The model?s performance was also shown to
be significantly better when sentence simplification
was used. We were able to match relation patterns
on complex sentences, and observed an increase of
10.96%, 13%, and 7.74% for the three evaluations
afore-mentioned, respectively. It is noteworthy that,
without the simplification, the performance is only
slightly better than the second baseline. This is be-
cause the second baseline uses information that is
also used by our system, although this does not in-
clude the lexico-syntactic patterns that identify the
type of relation between the gene and the term.
Given that the full system?s performance is much
better than both baselines, and that the system?s per-
formance without simplification is only slightly bet-
ter than the second baseline, we believe that: (1) the
pattern and type of relation determination are impor-
tant, and (2) sentence simplification is crucial for the
determination of the relationship type.
We are currently pursuing summaries for genes.
Since iTerms have been shown in previous evalua-
tions to represent important aspects of a gene?s func-
tionality and behavior, we are investigating whether
they are represented in gene summaries found in En-
trezGene and UniProtKB. If so, an extractive sum-
mary can be produced by choosing sentences for the
gene and its iTerms. We are also considering de-
veloping abstractive summaries. Our use of lexico-
syntactic patterns can be extended to pick the exact
relation between a gene and the iTerm. For exam-
ple, by using the lexico-syntactic patterns, coupled
with simplification, we can extract the following ex-
act relations from the four sentences shown in Fig-
ure 1: ?Groucho is a corepressor?, ?The wrpw motif
recruits groucho?, ?Groucho is implicated in notch
signaling?, and ?The eh1 repression domain binds
groucho?. With these relations extracted, using text
generation algorithms for textual realization and co-
hesion, we can produce abstractive summaries.
We would also like to investigate how to general-
ize this work to other pairs of entities, as well as how
to generalize this work for other applications which
may or may not require the same features as the ones
we used.
Acknowledgments
This work has been supported in part by USDA
Grant 2008-35205-18734 and the Agriculture and
Food Research Initiative Competitive USDA Grant
2011-67015-3032. We thank Cecilia Arighi, Kevin
Bullaughey, Teresia Buza, Fiona McCarthy, Lak-
shmi Pillai, Carl Schmidt, Liang Sun, Hui Wang,
and Qinghua Wang for participating in the anno-
tation task and/or for various discussions. We
also thank the anonymous reviewers for their com-
ments and suggestions, which helped us improve the
manuscript.
170
References
