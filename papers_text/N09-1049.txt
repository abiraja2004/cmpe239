Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 433?441,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Hierarchical Phrase-Based Translation with
Weighted Finite State Transducers
Gonzalo Iglesias? Adria` de Gispert?
? University of Vigo. Dept. of Signal Processing and Communications. Vigo, Spain
{giglesia,erbanga}@gts.tsc.uvigo.es
? University of Cambridge. Dept. of Engineering. CB2 1PZ Cambridge, U.K.
{ad465,wjb31}@eng.cam.ac.uk
Eduardo R. Banga? William Byrne?
Abstract
This paper describes a lattice-based decoder
for hierarchical phrase-based translation. The
decoder is implemented with standard WFST
operations as an alternative to the well-known
cube pruning procedure. We find that the
use of WFSTs rather than k-best lists requires
less pruning in translation search, resulting
in fewer search errors, direct generation of
translation lattices in the target language,
better parameter optimization, and improved
translation performance when rescoring with
long-span language models and MBR decod-
ing. We report translation experiments for
the Arabic-to-English and Chinese-to-English
NIST translation tasks and contrast the WFST-
based hierarchical decoder with hierarchical
translation under cube pruning.
1 Introduction
Hierarchical phrase-based translation generates
translation hypotheses via the application of hierar-
chical rules in CYK parsing (Chiang, 2005). Cube
pruning is used to apply language models at each
cell of the CYK grid as part of the search for a
k-best list of translation candidates (Chiang, 2005;
Chiang, 2007). While this approach is very effective
and has been shown to produce very good quality
translation, the reliance on k-best lists is a limita-
tion. We take an alternative approach and describe a
lattice-based hierarchical decoder implemented with
Weighted Finite State Transducers (WFSTs). In ev-
ery CYK cell we build a single, minimal word lattice
containing all possible translations of the source sen-
tence span covered by that cell. When derivations
contain non-terminals, we use pointers to lower-
level lattices for memory efficiency. The pointers
are only expanded to the actual translations if prun-
ing is required during search; expansion is otherwise
only carried out at the upper-most cell, after the full
CYK grid has been traversed.
We describe how this decoder can be easily im-
plemented with WFSTs. For this we employ the
OpenFST libraries (Allauzen et al, 2007). Using
standard FST operations such as composition, ep-
silon removal, determinization, minimization and
shortest-path, we find this search procedure to be
simpler to implement than cube pruning. The main
modeling advantages are a significant reduction in
search errors, a simpler implementation, direct gen-
eration of target language word lattices, and better
integration with other statistical MT procedures. We
report translation results in Arabic-to-English and
Chinese-to-English translation and contrast the per-
formance of lattice-based and cube pruning hierar-
chical decoding.
1.1 Related Work
Hierarchical phrase-based translation has emerged
as one of the dominant current approaches to statis-
tical machine translation. Hiero translation systems
incorporate many of the strengths of phrase-based
translation systems, such as feature-based transla-
tion and strong target language models, while also
allowing flexible translation and movement based
on hierarchical rules extracted from aligned paral-
lel text. We summarize some extensions to the basic
approach to put our work in context.
433
Hiero Search Refinements Huang and Chiang
(2007) offer several refinements to cube pruning to
improve translation speed. Venugopal et al (2007)
introduce a Hiero variant with relaxed constraints
for hypothesis recombination during parsing; speed
and results are comparable to those of cube prun-
ing, as described by Chiang (2007). Li and Khudan-
pur (2008) report significant improvements in trans-
lation speed by taking unseen n-grams into account
within cube pruning to minimize language model re-
quests. Dyer et al (2008) extend the translation of
source sentences to translation of input lattices fol-
lowing Chappelier et al (1999).
Extensions to Hiero Several authors describe ex-
tensions to Hiero, to incorporate additional syntactic
information (Zollmann and Venugopal, 2006; Zhang
and Gildea, 2006; Shen et al, 2008; Marton and
Resnik, 2008), or to combine it with discriminative
latent models (Blunsom et al, 2008).
Analysis and Contrastive Experiments Zollman et
al. (2008) compare phrase-based, hierarchical and
syntax-augmented decoders for translation of Ara-
bic, Chinese, and Urdu into English. Lopez (2008)
explores whether lexical reordering or the phrase
discontiguity inherent in hierarchical rules explains
improvements over phrase-based systems. Hierar-
chical translation has also been used to great effect
in combination with other translation architectures,
e.g. (Sim et al, 2007; Rosti et al, 2007).
WFSTs for Translation There is extensive work in
using Weighted Finite State Transducer for machine
translation (Bangalore and Riccardi, 2001; Casacu-
berta, 2001; Kumar and Byrne, 2005; Mathias and
Byrne, 2006; Graehl et al, 2008).
To our knowledge, this paper presents the first de-
scription of hierarchical phrase-based translation in
terms of lattices rather than k-best lists. The next
section describes hierarchical phrase-based transla-
tion with WFSTs, including the lattice construction
over the CYK grid and pruning strategies. Sec-
tion 3 reports translation experiments for Arabic-to-
English and Chinese-to-English, and Section 4 con-
cludes.
2 Hierarchical Translation with WFSTs
The translation system is based on a variant of the
CYK algorithm closely related to CYK+ (Chappe-
lier and Rajman, 1998). Parsing follows the de-
scription of Chiang (2005; 2007), maintaining back-
pointers and employing hypothesis recombination
without pruning. The underlying model is a syn-
chronous context-free grammar consisting of a set
R = {Rr} of rules Rr : N ? ??r,?r? / pr, with
?glue? rules, S ? ?X,X? and S ? ?S X,S X?. If a
rule has probability pr, it is transformed to a cost cr;
here we use the tropical semiring, so cr = ? log pr.
N denotes a non-terminal; in this paper, N can be
either S, X, or V (see section 3.2). T denotes the
terminals (words), and the grammar builds parses
based on strings ?, ? ? {{S,X, V } ? T}+. Each
cell in the CYK grid is specified by a non-terminal
symbol and position in the CYK grid: (N,x, y),
which spans sx+y?1x on the source sentence.
In effect, the source language sentence is parsed
using a context-free grammar with rules N ? ?.
The generation of translations is a second step that
follows parsing. For this second step, we describe
a method to construct word lattices with all possible
translations that can be produced by the hierarchical
rules. Construction proceeds by traversing the CYK
grid along the backpointers established in parsing.
In each cell (N,x, y) in the CYK grid, we build a
target language word lattice L(N,x, y). This lat-
tice contains every translation of sx+y?1x from every
derivation headed by N . These lattices also contain
the translation scores on their arc weights.
The ultimate objective is the word lattice
L(S, 1, J) which corresponds to all the analyses that
cover the source sentence sJ1 . Once this is built,
we can apply a target language model to L(S, 1, J)
to obtain the final target language translation lattice
(Allauzen et al, 2003).
We use the approach of Mohri (2002) in applying
WFSTs to statistical NLP. This fits well with the use
of the OpenFST toolkit (Allauzen et al, 2007) to
implement our decoder.
2.1 Lattice Construction Over the CYK Grid
In each cell (N,x, y), the set of rule indices used
by the parser is denoted R(N,x, y), i.e. for r ?
R(N,x, y), N ? ??r,?r? was used in at least one
derivation involving that cell.
For each rule Rr, r ? R(N,x, y), we build a lat-
tice L(N,x, y, r). This lattice is derived from the
target side of the rule ?r by concatenating lattices
434
R1: X ? ?s1 s2 s3,t1 t2?
R2: X ? ?s1 s2,t7 t8?
R3: X ? ?s3,t9?
R4: S ? ?X,X?
R5: S ? ?S X,S X?
L(S, 1, 3) = L(S, 1, 3, 4) ? L(S, 1, 3, 5)
L(S, 1, 3, 4) = L(X, 1, 3) = L(X, 1, 3, 1) =
= A(t1)?A(t2)
L(S, 1, 3, 5) = L(S, 1, 2)? L(X, 3, 1)
L(S, 1, 2) = L(S, 1, 2, 4) = L(X, 1, 2) =
= L(X, 1, 2, 2) = A(t7)?A(t8)
L(X, 3, 1) = L(X, 3, 1, 3) = A(t9)
L(S, 1, 3, 5) = A(t7)?A(t8)?A(t9)
L(S, 1, 3) = (A(t1)?A(t2))? (A(t7)?A(t8)?A(t9))
Figure 1: Production of target lattice L(S, 1, 3) using translation rules within CYK grid for sentence s1s2s3. The grid
is represented here in two dimensions (x, y). In practice only the first column accepts both non-terminals (S,X). For
this reason it is divided in two subcolumns.
corresponding to the elements of ?r = ?r1...?r|?r |.
If an ?ri is a terminal, creating its lattice is straight-
forward. If ?ri is a non-terminal, it refers to a cell
(N ?, x?, y?) lower in the grid identified by the back-
pointer BP (N,x, y, r, i); in this case, the lattice
used is L(N ?, x?, y?). Taken together,
L(N,x, y, r) = ?
i=1..|?r|
L(N,x, y, r, i) (1)
L(N,x, y, r, i) =
{
A(?i) if ?i ? T
L(N ?, x?, y?) else
(2)
where A(t), t ? T returns a single-arc accep-
tor which accepts only the symbol t. The lattice
L(N,x, y) is then built as the union of lattices cor-
responding to the rules in R(N,x, y):
L(N,x, y) = ?
r?R(N,x,y)
L(N,x, y, r) (3)
Lattice union and concatenation are performed
using the ? and ? WFST operations respectively, as
described by Allauzen et al(2007). If a rule Rr has
a cost cr, it is applied to the exit state of the lattice
L(N,x, y, r) prior to the operation of Equation 3.
2.1.1 An Example of Phrase-based Translation
Figure 1 illustrates this process for a three word
source sentence s1s2s3 under monotone phrase-
based translation. The left-hand side shows the state
of the CYK grid after parsing using the rules R1 to
R5. These include 3 rules with only terminals (R1,
R2, R3) and the glue rules (R4, R5). Arrows repre-
sent backpointers to lower-level cells. We are inter-
ested in the upper-most S cell (S, 1, 3), as it repre-
sents the search space of translation hypotheses cov-
ering the whole source sentence. Two rules (R4, R5)
are in this cell, so the lattice L(S, 1, 3) will be ob-
tained by the union of the two lattices found by the
backpointers of these two rules. This process is ex-
plicitly derived in the right-hand side of Figure 1.
2.1.2 An Example of Hierarchical Translation
Figure 2 shows a hierarchical scenario for the
same sentence. Three rules, R6, R7, R8, are added
to the example of Figure 1, thus providing two ad-
ditional derivations. This makes use of sublattices
already produced in the creation of L(S, 1, 3, 5) and
L(X, 1, 3, 1) in Figure 1; these are within {}.
2.2 A Procedure for Lattice Construction
Figure 3 presents an algorithm to build the lattice
for every cell. The algorithm uses memoization: if
a lattice for a requested cell already exists, it is re-
turned (line 2); otherwise it is constructed via equa-
tions 1,2,3. For every rule, each element of the tar-
get side (lines 3,4) is checked as terminal or non-
terminal (equation 2). If it is a terminal element
(line 5), a simple acceptor is built. If it is a non-
terminal (line 6), the lattice associated to its back-
pointer is returned (lines 7 and 8). The complete
lattice L(N,x, y, r) for each rule is built by equa-
tion 1 (line 9). The lattice L(N,x, y) for this cell
is then found by union of all the component rules
(line 10, equation 3); this lattice is then reduced by
435
R6: X ? ?s1,t20?
R7: X ? ?X1 s2 X2,X1 t10 X2?
R8: X ? ?X1 s2 X2,X2 t10 X1?
L(S, 1, 3) = L(S, 1, 3, 4) ?{L(S, 1, 3, 5)}
L(S, 1, 3, 4) = L(X, 1, 3) =
={L(X, 1, 3, 1)} ?L(X, 1, 3, 7)? L(X, 1, 3, 8)
L(X, 1, 3, 7) = L(X, 1, 1, 6)?A(t10)?L(X, 3, 1, 3) =
= A(t20)?A(t10)?A(t9)
L(X, 1, 3, 8) = A(t9)?A(t10)?A(t20)
L(S, 1, 3) = {(A(t1)?A(t2))} ?
?(A(t20)?A(t10)?A(t9))? (A(t9)?A(t10)?A(t20))?
?{(A(t7)?A(t8)?A(t9))}
Figure 2: Translation as in Figure 1 but with additional rules R6,R7,R8. Lattices previously derived appear within {}.
standard WFST operations (lines 11,12,13). It is
important at this point to remove any epsilon arcs
which may have been introduced by the various
WFST union, concatenation, and replacement oper-
ations (Allauzen et al, 2007).
1 function buildFst(N,x,y)
2 if ? L(N,x, y) return L(N,x, y)
3 for r ? R(N,x, y), Rr : N ? ??,??
4 for i = 1...|?|
5 if ?i ? T, L(N,x, y, r, i) = A(?i)
6 else
7 (N ?, x?, y?) = BP (?i)
8 L(N,x, y, r, i) = buildFst(N ?, x?, y?)
9 L(N,x, y, r)=?i=1..|?| L(N,x, y, r, i)
10 L(N,x, y) =?r?R(N,x,y) L(N,x, y, r)
11 fstRmEpsilon L(N,x, y)
12 fstDeterminize L(N,x, y)
13 fstMinimize L(N,x, y)
14 return L(N,x, y)
Figure 3: Recursive Lattice Construction.
2.3 Delayed Translation
Equation 2 leads to the recursive construction of lat-
tices in upper-levels of the grid through the union
and concatenation of lattices from lower levels. If
equations 1 and 3 are actually carried out over fully
expanded word lattices, the memory required by the
upper lattices will increase exponentially.
To avoid this, we use special arcs that serve as
pointers to the low-level lattices. This effectively
builds a skeleton of the desired lattice and delays
the creation of the final word lattice until a single
replacement operation is carried out in the top cell
(S, 1, J). To make this exact, we define a function
g(N,x, y) which returns a unique tag for each lattice
in each cell, and use it to redefine equation 2. With
the backpointer (N ?, x?, y?) = BP (N,x, y, r, i),
these special arcs are introduced as:
L(N,x, y, r, i) =
{
A(?i) if ?i ? T
A(g(N ?, x?, y?)) else
(4)
The resulting lattices L(N,x, y) are a mix of tar-
get language words and lattice pointers (Figure 4,
top). However each still represents the entire search
space of all translation hypotheses covering the
span. Importantly, operations on these lattices ?
such as lossless size reduction via determinization
and minimization ? can still be performed. Owing
to the existence of multiple hierarchical rules which
share the same low-level dependencies, these opera-
tions can greatly reduce the size of the skeleton lat-
tice; Figure 4 shows the effect on the translation ex-
ample. This process is carried out for the lattice at
every cell, even at the lowest level where there are
only sequences of word terminals. As stated, size
reductions can be significant. However not all redu-
dancy is removed, since duplicate paths may arise
through the concatenation and union of sublattices
with different spans.
At the upper-most cell, the lattice L(S, 1, J) con-
tains pointers to lower-level lattices. A single FST
replace operation (Allauzen et al, 2007) recursively
substitutes all pointers by their lower-level lattices
until no pointers are left, thus producing the com-
plete target word lattice for the whole source sen-
tence. The use of the lattice pointer arc was in-
spired by the ?lazy evaluation? techniques developed
by Mohri et al(2000). Its implementation uses the
infrastructure provided by the OpenFST libraries for
436
01
t1
2g(X,1,2)
3
g(X,1,1)
5
g(X,3,1)
7
t2
g(X,3,1)
4
t10
6t10
g(X,3,1)
g(X,1,1)
0
3g(X,1,1)
2g(X,1,2)
1
t1
4
g(X,3,1)
t10
6
g(X,3,1)
t2
5
t10
g(X,1,1)
Figure 4: Delayed translation WFST with derivations
from Figure 1 and Figure 2 before [t] and after minimiza-
tion [b].
delayed composition, etc.
2.4 Pruning in Lattice Construction
The final translation lattice L(S, 1, J) can grow very
large after the pointer arcs are expanded. We there-
fore apply a word-based language model, via WFST
composition, and perform likelihood-based prun-
ing (Allauzen et al, 2007) based on the combined
translation and language model scores.
Pruning can also be performed on sublattices
during search. One simple strategy is to monitor
the number of states in the determinized lattices
L(N,x, y). If this number is above a threshold, we
expand any pointer arcs and apply a word-based lan-
guage model via composition. The resulting lattice
is then reduced by likelihood-based pruning, after
which the LM scores are removed. This search prun-
ing can be very selective. For example, the pruning
threshold can depend on the height of the cell in the
grid. In this way the risk of search errors can be
controlled.
3 Translation Experiments
We report experiments on the NIST MT08 Arabic-
to-English and Chinese-to-English translation tasks.
We contrast two hierarchical phrase-based decoders.
The first decoder, Hiero Cube Pruning (HCP), is a k-
best decoder using cube pruning implemented as de-
scribed by Chiang (2007). In our implementation, k-
best lists contain unique hypotheses. The second de-
coder, Hiero FST (HiFST), is a lattice-based decoder
implemented with Weighted Finite State Transduc-
ers as described in the previous section. Hypotheses
are generated after determinization under the trop-
ical semiring so that scores assigned to hypotheses
arise from single minimum cost / maximum likeli-
hood derivations. We also use a variant of the k-best
decoder which works in alignment mode: given an
input k-best list, it outputs the feature scores of each
hypothesis in the list without applying any pruning.
This is used for Minimum Error Training (MET)
with the HiFST system.
These two language pairs pose very different
translation challenges. For example, Chinese-
to-English translation requires much greater word
movement than Arabic-to-English. In the frame-
work of hierarchical translation systems, we have
found that shallow decoding (see section 3.2) is
as good as full hierarchical decoding in Arabic-
to-English (Iglesias et al, 2009). In Chinese-to-
English, we have not found this to be the case.
Therefore, we contrast the performance of HiFST
and HCP under shallow hierarchical decoding for
Arabic-to-English, while for Chinese-to-English we
perform full hierarchical decoding.
Both hierarchical translation systems share a
common architecture. For both language pairs,
alignments are generated over the parallel data. The
following features are extracted and used in trans-
lation: target language model, source-to-target and
target-to-source phrase translation models, word and
rule penalties, number of usages of the glue rule,
source-to-target and target-to-source lexical models,
and three rule count features inspired by Bender et
al. (2007). The initial English language model is
a 4-gram estimated over the parallel text and a 965
million word subset of monolingual data from the
English Gigaword Third Edition. Details of the par-
allel corpus and development sets used for each lan-
guage pair are given in their respective section.
Standard MET (Och, 2003) iterative parameter
estimation under IBM BLEU (Papineni et al, 2001)
is performed on the corresponding development set.
For the HCP system, MET is done following Chi-
ang (2007). For the HiFST system, we obtain a k-
437
best list from the translation lattice and extract each
feature score with the aligner variant of the k-best
decoder. After translation with optimized feature
weights, we carry out the two following rescoring
steps.
? Large-LM rescoring. We build sentence-
specific zero-cutoff stupid-backoff (Brants et
al., 2007) 5-gram language models, estimated
using ?4.7B words of English newswire text,
and apply them to rescore either 10000-best
lists generated by HCP or word lattices gener-
ated by HiFST. Lattices provide a vast search
space relative to k-best lists, with translation
lattice sizes of 1081 hypotheses reported in the
literature (Tromble et al, 2008).
? Minimum Bayes Risk (MBR). We rescore the
first 1000-best hypotheses with MBR, taking
the negative sentence level BLEU score as the
loss function (Kumar and Byrne, 2004).
3.1 Building the Rule Sets
We extract hierarchical phrases from word align-
ments, applying the same restrictions as introduced
by Chiang (2005). Additionally, following Iglesias
et al (2009) we carry out two rule filtering strate-
gies:
? we exclude rules with two non-terminals with
the same order on the source and target side
? we consider only the 20 most frequent transla-
tions for each rule
For each development set, this produces approx-
imately 4.3M rules in Arabic-to-English and 2.0M
rules in Chinese-to-English.
3.2 Arabic-to-English Translation
We translate Arabic-to-English with shallow hierar-
chical decoding, i.e. only phrases are allowed to be
substituted into non-terminals. The rules used in this
case are, in addition to the glue rules:
X ? ??s,?s?
X ? ?V ,V ?
V ? ?s,t?
s, t ? T+; ?s, ?s ? ({V } ?T)+
For translation model training, we use all allowed
parallel corpora in the NIST MT08 Arabic track
(?150M words per language). In addition to the
MT08 set itself, we use a development set mt02-05-
tune formed from the odd numbered sentences of the
NIST MT02 through MT05 evaluation sets; the even
numbered sentences form the validation set mt02-
05-test. The mt02-05-tune set has 2,075 sentences.
The cube pruning decoder, HCP, employs k-best
lists of depth k=10000 (unique). Using deeper lists
results in excessive memory and time requirements.
In contrast, the WFST-based decoder, HiFST, re-
quires no local pruning during lattice construction
for this task and the language model is not applied
until the lattice is fully built at the upper-most cell of
the CYK grid.
Table 1 shows results for mt02-05-tune, mt02-
05-test and mt08, as measured by lowercased IBM
BLEU and TER (Snover et al, 2006). MET param-
eters are optimized for the HCP decoder. As shown
in rows ?a? and ?b?, results after MET are compara-
ble.
Search Errors Since both decoders use exactly the
same features, we can measure their search errors on
a sentence-by-sentence basis. A search error is as-
signed to one of the decoders if the other has found
a hypothesis with lower cost. For mt02-05-tune, we
find that in 18.5% of the sentences HiFST finds a hy-
pothesis with lower cost than HCP. In contrast, HCP
never finds any hypothesis with lower cost for any
sentence. This is as expected: the HiFST decoder
requires no pruning prior to applying the language
model, so search is exact.
Lattice/k-best Quality Rescoring results are dif-
ferent for cube pruning and WFST-based decoders.
Whereas HCP improves by 0.9 BLEU, HiFST im-
proves over 1.5 BLEU. Clearly, search errors in HCP
not only affect the 1-best output but also the quality
of the resulting k-best lists. For HCP, this limits the
possible gain from subsequent rescoring steps such
as large LMs and MBR.
Translation Speed HCP requires an average of 1.1
seconds per input word. HiFST cuts this time by
half, producing output at a rate of 0.5 seconds per
word. It proves much more efficient to process com-
pact lattices contaning many hypotheses rather than
to independently processing each one of them in k-
best form.
438
decoder mt02-05-tune mt02-05-test mt08
BLEU TER BLEU TER BLEU TER
a HCP 52.2 41.6 51.5 42.2 42.5 48.6
+5gram 53.1 41.0 52.5 41.5 43.3 48.3
+MBR 53.2 40.8 52.6 41.4 43.4 48.1
b HiFST 52.2 41.5 51.6 42.1 42.4 48.7
+5gram 53.3 40.6 52.7 41.3 43.7 48.1
+MBR 53.7 40.4 53.3 40.9 44.0 48.0
Decoding time in secs/word: 1.1 for HCP; 0.5 for HiFST.
Table 1: Constrative Arabic-to-English translation results (lower-cased IBM BLEU | TER) after MET and subsequent
rescoring steps. Decoding time reported for mt02-05-tune.
The mixed case NIST BLEU-4 for the HiFST sys-
tem on mt08 is 42.9. This is directly comparable to
the official MT08 Constrained Training Track eval-
uation results1.
3.3 Chinese-to-English Translation
We translate Chinese-to-English with full hierarchi-
cal decoding, i.e. hierarchical rules are allowed to be
substituted into non-terminals. We consider a maxi-
mum span of 10 words for the application of hierar-
chical rules and only glue rules are allowed at upper
levels of the CYK grid.
For translation model training, we use all avail-
able data for the GALE 2008 evaluation2, approx.
250M words per language. In addition to the MT08
set itself, we use a development set tune-nw and
a validation set test-nw. These contain a mix of
the newswire portions of MT02 through MT05 and
additional developments sets created by translation
within the GALE program. The tune-nw set has
1,755 sentences.
Again, the HCP decoder employs k-best lists of
depth k=10000. The HiFST decoder applies prun-
ing in search as described in Section 2.4, so that any
lattice in the CYK grid is pruned if it covers at least
3 source words and contains more than 10k states.
The likelihood pruning threshold relative to the best
path in the lattice is 9. This is a very broad threshold
so that very few paths are discarded.
1Full MT08 results are available at http://www.nist.gov/
speech/tests/mt/2008/doc/mt08 official results v0.html. It is
worth noting that many of the top entries make use of system
combination; the results reported here are for single system
translation.
2See http://projects.ldc.upenn.edu/gale/data/catalog.html.
Improved Optimization Table 2 shows results for
tune-nw, test-nw and mt08, as measured by lower-
cased IBM BLEU and TER. The first two rows show
results for HCP when using MET parameters opti-
mized over k-best lists produced by HCP (row ?a?)
and by HiFST (row ?b?). We find that using the k-
best list obtained by the HiFST decoder yields bet-
ter parameters during optimization. Tuning on the
HiFST k-best lists improves the HCP BLEU score,
as well. We find consistent improvements in BLEU;
TER also improves overall, although less consis-
tently.
Search Errors Measured over the tune-nw devel-
opment set, HiFST finds a hypothesis with lower
cost in 48.4% of the sentences. In contrast, HCP
never finds any hypothesis with a lower cost for any
sentence, indicating that the described pruning strat-
egy for HiFST is much broader than that of HCP.
Note that HCP search errors are more frequent for
this language pair. This is due to the larger search
space required in fully hierarchical translation; the
larger the search space, the more search errors will
be produced by the cube pruning k-best implemen-
tation.
Lattice/k-best Quality The lattices produced by
HiFST yield greater gains in LM rescoring than the
k-best lists produced by HCP. Including the subse-
quent MBR rescoring, translation improves as much
as 1.2 BLEU, compared to 0.7 BLEU with HCP.
The mixed case NIST BLEU-4 for the HiFST sys-
tem on mt08 is 27.8, comparable to official results
in the UnConstrained Training Track of the NIST
2008 evaluation.
439
decoder MET k-best tune-nw test-nw mt08
BLEU TER BLEU TER BLEU TER
a HCP HCP 31.6 59.7 31.9 59.7 ? ?
b HCP 31.7 60.0 32.2 59.9 27.2 60.2
+5gram HiFST 32.2 59.3 32.6 59.4 27.8 59.3
+MBR 32.4 59.2 32.7 59.4 28.1 59.3
c HiFST 32.0 60.1 32.2 60.0 27.1 60.5
+5gram HiFST 32.7 58.3 33.1 58.4 28.1 59.1
+MBR 32.9 58.4 33.4 58.5 28.9 58.9
Table 2: Contrastive Chinese-to-English translation results (lower-cased IBM BLEU|TER) after MET and subsequent
rescoring steps. The MET k-best column indicates which decoder generated the k-best lists used in MET optimization.
4 Conclusions
The lattice-based decoder for hierarchical phrase-
based translation described in this paper can be eas-
ily implemented using Weighted Finite State Trans-
ducers. We find many benefits in this approach
to translation. From a practical perspective, the
computational operations required can be easily car-
ried out using standard operations already imple-
mented in general purpose libraries. From a model-
ing perspective, the compact representation of mul-
tiple translation hypotheses in lattice form requires
less pruning in hierarchical search. The result is
fewer search errors and reduced overall memory use
relative to cube pruning over k-best lists. We also
find improved performance of subsequent rescor-
ing procedures which rely on the translation scores.
In direct comparison to k-best lists generated un-
der cube pruning, we find that MET parameter opti-
mization, rescoring with large language models, and
MBR decoding, are all improved when applied to
translations generated by the lattice-based hierarchi-
cal decoder.
Acknowledgments
This work was supported in part by the GALE pro-
gram of the Defense Advanced Research Projects
Agency, Contract No. HR0011- 06-C-0022. G.
Iglesias supported by Spanish Government research
grant BES-2007-15956 (project TEC2006-13694-
C03-03).
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In Proceedings of ACL, pages 557?
564.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of CIAA, pages 11?23.
Srinivas Bangalore and Giuseppe Riccardi. 2001. A
finite-state approach to machine translation. In Pro-
ceedings of NAACL.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language trans-
lation system. In Proceedings of ASRU, pages 396?
401.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proceedings of ACL-HLT,
pages 200?208.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of EMNLP-ACL,
pages 858?867.
Francisco Casacuberta. 2001. Finite-state transducers
for speech-input translation. In Proceedings of ASRU.
Jean-Ce?dric Chappelier and Martin Rajman. 1998.
A generalized CYK algorithm for parsing stochastic
CFG. In Proceedings of TAPD, pages 133?137.
Jean-Ce?dric Chappelier, Martin Rajman, Ramo?n
Aragu?e?s, and Antoine Rozenknop. 1999. Lattice
parsing for speech recognition. In Proceedings of
TALN, pages 95?104.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263?270.
440
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-HLT, pages 1012?1020.
Jonathan Graehl, Kevin Knight, and Jonathan May. 2008.
Training tree transducers. Computational Linguistics,
34(3):391?427.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of ACL, pages 144?151.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
EACL.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?176.
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation.
In Proceedings of HLT-EMNLP, pages 161?168.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In Pro-
ceedings of the ACL-HLT Second Workshop on Syntax
and Structure in Statistical Translation, pages 10?18.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of COLING, pages
505?512.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-HLT, pages 1003?1011.
Lambert Mathias and William Byrne. 2006. Statisti-
cal phrase-based speech translation. In Proceedings
of ICASSP.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2000. The design principles of a weighted finite-
state transducer library. Theoretical Computer Sci-
ence, 231:17?32.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2002. Weighted finite-state transducers in speech
recognition. In Computer Speech and Language, vol-
ume 16, pages 69?88.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL,
pages 311?318.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie Dorr.
2007. Combining outputs from multiple machine
translation systems. In Proceedings of HLT-NAACL,
pages 228?235.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-HLT, pages 577?585.
Khe Chai Sim, William Byrne, Mark Gales, Hichem
Sahbi, and Phil Woodland. 2007. Consensus net-
work decoding for statistical machine translation sys-
tem combination. In Proceedings of ICASSP, vol-
ume 4, pages 105?108.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA, pages 223?231.
Roy Tromble, Shankar Kumar, Franz J. Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-Risk
decoding for statistical machine translation. In Pro-
ceedings of EMNLP, pages 620?629.
Ashish Venugopal, Andreas Zollmann, and Vogel
Stephan. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In Proceed-
ings of HLT-NAACL, pages 500?507.
Hao Zhang and Daniel Gildea. 2006. Synchronous bi-
narization for machine translation. In Proceedings of
HLT-NAACL, pages 256?263.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of NAACL Workshop on Statistical Ma-
chine Translation, pages 138?141.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proceedings of COLING, pages 1145?1152.
441
