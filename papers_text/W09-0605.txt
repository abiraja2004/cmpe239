Proceedings of the 12th European Workshop on Natural Language Generation, page 33,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Probabilistic Approaches for Modeling Text Structure and their
application to Text-to-Text Generation
Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
regina@csail.mit.edu
Text-to-text generation aims to produce a coher-
ent text by extracting, combining and rewriting in-
formation given in input texts. Examples of its ap-
plications include summarization, answer fusion
in question-answering and text simplification. At
first glance, text-to-text generation seems a much
easier task than the traditional generation set-up
where the input consists of a non-linguistic rep-
resentation. Research in summarization over the
last decade proved that the opposite is true ? texts
generated by these methods rarely match the qual-
ity of those written by humans. One of the key
reasons is the lack of coherence in the generated
text.
In contrast to the traditional set-up in concept-
to-text generation, these applications do not have
access to semantic representations and domain-
specific communication knowledge. Therefore,
traditional approaches for content selection cannot
be employed in text-to-text applications. These
considerations motivate the development of novel
approaches for document organization that can ex-
clusively rely on information available in textual
input.
In this talk, I will present models of document
structure that can be effectively used to guide con-
tent selection in text-to-text generation. First, I
will focus on unsupervised learning of domain-
specific content models. These models capture
the topics addressed in a text, and the order in
which these topics appear; they are close in their
functionality to the content planners traditionally
used in concept-to-text generation. I will present
an effective method for learning content models
from unannotated domain-specific documents, uti-
lizing hierarchical Bayesian methods. Incorpora-
tion of these models into information ordering and
summarization applications yields substantial im-
provement over previously proposed methods.
Next, I will present a method for assessing
the coherence of a generated text. The key
premise of our work is that the distribution of en-
tities in coherent texts exhibits certain regulari-
ties. The models I will be presenting operate over
an automatically-computed representation that re-
flects distributional, syntactic, and referential in-
formation about discourse entities. This represen-
tation allows us to induce the properties of coher-
ent texts from a given corpus, without recourse
to manual annotation or a predefined knowledge
base. I will show how these models can be effec-
tively integrated in text-to-text applications such
as summarization and answer fusion.
This is joint work with Branavan, Harr Chen,
Mirella Lapata and Lillian Lee.
33
