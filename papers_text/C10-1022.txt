Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 188?196,
Beijing, August 2010
A Twin-Candidate Based Approach for Event Pronoun Resolution us-
ing Composite Kernel  
Chen Bin1 Su Jian2 Tan Chew Lim1 
1National University of Singapore 2Institute for Inforcomm Research, A-STAR 
{chenbin,tancl}@comp.nus.edu.sg sujian@i2r.a-star.edu.sg 
 
Abstract 
Event Anaphora Resolution is an important 
task for cascaded event template extraction 
and other NLP study. In this paper, we provide 
a first systematic study of resolving pronouns 
to their event verb antecedents for general 
purpose. First, we explore various positional, 
lexical and syntactic features useful for the 
event pronoun resolution. We further explore 
tree kernel to model structural information 
embedded in syntactic parses. A composite 
kernel is then used to combine the above di-
verse information. In addition, we employed a 
twin-candidate based preferences learning 
model to capture the pair wise candidates? pre-
ference knowledge. Besides we also look into 
the incorporation of the negative training in-
stances with anaphoric pronouns whose ante-
cedents are not verbs. Although these negative 
training instances are not used in previous 
study on anaphora resolution, our study shows 
that they are very useful for the final resolu-
tion through random sampling strategy. Our 
experiments demonstrate that it?s meaningful 
to keep certain training data as development 
data to help SVM select a more accurate hyper 
plane which provides significant improvement 
over the default setting with all training data. 
1 Introduction 
Anaphora resolution, the task of resolving a giv-
en text expression to its referred expression in 
prior texts, is important for intelligent text 
processing systems. Most previous works on 
anaphora resolution mainly aims at object ana-
phora in which both the anaphor and its antece-
dent are mentions of the same real world objects 
In contrast, an event anaphora as first defined 
in (Asher, 1993) is an anaphoric reference to an 
event, fact, and proposition which is representa-
tive of eventuality and abstract entity. Consider 
the following example: 
This was an all-white, all-Christian community 
that all the sudden was taken over -- not taken 
over, that's a very bad choice of words, but [in-
vaded]1 by, perhaps different groups. 
[It]2 began when a Hasidic Jewish family bought 
one of the town's two meat-packing plants 13 
years ago. 
The anaphor [It]2 in the above example refers 
back to an event, ?all-white and all-Christian city 
of Postville is diluted by different ethnic groups.? 
Here, we take the main verb of the event, [in-
vaded]1 as the representation of this event and 
the antecedent for pronoun [It]2.  
According to (Asher, 1993), antecedents of 
event pronoun include both gerunds (e.g. de-
struction) and inflectional verbs (e.g. destroying). 
In our study, we focus on the inflectional verb 
representation, as the gerund representation is 
studied in the conventional anaphora resolution. 
For the rest of this paper, ?event pronouns? are 
pronouns whose antecedents are event verbs 
while ?non-event anaphoric pronouns? are those 
with antecedents other than event verbs. 
 Entity anaphora resolution provides critical 
links for cascaded event template extraction. It 
also provides useful information for further infe-
rence needed in other natural language 
processing tasks such as discourse relation and 
entailment. Event anaphora (both pronouns and 
noun phrases) contributes a significant propor-
tion in anaphora corpora, such as OntoNotes. 
19.97% of its total number of entity chains con-
tains event verb mentions. 
In (Asher, 1993) chapter 6, a method to re-
solve references to abstract entities using dis-
course representation theory is discussed. How-
ever, no computation system was proposed for 
entity anaphora resolution. (Byron, 2002) pro-
posed semantic filtering as a complement to sa-
lience calculations to resolve event pronoun tar-
geted by us. This knowledge deep approach only 
188
works for much focused domain like trains spo-
ken dialogue with handcraft knowledge of rele-
vant events for only limited number of verbs in-
volved.  Clearly, this approach is not suitable for 
general event pronoun resolution say in news 
articles. Besides, there?s also no specific perfor-
mance report on event pronoun resolution, thus 
it?s not clear how effective their approach is. 
(M?ller, 2007) proposed pronoun resolution sys-
tem using a set of hand-crafted constraints such 
as ?argumenthood? and ?right-frontier condition? 
together with logistic regression model based on 
corpus counts. The event pronouns are resolved 
together with object pronouns. This explorative 
work produced an 11.94% F-score for event pro-
noun resolution which demonstrated the difficul-
ty of event anaphora resolution. In (Pradhan, 
et.al, 2007), a general anaphora resolution sys-
tem is applied to OntoNotes corpus. However, 
their set of features is designed for object ana-
phora resolution. There is no specific perfor-
mance reported on event anaphora. We suspect 
the event pronouns are not correctly resolved in 
general as most of these features are irrelevant to 
event pronoun resolution.  
In this paper, we provide the first systematic 
study on pronominal references to event antece-
dents. First, we explore various positional, lexi-
cal and syntactic features useful for event pro-
noun resolution, which turns out quite different 
from conventional pronoun resolution except 
sentence distance information. These have been 
used together with syntactic structural informa-
tion using a composite kernel. Furthermore, we 
also consider candidates? preferences informa-
tion using twin-candidate model. 
Besides we further look into the incorporation 
of negative instances from non-event anaphoric 
pronoun, although these instances are not used in 
previous study on co-reference or anaphora reso-
lution as they make training instances extremely 
unbalanced. Our study shows that they can be 
very useful for the final resolution after random 
sampling strategy.  
We further demonstrate that it?s meaningful to 
keep certain training data as development data to 
help SVM select a more accurate hyper-plane 
which provide significant improvement over the 
default setting with all training data.  
The rest of this paper is organized as follows.  
Section 2 introduces the framework for event 
pronoun resolution, the considerations on train-
ing instance, the various features useful for event 
pronoun resolution and SVM classifier with ad-
justment of hyper-plane. Twin-candidate model 
is further introduced to capture the preferences 
among candidates. Section 3 presents in details 
the structural syntactic feature and the kernel 
functions to incorporate such a feature in the res-
olution. Section 4 presents the experiment results 
and some discussion. Section 5 concludes the 
paper. 
2 The Resolution Framework 
Our event-anaphora resolution system adopts the 
common learning-based model for object ana-
phora resolution, as employed by (Soon et al, 
2001) and (Ng and Cardie, 2002a). 
2.1 Training and Testing instance 
In the learning framework, training or testing 
instance of the resolution system has a form of 
               where        is the i
th candi-
date of the antecedent of anaphor    . An in-
stance is labeled as positive if        is the ante-
cedent of      , or negative if        is not the 
antecedent of     . An instance is associated 
with a feature vector which records different 
properties and relations between     and       . 
The features used in our system will be discussed 
later in this paper.  
During training, for each event pronoun, we 
consider the preceding verbs in its current and 
previous two sentences as its antecedent candi-
dates. A positive instance is formed by pairing an 
anaphor with its correct antecedent. And a set of 
negative instances is formed by pairing an ana-
phor with its candidates other than the correct 
antecedent. In addition, more negative instances 
are generated from non-event anaphoric pro-
nouns. Such an instance is created by pairing up 
a non-event anaphoric pronoun with each of the 
verbs within the pronoun?s sentence and previous 
two sentences. This set of instances from non-
event anaphoric pronouns is employed to provide 
extra power on ruling out non-event anaphoric 
pronouns during resolution. This is inspired by 
the fact that event pronouns are only 14.7% of all 
the pronouns in the OntoNotes corpus. Based on 
these generated training instances, we can train a 
binary classifier using any discriminative learn-
ing algorithm. 
189
The natural distribution of textual data is of-
ten imbalanced. Classes with fewer examples are 
under-represented and classifiers often perform 
far below satisfactory. In our study, this becomes 
a significant issue as positive class (event ana-
phoric) is the minority class in pronoun resolu-
tion task. Thus we utilize a random down sam-
pling method to reduce majority class samples to 
an equivalent level with the minority class sam-
ples which is described in (Kubat and Matwin, 
1997) and (Estabrooks et al 2004). In (Ng and 
Cardie, 2002b), they proposed a negative sample 
selection scheme which included only negative 
instances found in between an anaphor and its 
antecedent. However, in our event pronoun reso-
lution, we are distinguishing the event-anaphoric 
from non-event anaphoric which is different 
from (Ng and Cardie, 2002b). 
2.2 Feature Space 
In a conventional pronoun resolution, a set of 
syntactic and semantic knowledge has been re-
ported as in (Strube and M?ller, 2003; Yang et al 
2004;2005a;2006). These features include num-
ber agreement, gender agreement and many oth-
ers. However, most of these features are not use-
ful for our task, as our antecedents are inflection-
al verbs instead of noun phrases. Thus we have 
conducted a study on effectiveness of potential 
positional, lexical and syntactic features. The 
lexical knowledge is mainly collected from cor-
pus statistics. The syntactic features are mainly 
from intuitions. These features are purposely en-
gineered to be highly correlated with positive 
instances. Therefore such kind of features will 
contribute to a high precision classifier.  
? Sentence Distance 
This feature measures the sentence distance be-
tween an anaphor and its antecedent candidate 
under the assumptions that a candidate in the 
closer sentence to the anaphor is preferred to be 
the antecedent. 
? Word Distance  
This feature measures the word distance between 
an anaphor and its antecedent candidate. It is 
mainly to distinguish verbs from the same sen-
tence. 
? Surrounding Words and POS Tags 
The intuition behind this set of features is to find 
potential surface words that occur most frequent-
ly with the positive instances. Since most of 
verbs occurred in front of pronoun, we have built 
a frequency table from the preceding 5 words of 
the verb to succeeding 5 surface words of the 
pronoun. After the frequency table is built, we 
select those words with confidence1  > 70% as 
features. Similar to Surrounding Words, we have 
built a frequency table to select indicative sur-
rounding POS tags which occurs most frequently 
with positive instances. 
? Co-occurrences of Surrounding Words 
The intuition behind this set of features is to cap-
ture potential surface patterns such as ?It 
caused?? and ?It leads to?. These patterns are 
associated with strong indication that pronoun 
?it? is an event pronoun. The range for the co-
occurrences is from preceding 5 words to suc-
ceeding 5 words. All possible combinations of 
word positions are used for a co-occurrence 
words pattern. For example ?it leads to? will 
generate a pattern as ?S1_S2_lead_to? where S1 
and S2 mean succeeding position 1 and 2. Simi-
lar to previous surrounding words, we will con-
duct corpus statistics analysis and select co-
occurrence patterns with a confidence greater 
than 70%. Following the same process, we have 
examined co-occurrence patterns for surrounding 
POS tags.  
? Subject/Object Features 
This set of features aims to capture the relative 
position of the pronoun in a sentence. It denotes 
the preference of pronoun?s position at the clause 
level. There are 4 features in this category as 
listed below. 
Subject of Main Clause 
This feature indicates whether a pronoun is at the 
subject position of a main clause. 
Subject of Sub-clause 
This feature indicates whether a pronoun is at the 
subject position of a sub-clause. 
Object of Main Clause 
This feature indicates whether a pronoun is at the 
object position of a main clause. 
Object of Sub-clause 
This feature indicates whether a pronoun is at the 
object position of a sub-clause. 
? Verb of Main/Sub Clause 
Similar to the Subject/Object features of pro-
noun, the following two features capture the rela-
                                                 
1               
                                        
                    
 
190
tive position of a verb in a sentence. It encodes 
the preference of verb position between main 
verbs in main/sub clauses. 
Main Verb in Main Clause 
This feature indicates whether a verb is a main 
verb in a main clause. 
Main Verb in Sub-clause 
This feature indicates whether a verb is a main 
verb in a sub-clause. 
2.3 Support Vector Machine 
In theory, any discriminative learning algorithm 
is applicable to learn a classifier for pronoun res-
olution. In our study, we use Support Vector Ma-
chine (Vapnik, 1995) to allow the use of kernels 
to incorporate the structure feature. One advan-
tage of SVM is that we can use tree kernel ap-
proach to capture syntactic parse tree information 
in a particular high-dimension space. 
Suppose a training set   consists of labeled 
vectors          , where    is the feature vector 
of a training instance and    is its class label. The 
classifier learned by SVM is: 
                     
   
  
where    is the learned parameter for a support 
vector   . An instance   is classified as positive 
if       . Otherwise,   is negative. 
? Adjust Hyper-plane with Development Data 
Previous works on pronoun resolution such as 
(Yang et al 2006) used the default setting for 
hyper-plane which sets       . And an in-
stance is positive if        and negative oth-
erwise. In our study, we look into a method of 
adjusting the hyper-plane?s position using devel-
opment data to improve the classifier?s perfor-
mance.  
Considering a default model setting for SVM 
as shown in Figure 2(for illustration purpose, we 
use a 2-D example). 
 
Figure 2: 2-D SVM Illustration 
The objective of SVM learning process is to find 
a set of weight vector   which maximizes the 
margin (defined as  
   
) with constraints defined 
by support vectors. The separating hyper-plane is 
given by         as bold line in the center. 
The margin is the region between the two dotted 
lines (bounded by         and     
    ). The margin is a space without any in-
formation from training instances. The actual 
hyper-plane may fall in any place within the 
margin. It does not necessarily occur in the. 
However, the hyper-plane is used to separate 
positive and negative instances during classifica-
tion process without consideration of the margin. 
Thus if an instance falls in the margin, SVM can 
only decide class label from hyper-plane which 
may cause misclassification in the margin. 
 Based on the previous discussion, we propose 
an adjustment of the hyper-plane using develop-
ment data. For simplicity, we adjust the hyper-
plane function value instead of modeling the 
function itself. The hyper-plane function value 
will be further referred as a threshold  . The fol-
lowing is a modified version of a learned SVM 
classifier. 
        
                          
   
   
                         
   
   
  
where   is the threshold,    is the learned para-
meter for a feature    and    is its class label. A 
set of development data is used to adjust the hy-
per-plane function threshold   in order to max-
imize the accuracy of the learned SVM classifier 
on development data. The adjustment of hyper-
plane is defined as: 
                            
   
  
where        is an indicator function which out-
put 1 if       is same sign as   and 0 otherwise. 
Thereafter, the learned threshold    is applied to 
the testing set. 
3 Incorporating Structural Syntactic In-
formation 
A parse tree that covers a pronoun and its ante-
cedent candidate could provide us much syntac-
tic information related to the pair which is expli-
citly or implicitly represented in the tree. There-
fore, by comparing the common sub-structures 
between two trees we can find out to what degree 
two trees contain similar syntactic information, 
which can be done using a convolution tree ker-
nel. The value returned from tree kernel reflects 
similarity between two instances in syntax. Such 
191
syntactic similarity can be further combined with 
other knowledge to compute overall similarity 
between two instances, through a composite ker-
nel. Normally, parsing is done at sentence level. 
However, in many cases a pronoun and its ante-
cedent candidate do not occur in the same sen-
tence. To present their syntactic properties and 
relations in a single tree structure, we construct a 
syntax tree for an entire text, by attaching the 
parse trees of all its sentences to an upper node. 
Having obtained the parse tree of a text, we shall 
consider how to select the appropriate portion of 
the tree as the structured feature for a given in-
stance. As each instance is related to a pronoun 
and a candidate, the structured feature at least 
should be able to cover both of these two expres-
sions. 
3.1 Structural Syntactic Feature 
Generally, the more substructure of the tree is 
included, the more syntactic information would 
be provided, but at the same time the more noisy 
information that comes from parsing errors 
would likely be introduced. In our study, we ex-
amine three possible structured features that con-
tain different substructures of the parse tree: 
 
? Minimum Expansion Tree 
This feature records the minimal structure cover-
ing both pronoun and its candidate in parse tree. 
It only includes the nodes occurring in the short-
est path connecting the pronoun and its candidate, 
via the nearest commonly commanding node.  
When the pronoun and candidate are from differ-
ent sentences, we will find a path through pseudo 
?TOP? node which links all the parse trees. Con-
sidering the example given in section 1,  
This was an all-white, all-Christian community 
that all the sudden was taken over -- not taken 
over, that's a very bad choice of words, but [in-
vaded]1 by, perhaps different groups. 
[It]2 began when a Hasidic Jewish family bought 
one of the town's two meat-packing plants 13 
years ago. 
The minimum expansion structural feature of the 
instance {invaded, it} is annotated with bold 
lines and shaded nodes in figure 1.  
? Simple Expansion Tree 
Minimum-Expansion could, to some degree, de-
scribe the syntactic relationships between the 
candidate and pronoun. However, it is incapable 
of capturing the syntactic properties of the can-
didate or the pronoun, because the tree structure 
surrounding the expression is not taken into con-
sideration. To incorporate such information, fea-
ture Simple-Expansion not only contains all the 
nodes in Minimum-Expansion, but also includes 
the first-level children of these nodes2 except the 
punctuations. The simple-expansion structural 
feature of instance {invaded, it} is annotated in 
figure 2. In the left sentence?s tree, the node ?NP? 
for ?perhaps different groups? is terminated to 
provide a clue that we have a noun phrase at the 
object position of the candidate verb. 
It began when a .
PRP VBD WRB DT
?...
.
NP
WHADVP
SBAR
NP
VP
S
VP
S
?...
TOP
S
.
.
groupsdifferentperhaps,invadedbutwasThis ?...
DT
NP VP
VBD NNSJJRB,VBNCC
NP
S
VP
VP PP NP
ADVP
by
IN
 
Figure 1: Minimum-Expansion Tree 
It began when a .
PRP VBD WRB DT
?...
.
NP
WHADVP
SBAR
NP
VP
S
VP
S
?...
TOP
S
.
.
groupsdifferentperhaps,invadedbutwasThis ?...
DT
NP VP
VBD NNSJJRB,VBNCC
NP
S
VP
VP PP NP
ADVP
by
IN
 
Figure 2: Simple Expansion Tree 
It began when a .
PRP VBD WRB DT
?...
.
NP
WHADVP
SBAR
NP
VP
S
VP
S
?...
TOP
S
.
.
groupsdifferentperhaps,invadedbutwasThis ?...
DT
NP VP
VBD NNSJJRB,VBNCC
NP
S
VP
VP PP NP
ADVP
by
IN
 
Figure 3: Full-Expansion Tree 
? Full Expansion Tree 
This feature focuses on the whole tree structure 
between the candidate and pronoun. It not only 
includes all the nodes in Simple-Expansion, but 
also the nodes (beneath the nearest commanding 
parent) that cover the words between the candi-
                                                 
2 If the pronoun and the candidate are not in the same sen-
tence, we will not include the nodes denoting the sentences 
before the candidate or after the pronoun. 
192
date and the pronoun3. Such a feature keeps the 
most information related to the pronoun and can-
didate pair. Figure 3 shows the structure for fea-
ture full-expansion for instance {invaded, it}. As 
illustrated, the ?NP? node for ?perhaps different 
groups? is further expanded to the POS level. All 
its child nodes are included in the full-expansion 
tree except the surface words. 
3.2 Convolution Parse Tree Kernel and Com-
posite Kernel 
To calculate the similarity between two struc-
tured features, we use the convolution tree kernel 
that is defined by Collins and Duffy (2002) and 
Moschitti (2004). Given two trees, the kernel 
will enumerate all their sub-trees and use the 
number of common sub-trees as the measure of 
similarity between two trees. The above tree ker-
nel only aims for the structured feature. We also 
need a composite kernel to combine the struc-
tured feature and the flat features from section 
2.2. In our study we define the composite kernel 
as follows: 
             
            
              
 
            
              
 
where       is the convolution tree kernel de-
fined for the structured feature, and       is the 
kernel applied on the flat features. Both kernels 
are divided by their respective length4 for norma-
lization. The new composite kernel      , de-
fined as the sum of normalized       and      , 
will return a value close to 1 only if both the 
structured features and the flat features have high 
similarity under their respective kernels. 
3.3 Twin-Candidate Framework using Rank-
ing SVM Model 
In a ranking SVM kernel as described in (Mo-
schitti et al 2006) for Semantic Role Labeling, 
two argument annotations (as argument trees) are 
presented to the ranking SVM model to decide 
which one is better.  In our case, we present two 
syntactic trees from two candidates to the rank-
ing SVM model. The idea is inspired by (Yang, 
et.al, 2005b;2008). The intuition behind the 
twin-candidate model is to capture the informa-
tion of how much one candidate is more pre-
                                                 
3 We will not expand the nodes denoting the sentences other 
than where the pronoun and the candidate occur. 
4  The length of a kernel   is defined as            
                   
ferred than another. The candidate wins most of 
the pair wise comparisons is selected as antece-
dent. 
The feature vector for each training instance 
has a form of                    . An in-
stance is positive if       is a better antecedent 
choice than       . Otherwise, it is a negative 
instance. For each feature vector, both tree struc-
tural features and flat features are used.  Thus 
each feature vector has a form of    
              where    and    are trees of candi-
date i and j respectively,    and    are flat feature 
vectors of candidate i and j respectively.  
In the training instances generation, we only 
generate those instances with one candidate is 
the correct antecedent. This follows the same 
strategy used in (Yang et al 2008) for object 
anaphora resolution. 
In the resolution process, a list of m candi-
dates is extracted from a three sentences window. 
A total of  
 
 
  instances are generated by pairing-
up the m candidates pair-wisely. We used a 
Round-Robin scoring scheme for antecedent se-
lection. Suppose a SVM output for an instance 
                   is 1, we will give a score 
1 for        and -1 for        and vice versa. At 
last, the candidate with the highest score is se-
lected as antecedent. In order to handle a non-
event anaphoric pronoun, we have set a threshold 
to distinguish event anaphoric from non-event 
anaphoric. A pronoun is considered as event 
anaphoric if its score is above the threshold. In 
our experiments, we kept a set of development 
data to find out the threshold in an empirical way. 
4 Experiments and Discussions 
4.1 Experimental Setup 
OntoNotes Release 2.0 English corpus as in 
(Hovy et al 2006) is used in our study, which 
contains 300k words of English newswire data 
(from the Wall Street Journal) and 200k words of 
English broadcast news data (from ABC, CNN, 
NBC, Public Radio International and Voice of 
America).  Table 1 shows the distribution of var-
ious entities. We focused on the resolution of 
502 event pronouns encountered in the corpus. 
The resolution system has to handle both the 
event pronoun identification and antecedent se-
lection tasks. To illustrate the difficulty of event 
pronoun resolution, 14.7% of all pronoun men-
tions are event anaphoric and only 31.5% of 
193
event pronoun can be resolved using ?most re-
cent verb? heuristics. Therefore a most-recent-
verb baseline will yield an f-score 4.63%. 
To conduct event pronoun resolution, an input 
raw text was preprocessed automatically by a 
pipeline of NLP components. The noun phrase 
identification and the predicate-argument extrac-
tion were done based on Stanford Parser (Klein 
and Manning, 2003a;b) with F-score of 86.32% 
on Penn Treebank corpus.  
Non-Event Anaphora:        4952   80.03% 
Event  
Anaphora: 
1235  
19.97% 
Event NP:        733   59.35% 
Event  
Pronoun: 
502   40.65% 
It:       29.0% 
This:   16.9% 
That:  54.1% 
Table 1: The distribution of various types of 6187 
anaphora in OntoNotes 2.0 
For each pronoun encountered during resolu-
tion, all the inflectional verbs within the current 
and previous two sentences are taken as candi-
dates. For the current sentence, we take only 
those verbs in front of the pronoun. On average, 
each event pronoun has 6.93 candidates. Non-
event anaphoric pronouns will generate 7.3 nega-
tive instances on average.  
4.2 Experiment Results and Discussion 
In this section, we will present our experimental 
results with discussions. The performance meas-
ures we used are precision, recall and F-score. 
All the experiments are done with a 10-folds 
cross validation. In each fold of experiments, the 
whole corpus is divided into 10 equal sized por-
tions. One of them is selected as testing corpus 
while the remaining 9 are used for training. In 
experiments with development data, 1 of the 9 
training portions is kept for development purpose. 
In case of statistical significance test for differ-
ences is needed, a two-tailed, paired-sample Stu-
dent?s t-Test is performed at 0.05 level of signi-
ficance. 
In the first set of experiments, we are aiming 
to investigate the effectiveness of each single 
knowledge source. Table 2 reports the perfor-
mance of each individual experiment. The flat 
feature set yields a baseline system with 40.6% f-
score. By using each tree structure along, we can 
only achieve a performance of 44.4% f-score 
using the minimum-expansion tree. Therefore, 
we will further investigate the different ways of 
combining flat and syntactic structure knowledge 
to improve resolution performances. 
 Precision Recall F-score 
Flat 0.406 0.406 0.406 
Min-Exp 0.355 0.596 0.444 
Simple-Exp 0.347 0.512 0.414 
Full-Exp 0.323 0.476 0.385 
Table 2: Contribution from Single Knowledge Source 
The second set of experiments is conducted to 
verify the performances of various tree structures 
combined with flat features. The performances 
are reported in table 3. Each experiment is re-
ported with two performances. The upper one is 
done with default hyper-plane setting. The lower 
one is done using the hyper-plane adjustment as 
we discussed in section 2.3. 
 Precision Recall F-score 
Min-Exp + 
Flat 
0.433 0.512 0.469 
(0.727) (0.446) (0.553) 
Simple-Exp 
+Flat 
0.423 0.534 0.472 
(0.652) (0.492) (0.561) 
Full-Exp + 
Flat 
0.416 0.526 0.465 
(0.638) (0.496) (0.558) 
Table 3: Comparison of Different Tree Structure +Flat 
As table 3 shows, minimum-expansion gives 
highest precision in both experiment settings. 
Minimum-expansion emphasizes syntactic struc-
tures linking the anaphor and antecedent. Al-
though using only the syntactic path may lose the 
contextual information, but it also prune out the 
potential noise within the contextual structures. 
In contrast, the full-expansion gives the highest 
recall. This is probably due to the widest know-
ledge coverage provides by the full-expansion 
syntactic tree. As a trade-off, the precision of 
full-expansion is the lowest in the experiments. 
One reason for this may be due to OntoNotes 
corpus is from broadcasting news domain. Its 
texts are less-formally structured. Another type 
of noise is that a narrator of news may read an 
abnormally long sentence. It should appear as 
several separate sentences in a news article. 
However, in broadcasting news, these sentences 
maybe simply joined by conjunction word ?and?. 
Thus a very nasty and noisy structure is created 
from it. Comparing the three knowledge source, 
simple-expansion achieves moderate precision 
and recall which results in the highest f-score. 
From this, we can draw a conclusion that simple-
expansion achieves a balance between the indica-
tive structural information and introduced noises. 
In the next set of experiments, we will com-
pare different setting for training instances gen-
eration. A typical setting contains no negative 
194
instances generated from non-event anaphoric 
pronoun. This is not an issue for object pronoun 
resolution as majority of pronouns in an article is 
anaphoric. However in our case, the event pro-
noun consists of only 14.7% of the total pro-
nouns in OntoNotes. Thus we incorporate the 
instances from non-event pronouns to improve 
the precision of the classifier. However, if we 
include all the negative instances from non-event 
anaphoric pronouns, the positive instances will 
be overwhelmed by the negative instances. A 
down sampling is applied to the training in-
stances to create a more balanced class distribu-
tion. Table 4 reports various training settings 
using simple-expansion tree structure.  
Simple-Exp Tree Precision Recall F-score 
Without Non-
event Negative 
0.423 0.534 0.472 
Incl. All Negative 0.733 0.410 0.526 
Balanced Negative 0.599 0.506 0.549 
Development Data 0.652 0.492 0.561 
Table 4: Comparison of Training Setup, Simple-Exp 
In table 4, the first line is experiment without 
any negative instances from non-event pronouns. 
The second line is the performance with all nega-
tive instances from non-event pronouns. Third 
line is performance using a balanced training set 
using down sampling. The last line is experiment 
using hyper-plane adjustment. The first line 
gives the highest recall measure because it has no 
discriminative knowledge on non-event anaphor-
ic pronoun. The second line yields the highest 
precision which complies with our claim that 
including negative instances from non-event 
pronouns will improve precision of the classifier 
because more discriminative power is given by 
non-event pronoun instances. The balanced train-
ing set achieves a better f-score comparing to 
models with no/all negative instances. This is 
because balanced training set provides a better 
weighted positive/negative instances which im-
plies a balanced positive/negative knowledge 
representation. As a result of that, we achieve a 
better balanced f-score. In (Ng and Cardie, 
2002b), they concluded that only the negative 
instances in between the anaphor and antecedent 
are useful in the resolution. It is same as our 
strategy without negative instances from non-
event anaphoric pronouns. However, our study 
showed an improvement by adding in negative 
instances from non-event anaphoric pronouns as 
showed in table 4. This is probably due to our 
random sampling strategy over the negative in-
stances near to the event anaphoric instances. It 
empowers the system with more discriminative 
power. The best performance is given by the hy-
per-plane adaptation model. Although the num-
ber of training instances is further reduced for 
development data, we can have an adjustment of 
the hyper-plane which is more fit to dataset.  
In the last set of experiments, we will present 
the performance from the twin-candidates based 
approach in table 5. The first line is the best per-
formance from single candidate system with hy-
per-plane adaptation. The second line is perfor-
mance using the twin-candidates approach. 
Simple-Exp Tree Precision Recall F-score 
Single Candidate 0.652 0.492 0.561 
Twin-Candidates 0.626 0.540 0.579 
Table 5: Single vs. Twin Candidates, Simple-Exp 
Comparing to the single candidate model, the 
recall is significantly improved with a small 
trade-off in precision. The difference in results is 
statistically significant using t-test at 5% level of 
significance. It reinforced our intuition that pre-
ferences between two candidates are contributive 
information sources in co-reference resolution.  
5 Conclusion and Future Work 
The purpose of this paper is to conduct a syste-
matic study of the event pronoun resolution. We 
propose a resolution system utilizing a set of flat 
positional, lexical and syntactic feature and 
structural syntactic feature. The state-of-arts 
convolution tree kernel is used to extract indica-
tive structural syntactic knowledge. A twin-
candidates preference learning based approach is 
incorporated to reinforce the resolution system 
with candidates? preferences knowledge. Last but 
not least, we also proposed a study of the various 
incorporations of negative training instances, 
specially using random sampling to handle the 
imbalanced data. Development data is also used 
to select more accurate hyper-plane in SVM for 
better determination. 
To further our research work, we plan to em-
ploy more semantic information into the system 
such as semantic role labels and verb frames.  
Acknowledgment 
We would like to thank Professor Massimo Poesio 
from University of Trento for the initial discussion of 
this work. 
195
References  
N. Asher. 1993. Reference to Abstract Objects in Dis-
course. Kluwer Academic Publisher. 1993. 
V. Vapnik. 1995. The Nature of Statistical Learning 
Theory. Springer.1995. 
M. Kubat and S. Matwin, 1997. Addressing the curse 
of imbalanced data set: One sided sampling. In 
Proceedings of the Fourteenth International Con-
ference on Machine Learning,1997. pg179?186. 
T. Joachims. 1999. Making large-scale svm learning 
practical. In Advances in Kernel Methods - Sup-
port Vector Learning. MIT Press.1999. 
W. Soon, H. Ng, and D. Lim. 2001. A machine learn-
ing approach to coreference resolution of noun 
phrases. In Computational Linguistics, Vol:27(4), 
pg521? 544. 
D. Byron. 2002. Resolving Pronominal Reference to 
Abstract Entities, in Proceedings of the 40th An-
nual Meeting of the Association for Computational 
Linguistics (ACL?02). July 2002. , USA  
M. Collins and N. Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL?02). July 
2002. , USA 
V. Ng and C. Cardie. 2002a. Improving machine 
learning approaches to coreference resolution. In 
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?02). 
July 2002. , USA. pg104?111. 
V. Ng, and C. Cardie. 2002b. Identifying anaphoric 
and non-anaphoric noun phrases to improve core-
ference resolution. In Proceedings of the 19th In-
ternational Conference on Computational Linguis-
tics (COLING02). (2002) 
M. Strube and C. M?ller. 2003. A Machine Learning 
Approach to Pronoun Resolution in Spoken Dialo-
gue. . In Proceedings of the 41st Annual Meeting of 
the Association for Computational Linguistics 
(ACL?03), 2003 
D. Klein and C. Manning. 2003a. Fast Exact Infe-
rence with a Factored Model for Natural Language 
Parsing. In Advances in Neural Information 
Processing Systems 15 (NIPS 2002), Cambridge, 
MA: MIT Press, pp. 3-10. 
D. Klein and C.Manning. 2003b. Accurate Unlexica-
lized Parsing. In Proceedings of the 41st Annual 
Meeting of the Association for Computational Lin-
guistics (ACL?03), 2003.  pg423-430. 
X. Yang, G. Zhou, J. Su, and C.Tan. 2003. Corefe-
rence Resolution Using Competition Learning Ap-
proach. In Proceedings of the 41st Annual Meeting 
of the Association for Computational Linguistics 
(ACL?03), 2003. pg176?183. 
A. Moschitti. 2004. A study on convolution kernels 
for shallow semantic parsing. In Proceedings of 
the 42nd Annual Meeting of the Association for 
Computational Linguistics (ACL?04), pg335?342. 
A. Estabrooks, T. Jo, and N. Japkowicz. 2004. A mul-
tiple resampling method for learning from imba-
lanced data sets. In Computational Intelligence  
Vol:20(1). pg18?36. 
X. Yang, J. Su, G. Zhou, and C. Tan. 2004. Improving 
pronoun resolution by incorporating coreferential 
information of candidates. In Proceedings of 42th 
Annual Meeting of the Association for Computa-
tional Linguistics, 2004. pg127?134. 
X. Yang, J. Su and C.Tan. 2005a. Improving Pronoun 
Resolution Using Statistics-Based Semantic Com-
patibility Information. In Proceedings of Proceed-
ings of the 43rd Annual Meeting of the Association 
for Computational Linguistics (ACL?05). June 
2005.  
X. Yang, J. Su and C.Tan. 2005b. A Twin-Candidates 
Model for Coreference Resolution with Non-
Anaphoric Identification Capability. In Proceed-
ings of IJCNLP-2005. Pp. 719-730, 2005 
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. 
Weischedel. 2006. OntoNotes: The 90\% Solution. 
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, 2006 
X. Yang, J. Su and C.Tan. 2006. Kernel-Based Pro-
noun Resolution with Structured Syntactic Know-
ledge. In Proceedings of the 44th Annual Meeting 
of the Association for Computational Linguistics 
(ACL?06). July 2006. Australia. 
A. Moschitti, Making tree kernels practical for natural 
language learning. In Proceedings EACL 2006, 
Trento, Italy, 2006. 
C. M?ller. 2007. Resolving it, this, and that in unre-
stricted multi-party dialog. In Proceedings of the 
45th Annual Meeting of the Association for Com-
putational Linguistics (ACL?07). 2007.  Czech Re-
public. pg816?823. 
X. Yang, J. Su and C.Tan. 2008. A Twin-Candidates 
Model for Learning-Based Coreference Resolution. 
In Computational Linguistics, Vol:34(3). pg327-
356. 
S. Pradhan, L. Ramshaw, R. Weischedel, J. Mac-
Bride, and L. Micciulla. 2007. Unrestricted Corefe-
rence: Identifying Entities and Events in Onto-
Notes. In Proceedings of the IEEE International 
Conference on Semantic Computing (ICSC), Sep. 
2007. 
196
