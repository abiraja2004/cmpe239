Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 33?40,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
A note on contextual binary feature grammars
Alexander Clark
Department of Computer Science
Royal Holloway, University of London
alexc@cs.rhul.ac.uk
Re?mi Eyraud and Amaury Habrard
Laboratoire d?Informatique Fondamentale
de Marseille, CNRS,
Aix-Marseille Universite?, France
remi.eyraud,amaury.habrard@lif.univ-mrs.fr
Abstract
Contextual Binary Feature Grammars
were recently proposed by (Clark et al,
2008) as a learnable representation for
richly structured context-free and con-
text sensitive languages. In this pa-
per we examine the representational
power of the formalism, its relationship
to other standard formalisms and lan-
guage classes, and its appropriateness
for modelling natural language.
1 Introduction
An important issue that concerns both natu-
ral language processing and machine learning
is the ability to learn suitable structures of a
language from a finite sample. There are two
major points that have to be taken into ac-
count in order to define a learning method use-
ful for the two fields: first the method should
rely on intrinsic properties of the language it-
self, rather than syntactic properties of the
representation. Secondly, it must be possible
to associate some semantics to the structural
elements in a natural way.
Grammatical inference is clearly an impor-
tant technology for NLP as it will provide a
foundation for theoretically well-founded un-
supervised learning of syntax, and thus avoid
the annotation bottleneck and the limitations
of working with small hand-labelled treebanks.
Recent advances in context-free grammati-
cal inference have established that there are
large learnable classes of context-free lan-
guages. In this paper, we focus on the ba-
sic representation used by the recent approach
proposed in (Clark et al, 2008). The authors
consider a formalism called Contextual Binary
Feature Grammars (CBFG) which defines a
class of grammars using contexts as features
instead of classical non terminals. The use of
features is interesting from an NLP point of
view because we can associate some semantics
to them, and because we can represent com-
plex, structured syntactic categories. The no-
tion of contexts is relevant from a grammatical
inference standpoint since they are easily ob-
servable from a finite sample. In this paper
we establish some basic language theoretic re-
sults about the class of exact Contextual Bi-
nary Feature Grammars (defined in Section 3),
in particular their relationship to the Chomsky
hierarchy: exact CBFGs are those where the
contextual features are associated to all the
possible strings that can appear in the corre-
sponding contexts of the language defined by
the grammar.
The main results of this paper are proofs
that the class of exact CBFGs:
? properly includes the regular languages
(Section 5),
? does not include some context-free lan-
guages (Section 6),
? and does include some non context-free
languages (Section 7).
Thus, this class of exact CBFGs is orthog-
onal to the classic Chomsky hierarchy but
can represent a very large class of languages.
Moreover, it has been shown that this class
is efficiently learnable. This class is therefore
an interesting candidate for modeling natural
language and deserves further investigation.
2 Basic Notation
We consider a finite alphabet ?, and ?? the
free monoid generated by ?. ? is the empty
string, and a language is a subset of ??. We
will write the concatenation of u and v as uv,
and similarly for sets of strings. u ? ?? is a
substring of v ? ?? if there are strings l, r ? ??
such that v = lur.
33
A context is an element of ?? ? ??. For a
string u and a context f = (l, r) we write f 
u = lur; the insertion or wrapping operation.
We extend this to sets of strings and contexts
in the natural way. A context is also known in
structuralist linguistics as an environment.
The set of contexts, or distribution, of a
string u of a language L is, CL(u) = {(l, r) ?
?? ? ??|lur ? L}. We will often drop the
subscript where there is no ambiguity. We
define the syntactic congruence as u ?L v iff
CL(u) = CL(v). The equivalence classes un-
der this relation are the congruence classes of
the language. In general we will assume that
? is not a member of any language.
3 Contextual Binary Feature
Grammars
Most definitions and lemmas of this section
were first introduced in (Clark et al, 2008).
3.1 Definition
Before the presentation of the formalism, we
give some results about contexts to help to
give an intuition of the representation. The
basic insight behind CBFGs is that there is a
relation between the contexts of a string w and
the contexts of its substrings. This is given by
the following trivial lemma:
Lemma 1. For any language L and for any
strings u, u?, v, v? if C(u) = C(u?) and C(v) =
C(v?), then C(uv) = C(u?v?).
We can also consider a slightly stronger result:
Lemma 2. For any language L and for any
strings u, u?, v, v? if C(u) ? C(u?) and C(v) ?
C(v?), then C(uv) ? C(u?v?).
C(u) ? C(u?) means that we can replace
any occurrence of u in a sentence, with a u?,
without affecting the grammaticality, but not
necessarily vice versa. Note that none of these
strings need to correspond to non-terminals:
this is valid for any fragment of a sentence.
We will give a simplified example from En-
glish syntax: the pronoun it can occur every-
where that the pronoun him can, but not vice
versa1. Thus given a sentence ?I gave him
away?, we can substitute it for him, to get the
1This example does not account for a number of syn-
tactic and semantic phenomena, particularly the distri-
bution of reflexive anaphors.
grammatical sentence I gave it away, but we
cannot reverse the process. For example, given
the sentence it is raining, we cannot substi-
tute him for it, as we will get the ungrammat-
ical sentence him is raining. Thus we observe
C(him) ( C(it).
Looking at Lemma 2 we can also say that,
if we have some finite set of strings K, where
we know the contexts, then:
Corollary 1.
C(w) ?
?
u?,v?:
u?v?=w
?
u?K:
C(u)?C(u?)
?
v?K:
C(v)?C(v?)
C(uv)
This is the basis of the representation: a
word w is characterised by its set of contexts.
We can compute the representation of w, from
the representation of its parts u?, v?, by looking
at all of the other matching strings u and v
where we understand how they combine (with
subset inclusion). In order to illustrate this
concept, we give here a simple example.
Consider the language {anbn|n > 0} and
the set K = {aabb, ab, abb, aab, a, b}. Suppose
we want to compute the set of contexts of
aaabbb, Since C(abb) ? C(aabbb), and vacu-
ously C(a) ? C(a), we know that C(aabb) ?
C(aaabbb). More generally, the contexts of ab
can represent anbn, those of aab the strings
an+1bn and the ones of abb the strings anbn+1.
The key relationships are given by context
set inclusion. Contextual binary feature gram-
mars allow a proper definition of the combina-
tion of context inclusion:
Definition 1. A Contextual Binary Feature
Grammar (CBFG) G is a tuple ?F, P, PL,??.
F is a finite set of contexts, called features,
where we write C = 2F for the power set of F
defining the categories of the grammar, P ?
C ? C ? C is a finite set of productions that
we write x ? yz where x, y, z ? C and PL ?
C ? ? is a set of lexical rules, written x ? a.
Normally PL contains exactly one production
for each letter in the alphabet (the lexicon).
A CBFG G defines recursively a map fG
34
from ?? ? C as follows:
fG(?) = ? (1)
fG(w) =
?
(c?w)?PL
c iff |w| = 1
(2)
fG(w) =
?
u,v:uv=w
?
x?yz?P :
y?fG(u)?
z?fG(v)
x iff |w| > 1.
(3)
We give here more explanation about the
map fG. It defines in fact the analysis of a
string by a CBFG. A rule z ? xy is applied
to analyse a string w if there is a cut uv = w
s.t. x ? fG(u) and y ? fG(v), recall that x
and y are sets of contexts. Intuitively, the re-
lation given by the production rule is linked
with Lemma 2: z is included in the set of fea-
tures of w = uv. From this relationship, for
any (l, r) ? z we have lwr ? L(G).
The complete computation of fG is then jus-
tified by Corollary 1: fG(w) defines all the
possible features associated by G to w with all
the possible cuts uv = w (i.e. all the possible
derivations).
Finally, the natural way to define the mem-
bership of a string w in L(G) is to have the
context (?, ?) ? fG(w) which implies that
?u? = u ? L(G).
Definition 2. The language defined by a
CBFG G is the set of all strings that are as-
signed the empty context: L(G) = {u|(?, ?) ?
fG(u)}.
As we saw before, we are interested in cases
where there is a correspondence between the
language theoretic interpretation of a context,
and the occurrence of that context as a feature
in the grammar. From the basic definition of
a CBFG, we do not require any specific con-
dition on the features of the grammar, except
that a feature is associated to a string if the
string appears in the context defined by the
feature. However, we can also require that fG
defines exactly all the possible features that
can be associated to a given string according
to the underlying language.
Definition 3. Given a finite set of contexts
F = {(l1, r1), . . . , (ln, rn)} and a language L
we can define the context feature map FL :
?? ? 2F which is just the map u 7? {(l, r) ?
F |lur ? L} = CL(u) ? F .
Using this definition, we now need a cor-
respondence between the language theoretic
context feature map FL and the representa-
tion in the CBFG fG.
Definition 4. A CBFG G is exact if for all
u ? ??, fG(u) = FL(G)(u).
Exact CBFGs are a more limited formalism
than CBFGs themselves; without any limits
on the interpretation of the features, we can
define a class of formalisms that is equal to
the class of Conjunctive Grammars (see Sec-
tion 4). However, exactness is an important
notion because it allows to associate intrinsic
components of a language to strings. Contexts
are easily observable from a sample and more-
over it is only when the features correspond to
the contexts that distributional learning algo-
rithms can infer the structure of the language.
A basic example of such a learning algorithm
is given in (Clark et al, 2008).
3.2 A Parsing Example
To clarify the relationship with CFG
parsing, we will give a simple worked
example. Consider the CBFG G =
?{(?, ?), (aab, ?), (?, b), (?, abb), (a, ?)(aab, ?)},
P, PL, {a, b}? with PL =
{{(?, b), (?, abb)} ? a, {(a, ?), (aab, ?)} ? b}
and P =
{{(?, ?)} ? {(?, b)}{(aab, ?)},
{(?, ?)} ? {(?, abb)}{(a, ?)},
{(?, b)} ? {(?, abb)}{(?, ?)},
{(a, ?)} ? {(?, ?)}{(aab, ?)}}.
If we want to parse the string w = aabb the
usual way is to have a bottom-up approach.
This means that we recursively compute the
fG map on the substrings of w in order to
check whether (?, ?) belongs to fG(w).
The Figure 1 graphically gives the main
steps of the computation of fG(aabb). Ba-
sically there are two ways to split aabb that
allow the derivation of the empty context:
aab|b and a|abb. The first one correspond
to the top part of the figure while the sec-
ond one is drawn at the bottom. We can
see for instance that the empty context be-
longs to fG(ab) thanks to the rule {(?, ?)} ?
{(?, abb)}{(a, ?)}: {(?, abb)} ? fG(a) and
{(a, ?)} ? fG(b). But for symmetrical reasons
35
the result can also be obtained using the rule
{(?, ?)} ? {(?, b)}{(aab, ?)}.
As we trivially have fG(aa) = fG(bb) = ?,
since no right-hand side contains the concate-
nation of the same two features, an induction
proof can be written to show that (?, ?) ?
fG(w) ? w ? {anbn : n > 0}.
a         a         b         bfG
{(?,b),(?,abb)} {(?,b),(?,abb)} {(a,?),(aab,?)} {(a,?),(aab,?)}
fG fG fG
Rule: (?,?) ? (?,b) (aab,?)
fG(ab)  ? {(?,?)}
Rule: (a,?) ? (?,?) (aab,?)
fG(abb)  ? {(a,?)}
Rule: (?,?) ? (?,abb) (a,?)
fG(aabb)  ? {(?,?)}
f G
{(?,b),(?,abb)} {(?,b),(?,abb)} {(a,?),(aab,?)} {(a,?),(aab,?)}
f G f G f G
Rule: (?,?) ? (?,abb) (a,?)
fG(ab)  ? {(?,?)}
Rule: (?,b) ? (?,abb) (?,?)
fG(aab)  ? {(?,b)}
Rule: (?,?) ? (?,b) (aab,?)
fG(aabb)  ? {(?,?)}
Figure 1: The two derivations to obtain (?, ?)
in fG(aabb) in the grammar G.
This is a simple example that illustrates
the parsing of a string given a CBFG. This
example does not characterize the power of
CBFG since no right handside part is com-
posed of more than one context. A more inter-
esting, example with a context-sensitive lan-
guage, will be presented in Section 7.
4 Non exact CBFGs
The aim here is to study the expressive power
of CBFG compare to other formalism recently
introduced. Though the inference can be done
only for exact CBFG, where features are di-
rectly linked with observable contexts, it is
still worth having a look at the more general
characteristics of CBFG. For instance, it is in-
teresting to note that several formalisms in-
troduced with the aim of representing natural
languages share strong links with CBFG.
Range Concatenation Grammars
Range Concatenation Grammars are a very
powerful formalism (Boullier, 2000), that is a
current area of research in NLP.
Lemma 3. For every CBFG G, there is
a non-erasing positive range concatenation
grammar of arity one, in 2-var form that de-
fines the same language.
Proof. Suppose G = ?F, P, PL,??. Define
a RCG with a set of predicates equal to F
and the following clauses, and the two vari-
ables U, V . For each production x ? yz in
P , for each f ? x, where y = {g1, . . . gi},
z = {h1, . . . hj} add clauses
f(UV ) ? g1(U), . . . gi(U), h1(V ), . . . hj(V ).
For each lexical production {f1 . . . fk} ? a
add clauses fi(a) ? . It is straightforward
to verify that f(w) `  iff f ? fG(w).
Conjunctive Grammar
A more exact correspondence is to the class of
Conjunctive Grammars (Okhotin, 2001), in-
vented independently of RCGs. For every ev-
ery language L generated by a conjunctive
grammar there is a CBFG representing L#
(where the special character # is not included
in the original alphabet).
Suppose we have a conjunctive grammar
G = ??, N, P, S? in binary normal form (as
defined in (Okhotin, 2003)). We construct the
equivalent CBFG G? = ?F, P ?, PL,?? as fol-
lowed:
? For every letter a we add a context (la, ra)
to F such that laara ? L;
? For every rules X ? a in P , we create a
rule {(la, ra)} ? a in PL.
? For every non terminal X ? N , for every
rule X ? P1Q1& . . .&PnQn we add dis-
tinct contexts {(lPiQi , rPiQi)} to F, such
that for all i it exists ui, lPiQiuirPiQi ? L
and PiQi
?
?G ui;
? Let FX,j = {(lPiQi , rPiQi) : ?i} the
set of contexts corresponding to the
jth rule applicable to X. For all
36
(lPiQi , rPiQi) ? FX,j , we add to P
? the
rules (lPiQi , rPiQi) ? FPi,kFQi,l (?k, l).
? We add a new context (w, ?) to F such
that S
?
?G w and (w, ?) ? # to PL;
? For all j, we add to P ? the rule (?, ?) ?
FS,j{(w, ?)}.
It can be shown that this construction gives
an equivalent CBFG.
5 Regular Languages
Any regular language can be defined by an ex-
act CBFG. In order to show this we will pro-
pose an approach defining a canonical form for
representing any regular language.
Suppose we have a regular language L, we
consider the left and right residual languages:
u?1L = {w|uw ? L} (4)
Lu?1 = {w|wu ? L} (5)
They define two congruencies: if l, l? ? u?1L
(resp. r, r? ? Lu?1) then for all w ? ??, lw ?
L iff l?w ? L (resp. wr ? L iff wr? ? L).
For any u ? ??, let lmin(u) be the lexico-
graphically shortest element such that l?1minL =
u?1L. The number of such lmin is finite by
the Myhil-Nerode theorem, we denote by Lmin
this set, i.e. {lmin(u)|u ? ??}. We de-
fine symmetrically Rmin for the right residuals
(Lr?1min = Lu
?1).
We define the set of contexts as:
F (L) = Lmin ?Rmin. (6)
F (L) is clearly finite by construction.
If we consider the regular language de-
fined by the deterministic finite automata
of Figure 2, we obtain Lmin = {?, a, b}
and Rmin = {?, b, ab} and thus F (L) =
{(?, ?), (a, ?), (b, ?), (?, b), (a, b), (b, b), (?, ab),
(a, ab), (b, ab)}.
By considering this set of features, we
can prove (using arguments about congruence
classes) that for any strings u, v such that
FL(u) ? FL(v), then CL(u) ? CL(v). This
means the set of feature F is sufficient to rep-
resent context inclusion, we call this property
the fiduciality.
Note that the number of congruence classes
of a regular language is finite. Each congru-
ence class is represented by a set of contexts
Figure 2: Example of a DFA. The left residuals
are defined by ??1L, a?1L, b?1L and the right
ones by L??1, Lb?1, Lab?1 (note here that
La?1 = L??1).
FL(u). Let KL be finite set of strings formed
by taking the lexicographically shortest string
from each congruence class. The final gram-
mar can be obtained by combining elements
of KL. For every pair of strings u, v ? KL, we
define a rule
FL(uv) ? FL(u), FL(v) (7)
and we add lexical productions of the form
FL(a) ? a, a ? ?.
Lemma 4. For all w ? ??, fG(w) = FL(w).
Proof. (Sketch) Proof in two steps: ?w ?
??, FL(w) ? fG(w) and fG(w) ? FL(w). Each
step is made by induction on the length of w
and uses the rules created to build the gram-
mar, the derivation process of a CBFG and
the fiduciality for the second step. The key
point rely on the fact that when a string w is
parsed by a CBFG G, there exists a cut of w
in uv = w (u, v ? ??) and a rule z ? xy in G
such that x ? fG(u) and y ? fG(v). The rule
z ? xy is also obtained from a substring from
the set used to build the grammar using the
FL map. By inductive hypothesis you obtain
inclusion between fG and FL on u and v.
For the language of Figure 2, the following
set is sufficient to build an exact CBGF:
{a, b, aa, ab, ba, aab, bb, bba} (this corresponds
to all the substrings of aab and bba). We have:
FL(a) = F (L)\{(?, ?), (a, ?)} ? a
FL(b) = F (L) ? b
FL(aa) = FL(a) ? FL(a), FL(a)
FL(ab) = F (L) ? FL(a), FL(b) = FL(a), F (L)
FL(ba) = F (L) ? FL(b), FL(a) = F (L), FL(a)
FL(bb) = F (L) ? FL(b), FL(b) = F (L), F (L)
37
FL(aab) = FL(bba) = FL(ab) = FL(ba)
The approach presented here gives a canon-
ical form for representing a regular language
by an exact CBFG. Moreover, this is is com-
plete in the sense that every context of every
substring will be represented by some element
of F (L): this CBFG will completely model the
relation between contexts and substrings.
6 Context-Free Languages
We now consider the relationship between
CFGs and CBFGs.
Definition 5. A context-free grammar (CFG)
is a quadruple G = (?, V, P, S). ? is a fi-
nite alphabet, V is a set of non terminals
(? ? V = ?), P ? V ? (V ? ?)+ is a finite
set of productions, S ? V is the start symbol.
In the following, we will suppose that a CFG
is represented in Chomsky Normal Form, i.e.
every production is in the form N ? UW with
N,U,W ? V or N ? a with a ? ?.
We will write uNv ?G u?v if there is a pro-
duction N ? ? ? P .
?
?G is the reflexive tran-
sitive closure of ?G. The language defined by
a CFG G is L(G) = {w ? ??|S
?
?G w}.
6.1 A Simple Characterization
A simple approach to try to represent a CFG
by a CBFG is to define a bijection between the
set of non terminals and the set of context fea-
tures. Informally we define each non terminal
by a single context and rewrite the productions
of the grammar in the CBFG form.
To build the set of contexts F , it is sufficient
to choose |V | contexts such that a bijection bC
can be defined between V and F with bC(N) =
(l, r) implies that S
?
? lNr. Note that we fix
bT (S) = (?, ?).
Then, we can define a CBFG
?F, P ?, P ?L,??, where P
? = {bT (N) ?
bT (U)bT (W )|N ? UW ? P} and
P ?L = {bT (N) ? a|N ? a ? P, a ? ?}.
A similar proof showing that this construction
produces an equivalent CBFG can be found
in (Clark et al, 2008).
If this approach allows a simple syntactical
convertion of a CFG into a CBFG, it is not
relevant from an NLP point of view. Though
we associate a non-terminal to a context, this
may not correspond to the intrinsic property
of the underlying language. A context could
be associated with many non-terminals and we
choose only one. For example, the context
(He is, ?) allows both noun phrases and ad-
jective phrases. In formal terms, the resulting
CBFG is not exact. Then, with the bijection
we introduced before, we are not able to char-
acterize the non-terminals by the contexts in
which they could appear. This is clearly what
we don?t want here and we are more interested
in the relationship with exact CBFG.
6.2 Not all CFLs have an exact CBFG
We will show here that the class of context-
free grammars is not strictly included in the
class of exact CBFGs. First, the grammar
defined in Section 3.2 is an exact CBFG for
the context-free and non regular language
{anbn|n > 0}, showing the class of exact
CBFG has some elements in the class of CFGs.
We give now a context-free language L that
can not be defined by an exact CBFG:
L = {anb|n > 0} ? {amcn|n > m > 0}.
Suppose that there exists an exact CBFG that
recognizes it and let N be the length of the
biggest feature (i.e. the longuest left part of
the feature). For any sufficiently large k >
N , the sequences ck and ck+1 share the same
features: FL(ck) = FL(ck+1). Since the CBFG
is exact we have FL(b) ? FL(ck). Thus any
derivation of ak+1b could be a derivation of
ak+1ck which does not belong to the language.
However, this restriction does not mean that
the class of exact CBFG is too restrictive for
modelling natural languages. Indeed, the ex-
ample we have given is highly unnatural and
such phenomena appear not to occur in at-
tested natural languages.
7 Context-Sensitive Languages
We now show that there are some exact
CBFGs that are not context-free. In particu-
lar, we define a language closely related to the
MIX language (consisting of strings with an
equal number of a?s, b?s and c?s in any order)
which is known to be non context-free, and
indeed is conjectured to be outside the class
of indexed grammars (Boullier, 2003).
38
Let M = {(a, b, c)?}, we consider the language
L = Labc?Lab?Lac?{a?a, b?b, c?c, dd?, ee?, ff ?}:
Lab = {wd|w ? M, |w|a = |w|b},
Lac = {we|w ? M, |w|a = |w|c},
Labc = {wf |w ? M, |w|a = |w|b = |w|c}.
In order to define a CBFG recognizing L, we
have to select features (contexts) that can rep-
resent exactly the intrinsic components of the
languages composing L. We propose to use the
following set of features for each sublanguages:
? For Lab: (?, d) and (?, ad), (?, bd).
? For Lac: (?, e) and (?, ae), (?, ce).
? For Labc: (?, f).
? For the letters a?, b?, c?, a, b, c we add:
(?, a), (?, b), (?, c), (a?, ?), (b?, ?), (c?, ?).
? For the letters d, e, f, d?, e?, f ? we add;
(?, d?), (?, e?), (?, f ?), (d, ?), (e, ?), (f, ?).
Here, Lab will be represented by (?, d), but we
will use (?, ad), (?, bd) to define the internal
derivations of elements of Lab. The same idea
holds for Lac with (?, e) and (?, ae), (?, ce).
For the lexical rules and in order to have an
exact CBFG, note the special case for a, b, c:
{(?, bd), (?, ce), (a?, ?)} ? a
{(?, ad), (b?, ?)} ? b
{(?, ad), (?, ae), (c?, ?)} ? c
For the nine other letters, each one is defined
with only one context like {(?, d?)} ? d.
For the production rules, the most impor-
tant one is: (?, ?) ? {(?, d), (?, e)}, {(?, f ?)}.
Indeed, this rule, with the presence of two
contexts in one of categories, means that an
element of the language has to be derived
so that it has a prefix u such that fG(u) ?
{(?, d), (?, e)}. This means u is both an ele-
ment of Lab and Lac. This rule represents the
language Labc since {(?, f ?)} can only repre-
sent the letter f .
The other parts of the language will be
defined by the following rules:
(?, ?) ? {(?, d)}, {(?, d?)},
(?, ?) ? {(?, e)}, {(?, e?)},
(?, ?) ? {(?, a)}, {(?, bd), (?, ce), (a?, ?)},
(?, ?) ? {(?, b)}, {(?, ad), (b?, ?)},
(?, ?) ? {(?, c)}, {(?, ad), (?, ae), (c?, ?)},
(?, ?) ? {(?, d?)}, {(d, ?)},
(?, ?) ? {(?, e?)}, {(e, ?)},
(?, ?) ? {(?, f ?)}, {(f, ?)}.
This set of rules is incomplete, since for rep-
resenting Lab, the grammar must contain the
rules ensuring to have the same number of a?s
and b?s, and similarly for Lac. To lighten the
presentation here, the complete grammar is
presented in Annex.
We claim this is an exact CBFG for a
context-sensitive language. L is not context-
free since if we intersect L with the regular
language {??d}, we get an instance of the
non context-free MIX language (with d ap-
pended). The exactness comes from the fact
that we chose the contexts in order to ensure
that strings belonging to a sublanguage can
not belong to another one and that the deriva-
tion of a substring will provide all the possible
correct features with the help of the union of
all the possible derivations.
Note that the Mix language on its own is
probably not definable by an exact CBFG: it
is only when other parts of the language can
distributionally define the appropriate partial
structures that we can get context sensitive
languages. Far from being a limitation of this
formalism (a bug), we argue this is a feature:
it is only in rather exceptional circumstances
that we will get properly context sensitive lan-
guages. This formalism thus potentially ac-
counts not just for the existence of non context
free natural language but also for their rarity.
8 Conclusion
The chart in Figure 3 summarises the different
relationship shown in this paper. The substi-
tutable languages (Clark and Eyraud, 2007)
and the very simple ones (Yokomori, 2003)
form two different learnable class of languages.
There is an interesting relationship with Mar-
cus External Contextual Grammars (Mitrana,
2005): if we defined the language of a CBFG
to be the set {fG(u)  u : u ? ??} we would
be taking some steps towards contextual gram-
mars.
In this paper we have discussed the weak
generative power of Exact Contextual Binary
Feature Grammars; we conjecture that the
class of natural language stringsets lie in this
class. ECBFGs are efficiently learnable (see
(Clark et al, 2008) for details) which is a com-
39
Context-free                                                   
Regular                        
Context sensitive
very simple
substi-tutable
Range Concatenation                         
                          Conjunctive = CBFG                                Exact CBFG
                   
Figure 3: The relationship between CBFG and
other classes of languages.
pelling technical advantage of this formalism
over other more traditional formalisms such as
CFGs or TAGs.
References
Pierre Boullier. 2000. A Cubic Time Extension
of Context-Free Grammars. Grammars, 3:111?
131.
Pierre Boullier. 2003. Counting with range con-
catenation grammars. Theoretical Computer
Science, 293(2):391?416.
Alexander Clark and Re?mi Eyraud. 2007. Polyno-
mial identification in the limit of substitutable
context-free languages. Journal of Machine
Learning Research, 8:1725?1745, Aug.
Alexander Clark, Re?mi Eyraud, and Amaury
Habrard. 2008. A polynomial algorithm for the
inference of context free languages. In Proceed-
ings of International Colloquium on Grammati-
cal Inference, pages 29?42. Springer, September.
V. Mitrana. 2005. Marcus external contextual
grammars: From one to many dimensions. Fun-
damenta Informaticae, 54:307?316.
Alexander Okhotin. 2001. Conjunctive grammars.
J. Autom. Lang. Comb., 6(4):519?535.
Alexander Okhotin. 2003. An overview of con-
junctive grammars. Formal Language Theory
Column, bulletin of the EATCS, 79:145?163.
Takashi Yokomori. 2003. Polynomial-time iden-
tification of very simple grammars from pos-
itive data. Theoretical Computer Science,
298(1):179?206.
Annex
(?, ?) ? {(?, d), (?, e)}, {(?, f ?)}
(?, ?) ? {(?, d)}, {(?, d?)}
(?, ?) ? {(?, e)}, {(?, e?)}
(?, ?) ? {(?, a)}, {(?, bd), (?, ce), (a?, ?)}
(?, ?) ? {(?, b)}, {(?, ad), (b?, ?)}
(?, ?) ? {(?, c)}, {(?, ad), (?, ae), (c?, ?)}
(?, ?) ? {(?, d?)}, {(d, ?)}
(?, ?) ? {(?, e?)}, {(e, ?)}
(?, ?) ? {(?, f ?)}, {(f, ?)}
(?, d) ? {(?, d)}, {(?, d)}
(?, d) ? {(?, ad)}, {(?, bd)}
(?, d) ? {(?, bd)}, {(?, ad)}
(?, d) ? {(?, d)}, {(?, ad), (?, ae), (c?, ?)}
(?, d) ? {(?, ad), (?, ae), (c?, ?)}, {(?, d)}
(?, ad) ? {(?, ad), (?, ae), (c?, ?)}, {(?, ad)}
(?, ad) ? {(?, ad)}, {(?, ad), (?, ae), (c?, ?)}
(?, ad) ? {(?, ad), (b?, ?)}, {(?, d)}
(?, ad) ? {(?, d)}, {(?, ad), (b?, ?)}
(?, bd) ? {(?, ad), (?, ae), (c?, ?)}, {(?, bd)}
(?, bd) ? {(?, bd)}, {(?, ad), (?, ae), (c?, ?)}
(?, bd) ? {(?, bd), (?, ce), (a?, ?)}, {(?, d)}
(?, bd) ? {(?, d)}, {(?, bd), (?, ce), (a?, ?)}
(?, e) ? {(?, e)}, {(?, e)}
(?, e) ? {(?, ae)}, {(?, ce)}
(?, e) ? {(?, ce)}, {(?, ae)}
(?, e) ? {(?, e)}, {(?, ad), (b?, ?)}
(?, e) ? {(?, ad), (b?, ?)}, {(?, e)}
(?, ae) ? {(?, ad), (b?, ?)}, {(?, ae)}
(?, ae) ? {(?, ae)}, {(?, ad), (b?, ?)}
(?, ae) ? {(?, ad), (?, ae), (c?, ?)}, {(?, e)}
(?, ae) ? {(?, e)}, {(?, ad), (?, ae), (c?, ?)}
(?, ce) ? {(?, ad), (b?, ?)}, {(?, ce)}
(?, ce) ? {(?, ce)}, {(?, ad), (b?, ?)}
(?, ce) ? {(?, bd), (?, ce), (a?, ?)}, {(?, e)}
(?, ce) ? {(?, e)}, {(?, bd), (?, ce), (a?, ?)}
{(?, bd), (?, ce), (a?, ?)} ? a
{(?, ad), (b?, ?)} ? b
{(?, ad), (?, ae), (c?, ?)} ? c
{(?, d?)} ? d
{(?, e?)} ? e
{(?, f ?)} ? f
{(?, a)} ? a?
{(?, b)} ? b?
{(?, c)} ? c?
{(d, ?)} ? d?
{(e, ?)} ? e?
{(f, ?)} ? f ?
40
