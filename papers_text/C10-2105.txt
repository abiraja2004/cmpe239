Coling 2010: Poster Volume, pages 910?918,
Beijing, August 2010
Opinion Summarization with Integer Linear Programming Formulation
for Sentence Extraction and Ordering
Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Matsuo and Genichiro Kikui
NTT Cyber Space Laboratories, NTT Corporation{ nishikawa.hitoshi, hasegawa.takaaki
matsuo.yoshihiro, kikui.genichiro
}
@lab.ntt.co.jp
Abstract
In this paper we propose a novel algorithm
for opinion summarization that takes ac-
count of content and coherence, simulta-
neously. We consider a summary as a se-
quence of sentences and directly acquire
the optimum sequence from multiple re-
view documents by extracting and order-
ing the sentences. We achieve this with a
novel Integer Linear Programming (ILP)
formulation. Our proposed formulation is
a powerful mixture of the Maximum Cov-
erage Problem and the Traveling Sales-
man Problem, and is widely applicable to
text generation and summarization tasks.
We score each candidate sequence accord-
ing to its content and coherence. Since
our research goal is to summarize reviews,
the content score is defined by opinions
and the coherence score is developed in
training against the review document cor-
pus. We evaluate our method using the
reviews of commodities and restaurants.
Our method outperforms existing opinion
summarizers as indicated by its ROUGE
score. We also report the results of human
readability experiments.
1 Introduction
The Web now holds a massive number of reviews
describing the opinions of customers about prod-
ucts and services. These reviews can help the cus-
tomer to reach purchasing decisions and guide the
business activities of companies such as product
improvement. It is, however, almost impossible to
read all reviews given their sheer number.
Automatic text summarization, particularly
opinion summarization, is expected to allow all
possible reviews to be efficiently utilized. Given
multiple review documents, our summarizer out-
puts text consisting of ordered sentences. A typ-
This restaurant offers customers a delicious menu and a
relaxing atmosphere. The staff are very friendly but the
price is a little high.
Table 1: A typical summary.
ical summary is shown in Table 1. This task is
considered as multidocument summarization.
Existing summarizers focus on organizing sen-
tences so as to include important information in
the given document into a summary under some
size limitation. A serious problem is that most of
these summarizers completely ignore coherence
of the summary, which improves reader?s compre-
hension as reported by Barzilay et al (2002).
To make summaries coherent, the extracted
sentences must be appropriately ordered. How-
ever, most summarization systems delink sentence
extraction from sentence ordering, so a sentence
can be extracted that can never be ordered natu-
rally with the other extracted sentences. More-
over, due to recent advances in decoding tech-
niques for text summarization, the summarizers
tend to select shorter sentences to optimize sum-
mary content. It aggravates this problem.
Although a preceding work tackles this prob-
lem by performing sentence extraction and order-
ing simultaneously (Nishikawa et al, 2010), they
adopt beam search and dynamic programming to
search for the optimal solution, so their proposed
method may fail to locate it.
To overcome this weakness, this paper proposes
a novel Integer Linear Programming (ILP) formu-
lation for searching for the optimal solution effi-
ciently. We formulate the multidocument sum-
marization task as an ILP problem that tries to
optimize the content and coherence of the sum-
mary by extracting and ordering sentences simul-
taneously. We apply our method to opinion sum-
marization and show that it outperforms state-of-
the-art opinion summarizers in terms of ROUGE
evaluations. Although in this paper we challenge
910
our method with opinion summarization, it can be
widely applied to other text generation and sum-
marization tasks.
This paper is organized as follows: Section 2
describes related work. Section 3 describes our
proposal. Section 4 reports our evaluation experi-
ments. We conclude this paper in Section 5.
2 Related Work
2.1 Sentence Extraction
Although a lot of summarization algorithms have
been proposed, most of them solely extract sen-
tences from a set of sentences in the source docu-
ment set. These methods perform extractive sum-
marization and can be formalized as follows:
S? = argmax
S?T
L(S) (1)
s.t. length(S) ? K
T stands for all sentences in the source docu-
ment set and S is an arbitrary subset of T . L(S)
is a function indicating the score of S as deter-
mined by one or more criteria. length(S) indi-
cates the length of S, K is the maximum size of
the summary. That is, most summarization algo-
rithms search for, or decode, the set of sentences S?
that maximizes function L under the given maxi-
mum size of the summary K. Thus most stud-
ies focus on the design of function L and efficient
search algorithms (i.e. argmax operation in Eq.1).
Objective Function
Many useful L functions have been proposed
including the cosine similarity of given sentences
(Carbonell and Goldstein, 1998) and centroid
(Radev et al, 2004); some approaches directly
learn function L from references (Kupiec et al,
1995; Hirao et al, 2002).
There are two approaches to defining the score
of the summary. One defines the weight on each
sentence forming the summary. The other defines
a weight for a sub-sentence, concept, that the sum-
mary contains.
McDonald (2007) and Martins and Smith
(2009) directly weight sentences and use MMR
to avoid redundancy (Carbonell and Goldstein,
1998). In contrast to their approaches, we set
weights on concepts, not sentences. Gillick
and Favre (2009) reported that the concept-based
model achieves better performance and scalability
than the sentence-based model when it is formu-
lated as ILP.
There is a wide range of choice with regard
to the unit of the concept. Concepts include
words and the relationship between named en-
tities (Filatova and Hatzivassiloglou, 2004), bi-
grams (Gillick and Favre, 2009), and word stems
(Takamura and Okumura, 2009).
Some summarization systems that target re-
views, opinion summarizers, extract particular
information, opinion, from the input sentences
and leverage them to select important sentences
(Carenini et al, 2006; Lerman et al, 2009). In
this paper, since we aim to summarize reviews,
the objective function is defined through opinion
as the concept that the reviews contain. We ex-
plain our detailed objective function in Section 3.
We describe features of above existing summariz-
ers in Section 4 and compare our method to them
as baselines.
Decoding Method
The algorithms proposed for argmax operation
include the greedy method (Filatova and Hatzivas-
siloglou, 2004), stack decoding (Yih et al, 2007;
Takamura and Okumura, 2009) and Integer Linear
Programming (Clarke and Lapata, 2007; McDon-
ald, 2007; Gillick and Favre, 2009; Martins and
Smith, 2009). Gillick and Favre (2009) and Taka-
mura and Okumura (2009) formulate summariza-
tion as a Maximum Coverage Problem. We also
use this formulation. While these methods focus
on extracting a set of sentences from the source
document set, our method performs extraction and
ordering simultaneously.
Some studies attempt to generate a single sen-
tence (i.e. headline) from the source document
(Banko et al, 2000; Deshpande et al, 2007).
While they extract and order words from the
source document as a unit, our model uses the unit
of sentences. This problem can be formulated as
the Traveling Salesman Problem and its variants.
Banko et al (2000) uses beam search to identify
approximate solutions. Deshpande et al (2007)
uses ILP and a randomized algorithm to find the
optimal solution.
2.2 Sentence Ordering
It is known that the readability of a collection of
sentences, a summary, can be greatly improved
by appropriately ordering them (Barzilay et al,
2002). Features proposed to create the appropri-
ate order include publication date of document
(Barzilay et al, 2002), content words (Lapata,
2003; Althaus et al, 2004), and syntactic role of
911
 
  
    
 
  
             	      	

     
     
Figure 1: Graph representation of summarization.
words (Barzilay and Lapata, 2005). Some ap-
proaches use machine learning to integrate these
features (Soricut and Marcu, 2006; Elsner et al,
2007). Generally speaking, these methods score
the discourse coherence of a fixed set of sentences.
These methods are separated from the extraction
step so they may fail if the set includes sentences
that are impossible to order naturally.
As mentioned above, there is a preceding work
that attempted to perform sentence extraction and
ordering simultaneously (Nishikawa et al, 2010).
Differences between this paper and that work are
as follows:
? This work adopts ILP solver as a decoder.
ILP solver allows the summarizer to search
for the optimal solution much more rapidly
than beam search (Deshpande et al, 2007),
which was adopted by the prior work. To
permit ILP solver incorporation, we propose
in this paper a totally new ILP formulation.
The formulation can be widely used for text
summarization and generation.
? Moreover, to learn better discourse coher-
ence, we adopt the Passive-Aggressive al-
gorithm (Crammer et al, 2006) and use
Kendall?s tau (Lapata, 2006) as the loss func-
tion. In contrast, the above work adopts Av-
eraged Perceptron (Collins, 2002) and has no
explicit loss function.
These advances make this work very different
from that work.
3 Our Method
3.1 The Model
We consider a summary as a sequence of sen-
tences. As an example, document set D =
{d1, d2, d3} is given to a summarizer. We de-
fine d as a single document. Document d1,
which consists of four sentences, is describe
by d1 = {s11, s12, s13, s14}. Documents d2
and d3 consist of five sentences and three sen-
tences (i.e. d2 = {s21, s22, s23, s24, s25}, d3 =
e1 e2 e3 . . . e6 e7 e8
s11 1 0 0 1 0 0
s12 0 1 0 0 0 0
s13 0 0 0 0 0 1
.
.
.
.
.
.
s31 0 0 0 0 0 0
s32 0 0 1 0 1 0
s33 0 0 0 0 0 1
Table 2: Sentence-Concept Matrix.
{s31, s32, s33}). If the summary consists of four
sentences s11, s23, s32, s33 and they are ordered as
s11 ? s23 ? s32 ? s33, we add symbols indicat-
ing the beginning of the summary s0 and the end
of the summary s4, and describe the summary as
S = ?s0, s11, s23, s32, s33, s4?. Summary S can
be represented as a directed path that starts at s0
and ends at s4 as shown in Fig. 1.
We describe a directed arc between si and sj as
ai,j ? A. The directed path shown in Fig. 1 is de-
composed into nodes, s0, s11, s23, s32, s33, s4, and
arcs, a0,11, a11,23, a23,32, a32,33, a33,4.
To represent the discourse coherence of two ad-
jacent sentences, we define weight ci,j ? C as
the coherence score on the directed arc ai,j . We
assume that better summaries have higher coher-
ence scores, i.e. if the sum of the scores of the arcs?
ai,j?S ci,jai,j is high, the summary is coherent.
We also assume that the source document set
D includes set of concepts e ? E. Each concept
e is covered by one or more of the sentences in
the document set. We show this schema in Ta-
ble 2. According to Table 2, document set D has
eight concepts e1, e2, . . . , e7, e8 and sentence s11
includes concepts e1 and e6 while sentence s12 in-
cludes e2.
We consider each concept ei has a weight wi.
We assume that concept ei will have high weight
wi if it is important. This paper improves sum-
mary quality by maximizing the sum of these
weights.
We define, based on the above assumption, the
following objective function:
L(S) = ?ei?S wiei +
?
ai,j?S ci,jai,j (2)
s.t. length(S) ? K
Summarization is, in this paper, realized by
maximizing the sum of weights of concepts in-
cluded in the summary and the coherence score of
all adjacent sentences in the summary under the
912
limit of maximum summary size. Note that while
S and T represents the set of sentences in Eq.1,
they represent the sequence of sentences in Eq.2.
Maximizing Eq.2 is NP-hard. If each sen-
tence in the source document set has one concept
(i.e. Table 2 is a diagonal matrix), Eq.2 becomes
the Prize Collecting Traveling Salesman Problem
(Balas, 1989). Therefore, a highly efficient decod-
ing method is essential.
3.2 Parameter Estimation
Our method requires two parameters: weights
w ? W of concepts and coherence c ? C of two
adjacent sentences. We describe them here.
Content Score
In this paper, as mentioned above, since we at-
tempt to summarize reviews, we adopt opinion
as a concept. We define opinion e = ?t, a, p?
as the tuple of target t, aspect a and its polarity
p ? {?1, 0, 1}. We define target t as the tar-
get of an opinion. For example, the target t of
the sentence ?This digital camera has good im-
age quality.? is digital camera. We define aspect
a as a word that represents a standpoint appro-
priate for evaluating products and services. With
regard to digital cameras, aspects include image
quality, design and battery life. In the above ex-
ample sentence, the aspect is image quality. Po-
larity p represents whether the opinion is positive
or negative. In this paper, we define p = ?1 as
negative, p = 0 as neutral and p = 1 as posi-
tive. Thus the example sentence contains opinion
e = ?digital camera, image quality, 1?.
Opinions are extracted using a sentiment ex-
pression dictionary and pattern matching from de-
pendency trees of sentences. This opinion extrac-
tor is the same as that used in Nishikawa et al
(2010).
As the weight wi of concept ei, we use only
the frequency of each opinion in the input docu-
ment set, i.e. we assume that an opinion that ap-
pears frequently in the input is important. While
this weighting is relatively naive compared to Ler-
man et al (2009)?s method, our ROUGE evalua-
tion shows that this approach is effective.
Coherence Score
In this section, we define coherence score c.
Since it is not easy to model the global coherence
of a set of sentences, we approximate the global
coherence by the sum of local coherence i.e. the
sum of coherence scores of sentence pairs. We
define local coherence score ci,j of two sentences
x = {si, sj} and their order y = ?si, sj? repre-
senting si ? sj as follows:
ci,j = w ? ?(x, y) (3)
w??(x, y) is the inner product ofw and ?(x, y),
w is a parameter vector and ?(x, y) is a feature
vector of the two sentences si and sj .
Since coherence consists of many different el-
ements and it is difficult to model all of them,
we approximate the features of coherence as the
Cartesian product of the following features: con-
tent words, POS tags of content words, named en-
tity tags (e.g. LOC, ORG) and conjunctions. Lap-
ata (2003) proposed most of these features.
We also define feature vector ?(x,y) of the bag
of sentences x = {s0, s1, . . . , sn, sn+1} and its
entire order y = ?s0, s1, . . . , sn, sn+1? as follows:
?(x,y) =
?
x,y
?(x, y) (4)
Therefore, the score of order y is w ? ?(x,y).
Given a training set, if trained parameter vector w
assigns score w ? ?(x,yt) to correct order yt that
is higher than score w ??(x, y?) assigned to incor-
rect order y?, it is expected that the trained parame-
ter vector will give a higher score to coherently or-
dered sentences than to incoherently ordered sen-
tences.
We use the Passive-Aggressive algorithm
(Crammer et al, 2006) to find w. The Passive-
Aggressive algorithm is an online learning algo-
rithm that updates the parameter vector by taking
up one example from the training examples and
outputting the solution that has the highest score
under the current parameter vector. If the output
differs from the training example, the parameter
vector is updated as follows;
min ||wi+1 ?wi|| (5)
s.t. s(x,yt;wi+1)? s(x, y?;wi+1) ? `(y?;yt)
s(x,y;w) = w ? ?(x,y)
wi is the current parameter vector and wi+1 is
the updated parameter vector. That is, Eq.5 means
that the score of the correct order must exceed the
score of an incorrect order by more than loss func-
tion `(y?;yt) while minimizing the change in pa-
rameters.
When updating the parameter vector, this al-
gorithm requires the solution that has the highest
score under the current parameter vector, so we
have to run an argmax operation. Since we are
913
attempting to order a set of sentences, the opera-
tion is regarded as solving the Traveling Salesman
Problem (Althaus et al, 2004); that is, we locate
the path that offers the maximum score through
all n sentences where s0 and sn+1 are starting and
ending points, respectively. This operation is NP-
hard and it is difficult to find the global optimal
solution. To overcome this, we find an approxi-
mate solution by beam search.1
We define loss function `(y?;yt) as follows:
`(y?;yt) = 1? ? (6)
? = 1 ? 4 S(y?,yt)N(N ? 1) (7)
? indicates Kendall?s tau. S(y?,yt) is the mini-
mum number of operations that swap adjacent ele-
ments (i.e. sentences) needed to bring y? to yt (La-
pata, 2006). N indicates the number of elements.
Since Lapata (2006) reported that Kendall?s tau
reliably reproduces human ratings with regard to
sentence ordering, using it to minimize the loss
function is expected to yield more reliable param-
eters.
We omit detailed derivations due to space limi-
tations. Parameters are updated as per the follow-
ing equation.
wi+1 = wi + ?i(?(x,yt)? ?(x, y?)) (8)
?i = `(y?;yt) ? s(x,yt;w
i) + s(x, y?;wi)
||?(x,yt)? ?(x, y?)||2 + 12C
(9)
C in Eq.9 is the aggressiveness parameter that
controls the degree of parameter change.
Note that our method learns w from documents
automatically annotated by a POS tagger and a
named entity tagger. That is, manual annotation
isn?t required.
3.3 Decoding with Integer Linear
Programming Formulation
This section describes an ILP formulation of the
above model. We use the same notation con-
vention as introduced in Section 3.1. We use
s ? S, a ? A, e ? E as the decision variable.
Variable si ? S indicates the inclusion of the i
th sentence. If the i th sentence is part of the
summary, then si is 1. If it is not part of the
1Obviously, ILP can be used to search for the path that
maximizes the score. While beam search tends to fail to find
out the optimal solution, it is tractable and the learning al-
gorithm can estimate the parameter from approximate solu-
tions. For these reasons we use beam search.
summary, then si is 0. Variable ai,j ? A indi-
cates the adjacency of the i th and j th sentences.
If these two sentences are ordered as si ? sj ,
then ai,j is 1. Variable ei ? E indicates the in-
clusion of the i th concept ei. Taking Fig.1 as
an example, variables s0, s11, s23, s32, s33, s4 and
a0,11, a11,23, a23,32, a32,33, a33,4 are 1. ei, which
correspond to the concepts in the above extracted
sentences, are also 1.
We represent the above objective function
(Eq.2) as follows:
max
?
?
??
?
ei?E
wiei + (1 ? ?)
?
ai,j?A
ci,jai,j
?
?
? (10)
Eq.10 attempts to cover as much of the concepts
included in input document set as possible accord-
ing to their weights w ? W and orders sentences
according to discourse coherence c ? C. ? is a
scaling factor to balance w and c.
We then impose some constraints on Eq.10 to
acquire the optimum solution.
First, we range the above three variables s ?
S, a ? A, e ? E.
si, ai,j , ei ? {0, 1} ?i, j
In our model, a summary can?t include the same
sentence, arc, or concept twice. Taking Table 2
for example, if s13 and s33 are included in a sum-
mary, the summary has two e8, but e8 is 1. This
constraint avoids summary redundancy.
The summary must meet the condition of maxi-
mum summary size. The following inequality rep-
resents the size constraint:
?
si?S
lisi ? K
li ? L indicates the length of sentence si. K is
the maximum size of the summary.
The following inequality represents the rela-
tionship between sentences and concepts in the
sentences.
?
i
mijsi ? ej ?j
The above constraint represents Table 2. mi,j is
an element of Table 2. If si is not included in the
summary, the concepts in si are not included.
Symbols indicating the beginning and end of
the summary must be part of the summary.
914
s0 = 1
sn+1 = 1
n is the number of sentences in the input docu-
ment set.
Next, we describe the constraints placed on
arcs.
The beginning symbol must be followed by a
sentence or a symbol and must not have any pre-
ceding sentences/symbols. The end symbol must
be preceded by a sentence or a symbol and must
not have any following sentences/symbols. The
following equations represent these constraints:
?
i
a0,i = 1
?
i
ai,0 = 0
?
i
an+1,i = 0
?
i
ai,n+1 = 1
Each sentence in the summary must be pre-
ceded and followed by a sentence/symbol.
?
i
ai,j +
?
i
aj,i = 2sj ?j
?
i
ai,j =
?
i
aj,i ?j
The above constraints fail to prevent cycles. To
rectify this, we set the following constraints.
?
i
f0,i = n
?
i
fi,0 ? 1
?
i
fi,j ?
?
i
fj,i = sj ?j
fi,j ? nai,j ?i, j
The above constraints indicate that flows f are
sent from s0 as a source to sn+1 as a sink. n unit
flows are sent from the source and each node ex-
pends one unit of flows. More than one flow has
to arrive at the sink. By setting these constraints,
the nodes consisting of a cycle have no flow. Thus
solutions that contain a cycle are prevented. These
constraints have also been used to avoid cycles in
headline generation (Deshpande et al, 2007).
4 Experiments
This section evaluates our method in terms of
ROUGE score and readability. We tested our
method and two baselines in two domains: re-
views of commodities and restaurants. We col-
lected 4,475 reviews of 100 commodities and
2,940 reviews of 100 restaurants from websites.
The commodities included items such as digital
cameras, printers, video games, and wines. The
average document size was 10,173 bytes in the
commodity domain and 5,343 bytes in the restau-
rant domain. We attempted to generate 300 byte
summaries, so the summarization rates were about
3% and 6%, respectively.
We prepared 4 references for each review, thus
there were 400 references in each domain. The au-
thors were not those who made up the references.
These references were used for ROUGE and read-
ability evaluation.
Since our method requires the parameter vec-
tor w for determining the coherence scores. We
trained the parameter vector for each domain.
Each parameter vector was trained using 10-fold
cross validation. We used 8 samples to train, 1
to develop, and 1 to test. In the restaurant do-
main, we added 4,390 reviews to each training set
to alleviate data sparseness. In the commodity do-
main, we add 47,570 reviews.2
As the solver, we used glpk.3 According to the
development set, ? in Eq.10 was set as 0.1.
4.1 Baselines
We compare our method to the references (which
also provide the upper bound) and the opinion
summarizers proposed by Carenini et al (2006)
and Lerman et al (2009) as the baselines.
In the ROUGE evaluations, Human indicates
ROUGE scores between references. To compare
our summarizer to human summarization, we cal-
culated ROUGE scores between each reference
and the other three references, and averaged them.
In the readability evaluations, we randomly se-
lected one reference for each commodity and each
restaurant and compared them to the results of the
three summarizers.
Carenini et al (2006)
Carenini et al (2006) proposed two opinion
2The commodities domain suffers from stronger review
variation than the restaurant domain so more training data
was needed.
3http://www.gnu.org/software/glpk/
915
summarizers. One uses a natural language genera-
tion module, and other is based on MEAD (Radev
et al, 2004). Since it is difficult to mimic the natu-
ral language generation module, we implemented
the latter one. The objective function Carenini et
al. (2006) proposed is as follows:
L1(S) =
?
a?S
?
s?D
|polaritys(a)| (11)
polaritys(a) indicates the polarity of aspect a
in sentence s present in source document set D.
That is, this function gives a high score to a sum-
mary that covers aspects frequently mentioned in
the input, and whose polarities tend to be either
positive or negative.
The solution is identified using the greedy
method. If there is more than one sentence that
has the same score, the sentence that has the
higher centroid score (Radev et al, 2004) is ex-
tracted.
Lerman et al (2009)
Lerman et al (2009) proposed three objective
functions for opinion summarization, and we im-
plemented one of them. The function is as fol-
lows:
L2(S) = ?(KL(pS(a), pD(a)) (12)
+
?
a?A
KL(N (x|?aS , ?2aS ),N (x|?aD , ?
2
aD)))
KL(p, q) means the Kullback-Leibler diver-
gence between probability distribution p and q.
pS(a) and pD(a) are probability distributions in-
dicating how often aspect a ? A occurs in sum-
mary S and source document set D respectively.
N (x|?, ?2) is a Gaussian distribution indicating
distribution of polarity of an aspect whose mean
is ? and variance is ?2. ?aS , ?aD and ?2aS , ?2aD
are the means and the variances of aspect a in
summary S and source document set D, respec-
tively. These parameters are determined using
maximum-likelihood estimation.
That is, the above objective function gives high
score to a summary whose distributions of aspects
and polarities mirror those of the source document
set.
To identify the optimal solution, Lerman et al
(2009) use a randomized algorithm. First, the
summarizer randomly extracts sentences from the
source document set, then iteratively performs in-
sert/delete/swap operations on the summary to in-
crease Eq.12 until summary improvement satu-
rates. While this method is prone to lock onto
Commodity R-2 R-SU4 R-SU9
(Carenini et al, 2006) 0.158 0.202 0.186
(Lerman et al, 2009) 0.205 0.247 0.227
Our Method 0.231 0.251 0.230
Human 0.384 0.392 0.358
Restaurant R-2 R-SU4 R-SU9
(Carenini et al, 2006) 0.251 0.281 0.258
(Lerman et al, 2009) 0.260 0.296 0.273
Our Method 0.285 0.303 0.273
Human 0.358 0.370 0.335
Table 3: Automatic ROUGE evaluation.
# of Sentences
(Carenini et al, 2006) 3.79
(Lerman et al, 2009) 6.28
Our Method 7.88
Human 5.83
Table 4: Average number of sentences in the sum-
mary.
local solutions, the summarizer can reach the op-
timal solution by changing the starting sentences
and repeating the process. In this experiment, we
used 100 randomly selected starting points.
4.2 ROUGE
We used ROUGE (Lin, 2004) for evaluating the
content of summaries. We chose ROUGE-2,
ROUGE-SU4 and ROUGE-SU9. We prepared
four reference summaries for each document set.
The results of these experiments are shown in
Table 3. ROUGE scores increase in the order of
(Carenini et al, 2006), (Lerman et al, 2009) and
our method, but no method could match the per-
formance of Human. Our method significantly
outperformed Lerman et al (2009)?s method over
ROUGE-2 according to the Wilcoxon signed-rank
test, while it shows no advantage over ROUGE-
SU4 and ROUGE-SU9.
Although our weighting of the set of sentences
is relatively naive compared to the weighting pro-
posed by Lerman et al (2009), our method out-
performs their method. There are two reasons
for this; one is that we adopt ILP for decoding,
so we can acquire preferable solutions efficiently.
While the score of Lerman et al (2009)?s method
may be improved by adopting ILP, it is difficult
to do so because their objective function is ex-
tremely complex. The other reason is the coher-
ence score. Since our coherence score is based on
916
Commodity (Carenini et al, 2006) (Lerman et al, 2009) Our Method Human
(Carenini et al, 2006) - 27/45 18/29 8/46
(Lerman et al, 2009) 18/45 - 29/48 11/47
Our Method 11/29 19/48 - 5/46
Human 38/46 36/47 41/46 -
Restaurant (Carenini et al, 2006) (Lerman et al, 2009) Our Method Human
(Carenini et al, 2006) - 31/45 17/31 8/48
(Lerman et al, 2009) 14/45 - 25/47 7/46
Our Method 14/31 22/47 - 8/50
Human 40/48 39/46 42/50 -
Table 5: Readability evaluation.
content words, it may impact the content of the
summary.
4.3 Readability
Readability was evaluated by human judges.
Since it is difficult to perform absolute evalua-
tion to judge the readability of summaries, we
performed a paired comparison test. The judges
were shown two summaries of the same input and
decided which was more readable. The judges
weren?t informed which method generated which
summary. We randomly chose 50 sets of reviews
from each domain, so there were 600 paired sum-
maries.4 However, as shown in Table 4, the aver-
age numbers of sentences in the summary differed
widely from the methods and this might affect the
readability evaluation. It was not fair to include
the pairs that were too different in terms of the
number of sentences. Therefore, we removed the
pairs that differed by more than five sentences.
In the experiment, 523 pairs were used, and 21
judges evaluated about 25 summaries each. We
drew on DUC 2007 quality questions5 for read-
ability assessment.
Table 5 shows the results of the experiment.
Each element in the table indicates the number
of times the corresponding method won against
other method. For example, in the commodity do-
main, the summaries that Lerman et al (2009)?s
method generated were compared with the sum-
maries that Carenini et al (2006)?s method gener-
ated 45 times, and Lerman et al (2009)?s method
won 18 times. The judges significantly preferred
the references in both domains. There were no
significant differences between our method and
the other two methods. In the restaurant do-
4
4C2 ? 100 = 600
5http://www-nlpir.nist.gov/projects/
duc/duc2007/quality-questions.txt
main, there was a significant difference between
(Carenini et al, 2006) and (Lerman et al, 2009).
Since we adopt ILP, our method tends to pack
shorter sentences into the summary. However,
our coherence score prevents this from degrading
summary readability.
5 Conclusion
This paper proposed a novel algorithm for opinion
summarization that takes account of content and
coherence, simultaneously. Our method directly
searches for the optimum sentence sequence by
extracting and ordering sentences present in the
input document set. We proposed a novel ILP
formulation against selection-and-ordering prob-
lems; it is a powerful mixture of the Maximum
Coverage Problem and the Traveling Salesman
Problem. Experiments revealed that the algo-
rithm creates summaries that have higher ROUGE
scores than existing opinion summarizers. We
also performed readability experiments. While
our summarizer tends to extract shorter sentences
to optimize summary content, our proposed co-
herence score prevented this from degrading the
readability of the summary.
One future work includes enriching the features
used to determine the coherence score. We expect
that features such as entity grid (Barzilay and La-
pata, 2005) will improve overall algorithm perfor-
mance. We also plan to apply our model to tasks
other than opinion summarization.
Acknowledgments
We would like to sincerely thank Tsutomu Hirao
for his comments and discussions. We would also
like to thank the anonymous reviewers for their
comments.
917
References
Althaus, Ernst, Nikiforos Karamanis and Alexander Koller.
2004. Computing Locally Coherent Discourses. In Proc.
of the 42nd Annual Meeting of the Association for Com-
putational Linguistics.
Balas, Egon. 1989. The prize collecting traveling salesman
problem. Networks, 19(6):621?636.
Banko, Michele, Vibhu O. Mittal and Michael J. Witbrock.
2000. Headline Generation Based on Statistical Transla-
tion. In Proc. of the 38th Annual Meeting of the Associa-
tion for Computational Linguistics.
Barzilay, Regina, Noemie Elhadad and Kathleen McKeown.
2002. Inferring Strategies for Sentence Ordering in Mul-
tidocument Summarization. Journal of Artificial Intelli-
gence Research, 17:35?55.
Barzilay, Regina and Mirella Lapata. 2005. Modeling Lo-
cal Coherence: An Entity-based Approach. In Proc. of
the 43rd Annual Meeting of the Association for Compu-
tational Linguistics.
Carbonell, Jaime and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering docu-
ments and producing summaries. In Proc. of the 21st An-
nual International ACM SIGIR Conference on Research
and Development in Information Retrieval.
Carenini, Giuseppe, Raymond Ng and Adam Pauls. 2006.
Multi-Document Summarization of Evaluative Text. In
Proc. of the 11th Conference of the European Chapter of
the Association for Computational Linguistics.
Clarke, James and Mirella Lapata. 2007. Modelling Com-
pression with Discourse Constraints. In Proc. of the 2007
Joint Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Language
Learning.
Collins, Michael. 2002. Discriminative Training Methods for
Hidden Markov Models: Theory and Experiments with
Perceptron Algorithms. In Proc. of the 2002 Conference
on Empirical Methods in Natural Language Processing.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning Re-
search, 7:551?585.
Deshpande, Pawan, Regina Barzilay and David R. Karger.
2007. Randomized Decoding for Selection-and-Ordering
Problems. In Proc. of Human Language Technologies
2007: The Conference of the North American Chapter of
the Association for Computational Linguistics.
Elsner, Micha, Joseph Austerweil and Eugene Charniak.
2007. A unified local and global model for discourse co-
herence. In Proc. of Human Language Technologies 2007:
The Conference of the North American Chapter of the As-
sociation for Computational Linguistics.
Filatova, Elena and Vasileios Hatzivassiloglou. 2004. A For-
mal Model for Information Selection in Multi-Sentence
Text Extraction. In Proc. of the 20th International Con-
ference on Computational Linguistics.
Gillick, Dan and Benoit Favre. 2009. A Scalable Global
Model for Summarization. In Proc. of Human Language
Technologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computational
Linguistics Workshop on Integer Linear Programming for
NLP.
Hirao, Tsutomu, Hideki Isozaki, Eisaku Maeda and Yuji
Matsumoto. 2002. Extracting important sentences with
support vector machines. In Proc. of the 19th Interna-
tional Conference on Computational Linguistics.
Kupiec, Julian, Jan Pedersen and Francine Chen. 1995. A
Trainable Document Summarizer. In Proc. of the 18th An-
nual International ACM SIGIR Conference on Research
and Development in Information Retrieval.
Lapata, Mirella. 2003. Probabilistic Text Structuring: Exper-
iments with Sentence Ordering. In Proc. of the 41st An-
nual Meeting of the Association for Computational Lin-
guistics.
Lapata, Mirella. 2006. Automatic Evaluation of Informa-
tion Ordering: Kendall?s Tau. Computational Linguistics,
32(4):471?484.
Lerman, Kevin, Sasha Blair-Goldensohn and Ryan McDon-
ald. 2009. Sentiment Summarization: Evaluating and
Learning User Preferences. In Proc. of the 12th Confer-
ence of the European Chapter of the Association for Com-
putational Linguistics.
Lin, Chin-Yew. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. In Proc. of Text Summarization
Branches Out.
Martins, Andre F. T., and Noah A. Smith. 2009. Summariza-
tion with a Joint Model for Sentence Extraction and Com-
pression. In Proc. of Human Language Technologies: The
2009 Annual Conference of the North American Chapter
of the Association for Computational Linguistics Work-
shop on Integer Linear Programming for NLP.
McDonald, Ryan. 2007. A Study of Global Inference Algo-
rithms in Multi-document Summarization. In Proc. of the
29th European Conference on Information Retrieval.
Nishikawa, Hitoshi, Takaaki Hasegawa, Yoshihiro Matsuo
and Genichiro Kikui. 2010. Optimizing Informativeness
and Readability for Sentiment Summarization. In Proc. of
the 48th Annual Meeting of the Association for Computa-
tional Linguistics.
Radev, Dragomir R., Hongyan Jing, Magorzata Sty and
Daniel Tam. 2004. Centroid-based summarization of mul-
tiple documents. Information Processing and Manage-
ment, 40(6):919?938.
Soricut, Radu and Daniel Marcu. 2006. Discourse Genera-
tion Using Utility-Trained Coherence Models. In Proc. of
the 21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association for
Computational Linguistics.
Takamura, Hiroya and Manabu Okumura. 2009. Text Sum-
marization Model based on Maximum Coverage Problem
and its Variant. In Proc. of the 12th Conference of the Eu-
ropean Chapter of the Association for Computational Lin-
guistics.
Yih, Wen-tau, Joshua Goodman, Lucy Vanderwende and
Hisami Suzuki. 2007. Multi-Document Summarization by
Maximizing Informative Content-Words. In Proc. of the
20th International Joint Conference on Artificial Intelli-
gence.
918
