Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 793?800
Manchester, August 2008
Metric Learning for Synonym Acquisition
Nobuyuki Shimizu
Information Technology Center
University of Tokyo
shimizu@r.dl.itc.u-tokyo.ac.jp
Masato Hagiwara
Graduate School of Information Science
Nagoya University
hagiwara@kl.i.is.nagoya-u.ac.jp
Yasuhiro Ogawa and Katsuhiko Toyama
Graduate School of Information Science
Nagoya University
{yasuhiro,toyama}@kl.i.is.nagoya-u.ac.jp
Hiroshi Nakagawa
Information Technology Center
University of Tokyo
n3@dl.itc.u-tokyo.ac.jp
Abstract
The distance or similarity metric plays an
important role in many natural language
processing (NLP) tasks. Previous stud-
ies have demonstrated the effectiveness of
a number of metrics such as the Jaccard
coefficient, especially in synonym acqui-
sition. While the existing metrics per-
form quite well, to further improve perfor-
mance, we propose the use of a supervised
machine learning algorithm that fine-tunes
them. Given the known instances of sim-
ilar or dissimilar words, we estimated the
parameters of the Mahalanobis distance.
We compared a number of metrics in our
experiments, and the results show that the
proposed metric has a higher mean average
precision than other metrics.
1 Introduction
Accurately estimating the semantic distance be-
tween words in context has applications for
machine translation, information retrieval (IR),
speech recognition, and text categorization (Bu-
danitsky and Hirst, 2006), and it is becoming
clear that a combination of corpus statistics can be
used with a dictionary, thesaurus, or other knowl-
edge source such as WordNet or Wikipedia, to in-
crease the accuracy of semantic distance estima-
tion (Mohammad and Hirst, 2006). Although com-
piling such resources is labor intensive and achiev-
ing wide coverage is difficult, these resources to
some extent explicitly capture semantic structures
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
of concepts and words. In contrast, corpus statis-
tics achieve wide coverage, but the semantic struc-
ture of a concept is only implicitly represented in
the context. Assuming that two words are semanti-
cally closer if they occur in similar contexts, statis-
tics on the contexts of words can be gathered and
compared for similarity, by using a metric such as
the Jaccard coefficient.
Our proposal is to extend and fine-tune the latter
approach with the training data obtained from the
former. We apply metric learning to this task. Al-
though still in their infancy, distance metric learn-
ing methods have undergone rapid development in
the field of machine learning. In a setting simi-
lar to semi-supervised clustering, where known in-
stances of similar or dissimilar objects are given,
a metric such as the Mahalanobis distance can be
learned from a few data points and tailored to fit a
particular purpose. Although classification meth-
ods such as logistic regression now play impor-
tant roles in natural language processing, the use
of metric learning has yet to be explored.
Since popular current methods for synonym ac-
quisition require no statistical learning, it seems
that supervised machine learning should easily
outperform them. Unfortunately, there are obsta-
cles to overcome. Since metric learning algorithms
usually learn the parameters of a Mahalanobis dis-
tance, the number of parameters is quadratic to the
number of features. They learn how two features
should interact to produce the final metric. While
traditional metrics forgo examining of the interac-
tions entirely, in applying metrics such as Jaccard
coefficient, it is not uncommon nowadays to use
more than 10,000 features, a number that a typical
metric learner is incapable of processing. Thus we
have two options: one is to find the most impor-
tant features and model the interactions between
793
them, and the other is simply to use a large number
of features. We experimentally examined the two
options and found that metric learning is useful in
synonym acquisition, despite it utilizing fewer fea-
tures than traditional methods.
The remainder of this paper is organized as fol-
lows: in section 2, we review prior work on syn-
onym acquisition and metric learning. In section
3, we introduce the Mahalanobis distance metric
and a learning algorithm based on this metric. In
section 4 and 5, we explain the experimental set-
tings and propose the use of normalization to make
the Mahalanobis distances work in practice, and
then in section 6, we discuss issues we encountered
when applying this metric to synonym acquisition.
We conclude in section 7.
2 Prior Work
As this paper is based on two different lines of re-
search, we first review the work in synonym acqui-
sition, and then review the work in generic metric
learning. To the best of the authors? knowledge,
none of the metric learning algorithms have been
applied to automatic synonym acquisition.
Synonym relation is important lexical knowl-
edge for many natural language processing
tasks including automatic thesaurus construction
(Croach and Yang, 1992; Grefenstette, 1994) and
IR (Jing and Croft, 1994). Various methods (Hin-
dle, 1990; Lin, 1998) of automatically acquiring
synonyms have been proposed. They are usu-
ally based on the distributional hypothesis (Har-
ris, 1985), which states that semantically simi-
lar words share similar contexts, and they can be
roughly viewed as the combinations of two steps:
context extraction and similarity calculation. The
former extracts useful features from the contexts of
words, such as surrounding words or dependency
structure. The latter calculates how semantically
similar two given words are based on similarity or
distance metrics.
Many studies (Lee, 1999; Curran and Moens,
2002; Weeds et al, 2004) have investigated
similarity calculation, and a variety of dis-
tance/similarity measures have already been com-
pared and discussed. Weeds et al?s work is espe-
cially useful because it investigated the character-
istics of metrics based on a few criteria such as
the relative frequency of acquired synonyms and
clarified the correlation between word frequency,
distributional generality, and semantic generality.
However, all of the existing research conducted
only a posteriori comparison, and as Weeds et al
pointed out, there is no one best measure for all ap-
plications. Therefore, the metrics must be tailored
to applications, even to corpora and other settings.
We next review the prior work in generic metric
learning. Most previous metric learning methods
learn the parameters of the Mahalanobis distance.
Although the algorithms proposed in earlier work
(Xing et al, 2002; Weinberger et al, 2005; Glober-
son and Roweis, 2005) were shown to yield excel-
lent classification performance, these algorithms
all have worse than cubic computational complex-
ity in the dimensionality of the data. Because of
the high dimensionality of our objects, we opted
for information-theoretic metric learning proposed
by (Davis et al, 2007). This algorithm only uses
an operation quadratic in the dimensionality of the
data.
Other work on learning Mahalanobis metrics in-
cludes online metric learning (Shalev-Shwartz et
al., 2004), locally-adaptive discriminative methods
(Hastie and Tibshirani, 1996), and learning from
relative comparisons (Schutz and Joahims, 2003).
Non-Mahalanobis-based metric learning methods
have also been proposed, though they seem to suf-
fer from suboptimal performance, non-convexity,
or computational complexity. Examples include
neighborhood component analysis (Goldberger et
al., 2004).
3 Metric Learning
3.1 Problem Formulation
To set the context for metric learning, we first de-
scribe the objects whose distances from one an-
other we would like to know. As noted above re-
garding the distributional hypothesis, our object is
the context of a target word. To represent the con-
text, we use a sparse vector in Rd. Each dimension
of an input vector represents a feature of the con-
text, and its value corresponds to the strength of
the association. The vectors of two target words
represent their contexts as points in multidimen-
sional feature-space. A suitable metric (for exam-
ple, Euclidean) defines the distance between the
two points, thereby estimating the semantic dis-
tance between the target words.
Given points x
i
, x
j
? R
d
, the (squared) Ma-
halanobis distance between them is parameter-
ized by a positive definite matrix A as follows
d
A
(x
i
, x
j
) = (x
i
? x
j
)
?
A(x
i
? x
j
). The Ma-
794
halanobis distance is a straightforward extension
of the standard Euclidean distance. If we let A
be the identity matrix, the Mahalanobis distance
reduces to the Euclidean distance. Our objective
is to obtain the positive definite matrix A that pa-
rameterizes the Mahalanobis distance, so that the
distance between the vectors of two synonymous
words is small, and the distance between the vec-
tors of two dissimilar words is large. Stated more
formally, the Mahalanobis distance between two
similar points must be smaller than a given upper
bound, i.e., d
A
(x
i
, x
j
) ? u for a relatively small
value of u. Similarly, two points are dissimilar if
d
A
(x
i
, x
j
) ? l for sufficiently large l.
As we discuss below, we were able to use
the Euclidean distance to acquire synonyms quite
well. Therefore, we would like the positive definite
matrix A of the Mahalanobis distance to be close to
the identity matrix I . This keeps the Mahalanobis
distance similar to the Euclidean distance, which
would help to prevent overfitting the data. To op-
timize the matrix, we follow the information theo-
retic metric learning approach described in (Davis
et al, 2007). We summarize the problem formula-
tion advocated by this approach in this section and
the learning algorithm in the next section.
To define the closeness between A and I , we
use a simple bijection (up to a scaling function)
from the set of Mahalanobis distances to the set
of equal mean multivariate Gaussian distributions.
Without loss of generalization, let the equal mean
be ?. Then given a Mahalanobis distance pa-
rameterized by A, the corresponding Gaussian is
p(x;A) =
1
Z
exp(?
1
2
d
A
(x, ?)) where Z is the
normalizing factor. This enables us to measure
the distance between two Mahalanobis distances
with the Kullback-Leibler (KL) divergence of two
Gaussians:
KL(p(x; I)||p(x;A)) =
?
p(x, I) log
(
p(x; I)
p(x;A)
)
dx.
Given pairs of similar points S and pairs of dis-
similar points D, the optimization problem is:
min
A
KL(p(x; I)||p(x;A))
subject to d
A
(x
i
, x
j
) ? u (i, j) ? S
d
A
(x
i
, x
j
) ? l (i, j) ? D
3.2 Learning Algorithm
(Davis and Dhillon, 2006) has shown that the
KL divergence between two multivariate Gaus-
sians can be expressed as the convex combination
of a Mahalanobis distance between mean vectors
and the LogDet divergence between the covariance
matrices. The LogDet divergence equals
D
ld
(A,A
0
) = tr(AA
?1
0
)? log det(AA
?1
0
)? n
for n by n matrices A,A
0
. If we assume the means
of the Gaussians to be the same, we have
KL(p(x;A
0
||p(x,A)) =
1
2
D
ld
(A,A
0
)
The optimization problem can be restated as
min
A0
D
ld
(A, I)
s.t. tr(A(x
i
? x
j
)(x
i
? x
j
)
?
) ? u (i, j) ? S
tr(A(x
i
? x
j
)(x
i
? x
j
)
?
) ? l (i, j) ? D
We then incorporate slack variables into the for-
mulation to guarantee the existence of a feasible
solution for A. The optimization problem be-
comes:
min
A0
D
ld
(A, I) + ?D
ld
(diag(?), diag(?
0
))
s.t. tr(A(x
i
? x
j
)(x
i
? x
j
)
?
) ? ?
c(i,j)
(i, j) ? S
tr(A(x
i
? x
j
)(x
i
? x
j
)
?
) ? ?
c(i,j)
(i, j) ? D
where c(i, j) is the index of the (i, j)-th constraint
and ? is a vector of slack variables whose compo-
nents are initialized to u for similarity constraints
and l for dissimilarity constraints. The tradeoff
between satisfying the constraints and minimiz-
ing D
ld
(A, I) is controlled by the parameter ?.
To solve this optimization problem, the algorithm
shown in Algorithm 3.1 repeatedly projects the
current solution onto a single constraint.
This completes the summary of (Davis et al,
2007).
4 Experimental Settings
In this section, we describe the experimental set-
tings including the preprocessing of data and fea-
tures, creation of the query word sets, and settings
of the cross validation.
4.1 Features
We used a dependency structure as the context for
words because it is the most widely used and one
of the best performing contextual information in
the past studies (Ruge, 1997; Lin, 1998). As the
extraction of an accurate and comprehensive de-
pendency structure is in itself a complicated task,
the sophisticated parser RASP Toolkit 2 (Briscoe
et al, 2006) was utilized to extract this kind of
word relation.
Let N(w, c) be the raw cooccurrence count of
word w and context c, the grammatical relation
795
Algorithm
3.1: INFORMATION THEORETIC METRIC LEARNING
Input :
X(d by n matrix), I(identity matrix)
S(set of similar pairs),D(set of dissimilar pairs)
?(slack parameter), c(constraint index function)
u, l(distance thresholds)
Output :
A(Mahalanobis matrix)
A := I
?
ij
:= 0
?
c(i,j)
:= u for (i, j) ? S; otherwise, ?
c(i,j)
:= l
repeat
Pick a constraint (i, j) ? S or (i, j) ? D
p := (x
i
? x
j
)
?
A(x
i
? x
j
)
? := 1 if (i, j) ? S,?1 otherwise.
? := min(?
ij
,
?
2
(
1
p
?
?
?
c(i,j)
))
? := ??/(1? ???
c(i,j)
)
?
c(i,j)
:= ??
c(i,j)
/(? + ???
c(i,j)
)
?
ij
:= ?
ij
? ?
A := A + ?A(x
i
? x
j
)(x
i
? x
j
)
?
A
until convergence
return (A)
in which w occurs. These raw counts were ob-
tained from New York Times articles (July 1994)
extracted from English Gigaword 1. The section
consists of 7,593 documents and approx. 5 million
words. As discussed below, we limited the vocab-
ulary to the nouns in the Longman Defining Vo-
cabulary (LDV) 2. The features were constructed
by weighting them using pointwise mutual infor-
mation: wgt(w, c) = PMI(w, c) = log P (w,c)
P (w)P (c)
.
Co-occurrence data constructed this way can
yield more than 10,000 context types, rendering
metric learning impractical. As the applications
of feature selection reduce the performance of the
baseline metrics, we tested them in two different
settings: with and without feature selection. To
mitigate this problem, we applied a feature selec-
tion technique to reduce the feature dimensional-
ity. We selected features using two approaches.
The first approach is a simple frequency cutoff, ap-
plied as a pre-processing to filter out words and
contexts with low frequency and to reduce com-
putational cost. Specifically, all words w such
that
?
c
N(w, c) < ?
f
and contexts c such that
?
w
N(w, c) < ?
f
, with ?
f
= 5, are removed from
the co-occurrence data.
The second approach is feature selection by con-
1http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2003T05
2http://www.cs.utexas.edu/users/kbarker/working notes/
ldoce-vocab.html
text importance (Hagiwara et al, 2008). First, the
context importance score for each context type is
calculated, and then the least important context
types are eliminated, until a desired numbers of
them remains. To measure the context importance
score, we used the number of unique words the
context co-occurs with: df(c) = |{w|N(w, c) >
0}|. We adopted this context selection criterion
on the assumption that the contexts shared by
many words should be informative, and the syn-
onym acquisition performance based on normal
distributional similarity calculation retains its orig-
inal level of performance until up to almost 90%
of context types are eliminated (Hagiwara et al,
2008). In our experiment, we selected features
rather aggressively, finally using only 10% of the
original contexts. These feature reduction oper-
ations reduced the dimensionality to a figure as
small as 1,281, while keeping the performance loss
at a minimum.
4.2 Similarity and Distance Functions
We compared seven similarity/distance functions
in our experiments: cosine similarity, Euclidean
distance, Manhattan distance, Jaccard coeffi-
cient, vector-based Jaccard coefficient (Jaccardv),
Jensen-Shannon Divergence (JS) and skew diver-
gence (SD99). We first define some notations. Let
C(w) be the set of context types that co-occur with
word w, i.e., C(w) = {c|N(w, c) > 0}, and w
i
be
the feature vector corresponding to word w, i.e.,
w
i
= [wgt(w
i
, c
1
) ... wgt(w
i
, c
M
)]
?
. The first
three, the cosine, Euclidean and Manhattan dis-
tance, are vector-based metrics.
cosine similarity
w
1
?w
2
||w
1
|| ? ||w
2
||
Euclidean distance
?
?
c?C(w
1
)?C(w
2
)
(wgt(w
1
, c)? wgt(w
2
, c))
2
Manhattan distance
?
c?C(w
1
)?C(w
2
)
|wgt(w
1
, c)? wgt(w
2
, c)|
Jaccard coefficient
?
c?C(w
1
)?C(w
2
)
min(wgt(w
1
, c),wgt(w
2
, c))
?
c?C(w
1
)?C(w
2
)
max(wgt(w
1
, c),wgt(w
2
, c))
,
796
vector-based Jaccard coefficient (Jaccardv)
w
i
?w
j
||w
i
|| + ||w
j
|| ?w
i
?w
j
.
Jensen-Shannon divergence (JS)
1
2
{KL(p
1
||m) + KL(p
2
||m)}, m = p
1
+ p
2
.
JS and SD99 are based on the KL divergence, so
the vectors must be normalized to form a probabil-
ity distribution. For notational convenience, we let
p
i
be the probability distribution representation of
feature vector w
i
, i.e., p
i
(c) = N(w
i
, c)/N(w
i
).
While the KL divergence suffers from the so-called
zero-frequency problem, a symmetric version of
the KL divergence called the Jensen-Shannon di-
vergence naturally avoids it.
skew divergence (SD99)
KL(p
1
||?p
2
+ (1? ?)p
1
).
As proposed by (Lee, 2001), the skew diver-
gence also avoids the zero-frequency problem by
mixing the original distribution with the target dis-
tribution. Parameter ? is set to 0.99.
4.3 Query Word Set and Cross Validation
To formalize the experiments, we must prepare a
set of query words for which synonyms are known
in advance. We chose the Longman Defining
Vocabulary (LDV) as the candidate set of query
words. For each word in the LDV, we consulted
three existing thesauri: Roget?s Thesaurus (Ro-
get, 1995), Collins COBUILD Thesaurus (Collins,
2002), and WordNet (Fellbaum, 1998). Each LDV
word was looked up as a noun to obtain the union
of synonyms. After removing words marked ?id-
iom?, ?informal? or ?slang? and phrases com-
prised of two or more words, this union was used
as the reference set of query words. LDV words for
which no noun synonyms were found in any of the
reference thesauri were omitted. From the remain-
ing 771 LDV words, there were 231 words that had
five or more synonyms in the combined thesaurus.
We selected these 231 words to be the query words
and distributed them into five partitions so as to
conduct five-fold cross validation. Four partitions
were used in training, and the remaining partition
was used in testing. For each fold, we created
the training set from four partitions as follows; for
each query word in the partitions, we randomly se-
lected five synonymous words and added the pairs
of query words and synonymous words to S, the
set of similar pairs. Similarly, five pairs of query
words and dissimilar words were randomly added
to D, the set of dissimilar pairs. The training set
for each fold consisted of S and D. Since a learner
trained on an imbalanced dataset may not learn
to discriminate enough between classes, we sam-
pled dissimilar pairs to create an evenly distributed
training dataset.
To make the evaluation realistic, we used a dif-
ferent method to create the test set: we paired each
query word with each of the 771 remaining words
to form the test set. Thus, in each fold, the training
set had an equal number of positive and negative
pairs, while in the test set, negative pairs outnum-
bered the positive pairs. While this is not a typical
setting for cross validation, it renders the evalua-
tion more realistic since an automatic synonym ac-
quisition system in operation must be able to pick
a few synonyms from a large number of dissimilar
words.
The meta-parameters of the metric learning
model were simply set u = 1, l = 2 and ? = 1.
Each training set consisted of 1,850 pairs, and the
test set consisted of 34,684 pairs. Since we con-
ducted five-fold cross validation, the reported per-
formance in this paper is actually a summary over
different folds.
4.4 Evaluation Measures
We used an evaluation program for KDD Cup
2004 (Caruana et al, 2004) called Perf to measure
the effectiveness of the metrics in acquiring syn-
onyms. To use the program, we used the following
formula to convert each distance metric to a simi-
larity metric. s(x
i
, x
j
) = 1/(1 + exp(d(x
i
, x
j
))).
Below, we summarize the three measures we
used: Mean Average Precision, TOP1, and Aver-
age Rank of Last Synonym.
Mean Average Precision (APR)
Perf implements a definition of average preci-
sion sometimes called ?expected precision?. Perf
calculates the precision at every recall where it is
defined. For each of these recall values, Perf finds
the threshold that produces the maximum preci-
sion, and takes the average over all of the recall
values greater than 0. Average precision is mea-
sured on each query, and then the mean of each
query?s average precision is used as the final met-
ric. A mean average precision of 1.0 indicates per-
fect prediction. The lowest possible mean average
797
precision is 0.0.
Average Rank of Last Synonym (RKL)
As in other evaluation measures, synonym can-
didates are sorted by predicted similarity, and this
metric measures how far down the sorted cases we
must go to find the last true synonym. A rank of
1 indicates that the last synonym is placed in the
top position. Given a query word, the highest ob-
tainable rank is N if there are N synonyms in the
corpus. The lower this measure is the better. Aver-
age ranks near 771 indicate poor performance.
TOP1
In each query, synonym candidates are sorted by
predicted similarity. If the word that ranks at the
top (highest similarity to the query word) is a true
synonym of the query word, Perf scores a 1 for
that query, and 0 otherwise. If there are ties, Perf
scores 0 unless all of the tied cases are synonyms.
TOP1 score ranges from 1.0 to 0.0. To achieve 1.0,
perfect TOP1 prediction, a similarity metric must
place a true synonym at the top of the sorted list
in every query. In the next section, we report the
mean of each query?s TOP1.
5 Results
The evaluations of the metrics are listed in Table
1. The figure on the left side of ? represents the
performance with 1,281 features, and that on the
right side with 12,812 features. Of all the met-
rics in Table 1, only the Mahalanobis L2 is trained
with the previously presented metric learning al-
gorithm. Thus, the values for the Mahalanobis
L2 are produced by the five-fold cross validation,
while the rest are given by the straight application
of the metrics discussed in Section 4.2 to the same
dataset. Strictly speaking, this is not a fair com-
parison, since we ought to compare a supervised
learning with a supervised learning. However, our
baseline is not the simple Euclidean distance; it
is the Jaccard coefficient and cosine similarity, a
handcrafted, best performing metric for synonym
acquisition, with 10 times as many features.
The computational resources required to obtain
the Mahalanobis L2 results were as follows: in the
training phase, each fold of cross validation took
about 80 iterations (less than one week) to con-
verge on a Xeon 5160 3.0GHz. The time required
to use the learned distance was a few hours at most.
At first, we were unable to perform competi-
tively with the Euclidean distance. As seen in Ta-
ble 1, the TOP1 measure of the Euclidean distance
is only 1.732%. This indicates that the likelihood
of finding the first item on the ranked list to be a
true synonym is 1.732%. The vector-based Jac-
card coefficient performs much better than the Eu-
clidean distance, placing a true synonym at the top
of the list 30.736% of the time.
Table 2 shows the Top 10 Words for Query
?branch?. The results for the Euclidean distance
rank ?hut? and other dissimilar words highly. This
is because the norm of such vectors is small, and in
a high dimensional space, the sparse vectors near
the origin are relatively close to many other sparse
vectors. To overcome this problem, we normal-
ized the input vectors by the L2 norm x? = x/||x||
This normalization enables the Euclidean distance
to perform very much like the cosine similarity,
since the Euclidean distance between points on a
sphere acts like the angle between the vectors. Sur-
prisingly, normalization by L2 did not affect other
metrics all that much; while the performances of
some metrics improved slightly, the L2 normaliza-
tion lowered that of the Jaccardv metric.
Once we learned the normalization trick, the
learned Mahalanobis distance consistently outper-
formed all other metrics, including the ones with
10 times more features, in all three evaluation
measures, achieving an APR of 18.66%, RKL of
545.09 and TOP1 of 45.455%.
6 Discussion
Examining the learned Mahalanobis matrix re-
vealed interesting features. The matrix essentially
shows the covariance between features. While it
was not as heavily weighted as the diagonal ele-
ments, we found that its positive non-diagonal el-
ements were quite interesting. They indicate that
some of the useful features for finding synonyms
are correlated and somewhat interchangeable. The
example includes a pair of features, (dobj begin
*) and (dobj end *). It was a pleasant surprise to
see that one implies the other. Among the diag-
onal elements of the matrix, one of the heaviest
features was being the direct object of ?by?. This
indicates that being the object of the preposition
?by? is a good indicator that two words are simi-
lar. A closer inspection of the NYT corpus showed
that this preposition overwhelmingly takes a per-
son or organization as its object, indicating that
words with this feature belong to the same class
of a person or organization. Similarly, the class
798
Metric APR RKL TOP1
Cosine 0.1184 ? 0.1324 580.27 ? 579.00 0.2987 ? 0.3160
Euclidean 0.0229 ? 0.0173 662.74 ? 695.71 0.0173 ? 0.0000
Euclidean L2 0.1182 ? 0.1324 580.30 ? 578.99 0.2943 ? 0.3160
Jaccard 0.1120 ? 0.1264 580.76 ? 579.51 0.2684 ? 0.2943
Jaccard L2 0.1113 ? 0.1324 580.29 ? 570.88 0.2640 ? 0.2987
Jaccardv 0.1189 ? 0.1318 580.50 ? 580.19 0.3073 ? 0.3030
Jaccardv L2 0.1184 ? 0.1254 580.27 ? 570.00 0.2987 ? 0.3160
JS 0.0199 ? 0.0170 681.97 ? 700.53 0.0129 ? 0.0000
JS L2 0.0229 ? 0.0173 679.21 ? 699.00 0.0303 ? 0.0086
Manhattan 0.0181 ? 0.0168 687.73 ? 701.47 0.0043 ? 0.0000
Manhattan L2 0.0185 ? 0.0170 686.56 ? 701.11 0.0043 ? 0.0086
SD99 0.0324 ? 0.1039 640.71 ? 588.16 0.0173 ? 0.2640
SD99 L2 0.0334 ? 0.1117 633.32 ? 586.78 0.0216 ? 0.2900
Mahalanobis L2 0.1866 545.09 0.4545
Table 1: Evaluation of Various Metrics, as Number of Features Increase from 1,281 to 12,812
Cosine Euclidean Euclidean L2 Jaccard Jaccardv Mahalanobis L2
1 (*) office hut (*) office (*) office (*) office (*) division
2 area wild area border area group
3 (*) division polish (*) division area (*) division (*) office
4 border thirst border plant border line
5 group hollow group (*) division group period
6 organization shout organization mouth organization organization
7 store fold store store store (*) department
8 mouth dear mouth circle mouth charge
9 plant hate plant stop plant world
10 home wake home track home body
(*) = a true synonym
Table 2: Top 10 Words for Query ?branch?
of words that ?to? and ?within?, take as an objects
were clear from the corpus: ?to? takes a person
or place, ?within? takes duration of time 3. Other
heavy features includes being the object of ?write?
or ?about?. While not obvious, we postulate that
having these words as a part of the context indi-
cates that a word is an event of some type.
7 Conclusion
We applied metric learning to automatic synonym
acquisition for the first time, and our experiments
showed that the learned metric significantly out-
performs existing similarity metrics. This outcome
indicates that while we must resort to feature se-
lection to apply metric learning, the performance
gain from the supervised learning is enough to off-
set the disadvantage and justify its usage in some
applications. This leads us to think that a com-
bination of the learned metric with unsupervised
metrics with even more features may produces the
best results. We also discussed interesting features
found in the learned Mahalanobis matrix. Since
3Interestingly, we note that not all prepositions were as
heavy: ?beyond? and ?without? were relatively light among
the diagonal elements. In the NYT corpus, the class of words
they take was not as clear as, for example, ?by?.
metric learning is known to boost clustering per-
formance in a semi-supervised clustering setting,
we believe these automatically identified features
would be helpful in assigning a target word to a
word class.
References
T. Briscoe, J. Carroll and R. Watson. 2006. The Sec-
ond Release of the RASP System. Proc. of the COL-
ING/ACL 2006 Interactive Presentation Sessions,
77?80.
T. Briscoe, J. Carroll, J. Graham and A. Copestake,
2002. Relational evaluation schemes. Proc. of the
Beyond PARSEVAL Workshop at the Third Interna-
tional Conference on Language Resources and Eval-
uation, 4?8.
A. Budanitsky and G. Hirst. 2006. Evaluat-
ing WordNet-based measures of semantic distance.
Computational Linguistics, 32(1):13?47.
R. Caruana, T. Jachims and L. Backstrom. 2004. KDD-
Cup 2004: results and analysis ACM SIGKDD Ex-
plorations Newslatter, 6(2):95?108.
C. J. Croach and B. Yang. 1992. Experiments in au-
tomatic statistical thesaurus construction. the 15th
799
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
77?88.
J. R. Curran and M. Moens. 2002. Improvements in
automatic thesaurus extraction. In Workshop on Un-
supervised Lexical Acquisition. Proc. of the ACL
SIGLEX, 231?238.
J. V. Davis and I. S. Dhillon. 2006. Differential En-
tropic Clustering of Multivariate Gaussians. Ad-
vances in Neural Information Processing Systems
(NIPS).
J. V. Davis, B. Kulis, P. Jain, S. Sra and I. S. Dhillon.
2007. Information Theoretic Metric Learning. Proc.
of the International Conference on Machine Learn-
ing (ICML).
A. Globerson and S. Roweis. 2005. Metric Learning by
Collapsing Classes. Advances in Neural Information
Processing Systems (NIPS).
J. Goldberger, S. Roweis, G. Hinton and R. Salakhut-
dinov. 2004. Neighbourhood Component Analysis.
Advances in Neural Information Processing Systems
(NIPS).
G. Grefenstette. 1994. Explorations in Automatic The-
suarus Discovery. Kluwer Academic Publisher.
M. Hagiwara, Y. Ogawa, and K. Toyama. 2008. Con-
text Feature Selection for Distributional Similarity.
Proc. of IJCNLP-08, 553?560.
Z. Harris. 1985. Distributional Structure. Jerrold
J. Katz (ed.) The Philosophy of Linguistics. Oxford
University Press. 26?47.
T. Hastie and R. Tibshirani. 1996. Discriminant adap-
tive nearest neighbor classification. Pattern Analysis
and Machine Intelligence, 18, 607?616.
D. Hindle. 1990. Noun classification from predicate-
argument structures. Proc. of the ACL, 268?275.
J. J. Jiang and D. W. Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxon-
omy. Proceedings of International Conference on
Research on Computational Linguistics (ROCLING
X), Taiwan.
Y. Jing and B. Croft. 1994. An Association The-
saurus for Information Retrieval. Proc. of Recherche
d?Informations Assiste?e par Ordinateur (RIAO),
146?160.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. Proc. of COLING/ACL 1998, 786?
774.
L. Lee. 1999. Measures of distributional similarity.
Proc. of the ACL, 23?32
L. Lee. 2001. On the Effectiveness of the Skew Diver-
gence for Statistical Language Analysis. Artificial
Intelligence and Statistics 2001, 65?72.
S. Mohammad and G. Hirst. 2006. Distributional mea-
sures of concept-distance: A task-oriented evalua-
tion. Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP),
Sydney, Australia.
P. Resnik. 1995. Using information content to evaluate
semantic similarity. Proceedings of the 14th Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-95), 448?453, Montreal, Canada.
G. Ruge. 1997. Automatic detection of thesaurus re-
lations for information retrieval applications. Foun-
dations of Computer Science: Potential - Theory -
Cognition, LNCS, Volume 1337, 499?506, Springer
Verlag, Berlin, Germany.
S. Shalev-Shwartz, Y. Singer and A. Y. Ng. 2004. On-
line and Batch Learning of Pseudo-Metrics. Proc. of
the International Conference on Machine Learning
(ICML).
M. Schutz and T. Joachims. 2003. Learning a Dis-
tance Metric from Relative Comparisons. Advances
in Neural Information Processing Systems (NIPS)..
J. Weeds, D. Weir and D. McCarthy. 2004. Character-
ising Measures of Lexical Distributional Similarity.
Proc. of COLING 2004, 1015?1021.
K. Q. Weinberger, J. Blitzer and L. K. Saul. 2005.
Distance Metric Learning for Large Margin Nearest
Neighbor Classification. Advances in Neural Infor-
mation Processing Systems (NIPS).
E. P. Xing, A. Y. Ng, M. Jordan and S. Russell 2002.
Distance metric learning with application to cluster-
ing with sideinformation. Advances in Neural Infor-
mation Processing Systems (NIPS).
Y. Yang and J. O. Pedersen. 1997. A Comparative
Study on Feature Selection in Text Categorization.
Proc. of the International Conference on Machine
Learning (ICML), 412?420.
800
