AUGMENTED PHRASE STRUCTURE GRAMMARS 
George E. Heidorn 
Computer Sciences Department 
IBM Thomas J. Watson Research Center 
Yorktown Heights, NY 
ABSTRACT 
Augmented phrase structure grammars 
consist of phrase structure rules ~with 
embedded condit ions and structure-bui ld ing 
actions written in a special ly developed 
language. An attr ibute-value, 
record-or iented information structure is an 
integral part of the theory. 
I. INTRODUCTION 
An augmented phrase structure grammar 
(APSG) consists of a col lection of phrase 
structure rules which are augmented by 
arbitrary condit ions and structure bui lding 
actions. This basic idea is not new, having 
been used in syntax-directed compil ing \[e.g. 
I\] as well as in natural  language processing 
\[e.g. 2\], but what is new are the 
part icular language in which these rules are 
written and the algor i thms that apply them. 
This brief paper is intended to serve 
as an introduct ion to augmented phrase 
structure grammars. First, the form of data 
structure used is discussed, fol lowed by 
discussions of the analysis and synthesis of 
text, i.e. decoding and encoding. 
(Although this session of the workshop is 
devoted to natural language inout, this 
brief discussion of synthesis is included 
because one of the important features of 
APSG is the consistent manner in which both 
decoding and encoding are specif ied.) Then 
there is a section on implementat ions and 
applications, fol lowed by concluding 
remarks. 
II. DATA STRUCTURE 
The data structure used by APSG is a 
form of semantic network, consist ing of 
"records" which are col lect ions of 
attr ibute-value pairs. Records represent 
entities, either physical or abstract, such 
diverse things as vehicles, actions, words 
and verb phrases. There are three dif ferent 
kinds of attr ibutes: relations, which have 
as their values pointers to other records; 
properties, which have as their values 
either numbers or character strings; and 
indicators, which have bit string values and 
usual ly serve in a role similar to features 
in l inguistic terminology. 
A record that has a NAME attr ibute is 
called a "named record" and can be referred 
to by using the value of the NAME attr ibute 
in single quotes. Named records are used to 
hold information that is re lat ively 
permanent, such as information about 
relevant words and concepts, and are defined 
in the fol lowing manner (where the 
parentheses enclose structure-bui ld ing 
information): 
SERVIC ( 'ACTIVITY' ,E,ES, ING,ED, 
TRANS,PS='VERB' ,XYZ=3) 
It is convenient to picture a record as 
a box enclosing a column of relation and 
property names on the left and a column of 
corresponding values on the right. 
Indicators which are present in the record 
(i.e. have a non-zero value) are listed at 
the bottom of the box. The named record 
"SERVIC" defined above could be drawn as: 
I NAME "SERVIC" SUP "ACTIVITY" PS "VERB" XYZ 3 
|E,ES, ING,ED,TRANS 
Double quotes enclose a character string, 
single quotes enclose the name of a named 
record. The values of the SUPerset and PS 
(part-of-speech) attr ibutes are real ly 
pointers to the records "ACTIVITY" and 
"VERB" and could be drawn as directed lines 
to those other records if they were included 
in the diagram. 
The named record "SERVIC" given here 
could be considered to be a dict ionary entry 
stating that the VERB stem SERVIC can take 
endings E, ES, ING and ED, the VERB SERVIC 
is TRANSitive, and the concept SERVIC is an 
ACTIVITY. (When a named record name appears 
without the expl icit  mention of an attr ibute 
name, the SUPerset attr ibute is assumed.) 
The XYZ attr ibute was included just to 
i l lustrate a numerical ly-valued property. 
Of course, the true meaning of any of this 
information depends completely upon the way 
it is used by the APSG rules. 
During decoding and encoding, records 
called "segment records" are employed to 
hold information about segments of text. 
For example, the segment "are servicing" 
could be descr ibed by the record: 
I SUP "SERVIC" PRES,P3,PLUR,PROG 
which could be interpreted as saying that 
"are servicing" is the present, third 
person, plural, progressive form of 
"service". Similarly, the sentence "The big 
men are servic ing a truck." could be 
described by: 
ISUP "MAN" 
SUP "SERVIC" 1 ~S IZE  "BIG" 
AGENT i 
GOAL L I ~ ~  
PRES,PROG ~ S U P  "TRUCK" 
' ~INDEF,S ING 
where the indicators DEF and INDEF mean 
definite and indefinite, respectively. The 
sentence "A truck is being serviced by the 
big men." could be described by exactly the 
same record structure but with the addit ion 
of a PASSIVE indicator in the record on the 
left. 
During a dialogue some records that 
begin as segment records may be kept to 
become part of longer term memory to 
represent the entit ies (in the broadest  
sense of the term) that are being discussed. 
Segment records then might have pointers 
into this longer term memory to show 
referrents. So, for example, the sentence 
"They are servic ing a truck." might be 
described by the same record structure shown 
above if the referrent of "they" was known 
to be a certain group of men who are big. 
III. ANALYSIS OF TEXT (DECODING) 
Decoding is the process by which record 
structures of the sort just shown are 
constructed from strings of text. The 
manner in which these records are to be 
built is specif ied by APSG decoding rules. 
A decoding rule consists of a list of one or 
more "segment types" (meta-symbols) on the 
left of an arrow to indicate which types of 
contiguous segments must be present in order 
for a segment of the type on the right of 
the arrow to be formed. Condit ions which 
must be satisf ied in order for the rule to 
be appl icable may be stated in parentheses 
on the left side of the rule, and 
structure-bui ld ing operations to be 
performed when a new segment record is 
created are stated in parentheses on the 
right side. 
For i l lustrat ive purposes, some o f  the 
rules which would be required to produce the 
segment records shown in the previous 
section will be discussed here. Complete 
examples are given in Reference 3. 
If the str ing "servicing" appeared in 
the input, and the substr ing "servic" were 
described by the VERBSTEM segment record 
I SUP "SERVIC" I 
then the rule 
VERBSTEM(ING*) I N G --> 
VERB(SUP(VERBSTEM,PRESTART)  
would form the VERB segment record 
I SUP "SERVIC" 1 
.PRESPART 
to describe the str ing "servicing", 
ident i fy ing it as the present part ic iple 
form of service. This rule says that if a 
segment of the str ing being decoded is 
described as a VERBSTEM, and the associated 
segment record has a SUP attr ibute which 
points to a named record which has an ING 
indicator (as the named record for "SERVIC" 
defined in the previous section would), and 
this segment is fol lowed immediately by the 
characters "i", "n" and "g", then create a 
VERB segment record with the same SUP as the 
VERBSTEM and with a PRESPART indicator, to 
describe the entire segment ("servicing" in 
this case). 
Then the rule 
VERB --> VERBPH(~VERB) 
would create aVERBPHrase segment record 
which is a copy (4) of the VERB segment 
record just shown. 
If the str ing "are" 
input were descr ibed 
record 
I SUP "BE" .PRES,P3,PLUR 
then the rule 
appearing in the 
by the VERB segment 
VERB('BE') VERBPH(PRESPART) --> 
VERBPH(PROG,FORM=FORM(VERB))  
would produce the new VERBPH segment record 
I sup SERWC I 
PRES,P3,PLUR,PROG I 
from the twojust shown, to describe the 
str ing "are servicing". This rule says that 
if a segment of the str ing being decoded is 
described as a VERB with a SUP of "BE', and 
it is fol lowed by a segment described as a 
VERBPH with a PRESPART indicator, then 
create a new VERBPH segment record which is 
a copy (automatical ly,  because the segment 
type is the same) of the VERBPH segment 
record referred to on the left of the rule, 
but which as a PROGressive indicator and the 
FORM information from the VERB. FORM would 
have previously been defined as the name of 
a group of indicators (i.e. those having to 
do with tense, person and number). S imi lar  
rules can be used to recognize passives, 
perfects and modal construct ions.  
Cont inuing with the example, if the 
str ing "the big men" were decoded to the 
NOUNPH segment record 
I SUP "MAN" 
SIZE "BIG" 
DEF,PLUR 
then the rule 
NOUNPH VERBPH(NUMB.EQ.NUMB(NOUNPH), SUBJECT) 
--> VERBPH(SUBJECT=NOUNPH,-NUMB,-PERS) 
would produce the new VERBPH segment record 
(the one on the left in this diagram) 
I SUP "SERVIC" ISUP "MAN" ISUBJECT ~ SIZE "BIG" PRES,PROG DEF,PLUR 
from the previous VERBPH record, to descr ibe 
the str ing "the big men are servicing". It 
is important to real ize that the record on 
the left in the above diagram is a segment 
record that "covers" the entire str ing and 
that the record shown on the right (which is 
the same one from the previous diagram) Just 
serves as the value of its SUBJECT 
attr ibute. The rule above says that if a 
NOUNPH is fol lowed by a VERBPH, and the 
NUMBer indicators of the VERBPH are the same 
as the NUMBer indicators of the NOUNPH, and 
the VERBPH does not already have a SUBJECT 
attribute, then create a new VERBPH segment 
record which is a copy of the old one, give 
it a SUBJECT attr ibute point ing to the 
NOUNPH record, and delete the NUMBer and 
I 
I 
t 
t 
! 
i 
I 
i 
i 
I 
I 
i 
I 
! 
I 
t 
I 
t 
ii 
PERson indicators. Considering the subject 
to be part of the verb phrase in this manner 
can s impl i fy the handling of some 
construct ions involv ing inverted word order. 
If the string being decoded were "the 
big men are servic ing a truck.", a rule 
similar to the last one shown above could be 
used to pick up the direct object. Then the 
rule 
. /VERBPH(SUBJECT,OBJECTI -TTRANS*IPASSIVE). 
--> SENT (~VERBPH) 
could be applied, which says if a VERBPH 
extending between two periods has a SUBJECT 
attr ibute and also either has an OBJECT 
attr ibute or does not need one because there 
is no TRANSit ive indicator in the named 
record pointed to by the SUP (i.e. the verb 
is intransit ive) or because there is a 
PASSIVE indicator, then call it a SENTence. 
To get the record structure descr ib ing 
this string into the form shown near the end 
of the previous section, one more rule would 
be needed: 
SENT($ 'ACTION' ,~PASSIVE,SUBJECT)  --> 
SENT(AGENT=SUBJECT,GOAL=OBJECT,  
-SUBJECT,-OBJECT) 
This says that for a non-PASSIVE ACTION 
SENTence that sti l l  has a SUBJECT attr ibute, 
set the AGENT and GOAL attr ibutes to the 
values of the SUBJECT and OBJECT attr ibutes, 
respectively, and then delete the SUBJECT 
and OBJECT attr ibutes from the record. The 
notat ion $'ACTION" is read "in the set 
"ACTION'" and means that the named record 
"ACTION" must appear somewhere in the 
SUPerset chain of the current record. In 
the previous section the named record 
"SERVIC" was def ined to have a SUP of 
"ACTIVITY'. If the named record "ACTIVITY" 
were s imi lar ly def ined to have a SUP of 
"ACTION', the segment record under 
discussion here would satisfy the condit ion 
$'ACTION'. 
From the above examples it can be seen 
that the condit ion speci f icat ions take the 
form of logical expressions involving the 
values of attr ibutes. Each element in a 
condit ion speci f icat ion is basicaly of the 
form value.re lat ion.value,  but this is not 
obvious because there are several notat ional  
shortcuts avai lable in the rule language. 
For example, "BE" is short for 
SUP.EG. 'BE ' ,PRESPART is short for 
PRESPART.NE.0,  and -~SUBJECT is short for 
SUBJECT.EQ.0. The elements are combined by 
and's (commas) and or's (vertical bars). 
In most cases the attr ibute whose value 
is being tested is to be found in the 
segment record associated with the 
constituent, but that is not always the 
case. For example, ING* tests the value of 
the ING indicator in the named record 
pointed to by the SUP of the segment record, 
and could be written ING(SUP) or 
ING(SUP).NE.0. Another example is 
NUMB(NOUNPH) which was used to refer to the 
value of the NUMB indicators in the NOUNPH 
segment in one of the rules above. 
From the examples it can also be seen 
that creation speci f icat ions take the form 
of short procedures consist ing of s ta tements  
for sett ing the values of attr ibutes. Each 
element in a creat ion speci f icat ion is 
basical ly of the form attr ibute=value (where 
"=" means replacement),  but again this is 
not obvious becuase of the notat ional  
shortcuts used. For example, SUP(VERBSTEM) 
is short for SUP=SUP(VERBSTEM),  PRESPART is 
short for PRESPART:I  (note that this form 
has a different meaning when it is used in a 
condit ion specif icat ion),  and -SUBJECT is 
short for SUBJECT=0. 
In all of the examples here, the 
attr ibute whose value is set would be in the 
segment record being built, but that need 
not always be the case. If, for example, 
there were some reason to want to give the 
AGENT record of an action an ABC attr ibute 
equal to one more than the XYZ attr ibute of 
the concept record associated with that 
act ion (i.e. the named record pointed to by 
its SUP), the fo l lowing could be included in 
the last rule shown: 
ABC(AGENT)=XYZ(SUP)+I  
which can be read as "set the ABC attr ibute 
of the AGENT of this record to the value of 
the XYZ attr ibute of the SUP of this record 
plus I." There is no limit to the nesting of 
attr ibute names used in this manner. 
Although in the example rules given 
here the condit ions are primari ly syntactic, 
semantic constra ints  can be stated in 
exact ly the same manner. Much of the record 
bui lding shown here can be considered 
semantic (and somewhat case oriented). The 
important point, however, is that the kind 
of condit ion test ing and structure bui ld ing 
done is at the discret ion of the person who 
writes the rules. Complete speci f icat ions 
for the APSG rule language are given in 
Reference 3. 
The decoding a lgor i thm used with APSG 
is basical ly  that of a bottom-up, 
left-to-r ight,  paral le l -processing,  
syntax-d i rected compiler. An important and 
novel feature of this a lgor i thm is something 
called a "rule instance record", which 
pr imari ly  mainta ins information abut the 
potential  appl icab i l i ty  of a rule. A rule 
instance record is in i t ia l ly  created for a 
rule whenever a segment which can be the 
first const i tutent  of that rule becomes 
avai lable. (A terminal  segment becomes 
avai lable by being obtained from the input 
stream, and a non-terminal  segment becomes 
avai lable whenever a rule is applied.) Then 
the rule instance record "waits" for a 
segment which can be the next const i tuent of 
the associated rule to become avai lable. 
When such a segment becomes available, the 
rule instance record is "extended". When a 
rule instance record becomes complete (i.e. 
all of its const i tuents are avai lable),  the 
associated rule is appl ied (i.e. the 
segment record speci f ied on the right is 
built and made avai lable).  There may be 
many rule instance records in existence for 
a part icular  rule at any point in time. 
Because of the parallel processing 
nature of the decoding algorithm, when a 
segment record is created to describe a 
portion of the input text it does not result 
in the destruct ion of other records 
describing the same portion or parts of it. 
Local ambiguit ies caused by mult iple word 
senses, idioms and the like may result in 
more than one segment record being created 
to describe a part icular portion of the 
text, but usual ly only one of them is able 
to combine with its neighbors to become part 
of the analysis for an entire sentence. 
IV. SYNTHESIS OF TEXT (ENCODING) 
Encoding is the process  by which 
strings of text are produced from record 
structures of the sort already shown. The 
manner in which this processing is to be 
done is specif ied by APSG encoding rules. 
The right side of an encoding rule specif ies 
what segments a segment of the type on the 
left s ide  is to be expanded into. 
Condit ions and structure-bui ld ing actions 
are included in exactly the same manner as 
in decoding rules. 
The encoding a lgor i thm begins with a 
single segment record and its associated 
type s ide-by-side on a stack. At each cycle 
through the algorithm, the top pair is 
removed from the stack and examined. If 
there is a rule that can be applied, it 
results in new pairs being put on the top of 
the stack, according to its right hand side. 
Otherwise, either the character string value 
of the NAME attr ibute of the SUP of the 
segment record (e.g. "servic") is put out, 
or the name of the segment type itself  (e.g. 
"I") is put out. Eventual ly the stack 
becomes empty and the algor ithm terminates, 
having produced the desired output string. 
For example, if at some point the 
fol lowing pair were to come off the top of 
the stack: 
VERBPH I SUP "SERVIC" I 
PRES,P3,PLUR,PROG 
the fol lowing encoding rule could be 
applied: 
VERBPH(PROG) --> 
VERB('BE',FORM:FORM(VERBPH)) 
VERB(-PROG,-FORM,PRESPART) 
result ing in the fol lowing two pairs being 
put on the top of the stack: 
VERB ISUP "BE" 
PRES,P3,PLUR 
VERBPH I SUPPRESPART'SERVIC" 1 
The above rule says that a VERBPH segment 
with a PROGressive indicator should be 
expanded into a VERB segment with a SUP of 
"BE" and the same FORM indicators as the 
VERBPH, fol lowed by a new VERBPH segment 
which begins as a copy (automatical ly) of 
the old one and then is modif ied by delet ing 
the PROG and FORM indicators and setting the 
PRESPART indicator. 
When the VERB segment shown above comes 
off the stack, a rule would be applied to 
put the string "are" into the output. Then, 
after appl icat ion of a couple more rules, 
the top of the stack would have the four 
pairs 
VERBSTEM \[ SUP "SERVIC" i 
I null 
N null 
G null 
which would result in the string "servicing" 
being produced after four cycles of the 
algorithm. Complete encoding examples may 
be found in Reference 3. 
V. IMPLEMENTATIONS AND APPLICATIONS 
As part of the original  work on APSG a 
computer system called NLP (~atural Language 
~rocessor)  was developed in 1968. This is a 
FORTRAN program for the IBM 360/370 
computers which wil l  accept as input named 
record def init ions and decoding and encoding 
rules in exact ly the form shown in this 
paper and then perfor m decoding and encoding 
of text \[3\]. A set of about 300 named 
record def in i t ions and 800 rules was written 
for NLP to implement a specif ic system 
(called NLPQ) which is capable of carrying 
on a dialogue in Engl ish about a simple 
queuing problem and then producing a program 
in the GPSS s imulat ion language to solve the 
problem \[3,4\]. 
More recently a LISP implementat ion of 
NLP has been done, which accepts exactly the 
same input and does the same processing as 
the FORTRAN version. An interest ing feature 
of this new version is that the compiler 
part, whose pr imary task is to translate 
condit ion and creat ion speci f icat ions (i.e. 
the information in parentheses) into lambda 
expressions, is itself  wr i t ten as a set of 
APSG rules. This work is part of a project 
at IBM Research to develop a system which 
wil l  produce appropr iate account ing 
appl icat ion programs after carrying on a 
natural  language dialogue with a businessman 
about his requirements.  APSG is also being 
used in the development of a natural  
a laaguage query system for relat ional  data 
bases and is being considered for use in 
other projects at IBM. None of this recent 
work has been documented yet. 
VI. CONCLUDING REMARKS 
APSG clearly has much in common with 
other current computat ional  l inguist ic 
theories, with the ideas of procedural  
speci f icat ion and arbitrary condit ions and 
st rucutre-bui ld ing actions being very 
popular at this time. It would seem to be 
most similar to Woods" augmented transit ion 
networks (ATN) \[5\], especial ly  as used by 
Simmons \[6\]. Registers in the ATN model 
correspond closely to attr ibutes of segment 
records in APSG, and the semantic network 
structures of Simmons are very close to the 
record structures of APSG. 
! 
! 
i 
| 
! 
! 
! 
p! 
| 
! 
I 
! 
! 
! 
D 
! 
! 
Context-free phrasestructure  grammars 
have been known to be inadequate for 
describing natural languages for many years, 
and context-sensitive phrase structure 
grammars have not been found to be very 
useful, either. Augmented phrase structure 
grammars, however, appear to be able to 
express the facts of a natural language in a 
very concise and convenient manner, they 
have the power of computer programs, while 
maintaining the appearance of grammars. 
Although APSG was used successfully to 
implement one fairly large system (NLPQ), it 
is too early to do a thorough appraisal of 
its capabilities. Through the extensive use 
anticipated in the next year however, its 
strengths and weaknesses should become more 
apparent. 
ACKNOWLEDGEMENTS 
I am indebted to my former students at the 
Naval Postgraudate School for their efforts 
on the original implementation and 
application, my colleagues at IBM 
Research -- Martin Mikelsons, Peter 
Sheridan, Irving Wladawsky and Ted 
Codd -- for their interest, ideas and work 
on the current implementatons and 
applications, and my wife, Beryl, for her 
typing assistance and general helpfulness. 
REFERENCES 
I. Balzer, R.M., and Farber, D.J., 
"APAREL - a parse-request language," 
COMM. ACM 12, 11 (Nov. 1969), 624-631. 
2. Thompson, F.B., Lockemann, P.C., Dostert, 
B., Deverill, R.S., "REL: a rapidly 
extensible language system," In PROC. 
24th NAT'L CONF., ACM, NY, 1969, 399-417. 
3. Heidorn, G.E., "Natural language inputs 
to a simulation programming system," 
Technical Report NPS-55HD72101A, Nava l  
Postgraduate School, Monterey, 
California, Oct. 1972. 
4. Heidorn, G.E., "English as a very high 
level language for simulation 
programming," Proc. Symp. on Very High 
Level Languages, SIGPLAN NOTICES 9,4 
(April 1974), 91-100. 
5. Woods, W.A., "Transition network grammars 
for natural language analysis," COMM. 
ACM 13, 10 (Oct. 1970), 591-606. 
6. Simmons, R.F., "Semantic networks: their 
computation and use for understanding 
English sentences," in COMPUTER MODELS OF 
THOUGHT AND LANGUAGE, R.C. Schank and 
K.M. Colby (Eds.), W.H. Freeman and 
Co., San Francisco, Calif., 1973, 63-113. 
! 
DIAGNOSIS AS A NOTION OF GRAMMAR 
Mitchel l  Marcus 
Art i f ic ia l  Intel l igence Laboratory 
M.I.T. 
This paper will sketch an approach to 
natural language parsing based on a new 
conception of what makes up a recognit ion 
grammar for syntactic analysis and how such 
a grammar should be structured. This theory 
of syntactic analysis formalizes a notion 
very much like the psychologist 's  notion of 
"perceptual strategies" \[Bever "70\] and 
makes this formal ized notion - which will be 
called the notion of wait -and-see 
diagnostics - a central and integral part of 
a theory of what one knows about the 
structure of language. By recognit ion 
grammar, we mean here what a speaker of a 
language knows about that language that 
allows him to assign grammatical  structure 
to the word strings that make up utterances 
in that language. 
This theory of grammar is based on the 
hypothesis that every language user knows as 
part of his recognit ion grammar a set of 
highly specif ic diagnostics that he uses to 
decide determinist ica l ly  what structure to 
build next at each point in the process of 
parsing an utterance. By determinist ica l i?  
I mean that once grammatical  structure is 
built, it cannot be discarded in the normal 
course of the parsing process, i.e. that no 
"backtracking" can take place unless the 
sentence is consciously perceived as being a 
"garden path". This notion of grammar puts 
knowledge about control l ing the parsing 
process on an equal footing with knowledge 
about its possible outputs. 
To test this theory of grammar, a 
parser has been implemented that provides a 
language for writ ing grammars of this sort, 
and a grammar for Engl ish is current ly being 
written that attempts to capture the 
wait -and-see diagnost ics needed to parse 
Engl ish within the constraints of the  
theory. The control  structure of the parser 
strongly ref lects the assumptions the theory 
makes about the structure of language, and 
the discussion below wil l  use the structure 
of the parser as an example of the 
impl icat ions of this theory for the parsing 
process. The current grammar of Engl ish is 
deep but not yet broad; this has al lowed 
invest igat ion of the sorts of wait -and-see 
diagnost ics needed to handle complex Engl ish 
construct ions without a need to wait unti l  a 
grammar for the entire range of Engl ish 
construct ions could be written. To give 
some idea of the scope of the grammar, the 
parser is capable of handling sentences 
like: 
Do all the boys the l ibrarian gave 
books to want to read them? 
The men John wanted to be bel ieved by shot 
him yesterday. 
It should be ment ioned that certain 
grammatical  phenomena are not handled at all 
by the present grammar, chief among them 
conjunct ion and certain important sorts of 
lexical ambiguity. There is every 
intention, however, of expanding the grammar 
to deal with them. 
Two Paradigms 
To explain exactly what the details of 
this wait -and-see (W&S) paradigm are, it is 
useful to compare this notion with the 
current prevai l ing parsing paradigm, which I 
wil l  call the guess-and-then-backup (G&B) 
paradigm. This paradigm is central to the 
parsers of both Terry' Winograd's SHRDLU 
\[Winograd "72\] and Bill Woods" LUNAR \[Woods 
"72\] systems. 
In a parser based on the G&B paradigm, 
various options are enumerated in the 
parser's grammar for the next possible 
const i tuent at any given point in the parse 
and these options are tested one at a time 
against the input. The parser assumes 
tentat ively that one of these options is 
correct and then proceeds with this option 
until  either the parse is completed or the 
option fails, at which point the parser 
simply backs up and tries the next option 
enumerated in the parser's grammar. This is 
the paradigm of G&B: enumerate all options, 
pick one, and then (if it fails) backup and 
pick another. While attempts have been made 
to make this backup process clever, 
especial ly  in Winograd's  SHRDLU, it seems 
that it is very diff icult,  if not impossible 
in general, to tell from the nature of the 
cul de sac exactly where the parser has gone 
astray. In order to parse a sentence of 
even moderate complexity, there are not one 
but many points at which a G&B parser must 
make guesses about what sort of structure to 
expect next and at all of these points the 
correct hypothesis must be found before the 
parse can be successful ly  completed. 
Furthermore, the parser may proceed 
arb i t rar i ly  far ahead on any of these 
hypotheses before discover ing that the 
hypothesis  was incorret, perhaps 
inval idat ing several other hypotheses 
contingent upon the first. In essence, the 
G&B paradigm considers the grammar of a 
natural  language to be a t ree-structured 
space through which the parser must blindly, 
though perhaps cleverly, search to find a 
correct parse. 
The W&S paradigm rejects the notion of 
backup as a standard control mechanism for 
parsing. At each point in the parsing 
process, a W&S parser wil l  only build 
grammatical  structure it is sure it can use. 
The parser does this by determining, by a 
two part process, which of the hypotheses 
possible at any given point of the parse is 
correct before attempting any of them. The 
parser first recognizes the specif ic  
s i tuat ion it is in, determined both on the 
basis of global expectat ions result ing from 
whatever structure it has parsed and 
absorbed, and from features of lower level 
substructures from a l ittle ahead in the 
input to which internal structure can be 
assigned with certainty but whose function 
is as yet undetermined. Each such s i tuat ion 
can be so defined that it restrains the set 
of possible hypotheses to at most two or 
three. If only one hypothesis is possible, 
a W&S parser wil l  take it as given, 
otherwise it wil l  proceed to the second step 
! 
! 
i 
! 
! 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
i 
I 
I 
! 
I 
of the determinat ion process ,  to do a 
~if ferential  diagnosis to decide between the 
competing hypotheses. For each dif ferent 
situation, a W&S grammar includes a series 
of easi ly computed tests that decides 
between the competing hypotheses. The key 
assumption of  the W&S paradigm, then? i_gs 
that the structure of  natural language 
provides enough and the right information t__~o 
~etermine exactly what too d__oo next at  each 
point of  ~ Parse. There is not suff ic ient 
room here to discuss this assumption; the 
reader is invited to read \[Marcus ~74\], 
which discusses this assumption at length. 
Th___~e Parser Itself 
? To  firm up this talk of "expectations',, 
"situations", and the like, it it useful to 
see how these notions are real ized in the 
exist ing W&S parsing system. Before we can 
do this, it wil l  be necessary to get an 
overview of the structure and operat ion of 
the parser itself. 
A grammar in this system is made up of 
packets of pattern- invoked demons, which 
wil l  be cal led modules. (The notion of 
packet here derives from work by Scott 
Fahlman \[Fahlman "73\].) The parser itself  
consists of two levels, a group level and a 
clause level, and any packet of modules is 
intended to function at one level or the 
o~her. Modules at group level are intended 
to work on a buffer of words and word level 
structures and to eventual ly bu i ld  group 
level structures, such as Noun Grouos (i.e. 
Noun Phrases up to the head noun) and Verb 
GrouPs (i.e. the verb cluster up to the 
main verb), which are then put onto the end 
of a buffer of group level structures not 
yet absorbed by higher level processes. 
Modules at clause level are intended to work 
on these substructures and to assemble them 
into clauses. The group buffer and the word 
buffer can both grow up to some 
predetermined length, on the order of 3, 4, 
or 5 structures. Thus the modules at the 
level above needn't immediately use each 
structure as it comes into the buffer; but 
rather can let a small number of structures 
"pile up" and then examine these structures 
before deciding how to use the first of 
them. In this sense the modules at each 
level have a l imited, sharply constra ined 
look-ahead abil ity; they can wait and see 
what sort of environment surrounds a 
substructure in the buffer below before 
deciding what the higher level funct ion of 
that substructure is. (It should be noted 
that the amount of look-ahead is constra ined 
not only by maximum buffer length but also 
by the restr ict ion that a module may access 
only the two substructures immediately 
fol lowing the one it is current ly trying to 
uti l ize. This constraint  is necessary 
because the substructure about to be 
ut i l ized at any moment may not be the first 
in the buffer, for various reasons.) 
Every module consists of a pattern, a 
pretest procedure, and a body to be executed 
if the pattern matches and the pretest 
succeeds. Each pattern consists of an 
ordered list of sets of features. As 
structures are built up by the parser, they 
are label led with features, where a feature 
is any property of a structure that the 
grammar wants to be visible at a glance to  
any module looking even casual ly at that 
structure. (Structures can also have 
registers attached to them, carrying more 
special ized sorts of information; the 
contents of a register are pr iv i leged in 
that a module can access the contents of a 
register only if it knows the name of that 
register.) A module's  pattern matches if the 
feature sets of the pattern are subsumed by 
the feature sets of consecut ive structures 
in the appropr iate buffer, with the match 
start ing at the effect ive beginning of the 
buffer. 
Very few modules in any W&S grammar ae 
always active, wait ing to be tr iggered when 
their patterns match; a module is active 
only when a packet it is in has been 
activated, i.e. added to the set of 
present ly active packets. Packets are 
act ivated or deact ivated by the parser at 
the specif ic order of individual  modules; 
any module can add or remove packets from 
the set of active packets if it has reason 
to do so. 
A pr ior i ty ordering of modules provides 
sti l l  further control. Every module is 
assigned a numerical  priority, creating a 
part ial  order ing on the active modules. At 
any time, only the h ighest -pr ior i t ied module 
of those whose patterns match wil l  be 
al lowed to run. Thus, a special  purpose 
module can edge out a general purpose module 
both of whose patterns match in a given 
environment,  or a module to handle some 
last-resort  case can lurk low in a pool of 
act ive modules, to serve as default only if 
no h lgher -pr ior i t ied  module responds to a 
situation. 
F irmin~ Up The Notion Of  Si tuat ion 
This, in brief, is the structure of the 
W&S parser; now we can turn to a discussion 
of how this structure ref lects the 
theoret ica l  f ramework discussed above. Let 
us begin by recast ing a statement made 
above: In decid ing what unique course of 
act ion to take at any point in the parse, 
the parser first recognizes a speci f ic  
wel l -def lned s i tuat ion on the basis of a 
combinat ion of global expectat ions and the 
specif ic  features of lower level 
substructures which are as yet unabsorbed. 
It should now become clear that what it 
means to have a global expectat ion is that 
the appropr iate packet is active in the 
parser, and that each module is i tself  the 
special ist  for the s i tuat ion that its 
packet, pattern and pretest define. The 
grammar act ivates and deact ivates packets to 
ref lect its global expectat ions about 
syntact ic structures that may be encountered 
as a result of what it has seen so far. 
(The parser might also act ivate packets on 
the basis of what some higher level process 
in a natural  language understanding system 
tells it to expect by way of d iscourse 
phenomena.)  These packets often ref lect 
rather large scale grammatical  expectat ions;  
for example, the fo l lowing are some packets 
