Coling 2010: Poster Volume, pages 1247?1255,
Beijing, August 2010
A Comparison of Models for Cost-Sensitive Active Learning
Katrin Tomanek and Udo Hahn
Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena
http://www.julielab.de
Abstract
Active Learning (AL) is a selective sam-
pling strategy which has been shown to
be particularly cost-efficient by drastically
reducing the amount of training data to be
manually annotated. For the annotation
of natural language data, cost efficiency
is usually measured in terms of the num-
ber of tokens to be considered. This mea-
sure, assuming uniform costs for all to-
kens involved, is, from a linguistic per-
spective at least, intrinsically inadequate
and should be replaced by a more ade-
quate cost indicator, viz. the time it takes
to manually label selected annotation ex-
amples. We here propose three differ-
ent approaches to incorporate costs into
the AL selection mechanism and evaluate
them on the MUC7T corpus, an extension
of the MUC7 newspaper corpus that con-
tains such annotation time information.
Our experiments reveal that using a cost-
sensitive version of semi-supervised AL,
up to 54% of true annotation time can be
saved compared to random selection.
1 Introduction
Active Learning (AL) is a selective sampling strat-
egy for determining those annotation examples
which are particularly informative for classifier
training, while discarding those that are already
easily predictable for the classifier given previous
training experience. While the efficiency of AL
has already been shown for many NLP tasks based
on measuring the number of tokens or sentences
that are saved in comparison to random sampling
(e.g., Engelson and Dagan (1996), Tomanek et al
(2007) or Settles and Craven (2008)), it is obvious
that just counting tokens under the assumption of
uniform annotation costs for each token is empir-
ically questionable, from a linguistic perspective,
at least.
As an alternative, we here explore annotation
costs that incur for AL based on an empirically
more plausible cost metric, viz. the time it takes
to annotate selected linguistic examples. We in-
vestigate three approaches to incorporate costs
into the AL selection mechanism by modifying
the standard (fully supervised) mode of AL and
a non-standard semi-supervised one according to
cost considerations. The empirical backbone of
this comparison is constituted by MUC7T , a re-
annotation of a part of the MUC7 newspaper
corpus that contains annotation time information
(Tomanek and Hahn, 2010).
2 Active Learning
Unlike random sampling, AL is a selective sam-
pling technique where the learner is in control of
the data to be chosen for training. By design, the
intention behind AL is to reduce annotation costs,
usually considered as the amount of labeled train-
ing material required to achieve a particular target
performance of the model. The latter is yielded
by querying labels only for those examples which
are assumed to have a high training utility. In this
section, we introduce different AL frameworks ?
the default, fully supervised AL approach (Sec-
tion 2.1), as well as a semi-supervised variant of
it (Section 2.2). In Section 2.3 we then propose
three methods how these approaches to AL can be
made cost-sensitive without further modifications.
1247
2.1 Fully Supervised AL (FuSAL)
As we consider AL for the NLP task of Named
Entity Recognition (NER), some design decisions
have to be made. Firstly, the selection granular-
ity is set to complete sentences ? a reasonable lin-
guistic annotation unit which still allows for fairly
precise selection. Second, a batch of examples in-
stead of a single example is selected per AL iter-
ation to reduce the computational overhead of the
sampling process.
We base our approach to AL on Conditional
Random Fields (CRFs), which we employ as base
learners (Lafferty et al, 2001). For observation
sequences ~x = (x1, . . . , xn) and label sequences
~y = (y1, . . . , yn), a linear-chain CRF is defined as
P?(~y|~x) =
1
Z?(~x)
?
n?
i=1
exp
k?
j=1
?jfj
(
yi?1, yi, ~x, i
)
where Z?(~x) is the normalization factor, and k
feature functions fj(?) with feature weights ? =
(?1, . . . , ?k) appear.
The core of any AL approach is a utility func-
tion u(p, ?) which estimates the informativeness
of each example p, a complete sentence p = (~x),
drawn from the pool P of all unlabeled examples,
for model induction. For our experiments, we em-
ploy two alternative utility functions which have
produced the best results in previous experiments
(Tomanek, 2010, Chapter 4). The first utility func-
tion is based on the confidence of a CRF model ?
in the predicted label sequence ~y? which is given
by the probability distribution P?(~y?|~x). The util-
ity function based on this probability boils down
to
uLC(p, ?) = 1? P?(~y ?|~x)
so that sentences for which the predicted label se-
quence ~y? has a low probability is granted a high
utility. Instead of calculating the model?s con-
fidence on the complete sequence, we might al-
ternatively calculate the model?s confidence in its
predictions on single tokens. To obtain an overall
confidence for the complete sequence, the aver-
age over the single token-confidence values can be
computed by the marginal probability P?(yi|~x).
Now that we are calculating the confidence on the
token level, we might also obtain the performance
of the second best label and calculate the margin
between the first and second best label as a con-
fidence score so that the final utility function is
obtained by
uMA(p, ?) = ?
1
n
n?
i=1
(
max
y?inY
P?(yi = y?|~x)?
max
y??inY
y? 6=y??
P?(yi = y??|~x)
)
Algorithm 1 formalizes our AL framework.
Depending on the utility function, the best b ex-
amples are selected per round, manually labeled,
and then added to the set of labeled data L which
feeds the classifier for the next training round.
Algorithm 1 NER-specific AL Framework
Given:
b: number of examples to be selected in each iteration
L: set of labeled examples l = (~x, ~y) ? Xn ? Yn
P: set of unlabeled examples p = (~x) ? Xn
T (L): a learning algorithm
u(p, ?): utility function
Algorithm:
loop until stopping criterion is met
1. learn model: ? ? T (L)
2. sort p ? P: let S ? (p1, . . . , pm) : u(pi, ?) ?
u(pi+1, ?), i ? [1,m], p ? P
3. select b examples pi with highest utility from S: B ?
{p1, . . . , pb}, b ? m, pi ? S
4. query labels for all p ? B: B? ? {l1, . . . , lb}
5. L ? L ? B?, P ? P \ B
return L? ? L and ?? ? T (L?)
The specification is still not cost-sensitive as the
selection of examples depends only on the utility
function. Using uLC will result in a reduction of
the number of examples (i.e., sentences) selected
irrespective of the sentence length so that a model
learns the most from it. As a result, we observed
that the selected sentences are quite long which
might even cause higher annotation costs per sen-
tence (Tomanek, 2010, Chapter 4). As for uMA
there is at least a slight normalization sensitive
to costs since the sum over all token-level utility
scores is normalized by the length of the selected
sentence.
1248
2.2 Semi-supervised AL (SeSAL)
Tomanek and Hahn (2009) extendeded this stan-
dard fully supervised AL framework by a semi-
supervised variant (SeSAL). The selection of sen-
tences is performed in a standard manner, i.e.,
similarly to the procedure in Algorithm 1. How-
ever, once selected, rather than manually annotat-
ing the complete sentence, only (uncertain) sub-
sequences of each selected sentence are manually
labeled, while the remaining (certain) ones are au-
tomatically annotated using the current version of
the classifier.
After the selection of an informative example
p = (~x) with ~x = (x1, . . . , xn), the subsequences
~x? = (xa, . . . , xb), 1 ? a ? b ? n, with low local
uncertainty have to be identified. For reasons of
simplicity, only sequences of length 1, i.e., single
tokens, are considered. For a token xi from a se-
lected sequence ~x the model?s confidence C?(y?i )
in label y?i is estimated. Token-level confidence
for a CRF is calculated as the marginal probabil-
ity so that
C?(y?i ) = P?(yi = y?i |~x)
where y?i specifies the label at the respective posi-
tion of the predicted label sequence ~y ? (the one
which is obtained by the Viterbi algorithm). If
C?(y?i ) exceeds a confidence threshold t, y?i is as-
signed as the putatively correct label. Otherwise,
manual annotation of this token is required.
Employing SeSAL, savings of over 80 % of the
tokens compared to random sampling are reported
by Tomanek and Hahn (2009). Even when com-
pared to FuSAL, still 60 % of the number of to-
kens are eliminated. A crucial question, however,
not answered in these experiments, is whether this
method actually reduces the overall annotation ex-
penses in time rather than just in the number of to-
kens. Also SeSAL does not incorporate labeling
costs in the selection process.
2.3 Cost-Sensitive AL (CoSAL)
In this section, we turn to an extension of FuSAL
and SeSAL which incorporates cost sensitivity
into the AL selection process (CoSAL). Three
different approaches of CoSAL will be explored.
The challenge we now face is that two contradic-
tory criteria ? utility and costs ? have to be bal-
anced.
2.3.1 Cost-Constrained Sampling
CoSAL can be realized in the most straight-
forward way by simply constraining the sampling
to a particular maximum cost cmax per example.
Therefore, in a pre-processing step all examples
p ? P for which cost(p) > cmax are removed from
P . The unmodified NER-specific AL framework
can then be applied.
An obvious shortcoming of Cost-Constrained
Sampling (CCS) is that it precludes any form of
compensation between utility and costs. Thus, an
exceptionally useful example with a cost factor
slightly above cmax will be rejected. Another crit-
ical issue is how to fix cmax. If chosen too low,
the pre-filtering of P results in a much too strong
restriction of selection options when only few ex-
amples remain inside P . If chosen too high, the
cost constraint becomes ineffective.
2.3.2 Linear Rank Combination
A general solution to fit different criteria into
a single one is by way of linear combination.
If, however, different units of measurement are
used, a transformation function for the alignment
of benefit, or utility, and costs must be found. This
can be difficult to determine. In our scenario, ben-
efits measured by utility scores and costs mea-
sured in seconds are clearly incommensurable. As
it is not immediately evident how to express utility
in monetary terms (or vice versa), we transform
utility and cost information into ranks R(u(p, ?))
andR?(cost(p)) instead. As for utility, higher util-
ity leads to higher ranks. As for costs, lower costs
lead to higher ranks. The linear rank combination
(LRK) is defined as
?LRK(~v(p)) = ?R
(
u(p, ?)
)
+(1??)R?
(cost(p))
where ? is a weighting term. In a CoSAL sce-
nario, where utility is the primary criterion, ? >
0.5 seems a reasonable choice. Alternatively, as
costs and utility are contradictory, allowing equal
influence for both criteria, as with ? = 0.5, it
may be difficult to find appropriate examples in
a medium-sized corpus. Thus, the choice of ? de-
pends on size and diversity with respect to combi-
nations of utility and costs within P .
1249
2.3.3 Benefit-Cost Ratio
Our third approach to CoSAL is based on the
Benefit-Cost Ratio (BCR). Given equal units of
measurement for benefits and costs, the benefit-
cost ratio indicates whether a scenario is profitable
(ratio > 1). BCR can also be applied when units
are incommensurable and a transformation func-
tion is available, as is the case for the combination
of utility and cost. This holds as long as bene-
fit and costs can be expressed in the same units
by a linear transformation function, i.e., u(p, ?) =
? ? cost(p) + b. If such a transformation function
exists, one can refrain from finding proper values
for the above variables b and ? and instead calcu-
late BCR as
?BCR(p) =
u(p, ?)
cost(p)
Since annotation costs are usually expressed on
a linear scale, this is also required for utility, if
we want to use BCR. But when utility is based
on model confidence as we do it here, this prop-
erty gets lost.1 Hence a non-linear transforma-
tion function is needed to fit the scales of utility
and costs. Assuming a linear relationship between
utility and costs, BCR has already been applied
by Haertel et al (2008) and Settles et al (2008).
Our approach provides a crucial extension as we
explicitly consider scenarios where such a linear
relationship is not given and a non-linear transfor-
mation function is required instead.
In a direct comparison of LRK with BCR, LRK
may be used when such a transformation function
would be needed but is unknown and hard to find.
Choosing LRK over BCR is also motivated by
findings in the context of data fusion in informa-
tion retrieval where Hsu and Taksa (2005) remark
that, given incommensurable units and scales, one
would do better when ranks rather than the actual
scores or values were combined.
3 Experiments
In the following, we study possible benefits of
CoSAL, relative to FuSAL and SeSAL, in the
1Though normalized to [0, 1], confidence estimates, es-
pecially for sequence classification, are often not on a linear
scale so that confidence values that are twice as high do not
necessarily mean that the benefit in training a model on such
an example is doubled.
light of real annotation times as a cost measure
(instead of the standard, yet inadequate one, viz.
the number of tokens being selected). Such timing
data is available in the MUC7T corpus (Tomanek
and Hahn, 2010), a re-annotation of the MUC7
corpus containing the ENAMEX types (persons,
locations, and organizations) and a time stamp re-
flecting the time it took annotators to decide on
each entity type. The MUC7T corpus contains
3,113 sentences (76,900 tokens).
The results we report on are averaged over 20
independent runs. For each run, we split the
MUC7T corpus randomly into a pool to select
from (90%) and an evaluation set (10%). AL was
started from a random seed set of 20 sentences.
As utility scores to estimate benefits we applied
uMA and uLC as defined in Section 2.1.
The plots in the following sections depict costs
in terms of annotation time (in seconds) relative
to annotation quality (expressed via F1-scores).
Learning curves are only shown for early AL it-
erations. Later on, in the convergence phase, due
to the two conflicting criteria now considered si-
multaneously, selection options become more and
more scarce so that CoSAL necessarily performs
sub-optimally.
3.1 Parametrization of CoSAL Approaches
Preparatory experiments were run to analyze how
different parameters affected different CoSAL set-
tings. For the CCS and LRK experiments, we
used the uLC utility function.
For CCS, we tested three cmax values, viz. 7.5,
10, and 15, to determine the maximum perfor-
mance attainable on MUC7T when only examples
below the chosen threshold were included. Our
choices of the maximum were based on the dis-
tributions of annotation times over the sentences
(see Figure 1) where 7.5s marks the 75% quantile
and 15s is just above the 90% quantile. For 7.5s,
we peaked at Fmax = 0.84, for 10s at Fmax =
0.86, and for 15s at Fmax = 0.88. Figure 2
(top) shows the learning curves of CoSAL with
CCS and different cmax values. With cmax = 15,
as could be expected from the boxplot in Fig-
ure 1, no difference can be observed compared
to cost-insensitive FuSAL. CCS with lower val-
ues for cmax stagnates at the maximum perfor-
1250
seconds
freq
uen
cy
0 5 10 15 20 25 30
0
200
400
600
800
Figure 1: Distribution of annotation times per sen-
tence in MUC7T .
mance reported above, but still improves upon
cost-insensitive FuSAL in early AL iterations.
At some point in time all economical exam-
ples, with costs below cmax but high utility, have
been consumed from the corpus. Even in a cor-
pus much larger than MUC7T this effect will only
occur with some delay. Indeed, any choice of a re-
strictive value for cmax will cause similar exhaus-
tion effects. Unfortunately, it is unclear how to
tune cmax suitably in a real-life annotation sce-
nario where pretests for maximum performance
for a particular cmax are not possible. For further
experiments, we chose cmax = 10.
For LRK, we tested three different weights ?,
viz. 0.5, 0.75, and 0.9. Figure 2 (bottom) shows
their effects on the learning curves. Similar ten-
dencies as for cmax for CCS can be observed.
With ? = 0.9, CoSAL does not fall below default
FuSAL, at least in the observed range. A lower
weight of ? = 0.75 results in larger improve-
ments in earlier AL iterations but then falls back
to FuSAL and in later AL iterations (not shown
here) even below FuSAL. If the time parameter
is granted too much influence, as with ? = 0.5,
performance even drops to random selection level.
This might also be due to corpus exhaustion. For
further experiments, we chose ? = 0.75 because
of its potential to improve upon FuSAL in early
iterations.
For BCR with uMA, we change this utility func-
tion to n ? uMA to compensate for the normaliza-
1000 2000 3000 4000 5000 6000
0.7
0
0.7
5
0.8
0
0.8
5
parameter test for CCS
seconds
F?s
cor
e
CCS 15s
CCS 10s
CCS 7.5s
FuSAL : uLC
RS
1000 2000 3000 4000 5000 6000
0.7
0
0.7
5
0.8
0
0.8
5
parameter test for LRK
seconds
F?s
cor
e
LRK 0.9
LRK 0.75
LRK 0.5
FuSAL : uLC
RS
Figure 2: Different parameter settings for CCS
and LRK based on FuSAL with uLC as utility
function. FuSAL: uLC refers to cost-insensitive
FuSAL, CCS and LRK to the cost-sensitive ver-
sions of FuSAL with the respective parameters.
tion by token length which is otherwise already
contained in uMA(n is the length of the respective
sentence). For uLC, the preparatory experiments
already showed that this utility function does not
behave on a linear scale. This is so because uLC is
based on P?(~y|~x) for confidence estimation of the
complete label sequence ~y. Hence, a uLC score
twice as high does not indicate doubled benefit for
classifier training. Thus, we need a non-linear cal-
ibration function to transform uLC into a proper
utility estimator on a linear scale so that BCR can
be applied.
To determine such a non-linear calibration
function, the true benefit of an example p would
1251
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
ll
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
ll
l l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l l
l l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
ll
l l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
ll
l
l
l
l
l
l
l l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
ll
l l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
lll
l
ll
l
l
l
l
l
l
l
l
l
l
0.5 0.6 0.7 0.8 0.9 1.0
0
2
4
6
8
uLC
n?
u M
A
corr: 0.6494
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
ll
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l l
l
l
l
l
l
l
l
l l
l
l l
l
ll
l
l
l
l
l
l l
ll
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l l
l
l
l
l l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
0e+00 1e+08 2e+08 3e+08 4e+08 5e+08
0
2
4
6
8
e??uLC
n?
u M
A
corr: 0.8959
Figure 3: Scatter plots for (a) uLC versus n?uMA and (b) e??uLC versus n?uMA
be needed. In the absence of such informa-
tion, we consider n ? uMA as a good approxima-
tion. To identify the relationship between uLC and
n ? uMA, we trained a model on a random subsam-
ple from P ? ? P and used this model to obtain
the scores for uLC and n ? uMA for each example
from the test set T .2 Figure 3 (left) shows a scat-
ter plot of these scores which provides ample evi-
dence that the relationship between uLC and ben-
efit is indeed non-linear. As calibration function
for uLC we propose f(p) = e??uLC(p). Experi-
mentally, we determined ? = 20 as a good value.
Figure 3 (right) reveals that e??uLC(p) is a better
utility estimator; the correlation with n ? uMA is
now corr = 0.8959 and the relationship is close
to being linear.
In Figure 4, learning curves for BCR with the
utility function uLC and the calibrated function
e??uLC(p) are compared. BCR with the uncali-
brated utility function uLC fails miserably (the
performance falls even below random selection).
This adds credibility to our claim that while uLC
may be appropriate for ranking examples (as for
standard, cost-insensitive AL), it is inappropriate
for estimating true benefit/utility which is needed
when costs are to be incorporated with the BCR
method. BCR with the calibrated utility e??uLC(p),
in contrast, outperforms cost-insensitive FuSAL.
For further experiments with BCR, we either ap-
ply n?uMA or e??uLC(p) as utility functions.
2We experimented with different sizes forP ?, with almost
identical results.
1000 2000 3000 4000 5000 6000
0.7
0
0.7
5
0.8
0
0.8
5
parameter test for BCR
seconds
F?s
cor
e
BCR : e20?uLC
BCR : uLC
FuSAL : uLC
RS
Figure 4: Different parameter settings for BCR
3.2 Comparison of CoSAL Approaches
We compared all three approaches to CoSAL in
the parametrization chosen above for the utility
functions uMA and uLC. Learning curves are
shown in Figure 5. Improvements over cost-
insensitive AL are only achieved in early AL iter-
ations up to 2,500s (for CoSAL based on uMA) or
4,000s (for CoSAL based on uLC) of annotation
time. This exclusiveness of early improvements
can be explained by the size of the corpus and, by
this, the limited number of good selection options.
Since AL selects with respect to two conflicting
criteria, the pool P should be much larger to in-
crease the chance for examples that are favorable
with respect to both criteria.
1252
0 1000 2000 3000 4000 5000 6000
0.7
0
0.7
5
0.8
0
0.8
5
0.9
0
utility function : uMA
seconds
F?s
cor
e
CCS (10s)
LRK (0.75)
BCR : n ? uMA
FuSAL : uMA
RS
0 1000 2000 3000 4000 5000 6000
0.7
0
0.7
5
0.8
0
0.8
5
0.9
0
utility function : uLC
seconds
F?s
cor
e
CCS (10s)
LRK (0.75)
BCR : e20?uLC
FuSAL : uLC
RS
Figure 5: Comparison of CoSAL approaches for the utility functions uMA and uLC. Baseline given by
random selection (RS) and standard FuSAL with either uMA or uLC.
Improvements for CoSAL based on uLC are
generally higher than for uMA. Moreover, cost-
insensitive AL based on uLC does not exhibit any
normalization where, in contrast, uMA is normal-
ized at least to the number of tokens per example.
In CoSAL, both uLC and uMA are normalized by
costs, which is methodologically a more substan-
tial enhancement for uLC than for uMA.
For CoSAL based on uMA we cannot proclaim
a clear winner among the different approaches.
All three CoSAL approaches improve upon cost-
insensitive AL. For CoSAL based on uLC, LRK
performs best, while CCS and BCR perform simi-
larly well. Given this result, we might prefer LRK
or CCS over BCR. A disadvantage of the first two
approaches is that they require corpus-specific pa-
rameters which may be difficult to find for a new
learning problem for which no data for experi-
mentation is at hand. Though not the best per-
former, BCR does not require further parametriza-
tion and appears more appropriate for real-life an-
notation projects ? as long as utility is an appro-
priate estimator for benefit. CoSAL with BCR has
already been studied by Settles et al (2008). They
also applied a utility function based on sequence-
confidence estimation which presumably, as with
our uLC utility function, is not a good benefit esti-
mator. The fact that Settles et al did not explicitly
treat this issue might explain why cost-sensitive
AL based on BCR often performed worse than
cost-insensitive AL in their experiments.
3.3 CoSAL Applied to SeSAL
We looked at a cost-sensitive version of SeSAL by
applying the cost-sensitive FuSAL approach to-
gether with BCR and the transformation function
for the utility as discussed above. On top of this
selection, we ran the standard SeSAL approach ?
only tokens below a confidence threshold were se-
lected for annotation. The following experiments
are all based on the uLC utility function (and the
transformation function of it).
Figure 6 depicts learning curves for cost-
insensitive and cost-sensitive SeSAL and FuSAL
which reveal that cost-sensitive SeSAL consid-
0 1000 2000 3000 4000 5000 6000
0.7
0
0.7
5
0.8
0
0.8
5
0.9
0
seconds
F?s
cor
e
SeSAL BCR
FuSAL BCR
SeSAL
FuSAL
RS
Figure 6: Cost-sensitive (BCR variants) vs. cost-
insensitive FuSAL and SeSAL with uLC as utility
function.
1253
erably outperforms cost-sensitive FuSAL. Cost-
sensitive SeSAL attains a target performance of
F=0.85 with only 2806s, while cost-sensitive
FuSAL needs 3410s, and random selection con-
sumes over 6060s. Thus, cost-sensitive SeSAL
here reduces true annotation time by about 54 %
compared to random selection, whereas cost-
sensitive FuSAL reduces annotation time by only
44 %.
4 Related Work
Although the reduction of data acquisition costs
that result from human labeling efforts have al-
ways been the main driver for AL studies, cost-
sensitive AL is a new branch of AL. In an early
study on cost metrics for AL, Becker and Osborne
(2005) examined whether AL, while decreasing
the sample size on the one hand, on the other
hand increased annotation efforts. For a real-
world AL annotation project, they demonstrated
that the actual sampling efficiency measure for
an AL approach depends on the cost metric be-
ing applied. In a companion paper, Hachey et al
(2005) studied how sentences selected by AL af-
fected the annotators? performance both in terms
of the time needed and the annotation accuracy
achieved. They found that selectively sampled ex-
amples are, on the average, more difficult to anno-
tate than randomly sampled ones. This observa-
tion, for the first time, questioned the widespread
assumption that all annotation examples can be as-
signed a uniform cost factor.
Making a standard AL approach cost-sensitive
by normalizing utility in terms of annotation time
has been proposed before by Haertel et al (2008),
Settles et al (2008), and Donmez and Carbonell
(2008). CoSAL based on the net-benefit (costs
subtracted from utility) was proposed by Vijaya-
narasimhan and Grauman (2009) for object recog-
nition in images and Kapoor et al (2007) for voice
message classification.
5 Conclusions
We investigated three approaches to incorporate
the notion of cost into the AL selection mecha-
nism, including a fixed maximal cost budget per
example, a linear rank combination to express net-
benefit, and a benefit-cost ratio. The cost metric
we applied was the time needed by human coders
for annotating particular annotation examples.
Among the three approaches to cost-sensitive
AL, we see a slight advantage for benefit cost ra-
tios in real-world settings because they do not re-
quire additional corpus-specific parametrization,
once a proper calibration function is found.
Another observation is that advantages of
the three cost-sensitive AL models over cost-
insensitive ones consistently occur only in early
iteration rounds ? a result we attribute to corpus
exhaustion effects since cost-sensitive AL selects
for two criteria (utility and cost) and thus requires
a extremely large pool to be able to pick up really
advantageous examples. Consequently, applied
to real-world annotation settings where the pools
may be extremely large, we expect cost-sensitive
approaches to be even more effective in terms of
the reduction of annotation time.
To be applicable in real-world scenarios, anno-
tation costs which, in our experiments, were di-
rectly traceable in the MUC7T corpus have to be
estimated since they are not known prior to anno-
tation. In Tomanek et al (2010), we investigated
the reading behavior during named entity annota-
tion using eye-tracking technology. With the in-
sights gained from this study on crucial factors in-
fluencing annotation time we were able to induce
such a much needed predictive model of annota-
tion costs. In future work, we plan to incorporate
this empirically founded cost model into our ap-
proaches to cost-sensitive AL and to investigate
whether our positive findings can be reproduced
with estimated costs as well.
Acknowledgements
This work was partially funded by the EC within
the CALBC (FP7-231727) project.
References
Becker, Markus and Miles Osborne. 2005. A two-
stage method for active learning of statistical gram-
mars. In IJCAI?05 ? Proceedings of the 19th Inter-
national Joint Conference on Artificial Intelligence,
pages 991?996. Edinburgh, Scotland, UK, July 31 -
August 5, 2005.
1254
Donmez, Pinar and Jaime Carbonell. 2008. Proactive
learning: Cost-sensitive active learning with mul-
tiple imperfect oracles. In CIKM?08 ? Proceed-
ing of the 17th ACM conference on Information
and Knowledge Management, pages 619?628. Napa
Valley, CA, USA, October 26-30, 2008.
Engelson, Sean and Ido Dagan. 1996. Minimizing
manual annotation cost in supervised training from
corpora. In ACL?96 ? Proceedings of the 34th An-
nual Meeting of the Association for Computational
Linguistics, pages 319?326. Santa Cruz, CA, USA,
June 24-27, 1996.
Hachey, Ben, Beatrice Alex, and Markus Becker.
2005. Investigating the effects of selective sampling
on the annotation task. In CoNLL?05 ? Proceed-
ings of the 9th Conference on Computational Natu-
ral Language Learning, pages 144?151. Ann Arbor,
MI, USA, June 29-30, 2005.
Haertel, Robbie, Kevin Seppi, Eric Ringger, and James
Carroll. 2008. Return on investment for active
learning. In Proceedings of the NIPS 2008 Work-
shop on Cost-Sensitive Machine Learning. Whistler,
BC, Canada, December 13, 2008.
Hsu, Frank and Isak Taksa. 2005. Comparing rank and
score combination methods for data fusion in infor-
mation retrieval. Information Retrieval, 8(3):449?
480.
Kapoor, Ashish, Eric Horvitz, and Sumit Basu. 2007.
Selective supervision: Guiding supervised learning
with decision-theoretic active learning. In IJCAI?07
? Proceedings of the 20th International Joint Con-
ference on Artifical Intelligence, pages 877?882.
Hyderabad, India, January 6-12, 2007.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML?01 ? Proceedings of the
18th International Conference on Machine Learn-
ing, pages 282?289. Williamstown, MA, USA, June
28 - July 1, 2001.
Settles, Burr and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In EMNLP?08 ? Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1069?1078. Waikiki, Hon-
olulu, Hawaii, USA, October 25-27, 2008.
Settles, Burr, Mark Craven, and Lewis Friedland.
2008. Active learning with real annotation costs. In
Proceedings of the NIPS 2008 Workshop on Cost-
Sensitive Machine Learning. Whistler, BC, Canada,
December 13, 2008.
Tomanek, Katrin and Udo Hahn. 2009. Semi-
supervised active learning for sequence labeling. In
ACL/IJCNLP?09 ? Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natu-
ral Language Processing, pages 1039?1047. Singa-
pore, August 2-7, 2009.
Tomanek, Katrin and Udo Hahn. 2010. Annotation
time stamps: Temporal metadata from the linguistic
annotation process. In LREC?10 ? Proceedings of
the 7th International Conference on Language Re-
sources and Evaluation. La Valletta, Malta, May 17-
23, 2010.
Tomanek, Katrin, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction
which cuts annotation costs and maintains cor-
pus reusability of annotated data. In EMNLP-
CoNLL?07 ? Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Language Learning,
pages 486?495. Prague, Czech Republic, June 28-
30, 2007.
Tomanek, Katrin, Udo Hahn, Steffen Lohmann, and
Ju?rgen Ziegler. 2010. A cognitive cost model of an-
notations based on eye-tracking data. In ACL?10 ?
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics. Uppsala,
Sweden, July 11-16, 2010.
Tomanek, Katrin. 2010. Resource-Aware Annotation
through Active Learning. Ph.D. thesis, Technical
University of Dortmund.
Vijayanarasimhan, Sudheendra and Kristen Grauman.
2009. What?s it going to cost you? predicting ef-
fort vs. informativeness for multi-label image anno-
tations. CVPR?09 ? Proceedings of the 2009 IEEE
Computer Vision and Pattern Recognition Confer-
ence.
1255
