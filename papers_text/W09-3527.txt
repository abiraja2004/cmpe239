Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 120?123,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
?-extension Hidden Markov Models and Weighted Transducers for
Machine Transliteration
Balakrishnan Vardarajan
Dept. of Electrical and Computer Engineering
Johns Hopkins University
bvarada2@jhu.edu
Delip Rao
Dept. of Computer Science
Johns Hopkins University
delip@cs.jhu.edu
Abstract
We describe in detail a method for translit-
erating an English string to a foreign
language string evaluated on five differ-
ent languages, including Tamil, Hindi,
Russian, Chinese, and Kannada. Our
method involves deriving substring align-
ments from the training data and learning a
weighted finite state transducer from these
alignments. We define an ?-extension Hid-
den Markov Model to derive alignments
between training pairs and a heuristic to
extract the substring alignments. Our
method involves only two tunable parame-
ters that can be optimized on held-out data.
1 Introduction
Transliteration is a letter by letter mapping of one
writing system to another. Apart from the obvi-
ous use in writing systems, transliteration is also
useful in conjunction with translation. For exam-
ple, machine translation BLEU scores are known
to improve when named entities are transliterated.
This engendered several investigations into auto-
matic transliteration of strings, named entities in
particular, from one language to another. See
Knight and Graehl(1997) and later papers on this
topic for an overview.
Hidden Markov Model (HMM) (Rabiner,
1989) is a standard sequence modeling tool used
in various problems in natural language process-
ing like machine translation, speech recognition,
part of speech tagging and information extraction.
There have been earlier attempts in using HMMs
for automatic transliteration. See (Abdul Jaleel
and Larkey, 2003; Zhou et al, 2008) for exam-
ple. In this paper, we define an ?-extension Hid-
den Markov Model that allows us to align source
and target language strings such that the charac-
ters in the source string may be optionally aligned
to the ? symbol. We also introduce a heuristic that
allows us to extract high quality sub-alignments
from the ?-aligned word pairs. This allows us to
define a weighted finite state transducer that pro-
duces transliterations for an English string by min-
imal segmentation.
The overview of this paper is as follows: Sec-
tion 2 introduces ?-extension Hidden Markov
Model and describes our alignment procedure.
Section 3 describes the substring alignment
heuristic and our weighted finite state transducer
to derive the final n-best transliterations. We con-
clude with a result section describing results from
the NEWS 2009 shared task on five different lan-
guages.
2 Learning Alignments
The training data D is given as pairs of strings
(e, f) where e is the English string with the cor-
responding foreign transliteration f . The English
string e consists of a sequence of English letters
(e1, e2, . . . , eN ) while f = (f1, f2, . . . , fM ) .
We represent E as the set of all English symbols
and F as the set of all foreign symbols.1 We also
assume both languages have a special null symbol
?, that is ? ? E and ? ? F .
Our alignment model is a Hidden Markov
Model H(X,Y,S,T,Ps), where
? X is the start state and Y is the end state.
? S is the set of emitting states with S = |S|.
The emitting states are indexed from 1 to S.
The start state X is indexed as state 0 and the
end state Y is indexed as state S + 1.
? T is an (S + 1) ? (S + 1) stochastic matrix
with T = [tij] for i ? {0, 1, . . . , S} and j ?
{1, 2, . . . , S + 1}.
1Alphabets and diacritics are treated as separate symbols.
120
? Ps = [pef ] is an |E| ? |F| matrix of joint
emission probabilities with pef = P (e, f |s)
?s ? S .
We define s? to be an ?-extension of a string of
characters s = (c1, c2, . . . , ck) as the string ob-
tained by pumping an arbitrary number of ? sym-
bols between any two adjacent characters cl and
cl+1. That is, s? = (di1 , . . . , di2 , . . . , dik) where
dij = cj and dl = ? for im < l < im+1 where
1 ? l < k. Observe that there are countably infi-
nite ?-extensions for a given string s since an arbi-
trary number of ? symbols can be inserted between
characters cm and cm+1. Let T (s) denote the set
of all possible ?-extensions for a given string s.
For a given pair of strings (u, v), we define a
joint ?-extension of (u, v) as the pair (u?, v?) s.t. u? ?
T (u) and v? ? T (v) with |u?| = |v?| and ?i s.t.
u?i = v?i = ?. Due to this restriction, there are finite
?-extensions for a pair (u, v) with the length of u?
and v? bounded above by |u| + |v|. 2 Let J(u, v)
denote the set of all joint ?-extensions of (u, v).
Given a pair of strings (e, f) with e =
(e1, e2, . . . , eN ) and f = (f1, f2, . . . , fM ), we
compute the probability ?(e, f, s?) that they are
transliteration pairs ending in state s? as
?(e, f, s?) =
?
(e?,?f)?J(e,f)
?
0=s0,...,s|e?|=s?
t0,s1
|e?|
?
i=1
tsi,si+1P (e?i, f?i|si)
In order to compute the probability Q(e, f) of a
given transliteration pair, the final state has to be
the end state S + 1. Hence
Q(e, f) =
S
?
s=1
?(e, f, s)ts,S+1 (1)
We also write the probability ?(e, f, s?) that they
are transliteration pairs starting in state s? as
?(e, f, s?) =
?
(e?,?f)?J(e,f)
?
s?=s0,...,s|e?|+1=S+1
ts0,s1
|e?|
?
i=1
tsi,si+1P (e?i, f?i|si)
Again noting that the start state of the HMM
H is 0, we have Q(e, f) =
S
?
s=1
?(e, f, s)t0,s. We
2|u?| = |v?| > |u| + |v| would imply ?i s.t. u?i = v?i = ?
which contradicts the definition of joint ?-extension.
denote a subsequence of a string u as umn =
(un, un+1, . . . , um) . Using these definitions, we
can define ?(ei1, f
j
1 , s) as
?
?
?
?
?
?
?
?
?
1 i = j = 0, s = 0
0 i = j = 0, s 6= 0
t0,sP (e1, f1|s) i = j = 1
PS
s?=1 ts?,s?(ei1, f
j?1
1 , s?)P (?, fj |s) i = 1, j > 1
PS
s?=1 ts?,s?(ei?11 , f
j
1 , s?)P (ei, ?|s) i > 1, j = 1
Finally for i > 1 and j > 1,
?(ei1, f j1 , s) =
?
s??S
ts?,s[?(ei1, f j?11 , s?)P (?, fj |s)+
?(ei?11 , f
j
1 , s?)P (ei, ?|s)+
?(ei?11 , f
j?1
1 , s?)P (ei, fj|s)]
Similarly the recurrence for ?(eNi , fMj , s)
?
?
?
?
?
ts,S+1 i = N + 1,
j = M + 1
PS
s?=1 ts,s??(eNi , fMj+1, s?)P (?, fj |s?) i = N, j < M
PS
s?=1 ts,s??(eNi+1, fMj , s?)P (ei, ?|s?) i < N, j = M
For i < N and j < M , ?(eNi , fMj , s) =
?
s??S
ts,s?[?(eNi , fMj+1, s?)P (?, fj |s?)+
?(eNi+1, fMj , s?)P (ei, ?|s?)+
?(eNi+1, fMj+1, s?)P (ei, fj|s?)]
In order to proceed with the E.M. estimation
of the parameters T and Ps , we collect the
soft counts c(e, f |s) for emission probabilities by
looping over the training data D as shown in Fig-
ure 1.
Similarly the soft counts ct(s?, s) for the tran-
sition probabilities are estimated as shown in Fig-
ure 2.
Finally the probabilities P (e, f |s) and tij are re-
estimated as
P? (e, f |s) = c(e, f |s)?
e?E,f?F c(e, f |s)
(2)
t?s?,s =
ct(s?, s)
?
s ct(s?, s)
(3)
We can also compute the most probable align-
ment (e?, f? ) between the two strings e and f as
121
c(e, f |s) =
?
(e,f)?D
1
Q(e, f)
N
?
i=1
M
?
j=1
?
s?
?(ei?11 , f
j?1
1 , s?)ts?,sP (ei, fj |s)?(eNi , fMj , s)1(ei = e, fj = f)
+
?
(e,f)?D
1
Q(e, f)
N
?
i=1
M
?
j=1
?
s?
?(ei?11 , f
j
1 , s?)ts?,sP (ei, ?|s)?(eNi , fMj , s)1(ei = e, fj = f)
+
?
(e,f)?D
1
Q(e, f)
N
?
i=1
M
?
j=1
?
s?
?(ei1, f j?11 , s?)ts?,sP (?, fj |s)?(eNi , fMj , s)1(ei = e, fj = f)
Figure 1: EM soft count c(e, f |s) estimation.
ct(s?, s) =
?
(e,f)?D
1
Q(e, f)
N
?
i=1
M
?
j=1
?(ei?11 , f
j?1
1 , s?)ts?,sP (ei, fj|s)?(eNi , fMj , s)
+
?
(e,f)?D
1
Q(e, f)
N
?
i=1
M
?
j=1
?(ei?11 , f
j
1 , s?)ts?,sP (ei, ?|s)?(eNi , fMj , s)
+
?
(e,f)?D
1
Q(e, f)
N
?
i=1
M
?
j=1
?(ei1, f j?11 , s?)ts?,sP (?, fj|s)?(eNi , fMj , s)
+
?
(e,f)?D
1
Q(e, f)?(e
N
1 , fM1 , s?)ts?,S+11(s = S + 1)
Figure 2: EM soft count ct(s?, s) estimation.
122
arg max
(e?,?f)?J(e,f)
?
0=s0,...,s|e?|+1=S+1
t0,s1
|e?|
?
i=1
tsi,si+1P (e?i, f?i|si)
The pair (e?, f?) is considered as an alignment be-
tween the training pair (e, f).
3 Transduction of the Transliterated
Output
Given an alignment (e?, f?), we consider all possi-
ble sub-alignments (e?ji , f?
j
i ) as pairs of substrings
obtained from (e?, f? ) such that e?i 6= ?, f?i 6= ?,
e?j+1 6= ? and f?j+1 6= ? . We extract all pos-
sible sub-alignments of all the alignments from
the training data. Let A be the bag of all sub-
alignments obtained from the training data. We
build a weighted finite state transducer that trans-
duces any string in E+ to F+ using these sub-
alignments.
Let (u,v) be an element of A. From the train-
ing data D, observe that A can have multiple re-
alizations of (u,v). Let N(u,v) be the number
of times (u,v) is observed in A. The empirical
probability of transducing string u to v is simply
P (v|u) = N(u,v)?
v:(u,v?)?A N(u,v?)
For every pair (u,v) ? A , we also compute the
probability of transliteration from the HMM H as
Q(u,v) from Equation 1.
We construct a finite state transducer Fu,v that
accepts only u and emits v with a weight wu,v
defined as
wu,v = ? log(P (v|u))?? log(Q(u,v))+? (4)
Finally we construct a global weighted finite
state transducer F by taking the union of all the
Fu,v and taking its closure.
F =
?
?
?
(u,v)?A
Fu,v
?
?
+
(5)
The weight ? is typically sufficiently high so
that a new english string is favored to be broken
into fewest possible sub-strings whose translitera-
tions are available in the training data.
We tune the weights ? and ? by evaluating the
accuracy on the held-out data. The n-best paths
in the weighted finite state transducer F represent
our n-best transliterations.
4 Results
We evaluated our system on the standard track data
provided by the NEWS 2009 shared task orga-
nizers on five different languages ? Tamil, Hindi,
Russian, and Kannada was derived from (Ku-
maran and Kellner, 2007) and Chinese from (Li et
al., 2004). The results of this evaluation on the test
data is shown in Table 1. For a detailed description
Language Top-1 mean MRR
Accuracy F1 score
Tamil 0.327 0.870 0.458
Hindi 0.398 0.855 0.515
Russian 0.506 0.901 0.609
Chinese 0.450 0.755 0.514
Kannada 0.235 0.817 0.353
Table 1: Results on NEWS 2009 test data.
of the evaluation measures used we refer the read-
ers to NEWS 2009 shared task whitepaper (Li et
al., 2009).
5 Conclusion
We described a system for automatic translitera-
tion of pairs of strings from one language to an-
other using ?-extension hidden markov models and
weighted finite state transducers. We evaluated
our system on all the languages for the NEWS
2009 standard track. The system presented is lan-
guage agnostic and can be trained for any language
pair within a few minutes on a single core desktop
computer.
References
Nasreen Abdul Jaleel and Leah Larkey. 2003. Statistical transliteration for english-arabic
cross language information retrieval. In Proceedings of the twelfth international con-
ference on Information and knowledge management, pages 139?146.
Kevin Knight and Jonathan Graehl. 1997. Machine transliteration. In Computational Lin-
guistics, pages 128?135.
A. Kumaran and Tobias Kellner. 2007. A generic framework for machine transliteration.
In SIGIR ?07: Proceedings of the 30th annual international ACM SIGIR conference
on Research and development in information retrieval, pages 721?722, New York, NY,
USA. ACM.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source-channel model for machine
transliteration. In ACL ?04: Proceedings of the 42nd Annual Meeting on Association
for Computational Linguistics, page 159, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir Pervouchine. 2009. Whitepaper of
news 2009 machine transliteration shared task. In Proceedings of ACL-IJCNLP 2009
Named Entities Workshop (NEWS 2009).
Lawrence Rabiner. 1989. A tutorial on hidden markov models and selected applications in
speech recognition. In Proceedings of the IEEE, pages 257?286.
Yilu Zhou, Feng Huang, and Hsinchun Chen. 2008. Combining probability models and web
mining models: a framework for jproper name transliteration. Information Technology
and Management, 9(2):91?103.
123
