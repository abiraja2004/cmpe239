NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 47?54,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Unsupervised Part of Speech Inference with Particle Filters
Gregory Dubbin and Phil Blunsom
Department of Computer Science
University of Oxford
Wolfson Building, Parks Road
Oxford, OX1 3QD, United Kingdom
Gregory.Dubbin@wolfson.ox.ac.uk Phil.Blunsom@cs.ox.ac.uk
Abstract
As linguistic models incorporate more subtle
nuances of language and its structure, stan-
dard inference techniques can fall behind. Of-
ten, such models are tightly coupled such that
they defy clever dynamic programming tricks.
However, Sequential Monte Carlo (SMC) ap-
proaches, i.e. particle filters, are well suited
to approximating such models, resolving their
multi-modal nature at the cost of generating
additional samples. We implement two par-
ticle filters, which jointly sample either sen-
tences or word types, and incorporate them
into a Gibbs sampler for part-of-speech (PoS)
inference. We analyze the behavior of the par-
ticle filters, and compare them to a block sen-
tence sampler, a local token sampler, and a
heuristic sampler, which constrains inference
to a single PoS per word type. Our findings
show that particle filters can closely approx-
imate a difficult or even intractable sampler
quickly. However, we found that high poste-
rior likelihood do not necessarily correspond
to better Many-to-One accuracy. The results
suggest that the approach has potential and
more advanced particle filters are likely to lead
to stronger performance.
1 Introduction
Modern research is steadily revealing more of the
subtle structure of natural language to create in-
creasingly intricate models. Many modern problems
in computational linguistics require or benefit from
modeling the long range correlations between latent
variables, e.g. part of speech (PoS) induction (Liang
et al., 2010), dependency parsing (Smith and Eis-
ner, 2008), and coreference resolution (Denis and
Baldridge, 2007). These correlations make infer-
ence difficult because they reflect the complicated
effect variables have on each other in such tightly
coupled models.
Sequential Monte Carlo (SMC) methods, like par-
ticle filters, are particularly well suited to estimating
tightly coupled distributions (Andrieu et al., 2010).
Particle filters sample sequences of latent variable
assignments by concurrently generating several rep-
resentative sequences consistent with a model?s con-
ditional dependencies. The sequential nature of the
sampling simplifies inference by ignoring ambigu-
ous correlations with unsampled variables at the
cost of sampling the sequence multiple times. The
few applications of particle filters in computational
linguistics generally focus on the online nature of
SMC (Canini et al., 2009; Borschinger and John-
son, 2011). However, batch applications still benefit
from the power of SMC to generate samples from
tightly coupled distributions that would otherwise
need to be approximated. Furthermore, the time cost
of the additional samples generated by SMC can be
mitigated by generating them in parallel.
This report presents an initial approach to the inte-
gration of SMC and block sampling, sometimes ref-
fered to as Particle Gibbs (PG) sampling (Andrieu
et al., 2010). Unsupervised PoS induction serves
as a motivating example for future extensions to
other problems. Section 3 reviews the PYP-HMM
model used for PoS inference. Section 4 explains the
Sequential Importance Sampling (SIS) algorithm, a
basic SMC method that generates samples for the
47
block sampler. This approach yields two implemen-
tations: a simple sentence-based block sampler (4.1)
and a more complicated type-based sampler (4.2).
Finally, section 5 evaluates both implementations on
a variety of unsupervised PoS inference tasks, ana-
lyzing the behavior of the SMC inference and com-
paring them to state-of-the-art approaches.
2 Background
SMC was introduced in 1993 as a Bayesian esti-
mator for signal processing problems with strong
non-linear conditional dependencies (Gordon et al.,
1993). Since then, SMC methods have been adopted
by many fields, including statistics, biology, eco-
nomics, etc. (Jasra et al., 2008; Beaumont, 2003;
Fernandez-Villaverde and Rubio-Ramirez, 2007).
The SMC approach is the probabilisitic analogue of
the beam search heuristic, where the beam width can
be compared to the number of particles and pruning
is analogous to resampling.
The basic SMC approach serves as the basis for
several variants. Many SMC implementations re-
sample the population of particles to create a new
population that minimizes the effect of increasing
sample variance with increasing sequence length
(Kitagawa, 1996). Particle smoothing variants of
SMC reduce the relative variance of marginals early
in the sequence, as well improving the diversity
of the final sample (Fearnhead et al., 2008). Par-
ticle Markov chain Monte Carlo (PMCMC) for-
mally augments classic Markov chain Monte Carlo
(MCMC) approaches, like Gibbs sampling, with
samples generated by particle filters (Andrieu et al.,
2010).
3 The PYP-HMM
The PYP-HMM model of PoS generation demon-
strates the tightly coupled correlations that com-
plicate many standard inference methods (Blunsom
and Cohn, 2011). The model applies a hierarchical
Pitman-Yor process (PYP) prior to a trigram hidden
Markov model (HMM) to jointly model the distri-
bution of a sequence of latent word classes, t, and
word tokens, w. This model performs well on cor-
pora in multiple languages, but the lack of a closed
form solution for the sample probabilities makes it a
strong canditate for PG sampling. The joint proba-
bility defined by a trigram HMM is
P?(t,w) =
N+1?
n=1
P?(tl|tn?1, tn?2)P?(wn|tn)
where N = |t| = |w| and the special tag $ is added
to the boundaries on the sentence. The model de-
fines transition and emission distributions,
tn|tn?1, tn?2, T ? Ttn?1,tn?2
wn|tn, E ? Etn
The PYP-HMM smoothes these distributions by ap-
plying hierarchical PYP priors to them. The hierar-
chical PYP describes a back-off path of simpler PYP
priors,
Tij |a
T , bT , Bi ? PYP(a
T , bT , Bi)
Bi|a
B, bB, U ? PYP(aB, bb, U)
U |aU , bU ? PYP(aU , bU ,Uniform).
Ei|a
E , bE , C ? PYP(aE , bE , Ci),
where Tij , Bi, and U are trigram, bigram, and un-
igram transition distributions respectively and Ci is
either a uniform distribution (PYP-HMM) or a bi-
gram character language model emission distribu-
tion (PYP-HMM+LM, intended to model basic mor-
phology).
Draws from the posterior of the hierarchical
PYP can be calculated with a variant of the Chi-
nese Restaraunt Process (CRP) called the Chinese
Restaurant Franchise (CRF) (Teh, 2006; Goldwater
et al., 2006). In the CRP analogy, each latent vari-
able in a sequence is represented by a customer en-
tering a restaurant and sitting at one of an infinite
number of tables. A customer chooses to sit at a ta-
ble in a restaurant according to the probability
P (zn = k|z1:n?1) =
{
c?k ?a
n?1+b 1 ? k ? K
?
K?a+b
n?1+b k = K
? + 1
(1)
where zn is the index of the table chosen by the nth
customer to the restaurant, z1:n?1 is the seating ar-
rangement of the previous n? 1 customers to enter,
c?k is the count of the customers at table k, and K
?
is the total number of tables chosen by the previ-
ous n? 1 customers. All customers at a table share
the same dish, representing the value assigned to the
48
latent variables. When customers sit at an empty ta-
ble, a new dish is assigned to that table according to
the base distribution of the PYP. To expand the CRP
analogy to the CRF for hierarchical PYPs, when a
customer sits at a new table, a new customer enters
the restaurant representing the PYP of the base dis-
tribution.
4 Sequential Monte Carlo
While MCMC approximates a distribution as the av-
erage of a sequence of samples taken from the poste-
rior of the distribution, SMC approximates a distri-
bution as the importance weighted sum of several se-
quentially generated samples, called particles. This
article describes two SMC samplers that jointly sam-
ple multiple tag assignments: a sentence based block
sampler (sent) and a word type based block sam-
pler (type). The basics of particle filtering are out-
lined below, while the implementation specifics of
the sent and type particle filters are described in
secions 4.1 and 4.2, respectively.
SMC is essentially the probabilistic analogue of
the beam search heuristic. SMC stores P sequences,
analogous to beam width, and extends each incre-
mentally according to a proposal distribution qn,
similar to the heuristic cost function in beam search.
Many particle filtering implementations also include
a resampling step which acts like pruning by reduc-
ing the number of unlikely sequences.
We implemented Sequential Importance
Sampling (SIS), detailed by Doucet and Jo-
hansen (2009), to approximate joint samples
from the sentence and word type distributions.
This approach approximates a target distribution,
pin(x1:n) =
?n(x1:n)
Zn
, of the sequence, x1:n, of n
random variables, that is ?n(x1:n) calculates the
unnormalized density of x1:n.
SIS initilizes each particle p ? [1, P ] by sampling
from the initial proposal distribution q1(x
p
1), where
xpn is the value assigned to the n-th latent variable for
particle p. The algorithm then sequentially extends
each particle according to the conditional proposal
distribution qn(x
p
n|x
p
1:n), where x
p
1:n is the sequence
of values assigned to the first n latent variables in
particle p. After extending a particle p, SIS updates
the importance weight ?pn = ?
p
n?1 ? ?n(x
p
1:n). The
weight update, defined as
?n(x1:n) =
?n(x1:n)
?n?1(x1:n?1)qn(xn|x1:n?1)
, (2)
accounts for the discrepancy between the proposal
distribution, qn, and the target distribution, pin,
without normalizing over x1:n, which becomes in-
tractable for longer sequences even in discrete
domains. The normalizing constant of the tar-
get distribution is approximately Zn ?
?P
p=1 ?
p
n
and the unnormalized density is ?n(x1:n) ??P
p=1 ?
p
nifx
p
1:n = x1:n. The particles can also be
used to generate an unbiased sample from pin by
choosing a particle p proportional to its weight ?pn.
Andrieu et al. (2010) shows that to ensure the
samples generated by SMC for a Gibbs sampler has
the target distribution as the invariant density, the
particle filter must be modified to perform a con-
ditional SMC update. This means that the particle
filter guarantees that one of the final particles is as-
signed the same values as the previous Gibbs iter-
ation. Our implementation of the conditional SMC
update reserves one special particle, 0, for which the
proposal distribution always chooses the previous it-
eration?s value at that site.
4.1 Sentence Sampling
The sent particle filter samples blocks of tag as-
signments tS1:n for a sentence, S, composed of to-
kens, wS1:n. Sampling an entire sentence minimizes
the risk of assigning a tag with a high probabil-
ity given its local context but minimal probability
given the entire sentence. Sentences can be sampled
by ignoring table counts while sampling a proposal
sentence, incorporating them after the fact with a
Metropolis-Hastings acceptance test (Gao and John-
son, 2008). The Metropolis-Hastings step simplifies
the sentence block particle filter further by not re-
quiring the conditional SMC update.
While there is already a tractable dynamic pro-
gramming approach to sampling an entire sentence
based on the Forward-Backward algorithm, parti-
cle filtering the sentences PYP-HMM model should
prove beneficial. For the trigram HMM defined
by the model, the forward-backward sampling ap-
proach has time complexity in O(NT 3) for a sen-
tence of length N with T possible tag assignments
at each site. Particle filters with P particles can ap-
proximate these samples in O(NTP ) time, which
49
becomes much faster as the number of tags, T , in-
creases.
Sampling of sentence S begins by removing all
of the transitions and emitions in S from the table
counts, z, resulting in the table counts z?S of tag as-
signments t?S the values assigned to the variables
outside of S. For each site index n ? [1, N ] in the
sentence, the particle filter chooses the new tag as-
signment, tS,pn , for each particle p ? [1, P ] from the
sentence proposal distribution,
qSn (t
S,p
n |t
S,p
1:n?1) ? P (t
S,p
n |t
S,p
n?2, t
S,p
n?1, t
?S , z?S)
? P (wS,pn |t
S,p
n , t
?S , z?S ,w?S).
After each new tag is assigned, the particle?s weight
is updated according to equation (2). The simplic-
ity of the proposal density hints at the advantage of
particle filtering over forward-backward sampling:
it tracks only P histories and their weights rather
than tracking the probability of over all possible his-
tories. Once each particle has assigned a value to
each site in the sentence, one tag sequence is chosen
proportional to its particle weight, ?S,pN .
4.2 Type Sampling
The type sampling case for the PYP-HMM is more
complicated than the sent sampler. The long-range
couplings defined by the hierarchical PYP priors
strongly influence the joint distribution of tags as-
signed to tokens of the same word type (Liang et
al., 2010). Therefore, the affects of the seating de-
cisions of new customers cannot be postponed dur-
ing filtering as in sentence sampling. To account
for this, the type particle filter samples sequences
of seating arrangements and tag assignments jointly,
xW1:n = (t
W
1:n, z
W
1:n), for the word-type, W . The fi-
nal table counts are resampled once a tag assignment
has been chosen from the particles.
Tracking the seating arrangement history for each
particle adds an additional complication to the type
particle filter. The exchangeability of seating deci-
sions means that only counts of customers are nec-
essary to represent the history. Each particle repre-
sents both a tag sequence, tW,p1:n , and the count deltas,
zW,p1:n . The count deltas of each particle are stored in a
hash table that maps a dish in one of the CRF restau-
rants to the number of tables serving that dish and
the total number of customers seated at those tables.
The count delta hash table ensures that it has suffi-
cient data to calculate the correct probabilities (per
equation (1)) by storing any counts that are different
from the base counts, z?W , and defering to the base
counts for any counts it does not have stored.
At each token occurence n, the next tag assign-
ment, tW,pn for each particle p ? [1, P ] is chosen first
according to the word type proposal distribution
qWn (t
W,p
n |t
W,p
1:n?1, z
W,p
1:n?1) ?
P (tW,pn |c
?2
n , c
?1
n , t
?W,p
1:n?1, z
?W,p
1:n?1)
? P (c+1n |c
?1
n , t
W,p
n , t
?W,p
1:n?1, z
?W,p
1:n?1)
? P (c+2n |t
W,p
n , c
+1
n , t
?W,p
1:n?1, z
?W,p
1:n?1)
? P (wWn |t
W,p
n , t
?W,p
1:n?1, z
?W,p
1:n?1,w
?W,p
1:n?1).
In this case, c?kn represents a tag in the context
of site tWn offset by k, while t
?W,p
1:n?1, z
W,p
1:n?1, and
w?W,p1:n?1 represent the tag assignments, table counts,
and word token values chosen by particle p as well
as the values at all of the sites where a word token
of type W does not appear. This proposal distribu-
tion ignores changes to the seating arrangement be-
tween the three transitions involving the site n. The
specific seating arrangement of a particle is chosen
after the tag choice, at which point the weights are
updated by the result of equation (2). As with the
sent sampler, once all of the particles have been
sampled, one of them is sampled with probability
proportional to its weight. This final sample is a
sample from the true target probability.
As mentioned earlier, the sequence of particle ap-
proximations do not have the target distribution as
invariant unless they use the conditional SMC up-
date. Therefore, a the special 0 particle is automat-
ically assigned the value from the prior iteration of
the Gibbs sampler at each site n, though the proposal
probability qWn (t
W,0
n |t
W,p
1:n?1, z
W,p
1:n?1) still has to be
calculated to update the weight ?W,pn properly. This
ensures that the type sampler has a non-zero prob-
ability of reverting to the prior iteration?s sequence.
5 Experiments and Results
We take two approaches to evaluating the SMC
based samplers. The first approach is an analysis of
the samplers as inference algorithms. The samplers
should tend to maximize the posterior likelihood of
the model over iterations, eventually converging to
50
the mode. Section 5.1 analyzes the particle filter
based samplers with various numbers of particles in
an effort to understand how they behave.
Then, section 5.2 evaluates each of the proposed
approaches on PoS inference tasks from several lan-
guages. These results allow a practical comparison
with other PoS inference approaches.
5.1 SMC Analysis
Before comparing the performance of the SMC
block samplers to other inference methods, we wish
to learn more about the approaches themselves. It
is not clear how well the benefits of block sampling
transfer to SMC based approaches. Both the sent
and type samplers are novel approaches to com-
putational linguistics, and many of their properties
are unclear. For example, the samples generated
from the particle filter should have a higher vari-
ance than the target distribution. If the variance is
too high, the sampler will be slower to converge.
While additional particles lower the relative vari-
ance, they also increase the run time linearly. It is
possible that there is a threshold of particles nec-
essary to ensure that some are high likelihood se-
quences, beyond which inference gains are minimal
the additional computational expense is wasted. All
of the experiments in this section were run on the
Arabic corpus from the CoNLL-X shared language
task, which is small enough to quickly experiment
with these issues (Buchholz and Marsi, 2006).
The sentence based sampler, sent, samples from
a distribution that can be exactly computed, facilitat-
ing comparisons between the exact sampler and the
SMC approach. Figure 5.1 compares the posterior
log-likelihoods of the sent sampler and the exact
sentence sampler over 200 iterations. As expected,
the likelihoods of the particle filters approach that of
the exact sentence sampler as the number of particles
increases from 25 to 100, which completely overlaps
the performance of the exact sampler by the 50th it-
eration. This is impressive, because even with 99
additional sequences sampled (one for each particle)
each iteration the SMC approach is still faster than
the exact sampler. Furthermore, the Arabic tagset
has only 20 distinct tags, while other data sets, e.g.
the WSJ and Bulgarian, use tagsets more than twice
as large. The particle filter, which is linear in the
number of tags, should take twice as long per token
0 50 100 150 200Iteration
6.6
6.4
6.2
6.0
5.8
5.6
5.4
5.2
5.0
4.8
Log
-Lik
elih
ood
1e5
Sent: ExactSent: 25 ParticlesSent: 50 ParticlesSent: 100 Particles
Log-Likelihood of Sentence Samplers over 200 Iterations
Figure 1: Posterior Log-Likelihood of PYP-HMM infer-
ence with exact as well as SMC sentence sampler with
various numbers of particles. Error bars represent on
standard deviation over three runs.
sampled on those data, relative to the arabic data.
On the other hand, the forward-backward per token
sample time, which is cubic in tagset size, should
increase at least eightfold. So the time savings im-
prove dramatically as the size of the tagset increases.
Figure 2 compares the table configuration log-
likelihood of the 1HMM approximation imple-
mented by Blunsom and Cohn (2011) with the type
particle filter based sampler as well as the local,
token-based sampler and the exact block sentence
sampler. Unlike the sentence based block sampler,
type sampler cannot be exactly calculated, even
with the 1HMM approach of constraining inference
to only consider sequences that assign the same tag
to every token of the same word type. The 1HMM
sampler approximates these probabilities using ex-
pected table counts. Theoretically, the type sam-
pler should be a better approximation, being guar-
anteed to approach the true distribution as the num-
ber of particles increases. However, the type sam-
pler does not constrain inference as the 1HMM does,
slowing convergence by wasting particles on less
likely tag sequences. As expected, the type sam-
pler converges with the 1HMM sampler with suffi-
ciently many particles in a few iterations. The exact
block sentence sampler surpases approaches by iter-
ation 75 and does not seem to have converged by the
end of 200 iterations.
The local sampler samples a single site at a time
with a standard, token-based Gibbs sampler. The
51
0 50 100 150 200Iteration
6.6
6.4
6.2
6.0
5.8
5.6
5.4
5.2
5.0
4.8
Log
-Lik
elih
ood
1e5
1HMMLocalSent: ExactType: 100 Particles
Log-Likelihood Comparison of Samplers over 200 Iterations
Figure 2: Posterior Log-Likelihood of PYP-HMM infer-
ence with particle filters, 1HMM approximation, and lo-
cal samplers. Error bars represent one standard deviation
over three runs.
local sampler performs surprisingly well, averaging
a slightly higher likelihood than both the 1HMM
and the type samplers. This may be an indication
that the PYP-HMM model is not too tightly coupled
for the local sampler to eventually migrate toward
more likely modes. Note that both the type and
1HMM samplers initially take much larger steps,
before eventually hitting a plateau. This suggests
that some sort of mixed sampler may outperform
its component samplers by only occasionally taking
large steps.
5.2 Unsupervised Part-of-Speech Tagging
The samplers evaluated in section 5.1 induce syn-
tactic categories analogous to PoS tags. However,
to induce PoS tags, each syntactic category must be
assigned to a PoS tag in the tagset. Each site in a
corpus is assigned the most commonly visited syn-
tactic category at that site over all iterations. The
many-to-one (M-1) assignment for a category is the
most common gold standard PoS tag of the tokens
assigned to that category. While this assignment
theoretically allows a perfect accuracy if each token
is assigned to its own category, these experiments
limit the number of induced categories to the size of
the tagset. Table 1 compares the M-1 accuracy of
the sent and type particle filter samplers, from
sections 4.1 and 4.2, with 100 particles each. The
particle filter based samplers rarely score a higher
accuracy than even the local sampler, which com-
pletes 500 iterations before the particle filters com-
plete 200.
While figure 2 shows that the sentence based
block sampler eventually surpasses the 1HMM sam-
pler in likelihood, the accuracies of the 1HMM and
1HMM-LM approximations remain well above the
other approaches. The 1HMM sampler and the 100
particle type sampler have approximately the same
likelihood, yet the M-1 accuracy of the 1HMM sam-
pler is much higher. This suggests that there are
high-likelihood assignments that produce lower ac-
curacy results, presumably related to the fact that the
type sampler is not restricted to assignments with
exactly one tag for each word type. If the model
assigns equal likelihood to these assignments, infer-
ence will not be able to distinguish between them.
Perhaps a model that assigned higher likelihoods to
tag sequences with fewer tags per word type would
have a stronger correlation between likelihood and
accuracy.
6 Future Work
While the results leave much room for improvement,
the approach presented here is the most basic of par-
ticle methods. There has been considerable research
in improvements to particle methods since their in-
troduction in 1993 (Gordon et al., 1993). Two com-
mon approaches to improving particle filters are re-
sampling and particle smoothing (Doucet and Jo-
hansen, 2009; Godsill and Clapp, 2001; Pitt, 2002).
Resampling ensures that particles aren?t wasted on
unlikely sequences. Particle smoothing reduces the
variability of the marginal distributions by combin-
ing the final particles.
7 Conclusion
This paper presented a preliminary approach to in-
corporating particle methods into computational lin-
guistic inference applications. Such approaches
show great potential for inference even in highly de-
pendent distributions, but at a serious computational
cost. However, the type particle filter itself can be
largely run in parallel, only bottlenecking when the
particle weights need to be normalized. Further ex-
pansion of the basic ideas presented will enable scal-
able inference in otherwise intractable models.
52
Language Sent-100 Type-100 Local 1HMM 1HMM-LM Tokens Tag types
WSJ 69.8% 70.1% 70.2% 75.6% 77.5% 1,173,766 45
Arabic 53.5% 57.6% 56.2% 61.9% 62.0% 54,379 20
Bulgarian 64.8% 67.8% 67.6% 71.4% 76.2% 190,217 54
Czech 59.8% 61.6% 64.5% 65.4% 67.9% 1,249,408 12c
Danish 65.0% 70.3% 69.1% 70.6% 74.6% 94,386 25
Dutch 61.6% 71.6% 64.1% 73.2% 72.9% 195,069 13c
Hungarian 61.8% 61.8% 64.8% 69.6% 73.2% 131,799 43
Portuguese 59.4% 71.1% 68.1% 72.0% 77.1% 206,678 22
Table 1: Many-to-1 accuracies on CoNLL and Penn-Treebank Wall Street Journal corpora for sentence- (Sent) and
type- (Type) based filtering. The table lists the average M-1 accuracy measured according to the maximum marginal
tag assignments over 3 seperate runs after 200 iterations for the sent, type, 1HMM and 1HMM-LM samplers,
and 500 iterations for the HMM local sampler. The 1HMM-LM model has been shown to achieve state-of-the-art
unsupervised M-1 accuracies on these datasets. and thus represents the limit of unsupervised M-1 accuracy.
References
