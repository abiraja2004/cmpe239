Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 79?84,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Second-Order Joint Eisner Model for Syntactic and Semantic Dependency
Parsing
Xavier Llu??s Stefan Bott Llu??s Ma`rquez
TALP Research Center ? Software Department (LSI)
Technical University of Catalonia (UPC)
{xlluis,sbott,lluism}@lsi.upc.edu
Abstract
We present a system developed for the
CoNLL-2009 Shared Task (Hajic? et al, 2009).
We extend the Carreras (2007) parser to
jointly annotate syntactic and semantic depen-
dencies. This state-of-the-art parser factor-
izes the built tree in second-order factors. We
include semantic dependencies in the factors
and extend their score function to combine
syntactic and semantic scores. The parser is
coupled with an on-line averaged perceptron
(Collins, 2002) as the learning method. Our
averaged results for all seven languages are
71.49 macro F1, 79.11 LAS and 63.06 seman-
tic F1.
1 Introduction
Systems that jointly annotate syntactic and semantic
dependencies were introduced in the past CoNLL-
2008 Shared Task (Surdeanu et al, 2008). These
systems showed promising results and proved the
feasibility of a joint syntactic and semantic pars-
ing (Henderson et al, 2008; Llu??s and Ma`rquez,
2008).
The Eisner (1996) algorithm and its variants are
commonly used in data-driven dependency pars-
ing. Improvements of this algorithm presented
by McDonald et al (2006) and Carreras (2007)
achieved state-of-the-art performance for English in
the CoNLL-2007 Shared Task (Nivre et al, 2007).
Johansson and Nugues (2008) presented a sys-
tem based on the Carreras? extension of the Eis-
ner algorithm that ranked first in the past CoNLL-
2008 Shared Task. We decided to extend the Car-
reras (2007) parser to jointly annotate syntactic and
semantic dependencies.
The present year Shared Task has the incentive
of being multilingual with each language presenting
their own particularities. An interesting particularity
is the direct correspondence between syntactic and
semantic dependencies provided in Catalan, Spanish
and Chinese. We believe that these correspondences
can be captured by a joint system. We specially look
at the syntactic-semantic alignment of the Catalan
and Spanish datasets.
Our system is an extension of the Llu??s and
Ma`rquez (2008) CoNLL-2008 Shared Task system.
We introduce these two following novelties:
? An extension of the second-order Car-
reras (2007) algorithm to annotate semantic
dependencies.
? A combined syntactic-semantic scoring for
Catalan and Spanish to exploit the syntactic-
semantic mappings.
The following section outlines the system archi-
tecture. The next sections present in more detail the
system novelties.
2 Architecture
The architecture consists on four main components:
1) Preprocessing and feature extraction. 2) Syntactic
preparsing. 3) Joint syntactic-semantic parsing. 4)
Predicate classification.
The preprocessing and feature extraction is in-
tended to ease and improve the performance of
the parser precomputing a binary representation of
79
each sentence features. These features are borrowed
from existing and widely-known systems (Xue and
Palmer, 2004; McDonald et al, 2005; Carreras et al,
2006; Surdeanu et al, 2007).
The following step is a syntactic pre-parse. It
is only required to pre-compute additional features
(e.g., syntactic path, syntactic frame) from the syn-
tax. These new features will be used for the semantic
role component of the following joint parser.
The joint parser is the core of the system. This
single algorithm computes the complete parse that
optimizes a score according to a function that de-
pends on both syntax and semantics. Some of the
required features that could be unavailable or expen-
sive to compute at that time are provided by the pre-
vious syntactic pre-parse.
The predicate sense classification is performed as
the last step. Therefore no features representing the
predicate sense are employed during the training.
The predicates are labeled with the most frequent
sense extracted from the training corpus.
No further postprocessing is applied.
3 Second-order Eisner model
The Carreras? extension of the Eisner inference al-
gorithm is an expensive O(n4) parser. The number
of assignable labels for each dependency is a hidden
multiplying constant in this asymptotic cost.
We begin describing a first-order dependency
parser. It receives a sentence x and outputs a de-
pendency tree y. A dependency, or first-order factor,
is defined as f1 = ?h,m, l?. Where h is the head
token, m the modifier and l the syntactic label. The
score for this factor f1 is computed as:
score1(f1, x,w) = ?(h,m, x) ?w(l)
Where w(l) is the weight vector for the syntactic la-
bel l and ? a feature extraction function.
The parser outputs the best tree y? from the set
T (x) of all projective dependency trees.
y?(x) = argmax
y?T (x)
?
f1?y
score(f1, x,w)
The second-order extension decomposes the de-
pendency tree in factors that include some children
of the head and modifier. A second-order factor is:
f2 = ?h,m, l, ch, cmo, cmi?
where ch is the daughter of h closest to m within
the tokens [h, . . . ,m]; cmo is the outermost daugh-
ter of m outside [h, . . . ,m]; and cmi is the furthest
daughter of m inside [h, . . . ,m].
The score for these new factors is computed by
score2(f2, x,w) = ?(h,m, x) ?w(l) +
?(h,m, ch, x) ?w(l)ch +
?(h,m, cmi, x) ?w(l)cmi +
?(h,m, cmo, x) ?w(l)cmo
The parser builds the best-scoring projective tree
factorized in second-order factors. The score of the
tree is also defined as the sum of the score of its
factors.
3.1 Joint second-order model
We proceeded in an analogous way in which the
Llu??s and Ma`rquez (2008) extended the first-order
parser. That previous work extended a first-order
model by including semantic labels in first-order de-
pendencies.
Now we define a second-order joint factor as:
f2syn-sem =?
h,m, l, ch, cmo, cmi, lsemp1 , . . . , lsempq
?
Note that we only added a set of semantic labels
lsemp1 , . . . , lsempq to the second-order factor. Each
one of these semantic labels represent, if any, one
semantic relation between the argument m and the
predicate pi. There are q predicates in the sentence,
labeled p1, . . . , pq.
The corresponding joint score to a given joint fac-
tor is computed by adding a semantic score to the
previously defined score2 second-order score func-
tion:
score2syn-sem(f2syn-sem, x,w) =
score2(f2, x,w) +
?
pi
scoresem(h,m, pi, lsempi , x,w)
q
where,
scoresem(h,m, pi, lsem, x,w) =
?sem(h,m, pi, x) ?w(lsem)
80
We normalize the semantic score by the number
of predicates q. The semantic score is computed as a
score betweenm and each sentence predicate pi. No
second-order relations are considered in these score
functions. The search of the best ch, cmo and cmi is
independent of the semantic components of the fac-
tor. The computational cost of the algorithm is in-
creased by one semantic score function call for every
m, h, and pi combination. The asymptotic cost of
this operation is O(q ? n2) and it is sequentially per-
formed among other O(n2) operations in the main
loop of the algorithm.
Algorithm 1 Extension of the Carreras (2007) algo-
rithm
C[s][t][d][m]? 0, ?s, t, d,m
O[s][t][d][l]? 0,?s, t, d, l
for k = 1, . . . , n do
for s = 0, . . . , n? k do
t? s+ k
?l O[s][t][?][l] = maxr,cmi,ch
C[s][r][?][cmi] + C[r + 1][t][?][ch]
+score(t, s, l)+scorecmi(t, s, cmi, l)+
scorech(t, s, l, ch)+?
pi maxlsemscoresem(t, s, pi, lsem)/q
?l O[s][t][?][l] = maxr,cmi,ch
C[s][r][?][ch] + C[r + 1][t][?][cmi]+
score(s, t, l)+scorecmi(s, t, cmi, l)+
scorech(s, t, l, ch)+?
pi maxlsemscoresem(t, s, pi, lsem)/q
?m C[s][t][?][m] = maxl,cmo
C[s][m][?][cmo] +O[m][t][?][l]+
scorecmo(s,m, l, cmo)
?m C[s][t][?][m] = maxl,cmo
O[s][m][?][l] + C[m][t][?][cmo]+
scorecmo(m, t, l, cmo)
end for
end for
Our implementation slightly differs from the orig-
inal Carreras algorithm description. The main dif-
ference is that no specific features are extracted for
the second-order factors. This allows us to reuse the
feature extraction mechanism of a first-order parser.
Algorithm 1 shows the Carreras? extension of the
Eisner algorithm including our proposed joint se-
mantic scoring.
The tokens s and t represent the start and end
tokens of the current substring, also called span.
The direction d ? {?,?} defines whether t or
s is the head of the last dependency built inside
the span. The score functions scorech,scorecmi and
scorecmo are the linear functions that build up the
previously defined second-order global score, e.g.,
scorech= ?(h,m, ch, x)?w(l)ch . The two tablesC and
O maintain the dynamic programming structures.
Note that the first steps of the inner loop are ap-
plied for all l, the syntactic label, but the semantic
score function does not depend on l. Therefore the
best semantic label can be chosen independently.
For simplicity, we omitted the weight vectors re-
quired in each score function and the backpointers
tables to save the local decisions. We also omit-
ted the definition of the domain of some variables.
Moreover, the filter of the set of assignable labels
is not shown. A basic filter regards the POS of the
head and modifier to filter out the set of possible ar-
guments for each predicate. Another filter extract
the set of allowed arguments for each predicate from
the frames files. These last filters were applied to the
English, German and Chinese.
3.2 Catalan and Spanish joint model
The Catalan and Spanish datasets (Taule? et al, 2008)
present two interesting properties. The first prop-
erty, as previously said, is a direct correspondence
between syntactic and semantic labels. The second
interesting property is that all semantic dependen-
cies exactly overlap with the syntactic tree. Thus
the semantic dependency between a predicate and
an argument always has a matching syntactic depen-
dency between a head and a modifier. The Chinese
data also contains direct syntactic-semantic map-
pings. But due to the Shared Task time constraints
we did not implemented a specific parsing method
for this language.
The complete overlap between syntax and seman-
tics can simplify the definition of a second-order
joint factor. In this case, a second-order factor will
only have, if any, one semantic dependency. We only
allow at most one semantic relation lsem between
the head token h and the modifier m. Note that h
must be a sentence predicate and m its argument if
81
lsem is not null. We extend the second-order fac-
tors with a single and possibly null semantic label,
i.e., f2syn-sem = ?h,m, l, ch, cmo, cmi, lsem?. This
slightly simplifies the scoring function:
score2syn-sem(f2syn-sem, x,w) =
score2(f2, x,w) +
? ? scoresem(h,m, x,w)
where ? is an adjustable parameter of the model and,
scoresem(h,m, x,w) = ?sem(h,m, x) ?w(lsem)
The next property that we are intended to exploit
is the syntactic-semantic mappings. These map-
pings define the allowed combinations of syntactic
and semantic labels. The label combinations can
only be exploited when there is semantic depen-
dency between the head h and the modifier m of a
factor. An argument identification classifier deter-
mines the presence of a semantic relation, given h
is a predicate. In these cases we only generate fac-
tors that are compliant with the mappings. If a syn-
tactic label has many corresponding semantic labels
we will score all of them and select the combination
with the highest score.
The computational cost is not significantly in-
creased as there is a bounded number of syntactic
and semantic combinations to score. In addition, the
only one-argument-per-factor constraint reduces the
complexity of the algorithm with respect to the pre-
vious joint extension.
We found some inconsistencies in the frames files
provided by the organizers containing the correspon-
dences between syntax and semantics. For this rea-
son we extracted them directly from the corpus. The
extracted mappings discard the 7.9% of the cor-
rect combinations in the Catalan development cor-
pus that represent a 1.7% of its correct syntactic de-
pendencies. The discarded semantic labels are the
5.14% for Spanish representing the 1.3% of the syn-
tactic dependencies.
4 Results and discussion
Table 1 shows the official results for all seven lan-
guages, including out-of-domain data labeled as
ood. The high computational cost of the second-
order models prevented us from carefully tuning the
system parameters. After the shared task evaluation
deadline, some bug were corrected, improving the
system performance. The last results are shown in
parenthesis.
The combined filters for Catalan and Spanish hurt
the parsing due to the discarded correct labels but
we believe that this effect is compensated by an im-
proved precision in the cases where the correct la-
bels are not discarded. For example, in Spanish
these filters improved the syntactic LAS from 85.34
to 86.77 on the development corpus using the gold
syntactic tree as the pre-parse tree.
Figure 1 shows the learning curve for the English
and Czech language. The results are computed in
the development corpus. The semantic score is com-
puted using gold syntax and gold predicate sense
classification. We restricted the learning curve to
the first epoch. Although the this first epoch is very
close to the best score, some languages showed im-
provements until the fourth epoch. In the figure we
can see better syntactic results for the joint system
with respect to the syntactic-only parser. We should
not consider this improvement completely realistic
as the semantic component of the joint system uses
gold features (i.e., a gold pre-parse). Nonetheless,
it points that a highly accurate semantic component
could improve the syntax.
Table 2 shows the training time for a second-order
syntactic and joint configurations of the parser. Note
that the time per instance is an average and some
sentences could require a significantly higher time.
Recall that our parser is O(n4) dependant on the
sentence length. We discarded large sentences dur-
ing training for efficiency reasons. We discarded
sentences with more than 70 words for all languages
except for Catalan and Spanish where the thresh-
old was set to 100 words in the syntactic parser.
This larger number of sentences is aimed to im-
prove the syntactic performance of these languages.
The shorter sentences used in the joint parsing and
the pruning of the previously described filters re-
duced the training time for Catalan and Spanish. The
amount of main memory consumed by the system is
0.5?1GB. The machine used to perform the compu-
tations is an AMD64 Athlon 5000+.
82
avg cat chi cze eng ger jap spa
macro F1 71.49 (74.90) 56.64 (73.21) 66.18 (70.91) 75.95 81.69 72.31 81.76 65.91 (68.46)
syn LAS 79.11 (82.22) 64.21(84.20) 70.53 (70.90) 75.00 87.48 81.94 91.55 83.09 (84.48)
semantic F1 63.06 (67.41) 46.79 (61.68) 59.72 (70.88) 76.90 75.86 62.66 71.60 47.88 (52.30)
ood macro F1 71.92 - - 74.56 73.91 67.30 - -
ood syn LAS 75.09 - - 72.11 80.92 72.25 - -
ood sem F1 68.74 - - 77.01 66.88 62.34 - -
Table 1: Overall results. In parenthesis post-evaluation results.
cat chi cze eng ger jap spa
syntax only (s/sentence) 18.39 8.07 3.18 2.56 1.30 1.07 15.31
joint system (s/sentence) 10.91 9.49 3.99 3.13 2.36 1.25 12.29
Table 2: Parsing time per sentence.
 
70
 
72
 
74
 
76
 
78
 
80
 
82
 
84
 
86
 
88
 
90
 
92  10
 
20
 
30
 
40
 
50
 
60
 
70
 
80
 
90
 
100
semanic f1, LAS
% of c
orpus
syn cz
syn cz
 joint
sem 
cz joint syn eng
syn en
g joint
sem 
eng join
t
Figure 1: Learning curves for the syntactic-only and joint
parsers in Czech and English.
5 Conclusion
We have shown that a joint syntactic-semantic
parsing can be based on the state-of-the-art Car-
reras (2007) parser at an expense of a reasonable
cost. Our second-order parser still does not repro-
duce the state-of-the art results presented by similar
systems (Nivre et al, 2007). Although we achieved
mild results we believe that a competitive system
based in our model can be built. Further tuning is
required and a complete set of new second-order fea-
tures should be implemented to improve our parser.
The multilingual condition of the task allows us to
evaluate our approach in seven different languages.
A detailed language-dependent evaluation can give
us some insights about the strengths and weaknesses
of our approach across different languages. Unfor-
tunately we believe that this objective was possibly
not accomplished due to the time constraints.
The Catalan and Spanish datasets presented in-
teresting properties that could be exploited. The
mapping between syntax and semantics should be
specially useful for a joint system. In addition
the semantic dependencies for these languages are
aligned with the projective syntactic dependencies,
i.e., the predicate-argument pairs exactly match syn-
tactic dependencies. This is a useful property to si-
multaneously build joint dependencies.
6 Future and ongoing work
Our syntactic and semantic parsers, as many others,
is not exempt of bugs. Furthermore, very few tuning
and experimentation was done during the develop-
ment of our parser due to the Shared Task time con-
straints. We believe that we still did not have enough
data to fully evaluate our approach. Further exper-
imentation is required to asses the improvement of
a joint architecture vs. a pipeline architecture. Also
a careful analysis of the system across the different
languages is to be performed.
Acknowledgments
We thank the corpus providers (Taule? et al, 2008;
Palmer and Xue, 2009; Hajic? et al, 2006; Surdeanu
et al, 2008; Burchardt et al, 2006; Kawahara et al,
2002) for their effort in the annotation and conver-
sion of the seven languages datasets.
83
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Xavier Carreras, Mihai Surdeanu, and Llu??s Ma`rquez.
2006. Projective dependency parsing with perceptron.
In Proceedings of the 10th Conference on Computa-
tional Natural Language Learning (CoNLL-2006).
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
11th Conference on Computational Natural Language
Learning (CoNLL-2007).
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the ACL-02 conference on Empirical methods in natu-
ral language processing.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of the 16th International Conference on Compu-
tational Linguistics (COLING-96).
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
James Henderson, Paola Merlo, Gabriele Musillo, and
Ivan Titov. 2008. A latent variable model of syn-
chronous parsing for syntactic and semantic depen-
dencies. In Proceedings of the 12th Conference on
Computational Natural Language Learning (CoNLL-
2008), Manchester, UK.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with propbank and nombank. In Proceedings of the
12th Conference on Computational Natural Language
Learning (CoNLL-2008), Manchester, UK.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Xavier Llu??s and Llu??s Ma`rquez. 2008. A joint model
for parsing syntactic and semantic dependencies. In
Proceedings of the 12th Conference on Computational
Natural Language Learning (CoNLL-2008), Manch-
ester, UK.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In 11th Conference of the European Chapter of
the Association for Computational Linguistics (EACL-
2006).
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL-
2005).
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Mihai Surdeanu, Llu??s Ma`rquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination strategies for se-
mantic role labeling. Journal of Artificial Intelligence
Research.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008).
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Nianwen Xue and Martha Palmer. 2004. Calibrating fea-
tures for semantic role labeling. In Proceedings of the
Empirical Methods in Natural Language Processing
(EMNLP-2004).
84
