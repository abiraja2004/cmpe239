Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 690?696,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Scalable Modified Kneser-Ney Language Model Estimation
Kenneth Heafield?,? Ivan Pouzyrevsky? Jonathan H. Clark? Philipp Koehn?
?University of Edinburgh
10 Crichton Street
Edinburgh EH8 9AB, UK
?Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
?Yandex
Zelenograd, bld. 455 fl. 128
Moscow 124498, Russia
heafield@cs.cmu.edu ivan.pouzyrevsky@gmail.com jhclark@cs.cmu.edu pkoehn@inf.ed.ac.uk
Abstract
We present an efficient algorithm to es-
timate large modified Kneser-Ney mod-
els including interpolation. Streaming
and sorting enables the algorithm to scale
to much larger models by using a fixed
amount of RAM and variable amount of
disk. Using one machine with 140 GB
RAM for 2.8 days, we built an unpruned
model on 126 billion tokens. Machine
translation experiments with this model
show improvement of 0.8 BLEU point
over constrained systems for the 2013
Workshop on Machine Translation task in
three language pairs. Our algorithm is also
faster for small models: we estimated a
model on 302 million tokens using 7.7%
of the RAM and 14.0% of the wall time
taken by SRILM. The code is open source
as part of KenLM.
1 Introduction
Relatively low perplexity has made modified
Kneser-Ney smoothing (Kneser and Ney, 1995;
Chen and Goodman, 1998) a popular choice for
language modeling. However, existing estima-
tion methods require either large amounts of RAM
(Stolcke, 2002) or machines (Brants et al, 2007).
As a result, practitioners have chosen to use
less data (Callison-Burch et al, 2012) or simpler
smoothing methods (Brants et al, 2007).
Backoff-smoothed n-gram language models
(Katz, 1987) assign probability to a word wn in
context wn?11 according to the recursive equation
p(wn|wn?11 ) =
{
p(wn|wn?11 ), if wn1 was seen
b(wn?11 )p(wn|wn2 ), otherwise
The task is to estimate probability p and backoff
b from text for each seen entry wn1 . This paper
Filesystem
Map
Reduce 1
Filesystem
Identity Map
Reduce 2
Filesystem ...
MapReduce Steps
Filesystem
Map
Reduce 1
Reduce 2...
Optimized
Figure 1: Each MapReduce performs three copies
over the network when only one is required. Ar-
rows denote copies over the network (i.e. to and
from a distributed filesystem). Both options use
local disk within each reducer for merge sort.
contributes an efficient multi-pass streaming algo-
rithm using disk and a user-specified amount of
RAM.
2 Related Work
Brants et al (2007) showed how to estimate
Kneser-Ney models with a series of five MapRe-
duces (Dean and Ghemawat, 2004). On 31 billion
words, estimation took 400 machines for two days.
Recently, Google estimated a pruned Kneser-Ney
model on 230 billion words (Chelba and Schalk-
wyk, 2013), though no cost was provided.
Each MapReduce consists of one layer of map-
pers and an optional layer of reducers. Mappers
read from a network filesystem, perform optional
processing, and route data to reducers. Reducers
process input and write to a network filesystem.
Ideally, reducers would send data directly to an-
other layer of reducers, but this is not supported.
Their workaround, a series of MapReduces, per-
forms unnecessary copies over the network (Fig-
ure 1). In both cases, reducers use local disk.
690
Writing and reading from the distributed filesys-
tem improves fault tolerance. However, the same
level of fault tolerance could be achieved by
checkpointing to the network filesystem then only
reading in the case of failures. Doing so would en-
able reducers to start processing without waiting
for the network filesystem to write all the data.
Our code currently runs on a single machine
while MapReduce targets clusters. Appuswamy et
al. (2013) identify several problems with the scale-
out approach of distributed computation and put
forward several scenarios in which a single ma-
chine scale-up approach is more cost effective in
terms of both raw performance and performance
per dollar.
Brants et al (2007) contributed Stupid Backoff,
a simpler form of smoothing calculated at runtime
from counts. With Stupid Backoff, they scaled to
1.8 trillion tokens. We agree that Stupid Backoff
is cheaper to estimate, but contend that this work
makes Kneser-Ney smoothing cheap enough.
Another advantage of Stupid Backoff has been
that it stores one value, a count, per n-gram in-
stead of probability and backoff. In previous work
(Heafield et al, 2012), we showed how to collapse
probability and backoff into a single value without
changing sentence-level probabilities. However,
local scores do change and, like Stupid Backoff,
are no longer probabilities.
MSRLM (Nguyen et al, 2007) aims to scal-
ably estimate language models on a single ma-
chine. Counting is performed with streaming algo-
rithms similarly to this work. Their parallel merge
sort also has the potential to be faster than ours.
The biggest difference is that their pipeline de-
lays some computation (part of normalization and
all of interpolation) until query time. This means
that it cannot produce a standard ARPA file and
that more time and memory are required at query
time. Moreover, they use memory mapping on en-
tire files and these files may be larger than physi-
cal RAM. We have found that, even with mostly-
sequential access, memory mapping is slower be-
cause the kernel does not explicitly know where
to read ahead or write behind. In contrast, we use
dedicated threads for reading and writing. Perfor-
mance comparisons are omitted because we were
unable to compile and run MSRLM on recent ver-
sions of Linux.
SRILM (Stolcke, 2002) estimates modified
Kneser-Ney models by storing n-grams in RAM.
Corpus
Counting
Adjusting Counts
DivisionSumming
Interpolation
Model
Figure 2: Data flow in the estimation pipeline.
Normalization has two threads per order: sum-
ming and division. Thick arrows indicate sorting.
It also offers a disk-based pipeline for initial steps
(i.e. counting). However, the later steps store
all n-grams that survived count pruning in RAM.
Without pruning, both options use the same RAM.
IRSTLM (Federico et al, 2008) does not imple-
ment modified Kneser-Ney but rather an approxi-
mation dubbed ?improved Kneser-Ney? (or ?mod-
ified shift-beta? depending on the version). Esti-
mation is done in RAM. It can also split the corpus
into pieces and separately build each piece, intro-
ducing further approximation.
3 Estimation Pipeline
Estimation has four streaming passes: counting,
adjusting counts, normalization, and interpolation.
Data is sorted between passes, three times in total.
Figure 2 shows the flow of data.
3.1 Counting
For a language model of order N , this step counts
all N -grams (with length exactly N ) by streaming
through the corpus. Words near the beginning of
sentence also formN -grams padded by the marker
<s> (possibly repeated multiple times). The end
of sentence marker </s> is appended to each sen-
tence and acts like a normal token.
Unpruned N -gram counts are sufficient, so
lower-order n-grams (n < N ) are not counted.
Even pruned models require unpruned N -gram
counts to compute smoothing statistics.
Vocabulary mapping is done with a hash table.1
Token strings are written to disk and a 64-bit Mur-
1This hash table is the only part of the pipeline that can
grow. Users can specify an estimated vocabulary size for
memory budgeting. In future work, we plan to support lo-
cal vocabularies with renumbering.
691
Suffix
3 2 1
Z B A
Z A B
B B B
Context
2 1 3
Z A B
B B B
Z B A
Figure 3: In suffix order, the last word is primary.
In context order, the penultimate word is primary.
murHash2 token identifier is retained in RAM.
Counts are combined in a hash table and spilled
to disk when a fixed amount of memory is full.
Merge sort also combines identical N -grams (Bit-
ton and DeWitt, 1983).
3.2 Adjusting Counts
The counts c are replaced with adjusted counts a.
a(wn1 ) =
{
c(wn1 ), if n = N or w1 = <s>
|v : c(vwn1 ) > 0|, otherwise
Adjusted counts are computed by streaming
through N -grams sorted in suffix order (Figure 3).
The algorithm keeps a running total a(wNi ) for
each i and compares consecutive N -grams to de-
cide which adjusted counts to output or increment.
Smoothing statistics are also collected. For each
length n, it collects the number tn,k of n-grams
with adjusted count k ? [1, 4].
tn,k = |{wn1 : a(wn1 ) = k}|
These are used to compute closed-form estimates
(Chen and Goodman, 1998) of discounts Dn(k)
Dn(k) = k ?
(k + 1)tn,1tn,k+1
(tn,1 + 2tn,2)tn,k
for k ? [1, 3]. Other cases are Dn(0) = 0 and
Dn(k) = Dn(3) for k ? 3. Less formally, counts
0 (unknown) through 2 have special discounts.
3.3 Normalization
Normalization computes pseudo probability u
u(wn|wn?11 ) =
a(wn1 )?Dn(a(wn1 ))?
x a(wn?11 x)
and backoff b
b(wn?11 ) =
?3
i=1Dn(i)|{x : a(wn?11 x) = i}|?
x a(wn?11 x)
2https://code.google.com/p/smhasher/
The difficulty lies in computing denominator?
x a(wn?11 x) for all wn?11 . For this, we sort in
context order (Figure 3) so that, for every wn?11 ,
the entries wn?11 x are consecutive. One pass col-
lects both the denominator and backoff3 terms
|{x : a(wn?11 x) = i}| for i ? [1, 3].
A problem arises in that denominator?
x a(wn?11 x) is known only after streaming
through all wn?11 x, but is needed immediately
to compute each u(wn|wn?11 ). One option is to
buffer in memory, taking O(N |vocabulary|) space
since each order is run independently in parallel.
Instead, we use two threads for each order. The
sum thread reads ahead to compute?x a(wn?11 x)
and b(wn?11 ) then places these in a secondary
stream. The divide thread reads the input and the
secondary stream then writes records of the form
(wn1 , u(wn|wn?11 ), b(wn?11 )) (1)
The secondary stream is short so that data read by
the sum thread will likely be cached when read by
the divide thread. This sort of optimization is not
possible with most MapReduce implementations.
Because normalization streams through wn?11 x
in context order, the backoffs b(wn?11 ) are com-
puted in suffix order. This will be useful later
(?3.5), so backoffs are written to secondary files
(one for each order) as bare values without keys.
3.4 Interpolation
Chen and Goodman (1998) found that perplex-
ity improves when the various orders within the
same model are interpolated. The interpolation
step computes final probability p according to the
recursive equation
p(wn|wn?11 ) = u(wn|wn?11 )+b(wn?11 )p(wn|wn?12 )
(2)
Recursion terminates when unigrams are interpo-
lated with the uniform distribution
p(wn) = u(wn) + b()
1
|vocabulary|
where  denotes the empty string. The unknown
word counts as part of the vocabulary and has
count zero,4 so its probability is b()/|vocabulary|.
3Sums and counts are done with exact integer arithmetic.
Thus, every floating-point value generated by our toolkit is
the result of O(N) floating-point operations. SRILM has nu-
merical precision issues because it uses O(N |vocabulary|)
floating-point operations to compute backoff.
4SRILM implements ?another hack? that computes
pSRILM(wn) = u(wn) and pSRILM(<unk>) = b() when-
ever p(<unk>) < 3? 10?6, as it usually is. We implement
both and suspect their motivation was numerical precision.
692
Probabilities are computed by streaming in suf-
fix lexicographic order: wn appears before wnn?1,
which in turn appears before wnn?2. In this way,
p(wn) is computed before it is needed to compute
p(wn|wn?1), and so on. This is implemented by
jointly iterating through N streams, one for each
length of n-gram. The relevant pseudo probability
u(wn|wn?11 ) and backoff b(wn?11 ) appear in the
input records (Equation 1).
3.5 Joining
The last task is to unite b(wn1 ) computed in ?3.3
with p(wn|wn?11 ) computed in ?3.4 for storage in
the model. We note that interpolation (Equation 2)
used the different backoff b(wn?11 ) and so b(wn1 )
is not immediately available. However, the back-
off values were saved in suffix order (?3.3) and in-
terpolation produces probabilities in suffix order.
During the same streaming pass as interpolation,
we merge the two streams.5 Suffix order is also
convenient because the popular reverse trie data
structure can be built in the same pass.6
4 Sorting
Much work has been done on efficient disk-based
merge sort. Particularly important is arity, the
number of blocks that are merged at once. Low
arity leads to more passes while high arity in-
curs more disk seeks. Abello and Vitter (1999)
modeled these costs and derived an optimal strat-
egy: use fixed-size read buffers (one for each
block being merged) and set arity to the number of
buffers that fit in RAM. The optimal buffer size is
hardware-dependent; we use 64 MB by default. To
overcome the operating system limit on file han-
dles, multiple blocks are stored in the same file.
To further reduce the costs of merge sort, we
implemented pipelining (Dementiev et al, 2008).
If there is enough RAM, input is lazily merged
and streamed to the algorithm. Output is cut into
blocks, sorted in the next step?s desired order, and
then written to disk. These optimizations elim-
inate up to two copies to disk if enough RAM
is available. Input, the algorithm, block sorting,
and output are all threads on a chain of producer-
consumer queues. Therefore, computation and
disk operations happen simultaneously.
5Backoffs only exist if the n-gram is the context of some
n+ 1-gram, so merging skips n-grams that are not contexts.
6With quantization (Whittaker and Raj, 2001), the quan-
tizer is trained in a first pass and applied in a second pass.
0
10
20
30
40
50
0 200 400 600 800 1000
RA
M
(G
B)
Tokens (millions)
SRI
SRI compact
IRST
This work
Figure 4: Peak virtual memory usage.
0
2
4
6
8
10
12
14
0 200 400 600 800 1000
CP
U
tim
e(
ho
urs
)
Tokens (millions)
SRI
SRI compact
IRST
This work
Figure 5: CPU usage (system plus user).
Each n-gram record is an array of n vocabu-
lary identifiers (4 bytes each) and an 8-byte count
or probability and backoff. At peak, records are
stored twice on disk because lazy merge sort is
not easily amenable to overwriting the input file.
Additional costs are the secondary backoff file (4
bytes per backoff) and the vocabulary in plaintext.
5 Experiments
Experiments use ClueWeb09.7 After spam filter-
ing (Cormack et al, 2011), removing markup, se-
lecting English, splitting sentences (Koehn, 2005),
deduplicating, tokenizing (Koehn et al, 2007),
and truecasing, 126 billion tokens remained.
7http://lemurproject.org/clueweb09/
693
1 2 3 4 5
393 3,775 17,629 39,919 59,794
Table 1: Counts of unique n-grams (in millions)
for the 5 orders in the large LM.
5.1 Estimation Comparison
We estimated unpruned language models in bi-
nary format on sentences randomly sampled from
ClueWeb09. SRILM and IRSTLM were run un-
til the test machine ran out of RAM (64 GB).
For our code, the memory limit was set to 3.5
GB because larger limits did not improve perfor-
mance on this small data. Results are in Figures
4 and 5. Our code used an average of 1.34?1.87
CPUs, so wall time is better than suggested in Fig-
ure 5 despite using disk. Other toolkits are single-
threaded. SRILM?s partial disk pipeline is not
shown; it used the same RAM and took more time.
IRSTLM?s splitting approximation took 2.5 times
as much CPU and about one-third the memory (for
a 3-way split) compared with normal IRSTLM.
For 302 million tokens, our toolkit used 25.4%
of SRILM?s CPU time, 14.0% of the wall time,
and 7.7% of the RAM. Compared with IRSTLM,
our toolkit used 16.4% of the CPU time, 9.0% of
the wall time, and 16.6% of the RAM.
5.2 Scaling
We built an unpruned model (Table 1) on 126 bil-
lion tokens. Estimation used a machine with 140
GB RAM and six hard drives in a RAID5 configu-
ration (sustained read: 405 MB/s). It took 123 GB
RAM, 2.8 days wall time, and 5.4 CPU days. A
summary of Google?s results from 2007 on differ-
ent data and hardware appears in ?2.
We then used this language model as an ad-
ditional feature in unconstrained Czech-English,
French-English, and Spanish-English submissions
to the 2013 Workshop on Machine Translation.8
Our baseline is the University of Edinburgh?s
phrase-based Moses (Koehn et al, 2007) submis-
sion (Durrani et al, 2013), which used all con-
strained data specified by the evaluation (7 billion
tokens of English). It placed first by BLEU (Pap-
ineni et al, 2002) among constrained submissions
in each language pair we consider.
In order to translate, the large model was quan-
tized (Whittaker and Raj, 2001) to 10 bits and
compressed to 643 GB with KenLM (Heafield,
8http://statmt.org/wmt13/
Source Baseline Large
Czech 27.4 28.2
French 32.6 33.4
Spanish 31.8 32.6
Table 2: Uncased BLEU results from the 2013
Workshop on Machine Translation.
2011) then copied to a machine with 1 TB RAM.
Better compression methods (Guthrie and Hepple,
2010; Talbot and Osborne, 2007) and distributed
language models (Brants et al, 2007) could reduce
hardware requirements. Feature weights were re-
tuned with PRO (Hopkins and May, 2011) for
Czech-English and batch MIRA (Cherry and Fos-
ter, 2012) for French-English and Spanish-English
because these worked best for the baseline. Un-
cased BLEU scores on the 2013 test set are shown
in Table 2. The improvement is remarkably con-
sistent at 0.8 BLEU point in each language pair.
6 Conclusion
Our open-source (LGPL) estimation code is avail-
able from kheafield.com/code/kenlm/
and should prove useful to the community. Sort-
ing makes it scalable; efficient merge sort makes
it fast. In future work, we plan to extend to the
Common Crawl corpus and improve parallelism.
Acknowledgements
Miles Osborne preprocessed ClueWeb09. Mo-
hammed Mediani contributed to early designs.
Jianfeng Gao clarified how MSRLM operates.
This work used the Extreme Science and Engi-
neering Discovery Environment (XSEDE), which
is supported by National Science Foundation grant
number OCI-1053575. We used Stampede and
Trestles under allocation TG-CCR110017. Sys-
tem administrators from the Texas Advanced
Computing Center (TACC) at The University of
Texas at Austin made configuration changes on
our request. This work made use of the resources
provided by the Edinburgh Compute and Data Fa-
cility (http://www.ecdf.ed.ac.uk/). The
ECDF is partially supported by the eDIKT ini-
tiative (http://www.edikt.org.uk/). The
research leading to these results has received fund-
ing from the European Union Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment 287658 (EU BRIDGE).
694
References
James M. Abello and Jeffrey Scott Vitter, editors.
1999. External memory algorithms. American
Mathematical Society, Boston, MA, USA.
Raja Appuswamy, Christos Gkantsidis, Dushyanth
Narayanan, Orion Hodson, and Antony Rowstron.
2013. Nobody ever got fired for buying a cluster.
Technical Report MSR-TR-2013-2, Microsoft Re-
search.
Dina Bitton and David J DeWitt. 1983. Duplicate
record elimination in large data files. ACM Trans-
actions on database systems (TODS), 8(2):255?265.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Language Learning, pages 858?867, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
Ciprian Chelba and Johan Schalkwyk, 2013. Em-
pirical Exploration of Language Modeling for the
google.com Query Stream as Applied to Mobile
Voice Search, pages 197?229. Springer, New York.
Stanley Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Harvard
University, August.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427?436. Association for Computational Lin-
guistics.
Gordon V Cormack, Mark D Smucker, and Charles LA
Clarke. 2011. Efficient and effective spam filtering
and re-ranking for large web datasets. Information
retrieval, 14(5):441?465.
Jeffrey Dean and Sanjay Ghemawat. 2004. MapRe-
duce: Simplified data processing on large clusters.
In OSDI?04: Sixth Symposium on Operating Sys-
tem Design and Implementation, San Francisco, CA,
USA, 12.
Roman Dementiev, Lutz Kettner, and Peter Sanders.
2008. STXXL: standard template library for XXL
data sets. Software: Practice and Experience,
38(6):589?637.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh?s machine trans-
lation systems for European language pairs. In Pro-
ceedings of the ACL 2013 Eighth Workshop on Sta-
tistical Machine Translation, Sofia, Bulgaria, Au-
gust.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Proceed-
ings of Interspeech, Brisbane, Australia.
David Guthrie and Mark Hepple. 2010. Storing the
web in memory: Space efficient language mod-
els with constant time retrieval. In Proceedings of
EMNLP 2010, Los Angeles, CA.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2012. Language model rest costs and space-efficient
storage. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, Jeju Island, Korea.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, UK, July. Association for Computational Lin-
guistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, July.
Slava Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech, and Signal Processing, ASSP-35(3):400?
401, March.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In
Proceedings of the IEEE International Conference
on Acoustics, Speech and Signal Processing, pages
181?184.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), Prague, Czech Repub-
lic, June.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit.
Patrick Nguyen, Jianfeng Gao, and Milind Mahajan.
2007. MSRLM: a scalable language modeling
toolkit. Technical Report MSR-TR-2007-144, Mi-
crosoft Research.
695
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
PA, July.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Sev-
enth International Conference on Spoken Language
Processing, pages 901?904.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine trans-
lation. In Proceedings of ACL, pages 512?519,
Prague, Czech Republic.
Edward Whittaker and Bhiksha Raj. 2001.
Quantization-based language model compres-
sion. In Proceedings of Eurospeech, pages 33?36,
Aalborg, Denmark, December.
696
