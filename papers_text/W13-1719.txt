Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 146?151,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Exploring Syntactic Representations
for Native Language Identification
Ben Swanson
Brown University
Providence, RI
chonger@cs.brown.edu
Abstract
Tree Substitution Grammar rules form a large
and expressive class of features capable of rep-
resenting syntactic and lexical patterns that
provide evidence of an author?s native lan-
guage. However, this class of features can
be applied to any general constituent based
model of grammar and previous work has
done little to explore these options, relying
primarily on the common Penn Treebank an-
notation standard. In this work we contrast
the performance of syntactic features for Na-
tive Language Indentification using five dif-
ferent formalisms. The use of different for-
malisms captures complementary information
from second language data, and can be used
in combination to yield classification perfor-
mance superior to any formalism taken on its
own.
1 Introduction
Native Language Identification, the automatic deter-
mination of an author?s native language (L1) from
their writing in a second language (L2), follows a
general trend of supervised classification using fea-
tures extracted from text. These systems can be opti-
mized by both classification algorithm selection and
the integration of diverse feature sets, and in this
work we focus on the latter.
Syntactic features have been shown to provide
a strong discriminative signal of an author?s na-
tive language (Wong and Dras, 2011; Swanson and
Charniak, 2012), but little work has been done to ex-
plore the various options for representation of syn-
tax of learner text. Many such representations ex-
ist, and are routinely employed to improve perfor-
mance on the widely studied task of parsing the Penn
Treebank. Furthermore, most techniques that prove
widely successful at this task have publicly available
implementations, making them very feasible options
for NLI systems.
In this work we investigate the use of Tree Sub-
stitution Grammars as features for NLI, focusing on
the implication of syntactic paradigm (constituent vs
dependency grammar) and the addition of annota-
tions that have proved useful in statistical parsing.
A Tree Substitution Grammar (TSG) is an intuitive
extension of the Context Free Grammar (CFG) that
allows rewrite rules of arbitrary tree structure. Alter-
natively, a CFG can be seen as a TSG in which the
rewrite rules obey the constraint that each is a tree
structure of unit depth.
While a collection of parsed data can be poten-
tially generated by a TSG that is exponential in the
length of the text, recent techniques allow for the ef-
ficient induction of compact grammars (Cohn and
Blunsom, 2010). At a high level, this technique
employs the rich-get-richer dynamics of a Dirich-
let Process to sample derivations for the trees in the
training corpus: the more that a rule is used in other
derivations, the more likely it is that we will choose
it when sampling a derivation.
We follow previous work in stylometry with
TSGs for the NLI in that we parse the entirety of
the training data and use it to induce a compact TSG
using the method described above.1 We then use the
1An alternative method of note that we do not consider in
this work is to induce TSG rules on hand-annotated data such
as the Penn Treebank, as in Bergsma et al (2012).
146
S@S
PP
IN
Without
NP
NN
deviation
NP
progress
VP
@VP
VBZ
is
RB
not
NP
JJ
possible
S-5
@S-2
PP-3
IN-10
Without
NP-12
NN-13
deviation
NP-15
progress
VP-2
@VP-1
VBZ-3
is
RB-5
not
NP-6
JJ-2
possible
Figure 1: Sample parse trees produced by the Berkeley Parser. An example of what the tree might look like with split
symbol annotations is shown on the right.
TSG rules as binary features for supervised classi-
fication such that the feature for a TSG rule is trig-
gered on a document if that rule appears in the parse
of some derivation of any of its sentences. This de-
scription purposefully treats the parsing of text as a
black box whose input is plain text and whose out-
put is any valid tree structure. Our work considers
five alternatives for this black box, and evaluates the
effect of this choice on the NLI Shared Task at the
BEA Workshop of NAACL 2013 (Tetreault et al,
2013).
2 Syntactic Representations
We investigate five variations on the output of the
parsing process. All five are easily produced by
freely available Java software; two with the Berkeley
Parser, two with the Stanford Parser, and one with a
combination of both software packages.
2.1 Berkeley Constituent Parses
Our first representation reproduces previous work by
using the output of the Berkeley Parser (Petrov et
al., 2006), one of highest performing systems on the
benchmark Penn Treebank task. The basic motivat-
ing principle involved is that the traditional nonter-
minal symbols used in Penn Treebank parsing are
too coarse to satisfy the context free assumption of
a CFG. To combat this, hierarchical latent annota-
tions are induced that split a symbol into several
subtypes, and a larger CFG is estimated on this set
of split nonterminals. A sentence is parsed using
this large CFG and each resulting symbol is mapped
back to its original unsplit supertype to produce the
final parse.
One important subtlety of the Berkeley Parser is
its default binarization, which we leave intact in our
downstream use of its parses. While binarization is
normally motivated by the desired cubic complexity
of parsing algorithms, it also benefits syntactic sty-
lometry. Consider the nugget of wisdom from the
great Frank Zappa shown on the left in Figure 1, in
which artificially introduced binarization nodes are
marked with the @ symbol.
The use of binarization allows us to capture pat-
terns such as verb phrases that begin with ?is not?
independent of the following child constituents. The
capabilities of TSG rules makes the use of binariza-
tion even more apt, as we can easily choose to re-
cover the unbinarized pattern with a slightly larger
fragment. This choice will be made in TSG induc-
tion based on the frequency with which the combi-
nation occurs, which intuitively aligns with our goal
of choosing representative features.
The second form that we investigate is identical
to the normal Berkeley Parser output, but with the
split annotations used in parsing left intact, as shown
the right of Figure 1. This parsed sentence shows
how each nonterminal is annotated with a split cat-
egory, and illustrates the potential advantages that
this method affords. For example, consider the @VP
node in the left-hand tree, whose subtree is gen-
erated with a CFG by first choosing to produce a
VBZ and RB, and then by lexicalizing each inde-
pendently. These two lexicalizations are not in fact
independent, as can be seen by the combination of
?is? with the RB ?may?, which is impossible al-
147
though each are independently quite likely. Splitting
the symbols as shown on the right allows us to cre-
ate a special RB node that is most likely to produce
?not? and VBZ node likely to produce ?is?. Their
likely co-occurrence can then be modeled as shown
by a rule with both specialized tags as children.
It is worth noting that this particular ability of
split symbol grammars to coordinate lexical items
is easily captured with the TSG rules that we induce
on these parses, regardless of the presence of split
symbols. The more orthogonal quality of these split
grammars is their ability to categorize symbols that
appear in similar syntactic situations. Consider that
some adjectives are more likely to appear in ?X is Y?
sentences in the ?Y? position, while some are more
likely to be used directly to the left of nouns. A split
symbol grammar handily captures this trait with a
split POS tag, while a TSG cannot associate patterns
containing different lexical items on its own.
2.2 Stanford Dependency Parses
The third and fourth syntactic models we employ
are derived from dependency parses produced by the
Stanford parser(Marneffe et al, 2006). In its stan-
dard form, a dependency parse is a directed tree in
which each word except the special ROOT node has
exactly one incoming edge and zero to many outgo-
ing edges, where edges represent syntactic depen-
dence. Arcs are labeled with the type of syntactic
dependence that they indicate. Following conven-
tion, we represent each word in combination with its
part of speech tag, as shown in the following exam-
ple dependency parse.
ROOT DT NN VBZ PRP
The poodle chews it
root
det nsubj dobj
In order to apply the techniques of TSG induction
to dependency parsed data, we implement a conver-
sion from dependency tree to constituent form. The
mechanics of this conversion are simple and illus-
trated in full by the following conversion of the de-
pendency tree shown above, and are similar to trans-
forms used in previous work in unsupervised depen-
dency parsing(Carroll and Charniak, 1992).
ROOT
VBZ-L
nsubj
NN-L
det
DT
the
NN
poodle
VBZ
chews
VBZ-R
dobj
PRP
it
Note that it is always the case that the arc labels
from the dependency parses are always produced
by unary rules. This allows the simple removal of
the nodes corresponding to arc labels, yielding our
fourth syntactic model.
ROOT
VBZ-L
NN-L
DT
the
NN
poodle
VBZ
chews
VBZ-R
PRP
it
Those familiar with the Stanford Parser may be
concerned that the dependency parses used here are
determined by a deterministic transform of a con-
stituent parse of Penn Treebank style, and then sim-
ply transformed back into constituent form. This is
especially concerning when considering the second
form in which arc labels have been removed; this
form can be constructed directly from the Berkeley
Parse form used above, and contains no additional
information. Our motivation in the investigation of
dependency parses is not that they offer new infor-
mation, but that they are organized differently than
constituent parses. When inducing a TSG, our abil-
ity to find a useful connections is impeded by phys-
ical distance between structures. In particular, in a
dependency parse, the head of the subject and the
verb are always contained in some TSG fragment
148
made up of small number of CFG rules, five or four
depending on the presence of arc labels. In con-
stituent parses, the presence of modifying phrases
can arbitrarily increase this distance.
2.3 Stanford Heuristic Annotations
Our final variation uses the annotations internal to
the Stanford Penn Treebank parser, as presented in
Klein and Manning (2003). These annotations are
motivated in the same way as Berkeley Parser split
states, but are deterministically applied to parse trees
using linguistic motivations. Besides handling ex-
plicit tracking of binarization and parent annotation,
several additional annotations are applied, such as
the splitting of certain POS tags into useful cate-
gories and annotation of some nodes with their num-
ber of children or siblings.
For ease of implementation, we do not use the
Stanford Parser itself to produce our trees, instead
we used our results from the Berkeley Parser. The
Stanford Parser annotations were then applied to
these trees after binarization symbols were first col-
lapsed. The following tree is an example of the
actual annotations applied by this process, and in-
cludes a fair subset of the many annotation types
that are used. The original symbol in each case is
the leftmost string of capital letters in the resulting
symbol strings shown.
ROOT
S-v
NP-B
NNP?NP
Ace
VP-VBF-v
VBZ?VB-BE
is
PP
IN?PP
in
NP-B
DT?NP
the
NN?NP
house
3 Experiments
We contrast the syntactic formalisms on the NLI
shared task experimental setup for the NAACL 2013
BEA workshop. This new data set (Blanchard et al,
2013) consists of TOEFL essays drawn from speak-
ers of 11 different L1 backgrounds. 9900 Essays
were supplied as a training set, with an additional
1100 development set essays and 1100 test essays.
Previous work in NLI has relied heavily on the
International Corpus of Learner English, but due to
significant topic biases along L1 lines in this data
set the explicit use of word tokens was frequently
limited to a predetermined set of stopwords. With
this in mind, the data set for the shared task was bal-
anced across TOEFL essay prompts and proficiency
levels. The result was that the participants in this
task were not forced to limit the word tokens explic-
itly employed, with the hopes that mitigating factors
had been minimized.
We prepared the data in the five forms described
above and induced TSGs on each version of the
parsed training set with the blocked sampling algo-
rithm of Cohn and Blunsom (2010). The resulting
rules were used as binary feature functions over doc-
uments indicating the presence of the rule in some
derivation of sentence in that document. We used
the Mallet implementation of a log-linear (MaxEnt)
classifier with a zero mean Gaussian prior with vari-
ance .1 on the classifier?s weights. Our results on the
development set are shown in Figure 3.
While a range of performance is achieved, when
we construct a classifier that simply averages the
predictive distributions of all five methods we get
better accuracy than any model on its own. We ob-
served further evidence of the orthogonality of these
methods by looking at pairs of formalisms and ob-
serving how many development set items were pre-
dicted correctly by one formalism and incorrectly by
another. This was routinely around 10 percent of the
development set in each direction for a given pair,
implying that gains of up to at least 20 percent classi-
fication accuracy are possible with an expert system
that approaches oracle selection of which formalism
to use.
As our submission to the shared task, we used the
Berkeley Parser output in isolation, the average of
the five classifiers, and the weighted average of the
classifiers using the optimal weights on the devel-
opment set. The former two models use the devel-
opment set as additional training data, which is one
possible explanation of the slightly higher perfor-
mance of the equally weighted average model. An-
149
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR P R F
ARA 76 2 4 1 2 2 2 1 4 3 3 76.8 76.0 76.4
CHI 2 86 0 1 1 0 4 4 1 0 1 81.1 86.0 83.5
FRE 2 1 77 3 2 6 2 1 5 1 0 82.8 77.0 79.8
GER 0 1 1 91 1 1 0 0 2 0 3 86.7 91.0 88.8
HIN 2 2 1 2 71 0 0 0 0 20 2 73.2 71.0 72.1
ITA 2 0 2 1 1 84 0 1 7 0 2 79.2 84.0 81.6
JPN 3 4 0 1 0 0 83 7 1 0 1 74.1 83.0 78.3
KOR 1 6 1 1 1 0 20 65 2 1 2 69.1 65.0 67.0
SPA 4 2 4 3 2 12 0 3 66 0 4 71.7 66.0 68.8
TEL 1 2 0 0 16 0 0 0 0 81 0 76.4 81.0 78.6
TUR 6 0 3 1 0 1 1 12 4 0 72 80.0 72.0 75.8
Figure 2: Confusion Matrix and per class results on the final test set evaluation using the evenly averaged model.
other explanation of note is that while the weight
optimization was carried out with EM over the like-
lihood of the development set labels, this did not
in correlate positively with classification accuracy;
even as we optimized on the development set the ac-
curacy in absolute classification of these items de-
creased slightly.
The confusion matrix for the evenly averaged
model, our best performing system, is shown in Fig-
ure 2. The most frequently confused L1 pairs were
Hindi and Telegu, Japanese and Korean, and Span-
ish and Italian. The similarity between Hindi and
Telegu is particularly troubling, as they come from
two completely different language families and their
most obvious similarity is that they are both spoken
primarily in India. This suggests that even though
the TOEFL corpus has been balanced by topic that
there is a strong geographical signal that is corre-
lated with but not caused by native language.
BP BPS DP DPA KM AVG
Acc 74.5 69.3 72.4 73.5 73.5 77.3
Figure 3: The resulting classification accuracies on the
development set for the various syntactic forms that we
considered. The forms used are plain Berkeley Parses
(BP), Berkeley Parses with split symbols (BPS), depen-
dency parses (DP), dependency parses without arc la-
bels (DPA), and the heuristic annotations from (Klein and
Manning, 2003) (KM). When the predictive distributions
of the five models are averaged (AVG), a higher accuracy
is achieved.
BP AVG AVG-EM
Acc 74.7 77.5 77.0
Figure 4: The classification accuracies obtained on the
test data using the Berkeley parser output alone (BP), the
arithmetic mean of all five predictive distributions (AVG)
and the weighted mean using the optimal weights from
the development set as determined with EM (AVG-EM)
4 Conclusion
In this work we open investigation of a generally un-
considered variable in syntactic stylometry: the ac-
tual syntactic formalism. We examine five poten-
tial candidates of which only one has been previ-
ously presented in the context of TSG features for
NLI. These five formalisms cover both constituent
and dependency grammars, and explore the possi-
bility of split state annotations for constituent gram-
mars and the inclusion of arc labels for dependency
grammars. We find that the use of different grammar
formalisms captures orthogonal information about
an author?s native language. Furthermore, the com-
bination of different formalisms can be used to in-
crease classification accuracy.
While our results are intriguing, they primarily
serve as a proof of concept that syntactic stylome-
try can benefit from a range of representations and
should not be taken as an exhaustive search for the
best representations to use. Other syntactic forms
exist, and even in our methods there are additional
variables that can be adjusted.
One such variable is the number of splits used in
150
the Berkeley Parser when split states are included;
the default number that we use in this work is 6,
the optimal value for the parsing task, but this may
be suboptimal as a representation for feature extrac-
tion. Binarization is another easily adjusted variable,
with several available options in the literature. For
example, binarization can be done that is aware of
head attachment. Another option is to binarize more
heavily, increasing the ability of TSG fragments to
separate sister nodes and find frequent patterns.
Alternative syntactic forms not explored in this
work are also available. These include well stud-
ied grammars such as Hierarchical Phrase Structure
Grammars and Combinatory Categorial Grammars,
and transforms that rearrange the tree such as the
Left Corner Transform used in Roark and Johnson
(1999). Furthermore, the use of the TSG as a fea-
ture extractor itself has the potential for extension
to more powerful systems such as Tree Adjoining
Grammars or Tree Insertion Grammars.
References
Shane Bergsma, Matt Post, and David Yarowsky. 2012.
Stylometric Analysis of Scientific Articles. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 327?
337, Montre?al, Canada, June. Association for Compu-
tational Linguistics.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. Toefl11: A cor-
pus of non-native english. Technical report, Educa-
tional Testing Service.
Glenn Carroll and Eugene Charniak. 1992. Two experi-
ments on learning probabilistic dependency grammars
from corpora. Technical Report CS-92-16, Brown
University, Providence, RI, USA.
Trevor Cohn and Phil Blunsom. 2010. Blocked inference
in bayesian tree substitution grammars. In ACL (Short
Papers), pages 225?230.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL, pages 423?430.
Marie Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In In
Proc. Intl Conf. on Language Resources and Evalua-
tion (LREC, pages 449?454.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433?440, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Brian Roark and Mark Johnson. 1999. Efficient proba-
bilistic top-down and left-corner parsing. In ACL.
Benjamin Swanson and Eugene Charniak. 2012. Na-
tive Language Detection with Tree Substitution Gram-
mars. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 193?197, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A report on the first native language identification
shared task. In Proceedings of the Eighth Workshop
on Innovative Use of NLP for Building Educational
Applications, Atlanta, GA, USA, June. Association for
Computational Linguistics.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
Parse Structures for Native Language Identification.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1600?1610, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
151
