Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 21?28,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Spanish DAL: A Spanish Dictionary of Affect in Language
Mat??as G. Dell? Amerlina R??os and Agust??n Gravano
Departamento de Computacio?n, FCEyN
Universidad de Buenos Aires, Argentina
{mamerlin,gravano}@dc.uba.ar
Abstract
The topic of sentiment analysis in text has
been extensively studied in English for the
past 30 years. An early, influential work by
Cynthia Whissell, the Dictionary of Affect in
Language (DAL), allows rating words along
three dimensions: pleasantness, activation and
imagery. Given the lack of such tools in Span-
ish, we decided to replicate Whissell?s work in
that language. This paper describes the Span-
ish DAL, a knowledge base formed by more
than 2500 words manually rated by humans
along the same three dimensions. We evalu-
ated its usefulness on two sentiment analysis
tasks, which showed that the knowledge base
managed to capture relevant information re-
garding the three affective dimensions.
1 Introduction
In an attempt to quantify emotional meaning in writ-
ten language, Whissell developed the Dictionary of
Affect in Language (DAL), a tool for rating words
and texts in English along three dimensions ? pleas-
antness, activation and imagery (Whissell et al,
1986; Whissell, 1989, inter alia). DAL works by
looking up individual words in a knowledge base
containing 8742 words. All words in this lexicon
were originally rated by 200 na??ve volunteers along
the same three dimensions.
Whissell?s DAL has subsequently been used in di-
verse research fields, for example as a keystone for
sentiment analysis in written text (Yi et al, 2003,
e.g.) and emotion recognition in spoken language
(Cowie et al, 2001). DAL has also been used to aid
the selection of emotionally balanced word stimuli
for Neuroscience and Psycholinguistics experiments
(Gray et al, 2002). Given the widespread impact of
DAL for the English language, it would be desirable
to create similar lexicons for other languages.
In recent years, there have been efforts to build
cross-lingual resources, such as using sentiment
analysis tools in English to score Spanish texts af-
ter performing machine translation (Brooke et al,
2009) or to automatically derive sentiment lexicons
in Spanish (Pe?rez-Rosas et al, 2012). The purpose
of the present work is to create a manually anno-
tated lexicon for the Spanish language, replicating
Whissell?s DAL, aiming at alleviating the scarcity
of resources for the Spanish language, and at deter-
mining if the lexicon-based approach would work
in Spanish as well as it does in English. We leave
for future work the comparison of the different ap-
proaches mentioned here. This paper describes the
three steps performed to accomplish that goal: i)
creating a knowledge base which is likely to have
a good word coverage on arbitrary texts from any
topic and genre (Section 2); ii) having a number of
volunteers annotate each word for the three affective
dimensions under study (Section 3); and iii) evaluat-
ing the usefulness of our knowledge base on simple
tasks (Section 4).
2 Word selection
The first step in building a Spanish DAL consists in
selecting a list of content words that is representa-
tive of the Spanish language, in the sense that it will
have a good coverage of the words in arbitrary input
texts from potentially any topic or genre. To accom-
plish this we decided to use texts downloaded from
Wikipedia in Spanish1 and from an online collection
of short stories called Los Cuentos.2 Articles from
Wikipedia cover a wide range of topics and are gen-
1http://es.wikipedia.org
2http://www.loscuentos.net
21
erally written in encyclopedia style. We downloaded
the complete set of articles in March, 2012, consist-
ing of 834,460 articles in total. Short stories from
Los Cuentos were written by hundreds of different
authors, both popular and amateur, on various gen-
res, including tales, essays and poems. We down-
loaded the complete collection from Los Cuentos in
April, 2012, consisting of 216,060 short stories.
2.1 Filtering and lemmatizing words
We extracted all words from these texts, sorted them
by frequency, and filtered out several word classes
that we considered convey no affect by themselves
(and thus it would be unnecessary to have them rated
by the volunteers). Prepositions, determinants, pos-
sessives, interjections, conjunctions, numbers, dates
and hours were tagged and removed automatically
using the morphological analysis function included
in the Freeling toolkit (Padro? et al, 2010).3 We
also excluded the following adverb subclasses for
the same reason: place, time, mode, doubt (e.g.,
quiza?s, maybe), negation, affirmation and amount.
Nouns and verbs were lemmatized using Freel-
ing as well, except for augmentative and diminu-
tive terminations, which were left intact due to their
potential effect on a word?s meaning and/or affect
(e.g., burrito is either a small donkey, burro, or a
type of Mexican food). Additionally, proper nouns
were excluded. Names of cities, regions, countries
and nationalities were marked and removed using
GeoWorldMap,4 a freely-available list of location
names from around the world. Names of people
were also filtered out. Proper names were manu-
ally inspected to avoid removing those with a lexical
meaning, a common phenomenon in Spanish (e.g.,
Victoria). Other manually removed words include
words in foreign languages (mainly in English), ro-
man numbers (e.g., XIX) and numbers in textual
form, such as seis (six), sexto (sixth), etc. Words
with one or two characters were removed automat-
ically, since we noticed that they practically always
corresponded to noise in the downloaded texts.
2.2 Counting ?word, word-class? pairs
We implemented a small refinement over Whissell?s
work, which consisted in considering ?word, word-
3http://nlp.lsi.upc.edu/freeling/
4http://www.geobytes.com/FreeServices.htm
class? pairs, rather than single words, since in Span-
ish the same lexical form may have different senses.
Thus, to each word (in its lemmatized form) we at-
tached one of four possible word classes ? noun,
verb, adjective or adverb. For example, bajoprep (un-
der) or bajonoun (bass guitar).
For each input word w, Freeling?s morphological
analysis returns a sequence of tuples ?lemma, POS-
tag, probability?, which correspond to the possible
lemmas and part-of-speech tags for w, together with
their prior probability. For example, the analysis
for the word bajo returns four tuples: ?bajo, SPS00
(i.e, preposition), 0.879?, ?bajo, AQ0MS0 (adjec-
tive), 0.077?, ?bajo, NCMS000 (noun), 0.040?,
and ?bajar, VMIP1S0 (verb), 0.004?. This means
that bajo, considered without context, has 87.9%
chances of being a noun, or 0.04% of being a verb.
Using this information, we computed the counts
of all ?word, word-class? pairs, taking into account
their prior probabilities. For example, assuming the
word bajo appeared 1000 times in the texts, it would
contribute with 1000?0.879 = 879 to the frequency
of bajoprep (i.e., bajo as a preposition), 77 to bajoadj,
40 to bajonoun, and 4 to bajarverb.
2.3 Merging Wikipedia and Los Cuentos
This process yielded 163,071 ?word, word-class?
pairs from the Wikipedia texts, and 30,544 from Los
Cuentos. To improve readability, hereafter we will
refer to ?word, word-class? pairs simply as words.
Figure 1 shows the frequency of each word count
in our two corpora. We note that both graphics are
practically identical, with a majority of low-count
words and a long tail with few high-count words.
To create our final word list to be rated by vol-
unteers, we needed to merge our two corpora from
Wikipedia and Los Cuentos. To accomplish this, we
Figure 1: Frequency of word counts in texts taken from
Wikipedia and Los Cuentos.
22
normalized all word counts for corpus size (normal-
ized count(w) = count(w) / corpus size), combined
both lists and sorted the resulting list by the normal-
ized word count (for the words that appeared in both
lists, we used its average count instead). The result-
ing list contained 175,413 words in total.
The top 10 words from Wikipedia were ma?sadv,
an?onoun, ciudadnoun, poblacio?nnoun, estadonoun, nom-
brenoun, veznoun, municipionoun, gruponoun and his-
torianoun (more, year, city, population, state, name,
time, as in ?first time?, municipality, group and his-
tory, respectively). The 10 words most common
from Los Cuentos were ma?sadv, veznoun, vidanoun,
d??anoun, tanadv, tiemponoun, ojonoun, manonoun,
amornoun and nochenoun (more, time, life, day, so,
time, eye, hand, love and night).
2.4 Assessing word coverage
Next we studied the coverage of the top k words
from our list on texts from a third corpus formed
by 3603 news stories downloaded from Wikinews in
Spanish in April, 2012.5 We chose news stories for
this task because we wanted a different genre for
studying the evolution of coverage.
Formally, let L be a word list, T any text, and
W (T ) the set of words occurring at least once in T .
We define the coverage of L on T as the percentage
of words in W (T ) that appear in L. Figure 2 shows
the evolution of the mean coverage on Wikinews ar-
ticles of the top k words from our word list. In
this figure we can observe that the mean coverage
grows rapidly, until it reaches a plateau at around
Figure 2: Mean coverage of the top k words from our list
on Wikinews articles.
5http://es.wikinews.org
80%. This suggests that even a low number of words
may achieve a relatively high coverage on new texts.
The 20% that remains uncovered, independently of
the size of the word list, may be explained by the
function words and proper names that were removed
from our word list. Note that news articles normally
contain many proper names, days, places and other
words that we intentionally discarded.
3 Word rating
After selecting the words, the next step consisted in
having them rated by a group of volunteers. For this
purpose we created a web interface, so that volun-
teers could complete this task remotely.
3.1 Web interface
On the first page of the web interface, volunteers
were asked to enter their month and year of birth,
their education level and their native language, and
was asked to complete a reCAPTCHA6 to avoid
bots. Subsequently, volunteers were taken to a page
with instructions for the rating task. They were
asked to rate each word along the three dimensions
shown in Table 1. These are the same three dimen-
Pleasantness Activation Imagery
1 Desagradable Pasivo Dif??cil de imaginar
(Unpleasant) (Passive) (Hard to imagine)
2 Ni agradable Ni activo Ni dif??cil ni fa?cil
ni desagradable ni pasivo de imaginar
(In between) (In between) (In between)
3 Agradable Activo Fa?cil de imaginar
(Pleasant) (Active) (Easy to imagine)
Table 1: Possible values for each of the three dimensions.
sions used in Whissell?s work. Importantly, these
concepts were not defined, to avoid biasing the judg-
ments. Volunteers were also encouraged to follow
their first impression, and told that there were no
?correct? answers. Appendix A shows the actual lo-
gin and instructions pages used in the study.
After reading the instructions, volunteers pro-
ceeded to judge two practice words, intended to help
them get used to the task and the interface, followed
by 20 target words. Words were presented one per
page. Figure 3 shows a screenshot of the page for
rating the word navegarverb. Note that the word class
6http://www.recaptcha.net
23
Figure 3: Screenshot of the web page for rating a word.
(verb in this example) is indicated right below the
word. After completing the first batch of 20 words,
volunteers were asked if they wanted to finish the
study or do a second batch, and then a third, a fourth,
and so on. This way, they were given the chance to
do as many words as they felt comfortable with. If
a volunteer left before completing a batch, his/her
ratings so far were also recorded.
3.2 Volunteers
662 volunteers participated in the study, with a mean
age of 33.3 (SD = 11.2). As to their level of educa-
tion, 76% had completed a university degree, 23%
had finished only secondary school, and 1% had
completed only primary school. Only volunteers
whose native language was Spanish were allowed
to participate in the study. Each volunteer was as-
signed 20 words following this procedure: (1) The
175,413 words in the corpus were sorted by word
count. (2) Words that had already received 5 or more
ratings were excluded. (3) Words that had already
been rated by a volunteer with the same month and
year of birth were excluded, to prevent the same vol-
unteer from rating twice the same word. (4) The top
20 words were selected.
Each volunteer rated 52.3 words on average (SD
= 34.0). Roughly 30% completed 20 words or
fewer; 24% completed 21-40 words; 18%, 41-60
words; and the remaining 28%, more than 60 words.
3.3 Descriptive statistics
A total of 2566 words were rated by at least 5 volun-
teers. Words with fewer annotations were excluded
from the study. We assigned each rating a numeric
value from 1 to 3, as shown in Table 1. Table 2
shows some basic statistics for each of the three di-
mensions.
Mean SD Skewness Kurtosis
Pleasantness 2.23 0.47 ?0.47 ?0.06
Activation 2.33 0.48 ?0.28 ?0.84
Imagery 2.55 0.42 ?0.90 0.18
Table 2: Descriptive statistics for the three dimensions.
The five most pleasant words, according to the
volunteers, were jugarverb, besonoun, sonrisanoun,
compan???anoun and reirverb (play, kiss, smile, com-
pany and laugh, respectively). The least pleas-
ant ones were asesinatonoun, caroadj, ahogarverb,
heridanoun and cigarronoun (murder, expensive,
drown, wound and cigar).
Among the most active words appear ideanoun,
publicarverb, violentoadj, sexualadj and talentonoun
(idea, publish, violent, sexual and talent). Among
the least active, we found yacerverb, espiritualadj,
quietoadj, esperarverb and cada?veradj (lay, spiritual,
still, wait and corpse).
The easiest to imagine include sucioadj, silen-
cionoun, darverb, peznoun and pensarverb (dirty, si-
lence, give, fish and think). Finally, the hardest
to imagine include consistirverb, constarverb, mor-
folog??anoun, piedadnoun and tendencianoun (consist,
consist, morphology, compassion and tendency).
We conducted Pearson?s correlation tests between
the different dimensions. Table 3 shows the correla-
tion matrix. Correlations among rating dimensions
were very weak, which supports the assumption that
pleasantness, activation and imagery are three inde-
pendent affective dimensions. These numbers are
very similar to the ones reported in Whissell?s work.
Pleasantness Activation Imagery
Pleasantness 1.00 0.14 0.10
Activation 0.14 1.00 0.11
Imagery 0.10 0.11 1.00
Table 3: Correlation between the different dimensions
Next, we computed Cohen?s ? to measure the de-
gree of agreement above chance between volunteers
(Cohen, 1968).7 Given that we used a three-point
scale for rating each affective dimension, we used
7This measure of agreement above chance is interpreted as
follows: 0 = None, 0 - 0.2 = Small, 0.2 - 0.4 = Fair, 0.4 - 0.6 =
Moderate, 0.6 - 0.8 = Substantial, 0.8 - 1 = Almost perfect.
24
a weighted version of ?, thus taking into account
the distance on that scale between disagreements.
For example, the distance between pleasant and un-
pleasant was 2, and the distance between pleasant
and in-between was 1. We obtained a weighted ?
measure of 0.42 for pleasantness, 0.30 for activation,
and 0.14 for imagery. Considering that these were
highly subjective rating tasks, the agreement lev-
els for pleasantness and activation were quite high.
The imagery task seemed somewhat more difficult,
although we still observed some agreement above
chance. These results indicate that our knowledge
base managed to, at least partially, capture informa-
tion regarding the three affective dimensions.
4 Evaluation
Next we proceeded to evaluate the usefulness of our
knowledge base. For this purpose, we developed a
simple system for estimating affect along our three
affective dimensions, and evaluated it on two differ-
ent sentiment-analysis tasks. The first task consisted
in a set of texts labeled by humans, and served to
compare the judgments of human labelers with the
predictions of our system. The second task consisted
in classifying a set of user product reviews into ?pos-
itive? or ?negative? opinions, a common application
for online stores.
4.1 Simple system for estimating affect
We created a simple computer program for automat-
ically estimating the degree of pleasantness, acti-
vation and imagery of an input text, based on the
knowledge base described in the previous sections.
For each word in the knowledge base, we cal-
culated its mean rating for each dimension. Sub-
sequently, for an input text T we used Freeling to
generate a full syntactic parsing, from which we ex-
tracted all ?word, word-class? pairs in T . The system
calculates the value for affective dimension d using
the following procedure:
score? 0
count? 0
for each word w in T (counting repetitions):
if w is included in KB:
score? score+KBd(w)
count? count+ 1
return score/count
where KB is our knowledge base, and KBd(w) is
the value for w in KB for dimension d.
For example, given the sentence ?Mi amiga espe-
raba terminar las pruebas a tiempo? (?My female-
friend was hoping to finish the tests on time?), and
assuming our knowledge base contains the numbers
shown in Table 4, the three values are computed as
follows. First, all words are lemmatized (i.e., mi
amigo esperar terminar el prueba a tiempo). Sec-
ond, the mean of each dimension is calculated with
the described procedure, yielding a pleasantness of
2.17, activation of 2.27 and imagery of 2.53.
word word-class mean P mean A mean I
amigo noun 3.0 2.4 3
esperar verb 1.2 1 2.8
poder verb 2.8 2.8 2.2
terminar verb 2.2 3 2.8
prueba noun 1.8 2.4 2.2
tiempo noun 2 2 2.2
mean: 2.17 2.27 2.53
Table 4: Knowledge base for the example text (P = pleas-
antness; A = activation; I = imagery).
It is important to mention that this system is just a
proof of concept, motivated by the need to evaluate
the effectiveness of our knowledge base. It could be
used as a baseline system against which to compare
more complex affect estimation systems. Also, if
results are good enough with such a simple system,
this would indicate that the information contained
in the knowledge base is useful, and in the future it
could help create more complex systems.
4.2 Evaluation #1: Emotion estimation
The first evaluation task consisted in comparing pre-
dictions made by our simple system against rat-
ings assigned by humans (our gold standard), on a
number of sentences and paragraphs extracted from
Wikipedia and Los Cuentos.
4.2.1 Gold standard
From each corpus we randomly selected 15 sen-
tences with 10 or more words, and 5 paragraphs with
at least 50 words and two sentences ? i.e. 30 sen-
tences and 10 paragraphs in total. These texts were
subsequently rated by 5 volunteers (2 male, 3 fe-
male), who were instructed to rate each entire text
(sentence or paragraph) for pleasantness, activation
25
and imagery using the same three-point scale shown
in Table 1. The weighted ?measure for these ratings
was 0.17 for pleasantness, 0.17 for activation and
0.22 for imagery. Consistent with the subjectivity
of these tasks, the degree of inter-labeler agreement
was rather low, yet still above chance level. Note
also that for pleasantness and activation the agree-
ment level was lower for texts than for individual
words, while the opposite was true for imagery.
4.2.2 Results
To evaluate the performance of our system, we
conducted Pearson?s correlation test for each affec-
tive dimension, in order to find the degree of cor-
relation between the system?s predictions for the 40
texts and their corresponding mean human ratings.
Table 5 shows the resulting ? coefficients.
System \ GS Pleasantness Activation Imagery
Pleasantness 0.59 * 0.15 * ?0.18 *
Activation 0.13 * 0.40 * 0.14 *
Imagery 0.16 0.19 0.07
Table 5: Correlations between gold standard and system?s
predictions. Statistically significant results are marked
with ?*? (t-tests, p < 0.05).
The coefficient for pleasantness presented a high
value at 0.59, which indicates that the system?s esti-
mation of pleasantness was rather similar to the rat-
ings given by humans. For activation the correlation
was weaker, although still significant. On the other
hand, for imagery this simple system did not seem
able to successfully emulate human judgments.
These results suggest that, at least for pleasant-
ness and activation, our knowledge base success-
fully captured useful information regarding how hu-
mans perceive those affective dimensions. For im-
agery, it is not clear whether the information base
did not capture useful information, or the estimation
system was too simplistic.
4.2.3 Effect of word count on performance
Next we studied the evolution of performance as
a function of the knowledge base size, aiming at as-
sessing the potential impact of increasing the num-
ber of words annotated by humans. Figure 4 sum-
marizes the results of a simulation, in which succes-
sive systems were built and evaluated using the top
250, 350, 450, ..., 2350, 2450 and 2566 words in our
knowledge base.
The green line (triangles) represents the mean
coverage of the system?s knowledge base on the gold
standard texts; the corresponding scale is shown on
the right axis. Similarly to Figure 2, the coverage
grew rapidly, starting at 18% when using 250 words
to 44% when using all 2566 words.
The blue (circles), red (squares) and purple (di-
amonds) lines correspond to the correlations of the
system?s predictions and the gold standard ratings
for pleasantness, activation and imagery, respec-
tively; the corresponding scale is shown on the left
axis. The black lines are a logarithmic function fit to
each of the three curves (?2 = 0.90, 0.72 and 0.68,
respectively).
Figure 4: Evolution of the correlation between system
predictions and Gold Standard, with respect to the knowl-
edge base size.
These results indicate that the system perfor-
mance (measured as the correlation with human
judgments) grew logarithmically with the number of
words in the knowledge base. Interestingly, the per-
formance grew at a slower pace than word cover-
age. In other words, an increase in the proportion
of words in a text that were known by the system
did not lead to a similar increase in the accuracy of
the predictions. An explanation may be that, once
an emotion had been established based on a percent-
age of words in the text, the addition of a few extra
words did not significantly change the outcome.
In consequence, if we wanted to do a substantial
improvement to our baseline system, it would prob-
ably not be a good idea to simply annotate more
26
words. Instead, it may be more effective to work
on how the system uses the information contained in
the knowledge base.
4.3 Evaluation #2: Classification of reviews
The second evaluation task consisted in using our
baseline system for classifying user product reviews
into positive or negative opinions.
4.3.1 Corpus
For this task we used a corpus of 400 user reviews
of products such as cars, hotels, dishwashers, books,
cellphones, music, computers and movies, extracted
from the Spanish website Ciao.es.8 This is the same
corpus used by Brooke (2009), who employed senti-
ment analysis tools in English to score Spanish texts
after performing machine translation.
On Ciao.es, users may enter their written reviews
and associate a numeric score to them, ranging from
1 to 5 stars. For this evaluation task, we made the
assumption that there was a strong relation between
the written reviews and their corresponding numeric
scores. Following this assumption, we tagged re-
views with 1 or 2 stars as ?negative? opinions, and
reviews with 4 or 5 stars as ?positive?. Reviews with
3 stars were considered neutral, and ignored.
4.3.2 Results
We used our system in a very simple way for pre-
dicting the polarity of opinions. First we computed
M , the mean pleasantness score on 80% of the re-
views. Subsequently, for each review in the remain-
ing 20%, if its pleasantness score was greater than
M , then it was classified as ?positive?; otherwise, it
was classified as ?negative?.
After repeating this procedure five times using
5-fold cross validation, the overall accuracy was
62.33%. Figure 5 shows the evolution of the sys-
tem?s accuracy with respect to the number of words
in the knowledge base. The green line (triangles)
represents the mean coverage of the system?s knowl-
edge base on user review texts; the corresponding
scale is shown on the right axis. The blue line (cir-
cles) corresponds to the classification accuracy; the
corresponding scale is shown on the left axis. The
black line is a logarithmic function fit to this curve
(?2 = 0.80).
8http://ciao.es
Figure 5: Evolution of the classification accuracy with
respect to the size of the knowledge base.
Notably, with as few as 500 words the accuracy
is already significantly above chance level, which is
50% for this task. This indicates that our knowl-
edge base managed to capture information on pleas-
antness that may aid the automatic classification of
positive and negative user reviews.
Also, similarly to our first evaluation task, we
observe that the accuracy increased as more words
were added to the knowledge base. However, it did
so at a logarithmic pace slower than the growth of
the word coverage on the user reviews. This sug-
gests that adding more words labeled by humans to
the knowledge base would only have a limited im-
pact on the performance of this simple system.
5 Conclusion
In this work we presented a knowledge base of Span-
ish words labeled by human volunteers for three
affective dimensions ? pleasantness, activation and
imagery, inspired by the English DAL created by
Whissell (1986; 1989). The annotations of these
three dimensions were weakly intercorrelated, indi-
cating a high level of independence of each other.
Additionally, the agreement between volunteers was
quite high, especially for pleasantness and activa-
tion, given the subjectivity of the labeling task.
To evaluate the usefulness of our lexicon, we built
a simple emotion prediction system. When used for
predicting the same three dimensions on new texts,
its output significantly correlated with human judg-
ments for pleasantness and activation, but the results
27
for imagery were not satisfactory. Also, when used
for classifying the opinion polarity of user product
reviews, the system managed to achieve an accuracy
better than random. These results suggest that our
knowledge base successfully captured useful infor-
mation of human perception of, at least, pleasant-
ness and activation. For imagery, either it failed to
capture any significant information, or the system
we created was too simple to exploit it accordingly.
Regarding the evolution of the system?s perfor-
mance as a function of the size of the lexicon, the
results were clear. When more words were included,
the system performance increased only at a loga-
rithmic pace. Thus, working on more complex sys-
tems seems to be more promising than adding more
human-annotated words.
In summary, this work presented a knowledge
base that may come handy to researchers and de-
velopers of sentiment analysis tools in Spanish. Ad-
ditionally, it may be useful for disciplines that need
to select emotionally balanced word stimuli, such as
Neuroscience or Psycholinguistics. In future work
we will compare the usefulness of our manually
annotated lexicon and cross-linguistic approaches
(Brooke et al, 2009; Pe?rez-Rosas et al, 2012).
Acknowledgments
This work was funded in part by ANPCYT PICT-2009-
0026 and CONICET. The authors thank Carlos ?Greg?
Diuk and Esteban Mocskos for valuable suggestions and
comments, and Julian Brooke, Milan Tofiloski and Maite
Taboada for kindly sharing the Ciao corpus.
References
J. Brooke, M. Tofiloski, and M. Taboada. 2009. Cross-
linguistic sentiment analysis: From English to Span-
ish. In International Conference on Recent Advances
in NLP, Borovets, Bulgaria, pages 50?54.
J. Cohen. 1968. Weighted kappa: Nominal scale
agreement provision for scaled disagreement or partial
credit. Psychological bulletin, 70(4):213.
R. Cowie, E. Douglas-Cowie, N. Tsapatsoulis, G. Votsis,
S. Kollias, W. Fellenz, and J.G. Taylor. 2001. Emo-
tion recognition in human-computer interaction. Sig-
nal Processing Magazine, IEEE, 18(1):32?80.
J.R. Gray, T.S. Braver, and M.E. Raichle. 2002. Integra-
tion of emotion and cognition in the lateral prefrontal
cortex. Proceedings of the National Academy of Sci-
ences, 99(6):4115.
L. Padro?, M. Collado, S. Reese, M. Lloberes, and
I. Castello?n. 2010. Freeling 2.1: Five years of open-
source language processing tools. In International
Conf. on Language Resources and Evaluation (LREC).
V. Pe?rez-Rosas, C. Banea, and R. Mihalcea. 2012.
Learning sentiment lexicons in spanish. In Int. Conf.
on Language Resources and Evaluation (LREC).
C. Whissell, M. Fournier, R. Pelland, D. Weir, and
K. Makarec. 1986. A dictionary of affect in language:
Iv. reliability, validity, and applications. Perceptual
and Motor Skills, 62(3):875?888.
Cynthia Whissell. 1989. The dictionary of affect in lan-
guage. Emotion: Theory, research, and experience,
4:113?131.
J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. 2003.
Sentiment analyzer: Extracting sentiments about a
given topic using NLP techniques. In 3rd IEEE Int.
Conf. on Data Mining, pages 427?434. IEEE.
A Login and instructions pages
Figures 6 and 7 show the screenshots of the login and
instructions pages of our web interface for rating words.
Figure 6: Screenshot of the login page.
Figure 7: Screenshot of the instructions page.
28
