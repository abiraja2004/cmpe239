Achilles: NiCT/ATR Chinese Morphological Analyzer for the Fourth Sighan
Bakeoff
Ruiqiang Zhang1,2 and Eiichiro Sumita1,2
1National Institute of Information and Communications Technology
2ATR Spoken Language Communication Research Laboratories
2-2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan
{ruiqiang.zhang,eiichiro.sumita}@atr.jp
Abstract
We created a new Chinese morpholog-
ical analyzer, Achilles, by integrating
rule-based, dictionary-based, and statis-
tical machine learning method, condi-
tional random fields (CRF). The rule-
based method is used to recognize regular
expressions: numbers, time and alphabets.
The dictionary-based method is used to
find in-vocabulary (IV) words while out-
of-vocabulary (OOV) words are detected
by the CRFs. At last, confidence measure
based approach is used to weigh all the re-
sults and output the best ones. Achilles
was used and evaluated in the bakeoff.
We participated the closed tracks of word
segmentation and part-of-speech tagging
for all the provided corpus. In spite of
an unexpected file encoding errors, the
system exhibited a top level performance.
A higher word segmentation accuracy for
the corpus ckip and ncc were achieved.
We are ranked at the fifth and eighth po-
sition out of all 19 and 26 submissions
respectively for the two corpus. Achilles
uses a feature combined approach for part-
of-speech tagging. Our post-evaluation re-
sults prove the effectiveness of this ap-
proach for POS tagging.
1 Introduction
Many approaches have been proposed in Chinese
word segmentation in the past decades. Segmen-
tation performance has been improved significantly,
from the earliest maximal match (dictionary-based)
approaches to HMM-based (Zhang et al, 2003) ap-
proaches and recent state-of-the-art machine learn-
ing approaches such as maximum entropy (Max-
Ent) (Xue and Shen, 2003), support vector ma-
chine (SVM) (Kudo and Matsumoto, 2001), con-
ditional random fields (CRF) (Peng and McCallum,
2004), and minimum error rate training (Gao et al,
2004). After analyzing the results presented in the
first and second Bakeoffs, (Sproat and Emerson,
2003) and (Emerson, 2005), we created a new Chi-
nese word segmentation system named as ?Achilles?
that consists of four modules mainly: Regular ex-
pression extractor, dictionary-based Ngram segmen-
tation, CRF-based subword tagging (Zhang et al,
2006), and confidence-based segmentation. Of the
four modules, the subword-based tagging, differing
from the existing character-based tagging, was pro-
posed in our work recently. We will give a detail de-
scription to this approach in the following sections.
In the followings, we illustrate our word seg-
mentation process in Section 2, where the subword-
based tagging is implemented by the CRFs method.
Section 3 illustrates our feature-based part-of-
speech tagging approach. Section 4 presents our ex-
perimental results. Section 5 describes current state-
of-the-art methods for Chinese word segmentation.
Section 6 provides the concluding remarks.
2 Introduction of main modules in
Achilles
The process of Achilles is illustrated in Fig. 1, where
three modules of Achilles are shown: a dictionary-
178
Sixth SIGHAN Workshop on Chinese Language Processing
????????+XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\
input
????????+XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\
Dictionary-based word segmentation
?%?,?,?2?2??%?,+XDQJ%<LQJ,&KXQ,OLYHV2LQ2%HLMLQJ%FLW\,
Subword-based IOB tagging
?%?,?,?2?2??%?,+XDQJ%<LQJ,&KXQ,OLYHV2LQ2%HLMLQJ%FLW\,
Confidence-based segmentation
????????+XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\
output
Figure 1: Outline of word segmentation process
based N-gram word segmentation for segmenting IV
words, a subword-based tagging by the CRF for rec-
ognizing OOVs, and a confidence-dependent word
segmentation used for merging the results of both
the dictionary-based and the IOB tagging. An ex-
ample exhibiting each step?s results is also given in
the figure.
The rule-based regular expression is not shown in
the figure because this module interweaves with the
other modules. This module can be called if needed
at any time. The function of this module is to recog-
nize numerical, temporal expression and others like
product number, telephone number, credit number
or alphabets. For example, ?????(35,000)?,
???(August)?, ?0774731301?, ?George Bush?.
2.1 Dictionary-based N-gram word
segmentation
Dictionary-based N-gram word segmentation is an
important module for Achilles. This module can
achieve a very high R-iv, but no OOV detection.
We combined with it the N-gram language model
(LM) to solve segmentation ambiguities. For a given
Chinese character sequence, C = c0c1c2 . . . cN , the
problem of word segmentation can be formalized
as finding a word sequence, W = wt0wt1wt2 . . .wtM ,
which satisfies
wt0 = c0 . . . ct0 , wt1 = ct0+1 . . . ct1
wti = cti?1+1 . . . cti , wtM = ctM?1+1 . . . ctM
ti > ti?1, 0 ? ti ? N, 0 ? i ? M
such that
W = arg max
W
P(W |C) = arg max
W
P(W)P(C|W)
= arg max
W
P(wt0wt1 . . .wtM )?(c0 . . . ct0 ,wt0)
?(ct0+1 . . . ct1 ,wt1) . . . ?(ctM?1+1 . . . cM,wtM )
(1)
We applied Bayes? law in the above derivation.
Because the word sequence must keep consistent
with the character sequence, P(C|W) is expanded
to be a multiplication of a Kronecker delta function
series, ?(u, v), equal to 1 if both arguments are the
same and 0 otherwise.
Equation 1 indicates the process of dictionary-
based word segmentation. We looked up the lexicon
to find all the IVs, and evaluated the word sequences
with the LMs.
2.2 Subword-based IOB tagging using CRFs
If dictionary-based module recognizes IVs success-
fully, the subword-based IOB tagging can recog-
nize OOVs. Before the subword-based tagging,
the character-based ?IOB? tagging approach has
been widely used in Chinese word segmentation
recently (Xue and Shen, 2003; Peng and McCal-
lum, 2004; Tseng et al, 2005). Under the scheme,
each character of a word is labeled as ?B? if it is
the first character of a multiple-character word, or
?O? if the character functions as an independent
word, or ?I? otherwise.? For example, ??(whole)
???(Beijing city)? is labeled as ??(whole)/O
?(north)/B?(capital)/I?(city)/I?.
We proposed the subword-based tagging (Zhang
et al, 2006) to improve the existing character-based
tagging. The subword-based IOB tagging assigns
tags to a pre-defined lexicon subset consisting of
the most frequent multiple-character words in addi-
tion to single Chinese characters. If only Chinese
characters are used, the subword-based IOB tagging
is downgraded into a character-based one. Taking
the same example mentioned above, ??(whole)?
??(Beijing city)? is labeled as ??(whole)/O ?
?(Beijing)/B?(city)/I? in the subword-based tag-
ging, where ???(Beijing)/B? is labeled as one
unit.
We used the CRFs approach to train the IOB tag-
ger (Lafferty et al, 2001) on the training data. We
179
Sixth SIGHAN Workshop on Chinese Language Processing
downloaded and used the package ?CRF++? from
the site ?http://www.chasen.org/t?aku/software.? Ac-
cording to the CRFs, the probability of an IOB tag
sequence, T = t0t1 ? ? ? tM, given the word sequence,
W = w0w1 ? ? ?wM, is defined by
p(T |W) =
exp
????????
M?
i=1
????????
?
k
?k fk(ti?1, ti,W) +
?
k
?kgk(ti,W)
????????
???????? /Z,
Z =
?
T=t0t1???tM
p(T |W)
(2)
where we call fk(ti?1, ti,W) bigram feature functions
because the features trigger the previous observa-
tion ti?1 and current observation ti simultaneously;
gk(ti,W), the unigram feature functions because they
trigger only current observation ti. ?k and ?k are
the model parameters corresponding to feature func-
tions fk and gk respectively.
The model parameters were trained by maximiz-
ing the log-likelihood of the training data using L-
BFGS gradient descent optimization method. In
order to overcome overfitting, a gaussian prior was
imposed in the training.
The types of unigram features used in our experi-
ments included the following types:
w0,w?1,w1,w?2,w2,w0w?1,w0w1,w?1w1,
w?2w?1,w2w0
where w stands for word. The subscripts are po-
sition indicators. 0 means the current word; ?1,?2,
the first or second word to the left; 1, 2, the first or
second word to the right.
For the bigram features, we only used the previ-
ous and the current observations, t?1t0.
As to feature selection, we simply used absolute
counts for each feature in the training data. We de-
fined a cutoff value for each feature type and se-
lected the features with occurrence counts over the
cutoff.
A forward-backward algorithm was used in the
training and viterbi algorithm was used in the de-
coding.
2.3 Confidence-dependent word segmentation
Before moving to this step in Figure 1, we produced
two segmentation results: the one by the dictionary-
based approach and the one by the IOB tagging.
However, neither was perfect. The dictionary-based
segmentation produced results with higher R-ivs but
lower R-oovs while the IOB tagging yielded the con-
trary results. In this section we introduce a con-
fidence measure approach to combine the two re-
sults. We define a confidence measure, CM(tiob|w),
to measure the confidence of the results produced
by the IOB tagging by using the results from
the dictionary-based segmentation. The confidence
measure comes from two sources: IOB tagging and
dictionary-based word segmentation. Its calculation
is defined as:
CM(tiob|w) = ?CMiob(tiob|w) + (1 ? ?)?(tw, tiob)ng
(3)
where tiob is the word w?s IOB tag assigned by the
IOB tagging; tw, a prior IOB tag determined by the
results of the dictionary-based segmentation. After
the dictionary-based word segmentation, the words
are re-segmented into subwords by FMM before be-
ing fed to IOB tagging. Each subword is given a
prior IOB tag, tw. CMiob(t|w), a confidence probabil-
ity derived in the process of IOB tagging, is defined
as
CMiob(t|wi) =
?
T=t0t1???tM ,ti=t P(T |W,wi)?
T=t0t1???tM P(T |W)
where the numerator is a sum of all the observation
sequences with word wi labeled as t.
?(tw, tiob)ng denotes the contribution of the
dictionary-based segmentation. It is a Kronecker
delta function defined as
?(tw, tiob)ng = { 1 if tw = tiob0 otherwise
In Eq. 3, ? is a weighting between the IOB tag-
ging and the dictionary-based word segmentation.
We found the value 0.7 for ?, empirically.
By Eq. 3 the results of IOB tagging were re-
evaluated. A confidence measure threshold, t, was
defined for making a decision based on the value.
If the value was lower than t, the IOB tag was re-
jected and the dictionary-based segmentation was
used; otherwise, the IOB tagging segmentation was
used. A new OOV was thus created. For the two
extreme cases, t = 0 is the case of the IOB tagging
while t = 1 is that of the dictionary-based approach.
In a real application, a satisfactory tradeoff between
180
Sixth SIGHAN Workshop on Chinese Language Processing
R-ivs and R-oovs could find through tuning the con-
fidence threshold.
3 Part-of-speech Tagging
Our POS tagging is a traditional maximum entropy
tagging (A.Ratnaparkhi, 1996) as follows,
p(t|h) = 1Z(h)exp(
M?
i=1
?i fi(h, t)) (4)
where Z(h) is a normalizing factor determined by
requirement ?t p(t|h) = 1 over all t:
Z(h) =
?
t
exp(
M?
i=1
?i fi(h, t)) (5)
In the evaluation, 17 categories of triggers were
used, which include:
(w, t) , (w?2w?1w, t) , (w?1ww1, t) , (ww1w2, t) ,
(w?1w, t) , (ww1, t) , (t?1, t) , (t?2t?1, t) , (t?1w1, t),
(t?1ww1, t), (w?1w1, t) , (w?1, t) , (w1, t) , (t?1w, t),
(t?2t?1w, t) , (w?2w?1, t) , (w1w2, t)
where:
w is the word whose tag we are predicting; t is the
tag we are predicting; t?1 is the tag to the left of tag
t; t?2 is the tag to the left of tag t?1; w?1 is the word
to the left of word w; w?2 is the word to the left of
word w?1; w1 is the word to the right of word w; w2
is the word to the right of word w1 ;
In addition to the ME based POS tagging ap-
proach, we also combined a N-gram based POS tag-
ging.
N-gram tagger is the most widely used tagger in
part-of-speech tagging methods. The basic idea is
to maximize a posterior probability p(T |W) given a
word sequence in order to find its tag sequence. By
using Bayes rule, this can be transformed as to max-
imize p(T ) ? p(W |T ). Prior probability p(T ) is a N-
gram language model of tag sequence. p(W |T ) is
thought as an unigram model. In this experiment we
used trigram to model p(T ).
Differing from the interpolation smoothing al-
gorithm used in(Merialdo, 1994), both p(T ) and
p(W |T ) were smoothed by back-off methods(Katz,
1987). Because a N-gram backoff model P(T ) is
well-known, a backoff implementation of p(W |T )
was given here only. It is of the following equation.
R P F R-oov R-iv
CKIP 0.938 0.931 0.935 0.640 0.966
CITYU 0.943 0.933 0.938 0.686 0.965
CTB 0.941 0.943 0.942 0.663 0.961
NCC 0.931 0.933 0.932 0.592 0.950
SXU 0.932 0.929 0.930 0.487 0.971
Table 1: Post evaluation of word segmentation.
p(w|t) =
{ p?(w|t) if p?(w|t) , 0
?(t) p?(w) otherwise (6)
where:
- p?(w|t) and p?(w) are discounting relative fre-
quencies of p(w|t) and p(w), calculated by
back-off discounting algorithm. The discount
thresholds of p?(w|t) and p?(w) in present exper-
iment were 12 and 1 respectively. A new word
?UNK? was added to the vocabulary, whose
probability p?(w) represents that of all the un-
seen words.
- ?(t) is a normalizing value to ensure ?w p(w|t) =
1.
4 Experiments
We participated all the closed evaluation of word
segmentation and part-of-speech tagging. Our
scores should have achieved better than the official
numbers if we had submitted the results in the right
format. Achilles outputs results in GBK/BIG5 for-
mat. However, the format determined by bakeoff
organizers is Unicode-16. We made a lethal er-
ror when we converted the files from GBK/BIG5
to Unicode-16. Hence, the official results display
wrong scores for our system?s results.
We evaluated our results again in the post-
evaluation. The results for word segmentation is
shown in Table 1. The results for POS tagging is
shown in Table 2.
Table 1 and Table 2 represent the real perfor-
mance of Achilles in this evaluation. The official
data do not.
5 Discussion
Achilles achieved good word segmentation results
as shown in Table 1. Achilles was designed through
181
Sixth SIGHAN Workshop on Chinese Language Processing
Acc. R-oov R-iv
CKIP 0.913 0.530 0.946
CITYU 0.881 0.470 0.914
CTB 0.934 0.709 0.947
NCC 0.945 0.575 0.963
PKU 0.937 0.646 0.952
Table 2: Post evaluation of part-of-speech tagging.
three perspectives: IV recognition, OOV recogni-
tion and regular expression recognition. IV recogni-
tion can be solved at higher accuracy by dictionary-
based approach. OOV recognition can be solved
by IOB tagging. However, the flexible numerical
and temporal expression cannot be solved by the
above two methods. Hence, we used regular expres-
sion. Finally, the inconsistency of the above meth-
ods are resolved by confidence measure approach.
These features causes higher performance achieved
by Achilles.
6 Conclusions
This paper described systematically the main fea-
tures of our Chinese morphological analyzer,
Achilles. Because of its delicate design and state-of-
the-art technological integration, Achilles achieved
better or comparable segmentation results when it
was compared with the world best segmenter.
You can get Achilles from the site
?http://www.slc.atr.jp/?rzhang/Achilles.html?.
References
A.Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of the Empirical Meth-
ods in Natural Language Processing Conference.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing, Jeju, Korea.
Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,
Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004.
Adaptive chinese word segmentation. In ACL-2004,
Barcelona, July.
S. Katz. 1987. Estimation of probabilities for sparse data
for the language model component of a speech rec-
ognizer. IEEE Transactions on Acoustics Speech and
Signal Processing, 35:400?401.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machine. In Proc. of NAACL-2001,
pages 192?199.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML-2001, pages 591?598.
B. Merialdo. 1994. Tagging english text with a proba-
bilistic model. Computational Linguistics, 20(2):155?
172.
Fuchun Peng and Andrew McCallum. 2004. Chinese
segmentation and new word detection using condi-
tional random fields. In Proc. of Coling-2004, pages
562?568, Geneva, Switzerland.
Richard Sproat and Tom Emerson. 2003. The first inter-
national chinese word segmentation bakeoff. In Pro-
ceedings of the Second SIGHAN Workshop on Chinese
Language Processing, Sapporo, Japan, July.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for Sighan bake-
off 2005. In Proceedings of the Fourth SIGHAN Work-
shop on Chinese Language Processing, Jeju, Korea.
Nianwen Xue and Libin Shen. 2003. Chinese word
segmentation as LMR tagging. In Proceedings of the
Second SIGHAN Workshop on Chinese Language Pro-
cessing.
Huaping Zhang, HongKui Yu, Deyi xiong, and Qun Liu.
2003. HHMM-based Chinese lexical analyzer ICT-
CLAS. In Proceedings of the Second SIGHAN Work-
shop on Chinese Language Processing, pages 184?
187.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.
2006. Subword-based tagging by conditional random
fields for chinese word segmentation. In Proc. of HLT-
NAACL.
182
Sixth SIGHAN Workshop on Chinese Language Processing
