Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1093?1102, Prague, June 2007. c?2007 Association for Computational Linguistics
Mandarin Part-of-Speech Tagging and Discriminative Reranking
Zhongqiang Huang1
1Purdue University
West Lafayette, IN 47907
zqhuang@purdue.edu
Mary P. Harper1,2
2University of Maryland
College Park, MD 20742
mharper@casl.umd.edu
Wen Wang
SRI International
Menlo Park, CA 94025
wwang@speech.sri.com
Abstract
We present in this paper methods to improve
HMM-based part-of-speech (POS) tagging
of Mandarin. We model the emission prob-
ability of an unknown word using all the
characters in the word, and enrich the stan-
dard left-to-right trigram estimation of word
emission probabilities with a right-to-left
prediction of the word by making use of the
current and next tags. In addition, we utilize
the RankBoost-based reranking algorithm
to rerank the N-best outputs of the HMM-
based tagger using various n-gram, mor-
phological, and dependency features. Two
methods are proposed to improve the gen-
eralization performance of the reranking al-
gorithm. Our reranking model achieves an
accuracy of 94.68% using n-gram and mor-
phological features on the Penn Chinese
Treebank 5.2, and is able to further improve
the accuracy to 95.11% with the addition of
dependency features.
1 Introduction
Part-of-speech (POS) tagging is potentially help-
ful for many advanced natural language processing
tasks, for example, named entity recognition, pars-
ing, and sentence boundary detection. Much re-
search has been done to improve tagging perfor-
mance for a variety of languages. The state-of-the-
art systems have achieved an accuracy of 97% for
English on the Wall Street Journal (WSJ) corpus
(which contains 4.5M words) using various mod-
els (Brants, 2000; Ratnaparkhi, 1996; Thede and
Harper, 1999). Lower accuracies have been reported
in the literature for Mandarin POS tagging (Tseng et
al., 2005; Xue et al, 2002). This is, in part, due to
the relatively small size and the different annotation
guidelines (e.g., granularity of the tag set) for the an-
notated corpus of Mandarin. Xue at el. (2002) and
Tseng at el. (2005) reported accuracies of 93% and
93.74% on CTB-I (Xue et al, 2002) (100K words)
and CTB 5.0 (500K words), respectively, each us-
ing a Maximum Entropy approach. The character-
istics of Mandarin make it harder to tag than En-
glish. Chinese words tend to have greater POS tag
ambiguity than English. Tseng at el. (2005) reported
that 29.9% of the words in CTB have more than one
POS assignment compared to 19.8% of the English
words in WSJ. Moreover, the morphological prop-
erties of Chinese words complicate the prediction of
POS type for unknown words.
These challenges for Mandarin POS tagging
suggest the need to develop more sophisticated
methods. In this paper, we investigate the use
of a discriminative reranking approach to in-
crease Mandarin tagging accuracy. Reranking ap-
proaches (Charniak and Johnson, 2005; Chen et al,
2002; Collins and Koo, 2005; Ji et al, 2006; Roark
et al, 2006) have been successfully applied to many
NLP applications, including parsing, named entity
recognition, sentence boundary detection, etc. To
the best of our knowledge, reranking approaches
have not been used for POS tagging, possibly due
to the already high levels of accuracy for English,
which leave little room for further improvement.
However, the relatively poorer performance of ex-
isting methods on Mandarin POS tagging makes
reranking a much more compelling technique to
evaluate. In this paper, we use reranking to improve
tagging performance of an HMM tagger adapted to
1093
Mandarin. Hidden Markov models are simple and
effective, but unlike discriminative models, such as
Maximum Entropy models (Ratnaparkhi, 1996) and
Conditional Random Fields (John Lafferty, 2001),
they have more difficulty utilizing a rich set of con-
ditionally dependent features. This limitation can be
overcome by utilizing reranking approaches, which
are able to make use of the features extracted from
the tagging hypotheses produced by the HMM tag-
ger. Reranking also has advantages over MaxEnt
and CRF models. It is able to use any features
extracted from entire labeled sentences, including
those that cannot be incorporated into MaxEnt and
CRF models due to inference difficulties. In addi-
tion, reranking methods are able to utilize the infor-
mation provided by N-best lists. Finally, the decod-
ing phase of reranking is much simpler.
The rest of the paper is organized as follows. We
describe the HMM tagger in Section 2. We discuss
the modifications to better handle unknown words in
Mandarin and to enrich the word emission probabil-
ities through the combination of bi-directional esti-
mations. In Section 3, we first describe the reranking
algorithm and then propose two methods to improve
its performance. We also describe the features that
will be used for Mandarin POS reranking in Sec-
tion 3. Experimental results are given in Section 4.
Conclusions and future work appear in Section 5.
2 The HMM Model
2.1 Porting English Tagger to Mandarin
The HMM tagger used in this work is a second-
order HMM tagger initially developed for English
by Thede and Harper (1999). This state-of-the-art
second-order HMM tagger uses trigram transition
probability estimations, P (ti|ti?2ti?1), and trigram
emission probability estimations, P (wi|ti?1ti). Let
ti1 denote the tag sequence t1, ? ? ? , ti, and w
i
1 denote
the word sequencew1, ? ? ? , wi. The tagging problem
can be formally defined as finding the best tag se-
quence ?(wN1 ) for the word sequence w
N
1 of length
N as follows1:
?(wN1 ) = arg max
tN1
P (tN1 |w
N
1 ) = arg max
tN1
P (tN1 w
N
1 )
P (wN1 )
= arg max
tN1
P (tN1 w
N
1 ) (1)
= arg max
tN1
?
i
P (ti|t
i?1
1 w
i?1
1 )P (wi|t
i
1w
i?1
1 )
1We assume that symbols exist implicitly for boundary con-
ditions.
? arg max
tN1
?
i
P (ti|ti?2ti?1)P (wi|ti?1ti) (2)
The best tag sequence ?(wN1 ) can be determined ef-
ficiently using the Viterbi algorithm.
For estimating emission probabilities of unknown
words (i.e., words that do not appear in the train-
ing data) in English (and similarly for other inflected
languages), a weighted sum of P (ski |ti?1ti) (with
k up to four) was used as an approximation, where
ski is the suffix of length k of word wi (e.g., s
1
i is
the last character of word wi). The suffix informa-
tion and three binary features (i.e., whether the word
is capitalized, whether the word is hyphenated, and
whether the word contains numbers) are combined
to estimate the emission probabilities of unknown
words.
The interpolation weights for smoothing tran-
sition, emission, and suffix probabilities were
estimated using the log-based Thede smoothing
method (Thede and Harper, 1999) as follows:
PThede(n-gram)
= ?(n-gram)PML(n-gram) +
(1? ?(n-gram))PThede((n-1)-gram)
where:
PML(n-gram) = the ML estimation
?(n-gram) = f(n-gram count)
f(x) =
loga(x+ 1) + b
loga(x+ 1) + (b+ 1)
While porting the HMM-based English POS tag-
ger to Mandarin is fairly straightforward for words
seen in the training data, some thought is required to
handle unknown words due to the morphology dif-
ferences between the two languages. First, in Man-
darin, there is no capitalization and no hyphenation.
Second, although Chinese has morphology, it is not
the same as in English; words tend to contain far
fewer characters than inflected words in English, so
word endings will tend to be short, say one or two
characters long. Hence, in our baseline model (de-
noted HMM baseline), we simply utilize word end-
ings of up to two characters in length along with a
binary feature of whether the word contains num-
bers or not. In the next two subsections, we describe
two ways in which we enhance this simple HMM
baseline model.
1094
2.2 Improving the Mandarin Unknown Word
Model
Chinese words are quite different from English
words, and the word formation process for Chinese
words can be quite complex (Packard, 2000). In-
deed, the last characters in a Chinese word are, in
some cases, most informative of the POS type, while
for others, it is the characters at the beginning. Fur-
thermore, it is not uncommon for a character in the
middle of a word to provide some evidence for the
POS type of the word. Hence, we chose to employ
a rather simple but effective method to estimate the
emission probability, P (wi|ti?1, ti), of an unknown
word, wi. We use the geometric average2 of the
emission probability of the characters in the word,
i.e., P (ck|ti?1, ti) with ck being the k-th character
in the word. Since some of the characters in wi may
not have appeared in any word tagged as ti in that
context in the training data, only characters that are
observed in this context are used in the computation
of the geometric average, as shown below:
P (wi|ti?1, ti) = n
? ?
ck?wi,P (ck|ti?1,ti)6=0
P (ck|ti?1, ti) (3)
where
n = |{ck ? wi|P (ck|ti?1, ti) 6= 0}|
2.3 Bi-directional Word Probability Estimation
In Equation 2, the word emission probability
P (wi|ti?1ti) is a left-to-right prediction that de-
pends on the current tag ti associated with wi, as
well as its previous tag ti?1. Although the interac-
tion between wi and the next tag ti+1 is captured to
some extent when ti+1 is generated by the model,
this implicit interaction may not be as effective as
adding the information more directly to the model.
Hence, we chose to apply the constraint explicitly in
our HMM framework by replacing P (wi|ti?1ti) in
Equation 2 with P ?(wi|ti?1ti)P 1??(wi|titi+1) for
both known and unknown words, with ?(wN1 ) deter-
mined by:
?(wN1 ) = arg max
tN1
?
i
(P (ti|ti?2ti?1)?
P?(wi|ti?1ti)P
1??(wi|titi+1)) (4)
2Based on preliminary testing, the geometric average pro-
vided greater tag accuracy than the arithmetic average.
This corresponds to a mixture model of two genera-
tion paths, one from the left and one from the right,
to approximate ?(wN1 ) in Equation 1 in a different
way.
?(wN1 ) = arg max
tN1
P (tN1 w
N
1 )
= arg max
tN1
P (tN1 )P (w
N
1 |t
N
1 )
P (tN1 ) ?
?
i
P (ti|ti?1ti?2)
P (wN1 |t
N
1 ) = P
?(wN1 |t
N
1 )P
1??(wN1 |t
N
1 )
?
?
i
P?(wi|ti?1ti)P
1??(wi|titi+1)
In this case, the decoding process involves the
computation of three local probabilities, i.e.,
P (ti|ti?2ti?1), P (wi|ti?1ti), and P (wi|titi+1).
By using a simple manipulation that shifts the
time index of P (wi|titi+1) in Equation 4 by two
time slices3 (i.e., by replacing P (wi|titi+1) with
P (wi?2|ti?2ti?1)), we are able to compute ?(wN1 )
in Equation 4 with the same asymptotic time com-
plexity of decoding as in Equation 2.
3 Discriminative Reranking
In this section, we describe our use of the
RankBoost-based (Freund and Schapire, 1997; Fre-
und et al, 1998) discriminative reranking approach
that was originally developed by Collins and Koo
(2005) for parsing. It provides an additional avenue
for improving tagging accuracy, and also allows us
to investigate the impact of various features on Man-
darin tagging performance. The reranking algorithm
takes as input a list of candidates produced by some
probabilistic model, in our case the HMM tagger,
and reranks these candidates based on a set of fea-
tures. We first introduce Collins? reranking algo-
rithm in Subsection 3.1, and then describe two mod-
ifications in Subsections 3.2 and 3.3 that were de-
signed to improve the generalization performance of
the reranking algorithm for our POS tagging task.
The reranking features that are used for POS tagging
are then described in Subsection 3.4.
3.1 Collins? Reranking Algorithm
For training the reranker for the POS tagging task,
there are n sentences {si : i = 1, ? ? ? , n} each with
ni candidates {xi,j : j = 1, ? ? ? , ni} along with
3Replacing P (wi|titi+1) with P (wi?1|ti?1ti) also gives
the same solution.
1095
the log-probability L(xi,j) produced by the HMM
tagger. Each tagging candidate xi,j in the training
data has a ?goodness? score Score(xi,j) that mea-
sures the similarity between the candidate and the
gold reference. For tagging, we use tag accuracy
as the similarity measure. Without loss of general-
ity, we assume that xi,1 has the highest score, i.e.,
Score(xi,1) ? Score(xi,j) for j = 2, ? ? ? , ni. To
summarize, the training data consists of a set of ex-
amples {xi,j : i = 1, ? ? ? , n; j = 1, ? ? ? , ni}, each
along with a ?goodness? score Score(xi,j) and a
log-probability L(xi,j).
A set of indicator functions {hk : k = 1, ? ? ? ,m}
are used to extract binary features {hk(xi,j) : k =
1, ? ? ? ,m} on each example xi,j . An example of an
indicator function for POS tagging is given below:
h2143(x) = 1 ifx contains n-gram ?go/VV to?
0 otherwise
Each indicator function hk is associated with a
weight parameter ?k which is real valued. In ad-
dition, a weight parameter ?0 is associated with
the log-probability L(xi,j). The ranking func-
tion of candidate xi,j is defined as ?0L(xi,j) +
m?
k=1
?khk(xi,j).
The objective of the training process is to set the
parameters ?? = {?0, ?1, ? ? ? , ?m} to minimize the
following loss function Loss(??) (which is an upper
bound on the training error):
Loss(??) =
?
i
ni?
j=2
Si,je
?Mi,j(??)
where Si,j is the weight function that gives the im-
portance of each example, and Mi,j(??) is the mar-
gin:
Si,j = Score(xi,1)? Score(xi,j)
Mi,j(??) = ?0(L(xi,1)? L(xi,j)) +
m?
k=1
?k(hk(xi,1)? hk(xi,j))
All of the ?i?s are initially set to zero. The value
of ?0 is determined first to minimize the loss func-
tion and is kept fixed afterwards. Then a greedy se-
quential 4 optimization method is used in each itera-
tion (i.e., a boosting round) to select the feature that
4Parallel optimization algorithms exist and have comparable
performance according to (Collins et al, 2002).
has the most impact on reducing the loss function
and then update its weight parameter accordingly.
For each k ? {1, ? ? ? ,m}, (hk(xi,1)? hk(xi,j)) can
only take one of the three values: +1, -1, or 0. Thus
the training examples can be divided into three sub-
sets with respect to k:
A+k = {(i, j) : (hk(xi,1)? hk(xi,j)) = +1}
A?k = {(i, j) : (hk(xi,1)? hk(xi,j)) = ?1}
A0k = {(i, j) : (hk(xi,1)? hk(xi,j)) = 0}
The new loss after adding the update parameter ?
to the parameter ?k is shown below:
Loss(??, k, ?) =
?
(i,j)?A+
k
Si,je
?Mi,j(??)?? +
?
(i,j)?A?
k
Si,je
?Mi,j(??)+? +
?
(i,j)?A0
k
Si,je
?Mi,j(??)
= e??W+k + e
?W?k +W
0
k
The best feature/update pair (k?, ??) that minimizes
Loss(??, k, ?) is determined using the following for-
mulas:
k? = arg max
k
?
?
?
?
?
W+k ?
?
W?k
?
?
?
? (5)
?? =
1
2
log
W+k?
W?k?
(6)
The update formula in Equation 6 is problematic
when either W+k? or W
?
k? is zero. W
+
k is zero if hk
never takes on a value 1 for any xi,1 with value 0 on
a corresponding xi,j for j = 2, ? ? ? , ni (and similarly
for W?k ). Collins introduced a smoothing parameter
 to address this problem, resulting in a slight modi-
fication to the update formula:
?? =
1
2
log
W+k? + Z
W?k? + Z
(7)
The value of  plays an important role in this for-
mula. If  is set too small, the smoothing factor Z
would not prevent setting ?? to a potentially overly
large absolute value, resulting in over-fitting. If  is
set too large, then the opposite condition of under-
training could result. The value of  is determined
based on a development set.
1096
3.2 Update Once
Collins? method allows multiple updates to the
weight of a feature based on Equations 5 and 7. We
found that for those features for which either W+k or
W?k equals zero, the update formula in Equation 7
can only increase their weight (in absolute value) in
one direction. Although these features are strong
and useful, setting their weights too large can be un-
desirable in that it limits the use of other features for
reducing the loss.
Based on this analysis, we have developed and
evaluated an update-once method, in which we use
the update formula in Equation 7 but limit weight
updates so that once a feature is selected on a cer-
tain iteration and its weight parameter is updated,
it cannot be updated again. Using this method, the
weights of the strong features are not allowed to pre-
vent additional features from being considered dur-
ing the training phase.
3.3 Regularized Reranking
Although the update-once method may attenuate
over-fitting to some extent, it also prevents adjust-
ing the value of any weight parameter that is initially
set too high or too low in an earlier boosting round.
In order to design a more sophisticated weight up-
date method that allows multiple updates in both di-
rections while penalizing overly large weights, we
have also investigated the addition of a regulariza-
tion term R(??), an exponential function of ??, to the
loss function:
RegLoss(??) =
?
i
ni?
j=2
Si,je
?Mi,j(??) +R(??)
R(??) =
m?
k=1
pk ? (e
??k + e?k ? 2)
where pk is the penalty weight of parameter ?k. The
reason that we chose this form of regularization is
that (e??k +e?k?2) is a symmetric, monotonically
decreasing function of |?k|, and more importantly it
provides a closed analytical expression of the weight
update formula similar to Equations 5 and 6. Hence,
the best feature/update pair for the regularized loss
function is defined as follows:
k? = arg max
k
?
?
?
?
?
W+k + pke
??k ?
?
W?k + pke
+?k
?
?
?
?
?? =
1
2
log
W+k? + pk?e
??k?
W?k? + pk?e
+?k?
There are many ways of choosing pk, the penalty
weight of ?k. In this paper, we use the values of
? ?(W+k +W
?
k ) at the beginning of the first iteration
(after ?0 is determined) for pk, where ? is a weight-
ing parameter to be tuned on the development set.
The regularized weight update formula has many ad-
vantages. It is always well defined no matter what
value W+k and W
?
k take, in contrast to Equation 6.
For all features, even in the case when either W+k or
W?k equals zero, the regularized update formula al-
lows weight updates in two directions. If the weight
is small, W+k and W
?
k have more impact on deter-
mining the weight update direction, however, when
the weight becomes large, the regularization factors
pke?? and pke+? favor reducing the weight.
3.4 Reranking Features
A reranking model has the flexibility of incorporat-
ing any type of feature extracted from N-best can-
didates. For the work presented in this paper, we
examine three types of features. For each window
of three word/tag pairs, we extract all the n-grams,
except those that are comprised of only one word/tag
pair, or only tags, or only words, or do not include
either the word or tag in the center word/tag pair.
These constitute the n-gram feature set.
In order to better handle unknown words, we also
extract the two most important types of morpho-
logical features5 that were utilized in (Tseng et al,
2005) for those words that appear no more than
seven times (following their convention) in the train-
ing set:
Affixation features: we use character n-gram pre-
fixes and suffixes for n up to 4. For example,
for word/tag pair D??/NN (Information-
Bag, i.e., folder), we add the following fea-
tures: (prefix1, D, NN), (prefix2, D?, NN),
(prefix3, D??, NN), (suffix1, ?, NN), (suf-
fix2,??, NN), (suffix3,D??, NN).
AffixPOS features6: we used the training set to
build a prefix/POS and suffix/POS dictionary
associating possible tags with each prefix and
5Tseng at el. also used other morphological features that
require additional resources to which we do not have access.
6AffixPOS features are somewhat different from the CTB-
Morph features used in (Tseng et al, 2005), where a mor-
pheme/POS dictionary with the possible tags for all morphemes
in the training set was used instead of two separate dictionaries
for prefix and suffix. AffixPOS features perform slightly better
in our task than the CTB-morph features.
1097
suffix in the training set. The AffixPOS fea-
tures indicate the set of tags a given affix could
have. For the same example D??/NN, D
occurred as prefix in both NN and VV words in
the training data. So we add the following fea-
tures based on the prefix D: (prefix, D, NN,
1, NN), (prefix, D, VV, 1, NN), and (prefix,
D, X, 0, NN) for every tag X not in {NN, VV},
where 1 and 0 are indicator values. Features are
extracted in the similar way for the suffix?.
The n-gram and morphological features are easy
to compute, however, they have difficulty in captur-
ing the long distance information related to syntac-
tic relationships that might help POS tagging ac-
curacy. In order to examine the effectiveness of
utilizing syntactic information in tagging, we have
also experimented with dependency features that are
extracted based on automatic parse trees. First a
bracketing parser (the Charniak parser (Charniak,
2000) in our case) is used to generate the parse
tree of a sentence, then the const2dep tool devel-
oped by Hwa was utilized to convert the bracket-
ing tree to a dependency tree based on the head
percolation table developed by the second author.
The dependency tree is comprised of a set of de-
pendency relations among word pairs. A depen-
dency relation is a triple ?word-a, word-b, relation?,
in which word-a is governed by word-b with gram-
matical relation denoted as relation. For example,
in the sentence ??(Tibet) ?N(economy) ?
?(construction) ??(achieves) >W(significant)
?(accomplishments)?, one example dependency
relation is ???, ?, mod?. Given these depen-
dency relations, we then extract dependency features
(in total 36 features for each relation) by examining
the POS tags of the words for each tagging candi-
date of a sentence. The relative positions of the word
pairs are also taken into account for some features.
For example, if?? and? in the above sentence
are tagged as VV and NN respectively in one can-
didate, then two example dependency features are
(dep-1, ??, VV, ?, NN, mod), (dep-14, ??,
VV, NN, right, mod), in which dep-1 and dep-14 are
feature types and right indicates that word-b (??)
is to the right of word-a (?).
4 Experiments
4.1 Data
The most recently released Penn Chinese Treebank
5.2 (denoted CTB, released by LDC) is used in our
experiments. It contains 500K words, 800K char-
acters, 18K sentences, and 900 data files, includ-
ing articles from the Xinhua news agency (China-
Mainland), Information Services Department of
HKSAR (Hongkong), and Sinorama magazine (Tai-
wan). Its format is similar to the English WSJ Penn
Treebank, and it was carefully annotated. There are
33 POS tags used, to which we add tags to discrim-
inate among punctuation types. The original POS
tag for punctuation was PU; we created new POS
tags for each distinct punctuation type (e.g., PU-?).
The CTB corpus was collected during different
time periods from different sources with a diversity
of articles. In order to obtain a representative split
of training, development, and test sets, we divide
the whole corpus into blocks of 10 files by sorted
order. For each block, the first file is used for de-
velopment, the second file is used for test, and the
remaining 8 files are used for training. Table 1 gives
the basic statistics on the data. The development
set is used to determine the parameter ? in Equa-
tion 4, the smoothing parameter  in Equation 7, the
weight parameter ? described in Section 3.3, and the
number of boosting rounds in the reranking model.
In order to train the reranking model, the method
in (Collins and Koo, 2005) is used to prepare the
N-best training examples. We divided the training
set into 20 chunks, with each chunk N-best tagged
by the HMM model trained on the combination of
the other 19 chunks. The development set is N-best
tagged by the HMM model trained on the training
set, and the test set is N-best tagged by the HMM
model trained on the combination of the training set
and the development set.
Train Dev Test
#Sentences 14925 1904 1975
#Words 404844 51243 52900
Table 1: The basic statistics on the data.
In the following subsections, we will first exam-
ine the HMM models alone to determine the best
HMM configuration to use to generate the N-best
candidates, and then evaluate the reranking mod-
els. Finally, we compare our performance with pre-
vious work. In this paper, we use the sign test
with p ? 0.01 to evaluate the statistical significance
of the difference between the performances of two
models.
1098
4.2 Results of the HMM taggers
The baseline HMM model ported directly from the
English tagger, as described in Subsection 2.1, has
an overall tag accuracy of 93.12% on the test set,
which is fairly low compared to the 97% accuracy
of many state-of-the-art taggers on WSJ for English.
By approximating the unknown word emission
probability using the characters in the word as in
Equation 3, the performance of the HMM tagger im-
proves significantly to 93.43%, suggesting that char-
acters in different positions of a Chinese word help
to disambiguate the word class of the entire word, in
contrast to English for which suffixes are most help-
ful.
Figure 1 depicts the impact of combining the left-
to-right and right-to-left word emission models us-
ing different weighting values (i.e., ?) on the devel-
opment set. Note that emission probabilities of un-
known words are estimated based on characters us-
ing the same ? for combination. When ? = 1.0, the
model uses only the standard left-to-right prediction
of words, while when ? = 0 it uses only the right-to-
left estimation. It is interesting to note that the right-
to-left estimation results in greater accuracy than the
left-to-right estimation. This might be because there
is stronger interaction between a word and its next
tag. Also as shown in Figure 1, the estimations in
the two directions are complementary to each other,
with ? = 0.5 performing best. The performance of
the HMM taggers on the test set is given in Table 2
for the best operating point, as well as the two other
extreme operating points to compare the left-to-right
and right-to-left constraints. Our best HMM tagger
further improves the tag accuracy significantly from
93.43% (? = 1.0) to 94.01% (? = 0.5).
Figure 1: The accuracy of the HMM tagger on the
development set with various ? values for combin-
ing the word emission probabilities.
Overall Known Unknown
HMM baseline 93.12% 94.65% 69.08%
HMM, ?=1.0 93.43% 94.71% 73.41%
HMM, ?=0.0 93.65% 94.88% 74.23%
HMM, ?=0.5 94.01% 95.21% 75.15%
Table 2: The performance of various HMM taggers
on the test set.
4.3 Results of the Reranking Models
The HMM tagger with the best accuracy (i.e., the
one with ? = 0.5 in Table 2) is used to generate
the N-Best tagging candidates, with a maximum of
100 candidates. As shown in Table 3, a maximum of
100-Best provides a reasonable margin for improve-
ment in the reranking task.
We first test the performance of the reranking
methods using only the n-gram feature set, which
contains around 18 million features. Later, we
will investigate the addition of morphological fea-
tures and dependency features. The smoothing
parameter  (for Collins? method and the update-
once method) and the weight parameter ? (for
the regularization method) both have great im-
pact on reranking performance. We trained vari-
ous reranking models with  values of 0.0001 ?
{1, 2.5, 5, 7.5, 10, 25, 50, 75, 100}, and ? values of
{0.1, 0.25, 0.5, 0.75, 1}. For all these parameter val-
ues, 600,000 rounds of iterations were executed on
the training set. The development set was used to
determine the early stopping point in training. If
not mentioned explicitly, all the results reported are
based on the best parameters tuned on the develop-
ment set.
1-Best 50-Best 100-Best
train 93.48% 96.96% 97.13%
dev 93.75% 97.68% 97.84%
test 93.19% 97.19% 97.35%
Table 3: The oracle tag accuracies of the 1-Best, 50-
Best, and 100-Best candidates in the training, devel-
opment, and test sets for the reranking experiments.
Note that the tagging candidates are prepared using
the method described in Subsection 4.1.
Table 4 reports the performance of the best HMM
tagger and the three reranking taggers on the test set.
All three reranking methods improve the HMM tag-
ger significantly. Also, the update-once and regu-
larization methods both outperform Collins? original
training method significantly.
1099
Overall Known Unknown
HMM, ?=0.5 94.01% 95.21% 75.15%
Collins 94.38% 95.56% 75.85%
Update-once 94.50% 95.67% 76.13%
Regularized 94.54% 95.70% 76.48%
Table 4: The performance on the test set of the
HMM tagger, and the reranking methods using the
n-gram features.
We observed that no matter which value the
smoothing parameter  takes, there are only about
10,000 non-zero features finally selected by Collins?
original method. In contrast, the two new methods
select substantially more features, as shown in Ta-
ble 5. As mentioned before, there are some strong
features that only appear in positive or negative sam-
ples, i.e., either W+k or W
?
k equals zero. Although
introducing the smoothing parameter  in Equation 7
prevents infinite weight values, the update to the
feature weights is no longer optimal (in terms of
minimizing the error function). Since the update
is not optimal, subsequent iterations may still fo-
cus on these features (and thus ignore other weaker
but informative features) and always increase their
weights in one direction, leading to biased training.
The update-once method at each iteration selects
a new feature that has the most impact in reduc-
ing the training loss function. It has the advantage
of preventing increasingly large weights from being
assigned to the strong features, enabling the update
of other features. The regularization method allows
multiple updates and also penalizes large weights.
Once a feature is selected and has its weight updated,
no matter how strong the feature is, the weight value
is optimal in terms of the current weights of other
features, so that the training algorithm would choose
another feature to update. A previously selected fea-
ture may be selected again if it becomes suboptimal
due to a change in the weights of other features.
#iterations #features percent
Collins 115400 10020 8.68%
Update-once 545100 545100 100%
Regularized 92500 70131 75.82%
Table 5: The number of iterations (for the best
performance), the number of selected features, and
the percentage of selected features, by Collins?
method, the update-once method, and the regular-
ization method on the development set.
Overall Known Unknown
HMM, ?=0.5 94.01% 95.21% 75.15%
Collins 94.44% 95.55% 77.05%
Update-once 94.68% 95.68% 78.91%
Regularized 94.64% 95.71% 77.84%
Table 6: The performance on the test set of the
HMM tagger and the reranking methods using n-
gram and morphological features.
We next add morphological features to the n-gram
features selected by the reranking methods7. As
can be seen by comparing Table 6 to Table 4, mor-
phological features improve the tagging accuracy of
unknown words. It should be noted that the im-
provement made by both update-one and regulariza-
tion methods is statistically significant over using n-
gram features alone; however, the improvement by
Collins? original method is not significant. This sug-
gests that the two new methods are able to utilize a
greater variety of features than the original method.
We trained several Charniak parsers using the
same method for the HMM taggers to generate auto-
matic parse trees for training, development, and test
data. The update-once method is used to evaluate
the effectiveness of dependency features for rerank-
ing, as shown in Table 7. The parser has an overall
tagging accuracy that is greater than that of the best
HMM tagger, but worse than that of the reranking
models using n-gram and morphological features. It
is interesting to note that reranking with the depen-
dency features alone improves the tagging accuracy
significantly, outperforming reranking models using
n-gram and morphological features. This suggests
that the long distance features based on the syntactic
structure of the sentence are very beneficial for POS
tagging of Mandarin. Moreover, n-gram and mor-
phological features are complementary to the depen-
dency features, with their combination performing
the best. The n-gram features improve the accuracy
on known words, while the morphological features
improve the accuracy on unknown words. The best
accuracy of 95.11% is an 18% relative reduction in
error compared to the best HMM tagger.
7Because the size of the combined feature set of all n-gram
features and morphological features is too large to be handled
by our server, we chose to add morphological features to the
n-gram features selected by the reranking methods, and then
retrain the reranking model.
1100
Overall Known Unknown
Parser 94.31% 95.57% 74.52%
dep 94.93% 96.01% 77.87%
dep+ngram 95.00% 96.11% 77.49%
dep+morph 94.98% 96.01% 78.79%
dep+ngram+morph 95.11% 96.12% 79.32%
Table 7: The tagging performance of the parser
and the update-once reranking models with depen-
dency features and their combination with n-gram
and morphological features.
4.4 Comparison to Previous Work
So how is our performance compared to previous
work? When working on the same training/test data
(CTB5.0 with the same pre-processing procedures)
as in (Tseng et al, 2005), our HMM model ob-
tained an accuracy of 93.72%, as compared to their
93.74% accuracy. Our reranking model8 using n-
gram and morphological features improves the ac-
curacy to 94.16%. Note that we did not use all the
morphological features as in (Tseng et al, 2005),
which would probably provide additional improve-
ment. The dependency features are expected to fur-
ther improve the performance, although they are not
included here in order to provide a relatively fair
comparison.
5 Conclusions and Future Work
We have shown that the characters in a word are
informative of the POS type of the entire word in
Mandarin, reflecting the fact that the individual Chi-
nese characters carry POS information to some de-
gree. The syntactic relationship among characters
may provide further information, which we leave
as future work. We have also shown that the ad-
ditional right-to-left estimation of word emission
probabilities is useful for HMM tagging of Man-
darin. This suggests that explicit modeling of bi-
directional interactions captures more sequential in-
formation. This could possibly help in other sequen-
tial modeling tasks.
We have also investigated using the reranking al-
gorithm in (Collins and Koo, 2005) for the Man-
darin POS tagging task, and found it quite effective
8Tseng at el.?s training/test split uses up the entire CTB cor-
pus, leaving no development data for tuning parameters. In
order to roughly measure reranking performance, we use the
update-once method to train the reranking model for 600,000
rounds with the other parameters tuned in Section 4. This sac-
rifices performance to some extent.
in improving tagging accuracy. The original algo-
rithm has a tendency to focus on a small subset of
strong features and ignore some of the other useful
features. We were able to improve the performance
of the reranking algorithm by utilizing two different
methods that make better use of more features. Both
are simple and yet effective. The effectiveness of de-
pendency features suggests that syntax-based long
distance features are important for improving part-
of-speech tagging performance in Mandarin. Al-
though parsing is computationally more demanding
than tagging, we hope to identify related features
that can be extracted more efficiently.
In future efforts, we plan to extract additional
reranking features utilizing more explicitly the char-
acteristics of Mandarin. We also plan to extend our
work to speech transcripts for Broadcast News and
Broadcast Conversation corpora, and explore semi-
supervised training methods for reranking.
Acknowledgments
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of DARPA. We gratefully acknowledge the com-
ments from the anonymous reviewers.
References
Thorsten Brants. 2000. TnT a statistical part-of-speech
tagger. In ANLP, pages 224?231.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In ACL.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the first conference on North
American chapter of the Association for Computa-
tional Linguistics, pages 132?139, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
John Chen, Srinivas Bangalore, Michael Collins, and
Owen Rambow. 2002. Reranking an n-gram supertag-
ger. In the Sixth International Workshop on Tree Ad-
joining Grammars and Related Frameworks.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25?70.
1101
Michael Collins, Robert E. Schapire, and Yoram Singer.
2002. Logistic regression, adaboost and bregman dis-
tances. Machine Learning, 48(1):253?285.
Yoav Freund and Robert E. Schapire. 1997. A decision-
theoretic generalization of on-line learning and an ap-
plication to boosting. Journal of Computer and System
Sciences, 1(55):119?139.
Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram
Singer. 1998. An efficient boosting algorithm for
combining preferences. In the Fifteenth International
Conference on Machine Learning.
Heng Ji, Cynthia Rudin, and Ralph Grishman. 2006. Re-
ranking algorithms for name tagging. In HLT/NAACL
06 Workshop on Computationally Hard Problems and
Joint Inference in Speech and Language Processing.
Fernando Pereira John Lafferty, Andrew McCallum.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In ICML.
Jerome Packard. 2000. The Morphology of Chinese.
Cambridge University Press.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In EMNLP.
Brian Roark, Yang Liu, Mary Harper, Robin Stewart,
Matthew Lease, Matthew Snover, Izhak Shafran, Bon-
nie Dorr, John Hale, Anna Krasnyanskaya, and Lisa
Yung. 2006. Reranking for sentence boundary detec-
tion in conversational speech. In ICASSP.
Scott M. Thede and Mary P. Harper. 1999. A second-
order hidden Markov model for part-of-speech tag-
ging. In ACL, pages 175?182.
Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005. Morphological features help pos tagging
of unknown words across language varieties. In the
Fourth SIGHAN Workshop on Chinese Language Pro-
cessing.
Nianwen Xue, Fu dong Chiou, and Martha Palmer. 2002.
Building a large-scale annotated chinese corpus. In
COLING.
1102
