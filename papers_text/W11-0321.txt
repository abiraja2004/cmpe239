Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 181?189,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Authorship Attribution with Latent Dirichlet Allocation
Yanir Seroussi Ingrid Zukerman
Faculty of Information Technology, Monash University
Clayton, Victoria 3800, Australia
firstname.lastname@monash.edu
Fabian Bohnert
Abstract
The problem of authorship attribution ? at-
tributing texts to their original authors ? has
been an active research area since the end of
the 19th century, attracting increased interest
in the last decade. Most of the work on au-
thorship attribution focuses on scenarios with
only a few candidate authors, but recently con-
sidered cases with tens to thousands of can-
didate authors were found to be much more
challenging. In this paper, we propose ways
of employing Latent Dirichlet Allocation in
authorship attribution. We show that our ap-
proach yields state-of-the-art performance for
both a few and many candidate authors, in
cases where these authors wrote enough texts
to be modelled effectively.
1 Introduction
The problem of authorship attribution ? attributing
texts to their original authors ? has received con-
siderable attention in the last decade (Juola, 2006;
Stamatatos, 2009). Most of the work in this field fo-
cuses on cases where texts must be attributed to one
of a few candidate authors, e.g., (Mosteller and Wal-
lace, 1964; Gamon, 2004). Recently, researchers
have turned their attention to scenarios with tens to
thousands of candidate authors (Koppel et al, 2011).
In this paper, we study authorship attribution with
few to many candidate authors, and introduce a new
method that achieves state-of-the-art performance in
the latter case.
Our approach to authorship attribution consists of
building models of authors and their texts using La-
tent Dirichlet Allocation (LDA) (Blei et al, 2003).
We compare these models to models built from texts
with unknown authors to find the most likely authors
of these texts (Section 3.2). Our evaluation shows
that our approach yields a higher accuracy than the
method recently introduced by Koppel et al (2011)
in several cases where prolific authors are consid-
ered, while requiring less runtime (Section 4).
This paper is structured as follows. Related work
is surveyed in Section 2. Our LDA-based approach
to authorship attribution is described in Section 3,
together with the baselines we considered in our
evaluation. Section 4 presents and discusses the re-
sults of our evaluation, and Section 5 discusses our
conclusions and plans for future work.
2 Related Work
The field of authorship attribution predates modern
computing. For example, in the late 19th century,
Mendenhall (1887) suggested that word length can
be used to distinguish works by different authors. In
recent years, increased interest in authorship attribu-
tion was fuelled by advances in machine learning,
information retrieval, and natural language process-
ing (Juola, 2006; Stamatatos, 2009).
Commonly used features in authorship attribu-
tion range from ?shallow? features, such as token
and character n-gram frequencies, to features that
require deeper analysis, such as part-of-speech and
rewrite rule frequencies (Stamatatos, 2009). As in
other text classification tasks, Support Vector Ma-
chines (SVMs) have delivered high accuracy, as
they are designed to handle feature vectors of high
dimensionality (Juola, 2006). For example, one-
vs.-all (OVA) is an effective approach to using bi-
nary SVMs for multi-class (i.e., multi-author) prob-
lems (Rifkin and Klautau, 2004). Given A authors,
181
OVA trains A binary classifiers, where each classi-
fier is trained on texts by one author as positive ex-
amples and all the other texts as negative examples.
However, ifA is large, each classifier has many more
negative than positive examples, often yielding poor
results due to class imbalance (Raskutti and Kowal-
czyk, 2004). Other setups, such as one-vs.-one or
directed acyclic graph, require training O(A2) clas-
sifiers, making them impractical where thousands of
authors exist. Multi-class SVMs have also been sug-
gested, but they generally perform comparably to
OVA while taking longer to train (Rifkin and Klau-
tau, 2004). Hence, using SVMs for scenarios with
many candidate authors is problematic (Koppel et
al., 2011). Recent approaches to employing binary
SVMs consider class similarity to improve perfor-
mance (Bickerstaffe and Zukerman, 2010; Cheng
et al, 2007). We leave experiments with such ap-
proaches for future work (Section 5).
In this paper, we focus on authorship attribution
with many candidate authors. This problem was pre-
viously addressed by Madigan et al (2005) and Luy-
ckx and Daelemans (2008), who worked on datasets
with texts by 114 and 145 authors respectively. In
both cases, the reported results were much poorer
than those reported in the binary case. More re-
cently, Koppel et al (2011) considered author sim-
ilarity to handle cases with thousands of candidate
authors. Their method, which we use as our base-
line, is described in Section 3.1.
Our approach to authorship attribution utilises La-
tent Dirichlet Allocation (LDA) (Blei et al, 2003)
to build models of authors from their texts. LDA
is a generative probabilistic model that is tradition-
ally used to find topics in textual data. The main
idea behind LDA is that each document in a cor-
pus is generated from a distribution of topics, and
each word in the document is generated according
to the per-topic word distribution. Blei et al (2003)
showed that using LDA for dimensionality reduction
can improve performance for supervised text clas-
sification. We know of only one case where LDA
was used in authorship attribution: Rajkumar et al
(2009) reported preliminary results on using LDA
topic distributions as feature vectors for SVMs, but
they did not compare the results obtained with LDA-
based SVMs to those obtained with SVMs trained
on tokens directly. Our comparison shows that both
methods perform comparably (Section 4.3).
Nonetheless, the main focus of our work is
on authorship attribution with many candidate au-
thors, where it is problematic to use SVMs. Our
LDA+Hellinger approach employs LDA without
SVM training (Section 3.2), yielding state-of-the-art
performance in several scenarios (Section 4).
3 Authorship Attribution Methods
This section describes the authorship attribution
methods considered in this paper. While all these
methods can employ various representations of doc-
uments, e.g., token frequencies or part-of-speech n-
gram frequencies, we only experimented with token
frequencies.1 This is because they are simple to ex-
tract, and can achieve good performance (Section 4).
Further, the focus of this paper is on comparing the
performance of our methods to that of the baseline
methods. Thus, we leave experiments on other fea-
ture types for future work (Section 5).
3.1 Baselines
We consider two baseline methods, depending on
whether there are two or many candidate authors.
If there are only two, we use Support Vector Ma-
chines (SVMs), which have been shown to de-
liver state-of-the-art performance on this task (Juola,
2006). If there are many, we follow Koppel et
al.?s (2011) approach, which we denote KOP.
The main idea behind KOP is that different pairs
of authors may be distinguished by different sub-
sets of the feature space. Hence, KOP randomly
chooses k1 subsets of size k2F (k2 < 1) from a set
of F features; for each of the k1 subsets, it calcu-
lates the cosine similarity between a test document
and all the documents by one author (each author is
represented by one feature vector); it then outputs
the author who had most of the top matches. KOP
also includes a threshold ?? to handle cases where
a higher level of precision is required, at the cost
of lower recall. If the top-matching author was the
top match less than ?? times, then KOP outputs ?un-
known author?. In our experiments we set ?? = 0 to
obtain full coverage, as this makes it easier to inter-
pret the results using a single measure of accuracy.
1Token frequency is the token count divided by the total
number of tokens.
182
3.2 Authorship Attribution with LDA
In this work, we follow the extended LDA model de-
fined by Griffiths and Steyvers (2004). Under the as-
sumptions of the extended model, given a corpus of
M documents, a document iwithN tokens is gener-
ated by choosing a document topic distribution ?i ?
Dir(?), where Dir(?) is a T -dimensional symmet-
ric Dirichlet distribution, and ? and T are parame-
ters of the model. Then, each token in the document
wij is generated by choosing a topic from the docu-
ment topic distribution zij ? Multinomial(?i), and
choosing a token from the token topic distribution
wij ? Multinomial(?zij ), where ?zij ? Dir(?), and
? is a parameter of the model. The model can be
inferred from the data using Gibbs sampling, as out-
lined in (Griffiths and Steyvers, 2004) ? an approach
we follow in our experiments.
Note that the topics obtained by LDA do not have
to correspond to actual, human-interpretable topics.
A more appropriate name may be ?latent factors?,
but we adopt the convention of calling these fac-
tors ?topics? throughout this paper. The meaning of
the factors depends on the type of tokens that are
used as input to the LDA inference process. For
example, if stopwords are removed from the cor-
pus, the resulting factors often, but not necessarily,
correspond to topics. However, if only stopwords
are retained, as is commonly done in authorship at-
tribution studies, the resulting factors lose their in-
terpretability as topics; rather, they can be seen as
stylistic markers. Note that even if stopwords are
discarded, nothing forces the factors to stand for ac-
tual topics. Indeed, in a preliminary experiment on a
corpus of movie reviews and message board posts,
we found that some factors correspond to topics,
with words such as ?noir? and ?detective? consid-
ered to be highly probable for one topic. However,
other factors seemed to correspond to authorship
style as reflected by authors? vocabulary, with net-
speak words such as ?wanna?, ?alot? and ?haha? as-
signed to one topic, and words such as ?compelling?
and ?beautifully? assigned to a different topic.
We consider two ways of using LDA in authorship
attribution: (1) Topic SVM, and (2) LDA+Hellinger.
The LDA part of both approaches consists of apply-
ing a frequency filter to the features in the training
documents,2 and then using LDA to reduce the di-
mensionality of each document to a topic distribu-
tion of dimensionality T .
Topic SVM. The topic distributions are used as
features for a binary SVM classifier that discrimi-
nates between authors. This approach has been em-
ployed in the past for document classification, e.g.,
in (Blei et al, 2003), but it has been applied to au-
thorship attribution only in a limited study that con-
sidered just stopwords (Rajkumar et al, 2009). In
Section 4.3, we present the results of more thorough
experiments in applying this approach to binary au-
thorship attribution. Our results show that the per-
formance of this approach is comparable to that ob-
tained without using LDA. This indicates that we
do not lose authorship-related information when em-
ploying LDA, even though the dimensionality of the
document representations is greatly reduced.
LDA+Hellinger. This method is our main contri-
bution, as it achieves state-of-the-art performance in
authorship attribution with many candidate authors,
where it is problematic to use SVMs (Section 2).
The main idea of our approach is to use the
Hellinger distance between document topic distribu-
tions to find the most likely author of a document:3
D(?1, ?2) =
?
1
2
?T
t=1
(?
?1,t ?
?
?2,t
)2
where ?i
is a T -dimensional multinomial topic distribution,
and ?i,t is the probability of the t-th topic.
We propose two representations of an author?s
documents: multi-document and single-document.
? Multi-document (LDAH-M). The LDA model
is built based on all the training documents.
Given a test document, we measure the
Hellinger distance between its topic distribu-
tion and the topic distributions of the training
documents. The author with the lowest mean
distance for all of his/her documents is returned
as the most likely author of the test document.
2We employed frequency filtering because it has been shown
to be a scalable and effective feature selection method for au-
thorship attribution tasks (Stamatatos, 2009). We leave experi-
ments with other feature selection methods for future work.
3We considered other measures for comparing topic dis-
tributions, including Kullback-Leibler divergence and Bhat-
tacharyya distance. From these measures, only Hellinger dis-
tance satisfies all required properties of a distance metric.
Hence, we used Hellinger distance.
183
? Single-document (LDAH-S). Each author?s
documents are concatenated into a single doc-
ument (the profile document), and the LDA
model is learned from the profile documents.4
Given a test document, the Hellinger distance
between the topic distributions of the test docu-
ment and all the profile documents is measured,
and the author of the profile document with the
shortest distance is returned.
The time it takes to learn the LDA model de-
pends on the number of Gibbs samples S, the num-
ber of tokens in the training corpusW , and the num-
ber of topics T . For each Gibbs sample, the al-
gorithm iterates through all the tokens in the cor-
pus, and for each token it iterates through all the
topics. Thus, the time complexity of learning the
model is O(SWT ). Once the model is learned, in-
ferring the topic distribution of a test document of
length N takes O(SNT ). Therefore, the time it
takes to classify a document when using LDAH-S
isO(SNT+AT ), whereA is the number of authors,
and O(T ) is the time complexity of calculating the
Hellinger distance between two T -dimensional dis-
tributions. The time it takes to classify a docu-
ment when using LDAH-M is O(SNT + MT ),
where M is the total number of training documents,
and M ? A, because every candidate author has
written at least one document.
An advantage of LDAH-S over LDAH-M is that
LDAH-S requires much less time to classify a test
document when many documents per author are
available. However, this improvement in runtime
may come at the price of accuracy, as authorship
markers that are present only in a few short doc-
uments by one author may lose their prominence
if these documents are concatenated to longer doc-
uments. In our evaluation we found that LDAH-
M outperforms LDAH-S when applied to one of
the datasets (Section 4.3), while LDAH-S yields
a higher accuracy when applied to the other two
datasets (Sections 4.4 and 4.5). Hence, we present
the results obtained with both variants.
4Concatenating all the author documents into one document
has been named the profile-based approach in previous studies,
in contrast to the instance-based approach, where each docu-
ment is considered separately (Stamatatos, 2009).
4 Evaluation
In this section, we describe the experimental setup
and datasets used in our experiments, followed
by the evaluation of our methods. We evaluate
Topic SVM for binary authorship attribution, and
LDA+Hellinger on a binary dataset, a dataset with
tens of authors, and a dataset with thousands of au-
thors. Our results show that LDA+Hellinger yields
a higher accuracy than Koppel et al?s (2011) base-
line method in several cases where prolific authors
are considered, while requiring less runtime.
4.1 Experimental Setup
In all the experiments, we perform ten-fold cross
validation, employing stratified sampling where pos-
sible. The results are evaluated using classification
accuracy, i.e., the percentage of test documents that
were correctly assigned to their author. Note that
we use different accuracy ranges in the figures that
present our results for clarity of presentation. Sta-
tistically significant differences are reported when
p < 0.05 according to a paired two-tailed t-test.
We used the LDA implementation from Ling-
Pipe (alias-i.com/lingpipe) and the SVM im-
plementation from Weka (www.cs.waikato.ac.
nz/ml/weka). Since our focus is on testing the
impact of LDA, we used a linear SVM kernel and
the default SVM settings. For the LDA param-
eters, we followed Griffiths and Steyvers (2004)
and the recommendations in LingPipe?s documenta-
tion, and set the Dirichlet hyperparameters to ? =
min(0.1, 50/T ) and ? = 0.01, varying only the
number of topics T . We ran the Gibbs sampling
process for S = 1000 iterations, and based the doc-
ument representations on the last sample. While
taking more than one sample is generally consid-
ered good practice (Steyvers and Griffiths, 2007),
we found that the impact of taking several samples
on accuracy is minimal, but it substantially increases
the runtime. Hence, we decided to use only one sam-
ple in our experiments.
4.2 Datasets
We considered three datasets that cover different
writing styles and settings: Judgement, IMDb62 and
Blog. Table 1 shows a summary of these datasets.
The Judgement dataset contains judgements by
three judges who served on the Australian High
184
Judgement IMDb62 Blog
Authors 3 62 19,320
Texts 1,342 62,000 678,161
Texts per
Author
Dixon: 902
McTiernan: 253
Rich: 187
1,000
Mean: 35.10
Stddev.: 104.99
Table 1: Dataset Statistics
Court from 1913 to 1975: Dixon, McTiernan and
Rich (available for download from www.csse.
monash.edu.au/research/umnl/data). In
this paper, we considered the Dixon/McTiernan and
the Dixon/Rich binary classification cases, using
judgements from non-overlapping periods (Dixon?s
1929?1964 judgements, McTiernan?s 1965?1975,
and Rich?s 1913?1928). We removed numbers from
the texts to ensure that dates could not be used to dis-
criminate between judges. We also removed quotes
to ensure that the classifiers take into account only
the actual author?s language use.5 Employing this
dataset in our experiments allows us to test our meth-
ods on formal texts with a minimal amount of noise.
The IMDb62 dataset contains 62,000 movie re-
views by 62 prolific users of the Internet Movie
database (IMDb, www.imdb.com, available upon
request from the authors of (Seroussi et al, 2010)).
Each user wrote 1,000 reviews. This dataset is nois-
ier than the Judgement dataset, since it may con-
tain spelling and grammatical errors, and the reviews
are not as professionally edited as judgements. This
dataset alows us to test our approach in a setting
where all the texts have similar themes, and the num-
ber of authors is relatively small, but is already much
larger than the number of authors considered in tra-
ditional authorship attribution settings.
The Blog dataset is the largest dataset we consid-
ered, containing 678,161 blog posts by 19,320 au-
thors (Schler et al, 2006) (available for download
from u.cs.biu.ac.il/?koppel). In contrast to
IMDb reviews, blog posts can be about any topic,
but the large number of authors ensures that every
topic is likely to interest at least some authors. Kop-
pel et al (2011) used a different blog dataset con-
sisting of 10,240 authors in their work on authorship
5We removed numbers and quotes by matching regular ex-
pressions for numbers and text in quotation marks, respectively.
attribution with many candidate authors. Unfortu-
nately, their dataset is not publicly available. How-
ever, authorship attribution is more challenging on
the dataset we used, because they imposed some re-
strictions on their dataset, such as setting a minimal
number of words per author, and truncating the train-
ing and testing texts so that they all have the same
length. The dataset we use has no such restrictions.
4.3 LDA in Binary Authorship Attribution
In this section, we present the results of our experi-
ments with the Judgement dataset (Section 4.2), test-
ing the use of LDA in producing feature vectors for
SVMs and the performance of our LDA+Hellinger
methods (Section 3.2).
In all the experiments, we employed a classifier
ensemble to address the class imbalance problem
present in the Judgement dataset, which contains 5
times more texts by Dixon than by Rich, and over 3
times more texts by Dixon than by McTiernan (Ta-
ble 1). Dixon?s texts are randomly split into 5 or
3 subsets, depending on the other author (Rich or
McTiernan respectively), and the base classifiers are
trained on each subset of Dixon?s texts together with
all the texts by the other judge. Given a text by an
unknown author, the classifier outputs are combined
using majority voting. We found that the accuracies
obtained with an ensemble are higher than those ob-
tained with a single classifier. We did not require
the vote to be unanimous, even though this increases
precision, because we wanted to ensure full cover-
age of the test dataset. This enables us to compare
different methods using only an accuracy measure.6
Experiment 1. Figure 1 shows the results of an
experiment that compares the accuracy obtained us-
ing SVMs with token frequencies as features (Token
SVMs) with that obtained using LDA topic distribu-
tions as features (Topic SVMs). We experimented
with several filters on token frequency, and differ-
ent numbers of LDA topics (5, 10, 25, 50, . . ., 250).
The x-axis labels describe the frequency filters: the
minimum and maximum token frequencies, and the
approximate number of unique tokens left after fil-
tering (in thousands). We present only the results
obtained with 10, 25, 100 and 200 topics, as the re-
6For all our experiments, the results for the Dixon/McTier-
nan case are comparable to those for Dixon/Rich. Therefore,
we omit the Dixon/McTiernan results to conserve space.
185
 45
 50
 55
 60
 65
 70
 75
 80
 85
 90
 95
 100
0
1E-5
9.2
0
5E-5
12.2
0
1E-4
13
0
5E-4
13.8
0
1
14
1E-5
5E-5
3
1E-5
1E-4
3.8
1E-5
5E-4
4.6
1E-5
1
4.8
5E-5
1E-4
0.7
5E-5
5E-4
1.5
5E-5
1
1.7
1E-4
5E-4
0.8
1E-4
1
1
5E-4
1
0.2
Ac
cur
acy
Token Frequency Filter
Min
Max
Tokens (K)
Token SVM
Majority Baseline
10 Topic SVM
25 Topic SVM
100 Topic SVM
200 Topic SVM
Figure 1: LDA Features for SVMs in Binary Authorship
Attribution (Judgement dataset, Dixon/Rich)
sults obtained with other topic numbers are consis-
tent with the presented results, and the results ob-
tained with 225 and 250 topics are comparable to
the results obtained with 200 topics.
Our results show that setting a maximum bound
on token frequency filters out important authorship
markers, regardless of whether LDA is used or
not (performance drops). This shows that it is un-
likely that discriminative LDA topics correspond to
actual topics, as the most frequent tokens are mostly
non-topical (e.g., punctuation and function words).
An additional conclusion is that using LDA for
feature reduction yields results that are comparable
to those obtained using tokens directly. While Topic
SVMs seem to perform slightly better than Token
SVMs, the differences between the best results ob-
tained with the two approaches are not statistically
significant. However, the number of features that
the SVMs consider when topics are used is usually
much smaller than when tokens are used directly, es-
pecially when no token filters are used (i.e., when
the minimum frequency is 0 and the maximum fre-
quency is 1). This makes it easy to apply LDA to dif-
ferent datasets, since the token filtering parameters
may be domain-dependent, and LDA yields good re-
sults without filtering tokens.
Experiment 2. Figure 2 shows the results of
an experiment that compares the performance of
the single profile document (LDAH-S) and multi-
ple author documents (LDAH-M) variants of our
LDA+Hellinger approach to the results obtained
with Token SVMs and Topic SVMs. As in Exper-
iment 1, we employ classifier ensembles, where the
 45
 50
 55
 60
 65
 70
 75
 80
 85
 90
 95
 100
 0  25  50  75  100  125  150  175  200
Ac
cur
acy
Number of Topics
Token SVM
Majority Baseline
Topic SVM
LDAH-S
LDAH-M
Figure 2: LDA+Hellinger in Binary Authorship Attribu-
tion (Judgement dataset, Dixon/Rich)
base classifiers are either SVMs or LDA+Hellinger
classifiers. We did not filter tokens, since Experi-
ment 1 indicates that filtering has no advantage over
not filtering tokens. Instead, Figure 2 presents the
accuracy as a function of the number of topics.
Note that we did not expect LDA+Hellinger to
outperform SVMs, since LDA+Hellinger does not
consider inter-class relationships. Indeed, Figure 2
shows that this is the case (the differences between
the best Topic SVM results and the best LDAH-
M results are statistically significant). However,
LDA+Hellinger still delivers results that are much
better than the majority baseline (the differences be-
tween LDA+Hellinger and the majority baseline are
statistically significant). This leads us to hypothe-
sise that LDA+Hellinger will perform well in cases
where it is problematic to use SVMs due to the large
number of candidate authors. We verify this hypoth-
esis in the following sections.
One notable result is that LDAH-S delivers high
accuracy even when only a few topics are used,
while LDAH-M requires about 50 topics to outper-
form LDAH-S (all the differences between LDAH-S
and LDAH-M are statistically significant). This may
be because there are only two authors, so LDAH-
S builds the LDA model based only on two profile
documents. Hence, even 5 topics are enough to ob-
tain two topic distributions that are sufficiently dif-
ferent to discriminate the authors? test documents.
The reason LDAH-M outperforms LDAH-S when
more topics are considered may be that some impor-
tant authorship markers lose their prominence in the
profile documents created by LDAH-S.
186
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  50  100  150  200  250  300  350  400
Ac
cur
acy
Number of Topics
KOP: k1 = 400, k2 = 0.2LDAH-S
LDAH-M
Figure 3: LDA+Hellinger with Tens of Authors (IMDb62
dataset)
4.4 LDA+Hellinger with Tens of Authors
In this section, we apply our LDA+Hellinger ap-
proaches to the IMDb62 dataset (Section 4.2), and
compare the obtained results to those obtained with
Koppel et al?s (2011) method (KOP). To this effect,
we first established a KOP best-performance base-
line by performing parameter tuning experiments for
KOP. Figure 3 shows the results of the comparison
of the accuracies obtained with our LDA+Hellinger
methods to the best accuracy yielded by KOP (ob-
tained in the parameter tuning experiment).
For this experiment, we ran our LDA+Hellinger
variants with 5, 10, 25, 50, . . ., 300, 350 and 400
topics. The highest LDAH-M accuracy was ob-
tained with 300 topics (Figure 3). However, LDAH-
S yielded a much higher accuracy than LDAH-M.
This may be because the large number of training
texts per author (900) may be too noisy for LDAH-
M. That is, the differences between individual texts
by each author may be too large to yield a meaning-
ful representation of the author if they are considered
separately. Finally, LDAH-S requires only 50 topics
to outperform KOP, and outperforms KOP by about
15% for 150 topics. All the differences between the
methods are statistically significant.
This experiment shows that LDAH-S models the
authors in IMDb62 more accurately than KOP. The
large improvement in accuracy shows that the com-
pact author representation employed by LDAH-S,
which requires only 150 topics to obtain the highest
accuracy, has more power to discriminate between
authors than KOP?s much heavier representation, of
400 subsets with more than 30,000 features each. In
addition, the per-fold runtime of the KOP baseline
was 93 hours, while LDAH-S required only 15 hours
per fold to obtain the highest accuracy.
4.5 LDA+Hellinger with Thousands of Authors
In this section, we compare the performance of our
LDA+Hellinger variants to the performance of KOP
on several subsets of the Blog dataset (Section 4.2).
For this purpose, we split the dataset according to
the prolificness of the authors, i.e., we ordered the
authors by the number of blog posts, and considered
subsets that contain all the posts by the 1000, 2000,
5000 and 19320 most prolific authors.7 Due to the
large number of posts, we could not run KOP for
more than k1 = 10 iterations on the smallest subset
of the dataset and 5 iterations on the other subsets,
as the runtime was prohibitive for more iterations.
For example, 10 iterations on the smallest subset re-
quired about 90 hours per fold (the LDA+Hellinger
runtimes were substantially shorter, with maximum
runtimes of 56 hours for LDAH-S and 77 hours for
LDAH-M, when 200 topics were considered). Inter-
estingly, running KOP for 5 iterations on the larger
subsets decreased performance compared to running
it for 1 iteration. Thus, on the larger subsets, the
most accurate KOP results took less time to obtain
than those of our LDA+Hellinger variants.
Figure 4 shows the results of this experiment.
For each author subset, it compares the results ob-
tained by LDAH-S and LDAH-M to the best re-
sult obtained by KOP. All the differences between
the methods are statistically significant. For up to
2000 prolific authors (Figures 4(a), 4(b)), LDAH-S
outperforms KOP by up to 50%. For 5000 prolific
users (figure omitted due to space limitations), the
methods perform comparably, and KOP outperforms
LDAH-S by a small margin. However, with all the
authors (Figure 4(c)), KOP yields a higher accuracy
than both LDA+Hellinger variants. This may be
because considering non-prolific authors introduces
noise that results in an LDA model that does not cap-
ture the differences between authors. However, it is
encouraging that LDAH-S outperforms KOP when
less than 5000 prolific authors are considered.
7These authors make up about 5%, 10%, 25% and exactly
100% of the authors, but they wrote about 50%, 65%, 80% and
exactly 100% of the texts, respectively.
187
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 0  25  50  75  100  125  150  175  200
Acc
urac
y
Number of Topics
KOP: k1 = 10, k2 = 0.6LDAH-SLDAH-M
(a) 1,000 Prolific Authors
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 0  25  50  75  100  125  150  175  200
Acc
urac
y
Number of Topics
KOP: k1 = 1, k2 = 1.0LDAH-SLDAH-M
(b) 2,000 Prolific Authors
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 0  25  50  75  100  125  150  175  200
Acc
urac
y
Number of Topics
KOP: k1 = 1, k2 = 1.0LDAH-SLDAH-M
(c) 19,320 (all) Authors
Figure 4: LDA+Hellinger with Thousands of Authors (Blog dataset)
The accuracies obtained in this section are rather
low compared to those obtained in the previous
sections. This is not surprising, since the author-
ship attribution problem is much more challenging
with thousands of candidate authors. This chal-
lenge motivated the introduction of the ?? thresh-
old in KOP (Section 3.1). Our LDA+Hellinger vari-
ants can also be extended to include a threshold: if
the Hellinger distance of the best-matching author is
greater than the threshold, the LDA+Hellinger algo-
rithm would return ?unknown author?. We leave ex-
periments with this extension to future work, as our
focus in this paper is on comparing LDA+Hellinger
to KOP, and we believe that this comparison is
clearer when no thresholds are used.
5 Conclusions and Future Work
In this paper, we introduced an approach to author-
ship attribution that models texts and authors using
Latent Dirichlet Allocation (LDA), and considers
the distance between the LDA-based representations
of the training and test texts when classifying test
texts. We showed that our approach yields state-of-
the-art performance in terms of classification accu-
racy when tens or a few thousand authors are consid-
ered, and prolific authors exist in the training data.
This accuracy improvement was achieved together
with a substantial reduction in runtime compared to
Koppel et al?s (2011) baseline method.
While we found that our approach performs well
on texts by prolific authors, there is still room for
improvement on authors who have not written many
texts ? an issue that we will address in the future.
One approach that may improve performance on
such authors involves considering other types of fea-
tures than tokens, such as parts of speech and char-
acter n-grams. Since our approach is based on LDA,
it can easily employ different feature types, which
makes this a straightforward extension to the work
presented in this paper.
In the future, we also plan to explore ways of ex-
tending LDA to model authors directly, rather than
using it as a black box. Authors were considered by
Rosen-Zvi et al (2004; 2010), who extended LDA
to form an author-topic model. However, this model
was not used for authorship attribution, and was
mostly aimed at topic modelling of multi-authored
texts, such as research papers.
Another possible research direction is to improve
the scalability of our methods. Our approach, like
Koppel et al?s (2011) baseline, requires linear time
in the number of possible authors to classify a single
document. One possible way of reducing the time
needed for prediction is by employing a hierarchi-
cal approach that builds a tree of classifiers based on
class similarity, as done by Bickerstaffe and Zuker-
man (2010) for the sentiment analysis task. Under
this framework, class similarity (in our case, author
similarity) can be measured using LDA, while small
groups of classes can be discriminated using SVMs.
In addition to authorship attribution, we plan to
employ text-based author models in user modelling
tasks such as rating prediction ? a direction that we
already started working on by successfully applying
our LDA-based approach to model users for the rat-
ing prediction task (Seroussi et al, 2011).
Acknowledgements
This research was supported in part by grant
LP0883416 from the Australian Research Council.
The authors thank Russell Smyth for the collabora-
tion on initial results on the judgement dataset.
188
References
Adrian Bickerstaffe and Ingrid Zukerman. 2010. A hier-
archical classifier applied to multi-way sentiment de-
tection. In COLING 2010: Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 62?70, Beijing, China.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3(Jan):993?1022.
Haibin Cheng, Pang-Ning Tan, and Rong Jin. 2007. Lo-
calized support vector machine and its efficient algo-
rithm. In SDM 2007: Proceedings of the 7th SIAM
International Conference on Data Mining, pages 461?
466, Minneapolis, MN, USA.
Michael Gamon. 2004. Linguistic correlates of style:
Authorship classification with deep linguistic analysis
features. In COLING 2004: Proceedings of the 20th
International Conference on Computational Linguis-
tics, pages 611?617, Geneva, Switzerland.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl. 1):5228?5235.
Patrick Juola. 2006. Authorship attribution. Founda-
tions and Trends in Information Retrieval, 1(3):233?
334.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2011. Authorship attribution in the wild. Language
Resources and Evaluation, 45(1):83?94.
Kim Luyckx and Walter Daelemans. 2008. Authorship
attribution and verification with many authors and lim-
ited data. In COLING 2008: Proceedings of the 22nd
International Conference on Computational Linguis-
tics, pages 513?520, Manchester, UK.
David Madigan, Alexander Genkin, David D. Lewis,
Shlomo Argamon, Dmitriy Fradkin, and Li Ye. 2005.
Author identification on the large scale. In Proceed-
ings of the Joint Annual Meeting of the Interface and
the Classification Society of North America, St. Louis,
MO, USA.
Thomas C. Mendenhall. 1887. The characteristic curves
of composition. Science, 9(214S):237?246.
Frederick Mosteller and David L. Wallace. 1964. In-
ference and Disputed Authorship: The Federalist.
Addison-Wesley.
Arun Rajkumar, Saradha Ravi, Venkatasubramanian
Suresh, M. Narasimha Murthy, and C. E. Veni Mad-
havan. 2009. Stopwords and stylometry: A latent
Dirichlet alocation approach. In Proceedings of the
NIPS 2009 Workshop on Applications for Topic Mod-
els: Text and Beyond (Poster Session), Whistler, BC,
Canada.
Bhavani Raskutti and Adam Kowalczyk. 2004. Extreme
re-balancing for SVMs: A case study. ACM SIGKDD
Explorations Newsletter, 6(1):60?69.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense of
one-vs-all classification. Journal of Machine Learning
Research, 5(Jan):101?141.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and
Padhraic Smyth. 2004. The author-topic model for
authors and documents. In UAI 2004: Proceedings of
the 20th Conference on Uncertainty in Artificial Intel-
ligence, pages 487?494, Banff, AB, Canada.
Michal Rosen-Zvi, Chaitanya Chemudugunta, Thomas
Griffiths, Padhraic Smyth, and Mark Steyvers. 2010.
Learning author-topic models from text corpora. ACM
Transactions on Information Systems, 28(1):1?38.
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and
James W. Pennebaker. 2006. Effects of age and gen-
der on blogging. In Proceedings of AAAI Spring Sym-
posium on Computational Approaches for Analyzing
Weblogs, pages 199?205, Stanford, CA, USA.
Yanir Seroussi, Ingrid Zukerman, and Fabian Bohnert.
2010. Collaborative inference of sentiments from
texts. In UMAP 2010: Proceedings of the 18th In-
ternational Conference on User Modeling, Adaptation
and Personalization, pages 195?206, Waikoloa, HI,
USA.
Yanir Seroussi, Fabian Bohnert, and Ingrid Zukerman.
2011. Personalised rating prediction for new users us-
ing latent factor models. In Hypertext 2011: Proceed-
ings of the 22nd ACM Conference on Hypertext and
Hypermedia, Eindhoven, The Netherlands.
Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. Journal of the Ameri-
can Society for Information Science and Technology,
60(3):538?556.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. In Thomas K. Landauer, Danielle S.
McNamara, Simon Dennis, and Walter Kintsch, ed-
itors, Handbook of Latent Semantic Analysis, pages
427?448. Lawrence Erlbaum Associates.
189
