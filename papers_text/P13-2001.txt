Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1?6,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Translating Dialectal Arabic to English
Hassan Sajjad, Kareem Darwish
Qatar Computing Research Institute
Qatar Foundation
{hsajjad,kdarwish}@qf.org.qa
Yonatan Belinkov
CSAIL
Massachusetts Institute of Technology
belinkov@mit.edu
Abstract
We present a dialectal Egyptian Arabic
to English statistical machine translation
system that leverages dialectal to Modern
Standard Arabic (MSA) adaptation. In
contrast to previous work, we first nar-
row down the gap between Egyptian and
MSA by applying an automatic character-
level transformational model that changes
Egyptian to EG?, which looks simi-
lar to MSA. The transformations include
morphological, phonological and spelling
changes. The transformation reduces
the out-of-vocabulary (OOV) words from
5.2% to 2.6% and gives a gain of 1.87
BLEU points. Further, adapting large
MSA/English parallel data increases the
lexical coverage, reduces OOVs to 0.7%
and leads to an absolute BLEU improve-
ment of 2.73 points.
1 Introduction
Modern Standard Arabic (MSA) is the lingua
franca for the Arab world. Arabic speakers gen-
erally use dialects in daily interactions. There are
6 dominant dialects, namely Egyptian, Moroccan,
Levantine, Iraqi, Gulf, and Yemeni1. The dialects
may differ in vocabulary, morphology, syntax, and
spelling from MSA and most lack spelling con-
ventions.
Different dialects often make different lexical
choices to express concepts. For example, the con-
cept corresponding to ?Oryd? YK
P

@ (?I want?) is
expressed as ?EAwz? 	P?A? in Egyptian, ?Abgy?
?


	?K. @ in Gulf, ?Aby? ?
G. @ in Iraqi, and ?bdy? ?
 YK.
in Levantine2. Often, words have different or op-
posite meanings in different dialects.
1http://en.wikipedia.org/wiki/
Varieties_of_Arabic
2All transliterations follow the Buckwalter scheme
Arabic dialects may differ morphologically
from MSA. For example, Egyptian Arabic uses a
negation construct similar to the French ?ne pas?
negation construct. The Egyptian word ?mlEbt$?
?

J.??? (or alternatively spelled ?J.??A?) (?I did
not play?) is composed of ?m+lEbt+$?.
The pronunciations of letters often differ from
one dialect to another. For example, the letter ?q?

? is typically pronounced in MSA as an unvoiced
uvular stop (as the ?q? in ?quote?), but as a glot-
tal stop in Egyptian and Levantine (like ?A? in
?Alpine?) and a voiced velar stop in the Gulf (like
?g? in ?gavel?). Differing pronunciations often re-
flect on spelling.
Social media platforms allowed people to ex-
press themselves more freely in writing. Although
MSA is used in formal writing, dialects are in-
creasingly being used on social media sites. Some
notable trends on social platforms include (Dar-
wish et al, 2012):
- Mixed language texts where bilingual (or mul-
tilingual) users code switch between Arabic and
English (or Arabic and French). In the exam-
ple ?wSlny mrsy? ?


??Q? ?


	
???? (?got it thank
you?), ?thank you? is the transliterated French
word ?merci?.
? The use of phonetic transcription to match di-
alectal pronunciation. For example, ?Sdq? ?Y?
(?truth?) is often written as ?Sj? l .? in Gulf di-
alect.
? Creative spellings, spelling mistakes, and word
elongations are ubiquitous in social texts.
? The use of new words like ?lol? ??? (?LOL?).
? The attachment of new meanings to words such
as using ?THn? 	?j? to mean ?very? while it
means ?grinding? in MSA.
The Egyptian dialect has the largest number of
speakers and is the most commonly understood di-
alect in the Arab world. In this work, we focused
on translating dialectal Egyptian to English us-1
ing Egyptian to MSA adaptation. Unlike previous
work, we first narrowed the gap between Egyptian
and MSA using character-level transformations
and word n-gram models that handle spelling mis-
takes, phonological variations, and morphological
transformations. Later, we applied an adaptation
method to incorporate MSA/English parallel data.
The contributions of this paper are as follows:
? We trained an Egyptian/MSA transformation
model to make Egyptian look similar to MSA. We
publicly released the training data.
? We built a phrasal Machine Translation (MT)
system on adapted Egyptian/English parallel data,
which outperformed a non-adapted baseline by
1.87 BLEU points.
? We used phrase-table merging (Nakov and Ng,
2009) to utilize MSA/English parallel data with
the available in-domain parallel data.
2 Previous Work
Our work is related to research on MT from a re-
source poor language (to other languages) by piv-
oting on a closely related resource rich language.
This can be done by either translating between
the related languages using word-level translation,
character level transformations, and language spe-
cific rules (Durrani et al, 2010; Hajic? et al, 2000;
Nakov and Tiedemann, 2012), or by concatenating
the parallel data for both languages (Nakov and
Ng, 2009). These translation methods generally
require parallel data, for which hardly any exists
between dialects and MSA. Instead of translating
between a dialect and MSA, we tried to narrow
down the lexical, morphological and phonetic gap
between them using a character-level conversion
model, which we trained on a small set of parallel
dialect/MSA word pairs.
In the context of Arabic dialects3, most previous
work focused on converting dialects to MSA and
vice versa to improve the processing of dialects
(Sawaf, 2010; Chiang et al, 2006; Mohamed et
al., 2012; Utiyama and Isahara, 2008). Sawaf
(2010) proposed a dialect to MSA normalization
that used character-level rules and morphological
analysis. Salloum and Habash (2011) also used a
rule-based method to generate MSA paraphrases
of dialectal out-of-vocabulary (OOV) and low fre-
quency words. Instead of rules, we automatically
3Due to space limitations, we restrict discussion to work
on dialects only.
learnt character mappings from dialect/MSA word
pairs.
Zbib et al (2012) explored several methods for
dialect/English MT. Their best Egyptian/English
system was trained on dialect/English parallel
data. They used two language models built from
the English GigaWord corpus and from a large
web crawl. Their best system outperformed man-
ually translating Egyptian to MSA then translat-
ing using an MSA/English system. In contrast, we
showed that training on in-domain dialectal data
irrespective of its small size is better than training
on large MSA/English data. Our LM experiments
also affirmed the importance of in-domain English
LMs. We also showed that a conversion does not
imply a straight forward usage of MSA resources
and there is a need for adaptation which we ful-
filled using phrase-table merging (Nakov and Ng,
2009).
2.1 Baseline
We constructed baselines that were based on the
following training data:
- An Egyptian/English parallel corpus consist-
ing of ?38k sentences, which is part of the
LDC2012T09 corpus (Zbib et al, 2012). We ran-
domly divided it into 32k sentences for training,
2k for development and 4k for testing. We hence-
forth refer to this corpus as EG and the English
part of it as EGen. We did not have access to the
training/test splits of Zbib et al (2012) to directly
compare to their results.
- An MSA/English parallel corpus consisting of
200k sentences from LDC4. We refer to this cor-
pus as the AR corpus.
For language modeling, we used either EGen
or the English side of the AR corpus plus the En-
glish side of NIST12 training data and English Gi-
gaWord v5. We refer to this corpus as GW.
We tokenized Egyptian and Arabic accord-
ing to the ATB tokenization scheme using the
MADA+TOKAN morphological analyzer and to-
kenizer v3.1 (Roth et al, 2008). Word elonga-
tions were already fixed in the corpus. We word-
aligned the parallel data using GIZA++ (Och and
Ney, 2003), and symmetrized the alignments using
grow-diag-final-and heuristic (Koehn et al, 2003).
We trained a phrasal MT system (Koehn et al,
2003). We built five-gram LMs using KenLM
4Arabic News (LDC2004T17), eTIRR (LDC2004E72),
and parallel corpora the GALE program2
Train LM BLEU OOV
B1 AR GW 7.48 6.7
B2 EG GW 12.82 5.2
B3 EG EGen 13.94 5.2
B4 EG EGenGW 14.23 5.2
Table 1: Baseline results using the EG and AR
training sets with GW and EGen corpora for LM
training
with modified Kneser-Ney smoothing (Heafield,
2011). In case of more than one LM, we tuned
their weights on a development set using Mini-
mum Error Rate Training (Och and Ney, 2003).
We built several baseline systems as follows:
? B1 used AR for training a translation model and
GW for LM.
? B2-B4 systems used identical training data,
namely EG, with the GW, EGen, or both for B2,
B3, and B4 respectively for language modeling.
Table 1 reports the baseline results. The system
trained on AR (B1) performed poorly compared
to the one trained on EG (B2) with a 6.75 BLEU
points difference. This highlights the difference
between MSA and Egyptian. Using EG data for
training both the translation and language models
was effective. B4 used two LMs and yielded the
best results. For later comparison, we only use the
B4 baseline.
3 Proposed Methods
3.1 Egyptian to EG? Conversion
As mentioned previously, dialects differ from
MSA in vocabulary, morphology, and phonology.
Dialectal spelling often follows dialectal pronun-
ciation, and dialects lack standard spelling con-
ventions. To address the vocabulary problem, we
used the EG corpus for training.
To address the spelling and morphological dif-
ferences, we trained a character-level mapping
model to generate MSA words from dialectal
ones using character transformations. To train the
model, we extracted the most frequent words from
a dialectal Egyptian corpus, which had 12,527
news comments (containing 327k words) from Al-
Youm Al-Sabe news site (Zaidan and Callison-
Burch, 2011) and translated them to their equiv-
alent MSA words. We hired a professional trans-
lator, who generated one or more translations of
the most frequent 5,581 words into MSA. Out of
these word pairs, 4,162 involved character-level
transformations due to phonological, morphologi-
cal, or spelling changes. We aligned the translated
pairs at character level using GIZA++ and Moses
in the manner described in Section 2.1. As in the
baseline of Kahki et al (2011), given a source
word, we produced all of its possible segmenta-
tions along with their associated character-level
mappings. We restricted individual source char-
acter sequences to be 3 characters at most. We
retained all mapping sequences leading to valid
words in a large lexicon. We built the lexicon from
a set of 234,638 Aljazeera articles5 that span a 10
year period and contain 254M tokens. Spelling
mistakes in Aljazeera articles were very infre-
quent. We sorted the candidates by the product of
the constituent mapping probabilities and kept the
top 10 candidates. Then we used a trigram LM that
we built from the aforementioned Aljazeera arti-
cles to pick the most likely candidate in context.
We simply multiplied the character-level transfor-
mation probability with the LM probability ? giv-
ing them equal weight. Since Egyptian has a ?ne
pas? like negation construct that involves putting a
??? and ? ?? at the beginning and end of verbs,
we handled words that had negation by remov-
ing these two letters, then applying our character
transformation, and lastly adding the negation ar-
ticle ?lA? B before the verb. We converted theEG
train, tune, and test parts. We refer to the converted
corpus as EG?.
As an example, our system transformed
Yg ?. j. ?J
? ????jJ
K. ?


?

? @ ?. (?what is hap-
pening to them does not please anyone?) to
Yg I. j. ?K
 B ??? ??m
'

 ?

	
Y?@ ?. . Transform-
ing ?Ally? ?


?

? @ to ?Al*y? ?


	
Y?@ involved a spelling
correction. The transformation of ?byHSlhm?
????jJ
K. to ?yHSl lhm? ??? ??m'
 involved a mor-
phological change and word splitting. Chang-
ing ?myEjb$? ?. j. ?J
? to ?lA yEjb? I. j. ?K
 B in-
volved morphologically transforming a negation
construct.
3.2 Combining AR and EG?
The aforementioned conversion generated a lan-
guage that is close, but not identical, to MSA.
In order to maximize the gain using both paral-
lel corpora, we used the phrase merging technique
described in Nakov and Ng (2009) to merge the
phrase tables generated from theAR andEG? cor-
pora. If a phrase occurred in both phrase tables, we
5http://www.aljazeera.net3
adopted one of the following three solutions:
- Only added the phrase with its translations and
their probabilities from the AR phrase table. This
assumed AR alignments to be more reliable.
- Only added the phrase with its translations and
their probabilities from the EG? phrase table. This
assumed EG? alignments to be more reliable.
- Added translations of the phrase from both
phrase tables and left the choice to the decoder.
We added three additional features to the new
phrase table to avail the information about the ori-
gin of phrases (as in Nakov and Ng (2009)).
3.3 Evaluation and Discussion
We performed the following experiments:
- S0 involved translating the EG? test using AR.
- S1 and S2 trained on the EG? with EGen and
both EGen and GW for LM training respectively.
- S? used phrase merging technique. All systems
trained on both EG? and AR corpora. We built
separate phrase tables from the two corpora and
merged them. When merging, we preferred AR or
EG? for SAR and SEG? respectively. For SALL,
we kept phrases from both phrase tables.
Table 2 summarizes results of using EG? and
phrase table merging. S0 was slightly better than
B1, but lagged considerably behind training using
EG or EG?. S1, which used only EG? for train-
ing showed an improvement of 1.67 BLEU points
from the best baseline system (B4). Using both
language models (S2) led to slight improvement.
Phrase merging that preferred phrases learnt from
EG? data over AR data performed the best with a
BLEU score of 16.96.
Train LM BLEU OOV
B4 EG EGenGW 14.23 5.2
S0 AR EGen 8.61 2.0
S1 EG? EGen 15.90 2.6
S2 EG? EGenGW 16.10 2.6
SAR PTAR EGenGW 16.14 0.7
SEG? PTEG? EGenGW 16.96 0.7
SALL PTEG?,AR EGenGW 16.73 0.7
Table 2: Summary of results using different com-
binations of EG?/English and MSA/English train-
ing data
We analyzed 100 test sentences that led to the
greatest absolute change in BLEU score, whether
positive or negative, between training with EG
and EG?. The largest difference in BLEU was
0.69 in favor of EG?. Translating the Egyp-
tian sentence ?wbyHtrmwA AlnAs AltAnyp?

?J

	
K A

J? @ ?A
	
J? @ @??Q

jJ
K. ? produced ? @??QjJ
K. ? (OOV)
the second people? (BLEU = 0.31). Conver-
sion changed ?wbyHtrmwA? to ?wyHtrmwA? and
?AltAnyp? ?J
 	K AJ? @ to ?AlvAnyp? ?J
 	K AJ? @, leading to
?and they respect other people? (BLEU = 1).
Training with EG? outperformed EG for 63 of the
sentences. Conversion improved MT, because it
reduced OOVs, enabled MADA+TOKAN to suc-
cessfully analyze words, and reduced spelling mis-
takes.
In further analysis, we examined 1% of the sen-
tences with the largest difference in BLEU score.
Out of these, more than 70% were cases where the
EG? model achieved a higher BLEU score. For
each observed conversion error, we identified its
linguistic character, i.e. whether it is lexical, syn-
tactic, morphological or other. We found that in
more than half of the cases (?57%) using morpho-
logical information could have improved the con-
version. Consider the following example, where
(1) is the original EG sentence and its EG/EN
translation, and (2) is the converted EG? sentence
and its EG?/EN translation:
1. ?JJ. 	?P I. ?k ?
 X
	
?B
lAn dy Hsb rgbtk
because this is according to your desire
2. ?JJ. 	?P I. ?k ? 	Y? 	?

B
lOn h*h Hsb rgbth
because this is according to his desire
In this case, ?rgbtk? ?JJ. 	?P (?your wish?) was con-
verted to ?rgbth? ?JJ. 	?P (?his wish?) leading to an
unwanted change in the translation. This could be
avoided, for instance, by running a morphologi-
cal analyzer on the original and converted word,
and making sure their morphological features (in
this case, the person of the possessive) correspond.
In a similar case, the phrase ?mEndy$ AEdA?
Z @Y?@ ?
Y
	
J?? was converted to ?Endy OEdA??
Z @Y?@ ?


Y
	
J?, thereby changing the translation from
?I don?t have enemies? to ?I have enemies?. Here,
again, a morphological analyzer could verify the
retaining of negation after conversion.
In another sentence, ?knty? ?



?
	
J? (?you (fm.)
were?) was correctly converted to the MSA ?knt?
I
	
J?, which is used for feminine and masculine
forms. However, the induced ambiguity ended up
hurting translation.4
Aside from morphological mistakes, conversion
often changed words completely. In one sen-
tence, the word ?lbAnh? ? 	KAJ. ? (?chewing gum?)
was wrongly converted to ?lOnh? ? 	K B (?because
it?), resulting in a wrong translation. Perhaps a
morphological analyzer, or just a part-of-speech
tagger, could enforce (or probabilistically encour-
age) a match in parts of speech.
The conversion also faces some other chal-
lenges. Consider the following example:
1. ?J
K
 @ A 	J??? A 	Jk@ @??
hwA AHnA EmlnA Ayyyh
he is we did we What ? ?
2. ?K
 @ A 	J??? 	?m 	' ??
hw nHn EmlnA Ayh
he we did we do ? ?
While the first two words ?hwA AHnA? A 	Jk@ @??
were correctly converted to ?hw nHn? 	?m 	' ??, the
final word ?Ayyyh? ?J
K
 @ (?what?) was shortened
but remained dialectal ?Ayh? ?K
 @ rather than MSA
?mA/mA*A? A?/ @ 	XA?. There is a syntactic chal-
lenge in this sentence, since the Egyptian word or-
der in interrogative sentences is normally different
from the MSA word order: the interrogative par-
ticle appears at the end of the sentence instead of
at the beginning. Addressing this problem might
have improved translation.
The above analysis suggests that incorporat-
ing deeper linguistic information in the conversion
procedure could improve translation quality. In
particular, using a morphological analyzer seeems
like a promising possibility. One approach could
be to run a morphological analyzer for dialectal
Arabic (e.g. MADA-ARZ (Habash et al, 2013))
on the original EG sentence and another analyzer
for MSA (such as MADA) on the converted EG?
sentence, and then to compare the morphological
features. Discrepancies should be probabilistically
incorporated in the conversion. Exploring this ap-
proach is left for future work.
4 Conclusion
We presented an Egyptian to English MT system.
In contrast to previous work, we used an auto-
matic conversion method to map Egyptian close
to MSA. The converted Egyptian EG? had fewer
OOV words and spelling mistakes and improved
language handling. The MT system built on the
adapted parallel data showed an improvement of
1.87 BLEU points over our best baseline. Using
phrase table merging that combined AR and EG?
training data in a way that preferred adapted di-
alectal data yielded an extra 0.86 BLEU points.
We will make the training data for our conversion
system publicly available.
For future work, we want to expand our work
to other dialects, while utilizing dialectal morpho-
logical analysis to improve conversion. Also, we
believe that improving English language model-
ing to match the genre of the translated sentences
can have significant positive impact on translation
quality.
References
David Chiang, Mona T. Diab, Nizar Habash, Owen
Rambow, and Safiullah Shareef. 2006. Parsing Ara-
bic dialects. In Proceedings of the 11th Conference
of the European Chapter of the Association for Com-
putational Linguistics, Trento, Italy.
Kareem Darwish, Walid Magdy, and Ahmed Mourad.
2012. Language processing for Arabic microblog
retrieval. In Proceedings of the 21st ACM inter-
national conference on Information and knowledge
management, CIKM ?12, Maui, Hawaii, USA.
Nadir Durrani, Hassan Sajjad, Alexander Fraser, and
Helmut Schmid. 2010. Hindi-to-Urdu machine
translation through transliteration. In Proceedings
of the 48th Annual Conference of the Association for
Computational Linguistics, Uppsala, Sweden.
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-
kander, , and Nadi Tomeh. 2013. Morphological
analysis and disambiguation for dialectal Arabic. In
Proceedings of the Main Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Atlanta, US.
Jan Hajic?, Jan Hric, and Vladislav Kubon?. 2000. Ma-
chine translation of very close languages. In Pro-
ceedings of the sixth conference on Applied natural
language processing, Seattle, Washington.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, UK.
Ali El Kahki, Kareem Darwish, Ahmed Saad El
Din, Mohamed Abd El-Wahab, Ahmed Hefny, and
Waleed Ammar. 2011. Improved transliteration
mining using graph reinforcement. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, Edinburgh, UK.5
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology and North
American Association for Computational Linguis-
tics Conference, Edmonton, Canada.
Emad Mohamed, Behrang Mohit, and Kemal Oflazer.
2012. Transforming standard Arabic to colloquial
Arabic. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics, Short Paper, Jeju Island, Korea.
Preslav Nakov and Hwee Tou Ng. 2009. Improved
statistical machine translation for resource-poor lan-
guages using related resource-rich languages. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, Singa-
pore.
Preslav Nakov and Jo?rg Tiedemann. 2012. Combining
word-level and character-level models for machine
translation between closely-related languages. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics, Short Paper,
Jeju Island, Korea.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,
and Cynthia Rudin. 2008. Arabic morphological
tagging, diacritization, and lemmatization using lex-
eme models and feature ranking. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, Columbus, Ohio.
Wael Salloum and Nizar Habash. 2011. Dialectal
to standard Arabic paraphrasing to improve Arabic-
English statistical machine translation. In Proceed-
ings of the First Workshop on Algorithms and Re-
sources for Modelling of Dialects and Language Va-
rieties, Edinburgh, Scotland.
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the Confer-
ence of the Association for Machine Translation in
the Americas, Denver, Colorado.
Masao Utiyama and Hitoshi Isahara. 2008. A hybrid
approach for converting written Egyptian colloquial
dialect into diacritized Arabic. In Proceedings of
the 6th International Conference on Informatics and
Systems, Cairo University, Egypt.
Omar F. Zaidan and Chris Callison-Burch. 2011. The
Arabic online commentary dataset: an annotated
dataset of informal Arabic with high dialectal con-
tent. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies: short papers - Volume
2, Portland, Oregon.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-
Burch. 2012. Machine translation of Arabic di-
alects. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Montreal, Canada.
6
