Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 1?8,
Uppsala, Sweden, 15 July 2010.
c
?2010 Association for Computational Linguistics
Instance-based acquisition of vowel harmony
Fr
?
ed
?
eric Mailhot
Institute of Cognitive Science
Carleton University
Ottawa, ON, Canada
fmailhot@connect.carleton.ca
Abstract
I present LIBPHON, a nonparametric
regression-based model of phonologi-
cal acquisition that induces a gener-
alised and productive pattern of vowel
harmony?including opaque and transpar-
ent neutrality?on the basis of simplified
formant data. The model quickly learns to
generate harmonically correct morpholog-
ically complex forms to which it has not
been exposed.
1 Explaining phonological patterns
How do infants learn the phonetic categories
and phonotactic patterns of their native lan-
guages? How strong are the biases that learn-
ers bring to the task of phonological acquis-
tion? Phonologists from the rationalist tradition
that dominated the past half-century of linguis-
tic research typically posit strong biases in ac-
quisition, with language learners using innately-
given, domain-specific representations (Chomsky
and Halle, 1968), constraints (Prince and Smolen-
sky, 2004) and learning algorithms (Tesar and
Smolensky, 2000; Dresher, 1999) to learn abstract
rules or constraint rankings from which they can
classify or produce novel instances.
In the last decade, however, there has been a
shift toward empiricist approaches to phonologi-
cal acquisition, use and knowledge. In this liter-
ature, eager learning algorithms (Aha, 1997), in
which training data are used to update intensional
representations of functions or categories then dis-
carded, have been the norm.
1
However, research
in related fields?particularly speech perception?
indicates that speakers? knowledge and use of
language, both in production and comprehen-
sion, is at least partly episodic, or instance-based
(Goldinger, 1996; Johnson, 1997). Additionally,
1
Daelemans et al (1994) is a notable exception.
motivation for instance-based models of categori-
sation has a lengthy history in cognitive psychol-
ogy (Medin and Schaffer, 1978), and these meth-
ods are well-known in the statistical and machine
learning literature, having been studied for over
half a century (Fix and Hodges, 1951; Cover and
Hart, 1967; Hastie et al, 2009). Consequently, it
seems a worthy endeavour applying an instance-
based method to a problem that is of interest to
traditional phonologists, the acquisition and use
of vowel harmony, while simultaneously effecting
a rapprochement with adjacent disicplines in the
cognitive sciences. In sections 2 and 3 I give some
brief background on vowel harmony and instance-
based models, respectively. Section 4 introduces
my model, LIBPHON, and section 5 the languages
it learns. I discuss some simulations and results in
section 6, and conclude in section 7.
2 Vowel harmony
Vowel harmony is a phonological phenomenon in
which there are co-occurrence constraints on vow-
els within words.
2
The vowels in a language with
vowel harmony can be classified into disjoint sets,
such that words contain vowels from only one of
the sets. The Finnish system of vowel harmony ex-
emplified by the forms in Table 1 provides a stan-
dard example from the literature (van der Hulst
and van de Weijer, 1995).
surface form gloss
a. tuhmasta ?naughty? (elative)
b. tu?hma?sta? ?stupid? (elative)
Table 1: Finnish backness harmony
Crucially, the elative case marker alternates
systematically between front and back vowel
2
?Word? is used pre-theoretically here; harmony can oc-
cur over both supra- and sublexical domains.
1
variants?as -st?a or -sta?depending on whether
the stem has front {?u, ?a} or back {u, a} vowels.
2.1 Neutral vowels
In most languages with vowel harmony, there are
one or more vowels that systematically fail to
alternate. These are called neutral vowels, and
are typically further subclassified according to
whether or not they induce further harmonic al-
ternations in other vowels.
3 Instance-based models
Instance-based approaches to cognitive process-
ing, also called memory-based, case-based, and
exemplar-based models, have their modern origins
in psychological theories and models of percep-
tual categorisation and episodic memory (Medin
and Schaffer, 1978; Nosofsky, 1986), although
the earliest explicit discussion seems to be (Se-
mon, 1921); a theory of memory that anticipates
many features of contemporary models. The core
features of these models are: (i) explicit stor-
age/memorisation (viz. extensional representa-
tion) of training data, (ii) classification/processing
of novel data via similarity-based computation,
and (iii) lazy evaluation (Aha, 1997), whereby
all computations are deferred until the model is
queried with data.
3
Instance-based models were introduced to lin-
guistics via research in speech perception suggest-
ing that at least some aspects of linguistic perfor-
mance rely on remembered experiential episodes
(Johnson and Mullenix, 1997). The models imple-
mented to date in phonetics and phonology have
largely focused on perception (e.g. speaker nor-
malisation in Johnson (1997)), or on diachronic
processes (e.g. lenition in Pierrehumbert (2001),
chain shifts in Ettlinger (2007)), leaving the types
of phenomena that typically interest ?traditional?
phonologists, viz. productive, generalised pat-
terns, comparatively neglected.
4
4 LIBPHON
LIBPHON, the Lazy Instance-based Phonologist,
is a lazy learning algorithm whose purpose (in the
3
Compare eager learners, e.g. connectionist systems,
which build a global intensional representation of the func-
tion being learned on the basis of training data which are sub-
sequently discarded.
4
Kirchner and Moore (2009) give a model of a syn-
chronic lenition process, and Daelemans and colleagues
give memory-based analyses of several linguistic phenomena
(Daelemans and van den Bosch, 2005).
context of the simulations described here) is to
model an instance-based approach to the core as-
pects of the acquisition and subsequent productive
usage of vowel harmony.
4.1 Decisions & mechanisms
As discussed in (Johnson, 2007), there are some
decisions that need to be made in implementing an
instance-based model of phonological knowledge
involving the basic units of analysis (e.g. their
size), the relevant type of these units (e.g. discrete
or continuous), and the mechanisms for similarity-
matching and activation spread in the lexicon.
Units The arguments given by Johnson (2007)
and V?alimaa-Blum (2009) for the ?word-sized?
(rather than e.g. segmental) experience of lan-
guage, suggest that ?words? are the correct basic
unit of analysis in instance-based langugage mod-
els (a fortiori in LIBPHON). Stronger evidence
comes from the wealth of psycholinguistic data
(reviewed in (Lodge, 2009)) showing that illiter-
ates and literates of non-alphabetic writing sys-
tems have poor phonemic (or at least segmental)
awareness, both in monitoring and manipulation.
On this basis, I take meaning-bearing unanalysed
acoustic chunks to be the relevant units of repre-
sentation for LIBPHON.
5
Feature type Having determined the size of
LIBPHON?s basic unit, I move now to its em-
bedding space, where distinctive features present
themselves as obvious candidate dimensions.
Since the middle of the 20
th
century (ca. Chom-
sky and Halle (1968)), phonological theories have
nearly all supposed that lexical representations are
stored in terms of articulatory features (cf. (Halle,
1997) for explicit discussion of this viewpoint).
Coleman (1998), citing evidence from the neuro-
scientific and psycholinguistic literatures on lexi-
cal representation, claims that evidence for this po-
sition (e.g. from speech perception and phoneme
monitoring experiments) is weak at best, and that
lexical representations are more likely to be acous-
tic than articulatory. In addition, Phillips et al
(2000) review neurolinguistic evidence for the role
of acoustic cortex in phonetics and phonology, and
5
The assumption that word-level segmentation of the
speech signal is available to the language learner prior to ac-
quisition of phonological phenomena is relatively uncontro-
versial, although there is evidence for the development of at
least some phonotactic knowledge prior to the emergence of
a productive lexicon (Jusczyk, 1999).
2
Mielke (2008) discusses several aspects of the in-
duction of distinctive phonological features from
acoustic representations. Recognising that the is-
sue is far from resolved, for the purposes of the
simulations run here, I take LIBPHON?s instance
space to be acoustically-based, and use formant
values as the embedding dimension. Vowels are
specified by their midpoint formant values,
6
and
consonants are specified by so-called ?locus? val-
ues, which can be identified by inspecting the tra-
jectories of consonant-vowel transitions in speech
(Sussman et al, 1998). Since I am modelling
palatal harmony in particular, and F2 magnitude is
the primary acoustic correlate of vowel palatality, I
omit F3 and F4, restricting LIBPHON?s acoustic
representations to sequences of (F1, F2) values,
henceforth trajectories.
Similarity Given that LIBPHON?s instance-
space is continuous, and has a fairly intuitive
metric, I take simple Euclidean distance to be
LIBPHON?s similarity (or rather, dissimilarity)
function.
7
Fixed-rate representations For the simulations
described here, I use fixed-rate trajectories, in
which consonants and vowels are represented
in a temporally coarse-grained manner with sin-
gle (F1, F2) tuples. Evidently, consonants and
vowels in actual human speech unfold in time,
but modelling segments at this level introduces
the problem of temporal variability; repeated to-
kens of a given word?both within and across
speakers?vary widely in duration. This variabil-
ity is one of the main obstacles in the develop-
ment of instance-based models of speech produc-
tion, due to the difficulty of aligning variable-
length forms. Although algorithms exist for align-
ing variable-length sequences, these require cog-
nitively implausible dynamic programming al-
gorithms, e.g. dynamic time warping (DTW)
6
A reviewer asks about the psychological plausibility of
Hz-based formant represetations and the choice of point val-
ues for vowel and consonant representations, e.g. rather than
formant values at 20% and 80% of the vowel. These are
purely in the interests of simplicity for the work reported
here. As discussed below, future work with real speech ex-
emplars in psychophysically-motivated representational for-
mats, e.g. perceptual linear predictive coding (Hermansky,
1990), will render this issue moot.
7
Often the measure of similarity in an instance-based
model is an exponential function of distance, d(x
i
, x
j
) of the
form exp(?cd(x
i
, x
j
)), so that increasing distance yields de-
creasing similarity (Nosofsky, 1986). The Euclidean measure
here is sufficient for the purpose at hand, although the shape
of the similarity measure is ultimately an empirical question.
and hidden Markov models (Rabiner and Juang,
1993). Even as proofs of concept, these may
be empirically inadequate; Kirchner and Moore
(2009) use DTW to good effect in an instance-
based production model of spirantisation using
real, temporally variable, speech signals. How-
ever, their inputs were all the same length in terms
of segmental content, and the model was only re-
quired to generalise within a word type. I am
currently investigating whether DTW can func-
tion as a proof of concept in a problem domain
like that addressed here, which involves learn-
ing about variably-sized ?pieces? of morphology
across class labels.
4.2 Perception/categorisation
LIBPHON?s method of perception/categorisation
of inputs is a relatively standard nearest-
neighbour-based classification algorithm. See Al-
gorithm 1 for a description in pseudocode.
Algorithm 1 PERCEIVE(input, k)
Require: input as (LABEL ? [LEX](PL)[NOM |
ACC], instance ? Z
2x{8,10,12}
), k ? Z
if LABEL is not empty then
if LABEL /? lexicon then
Create LABEL in lexicon
end if
Associate(instance, LABEL)
else
neighbours ? k-nearest neighbours of
instance
LABEL ? majority class label of
neighbours
Associate(instance,LABEL)
end if
If LABEL is not empty, LIBPHON checks its lex-
icon to see whether it knows the word being pre-
sented to it, i.e. whether it exists as a class label.
If so, it simply appends the input acoustic form to
the set of forms associated with the input mean-
ing/label. If it has no corresponding entry, a new
lexical entry is created for the input meaning, and
the input trajectory is added as its sole associated
acoustic form.
If LABEL is empty, LIBPHON assigns
instance to the majority class of its k
nearest neighbours in acoustic space.
3
4.3 Production
In production, LIBPHON is provided with a LA-
BEL and has to generate a suitable instance for
it. LABELs are decomposable, signalling an ar-
bitrary ?lexical? meaning, an optional plural mor-
pheme, PL, and an obligatory case marker from
{NOM, ACC}. Thus, there are several different
possibilities to consider in generating output for
some queried meaning.
In the two simplest cases, either the full queried
meaning (viz. lexical label with all inflections)
is already in the lexicon, or else there are no
class LABELs with the same lexical meaning (i.e.
LIBPHON is being asked to produce a word that it
doesn?t know). In the former case, a stored trajec-
tory is uniform
8
randomly selected from the list of
acoustic forms associated with the queried label as
a seed token, the entire set of associated acoustic
forms is used as the analogical set, and an output
is generated by taking a distance-weighted mean
over the seed?s k nearest neighbours.
9
In the case
where the lexical meaning of the queried LABEL
is unknown, the query is ignored.
In the more interesting cases, LIBPHON has a
LABEL in its lexicon with the same lexical mean-
ing, but with differing inflectional specification.
Consider the case in which LIBPHON knows only
the singular NOM form of a query label that is
specified as PL ACC. A seed instance is (uni-
form) randomly selected from the set of trajec-
tories associated to the NOM entry in the agent?s
lexicon, as this is the only entry with the corre-
sponding lexical meaning, and it is a variant of this
meaning that LIBPHON must produce. In this case
the analogical set, the set of instances from which
the final output is computed, is composed of the
seed?s nearest neighbours in the set of all trajec-
tories associated with LABELs of the form [LEX
PL ACC]. Once again, the output produced is a
distance-weighted mean of the analogical set.
This general procedure (viz. seed from a known
item with same lexical meaning, analogical set
from all items with desired inflection) is carried
out in parallel cases with all other possible LA-
BEL mismatches, e.g. a singular LABEL queried,
8
Exemplar models often bias the selection of seed to-
kens with degrees of ?activation? that take into account re-
cency and frequency. Although the results discussed below
show that this is not necessary for ultimate attainment, it is
likely that this kind of bias will need to be incorporated into
LIBPHON to accurately model more nuanced aspects of the
acquisition path.
9
k = 5 for all results reported here.
but only a plural LABEL in the lexicon, a NOM
query with only an ACC form in the lexicon, etc.
In the cases where the lexicon contains multiple
entries with the same lexical meaning, but not the
query, the seed is selected from the LABEL with
the closest ?semantic? match. Algorithm 2 gives
pseudocode for LIBPHON?s production algorithm.
Algorithm 2 PRODUCE(LABEL, k)
Require: LABEL? [LEX](PL)[NOM | ACC], k ?Z
if LABEL ? lexicon then
seed ? uniform random selection from
instances associated to LABEL
cloud? all instances associated to LA-
BEL
else if ? LABEL
?
? lexicon s.t. lex(LABEL
?
) =
lex(LABEL) then
seed ? uniform random selection from
instances associated to LABEL
?
)
cloud ? all instances associated to
plural(LABEL) ? case(LABEL)
else
pass
end if
neighbours? k-nearest neighbours of seed in
cloud
return distance-weighted mean of neighbours
4.4 Production as regression
The final steps in LIBPHON?s production algo-
rithm, finding the analogical set and computing the
output as a weighted average, together constitute
a technique known in the statistical learning lit-
erature as kernel-smoothed nearest-neighbour re-
gression, and in particular are closely related to the
well-known Nadaraya-Watson estimator (Hastie et
al., 2009):
?
f(x) =
?
N
i=1
K
?
(x, x
i
)y
i
?
N
i=1
K
?
(x, x
i
)
with inverse-distance as the kernel smoother, K,
and the bandwidth function, h
?
(x) determined by
the number k of nearest neighbours. This link to
the statistical learning literature puts LIBPHON on
sound theoretical footing and opens the door to a
variety of future research paths, e.g. experiment-
ing with different kernel shapes, or formal analysis
of LIBPHON?s expected error bounds.
4
5 The languages
On the view taken here, phonological knowledge
is taken to emerge from generalisation over lexical
items, and so the key to acquiring some phono-
logical pattern lies in learning a lexicon (Jusczyk,
2000). Consequently, the languages learned in
LIBPHON abstract away from sentence-level phe-
nomena, and the training data are simply labelled
formant trajectories, (LABEL, instance).
In order to get at the essence of the problem (viz.
the acquisition of vowel harmony as characterised
by morphophonological alternations), and in the
interests of computational tractability/efficiency,
the artificial languages learned by LIBPHON are
highly simplified, displaying only enough struc-
ture to capture the phenomena of interest.
5.1 Phonological inventory
The phonological inventory consists of three con-
sonants, {b, d, g}, and four vowels?two with high
F2 and two with low F2?which I label {i, e, u,
o}, for convenience.
10
The formant values used
were generated from formant synthesis equations
in (de Boer, 2000), and from the locus equations
for CV-transitions in (Sussman et al, 1998).
5.2 Lexical items
LIBPHON?s lexicon is populated with instance
trajectories consisting of four-syllable
11
?roots?
with zero, one or two one-syllable ?affixes?. These
trajectories have associated class labels, which
from a formal point of view are contentless in-
dices. Rather than employing e.g. natural num-
bers as labels, I use character strings which corre-
spond more or less to the English pronounciations
of their associated trajectories. LABELs func-
tion, metaphorically-speaking, as ?meanings?.
These are compositional, comprising a ?lexical
meaning? (arbitrary CVCVCVCV string from the
phoneme set listed above), one of two obligato-
rily present ?case markers? (NOM|ACC), and an
optionally present ?plural marker? (PL). Hence,
word categories in the artificial languages come in
four forms, NOM-SG, NOM-PL, ACC-SG, and ACC-
PL.
10
Because LIBPHON?s representations lack F3, the pri-
mary acoustic correlate of rounding, the back vowels would
be more standardly represented as /7, W/, but these are com-
paratively uncommon IPA symbols, so we will use the sym-
bols for the rounded variants. Nothing in the results or dis-
cussion hinges on this.
11
All syllables are CV-shaped.
0 2 4 6 8 10 12
0
500
1000
1500
2000
2500
3000
GIDEGEBI NOM
gidegebi
F1
F2
0 2 4 6 8 10 12
0
500
1000
1500
2000
2500
3000
GIDEGEBI ACC
gidegebibe
F1
F2
Figure 1: Graphical representation of singular
forms of GIDEGEBI, as produced by teacher agent
Figure 1 gives examples of the singular NOM
and ACC forms of a high-F2 word. The NOM-
labelled trajectory has no suffixal morphology, and
corresponds to a bare form. The trajectory is eight
segments long,
12
and the vowels in this case have
all high F2 (as in lexical front/back vowel har-
mony).
13
Note also that ACC is realised with high
F2, in agreement with the root vowels.
5.3 Neutral vowels
The harmony processes seen thus far are in some
sense ?local?, being describable in terms of vowel
adjacency e.g. adjacency on a hypothesised au-
tosegmental tier (although the presence of inter-
vening consonants still renders the harmony pro-
cess ?nonlocal? in some more concrete articula-
tory sense). One of the hallmarks of vowel har-
mony, as discussed in subsection 2.1, is the phe-
nomenon of neutral vowels. These vowels fail to
alternate, and may or may not induce harmonic al-
ternations in vowels that precede or follow them.
To introduce a neutral vowel, I added a category
label, PL, whose realisation corresponds roughly
to [gu], and which is treated as being either opaque
or transparent in the simulations described below.
Figures 2 and 3 show the ?plural inflected?
forms of the same root as in 1. We see that the
12
The even-numbered indices on the x-axis correspond to
consonants and the odd-numbered indices, the ?pinches? in
the graphs, correspond to vowels.
13
The languages LIBPHON learns have only harmonic
singular forms. This is unrealistic, as speakers of vowel
harmony languages typically have some exceptional dishar-
monic forms in their lexicons. The effect of these forms on
LIBPHON?s performance is currently being investigated.
5
realisation of PL has fixed, low F2, and that the
realisation of ACC has alternating F2, which real-
isations corresponding roughly to [be] (high F2)
and [bo] (low F2).
Figure 2: Graphical representation of plural forms
of GIDEGEBI, as produced by teacher agent, with
opaque PL realisation.
0 2 4 6 8 10 12
0
500
1000
1500
2000
2500
3000
GIDEGEBI NOM
gidegebigu
F1
F2
0 2 4 6 8 10 12
0
500
1000
1500
2000
2500
3000
GIDEGEBI ACC
gidegebigube
F1
F2
Figure 3: Graphical representation of plural forms
of GIDEGEBI, as produced by teacher agent, with
transparent PL realisation.
These figures also illustrate the difference be-
tween languages with opaque versus transparent
PL, as reflected in the realisation of the word-final
ACC marker in the two lower graphs, which agrees
in F2 with the realised form of the PL or root, re-
spectively.
6 The experiments
Assessing successful learning/generalisation in a
computational model requires some measurable
outcome that can be tracked over time. Because
LIBPHON is an output-oriented model, its cate-
gorisation of inputs is a poor indicator of the ex-
tent to which it has learned a productive ?rule? of
vowel harmony. In lieu of this measure, I have
opted to pursue two difference courses of evalua-
tion.
For the harmony cases, LIBPHON is queried on
a held-out test set of 500 previously unseen LA-
BELs and its output is compared to the mean value
of the teacher?s stored trajectories for the same LA-
BELs. In particular, given some LABEL which was
not in the training data, we can query LIBPHON
at various stages of acquisition (viz. with lexicons
of increasing size) by having it produce an output
for that LABEL, and track the change in its perfor-
mance over time.
The actual measure of error taken is the root-
mean-squared deviation between the learner?s out-
put, y and the mean, t, of the teacher?s stored
forms for some label, l, over all of the consonants
and vowels within a word, averaged across the re-
maining unseen items of the test set:
RMSE =
1
N
?
l?lex
?
?
i
(t
i
? y
i
)
2
len(t)
Figures 4 and 5 show RMSE vs. lexicon size
for both opaque and transparent neutrality (cf. the
cases in Figures 2 and 3), for five simulation runs
each. We can see clearly that error drops as the
lexicon grows, hence that LIBPHON is learning to
make its outputs more like those of the teacher,
but the informativity of this measure stops there.
From a linguistic point of view, we are interested
in what LIBPHON?s outputs look like, viz. has it
learned vowel harmony?
Figure 4: RMSE 1000-word lexicon. Opaque neu-
trality.
6
0 10 20 30 40 50 60 70 80 90 100 110 120
Lexicon size/10
300
400
500
600
700
800
900
1000
R
M
S
E
p
e
r
w
o
r
d
Learner 0
Learner 1
Learner 2
Learner 3
Learner 4
Figure 5: RMSE 1000-word lexicon. Transparent
neutrality.
Figures 6 and 7 show that vowel harmony is
learned, and moreover quite quickly, after going
through a brief initial phase of spurious outputs. In
these figures, LIBPHON is being asked to produce
outputs for all forms of the label GUBOGOBU. For
the particular run shown here, at the 10-word stage
(i.e. when LIBPHON had seen tokens from 10 la-
bels), the only tokens marked PL-ACC were from
high F2 (?front?) trajectories. Hence the nearest
neighbour calculation in the production algorithm
resulted in a fronted form being output. Although
acquisition research in vowel harmony languages
is relatively rare, or inaccessible to us due to lan-
guage barriers, what research there is seems to in-
dicate that harmony is mastered very quickly, with
virtually no errors by 2 years of age, hence it is un-
clear what status to assign to output patterns like
the one discussed here. Moreover, given the well-
known facts that (i) comprehension precedes pro-
duction, and (ii) infants avoid saying unfamiliar
words, it is unlikely that an infant could be coaxed
into producing an output form for such an early-
stage class.
7 Discussion and future work
The experiments discussed here show that on the
basis of limited input data, LIBPHON, an instance-
based learner that produces output via kernel-
smoothed nearest-neighbour regression, learns to
produce harmonically correct novel outputs. In
particular, it is able to generalise and produce cor-
rect morphologically complex forms to which it
has not been exposed in its training data, i.e. a
previously unseen case-marked form will be out-
put with harmonically correct F2, including neu-
trality (opaque or transparent). In ongoing re-
0 2 4 6 8 10 120
500
1000
1500
2000
2500 NOMACCPL NOMPL ACC
Figure 6: Evolution of gubogobu in early acqui-
sition: 10 words
Figure 7: Evolution of gubogobu in early acqui-
sition: 30 words
search I am (i) evaluating LIBPHON?s perfor-
mance with respect to more traditional measures,
in particular F -score, on held-out data as the lexi-
con grows, and (ii) assessing the viability of DTW-
based alignment for preprocessing real speech to-
kens as inputs to LIBPHON.
Acknowledgments
Many thanks to Ash Asudeh, Lev Blumenfeld,
Andrea Gormley, Jeff Mielke, Alan Hogue and
Andy Wedel for discussion and comments on this
line of research, and to three anonymous referees
for feedback that greatly improved this paper. This
work carried out with the support of NSERC Dis-
covery Grant 371969 to Dr. Ash Asudeh.
References
David Aha. 1997. Lazy learning. In Lazy Learning,
pages 7?10. Kluwer Academic Publishers.
7
Noam Chomsky and Morris Halle. 1968. The Sound
Pattern of English. Harper and Row.
John Coleman. 1998. Cognitive reality and the phono-
logical lexicon: A review. Journal of Neurolinguis-
tics, 11(3):295?-320.
Thomas Cover and Peter Hart. 1967. Nearest neighbor
pattern classification. IEEE Transactions on Infor-
mation Theory, 13:21?27.
Walter Daelemans and Antal van den Bosch. 2005.
Memory-Based Language Processing. Studies in
Natural Language Processing. Cambridge Univer-
sity Press.
Walter Daelemans, Steven Gillis, and Gert Durieux.
1994. The acquisition of stress: A data-oriented ap-
proach. Computational Linguistics, 20(3).
Bart de Boer. 2000. Self-organization in vowel sys-
tems. Journal of Phonetics, 28:441?465.
B. Elan Dresher. 1999. Charing the learning path:
Cues to parameter setting. Linguistic Inquiry,
30(1):27?67.
Marc Ettlinger. 2007. An exemplar-based model of
chain shifts. In Proceedings of the 16th Interna-
tional Congress of the Phonetic Science, pages 685?
688.
Evelyn Fix and J.L. Hodges. 1951. Discrimina-
tory analysis, nonparametric discrimination: Con-
sistency properties. Technical Report 4, USAF
School of Aviation Medicine.
Stephen Goldinger. 1996. Words and voices: Episodic
traces in spoken word identification and recogni-
tion memory. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 22:1166?1183.
Morris Halle. 1997. Some consequences of the repre-
sentation of words in memory. Lingua, 100:91?100.
Trevor Hastie, Robert Tibshirani, and Jerome Fried-
man. 2009. Elements of Statistical Learning.
Springer Series in Statistics. Springer-Verlag, 2 edi-
tion.
Hynek Hermansky. 1990. Perceptual linear predictive
(plp) analysis of speech. Journal of the Acoustical
Society of America, 87(4):1738?1752.
Keith Johnson and John W. Mullenix, editors. 1997.
Talker Variability in Speech Processing. Academic
Press.
Keith Johnson. 1997. Speech perception without
speaker normalization: an exemplar model. In
Talker Variability in Speech Processing, chapter 8,
pages 145?166. Academic Press.
Keith Johnson, 2007. Decision and Mechanisms in
Exemplar-based Phonology, chapter 3, pages 25?40.
Oxford University Press.
Peter Jusczyk. 1999. How infants begin to extract
words from speech. Trends in Cognitive Sciences,
3(9):323?328.
Peter Jusczyk. 2000. The Discovery of Spoken Lan-
guage. MIT Press.
Robert Kirchner and Roger Moore. 2009. Computing
phonological generalization over real speech exem-
plars. ms.
Ken Lodge. 2009. Fundamental Concepts in Phonol-
ogy: Sameness and difference. Edinburgh Univer-
sity Press.
Douglas Medin and Marguerite Schaffer. 1978. Con-
text theory of classification learning. Psychological
Review, 85(3):207?238.
Jeff Mielke. 2008. The Emergence of Distinctive Fea-
tures. Oxford Studies in Typology and Linguistic
Theory. Oxford University Press.
Robert Nosofsky. 1986. Attention, similarity, and the
identification-categorization relationship. Journal
of Experimental Psychology: General, 115(1):39?
57.
Colin Phillips, Thomas Pellathy, Alec Marantz, El-
ron Yellin, Kenneth Wexler, David Poeppel, Martha
McGinnis, and Timothy Roberts. 2000. Auditory
cortex accesses phonological categories: An meg
mismatch study. Journal of Cognitive Neuroscience,
12(6):1038?1055.
Janet Pierrehumbert. 2001. Exemplar dynamics: Word
frequency, lenition, and contrast. In Frequency ef-
fects and the emergence of linguistic structure, pages
137?157. John Benjamins.
Alan Prince and Paul Smolensky. 2004. Optimality
Theory: Constraint interaction in generative gram-
mar. Blackwell.
Lawrence Rabiner and Biing-Hwang Juang. 1993.
Fundamentals of Speech Recognition. Prentice Hall.
Richard Semon. 1921. The Mneme. George Allen and
Unwin.
Harvey Sussman, David Fruchter, Jon Hilbert, and
Joseph Sirosh. 1998. Linear correlates in the speech
signal: The orderly output constraint. Behavioral
and Brain Sciences, 21:241?299.
Bruce Tesar and Paul Smolensky. 2000. Learnability
in Optimality Theory. MIT Press.
Riitta V?alimaa-Blum. 2009. The phoneme in cognitive
phonology: episodic memories of both meaningful
and meaningless units? Cognitextes, 2.
Harry van der Hulst and Jeroen van de Weijer. 1995.
Vowel harmony. In John Goldsmith, editor, Hand-
book of Phonological Theory. Blackwell.
8
