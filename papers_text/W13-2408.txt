Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 48?57,
Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational Linguistics
Lemmatization and Morphosyntactic Tagging of Croatian and Serbian
Z?eljko Agic?? Nikola Ljubes?ic?? Danijela Merkler?
?Department of Information and Communication Sciences
?Department of Linguistics
Faculty of Humanities and Social Sciences, University of Zagreb
Ivana Luc?ic?a 3, 10000 Zagreb, Croatia
zagic@ffzg.hr nljubesi@ffzg.hr dmerkler@ffzg.hr
Abstract
We investigate state-of-the-art statistical
models for lemmatization and morphosyn-
tactic tagging of Croatian and Serbian.
The models stem from a new manually
annotated SETIMES.HR corpus of Croa-
tian, based on the SETimes parallel cor-
pus. We train models on Croatian text
and evaluate them on samples of Croat-
ian and Serbian from the SETimes corpus
and the two Wikipedias. Lemmatization
accuracy for the two languages reaches
97.87% and 96.30%, while full morphosyn-
tactic tagging accuracy using a 600-tag
tagset peaks at 87.72% and 85.56%, respec-
tively. Part of speech tagging accuracies
reach 97.13% and 96.46%. Results indicate
that more complex methods of Croatian-to-
Serbian annotation projection are not re-
quired on such dataset sizes for these par-
ticular tasks. The SETIMES.HR corpus, its
resulting models and test sets are all made
freely available.
1 Introduction
Part of speech tagging (POS tagging) is an natu-
ral language processing task in which words are
annotated with the corresponding grammatical cate-
gories ? parts of speech: verb, noun, adjective, pro-
noun, etc. ? in a given context. It is also frequently
called morphosyntactic tagging (MSD tagging, i.e.,
tagging with morphosyntactic descriptions), espe-
cially when addressing highly inflected languages,
for which the tagging process often includes as-
signing additional subcategories to words, such as
gender and case for nouns or tense and person for
verbs. POS/MSD tagging is a well-known task and
an important preprocessing step in natural language
processing. It is often preceded or followed by
lemmatization ? the process of mapping inflected
word forms to corresponding base forms or lemmas.
State of the art in POS/MSD tagging and lemma-
tization across languages is generally achieved ?
both in terms of per token accuracy and speed and
robustness ? by statistical methods, which involve
training annotation models on manually annotated
corpora.
In this paper, we investigate the possibility of uti-
lizing statistical models trained on corpora of Croa-
tian in lemmatization and MSD tagging of Croatian
and Serbian. We present a new manually annotated
corpus of Croatian ? the SETIMES.HR corpus. We
test a number of lemmatizers and MSD taggers on
Croatian and Serbian test sets from two different
domains and consider options of annotation trans-
fer between the two languages. We also outline a
first version of the Multext East v5 tagset and three
usable reductions of this tagset. Special emphasis
is given to rapid resource development and public
availability of our research. Thus, the SETIMES.HR
corpus, the test sets and the best lemmatization and
MSD tagging models are made publicly available.1
In the following section, we discuss related work
on lemmatization and tagging of Croatian and Ser-
bian. We then present the SETIMES.HR corpus and
the test sets, selected lemmatizers and morphosyn-
tactic taggers and the experimental method. Finally
we provide a discussion of the evaluation results
and indicate future work directions.
2 Related work
The task of tagging English sentences with parts of
speech is generally considered a closed issue. This
is due to the fact that, over the course of the past 11
years, from (Brants, 2000) to (S?gaard, 2011), the
current state of the art in tagging English has im-
proved by 1.04 ? to 97.50% in terms of per token
accuracy. This is, however, not the case for lan-
guages with richer morphology and free sentence
1http://nlp.ffzg.hr/resources/models/
48
word order, such as Croatian and Serbian.
Current state of the art for statistical MSD tag-
ging of Croatian is reported at 86.05% (Agic? et
al., 2008). It involves a hidden Markov model tri-
gram tagger CroTag, trained on the Croatia Weekly
100 thousand wordform (100 kw) subcorpus of
Croatian newspaper text from Croatian National
Corpus (Tadic?, 2009), manually MSD-tagged and
lemmatized using the Multext East v3 tagset (MTE
v3) (Erjavec, 2004) and Croatian Lemmatization
Server (Tadic?, 2005) for guided annotation. The
tagger is not publicly available. Just recently, the
Croatia Weekly corpus has been made publicly
available through META-SHARE.2 Another line of
research reports on a prototype constraint grammar
tagger for Croatian (Peradin and S?najder, 2012),
which scores at 86.36% using a MTE-based tagset.
This tagger is also not publicly available as it is in
prototype stage and it currently does not analyze
out-of-vocabulary word forms. The top score for
lemmatizing Croatian text is reported at 96.96%
by combining CroTag and Croatian Morphological
Lexicon (Agic? et al, 2009). The lemmatizer is not
publicly available.
Lemmatization and tagging of Serbian text
was recently addressed in (Gesmundo and
Samardz?ic?, 2012a; Gesmundo and Samardz?ic?,
2012b). It involves BTagger, a combined bidirec-
tional tagger-lemmatizer tool which implements a
lemmatization-as-tagging paradigm. Models are
trained on the Serbian Multext East 1984 corpus,
they are publicly available3 under a permissive li-
cense, reaching overall accuracies of 97.72% for
lemmatization and 86.65% for MSD tagging. It
should be noted, however, that BTagger evaluation
in terms of spatial and temporal complexity was not
documented and that the results provided for Ser-
bian are obtained on specific in-domain data, i.e.,
a corpus of fiction and are thus not directly com-
parable to, e.g., results for Croatian on the Croatia
Weekly newspaper corpus.
Other lines of research in Serbian lemmatization
and tagging exists. Delic? et al (2009) deals with
transformation-based tagging of Serbian text, but
it does not provide state-of-the-art results or freely
available resources. Rule-based approaches to pro-
cessing Serbian using NooJ 4 and similar linguistic
development environments have been thoroughly
2http://metashare.elda.org/
3https://github.com/agesmundo/BTagger
4http://www.nooj4nlp.net/
explored (Vitas et al, 2003). Several resources rel-
evant for Serbian lemmatization and tagging are
provided to the public. The Serbian version of
Jules Verne 60 kw manually lemmatized and MTE-
tagged corpus implements a small deviation from
MTE v4 and deals with specific fictional closed-
vocabulary data. SrpLemKor is a 3.7 Mw corpus of
Serbian newspaper text, automatically lemmatized
and POS-tagged using TreeTagger (Schmid, 1995)
with a tagset of 16 POS tags. A morphological dic-
tionary of 85 thousand Serbian lemmas with sligtly
deviated MTE v4 tagset is available through NooJ.
Public availability of these resources is enabled
through META-SHARE, with somewhat more re-
strictive licensing that involves non-commercial
use in all cases and for some of them it also im-
poses no redistribution.
Related work on lemmatizer and tagger compar-
ison exists for many languages. Restraining the
search to closely related Slavic languages, exten-
sive work in this domain has been done for Bul-
garian (Georgiev et al, 2012), Czech (Spoustova?
et al, 2007) and Slovene (Erjavec and Dz?eroski,
2004; Rupnik et al, 2008). For Croatian, prelim-
inary work on tagger evaluation for tagger voting
has been conducted (Agic? et al, 2010).
3 SETIMES.HR corpus
SETIMES.HR is a new manually lemmatized and
MSD-tagged corpus of Croatian. It is built on top
of the SETimes parallel newspaper corpus involv-
ing 10 languages from the SEE region,5 Croatian
and Serbian included. This initial dataset selection
was deliberate in terms of enabling us with possibil-
ity of cross-lingual annotation projection and other
cross-lingual experiments. SETIMES.HR was anno-
tated by experts using the Croatian Lemmatization
Server (HML)6 (Tadic?, 2005) to facilitate the pro-
cess. We made a number of changes to the initial
annotation provided by human annotators. Namely,
HML provides MSD tags using an undocumented
alteration of the initial MTE tagset, which we cor-
rected to conform entirely to the MTE v4 standard
(Erjavec, 2012). Also, for certain lemmas HML
provides lemmatization with morphosemantic cues
encoded by lemma numbering ? e.g. biti1 (en. to
be) and biti2 (en. to beat) ? which we omitted as
they are used only in the process of generating the
morphological lexicon (Tadic? and Fulgosi, 2003)
5http://www.nljubesic.net/resources/corpora/setimes/
6http://hml.ffzg.hr
49
Corpus Sent?s Tokens Types Lemmas
SETIMES.HR 4 016 89 785 18 089 8 930
set.test.hr 100 2 297 1 270 991
set.test.sr 100 2 320 1 251 981
wiki.test.hr 100 1 887 1 027 802
wiki.test.sr 100 1 953 1 055 795
Table 1: Stats for SETIMES.HR and test sets
and are thus not required for purposes of lemmati-
zation and MSD tagging. We make the resulting 90
kw SETIMES.HR corpus, along with the four test
sets, publicly available under the CC-BY-SA-3.0
license.7 Corpus stats are given in Table 1.
For purposes of this experiment, we propose an
alteration of the baseline MTE v4 tagset in form
of a first version for the MTE v5 standard.8 The
biggest changes in the new version are participal ad-
jectives and adverbs moving from the verbal subset
? which was very complex in v4 ? to the adjectival
and adverbial subsets. Additionally, acronyms are
moved from the abbreviation subset to the noun
subset. A general shrinking of the length of many
tags was performed as well because from v4 on-
wards the MTE standard does not require one tagset
for all languages in the standard. We also suggest
three reductions of the suggested MTE v5 tagset:
1. without adjective definiteness (v5r1),
2. without common (Nc) vs. proper (Np) distinc-
tion for nouns (v5r2) and
3. without both (v5r3).
Adjectival definiteness is a category which is easy
to implement in a morphological lexicon, but is
very hard to distinguish in context as many of its
variants are homographs. We question the distinc-
tion between common and proper nouns as well
since they are contextually very hard to discrimi-
nate. On the other hand, some foreign proper nouns
are inflected by specific paradigms and suffix tries
used on unknown words could profit from this dis-
tinction. Stats for the MTE v5 and the reduced
tagset versions in comparison with the baseline
MTE v4 tagset version of SETIMES.HR are given
in Table 2. They reflect the design choices we
made: MTE v5 has a comparable amount of tags
as MTE v4, gaining additional tags in the adjective
subset, but losing tags in the verb and abbreviation
subsets, while the reductions subsequently lower
the overall MSD tag count.
7http://creativecommons.org/licenses/by-sa/3.0/
8http://nl.ijs.si/ME/V5/msd/html/
set.test wiki.test
Tagset SETIMES.HR hr sr hr sr
MTE v4 660 235 236 188 192
MTE v5 663 233 234 192 195
MTE v5r1 618 213 216 176 180
MTE v5r2 634 216 217 178 181
MTE v5r3 589 196 199 162 166
Table 2: Tagset variation in tag counts
4 Experiment setup
In this section, we define specific experiment goals
and the experiment design. We also present the
datasets and tools used in the experiment.
4.1 Objectives
The principal goal of this experiment is to provide
prospective users with freely available ? download-
able, retrainable and usable, both for research pur-
poses and for commercial use ? state-of-the-art
lemmatization and tagging modules for Croatian
and Serbian. An additional goal of our experi-
ment is to inspect lemmatization and tagging tools
available under permissive licenses and give an
overview regarding their accuracy and time com-
plexity when used on languages of morphological
complexity such as Croatian and Serbian.
Regarding the previously discussed constraints
on existing corpora and tools for Croatian and Ser-
bian tagging and lemmatization, our objective im-
plies exclusive usage of the SETIMES.HR corpus in
the experiment.9 Since SETIMES.HR is part of the
SETimes parallel corpus which, among other lan-
guages, includes both Croatian and Serbian, manu-
ally annotated SETIMES.HR text has a freely avail-
able Serbian equivalent. Our first course of action
was thus to train a number of taggers and lemma-
tizers on SETIMES.HR and test it on Croatian and
Serbian held out text to verify state-of-the-art ac-
curacy on Croatian text and to observe whether
the expected decline in accuracy on Serbian text is
substantial or not.
In case of substantial decrease in accuracy for
lemmatizing and tagging Serbian using Croatian
models, we designed multiple schemes for project-
ing annotation from SETIMES.HR to its Serbian
9Considering corpora of Croatian and Serbian stated in
related work, we chose not to use non-MTE resources and
corpora of fiction as an experiment basis. Importance of en-
coding the full set of morphological features from the MTE
tagset is illustrated by its benefits for dependency parsing of
Croatian (Agic? and Merkler, 2013).
50
equivalent from the SETimes parallel corpus. The
general directions for identifying the bitext sub-
set for annotation projection were using parallel
sentences which have the highest longest common
subsequence or using statistical machine transla-
tion to produce Serbian sentences with minimum
difference to the Croatian counterpart. Projecting
tags on a bitext of high similarity would include
heuristics of annotating the variation with the same
morphosyntactic category if the variation was one
token long or annotating it with the existing model
for tagging if the variation was longer than that.
Lemmatization of the single-token variation would
be reapplied if the token ending in both languages
was identical while other cases would be annotated
with the existing lemmatization model.
4.2 Experiment workflow
We do four batches of experiments:
1. to identify the best available tool and underly-
ing paradigm for lemmatization and tagging
of both languages by observing overall accu-
racy and execution time,
2. to establish the need for annotation projec-
tion from Croatian SETIMES.HR corpus to its
Serbian counterpart,
3. to select the best of the proposed MTE-based
tagsets for both tasks and
4. to provide in-depth evaluation of the selected
top-performing lemmatizer and tagger on both
languages by using the top-performing tagset.
In the first experiment batch, we test the tools only
on Croatian data from SETimes. The second batch
establishes the need for ? or needlessness of ? an-
notation projection for improved processing of Ser-
bian text by testing the tools selected in the first
batch on both languages. The in-depth evaluation
of the third and fourth experiment batch includes,
for both languages and all test sets, observing the
influence of tagset selection to overall accuracy and
investigating tool performance in more detail. We
measure precision, recall and F1 scores for selected
parts of speech and inspect lemmatization and tag-
ging confusion matrices for detailed analysis and
possible prediction of tool operation in real-world
language processing environments.
We aim for the experiment to serve as underly-
ing documentation for enabling prospective users
in implementing more complex natural language
processing systems for Croatian and Serbian by us-
ing these resources. Additionally, the overview of
the usability of tools available is informative for re-
searchers developing basic language technologies
for other languages. We test statistical significance
of observed differences in our results by using the
approximate randomization test.
4.3 Datasets
All models are trained on SETIMES.HR. To at
least partially avoid the possible pitfall of exclu-
sive in-domain testing, we define two test sets for
each language. The first test set consists of 100
Croatian-Serbian parallel sentence pairs taken by
random sampling from the relative complement
of the SETimes parallel corpus and SETIMES.HR.
The second test set is taken from the Croatian and
Serbian Wikipedia by manually selecting 20 match-
ing Wikipedia articles and manually extracting 100
approximate sentence pairs. We chose manual over
random sampling from Wikipedia to account for
the fact that a certain number of articles is virtu-
ally identical between the two Wikipedias due to
language similarity and mutual copying between
Wikipedia users. All four test sets were manually
annotated using the same procedure that was used
for SETIMES.HR. The stats are given in Table 1. In
addition, we have verified the difference between
language test sets by measuring lexical coverage
using HML as a high-coverage morphological lex-
icon of Croatian. For the Croatian SETimes and
Wikipedia samples, we detected 5.2% and 3.9%
out-of-vocabulary word forms and 11.40% and
8.86% were observed for the corresponding Ser-
bian samples, supporting well-foundedness of the
test sets in terms of maintaining the differences
between the two languages.
4.4 Lemmatizers and taggers
As lemmatizers and taggers with permissive licens-
ing schemes and documented cross-lingual state-of-
the-art performance have become largely available,
we chose not to implement our own but to obtain a
set of tools and test them using our data, i.e., train
them on the SETIMES.HR corpus and test them
on Croatian and Serbian SETimes and Wikipedia
test samples. We selected the tools on the basis of
availability and underlying stochastic paradigms as
to identify the best tools and best paradigms.
We tested hidden Markov model trigram taggers
HunPos10 (Hala?csy et al, 2007) and lemmatization-
capable PurePos11 (Orosz and Nova?k, 2012),
10https://code.google.com/p/hunpos/
11https://github.com/ppke-nlpg/purepos
51
Tool Lem. MSD Train (sec) Test (sec)
BTagger 96.22 86.63 24 864.47 87.01
CST 97.78 ? 1.80 0.03
+ lex 97.04 ? 1.87 0.12
HunPos ? 87.11 1.10 0.11
+ lex ? 84.81 10.79 0.45
PurePos 74.40 86.63 5.49 4.42
SVMTool ? 84.99 1 897.08 3.28
TreeTagger 90.51 85.07 7.49 0.19
+ lex 94.12 87.01 17.48 0.31
Table 3: Preliminary evaluation
lemmatization-capable decision-tree-based Tree-
Tagger12 (Schmid, 1995), support vector machine
tagger SVMTool13 (Gime?nez and Ma`rquez, 2004)
and CST?s14 data-driven rule-based lemmatizer (In-
gason et al, 2008). Keeping in mind the previously
mentioned state-of-the-art scores on Serbian 1984
corpus and statistical lemmatization capability, we
also tested BTagger (Gesmundo and Samardz?ic?,
2012a; Gesmundo and Samardz?ic?, 2012b). Since
some lemmatizers and taggers are capable of using
an external morphological lexicon, we used a MTE
v5r1 version of Apertium?s lexicon of Croatian15
(Peradin and Tyers, 2012) where applicable.16 All
tools are well-documented and successfully applied
across languages, as indicated in related work.
5 Results and discussion
A discussion of the experiment results follows in
the next four subsections. Each subsection repre-
sents one batch of experiments. First we select the
best lemmatizer and tagger, next we check for a
need of annotation projection to the Serbian corpus,
then the best MTE-based tagset using the best tool
combination. Finally we provide a more detailed
insight into the results of the top-performing pair
of selected tools and tagset.
5.1 Tool selection
Results of the first experimental batch, consisting
of testing the selected set of lemmatizers and tag-
gers on the MTE v5r1 version of Croatian SETimes
test set, are given in Table 3. In terms of lemmati-
12http://www.cis.uni-muenchen.de/ schmid/tools/TreeTagger/
13http://www.lsi.upc.edu/ nlp/SVMTool/
14http://cst.dk/online/lemmatiser/uk/
15http://www.apertium.org/
16As with already existing Croatian annotated corpora,
HML is not fully MTE compliant. For future work, we might
utilize a compliant version in our experiment and resulting
models, being that its coverage is generally greater than the
one of Apertium?s lexicon due to size difference.
set.test wiki.test
POS hr sr hr sr
HunPos 97.04 95.47 94.25 96.46
+ lex 96.60 95.09 94.62 95.58
MSD
HunPos 87.11 85.00 80.83 82.74
+ lex 84.81 81.59 78.49 79.20
Table 4: Overall tagging accuracy with and without
the inflectional lexicon
set.test wiki.test
Model hr sr hr sr
CST 97.78 95.95 96.59 96.30
+ lex 97.04 95.52 96.38 96.61
Table 5: Overall lemmatization accuracy with and
without the inflectional lexicon
zation and tagging accuracy as well as processing
speed in both training and testing, the top perform-
ing tools are CST lemmatizer and HunPos tagger.
Thus, we chose these two for further investigation
in the following batches of experiments. It should
be noted that, even though its performance is com-
parable to the one of CST and HunPos, BTagger
was not chosen for the other batches primarily be-
cause of its temporal complexity, as it is orders of
magnitude higher than for the selected tools. Given
that lemmatization and tagging are considered pre-
requisites for further processing of text tata, the
data itself often being fed to these modules in large
quantities (e.g., web corpora), we insist on the sig-
nificance of temporal complexity in tool selection.
The other results are comparable with previous re-
search in tagging Croatian. Where applicable, we
tried assisting the tools by providing Apertium?s
lexicon as an optional input for improved lemma-
tization and tagging. Only TreeTagger lemmatiza-
tion and tagging benefited from lexicon inclusion.
However, it should be noted that TreeTagger imple-
ments a very simple approach to lemmatization, as
it only performs dictionary matching and does not
lemmatize unknown words. Inclusion of a larger
lexicon such as HML might be more beneficial for
all the tools.
5.2 Annotation projection
HunPos tagging accuracy on all Croatian and Ser-
bian test sets for both POS only and full MSD is
given in Table 4 for the default variant and for the
52
Tagset set.test wiki.test
POS hr sr hr sr
MTE v4 96.08 94.61 93.96 95.85
MTE v5 97.04 95.52 94.30 96.40
MTE v5r1 97.04 95.47 94.25 96.46
MTE v5r2 97.00 95.60 94.20 96.30
MTE v5r3 97.13 95.56 94.09 96.15
MSD
MTE v4 86.24 83.45 80.45 81.98
MTE v5 86.77 84.48 80.46 82.43
MTE v5r1 87.11 85.00 80.83 82.74
MTE v5r2 87.11 84.96 81.20 82.38
MRE v5r3 87.72 85.56 81.52 82.79
Table 6: HunPos POS and MSD tagging accuracy
for all tagsets
set.test wiki.test
Tagset hr sr hr sr
MTE v4 97.78 95.82 96.66 96.11
MTE v5 97.82 95.86 96.81 96.30
MTE v5r1 97.78 95.95 96.59 96.30
MTE v5r2 97.87 95.99 96.75 96.20
MTE v5r3 97.74 95.99 96.54 96.20
Table 7: CST lemmatization accuracy for all tagsets
one using Apertium?s lexicon. These results serve
as the first decision point regarding the need for
Croatian-to-Serbian annotation projection, the sec-
ond one being the lemmatization scores in Table
5. Here we observed an unsubstantial decrease
in POS and MSD tagging between Croatian and
Serbian test sets ? the observed difference is, in
fact, more substantial across domains than across
languages. Overall, Croatian and Serbian scores
differ less than 3%. Results for Serbian Wikipedia
sample are even consistently better than for Croa-
tian Wikipedia, emphasizing domain significance
over language difference. The tagger does not ben-
efit from the inclusion of the inflectional lexicon
in POS tagging and it even incurs a substantial 2%
to 4% penalty in MSD tagging. Since such obser-
vations were not made while including the lexicon
with the TreeTagger tool ? which implements the
simplest form of dictionary lemmatization ? we
performed a small results analysis and noticed an
unnaturally high percentage of categories that are
as expected present in the lexicon, but very rare in
the training corpus (like the vocative case) point-
ing to a na??ve implementation of the procedure.
Thus we chose not to use the lexicon in further
observations. Lack of more substantial differences
Tagsets v5 v5r1 v5r2 v5r3
v4 0.268 <0.05 <0.05 <0.01
v5 / <0.01 <0.05 <0.01
v5r1 / / 0.877 <0.05
v5r2 / / / <0.01
Table 8: Statistical significance of differences in
full MSD tagging between tagsets (p-values using
approximate randomization)
in tagging scores between Croatian and Serbian
for this specific test scenario implied no need for
annotation projection.
This is further supported by overall lemmatiza-
tion scores in Table 5. Even with the observed
lexical differences between the languages, as we
indicated in the description of the test sets by mea-
suring lexical coverage using HML, the learned
CST lemmatizer rules are more robust consider-
ing language alteration than the trigram tagging
model of HunPos. Lemmatization accuracy stays
in the margins of approximately 97%?1% for both
languages. Average accuracy on Croatian is less
than 2% higher than for Serbian and the domain
patterns observed for tagging are also observed for
lemmatization. Benefits of an inflectional lexicon
for lemmatization are minor, if any, which can be
followed back to the small size of the lexicon and
high quality of the CST lemmatizer. On the con-
trary, TreeTagger?s simple lemmatization does gain
four points by using the lexicon, but it initially
performs seven points worse than CST.
5.3 Tagset selection
Tables 6 and 7 show the influence of tagset de-
sign on tagging and lemmatization accuracy. They
are accompanied by Table 8, i.e., results of testing
statistical significance of differences between the
tagsets in the task of full MSD tagging from Table 6.
Statistical significance is calculated with all test
sets merged into one. Differences in lemmatization
accuracy are virtually non-existent regarding the
tagset choice. Full MSD tagging follows the usual
pattern of inverse proportionality between tagset
size and overall accuracy. It should be noted that
MTE v5 accuracy is not significantly higher than
MTE v4 accuracy (p = 0.268), but we consider the
new tagset to be easier to use for humans since its
tags are shortened by removing placehodlers for
features used in other MTE languages. Consider-
ing that only tagging accuracy using the MTE v5r3
tagset is significantly better than tagging using all
53
Croatian Serbian
POS P R F1 P R F1
Adj 94.33 90.14 92.19 94.34 93.98 94.16
66.80 63.83 65.28 66.79 66.54 66.66
Adv 84.56 82.73 83.63 82.57 73.77 77.92
84.56 82.73 83.63 82.57 73.77 77.92
Conj 95.29 93.82 94.55 97.92 95.29 96.59
94.12 92.66 93.38 96.89 94.28 95.57
Noun 95.70 96.34 96.02 95.42 96.59 96.00
76.78 77.30 77.04 75.38 76.30 75.84
Num 94.57 97.75 96.13 96.51 93.26 94.86
91.30 94.38 92.81 94.19 91.01 92.57
Prep 98.10 99.72 98.90 98.45 98.70 98.57
95.93 97.52 96.72 94.30 94.55 94.42
Pron 95.97 97.54 96.75 95.78 97.42 96.59
81.85 83.20 82.52 81.43 82.83 82.12
Verb 95.88 98.07 96.96 95.23 95.72 95.47
93.81 95.96 94.87 93.36 93.84 93.60
Table 9: Precision (P), recall (R) and F1 score for
POS only (1st column) and full MSD (2nd column)
on Croatian and Serbian
other suggested tagsets, we chose this tagset and
tagging model for further observation of lemmatiza-
tion and tagging properties in the remainder of the
paper. Still, in this section, we present the results
on all tagsets to serve as underlying documentation
of the observed differences, mainly because of the
fact that only MTE v4 is officially supported at
this moment and MTE v5 is a newly-introduced
prototype that displays better performance in this
specific experiment.
5.4 In-depth analysis
In Table 9 we merge SETimes and Wikipedia test
sets by language and provide POS and MSD tag-
ging precision, recall and F1 score for selected
Croatian and Serbian parts of speech. In terms of
POS only, the most difficult-to-tag part of speech
is the adverb, followed by the adjective in both
Croatian and Serbian. The other categories are
consistently POS-tagged with an F1 score of ap-
proximately 95% or higher. The decrease for ad-
verbs and adjectives is somewhat more evident in
precision than in recall and the POS confusion ma-
trix for both languages, given in Table 10, shows
that these two parts of speech are often mistaken
for each other by the tagger. Regarding full MSD
tagging using the MTE v5r3 tagset, for both lan-
guages, the lowest F1 scores are observed for ad-
jectives (approximately 66%), nouns (76%) and
pronouns (82%). This is most likely due to the fact
that these parts of speech have the largest tagset
subsets, making it easier for the tagger to get con-
fused.17 Performance for other parts of speech is
satisfactory, especially for verbs, keeping in mind,
e.g., possible subsequent dependency parsing of the
two languages. The absolute difference between
POS and MSD tagging score is most substantial
for adjectives (approximately 27%), indicating that
certain MSD features might be triggering the de-
crease. This is partially supported by our tagset
design investigation as dropping adjective definite-
ness atribute yielded substantial overall tagging
accuracy increase when compared with the tagsets
in which this attribute is still encoded.
In Table 10 we provide a part of speech confu-
sion matrix for Croatian and Serbian on test sets
merged by language. In Croatian test sets, the most
frequent confusions are those between adjectives
and nouns (28.9%), nouns and verbs (14.5%), ad-
jectives and adverbs (11.6%) and nouns and ad-
verbs (6.9%). In Serbian text, the tagger most fre-
quently confuses nouns ? for adjectives (21.1%),
verbs (20%) and adverbs (16%). Merging the test
sets by language mostly evens out the tagging dif-
ferences as there is a total of 173 MSD confusions
in Croatian test sets and only 3 more, i.e., 175 in
the Serbian test sets.
POS scores for both languages neared the level
of human error in our experiment. Keeping that
in mind, upon observing the confusion instances
themselves, we spotted a confusion between adjec-
tives and nouns (e.g. names of countries (Hrvatska
(en. Croatia, Croatian)), homographic forms
(strana (en. foreign, side), svet (en. world, holy))
and confusion between adjectives and adverbs. Ad-
verbs and prepositions are sometimes confused
with nouns, especially for nouns in instrumental
case (e.g. godinama (en. year, yearly), tijekom
(en. duration, during)). Conjuctions are at times
incorrently tagged because various words can have
a conjuctional function, most frequently pronouns
and adverbs: s?to (en. what), kako (en. how), kada
(en. when). Interestingly, there is some confusion
between nouns and verbs in Wikipedia test sets,
while in SETimes test sets there are almost none.
This confusion arises from the homographic forms
? e.g. mora (en. must, seas) ? or from nouns with
17There are 589 MTE v5r3 tags in SETIMES.HR. Out of
these, 164 are used for tagging adjectives, 42 for nouns and
268 for pronouns, thus accounting for 80.47% of the tagset.
There are also 50 verb tags.
54
POS Abbr Adj Adv Conj Noun Num Part Prep Pron Res Verb
Abbr 0 0 0 1 3 0 0 0 0 0
Adj 0 20 0 50 0 1 0 3 1 4
Adv 0 10 9 12 0 0 2 0 0 2
Conj 0 0 5 2 0 5 5 7 0 0
Noun 0 37 28 0 4 0 1 5 7 25
Num 2 4 0 0 2 0 0 0 0 0
Part 0 0 0 3 0 0 0 0 0 3
Prep 0 0 2 3 2 0 1 0 0 0
Pron 0 2 1 9 3 0 1 0 0 1
Res 0 0 1 0 4 0 0 2 0 0
Verb 0 9 4 0 35 1 2 1 0 1
Table 10: POS confusion matrix for Croatian (top right) and Serbian (bottom left)
Figure 1: Learning curves for Croatian and Serbian lemmatization and tagging
suffixes -la and -lo, which are used for denoting
participles in feminine and neuter gender, or with
suffix -ti, which is also a suffix for infinitive.
Most MSD tag confusions arise from the fact that
the same suffix can denote different cases in dif-
ferent declensions. We observed confused number
and gender category (mostly in adjectives in mas-
culine and neuter gender), but the most frequent
confusion occurs for accusative forms in masculine
gender, which have different suffixes when they de-
note animacy (suffix is the same as in the genitive
case: pobjednika (en. winner), kandidata (en. can-
didate)) and when they denote inanimacy (suffix
is the same as in the nominative case: metak (en.
bullet), bubnjar (en. drummer)).
In lemmatization, as in POS tagging, errors are
generally very infrequent. Some occur with adjec-
tives, when an assigned lemma represents a definite
form of an adjective, instead of an indefinitive form
(and less frequently vice versa). Besides, adjec-
tives are sometimes confused with adverbs (e.g.,
target lemma is znac?ajno (en. significantly), but
the lemma znac?ajan (en. significant) is assigned,
and vice versa). Other less frequent examples in-
clude cases in which the assigned lemma is not in
its canonical form, but a case other than the nomi-
native case, or when the assigned lemma is a word
stem. A small number of errors also occurs due
to slight differences in Croatian and Serbian word-
forms, e.g., when a Serbian nominative form is not
a nominative form in Croatian (planeta as Serbian
nominative and Croatian genitive, planet being the
Croatian nominative).
Figure 1 provides lemmatization, POS and MSD
tagging learning curves for both languages on
merged test sets. Apart from the slight difference
in lemmatization scores in favor of Croatian, the
learning curves and overall scores on merged test
sets are virtually identical. The easiest task to learn
is lemmatization while the most complex one is
applying MSD.
6 Conclusions and future work
In this paper, we have addressed the issue of lemma-
tization and morphosyntactic tagging of two gener-
ally under-resourced languages, Croatian and Ser-
bian. Our goal was to provide the general public
with freely available language resources and state-
55
of-the-art models for lemmatization and tagging of
these two languages in terms of accuracy, robust-
ness and speed. We also aimed at using lemmati-
zation and tagging as a platform for implicit com-
parison of the two languages in natural language
processing terms, as to provide partial insight to
how difficult and lossy ? or, more desireably, how
easy and straightforward ? would it be to port lin-
guistic resources and language processing tools
from one language to another.
While developing the models, we completed a
series of experiments. We used the Croatian text
from the freely available SETimes parallel corpus
to create a new manually lemmatized and mor-
phosyntactically tagged corpus of Croatian ? the
SETIMES.HR corpus. Beside the Multext East v4
morphosyntactic tagset specification for Croatian
which was used for initial corpus annotation, we
designed and implemented a first version of the
Multext East v5 tagset and its three reductions and
applied these to SETIMES.HR. Using SETimes
and Wikipedia as starting point resources, we cre-
ated two gold standard test sets for each language
in order to test existing state-of-the-art lemmatiz-
ers and taggers. We ran preliminary tests on a
number of tools to select CST lemmatizer and
HunPos tagger as tools of choice considering ob-
served accuracy, training time and text processing
time. In an in-depth evaluation of these tools, we
obtained peak overall lemmatization accuracy of
97.87% and 96.30% for Croatian and Serbian and
full morphosyntactic tagging accuracy of 87.72%
and 85.56%, with basic part of speech tagging ac-
curacy at 97.13% and 96.46%. In this specific test
scenario and with this specific training set, we have
shown the differences in results between Croatian
and Serbian not to be significant enough to justify
an effort in more elaborate strategy of adapting
Croatian models to Serbian data ? simply training
the models on Croatian text from SETIMES.HR
corpus and using them on Serbian text provided
state-of-the-art results in lemmatization and tag-
ging, while maintaining and even topping previ-
ously documented state of the art for Croatian.
The SETIMES.HR corpus, Croatian and Serbian
test sets and top-performing lemmatization and tag-
ging models are publicly available and freely down-
loadable18 under the CC-BY-SA-3.0 license.
Our future work plans include both enlarging
and enhancing SETIMES.HR. The presented learn-
18http://nlp.ffzg.hr/resources/models/
ing curves show significant room for improvement
by annotating additional data. The dataset aleady
serves as a basis for the SETIMES.HR treebank of
Croatian (Agic? and Merkler, 2013), implementing
a novel dependency syntactic formalism and en-
abling experiments with joint dependency parsing
of Croatian and Serbian. Should dependency pars-
ing experiments show the need for more elaborate
language adaptation strategies, we will most likely
implement them also on the level of lemmas and
morphosyntactic tags before addressing syntactic
issues. This will possibly be helped by statistical
machine translation between Croatian and Serbian
to enhance bitext similarity and empower projec-
tion strategies. An effort could be made to adapt
existing Croatian and Serbian resources and subse-
quently to attempt achieving better lemmatization
and tagging performance by combining these with
SETIMES.HR. We will use the models presented in
this paper to annotate the web corpora of Croatian
and Serbian (Ljubes?ic? and Erjavec, 2011) ? hrWaC
and srWaC.
Acknowledgement
The research leading to these results has re-
ceived funding from the European Union Sev-
enth Framework Programme FP7/2007-2013 un-
der grant agreement n? PIAP-GA-2012-324414
(project Abu-MaTran).
References
Z?eljko Agic? and Danijela Merkler. 2013. Three Syn-
tactic Formalisms for Data-Driven Dependency Pars-
ing of Croatian. In Text, Speech and Dialogue. Lec-
ture Notes in Computer Science. Springer.
Z?eljko Agic?, Marko Tadic?, and Zdravko Dovedan.
2008. Improving Part-of-Speech Tagging Accuracy
for Croatian by Morphological Analysis. Informat-
ica, 32(4):445?451.
Z?eljko Agic?, Marko Tadic?, and Zdravko Dovedan.
2009. Evaluating Full Lemmatization of Croatian
Texts. In Recent Advances in Intelligent Information
Systems, pages 175?184. Exit Warsaw.
Z?eljko Agic?, Marko Tadic?, and Zdravko Dovedan.
2010. Tagger Voting Improves Morphosyntactic
Tagging Accuracy on Croatian Texts. In Proceed-
ings of ITI, pages 61?66.
Thorsten Brants. 2000. TnT: A Statistical Part-of-
Speech Tagger. In Proceedings of ANLP, pages 224?
231.
56
Vlado Delic?, Milan Sec?ujski, and Aleksandar Ku-
pusinac. 2009. Transformation-Based Part-of-
Speech Tagging for Serbian Language. In Proceed-
ings of CIMMACS.
Tomaz? Erjavec and Sas?o Dz?eroski. 2004. Machine
Learning of Morphosyntactic Structure: Lemmatiz-
ing Unknown Slovene Words. Applied Artificial In-
telligence, 18:17?41.
Tomaz? Erjavec. 2004. MULTEXT-East Version 3:
Multilingual Morphosyntactic Specifications, Lexi-
cons and Corpora. In Proceedings of LREC.
Tomaz? Erjavec. 2012. MULTEXT-East: Morphosyn-
tactic Resources for Central and Eastern European
Languages. Language Resources and Evaluation,
46(1):131?142.
Georgi Georgiev, Valentin Zhikov, Kiril Simov, Petya
Osenova, and Preslav Nakov. 2012. Feature-Rich
Part-of-speech Tagging for Morphologically Com-
plex Languages: Application to Bulgarian. In Pro-
ceedings of EACL, pages 492?502.
Andrea Gesmundo and Tanja Samardz?ic?. 2012a. Lem-
matisation as a Tagging Task. In Proceedings of
ACL.
Andrea Gesmundo and Tanja Samardz?ic?. 2012b. Lem-
matising Serbian as Category Tagging with Bidirec-
tional Sequence Classification. In Proceedings of
LREC.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool:
A general POS Tagger Generator Based on Support
Vector Machines. In Proceedings of LREC.
Pe?ter Hala?csy, Andra?s Kornai, and Csaba Oravecz.
2007. HunPos: An Open Source Trigram Tagger.
In Proceedings of ACL, pages 209?212.
Anton Karl Ingason, Sigru?n Helgado?ttir, Hrafn Lofts-
son, and Eir??kur Ro?gnvaldsson. 2008. A Mixed
Method Lemmatization Algorithm Using a Hierar-
chy of Linguistic Identities (HOLI). In Proceedings
of GoTAL, pages 205?216.
Nikola Ljubes?ic? and Tomaz? Erjavec. 2011. hrWaC
and slWaC: Compiling Web Corpora for Croatian
and Slovene. In Text, Speech and Dialogue, pages
395?402. Springer.
Gyo?rgy Orosz and Attila Nova?k. 2012. PurePos ?
An Open Source Disambiguator. In Proceedings of
NLPCS.
Hrvoje Peradin and Jan S?najder. 2012. Towards a
Constraint Grammar Based Morphological Tagger
for Croatian. In Text, Speech and Dialogue, pages
174?182. Springer.
Hrvoje Peradin and Francis M. Tyers. 2012. A Rule-
Based Machine Translation System from Serbo-
Croatian to Macedonian. In Proceedings of
FREERBMT12, pages 55?65.
Jan Rupnik, Miha Grc?ar, and Tomaz? Erjavec. 2008.
Improving Morphosyntactic Tagging of Slovene
Language Through Meta-Tagging. Informatica,
32(4):437?444.
Helmut Schmid. 1995. Improvements in Part-of-
Speech Tagging With an Application to German. In
Proceedings of ACL SIGDAT Workshop.
Anders S?gaard. 2011. Semi-Supervised Condensed
Nearest Neighbor for Part-of-Speech Tagging. In
Proceedings of ACL-HLT, pages 48?52.
Drahom??ra ?johanka? Spoustova?, Jan Hajic?, Jan
Votrubec, Pavel Krbec, and Pavel Kve?ton?. 2007.
The Best of Two Worlds: Cooperation of Statistical
and Rule-Based Taggers for Czech. In Proceedings
of BSNLP, pages 67?74.
Marko Tadic? and Sanja Fulgosi. 2003. Building the
Croatian Morphological Lexicon. In Proceedings of
EACL 2003 Workshop on Morphological Processing
of Slavic Languages, pages 41?46.
Marko Tadic?. 2005. Croatian Lemmatization Server.
Southern Journal of Linguistics, 29(1):206?217.
Marko Tadic?. 2009. New Version of the Croatian Na-
tional Corpus. After Half a Century of Slavonic Nat-
ural Language Processing, pages 199?205.
Dus?ko Vitas, Cvetana Krstev, Ivan Obradovic?,
Ljubomir Popovic?, and Gordana Pavlovic?-Laz?etic?.
2003. An Overview of Resources and Basic Tools
for Processing of Serbian Written Texts. In Pro-
ceedings of the Workshop on Balkan Language Re-
sources, 1st Balkan Conference in Informatics.
57
