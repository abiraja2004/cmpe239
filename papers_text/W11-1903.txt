Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 35?39,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
RelaxCor Participation in CoNLL Shared Task on Coreference Resolution
Emili Sapena, Llu??s Padro? and Jordi Turmo?
TALP Research Center
Universitat Polite`cnica de Catalunya
Barcelona, Spain
{esapena, padro, turmo}@lsi.upc.edu
Abstract
This paper describes the participation of
RELAXCOR in the CoNLL-2011 shared
task: ?Modeling Unrestricted Coreference in
Ontonotes?. RELAXCOR is a constraint-based
graph partitioning approach to coreference
resolution solved by relaxation labeling. The
approach combines the strengths of groupwise
classifiers and chain formation methods in one
global method.
1 Introduction
The CoNLL-2011 shared task (Pradhan et al, 2011)
is concerned with intra-document coreference reso-
lution in English, using Ontonotes corpora. The core
of the task is to identify which expressions (usually
NPs) in a text refer to the same discourse entity.
This paper describes the participation of RELAX-
COR and is organized as follows. Section 2 de-
scribes RELAXCOR, the system used in the task.
Next, Section 3 describes the tuning needed by the
system to adapt it to the task issues. The same sec-
tion also analyzes the obtained results. Finally, Sec-
tion 4 concludes the paper.
2 System description
RELAXCOR (Sapena et al, 2010a) is a coreference
resolution system based on constraint satisfaction.
It represents the problem as a graph connecting any
?Research supported by the Spanish Science and Innova-
tion Ministry, via the KNOW2 project (TIN2009-14715-C04-
04) and from the European Community?s Seventh Framework
Programme (FP7/2007-2013) under Grant Agreement number
247762 (FAUST)
pair of candidate coreferent mentions and applies re-
laxation labeling, over a set of constraints, to decide
the set of most compatible coreference relations.
This approach combines classification and cluster-
ing in one step. Thus, decisions are taken consider-
ing the entire set of mentions, which ensures consis-
tency and avoids local classification decisions. The
RELAXCOR implementation used in this task is an
improved version of the system that participated in
the SemEval-2010 Task 1 (Recasens et al, 2010).
The knowledge of the system is represented as a
set of weighted constraints. Each constraint has an
associated weight reflecting its confidence. The sign
of the weight indicates that a pair or a group of men-
tions corefer (positive) or not (negative). Only con-
straints over pairs of mentions were used in the cur-
rent version of RELAXCOR. However, RELAXCOR
can handle higher-order constraints. Constraints can
be obtained from any source, including a training
data set from which they can be manually or auto-
matically acquired.
The coreference resolution problem is represented
as a graph with mentions in the vertices. Mentions
are connected to each other by edges. Edges are as-
signed a weight that indicates the confidence that the
mention pair corefers or not. More specifically, an
edge weight is the sum of the weights of the con-
straints that apply to that mention pair. The larger
the edge weight in absolute terms, the more reliable.
RELAXCOR uses relaxation labeling for the res-
olution process. Relaxation labeling is an iterative
algorithm that performs function optimization based
on local information. It has been widely used to
solve NLP problems. An array of probability values
35
is maintained for each vertex/mention. Each value
corresponds to the probability that the mention be-
longs to a specific entity given all the possible enti-
ties in the document. During the resolution process,
the probability arrays are updated according to the
edge weights and probability arrays of the neighbor-
ing vertices. The larger the edge weight, the stronger
the influence exerted by the neighboring probability
array. The process stops when there are no more
changes in the probability arrays or the maximum
change does not exceed an epsilon parameter.
2.1 Attributes and Constraints
For the present study, all constraints were learned
automatically using more than a hundred attributes
over the mention pairs in the training sets. Usual at-
tributes were used for each pair of mentions (mi,mj)
?where i < j following the order of the document?
, like those in (Sapena et al, 2010b), but bina-
rized for each possible value. In addition, a set
of new mention attributes were included such as
SAME SPEAKER when both mentions have the
same speaker1 (Figures 1 and 2).
A decision tree was generated from the train-
ing data set, and a set of constraints was extracted
with the C4.5 rule-learning algorithm (Quinlan,
1993). The so-learned constraints are conjunctions
of attribute-value pairs. The weight associated with
each constraint is the constraint precision minus a
balance value, which is determined using the devel-
opment set. Figure 3 is an example of a constraint.
2.2 Training data selection
Generating an example for each possible pair of
mentions produces an unbalanced dataset where
more than 99% of the examples are negative (not
coreferent), even more considering that the mention
detection system has a low precision (see Section
3.1). So, it generates large amounts of not coref-
erent mentions. In order to reduce the amount of
negative pair examples, a clustering process is run
using the positive examples as the centroids. For
each positive example, only the negative examples
with distance equal or less than a threshold d are
included in the final training data. The distance is
computed as the number of different attribute values
1This information is available in the column ?speaker? of
the corpora.
Distance and position:
Distance between mi and mj in sentences:
DIST SEN 0: same sentence
DIST SEN 1: consecutive sentences
DIST SEN L3: less than 3 sentences
Distance between mi and mj in phrases:
DIST PHR 0, DIST PHR 1, DIST PHR L3
Distance between mi and mj in mentions:
DIST MEN 0, DIST MEN L3, DIST MEN L10
APPOSITIVE: One mention is in apposition with the other.
I/J IN QUOTES: mi/j is in quotes or inside a NP
or a sentence in quotes.
I/J FIRST: mi/j is the first mention in the sentence.
Lexical:
STR MATCH: String matching of mi and mj
PRO STR: Both are pronouns and their strings match
PN STR: Both are proper names and their strings match
NONPRO STR: String matching like in Soon et al (2001)
and mentions are not pronouns.
HEAD MATCH: String matching of NP heads
TERM MATCH: String matching of NP terms
I/J HEAD TERM: mi/j head matches with the term
Morphological:
The number of both mentions match:
NUMBER YES, NUMBER NO, NUMBER UN
The gender of both mentions match:
GENDER YES, GENDER NO, GENDER UN
Agreement: Gender and number of both mentions match:
AGREEMENT YES, AGREEMENT NO, AGREEMENT UN
Closest Agreement: mi is the first agreement found
looking backward from mj : C AGREEMENT YES,
C AGREEMENT NO, C AGREEMENT UN
I/J THIRD PERSON: mi/j is 3rd person
I/J PROPER NAME: mi/j is a proper name
I/J NOUN: mi/j is a common noun
ANIMACY: Animacy of both mentions match (person, object)
I/J REFLEXIVE: mi/j is a reflexive pronoun
I/J POSSESSIVE: mi/j is a possessive pronoun
I/J TYPE P/E/N: mi/j is a pronoun (p), NE (e) or nominal (n)
Figure 1: Mention-pair attributes (1/2).
inside the feature vector. After some experiments
over development data, the value of d was assigned
to 5. Thus, the negative examples were discarded
when they have more than five attribute values dif-
ferent than any positive example. So, in the end,
22.8% of the negative examples are discarded. Also,
both positive and negative examples with distance
zero (contradictions) are discarded.
2.3 Development process
The current version of RELAXCOR includes a pa-
rameter optimization process using the development
data sets. The optimized parameters are balance and
pruning. The former adjusts the constraint weights
to improve the balance between precision and re-
call as shown in Figure 4; the latter limits the num-
ber of neighbors that a vertex can have. Limiting
36
Syntactic:
I/J DEF NP: mi/j is a definite NP.
I/J DEM NP: mi/j is a demonstrative NP.
I/J INDEF NP: mi/j is an indefinite NP.
NESTED: One mention is included in the other.
MAXIMALNP: Both mentions have the same NP parent
or they are nested.
I/J MAXIMALNP: mi/j is not included in any other NP.
I/J EMBEDDED: mi/j is a noun and is not a maximal NP.
C COMMANDS IJ/JI: mi/j C-Commands mj/i.
BINDING POS: Condition A of binding theory.
BINDING NEG: Conditions B and C of binding theory.
I/J SRL ARG N/0/1/2/X/M/L/Z: Syntactic argument of mi/j .
SAME SRL ARG: Both mentions are the same argument.
I/J COORDINATE: mi/j is a coordinate NP
Semantic:
Semantic class of both mentions match
(the same as (Soon et al, 2001))
SEMCLASS YES, SEMCLASS NO, SEMCLASS UN
One mention is an alias of the other:
ALIAS YES, ALIAS NO, ALIAS UN
I/J PERSON: mi/j is a person.
I/J ORGANIZATION: mi/j is an organization.
I/J LOCATION: mi/j is a location.
SRL SAMEVERB: Both mentions have a semantic role
for the same verb.
SRL SAME ROLE: The same semantic role.
SAME SPEAKER: The same speaker for both mentions.
Figure 2: Mention-pair attributes (2/2).
DIST SEN 1 & GENDER YES & I FIRST &
I MAXIMALNP & J MAXIMALNP &
I SRL ARG 0 & J SRL ARG 0 &
I TYPE P & J TYPE P
Precision: 0.9581
Training examples: 501
Figure 3: Example of a constraint. It applies when the distance
between mi and mj is exactly 1 sentence, their gender match,
both are maximal NPs, both are argument 0 (subject) of their
respective sentences, both are pronouns, and mi is not the first
mention of its sentence. The final weight will be weight =
precision? balance.
the number of neighbors reduces the computational
cost significantly and improves overall performance
too. Optimizing this parameter depends on proper-
ties like document size and the quality of the infor-
mation given by the constraints.
The development process calculates a grid given
the possible values of both parameters: from 0 to 1
for balance with a step of 0.05, and from 2 to 14
for pruning with a step of 2. Both parameters were
empirically adjusted on the development set for the
evaluation measure used in this shared task: the un-
weighted average of MUC (Vilain et al, 1995), B3
(Bagga and Baldwin, 1998) and entity-based CEAF
(Luo, 2005).
Figure 4: Development process. The figure shows MUC?s pre-
cision (red), recall (green), and F1 (blue) for each balance value
with pruning adjusted to 6.
3 CoNLL shared task participation
RELAXCOR has participated in the CoNLL task in
the Closed mode. All the knowledge required by the
feature functions is obtained from the annotations
of the corpora and no external resources have been
used with the exception of WordNet (Miller, 1995),
gender and number information (Bergsma and Lin,
2006) and sense inventories. All of them are allowed
by the task organization and available in their web-
site.
There are many remarkable features that make
this task different and more difficult but realistic
than previous ones. About mention annotation, it
is important to emphasize that singletons are not an-
notated, mentions must be detected by the system
and the mapping between system and true mentions
is limited to exact matching of boundaries. More-
over, some verbs have been annotated as corefering
mentions. Regarding the evaluation, the scorer uses
the modification of (Cai and Strube, 2010), unprece-
dented so far, and the corpora was published very re-
cently and there are no published results yet to use as
reference. Finally, all the preprocessed information
is automatic for the test dataset, carring out some
noisy errors which is a handicap from the point of
view of machine learning.
Following there is a description of the mention de-
tection system developed for the task and an analysis
of the obtained results in the development dataset.
37
3.1 Mention detection system
The mention detection system extracts one mention
for every NP found in the syntactic tree, one for ev-
ery pronoun and one for every named entity. Then,
the head of every NP is determined using part-of-
speech tags and a set of rules from (Collins, 1999).
In case that some NPs share the same head, the
larger NP is selected and the rest discarded. Also the
mention repetitions with exactly the same bound-
aries are discarded. In addition, nouns with capital
letters and proper names not included yet, that ap-
pear two or more times in the document, are also in-
cluded. For instance, the NP ?an Internet business?
is added as a mention, but also ?Internet? itself is
added in the case that the word is found once again
in the document.
As a result, taking into account that just exact
boundary matching is accepted, the mention detec-
tion achieves an acceptable recall, higher than 90%,
but a low precision (see Table 1). The most typ-
ical error made by the system is to include ex-
tracted NPs that are not referential (e.g., predicative
and appositive phrases) and mentions with incorrect
boundaries. The incorrect boundaries are mainly
due to errors in the predicted syntactic column and
some mention annotation discrepancies. Further-
more, verbs are not detected by this algorithm, so
most of the missing mentions are verbs.
3.2 Results analysis
The results obtained by RELAXCOR can be found
in Tables 1 and 2. Due to the lack of annotated sin-
gletons, mention-based metrics B3 and CEAF pro-
duce lower scores ?near 60% and 50% respectively?
than the ones typically achieved with different anno-
tations and mapping policies ?usually near 80% and
70%. Moreover, the requirement that systems use
automatic preprocessing and do their own mention
detection increase the difficulty of the task which ob-
viously decreases the scores in general.
The measure which remains more stable on its
scores is MUC given that it is link-based and not
takes singletons into account anyway. Thus, it is the
only one comparable with the state of the art right
now. The results obtained with MUC scorer show an
improvement of RELAXCOR?s recall, a feature that
needed improvement given the previous published
Measure Recall Precision F1
Mention detection 92.45 27.34 42.20
mention-based CEAF 55.27 55.27 55.27
entity-based CEAF 47.20 40.01 43.31
MUC 54.53 62.25 58.13
B3 63.72 73.83 68.40
(CEAFe+MUC+B3)/3 - - 56.61
Table 1: Results on the development data set
Measure Recall Precision F1
mention-based CEAF 53.51 53.51 53.51
entity-based CEAF 44.75 38.38 41.32
MUC 56.32 63.16 59.55
B3 62.16 72.08 67.09
BLANC 69.50 73.07 71.10
(CEAFe+MUC+B3)/3 - - 55.99
Table 2: Official test results
results with a MUCs recall remarkably low (Sapena
et al, 2010b).
4 Conclusion
The participation of RELAXCOR to the CoNLL
shared task has been useful to evaluate the system
using data never seen before in a totally automatic
context: predicted preprocessing and system men-
tions. Many published systems typically use the
same data sets (ACE and MUC) and it is easy to un-
intentionally adapt the system to the corpora and not
just to the problem. This kind of tasks favor com-
parisons between systems with the same framework
and initial conditions.
The obtained performances confirm the robust-
ness of RELAXCOR and a recall improvement. And
the overall performance seems considerably good
taking into account the unprecedented scenario.
However, a deeper error analysis is needed, specially
in the mention detection system with a low precision
and the training data selection process which may
be discarding positive examples that could help im-
proving recall.
Acknowledgments
The research leading to these results has received funding from the
European Community?s Seventh Framework Programme (FP7/2007-
2013) under Grant Agreement number 247762 (FAUST), and from
the Spanish Science and Innovation Ministry, via the KNOW2 project
(TIN2009-14715-C04-04).
38
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of the Lin-
guistic Coreference Workshop at LREC 98, pages 563?
566, Granada, Spain.
S. Bergsma and D. Lin. 2006. Bootstrapping path-based
pronoun resolution. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 33?40. Association
for Computational Linguistics.
Jie Cai and Michael Strube. 2010. Evaluation met-
rics for end-to-end coreference resolution systems. In
Proceedings of SIGDIAL, pages 28?36, University of
Tokyo, Japan.
M. Collins. 1999. Head-driven statistical models for
natural language parsing. Ph.D. thesis, University of
Pennsylvania.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of the Joint Con-
ference on Human Language Technology and Empir-
ical Methods in Natural Language Processing (HLT-
EMNLP 2005, pages 25?32, Vancouver, B.C., Canada.
G.A. Miller. 1995. WordNet: a lexical database for En-
glish.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011), Portland, Oregon,
June.
J.R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena,
M. Anto`nia Mart??, Mariona Taule?, Ve?ronique
Hoste, Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in multi-
ple languages. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 1?8, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Emili Sapena, Llu??s Padro?, and Jordi Turmo. 2010a. A
Global Relaxation Labeling Approach to Coreference
Resolution. In Proceedings of 23rd International Con-
ference on Computational Linguistics, COLING, Bei-
jing, China, August.
Emili Sapena, Llu??s Padro?, and Jordi Turmo. 2010b. Re-
laxCor: A Global Relaxation Labeling Approach to
Coreference Resolution. In Proceedings of the ACL
Workshop on Semantic Evaluations (SemEval-2010),
Uppsala, Sweden, July.
W.M. Soon, H.T. Ng, and D.C.Y. Lim. 2001. A
Machine Learning Approach to Coreference Resolu-
tion of Noun Phrases. Computational Linguistics,
27(4):521?544.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the Sixth Message Understanding Conference
(MUC-6), pages 45?52.
39
