Proceedings of the Second Workshop on Statistical Machine Translation, pages 40?47,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Dependency Treelet String Correspondence
Model for Statistical Machine Translation
Deyi Xiong, Qun Liu and Shouxun Lin
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
Beijing, China, 100080
{dyxiong, liuqun, sxlin}@ict.ac.cn
Abstract
This paper describes a novel model using
dependency structures on the source side
for syntax-based statistical machine transla-
tion: Dependency Treelet String Correspon-
dence Model (DTSC). The DTSC model
maps source dependency structures to tar-
get strings. In this model translation pairs of
source treelets and target strings with their
word alignments are learned automatically
from the parsed and aligned corpus. The
DTSC model allows source treelets and tar-
get strings with variables so that the model
can generalize to handle dependency struc-
tures with the same head word but with dif-
ferent modifiers and arguments. Addition-
ally, target strings can be also discontinuous
by using gaps which are corresponding to
the uncovered nodes which are not included
in the source treelets. A chart-style decod-
ing algorithm with two basic operations?
substituting and attaching?is designed for
the DTSC model. We argue that the DTSC
model proposed here is capable of lexical-
ization, generalization, and handling discon-
tinuous phrases which are very desirable for
machine translation. We finally evaluate our
current implementation of a simplified ver-
sion of DTSC for statistical machine trans-
lation.
1 Introduction
Over the last several years, various statistical syntax-
based models were proposed to extend traditional
word/phrase based models in statistical machine
translation (SMT) (Lin, 2004; Chiang, 2005; Ding
et al, 2005; Quirk et al, 2005; Marcu et al, 2006;
Liu et al, 2006). It is believed that these models
can improve the quality of SMT significantly. Com-
pared with phrase-based models, syntax-based mod-
els lead to better reordering and higher flexibility
by introducing hierarchical structures and variables
which make syntax-based models capable of hierar-
chical reordering and generalization. Due to these
advantages, syntax-based approaches are becoming
an active area of research in machine translation.
In this paper, we propose a novel model based on
dependency structures: Dependency Treelet String
Correspondence Model (DTSC). The DTSC model
maps source dependency structures to target strings.
It just needs a source language parser. In contrast to
the work by Lin (2004) and by Quirk et al (2005),
the DTSC model does not need to generate target
language dependency structures using source struc-
tures and word alignments. On the source side, we
extract treelets which are any connected subgraphs
and consistent with word alignments. While on the
target side, we allow the aligned target sequences
to be generalized and discontinuous by introducing
variables and gaps. The variables on the target side
are aligned to the corresponding variables of treelets,
while gaps between words or variables are corre-
sponding to the uncovered nodes which are not in-
cluded by treelets. To complete the translation pro-
cess, we design two basic operations for the decod-
ing: substituting and attaching. Substituting is used
to replace variable nodes which have been already
translated, while attaching is used to attach uncov-
40
ered nodes to treelets.
In the remainder of the paper, we first define de-
pendency treelet string correspondence in section
2 and describe an algorithm for extracting DTSCs
from the parsed and word-aligned corpus in section
3. Then we build our model based on DTSC in sec-
tion 4. The decoding algorithm and related pruning
strategies are introduced in section 5. We also spec-
ify the strategy to integrate phrases into our model
in section 6. In section 7 we evaluate our current
implementation of a simplified version of DTSC for
statistical machine translation. And finally, we dis-
cuss related work and conclude.
2 Dependency Treelet String
Correspondence
A dependency treelet string correspondence pi is a
triple < D,S,A > which describes a translation
pair < D,S > and their alignment A, where D is
the dependency treelet on the source side and S is
the translation string on the target side. < D,S >
must be consistent with the word alignment M of
the corresponding sentence pair
?(i, j) ? M, i ? D ? j ? S
A treelet is defined to be any connected subgraph,
which is similar to the definition in (Quirk et al,
2005). Treelet is more representatively flexible than
subtree which is widely used in models based on
phrase structures (Marcu et al, 2006; Liu et al,
2006). The most important distinction between the
treelet in (Quirk et al, 2005) and ours is that we al-
low variables at positions of subnodes. In our defini-
tion, the root node must be lexicalized but the subn-
odes can be replaced with a wild card. The target
counterpart of a wildcard node in S is also replaced
with a wild card. The wildcards introduced in this
way generalize DTSC to match dependency struc-
tures with the same head word but with different
modifiers or arguments.
Another unique feature of our DTSC is that we al-
low target strings with gaps between words or wild-
cards. Since source treelets may not cover all subn-
odes, the uncovered subnodes will generate a gap as
its counterpart on the target side. A sequence of con-
tinuous gaps will be merged to be one gap and gaps
at the beginning and the end of S will be removed
automatically.
??
eeeeeee
?
?
?
? ??eeeeeee
s s
s s
s s
s s??
??
? ?eeeeeee
?
?
?
?
]]]]]]]]]]
the conference cooperation of the ?
??
eeeeeeebbbbbbbbbbbbb
s s
s s
s s
s s?1
w w
w w
? YYYYYYY
SSS
SSS
S ?
?2 ]]]]]]]]]]?1 keep a G with the ?2
Figure 1: DTSC examples. Note that ? represents
variable and G represents gap.
Gap can be considered as a special kind of vari-
able whose counterpart on the source side is not
present. This makes the model more flexible to
match more partial dependency structures on the
source side. If only variables can be used, the model
has to match subtrees rather than treelets on the
source side. Furthermore, the positions of variables
on the target side are fixed so that some reorderings
related with them can be recorded in DTSC. The po-
sitions of gaps on the target side, however, are not
fixed until decoding. The presence of one gap and
its position can not be finalized until attaching op-
eration is performed. The introduction of gaps and
the related attaching operation in decoding is the
most important distinction between our model and
the previous syntax-based models.
Figure 1 shows several different DTSCs automat-
ically extracted from our training corpus. The top
left DTSC is totally lexicalized, while the top right
DTSC has one variable and the bottom has two vari-
ables and one gap. In the bottom DTSC, note that
the node ? which is aligned to the gap G of the
target string is an uncovered node and therefore not
included in the treelet actually. Here we just want
to show there is an uncovered node aligned with the
gap G.
Each node at the source treelet has three attributes
1. The head word
2. The category, i.e. the part of speech of the head
word
3. The node order which specifies the local order
of the current node relative to its parent node.
41
??/VV
eeeeeeebbbbbbbbbbbbbb ]]]]]]]]]]]]]]]]]]]]]
?
?
?
?
?
??/VV
?
?
?
DD
DD
D ?/P YYYYYYY
XXXXX
XXXXX
XXXX
??/NN
eeeeeee
z z
z z
z
????/NR
\\\\\\\\\\
\\\\ ??/NN
j j j
j??
go on providingfinancial aid to Palestine
1 2 3 4 5 6 7
Figure 2: An example dependency tree and its align-
ments
Note that the node order is defined at the context of
the extracted treelets but not the context of the orig-
inal tree. For example, the attributes for the node?
in the bottom DTSC of Figure 1 are {?, P, -1}. For
two treelets, if and only if their structures are iden-
tical and each corresponding nodes share the same
attributes, we say they are matched.
3 Extracting DTSCs
To extract DTSCs from the training corpus, firstly
the corpus must be parsed on the source side and
aligned at the word level. The source structures pro-
duced by the parser are unlabelled, ordered depen-
dency trees with each word annotated with a part-of-
speech. Figure 2 shows an example of dependency
tree really used in our extractor.
When the source language dependency trees and
word alignments between source and target lan-
guages are obtained, the DTSC extraction algorithm
runs in two phases along the dependency trees and
alignments. In the first step, the extractor annotates
each node with specific attributes defined in section
3.1. These attributes are used in the second step
which extracts all possible DTSCs rooted at each
node recursively.
3.1 Node annotation
For each source dependency node n, we define three
attributes: word span, node span and crossed.
Word span is defined to be the target word sequence
aligned with the head word of n, while node span is
defined to be the closure of the union of node spans
of all subnodes of n and its word span. These two at-
tributes are similar to those introduced by Lin (Lin,
2004). The third attribute crossed is an indicator that
has binary values. If the node span of n overlaps
the word span of its parent node or the node span
of its siblings, the crossed indicator of n is 1 and
n is therefore a crossed node, otherwise the crossed
indicator is 0 and n is a non-crossed node. Only
non-crossed nodes can generate DTSCs because the
target word sequence aligned with the whole subtree
rooted at it does not overlap any other sequences and
therefore can be extracted independently.
For the dependency tree and its alignments shown
in Figure 2, only the node ?? is a crossed node
since its node span ([4,5]) overlaps the word span
([5,5]) of its parent node??.
3.2 DTSCs extraction
The DTSC extraction algorithm (shown in Figure 3)
runs recursively. For each non-crossed node, the al-
gorithm generates all possible DTSCs rooted at it by
combining DTSCs from some subsets of its direct
subnodes. If one subnode n selected in the com-
bination is a crossed node, all other nodes whose
word/node spans overlap the node span of n must be
also selected in this combination. This kind of com-
bination is defined to be consistent with the word
alignment because the DTSC generated by this com-
bination is consistent with the word alignment. All
DTSCs generated in this way will be returned to the
last call and outputted. For each crossed node, the
algorithm generates pseudo DTSCs1 using DTSCs
from all of its subnodes. These pseudo DTSCs will
be returned to the last call but not outputted.
During the combination of DTSCs from subnodes
into larger DTSCs, there are two major tasks. One
task is to generate the treelet using treelets from
subnodes and the current node. This is a basic tree
generation operation. It is worth mentioning that
some non-crossed nodes are to be replaced with a
wild card so the algorithm can learn generalized
DTSCs described in section 2. Currently, we re-
place any non-crossed node alone or together with
their sibling non-crossed nodes. The second task
is to combine target strings. The word sequences
aligned with uncovered nodes will be replaced with
a gap. The word sequences aligned with wildcard
nodes will be replaced with a wild card.
If a non-crossed node n has m direct subnodes,
all 2m combinations will be considered. This will
generate a very large number of DTSCs, which is
1Some words in the target string are aligned with nodes
which are not included in the source treelet.
42
DTSCExtractor(Dnode n)
< := ? (DTSC container of n)
for each subnode k of n do
R := DTSCExtractor(k)
L := L?R
end for
if n.crossed! = 1 and there are no subnodes whose span
overlaps the word span of n then
Create a DTSC pi =< D,S,A > where the dependency
treelet D only contains the node n (not including any chil-
dren of it)
output pi
for each combination c of n?s subnodes do
if c is consistent with the word alignment then
Generate all DTSCs R by combining DTSCs (L)
from the selected subnodes with the current node n
< := <?R
end if
end for
output <
return <
else if n.crossed == 1 then
Create pseudo DTSCs P by combining all DTSCs from
n?s all subnodes.
< := <?P
return <
end if
Figure 3: DTSC Extraction Algorithm.
undesirable for training and decoding. Therefore we
filter DTSCs according to the following restrictions
1. If the number of direct subnodes of node n is
larger than 6, we only consider combining one
single subnode with n each time because in this
case reorderings of subnodes are always mono-
tone.
2. On the source side, the number of direct subn-
odes of each node is limited to be no greater
than ary-limit; the height of treelet D is limited
to be no greater than depth-limit.
3. On the target side, the length of S (including
gaps and variables) is limited to be no greater
than len-limit; the number of gaps in S is lim-
ited to be no greater than gap-limit.
4. During DTSC combination, the DTSCs from
each subnode are sorted by size (in descending
order). Only the top comb-limit DTSCs will be
selected to generate larger DTSCs.
As an example, for the dependency tree and its
alignments in Figure 2, all DTSCs extracted by the
Treelet String
(??/VV/0) go on
(????/NR/0) Palestine
(?/P/0) to
(?/P/0 (????/NR/1)) to Palestine
(?/P/0 (?/1)) to ?
(??/NN/0 (??/NN/-1)) financial aid
(??/VV/0) providing
(??/VV/0 (?/1)) providing ?
(??/VV/0 (?/-1)) providing G ?
(??/VV/0 (??/VV/-1)) go on providing
(??/VV/0 (?/-1)) ? providing
(??/VV/0 (?1/-1) (?2/1)) providing ?2 ?1
(??/VV/0 (?1/-1 ) (?2/1)) ?1 providing ?2
Table 1: Examples of DTSCs extracted from Figure
2. Alignments are not shown here because they are
self-evident.
algorithm with parameters { ary-limit = 2, depth-
limit = 2, len-limit = 3, gap-limit = 1, comb-limit
= 20 } are shown in the table 1.
4 The Model
Given an input dependency tree, the decoder gen-
erates translations for each dependency node in
bottom-up order. For each node, our algorithm will
search all matched DTSCs automatically learned
from the training corpus by the way mentioned in
section 3. When the root node is traversed, the trans-
lating is finished. This complicated procedure in-
volves a large number of sequences of applications
of DTSC rules. Each sequence of applications of
DTSC rules can derive a translation.
We define a derivation ? as a sequence of appli-
cations of DTSC rules, and let c(?) and e(?) be the
source dependency tree and the target yield of ?, re-
spectively. The score of ? is defined to be the prod-
uct of the score of the DTSC rules used in the trans-
lation, and timed by other feature functions:
?(?) =
?
i
?(i) ? plm(e)?lm ? exp(??apA(?)) (1)
where ?(i) is the score of the ith application of
DTSC rules, plm(e) is the language model score,
and exp(??apA(?)) is the attachment penalty,
where A(?) calculates the total number of attach-
ments occurring in the derivation ?. The attach-
ment penalty gives some control over the selection
of DTSC rules which makes the model prefer rules
43
with more nodes covered and therefore less attach-
ing operations involved.
For the score of DTSC rule pi, we define it as fol-
lows:
?(pi) =
?
j
fj(pi)?j (2)
where the fj are feature functions defined on DTSC
rules. Currently, we used features proved to be ef-
fective in phrase-based SMT, which are:
1. The translation probability p(D|S).
2. The inverse translation probability p(S|D).
3. The lexical translation probability plex(D|S)
which is computed over the words that occur
on the source and target sides of a DTSC rule
by the IBM model 1.
4. The inverse lexical translation probability
plex(S|D) which is computed over the words
that occur on the source and target sides of a
DTSC rule by the IBM model 1.
5. The word penalty wp.
6. The DTSC penalty dp which allows the model
to favor longer or shorter derivations.
It is worth mentioning how to integrate the N-
gram language mode into our DTSC model. During
decoding, we have to encounter many partial transla-
tions with gaps and variables. For these translations,
firstly we only calculate the language model scores
for word sequences in the translations. Later we up-
date the scores when gaps are removed or specified
by attachments or variables are substituted. Each up-
dating involves merging two neighbor substrings sl
(left) and sr (right) into one bigger string s. Let the
sequence of n ? 1 (n is the order of N-gram lan-
guage model used) rightmost words of sl be srl and
the sequence of n?1 leftmost words of sr be slr. we
have:
LM(s) = LM(sl) + LM(sr) + LM(srl slr)
?LM(srl )? LM(slr) (3)
where LM is the logarithm of the language model
probability. We only need to compute the increment
of the language model score:
4LM = LM(srl slr)? LM(srl )? LM(slr) (4)
for each node n of the input tree T , in bottom-up order do
Get al matched DTSCs rooted at n
for each matched DTSC pi do
for each wildcard node n? in pi do
Substitute the corresponding wildcard on the target
side with translations from the stack of n?
end for
for each uncovered node n@ by pi do
Attach the translations from the stack of n@ to the
target side at the attaching point
end for
end for
end for
Figure 4: Chart-style Decoding Algorithm for the
DTSC Model.
Melamed (2004) also used a similar way to integrate
the language model.
5 Decoding
Our decoding algorithm is similar to the bottom-up
chart parsing. The distinction is that the input is a
tree rather than a string and therefore the chart is in-
dexed by nodes of the tree rather than spans of the
string. Also, several other tree-based decoding al-
gorithms introduced by Eisner (2003), Quirk et al
(2005) and Liu et al (2006) can be classified as the
chart-style parsing algorithm too.
Our decoding algorithm is shown in Figure 4.
Given an input dependency tree, firstly we generate
the bottom-up order by postorder transversal. This
order guarantees that any subnodes of node n have
been translated before node n is done. For each
node n in the bottom-up order, all matched DTSCs
rooted at n are found, and a stack is also built for it to
store the candidate translations. A DTSC pi is said to
match the input dependency subtree T rooted at n if
and only if there is a treelet rooted at n that matches
2 the treelet of pi on the source side.
For each matched DTSC pi, two operations will
be performed on it. The first one is substituting
which replaces a wildcard node with the correspond-
ing translated node. The second one is attaching
which attaches an uncovered node to pi. The two op-
erations are shown in Figure 5. For each wildcard
node n?, translations from the stack of it will be se-
lected to replace the corresponding wildcard on the
2The words, categories and orders of each corresponding
nodes are matched. Please refer to the definition of matched
in section 2.
44
(a) A
eeeeeee YYYYYYYB
eeeeeee
? + D
C ? De
?e Ae Be Ce
Substitute ?
(b) A
eeeeeee YYYYYYYB
eeeeeee
D + E
C ? Ee
De Ae Be Ce
Attach ?
(c) A
eeeeeee YYYYYYYB
eeeeeee YYYYYYY
D
C E
De Ae Be Ee Ce
Figure 5: Substituting and attaching operations for
decoding. Xe is the translation of X . Node that ? is
a wildcard node to be substituted and node ? is an
uncovered node to be attached.
target side and the scores of new translations will be
calculated according to our model. For each uncov-
ered node n@, firstly we determine where transla-
tions from the stack of n@ should be attached on the
target side. There are several different mechanisms
for choosing attaching points. Currently, we imple-
ment a heuristic way: on the source side, we find the
node n@p which is the nearest neighbor of n@ from
its parent and sibling nodes, then the attaching point
is the left/right of the counterpart of n@p on the target
side according to their relative order. As an example,
see the uncovered node ? in Figure 5. The nearest
node to it is node B. Since node ? is at the right
of node B, the attaching point is the right of Be.
One can search all possible points using an ordering
model. And this ordering model can also use infor-
mation from gaps on the target side. We believe this
ordering model can improve the performance and let
it be one of directions for our future research.
Note that the gaps on the target side are not neces-
sarily attaching points in our current attaching mech-
anism. If they are not attaching point, they will be
removed automatically.
The search space of the decoding algorithm is
very large, therefore some pruning techniques have
to be used. To speed up the decoder, the following
pruning strategies are adopted.
1. Stack pruning. We use three pruning ways.
The first one is recombination which converts
the search to dynamic programming. When
two translations in the same stack have the
same w leftmost/rightmost words, where w de-
pends on the order of the language model, they
will be recombined by discarding the transla-
tion with lower score. The second one is the
threshold pruning which discards translations
that have a score worse than stack-threshold
times the best score in the same stack. The
last one is the histogram pruning which only
keeps the top stack-limit best translations for
each stack.
2. Node pruning. For each node, we only keep
the top node-limit matched DTSCs rooted at
that node, as ranked by the size of source
treelets.
3. Operation pruning. For each operation, sub-
stituting and attaching, the decoding will gen-
erate a large number of partial translations3
for the current node. We only keep the top
operation-limit partial translations each time
according to their scores.
6 Integrating Phrases
Although syntax-based models are good at dealing
with hierarchical reordering, but at the local level,
translating idioms and similar complicated expres-
sions can be a problem. However, phrase-based
models are good at dealing with these translations.
Therefore, integrating phrases into the syntax-based
models can improve the performance (Marcu et al,
2006; Liu et al, 2006). Since our DTSC model is
based on dependency structures and lexicalized nat-
urally, DTSCs are more similar to phrases than other
translation units based on phrase structures. This
means that phrases will be easier to be integrated
into our model.
The way to integrate phrases is quite straightfor-
ward: if there is a treelet rooted at the current node,
3There are wildcard nodes or uncovered nodes to be han-
dled.
45
of which the word sequence is continuous and iden-
tical to the source of some phrase, then a phrase-
style DTSC will be generated which uses the target
string of the phrase as its own target. The procedure
is finished during decoding. In our experiments, in-
tegrating phrases improves the performance greatly.
7 Current Implementation
To test our idea, we implemented the dependency
treelet string correspondence model in a Chinese-
English machine translation system. The current im-
plementation in this system is actually a simplified
version of the DTSC model introduced above. In
this version, we used a simple heuristic way for the
operation of attaching rather than a sophisticated sta-
tistical model which can learn ordering information
from the training corpus. Since dependency struc-
tures are more?flattened? compared with phrasal
structures, there are many subnodes which will not
be covered even by generalized matched DTSCs.
This means the attaching operation is very common
during decoding. Therefore better attaching model
which calculates the best point for attaching , we be-
lieve, will improve the performance greatly and is a
major goal for our future research.
To obtain the dependency structures of the source
side, one can parse the source sentences with a de-
pendency parser or parse them with a phrasal struc-
ture parser and then convert the phrasal structures
into dependency structures. In our experiments we
used a Chinese parser implemented by Xiong et
al. (2005) which generates phrasal structures. The
parser was trained on articles 1-270 of Penn Chinese
Treebank version 1.0 and achieved 79.4% (F1 mea-
sure). We then converted the phrasal structure trees
into dependency trees using the way introduced by
Xia (1999).
To obtain the word alignments, we use the way
of Koehn et al (2005). After running GIZA++
(Och and Ney, 2000) in both directions, we apply
the ?grow-diag-final? refinement rule on the in-
tersection alignments for each sentence pair.
The training corpus consists of 31, 149 sentence
pairs with 823K Chinese words and 927K English
words. For the language model, we used SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) to train a
trigram model with modified Kneser-Ney smooth-
Systems BLEU-4
PB 20.88 ? 0.87
DTSC 20.20 ? 0.81
DTSC + phrases 21.46 ? 0.83
Table 2: BLEU-4 scores for our system and a
phrase-based system.
ing on the 31, 149 English sentences. We selected
580 short sentences of length at most 50 characters
from the 2002 NIST MT Evaluation test set as our
development corpus and used it to tune ?s by max-
imizing the BLEU score (Och, 2003), and used the
2005 NIST MT Evaluation test set as our test corpus.
From the training corpus, we learned 2, 729,
964 distinct DTSCs with the configuration { ary-
limit = 4, depth-limit = 4, len-limit = 15, gap-limit
= 2, comb-limit = 20 }. Among them, 160,694
DTSCs are used for the test set. To run our de-
coder on the development and test set, we set stack-
thrshold = 0.0001, stack-limit = 100, node-limit =
100, operation-limit = 20.
We also ran a phrase-based system (PB) with a
distortion reordering model (Xiong et al, 2006) on
the same corpus. The results are shown in table 2.
For all BLEU scores, we also show the 95% confi-
dence intervals computed using Zhang?s significant
tester (Zhang et al, 2004) which was modified to
conform to NIST?s definition of the BLEU brevity
penalty. The BLEU score of our current system with
the DTSC model is lower than that of the phrase-
based system. However, with phrases integrated, the
performance is improved greatly, and the new BLEU
score is higher than that of the phrase-based SMT.
This difference is significant according to Zhang?s
tester. This result can be improved further using a
better parser (Quirk et al, 2006) or using a statisti-
cal attaching model.
8 Related Work
The DTSC model is different from previous work
based on dependency grammars by Eisner (2003),
Lin (2004), Quirk et al (2005), Ding et al (2005)
since they all deduce dependency structures on the
target side. Among them, the most similar work is
(Quirk et al, 2005). But there are still several major
differences beyond the one mentioned above. Our
46
treelets allow variables at any non-crossed nodes and
target strings allow gaps, which are not available in
(Quirk et al, 2005). Our language model is calcu-
lated during decoding while Quirk?s language model
is computed after decoding because of the complex-
ity of their decoding.
The DTSC model is also quite distinct from pre-
vious tree-string models by Marcu et al (2006)
and Liu et al (2006). Firstly, their models are
based on phrase structure grammars. Secondly, sub-
trees instead of treelets are extracted in their mod-
els. Thirdly, it seems to be more difficult to integrate
phrases into their models. And finally, our model al-
low gaps on the target side, which is an advantage
shared by (Melamed, 2004) and (Simard, 2005).
9 Conclusions and Future Work
We presented a novel syntax-based model using
dependency trees on the source side?dependency
treelet string correspondence model?for statistical
machine translation. We described an algorithm to
learn DTSCs automatically from the training corpus
and a chart-style algorithm for decoding.
Currently, we implemented a simple version of
the DTSC model. We believe that our performance
can be improved greatly using a more sophisticated
mechanism for determining attaching points. There-
fore the most important future work should be to de-
sign a better attaching model. Furthermore, we plan
to use larger corpora for training and n-best depen-
dency trees for decoding, which both are helpful for
the improvement of translation quality.
Acknowledgements
This work was supported by National Natural Sci-
ence Foundation of China, Contract No. 60603095
and 60573188.
References
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of ACL.
Yuan Ding and Martha Palmer. 2005. Machine Translation Us-
ing Probabilistic Synchronous Dependency Insertion Gram-
mars. In Proceedings of ACL.
Jason Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In Proceedings of ACL.
Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris
Callison-Burch, Miles Osborne and David Talbot. 2005.
Edinburgh System Description for the 2005 IWSLT Speech
Translation Evaluation. In International Workshop on Spo-
ken Language Translation.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. SPMT: Statistical Machine Translation with
Syntactified Target Language Phraases. In Proceedings of
EMNLP.
I. Dan Melamed. 2004. Algorithms for Syntax-Aware Statisti-
cal Machine Translation. In Proceedings of the Conference
on Theoretical and Methodological Issues in Machine Trans-
lation (TMI), Baltimore, MD.
Dekang Lin. 2004. A path-based transfer model for machine
translation. In Proceedings of COLING.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-String
Alignment Template for Statistical Machine Translation. In
Proceedings of ACL.
Franz Josef Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of ACL.
Franz Josef Och and Hermann Ney. 2000. Improved statistical
alignment models. In Proceedings of ACL.
Chris Quirk, Arul Menezes and Colin Cherry. 2005. Depen-
dency Treelet Translation: Syntactically Informed Phrasal
SMT. In Proceedings of ACL.
Chris Quirk and Simon Corston-Oliver. 2006. The impact of
parse quality on syntactically-informed statistical machine
translation. In Proceedings of EMNLP, Sydney, Australia.
Michel Simard, Nicola Cancedda, Bruno Cavestro, Marc
Dymetman, Eric Gaussier, Cyril Goutte, Kenji Yamada.
2005. Translating with non-contiguous phrases. In Proceed-
ings of HLT-EMNLP.
Andreas Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proceedings of International Conference on
Spoken Language Processing, volume 2, pages 901-904.
Fei Xia. 1999. Automatic Grammar Generation from Two Dif-
ferent Perspectives. PhD thesis, University of Pennsylvania.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum
Entropy Based Phrase Reordering Model for Statistical Ma-
chine Translation. In Proceedings of COLING-ACL, Sydney,
Australia.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, Yueliang
Qian. 2005. Parsing the Penn Chinese Treebank with Se-
mantic Knowledge. In Proceedings of IJCNLP, Jeju Island,
Korea.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement do we
need to have a better system? In Proceedings of LREC,
pages 2051? 2054.
47
