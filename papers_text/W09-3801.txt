Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 1?12,
Paris, October 2009. c?2009 Association for Computational Linguistics
Parsing Algorithms based on Tree Automata
Andreas Maletti
Departament de Filologies Roma`niques
Universitat Rovira i Virgili, Tarragona, Spain
andreas.maletti@urv.cat
Giorgio Satta
Department of Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Abstract
We investigate several algorithms related
to the parsing problem for weighted au-
tomata, under the assumption that the in-
put is a string rather than a tree. This
assumption is motivated by several natu-
ral language processing applications. We
provide algorithms for the computation of
parse-forests, best tree probability, inside
probability (called partition function), and
prefix probability. Our algorithms are ob-
tained by extending to weighted tree au-
tomata the Bar-Hillel technique, as defined
for context-free grammars.
1 Introduction
Tree automata are finite-state devices that recog-
nize tree languages, that is, sets of trees. There
is a growing interest nowadays in the natural
language parsing community, and especially in
the area of syntax-based machine translation, for
probabilistic tree automata (PTA) viewed as suit-
able representations of grammar models. In fact,
probabilistic tree automata are generatively more
powerful than probabilistic context-free gram-
mars (PCFGs), when we consider the latter as de-
vices that generate tree languages. This difference
can be intuitively understood if we consider that a
computation by a PTA uses hidden states, drawn
from a finite set, that can be used to transfer infor-
mation within the tree structure being recognized.
As an example, in written English we can em-
pirically observe different distributions in the ex-
pansion of so-called noun phrase (NP) nodes, in
the contexts of subject and direct-object positions,
respectively. This can be easily captured using
some states of a PTA that keep a record of the dif-
ferent contexts. In contrast, PCFGs are unable to
model these effects, because NP node expansion
should be independent of the context in the deriva-
tion. This problem for PCFGs is usually solved by
resorting to so-called parental annotations (John-
son, 1998), but this, of course, results in a different
tree language, since these annotations will appear
in the derived tree.
Most of the theoretical work on parsing and es-
timation based on PTA has assumed that the in-
put is a tree (Graehl et al, 2008), in accordance
with the very definition of these devices. How-
ever, both in parsing as well as in machine transla-
tion, the input is most often represented as a string
rather than a tree. When the input is a string, some
trick is applied to map the problem back to the
case of an input tree. As an example in the con-
text of machine translation, assume a probabilistic
tree transducer T as a translation model, and an
input string w to be translated. One can then inter-
mediately construct a tree automaton Mw that rec-
ognizes the set of all possible trees that have w as
yield, with internal nodes from the input alphabet
of T . This automaton Mw is further transformed
into a tree transducer implementing a partial iden-
tity translation, and such a transducer is composed
with T (relation composition). This is usually
called the ?cascaded? approach. Such an approach
can be easily applied also to parsing problems.
In contrast with the cascaded approach above,
which may be rather inefficient, in this paper we
investigate a more direct technique for parsing
strings based on weighted and probabilistic tree
automata. We do this by extending to weighted
tree automata the well-known Bar-Hillel construc-
tion defined for context-free grammars (Bar-Hillel
et al, 1964) and for weighted context-free gram-
mars (Nederhof and Satta, 2003). This provides
an abstract framework under which several pars-
ing algorithms can be directly derived, based on
weighted tree automata. We discuss several appli-
cations of our results, including algorithms for the
computation of parse-forests, best tree probability,
inside probability (called partition function), and
prefix probability.
1
2 Preliminary definitions
Let S be a nonempty set and ? be an associative
binary operation on S. If S contains an element 1
such that 1 ? s = s = s ? 1 for every s ? S, then
(S, ?, 1) is a monoid. A monoid (S, ?, 1) is com-
mutative if the equation s1 ? s2 = s2 ? s1 holds
for every s1, s2 ? S. A commutative semiring
(S,+, ?, 0, 1) is a nonempty set S on which a bi-
nary addition + and a binary multiplication ? have
been defined such that the following conditions are
satisfied:
? (S,+, 0) and (S, ?, 1) are commutative
monoids,
? ? distributes over + from both sides, and
? s ? 0 = 0 = 0 ? s for every s ? S.
A weighted string automaton, abbreviated WSA,
(Schu?tzenberger, 1961; Eilenberg, 1974) is a sys-
tem M = (Q,?,S, I, ?, F ) where
? Q is a finite alphabet of states,
? ? is a finite alphabet of input symbols,
? S = (S,+, ?, 0, 1) is a semiring,
? I : Q? S assigns initial weights,
? ? : Q???Q? S assigns a weight to each
transition, and
? F : Q? S assigns final weights.
We now proceed with the semantics of M . Let
w ? ?? be an input string of length n. For each
integer i with 1 ? i ? n, we write w(i) to denote
the i-th character of w. The set Pos(w) of posi-
tions of w is {i | 0 ? i ? n}. A run of M on w
is a mapping r : Pos(w) ? Q. We denote the set
of all such runs by RunM (w). The weight of a
run r ? RunM (w) is
wtM (r) =
n?
i=1
?(r(i? 1), w(i), r(i)) .
We assume the right-hand side of the above equa-
tion evaluates to 1 in case n = 0. The WSA M
recognizes the mapping M : ?? ? S, which is
defined for every w ? ?? of length n by1
M(w) = ?
r?RunM (w)
I(r(0)) ?wtM (r) ?F (r(n)) .
In order to define weighted tree automata (Bers-
tel and Reutenauer, 1982; E?sik and Kuich, 2003;
Borchardt, 2005), we need to introduce some addi-
tional notation. Let ? be a ranked alphabet, that
1We overload the symbolM to denote both an automaton
and its recognized mapping. However, the intended meaning
will always be clear from the context.
is, an alphabet whose symbols have an associated
arity. We write ?k to denote the set of all k-ary
symbols in ?. We use a special symbol e ? ?0
to syntactically represent the empty string ?. The
set of ?-trees, denoted by T?, is the smallest set
satisfying both of the following conditions
? for every ? ? ?0, the single node labeled ?,
written ?(), is a tree of T?,
? for every ? ? ?k with k ? 1 and for every
t1, . . . , tk ? T?, the tree with a root node la-
beled ? and trees t1, . . . , tk as its k children,
written ?(t1, . . . , tk), belongs to T?.
As a convention, throughout this paper we assume
that ?(t1, . . . , tk) denotes ?() if k = 0. The size
of the tree t ? T?, written |t|, is defined as the
number of occurrences of symbols from ? in t.
Let t = ?(t1, . . . , tk). The yield of t is recur-
sively defined by
yd(t) =
?
??
??
? if ? ? ?0 \ {e}
? if ? = e
yd(t1) ? ? ? yd(tk) otherwise.
The set of positions of t, denoted by Pos(t), is
recursively defined by
Pos(?(t1, . . . , tk)) =
{?} ? {iw | 1 ? i ? k,w ? Pos(ti)} .
Note that |t| = |Pos(t)| and, according to our con-
vention, when k = 0 the above definition provides
Pos(?()) = {?}. We denote the symbol of t at
position w by t(w) and its rank by rkt(w).
A weighted tree automaton (WTA) is a system
M = (Q,?,S, ?, F ) where
? Q is a finite alphabet of states,
? ? is a finite ranked alphabet of input symbols,
? S = (S,+, ?, 0, 1) is a semiring,
? ? is an indexed family (?k)k?N of mappings
?k : ?k ? SQ?Qk , and
? F : Q? S assigns final weights.
In the above definition, Qk is the set of all strings
over Q having length k, with Q0 = {?}. Fur-
ther note that SQ?Qk is the set of all matrices
with elements in S, row index set Q, and column
index set Qk. Correspondingly, we will use the
common matrix notation and write instances of ?
in the form ?k(?)q0,q1???qk . Finally, we assume
q1 ? ? ? qk = ? if k = 0.
We define the semantics also in terms of runs.
Let t ? T?. A run of M on t is a mapping
r : Pos(t)? Q. We denote the set of all such runs
2
by RunM (t). The weight of a run r ? RunM (t)
is
wtM (r) =
?
w?Pos(t)
rkt(w)=k
?k(t(w))r(w),r(w1)???r(wk) .
Note that, according to our convention, the string
r(w1) ? ? ? r(wk) denotes ? when k = 0. The
WTA M recognizes the mapping M : T? ? S,
which is defined by
M(t) = ?
r?RunM (t)
wtM (r) ? F (r(?))
for every t ? T?. We say that t is recognized
by M if M(t) 6= 0.
In our complexity analyses, we use the follow-
ing measures. The size of a transition (p, ?, q) in
(the domain of ? in) a WSA is |p?q| = 3. The size
of a transition in a WTA, viewed as an instance
(?, q0, q1 ? ? ? qk) of some mapping ?k, is defined
as |?q0 ? ? ? qk|, that is, the rank of the input symbol
occurring in the transition plus two. Finally, the
size |M | of an automaton M (WSA or WTA) is
defined as the sum of the sizes of its nonzero tran-
sitions. Note that this does not take into account
the size of the representation of the weights.
3 Binarization
We introduce in this section a specific transfor-
mation of WTA, called binarization, that reduces
the transitions of the automaton to some normal
form in which no more than three states are in-
volved. This transformation maps the set of rec-
ognized trees into a special binary form, in such a
way that the yields of corresponding trees and their
weights are both preserved. We use this transfor-
mation in the next section in order to guarantee
the computational efficiency of the parsing algo-
rithm we develop. The standard ?first-child, next-
sibling? binary encoding for trees (Knuth, 1997)
would eventually result in a transformed WTA of
quadratic size. To obtain instead a linear size
transformation, we introduce a slightly modified
encoding (Ho?gberg et al, 2009, Section 4), which
is inspired by (Carme et al, 2004) and the classical
currying operation.
Let ? be a ranked alphabet and assume a
fresh symbol @ /? ? (corresponding to the ba-
sic list concatenation operator). Moreover, let
? = ?2 ? ?1 ? ?0 be the ranked alphabet such
that ?2 = {@}, ?1 = ?k?1 ?k, and ?0 = ?0. In
?
?
?
?
? ?
? ?
?
?
@
?
?
?
@
? @
?
@
? ?
?
Figure 1: Input tree t and encoded tree enc(t).
words, all the original non-nullary symbols from
? are now unary, @ is binary, and the original
nullary symbols from ? have their rank preserved.
We encode each tree of T? as a tree of T? as fol-
lows:
? enc(?) = ?() for every ? ? ?0,
? enc(?(t)) = ?(enc(t)) for every ? ? ?1 and
t ? T?, and
? for k ? 2, ? ? ?k, and t1, . . . , tk ? T?
enc(?(t1, . . . , tk)) =
?(@(enc(t1), . . .@(enc(tk?1), enc(tk)) ? ? ? )).
An example of the above encoding is illustrated
in Figure 1. Note that |enc(t)| ? O(|t|) for every
t ? T?. Furthermore, t can be easily reconstructed
from enc(t) in linear time.
Definition 1 LetM = (Q,?,S, ?, F ) be a WTA.
The encoded WTA enc(M) is (P,?,S, ??, F ?)
where
P = {[q] | q ? Q} ?
? {[w] |?k(?)q,uw 6= 0, u ? Q?, w ? Q+},
F ?([q]) = F (q) for every q ? Q, and the transi-
tions are constructed as follows:
(i) ??0(?)[q],? = ?0(?)q,? for every ? ? ?0,
(ii) ??1(?)[q],[w] = ?k(?)q,w for every ? ? ?k,
k ? 1, q ? Q, and w ? Qk, and
(iii) ??2(@)[qw],[q][w] = 1 for every [qw] ? P with
|w| ? 1 and q ? Q.
All remaining entries in F ? and ?? are 0. 2
Notice that each transition of enc(M) involves no
more than three states from P . Furthermore, we
have |enc(M)| ? O(|M |). The following result is
rather intuitive (Ho?gberg et al, 2009, Lemma 4.2);
its proof is therefore omitted.
3
Theorem 1 Let M = (Q,?,S, ?, F ) be a WTA,
and let M ? = enc(M). Then M(t) = M ?(enc(t))
for every t ? T?. 2
4 Bar-Hillel construction
The so-called Bar-Hillel construction was pro-
posed in (Bar-Hillel et al, 1964) to show that
the intersection of a context-free language and
a regular language is still a context-free lan-
guage. The proof of the result consisted in an
effective construction of a context-free grammar
Prod(G,N) from a context-free grammar G and
a finite automaton N , such that Prod(G,N) gen-
erates the intersection of the languages generated
by G and N .
It was later recognized that the Bar-Hillel con-
struction constitutes one of the foundations of the
theory of tabular parsing based on context-free
grammars. More precisely, by taking the finite
automaton N to be of some special kind, accept-
ing only a single string, the Bar-Hillel construction
provides a framework under which several well-
known tabular parsing algorithms can easily be de-
rived, that were proposed much later in the litera-
ture.
In this section we extend the Bar-Hillel con-
struction to WTA, with a similar purpose of es-
tablishing an abstract framework under which one
could easily derive parsing algorithms based on
these devices. In order to guarantee computational
efficiency, we avoid here stating the Bar-Hillel
construction for WTA with alphabets of arbitrary
rank. The next result therefore refers to WTA with
alphabet symbols of rank at most 2. These may,
but need not, be automata obtained through the bi-
nary encoding discussed in Section 3.
Definition 2 Let M = (Q,?,S, ?, F ) be a WTA
such that the maximum rank of a symbol in ? is 2,
and let N = (P,?0 \ {e},S, I, ?,G) be a WSA
over the same semiring. We construct the WTA
Prod(M,N) = (P ?Q? P,?,S, ??, F ?)
as follows:
(i) For every ? ? ?2, states p0, p1, p2 ? P , and
states q0, q1, q2 ? Q let
??2(?)(p0,q0,p2),(p0,q1,p1)(p1,q2,p2) = ?2(?)q0,q1q2 .
(ii) For every symbol ? ? ?1, states p0, p1 ? P ,
and states q0, q1 ? Q let
??1(?)(p0,q0,p1),(p0,q1,p1) = ?1(?)q0,q1 .
p0 p2
p0 p1 p1 p2
?
= = =
p0 p1
p0 p1
?
= =
p0 ? p1
?(p0, ?, p1)
p0 e p0
=
Figure 2: Information transport in the first and
third components of the states in our Bar-Hillel
construction.
(iii) For every symbol ? ? ?0, states p0, p1 ? P ,
and q ? Q let
??0(?)(p0,q,p1),? = ?0(?)q,? ? s
where
s =
{
?(p0, ?, p1) if ? 6= e
1 if ? = e and p0 = p1 .
(iv) F ?(p0, q, p1) = I(p0) ?F (q) ?G(p1) for every
p0, p1 ? P and q ? Q.
All remaining entries in ?? are 0. 2
Theorem 2 Let M and N be as in Definition 2,
and let M ? = Prod(M,N). If S is commutative,
thenM ?(t) = M(t) ?N(yd(t)) for every t ? T?.2
PROOF For a state q ? P ? Q ? P , we write qi
to denote its i-th component with i ? {1, 2, 3}.
Let t ? T? and r ? RunM ?(t) be a run of M ?
on t. We call the run r well-formed if for every
w ? Pos(t):
(i) if t(w) = e, then r(w)1 = r(w)3,
(ii) if t(w) /? ?0, then:
(a) r(w)1 = r(w1)1,
(b) r(w rkt(w))3 = r(w)3, and
(c) if rkt(w) = 2, then r(w1)3 = r(w2)1.
Note that no conditions are placed on the second
components of the states in r. We try to illustrate
the conditions in Figure 2.
A standard proof shows that wtM ?(r) = 0 for
all runs r ? RunM ?(t) that are not well-formed.
We now need to map runs of M ? back into ?cor-
responding? runs for M and N . Let us fix some
t ? T? and some well-formed run r ? RunM ?(t).
4
We define the run piM (r) ? RunM (t) by letting
piM (r)(w) = r(w)2,
for every w ? Pos(t). Let {w1, . . . , wn} =
{w? | w? ? Pos(t), t(w?) ? ?0 \ {e}}, with
w1 < ? ? ? < wn according to the lexico-
graphic order on Pos(t). We also define the run
piN (r) ? RunN (yd(t)) by letting
piN (r)(i? 1) = r(wi)1,
for every 1 ? i < n, and
piN (r)(n) = r(wn)3 .
Note that conversely every run of M on t and ev-
ery run of N on yd(t) yield a unique run of M ?
on t.
Now, we claim that
wtM ?(r) = wtM (piM (r)) ? wtN (piN (r))
for every well-formed run r ? RunM ?(t). To
prove the claim, let t = ?(t1, . . . , tk) for some
? ? ?k, k ? 2, and t1, . . . , tk ? T?. Moreover,
for every 1 ? i ? k let ri(w) = r(iw) for every
w ? Pos(ti). Note that ri ? RunM ?(ti) and that
ri is well-formed for every 1 ? i ? k.
For the induction base, let ? ? ?0; we can write
wtM ?(r)
= ??0(?)r(?),?
=
{
?0(?)r(?)2,? ? ?(r(?)1, ?, r(?)3) if ? 6= e
?0(?)r(?)2,? if ? = e
= wtM (piM (r)) ? wtN (piN (r)) .
In the induction step (i.e., k > 0) we have
wtM ?(r)
= ?
w?Pos(t)
rkt(w)=n
??n(t(w))r(w),r(w1)???r(wn)
= ??k(?)r(?),r(1)???r(k) ?
k?
i=1
wtM ?(ri) .
Using the fact that r is well-formed, commutativ-
ity, and the induction hypothesis, we obtain
= ?k(?)r(?)2,r(1)2???r(k)2 ?
?
k?
i=1
(
wtM (piM (ri)) ? wtN (piN (ri))
)
= wtM (pi2(r)) ? wtN (piN (r)) ,
where in the last step we have again used the fact
that r is well-formed. Using the auxiliary state-
ment wtM ?(r) = wtM (piM (r)) ?wtN (piN (r)), the
main proof now is easy.
M ?(t)
= ?
r?RunM? (t)
wtM ?(r) ? F ?(r(?))
= ?
r?RunM? (t)
r well-formed
wtM (piM (r)) ? wtN (piN (r)) ?
? I(r(?)1) ? F (r(?)2) ?G(r(?)3)
=
( ?
r?RunM (t)
wtM (r) ? F (r(?))
)
?
?
( ?
w=yd(t)
r?RunN (w)
I(r(0)) ? wtN (r) ?G(r(|w|))
)
= M(t) ?N(yd(t)) 
Let us analyze now the computational complex-
ity of a possible implementation of the construc-
tion in Definition 2. In step (i), we could restrict
the computation by considering only those transi-
tions inM satisfying ?2(?)q0,q1q2 6= 0, which pro-
vides a number of choices in O(|M |). Combined
with the choices for the states p0, p1, p2 of N ,
this provides O(|M | ? |P |3) non-zero transitions
in Prod(M,N). This is also a bound on the over-
all running time of step (i). Since we additionally
assume that weights can be multiplied in constant
time, it is not difficult to see that all of the remain-
ing steps can be accommodated within such a time
bound. We thus conclude that the construction in
Definition 2 can be implemented to run in time and
space O(|M | ? |P |3).
5 Parsing applications
In this section we discuss several applications of
the construction presented in Definition 2 that are
relevant for parsing based on WTA models.
5.1 Parse forest
Parsing is usually defined as the problem of con-
structing a suitable representation for the set of all
possible parse trees that are assigned to a given in-
put string w by some grammar model. The set of
all such parse trees is called parse forest. The ex-
tension of the Bar-Hillel construction that we have
5
presented in Section 4 can be easily adapted to ob-
tain a parsing algorithm for WTA models. This is
described in what follows.
First, we should represent the input string w in
a WSA that recognizes the language {w}. Such
an automaton has a state set P = {p0, . . . , p|w|}
and transition weights ?(pi?1, w(i), pi) = 1 for
each i with 1 ? i ? |w|. We also set I(p0) = 1
and F (p|w|) = 1. Setting all the weights to 1 for
a WSA N amounts to ignoring the weights, i.e.,
those weights will not contribute in any way when
applying the Bar-Hillel construction.
Assume now that M is our grammar model,
represented as a WTA. The WTA Prod(M,N)
constructed as in Definition 2 is not necessarily
trim, meaning that it might contain transitions
with non-zero weight that are never used in the
recognition. Techniques for eliminating such use-
less transitions are well-known, see for instance
(Ge?cseg and Steinby, 1984, Section II.6), and can
be easily implemented to run in linear time. Once
Prod(M,N) is trim, we have a device that rec-
ognizes all and only those trees that are assigned
by M to the input string w, and the weights of
those trees are preserved, as seen in Theorem 2.
The WTA Prod(M,N) can then be seen as a rep-
resentation of a parse forest for the input string w,
and we conclude that the construction in Defini-
tion 2, combined with some WTA reduction al-
gorithm, represents a parsing algorithm for WTA
models working in cubic time on the length of the
input string and in linear time on the size of the
grammar model.
More interestingly, from the framework devel-
oped in Section 4, one can also design more effi-
cient parsing algorithms based on WTA. Borrow-
ing from standard ideas developed in the litera-
ture for parsing based on context-free grammars,
one can specialize the construction in Definition 2
in such a way that the number of useless transi-
tions generated for Prod(M,N) is considerably
reduced, resulting in a more efficient construction.
This can be done by adopting some search strat-
egy that guides the construction of Prod(M,N)
using knowledge of the input string w as well as
knowledge about the source model M .
As an example, we can apply step (i) only on de-
mand, that is, we process a transition ??2(?)q0,q1q2
in Prod(M,N) only if we have already computed
non-zero transitions of the form ??k1(?1)q1,w1 and
??k2(?2)q2,w2 , for some ?1 ? ?k1 , w1 ? Qk1 and
?2 ? ?k2 , w2 ? Qk2 where Q is the state set
of Prod(M,N). The above amounts to a bottom-
up strategy that is also used in the Cocke-Kasami-
Younger recognition algorithm for context-free
grammars (Younger, 1967).
More sophisticated strategies are also possible.
For instance, one could adopt the Earley strategy
developed for context-free grammar parsing (Ear-
ley, 1970). In this case, parsing is carried out in
a top-down left-to-right fashion, and the binariza-
tion construction of Section 3 is carried out on the
flight. This has the additional advantage that it
would be possible to use WTA models that are not
restricted to the special normal form of Section 3,
still maintaining the cubic time complexity in the
length of the input string. We do not pursue this
idea any further in this paper, since our main goal
here is to outline an abstract framework for pars-
ing based on WTA models.
5.2 Probabilistic tree automata
Let us now look into specific semirings that are
relevant for statistical natural language process-
ing. The semiring of non-negative real numbers
is R?0 = (R?0,+, ?, 0, 1). For the remainder of
the section, let M = (Q,?,R?0, ?, F ) be a WTA
over R?0. M is convergent if
?
t?T?
M(t) < ?.
We say that M is a probabilistic tree automa-
ton (Ellis, 1971; Magidor and Moran, 1970),
or PTA for short, if ?k(?)q,q1???qk ? [0, 1]
and F (q) ? [0, 1], for every ? ? ?k and
q, q1, . . . , qk ? Q. In other words, in a PTA all
weights are in the range [0, 1] and can be inter-
preted as probabilities. For a PTA M we therefore
write pM (r) = wt(r) and pM (t) = M(t), for
each t ? T? and r ? RunM (t).
A PTA is proper if?q?Q F (q) = 1 and
?
???k,k?0,w?Qk
?k(?)q,w = 1
for every q ? Q. Since the set of symbols is finite,
we could have only required that the sum over all
weights as shown with w ? Qk equals 1 for every
q ? Q and ? ? ?k. A simple rescaling would then
be sufficient to arrive at our notion. Furthermore, a
PTA is consistent if ?t?T? pM (t) = 1. If a PTAis consistent, then pM is a probability distribution
over the set T?.
6
The WTAM is unambiguous if for every input
tree t ? T?, there exists at most one r ? RunM (t)
such that r(?) ? F and wtM (r) 6= 0. In other
words, in an unambiguous WTA, there exists at
most one successful run for each input tree. Fi-
nally, M is in final-state normal form if there ex-
ists a state qS ? Q such that
? F (qS) = 1,
? F (q) = 0 for every q ? Q \ {qS}, and
? ?k(?)q,w = 0 if w(i) = qS for some
1 ? i ? k.
We commonly denote the unique final state by qS .
For the following result we refer the reader
to (Droste et al, 2005, Lemma 4.8) and (Bozapa-
lidis, 1999, Lemma 22). The additional properties
mentioned in the items of it are easily seen.
Theorem 3 For every WTA M there exists an
equivalent WTA M ? in final-state normal form.
? If M is convergent (respectively, proper, con-
sistent), then M ? is such, too.
? If M is unambiguous, then M ? is
also unambiguous and for every
t ? T? and r ? RunM (t) we have
wtM ?(r?) = wtM (r) ? F (r(?)) where
r?(?) = qS and r?(w) = r(w) for every
w ? Pos(t) \ {?}. 2
It is not difficult to see that a proper PTA in
final-state normal form is always convergent.
In statistical parsing applications we use gram-
mar models that induce a probability distribution
on the set of parse trees. In these applications,
there is often the need to visit a parse tree with
highest probability, among those in the parse for-
est obtained from the input sentence. This imple-
ments a form of disambiguation, where the most
likely tree under the given model is selected, pre-
tending that it provides the most likely syntactic
analysis of the input string. In our setting, the
above approach reduces to the problem of ?unfold-
ing? a tree from a PTA Prod(M,N), that is as-
signed the highest probability.
In order to find efficient solutions for the above
problem, we make the following two assumptions.
? M is in final-state normal form. By Theo-
rem 3 this can be achieved without loss of
generality.
? M is unambiguous. This restrictive assump-
tion avoids the so-called ?spurious? ambigu-
ity, that would result in several computations
in the model for an individual parse tree.
It is not difficult to see that PTA satisfying these
1: Function BESTPARSE(M)
2: E ? ?
3: repeat
4: A ? {q |?k(?)q,q1???qk > 0, q /? E ,
q1, . . . , qk ? E}
5: for all q ? A do
6: ?(q)? max
???k,k?0
q1,...,qk?E
?k(?)q,q1???qk ?
k?
i=1
?(qi)
7: E ? E ? {argmax
q?A
?(q)}
8: until qS ? E
9: return ?(qS)
Figure 3: Search algorithm for the most probable
parse in an unambiguous PTAM in final-state nor-
mal form.
two properties are still more powerful than the
probabilistic context-free grammar models that are
commonly used in statistical natural language pro-
cessing.
Once more, we borrow from the literature on
parsing for context-free grammars, and adapt a
search algorithm developed by Knuth (1977); see
also (Nederhof, 2003). The basic idea here is
to generalize Dijkstra?s algorithm to compute the
shortest path in a weighted graph. The search al-
gorithm is presented in Figure 3.
The algorithm takes as input a trim PTA M that
recognizes at least one parse tree. We do not im-
pose any bound on the rank of the alphabet sym-
bols forM . Furthermore,M needs not be a proper
PTA. In order to simplify the presentation, we pro-
vide the algorithm in a form that returns the largest
probability assigned to some tree by M .
The algorithm records into the ?(q) variables
the largest probability found so far for a run that
brings M into state q, and stores these states into
an agenda A. States for which ?(q) becomes opti-
mal are popped from A and stored into a set E .
Choices are made on a greedy base. Note that
when a run has been found leading to an optimal
probability ?(q), from our assumption we know
that the associated tree has only one run that ends
up in state q.
Since E is initially empty (line 2), only weights
satisfying ?0(?)q,? > 0 are considered when line 4
is executed for the first time. Later on (line 7)
the largest probability is selected among all those
that can be computed at this time, and the set E is
populated. As a consequence, more states become
7
available in the agenda in the next iteration, and
new transitions can now be considered. The algo-
rithm ends when the largest probability has been
calculated for the unique final state qS .
We now analyze the computational complexity
of the algorithm in Figure 3. The ?repeat-until?
loop runs at most |Q| times. Entirely reprocess-
ing setA at each iteration would be too expensive.
We instead implement A as a priority heap and
maintain a clock for each weight ?k(?)q,q1???qk ,
initially set to k. Whenever a new optimal proba-
bility ?(q) becomes available through E , we decre-
ment the clock associated with each ?k(?)q,q1???qk
by d, in case d > 0 occurrences of q are found
in the string q1 ? ? ? qk. In this way, at each it-
eration of the ?repeat-until? loop, we can con-
sider only those weights ?k(?)q,q1???qk with asso-
ciated clock of zero, compute new values ?(q),
and update the heap. For each ?k(?)q,q1???qk > 0,
all clock updates and the computation of quan-
tity ?k(?)q,q1???qk ?
?k
i=1 ?(qi) (when the associ-
ated clock becomes zero) both take an amount of
time proportional to the length of the transition
itself. The overall time to execute these opera-
tions is therefore linear in |M |. Accounting for
the heap, the algorithm has overall running time
in O(|M |+ |Q| log|Q|).
The algorithm can be easily adapted to return a
tree having probability ?(qS), if we keep a record
of all transitions selected in the computation along
with links from a selected transition and all of the
previously selected transitions that have caused its
selection. If we drop the unambiguity assump-
tion for the PTA, then the problem of comput-
ing the best parse tree becomes NP-hard, through
a reduction from similar problems for finite au-
tomata (Casacuberta and de la Higuera, 2000). In
contrast, the problem of computing the probability
of all parse trees of a string, also called the inside
probability, can be solved in polynomial time in
most practical cases and will be addressed in Sub-
section 5.4.
5.3 Normalization
Consider the WTA Prod(M,N) obtained as in
Definition 2. If N is a WSA encoding an in-
put string w as in Subsection 5.1 and if M is a
proper and consistent PTA, then Prod(M,N) is
a PTA as well. However, in general Prod(M,N)
will not be proper, nor consistent. Properness and
consistency of Prod(M,N) are convenient in all
those applications where a statistical parsing mod-
ule needs to be coupled with other statistical mod-
ules, in such a way that the composition of the
probability spaces still induces a probability dis-
tribution. In this subsection we deal with the more
general problem of how to transform a WTA that
is convergent into a PTA that is proper and con-
sistent. This process is called normalization. The
normalization technique we propose here has been
previously explored, in the context of probabilis-
tic context-free grammars, in (Abney et al, 1999;
Chi, 1999; Nederhof and Satta, 2003).
We start by introducing some new notions. Let
us assume that M is a convergent WTA. For every
q ? Q, we define
wtM (q) =
?
t?T?,r?RunM (t)
r(?)=q
wtM (r) .
Note that quantity wtM (q) equals the sum of the
weights of all trees in T? that would be recognized
by M if we set F (q) = 1 and F (p) = 0 for each
p ? Q \ {q}, that is, if q is the unique final state
of M . It is not difficult to show that, since M is
convergent, the sum in the definition of wtM (q)
converges for each q ? Q. We will show in Sub-
section 5.4 that the quantities wtM (q) can be ap-
proximated to any desired precision.
To simplify the presentation, and without any
loss of generality, throughout this subsection we
assume that our WTA are in final-state normal
form. We can now introduce the normalization
technique.
Definition 3 Let M = (Q,?,R?0, ?, F ) be a
convergent WTA in final-state normal form. We
construct the WTA
Norm(M) =(Q,?,R?0, ??, F ) ,
where for every ? ? ?k, k ? 0, and
q, q1, . . . , qk ? Q
??k(?)q,q1???qk = ?k(?)q,q1???qk ?
? wtM (q1) ? . . . ? wtM (qk)wtM (q) . 2
We now show the claimed property for our
transformation.
Theorem 4 Let M be as in Definition 3, and let
M ? = Norm(M). Then M ? is a proper and
consistent PTA, and for every t ? T? we have
M ?(t) = M(t)wtM (qS) . 2
8
PROOF Clearly, M ? is again in final-state normal
form. An easy derivation shows that
wtM (q) =
?
???k
q1,...,qk?Q
?k(?)q,q1???qk ?
k?
i=1
wtM (qi)
for every q ? Q. Using the previous remark, we
obtain
?
???k,q1,...,qk?Q
??k(?)q,q1???qk
= ?
???k,q1,...,qk?Q
?k(?)q,q1???qk ?
? wtM (q1) ? . . . ? wtM (qk?)wtM (q)
=
?
???k,
q1,...,qk?Q
?k(?)q,q1???qk ?
k?
i=1
wtM (qi)
?
???k,
p1,...,pk?Q
?k(?)q,p1???pk ?
k?
i=1
wtM (pi)
= 1 ,
which proves that M ? is a proper PTA.
Next, we prove an auxiliary statement. Let
t = ?(t1, . . . , tk) for some ? ? ?k, k ? 0, and
t1, . . . , tk ? T?. We claim that
wtM ?(r) = wtM (r)wtM (r(?))
for every r ? RunM (t) = RunM ?(t). For ev-
ery 1 ? i ? k, let ri ? RunM (ti) be such that
ri(w) = r(iw) for every w ? Pos(ti). Then
wtM ?(r) =
?
w?Pos(t)
rkt(w)=n
??n(t(w))r(w),r(w1)???r(wn)
= ??k(?)r(?),r(1)???r(k) ?
k?
i=1
wtM ?(ri)
= ??k(?)r(?),r1(?)???rk(?) ?
k?
i=1
wtM (ri)
wtM (ri(?))
= ?k(?)r(?),r(1)???r(k) ? wtM (r1) ? ? ? ? ? wtM (rk)wtM (r(?))
= wtM (r)wtM (r(?)) .
Consequently,
M ?(t) = ?
r?RunM? (t)
r(?)=qS
wtM ?(r)
= ?
r?RunM (t)
r(?)=qS
wtM (r)
wtM (qS) =
M(t)
wtM (qS)
and
?
t?T?
M ?(t) = ?
t?T?,r?RunM? (t)
r(?)=qS
wtM ?(r)
= ?
t?T?,r?RunM (t)
r(?)=qS
wtM (r)
wtM (qS)
= wtM (qS)wtM (qS) = 1 ,
which prove the main statement and the consis-
tency of M ?, respectively. 
5.4 Probability mass of a state
AssumeM is a convergent WTA. We have defined
quantities wtM (q) for each q ? Q. Note that when
M is a proper PTA in final-state normal form, then
wtM (q) can be seen as the probability mass that
?rests? on state q. When dealing with such PTA,
we use the notation ZM (q) in place of wtM (q),
and call ZM the partition function of M . This
terminology is borrowed from the literature on ex-
ponential or Gibbs probabilistic models.
In the context of probabilistic context-free
grammars, the computation of the partition func-
tion has several applications, including the elim-
ination of epsilon rules (Abney et al, 1999) and
the computation of probabilistic distances between
probability distributions realized by these for-
malisms (Nederhof and Satta, 2008). Besides
what we have seen in Subsection 5.3, we will pro-
vide one more application of partition functions
for the computations of so-called prefix probabil-
ities in Subsection 5.5 We also add that, when
computed on the Bar-Hillel automata of Section 4,
the partition function provides the so-called inside
probabilities of (Graehl et al, 2008) for the given
states and substrings.
Let |Q| = n and let us assume an arbitrary or-
dering q1, . . . , qn for the states in Q. We can then
rewrite the definition of wtM (q) as
wtM (q) =
?
???k,k?0
qi1 ,...,qik?Q
?k(?)q,qi1 ???qik ?
k?
j=1
wtM (qij )
(see proof of Theorem 4). We rename wtM (qi)
with the unknown Xqi , 1 ? i ? n, and derive a
9
system of n nonlinear polynomial equations of the
form
Xqi =
?
???k,k?0
qi1 ,...,qik?Q
?k(?)q,qi1 ???qik ?Xqi1 ? . . . ?Xqik
= fqi(Xq1 , . . . , Xqn) , (1)
for each i with 1 ? i ? n.
Throughout this subsection, we will consider
solutions of the above system in the extended non-
negative real number semiring
R??0 = (R?0 ? {?},+, ?, 0, 1)
with the usual operations extended to ?. We
can write the system in (1) in the compact form
X = F (X), where we represent the unknowns
as a vector X = (Xq1 , . . . , Xqn) and F is a map-
ping of type (R??0)n ? (R??0)n consisting of the
polynomials fqi(X).
We denote the vector (0, . . . , 0) ? (R??0)n as
X0. Let X,X ? ? (R??0)n. We write X ? X ?
if Xqi ? X ?qi for every 1 ? i ? n. Sinceeach polynomial fqi(X) has coefficients repre-
sented by positive real numbers, it is not difficult
to see that, for each X,X ? ? (R??0)n, we have
F (X) ? F (X ?) whenever X0 ? X ? X ?. This
means that F is an order preserving, or monotone,
mapping.
We observe that ((R??0)n,?) is a complete
lattice with least element X0 and greatest el-
ement (?, . . . ,?). Since F is monotone on
a complete lattice, by the Knaster-Tarski theo-
rem (Knaster, 1928; Tarski, 1955) there exists a
least and a greatest fixed-point of F that are solu-
tions ofX = F (X).
The Kleene theorem states that the least fixed-
point solution of X = F (X) can be obtained
by iterating F starting with the least element X0.
In other words, the sequence Xk = F (Xk?1),
k = 1, 2, . . . converges to the least fixed-point so-
lution. Notice that each Xk provides an approxi-
mation for the partition function of M where only
trees of depth not larger than k are considered.
This means that limk??Xk converges to the par-
tition function of M , and the least fixed-point so-
lution is also the sought solution. Thus, we can
approximate wtM (q) with q ? Q to any degree by
iterating F a sufficiently large number of times.
The fixed-point iteration method discussed
above is also well-known in the numerical calcu-
lus literature, and is frequently applied to systems
of nonlinear equations in general, because it can
be easily implemented. When a number of stan-
dard conditions are met, each iteration of the algo-
rithm (corresponding to the value of k above) adds
a fixed number of bits to the precision of the ap-
proximated solution; see (Kelley, 1995) for further
discussion.
Systems of the form X = F (X) where all
fqi(X) are polynomials with nonnegative real co-
efficients are called monotone system of poly-
nomials. Monotone systems of polynomials as-
sociated with proper PTA have been specifically
investigated in (Etessami and Yannakakis, 2005)
and (Kiefer et al, 2007), where worst case results
on exponential rate of convergence are reported
for the fixed-point method.
5.5 Prefix probability
In this subsection we deal with one more applica-
tion of the Bar-Hillel technique presented in Sec-
tion 4. We show how to compute the so-called
prefix probabilities, that is, the probability that a
tree recognized by a PTA generates a string start-
ing with a given prefix. Such probabilities have
several applications in language modeling. As an
example, prefix probabilities can be used to com-
pute the probability distribution on the terminal
symbol that follows a given prefix (under the given
model).
For probabilistic context-free grammars, the
problem of the computation of prefix probabili-
ties has been solved in (Jelinek et al, 1992); see
also (Persoon and Fu, 1975). The approach we
propose here, originally formulated for probabilis-
tic context-free grammars in (Nederhof and Satta,
2003; Nederhof and Satta, 2009), is more abstract
than the previous ones, since it entirely rests on
properties of the Bar-Hillel construction that we
have already proved in Section 4.
Let M = (Q,?,R?0, ?, F ) be a proper
and consistent PTA in final-state normal form,
? = ?0 \ {e}, and let u ? ?+ be some string.
We assume here that M is in the binary form
discussed in Section 3. In addition, we assume
that M has been preprocessed in order to remove
from its recognized trees all of the unary branches
as well as those branches that generate the null
string ?. Although we do not discuss this con-
struction at length in this paper, the result follows
from a transformation casting weighted context-
free grammars into Chomsky Normal Form (Fu
10
and Huang, 1972; Abney et al, 1999).
We define
Pref(M,u) = {t | t ? T?, M(t) > 0,
yd(t) = uv, v ? ??} .
The prefix probability of u underM is defined as
?
t?Pref(M,u)
pM (t) .
Let |u| = n. We define a WSA Nu with state
set P = {p0, . . . , pn} and transition weights
?(pi?1, u(i), pi) = 1 for each i with 1 ? i ? n,
and ?(pn, ?, pn) = 1 for each ? ? ?. We also
set I(p0) = 1 and F (pn) = 1. It is easy to see
that Nu recognizes the language {uv | v ? ??}.
Furthermore, the PTA Mp = Prod(M,Nu) spec-
ified as in Definition 2 recognizes the desired tree
set Pref(M,u), and it preserves the weights of
those trees with respect to M . We therefore con-
clude that ZMp(qS) is the prefix probability of u
under M . Prefix probabilities can then be approx-
imated using the fixed-point iteration method of
Subsection 5.4. Rather than using an approxima-
tion method, we discuss in what follows how the
prefix probabilities can be exactly computed.
Let us consider more closely the product au-
tomaton Mp, assuming that it is trim. Each state
of Mp has the form pi = (pi, q, pj), pi, pj ? P and
q ? Q, with i ? j. We distinguish three, mutually
exclusive cases.
(i) j < n: From our assumption that M (and
thus Mp) does not have unary or ? branches,
it is not difficult to see that all ZMp(pi) can be
exactly computed in time O((j ? i)3).
(ii) i = j = n: We have pi = (pn, q, pn).
Then the equations for ZMp(pi) exactly
mirror the equations for ZM (q), and
ZMp(pi) = ZMp(q). Because M is proper
and consistent, this means that ZMp(pi) = 1.
(iii) i < j = n: A close inspection of Definition 2
reveals that in this case the equations (1) are
all linear, assuming that we have already re-
placed the solutions from (i) and (ii) above
into the system. This is because any weight
?2(?)pi0,pi1pi > 0 in Mp with pi = (pi, q, pn)
and i < n must have (pi1)3 < n. Quanti-
ties ZMp(pi) can then be exactly computed as
the solution of a linear system of equations in
time O(n3).
Putting together all of the observations above,
we obtain that for a proper and consistent PTA that
has been preprocessed, the prefix probability of u
can be computed in cubic time in the length of the
prefix itself.
6 Concluding remarks
In this paper we have extended the Bar-Hillel con-
struction to WTA, closely following the method-
ology proposed in (Nederhof and Satta, 2003) for
weighted context-free grammars. Based on the ob-
tained framework, we have derived several parsing
algorithms for WTA, under the assumption that the
input is a string rather than a tree.
As already remarked in the introduction, WTA
are richer models than weighted context-free
grammar, since the formers use hidden states in
the recognition of trees. This feature makes it
possible to define a product automaton in Defini-
tion 2 that generates exactly those trees of interest
for the input string. In contrast, in the context-
free grammar case the Bar-Hillel technique pro-
vides trees that must be mapped to the tree of in-
terest using some homomorphism. For the same
reason, one cannot directly convert WTA into
weighted context-free grammars and then apply
existing parsing algorithms for the latter formal-
ism, unless the alphabet of nonterminal symbols
is changed. Finally, our main motivation in de-
veloping a framework specifically based on WTA
is that this can be extended to classes of weighted
tree transducers, in order to deal with computa-
tional problems that arise in machine translation
applications. We leave this for future work.
Acknowledgments
The first author has been supported by the Minis-
terio de Educacio?n y Ciencia (MEC) under grant
JDCI-2007-760. The second author has been par-
tially supported by MIUR under project PRIN No.
2007TJNZRE 002.
References
S. Abney, D. McAllester, and F. Pereira. 1999. Relat-
ing probabilistic grammars and automata. In 37th
Annual Meeting of the Association for Computa-
tional Linguistics, Proceedings of the Conference,
pages 542?549, Maryland, USA, June.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On
formal properties of simple phrase structure gram-
mars. In Y. Bar-Hillel, editor, Language and Infor-
mation: Selected Essays on their Theory and Appli-
cation, chapter 9, pages 116?150. Addison-Wesley,
Reading, Massachusetts.
11
J. Berstel and C. Reutenauer. 1982. Recognizable for-
mal power series on trees. Theoret. Comput. Sci.,
18(2):115?148.
B. Borchardt. 2005. The Theory of Recognizable Tree
Series. Ph.D. thesis, Technische Universita?t Dres-
den.
S. Bozapalidis. 1999. Equational elements in additive
algebras. Theory Comput. Systems, 32(1):1?33.
J. Carme, J. Niehren, and M. Tommasi. 2004. Query-
ing unranked trees with stepwise tree automata. In
Proc. RTA, volume 3091 of LNCS, pages 105?118.
Springer.
F. Casacuberta and C. de la Higuera. 2000. Com-
putational complexity of problems on probabilis-
tic grammars and transducers. In L. Oliveira, edi-
tor, Grammatical Inference: Algorithms and Appli-
cations; 5th International Colloquium, ICGI 2000,
pages 15?24. Springer.
Z. Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1):131?160.
M. Droste, C. Pech, and H. Vogler. 2005. A Kleene
theorem for weighted tree automata. Theory Com-
put. Systems, 38(1):1?38.
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94?102,
February.
S. Eilenberg. 1974. Automata, Languages, and Ma-
chines, volume 59 of Pure and Applied Math. Aca-
demic Press.
C. A. Ellis. 1971. Probabilistic tree automata. Infor-
mation and Control, 19(5):401?416.
Z. E?sik and W. Kuich. 2003. Formal tree series. J.
Autom. Lang. Combin., 8(2):219?285.
K. Etessami and M. Yannakakis. 2005. Recursive
Markov chains, stochastic grammars, and monotone
systems of nonlinear equations. In 22nd Interna-
tional Symposium on Theoretical Aspects of Com-
puter Science, volume 3404 of Lecture Notes in
Computer Science, pages 340?352, Stuttgart, Ger-
many. Springer-Verlag.
K.S. Fu and T. Huang. 1972. Stochastic grammars and
languages. International Journal of Computer and
Information Sciences, 1(2):135?170.
F. Ge?cseg and M. Steinby. 1984. Tree Automata.
Akade?miai Kiado?, Budapest.
J. Graehl, K. Knight, and J. May. 2008. Training tree
transducers. Comput. Linguist., 34(3):391?427.
J. Ho?gberg, A. Maletti, and H. Vogler. 2009. Bisim-
ulation minimisation of weighted automata on un-
ranked trees. Fundam. Inform. to appear.
F. Jelinek, J.D. Lafferty, and R.L. Mercer. 1992. Basic
methods of probabilistic context free grammars. In
P. Laface and R. De Mori, editors, Speech Recogni-
tion and Understanding ? Recent Advances, Trends
and Applications, pages 345?360. Springer-Verlag.
M. Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613?632.
C. T. Kelley. 1995. Iterative Methods for Linear and
Nonlinear Equations. Society for Industrial and Ap-
plied Mathematics, Philadelphia, PA.
S. Kiefer, M. Luttenberger, and J. Esparza. 2007. On
the convergence of Newton?s method for monotone
systems of polynomial equations. In Proceedings of
the 39th ACM Symposium on Theory of Computing,
pages 217?266.
B. Knaster. 1928. Un the?ore`me sur les fonctions
d?ensembles. Ann. Soc. Polon. Math., 6:133?134.
D. E. Knuth. 1977. A generalization of Dijkstra?s al-
gorithm. Information Processing Letters, 6(1):1?5,
February.
D. E. Knuth. 1997. Fundamental Algorithms. The Art
of Computer Programming. Addison Wesley, 3rd
edition.
M. Magidor and G. Moran. 1970. Probabilistic tree
automata and context free languages. Israel Journal
of Mathematics, 8(4):340?348.
M.-J. Nederhof and G. Satta. 2003. Probabilistic pars-
ing as intersection. In 8th International Workshop
on Parsing Technologies, pages 137?148, LORIA,
Nancy, France, April.
M.-J. Nederhof and G. Satta. 2008. Computation of
distances for regular and context-free probabilistic
languages. Theoretical Computer Science, 395(2-
3):235?254.
M.-J. Nederhof and G. Satta. 2009. Computing parti-
tion functions of PCFGs. Research on Language &
Computation, 6(2):139?162.
M.-J. Nederhof. 2003. Weighted deductive parsing
and Knuth?s algorithm. Computational Linguistics,
29(1):135?143.
E. Persoon and K.S. Fu. 1975. Sequential classi-
fication of strings generated by SCFG?s. Interna-
tional Journal of Computer and Information Sci-
ences, 4(3):205?217.
M. P. Schu?tzenberger. 1961. On the definition of a
family of automata. Information and Control, 4(2?
3):245?270.
A. Tarski. 1955. A lattice-theoretical fixpoint theorem
and its applications. Pacific J. Math., 5(2):285?309.
D. H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10:189?208.
12
