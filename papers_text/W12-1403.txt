JEP-TALN-RECITAL 2012, Atelier ILADI 2012: Interactions Langagi?res pour personnes Ag?es Dans les habitats Intelligents, pages 17?30,
Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCP
Interactions sonores et vocales dans l?habitat
Pierrick Milhorat1, Dan Istrate3, J?r?me Boudy2, G?rard Chollet1
(1) T?l?com ParisTech, 37-39 rue Dareau, 75014, Paris, France
(2) T?l?com SudParis, 9 rue Charles Fourier, 91011 Evry Cedex, France 
(3) ESIGETEL, 1 Rue du Port de Valvins, 77210 Avon Cedex, France
milhorat@telecom-paristech.fr, dan.istrate@esigetel.fr, 
jerome.boudy@it-sudparis.eu, chollet@telecom-paristech.fr
R?SUM?___________________________________________________________________________________________________________
Cet article pr?sente le syst?me de reconnaissance son/parole en continu d?velopp? et 
?valu? dans le cadre du projet  europ?en CompanionAble.  Ce syst?me analyse le flux 
sonore en continu, d?tecte et reconna?t des sons de la vie quotidienne et des commandes 
vocales gr?ce ? un microphone. L?architecture et la description de chaque module sont 
d?taill?es. Des contraintes ont ?t? impos?es ? l?utilisateur et au concepteur, telle que la 
limitation du vocabulaire, dans le but d?obtenir des taux de reconnaissance et de rejet 
acceptables.  Les  premiers  r?sultats  sont  pr?sent?s,  les  essais  finaux sur le  terrain du 
projet sont en cours dans une maison intelligente ? Eindhoven.
ABSTRACT__________________________________________________________________________________________________________
Acoustic Interaction At Home
This  paper  describes  a  hands-free  speech/sound  recognition  system  developed  and 
evaluated in the framework of the European Project CompanionAble. The system is able 
to work continuously on a distant microphone and detect not only vocal commands but 
also everyday life sounds. The proposed architecture and the description of each module 
are outlined. In order to have good recognition rate some constraints were defined for 
the user and the vocabulary was limited. First results are presented; currently project 
trials are underway.
MOTS-CL?S: reconnaissance  vocale,  traitement  du  son,  reconnaissance  des  sons, 
domotique. 
KEYWORDS: hands-free speech recognition, sound processing, sound recognition, domotics.
17
1. Introduction
Le projet europ?en CompanionAble a pour objectif l?int?gration d?un robot compagnon 
dans  une  maison  intelligente  ?  destination  de  seniors  d?pendants.  Le  robot  sert 
d?interface  entre  les  fonctionnalit?s  de  la  maison  (allumage/extinction  des  lumi?res, 
ouverture/fermeture  des  rideaux,  lecture/pause  de  la  cha?ne  Hi-Fi...)  ainsi  que 
d?assistant ? la vie quotidienne. A l?aide de capteurs diss?min?s dans l?habitat (capteurs 
infra-rouges, capteurs d?ouverture de porte...) et de ses propres informations (cam?ra, 
ultrasons...),  il  assiste  les  r?sidents  suivant  des  sc?narios  pr?d?finis  (entr?e  dans  la 
maison,  sortie,  appel  en  visioconf?rence...)  ou  d?finis  par  l?utilisateur  (rappel  de 
l?agenda, alerte de prise de m?dicaments, objets plac?s dans les paniers du robot...). 
Dans ce contexte, le robot, porteur d?un ?cran tactile, un ?cran interactif install? dans la 
cuisine  et  une  tablette  portative  sont  ?quip?s  d?une  application  graphique  identique 
pr?sentant toutes les fonctionnalit?s disponibles.
L?Institut Mines-T?l?com est en charge de porter l?interaction vers une communication 
vocale. Un ensemble de commandes domotiques d?riv?es d?exp?rimentation pratiques a 
?t? ?tablis auxquelles le syst?me d?analyse acoustique doit r?agir. 
L?interaction avec les applications autres telles que l?agenda, les exercices cognitifs ou le 
contr?le du robot a ?galement fait l?objet de d?finitions de commandes. Dans les deux 
cas, les commandes ne sont pas uniquement des mots mais des phrases compl?tes.
De nombreux travaux ont port? sur la reconnaissance vocale et les performances des 
syst?mes commerciaux actuels d?montrent leur g?n?ralisation ? venir. La sp?cificit? de 
nos travaux inclus dans ce projet r?side dans la r?solution du probl?me de la distance du 
microphone au locuteur. Celui-ci, unique, se trouve int?gr? au robot, soit ? environ un 
m?tre  du  sol  et  mobile.  La  distance  au  locuteur  et  le  bruit  environnant  sont 
incontr?lables et variables. Les travaux de soustraction du bruit ? l?aide de microphones 
enregistrant les sources de bruits ne s?appliquent pas aux conditions variables rencontr?s 
lors des ?tudes d?usage pr?liminaires. 
La deuxi?me section de cet article d?crit le projet CompanionAble. Les sections 3 et 4 
sont consacr?es ? la reconnaissance des sons. Vient ensuite la description du syst?me de 
reconnaissance de la parole utilis? et adapt? ? ce contexte, en section 5. La sections 6 
pr?sente les ?valuations du syst?me. Les futurs axes de recherche et conclusions tir?s de 
ce projet seront exprim?s dans la derni?re partie. 
     
18
2. CompanionAble
CompanionAble est l?acronyme de Integrated Cognitive Assistive & Domotic Companion 
Robotic Systems for Ability & Security.
C?est  un  projet  financ?  par  la  commission  europ?enne  qui  r?unit  18  partenaires 
acad?miques et industriels.
Les objectifs sont les suivants :
-  combiner  les  capacit?s  d?un  robot  ? compagnon ?  mobile  avec  les 
fonctionnalit?s statiques d?un environnement intelligent.
- int?grer les donn?es des capteurs de l?habitat ? celles du robot
- cr?er un lien social entre les s?niors et leurs proches et/ou leurs entourage 
m?dical
- am?liorer la qualit? de vie et l?autonomie des personnes d?pendantes
Les partenaires sont localis?s dans 7 pays, ? savoir : la France, l?Allemagne, l?Espagne, 
l?Autriche, la Belgique, les Pays-Bas et le Royaume-Unis.
L?Institut Mines-T?l?com (anciennement Groupe des Ecoles des T?l?communications) a 
pour r?le l?addition d?une interface vocale, d?un module multimodal de d?tection des 
situations  de  d?tresse  et  participe  ?galement  ?  la  localisation  des  personnes  dans 
l?habitat. Cet article se concentre sur la partie acoustique des travaux.
Actuellement, le projet est entr? dans une phase d?exp?rimentation pratique en situation 
r?elles.  Cela  se  d?roulera  ?  Eindhoven  (Pays-Bas)  et  ?  Gits  (Belgique)  un  panel 
d?utilisateurs potentiels sera amen? ? tester le syst?me complet. 
3. Architecture de traitement du son
Le son est acquis en continu par deux syst?mes parall?les : l?un attribue un type au son 
(parole/son et type de son) tandis que l?autre transcrit la parole en texte. La figure 1 
montre la communication entre les modules sonores qui utilise un protocole TCP/IP. Les 
sons reconnus ou les commandes vocales sont transmises au serveur du projet via un 
protocole SOAP. La reconnaissance vocale est filtr?e par le module de reconnaissance des 
sons pour ?viter les fausses alarmes (faux-positifs).
19
Figure 1 ? Architecture de traitement des sons
4. Reconnaissance des sons
Le syst?me de reconnaissance sonore consiste dans notre application en une s?quence de 
deux proc?d?s : un module de d?tection bas? sur unetransform?e en ondelettes et un 
module de reconnaissance hi?rarchique (son/parole et classification des sons) bas? sur 
des GMM (Rougui, 2009).
Les classes de sons utilis?es lors des essais CompanionAble ont ?t? apprises avec des 
enregistrements effectu?s avec un CMT (microphone produit par AKG) (Rougui, 2009) 
dans la maison SmartHomes ? Eindhoven. Actuellement, le syst?me dispose de 5 classes 
de son : chute d?objets, sonnette, cl?s, toux et applaudissements.  Les classes de son ont 
?t? choisies pour la d?tection de situations de d?tresse et d?actions non parl?es utiles 
pour le syst?me domotique.
La sortie de la reconnaissance vocale est filtr?e par le module de reconnaissance des sons 
pour emp?cher une fausse commande associ?e ? un son non parl?.  Comme les deux 
modules  tournent  en  parall?le,  une  synchronisation  est  requise.  Le  module  sonore 
enregistre constamment les trois derni?res d?cisions d??tiquetage sons/parole et d?cide 
de valider  ou rejeter la reconnaissance vocale en fonction de la corr?lation entre les 
deux. 
Chaque module a ?t? initialement ?valu? sur des bases de donn?es. Les r?sultats de la 
classification  des  sons  selon  15  classes  sont  de  80%  de  bonne  reconnaissance  .  La 
reconnaissance  parole/son  a  ?t?  et  sera  ?valu?e  dans  la  maison  SmartHomes ;  les 
premiers r?sultats ont montr? un taux avoisinant les 95%
5. Reconnaissance de la parole
Concernant  le  volet  reconnaissance  vocale,  il  se  justifie,  au  sein  du  projet  par  la 
difficult?, voire l?incapacit? d?une grande partie des utilisateurs cibles d?interagir avec un 
syst?me informatique par le biais de menus sur des ?crans tactiles. Les troubles cognitifs 
20
ou des probl?mes de mobilit? trop importants rendraient le syst?me obsol?te s?il n??tait 
pas dot? d?une interface vocale ? distance. Les commandes interpr?tables portent sur 
l?interaction ? face au robot ? combin? avec l?affichage graphique et sur les interactions ? 
distance. Les trois probl?matiques auxquelles nous proposons une solution sont :
- la reconnaissance de commandes vocales dans un environnement bruit? pour 
lequel les sources de bruit sont inconnues
- la reconnaissance de commandes vocales ? distance variable
- la reconnaissance de commandes vocales toujours active
Les  centres d?exp?rimentation bas?s aux Pays-Bas et en Belgique flamande contraignent 
le projet ? r?aliser l?interface vocale du robot en hollandais. 
Julius,  d?velopp?  par  le  Kawahara  lab  de  Tokyo,  a  ?t?  s?lectionn?  comme ?tant  le 
d?codeur le plus appropri?  pour une application basique ?tat de l?art  (Lee, 2008).  Il 
permet une reconnaissance sur un large vocabulaire (60 000 mots) en temps quasi-r?el 
gr?ce ? un algorithme ? deux passes. Il s ?appuie sur des mod?les de langage N-grams et 
des  mod?les  acoustiques  encod?es  sous  forme  de  mod?les  de  Markov  cach?s.  La 
modularit? de ce moteur de reconnaissance permet de traiter une m?me entr?e (audio) 
avec plusieurs mod?les (acoustiques et de langage) diff?renci?s selon les besoins. 
Les  mod?les  de  Markov  cach?s  des  phon?mes  composant  le  mod?le  acoustique 
hollandais ont ?t? appris sur le Corpus Gesproken Nederlands (CGN). Cela repr?sente 
800 heures d?enregistrements audio transcrits dans lesquelles presque 9 millions de mots 
sont prononc?s, faisant du CGN le plus grand corpus pour le hollandais contemporain. 
Les sources sont r?parties entre des sources monolocuteurs et multilocuteurs, prompt?es 
ou spontan?es.
Fran?ais Fen?tre Hollandais
Non GoodBye Frame Nee
Hector Main Frame Hector
Je reviens tout de suite GoodBye Frame Ik ben zo terug/Ik ga niet weg/Zo terug
Quelques jours GoodBye Frame Een paar dagen/Paar dagen
Environ une heure GoodBye Frame Een uurtje/Een uur/Uur
Affiche les 
appels 
manqu?s
Greeting Frame Gemiste oproepen/Laat gemiste oproepen
Affiche la liste des 
choses ? faire Greeting Frame Taken/Laat taken zien/Start taken
Table 1 ? Exemple de commandes vocales
21
?tant donn?es les conditions impos?es (toujours actif, distance au microphone variable, 
vari?t?  des  fonds  sonores,  etc...),  des  solutions  ont  ?t?  propos?es  pour  am?liorer  la 
robustesse du syst?me.
? L?attention ?
Le gestionnaire du dialogue propose un moyen de limiter le nombre de faux-positifs avec 
l?utilisation  d?un  mot  ? d?attention ?.  Ce  mot,  quand il  est  d?tect?,  accro?t  le  niveau 
d?attention  qui  d?cro?t  avec  le  temps.  Un  niveau  d?attention  non  nul  d?clenche  le 
traitement et l?analyse des donn?es de la reconnaissance. Par exemple, ? l??tat initial, le 
niveau d?attention est nulle : le module de reconnaissance vocale est toujours actif et 
transmet  ces  r?sultats,  tant  que  le  mot  cl?  n?est  pas  d?tect?,  les  transcriptions  sont 
ignor?es par le gestionnaire de dialogue. D?s lors que le niveau d?attention est sup?rieur 
? 0, le dialogue s?engage, et le gestionnaire interpr?te toutes les commandes re?ues. Tant 
que le dialogue est soutenu (soit par r?p?tition du mot d?attention, soit par une ?volution 
du dialogue), le niveau d?attention s?accro?t alors que les silences, du point de vue du 
gestionnaire de dialogue diminuent la variable d?attention qui, si elle atteint sa valeur 
plancher  (nulle)  coupe  l?interpr?tation.  Le  choix  d?un  mot  d?attention  le  plus 
discriminatoire possible augmente l?efficacit? d?un tel m?canisme. 
La classification sons/parole
Un moteur de reconnaissance vocale tel que Julius cherche la s?quence de mots qui 
correspond  au  mieux  ?  la  s?quence  de  vecteurs  acoustiques  pr?sent?e  selon  les 
probabilit?s contenues dans la combinaison des mod?les acoustiques et de langage. Il est 
possible de cr?er un mot ? poubelle ? qui remplacerait l?ensemble des mots dont le score 
de  reconnaissance  serait  trop  faible.  Dans  notre  application,  nous  avons  choisi  de 
d?coder syst?matiquement les sons de l?habitat. Ainsi, les bruits qui se distinguent de la 
parole sont mis en correspondance avec une s?quence de mots erron?e. La classification 
des  sons  en  deux  cat?gories  (parole/non-parole)  permet  un  filtrage  des  donn?es 
acoustiques. Ce filtrage est effectu? en parall?le du processus de reconnaissance pour 
conserver les aspects temps r?el inh?rents au projet.  
L?adaptation
Les techniques d?adaptation qui auraient pu ?tre utilis?es ont ?t? les premi?res ? ?tre 
impl?ment?es et test?es. Un mod?le de langage (N-grams) a ?t? ?labor? sur un corpus de 
22
plus de 57500 phrases d?riv?es d?exp?riences pratiques et de paraphrases. 
Une comparaison entre deux proc?dures d?adaptation, Maximum A Posteriori (MAP) et 
Maximum Likelihood Linear Regression (MLLR),  a ?t? faite (Caon, 2011).  
Le locuteur est le m?me pour toute l?exp?rience. Il a ?t? enregistr? et les fichiers audio 
sont  jou?s  par  un  haut-parleur.  Comme  pr?vu,  ?tant  donn?  le  peu  de  donn?e 
d?adaptation disponibles (10 phrases par locuteur), l'adaptation par MLLR donne donc 
les  meilleurs  r?sultats.  Sans  adaptation,  60%  des  allocutions  ont  ?t?  correctement 
retranscrites par Julius. Ce taux s??l?ve ? 70% avec l?adaptation par MAP et 73% avec 
l?adaptation MLLR. De fait, MLLR a ?t? confirm? comme la technique d?adaptation la 
plus idoine.
La combinaison de mod?les de langage
Dans une premi?re version de l?application, un N-gram unique, appris sur un corpus de 
57 658 phrases a ?t? utilis?. La voix de chaque utilisateur ?tait adapt?e avant l?utilisation 
du syst?me : la reconnaissance est cibl?e pour un utilisateur d?termin? par l?adaptation 
pr?alable de sa voix. Cette premi?re version pr?sentait trop de ? faux-positifs ?, i.e. de 
commandes non d?sir?es lors de tests pratiques.
Dans le but d?am?liorer ? la fois les taux de rejet et de reconnaissance, un filtre, d?cris 
ci-dessous, a ?t? impl?ment?.
Le  gestionnaire  de  dialogue  mod?lise  le  dialogue  comme  un  ensemble  de  fen?tres 
(M?ller, 2010). Celles-ci contiennent chacune un graphe du sous-dialogue pour lequel les 
transitions sont d?clench?es par l??tat de variables internes au robot ou par des actions 
de l?utilisateur (commande vocale,  pression de bouton,  excitation de capteur...).  Une 
fen?tre devient active lorsque l?une de ses conditions suffisantes d?activation est remplie, 
ce sont les m?me type de variable que celles associ?es aux transitions intra-fen?tre. De 
fait, il est possible de construire une hi?rarchie du dialogue. La fen?tre principale ou 
racine,  initialement  active,  contient  (uniquement)  tous  les  d?clencheurs  des  sous-
dialogues.  Les  fen?tres  contiennent  des  noeuds  terminaux,  ce  qui  permet  une  auto-
d?sactivation et un retour ? la fen?tre principale.
23
Figure 2 ? Syst?me de fen?tre du gestionnaire de dialogue
Les diff?rentes sous-fen?tres ont ?t? regroup?es en 8 cat?gories. A chaque cat?gorie 
correspond un ensemble des commandes vocables possibles dans les sous-dialogues qui 
la compose. Pour chaque cat?gorie, un mod?le de langage a ?t? appris sur l?ensemble des 
commandes correspondantes.
Un 9?me mod?le est appris sur l?ensemble des commandes vocales qui activent les sous-
fen?tres, il est associ? ? la fen?tre principale.
Bien que le module de reconnaissance vocale ne connaisse pas l??tat du dialogue (la 
fen?tre active) puisque la communication est uni-directionnelle pour garantir au mieux 
la synchronisation et parce que Julius le permet, 9 instances du d?codeur, param?tris?es 
avec un mod?le acoustique identique et un mod?le de langage sp?cifique, analyse en 
parall?le les sons de l?habitat.
Cette m?thode permet de favoriser  en grande partie  les  commandes autoris?es  selon 
l??tat  du dialogue,  cependant,  elle aggrave  les  probl?mes de rejet  des  allocutions  en 
dehors de l?application.
Le test de similarit? 
La similarit? entre deux hypoth?ses est mesur?e en terme de distance de Levenshtein sur 
les mots. Elle cumule le nombre de substitution, d?ajout et de retrait de mots. De plus 
elle  est  ensuite  divis?e  par  la  longueur  des  phrases,  rendant  alors  une  moyenne du 
nombre de diff?rences par mots. La valeur de cette variable, relative ? un seuil, d?finit la 
validation ou le  rejet de l?hypoth?se de commande vocale reconnu par l?instance du 
d?codeur bas?e sur un vocabulaire restreint. Ce test permet de :
-  confirmer  les  hypoth?ses  correctes :  une  commande reconnue correctement 
(vrai-positif)  par  un  d?codeur  sp?cialis?  et  reconnue  correctement  par  le 
24
d?codeur  g?n?ral  est  valid?  par  le  test,  l?hypoth?se  fournie  par  le  d?codeur 
sp?cialis? est transmise.
-  rejeter  les hypoth?ses  incorrectes :  une commande reconnue incorrectement 
(faux-positif)  par  un  d?codeur  sp?cialis?  et  reconnue  correctement  ou 
similairement par le d?codeur g?n?ral est rejet?e par le test, l?hypoth?se fournie 
par le d?codeur sp?cialis? est ignor?e.
-  corriger  les  hypoth?ses  partiellement  incorrectes :  une commande reconnue 
correctement  par  un  d?codeur  sp?cialis?  et  reconnue  similairement  par  le 
d?codeur  g?n?ral  est  valid?e par le test,  l?hypoth?se fournie par le  d?codeur 
sp?cialis? est transmise.
Le  mod?le  de  langage  g?n?ral  doit,  dans  cette  configuration,  pouvoir  mod?liser  les 
s?quences de mots d?finies dans les mod?les de langage sp?cialis?s. Il est donc n?cessaire 
d?ajouter les commandes vocales dans le corpus d?apprentissage du mod?le g?n?ral, de 
plus,  nous  introduisons  un  poids  ?  ces  commandes.  Le  poids  optimal  a  ?t?  d?fini 
exp?rimentalement comme ?tant 1000, i.e. les commandes vocales ont ?t? ajout?es mille 
fois au corpus CGN avant l?apprentissage.
Finalement, le test de similarit? ne s?effectue pas sur une transcription par d?codeur, il a 
?t?  d?couvert,  exp?rimentalement,  que  l?utilisation  des  n  meilleures  hypoth?ses 
am?liorait le taux de reconnaissance sans impacter sensiblement le taux de rejet :
-  au  vu  de  la  taille  des  mod?les  de  langage  restreints,  seule  la  meilleure 
hypoth?se est compar?e.
- plusieurs hypoth?ses (les 3 meilleures dans notre application) fournies par le 
d?codeur g?n?ral passent le test de similarit?.
L?ensemble de ces am?liorations ont ?t? impl?ment?es, pour certaines directement dans 
le code de Julius ou du gestionnaire de dialogue. Elles sont ?valu?es dans la section 
suivante.
25
Figure 3 ? Syst?me de reconnaissance de la parole incluant le test de similarit?
6. ?valuations
Une  batterie  de  test  pour  ?prouver  la  robustesse  du  syst?me  a  ?t?  effectu?e,  nous 
pr?sentons ici les r?sultats les plus probants et significatifs.
Pour l?ensemble des ?valuations, un corpus de test a ?t? pr?alablement enregistr? aupr?s 
de  5  r?sidents  hollandais.  Chacun  d?eux  a  enregistr?  58  phrases :  10  phrases 
d?adaptation,  20  commandes  de  l?application,  22  allocutions  hors  application  et  6 
commandes  d?riv?es.  Une  commande  d?riv?e  est  une  commande  compos?e  du 
vocabulaire de l?application mais dont la grammaire est inexacte.
L?installation de l?exp?rience est pr?sent? sch?matiquement sur la figure 4. Les s?quences 
sonores  en  hollandais  sont  produites  par  un  haut-parleur  et  enregistr?es  par  un 
microphone ? distance variable.  Un second haut-parleur,  plac? au-dessus du premier 
simule des bruits ambiants dans la seconde phase de l?exp?rience. Le volume sonore des 
locuteurs hollandais est ajust? aux situations r?elles (environ 60 dBA).   
26
Figure 4 ? Installation pour l?exp?rience
Lors d?une premi?re phase, l?addition du test de similarit?, le poids des commandes 
vocales dans le corpus d?apprentissage du mod?le g?n?ral et le nombre d?hypoth?ses 
pr?sent?es ? la comparaison par le d?codeur g?n?ral sont ?valu?. 
Dans le cas de commandes vocales autoris?es par l?application, le d?codeur de base, qui 
utilise un mod?le unique de langage g?n?ral comprenant les commandes vocales sans 
poids associ?, obtient des r?sultats de reconnaissance de 15%. L?ajout du test de 
similarit? sans modifier ce mod?le de langage g?n?ral porte le taux ? 20% mais laisse 
appara?tre des faux-positifs. Le syst?me le plus ?volu? obtient des performances de 85% 
de reconnaissance pour un taux de faux-positifs nul.
Toutes les allocutions hors de l?application sont rejet?es par le syst?me. 
En ce qui concerne les commandes d?riv?es, elles sont peu rejet?es car proche des 
commandes r?elles.  
Syst?me Taux de reconnaissance Taux de faux-positifs
Base + adaptation 15 0
Base + adaptation + test de similarit? (poids des 
commandes : 1 ; hypoth?ses du d?codeur g?n?ral : 1) 20 10
Base + adaptation + test de similarit? (poids des 
commandes : 1000 ; hypoth?ses du d?codeur g?n?ral : 1) 55 0
Base + adaptation + test de similarit? (poids des 
commandes : 1000 ; hypoth?ses du d?codeur g?n?ral : 3) 85 0
Table 2 ? Taux de reconnaissance correcte et de faux-positifs pour les commandes de l?application
Syst?me Taux de reconnaissance Taux de faux-positifs
Base + adaptation 9.09 0
Base + adaptation + test de similarit? (poids des 
commandes : 1 ; hypoth?ses du d?codeur g?n?ral : 1) 0 0
Base + adaptation + test de similarit? (poids des 
commandes : 1000 ; hypoth?ses du d?codeur g?n?ral : 1) 0 0
Base + adaptation + test de similarit? (poids des 
commandes : 1000 ; hypoth?ses du d?codeur g?n?ral : 3) 0 0
27
Table 3 ? Taux de reconnaissance correcte et de faux-positifs pour des allocutions hors application
Syst?me Taux de reconnaissance Taux de faux-positifs
Base + adaptation 16.67 0
Base + adaptation + test de similarit? (poids des 
commandes : 1 ; hypoth?ses du d?codeur g?n?ral : 1) 33.33 0
Base + adaptation + test de similarit? (poids des 
commandes : 1000 ; hypoth?ses du d?codeur g?n?ral : 1) 66.67 0
Base + adaptation + test de similarit? (poids des 
commandes : 1000 ; hypoth?ses du d?codeur g?n?ral : 3) 66.67 0
Table 4 ? Taux de reconnaissance correcte et de faux-positifs pour des commandes de l?application d?riv?es
La deuxi?me phase de l?exp?rience consistait en un test de r?sistance au bruit. Un second 
haut-parleur, simulant du bruit ambiant (non-stationnaire) est ajout? au dispositif. 
Cela a pour cons?quence de diminuer les performances du syst?me, autant du point de 
vue de la reconnaissance, que de celui du rejet.
Syst?me Taux de reconnaissance Taux de faux-positifs
Machine ? laver 74 11
Locuteur hollandais 53 11
Musique 47 5
Foule 42 11
Table 5 ? Taux de reconnaissance correcte et de faux-positifs pour les commandes de l?application
Syst?me Taux de reconnaissance Taux de faux-positifs
Machine ? laver 0 0
Locuteur hollandais 0 0
Musique 0 0
Foule 0 3.64
Table 6 ? Taux de reconnaissance correcte et de faux-positifs pour des allocutions hors application
Syst?me Taux de reconnaissance Taux de faux-positifs
Machine ? laver 40 0
Locuteur hollandais 60 0
Musique 20 0
Foule 60 0
Table 7 ? Taux de reconnaissance correcte et de faux-positifs pour des commandes de l?application d?riv?es
28
7.Conclusion et perspectives
Le syst?me pr?sent? dans cet article propose de compl?ter l?interaction entre un robot et 
un humain par ajout de commandes vocales dans une maison intelligente. Le robot est 
toujours actif, tout comme doit l??tre l?analyse des commandes vocales accessibles ? tout 
moment. De par ces contraintes, la caract?ristique la plus importante dont on doit tenir 
compte  est  la  robustesse  d?un  tel  syst?me.  Cela  combine  ?  la  fois  un  taux  de 
reconnaissance correct et un taux de rejet acceptable.
Un ?quilibre entre ces deux aspects doit  ?tre trouv?. Accepte-t-on de reconna?tre des 
commandes  erron?es ?  Peut-on  demander  ?  l?utilisateur  de  r?p?ter  plusieurs  fois  les 
commandes ? Pendant les tests  pratiques pr?c?dents,  il  a ?t? d?montr? que les faux-
positifs perturbaient l?utilisateur et g?n?raient des comportements inattendus. Pour parer 
? ce probl?me, les possibilit?s de commandes ont ?t? restreintes, cr?ant deux nouveaux 
obstacles. Les utilisateurs cibles sont des s?niors qui pourraient avoir des difficult?s ? se 
rappeler les commandes pr?cises. De plus, ils pourraient rapidement se d?sint?resser des 
fonctionnalit?s vocales s?ils  per?oivent  une fiabilit?  faible dans l'ex?cution des  ordres 
qu?ils ?mettent.
Nous avons propos?, dans ce travail,  d?exp?rimenter une combinaison de mod?les de 
langage associ?e ? un test de similarit? pour am?liorer  la pr?cision du syst?me.    
Un nouveau mod?le de langage g?n?ral a ?t? construit ? partir de la base n?erlandaise 
CGN, supposons qu?il est capable de reconna?tre n?importe quelle phrases en n?erlandais 
ou une phrase proche.  Une passe de la reconnaissance utilise un mod?le de langage 
sp?cifique ? l?application, voire ? une partie de l?application. La similarit? entre les deux 
sorties, i.e. la distance de Levenshtein entre les deux phrases, agit comme un filtre pour 
valider ou rejeter les sorties.  
Ce  syst?me  plus  ?labor?  a  d?montr?  ?tre  plus  robuste,  autorisant  un  taux  de 
reconnaissance correct ainsi que des cas limit? de faux-positifs. Cependant, l?exp?rience a 
montr? ses faiblesses dans le traitement et le rejet des allocutions courtes, i.e. phrases 
compos?es d?un seul mot. L?utilisation du mot-cl? ? d?attention ? emp?che la plupart du 
temps ce genre de situation de se produire.
Courant  avril  et  mai  de  l?ann?e  2012,  des  essais  en  situation  r?elle  auront  lieu  ? 
Eindhoven  et  Gits.  Des  couples  de  seniors  sont  invit?s  ?  vivre  dans  une  maison 
intelligente dans laquelle un robot compagnon interagira avec eux. Jusqu?? pr?sent, le 
29
test en conditions r?elles le plus significatif eu lieu dans cette m?me maison (Eindhoven) 
dans  un environnement sonore  stationnaire.  Par stationnaire,  il  est  entendu que des 
personnes parlaient dans les pi?ces voisines et que leur voix parvenaient dans la pi?ce de 
test sans qu?? elles seules elles ne d?clenchent le processus de reconnaissance programm? 
pour ?tre effectif ? partir d?un certain niveau sonore per?u. Un locuteur n?erlandais, qui 
avait pr?c?demment adapt? le mod?le acoustique ? sa voix, prononce d?s lors les 168 
commandes d?finies ? ce moment.  Il  ?tait  autoris?  ? prononcer une seconde fois  les 
commandes  mal  ou  non  reconnues  au  premier  essai.  Le  taux  de  reconnaissances 
correctes constat?es s??l?ve alors ? 89%, constituant le seuil bas pour l??valuation de 
l??volution du syst?me. Pour des raisons de respect de la vie priv?e, les essais pratiques ? 
venir  ne  seront  pas  enregistr?s,  un  protocole  d??valuation  devrait  ?tre  ?tabli  pour 
reporter les r?sultats significatifs et scientifiques.  
8. Remerciements
Ce travail a ?t? soutenu par le projet europ?en CompanionAble. Nous remercions AKG 
(Vienne) et SmartHome (Eindhoven) pour leur appui. Nous remercions ?galement Daniel 
Caon et Pierre Sendorek pour leur aide dans les premi?res impl?mentations du syst?me 
de reconnaissance.
9. R?f?rences
LEE, A. (2008). The Julius Book.
ROUGUI,  J.  E.,  ISTRATE,  D.  et SOUIDENE,  W. (2009).  Audio  Sound Event Identification for 
distress situations and context awareness.  In EMBC2009, September 2-6, Minneapolis, 
USA, pp. 3501-3504.
ROUGUI,  J.  E.,  ISTRATE,  D.,   SOUIDENE,  W.,  OPITZ,  M.  et RIEMANN,  M. (2009).  Audio  based 
surveillance for cognitive assistance using a CMT microphone within socially assistive 
technology. In EMBC2009, September 2-6, Minneapolis, USA,  pp.2547-2550.
CAON,  D., SIMMONET,  T., BOUDY,  J.  et CHOLLET,  G. (2011).  vAssist:  The Virtual Interactive 
assistant  for  Daily  Home-care.  In  pHealth  conference,  8th International  Conference  on  
Wearable Nano and Macro Technologies for Personalized Health, Lyon, France.
M?LLER, S., SCHROETER, C. et GROSS, H.-M. (2010). Aspects of user specific dialog adaptation 
for an autonomous robot. In International Scientific Colloquium, Ilmenau, Allemagne.
30
