Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 62?71,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Assigning Deep Lexical Types
Using Structured Classifier Features for Grammatical Dependencies
Joa?o Silva
University of Lisbon
Dept. Informatics, Faculty of Sciences
Campo Grande, Lisboa, Portugal
jsilva@di.fc.ul.pt
Anto?nio Branco
University of Lisbon
Dept. Informatics, Faculty of Sciences
Campo Grande, Lisboa, Portugal
antonio.branco@di.fc.ul.pt
Abstract
Deep linguistic grammars are able to pro-
vide rich and highly complex grammatical
representations of sentences, capturing, for
instance, long-distance dependencies and re-
turning a semantic representation. These
grammars lack robustness in the sense that
they do not gracefully handle words miss-
ing from their lexicon. Several approaches
have been explored to handle this problem,
many of which consist in pre-annotating the
input to the grammar with shallow processing
machine-learning tools. Most of these tools,
however, use features based on a fixed win-
dow of context, such as n-grams. We investi-
gate whether the use of features that encode
discrete structures, namely grammatical de-
pendencies, can improve the performance of
a machine learning classifier that assigns deep
lexical types. In this paper we report on the
design and evaluation of this classifier.
1 Introduction
Parsing is one of the fundamental tasks in Nat-
ural Language Processing and a critical step in
many applications. Many of the most com-
monly used parsers rely on probabilistic approaches.
These parsers are obtained through data-driven
approaches, by inferring a probabilistic language
model over a dataset of annotated sentences.
Though these parsers always produce some analy-
sis of their input sentences, they do not go into deep
linguistic analysis.
Deep grammars, also referred to as precision
grammars, seek to make explicit information about
highly detailed linguistic phenomena and produce
complex grammatical representations for their in-
put sentences. For instance, they are able to cap-
ture long-distance dependencies and produce the se-
mantic representation of a sentence. Although there
is a great variety of parsing methods (see (Mitkov,
2004) for an overview), all CKY-based algorithms
require a lexical look-up initialization step that, for
each word in the input, returns all its possible cate-
gories.
From this it follows that if any of the words in
a sentence is not present in the lexicon?an out-
of-vocabulary (OOV) word?a full parse of that
sentence is impossible to obtain. Given that nov-
elty is one of the defining characteristics of natu-
ral languages, unknown words will eventually oc-
cur. Hence, being able to handle OOV words is of
paramount importance if one wishes to use a gram-
mar to analyze unrestricted texts.
Another important issue is that of lexical ambigu-
ity. That is, words that may bear more than one lexi-
cal category. The combinatorial explosion of lexical
and syntactic ambiguity may hinder parsing due to
increased requirements in terms of parsing time and
memory usage. Thus, even if there were no OOV
words in the input, being able to assign syntactic cat-
egories to words prior to parsing may be desirable
for efficiency reasons.
For the shallower parsing approaches, such as
plain constituency parsing, it suffices to determine
the part-of-speech of words, so pre-processing the
input with a POS tagger is a common and effective
way to tackle either of these problems. However, the
linguistic information contained in the lexicon of a
62
deep grammar is much more fine-grained, includ-
ing, in particular, the subcategorization frame (SCF)
of the word, which further constraints what can be
taken as a well-formed sentence by imposing sev-
eral restrictions on co-occurring expressions.
Thus, what for a plain POS tagger corresponds to
a single category is often expanded into hundreds of
different distinctions, and hence tags, when at the
level of detail required by a deep grammar. For in-
stance, the particular grammar we will be using for
the study reported in this paper?a grammar follow-
ing the HPSG framework?has in its current ver-
sion a lexicon with roughly 160 types for verbs and
nearly 200 types for common nouns.
While the deep grammar may proceed with the
analysis knowing only the base POS category of a
word, it does so at the cost of vastly increased am-
biguity1 which may even allow the grammar to ac-
cept ungrammatical sentences as valid. This has lead
to research that specifically targets annotating words
with a tagset suitable for deep grammars.
Current approaches tend to use shallow features
with limited context (e.g. n-grams). However, given
that the SCF is one of the most relevant pieces of
information that is associated with a word in the
lexicon of a deep grammar, one would expect that
features describing the inter-word dependencies in
a sentence would be highly discriminative and help
to accurately assign lexical types. Accordingly,
in this paper we investigate the use of structured
features that encode grammatical dependencies in
a machine-learning classifier and how it compares
with state-of-the-art approaches.
Our study targets Portuguese, a Romance lan-
guage with a rich morphology, in particular in what
concerns verb inflection (see for instance, (Mateus et
al., 2003) for a detailed account of Portuguese gram-
mar and (Branco et al, 2008) for an assessment of
the issues raised by verbal ambiguity).
Paper outline: Section 2 provides an overview of
related work, with a focus on supertagging, and in-
troduces tree kernels as a way of handling structured
classifier features. Section 3 introduces the particu-
lar deep grammar that is used in this work and how it
supports the creation of the corpus that provides the
1For instance, a common noun POS tag could be taken as
being any of the nearly 200 common noun types existing in the
lexicon of the grammar we use in this paper.
data for training and evaluation of the classifier. The
classifier itself, and the features it uses, are described
in Section 4. Section 5 covers empirical evaluation
and comparison with other approaches. Finally, Sec-
tion 6 concludes with some final remarks.
2 Background and Related Work
The construction of a hand-crafted lexicon for a deep
grammar is a time-consuming task requiring trained
linguists. More importantly, such lexica are invari-
ably incomplete since they often do not cover spe-
cialized domains and are slow to incorporate new
words.
Accordingly, much research in this area has been
focused on automatic lexical acquisition (Brent,
1991; Briscoe and Carroll, 1997; Baldwin, 2005).
That is, approaches that try to discover all the lex-
ical types a given unknown word may occur with,
thus effectively creating a new lexical entry. How-
ever, at run-time, it is still up to the grammar using
the newly acquired lexical entry to choose which of
those lexical types is the correct one for each par-
ticular occurrence of that word; and, ultimately, one
can only acquire the lexicon entries for those words
that are present in the corpus. Thus, any system that
is constantly exposed to new text?e.g. parsing text
from the Web?will eventually come across some
unknown word that has not yet been acquired. More-
over, such words must be dealt with on-the-fly, since
it is unlikely that the system can afford to wait until
it has accumulated enough occurrences of the un-
known word to be able to apply offline lexicon ac-
quisition methods.
In the work reported in the present paper we use
a different approach, closer to what is known as su-
pertagging, where we assign on-the-fly a single lex-
ical type to a word.
2.1 Supertagging
POS tagging is a task that relies only on local infor-
mation (e.g. the word and a small window of con-
text) to achieve a form of syntactic disambiguation.
As such, POS tags are commonly assigned prior
to parsing as a way of reducing parsing ambiguity
by restricting words to a certain syntactic category.
Less ambiguity leads to a greatly reduced search
space and, as a consequence, faster parsing.
63
Supertagging, first introduced by Bangalore and
Joshi (1994), can be seen as a natural extension of
this idea to a richer tagset, in particular to one that
includes information on subcategorization frames.
In (Bangalore and Joshi, 1994) supertagging was
applied to the Lexicalized Tree Adjoining Grammar
(LTAG) formalism. As the name indicates, this is a
lexicalized grammar, like HPSG, but in LTAG each
lexical item is associated with one or more trees,
the elementary structures, which localize informa-
tion on dependencies, even long-range ones, by re-
quiring that all and only the dependents be present
in the structure.
The supertagger in (Bangalore and Joshi, 1994)
assigns an elementary structure to each word us-
ing a simple trigram model. The data for training
was obtained by taking the sentences of length un-
der 15 words in the Wall Street Journal together with
some other minor corpora, and parsing them with
XTAG, a wide-coverage grammar for English based
on LTAG. In addition, and due to data-sparseness,
POS tags were used in training instead of words.
Evaluation was performed over 100 held-out sen-
tences from the Wall Street Journal. For a tagset of
365 elementary trees, this supertagger achieved 68%
accuracy, which is far too low to be useful for pars-
ing.
In a later experiment, the authors improved
the supertagger by smoothing model parameters
and adding additional training data (Bangalore and
Joshi, 1999). The larger dataset was obtained by
extending the corpus from the previous experiment
with Penn Treebank parses that were automatically
converted to LTAG. The conversion process relied
on several heuristics, and though it is not perfect,
the authors found that the issues concerning conver-
sion were far outweighed by the benefit of increased
training data.
The improved supertagger increased accuracy to
92% (Bangalore and Joshi, 1999). The supertagger
can also assign the n-best tags, which increases the
chances of it assigning the correct supertag at the
cost of leaving more unresolved ambiguity. With 3-
best tagging, it achieved 97% accuracy.
A supertagger was also used by Clark and Curran
(2007), in their case for a Combinatory Categorial
Grammar (CCG). This formalism uses a set of log-
ical combinators to manipulate linguistic construc-
tion tough, for our purposes here, it matters only
that lexical items receive complex tags that describe
the constituents they require to create a well-formed
construction.
The set of 409 lexical categories to be assigned
was selected by taking those categories that occur at
least 10 times in sections 02?21 of a CCG automatic
annotation of Penn Treebank (CCGBank).
Evaluation was performed over section 00 of
CCGBank, and achieved 92% per word accuracy.
As with the LTAG supertagger, assigning more
than one tag can greatly increase accuracy. How-
ever, instead of a fixed n-best number of tags?
which might be to low, or too high, depending on
the case at hand?the CCG supertagger assigns all
tags with a likelihood within a factor ? of the best
tag. A value for ? as small as 0.1, which results in
an average of 1.4 tags per word, is enough to boost
accuracy up to 97%.
Supertagging for HPSG: There has been some
work on using supertagging together with the HPSG
framework. As with other works on supertag-
ging, it is mostly concerned with restricting the
parser search space in order to increase parsing ef-
ficiency, and not specifically with the handling of
OOV words.
Prins and van Noord (2003) present an HMM-
based supertagger for the Dutch Alpino grammar.
An interesting feature of their approach is that the
supertagger is trained over the output of the parser
itself, thus avoiding the need for a hand-annotated
dataset.
The supertagger was trained over 2 million sen-
tences of newspaper text parsed by Alpino. A gold
standard was created by having Alpino choose the
best parse for a set of 600 sentences. The supertag-
ger, when assigning a single tag (from a tagset with
2,392 tags), achieves a token accuracy close to 95%.
It is not clear to what extent these results can be
affected by some sort of bias in the disambiguation
module of Alpino, given that both the sequence of
lexical types in the training dataset and in the gold
standard are taken from the best parse produced by
Alpino.
Matsuzaki et al (2007) use a supertagger with
the Enju grammar for English. The novelty in their
work comes from the use of a context-free gram-
mar (CFG) to filter the tag sequences produced by
64
the supertagger before running the HPSG parser. In
this approach, a CFG approximation of the HPSG
is created. The key property of this approxima-
tion is that the language it recognizes is a superset
of the parsable supertag sequences. Hence, if the
CFG is unable to parse a sequence, it can be safely
discarded, thus further reducing the amount of se-
quences the HPSG parser has to deal with.
The provided evaluation is mostly concerned with
showing the improvement in parsing speed. Nev-
ertheless, the quality of the supertagging process
can be inferred from the accuracy of the parse re-
sults, which achieved a labeled precision and recall
for predicate-argument relations of 90% and 86%,
respectively, over 2,300 sentences with up to 100
words in section 23 of the Penn Treebank.
Dridan (2009) tests two supertaggers, one induced
using the TnT tagger (Brants, 2000) and another us-
ing the C&C supertagger (Clark and Curran, 2007),
over different datasets. For simplicity, we will only
refer to the results of TnT over a dataset of 814 sen-
tences of tourism data.
The author experiments with various tag granu-
larities in order to find a balance between tag ex-
pressiveness and tag predictability. For instance, as-
signing only POS?a tagset with only 13 tags?is
the easiest task, with 97% accuracy, while a highly
granular supertag formed by the lexical type con-
catenated with any selectional restriction present in
the lexical entry increases the number of possible
tags to 803, with accuracy dropping to 91%.
2.2 Support-Vector Machines and Tree Kernels
Support-vector machines (SVM) are a well known
supervised machine-learning algorithm for linear
binary classification. They are part of the fam-
ily of kernel-based methods where a general pur-
pose learning algorithm is coupled with a problem-
specific kernel function (Cristianini and Shawe-
Taylor, 2000).
For the work presented in this paper we wish
to apply the learning algorithm over discrete tree-
like structures that encode grammatical dependen-
cies (see Figure 1 for an example). A suitable ker-
nel for such a task is the tree kernel introduced by
Collins and Duffy (2002), which uses a represen-
tation that implicitly tracks all subtrees seen in the
training data.
This representation starts by implicitly enumerat-
ing all subtrees that are found in the training data. A
given tree, T , is then represented by a (huge) vector
where the n-th position counts the number of occur-
rences of the n-th subtree in T .
Under this representation, the inner product of
two trees gives a measure of their similarity. How-
ever, explicitly calculating such an operation is pro-
hibitively expensive due to the high dimensions of
the feature space. Fortunately, the inner product can
be replaced by a rather simple kernel function that
sums over the subtrees that are common to both trees
(see (Collins and Duffy, 2002) for a proof).
3 Grammar and Base Dataset
The deep linguistic grammar used in this study
is LXGram, a hand-built HPSG grammar for Por-
tuguese (Branco and Costa, 2008; Branco and Costa,
2010).
We used this grammar to support the annota-
tion of a corpus. That is, the grammar is used
to provide the set of possible analyses for a sen-
tence (the parse forest). Human annotators then
perform manual disambiguation by picking the cor-
rect analysis from among all those that form the
parse forest.2 This grammar-supported approach to
corpus annotation ensures that the various linguis-
tic annotation layers?morphological, syntactic and
semantic?are consistent.
The corpus that was used is composed mostly by a
subset of the sentences in CETEMPu?blico, a corpus
of plain text excerpts from the Pu?blico newspaper.
After running LXGram and manually disam-
biguating the parse forests, we were left with a
dataset consisting of 5,422 sentences annotated with
all the linguistic information provided by LXGram.
4 Classifier and Feature Extraction
For training and classification we use SVM-light-TK
(Moschitti, 2006), an extension to the widely-used
SVM-light (Joachims, 1999) software for SVMs that
adds a function implementing the tree kernel intro-
duced in Section 2.2. With SVM-light-TK one can
2In our setup, two annotators work in a double-blind
scheme, where those cases where they disagree are adjudicated
by a third annotator. Inter-annotator agreement is 0.86.
65
directly provide one or more tree structures as fea-
tures (using the standard parenthesis representation
of trees) together with the numeric feature vectors
that are already accepted by SVM-light.
Given that the task at stake is a multi-class clas-
sification problem but an SVM is a binary classi-
fier, the problem must first be binarized (Galar et
al., 2011). For this work we have chosen a one-
vs-one binarization scheme, where multiple classi-
fiers are created, each responsible for discriminat-
ing between a pair of classes. This divides a prob-
lem with n classes into n(n ? 1)/2 separate binary
problems (i.e. one classifier for each possible class
pairing). Each classifier then performs a binary de-
cision, voting for one of the two classes it is tasked
with discriminating, and the class with the overall
largest number of votes is chosen.
The dataset, having been produced with the help
of a deep grammar, contains a great deal of linguistic
information. The first step is thus to extract from
each sentence the relevant features in a format that
can be used by SVM-light-TK.
Since we are aiming at discriminating between
deep lexical types, which, among other information,
encode the SCF of a word, the dependency structure
associated with a word is expected to be a piece of
highly relevant information. We start by extracting
the dependency representation of a sentence from
the output of LXGram.3 The dependency represen-
tation that is obtained through this process consists
of a list of tuples, each relating a pair of words in the
sentence through a grammatical relation.
The example in Figure 1 shows the dependency
representation of the sentence ?a o segundo dia de
viagem encontra?mos os primeiros golfinhos? (Eng.:
by the second day of travel we found the first dol-
phins).4 Note that each word is also annotated with
its lexical type, POS tag and lemma, though this is
not shown in the example for the sake of readability.
For a one-vs-one classifier tasked with discrim-
inating between types A and B we are concerned
with finding instances of type A to be taken as posi-
tive examples and instances of type B to be taken as
3The details of this process are outside the scope of the cur-
rent paper and will be reported elsewhere.
4Relations in the example: ADV (adverb), C (complement),
DO (direct object), PRED (predicate), SP (specifier) and TMP
(temporal modifier).
negative examples.
Take, for instance, the word ?encontra?mos? from
the example in Figure 1. Its lexical type in this par-
ticular occurrence is verb-dir trans-lex, the type as-
signed to transitive verbs by LXGram. A one-vs-one
classifier tasked with recognizing this type (against
some other type) will take this instance as a positive
example.
However, the full dependency representation of
the sentence has too many irrelevant features for
learning how to classify this word. Instead, we fo-
cus more closely on the information that is relevant
to determining the SCF of the word by looking only
at its immediate neighbors in the dependency graph:
its dependents and the word it depends on.
This information is encoded in two trees, shown
in Figure 2, which are the actual features given to
SVM-light-TK.
One tree, labeled with H as root, is used to repre-
sent the word and its dependents. The target word is
marked by being under an asterisk ?category? while
the dependents fall under a ?category? correspond-
ing to the relation between the target word and the
dependent. The words appears as the leafs of the
tree, with their POS tags as the pre-terminal nodes.5
The second feature tree, labeled with D as root,
encodes the target word?again marked with an
asterisk?and the word it is dependent on. In the
example shown in Figure 2, since the target word is
the main verb of the sentence, the feature tree has no
other nodes apart from that of the target word.
5 Evaluation
The following evaluation results were obtained fol-
lowing a standard 10-fold cross-validation approach,
where the folds were taken from a random shuffle of
the sentences in the corpus.
We compare the performance of our tree kernel
(TK) approach with two other automatic annotators,
TnT (Brants, 2000) and SVMTool (Gime?nez and
Ma`rquez, 2004).
TnT is a statistical POS tagger, well known for
its efficiency?in terms of training and tagging
speed?and for achieving state-of-the-art re-
sults despite having a quite simple underlying
5POS tags in the example: V (verb), PREP (preposition) and
CN (common noun).
66
C(de, viagem) SP(dia, o) C(a, dia)
ADV(dia, de) PRD(golfinhos, primeiros) TMP(encontra?mos, a)
PRD(dia, segundo) SP(golfinhos, os) DO(encontra?mos, golfinhos)
Figure 1: Dependency representation
H
TMP
PREP
a
by
DO
CN
golfinhos
dolphins
*
V
encontra?mos
we-found
D
*
V
encontra?mos
we-found
Figure 2: Features for SVM-light-TK
model. It is based on a second-order hidden
Markov model extended with linear smooth-
ing of parameters to address data-sparseness is-
sues and suffix analysis for handling unknown
words. TnT was used as a supertagger in (Dri-
dan, 2009), where it achieved the best results
for this task, and is thus a good representative
for this approach to supertagging. We run it
out-of-the-box using the default settings.
SVMTool is another statistical sequential tagger
which, as the name indicates, is based on
support-vector machines. It is extremely flexi-
ble in allowing to define which features should
be used in the model (e.g. size of word win-
dow, number of POS bigrams, etc.) and the tag-
ging strategy (left to right, bidirectional, num-
ber of passes, etc). In fact, due to this flexibil-
ity, it is described as being a tagger generator.
It beat TnT in a POS tagging task (Gime?nez
and Ma`rquez, 2004), so we use it in the current
paper to evaluate whether that lead is kept in
a supertagging task. We used the simplest set-
tings, ?M0 LR?, which uses Model 0 in a left
to right tagging direction.6
6See (Gime?nez and Ma`rquez, 2006) for an explanation of
these settings.
The type distribution in the dataset is highly
skewed. For instance, from the number of com-
mon noun types that occur in this corpus, the two
most frequent ones are enough to account for 57%
of all the common noun tokens. Such skewed cat-
egory distributions are usually a problematic issue
for machine-learning approaches since the number
of instances of the more rare categories is too small
to properly estimate the parameters of the model.
For many types there are not enough instances in
the dataset to train a classifier. Hence, the evalua-
tion that follows is done only for the most frequent
types. For instance, top-10 means picking the 10
most frequent types in the corpus, training one-vs-
one classifiers for those types, and evaluating only
over tokens with one of those types. In addition, we
show only the evaluation results of verb types, for
which SCF information is more varied and relevant.
Table 1 show the accuracy results for each tool
over the top-10, top-20 and top-30 most frequent
verb types.
Comparing both sequential supertaggers, one
finds that SVMTool is consistently better than TnT,
which is in accordance with the results for POS tag-
ging reported in (Gime?nez and Ma`rquez, 2004).
Our TK approach beats both supertaggers when
67
TnT SVMTool TK
top-10 92.98% 94.22% 94.71%
top-20 91.53% 92.39% 90.21%
top-30 91.42% 92.38% 88.70%
Table 1: Accuracy over frequent verb types
looking at the top-10 verb types, but falls behind as
soon as the number of types under consideration in-
creases. This seems to point towards data-sparseness
issues, an hypothesis we test by automatically ex-
tending the dataset, as discussed next.
5.1 Experiments with an Extended Dataset
The extended datasets were created by taking ad-
ditional sentences from the Pu?blico newspaper, as
well as sentences from the Portuguese Wikipedia
and from the Folha de Sa?o Paulo newspaper, pre-
processing them with a POS tagger, and running
them through LXGram.
Such an approach is only made possible because
LXGram, like many other modern HPSG gram-
mars, includes a stochastic disambiguation module
that automatically chooses the most likely analysis
among all those returned in the parse forest, instead
of requiring a manual choice by a human annota-
tor (Branco and Costa, 2010). The authors do not
provide a complete evaluation of this disambigua-
tion module. Instead, they perform a manual evalu-
ation of a sample of 50 sentences that indicates that
this module picks the correct reading in 40% of the
cases.
If this ratio is kept, 60% of the sentences in the ex-
tended datasets will have an analysis that is, in some
way, the wrong analysis, though it is not clear how
this translates into errors in the lexical types that end
up being assigned to the tokens. For instance, when
faced with the rather common case of PP-attachment
ambiguity, the disambiguation module may choose
the wrong attachment, which will count as being a
wrong analysis though most lexical types assigned
to the words in the sentence may be correct.
To evaluate this, we tested the disambiguation
module over the base dataset, where we know what
the correct parses are, and found that the grammar
picks the correct parse in 44% of the cases. If we
just look at whether the lexical types are correct, the
dataset sentences tokens unique oov
base 5,422 51,483 8,815 10.0%
+ Pu?blico 10,727 139,330 18,899 7.6%
+ Wiki 15,108 205,585 24,063 6.6%
+ Folha 21,217 288,875 30,204 6.0%
Table 2: Cumulative size of datasets
grammar picks a sentence with fully correct types in
68% of the cases.
LXGram displayed a coverage of roughly 30%,
and allowed us to build progressively larger datasets
as more data was added. The cumulative sizes of the
resulting datasets are shown in Table 2. The Table
also shows the ratio of OOV words, which was de-
termined by taking the average of the ratio for each
of the 10 folds (i.e. words that occur in a fold but not
in any of the other 9 folds).
We can now evaluate the tools over the four pro-
gressively larger datasets and plot their learning
curves. In the following Figures, the errors bars rep-
resent a 95% confidence interval.
All learning curves in the following Figures tell a
somewhat similar story.
The lead that SVMTool has over TnT when look-
ing only at the base corpus is kept in the extended
corpora. Both sequential supertaggers only start to
benefit from the increased dataset at the final stage,
when sentences from Folha de Sa?o Paulo are added.
Before that stage the added data seems to be slightly
detrimental to them, possibly due to them being sen-
sitive to noise in the automatically generated data.
The learning curves give credence to the hypoth-
esis put forward earlier that our TK approach was
being adversely affected by data-sparseness issues
when classifying a greater number of verb types, and
that it has much to gain by an increase in the amount
of training data.
For the top-10 verb types, for which there is
enough data in the base dataset, TK starts ahead
from the outset and significantly increases its mar-
gin over the two supertaggers.
For the top-20 and top-30 verb types, TK starts
behind but its accuracy raises quickly as more data
are added, ending slightly ahead of SVMTool when
running over the largest dataset.
68
5000 10000 15000 20000
0.90
0.92
0.94
0.96
Over top?10 verb types
Dataset size
Accu
racy
l
l l
l
l TnTSVMToolSVM?TK
Figure 3: Learning curves (over top-10 verb types)
5000 10000 15000 20000
0.90
0.92
0.94
0.96
Over top?20 verb types
Dataset size
Accu
racy
l l
l
l
l TnTSVMToolSVM?TK
Figure 4: Learning curves (over top-20 verb types)
5000 10000 15000 20000
0.90
0.92
0.94
0.96
Over top?30 verb types
Dataset size
Accu
racy
l l
l
l
l TnTSVMToolSVM?TK
Figure 5: Learning curves (over top-30 verb types)
dataset accuracy
base 87.24%
+ Pu?blico 82.67%
+ Wiki 82.30%
+ Folha 83.92%
Table 3: MaltParser labeled accuracy
5.2 Running over Predicted Dependencies
In the previous section, we were concerned with
evaluating the classifier itself. Accordingly, the fea-
tures used by the classifier were the gold dependen-
cies in the corpus. However, on a running system,
the features used by the classifier will be automati-
cally generated by a dependency parser. To evaluate
this setup, we used MaltParser (Nivre et al, 2007).
Like the other tools, the parser was run out-of-the-
box. The 10-fold average labeled accuracy scores
for each dataset shown in Table 3 can thus be seen
as a lower bound on the achievable accuracy. De-
spite this, the performance over the base dataset is
extremely good, on par with the best scores achieved
for other languages (cf. (Nivre et al, 2007)). How-
ever, performance drops sharply when automatically
annotated data is used, only beginning to pick up
again when running over the largest dataset.
As expected, the noisy features that result from
the automatic process have a detrimental effect on
the accuracy of the classifier. For the same set of
experiments reported previously, the accuracy of the
SVM-TK classifier when running over predicted de-
pendencies tends to trail 2.0?2.5% points behind
that of the classifier that uses gold dependencies, as
shown in Table 4.
6 Concluding Remarks
In this paper we reported on an novel approach to as-
signing deep lexical types. It uses an SVM classifier
with a tree kernel that allows it to seamlessly work
with features encoding discrete structures represent-
ing the grammatical dependencies between words.
Evaluation over the top-10 most frequent verb
types showed that the grammatical dependencies of
a word, which can be seen as information on its SCF,
are very helpful in allowing the classifier to accu-
rately assign lexical types. Our classifier clearly im-
69
top-10 top-20 top-30
dataset gold pred. gold pred. gold pred.
base 94.71% 93.14% 90.21% 88.66% 88.70% 87.01%
+ Pu?blico 96.02% 93.83% 92.34% 90.35% 91.32% 88.97%
+ Wiki 96.48% 93.95% 93.54% 91.29% 92.80% 90.21%
+ Folha 96.98% 94.55% 94.46% 92.26% 93.93% 91.50%
Table 4: SVM-TK classifier accuracy over gold and predicted features
proves over TnT, which had displayed the best su-
pertagging performance in other studies.
When running the classifier for a greater number
of verb types, data-sparseness issues led to a drop
in performance, which motivated additional experi-
ments where the dataset was extended with automat-
ically annotated data. This allowed us to plot learn-
ing curves that show that our approach can maintain
a lead in accuracy when given more training data.
Running the classifier over predicted features
shows an expected drop in performance. However,
we anticipate that using larger corpora will also
be effective in raising these scores since additional
training data not only improve the classifier, but also
the underlying parser that provides the dependencies
that are used as features.
References
Timothy Baldwin. 2005. Bootstrapping deep lexical re-
sources: Resources for courses. In Timothy Baldwin,
Anna Korhonen, and Aline Villavicencio, editors, Pro-
ceedings of the ACL-SIGLEX Workshop on Deep Lex-
ical Acquisition, pages 67?76.
Srinivas Bangalore and Aravind Joshi. 1994. Disam-
biguation of super parts of speech (or supertags): Al-
most parsing. In Proceedings of the 15th Conference
on Computational Linguistics (COLING), pages 154?
160.
Srinivas Bangalore and Aravind Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational
Linguistics, 25(2):237?265.
Anto?nio Branco and Francisco Costa. 2008. A computa-
tional grammar for deep linguistic processing of Por-
tuguese: LX-Gram, version A.4.1. Technical Report
DI-FCUL-TR-08-17, University of Lisbon.
Anto?nio Branco and Francisco Costa. 2010. A deep lin-
guistic processing grammar for Portuguese. In Pro-
ceedings of the 9th Encontro para o Processamento
Computacional da L??ngua Portuguesa Escrita e Fal-
ada (PROPOR), LNAI, pages 86?89. Springer.
Anto?nio Branco, Francisco Costa, and Filipe Nunes.
2008. The processing of verbal inflection ambiguity:
Characterization of the problem space. In Proceedings
of the 21st Encontro Anual da Associac?a?o Portuguesa
de Lingu??stica (APL), pages 2577?2583.
Thorsten Brants. 2000. TnT ? a statistical part-of-
speech tagger. In Proceedings of the 6th Applied Natu-
ral Language Processing Conference and the 1st North
American Chapter of the Association for Computa-
tional Linguistics, pages 224?231.
Michael Brent. 1991. Automatic acquisition of subcat-
egorization frames from untagged text. In Proceed-
ings of the 29th Annual Meeting of the Association for
Computational Linguistics, pages 209?214.
Ted Briscoe and John Carroll. 1997. Automatic extrac-
tion of subcategorization from corpora. In Proceed-
ings of the 5th Applied Natural Language Processing
Conference, pages 356?363.
Stephen Clark and James Curran. 2007. Wide-coverage
efficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33:493?552.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 263?270.
Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and Other
Kernel-Based Learning Methods. Cambridge Univer-
sity Press.
Rebecca Dridan. 2009. Using Lexical Statistics to Im-
prove HPSG Parsing. Ph.D. thesis, University of Saar-
land.
Mikel Galar, Alberto Fernande?z, Edurne Barrenechea,
Humberto Bustince, and Francisco Herrera. 2011. An
overview of ensemble methods for binary classifiers
in multi-class problems: Experimental study in one-
vs-one and one-vs-all schemes. Pattern Recognition,
44:1761?1776.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool: A
general POS tagger generator based on support vector
70
machines. In Proceedings of the 4th Language Re-
sources and Evaluation Conference (LREC).
Jesu?s Gime?nez and Llu??s Ma`rquez, 2006. SVMTool:
Technical Manual v1.3. TALP Research Center, LSI
Department, Universitat Politecnica de Catalunya.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Scho?lkopf, C. Burges, and A.
Smola, editors, Advances in Kernel Methods ? Sup-
port Vector Learning, chapter 11, pages 169?184. MIT
Press, Cambridge, MA.
Maria Helena Mira Mateus, Ana Maria Brito, Ine?s
Duarte, Isabel Hub Faria, So?nia Frota, Gabriela Matos,
Fa?tima Oliveira, Marina Viga?rio, and Alina Villalva.
2003. Grama?tica da L??ngua Portuguesa. Caminho,
5th edition.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Efficient HPSG parsing with supertagging and
CFG-filtering. In Proceedings of the 20th Interna-
tional Joint Conference on Artificial Intelligence (IJ-
CAI), pages 1671?1676.
Ruslan Mitkov, editor. 2004. The Oxford Handbook of
Computational Linguistics. Oxford University Press.
Alessando Moschitti. 2006. Making tree kernels prac-
tical for natural language learning. In Proceedings
of the 11th European Chapter of the Association for
Computational Linguistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Robbert Prins and Gertjan van Noord. 2003. Reinforc-
ing parser preferences through tagging. Traitment Au-
tomatique des Langues, 44:121?139.
71
