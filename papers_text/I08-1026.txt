A Study on Effectiveness of Syntactic Relationship in Dependence Re-
trieval Model 
Fan Ding1,2
1: Graduate University,  
Chinese Academy of Sciences  
Beijing, 100080, China  
dingfan@ict.ac.cn 
Bin Wang2
2: Institute of Computing Technology, 
Chinese Academy of Sciences  
Beijing, 100080, China 
wangbin@ict.ac.cn 
 
 
Abstract 
To relax the Term Independence Assump-
tion, Term Dependency is introduced and it 
has improved retrieval precision dramati-
cally. There are two kinds of term depend-
encies, one is defined by term proximity, 
and the other is defined by linguistic de-
pendencies. In this paper, we take a com-
parative study to re-examine these two 
kinds of term dependencies in dependence 
language model framework. Syntactic rela-
tionships, derived from a dependency 
parser, Minipar, are used as linguistic term 
dependencies. Our study shows: 1) Lin-
guistic dependencies get a better result than 
term proximity. 2) Dependence retrieval 
model achieves more improvement in sen-
tence-based verbose queries than keyword-
based short queries. 
1 Introduction 
For the sake of computational simplicity, Term 
Independence Assumption (TIA) is widely used in 
most retrieval models. It states that terms are statis-
tically independent from each other. Though un-
reasonable, TIA did not cause very bad perform-
ance. However, relaxing the assumption by adding 
term dependencies into the retrieval model is still a 
basic IR problem. Relaxing TIA is not easy be-
cause improperly relaxing may introduce much 
noisy information which will hurt the final per-
formance. Defining the term dependency is the 
first step in dependence retrieval model. Two re-
search directions are taken to define the term de-
pendency. The first is to treat term dependencies as 
term proximity, for example, the Bi-gram Model 
(F. Song and W. B. Croft, 1999) and Markov Ran-
dom Field Model (D. Metzler and W. B. Croft, 
2005) in language model. The second direction is 
to derive term dependencies by using some linguis-
tic structures, such as POS block (Lioma C. and 
Ounis I., 2007) or Noun/Verb Phrase (Mitra et al, 
1997), Maximum Spanning Tree (C. J. van 
Rijsbergen, 1979) and Linkage Model (Gao et al, 
2004) etc.  
Though linguistic information is intensively 
used in QA (Question Answering) and IE (Infor-
mation Extraction) task, it is seldom used in docu-
ment retrieval (T. Brants, 2004). In document re-
trieval, how effective linguistic dependencies 
would be compared with term proximity still needs 
to be explored thoroughly. 
In this paper, we use syntactic relationships de-
rived by a popular dependency parser, Minipar (D. 
Lin, 1998), as linguistic dependencies. Minipar is a 
broad-coverage parser for the English language. It 
represents the grammar as a network of nodes and 
links, where the nodes represent grammatical cate-
gories and the links represent types of dependency.  
We extract the dependencies between content 
words as term dependencies. 
To systematically compare term proximity with 
syntactic dependencies, we study the dependence 
retrieval models in language model framework and 
present a smooth-based dependence language 
model (SDLM). It can incorporate these two kinds 
of term dependencies. The experiments in TREC 
collections show that SDLM with syntactic rela-
tionships achieves better result than with the term 
proximity. 
The rest of this paper is organized as follows. 
Section 2 reviews some previous relevant work, 
197
Section 3 presents the definition of term depend-
ency using syntactic relationships derived by 
Minipar. Section 4 presents in detail the smooth-
based dependence language model. A series of ex-
periments on TREC collections are presented in 
Section 5. Some conclusions are summarized in 
Section 6. 
2 Related Work 
Generally speaking, when using term dependencies 
in language modeling framework, two problems 
should be considered: The first is to define and 
identify term dependencies; the second is to 
integrate term dependencies into a weighting 
schema. Accordingly, this section briefly reviews 
some recent relevant work, which is summarized 
into two parts: the definition of term dependencies 
and weight of term dependencies. 
2.1 Definition of Term Dependencies 
In definition of term dependencies, there are two 
main methods: shallow parsing by some linguistic 
tools and term proximity with co-occurrence in-
formation. Both queries and documents are repre-
sented as a set of terms and term dependencies 
among terms. Table 1 summarizes some recent 
related work according to the method they use to 
identify term dependencies in queries and docu-
ments. 
Methods Document 
Parsing 
Document Proximity
Query 
Parsing 
I: DM,LDM, 
etc. 
II: CULM, RP, etc. 
Query 
Proximity 
III: NIL IV: BG ,WPLM, 
MRF, etc. 
Table 1. Methods in identifying dependencies 
In the part I of table 1, DM is Dependence Lan-
guage Model (Gao et al, 2004). It introduces a de-
pendency structure, called linkage model. The 
linkage structure assumes that term dependencies 
in a sentence form an acyclic, planar graph, where 
two related terms are linked. LDM (Gao et al, 
2005) represents the related terms as linguistic 
concepts, which can be semantic chunks (e.g. 
named entities like person name, location name, 
etc.) and syntactic chunks (e.g. noun phrases, verb 
phrases, etc.).  
In the part II of table 1, CULM (M. Srikanth and 
R. Srihari, 2003) is a concept unigram language 
model. The parser tree of a user query is used to 
identify the concepts in the query. Term sequence 
in a concept is treated as bi-grams in the document 
model.  RP (Recognized Phrase, S. Liu et al, 2004) 
uses some linguistic tools and statistical tools to 
recognize four types of phrase in the query, includ-
ing proper names, dictionary phrase, simple phrase 
and complex phrase. A phrase is in a document if 
all its content words appear in the document within 
a certain window size. The four kinds of phrase 
correspond to variant window size. 
In the part IV of table 1, BG (bi-gram language 
model) is the simplest model which assumes term 
dependencies exist only between adjacent words 
both in queries and documents. WPLM (word pairs 
in language model, Alvarez et al, 2004) relax the 
co-occurrence window size in documents to 5 and 
relax the order constraint in bi-gram model. MRF 
(Markov Random Field) classify the term depend-
encies in queries into sequential dependence and 
full dependence, which respectively corresponds to 
ordered and unordered co-occurrence within a pre-
define-sized window in documents. 
From above discussion we can see that when the 
query is sentence-based, parsing method is pre-
ferred to proximity method. When the query is 
keyword-based, proximity method is preferred to 
parsing method. Thorsten (T. Brants, 2004) note: 
the longer the queries, the bigger the benefit of 
NLP. This conclusion also holds for the definition 
of query term dependencies. 
2.2 Weight of Term Dependencies 
In dependence retrieval model, the final relevance 
score of a query and a document consists of both 
the independence score and dependence score, 
such as Bahadur Lazarsfeld expansion (R. M. 
Losee, 1994) in classical probabilistic IR models. 
However, Spark Jones et al point out that without 
a theoretically motivated integration model, docu-
ments containing dependencies (e.g. phrases) may 
be over-scored if they are weighted in the same 
way as single words (Jones et al, 1998). Smooth-
ing strategy in language modeling framework pro-
vide such an elegant solution to incorporate term 
dependencies. 
In the simplest bi-gram model, the probability of 
bi-gram (qi-1,qi) in document D is smoothed by its 
unigram: 
198
)|(
)|(
),|(,
),|()1()|(),|(
1
1
1
11
DqP
DqqP
DqqPwhere
DqqPDqPDqqP
i
ii
ii
iiiiismoothed
?
?
?
??
?
??+?= ??
  (1) 
Further, the probability of bi-gram (qi-1,qi) in 
document P(qi|qi-1,D) can be smoothed by its prob-
ability in collection P(qi|qi-1,C). If P(qi|qi-1,D) is 
smoothed as Equation (1), the relevance score of 
query Q={q1q2?qm} and document D is: 
)|()|(
)|(
log)|,(,
)|,()|(log
)
)|()|(
)|(1
1log()|(log
)
)|(
),|(
)1(log()|(log
)),|()1()|(log()|(log
),|(log)|(log)|(log
1
1
1
...2
1
...1
...2 1
1
...1
...2
1
...1
...2
11
...2
11
DqPDqP
DqqP
DqqMIusually
DqqMIDqP
DqPDqP
DqqP
DqP
DqP
DqqP
DqP
DqqPDqPDqP
DqqPDqPDQP
ii
ii
ii
mi
iismoothed
mi
i
mi ii
ii
mi
i
mi i
ii
mi
i
mi
iii
mi
iismoothed
?
?
?
=
?
=
= ?
?
=
=
?
=
=
?
=
?
??
+=
??
?++?
??++=
??+?+=
+=
??
??
??
?
?
?
?
??
??
 (2) 
In Equation (2), the first score term is independ-
ence unigram score and the second score term is 
smoothed dependence score. Usually ? is set to 0.9, 
i.e., the dependence score is given a less weight 
compared with the independence score. 
DM (Gao et al, 2004), which can be regarded as 
the generalization of the bi-gram model, gives the 
relevance score of a document as:  
?
?
?
=
+
+=
Lji
ji
mi
i
DLqqMI
DLPDqPDQP
),(
...1
),|,(
)|(log)|(log)|(log
     (3) 
In Equation (3),L is the set of term dependencies 
in query Q. The score function consists of three 
parts: a unigram score, a smoothing factor 
logP(L|D), and a dependence score MI(qi,qj|L,D). 
MRF (D. Metzler and W. B. Croft, 2005) com-
bines the score of full independence, sequential 
dependence and full dependence in an interpolated 
way with the weight (0.8, 0.1, 0.1).  
Though these above models are derived from 
different theories, smoothing is an important part 
when incorporating term dependencies.  
3 Syntactic Parsing of Queries and 
Documents 
Term dependencies defined as term proximity may 
contain many ?noisy? dependencies. It?s our belief 
that parsing technique can filter out some of these 
noises and syntactic relationship is a clue to define 
parser, Minipar, to extract the syntactic depend-
ency between words.  In this section we will dis-
cuss the extraction of syntactic dependencies and 
the indexing schemes of term dependencies. 
3.1 Extraction of Syntactic Dependencie
term dependencies.  We use a popular dependency 
s 
ary 
an
des in the parsing result are single 
w
A dependency relationship is an asymmetric bin
relationship between a word called head (or 
governor, parent), and another word called 
modifier (or dependent, daughter). Dependency 
grammars represent sentence structures as a set of 
dependency relationships. For example, Figure 1 
takes the description field of TREC topic 651 as an 
example and shows part of the parsing result of 
Minipar. 
 
In Figure 1, Cat is the lexical category of word, 
d Rel is a label assigned to the syntactic depend-
encies, such as subject (sub), object (obj), adjunct 
(mod:A), prepositional attachment (Prep:pcomp-n), 
etc. Since function words have no meaning, the 
dependency relationships including function words, 
such as N:det:Det, are ignored. Only the depend-
ency relationships between content words are ex-
tracted. However, prepositional attachment is an 
exception. A prepositional noun phrase contains 
two parts: (N:mod:Prep) and (Prep:pcomp-n:N). 
We combine these two parts and get a relationship 
between nouns. 
Mostly, the no
ords. When the nodes are proper names, diction-
ary phrases, or compound words connected by hy-
phen, there are more than one word in the node. 
For example, the 5th and 6th relationship in Figure 1 
describes a compound word ?make up?.  We di-
vide these nodes into bi-grams, which assume de-
pendencies exist between adjacent words inside the 
Figure 1. Parsing Result of Minipar 
Node2 Node1 Cat1:Rel:Cat2 
TREC Topic 651: ?How is the ethnic make-
up of the U.S. population changing?? 
? 
3   makeup N:det:Det the 
4   makeup N:mod:A ethnic 
5   makeup N:lex-mod:U make 
6   makeup N:lex-mod:U - 
8   makeup N:mod:Prep of 
11 of  Prep:pcomp-n:N population 
9   population N:det:Det the 
10 population N:nn:N  U.S. 
? 
199
nodes.  If the compound-word node has a relation-
ship with other nodes, each word in the compound-
word node is assumed to have a relationship with 
the other nodes. Finally, the term dependencies are 
represented as word pairs. The direction of syntac-
tic dependencies is ignored. 
3.2 Indexing of Term Dependencies 
And the 
 
of
e that 
trieval status value (RSV) has the form: 
Parsing is a time-consuming process. 
documents parsing should be an off-line process. 
The parsing results, recognized as term dependen-
cies, should be organized efficiently to support the 
computation of relevance score at the retrieval step. 
As a supplement of regular documents?words 
inverted index, the indexing of term dependencies 
is organized as documents?dependencies lists. 
For example, Document A has n unique words; 
each of these n words has relationships with at 
least one other word. Then the term dependencies 
inside these n words can be represented as a half-
angle matrix as Figure 2 shows.  
 
The (i,j)-th element of the matrix is the number
 times that tidi and tidj have a dependency in 
document A. The matrix has the size of (n-1)*n/2 
and it is stored as list of size (n-1)*n/2. Each 
document corresponds to such a matrix. When ac-
cessing the term dependencies index, the global 
word id in the regular index is firstly converted to 
the internal id according to the word?s appearance 
order in the document. The internal id is the index 
of the half-angle matrix. Using the internal id pair, 
we can get its position in the matrix. 
4 Smooth-based Dependence Model 
From the discussion in section 2.2, we can se
smoothing is very important not only in unigram 
language model, but also in dependence language 
model. Taking the smoothed unigram model (C. 
Zhai and J. Lafferty, 2001) as the example, the re-
D
DQw D
DML
UG QCwp
Dwp
QwcDQRSV ?? log||)|(
)|(
log),(),( += ?
??
In Equation (4), c(w,Q) is the frequency of w
Q. The equation has three parts: PDML(w|D), ?D and 
P(
 (4) 
 in 
w|C). PDML(w|D) is the discounted maximum 
likelihood estimation of unigram P(w|D), ?D is the 
smoothing coefficient of document D, and P(w|C) 
is collection language model. If we use a smooth-
ing strategy as the smoothed MI in Equation (2), 
and replace term w with term pair (wi,wj), we can 
get the smoothed dependence model as: 
?
??
?+
=
DLww jismooth
j
ji
ji
Cwwp
D
Qwwc
),(
0 ))|,(
)|
1log(),,( ?  (5) 
In Equation (5), ?0 is the smoothing coefficient. 
Psm (w ,w |D) and Psm (w ,w |C) is the smoothed 
w
i j
e the Psmooth(wi,wj|D): 
 pair with relation-
sh
ismooth
DEP
wwp
DQRSV
,(
),(
ooth i j ooth i j
eight of term pair (wi,wj) in document D and col-
lection C.  
4.1 Smoothing P(w ,w |D) 
We use two parts to estimat
one is the weight of the term
ips in D, P(wi,wj|R,D), the other is the weight of 
the term co-occurrence in D, Pco(wi,wj|D). These 
two parts are defined as below: 
|D|)/(wCD)|P(w
D)|P(wD)|P(wD)|w,(wP
|D|R)/,w,(wCD)R,|w,P(w jiDji
?=
tid1 tid2 ? tidn-1 tidn
tid1
tid2
? 
 
tidn-1
tidn
 
??
?
?
??
?
?
?
??
?
?
??
?
?
?
0****
10...**
03...**
45..0*
20...10
 
iDi
jijiCO
=
=
   (6) 
|D| is the document length, CD(wi,wj,R) denotes 
the count of the dependency (wi,w ) in the docu-
m
jico1
j
ent D, and CD(wi) is the frequency of word wi in 
D. Psmooth(wi,wj|D) is defined as a combination of 
the two parts: 
P )?-(1
D)R,|w,P(w? D)|w,(wP ji1jismooth
?+ D)|w,(w
?=
 (7) 
Figure 2. Half-angle matrix of term dependencies
4.2 Smoothing P(wi,wj|C) 
bability of term pair 
. We use docu-
m
To directly estimate the pro
(w ,w ) in the collection is not easyi j
ent frequency of term pair (wi,wj) as its approxi-
mation. Same as Psmooth(wi,wj|D), Psmooth(wi,wj|C) 
consists of two parts: one is the document fre-
quency of term pair (wi,wj), DF(wi,wj), the other is 
the averaged document frequency of wi and wj. 
Then, Psmooth(wi,wj|C) is defined as: 
Dji
Djijismooth
CwDFwDF
CwwDFCwwP
||)()()1(
||),()|,( 2
???+ 2
?=
?
?
  (8) 
200
In Equation (8), |C|D is the count of Document in 
Collection C.  
Finally, if substituting Equation (7) and (8) into 
Eq
). The final retrieval status value of 
th
s 
To answer the question whether the syntactic de-
term proximity, 
,w ,R) in Equa-
tio
parameter 
is 
ns. Some statistics of the col-
lec
rameters (? ,? ,? ), 
SD
(MB) Doc.
uation (5), there are three parameters (?0,?1,?2) 
in RSVDEP(Q,D
e smooth-based dependence model, RSVSDLM, is 
the sum of RSVDEP and RSVUG: 
),(),(),( DQRSVDQRSVDQRSV UGDEPSDLM +=   (9) 
5 Experiments and Result
pendencies is more effective than 
we systematically compared their performance on 
two kinds of queries. One is verbose queries (the 
description field of TREC topics), the other is short 
queries (the title field of TREC topics). Since the 
verbose queries are sentence-level, they are parsed 
by Minipar to get the syntactic dependencies. In 
short queries, term proximity is used to define the 
dependencies, which assume every two words in 
the queries have a dependency.  
Our smooth-based dependence language model 
(SDLM) is used as dependence retrieval model in 
the experiments. If defining CD(wi j
n (6) to different meanings, we can get a de-
pendence model with syntactic dependencie, 
SDLM_Syn, or a dependence model with term 
proximity, SDLM_Prox. In SDLM_Syn, 
CD(wi,wj,R) is the count of syntactic dependencies 
between wi and wj in D. In SDLM_Prox, 
CD(wi,wj,R) is the number of times the terms wi 
and wj appear within a window N terms. 
We use Dirichlet-Prior smoothed KL-
Divergence model as the unigram model in Equa-
tion (9). The Dirichlet-Prior smoothing 
set to 2000. This unigram model, UG, is also the 
baseline in the experiments. The main evaluation 
metric in this study is the non-interpolated average 
precision (AvgPr.) 
We evaluated the smooth-based dependence 
language model in two document collections and 
four query collectio
tions are shown in Table 2. 
Three retrieval models are evaluated in the 
TREC collections: UG, SDLM_Syn and 
SDLM_Prox. Besides the pa 0 1 2
LM_Prox has one more parameter than 
SDLM_Syn. It is the window size N of 
CD(wi,wj,R). In the experiments, we tried the win-
dow size N of 5, 10, 20 and 40 to find the optimal 
setting. We find the optimal N is 10. This size is 
close to sentence length and it is used in the fol-
lowing experiments. 
Coll. Queries Documents Size # 
AP 51-200 Associated Press 
(1  
489 164,597
988,1989) in
Disk2 
TR -8EC7 351-450
Ro
Hard queries in
bust04 35 hard
351-450
Robust04
New 
651-700 
ex.672
Disk 4&5 
(no CR) 
3,120 528,155
Table 2.  TREC collections 
eter ,?2) were trained on three query 
se 700. Each query set 
was divided into two halves, and we applied two-
fo
Param s (?0,?1
ts: 51-200, 351-450 and 651-
ld cross validation to get the final result. We 
trained (?0,?1,?2) by directly maximizing MAP 
(mean average precision). Since the parameter 
range was limited, we used a linear search method 
at step 0.1 to find the optimal setting of (?0,?1,?2). 
 
Topic Index
0 20 40 60 80 100 120 140 160
A
vg
P
r
0.0
.2
.4
.6
.8
1.0
UG
SDLM_Syn
Topic Index
0 20 40 60 80 100
A
vg
P
r
0.0
.2
.4
.6
.8
1.0
UG
SDLM_Syn
Topic Index
0 10 20 30 40
A
vg
P
r
0.00
.05
.10
.15
.20
.25
.30
.35
UG
SDLM_Syn
Topic Index
0 10 20 30 40 50
A
vg
P
r
0.0
.2
.4
.6
.8
1.0
UG
SDLM_Syn
 
Figure 3 UG vs. SDLM_Syn in verbose queries: 
Top Left (51-200), Top Right (351-450), Bottom 
Left (hard topics in 351-450), and Bottom Right 
 Table 3 and Table 4 respectively. The 
settings of (?0,?1,?2) used in the experiments are 
al
(651-700) 
The results on verbose queries and short queries 
are listed in
so listed. A star mark after the change percent 
value indicates a statistical significant difference at 
the 0.05 level(one-sided Wilcoxon test). In verbose 
queries, we can see that SDLM has distinct 
201
 
UG SDLM_Prox SDLM_Syn collections 
AvgPr. AvgPr. %ch over UG (?0,?1,?2) AvgPr. %ch over UG (?0,?1,?2) 
AP 0.2159 0.2360 9.31* (1.8,0.6,0.9) 0.2393 10.84* (1.9,0.7,0.9)
TREC7-8 0.1893 0.2049 8.24* (1.2,0.1,0.2) 0.2061 8.87* (0.4,0.1,0.9)
Robust04_hard 0.0909 0.1049 15.40* (1.2,0.1,0.2) 0.1064 17.05* (0.4,0.1,0.9)
Robust04_new 0.2754 0.3022 9.73* (0.7,0.1,0.3) 0.3023 9.77* (0.7,0.1,0.3)
Table 3. Comparison results on verbose queries 
UG SDLM_Prox SDLM_Syn collections 
AvgPr. AvgPr. %ch over UG (?0,?1,?2) AvgPr. %ch over UG (?0,?1,?2) 
AP 0.2643 0.2644 0 (1.3,0.6,0.1) 0.2647 0.15 (1.1,0.5,0.2)
TREC7-8 0.2069 0.2076 0.34 (1.2,0.3,0.2) 0.2070 0 (1,0.1,0.2) 
Robust04_hard 0.1037 0.1044 0.68 (1.2,0.3,0.2) 0.1045 0.77 (1,0.1,0.2) 
Robust04_new 0.2771 0.2888 4.22* (1.3,0.3,0.4) 0.2869 3.54* (1.3,0.1,0.4)
Table 4. Comparison results on short queries 
Topic Index
0 20 40 60 80 100 120 140 160
A
vg
P
r
0.0
.2
.4
.6
.8
1.0
SDLM_Prox
SDLM_Syn
Topic Index
0 20 40 60 80 100
A
vg
P
r
0.0
.2
.4
.6
.8
1.0
SDLM_Prox
SDLM_Syn
 
Topic Index
0 10 20 30 40
A
vg
P
r
0.00
.05
.10
.15
.20
.25
.30
.35
SDLM_Prox
SDLM_Syn
Topic Index
0 10 20 30 40 50
A
vg
P
r
0.0
.2
.4
.6
.8
1.0
SDLM_Prox
SDLM_Syn
 
Figure 4. SDLM_Prox vs. SDLM_Syn in verbose 
queries: Top Left (51-200), Top Right (351-450), 
Bottom Left (hard topics in 351-450), Bottom 
Right (651-700) 
improvement over UG and SDLM_Syn has robust 
improvement over SDLM_Prox. In short queries, 
SDLM has slight improvement over UG and 
SDLM_Syn is comparative with SDLM_Prox. 
To study the effectiveness of syntactic depend-
encies in detail, Figure 3 and 4 compare 
SDLM_Syn and UG, SDLM_Syn and 
SDLM_Prox topic by topic in verbose queries. 
As shown in Figure 3 and Figure 4, SDLM_Syn 
achieves substantial improvements over UG in the 
majority of queries. While SDLM_Syn is com-
parative with SDLM_Prox in most of the queries, 
SDLM_Syn still get some noticeable improve-
ments over SDLM_Prox. 
From Table 3 and 4, we can see while the pa-
rameters (?0,?1,?2) change a lot in two different 
document collections, there is little change in the 
same document collection. This shows the robust-
ness of our smooth-based dependence language 
model. 
6 Conclusion 
In this paper we have systematically studied the 
effectiveness of syntactic dependencies compared 
with term proximity in dependence retrieval 
model. To compare the effectiveness of syntactic 
dependencies and term proximity, we develop a 
smooth-based dependence language model that 
can incorporate different term dependencies.  
Experiments on four TREC collections indicate 
the effectiveness of syntactic dependencies: In 
verbose queries, the improvement of syntactic 
dependencies over term proximity is noticeable; 
In short queries, the improvement is not notice-
able.  For keywords-based short queries with av-
erage length of 2-3 words, the term dependencies 
in the queries are very few. So the improvement 
of dependence retrieval model over independence 
unigram model is very limited. Meanwhile, the 
difference between syntactic dependencies and 
term proximity is not noticeable. For dependence 
retrieval model, we can get the same conclusion as 
Thorsten Brants: the longer the queries are, the 
bigger the benefit of NLP is. 
 
202
References 
ijsber R
worths, 1979. 
varez, s, Ji Nie
e g fo tion
s  200 686-
s for l e m  a
formation re ,
ges 334?342
b aluati MINI
 the E ion o
ndom 
field model for term dependencies, In Proceedings of 
SIGIR?05, Pages 472-479, 2005 
Fei Song and W. Bruce Croft. A general language 
model for information retrieval. In Proceedings of 
SIGIR?99, pages 279-280, 1999. 
Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu and Gu -
hong Cao, Dependence Language Model for Info -
mation Retrieval, In Proceedings of SIGIR?04, 
Pages:170-177, 2004 
Jianfeng Gao, Haoliang Qi, Xinsong Xia and Jian-Yun 
Nie. Linear Discriminant Model for Information Re-
trieval. In Proceedings of SIGIR?05, Pages:290-297, 
2005 
y Computer Laboratory. 1998 
Shuang Liu, Fang Liu, Clement Yu and Weiyi Meng, 
An Effective Approach to Document Retrieval via 
Utilizing WordNet an ses, In Pro-
in I
 Ter ence he 
u feld e n. Inf ess-
d men 3?3
 atur uage  In-
formation Retrieval . In Proceedings of 20th Interna-
tional Conference o nal Linguistics, 
e
C. J. van R g foen. In rmation etrieval. Butter-
Carmen Al  Philippe Langlai an-Yun , Bahad
Word Pairs in 
Retrieval, In Pr
Languag
oceeding
 Modelin
 of RIAO
r Informa
4, Pages 
 ing an
705, 2004 
Chengxiang 
ing method
Zhai an n Lafferty. A stud Joh
angua
dy of smooth-
lied to ad hoc g
trieval. In
odels
 Proceedi
pp
ngs of SIGIR?01in
pa
 
 , 2001 
Dekang Lin, Dep
PAR, Proceedin
endency-
gs of Wo
ased Ev
rkshop on
on of 
valuat
-
f 
Parsing Systems, Granada, Spain, May,
Donald Metzler and W. Bruce Croft, A Markov ra
 1998. 
i
r
K. Sparck Jones, S. Walker, and S. E. Robertson, A 
probabilistic model of information retrieval: devel-
opment and status. Technical Report TR-446, Cam-
bridge Universit
Lioma C. and Ounis I., A Syntactically-Based Query 
Reformulation Technique for Information Retrieval, 
Information Processing and Management (IPM), El-
sevier Science, 2007 
M. Mitra, C. Buckley, A. Singhal, and C. Cardie. An 
Analysis of Statistical and Syntactic Phrases. In 
Proceedings of RIAO-97, 5th International Confer-
ence ?Recherche d?Information Assistee par Ordi-
nateur?, pages 200-214, Montreal, CA, 1997. 
Munirathnam Srikanth and Rohini Srihari, Exploiting 
Syntactic Structure of Queries in a Language Mod-
eling Approach to IR, In Proceedings of CIKM?03, 
Pages: 476-483, 2003 
 
d Recognizing Phra
-2ceed gs of SIG R?04, Pages: 266 72, 2004 
Robert M. Losee. m depend : Truncating t
r Lazars xpansio ormation Proc
 Manage t, 30(2):29 03, 1994. 
Thorsten Brants. N al Lang Processing in
n Computatio
Antw rp, Belgium, 2004:1-13. 
 
203
