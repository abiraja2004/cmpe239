Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 88?97,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
Towards an ACL Anthology Corpus with Logical Document Structure
An Overview of the ACL 2012 Contributed Task
Ulrich Sch?fer
DFKI Language Technology Lab
Campus D 3 1
D-66123 Saarbr?cken, Germany
ulrich.schaefer@dfki.de
Jonathon Read, Stephan Oepen
Department of Informatics
Universitetet i Oslo
0316 Oslo, Norway
{jread |oe}@ifi.uio.no
Abstract
The ACL 2012 Contributed Task is a com-
munity effort aiming to provide the full ACL
Anthology as a high-quality corpus with rich
markup, following the TEI P5 guidelines?
a new resource dubbed the ACL Anthology
Corpus (AAC). The goal of the task is three-
fold: (a) to provide a shared resource for ex-
perimentation on scientific text; (b) to serve
as a basis for advanced search over the ACL
Anthology, based on textual content and cita-
tions; and, by combining the aforementioned
goals, (c) to present a showcase of the benefits
of natural language processing to a broader au-
dience. The Contributed Task extends the cur-
rent Anthology Reference Corpus (ARC) both
in size, quality, and by aiming to provide tools
that allow the corpus to be automatically ex-
tended with new content?be they scanned or
born-digital.
1 Introduction?Motivation
The collection of the Association for Computational
Linguistics (ACL) Anthology began in 2002, with
3,100 scanned and born-digital1 PDF papers. Since
then, the ACL Anthology has become the open ac-
cess collection2 of scientific papers in the area of
Computational Linguistics and Language Technol-
ogy. It contains conference and workshop proceed-
ings and the journal Computational Linguistics (for-
merly the American Journal of Computational Lin-
guistics). As of Spring 2012, the ACL Anthol-
1The term born-digital means natively digital, i.e. prepared
electronically using typesetting systems like LATEX, OpenOffice,
and the like?as opposed to digitized (or scanned) documents.
2
http://aclweb.org/anthology
ogy comprises approximately 23,000 papers from 46
years.
Bird et al. (2008) started collecting not only the
PDF documents, but also providing the textual con-
tent of the Anthology as a corpus, the ACL Anthol-
ogy Reference Corpus3 (ACL-ARC). This text ver-
sion was generated fully automatically and in differ-
ent formats (see Section 2.2 below), using off-the-
shelf tools and yielding somewhat variable quality.
The main goal was to provide a reference cor-
pus with fixed releases that researchers could use
and refer to for comparison. In addition, the vision
was formulated that manually corrected ground-
truth subsets could be compiled. This is accom-
plished so far for citation links from paper to paper
inside the Anthology for a controlled subset. The
focus thus was laid on bibliographic and bibliomet-
ric research and resulted in the ACL Anthology Net-
work (Radev et al., 2009) as a public, manually cor-
rected citation database.
What is currently missing is an easy-to-process
XML variant that contains high-quality running text
and logical markup from the layout, such as section
headings, captions, footnotes, italics etc. In prin-
ciple this could be derived from LATEX source files,
but unfortunately, these are not available, and fur-
thermore a considerable amount of papers have been
typeset with various other word processing software.
Here is where the ACL 2012 Contributed Task
starts: The idea is to combine OCR and PDFBox-
like born-digital text extraction methods and re-
assign font and logical structure information as part
of a rich XML format. The method would rely on
OCR exclusively only in cases where no born-digital
3
http://acl-arc.comp.nus.edu.sg
88
PDFs are available?in case of the ACL Anthology
mostly papers published before the year 2000. Cur-
rent results and status updates will always be acces-
sible through the following address:




	
http://www.delph-in.net/aac/
We note that manually annotating the ACL An-
thology is not viable. In a feasibility study we took
a set of five eight-page papers. After extracting
the text using PDFBox4 we manually corrected the
output and annotated it with basic document struc-
ture and cross-references; this took 16 person-hours,
which would suggest a rough estimate of some 25
person-years to manually correct and annotate the
current ACL Anthology. Furthermore, the ACL An-
thology grows substantially every year, requiring a
sustained effort.
2 State of Affairs to Date
In the following, we briefly review the current status
of the ACL Anthology and some of its derivatives.
2.1 ACL Anthology
Papers in the current Anthology are in PDF format,
either as scanned bitmaps or digitally typeset with
LATEX or word processing software. Older scanned
papers were often created using type writers, and
sometimes even contained hand-drawn graphics.
2.2 Anthology Reference Corpus (ACL-ARC)
In addition to the PDF documents, the ACL-ARC
also contains (per page and per paper)
? bitmap files (in the PNG file format)
? plain text in ?normal? reading order
? formatted text (in two columns for most of the
papers)
? XML raw layout format containing position in-
formation for each word, grouped in lines, with
font information, but no running text variant.
The latter three have been generated using OCR
software (OmniPage) operating on the bitmap files.
4
http://pdfbox.apache.org
However, OCR methods tend to introduce charac-
ter and layout recognition errors, from both scanned
and born-digital documents.
The born-digital subset of the ACL-ARC (mostly
papers that appeared in 2000 or later) also contains
PDFBox plain text output. However, this is not
available for approximately 4% of the born-digital
PDFs due to unusual font encodings. Note though,
that extracting text from PDFs in normal reading
order is not a trivial task (Berg et al., 2012), and
many errors exist. Furthermore, the plain text is
not dehyphenated, necessitating a language model
or lexicon-based lookup for post-processing.
2.3 ACL Anthology Network
The ACL Anthology Network (Radev et al., 2009)
is based on the ACL-ARC text outputs. It addition-
ally contains manually-corrected citation graphs, au-
thor and affiliation data for most of the Anthology
(papers until 2009).
2.4 Publications with the ACL Anthology as a
Corpus
We did a little survey in the ACL Anthology of pa-
pers reporting on having used the ACL Anthology as
corpus/dataset. The aim here is to get an overview
and distribution of the different NLP research tasks
that have been pursued using the ACL Anthology as
dataset. There are probably other papers outside the
Anthology itself, but these have not been looked at.
The pioneers working with the Anthology as cor-
pus are Ritchie et al. (2006a, 2006b). They did work
related to citations which also forms the largest topic
cluster of papers applying or using Anthology data.
Later papers on citation analysis, summarization,
classification, etc. are Qazvinian et al. (2010), Abu-
Jbara & Radev (2011), Qazvinian & Radev (2010),
Qazvinian & Radev (2008), Mohammad et al.
(2009), Athar (2011), Sch?fer & Kasterka (2010),
and Dong & Sch?fer (2011).
Text summarization research is performed in
Qazvinian & Radev (2011) and Agarwal et al.
(2011a, 2011b).
The HOO (?Help our own?) text correction shared
task (Dale & Kilgarriff, 2010; Zesch, 2011; Ro-
zovskaya et al., 2011; Dahlmeier et al., 2011) aims
at developing automated tools and techniques that
89
assist authors, e.g. non-native speakers of English,
in writing (better) scientific publications.
Classification/Clustering related publications are
Muthukrishnan et al. (2011) and Mao et al. (2010).
Keyword extraction and topic models based on
Anthology data are addressed in Johri et al. (2011),
Johri et al. (2010), Gupta & Manning (2011), Hall
et al. (2008), Tu et al. (2010) and Daudaravic?ius
(2012). Reiplinger et al. (2012) use the ACL An-
thology to acquire and refine extraction patterns for
the identification of glossary sentences.
In this workshop several authors have used the
ACL Anthology to analyze the history of compu-
tational linguistics. Radev & Abu-Jbara (2012) ex-
amine research trends through the citing sentences
in the ACL Anthology Network. Anderson et al.
(2012) use the ACL Anthology to perform a people-
centered analysis of the history of computational
linguistics, tracking authors over topical subfields,
identifying epochs and analyzing the evolution of
subfields. Sim et al. (2012) use a citation analysis to
identify the changing factions within the field. Vo-
gel & Jurafsky (2012) use topic models to explore
the research topics of men and women in the ACL
Anthology Network. Gupta & Rosso (2012) look
for evidence of text reuse in the ACL Anthology.
Most of these and related works would benefit
from section (heading) information, and partly the
approaches already used ad hoc solutions to gather
this information from the existing plain text ver-
sions. Rich text markup (e.g. italics, tables) could
also be used for linguistic, multilingual example ex-
traction in the spirit of the ODIN project (Xia &
Lewis, 2008; Xia et al., 2009).
3 Target Text Encoding
To select encoding elements we adopt the TEI P5
Guidelines (TEI Consortium, 2012). The TEI en-
coding scheme was developed with the intention of
being applicable to all types of natural language, and
facilitating the exchange of textual data among re-
searchers across discipline. The guidelines are im-
plemented in XML; we currently use inline markup,
but stand-off annotations have also been applied
(Ban?ski & Przepi?rkowski, 2009).
We use a subset of the TEI P5 Guidelines as
not all elements were deemed necessary. This pro-
cess was made easier through Roma5, an online
tool that assists in the development of TEI valida-
tors. We note that, while we initially use a simpli-
fied version, the schemas are readily extensible. For
instance, Przepi?rkowski (2009) demonstrates how
constituent and dependency information can be en-
coded following the guidelines, in a manner which
is similar to other prominent standards.
A TEI corpus is typically encoded as a sin-
gle XML document, with several text elements,
which in turn contain front (for abstracts), body
and back elements (for acknowledgements and bib-
liographies). Then, sections are encoded using div
elements (with xml:ids), which contain a heading
(head) and are divided into paragraphs (p). We
aim for accountability when translating between for-
mats; for example, the del element records deletions
(such as dehyphenation at line breaks).
An example of a TEI version of an ACL Anthol-
ogy paper is depicted in Figure 1 on the next page.
4 An Overview of the Contributed Task
The goal of the ACL 2012 Contributed Task is to
provide a high-quality version of the textual content
of the ACL Anthology as a corpus. Its rich text
XML markup will contain information on logical
document structure such as section headings, foot-
notes, table and figure captions, bibliographic ref-
erences, italics/emphasized text portions, non-latin
scripts, etc.
The initial source are the PDF documents of the
Anthology, processed with different text extraction
methods and tools that output XML/HTML. The in-
put to the task itself then consists of two XML for-
mats:
? PaperXML from the ACL Anthology Search-
bench6 (Sch?fer et al., 2011) provided
by DFKI Saarbr?cken, of all approximately
22,500 papers currently in the Anthology (ex-
cept ROCLING which are mostly in Chi-
nese). These were obtained by running a com-
mercial OCR program and applying logical
markup postprocessing and conversion to XML
(Sch?fer & Weitz, 2012).
5
http://www.tei-c.org/Roma/
6
http://aclasb.dfki.de
90
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 aclarc.tei.xsd" xml:lang="en">
<teiHeader>
<fileDesc>
<titleStmt>
<title>Task-oriented Evaluation of Syntactic Parsers and Their Representations</title>
<author>
Yusuke Miyao? Rune S?tre? Kenji Sagae? Takuya Matsuzaki? Jun?ichi Tsujii??*
?Department of Computer Science, University of Tokyo, Japan
?School of Computer Science, University of Manchester, UK
National Center for Text Mining, UK
{yusuke,rune.saetre,sagae,matuzaki,tsujii}@is.s.u-tokyo.ac.jp
</author>
</titleStmt>
<publicationStmt>
<publisher>Association for Computational Linguistics</publisher>
<pubPlace> Columbus, Ohio, USA</pubPlace>
<date>June 2008</date>
</publicationStmt>
<sourceDesc> [. . . ] </sourceDesc>
</fileDesc>
<encodingDesc> [. . . ] </encodingDesc>
</teiHeader>
<text>
<front>
<div type="abs">
<head>Abstract</head>
<p> [. . . ] </p>
</div>
</front>
<body>
<div xml:id="SE1">
<head>Introduction</head>
<p>
Parsing technologies have improved considerably in
the past few years, and high-performance syntactic
parsers are no longer limited to PCFG-based frame<del type="lb">-</del>
works (<ref target="#BI6">Charniak, 2000</ref>;
[. . . ]
</p>
</div>
</body>
<back>
<div type="ack">
<head>Acknowledgements</head>
<p> [. . . ] </p>
</div>
<div type="bib">
<head>References</head>
<listBibl>
<bibl xml:id="BI1">
D. M. Bikel. 2004. Intricacies of Collins? parsing model.
<hi rend="italic">Computational Linguistics</hi>, 30(4):479?511.
</bibl>
[. . . ]
</listBibl>
<pb n="54"/>
</div>
</back>
</text>
</TEI>
Figure 1: An example of a TEI-compliant version of an ACL Anthology document P08-1006. Some elements are
truncated ([. . . ]) for brevity.
91
? TEI P5 XML generated by PDFExtract. For pa-
pers from after 1999, an additional high-quality
extraction step took place, applying state-of
the art word boundary and layout recognition
methods directly to the native, logical PDF
structure (Berg et al., 2012). As no charac-
ter recognition errors occur, this will form the
master format for textual content if available.
Because both versions are not perfect, a large, ini-
tial part of the Contributed Task requires automat-
ically adding missing or correcting markup, using
information from OCR where necessary (e.g. for ta-
bles). Hence, for most papers from after 1999 (cur-
rently approx. 70% of the papers), the Contributed
Task can make use of both representations simulta-
neously.
The role of paperXML in the Contributed Task is
to serve as fall-back source (1) for older, scanned
papers (mostly published before the year 2000), for
which born-digital PDF sources are not available,
or (2) for born-digital PDF papers on which the
PDFExtract method failed, or (3) for document parts
where PDFExtract does not output useful markup
such as currently for tables, cf. Section 4.2 below.
A big advantage of PDFExtract is its ability to ex-
tract the full Unicode character range without char-
acter recognition errors, while the OCR-based ex-
traction methods in our setup are basically limited
to Latin1 characters to avoid higher recognition er-
ror rates.
We proposed the following eight areas as possible
subtasks towards our goal.
4.1 Subtask 1: Footnotes
The first task addresses identification of footnotes,
assigning footnote numbers and text, and generating
markup for them in TEI P5 style. For example:
We first determine lexical heads of nonterminal
nodes by using Bikel's implementation of
Collins' head detection algorithm
<note place="foot" n="9">
<hi rend="monospace">http://www.cis.upenn.edu/
~dbikel/software.html</hi>
</note>
(<ref target="#BI1">Bikel, 2004</ref>;
<ref target="#BI11">Collins, 1997</ref>).
Footnotes are handled to some extent in PDFEx-
tract and paperXML, but the results require refine-
ment.
4.2 Subtask 2: Tables
Task 2 identifies figure/table references in running
text and links them to their captions. The latter
will also have to be distinguished from running text.
Furthermore, tables will have to be identified and
transformed into HTML style table markup. This
is currently not generated by PDFExtract, but the
OCR tool used for paperXML generation quite re-
liably recognizes tables and transforms tables into
HTML. Thus, a preliminary solution would be to in-
sert missing table content in PDFExtract output from
the OCR results. In the long run, implementing table
handling in PDFExtract would be desirable.
<ref target="#TA3">Table 3</ref> shows the
time for parsing the entire AImed corpus,...
<figure xml:id="TA3">
<head>Table 3: Parsing time (sec.)</head>
<!-- TEI table content markup here -->
</figure>
4.3 Subtask 3: Bibliographic Markup
The purpose of this task is to identify citations in
text and link them to the bibliographic references
listed at the end of each paper. In TEI markup, bibli-
ographies are contained in listBibl elements. The
contents of listBibl can range from formatted text
to moderately-structured entries (biblStruct) and
fully-structured entries (biblFull). For example:
We follow the PPI extraction method of
<ref target="#BI39">S?tre et al. (2007)</ref>,
which is based on SVMs ...
<div type="bib">
<head>References</head>
<listBibl>
<bibl xml:id="BI39">
R. S?tre, K. Sagae, and J. Tsujii. 2007.
Syntactic features for protein-protein
interaction extraction. In
<hi rend="italic">LBM 2007 short papers</hi>.
</bibl>
</listBibl>
</div>
A citation extraction and linking tool that is
known to deliver good results on ACL Anthology
papers (and even comes with CRF models trained
on this corpus) is ParsCit (Councill et al., 2008). In
this volume, Nhat & Bysani (2012) provide an im-
plementation for this task using ParsCit and discuss
possible further improvements.
92
4.4 Subtask 4: De-hyphenation
Both paperXML and PDFExtract output contain soft
hyphenation indicators at places where the original
paper contained a line break with hyphenation. In
paperXML, they are represented by the Unicode soft
hyphen character (in contrast to normal dashes that
also occur). PDFExtract marks hyphenation from
the original text using a special element. How-
ever, both tools make errors: In some cases, the hy-
phens are in fact hard hyphens. The idea of this
task is to combine both sources and possibly ad-
ditional information, as in general the OCR pro-
gram used for paperXML more aggressively pro-
poses de-hyphenation than PDFExtract. Hyphen-
ation in names often persists in paperXML and
therefore remains a problem that will have to be ad-
dressed as well. For example:
In this paper, we present a comparative
eval<del type="lb">-</del>uation of syntactic
parsers and their output
represen<del type="lb">-</del>tations based on
different frameworks:
4.5 Subtask 5: Remove Garbage such as
Leftovers from Figures
In both paperXML and PDFExtract output, text
remains from figures, illustrations and diagrams.
This occurs more frequently in paperXML than in
PDFExtract output because text in bitmap figures
undergoes OCR as well. The goal of this subtask
is to recognize and remove such text.
Bitmaps in born-digital PDFs are embedded ob-
jects for PDFExtract and thus can be detected and
encoded within TEI P5 markup and ignored in the
text extraction process:
<figure xml:id="FI3">
<graphic url="P08-1006/FI3.png" />
<head>
Figure 3: Predicate argument structure
</head>
</figure>
4.6 Subtask 6: Generate TEI P5 Markup for
Scanned Papers from paperXML
Due to the nature of the extraction process, PDFEx-
tract output is not available for older, scanned pa-
pers. These are mostly papers from before 2000, but
also e.g. EACL 2003 papers. On the other hand, pa-
perXML versions exist for almost all papers of the
ACL Anthology, generated from OCR output. They
still need to be transformed to TEI P5, e.g. using
XSLT. The paperXML format and transformation to
TEI P5 is discussed in Sch?fer & Weitz (2012) in
this volume.
4.7 Subtask 7: Add Sentence Splitting Markup
Having a standard for sentence splitting with unique
sentence IDs per paper to which everyone can refer
to later could be important. The aim of this task is to
add sentence segmentation to the target markup. It
should be based on an open source tokenizer such as
JTok, a customizable open source tool7 that was also
used for the ACL Anthology Searchbench semantic
index pre-processing, or the Stanford Tokenizer8.
<p><s>PPI extraction is an NLP task to identify
protein pairs that are mentioned as interacting
in biomedical papers.</s> <s>Because the number
of biomedical papers is growing rapidly, it is
impossible for biomedical researchers to read
all papers relevant to their research; thus,
there is an emerging need for reliable IE
technologies, such as PPI identification.
</s></p>
4.8 Subtask 8: Math Formulae
Many papers in the Computational Linguistics area,
especially those dealing with statistical natural lan-
guage processing, contain mathematical formulae.
Neither paperXML nor PDFExtract currently pro-
vide a means to deal with these.
A math formula recognition is a complex task, in-
serting MathML9 formula markup from an external
tool (formula OCR, e.g. from InftyReader10) could
be a viable solution.
For example, the following could become the tar-
get format of MathML embedded in TEI P5, for
?? > 0 3 f (x) < 1:
<mrow>
<mo> there exists </mo>
<mrow>
<mrow>
<mi> &#916; <!--GREEK SMALL DELTA--></mi>
<mo> &gt; </mo>
<mn> 0 </mn>
7
http://heartofgold.opendfki.de/repos/trunk/
jtok; LPGL license
8
http://nlp.stanford.edu/software/tokenizer.
shtml; GPL V2 license
9
http://www.w3.org/TR/MathML/
10
http://sciaccess.net/en/InftyReader/
93
</mrow>
<mo> such that </mo>
<mrow>
<mrow>
<mi> f </mi>
<mo> &#2061; <!--FUNCTION APPL.--></mo>
<mrow>
<mo> ( </mo>
<mi> x </mi>
<mo> ) </mo>
</mrow>
</mrow>
<mo> &lt; </mo>
<mn> 1 </mn>
</mrow>
</mrow>
</mrow>
An alternative way would be to implement math
formula recognition directly in PDFExtract using
methods known from math OCR, similar to the page
layout recognition approach.
5 Discussion?Outlook
Through the ACL 2012 Contributed Task, we have
taken a (small, some might say) step further towards
the goal of a high-quality, rich-text version of the
ACL Anthology as a corpus?making available both
the original text and logical document structure.
Although many of the subtasks sketched above
did not find volunteers in this round, the Contributed
Task, in our view, is an on-going, long-term com-
munity endeavor. Results to date, if nothing else,
confirm the general suitability of (a) using TEI P5
markup as a shared target representation and (b) ex-
ploiting the complementarity of OCR-based tech-
niques (Sch?fer & Weitz, 2012), on the one hand,
and direct interpretation of born-digital PDF files
(Berg et al., 2012), on the other hand. Combin-
ing these approaches has the potential to solve the
venerable challenges that stem from inhomogeneous
sources in the ACL Anthology?e.g. scanned, older
papers and digital newer papers, generated from a
broad variety of typesetting tools.
However, as of mid-2012 there still is no ready-to-
use, high-quality corpus that could serve as a shared
starting point for the range of Anthology-based NLP
activities sketched in Section 1 above. In fact, we
remain slightly ambivalent about our recommenda-
tions for utilizing the current state of affairs and ex-
pected next steps?as we would like to avoid much
work getting underway with a version of the corpus
that we know is unsatisfactory. Further, obviously,
versioning and well-defined release cycles will be a
prerequisite to making the corpus useful for compa-
rable research, as discussed by Bird et al. (2008).
In a nutshell, we see two possible avenues for-
ward. For the ACL 2012 Contributed Task, we col-
lected various views on the corpus data (as well as
some of the source code used in its production) in a
unified SVN repository. Following the open-source,
crowd-sourcing philosophy, one option would be to
make this repository openly available to all inter-
ested parties for future development, possibly aug-
menting it with support infrastructure like, for ex-
ample, a mailing list and shared wiki.
At the same time, our experience from the past
months suggests that it is hard to reach sufficient
momentum and critical mass to make substantial
progress towards our long-term goals, while con-
tributions are limited to loosely organized volun-
teer work. A possibility we believe might overcome
these limitations would be an attempt at formaliz-
ing work in this spirit further, for example through a
funded project (with endorsement and maybe finan-
cial support from organizations like the ACL, ICCL,
AFNLP, ELRA, or LDC).
A potential, but not seriously contemplated ?busi-
ness model? for the ACL Anthology Corpus could be
that only groups providing also improved versions
of the corpus would get access to it. This would
contradict the community spirit and other demands,
viz. that all code should be made publicly available
(as open source) that is used to produce the rich-text
XML for new papers added to the Anthology. To de-
cide on the way forward, we will solicit comments
and expressions of interest during ACL 2012, in-
cluding of course from the R50 workshop audience
and participants in the Contributed Task. Current
results and status updates will always be accessible
through the following address:




	
http://www.delph-in.net/aac/
The ACL publication process for conferences and
workshops already today supports automated collec-
tion of metadata and uniform layout/branding. For
future high-quality collections of papers in the area
of Computational Linguistics, the ACL could think
94
about providing extended macro packages for con-
ferences and journals that generate rich text and doc-
ument structure preserving (TEI P5) XML versions
as a side effect, in addition to PDF generation. Tech-
nically, it should be possible in both LATEX and (for
sure) in word processors such as OpenOffice or MS
Word. It would help reducing errors induced by
the tedious PDF-to-XML extraction this Contributed
Task dealt with.
Finally, we do think that it will well be possible to
apply the Contributed Task ideas and machinery to
scientific publications in other areas, including the
envisaged NLP research and existing NLP applica-
tions for search, terminology extraction, summariza-
tion, citation analysis, and more.
6 Acknowledgments
The authors would like to thank the ACL, the work-
shop organizer Rafael Banchs, the task contributors
for their pioneering work, and the NUS group for
their support. We are indebted to Rebecca Dridan
for helpful feedback on this work.
The work of the first author has been funded
by the German Federal Ministry of Education and
Research, projects TAKE (FKZ 01IW08003) and
Deependance (FKZ 01IW11003). The second and
third authors are supported by the Norwegian Re-
search Council through the VerdIKT programme.
References
Abu-Jbara, A., & Radev, D. (2011). Coherent
citation-based summarization of scientific papers.
In Proceedings of the 49th annual meeting of the
association for computational linguistics: Human
language techologies (pp. 500?509). Portland,
OR.
Agarwal, N., Reddy, R. S., Gvr, K., & Ros?, C. P.
(2011a). Scisumm: A multi-document summa-
rization system for scientific articles. In Proceed-
ings of the ACL-HLT 2011 system demonstrations
(pp. 115?120). Portland, OR.
Agarwal, N., Reddy, R. S., Gvr, K., & Ros?, C. P.
(2011b). Towards multi-document summarization
of scientific articles: Making interesting compar-
isons with SciSumm. In Proceedings of the work-
shop on automatic summarization for different
genres, media, and languages (pp. 8?15). Port-
land, OR.
Anderson, A., McFarland, D., & Jurafsky, D.
(2012). Towards a computational history of the
ACL:1980?2008. In Proceedings of the ACL-
2012 main conference workshop: Rediscovering
50 years of discoveries. Jeju, Republic of Korea.
Athar, A. (2011). Sentiment analysis of citations us-
ing sentence structure-based features. In Proceed-
ings of the ACL 2011 student session (pp. 81?87).
Portland, OR.
Ban?ski, P., & Przepi?rkowski, A. (2009). Stand-off
TEI annotation: the case of the National Corpus
of Polish. In Proceedings of the third linguistic
annotation workshop (pp. 64?67). Suntec, Singa-
pore.
Berg, ?. R., Oepen, S., & Read, J. (2012). To-
wards high-quality text stream extraction from
PDF. Technical background to the ACL 2012
Contributed Task. In Proceedings of the ACL-
2012 main conference workshop on Rediscover-
ing 50 Years of Discoveries. Jeju, Republic of
Korea.
Bird, S., Dale, R., Dorr, B., Gibson, B., Joseph, M.,
Kan, M.-Y., Lee, D., Powley, B., Radev, D., &
Tan, Y. F. (2008). The ACL Anthology Reference
