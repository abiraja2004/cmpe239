NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 64?80,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
The PASCAL Challenge on Grammar Induction
Douwe Gelling and Trevor Cohn
Department of Computer Science
University of Sheffield, UK
{d.gelling,t.cohn}@sheffield.ac.uk
Phil Blunsom
Department of Computer Science
University of Oxford, UK
Phil.Blunsom@cs.ox.ac.uk
Joa?o Grac?a
L2F Spoken Language Systems Laboratory
INESC ID Lisboa, Portugal
joao.graca@l2f.inesc-id.pt
Abstract
This paper presents the results of the PASCAL
Challenge on Grammar Induction, a compe-
tition in which competitors sought to predict
part-of-speech and dependency syntax from
text. Although many previous competitions
have featured dependency grammars or parts-
of-speech, these were invariably framed as
supervised learning and/or domain adaption.
This is the first challenge to evaluate unsuper-
vised induction systems, a sub-field of syntax
which is rapidly becoming very popular. Our
challenge made use of a 10 different treebanks
annotated in a range of different linguistic for-
malisms and covering 9 languages. We pro-
vide an overview of the approaches taken by
the participants, and evaluate their results on
each dataset using a range of different evalua-
tion metrics.
1 Introduction
Inducing grammatical structure from text has long
been fundamental problem in Computational Lin-
guistics and Natural Language Processing. In re-
cent years interest has grown, spurred by advances
in unsupervised statistical modelling and machine
learning. The task has relevance to cognitive scien-
tists and linguists attempting to gauge the learnabil-
ity of natural language by human children, and also
natural language processing researchers who seek
syntactic representations for languages with few lin-
guistic resources.
Grammar learning has been popular in previous
challenges. For example the CoNLL shared tasks in
2006 and 2007 (Buchholz and Marsi, 2006; Nivre
et al., 2007) involved supervised learning of de-
pendency parsers across a wide range of different
languages. Our challenge has many similarities to
these, in that we focus on dependency grammars,
however we seek to evaluate unsupervised algo-
rithms only using syntactically annotated data for
evaluation and not for training. Additionally we also
consider the related task of part-of-speech (POS) in-
duction, and the next logical challenge: the joint
task of POS and dependency induction. Other re-
lated challenges can be found in the formal gram-
mar community (e.g., the Omphalos1 competition)
in which competitors seek to learn synthetic lan-
guages. In contrast we seek to model natural lan-
guage text, which entails many different challenges.
Research into unsupervised grammar and POS in-
duction holds considerable promise, although cur-
rent approaches are still a long way from solving
the general problem. For example, the majority of
recent research into dependency grammar induction
has adopted the evaluation setting of Klein and Man-
ning (2004) who learn grammars on strings of POS
tags, rather than on words themselves. One aim of
this challenge is to popularise the more difficult and
ambitious task of inducing grammars directly from
text, which can be viewed as integrating the POS and
grammar induction tasks. A second aim is to foster
grammar and POS induction research across a wider
variety of languages, and improving the standard of
evaluation.
We have collated data from existing treebanks in
a variety of different languages, domains and lin-
guistic formalisms. This gives a diverse range of
1See http://www.irisa.fr/Omphalos
64
data upon which to test induction algorithms, yield-
ing a deeper insight into their strengths and short-
comings. One key problem in grammar induction
research is how to evaluate the models? predictions
given that often many different analyses are linguis-
tically plausible, e.g., the choice of whether deter-
miners or nouns should head noun phrases, or how to
represent coordination. Simply comparing against a
single gold standard often results in poor reported
performance because the model has discovered a
different analysis to that used when annotating the
treebank. For this reason it has been popular to use
lenient measures for comparing predicted trees to
the treebank gold standard trees, such as undirected
accuracy and the neutral edge distance (Schwartz et
al., 2011). As well as evaluating using these popular
metrics, we also propose a new method of evaluation
which is also lenient in that it rewards different types
of linguistically plausible output, but requires con-
sistency in the output, something the previous meth-
ods cannot do.
The paper is organised as follows. Section 2 de-
scribes the tasks and our data format and section 3
outlines the different treebanks used for the chal-
lenge. The baselines, our own benchmark systems
and the competitors entries are described in section
5. In section 6 we present and analyse the results
for the three different tracks. Finally we conclude in
section 7.
2 Task Definition
The three tracks of the WILS challenge are de-
scribed below. First we describe the data format for
the submissions common to the three tracks (POS
induction, Dependency induction, and jointly induc-
ing both), and then the three tracks are described
along with the respective evaluation metrics.
2.1 Data format
All datasets were presented in a file format similar to
that used in the CoNLL tasks, but with slight mod-
ifications. In particular the last two columns are re-
moved, as no projective head or projective depen-
dency relations were used, and an extra POS column
was inserted at column 6 to accommodate the Uni-
versal POS tagset (Petrov et al., 2011). Each line in a
file then either consists of 9 columns, separated by a
tab character, or is an empty line. Empty lines sepa-
rate sentences, and all other lines give the annotation
for a single token in the sentence as follows:
1. ID: Token counter, gives the index of current
word in the sentence. Indexing starts at 1.
2. FORM: Surface form of the token in the sen-
tence.
3. LEMMA: Stemmed form of the word form if
available.
4. CPOSTAG: Coarse-grained POS tag.
5. POSTAG: Fine-grained POS tag, or CPOSTAG
again if not available.
6. UPOSTAG: Universal POS tag, based on the
POSTAG and CPOSTAG.
7. FEATS: List of syntactic / morphological fea-
tures, separated by a vertical pipe (|).
8. HEAD: Syntactic head of the token, with 0 in-
dicating the root node.
9. DEPREL: The general type of the dependency
relation, e.g., subject.
In this setup, the LEMMA, FEATS and DEPREL
columns are optional, in which case an underscore
( ) will be used as a placeholder. Each treebank
was split into training, development and testing par-
titions. The HEAD and DEPREL entries were only
supplied for the development and the final testing
sets,2 but not for the training partition. The com-
petitors were encouraged to develop their unsuper-
vised entries on the union of the three partitions, and
make sparse use of the development set, i.e., for san-
ity checking more than model fitting in order to min-
imise the extent of supervision.
2.2 POS induction
In the POS induction track, participants developed
systems to induce the Part-of-Speech (POS) classes
for each word in the testing corpus. In order to train
the systems, the same training and development sets
were used as for the other tracks. These corpora in-
cluded manually supplied POS tags for each token,
2For the initial test set these fields were omitted.
65
which were not to be used for training, only evalua-
tion. Participants submitted predicted tags for each
token, which were scored against the gold-standard.
For evaluation, we used 4 different metrics. The
first is the many-to-one metric (M-1) (also known
as cluster purity), which is widely used for cluster
evaluation as well as evaluation of POS induction.
This metric assigns each word cluster to its most
common tag, and then measures the proportion of
correctly tagged words. The second metric is the
one-to-one mapping (1-1), a constrained version of
Many-to-one mapping in which each predicted tag
is associated with only one gold-standard tag and
vice versa (Haghighi and Klein, 2006). Word clus-
ters are assigned greedily to tags, and in the event
of there being more word classes than tags, some
word classes will be left unassigned. Another met-
ric that was used is Variation of information (VI)
(Meila, 2003), which is based the conditional en-
tropy of between the two different clusterings (John-
son, 2007). Lastly, we use the V-measure (VM) met-
ric (Rosenberg and Hirschberg, 2007), which is an-
other entropy-based measure, but defined in terms of
a F score to balance precision and recall terms (we
use equal weighting of the two factors). Please see
Christodoulopoulos et al. (2010) for further details
about these metrics.3 For these metrics, a higher
score is better, with the exception of VI.
For all these metrics, the induced tags are eval-
uated against the universal pos tags, as this means
there are a consistent number of tags across the lan-
guages. Using these metrics, the results will vary as
a result of predicting a different number of tags (in
particular, more tags will mean a higher score for M-
1, and the converse is true for 1-1). However, using
the universal POS tags, we think will make results
less sensitive to large differences in POS inventory
between languages (such as for the Dutch dataset).
2.3 Dependency induction
For the Dependency induction track, the training
data consisted of the original treebank data, but
without dependency annotations. A development set
was also provided, which included the dependency
annotations, but this was meant mainly as a way to
3Thanks to Christos Christodoulopoulos for sharing his im-
plementation of the POS induction metrics, which we have used
in our evaluation.
verify systems, as we mean to minimise the amount
of supervision in the task. The participants were
later supplied with test sets for which the systems
could generate predictions. Only after the predic-
tions were submitted were the fully annotated test
sets released.
The dependency inductions were evaluated on
3 metrics: directed accuracy, undirected accuracy
and Neutral Edge Detection (NED) (Schwartz et
al., 2011). Directed accuracy is the ratio of cor-
rectly predicted dependencies (including direction)
over total amount of predicted dependencies. Undi-
rected accuracy is much the same, but also considers
a predicted dependency correct if the direction of the
dependency is reversed (e.g. if the predicted depen-
dency is not A ? B, but B ? A). Lastly, the NED
metric is a variant of undirected accuracy that also
rewards cases where an edge-flip occurs, meaning
that the predicted parent of a token is actually the
grandparent of that same token in the gold-standard
data. Note that before evaluating with these metrics
punctuation was removed from all sentences, and
any child words under a punctuation node were re-
attached to their nearest ancestor that wasn?t punc-
tuation.
The final ?joint? task consisted of inducing depen-
dency structure from only the tokens in the corpus,
without recourse to the gold POS tags. Where POS
is predicted (e.g., in a pipeline), we included these
in our general POS evaluation. The induced depen-
dency trees were evaluated with the same metrics as
in the dependency induction track, but are consid-
ered separately. We expect these systems to have
lower scores overall due to the lack of gold-standard
POS tags.
3 Treebanks
We selected a number of different treebanks for use
in the challenge, aiming to represent a wide range
of different languages, dialects and genres of text.
In total we used ten different treebanked corpora
in nine different languages. For the practical rea-
sons of simplifying the administration of the chal-
lenge and allowing the data to be reused in future re-
search, we chose corpora with licences allowing ei-
ther free redistribution, or those held by the Linguis-
66
tic Data Consortium (LDC).4 Many of these datasets
have been used before in dependency grammar or
part-of-speech research, particularly the shared tasks
at CoNLL 2006 and 2007. For the purpose of
the competition, we have updated these datasets to
include any annotation updates or additional data,
where available. It is important for unsupervised ap-
proaches to have sufficient amounts of data, espe-
cially given the common sentence length limitations
imposed by most dependency grammar models. As
described in section 2, we have included an extra
field for the universal part-of-speech (UPOS) using
Petrov et al. (2011)?s automatic conversion tool.5
Below we describe the different treebanks used,
and the conversion process into our data format for
the purpose of the competition. Please see Table 1
for statistics on each of the treebanks.
Dependency treebanks We used the following
dependency treebanks: Arabic The Prague Ara-
bic Dependency Treebank V1 (Hajic? et al., 2004).6
Basque The Basque 3lb dependency treebank
(Aduriz et al., 2003). Czech The Prague Depen-
dency Treebank 2.0 (Bo?hmova? et al., 2001).7 Dan-
ish The Copenhagen Dependency Treebank ver-
sion 2 (Buch-Kromann et al., 2007). English The
CHILDES US/Brown subcorpus (Sagae et al.,
2007). Slovene The jos500k Treebank (Erjavec et
al., 2010). 8 Swedish The Talbanken treebank
(Nivre et al., 2006). The conversion of each of these
treebanks was quite straightforward as they were al-
ready annotated for dependencies. Moreover, many
of these corpora had been used previously in the
CoNLL 2006 and 2007 shared tasks, and therefore
we were able to reuse this data and/or their conver-
sion scripts. In the case of Arabic and Swedish we
used the exact same data, simply converting from
CoNLL dependency format into our own format (re-
moving redundant columns and adding a UPOS col-
umn). While many of the other corpora had also
4In the following corpus descriptions, when not otherwise
specified the corpus is freely available for research purposes.
5http://code.google.com/p/
universal-pos-tags
6LDC catalogue number LDC2004T23.
7LDC catalogue number LDC2006T01.
8For the shared task, the annotation was converted to english
using the tables found at the JOS website: http://nl.ijs.
si/jos/msd/html-en/index.html
been used previously, our data is different, making
use of subsequent corrections to these treebanks and
additional annotated data now available.
First language acquisition provides an important
motivation for grammar induction research, conse-
quently we have included data from the CHILDES
database of child-directed speech. We use the
Brown sub-corpus, a longitudinal study of parent-
child interactions for three children aged between 18
months and 5 years old. The corpus has been man-
ually annotated with syntactic dependencies (Sagae
et al., 2007) and morphology. From this we take all
child-directed utterances, extracting word, morphol-
ogy, part-of-speech and dependency markup, and
developed our own conversion into UPOS. Our test-
ing and development sets were drawn from the first
15 Eve files which were manually annotated for de-
pendency structure. The rest of the corpus, which
had not been manually annotated for syntax, was
merged to form the training set.
Phrase-structure treebanks As well as depen-
dency treebanks, we used three different phrase-
structure treebanks: The Dutch Alpino treebank
(Bouma et al., 2000), the English Penn Treebank
V3 (Marcus et al., 1993),9 and the Portuguese Flo-
resta Sinta?(c)tica treebank (Afonso et al., 2002). As
these treebanks do not explicitly mark dependen-
cies, we automatically extracted these using head
finding heuristics. Thankfully the difficult work
of creating such scripts has already been done as
part of the CoNLL shared tasks. We have reused
their scripts to create dependency representations of
these treebanks, before converting into our file for-
mat and augmenting with UPOS annotation. In the
case of Dutch, we have reused the same CoNLL
2006 data; note that this dataset includes predicted
part-of-speech rather than gold standard annotation
(Buchholz and Marsi, 2006). For the Portuguese,
we used the same Bosque 7.3 sub-corpus10 from
CoNLL 2006, additionally including in our training
set the recently-annotated Selva 1.0 subcorpus.
The Penn Treebank is the most common data set
in parsing and grammar induction. We have patched
9LDC catalogue number LDC99T42.
10An updated version of this corpus is available, however
the file format had changed significantly and we were unable
to adapt the conversion scripts in time for the competition.
67
ar cs da en-childes en-ptb eu nl pt sl sv
annotation d d d d p d p p d d
Training data
Tokens 106.6k 1.2M 68.5k 312.8k 1.1M 124.7k 192.2k 196.4k 193k 184.6k
Sentences 2.8k 68.5k 3.6k 57.4k 45.4k 9.1k 13k 8.7k 9.4k 10.7k
Tokens/sent 38.4 17.1 18.8 5.5 23.9 13.7 14.8 22.6 20.5 17.3
CPOSTAG 15 12 25 31 31 16 13 16 13 41
POSTAG 21 61 141 76 45 50 300 22 31 41
FEATS 22 75 338 29 0 269 310 146 46 0
Development data
Tokens 5.1k 159k 17k 25.3k 32.9k 12.6k 2.9k 10.3k 20.2k 6.9k
Sentences 139 9.3k 1k 5k 1.3k 1k 386 400 1k 389
Tokens/sent 36.8 17.1 17 5.1 24.4 12.5 7.4 25.8 20.2 17.6
% New words 27.5 26 49.8 9.8 11.4 46.1 18.8 27.5 38.7 13.8
Test data
Tokens 5.1k 173.6k 14.7k 28.4k 56.7k 14.3k 5.6k 5.9k 22.6k 5.7k
Sentences 131 10.1k 1k 5.2k 2.4k 1.1k 386 288 1k 389
Tokens/sent 39.1 17.1 14.7 5.4 23.5 12.7 14.5 20.4 22.6 14.5
% New words 24.3 25.3 43.7 9 12.1 51.5 40.5 25.2 37.1 34.6
Table 1: Properties of the treebanks. We report the linguistic annotation method (dependency vs. phrase-structure),
the size of each treebank, the number of types for the different granularities of part-of-speech tags and morphological
features (note that UPOS has a fixed set of 12 tags), and the proportion of word types that were not present in training.
the treebank to include NP-internal structure using
Vadas and Curran?s annotations (Vadas and Cur-
ran, 2007), which was then converted to dependency
structures using the penn-converter11 script
(Johansson and Nugues, 2007). This tool has a num-
ber of options controlling the linguistic decisions
in converting from phrase-structure to dependency
trees, e.g., the treatment of coordination. We ex-
tracted five versions of the treebank, each encoding
each different sets of linguistic assumptions (Tsar-
faty et al., 2011).12 These are denoted default, old-
LTH, CoNLL-2007, functional and lexical; for the
main results we used the standard options, we also
report separately evaluations using each of the five
variants. The treebank was partitioned into training
(sections 0-22), development (sec. 24) and testing
sets (sec. 23).
4 Baselines and Benchmarks
A number of standard baselines and previously pub-
lished benchmark systems were implemented for
each task in order to place the submitted systems in
context.
11http://nlp.cs.lth.se/software/treebank_
converter
12Note that Tsarfaty et al. (2011) also propose an evalua-
tion metric for comparing dependency trees, which we have not
used. Note however that it could, in principle, be used for simi-
lar evaluations.
The standard baseline for grammar induction
models is to assume either left branching or right
branching analyses (LB, RB). These capture the ten-
dency for languages to favour one attachment direc-
tion over another. The most frequently cited and
extended model for dependency induction is DMV
(Klein and Manning, 2004). We provide results for
this model trained on each of the coarse (DMVc), fine
(DMVp), and universal (DMVu) POS tag sets, all ini-
tialised with the original harmonic initialiser. As a
further baseline we also evaluated the dependency
trees resulting from directly using the harmonic ini-
tialiser without any training (H).
As a strong benchmark we include the results of
the non-parametric Bayesian model previously pub-
lished in Blunsom and Cohn (2010) (BC). The stated
results are for the unlexicalised model described in
that paper where the final analysis is formed by
choosing the maximum marginal probability depen-
dency links estimated from forty independent Gibbs
sampler runs.
For part-of-speech tagging we include results
from an implementation of the Brown word clus-
tering algorithm (Brown et al., 1992) (Bc,p,u), and
the mkcls tool written by Franz Och (Och, 1999)
(MKc,p,u). Both of these benchmarks were trained
with the number of classes matching the number
in the gold standard of each of the tagsets in turn:
coarse (c), fine (p), and universal (u). A notable
68
property of both of these word class models is that
they enforce a one-tag-per-type restriction that en-
sures there is a one-to-one mapping between word
types and classes.
For POS tagging we also provide benchmark re-
sults from two previously published models. The
first of these is the Pitman-Yor HMM model de-
scribed in (Blunsom and Cohn, 2011), which in-
corporates ta one-tag-per-type restriction (BC). This
model was trained with the same number of tags as
in the gold standard fine tag set for each corpus. The
second benchmark is the HMM with Sparsity Con-
straints trained using Posterior Regularization (PR)
described in (Grac?a et al., 2011). In this model
the HMM emission probabilitiy distribution are esti-
mated using small Maximum Entropy models (fea-
tures set described in the original paper). The mod-
els were trained for 200 iterations of PR using both
the same number of hidden states as the coarse Gc
and universal Gu gold standard. All parameters were
set to the values described in the original paper.
5 Submissions
The shared task received submissions covering a di-
verse range of approaches to the dependency and
part-of-speech induction challenges. Encouragingly
all of these submissions made significant departures
from the benchmark HMM and DMV approaches
which have dominated the published literature on
these tasks in recent years. The submissions were
characterised by varied choices of model structure,
parameterisation, regularisation, and the degree to
which light supervision was provided through con-
straints or the use of labelled tuning data. In the fol-
lowing sections we summarise the approaches taken
by the systems submitted for each task.
5.1 Part-of-Speech Induction
The part-of-speech induction challenge received two
submission, (Chrupa?a, 2012; Christodoulopoulos et
al., 2012). Both of these submissions based their in-
duction systems on LDA inspired models for cluster-
ing word types by the contexts in which they appear.
Notably, the strongest of the provided benchmarks
and the two submissions modelled part-of-speech
tags at the type level, thus restricting all tokens of
a given word type to share the same tag. Though
clearly out of step with the gold standard tagging,
this one-tag-per-type restriction has previously been
shown to be a crude but effective way of regularising
models towards a good solution. Below we sum-
marise the approach of each submission, identified
by the surname of the first author on the submitted
system description.
Chrupa?a (2012) employed a two stage approach
to inducing part-of-speech tags. The first stage used
an LDA style probabilistic model to induce a dis-
tribution over possible tags for a given word type.
These distributions were then hierarchically clus-
tered and the final tags selected using the prefix of
the path from the root node to the word type in the
cluster tree. The length of the prefixes, and thus the
number of tags, was tuned on the labelled develop-
ment data.
The system of Christodoulopoulos et al. (2012)
was based upon an LDA type model which included
both contexts and other conditionally independent
features (Christodoulopoulos et al., 2011). This base
system was then iterated with a DMV system and
with the resultant dependencies being repeatedly fed
back into the POS model as features. This submis-
sion is notable for being one of the first to attempt
joint POS and dependency induction rather than tak-
ing a pipeline approach.
5.2 Dependency Induction
The dependency parsing task saw a variety of ap-
proaches with only a couple based on the previously
dominant DMV system. Two forms of light super-
vision were popular, the first being the inclusion of
pre-specified constraints or rules for allowable de-
pendency links, and the second being the tuning of
model parameters or selecting between competing
models on the labelled development data. Obviously
the merits of such supervision would depend on the
desired application for the induced parser. The di-
rect comparison of models which include a form of
universal prior syntactic information with those that
don?t does permit interesting development linguistic
questions to be explored in future.
Bisk and Hockenmaier (2012) chose to induce a
restricted form of Combinatory Categorial Grammar
(CCG), the parses of which were then mapped to
dependency structures. Restrictions on head-child
dependencies were encoded in the allowable cate-
69
gories for each POS tag and the heads of sentences.
Key features of their approach were a maximum
likelihood objective function and an iterative proce-
dure for generating composite categories from sim-
ple ones. Such composite categories allow the pa-
rameterisation of larger units than just head-child
dependencies, improving over the more limited con-
ditioning of DMV.
Marac?ek and Z?abokrtsky? (2012) introduced a
number of novel features in their dependency induc-
tion submission. Wikipedia articles were used to
quantify the reducibility of word types, the degree
to which the word could be removed from a sen-
tence and grammaticality maintained. This metric
was then used, along with a model of child fertil-
ity and dependency distance, within a probabilistic
model. Inference was performed by using a local
Gibbs sampler to approximate the marginal distribu-
tion over head-child links.
S?gaard (2012) presented two model-free heuris-
tic algorithms. The first was based on heuristically
adding dependency edges based on rules such as ad-
jacency, function words, and morphology. The re-
sulting structure is then run through a PageRank al-
gorithm and another heuristic is used to select a tree
from the resulting ranked dependency edges. The
second approach takes the universal rules of Naseem
et al. (2010) but rather than estimating a probabilis-
tic model with these rules, a rule based heuristic is
used to select a parse rather. This second model-free
approach in particular provides a strong baseline for
probabilistic models built upon hand-specified de-
pendency rules.
Tu (2012) described a system based on an ex-
tended DMV model. Their work focussed on the
exploration of multiple forms of regularisation, in-
cluding Dirichlet priors and posterior regularisation,
to favour both sparse conditional distributions and
low ambiguity in the induced parse charts. While
many previous works have included sparse priors
on the conditional head-child distributions the ad-
ditional regularisation of the ambiguity over parse
trees is a novel and interesting addition. The la-
belled development sets were employed to both se-
lect between models employing different regularisa-
tion, and to tune model parameters.
5.3 POS and Dependency Induction
There was only a single submission for the task of
inducing dependencies without gold standard part-
of-speech tags supplied. Christodoulopoulos et al.
(2012) submitted the same joint tagging and DMV
system used for the POS induction task to the depen-
dency induction task. Results on the development
data indicated that this iterated joint training had a
significant benefit for the induced tags and a smaller
benefit for the dependency structures induced.
6 Results
The main results for the three tasks are shown in Ta-
bles 2, 3, and 4, for the POS induction, dependency
induction and joint tasks, respectively.13 We now
present a detailed analysis of each of the three tasks.
6.1 POS induction
The main evaluation results for the POS induc-
tion task are shown in Table 2, which compares
the induced clusters against the gold universal tags
(UPOS).14 Given the diversity of scenarions used by
each system (e.g. number of hidden states, tuning
on development data) a direct comparison between
the systems can only be illustrative. A first obser-
vation is that depending on the particular evaluation
metric employed the ranking of the systems changes
substantially, for instance the Gu system is the best
using the 1-1 and VI metric but is the worst of the en-
tries (excepting the baselines) when using the other
two metrics. Focusing on the VM metric, which
was shown empirically not to have low bias with re-
spect to the word classes (Christodoulopoulos et al.,
2010), the best entry is the BC system which has the
best performance in 9 out of 10 entries followed by
the CGS and the C system. Note that this ranking
holds also for the comparison against fine POS tags,
shown in Table 7.
An interesting aspect is that almost all systems
beat the strong Brown (B) and mkcls (MK) base-
line across the different metrics when we restrict
our attention to the cases where the same number
13Additional tables of results are in the appendix, and fur-
ther results are online at http://wiki.cs.ox.ac.uk/
InducingLinguisticStructure.
14See also Table 7 for the comparison against the fine POS
tags; we base our analysis on UPOS instead as this tag set has a
fixed size irrespective of the treebank.
70
M-1
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 83.80 N/A 83.33 83.33 65.05 66.61 66.20 69.89 66.22 71.08 72.72 68.03
basque 80.85 79.54 86.67 86.67 77.37 73.88 74.73 77.49 73.32 74.80 78.63 71.40
czech 83.10 66.78 72.27 77.97 N/A N/A 60.85 75.57 60.42 65.43 79.35 57.16
danish 81.44 77.76 84.13 84.92 68.16 53.78 72.12 79.77 47.09 72.26 82.59 53.07
dutch 80.75 70.13 74.04 76.11 63.37 57.64 57.99 84.17 57.31 68.18 84.78 63.04
en-childes 90.36 85.42 91.50 91.50 N/A N/A 82.65 89.70 70.12 86.27 91.44 75.63
en-ptb 86.73 81.93 78.11 84.35 77.14 71.10 77.29 80.88 63.74 79.99 83.88 63.34
portuguese 81.69 77.38 80.38 81.90 75.54 74.35 70.07 74.25 67.60 70.79 72.90 68.08
slovene 70.81 65.31 75.53 75.92 67.94 59.96 61.58 68.93 58.32 58.43 65.69 50.36
swedish 78.61 80.45 79.60 79.60 69.91 58.79 71.69 71.69 57.55 76.45 76.45 57.30
averages 81.82 76.08 80.56 82.23 70.56 64.51 69.52 77.23 62.17 72.37 78.84 62.74
1-1
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 53.67 N/A 39.44 39.44 39.83 55.52 40.55 33.57 43.31 51.54 40.24 51.58
basque 36.10 36.03 47.15 47.15 47.09 54.70 32.61 20.53 40.62 34.80 27.28 37.65
czech 31.82 49.30 30.49 27.20 N/A N/A 46.19 26.66 45.10 43.70 24.48 39.25
danish 42.54 42.77 31.67 31.04 39.95 45.58 36.04 17.74 39.19 43.89 22.18 44.23
dutch 42.79 56.15 43.10 39.62 56.45 45.37 48.18 21.36 43.12 55.99 21.32 54.09
en-childes 38.79 42.57 43.76 43.76 N/A N/A 40.78 35.54 57.71 43.45 32.00 59.18
en-ptb 41.55 39.57 43.86 31.56 42.07 51.70 39.79 33.90 46.50 40.55 36.22 51.17
portuguese 59.66 47.45 35.90 35.50 46.50 56.08 51.15 42.68 51.58 44.28 35.38 46.31
slovene 39.02 53.04 33.18 32.50 50.90 48.50 46.83 40.16 42.28 40.34 39.32 40.58
swedish 42.38 32.44 26.45 26.45 34.99 54.92 27.56 27.56 51.34 35.82 35.82 43.60
averages 42.83 44.37 37.50 35.42 44.72 51.55 40.97 29.97 46.07 43.44 31.42 46.76
VM
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 61.75 N/A 51.27 51.27 44.81 47.07 39.93 42.43 39.92 47.47 43.91 44.49
basque 42.17 41.52 43.04 43.04 40.86 40.05 34.85 33.33 36.08 36.32 34.35 33.42
czech 52.26 45.31 40.22 39.20 N/A N/A 38.56 42.90 37.46 41.70 46.03 37.34
danish 56.57 54.63 52.46 52.32 47.26 41.96 47.89 44.37 35.13 50.52 48.17 39.96
dutch 56.96 53.35 54.87 52.90 48.57 45.80 43.34 49.33 43.67 51.37 50.11 47.20
en-childes 64.53 62.32 62.76 62.76 N/A N/A 58.87 60.31 57.06 62.76 60.92 60.51
en-ptb 60.73 57.99 53.14 52.09 55.10 52.54 54.76 55.08 48.04 56.81 57.29 48.46
portuguese 64.17 58.41 52.54 52.32 55.96 58.14 52.09 53.18 50.32 52.48 50.87 50.18
slovene 51.15 51.29 46.60 46.50 50.98 45.98 44.49 45.80 38.61 36.79 43.43 36.43
swedish 57.05 54.21 47.08 47.08 48.89 45.73 45.87 45.87 40.84 49.77 49.77 42.83
averages 56.73 53.23 50.40 49.95 49.05 47.16 46.06 47.26 42.71 48.60 48.48 44.08
VI
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 2.48 N/A 3.70 3.70 3.39 2.98 3.78 3.94 3.53 3.31 3.82 3.30
basque 3.82 3.44 3.98 3.98 3.25 2.82 3.92 4.98 3.45 3.79 4.76 3.58
czech 3.83 3.41 4.92 5.77 N/A N/A 3.70 4.76 3.69 3.63 4.53 3.83
danish 3.36 3.34 4.31 4.38 3.78 3.46 3.86 5.43 3.79 3.64 4.90 3.59
dutch 3.56 3.13 3.28 3.71 3.30 3.44 3.66 5.22 3.60 3.26 5.15 3.39
en-childes 2.81 2.86 3.06 3.06 N/A N/A 3.13 3.34 2.59 2.84 3.33 2.50
en-ptb 3.18 3.28 3.67 4.36 3.34 3.03 3.46 3.62 3.36 3.36 3.52 3.28
portuguese 2.47 2.83 3.96 4.09 2.96 2.62 3.19 3.36 3.10 3.21 3.52 3.15
slovene 3.62 3.14 4.80 4.86 3.16 3.30 3.61 4.09 3.73 4.15 4.33 3.99
swedish 3.31 3.68 4.98 4.98 3.90 3.32 4.46 4.46 3.70 4.07 4.07 3.62
averages 3.24 3.23 4.07 4.29 3.39 3.12 3.68 4.32 3.45 3.53 4.19 3.42
Table 2: Results for the POS induction task, showing one-to-one, many-to-one, VM and VI scores, measured against
the gold UPOS tags. Each system is shown in a column, where the title is an acronym of the authors? last names, or
else the name of a benchmark system (B is the Brown clusterer and MK is mkcls). The superscripts c, p and u denote
different applications of the same method with a number of word classes set to equal the true number of coarse tags,
full tags or universal tags, respectively, for each treebank.
71
of hidden states are used (the exception being the G
system which occasionally under-performed against
MK). Interestingly the assumption of one-tag-per-
word, made by all but the G system, works very
well in practice leading to consistently strong re-
sults. This suggests that dealing with word ambigu-
ity is still an unresolved issue in unsupervised POS
induction.
Comparing the performance of the systems for
different languages, as expected the languages for
which we have a larger corpora (English CHILDES
and PTB and Czech) tend to result in systems with
better accuracies. An interesting future question is
how do the propose methods scale when training on
really large corpora (e.g., wikipedia) both in terms
of performance (accuracy) but also in the resources
they required.
Finally, the wild divergences in the system rank-
ings when considering the different evaluation met-
rics calls for some sort of external evaluation using
the induced clusters as features to other end sys-
tems, for instance semi-supervised tagging. The
main question is if there will be a definitive ranking
between systems for a diverse set of tasks, or if on
the contrary the effectiveness of the output of each
system will vary according to the task at hand.
6.2 Dependency induction
The main evaluation results for the dependency task
are shown in Table 3. From this we make several
observations.15 Firstly, for almost all the corpora
the participants systems have outperformed the sim-
ple baselines, and by a significant margin. There
are three exceptions to this: for Arabic, Basque and
Danish the left or right-branching baselines outper-
forms most or all of the competitors. This may in-
dicate that these languages are inherently difficult,
or may simply be a consequence of these three lan-
guages having the least data of all of our corpora.
Basque and Dutch proved to be the hardest of the
treebanks, with the lowest overall scores, and the
CHILDES (English) and Portuguese were the eas-
iest. The reasons for this are not immediately clear,
15Table 3 evaluates against the full test sets, however it is
traditional to present results for short sentences mirroring the
common training setup. See Tables 8 and 9 for results over
sentences with 10 words or fewer, excluding punctuation. Note
that our analysis is based on the results for the full test set.
although we speculate that Basque is difficult due to
its dissimilarity from other European languages, and
therefore may not match the assumptions underly-
ing models developed primarily on English. Dutch
is difficult as its annotation was non-projective, and
it has a very large set of POS tags, while CHILDES
is made easier due to its extremely short and simple
sentences.
In terms of declaring a ?winner?, it is clear that
Tu?s system ranks best under directed accuracy and
NED, and a very narrow second (to the organisers?
submission, BC) for undirected accuracy. Moreover
Tu?s system was a consistent performed across all
corpora, with no single result well below the results
of the other participants. Note that the three different
metrics often predict the same winner across the dif-
ferent treebanks, however there are some large dis-
crepancies, such as Portuguese and Dutch where the
directed and undirected accuracy metrics concur, but
NED produces a very different ranking. It is unclear
which metric should be trusted more than another;
this could only be assessed by correlating these met-
rics with some form of secondary evaluation, such
as in a task based setting or obtaining human gram-
maticality judgements.16
The benchmark systems include DMV (Klein and
Manning, 2004), which has historical importance
in terms of being the first research systems to out-
perform simple baselines for dependency induction,
and also the model upon which most recent depen-
dency induction research is based, including many
of the competitors in the competition. We ob-
serve that in most cases the competitors have out-
performed the DMV models, in many cases by a
large margin. In all cases DMV improved over
its initialisation condition (the harmonic initialiser),
although often this improvement was only slight,
underscoring the importance of good initialisation.
The effect of inducing DMV grammars from var-
ious different granularity of POS tags made little
difference in most cases, although for Dutch17 and
the English PTB there change was more dramatic.
16It was our intention to include a task-based evaluation for
machine translation, but this proved impractical for the compe-
tition due to the volumes of data that we would require each
participant to process.
17Note that for Dutch the full POS tags were not gold stan-
dard, but were system predictions.
72
Directed
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
arabic 61.1 47.2 15.7 64.8 64.8 47.2 54.6 66.7 46.3 45.4 50.0 42.6 9.3 64.8
basque 56.3 50.3 28.0 30.3 27.2 33.3 22.3 58.6 46.3 43.2 31.3 21.8 34.3 24.4
czech 50.0 48.5 61.3 57.5 57.7 45.5 51.2 59.0 30.1 31.2 31.8 24.7 28.9 34.3
danish 46.2 49.3 60.2 51.3 61.4 56.9 60.5 60.8 47.2 50.2 35.3 36.4 18.7 49.2
dutch 50.5 50.8 37.0 49.5 38.4 38.9 50.0 51.7 48.7 39.7 49.1 35.1 34.0 39.5
en-childes 48.1 62.2 56.8 47.2 51.8 50.5 53.5 56.0 51.7 51.9 39.0 31.7 36.0 23.3
en-ptb 72.1 73.7 58.9 67.4 52.2 44.8 61.0 74.7 31.7 44.7 30.6 35.2 40.4 19.9
portuguese 54.3 76.3 63.6 59.9 44.3 47.7 71.1 55.7 27.1 37.2 26.9 31.1 28.1 37.7
slovene 65.8 53.9 42.1 51.4 39.2 39.7 50.3 67.7 35.7 37.2 35.6 25.7 35.9 14.7
swedish 65.8 66.7 61.4 63.7 70.8 48.2 72.0 76.5 44.2 44.2 45.8 39.1 33.2 31.3
averages 57.0 57.9 48.5 54.3 50.8 45.3 54.7 62.7 40.9 42.5 37.5 32.3 29.9 33.9
Undirected
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
arabic 57.3 29.7 57.6 62.0 58.7 48.0 58.4 59.3 41.8 42.0 43.7 41.2 61.7 63.9
basque 58.0 47.2 43.3 45.0 43.2 47.5 24.3 53.3 48.1 47.7 40.3 37.6 53.9 53.1
czech 59.0 45.0 57.8 54.3 55.5 49.3 55.8 61.4 46.2 46.7 45.3 38.5 51.5 52.3
danish 60.8 50.7 60.7 56.1 60.3 56.6 60.5 61.6 55.1 54.1 51.6 46.0 58.7 59.9
dutch 61.0 45.0 47.5 51.5 48.9 46.8 51.4 54.6 52.2 45.0 52.2 37.2 50.1 50.8
en-childes 63.5 68.4 67.2 59.9 61.4 62.0 62.4 66.9 63.8 64.0 57.5 49.0 50.0 49.9
en-ptb 66.2 58.1 49.7 57.6 48.2 49.5 58.8 62.1 43.1 53.1 43.0 36.2 51.7 51.5
portuguese 56.6 72.4 61.4 61.9 49.8 52.6 66.9 61.4 44.3 48.1 43.6 41.2 55.7 56.8
slovene 58.1 47.9 45.2 49.1 44.5 42.4 53.5 61.8 42.1 40.6 42.1 32.5 40.8 41.1
swedish 70.0 58.5 58.8 59.3 60.4 53.5 65.2 66.9 51.1 51.1 53.3 44.5 53.0 53.2
averages 61.0 52.3 54.9 55.7 53.1 50.8 55.7 60.9 48.8 49.2 47.3 40.4 52.7 53.2
NED
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
arabic 63.6 37.3 59.2 67.1 63.2 56.5 65.8 64.1 48.9 48.0 48.8 47.5 62.7 69.0
basque 69.6 55.8 51.5 55.6 53.4 58.8 38.0 65.8 57.6 57.1 51.5 49.3 67.2 59.1
czech 71.0 55.7 70.2 65.2 67.3 63.2 69.7 71.6 53.2 52.9 54.1 47.6 56.3 68.6
danish 72.0 63.1 72.9 69.5 73.5 65.9 71.8 76.4 64.8 63.5 58.9 53.5 61.6 71.5
dutch 71.6 58.6 68.6 72.0 69.7 60.6 63.8 66.9 63.5 54.5 63.5 46.9 55.1 67.0
en-childes 80.9 79.6 82.8 74.1 72.7 77.1 83.2 80.4 78.1 78.3 77.5 67.2 61.0 75.2
en-ptb 75.2 69.8 69.4 73.8 67.2 64.1 71.6 69.8 49.8 67.0 49.6 44.8 53.9 68.1
portuguese 67.5 79.8 75.6 75.7 71.7 66.9 78.2 80.4 62.1 66.6 61.3 51.8 57.3 75.4
slovene 64.4 60.7 56.9 58.9 57.1 55.9 66.7 68.7 49.2 47.3 49.2 38.9 43.8 56.6
swedish 80.1 70.9 73.2 73.8 72.7 66.7 77.0 77.1 64.0 64.0 62.0 56.0 56.5 71.0
averages 71.6 63.1 68.0 68.6 66.9 63.6 68.6 72.1 59.1 59.9 57.6 50.3 57.6 68.1
Table 3: Directed accuracy, undirected accuracy and NED results for the dependency task (using supplied POS). The
first column (BC) is our benchmark system, the next seven are participants systems, and the remaining columns consist
of the DMV benchmark and various simple baselines. The superscripts c, p and u denote which type of POS was used,
and S1 and S2 denote two different submissions for S?gaard (2012).
Overall the full POS tagset lead to the best perfor-
mance over the coarse and universal tags (consider-
ing undirected accuracy or NED), which is to be ex-
pected as there is considerably more syntactic infor-
mation contained in the full POS. This must be bal-
anced against the additional model complexity from
expanding its parameter space, which may explain
why the difference in performance differences are so
small. The same pattern can also be seen in Marac?ek
and Z?abokrtsky? (2012)?s submission, whose system
using full POS (Mp) outperformed their other vari-
ants.
6.3 Joint task
As we had only one submission for the joint prob-
lem of POS and dependency induction, there are
few conclusions we can draw for this joint task (see
Table 4 for the results, and Table 9 for the short
sentence evaluation). Compared to the dependency
induction task using gold standard POS, as shown
in Table 3, the accuracy for the joint models are
lower. Interestingly, the DMV model performs best
when using the same number of word clusters as
there are POS tags, mirroring the findings reported
73
directed
testset CGS DMVc DMVp DMVu
arabic N/A 35.3 44.4 34.2
basque 24.5 27.5 25.1 28.7
czech 24.7 19.9 33.2 20.0
danish 21.4 23.3 31.9 10.0
dutch 15.1 20.6 33.7 20.5
en-childes 29.9 38.6 42.2 40.3
en-ptb 21.5 22.5 23.3 17.2
portuguese 19.7 28.5 28.0 17.1
slovene 19.2 13.9 11.5 14.4
swedish 23.6 26.4 26.4 20.5
averages 22.2 25.7 30.0 22.3
undirected
testset CGS DMVc DMVp DMVu
arabic N/A 45.5 52.5 45.0
basque 43.5 46.4 47.3 47.0
czech 38.9 37.5 50.9 38.5
danish 51.4 52.2 48.8 37.3
dutch 40.3 41.9 48.6 40.8
en-childes 54.9 59.2 60.8 58.1
en-ptb 43.4 45.4 48.8 39.4
portuguese 45.5 51.8 52.7 39.8
slovene 32.8 33.3 36.7 32.8
swedish 45.6 48.9 48.9 40.3
averages 44.0 46.2 49.6 41.9
NED
testset CGS DMVc DMVp DMVu
arabic N/A 53.4 57.6 53.3
basque 55.9 55.6 54.4 54.7
czech 51.2 49.3 63.4 51.5
danish 61.7 60.3 60.4 46.3
dutch 47.2 57.5 56.8 55.2
en-childes 78.2 77.7 78.1 76.5
en-ptb 53.9 60.2 63.5 47.5
portuguese 50.0 69.4 70.8 57.9
slovene 40.7 38.7 47.5 40.3
swedish 54.5 65.4 65.4 54.3
averages 54.8 58.8 61.8 53.8
Table 4: Directed, undirected and NED accuracy results
for evaluating the predicted dependency structures in the
joint task (i.e., not using supplied POS tags). The first
column is the participant?s system and the next three are
DMV models trained on the Brown word clusters (see
section 6.1).
above with gold standard tags. The best joint sys-
tem was the DMVp model, which only marginally
under-performed the equivalent DMV model trained
on gold POS. This is an encouraging finding, sug-
gesting that word clusters are able to represent im-
portant POS distinctions to inform deeper syntactic
processing.
6.4 Analysis
Until now we have adopted the standard metrics in
dependency evaluation: namely directed head at-
tachment accuracy, and its more lenient counter-
parts, undirected accuracy and NED. The latter met-
rics reward structures that almost match the gold
standard tree, by way of rewarding child-parent
edges that are predicted in the reverse direction, i.e.,
attaching the child as the parent (NED takes this fur-
ther, by also rewarding the grandparent-child edge
when this occurs). This allows some degree of flexi-
bility when considering various contentious linguis-
tic decisions such as whether a preposition should
head a preposition phrase, or the head of the child
noun-phrase. This added leniency comes at a price,
as shown in Table 3 where the undirected accuracy
and NED results are considerably higher than di-
rected accuracy, and display less spread of values
(look in particular at the random trees, Ra). Is is
unclear that the predicted trees are truly predicting
linguistically plausible structures, but instead that
the differences are due largely to chance. Moreover,
systems that predict linguistic phenomena inconsis-
tently between sentences or across types of related
phenomena are rewarded under these lenient met-
rics.
For these reasons we also consider a different,
less permissive, evaluation method, using multiple
references of the treebank where each is annotated
with different styles of dependency. As described
in section 2, we processed the Penn treebank five
times with different options to the LTH conversion
tool. This affected the treatment of coordination,
preposition phrases, subordinate clauses, infinitival
clauses etc. Next we compare the directed accu-
racy of the systems against these five different ?gold
standard? references, which are displayed in Table 5,
alongside the maximum score for each system. Note
that most systems performed well against the stan-
dard, conll2007 and functional references but poorly
against the lexical and oldLTH references.18 Con-
sidering the latter two references, a different system
would be selected as the highest performing, namely
Bisk and Hockenmaier (2012) (BH) over Blunsom
and Cohn (2010) (BC) which wins in the other cases.
18The common difference here is that the latter two refer-
ences do not treat prepositions as heads of PPs.
74
This evaluation method rewards many different lin-
guistically plausible structures, but in such a way
that the predictions must be consistent between dif-
ferent sentences in the testing set, and in their treat-
ment of related linguistic phenomena. One caveat
is that this method can only be used when there
are many references, although in many cases differ-
ent outputs can be generated automatically, e.g., by
adjusting head-finding heuristics in converting be-
tween phrase-structure to dependency trees.
The previous analysis has rated each system in
terms of overall performance against treebank trees,
however this doesn?t necessarily mean that the pre-
dictions of the best ranked system will be the most
useful ones in a task-based setting. Take the ex-
ample of information extraction, in which a central
problem is to identify the arguments (subject, object
etc) of a given verb. This setting gives rise to some
types of dependency edges being more valuable than
others. We present comparative results for the Penn
treebank in Table 6 showing the directed accuracy
for different types of dependency relations. Observe
that there is a wide spread of accuracies for predict-
ing the head word of the sentence (ROOT), and simi-
larly for verbs? subject and object arguments. These
scores are similar to the scores for the local modi-
fiers shown, such as NMOD which describe the ar-
guments of a noun. This is surprising as noun edges
tend to be much shorter than for the arguments to a
verb, and thus should be easier to predict. Also in-
teresting are the spread of results for the CC edges
(these link a coordinating conjunction to its head),
suggesting that the systems learn to represent coor-
dination in very different ways to the method used
in the reference.
Figure 1 illustrates the directed accuracy over dif-
ferent lengths of dependency edge. For all systems
the accuracy diminishes with edge length, however
some fall at a much faster rate. The two best systems
(Tu, BC) have similar overall accuracy results, but
it is clear that Tu does better on short edges while
BC does better on longer ones. The same pattern
was also observed when considering the average ac-
curacy over all treebanks (not shown), although the
systems? results were closer together.
system ROOT SBJ OBJ PRD NMOD COORD CC
Tu 71.0 64.8 53.7 49.4 56.9 36.8 11.4
LB 17.8 40.1 15.3 18.0 41.9 27.7 9.7
BC 74.9 65.7 53.0 50.2 56.8 36.3 71.4
DMVc 17.0 11.7 16.0 31.3 27.8 25.7 9.2
DMVu 17.6 9.3 16.4 25.0 27.8 25.7 8.6
BH 67.5 55.3 44.9 45.6 58.6 27.6 62.7
Mu 29.3 42.4 38.8 51.8 34.5 30.5 33.0
R 12.9 9.4 16.1 21.1 12.1 15.7 2.7
Mc 60.7 47.4 39.9 45.8 36.5 33.9 44.3
RB 17.9 12.4 26.2 36.5 15.3 25.4 1.1
H 19.4 29.3 12.2 22.2 17.3 20.9 10.3
DMVp 54.7 42.0 30.7 30.1 28.9 25.4 24.3
S2 45.2 41.9 44.2 49.8 39.7 25.4 63.8
Mp 67.8 54.3 49.6 59.4 47.7 37.7 49.7
S1 43.1 47.9 36.3 46.7 27.9 23.5 7.6
Table 6: Directed accuracy results on the Penn treebank,
stratified by dependency relation. For clarity, only 9 im-
portant relation types are shown. The vertical bars sepa-
rate different groups of relations, from left to right, relat-
ing to the main verb, general modifiers and coordination.
7 Conclusion
This challenge set out to evaluate the state-of-the-
art in part-of-speech and dependency grammar in-
duction, promoting research in this field and, im-
portantly, providing a fair means of evaluation. The
participants submissions used a wide variety of dif-
ferent approaches, many of which we shown to
have improved over competitive benchmark sys-
tems. While the results were overall very positive,
it is fair to say that the tasks of part-of-speech and
grammar induction are still very much open chal-
lenges, and that there is still considerable room for
improvement. The data submitted to this evaluation
campaign will provide a great resource for devising
new methods of evaluation, and we plan to pursue
this avenue in future work, in particular task-based
evaluation such as in an information extraction or
machine translation setting.
8 Acknowledgements
This challenge was funded by the PASCAL 2 (Pat-
tern Analysis, Statistical Modelling and Compu-
tational Intelligence) European Network of Excel-
lence. We would also like to thank the treebank
providers for allowing us to use their resources, as-
sisting us in converting these into our desired for-
mat, and helping to resolve various questions. In
particular, special thanks to Zdenek Zabokrtsky and
Jan (Czech and Arabic), Tomaz Erjavec (Slovene),
and Eckhard Bick and Diana Santos (Portuguese).
We are also indebted to the organisers of the previ-
75
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
conll2007 54.9 51.7 40.4 49.2 36.8 32.2 41.7 54.2 20.9 33.2 20.4 18.0 30.1 20.3
functional 59.6 52.4 41.5 47.4 36.2 30.6 40.0 58.5 20.9 37.2 20.6 19.3 29.2 23.7
lexical 40.6 41.9 28.5 37.3 24.8 27.7 35.5 39.5 23.5 23.1 23.0 14.4 33.1 10.1
oldLTH 41.4 43.6 28.8 37.8 24.6 28.6 36.1 39.5 22.3 23.7 21.8 14.3 32.0 10.7
standard 56.0 50.4 41.0 50.3 37.5 32.8 42.5 55.5 22.3 33.5 21.8 18.4 31.4 20.4
best 59.6 52.4 41.5 50.3 37.5 32.8 42.5 58.5 23.5 37.2 23.0 19.3 33.1 23.7
Table 5: Directed accuracy results measured against different conversions of the Penn Treebank into dependency trees.
l l
l
l
l
l l
l l
2 4 6 8
10
20
30
40
50
60
70
edge length
direc
ted a
ccura
cy (%)
l TuBCBHMZ?pS?2DMV?p
Figure 1: Directed accuracy on the Penn treebank strat-
ified by dependency length. For clarity only a subset of
the systems are shown, and edges of length 10 or more
were omitted.
ous CoNLL 2006 and 2007 competitions, who con-
tributed significant efforts into collating so many
treebanks and developing treebank conversion tools,
making our job much easier than it would other-
wise have been. Thanks to Sebastian Reidel, Joakim
Nivre and Sabine Buchholz for promptly answer-
ing our questions. We would like to thank the
LDC, who allowed their licenced data to be used
free of charge by the competitors, and Ilya Ahtaridis
who administered the licencing and corpus distribu-
tion. Thanks also to Valentin Spitkovski and Chris-
tos Christodoulopoulos who kindly provided us with
their evaluation scripts, and finally, the participants
themselves for taking part.
References
