Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 108?118,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Constructing a Practical Constituent Parser from a Japanese Treebank with
Function Labels
Takaaki Tanaka and Masaaki Nagata
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
{tanaka.takaaki, nagata.masaaki}@lab.ntt.co.jp
Abstract
We present an empirical study on construct-
ing a Japanese constituent parser, which can
output function labels to deal with more de-
tailed syntactic information. Japanese syn-
tactic parse trees are usually represented as
unlabeled dependency structure between bun-
setsu chunks, however, such expression is in-
sufficient to uncover the syntactic information
about distinction between complements and
adjuncts and coordination structure, which is
required for practical applications such as syn-
tactic reordering of machine translation. We
describe a preliminary effort on constructing
a Japanese constituent parser by a Penn Tree-
bank style treebank semi-automatically made
from a dependency-based corpus. The eval-
uations show the parser trained on the tree-
bank has comparable bracketing accuracy as
conventional bunsetsu-based parsers, and can
output such function labels as the grammatical
role of the argument and the type of adnominal
phrases.
1 Introduction
In Japanese NLP, syntactic structures are usually
represented as dependencies between grammatical
chunks called bunsetsus. A bunsetsu is a grammat-
ical and phonological unit in Japanese, which con-
sists of an independent-word such as noun, verb
or adverb followed by a sequence of zero or more
dependent-words such as auxiliary verbs, postposi-
tional particles or sentence final particles. It is one
of main features of Japanese that bunsetsu order is
much less constrained than phrase order in English.
Since dependency between bunsetsus can treat flexi-
ble bunsetsu order, most publicly available Japanese
parsers including CaboCha (Kudo et al, 2002) and
KNP (Kawahara et al, 2006) return bunsetsu-based
dependency as syntactic structure. Such bunsetsu-
based parsers generally perform with high accuracy
and have been widely used for various NLP applica-
tions.
However, bunsetsu-based representations also
have serious shortcomings for dealing with Japanese
sentence hierarchy. The internal structure of a bun-
setsu has strong morphotactic constraints in contrast
to flexible bunsetsu order. A Japanese predicate
bunsetsu consists of a main verb followed by a se-
quence of auxiliary verbs and sentence final parti-
cles. There is an almost one-dimensional order in
the verbal constituents, which reflects the basic hi-
erarchy of the Japanese sentence structure including
voice, tense, aspect and modality. Bunsetsu-based
representation cannot provide the linguistic structure
that reflects the basic sentence hierarchy.
Moreover, bunsetsu-based structures are unsuit-
able for representing such nesting structure as co-
ordinating conjunctions. For instance, bunsetsu rep-
resentation of a noun phrase ???-? (technology-
GEN) / ??-? (improvement-CONJ) / ??-?
(economy-GEN) / ?? (growth) ? technology im-
provement and economic growth does not allow
us to easily interpret it, which means ((technol-
ogy improvement) and (economic growth)) or (tech-
nology (improvement and economic growth)), be-
cause bunsetsu-based dependencies do not con-
vey information about left boundary of each noun
phrase (Asahara, 2013). This drawback complicates
108
operating syntactically meaningful units in such ap-
plications as statistical machine translation, which
needs to recognize syntactic units in building a trans-
lation model (e.g. tree-to-string and tree-to-tree) and
in preordering source language sentences.
Semantic analysis, such as predicate-argument
structure analysis, is usually done as a pipeline pro-
cess after syntactic analysis (Iida et al, 2011 ;
Hayashibe et al, 2011 ); but in Japanese, the dis-
crepancy between syntactic and semantic units cause
difficulties integrating semantic analysis with syn-
tactic analysis.
Our goal is to construct a practical constituent
parser that can deal with appropriate grammatical
units and output grammatical functions as semi-
semantic information, e.g., grammatical or seman-
tic roles of arguments and gapping types of relative
clauses. We take an approach to deriving a grammar
from manually annotated corpora by training prob-
abilistic models like current statistical constituent
parsers of de facto standards (Petrov et al, 2006;
Klein et al, 2003 ; Charniak, 2000; Bikel, 2004).
We used a constituent-based treebank that Uematsu
et al (2013) converted from an existing bunsetsu-
based corpus as a base treebank, and retag the non-
terminals and transform the tree structures in de-
scribed in Section 3. We will present the results of
evaluations of the parser trained with the treebank in
Section 4, and show some analyses in Section 5.
2 Related work
The number of researches on Japanese constituent-
based parser is quite few compared to that of
bunsetsu-dependency-based parser. Most of them
have been conducted under lexicalized grammatical
formalism.
HPSG (Head-driven Phrase Structure Gram-
mar) (Sag et al, 2003 ) is a representative one.
Gunji et al (1987) proposed JPSG (Japanese Phrase
Structure Grammar) that is theoretically precise to
handle the free word order problem of Japanese. Na-
gata et al ( 1993 ) built a spoken-style Japanese
grammar and a parser running on it. Siegel et al(
2002 ) constructed a broad-coverage linguistically
precise grammar JACY, which integrates semantics,
MRS (Minimal Recursion Semantics) (Copestake,
2005). Bond et al ( 2008 ) built a large-scale
Japanese treebank Hinoki based on JACY and used
it for parser training.
Masuichi et al(2003) developed a Japanese LFG
(Lexicalized-Functional Grammar) (Kaplan et al,
1982) parser whose grammar is sharing the de-
sign with six languages. Uematsu et al (2013)
constructed a CCG (Combinatory Categorial Gram-
mar) bank based on the scheme proposed by
Bekki (2010), by integrating several corpora includ-
ing a constituent-based treebank converted from a
dependency-base corpus.
These approaches above use a unification-based
parser, which offers rich information integrating
syntax, semantics and pragmatics, however, gener-
ally requires a high computational cost. We aim
at constructing a more light-weighted and practical
constituent parser, e.g. a PCFG parser, from Penn
Treebank style treebank with function labels. Gab-
bard et al (2006) introduced function tags by modi-
fying those in Penn Treebank to their parser. Even
though Noro et al (2005) built a Japanese corpus for
deriving Japanese CFG, and evaluated its grammar,
they did not treat the predicate-argument structure or
the distinction of adnominal phrases.
This paper is also closely related to the work of
Korean treebank transformations (Choi et al, 2012).
Most of the Korean corpus was built using grammat-
ical chunks eojeols, which resemble Japanese bun-
setsus and consist of content words and morphemes
that represent grammatical functions. Choi et al
transformed the eojeol-based structure of Korean
treebanks into entity-based to make them more suit-
able for parser training. We converted an existing
bunsetsu-based corpus into a constituent-based one
and integrating other information into it for training
a parser.
3 Treebank for parser training
In this section, we describe the overview of our tree-
bank for training a parser.
3.1 Construction of a base treebank
Our base treebank is built from a bunsetsu-
dependency-based corpus, the Kyoto Corpus (Kuro-
hashi et al, 2003), which is a collection of news-
paper articles, that is widely used for training data
for Japanese parsers and other applications. We
109
SIP-MAT[nad]:A
VP[nad]:A
VP[nad]:A
AUX
?
-PAST
VB[nad]
??
give
PP-OBJ
PCS
?
-ACC
NN
?
book
PP-OB2
PCS
?
-DAT
NN
??
student
S
IP-MAT[nad]:A
VP[nad]:A
AUX
?
-PAST
VB[nad]
??
give
PP-OBJ
PCS
?
-ACC
NN
?
book
PP-OB2
PCS
?
-DAT
NN
??
student
(I) gave the student a book.
binary tree n-ary (flattened) tree
Figure 1: Verb Phrase with subcategorization and voice information
NN General noun
NNP Proper noun
NPR Pronoun
NV Verbal noun
NADJ Adjective noun
NADV Adverbial noun (incl. temporal noun)
NNF Formal noun (general)
NNFV Formal noun (adverbial)
PX Prefix
SX Suffix
NUM Numeral
CL Classifier
VB Verb
ADJ Adjective
ADNOM Adnominal adjective
ADV Adverb
PCS Case particle
PBD Binding particle
PADN Adnominal particle
PCO Parallel particle
PCJ Conjunctive particle
PEND Sentence-ending particle
P Particle (others)
AUX Auxiliary verb
CONJ Conjunction
PNC Punctuation
PAR Parenthesis
SYM Symbol
FIL Filler
Table 1: Preterminal tags
automatically converted from dependency structure
to phrase structure by the previously described
method (Uematsu et al, 2013), and conversion er-
rors of structures and tags were manually corrected.
We adopted the annotation schema used in
Japanese Keyaki treebank (Butler et al, 2012) and
Annotation Manual for the Penn Historical Corpora
and the PCEEC (Santorini, 2010) as reference to re-
tag the nonterminals and transform the tree struc-
tures.
The original Kyoto Corpus has fine-grained part-
of-speech tags, which we converted into simpler
preterminal tags shown in Table 1 for training by
lookup tables. First the treebank?s phrase tags ex-
cept function tags are assigned by simple CFG rule
sets, then, function tags are added by integrating the
information from the other resources or manually
annotated. We integrate predicate-argument infor-
mation from the NAIST Text Corpus (NTC) (Iida et
al., 2007 ) into the treebank by automatically con-
verting and adding tag suffixes (e.g. -SBJ, -ARG0
described in section 3.3) to the original tags of the
argument phrases. The structure information about
coordination and apposition are manually annotated.
3.2 Complementary information
We selectively added the following information as
tag suffixes and tested their effectiveness.
Inflection We introduced tag suffixes for inflec-
tion as clues to identify the attachment position of
the verb and adjective phrases, because Japanese
verbs and adjectives have inflections, which depends
110
(no label) base form
cont continuative form
attr attributive form
neg negative form
hyp hypothetical form
imp imperative form
stem stem
Table 2: Inflection tag suffixes
on their modifying words and phrases (e.g. noun
and verb phrases). Symbols in Table 2 are attached
to tags VB, ADJ and AUX, based on their inflection
form. The inflection information is propagated to the
phrases governing the inflected word as a head. We
adopted these symbols from the notation of Japanese
CCG described in (Bekki, 2010).
Subcategorization and voice Each verb has a
subcategorization frame, which is useful for build-
ing verb phrase structure. For instance, ??
tsukamu ?grasp? takes two arguments, nominative
and accusative cases, ??? ataeru ?give? takes
three arguments: nominative, accusative and dative
cases. We also added suffixes to verb tags to de-
note which arguments they require (n:nominative,
a:accusative and d: dative). For instance, the
verb??? ?give? takes three arguments (nomina-
tive, accusative and dative cases), it is tagged with
VB[nad].
We retrieve this information from a Japanese case
frame dictionary, Nihongo Goitaikei (Ikehara et al,
1997), which has 14,000 frames for 6,000 verbs and
adjectives. As an option, we also added voice infor-
mation (A:active, P:passive and C:causative) to the
verb phrases, because it effectively helps to discrim-
inate cases.
3.3 Annotation schema
We introduce phrase and function tags in Table 3 and
use them selectively based on the options described
below.
Tree Structure We first built a treebank with bi-
nary tree structure (except the root and terminal
nodes), because it is comparably easy to convert
the existing Japanese dependency-based corpus to
it. We converted the dependency-based corpus by
a previously described method in (Uematsu et al,
2013). The binary tree?s structure has the follow-
NP Noun phrase
PP Postposition phrase
VP Verb phrase
ADJP Adjective phrase
ADVP Adverbial phrase
CONJP Conjunction phrase
S Sentence (=root)
IP Inflectional phrase
IP-MAT Matrix clause
IP-ADV Adverb clause
IP-REL Gapping relative clause
IP-ADN Non-gapping adnominal clause
CP Complementizer phrase
CP-THT Sentential complement
Function tags
semantic role for mandatory argument (gap notation)
-ARG0 ( arg0)
-ARG1 ( arg1)
-ARG2 ( arg2)
grammatical role for mandatory argument (gap notation)
-SBJ ( sbj) Subjective case
-OBJ ( obj) Objective case
-OB2 ( ob2) Indirect object case
arbitrary argument
-TMP Temporal case
-LOC Locative case
-COORD Coordination (for n-ary)
-NCOORD Left branch of NP coord. (for binary)
-VCOORD Left branch of VP coord. (for binary)
-APPOS Apposition
-QUE Question
Table 3: Phrase tags
ing characteristics about verb phrase (VP) and post-
position phrase (PP): VP from the same bunsetsu
is a left-branching subtree and the PP-VP structure
(roughly corresponding to the argument-predicate
structure) is a right-branching subtree. Pure binary
trees tend to be very deep and difficult to annotate
and interpret by humans. We also built an n-ary tree
version by flattening these structures.
The predicate-argument structure, which is usu-
ally represented by PPs and a VP in the treebank,
particularly tends to be deep in binary trees based
on the number of arguments. To flatten the structure,
we remove the internal VP nodes by intermediately
re-attaching all of the argument PPs to the VP that
dominates the predicate. Figure 1 shows an example
of flattening the PP-VP structure.
For noun phrases, since compound nouns and nu-
merals cause deep hierarchy, the structure that in-
cludes them is flattened under the parent NP. The
coordinating structure is preserved, and each NP el-
ement of the coordination is flattened
111
IP-MAT
VP
VP
P
?
-PAST
VB
????
chase
PP-OBJ
PCS
?
-ACC
NN
?
cat
PP-SBJ
PCS
?
-NOM
NN
?
dog
NP
NP
NN
?
dog
IP-REL sbj
VP
P
?
-PAST
VB
????
chase
PP-OBJ
PCS
?
-ACC
NN
?
cat
NP
NP
NN
??
photo
IP-ADN
VP
VP
AUX
??
-PROG
VP
P
?
VB
????
chase
PP-OBJ
PCS
?
-ACC
NN
?
cat
PP-SBJ
PCS
?
-NOM
NP
?
dog
The dog chased the cat. The dog that chased the cat The photo of a dog chasing a cat
Figure 2: Leftmost tree shows annotation of grammatical roles in a basic inflectional phrase. Right two trees show
examples of adnominal phrases.
Predicates and arguments The predicate?s argu-
ment is basically marked with particles, which rep-
resent cases in Japanese; thus, they are represented
as a postpositional phrase, which is composed of
a noun phrase and particles. The leftmost tree in
Figure 2 is an example of the parse result of the
following sentence: ?-? inu-ga ?dog-NOM? ?-
? neko-o ?cat-ACC?????? oikaketa ?chased?
(The dog chased the cat.)
We annotated predicate arguments by two dif-
ferent schemes (different tag sets) in our treebank:
grammatical roles and semantic roles. In using a tag
set based on grammatical roles, the arguments are
assigned with the suffixes based on their syntactic
roles in the sentence, like Penn Treebank: SBJ (sub-
ject), OBJ (direct object), and OB2 (indirect object).
Figure 2 is annotated by this scheme.
Alternatively, the arguments are labeled based on
their semantic roles from case frame of predicates,
like PropBank (Palmer et al, 2005 ): ARG0, ARG1
and ARG2. These arguments are annotated by con-
verting semantic roles defined in the case frame dic-
tionary Goitaikei into simple labels, the labels are
not influenced by case alternation.
In both annotation schemes, we also annotated
two types of arbitrary arguments with semantic role
labels: LOC (locative) and TMP (temporal), which
can be assigned consistently and are useful for vari-
ous applications.
Adnominal clauses Clauses modifying noun
phrases are divided into two types: (gapping) rela-
tive and non-gapping adnominal clauses. Relative
clauses are denoted by adding function tag -REL to
phrase tag IP. Such a gap is directly attached to
IP-REL tag as a suffix consisting of an underscore
and small letters in our treebank, e.g., IP-REL sbj
for a subject-gap relative clause, so that the parser
can learn the type of gap simultaneously, unlike
the Penn Treebank style, where gaps are marked
as trace ?*T*?. For instance, note the structure of
the following noun phrase, which is shown in the
middle tree in Figure 2: ?-? neko-o ?cat-ACC?
????? oikake-ta ?to chase? ? inu ?dog?
?neko-o (cat-ACC) oikaketa (chase) inu? (The dog
that chased the cat.). We also adopt another type of
gap notation that resembles the predicate-argument
structure: semantic role notation. In the example
above, tag IP-REL arg0 is attached to the relative
clause instead.
We attach tag IP-ADN to another type of ad-
nominal clauses, which has no gap, the modified
noun phrase is not an argument of the predicate in
the adnominal clause. The rightmost in Figure 2 is
an example of a non-gapping clause: ?-? inu-ga
?dog-NOM? ?-? neko-o ?cat-ACC? ?????
?? oikake-teiru ?chasing? ?? shashin ?photo?
(A photo of a dog chasing a cat.), where there is no
predicate-argument relation between the verb ??
??? chase and the noun?? photo.
112
Coordination and apposition The notation of
such parallel structure as coordination and apposi-
tion differs based on the type of tree structure. For
binary trees, the coordination is represented by a
left-branching tree, which is a conjunction or a con-
junction particle that first joined a left hand con-
stituent; the phrase is marked as a modifier consist-
ing of coordination (-NCOORD and -VCOORD for
NP and VP coordinations), as shown on the left side
of Figure 3. On the other hand, in n-ary trees, all the
coordination elements and conjunctions are aligned
flatly under the parent phrase with suffix -COORD.
The apposition is represented in the same way using
tag -APPOS instead.
Phrase and sentential elements Since predicate
arguments are often omitted in Japanese, discrimi-
nation between the fragment of larger phrases and
sentential elements is not clear. In treebank, we em-
ploy IP and CP tags for inflectional and comple-
mentizer phrases, assuming that tags with function
tag suffixes to the phrase correspond to the max-
imum projection of the predicate (verb or adjec-
tive). The matrix phrase and the adverbial phrase
have IP-MAT and IP-ADV tags respectively. This
annotation schema is adopted based on the Penn
Historical Corpora (Santorini, 2010) and Japanese
Keyaki treebank (Butler et al, 2012) as previously
described, while IP in our treebank is not so flat as
them.
Such sentential complements as that-clauses in
English are tagged with CP-THT. In other words,
the sentential elements, which are annotated with
SBAR, S, and trace *T* in the Penn Treebank, are
tagged with CP or IP in our treebank.
4 Evaluation
The original Kyoto Corpus has 38,400 sentences
and they were automatically converted to constituent
structures. The function tags are also added to the
corpus by integrating predicate-argument informa-
tion in the NAIST Text corpus. Since the conver-
sion contains errors of structures and tags, about half
of them were manually checked to avoid the effects
of the conversion errors.
We evaluated our treebank?s effectiveness for
parser training with 18,640 sentences, which were
divided into three sets: 14,895 sentences for a train-
Tag set LF1 Comp UF1 Comp
binary tree
Base 88.4 34.0 89.6 37.9
Base inf 88.5? 33.5 90.0? 39.3
Fullsr 80.7 13.6 88.4 35.9
Fullsr inf 81.1? 15.5 ? 88.7? 36.9
Fullsr lex 79.8? 13.1 87.7? 34.3
Fullsr vsub 80.3? 12.5 87.9? 35.1
Fullsr vsub alt 78.6? 13.3 86.7? 32.5?
Fullgr 81.0 15.6 88.5 37.3
Fullgr inf 81.3? 15.3 88.8 37.2
Fullgr lex 80.3? 14.2 87.9? 33.6?
Fullgr vsub 81.2 15.5 88.5 35.2
Fullgr vsub alt 77.9? 11.7? 86.0? 29.9?
n-ary tree
Fullsr 76.7 11.4 85.3 28.0
Fullsr inf 76.9 11.6 85.4 28.7
Fullsr lex 76.5 11.1 84.7? 27.9
Fullsr vsub 76.5 10.8 84.9? 26.2
Fullsr vsub alt 76.6 11.0 84.8? 27.2
Fullgr 77.2 13.2 85.3 29.2
Fullgr inf 77.4 12.0? 85.5 28.3
Fullgr lex 77.6 12.2? 85.0 28.5
Fullgr vsub 77.1 12.7? 84.8? 28.8
Fullgr vsub alt 76.9 12.2? 84.7? 26.3?
Table 4: Parse results displayed by labeled and unla-
beled F1 metrics and proportion of sentences completely
matching gold standard (Comp). Base contains only ba-
sic tags, not grammatical function tags. Figures with ???
indicate statistically significant differences (? = 0.05)
from the results without complementary information, i.e.,
Fullsr or Fullgr.
113
NP
NN
??
interest
PP
PADN
?
-GEN
NP
NNP
B ?
B Company
PP-NCOORD
PCJ
?
CONJ
NNP
A ?
A Company
NP
NN
??
interest
PP
PADN
?
-GEN
NP-COORD
NNP
B ?
B Company
PCJ
?
CONJ
NNP
A ?
A Company
the interests of A Company and B Company
Figure 3: Noun phrase coordination
tag set UAS
binary tree
Base 89.1
Base inf 89.4
Fullsr 87.9
Fullsr inf 88.3
Fullgr 88.0
Fullgr inf 88.5?
n-ary (flattened) tree
Fullsr 82.8
Fullsr inf 83.3
Fullgr 82.9
Fullgr inf 83.0
Table 5: Dependency accuracies of the results converted
into bunsetsu dependencies.
ing set, 1,860 sentences for a test set, and the re-
mainder for a development set.
The basic evaluations were under the condition of
using the original tag sets: the basic set Base, which
contains all the preterminal tags in Table 1 and the
phrase tags in Table Table 3, except the IP and CP
tags, and the full set Full, which has Base + IP, CP
tags, and all the function tags. The basic set Base
is provided to evaluate the constituent parser perfor-
mance in case that we need better performance at the
cost of limiting the information.
We used two types of function tag sets: Full sr for
semantic roles and Full gr for grammatical roles.
We added the following complementary informa-
tion to the tags and named the new tag sets Base or
Full and suffix:
inf: add inflection information to the POS tag
(verbs, adjectives, and auxiliary verbs) and the
phrase tags (Table 2).
lex: lexicalize the closed words, i.e., auxiliary
verbs and particles.
vsub: add verb subcategorization to the verb and
verb phrase tags.
vsub alt: add verb subcategorization and case al-
ternation to the verb and verb phrase tags.
In comparing the system output with the gold stan-
dard, we remove the complementary information to
ignore different level of annotation, thus, we do not
discriminate between VB[na] and VB[nad] for
example.
We used the Berkeley parser (Petrov et al, 2006)
for our evaluation and trained with six iterations for
latent annotations. In training the n-ary trees, we
used a default Markovization parameter (h = 0, v =
1), because the parser performed the best with the
development set.
Table 4 shows the parsing results of the test sets.
On the whole, the binary tree outperformed the n-
ary tree. This indicates that the binary tree struc-
ture was converted from bunsetsu-based dependen-
cies, whose characteristics are described in Section
3.3, and is better for parser training than the partially
flattened structure.
114
As for additional information, the inflection suf-
fixes slightly improved the F1-metrics. This is
mainly because the inflection information gives the
category of the attached phrase (e.g., the attributive
form for noun phrases). The others did not provide
any improvement, even though we expected the sub-
categorization and case alternation information to
help the parser detect and discriminate the grammat-
ical roles, probably because we simply introduced
the information by concatenating the suffixes to the
base tags to adapt an off-the-shelf parser in our eval-
uation. For instance, VB[n] and VB[na] are rec-
ognized as entirely independent categories; a sophis-
ticated model, which can treat them hierarchically,
would improve the performance.
For comparison with a bunsetsu-based depen-
dency parser, we convert the parser output into unla-
beled bunsetsu dependencies by the following sim-
ple way. We first extract all bunsetsu chunks in
a sentence and find a minimum phrase including
each bunsetsu chunk from a constituent structure.
For each pair of bunsetsus having a common parent
phrase, we add a dependency from the left bunsetsu
to the right one, since Japanese is a head-final lan-
guage.
The unlabeled attachment scores of the converted
dependencies are shown as the accuracies in Table 5,
since most bunsetsu-based dependency parsers out-
put only unlabeled structure.
The Base inf results are comparable with the
bunsetsu-dependency results (90.46%) over the
same corpus (Kudo et al, 2002)1, which has only
the same level of information. Constituent parsing
with treebank almost matched the current bunsetsu
parsing.
5 Analysis
In this section, we analyze the error of parse results
from the point of view of the discrimination of gram-
matical and semantic roles, adnominal clause and
coordination.
Grammatical and semantic roles Predicate argu-
ments usually appeared as PP, which is composed of
noun phrases and particles. We focus on PPs with
function labels. Table 6 shows the PP results with
1The division for the training and test sets is different.
tag P R F1
PP-ARG0 64.9 75.0 69.6
PP-ARG1 70.6 80.1 75.1
PP-ARG2 60.3 68.5 64.1
PP-TMP 40.1 43.6 41.8
PP-LOC 23.8 17.2 20.0
tag P R F1
PP-SBJ 69.6 81.5 75.1
PP-OBJ 72.6 83.5 77.7
PP-OB2 63.6 71.4 67.3
PP-TMP 45.0 48.0 46.5
PP-LOC 21.3 15.9 18.2
Table 6: Discrimination of semantic role and grammati-
cal role labels (upper: semantic roles, lower: grammatical
role)
system \ gold PP-SBJ PP-OBJ PP-OB2
PP-SBJ *74.9 6.5 2.3
PP-OBJ 5.8 *80.1 0.5
PP-OB2 1.7 0.3 *68.5
PP-TMP 0.2 0.0 0.5
PP-LOC 0.2 0.0 0.4
PP 6.5 2.0 16.8
other labels 0.5 0.2 0.3
no span 10.2 10.9 11.0
system \ gold PP-TMP PP-LOC
PP-SBJ 4.7 4.1
PP-OBJ 0.0 0.0
PP-OB2 6.0 13.8
PP-TMP *43.6 2.8
PP-LOC 2.0 *17.2
PP 37.6 49.7
other labels 1.4 5.0
no span 4.7 7.4
Table 7: Confusion matrix for grammatical role labels
(recall). Figures with ?*? indicate recall.(binary tree,
Fullgr)
tag P R F1
IP-REL sbj 48.4 54.3 51.1
IP-REL obj 27.8 22.7 24.9
IP-REL ob2 17.2 29.4 21.7
IP-ADN 50.9 55.4 53.1
CP-THT 66.1 66.6 66.3
Table 8: Results of adnominal phrase and sentential ele-
ment (binary tree, Fullgr)
115
grammatical and semantic labels under the Fullsr
and Fullgr conditions respectively.
The precision and the recall of mandatory argu-
ments did not reach a high level. The results are
related to predicate argument structure analysis in
Japanese. But, they cannot be directly compared,
because the parser in this evaluation must output a
correct target phrase and select it as an argument, al-
though most researches select a word using a gold
standard parse tree. Hayashibe et al ( 2011 ) re-
ported the best precision of ARG0 discrimination to
be 88.42 % 2, which is the selection results from
the candidate nouns using the gold standard parse
tree of NTC. If the cases where the correct candi-
dates did not appear in the parser results are ex-
cluded (10.8 %), the precision is 72.7 %. The main
remaining error is to label to non-argument PP with
suffix -ARG0 (17.0%), thus, we must restrain the
overlabeling to improve the precision.
The discrimination of grammatical role is higher
than that of semantic role, which is more directly es-
timated by case particles following the noun phrases.
The confusion matrix for the recall in Table 7 shows
main problem is parse error, where correct phrase
span does not exist (no span), and marks 10-11%.
The second major error is discrimination from bare
PPs (PPs without suffixes), mainly because the clues
to judge whether the arguments are mandatory or ar-
bitrary lack in the treebank. Since even the manda-
tory arguments are often omitted in Japanese, it is
not facilitate to identify arguments of predicates by
using only syntactic information.
Adnominal phrases We need to discriminate be-
tween two types of adnominal phrases as described
in Section 3.3: IP-REL and IP-ADN. Table 8
shows the discrimination results of the adnominal
phrase types. The difference between IP-REL
(gapped relative clauses) and IP-ADN is closely re-
lated to the discrimination of the grammatical role:
whether the antecedent is the argument of the head
predicate of the relative clause.
Table 8 shows the discrimination results of the
adnominal phrases. The results indicate the diffi-
culties of discriminating the type of gaps of rela-
2The figure is calculated only for the arguments that appear
as the dependents of predicates, excluding the omitted argu-
ments.
tive clause IP-REL. The confusion matrix in Ta-
ble 9 shows that the discrimination between gaps
and non-gaps, i.e., IP-REL and IP-ADN, is moder-
ate as for IP-REL sbj and IP-REL obj. How-
ever, IP-REL ob2 is hardly recognized, because
it is difficult to determine whether the antecedent,
which is marked with particle ?ni?, is a mandatory ar-
gument (IP-REL ob2) or not (IP-ADN). Increas-
ing training samples would improve the discrimina-
tion, since there are only 290 IP-REL ob2 tags for
8,100 IP-ADN tags in the training set.
Naturally discrimination only by syntactic infor-
mation has limitation; this baseline can be improved
by incorporating semantic information.
Coordination Figure 10 shows the coordination
results, which are considered the baseline for only
using syntactic information. Improvement is possi-
ble by incorporating semantic information, since the
disambiguation of coordination structure essentially
needs semantic information.
6 Conclusion
We constructed a Japanese constituent-based parser
to be released from the constraints of bunsetsu-based
analysis to simplify the integration of syntactic and
semantic analysis. Our evaluation results indicate
that the basic performance of the parser trained with
the treebank almost equals bunsetsus-based parsers
and has the potential to supply detailed syntactic
information by grammatical function labels for se-
mantic analysis, such as predicate-argument struc-
ture analysis.
Future work will be to refine the annotation
scheme to improve parser performance and to eval-
uate parser results by adapting them to such NLP
applications as machine translation.
Acknowledgments
We would like to thank Associate Professor Yusuke
Miyao, Associate Professor Takuya Matsuzaki of
National Institute of Informatics and Sumire Ue-
matsu of the University of Tokyo for providing us
the language resources and giving us valuable sug-
gestions.
116
system \ gold IP-REL sbj IP-REL obj
IP-REL sbj *55.0 30.3
IP-REL obj 8.5 *33.3
IP-REL ob2 0.5 0.0
IP-ADN 10.0 9.0
IP-ADV 0.3 0.0
VP 8.5 7.6
other labels 1.2 6.2
no span 16.0 13.6
system \ gold IP-REL ob2 IP-ADN
IP-REL sbj 29.4 7.5
IP-REL obj 0.0 0.6
IP-REL ob2 *11.8 0.0
IP-ADN 23.5 *57.3
IP-ADV 0.5 0.3
VP 17.6 9.3
other labels 5.4 3.0
no span 11.8 22.0
Table 9: Confusion matrix for adnominal phrases (recall).
Figures with ?*? indicate recall.(binary tree, Fullgr)
tag P R F1
NP-COORD 62.6 60.7 61.6
VP-COORD 57.6 50.0 53.5
NP-APPOS 46.0 40.0 42.8
Table 10: Results of coordination and apposition (binary
tree, Fullgr)
References
Masayuki Asahara. 2013. Comparison of syntactic de-
pendency annotation schemata . In Proceedings of
the 3rd Japanese Corpus Linguistics Workshop, In
Japanese.
Daisuke Bekki. 2010. Formal theory of Japanese syntax.
Kuroshio Shuppan, In Japanese.
Daniel M. Bikel. 2004. A distributional analysis of a
lexicalized statistical parsing model. In Proceedings
of Empirical Methods in Natural Language Processing
(EMNLP 2004), Vol.4, pp. 182?189.
Francis Bond, Sanae Fujita and Takaaki Tanaka. 2008.
The Hinoki syntactic and semantic treebank of
Japanese. In Journal of Language Resources and
Evaluation, Vol.42, No. 2, pp. 243?251
Alastair Butler, Zhu Hong, Tomoko Hotta, Ruriko
Otomo, Kei Yoshimoto and Zhen Zhou. 2012. Keyaki
Treebank: phrase structure with functional informa-
tion for Japanese. In Proceedings of Text Annotation
Workshop.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American
chapter of the Association for Computational Linguis-
tics conference, (NAACL 2000), pp. 132?139.
DongHyun Choi, Jungyeul Park and Key-Sun Choi.
2012. Korean treebank transformation for parser train-
ing. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (ACL
2012), pp. 78-88.
Ann Copestake, Dan Flickinger, Carl Pollard and Ivan A.
Sag. 2005. Minimal recursion semantics: an introduc-
tion. Research on Language and Computation, Vol. 3,
No. 4, pp. 281-332.
Ryan Gabbard, Mitchell Marcus and Seth Kulick. 2006.
Fully parsing the Penn Treebank. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association of Compu-
tational Linguistics (HLT-NAACL 2006), pp. 184?191.
Takao Gunji. 1987 Japanese phrase structure grammar:
a unification-based approach. D.Reidel.
Yuta Hayashibe, Mamoru Komachi and Yujzi Mat-
sumoto. 2011. Japanese predicate argument struc-
ture analysis exploiting argument position and type. In
Proceedings of the 5th International Joint Conference
on Natural Language Processing (IJCNLP 2011), pp.
201-209.
Ryu Iida, Mamoru Komachi Kentaro Inui and Yuji Mat-
sumoto. 2007. Annotating a Japanese text corpus with
predicate-argument and coreference relations. In Pro-
ceedings of Linguistic Annotation Workshop, pp. 132?
139.
Ryu Iida, Massimo Poesio. 2011. A cross-lingual ILP
solution to zero anaphora resolution. In Proceedings
117
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies (ACL-HLT 2011), pp. 804-813.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Kentaro Ogura, Yoshifumi Ooyama and Yoshi-
hiko Hayashi. 1998. Nihongo Goitaikei. Iwanami
Shoten, In Japanese.
Ronald M. Kaplan and Joan Bresnan. 1982. Lexical-
Functional Grammar: a formal system for grammat-
ical representation. In the Mental Representation of
Grammatical Relations (Joan Bresnan ed.), pp. 173?
281. The MIT Press.
Daisuke Kawahara and Sadao Kurohashi. 2006. A
fully-lexicalized probabilistic model for Japanese syn-
tactic and case structure analysis. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association of Compu-
tational Linguistics (HLT-NAACL 2006), pp. 176?183.
Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
processing. Advances in Neural Information Process-
ing Systems, 15:3?10.
Taku Kudo and Yuji Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In Pro-
ceedings of the 6th Conference on Natural Language
Learning (CoNLL-2002), Volume 20, pp. 1?7.
Sadao Kurohashi and Makoto Nagao. 2003. Building a
Japanese parsed corpus ? while improving the parsing
system. In Abeille (ed.), Treebanks: Building and us-
ing parsed corpora, Chap. 14, pp. 249?260. Kluwer
Academic Publishers.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. In Journal of Com-
putational Linguistics. Vol.19, No.2, pp. 313?330.
Hiroshi Masuichi, Tomoko Okuma, Hiroki Yoshimura
and Yasunari Harada. 2003 Japanese parser on the ba-
sis of the Lexical-Functional Grammar formalism and
its evaluation. In Proceedings of the 17th Pacific Asia
Conference on Language, Information and Computa-
tion (PACLIC 17), pp. 298-309.
Masaaki Nagata and Tsuyoshi Morimoto,. 1993.
A unification-based Japanese parser for speech-to-
speech translation. In IEICE Transaction on Informa-
tion and Systems. Vol.E76-D, No.1, pp. 51?61.
Tomoya Noro, Taiichi Hashimoto, Takenobu Tokunaga
and Hotsumi Tanaka. 2005. Building a large-scale
Japanese syntactically annotated corpus for deriving a
CFG. in Proceedings of Symposium on Large-Scale
Knowledge Resources (LKR2005), pp..159 ? 162.
Matha Palmer, Daniel Gildea and Paul Kingsbury. 2005.
The Proposition Bank: n annotated corpus of semantic
roles. Computational Linguistics, Vol.31 No. 1, pp.
71?106.
Slav Petrov, Leon Barrett, Romain Thibaux and Dan
Klein.. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the Association
for Computational Linguistics (COLING-ACL 2006),
pp. 433-440.
Ivan A. Sag, Thomas Wasow and Emily M. Bender,.
2003. Syntactic theory: a formal introduction. 2nd
Edition, CSLI Publications.
Beatrice Santorini. 2010. Annotation manual for the
Penn Historical Corpora and the PCEEC (Release 2).
Department of Linguistics, University of Pennsylva-
nia.
Melanie Siegel and Emily M. Bender. 2002. Efficient
deep processing of Japanese. In Proceedings of the
3rd Workshop on Asian Language Resources and In-
ternational Standardization at the 19th International
Conference on Computational Linguistics, Vol. 12, pp.
1?8.
Sumire Uematsu, Takuya Matsuzaki, Hiroaki Hanaoka,
Yusuke Miyao and Hideki Mima. 2013. Integrat-
ing multiple dependency corpora for inducing wide-
coverage Japanese CCG resources. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (ACL 2013), pp. 1042?1051.
118
