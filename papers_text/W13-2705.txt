Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 36?42,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Language Technology for Agile Social Media Science
Simon Wibberley
Department of Informatics
University of Sussex
sw206@susx.ac.uk
Jeremy Reffin
Department of Informatics
University of Sussex
j.p.reffin@susx.ac.uk
David Weir
Department of Informatics
University of Sussex
davidw@susx.ac.uk
Abstract
We present an extension of the DUALIST
tool that enables social scientists to engage
directly with large Twitter datasets. Our
approach supports collaborative construc-
tion of classifiers and associated gold stan-
dard data sets. The tool can be used to
build classifier cascades that decomposes
tweet streams, and provide analysis of tar-
geted conversations. A central concern is
to provide an environment in which social
science researchers can rapidly develop an
informed sense of what the datasets look
like. The intent is that they develop, not
only an informed view as to how the data
could be fruitfully analysed, but also how
feasible it is to analyse it in that way.
1 Introduction
In recent years, automatic social media analysis
(SMA) has emerged, not only as a major focus
of attention within the academic NLP community,
but as an area that is of increasing interest to a va-
riety of business and public sectors organisations.
Among the many social media platforms in use to-
day, the one that has received the most attention is
Twitter, the second most popular social media net-
work in the world with over 400 million tweets
sent each day. The popularity of Twitter as a tar-
get of SMA derives from both the public nature
of tweets, and the availability of the Twitter API
which provides a variety of flexible methods for
scraping tweets from the live Twitter stream.
A plethora of social media monitoring plat-
forms now exist, that are mostly concerned with
providing product marketing oriented services1.
For example, brand monitoring services seek to
provide companies with an understanding of what
1http://wiki.kenburbary.com/social-media-monitoring-
wiki lists 230 Social Media Monitoring Solutions
is being said about their brands and products,
with language processing technology being used
to capture relevant comments or conversations and
apply some form of sentiment analysis (SA), in or-
der to derive insights into what is being said. This
paper forms part of a growing body of work that
is attempting to broaden the scope of SMA be-
yond the realm of product marketing, and into ar-
eas of concern to social scientists (Carvalho et al,
2011; Diakopoulos and Shamma, 2010; Gonzalez-
Bailon et al, 2010; Marchetti-Bowick and Cham-
bers, 2012; O?Connor et al, 2010; Tumasjan et al,
2011; Tumasjan et al, 2010).
Social media presents an enormous opportunity
for the social science research community, consti-
tuting a window into what large numbers of people
are talking. There are, however, significant obsta-
cles facing social scientists interested in making
use of big social media datasets, and it is important
for the NLP research community to gain a better
understanding as to how language technology can
support such explorations.
A key requirement, and the focus of this paper,
is agility: the social scientist needs to be able to
engage with the data in a way that supports an it-
erative process, homing in on a way of analysing
the data that is likely to produce valuable insight.
Given what is typically a rather broad topic as a
starting point, there is a need to see what issues re-
lated to that topic are being discussed and to what
extent. It can be important to get a feeling for the
kind of language being used in these discussions,
and there is a need to rapidly assess the accuracy
of the automated decision making. There is little
value in developing an analysis of the data on an
approach that relies on the technology making de-
cisions that are so nuanced that the method being
used is highly unreliable. As the answers to these
questions are being exposed, insights emerge from
the data, and it becomes possible for the social sci-
entist to progressively refine the topics that are be-
36
ing targetted, and ultimately create a way of au-
tomatically analysing the data that is likely to be
insightful.
Supporting this agile methodology presents se-
vere challenges from an NLP perspective, where
the predominant approaches use classifiers that
involve supervised machine learning. The need
for substantial quantities of training data, and
the detrimental impact on performance that re-
sults when applying them to ?out-of-domain? data
mean that exisiting approaches cannot support the
agility that is so important when social scientists
engage with big social media datasets.
We describe a tool being developed in collab-
oration with a team of social scientists to support
this agile methodology. We have built a frame-
work based on DUALIST, an active learning tool
for building classifiers (Settles, 2011; Settles and
Zhu, 2012). This framework provides a way for
a group of social scientists to collaboratively en-
gage with a stream of tweets, with a goal of con-
structing a chain (or cascade) of automatic docu-
ment classification layers that isolate and analyse
targeted conversions on Twitter. Section 4 dis-
cusses ways in which the design of our frame-
work is intended to support the agile methodol-
ogy mentioned above, with particular emphasis on
the value of DUALIST?s active learning approach,
and the crucial role of the collaborative gold stan-
dard and model building activities. Section 4.3
discusses additional data processing step that have
been introduced to increase the frameworks use-
fulness, and section 5 introduces some projects to
which the framework is being applied.
2 Related Work
Work that focuses on addressing sociological
questions with SMA broadly fall into one of three
categories.
? Approaches that employ automatic data analy-
sis without tailoring the analysis to the specifics of
the situation e.g. (Tumasjan et al, 2010; Tumas-
jan et al, 2011; O?Connor et al, 2010; Gonzalez-
Bailon et al, 2010; Sang and Bos, 2012; Bollen
et al, 2011). This body of research involves lit-
tle or no manual inspection of the data. An an-
alytical technique is selected a-priori, applied to
the SM stream, and the results from that analy-
sis are then aligned with a real-world phenomenon
in order to draw predictive or correlative conclu-
sions about social media. A typical approach is
to predict election outcomes by counting mentions
of political parties and/or politicians as ?votes? in
various ways. Further content analysis is then
overlaid, such as sentiment or mood anlysis, in
an attempt to improve performance. However the
generic language-analysis techniques that are ap-
plied lead to little or no gain, often causing ad-
justments to target question to something with less
strict assessment criteria, such as poll trend instead
of election outcome (Tumasjan et al, 2010; Tu-
masjan et al, 2011; O?Connor et al, 2010; Sang
and Bos, 2012). This research has been criticised
for applying out-of-domain techniques in a ?black
box? fashion, and questions have been raised as
to how sensitive the results are to parameters cho-
sen (Gayo-Avello, 2012; Jungherr et al, 2012).
? Approaches that employ manual analysis of
the data by researchers with a tailored analyti-
cal approach (Bermingham and Smeaton, 2011;
Castillo et al, 2011).This approach reflects tra-
ditional research methods in the social sciences.
Through manual annotation effort, researchers en-
gage closely with the data in a manual but in-
teractive fashion, and this effort enables them to
uncover patterns in the data and make inferences
as to how SM was being used in the context of
the sociocultural phenomena under investigation.
This research suffers form either being restricted
to fairly small datasets.
? Approaches that employ tailored automatic
data analysis, using a supervised machine-learning
approach(Carvalho et al, 2011; Papacharissi and
de Fatima Oliveira, 2012; Meraz and Papacharissi,
2013; Hopkins and King, 2010). This research in-
fers properties of the SM data using statistics from
their bespoke machine learning analysis. Mannual
annotation effort is required to train the classifiers
and is typically applied in a batch process at the
commencement of the investigation.
Our work aims to expand this last category, im-
proving the quality of research by capturing more
of the insight-provoking engagement with the data
seen in more traditional research.
3 DUALIST
Our approach is built around DUALIST (Settles,
2011; Settles and Zhu, 2012), an open-source
project designed to enable non-technical analysts
to build machine-learning classifiers by annotat-
ing documents with just a few minutes of effort.
37
In Section 4, we discuss various ways in which
we have extended DUALIST, including function-
ality allowing multiple annotators to work in par-
allel; incorporating functionality to create ?gold-
standard? test sets and measure inter-annotator
agreement; and supporting on-going performance
evaluation against the gold standard during the
process of building a classifier. DUALIST pro-
vides a graphical interface with which an annota-
tor is able to build a Na??ve Bayes? classifier given
a collection of unlabelled documents. During the
process of building a classifier, the annotator is
presented with a selection of documents (in our
case tweets) that he/she has an opportunity to la-
bel (with one of the class labels), and, for each
class, a selection of features (tokens) that the an-
notator has an opportunity to mark as being strong
features for that class.
Active learning is used to select both the docu-
ments and the features being presented for annota-
tion. Documents are selected on the basis of those
that the current model is most uncertain about
(as measured by posterior class entropy), and fea-
tures are selected for a given class on the basis
of those with highest information gain occurring
frequently with that class. After a batch of docu-
ments and features have been annotated, a revised
model is built using both the labelled data and the
current model?s predictions for the remaining un-
labelled data, through the use of the Expectation-
Maximization algorithm. This new model is then
used as the basis for selecting the set of documents
and features that will be presented to the annotator
for the next iteration of the model building pro-
cess. Full details can be found in Settles (2011).
The upshot of this is two-fold: not only can a
reasonable model be rapidly created, but the re-
searcher is exposed to an interesting non-uniform
sample of the training data. Examples that are rel-
atively easy for the model to classify, i.e. those
with low entropy, are ranked lower in the list of
unlabelled data awaiting annotation. The effect of
this is that the training process facilitates a form of
data exploration that exposes the user to the hard-
est border cases.
4 Extending DUALIST for Social Media
Science Research
This section describes ways in which we have ex-
tended DUALIST to provide an integrated data ex-
ploration tool for social scientists. As outlined in
the introduction, our vision is that a team of social
scientists will be able to use this tool to collabora-
tively work towards the construction of a cascade
of automatic document classification layers that
carve up an incoming Twitter data stream in order
to pick out one or more targeted ?conversations?,
and provide an analysis of what is being discussed
in each of these ?conversations?. In what follows,
we refer to the social scientists as the researchers
and the activity during which the researchers are
working towards delivering a useful classifier cas-
cade as data engagement.
4.1 Facilitating data engagement
When embarking on the process of building one
of the classifiers in the cascade, researchers bring
preconceptions as to the basis for the classifica-
tion. It is only when engaging with the data that
it becomes possible to develop an adequate clas-
sification policy. For example, when looking for
tweets that express some attitude about a targeted
issue, one needs a policy as to how a tweet that
shares a link to an opinion piece on that topic
without any further comment should be classified.
There are a number of ways in which we support
the classification policy development process.
? One of the impacts of the active learning ap-
proach adopted in DUALIST is that by presenting
tweets that the current model is most unsure of,
DUALIST will very rapidly expose issues around
how to make decisions on boundary cases.
? We have extended DUALIST to allow multi-
ple researchers to build a classifier concurrently.
In addition to reducing the time it takes to build
classifiers, this fosters a collaborative approach to
classification policy development.
? We have added functionality that allows for the
collaborative construction of gold standard data
sets. Not only does this provide feedback dur-
ing the model building process as to when perfor-
mance begins to plateau, but, as a gold standard
is being built, researchers are shown the current
inter-annotator agreement score, and are shown
examples of tweets where there is disagreement
among annotators. This constitutes yet another
way in which researchers are confronted with the
most problematic examples.
4.2 Building classifier cascades
Having considered issues that relate to the con-
struction of an individual classifier, we end this
38
section by briefly considering issues relating to
the classifier cascade. The Twitter API provides
basic boolean search functionality that is used to
scrape the Twitter stream, producing the input to
the cascade. A typical strategy is to select query
terms for the boolean search with a view to achiev-
ing a reasonably high recall of relevant tweets2.
An effective choice of query terms that actually
achieves this is one of the things that is not well
understood in advance, but which we expect to
emerge during the data engagement phase. Cap-
turing an input stream that contains a sufficiently
large proportion of interesting (relevant) tweets is
usually achieved at the expense of precision (the
proportion of tweets in the stream being scraped
that are relevant). As a result, the first task that is
typically undertaken during the data engagement
phase involves building a relevancy classifier, to
be deployed at the top of the classifier cascade,
that is designed to filter out irrelevant tweets from
the stream of tweets being scraped.
When building the relevancy classifier, the re-
searchers begin to see how well their preconcep-
tions match the reality of the data stream. It is only
through the process of building this classifier that
the researchers begin to get a feel for the compo-
sition of the relevant data stream. This drives the
researcher?s conception as to how best to divide
up the stream into useful sub-streams, and, as a
result, provides the first insights into an appropri-
ate cascade architecture. Our experience is that in
many cases, classifiers at upper levels of the cas-
cade are involved in decomposing data streams in
useful ways, and classifiers that are lower down
in the cascade are designed to measure some facet
(e.g. sentiment polarity) of the material on some
particular sub-stream.
4.3 Tools for Data Analysis
As social scientists are starting to engage with
real-world data using this framework, it has
emerged that certain patterns of downstream data
analysis are of particular use.
Time series analysis. For many social phenom-
ena, the timing and sequence of social media mes-
sages are of critical importance, particularly for a
platform such as Twitter. Our framework supports
tweet volume analysis across any time frame, al-
2In many cases it is very hard to estimate recall since there
is no way to estimate accurately the volume of relevant tweets
in the full Twitter stream.
lowing researchers to review changes over time
in any classifier?s input or output tweet flows
(classes). This extends the common approach of
sentiment tracking over time to tracking over time
any attitudinal (or other) response whose essen-
tial features can be captured by a classifier of this
kind. These class-volume-by-time-interval plots
can provide insight into how and when the stream
changes in response to external events.
Link analysis. It is becoming apparent that link
sharing (attaching a URL to a tweet, typically
pointing to a media story) is an important aspect of
how information propagates through social media,
particularly on Twitter. For example, the mean-
ing of a tweet can sometimes only be discerned by
inspecting the link to which it points. We are in-
troducing to the framework automatic expansion
of shortened URLs and the ability to inspect link
URL contents, allowing researchers to interpret
tweets more rapidly and accurately. A combina-
tion of link analysis with time series analysis is
also providing researchers with insights into how
mainstream media stories propagate through soci-
ety and shape opinion in the social media age.
Language use analysis. Once a classifier has
been initially established, the framework analyses
the language employed in the input tweets using
an information gain (IG) measure. High IG fea-
tures are those that have occurrence distributions
that closely align the document classification dis-
tributions; essentially they are highly indicative of
the class. This information is proving useful to so-
cial science researchers for three purposes. First,
it helps identify the words and phrases people em-
ploy to convey a particular attitude or opinion in
the domain of interest. Second, it can provide in-
formation on how the language employed shifts
over time, for example as new topics are intro-
duced or external events occur. Third, it can be
used to select candidate keywords with which to
augment the stream?s boolean scraper query. In
this last case, however, we need to augment the
analysis; many high IG terms make poor scraper
terms because they are poorly selective in the more
general case (i.e. outside of the context of the ex-
isting query-selected sample). We take a sample
using the candidate term alone with the search API
and estimate the relevancy precision of the scraped
tweet sample by passing the tweets through the
first-level relevancy classifier. The precision of the
39
new candidate term can be compared to the preci-
sion of existing terms and a decision made.
5 Applications and Extensions
The framework?s flexibility enables it to be applied
to any task that can be broken down into a series of
classification decisions, or indeed where this ap-
proach materially assists the social scientist in ad-
dressing the issue at hand. In order to explore its
application, our framework is being applied to a
variety of tasks:
Identifying patterns of usage. People use the
same language for different purposes; the frame-
work is proving to be a valuable tool for eluci-
dating these usage patterns and for isolating data
sets that illustrate these patterns. As an example,
the authors (in collaboration with a team of so-
cial scientists) are studying the differing ways in
which people employ ethnically and racially sensi-
tive language in conversations on-line. The frame-
work has helped to reveal and isolate a number of
distinct patterns of usage.
Tracking changes in opinion over time. Sen-
timent classifiers trained in one domain perform
poorly when applied to another domain, even
when the domains are apparently closely related
(Pang and Lee, 2008). Traditionally, this has
forced a choice between building bespoke clas-
sifiers (at significant cost), or using generic sen-
timent classifiers (which sacrifice performance).
The ability to rapidly construct sentiment classi-
fiers that are specifically tuned to the precise do-
main can significantly increase classifier perfor-
mance without imposing major additional costs.
Moving beyond sentiment, with these bespoke
classifiers it is in principle possible to track over
time any form of opinion that is reflected in lan-
guage. In a second study, the authors are (in col-
laboration with a team of social scientists) build-
ing cascades of bespoke classifiers to investigate
shifts in citizens? attitudes over time (as expressed
in social media) to a range of political and social
issues arising across the European Union.
Entity disambiguation. References to individ-
uals are often ambiguous. In the general case,
word sense disambiguation is most success-
fully performed by supervised-learning classifiers
(Ma`rquez et al, 2006), and the low cost of pro-
ducing classifiers using this framework makes this
approach practical for situations where we require
repeated high recall, high precision searches of
large data sets for a specific entity. As an example,
this approach is being employed in the EU attitu-
dinal survey study.
Repeated complex search. In situations where
a fixed but complex search needs to be performed
repeatedly over a relatively long period of time,
then a supervised-learning classifier can be ex-
pected both to produce the best results and to be
cost-effective in terms of the effort required to
train it. The authors have employed this approach
in a commercial environment (Lyra et al, 2012),
and the ability to train classifiers more quickly
with this framework reduces the cost still further
and makes this a practical approach in a wider
range of circumstances.
With regard to extension of the framework, we
have identified a number of avenues for expansion
and improvement that will significantly increase
its usefulness and applicability to real-world sce-
narios, and we have recently commenced an 18-
month research programme to formalise and ex-
tend the framework and its associated methodol-
ogy for use in social science research3.
Conclusions and Future Work
We describe an agile analysis framework built
around the DUALIST tool designed to support ef-
fective exploration of large twitter data sets by
social scientists. The functionality of DUAL-
IST has been extended to allow the scraping of
tweets through access to the Twitter API, collab-
orative construction of both gold standard data
sets and Na??ve Bayes? classifiers, an Information
Gain-based method for automatic discovery of
new search terms, and support for the construction
of classifier cascades. Further extensions currently
under development include grouping tweets into
threads conversations, and automatic clustering of
relevant tweets in order to discover subtopics un-
der discussion.
Acknowledgments
We are grateful to our collaborators at the Cen-
tre for the Analysis of social media, Jamie Bartlett
and Carl Miller for valuable contributions to this
work. We thank the anonymous reviewers for their
helpful comments. This work was partially sup-
ported by the Open Society Foundation.
3Towards a Social Media Science, funded by the UK
ESRC National Centre for Research Methods.
40
References
[Bermingham and Smeaton2011] Adam Bermingham
and Alan F Smeaton. 2011. On using Twitter to
monitor political sentiment and predict election
results. In Proceedings of the Workshop on Senti-
ment Analysis where AI meets Psychology (SAAIP),
IJCNLP 2011, pages 2?10.
[Bollen et al2011] Johan Bollen, Alberto Pepe, and
Huina Mao. 2011. Modeling public mood and emo-
tion: Twitter sentiment and socio-economic phe-
nomena. In Proceedings of the Fifth International
AAAI Conference on Weblogs and Social Media,
pages 450?453.
[Carvalho et al2011] Paula Carvalho, Lu??s Sarmento,
Jorge Teixeira, and Ma?rio J. Silva. 2011. Liars and
saviors in a sentiment annotated corpus of comments
to political debates. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: short
papers - Volume 2, pages 564?568, Stroudsburg, PA,
USA.
[Castillo et al2011] Carlos Castillo, Marcelo Mendoza,
and Barbara Poblete. 2011. Information credibility
on Twitter. In Proceedings of the 20th International
Conference on World wide web, pages 675?684.
[Diakopoulos and Shamma2010] Nicholas A Di-
akopoulos and David A Shamma. 2010. Charac-
terizing debate performance via aggregated Twitter
sentiment. In Proceedings of the 28th international
conference on Human factors in computing systems,
pages 1195?1198.
[Gayo-Avello2012] Daniel Gayo-Avello. 2012. I
wanted to predict elections with twitter and all i
got was this lousy paper a balanced survey on elec-
tion prediction using Twitter data. arXiv preprint
arXiv:1204.6441.
[Gonzalez-Bailon et al2010] Sandra Gonzalez-Bailon,
Rafael E Banchs, and Andreas Kaltenbrunner. 2010.
Emotional reactions and the pulse of public opin-
ion: Measuring the impact of political events on
the sentiment of online discussions. arXiv preprint
arXiv:1009.4019.
[Hopkins and King2010] Daniel J. Hopkins and Gary
King. 2010. A method of automated nonparametric
content analysis for social science. American Jour-
nal of Political Science, 54(1):229?247.
[Jungherr et al2012] Andreas Jungherr, Pascal Ju?rgens,
and Harald Schoen. 2012. Why the Pirate Party
won the German election of 2009 or the trouble
with predictions: A response to Tumasjan, Sprenger,
Sander, & Welpe. Social Science Computer Review,
30(2):229?234.
[Lyra et al2012] Matti Lyra, Daoud Clarke, Hamish
Morgan, Jeremy Reffin, and David Weir. 2012.
Challenges in applying machine learning to media
monitoring. In Proceedings of Thirty-second SGAI
International Conference on Artificial Intelligence
(AI-2012).
[Marchetti-Bowick and Chambers2012] Micol
Marchetti-Bowick and Nathanael Chambers.
2012. Learning for microblogs with distant su-
pervision: political forecasting with Twitter. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational
Linguistics, pages 603?612.
[Ma`rquez et al2006] Llu??s Ma`rquez, Gerard Escudero,
David Mart??nez, and German Rigau. 2006. Su-
pervised corpus-based methods for wsd. In Eneko
Agirre and Philip Edmonds, editors, Word Sense
Disambiguation, volume 33 of Text, Speech and
Language Technology, pages 167?216. Springer
Netherlands.
[Meraz and Papacharissi2013] Sharon Meraz and Zizi
Papacharissi. 2013. Networked gatekeeping and
networked framing on #egypt. The International
Journal of Press/Politics, 18(2):138?166.
[O?Connor et al2010] Brendan O?Connor, Ramnath
Balasubramanyan, Bryan R Routledge, and Noah A
Smith. 2010. From tweets to polls: Linking text
sentiment to public opinion time series. In Proceed-
ings of the International AAAI Conference on We-
blogs and Social Media, pages 122?129.
[Pang and Lee2008] Bo Pang and Lillian Lee. 2008.
Opinion mining and sentiment analysis. Founda-
tions and trends in Information Retrieval, 2(1-2):1?
135.
[Papacharissi and de Fatima Oliveira2012] Zizi Pa-
pacharissi and Maria de Fatima Oliveira. 2012.
Affective news and networked publics: the rhythms
of news storytelling on #egypt. Journal of Commu-
nication, 62(2):266?282.
[Sang and Bos2012] Erik Tjong Kim Sang and Johan
Bos. 2012. Predicting the 2011 dutch senate elec-
tion results with Twitter. Proceedings of the Euro-
pean Chapter of the Association for Computational
Linguistics 2012, page 53.
[Settles and Zhu2012] Burr Settles and Xiaojin Zhu.
2012. Behavioral factors in interactive training of
text classifiers. In Proceedings of the 2012 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 563?567.
[Settles2011] Burr Settles. 2011. Closing the loop:
Fast, interactive semi-supervised annotation with
queries on features and instances. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 1467?1478.
[Tumasjan et al2010] Andranik Tumasjan, Timm O
Sprenger, Philipp G Sandner, and Isabell M Welpe.
2010. Predicting elections with Twitter: What 140
characters reveal about political sentiment. In Pro-
ceedings of the fourth international AAAI confer-
ence on weblogs and social media, pages 178?185.
41
[Tumasjan et al2011] Andranik Tumasjan, Timm O
Sprenger, Philipp G Sandner, and Isabell M Welpe.
2011. Election forecasts with Twitter how 140 char-
acters reflect the political landscape. Social Science
Computer Review, 29(4):402?418.
42
