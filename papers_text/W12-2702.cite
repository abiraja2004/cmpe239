Yoshua Bengio and Rejean Ducharme. 2001. A neu-
ral probabilistic language model. In NIPS, volume 13,
pages 932?938.
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3(2):1137?1155.
Yoshua Bengio. 2007. learning deep architectures for
AI. Technical report, University of Montre?al.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In EMNLP, pages 858?867.
Stanley F. Chen and Joshua T. Goodman. 1999. An
empirical study of smoothing techniques for language
modeling. Computer Speech & Language, 13(4):359?
394.
Marcello Federico and Maura Cettolo. 2007. Efficient
handling of n-gram language models for statistical ma-
chine translation. In Second Workshop on SMT, pages
88?95.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Sixth Workshop on SMT,
pages 187?197.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
demonstration session.
L. Lamel, J.-L. Gauvain, V.-B. Le, I. Oparin, , and
S. Meng. 2011. Improved models for mandarin
speech-to-text transcription. In ICASSP, pages 4660?
4663.
H.S. Le, A. Allauzen, G. Wisniewski, and F. Yvon. 2010.
Training continuous space language models: Some
practical issues. In EMNLP, pages 778?788.
H.S. Le, I. Oparin, A. Allauzen, J-L. Gauvain, and
F. Yvon. 2011a. Structured output layer neural net-
work language model. In ICASSP, pages 5524?5527.
H.S. Le, I. Oparin, A. Messaoudi, A. Allauzen, J-L. Gau-
vain, and F. Yvon. 2011b. Large vocabulary SOUL
neural network language models. In Interspeech.
X. Liu, M. J. F. Gales, and P. C. Woodland. 2011. Im-
proving LVCSR system combination using neural net-
work language model cross adaptation. In Interspeech,
pages 2857?2860.
Toma?s? Mikolov, Martin Karafia?t, Luka?s? Burget, Jan
C?ernocky?, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Interspeech,
pages 1045?1048.
T. Mikolov, S. Kombrink, L. Burget, J.H. Cernocky, and
S. Khudanpur. 2011. Extensions of recurrent neural
network language model. In ICASSP, pages 5528?
5531.
Andriy Mnih and Geoffrey Hinton. 2008. A scalable
hierarchical distributed language model. In NIPS.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In ACL,
pages 220?224.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the Tenth International Workshop on
Artificial Intelligence and Statistics.
Junho Park, Xunying Liu, Mark J. F. Gales, and Phil C.
Woodland. 2010. Improved neural network based lan-
guage modelling and adaptation. In Interspeech, pages
1041?1044.
Holger Schwenk and Yannick Este`ve. 2008. Data selec-
tion and smoothing in an open-source system for the
2008 NIST machine translation evaluation. In Inter-
speech, pages 2727?2730.
Holger Schwenk and Jean-Luc Gauvain. 2002. Connec-
tionist language modeling for large vocabulary contin-
uous speech recognition. In ICASSP, pages I: 765?
768.
Holger Schwenk, Daniel De?chelotte, and Jean-Luc Gau-
vain. 2006. Continuous space language models for
statistical machine translation. In Proceedings of the
COLING/ACL 2006 Main Conference Poster Sessions,
pages 723?730.
Holger Schwenk. 2004. Efficient training of large neu-
ral networks for language modeling. In IJCNN, pages
3059?3062.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Holger Schwenk. 2010. Continuous space language
models for statistical machine translation. The Prague
Bulletin of Mathematical Linguistics, (93):137?146.
David Talbot and Miles Osborne. 2007. Smoothed
bloom filter language models: Tera-scale lms on the
cheap. In EMNLP, pages 468?476.
Puyang Xu, Asela Gunawardana, and Sanjeev Khudan-
pur. 2011. Efficient subsampling for training complex
language models. In EMNLP, pages 1128?1136.