Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 60?65,
Prague, June 2007. c?2007 Association for Computational Linguistics
Machine Learning with Semantic-Based Distances Between Sentences for
Textual Entailment
Daniel Ferre?s
TALP Research Center
Software Department
Universitat Polite`cnica de Catalunya
dferres@lsi.upc.edu
Horacio Rodr??guez
TALP Research Center
Software Department
Universitat Polite`cnica de Catalunya
horacio@lsi.upc.edu
Abstract
This paper describes our experiments on
Textual Entailment in the context of the
Third Pascal Recognising Textual Entail-
ment (RTE-3) Evaluation Challenge. Our
system uses a Machine Learning approach
with Support Vector Machines and Ad-
aBoost to deal with the RTE challenge. We
perform a lexical, syntactic, and semantic
analysis of the entailment pairs . From this
information we compute a set of semantic-
based distances between sentences. The re-
sults look promising specially for the QA en-
tailment task.
1 Introduction
This paper describes our participation in the RTE-
3 challenge. It is our first attempt to RTE and we
have taken profit of an analysis of the approaches
followed in previous challenges (see (Dagan et al,
2005), and (Bar-Haim et al, 2006) for overviews
of RTE-1 and RTE-2). Our approach, however, is
based on a set of semantic-based distance measures
between sentences used by our group in previous
contests in Question Answering (TREC 2004, see
(Ferre?s et al, 2005), and CLEF 2004, see (Ferre?s
et al, 2004)) , and Automatic Summarization (DUC
2006, see (Fuentes et al, 2006)). Although the use
of such measures (distance between question and
sentences in passages candidates to contain the an-
swer, distance between query and sentences candi-
dates to be included in the summary, ...) is different
for RTE task, our claim is that with some modifica-
tions the approach can be useful in this new scenario.
The organization of this paper is as follows. Af-
ter this introduction we present in section 2 a de-
scription of the measures upon which our approach
is built. Section 3 describes in detail our proposal.
Results are discussed in section 4. Conclusions and
further work is finally included in section 5.
2 System Description
Our approach for computing distance measures be-
tween sentences is based on the degree of overlap-
ping between the semantic content of the two sen-
tences. Obtaining the semantic content implies a
depth Linguistic Processing. Upon this semantic
representation of the sentences several distance mea-
sures are computed. We next describe such issues.
2.1 Linguistic Processing
Linguistic Processing (LP) consists of a pipe of
general purpose Natural Language (NL) processors
that performs tokenization, morphologic tagging,
lemmatization, Named Entities Recognition and
Classification (NERC) with 4 basic classes (PER-
SON, LOCATION, ORGANIZATION, and OTH-
ERS), syntactic parsing and semantic labelling, with
WordNet synsets, Magnini?s domain markers and
EuroWordNet Top Concept Ontology labels. The
Spear1 parser performs full parsing and robust de-
tection of verbal predicate arguments. The syntactic
constituent structure of each sentence (including the
specification of the head of each constituent) and the
relations among constituents (subject, direct and in-
direct object, modifiers) are obtained. As a result
1Spear. http://www.lsi.upc.edu/?surdeanu/
spear.html
60
of the performance of these processors each sen-
tence is enriched with a lexical and syntactic lan-
guage dependent representations. A semantic lan-
guage independent representation of the sentence
(called environment) is obtained from these analy-
ses (see (Ferre?s et al, 2005) for details). The en-
vironment is a semantic network like representation
built using a process to extract the semantic units
(nodes) and the semantic relations (edges) that hold
between the different tokens in the sentence. These
units and relations belong to an ontology of about
100 semantic classes (as person, city, action, mag-
nitude, etc.) and 25 relations (mostly binary) be-
tween them (e.g. time of event, actor of action, lo-
cation of event, etc.). Both classes and relations are
related by taxonomic links (see (Ferre?s et al, 2005)
for details) allowing inheritance. Consider, for in-
stance, the sentence ?Romano Prodi 1 is 2 the 3
prime 4 minister 5 of 6 Italy 7?. The following envi-
ronment is built:
i en proper person(1), entity has quality(2),
entity(5), i en country(7), quality(4),
which entity(2,1), which quality(2,5), mod(5,7),
mod(5,4).
2.2 Semantic-Based Distance Measures
We transform each environment into a labelled di-
rected graph representation with nodes assigned to
positions in the sentence, labelled with the corre-
sponding token, and edges to predicates (a dummy
node, 0, is used for representing unary predicates).
Only unary (e.g. entity(5) in Figure 1) and binary
(e.g. in Figure 2 which quality(2,5)) predicates are
used. Over this representation a rich variety of
lexico-semantic proximity measures between sen-
tences have been built. Each measure combines two
components:
? A lexical component that considers the set of
common tokens occurring in both sentences.
The size of this set and the strength of the com-
patibility links between its members are used
for defining the measure. A flexible way of
measuring token-level compatibility has been
set ranging from word-form identity, lemma
identity, overlapping of WordNet synsets, ap-
proximate string matching between Named En-
tities etc. For instance, ?Romano Prodi? is lex-
ically compatible with ?R. Prodi? with a score
of 0.5 and with ?Prodi? with a score of 0.41.
?Italy? and ?Italian? are also compatible with
score 0.7. This component defines a set of (par-
tial) weighted mapping between the tokens of
the two sentences that will be used as anchors
in the next component.
? A semantic component computed over the sub-
graphs corresponding to the set of lexically
compatible nodes (anchors). Four different
measures have been defined:
? Strict overlapping of unary predicates.
? Strict overlapping of binary predicates.
? Loose overlapping of unary predicates.
? Loose overlapping of binary predicates.
The loose versions allow a relaxed match-
ing of predicates by climbing up in the ontol-
ogy of predicates (e.g. provided that A and B
are lexically compatible, i en city(A) can match
i en proper place(B), i en proper named entity(B),
location(B) or entity(B)) 2. Obviously, loose over-
lapping implies a penalty on the score that depends
on the length of the path between the two predicates
and their informative content.
Romano Prodi1 is2 prime4 minister5 Italy7
0i_en_proper_person i_en_country
entity_has_quality
which_qualitywhich_entity mod
mod
quality entity
Figure 1: Example of an environment of a sentence.
2The ontology contains relations as i en city
isa i en proper place, i en proper place isa
i en proper named entity, proper place isa location,
i en proper named entity isa entity, location isa entity
61
3 System Architecture
We have adapted the set of measures described be-
fore for RTE in the following way:
1. We follow a Machine Learning (ML) approach
for building a classifier to perform the RTE
task. In previous applications the way of
weighting and combining the different mea-
sures was based on a crude optimization using
a development corpus.
2. We extract a more complex set of features for
describing the semantic content of the Text (T)
and the Hypothesis (H) as well as the set of se-
mantic measures between them. Table 1 con-
tains a brief summary of the features used.
3. We perform minor modifications on the token-
level compatibility measures for dealing with
the asymmetry of the entailment relation (basi-
cally using the hyponymy and the verbal entail-
ment relations of WordNet)
4. We add three new task-specific features (see
Table 1)
The overall architecture of the system is depicted
in Figure 2. As usual in ML, the system proceeds in
two phases, learning and classification. The left side
of the figure shows the learning process and the right
part the classification process. The set of examples
(tuples H, T) is first processed, in both phases, by LP
for obtaining a semantic representation of the tuple
(Hsem and Tsem). From this representation a Fea-
ture Extraction component extracts a set of features.
This set is used in the learning phase for getting a
classifier that is applied to the set of features of the
test, during the classification phase, in order to ob-
tain the answer.
4 Experiments
Before the submission we have performed a set of
experiments in order to choose the Machine Learn-
ing algorithms and the training sets to apply in the
final submission.
HTraining setT H
Test setT
Linguistic ProcessingLinguistic Processing
HsTraining set (sem)Ts
Feature Extraction Feature Extraction
HsTest set (sem)Ts
Features Features
Machine Learning Classifier
Answers
Figure 2: System Architecture.
4.1 Machine Learning Experiments
We used the WEKA3 ML platform (Witten and
Frank, 2005) to perform the experiments. We tested
9 different ML algorithms: AdaBoostM1, Bayes
Networks, Logistic Regression, MultiBoostAB,
Naive Bayes, RBF Network, LogitBoost (Simple Lo-
gistic in WEKA), Support Vector Machines (SMO in
WEKA), and Voted Perceptron. We used the previ-
ous corpora of the RTE Challenge (RTE-1 and RTE-
2) and the RTE-3 development test. A filtering pro-
cess has been applied removing pairs with more than
two sentences in the text or hypothesis, resulting a
total of 3335 Textual Entailment (TE) pairs. The re-
sults over 10-fold-Cross-Validation using a data set
composed by RTE-1, RTE-2, and RTE-3 develop-
ment set are shown in Table 2.
The results shown that AdaBoost, LogitBoost, and
SVM obtain the best results. Then we selected Ad-
aBoost and SVM to perform the classification of the
RTE-3 test set. The SVM algorithm tries to compute
the hyperplane that best separates the set of training
examples (the hyperplane with maximum margin)
(Vapnik, 1995). On the other hand, AdaBoost com-
3WEKA. http://www.cs.waikato.ac.nz/ml/
weka/
62
Features #features Description
semantic content of T 12 #locations, #persons, #dates, #actions, ...
semantic content of H 12 ...
intersection of T and H 12 ...
length of intersection
score of intersection
Strict overlapping of unary predicates 5 ratio of intersection related to shortest env
ratio of intersection related to longest env
ratio of intersection related to both (union of)
Strict overlapping of binary predicates 5 . . .
Loose overlapping of unary predicates 5 . . .
Loose overlapping of binary predicates 5 ...
Verbal entailment (WordNet) 1 V1 ? T, V2 ? H, such that V1 verbal entails V2
Antonymy 1 A1 ? T, A2 ? H, such that A1 and A2 are antonyms and
no token compatible with A2
#occurs in H Negation 1 Difference between # negation tokens in H and T
Table 1: Features used for classification with Machine Learning algorithms.
Algorithm #correct Accuracy
AdaBoostM1 1989 59.6402
BayesNet 1895 56.8216
Logistic 1951 58.5007
MultiBoostAB 1959 58.7406
NaiveBayes 1911 57.3013
RBFNetwork 1853 55.5622
LogitBoost 1972 59.1304
SVM 1972 59.1304
VotedPerceptron 1969 59.0405
Table 2: Results over 10-fold-Cross-Validation us-
ing a filtered data set composed by RTE-1, RTE-2,
and RTE-3 (a total of 3335 entailment pairs).
bines a set of weak classifiers into a strong one us-
ing lineal combination (Freund and Schapire, 1996).
The idea is combining many moderately accurate
rules into a highly accurate prediction rule. A weak
learning algorithm is used to find the weak rules.
4.2 Training Set Experiments
We designed two experiments in order to decide the
best training set to apply in the RTE-3 challenge. We
performed an experiment using RTE-1 and RTE-2
data sets as a training set and the RTE-3 develop-
ment set filtered (541 TE pairs) as a test set. In this
experiment AdaBoost and SVM obtained accuracies
of 0.6672 and 0.6396 respectively (see results in Ta-
ble 3. We performed the same experiment joining
the Answer Validation Exercise4 (AVE) 2006 En-
glish data set (Pen?as et al, 2006) and the Microsoft
Research Paraphrase Corpus5 (MSRPC) (Dolan et
al., 2004) to the previous corpora (RTE-1 and RTE-
2) resulting a total of 8585 entailment pairs filtering
pairs with a text or a hypothesis with more than 1
sentence. In our approach we considered that para-
phrases were bidirectional entailments. The para-
phrases of the MSRPC have been used as textual en-
tailments in only one direction: the first sentence in
the paraphrase has been considered the hypothesis
and the second one has been considered the text.
Using the second corpus for training and the RTE-
3 development set as test set resulted in a notable
degradation of accuracy (see Table 3).
Accuracy
Algorithm Corpus A Corpus B
AdaBoost 66.72% 53.78%
SVM 63.95% 59.88%
Table 3: Results over the RTE-3 development set
filtered (541 TE pairs) using as training set corpus A
(RTE-1 and RTE-2) and corpus B (RTE-1, RTE-2,
MSRPC, and AVE2006 English)
Finally, we performed a set of experiments to de-
tect the contribution of the different features used for
Machine Learning. These experiments revealed that
4AVE. http://nlp.uned.es/QA/AVE
5MSRPC. http://research.microsoft.com/
nlp/msr_paraphrase.htm
63
the three most relevant features were: Strict overlap-
ping of unary predicates, Semantic content of Hy-
pothesis, and Loose overlapping of unary predicates.
4.3 Official Results
Our official results at RTE-3 Challenge are shown
in Table 4. We submitted two experiments: the first
one with AdaBoost (run1) and the second one with
SVM (run2). Training data set for final experiments
were corpus: RTE-1 (development and test), RTE-
2 (development and test), and RTE-3 development.
The test set was the RTE-3 test set without filtering
the entailments (text or hypothesis) with more than
one sentence. In this case we joined multiple sen-
tences in a unique sentence that has been processed
by the LP component.
We obtained accuracies of 0.6062 and 0.6150. In
the QA task we obtained the best per-task results
with accuracies of 0.7450 and 0.7000 with AdaBoost
and SVM respectively.
Accuracy
Task run1 run2
AdaBoost SVM
IE 0.4350 0.4950
IR 0.6950 0.6800
QA 0.7450 0.7000
SUM 0.5500 0.5850
Overall 0.6062 0.6150
Table 4: RTE-3 official results.
5 Conclusions and Further Work
This paper describes our experiments on Textual En-
tailment in the context of the Third Pascal Recog-
nising Textual Entailment (RTE-3) Evaluation Chal-
lenge. Our approach uses Machine Learning al-
gorithms (SVM and AdaBoost) with semantic-based
distance measures between sentences. Although fur-
ther analysis of the results is in process, we observed
that our official per-task results at RTE-3 show a dif-
ferent distribution compared with the global results
of all system at RTE-2 challenge. The RTE-2 per-
task analysis showed that most of the systems scored
higher in accuracy in the multidocument summariza-
tion (SUM) task while in our system this measure is
low. Our system at RTE-3 challenge scored higher
in the QA and IR tasks with accuracies of 0.7450
and 0.6950 respectively in the first run.
References
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second pascal recognising textual entail-
ment challenge. In Proceedings of the Second PAS-
CAL Challenges Workshop on Recognising Textual
Entailment.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In Joaquin Quin?onero Candela, Ido Dagan,
Bernardo Magnini, and Florence d?Alche? Buc, editors,
MLCW, volume 3944 of Lecture Notes in Computer
Science, pages 177?190. Springer.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In COL-
ING ?04: Proceedings of the 20th international con-
ference on Computational Linguistics, page 350, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Daniel Ferre?s, Samir Kanaan, Alicia Ageno, Edgar
Gonza?lez, Horacio Rodr??guez, Mihai Surdeanu, and
Jordi Turmo. 2004. The TALP-QA System for Span-
ish at CLEF 2004: Structural and Hierarchical Relax-
ing of Semantic Constraints. In Carol Peters, Paul
Clough, Julio Gonzalo, Gareth J. F. Jones, Michael
Kluck, and Bernardo Magnini, editors, CLEF, volume
3491 of Lecture Notes in Computer Science, pages
557?568. Springer.
Daniel Ferre?s, Samir Kanaan, Edgar Gonza?lez, Alicia
Ageno, Horacio Rodr??guez, Mihai Surdeanu, and Jordi
Turmo. 2005. TALP-QA System at TREC 2004:
Structural and Hierarchical Relaxation Over Seman-
tic Constraints. In Proceedings of the Text Retrieval
Conference (TREC-2004).
Yoav Freund and Robert E. Schapire. 1996. Experiments
with a new boosting algorithm. In International Con-
ference on Machine Learning, pages 148?156.
Maria Fuentes, Horacio Rodr??guez, Jordi Turmo, and
Daniel Ferre?s. 2006. Femsum at duc 2006: Semantic-
based approach integrated in a flexible eclectic mul-
titask summarizer architecture. In Proceedings of
the Document Understanding Conference 2006 (DUC
2006). HLT-NAACL 2006 Workshop., New York City,
NY, USA, June.
Anselmo Pen?as, ?Alvaro Rodrigo, Valent??n Sama, and Fe-
lisa Verdejo. 2006. Overview of the answer val-
idation exercise 2006. In Working Notes for the
64
CLEF 2006 Workshop. ISBN: 2-912335-23-x, Ali-
cante, Spain, September.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc., New
York, NY, USA.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques, Second
Edition (Morgan Kaufmann Series in Data Manage-
ment Systems). Morgan Kaufmann, June.
Acknowledgments
This work has been supported by the Spanish Re-
search Dept. (TEXT-MESS, TIN2006-15265-C06-
05). Daniel Ferre?s is supported by a UPC-Recerca
grant from Universitat Polite`cnica de Catalunya
(UPC). TALP Research Center is recognized as
a Quality Research Group (2001 SGR 00254) by
DURSI, the Research Department of the Catalan
Government.
65
