GRICE INCORPORATED 
Cooperativity in Spoken Dialogue 
Laila Dybkj~er, Niels Ole Bernsen and Hans Dybkj~er 
Centre for Cognitive Science, Roskilde University 
PO Box 260, DK-4000 Roskilde, Denmark 
emails: laila@cog.ruc.dk, nob@cog.ruc.dk, dybkjaer@cog.ruc.dk 
phone: +45 46 75 77 l 1 fax: +45 46 75 45 02 
Abstract 
The paper presents a consolidated set of princip- 
les of cooperative spoken human-machine dialo- 
gue which have the potential tor being turned 
into practically applicable design guidelines. The 
principles have been validated in three ways. 
They were established fi'om a Wizard of Oz sim- 
ulation corpus used to develop the dialogue 
model for a spoken language dialogue system. 
Developed independently of Gricean theory, 
some of the principles were refined through com- 
parison with Grice's maxims of cooperativity in
conversation. Finally, the principles were tested 
in the user test of the implemented dialogue sys- 
tem. The paper shows that Grice's maxims con- 
stitute a sub-set of the principles. The non- 
Gricean principles and dialogue aspects they in- 
troduce am presented and discussed. 
1 Introduction 
In the last four years, we have designed and imple- 
mented the dialogue component of a spoken language 
dialogue system (SLDS) prototype in the domain of 
flight ticket reservation. The aim has been to develop 
a realistic, application-oriented prototype whose dia- 
logue management allows users to perform their res- 
ervation task in spontaneous and natural spoken lan- 
guage. Being well-structured, the ticket reservation 
task generally lends itself to system-directed dialogue 
in which the user answers questions posed by the 
system. The only user initiative our system permits is 
that users may initiate clarification and repair meta- 
communication through uttering the keywords 
'repeat' and 'change'. In designing such a system, it 
is crucial to reduce the number of situations in which 
users are inclined to take other forms of dialogue 
initiative, such as asking questions when they do not 
understand the system's dialogue behaviour or pro- 
viding information which the system did not ask for 
(Schegloff et al 1977). This is why the issue of dia- 
logue cooperativity came to play a central role in our 
design of the dialogue structure. We needed to opti- 
mist system dialogue cooperativity in order to pre- 
vent situations uch as those described above. To this 
end, we developed a set of general principles to be 
observed in the design of cooperative, spoken human- 
machine dialogue. The principles have been validated 
in three ways. Firstly, they were developed on the 
basis of a simulated human-nmchine dialogue corpus 
collected uring dialogue model design. Secondly, we 
compared the principles with Grice's maxims of co- 
operative human-human dialogue. Thirdly, the prin- 
ciples were tested against he dialogue corpus fi'om 
the user test of the implemented system. 
This paper analyses the relationship between our 
principles and Grice's maxims. We first describe how 
the principles were developed (Section 2). We then 
justify the comparison between principles and max- 
ires (Section 3). Section 4 compares principles and 
maxims. Section 5 briefly describes how the princi- 
ples were tested the on user test dialogue corpus, and 
Section 6 concludes the paper. 
2 Developing and Testing Principles of 
Cooperative Human-Machine Dialogue 
The dialogue model for otu" flight reservation system 
was developed by the Wizard of Oz (WOZ) experi- 
mental prototyping method in which a person simu- 
lates the system to be designed (Fraser and Gilbert 
1991). Development was iterated until the dialogue 
model satisfied the design constraints on, i.a:, average 
user utterance length. The dialogues were recorded, 
transcribed, analysed and used as a basis for ina- 
provements oil the dialogue model. We perlormed 
seven WOZ iterations yielding a transcribed corpus of 
125 task-oriented human-machine dialogues corre- 
sponding to approximately seven hours of spoken 
dialogue. The 94 dialogues that were recorded uring 
the last two WOZ iterations were performed by ex- 
ternal subjects whereas only system designers and 
colleagues had participated in the earlier iterations. A
total of 24 different subjects were involved in the 
seven iterations. Dialogues were based on written 
descriptions of reservation tasks (scenarios). 
A major concern during WOZ was to detect prob- 
lems of user-system interaction. We eventually used 
the following two approaches to systematically dis- 
cover such problems: ( ) prior to each WOZ iteration 
we matched the scenarios to be used against he cur- 
rent dialogue model. The model was represented as a 
graph structure with system phrases in the nodes and 
expected contents of user answers along the edges. If 
a deviation from the graph occurred during the mat- 
ching process, this would indicate a potential dia- 
328 
logue design problem which should be removed, il' 
possible. (ii) The recorded dialogues were plotted 
onto the graph representing the dialogue model. As in 
(i), graph deviations indicated potential dialogue de- 
sign problems. Deviations were marked and their 
causes analysed whereupon the dialogue model was 
revised, if necessary. 
At the end of the WOZ design phase, we began a 
lllOl'e theoretical, forward-looking exercise. All the 
problenis of inleractioii uncovered dr!ring WOZ wore 
analysed and represented asviolations of principles of 
cooperative dialogue. Each problem was considered a 
case in which the system, in addressing the user, iiad 
violated a principle of cooperative dialogue. The 
principles of cooperative dialogue were made ex- 
plicit, based on the problems analysis. The WOZ cor- 
pus analysis led to the identification of 14 principles 
el: cooperative hun3an-machine dialogue (Section 4) 
based on analysis o1' 120 examples o1: user-systeill 
interaction problems. \[1: the principles were observed 
in the design of the system's dialogue behaviour, we 
assunled, this would serve to reduce the occurrence of 
user diah)gue behaviour that lhe sysiem had not been 
designed to handle. 
3 Maxims and Principles of Cooperative 
Dialogue 
We had developed our principles of cooperative hu- 
nlan-nmchine dialogue indel)endently o1" Gricean co-, 
operativity theory (Bernsen et al, 1996a). \])rior to the 
user test (Section 5), we colni)ared the principles with 
(h'ice's Cooperative Principle and maxims. In lhis 
process Ihe principles achieved their current lornl as 
shown in Table 1. Their original expression is pre- 
sented in Section 4. Grice's Cooperative Principle 
(CP) is a general principle which says thai, to act 
cooperatively in conversation, oilc should make one's 
"conversational contribution such as is required, at 
the stage at which it occtlrs, by tile accepted purpose 
or direction of the talk exchange in which one is en- 
gaged" (Grice 1975). Grice proposes lhat the CP can 
be explicated in terms of four groups of simple max- 
ires which are not claimed to be jointly exhaustive. 
The maxims are marked with an asterisk in Table 1. 
Grice focuses on dialogues in which the interloc- 
utors want to achieve a shared goal (Grandy 1989, 
Sarangi and Slembrouck 1992). In such dialogues, lie 
claims, adherence to the maxims is rational because it
enstu'es Ihal the interlocutors pursue the shared goal 
most efl'iciently. Task-oriented ialogue, such as that 
of our SLDS, is a paradigm case of shared-goal dia- 
logue. Grico, however, did not develop the inaxinls 
with the purpose of preveuthlg coinmunication failure 
in shared-goal dialogue. Rather, his interest lies iri the 
inl~rences which an interlocutor is able to make when 
the speaker deliberately violates oue of the maxims. 
I\[te calls such deliberate speaker's messages 'conver- 
sational implicatures'. Grice's maxims, although 
having been conceived for a dilTerent purpose, never- 
theless erve the same objective as do ()tit" p,inciples, 
namely that of achieving the dialogue goal as directly 
and smoothly as possible, e.g. by preventing ques- 
tions of claril\]cation. It is exactly when a hunian or, 
for that inatler, an SLDS, non-deliberately viohttes a 
maxim, that dialogue clarification problems are likely 
to occur. Thus, the main dil:ference between Grice's 
work and ours is that the maxims were developed to 
account for cooperalivity in hUlllall-hUlnal/ dialogue, 
whereas our principles were developed to aceoulll \['or 
cooperativity in hunlan-nmchhle dialogue. Givca the 
conllnonalily of purpose, it beconies of interest to 
conlpare principles and Illaxinls. We waut to show 
that the principles include the illaXilllS as a subset end 
thus provides a corpus-based confirmation of their 
validity for spoken human-machine dialogue. More(> 
vet', the principles manifest aspects of cooperative 
task-oriented dialogue which were not addressed by 
Grice. 
4 Comparison between Maxims and 
Principles 
In this section we analyse the relationship between 
Grice's nmxiins and our principles el: dialogue coop- 
erativity. A \[irst aim is to demonstrate hat a sub-set 
of the principles are roughly equivalent to tile ntax- 
hns. We Ihen argue that the renlaining principles ex- 
press additional aspects of cooperativity. The dislinc- 
lion between I, ri,lcQ)h" andaxlWCt (Table l) is theo- 
retically intportant because an aspect represents tile 
l)roperty o1' dialogue addressed by a particular maxim 
or prhlciple. One result of analysing Ihe rehltionship 
between principles and nlaxhns is the distinction, 
shown in the tables, 1)elween ?,eneric and specific 
principles. Grice's ulaxims are all generic. A generic 
principle lnay subsunle one or ltlore specific princi- 
ples which specialise the generic principle to certain 
classes of phenomena. Although important to SIA)S 
design, specific principles may be less signil\]cant to a 
general account of dialogue cooperativity. 
4.1 Pr inc ip les wh ich  are Reduc ib le  to Max ims  
Grice's nmxims of truth and evidence (GP3, (7I)4) 
have no coui/terparts aniong ()ttr t~,'inciples but inay 
simply be inchided among the principles. The reason 
is that one does not design an SLDS in the domain o1' 
air ticket reservation which provides l:alse or un 
founded information to cuslomers. In other words, the 
maxims of truth and evidence are so important to the 
design o1: SLI)Ss that they are unlikely to emerge 
duriug dialogue design problenl-solving. During sys- 
toni inll)lenlenlalion , one constantly worries about 
truth and evidence. It canuoi be allowed, for instance, 
that the system confirrns infornlatioll which has ilOl 
been checked with the database and which might be 
false or impossible. Grice (1975) observed the i:un- 
danlental nature of the maxims of truth and evidence 
in general and GP3 in particuhir (of. Searle 1992). 
329  
Table 1. The generic and specific principles of cooperativity in dialogue. The generic principles are expressed at the same 
level of generality as are the Gricean maxims (marked with an *). Each specific principle is subsumed by a generic principle. 
The left-hand column characterises the aspect of dialogue addressed by each principle. 
Dialogue Aspect 
Group 1: lnfl}rmativeness 
Group 2: 
Truth and evidence 
Group 3: 
Relewmce 
Group 4: 
Manner 
Group 5: 
Partner asymmetry 
Group 6: 
' Background knowledge 
Group 7: 
~ P, epair and clarification 
GPno. SPno. 
GP1 
GPI SP1 
GPI SP2 
GP2 
GP3 
GP4 
GP5 
GP6 
GP7 
GP7 SP3 
GP8 
GP9 
GP I 0 
GPI0 SP4 
GPI0 SP5 
GP11 
GP 11 SP6 
GPII SP7 
GPI2 
GPI2 SP8 
GPI3 
GP 13 ~P9 
GP13 SPI0 
GPI3 SPll  
Generic or Specific Principle 
i 
*Make your contribution as informative as is required (for the current pur- 
poses of the exchange). 
Be fully explicit in communicating to users the commitments they have 
made. 
, , .  
Provide feedback on each piece of information provided b~? the user. 
*Do not make ~our contribution more infornmtive than is required. 
*Do not say what you believe to be false. 
*Do not say that for which you lack adequate evidence. 
*Be relevant, i.e. Be appropriate othe immediate needs tit each stage of the 
transaction. 
*Avoid obscurity of expressio{I.. 
*Avoid ambiguity:.. 
! Provide same formulation of the same question (or addre~)'to users every-- 
where in the s},steln's dialo~uc, turns. 
*Be brief (avoid unnecessary/t~rolixit~/). .~. 
* Be orderly. 
Inform the dialogue partners of important non-normal ch?racteristics which 
they should take into account in order to behave cooperatively in diak)gue. 
Provide clear and comprehensible communication f what the system can 
and cannot do. 
I Provide clear and sttfficient instructions tousers on how to interact with the 
system. 
'Fake partners' relevant b.ack~round knowledge into account. 
Take into account possible (and possibly emmeous) user inferences by anal- 
ogy from related task domains. 
Separate whenever possihle between tire needs of novice and expert users 
(user-adaptive dialogue). 
Take into account legitimate partner expectalions a to your own background 
k n o w l c d s e . . . .  ... 
Provide sufficient task domain knowledge and inference. 
Initiate repair or clarification l cta-connnunication in case of comlntlnication 
l'ailure. 
Provide ability to initiate repair it' s~/stem understandin~ has failed. 
Initiate clarification recta-communication in case of inconsistent user input. 
hfitiate clarification recta-communication in case of ambifzuous user input. 
The following principles have counterparts among the 
maxims: 
1. Avoid 'semantical noise' in addressing users. 
(1) is a generalised version of GP6 (non-obscurity) 
and GP7 (non-ambiguity). Its infelicitous expression 
was due to the fact that we wanted to cover observed 
ambiguity and related phenomena in one principle but 
failed to find an appropriate technical term for the 
purpose. (I) may, without any consequence other than 
improved clarity, be replaced by GP6 and GP7. 
2. Avoid superfluous or redundant interactions 
with users (relative to their contextual needs). 
(2) is virtually equivalent o GP2 (do not overdo in- 
lormativeness) and GP5 (relevance). Grice observed 
the overlap between GP2 and GP5 (Grice 1975). (2) 
may, without any consequence other than improved 
clarity, be replaced by GP2 and GP5. 
3. It should be possible for users to fully exploil 
the system's task domain knowledge when they 
need it. 
(3) can be considered an application o1' GPI ( in fer  
mativeness) and GP9 (orderliness), as follows. If the 
system adheres to GPI and GP9, there is a maximum 
likelihood that users obtain the task domain knowl- 
edge they need from the syslem when they need it. 
Tbe system should say enough and address the task- 
relevant dialogue topics in an order which is as close 
as possible to the order expected hy users. If  the user 
expects ome topic to come tip early in the dialogue, 
that topic's non-occurrence at its expected "place" 
may cause a clarif ication sub<lialogue which the 
330 
systeln c;_innc)t understand. In WOZ Iteration 3, for 
instance, the system did not ask users about their in- 
terest in discount fare. Having expected the topic to 
come tip for seine time, users therefore began to in- 
quire about discount when approaching lhe end of the 
reservalion dialogue. (3) may be replaced by GP 1 and 
GP9 without significant loss. 
4. P, educe system lalk as nnich as possible during 
individual dialogue turns. 
(4) is near-equivalent to GP8 (brevity). 
Sunlmarising, the generic principles (1)-(4) may 
he replaced by maxims GPI ,  GP2 and GP5-GP9. 
These maxilns are capable of perforii/illg the same 
task in guiding dialogue design. In fact, as argtled, the 
maxims ai'o able to do the better job because they, i.e. 
GP6 and (IP7, aild GPI and GP9, respectively, spell 
Otll the illleilded coilteiits ill" two of Ihe princilfles. 
This provides COl'ptis-l)ased Coilfiiilllit\[Oil I)\[" lilUXililS 
GPI ,  (1152 and CII:'5-(;P9, i.e. of ttleir staling basic 
principles of cooperative, task-oriented hulliall-illa- 
chine dialogue. However, \[or dialogue design ptirpo- 
SOS, lhe illaXill/S illtlSt be augnlenled hy task-slJecl:/Tc 
or domain-sl)ecific princOHes, such as the \[bllowing. 
5 (SP3). Provide same fornluhition of the same 
qucslion (or address) to users everywhere in the 
system's dialogue ttlrlls. 
(5) represents an additional lWCCaUtion against the 
occurrence of ambigttity in niachine speech. It can be 
soeii as a Sl)ccial-purpose application o1' GP'/ (ilon- 
aulbiguity). 
6 (SPI). t~;e fully explicil in et)lllnlunicating to tlS- 
el'S the COillillitlllents they have Illade, 
7 (SP2). Provide feedback Oil e~lch piece of infor- 
Ination provided by lhe riser. 
Those principles are closely related. The novel coot> 
orativity aspoel they introduce is lhal they require the 
cooperative speaker to produce a specific dialogue 
contl'il~ution which explicitly expresses an intorprola- 
lion of the intorlocuior's previous diah)guo conlribu- 
lion(s), provided Ihal the interlocutor has inado ;.l 
dialogue contribution of a certain lypo, such as a 
coninlitnlonl Io book a flight. We propose ihal these 
principles be suhsunlod by (i l '  1 (infornialiveness). 
4.2 Prindples hicldng Equivalenls lililOlil4 the 
Maxims 
The principles discussed in this section appear irre- 
ducible to maxims and thus serve to augment the 
scope of a theory of cooperativity. 
4.2.1 Dialogue Partner  Asymmetry  
Dialogue partner asynimctry occurs, roughly, when 
OllO or liloi'e of Iho dialogue partners is llot in a nor- 
lllal conditioll or situation, leer installCO, a dialogue 
partner may have a hoalitlg deficiency or be located 
in a particularly noisy environnlcnt, in such cases, 
dialogue cooperativity depends Oll the taking into 
accotlnt o\[' that participant's pecial characteristics. 
For obvious reasons, dialogue partner asynmietry is 
important in SI,DS dialogue design. The machine is 
not a nornml dialogue partner and users have to be 
aware of this if communical ion faihire is to be 
avoided. The following two principles address dia- 
logue parlnor asynllnelry: 
8 (SP4). Provide clear and comprehensible com- 
Inunication ?)1: what the system can and cannot do. 
9 (SP5). I'rovide clear and sufficient instructions 
Io users oil hOW \[() interact with tile system. 
Being limitcd in its task capabilities and intended for 
walk-up-and-use application, our SLDS needs to pro- 
tect itself from unmanageahlc dialogue contributions 
by providing users with an upq'ront mental model of 
what it can and cannot do. If this inclmtl model is too 
complex, uscrs will not acquire it; and il' the nlodcl is 
too situplistic> its remaining details must be provided 
elsewhere during dialogue. (8) adds an ilnportant 
clement to Ihc analysis of dialogue coopcrativily by 
aiming at inlproving user coopcrativily. It shows that, 
at least in hunlan-nlachinc dialogue, coopcraiivity is a 
fornmlly nlorc coniplcx pheuonlcnon than anticipated 
by Gricc. In addition to principles stating how a 
speaker should hehavc, principles are needed tic- 
cording to which the speaker should consider trans- 
ferring part of the responsibility for cooperation to the 
interlocutor. (9) has a role sitnihu" to lhat of (8). 
The lnincil)lcs cxanlincd in this section inlroducc 
a new aspect o1 dialogue cooperativity, naniely part- 
her asymmetry and speaker's consequent obligation 
to inform the partner(s) of non-nornml speaker char- 
acteristics, l)ue to lhe latter, the principles cannot be 
subsumed by any olhcr l)rinciplc or maxim. We pro- 
pose Ihat (8) and (9) are both .vl~eci/k' princilJcs sub- 
sumed by a new generic pri,lcilfle: 
GPI0. hlfortn the dialogue parttlors of inll)Ot'tant 
ilOll-il(ll'lllal charactcrislics which they should take 
into accotilll in order tt) behave cooperatively in 
dialogue. 
4.2.2 llackgrotmd Knowledge 
10 (GP I I ) .  Take users' relevant background 
knowledge into account. 
(;1511 is expressed at the level of generality of (h'icc's 
theory. The principle explicitly introduces two no- 
tions: the notion o1' interlocutors' background knowl- 
edge and that of possible dilTcrcnccs in background 
knowledge between dilTercnt user populations and 
individual users. (;P1 I appears to be presupposed by 
maxinm GPI,  GP2 and G155-(;1'9 in the sense that it is 
not possible to adhc,'e to any of Ihese maxims without 
adhering to GP I I .  Moreover, in order to adhere to 
GPI I, it is necessary lkn" the speaker to recognise 
relevant differences among inlerlocutors and inter- 
locutor groups in Icrms o1' background knowledge. 
Based on this recognition, a speaker either ah'cady 
has built prior to the dialogue, or adaptively buikts 
during dialogue, a model o1' the interlocutor which 
serves to guide speaker coopcrativily. Increased user 
331 
adaptivity in this sense is an important goal in SLDS 
design (Bernsen et al 1994). 
GPI l cannot be reduced to GPI (informativeness) 
because, first, GP1 does not refer to the notions of 
background knowledge and differences in back- 
ground knowledge among interlocutors. Second, a 
speaker may adhere perfectly to 'exchange purpose' 
(cf. GPI)  while ignoring the interlocutor's back- 
ground knowledge. For instance, in the user test a 
user wanted to order a one-way ticket at discount 
price. The system, however, knew that discount is 
only possible on return tickets. It therefore did not 
offer the discount option to this user nor did it correct 
the user's misunderstanding. At the end of the dia- 
logue, the frustrated user asked whether or not dis- 
count had been granted. Third, as argued above, 
GPl l  is presupposed by maxims GPI, GP2 and GP5- 
GP9. Grice, however, does not argue that GP1 is pre- 
supposed by those maxims whereas he does argue 
that GP3 (truth) and GP4 (evidence) are presupposed 
by them (Grice 1975). For similar reasons, GP5 
(relevance) (Sperber and Wilson 1987), cannot re- 
place GPI 1. Informativeness and relewmce, there- 
fore, are not only functions of the purpose(s) of the 
exchange of infornmtion but also of the knowledge of 
the interlocutor. 
11 (SP8). Provide sufficient ask domain knowl- 
edge and inference. 
(11) may appear trivial as supportive of the design of 
usable information service systems. However, desig- 
ners of such systems are continuously confronted 
with questions about what the system should know 
and what is just within, or barely outside, the sys- 
tem's intended or expected onmin of expertise. The 
system should behave as a perfect expert vis-~>vis its 
users within its declared domain of expertise, other- 
wise it is at fault. In WOZ Iteration 7, for instance, a
subject expressed surprise at not having been offered 
the option of being put on a waiting list in a case in 
which a flight was already fully booked. We became 
aware of the problem during the post-experimental 
interview. However, the subject might just as well 
have asked a question during the dialogue. Since ( 11 ) 
deals with speaker's knowledge, it cannot be sub- 
smncd by GPl l .  We therefore propose to introduce a
new generic principle which mirrors GP11: 
GPI2. Take into account legitimate partner expec- 
tations as to your own background knowledge. 
(11), then, is a specific principle subsumed by GPI2. 
12 (SP6). Take into account possible (and possi- 
bly erroneous) user inferences by analogy fi'om 
related task domains. 
(12) is a specific principle subsumed by GP1 l (back- 
ground knowledge). It was developed from examples 
of user mistmderstandings of the system due to rea- 
soning by analogy. For instance, the fact that it is 
possible to make reservations of stand-by tickets on 
international ilights may lead users to conclude (erro- 
neously) that this is also possible on domestic lqights. 
13 (SP7). Separate whenever possible between the 
needs of novice and expert users (user-adaptive 
dialogue). 
(13) is another specifi'c principle subsumed by GPI I .  
Interlocutors may belong to different populations 
with correspondingly different needs of information 
in cooperative dialogue. For instance, a user who has 
successftdly used the dialogue system on several oc- 
casions no longer needs to be introduced to the sys- 
tem but is capable of launching on the ticket reserva- 
tion task right away. A novice user, however, will 
need to listen to the system's introdnction to itself. 
This distinction between the needs of expert and 
novice users was introduced in WOZ Iteration 7 when 
several users had complained that the system talked 
too much. 
4.2.3 Meta-eommunication 
Even if an SLDS is able to conduct a perfectly coop- 
erative dialogue, it will need to initiate repair and 
clarification meta-communication whenever it has 
failed to understand the user, for instance because of 
speech recognition or language understanding failure: 
14 (SP9). Provide ability to initiate repair il' sys- 
tem understanding has failed. 
(14) states what the cooperative speaker should do in 
case of failure to understand utterances made by the 
interlocutor. Our system adheres to (14) in that it 
communicates it failure to understand what the user 
just said. (14) cannot be subsumed by GPI (infor- 
mativeness) which ignores communication faihne. 
Together with the new specific principles from the 
user test SPI0 and SP11 (Section 2), (14) is a specific 
principle of htmmn-machine dialogue which may be 
subsumed by: 
OPl 3. Initiate repair or clarification meta-commu- 
nicatkm in case of communication failure. 
5. The user test 
The set of principles presented in Table 1, apart fi'om 
SP10 and SPII  (see below), were tested against he 
nser test dialogue corpus (Dybkjmr et al, 1996). In 
the user test the speech recogniser was simulated 
whereas the other system components were the imple- 
mented ones, The system was tested with 12 external 
users. The 57 recorded and transcribed ialognes 
were scenario-based and covered the full functional- 
ity of the system. We systematically analysed the 
transcribed ialogues to detect those deviations be- 
tween expected and actual user utterances that would 
signal problems of user-system interaction caused by 
non-cooperative dialogue design. An example is pre- 
sented below of a piece of dialogue fi'om the user test 
in which two system cooperativity problems occur (S 
means ystem and U means user): 
S 1 : Do you want retnrn tickets'? 
UI: Yes please. 
$2: Are you particularly interested in discotmt? 
U2: Yes. 
332 
$3: On which date will the journey start? 
U3: Monday. 
$4: Monday January 16. At which tin\]e? 
U4: 7:20. 
$5: There is no departure at 7:20. The closesl 
other red departure is at 10:50. Do you want this 
deparlure? 
US: No thanks. 
Each identified problem in the dialogue corpus was 
categorised according to which principle had been 
violated and described in terms of the sympton\] (S), a 
diagnosis (D) and a cure (C). Below is shown the 
description for the two problems in $5. 
GPI :  Make your contrihution as inl'ormalive as is 
required (for the current purl\]oses o1 the exchan- 
ge). 
S: U: interested in discount (red) + out departure 
time at 7:20. S: no departure at 7:20. 
D: The system provides insufficient informalion. 
It does not tell that lhere is a blue departure at 
7:20. 
C: The system should provide sufficient infornta- 
lion, e.g. by telling that there is no red departure 
bul lhal lhere is a bh,e del)arture at the chosen 
hour. 
SPIl l: Initiate clarification recta-communication 
in case of inconsistent user input. 
S: U: interested in discount (red) + out departure 
time at 7:20; S: no departure at 7:20. However, 
7:20 does exist but wilhout discount. 
1): S gives priority to discount over time without 
proper reason. 
C: S should ask U about priority: 7:20 is not a 
discount departure. Red discount can be oblained 
on the departures at x, y and z. Which departure 
do you want. \ [ f lU provides a new departure time: 
S: do you still want discount. If U: no; S: non- 
discount departures\[. 
It turned out that ahnost all of Ihe 86 system dialogue 
problems identified could be ascribed to violations of 
the cooperative principles (Bernsen et al, 1996b). We 
only had to add two specific f~rinciples o1' meta- 
contmunication (SPI0 and SPI I  in Tahle 1). Since 
naeta-comnmnication had not been simulated uring 
the WOZ experiments, this came as no SLUT)rise. The 
lollowing GPs and SPs were found violated at least 
once: GPs I, 3, 5, 6, 7, 10, I I, 12, 13 and SPs 2, 4, 5, 
6,8,  10, I I .  
The user lest confirmed the broad coverage of the 
principles with respect o cooperative, spoken user- 
system dialogue. Less flattering, of course, the test 
thereby revealed several deficiencies in our coopera-- 
live dialogue design. 
6 Conclusion 
Comparison between our principles and (;rice's max- 
ims has shown that there are more generic principles 
of cooperativity it\] human-machine dialogue than 
those identified by Grice. Three groups of principles 
reveal aspecls of cooperative dialogue left unaddres- 
sed hy the maxims. This produces a lotal o1' sevcn 
dialogue aspects, each of which is addressed by one 
or more generic principles (Table 1). Some generic 
principles subsume specific principles. It may be 
asked why Gfice was not aware of the Ihree generic 
aspects of dialogue partner asymmetry, background 
knowledge and recta-communication. It seems obvi- 
ous that it cannot he because Ihese aspects arc absent 
from human-huntan spoken dialogue. More plausibly, 
dialogue partner asymmelry is ahsent from prototypi- 
cal cases of human-hunmn dialogue; background 
knowledge is so perwmive as lo he easily ignored; and 
Grice explicitly was not concerned with dialogue fail- 
ure pure and simple. 
The results from the comparison will\] Grice's tllaXilllS 
and from the user test suggest lhat the principles of 
cooperative spoken human-machine dialogue lllay 
represent a step towards a IllOle or less complete \ ] l id 
practically applicable set of design gtfidelines for 
cooperative SIJ)S dialogue. 
References 
I~ernsen, N.O., l)ybkj:er, 11. and l)ybkj;er, I,. (1996a). 
"Coopcralivily in \[hmmn-Machinc and lluman-lltmmn 
Spoken Dialogue," Discour~'e Pr<)cesses, 21,2, 213-236. 
Bcrnsen, N.O., I)ybkfier, H. and Dybkj~er, I~. (1996b). 
"l~rillciples for the Design of Cooperative Spoken l lure\]n- 
Machine Dialogue." To appear in Proceedings of 1CSLP 
'96, l~hiladcll~hia, October. 
t~crnscn, N.O., l)ybkj~er, L. and l)yhkj~er, II. (1994). '% 
dedicated task-oriented ialogue theory in support of spo- 
ken language dialogue systems design." Proceedings oj 
ICSLP '94, Yokohama, Seplcmbcr, 875-878. 
Dybkjmr, L., Bcrnsen, N.O. and Dybkj~er, 11. (1996). 
"l{wllualion of Spoken l)ialogucs. User Test wilh a Simu- 
lated Speech Rccogniscr." Report 9bj}'om the Danish l'r@ 
eel in &)okel~ l,a/~guage Dialogue Syxlems. Roskildc Uni- 
versity, l;ebmary. 
f:rasef, N.M. and Gilbert, G.N. ( 1991 ). "Simulating speech 
systems." ~'ompuler Speech aml Language 5, 81-99. 
Grandy, P,.E. (1989). "On Grice on language." The Journal 
ojlVlilosol)t O, 86, 10, 514-25. 
(;rice, P. (1975). "lx~gic and conversation." In P. Cole & 
J.L. Morgan (Eds.), Synta.r aml semal~lics Vol. 3." Si)eech 
acts (41-58). New York: Academic Press. Reprinted in 
Gricc, P.: Studies in the way of words. Cambridge, MA: 
I lmward University Press 1989. 
Sarangi, A.K .  and Slcnlbrot.lck, S. (1992). "Non- 
cooperation i communication: A reassessmenl of Gficcan 
pragmatics." ,Iourmd q/Pragmatics 17, I 17-154. 
Sclmgloff, E.A., Jefferson, G. and Sacks, 11. (1977). "The 
preference for self-correction i the organization of repair 
in conversation." lzmguage 53,361-82. 
Searlc, J.R. (1992). "Conversation." In Searle, ,I.R. et al 
(l:,ds.), (On) Searle (m com,ers'ation. Amsterdam: John 
llenjamin's I~ul~lishing Company. 
Sperber, D. and Wilson, 1). (1987). "Prdcis of felevance, 
communication a d cognition with open peer commentary." 
Behavioral aml Brain Sciences 10, 4, 697-754. 
333  
