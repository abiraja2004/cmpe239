Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 2?11,
Columbus, Ohio, USA June 2008. c?2008 Association for Computational Linguistics
Bayesian Learning Over Conflicting Data:
Predictions for language change
Rebecca Morley
Cognitive Science Department
Johns Hopkins University
3400 N. Charles St.
Baltimore, MD 21218
morley@cogsci.jhu.edu
Abstract
This paper is an analysis of the claim that a
universal ban on certain (?anti-markedness?)
grammars is necessary in order to explain their
non-occurrence in the languages of the world.
To assess the validity of this hypothesis I ex-
amine the implications of one sound change (a
> ?) for learning in a specific phonological
domain (stress assignment), making explicit
assumptions about the type of data that results,
and the learning function that computes over
that data.  The preliminary conclusion is that
restrictions on possible end-point languages
are unneeded, and that the most likely outcome
of change is a lexicon that is inconsistent with
respect to a single generating rule.
1 Introduction
The basic tenet of Evolutionary Phonology is that
the observed universal commonalities in
phonological systems of the world arise from the
universal commonality of the way listeners and
speakers produce and perceive sound structures
(Blevins, 2004). Diachronic processes operating
via the transmission of the speech signal act with-
out regard for the subsequent system they create.
Alternate theories in the tradition of Chomsky ar-
gue for universal prohibitions which would serve
to ban or repair certain changes just in case they
would result in a ?disallowed? system (Kiparsky
2004, 2006).  In Optimality Theoretic terms, this
would be a grammar that violates the canonical set
of universal markedness constraints. I will call this
claim the Universal-Grammar-Delimited Hypothe-
sis  Space (UG-Delimited H) Principle.
Without this check, Kiparsky argues, common
and natural sound changes (?blind? Evolutionary
Phonology) would frequently produce unnatural
and in fact unobserved ?anti-markedness? lan-
guages (such as a system in which lower sonority
vowels were stressed in preference to higher so-
nority vowels).
An analysis of the properties of possible
grammars is an analysis that involves explicitly
characterizing the properties of the learner, as well
as of the data to which the learner is exposed.  The
work in this paper is, to my knowledge, the first
attempt to do exactly this kind of analysis, for ex-
actly the type of scenario in which a dispreferred,
but hypothetically learnable, grammar might arise.
Diachronic changes that are caused by factors
outside of the grammar have the capability of dis-
rupting a categorical rule system, introducing ir-
regularities into a previously regular pattern. These
irregularities may have an ?unnatural? or anti-
markedness character, but typically, they will co-
exist alongside remnants of the prior natural pat-
tern.  That is the first observation.  The second is
that if learners are allowed to adopt mixed-
grammar hypotheses (?co-phonologies? (Inkelas
1997), ?stratal faithfulness? (Ito and Mester 2001),
?lexical indexation? (Pater 2000)), then under a
posterior-maximizing learning model, these hybrid
systems are the most likely outcome (rather than a
categorical ?anti-markedness? grammar).
I will work through a case study of sonority
sensitive stress, paying special attention to the
lexicon that would be produced after a hypothetical
sound change of the type Kiparsky proposes.  By
examining the output of Bayesian hypothesis test-
ing in this domain I will conclude that for the pure
anti-markedness grammar to arise, not only is a
2
certain type of diachronic change necessary, but
also a certain type of non-uniform lexical distribu-
tion. To first approximation, this confluence of
circumstances appears rather rare, leading me to
tentatively reject the hypothesis that categorical
bans on allowed grammars are necessary to explain
the distribution of the world?s languages.
2 Gujarati Phonology
Kiparsky uses Gujarati to provide a concrete illus-
tration of the relevant phonological paradigm: a
sonority-sensitive stress system that respects the
posited universal implicational hierarchy.  There
are eight vowels in Gujarati, corresponding to three
sonority tiers: low: (?), mid: (i,e,?,o,?,u), and high:
(a).  The stress system is described as conforming
to the following position- and sonority- dependent
rules.
[1]   GUJARATI: Sonority & Position -to-Stress
? stress penultimate [a] (the most sonorous vowel)
? otherwise stress ante-penultimate [a]
? otherwise stress final [a]
? otherwise stress penultimate mid-sonority vowel
(any of [i,e,?,o,?,u])
? otherwise stress ante-penultimate mid-sonority
vowel
? otherwise stress the penultimate position (which
must be [?] (the lowest sonority vowel))
This type of system is easily describable within
a standard OT framework (Prince and Smolensky
1993/2004) that utilizes a universally ordered so-
nority scale with respect to the markedness of (or
dispreference for) stressing a particular vowel.
Crucially, however, the reverse type of system, in
which lower sonority vowels are the ones that at-
tract stress, is so far unattested, and predicted,
within the same framework, to be impossible.
2.1 Gujarati?
In stating his claim about the necessity of intrinsic
bans on possible grammars, Kiparsky makes the
following assumption: A common and natural type
of sound change is one in which all a?s of a lan-
guage change to ??s
 1
.  I will adopt this assumption
                                                 
1
 In fact, it is not clear how likely an internally motivated la n-
guage change of a completely general nature is.  What might
as well for the sake of argument, leaving aside a
discussion of the evidence for how plausible it may
be.  It should be kept in mind that this particular
change is being considered only as a stand-in for a
class of possible sound changes that could produce
similar outcomes with respect to markedness im-
plications.
A change in vowel quality (with unchanged
stress placement) will alter the make-up of the
Gujarati lexicon, and raise the possibility of a sys-
tem in which stress preferentially falls on the low-
est-sonority vowel, [?] (formerly [a], the most so-
norous vowel)
2
. This new lexicon will, in turn, act
as the input to the learner of Gujarati?. To deter-
mine the outcome of learning over this data set,
some sort of characterization of the learner?s hy-
pothesis space is necessary.  The list in [2] repre-
sents the full hypothesis set considered in this pa-
per
3
.  To begin, I will consider only hypotheses 1)-
3), leaving aside the discussion of hypotheses 4)
and 5) until Section 3.3.
[2]  H :Hypothesis Space
1) PENULT: Stress Penultimate Vowel
2) GUJARATI: Sonority & Position  -to-Stress
3) GUJARATI*: Reversed-Sonority & Position -to-
Stress
4
4) NULL(G*/G): GUJARATI* and G UJARATI equally
likely generators of data
5) MAX(G*/G): mixed-grammar of GUJARATI* and
GUJARATI with variable weights
                                                                             
be more plausible is that such changes would depend very
heavily on context, with tokens that were less fully realized
(e.g., shorter) being more likely to undergo the change than
more fully /a/-like tokens.  This, of course, would be corre-
lated with their stress status.
2
 An alternative traditional generativist account, rather than
admitting an anti-markedness hypothesis, might propose a
difference between stress-attracting ??s and non-stress-
attracting ??s based on differences in their underlying repre-
sentations, effectively encoding the diachronic change within
the synchronic grammar.  This type of analytic bias will im-
pede or prevent changes from affecting the rule system
(grammar) of a language, and thus it is  not pursued in the
present work.
3
 This is clearly far from the only way in which the learning
problem can be formulated.  Given that this is, to my knowl-
edge, the first study of its kind, a number of somewhat arbi-
trary representational decisions had to be made.  For the pur-
poses of this work the given H-space is the result of  what I
view as a minimal departure from the standard formalisms
both of  linguistic theory and Bayesian learning.
4
 As in [1], but with the sonority classes reversed.
3
2.2 Evidence to the Learner: Gujarati Lexicon
The hypothetical lexicon of Gujarati? (L?) depends
on the inventory of the old Gujarati (L ).  For a
given possible Gujarati, L is mapped to L? via the
sound change a > ?. To construct the space of L I
start by making a list of all possible word types,
where the type depends on features that are rele-
vant to the hypotheses under consideration, namely
the vowel identities.  This listing also corresponds
to a particular lexicon L
MU
 ?  L; this is the word
inventory under what I will call the Minimal Lexi-
con Uniformity assumption: that all types are rep-
resented in equal numbers, and each type occurs
exactly once. For 3-syllable words and an 8 vowel
inventory, there are 8
3
, or 512 distinct types.  For
2-syllable words, there are 8
2
, or 64 types.
Table 1 lists the word types for 3-syllable
words.  ?Case? refers to the type (vowel make-up)
of the word before the hypothetical sound change
(where M indicates any of the mid-sonority vowel
class {i,e,?,o,?,u}).  We will restrict ourselves for
the moment to considering only the first three hy-
potheses in the space: PENULT(P ), GUJARATI(G),
and GUJARATI*(G*)).
Case Gujarati Example
L > L?
# types
H
1
(?,(?,M),a)
(M,?,a)
(a,?,M)
(a,?,(?,a))
[p??ik?a?]>[p?rik???]
21
2
(M,M,a)
(a,M,(M,a,?))
[ho?ija??]>[ho?ij??r]
84
G*
3
(M,a,(?,M,a))
[muba???k]>[mub????k]
48
G*,P
4
((?,M), M,?)
(?,M,M)
[t?um?o?t??]>[t?um?o?t??]
78
G,P
5
(M,?,M)
(M,?,?)
[ko?j?ldi]>[ko?j?ldi]
42
G
6
(a,a,(a,M))
(?,(a,?),(a,?,M))
(M,M,M)
[aw?a?na?]>[?w???n??]
239
G,G*,P
Table 1. Uniform Gujarati Lexicon: three-syllable
words (words taken from de Lacy (2006))
Each row represents positive evidence for
some subset of the three hypotheses under consid-
eration; the hypotheses consistent with a given
case are specified in the last column below the type
counts. For example, in Row 3, the word
[muba ???k] in Gujarati, with stress determined by
the markedness-abiding grammar described in [1]
has become [mub????k] in Gujarati?.  This form
now exhibits stress on the lowest (rather than the
highest) sonority vowel in the word.  This pattern
is consistent with the anti-markedness grammar
GUJARATI*.  However, the stress placement in this
word is also consistent with the simple positional
grammar PENULT.  If we indicate the number of
types that support none of the hypotheses as A
(=arbitrary), and the number that support all hy-
potheses as N (= neutral), then we can calculate the
total type counts in support of each hypothesis
(A=21; G*=371; G=359; N=239; P=365;
T=512).  Note that G*  exceeds P by six word
types.
3 The Bayesian Learner
The numbers in Table 1 represent the make-up of a
possible lexicon of Gujarati?, namely, L
MU
?.  This
will act as the initial input to our Bayesian learner
(for simplicity, all calculations in this section will
be performed only for 3-syllable words).
The Bayesian model has been extensively ap-
plied to learning scenarios in a number of cognitive
domains (e.g., Chater et al, 2006; Kemp et al,
2007; Kording and Wolpert, 2006; Tenenbaum et
al., 2007), and involves a fairly minimal and intui-
tive apparatus.  Bayes theorem, which provides a
formula for computing the posterior probability of
a hypothesis given the data, and thus a method for
evaluating competing grammars, is given in (1).
? 
p(h | d) =
p(d | h)p(h)
p(d )
          (1)
For the problem at hand, the members of d are
stress assignments corresponding to each of the n
words of the lexicon. The conditional probability
of a stress assignment d
i
 under hypothesis h is
more properly written as p(d
i
|h,y
i
), where stress
assignment (as can be seen from Table 1) depends
on the particular word type y
i
. I will assume that
the conditional probability of each surface stressed
form is independent of any other.  The probability
of the set d given h and y (where h = GUJARATI*,
PENULT, or GUJARATI) can then be expanded as the
product of the probability of each member of d
4
given h and each member of  y (see Equation (2)).
3.1 ?Non-Deterministic? Hypothesis Space
Applying Bayes Theorem to the first three hy-
potheses of [2] returns a value of p(h|d)=0 for each
grammar.  To avoid this collapse (due to the exis-
tence of contradictory data), let us assign a small
probability of error (2?) under each hypothesis.
For a given 3-syllable word type, y, there are three
stress possibilities: C = {1,2,3}, and the stress class
assigned by a given hypothesis H
i
 is written as a
function of the input word type: H
i
(y) ? C.  For the
Non-Deterministic version of the same hypothesis,
written as H
i
?
, stress will be assigned to the con-
sistent position (c=H
i
(y)) with probability 1-2?,
and to either of the two inconsistent positions with
probability ?. See [3].
[3] H
i
?
 : Non-Deterministic Version of H
i
? 
p(c |H
i
?
,y) =
1?2? c = H
i
(y)? c ? H
i
(y)
? 
? 
? 
We are assessing the consequences of learning
with no markedness biases, so we will let the prior
probability in Equation (1) be uniform over the
hypothesis space. Since we are concerned with the
winner in any two-hypothesis competition, we will
work with the ratio of their posteriors. Here the
hypotheses GUJARATI*
?
,  GUJARATI
?
 and P ENULT
?
are the Non-Deterministic counterparts of the pre-
viously introduced hypotheses of the same names,
and the numerical values of G*, P and T are ex-
tracted from Table 1, under L
MU
? (and given at the
end of Section 2.1).
? 
p(GUJARATI
*?
| d)
p(PENULT
?
| d)
=
p(d
i
|GUJARATI
*?
, y
i
)
i
?
p(d
i
| PENULT
?
, y
i
)
i
?
  
? 
?
[d
i
?G
*
( y
i
)]
?
(1? 2?)
[d
i
=G
*
( y
i
)]
??
[d
i
?P ( y
i
)]
?
(1? 2?)
[ d
i
=P ( y
i
)]
?
=
? T?G* (1? 2?)G*? T?P (1? 2?)P         (2)
As we can see from Equation (2), the relative
probability advantage is highly dependent on the
magnitude of ?. Since ? is an error term, it should
remain relatively small. Within this constraint, we
could allow the learner to fit this parameter based
on maximizing hypothesis likelihood.  For the 3-
syllable uniform lexicon, ?
ML
 computed with re-
spect to GUJARATI* is approximately .14.  Using
this value in Equation (2) we find that GUJARATI*
?
wins out over both GUJARATI
?
 and PENULT
?
  by
s e v e r a l  o r d e r s  o f  m a g n i t u d e :
? 
p(G
*
| d)
p(P | d)
?1.85?10
4
;
? 
p(G
*
| d)
p(G | d)
? 3.4 ?10
8
 .
This initial result seems to provide strong sup-
port for The UG-Delimited H  Principle: the
GUJARATI* grammar seems overwhelmingly likely
to arise, and yet is unattested.  However, it is in-
structive to consider the inherent sensitivity of the
Bayesian learner to quite small differences be-
tween the linguistic hypotheses in question. A dis-
crepancy between data coverage of a mere 6
words, as seen in the above case, can lead to a hy-
pothesis advantage of four orders of magnitude.
And, in fact, a discrepancy of even 1 word can give
a posterior advantage on the order of a factor of 5
or greater (depending on the value of ?).  This re-
sult is the consequence of the extreme probability
distribution over only two types of data (consistent
and inconsistent -- with values close to 1 in the
first case, and close to 0 in the second). Since the
probability of an independent collection of out-
comes (a particular input lexicon) is computed via
multiplication, each additional difference in data
coverage compounds the single point case, such
that the ratio grows exponentially.
If this behavior is indeed a problem for our
linguistic domain (where different sub-regions of
phonological regularity are often observed to co-
exist stably in natural language (Inkelas 1997))
then there are various means at our disposal to
modify the learning model. In the following sec-
tion I will consider an alternative weighted deci-
sion metric; in Section 3.3 I will expand the hy-
pothesis space to include mixed-grammar com-
petitors; and in Section 4 I will alter the parameters
of the learning rule to provide a more stringent
threshold for success in hypothesis competition.
3.2 Optimal Bayes Classifier
So far, we have been implicitly assuming a winner-
take-all classification strategy whereby the hy-
pothesis with the highest likelihood given the data
is the one selected by the learner, and all others
discarded.  Let us now consider, instead, the Opti-
mal Bayes Classifier which categorizes new in-
stances of data by taking a weighted sum of the
5
predictions of all hypotheses in the space.
As expressed in Equation (3), the probability
that a new word y will be assigned to category c
m
(stress syllable m), given the body of training data
d ? p(c
m
|d,y) ? is the weighted sum of the prob-
ability each hypothesis gives of c
m
 classification ?
p(c
m
|H
s
,y ) ? where each of these terms is
weighted by the a posteriori probability of the par-
ticular hypothesis given the training data, p(H
s
 | d).
? 
p(c
m
| d,y) = p(c
m
H
s
?
|H
s
,y)p(H
s
| d)       (3)
Consider now the situation where there are
three hypotheses in the space: H
i
?
, H
j
?
, and H
k
?
.
The formulation of the selector function in Equa-
tion (3) allows for the possibility of a ganging-up
effect whereby H
j
?
 and H
k
?
, even if they individu-
ally have lower posterior probability over d than
does H
i
?
, can act together to influence the classifi-
cation of a new data point y.  We can choose the
lexicon in this example so as to showcase the larg-
est possible effect these two subordinate rules
could have by making the difference in consistent
data between the (deterministic) hypotheses as
small as possible, such that H
i   
has a coverage ad-
vantage of only one data point over both H
j
 and H
k
.
We will also consider those words for which H
j
and H
k
 differ from the classification predicted by
H
i
 (H
i
(y)=c
1
), but agree with one another in se-
lecting c
2
 with the highest probability (H
j
(y)=
H
k
(y)= c
2
).
From Equation (2), with G*-P=1,
? 
p(H
j / k
?
| d) =
?
1?2? p(Hi? | d)          (4a)
Substituting (4a) into Equation (3) gives the prob-
ability that classification will occur in line with the
dominant hypothesis H
i
:
? 
p(c
1
| d , y) = (1?2? )P(H
i
?
| d) +? ?
1?2? P(Hi? | d )
       
? 
+ ? ?
1? 2? P(Hi? | d )             (4b)
And the probability that classification will occur in
line with the subordinate, but mutually reinforcing,
H
j
 and H
k
 can be calculated similarly.
The ratio of the probability of categorizing the
new item consistently with H
i
 to that of categoriz-
ing consistently with H
j
 and H
k
 can then be shown
to be
? 
p(c
1
| d , y)
p(c
2
| d , y)
=
6? 2 ? 4? +1
3?(1? 2?)           (5)
Now take H
i
 = GUJARATI*, H
j
= GUJARATI, and H
k
= PENULT; y is a new word of the type in Row 4 of
Table 1.  The gang-up phenomenon, where
GUJARATI and PENULT
 
collude to move stress away
from the position preferred by GUJARATI*, may be
seen to have any kind of appreciable effect (where
? 
p(c
1
| d,y)
p(c
2
| d,y)
?1.5 ) only in the region .17 < ? < .4
(relatively large values for ?). Outside of this re-
gion GUJARATI*
 
dominates. And keep in mind, the
advantage to GUJARATI* only gets higher for larger
differences in coverage (in Equation (5) only a sin-
gle data point separates the three hypotheses), and
for instances of lexical items where GUJARATI and
PENULT disagree (Row 5 of Table 1).
So far we have seen that the Bayesian frame-
work exhibits a potential over-sensitivity when
applied to problems of the type formulated in this
paper: learning over a space of quasi-categorical,
contradictory hypotheses.  This is true whether we
consider learning to result in a single winner-take-
all hypothesis, or instead opt for the weighted deci-
sion metric of the Optimal Bayes Classifier.  We
will return to this issue in Section 4.  First, how-
ever, I will expand the hypothesis space under con-
sideration, in Section 3.3, and introduce, in Section
3.4, a non-uniform prior, adding principled biases
on the selection of those different hypotheses.
3.3 Mixed-Grammar Hypotheses
Before we can assess the performance of the Baye-
sian learner with respect to the UG-Delimited H
Principle we must make sure we consider all po-
tential competitor hypotheses that might be better
predictors of the data than those examined so far.
In particular, it is instructive to introduce some-
thing like a class of null hypotheses: hybrid gram-
mars which explicitly encode equality between any
pair of competing alternatives? ability to explain
the data
5
.
                                                 
5
 The effect of mixed-grammar hypotheses can also be
realized by allowing a selection procedure over a set of simple
grammars, as described in Section 3.2, but, crucially, with the
weights calculated under the assumption that data are
generated by a combination of grammars (see, for example,
the variational model proposed by Yang (1999), or the
6
I define this class as follows:  the posterior
probability that the hypothesis NULL(i/j)
?
 assigns to
a stress class c is calculated by allotting equal
probability to selecting the H
i
?
 or the H
j
?
 rule to
produce an output of that class:
? 
p(c | NULL(i / j)
?
, y) = w
i
p(c |H
i
?
, y) + w
j
p(c |H
j
?
, y) (6)
where w
i 
= w
j 
=  .5.  From Equation (6) and the
definition in [3], we can compute the probability
distribution of stress assignment c given the appli-
cation of NULL(i/j)
?
 to a particular word, y
[4] NULL(i/j)
?
: ?Null Hypothesis?
? 
p(c | NULL(i / j)
?
, y) =
1? 2? c = H
i
(y) = H
j
(y)
1??
2
c = H
i
(y) XOR c = H
j
(y)? c ? H
i
(y) &c ? H
j
(y)
? 
? 
? 
? 
? 
It can be shown that, for L
MU
? (the Gujarati?
lexicon generated from the Gujarati minimum uni-
form lexicon), the null hypothesis, NULL(G*/G)
?
,
is the decisive winner over GUJARATI*
?
 (by ap-
proximately 30 orders of magnitude).  With this
broader consideration of the hypothesis space, the
anti-markedness grammar is no longer the outcome
of learning.  And it turns out that we can specify
another hypothesis that gives an even higher likeli-
hood over the data.
The ?maximum likelihood? hypotheses are
specified by allowing all three parameters (w
i
, w
j
,
and ?  (now ?)) in Equation (6) to be estimated
from the data. MAX(i/j)
?
 is defined explicitly below
in [5] for any given weighted combination of H
i
?
and H
j
?
.
[5] MAX(i/j)
?
: ?Maximum Likelihood?
? 
p(c |MAX (i / j)
?
, y) =
         
? 
(w
i
+ w
j
)(1? 2? ) c = H
i
(y) = H
j
(y)
(1? 2? )w
i
+?w
j
c = H
i
(y) &c ? H
j
(y)
(1? 2? )w
j
+?w
i
c = H
j
(y) &c ? H
i
(y)
(w
i
+ w
j
)? c ? H
i
(y) &c ? H
j
(y)
? 
? 
? 
? 
? 
? 
? 
When H
i
 = GUJARATI* and H
j
 = GUJARATI,
MAX(G*/G)
?
 assigns the highest posterior of any
we have seen so far (approximately 56 orders of
magnitude larger than G*).  This is because, within
the space of candidates, it gives the highest likeli-
hood to the observed data, and the prior probability
                                                                             
probabilistic version of Optimality Theory over rankings
utilized by Jarosz (2006)).
(assumed so far to be uniform) plays no role in this
calculation.  As the hypotheses we are considering
become more complicated, however, we are led to
consider an alternative to this assumption, one in
which hypotheses with longer description lengths,
or greater complexity, are penalized (Rissanen
1989).
3.4 Non-Uniform Prior: Hypothesis Description
Length
Under the uniform prior assumption, only with a
lexicon in which GUJARATI* accounts for at least
44 times as much data as does GUJARATI will
MAX(G*/G)
?
 be defeated.  In this section I will
show how that result would be altered by consid-
ering a better approximation to the prior probabil-
ity distribution over those hypotheses. MAX(G*/G)
?
and GUJARATI*
?
 can be seen to differ in a basic
way related to the number of parameters and rules
they must each keep track of.  A domain-
independent means of determining a prior prob-
ability based on this difference in size, or com-
plexity, can be found in the information theoretic
notion of coding cost, or description length.
Each hypothesis uses a particular labeling
strategy to encode the input data (which can be
quantified by the number of binary pieces of in-
formation, or bits needed to transmit that informa-
tion to a waiting decoder).  In addition, a certain
number of bits is needed to encode the hypothesis
itself. The total description length for a string (or
set of data) d and a particular hypothesis H is given
by the following general formula for two-part
coding.
? 
L(d,H ) = L(d |H ) + L(H)
        (7)
The relation of (7) to Bayes Theorem becomes
clear when we introduce the fundamental trans-
formation from probability to optimal code length
given by
      
? 
L(x) = ? logP(x)
            (8)
Intuitively, Equation (8) calls for assigning shorter
length codes to higher probability symbols x
which, on average, will minimize the code length
for a string, d, of symbols drawn from distribution
P(x).  The ability to transform between length and
probability allows for the conceptualization of the
prior probabilities over the hypothesis space as
biases against complexity.
7
We can think of the hypotheses in H as deci-
sion trees which produce stressed outputs from
input words.  In order to encode such decision trees
we need something like the binary coding scheme
given in Rissanen (1989, section 7.2).
? 
L(T ) = log
k
T
+ m
T
? 2
k
T
? 
? 
? 
? 
? 
? 
        (9)
Here k
T
 is the number of internal (non-terminal)
nodes in the tree and m
T
 is the number of leaf (ter-
minal) nodes.  Equation (9) provides a measure of
how much the grammar compresses its input ? or
how many classes it must keep track of to produce
the correct output.  For a series of decisions, based
on querying for a series of features at a series of
internal nodes, there will be a particular outcome at
a particular leaf node.  For the GUJARATI* gram-
mar, k
T
=5 (corresponding to the relevant questions
about vowel identity listed in definition [1] above),
and m
T
=6 (corresponding to the possible stress
decisions resulting from the answers to each of
those questions).
Additionally, all Non-Deterministic hypothe-
ses require the estimation of at least one error term.
I will approximate the coding length for a set of k
free parameters (
? 
? ? ), estimated over a string of
length n, by Equation (10) (Rissanen 1989, section
3.1).
? 
L(
? ? ) = k
2
logn
     (10)
Since I am only interested here in computing
the length associated with the hypotheses them-
selves (the negative log of their prior probability),
we will focus on the second term of Equation (7),
which can be written as the sum of (9) and (10).
MAX(G*/G)
?
 consists of a decision tree that is
twice as large as that of GUJARATI*
?
 (since it keeps
track of both GUJARATI*
?
  and GUJARATI
?
). Addi-
tionally, the combination hypothesis makes use of
one more estimated parameter (w
G*
).
Under L
MU
?, where n=512 words, the prior
probability ratio
6
 of MAX(G*/G)
? 
to GUJARATI*
?
 is
1.7x10
4
. From this result we can calculate that the
type of lexicon in which the mixed-grammar hy-
pothesis would be rejected is one in which the
GUJARATI* hypothesis accounts for at least eight
                                                 
6
 the contribution of the hypotheses lengths,  converted back to
probability via Equation (8)
times more data than does GUJARATI (G*/G = 8).
This value must be regarded as an approxima-
tion due to its dependence on the particular coding
scheme used
7
.  It is, however, likely the best and
most principled estimate of the linguistic-bias-free
prior we can achieve
8
.
Under the information theoretic treatment, its
lower probability prior is still not enough to pre-
vent MAX(G*/G)
?
 from winning under L
MU
? (by 52
orders of magnitude over GUJARATI*
?
). The pro-
ductions of a learner who has converged on this
grammar would not be obviously consistent with a
reversed sonority-to-stress output (since many
words would show a stress pattern that is incom-
patible with that hypothesis), but neither would
those productions be inconsistent with such a
grammar (since a (slim) majority of words provide
positive evidence for such a hypothesis).  The ty-
pological status of such languages will be dis-
cussed in the following section.
4 Discussion & Conclusion
The foregoing analysis has served to address the
question of whether the observed frequency of oc-
currence (approximately never) of anti-markedness
systems (such as a grammar with a preference for
stressing low sonority vowels over high) requires
an active constraint that removes those grammars
from the learner?s hypothesis space.  The central
claim within this paper has been that attempts to
answer this question must involve a careful exami-
nation and specification of the learning process, as
well as the inputs to the learner.
Given that systems, at any particular time, tend
                                                 
7
 In practice, a code length exactly equal to the negative log of
the probability of a particular symbol may be unattainable, and
the relationship in Equation (8) becomes an approximation
which may be better in some cases than others.  Due to this
limitation, it is not clear how much the exact magnitude of a
result obtained with this method can be relied upon (for a brief
discussion of this issue see, for example, Brent (1999).)
8
 An alternative to this approach is to imagine all grammars as
potential mixtures, and to stipulate a prior probability distri-
bution over the possible weight values.  Each grammar in this
view is equally complex, but certain weight combinations may
be more likely than others (such as the ?simple? 0/100% distri-
bution over weights).  Conceptually this seems at least as rea-
sonable as the current approach.  We are still left, however,
with the problem of determining the prior probability distribu-
tion over the weights, in a manner which, ideally, would be
independent of the problem at hand.
8
to be in a state in which higher sonority vowels
attract stress (due to assumed perceptual factors),
the hypothetical sound change that disrupts the
natural order must act over forms that are origi-
nally markedness-abiding.  Thus, there will be a
residue of those forms in the language even after
the change has occurred (those in which /?/?s not
derived from /a/?s fail to attract stress in the pres-
ence of mid-sonority vowels).  If this residue is
small enough then the anti-markedness hypothesis
might emerge as the winner.  In turn, for this resi-
due to be small, the lexicon before the change must
exhibit a certain make-up, such that some word
types either fail to appear or occur with much
lower frequency than others.
In order to approximate these conditions I cre-
ated 1000 (x5) simulated lexicons by sampling
(without replacement) from the uniform word in-
ventory (L
MU
) at five different rates; for 3-syllable
words: 1% (=5 types), 3% (=15 types), 5% (=26
types), 7% (=36 types), and 10% (=51 types).
Higher sampling rates meant a greater likelihood
of reproducing the underlying uniform type distri-
bution over the 1000 trials, while lower sampling
rates (under-sampling) allowed for a higher likeli-
hood of departure from uniformity, and a greater
chance for skewed, or outlier, lexicons to emerge.
These simulations were done for the full set of
both 3-syllable and 2-syllable words (a more real-
istic distribution of input to the learner). To com-
bine the two word lengths, with differing numbers
of types, I scaled selection from the two classes.  A
cursory examination of the online English database
CELEX (1993) gives a count of 45,652 for 3-
syllable words, and 61,738 for 2-syllable words, a
1:1.4 relationship.  Using this as a rough guide, and
since the ratio of total types between 3-syllable and
2-syllable words is 512:64, a 1:10 scale was used
(giving a proportion of 512:640=1:1.25). Each of
the five sampling rates maintained this 1:10 scaling
factor, such that the lexicon containing 3-syllable
word types sampled at 7%, also contained 2-
syllable word types sampled at 70%; this is the
lexicon of 36 3-syllable word types (out of a possi-
ble total of 512) and 45 2-syllable word types (out
of a possible total of 64) (Row 5: [36,45] in Table
2).
Each lexicon, L, at a particular sampling rate,
was transformed to its L ? counterpart (via the
change a>?), and the coverage ratio between hy-
potheses GUJARATI* and GUJARATI over L ? was
computed.  As given at the end of Section 3.4 for
the description-length prior, a value greater than
G*/G = 8 is needed for a GUJARATI* outcome.
Here, due to concerns about the sensitivity of the
Bayesian learner, and the degree of uncertainty in
the calculation of the prior, I relax this criterion.
The last four columns of Table 2 correspond to
four (largely arbitrary) values for the G*/G ratio
which were stipulated as thresholds (or possible
prior probability ratios) that would allow GU-
JARATI* to beat MAX(G*/G)
?
.  Each cell contains
the percentage of anti-markedness outcomes (cal-
culated from 1000 runs) for a given threshold, at a
given sampling rate.
G*/GSampling
Rate
[3,2]-syllable
word types
5 2.5 1.7 1.25
1%,10% [5,6] 0 0 .4% 6.4%
3%,30% [15,19] 0 0 0 .9%
5%,50% [26,32] 0 0 0 .1%
7%,70% [36,45] 0 0 0 0
10%,100% [51,64] 0 0 0 0
Table 2: Estimated probabilities of learned anti-
markedness grammar: under 5 different sampling rates
(given as [number of 3-syllable,2-syllable word types]),
for four different threshold coverage ratios.
The very low occurrence rates of Table 2 show
that changing our assumptions about the make-up
of the lexicon (departing from uniformity) do not
qualitatively alter the results of the previous sec-
tions.  A pure anti-markedness grammar (GU -
JARATI*) seems to be a relatively rare outcome as
compared to a mixed-grammar competitor
(MAX(G*/G)
?
), even under relaxed acceptance cri-
teria.
The above work relies heavily on the existence
of a residue of natural patterns in a post-sound
change language.  Under circumstances in which
sound change is non-neutralizing (that is, ? is ab-
sent from the inventory of Gujarati before the
sound change), there will be no contradictory evi-
dence to the learner of Gujarati?: all data is consis-
tent with the GUJARATI* hypothesis.  Furthermore,
there is a long-standing intuition in the literature
that the most likely sound changes might actually
9
be of this type (Martinet 1955)
9
.
Under these circumstances we might expect
GUJARATI* to emerge as the clear winner.  This
will depend critically on whether or not we con-
sider the lack of conflicting data to be an over-
whelming factor in hypothesis selection.  If, in-
stead, we maintain our space of non-deterministic
hypotheses, then there is still competition from the
mixed-grammar alternatives. Under the non-
neutralizing scenario, Gujarati has 7 vowels (rather
than 8); for 3-syllable words, all 343 types support
the G UJARATI*
?
 hypothesis, while 265 are also
consistent with PENULT
?
.  And G*/P = 1.3. 2-
syllable words will provide somewhat less of an
advantage to the anti-markedness grammar
(49:46~1.13), and with a larger weight (10 times
greater frequency to approximate the CELEX ra-
tios), giving an adjusted ratio of roughly 1.15.
Whether this is enough of an advantage to cause
GUJARATI*
 
to be selected will depend on the pa-
rameters of our learner, as well as the prior prob-
ability ratio between the two hypotheses: the dif-
ference in complexity between the GUJARATI* rule,
which computes stress location based on both po-
sition and sonority, and the PENULT rule, which
only computes over position.
What the above discussion illustrates is that the
actual form of common or likely sound changes
can significantly alter the outcome of analysis.  If
non-neutralizing sound changes are the norm, then
the dispreferred grammar might have a higher pre-
dicted likelihood than that calculated here.  Alter-
natively, if chain shifts predominate, whereby all
the vowels in the system undergo related incre-
mental changes in quality, the outcome might be
different again.  And if realistic sound changes op-
erate on a word by word basis, as predicted by
Evolutionary Phonology, such that results are even
less consistent in terms of sonority class, an even
lower likelihood for a true anti-markedness gram-
mar might be the result
10
.
                                                 
9
 Thanks to Adam Albright for bringing this to my attention.
10
 Another issue so far undiscussed is the aptness of describing
the GUJARATI* hypothesis as a reversed sonority-to-stress
scale.  In either instantiation of Gujarati? (deriving either from
the 7- or 8-vowel system) there are only two operable sonority
categories {MID,?}.  Stressing ? preferentially over a higher-
sonority mid vowel is already dispreferred behavior from a
universalist perspective, but it is qualitatively different than a
hypothesis that targets sonority as the deciding factor (rather
than vowel identity).  This second hypothesis, for example,
This work has been a preliminary attempt to
accurately lay out the methodological requirements
for addressing questions of how grammars arise.
Further research ought to be concerned with ex-
actly the complications to the question just raised.
For present purposes, however, there are two gen-
eral points to be made.  The first is that, in order to
determine what any theory predicts in this domain,
one has to make assumptions about what consti-
tutes a realistic language learner, as well as estab-
lish estimates of the normal state of lexical statis-
tics.  The second point is that determining those
predictions tells us what the relevant typological
facts are.  The work here suggests that it is the oc-
currence, not so much of  pure anti-markedness
systems, but of partial anti-markedness (mixed-
grammar) systems that is the critical issue.  It may
turn out to be the case that these systems are also
very rare, and the over-prediction claim holds in its
revised form.  However, the true distribution of
these types of  languages seems far from clear at
the present time, and work will have to be done to
establish the fact of the matter
11
.
Acknowledgments
This work was supported by an NSF IGERT grant
and a Department of Education Javits Fellowship.
I would like to thank Paul Smolensky, Colin Wil-
son, and Simon Fischer-Baum for their invaluable
assistance.  Thanks also go to the three reviewers
of this paper, especially Adam Albright for his ex-
tensive and extremely helpful comments.
                                                                             
would avoid stressing newly encountered a?s, precisely be-
cause of the high sonority of the vowel.  The likelihood of
achieving a true sonority scale reversal seems even lower than
that of learning the ?stress-?? rule.  This is because the strong-
est evidence for a sonority-sensitive scale involves multiple
tiers or classes of sonority (probably at least three).  However,
the more different classes of vowels (the more complications
to the calculation of stress) the less likely it seems that an
indirect sound change (one that does not target sonority itself)
will produce a clean reversal of the pattern.  Again, disorder,
or proliferating ?co-phonologies? seem more likely to carry the
day.
11
 In the first place, it is not a given that pure anti-markedness
systems are completely  non-occurring (see, for example,
Poppe (1960); McLendon (1975); Breen and Pensalfini
(1999)).  As for potential mixed-grammar languages, these
might include systems that have been analyzed as exhibiting
high degrees of lexical exceptionality, or gone largely un-
analyzed due to what is perceived as patternless behavior.
10
References
Blevins, J. (2004). Evolutionary Phonology: the emer-
gence of sound patterns. New York, Cambridge Univer-
sity Press.
Breen, G. and R. Pensalfini (1999). "Arrernte: a lan-
guage with no syllable onsets." Linguistic Inquiry 30(1):
1-25.
Chater, N., J. B. Tenenbaum, et al (2006). "Probabilis-
tic models of cognition: conceptual foundations."
Trends in Cognitive Science 10(7): 287-291.
Court, C. (1970). Nasal harmony and some indonesian
sound laws. Pacific Linguistics Series C No.13. S. A.
Wurm and C. Laycock.
de Lacy, P. (2006). Markedness: Reduction and Preser-
vation in Phonology, Cambridge University Press.
Inkelas, S. (1997). The theoretical status of morphologi-
cally conditioned phonology: a case study of dominance
effects. Yearbook of Morphology. G. Booij and J. van
Marle, Kluwer Academic Publishers: 121-155.
Ito, J. and A. Mester (2001). "Covert generalizations in
Optimality Theory: the role of stratal faithfulness con-
straints." Studies in Phonetics, Phonology and Mor-
phology 7: 273-299.
Jarosz, G. (2006). Richness of the base and probabilistic
unsupervised learning in Optimality Theory. Proceed-
ings of the Eighth Meeting of the ACL Special Interest
Group in Computational Phonology and Morphology,
New York City.
Kemp, C., A. Perfors, et al (2007). "Learning overhy-
potheses with hierarchical Bayesian models." Develop-
mental Science 10(3): 307-321.
Kiparsky, P. (2004). "Universals constrain change;
change results in typological generalizations." ms.
Kiparsky, P. (2006). "The Amphichronic Program vs.
Evolutionary Phonology." Theoretical Linguistics 32:
217-236.
Kording, K. P. and D. M. Wolper (2006). "Bayesian
decision theory in sensorimotor control." Trends in
Cognitive Science 10(7): 319-326.
Martinet, A. (1955). Economie des changements pho-
netiques. Bern, Francke.
McLendon, S. (1975). A Grammar of Eastern Pomo,
University of California Press.
Mitchell, T. M. (1997). Machine Learning, McGraw-
Hill.
Pater, J. (2000). "Non-uniformity in English seconday
stress: the role of ranked and lexically specific con-
straints." Phonology 17: 237-274.
Poppe, N. N. (1960). Buriat Grammar, Indiana Univer-
sity Publications.
Prince, A. and P. Smolenksy (1993/2004). Optimality
Theory, Blackwell Publishing.
Rissanen, J. (1989). Stochastic Complexity in Statistical
Enquiry, World Scientific Publishing Co.
Tenenbaum, J. B., C. Kemp, et al (2007). Theory-based
Bayesian models of inductive reasoning. Inductive Rea-
soning. A. Feeney and E. Heit, Cambridge University
Press.
Yang, C. (1999). A Selectionist Theory of Language
Acquisition. 27th Annual Meeting of the Association for
Computational Linguistics, College Park, MD.
11
