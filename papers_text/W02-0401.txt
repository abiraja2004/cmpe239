Using Maximum Entropy for Sentence Extraction
Miles Osborne
osborne@cogsci.ed.ac.uk
Division of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW
United Kingdom.
Abstract
A maximum entropy classier can be used
to extract sentences from documents. Ex-
periments using technical documents show
that such a classier tends to treat features
in a categorical manner. This results in per-
formance that is worse than when extract-
ing sentences using a naive Bayes classier.
Addition of an optimised prior to the max-
imum entropy classier improves perfor-
mance over and above that of naive Bayes
(even when naive Bayes is also extended
with a similar prior). Further experiments
show that, should we have at our disposal
extremely informative features, then max-
imum entropy is able to yield excellent re-
sults. Naive Bayes, in contrast, cannot ex-
ploit these features and so fundamentally
limits sentence extraction performance.
1 Introduction
Sentence extraction |the recovery of a given set
of sentences from some document| is useful for
tasks such as document summarisation (where the
extracted sentences can form the basis of a summary)
or question-answering (where the extracted sentences
can form the basis of an answer). In this paper, we
concentrate upon extraction of sentences for inclu-
sion into a summary. From a machine learning per-
spective, sentence extraction is interesting because
typically, the number of sentences to be extracted
is a very small fraction of the total number of sen-
tences in the document. Furthermore, those clues
which determine whether a sentence should be ex-
tracted or not tend to be either extremely specic,
or very weak, and furthermore interact together in
non-obvious ways. From a linguistic perspective, the
task is challenging since success hinges upon the abil-
ity to integrate together diverse levels of linguistic
description.
Frequently (see section 6 for examples), sentence
extraction systems are based around simple algo-
rithms which assume independence between those
features used to encode the task. A consequence of
this assumption is that such approaches are funda-
mentally unable to exploit dependencies which pre-
sumably exist in the features that would be present
in an ideal sentence extraction system. This situ-
ation may be acceptable when the features used to
model sentence extraction are simple. However, it
will rapidly become unacceptable when more sophis-
ticated heuristics, with complicated interactions, are
brought to bear upon the problem. For example,
Boguraev and Ne (2000a) argue that the quality of
summarisation can be increased if lexical cohesion
factors (rhetorical devices which help achieve cohe-
sion between related document utterances) are mod-
elled by a sentence extraction system. Clearly such
devices (for example, lexical repetition, ellipsis, co-
reference and so on) all contribute towards the gen-
eral discourse structure of some text and furthermore
are related to each other in non-obvious ways.
Maximum entropy (log-linear) models, on the
other hand, do not make unnecessary independence
assumptions. Within the maximum entropy frame-
work, we are able to optimally integrate together
whatever sources of knowledge we believe potentially
to be useful for the task. Should we use features that
are benecial, then the model will be able to exploit
this fact. Should we use features that are irrelevant,
then again, the model will be able to notice this, and
eectively ignore them. Models based on maximum
entropy are therefore well suited to the sentence ex-
traction task, and furthermore, yield competitive re-
sults on a variety of language tasks (Ratnaparkhi,
1996; Berger et al, 1996; Charniak, 1999; Nigam et
al., 1999).
In this paper, we outline a conditional maximum
         Philadelphia, July 2002, pp. 1-8.  Association for Computational Linguistics.
          Proceedings of the Workshop on Automatic Summarization (including DUC 2002),
entropy classication model for sentence extraction.
Our model works incrementally, and does not always
need to process the entire document before assign-
ing classication.
1
It discriminates between those
sentences which should and should not be extracted.
This contrasts with ranking approaches which need
to process the entire document before extracting sen-
tences. Because we model whether a sentence should
be extracted or not in terms of features that are ex-
tracted from the sentence (and its context in the doc-
ument), we do not need to specify the size of the sum-
mary. Again, this contrasts with ranking approaches
which need to specify a priori the summary size.
Our maximum entropy approach for sentence ex-
traction does not come without problems. Using
reasonably standard features, and when extracting
sentences from technical papers, we nd that pre-
cision levels are high, but recall is very low. This
arises from the fact that those features which pre-
dict whether a sentence should be extracted tend to
be very specic and occur infrequently. Features for
sentences that should not be extracted tend to be
much more abundant, and so more likely to be seen
in the future. A simple prior probability is shown
to help counter-act this tendency. Using our prior,
we nd that the maximum entropy approach is able
to yield results that are better than a naive Bayes
classier.
Our nal set of experiments looks more closely at
the dierences between maximum entropy and naive
Bayes. We show that when we have access to an ora-
cle that is able to tell us when to extract a sentence,
then in the situation when that information is en-
coded in dependent features, maximum entropy eas-
ily outperforms naive Bayes. Furthermore, we also
show that even when that information is encoded in
terms of independent features, naive Bayes can be
incapable of fully utilising this information, and so
produces worse results than maximum entropy.
2
1
Incremental classication means that a document is
processed from start-to-nish and decisions are made as
soon as sentences are encountered. Some of our features
(in particular, those which encode sentence position in
a document) do require processing the entire document.
Using such features prevents true incremental process-
ing. However, it is trivial to remove such features and so
ensure true incrementality.
2
As a reviewer commented, under certain circum-
stances, naive Bayes can do well even when there are
strong dependencies within features (Domingos and Paz-
zani, 1997). For example, when the sample size is small,
naive Bayes can be competitive with more sophisticated
approaches such as maximum entropy. Given this, a
fuller comparison of naive Bayes and maximum entropy
for sentence extraction requires considering sample size
in addition to the choice of features.
The rest of this paper is as follows. Section 2
outlines the general framework for sentence extrac-
tion using maximum entropy modelling. Section 3
presents our naive Bayes classier (which is used as a
comparison with maximum entropy). We then show
in section 4 how both our maximum entropy and
naive Bayes classiers can be extended with an (op-
timised) prior. The issue of summary size is touched
upon in section 5. Section 6 discusses related work.
We then present our main results (section 7). Fi-
nally, section 8 discusses our results and considers
future work.
2 Maximum Entropy for Sentence
Extraction
2.1 Conditional Maximum Entropy
The parametric form for a conditional maximum en-
tropy model is as follows (Nigam et al, 1999):
P (c j s) =
1
Z(s)
exp(
X
i

i
f
i
(c; s)) (1)
Z(s) =
X
c
exp(
X
i

i
f
i
(c; s)) (2)
Here, c is a label (from the set of labels C) and s is
the item we are interested in labelling (from the set
of items S). In our domain, C simply consists of two
labels: one indicating that a sentence should be in
the summary (`keep'), and another label indicating
that the sentence should not be in the summary (`re-
ject'). S consists of a training set of sentences, linked
to their originating documents. This means that we
can recover the position of any given sentence in any
given document.
Within maximum entropy models, the training set
is viewed in terms of a set of features. Each fea-
ture expresses some characteristic of the domain.
For example, a feature might capture the idea that
abstract-worthy sentences contain the words in this
paper. In equation 1, f
i
(c; s) is a feature. In this pa-
per we restrict ourselves to integer-valued functions.
An example feature might be as follows:
f
i
(c; s) =
8
>
>
<
>
>
:
1 if s contains the phrase
in this paper
and c is the label keep
0 otherwise
(3)
Features are related to each other through weights
(as can be seen in equation 1, where some feature
f
i
has a weight 
i
). Weights are real-valued num-
bers. When a closed form solution cannot be found,
they are determined by numerical optimisation tech-
niques. In this paper, we use conjugate gradient de-
scent to nd the optimal set of weights. Conjugate
Gradient descent converges faster than Improved It-
erative Scaling (Laerty et al, 1997), and empirically
we nd that it is numerically more stable.
2.2 Maximum Entropy Classication
When classifying sentences with maximum entropy,
we use the equation:
label(s) = argmax
c2C
P (c j s) (4)
In practice, we are not interested in the probabil-
ity of a label given a sentence. Instead we use the
unnormalised score:
label(s) = argmax
c2C
exp(
X
i

i
f
i
(c; s)) (5)
Note that this maximum entropy classier assumes
a uniform prior. Section 4 shows how a non-uniform
prior is used in place of this uniform prior.
We now present our basic naive Bayes classier.
Afterwards, we extend this classier with a non-
uniform prior.
3 Naive Bayes Classication
As an alternative to maximum entropy, we also inves-
tigated a naive Bayes classier. Unlike maximum en-
tropy, naive Bayes assumes features are conditionally
independent of each other. So, comparing the two to-
gether will give an indication of the level of statistical
dependencies which exist between features in the sen-
tence extraction domain. For our experiments, we
used a variant of the multi-variate Bernoulli event
model (McCallum and Nigam, 1998). In particular,
we did not consider features that are absent in some
example. This allows us to avoid summing over all
features in the model for each example. Note that
our maximum entropy model also did not consider
absent features.
Within our naive Bayes approach, the probability
of a label given the sentence is as follows:
P (c j s) =
P (c)
Q
n
i=1
P (g
i
j c)
P (s)
(6)
As before, s is some sentence, c the label, and g
i
is some active feature describing sentence s. Naive
Bayes models can be estimated in a closed form
by simple counting. For features which have zero
counts, we use add-k smoothing (where k is a small
number less than one).
Since the probability of the data (P (s)) is con-
stant:
P (c j s) / P (c)
n
Y
i=1
P (g
i
j c) (7)
If we assume a uniform prior (in which case P (c) is a
constant for all c), this can be further simplied to:
P (c j s) /
n
Y
i=1
P (g
i
j c) (8)
Our basic naive Bayes classier is as follows:
label(s) = argmax
c2C
n
Y
i=1
P (g
i
j c) (9)
As with the maximum entropy classier, we later
replace the uniform prior with a non-uniform prior.
4 Maximum a Posteriori
Classication
In this section, we show how our classiers can be
extended with a non-uniform prior. We also describe
how such a prior can be optimised.
4.1 Adding a non-uniform prior
Now, the two classiers mentioned previously (equa-
tions 9 and 5) are both based on maximum likeli-
hood estimation. However, as we describe later, for
sentence extraction, the maximum entropy classier
tends to over-select labels. In particular, it tends
to reject too many sentences for inclusion into the
summary. So, it it useful to extend the two previous
classiers with a non-uniform prior. For the naive
Bayes classier, we have:
label(s) = argmax
c2C
P (c)
n
Y
i=1
P (g
i
j c) (10)
Here, P (c) is our prior. The probability of the data
(P (s)) is constant and so can be dropped.
For the maximum entropy case, we are not inter-
ested in the actual probability:
label(s) = argmax
c2C
F (c) exp(
X
i

i
f
i
(c; s)) (11)
F (c) is a function equivalent to the prior when
using the unnormalised classier. When this prior
distribution (or equivalent function) is uniform, clas-
sication is as before (namely as outlined in sections
2 and 3), and depends upon the maximum entropy
or naive Bayes component. When the prior is non-
uniform, the classier behaviour will change. This
prior therefore allows us to aect the performance
of our system. In particular, we can change the
precision-recall balance.
4.2 Optimising the prior
We treat the problem of selecting a prior as an opti-
misation task: select some P (c) (or F (c)) such that
performance, as measured by some objective func-
tion of the overall classier, is maximised. Since the
choice of objective function is up to us, we can eas-
ily optimise the classier in any way we decide. For
example, we could optimise for recall by using as our
objective function an f-measure that weighted recall
more highly than precision. In this paper, we opti-
mise the prior using as an objective function the f2
score of the classier (section 7 details this score).
Our prior therefore does not reect relative frequen-
cies of labels (as found in some corpus).
We now need to optimise our prior. Brent's one
dimensional function minimisation method is well
suited to this task (Press et al, 1993), since for a
random variable taking two values, the probability
of one value can be dened in terms of the other
value. Section 7 describes the held-out optimisation
strategy used in our experiments.
Should we decide to use a more elaborate prior (for
example, one which was also sensitive to properties
of documents) then we would need to use a multi-
dimensional function minimisation method.
Note that we have not simultaneously optimised
the likelihood and prior probabilities. This means
that we do not necessarily nd the optimal maxi-
mum a posteriori (MAP) solution. It is possible to
integrate into maximum entropy estimation (simple)
conjugate priors that do allow MAP solutions to be
found (Chen and Rosenfeld, 1999). Although it is
an open question whether more complex priors can
be directly integrated, future work ought to consider
the e?cacy of such approaches in the context of sum-
marisation.
5 Summary size
Determining the size of the summary is an impor-
tant consideration for summarisation. Frequently,
this is carried out dynamically, and specied by the
user. For example, when there is limited opportu-
nity to display long summaries a user might want a
terse summary. Alternatively, when recall is impor-
tant, a user might prefer a longer summary. Usually,
systems rank all sentences in terms of how abstract-
worthy that are, and then take the top n most highly
ranked sentences. This always requires the size of
summary to be specied.
In our classication framework, sentences are pro-
cessed (largely) independently of each other, and so
there is no direct way of controlling the size of the
summary. Altering the prior will indirectly inuence
the summary size. For more direct control over sum-
mary size, we can rank sentences using our classiers
(we not only label but can also assign label prob-
abilities) and select the top n most highly ranked
sentences.
Within our classication approach, the optimised
prior plays a similar role to the user-dened number
of sentences that a ranking approach might return.
Experiments (not reported here) showed that
ranking sentences using our maximum entropy classi-
er, and then selecting the top n most highly ranked
sentences produced slightly worse results than when
selecting sentences in terms of classication.
6 Related Work
The summarisation literature is large. Here we con-
sider only a representative sample.
Kupiec et al (1995) used Naive Bayes for sentence
extraction. They did not consider the role of the
prior, nor did they use Naive Bayes for classica-
tion. Instead, they used it to rank sentences and
selected the top n sentences. The TEXTRACT
system included a sentence extraction component
that is frequency-based (Boguraev and Ne, 2000b).
Whilst the system uses a wide variety of linguis-
tic cues when scoring sentences, it does not com-
bine these scores in an optimal manner. Also, it
does not consider interactions between the linguistic
cues. Goldstein et al (1999) used a centroid similar-
ity measure to score sentences. They do not appear
to have optimised their metric, nor do they deal with
statistical dependencies between their features.
7 Experiments
Summarisation evaluation is a hard task, principally
because the notion of an objective summary is ill-
dened. That aside, in order to compare our various
systems, we used an intrinsic evaluation approach.
Our summaries were evaluated using the standard
f2 score:
r =
j
m
p =
j
k
f2 =
2pr
p + r
where:
r = Recall
p = Precision
j = Number of correct sentences in summary
k = Number of sentences in summary
m = Number of correct sentences in the document
A sentence being `correct' means that it was
marked as being somehow important (abstract-
worthy) by a human and labelled `keep' by one of
our classiers. Summaries produced by our systems
will therefore attempt to mimic the process of select-
ing what it means for a sentence to be important in
a document.
Naturally this premise |that an annotator can
decide a priori whether a sentence is abstract-worthy
or not| is open to question. That aside, in other
sentence extraction scenarios, it may well be the case
that sentences can be reliably annotated.
The f2 score treats recall and precision equally.
This is a sensible metric to use as we have no a priori
reason to believe in some other non-equal ratio of the
two components.
Our evaluation results are based on the following
approach:
1. Split the set of documents into two disjoint sets
(T1 and T2), with 70 documents in T1 and 10
documents in T2.
2. Further split T1 into two disjoint sets T3 and
T4. T3 is used to train a model, and T4 is
a held-out set. The prior is estimated using
Brent's line minimisation method, when train-
ing using T3 and evaluating on T4. T3 consisted
of 60 documents and T4 consisted of 10 docu-
ments.
3. Results are then presented using a model trained
on T1, with the prior just found, and evaluated
using T2. T1 is therefore the training set and
T2 is the testing set. Results are also presented
using a at prior.
4. The whole process is then repeated after ran-
domising the documents. The nal results are
then averaged over these n runs. We set n to
40.
7.1 Document set
For data, we used the same documents that
Teufel (2001) used in her experiments.
3
In brief,
these were 80 conference papers, taken from the
Comp-lang preprint archive, and semi-automatically
converted from L
A
T
E
Xto XML. The XML annotated
documents were then additionally manually marked-
up with tags indicating the status of various sen-
tences. This document set is modest in size. On the
other hand, the actual documents are longer than
newswire messages typically used for summarisation
tasks. Also, the documents show variation in style.
For example, some documents are written by non-
native speakers, some by students, some by multiple
authors and so on. Summarisation is therefore hard.
3
A superset of the documents is described in (Teufel
and Moens, 1997).
Here are some properties of the documents. On
average, each document contained 8 sentences that
were marked as being abstract-worthy (standard de-
viation of 3.1). The documents on average each
contained in total 174 sentences (standard deviation
50.7). Here, a `sentence' is either any sequence of
words that happened to be in a title, or else any se-
quence of words in the rest of the document. As can
be seen, the summaries are not uniformly long. Also,
the documents vary considerably in length. Sum-
mary size is therefore not constant.
7.2 Features
We used the following, fairly standard features when
describing all sentences in the documents:
 Word pairs. Word pairs are consecutive words
as found in a sentence. A word pair feature sim-
ply indicates whether a particular word pair is
present. All words were reduced: truncated to
be at most 10 characters long. Stemming (as
for example carried out by the Porter stemmer)
produced worse results. We extracted all word
pairs found in all sentences, and for any given
sentence, found the set of (reduced) word pairs.
 Sentence length. We encoded in three binary
features whether a sentence was less than 6
words in length, whether it was greater than 20
words in length, or whether it was in between
these two ranges. We also used a feature which
encoded whether a previous sentence was less
than 5 words or longer. This captured the idea
that summary sentences tend to follow headings
(which are short).
 Sentence position. Summary sentences tend to
occur either at the start, or the end of a docu-
ment. We used three features: whether a given
sentence was within the rst 8 paragraphs of a
document, whether a sentence was in the last
3 paragraphs, or whether the sentence was in
a paragraph between these two ranges to en-
code sentence position. Note that this feature
requires the whole document to be processed be-
fore classication can take place.
 (Limited) discourse features. Our features de-
scribed whether a sentence immediately followed
typical headings such as conclusion or introduc-
tion, whether a sentence was at the start of a
paragraph, or whether a sentence followed some
generic heading.
Our features are not exhaustive, and are not designed
to maximise performance. Instead, they are designed
to be typical of those found in sentence extraction
systems. Note that some of our features exploit the
fact that the documents are annotated with struc-
tural information (such as headers etc).
Experiments with removing stop words from docu-
ments resulted in decreased performance. We conjec-
ture that this is because our word pairs are extremely
crude syntax approximations. Removing stop words
from sentences and then creating word pairs makes
these pairs even worse syntax approximations. How-
ever, using stop words increased the number of fea-
tures in our model, and so again reduced perfor-
mance. We therefore compromised between these
two positions, and mapped all stop words to the same
symbol prior to creation of word pair features. We
also found it useful to remove word pairs which con-
sisted solely of stop words. Finally, for maximum
entropy, we deleted any feature that occurred less
than 4 times. Naive Bayes did not benet from a
frequency-based cuto.
7.3 Classier comparison
Here we report on our classiers.
As a baseline model, we simply extracted the rst
n sentences from a given document. Figure 1 sum-
marises our results as n varies. In this table, as in all
subsequent tables, P and R are averaged precision
and recall values, whilst F2 is the f2 score of these
averaged values.
n F2 P R n F2 P R
1 0 0 0 26 16 10 36
6 3 3 2 31 18 12 45
11 19 15 26 36 18 11 53
16 20 16 29 41 17 10 58
21 23 16 38 46 16 9 58
Figure 1: Results for the baseline model
Figure 2 shows our results for maximum entropy,
both with and without the prior. Prior optimisation
was with respect to the f2 score. As in subsequent
tables, we show system performance when adding
more and more features.
Performance without the prior is heavily skewed
towards precision. This is because our features are
largely acting categorically: the sheer presence of
some feature is su?cient to inuence labelling choice.
Further evidence for this analysis is supported by in-
specting one of the models produced when using the
full set of all feature types. We see that of the 85883
Features Flat prior Optimised prior
F2 P R F2 P R
Word pairs 8 5 30 20 40 14
and sent length 25 63 16 36 36 36
and sent position 28 62 18 39 35 45
and discourse 35 63 24 42 43 41
Figure 2: Results for the maximum entropy model
feature instances in the model, the vast majority are
deeded irrelevant by maximum entropy, and assigned
a zero weight. Only 7086 features (roughly 10% in
total) had non zero weights.
Performance using the optimised prior shows more
balanced results, with an increase in F2 score.
Clearly optimising the prior has helped counter the
categorical behaviour of features in our maximum
entropy classier.
Figure 3 shows the results we obtained when us-
ing a naive Bayes classier. As before, the results
show performance with and without the addition of
the optimised prior. Naive Bayes outperforms maxi-
mum entropy when both classiers do not use a prior.
Performance with and without the prior however, is
worse than the performance of our maximum entropy
classier with the prior. Evidently, even our rela-
tively simple features interact with each other, and
so approaches such as maximum entropy are required
to fully exploit them.
Features Flat prior Optimised prior
F2 P R F2 P R
Word pairs 26 29 23 29 26 32
and sent length 31 33 28 32 29 35
and sent position 33 34 33 36 31 43
and discourse 38 39 37 39 38 40
Figure 3: Results for the naive Bayes model
7.4 Using informative features
Our previous results showed that maximum entropy
could outperform naive Bayes. However, the dier-
ences, though present, were not large. Clearly, our
feature set was imperfect.
4
It is therefore instruc-
tive to see what happens if we had access to an or-
acle who always told us the true status of some un-
seen sentence. To make things more interesting, we
4
Another possible reason for the closeness of the re-
sults is the small sample size. There may just not be
enough evidence to reliably estimate dependencies within
the data.
Features Naive Bayes Maxent
F2 P R F2 P R
Word pairs 30 34 26 32 93 19
and sent length 35 38 32 99 100 99
and sent position 40 41 39 100 100 100
and discourse 43 44 41 99 100 97
Figure 4: Results for basic naive Bayes and max-
imum entropy models using dependent informative
features
Features Naive Bayes Maxent
F2 P R F2 P R
Word pairs 84 74 97 25 15 91
and sent length 85 75 97 100 100 100
and sent position 84 73 97 100 100 100
and discourse 84 74 97 100 100 100
Figure 5: Results for basic naive Bayes and maxi-
mum entropy models using independent informative
features
encoded this information in terms of dependent fea-
tures. We simulated this oracle by using two features
which were active whenever a sentence should not
be in the summary; for sentences that should be in-
cluded in the summary, we let either one of those two
features be active, but on a random basis. Our fea-
tures therefore are only informative when the learner
is capable of noting that there are dependencies. We
then repeated our previous maximum entropy and
naive Bayes experiments. Figure 4 summarise our
results.
Unsurprisingly, we see that when features are
highly dependent upon each other, maximum en-
tropy easily outperforms naive Bayes.
Even when we have access to features that are in-
dependent of each other, naive Bayes can still do
worse than maximum entropy. To demonstrate this,
we used a feature that was active whenever a sen-
tence should be in the summary. This feature was
not active on sentences that should not be in the
summary. Figure 5 summarises our results.
As can be seen (gure 5), even when naive Bayes
has access to a perfectly reliable informative feature,
the fact that the other features are not suitably dis-
counted means that performance is worse than that
of maximum entropy. Maximum entropy can dis-
count the other features, and so can take advantage
of reliable features.
8 Comments and Future Work
We showed how maximum entropy could be used for
sentence extraction, and in particular, that adding
a prior could deal with the categorical nature of
the features. Maximum entropy, with an opti-
mised prior, did yield marginally better results than
naive Bayes (with and without a similarly optimised
prior). However, the dierences were not that great.
Our further experiments with informative features
showed that this lack of dierence was probably due
(at least in part) to the actual features used, and not
due to the technique itself.
Our oracle results are an idealisation. A fuller
comparison should use more sophisticated features,
along with more data. As a result of this, we conjec-
ture that should we use a much more sophisticated
feature set, we would expect that the dierences be-
tween maximum entropy and naive Bayes would be-
come greater.
Our approach treated sentences largely indepen-
dently of each other. However, abstract-worthy sen-
tences tend to bunch together, particularly at the
beginning and end of a document. We intend cap-
turing this idea by making our approach sequence-
based: future decisions should also be conditioned
on previous choices.
A problem with supervised approaches (such as
ours) is that we need annotated material (Marcu,
1999). This is costly to produce. Future work will
consider weakly supervised approaches (for example
cotraining) as a way of bootstrapping labelled mate-
rial from unlabelled documents (Blum and Mitchell,
1998). Note that there is a close connection between
multi-document summarisation (where many alter-
native documents all consider similar issues) and the
concept of a view in cotraining. We expect that this
redundancy could be exploited as a means of provid-
ing more annotated training material, and so yield
better results.
In summary, maximum entropy can be benecially
used in sentence extraction. However, one needs to
guard against categorial features. An optimised prior
can provide such help.
Acknowledgement
We would like to thank Rob Malouf for supplying the
excellent log-linear estimation code, Simone Teufel
for providing the annotated data, Karen Spark Jones
for a discussion about summarisation, Steve Clark
for spotting textual bugs and the anonymous review-
ers for useful comments.
References
Adam Berger, Stephen Della Pietra, and Vin-
cent Della Pietra. 1996. A maximum entropy
approach to natural language processing. Com-
putational Linguistics, 21{22.
Avrim Blum and Tom Mitchell. 1998. Combin-
ing labeled and unlabeled data with co-training.
In Proceedings of the Workshop on Computational
Learning Theory. Morgan Kaufmann Publishers.
Branimir K. Boguraev and Mary S. Ne. 2000a. The
eects of analysing cohesion on document sum-
marisation. In Proceedings of the 18
th
Interna-
tional Conference on Computational Linguistics,
volume 1, pages 76{82, Saarbrucken.
Branmir K. Boguraev and Mary S. Ne. 2000b. Dis-
course Segmentation in Aid of Document Summa-
rization. In Proceedings of the 33
rd
Hawaii Inter-
national Conference on Systems Science.
Eugene Charniak. 1999. A maximum-entropy-
inspired parser. Technical Report CS99-12, De-
partment of Computer Science, Brown University.
Stanley F. Chen and Ronald Rosenfeld. 1999.
A Gaussian prior for smoothing maximum en-
tropy models. Technical Report CMU-CS-99-108,
Carnegie Mellon University.
Pedro Domingos and Michael J. Pazzani. 1997. On
the optimality of the simple bayesian classier un-
der zero-one loss. Machine Learning, 29(2-3):103{
130.
Jade Goldstein, Mark Kantrowitz, Vibhu O. Mit-
tal, and Jaime G. Carbonell. 1999. Summarizing
text documents: Sentence selection and evaluation
metrics. In Research and Development in Informa-
tion Retrieval, pages 121{128.
Julian Kupiec, Jan Pedersen, and Francine Chen.
1995. A Trainable Document Summarizer. In
Proceedings of the 18
th
ACM-SIGIR Conference
on Research and Development in Information Re-
trieval, pages 68{73.
J. Laerty, S. Della Pietra, and V. Della Pietra.
1997. Inducing features of random elds. IEEE
Transactions on Pattern Analysis and Machine In-
telligence, 19(4):380{393, April.
Daniel Marcu. 1999. The automatic construction
of large-scale corpora for summarization research.
In Research and Development in Information Re-
trieval, pages 137{144.
A. McCallum and K. Nigam. 1998. A comparison of
event models for naive bayes text classicatio. In
AAAI-98 Workshop on Learning for Text Catego-
rization.
Kamal Nigam, John Laerty, , and Andrew Mc-
Callum. 1999. Using maximum entropy for text
classication. In IJCAI-99 Workshop on Machine
Learning for Information Filtering,.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 1993. Numeri-
cal Recipes in C: the Art of Scientic Computing.
Cambridge University Press, second edition.
Adwait Ratnaparkhi. 1996. A Maximum En-
tropy Part-Of-Speech Tagger. In Proceed-
ings of Empirical Methods in Natural Lan-
guage, University of Pennsylvania, May. Tagger:
ftp://ftp.cis.upenn.edu/pub/adwait/jmx.
S. Teufel and M. Moens. 1997. Sentence extraction
as a classication task. In ACL/EACL-97 Work-
shop on Intelligent and Scalable Text Summariza-
tion, Madrid, Spain.
Simone Teufel. 2001. Task-Based Evaluation of
Summary Quality: Describing Relationships Be-
tween Scientic Papers. In NAACL Workshop on
Automatic Summarization, Pittsburgh, Pennsyl-
vania, USA, June. Carnegie Mellon University.
