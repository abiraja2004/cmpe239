Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 50?59,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Structural Semantic Relatedness: A Knowledge-Based Method to 
Named Entity Disambiguation 
 
Xianpei Han        Jun Zhao? 
National Laboratory of Pattern Recognition  
 Institute of Automation, Chinese Academy of Sciences 
Beijing 100190, China  
{xphan,jzhao}@nlpr.ia.ac.cn 
 
  
 
                                                 
? Corresponding author 
Abstract 
 
Name ambiguity problem has raised urgent 
demands for efficient, high-quality named ent-
ity disambiguation methods. In recent years, 
the increasing availability of large-scale, rich 
semantic knowledge sources (such as Wikipe-
dia and WordNet) creates new opportunities to 
enhance the named entity disambiguation by 
developing algorithms which can exploit these 
knowledge sources at best. The problem is that 
these knowledge sources are heterogeneous 
and most of the semantic knowledge within 
them is embedded in complex structures, such 
as graphs and networks. This paper proposes a 
knowledge-based method, called Structural 
Semantic Relatedness (SSR), which can en-
hance the named entity disambiguation by 
capturing and leveraging the structural seman-
tic knowledge in multiple knowledge sources. 
Empirical results show that, in comparison 
with the classical BOW based methods and 
social network based methods, our method can 
significantly improve the disambiguation per-
formance by respectively 8.7% and 14.7%. 
1 Introduction 
Name ambiguity problem is common on the Web. 
For example, the name ?Michael Jordan? 
represents more than ten persons in the Google 
search results. Some of them are shown below: 
Michael (Jeffrey) Jordan, Basketball Player 
Michael (I.) Jordan, Professor of Berkeley 
Michael (B.) Jordan, American Actor 
The name ambiguity has raised serious prob-
lems in many relevant areas, such as web person 
search, data integration, link analysis and know-
ledge base population. For example, in response 
to a person query, search engine returns a long, 
flat list of results containing web pages about 
several namesakes. The users are then forced 
either to refine their query by adding terms, or to 
browse through the search results to find the per-
son they are seeking. Besides, an ever-increasing 
number of question answering and information 
extraction systems are coming to rely on data 
from multi-sources, where name ambiguity will 
lead to wrong answers and poor results. For ex-
ample, in order to extract the birth date of the 
Berkeley professor Michael Jordan, a system 
may return the birth date of his popular name-
sakes, e.g., the basketball player Michael Jordan. 
So there is an urgent demand for efficient, 
high-quality named entity disambiguation me-
thods. Currently, the common methods for 
named entity disambiguation include name ob-
servation clustering (Bagga and Baldwin, 1998) 
and entity linking with knowledge base (McNa-
mee and Dang, 2009). In this paper, we focus on 
the method of name observation clustering. Giv-
en a set of observations O = {o1, o2, ?, on} of the 
target name to be disambiguated, a named entity 
disambiguation system should group them into a 
set of clusters C = {c1, c2, ?, cm}, with each re-
sulting cluster corresponding to one specific enti-
ty. For example, consider the following four ob-
servations of Michael Jordan: 
1) Michael Jordan is a researcher in Computer 
Science. 
2) Michael Jordan plays basketball in Chicago Bulls. 
3) Michael Jordan wins NBA MVP. 
4) Learning in Graphical Models: Michael Jordan. 
A named entity disambiguation system should 
group the 1st and 4th Michael Jordan observations 
into one cluster for they both refer to the Berke-
50
ley professor Michael Jordan, meanwhile group 
the other two Michael Jordan into another clus-
ter as they refer to another person, the Basketball 
Player Michael Jordan. 
To a human, named entity disambiguation is 
usually not a difficult task as he can make deci-
sions depending on not only contextual clues, but 
also the prior background knowledge. For exam-
ple, as shown in Figure 1, with the background 
knowledge that both Learning and Graphical 
models are the topics related to Machine learning, 
while Machine learning is the sub domain of 
Computer science, a human can easily determine 
that the two Michael Jordan in the 1st and 4th ob-
servations represent the same person. In the same 
way, a human can also easily identify that the 
two Michael Jordan in the 2nd and 3rd observa-
tions represent the same person. 
 
Figure 1. The exploitation of knowledge in human 
named entity disambiguation 
The development of systems which could rep-
licate the human disambiguation ability, however, 
is not a trivial task because it is difficult to cap-
ture and leverage the semantic knowledge as 
humankind. Conventionally, the named entity 
disambiguation methods measure the similarity 
between name observations using the bag of 
words (BOW) model (Bagga and Baldwin (1998); 
Mann and Yarowsky (2006); Fleischman and 
Hovy (2004); Pedersen et al (2005)), where a 
name observation is represented as a feature vec-
tor consisting of the contextual terms. This mod-
el measures similarity based on only the co-
occurrence statistics of terms, without consider-
ing all the semantic relations like social related-
ness between named entities, associative related-
ness between concepts, and lexical relatedness 
(e.g., acronyms, synonyms) between key terms. 
 
Figure 2. Part of the link structure of Wikipedia 
Fortunately, in recent years, due to the evolu-
tion of Web (e.g., the Web 2.0 and the Semantic 
Web) and many research efforts for the construc-
tion of knowledge bases, there is an increasing 
availability of large-scale knowledge sources, 
such as Wikipedia and WordNet. These large-
scale knowledge sources create new opportuni-
ties for knowledge-based named entity disam-
biguation methods as they contain rich semantic 
knowledge. For example, as shown in Figure 2, 
the link structure of Wikipedia contains rich se-
mantic relations between concepts. And we be-
lieve that the disambiguation performance can be 
greatly improved by designing algorithms which 
can exploit these knowledge sources at best. 
The problem of these knowledge sources is 
that they are heterogeneous (e.g., they contain 
different types of semantic relations and different 
types of concepts) and most of the semantic 
knowledge within them is embedded in complex 
structures, such as graphs and networks. For ex-
ample, as shown in Figure 2, the semantic rela-
tion between Graphical Model and Computer 
Science is embedded in the link structure of the 
Wikipedia. In recent years, some research has 
investigated to exploit some specific semantic 
knowledge, such as the social connection be-
tween named entities in the Web (Kalashnikov et 
al. (2008), Wan et al (2005) and Lu et al 
(2007)), the ontology connection in DBLP (Has-
sell et al, 2006) and the semantic relations in 
Wikipedia (Cucerzan (2007), Han and Zhao 
(2009)). These knowledge-based methods, how-
ever, usually are specialized to the knowledge 
sources they used, so they often have the know-
ledge coverage problem. Furthermore, these me-
thods can only exploit the semantic knowledge to 
a limited extent because they cannot take the 
structural semantic knowledge into consideration. 
To overcome the deficiencies of previous me-
thods, this paper proposes a knowledge-based 
method, called Structural Semantic Relatedness 
(SSR), which can enhance the named entity dis-
ambiguation by capturing and leveraging the 
structural semantic knowledge from multiple 
knowledge sources. The key point of our method 
is a reliable semantic relatedness measure be-
tween concepts (including WordNet concepts, 
NEs and Wikipedia concepts), called Structural 
Semantic Relatedness, which can capture both 
the explicit semantic relations between concepts 
and the implicit semantic knowledge embedded 
in graphs and networks. In particular, we first 
extract the semantic relations between two con-
cepts from a variety of knowledge sources and 
Computer Science
Machine learning
Statistics 
Graphical model Learning
Mathematic 
Probability Theory 
2) Michael Jordan plays basketball in Chicago Bulls. 
1) Michael Jordan is a researcher in Computer Science.
4) Learning in Graphical Models: Michael Jordan
3) Michael Jordan wins NBA MVP. 
Machine learning 
51
represent them using a graph-based model, se-
mantic-graph. Then based on the principle that 
?two concepts are semantic related if they are 
both semantic related to the neighbor concepts of 
each other?, we construct our Structural Seman-
tic Relatedness measure. In the end, we leverage 
the structural semantic relatedness measure for 
named entity disambiguation and evaluate the 
performance on the standard WePS data sets. 
The experimental results show that our SSR me-
thod can significantly outperform the traditional 
methods. 
This paper is organized as follows. Section 2 
describes how to construct the structural seman-
tic relatedness measure. Next in Section 3 we 
describe how to leverage the captured knowledge 
for named entity disambiguation. Experimental 
results are demonstrated in Sections 4. Section 5 
briefly reviews the related work. Section 6 con-
cludes this paper and discusses the future work. 
2 The Structural Semantic Relatedness 
Measure 
In this section, we demonstrate the structural se-
mantic relatedness measure, which can capture 
the structural semantic knowledge in multiple 
knowledge sources. Totally, there are two prob-
lems we need to address: 
1) How to extract and represent the seman-
tic relations between concepts, since there are 
many types of semantic relations and they may 
exist as different patterns (the semantic know-
ledge may exist as explicit semantic relations or 
be embedded in complex structures). 
2) How to capture all the extracted seman-
tic relations between concepts in our semantic 
relatedness measure. 
To address the above two problems, in follow-
ing we first introduce how to extract the semantic 
relations from multiple knowledge sources; then 
we represent the extracted semantic relations us-
ing the semantic-graph model; finally we build 
our structural semantic relatedness measure. 
2.1 Knowledge Sources 
We extract three types of semantic relations (se-
mantic relatedness between Wikipedia concepts, 
lexical relatedness between WordNet concepts 
and social relatedness between NEs) correspon-
dingly from three knowledge sources: Wikipedia, 
WordNet and NE Co-occurrence Corpus. 
1. Wikipedia1, a large-scale online encyc-
lopedia, its English version includes more than 
3,000,000 concepts and new articles are added 
quickly and up-to-date. Wikipedia contains rich 
semantic knowledge in the form of hyperlinks 
between Wikipedia articles, such as Polysemy 
(disambiguation pages), Synonym (redirect pages) 
and Associative relation (hyperlinks between 
Wikipedia articles). In this paper, we extract the 
semantic relatedness sr between Wikipedia con-
cepts using the method described in Milne and 
Witten(2008): 
log(max( )) log( )
( , ) 1
log( ) log(min( , ))
A B A B
sr a b
W A B
?= ? ?
??
 
where a and b are the two concepts of interest, A 
and B are the sets of all the concepts that are re-
spectively linked to a and b, and W is the entire 
Wikipedia. For demonstration, we show the se-
mantic relatedness between four selected con-
cepts in Table 1. 
 Statistics Basketball
Machine learning 0.58 0.00 
MVP 0.00 0.45 
Table 1. The semantic relatedness table of four se-
lected Wikipedia concepts 
2. WordNet 3.02 (Fellbaum et al, 1998), a 
lexical knowledge source includes over 110,000 
WordNet concepts (word senses about English 
words). Various lexical relations are recorded 
between WordNet concepts, such as hyponyms, 
holonym and synonym. The lexical relatedness lr 
between two WordNet concepts are measured 
using the Lin (1998)?s WordNet semantic simi-
larity measure. Table 2 shows some examples of 
the lexical relatedness. 
 school science 
university 0.67 0.10 
research 0.54 0.39 
Table 2. The lexical relatedness table of four selected 
WordNet concepts 
3. NE Co-occurrence Corpus, a corpus of 
documents for capturing the social relatedness 
between named entities. According to the fuzzy 
set theory (Baeza-Yates et al, 1999), the degree 
of named entities co-occurrence in a corpus is a 
measure of the relatedness between them. For 
example, in Google search results, the ?Chicago 
Bulls? co-occurs with ?NBA? in more than 
                                                 
1 http://www.wikipedia.org/ 
2 http:// wordnet.princeton.edu/ 
52
7,900,000 web pages, while only co-occurs with 
?EMNLP? in less than 1,000 web pages. So the 
co-occurrence statistics can be used to measure 
the social relatedness between named entities. In 
this paper, given a NE Co-occurrence Corpus D, 
the social relatedness scr between two named 
entities ne1 and ne2 is measured using the Google 
Similarity Distance (Cilibrasi and Vitanyi, 2007): 
1 2 1 2
1 2
1 2
log(max( , )) log( )
( , ) 1
log( ) log(min( , ))
D D D D
scr ne ne
D D D
?= ? ?
?  
where D1 and D2 are the document sets corres-
pondingly containing ne1 and ne2. An example of 
social relatedness is shown in Table 3, which is 
computed using the Web corpus through Google. 
 ACL NBA 
EMNLP 0.61 0.00 
Chicago Bulls 0.19 0.55 
Table 3. The social relatedness table of four selected 
named entities 
2.2 The Semantic-Graph Model 
In this section we present a graph-based repre-
sentation, called semantic-graph, to model the 
extracted semantic relations as a graph within 
which the semantic relations are interconnected 
and transitive. Concretely, the semantic-graph is 
defined as follows: 
A semantic-graph is a weighted graph G = (V, 
E), where each node represents a distinct con-
cept; and each edge between a pair of nodes 
represents the semantic relation between the 
two concepts corresponding to these nodes, 
with the edge weight indicating the strength of 
the semantic relation. 
For demonstration, Figure 3 shows a semantic-
graph which models the semantic knowledge 
extracted from Wikipedia for the Michael Jordan 
observations in Section 1. 
 
Figure 3. An example of semantic-graph 
Given a set of name observations, the con-
struction of semantic-graph takes two steps: con-
cept extraction and concept connection. In the 
following we respectively describe each step. 
1) Concept Extraction. In this step we ex-
tract all the concepts in the contexts of name ob-
servations and represent them as the nodes in the 
semantic-graph. We first gather all the N-grams 
(up to 8 words) and identify whether they corres-
pond to semantically meaningful concepts: if a 
N-gram is contained in the WordNet, we identify 
it as a WordNet concept, and use its primary 
word sense as its semantic meaning; to find 
whether a N-gram is a named entity, we match it 
to the named entity list extracted using the open-
Calais API3, which contains more than 30 types 
of named entities, such as Person, Organization 
and Award; to find whether a N-gram is a Wiki-
pedia concept, we match it to the Wikipedia anc-
hor dictionary, then find its corresponding Wiki-
pedia concept using the method described in 
(Medelyan et al 2008). After concept identifica-
tion, we filter out all the N-grams which do not 
correspond to the semantic meaningful concepts, 
such as the N-grams ?learning in? and ?wins 
NBA MVP?. The retained N-grams are identified 
as concepts, corresponding with their semantic 
meanings (a concept may have multiple semantic 
meaning explanation, e.g., the ?MVP? has three 
semantic meaning, as ?most valuable player, 
MVP? in WordNet, as the ?Most Valuable Play-
er? in Wikipedia and as a named entity of Award 
type). 
2) Concept Connection. In this step we 
represent the semantic relations as the edges be-
tween nodes. That is, for each pair of extracted 
concepts, we identify whether there are semantic 
relations between them: 1) If there is only one 
semantic relation between them, we connect 
these two concepts with an edge, where the edge 
weight is the strength of the semantic relation; 2) 
If there is more than one semantic relations be-
tween them, we choose the most reliable seman-
tic relation, i.e., we choose the semantic relation 
in the knowledge sources according to the order 
of WordNet, Wikipedia and NE Co-concurrence 
corpus (Suchanek et al, 2007). For example, if 
both Wikipedia and WordNet provide the seman-
tic relation between MVP and NBA, we choose 
the semantic relation provided by WordNet. 
                                                 
3 http://www.opencalais.com/ 
Researcher Graphical  
Model 
Learning 
NBA
MVP
Basketball 
Chicago Bulls 
Computer 
Science 
0.32 0.28 
0.48 
0.41 
0.58 
0.76 
0.45 
0.71 
0.71 0.57 
53
2.3 The Structural Semantic Relatedness 
Measure 
In this section, we describe how to capture the 
semantic relations between the concepts in se-
mantic-graph using a semantic relatedness meas-
ure. Totally, the semantic knowledge between 
concepts is modeled in two forms: 
1) The edges of semantic-graph. The 
edges model the direct semantic relations be-
tween concepts. We call this form of semantic 
knowledge as explicit semantic knowledge. 
2) The structure of semantic-graph. Ex-
cept for the edges, the structure of the semantic-
graph also models the semantic knowledge of 
concepts. For example, the neighbors of a con-
cept represent all the concepts which are explicit-
ly semantic-related to this concept; and the paths 
between two concepts represent all the explicit 
and implicit semantic relations between them. 
We call this form of semantic knowledge as 
structural semantic knowledge, or implicit se-
mantic knowledge. 
Therefore, in order to deduce a reliable seman-
tic relatedness measure, we must take both the 
edges and the structure of semantic-graph into 
consideration. Under the semantic-graph model, 
the measurement of semantic relatedness be-
tween concepts equals to quantifying the similar-
ity between nodes in a weighted graph. To simpl-
ify the description, we assign each node in se-
mantic-graph an integer index from 1 to |V| and 
use this index to represent the node, then we can 
write the adjacency matrix of the semantic-graph 
G as A, where A[i,j] or Aij is the edge weight be-
tween node i and node j. 
The problem of quantifying the relatedness be-
tween nodes in a graph is not a new problem, e.g., 
the structural equivalence and structural similar-
ity (the SimRank in Jeh and Widom (2002) and 
the similarity measure in Leicht et al (2006)). 
However, these similarity measures are not suit-
able for our task, because all of them assume that 
the edges are uniform so that they cannot take 
edge weight into consideration. 
In order to take both the graph structure and 
the edge weight into account, we design the 
structural semantic relatedness measure by ex-
tending the measure introduced in Leicht et al 
(2006). The fundamental principle behind our 
measure is ?a node u is semantically related to 
another node v if its immediate neighbors are 
semantically related to v?. This definition is natu-
ral, for example, as shown in Figure 3, the con-
cept Basketball and its neighbors NBA and Chi-
cago Bulls are all semantically related to MVP. 
This definition is recursive, and the starting point 
we choose is the semantic relatedness in the edge. 
Thus our structural semantic relatedness has two 
components: the neighbor term of the previous 
recursive phase which captures the graph struc-
ture and the semantic relatedness which captures 
the edge information. Thus, the recursive form of 
the structural semantic relatedness Sij between 
the node i and the node j can be written as: 
i
il
ij lj ij
l N i
A
S S A
d
? ?
?
= +?  
where ?  and ?  control the relative importance 
of the two components and 
Ni={j | Aij > 0} is the set of the immediate 
neighbors of node i; 
j Ni
d Aiji ?
?= is the degree of node i. 
In order to solve this formula, we introduce the 
following two notations: 
T: The relatedness transition matrix, where 
T[i,j]=Aij/di, indicating the transition rate of re-
latedness from node j to its neighbor i. 
S: The structural semantic relatedness matrix, 
where S[i,j]=Sij. 
Now we can turn our first form of structural se-
mantic relatedness into the matrix form: 
S TS A? ?= +  
By solving this equation, we can get: 
1( )S I T A? ? ?= ?  
where I is the identity matrix. Since ?  is a pa-
rameter which only contributes an overall scale 
factor to the relatedness value, we can ignore it 
and get the final form of the structural semantic 
relatedness as: 
1( )S I T A? ?= ?  
Because the S is asymmetric, the finally related-
ness between node i and node j is the average of 
Sij and Sji. 
The meaning of ? : The last question of our 
structural semantic relatedness measure is how to 
set the free parameter ? . To understand the 
meaning of ? , let us expand the similarity as a 
power series thus: 
2 2( ... ...)k kS I T T T A? ? ?= + + + + +  
Noting that the [Tk]ij element is the relatedness 
transition rate from node i to node j with path 
length k, we can view the ?  as a penalty factor 
for the transition path length: by setting the ?  
with a value within (0, 1), a longer graph path 
will contribute less to the final relatedness value. 
The optimal value of ?  is 0.6 through a learning 
54
process shown in Section 4. For demonstration, 
Table 4 shows some structural semantic related-
ness values of the Semantic-graph in Figure 3 
(CS represents computer science and GM 
represents Graphical model). From Table 4, we 
can see that the structural semantic relatedness 
can successfully capture the semantic knowledge 
embedded in the structure of semantic-graph, 
such as the implicit semantic relation between 
Researcher and Learning. 
 Researcher CS GM Learning
Researcher --- 0.50 0.27 0.31 
CS 0.50 --- 0.62 0.73 
GM 0.27 0.62 --- 0.80 
Learning 0.31 0.73 0.80 --- 
Table 4. The structural semantic relatedness of the 
semantic-graph shown in Figure 3 
3 Named Entity Disambiguation by Le-
veraging Semantic Knowledge 
In this section we describe how to leverage the 
semantic knowledge captured in the structural 
semantic relatedness measure for named entity 
disambiguation. Because the key problem of 
named entity disambiguation is to measure the 
similarity between name observations, we inte-
grate the structural semantic relatedness in the 
similarity measure, so that it can better reflect the 
actual similarity between name observations. 
Concretely, our named entity disambiguation 
system works as follows: 1) Measuring the simi-
larity between name observations; 2) Grouping 
name observations using the clustering algorithm. 
In the following we describe each step in detail. 
3.1 Measuring the Similarity between Name 
Observations 
Intuitively, if two observations of the target name 
represent the same entity, it is highly possible 
that the concepts in their contexts are closely re-
lated, i.e., the named entities in their contexts are 
socially related and the Wikipedia concepts in 
their contexts are semantically related. In con-
trast, if two name observations represent differ-
ent entities, the concepts within their contexts 
will not be closely related. Therefore we can 
measure the similarity between two name obser-
vations by summarizing all the semantic related-
ness between the concepts in their contexts. 
To measure the similarity between name ob-
servations, we represent each name observation 
as a weighted vector of concepts (including 
named entities, Wikipedia concepts and Word-
Net concepts), where the concepts are extracted 
using the same method described in Section 2.2, 
so they are just the same concepts within the se-
mantic-graph. Using the same concept index as 
the semantic-graph, a name observation oi is then 
represented as 1 2{ , ,..., }i i i ino w w w= , where wik is 
the kth concept?s weight in observation oi, com-
puted using the standard TFIDF weight model, 
where the DF is computed using the Google 
Web1T 5-gram corpus4. Given the concept vec-
tor representation of two name observations oi 
and oj, their similarity is computed as: 
( , )i j il jk lk il jk
l k l k
SIM o o w w S w w=?? ??  
which is the weighted average of all the structur-
al semantic relatedness between the concepts in 
the contexts of the two name observations. 
3.2 Grouping Name Observations through 
Hierarchical Agglomerative Clustering 
Given the computed similarities, name observa-
tions are disambiguated by grouping them ac-
cording to their represented entities. In this paper, 
we group name observations using the hierar-
chical agglomerative clustering(HAC) algorithm, 
which is widely used in prior disambiguation 
research and evaluation task (WePS1 and 
WePS2). The HAC produce clusters in a bottom-
up way as follows: Initially, each name observa-
tion is an individual cluster; then we iteratively 
merge the two clusters with the largest similarity 
value to form a new cluster until this similarity 
value is smaller than a preset merging threshold 
or all the observations reside in one common 
cluster. The merging threshold can be deter-
mined through cross-validation. We employ the 
single-link method to compute the similarity be-
tween two clusters, which has been applied wide-
ly in prior research (Bagga and Baldwin (1998); 
Mann and Yarowsky (2003)). 
4 Experiments 
To assess the performance of our method and 
compare it with traditional methods, we conduct 
a series of experiments. In the experiments, we 
evaluate the proposed SSR method on the task of 
personal name disambiguation, which is the most 
common type of named entity disambiguation. In 
the following, we first explain the general expe-
rimental settings in Section 4.1, 4.2 and 4.3; then 
evaluate and discuss the performance of our me-
thod in Section 4.4. 
                                                 
4 www.ldc.upenn.edu/Catalog/docs/LDC2006T13/ 
55
4.1 Disambiguation Data Sets 
We adopted the standard data sets used in the 
First Web People Search Clustering Task 
(WePS1) (Artiles et al, 2007) and the Second 
Web People Search Clustering Task (WePS2) 
(Artiles et al, 2009). The three data sets we used 
are WePS1_training data set, WePS1_test data 
set, and WePS2_test data set. Each of the three 
data sets consists of a set of ambiguous personal 
names (totally 109 personal names); and for each 
name, we need to disambiguate its observations 
in the web pages of the top N (100 for WePS1 
and 150 for WePS2) Yahoo! search results. 
The experiment made the standard ?one per-
son per document? assumption, which is widely 
used in the participated systems in WePS1 and 
WePS2, i.e., all the observations of the same 
name in a document are assumed to represent the 
same entity. Based on this assumption, the fea-
tures within the entire web page are used to dis-
ambiguate personal names. 
4.2 Knowledge Sources 
There were three knowledge sources we used for 
our experiments: the WordNet 3.0; the Sep. 9, 
2007 English version of Wikipedia; and the Web 
pages of each ambiguous name in WePS datasets 
as the NE Co-occurrence Corpus. 
4.3 Evaluation Criteria 
We adopted the measures used in WePS1 to eva-
luate the performance of name disambiguation. 
These measures are: 
Purity (Pur): measures the homogeneity of 
name observations in the same cluster; 
Inverse purity (Inv_Pur): measures the com-
pleteness of a cluster; 
F-Measure (F): the harmonic mean of purity 
and inverse purity. 
The detailed definitions of these measures can 
be found in Amigo, et al (2008). We use F-
measure as the primary measure just liking 
WePS1 and WePS2. 
4.4 Experimental Results 
We compared our method with four baselines: (1) 
BOW: The first one is the traditional Bag of 
Words model (BOW) based methods: hierarchic-
al agglomerative clustering (HAC) over term 
vector similarity, where the features including 
single words and NEs, and all the features are 
weighted using TFIDF. This baseline is also the 
state-of-art method in WePS1 and WePS2. (2) 
SocialNetwork: The second one is the social 
network based methods, which is the same as the 
method described in Malin et al (2005): HAC 
over the similarity obtained through random 
walk over the social network built from the web 
pages of the top N search results. (3)SSR-
NoKnowledge: The third one is used as a base-
line for evaluating the efficiency of semantic 
knowledge: HAC over the similarity computed 
on semantic-graph with no knowledge integrated, 
i.e., the similarity is computed as: 
( , )i j il jl il jk
l l k
SIM o o w w w w=? ??  
(4) SSR-NoStructure: The fourth one is used as 
a baseline for evaluating the efficiency of the 
semantic knowledge embedded in complex struc-
tures: HAC over the similarity computed by only 
integrating the explicit semantic relations, i.e., 
the similarity is computed as: 
( , )i j il jk lk il jk
l k l k
SIM o o w w A w w=?? ??  
4.4.1 Overall Performance 
We conducted several experiments on all the 
three WePS data sets: the four baselines, the pro-
posed SSR method and the proposed SSR me-
thod with only one special type knowledge added, 
respectively SSR-NE, SSR-WordNet and SSR-
Wikipedia. All the optimal merging thresholds 
used in HAC were selected by applying leave-
one-out cross validation. The overall perfor-
mance is shown in Table 5. 
Method 
WePS1_training
Pur Inv_Pur F
BOW 0.71 0.88 0.78
SocialNetwork 0.66 0.98 0.76
SSR-NoKnowledge 0.79 0.89 0.81
SSR-NoStructure 0.87 0.83 0.83
SSR-NE 0.80 0.86 0.82
SSR-WordNet 0.80 0.91 0.83
SSR-Wikipedia 0.82 0.90 0.84
SSR 0.82 0.92 0.85
WePS1_test 
Pur Inv_Pur F
BOW 0.74 0.87 0.74
SocialNetwork 0.83 0.63 0.65
SSR-NoKnowledge 0.80 0.74 0.75
SSR-NoStructure 0.80 0.78 0.78
SSR-NE 0.73 0.80 0.74
SSR-WordNet 0.81 0.77 0.77
SSR-Wikipedia 0.88 0.77 0.81
SSR 0.85 0.83 0.84
WePS2_test 
Pur Inv_Pur F
BOW 0.80 0.80 0.77
SocialNetwork 0.62 0.93 0.70
SSR-NoKnowledge 0.84 0.80 0.80
SSR-NoStructure 0.84 0.83 0.81
SSR-NE 0.78 0.88 0.80
SSR-WordNet 0.85 0.82 0.83
SSR-Wikipedia 0.84 0.81 0.82
SSR 0.89 0.84 0.86
Table 5. Performance results of baselines and SSR 
methods 
56
From the performance results in Table 5, we 
can see that: 
1) The semantic knowledge can greatly im-
prove the disambiguation performance: com-
pared with the BOW and the SocialNetwork 
baselines, SSR respectively gets 8.7% and 14.7% 
improvement on average on the three data sets. 
2) By leveraging the semantic knowledge 
from multiple knowledge sources, we can obtain 
a better named entity disambiguation perfor-
mance: compared with the SSR-NE?s 0% im-
provement, the SSR-WordNet?s 2.3% improve-
ment and the SSR-Wikipedia?s 3.7% improve-
ment, the SSR gets 6.3% improvement over the 
SSR-NoKnowledge baseline, which is larger than 
all the SSR methods with only one type of se-
mantic knowledge integrated. 
3) The exploitation of the structural seman-
tic knowledge can further improve the disambig-
uation performance: compared with SSR-
NoStructure, our SSR method achieves 4.3% im-
provement. 
 
Figure 4. The F-Measure vs. ?  on three data sets 
4.4.2 Optimizing Parameters 
There is only one parameter ?  needed to be con-
figured, which is the penalty factor for the rela-
tedness transition path length in the structural 
semantic relatedness measure. Usually a smaller 
?  will make the structural semantic knowledge 
contribute less in the resulting relatedness value. 
Figure 4 plots the performance of our method 
corresponding to the special ?  settings. As 
shown in Figure 4, the SSR method is not very 
sensitive to the ?  and can achieve its best aver-
age performance when the value of ?  is 0.6. 
4.4.3 Detailed Analysis 
To better understand the reasons why our SSR 
method works well and how the exploitation of 
structural semantic knowledge can improve per-
formance, we analyze the results in detail. 
The Exploitation of Semantic Knowledge. The 
primary advantage of our method is the exploita-
tion of semantic knowledge. Our method exploits 
the semantic knowledge in two directions: 
1) The Integration of Multiple Semantic 
Knowledge Sources. Using the semantic-graph 
model, our method can integrate the semantic 
knowledge extracted from multiple knowledge 
sources, while most traditional knowledge-based 
methods are usually specialized to one type of 
knowledge. By integrating multiple semantic 
knowledge sources, our method can improve the 
semantic knowledge coverage. 
2) The exploitation of Semantic Knowledge 
embedded in complex structures. Using the struc-
tural semantic relatedness measure, our method 
can exploit the implicit semantic knowledge em-
bedded in complex structures; while traditional 
knowledge-based methods usually lack this abili-
ty. 
The Rich Meaningful Features. One another 
advantage of our method is the rich meaningful 
features, which is brought by the multiple seman-
tic knowledge sources. With more meaningful 
features, our method can better describe the 
name observations with less information loss. 
Furthermore, unlike the traditional N-gram fea-
tures, the features enriched by semantic know-
ledge sources are all semantically meaningful 
units themselves, so little noisy features will be 
added. The effect of rich meaningful features can 
also be shown in Table 5: by adding these fea-
tures, the SSR-NoKnowledge respectively 
achieves 2.3% and 9.7% improvement over the 
BOW and the SocialNetwork baseline. 
5 Related Work 
In this section, we briefly review the related 
work. Totally, the traditional named entity dis-
ambiguation methods can be classified into two 
categories: the shallow methods and the know-
ledge-based methods. 
Most of previous named entity disambiguation 
researches adopt the shallow methods, which are 
mostly the natural extension of the bag of words 
(BOW) model. Bagga and Baldwin (1998) 
represented a name as a vector of its contextual 
words, then two names were predicted to be the 
same entity if their cosine similarity is above a 
threshold. Mann and Yarowsky (2003) and Niu 
et al (2004) extended the vector representation 
with extracted biographic facts. Pedersen et al 
(2005) employed significant bigrams to represent 
57
a name observation. Chen and Martin (2007) ex-
plored a range of syntactic and semantic features. 
In recent years some research has investigated 
employing knowledge sources to enhance the 
named entity disambiguation. Bunescu and Pasca 
(2006) disambiguated the names using the cate-
gory information in Wikipedia. Cucerzan (2007) 
disambiguated the names by combining the BOW 
model with the Wikipedia category information. 
Han and Zhao (2009) leveraged the Wikipedia 
semantic knowledge for computing the similarity 
between name observations. Bekkerman and 
McCallum (2005) disambiguated names based 
on the link structure of the Web pages between a 
set of socially related persons. Kalashnikov et al 
(2008) and Lu et al (2007) used the co-
occurrence statistics between named entities in 
the Web. The social network was also exploited 
for named entity disambiguation, where similari-
ty is computed through random walking, such as 
the work introduced in Malin (2005), Malin and 
Airoldi (2005), Yang et al(2006) and Minkov et 
al. (2006). Hassell et al (2006) used the relation-
ships from DBLP to disambiguate names in re-
search domain. 
6 Conclusions and Future Works 
In this paper we demonstrate how to enhance the 
named entity disambiguation by capturing and 
exploiting the semantic knowledge existed in 
multiple knowledge sources. In particular, we 
propose a semantic relatedness measure, Struc-
tural Semantic Relatedness, which can capture 
both the explicit semantic relations and the im-
plicit structural semantic knowledge. The expe-
rimental results on the WePS data sets demon-
strate the efficiency of the proposed method. For 
future work, we want to develop a framework 
which can uniformly model the semantic know-
ledge and the contextual clues for named entity 
disambiguation. 
Acknowledgments 
The work is supported by the National Natural 
Science Foundation of China under Grants no. 
60875041 and 60673042, and the National High 
Technology Development 863 Program of China 
under Grants no. 2006AA01Z144. 
References  
Amigo, E., Gonzalo, J., Artiles, J. and Verdejo, F. 
2008. A comparison of extrinsic clustering evalua-
tion metrics based on formal constraints. Informa-
tion Retrieval. 
Artiles, J., Gonzalo, J. & Sekine, S. 2007. The Se-
mEval-2007 WePS Evaluation: Establishing a 
benchmark for the Web People Search Task. In 
SemEval. 
Artiles, J., Gonzalo, J. and Sekine, S. 2009. WePS2 
Evaluation Campaign: Overview of the Web 
People Search Clustering Task. In WePS2, WWW 
2009. 
Baeza-Yates, R., Ribeiro-Neto, B., et al 1999. Mod-
ern information retrieval. Addison-Wesley Reading, 
MA. 
Bagga, A. & Baldwin, B. 1998. Entity-based cross-
document coreferencing using the vector space 
model. Proceedings of the 17th international confe-
rence on Computational linguistics-Volume 1, pp. 
79-85. 
Bekkerman, R. & McCallum, A. 2005. Disambiguat-
ing web appearances of people in a social network. 
Proceedings of the 14th international conference on 
World Wide Web, pp. 463-470. 
Bunescu, R. & Pasca, M. 2006. Using encyclopedic 
knowledge for named entity disambiguation. Pro-
ceedings of EACL, vol. 6.  
Chen, Y. & Martin, J. 2007. Towards robust unsuper-
vised personal name disambiguation. Proceedings 
of EMNLP and CoNLL, pp. 190-198. 
Cilibrasi, R. L., Vitanyi, P. M. & CWI, A. 2007. The 
google similarity distance, IEEE Transactions on 
knowledge and data engineering, vol. 19, no. 3, pp. 
370-383. 
Cucerzan, S. 2007, Large-scale named entity disam-
biguation based on Wikipedia data. Proceedings of 
EMNLP-CoNLL, pp. 708-716. 
Fellbaum, C., et al 1998. WordNet: An electronic 
lexical database. MIT press Cambridge, MA. 
Fleischman, M. B. & Hovy, E. 2004. Multi-document 
person name resolution. Proceedings of ACL, Ref-
erence Resolution Workshop. 
Han, X. & Zhao, J. 2009. Named entity disambigua-
tion by leveraging Wikipedia semantic knowledge. 
Proceeding of the 18th ACM conference on Infor-
mation and knowledge management, pp. 215-224. 
Hassell, J., Aleman-Meza, B. & Arpinar, I. 2006. On-
tology-driven automatic entity disambiguation in 
unstructured text. Proceedings of The 2006 ISWC, 
pp. 44-57. 
Jeh, G. & Widom, J. 2002. SimRank: A measure of 
structural-context similarity, Proceedings of the 
eighth ACM SIGKDD international conference on 
Knowledge discovery and data mining, p. 543.  
58
Kalashnikov, D. V., Nuray-Turan, R. & Mehrotra, S. 
2008. Towards Breaking the Quality Curse. A 
Web-Querying Approach to Web People Search. In 
Proc. of SIGIR. 
Leicht, E. A.,  Petter Holme,  & M. E. J. Newman. 
2006. Vertex similarity in networks. Physical Re-
view E , vol. 73, no. 2, p. 26120. 
Lin., D. 1998. An information-theoretic definition of 
similarity. In Proc. of ICML. 
Lu, Y. & Nie , Z. et al 2007. Name Disambiguation 
Using Web Connection. In Proc. of AAAI. 
Malin, B. 2005. Unsupervised name disambiguation 
via social network similarity. SIAM SDM Work-
shop on Link Analysis, Counterterrorism and Secu-
rity. 
Malin, B., Airoldi, E. & Carley, K. M. 2005. A net-
work analysis model for disambiguation of names 
in lists. Computational & Mathematical Organiza-
tion Theory, vol. 11, no. 2, pp. 119-139. 
Mann, G. S. & Yarowsky, D. 2003. Unsupervised 
personal name disambiguation, Proceedings of the 
seventh conference on Natural language learning at 
HLT-NAACL 2003-Volume 4, p. 40. 
McNamee, P. & Dang, H. Overview of the TAC 2009 
Knowledge Base Population Track. In Proceedings 
of Text Analysis Conference (TAC-2009), 2009. 
Medelyan, O., Witten, I. H. & Milne, D. 2008. Topic 
indexing with Wikipedia. Proceedings of the AAAI 
WikiAI workshop. 
Milne, D., Medelyan, O. & Witten, I. H. 2006. Min-
ing domain-specific thesauri from wikipedia: A 
case study. IEEE/WIC/ACM International Confe-
rence on Web Intelligence, pp. 442-448. 
Minkov, E., Cohen, W. W. & Ng, A. Y. 2006. Con-
textual search and name disambiguation in email 
using graphs, Proceedings of the 29th annual inter-
national ACM SIGIR conference on Research and 
development in information retrieval, pp. 27-34. 
Niu C., Li W. and Srihari, R. K. 2004. Weakly Super-
vised Learning for Cross-document Person Name 
Disambiguation Supported by Information Extrac-
tion. Proceedings of ACL, pp. 598-605. 
Pedersen, T., Purandare, A. & Kulkarni, A. 2005. 
Name discrimination by clustering similar contexts. 
Computational Linguistics and Intelligent Text 
Processing, pp. 226-237. 
Strube, M. & Ponzetto, S. P. 2006. WikiRelate! Com-
puting semantic relatedness using Wikipedia, Pro-
ceedings of the National Conference on Artificial 
Intelligence, vol. 21, no. 2, p. 1419. 
Suchanek, F. M., Kasneci, G. & Weikum, G. 2007. 
Yago: a core of semantic knowledge, Proceedings 
of the 16th international conference on World 
Wide Web, p. 706. 
Wan, X., Gao, J., Li, M. & Ding, B. 2005. Person 
resolution in person search results: Webhawk. Pro-
ceedings of the 14th ACM international conference 
on Information and knowledge management, p. 
170. 
Witten, D. M. & Milne, D. 2008. An effective, low-
cost measure of semantic relatedness obtained from 
Wikipedia links. Proceeding of AAAI Workshop 
on Wikipedia and Artificial Intelligence: an Evolv-
ing Synergy, AAAI Press, Chicago, USA, pp. 25-
30. 
Yang, K. H., Chiou, K. Y., Lee, H. M. & Ho, J. M. 
2006. Web appearance disambiguation of personal 
names based on network motif. Proceedings of the 
2006 IEEE/WIC/ACM International Conference on 
Web Intelligence, pp. 386-389. 
 
59
