Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 123?127,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
KUNLP Grammatical Error Correction System
For CoNLL-2013 Shared Task
Bong-Jun Yi, Ho-Chang Lee, and Hae-Chang Rim
Department of Computer and Radio Communications Engineering
Korea University
Anam-dong 5-ga, Seongbuk-gu, Seoul, South Korea
{bjyi,hclee,rim}@nlp.korea.ac.kr
Abstract
This paper describes an English grammat-
ical error correction system for CoNLL-
2013 shared task. Error types covered by
our system are article/determiner, prepo-
sition, and noun number agreement. This
work is our first attempt on grammatical
error correction research. In this work,
we only focus on reimplementing the tech-
niques presented before and optimizing
the performance. As a result of the imple-
mentation, our system?s final F1-score by
m2 scorer is 0.1282 in our internal test set.
1 Introduction
As the number of English learners is increas-
ing world widely, the research topic of auto-
mated grammar error correction is lively dis-
cussed. However, automated grammar error cor-
rection is a very difficult field and the result is not
satisfactory. Therefore, the shared task about En-
glish error correction has been annually held and
many researchers are trying to solve this problem.
Helping Our Own (HOO) 2011 is a pilot shared
task for automated correction of errors in non-
native English speakers? papers. The shared task
evaluates the performance of detection, recogni-
tion, correction on thirteen types of English gram-
matical errors by using F1-score. Because each
error type has different characteristics, they have
to use different approaches to correct appropriate
error types.
In HOO 2012, only two types of errors, prepo-
sition and determiner were handled. This shared
task also evaluated the performance of detection,
recognition, correction by using F1-score. The
best result of the preposition error correction is
0.2371 in F1-score and the determiner error cor-
rection is 0.3460 in F1-score. These are remark-
able achievement.
This year CoNLL 2013 shared task covers five
types of errors based on the result of HOO 2012.
These error types are determiner, preposition,
noun number, verb form, and subject-verb agree-
ment. Because of the limited amount of time and
manpower, we only focus on preposition, deter-
miner and noun number.
2 Previous Works
Most methods for grammar error correction have
tried to correct one type of errors. Researchers
have never attempted to correct different types of
errors at the same time.
In this work, we try to solve the error correction
problem based on the previous research presenting
good performance.
First, the preposition error correction is based
on (Han et al, 2010). They tried to correct the
most commonly used 10 preposition errors based
on the classification approach. 10 prepositions are
about, at, by, for, from, in, of, on, to, with. They
have implemented 11-way classifier to output 11
types of proper word(10 prepositions + ?NULL?)
for 11 types of source words. This work assumes
that three kinds of corrections exist. If ?NULL? is
taken as input and some preposition is produced, it
is omission. If some preposition is taken as input
and another preposition is produced, it is replace-
ment. If some preposition is taken as input and
?NULL? is produced, it is commission. In the case
of replacement, correction precision is 0.817 and
recall is 0.132. Furthermore, they reported that
the performance is much better when they train the
model with well edited error tagged corpus.
(Felice and Pulman, 2008) also used a method
based on classification. It is, nevertheless, unusual
that they did not use error tagged learner?s corpus
but error free British National Corpus. Without
using an error tagged corpus, they have achieved
51.5% accuracy for error correction.
To improve low recall of Han?s method, to con-
123
struct large training data is the best way. However,
it is very costly and hard work to obtain well edited
error tagged corpus. By the way, error free corpus
like news articles is relatively easy to acquire. We
plan to utilize large error free corpus as the train-
ing data to overcome the problem of low recall.
That plan motivated by Felice?s work has not been
tried on the proposed system. We will attempt to
reimplement the system by utilizing the error free
corpus in the near future.
3 System Description
Our system is composed of three components,
preposition error corrector, article error corrector,
and noun number error corrector. In this work,
we do not consider complex cases of grammar er-
rors, thus we assume that the order of correction
does not influence the result of correction. And
all components are based on the machine learning
method.
3.1 Preposition Error Correction
In the training corpus, there are more article errors
than preposition errors in number. However, the
preposition error correction is much more difficult
and the performance of correction is worse than
the article error correction.
We select preposition error candidates for re-
placement or commission or omission as follows.
? Replacement or Commission
? Preposition : tagged as ?IN? or ?TO?
and dependency relation with its par-
ent(DPREL) is identified as a ?prep?
? Omission
? In front of a noun phrase : the preceding
word of the noun phrase is not preposi-
tion
? In front of a verb phrase : the preced-
ing word of the verb phrase including
?VBG?(verb, gerund/present participle)
is not preposition
As described above, we use all
words(preposition and ?NULL? when omis-
sion) in that place as source word for preposition
correction.
We have implemented only one classifier that
takes a source word as input and produces cor-
rected preposition or ?NULL? as output. We use
No. Feature Name Description
1 s the source word
2 wd-1 the word preceding s
3 wd+1 the word following s
4 wd-1,2 s the two words preceding s
and s
5 s wd+1,2 s and the two words fol-
lowing s
6 3GRAM the trigram, wd-1, s,
wd+1
7 5GRAM the five-gram, wd-2, wd-
1, s, wd+1, wd+2
8 MOD the lexical head which the
preposition modifies
9 ARG the lexical head of the
preposition argument
10 MOD ARG MOD and ARG
11 MOD s MOD and s
12 s ARG s and ARG
13 MOD s ARG MOD, s, ARG
14 MODt ARGt POS tags of MOD and
ARG
15 MODt s the POS tag of MOD and
s
16 s ARGt s and the POS tag of ARG
17 MODt s ARGt MODt and s and ARGt
18 TRIGRAMt POS trigram
19 wd L 3 words preceding s
20 wd R 3 words following s
Table 1: Set of features proposed by (Han et al,
2010)
the part of feature set(Table 1) proposed by (Han
et al, 2010) for learning. They are presented in
the experiment part.
Each feature represents the word itself in the
Han?s work. However, the same word can be ex-
tracted again as a different kind of features. In or-
der to distinguish the same word used for the dif-
ferent features, we attach the feature name to the
word as postfix. This naming convention can make
the feature sparse, but increase the discrimination
power and improve the performance of the classi-
fier. In our experiment, we have tested the system
with two different sets of features(i.e. raw word
and with feature names).
124
3.2 Article Error Correction
We have implemented the article error corrector
just like the preposition error corrector. When we
experiment the pilot article correction system just
like the preposition correction system, it shows a
good performance unexpectedly. There is a little
difference in presenting set of features. In prepo-
sition error corrector, we add postfix to the set of
feature to keep sort of features(e.g. word ?in? as
source word feature, postfix is ?S?, final feature
is ?in S?). This method gives more discrimination
ability to the classifier. But in case of article, using
raw word lead to a better result.
3.3 Noun Number Error Correction
Noun number error indicates improper use of
singular or plural form of nouns. For example,
the singular form ?problem? should be corrected
to the plural form ?problems? in the following
sentence.
?They are educational and resource problem.?
As far as we know, there have been few at-
tempts to correct noun number agreement errors.
In this shared task, we propose a novel noun num-
ber agreement correction system based on a ma-
chine learning method trained with basic features.
In order to extract nouns from the input sen-
tences, we parse the sentence and extract the last
noun in every noun phrase for the error correction
candidates. If there is a coordinating conjunction
in the noun phrase, we split the noun phrase into
two parts and extract two candidates.
[S [NP Relevant information] [VP are [ADJP
readily ROOT available [PP on [NP [NP the
internet] and [NP article] [PP in [NP maga-
zines and newspapers] .]]]
Figure 1: Extracting candidates for error correc-
tion of noun number(candidates are indicated by
bold)
Figure 1 shows the example of selecting can-
didates for the error correction of noun number.
We classify a noun into four classes using fea-
tures of Table 2 based on the machine learn-
ing method. Four classes are NN(plural noun),
NNP(plural proper noun), NNS(singular noun),
and NNPS(singular proper noun). It is based on
the observation that the common noun and the
No. Feature Name Description
1 p POS tag for the source word
2 s the source word
3 DET the determiner of the noun ar-
gument
4 CD boolean value whether the
cardinal number exists
5 UCNT boolean value whether the
noun is uncountable
6 MOD the lexical head which the
noun modifies
7 MMOD the lexical head which MOD
modifies
8 ARG the lexical head of the noun
argument
9 AARG the lexical head of the ARG
argument
10 MOD s ARG MOD, s, and ARG
11 MODt POS tag for MOD
12 MMODt POS tag for MMOD
13 ARGt POS tag for ARG
14 AARGt POS tag for AARG
15 MODt p ARGt MODt, p, and ARGt
16 MMOD MOD s MMOD, MOD, and s
17 s ARG AARG s, ARG, and AARG
18 MOD s ARG MOD, s, and ARG
19 MM M s A AA MMOD, MOD, s, ARG, and
AARG
20 MMODt MODt p MMODt, MODt, and p
21 p ARGt AARGt p, ARGt, and AARGt
22 MODt p ARGt MODt, p, and ARGt
23 MMt Mt s At AAt MMODt, MODt, s, ARGt,
and AARGt
Table 2: Set of features for learning noun number
error correction
proper noun have many different characteristic.
The set of features used for learning is shown in
Table 2.
4 Experiments
4.1 Corpus
We use only NUS Corpus of Leaner En-
glish(NUCLE)((Dahlmeier, 2013)) provided from
CoNLL 2013 shared task. We construct the devel-
opment set with first sentences for every 10 sen-
tence and the test set with second sentences and
the training set with the rest of sentences. The
system is trained to learn error correction with the
training set and optimized with the development
set and finally evaluated with the test set.
4.2 Preposition Correction Experiment
Table 1 shows 20 types of features used by (Han
et al, 2010). We have found that the features con-
sist of various types and the learning world be dis-
turbed by too many features. In our experiment,
125
Number of feature 20 18 9
Raw
Word
Precision 0.0571 0.1194 0.0196
Recall 0.0402 0.0402 0.1256
F1-score 0.0472 0.0602 0.0339
With
Feature
Name
Precision 0.1034 0.1750 0.0208
Recall 0.0302 0.0352 0.1307
F1-score 0.0467 0.0586 0.0359
Table 3: The result of preposition error correction
we exclude wd L(19), wd R(20) and employ 18
kinds of features.
We will try to train the correction model by us-
ing large amount of error free corpus in order to
overcome the problem of low recall. To parse large
corpus is very time consuming task. So, in this
experiment, we select 9 features which can be ex-
tracted without parsing, and test the possibility of
using 9 features by training and testing the correc-
tion model.
We have performed two different experiments.
In the first experiment, we have used the word it-
self as a feature. In the tables 3?5, ?Raw Word?
represents the case when we use just the word
itself. In the second experiment, we have used
the feature name as the postfix of the feature. In
the tables 3?5, ?With Feature Name? represents
the case when we attach the feature name to the
feature and use it as a feature. For all experi-
ments, we have tried to differentiate the number
of features. 20 features are same as Han?s work.
18 features are the case when we exclude 2 fea-
tures(i.e. wd L(19), wd R(20)). 9 features are the
case when we use only features which do not re-
quire parsing.
We have experimented with Maximum Entropy
learning method, and fixed the iteration number to
200. Table 3 shows that the precision has highly
increased although the recall has decreased when
we add the feature name to the set of features used
for learning.
When we use 18 features except wd L(3 words
preceding s) and wd R(3 words following s), the
error correction system achieves the best perfor-
mance. According to the experimental result, we
can achieve the better result when we use 18 fea-
tures and the raw word. But we select final option
using 18 features and the word with feature name
because of optimization strategies that improve the
precision.
Number of feature 20 18 9
Raw
Word
Precision 0.1827 0.3176 0.0914
Recall 0.1123 0.1264 0.1139
F1-score 0.1391 0.1808 0.1014
With
Feature
Name
Precision 0.1942 0.3174 0.1059
Recall 0.1154 0.1139 0.1061
F1-score 0.1448 0.1676 0.1060
Table 4: The result of article error correction
Kinds of feature Basic
Basic&
Indep-
endent
Basic&
Com-
plex
Raw
Word
Precision 0.2462 0.1435 0.2469
Recall 0.0379 0.0811 0.0540
F1-score 0.0662 0.1036 0.0887
With
Feature
Name
Precision 0.2413 0.1676 0.2875
Recall 0.0378 0.0838 0.0621
F1-score 0.0654 0.1117 0.1022
Table 5: The result of noun number error correc-
tion
4.3 Article Correction Experiment
Table 4 shows that the feature name addition does
not improve the precision in the case of article cor-
rection, and the set of 18 features achieves the best
performance for article correction. Therefore, we
just use raw words for features and select 18 fea-
tures for article correction.
4.4 Noun Number Correction Experiment
In Table 2, features of number 1?5 belong to the
basic feature set and features of number 6?15 be-
long to the independent feature set and features
of number 16?23 belong to the complex feature
set. The experimental result with various combi-
nations of feature sets shows that the set of basic
and complex features achieves the best precision
in spite of low recall as shown in Table 5. We use
this option and experimentally select the iteration
number 700.
5 Conclusions
We develop a grammatical error correction system
which can recognize and correct preposition, arti-
cle, and noun number errors. In this experiment,
we have found out the set of good features for
preposition and article error correction, and pro-
posed a novel noun number error correction tech-
nique based on the machine learning method. For
126
the future work, we will try to utilize large amount
of external resources such as well written error
free corpus.
References
Daniel Dahlmeier, Hwee Tou Ng, Siew Mei Wu 2013.
Building a Large Annotated Corpus of Learner En-
glish: The NUS Corpus of Learner English, Pro-
ceedings of the 8th Workshop on Innovative Use of
NLP for Building Educational Applications (BEA
2013), Atlanta, Georgia, USA.
Rachele De Felice and Stephen G. Pulman 2008. A
classifier-based approach to preposition and deter-
miner error correction in L2 English, Proceedings
of the 22nd International Conference on Computa-
tional Linguistics (Coling 2008), 169?176, Manch-
ester, UK
Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, Jin-Young
Ha 2010. Using an Error-Annotated Learner
Corpus to Develop an ESL/EFL Error Correction
System, Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC 2010), 763?770, Malta
127
