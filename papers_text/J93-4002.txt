Parsing Some Constrained Grammar 
Formalisms 
K. Vijay-Shanker* 
University of Delaware 
David J. Weir* 
University of Sussex 
In this paper we present a scheme to extend a recognition algorithm for Context-Free Gram- 
mars (CFG) that can be used to derive polynomial-time r cognition algorithms for a set of for- 
malisms that generate a superset of languages generated by CFG. We describe the scheme by 
developing a Cocke-Kasami-Younger (CKY)-like pure bottom-up recognition algorithm for Lin- 
ear Indexed Grammars and show how it can be adapted to give algorithms for Tree Adjoining 
Grammars and Combinatory Categorial Grammars. This is the only polynomial-time recognition 
algorithm for Combinatory Categorial Grammars that we are aware of. 
The main contribution of this paper is the general scheme we propose for parsing a variety of 
formalisms whose derivation process is controlled by an explicit or implicit stack. The ideas pre- 
sented here can be suitably modified for other parsing styles or used in the generalized framework 
set out by Lang (1990). 
1. Introduct ion 
This paper presents a scheme to extend known recognition algorithms for Context-Free 
Grammars (CFG) in order to obtain recognition algorithms for a class of grammatical 
formalisms that generate a strict superset of the set of languages generated by CFG. 
In particular, we use this scheme to give recognition algorithms for Linear Indexed 
Grammars (LIG), Tree Adjoining Grammars (TAG), and a version of Combinatory 
Categorial Grammars (CCG). These formalisms belong to the class of mildly context- 
sensitive grammar formalisms identified by Joshi (1985) on the basis of some properties 
of their generative capacity. The parsing strategy that we propose can be applied to 
the formalisms listed as well as others that have similar characteristics (as outlined 
below) in their derivational process. Some of the main ideas underlying our scheme 
have been influenced by the observations that can be made about the constructions 
used in the proofs of the equivalence of these formalisms and Head Grammars (HG) 
(Vijay-Shanker 1987; Weir 1988; Vijay-Shanker and Weir 1993). 
There are similarities between the TAG and HG derivation processes and that of 
Context-Free Grammars (CFG). This is reflected in common features of the parsing 
algorithms for HG (Pollard 1984) and TAG (Vijay-Shanker and Joshi 1985) and the 
CKY algorithm for CFG (Kasami 1965; Younger 1967). In particular, what can happen 
at each step in a derivation can depend only on which of a finite set of "states" the 
derivation is in (for CFG these states can be considered to be the nonterminal symbols). 
This property, which we refer to as the context-freeness property, is important because 
it allows one to keep only a limited amount of context during the recognition process, 
* Department of Computer and Information Sciences, University of Delaware, Newark, DE 19716. 
E-mail: vijay@udel.edu. 
School of Cognitive and Computing Sciences, University ofSussex, Brighton BN1 9QH, U.K. E-mail: 
davidw@cogs.susx,ac.uk. 
? 1994 Association for Computational Linguistics 
Computational Linguistics Volume 19, Number 4 
which results in polynomial time algorithms. In the recognition algorithms mentioned 
above for CFG, HG, and TAG this is reflected in the fact that the recognizer can encode 
intermediate stages of the derivation with a bounded number  of states. An array is 
used whose entries are associated with a given component of the input. In the case of 
the CKY algorithm, the presence of a particular nonterminal in an array entry is used 
to encode the fact that the nonterminal derives the associated substring of the input. 
The context-freeness of CFG has the consequence that there is no need to encode the 
way, or ways, in which a nonterminal came to be placed in an array entry. 
In this respect, the derivation processes of CCG and LIG would appear to differ 
from that of CFG. In these systems unbounded stacklike structures replace the role 
played by nonterminals in controlling derivation choices. This would seem to suggest 
that the context-freeness property of CFG, HG, and TAG derivations no longer holds. 
Unbounded stacks can encode an unbounded number  of earlier derivation choices. In 
fact, while the path sets 1 of CFG, HG, and TAG derivation trees are regular languages, 
the path sets of CCG and LIG are context-free languages. With respect o recognition 
algorithms, this suggests that the array (whose entries contain nonterminals in the 
case of CFG) would need to contain complete ncodings of unbounded stacks giving 
an exponential time algorithm. 
However, in LIG and CCG, the use of stacks to control derivations is limited in 
that different branches of a derivation cannot share stacks. Thus, despite the above 
observations, the context-freeness property does in fact hold. A detailed explanation 
of why this is so will be presented below. We propose a method to extend the CKY 
algorithm to handle the limited use of stacks found in CCG and LIG. We have chosen to 
adapt the CKY algorithm since it is the simplest form of bottom-up arsing. A similar 
approach using Earley algorithm is also possible, although not considered here. Since 
the use of the stacks is most explicit in the LIG formalism we describe our approach in 
detail by developing a recognition algorithm for LIG (Sections 2 and 3). We then show 
how the general approach suggested in the parser for LIG can be tailored to CCG (in 
Section 4). In the above discussion TAG has been grouped with HG. However,  TAG 
can also be viewed as making use of stacks in the same way as LIG and CCG. In 
Section 5 we show how the LIG algorithm presented in Section 3 can be adapted for 
TAG. 
2. Linear Indexed Grammars 
An Indexed Grammar  (Aho 1968) can be viewed as a CFG in which objects are nonter- 
minals with an associated stack of symbols. In addition to rewriting nonterminals, the 
rules of the grammar  can have the effect of pushing or popping symbols on top of the 
stacks that are associated with each nonterminal. Gazdar (1988) discussed a restricted 
form of Indexed Grammars  in which the stack associated with the nonterminal on the 
left of each production can only be associated with one of the occurrences of non- 
terminals on the right of the production. Stacks of bounded size are associated with 
other occurrences of nonterminals on the right of the production. We call this Linear 
Indexed Grammars  (LIG). 2 
1 The path set of a tree is the set of strings labeling paths from the root to the frontier of the tree. The 
path set of a tree set is the union of path sets of trees in the set. 
2 The name Linear Indexed Grammars i used by Duske and Parchmann (1984) to refer to a different 
restriction on Indexed Grammars in which production was restricted to have only a single nonterminal 
on their right-hand side. 
592 
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms 
Definition 2.1 
A LIG, G, is denoted by (VN, VT, VI, S, P) where 
VN is a finite set of nonterminals, 
VT is a finite set of terminals, 
VI is a finite set of indices (stack symbols), 
S c VN is the start symbol, and 
P is a finite set of productions. 
We adopt the convention that (~, fl (with or without subscripts and primes) de- 
note members of V~, and ~ denotes a stack symbol. As usual, A, B, C will denote 
nonterminals, a b, c will denote terminals, and u, v, w will denote members of V~. 
Definition 2.2 
A pair consisting of a nonterminal, say A, and a string of stack symbols, say (~, will 
be called an object of the grammar and will be written as A (c~). Given a grammar, G, 
we define the set of objects Vc(G) = { A ((~) I A C VN, (~ E V~ }. 
We use T to denote strings in (Vc(G) U VT)*. We write A(-.~) to denote the non- 
terminal A associated with an arbitrary stack (~ with the string on top. Also, we use 
A () to denote that an empty stack is associated with A. The general form of a pro- 
duction in a LIG is: 
a (.. (~) --+ Wla l  (oL1)w2... ai-1 (oq-1) wiai (.. oq) Wi+lai+ 1 (oq+ 1)... A n (o@) Wn+ 1 for n > 
0 and wl . . . ,  W,+l are members of V~-. 
Definition 2.3 
The derivation relation, ~ ,  is defined below. If the above production is used then for 
any fl ~ V{, T1, T2 E (Vc(G) U Wv) *: 
T1A (rico T2 ~ TlWlA1 (o~1)W2... Ai-1 (oq-1) wiAi (tic, i)Wi+lAi+l (Oq+l) 
? .. An (oln) wn+IT2. 
We use ~ as the reflexive, transitive closure of ~ .  As a result of the linearity in 
the general form of the rules, we can observe that the stack flc~ associated with the 
object in the left-hand side of the derivation and flc~i associated with one object in 
the right-hand side have the initial part fl in common. In the derivation above, we 
will say that this object a i (flOq) is the distinguished child of A (flo0. Given a deriva- 
tion, the distinguished descendant relation is the reflexive, transitive closure of the 
distinguished child relation. 
The language generated by a LIG, G, L(G) = { w I S() ~ w }. 
Example 2.1 
The LIG, G = ({ S, T }, { a, b, c }, { ")/a~ "Yb )~ S~/9) generates ( wcw \] w C {a, b} + } where P 
contains the following productions. 
S( . . ) - *aS( . .%)  S(..)--~bS(..q/b) S(..)---~ T(..) 
T(. .%)--,  T(. .)a T(..',/b)-+ T(. .)b T() - -*c  
A derivation tree for the string abbcabb is given in Figure 1. 
593 
Computational Linguistics Volume 19, Number 4 
sf ) 
b s%v 
"~aV b 
T(~_ ) b 
T()  a 
I 
? 
Figure 1 
Derivation tree for LIG. 
In this paper rather than adopting the general form of rules as given above, we 
restrict our attention to grammars whose rules have the following form. In fact, this 
can be easily seen to constitute a normal form for LIG. 
1. A (c0 ~ c where ~ C VT U {c} and length of c~, len (,9<) >>_ 1. 
2. A (.. "/1.. .  Q/m) ----> Ap (.. Vp) As (O<s) where m > 0. 
3. a ('" '71". "Ym) --" As (OLs) ap (.. ~p) where m > 0. 
4. A ("71.. .  7m) "--+ Ap (.. 7p) where m > 0. 
We allow at most two symbols in the right-hand side of productions because we 
intend to develop CKY-style algorithms. In the above rules we say that AF (.. "yp) is 
the primary constituent and As (c~s) is the secondary constituent. Notice also that 
in a derivation using such a rule, the primary constituent yields the distinguished 
child. (In grammatical theories that use a stack of subcategorized arguments, the top 
of the stack in the primary constituent determines which secondary constituent i can 
combine with.) 
2.1 Terminators 
Let us consider how we may extend the CKY algorithm for the recognition of LIG. 
Given a fixed grammar G and an input al ? .. an, the recognition algorithm will complete 
an n x n array P such that an encoding of A (cO is stored in P \[i, d\] if and only if A (oQ 
ai... ai+d-1. The algorithm will operate bottom-up. For example, if G contains the rule 
a ('" ")11... "Ym) ---+ ap (.. "~p) A s (O~s) and we find an encoding of Ap (O<p'yp) in P Ii, dp\] and 
an encoding of As (C~s) in P Ii + dp~ ds\] then an encoding of A (C~p'yl... "Ym) will be stored 
594 
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms 
in P Ii, dp + dsl. What encoding scheme should be used? The most straightforward 
possibility would be to store a complete encoding of A (c~p3,~... 3,m) in P \[i, dp + ds\]. 
However, in general, if an object A (~) derives a string of length d then the length of 
o~ is (,.9(d). 3 Hence there can be O(/d) objects that derive a substring of the input (of 
length d), for some constant k. Hence, the space and time complexity of this algorithm 
is exponential in the worst case. 4
The inefficiency of this approach can be seen by drawing an analogy with the 
following algorithm for CFG. Suppose rather than storing sets of nonterminals in each 
array entry, we store a set of trees containing all derivation subtrees that yield the 
corresponding substring. The problem with this is that the number of derivation trees is 
exponential with respect o the length of the string spanned. However, there is no need 
to store derivation trees since in considering the combination of subderivation trees 
in the CFG, only the nonterminals at the root of the tree are relevant in determining 
whether there is a production that licenses the combination. 
Likewise because of the last-in first-out behavior in the manipulation of stacks 
in LIG, we will argue that it is not necessary to store the entire stack. For instance, 
consider the derivation (depicted by the tree shown in Figure 2) from the point of 
view of recording the derivation in a bottom-up arser (such as CKY). Let a node ~?1 
labeled B (fl3,1 .. .  3,k... 3,m) be a distinguished descendant of a node ~1 labeled A (fl3,1 .. .  3,k) 
as shown in the figure. Viewing the tree bottom-up, let the node ~\], labeled A (fl3,1 ? ?. 3,k), 
be the first node above the node ~71, labeled B (fl3,1 ?. ? 3,k. ? ? 3,m), where 3,k gets exposed 
as the top of the stack. Because of the last-in first-out behavior, every distinguished 
descendant of ~\] above 711 will have a label of the form A I (fl3,1 .. .  3,k~) where len (~) > 1. 
In order to record the derivation from A (fl3,1 .. .  3,k) it would be sufficient o store A 
and 3'1 .. ? 3,k if we could also access the entry that records the derivation from At (fl3,t). 
In the entry for ~?, using a pointer to the entry for At (fl3,t) would enable the recovery 
of the stack below the top k symbols, 3,1 ? .. "Yk. However, this scheme works well only 
when k _> 2. For instance, when k = 1, suppose we recorded only A, 3,1, and a pointer 
to entry for At (fl3,t). Suppose that we are looking for the symbol below 3,1, i.e., the 
top of ft. Then it is possible that in a similar way the latter entry could also record 
just At~ 3,t, and a pointer to some other entry to retrieve ft. This situation can occur 
arbitrarily many times. 
Consider the derivation depicted in Figure 3. In this derivation we have indi- 
cated the branch containing only the distinguished escendants. We will assume that 
the node labeled D (f13,, ..-3,k-13,~ .-. 3/~n ,) is the closest distinguished escendant of 
C (fl3,1..-3,k-13,~) such that every node between them will have a label of the form 
C' (fl"Yl-., 3,k-13,~ O/) where len (~') > 1. Therefore, any node between that labeled 
C (fl3,1..-3,k-13,~) and B(fl3,1...3,rn) will have a label of the form C" (fl3,1..-"~k-10/') 
where fen (c~") > 1. Now the entries representing derivations from both A(fl3,1... 
3,k-13,k) and C (fl3,1... 3,k-13,~) could point back to the entry for the derivation from 
At (fl3,t), whereas the entry for C' (fl3,1 ...3,k-13,~c~') will point back to the entry for 
A 
We shall now formalize these notions by defining a terminator. 
3 For instance, consider the grammar in Example 2.1 and the derivation in Figure 1. In general we can 
have derivations of the form T (q'a3"~) ~ cab n. However, if there exists productions of the form 
A (c~) --~ ~ then the length of the stack in objects is not even bounded by the length of strings they 
derive. 
4 The CCG parsing algorithms that have been proposed so far follow this strategy (Pareschi and 
Steedman 1987; Tomita 1988). 
595 
Computational Linguistics Volume 19, Number 4 
H 1 
Figure 2 
Recovering the rest of stack-1. 
)m Z 
Figure 3 
Recovering the rest of stack-2. 
v v 
Figure 4 
Definition of a Terminator. 
596 
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms 
Definition 2.4 
Suppose that we have the derivation tree in Figure 4 that depicts the following deriva- 
tion: 
A (fl3q ... %-17) ~ uB (fl'Ya .. .  q/k--lq/k.'' q/m) W 
uAt (flq/t) As (O~s) w 
uvw 
or similarly: 
A (flq/1"'- q/k-lq/) uB (flq/1... q/k-lq/i. . ,  q/m) W 
uAs (Ols) At (flq/t) w 
l~ V W 
where the following conditions hold 
2<k<m 
The nodes labeled B (flq/1 . . .  q /k - lq /k  . . . q/m) and At (flq/t) are distinguished 
descendants of the node labeled A (flq/1 ... q/k-lq/) in the respective trees. 
For any distinguished escendent labeled C (c~') between the nodes 
labeled A (flq/1... ")~k-lq/) and B (flq/1.-. q/k-lq/k' ' '  q/m), O/ is of the form 
flq/1 ? ? ? q/kC~ where len (c~) > 1. Note that the nodes labeled 
a (flq/1... q/k-lq/) and B (flq/1... q/k-lq/k''" q/m) need not be different. 
The node labeled At (flq/t) is the k-terminator of the node labeled A (flq/1 .. .  q/k-lq/). 
When it is clear from context, rather than saying that a node is a terminator of 
another we will assume that terminators have been defined on objects that participate 
in a derivation as well. For instance, in the above derivations, we will say that At (flq/t) 
is the k-terminator of A (fl71 ... 7k-l"Y). Also when the derivation is clear from context, 
we will omit the mention of the derivation (or derivation tree). Additionally, we will 
say that a node (object) has a terminator, if it has a k-terminator for some k. 
We will now state some properties of terminators that influence the design of our 
recognition algorithm. 
Definition 2.5 
Given a grammar, G, define MCL(G) (Maximum Change in Length) as: 
MCL(G) = max { m \] A (.. q/1. . . q/m) --* T1Ap ('" ~p) T2 is a production of G } 
Henceforth, we will write MCL since the grammar in question will always be known 
from context. 
Observation 2.1 
In a derivation tree, if a node (say ~) has a k-terminator (say ~t) then ~t is a dis- 
tinguished descendant of ~/. If the node ~/is labeled A (flc~) (where len (c~) = k) then 
the node 7/t must be labeled A t (flq/t) for some At C VN and q/t ff VI. Furthermore, 
2 < k < MCL. 
Observation 2.2 
In a derivation tree, if a node has a k-terminator then it has a unique terminator. 
597 
Computational Linguistics Volume 19, Number 4 
If ~/is the node in question then we are claiming here that not only does it have a 
unique k-terminator but also that there does not exist k ~ with k' ~ k such that ~ has a 
kMerminator. To see why this is the case, let some node ~? have a k-terminator (for some 
k), say ~t. Using Observation 2.1 we can assume that they are labeled A (fl~l . . .  ~k-l"Y) 
and At (flq/t), respectively, where we have (k-1)  > 1. From the definition of terminators 
we can assume that the parent of the terminator, ~/t, is a node (say ~') that has a label of 
the form B (fl3'1 .. .  "/k-l"~k... "Ym). Since (from the definition of terminators) every node 
between ~ and 7/~ (inclusive) must have a label of the form C (fl'Yl -.. ")'k-la ~) where 
len (a ~) >_ 1, it immediately follows that Tit is the closest distinguished escendant of 
such that the length of the stack in the object labeling ~\]t is strictly less than the length 
of the stack in the object labeling ~/. From this, the uniqueness of terminators follows. 
Observation 2.3 
Consider the derivation A (fl"Yl . . .  "Yk-l"~) ~ uAt (fl"Yt) w ~ uvw where At (fl'~t) is the 
k-terminator of A (fl~/1---'Tk-l"Y). Then for any fl' and v', if At (fl'~'t) ~ v' then we 
have the derivation A (fl'~l . . .  "/k-~"/) ~ uAt (fl"Yt) w ~ uv'w where At (fl"~t) is the 
k-terminator of A (fl"~l ... 3~k-~'Y). 
This follows from the fact that the derivation of uAt (fl"yt) w from A (fl'Yl . . .  "Yk-l"7) 
is independent of ft. Therefore we can replace At (fl')'t) ~ v by At (fl'"/t) =~ v'. This 
is a very important property that is crucial for obtaining polynomial-time algorithm. 
Note that not all nodes have terminators. For example, if a node labeled A (a) is the 
parent of a node labeled a (i.e., corresponding to the use of the production A (a) --* a 
where a is a terminal symbol) then obviously this node does not have a terminator. 
Definit ion 2.6 
Given a grammar, G, we define MTL(G) (Maximum Length in, Terminal production) 
as :  
MTL(G) = max { len (a) \] A (a) --* c is a production of G where ~ c VT (_J{?} }. 
As in the case of MCL, we will use MTL rather than MTL(G). 
Observation 2.4 
In the derivation A (a) ~ w if len (a) > MTL then A (a) has a terminator. 
There must be at least two steps in the above derivation since len (a) > MTL. 
However, we can assume that the node (say 7) in question labeled by the object 
1 
A (a) has a distinguished escendant, say ~/~, with label B (fl) such that B (fl) ~ ?. 
Therefore, len (fl) <_ MTL and we may rewrite w as u?v. Since fen (a) > len (fl) we 
can find the closest distinguished escendant of ~/labeled C (a ~) for some C, a ~ such 
that len (a ~) < fen (a). That node is the terminator of ~\] from the arguments made in 
Observation 2.2. 
The above observations will be used in the following sections to explain the way 
in which we represent derivations in the parsing table. We conclude this section with 
an observation that has a bearing on the steps of the recognition algorithm. 
598 
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms 
Observation 2.5 
Consider the following derivation. 
a (fl'~l""" /k-l) T IA .  (fl~/l... ')'k-l"Yk) T2 
l~llap (fl71... 7k-17k) U2 
Ul Vl At (fl'/t) v2u2 
Ul Vl WV2U2 
where Ap (fiVe... "Yk-l'Yk) is the distinguished child of A (f17~... ')'k-l) and At (flVt) is the 
k-terminator of Ap (fl71..- 7k-l"Yk). At (fl~t) is the (k - 1)-terminator f A (fl71 .-- ")'k-l) 
if and only if k > 2. If k = 2 then A (fl71) has a terminator if and only if At (fl'Tt) 
does. In fact, in this case, if At (fl'Tt) has a kMerminator then that terminator is also the 
k~-terminator f A (flVt). 
This can be seen by considering the derivation shown in Figure 3 and noting the 
sharing of the terminator of C (fl3'1.-. 7k-17~) and A (fl"/1-.. 7k-l")/k) ?
3. Recognition Algorithms 
As in the CKY algorithm we will use a two-dimensional rray, P, such that if A (c~) 
ai.. ? ai+d-1 then a representation f this derivation will be recorded with an encoding of 
A ((~) in P \[i, d\]. Here we assume that the given input is al .. .  an. We start our discussion 
by considering the data structures we use to record such objects and derivations from 
them. 
3.1 Anatomy of an Entry 
We mentioned earlier that the stack in an object can be unboundedly large. We must 
first find a compact way to store encodings of such objects whose size is not bounded 
by the grammar. In this section we provide some motivation for the encoding scheme 
used in the recognition algorithm by considering the bottom-up application of the rule 
and the encoding of the primary constituent: 
A (..'y1. . .'~m) --* Ap ("~/p) As (,~s) 
The Head. An object with nonterminal Ap and top of stack "Tp will match the primary 
category of this rule. Thus, the first requirement is that at least this much of the object 
must be included in every entry since it is needed to determine if the rule can apply. 
This component is denoted lap, vp/and called the head of the entry. Thus, in general, 
an entry in P Ii, d I with the head {A,'~/ encodes derivations of ai...ai+cl-1 from an 
object of the form A (fl'y) for some f l?  V 7. 
Terminator-pointer. An encoding of the object Ap (fl'Tp) (the primary constituent) hat 
derives the substring ai ... ai+dp_ 1 (o f  the input string al ? ? .  an) will be stored in the array 
element P {i, dp\] in our CKY-style recognition algorithms. Now consider the encoding 
of Ap (fl'yp) for some sufficiently long fl-yp. While the head, lAp, ~p), of the entry is 
sufficient o determine whether the object in question can match the primary category 
of the rule, we will need to store more information in order that we can determine the 
content of the rest of the stack. In the above production, if m = 0 then the combination 
of Ap (fl~/p) and As (~s) results in A (fl). In order to record the derivation from A (fl), 
we need to know the top symbol in the stack fl, i.e., the symbol below the top of 
the stack associated with the primary constituent. We need to recover the identity of 
599 
Computational Linguistics Volume 19, Number 4 
this symbol from the encoding of the primary category. This is why we introduced 
the notion of terminators. As mentioned in Section 2.1, terminators can be used to 
access information about the rest of the stack. In the encoding of Ap (fl'yp), we will 
store information that allows us to access the encoding of its terminator. The part of 
the entry encoding the terminator will be called terminator pointer. 
The Middle. Note that the object Ap (fl,yp) (in the derivation Ap (fl3'p) =~ a i . . .  ai+dp-1) 
can have a k-terminator where k is between 2 and MCL. Therefore, from Observa- 
tion 2.1 it follows that the terminator-pointer can only be used to determine the (k+l )  st 
symbol from the top. Therefore, assuming that fl = fl"yl ? .. "Yk-1, the terminator-pointer 
will allow us to access fl~. (Recall from the definition, ak-terminator of A (fl"yl . . .  "Yk-13'p) 
will have the form At (fl"Yt). Thus the (k + 1) st symbol from the top in A (fl-yp) is the 
same as the symbol below the top of the stack of the terminator.) Thus, we will need 
to record the string "yl - ' '  "Yk-1 in the encoding of Ap (fl'q/1 . . .  3'k-1~'p) as well. This part 
of the entry will be called the middle. 
To summarize, the entry stored in P \[i, dp\] (where f l"yl. . .  "Yk-l"Yp is assumed to be 
sufficiently long that we know A m (fl'71-.. 7k-l"Yp) is guaranteed to have a termina- 
tor) will have a head, (Ap,-yp); and a tail comprised of a middle, "Yl..-'Yk-1; and a 
terminator-pointer. Note that the length of the middle must be at least one, but at 
most MCL - 1, since from Observation 2.1, we know 2 < k < MCL. We will call an 
entry of this kind a terminator-type entry. 
We will now discuss what we need to store in order to point to the termina- 
tor. Suppose we would like to record in P\[i,d\] the derivation of ai. . .ai+d-1 from 
A (fl'Yl... 7k-l"Y) as shown below. We assume that At (fl'yt) is the terminator in this 
derivation. 
a (fl"/1...'Yk-l"Y) ai. .. at_lAt (fl')'t) at+dt . . . ai+d-1 
ai ? ? ? at- l  at ? ? ? a t+dt - l  at q-dt ? ? ? ai+d-1 
~- ai ? ?. ai+d-1 
From Observation 2.3, it follows that it would be sufficient o use  ((at~ "Ytl~ \[t~ dt\]) as 
the terminator-pointer. This is because any entry with the head (At~ ")'tl in P It, dt\] will 
represent in general a derivation At (fl"Yt) ~ at . . .  at+dr-1. This not only matches the 
above case, but even if fl' ~ fl, from the Observation 2.1, we have 
A (fl'"/1... "Yk-lq/) ~ a i . . .  at - lAt  (fl'"/t) at+dr.., aiq-d--1 ~ ai . . .  ai+d-1. 
Thus, the use of the head information (plus the two indices) in the terminator-pointer 
captures the essence of Observation 2.3. It is this structure-sharing that allows us to 
achieve polynomial bounds for space and time. Note that the string derived from 
the terminator, at. . .at+dr- i ,  is a substring of ai. . .ai+d-1. In such a case, i.e., when 
i G t and i+  t >>_ t + dr, we will say that /t, dt/ <_ {i~dl. We define {t, dt/ < {i, dl if 
{t, dtl <_ {i, dl and {t, dtl # {i, dl. Since any terminator-type entry in P\[i,d\] can only 
have terminator-pointers of the form ({At ,  "Ytl ~ {t, dtl ) where It, dtl <_ {i, dl, the number 
of terminator-type entries in P \[i, d\] is O(d2). 
Definition 3.1 
Given a grammar, G, define MSL(G) (Maximum Secondary constituent's stack Length) 
as MSL(G) = max { len (as) I As (e~s) is the secondary constituent of a production } 
Henceforth we will use MSL rather than MSL(G). 
600 
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms 
We now consider the question of when a terminator-type entry is appropriate. Of 
course, if A (~) ~ a i . . .  ai+d-1 we could store such an entry in P Ii, dpl only when A (c~) 
has a terminator in this derivation. From Observation 2.4 we know that if len (c~) > 
MTL then there exists a terminator of A (~) in this derivation. However, it is possible 
that for some grammar  MSL > MTL. Therefore even when len (c~) > MTL (i.e., the 
object has a terminator) A (~) can still match the secondary category of a rule if len (c~) G 
MSL. In order to verify that an object matches the secondary category of a rule we 
need to consider the entire stack in the object. When A (~) ~ ai . . .  ai+d-1 and length 
of ~ does not exceed MSL, it would be convenient o store A as well as the entire 
stack c~ because such an object can potentially match a secondary category of a rule. 
To be certain that such an object is stored in its entirety when len (~) < MSL, the 
terminator-type entry can only be used when len (c~) > max(MSL~ MTL). However, 
we prefer to use the terminator-type entry for representing a derivation from A (~) 
only when its terminator, say At (fl), is such that len (fl) >_ max(MSL~ MTL) rather than 
when len (c~) > max(MSL~ MTL). Again, we point out that this choice is made only for 
convenience and because we feel it leads to a simpler algorithm. The alternate choice 
could also be made, which would lead to a slightly different algorithm. 
Definition 3.2 
Define the constant TTC (Terminal-Type Case) as TTC = max(MSL MTL). In a deriva- 
tion A (fl71 ... 7k) ~ W we will say that A (flY1 -.. Vk) has the TC-property iff it has a 
k-terminator, say At (flTt), such that len (flVt) _> TTC. 
If A (fl31 . . .  3k) ~ ai. . .  ai+d_l, where A (fl31 .-. 3k) does not have the TC-property then 
we record the object in its entirety in P Ii~ d\]. In order for such an entry to have the 
same format as the terminator-type entry, we say that the entry has a head /A~ 3k); a 
tail with a middle 31.-.  7k-1 and a nil terminator-pointer. Note that in this case the 
middle can be an empty string; for instance, when we encode A (V) ~ ai.. ? ai+d-1. In 
general, if c~ = f13 then we say top (~) = 3 and rest (c~) = ft. If o~ = ? then we say that 
top (c~) = rest (c~) ~- ~. 
To summarize, the structure of an entry in P Ii, d I is described by the following 
rules. 
? An entry consists of a head and a tail. 
? A head consists of a nonterminal and a stack symbol. 
? A tail consists of a middle and a terminator-pointer. The exact nature of 
the middle and the terminator-pointer are as given below. 
- -  The terminator-pointer may be of the form (IAt~ 7tl~ \[t~dtl) 
where At E VN~ 3t E W I and It~ dtl <_ li~ d). In this case, the middle 
is a string of stack symbols of length at least one. This form of a 
terminator pointer is used in the encoding of a derivation from 
an object if its terminator has a stack length greater than or 
equal to TTC. Recall that we had called this type of an entry a 
terminator-type ntry. 
A terminator-pointer can be a nil. Then the middle is a (possibly 
empty) string of stack symbols. However, the length of the 
middle is less than TTC + MCL - 1. This form of a terminator 
pointer is used in the encoding of a derivation from an object if 
it does not satisfy the TC-property; i.e., either it has no 
601 
Computational Linguistics Volume 19, Number 4 
terminator or if the terminator exists then its stack length is less 
than TTC. 
3.2 Recognition Algorithms for LIG 
Since the full algorithm involves a number of cases, we develop it in stages by restrict- 
ing the forms of productions. The first algorithm that considers the most restricted 
form of productions introduces much of what lies at the core of our approach. Next 
we relax these restrictions to some degree. After giving the algorithm at this stage, 
we switch to discuss how this algorithm can be adapted to yield one for CCG. Later, 
in Section 5, we consider further relaxation of the restrictions on the form of LIG 
productions, which can help us produce an algorithm for TAG. 
Regardless of which set of restrictions we consider, in every algorithm we shall 
establish that the following proposition holds. 
Proposition 3.1 
? ((at ~k) (')'1.-. "Yk-1, ((at,,,/t), \[t, dt\]))) E P\[i,d\] if and only if for some 
fl c v~, 
a (fl~'l... "Yk-l"/k) ~ ai... at- la (fl"/t) at+dt-1...ai+d-1 
ai. . .ai+a-1 
where At (fl"/t) is the k-terminator of A (fl"/1 ' ' '  "Yk) and len (fl'Yt) >_ TTC. 
? ((A,'yk) (3'1...Tk-1,nil)) E P\[i,d\] i fand  only if 
a ( ' ) '1 . . .  q/k-lq/k) ~ ai...ai+d-1 
where in this derivation A ("/1 . . .  "Yk-l'Yk) does not have the TC-property. 
3.2.1 Algorithm 1. Recall that the general form of rules that are to be considered are 
as follows. 
1. A (c~) --* c where e ? {e} U VT, and len (c~) > 1. 
2. A(..'y~. . ' rm)~ Ap(..'~p)As(~s) 
3. a(..~l..."ym)----~ as(ozs)ap (.."fp). 
4. a ( "~l . . .3 'm)~Ap("3 'p ) .  
At this stage we assume that the following restrictions hold of the above rules. 
In the first type of production we assume that e c VT and len (c~) > 1. 
Thus MTL > 1. 
len (C~s) _> 1 in productions of type 2 and type 3, i.e., MSL > 1. 
There are no productions of type 4. 
We will now give the following rules that specify how entries get added in the 
parsing array. The control structure of the algorithm (a CKY-style dynamic program- 
ming structure) will be added later. We assume that the input given is al ... an, where 
n>l .  
602 
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms 
Init ial izat ion Phase 
In the initialization phase of the algorithm we store lexical objects (objects deriving a 
terminal symbol in one step) entirely in a single entry. In other words, 
Rule 1.L 
A (c~) ---~ a a = ai l< i<n 
( {A, top (c~)} (rest (ee), nil)) ? P\[ i, 1\] 
Inductive phase 
Here productions of type 2 and type 3 will be considered. Let us assume the presence 
of the following production in the grammar: A (.. "Yl . . .  " /m) --+ Ap (.. %) As (o~s). 5 
Suppose that while considering which entries are to be included in P \[i, d\] we find 
the following for some dp, ds such that dp + ds = d. 
? The entry ((Ap,,ypl ( f lp,tpp))E P\[i, dp\]. This is consistent with the rule's 
primary constituent. Regardless of whether tpp = nil or not, for some 
fl E V~: Ap (flflp'yp) ~ ai...ai+dp-1. That is, when tpp = nil we have 
? The entry ((As, top (o~s)) (rest (c~s), nil)) E P \[i + dp, ds\]. This is consistent 
with the rule's secondary object. Thus if d = dp + ds we may assume 
As (Ols) ~ ai+dp . . . ai+d_l. 
From the presence of the two entries pecified above (and the derivations they rep- 
resent) we have A (flflp'Yl ... 7m) ~ Ap (flflp'yp) As (c~s) ~ ai.. .  ai+d-1. This derivation 
must be recorded with an entry in P \[i, d\]. The content of the entry depends on sev- 
eral factors: the value of m; whether or not the terminator-pointer in the entry for the 
primary constituent (i.e., tpp) is nil; and the length of the middle in this entry (i.e., tip). 
These determine whether or not the new entry will be a terminator-type entry. We 
have cases for m = 0, m = 1 and m _> 2. 
CASE WHEN m = 0 
The new object to be stored is A (flflp). The top of the stack in this object can be 
obtained from the stack associated with the primary constituent. How this is done 
depends on whether the entry encoding the primary constituent is of terminator type 
or not. 
When m = 0 and tpp = nil 
This means that the primary constituent has been represented in its entirety; i.e., the 
primary constituent is Ap (flpTp). Since tpp = nil the primary constituent does not satisfy 
the TC-property (i.e., it does not have a terminator with a stack of length greater than 
or equal to TTC), the new constituent too cannot be encoded using a terminator-type 
entry. Therefore, 
Rule 2.ps.L 
(lAp, 3'pl (tip, nil)) ? P\[i, dp\] ((As, top(c~s)) (rest(o~s),nil) ) C P\[i +dp, d -dp\] 
( IA, t?P (flp) l (rest (flp),nil) ) E P\[i,d\] 
5 Similar arguments can be used when we consider the production: A (.. 3'1 ... 7m) --* As (C~s) Ap (" 3'p). 
603 
Computational Linguistics Volume 19, Number 4 
The following rule is the counterpart of Rule2.ps.L 6 that corresponds to the use of the 
production A (..) ~ As (C~s) Ap (.. 7p). 
Rule 2.sp.L 
( (As, top(c~s)) (rest(c~s),nil) ) ? P\[i, ds\] ((Ap, Vp ) (tip,nil)) ? P\[i + ds,d-  ds\] 
((A, top (tip)) (rest (tip),nil) ) ? P\[i,d\] 
When m = 0 and  tpp ~ nil 
Let the entry for the primary constituent be ((Ap, 7p) (tip, ((At, 7t), It, dr\]))). Since the 
primary constituent is Ap (flflpTp) we will assume that its terminator is At (fl'Yt) where 
len (flVt) > TTC. Note also that len (flp'yp) > 2. The entry for the new object (A (flflp)) 
is determined based on whether len (tip) = 1 or len (tip) > 1. In the latter case the 
len (flpVp)-terminator f the primary constituent is the len (&)-terminator of the new 
object. This is not so in the former case, as noted in Observation 2.5. 
Considering the latter case first, i.e., len (tip) > 1, we may write tip as 71...'Yk-lVk 
where k > 2. Since in this case the new object and the primary constituent have the 
same terminator and since the primary constituent has the TC-property (tpp ~ nil), 
the new object must also be encoded with a terminator-type entry. Thus we have the 
following rule: 
Rule 3.ps.L 
((Ap,~/p)('y1...3%tpp)) ?P\[i ,  dp\] 
tpp = ((At,vt), \[t, dt\]) ,k >_ 2 ((As, top (c~s)) (rest (c~s), nil)) ? P \[i + dp, d - dp\] 
( (A,"Ykl  ( "Y1 . . .Tk - l , tpp) )  cP \ [ i ,d  1 
Henceforth we shall give the ps versions of the rules only and omit sp versions. 
Now let us consider the case when len (tip) = 1. Rewriting tip as 71, the entries 
represent derivation for fl E V~ (len (fl'yl) = len (fl"/t) > TTC). 
a (ti"/1) ~ ap (ti"/l"/p) As (0@) 
ai . . . at- l At (tiTt ) at+a,.., ai+ap- l As (C~s) 
ai . . . at_  lat . . . at+d t_ lat+dt ? . .  ai+dp_ laiq-d p . . .  a i+d_l  
where At (ti'Yt) is the 2-terminator of Ap (ti"/l"Yp)- From Observation 2.5 it follows that 
if At(tiVt) has a terminator then the terminator of A(ti'yl) in this derivation is the 
same as the terminator of At (fl'Yt); and if At (fl'Yt) has no terminator then neither does 
A (ti'Yl). Additionally, in this derivation A (ti'yl) satisfies the TC-property if and only 
if At (ti'Yt) has the TC-property. That is, we should use a terminator-type entry to 
record this derivation from A (ti'Yl) if and only if a terminator-type entry has been 
used for At (tiTt). Since these two objects share the same terminator (if it exists) the 
terminator-pointer must be the same when we record derivations from them. There- 
fore, suppose we use the terminator-pointer of ((Ap, ?) (tip, ((At, "Yt), \[t, dt\]))) to lo- 
cate an entry ((At, "Yt) (tit, tPt)) ? P It, dr\]. This would suggest he addition of the entry 
6 Here L indicates a rule we use in LIG parsing; ps indicates that the primary constituent appears before 
the secondary constituent. Similarly, sp will be used to indicate that the secondary constituent appears 
before the primary constituent. 
604 
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms 
(IA, "/1)(tit, tpt) ) to P\[i,d\], regardless of whether or not tPt = nil. However, we give 
the two cases (tpt = nil or tPt = (IAr, %1, \[r, dr\]) for some At, %, r, dr) in the form of 
two different rules. This is because (as we shall see later) these two rules will have to 
appear in different points of the control-structure of the parsing algorithm. 
Rule 4.ps.L 
((Ap, 7p) (71, ((At, 7t), \[t, dt\]) ) 
cP\[i, dp\] 
((As, top (c~s) ) (rest (c~s), nil)) 
? P\[i+dp,d-dp\] 
( (At, Tt) (fit,nil)) 
E P\[t, dt\] 
( (A, 71) (fit,nil)) E P\[i,d\] 
Rule 5.ps.L 
( (Ap, vp) (71, ( (At, 7t) , \[t, dt\] ) ) ) 
? P\[i,d~,\] 
((As, top (c~s) ) (rest (C~s), nil)) 
? P\[i+dp,d-dp\] 
((At, "~/t) (fit, tPt) ) 
 P\[t, at\] 
tpt = ((Ar,'Yr), Jr, dr\]) 
((A, 7"/1) (flt,tpt) E P\[i,d\] 
CASE WHEN m -- 1 
The length of the stack in the new object is equal to that of the primary object. In 
fact, the terminator of the primary object (if it exists) is the same as the terminator of 
the new object, and when the primary object has no terminator neither does the new 
object. Therefore the encoding of the new object can easily be derived from that of the 
primary object by simply modifying the head (to change the top of the stack symbol). 
Thus we have: 
Rule 6.ps.L 
((Ap,'Tp) (tip, nil)) ? P\[i, dp\] ((As, top(c~s)) (rest(c~s),nil)) c P\ [ i+dp,d-dp\ ]  
((A,")'I) (tip, nil)) ? P\[i,d\] 
Rule 7.ps.L 
( (Ap, 7p) (tip, ((At, 7t) , \[t, dt\] ) ) ) ? P \[i, dp\] ( ( A~ ,top ( c~s ) ) (rest(as), nil)) E P \[ i + dp , d - dp \] 
CASE WHEN m > 2 
If the primary constituent is Ap (titip,,/p) then the new constituent is A (tiflp'Yl... "/m). In 
fact, in this case, we have the primary constituent being the m-terminator of 
A (fltip3'l... "Ym). Of course, this does not mean that the derivation from the new object 
should be recorded with the use of a terminator-type entry. We use the terminator-type 
entry only when len (tip3'p) ~ TTC. In order to determine the length of this stack we 
have to use the entry for the primary constituent (i.e., (IAp,.Tp)(tip, tpp) lE  PIi, dp\]) 
and consider whether this is a terminator-type entry or not (i.e., whether tpp = nil or 
not). 
605 
Computational Linguistics Volume 19, Number 4 
When m _> 2 and tpp ~ nil 
Therefore the length of the stack of the terminator of the primary constituent is greater 
than or equal to TTC. This means that stack length of the primary constituent ( he 
terminator of the new object) exceeds TTC. Thus we have the following rule: 
Rule 8.ps.L 
({Ap,'Tp) (tip, tpp) ) ? P\[i, dp\] 
tpp = (Iat,'~t), It, dr\]) ({As, top (C~s)) (rest (as), nil)) E P\[i +dp, d - dpl 
(/A, ~m) (")/1.-. "/m-l, ( {Ap, "yp) , \[i, dp\]) ) c P\[i,d\] 
When m _> 2 and tpp = nil 
The primary constituent (which is the terminator of the new object) should be repre- 
sented in its entirety. Therefore, in order to determine whether we have to encode the 
new object with a terminator-type entry or not, we have to look at the entry for the 
primary constituent. Thus we obtain the following rules: 
Rule 9.ps.L 
len (tip'yp) < TTC 
( IAp, 3,pl (tip, nil)) E P\[i, dp\] ((As, top (as)) (rest (as), nil)) E P \[i + dp, d - dp\] 
( {A, ~m) (tipVl . . . Tm-l,nil) ) E P\[i,d\] 
Rule 10.ps.L 
len (tip-yp) > TTC 
( (Ap, 7p) (tip, nil)) c P\[i, dp\] ( IAs, top (as)) (rest (~s), nil)) E P \[i + dp, d - dp\] 
((A, "Ym) ('Yl... "/m-l, (lAp, "/p) , \[i, dp\] ) ) ) C P\[i,d\] 
In the discussions that follow, we find it convenient torefer to the entries mentioned in
the above rules as either antecedent entries (or entries that appear in the antecedent) of 
a rule or consequent entry (or entry that appears in the consequent) of a rule. For ex- 
ample, (lAp, q/pl (tip, nil)) in PIi, dpl and ( IAs,top(e~s)l (rest (c~s),nil) in PIi + dp,d - dpl 
are the antecedent entries of Rule 10.ps.L and ( IA, "Yml ('Y'"" "Ym--l~ ( I ap~ "~p I ' \[i, dp\] ) ) ) 
that is added to P \[i, d I is the entry in the consequent of Rule 10.ps.L. 
3.3 The Control  Structure 
We will start by giving a simple control structure for the recognition algorithm that 
follows the dynamic programming style used in the CKY algorithm. 
606 
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms 
In this section we modify the notation for entries lightly. In the above discussion, 
the terminator-pointer of a terminator-type entry contains a pair of indices repre- 
senting input positions. Thus, in effect, P is a four-dimensional rray. As an alter- 
native to saying that (( A, 3,) (fl, (CA', 3"), \[t,d;\]))) is in P\[i,d\] we will sometimes say 
(C A, -y)(fl, (A', 3/))) is in P\[i, d\] \[t, dt\]. Also  as an alternative to saying (C A, oe)(fl, nil)) 
is in P\[i,d\] we will sometimes ay ((A, c,) (fl, nil)) is in P\[i,d\]\[O,O\]. Thus P can be 
considered to be an array of size n x n x (n + 1) x (n + 1). 
In the specification of the algorithm (Figure 5) we will not restate all the rules we 
discussed in the previous section. Instead we will only indicate where in the control 
structure ach rule fits. As an example, when we state "Use Rule 2.ps.L with dp = d" 
within the i, d, and d' loops we mean the following: for current values of i, d, and d' (and 
hence dp, ds) consider every production of the form A (.. "Yl .. .  7m) --+ Ap (.. 7p) As (as) 
with m = 0. For each such production, look for entries of the form ((Ap, 7p) (tip, nil)) E 
P \[i, dp\] \[0,0\] for some tip and ( (As, top (as) ) (rest (as), nil) ) E P \[i + dp, d - dp\] \[0,0\]. In the 
event we find such entries, we add ((A, top (tip)) (rest (flp),nil)) to P\[i,d\] \[0,0\] if it is 
not already there. 
Since the entries in P\[i, d\] have the form ( (A, -,/) (fl, (CAt, 3q) , It, dt\] ) ) ) (where (t, dt) G 
(i,d)) or the form (CA,')')(fl, nil)), there are O(d 2) many entries in P\[i,d\] (where 
1 G i < n and 1 G d G n - d). Thus space complexity of this algorithm is O(nd). 
Note that within the body within the r loop will be attempted for all possible values 
of i, d, d', t, dt, r, dr. Since the range of each loop is O(n), the time complexity is O(n7). 
The asymptotic complexity of the above algorithm can be improved to O(n 6) with 
a simple rearrangement of the control structure. The key point here is that the steps 
involving the use of rules 5.ps.L and 5.sp.L can be split into two parts each. Consider, 
for example, the use of the Rule 5.ps.L, which is repeated below. 
Rule 5.ps.L 
((Ap,',/p) (~I, ((At, q:t) , \[t, dt\]) ) ) 
EP\[i, dp\] 
((A,, top ( o~, )) (rest (c~), nil)) 
GP\[i+dp,d-dp\] 
((At, ")'t) (fit, tpt) ) 
tpt = ((Ar,")'r), \[r, dr\]) 
((A, q'l) (fit, ((Ar,'Yr), \[r, dr\]))) E P\[i,d\] 
This rule corresponds to the use of the production A (..) ~ Ap (.. q/p) As (as). The values 
of i, d, d', t, d t are necessary to determine the span of the substrings derived from the 
primary constituent and the secondary constituent, and the values of i, d, t, dt, r, dr are 
needed to locate the entry for the terminator, i.e., (CAt,,),t)(fit, (CAr,"/r), \[r, dr\]))) and 
to place the new entry in the appropriate parsing table element. That is, the values of 
r and dr are not required for the first part and the value of d' need not be known for 
the second part. This indicates that the second part need not be done within the loop 
for dq Therefore, we can modify the control structure in the following way. Within 
the t loop (which appears within the loops for d, i, d',dt) we find the entries for the 
primary and secondary constituents. Having found the two relevant entries, we must 
record the head of the new entry (A, tip) and the terminator-pointer of the primary 
constituent, i.e., (CAt, ~t), \[t, dt\]). We can do this by using a two-dimensional rray 
called TEMP where we store CA, q'l, At, 7t). Outside the d' loop (and hence outside the 
loops for t and dt as well), but within the loops for i and d, we can have the loops 
that vary t, dt, r, dr (note (r, dr) < (t, dr)) in order to locate the entry for the terminator 
by using the information recorded in TEMP. Finally, having found the entry for the 
607 
Computational Linguistics Volume 19, Number 4 
Algorithm 1 
begin 
for i:= 2 to n do 
Initialization phase 
Use Rule 1 
for d := 2 to n do % d loop 
fo r i := l ton-d+ldo%i loop  
begin 
fo rd ' := l tod- ldo  %d' loop  
begin 
Use Rule 2.ps.L, 6.ps.L, 9.ps.L, 10.ps.L with dp = d'. 
for dt := (d' - 1) to 1 do % dt loop 
for t := i to (i + d' - dt) do % t' loop 
begin 
Use Rule 3.ps.L, 4.ps.L, 7.ps.L, 8.ps.L with dp = d' 
for dr :-- dt to 1 do 
for r := t to t + dt - dr do 
begin 
Use Rule 5.ps.L with dp = d' 
end 
% end of dr loop 
% end of r loop 
end 
% end of t loop 
% end of dt loop 
fo r  dt :=  (d  - d '  - 1) to 1 do % dt loop 
for t := (i + d') to (i + d - dr) do % t' loop 
begin 
Use Rule 3.ps.L, 4.ps.L, 7.ps.L, 8.ps.L with ds = d' 
for dr := dt - 1 to 1 do 
for r := t to (t + dt - dr) do 
begin 
Use Rule 5.sp.L with ds = d' 
end 
% end of r loop 
% end of dr loop 
end 
% end of t loop 
% end of dt loop 
end 
% end of d' loop 
end 
% end of i loop 
% end of d loop 
Figure 5 
Algorithm 1. 
608 
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms 
terminator we then store the resulting entry in P \[i, d\]. These steps are captured by the 
following rules. For a specific value of (i, d) we have 
Rule 5.i.ps.L 
( (Ap, q/p) (71, ( (At, q/t) , It, dr\]))) 
C P\[i, dp\] 
((As, top (C~s)) (rest (C~s), nil)) 
e\[ i  + clp, cl - G\] 
(A, q/1,At, 7t) E TEMP\[t, dt\] 
Rule 5.ii.ps.L 
(a~"/l~.,z~t,q/t) E TEMP\[t, dt\] ((At, q/t) (fit, ((Ar, q/r), \[r, dr\]))) C P\[t, dt\] 
((A,q/1) (fit, ((Ar, q/r), \[r, dr\]))) C P\[i,d\] 
Similarly, we assume we have the pair Rule 5.i.sp.L and Rule 5.ii.sp.L corresponding 
to Rule 5.sp.L. This leads to the algorithm given in Figure 6. In this algorithm we drop 
the sp rules and specify the ps rules only for the sake of simplicity. 
The correctness of Algorithm 2 can be established from the correctness of Algo- 
rithm 1 (which is established in Appendix A) and the following Lemma. 
Lemma 3.1 
Given a grammar G and an input al. . .an an entry ((A, q/} (fl, tp)) is added to P\[i,d\] 
by Algorithm 1 if and only if ((A,q/) (fl, tp)) is added to P\[i,d\] by Algorithm 2. 
Outline of Proof: Using induction on d. The base case corresponding to d = 1 in- 
volves only the initialization step, which is the same in the two algorithms. The 
only difference between the two algorithms (apart from the control structure) is the 
use of Rule 5.ps.L (and Rule 5.sp.L) by Algorithm 1 versus the use of Rule 5.i.ps.L 
and Rule 5.ii.ps.L (Rule 5.i.sp.L and Rule 5.i.sp.L) in Algorithm 2. Rule 5.ps.L is 
used to add entries of the form ((A, ~Yl)(fit, ((ar~ q/r)~ Jr, dr\]))). We can establish that 
((A, ,`/1) (fit, ((Ar, q/r), \[r, dr\]))) is added to P \[i, d\] due to the application of Rule 5.ps.L 
if and only if there exist entries of the form ((Ap, q/p) ('71, ((At, q/t), \[t, dt\]))) in P\[i, dp\]; 
((As, top(c~s))(rest(c~s),nil)) in P\[i+dp,d-dp\]; ((at, q/t)(flt~((ar, q/r)~\[Y~dr\])) ) in 
P It, dt\]; and the production A (..) --* Ap (.. q/p) As (~s). Using induction, we can estab- 
lish that these entries exist if and only if (A, q/1,At~ q/t) is added to TEMP\[t, dt\] using 
Rule 5.ps.i.L (or Rule 5.sp.i.L) and ((A, q/1) (fit, ((Ar, q/r), Jr, dr\]))) is added to P\[i,d\] 
using Rule 5.ii.ps.L. 
4. Combinatory Categorial Grammars 
Combinatory Categorial Grammars (CCG) (Steedman 1985, 1986) are extensions of 
Classical Categorial Grammars in which both function composition and function ap- 
plication are allowed. In addition, forward and backward slashes are used to place 
conditions concerning the relative ordering of adjacent categories that are to be com- 
bined. 
Definition 4.1 
The set of categories generated from a set, VN, of atomic categories i defined as the 
smallest set such that all members of VN are categories, and if cl, c2 are categories then 
so are (Cl/C2) and (el\e2). 
609 
Computational Linguistics Volume 19, Number 4 
Algorithm 2 
begin 
for i:= 1 to n do 
Initialization phase 
Use Rule 1 
fo rd :=2tondo%dloop  
for i := 1 to n -  d + 1 do % i loop 
begin 
Initialize TEMP It, dt\] to ~ for all (t~ dt) ~ (i~ d) 
fo rdp := l tod- ldo  %dploop 
Use Rule 2.ps.L, 6.ps.L, 9.ps.L, 10.ps.L 
for dt := dp - 1 to 1 do % dt loop 
for t := i to i + dp - dt do % t loop 
Use Rule 3.ps.L, 4.ps.L, 5.i.ps.L, 7.ps.L, 8.ps.L 
% end of t loop 
% end of dt loop 
% end of dp loop 
for dt := d - 1 to 1 do % dt loop 
for t := i to i + d - dt do % t loop 
for dr := dt - 1 to 1 do 
for r := t to t + dt - dr do 
begin 
Use Rule 5.ii.ps.L 
end 
% end of r loop 
% end of dr loop 
% end of dt loop 
% end of t loop 
end 
% end of i loop 
% end of d loop 
Figure 6 
Algorithm 2. 
Definition 4.2 
A CCG, G, is denoted by (VT, VN, S~f~ R) where 
VT is a finite set of terminals (lexical items), 
VN is a finite set of nonterminals  (atomic categories), 
S is a dist inguished member  of VN, 
f is a function that maps  each element of VT to a finite set of categories, 
R is a finite set of combinatory  rules, where combinatory  rules have the fol lowing 
form. 
1. A forward rule has the fol lowing form where m > 0. 
(x/y) (yllZl\[2... ImZm) ---4. (XllZll2... \[mZm) 
2. A backward rule has the fol lowing form where m > 0. 
(y11Zl12... ImZm) (x\y)  ---+ (XIIZl12... \]mZm) 
610 
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms 
Here x,y, z l , . . .  ,Zm are meta-variables and h , . . - ,  \[m E {\, /}. For m -- 0 these rules 
correspond to function application and for m > 0 to function composition. Note that 
the set R contains a finite subset of these possible forward and backward rules; i.e., 
for a given CCG only some of the combinatory rules will be available. 
Definition 4.3 
In the forward and backward rules given above, we say that (x/y) (resp. (x\y)) is the 
primary constituent of the forward (resp. backward) rules and (y\]1zl\[2... \]mZm) is the 
secondary constituent of the rule. The notion of a distinguished child is defined as in 
the case of LIG, i.e., a category is the distinguished child of its parent if it corresponds 
to the primary constituent of the rule used. As before, the distinguished escendant 
is the reflexive, transitive closure of the distinguished child relation. 
In discussing CCG we use the notational conventions that the variables \] and 
c (when used with or without primes and subscripts) range over the forward and 
backward slashes and categories, respectively. We use x,y, z for meta-variables; a, fl 
for strings of directional categories (i.e., a string of the form \]1Cl\]2.,. \]nOn from some 
n ~ 0); and A, B, C for atomic categories (i.e., members of VN). 
Derivations in a CCG, G = (VT, VN~ S,f, R), involve the use of the combinatory 
rules in R. Let ~ be defined as follows, where T1 and T2 are strings of categories 
G 
and terminal symbols. 
If ClC 2 ---+ C is an instance of a rule in R, then TlCT 2 ~ "~1ClC2T2 . 
G 
If c C f(a) for some a c VT and c is a category, then TlCT2 ~ TlaT2. 
The string languages generated by a CCG, G, L(G) = { w \] S ~ w \] w E V~ }. 
G 
Example 4.1 
The following CCG generates { wcw \] w E {a, b} + }. Let G = ({at b, c}, {S, T, A, B}, S,f, R) 
where 
f(a) = (A, T \A/T ,  T\A} f(b) = {B, T\B/T,  T\B} f(c) = (S/T} 
The set of rules R includes the following three rules. 
y (x\y) ~ x (x/y) (y\zl/z2) ---+ (y\zl/z2) (x/y) (y\zl) ~ (y\zl) 
In each of these rules, the target of the category matched with x must be S. 7 Figure 7 
shows a derivation of the string abbcabb. 
We find it convenient to represent categories in a minimally parenthesized form 
(i.e., without parentheses unless they are needed to override the left associativity of 
the slashes), where minimally parenthesized form is defined as follows. 
7 Following Steedman (1985), we allow certain very limited restrictions on the substitutions of variables 
in the combinatory ules. A discussion on the use of such restrictions is given in Vijay-Shanker and 
Weir (in press). However, we have not included this in the formal definition since it does not have a 
significant impact on the algorithm presented. 
611 
Computational Linguistics Volume 19, Number 4 
A S~A 
S~A~B 
B 
b 
S~AkBkB 
S~B/T 
S'~JF 
SIT T'Odr 
I I 
a 
c 
T~B/F 
I 
b 
T~B 
I 
b 
Figure 7 
CCG example derivation tree. 
Definit ion 4.4 
? A is the minimally parenthesized form of A where A C VN. 
? If c l , . . . ,c ,  are the minimally parenthesized forms of categories c~,..., c" 
respectively, then (al lcl l2.. .  Incn) is the minimally parenthesized form of 
(('-' (allc~)12'' ")l~c'). 
A category c is in minimally parenthesized form if c is the minimally parenthesized 
form of itself. 
Definit ion 4.5 
Let a category c = A l lC112. . .  \[nCn be in minimally parenthesized such that n > 0, 
A E VN, and Cl,..., Cn are minimally parenthesized categories. 
? The target category of c = allClI2... InCn denoted by tar(c) is A. 
? The arity of c = AllC112... InCn, denoted as arity (c), is n. 
? The argument categories of c = AllC 112... Inch denoted by 
args (c) = { ci \] 1 < i < n }. 
4.1 CCG and LIG 
Before showing how the general parsing scheme illustrated by the LIG recognition 
algorithm can be instantiated as a recognition algorithm for CCG, we show that CCG 
and LIG are very closely related. The details of the examination of the relationship 
between CCG and LIG may be found in Weir and Joshi (1988) and Weir (1988). 
A minimally parenthesized category (AIlc 112... InCn) can be viewed as the atomic 
category, A, associated with a stack of directional argument categories, ILC112... Inc,. 
The rule 
(x /y)  (y l lZ l l2 . . .  ImZm) ~ (X I1Z l l2 . . .  ImZm) 
! ! ! ! has as  an  instance (ApieCe... InCh~As) (AsliCl}2...  ImCm) --~ (apieCe...  InCnllCll2... ImCm) 
I I I I I I as we l l  as (A~L~c~... InCn/(Asl c )) (AI'c'L~c~I2... I,~cm) ~ (&l~c'~... I~Cnh~lL2... Imam) 
612 
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms 
! ! as an instance. Thus x matches the category (Apl~c ~ . . .  \[nCn), y matches an atomic cat- 
egory As in the first example and a nonatomic ategory (As\]~C ') in the second, and 
each zi matches ci for 1 ( i ( m. A derivation involving the second instance (viewed 
bottom-up) can be seen as popping the top directional argument /(Asl'c') from the 
primary category and pushing the m directional arguments IlCl\]2.-. ImCm ? Thus, each 
instance of the combinatory rule appears to closely resemble a LIG production. For 
example, in case of the second instance we have 
ap (.. ILC112... ImCm) ---+ ap ( . . / (As\] 'c '))  As (\['c'11c112... ImCm) . 
We now show that, like the set of stack symbols of a LIG, the set of directional argu- 
ment categories that we need to be concerned with is finite. 
Definition 4.6 
Let c be a useful category with respect o a grammar G if and only if c ~ w for 
some w E V~. The set of argument categories, args (G) of a CCG, G = (VT~ VN~ S,f, R), 
is defined as args (G) = Uc~f(a) args (c). 
Observation 4.1 
If c is a useful category then args (c) c args (G), a finite set determined by the gram- 
mar, G. 
This observation can be shown by an induction on the length of the derivation 
of some string from c. The base case corresponds to a lexical assignment and hence 
trivially args (c) C args (G). The inductive step corresponds to the use of a combination 
using a rule of the form 
(x /y)  (yllZll2... ImZm) ---+ (XIlZll2... ImZm ) 
or 
(yllZll2...\]mZm) (x \y)  ~ (XIlZll2...ImZm) 
By inductive hypothesis, any useful category matching either (x/y)} (x\y) or 
(y\]lZll2... ImZm) must take its arguments from args (G) (a finite set) and therefore the 
resulting useful category also shares this property. 
The above property makes it possible to adapt the LIG algorithm for CCG. Note 
that in the CKY-style CCG recognition we only need to record the derivations from 
useful categories. From Observation 4.1 it follows that the lexical category assignment, 
f, determines the number of "stack" symbols we need to be concerned with. Therefore, 
only one of the variables (x) in a combinatory rule is essential in the sense that the 
number of categories that it can usefully match is not bound by the grammar. There- 
fore, it would be possible to map each combinatory rule to an equivalent finite set of 
instances in which ground categories (from args (G)) were substituted for all variables 
other than x; i.e., y, zl}...Zm in the combinatory rule above. This would result in a 
grammar that was a slight notational variant of a LIG where the CCG variable x and 
the LIG notation .- perform similar roles. However, for the purpose of constructing a 
recognition algorithm it is both unnecessary and undesirable to expand the number of 
rules in this way. We adapt the LIG algorithm so that it, in effect, constructs appropriate 
instances of the combinatory rules as needed during the recognition process. 
613 
Computational Linguistics Volume 19, Number 4 
4.2 Recognition of CCG 
The first step in modifying the LIG algorithm is to define the constants MSL and MTL 
for the case of CCG. Let G -- (VT, VN, S,f,  R) be a CCG. These definitions follow im- 
mediately from the similarities between CCG combinatory rules and LIG productions. 
Observation 4.2 
If we were to express a combinatory rule 
(x /y )  (yIlZl...ImZm) ~ (X\]IZ1...\]mZm) 
in terms of LIG production 
A ("")'1..."/m) "-'+ Ap (""yp) As(o@) 
then we have the following correspondences: 
? % w i th /y .  
? "~i with = \]izi for 1 < i < m, i.e., "Y1 ' ' '  "Ym with IlZl . . .  \]mZm. 
? A = Ap. 
? As (as) with yhz~. . .  ImZm . 
Given such a direct correspondence b tween combinatory rules and LIG productions, 
we will define the following constants to be used in the the CCG algorithm with 
minimal explanation. 
? MTL is the maximum arity of a lexical category. Thus, 
MTL = max { arity (c) \] c c f(a), a C VT }. 
? MSL should be the maximum arity of a useful category that can match 
the secondary category of a rule. Note that a category matching 
(y\]lZ1\]2-.. ImZm) will have an arity that is the sum of m and the arity of 
the category matching y. Furthermore, note that since y is an argument 
of the primary category it must be bound to a member of args (G). Thus, 
MSL = max { m \] (y\[lz112... \]mZm) } is the secondary category of a rule in 
R + max { arity (c) \] c E args (G) }. 
? Note that in the case of CCG, MCL need not be defined independently 
of MSL. 
? As before, we define TTC as TTC = max { MSL, MTL }. 
Since directional categories play the same role that stack symbols have in LIG, we 
revise the notions of length top ( ) and rest ( ) as follows. We say that the string of direc- 
tional arguments categories \]lCl I2"'" InCh has a length n, i.e., len (IlCl 12.-. \]nCn) = n. Note 
that arity ((A\]1c112... InCn)) = len (\[1Cl\]2... \]nCn) = n. We define top ((IlCl\]2... InCn)) = 
\[nCn and rest ((\]1c112... \]nCn)) = \]1Cl \]2.-. \]n--lCn--1 ? Additionally, top (?) = rest (~) = ?. 
4.2.1 Terminators in CCG. We can define a k-terminator in essentially the same way 
as in the case for LIG. Note that a category shares its target category with all of its 
distinguished escendants. 
614 
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms 
Definit ion 4.7 
Suppose that we have the following derivation: 
af l l lC l  . . . Ik_lCk_l lC ~ u a f l l lC l  . .  . \ ]k_lCk_l lkCk. . . ImCm 
==~ U a f l /Cp  CpllCl . . . IkCk. . . ImCm 
ldVW 
W 
W 
or similarly 
af l l lC l . . . I k_ lCk - l lC  ~ U a f l l lC l . . . I k_ lCk_ l l kCk . . . ImC m W 
u CpllCl...\[kCk...ImCm Aft\c, w 
UVW 
where the following conditions hold 
? fl is a string of direction categories, i.e., fl E ({/~ \}args (G))*. 
? k - l> l ,  
? A f l l lC l . . .  Ik-lCk-llkCk... ImCm and Afl/c are distinguished escendents of
af l l lC l  . . . Ik_ lCk_l  \[C 
? any distinguished escendent between Afl/Cp and AflllCl ... \]k-lCk-llC can 
be expressed in the form afll~cl ... Ik-lCk-1 le~ where len (c~) >_ 1 
We say that Aft/@ is the len (11Cl . . .  Ik_lCk_llc)-terminator f Afl l lc l . . .  Ik_lCk_l\[C. 
Note that cp need not be atomic. Hence if we write the secondary category as 
cpllcl ... ImCm we are not necessarily expressing it in minimal parenthesis form. 
4.2.2 Anatomy of a CCG Entry. In the CCG algorithm we will use entries that have a 
form similar to that of the entries in the LIG algorithm. The choices we make are based 
on Observation 4.2. For a derivation AllCl ... Ijcj ~ al ... ai+d-1 (where the input is 
al . . .  an), we will have an entry in P Ii~ d\] with a head IA~ IjCjl, where  A E VN, Ij C {\~/}, 
and cj E args (G). 
First consider the case when a terminator-type entry is used. The terminator- 
type entry is applicable when Aflllcl ...  Ik-lCk-11c has a k-terminator, say Af l l t c t  where 
len (flltCt) ~_ TTC. As before we say that in such a case  Af l l lC l . . .  \]k_lCk_ 11c satisfies the 
TC-property. Assuming the terminator derives the substring at.. .  at+d~-l, we can use 
the terminator-pointer (llctl~ \[t~ dtl) and a middle I1c1... Ik_lCk_l . Notice that since the 
target of the category Alfl\] lcl. . .  Ik-lCk-11C as well as the target of its terminator is A 
and since A is already noted in the head, it is not recorded in the terminator-pointer. 
For entries that are not terminator-pointer, the entire category is noted in the 
entry. Such an entry has the form (IA~ Ijcj~ (11cl... Ij-lCj-l, nil)) assuming that j > 1. 
However, it is possible that j = 0. In this case the category being represented is A, 
and the entry will be written as (IA~ c I (~ nil)). In general, we use the non-terminator- 
type entry for recording a derivation from As when it has no terminator or when the 
terminator, say af l l tCt  (rewriting c~ as flllCl... Ik-lCk-1 \[C) is such that len (flltCt) ~__ TTC; 
i.e., when the category As does not satisfy the TC-property. 
4.2.3 CCG Algorithm. It is straightforward to derive the rules for the CCG recognition 
algorithm from those used in LIG algorithm. Using Observation 4.2, we can now give 
the rules for the CCG algorithm with no explanation. 
615 
Computational Linguistics Volume 19, Number 4 
Rule 1.C 
Ac~cf(a) a=ai  l< i<n 
Assume the combinatory rule (x/y) 
When m = 0 and tpp = nil 
Rule 2.ps.C 
((Ap,/Cp) (tip, nil)) C P\[i, dp\] 
((Ap, top (~)) (rest (c~), nil)) E P\[i, 1\] 
(YllZl --. ImZm) ---+ (XIlZl ... ImZm). 
Asc~s = Cp 
((As, top (C~s)) (rest (C~s), nil)) E P \[i + de, d - dp\] 
((Ap, top (tip)) (rest (tip),nil) ) E P\[i,d\] 
When m = 0 and  tpp ? nil 
Rule 3.ps.C 
tp e = ((I,c,), \[t,d~\]) 
k>2 Asc~s = Cp 
((As, top(as)) (rest(o~s),nil) ) C P\[i + de,d-de\] 
( (Ap, IkCk) (llCl... Ik_lCk_i, tpp) ) ~ P\[i,d\] 
Rule 4.ps .C  
((ap,/ep) (lie1, ((\]tct), \[t, dt\]) ) 
E P\[i, dp\] 
Asc~s = Cp 
((As, top (cts)) (rest (C~s), nil) ) 
~ P\[i+de, a -de\ ]  
( ( )) (A e, \]tct) tit, nil 
* P\[t, dt\] 
((Ap, I1c,) (tit,nil)) C P\[i,d\] 
Rule 5.ps.C 
((Ae,/ce) O~c,, ((Ltc,), \[t,d,\]))) 
C P\[i, de\] 
Asc~s = Cp 
((At, top (C~s)) (rest (C~s), nil)) 
cP\[ i+dp,d-de\]  
tpt-~ ((IrCr), \[r, dr\]) 
( (Ae, ltct) (ti,, tp,) ) 
((A e, Ilcl) (tir, tPt) ) C P\[i,d\] 
When m -- 1 
Rule 6.ps.C 
( (Ap, /Cp) (tip, nil)) C P\[i, dp\] 
Asc~s = CpllCl 
((As, top(c~s)) (rest(c~s),nil) ) C P\[i + dp,d-d,\] 
((A e, \]1c,) (tie,nil)) E P\[i,d\] 
616 
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms 
Rule 7.ps.C 
( (Ap, /Cp) (flp, ( (Itct) , \[t, dt\] )) ) c Pfi, dp\] 
As~s = Cp\[lCl 
((As,top(c~s)l (rest(c~s),nil)) C P\[i +dp~d-dp\] 
((Ap, llCl ) (tip, ((ItCt), \[t, dt\]) ) C P\[i,d\] 
When m > 2 and tpp ~ nil 
Rule 8.ps.C 
tpp = (l l tct l ,  \[t~dt\]) 
(IAp, /Cp> (tip, tpp) ) E PIi, dp\] 
As~s = CpllCl . . .  ImCm 
( IA~, top (c~s) l (rest (c~s), nil)) E P \[i + dp, d - dp\] 
( (Ap, ImCm} (11Cl... Im--lCm--l~ ( (/Cp) , \[i, dp\] )) ) C P\[i,d\] 
When m > 2 and tpp = nil 
Rule 9.ps.C 
len (tip/@) < TTC 
((Ap,/Cp) (tip,nil)) E PIi, dp\] 
asoz s = CpllCl . . . \]mCm 
( IAs, top (c~s) ) (rest (c~s), nil) ) E P \[i + dp, d - dp\] 
(lap, Imem) (tip\]lCl . . . Im-lCm--l,nil) E P\[i,d\] 
Rule 10.ps.C 
len (tip/ep) > TTC 
((Ap,/Cp) (tip, nil)) E P\[i, dp\] 
aso @ = CpllCl . . . \[mCm 
((As, top(c~s)) (rest(c~s),nil) ) c P\[i + dp,d -dp\] 
( (ap,  IrnCrn} (11Cl... Irn--lCm--l~ ((/Cp} , Ii, dp\]) ) ) C P\[ i ,d\] 
Proposition 4.1 
The CCG recognition algorithm can be seen to establish the following. 
? (lAp, Ic~ (t3, (lltctl, \[t~ dt\]))) C P \[i~ d\] if and only if there is some c~ such 
that Ac~tilc ~ ai... ai+d-1 and the (len (/3) + 1)-terminator (Aozltct) of 
Ac~tilc derives the string at... at+dr-1 and len (O~ltCt) ~ TTC. 
? ((Ap, top(cO~, (rest(cO, nil,)) E P\[i,d\] if and only if Ac~ ~ ai...ai+d-1 
and either Ac~ has no terminator or its terminator, say Ac~ t is such that 
len (c~') < TTC. 
5. TAG Recognition 
We begin this section by first considering how to extend our algorithm for LIG to 
handle unary productions. This will be needed to show we can instantiate our scheme 
to give a recognition algorithm for TAG. 
5.1 Handling Unary Productions and Epsilon Productions 
We will now show how the LIG algorithm given earlier can be extended to consider 
unary productions of the form A (.. ~1. . .  "Ym) -'-9 Ap (.. 3'p) as well as e productions of 
617 
Computational Linguistics Volume 19, Number 4 
the form: A (c~) --+ e. However, we will now assume that m G 2 in productions of the 
form A (.. 2/1 ... q/m) --+ T1ap ('" q/p) T2. Thus, henceforth MCL < 2. Note that this refers 
to both unary and binary productions. This additional restriction does not change 
the generative power. We have introduced these restrictions in order to reduce the 
number of cases we have to consider and also because we can restrict our attention 
to the productions that are used in the TAG to LIG construction. 
Consider the processing of a binary production A (.. 2/1 . . .  2 /m) ---+ Ap (.. "yp) As (as). 
Since CKY-style parsers work bottom-up, we check to see if the primary and secondary 
categories derive adjacent strings (say ai...ai+dp-1 and ai+dp...ai+dp+&, respectively) 
and then we store an encoding for the new object that results from the combination. 
The processing of unary productions i similar except hat we do not have to consider 
a secondary constituent. The rules that express the processing of such productions will 
be very similar to those for the binary productions. For example, consider Rule 2.ps.L 
for the binary production A (..) ---+ Ap (.. ",/p) As (C~s). 
Rule 2.ps.L 
((Ap,,yp) (tip, nil)) E P\[i, dp\] ((As,top(e~s)) (rest(e~s),nil)) E P \ [ i+dp,d -dp \ ]  
( ( A, top (tip)) (rest (tip), nil) ) E P \[ i, d\] 
Given a unary production A (..) --+ Ap (.. 2/p) we have the Rule 2.u.L (where u stands 
for unary). 
Rule 2.u.L 
((Ap,',/p) (tip,nil)) c P\[i, dp\] 
((A, top (G) ) (rest (&),ni l))  E P\[i,d\] 
In addition, with the introduction of e productions, we have to consider derivations 
of strings of length d -- 0. We shall assume that if A (c~) ~ e then an encoding of 
A (~) will be stored in P\[i, 0\] (for all i). We must also consider the possibility that the 
primary constituent or the secondary constituent derive the empty string, i.e., dp = 0 
or ds = 0. Processing of such cases becomes imilar to that of unary productions. 
To indicate the additional processing required due to the introduction of unary 
productions and the possibility of the derivation of the empty string, let us consider 
Rule 8.ps.L. "Use Rule 8.ps.U' can be paraphrased as follows. 
If there exists a production A (.. 71 ... 2/m) -+ A m ('" 7p) As (C~s) where 
m >__ 2, el = ((Ap,q'p)(flp,(at,'Tt))) belongs to P\[i,d\]\[t, dt\] and e2 = 
((As, top (C~s))(rest (c~s), nil)) belongs to P\[i +dp, d - dp\] \[0, 0\] then add 
e3 = ((A~'ym)(qrl...~/m-l,(Ap~2/p))) to P\[i,d\]\[i~dp\] if e3 is not already 
present in this array element. 
If we allow ~ productions it is possible that ds --- d -  dp -- 0. Consider the case where we 
have As (as) ~ c. That is, we expect he entry e2 to be present in P \[i + d, 0\] \[0, 0\]. This 
means that the resulting entry e3 must be added to P \[i, d\] \[i, d\] since we now have 
dp = d. Note that the addition of e3 = ((A,"/m)("yl..."Yrn--l~ZZ~p~p~)) (that encodes 
the derivation from A (fl'yl ... %) for some fl) can result in more entries being added 
to the same array element P \[i, d\] \[i, d\] (for instance, when we have the production 
B (--2/~ ... 2/~) --+ A (-. "Ym)). This is similar to the prediction phase in Earley's algorithm 
and the state construction in LR parsing. Based on this analogy, we will define our 
618 
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms 
notion of closure. Closure (e,i,d,t, dt) will add entries to PIi, d\] It, dr\] or PIi, d\] Ii, d\] that 
result from the inclusion of the entry P Ii, d\] It, dt\] by considering unary productions 
(or binary productions when the primary or secondary constituent derives the empty 
string). Before we define Closure () we note that for each occurrence in the algorithm 
of "use Rule X" is replaced by "use closure of Rule X." For example, "Use closure of 
Rule 8.ps.U' stands for 
If we have the production A ("~1. . . ?m) ~ Ap (.. %) As (C~s) where m ~ 2, 
el = (IAp, ~pl (flp,At~',/t) ) belongs to PIi, d\] It, dt\] and 
e2 = ( IAs~ top (C~s) l (rest (C~s)~ nil) ) belongs to P Ii + dp~ d - dp\] IO~ 0\] and 
e3 ~- (/a~ '~m/(~/1-.. "Ym-l~ap~ "Yp)) does not belong to P\[i,d\] Ii, dp\] then 
add e3 to PIi, d\] Ii~dp\] and then invoke Closure (e3,i~d~i~dp). 
Closure is defined as follows: 
Closure (e, il, dl , t, dr) 
begin 
use closure of Rule 2.ps.L, 6.ps.L, 7.ps.L, 8.ps.L, 9.ps.L, 10.ps.L 
with d = dp and the entry e as the primary constituent in the antecedent. 
use closure of Rule 2.sp.L, 6.sp.L, 7.sp.L, 8.sp.L, 9.sp.L, 10.sp.L 
with ds -- d and the entry e as the secondary constituent in the antecedent. 
use closure of Rule 2.u.L, 6.u.L, 7.u.L, 8.u.L, 9.u.L, 10.u.L 
with d = dp and the entry e as the primary constituent in the antecedent. 
end. 
Note Rule 3 does not apply since we have to assume MCL _< 2 (hence any ter- 
minator is a 2-terminator and the length of the middle in a terminator-type entry is 
always one). We have not included Rule 4 and Rule 5 while computing the closure. 
These correspond irectly to the completor step in Earley's algorithm and to the pop- 
ping of stack elements and hence are not considered a part of the closure. They have 
to be applied later in the control structure. 
We will now consider the effect of including unary rules on the control structure of 
the algorithm. Let/ / i l ,  dl/~/i2, d2//G// i3,  d3/~ lid, dd// i f  and only if (1) (il, dl/ < (i3, d3/ 
or (2) /il~ dl/ --- /i3~ d3/and/i2~ d2/ < (i4~ dd/. The simplicity of the loop structure in the 
algorithms een thus far stems from the fact that for any parsing rule if the entry in the 
consequent is to be added to P Ii3, d3\] Iid~ dd\] based on the existence of an antecedent 
entry in P Iil, dl\] I/'2, d2\], then Ilil, dll, lid, d211 ~ Ili3, d31~ lid, dd}l. This no longer holds 
when we consider Rule 5.u.L or Rule 5.ps.L when the secondary constituent derives the 
empty string. Consider the following derivation (and the presence of the productions 
assumed) for a sufficiently long fl: 
a (flo~) ~ a 1 (fl"~l) ~ A2 (fl~l~Y2) ~ A3 (fl'y) ~ ai...  ai+d-1 
Consider the addition of an entry e3 to P\[i,d\] It, dr\] (for some It, dtl) to record the 
derivation from A3 (fl'y). Closure (e3~ i,d, t, dr) is invoked, resulting in the addition of 
e2 (corresponding to A2 (fl'Y13'2)) to P\[i, d\] Ii, d\]. From Rule 5.u.L and the presence of 
entry e2 and e3 we would add el (corresponding to al  (fl3/1) to P Ii, d\] It, dt\]). This could 
result in the need to add more entries to P Ii, d\] Ii, d\], which in turn could cause new 
entries being added back to PIi, d\] It, dt\], and so on. Thus we have a situation where 
619 
Computational Linguistics Volume 19, Number 4 
initialization phase 
for loops for d, i, d' as before 
begin 
consider closure of Rules in Rule set I 
for dt :-- d' - 1 to 1 do 
for t := i to i + d' - dt do 
repeat 
consider closure of Rules in Rule set II 
for dr := dt - 1 to 1 do 
for r := t to t + dt - dr do 
consider closure of Rules 5.ps.L and Rule 5.u.L 
until no new entries are added to P\[i,d\] \[t~dt\] 
Figure 8 
Control structure with unary productions. 
an antecedent entry in P\[i,d\] It, dr\] ((t, dt) < (i,d)) causes an entry to be added to 
P \[i, d\] \[i, d\], which, acting as an antecedent entry, causes a new entry to be added to 
P\[i,d\] It, dr\]. 
A simple strategy to take care of this situation would be to add another loop 
within the t loop (as shown in the partial control structure given in Figure 8) that is 
repeated until no new entries are added to P \[i, d\] It, dt\]. It is straightforward to prove 
the correctness of the algorithm with this additional loop and also that the asymptotic 
complexity remains the same. The latter is the case because only a bounded number 
of entries can belong to P\[i,d\] It, dr\] for any fixed value of i, d, t, dr, and hence the 
repeat loop can be iterated only a bounded number of times (as determined by the 
grammar). In the partially specified control structure given in Figure 8, we have not 
considered the sp rules. Also we only consider the changes that need to be made to 
Algorithm 1; the changes to Algorithm 2 can be made in a similar fashion. Finally, for 
purposes of abbreviation, we have grouped Rules 2.ps.L, 6.ps.L, 9.ps.L, and 10.ps.L 
together and called it the Rule set I, and Rules 3.ps.L, 4.ps.L, 7.ps.L, and 8.ps.L the 
Rule set II. 
The repeat loop shown in Figure 8 is not needed in some situations. Consider 
the derivation and the sequence of addition of entries, e3~ e2~ el, as discussed above. 
Viewing this derivation as a bottom-up recognizer would, we have a "prediction" from 
entry e3 followed by a "completion" that results in the entry el. In this case the two 
entries both encode objects with the same stack length. We generalize this situation 
and call such derivations auxiliary derivations (named after auxiliary trees in TAG). 
a (fl"Yl) ~ TIA1 (fl~/l"/2)T2 ~ TlUAt (fl'Tt)wT2 ~ UlUAt ( f l "Y t )WWl  
where At (fl'Tt) is the 2-terminator of A1 (fl"/1"/2). We will say that this auxiliary deriva- 
tion spans at least one terminal if len (UlUWWl) _> 1. Notice that if for a particular gram- 
mar every auxiliary derivation spans at least one terminal, then the extra repeat loop 
added becomes unnecessary. This is because now, with this assumption, for every pars- 
ing rule if the entry in the consequent is to be added to P \[/3~ d3\] \[i4~ dd\] based on the exis- 
tence of an antecedent entry in P\[/1, dl\] \[12~ d2\] then ((il, dl) ,  (12~ d2)) -< ((i3, d3), (i4~ dd)). 
620 
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms 
We end this section by noting that in the case of a lexicalized TAG, we can verify 
that every auxiliary derivation spans at least one terminal, and hence in the TAG 
algorithm we do not have to include this additional repeat loop. 
5.2 Tree Adjoining Grammars 
Tree Adjoining Grammars (TAG) is a tree generating formalism introduced by Joshi, 
Levy, and Takahashi (1975). A TAG is defined by a finite set of trees composed by 
means of the operation of tree adjunction. 
Definition 5.1 
A TAG, G, is denoted by (VN, VT~ 57 Iv A) where 
VN is a finite set of nonterminals symbols, 
VT is a finite set of terminal symbols, 
S E VN is the start symbol, 
I is a finite set of initial trees, 
A is a finite set of auxiliary trees. 
An initial tree is a tree with root labeled by S and internal nodes and leaf nodes 
labeled by nonterminal and terminal symbols, respectively. An auxiliary tree is a tree 
that has a leaf node (the foot node) that is labeled by the same nonterminal that labels 
the root node. The remaining leaf nodes are labeled by terminals and all internal nodes 
labeled by nonterminals. The path from the root node to the foot node of an auxiliary 
tree is called the spine of the auxiliary tree. An elementary tree is either an initial 
tree or an auxiliary tree. We will use c~ to refer to an initial tree, and fl to refer to 
an auxiliary tree. "y may be used to refer to either an elementary tree or a tree that is 
derived from an elementary tree. 
We will call a node in an elementary tree an elementary node. We can give a 
unique name to each elementary node by using an elementary node address. An 
elementary node address is a pair composed of the name of the elementary tree to 
which the node belongs and the address of the node within that tree. We will assume 
the standard addressing scheme where the root node has an address c. If a node 
addressed # has k children then the k children (in left to right order) have addresses 
# ? 1 , . . . ,  # ? k. Thus, if dV" is the set of natural numbers then # E W'*. In this section 
we will use # to refer to addresses and ~/to refer to elementary node addresses. In 
general, we can write ~ = IV, #/ where 3  `is an elementary tree and # E Domain (3'). 
We will use Domain (3') for the set of addresses of the nodes in % 
Definition 5.2 
Let 3  `be a tree with internal node labeled by a nonterminal A. Let fl be an auxiliary 
tree with root and foot node labeled by the same nonterminal A. The tree, 3`~, that 
results from the adjunction of fl at the node in 3  `labeled A (as shown in Figure 9) is 
formed by removing the subtree of 3  `rooted at this node, inserting fl in its place, and 
substituting it at the foot node of ft. 
Each elementary node is associated with a selective adjoining (SA) constraint that 
determines the set of auxiliary trees that can be adjoined at that node. In addition, 
when adjunction is mandatory at a node it is said to have an obligatory adjoining 
(OA) constraint. Figure 9 shows how constraints are associated with nodes in trees 
derived from adjunctions. Whether fl can be adjoined at the node (labeled by A) in 3  `
is determined by c, the SA constraint of the node. In 3 r` the nodes contributed by fl 
621 
Computational Linguistics Volume 19, Number 4 
Acl 
A c2 
A c2 
aN 
Figure 9 
The operation of adjoining. 
have the same constraints as those associated with the corresponding nodes in ft. The 
remaining nodes in 7' have the constraints of the corresponding nodes in 3 .` 
Given/* E Domain (3`), by LABEL(% #/we refer to the label of the node addressed 
/* in 7. If the tree in question is clear from context, we will simply use LABELI#/. 
Similarly, we will use SA(%/*) (or SA(#)) and OA(% #) (or OA(/*)) to refer to the SA 
and OA constraints of a node addressed /* in a tree 3 .` Finally, we will use ft (/3) to 
refer to the address of the foot node of an auxiliary tree/3. 
To be precise, we define the adjunction of/3 at a node in 7 with address /* as 
follows. This operation is defined when/3 is included in the SA constraints of node 
addressed #in 3 .` If the operation is defined, we will use ADJ (3`, #,/3) to refer to the tree 
that results. Let 3'' = AD3 (%/*,/3). Then the nodes in 3'' and their labels and adjoining 
constraints are defined as follows. 
? Domain (-y') = {/.1 I /.1 E Domain(7) , / .1  ~/* ' / *2 ,  for some/*2 E Af*} U 
{/* ",1 \[/.1 E Domain (fl)} U {#. ft (fl)'/~l I/* "/.1 C Domain (3'), and/.1  e} 
? When/.1 C Domain (3`) such that/.1 #/* ?/.1 for some/.1 E d~ f*}, i.e., the 
node in ~ with address/.1 is not equal to or dominated by the node 
addressed/* in 3,: 
LABEL(-/',/*~) = LABEL('),,/.1), 
- -  SA(3" , / .1 )  = SA(3` , / .1 ) ,  
- -  OA(7',/*1) = OA(-y,/.1), 
? when/* '>1 E Domain (7') such that #1 C Domain(fl): 
LABEL(7',/*. >1) = LABEL(fl,/*I), 
- -  SA(" / t , / * " / .1 )  = SA(fl,/.1), 
- -  OA(3`',/*'/*1) ~-  OA(fl,/.1), 
? when/*,  ft (fl)./.1 E Domain (3") such that/*./.1 C Domain (7) and/.1 # e: 
LABEL ('T t,/*- ft (/3). /.1) = LABEL (-y, #./xl ), 
622 
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms 
{z S {IBt,IB2} 
s 
b s {131,132} 
b s?  a 
Figure 10 
Example of a TAG G. 
- -  SA(3/, # ? ft (fl) - it1 ) = SA('y,# -#1}, 
OA(~/', # - ft (fl). #~) = OA(%#.  #1), 
In general, if # is the address of a node in "~ then by (% #) we refer to the elementary 
node address of the node that contributes to its presence, and hence its label and 
constraints. 
The tree language, T(G), generated by a TAG, G, is the set of trees derived starting 
from an initial tree such that no node in the resulting tree has an OA constraint. The 
(string) language, L(G), generated by a TAG, G, is the set of strings that appear on the 
frontier of trees in T(G). 
Example 5.1 
Figure 10 gives a TAG, G, which generates the language {wcw I w E {a, b}+}. The 
constraints associated with the root and foot of fl specify that no auxiliary trees can 
be adjoined at these nodes. This is indicated in Figure 10 by associating the empty set, 
G with these nodes. An example derivation of the strings aca and abcab is shown in 
Figure 11. 
5.3 TAG and LIG 
In this section, we examine bottom-up recognition of a TAG. In doing so, we construct 
a LIG that simulates the derivations of the TAG. Based on this construction, we derive 
a recognizer for TAG from the algorithms given earlier. 
Consider bottom-up TAG recognition. Having recognized the substring dominated 
by an elementary node there are two possible actions: (1) move up the tree by combin- 
ing this node with its siblings; or (2) consider adjunction at that node. In bottom-up 
recognition, the second action (i.e., adjunction) must be considered before the first. 
Therefore, there are two phases involved in the consideration of each node. On enter- 
ing the bottom phase of a node, having just combined the derivations of its children, 
we predict an adjunction. On entering the top phase, having just finished adjunction 
at that node, we must now combine with any siblings in order to move up the tree. 
Note that in the bottom phase we may also predict hat there is no adjunction at the 
node (if there is no OA constraint on that node) and hence move to its top phase 
directly. 
Figure 12 shows why, because of the nature of the adjoining operation, TAG can be 
seen to involve stacking. Suppose, during recognition, the bottom phase of a node, 7\], 
623 
Computational Linguistics Volume 19, Number 4 
3,1 
c~ S {\[31,\[52} 
I 
c 
Figure 11 
Sample derivations in G. 
s~ 
a S {61,~2} 
Soo a 
I 
c 
s~ 
a S~ 
b S {\[51,\[52} 
Sq~ b 
S~ a 
I 
c 
Figure 12 
Stacking in a TAG. 
has been reached. When adjunction by the auxiliary tree fl is predicted, control shifts 
to the bottom phase of fl's foot node. As we move up the spine of fl it is necessary to 
remember that fl was adjoined at 7. On reaching the top phase of fl's root we must 
return to (the top phase of) 7. Therefore, the adjunction point, ~7, must be propagated 
up the spine of ft. In general, we may need to propagate a stack of adjunction points 
as we move up the spine as shown in Figure 12 where 3'2 is obtained by adjoining 131 
at a node ~11 on the spine of ft. From this figure, it can be seen that the information 
about the adjunction points (that must be propagated along the spine of an auxiliary 
tree) follows the stack (last-in first-out) discipline. Notice also that only the nodes on 
the spine participate in the propagation of adjunction points. 
Consider how a LIG that simulates this process can be constructed. The details of 
the equivalence between LIG and TAG can be found in Vijay-Shanker (1987). In the 
624 
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms 
LIG, we use two nonterminals, t and b to capture the differences between the top and 
bottom phases associated with a node. The stack holds an appropriate sequence of 
adjunction points in the form of elementary node addresses. The top of the stack is 
the elementary node address of the node that is currently being visited (thus all objects 
have at least one element on the stack). Nodes that are not on the spine, or belong to 
an initial tree, do not participate in the propagation of adjunction points. Therefore, the 
objects for such nodes will have stacks that contain only their elementary node address. 
The set of LIG productions is determined as follows. We assume that internal 
elementary nodes have either a single child labeled by a terminal symbol (or ~), or 
exactly two children labeled by nonterminals. In this discussion below, we will use ~\] 
for a node and its elementary node address interchangeably. 
1. If ?7 is a node that is labeled e where ~ c V T U{~} then we will include 
t 07) ---+ c. 
2. If ?Tp and qs are the children of a node ?7 such that the left sibling ~/p (and 
hence ?7) is on the spine then the following holds: (1) the object 
corresponding to ~/p can have an unboundedly large stack, whereas the 
object for ?7s will have a stack of size one; (2) the top of the stack in these 
objects will be ?Tp and ~Ts; (3) combination of these two sibling nodes is 
possible only after the top parts of these nodes are reached; (4) the stack 
in the object for ~\]p must be propagated to object for 77, except hat the 
top symbol ?7p is replaced by ?7; (5) when the two sibling nodes are 
combined we reach the bottom part of ?7. Hence, we include the 
production b(.. ?7) ---+ t (.. ?Tp) t 07s). 
3. If ?7p, ~\]s are children of ~ as in the previous case except hat ?7p is the 
right sibling and is on the spine, then we include the production 
b ('" n) -~ t(ns) t (..'qp). 
4. If ?Tp, qs, and 77 are as before except hat neither sibling is on the spine of 
an auxiliary tree then we include the production b(.. ?/) ~ t (.- ?7p) t 07s). 
5. If ?7p is the only child of 77 we have b (.. ?7) --+ t (-- 77p ). 
6. If ?7 is a node where fl can be adjoined and we are at the bottom of ~7, 
then, by predicting adjunction by fl, control moves to the bottom part of 
771 (the foot node of fl). This is illustrated in Figure 13. In this case we 
add the production b(.. 77771) --+ b (.. 77). When there is no OA constraint at 
77 then we can predict that no adjunction takes place. This is captured 
with the production t (.. 7/) --+ b (-- ?7). 
7. Suppose we have reached the top part of the root node, 772, of the 
auxiliary tree ft. The corresponding object has the nonterminal t with 772 
on top of the stack and the node at which fl was adjoined is immediately 
below 772. Having reached the top of the root node of fl we must return 
to the top of the node where fl was adjoined. This is accomplished with 
the production t (..) ---+ t (.. 72) (see Figure 13). 
Figure 13 captures the essence of the connection between TAG and LIG-- in par- 
ticular the way the adjoining operation in TAG can be simulated in LIG. This figure 
is also useful in order to understand the notion of terminators. As in the case of CCG, 
the construction of the LIG equivalent of the given grammar is unnecessary. However, 
as in the case of CCG, this discussion of the connection between TAG and LIG can be 
625 
Computational Linguistics Volume 19, Number 4 
A 
7 J 
n 1 
t (I~l) 
I 
b(r'n) / \  
Figure 13 
TAG/LIG relationship. 
used to motivate the choices we make in the form of entries in TAG parser as well as 
the rules in the algorithm. 
5.4 Recognit ion of TAG 
We now give a CKY-style recognition algorithm for TAG. But first we shall consider 
the LIG constructed from a given TAG as described in Section 5.3. Given this LIG 
grammar, consider the objects derived and the form of entries that will be used by the 
LIG algorithm. 
? If ~ is an elementary node address of a node on the spine of an auxiliary 
tree, say fl, then any object that has ~ as the top symbol of its stack must 
be of the form A (~h ... 7\]k~\]t~) where k > 0, A C {t, b}, and/It is the 
elementary node address of a node where fl can be adjoined. 
Furthermore, in any derivation, the terminator of A (~t77) will be b (~rlt). 
? For this LIG, MSL = MTL = TTC = 1 and MCL = 2. Hence it follows that 
any terminator is a 2-terminator. From the discussion above, an object 
A (9~) (where A C {t, b} and len (~) > 0) has a terminator if and only if 
is an elementary node address of a node on the spine of an auxiliary tree. 
? Consider the forms of entries for a LIG in this form. First, the length of 
the middle in a terminator-type entry will be one always, since any 
terminator is a 2-terminator. Note that the terminator of A (9~t~) will be 
b (~t ) .  Thus, a terminator type entry in a parsing array entry, say P \[i, d\] 
will have the form ({A,~)O\]t, ({b,~lt}, \[t, dt\]))) where A c {t,b} and 
It, dr} < li, d}. Note that {b, ~Tt) in the terminator-pointer is edundant. 
? From the discussion above, a non-terminator-type entry wilt be used to 
record derivations from A (7) where A E {t, b} and r/is the elementary 
node address of a node that belongs to an initial tree or of a node that is 
626 
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms 
not on the spine of an auxiliary tree. To record this object the entry 
((A, 71, nil) would have been used. 
From the above discussion it makes sense that terminator-type entries in the TAG 
parser have the form ((A, T\]/ (7t, t, dt)) where A E {t,b}, 7 is an elementary node 
address of a node on the spine of an auxiliary tree, say fl, and /'It is the elementary 
node address of a node where fl can be adjoined in. A non-terminator-type entry has 
the form ((A, 7), nil) where A E {t, b}, and 7 is an elementary node address of a node 
that is not on the spine of an auxiliary tree. 
Finally, consider an auxiliary derivation in the LIG obtained from a TAG as de- 
scribed in Section 5.3. Recall that an auxiliary derivation has the form 
A (~71) ~ TIA1 (~')'172) T2 
T~uAt (9~"yt) wT 2 
In this case we would have: 
? 3/1 = "Yt, 
? A = A1 = t, 
? At = b, and 
? "72 is the root of an auxiliary tree that can be adjoined at the node whose 
elementary node address is given by ~?t. 
Since every auxiliary tree in a lexicalized TAG has at least one terminal node in its 
frontier, every auxiliary derivation spans at least one terminal in the LIG we have 
constructed. 
5.5 Recognition Algorithm 
We begin with a description of the cases involved in TAG recognition algorithm. 
Predicting adjunction: During the recognition phase, on reaching the 
bottom part of a node 7, we predict adjunction by each auxiliary tree, fl 
that can be adjoined at 7 as determined by its SA constraints. As given 
in Case 6 of the construction i  Section 5.3, this prediction is captured 
with the LIG production b(.. ~/~\]1) -~ b (.. 7) where 71 is the foot node of 
the auxiliary tree, ft. Depending on whether 7 is on the spine of an 
auxiliary tree or not, we have the following counterparts of Rule 8.u.L 
and Rule 10.u.L: 
Rule 8.u.T 
71 = (fl, ft(fl)) fl c SA(7) ((b,7) (~h,t, dt)) E P\[i,d\] 
((b, 111) (7, i,d) ) C P\[i,d\] 
Rule 10.u.T 
7, = (fl~ft(fl)) fl C SA(7) ((b~7)~nil) E P\[i~d\] 
((b, 71) (7, i,d)) E P\[i,d\] 
627 
Computational Linguistics Volume 19, Number 4 
As in the second part of Case 6 of the LIG construction (i.e., when there 
is no OA constraint at the node ~7) we have the following counterparts of
Rule 6.u.L and Rule 7.u.L: 
Rule 6.i.u.T 
OA(rl) =false ((b,r/)(r\]t,t, dt)) E P\[i,d\] 
((t,) (,t,t, dt)) E P\[i,d\] 
Rule 7.i.u.T 
OA(~)) =false ((b,~7),nil) E P\[i,d\] 
((t, rl),nil) E P\[i,d\] 
Left sibling on the spine: This corresponds to Case 2 of the LIG 
construction. The following rule that captures this situation corresponds 
to Rule 7.ps.L. 
Rule 7.ps.T 
~p is left child of 
~?p is on the spine of an auxiliary tree 
((t,~/p/(,,,t, dt)) E P\[i, dp\] 
~/p is right child of 7/ 
((t, ~/s), nil) C P \[i + dp, d - dp\] 
((b,,) 01t, t, dt) ) C P\[i,d\] 
The following covers Case 4 of LIG construction where the two siblings 
are not on the spine or belong to an initial tree and corresponds to 
Rule 6.ps.L (or Rule 6.sp.L). 
Rule 6.ps.T 
r/p is left child of z/ 
r/is not on the spine of any auxiliary tree 
((t,,p} ,nil) C P\[i, dp\] ~p is right child of (t, rls}, nil) E P \[i + dp, d -- dp\] 
((b, rl} ,nil) E P\[i,d\] 
Right sibling on the spine: Corresponding to Case 3 of LIG construction 
and Rule 7.sp.L we have 
Rule 7.sp.T 
rls is left child of ~1 
((t,,s} ,nil) E P\[i, ds\] 
~/p is right child of 
Wp is on the spine of an auxiliary tree 
((t,,p} (Zlt, t, dt)) E P\[i+ds,d-ds\] 
((b, rl) (rlt, t, dt)) C P\[i,d\] 
Single child case: Corresponding to Case 5 of LIG construction, 
Rule 7.u.L and Rule 6.u.L. 
Rule 7.ii.u.T 
~p is only child of 
rlp ison the spine of some auxiliary tree ((t,~/p} (rlt, t, dt)) E e\[i,d\] 
((b,,) (rlt, t, dt)) C P\[i,d\] 
628 
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms 
Rule 6.ii.u.T 
~/p is only child of ~/ 
is not on the spine of any auxiliary tree ((t ,%),ni l)  E P\[i,d\] 
((b,r/) ,nil) E P\[i,d\] 
Completing an adjunction: Corresponding to Case 7 of the construction 
and depending on whether the node of adjunction is on the spine of an 
auxiliary tree, we have the following counterparts of Rule 4.u.L, 
Rule 5.u.L. 
Rule 4.u.T 
((t,~lp) (zlt, t, dt)) E P\[i,d\] 
~?t is not on the spine of any auxiliary tree 
((b,z\]t),nil) E Pit, dr\] 
((t,,t) ,nil) E P\[i,d\] 
Rule 5.u.T 
((t,e) (,t,t, dt)) E P\[i,d\] 
7/t is on the spine of an auxiliary tree 
((b,r\]t) (l'\]r, Gdr) ) E P\[t, dt\] 
((t,~\]t) (~lr, r, dr)) E P\[i,d\] 
From the nature of entries being created it will follow that if ~\]p = (fl, e), 
for some auxiliary tree fl, then fl is adjoinable at z\]t. Similarly, if 
~t ~- (fl', #) for some auxiliary tree fl', then fl' is adjoinable at/'Jr. 
Scanning a terminal symbol: If z/is a node labeled by a terminal 
matching the i th input symbol, ai, then we have (corresponding to 
Rule 1.L): 
Rule 1.T 
LABEL(7\])=ai l < i < n 
((t, r/), nil) E P\[i, 1\] 
Scanning empty string: If ~ is a node labeled by e, then we have 
(corresponding to Rule lx.L): 
Rule 1.e.T 
LABEL(z\]) = e 
(It, n) ,nil) E P\[i,0\] 
This concludes our discussion of the parsing rules for TAG. With the correspon- 
dences with the LIG parsing rules given (via the numbering of rules), these rules may 
be placed in the control structure as suggested in Section 5.1. As noted earlier, in the 
case of a lexicalized TAG, since every auxiliary derivation spans at least one terminal 
we do not require the repeat loop discussed in Section 5.1. 
629 
Computational Linguistics Volume 19, Number 4 
6. Conclusion 
In this paper we have presented a general scheme for parsing a set of grammar for- 
malisms whose derivation process is controlled by (explicit or implicit) stacking ma- 
chinery. We have shown how this scheme can be instantiated to give polynomial 
time algorithms for LIG, CCG, and TAG. In the case of CCG, this provides the only 
polynomial parsing algorithm (apart from a slight variant of this scheme given in 
Vijay-Shanker and Weir (1990)) we are aware of. 
The main contribution of this paper is the general recognition scheme and defi- 
nitions of some notions (e.g., terminators, data structures haring of stacks) crucial to 
this scheme. We believe that these ideas can be suitably adapted in order to produce 
parsing schemes based on other CFG parsing algorithms (such as Earley's algorithm). 
For instance, the definition of terminator given here was tailored for pure bottom-up 
parsing. In the case of Earley's algorithm, a bottom-up arser with top-down predic- 
tion, an additional notion of terminator for the top-down prediction component can 
be obtained in a straightforward manner. 
We have also introduced a new method of representing derivations in a TAG, one 
that we believe is appropriate in capturing the stacking that occurs during a TAG 
derivation. The derivations themselves represented can be in another TAG that we 
call the derivation grammar (see Vijay-Shanker and Weir (1993)). 
We have not discussed the extraction of parses after the recognition is complete 
because of space considerations. However, an algorithm to extract he parses and build 
a shared forest representation of all parses for CCG was proposed in Vijay-Shanker 
and Weir (1990). This scheme was based on the approach we have taken in our general 
scheme. The method of extracting parses and representing them using a shared forest 
given in Vijay-Shanker and Weir (1990) can be generalized in a straightforward manner 
to be compatible with the generalized recognition scheme given here. 
Acknowledgments 
This work has been partially supported by 
NSF Grants IRI-8909810 and IRI-9016591. 
We would like to thank A. K. Joshi, B. Lang, 
Y. Schabes, S. M. Shieber, and 
M. J. Steedman for many discussions. We 
are grateful to the anonymous reviewers for 
their numerous uggestions. 
References 
Aho, A. V. (1968). "Indexed grammars--An 
extension to context free grammars." 
J. ACM, 15, 647-671. 
Duske, J., and Parchmann, R. (1984). "Linear 
indexed languages." Theoretical Comput. 
Sci., 32, 47-60. 
Gazdar, G. (1988). "Applicability of indexed 
grammars to natural anguages." In
Natural Language Parsing and Linguistic 
Theories, edited by U. Reyle and 
C. Rohrer. D. Reidel, 69-94. 
Joshi, A. K. (1985). "How much 
context-sensitivity is necessary for 
characterizing structural 
descriptions--tree adjoining grammars." 
In Natural Language Processing--Theoretical, 
Computational nd Psychological Perspective, 
edited by D. Dowty, L. Karttunen, and 
A. Zwicky. Cambridge University Press, 
206-250. 
Joshi, A. K.; Levy, L. S.; and Takahashi, M. 
(1975). "Tree adjunct grammars." \]. 
Comput. Syst. Sci., 10(1), 136-163. 
Kasami, T. (1965). "An efficient recognition 
and syntax algorithm for context-free 
languages." Technical Report 
AF-CRL-65-758, Air Force Cambridge 
Research Laboratory, Bedford, MA. 
Lang, B. (1990). "Towards a uniform formal 
framework for parsing." In Current Issues 
in Parsing Technology, edited by M. Tomita. 
Kluwer Academic Publishers, 153-171. 
Pareschi, R., and Steedman, M. J. (1987). "A 
lazy way to chart-parse with categorial 
grammars." In Proceedings, 25th Meeting of 
the Association for Computational Linguistics, 
81-88. 
Pollard, C. (1984). Generalized Phrase 
Structure Grammars, Head Grammars and 
Natural Language. Doctoral dissertation, 
Stanford University. 
Steedman, M. (1986). "Combinators and 
grammars." In Categorial Grammars and 
630 
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms 
Natural Language Structures, edited by 
R. Oehrle, E. Bach, and D. Wheeler. Foris, 
417-442. 
Steedman, M. J. (1985). "Dependency and 
coordination i the grammar of Dutch and 
English." Language, 61:523-568. 
Tomita, M. (1988). "Graph-structured stack 
and natural language parsing." In 
Proceedings, 26th Meeting of the Association 
for Computational Linguistics, 248-257. 
Vijay-Shanker, K. (1987). A study of tree 
adjoining rammars. Doctoral dissertation, 
University of Pennsylvania, Philadelphia, 
PA. 
Vijay-Shanker, K., and Joshi, A. K. (1985). 
"Some computational properties of tree 
adjoining rammars." In Proceedings, 23rd 
Meeting of the Association for Computational 
Linguistics, 82-93. 
Vijay-Shanker, K., and Weir, D. J. (In press). 
"The equivalence of four extensions of 
context-free grammars." Mathematical 
Systems Theory. 
Vijay-Shanker, K., and Weir, D. J. (1990). 
"Polynomial parsing of combinatory 
categorial grammars." In Proceedings, 28th 
Meeting of the Association for Computational 
Linguistics, Pittsburgh, PA, 1-8. 
Vijay-Shanker, K., and Weir, D. J. (1993). 
"The use of shared forests in TAG 
parsing." In Proceedings, 6th Meeting of the 
European Association for Computational 
Linguistics, Utrecht, The Netherlands, 
384-393. 
Weir, D. J. (1988). Characterizing mildly 
context-sensitive grammar formalisms. 
Doctoral dissertation, University of 
Pennsylvania, Philadelphia, PA. 
Weir, D. J., and Joshi, A. K. (1988). 
"Combinatory categorial grammars: 
Generative power and relationship to 
linear context-free r writing systems." In 
Proceedings, 26th Meeting of the Association 
for Computational Linguistics, 278-285. 
Younger, D. H. (1967). "Recognition and 
parsing of context-free languages in time 
n3.  " Inf. Control, 10(2), 189-208. 
Appendix A: Correctness of Algorithm 1 
We will now prove the correctness of Algorithm 1. In doing so, we will start by 
observing some properties of the rules and the control structure used. 
Firstly, given an input is a l . . .  an, we can note that every entry added by a rule 
(i.e., consequents of rules) satisfies the requirements for the terminator-type and non-  
terminator-type entries; viz., if ((A,'y)(/3, ((At, "Yt), It, dr\]))) is added to an array ele- 
ment P \[i, d\] then 
? A, At E VN, 
? %'YtEVI, 
? fl E V + where 1 < len(fl) <_ MCL-  1 and 
? (t, dt) < li, d) < (1, n) where d _> 2. 
We can also note that if ((A,'71 (fl, nil)) is added to P\[i,d\] then 
? AEVN,  
? "yEVI, 
? f l EV  7 where0~len( f l )  KTTC+MCL-1  and 
? d>_l. 
These can be verified from noting the form of the rules and by simple induction on 
(i, d/. We can also observe from the control structure given that entries to P \[il, dl \]\[/2, d21 
are added before entries are added to PIi3,d3\] \[id,dd\] if and only if (il,dll < (i3,d31 or 
(/1, dl) = (/3, d3) and (/2, d2) > (/4, dd). This observation can be used to show that when 
631 
Computational Linguistics Volume 19, Number 4 
a rule is considered for the purposes of adding an entry to P\[il,dl\] \[/2,d2\] then the 
array elements pecified in the antecedent of that rule would have already been filled. 
Verifying these properties of the algorithm enables us to establish the correctness of 
the algorithm more easily. 
Theorem A.1 
if and only if 
((A, 7) (c~, ((At, 7t), \[t, dt\]))) C P\[i,d\] 
A (flo~"y) ~ a i .  . . a t - lA t  (fl3/t) a t .  . . a i+d_ l  
ai . ? ? a i+d-1  
for some fl such that At (fl3/t) is the len (aT)-terminator f A (fla'~) in this 
derivation and len (flVt) k TTC. 
((A,7) (o~,nil)) E P\[i,d\] 
if and only if 
A (oc'y) ~ ai...ai+d-1 
where A (a7) does not have the TC-property, i.e., A (aT) has no 
terminator in this derivation or the terminator, say At (fl3q), is such that 
len (flTt) < TTC. 
Proof of Soundness: 
We prove the soundness by inducting on d. The base case corresponds to d = 1. We 
have to consider only entries of the form ((A,'y)(c~,nil)) in P\[i, 1\]. Such entries are 
added only by the application of Rule 1. Therefore, we have A (~/) --+ a and a = ai. 
Hence A (c~,y) ~ ai as required. 
Now, for the inductive step, let d > 2. Any entry ((A,3'/(a, tp)) added to P\[i,d\] 
where d > 2 must be due to a rule other than Rule 1.L. This means that we have either 
a production A (.. "/1... 3'm) --* Ap (.. 3'p) As (C~s) or A (.. 71... "Ym) --+ As (O~s) A m ('" q/p). Let 
us assume that the first production was used. We will discuss the cases for m = 0, 
m = 1, and m _> 2 separately. 
Let m = 0. In this case the production is A (..) --* Ap (.. ",/p) As (as). Then 
the entry ((A, "~) (a, tp)) should have been added by using one of rules 
1.ps.L through 5.ps.L. We take Rule 4.ps.L as a representative. If 
((A, "Yl) (fit, nil)) were to be added as a result of this rule, then we have 
to show that A (fit"Y1) :~  ai... ai+d-1 where A (fit"Y1) does not meet the 
TC-property. Since (i, dp) < (i, d I, (i +dp, d - ds) < (i, d), and (t, tit) ( (i, d) 
the inductive hypothesis applies to the three entries in the antecedent. 
Thus, we have for some a the following derivations: 
At  (fit'}q) 
A~ (o~s) 
at . . .  a t+dt -1  
ai+dp ? . .  a i+d_ l  
a i .  ? ? a t - lA t  (oz"/t) at+dr.  ? ? a i+dp-1 
ai ?. ? a i+dp-1 
632 
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms 
such that At (fltTt) does not meet the TC-property. However, Ap (ol'/l'/p) 
satisfies the TC-property and furthermore At (o~'/t) is the 2-terminator of 
Ap (c~'/l'/p). From Observation 2.1, we can also infer the existence of the 
following derivation. 
Ap (flt'/l'/p) ~ ai. . .at-lAt (flt'/t)at+dt . . .ai+dp-1 
ai... ai+dp-1 
Combining this derivation with the derivation from As (c~s) we have 
a( f l t ' /1 )~ 
ai ? ? .  at_l At (flt'Tt ) at+dr. ? ? a i+dp- l  ai+dp ? ? ? a i+d-1 
a i . . .  a i+d_l  
From Observation 2.5, we know that the terminator of At (fit'~t) in this 
derivation is also the terminator of A (fit'~l) (and if At (fit'~t) has no 
terminator then neither does A (fit'/I)). Since At (fit'~t) does not satisfy the 
TC-property (i.e., it does not have a terminator with stack length greater 
than or equal to TTC), A (fit'~l) does not satisfy the TC-property either. 
Thus we have shown the existence of the required derivation. 
Let m = 1. Therefore the production may be written as 
A (.. "/1) --+ Ap (.. %) As (oes). This time we will take Rule 6.ps.L as a 
representative. Hence, we can assume that the entry added to P \[i, d\] has 
the form ((A,'/1)(tip, nil)). Since (i, dp) < (i,d), and 
(i + dp, d -ds)  < (i, d), the inductive hypothesis applies to the two entries 
in the antecedent. Thus, we have the following derivations: 
Ap (gp'/,) a ,  ai+  -i 
As (Oes) ~ ai+G.., ai+d-1 
Therefore we have the derivation: 
A (tip'/l) ~ ap (tip'~p) As (c~s) 
ai . . . . . .  a i+dp-l  ai+dp ? ? ? a i+d-1 
= ai ? ?. a i+d-1 
Note that any terminator of Ap (tip'~p) is also the terminator of A (tip'~l) 
(and if Ap (tip'~p) has no terminator then neither has A (flpVl)). Since 
Ap (tip'~p) does not meet the TC-property in this derivation (from 
inductive hypothesis), neither does A (tip'~p). Thus we have shown the 
existence of the required derivation. 
Let m > 2. We will consider the application of Rule 10.ps.L as a 
representative. Again, applying the inductive hypothesis we have the 
following derivations: 
Ap (tip')p) ~ a i . . .a i+dp-1  
As(o@) ~ a i+dp. . .a i+d_  1 
633 
Computational Linguistics Volume 19, Number 4 
where len (flpTp) > TTC. Combining the two derivations, we have: 
A ( t ip"Y1 . . .  "Ym) ap (~p-yp) As (as) 
ai . . . . . .  ?l i+dp_ l ai-bdp ? . .  a i+d_  l 
ai ? ? ? a iq -d - I  
Since m > 2, Ap (tip'/p) is the m-terminator of A (flp"Yl... "Ym) in the above 
derivation. Since fen (flp'yp) >_ TTC, we have shown the existence of the 
required derivation and that A (tip71...'Ym) satisfies the TC-property. 
In a similar manner we can consider other rules (including those that assume a pro- 
duction of the form A (.. "Yl ' . .  "Ym) ---+ As (c~s) Ap (.. 7p)) as well. 
Proof of Completeness: 
We will now show the completeness of Algorithm 1. This time we use induction on 
the number of steps in a derivation. Suppose A (fl) ~ ai...ai+d-1; we have to show 
that there is a corresponding entry (as specified in Theorem A.1) in P\[i, d\]. 
The base case corresponds to l = 1. From the form of the productions being 
considered we can assume that d = 1 and that there exists a production A (~) ~ ai. 
Rule 1 would apply and thus we have the required entry. 
Let A (fl) ~ ai . . .  ai+d-i where l >__ 1. The first production used in this derivation 
must have the form A (.. "rl . . . "Ym)--+ Ap  ( "  ")'p) As (C~s) or A (.. 3Zl... "ym)---+ As (ees) Ap (.. "yp). 
We will only assume that the production is A (..q,~ . . .  q,,,) ~ Ap (.. 7p)As (c~s). Argu- 
ments similar to the one given below can be used when the production of the form 
A (.. "~1 .. .  7m) --+ As (%) Ap (.. %) is involved as the first step of the derivation. 
Case m = 0: We begin by considering the case when m = 0. Since the first production 
used in A (fl) =~ ai. . . ai+d-1 is A (..) ---+ Ap (.. "yp) As (C~s), we can write the derivation 
as  
A (fl) ~ Ap (9"Yp) As (c~s) 
ai . . .ai+dp-lAs (o~s) 
ai . . . a i+dp- la i+4 . . . a i+d-1  
for some I < dp< d and lp + ls = I. Applying the inductive hypothesis to the derivation 
As  (o@) ~ ai+clp . . .  a i+d-1 ,  we can assume the existence of the entry 
(IAs, tOp(~s)) (rest(C~s),nil) ) 
in P \[i + dp, d -dp\ ] .  
In order to show the existence of the appropriate type of entry corresponding to 
the derivation of ai . . .  ai+cl-1 from A (fl), we need to consider whether A (fl) satisfies the 
TC-property in this derivation. This could depend on whether the primary constituent 
A, (flpVp) does. Since the inductive hypothesis applies for the derivation Ap (flVp) G 
ai . . .  ai+dp-1. Let us start by assuming that A (fl) satisfies the TC-property. This means 
that it has a (say) (k + 1)-terminator whose stack length is greater than or equal to TTC. 
Expressing fl as flt71 . . .  7k, we can then rewrite the derivation from A (fl) as follows. 
a ( f l t "Y l . . .  ~k)  ~ ap  ( f i t l Y1 . . .  ~k"Yp) As  (o@) 
ai  . . . a t -  l A t  (flt"Yt ) at+at . . .  a i+dr -  l ai +d p . . .  a i+d-1  
ai . ? ? a t - la t  ? ? .  a t+dt - la t+d t ? .  ? a i+d-1  
634 
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms 
where At ( f l tq/t)  is the terminator of Ap (fltq/~... q/kq/p). Thus, len ( f l tq/t)  ~ TTC and k _> 1. 
Now, At  (fltq/t) is the terminator of A (fltq/1 . ' '  q/k) if and only if k > 1 (from Observa- 
tion 2.5). 
Let k > 1. At (fltq/t) is the terminator of A (fltq/1 ... q/k) and len (fit'~t) >_ TTC. 
Thus, A (fltq/1 . ."  q/k) satisfies the TC-property. Therefore we must show 
that the entry ((A, q/k) (q/1 ...  q/k-l, ((At, q/t)~ \[t, dt\]))) belongs to P\[i,d\]. By 
inductive hypothesis we may assume 
( (Ap, q/p) ("/1. . . q/k, ((At, q/t) , \[t, dt\]))) belongs to P\[i, dp\]. Now all the 
conditions in the antecedent of Rule 3.ps.L have been met and thus we 
have shown the existence of the appropriate ntry to record the 
derivation of a i . . .  ai+d_l from A (fl). 
Let k -- 1. From Observation 2.5 it follows that the k'-terminator of 
At  (fltq/t) (if it exists) is also the k~-terminator f A (fltq/1), and if At (fltq/t) 
has no terminator then neither does A (fltq/1). Therefore A (fl) = A (flt'yl) 
satisfies the TC-property if and only if At (fit'~t) does. Suppose At (fltq/t) 
satisfies the TC-property; then all conditions tated in the antecedent of
Rule 5.ps.L are met and the appropriate ntry is added to record the 
derivation from A (fl). On the other hand, if At (fit'/t) does not satisfy the 
TC-property then all conditions tated in the antecedent of Rule 4.ps.L 
are met and the appropriate ntry is added to record the derivation 
from A (fl). 
Case m = 1" Here we are concerned with the situation where A (.. ")/1) ---+ Ap (.- q/p) As (as) 
is the first production used in the derivation of a i . . .  ai+d-1 from A (fl). Rewriting fl as 
flpq/1 we have 
A (flpq/1) ~ Ap (flpq/p) As(~s) 
a i . . . . . .  a i+dp- la i+dp ? . .  a i+d_ l  
Applying the inductive hypothesis we have 
((As, top (C~s) ) (rest (o~s), nil)) c P \[i +dp, d - dp\] .
Now any k-terminator f Ap (tipq/p) is also the k-terminator f A (tipq/1) (and if Ap (tipq/,) 
has no terminator then neither does A (flpq/1)). That is, A (flpq/1) satisfies the WC- 
property in this derivation if and only if Ap (fl/~p) does. If Ap (flpq/p) does not satisfy 
the TC-property, then, by inductive hypothesis, we have ((Am, q/p)(tip, nil)) c P\[i, dp\]. 
Thus the entries corresponding to the antecedents of Rule 6.ps.L exist and the algo- 
rithm would have added the entry ((A,q/1)(tip,nil)) c P\[i,d\] as desired. If A m (flpq/p) 
does satisfy the TC-property then Rule 7.ps.L would add the required entry to record 
the derivation from A (fl). 
Case m > 2" Finally, consider that case when m > 2. The given derivation may be 
expressed as 
A (flpq/,...q/m) =~ Ap (flpq/p) As (as) 
ai . . . . . .  a i+dp- l  ai+dp ? ? ? a i+d-1  
= ai ? ? . a i+d-1  
Applying the inductive hypothesis we have 
((As, top (C~s) ) (rest (C~s), nil)) E P \[i +dp, d -dp\]. 
635 
Computational Linguistics Volume 19, Number 4 
Since Ap (flp3`p) is the m-terminator f A (tip3`1... 3`m), we have to consider its length 
in order to know whether A (flp'Yl ... 3`m) satisfies the TC-property, i.e., how it must be 
represented. Suppose len (flp3`p) < TTC, then by inductive hypothesis we have the en- 
try ((Ap, 3`p) (tip,nil)) E P\[i, dp\]. Thus all antecedents of Rule 9.ps.C have been found. 
Since the terminator of A (tip3`1 ... 3`m) has a stack of length less than TTC, the required 
entry, ((A,3`m)(flfYl...'~m-l,nil)), is added by the algorithm by the application of 
Rule 9.ps.L. Suppose len (flp3`p) >__ TTC, then Ap (flp3`p) may or may not be represented 
as a terminator-type entry. Let us take the case where Ap (flp3`p) does not satisfy the 
WC-property. Again by inductive hypothesis, we have the entry ((Ap, 3'p)(tip,nil)) E 
P\[i, dp\]. Since len (flp3`p) > TTC and the antecedents entries of Rule 10.ps.L exist, the al- 
gorithm would add ((A, 3`m)(3'1-.. 3`m--1~ ( ap~ 3`pl ~ \[i~dp\]))) to P\[i~d\] as desired. If we 
had assumed Ap (flp3`p) satisfies the TC-property, then by applying the inductive hy- 
pothesis we can guarantee the existence of the entries corresponding tothe antecedent 
of Rule 8.ps.L, and therefore the algorithm would have added 
((A~ 3`m)(3`1 ..-3`m-1, ((Ap, "YPI , \[i, dp\] ) ) ) 
to P \[i, d\] as desired. 
636 
