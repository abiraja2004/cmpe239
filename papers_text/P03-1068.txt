Towards a Resource for Lexical Semantics:
A Large German Corpus with Extensive Semantic Annotation
Katrin Erk and Andrea Kowalski and Sebastian Pado? and Manfred Pinkal
Department of Computational Linguistics
Saarland University
Saarbru?cken, Germany
{erk, kowalski, pado, pinkal}@coli.uni-sb.de
Abstract
We describe the ongoing construction of
a large, semantically annotated corpus
resource as reliable basis for the large-
scale acquisition of word-semantic infor-
mation, e.g. the construction of domain-
independent lexica. The backbone of the
annotation are semantic roles in the frame
semantics paradigm. We report expe-
riences and evaluate the annotated data
from the first project stage. On this ba-
sis, we discuss the problems of vagueness
and ambiguity in semantic annotation.
1 Introduction
Corpus-based methods for syntactic learning and
processing are well-established in computational
linguistics. There are comprehensive and carefully
worked-out corpus resources available for a num-
ber of languages, e.g. the Penn Treebank (Marcus et
al., 1994) for English or the NEGRA corpus (Skut
et al, 1998) for German. In semantics, the sit-
uation is different: Semantic corpus annotation is
only in its initial stages, and currently only a few,
mostly small, corpora are available. Semantic an-
notation has predominantly concentrated on word
senses, e.g. in the SENSEVAL initiative (Kilgarriff,
2001), a notable exception being the Prague Tree-
bank (Hajic?ova?, 1998) . As a consequence, most
recent work in corpus-based semantics has taken an
unsupervised approach, relying on statistical meth-
ods to extract semantic regularities from raw cor-
pora, often using information from ontologies like
WordNet (Miller et al, 1990).
Meanwhile, the lack of large, domain-
independent lexica providing word-semantic
information is one of the most serious bottlenecks
for language technology. To train tools for the
acquisition of semantic information for such lexica,
large, extensively annotated resources are necessary.
In this paper, we present current work of the
SALSA (SAarbru?cken Lexical Semantics Annota-
tion and analysis) project, whose aim is to provide
such a resource and to investigate efficient methods
for its utilisation. In the current project phase, the
focus of our research and the backbone of the an-
notation are semantic role relations. More specif-
ically, our role annotation is based on the Berke-
ley FrameNet project (Baker et al, 1998; Johnson
et al, 2002). In addition, we selectively annotate
word senses and anaphoric links. The TIGER corpus
(Brants et al, 2002), a 1.5M word German newspa-
per corpus, serves as sound syntactic basis.
Besides the sparse data problem, the most seri-
ous problem for corpus-based lexical semantics is
the lack of specificity of the data: Word meaning is
notoriously ambiguous, vague, and subject to con-
textual variance. The problem has been recognised
and discussed in connection with the SENSEVAL
task (Kilgarriff and Rosenzweig, 2000). Annotation
of frame semantic roles compounds the problem as
it combines word sense assignment with the assign-
ment of semantic roles, a task that introduces vague-
ness and ambiguity problems of its own.
The problem can be alleviated by choosing a suit-
able resource as annotation basis. FrameNet roles,
which are local to particular frames (abstract sit-
uations), may be better suited for the annotation
task than the ?classical? thematic roles concept with
a small, universal and exhaustive set of roles like
agent, patient, theme: The exact extension of the
role concepts has never been agreed upon (Fillmore,
1968). Furthermore, the more concrete frame se-
mantic roles may make the annotators? task easier.
The FrameNet database itself, however, cannot be
taken as evidence that reliable annotation is pos-
sible: The aim of the FrameNet project is essen-
tially lexicographic and its annotation not exhaus-
tive; it comprises representative examples for the use
of each frame and its frame elements in the BNC.
While the vagueness and ambiguity problem may
be mitigated by the using of a ?good? resource, it
will not disappear entirely, and an annotation format
is needed that can cope with the inherent vagueness
of word sense and semantic role assignment.
Plan of the paper. In Section 2 we briefly intro-
duce FrameNet and the TIGER corpus that we use
as a basis for semantic annotation. Section 3 gives
an overview of the aims of the SALSA project, and
Section 4 describes the annotation with frame se-
mantic roles. Section 5 evaluates the first annotation
results and the suitability of FrameNet as an anno-
tation resource, and Section 6 discusses the effects
of vagueness and ambiguity on frame semantic role
annotation. Although the current amount of anno-
tated data does not allow for definitive judgements,
we can discuss tendencies.
2 Resources
SALSA currently extends the TIGER corpus by se-
mantic role annotation, using FrameNet as a re-
source. In the following, we will give a short
overview of both resources.
FrameNet. The FrameNet project (Johnson et al,
2002) is based on Fillmore?s Frame Semantics. A
frame is a conceptual structure that describes a situ-
ation. It is introduced by a target or frame-evoking
element (FEE). The roles, called frame elements
(FEs), are local to particular frames and are the par-
ticipants and props of the described situations.
The aim of FrameNet is to provide a comprehen-
sive frame-semantic description of the core lexicon
of English. A database of frames contains the
frames? basic conceptual structure, and names and
descriptions for the available frame elements. A
lexicon database associates lemmas with the frames
they evoke, lists possible syntactic realizations of
FEs and provides annotated examples from the
BNC. The current on-line version of the frame
database (Johnson et al, 2002) consists of almost
400 frames, and covers about 6,900 lexical entries.
Frame: REQUEST
FE Example
SPEAKER Pat urged me to apply for the job.
ADDRESSEE Pat urged me to apply for the job.
MESSAGE Pat urged me to apply for the job.
TOPIC Kim made a request about changing the title.
MEDIUM Kim made a request in her letter.
Frame: COMMERCIAL TRANSACTION (C T)
BUYER Jess bought a coat.
GOODS Jess bought a coat.
SELLER Kim sold the sweater.
MONEY Kim paid 14 dollars for the ticket.
PURPOSE Kim bought peppers to cook them.
REASON Bob bought peppers because he was hungry.
Figure 1: Example frame descriptions.
Figure 1 shows two frames. The frame REQUEST
involves a FE SPEAKER who voices the request,
an ADDRESSEE who is asked to do something, the
MESSAGE, the request that is made, the TOPIC that
the request is about, and the MEDIUM that is used to
convey the request. Among the FEEs for this frame
are the verb ask and the noun request. In the frame
COMMERCIAL TRANSACTION (henceforth C T), a
BUYER gives MONEY to a SELLER and receives
GOODS in exchange. This frame is evoked e.g. by
the verb pay and the noun money.
The TIGER Corpus. We are using the TIGER
Corpus (Brants et al, 2002), a manually syntacti-
cally annotated German corpus, as a basis for our
annotation. It is the largest available such cor-
pus (80,000 sentences in its final release compared
to 20,000 sentences in its predecessor NEGRA)
and uses a rich annotation format. The annotation
scheme is surface oriented and comparably theory-
neutral. Individual words are labelled with POS
information. The syntactic structures of sentences
are described by relatively flat trees providing in-
formation about grammatical functions (on edge la-
bels), syntactic categories (on node labels), and ar-
gument structure of syntactic heads (through the
use of dependency-oriented constituent structures,
which are close to the syntactic surface). An exam-
ple for a syntactic structure is given in Figure 2.
3 Project overview
The aim of the SALSA project is to construct a large
semantically annotated corpus and to provide meth-
ods for its utilisation.
Corpus construction. In the first phase of the
project, we annotate the TIGER corpus in part man-
Figure 2: A sentence and its syntactic structure.
ually, in part semi-automatically, having tools pro-
pose tags which are verified by human annotators.
In the second phase, we will extend these tools for
the weakly supervised annotation of a much larger
corpus, using the TIGER corpus as training data.
Utilisation. The SALSA corpus is designed to
be utilisable for many purposes, like improving sta-
tistical parsers, and extending methods for informa-
tion extraction and access. The focus in the SALSA
project itself is on lexical semantics, and our first
use of the corpus will be to extract selectional pref-
erences for frame elements.
The SALSA corpus will be tagged with the fol-
lowing types of semantic information:
FrameNet frames. We tag all FEEs that oc-
cur in the corpus with their appropriate frames, and
specify their frame elements. Thus, our focus is
different from the lexicographic orientation of the
FrameNet project mentioned above. As we tag all
corpus instances of each FEE, we expect to en-
counter a wider range of phenomena. which Cur-
rently, FrameNet only exists for English and is still
under development. We will produce a ?light ver-
sion? of a FrameNet for German as a by-product
of the annotation, reusing as many as possible of
the semantic frame descriptions from the English
FrameNet database. Our first results indicate that
the frame structure assumed for the description of
the English lexicon can be reused for German, with
minor changes and extensions.
Word sense. The additional value of word sense
disambiguation in a corpus is obvious. However,
exhaustive word sense annotation is a highly time-
consuming task. Therefore we decided for a selec-
tive annotation policy, annotating only the heads of
frame elements. GermaNet, the German WordNet
version, will be used as a basis for the annotation.
            request         conversation
SPKR
FEE
ADD
MSG
FEE FEE
TOPIC
INTLC_1
Figure 3: Frame annotation.
Coreference. Similarly, we will selectively anno-
tate coreference. If a lexical head of a frame element
is an anaphor, we specify the antecedent to make the
meaning of the frame element accessible.
4 Frame Annotation
Annotation schema. To give a first impression of
frame annotation, we turn to the sentence in Fig. 2:
(1) SPD fordert Koalition zu Gespra?ch u?ber Re-
form auf.
(SPD requests that coalition talk about reform.)
Fig. 3 shows the frame annotation associated with
(1). Frames are drawn as flat trees. The root node is
labelled with the frame name. The edges are labelled
with abbreviated FE names, like SPKR for SPEAKER,
plus the tag FEE for the frame-evoking element. The
terminal nodes of the frame trees are always nodes
of the syntactic tree. Cases where a semantic unit
(FE or FEE) does not form one syntactic constituent,
like fordert . . . auf in the example, are represented
by assignment of the same label to several edges.
Sentence (1), a newspaper headline, contains at
least two FEEs: auffordern and Gespra?ch. auf-
fordern belongs to the frame REQUEST (see Fig. 1).
In our example the SPEAKER is the subject NP SPD,
the ADDRESSEE is the direct object NP Koalition,
and the MESSAGE is the complex PP zu Gespra?ch
u?ber Reform. So far, the frame structure follows the
syntactic structure, except for that fact that the FEE,
as a separable prefix verb, is realized by two syntac-
tic nodes. However, it is not always the case that
frame structure parallels syntactic structure. The
second FEE Gespra?ch introduces the frame CON-
VERSATION. In this frame two (or more) groups
talk to one another and no participant is construed
as only a SPEAKER or only an ADDRESSEE. In
our example the only NP-internal frame element is
the TOPIC (?what the message is about?) u?ber Re-
form, whereas the INTERLOCUTOR-1 (?the promi-
nent participant in the conversation?) is realized by
the direct object of auffordern.
As shown in Fig. 3, frames are annotated as trees
of depth one. Although it might seem semantically
more adequate to admit deeper frame trees, e.g. to
allow the MSG edge of the REQUEST frame in Fig.
3 to be the root node of the CONVERSATION tree,
as its ?real? semantic argument, the representation
of frame structure in terms of flat and independent
semantic trees seems to be preferable for a number
of practical reasons: It makes the annotation process
more modular and flexible ? this way, no frame an-
notation relies on previous frame annotation. The
closeness to the syntactic structure makes the an-
notators? task easier. Finally, it facilitates statistical
evaluation by providing small units of semantic in-
formation that are locally related to syntax.
Difficult cases. Because frame elements may
span more than one sentence, like in the case of
direct speech, we cannot restrict ourselves to an-
notation at sentence level. Also, compound nouns
require annotation below word level. For ex-
ample, the word ?Gagenforderung? (demand for
wages) consists of ?-forderung? (demand), which in-
vokes the frame REQUEST, and a MESSAGE element
?Gagen-?. Another interesting point is that one word
may introduce more than one frame in cases of co-
ordination and ellipsis. An example is shown in (2).
In the elliptical clause only one fifth for daughters,
the elided bought introduces a C T frame. So we let
the bought in the antecedent introduce two frames,
one for the antecedent and one for the ellipsis.
(2) Ein Viertel aller Spielwaren wu?rden fu?r So?hne
erworben, nur ein Fu?nftel fu?r To?chter.
(One quarter of all toys are bought for sons, only one fifth
for daughters.)
Annotation process. Frame annotation proceeds
one frame-evoking lemma at a time, using subcor-
pora containing all instances of the lemma with
some surrounding context. Since most FEEs are
polysemous, there will usually be several frames rel-
evant to a subcorpus. Annotators first select a frame
for an instance of the target lemma. Then they assign
frame elements.
At the moment the annotation uses XML tags on
bare text. The syntactic structure of the TIGER-
sentences can be accessed in a separate viewer. An
annotation tool is being implemented that will pro-
vide a graphical interface for the annotation. It will
display the syntactic structure and allow for a graph-
ical manipulation of semantic frame trees, in a simi-
lar way as shown in Fig. 3.
Extending FrameNet. Since FrameNet is far
from being complete, there are many word senses
not yet covered. For example the verb fordern,
which belongs to the REQUEST frame, additionally
has the reading challenge, for which the current ver-
sion of FrameNet does not supply a frame.
5 Evaluation of Annotated Data
Materials. Compared to the pilot study we previ-
ously reported (Erk et al, 2003), in which 3 annota-
tors tagged 440 corpus instances of a single frame,
resulting in 1,320 annotation instances, we now dis-
pose of a considerably larger body of data. It con-
sists of 703 corpus instances for the two frames
shown in Figure 1, making up a total of 4,653 an-
notation instances. For the frame REQUEST, we
obtained 421 instances with 8-fold and 114 with
7-fold annotation. The annotated lemmas com-
prise auffordern (to request), fordern, verlangen (to
demand), zuru?ckfordern (demand back), the noun
Forderung (demand), and compound nouns ending
with -forderung. For the frame C T we have 30, 40
and 98 instances with 5-, 3-, and 2-fold annotation
respectively. The annotated lemmas are kaufen (to
buy), erwerben (to acquire), verbrauchen (to con-
sume), and verkaufen (to sell).
Note that the corpora we are evaluating do not
constitute a random sample: At the moment, we
cover only two frames, and REQUEST seems to be
relatively easy to annotate. Also, the annotation re-
sults may not be entirely predictive for larger sam-
ple sizes: While the annotation guidelines were be-
ing developed, we used REQUEST as a ?calibration?
frame to be annotated by everybody. As a result, in
some cases reliability may be too low because de-
tailed guidelines were not available, and in others
it may be too high because controversial instances
were discussed in project meetings.
Results. The results in this section refer solely to
the assignment of fully specified frames and frame
elements. Underspecification is discussed at length
frames average best worst
REQUEST 96.83% 100% 90.73%
COMM. 97.11% 98.96% 88.71%
elements average best worst
REQUEST 88.86% 95.69% 66.57%
COMM. 74.25% 90.30% 69.33%
Table 1: Inter-annotator agreement on frames (top)
and frame elements (below).
in Section 6. Due to the limited space in this pa-
per, we only address the question of inter-annotator
agreement or annotation reliability, since a reliable
annotation is necessary for all further corpus uses.
Table 1 shows the inter-annotator agreement on
frame assignment and on frame element assignment,
computed for pairs of annotators. The ?average?
column shows the total agreement for all annotation
instances, while ?best? and ?worst? show the fig-
ures for the (lemma-specific) subcorpora with high-
est and lowest agreement, respectively. The upper
half of the table shows agreement on the assignment
of frames to FEEs, for which we performed 14,410
pairwise comparisons, and the lower half shows
agreement on assigned frame elements (29,889 pair-
wise comparisons). Agreement on frame elements is
?exact match?: both annotators have to tag exactly
the same sequence of words. In sum, we found that
annotators agreed very well on frames. Disagree-
ment on frame elements was higher, in the range of
12-25%. Generally, the numbers indicated consider-
able differences between the subcorpora.
To investigate this matter further, we computed
the Alpha statistic (Krippendorff, 1980) for our an-
notation. Like the widely used Kappa, ? is a chance-
corrected measure of reliability. It is defined as
? = 1 ? observed disagreement
expected disagreement
We chose Alpha over Kappa because it also indi-
cates unreliabilities due to unequal coder preference
for categories. With an ? value of 1 signifying total
agreement and 0 chance agreement, ? values above
0.8 are usually interpreted as reliable annotation.
Figure 4 shows single category reliabilities for
the assignment of frame elements. The graphs
shows that not only did target lemmas vary in
their difficulty, but that reliability of frame ele-
ment assignment was also a matter of high varia-
tion. Firstly, frames introduced by nouns (Forderung
and -forderung) were more difficult to annotate than
verbs. Secondly, frame elements could be assigned
to three groups: frame elements which were al-
ways annotated reliably, those whose reliability was
highly dependent on the FEE, and the third group
whose members were impossible to annotate reli-
ably (these are not shown in the graphs). In the
REQUEST frames, SPEAKER, MESSAGE and AD-
DRESSEE belong to the first group, at least for verbal
FEEs. MEDIUM is a member of the second group,
and TOPIC was annotated at chance level (? ? 0).
In the COMMERCE frame, only BUYER and GOODS
always show high reliability. SELLER can only be re-
liably annotated for the target verkaufen. PURPOSE
and REASON fall into the third group.
5.1 Discussion
Interpretation of the data. Inter-annotator agree-
ment on the frames shown in Table 1 is very high.
However, the lemmas we considered so far were
only moderately ambiguous, and we might see lower
figures for frame agreement for highly polysemous
FEEs like laufen (to run).
For frame elements, inter-annotator agreement
is not that high. Can we expect improvement?
The Prague Treebank reported a disagreement of
about 10% for manual thematic role assignment
( ?Zabokrtsky?, 2000). However, in contrast to our
study, they also annotated temporal and local modi-
fiers, which are easier to mark than other roles.
One factor that may improve frame element
agreement in the future is the display of syntactic
structure directly in the annotation tool. Annotators
were instructed to assign each frame element to a
single syntactic constituent whenever possible, but
could only access syntactic structure in a separate
viewer. We found that in 35% of pairwise frame ele-
ment disagreements, one annotator assigned a single
syntactic constituent and the other did not. Since a
total of 95.6% of frame elements were assigned to
single constituents, we expect an increase in agree-
ment when a dedicated annotation tool is available.
As to the pronounced differences in reliability be-
tween frame elements, we found that while most
central frame elements like SPEAKER or BUYER
were easy to identify, annotators found it harder to
agree on less frequent frame elements like MEDIUM,
PURPOSE and REASON. The latter two with their
 0.6
 0.8
 1
auffordern fordern verlangen Forderung -forderung
a
lp
ha
 v
al
ue
addressee
medium
message
speaker
 0.6
 0.8
 1
erwerben kaufen verkaufen
a
lp
ha
 v
al
ue
buyer
seller
money
goods
Figure 4: Alpha values for frame elements. Left: REQUEST. Right: COMMERCIAL TRANSACTION.
particularly low agreement (? < 0.8) contribute to-
wards the low overall inter-annotator agreement of
the C T frame. We suspect that annotators saw too
few instances of these elements to build up a reli-
able intuition. However, the elements may also be
inherently difficult to distinguish.
How can we interpret the differences in frame el-
ement agreement across target lemmas, especially
between verb and noun targets? While frame ele-
ments for verbal targets are usually easy to identify
based on syntactic factors, this is not the case for
nouns. Figure 3 shows an example: Should SPD
be tagged as INTERLOCUTOR-2 in the CONVERSA-
TION frame? This appears to be a question of prag-
matics. Here it seems that clearer annotation guide-
lines would be desirable.
FrameNet as a resource for semantic role an-
notation. Above, we have asked about the suitabil-
ity of FrameNet for semantic role annotation, and
our data allow a first, though tentative, assessment.
Concerning the portability of FrameNet to other
languages than English, the English frames worked
well for the German lemmas we have seen so far.
For C T a number of frame elements seem to be
missing, but these are not language-specific, like
CREDIT (for on commission and in installments).
The FrameNet frame database is not yet complete.
How often do annotators encounter missing frames?
The frame UNKNOWN was assigned in 6.3% of the
instances of REQUEST, and in 17.6% of the C T in-
stances. The last figure is due to the overwhelm-
ing number of UNKNOWN cases in verbrauchen, for
which the main sense we encountered is ?to use up
a resource?, which FrameNet does not offer.
Is the choice of frame always clear? And can
frame elements always be assigned unambiguously?
Above we have already seen that frame element as-
signment is problematic for nouns. In the next sec-
tion we will discuss problematic cases of frame as-
signment as well as frame element assignment.
6 Vagueness, Ambiguity and
Underspecification
Annotation Challenges. It is a well-known prob-
lem from word sense annotation that it is often im-
possible to make a safe choice among the set of pos-
sible semantic correlates for a linguistic item. In
frame annotation, this problem appears on two lev-
els: The choice of a frame for a target is a choice
of word sense. The assignment of frame elements to
phrases poses a second disambiguation problem.
An example of the first problem is the Ger-
man verb verlangen, which associates with both the
frame REQUEST and the frame C T. We found sev-
eral cases where both readings seem to be equally
present, e.g. sentence (3). Sentences (4) and (5) ex-
emplify the second problem. The italicised phrase in
(4) may be either a SPEAKER or a MEDIUM and the
one in (5) either a MEDIUM or not a frame element
at all. In our exhaustive annotation, these problems
are much more virulent than in the FrameNet corpus,
which consists mostly of prototypical examples.
(3) Gleichwohl versuchen offenbar Assekuranzen,
[das Gesetz] zu umgehen, indem sie von Nicht-
deutschen mehr Geld verlangen.
(Nonetheless insurance companies evidently try to cir-
cumvent [the law] by asking/demanding more money
from non-Germans.)
(4) Die nachhaltigste Korrektur der Programmatik
fordert ein Antrag. . .
(The most fundamental policy correction is requested by
a motion. . . )
(5) Der Parteitag billigte ein Wirtschaftskonzept, in
dem der Umbau gefordert wird.
(The party congress approved of an economic concept in
which a change is demanded.)
Following Kilgarriff and Rosenzweig (2000), we
distinguish three cases where the assignment of a
single semantic tag is problematic: (1), cases in
which, judging from the available context informa-
tion, several tags are equally possible for an ambigu-
ous utterance; (2), cases in which more than one tag
applies at the same time, because the sense distinc-
tion is neutralised in the context; and (3), cases in
which the distinction between two tags is systemati-
cally vague or unclear.
In SALSA, we use the concept of underspecifica-
tion to handle all three cases: Annotators may assign
underspecified frame and frame element tags. While
the cases have different semantic-pragmatic status,
we tag all three of them as underspecified. This is in
accordance with the general view on underspecifica-
tion in semantic theory (Pinkal, 1996). Furthermore,
Kilgarriff and Rosenzweig (2000) argue that it is im-
possible to distinguish those cases
Allowing underspecified tags has several advan-
tages. First, it avoids (sometimes dubious) decisions
for a unique tag during annotation. Second, it is use-
ful to know if annotators systematically found it hard
to distinguish between two frames or two frame ele-
ments. This diagnostic information can be used for
improving the annotation scheme (e.g. by removing
vague distinctions). Third, underspecified tags may
indicate frame relations beyond an inheritance hier-
archy, horizontal rather than vertical connections. In
(3), the use of underspecification can indicate that
the frames REQUEST and C T are used in the same
situation, which in turn can serve to infer relations
between their respective frame elements.
Evaluating underspecified annotation. In the
previous section, we disregarded annotation cases
involving underspecification. In order to evalu-
ate underspecified tags, we present a method of
computing inter-annotator agreement in the pres-
ence of underspecified annotations. Represent-
ing frames and frame elements as predicates that
each take a sequence of word indices as their
argument, a frame annotation can be seen as a
pair (CF,CE) of two formulae, describing the
frame and the frame elements, respectively. With-
out underspecification, CF is a single predicate
and CE is a conjunction of predicates. For the
CONVERSATION frame of sentence (1), CF has
the form CONVERSATION(Gespra?ch)1 , and CE is
INTLC 1(Koalition) ? TOPIC(u?ber Reform). Un-
derspecification is expressed by conjuncts that are
disjunctions instead of single predicates. Table 2
shows the admissible cases. For example, the CE
of (4) contains the conjunct SPKR(ein Antrag) ?
MEDIUM(ein Antrag). Our annotation scheme guar-
antees that every FE name appears in at most one
conjunct of CE. Exact agreement means that ev-
ery conjunct of annotator A must correspond to a
conjunct by annotator B, and vice versa. For partial
agreement, it suffices that for each conjunct of A,
one disjunct matches a disjunct in a conjunct of B,
and conversely.
frame annotation
F(t) single frame: F is assigned to t
(F1(t)?F2(t)) frame disjunction: F1 or F2 is
assigned to t
frame element annotation
E(s) single frame element: E is as-
signed to s
(E1(s)?E2(s)) frame element disjunction: E1
or E2 is assigned to s
(E(s)?NOFE(s)) optional element: E1 or no
frame element is assigned to s
(E(s)?E(s1ss2)) underspecified length: frame
element E is assigned to s
or the longer sequence s1ss2,
which includes s
Table 2: Types of conjuncts. F is a frame name, E
a frame element name, and t and s are sequences of
word indices (t is for the target (FEE))
Using this measure of partial agreement, we now
evaluate underspecified annotation. The most strik-
ing result is that annotators made little use of under-
specification. Frame underspecification was used in
0.4% of all frames, and frame element underspecifi-
cation for 0.9% of all frame elements. The frame el-
ement MEDIUM, which was rarely assigned outside
1We use words instead of indices for readability.
underspecification, accounted for roughly half of all
underspecification in the REQUEST frame. 63% of
the frame element underspecifications are cases of
optional elements, the third class in the lower half of
Table 2. (Partial) agreement on underspecified tags
was considerably lower than on non-underspecified
tags, both in the case of frames (86%) and in the
case of frame elements (54%). This was to be ex-
pected, since the cases with underspecified tags are
the more difficult and controversial ones. Since un-
derspecified annotation is so rare, overall frame and
frame element agreement including underspecified
annotation is virtually the same as in Table 1.
It is unfortunate that annotators use underspecifi-
cation only infrequently, since it can indicate inter-
esting cases of relatedness between different frames
and frame elements. However, underspecification
may well find its main use during the merging of
independent annotations of the same corpus. Not
only underspecified annotation, also disagreement
between annotators can point out vague and ambigu-
ous cases. If, for example, one annotator has as-
signed SPEAKER and the other MEDIUM in sentence
(4), the best course is probably to use an underspec-
ified tag in the merged corpus.
7 Conclusion
We presented the SALSA project, the aim of which
is to construct and utilize a large corpus reliably
annotated with semantic information. While the
SALSA corpus is designed to be utilizable for many
purposes, our focus is on lexical semantics, in or-
der to address one of the most serious bottlenecks
for language technology today: the lack of large,
domain-independent lexica.
In this paper we have focused on the annotation
with frame semantic roles. We have presented the
annotation scheme, and we have evaluated first an-
notation results, which show encouraging figures for
inter-annotator agreement. We have discussed the
problem of vagueness and ambiguity of the data and
proposed a representation for underspecified tags,
which are to be used both for the annotation and the
merging of individual annotations.
Important next steps are: the design of a tool for
semi-automatic annotation, and the extraction of se-
lectional preferences from the annotated data.
Acknowledgments. We would like to thank the
following people, who helped us with their sugges-
tions and discussions: Sue Atkins, Collin Baker,
Ulrike Baldewein, Hans Boas, Daniel Bobbert,
Sabine Brants, Paul Buitelaar, Ann Copestake,
Christiane Fellbaum, Charles Fillmore, Gerd Flied-
ner, Silvia Hansen, Ulrich Heid, Katja Markert and
Oliver Plaehn. We are especially indebted to Maria
Lapata, whose suggestions have contributed to the
current shape of the project in an essential way. Any
errors are, of course, entirely our own.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceedings of
COLING-ACL, Montreal, Canada.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lez-
ius, and George Smith. 2002. The TIGER treebank. In
Proceedings of the Workshop on Treebanks and Linguistic
Theories, Sozopol, Bulgaria.
Katrin Erk, Andrea Kowalski, and Manfred Pinkal. 2003. A
corpus resource for lexical semantics. In Proceedings of
IWCS5, pages 106?121, Tilburg, The Netherlands.
Charles J. Fillmore. 1968. The case for case. In Bach and
Harms, editors, Universals in Linguistic Theory, pages 1?88.
Holt, Rinehart, and Winston, New York.
Eva Hajic?ov a?. 1998. Prague Dependency Treebank: From An-
alytic to Tectogrammatical Annotation. In Proceedings of
TSD?98, pages 45?50, Brno, Czech Republic.
C. R. Johnson, C. J. Fillmore, M. R. L. Petruck, C. F. Baker,
M. Ellsworth, J. Ruppenhofer, and E. J. Wood. 2002.
FrameNet: Theory and Practice. http://www.icsi.
berkeley.edu/?framenet/book/book.html.
Adam Kilgarriff and Joseph Rosenzweig. 2000. Framework
and results for English Senseval. Computers and the Hu-
manities, 34(1-2).
Adam Kilgarriff, editor. 2001. SENSEVAL-2, Toulouse.
Klaus Krippendorff. 1980. Content Analysis. Sage.
M. Marcus, G. Kim, M.A. Marcinkiewicz, R. MacIntyre,
A. Bies, M. Gerguson, K. Katz, and B. Schasberger. 1994.
The Penn Treebank: Annotating predicate argument struc-
ture. In Proceedings of the ARPA HLT Workshop.
G. Miller, R. Beckwith, C. Fellbaum, D. Gros, and K. Miller.
1990. Introduction to WordNet: An on-line lexical database.
International Journal of Lexicography, 3(4):235?44.
Manfred Pinkal. 1996. Vagueness, ambiguity, and underspeci-
fication. In Proceedings of SALT?96, pages 185?201.
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and Hans
Uszkoreit. 1998. A linguistically interpreted corpus of Ger-
man newspaper text. In Proceedings of LREC?98, Granada.
Zdene?k ?Zabokrtsk y?. 2000. Automatic functor assignment
in the Prague Dependency Treebank. In Proceedings of
TSD?00, Brno, Czech Republic.
