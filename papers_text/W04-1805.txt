Discovering Specific Semantic Relationships between Nouns and
Verbs in a Specialized French Corpus
Vincent CLAVEAU and Marie-Claude L'HOMME
OLST - University of Montreal
C.P. 6128, succ. Centre-Ville
Montr?al, QC, H3C 3J7
Canada
{Vincent.Claveau,Marie-Claude.L'Homme}@umontreal.ca
Abstract
Recent literature in computational termi-
nology has shown an increasing interest in
identifying various semantic relationships
between terms. In this paper, we pro-
pose an original strategy to find specific
noun-verb combinations in a specialized cor-
pus. We focus on verbs that convey a
meaning of realization. To acquire these
noun-verb pairs, we use asares, a machine
learning technique that automatically in-
fers extraction patterns from examples and
counter-examples of realization noun-verb
pairs. The patterns are then applied to the
corpus to retrieve new pairs. Results, mea-
sured with a large test set, show that our
acquisition technique outperforms classical
statistical methods used for collocation ac-
quisition. Moreover, the inferred patterns
yield interesting clues on which structures
are more likely to convey the target seman-
tic link.
1 Introduction
Recent literature in computational terminology
has shown an increasing interest in identifying
various semantic relationships between terms.
Different strategies have been developed in or-
der to identify pairs of terms that share a spe-
cific semantic relationship (such as hyperonymy
or meronymy) or to build classes of terms.
However, most strategies are based on in-
ternal or external methods (Grabar and
Zweigenbaum, 2002), i.e. methods that rely on
the form of terms or on the information gathered
from contexts. (In some cases, an additional
resource, such as a dictionary or a thesaurus,
is used during the identification process.) The
work reported here infers specific semantic rela-
tionships based on sets of examples and counter-
examples.
In this paper, the method is applied to a
French corpus on computing to find noun-verb
combinations in which verbs convey a meaning
of realization. The work is carried out in order
to assist terminographers in the enrichment of a
dictionary on computing that includes colloca-
tional information (L'Homme, 2004).
Even though this work is carried out for ter-
minographical and lexicographical purposes, it
can certainly be of use in other applications,
namely information retrieval. Indeed, such rich
semantic links can be used to extend indices
or reformulate queries (similar to the work by
Voorhees (1994) with WordNet relations).
2 Objectives
The noun-verb combinations we aim to identify
have the following characteristics. They share:
? A syntactic relationship : nouns can be
subjects (e.g., ordinateur tourne; com-
puter runs); direct objects (e.g., configurer
l'application; configure the application); or
second complement (e.g., charger x dans la
m?moire;  load x into memory);
? A valid semantic relationship. The follow-
ing semantic relationships are sought:
1. verbs that refer to activities carried out
by the nouns (e.g., serveur d?marre;
server starts);
2. verbs that refer to uses of the nouns
(e.g., activer une option; activate an
option or naviguer sur Internet; surf
the Internet);
3. verbs that refer to activities carried out
by means of the nouns (e.g., activer
l'option au moyen de cette commande;
activate this option with this com-
mand ); and, finally,
4. verbs that refer to the processes by
which nouns are prepared fur use (e.g.,
 installer un logiciel; install an applica-
tion).
These noun-verb combinations will hereafter be
called valid N-V pairs.
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 39
The semantic relationships listed above cor-
respond to a set of lexical functions (LFs) de-
fined in (Mel'?uk et al, 1984 1999), i.e. LFs
used to represent realization (e.g., Facti, Reali,
Labrealij) and to represent the process by which
something is prepared (Prepar)1. (Both of
these types of LFs can be combined with oth-
ers [to create complex lexical functions].) These
LFs are opposed to support verbs represented
by the LFs Funci, Operi, Laborij (e.g., cr?er un
fichier; create a file; definir une variable; de-
fine a variable).).
Realization verbs (and verbs denoting the
preparation of nouns) were chosen since they
are believed to be frequent in technical corpora,
such as the corpus of computing used in this ex-
periment. However, this is seen as a first step
in order to validate an acquisition process for
semantically-related V-N pairs. Other semantic
relationships could be sought in the future.
3 Related work
A number of applications have relied on distri-
butional analysis (Harris, 1971) in order to build
classes of semantically related terms. This ap-
proach, which uses words that appear in the con-
text of terms to formulate hypotheses on their
semantic relatedness (Habert et al, 1996, for ex-
ample), does not specify the relationship itself.
Hence, synonyms, co-hyponyms, hyperonyms,
etc. are not differentiated.
More recent work on terminology structuring
has focussed on formal similarity to develop hy-
potheses on the semantic relationships between
terms: Daille (2003) uses derivational morphol-
ogy; Grabar and Zweigenbaum (2002) use, as a
starting point, a number of identical characters.
Up to now, the focus has been on nouns
and adjectives, since these structuring methods
have been applied to lists of extracted candidate
terms (Habert et al, 1996; Daille, 2003) or to
lists of admitted terms (Grabar and Zweigen-
baum, 2002). As a consequence, relationships
considered have been mostly synonymic or tax-
onomic, or defined as term variations.
On the other hand, other work has been car-
ried out in order to acquire collocations. Most of
these endeavours have focused on purely statis-
tical acquisition techniques (Church and Hanks,
1
However, our interpretation of LFs in this work is
much looser, since we admitted verbs that would not be
considered to be members of true collocations as Mel'?uk
et al (1984 1999) define them, i.e. groups of lexical units
that share a restricted cooccurrence relationship.
1990), on linguisitic acquisition (by the use of
Part-of-Speech filters hand-crafted by a linguist)
(Oueslati, 1999) or, more frequently, on a combi-
nation of the two (Smadja, 1993; Kilgarriff and
Tugwell, 2001, for example). It is worth noting
that although these techniques are able to iden-
tify N-V pairs, they do not specify the relation-
ship between N and V, nor are they capable of
focusing on a subset of N-V pairs. The original
acquisition methodology we present in the next
section will allow us to overcome this limitation.
4 Methodology for finding valid
noun-verb pairs
This section is devoted to the description of the
methodology and the data we use to acquire
semantically related noun-verb pairs. We first
describe the specialized corpus used in this ex-
periment. Then, we briefly present asares, a
pattern inference tool, on which our acquisition
strategy relies. Finally, we explain the different
steps of the acquisition process.
4.1 Computer science corpus
The French corpus used in our experiments is
composed of more than 50 articles from books
or web sites specialized in computer science; all
of them were published between 1988 and 2003.
It covers different computer science sub-domains
(networking, managing Unix computers, web-
cams...) and comprises 600,000 words.
Segmentation, morpho-syntactic tagging and
lemmatization have been carried out using the
tool cordial
2
. Each word is accompanied by
its lemma and Part-of-Speech tag (noun, verb,
adjective). Also, the tool indicates inflection
(gender and number for nouns, tense and per-
son for verbs) and gives syntactic information
(head-modifier) for noun phrases.
4.2 Overview of asares
The method used for the acquisition of N-V
pairs relies mainly on asares, a pattern in-
ference tool. Asares is presented in detail in
(Claveau et al, 2003). We simply give a short
account of its basic principles herein.
Asares is based on a Machine Learning
technique, Inductive Logic Programming (ILP)
(Muggleton and De-Raedt, 1994), which infers
general morpho-syntactic patterns from a set
of examples (this set is noted E+ hereafter)
and counter-examples (E?) of the elements one
2
Cordial is a commercial product of Synapse-
D?veloppement.
CompuTerm 2004  -  3rd International Workshop on Computational Terminology40
wants to acquire and their context. The con-
textual patterns produced can then be applied
to the corpus in order to retrieve new elements.
The acquisition process can be summarized in 3
steps:
1. construction of the sets of examples (and
counter-examples);
2. inference of extraction patterns with
asares; and
3. extraction of N-V pairs from the corpus
with the inferred patterns.
Asares has been previously applied to the ac-
quisition of word pairs sharing semantic rela-
tions defined in the Generative Lexicon frame-
work (Pustejovsky, 1995) and called qualia rela-
tions (Bouillon et al, 2001). Here, we propose
to use asares in a quite similar way to retrieve
our valid N-V pairs. However, the N-V combi-
nations sought are more specific than those that
were identified in these previous experiments.
Formally, ILP aims at inferring logic pro-
grams (sets of Horn clauses, notedH) from a set
of facts (examples and counter-examples of the
concept to be learnt) and background knowledge
(B), such that the program H logically entails
the examples with respect to the background
knowledge and rejects (most of) the counter-
examples. This is transcribed by the two logical
formulae B ?H |= E+, B ?H 6|= E?, which set
the aim of an ILP algorithm.
In this framework, asares infers clauses ex-
pressing morpho-syntactic patterns that gener-
alize the structures of sentences containing the
target element (examples) but not the structures
of the sentences containing counter-examples.
The background knowledge encodes information
about each word occurring in the example or
counter-example, namely the meaning of its tag
(e.g., adjective in plural form, infinitive verb).
The main benefits of this acquisition tech-
nique lie in the inferred patterns. Indeed, con-
trary to the more classical statistical methods
(Mutual Information, Loglike..., see below) used
for collocation acquisition (see (Pearce, 2002)
for a review), these patterns allow:
1. understanding of the results, that is, why a
specific element has been retrieved or not;
2. highlighting of the corpus-specific struc-
tures conveying the target element.
In addition to its explanatory capacity, this sym-
bolic acquisition technique has obtained good
results for other acquisition tasks when com-
pared to existing statistical techniques (Bouillon
et al, 2002).
4.3 Acquisition process
To infer extraction patterns, asares needs a set
of examples (E+) and a set of counter-examples
(E?) of the elements we want to retrieve. In
our case, E+ must thus be composed of (POS-
tagged) sentences containing valid N-V pairs;
conversely, E? must be composed of sentences
containing non-valid N-V pairs. While this step
is tedious and usually carried out manually, the
originality of our work lies in the fact that E+
and E? are obtained automatically.
To produce the positive examples, we use the
existing entries of a terminological database we
are currently developing. These entries are thus
a kind of bootstrap in our acquisition process.
More precisely, every N-V pair in which V is de-
fined in the database as a realization verb for N
is included. Then, all sentences in our corpus
containing this N-V pair are considered as ex-
amples and added to E+. Note that we do not
check if each occurrence of the N-V pair actually
shares the target semantic link or even a syn-
tactic link in the sentences that are extracted.
Some of the examples in E+ might be incorrect,
but asares tolerates a certain amount of noise.
A totally different technique is needed to pro-
duce the E? set, since no information concern-
ing verbs that are not semantically related is
available in the terminological database. To ob-
tain a list of invalid N-V pairs, we acquire them
from our corpus using a statistical technique.
This produces a list of all N-V pairs that ap-
pear in the same sentence, and assigns each a
score. Many statistical coefficients exist (Man-
ning and Sch?tze, 1999); most of them can be
easily expressed with the help of a contingency
table similar to that reproduced in Table 1 and
by noting S = a + b + c + d. For example, the
Vj Vk, k 6= j
Ni a b
Nl, l 6= i c d
Table 1: Contingency table for the pair Ni-Vj
Mutual Information coefficient is defined as:
MI = log2 a(a+ b)(a+ c)
and the loglike coefficient (Dunning, 1993) as:
Log = a log a + b log b + c log c + d log d ? (a +
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 41
b) log(a+ b)? (a+ c) log(a+ c)? (b+ d) log(b+
d)? (c+ d) log(c+ d) + S log S.
In the work presented here, we have adopted
the Loglike coefficient. From the N-V pair list
produced with this method, we have chosen the
pairs that obtained the lower Loglike scores. As
for the positive examples, we consider that each
sentence containing one of the pairs is a counter-
example and is added to E?.
Finally, asares is launched with these E+
and E? sets; each containing about 2600 sen-
tences. About 80 patterns are then produced;
some of them are presented and discussed in sec-
tion 5.1. These patterns can now be applied to
the corpus in order to retrieve valid N-V pairs.
5 Performance evaluation
5.1 Inferred patterns
The inferred patterns give some interesting in-
formation about the way the target semantic re-
lationships are expressed in our corpus. While
some structures are general, others seem very
specific to our corpus.
First of all, proximity is an important factor
in valid relationships between nouns and verbs;
it can be observed in numerous patterns. For
example, the inferred Horn clause:
realization(N,V) :- common_noun(N), contigu-
ous(N,V), N 6=V. transcribes the fact that a noun
and a verb may be a valid N-V pair if N is
a common noun (common_noun(N)) and V is
contiguous to N (contiguous(N,V). (Determiners
are not taken into account.) This pattern
covers the case in which N precedes V, such as
 les utilisateurs lancent tour-?-tour leurs programmes
(users launch in turn their programs) or
in which V precedes N, such as  il est donc
n?cessaire d'ex?cuter la commande depmod  (thus,
it is necessary to run the depmod command .
A quite similar clue is given in the following
pattern:
realization(N,V) :- near_verb(N,V), suc(V,C),
suc(C,N), noun(N), V 6=C, N 6=C, N 6=V. The
near_verb(N,V) predicate means that no verb
occurs between N and V, and suc(X,Y) that
the word X is followed by Y. This clause can
be expressed in a more classical way as V +
(anything but a verb) + N and retrieves pairs like
C'est un service qui vous permet de vous connecter
? Internet (This is a service that allows you to
connect to the Internet).
Another frequent clue in the patterns pro-
duced is, unsurprisingly, that N must be the
head of a noun phrase. For example, realiza-
tion(N,V) :- near_word(N,V), near_verb(N,V), pre-
cedes(V,N), noun_ph_head(N), pred(N,C), preposi-
tion(C), V 6=C, N 6=C, N 6=V. This pattern means
V + (anything but a verb)? + (preposition) + N
head of a noun phrase and retrieves pairs such as:
 les dispositifs d'impression n'?taient pas contr?l?s par
ordinateur (the printing devices were not con-
trolled by computer ).
Prepositions also play an important part
and appear frequently in the patterns: re-
alization(N,V) :- near_verb(N,V), pred(N,C),
lemma(C,sur), V 6=C, N 6=C, N 6=V. (that is V +
(anything but a verb)* + sur + N) covers for
example un terminal (...) permet de travailler sur
l'ordinateur (a terminal (...) allows [one/the
user] to work on the computer ).
The pattern realization realization(N,V)
:- near_verb(N,V), precedes(V,N), pred(N,C),
lemma(C,?), V 6=C, N 6=C, N 6=V., that is V +
(anything but a verb)* + ? + N, covers comment
vous connecter ? Internet (How to connect to the
Internet).
realization(N,V) :- near_verb(N,V), precedes(N,V),
pred(V,C), lemma(C,?), common_noun(N), V 6=C,
N 6=C, N 6=V., that is N + (anything but a verb)* +
? + V , covers (...) mode de traitement des don-
n?es suivant lequel les programmes ? ex?cuter (...)
(...mode of data processing in which the pro-
grams to execute...).
A certain number of patterns express struc-
tures more specific to our corpus. For example,
the clause:
realization(N,V) :- near_verb(N,V), precedes(V,N),
suc(N,C), proper_noun(C), common_noun(N), V 6=C,
N 6=C, N 6=V. (that is, V + (anything but a verb)* +
common noun N + (proper noun)) is very specific to
structures including a proper noun, such as the
sentence: (...) Internet utilise le protocole TCP/IP
(...) (the Internet uses the TCP/IP protocol).
5.2 Methodology for evaluation
In order to evaluate the quality of the extracted
N-V pairs, we are interested in two different
measures. The first one expresses the complete-
ness of the set of retrieved N-V pairs, that is,
how many valid pairs are found with respect
to the total number of pairs which should have
been found; this is the recall rate. The second
measure indicates the reliability of the set of re-
trieved N-V pairs, that is, how many valid pairs
are found with respect to the total number of re-
trieved pairs; this is the precision rate (defined
below). These two rates were evaluated using a
test sample containing all this information.
CompuTerm 2004  -  3rd International Workshop on Computational Terminology42
To construct this test set, we have focused
our attention on ten domain-specific terms: com-
mande (command), configuration, fichier (file), In-
ternet, logiciel (software), option, ordinateur (com-
puter), serveur (server), syst?me (system), utilisa-
teur (user). The terms have been identified as
the most specific to our corpus by a program
developed by Drouin (2003) and called Termo-
Stat. The ten most specific nouns have been
produced by comparing our corpus of comput-
ing to the French corpus Le Monde, composed
of newspaper articles (Lemay et al, 2004). Note
that to prevent any bias in the results, none of
these terms were used as positive examples dur-
ing the pattern inference step. (They were re-
moved from the example set.)
For each of these 10 nouns, a manual identifi-
cation of valid and invalid pairs was carried out.
Linguists were asked to analyze the sentences
and decide whether the highlighted pairs were
valid N-V pairs. Examples of the sentences pro-
duced are given in Table 2 for the term utilisateur
(user). A pair is considered as valid if at least
one of its occurrences has the desired semantic
(and syntactic) relationship (cf. section 2).
Finally, 603 of the N-V pairs examined are
valid and 4446 are considered not to be valid.
The results for each noun are detailed in Table 3.
valid non-valid total
commande 114 346 460
configuration 4 361 365
fichier 21 604 625
option 69 324 393
syst?me 82 605 687
Internet 9 535 544
ordinateur 85 432 517
utilisateur 96 435 531
logiciel 64 440 504
serveur 59 364 423
Total 603 4446 5049
Table 3: Summary of the test set
5.3 Results
To compare the results obtained by our tech-
nique with the analysis carried out manually,
we use the traditional precision/recall approach.
Thus we applied the patterns to the corpus and
kept all the pairs retrieved when N is one of the
ten specific nouns. The results of the compari-
son are summarized with the help of confusion
matrices like the one presented in Table 4
3
.
actual actual
valid non-valid Total
predicated
valid TP FP PrP
predicated
non-valid FN TN PrN
Total AP AN S
Table 4: Confusion matrix
It is important to note here that the values in
this confusion matrix depend on a parameter: a
detection threshold. Indeed, a single occurrence
of a N-V pair with the patterns is not sufficient
for a pair to be considered as valid. The thresh-
old, called s, represents the minimal number of
occurrences to be detected to consider a pair as
valid. The recall and precision rates (respec-
tively R and P ), measured on our test set, are
thus defined according to s.
In order to represent every possible value of R
and P according to s, we draw a recall-precision
graph in which each value of P is related to
its corresponding value of R. Figure 1 gives
the graph obtained when applying the inferred
patterns to the test set. For comparison pur-
poses, Figure 1 also indicates the recall-precision
graphs obtained by two common statistical tech-
niques for collocation acquisition (the Loglike
et Mutual Information coefficients presented in
section 4.3). As a baseline, this graph also gives
the density, computed as AP/S, which repre-
sents the precision that would be obtained by
a system deciding randomly if a pair is valid or
not.
The recall-precision graph shows that our
symbolic technique outperforms the two statis-
tical ones; for a fixed recall, the precision gain
is up to 45% with respect to the Loglike results
and is even higher with respect the MI coeffi-
cient. Thus, our acquisition technique meets the
objective of offering assistance to the terminog-
rapher, since it provides many reliable N-V pair
candidates. However, results show that invalid
pairs are also retrieved; thus a manual analysis
remains unavoidable.
5.4 Discussion of the results
When examining the retrieved pairs, it appears
that most invalid N-V pairs can be classified in
3
The meaning of the variables is given by the combi-
nation of the letters: A means actual, Pr predicated, T
true, F false, P positive and N negative.
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 43
Examples Comment
utilisateur___ex?cuter 162.823 13.907792621292
* MemCheckBoxInRunDlg Autorise les utilisateurs#N# ? ex?cuter#V# un
programme 16 bits dans un processus VDM ( Virtual DOS Machine ) d?di?
( non partag? ) .
Valid syntactic and seman-
tic relationship: "user runs
a program"
utilisateur___formater 162.823 0.27168791461736
* AllocateDasd D?termine quels utilisateurs#N# peuvent formater#V# et
?jecter les disques_durs amovibles .
Valid syntactic and seman-
tic relationship: "user for-
mats a hard disk"
utilisateur___lancer 162.823 17.9010990569368
* Il utilise donc le num?ro de l_ utilisateur#N# r?el qui a lanc?#V#
la commande pour savoir si c_ est bien l_ utilisateur#N# root qui l_
a lanc?#V# .
Valid syntactic and se-
mantic relationship: "user
launches a command"
Counter-examples Comment
utilisateur___accepter 162.823 0.0059043737128377
* Ces commandes sont absolument essentielles pour pouvoir
utiliser le syst?me , mais elles sont assez r?barbatives et peu_d_
utilisateurs#N# acceptent#V# de s_ en contenter .
Syntactic relationship is
valid; Semantic relation-
ship is not valid
utilisateur___entourer 162.823 0.41335402801632
* Mais , c?t? utilisateur#N# , plus on a entour?#V# ou rempli
son Macintosh de p?riph?riques , plus grands sont les risques de
rencontrer des blocages ou des sautes_d_humeur .
Syntactic relationship is
not valid
Table 2: Positive and negative examples with utilisateur (Engl. user)
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
Pr
ec
is
io
n
Recall
ASARES
Density
Mutual Information
Loglike
Figure 1: Recall-Precision graph
one of the four following categories.
First, some errors are due to tagging mistakes.
For example, in the sentence  la premi?re solution
est d'utiliser la commande date  (the first solution
is to use the date command ) the noun date
was incorrectly tagged as a verb, and the N-V
pair commande-dater was retrieved by one of the
inferred patterns. Even if this kind of error does
not come directly from our acquisition technique
and does not call into question our approach, it
is still a factor that should be taken into ac-
count, especially when considering the choice of
CompuTerm 2004  -  3rd International Workshop on Computational Terminology44
the tagger and the quality of the texts compos-
ing our corpus.
Secondly, in a few cases, there is no syn-
tactic link between N and V, as is the
case with logiciel-garantir (software-guarantee) in
cette phase garantit la v?rification du logiciel (this
step guarantees the software verification). Even
if these errors are rare in the retrieved pairs ex-
amined, our symbolic extraction system could
certainly be enhanced with information about
the syntactic function of nouns (subject, direct
object...). The learning algorithm could then
incorporate this information and produce more
relevant patterns.
Thirdly, some N-V pairs are retrieved, al-
though there is no semantic link between N and
V, or at least, not a semantic link that would
be encoded by a terminographer in a dictionary.
This is the case for ordinateur-savoir (computer-
know) in  l'ordinateur ne sait pas o? chercher (the
computer does not know where to look ). Again,
morpho-syntactic information is not always suf-
ficient to distinguish between semantically re-
lated pairs and non-semantically (but syntacti-
cally) related ones.
Finally, other very frequent errors are caused
by the fact that there actually is an interesting
semantic link between N and V in a retrieved
pair, but not the realization link we are looking
for. Indeed, some nouns belonging to specific
semantic classes will often cooccur with verbs
expressing realization meanings (and thus often
appear in valid N-V pairs) while others do not.
For example, nouns like ordinateur (computer)
and utilisateur (user) often appear in valid N-V
pairs. On the other hand, other nouns clearly
do not appear in combinations with realization
verbs (e.g., configuration; Internet, refer to Table
3).
These last two kinds of error clearly illustrate
the limitations of our symbolic approach (but
are also very frequent errors in statistical ap-
proaches since cooccurrence is not enough to
capture subtle semantic distinctions). In fact,
they tend to show that our method could be
enhanced if it could incorporate richer linguis-
tic information. Indeed, morpho-syntactic infor-
mation is not always sufficient to operate fine-
grained sense distinctions. For example, the
two sentences below have the same combina-
tion of Part-of-Speech tags: vous pouvez utiliser la
commande exit (you can use the exit command)
and vous devez choisir l'option 64MB (you must
choose the 64MB option); in the first one the
underlined N-V pair is valid whereas this is not
the case in the second one. Realization verbs
for option would be valider (validate) and activer
(activate), for example. Here again, these subtle
distinctions could be handled by our symbolic
method provided that some semantic informa-
tion is given on nouns This could be supplied
by a semantic tagger.
6 Concluding remarks and future
work
We have presented an original acquisition pro-
cess for noun-verb pairs in which verbs convey a
realization meaning. These noun-verb pairs are
acquired from a domain-specific corpus of com-
puting. Our acquisition method, which relies
on asares, a extraction pattern inference tech-
nique, produces results that are quite good and
could improve manual terminographical work.
In particular, our method outperforms classical
statistical techniques often used for collocation
acquisition. Moreover, the inferred patterns give
interesting clues about the structures that are
likely to convey the target semantic link.
Many possibilities for future work are sug-
gested by this experiment. Concerning our ac-
quisition process, some adaptations could cer-
tainly improve the results, currently limited
by the sole use of Part-of-Speech tags and
noun-phrase information. As was previously
mentioned, syntactic and semantic information
could be added to the corpus through a tag-
ging and parsing process. These two enrich-
ments could help to overcome some limitations
of our symbolic approach to capture the nature
of N-V relationships. In terms of applications, it
would be interesting to use a similar technique
for the acquisition of other more specific seman-
tic links between nouns and verbs and even be-
tween nouns and nouns or other categories of
words. These semantic relationships would al-
low us to complete the description of the ter-
minological units contained in our dictionary of
computing. The comparison of the acquisition
results and of the inferred patterns could lead
to interesting insights.
Aknowlegements
The authors would like to thank Sahara Iveth
Carre?o Cruz and L?onie Demers-Dion for their
help in analyzing the data and Elizabeth Marsh-
man for her comments on a previous version of
this paper.
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 45
References
Pierrette Bouillon, Vincent Claveau, C?cile
Fabre, and Pascale S?billot. 2001. Using
Part-of-Speech and Semantic Tagging for the
Corpus-Based Learning of Qualia Structure
Elements. In First International Workshop
on Generative Approaches to the Lexicon,
GL'2001, Geneva, Switzerland.
Pierrette Bouillon, Vincent Claveau, C?cile
Fabre, and Pascale S?billot. 2002. Acqui-
sition of Qualia Elements from Corpora 
Evaluation of a Symbolic Learning Method.
In 3rd International Conference on Language
Resources and Evaluation, LREC 02, Las Pal-
mas de Gran Canaria, Spain.
Kenneth W. Church and Patrick Hanks. 1990.
Word Association Norms, Mutual Informa-
tion, and Lexicography. Computational Lin-
guistics, 16(1):2229.
Vincent Claveau, Pascale S?billot, C?cile Fabre,
and Pierrette Bouillon. 2003. Learning Se-
mantic Lexicons from a Part-of-Speech and
Semantically Tagged Corpus using Induc-
tive Logic Programming. Journal of Ma-
chine Learning Research, special issue on ILP,
4:493525.
B?atrice Daille. 2003. Conceptual structuring
through term variation. InWorkshop on Mul-
tiword Expressions. Analysis, Acquisition and
Treatment. Proceedings of the ACL'03, Sap-
poro, Japan.
Patrick Drouin. 2003. Term-extraction using
non-technical corpora as a point of leverage.
Terminology, 9(1):99115.
Ted E. Dunning. 1993. Accurate methods for
the statistics of surprise and coincidence.
Computational Linguistics, 19(1):6174.
Natalia Grabar and Pierre Zweigenbaum.
2002. Lexically-based terminology structur-
ing. some inherent limits. In Second Work-
shop on Computational Terminology, Com-
puterm 2002. Coling 2002, Taipei, Taiwan.
Beno?t Habert, Ellie Naulleau, and Adeline
Nazarenko. 1996. Symbolic word clustering
for medium-sized corpora. In Proceedings of
the 16th Conference on Computational Lin-
guistics, Coling'96, Copenhagen, Denmark.
Zellig Harris. 1971. Structures math?matiques
du langage. Paris: Dunod.
Adam Kilgarriff and David Tugwell. 2001.
Word-Sketch: Extraction and Display of
Significant Collocations for Lexicography. In
Workshop on Collocation: Computational
Extraction, Analysis and Exploitation, 39th
ACL and 10th EACL Conference, Toulouse,
France.
Chantal Lemay, Marie-Claude L'Homme, and
Patrick Drouin. 2004. Two methods for ex-
tracting "specific" single-word terms from
specialized corpora. Forthcoming.
Marie-Claude L'Homme. 2004. S?lection de ter-
mes dans un dictionnaire d'informatique :
Comparaison de corpus et crit?res lexico-
s?mantiques. In Euralex 2004. Proceedings,
Lorient, France. Forthcoming.
Christopher D. Manning and Hinrich Sch?tze.
1999. Foundations of Statistical Natural Lan-
guage Processing. The MIT Press, Cam-
bridge, MA, USA.
Igor Mel'?uk, Nadia Arbatchewsky-Jumarie,
L?o Elnitsky, Lidija Iordanskaja, Ad?le Les-
sard, Louise Dagenais, Marie-No?lle Lefeb-
vre, Suzanne Mantha, and Alain Polgu?re.
19841999. Dictionnaire explicatif et combi-
natoire du fran?ais contemporain, Recherches
lexico-s?mantiques, volumes I-IV. Les Presses
de l'Universit? de Montr?al, Montr?al, QC,
Canada.
Stephen Muggleton and Luc De-Raedt. 1994.
Inductive Logic Programming: Theory and
Methods. Journal of Logic Programming, 19-
20:629679.
Rochdi Oueslati. 1999. Aide ? l'acquisition
de connaissances ? partir de corpus. Ph.D.
thesis, Univesit? Louis Pasteur, Strasbourg,
France.
Darren Pearce. 2002. A Comparative Evalu-
ation of Collocation Extraction Techniques.
In 3rd International Conference on Language
Resources and Evaluation, LREC 02, Las Pal-
mas de Gran Canaria, Spain.
James Pustejovsky. 1995. The Generative Lexi-
con. The MIT Press, Cambridge, MA, USA.
Frank Smadja. 1993. Retrieving Collocations
from Text: Xtract. Computational Linguis-
tics, 19(1):143178.
Ellen M. Voorhees. 1994. Query Expansion Us-
ing Lexical-Semantic Relations. In Proceed-
ings of ACM SIGIR'94, Dublin, Ireland.
CompuTerm 2004  -  3rd International Workshop on Computational Terminology46
