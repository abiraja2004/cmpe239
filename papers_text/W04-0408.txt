Multiword expressions as dependency subgraphs
Ralph Debusmann
Programming Systems Lab
Saarland University
Postfach 15 11 50
66041 Saarbru?cken, Germany
rade@ps.uni-sb.de
Abstract
We propose to model multiword expres-
sions as dependency subgraphs, and re-
alize this idea in the grammar formal-
ism of Extensible Dependency Gram-
mar (XDG). We extend XDG to lexi-
calize dependency subgraphs, and show
how to compile them into simple lexical
entries, amenable to parsing and gener-
ation with the existing XDG constraint
solver.
1 Introduction
In recent years, dependency grammar (DG)
(Tesnie`re, 1959; Sgall et al, 1986; Mel?c?uk, 1988)
has received resurgent interest. Core concepts
such as grammatical functions, valency and the
head-dependent asymmetry have now found their
way into most grammar formalisms, including
phrase structure-based ones such as HPSG, LFG
and TAG. This renewed interest in DG has also
given rise to new grammar formalisms based di-
rectly on DG (Nasr, 1995; Heinecke et al, 1998;
Bro?ker, 1999; Gerdes and Kahane, 2001; Kruijff,
2001; Joshi and Rambow, 2003).
A controversy among DG grammarians cir-
cles around the question of assuming a 1:1-
correspondence between words and nodes in the
dependency graph. This assumption simplifies
the formalization of DGs substantially, and is of-
ten required for parsing. But as soon as se-
mantics comes in, the picture changes. Clearly,
the 1:1-correspondence between words and nodes
does not hold anymore for multiword expressions
(MWEs), where one semantic unit, represented
by a node in a semantically oriented dependency
graph, corresponds not to one, but to more than
one word.
Most DGs interested in semantics have thus
weakened the 1:1-assumption, starting with
Tesnie`re?s work. Tesnie`re proposed the con-
cept of nuclei to group together sets of nodes.
FGD, on the other hand, allows for the dele-
tion of solely syntactically motivated nodes in the
tectogrammatical structure. Similarly, in MTT,
nodes present on the syntactic structures can be
deleted on the semantic structure. This can happen
e.g. through paraphrasing rules implemented by
lexical functions (Mel?c?uk, 1996). Unfortunately,
these attempts to break the 1:1-correspondence
have not yet been formalized in a declarative way.
Extensible Dependency Grammar (XDG) is a
new grammar formalism based on Topological
Dependency Grammar (TDG) (Duchier and De-
busmann, 2001). From TDG, it inherits declara-
tive word order constraints, the ability to distin-
guish multiple dimensions of linguistic descrip-
tion, and an axiomatization as a constraint sat-
isfaction problem (Duchier, 2003) solvable using
constraint programming (Apt, 2003). One of the
benefits of this axiomatization is that the linear or-
der of the words can be left underspecified, with
the effect that the constraint solver can be applied
for both parsing and generation.
XDG solving is efficient at least for the smaller-
scale example grammars tested so far, but these
good results hinge substantially on the assump-
Second ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp. 56-63
tion of a 1:1-correspondence between words and
nodes. As XDG has been created to cover not only
syntax but also semantics, we have no choice but
to weaken the 1:1-correspondence.
In this paper, we outline a way to break out of
the 1:1-straightjacket, without sacrificing the po-
tential for efficient parsing and generation. We in-
troduce a new layer of lexical organization called
groups above the basic XDG lexicon, allowing us
to describe MWEs as tuples of dependency sub-
graphs. Groups can be compiled into simple lexi-
cal entries, which can then be used by the already
existing XDG solver for parsing and generation.
With groups, we can omit nodes present in the syn-
tactic dimensions in the semantic dimensions, and
thus get away from the 1:1-correspondence.
Groups are motivated by the continuity hypothe-
sis of (Kay and Fillmore, 1999), assuming the con-
tinuity of the lexicon and the construction. They
can also be regarded as a declarative formalization
of Mel?c?uk?s paraphrasing rules, or as a realization
of the extended domain of locality of TAG (Joshi,
1987) in terms of dependency grammar, as also
proposed in (Nasr, 1996) and (Joshi and Rambow,
2003).
The structure of this paper is as follows. We in-
troduce XDG in section 2. In section 3, we intro-
duce groups, the new layer of lexical organization
required for the modeling of MWEs, and in sec-
tion 4, we show how to compile groups into sim-
ple lexical entries amenable for parsing and gener-
ation. Section 5 concludes the paper and outlines
future work.
2 XDG
In this section, we explain the intuitions behind
XDG, before proceeding with its formalization,
and a description of the XDG solver for parsing
and generation.
2.1 XDG intuitions
Extensible Dependency Grammar (XDG) is a new
grammar formalism generalizing Topological De-
pendency Grammar (TDG) (Duchier and Debus-
mann, 2001). XDG characterizes linguistic struc-
ture along arbitrary many dimensions of descrip-
tion. All dimensions correspond to labeled graphs,
sharing the same set of nodes but having different
edges.
The well-formedness conditions for XDG anal-
yses are determined by principles. Principles can
either be one-dimensional, applying to a single
dimension only, or multi-dimensional, constrain-
ing the relation of several dimensions. Basic one-
dimensional principles are treeness and valency.
Multi-dimensional principles include climbing (as
in (Duchier and Debusmann, 2001); one dimen-
sion must be a flattening of another) and linking
(e.g. to specify how semantic arguments must be
realized syntactically).
The lexicon plays a central role in XDG. For
each node, it provides a set of possible lexical en-
tries (feature structures) serving as the parameters
for the principles. Because a lexical entry con-
strains all dimensions simultaneously, it can also
help to synchronize the various dimensions, e.g.
with respect to valency. For instance, a lexical en-
try could synchronize syntactic and semantic di-
mensions by requiring a subject in the syntax, and
an agent in the semantics.
As an example, we show in (1) an analysis
for the sentence He dates her, along two dimen-
sions of description, immediate dominance (ID)
and predicate-argument structure (PA). We display
the ID part of the analysis on the left, and the PA
part on the right:1
.
He dates her
objsubj
.
He dates her
arg1
arg2
(1)
The set of edge labels on the ID dimension in-
cludes subj for subject and obj for object. On
the PA dimension, we have arg1 and arg2 stand-
ing for the argument slots of semantic predicates.2
The ID part of the analysis states that He is the sub-
ject, and her the object of dates. The PA part states
that He is the first argument and her the second ar-
gument of dates.
1For lack of space, we omit the dimension of linear prece-
dence (LP) from the presentation in this paper. For this di-
mension, we use the same theory as for TDG (Duchier and
Debusmann, 2001).
2We could also use some set of thematic roles for the PA
edge labels, but since the assumption of thematic roles is very
controversial, we decided to choose more neutral labels.
The principles of the underlying grammar re-
quire that the ID part of each analysis is a tree,
and the PA part a directed acyclic graph (dag).34
In addition, we employ the valency principle on
both dimensions, specifying the licensed incom-
ing and outgoing edges of each node. The only
employed multi-dimensional principle is the link-
ing principle, specifying how semantic arguments
are realized syntactically.
Figure 1 shows the lexicon of the underlying
grammar. Each lexical entry corresponds to both
a word and a semantic literal. inID and outID
parametrize the valency principle on the ID di-
mension. inID specifies the licensed incoming,
and outID the licensed outgoing edges. E.g.
He licenses zero or one incoming edges labeled
subj, and no outgoing edges. inPA and outPA
parametrize the valency principle on the PA dimen-
sion. E.g. dates licenses no incoming edges, and
requires precisely one outgoing edge labeled arg1
and one labeled arg2. link parametrizes the multi-
dimensional linking principle. E.g. dates syntac-
tically realizes its first argument by a subject, the
second argument by an object.
Observe that all the principles are satisfied in
(1), and hence the analysis is well-formed. Also
notice that we can use the same grammar and lex-
icon for both parsing (from words) and generation
(from semantic literals).
2.2 XDG formalization
Formally, an XDG grammar is built up of dimen-
sions, a lexicon and principle, and characterizes a
set of well-formed analyses.
A dimension is a tuple D = (Lab, Fea, Val, Pri)
of a set Lab of edge labels, a set Fea of fea-
tures, a set Val of feature values, and a set of one-
dimensional principles Pri. A lexicon for the di-
mension D is a set Lex ? Fea ? Val of total fea-
ture assignments called lexical entries. An analy-
sis on dimension D is a triple (V,E, F ) of a set V
of nodes, a set E ? V ? V ? Lab of directed la-
beled edges, and an assignment F : V ? (Fea ?
Val) of lexical entries to nodes. V and E form a
3In the following, we will call the ID part of an analysis
ID tree, and the PA part PA dag.
4The PA structure is a dag and not a tree because we
want it to reflect the re-entrancy e.g. in control constructions,
where the same subject is shared by more than one node.
graph. We write AnaD for the set of all possible
analyses on dimension D. The principles charac-
terize subsets of AnaD. We assume that the ele-
ments of Pri are finite representations of such sub-
sets.
An XDG grammar ((Labi, Feai, Vali, Prii)ni=1,
Pri, Lex) consists of n dimensions, multi-
dimensional principles Pri, and a lexicon Lex.
An XDG analysis (V,Ei, Fi)ni=1 is an element of
Ana = Ana1 ? ? ? ? ? Anan where all dimensions
share the same set of nodes V . We call a dimen-
sion of a grammar grammar dimension.
Multi-dimensional principles specify subsets of
Ana, i.e. of tuples of analyses for the individual di-
mensions. The lexicon Lex ? Lex1 ? ? ? ? ? Lexn
constrains all dimensions at once, thereby syn-
chronizing them. An XDG analysis is licensed by
Lex iff (F1(v), . . . , Fn(v)) ? Lex for every node
v ? V .
In order to compute analyses for a given input,
we employ a set of input constraints (Inp), which
again specify a subset of Ana. XDG solving then
amounts to finding elements of Ana that are li-
censed by Lex, and consistent with Inp and Pri.
The input constraints e.g. determine whether XDG
solving is to be used for parsing or generation. For
parsing, they specify a sequence of words, and for
generation, a multiset of semantic literals.
2.3 XDG solver
XDG solving has a natural reading as a constraint
satisfaction problem (CSP) on finite sets of inte-
gers, where well-formed analyses correspond to
the solutions of the CSP (Duchier, 2003). We
have implemented an XDG solver (Debusmann,
2003) using the Mozart-Oz programming system
(Mozart Consortium, 2004).
XDG solving operates on all dimensions con-
currently. This means that the solver can infer in-
formation about one dimension from information
on another, if there is either a multi-dimensional
principle linking the two dimensions, or by the
synchronization induced by the lexical entries. For
instance, not only can syntactic information trig-
ger inferences in syntax, but also vice versa.
Because XDG allows us to write grammars
with completely free word order, XDG solving is
an NP-complete problem (Koller and Striegnitz,
word literal inID outID inPA outPA link
He he? {subj?} {} {arg1?, arg2?} {} {}
dates date? {} {subj!, obj!} {} {arg1!, arg2!} {arg1 7? {subj}, arg2 7? {obj}}
her she? {obj?} {} {arg1?, arg2?} {} {}
Figure 1: Lexicon for the sentence He dates her.
2002). This means that the worst-case complex-
ity of the solver is exponential. The average-case
complexity of many smaller-scale grammars that
we have experimented with seems polynomial, but
it remains to be seen whether we can scale this up
to large-scale grammars.
3 Groups
In this section, we consider MWE paraphrases and
propose to model them as tuples of dependency
subgraphs called groups. We start with an exam-
ple: Consider (3), a paraphrase of (2):
He dates her. (2)
He takes her out. (3)
We display the XDG analysis of (3) in (4).
Again, the ID tree is on the left, and the PA dag
on the right:5
.
He takes her out
obj partsubj
.
He takes her out
arg1
arg2
(4)
This example demonstrates that we cannot sim-
ply treat MWEs as contiguous word strings and
include those in the lexicon, since the MWE takes
out is interrupted by the object her in (3). Instead,
we choose to implement the continuity hypothe-
sis of (Kay and Fillmore, 1999) in terms of DG,
and model MWEs as dependency subgraphs. We
realize this idea by a new layer of lexical orga-
nization called groups. A group is tuple of de-
pendency subgraphs covering one or more nodes,
where each component of the tuple corresponds to
a grammar dimension. We call a group compo-
nent group dimension. We display the group cor-
responding to dates in (5), and the one correspond-
5In the ID tree, part stands for a particle.
ing to takes out in (6).
.
dates
objsubj
.
dates
arg
1 arg2
(5)
.
takes out
obj partsubj
.
takes out
arg
1 arg2
(6)
Groups can leave out nodes present in the ID
dimension on the PA dimension. E.g. in (6), the
node corresponding to the particle out is present
on the ID dimension, but left out on the PA dimen-
sion. In this way, groups help us to weaken the
1:1-correspondence between words and nodes.
We can make use of groups also to handle more
complicated MWE paraphrases. Consider (8) be-
low, a support verb construction paraphrasing (7):
He argues with her. (7)
He has an argument with her. (8)
In (8), has is only a support verb; the semantic
head of the construction is the noun argument. We
display the ID tree and PA dag of (7) in (9). The ID
tree of (8) is in (10), and the PA dag in (11):6
.
He argues with her
pobjsubj
pcomp
.
He argues with her
arg1 arg2
(9)
6In the ID trees, pobj stands for a prepositional object,
pcomp for the complement of a preposition, pmod for a
prepositional modifier, and det for a determiner.
.He has an argument with her
objsubj
det
pmod
pcomp
(10).
He has an argument with her
arg1 arg2
(11)
In (9), the node corresponding to with is deleted
in the PA dag. In (11), the support verb construc-
tion leads to the deletion of three nodes (corre-
sponding to resp. has, an and with). These dele-
tions are reflected in the corresponding groups.
The group corresponding to argues with is dis-
played in (12), and the group corresponding to has
an argument with in (13) (ID) and (14) (PA):
.
argues with
pobjsubj
pcomp
.
argues with
arg1 arg2 (12)
.
has an argument with
objsubj
det
pmod
pcomp
(13).
has an argument with
arg1 arg2
(14)
Groups can capture difficult constructions such
as the support verb construction above in an el-
egant and transparent way, without having to re-
sort to complicated machinery. The key aspect
of groups is their multi-dimensionality, describing
tuples of dependency subgraphs spanning over a
shared set of nodes. This sharing enables groups
to express interdependencies between the differ-
ent dimensions. For instance in (13) and (14), the
noun argument, the object of the support verb has
in the ID dimension, is the semantic head in the PA
dimension.
4 Compilation
In this section, we show how to compile groups
into simple lexical entries. The benefit of this is
that we can retain XDG in its entirety, i.e. we can
retain its formalization and its axiomatization as
a constraint satisfaction problem. This means we
can also retain the implementation of the XDG
solver, and continue to use it for parsing and gen-
eration.
4.1 Node deletion
The 1:1-correspondence between words and nodes
is a key assumption of XDG. It requires that on
each dimension, each word must correspond to
precisely one node in the dependency graph. The
groups shown in the previous section clearly vio-
late this assumption as they allow nodes present on
the ID dimension to be omitted on the PA dimen-
sion.
The first step of the compilation aims to accom-
modate this deletion of nodes. To this end, we
assume for each analysis an additional root node,
e.g. corresponding to the full stop at the end of the
sentence. Each root in an XDG dependency graph
becomes a daughter of this additional root node,
the edge labeled root. The trick for deleting nodes
is now the following: Each deleted node also be-
comes a daughter of the additional root node, but
with a special node label indicating that this node
is regarded to be deleted (del).
As an example, here is the PA dag for example
(3) including the additional root node:
.
He takes her out .
arg1
arg2
delroot
(15)
4.2 Dependency subgraphs
The second step of the compilation is to compile
the dependency subgraphs into lexical entries for
individual words. To this end, we make use of the
valency principle. For each edge from v to v ? la-
beled l within a group, we require that v has an
outgoing edge labeled l, and that v? licenses an in-
coming edge labeled l. I.e. we include l! in the out
specification of the lexical entry for v, and l? in
the in specification of the lexical entry for v ?.
4.3 Group coherence
The final step of the compilation is about ensur-
ing group coherence, i.e. to ensure that the inner
nodes of a group dimension (neither the root nor
the leaves) stay within this group in the depen-
dency analysis. In other words, group coherence
make sure that the inner nodes of a group dimen-
sion cannot become daughters of nodes of a dif-
ferent group. In our support verb example, group
coherence ensures that e.g. that the determiner an
cannot become the determiner of a noun of an-
other group. We realize this idea through a new
XDG principle called group coherence principle.
This principle must hold on all dimensions of the
grammar.
Given a set of group identifiers Group , the prin-
ciple assumes two new features: group : Group ,
and outgroups : Lab ? 2Group . For each node v,
group(v) denotes the group identifier of the group
of v. For each edge within a group from v to
v? labeled l, i.e. for each edge to an inner node,
outgroups(v)(l) denotes the singleton set contain-
ing the group of both v and v?. For each edge
from v labeled l which goes outside of a group,
outgroups(v)(l) = Group , i.e. the edge can end
in any other group. We formalize the group coher-
ence principle as follows, where v l? v? denotes
an edge from v to v? labeled l:
v l? v? ? group(v?) ? outgroups(v)(l)
(16)
4.4 Examples
For illustration, we display the lexical entries of
two compiled groups: The group argues with
(with identifier g1), and the group has an argu-
ment with (with identifier g2), resp. in Figure 2 and
Figure 3. We omit the specification of outgroups
for the PA dimension for lack of space, and since it
is not relevant for the example: In all groups, there
are no edges which stay within a group in the PA
dimension.
4.5 Parsing and generation
We can use the same group lexicon for parsing and
generation, but we have to slightly adapt the com-
pilation for the generation case.
For parsing, we can use the XDG parser as be-
fore, without any changes.
For generation, we assume a set Sem of seman-
tic literals, multisets of which must be verbalized.
To do this, we assume a function groups : Sem ?
2Group , mapping each semantic literal to a set of
groups which verbalize it.
Now before using the XDG solver for genera-
tion, we have to make sure for each semantic lit-
eral s to be verbalized that the groups which can
potentially verbalize it have the same number of
nodes. For this, we calculate the maximum num-
ber n of syntactic nodes for the groups assigned
to s, and fill up the groups having less syntactic
nodes with deleted nodes. Then for XDG solving,
we introduce precisely n nodes for literal s. Using
groups for generation thus comes at the expense of
having to introduce additional nodes.7
As an example, consider we want to verbal-
ize the semantic literal argue. groups(argue) =
{g1, g2}, i.e. either groups g1 or g2 can verbalize
it. The maximum number n of syntactic nodes for
the two groups is 4 for g2 (has an argument with).
Hence, we have to fill up the groups having less
syntactic nodes with deleted nodes. In this case,
we have to add two deleted nodes to the group g1
(argue with) to get to four nodes. The resulting
lexical entries encoding g1 are displayed in Fig-
ure 4. The lexical entries for g2 stay the same as
in Figure 3.
After this step is done, we can use the existing
XDG solver to generate from the semantic literals
precisely the two possible verbalizations (7) and
(8), as intended.
7We should emphasize that n is not at all unrestricted.
For each semantic literal s to be verbalized, we have to intro-
duce only as many nodes as are contained in the largest group
which can verbalize s.
word literal group outgroupsID inID outID inPA outPA link
argues argue? g1 {pobj 7? {g1}} {root?} {subj!, pobj!} {root?} {arg1, arg2} { arg1 7? {subj}arg2 7? {pcomp}}
with argue? g1 {} {pobj?} {pcomp!} {del?} {} {}
Figure 2: Lexical entries encoding the group for argues with
word literal group outgroupsID inID outID inPA outPA link
has argue? g2 {obj 7? {g2}} {root?} {subj!, obj!} {del?} {} {}
an argue? g2 {} {det?} {} {del?} {} {}
argument argue? g2 { det 7? {g2}pmod 7? {g2}} {obj?} {det!, pmod!} {root?} {arg1!, arg2!} {
arg1 7? {subj}
arg2 7? {pcomp}}
with argue? g2 {} {pmod?} {pcomp!} {del?} {} {}
Figure 3: Lexical entries encoding the group for has an argument with
5 Conclusion
We extended the XDG grammar formalism with
a means to break out of the straightjacket of the
1:1-correspondence between words and nodes. To
this end, we proposed the new notion of groups,
allowing to enrich the XDG lexicon with tuples
of dependency subgraphs. We illustrated how to
tackle complicated MWEs such as support verb
constructions with this new idea, and how to com-
pile groups into simple lexical entries.
We see two main benefits of our approach. The
first is that we can retain the XDG formalization,
and also its axiomatization as a constraint satisfac-
tion problem, in its entirety. Thus, we can simply
continue to use the existing XDG solver for pars-
ing and generation. The second benefit is that we
can use the same group lexicon for both parsing
and generation, the only difference being that for
generation, we have to slightly adapt the compila-
tion into lexical entries.
There are many issues which have remained un-
touched in this paper. For one, we did not talk
about our treatment of word order in this paper for
lack of space. Word order is among the best re-
searched issues in the context of XDG. For a thor-
ough discussion about word order in XDG, we re-
fer to (Duchier and Debusmann, 2001) and (De-
busmann, 2001).
Another issue is that of the relation between
groups and the meta-grammatical functionality
of the XDG lexicon, offering lexical inheritance,
templates and also disjunction in the sense of
crossings (Candito, 1996) to lexically state lin-
guistic generalizations. How well groups can be
integrated with this meta-grammatical functional-
ity is an open question.
XDG research has so far mainly been focused
on parsing, only to a very small extent on genera-
tion, and to no extent at all on Machine Translation
(MT). There is still a lot to do on both of the lat-
ter topics, and even for parsing, we are only at the
beginning. Although with our smaller-scale exam-
ple grammars, parsing and generation takes poly-
nomial time, we have yet to find out how we can
scale this up to large-scale grammars. We have
started importing and inducing large-scale gram-
mars from existing resources, but can so far only
speculate about if and how we can parse them ef-
ficiently. In a related line of research, we are also
working on the incorporation of statistical infor-
mation (Dienes et al, 2003) to help us to guide the
search for solutions. This could improve perfor-
mance because we would only have to search for
a few good analyses instead of enumerating hun-
dreds of them.
Acknowledgements
The idea for groups and this paper was triggered
by three events in fall 2003. The first was a work-
shop where Peter Dienes, Stefan Thater and me
worked out how to do generation with XDG. The
second was a presentation on the same workshop
by Aravind Joshi and Owen Rambow of their en-
coding of DG in TAG, and the third was a talk
by Charles Fillmore titled Multiword Expressions:
An Extremist Approach. I?d like to thank all of
them. And I?d like to thank all the others involved
with XDG, in alphabetical order: Ondrej Bojar,
Denys Duchier, Alexander Koller, Geert-Jan Krui-
word literal group outgroupsID inID outID inPA outPA link
argues argue? g1 {pobj 7? {g1}} {root?} {subj!, pobj!} {root?} {arg1, arg2} { arg1 7? {subj}arg2 7? {pcomp}}
with argue? g1 {} {pobj?} {pcomp!} {del?} {} {}
 argue? g1 {} {del?} {} {del?} {} {}
 argue? g1 {} {del?} {} {del?} {} {}
Figure 4: Lexical entries for generation, encoding the group for argues with
jff, Vladislav Kubon, Marco Kuhlmann, Joachim
Niehren, Martin Platek and Gert Smolka for their
support and many helpful discussions.
References
Krzysztof R. Apt. 2003. Principles of Constraint Pro-
gramming. Cambridge University Press.
Norbert Bro?ker. 1999. Eine Dependenzgrammatik
zur Kopplung heterogener Wissensquellen. Lin-
guistische Arbeiten 405. Max Niemeyer Verlag,
Tu?bingen/GER.
Marie-H e`le?ne Candito. 1996. A principle-based hier-
archical representation of LTAG. In Proceedings of
COLING 1996, Kopenhagen/DEN.
Ralph Debusmann. 2001. A declarative grammar for-
malism for dependency grammar. Master?s thesis,
University of Saarland.
Ralph Debusmann. 2003. A parser system for extensi-
ble dependency grammar. In Denys Duchier, editor,
Prospects and Advances in the Syntax/Semantics In-
terface, pages 103?106. LORIA, Nancy/FRA.
Peter Dienes, Alexander Koller, and Marco Kuhlmann.
2003. Statistical A* Dependency Parsing. In
Prospects and Advances in the Syntax/Semantics In-
terface, Nancy/FRA.
Denys Duchier and Ralph Debusmann. 2001. Topo-
logical dependency trees: A constraint-based ac-
count of linear precedence. In Proceedings of ACL
2001, Toulouse/FRA.
Denys Duchier. 2003. Configuration of labeled trees
under lexicalized constraints and principles. Re-
search on Language and Computation, 1(3?4):307?
336.
Kim Gerdes and Sylvain Kahane. 2001. Word order
in german: A formal dependency grammar using a
topological hierarchy. In Proceedings of ACL 2001,
Toulouse/FRA.
Johannes Heinecke, Ju?rgen Kunze, Wolfgang Menzel,
and Ingo Schro?der. 1998. Eliminative parsing with
graded constraints. In Proceedings of COLING/ACL
1998, pages 526?530, Montreal/CAN.
Aravind Joshi and Owen Rambow. 2003. A formal-
ism for dependency grammar based on tree adjoin-
ing grammar. In Proceedings of MTT 2003, pages
207?216, Paris/FRA.
Aravind K. Joshi. 1987. An introduction to tree-
adjoining grammars. In Alexis Manaster-Ramer, ed-
itor, Mathematics of Language, pages 87?115. John
Benjamins, Amsterdam/NL.
Paul Kay and Charles J. Fillmore. 1999. Grammati-
cal constructions and linguistic generalizations: the
what?s x doing y? construction. Language, 75:1?33.
Alexander Koller and Kristina Striegnitz. 2002. Gen-
eration as dependency parsing. In Proceedings of
ACL 2002, Philadelphia/USA.
Geert-Jan M. Kruijff. 2001. A Categorial-Modal Ar-
chitecture of Informativity. Ph.D. thesis, Charles
University, Prague/CZ.
Igor Mel?c?uk. 1988. Dependency Syntax: Theory
and Practice. State Univ. Press of New York, Al-
bany/USA.
Igor Mel?c?uk. 1996. Lexical functions: a tool for the
description of lexical relations in a lexicon. In Leo
Wanner, editor, Lexical Functions in Lexicography
and Natural Language Processing. John Benjamins.
Mozart Consortium. 2004. The Mozart-Oz website.
http://www.mozart-oz.org/.
Alexis Nasr. 1995. A formalism and a parser for lexi-
calised dependency grammars. In 4th International
Workshop on Parsing Technologies, pages 186?195,
Prague/CZ.
Alexis Nasr. 1996. Un syste`me de reformulation au-
tomatique de phrases fonde? sur la The?orie Sens-
Texte: application aux langues contro?le?es. Ph.D.
thesis, Universite? Paris 7.
Petr Sgall, Eva Hajicova, and Jarmila Panevova. 1986.
The Meaning of the Sentence in its Semantic and
Pragmatic Aspects. D. Reidel, Dordrecht/NL.
Lucien Tesni e`re. 1959. El?ements de Syntaxe Struc-
turale. Klincksiek, Paris/FRA.
