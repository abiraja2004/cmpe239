Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions, pages 31?37,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Measuring the Productivity of Determinerless PPs
Florian Do?mges, Tibor Kiss, Antje Mu?ller, Claudia Roch
Sprachwissenschaftliches Institut
Ruhr-Universita?t Bochum
florian.doemges@rub.de
tibor@linguistics.rub.de
antje.mueller@rub.de
claudia.roch@rub.de
Abstract
We determine the productivity of determin-
erless PPs in German quantitatively, restrict-
ing ourselves to the preposition unter. The
study is based on two German newspa-
per corpora, comprising some 210 million
words. The problematic construction, i.e.
unter followed by a determinerless singular
noun occurs some 16.000 times in the cor-
pus. To clarify the empirical productivity
of the construction, we apply a productivity
measure developed by Baayen (2001) to the
syntactic domain by making use of statisti-
cal models suggested in Evert (2004). We
compare two different models and suggest a
gradient descent search for parameter esti-
mation. Our results show that the combina-
tion of unter+noun must in fact be character-
ized as productive, and hence that a syntactic
treatment is required.
Kiss (2006),Kiss (2007),Li (1992), Zipf (1949)
1 Introduction
The combination of a preposition with a singular
count noun, illustrated in (1) with the preposition
unter, is a frequent construction in written and spo-
ken German. From a theoretical perspective, con-
structions like (1) are interesting since they seem
to violate the near universal rule that determiners
should accompany singular count nouns if the lan-
guage in question shows determiners at all (cf. Him-
melmann (1998)).
unter Vorbehalt (with reservation),
(1)unter Androhung (on pain),
unter Lizenz (under licence),
unter Vorwand (pretending)
Baldwin et al (2006) follow a tradition of En-
glish grammar and call constructions like (1) deter-
minerless PPs (D-PP), defined as PPs whose NP-
complement consists of a singular count noun with-
out an accompanying determiner (as e.g. English
by bus, in mind ). It has been claimed that D-PPs
are mostly idiomatic and not productive. Hence,
computational grammars often include D-PPs only
as stock phrases or listed multiword expressions and
do not offer a grammatical treatment. However, both
claims have to be doubted seriously. Kiss (2006,
2007) shows that the class of D-PPs does not con-
tain more idiomatic phrases than a typical phrasal
category should and also argues against a ?light P
hypothesis? which allows a pseudo-compositional
treatment of D-PPs by ignoring the semantics of the
preposition altogether. Trawinski (2003), Baldwin
et al (2006), as well as Trawinski et al (2006) offer
grammatical treatments of D-PPs, or at least of some
subclasses of D-PPs. Interestingly, (Baldwin et al
(2006), 175f.) take the productivity of a subclass
of D-PPs for granted and propose a lexical entry for
prepositions which select determinerless N?s as their
complement. While we are sympathetic to a syn-
tactic treatment of D-PPs in a computational gram-
mar, we think that the productivity of such construc-
tions must be considered more closely. The analysis
of Baldwin et al (2006) allows the unlimited com-
bination of prepositions meeting their lexical spec-
ification with a determinerless N projection. This
31
assumption is not in line with speaker?s intuitions
with regard to producing or judging such construc-
tions. As has been pointed out by Kiss (2006, 2007),
speakers of German can neither freely produce se-
quences consisting of unter and determinerless N
projections (typically a noun) nor can they judge
such constructions in isolation. In addition, not even
very similar nouns can be interchanged in a D-PP,
as can be witnessed by comparing near-synonyms
Voraussetzung and Pra?misse which both translate as
prerequisite, or as provided in the examples in (2).
The examples in (2) illustrate that Voraussetzung
cannot be replaced by Pra?misse in a D-PP (2a, b),
while it can be replaced as a head noun in a full
PP (2c, d). While the contrast in (2) casts doubt on
a productive analysis on the basis of the speakers
knowledge of language, the present paper will show
that unter+noun has to be classified as productive
from an empirical perspective.
a. Auch Philippe Egli besteht auf einer
(2)
eigenen Handschrift - unter
Voraussetzung des Einversta?ndnisses
des Ensembles.
b. * Auch Philippe Egli besteht auf einer
eigenen Handschrift - unter Pra?misse
des Einversta?ndnisses des Ensembles.
c. Auch Philippe Egli besteht auf einer
eigenen Handschrift - unter der
Voraussetzung des Einversta?ndnisses
des Ensembles.
d. Auch Philippe Egli besteht auf einer
eigenen Handschrift - unter der
Pra?misse des Einversta?ndnisses des
Ensembles.
?Philippe Egli insists on his individual way
of dealing with the issue, provided the
ensemble agrees.?
Our investigation is based of a corpus analysis of
D-PPs, consisting of the preposition unter and a fol-
lowing noun, and employs a quantitative measure
of productivity, first developed by Harald Baayen
to analyze morphological productivity. The pre-
liminary conclusion to be drawn from this result
will be that empirical and intuitive productivity of
unter+noun sequences do not match.
In applying Baayen?s productivity measure to
syntactic sequences, however, we are faced with
a serious problem. Baayen?s productivity measure
P (N) is based on the expectation of the hapax
legomena ? E[V1] ? occurring in a vocabulary of
size N, i.e. P (N) = E[V1]N .
 1
 10
 100
 1  10  100  1000
Cardinalities of the frequency classes
Figure 1: Cardinalities of the frequency classes. The
frequency of each type was counted, then the types
were grouped into classes of equal frequency. The
number of types in each class was counted. The fre-
quency values m are assigned to the x-axis, the size
of the class Vm to the y-axis. Both are scaled loga-
rithmically.
Since we cannot derive the expectation of the ha-
pax legomena directly from the corpus, we have to
approximate it by use of regression models. To sim-
plify matters somewhat, Baayen?s models can only
be applied to unigrams, while we have to consider
bigrams ? the preposition and the adjacent noun. To
circumvent this problem, Kiss (2006,2007) calcu-
lated P (N) on the basis of the empirical distribu-
tion of V1 as N gets larger. Evert (2004) offers re-
gression models to determine E[V1] for n-grams and
suggests two different models, the Zipf-Mandelbrot
32
model (ZM) and the finite Zipf-Mandelbrot model
(fZM). The difference between these two models
is that fZM assumes a finite vocabulary. In the
present paper, we apply Evert?s models to sequences
of unter+noun. We differ from Evert?s proposal in
estimating the free parameter ? in both models on
the basis of the gradient descent algorithm. Contrary
to Evert?s assumptions, we will show that the results
of the ZM model are much closer to the empirical
observations than the results of the fZM model.
The paper is structured as follows. Section 2 de-
scribes the empirical basis of the experiment, a cor-
pus study of unter+textnounsg sequences. Section
3 introduces the models suggested by Evert (2004).
Section 3.1 introduces the models, section 3.2 shows
how the free parameter is estimated by making use
of the gradient descent algorithm. The results are
compared in section 3.3.
2 Corpus Study
The present study is based on two German corpora,
with a total of 213 million words: the NZZ-corpus
1995-1998 (Neue Zu?rcher Zeitung) and the FRR-
corpus 1997-1999 (Frankfurter Rundschau). Mak-
ing use of the orthographic convention that nouns
are capitalized in German, we have automatically
extracted 12.993 types, amouting to some 71.000
tokens of unter and a following noun. From these
12.993 types, we have removed all candidates where
the noun is a proper noun, or realized as a plural,
or as member of a support verb construction. Also,
we have excluded typical stock phrases and all mass
nouns. The extraction process was done both man-
ually (proper nouns, mass nouns, support verb con-
structions) and automatically (plurals, mass nouns).
As a result of the extraction process, a total num-
ber of 1.103 types remained, amounting to 16.444
tokens. The frequency of every type was determined
and types with the same frequency were grouped
into classes. 65 equivalence classes were established
according to their frequency m (cf. Figure 1). The
number of elements in every class was counted and
the various count results were associated with the
variables Vm = V1, V2, . . . , V2134.
3 LNRE Model Regression
Baayen (2001) uses the term LNRE models (large
number of rare events) to describe a class of mod-
els that allow the determination of the expectation
with a small set of parameters. Evert (2004) pro-
poses two LNRE models with are based on Zipf?s
Law (Zipf(1949), Li (1992)) to identify the expec-
tations E[V1], . . . , E[Vmax]. Both models are based
on the Zipf-Mandelbrot law.
Zipf?s Law (Zipf(1949), Li (1992)) posits that the
frequency of the r-most frequent type is proportional
to 1r . The distribution of random texts displays a
strong similarity to the results expected according to
Zipf?s Law (cp. Li (1992)). Mandelbrot (1962) et
al. explain this phenomenon by Zipf?s Principle of
Least Effort.
Rouault (1978) shows that the probability of types
with a low frequency asymptotically behaves as
posited by the Zipf-Mandelbrot Law
?i =
C
(i + b)a
with a > 1 and b > 0.
The models are introduced in section 3.1. Both
require a parameter ?, whose value was determined
by employing a gradient descent algorithm imple-
mented in Perl. The optimal value for the free pa-
rameter was found by constructing an error function
to minimise ?. The calculation was carried out for
both models, but better results are produced if the
assumption is given up that the vocabulary is finite.
3.1 Finite and general Zipf-Mandelbrot models
Evert (2004) proposes the finite Zipf-Mandelbrot
model (fZM) and the general Zipf-Mandelbrot
model (ZM) for modelling the expectations of the
frequency classes Vm, i.e. E[V1], . . . , E[Vmax] and
the expected vocabulary size, i.e. the expectation
of the different types E[V ]. The two models make
different assumptions about the probability distribu-
tions of the frequency classes. The fZM assumes
that there is a minimal probability A ? defined as
?A : ?i : A ? ?i. This amounts to the assumption
that the vocabulary size itself is finite. Hence, it can
be expected according to the fZM model that the set
of observed types does not increase once N ? 1A is
reached. In the general ZM model, there is no such
minimal probability.
Assuming a fZM model, Evert (2004) proposes
the following results to estimate the expectation of
33
the frequency classes E[Vm] and the expected vo-
cabulary size E[V ]. In the following equations,
B stands for the maximum probability, defined as
?i : B ? ?i.
E[Vm] =
1 ? ?
(B1?? ?A1??) ?m! ?
N? ? ?(m? ?,N ? A) (3)
E[V ] = 1 ? ?(B1?? ?A1??) ?N
? ? ?(1 ? ?,N ?A)? +
1 ? ?
(B1?? ?A1??) ? ? ? A? ? (1 ? e
?N ?A) (4)
As can be witnessed from the formulae given, N ,
A, and B are already known or directly derivable
from our observations, leaving us with the determi-
nation of the free parameter ?.
Using the general Zipf-Mandelbrot model, we end
with the following estimations, again suggested by
Evert (2004):
E[Vm] =
1 ? ?
B1?? ?m! ?N
? ? ?(m? ?) (5)
E[V ] = 1 ? ?B1?? ?N
? ? ?(1 ? ?)? (6)
As there is no minimal probability, we are left
with the maximal probability B, the token size N,
and again a free parameter ?.
3.2 Parameter estimation through gradient
descent
Since the expectation of the frequency classes in (3)
and (5) depend on the free parameter ?, this pa-
rameter must be estimated in a way that minimises
the deviation of expected and observed values. We
measure the deviation with a function that takes into
account all observed frequencies and their expected
values. A function satisfying these criteria can be
found by treating observed frequency classes and ex-
pectations as real-valued vectors in a vector space.
OT = (V, V1, V2, . . . , V2134) ? R66 (7)
ET (?) =
(E(V )(?), E(V1)(?), . . . , E(V2134)(?)) ? R66 (8)
 1
 10
 100
 1  10  100  1000
Cardinalities of the frequency classes
Figure 2: The application of the fZM LNRE Model
combined with Rouault?s estimation method leads to
a strong deviation from the observed data. The ob-
served data is depicted as a solid line, the data from
the model as a dotted line. The frequency values m
are assigned to the x-axis, the size of the class Vm
respectively the expected size E(Vm) to the y-axis.
Both are scaled logarithmically.
A natural choice for a measure of error is the
quadratic norm of the difference vector between ob-
servation and expectation. As we have no infor-
34
mation about the relationship between different fre-
quencies we assume that the covariance matrix is the
unit matrix.
These thoughts result in the following error func-
tion:
g(?) = (E(V )(?) ? V )2+
?
m=1,...,2134
(E(Vm)(?) ? Vm)2 (9)
The minimal ? is equal to the root of the deriva-
tive of the error function with respect to ?. The
derivative of the error function is:
?g
?? = 2
?E(V )
?? (E(V )(?) ? V )+
2
?
m=1,...,2134
?E(Vm)
?? (E(Vm)(?) ? Vm) (10)
One way to find the minimum ?? =
argmin? g(?) would be to derive the expected
values with respect to ? and solve g?(??) = 0 for
?. As there is no way known to the authors to
accomplish this in a symbolic way, the use of a
numeric method to calculate ?? is advised.
We chose to find ?? by employing a gradient de-
scent method and approximating ?g?? by evaluating
g(?) in small steps ??(i) and calculating ?g(k)??(k) =
g(?0+
Pk
j=1 ??(j))?g(?0+
Pk?1
j=1 ??(j))
??(k) , where k is num-
ber of the iteration.
In the vicinity of a minimum ?g??(?) decreases un-
til it vanishes at ??.
After every iteration the new ??(k) is chosen by
taking under consideration the change of ?g(k)??(k) and
the sign of ??(k? 1). If ?g(k)??(k) increased, the sign of
??(k ? 1) is inverted: ??(k) = ???(k ? 1).
To prevent the algorithm from oscillat-
ing around the minimum the last two values
g(?0 +
?k?2
j=1 ??(j)) and g(?0 +
?k?1
j=1 ??(j)) are
saved.
When a step would result in returning to a previ-
ous value g(?0 +
?k?1
j=1 ??(j) + ??(k)) = g(?0 +
?k?2
j=1 ??(j)), the step size is multiplied by a con-
stant 0 < ? ? 1: ??(k) = ???(k ? 1). The al-
gorithm is stopped when the absolute value of the
step size drops under a predetermined threshold:
|??(k)| < ?threshold.
3.3 Results
Interestingly, ? as determined by gradient descent
on the basis of a fZM leads to a value of 0.666,
which does not match well with our observations,
as can be witnessed in Figure 2.
 1
 10
 100
 1  10  100  1000
Cardinalities of the frequency classes
Figure 3: The ZM LNRE Model leads to a far better
result with less deviation from the observation. The
observed data is depicted as a solid line, the data
from the model as a dotted line. The frequency val-
ues m are assigned to the x-axis, the size of the class
Vm respectively the expected size E(Vm) to the y-
axis. Both are scaled logarithmically.
A gradient descent search on the basis of the ZM
model delivered a value of ? = 0.515, a much better
approximation (with a ?2-Value of 4.514), as can be
35
witnessed from Figure 3. The value thus reached
also converges with the estimation procedure for ?
suggested by Rouault (1978), and taken up by Evert
(2004), i.e. ? = V1V . Consequently, we assume a
ZM model for estimating of expected frequencies.
 0
 0.05
 0.1
 0.15
 0.2
 0  0.2  0.4  0.6  0.8  1
Estimated Productivity
Observed Productivity
Figure 4: The parts of the corpus were appended
to each other and after every step the productivity
P (N) was calculated directly from the data as well
as from the fitted ZM model. The percentage of
the corpus is assigned to the x-axis, the productiv-
ity P (N) is assigned to the y-axis. The productivity
values that were deduced directly from data are plot-
ted as a dotted line, the productivity values from the
ZM model are plotted as a solid line.
To chart the productivity of sequences of the form
unter+noun, we have divided our corpus into six
smaller parts and sampled V , N , and V1 at these
parts. The distribution of the observations thus
gained can be found in Figure 4, together with the
expectations derived from the ZM model. We ob-
serve that both distributions are strikingly similar
and converge at the values for the full corpus.
N V1 E[V1] P (N)
542 74 96.66 0.182
1068 104 123.47 0.118
2151 169 166.41 0.079
4262 282 249.93 0.059
6222 384 332.19 0.054
8365 469 400.43 0.048
16444 746 748.81 0.022
Table 1: Overview of the observed and expected
numbers of hapax legomena and the associated pro-
ductivity value at different corpus sizes.
In a broader perspective, Figure 4 shows that the
combination of unter+noun is a productive process,
when its empirical distribution is considered. As
was already pointed out in section 1, this finding
is at odds with speaker?s intuitions about combina-
tions of unter+noun. Assuming that this result can
be extended to other subclasses of D-PPs, we would
suggest restricting lexical specifications for preposi-
tions to subclasses of nouns, depending on the perti-
nent preposition. Future research will have to show
whether such clear-cut subclasses can be identified
by looking more closely at the empirical findings,
other whether we are confronted with a continuum,
which would require alternative rule types.
References
Harald Baayen. 2001. Word Frequency Distributions.
Kluwer, Dordrecht.
Timothy Baldwin, John Beavers, Leonoor van der Beek,
Francis Bond, Dan Flickinger, and Ivan A. Sag. 2006.
In Search of a Systematic Treatment of Determinerless
PPs. In Patrick Saint-Dizier, editor, Syntax and Se-
mantics of Prepositions, pages 163?179. Springer.
Stefan Evert. 2004. A Simple LNRE Model for Ran-
dom Character Sequences. In Proceedings of the
7mes Journees Internationales d?Analyse Statistique
des Donnees Textuelles, pages 411?422.
Nikolaus Himmelmann. 1998. Regularity in Irregular-
ity: Article Use in Adpositional Phrases. Linguistic
Typology, 2:315?353.
Tibor Kiss. 2006. Do we need a grammar of irregular
sequences? In Miriam Butt, editor, Proceedings of
KONVENS, pages 64?70, Konstanz.
36
Tibor Kiss. 2007. Produktivita?t und Idiomatizita?t
von Pra?position-Substantiv-Sequenzen. forthcoming
in Zeitschrift fu?r Sprachwissenschaft.
W. Li. 1992. Random texts exhibit zipf?s-law-like word
frequency distribution. IEEE Transactions on Infor-
mation Theory.
B. Mandelbrot. 1962. On the theory of word frequencies
and on related Markovian models of discourse. Amer-
ican Mathematical Society.
A. Rouault. 1978. Lois de Zipf et sources markoviennes.
Annales de l?Institut H. Poincare.
Beata Trawinski, Manfred Sailer, and Jan-Philipp Soehn.
2006. Combinatorial Aspects of Collocational Prepo-
sitional Phrases. In Patrick Saint-Dizier, editor, Syn-
tax and Semantics of Prepositions, pages 181?196.
Springer.
Beata Trawinski. 2003. The Syntax of Complex Preposi-
tions in German: An HPSG Approach. In Proceedings
of GLIP, volume 5, pages 155?166.
G. K. Zipf. 1949. Human Behavior and the Principle of
Least Effort. Addison-Wesley, Campridge.
37
