Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 54?62,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Using Artiially Generated Data
to Evaluate Statistial Mahine Translation
Manny Rayner, Paula Estrella, Pierrette Bouillon
University of Geneva, TIM/ISSCO
40 bvd du Pont-d'Arve, CH-1211 Geneva 4, Switzerland
fEmmanuel.Rayner,Paula.Estrella,Pierrette.Bouillongunige.h
Beth Ann Hokey
Mail Stop 19-26, UCSC UARC
NASA Ames Researh Center, Moffett Field, CA 94035?1000
bahokeyus.edu
Yukie Nakao
LINA, Nantes University, 2, rue de la Houssiniere, BP 92208 44322 Nantes Cedex 03
yukie.nakaouniv-nantes.fr
Abstrat
Although Statistial Mahine Translation
(SMT) is now the dominant paradigm
within Mahine Translation, we argue that
it is far from lear that it an outperform
Rule-Based Mahine Translation (RBMT)
on small- to medium-voabulary applia-
tions where high preision is more impor-
tant than reall. A partiularly important
pratial example is medial speeh trans-
lation. We report the results of exper-
iments where we ongured the various
grammars and rule-sets in an Open Soure
medium-voabulary multi-lingual medial
speeh translation system to generate large
aligned bilingual orpora for English !
Frenh and English ! Japanese, whih
were then used to train SMT models based
on the ommon ombination of Giza++,
Moses and SRILM. The resulting SMTs
were unable fully to reprodue the per-
formane of the RBMT, with performane
topping out, even for English ! Frenh,
with less than 70% of the SMT translations
of previously unseen sentenes agreeing
with RBMT translations. When the out-
puts of the two systems differed, human
judges reported the SMT result as fre-
quently being worse than the RBMT re-
sult, and hardly ever better; moreover, the
added robustness of the SMT only yielded
a small improvement in reall, with a large
penalty in preision.
1 Introdution
When Statistial Mahine Translation (SMT) was
rst introdued in the early 90s, it enountered a
hostile reeption, and many people in the researh
ommunity were unwilling to believe it ould ever
be a serious ompetitor to symboli approahes
(f. for example (Arnold et al, 1994)). The pendu-
lum has now swung all the way to the other end of
the sale; right now, the prevailing wisdom within
the researh ommunity is that SMT is the only
truly viable arhiteture, and that rule-based ma-
hine translation (RBMT) is ultimately doomed to
failure. In this paper, one of our initial onerns
will be to argue for a ompromise position. In our
opinion, the initial septiism about SMT was not
groundless; the arguments presented against it of-
ten took the form of examples involving deep lin-
guisti reasoning, whih, it was laimed, would be
hard to address using surfae methods. Proponents
of RBMT had, however, greatly underestimated
the extent to whih SMT would be able to takle
the problem of robustness, where it appears to be
far more powerful than RBMT. For most mahine
translation appliations, robustness is the entral
issue, so SMT's urrent preeminene is hardly sur-
prising.
Even for the large-voabulary tasks where SMT
does best, the situation is by no means as lear as
one might imagine: aording to (Wilks, 2007),
purely statistial systems are still unable to out-
perform SYSTRAN. In this paper, we will how-
ever be more onerned with limited-domain MT
tasks, where robustness is not the key requirement,
and auray is paramount. An immediate exam-
54
ple is medial speeh translation, whih is estab-
lishing itself as an an appliation area of some sig-
niane (Bouillon et al, 2006; Bouillon et al,
2008a). Translation in medial appliations needs
to be extremely aurate, sine mistranslations
an have serious or even fatal onsequenes. At
the panel disussion at the 2008 COLING work-
shop on safety-ritial speeh translation (Rayner
et al, 2008), the onsensus opinion, based on in-
put from pratising physiians, was that an appro-
priate evaluation metri for medial appliations
would be heavily slanted towards auray, as op-
posed to robustness. If the metri is normalised so
as to award 0 points for no translation, and 1 point
for a orret translation, the estimate was that a
suitable sore for an inorret translation would
be something between ?25 and ?100 points. With
these requirements, it seems unlikely that a robust,
broad-overage arhiteture has muh hane of
suess. The obvious strategy is to build a limited-
domain ontrolled-language system, and tune it to
the point where auray reahes the desired level.
For systems of this kind, it is at least oneiv-
able that RBMT may be able to outperform SMT.
The next question is how to investigate the issues
in a methodologially even-handed way. A few
studies, notably (Seneff et al, 2006), suggest that
rule-based translation may in fat be preferable in
these ases. (Another related experiment is de-
sribed in (Dugast et al, 2008), though this was
arried out in a large-voabulary system). These
studies, however, have not been widely ited. One
possible explanation is suspiion about method-
ologial issues. Seneff and her olleagues trained
their SMT system on 20 000 sentene pairs, a
small number by the standards of SMT. It is a pri-
ori not implausible that more training data would
have enabled them to reate an SMT system that
was as good as, or better than, the rule-based sys-
tem.
In this paper, our primary goal is to take this
kind of objetion seriously, and develop a method-
ology designed to enable a tight omparison be-
tween rule-based and statistial arhitetures. In
partiular, we wish to examine the widely be-
lieved laim that SMT is now inherently better
than RBMT. In order to do this, we start with a
limited-domain RBMT system; we use it to auto-
matially generate a large orpus of aligned pairs,
whih is used to train a orresponding SMT sys-
tem. We then ompare the performane of the two
systems.
Our argument will be that this situation essen-
tially represents an upper bound for what is possi-
ble using the SMT approah in a limited domain.
It has been widely remarked that quality, as well
as quantity, of training data is important for good
SMT; in many projets, signiant effort is ex-
pended to lean the original training data. Here,
sine the data is automatially generated by a rule-
based system, we an be sure that it is already
ompletely lean (in the sense of being internally
onsistent), and we an generate as large a quan-
tity of it as we require. The appliation, more-
over, uses only a smallish voabulary and a fairly
onstrained syntax. If the derived SMT system is
unable to math the original RBMT system's per-
formane, it seems reasonable to laim that this
shows that there are types of appliations where
RBMT arhitetures are superior.
The experiments desribed have been arried
out using MedSLT, an Open Soure interlingua-
based limited-domain medial speeh translation
system. The rest of the paper is organised as fol-
lows. Setion 2 provides bakground on the Med-
SLT system. Setion 3 desribes the experimen-
tal framework, and Setion 4 the results obtained.
Setion 5 onludes.
2 The MedSLT System
MedSLT (Bouillon et al, 2005; Bouillon et al,
2008b) is a medium-voabulary interlingua-based
Open Soure speeh translation system for dotor-
patient medial examination questions, whih
provides any-language-to-any-language transla-
tion apabilities for all languages in the set En-
glish, Frenh, Japanese, Arabi, Catalan. Both
speeh reognition and translation are rule-based.
Speeh reognition runs on the Nuane 8.5 reog-
nition platform, with grammar-based language
models built using the Open Soure Regulus om-
piler. As desribed in (Rayner et al, 2006),
eah domain-spei language model is extrated
from a general resoure grammar using orpus-
based methods driven by a seed orpus of domain-
spei examples. The seed orpus, whih typi-
ally ontains between 500 and 1500 utteranes,
is then used a seond time to add probabilisti
weights to the grammar rules; this substantially
improves reognition performane (Rayner et al,
2006, x11.5). Voabulary sizes and performane
measures for speeh reognition in the three lan-
55
guages where serious evaluations have been ar-
ried out are shown in Figure 1.
Language Voab WER SemER
English 447 6% 11%
Frenh 1025 8% 10%
Japanese 422 3% 4%
Table 1: Reognition performane for English,
Frenh and Japanese MedSLT reognisers. ?Vo-
ab? = number of surfae words in soure lan-
guage reogniser voabulary; ?WER? = Word Er-
ror Rate for soure language reogniser, on in-
overage material; ?SemER? = semanti error rate
(proportion of utteranes failing to produe orret
interlingua) for soure language reogniser, on in-
overage material.
At run-time, the reogniser produes a soure-
langage semanti representation. This is rst
translated by one set of rules into an interlingual
form, and then by a seond set into a target lan-
guage representation. A target-language Regu-
lus grammar, ompiled into generation form, turns
this into one or more possible surfae strings, af-
ter whih a set of generation preferenes piks
one out. Finally, the seleted string is realised in
spoken form. Robustness issues are addressed by
means of a bak-up statistial reogniser, whih
drives a robust embedded help system. The pur-
pose of the help system (Chatzihrisas et al,
2006) is to guide the user towards supported ov-
erage; it performs approximate mathing of out-
put from the statistial reogniser again a library
of sentenes whih have been marked as orretly
proessed during system development, and then
presents the losest mathes to the user.
Examples of typial English domain sentenes
and their translations into Frenh and Japanese are
shown in Figure 2.
3 Experimental framework
In the literature on language modelling, there is
a known tehnique for bootstrapping a statisti-
al language model (SLM) from a grammar-based
language model (GLM). The grammar whih
forms the basis of the GLM is sampled randomly
in order to reate an arbitrarily large orpus of ex-
amples; these examples are then used as a train-
ing orpus to build the SLM (Jurafsky et al, 1995;
Jonson, 2005). We adapt this proess in a straight-
forward way to onstrut an SMT for a given
language pair, using the soure language gram-
mar, the soure-to-interlingua translation rules, the
interlingua-to-target-language rules, and the tar-
get language generation grammar. We start in the
same way, using the soure language grammar to
build a randomly generated soure language or-
pus; as shown in (Hokey et al, 2008), it is im-
portant to have a probabilisti grammar. We then
use the omposition of the other omponents to
attempt to translate eah soure language sentene
into a target language equivalent, disarding the
examples for whih no translation is produed.
The result is an aligned bilingual orpus of ar-
bitrary size, whih an be used to train an SMT
model.
We used this method to generate aligned or-
pora for the two MedSLT language pairs English
! Frenh and English ! Japanese. For eah lan-
guage pair, we rst generated one million soure-
language utteranes; we next ltered them to keep
only examples whih were full sentenes, as op-
posed to elliptial phrases, and nally used the
translation rules and target-language generators to
attempt to translate eah sentene. This reated
approximately 305K aligned sentene-pairs for
English ! Frenh (1901K words English, 1993K
words Frenh), and 311K aligned sentene-pairs
for English ! Japanese (1941K words English,
2214K words Japanese). We held out 2.5% of
eah set as development data, and 2.5% as test
data. Using Giza++, Moses and SRILM (Oh and
Ney, 2000; Koehn et al, 2007; Stolke, 2002), we
trained SMT models from inreasingly large sub-
sets of the training portion, using the development
portion in the usual way to optimize parameter val-
ues. Finally, we used the resulting models to trans-
late the test portion.
Our primary goal was to measure the extent to
whih the derived versions of the SMT were able
to approximate the original RBMT on data whih
was within the RBMT's overage. There is a sim-
ple and natural way to perform this measurement:
we apply the BLEU metri (Papineni et al, 2001),
with the RBMT's translation taken as the refer-
ene. This means that perfet orrespondene be-
tween the two translations would yield a BLEU
sore of 1.0.
This raises an important point. The BLEU
sores we are using here are non-standard; they
measure the extent to whih the SMT approxi-
mates the RBMT, rather than, as usual, measuring
56
English Is the pain above your eye?
Frenh Avez-vous mal au dessus des yeux?
Japanese Itami wa me no ue no atari desu ka?
English Have you had the pain for more than a month?
Frenh Avez-vous mal depuis plus d'un mois?
Japanese Ikkagetsu ijou itami wa tsuzuki mashita ka?
English Is the pain assoiated with nausea?
Frenh Avez-vous des nause?es quand vous avez la douleur?
Japanese Itamu to hakike wa okori masu ka?
English Does bright light make the pain worse?
Frenh La douleur est-elle aggrave?e par une lumiere forte?
Japanese Akarui hikari wo miru to zutsu wa hidoku nari masu ka?
Table 2: Examples of English domain sentenes, and the system's translations into Frenh and Japanese.
the extent to whih it approximates human trans-
lations. It is important to bring in human judge-
ment, to evaluate the ases where the SMT and
RBMT differ. If, in these ases, it transpired that
human judges typially thought that the SMT was
as good as the RBMT, then the differene would
be purely aademi. We need to satisfy ourselves
that human judges typially asribe differenes be-
tween SMT and RBMT to shortomings in the
SMT rather than in the RBMT.
Conretely, we olleted all the different
hSoure, SMT-translation, RBMT-translationi
triples produed during the ourse of the ex-
periments, and extrated those where the two
translations were different. We randomly seleted
a set of examples for eah language pair, and
asked human judges to lassify them into one of
the following ategories:
 RBMT better: The RBMT translation was
better, in terms of preserving meaning and/or
being grammatially orret;
 SMT better: The SMT translation was bet-
ter, in terms of preserving meaning and/or be-
ing grammatially orret;
 Similar: Both translations were about
equally good OR the soure sentene was
meaningless in the domain.
In order to show that our metris are intuitively
meaningful, it is sufient to demonstrate that the
frequeny of ourrene of RBMT better is both
large in omparison to that of SMT better, and
aounts for a substantial proportion of the total
population.
Finally, we onsider the question of whether
the SMT, whih is apable of translating out-of-
grammar sentenes, an add useful robustness to
the base system. We olleted, from the set used in
the experiments desribed in (Rayner et al, 2005),
all the English sentenes whih failed to be trans-
lated into Frenh. We used the best version of
the English ! Frenh SMT to translate eah of
these sentenes, and asked human judges to eval-
uate the translations as being learly aeptable,
learly unaeptable, or borderline.
In the next setion, we present the results of the
various experiments we have just desribed.
4 Results
We begin with Figure 1, whih shows non-
standard BLEU sores for versions of the English
! Frenh SMT system trained on quantities of
data inreasing from 14 287 to 285 740 pairs. As
an be seen, translation performane improves up
to about 175 000 pairs. After this, it levels out
at around BLEU = 0.90, well below that of the
RBMT system with whih it is being ompared.
A more diret way to report the result is simply to
ount the proportion of test sentenes that are not
in the training data, whih are translated similarly
by the SMT and the RBMT. This gure tops out at
around 68%.
The results strongly suggest that the SMT is
unable to repliate the RBMT's performane at
all losely even in an easy language-pair, irre-
spetive of the amount of training data available.
Out of uriosity, and to reassure ourselves that the
automati generation proedure was doing some-
thing useful, we also tried training the English !
Frenh SMT on pairs derived from the 669 ut-
57
Figure 1: Non-standard BLEU sores against
number of pairs of training sentenes for English
! Frenh; training and test data both indepen-
dently generated, hene overlapping.
terane ?seed orpus? used to generate the gram-
mar (f. Setion 2). This produed utterly dis-
mal performane, with BLEU = 0.52. The result is
more interesting than it may rst appear, sine, in
speeh reognition, the differene in performane
between the SLMs trained from seed orpora and
large generated orpora is fairly small (Hokey et
al., 2008).
It seemed possible that the improvement in per-
formane with inreased quantities of training data
might, in effet, only be due to the SMT fun-
tioning as a translation memory; sine training
and test data are independently generated by the
same random proess, they overlap, with the de-
gree of overlap inreasing as the training set gets
larger. In order to investigate this hypothesis,
we repeated the experiments with data whih had
been uniqued, so that the training and test sets
were ompletely disjoint, and neither ontained
any dupliate sentenes
1
. In fat, Figure 2 show
that the graph for uniqued English ! Frenh data
are fairly similar to the one for the original non-
uniqued data shown in Figures 1. The main differ-
ene is that the non-standard BLEU sore for the
1
Our opinion is that this is not a realisti way to evaluate
the performane of a small-voabulary system; for example,
in MedSLT, one expets that at least some training sentenes,
e.g. ?Where is the pain??, will also our frequently in test
data.
Figure 2: Non-standard BLEU sores against
number of pairs of training sentenes for English
! Frenh; training and test data both indepen-
dently generated, then uniqued to remove dupli-
ates and overlapping items.
uniqued data, unsurprisingly, tops out at a lower
level, reeting the fat that a ?translation mem-
ory? effet does indeed our to some extent.
Results for English ! Japanese showed the
same trends as English ! Frenh, but were more
pronouned. Table 3 ompares the performane
of the best versions of the SMTs for the two
language-pairs, using both plain and artiially
uniqued data. We see that, with plain data, the
English ! Japanese SMT falls even further short
of repliating the performane of the RBMT than
was the ase for English ! Frenh; BLEU is
only 0.76. The differene between the plain and
uniqued versions is also more extreme. BLEU
(0.64) is onsiderably lower for the version trained
on uniqued data, suggesting that the SMT for this
language pair is nding it harder to generalise,
and is in effet loser to funtioning as a trans-
lation memory. This is onrmed by ounting
the sentenes in test data and not in training data
whih were translated similarly by the SMT and
the RBMT; we nd that the gure tops out at the
very low value of 26%.
As noted in our disussion of the experimental
framework, the non-standard BLEU sores only
address the question of whether the performane
of the SMT and RBMT systems is the same. It is
58
Training data Test data BLEU
English ! Frenh
Generated Generated 0.90
Gen/uniqued Gen/uniqued 0.85
English ! Japanese
Generated Generated 0.76
Gen/uniqued Gen/uniqued 0.64
Table 3: Translation performane, in terms of non-
standard BLEU metri, for different ongura-
tions, training on all available data of the spe-
ied type. ?Generated? = data randomly gener-
ated; ?Gen/uniqued? = data randomly generated,
then uniqued so that dupliates are removed and
test and training pairs do not overlap.
neessary to establish what the differenes mean
in terms of human judgements. We onsequently
turn to evaluation of the pairs for whih the SMT
and the RBMT systems produed different trans-
lation results.
Table 4 shows the ategorisation, aording to
the riteria outlined at the end of Setion 3, for 500
English ! Frenh pairs randomly seleted from
the set of examples where RBMT and SMT gave
different results; we asked three judges to evalu-
ate them independently, and ombined their judg-
ments by majority deision where appropriate. We
observed a very heavy bias towards the RBMT,
with unanimous agreement among the judges that
the RBMT translation was better in 201/500 ases,
and 2-1 agreement in a further 127. In ontrast,
there were only 4/500 ases where the judges
unanimously thought that the SMT translation was
preferable, with a further 12 supported by a ma-
jority deision. The rest of the table gives the
ases where the RBMT and SMT translations were
judged the same or ases in whih the judges dis-
agreed; there were only 41/500 ases where no
majority deision was reahed. Our overall on-
lusion is that we are justied in evaluating the
SMT by using the BLEU sores with the RBMT as
the referene. Of the ases where the two systems
differ, only a tiny fration, at most 16/500, indi-
ate a better translation from the SMT, and well
over half are translated better by the RBMT. Ta-
ble 5 presents typial examples of bad SMT trans-
lations in the English ! Frenh pair, ontrasted
with the translations produed by the RBMT. The
rst two are grammatial errors (a superuous ex-
tra verb in the rst, and agreement errors in the
seond). The third is an bad hoie of tense and
preposition; although grammatial, the target lan-
guage sentene fails to preserve the meaning, and,
rather than referring to a 20 day period ending
now, instead refers to a 20 day period some time
in the past.
Result Agreement Count
RBMT better all judges 201
RBMT better majority 127
SMT better all judges 4
SMT better majority 12
Similar all judges 34
Similar majority 81
Unlear disagree 41
Total 500
Table 4: Comparison of RBMT and SMT perfor-
mane on 500 randomly hosen English ! Frenh
translation examples, evaluated independently by
three judges.
Table 6 shows a similar evaluation for the En-
glish ! Japanese. Here, the differene between
the SMT and RBMT versions was so pronouned
that we felt justied in taking a smaller sample, of
only 150 sentenes. This time, 92/150 ases were
unanimously judged as having a better RBMT
translation, and there was not a single ase where
even a majority found that the SMT was better.
Agreement was good here too, with only 8/150
ases not yielding at least a majority deision.
Result Agreement Count
RBMT better all judges 92
RBMT better majority 32
SMT better all judges 0
SMT better majority 0
Similar all judges 2
Similar majority 16
Unlear disagree 8
Total 150
Table 6: Comparison of RBMT and SMT per-
formane on 150 randomly hosen English !
Japanese translation examples, evaluated indepen-
dently by three judges.
Finally, we look at the performane of the SMT
on material whih the RBMT is not able to trans-
late. This would seem to be a situation where
59
English does a temperature hange ause the headahe
RBMT Frenh vos maux de t?ete sont-ils ause?s par des hangements de tempe?rature
(your headahes are-they aused by hanges of temperature)
SMT Frenh avez-vous vos maux de t?ete sont-ils ause?s par des hangements de tempe?rature
(have-you your headahes are-they aused by hanges of temperature)
English are headahes relieved in the afternoon
RBMT Frenh vos maux de t?ete diminuent-ils l'apres-midi
(your headahes (MASC-PLUR) derease-MASC-PLUR the afternoon)
SMT Frenh vos maux de t?ete diminue-t-elle l'apres-midi
(your headahes (MASC-PLUR) derease-FEM-SING the afternoon)
English have you had them for twenty days
RBMT Frenh avez-vous vos maux de t?ete depuis vingt jours
(have-you your headahes sine twenty days)
SMT Frenh avez-vous eu vos maux de t?ete pendant vingt jours
(have-you had your headahes during twenty days)
Table 5: Examples of inorret SMT translations from English into Frenh. Errors are highlighted in
bold.
the SMT ould have an advantage; robustness is
generally a strength of statistial approahes. We
return to English ! Frenh in Table 7, whih
presents the result of running the best SMT model
on the 357 examples from the test set in (Rayner
et al, 2005) whih failed to be translated by the
RBMT. We divide the set into ategories based on
the reason for failure of the RBMT.
In the most populous group, translations that
failed due to out of voabulary items, the SMT
was, more or less by onstrution, also unable
to produe a translation. For the 110 items that
were out of grammar overage for the RBMT, the
SMT produed 38 good translations, and another 4
borderline translations. There were 50 items that
were within the soure grammar overage of the
RBMT, but failed somewhere in transfer and gen-
eration proessing. Of those, the majority (32)
represented ?bad? soure sentenes, onsidered as
ill-formed for the purposes of this experiment. Out
of the remaining items that were within RBMT
grammar overage, the SMT managed to produe
5 good translations and 1 borderline translation. In
total, on the most lenient interpretation, the SMT
produed 48 additional translations out of 357.
While this improvement in reall is arguably worth
having, it would ome at the prie of a substantial
deline in preision.
5 Disussion and Conlusions
We have presented a novel methodology for om-
paring RBMT and SMT, and tested it on a spe-
Result Count
Out of voabulary
Bad translation 187
Out of soure grammar overage
Good translation 38
Bad translation 44
Borderline translation 4
Bad soure sentene 34
In soure grammar overage
Good translation 5
Bad translation 12
Borderline translation 1
Bad soure sentene 32
Total 357
Table 7: English ! Frenh SMT performane on
examples from the test set whih failed to be trans-
lated by the RBMT, evaluated by one judge.
i pair of RBMT and SMT arhitetures. Our
laim is that these results show that the version
of SMT used here is not in fat apable of repro-
duing the output of the RBMT system. Although
there has been some interest in attempting to train
SMT systems from RBMT output, the evaluation
issues that arise when omparing SMT and RBMT
versions of a high-preision limited-domain sys-
tem are different from those arising in most MT
tasks, and neessitate a orrespondingly different
methodology. It is easy to gain the impression that
it is unsound, and that the experiment has been set
60
up in suh a way that only one result is possible.
This is not, in fat, true.
When we have disussed the methodology with
people who work primarily with SMT, we have
heard two main objetions. The rst is that the
SMT is being trained on RBMT output, and hene
an only be worse; a ommon suggestion is that
a system trained on human-produed translations
ould yield better results. It is not at all implau-
sible that an SMT trained on this kind of data
might perform better on material whih is outside
the overage of the RBMT system. In this do-
main, however, the important issue is preision,
not reall; what is ritial is the ability to trans-
late aurately on material that is within the on-
strained language dened by the RBMT overage.
The RBMT engine gives very good performane
on in-overage data, as has been shown in other
evaluations of the MedSLT system, e.g. (Rayner et
al., 2005); over 97% of all in-overage sentenes
are orretly translated. Human-generated transla-
tions would often, no doubt, be more natural than
those produed by the RBMT, and there would be
slightly fewer outright mistranslations. But the
primary reason why the SMT is doing badly is
not that the training material ontains bad trans-
lations, but rather that the SMT is inapable of
orretly reproduing the translations it sees in the
training data. Even in the easy English ! Frenh
language-pair, the SMT often produes a different
translation from the RBMT. It ould a priori have
been oneivable that the differenes were unin-
teresting, in the sense that SMT outputs different
from RBMT outputs were as good, or even better.
In fat, Table 4 show that this is not true; when the
two translations differ, although the SMT transla-
tion an oasionally be better, it is usually worse.
Table 6 shows that this problem is onsiderably
more aute in English ! Japanese. Thus the
SMT system's inability to model the RBMT sys-
tem points to a real limitation.
If the SMT had instead been trained on human-
generated data, its performane on in-overage
material ould only have improved substantially if
the SMT for some reason found it easier to learn to
reprodue patterns in human-generated data than
in RBMT-generated data. This seems unlikely.
The SMT is being trained from a set of translation
pairs whih are guaranteed to be ompletely on-
sistent, sine they have been automatially gener-
ated by the RBMT; the fat that the RBMT system
only has a small voabulary should also work in
its favour. If the SMT is unable to reprodue the
RBMT's output, it is reasonable to assume it will
have even greater difulty reproduing transla-
tions present in normal human-generated training
data, whih is always far from onsistent, and will
have a larger voabulary.
The seond objetion we have heard is that the
non-standard BLEU sores whih we have used to
measure performane use the RBMT translations
as a referene. People are quik to point out that,
if real human translations were sored in this way,
they would do less well on the non-standard met-
ris than the RBMT translations. This is, indeed,
absolutely true, and explains why it was essential
to arry out the omparison judging shown in Ta-
bles 4 and 6. If we had ompared human transla-
tions with RBMT translations in the same way, we
would have found that human translations whih
differed from RBMT translations were sometimes
better, and hardly ever worse. This would have
shown that the non-standard metris were inap-
propriate for the task of evaluating human trans-
lations. In the atual ase onsidered in this paper,
we nd a ompletely different pattern: the differ-
enes are one-sided in the opposite diretion, in-
diating that the non-standard metris do in fat
agree with human judgements here.
A general objetion to all these experiments is
that there may be more powerful SMT arhite-
tures. We used the Giza++/Moses/SRILM om-
binination beause it is the de fato standard. We
have posted the data we used at http://www.
bahr.net/geaf2009; this will allow other
groups to experiment with alternate arhitetures,
and determine whether they do in fat yield sig-
niant improvements. For the moment, however,
we think it is reasonable to laim that, in domains
where high auray is required, it remains to be
shown that SMT approahes are apable of ahiev-
ing the levels of performane that rule-based sys-
tems an deliver.
61
Referenes
D. Arnold, L. Balkan, S. Meijer, R.L. Humphreys, and
L. Sadler. 1994. Mahine Translation: An Introdu-
tory Guide. Blakwell, Oxford.
P. Bouillon, M. Rayner, N. Chatzihrisas, B.A.
Hokey, M. Santaholma, M. Starlander, Y. Nakao,
K. Kanzaki, and H. Isahara. 2005. A generi multi-
lingual open soure platform for limited-domain
medial speeh translation. In Proeedings of the
10th Conferene of the European Assoiation for
Mahine Translation (EAMT), pages 50?58, Bu-
dapest, Hungary.
P. Bouillon, F. Ehsani, R. Frederking, and M. Rayner,
editors. 2006. Proeedings of the HLT-NAACL In-
ternational Workshop on Medial Speeh Transla-
tion, New York.
P. Bouillon, F. Ehsani, R. Frederking, M. MTear,
and M. Rayner, editors. 2008a. Proeedings of
the COLING Workshop on Speeh Proessing for
Safety Critial Translation and Pervasive Applia-
tions, Manhester.
P. Bouillon, G. Flores, M. Georgesul, S. Halimi,
B.A. Hokey, H. Isahara, K. Kanzaki, Y. Nakao,
M. Rayner, M. Santaholma, M. Starlander, and
N. Tsourakis. 2008b. Many-to-many multilingual
medial speeh translation on a PDA. In Proeed-
ings of The Eighth Conferene of the Assoiation
for Mahine Translation in the Amerias, Waikiki,
Hawaii.
N. Chatzihrisas, P. Bouillon, M. Rayner, M. San-
taholma, M. Starlander, and B.A. Hokey. 2006.
Evaluating task performane for a unidiretional
ontrolled language medial speeh translation sys-
tem. In Proeedings of the HLT-NAACL Interna-
tional Workshop on Medial Speeh Translation,
pages 9?16, New York.
L. Dugast, J. Senellart, and P. Koehn. 2008. Can we
relearn an RBMT system? In Proeedings of the
Third Workshop on Statistial Mahine Translation,
pages 175?178, Columbus, Ohio.
B.A. Hokey, M. Rayner, and G. Christian. 2008.
Training statistial language models from grammar-
generated data: A omparative ase-study. In Pro-
eedings of the 6th International Conferene on Nat-
ural Language Proessing, Gothenburg, Sweden.
R. Jonson. 2005. Generating statistial language mod-
els from interpretation grammars in dialogue sys-
tems. In Proeedings of the 11th EACL, Trento,
Italy.
A. Jurafsky, C. Wooters, J. Segal, A. Stolke, E. Fos-
ler, G. Tajhman, and N. Morgan. 1995. Us-
ing a stohasti ontext-free grammar as a language
model for speeh reognition. In Proeedings of
the IEEE International Conferene on Aoustis,
Speeh and Signal Proessing, pages 189?192.
P. Koehn, H. Hoang, A. Birh, C. Callison-Burh,
M. Federio, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open soure
toolkit for statistial mahine translation. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 2.
F.J. Oh and H. Ney. 2000. Improved statistial align-
ment models. In Proeedings of the 38th Annual
Meeting of the Assoiation for Computational Lin-
guistis, Hong Kong.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001.
BLEU: a method for automati evaluation of ma-
hine translation. Researh Report, Computer Si-
ene RC22176 (W0109-022), IBM Researh Divi-
sion, T.J.Watson Researh Center.
M. Rayner, P. Bouillon, N. Chatzihrisas, B.A.
Hokey, M. Santaholma, M. Starlander, H. Isahara,
K. Kanzaki, and Y. Nakao. 2005. A methodol-
ogy for omparing grammar-based and robust ap-
proahes to speeh understanding. In Proeedings
of the 9th International Conferene on Spoken Lan-
guage Proessing (ICSLP), pages 1103?1107, Lis-
boa, Portugal.
M. Rayner, B.A. Hokey, and P. Bouillon. 2006.
Putting Linguistis into Speeh Reognition: The
Regulus Grammar Compiler. CSLI Press, Chiago.
M. Rayner, P. Bouillon, G. Flores, F. Ehsani, M. Star-
lander, B. A. Hokey, J. Brotanek, and L. Biewald.
2008. A small-voabulary shared task for medial
speeh translation. In Proeedings of the COLING
Workshop on Speeh Proessing for Safety Criti-
al Translation and Pervasive Appliations, Manh-
ester.
S. Seneff, C. Wang, and J. Lee. 2006. Combining lin-
guisti and statistial methods for bi-diretional En-
glish Chinese translation in the ight domain. In
Proeedings of AMTA 2006.
A. Stolke. 2002. SRILM - an extensible language
modeling toolkit. In Seventh International Confer-
ene on Spoken Language Proessing. ISCA.
Y. Wilks. 2007. Stone soup and the Frenh room. In
K. Ahmad, C. Brewster, and M. Stevenson, editors,
Words and Intelligene I: Seleted Papers by Yorik
Wilks, pages 255?265.
62
