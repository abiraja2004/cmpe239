Determining the Unithood of Word Sequences using a Probabilistic
Approach
Wilson Wong, Wei Liu and Mohammed Bennamoun
School of Computer Science and Software Engineering
University of Western Australia
Crawley WA 6009
{wilson,wei,bennamou}@csse.uwa.edu.au
Abstract
Most research related to unithood were con-
ducted as part of a larger effort for the deter-
mination of termhood. Consequently, nov-
elties are rare in this small sub-field of term
extraction. In addition, existing work were
mostly empirically motivated and derived.
We propose a new probabilistically-derived
measure, independent of any influences of
termhood, that provides dedicated measures
to gather linguistic evidence from parsed
text and statistical evidence from Google
search engine for the measurement of unit-
hood. Our comparative study using 1, 825
test cases against an existing empirically-
derived function revealed an improvement in
terms of precision, recall and accuracy.
1 Introduction
Automatic term recognition, also referred to as term
extraction or terminology mining, is the process of
extracting lexical units from text and filtering them
for the purpose of identifying terms which charac-
terise certain domains of interest. This process in-
volves the determination of two factors: unithood
and termhood. Unithood concerns with whether or
not a sequence of words should be combined to
form a more stable lexical unit. On the other hand,
termhood measures the degree to which these sta-
ble lexical units are related to domain-specific con-
cepts. Unithood is only relevant to complex terms
(i.e. multi-word terms) while termhood (Wong et
al., 2007a) deals with both simple terms (i.e. single-
word terms) and complex terms. Recent reviews by
(Wong et al, 2007b) show that existing research on
unithood are mostly carried out as a prerequisite to
the determination of termhood. As a result, there
is only a small number of existing measures dedi-
cated to determining unithood. Besides the lack of
dedicated attention in this sub-field of term extrac-
tion, the existing measures are usually derived from
term or document frequency, and are modified as
per need. As such, the significance of the different
weights that compose the measures usually assume
an empirical viewpoint. Obviously, such methods
are at most inspired by, but not derived from formal
models (Kageura and Umino, 1996).
The three objectives of this paper are (1) to sepa-
rate the measurement of unithood from the determi-
nation of termhood, (2) to devise a probabilistically-
derived measure which requires only one thresh-
old for determining the unithood of word se-
quences using non-static textual resources, and (3)
to demonstrate the superior performance of the new
probabilistically-derived measure against existing
empirical measures. In regards to the first objective,
we will derive our probabilistic measure free from
any influence of termhood determination. Follow-
ing this, our unithood measure will be an indepen-
dent tool that is applicable not only to term extrac-
tion, but many other tasks in information extraction
and text mining. Concerning the second objective,
we will devise our new measure, known as the Odds
of Unithood (OU), which are derived using Bayes
Theorem and founded on a few elementary probabil-
ities. The probabilities are estimated using Google
page counts in an attempt to eliminate problems re-
lated to the use of static corpora. Moreover, only
103
one threshold, namely, OUT is required to control
the functioning of OU . Regarding the third objec-
tive, we will compare our new OU against an ex-
isting empirically-derived measure called Unithood
(UH) (Wong et al, 2007b) in terms of their preci-
sion, recall and accuracy.
In Section 2, we provide a brief review on some of
existing techniques for measuring unithood. In Sec-
tion 3, we present our new probabilistic approach,
the measures involved, and the theoretical and in-
tuitive justification behind every aspect of our mea-
sures. In Section 4, we summarize some findings
from our evaluations. Finally, we conclude this pa-
per with an outlook to future work in Section 5.
2 Related Works
Some of the most common measures of unithood
include pointwise mutual information (MI) (Church
and Hanks, 1990) and log-likelihood ratio (Dunning,
1994). In mutual information, the co-occurrence fre-
quencies of the constituents of complex terms are
utilised to measure their dependency. The mutual
information for two words a and b is defined as:
MI(a, b) = log
2
p(a, b)
p(a)p(b) (1)
where p(a) and p(b) are the probabilities of occur-
rence of a and b. Many measures that apply sta-
tistical techniques assuming strict normal distribu-
tion, and independence between the word occur-
rences (Franz, 1997) do not fare well. For handling
extremely uncommon words or small sized corpus,
log-likelihood ratio delivers the best precision (Kurz
and Xu, 2002). Log-likelihood ratio attempts to
quantify how much more likely one pair of words is
to occur compared to the others. Despite its poten-
tial, ?How to apply this statistic measure to quan-
tify structural dependency of a word sequence re-
mains an interesting issue to explore.? (Kit, 2002).
(Seretan et al, 2004) tested mutual information, log-
likelihood ratio and t-tests to examine the use of re-
sults from web search engines for determining the
collocational strength of word pairs. However, no
performance results were presented.
(Wong et al, 2007b) presented a hybrid approach
inspired by mutual information in Equation 1, and
C-value in Equation 3. The authors employ Google
page counts for the computation of statistical evi-
dences to replace the use of frequencies obtained
from static corpora. Using the page counts, the au-
thors proposed a function known as Unithood (UH)
for determining the mergeability of two lexical units
ax and ay to produce a stable sequence of words s.
The word sequences are organised as a set W =
{s, ax, ay} where s = axbay is a term candidate,
b can be any preposition, the coordinating conjunc-
tion ?and? or an empty string, and ax and ay can
either be noun phrases in the form Adj?N+ or an-
other s (i.e. defining a new s in terms of other s).
The authors define UH as:
UH(ax, ay) =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
1 if (MI(ax, ay) > MI+) ?
(MI+ ? MI(ax, ay)
? MI??
ID(ax, s) ? IDT ?
ID(ay, s) ? IDT ?
IDR+ ? IDR(ax, ay)
? IDR?)
0 otherwise
(2)
where MI+, MI?, IDT , IDR+ and IDR?
are thresholds for determining mergeability deci-
sions, and MI(ax, ay) is the mutual information be-
tween ax and ay, while ID(ax, s), ID(ay, s) and
IDR(ax, ay) are measures of lexical independence
of ax and ay from s. For brevity, let z be either ax or
ay, and the independence measure ID(z, s) is then
defined as:
ID(z, s) =
{
log
10
(nz ? ns) if(nz > ns)
0 otherwise
where nz and ns is the Google page count for z and
s respectively. On the other hand, IDR(ax, ay) =
ID(ax,s)
ID(ay ,s) . Intuitively, UH(ax, ay) states that the two
lexical units ax and ay can only be merged in two
cases, namely, 1) if ax and ay has extremely high
mutual information (i.e. higher than a certain thresh-
old MI+), or 2) if ax and ay achieve average mu-
tual information (i.e. within the acceptable range of
two thresholds MI+ and MI?) due to both of their
extremely high independence (i.e. higher than the
threshold IDT ) from s.
(Frantzi, 1997) proposed a measure known as
Cvalue for extracting complex terms. The measure
104
is based upon the claim that a substring of a term
candidate is a candidate itself given that it demon-
strates adequate independence from the longer ver-
sion it appears in. For example, ?E. coli food poi-
soning?, ?E. coli? and ?food poisoning? are accept-
able as valid complex term candidates. However,
?E. coli food? is not. Therefore, some measures
are required to gauge the strength of word combina-
tions to decide whether two word sequences should
be merged or not. Given a word sequence a to be
examined for unithood, the Cvalue is defined as:
Cvalue(a) =
{
log
2
|a|fa if |a| = g
log
2
|a|(fa ?
?
l?La fl
|La| ) otherwise
(3)
where |a| is the number of words in a, La is the
set of longer term candidates that contain a, g is
the longest n-gram considered, fa is the frequency
of occurrence of a, and a /? La. While certain re-
searchers (Kit, 2002) consider Cvalue as a termhood
measure, others (Nakagawa and Mori, 2002) accept
it as a measure for unithood. One can observe that
longer candidates tend to gain higher weights due to
the inclusion of log
2
|a| in Equation 3. In addition,
the weights computed using Equation 3 are purely
dependent on the frequency of a.
3 A Probabilistically-derived Measure for
Unithood Determination
We propose a probabilistically-derived measure for
determining the unithood of word pairs (i.e. po-
tential term candidates) extracted using the head-
driven left-right filter (Wong, 2005; Wong et al,
2007b) and Stanford Parser (Klein and Manning,
2003). These word pairs will appear in the form of
(ax, ay) ? A with ax and ay located immediately
next to each other (i.e. x + 1 = y), or separated
by a preposition or coordinating conjunction ?and?
(i.e. x + 2 = y). Obviously, ax has to appear before
ay in the sentence or in other words, x < y for all
pairs where x and y are the word offsets produced by
the Stanford Parser. The pairs in A will remain as
potential term candidates until their unithood have
been examined. Once the unithood of the pairs in
A have been determined, they will be referred to as
term candidates. Formally, the unithood of any two
lexical units ax and ay can be defined as
Definition 1 The unithood of two lexical units is the
?degree of strength or stability of syntagmatic com-
binations and collocations? (Kageura and Umino,
1996) between them.
It is obvious that the problem of measuring the
unithood of any pair of words is the determination
of their ?degree? of collocational strength as men-
tioned in Definition 1. In practical terms, the ?de-
gree? mentioned above will provide us with a way to
determine if the units ax and ay should be combined
to form s, or left alone as separate units. The collo-
cational strength of ax and ay that exceeds a certain
threshold will demonstrate to us that s is able to form
a stable unit and hence, a better term candidate than
ax and ay separated. It is worth pointing that the
size (i.e. number of words) of ax and ay is not lim-
ited to 1. For example, we can have ax=?National
Institute?, b=?of? and ay=?Allergy and Infectious
Diseases?. In addition, the size of ax and ay has no
effect on the determination of their unithood using
our approach.
As we have discussed in Section 2, most of
the conventional practices employ frequency of oc-
currence from local corpora, and some statistical
tests or information-theoretic measures to determine
the coupling strength between elements in W =
{s, ax, ay}. Two of the main problems associated
with such approaches are:
? Data sparseness is a problem that is well-
documented by many researchers (Keller et al,
2002). It is inherent to the use of local corpora
that can lead to poor estimation of parameters
or weights; and
? Assumption of independence and normality of
word distribution are two of the many problems
in language modelling (Franz, 1997). While
the independence assumption reduces text to
simply a bag of words, the assumption of nor-
mal distribution of words will often lead to in-
correct conclusions during statistical tests.
As a general solution, we innovatively employ re-
sults from web search engines for use in a proba-
bilistic framework for measuring unithood.
As an attempt to address the first problem, we
utilise page counts by Google for estimating the
probability of occurrences of the lexical units in W .
105
We consider the World Wide Web as a large general
corpus and the Google search engine as a gateway
for accessing the documents in the general corpus.
Our choice of using Google to obtain the page count
was merely motivated by its extensive coverage. In
fact, it is possible to employ any search engines on
the World Wide Web for this research. As for the
second issue, we attempt to address the problem of
determining the degree of collocational strength in
terms of probabilities estimated using Google page
count. We begin by defining the sample space, N as
the set of all documents indexed by Google search
engine. We can estimate the index size of Google,
|N | using function words as predictors. Function
words such as ?a?, ?is? and ?with?, as opposed to
content words, appear with frequencies that are rel-
atively stable over many different genres. Next, we
perform random draws (i.e. trial) of documents from
N . For each lexical unit w ? W , there will be a cor-
responding set of outcomes (i.e. events) from the
draw. There will be three basic sets which are of
interest to us:
Definition 2 Basic events corresponding to each
w ? W :
? X is the event that ax occurs in the document
? Y is the event that ay occurs in the document
? S is the event that s occurs in the document
It should be obvious to the readers that since the doc-
uments in S have to contain all two units ax and ay,
S is a subset of X ? Y or S ? X ? Y . It is worth
noting that even though S ? X ? Y , it is highly
unlikely that S = X ? Y since the two portions
ax and ay may exist in the same document without
being conjoined by b. Next, subscribing to the fre-
quency interpretation of probability, we can obtain
the probability of the events in Definition 2 in terms
of Google page count:
P (X) = nx|N | (4)
P (Y ) = ny|N |
P (S) = ns|N |
where nx, ny and ns is the page count returned as
the result of Google search using the term [+?ax?],
[+?ay?] and [+?s?], respectively. The pair of
quotes that encapsulates the search terms is the
phrase operator, while the character ?+? is the re-
quired operator supported by the Google search en-
gine. As discussed earlier, the independence as-
sumption required by certain information-theoretic
measures and other Bayesian approaches may not al-
ways be valid, especially when we are dealing with
linguistics. As such, P (X ? Y ) 6= P (X)P (Y )
since the occurrences of ax and ay in documents are
inevitably governed by some hidden variables and
hence, not independent. Following this, we define
the probabilities for two new sets which result from
applying some set operations on the basic events in
Definition 2:
P (X ? Y ) = nxy|N | (5)
P (X ? Y \ S) = P (X ? Y )? P (S)
where nxy is the page count returned by Google
for the search using [+?ax? +?ay?]. Defining
P (X?Y ) in terms of observable page counts, rather
than a combination of two independent events will
allow us to avoid any unnecessary assumption of in-
dependence.
Next, referring back to our main problem dis-
cussed in Definition 1, we are required to estimate
the strength of collocation of the two units ax and
ay. Since there is no standard metric for such mea-
surement, we propose to address the problem from
a probabilistic perspective. We introduce the proba-
bility that s is a stable lexical unit given the evidence
s possesses:
Definition 3 Probability of unithood:
P (U |E) = P (E|U)P (U)P (E)
where U is the event that s is a stable lexical unit
and E is the evidences belonging to s. P (U |E) is
the posterior probability that s is a stable unit given
the evidence E. P (U) is the prior probability that s
is a unit without any evidence, and P (E) is the prior
probability of evidences held by s. As we shall see
later, these two prior probabilities will be immaterial
in the final computation of unithood. Since s can
either be a stable unit or not, we can state that,
P (U? |E) = 1? P (U |E) (6)
106
where U? is the event that s is not a stable lexical unit.
Since Odds = P/(1 ? P ), we multiply both sides
of Definition 3 by (1? P (U |E))?1 to obtain,
P (U |E)
1? P (U |E) =
P (E|U)P (U)
P (E)(1? P (U |E)) (7)
By substituting Equation 6 in Equation 7 and later,
applying the multiplication rule P (U? |E)P (E) =
P (E|U?)P (U?) to it, we will obtain:
P (U |E)
P (U? |E) =
P (E|U)P (U)
P (E|U?)P (U?) (8)
We proceed to take the log of the odds in Equation 8
(i.e. logit) to get:
log P (E|U)
P (E|U?) = log
P (U |E)
P (U? |E) ? log
P (U)
P (U?) (9)
While it is obvious that certain words tend to co-
occur more frequently than others (i.e. idioms and
collocations), such phenomena are largely arbitrary
(Smadja, 1993). This makes the task of deciding
on what constitutes an acceptable collocation dif-
ficult. The only way to objectively identify sta-
ble lexical units is through observations in samples
of the language (e.g. text corpus) (McKeown and
Radev, 2000). In other words, assigning the apri-
ori probability of collocational strength without em-
pirical evidence is both subjective and difficult. As
such, we are left with the option to assume that
the probability of s being a stable unit and not be-
ing a stable unit without evidence is the same (i.e.
P (U) = P (U?) = 0.5). As a result, the second term
in Equation 9 evaluates to 0:
log P (U |E)
P (U? |E) = log
P (E|U)
P (E|U?) (10)
We introduce a new measure for determining the
odds of s being a stable unit known as Odds of Unit-
hood (OU):
Definition 4 Odds of unithood
OU(s) = log P (E|U)
P (E|U?)
Assuming that the evidences in E are independent
of one another, we can evaluate OU(s) in terms of:
OU(s) = log
?
i P (ei|U)
?
i P (ei|U?)
(11)
=
?
i
log P (ei|U)
P (ei|U?)
(a) The area with darker
shade is the set X ? Y \ S.
Computing the ratio of P (S)
and the probability of this area
will give us the first evidence.
(b) The area with darker
shade is the set S?. Comput-
ing the ratio of P (S) and the
probability of this area (i.e.
P (S?) = 1? P (S)) will give
us the second evidence.
Figure 1: The probability of the areas with darker
shade are the denominators required by the evi-
dences e
1
and e
2
for the estimation of OU(s).
where ei are individual evidences possessed by s.
With the introduction of Definition 4, we can ex-
amine the degree of collocational strength of ax
and ay in forming s, mentioned in Definition 1 in
terms of OU(s). With the base of the log in Def-
inition 4 more than 1, the upper and lower bound
of OU(s) would be +? and ??, respectively.
OU(s) = +? and OU(s) = ?? corresponds to
the highest and the lowest degree of stability of the
two units ax and ay appearing as s, respectively. A
high1 OU(s) would indicate the suitability for the
two units ax and ay to be merged to form s. Ulti-
mately, we have reduced the vague problem of the
determination of unithood introduced in Definition
1 into a practical and computable solution in Defini-
tion 4. The evidences that we propose to employ for
determining unithood are based on the occurrences
of s, or the event S if the readers recall from Defini-
tion 2. We are interested in two types of occurrences
of s, namely, the occurrence of s given that ax and
ay have already occurred or X ? Y , and the occur-
rence of s as it is in our sample space, N . We refer
to the first evidence e
1
as local occurrence, while
the second one e
2
as global occurrence. We will
discuss the intuitive justification behind each type of
occurrences. Each evidence ei captures the occur-
rences of s within a different confinement. We will
estimate these evidences in terms of the elementary
probabilities already defined in Equations 4 and 5.
The first evidence e
1
captures the probability of
occurrences of s within the confinement of ax and ay
1A subjective issue that may be determined using a threshold
107
or X?Y . As such, P (e
1
|U) can be interpreted as the
probability of s occurring within X ? Y as a stable
unit or P (S|X ? Y ). On the other hand, P (e
1
|U?)
captures the probability of s occurring in X ? Y not
as a unit. In other words, P (e
1
|U?) is the probability
of s not occurring in X ? Y , or equivalently, equal
to P ((X ? Y \ S)|(X ? Y )). The set X ? Y \ S is
shown as the area with darker shade in Figure 1(a).
Let us define the odds based on the first evidence as:
OL =
P (e
1
|U)
P (e
1
|U?) (12)
Substituting P (e
1
|U) = P (S|X ? Y ) and
P (e
1
|U?) = P ((X ? Y \ S)|(X ? Y )) into Equa-
tion 12 will give us:
OL =
P (S|X ? Y )
P ((X ? Y \ S)|(X ? Y ))
= P (S ? (X ? Y ))P (X ? Y )
P (X ? Y )
P ((X ? Y \ S) ? (X ? Y ))
= P (S ? (X ? Y ))P ((X ? Y \ S) ? (X ? Y ))
and since S ? (X?Y ) and (X?Y \S) ? (X?Y ),
OL =
P (S)
P (X ? Y \ S) if(P (X ? Y \ S) 6= 0)
and OL = 1 if P (X ? Y \ S) = 0.
The second evidence e
2
captures the probability
of occurrences of s without confinement. If s is a
stable unit, then its probability of occurrence in the
sample space would simply be P (S). On the other
hand, if s occurs not as a unit, then its probability of
non-occurrence is 1?P (S). The complement of S,
which is the set S? is shown as the area with darker
shade in Figure 1(b). Let us define the odds based
on the second evidence as:
OG =
P (e
2
|U)
P (e
2
|U?) (13)
Substituting P (e
2
|U) = P (S) and P (e
2
|U?) = 1 ?
P (S) into Equation 13 will give us:
OG =
P (S)
1? P (S)
Intuitively, the first evidence attempts to capture
the extent to which the existence of the two lexical
units ax and ay is attributable to s. Referring back
to OL, whenever the denominator P (X ?Y \S) be-
comes less than P (S), we can deduce that ax and
ay actually exist together as s more than in other
forms. At one extreme when P (X ? Y \ S) = 0,
we can conclude that the co-occurrence of ax and
ay is exclusively for s. As such, we can also refer to
OL as a measure of exclusivity for the use of ax and
ay with respect to s. This first evidence is a good
indication for the unithood of s since the more the
existence of ax and ay is attributed to s, the stronger
the collocational strength of s becomes. Concerning
the second evidence, OG attempts to capture the ex-
tent to which s occurs in general usage (i.e. World
Wide Web). We can consider OG as a measure of
pervasiveness for the use of s. As s becomes more
widely used in text, the numerator in OG will in-
crease. This provides a good indication on the unit-
hood of s since the more s appears in usage, the like-
lier it becomes that s is a stable unit instead of an oc-
currence by chance when ax and ay are located next
to each other. As a result, the derivation of OU(s)
using OL and OG will ensure a comprehensive way
of determining unithood.
Finally, expanding OU(s) in Equation 11 using
Equations 12 and 13 will give us:
OU(s) = logOL + logOG (14)
= log P (S)P (X ? Y \ S) + log
P (S)
1? P (S)
As such, the decision on whether ax and ay should
be merged to form s can be made based solely on
the Odds of Unithood (OU) defined in Equation 14.
We will merge ax and ay if their odds of unithood
exceeds a certain threshold, OUT .
4 Evaluations and Discussions
For this evaluation, we employed 500 news arti-
cles from Reuters in the health domain gathered be-
tween December 2006 to May 2007. These 500 arti-
cles are fed into the Stanford Parser whose output is
then used by our head-driven left-right filter (Wong,
2005; Wong et al, 2007b) to extract word sequences
in the form of nouns and noun phrases. Pairs of word
sequences (i.e. ax and ay) located immediately next
to each other, or separated by a preposition or the
conjunction ?and? in the same sentence are mea-
108
sured for their unithood. Using the 500 news arti-
cles, we managed to obtain 1, 825 pairs of words to
be tested for unithood.
We performed a comparative study of our
new probabilistic approach against the empirically-
derived unithood function described in Equation 2.
Two experiments were conducted. In the first one,
we assessed our probabilistically-derived measure
OU(s) as described in Equation 14 where the de-
cisions on whether or not to merge the 1, 825 pairs
are done automatically. These decisions are known
as the actual results. At the same time, we inspected
the same list manually to decide on the merging of
all the pairs. These decisions are known as the ideal
results. The threshold OUT employed for our evalu-
ation is determined empirically through experiments
and is set to ?8.39. However, since only one thresh-
old is involved in deciding mergeability, training al-
gorithms and data sets may be employed to automat-
ically decide on an optimal number. This option is
beyond the scope of this paper. The actual and ideal
results for this first experiment are organised into
a contingency table (not shown here) for identify-
ing the true and the false positives, and the true and
the false negatives. In the second experiment, we
conducted the same assessment as carried out in the
first one but the decisions to merge the 1, 825 pairs
are based on the UH(ax, ay) function described in
Equation 2. The thresholds required for this func-
tion are based on the values suggested by (Wong et
al., 2007b), namely, MI+ = 0.9, MI? = 0.02,
IDT = 6, IDR+ = 1.35, and IDR? = 0.93.
Table 1: The performance of OU(s) (from Exper-
iment 1) and UH(ax, ay) (from Experiment 2) in
terms of precision, recall and accuracy. The last
column shows the difference in the performance of
Experiment 1 and 2.
Using the results from the contingency tables,
we computed the precision, recall and accuracy for
the two measures under evaluation. Table 1 sum-
marises the performance of OU(s) and UH(ax, ay)
in determining the unithood of 1, 825 pairs of lex-
ical units. One will notice that our new measure
OU(s) outperformed the empirically-derived func-
tion UH(ax, ay) in all aspects, with an improvement
of 2.63%, 3.33% and 2.74% for precision, recall and
accuracy, respectively. Our new measure achieved a
100% precision with a lower recall at 95.83%. As
with any measures that employ thresholds as a cut-
off point in accepting or rejecting certain decisions,
we can improve the recall of OU(s) by decreasing
the threshold OUT . In this way, there will be less
false negatives (i.e. pairs which are supposed to be
merged but are not) and hence, increases the recall
rate. Unfortunately, recall will improve at the ex-
pense of precision since the number of false pos-
itives will definitely increase from the existing 0.
Since our application (i.e. ontology learning) re-
quires perfect precision in determining the unithood
of word sequences, OU(s) is the ideal candidate.
Moreover, with only one threshold (i.e. OUT ) re-
quired in controlling the function of OU(s), we are
able to reduce the amount of time and effort spent
on optimising our results.
5 Conclusion and Future Work
In this paper, we highlighted the significance of unit-
hood and that its measurement should be given equal
attention by researchers in term extraction. We fo-
cused on the development of a new approach that
is independent of influences of termhood measure-
ment. We proposed a new probabilistically-derived
measure which provide a dedicated way to deter-
mine the unithood of word sequences. We refer to
this measure as the Odds of Unithood (OU). OU is
derived using Bayes Theorem and is founded upon
two evidences, namely, local occurrence and global
occurrence. Elementary probabilities estimated us-
ing page counts from the Google search engine are
utilised to quantify the two evidences. The new
probabilistically-derived measure OU is then eval-
uated against an existing empirical function known
as Unithood (UH). Our new measure OU achieved a
precision and a recall of 100% and 95.83% respec-
tively, with an accuracy at 97.26% in measuring the
unithood of 1, 825 test cases. OU outperformed UH
by 2.63%, 3.33% and 2.74% in terms of precision,
109
recall and accuracy, respectively. Moreover, our new
measure requires only one threshold, as compared to
five in UH to control the mergeability decision.
More work is required to establish the coverage
and the depth of the World Wide Web with regards
to the determination of unithood. While the Web has
demonstrated reasonable strength in handling gen-
eral news articles, we have yet to study its appropri-
ateness in dealing with unithood determination for
technical text (i.e. the depth of the Web). Similarly,
it remains a question the extent to which the Web
is able to satisfy the requirement of unithood deter-
mination for a wider range of genres (i.e. the cov-
erage of the Web). Studies on the effect of noises
(e.g. keyword spamming) and multiple word senses
on unithood determination using the Web is another
future research direction.
Acknowledgement
This research was supported by the Australian En-
deavour International Postgraduate Research Schol-
arship, and the Research Grant 2006 by the Univer-
sity of Western Australia.
References
K. Church and P. Hanks. 1990. Word association norms,
mutual information, and lexicography. Computational
Linguistics, 16(1):22?29.
T. Dunning. 1994. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
K. Frantzi. 1997. Incorporating context information for
the extraction of terms. In Proceedings of the 35th An-
nual Meeting on Association for Computational Lin-
guistics, Spain.
A. Franz. 1997. Independence assumptions considered
harmful. In Proceedings of the 8th Conference on Eu-
ropean Chapter of the Association for Computational
Linguistics, Madrid, Spain.
K. Kageura and B. Umino. 1996. Methods of automatic
term recognition: A review. Terminology, 3(2):259?
289.
F. Keller, M. Lapata, and O. Ourioupina. 2002. Using
the web to overcome data sparseness. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), Philadelphia.
C. Kit. 2002. Corpus tools for retrieving and deriving
termhood evidence. In Proceedings of the 5th East
Asia Forum of Terminology, Haikou, China.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In Proceedings of the 41st Meeting of the As-
sociation for Computational Linguistics.
D. Kurz and F. Xu. 2002. Text mining for the extrac-
tion of domain relevant terms and term collocations.
In Proceedings of the International Workshop on Com-
putational Approaches to Collocations, Vienna.
K. McKeown and D. Radev. 2000. Collocations. In
R. Dale, H. Moisl, and H. Somers, editors, Handbook
of Natural Language Processing. Marcel Dekker.
H. Nakagawa and T. Mori. 2002. A simple but powerful
automatic term extraction method. In Proceedings of
the International Conference On Computational Lin-
guistics (COLING).
V. Seretan, L. Nerima, and E. Wehrli. 2004. Using
the web as a corpus for the syntactic-based colloca-
tion identification. In Proceedings of the International
Conference on on Language Resources and Evaluation
(LREC), Lisbon, Portugal.
F. Smadja. 1993. Retrieving collocations from text:
Xtract. Computational Linguistics, 19(1):143?177.
W. Wong, W. Liu, and M. Bennamoun. 2007a. Deter-
mining termhood for learning domain ontologies in a
probabilistic framework. In Proceedings of the 6th
Australasian Conference on Data Mining (AusDM),
Gold Coast.
W. Wong, W. Liu, and M. Bennamoun. 2007b. Deter-
mining the unithood of word sequences using mutual
information and independence measure. In Proceed-
ings of the 10th Conference of the Pacific Associa-
tion for Computational Linguistics (PACLING), Mel-
bourne, Australia.
W. Wong. 2005. Practical approach to knowledge-
based question answering with natural language un-
derstanding and advanced reasoning. Master?s thesis,
National Technical University College of Malaysia,
arXiv:cs.CL/0707.3559.
110
