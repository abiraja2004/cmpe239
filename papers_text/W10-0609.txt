Proceedings of the NAACL HLT 2010 First Workshop on Computational Neurolinguistics, pages 70?78,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using fMRI activation to conceptual stimuli to evaluate methods for
extracting conceptual representations from corpora
Barry Devereux
Centre for Speech, Language and the Brain
Department of Experimental Psychology
University of Cambridge
barry@csl.psychol.cam.ac.uk
Colin Kelly & Anna Korhonen
Computer Laboratory
University of Cambridge
{ck329,alk23}@cam.ac.uk
Abstract
We present a series of methods for deriv-
ing conceptual representations from corpora
and investigate the usefulness of the fMRI
data and machine learning methodology of
Mitchell et al (2008) as a basis for evaluat-
ing the different models. Within this frame-
work, the quality of a semantic model is quan-
tified by its ability to predict the fMRI ac-
tivation associated with conceptual stimuli.
Mitchell et al used a manually-acquired set of
verbs as the basis for their semantic model; in
this paper, we also consider automatically ac-
quired feature-norm-like semantic representa-
tions. These models make different assump-
tions about the kinds of information avail-
able in corpora that is relevant to represent-
ing conceptual knowledge. Our results in-
dicate that automatically-acquired representa-
tions can make equally powerful predictions
about the brain activity associated with the
stimuli.
1 Introduction
Mitchell et al (2008) presented a novel approach for
predicting human brain activity associated with con-
ceptual stimuli. This approach represents a useful
development for interdisciplinary researchers inter-
ested in lexical semantics, for several reasons. Most
broadly, it is useful in testing the hypothesis that
distributional properties of words in corpora can re-
veal important information about the meanings of
words. A strong version of this hypothesis (i.e. that
children in part learn the meaning of concrete con-
cept words from co-occurring words in discourse
that they are exposed to) has formed the basis of
one class of probabilistic cognitive models of con-
ceptual representation (Andrews et al, 2005; An-
drews et al, 2009; Steyvers, 2010). Furthermore
this approach is useful for testing hypotheses about
the kind of co-occurring information that is useful
for representing conceptual semantics. In Mitchell
et al?s work (2008), for example, they adopt the po-
sition that the meaning of concrete concepts is en-
coded in the brain with information associated with
basic sensory and motor activities (such as actions
involving changes to spatial relationships and phys-
ical actions performed on objects).
At a more technical level, Mitchell et al?s fMRI
activation data1 give researchers developing feature-
based models of conceptual representation an im-
portant benchmark for evaluation. For these re-
searchers, a key problem is the lack of a reason-
able ?gold standard? against which the quality of the
representations generated by a computational model
may be evaluated. Previous research has adopted
two main approaches to evaluation. Firstly, some
models ? especially those aiming to extract repre-
sentations composed of psychologically meaningful
semantic feature units, such as Baroni et al (2009)
? have been evaluated against features gathered in
large scale property norming studies (e.g. McRae
et al (2005)).2 By comparing the system output
against features elicited by people, this kind of eval-
1fMRI data measures changes in oxygen concentrations in
the brain. These changes are tied to cognitive processes.
2In property norming studies, a group of human subjects are
asked to cite features which come to mind for a given concept.
These features are compiled by frequency (with a minimum fre-
quency cut-off) to generate a list of features for each concept.
70
uation aims to test the psychological validity of com-
putational methods. Furthermore, it allows a fine-
grained analysis of performance, for example by re-
vealing the classes of features (part-of, taxonomic,
etc) which a given model is particularly good at ex-
tracting (Baroni et al, 2008).
However, property norms come with important
caveats. One problem is that they tend to over-
represent informative or salient information about
concepts whilst under-representing other kinds of
features. For example, participants report that
camels have humps, but not that camels have hearts,
even though all participants are likely to have both
pieces of information accessible in their representa-
tion of the concept CAMEL. If a model is successful
in extracting these less salient features, there is no
way of evaluating their correctness using property
norms. A related issue is that participants can only
report verbalizable features, which may not repre-
sent the total sum of their conceptual knowledge
(Murphy, 2002; McRae et al, 2005).
A second problem with using property norms as
the basis of evaluation is that there is often no direct
lexical match between feature terms appearing in the
system output and the norms. Feature norms are typ-
ically normalized such that near-synonymous prop-
erties (e.g. is endangered, is an endangered species,
is almost extinct, etc., for WHALE) given by differ-
ent participants are mapped to the same feature la-
bel (e.g. is endangered). As a consequence, a model
may correctly extract endangered for WHALE, but
other lexical forms of the same feature will not
match any feature in the norms. One solution to this
is to create an expansion set for each feature which
includes its synonyms (Baroni et al, 2008). How-
ever, this is only a partial solution because lexical
variation in features is not limited to synonyms.
A second approach to evaluating semantic mod-
els uses classification or similarity data. For exam-
ple, Andrews et al (2009) evaluated their models by
calculating cosine similarity scores between seman-
tic representations and using these similarity scores
to predict behavioral data which are contingent on
the semantic similarity between pairs of concepts
(e.g. lexical substitution errors, semantic priming
latencies, word-association norms, etc). Although
this approach is psychologically motivated, it evalu-
ates a set of extracted features more indirectly than
comparison with norm data. In computational lin-
guistics, a similarly indirect evaluation method is to
cluster the extracted representations. This approach
avoids the difficulties in evaluating individual fea-
tures; however it only allows consideration along
one dimension of the data, namely the similarity be-
tween pairs of concepts.
fMRI data such as the Mitchell et al (2008)
dataset offers an advancement over both of these
evaluation techniques. Unlike, for example, prop-
erty norming data, fMRI data offers direct insight
into how the brain is functioning in response to given
stimuli. Its multidimensional nature makes it eas-
ier to inspect what aspects of meaning a particular
model is performing strongly or weakly on, and al-
lows for better control of experimental variation. Fi-
nally, it avoids the two major issues associated with
property norms, which we outlined above.
This paper is structured as follows. In the next
section, we briefly describe the models which we
used to extract conceptual representations for the 60
concepts in the Mitchell et al (2008) dataset. In
Section 3, we outline our experimental objectives,
and the framework we adopt for testing our seman-
tic models. In Section 4, we present the results of
our evaluation, which indicate above chance perfor-
mance for each of the models. Finally, we exam-
ine the differences between models by investigating
for which concepts prediction of the fMRI activity is
poorest, and discuss these differences with respect to
the differing assumptions made by the methods.
2 Semantic models
We consider four different semantic models in this
paper, which are described briefly below. These
models were selected as we were interested in the
various kinds of knowledge (part-of-speech, syntac-
tic, and semantic) in corpora available to the extrac-
tion process, and the extent to which the use of these
types of knowledge can affect the quality of the ex-
tracted conceptual representations.
2.1 Mitchell verb-based semantic model
The first semantic model we considered was that
of Mitchell et al (2008). This model assumes that
sensory-motor information is an important aspect of
conceptual representation, and that the information
71
relevant to a target concept?s representation can be
estimated from the concept word?s frequency of co-
occurrence with 25 sensory-motor verbs (eat, ma-
nipulate, push, etc) in a very large corpus. Our reim-
plementation of this method used the co-occurrence
statistics provided by Mitchell et al3 which were
extracted from the Google n-gram corpus consisting
of 1 trillion words of web text.
2.2 SVD model
Secondly, we implemented a co-occurrence-based
Singular Value Decomposition (SVD) model based
on the one described by Baroni and colleagues (Ba-
roni and Lenci, 2008; Baroni et al, 2009). This
model combines aspects of both the HAL (Landauer
et al, 1998) and LSA (Lund and Burgess, 1996)
models in constructing representations for words
based on their co-occurrences in texts. A word-
by-word co-occurrence matrix was constructed for
our corpus, storing how often each target word co-
occurred with each context word. The set of context
words consisted of the 5,000 most frequent content
words (i.e. words not occurring in a stop-list of func-
tion words) appearing in the corpus. The set of target
words consisted of the 60 concept terms appearing
in the fMRI dataset, supplemented with the 10,000
most frequent content words in the corpus (with the
exception of the top 10 most frequent words). For
calculating co-occurrence frequency between target
and context words, the context window was defined
by sentence boundaries: two words were considered
to co-occur if they appeared in the same sentence4.
Following Baroni and Lenci (2008), the dimen-
sionality of the target-word ? context-word co-
occurrence matrix was reduced to 150 columns by
singular value decomposition. That is, the singu-
lar value decomposition of the co-occurrence matrix
was computed and the 150 left singular vectors that
accounted for most of the variance, multiplied by the
corresponding singular values, were used as the 150-
dimensional representation of each target term. Sim-
3http://www.cs.cmu.edu/?tom/science2008/
semanticFeatureVectors.html
4In Baroni et al?s implementation a context window of 5
(Baroni and Lenci, 2008) or 20 (Baroni et al, 2009) words
either side of the target word was used instead; we chose a
sentence-based context window as it is analogous to the context
used in our experimental method (described in the following
section).
ilarity between pairs of target words was calculated
as the cosine between their vectors, and for each of
the 60 concept words in the experimental stimuli we
chose the 200 most similar target words to act as the
feature terms extracted by the model. The corpus
used with this model was the British National Cor-
pus (BNC) (Leech et al, 1994).
2.3 Novel extraction method
Finally we implemented a novel extraction method,
which aims to extract property-norm-like, psy-
chologically meaningful features from corpus data
(Kelly et al, 2010). The method aims to extract se-
mantically unconstrained feature triples of the form
concept-relation-feature , where feature is a feature
(either noun or adjective) of the target concept and
relation is a verb representing the semantic relation-
ship between them. Examples of extracted triples
include: swan be white, swan have neck and screw-
driver be tool. The model uses a corpus parsed for
grammatical relations (GRs) using Robust Accurate
Statistical Parsing (RASP) (Briscoe et al, 2006).
For each sentence containing a target concept, the
set of GRs for that sentence are examined to test
whether they match manually-created rules. These
rules include prototypical feature-relation GR struc-
tures connecting elements of the sentence and rep-
resent dependency patterns which encode potential
semantic relationships between the concept and can-
didate feature terms occurring in the sentence. A
large set of candidate triples are extracted by ap-
plying these rules to each sentence in the corpus
containing a target concept, and the triples for each
concept are ranked by their frequency of extraction.
In the second stage of the method, the extracted
triples are reweighted on the basis of probabilistic
high-level semantic information obtained from hu-
man property norm data. This subsequent stage has
the effect of increasing the weight associated with
more high-quality features and downgrading lower-
quality features. The extraction method is described
more fully in Kelly et al (2010). For this method
we also used the BNC. The top 200 triples ranked
by frequency (i.e. unweighted) and the top 200 fea-
tures after reweighting with the semantic data were
used in our experiments.
72
3 Experiment
As mentioned above, we are primarily interested in
using the fMRI data to evaluate the quality of the
different methods for extracting conceptual repre-
sentations from corpora (rather than being interested
in investigating methods for predicting fMRI activa-
tion). We make no attempt to build on the method
described by Mitchell et al (2008), although there
are likely to be many interesting avenues through
which that method could be extended.5 We therefore
followed the Mitchell et al methodology as closely
as possible, using the same multiple regression train-
ing and leave-two-out cross-validation paradigms as
presented in their paper and supporting online ma-
terial. The only parameter that we varied was the
extraction method (and corpus) that was used to gen-
erate the feature-vectors associated with the 60 con-
cepts that were used during the training phase. The
quality of the predictions generated for the concepts
using each semantic model can therefore be adopted
as an index of model performance.
The Mitchell et al method uses co-occurrence
with a specific set of 25 manually selected verbs
(eat, push, etc) that are the same for each concept.
This results in 25-dimensional feature vectors for in-
put into training. However, for both the SVD model
and our triple extraction models there are no a pri-
ori constraints on the number of unique features that
can be extracted for the concepts. For these mod-
els, we selected the top 200 features associated with
each concept; therefore, across all 60 concepts in the
Mitchell et al dataset, there are thousands of unique
features extracted which are used in the concepts?
representations. To ensure that the linear regres-
sion model for each method would be fitted using
the same number of free parameters during training
(thereby maximizing the comparability of the dif-
ferent methods), we reduced the dimensionality of
the generated feature spaces for the SVD method
and the two triple-extraction methods using Prin-
cipal Components Analysis (PCA). The concept ?
feature extraction frequency matrices for the three
models were submitted to PCA, and the first 25 com-
ponents (i.e. those components which best charac-
5For example, the method currently makes the simplifying
assumption that the activity in neighbouring voxels is indepen-
dent.
Triples (weighted) SVD
PCA1 PCA2 PCA1 PCA2
Highest-valued concepts
horse house coat butterfly
cat apartment skirt cow
cow dog shirt ant
dog igloo pants bee
beetle car dress lettuce
Lowest-valued concepts
knife pants car desk
door coat watch arm
hammer dress horse chair
saw skirt dog knife
chisel shirt fly leg
Table 1: Highest- and lowest-valued concepts for the
first two components for the SVD and weighted triple-
extraction methods.
terized the variance of the original features) for each
model were selected. In the case of the SVD model,
these 25 dimensions explained 77.7% of the vari-
ance in the original 3,061-dimensional vectors. For
our unweighted extraction method, the 25 extracted
components explained 63.0% of the original 5,525
dimensions; for the weighted method the compo-
nents explained 71.5% of the original 6,567 dimen-
sions.
It is interesting to consider the kind of seman-
tic information that is being captured by the resul-
tant PCA components. In particular, the compo-
nents appear to capture meaningful distinctions be-
tween stimuli. For example, the first PCA com-
ponent for our weighted triple extraction method
can be interpreted as the concepts? degree of ?an-
imalness? (animal stimuli have high values on this
component). Table 1 presents the five highest and
lowest-valued concepts for the first two components
for the SVD model and the weighted triple extrac-
tion model. Concepts which overlap with respect
to a specific set of semantic properties tend to have
high or low values on a given dimension, indicating
that that component is capturing a specific cluster of
co-occurring semantic features. For example, PCA1
for SVD can be interpreted as ?has features associ-
ated with clothing?.
Therefore, a key difference between the Michell
73
Method Feature Type POS Syntax Semantics
Mitchell 25 verbs no no no
SVD tuples (content-words) yes no no
triple-extraction method (unweighted) feature-triples yes yes no
triple-extraction method (weighted) feature-triples yes yes yes
Table 2: Comparison of the information available to each model.
et al model and our models is that while Mitchell
et al posit that certain sensory-motor function verbs
can act as important features of concepts, our models
instead place more importance on intrinsic semantic
features.
Finally, Table 2 gives a summary comparison of
the different models, in terms of whether or not each
uses part of speech (POS) data, syntactic informa-
tion (i.e. GRs), and semantic filtering (Section 2.3).
It should be noted that the BNC corpus (used with
the SVD model and our triple-extraction method) is
10,000 times smaller than the corpus from which
the Mitchell et al feature vectors are derived. As
such the semantic representations we extract with
our method need to make better use of the data avail-
able in the corpus if they are to compete with the
verb-based features used by Mitchell et al?s method.
4 Results
The accuracy for each of the four methods was eval-
uated using a leave-two-out validation paradigm.
There are 1,770 possible pairs of concepts that can
be drawn from the set of 60 concept stimuli. Train-
ing was performed separately for each participant
and for each of the 1,770 held-out pairs. Given
a particular participant and held-out pair, for each
voxel v we fit the activation at that voxel to the set
of 58 training items with multiple linear regression,
using as predictor variables the elements of the 25-
dimensional feature vectors associated with each of
the 58 concepts. Training therefore yields a set of
25 ?-coefficients, which can be used to generate a
prediction for the activation yv of voxel v for the
held-out word w using the equation
ypredv =
25?
i=1
?v,ifi,w (1)
where fi,w is the ith element of the feature vector for
word w (see Mitchell et al (2008) for details). Over
all voxels, this method gives a prediction for the ac-
tivation with respect to the held-out word w which
can then be compared to the observed activation for
that stimulus.
Rather than comparing the activity between pre-
dicted and observed images using all voxels, we
compared images using only the 500 most stable
voxels for each participant. For each participant, the
500 most stable voxels were the voxels which gave
the most consistent pattern of activation across the
six presentations of all 60 stimuli (see Mitchell et al
(2008) for details).
The top row of Figure 1 presents the learned co-
efficients for one feature dimension for each of the
four semantic models considered in our experiments
(for these images, all voxels rather then the 500
most stable voxels are used). For the Mitchell et al
method, the coefficients presented correspond to the
verb eat; for the other models the feature is the PCA
component that explained the most variance in the
original representations. We also present the pre-
dicted images for the concepts CELERY and AIR-
PLANE, calculated on the coefficients learned over
the remaining 58 concepts. Importantly, for the
Mitchell et al method (column (a)), the learned co-
efficients for eat and the predicted images for CEL-
ERY and AIRPLANE agree with those reported by
Mitchell et al (2008, Figure 2 & online supplemen-
tary material6).
We calculated similarity between predicted and
observed images using both cosine and Pearson cor-
relation and the 500 most stable voxels; we report
the results using Pearson correlation here as this
measure consistently gave slightly better accuracies
6http://www.cs.cmu.edu/?tom/science2008/
featureSignaturesP1.html
74
(a) ?eat? (b) PCA1/clothes (c) PCA1/clothes (d) PCA1/animals
(a) celery (b) celery (c) celery (d) celery
(a) airplane (b) airplane (c) airplane (d) airplane
Figure 1: Learned coefficients on a selected feature dimension (top row) and predicted activation for CELERY (middle
row) and AIRPLANE (bottom row) for four semantic models: (a) Mitchell et al (2008), (b) SVD (c) triple extraction
method (unweighted), and (d) triple extraction method (weighted). Warmer colours indicate higher values (i.e. larger
?-coefficients for the feature dimensions and higher predicted activation for the concepts). PCA components have been
given intuitive labels indicating the kind of information described by that component (see Table 1). As in Figure 2 of
Mitchell et al (2008), the figure shows just one slice in the horizontal plane (z = -12 in MNI space) for one participant
(P1). The predicted images for CELERY and AIRPLANE were generated from the feature coefficients learned on the
other 58 concepts using each of the four models; the corresponding observed images for CELERY and AIRPLANE can
be found in Mitchell et al (2008) Figure 2 B.
75
Method P1 P2 P3 P4 P5 P6 P7 P8 P9 Mean
Mitchell et al (2008) 0.84 0.83 0.76 0.81 0.79 0.66 0.73 0.64 0.68 0.75
SVD 0.82 0.67 0.79 0.83 0.74 0.64 0.64 0.70 0.75 0.73
Triple-extraction (unweighted) 0.82 0.71 0.79 0.80 0.70 0.69 0.65 0.53 0.78 0.72
Triple-extraction (weighted) 0.82 0.72 0.76 0.83 0.73 0.65 0.68 0.51 0.76 0.72
Table 3: Accuracy results for the four semantic models.
for each of the four models (the results are very simi-
lar using the cosine measure). Following Mitchell et
al. (2008; supplementary material), a match score
for each held out pair w1 and w2 was calculated
as the sum of the similarities between the correctly
aligned predicted and observed images:
a = sim(wpred1 , w
obs
1 ) + sim(w
pred
2 , w
obs
2 ) (2)
Similarly a mismatch score was calculated as
b = sim(wpred1 , w
obs
2 ) + sim(w
pred
2 , w
obs
1 ) (3)
Cases where the match score is greater than the mis-
match score (i.e. a > b) count as successes for the
model (i.e. the model correctly identifies the two
predicted images). Otherwise there is a failure by
the model (i.e. the model identifies the observed im-
age for w1 as being w2 and vice-versa).
Table 3 presents the results of the leave-two-
out cross-validation evaluation, giving the propor-
tion (across all 1,770 pairs) of predicted images for
the held-out pairs that were correctly matched to
the observed images.7 The original Mitchell et al
(2008) model has the best mean performance, al-
though across the nine participants, there is no sig-
nificant difference in accuracy between any of the
models (|t(8)| < 1.49, p > 0.17, for all pairwise
paired t-tests between Mitchell et al (2008), SVD,
and weighted triple extraction).
That there is no difference between the perfor-
mance of the Mitchell et al (2008), SVD and triple
7Our results for the Mitchell et al (2008) method are simi-
lar, though not identical, to those reported in that paper (where
the reported mean accuracy across all participants is 0.77, using
cosine similarity). Our implementation of the method for select-
ing the 500 most stable voxels yields slightly different voxels
from those obtained by Mitchell et al (2008; see supplemen-
tary material). In any case, the same set of 500 voxels for each
participant were used for generating the results of each model
presented here, and so we do not believe that this discrepancy
affects comparison of the different models.
extraction methods is surprising, given the different
kinds of information that are available to the dif-
ferent models. In particular, the models that auto-
matically acquire very general and semantically un-
constrained feature-based representations perform
as well as the model which uses a set of manually-
selected sensory-motor verbs, even though the rep-
resentations generated for these models are derived
from 10,000 times less corpus data.
As mentioned in our introduction, an advantage of
evaluating against the fMRI dataset is that this multi-
dimensional data allows us to investigate strengths
and weaknesses of different models in a way which
is not possible using similarity or clustering-based
evaluation. As a very simple investigation of spe-
cific differences in model performance, we present
in Table 4 the pairs of concepts for which each of the
models performs most poorly on. The Mitchell et al
(2008) method appears to do poorly on pairs of con-
cepts where a constituent word can be ambiguous
with respect to its part-of-speech (e.g. SAW, BEAR).
This is not surprising, given that part-of-speech data
is not available in the Google n-gram corpus used
with this method. The performance of the Mitchell
et al method might therefore be improved signifi-
cantly by applying heuristics to the n-gram data to
make inferences about the correct part-of-speech of
instances of words like SAW and BEAR. For the SVD
and weighted triple extraction methods, which both
use the BNC corpus, there is some evidence that
the models are performing poorly for relatively low
frequency words8 (e.g. CHISEL), words which are
semantically ambiguous as nouns (e.g. ARM), and
pairs which are semantically similar (e.g. SPOON &
KNIFE). This suggests that the SVD and triple ex-
traction methods may perform better with a larger
and more diverse corpus.
8AIRPLANE is relatively low frequency in the BNC; it may
be more sensible to use the word AEROPLANE with a British
corpus.
76
Mitchell et al SVD Triple Extraction (weighted)
Pair Nr. Pair Nr. Pair Nr.
bear saw 0 cup airplane 0 dresser chimney 0
bell carrot 0 cup lettuce 0 airplane chisel 0
bell saw 0 horse beetle 0 airplane hand 0
knife bear 0 chisel arm 0 airplane tomato 0
cup saw 1 hammer arm 1 spoon chisel 0
bear tomato 1 dresser arch 1 spoon knife 0
Table 4: Leave-out pairs for which each model performs least accurately, across the nine participants. Nr. = the number
of participants for which this leave-out pair was correctly matched.
5 Conclusion
The fMRI dataset and training and evaluation
methodology presented by Mitchell et al (2008)
gives researchers an interesting new framework with
which to evaluate the quality of feature-based con-
ceptual representations extracted from corpora. This
framework avoids some of the problems inherent in
evaluating extracted representations against a ?gold
standard? based on participant-generated property
norms. It also provides a rich multi-dimensional
dataset through which the strengths and weaknesses
of extraction methods can be identified.
We have applied this evaluation framework to
four feature extraction methods which use different
sources of information available in corpora to extract
conceptual representations. Surprisingly, in spite of
their major differences, we did not find any signifi-
cant difference in performance between the models.
This finding has interesting theoretical implica-
tions, given that previous research has suggested
that aspects of meaning defined by sensory-motor
verbs may have a somewhat distinctive role to play
in predicting the fMRI activation associated with
conceptual stimuli (Mitchell et al, 2008). Our re-
sults suggest that general feature-based representa-
tions of concepts, which place no a priori distinc-
tion on sensory-motor properties, may be equally
capable of predicting activation to conceptual stim-
uli. This highlights the potential for the Mitchell
et al method to be used to inform both distributed
and sensory-motor accounts of conceptual represen-
tation (e.g. McRae et al (1997), Cree et al (2006),
Tyler et al (2000), Tyler & Moss (2001), Moss et
al. (2007), Martin & Chao (2001)), as well as pro-
viding a benchmark with which to assess semantic
model development. In a similar vein, Murphy et
al. (2009) used a dependency-parsed corpus yielding
verb co-occurrence statistics to predict EEG9 activa-
tion patterns with significant accuracy.
The training and evaluation framework presented
by Mitchell et al (2008) represents just one point
in a large space of possibilities for using computa-
tional modelling to predict human brain activity as-
sociated with conceptual stimuli. In these initial ex-
periments, we have chosen to follow the Mitchell
et al approach as closely as possible, in order to
maximize comparability with their results. In future
work, we aim to investigate other methods for train-
ing and evaluation, other corpora and other sources
of imaging data. Furthermore, we aim to use the
evaluation results from such work to inform the de-
velopment of our extraction method.
Acknowledgments
Our work was funded by the EPSRC grant
EP/F030061/1, and the Royal Society University
Research Fellowship, UK. We thank Mitchell et
al. (2008) and McRae et al (2005) for making their
data publically available.
References
Mark Andrews, G. Vigliocco, and D. Vinson. 2005.
Integrating attributional and distributional informa-
tion in a probabilistic model of meaning representa-
tion. In Timo Honkela et al, editor, Proceedings of
AKRR?05, International and Interdisciplinary Confer-
ence on Adaptive Knowledge Representation and Rea-
9EEG measures voltages induced by neuronal firing across
the human scalp.
77
soning, pages 15?25, Espoo, Finland: Helsinki Uni-
versity of Technology.
Mark Andrews, Gabriella Vigliocco, and David Vinson.
2009. Integrating experiential and distributional data
to learn semantic representations. Psychological Re-
view, 116(3):463?498.
Marco Baroni and Alessandro Lenci. 2008. Concepts
and properties in word spaces. From context to mean-
ing: Distributional models of the lexicon in linguis-
tics and cognitive science (Special issue of the Italian
Journal of Linguistics), 20(1):55?88.
Marco Baroni, Stefan Evert, and Alessandro Lenci, edi-
tors. 2008. ESSLLI 2008 Workshop on Distributional
Lexical Semantics.
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2009. Strudel: A corpus-based semantic
model based on properties and types. Cognitive Sci-
ence, pages 1?33.
E. Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the Interactive Demo Session of COLING/ACL-
06, pages 77?80.
George S. Cree, Chris McNorgan, and Ken McRae.
2006. Distinctive features hold a privileged status
in the computation of word meaning: Implications
for theories of semantic memory. Journal of Experi-
mental Psychology. Learning, Memory, and Cognition,
32(4):643?58.
Colin Kelly, Barry Devereux, and Anna Korhonen. 2010.
Acquiring human-like feature-based conceptual repre-
sentations from corpora. In Brian Murphy, Kai min
Kevin Chang, and Anna Korhonen, editors, Proceed-
ings of the NAACL-HLT Workshop on Computational
Neurolinguistics, Los Angeles, USA.
T.K. Landauer, P.W. Foltz, and D. Laham. 1998. An in-
troduction to latent semantic analysis. Discourse Pro-
cesses, 25:259?284.
G. Leech, R. Garside, and M. Bryant. 1994. CLAWS4:
the tagging of the British National Corpus. In Pro-
ceedings of the 15th conference on Computational
linguistics-Volume 1, pages 622?628. Association for
Computational Linguistics.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments,
and Computers, 28(2):203?208.
Alex Martin and Linda L. Chao. 2001. Semantic mem-
ory and the brain: structure and processes. Current
Opinion in Neurobiology, 11(2):194?201.
Ken McRae, Virginia R. de Sa, and Mark S. Seidenberg.
1997. On the nature and scope of featural representa-
tions of word meaning. Journal of Experimental Psy-
chology: General, 126(2):99?130.
Ken McRae, George S. Cree, Mark S. Seidenberg, and
Chris McNorgan. 2005. Semantic feature production
norms for a large set of living and nonliving things.
Behavior Research Methods, 37:547?559.
Tom M. Mitchell, Svetlana V. Shinkareva, Andrew Carl-
son, Kai-Min Chang, Vicente L. Malave, Robert A.
Mason, and Marcel A. Just. 2008. Predicting human
brain activity associated with the meanings of nouns.
Science, 320(5880):1191?1195.
Helen E. Moss, Lorraine K. Tyler, and Kirsten I. Taylor.
2007. Conceptual structure. In M. Gareth Gaskell, ed-
itor, The Oxford handbook of psycholinguistics, pages
217?234. Oxford University Press, Oxford, UK.
B. Murphy, M. Baroni, and M. Poesio. 2009. Eeg re-
sponds to conceptual stimuli and corpus semantics.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP 2009),
pages 619?627, East Stroudsburg, PA.
Gregory Murphy. 2002. The big book of concepts. The
MIT Press, Cambridge, MA.
Mark Steyvers. 2010. Combining feature norms and
text data with topic models. Acta Psychologica,
133(3):234?243.
Lorraine K. Tyler and Helen E. Moss. 2001. Towards a
distributed account of conceptual knowledge. Trends
in Cognitive Sciences, 5(6):244?252.
L. K. Tyler, H. E. Moss, M. R. Durrant-Peatfield, and J. P.
Levy. 2000. Conceptual structure and the structure of
concepts: A distributed account of category-specific
deficits. Brain and Language, 75(2):195?231.
78
