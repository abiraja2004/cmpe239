Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 53?56,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Transliteration Mining with Phonetic Conflation and Iterative Training 
Kareem Darwish 
Cairo Microsoft Innovation Center 
Cairo, Egypt 
kareemd@microsoft.com 
Abstract 
This paper presents transliteration mining on the 
ACL 2010 NEWS workshop shared translitera-
tion mining task data.  Transliteration mining 
was done using a generative transliteration model 
applied on the source language and whose output 
was constrained on the words in the target lan-
guage.  A total of 30 runs were performed on 5 
language pairs, with 6 runs for each language 
pair.  In the presence of limited resources, the 
runs explored the use of phonetic conflation and 
iterative training of the transliteration model to 
improve recall.  Using letter conflation improved 
recall by as much as 48%, with improvements in 
recall dwarfing drops in precision.  Using itera-
tive training improved recall, but often at the cost 
of significant drops in precision.  The best runs 
typically used both letter conflation and iterative 
learning. 
1 Introduction 
Transliteration Mining (TM) is the process of find-
ing transliterated word pairs in parallel or compa-
rable corpora.  TM has many potential applications 
such as building training data for training translit-
erators and improving lexical coverage for machine 
translation and cross language search via transla-
tion resource expansion.  TM has been gaining 
some attention of late with a shared task in the 
ACL 2010 NEWS workshop
1
.  In this paper, TM 
was performed using a transliterator that was used 
to generate possible transliterations of a word while 
constraining the output to tokens that exist in a tar-
get language word sequence.  The paper presents 
the use of phonetic letter conflation and iterative 
transliterator training to improve TM when only 
limited transliteration training data is available.  
For phonetic letter conflation, a variant of 
SOUNDEX (Russell, 1918) was used to improve 
the coverage of existing training data.  As for itera-
tive transliterator training, an initial transliterator, 
which was trained on initial set of transliteration 
pairs, was used to mine transliterations in parallel 
text.  Then, the automatically found transliterations 
pairs were considered correct and were used to re-
train the transliterator. 
                                                 
1 http://translit.i2r.a-star.edu.sg/news2010/  
The proposed improvements in TM were tested 
using the ACL 2010 NEWS workshop data for Ar-
abic, English-Chinese, English-Hindi, English-
Russian, and English-Tamil.  For language pair, a 
base set of 1,000 transliteration pairs were available 
for training. 
The rest of the paper is organized as follows:  Sec-
tion 2 surveys prior work on transliteration mining; 
Section 3 describes the TM approach and the pro-
posed improvements; Section 4 describes the ex-
perimental setup including the evaluation sets; Sec-
tion 5 reports on experimental results; and Section 
6 concludes the paper. 
2 Background  
Much work has been done on TM for different lan-
guage pairs such as English-Chinese (Kuo et al, 
2006; Kuo et al, 2007; Kuo et al, 2008; Jin et al 
2008;), English-Tamil (Saravanan and Kumaran, 
2008; Udupa and Khapra, 2010), English-Korean 
(Oh and Isahara, 2006; Oh and Choi, 2006), Eng-
lish-Japanese (Brill et al, 2001; Oh and Isahara, 
2006), English-Hindi (Fei et al, 2003; Mahesh and 
Sinha, 2009), and English-Russian (Klementiev 
and Roth, 2006).  The most common approach for 
determining letter sequence mapping between two 
languages is using automatic letter alignment of a 
training set of transliteration pairs.  Automatic 
alignment can be performed using different algo-
rithms such as the EM algorithm (Kuo et al, 2008; 
Lee and Chang, 2003) or using an HMM aligner 
(Udupa et al, 2009a; Udupa et al, 2009b).  Anoth-
er method is to use automatic speech recognition 
confusion tables to extract phonetically equivalent 
character sequences to discover monolingual and 
cross lingual pronunciation variations (Kuo and 
Yang, 2005).  Alternatively, letters can be mapped 
into a common character set.  One example of that 
is to use a predefined transliteration scheme to 
transliterate a word in one character set into another 
character set (Oh and Choi, 2006).  Different meth-
ods were proposed to ascertain if two words can be 
transliterations of each other.  One such way is to 
use a generative model that attempts to generate 
possible transliterations given the character map-
pings between two character sets (Fei et al, 2003; 
Lee and Chang, 2003, Udupa et al, 2009a).  A sim-
ilar alternative is to use back-transliteration to de-
53
termine if one sequence could have been generated 
by successively mapping character sequences from 
one language into another (Brill et al, 2001; Bilac 
and Tanaka, 2005; Oh and Isahara, 2006).  Another 
mapping method is to map candidate translitera-
tions into a common representation space (Udupa 
et al, 2010).  When using a predefined translitera-
tion scheme, edit distance was used to determine if 
candidate transliterations were indeed translitera-
tions (Oh and Choi, 2006).  Also letter conflation 
was used to find transliterations (Mahesh and Sin-
ha, 2009).  Different methods were proposed to 
improve the recall of mining.  For example, Oh and 
Choi (2006) used a SOUNDEX like scheme to 
minimize the effect of vowels and different 
schemes of phonetically coding names.  
SOUNDEX is used to convert English words into a 
simplified phonetic representation, in which vowels 
are removed and phonetically similar characters are 
conflated. Another method involved expanding 
character sequence maps by automatically mining 
transliteration pairs and then aligning these pairs to 
generate an expanded set of character sequence 
maps (Fei et al, 2003). 
3 Transliteration Mining 
TM proposed in this paper uses a generative trans-
literation model, which is trained on a set of trans-
literation pairs.  The training involved automatical-
ly aligning character sequences.  SOUNDEX like 
letter conflation and iterative transliterator training 
was used to improve recall.  Akin to phrasal align-
ment in machine translation, character sequence 
alignment was treated as a word alignment problem 
between parallel sentences, where transliterations 
were treated as if they were sentences and the char-
acters from which they were composed were treat-
ed as if they were words.  The alignment was per-
formed using a Bayesian learner that trained on 
word dependent transition models for HMM based 
word alignment (He, 2007).  Alignment produced a 
mapping of source character sequence to a target 
character sequence along with the probability of 
source given target. 
For all the work reported herein, given an English- 
foreign language transliteration candidate pair, 
English was treated as the target language and the 
foreign language as the source.  Given a foreign 
source language word sequence   
  and an English 
target word sequence   
 ,      
   is a potential 
transliteration of      
 .  Given Fi, composed of 
the character sequence f1 ? fo, and Ej, composed of 
the character sequence e1 ? ep, P(Fi|Ej) is calculat-
ed using the trained model, as follows: 
 (  |  )  ?                 
         
 
The non-overlapping segments fx ? fy are generated 
by finding all possible 2
n-1
 segmentations of the 
word Fi.  For example, given ?man? then all pos-
sible segmentations are (m,a,n), (ma,n), (m,an), and 
(man).  The segmentation producing the highest 
probability is chosen.  All segment sequences e?k ... 
e?l known to produce fx ? fy for each of the possible 
segmentations are produced.  If a set of non-
overlapping sequences of e?k ... e?l generates the 
sequence e1 ? ep (word      
 ), then Ej is con-
sidered a transliteration of Fi.  If multiple target 
words have P(Fi|Ej) > 0, then Ej that maximizes 
P(Fi|Ej) is taken as the proper transliteration.  A 
suffix tree containing   
  was used to constrain 
generation, improving efficiency.  No smoothing 
was used. 
To improve recall, a variant of SOUNDEX was 
used on the English targets.  The original 
SOUNDEX scheme applies the following rules: 
1. Retain the first letter in a word 
2. Remove all vowels, H, and W 
3. Perform the following mappings: 
B, F, P, V ? 1 C, G, J, K, Q, S, X, Z ? 2 
D,T ? 3 L ? 4 
M,N ? 5 R ? 6 
4. Trim all result sequences to 4 characters 
5. Pad short sequences with zeros to have exactly 
4 characters. 
SOUNDEX was modified as follows: 
1. The first letter in a word was not retained and 
was changed according the mapping in step 3 
of SOUNDEX. 
2. Resultant sequences longer than 4 characters 
were not trimmed. 
3. Short resultant strings were not padded with 
zeros. 
SOUNDEX after the aforementioned modifications 
is referred at S-mod.  Alignment was performed 
between transliteration pairs where English words 
were replaced with their S-mod representation.  
Case folding was always applied to English. 
Iterative transliterator training involved training a 
transliterator using an initial seed of transliteration 
pairs, which was used to automatically mine trans-
literations from a large set of parallel words se-
quences.  Automatically mined transliteration pairs 
were assumed to be correct and were used to retrain 
the transliterator.  S-mod and iterative training were 
used in isolation or in combination as is shown in 
the next section. 
Russian and Arabic were preprocessed as follows: 
? Russian: characters were case-folded 
? Arabic: the different forms of alef (alef, alef 
maad, alef with hamza on top, and alef with 
hamza below it) were normalized to alef, ya 
and alef maqsoura were normalized to ya, and 
ta marbouta was mapped to ha. 
54
No preprocessing was performed for the other lan-
guages.  Since Wikipedia English entries often had 
non-English characters, the following letter confla-
tions were performed: 
?, ? ? z                ?, ?, ?, ?, ?, ?, ?, ? ? a 
?, ?, ? ? e ?, ?, ? ? c 
? ? l ?, ?, ?, ? ? i 
?, ?, ?, ? ? o ?, ?, ? ? n 
?, ?, ?, ? ? s ? ? r 
? ? y ?, ?, ?, ? ? u 
Language Pair # of Parallel Sequences 
English-Arabic 90,926 
English-Chinese 196,047 
English-Hindi 16,963 
English-Russian 345,969 
English-Tamil 13,883 
Table 1: Language pairs and no. of parallel sequences 
Run Precision Recall F-score 
1 0.900 0.796 0.845 
2  0.966 0.587 0.730 
3 0.952 0.588 0.727 
4  0.886 0.817 0.850 
5 0.895 0.678 0.771 
6  0.818 0.827 0.822 
Table 2: English-Arabic mining results 
Run Precision Recall F-score 
1 1.000 0.024 0.047 
2  1.000 0.016 0.032 
3 1.000 0.016 0.032 
4  1.000 0.026 0.050 
5 1.000 0.022 0.044 
6  1.000 0.030 0.059 
Table 3: English-Chinese mining results 
Run Precision Recall F-score 
1 0.959 0.786 0.864 
2  0.987 0.559 0.714 
3 0.984 0.569 0.721 
4  0.951 0.812 0.876 
5 0.981 0.687 0.808 
6  0.953 0.855 0.902 
Table 4: English-Hindi mining results 
Run Precision Recall F-score 
1 0.813 0.839 0.826 
2  0.868 0.748 0.804 
3 0.843 0.747 0.792 
4  0.716 0.868 0.785 
5 0.771 0.794 0.782 
6  0.673 0.881 0.763 
Table 5: English-Russian mining results 
Run Precision Recall F-score 
1 0.963 0.604 0.743 
2  0.976 0.407 0.575 
3 0.975 0.446 0.612 
4  0.952 0.668 0.785 
5 0.968 0.567 0.715 
6  0.939 0.741 0.828 
Table 6: English-Tamil mining results 
For each foreign language (F) and English (E) pair, 
a set of 6 runs were performed.  The first two runs 
involved training a transliterator using the 1,000 
transliteration pairs and using it for TM as in sec-
tion 3.  The runs were: 
Run 1:  align F with S-mod(E)  
Run 2:  align F with E  
The four other runs involved iterative training in 
which all automatically mined transliterations from 
Runs 1 and 2 were considered correct, and were 
used to retrain the transliterator.   The runs were: 
Run 3:  Use Run 2 output, align F with E 
Run 4:  Use Run 2 output, align F with S-mod(E) 
Run 5:  Use Run 1 output, align F with E 
Run 6:  Use Run 1 output, align F with S-mod(E) 
For evaluation, the system would mine translitera-
tions and a set of 1,000 parallel sequences were 
chosen randomly for evaluation.  The figures of 
merit are precision, recall, F1 measure. 
4 Experimental Setup 
The experiments were done on the ACL-2010 
NEWS Workshop TM shared task datasets.  The 
datasets cover 5 language pairs.  For each pair, a 
dataset includes a list of 1,000 transliterated words 
to train a transliterator, and list of parallel word 
sequences between both languages.  The parallel 
sequences were extracted parallel Wikipedia article 
titles for which cross language links exist between 
both languages.  Table 1 lists the language pairs 
and the number of the parallel word sequences. 
5 Experimental Results 
Tables 2, 3, 4, 5, and 6 report results for Arabic, 
Chinese, Hindi, Russian and Tamil respectively.  
As shown in Table 3, the recall for English-Chinese 
TM was dismal and suggests problems in experi-
mental setup.  This would require further investiga-
tion.  For the other 4 languages, the results show 
that not using S-mod and not using iterative train-
ing, as in Run 2, led to the highest precision.  Using 
both S-mod and iterative training, as in Run 6, led 
to the highest recall.   
In comparing Runs 1 and 2, where 1 uses S-mod 
and 2 does not, using S-mod led to 35.6%, 40.6%, 
12.2%, and 48.4% improvement in recall and to 
6.8%, 2.8%, 6.3%, and 1.3% decline in precision 
for Arabic, Chinese, Russian, and Tamil respective-
ly.  Except for Russian, the improvements in recall 
dwarf decline in precision, leading to overall im-
provements in F-measure for all 4 languages. 
In comparing runs 2 and 3 where iterative training 
is used, iterative training had marginal impact on 
precision and recall.  When using S-mod, compar-
ing run 6 where iterative training was performed 
over the output from run 1, recall increased by 
55
3.9%, 8.8%, 5.0%, and 22.7% for Arabic, Chinese, 
Russian, and Tamil respectively.  The drop in pre-
cision was 9.1% and 17.2% for Arabic and Russian 
respectively and marginal for Hindi and Tamil. 
Except for Russian, the best runs for all languages 
included the use of S-mod and iterative training.  
The best runs were 4 for Arabic and Hindi and 6 
for Tamil.  For Russian, the best runs involved us-
ing S-mod only without iterative training.  The 
drop in Russian could be attributed to the relatively 
large size of training data compared to the other 
languages (345,969 parallel word sequences). 
6 Conclusion  
This paper presented two methods for improving 
transliteration mining, namely phonetic conflation 
of letters and iterative training of a transliteration 
model.  The methods were tested using on the ACL 
2010 NEWS workshop shared transliteration min-
ing task data.  Phonetic conflation of letters in-
volved using a SOUNDEX like conflation scheme 
for English.  This led to much improved recall and 
general improvements in F-measure.  The iterative 
training of the transliteration model led to im-
proved recall, but recall improvements were often 
offset by decreases in precision.  However, the best 
experimental setups typically involved the use of 
both improvements. 
The success of phonetic conflation for English 
may indicate that similar success may be attained if 
phonetic conflation is applied to other languages.  
Further, the use of smoothing of the transliteration 
model may help improve recall.  The recall for 
transliteration mining between English and Chinese 
were dismal and require further investigation.  
References  
Slaven Bilac, Hozumi Tanaka. Extracting transliteration 
pairs from comparable corpora. NLP-2005, 2005.   
Eric Brill, Gary Kacmarcik, Chris Brockett. Automati-
cally harvesting Katakana-English term pairs from 
search engine query logs. NLPRS 2001, pages 393?
399, 2001.   
Huang Fei, Stephan Vogel, and Alex Waibel. 2003. Ex-
tracting Named Entity Translingual Equivalence with 
Limited Resources.  TALIP, 2(2):124?129.  
Xiaodong He, 2007. Using Word-Dependent Transition 
Models in HMM based Word Alignment for Statisti-
cal Machine Translation. ACL-07 2
nd
 SMT workshop. 
Chengguo Jin, Dong-Il Kim, Seung-Hoon Na, Jong-
Hyeok Lee.  2008.  Automatic Extraction of English-
Chinese Transliteration Pairs using Dynamic Window 
and Tokenizer.  Sixth SIGHAN Workshop on Chi-
nese Language Processing, 2008. 
Alexandre Klementiev and Dan Roth. 2006. Named En-
tity Transliteration and Discovery from Multilingual 
Comparable Corpora. HLT Conf. of the North Ameri-
can Chapter of the ACL, pages 82?88.  
Jin-Shea Kuo, Haizhou Li, Ying-Kuei Yang. 2006. 
Learning Transliteration Lexicons from the Web. 
COLING-ACL2006, Sydney, Australia, 1129 ? 1136. 
Jin-shea Kuo, Haizhou Li, Ying-kuei Yang.  A phonetic 
similarity model for automatic extraction of translit-
eration pairs. TALIP, 2007 
Jin-Shea Kuo, Haizhou Li, Chih-Lung Lin.  2008.  Min-
ing Transliterations from Web Query Results: An In-
cremental Approach.  Sixth SIGHAN Workshop on 
Chinese Language Processing, 2008.  
Jin-shea Kuo, Ying-kuei Yang. 2005.  Incorporating 
Pronunciation Variation into Extraction of Translit-
erated-term Pairs from Web Corpora.  Journal of Chi-
nese Language and Computing, 15 (1): (33-44). 
Chun-Jen Lee, Jason S. Chang. Acquisition of English-
Chinese transliterated word pairs from parallel-
aligned texts using a statistical machine transliteration 
model. Workshop on Building and Using Parallel 
Texts, HLT-NAACL-2003, 2003.  
R. Mahesh, K. Sinha. 2009.  Automated Mining Of 
Names Using Parallel Hindi-English Corpus.  7th 
Workshop on Asian Language Resources, ACL-
IJCNLP 2009, pages 48?54, Suntec, Singapore, 2009.  
Jong-Hoon Oh, Key-Sun Choi. 2006.  Recognizing 
transliteration equivalents for enriching domain-
specific thesauri. 3rd Intl. WordNet Conf. (GWC-06), 
pages 231?237, 2006. 
Jong-Hoon Oh, Hitoshi Isahara.  2006.  Mining the Web 
for Transliteration Lexicons: Joint-Validation Ap-
proach.  pp.254-261, 2006 IEEE/WIC/ACM Intl. 
Conf. on Web Intelligence (WI'06), 2006. 
Raghavendra Udupa, K. Saravanan, Anton Bakalov, and 
Abhijit Bhole.  2009a.  "They Are Out There, If You 
Know Where to Look": Mining Transliterations of 
OOV Query Terms for Cross-Language Information 
Retrieval.  ECIR-2009, Toulouse, France, 2009.  
Raghavendra Udupa, K. Saravanan, A. Kumaran, and 
Jagadeesh Jagarlamudi.  2009b.  MINT: A Method 
for Effective and Scalable Mining of Named Entity 
Transliterations from Large Comparable Corpora.  
EACL 2009.  
Raghavendra Udupa and Mitesh Khapra.  2010.  Trans-
literation Equivalence using Canonical Correlation 
Analysis.  ECIR-2010, 2010.  
Robert Russell.  1918.  Specifications of Letters.  US 
patent number 1,261,167.  
K Saravanan, A Kumaran.  2008.  Some Experiments in 
Mining Named Entity Transliteration Pairs from 
Comparable Corpora.  The 2
nd
 Intl. Workshop on 
Cross Lingual Information Access addressing the 
need of multilingual societies, 2008.   
56
