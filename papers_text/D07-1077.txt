Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 737?745, Prague, June 2007. c?2007 Association for Computational Linguistics
Chinese Syntactic Reordering for Statistical Machine Translation
Chao Wang
MIT CSAIL
32 Vassar Street, Room 362
Cambridge, MA 02139, USA
wangc@csail.mit.edu
Michael Collins
MIT CSAIL
32 Vassar Street, Room G-484
Cambridge, MA 02139, USA
mcollins@csail.mit.edu
Philipp Koehn
School of Informatics
2 Buccleuch Place, 5BP 2L2
Edinburgh, EH8 9LW, UK
pkoehn@inf.ed.ac.uk
Abstract
Syntactic reordering approaches are an ef-
fective method for handling word-order dif-
ferences between source and target lan-
guages in statistical machine translation
(SMT) systems. This paper introduces a re-
ordering approach for translation from Chi-
nese to English. We describe a set of syntac-
tic reordering rules that exploit systematic
differences between Chinese and English
word order. The resulting system is used
as a preprocessor for both training and test
sentences, transforming Chinese sentences
to be much closer to English in terms of their
word order. We evaluated the reordering
approach within the MOSES phrase-based
SMT system (Koehn et al, 2007). The
reordering approach improved the BLEU
score for the MOSES system from 28.52 to
30.86 on the NIST 2006 evaluation data. We
also conducted a series of experiments to an-
alyze the accuracy and impact of different
types of reordering rules.
1 Introduction
Syntactic reordering approaches are an effective
method for handling systematic differences in word
order between source and target languages within
the context of statistical machine translation (SMT)
systems (Xia and McCord, 2004; Collins et al,
2005). In reordering approaches, sentences in the
source language are first parsed, for example using a
Treebank-trained parser. A series of transformations
is then applied to the resulting parse tree, with the
goal of transforming the source language sentence
into a word order that is closer to that of the target
language. The reordering process is used to prepro-
cess both the training and test data used within an
existing SMT system. Reordering approaches have
given significant improvements in performance for
translation from French to English (Xia and Mc-
Cord, 2004) and from German to English (Collins
et al, 2005).
This paper describes a syntactic reordering ap-
proach for translation from Chinese to English. Fig-
ure 1 gives an example illustrating some of the dif-
ferences in word order between the two languages.
The example shows a Chinese sentence whose literal
translation in English is:
this is French delegation at Winter
Olympics on achieve DEC best accom-
plishment
and where a natural translation would be
this is the best accomplishment that the
French delegation achieved at the Winter
Olympics
As exemplified by this sentence, Chinese differs
from English in several important respects: for ex-
ample, relative clauses appear before the noun being
modified; prepositional phrases often appear before
the head they modify; and so on. It can be seen that
some significant reordering of the input is required
to produce a good English translation. For this ex-
ample, application of reordering rules leads to a new
Chinese string whose word-by-word English para-
phrase is:
737
Before syntactic reordering After syntactic reordering
IP NP PN ?(this)
VP VC(is)
NP CP IP NP NR {I(French)
NN ?L?(delegation)
VP PP P 3(at)
LCP NP NN ?G
(Winter)
NR $?
(Olympics)
LC ?(on)
VP-A VV (achieve)
DEC (DEC)
ADJP JJ ?(best)
NPB NN ?1(accomplishment)
IP NP PN ?(this)
VP VC(is)
NP ADJP JJ ?(best)
NPB NN ?1(accomplishment)
CP DEC (DEC)
IP NP NR {I(French)
NN ?L?(delegation)
VP VP-A VV (achieve)
PP P 3(at)
LCP LC ?(on)
NP NN ?G
(Winter)
NR $?
(Olympics)
Figure 1: Original (left) and reordered (right) parse trees for the Chinese sentence ???{I?L?3
?G$????1,? which translates into ?This is the best accomplishment that the French
delegation achieved at the Winter Olympics? in English.
this is best accomplishment DEC French
delegation achieve at on Winter Olympics
This reordering is relatively easy to express using
syntactic transformations?for example, it is simple
to move the entire relative clause ?French delega-
tion at Winter Olympics on achieve DEC? to a posi-
tion that is after the noun phrase it modifies, namely
?best accomplishment.? Phrase-based systems are
quite limited in their ability to perform transforma-
tions of this type. More recently developed hier-
archical systems (e.g., (Yamada and Knight, 2001;
Chiang, 2005; Marcu et al, 2006)) may be better
equipped to deal with reordering of this type; how-
ever, in this example they would effectively have to
first identify the span of the relative clause, and then
move it into the correct position, without any explicit
representation of the source language syntax.
In this paper, we describe a set of syntactic re-
ordering rules that exploit systematic differences be-
tween Chinese and English word order. The result-
ing system is used as a preprocessor for both training
and test sentences, transforming Chinese sentences
to be much closer to English. We report results for
the method on the NIST 2006 evaluation data, us-
ing the MOSES phrase-based SMT system (Koehn
et al, 2007). The reordering rules give an improve-
ment in accuracy from 28.52 to 30.86 BLEU score.
A concern for methods that make use of Chinese
parsers is that these parsers are typically of relatively
low accuracy, particularly given that Chinese re-
quires a word-segmentation step that is not required
in languages such as English. Our results show that
Chinese parses are useful in SMT in spite of this
problem. We report results showing the precision
of the reordering rules?essentially testing how of-
ten the Chinese sentences are correctly reordered?
to give more insight into this issue. We also report
experiments which assess the impact of each type of
reordering rule on translation accuracy.
2 Related Work
A number of researchers (Brown et al, 1992; Berger
et al, 1996; Niessen and Ney, 2004; Xia and Mc-
Cord, 2004; Collins et al, 2005) have described ap-
proaches that preprocess the source language input
in SMT systems. We are not, however, aware of
work on this topic for translation from Chinese to
English. Brown et al (1992) describe an analysis
component for French which moves phrases around
(in addition to other transformations) so the source
and target sentences are closer to each other in word
order. Berger et al (1996) describe an approach for
French that reorders phrases of the form NOUN1 de
NOUN2. Xia and McCord (2004) describe an ap-
proach for French, where reordering rules that oper-
ate on context-free rule productions are acquired au-
738
tomatically. Niessen and Ney (2004) describe an ap-
proach for translation from German to English that
combines verbs with associated particles, and also
reorders questions. Collins et al (2005) also de-
scribe an approach for German, concentrating on re-
ordering German clauses, which have quite different
word order from clauses in English. Our approach
is most similar to that of Collins et al (2005).
Most SMT systems employ some mechanism that
allows reordering of the source language during
translation (i.e., non-monotonic decoding). The
MOSES phrase-based system that we use has a rel-
atively simple reordering model which has a fixed
penalty for reordering moves in the decoder. More
sophisticated models include reordering parame-
ters that are sensitive to lexical information (Till-
mann, 2004; Kumar and Byrne, 2005; Koehn et
al., 2005). The model of Chiang (2005) employs
a synchronous context-free grammar to allow hi-
erarchical approaches to reordering. The syntax-
based models of Yamada and Knight (2001) and
Marcu et al (2006) build a full parse tree in the tar-
get language, again effectively allowing hierarchi-
cal reordering based on synchronous grammars. It
is worth noting that none of these approaches to re-
ordering make use of explicit syntactic information
in the source language?for example, none of the
methods make use of an existing source-language
parser (the systems of Yamada and Knight (2001)
and Marcu et al (2006) make use of a parser in the
target language, i.e., English).
Finally, note that a number of statistical MT
systems make use of source language syntax in
transducer-style approaches; see (Lin, 2004; Ding
and Palmer, 2005; Quirk et al, 2005; Liu et al,
2006; Huang et al, 2006). In contrast to the prepro-
cessing approach, they attempt to incorporate syntax
directly into the decoding stage.
3 Chinese Syntactic Reordering Rules
We used the Penn Chinese Treebank guidelines (Xue
et al, 2005) in searching for a suitable set of reorder-
ing rules. We examined all phrase types in the Tree-
bank; potentially phrases of any type could be can-
didates for reordering rules. Table 1 provides a list
of Treebank phrase tags for easy reference. We ruled
out several phrase types as not requiring reordering
ADJP adjective phrase
ADVP adverbial phrase headed by AD (adverb)
CLP classifier phrase
CP clause headed by C (complementizer)
DNP phrase formed by ?XP+DEG?
DP determiner phrase
DVP phrase formed by ?XP+DEV?
FRAG fragment
IP simple clause headed by I (INFL)
LCP phrase formed by ?XP+LC?
LST list marker
NP noun phrase
PP preposition phrase
PRN parenthetical
QP quantifier phrase
UCP unidentical coordination phrase
VP verb phrase
Table 1: Penn Chinese Treebank phrase tags.
rules. For example, Chinese ADJPs, ADVPs, DPs,
QPs, and PPs all have similar internal word order-
ing to their English counterparts. Also similar are a
group of special structures such as LST, FRAG, and
PRN.
We identified three categories that we considered
to be the most prominent candidates for reorder-
ing. These phrases include VPs (verb phrases), NPs
(noun phrases), and LCPs (localizer phrases, which
frequently map to prepositional phrases in English).
In the following, we discuss each of the three main
categories in more detail.
3.1 Verb Phrases
In Chinese, verb phrase modifiers typically occur in
pre-verbal position. VP modifiers can be ADVPs,
temporal and spatial NPs, QP, PPs, CPs, IPs,
DVPs, and LCPs. The ADVPs are simple adverbs,
which can occur both preverbal and postverbal in an
English verb phrase, so we do not attempt to move
them. Similarly, the CP, IP, and DVP modifiers
are typically adverbial phrases, which do not have a
fixed position in English verb phrases. In the follow-
ing, we only consider cases involving PPs, LCPs,
temporal and spatial NPs, and QPs.
PPs and LCPs Figure 2 shows an example verb
phrase with a PP modifier, which translates literally
739
VP PP P 3(at)
NP-A NPB NN ??(Eastern)
NN ??(Division)
VP-A VV ?(rank)
QP OD 1?(10th)
Figure 2: Example VP with PP modifier. The phrase
translates into ?ranks 10th in the Eastern Division.?
VP NP NPB NT U(same day)
NT ??(morning)
VP-A VV uL(issue)
NP-A NPB NN (?(statement)
Figure 3: Example VP with temporal NP modifier.
The phrase translates into ?issued a statement that
morning.?
into ?at Eastern Division rank 10th.? Recognizing
that PPs in English verb phrases almost always oc-
cur after the verb, we use a simple VP(PP:VP) re-
ordering rule which states that a PP in a parent VP
needs to be repositioned after the sibling VP. LCPs
are similar to PPs and typically map to prepositional
phrases in English. Thus they are handled similarly
to PPs, i.e., LCPs in a parent VP are repositioned
after the sibling VP.
NPs Figure 3 gives an example of a verb phrase
with a temporal NP modifier, which literally trans-
lates into ?same day morning issue statement.? In
English, temporal phrases such as these almost al-
ways occur after the head verb. Conveniently, the
Chinese Treebank uses the part of speech (POS) tag
NT for temporal nouns. Thus, we use a rule which
states that a preverbal NP will be repositioned af-
ter the sibling VP if there is at least one NT in the
NP subtree. A similar rule might apply to locative
NPS; however, there is no special POS tag in the
Treebank marking locations,1 so we do not have a
syntax-based reordering rule to handle locative NPs.
QPs QP modifiers in verb phrases often corre-
spond to time-related concepts such as duration and
frequency. Figure 4 shows an example verb phrase
with a QP modifier, literally translating into ?many
time injured.? Since temporal phrases almost always
occur after the verb in English verb phrases, we han-
1One can argue that NR (proper nouns) in that context are
likely to be places. However, there also exist many exceptions,
and so we decided not to exploit the NR tag.
VP QP CD ?(many)
CLP M g(time)
VP-A VV ??(injured)
Figure 4: Example VP with QP modifier. The phrase
translates into ?injured many times.?
NP-A DNP PP P ?(to)
NP-A NPB NR 9n??(Zimbabwe)
DEG (DEG)
NPB NN ?L(financial)
NN ?(aid)
Figure 5: An example Chinese NP with a DNP mod-
ifier headed by a PP. The phrase translates into ?the
financial aid to Zimbabwe? in English.
dle such cases by a simple rule which states that the
QP in a parent VP will be repositioned after the sib-
ling VP.
3.2 Noun Phrases
Noun phrases in Chinese can take several types of
modifiers: for example, phrases of type QP, DP,
ADJP, NP, DNP, and CP. The placement of QP, DP,
and ADJP modifiers is somewhat similar to English
in that these phrases typically occur before the noun
they modify. The case of NP modifiers in NPs is
very limited in the Chinese Treebank, since most
noun-noun sequences form compounds in a single
NP. Hence we only developed reordering rules to
handle DNP and clausal (CP) modifiers.
DNPs DNPs are formed by ?XP+DEG,? where XP
can be a phrase of the type ADJP, QP, PP, LCP, or
NP. When the XP is an ADJP or a QP, no reordering
is needed because the word order is the same as that
of English.
When the XP is a PP or an LCP, the DNP essen-
tially corresponds to a prepositional phrase in En-
glish, which almost always appears after the noun
being modified. Figure 5 shows an example where
the XP in the DNP is a PP. The reordering rule to
handle these two cases states that, if a parent NP has
a child DNP which in turn has a child PP or LCP,
then the DNP is repositioned after the last sibling NP.
Figure 6 shows an example noun phrase for which
the XP in the DNP is NP. On the surface, the Chinese
?NP1 DEG NP2? sequence is analogous to the En-
glish possessive structure of ?NP1?s NP2? and does
740
NP-A DNP NP DP DT T(this)
CLP M ?(measure word)
NPB NN E?(technique)
DEG (DEG)
NPB NN ??(mastery)
Figure 6: An example Chinese NP phrase with a
DNP modifier headed by a NP. The phrase translates
into ?the mastery of this technique? in English.
not require reordering, for example, ??(Sue) (?s)
*l(friend)? in Chinese and ?Sue?s friend? in En-
glish. However, the Chinese possessive structure
?NP1 DEG NP2? can express more sophisticated re-
lationships which are inappropriate for the ?NP1?s
NP2? expression. For example, the phrase in Fig-
ure 6 can only be translated into ?the mastery of
this technique,? but not ?this technique?s mastery.?
We decide to reorder DNPs of the ?NP+DEG? for-
mat, because they often can only map to the ?NP2 of
NP1? expression in English. Additionally, the ?NP2
of NP1? expression is more general and can replace
?NP1?s NP2? in many cases. One exception is when
the NP is a pronoun (PN), e.g., ?(he) (?s) ?
i(name),? in which case the DNP acts simply like a
possessive pronoun. Our reordering rule thus states
that, if a parent NP has a child DNPwhich in turn has
a child NP that is not a PN, then the DNP is reposi-
tioned after the last sibling NP.
CPs Relative clauses correspond to the CP cate-
gory in the Treebank. Figure 7 shows an example
noun phrase with two nested CP modifiers. As illus-
trated in the figure, relative clauses in Chinese also
occur before the noun they modify, which makes
the word order of this sentence quite different from
that of the English translation. Such distortions in
the word reordering will be quite difficult for the
word or phrase-based alignment model to capture.
However, with the application of a reordering rule
to reposition the child CP after its sibling NP un-
der a parent NP, and the PP VP reordering rule for
VP introduced previously, the sentence can be easily
transformed into ?French delegation participate 8th
handicap people Winter Olympics hold at US Salt
Lake City,? a sentence whose word order is much
closer to that of English.
CP is typically formed by ?IP+DEC?, in which
DEC?s only function is to mark the IP as a relative
NP CP IP VP VV ?\ (participate)
NP CP IP VP PP P 3 (at)
NP NR {I(US)
NR ??
(Salt Lake City)
VP VV ?1 (hold)
DEC  (DEC)
QP OD 1l (8th)
CLP M 3 (measure word)
NPB NN ?;<
(handicap people)
NR ??
(Winter Olympics)
DEC  (DEC)
NPB NR {I (French)
NPB NN ?L? (delegation)
Figure 7: An example with two nested CP modi-
fiers. The phrase translates into ?the French delega-
tion participating in the 8th Special Winter Olympics
held in Salt Lake City US.?
LCP IP NP-A NPB NN ?(accident)
VP VV u)(happen)
LC  (after)
Figure 8: An example Chinese localizer phrase. The
phrase translates into ?after the accident happened?
in English.
clause, similar to the function of ?that? in English.
We use a rule to bring DEC to the front of IP under
CP, to make it more aligned with the ?that + clause?
structure of English.
3.3 Localizers
Figure 8 shows an example phrase of the type LCP.
Localizers (tagged LC in the Treebank) in Chi-
nese can be thought of as a post-phrasal preposi-
tion which is often used with temporal and locative
phrases or clauses to mark directional information.
They function similarly to prepositions and conjunc-
tions in English such as ?before,? ?on,? ?when,? etc.
Constituents of type LCP have a similar function
to prepositional phrases. Sometimes they are com-
bined with a pre-phrasal generic preposition ?3?
(roughly corresponding to ?at? in English) to form
a PP explicitly. An example is shown in Figure 9.
We developed a simple reordering rule which
moves an LC node to immediately before its left sib-
ling under a parent LCP node. This will result in a
word order that is more similar to that of the English
741
PP P 3(at)
LCP IP NP-A NPB NN ?(accident)
VP VV u)(happen)
LC  (after)
Figure 9: An example Chinese PP encompassing an
LCP. The phrase translates into ?after the accident
happened? in English.
prepositional phrase: the example in Figure 8 has
the paraphrase ?after accident happen? after the re-
ordering rule is applied. In the case where an LCP is
embedded in a parent PP phrase, the LC reordering
rule will essentially merge the post-phrasal localizer
with the pre-phrasal preposition. For example, the
phrase in Figure 9 becomes ?at after accident hap-
pen? after reordering. The phrase-based SMT sys-
tem will have little problem in learning that ?at af-
ter? translates into ?after? in English.
4 Evaluation
Our baseline is a phrase-based MT system trained
using the MOSES toolkit (Koehn et al, 2007).
The training data consists of nearly 637K pairs of
sentences from various parallel news corpora dis-
tributed by the Linguistic Data Consortium (LDC).2
For tuning and testing, we use the official NIST
MT evaluation data for Chinese from 2002 to 2006,
which have four human generated English reference
translations for each Chinese input. The evaluation
data from 2002 to 2005 were split into two sets of
roughly equal sizes: a tuning set of 2347 sentences
is used for optimizing various parameters using min-
imum error training (also using the MOSES toolkit),
and a development set of 2320 sentences is used for
various analysis experiments. We report results on
the NIST 2006 evaluation data.
A series of processing steps are needed before the
reordering rules can be applied, which include seg-
mentation, part-of-speech tagging, and parsing. We
trained a Chinese Treebank-style tokenizer and part-
of-speech tagger, both using a tagging model based
on a perceptron learning algorithm (Collins, 2002).
We used the Chinese parser described by Sun and
Jurafsky (2004), which was adapted from the parser
2We used 8 corpora for training, including LDC2002E18,
LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06,
LDC2006E26, LDC2006E8, and LDC2006G05.
Dev Nist06
Baseline 31.57 28.52
Reorder 32.86 30.86
Gain +1.29 +2.34
Table 2: BLEU score of the baseline and reordered
systems.
presented in Collins (1997). We then applied the re-
ordering rules described in the previous section to
the parse tree of each input. The reordered sen-
tence is then re-tokenized to be consistent with the
baseline system, which uses a different tokenization
scheme that is more friendly to the MT system.3
We use BLEU scores as the performance measure
in our evaluation (Papineni et al, 2002). Table 2
gives results for the baseline and reordered systems
on both the development and test sets. As shown in
the table, the reordering method is able to improve
the BLEU scores by 1.29 points on the development
set, and by 2.34 on the NIST 2006 set.
4.1 Frequency and Accuracy of Reordering
Rules
We collected statistics to evaluate how often and ac-
curately the reordering rules are applied in the data.
The accuracy is measured in terms of the percent-
age of rule applications that correctly reorder sen-
tences. The vast majority of reordering errors are
due to parsing mistakes.
Table 3 summarizes the count of each rule in
the training data, ignoring rules occurring less than
500 times in the training data, and the number
of sentences each rule impacts. The most fre-
quent three rules are NP(CP:NP), VP(PP:VP),
and DNP(NP):NP, which account for over 76% of
all the reordering instances and jointly affect 74%
of all the training sentences. This shows the preva-
lence of systematic word order differences between
Chinese and English. Only 122,076 (or 19.2%) sen-
tences remain unchanged after the reordering rules
are applied.
Each of the processing steps in producing the Chi-
nese parse tree is prone to error and could lead to
mistakes in the reordering of the Chinese sentence.
3The tokenizer used by the MT system favors smaller word
units, and backs off to a character by character scheme for un-
known words.
742
Type Rule Name Counts # Sent.
VP VP(PP:VP) 331,827 258,214
VP(NT:VP) 23,353 22,926
VP(LCP:VP) 8,674 8,661
VP(QP:VP) 7,834 7,777
NP NP(CP:NP) 345,165 262,588
DNP(NP):NP 280,367 218,865
DNP(PP):NP 38,225 36,295
DNP(LCP):NP 15,801 15,253
LC LCP(NP:LC) 146,784 12,8333
LCP(IP:LC) 36,923 35,749
LCP(QP:LC) 14,893 14,287
Total 1,249,846 636,686
Table 3: Statistics of various reordering rules in the
training data.
To assess the accuracy of reordering rules, we con-
ducted human evaluations on a set of 200 sentences
randomly selected from the development set. Within
this set, there were in total 155 sentences containing
at least one reordering rule, with 339 rules in total.
A bilingual speaker was presented with the Chinese
parse tree, the sentence before and after the reorder-
ing, and the particular reordering rules applied to the
sentence. The bilingual rater determined the correct-
ness of each rule by first identifying the scope of the
rule and comparing the string before and after re-
ordering, referencing the corresponding parse struc-
ture if necessary. Table 4 summarizes the accuracy
(precision) for each type of rule. Notice that our hu-
man evaluation of the reordering rules does not take
into account missed reordering.
Overall, there are a lot of reordering errors caused
by incorrect parses. On a sentence level, only 57
out of the 155 reordered sentences (36.8%) are error
free. Nevertheless, syntactic reordering seems to be
helpful in improving the translation quality, despite
noise introduced into the data due to the errors.
4.2 Impact of Individual Reordering Rules
In order to assess the relative effectiveness of the
reordering rules, we conducted an experiment in
which we trained and tested systems using data
that were reordered using different subsets of the
reordering rules. Table 5 summarizes the BLEU
scores of the reordered system for each rule type.
Count Accuracy
VP rules 108 65.7%
NP rules 209 54.6%
LC rules 76 77.6%
All rules 393 62.1%
Table 4: Accuracy of reordering rules on a set of 200
sentences randomly selected from the development
set.
BLEU Gain
Baseline 31.57 -
VP rules 32.71 +1.14
NP rules 32.23 +0.66
LC rules 31.59 +0.02
All rules 32.86 +1.29
Table 5: Comparison of translation performance
with different types of reordering rules. Gain is the
change in BLEU score when compared to the base-
line system. All results are on the development set.
As shown in the table, the VP rules are more effec-
tive than the NP rules, even though the NP rules are
more frequent than the VP rules in the data. This
is perhaps because the reordering of VP modifiers
achieves a slightly higher accuracy than that of the
NP modifiers. We are a bit surprised by the lack
of performance gains with the LC rules only. More
analysis is needed to explain this behavior.
4.3 Better Alignment?
There could be two reasons why the syntactic
reordering approach improves over the baseline
phrase-based SMT system. One obvious benefit is
that the word order of the transformed source sen-
tence is much closer to that of the target sentence,
which reduces the reliance on the distortion model
to perform reordering during decoding. Another po-
tential benefit is that the alignment between the two
sides will be of higher quality because of fewer ?dis-
tortions? between the source and the target, so that
the resulting phrase table of the reordered system
would be better. However, a counter argument is that
the reordering is very error prone, so that the added
noise in the reordered data would actually hurt the
alignments and hence the phrase table.
Lacking a good way to measure the quality of
743
Original Dev Reordered Dev
Baseline 31.57 32.19
Reorder 30.67 32.86
Table 6: Comparison of BLEU scores in matched
and mismatched conditions. The baseline and re-
ordered systems were first tuned on mismatched data
before being tested on mismatched data.
the phrase table directly, we conducted an experi-
ment in which we tested the baseline and reordered
systems with both the original and reordered devel-
opment data. The idea is to compare the two sys-
tems given the same type of input: if the reordered
system learned a better phrase table, then it might
outperform the baseline system on un-reordered in-
puts despite the mismatch; on the other hand, if the
baseline system learned a better phrase table, then it
might outperform the reordered system on reordered
inputs despite the mismatch. However, the results in
Table 6 did not settle our question: the reordered
system performed worse than the baseline on unre-
ordered data, while the baseline system performed
worse than the reordered system on reordered data,
both of which can be explained by the mismatched
conditions between training and testing. Perhaps
more interesting is the performance gap of the base-
line system on the reordered data vs. on the original
data: it achieved 0.62 BLEU score gain despite the
mismatch in training and testing conditions.
5 Discussion and Future Work
In this paper, we described a set of syntactic reorder-
ing rules that exploit systematic differences between
Chinese and English word order to transform Chi-
nese sentences to be much closer to English in terms
of their word order. We evaluated the reordering ap-
proach within the MOSES phrase-based SMT sys-
tem (Koehn et al, 2007). The reordering approach
improved the BLEU score for the MOSES system
from 28.52 to 30.86 on the NIST 2006 evaluation
data. Our manual evaluation of the reordering accu-
racy indicated that the reordering approach is help-
ful at improving the translation quality despite rel-
atively frequent reordering errors. The reordering
approach even achieved a 0.62 gain in BLEU score
when only the test data are reordered.
An important category we examined but did not
reorder was clauses of type IP, which generally
corresponds to declarative sentences in Chinese.
Sentences of this form have quite similar top-level
constituent ordering to English: both follow SVO
(subject-verb-object) order. There are several spe-
cial cases in which English and Chinese differ, the
most notable being the topicalization of objects or
temporal and locative noun phrases (which function
as adverbial phrases). We did not try to restore them
to the canonical order for several reasons. First, top-
icalization of temporal and locative phrases happens
in English as well. For example, ?In Israel yesterday,
an explosion killed one person and injured twelve?
is a perfectly acceptable English sentence. Second,
the parser?s performance on special constructions is
likely to be poor, resulting in frequent reordering er-
rors. Third, special constructions that do not occur
often in the data are less likely to have a significant
impact on the translation performance. Thus our
strategy has been to find reordering rules for syntac-
tic categories that are common in the data and sys-
tematically different between the two languages.
In our experiments, the phrase-based MT sys-
tem uses an un-lexicalized reordering model, which
might make the effects of the syntactic reordering
method more pronounced. However, in an early ex-
periment4 submitted to the official NIST 2006 MT
evaluation, the reordered system also improved the
BLEU score substantially (by 1.34 on NIST 2006
data) over a phrase-based MT system with lexical-
ized reordering models (Koehn et al, 2005). The
same set of reordering rules in the experimental set-
ting in the current paper achieve a 1.82 BLEU im-
provement on the same data set, which is compara-
ble to the 1.34 gain for the lexicalized system.
We plan to output reordered lattices in the future,
so that the approach would be more robust to errors
made during parsing/reordering.
Acknowledgements
We would like to thank Brooke Cowan, Stephanie
Seneff, and the three anonymous reviewers for their
valuable comments. Thanks to Yushi Xu for evalu-
ating the accuracy of the reordering rules. This work
4This experiment made use of a subset of the reordering
rules we have presented here.
744
was supported under the GALE program of the De-
fense Advanced Research Projects Agency, Contract
No. HR0011-06-C-0022.
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?69.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, John D. Lafferty, and Robert L. Mercer. 1992.
Analysis, statistical transfer, and synthesis in machine
translation. In Proceedings of Conference on Theoret-
ical and Methodological Issues in Machine Transla-
tion.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263?270.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Poceedings of ACL, pages 531?540.
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proceedings of ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP.
Yuan Ding and Martha Palmer. 2005. Machine transla-
tion using probablistic synchronous dependency inser-
tion grammars. In Proceedings of ACL, pages 541?
548, Ann Arbor, Michigan.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, and Chris Callison-Burch. 2005. Edinburgh
system description. In IWSLT Speech Translation
Evaluation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
strantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL, Demonstration Session.
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation.
In Proceedings of HLT-EMNLP.
Dekang Lin. 2004. A path-based transfer model for
machine translation. In Proceedings of Coling 2004,
pages 625?630, Geneva, Switzerland, Aug 23?Aug
27. COLING.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of ACL, pages 609?616.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proceedings of EMNLP, pages 44?52, Sydney, Aus-
tralia.
Sonja Niessen and Hermann Ney. 2004. Statistical ma-
chine translation with scarce resources using morpho-
syntactic information. Computational Linguistics,
30(2):181?204.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of ACL, pages 271?279,
Ann Arbor, Michigan.
Honglin Sun and Daniel Jurafsky. 2004. Shallow se-
mantic parsing of Chinese. In Proceedings of NAACL-
HLT.
Christoph Tillmann. 2004. A block orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL, Boston, MA, USA.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical MT system with automatically learned rewrite
patterns. In Proceedings of COLING.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of ACL.
745
