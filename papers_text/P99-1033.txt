Dependency  Parsing with an Extended Finite State Approach 
Kemal  Of lazer  
Depar tment  of Computer  Engineering 
Bilkent University 
Ankara, 06533,Turkey 
ko?cs, b i l kent ,  edu.  t r  
Comput ing  Research Laboratory  
New Mexico State University 
Las Cruces, NM, 88003 USA 
ko@cr l ,  nmsu. edu 
Abstract 
This paper presents a dependency parsing scheme 
using an extended finite state approach. The parser 
augments input representation with "channels" so 
that links representing syntactic dependency rela- 
tions among words can be accommodated, and it- 
erates on the input a number of times to arrive at 
a fixed point. Intermediate configurations violating 
various constraints of projective dependency repre- 
sentations such as no crossing links, no independent 
items except sentential head, etc, are filtered via fi- 
nite state filters. We have applied the parser to de- 
pendency parsing of Turkish. 
1 Introduction 
Recent advances in the development of sophisticated 
tools for building finite state systems (e.g., XRCE 
Finite State Tools (Karttunen et al, 1996), ATgzT 
Tools (Mohri et al, 1998)) have fostered the develop- 
ment of quite complex finite state systems for natu- 
ral language processing. In the last several years, 
there have been a number of studies on develop- 
ing finite state parsing systems, (Koskenniemi, 1990; 
Koskenniemi et al, 1992; Grefenstette, 1996; Ait- 
Mokhtar and Chanod, 1997). There have also been 
a number of approaches to natural anguage pars- 
ing using extended finite state approaches in which 
a finite state engine is applied multiple times to the 
input, or various derivatives thereof, until some stop- 
ping condition is reached. Roche (1997) presents 
an approach for parsing in which the input is itera- 
tively bracketed using a finite state transducer. Ab- 
ney(1996) presents a finite state parsing approach 
in which a tagged sentence is parsed by transducers 
which progressively transform the input to sequences 
of symbols representing phrasal constituents. This 
paper presents an approach to dependency parsing 
using an extended finite state model resembling the 
approaches of Roche and Abney. The parser pro- 
duces outputs that encode alabeled ependency tree 
representation f the syntactic relations between the 
words in the sentence. 
We assume that the reader is familiar with the 
basic concepts of finite state transducers (FST here- 
after), finite state devices that map between two reg- 
ular languages U and L (Kaplan and Kay, 1994). 
2 Dependency  Syntax 
Dependency approaches to syntactic representation 
use the notion of syntactic relation to associate sur- 
face lexical items. The book by Mel~uk (1988) 
presents a comprehensive exposition of dependency 
syntax. Computational pproaches to dependency 
syntax have recently become quite popular (e.g., 
a workshop dedicated to computational pproaches 
to dependency grammars has been held at COL- 
ING/ACL'98 Conference). J~irvinen and Tapana- 
ninen have demonstrated anefficient wide-coverage 
dependency parser for English (Tapanainen and 
J~irvinen, 1997; J?rvinen and Tapanainen, 1998). 
The work of Sleator and Temperley(1991) on link 
grammar, an essentially exicalized variant of depen- 
dency grammar, has also proved to be interesting in
a number of aspects. Dependency-based statistical 
language modeling and analysis have also become 
quite popular in statistical natural language process- 
ing (Lafferty et al, 1992; Eisner, 1996; Chelba and 
et al, 1997). 
Robinson(1970) gives four axioms for well-formed 
dependency structures, which have been assumed in 
almost all computational pproaches. In a depen- 
dency structure of a sentence (i) one and only one 
word is independent, i.e., not linked to some other 
word, (ii) all others depend directly on some word, 
(iii) no word depends on more than one other, and, 
(iv) if a word A depends directly on B, and some 
word C intervenes between them (in linear order), 
then C depends directly on A or on B, or on some 
other intervening word. This last condition of pro- 
jectivity (or various extensions of it; see e.g., Lau 
and Huang (1994)) is usually assumed by most com- 
putational approaches to dependency grammars as 
a constraint for filtering configurations, and has also 
been used as a simplifying condition in statistical 
approaches for inducing dependencies from corpora 
(e.g., Yiiret(1998).) 
3 Turkish 
Turkish is an agglutinative language where a se- 
quence of inflectional and derivational morphemes 
get affixed to a root (Oflazer, 1993). Derivations are 
very productive, and the syntactic relations that a 
word is involved in as a dependent or head element, 
are determined by the inflectional properties of the 
254 
"~41~tCJ  f ru i t  DopeDdoDt:g L:Lnk t:o Head 
1 II 
IG3 io,  }1 
Figure h Links and Inflectional Groups 
one or more (intermediate) derived forms. In this 
work, we assume that a Turkish word is represented 
as a sequence of inflectional groups (IGs hereafter), 
separated by "DBs denoting derivation boundaries, 
in the following general form: 
root+Infl1"DB+Infl2"DB+. ? .'DB+Infl. 
where Infli denote relevant inflectional features 
including the part-of-speech for the root, or any 
of the derived forms. For instance, the derived 
determiner saglamla?tlrdzgzmzzdaki I would be 
represented as:2 
s aglam+hdj "DB+Verb+Be come "DB+Verb+Caus+Po s
"DB+Adj +PastPart+P i sg* DB 
+Noun+Zero+A3sg+Pnon+Loc'DB+Det 
This word has 6 IGs: 
I. sa~lam+Adj 2. +Verb+Become 
3. +Verb+Caus+Pos 4. +Adj+PastPart+Plsg 
5. +Noun+Zero+A3sg 6. +Det 
+Pnon+Loc 
A sentence would then be represented as a sequence 
of the IGs making up the words. 
An interesting observation that we can make 
about Turkish is that, when a word is considered 
as a sequence of IGs, syntactic relation links only 
emanate from the last IG of a (dependent) word, 
and land on one of the IG's of the (head) word on 
the right (with minor exceptions), as exemplified in 
Figure 1. A second observation is that, with minor 
exceptions, the dependency links between the IGs, 
when drawn above the IG sequence, do not cross. 
Figure 2 shows a dependency tree for a sentence laid 
on top of the words segmented along IG boundaries. 
4 F in i te  S ta te  Dependency  Pars ing  
The approach relies on augmenting the input with 
"channels" that (logically) reside above the IG se- 
quence and "laying" links representing dependency 
relations in these channels, as depicted Figure 3 a). 
The parser operates in a number of iterations: At 
each iteration of the parser, an new empty channel 
1Literally, "(the thing existing) at the time we caused 
(something) to become strong". Obviously this is not a word 
that one would use everyday. Turkish words found in typical 
text average about 3-4 morphemes including the stem. 
2 The morphological features other than the obvious POSe 
are: +Become: become verb, +Caus: causative verb, PastPart: 
Derived past participle, Ptsg: leg possessive agreement, 
A3sg: 3sg number-person agreement,+Zero: Zero derivation 
with no overt morpheme, +Pnon: No possessive agreement, 
+Loc:Locative case, +Poe: Positive Polarity. 
a) Input sequence ofIGs am augmented with symbols to represent Channels. 
(IGl) (IG2) (IG3)... (IGi)... (IGn_{) (IG,) 
b) Links are embedded in channels. 
, . . - . . . , . , ,% , , , : . . . , . . . r ,  . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .~ , , . . . . .~  . . . . . .  
(IGl) (IG2) (IG3)... (IGi)... (IG._l) (IG.) 
c) New channels are "stacked on top of each other". 
? u . . . . . .  ~ . . .T  . ' , . . , L .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ~ . . . . . . .~  . . . . .  
.n . . . . . . . r . . , . : , . . . . . .~  . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .~ . . . . , . .~  . . . . . .  
(IGI) (IG2) (IG3)... (IGi)... (IG..I) (IG.) 
d) So that links that can not be accommodated in lower channels can be established. 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ? . l  . . . . . . . . . . . . . . . . . . . . . .  ; .  . . . . .  
(IGl) (IG2) (IG3)... (IGi)... (IG,.l) (1G,) 
? .~ . - - . - -  ~-  "A ' " "  ~ . . . . . . . . . . . . .  ~ . . . . . . . . . . . .  ~"""1~ . . . . .  
(IG,) (IG,) (IG0... (IG~)... (IG?.,) 0G,) 
Figure 3: Channels and Links 
is "stacked" on top of the input, and any possible 
links are established using these channels, until no 
new links can be added. An abstract view of this is 
presented in parts b) through e) of Figure 3. 
4.1 Represent ing  Channe ls  and  Syntact i c  
Re la t ions  
The sequence (or the chart) of IGs is produced by 
a a morphological nalyzer FST, with each IG be- 
ing augmented by two pairs of delimiter symbols, as 
<(IG)>. Word final IGs, IGs that links will emanate 
from, are further augmented with a special marker ?. 
Channels are represented by pairs of matching sym- 
bols that surround the <. . .  ( and the ) . . .>  pairs. 
Symbols for new channels (upper channels in Figure 
3) are stacked so that the symbols for the topmost 
channels are those closest o the ( . . . ) .a  The chan- 
nel symbol 0 indicates that the channel segment is 
not used while 1 indicates that the channel is used 
by a link that starts at some IG on the left and 
ends at some IG on the right, that is, the link is 
just crossing over the IG. If a link starts from an 
IG (ends on an IG), then a start (stop) symbol de- 
noting the syntactic relation is used on the right 
(left) side of the IG. The syntactic relations (along 
with symbols used) that we currently encode in our 
parser are the following: 4 S (Subject), 0 (Object), 
M (Modifier, adv/adj), P (Possessor), C (Classifier), 
D (Determiner), T (Dative Adjunct), L ( Locative 
Adjunct), A: (Ablative Adjunct) and I (Instrumen- 
tal Adjunct). For instance, with three channels, the 
two IGs of bahgedeki in Figure 2, would be repre- 
sented as <MD0(bah~e+Noun+h3sg+Pnon+Loc)000> 
<000(+Det?)00d>. The M and the D to the left of 
3 At any time, the number  of channel symbols on both sides 
of an IG are the same. 
4We use the lower case symbol to mark the start of the 
link and the upper case symbol to encode the end of the link. 
255 
Det Pos Subj 
D ADJ N D N ADV V N PN ADV V 
Last line shows the final POS for each word. 
Figure 2: Dependency Links in an example Turkish Sentence 
the first IG indicate the incoming modifier and de- 
terminer links, and the d on the right of the second 
IG indicates the outgoing determiner link. 
4.2 Components  o f  a Parser  Stage 
The basic strategy of a parser stage is to recognize by 
a rule (encoded as a regular expression) a dependent 
IG and a head IG, and link them by modifying the 
"topmost" channel between those two. To achieve 
this: 
1. we put temporary brackets to the left of the 
dependent IG and to the right of the head IG, 
making sure that (i) the last channel in that 
segment is free, and (ii) the dependent is not 
already linked (at  one of the lower channels), 
2. we mark the channels of the start, intermediate 
and ending IGs with the appropriate symbols 
encoding the relation thus established by the 
brackets, 
3. we remove the temporary brackets. 
A typical linking rule looks like the following: 5
\[LL IGI LR\] \[ML IG2 MR\]* \[RL IG3 RR\] (->) 
"{s" . . .  "s}" 
This rule says: (optionally) bracket (with {S and 
S}), any occurrence of morphological pattern IG1 
(dependent), skipping over any number of occur- 
rences of pattern IG2, finally ending with a pat- 
tern IG3 (governor). The symbols L(eft)L(eft), 
LR, ML, MR, RL and RR are regular expressions 
that encode constraints on the bounding chan- 
nel symbols. For instance, LI~ is the pattern 
"? .. . .  ) . . . .  0" \["0" I 1\]* ">" which checks that 
(i) this is a word-final IG (has a "?"), (ii) the right 
side "topmost" channel is empty (channel symbol 
nearest o ")" is "0"), and (iii) the IG is not linked 
to any other in any of the lower channels (the only 
symbols on the right side are 0s and ls.) 
For instance the example rule 
\[LL NominativeNominalA3pl LR\] \[ML AnyIG MR\]* 
\[RL \[FiniteVerbA3sg I FiniteVerbl3pl\] RR \] 
(->) "{s . . . .  s}" 
SWe use the XRCE Regular Expression Language 
Syntax; see ht tp  ://www. xrce.  xerox,  com/resea.vch/ 
taltt/fst/fssyntax.htral for details. 
is used to bracket a segment starting with a plural 
nominative nominal, as subject of a finite verb on 
the right with either +A3sg or +A3pl number-person 
agreement (allowed in Turkish.) The regular expres- 
sion NominativeNominalA3pl matches any nomi- 
nal IG with nominative case and A3pl agreement, 
while the regular expression \[F initeVerbA3sg J 
F in i teVerbA3pl \ ]  matches any finite verb IG with 
either A3sg or A3pl agreement. The regular expres- 
sion AnyIG matches any IG. 
All the rules are grouped together into a parallel 
bracketing rule defined as follows: 
Bracket = \[ 
Pat tern l  (->) "{Re l l "  ... "Re l l}" ,  
Pat tern2  (->) "{Re l2"  ... "Re l2}" ,  
\ ] ;  
which will produce all possible bracketing of the in- 
put IG sequence. 6 
4.3 F i l te r ing  Cross ing L ink  Conf igurat ions  
The bracketings produced by Bracket  contain con- 
figurations that may have crossing links. This hap- 
pens when the left side channel symbols of the IG 
immediately right of a open bracket contains the 
symbol 1 for one of the lower channels, indicating 
a link entering the region, or when the right side 
channel symbols of the IG immediately to the left 
of a close bracket contains the symbol 1 for one of 
the lower channels, indicating a link exiting the seg- 
ment, i.e., either or both of the following patterns 
appear in the bracketed segment: 
(i) {S < . . .  1 . . .  0 ( . . .  ) . . .  
(ii) ... ( ... ) 0 ... 1 ... > S} 
Configurations generated by bracketing are filtered 
by FSTs implementing suitable regular expressions 
that reject inputs having crossing links. 
A second configuration that may appear is the fol- 
lowing: A rule may attempt to put a link in the 
topmost channel even though the corresponding seg- 
ment is not utilized in a previous channel, e.g., the 
corresponding segment one of the previous channels 
may be all Os. This constraint filters such cases to 
6{Reli and Roli} are pairs of brackets; there is a distinct 
pair for each syntactic relation to be identified by these rules. 
256 
prevent redundant configurations from proliferating 
for later iterations of the parser. 7 For these two con- 
figuration constraints we define F i l te raonf igs  as s 
FilterConfigs = \[ Fi lterCrossingLinks .o. 
Filt erEmptySegment s\] ; 
We can now define one phase (of one iteration) of 
the parser as: 
Phase  = Bracket  .o .  F i l te rCon2 igs  .o .  
MarkChanne ls  .o .  RemoveTempBrackets ;  
The transducer MarkChannels modifies the chan- 
nel symbols in the bracketed segments to either 
the syntactic relation start or end symbol, or a 
1, depending on the IG. Finally, the transducer 
RemoveTempBrackets ,  removes  the brackets. 9 
The formulation up to now does not allow us to 
bracket an IG on two consecutive non-overlapping 
links in the same channel. We would need a brack- 
eting configuration like 
... {S < ... > {H < ... > S} ... < ... > M} ... 
but this would not be possible within Bracket, as 
patterns check that no other brackets are within 
their segment of interest. Simply composing the 
Phase transducer with itself without introducing a 
new channel solves this problem, giving us a one- 
stage parser, i.e., 
Parse  = Phase  . o .  Phase ;  
4.4 Enforc ing  Syntact i c  Const ra in ts  
The rules linking the IGs are overgenerating in that 
they may generate configurations that may vio- 
late some general or language specific constraints. 
For instance, more than one subject or one ob- 
ject may attach to a verb, or more that one deter- 
miner or possessor may attach to a nominal, an ob- 
ject may attach to a passive verb (conjunctions are 
handled in the manner described in J?rvinen and 
Tapanainen(1998)), or a nominative pronoun may 
be linked as a direct object (which is not possible 
in Turkish), etc. Constraints preventing these may 
can be encoded in the bracketing patterns, but do- 
ing so results in complex and unreadable rules. In- 
stead, each can be implemented as a finite state filter 
which operate on the outputs of Parse  by checking 
the symbols denoting the relations. For instance we 
can define the following regular expression for fil- 
tering out configurations where two determiners are 
attached to the same IG: l? 
7This constra int  is a bit  tr ickier since one has to check that  
the same number  of channels on both sides are empty;  we l imit 
ourselves to the last 3 channels in the implementat ion.  
8. o. denotes the t ransducer  composit ion operator.  We 
also use, for exposit ion purposes, =, instead of the XRCE 
def ine  command.  
9 The detai ls  of these regular expressions are quite uninter-  
esting. 
l?LeftChannelSymbols and RightChannelSymbols denote 
the sets of symbols that can appear on the left and right side 
channels. 
AtMost0neDet = 
\[ "<" \[ ~ \[\[$"D"\]'I\] & LeftCharmelSymbols* \] 
"(" AnyIG ("@") ")" 
RightChannelSymbols* ">" \]*; 
The FST for this regular expression makes sure that 
all configurations that are produced have at most 
one D symbol among the left channel symbols, n
Many other syntactic onstraints (e.g., only one ob- 
ject to a verb) can be formulated similar to above. 
All such constraints Consl ,  Cons2 .. .ConsN, can 
then be composed to give one FST that enforces all 
of these: 
Syntact i cF i l te r  = \[ Cons l  .o .  Cons2 .o .  
Cons3 .o . . . . .  o.  ConsN\] 
4.5 I te ra t lve  app l i ca t ion  of  the  parser  
Full parsing consists of iterative applications of the 
Parser and Syntact icF i l ter  FSTs. Let Input be 
a transducer that represents the word sequence. Let 
LastChannelNotEmpt y = 
\["<" Lef tChannelSymbels+ 
"(" AnyIG ("@") ")" 
RightCharmelSymbols+ ">"\]* -
\["<" LeftChannelSymbols* 0 
"(" AnyIG ("@") ")" 
0 RightChannelSymbols* ">"\]*; 
be a transducer which detects if any configuration 
has at least one link established in the last channel 
added (i.e., not all of the "topmost" channel sym- 
bols are O's.) Let Morpholog ica lD isambiguator  
be a reductionistic finite state disambiguator which 
performs accurate but very conservative local dis- 
ambiguation and multi-word construct coalescing, to 
reduce morphological mbiguity without making any 
errors. 
The iterative applications of the parser can now 
be given (in pseudo-code) as: 
# Map sentence to a transducer representing 
a chart of IGs 
M = \[Sentence .o. MorphologicalAnalyzer\] .o. 
MorphologicalDisambi~nlat or; 
repeat { 
M = M .o. AddChannel .o. Parse .o. 
Synt act icFilter ; 
} 
unti l  ( \[M .o. LastChannelNotEmpty\]. l  == { }) 
M = M .o. 0nly0neUnlinked ; 
Parses = M.I; 
This procedure iterates until the most recently 
added channel of every configuration generated is 
unused (i.e., the (lower regular) language recognized 
by M .o. LastChannelNotEmpty is empty.) 
The step after the loop, M = M .o. 
0nly0neUnlinked, enforces the constraint that 
11 The crucial  port ion at the beginning says "For any IG it is 
not the case that  there is more than one subst r ing  conta in ing 
D among the left channel  symbols of that  IG." 
257 
in a correct dependency parse all except one of 
the word final IGs have to link as a dependent 
to some head. This transduction filters all those 
configurations (and usually there are many of them 
due to the optionality in the bracketing step.) 
Then, Parses defined as the (lower) language of the 
resulting FST has all the strings that encode the 
IGs and the links. 
4.6 Robust  Pars ing  
It is possible that either because of grammar cover- 
age, or ungrammatical input, a parse with only one 
unlinked word final IG may not be found. In such 
cases Parses above would be empty. One may how- 
ever opt to accept parses with k > 1 unlinked word 
final IGs when there are no parses with < k un- 
linked word final IGs (for some small k.) This can be 
achieved by using the lenient composition operator 
(Karttunen, 1998). Lenient composition, notated 
as . 0., is used with a generator-filter combination. 
When a generator transducer Gis leniently composed 
with a filter transducer, F, the resulting transducer, 
G . 0. F, has the following behavior when an input 
is applied: If any of the outputs of G in response to 
the input string satisfies the filter F, then G .0. F 
produces just these as output. Otherwise, G .0. F 
outputs what G outputs. 
Let Unlinked_i denote a regular expression which 
accepts parse configurations with less than or equal 
i unlinked word final IGs. For instance, for i = 2, 
this would be defined as follows: 
- \ [ \ [$\ [  "<" LeftChannelSymbols* "(" AnyIG "@ . . . .  )" 
E"0" I 13. ">"\]3" > 2 \]; 
which rejects configurations having more than 2 
word final IGs whose right channel symbols contain 
only 0s and is, i.e., they do not link to some other 
IG as a dependent. 
Replacing line M = H .o. Only0neUnlinked, 
with, for instance, M = M .0. Unlinked_l .0. 
Unlinked_2 .0. Unlinked_3; will have the parser 
produce outputs with up to 3 unlinked word final 
IGs, when there are no outputs with a smaller num- 
ber of unlinked word final IGs. Thus it is possible to 
recover some of the partial dependency structures 
when a full dependency structure is not available 
for some reason. The caveat would be however that 
since Unlinked_l is a very strong constraint, any 
relaxation would increase the number of outputs 
substantially. 
5 Exper iments  w i th  dependency  
pars ing  o f  Turk i sh  
Our work to date has mainly consisted of developing 
and implementing the representation a d finite state 
techniques involved here, along with a non-trivial 
grammar component. We have tested the resulting 
system and grammar on a corpus of 50 Turkish sen- 
tences, 20 of which were also used for developing and 
testing the grammar. These sentences had 4 to 24 
words with an average 10 about 12 words. 
The grammar has two major components. The 
morphological analyzer is a full coverage analyzer 
built using XRCE tools, slightly modified to gen- 
erate outputs as a sequence of IGs for a sequence 
of words. When an input sentence (again repre- 
sented as a transducer denoting a sequence of words) 
is composed with the morphological analyzer (see 
pseudo-code above), a transducer for the chart rep- 
resenting all IGs for all morphological ambiguities 
(remaining after morphological disambiguation) is
generated. The dependency relations are described 
by a set of about 30 patterns much like the ones 
exemplified above. The rules are almost all non- 
lexical establishing links of the types listed earlier. 
Conjunctions are handled by linking the left con- 
junct to the conjunction, and linking the conjunction 
to the right conjunct (possibly at a different chan- 
nel). There are an additional set of about 25 finite 
state constraints that impose various syntactic and 
configurational constraints. The resulting Parser  
transducer has 2707 states 27,713 transitions while 
the Syntact i cConst ra in ts  transducer has 28,894 
states and 302,354 transitions. The combined trans- 
ducer for morphological nalysis and (very limited) 
disambiguation has 87,475 states and 218,082 arcs. 
Table 1 presents our results for parsing this set of 
50 sentences. The number of iterations also count 
the last iteration where no new links are added. In- 
spired by Lin's notion of structural complexity (Lin, 
1996), measured by the total length of the links in 
a dependency parse, we ordered the parses of a sen- 
tence using this measure. In 32 out of 50 sentences 
(64%), the correct parse was either the top ranked 
parse or among the top ranked parses with the same 
measure. In 13 out of 50 parses (26%) the correct 
parse was not among the top ranked parses, but was 
ranked lower. Since smaller structural complexity 
requires, for example, verbal adjuncts, etc. to attach 
to the nearest verb wherever possible, topicalization 
of such items which brings them to the beginning of 
the sentence, will generate a long(er) link to the verb 
(at the end) increasing complexity. In 5 out of 50 
sentences (5%), the correct parse was not available 
among the parses generated, mainly due to grammar 
coverage. The parses generated in these cases used 
other (morphological) ambiguities of certain lexical 
items to arrive at some parse within the confines of 
the grammar. 
The finite state transducers compile in about 
2 minutes on Apple Macintosh 250 Mhz Power- 
book. Parsing is about a second per iteration in- 
cluding lookup in the morphological nalyzer. With 
completely (and manually) morphologically disam- 
biguated input, parsing is instantaneous. Figure 4 
presents the input and the output of the parser for a 
sample Turkish sentence. Figure 5 shows the output 
258 
Input Sentence: Diinya Bankas~T/irkiye Direkthdi English: World Bank Turkey Director said that as a re- 
h/ikfimetin izledi~i ekonomik programln sonucunda sult of the economic program fonowed by the government, 
5nemfi achmlann atflchg\]m s6yledi, important steps were taken. 
Parser Output after 3 iterations: 
Parsel: 
<O00(dUnya+Noun+A3sg+Pnon+Nom@)OOc><COO(banka+Noun+A3sg+P3sg+Bom~)OcO> <OlO(tUrkiye+Noun+Prop+A3sg+Pnon+Nom@)Olc> 
<CC~(direkt~r+N~un+A3sg+~3sg+N~m@)s~><~1(hUkUmet+B~un+A3sg+~n~n+Gen@)1~s><~1(iz1e+verb+p~s)1~> 
<~(+Adj+Past~art+p3sg@)1m~><~11(ek~n~mik+Adj@)1~m><MM1(pr~gram+B~un+A3sg+~n~n+Gen~)~p> 
<P~(s~nuC+N~un+A3sg+P3s~?L~c@)~1~><~(~nem+N~un)~><~11(+Adj+with@)1~m><M1~(adIm+N~un+A3p1+Pn~n+Gen~)1~s> 
<S~(at+Verb)~><~(+verb+~ass+P~s)~><~(+I~un+~ast~art+A3sg+~3s~Acc@)~1~><~L~(s~y1e+verb+p~s+~ast+A3sg@)~> 
Parse2: 
<~(dUnya+I~un+A3sg+~n~n+N~m@)~c><C~(banka+N~un+A3sg+~3sg+I~m~)~c~><~1~(tUrkiye+N~un+pr~p+A3sg+pn~n+l~m@)~c> 
<CC~(direkt~r+N~un+A3sg+p3sg+N~m@)s~><~(hUkUmet+l~un+A3sg+pn~n+Gen@)1~s><~(iz1e+Verb+p~s)1~> 
<~(+Adj+Past~art+~3sg@)~m~><~(ek~n~mik+AdjQ)~m><RM1(pr~ram+N~un+A3s~+pn~n+GenQ)~p> 
<p~(s~nuC+N~un+A3sg+~3sg+L~)~1~><~1~(~nem+|~un)~><~1(+Adj+with@)1~m><M~1(adIm+N~un+A3p1+~n~n+Gen~)1~s> 
<SL1(at+Verb)1~><~1(+Verb+~ass+~s)1~><~(+N~un+~astpart+A3sg+~3sg+Acc@)1~><~(s~y1e+verb+p~s+~ast+A3sg@)~> 
The only difference in the two are parses are in the locative adjunct attachment (to verbs at and sSyle, 
highlighted with ***). 
Figure 4: Sample Input and Output of the parser 
Avg. Words /Sentence:  
Avg. IGs /Sentence:  
Avg. Parser  Iterations: 
Avg. Parses /Sentence:  
11.7 (4 - 24) 
16.4 (5 - 36) 
5.2 (3 - 8) 
23.9 (1 - 132) 
Table 1: Statistics from Parsing 50 Turkish Sen- 
tences 
of the parser processed with a Perl script to provide 
a more human-consumable presentation: 
6 D iscuss ion  and  Conc lus ions  
We have presented the architecture and implemen- 
tation of novel extended finite state dependency 
parser, with results from Turkish. We have formu- 
lated, but not yet implemented at this stage, two 
extensions. Crossing dependency links are very rare 
in Turkish and almost always occur in Turkish when 
an adjunct of a verb cuts in a certain position of a 
(discontinuous) noun phrase. We can solve this by 
allowing such adjuncts to use a special channel "be- 
low" the IG sequence so that limited crossing link 
configurations can be allowed. Links where the de- 
pendent is to the right of its head, which can happen 
with some of the word order variations (with back- 
grounding of some dependents of the main verb) can 
similarly be handled with a right-to-left version of 
Parser  which is applied during each iteration, but 
these cases are very rare. 
In addition to the reductionistic disambiguator 
that we have used just prior to parsing, we have im- 
plemented a number of heuristics to limit the num- 
ber of potentially spurious configurations that re- 
sult because of optionality in bracketing, mainly by 
enforcing obligatory bracketing for immediately se- 
quential dependency configurations (e.g., the com- 
plement of a postposition is immediately before it.) 
Such heuristics force such dependencies to appear in 
the first channel and hence prune many potentially 
useless configurations popping up in later stages. 
The robust parsing technique has been very instru- 
mental during the process mainly in the debugging 
of the grammar, but we have not made any substan- 
tial experiments with it yet. 
7 Acknowledgments  
This work was partially supported by a NATO 
Science for Stability Program Project Grant, TU- 
LANGUAGE made to Bilkent University. A portion 
of this work was done while the author was visit- 
ing Computing Research Laboratory at New Mexico 
State University. The author thanks Lauri Kart- 
tunen of Xerox Research Centre Europe, Grenoble 
for making available XRCE Finite State Tools. 
Re ferences  
Steven Abney. 1996. Partial parsing via finite state 
cascades. In Proceedings of the ESSLLI'96 Robust 
Parsing Workshop. 
Salah Ait-Mokhtar and Jean-Pierre Chanod. 1997. 
Incremental finite-state parsing. In Proceedings of 
ANLP'97, pages 72 - 79, April. 
Ciprian Chelba and et al 1997. Structure and esti- 
mation of a dependency language model. In Pro- 
cessings of Eurospeech '97. 
Jason Eisner. 1996. Three new probabilistic models 
for dependency parsing: An exploration. In Pro- 
ceedings of the 16th International Conference on 
Computational Linguistics (COLING-96), pages 
340-345, August. 
259 
S . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
c . . . . . . . . . . . . . . . . .  C s s . . . . . . . . . . . . . . . . .  R 
c - - -C  c c - - -CC s s - - -S  m s - - -NR p -  
dUnya banka  tUrk iye  d i rektOr  hUkUmet  i z le  ekonomik  program 
Noun Noun Noun Noun Noun Verb AdS AdS@ Noun 
A3sg A3sg Prop A3sg A3sg Pos PastPar t  A3sg 
Pnon P3sg A3sg P3sg Pnon P3sg@ Pnon 
Nos@ Sos@ Pnon Nom@ Gen@ Gen@ 
Nom@ 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  S 
1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  L S 
- -P  1 m---R s - - -SL  o - - -0  S 
sonuC Ones adIs at sOyle 
Noun Noun Adj Noun Verb Verb Noun Verb 
A3sg With@ A3pI Pass PastPart Pos 
P3sg Pnon Pos A3sg Past 
Loc@ Gen@ P3sg A3sg@ 
Acc@ 
Figure 5: Dependency tree for the second parse 
Gregory Grefenstette. 1996. Light parsing as finite- 
state filtering. In ECAI '96 Workshop on Ex- 
tended finite state models of language. August. 
Timo J~irvinen and Pasi Tapanainen. 1998. Towards 
an implementable dependency grammar. In Pro- 
ceedings of COLING/ACL'98 Workshop on Pro- 
cessing Dependency-based Grammars, pages 1-10. 
Ronald M. Kaplan and Martin Kay. 1994. Regular 
models of phonological rule systems. Computa- 
tional Linguistics, 20(3):331-378, September. 
Lauri Karttunen, Jean-Pierre Chanod, Gregory 
Grefenstette, and Anne Schiller. 1996. Regu- 
lar expressions for language ngineering. Natural 
Language Engineering, 2(4):305-328. 
Lauri Karttunen. 1998. The proper treatment of 
optimality theory in computational linguistics. In 
Lauri Karttunen and Kemal Oflazer, editors, Pro- 
ceedings of the International Workshop on Finite 
State Methods in Natural Language Processing- 
FSMNLP, June. 
Kimmo Koskenniemi, Pasi Tapanainen, and Atro 
Voutilainen. 1992. Compiling and using finite- 
state syntactic rules. In Proceedings of the 14th 
International Conference on Computational Lin- 
guistics, COLING-92, pages 156-162. 
Kimmo Koskenniemi. 1990. Finite-state parsing 
and disambiguation. In Proceedings of the 13th 
International Conference on Computational Lin- 
guistics, COLING'90, pages 229 - 233. 
John Lafferty, Daniel Sleator, and Davy Temper- 
ley. 1992. Grammatical trigrams: A probabilis- 
tic model of link grammars. In Proceedings of the 
1992 AAAI Fall Symposium on Probablistic Ap- 
proaches to Natural Language. 
Bong Yeung Tom Lai and Changning Huang. 1994. 
Dependency grammar and the parsing of Chinese 
sentences. In Proceedings of the 1994 Joint Con- 
ference of 8th ACLIC and 2nd PaFoCol. 
Dekang Lin. 1996. On the structural complexity of 
natural anguage sentences. In Proceedings of the 
16th International Conference on Computational 
Linguistics (COLING-96). 
Igor A. Mel~uk. 1988. Dependency Syntax: Theory 
and Practice. State University of New York Press. 
Mehryar Mohri, Fernando Pereira, and Michael Ri- 
ley. 1998. A rational design for a weighted finite- 
state transducer library. In Lecture Notes in Com- 
puter Science, 1.~36. Springer Verlag. 
Kemal Oflazer. 1993. Two-level description of Turk- 
ish morphology. In Proceedings of the Sixth Con- 
ference of the European Chapter of the Associa- 
tion for Computational Linguistics, April. A full 
version appears in Literary and Linguistic Com- 
puting, Vol.9 No.2, 1994. 
Jane J. Robinson. 1970. Dependency structures and 
transformational rules. Language, 46(2):259-284. 
Emmanuel Roche. 1997. Parsing with finite state 
transducers. In Emmanuel Roche and Yves Sch- 
abes, editors, Finite-State Language Processing, 
chapter 8. The MIT Press. 
Daniel Sleator and Davy Temperley. 1991. Parsing 
English with a link grammar. Technical Report 
CMU-CS-91-196, Computer Science Department, 
Carnegie Mellon University. 
Pasi Tapanainen and Timo J~rvinen. 1997. A non- 
projective dependency parser. In Proceedings of 
ANLP'97, pages 64 - 71, April. 
Deniz Y/iret. 1998. Discovery of Linguistic Rela- 
tions Using Lexical Attraction. Ph.D. thesis, De- 
partment of Electrical Engineering and Computer 
Science, Massachusetts Institute of Technology. 
260 
