Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 140?148,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Relation Annotation for Understanding Research Papers
Yuka Tateisi? Yo Shidahara? Yusuke Miyao? Akiko Aizawa?
?National Institute of Informatics, Tokyo, Japan
{yucca,yusuke,aizawa}@nii.ac.jp
?Freelance Annotator
yo.shidahara@gmail.com
Abstract
We describe a new annotation scheme for
formalizing relation structures in research
papers. The scheme has been developed
through the investigation of computer sci-
ence papers. Using the scheme, we are
building a Japanese corpus to help develop
information extraction systems for digital
libraries. We report on the outline of the
annotation scheme and on annotation ex-
periments conducted on research abstracts
from the IPSJ Journal.
1 Introduction
Present day researchers need services for search-
ing research papers. Search engines and pub-
lishing companies provide specialized search ser-
vices, such as Google Scholar, Microsoft Aca-
demic Search, and Science Direct. Academic so-
cieties provide archives of journal articles and/or
conference proceedings such as the ACL Anthol-
ogy. These services focus on simple keyword-
based searches as well as extralinguistic relations
among research papers, authors, and research top-
ics. However, because contemporary research is
becoming increasingly complicated and interre-
lated, intelligent content-based search systems are
desired (Banchs, 2012). A typical query in compu-
tational linguistics could be what tasks have CRFs
been used for?, which includes the elements of
a typical schema for searching research papers;
researchers want to find relationships between a
technique and its applications (Gupta and Man-
ning, 2011). Answers to this query can be found
in various forms in published papers, for example,
(1) CRF-based POS tagging has achieved state-of-
the-art accuracy.
(2) CRFs have been successfully applied to se-
quence labeling problems including POS tagging
and named entity recognition.
(3) We apply feature reduction to CRFs and show
its effectiveness in POS tagging.
(4) This study proposes a new method for the ef-
ficient training of CRFs. The proposed method is
evaluated for POS tagging tasks.
Note that the same semantic relation, i.e., the
use of CRFs for POS tagging, is expressed by var-
ious syntactic constructs: internal structures of the
phrase in (1), clause-level structures in (2), inter-
clause structures in (3), and discourse-level struc-
tures in (4). This implies that an integrated frame-
work is required to represent semantic relations for
phrase-level, clause-level, inter-clause level, and
discourse-level structures. Another interesting fact
is that we can recognize various fragments of in-
formation from single texts. For example, from
sentence (1), we can identify CRF is applied to
POS tagging, state-of-the-art accuracy is achieved
for POS tagging, and CRFs achieve high POS tag-
ging accuracy, all of which is valuable content for
different search requests. This indicates that we
need a framework that can cover (almost) all con-
tent in a text.
In this paper we describe a new annotation
scheme for formalizing typical schemas for repre-
senting relations among concepts in research pa-
pers, such as techniques, resources, and effects.
Our study aims to establish a framework for rep-
resenting the semantics of research papers to help
construct intelligent search systems. In particular,
we focus on the formalization of typical schemas
that we believe exemplify common query charac-
teristics.
From the above observations, we have de-
veloped the following criteria for our proposed
framework: use the same scheme for annotating
contents in all levels of linguistic structures, an-
notate (almost) all contents presented in texts, and
capture relations necessary for surveying research
papers. We investigated 71 computer science ab-
stracts (498 sentences) and defined an annotation
140
scheme comprising 16 types of semantic relations.
Computer science is particularly suitable for our
purpose because it is primarily concerned with ab-
stract concepts rather than concrete entities, which
are typically the primary focus of empirical sci-
ences such as physics and biology. In addition,
computer and computational methods can be ap-
plied to an extraordinarily wide range of top-
ics; computer science papers might discuss a bus
timetable (for automatic optimization), a person?s
palm (as a device for projecting images), or look-
ing over another person!Gs shoulder (to obtain pass-
words). Therefore, to annotate all computer sci-
ence papers, we cannot develop predefined entity
ontologies, which is the typical approach taken in
biomedical text mining (Kim et al, 2011).
However, most computer science papers have
characteristic schemata: the papers describe a
problem, postulate a method, apply the method to
the problem using particular data or devices, and
perform experiments to evaluate the method. The
typical schemata clearly represent the structure of
interests in this research field. Therefore, we can
focus on typical schemata, such as application of
a method to a problem and evaluation of a method
for a task. As we will demonstrate in this paper,
the proposed annotation scheme can cover almost
all content, from phrase levels to discourse levels,
in computer science papers.
Note that this does not necessarily mean that our
framework can only be applied to computer sci-
ence literature. The characteristics of the schemata
described above are universal in contemporary sci-
ence and engineering, and many other activities in
human society. Thus, the framework presented in
this study can be viewed as a starting point for re-
search focusing on representative schemata of hu-
man activities.
2 Related Work
Traditionally, research on searching research pa-
pers has focused more on the social aspects of
papers and their authors, such as citation links
and co-authorship analysis implemented in the
aforementioned services. Recently, research on
content-based analysis of research papers has been
emerging.
For example, methods of document zoning have
been proposed for research papers in biomedicine
(Mizuta et al, 2006; Agarwal and Yu, 2009; Li-
akata et al, 2010; Guo et al, 2011; Varga et
al., 2012), and chemistry and computational lin-
guistics (Teufel et al, 2009). Zoning provides
a sentence-based information structure of papers
to help identify the components such as the pro-
posed method and the results obtained in the study.
As such, zoning can narrow down the sections of
a paper in which the answer to a query can be
found. However, zoning alone cannot always cap-
ture the relation between the concepts described in
the sections as it focuses on relation at a sentence
level. For example, the examples (1), (2), (3) in the
previous section require intra-sentence analysis to
capture the relation between CRF and POS tag-
ging. Our annotation scheme, which can be seen
as conplementary to zoning, attempts to provide
a structure for capturing the relationship between
concepts at a finer-grained level than a sentence.
Establishing semantic relations among scien-
tific papers has also been studied. For example,
the ACL Anthology Searchbench (Scha?fer et al,
2011) provides querying by predicate-argument
relations. The system accepts specifications of
subject, predicate, and object, and searches for
texts that semantically match the query using the
results from an HPSG parser. It can also search
by topics automatically extracted from the papers.
Gupta and Manning (2011) proposed a method for
extracting Focus, Domain, and Technique from pa-
pers in the ACL anthology: Focus is a research
article?s main contribution, Domain is an applica-
tion domain, and Technique is a method or a tool
used to achieve the Focus. The change in these as-
pects over time is traced to measure the influence
of research communities on each other. Fukuda et
al. (2012) developed a method of technical trend
analysis that can be applied to both patent appli-
cations and academic papers, using the distribu-
tion of named entities. However, as processes and
functions are key concepts in computer science,
elements are often described in a unit with its own
internal structures which include data, systems,
and other entities as substructures. Thus, tech-
nical concepts such as technique cannot be cap-
tured fully by extracting named entities. Gupta
and Manning (2011) analyzed the internal struc-
tures of concepts syntactically using a dependency
parser, but did not further investigate the structure
semantically.
In addition to the methodological aspects of re-
search, i.e., what techniques are applied to what
domain, a research paper can include other infor-
141
mation that we also want to capture, such as how
the author evaluates current systems and methods
or the previous efforts of others. An attempt to
identify the evaluation and other meta-aspects of
scientific papers was made by Thompson et al
(2011), which, on top of the biomedical events
annotated in the GENIA event corpus (Kim et
al., 2008), annotated meta-knowledge such as the
certainty level of the author, polarity (positive?
negative), and manner (strong?weak) of events, as
well as source (whether the event is attributed to
the current study or previous studies), along with
the clue mentioned in the text. For in-domain
relations within and between the events, they re-
lied on the underlying GENIA annotation, which
maps events and their participants to a subset of
Gene Ontology (The Gene Ontology Consortium,
2000), a standard ontology in genome science.
We cannot assume the existence of standard do-
main ontology in the variety of domains to which
computer systems are applied, as was mentioned
in Section 1. On the other hand, using domain-
general linguistic frameworks, such as FrameNet
(Ruppenhofer et al, 2006) or the Lexical Concep-
tual Structure (Jackendoff, 1990) is also not sat-
isfactory for our purpose. These frameworks at-
tempt to identify the relations lexicalized by verbs
and their case arguments; however, they do not
consider discourse or other levels of linguistic rep-
resentation. In addition, relying on a linguistic the-
ory requires that annotators understand linguistics.
Most computer scientists, the best candidates for
performing the annotation task, would not have the
necessary knowledge of linguistics and would re-
quire training, which would increase costs for cor-
pus annotation.
3 Annotation Scheme
The principle is to employ a uniform structure to
represent semantic relations in scientific papers
in phrase-level, clause-level, inter-clause level,
and discourse-level structures. For this purpose,
a bottom-up strategy that identifies relations be-
tween the entities mentioned is used. This strat-
egy is similar to dependency parsing/annotation,
which identifies the relations between constituents
to find the overall structure of sentences.
We did not want the relations to be uncondi-
tionally concrete and domain-specific, because, as
mentioned in the previous section, new concepts
and relations that may not be expressed by pre-
In this paper, we propose a novel strategy for
parallel preconditioning of large scale linear
systems by means of a two-level approximate
inverse technique with AISM method. Accord-
ing to the numerical results on an origin 2400 by
using MPI, the proposed parallel technique of
computing the approximate inverse makes the
speedup of about 136.72 times with 16 proces-
sors.
Figure 1: Sample Abstract
defined (concrete, domain-specific) concepts and
relations may be created. For the same reason,
we did not set specific entity types on the basis of
domain ontology. We simply classified entities as
?general object,? ?specific object,? and ?measure-
ment.?
To illustrate our scheme, consider the two-
sentence abstract1 shown in Figure 12.
In the first sentence, we can read that a method
called two-level approximate inverse is used for
parallel preconditioning (1), the preconditioning
is applied to large-scale linear systems, the AISM
method is a subcomponent or a substage of the
two-level technique, and the author claims that the
use of two-level approximate inverse is a novel
strategy.
In the second sentence, we can read that the
author has conducted a numerical experiment,
the experiment was conducted on an origin 2400
(a computer system), message Passing Interface
(MPI, a standardized method for message passing)
was used in the experiment, the proposed parallel
technique was 136.72 times quicker than existing
methods, and the speedup was achieved using 16
processors.
In addition, by comparing the two sentences, we
can determine that the proposed parallel technique
in the second sentence refers to the parallel pre-
conditioning using two-level approximate inverse
mentioned in the first sentence. Consequently, we
can infer the author?s claim that the parallel pre-
conditioning using two-level approximate inverse
achieved 136.72 times speedup.
We define binary relations including
APPLY TO(A, B) (A method A is applied
to achieve the purpose B or used for do-
ing B), EVALUATE(A, B) (A is evaluated as
1Linjie Zhang, Kentaro Moriya and Takashi Nodera.
2008. Two-level Parallel Computation for Approximate In-
verse with AISM Method. IPSJ Journal, 48 (6): 2164-2168.
2Although the annotation was done for abstracts in
Japanese, we present examples in English except where we
discuss issues that we believe are specific to Japanese.
142
APPLY TO(two-level approximate inverse, parallel preconditioning)
APPLY TO(parallel preconditioning, large scale linear systems)
SUBCONCEPT(AISM method, two-level approximate inverse)
EVALUATE(two-level approximate inverse, novel)
RESULT(numerical results, 136.72 times speedup)
CONDITION(origin 2400, 136.72 times speedup)
APPLY TO(MPI, numerical results)
EVALUATE(the proposed parallel technique, 136.72 times speedup)
CONDITION(16 processors, 136.72 times speedup)
EQUIVALENCE(the proposed parallel technique, two-level approximate inverse)
Figure 2: Relations Found in the Sentences in Figure 1
B), SUBCONCEPT(A, B) (A is a part of B),
RESULT(A, B) (The result of experiment A is B),
CONDITION(A, B) (The condition A holds in
situation B), and EQUIVALENCE(A, B) (A and
B refer to the same entity), with which we can
express the relations mentioned in the example, as
shown in Figure 2.
Note that it is the use of two-level approximate
inverse for parallel preconditioning(A) that the au-
thor claims to be novel. However, the relation in A
is already represented by the first APPLY TO rela-
tion. Consequently, it is sufficient to annotate the
EVALUATE relation between two-level approxi-
mate inverse and novel. This is approximately
equivalent to paraphrasing the use of two-level ap-
proximate inverse for parallel preconditioning is
novel as two-level approximate inverse used for
parallel preconditioning is novel. The same holds
for the equivalence relation involving the proposed
method.
Expressing the content as the set of relations fa-
cilitates discovery of a concept that plays a par-
ticular role in the work. For example, if a reader
wants to know the method for achieving paral-
lel preconditioning, X, which satisfies the relation
APPLY TO(X, parallel preconditioning) must be
searched for. By using the APPLY TO relations
mentioned in Figure 2 and inference on an is-a re-
lation expressed by the SUBCONCEPT, we can ob-
tain the result that AISM method is used for paral-
lel preconditioning.
After a series of trial annotations on 71 abstracts
from the IPSJ Journal (a monthly peer-reviewed
journal published by the Information Processing
Society of Japan), the following tag set was fixed.
The annotation was conducted by the two of the
authors of this paper.
3.1 Entity and Relation Types
The current tag set has 16 relation types and three
entity types. An entity is whatever can be an argu-
Type Definition Example
OBJECT the name of concrete entities such as
a system, a person, and a company
Origin
2400, SGI
MEASURE value, measurement, necessity, obli-
gation, expectation, and possibility
novel,
136.72
TERM any other
Table 1: Entity Tags
ment or a participant in a relation. Entity types
are OBJECT, MEASURE, or TERM, as shown in
Table 1. Note that, unlike most schemes where
the term entity refers to a nominal (named entity),
in our scheme, almost all syntactic types of con-
tent words can be an entity, including numbers,
verbs, adjectives, adverbs, and even some auxil-
iaries. The 16 types of relations are shown in Ta-
ble 2. They are binary relations are directed from
A to B.
All relations except EVALUATE COMPARE, and
ATTRIBUTE can hold between any types of en-
tity. EVALUATE and COMPARE relations hold
between an entity (of any type) and an entity
of the MEASURE type. The entities involved
in an ATTRIBUTE relation must not be of the
MEASURE type.
The INPUT and OUTPUT relations were intro-
duced to deal with the distinction between the data
and method used in computer systems. We ex-
tend the use of the scheme to annotate the in-
ner structure of sentences and predicates, by es-
tablishing the relations between verbs and their
case elements. For example, in automatically
generated test data, obviously test data is an
output of the action of generate, and automati-
cally is the manner of generation. We annotate
the test data as an OUTPUT and automatically
as an ATTRIBUTE of generate. In another ex-
ample, a protocol that combines biometrics and
zero-knowledge proof, the protocol is the product
of an action of combining biometrics and zero-
143
Type Definition Example
APPLY TO(A, B) A method A is applied to achieve the purpose B or used for
conducting B
CRFA-based taggerB
RESULT(A, B) A results in B in the sense that B is either an experimental
result, a logical conclusion, or a side effect of A
experimentA shows the increaseB in F-
score compared to the baseline
PERFORM(A, B) A is the agent of an intentional action B a frustrated playerA of a gameB
INPUT(A, B) A is the input of a system or a process B, A is something
obtained for B
corpusA for trainingB
OUTPUT(A, B) A is the output of a system or a processB, A is something
generated from B
an imagea displayedB on a palm
TARGET(A, B) Ais the target of an action B, which does not suffer alteration to driveB a busA
ORIGIN(A, B) A is the starting point of action B to driveB from ShinjukuA
DESTINATION(A, B) A is the ending point of action B an image displayedB on a palmA
CONDITION(A, B) The condition A holds in situation B, e.g, time, location, ex-
perimental condition
a surveyB conducted in Indiaa
ATTRIBUTE(A, B) A is an attribute or a characteristic of B accuracyA of the taggerB
STATE(A, B) A is the sentiment of a person B other than the author, e.g. a
user of a computer system or a player of a game
a frustratedA playerB of a game
EVALUATE(A, B) A is evaluated as B in comparison to C experiment shows an increaseB
COMPARE(C, B) in F?scoreA compared to the baselineC
SUBCONCEPT(A, B) A is-a, or is a part-of B a corpusA such as PTBa
EQUIVALENCE(A, B) terms A and B refer to the same entity: definition, abbrevia-
tion, or coreference
DoSB (denial ? of ? serviceA) attack
SPLIT(A, B) a term is split by parenthesical expressions into A and B DoSB (denial-of-service) attackA
Table 2: Relation Tags
knowledge proof. Therefore, both biometrics and
zero-knowledge proof are annotated as INPUTs of
combines, and protocol is annotated as OUTPUT
of combines. This scheme is not only used for
computer-related verbs, but is further extended
to any verb phrases or phrases with nominalized
verbs. In change in a situation, situation is an-
notated as both INPUT and OUTPUT of change.
It is as if we regard change as a machine that
changes something, and when we input a situa-
tion, the change-machine processes it and output
a different situation. Similarly, in evolution of mo-
bile phones, mobile phones is annotated as both
INPUT and OUTPUT of evolution. Here we re-
gard evolution as a machine, and when we input
(old-style) mobile phones, the evolution-machine
processes them and outputs (new-style) mobile
phones. We have found that a wide variety of pred-
icates can be interpreted using these relations.
3.2 Other Features
Although we aim to annotate all possible relations
mentioned, some conventions are introduced to re-
duce the workload.
First, we do not annotate the structure within
entities. No nested entities are allowed, and com-
pound words are treated as a single word. In ad-
dition, polarity (negation) is not expressed as a re-
lation but as a part of an entity. We assume that
the internal structure of entities can be analyzed
by mechanisms such as technical term recognition.
On the other hand, nested and crossed relations are
allowed.
Second, we do not annotate words that indicate
the existence of relations. This is because the re-
lations are usually indicated by case markers and
punctuation 3 and marking them up was found to
be a considerable mental workload. In addition,
words and phrases that directly represent the re-
lations themselves are not annotated as entities.
For example, in CG iteration was applied to the
problem, we directly CG relation and the problem
directly with APPLY TO and skip the phrase was
applied to.
Third, relations other than EQUIVALENCE and
SUBCONCEPT are annotated within a sentence.
We assume that the discourse-level relation can be
inferred by the composition of relations.
In addition, the annotation of frequent verbs and
their case elements was examined in the trial pro-
cess. Verbs were classified, according to the pat-
tern of the annotated relation with the case ele-
ments. For example, verbs semantically similar to
assemble and compile form a class. The semantic
role of the direct object of these verbs varies by
context. For example, the materials in phrases like
compile source codes or the product in phrases like
3This is in the case with Japanese. In languages such as
English, there may be no trigger words, as the semantic rela-
tions are often expressed by the structure of sentences.
144
compile the driver from the source codes. In our
scheme, the former is the INPUT of the verb, and
the latter is the OUTPUT of the verb. Another ex-
ample is the class of verbs that includes learn and
obtain. The direct object (what is learned) is the
INPUT to the system but is also the result or an
output of the learning process. In such cases, we
decided that both INPUT and OUTPUT should be
annotated between the verb and its object.
Other details of annotation fixed in the process
of trial annotation include:
1) The span of entities, which is determined to be
the longest possible sequences delimited by case
suffix (-ga,-wo, etc.) in the case of nominals and to
separate the -suru suffix of verbs and the -da suffix
of adjectives but retain other conjugation suffixes;
2) How to annotate evaluation sentences involv-
ing nouns derived from adjectives that imply eval-
uation and measurement, such as necessity, diffi-
culty, and length. The initial agreement was that
we would consider that they lose MEASURE-ness
when nominalized; however, with the similarity of
Japanese expressions hitsuyou/mondai de aru (is
necessary/problematic) and hitsuyou/mondai ga
aru(there is a necessity/problem), there was con-
fusion about which word should be the MEASURE
argument necessary for the EVALUATE relation.
It was determined that, for example, in hit-
suyou/mondai de aru, de aru, a copula, is ig-
nored and hitsuyou/mondai is the MEASURE. In
hitsuyou/mondai ga aru, aru is the MEASURE;
3) How to annotate phrases like the tagger was
better in precision, where it can be understood that
the system is evaluated as being better in precision.
While what is actually measured in the evaluation
process described in the paper is the precision (an
attribute) of the tagger and the sentence has almost
the same meaning as the tagger?s precision was
better, the surface (syntactic) subject of is better
is the tagger. This can lead to two possibilities
for the target of the EVALUATE relation. We de-
cided that the EVALUATE relation holds between
precision and better, and the ATTRIBUTE relation
holds between precision and tagger, as illustrated
in Figure 3.
A set of annotation guidelines was compiled as
the result of the trial annotation, including the clas-
sifications and the pattern of annotation on fre-
quent verbs and their arguments.
Figure 3: Annotation of the tagger was better in
precision
Entity Relation
Conunt % Conunt %
Total 1895 100.0 Total 2269 100.0
OK 1658 87.5 OK 1110 48.9
Type 56 3.0 Type 250 11.0
Span 67 3.5 Direction 6 0.3
Direction+Type 106 4.7
None 114 6.0 None 797 35.1
Table 3: Tag Counts
4 Annotation Experiment
We conducted an experiment on another 30 ab-
stracts (197 sentences) from the IPSJ Journal. The
two annotators who participated in the develop-
ment of the guidelines annotated the abstracts in-
dependently, and inter-annotator discrepancy was
checked. The annotation was performed man-
ually using the brat annotation tool(Stenetorp et
al., 2012). No automatic preprocessing was per-
formed. Figure 4 shows the annotation results for
the abstract shown in Figure 1. The 30 pairs of an-
notation results were aligned automatically; The
results are shown in Tables 3, 4, and 5.
Table 3 shows the matches between the two
annotators. ?Total? denotes the count of enti-
ties/relations that at least one annotator found,
?OK? denotes complete matches, ?Type? denotes
cases where two annotations on the same span
have different entity/relation types, ?Span? de-
notes entities where two annotations partially
overlap, ?Direction? denotes the count of relations
where (only) the direction is different, and ?Direc-
tion+Type?denotes relations where the same pair
of entities were in different types of relation and
in opposite directions, and ?None? denotes cases
where no counterpart was found in the other re-
sult.
Tables 4 and 5 are the confusion matrices for
entity type and relation type, respectively. The
differences in the span and direction are ignored.
Agreement in F-score calculated in the same man-
ner as in Brants (2000) for each relation is shown
in column F, with the overall (micro-average) F-
score shown in the bottom row of column F.
If we assume the number of cases that none of
145
Figure 4: Annotation Results with brat
TERM OBJECT MEASURE NONE Total F(%)
TERM 1458 2 38 14 1512 94.9
OBJECT 0 17 0 0 17 94.4
MEASURE 28 0 238 18 284 83.8
None 74 0 8 X 82
Total 1560 19 284 32 93.0
Table 4: Confusion Matrix for Entity
the annotators recognized (the value of the cell
X in the tables) to be zero, the observed agree-
ment and Cohen?s ? coefficient are 90.3% and
70.0% for entities, and 49.3% and 43.5% for re-
lations, respectively. If we ignore the count for the
cases where one annotator did not recognize the
entity/relation (?None? rows and columns in the
tables), the observed agreement and ? are 96.1%
and 89.3% for entities, and 76.1% and 74.3% for
relations, respectively. The latter statistics indi-
cate the agreement on types for entities/relations
that both annotators recognized.
These results show that entity annotation was
consistent between the annotators but the agree-
ment for relation annotation varied, depending on
the relation type. Table 5 shows that agreement
for DESTINATION, ORIGIN, EVALUATE, and
SPLIT was reasonably high, but was low for
CONDITION and TARGET. The rise in agreement
(simple and ?) by excluding cases where only one
annotator recognized the relation indicate that the
problem is recognition, rather than classification,
of relations4.
From the investigation of the annotated text, the
following was found:
(1) ATTRIBUTE/CONDITION decision was in-
consistent in phrases involving EVALUATE rela-
tion, such as the disk space is smaller for the im-
age (Figure 5). The EVALUATE relation between
the disk space and smaller was agreed; however,
the two annotators recognized different relations
between the image and other words. One annota-
4The same observation was true for entities
tor recognized the ATTRIBUTE relation between
the disk space and the image (?the disk space as a
feature of the image is smaller?). The other recog-
nized the CONDITION relation between the image
and smaller (?the disk space is smaller in the case
of the image?).
(2) We were not in complete agreement about
skipping phrases that directly represent a relation.
The expressions to be skipped in the 71 trial ab-
stracts were listed in the guidelines; however, it is
difficult to exhaust all such expressions.
(3) In the case of some verbs, an argument can
be INPUT and OUTPUT simultaneously (Section
3.1). We agreed that an object that undergoes alter-
ation in a process should be tagged as both INPUT
and OUTPUT but one that does not undergo al-
teration or which is just moved is the TARGET.
Conflicts occurred for verbs that denote preven-
tion of some situations such as prevent, avoid, and
suppress, as illustrated in Figure 6. One annota-
tor claimed that the possibility of DoS attacks is
reduced to zero; hence the argument of the verb
should be annotated with INPUT and OUTPUT.
The other claims that since the DoS attack itself
does not change, it is a TARGET.
(4) In a coordination expression, logical inference
may be implicitly stated. For example, in it re-
quires the linguistic knowledge and is costly, the
reason for costly is likely to be the need for lin-
guistic knowledge, i.e., employment of an expert
linguist. However, the relation is not readily ap-
parent. We wanted to capture the relation in such
cases, but the disagreement shows that it is diffi-
cult to judge such a relation consistently.
(5) The decision on whether to split expressions
like XX dekiru and XX kanou (can/able to XX) was
also problematic. The guideline was to split them.
This contradicts the decision for the compound
words in general that we do not split them; how-
ever, we determined that dekiru/kanou cases had
146
APP ATT COMP COND DEST EQU EVAL IN ORIG OUT PER RES SPL STA SUB TAR None Total F(%)
APPLY TO 136 9 0 2 1 1 2 10 1 0 0 3 0 0 1 0 65 231 53.0
ATTRIBUTE 14 154 0 19 6 0 9 5 1 0 7 1 0 0 3 0 28 247 59.7
COMPARE 0 0 6 1 0 0 0 0 0 0 0 0 0 0 0 0 4 11 54.5
CONDITION 4 11 1 77 0 0 1 4 0 0 0 5 0 0 0 0 49 152 48.7
DESTINATION 6 0 0 0 39 0 0 0 0 1 0 0 0 0 0 0 4 50 77.2
EQUIVALENCE 4 1 0 1 0 54 0 0 0 0 0 0 0 0 4 0 23 87 60.0
EVALUATE 0 11 0 0 0 0 215 3 0 9 0 0 0 0 0 1 41 280 76.1
INPUT 12 2 0 0 0 1 4 96 0 11 0 0 0 0 0 9 15 150 58.7
ORIGIN 0 0 0 0 0 0 0 0 16 0 0 0 0 0 0 0 2 18 78.0
OUTPUT 2 1 0 3 0 0 4 23 0 141 0 0 0 0 0 18 37 229 56.5
PERFORM 1 0 0 0 0 0 0 0 0 0 19 0 0 0 0 0 2 22 74.5
RESULT 8 1 0 0 0 0 1 1 0 0 0 38 0 0 0 0 22 71 54.3
SPLIT 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 80.0
STATE 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
SUBCONCEPT 14 10 0 3 0 4 5 0 0 2 0 0 0 0 81 0 34 153 58.1
TARGET 6 2 1 3 2 0 7 12 0 14 1 0 0 0 0 42 6 96 47.7
None 75 67 3 55 3 33 37 23 5 92 2 22 1 0 37 10 X 465
Total 282 269 11 164 51 93 285 177 23 270 29 69 3 0 126 80 332 59.8
Table 5: Confusion Matrix for Relation
Figure 5: ATTRIBUTE/CONDITION Disagreement
Figure 6: INPUT/OUTPUT/TARGET Disagreement
to be exceptions because the possibility of XX is
expressed by dekiru/kanou and it seemed natural
to relate XX and dekiru/kanou with EVALUATE.
Unfortunately, confusion about splitting them re-
mains.
5 Conclusions
We set up a scheme to annotate the content of re-
search papers comprehensively. Sixteen semantic
relations were defined, and guidelines for anno-
tating semantic relations between concepts using
the relations were established. The experimen-
tal results on 30 abstracts show that fairly good
agreement was achieved, and that while entity-
and relation-type determination can be performed
consistently, determining whether a relation exists
between particular pairs of entities remains prob-
lematic. We also found several discrepancy pat-
terns that should be resolved and included in a fu-
ture revision of the guidelines.
Traditionally, in semantic annotation of texts
in the science/engineering domains, corpus cre-
ators focus on specific types of entities or events
in which they are interested. On the other hand,
we did not assume such specific types of entities
or events, and we attempted to design a scheme
that annotates more general relations in computer
science/engineering domain.
Although the annotation is conducted for com-
puter science abstracts in Japanese, we believe the
scheme can be used for other languages, or for
the broader science/engineering domains. The an-
notated corpus can provide data for constructing
comprehensive semantic relation extraction sys-
tems. This would be challenging but worthwhile
since such systems are in great demand. Such
relation extraction systems will be the basis for
content-based retrieval and other applications, in-
cluding paraphrasing and translation.
The abstracts annotated in the course of the ex-
periment have been cleaned up and are available
on request. We are planning to increase the vol-
ume and make the corpus widely available.
In the future, we will assess machine-learning
performance and incorporate the relation extrac-
tion mechanisms into search systems. Comparison
of the annotated structure and the structures that
can be given by existing semantic theories could
be an interesting theoretical subject for future re-
search.
Acknowledgments
This study was partially supported by the Japan
Ministry of Education, Culture, Sports, Science
and Technology Grant-in-Aid for Scientific Re-
search (B) No. 22300031.
147
References
Shashank Agarwal and Hong Yu. 2009. Automatically
classifying sentences in full-text biomedical articles
into introduction, methods, results and discussion.
Bioinformatics, 25(23):3174?3180.
Rafael E. Banchs, editor. 2012. Proceedings of the
ACL-2012 Special Workshop on Rediscovering 50
Years of Discoveries. Association for Computational
Linguistics.
Thorsten Brants. 2000. Inter-annotator agreement for
a German newspaper corpus. In Proceedings of the
Second International Conference on Language Re-
sources and Evaluation.
Satoshi Fukuda, Hidetsugu Nanba, and Toshiyuki
Takezawa. 2012. Extraction and visualization of
technical trend information from research papers
and patents. In Proceedings of the 1st International
Workshop on Mining Scientific Publications.
Yufan Guo, Anna Korhonen, and Thierry Poibeau.
2011. A weakly-supervised approach to argumen-
tative zoning of scientific documents. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing, pages 273?283.
Sonal Gupta and Christopher D Manning. 2011. An-
alyzing the dynamics of research by extracting key
aspects of scientific papers. In Proceedings of 5th
IJCNLP.
Ray Jackendoff. 1990. Semantic Structures. The MIT
Press.
Jin-Dong Kim, Tomoko Ohta, and Jun ichi Tsujii.
2008. Corpus annotation for mining biomedical
events from literature. BMC Bioinformatics, 9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of bionlp shared task 2011. In Proceed-
ings of BioNLP Shared Task 2011 Workshop, pages
1?6.
Maria Liakata, Simone Teufel, Advaith Siddharthan,
and Colin Batchelor. 2010. Corpora for concep-
tualisation and zoning of scientific papers. In Pro-
ceedings of LREC 2010.
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel
Collier. 2006. Zone analysis in biology articles as a
basis for information extraction. International Jour-
nal of Medical Informatics, 75(6):468?487.
Josef Ruppenhofer, Michael Ellsworth, Miriam R.L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2006. FrameNet II: Extended Theory and
Practice. International Computer Science Institute.
Ulrich Scha?fer, Bernd Kiefer, Christian Spurk, Jo?rg
Steffen, and Rui Wang. 2011. The ACL anthology
searchbench. In Proceedings of the ACL-HLT 2011
System Demonstrations, pages 7?13.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a web-based tool for NLP-assisted
text annotation. In Proceedings of the Demonstra-
tions Session at EACL.
Simone Teufel, Advaith Siddharthan, and Colin Batch-
elor. 2009. Towards discipline-independent ar-
gumentative zoning: evidence from chemistry and
computational linguistics. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 1493?1502.
The Gene Ontology Consortium. 2000. Gene ontol-
ogy: tool for the unification of biology. Nature Ge-
netics, 25(1):25?29.
Paul Thompson, Raheel Nawaz, John McNaught, and
Sophia Ananiadou. 2011. Enriching a biomedi-
cal event corpus with meta-knowledge annotation.
BMC Bioinformatics, 12.
Andrea Varga, Daniel Preotiuc-Pietro, and Fabio
Ciravegna. 2012. Unsupervised document zone
identification using probabilistic graphical models.
In Proceedings of LREC 2012, pages 1610?1617.
148
