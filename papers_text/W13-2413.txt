Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 84?93,
Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational Linguistics
On Named Entity Recognition in Targeted Twitter Streams in Polish
Jakub Piskorski
Linguistic Engineering Group
Polish Academy of Sciences
Jakub.Piskorski@ipipan.waw.pl
Maud Ehrmann
Department of Computer Science
Sapienza University of Rome
ehrmann@di.uniroma1.it
Abstract
This paper reports on some experiments
aiming at tuning a rule-based NER sys-
tem designed for detecting names in Pol-
ish online news to the processing of tar-
geted Twitter streams. In particular, one
explores whether the performance of the
baseline NER system can be improved
through the incremental application of
knowledge-poor methods for name match-
ing and guessing. We study various set-
tings and combinations of the methods and
present evaluation results on five corpora
gathered from Twitter, centred around ma-
jor events and known individuals.
1 Introduction
Recently, Twitter emerged as an important so-
cial medium providing most up-to-date informa-
tion and comments on current events of any kind.
This results in an ever-growing interest of vari-
ous organizations in tools for real-time monitor-
ing of Twitter streams to collect their business-
specific information therefrom for analysis pur-
poses. Since monitoring the entire Twitter stream
appears to be unfeasible due to the high volume
of published tweets, one usually monitors targeted
Twitter streams, i.e., streams of tweets potentially
satisfying specific information needs.
Applications for monitoring Twitter streams
usually require named entity recognition (NER)
capacity. However, due to the nature of Twitter
messages, i.e., being short, noisy, written in an in-
formal style, lacking punctuation and capitaliza-
tion, containing misspellings, non-standard abbre-
viations, and non grammatically correct sentences,
the application of even basic NLP tools (trained on
formal texts) on tweets usually results in poor per-
formances. In the case of well-formed texts such
as online news, exploitation of contextual clues is
crucial to named entity identification and classifi-
cation (e.g., ?Mayor of ? in the left context of a cap-
italized token is a reliable pattern to classify it as
city name). Such external evidence is often miss-
ing in tweets, and entity names are frequently in-
complete, abbreviated or glued with other words.
Furthermore, deployment of supervised ML-based
techniques for NER from tweets is challenging
due to the dynamic nature of Twitter.
In this paper, we report on experiments aiming
at tuning a rule-based NER system, initially de-
signed for detecting names in Polish online news,
to the processing of targeted Twitter streams. In
particular, we explore whether the performance of
the baseline NER system can be improved through
the utilization of knowledge-poor methods (based
on string distance metrics) for name matching
and name guessing. In comparison to English,
Polish is a free-word order and highly inflective
language, with particularly complex declension
paradigm of proper names, which makes NER for
Polish a more difficult task.
The remaining part of the paper is structured
as follows. First, Section 2 provides information
on related work. Next, Section 3 describes the
baseline NER system and the knowledge-poor en-
hancements. Subsequently, Section 4 presents the
evaluation results. Finally, Section 5 gives a sum-
mary and an outlook as regards future research.
2 Related Work
The problem of NER has gained lot of attention in
the last two decades and a vast bulk of research
on development of NER from formal texts ex-
ists (Nadeau and Sekine, 2007). Although most of
the reported work focused on NER for major lan-
guages, efforts on NER for Polish have also been
reported. (Piskorski, 2005) describes a rule-based
NER system for Polish that covers the classical
named-entity types, i.e., persons, locations, orga-
nizations, as well as numeral and temporal expres-
84
sions. (Marcin?czuk and Piasecki, 2007) and (Mar-
cin?czuk and Piasecki, 2010) report on a memory-
based learning and Hidden Markov Model ap-
proach resp. to automatic extraction of informa-
tion on events in the reports of Polish Stockhold-
ers, which involves NER. Also in (Lubaszewski,
2007) and (Lubaszewski, 2009) some general-
purpose information extraction tools for Polish
are addressed. Efforts related to creation of a
dictionary of Warsaw urban proper names ori-
ented towards NER is reported in (Savary et al,
2009; Marciniak et al, 2009). (Gralin?ski et al,
2009) present NERT, another rule-based NER sys-
tem for Polish which covers similar types of NEs
as (Piskorski, 2005). Finally, some efforts on
CRF-based NER methods for Polish are reported
in (Waszczuk et al, 2010) and (Marcin?czuk and
Janicki, 2012).
While NER from formal texts has been well
studied, relatively little work on NER for Twit-
ter was reported. (Locke and Martin, 2009) pre-
sented a SVM-based classifier for classifying per-
sons, locations and organizations in Twitter. (Rit-
ter et al, 2011) described an approach to segmen-
tation and classification of a wider range of names
in tweets based on CRFs (using POS and shallow
parsing features) and Labeled LDA resp. (Liu et
al., 2011) proposed NER (segmentation and clas-
sification) approach for tweets, which combines
KNN and CRFs paradigms. The reported preci-
sion/recall figures are significantly lower than the
state-of-the-art results for NER from well-formed
texts and oscillate around 50-80%. Better results
were reported in case of extracting names from
targeted tweets (person names from tweets on
live sport events) (Choudhury and Breslin, 2011).
(Nebhi, 2012) presented a rule-based NER system
for detecting persons, organizations and locations
which exploits an external global knowledge base
on entities to disambiguate NE type. (Liu et al,
2012) proposed a factor graph-based approach to
jointly conducting NER and NEN (Named Entity
Normalization), which improves F-measure per-
formance of NER and accuracy of NEN when
run sequentially. An Expectation-Maximization
approach to NE disambiguation problem was re-
ported by (Davis et al, 2012). Finally, (Li et al,
2012) presented an unsupervised system for ex-
tracting (no classification) NEs in targeted Twitter
streams, which exploits knowledge gathered from
the web and exhibits comparable performance to
the supervised approaches mentioned earlier.
Most of the above mentioned work on NER in
tweets focused on English. To our best knowledge
no prior work on NER in tweets in Polish has been
reported, which makes our effort a pioneering con-
tribution in this specific field. Our work also con-
tributes to NER from targeted Twitter streams.
3 Named Entity Extraction from
Targeted Tweets in Polish
The objective of this work is to explore vari-
ous linguistically lightweight strategies to adapt
an existing news-oriented rule-based NER system
for Polish to the processing of tweets in targeted
Twitter streams. Starting from the adaptation of
a NER rule-based system to the processing of
tweets (Section 3.1), we incrementally refine the
approach with, first, the introduction of a string
similarity-based name matching step (Section 3.2)
and, second, the exploitation of corpus statistics
and knowledge-poor method for name guessing
(Section 3.3).
3.1 NER Grammar for Polish
The starting point of our explorations is an exist-
ing NER system for Polish, modeled as a cascade
of finite-state grammars using the EXPRESS for-
malism (Piskorski, 2007). Similarly to rule-based
approaches to NER for many other Indo-European
languages, the grammars consist of a set of extrac-
tion patterns for person, organization and location
names. The patterns exploit both internal (e.g.,
company designators) and external clues (e.g., ti-
tles and functions of a person, etc.) for name de-
tection and classification; a simple extraction pat-
tern for person names can be illustrated as follows:
PER :> ( ( gazetteer & [TYPE: "firstname",
SURFACE: #F] )
( gazetteer & [TYPE: "initial",
SURFACE: #I] ) ?
( surname-candidate & [SURFACE: #L] )
):name
-> name: person & [NAME: #FULL-NAME]
& #full_name := ConcWithBlanks(#F,#I,#L).
This rule first matches a sequence consisting of: a
first name (through a gazetteer look-up), an op-
tional initial (gazetteer look-up as well) and, fi-
nally, a sequence of characters considered as sur-
name candidate (e.g., capitalized tokens), which
was detected by a lower-level grammar1 and
is represented as a structure of type surname-
candidate. The right-hand side of the extraction
1Lower-level grammar extract small-scale structures
which might constitute parts of named entities.
85
pattern specifies the output structure of type per-
son with one attribute called NAME, whose value
is simply a concatenation of the values of the vari-
ables #F, #I and #L assigned to the surface forms
of the matched first name, initial and surname can-
didate respectively.
Overall the grammar contains 15 extraction pat-
terns for person names, 10 for location names,
and 10 for organization names. It relies on a
huge gazetteer of circa 294K entries, which is
an extended version of the gazetteer described
in (Savary and Piskorski, 2011) and includes, i.a.,
39K inflected forms of both Polish and foreign
first names, 86K inflected forms of surnames, 5K
of organisation names (only partially inflected),
10K of inflected location names (e.g., city names,
country names, rivers, etc.). No morphological an-
alyzer for Polish is used and only a tiny fraction of
the extraction patterns relies on morphological in-
formation (encoded in the gazetteer). In this orig-
inal grammar, the patterns are divided into sure-
fire patterns and less reliable patterns (whose pre-
cision is expected to be lower). The latter ones
are patterns that rely solely on gazetteer informa-
tion (simple look-up), which might have ambigu-
ous interpretation, e.g., patterns that only match
first names in text. When applied on conven-
tional online news, the performance of this orig-
inal NER grammar oscillates around 85% in terms
of F-measure.
In order to process tweets, we slightly modi-
fied this grammar, mostly by simplifying it. Since
mentions of entities in tweets frequently occur as
single tokens (e.g., external evidence as in clas-
sical news is often missing), we did not keep the
distinction between sure-fire and less-reliable pat-
terns. Furthermore, the original NER grammar
?included? a mechanism (encoded directly in pat-
tern specification) to lemmatize the recognized
names as well as to extract various attributes such
as titles (e.g., ?Pan? (Mr.)) and position (e.g.,
?Prezydent? (president)) for persons. As we are
mainly interested in the detection and classifica-
tion of NEs while processing tweets, these func-
tionalities were not needed and the grammar sim-
ply extracts names and their type. This ?reduced?
NER grammar constitutes the baseline approach,
and will be referred to as BASE in the remain-
ing part of the paper. It is worth mentioning that
we tested as well a version of the grammar with
lower-cased lexical resources, but due to poor re-
sults (mainly due to high ambiguity of lower-case
lexical entries) we did not conduct further explo-
rations in this direction.
3.2 String distance-based Name Matching
In tweets, names are often abbreviated (e.g., ?Parl.
Europ.? and ?PE? are abbreviations of ?Parla-
ment Europejski?), glued to other words (e.g.,
?prezydent Komorowski? is sometimes written as
?prezydentKomorowski?) and misspelled variants
are frequent (e.g., ?Donlad Tusk? is a frequent
misspelling of ?Donald Tusk?). The NER gram-
mar ?as is? would fail to recognize the particular
names in the aforementioned examples. There-
fore, in order to improve the recall of the ?tweet
grammar?, we perform a second run deploying
string distance metrics (in the entire targeted Twit-
ter stream) for matching new mentions of names
previously recognized by the NER grammar (see
Section 3.1). Furthermore, due to the highly in-
flective character of Polish, we also expect to cap-
ture with string distance metrics non-nominative
mentions of names (e.g., ?Rzeczpospolitej - geni-
tive/dative/locative form of ?Rzeczpospolita? - the
name of a Polish daily newspaper), which the NER
grammar might have failed to recognize.
Inspired by the work reported in (Piskorski et
al., 2009) we explored the performance of sev-
eral string distance metrics. First, we tested the
baseline Levenshtein edit distance metric given
by the minimum number of character-level oper-
ations (insertion, deletion, or substitution) needed
to transform one string into another (Levenshtein,
1965). Next, we used an extension thereof, namely
Smith-Waterman (SW) metric (Smith and Water-
man, 1981), which additionally allows for vari-
able cost adjustment to the cost of a gap and vari-
able cost of substitutions (mapping each pair of
symbols from alphabet to some cost). We used a
variant of this metric, where the Smith-Waterman
score is normalized using the Dice coefficient (the
average length of strings compared).
Subsequently, we explored variants of the Jaro
metric (Jaro, 1989; Winkler, 1999). It considers
the number and the order of the common char-
acters between the two strings being compared.
More precisely, given two strings s = a1 . . . aK
and t = b1 . . . bL, we say that ai in s is common
with t if there is a bj = ai in t such that i ? R ?
j ? i+R, where R = bmax(|s|, |t|)/2c? 1. Fur-
thermore, let s? = a?1 . . . a
?
K? be the characters in
86
s which are common with t (with preserved order
of appearance in s) and let t? = b?1 . . . b
?
L? be de-
fined analogously. A transposition for s? and t? is
defined as any position i such that a?i 6= b
?
i. Let us
denote the number of transpositions for s? and t?
as Ts?,t? . The Jaro similarity is then calculated as:
J(s, t) =
1
3
? (
|s?|
|s|
+
|t?|
|t|
+
|s?| ? bTs?,t?/2c
|s?|
)
A Winkler variant of Jaro metric boosts this
similarity for strings with agreeing initial charac-
ters and is calculated as:
JW (s, t) = J(s, t) + ? ? boostp(s, t) ? (1? J(s, t))
where ? denotes the common prefix adjustment
factor (default value is 0.1) and boostp(s, t) =
min(|lcp(s, t)|, p). Here lcp(s, t) denotes the
longest common prefix between s and t. Further, p
stands for the upper bound of |lcp(s, t)|2 , i.e., up
from a certain length of lcp(s, t) the ?boost value?
remains the same.
The q-gram metric (Ukkonen, 1992) is based
on the intuition that two strings are similar if
they share a large number of character-level q-
grams. We used a variant thereof, namely skip-
gram metric (Keskustalo et al, 2003), which ex-
hibited better performance than any other variant
of character-level q-grams based metrics. It is
based on the idea that in addition to forming bi-
grams of adjacent characters, bigrams that skip
characters are considered. Gram classes are de-
fined that specify what kind of skip-grams are cre-
ated, e.g. {0, 1} class means that normal bigrams
are formed, and bigrams that skip one character.
In particular, we tested {0, 1} and {0, 2} classes.
Due to the nature of Twitter we expected skip-
grams to be particularly useful in our experiments.
Considering the declension paradigm of Polish
we also considered the basic CommonPrefix met-
ric introduced in (Piskorski et al, 2009), which is
based on the longest common prefix. It is calcu-
lated as:
CP (s, t) = (|lcp(s, t)|)2/|s| ? |t|
Finally, we evaluated the performance of
longest common sub-strings distance metric,
which recursively finds and removes the longest
2Here p is set to 6.
common sub-string in the two strings compared.
Let lcs(s, t) denote the first longest common sub-
string for s and t and let s?p denote a string ob-
tained by removing from s the first occurrence of
p in s. The LCS metric is calculated as:
LCS(s, t) =
?
?
??
?
??
0 if |lcs(s, t)| ? 2
|lcs(s, t)|+ LCS(s?lcs(s,t), t?lcs(s,t))
otherwise
The string distance-based name matching de-
scribed in this section will be referred to as
MATCH-X, with X standing for the name of the
string distance metric being used.
3.3 Name Clustering
Since contextual clues for recognizing names in
formal texts are often missing in tweets, we ad-
ditionally developed a rudimentary name guesser
to boost the recall. Let us also observe that using
string distance metrics described in Section 3.2 to
match all not yet captured mentions of previously
recognized names might not be easy due the fact
that the process of creating abbreviations in Twit-
ter is very productive, e.g., ?Rzeczpospolita? ap-
pears abbreviated as ? Rzepa?, Rzp. or ?RP, which
are substantially different from the original name.
The main idea beyond the name guesser is based
on the following assumption: given a targeted
Twitter stream, if a capitalized word n-gram has
a couple of ?similar? word n-grams in the same
stream, most of which are not recognized as valid
word forms, then such a group of n-grams word
are most likely named mentions of the same entity
(e.g., person, organization or location, etc.). To be
more precise, the name guesser works as follows.
1. Compute S = {s1, s2, ....sk} - a set of word
uni- and bigrams (cluster seeds) in the Twit-
ter stream3, where frequency(si) ? ?4 and
character ? length(si) ? 3 for all si ? S.
2. Create an initial set of singleton ?name? clus-
ters: C = {C1, C2, . . . , Ck} with Ci = {si}.
3. Build clusters of simmilar n-grams
around the selected uni- and bigrams
3The vast majority of names annotated in our test corpus
are either word unigrams or bigrams (see Section 4.1.)
4? We explored various values of this parameter, which is
described in Section 4.2
87
using the string distance metric m: As-
sign each word n-gram w in the Twitter
stream to at most one cluster Cj with
j ? arg minx?{1,2,...,k} distm(sx, w)
5, and
distm(sj , w) ? maxDist, where maxDist
is a predefined constant.
4. Iteratively merge most-simmilar clusters in
C: If ?Cx, Cy ? C with DIST (Cx, Cy) ?
DIST (Ci, Cj) for i, j ? {1, . . . , |C|}6 and
DIST (Cx, Cy) ? maxDist then C = C \
{Cx, Cy} ? (Cx ? Cy).
5. Discard ?small? clusters:
C = {Cx ? C : |Cx| ? 3}
6. Discard clusters containing high number of
n-grams, whose parts are valid word forms,
but not proper names: C = {Cx ?
C : ?w?Cx
WordForm?(w)
|Cx|
? 0.3}, where
WordForm?(w) = 1 if all the words
constituting the word n-gram w are valid
word forms, but not proper names, and
WordForm?(w) = 0 otherwise, e.g.,
WordForm?(Jan Grzyb) = 0 since Grzyb
(eng. mushroom) can be interpreted as a
valid word form, which is not a proper name,
whereas Jan has only proper name interpre-
tation.
7. Use the n-grams in the remaining clusters
in C (each of them is considered to contain
named mentions of the same entity) to match
names in the Twitter stream through simple
lexicon look-up.
For computing similarity of n-grams and merg-
ing clusters we used the longest common sub-
strings (LCS) metric which performed on average
best (in terms of F-measure) in the context of name
matching (see Section 3.2 and 4). For checking
whether tokens constitute valid word forms we ex-
ploited PoliMorf (Wolin?ski et al, 2012), a freely
available morphological dictionary of Polish, con-
sisting of circa 6.7 million word forms, includ-
ing proper names. Proper names are distinguished
from other entries in the aforementioned resource.
The name guesser sketched above will be re-
ferred to as CLUSTERING. Instead of building the
5We denote the distance between two strings x and y mea-
sured with the string distance metric m as distm(x, y)
6DIST (Cx, Cy) = ?s?Cx?t?Cy
distm(s,t)
|Cx|?|Cy|
(average
distance between strings in the two clusters)
name clusters around n-grams, whose frequency
exceeds certain threshold, we also tested building
clusters around least frequent n-grams (i.e., whose
frequency is ? 3), which will be referred to as
CLUSTERING-INFRQ. The name guesser runs ei-
ther independently or on top of the NER grammar
described in Section 3.1 in order to detect ?new?
names in the unconsumed part of the tweet collec-
tion, i.e., names recognized by the grammar are
preserved. It is important to emphasize that the
clustering-based name guesser only detects names
without classifying them.
4 Experiments
4.1 Dataset
We have gathered tweet collections using Twit-
ter search API7 focusing on some major events in
2012/2013 and on famous individuals, namely: (a)
Boston marathon bombings, (b) general comments
on Donald Tusk, the prime minister of Poland,
(c) discussion on the public comments of Antoni
Macierewicz (a politician of the Law and Justice
opposition party in Poland) on the Polish presi-
dent crash in Smolen?sk (Russia) in 2010, (d) de-
bate on the controversial firing of the journalist
Cezary Gmyz from one of the major Polish news-
papers Rzeczpospolita and, (e) a collection of ran-
dom tweets in Polish. Each tweet collection was
extracted using simple queries, e.g., "zamach AND
(Boston OR Bostonie)" ("attack" AND "?Boston"?
either in nominative of locative form) for collect-
ing tweets on the Boston bombings. From each
collection a subset of randomly chosen tweets was
selected for evaluation purposes. We will refer
to the latter as the test corpus, whereas the entire
tweet collections will be referred to as the stream
corpus.
In the stream corpus, we computed for each
tweet: (a) the text-like fraction of its body, i.e., the
fraction of the body which contains text, and (b)
the lexical validity, i.e., the percentage of tokens in
the text-like part of the body of the tweet which are
valid word forms in Polish8. Figure 1 and 2 show
the histograms for text-like fraction and lexical va-
lidity of the tweets in each collection in the stream
corpus. We can observe that large portion of the
tweets contains significant text-like part, which is
7https://dev.twitter.com
8For computing lexical validity we used
PoliMorf (Wolin?ski et al, 2012), already mentioned in
the previous section.
88
also lexically valid. Interestingly, the random col-
lection exhibits lower lexical validity, which is due
to more colloquial language used in the tweets in
this collection.
  10 20 30 40 50 60 70 80 90 1000
5
10
15
20
25
30 Boston Tusk Macierewicz Gmyz Random
Text-like fraction
Percent
age of T
weets
Figure 1: Text-like fraction of the tweets in each
collection.
  10 20 30 40 50 60 70 80 90 1000
5
10
15
20
25
30
35 Boston Tusk Macierewicz Gmyz Random
Text-lik litftra
ceo-enr
lPekgwks
?eer?
Figure 2: Lexical validity of the tweets in each
collection.
We built the test corpus by randomly select-
ing tweets whose text-like fraction of the body
was ? 80%, additionally checking the language
and removing duplicates. These tweets were af-
terwards manually annotated with person, loca-
tion and organization names, according to the fol-
lowing guidelines: consideration of unigram en-
tities, non-inclusion of titles, functions and alike,
non-inclusion of spurious punctuation marks and
exclusion of names starting with ?@?, since their
recognition as names is trivial.
The test corpus statistics are provided in Ta-
ble 1. We provide in brackets the number of tweets
in the corresponding tweet collections in the en-
tire stream corpus. In this test corpus, 86,7% of
the annotated names are word unigrams, whereas
bigrams constitute 12,7% of the annotated names
and 3- and 4-grams account only for a tiny frac-
tion (0,6%); this is in line with the characteristics
of the Twitter language, which favours quick and
simple expressions. For each collection, we com-
puted the name diversity as the ratio between en-
tity occurrences and unique entities, as well as the
average number of entities per tweet9. Targeted
stream corpora show a medium name diversity
(except for Boston and Gmyz collections, centred
on a very specific location and person name resp.)
and a high rate of entity per tweet (around 2.2), in
contrast with random corpus which shows a high
name diversity (0.79) for a low average number of
entity per tweets. Reported to the limited number
of characters in tweets (140), the important signifi-
cant number of entity per tweet in targeted streams
accounts, on the one hand, for the usefulness of
working on targeted streams and, on the other, for
the importance of NER in tweets.
Corpus #tweets name #names #PER #LOC #ORG
diversity per
tweet
Boston 198 0.24 2.16 34 298 96
(2953)
Tusk 232 0.36 2.42 393 88 80
(1186)
Macierewicz 303 0.32 2.17 494 60 104
(931)
Gmyz 310 0.24 2.09 471 18 159
(672)
Random 286 0.79 0.36 59 19 27
(7806)
Table 1: Test corpus statistics.
4.2 Evaluation
In our experiments we evaluated the performance
of (i) the NER grammar (BASE), a combina-
tion thereof with (ii) different name matching
strategies (MATCH) and (iii) different variants of
the name guesser (CLUSTERING, CLUSTERING-
INFRQ) and, finally, (iv) the combinations of all
techniques. Within the MATCH configuration, we
experimented all string distance metrics presented
in 3.2 but since Jaro, Jaro-Winkler and Smith-
Waterman metrics performed on average worse
than the others, we did not consider them in
further experiments. We selected the best per-
forming metric, LCS 10, as the one used by the
name guesser (CLUSTERING) in subsequent exper-
iments. As a complement, we measured the per-
formance of the name guesser alone to compare
it with BASE. Furthermore, name matching and
9In the limit of our reference corpora, i.e. entities of type
person, location and organization.
10Skip-grams was the other metric which exhibited similar
performance
89
name guessing algorithms were using the tweet
collections in the stream corpus (as quasi ?Twitter
stream window?) in order to gather knowledge for
matching/guessing ?new? names in the test corpus.
We measured the performance of the different
configurations in terms of Precision (P), Recall
(R) and F-measure (F), according to two differ-
ent schemes: exact match, where entity types and
both boundaries should match perfectly, and fuzzy
match, which allows for one name boundary re-
turned by the system to be different from the ref-
erence, i.e., either too short or too long on the left
or on the right, but not on both. Furthermore, since
the clustering-based name guesser described in 3.3
does not classify names, for any settings with this
technique we only evaluated name detection per-
formance, i.e., no distinction between name types
was made. The overall summary of the results for
the entire pool of tweet collections, is presented in
Table 3.
In the context of the CLUSTERING algorithm we
explored various settings as regards the minimum
frequency of an n-gram to be considered as clus-
ter seed (? parameter - see Section 3.3). More
precisely, we tested values in the range of 1 to
30 for all corpora and system settings which in-
cluded CLUSTERING, and compared the resulting
P/R and F figures. An example of a curve with P/R
values (exact match) of BASE-CLUSTERING algo-
rithm applied on the ?Boston? corpus with vary-
ing values of ? is given in Figure 3. One can ob-
serve and hypothesize that the frequency threshold
does not impact much the performance. Suchlike
curves for other settings were of a similar nature.
Therefore we decided to set the ? to 1 in all set-
tings reported in Table 3.
4.3 Results analysis
The performance of the NER grammars is surpris-
ingly good, both in case of exact and fuzzy match
evaluation. Except for random corpus (which
shows rather low performance with 55% precision
and 39% recall), precision figures oscillate around
85-95%, whereas recall is somewhat worse (60-
75%), as was to be expected. The low recall for
?Gmyz? corpus is due to the non-matching of a fre-
quently occurring person name. Precision and re-
call figures for each entity type for BASE are given
in Table 2. In general, recognition of organization
names appears to be more difficult (lower recall),
especially in the random corpus.
  1 0 2 3 4 5 6 7 8 19 11 10 12 13 14 15 16 17 18 09 01 00 02 03 04 05 06 07 08 29
99B1
9B09B2
9B39B4
9B59B6
9B79B8
1 ostnTuTkM atncii
Texet-tlik frtlafco-cxnPllgwlsclnwx?e?cfc?lrln?-?gcfl?cc?
Figure 3: Precision and Recall figures for BASE-
CLUSTERING applied on ?Boston? corpus, with
different frequency thresholds of n-grams to be
considered cluster seeds.
Corpus PER ORG LOC
P R P R P R
Boston 31.6 35.3 87.9 30.2 94.3 71.8
Tusk 87.6 71.2 82.4 35.0 89.9 70.5
Gmyz 85.5 32.5 82.8 15.1 88.9 44.4
Macierewicz 93.6 80.2 71.2 35.6 83.7 60.0
Random 56.7 55.9 0 0 53.3 42.1
Table 2: Precision/recall figures for person, or-
ganization and location name recognition (exact
match) with BASE.
Extending BASE with MATCH yields some im-
provements in terms of recall (including random
corpus), whereas precision either oscillates around
the figures achieved by BASE, or deteriorates. In
case of ?Gmyz? corpus, we can observe significant
gain in both recall and precision through using the
name matching step. With regard to the other cor-
pora, the reason for not obtaining a significant gain
could be due to two reasons: (a) the n-grams iden-
tified as similar to the names recognized by BASE
are already covered by BASE with some patterns
(e.g., inflected forms of many entities are stored in
the gazetteer), or (b) using string distance metrics
in the MATCH step might not be the best method to
capture mentions of a recognized entity, as exem-
plified in Table 4, where the mentions of a news-
paper Rzeczpospolita (captured by BASE) may be
significantly different, e.g., in terms of the charac-
ter length.
Regarding the results for CLUSTERING-INFRQ,
running it alone, yielded poor results for all cor-
pora, only in case of the?Gmyz? corpus a gain
could be observed. CLUSTERING performed better
than CLUSTERING-INFRQ for all corpora.
Deploying BASE with CLUSTERING on top of
it results in up to 1.5-6% (exact match) and 4-
90
EXACT MATCH
Method Boston Tusk Gmyz Macierewicz AVERAGE
P R F P R F P R F P R F P R F
BASE 85.6 59.6 70.2 87.7 65.9 75.3 85.3 28.5 42.8 90.5 71.3 79.8 87.3 56.3 67.0
BASE-MATCH-LEV 80.8 62.9 70.7 87.4 66.5 75.5 90.9 63.6 74.8 90.2 72.3 80.3 87.3 66.3 75.3
BASE-MATCH-SW 70.9 62.1 66.3 76.6 67.5 71.8 78.0 59.1 68.0 89.4 73.1 80.4 78.7 65.5 71.6
BASE-MATCH-J 67.7 62.1 64.8 79.3 68.1 73.3 60.9 48.3 53.9 60.0 73.3 65.9 67.0 63.0 64.5
BASE-MATCH-JW 63.2 62.1 62.7 75.5 68.3 71.7 48.2 48.9 48.6 58.0 74.0 65.0 61.2 63.3 62.0
BASE-MATCH-SKIP(0,1) 80.9 62.1 70.3 87.6 66.5 75.6 91.3 63.0 74.5 90.3 72.2 80.2 87.5 66.0 75.2
BASE-MATCH-SKIP(0,2) 80.9 62.1 70.3 87.7 66.3 75.5 91.5 63.0 74.6 90.6 72.2 80.4 87.7 65.9 75.2
BASE-MATCH-CP 80.2 59.6 68.4 87.7 66.0 75.3 83.5 58.6 68.9 90.2 71.4 79.7 85.4 63.9 73.1
BASE-MATCH-LCS 80.7 63.6 71.1 86.8 67.0 75.7 82.3 59.0 68.7 90.2 72.9 80.7 85 65.6 74.1
CLUSTERING 66.2 10.0 17.4 60.6 33.2 42.9 61.3 36.0 45.3 52.9 33.4 41.0 60.3 28.2 36.7
CLUSTERING-INFRQ 37.5 1.4 2.7 27.3 1.1 2.1 60.7 31.5 41.5 54.8 28.6 37.6 45.1 15.7 21.0
BASE-CUSTERING 86.8 67.8 76.1 91.1 72.7 80.9 80.6 61.0 69.4 86.3 74.6 80.0 86.2 69.0 76.6
BASE-CLUSTERING-INFRQ 89.7 65.0 75.3 89.4 69.3 78.1 81.2 58.5 68.0 89.9 74.2 81.3 87.6 66.8 75.7
BASE-MATCH-CLUSTERING 87.6 75.9 81.4 90.2 73.8 81.2 74.1 62.8 68.0 86.1 76.3 80.9 84.5 72.2 77.9
BASE-MATCH-CLUSTERING-INFRQ 90.0 73.4 80.8 88.6 70.4 78.5 74.3 60.3 66.6 89.6 75.8 82.1 85.6 70.0 77.0
FUZZY MATCH
Method Boston Tusk Gmyz Macierewicz AVERAGE
P R F P R F P R F P R F P R F
BASE 86.6 60.3 71.1 92.2 69.3 79.1 88.0 29.5 44.2 95.0 74.8 83.7 90.5 58.5 69.5
BASE-MATCH-LEV 81.7 63.6 71.5 92.3 70.2 79.8 93.6 65.4 77.0 94.9 76.1 84.5 90.6 68.8 78.2
BASE-MATCH-SW 73.3 64.3 68.5 80.8 71.3 75.8 91.4 67.6 77.7 94.2 77.1 84.8 84.9 70.1 76.7
BASE-MATCH-J 70.5 64.7 67.5 85.5 73.4 79.0 86.2 68.4 76.2 63.4 77.5 69.8 76.4 71.0 73.1
BASE-MATCH-JW 65.8 64.7 65.3 81.9 74.0 77.7 68.2 69.1 68.7 61.4 78.4 68.9 69.3 71.6 70.2
BASE-MATCH-SKIP(0,1) 81.8 62.9 71.1 92.3 70.1 79.6 94.0 64.8 76.7 95.1 76.0 84.5 90.8 68.5 78.0
BASE-MATCH-SKIP(0,2) 81.8 62.9 71.1 92.2 69.7 79.4 94.2 64.8 76.8 95.0 75.7 84.3 90.8 68.3 77.9
BASE-MATCH-CP 81.1 60.3 69.2 92.2 69.3 79.1 93.8 65.9 77.4 95.0 75.2 84.0 90.5 67.7 77.4
BASE-MATCH-LCS 81.6 64.3 71.9 92.4 71.3 80.5 93.1 66.7 77.7 94.9 76.7 84.9 90.5 69.8 78.8
CLUSTERING 83.1 12.6 21.9 96.4 52.8 68.2 89.2 52.3 66.0 87.7 55.5 68.0 89.1 43.3 56.0
CLUSTERING-INFRQ 87.5 3.3 6.3 68.2 2.7 5.1 91.1 47.2 62.2 94.2 49.1 64.5 85.3 25.6 34.5
BASE-CLUSTERING 93.1 72.7 81.6 96.9 77.4 86.0 94.5 71.4 81.4 91.7 79.3 85.1 94.1 75.2 83.5
BASE-CLUSTERING-INFRQ 95.5 69.2 80.2 95.9 74.3 83.7 96.4 69.4 80.7 96.9 79.9 87.6 96.2 73.2 83.1
BASE-MATCH-CLUSTERING 93.3 80.8 86.6 96.5 79.0 86.9 92.9 78.7 85.2 91.8 81.3 86.2 93.6 80.0 86.2
BASE-MATCH-CLUSTERING-INFRQ 95.1 77.6 85.5 96.0 76.3 85.0 94.5 76.7 84.7 96.6 81.8 88.6 95.6 78.1 86.0
Table 3: Precision, Recall and F-measure figures for exact (top) and fuzzy match (bottom). The best
results are highlighted in bold.
CEZARY GMYZ zwolniony z "Rzeczpospolitej". To efekt spotkania z
Zarza?dem i Rada? Nadzorcza? wydawcy dziennika http://t.co/QspE3edh
@agawaa ...usi?ujesz czepic? sie szczeg??u, gdy istota sprawy jest taka:
Rzepa/Gmyz pitolili bez sensu.
Konflikt w Rzepie? Ta ca?a sytuacja na to wskazuje. Gmyz sie? nie wycofuje,
a Rzepa jak najbardziej.
@volanowski Nowa linia: Gmyz wyrzucony z Rzepy czyli PO we wszystkich
sprawach smolen?skich jest cacy i super. Ludzie na to nie p?jda.
@TomaszSkory Byc? moz?e "Rz" i Gmyz p?aca? teraz w?as?nie za "skr?ty
mys?lowe" swoich informator?w. Dlaczego RMF nie p?aci za "skr?ty" swoich?
Gmyz wylecia? z RP, a Ziemkiewicz straci? Subotnik? Nie lepiej by?o nieco
zejs?c? z 3.50 z?, czy chodzi o cos? zupe?nie innego?
Gmyz wyrzucony z "Rzeczpospolitej". "Dzisiaj zwolniono mnie dyscyp-
linarnie": Cezary Gmyz straci? prace? w "Rzeczp... http://t.co/ObZIxXML
Table 4: Examples of various ways of referring to
a newspaper Rzeczpospolita in tweets.
10% (fuzzy match) gain in F-measure compared
to BASE (mainly thanks to gain in recall), ex-
cept ?Gmyz? corpus, where the gain is higher.
The average gain over the four targeted corpora
against the best combination of BASE-MATCH in
F-measure is 1.3%. We observed comparable im-
provement for the random corpus. It turned out
CLUSTERING often contributes to the recognition
of names glued to other words and/or character se-
quences.
Combining BASE with MATCH-LCS and CLUS-
TERING/CLUSTERING-INFRQ yields further im-
provements against the other settings. In par-
ticular, the gain in F-measure of BASE-MATCH-
CLUSTERING against BASE, measured over the
four targeted corpora, is 10.9% and 16.7% for ex-
act and fuzzy match respectively (mainly due to
gain in recall).
Considering the nature of Twitter messages the
average F-measure score over the four targeted
corpora for BASE-MATCH-CLUSTERING, amount-
ing to 77.9% (exact match) and 86.2% (fuzzy
match) can be seen as a fairly good result. Al-
though the difference in some of the correspond-
ing scores for exact and fuzzy match appear sub-
stantial, it is worth mentioning that CLUSTERING
algorithm often guesses name candidates that are
either preceded or followed by some characters
not belonging to the name itself, which is pe-
nalized in exact-match evaluation. This problem
could be alleviated through deployment of heuris-
tics to trim such ?unwanted? characters. Another
source of false positives extracted by CLUSTER-
ING is the fact that this method might, beyond
person, organization and location types, recognize
any kind of NEs, which, even not very frequent, is
penalized since they are not present in our refer-
ence corpus.
In general, considering the shortness of names
in Twitter, the major type of errors in all settings
are either added or missed entities, but more rarely
overlapping problems. One of the main source of
errors is due to the fact that single-token names,
which are frequent in tweets, often exhibit type
91
ambiguity. Once badly recognized, these errors
are propagated over the next processing steps.
5 Conclusions and Outlook
In this paper we have reported on experiments on
tuning an existing finite-state based NER gram-
mar for processing formal texts to NER from
targeted Twitter streams in Polish through com-
bining it with knowledge-poor techniques for
string distance-based name matching and corpus
statistics-based name guessing. Surprisingly, the
NER grammar alone applied on the four test cor-
pora (including circa 2300 proper names) yielded
P, R, and F figures for exact (fuzzy) matching
proper names (including: person, organization and
locations) of 87.3% (90.5%), 56.3% (58.5) and
67% (69.5%) resp., which can be considered fairly
reasonable result, though some variations across
tweet collections could be observed (depending
on the topic and how people ?tweet? about).
The integration of the presented knowledge-poor
techniques for name matching/guessing resulted
in P, R and F figures for exact (fuzzy) match-
ing names of 84.5% (93.6%), 72.2% (80.0) and
77.9% (86.2%) resp. (setting with best F-measure
scores), which constitutes a substantial improve-
ment against the grammar-based approach. We
can observe that satisfactory-performing NER
from targeted Twitter streams in Polish can be
achieved in a relatively straightforward manner.
As future work to enhance our experiments, we
envisage to: (a) enlarge the pool of test corpora,
(b) carry out a more thorough error analysis, (c)
test a wider range of string distance metrics (Co-
hen et al, 2003), (d) study the applicability of the
particular NER grammar rules w.r.t. their useful-
ness in NER in targeted Twitter streams and (e),
compare our approach with an unsupervised ML-
approach, e.g. as in (Li et al, 2012).
Acknowledgments
The authors gratefully acknowledge the support of
the ERC Starting Grant MultiJEDI No. 259234
and the Polish National Science Centre grant N
N516 481940 ?Diversum?.
References
Smitashree Choudhury and John Breslin. 2011. Ex-
tracting Semantic Entities and Events from Sports
Tweets. In Proceedings of the 1st Workshop on
Making Sense of Microposts (#MSM2011), pages
22?32.
William W. Cohen, Pradeep Ravikumar, and
Stephen E. Fienberg. 2003. A Comparison of
String Distance Metrics for Name-matching Tasks.
In Proceedings of the IJCAI-2003 Workshop on
Information Integration on the Web (IIWeb-03),
pages 73?78.
Alexandre Davis, Adriano Veloso, Altigran S. da Silva,
Wagner Meira, Jr., and Alberto H. F. Laender.
2012. Named Entity Disambiguation in Streaming
Data. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Long Papers - Volume 1, ACL ?12, pages 815?824,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Filip Gralin?ski, Krzysztof Jassem, Micha? Marcin?czuk,
and Pawe? Wawrzyniak. 2009. Named entity recog-
nition in machine anonymization. In Recent Ad-
vances in Intelligent Information Systems, pages
247?260, Warsaw. Exit.
Mathew Jaro. 1989. Advances in record linking
methodology as applied to the 1985 census of Tampa
Florida. Journal of the American Statistical Society,
84(406):414?420.
Heikki Keskustalo, Ari Pirkola, Kari Visala, Erkka
Lepp?nen, and Kalervo J?rvelin. 2003. Non-
adjacent Digrams Improve Matching of Cross-
lingual Spelling Variants. In Proceedings of SPIRE,
LNCS 22857, Manaus, Brazil, pages 252?265.
Vladimir Levenshtein. 1965. Binary Codes for Cor-
recting Deletions, Insertions, and Reversals. Dok-
lady Akademii Nauk SSSR, 163(4):845?848.
Chenliang Li, Jianshu Weng, Qi He, Yuxia Yao, An-
witaman Datta, Aixin Sun, and Bu-Sung Lee. 2012.
TwiNER: Named Entity Recognition in Targeted
Twitter Stream. In Proceedings of the 35th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ?12,
pages 721?730, New York, NY, USA. ACM.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011. Recognizing Named Entities in
Tweets. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1,
pages 359?367, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Xiaohua Liu, Ming Zhou, Furu Wei, Zhongyang Fu,
and Xiangyang Zhou. 2012. Joint Inference of
Named Entity Recognition and Normalization for
Tweets. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers - Volume 1, ACL ?12, pages 526?
535, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
92
Brian Locke and James Martin. 2009. Named Entity
Recognition: Adapting to Microblogging. Senior
Thesis, University of Colorado.
Wies?aw Lubaszewski. 2007. Information extraction
tools for polish text. In Proc. of LTC?07, Poznan?,
Poland, Poznan?. Wydawnictwo Poznanskie.
Wies?aw Lubaszewski. 2009. S?owniki komputerowe i
automatyczna ekstrakcja informacji z tekstu. AGH
Uczelniane Wydawnictwa Naukowo-Dydaktyczne,
Krak?w.
Micha? Marcin?czuk and Maciej Janicki. 2012. Opti-
mizing CRF-Based Model for Proper Name Recog-
nition in Polish Texts. In A. Gelbukh, editor,
CICLing 2012, Part I, volume 7181 of Lecture
Notes in Computer Science (LNCS), pages 258?
?269. Springer, Heidelberg.
Micha? Marcin?czuk and Maciej Piasecki. 2007. Pat-
tern extraction for event recognition in the reports
of polish stockholders. In Proceedings of IMCSIT?
AAIA?07, Wis?a, Poland, pages 275?284.
Micha? Marcin?czuk and Maciej Piasecki. 2010.
Named Entity Recognition in the Domain of Pol-
ish Stock Exchange Reports. In Proceedings of In-
telligent Information Systems 2010, Siedlce, Poland,
pages 127?140.
Ma?gorzata Marciniak, Joanna Rabiega-Wis?niewska,
Agata Savary, Marcin Wolin?ski, and Celina Heliasz.
2009. Constructing an Electronic Dictionary of Pol-
ish Urban Proper Names. In Recent Advances in In-
telligent Information Systems. Exit.
David Nadeau and Satoshi Sekine. 2007. A Sur-
vey of Named Entity Recognition and Classification.
Lingvisticae Investigationes, 30(1):3?26.
Kamel Nebhi. 2012. Ontology-Based Information Ex-
traction from Twitter. In Proceedings of the COL-
ING 2012 IEEASM Workshop, Mumbai, India.
Jakub Piskorski, Karol Wieloch, and Marcin Sydow.
2009. On Knowledge-poor Methods for Person
Name Matching and Lemmatization for Highly
Inflectional Languages. Information Retrieval,
12(3):275?299.
Jakub Piskorski. 2005. Named-Entity Recognition for
Polish with SProUT. In LNCS Vol 3490: Proceed-
ings of IMTCI 2004, Warsaw, Poland.
Jakub Piskorski. 2007. ExPRESS ? Extraction Pat-
tern Recognition Engine and Specification Suite. In
Proceedings of FSMNLP 2007.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named Entity Recognition in Tweets: An Ex-
perimental Study. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2011), pages 1524?1534, Ed-
inburgh, Scotland, UK. Association for Computa-
tional Linguistics.
Agata Savary and Jakub Piskorski. 2011. Language
Resources for Named Entity Annotation in the Na-
tional Corpus of Polish. Control and Cybernetics,
40(2):361?391.
Agata Savary, Joanna Rabiega-Wis?niewska, and
Marcin Wolin?ski. 2009. Inflection of Polish Multi-
Word Proper Names with Morfeusz and Multiflex.
LNAI, 5070.
T. Smith and M. Waterman. 1981. Identification
of Common Molecular Subsequences. Journal of
Molecular Biology, 147:195?197.
Esko Ukkonen. 1992. Approximate String Matching
with q-grams and Maximal Matches. Theoretical
Computer Science, 92(1):191?211.
Jakub Waszczuk, Katarzyna G?owin?ska, Agata Savary,
and Adam Przepi?rkowski. 2010. Tools and
Methodologies for Annotating Syntax and Named
Entities in the National Corpus of Polish. In Pro-
ceedings of the International Multiconference on
Computer Science and Information Technology (IM-
CSIT 2010): Computational Linguistics ? Applica-
tions (CLA?10), pages 531?539.
William Winkler. 1999. The State of Record Link-
age and Current Research Problems. Technical re-
port, Statistical Research Division, U.S. Bureau of
the Census, Washington, DC.
Marcin Wolin?ski, Marcin Mi?kowski, Maciej Ogrod-
niczuk, Adam Przepi?rkowski, and ?ukasz Sza-
lkiewicz. 2012. PoliMorf: A (not so) new open
morphological dictionary for Polish. In Proceedings
of the Eighth International Conference on Language
Resources and Evaluation, LREC 2012, pages 860?
864.
93
