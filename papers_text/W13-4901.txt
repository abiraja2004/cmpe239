Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 1?11,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Working with a small dataset - semi-supervised dependency parsing for Irish
Teresa Lynn1,2, Jennifer Foster1, Mark Dras2 and Josef van Genabith1
1NCLT/CNGL, Dublin City University, Ireland
2Department of Computing, Macquarie University, Sydney, Australia
1{tlynn,jfoster,josef}@computing.dcu.ie
2{teresa.lynn,mark.dras}@mq.edu.au,
Abstract
We present a number of semi-supervised pars-
ing experiments on the Irish language carried
out using a small seed set of manually parsed
trees and a larger, yet still relatively small, set
of unlabelled sentences. We take two pop-
ular dependency parsers ? one graph-based
and one transition-based ? and compare re-
sults for both. Results show that using semi-
supervised learning in the form of self-training
and co-training yields only very modest im-
provements in parsing accuracy. We also try
to use morphological information in a targeted
way and fail to see any improvements.
1 Introduction
Developing a data-driven statistical parser relies on
the availability of a parsed corpus for the language
in question. In the case of Irish, the only parsed
corpus available to date is a dependency treebank,
which is currently under development and still rel-
atively small, with only 803 gold-annotated trees
(Lynn et al, 2012a). As treebank development is
a labour- and time-intensive process, in this study
we evaluate various approaches to bootstrapping a
statistical parser with a set of unlabelled sentences
to ascertain how accurate parsing output can be
at this time. We carry out a number of differ-
ent semi-supervised bootstrapping experiments us-
ing self-training, co-training and sample-selection-
based co-training. Our studies differ from previous
similar experiments as our data is taken from a work-
in-progress treebank. Thus, aside from the current
small treebank which is used for training the initial
seed model and for testing, there is no additional
gold-labelled data available to us to directly com-
pare supervised and semi-supervised approaches us-
ing training sets of comparable sizes.
In the last decade, data-driven dependency pars-
ing has come to fore, with two main approaches
dominating ? transition-based and graph-based. In
classic transition-based dependency parsing, the
training phase consists of learning the correct parser
action to take given the input string and the parse
history, and the parsing phase consists of the greedy
application of parser actions as dictated by the
learned model. In contrast, graph-based depen-
dency parsing involves the non-deterministic con-
struction of a parse tree by predicting the maximum-
spanning-tree in the digraph for the input sentence.
In our study, we employ Malt (Nivre et al, 2006),
a transition-based dependency parsing system, and
Mate (Bohnet, 2010), a graph-based parser.
In line with similar experiments carried out on
English (Steedman et al, 2003), we find that co-
training is more effective than self-training. Co-
training Malt on the output of Mate proves to be the
most effective method for improving Malt?s perfor-
mance on the limited data available for Irish. Yet, the
improvement is relatively small (0.6% over the base-
line for LAS, 0.3% for UAS) for the best co-trained
model. The best Mate results are achieved through a
non-iterative agreement-based co-training approach,
in which Mate is trained on trees produced by Malt
which exhibit a minimum agreement of 85% with
Mate (LAS increase of 1.2% and UAS of 1.4%).
The semi-supervised parsing experiments do not
explicitly take into account the morphosyntactic
properties of the Irish language. In order to examine
the effect of this type of information during parsing,
we carry out some orthogonal experiments where we
1
reduce word forms to lemmas and introduce mor-
phological features in certain cases. These changes
do not bring about an increase in parsing accuracy.
The paper is organised as follows. Section 2 is
an overview of Irish morphology. In Section 3 our
previous work carried out on the development of an
Irish dependency treebank is discussed followed in
Section 4 by a description of some of our prior pars-
ing results. Section 5 describes the self-training, co-
training and sample-selection-based co-training ex-
periments, Section 6 presents the preliminary pars-
ing experiments involving morphological features,
and, finally, Section 7 discusses our future work.
2 Irish as a morphologically rich language
Irish is a Celtic language of the Indo-European lan-
guage family. It has a VSO word order and is rich in
morphology. The following provides an overview of
the type of morphology present in the Irish language.
It is not a comprehensive summary as the rules gov-
erning morphological changes are too extensive and
at times too complex to document here.
Inflection in Irish mainly occurs through suffixa-
tion, but initial mutation through lenition and eclip-
sis is also common (Christian-Brothers, 1988). A
prominent feature of Irish (also of Scottish and
Manx), which influences inflection, is the existence
of two sets of consonants, referred to as ?broad? and
?slender? consonants (O? Siadhail, 1989). Conso-
nants can be slenderised by accompanying the con-
sonant with a slender vowel, either e or i. Broaden-
ing occurs through the use of broad vowels; a, o or
u. For example, buail ?to hit? becomes ag bualadh
?hitting? in the verbal noun form. In general, there
needs to be vowel harmony (slender or broad) be-
tween stem endings and the initial vowel in a suffix.
A process known as syncopation also occurs
when words with more than one syllable have a
vowel-initial suffix added. For example imir ?to
play? inflects as imr??m ?I play?.
Nouns While Old Irish employed several gram-
matical cases, Modern Irish uses only three: Nomi-
native, Genitive and Vocative. The nominative form
is sometimes regarded as the ?common form? as it is
now also used to account for accusative and dative
forms. Nouns in Irish are divided into five classes, or
declensions, depending on the manner in which the
genitive case is formed. In addition, there are two
grammatical genders in Irish - masculine and fem-
inine. Case, declension and gender are expressed
through noun inflection. For example, pa?ipe?ar ?pa-
per? is a masculine noun in the first declension. Both
lenition and slenderisation are used to form the geni-
tive singular form: pha?ipe?ir. In addition, possessive
adjectives cause noun inflection through lenition,
eclipsis and prefixation. For example, teach ?house?,
mo theach ?my house?, a?r dteach ?our house?; ainm
?name?, a hainm ?her name?.
Verbs Verbs can incorporate their subject, inflect-
ing for person and number through suffixation. Such
forms are referred to as synthetic verb forms. In
addition, verb tense is often indicated through var-
ious combinations of initial mutation, syncopation
and suffixation. For example, scr??obh ?write? can in-
flect as scr??obhaim ?I write?. The past tense of the
verb tug ?give? is thug ?gave?. Lenition occurs af-
ter the negative particle n??. For example, tugaim ?I
give?; n?? thugaim ?I do not give?; n??or thug me? ?I
did not give?. Eclipsis occurs following clitics such
as interrogative particles (an, nach); complementis-
ers (go, nach); and relativisers (a, nach) (Stenson,
1981). For example, an dtugann se?? ?does he give??;
nach dtugann se?? ?does he not give??.
Adjectives In general, adjectives follow nouns and
agree in number, gender and case. Depending on
the noun they modify, adjectives can also inflect.
Christian-Brothers (1988) note eight declensions of
adjectives. They can decline for genitive singular
masculine, genitive singular feminine and nomina-
tive plural. For example, bacach ?lame? inflects as
bacaigh (Gen.Sg.Masc), baca?? (Gen.Fem.Sg) and
bacacha (Nom.PL). Comparative adjectives are also
formed through inflection. For example, la?idir
?strong?, n??os la?idre ?stronger?; de?anach ?late?, is
de?ana?? ?latest?.
Prepositions Irish has simple and compound
prepositions. Most of the simple prepositions can
inflect for person and number (known as preposi-
tional pronouns or pronominal prepositions), thus
including a nominal element. For example, com-
pare bh?? se? ag labhairt le fear ?he was speaking
with a man? with bh?? se? ag labhairt leis ?he was
speaking with him?. These forms are used quite fre-
2
quently, not only with regular prepositional attach-
ment where pronominal prepositions operate as ar-
guments of verbs or modifiers of nouns and verbs,
but also in idiomatic use where they express emo-
tions and states, e.g. ta? bro?n orm (lit. ?be-worry-
on me?) ?I am worried? or ta? su?il agam (lit. ?be-
expectation-with me?) ?I hope?. Noted by Greene
(1966) as a noun-centered language, nouns are of-
ten used to convey the meaning that verbs often
would. Pronominal prepositions are often used in
these types of structures. For example, bhain me?
geit aisti (lit. extracted-I-shock-from her) ?I fright-
ened her?; bhain me? mo cho?ta d??om (lit. extracted-I-
coat-from me) ?I took off my coat?; bhain me? u?sa?id
as (lit. extracted-I-use-from it) ?I used it?; bhain
me? triail astu (lit. extracted-I-attempt-from them)?I
tried them?.
Derivational morphology There are also some
instances of derivational morphology in Irish. U??
Dhonnchadha (2009) notes that all verb stems and
agentive nouns can inflect to become verbal nouns.
Verbal adjectives are also derived from verb stems
through suffixation. For example, the verb du?n
?close? undergoes suffixation to become du?nadh
?closing? (verbal noun) and du?nta ?closed? (verbal
adjective). An emphatic suffix -sa/-se (both broad
and slender form) can attach to nouns or pronouns.
It can also be attached to any verb that has been in-
flected for person and number and also to pronom-
inal prepositions. For example mo thuairim ?my
opinion??mo thuairimse ?my opinion; tu? ?you?(sg)
? tusa ?you?; cloisim ?I hear?? cloisimse ?I hear?;
liom ?with me?? liomsa ?with me?. In addition, the
diminutive suffix -??n can attach to all nouns to form
a derived diminutive form. The rules of slenderisa-
tion apply here also. For example, buachaill ?boy?
becomes buachaill??n ?little boy?, and tamall ?while?
becomes tamaill??n ?short while?.
3 The Irish Dependency Treebank
Irish is the official language of Ireland, yet English
is the primary language for everyday use. Irish is
therefore considered an EU minority language and
is lacking in linguistic resources that can be used to
develop NLP applications (Judge et al, 2012).
Recently, in efforts to address this issue, we have
begun work on the development of a dependency
treebank for Irish (Lynn et al, 2012a). The treebank
has been built upon a gold standard 3,000 sentence
POS-tagged corpus1 developed by U?? Dhonnchadha
(2009). Our labelling scheme is based on an ?LFG-
inspired? dependency scheme developed for English
by C?etinog?lu et al (2010). This scheme was adopted
with the aim of identifying functional roles while
at the same time circumventing outstanding, unre-
solved issues in Irish theoretical syntax.2 The Irish
labelling scheme has 47 dependency labels in the la-
bel set. The treebank is in the CoNLL format with
the following fields: ID, FORM, LEMMA, CPOSTAG,
POSTAG, HEAD and DEPREL. The coarse-grained
part of speech of a word is marked by the la-
bel CPOSTAG, and POSTAG marks the fine-grained
part of speech for that word. For example, prepo-
sitions are tagged with the CPOSTAG Prep and
one of the following POSTAGs: Simple: ar ?on?,
Compound: i ndiaidh ?after?, Possessive: ina
?in its?, Article: sa ?in the?.
At an earlier stage of the treebank?s develop-
ment, we carried out on an inter-annotator agree-
ment (IAA) study. The study involved four stages.
(i) The first experiment (IAA-1) involved the as-
sessment of annotator agreement following the in-
troduction of a second annotator. The results re-
ported a Kappa score of 0.79, LAS of 74.4% and
UAS of 85.2% (Lynn et al, 2012a). (ii) We then
held three workshops that involved thorough anal-
ysis of the output of IAA-1, highlighting disagree-
ments between annotators, gaps in the annotation
guide, shortcomings of the labelling scheme and lin-
guistic issues not yet addressed. (iii) The annotation
guide, labelling scheme and treebank were updated
accordingly, addressing the highlighted issues. (iv)
Finally, a second inter-annotator agreement exper-
iment (IAA-2) was carried out presenting a Kappa
score of 0.85, LAS of 79.2% and UAS of 87.8%
(Lynn et al, 2012b).
We found that the IAA study was valuable in the
development of the treebank, as it resulted in im-
1A tagged, randomised subset of the NCII, (New Corpus for
Ireland - Irish http://corpas.focloir.ie/), comprised of text from
books, news data, websites, periodicals, official and government
documents.
2For example there are disagreements over the existence of
a VP in Irish and whether the language has a VSO or an under-
lying SVO structure.
3
provement of the quality of the labelling scheme,
the annotation guide and the linguistic analysis of
the Irish language. Our updated labelling scheme
is now hierarchical, allowing for a choice between
working with fine-grained or coarse-grained labels.
The scheme has now been finalised. A full list of
the labels can be found in Lynn et al (2012b). The
treebank currently contains 803 gold-standard trees.
4 Preliminary Parsing Experiments
In our previous work (Lynn et al, 2012a), we car-
ried out some preliminary parsing experiments with
MaltParser and 10-fold cross-validation using 300
gold-standard trees. We started out with the fea-
ture template used by C?etinog?lu et al (2010) and ex-
amined the effect of omitting LEMMA, WORDFORM,
POSTAG and CPOSTAG features and combinations
of these, concluding that it was best to include all
four types of information. Our final LAS and UAS
scores were 63.3% and 73.1% respectively. Follow-
ing the changes we made to the labelling scheme
as a result of the second IAA study (described
above), we re-ran the same parsing experiments on
the newly updated seed set of 300 sentences - the
LAS increased to 66.5% and the UAS to 76.3%
(Lynn et al, 2012b).
In order to speed up the treebank creation, we also
applied an active learning approach to bootstrapping
the annotation process. This work is also reported in
Lynn et al (2012b). The process involved training a
MaltParser model on a small subset of the treebank
data, and iteratively, parsing a new set of sentences,
selecting a 50-sentence subset to hand-correct, and
adding these new gold sentences to the training set.
We compared a passive setup, in which the parses
that were selected for correction were chosen at ran-
dom, to an active setup, in which the parses that
were selected for correction were chosen based on
the level of disagreement between two parsers (Malt
and Mate). The active approach to annotation re-
sulted in superior parsing results to the passive ap-
proach (67.2% versus 68.1% LAS) but the differ-
ence was not statistically significant.
5 Semi-Supervised Parsing Experiments
In order to alleviate data sparsity issues brought
about by our lack of training material, we experi-
ment with automatically expanding our training set
using well known semi-supervised techniques.
5.1 Self-Training
5.1.1 Related Work
Self-training, the process of training a system on
its own output, has a long and chequered history in
parsing. Early experiments by Charniak (1997) con-
cluded that self-training is ineffective because mis-
takes made by the parser are magnified rather than
smoothed during the self-training process. The self-
training experiments of Steedman et al (2003) also
yielded disappointing results. Reichart and Rap-
paport (2007) found, on the other hand, that self-
training could be effective if the seed training set
was very small. McClosky et al (2006) also re-
port positive results from self-training, but the self-
training protocol that they use cannot be considered
to be pure self-training as the first-stage Charniak
parser (Charniak, 2000) is retrained on the output of
the two-stage parser (Charniak and Johnson, 2005)
They later show that the extra information brought
by the discriminative reranking phase is a factor
in the success of their procedure (McClosky et al,
2008). Sagae (2010) reports positive self-training re-
sults even without the reranking phase in a domain
adaptation scenario, as do Huang and Harper (2009)
who employ self-training with a PCFG-LA parser.
5.1.2 Experimental Setup
The labelled data available to us for this experi-
ment comprises the 803 gold standard trees referred
to in Section 3. This small treebank includes the
150-tree development set and 150-tree test set used
in experiments by Lynn et al (2012b). We use the
same development and test sets for this study. As
for the remaining 503 trees, we remove any trees
that have more than 200 tokens. The motivation for
this is two-fold: (i) we had difficulties training Mate
parser with long sentences due to memory resource
issues, and (ii) in keeping with the findings of Lynn
et al (2012b), the large trees were sentences from
legislative text that were difficult to analyse for au-
tomatic parsers and human annotators. This leaves
us with 500 gold-standard trees as our seed training
data set.
For our unlabelled data, we take the next 1945
sentences from the gold standard 3,000-sentence
4
A is a parser.
M iA is a model of A at step i.
P iA is a set of trees produced using M
i
A.
U is a set of sentences.
U i is a subset of U at step i.
L is the manually labelled seed training set.
LiA is labelled training data for A at step i.
Initialise:
L0A ? L.
M0A ? Train(A,L
0
A)
for i = 1? N do
U i ? Add set of unlabelled sentences from U .
P iA ? Parse(U
i , M iA)
Li+1A ? L
i
A + P
i
A
M i+1A ? Train(A,L
i+1
A )
end for
Figure 1: Self-training algorithm
POS-tagged corpus referred to in Section 3. When
we remove sentences with more than 200 tokens, we
are left with 1938 sentences in our unlabelled set.
The main algorithm for self-training is given in
Figure 1. We carry out two separate experiments
using this algorithm. In the first experiment we use
Malt. In the second experiment, we substitute Mate
for Malt.3
The steps are as follows: Initialisation involves
training the parser on a labelled seed set of 500 gold
standard trees (L0A), resulting in a baseline parsing
model: M iA. We divide the set of gold POS-tagged
sentences (U ) into 6 sets, each containing 323 sen-
tences U i. For each of the six iterations in this ex-
periment i = [1?6], we parse U i. Each time, the set
of newly parsed sentences (PA) is added to the train-
ing set LiA to make a larger training set of L
i+1
A . A
new parsing model (M i+1A ) is then induced by train-
ing with the new training set.
5.1.3 Results
The results of our self-training experiments are
presented in Figure 2. The best Malt model was
trained on 2115 trees, at the 5th iteration (70.2%
LAS). UAS scores did not increase over the baseline
(79.1%). The improvement in LAS over the baseline
is not statistically significant. The best Mate model
was trained on 1792 trees, at the 4th iteration (71.2%
3Versions used: Maltparser v1.7 (stacklazy parsing algo-
rithm); Mate tools v3.3 (graph-based parser)
Figure 2: Self-Training Results on the Development Set
LAS, 79.2% UAS). The improvement over the base-
line is not statistically significant.
5.2 Co-Training
5.2.1 Related Work
Co-training involves training a system on the out-
put of a different system. Co-training has found
more success in parsing than self-training, and it
is not difficult to see why this might be the case
as it can be viewed as a method for combining the
benefits of individual parsing systems. Steedman
et al (2003) directly compare co-training and self-
training and find that co-training outperforms self-
training. Sagae and Tsujii (2007) successfully em-
ploy co-training in the domain adaption track of the
CoNLL 2007 shared task on dependency parsing.
5.2.2 Experimental Setup
In this and all subsequent experiments, we use
both the same training data and unlabelled data that
we refer to in Section 5.1.2.
Our co-training algorithm is given in Figure 3 and
it is the same as the algorithm provided by Steedman
et al (2003). Again, our experiments are carried out
using Malt and Mate. This time, the experiments are
run concurrently as each parser is bootstrapped from
the other parser?s output.
5
A and B are two different parsers.
M iA and M
i
B are models of A and B at step i.
P iA and P
i
B are a sets of trees produced using M
i
A and M
i
B .
U is a set of sentences.
U i is a subset of U at step i.
L is the manually labelled seed training set.
LiA and L
i
B are labelled training data for A and B at step i.
Initialise:
L0A ? L
0
B ? L.
M0A ? Train(A,L
0
A)
M0B ? Train(B,L
0
B)
for i = 1? N do
U i ? Add set of unlabelled sentences from U .
P iA ? Parse(U
i , M iA)
P iB ? Parse(U
i , M iB)
Li+1A ? L
i
A + P
i
B
Li+1B ? L
i
B + P
i
A
M i+1A ? Train(A,L
i+1
A )
M i+1B ? Train(B,L
i+1
B )
end for
Figure 3: Co-training algorithm
The steps are as follows: Initialisation involves
training both parsers on a labelled seed set of 500
gold standard trees (L0A and L
0
B), resulting in two
separate baseline parsing models: M iA (Malt) and
M iB (Mate). We divide the set of gold POS-tagged
sentences (U ) into 6 sets, each containing 323 sen-
tences U i. For each of the six iterations in this ex-
periment i = [1? 6], we use Malt and Mate to parse
U i. This time, the set of newly parsed sentences P iB
(Mate output) is added to the training set LiA to make
a larger training set of Li+1A (Malt training set). Con-
versely, the set of newly parsed sentences P iA (Malt
output) is added to the training set LiB to make a
larger training set of Li+1B (Mate training set). Two
new parsing models (M i+1A and M
i+1
B ) are then in-
duced by training Malt and Mate respectively with
their new training sets.
5.2.3 Results
The results of our co-training experiment are pre-
sented in Figure 4. The best Malt model was trained
on 2438 trees, at the final iteration (71.0% LAS
and 79.8% UAS). The improvement in UAS over
the baseline is statistically significant. Mate?s best
model was trained on 823 trees on the second iter-
ation (71.4% LAS and 79.9% UAS). The improve-
ment over the baseline is not statistically significant.
Figure 4: Co-Training Results on the Development Set
5.3 Sample-Selection-Based Co-Training
5.3.1 Related Work
Sample selection involves choosing training items
for use in a particular task based on some criteria
which approximates their accuracy in the absence of
a label or reference. In the context of parsing, Re-
hbein (2011) chooses additional sentences to add to
the parser?s training set based on their similarity to
the existing training set ? the idea here is that sen-
tences that are similar to training data are likely to
have been parsed properly and so are ?safe? to add
to the training set. In their parser co-training experi-
ments, Steedman et al (2003) sample training items
based on the confidence of the individual parsers (as
approximated by parse probability).
In Active Learning research, the Query By Com-
mittee selection method (Seung et al, 1992) is used
to choose items for annotation ? if a committee of
two or more systems disagrees on an item, this is ev-
idence that the item needs to be prioritised for man-
ual correction (see for example Lynn et al (2012b)).
Steedman et al (2003) discuss a sample selection
approach based on differences between parsers ? if
parser A and parser B disagree on an analysis, parser
A can be improved by being retrained on parser B?s
analysis, and vice versa. In contrast, Ravi et al
(2008) show that parser agreement is a strong in-
6
dicator of parse quality, and in parser domain adap-
tation, Sagae and Tsujii (2007) and Le Roux et al
(2012) use agreement between parsers to choose
which automatically parsed target domain items to
add to the training set.
Sample selection can be used with both self-
training and co-training. We restrict our attention
to co-training since our previous experiments have
demonstrated that it has more potential than self-
training. In the following set of experiments, we ex-
plore the role of both parser agreement and parser
disagreement in sample selection in co-training.
5.3.2 Agreement-Based Co-Training
Experimental Setup The main algorithm for
agreement-based co-training is given in Figure 5.
Again, Malt and Mate are used. However, this algo-
rithm differs from the co-training algorithm in Fig-
ure 3 in that rather than adding the full set of 323
newly parsed trees (P iA and P
i
B) to the training set
at each iteration, selected subsets of these trees (P iA?
and P iB?) are added instead. To define these subsets,
we identify the trees that have 85% or higher agree-
ment between the two parser output sets. As a re-
sult, the number of trees in the subsets differ at each
iteration. For iteration 1, 89 trees reach the agree-
ment threshold; iteration 2, 93 trees; iteration 3, 117
trees; iteration 4, 122 trees; iteration 5, 131 trees;
iteration 6, 114 trees. The number of trees in the
training sets is much smaller compared with those
in the experiments of Section 5.2.
Results The results for agreement-based co-
training are presented in Figure 6. Malt?s best
model was trained on 1166 trees at the final iteration
(71.0% LAS and 79.8% UAS). Mate?s best model
was trained on 1052 trees at the 5th iteration (71.5%
LAS and 79.7% UAS). Neither result represents a
statistically significant improvement over the base-
line.
5.3.3 Disagreement-based Co-Training
Experimental Setup This experiment uses the
same sample selection algorithm we used for
agreement-based co-training (Figure 5). For this ex-
periment, however, the way in which the subsets
of trees (P iA? and P
i
B?) are selected differs. This
time we choose the trees that have 70% or higher
disagreement between the two parser output sets.
A and B are two different parsers.
M iA and M
i
B are models of A and B at step i.
P iA and P
i
B are a sets of trees produced using M
i
A and M
i
B .
U is a set of sentences.
U i is a subset of U at step i.
L is the manually labelled seed training set.
LiA and L
i
B are labelled training data for A and B at step i.
Initialise:
L0A ? L
0
B ? L.
M0A ? Train(A,L
0
A)
M0B ? Train(B,L
0
B)
for i = 1? N do
U i ? Add set of unlabelled sentences from U .
P iA ? Parse(U
i , M iA)
P iB ? Parse(U
i , M iB)
P iA? ? a subset of X trees from P
i
A
P iB ? ? a subset of X trees from P
i
B
Li+1A ? L
i
A + P
i
B ?
Li+1B ? L
i
B + P
i
A?
M i+1A ? Train(A,L
i+1
A )
M i+1B ? Train(B,L
i+1
B )
end for
Figure 5: Sample selection Co-training algorithm
Again, the number of trees in the subsets differ at
each iteration. For iteration 1, 91 trees reach the dis-
agreement threshold; iteration 2, 93 trees; iteration
3, 73 trees; iteration 4, 74 trees; iteration 5, 68 trees;
iteration 6, 71 trees.
Results The results for our disagreement-based
co-training experiment are shown in Figure 7. The
best Malt model was trained with 831 trees at the
4th iteration (70.8% LAS and 79.8% UAS). Mate?s
best models were trained on (i) 684 trees on the 2nd
iteration (71.0% LAS) and (ii) 899 trees on the 5th
iteration (79.4% UAS). Neither improvement over
the baseline is statistically significant.
5.3.4 Non-Iterative Agreement-based
Co-Training
In this section, we explore what happens when
we add the additional training data at once rather
than over several iterations. Rather than testing this
idea with all our previous setups, we choose sample-
selection-based co-training where agreement be-
tween parsers is the criterion for selecting additional
training data.
Experimental Setup Again, we also follow the
algorithm for agreement-based co-training as pre-
sented in Figure 5. However, two different ap-
7
Figure 6: Agreement-based Co-Training Results on the
Development Set
proaches are taken this time, involving only one it-
eration in each. For the first experiment (ACT1a),
the subsets of trees (P iA? and P
i
B?) that are added to
the training data are chosen based on an agreement
threshold of 85% between parsers, and are taken
from the full set of unlabelled data (where U i = U ),
comprising 1938 trees. In this instance, the subset
consists of 603 trees, making a final training set of
1103 trees.
For the second experiment (ACT1b), only trees
meeting a parser agreement threshold of 100% are
added to the training data. 253 trees (P iA? and P
i
B?)
out of 1938 trees (U i = U ) meet this threshold. The
final training set consists of 753 trees.
Results ACT1a proved to be the most accurate
parsing model for Mate overall. The addition of
603 trees that met the agreement threshold of 85%
increased the LAS and UAS scores over the base-
line by 1.0% and 1.3% to 71.8 and 80.4 respec-
tively. This improvement is statistically significant.
Malt showed a LAS improvement of 0.93% and
a UAS improvement of 0.42% (71.0% LAS and
79.6% UAS). The LAS improvement over the base-
line is statistically significant.
The increases for ACT1b, where 100% agreement
trees are added, are less pronounced and are not sta-
Figure 7: Disagreement-based Co-Training Results on
the Development Set
tistically significant. Results showed a 0.5% LAS
and 0.2% UAS increase over the baseline with Malt,
based on the 100% agreement threshold (adding 235
trees). Mate performs at 0.5% above the LAS base-
line and 0.1% above the UAS baseline.
5.4 Analysis
We perform an error analysis for the Malt and Mate
baseline, self-trained and co-trained models on the
development set. We observe the following trends:
? All Malt and Mate parsing models confuse the
subj and obj labels. A few possible rea-
sons for this stand out: (i) It is difficult for
the parser to discriminate between analytic verb
forms and synthetic verb forms. For example,
in the phrase pho?sfainn thusa ?I would marry
you?, pho?sfainn is a synthetic form of the verb
po?s ?marry? that has been inflected with the in-
corporated pronoun ?I?. Not recognising this,
the parser decided that it is an intransitive verb,
taking ?thusa?, the emphatic form of the pro-
noun tu? ?you?, as its subject instead of object.
(ii) Possibly due to a VSO word order, when
the parser is dealing with relative phrases, it
can be difficult to ascertain whether the follow-
ing noun is the subject or object. For example,
an chail??n a chonaic me? inne? ?the girl whom
8
I saw yesterday/ the girl who saw me yester-
day?.4 (iii) There is no passive verb form in
Irish. The autonomous form is most closely
linked with passive use and is used when the
agent is not known or mentioned. A ?hidden?
or understood subject is incorporated into the
verbform. Casadh eochair i nglas ?a key was
turned in a lock? (lit. somebody turned a key
in a lock). In this sentence, eochair ?key? is the
object.
? For both parsers, there is some confusion be-
tween the labelling of obl and padjunct,
both of which mark the attachment between
verbs and prepositions. Overall, Malt?s con-
fusion decreases over the 6 iterations of self-
training, but Mate begins to incorrectly choose
padjunct over obl instead. Mixed results
are obtained using the various variants of co-
training.
? Mate handles coordination better than Malt.5 It
is not surprising then that co-training Malt us-
ing Mate parses improves Malt?s coordination
handling whereas the opposite is the case when
co-training Mate on Malt parses, demonstrat-
ing that co-training can both eliminate and in-
troduce errors.
? Other examples of how Mate helps Malt during
co-training is in the distinction between top
and comp relations, between vparticle
and relparticle, and in the analysis of
xcomps.
? Distinguishing between relative and cleft par-
ticles is a frequent error for Mate, and there-
fore Malt also begins to make this kind of error
when co-trained using Mate. Mate improves
using sample-selection-based co-training with
Malt.
? The sample-selection-based co-training vari-
ants show broadly similar trends to the basic
co-training.
4Naturally ambiguous Irish sentences like this require con-
text for disambiguation.
5Nivre and McDonald (2007) make a similar observation
when they compare the errors made by graph and transition
based dependency parsers.
Parsing Models LAS UAS
Development Set
Malt Baseline: 70.0 79.1
Malt Best (co-train) : 71.0 80.2
Mate Baseline: 70.8 79.1
Mate Best (85% threshold ACT1a): 71.8 80.4
Test Set
Malt Baseline: 70.2 79.5
Malt Best (co-train) : 70.8 79.8
Mate Baseline: 71.9 80.1
Mate Best (85% threshold ACT1a): 73.1 81.5
Table 1: Results for best performing models
5.5 Test Set Results
The best performing parsing model for Malt on
the development set is in the final iteration of the
basic co-training approach in Section 5.2. The
best performing parsing model for Mate on the de-
velopment set is the non-iterative 85% threshold
agreement-based co-training approach described in
Section 5.3.4. The test set results for these opti-
mal development set configurations are also shown
in Table 1. The baseline model for Malt obtains
a LAS of 70.2%, the final co-training iteration a
LAS of 70.8%. The baseline model for Mate ob-
tains a LAS of 71.9%, and the non-iterative 85%
agreement-based co-trained model obtains a LAS of
73.1%.
6 Parsing Experiments Using
Morphological Features
As well as the size of the dataset, data sparsity is
also confounded by the number of possible inflected
forms for a given root form. With this in mind,
and following on from the discussion in Section 5.4,
we carry out further parsing experiments in an at-
tempt to make better use of morphological informa-
tion during parsing. We attack this in two ways: by
reducing certain words to their lemmas and by in-
cluding morphological information in the optional
FEATS (features) field. The reasoning behind re-
ducing certain word forms to lemmas is to further
reduce the differences between inflected forms of
the same word, and the reasoning behind including
morphological information is to make more explicit
the similarity between two different word forms in-
flected in the same way. All experiments are car-
9
Parsing Models (Malt) LAS UAS
Baseline: 70.0 79.1
Lemma (Pron Prep): 69.7 78.9
Lemma + Pron Prep Morph Features: 69.6 78.9
Form + Pron Prep Morph Features: 69.8 79.1
Verb Morph Features: 70.0 79.1
Table 2: Results with morphological features on the de-
velopment set
ried out with MaltParser and our seed training set
of 500 gold trees. We focus on two phenomena:
prepositional pronouns or pronominal prepositions
(see Section 2) and verbs with incorporated subjects
(see Section 2 and Section 5.4).
In the first experiment, we include extra mor-
phological information for pronominal prepositions.
We ran three parsing experiments: (i) replacing the
value of the surface form (FORM) of pronominal
prepositions with their lemma form (LEMMA), for
example agam?ag, (ii) including morphological in-
formation for pronominal prepositions in the FEATS
column. For example, in the case of agam ?at me?,
we include Per=1P|Num=Sg, (iii) we combine
both approaches of reverting to lemma form and also
including the morphological features. The results
are given in Table 2.
In the second experiment, we include morpholog-
ical features for verbs with incorporated subjects:
imperative verb forms, synthetic verb forms and au-
tonomous verb forms such as those outlined in Sec-
tion 5.4. For each instance of these verb types, we
included incorpSubj=true in the FEATS col-
umn. The results are also given in Table 2.
The experiments on the pronominal prepositions
show a drop in parsing accuracy while the experi-
ments carried out using verb morphological infor-
mation showed no change in parsing accuracy.6 In
the case of inflected prepositions, perhaps we have
not seen any improvement because we have not fo-
cused on a phenomenon which is critical for parsing.
More experimentation is necessary.
7 Concluding Remarks
We have presented two sets of experiments which
aim to improve dependency parsing performance for
6Although the total number of correct attachments are the
same, the parser output is different.
a minority language with a very small treebank. In
the first set of experiments, the main focus of the pa-
per, we tried to overcome the limited treebank size
by increasing the parsers? training sets using auto-
matically parsed sentences. While we do manage
to achieve statistically significant improvements in
some settings, it is clear from the results that the
gains in parser accuracy through semi-supervised
bootstrapping methods are fairly modest. Yet, in the
absence of more gold labelled data, it is difficult to
know now whether we would achieve similar or im-
proved results by adding the same amount of gold
training data. This type of analysis will be interest-
ing at a later date when the unlabelled trees used in
these experiments are eventually annotated and cor-
rected manually.
The second set of experiments tries to mitigate
some of the data sparseness issues by exploiting
morphological characteristics of the language. Un-
fortunately, we do not see any improvements but we
may get different results if we repeat these experi-
ments using the larger semi-supervised training sets
from the first set of experiments.
There are many directions this parsing research
could take us in the future. Our unlabelled data con-
sisted of sentences annotated with gold POS tags.
In the future we would like to take advantage of
the fully unlabelled, untagged data in the New Cor-
pus for Ireland ? Irish, which consists of 30 million
words. We would also like to experiment with a fully
unsupervised parser using this dataset. Our Malt fea-
ture models are manually optimised ? it would be in-
teresting to experiment with optimising them using
MaltOptimizer (Ballesteros, 2012). An additional
avenue of research would be to exploit the hierar-
chical nature of the dependency scheme to arrive at
more flexible way of measuring agreement or dis-
agreement in sample selection.
Acknowledgements
We thank the three anonymous reviewers for their
helpful feedback. This work is supported by Sci-
ence Foundation Ireland (Grant No. 07/CE/I1142)
as part of the Centre for Next Generation Localisa-
tion (www.cngl.ie) at Dublin City University.
10
References
Miguel Ballesteros. 2012. Maltoptimizer: A sys-
tem for maltparser optimization. In Proceedings of
the Eighth International Conference on Linguistic Re-
sources and Evaluation (LREC), pages 2757?2763, Is-
tanbul, Turkey.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of COL-
ING.
O?zlem C?etinog?lu, Jennifer Foster, Joakim Nivre, Deirdre
Hogan, Aoife Cahill, and Josef van Genabith. 2010.
LFG without c-structures. In Proceedings of the 9th
International Workshop on Treebanks and Linguistic
Theories.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd ACL.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of AAAI.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of the First Annual Meeting
of the North American Chapter of the Association for
Computational Linguistics (NAACL-00).
Christian-Brothers. 1988. New Irish Grammar. Dublin:
C J Fallon.
David Greene. 1966. The Irish Language. Dublin: The
Three Candles.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In Proceedings of EMNLP.
John Judge, Ailbhe N?? Chasaide, Rose N?? Dhubhda,
Kevin P. Scannell, and Elaine U?? Dhonnchadha. 2012.
The Irish Language in the Digital Age. Springer Pub-
lishing Company, Incorporated.
Joseph Le Roux, Jennifer Foster, Joachim Wagner, Ra-
soul Samed Zadeh Kaljahi, and Anton Bryl. 2012.
DCU-Paris13 systems for the sancl 2012 shared task.
In Working Notes of SANCL.
Teresa Lynn, O?zlem C?etinog?lu, Jennifer Foster, Elaine U??
Dhonnchadha, Mark Dras, and Josef van Genabith.
2012a. Irish treebanking and parsing. In Proceedings
of the Eight International Conference on Language
Resources and Evaluation, pages 1939?1946.
Teresa Lynn, Jennifer Foster, Mark Dras, and Elaine U??
Dhonnchadha. 2012b. Active learning and the Irish
treebank. In Proceeedings of the Australasian Lan-
guage Technology Workshop (ALTA), pages 23?32.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, Main Conference, pages 152?159, New
York City, USA, June. Association for Computational
Linguistics.
David McClosky, Eugene Charniak, and Mark Johnson.
2008. When is self-training effective for parsing? In
Proceedings of COLING.
Joakim Nivre and Ryan McDonald. 2007. Characteriz-
ing the errors of data-driven dependency parsing mod-
els. In Proceedings of EMNLP-CoNLL, Prague, Czech
Republic.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation
(LREC2006).
M??chea?l O? Siadhail. 1989. Modern Irish: Grammatical
structure and dialectal variation. Cambridge: Cam-
bridge University Press.
Sujith Ravi, Kevin Knight, and Radu Soricut. 2008. Au-
tomatic prediction of parser accuracy. In Proceedings
of EMNLP, Hawaii.
Ines Rehbein. 2011. Data point selection for self-
training. In Proceedings of the Second Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2011), Dublin, Ireland.
Roi Reichart and Ari Rappaport. 2007. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. In Proceedings of
ACL.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL shared task
session of EMNLP-CoNLL.
Kenji Sagae. 2010. Self-training without reranking for
parser domain adapation and its impact on semantic
role labelling. In Proceedings of the ACL Workshop
on Domain Adaptation for NLP.
Sebastian Seung, Manfred Opper, and Haim Sompolin-
sky. 1992. Query by committee. In Proceedings
of the Fifth Annual ACM Workshop on Computational
Learning Theory.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrap-
ping statistical parsers from small datasets. In Pro-
ceedings of the tenth conference on European chapter
of the Association for Computational Linguistics - Vol-
ume 1, EACL ?03, pages 331?338, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Nancy Stenson. 1981. Studies in Irish Syntax. Tu?bingen:
Gunter Narr Verlag.
Elaine U?? Dhonnchadha. 2009. Part-of-Speech Tagging
and Partial Parsing for Irish using Finite-State Trans-
ducers and Constraint Grammar. Ph.D. thesis, Dublin
City University.
11
