NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 45?48,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
One Year of Contender: What Have We Learned about Assessing and
Tuning Industrial Spoken Dialog Systems?
David Suendermann
SpeechCycle, New York, USA
david@suendermann.com
Roberto Pieraccini
ICSI, Berkeley, USA
roberto@icsi.berkeley.edu
Abstract
A lot. Since inception of Contender, a ma-
chine learning method tailored for computer-
assisted decision making in industrial spo-
ken dialog systems, it was rolled out in over
200 instances throughout our applications pro-
cessing nearly 40 million calls. The net ef-
fect of this data-driven method is a signifi-
cantly increased system performance gaining
about 100,000 additional automated calls ev-
ery month.
1 From the unwieldiness of data to the
Contender process
Academic institutions involved in the research on
spoken dialog systems often lack access to data for
training, tuning, and testing their systems. This is
simply because the majority of systems only live in
laboratory environments and hardly get deployed to
the live user1. The lack of data can result in sys-
tems not sufficiently tested, models trained on non-
representative or artificial data, and systems of lim-
ited domains (usually restaurant or flight informa-
tion).
On the other hand, in industrial settings, spoken
dialog systems are often deployed to take over tasks
of call center agents associated with potentially very
large amounts of traffic. Here, we are speaking of
applications which may process more than one mil-
lion calls per week. Having applications log every
1One of the few exceptions to this rule is the Let?s Go bus in-
formation system maintained at the Carnegie Mellon University
in Pittsburgh (Raux et al., 2005).
action they take during the course of a call can pro-
vide developers with valuable data to tune and test
the systems they maintain. As opposed to the aca-
demic world, often, there appears to be too much
data to capture, permanently store, mine, and re-
trieve. Harddisks on application servers run full,
log processing scripts demand too much comput-
ing capacity, database queues get stuck, queries slow
down, and so on and so forth. Even if these billions
and billions of log entries are eventually available
for random access from a highly indexed database
cluster, it is not clear what one should search for
in an attempt to improve a dialog system?s perfor-
mance.
About a year and a half ago, we proposed a
method we called Contender playing the role of a
live experiment in a deployed spoken dialog sys-
tem (Suendermann et al., 2010a). Conceptually, a
Contender is an activity in a call flow which has an
input transition and multiple output transitions (al-
ternatives). When a call hits a Contender?s input
transition, a randomization is carried out to deter-
mine which alternative the call will continue with
(see Figure 1). The Contender itself does not do any-
thing else but performing the random decision dur-
ing runtime. The different call flow activities and
processes the individual alternatives get routed to
make calls depend on the Contenders? decisions.
Say, one wants to find out which of ten possible
time-out settings in an activity is optimal. This could
be achieved by duplicating the activity in question
ten times and setting each copy?s time-out to a dif-
ferent value. Now, a Contender is placed whose ten
alternatives get connected to the ten competing ac-
45
randomizer
Alternative 1 Alternative 2 Alternative 3
randomization 
weights
Figure 1: Contender with three alternatives.
tivities. Finally, the outbound transitions of the com-
peting activities have to be bundled to make the rest
of the application be independent of the Contender.
A Contender can be used for all sorts of exper-
iments in dialog systems. For instance, if system
designers are unsure about which of a number of
prompts has more expressive power, they can imple-
ment all of them in the application and have the Con-
tender decide at runtime which one to play. Or if it is
unclear which actions to perform in which order, dif-
ferent strategies can be compared using a Contender.
The same applies to certain parameter settings, error
handling approaches, confirmation strategies, and so
on. Every design aspect with one or more alterna-
tives can be implemented by means of a Contender.
Once an application featuring Contenders starts
taking live production traffic, an analysis has to be
carried out, to determine which alternative results
in the highest average performance. In doing so,
it is crucial to implement some measure of statisti-
cal significance as, otherwise, conclusions may be
misleading. If no statistical significance measure
was in place, processing two calls in a two-way
Contender, one routed to Alternative 1 and ending
up automated and one routed to Alternative 2 end-
ing up non-automated, could lead to the conclu-
sion that Alternative 1?s automation rate is 100%
and Alternative 2?s is 0. To avoid such potentially
erroneous conclusions, we are using two-sample t-
tests for Contenders with two alternatives and pair-
wise two-sample t-tests with probability normaliza-
tion for more alternatives as measures of statistical
significance. A more exact but computationally very
expensive method was explained in (Suendermann
et al., 2010a), but for the sake of performing statis-
tical analysis with acceptable delays given the vast
amount of data, we primarily use the former in pro-
duction deployments.
If an alternative is found to statistically signifi-
cantly outperform the other alternatives, it is deemed
the winner, and it would be advisable routing most
(if not all) calls to that alternative. While this hard
reset maximizes performance induced by this Con-
tender going forward, it sometimes takes quite a
while before the required statistical significance is
actually reached. Hence, in the time span before
this hard reset, the Contender may perform subop-
timally. Furthermore, even though statistical mea-
sures could indicate which alternative the likely win-
ner is, this fact is potentially subject to change over
time depending upon alterations in the caller popu-
lation, the distribution of call reasons, or the appli-
cation itself. For this reason, it is recommendable to
keep exploring seemingly underperforming alterna-
tives by routing a very small portion of calls to them.
The statistical model we discussed in (Suender-
mann et al., 2010a) presents a solution to the above
listed issues. The model associates each alternative
of a Contender with a weight controlling which per-
centage of traffic is routed down this alternative on
average. As derived in (Suendermann et al., 2010a),
the weight for an alternative is generated based on
the probability that this alternative is the actual win-
ner of the Contender given the available historic
data. The weights are subject to regular updates
computed by a statistical analysis engine that con-
tinuously analyzes the behavior of all Contenders in
production deployment. In order to do so, the en-
gine accesses the entirety of available application
logs associating performance metrics, such as au-
tomation rate (the fraction of processed calls that
satisfied the call reason) or average handling time
(average call duration), with Contenders and their
alternatives. This is relatively straightforward since
the application can log call category (to tell whether
a call was automated or not), call duration, the Con-
tenders visited and the results of the randomization
at each of the Contender. In Figure 2, a high-level
diagram of the Contender process is shown.
46
w1 wn
application logs
Application
statistical analysis
Analysis Engine
w2
randomizer
Alternative 1 Alternative 2 Alternative n
w1,
w2,
...,
wn
Figure 2: Contender process.
Since statistical analysis of Contenders involves
data points of hundreds of thousands of calls, per-
formance measurement needs to be based on autom-
ically derivable, i.e. objective, metrics. Popular ob-
jective metrics are automation rate, average handling
time, ?speech errors?, retry rate, number of hang-ups
or opt-outs (Suendermann et al., 2010c). There are
also techniques correlating objective metrics to sub-
jective ones in an attempt to predict user or caller ex-
perience, i.e., to evaluate interaction quality as per-
ceived by the caller (Walker et al., 1997; Evanini et
al., 2008; Mo?ller et al., 2008). Despite the impor-
tance of making interactions as smooth and pleasant
as possible, stakeholders of industrial systems often
insist on using metrics directly tied to the savings
generated by the deployed spoken dialog system. As
we introduced in (Suendermann et al., 2010b), sav-
ings mainly depend on automation rate (A) and av-
erage handling time (T ) and can be expressed by the
reward
R = TAA? T
where TA is a trade-off factor that depends on aver-
age agent salary and hosting and telecommunication
fees.
2 A snapshot of our last year?s experiences
Shortly after setting the mathematical foundations of
the Contender process and establishing the involved
software and hardware pieces, the first Contenders
were implemented in production applications. Un-
der the close look of operations, quality assurance,
engineering, speech science, as well as technical ac-
count management departments, the process under-
went a number of refinement cycles. In the mean-
time, more and more Contenders were implemented
into a variety of applications and released into pro-
duction traffic. Until to date, 233 Contenders were
released into production systems processing an total
call volume of 39 million calls. Table 1 shows some
statistics of a number of example Contenders per ap-
plication. These statistics are drawn from spoken di-
alog systems for technical troubleshooting of cable
services as discussed e.g. in (Acomb et al., 2007).
Such applications assist callers fixing problems with
their cable TV or Internet (such as no, slow, or inter-
mittent connection, e-mail issues). In addition to the
application and a short description of the Contender,
the table shows three quantities:
? the number of calls processed by the Contender
since its establishment (# calls),
? the reward difference between the highest- and
lowest-performing alternative of a Contender
?R (a high value indicates that the best-
performing alternative is substantially better
than the worst-performing one, that is, the Con-
tender is very effective), and
? an estimate of the number of automated calls
gained or saved per month by running the Con-
tender ?At [mo?1] (this value indicates the
net effect of having all calls route through
the best-performing alternative vs. the worst-
performing one, that is, the upper bound of how
many calls were gained or saved). This metric
47
Table 1: Statistics of example Contenders.
application Contender # calls ?At [mo?1] ?R
TV problem capture 13,477,810 40,362 0.05
TV cable box reboot order 4,322,428 28,975 0.11
TV outage prediction 2,758,963 08,198 0.04
TV on demand 485,300 08,123 0.17
TV input source troubleshooting 1,162,445 03,487 0.05
TV account lookup 9,627 03,201 0.02
Internet troubleshooting paths I 275,248 05,568 0.02
Internet troubleshooting paths II 1,389,489 03,530 0.01
Internet computer monitor instruction 1,500,010 03,271 0.01
TV/Internet opt in 6,865,929 31,764 0.05
is calculated by multiplying the observed dif-
ference in automation rate?Awith the number
of monthly calls hitting the Contender (t).
3 Conclusion
We have seen that the use of Contenders (a method
to assess and tune arbitrary components of indus-
trial spoken dialog systems) can be very benefi-
cial in multiple respects. Applications can self-
correct as soon as reliable data becomes available
without additional manual analysis and intervention.
Moreover, performance can increase substantially
in applications implementing Contenders. Looking
at only the 10 best-performing Contenders out of
233 running in our applications to-date, the number
of automated calls increased by about 100,000 per
month.
However, multiple Contenders that are active in
the same call flow cannot always be regarded inde-
pendent of each other. A routing decision made in
Contender 1 earlier in the call can potentially have
an impact on which decision is optimal in Contender
2 further down the call. In this respect, reward gains
of Contenders installed in the same application are
not necessarily additive. Not only can optimal deci-
sions in a Contender depend on other Contenders but
also on other runtime parameters such as time of the
day, day of the week, geographic origin of the caller
population, or the equipment used by the caller. Our
current research focuses on evaluating these depen-
dencies and accordingly optimize the way decisions
are made in Contenders.
References
