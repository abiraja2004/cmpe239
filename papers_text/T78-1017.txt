Speech Acts as a Basis for Understanding Dialogue Coherence 
by 
C. Raymond Perraul t  and James F. Al len 
Dept. of Computer Science 
Univers i ty  of Toronto 
Toronto Canada 
and 
Phi l ip  R. Cohen 
Bolt Beranek and Newman 
Cambridge Mass. 
i. Introduct ion 
Webster 's  d ict ionary def ines 
"coherence" as "the qual i ty of being 
logical ly integrated, consistent,  and 
intel l igible". If one were asked whether 
a sequence of physical  acts being 
performed by an agent was coherent, a 
crucial  factor in the decis ion would be 
whether the acts were perceived as 
contr ibut ing to the achievement of an 
overal l  goal. In that case they can 
frequently be described briefly, by naming 
the goal or the procedure executed to 
achieve it. Once the intended goal has 
been conjectured, the sequence can be 
descr ibed as a more or less correct, more 
or less optimal attempt at the achievement 
of the goal. 
One of the mainstreams of AI research 
has been the study of problem solving 
behaviour in humans and its s imulat ion by 
machines. This can be considered as the 
task of transforming an initial state of 
the world into a goal state by f inding an 
appropr iate sequence of appl icat ions of 
operators from a given set. Each operator 
has two modes of execution: in the f irst 
it changes the "real world", and in the 
second it changes a model of the real 
world. Sequences of these operators we 
call plans. They can be constructed, 
simulated, executed, opt imized and 
debugged. Operators are usual ly thought 
of as achieving certain effects and of 
being appl icable only when certain 
precondit ions hold. 
The effects of one agent executing his 
plans may be observable by other agents, 
who, assuming that these plans were 
produced by the first agent's plan 
construct ion algorithms, may try to infer 
the plan being executed from the observed 
changes to the world. The fact that this 
inferencing may be intended by the f irst 
agent underl ies human communicat ion.  
* This research was supported in part by 
the National Research Counci l  of Canada. 
Each agent maintains a model of the 
world, including a model of the models of 
other agents. L inguist ic  utterances are 
the result  of the execut ion of operators 
whose effects are main ly  on the models 
that the speaker and hearer maintain of 
each other. These effects are intended by 
the speaker to be produced part ly  by the 
hearer 's  recognit ion of the speaker's 
plan. 
This view of the communicat ion process 
is very c lose in spir it  to the Aust in-  
Gr ice-St rawson-Sear le  approach to 
i l locut ionary acts, and indeed was 
strongly inf luenced by it. We are working 
on a theory of speech acts based on the 
notions of plans, world models, plan 
construct ion and plan recognit ion. It is 
intended that this theory should answer 
quest ions such as: 
(i) Under what c i rcumstances can an 
observer bel ieve that a speaker has 
s incerely and non-defect ive ly  performed a 
part icular i l locut ionary act in producing 
utterance for a hearer? The observer 
could also be the hearer or speaker. 
(2) What changes does the successful 
execut ion of a speech act make to the 
speaker's  model of the hearer, and to the 
heater 's  model of the speaker? 
(3) How is the meaning (sense/reference) 
of an utterance x related to the acts that 
can be performed in uttering x? 
A theory of speech acts based on plans 
must specify at least the fol lowing: 
(i) A Planning System: a language for 
descr ib ing states of the world, a language 
for descr ib ing operators and algor i thms 
for plan construct ion and plan inference. 
Semantics for the languages should also be 
given. 
(2) Def in i t ions of speech acts as  
operators in the planning system. What 
are their effects? When are they 
appl icable? How can they be real ized in 
words? 
125 
To make possib le a first attempt at 
such a theory we have imposed several 
restr ict ions on the system to be model led. 
(I) Any agent Al's model of another agent 
A2 is def ined in terms of "facts" that A1 
bel ieves A2 believes, and goals that A1 
bel ieves A2 is attempting to achieve. We 
are not attempting to model obl igations, 
feel ings ~ etc. 
(2) The only speech acts we try to model 
are some that appear to be def inable in 
terms of bel iefs and goals, namely REQUEST 
and INFORM. We have been taking these to 
be prototypical  members of Sear le 's  
"direct ive" and "representat ive" classes 
(Searle (1976)). We represent quest ions 
as REQUESTs to INFORM. These acts are 
interesting for they have a wide range of 
syntact ic real izat ions, and account for a 
large proport ion of everyday utterances. 
(3) We have l imited ourselves so far to 
the study of so-cal led task-or iented 
d ia logues which we interpret to be 
conversat ions between two agents 
cooperat ing in the achievement of a single 
high- level  goal. These dia logues do not 
al low changes in the topic of d iscourse 
but stil l  d isplay a wide range of 
l inguist ic behaviour.  
Much of our work so far has dealt  with 
the problem of generat ing plans containing 
REQUEST and INFORM, as well as non- 
l inguist ic operators.  Suppose that an 
agent is attempting to achieve some task, 
with incomplete knowledge of that task and 
of the methods to complete it, but with 
some knowledge of the abi l i t ies of another 
agent. How can the f irst agent make use of 
the abi l i t ies of the second? Under what 
c i rcumstances can the first useful ly 
produce utterances to transmit or acquire 
facts and goals? How can he init iate 
act ion on the part of the second? 
We view the plan related aspects of 
language generat ion and recognit ion as 
indissociable,  and strongly related to the 
process by which agents cooperate in the 
achievement of goals. For example, for 
agent2 to reply "It's closed" to agent l 's  
query "Where's the nearest service 
station?" seems to require him to infer 
that agentl wants to make use of the  
service stat ion which he could not do if 
it were closed. The reply "Two blocks 
east" would be seen as mis leading if g iven 
alone, and unnecessary if g iven along with 
"It 's closed". Thus part  of cooperat ive 
behaviour is the detect ion by one a~ent of 
obstacles in the plans he bel ieves the 
other agent holds, possib ly fo l lowed by an 
attempt to overcome them. We claim that 
speakers expect (and intend) hearers to 
operate this way and therefore that any 
hearer can assume that inferences that he 
can draw based on knowledge that is shared 
with the speaker are in fact intended by 
the speaker. These processes under!~e our 
analysis  of indirect speech acts (such as 
"Can you pass the salt?") - utterances 
which appear to result from one 
i l locut ionary act but can be used to 
perform another. 
Sect ion 2 of this paper out l ines some 
requirements on the models which the 
var ious agents must have of each other. 
Sect ion 3 descr ibes the planning operators 
for REQUEST and INFORM, and how they can 
be used to generate plans which include 
assert ions,  imperatives, and several types 
of questions. 
Sect ion 4 d iscusses the relat ion 
between the operators  of sect ion 3 and the 
l inguist ic  sentences which can realize 
them. We concentrate on the problem of 
identi fying i l locut ionary force, in 
part icular  on indirect speech acts. A 
useful consequence of the i l locut ionary 
force ident i f icat ion process is that it 
provides a natural way to understand some 
el l ipt ical  utterances, and utterances 
whose purpose is to acknowledge, correct  
or c lar i fy  interpretat ions of previous 
utterances. 
A cr i t ical  part of communicat ion is 
the process by which a speaker can 
construct  descr ipt ions of objects involved 
in his plans such that the hearer can 
identify the intended referent. Why can 
someone asking "Where's the screwdriver?" 
be answered with "In the drawer with the 
hammer" if it is assumed he knows where 
the hammer is, but maybe by "In the third 
drawer from the left" if he doesn't.  How 
accurate must descr ipt ive phrases be? 
Sect ion 5 examines how the speaker and 
hearer 's  models of each other inf luence 
their references. Final ly, sect ion 6 
contains some ideas on future research. 
Most examples in the paper are drawn 
from a s i tuat ion in which one part ic ipant  
is an information clerk at a train 
station, whose object ive is to assist 
passengers  in boarding and meeting trains. 
The domain is obviously l imited, but stil l  
provides a natural  setting for a wide 
range of utterances, both in form and in 
intention. 
2. On models  of others 
In this section we present  cr i ter ia 
that one agent 's  model of another ought to 
satisfy. For convenience we dub the 
agents SELF and OTHER. Our research has 
concentrated on model l ing bel iefs and 
goals. We claim that a theory of language 
need not be concerned with what is 
actual ly  true in the real world: it 
should descr ibe language processing in 
terms of a person's bel iefs  about the 
world. Accordingly,  SELF's model of OTHER 
should be based on "believe" as descr ibed, 
for example, in Hint ikka(1962) and not on 
"know" in its sense of "true belief".  
126 
Henceforth, all uses of the words "know" 
and "knowledge" are to be treated as 
synonyms for "believe" and "beliefs". We 
have neglected other aspects of a model of 
another, such as focus of attent ion (but 
see Grosz(1977)) .  
Bel ief 
Clearly, SELF ought to be able to 
d ist inguish his bel iefs about the world 
from what he bel ieves other bel ieves. 
SELF ought to have the poss ib i l i ty  of 
bel ieving a proposi t ion P, of bel ieving 
not-P, or of being ignorant of P. 
Whatever his stand on P, he should also be 
able to bel ieve that OTHER can hold any of 
these posit ions on P. Not ice that such 
d isagreements  cannot be represented if the 
representat ion is based on "know" as in 
Moore(1977).  
SELF's bel ief representat ion ought to 
al low him to represent the fact that OTHER 
knows whether some proposi t ion P is true, 
without SELFIs having to know which of P 
or -P he does believe. Such information 
can be represented as a d is junct ion of 
bel iefs (e.g., OR(OTHER BELIEVE P, OTHER 
BELIEVE ~P)) .  Such d is junct ions are 
essential  to the planning of yes/no 
questions. 
Finally, a bel ief  representat ion must 
d ist inguish between situat ions l ike the 
fol lowing: 
I. OTHER bel ieves that the train leaves 
from gate 8. 
2. OTHER bel ieves that the train has a 
departure gate. 
3. OTHER knows what the departure gate for 
the train is. 
Case 1 can be represented by a proposi t ion 
that contains no variables. Case 2 can be 
represented by a bel ief  of a quanti f ied 
proposi t ion -- i.e., 
OTHER BELIEVE ( 
x (the y ~ GATE(TRAIN,y) = x)) 
However, case 3 is represented 
quanti f ied bel ief namely, 
x OTHER BELIEVE 
(the y : GATE(TRAIN,y) = x) 
by a 
The formal semantics such bel iefs have 
been problematic for phi losophers (cf. 
Quine (1956) and Hint ikka (1962)). Our 
approach to them is discussed in Cohen 
(1978). In Section 3, we discuss how 
quanti f ied bel iefs are used during 
planning, and how they can be acquired 
during conversat ion.  
Want 
Any representat ion of OTHER's goals 
(wants) must d ist inguish such information 
from: OTHER'S beliefs, SELF's bel iefs and 
goals, and (recursively) from the other 's  
model of someone else's bel iefs and goals. 
The representat ion for WANT must also 
al low for d i f ferent scopes of quantif iers.  
For example, it should d ist inguish between 
the readings of "John wants to take a 
train" as "There is a specif ic train which 
John wants to take" or as "John wants to 
take any train". F inal ly it should al low 
arbi t rary embeddings with BELIEVE. Wants 
of bel iefs (as in "SELF wants OTHER to 
bel ieve P") become the reasons for tel l ing 
P to OTHER, whi le bel iefs of wants (e.g., 
SELF Bel ieves SELF wants P) will be the 
way to represent SELF's goals P. 
Level____~s of  Embedding 
A natural quest ion to ask is how many 
levels of bel ief embedding are needed by 
an agent capable of part ic ipat ing in a 
dialogue. Obviously,  to be able to deal 
with a disagreement,  SELF needs two levels 
(SELF BELIEVE and SELF BELIEVE OTHER 
BELIEVE ). If SELF were to lie to OTHER, 
he would have to be able to bel ieve some 
proposi t ion P (i.e. SELF BELIEVE (P)), 
whi le OTHER bel ieves that SELF bel ieves 
not P (i.e. SELF BELIEVE OTHER BELIEVE 
SELF BELIEVE (~P)), and hence he would 
need at least three levels. 
We show in Cohen (1978) how one can 
represent, in a f inite fashion, the 
unbounded number of bel iefs created by any 
communicat ion act or by face-to-face 
situations. The finite representat ion,  
which employs a circular data structure, 
formal izes the concept of mutual bel ief 
(cf. Schiffer (1972)). Typical ly,  all 
these levels of bel ief embedding can be 
represented in three levels, but 
theoret ical ly,  any finite number are 
possible. 
3. U?in@ a Model of the Other to Decide 
What to Say 
As an  aid in evaluat ing speech act 
def init ions,  we have constructed a 
computer program, OSCAR, that plans a 
range of speech acts. The goal of the 
program is to character ize a speaker's 
capaci ty  to issue speech acts by 
predict ing, for specif ied situations, all 
and only those speech acts that would be 
appropr iate ly  issued by a person under the 
c ircumstances.  In this section, we will 
make reference to prototypical  speakers by 
way of the OSCAR program, and to hearers 
by way of the program's user. 
Specif ia l ly,  the program is able to: 
- Plan REQUEST speech acts, for instance 
a speech act that could be real ized by 
127 
"Please open the door", when its goal is 
to get the user to want to perform some 
action. 
- Plan INFORM speech acts, such as one 
that could be real ized by "The door is 
locked", when its goal is to get the user 
to bel ieve some proposit ion.  
- Combine the above to produce mult ip le  
speech acts in one plan, where one speech 
act may establ ish bel iefs of the user that 
can then be employed in the planning of 
another speech act. 
- Plan quest ions as requests that the 
user inform, when its goal is to bel ieve 
something and when it be l ieves that the 
user knows the answer. 
- P lan speech acts incorporat ing third 
part ies,  as in "Ask Tom to tell you where 
the key is and then tell me." 
To i l lustrate the planning of speech 
acts, consider f irst the fol lowing 
s impl i f ied def in i t ions  of REQUEST and 
INFORM as STRIPS- l ike  operators  (cf. F ikes 
and Ni l sson (1971)). Let SP denote the 
speaker, H the hearer, ACT some action, 
and PROP some proposi t ion.  Due to space 
l imitat ions,  the intuit ive Engl ish 
meanings of the formal terms appearing in 
these def in i t ions  wil l  have to suff ice as 
explanat ion.  
REQUEST(SP,H,ACT) 
precondit ions:  
SP BELIEVE H CANDO ACT 
SP BELIEVE H BELIEVE H CANDO ACT 
SP BELIEVE SP WANT TO REQUEST 
effects: 
H BELIEVE SP BELIEVE SP WANT H TO ACT 
INFORM(SP,H,PROP) 
precondit ions:  
SP BELIEVE PROP 
SP BELIEVE SP WANT TO INFORM 
effects: 
H BELIEVE SP BELIEVE PROP 
The program uses a s impl ist ic  
backward-chain ing a lgor i thm that plans 
actions when their ef fects  are wanted as 
subgoals that are not bel ieved to be 
true. It is the testing of precondi t ions  
of the newly planned act ion before 
creating new subgoals that exerc ises the 
program's model of its user. We shall 
br ief ly sketch how to plan a REQUEST. 
Every act ion has "want precondit ions" ,  
which speci fy  that before an agent does 
that action, he must want to do it. OSCAR 
plans REQUEST speech acts to achieve 
prec ise ly  this precondi t ion  of act ions 
that it wants the user to perform. 
Similar ly,  the goal of the user's 
bel ieving some propos i t ion PROP becomes 
OSCAR'S reason for planning to INFORM him 
of PROP. 
Suppose, for example, that OSCAR is 
outs ide a room whose door is c losed and 
that it be l ieves that the user is inside. 
When planning to move itself into the 
room, it might  REQUEST that the user open 
the door. However, it would only plan 
this speech act if it bel ieved that the 
user did not a lready want to open the door 
and if it bel ieved (and bel ieved the user 
believed) that the precondi t ions  to 
opening the door held. If that were not 
so, OSCAR could plan addit ional  INFORM or 
REQUEST speech acts. For example, assume 
that to open a door one needs to have the 
key and OSCAR bel ieves the user doesn ' t  
know where it is. Then OSCAR could plan 
"Please open the door. The key is in the 
closet".  OSCAR thus employs its user 
model  in tel l ing him what it bel ieves he 
needs to know. 
Mediat ing Acts and Per locut ionary  Ef fects  
The effects of INFORM (and REQUEST) 
are model led so that the bearer 's  
bel ieving P (or wanting to do ACT) is not 
essent ia l  to the successful  complet ion of 
the speech act. Speakers, we claim, 
cannot inf luence their hearers' bel iefs  
and goals direct ly.  Thus, the 
per locut ionary  effects of a speech act are 
not part of that act's def in i t ion.  We 
propose, then, as a pr inc ip le  of 
communicat ion  that a speaker 's  purpose in 
s incere communicat ion is to produce in the 
hearer an accurate model of his mental  
state. 
To br idge the gap between the speech 
acts and their intended per locut ionary  
effects,  we posi t  mediat ing acts, named 
CONVINCE and DECIDE, which model what it 
takes to get someone to bel ieve something 
or want to do something. Our current  
analys is  of these mediat ing acts 
t r iv ia l i zes  the processes that they are 
intended to model  by proposing that to 
convince someone of something, for 
example, one need only get that person to 
know that one bel ieves it. 
Using Quant i f ied Bel iefs  -- P lanning 
Quest ions  
Not ice that the 
OSCAR's  gett ing the key 
it is -- is of the form: 
precond i t ion  to 
-- knowing where 
x OSCAR BELIEVE 
(the y : LOC(KEY,y) = x) 
When such a quant i f ied bel ief  is a goal, 
it leads OSCAR to plan the quest ion "Where 
is the key?" (i.e., REQUEST(OSCAR,  USER, 
INFORM(USER, OSCAR, the y 
LOC(KEY,y)) ) .  In creat ing this question, 
OSCAR f irst p lans a CONVINCE and then 
plans the user's INFORM speech act, which 
it then tries to get him to perform by way 
of request ing. 
128 
The above def in i t ion of INFORM is 
inadequate for deal ing with the quant i f ied 
bel iefs that arise in model l ing someone 
else. This INFORM should be viewed as 
that version of the speech act that the 
planning agent (e.g., OSCAR) plans for 
itself to perform. A di f ferent v iew of 
INFORM, say INFORM-BY-OTHER, is necessary 
to represent acts of informing by agents 
other than the speaker. The d i f ference 
between the two INFORMs is that for the 
first, the planner knows what he wants to 
say, but he obviously does not have such 
knowledge of the content of the second 
act. 
The precondit ion for this new act is a 
quanti f ied speaker-bel ief :  
x USER BELIEVE 
(the y : LOC(KEY,y) = x) 
where the user is to be the speaker. For 
the system to plan an INFORM-BY-OTHER act 
for the user, it must bel ieve that the 
user knows where the key is, but it does 
not have to know that location! 
Similarly, the effects of the INFORM-BY- 
OTHER act is also a quanti f ied belief, as 
in 
x OSCAR BELIEVE 
USER BELIEVE 
(the y .~ LOC(KEY,y) = x) 
Thus, OSCAR plans this INFORM-BY-OTHER act 
of the key's locat ion in order to know 
where the user thinks the key is. 
Such information has been lacking 
from all other formulat ions of ASK (or 
INFORM) that we have seen in the 
l i terature (e.g., Schank (1975), Mann et 
al. (1976), Searle (1969)). Cohen (1978) 
presents one approach to def ining this new 
view of INFORM, and its associated 
mediat ing act CONVINCE. 
4. Recogniz in @ Speech Acts 
In the previous section we discussed 
the structure of p lans  that include 
instances of the operators REQUEST and 
INFORM without explaining the relat ion 
between these speech acts and sentences 
used to perform them. This sect ion 
sketches our first steps in explor ing this 
relation. We have been part icu lar ly  
concerned with the problem of recogniz ing 
i l l ocut ionary  force and proposi t ional  
content of the utterances of a speaker. 
Detai led algor i thms which handle the 
examples given in this section have been 
designed by J. Al len and are being 
implemented by him. Further detai ls  can 
be found in (Allen and Perrault  1978) and 
Al len's  forthcoming Ph.D. d issertat ion.  
Certain syntactic clues in an 
utterance such as its mood and the use of 
expl ic i t  per format ives indicate what act 
the speaker intends to perform, but' as is 
well known, utterances which taken 
l i tera l ly  would indicate one i l locut ionary 
force can be used to indicate another. 
Thus "Can you close the door?" can be a 
request as well as a question. These so- 
called indirect speech acts are the acid 
test of a theory of speech acts. We claim 
that a p lan-based theory gives some 
insight into this phenomenon. 
Searle(1975) correct ly suggests that 
"In cases where these sentences <indirect 
forms of requests> are uttered as 
requests, they sti l l  have their l iteral 
meaning and are uttered with and as having 
that l i teral meaning".  How then can they 
also have their indirect meaning? 
Our answer rel ies in part on the fact 
that an agent part ic ipat ing in a 
cooperat ive d ia logue must have processes 
to: 
(I) Achieve goals based on what he 
believes. 
(2) Adopt goals of other agents as his 
own. 
(3) Infer goals of other agents. 
(4) Predict  future behaviour of other 
agents. 
These processes would be necessary even if 
all speech acts were l i te ra l  to account 
for exchanges where the response indicates 
a knowledge of the speaker's plan. For 
example 
Passenger: "When does the next train to 
Montreal  leave?" 
Clerk : "At 6:15 at Gate 7" 
or 
Clerk - "There won't  be one until 
tomorrow." 
Speakers expect hearers to be 
execut ing these processes and they expect 
hearers to know this. Inferences that a 
hearer can draw by executing these 
processes based on information he thinks 
the speaker bel ieves can be taken by the 
hearer to be intended by the speaker. 
This accounts for many of the standard 
examples of indirect speech acts such as 
"Can you close the door?" and "It's cold 
here". For instance, even if "It 's cold 
here" is intended l i teral ly  and is 
recognized as such, the helpful hearer may 
sti l l  c lose the window. When the sentence 
is uttered as a request, the speaker 
intends the hearer to recognize the 
speaker 's  intention that the hearer should 
perform the helpful behaviour. 
If indirect speech acts are to be 
explained in terms of inferences speakers 
can expect of hearers, then a theory of 
speech acts must concern itself with how 
such inferences are control led. Some 
heur ist ics  are part icu lar ly  helpful. If a 
chain of inference by the hearer has the 
speaker planning an action whose effects 
129 
are true before the action is executed, 
then the chain is l ikely to be wrong, or 
else must be cont inued further. This 
accounts for "Can you pass the salt?" as a 
request for the salt, not a quest ion about 
salt -passing prowess. As Searle(1975) 
points out, a crucial part of 
understanding indirect speech acts is 
being able to recognize that they are not 
to be interpreted l iteral ly. 
A second heur ist ic  is that a chain of 
inference that leads to an action whose 
precondi t ions are known to be not easi ly 
achievable is l ikely to be wrong. 
Inferencing can also be control led 
through the use of expectat ions about the 
speaker 's  goals. Pr ior i ty can be given to 
inferences which relate an observed speech 
act to an expected goal. Expectat ions 
enable inferencing to work top-down as 
well as bottom-up. 
The use of expected goals to guide the 
inferencing has another advantage: it 
a l lows for the recognit ion of 
i l locut ionary force in el l ipt ical  
utterances such as "The 3:15 train to 
Windsor?",  wi thout  requir ing that the 
syntact ic and semantic analysis 
"reconst i tute" a complete semantic 
representat ion such as "Where does the 
3:15 train to Windsor leave?". For 
example, let the clerk assume that 
passengers want to either meet incoming 
trains or board depart ing ones. Then the 
utterance "The 3:15 train to Windsor?" is 
f irst interpreted as a REQUEST about a 
train to Windsor with 3:15 as either 
arr ival or departure time. Only depart ing 
trains have dest inat ions d i f ferent  from 
Toronto and this leads to bel ieving that 
the passenger wants to board a 3:15 train 
to Windsor. Attempting to identify 
obstacles in the passenger 's  plan leads to 
f inding that the passenger knows the time 
but probably not the place of departure.  
Final ly, overcoming the obstacle then 
leads to an INFORM like "Gate 8". 
Our analysis of el l ipt ical  utterances 
raises two questions. First, what 
information does the i l locut ionary force 
recognit ion module expect from the syntax 
and semantics? Our approach here has been 
to require from the syntax and semantics a 
hypothesis  about the l iteral i l locut ionary 
force and a predicate ca lcu lus- l ike 
representat ion of the proposi t ional  
content, but where undetermined predicates 
and objects could be replaced by patterns 
on which certain restr ict ions can be 
imposed. As part of the plan inferencing 
process these patterns become further 
specif ied. 
The second quest ion is: what should 
the hearer do if more than one path 
between the observed utterance and the 
expectat ions is possible? He may suspend 
plan deduct ion and start planning to 
achieve a goal which would al low plan 
deduct ion to continue. Consider the 
fol lowing example. 
Passenger : When is the Windsor train? 
Clerk : The train to Windsor? 
Passenger : Yes. 
Clerk : 3:15. 
After the first sentence the clerk 
cannot d ist inguish between the 
expectat ions "Passenger travel by train to 
Windsor" and "Passenger meets train from 
Windsor",  so he sets up a goal : (clerk 
bel ieves passenger wants to travel) or 
(clerk bel ieves passenger wants to meet 
train). The planning for this goal 
produces a plan that involves asking the 
passenger if he wants one of the 
al ternat ives,  and receiving back the 
answer. The execut ion of this plan 
produces the clerk response "The train to 
Windsor?" and recognizes the response 
"Yes". Once the passenger 's  goal is 
known, the clerk can cont inue the or iginal  
deduct ion process with the "travel to 
Windsor" a l ternat ive favoured. This plan 
is accepted and the clerk produces the 
response "3:15" to overcome the obstac le 
"passenger knows departure time". 
5. Reference and the Model of the Other 
We have shown that quant i f ied bel iefs 
are needed in deciding to ask someone a 
question. They are also involved, we 
claim, in the representat ion of s ingular 
def in i te  noun phrases and hence any 
natural  language system wil l  need them. 
According to our analysis,  a hearer should 
represent the referr ing phrase in a 
speaker 's  statement "The pi lot  of TWA 510 
is drunk" by: 
x SPEAKER BELIEVE 
(the y : P ILOT(y,TWA510) = x & 
DRUNK (x)) 
This is the reading whereby the speaker is 
bel ieved to "know who the pi lot  of TW~ 510 
is" (at least part ia l ly  accounting for 
Donnel lan 's  (1966) referent ial  reading). 
This is to be contrasted with the reading 
of whoever is pi lot ing that plane is drunk 
(Donnel lan's attr ibut ive noun phrases).  
In this latter case, the existent ia l  
quant i f ier  would be inside the scope of 
the belief. 
These existent ia l  presuppos i t ions  of 
def in i te referential  noun phrases give one 
important way for hearers to acquire 
quanti f ied speaker-bel iefs .  Such bel iefs,  
we have seen, can be used as the basis for 
planning further c lar i f icat ion questions. 
We agree with Strawson (1950) (and 
many others) that hearers understand 
referr ing phrases based on what they 
bel ieve speakers intend to refer to. 
130 
Undoubtedly,  a hearer wil l  understand a 
speaker's  (reference) intentions by using 
a model of that speaker's beliefs. 
Speakers, of course, know of these 
interpretat ion strategies and thus plan 
their referr ing phrases to take the 
appropr iate referent within the hearer 's  
model of them. A speaker cannot use 
pr ivate descr ipt ions,  nor descr ipt ions 
that he thinks the hearer thinks are 
private, for communicat ion.  
For instance, consider the fol lowing 
variant of an example of Donnel lan's  
(1966): At a party, a woman is holding a 
mart ini  glass which Jones bel ieves 
contains water, but of which he is certain 
everyone else bel ieves (and bel ieves he 
believes) contains a martini .  Jones would 
understand that Smith, via quest ion (I), 
but not via quest ion (2) is referr ing to 
this woman. 
(i) Who is the woman holding the mart ini?  
(2) Who is the woman holding the water? 
since Jones does not bel ieve Smith knows 
about the water in her glass. 
Conversely,  if Jones wanted to refer 
to the woman in an utterance intended for 
Smith, he could do so using (i) but not 
(2) since in the latter case he would not 
think the hearer could pick out his 
intended referent. 
Thus it appears that for a speaker to 
plan a successful  singular def in i te 
referential  express ion requires that the 
speaker bel ieve the express ion he f inal ly 
chooses have the right referent in the 
hearer 's  model of the speaker. Our 
concept of mutual bel ief  can be used (as 
in Cohen (1978)) to ensure that the 
expression denotes appropr iate ly  in all 
further embedded bel ief models. This 
example is problematic  for any approach to 
reference where a communicat ing party 
assumes that its real i ty is the only 
reality. Speakers and hearers can be 
"wrong" or "ignorant" and yet 
communicat ion can stil l  be meaningful  and 
successful.  
6. Further Research 
We bel ieve that speech acts provide an 
excel lent  way of explaining the relat ions 
between utterances in a dialogue, as well 
as relating l inguist ic  to non- l inguist ic  
activity. Unti l  we better understand the 
mechanisms by which conversants change the 
topic and goals of the conversat ion it 
will be d i f f icu l t  to extend this analysis 
beyond exchanges of a few utterances, in 
part icular to non-task or iented dialogues. 
Fuller just i f icat ion of our approach also 
requires its appl icat ion to a much broader 
range of speech acts. Here the problem is 
main ly  representat ional :  how can we 
handle promises without f irst deal ing with 
obl igat ions,  or warnings without the 
not ions of danger and undesirabi l i ty? We 
are current ly  consider ing an extension of 
the approach to understanding stories 
which report s imple dialogue. 
Much remains to be done on the 
representat ion of the abi l i t ies of angther 
agent. A simple sett ing suggests a number 
of problems. Let one agent H be seated in 
a room in front of a table with a 
col lect ion of blocks. Let another agent 
S be outside the room but communicat ing by 
telephone. If S bel ieves that there is a 
green block on the table and wants it 
cleared, but knows nothing about any other 
blocks except that H can see them, then 
how can S ask H to clear the green block? 
The blocks S wants removed are those which 
are in fact there, perhaps those which he 
could perceive to be there if he were in 
the room. The goal seems to be of the 
form 
S BELIEVE 
x (x on the green block => S WANT 
(x removed from green block)) 
but our planning machinery and def in i t ion 
of REQUEST are inadequate for generat ing 
"I request you to clear the green block". 
We have not yet spent much time 
invest igat ing the process of giving 
answers to How and Why questions, or to WH 
quest ions requir ing an event descr ipt ion 
as an answer. We conjecture that because 
of the speech act approach answers to 
"What did he say?" should be found in much 
the same way as answers to "What did he 
do?" and that this para l le l ism should 
extend to other quest ion types. The 
natural extension of our analysis  would 
suggest represent ing "How did AGT achieve 
goal G?" as a REQUEST by the speaker that 
the hearer inform him of a plan by which 
AGT achieved G. We have not yet 
invest igated the repercuss ions of this 
extension on the representat ion language. 
F inal ly consider the fol lowing 
dialogue. Assume that S is a shady 
businessman, A his secretary. 
A : IRS is on the phone. 
S : I 'm not here. 
How is A to understand S's utterance? 
Al though its proposi t ional  content is 
l i tera l ly  false, maybe even nonsensical ,  
the utterance's  intention is unmistakable.  
How tolerant does the understanding system 
have to be to infer its way to a correct  
interpretat ion? Must "I'm not here" be 
treated id iomatical ly? 
131 
B ibl log r aphy 
Allen, J.F. and Perrault, C.R., 
"Participating in Dialogue: 
Understanding via Plan Deduction", 2nd 
National Conference of the Canadian 
Society for Studies in Computational 
Intelligence, Toronto, July, 1978. 
Cohen, P.R., "On Knowing What to Say: 
Planning Speech Acts", TRII8 Dept. of 
Computer Science, University of 
Toronto, 1978. 
Donnellan, K., "Reference and Definite 
Description", The Philosophical 
Review, vol. 75, 1960, pp280-304. 
Reprinted in Semantics, Steinberg and 
Jacobovits, eds., Cambridge University 
Press, 1970. 
Fikes, R. E. and Nilsson, N. J., 1970, 
"STRIPS: A new approach to the 
application of theorem proving", 
Artificial Intelligence 2, 1970. 
Grosz, B. J., "The Representation and Use 
of Focus in Natural Language 
Dialogues", 5IJCAI, 1977. 
Hintikka, K.J., Knowled~\[e and Belief, 
Cornell University Press, 1962. 
Mann, W.C., Moore, J.A., Levin, J.A.; "A 
Comprehension Model for Human 
Dialogue", 5IJCAI, 1977. 
Moore, R.C.; "Reasoning about Knowledge 
and Action", 5IJCAI, 1977. 
Quine, w.v., "Quantifiers and 
Propositional Attitudes", The Journal 
of Philosophy 53, (1956), 177-187. 
Schiffer, S., Meaning, Oxford University 
Press, 1972. 
Schank, R. and Abelson, R., "Scripts, 
Plans and Knowledge", 4IJCAI, 1975. 
Searle, J. R., Speech Acts, Cambridge 
University Press, 1969. 
Searle, J. R.; "Indirect Speech Acts" in 
Syntax and Semantics, Vol. 3: Speech 
Acts, Cole and Morgan (eds), Academic 
Press, 1975. 
Searle, J. R., "A Taxonomy of 
Illocutionary Acts", Language, Mind 
and Knowledge, K. Gunderson (ed.), 
University of Minnesota Press, 1976. 
Strawson, P. F., "On Referring", Mind, 
1950. 
132 
