Word Sense Disambiguation by Learning from Unlabeled Data
Seong-Bae Park
y
, Byoung-Tak Zhang
y
and Yung Taek Kim
z
Articial Intelligence Lab (SCAI)
School of Computer Science and Engineering
Seoul National University
Seoul 151-742, Korea
y
fsbpark,btzhangg@scai.snu.ac.kr
z
ytkim@cse.snu.ac.kr
Abstract
Most corpus-based approaches to
natural language processing suer
from lack of training data. This
is because acquiring a large num-
ber of labeled data is expensive.
This paper describes a learning
method that exploits unlabeled data
to tackle data sparseness problem.
The method uses committee learn-
ing to predict the labels of unla-
beled data that augment the exist-
ing training data. Our experiments
on word sense disambiguation show
that predictive accuracy is signi-
cantly improved by using additional
unlabeled data.
1 Introduction
The objective of word sense disambiguation
(WSD) is to identify the correct sense of a
word in context. It is one of the most critical
tasks in most natural language applications,
including information retrieval, information
extraction, and machine translation. The
availability of large-scale corpus and various
machine learning algorithms enabled corpus-
based approach to WSD (Cho and Kim, 1995;
Hwee and Lee, 1996; Wilks and Stevenson,
1998),but a large scale sense-tagged corpus
or aligned bilingual corpus is needed for a
corpus-based approach.
However, most languages except English
do not have a large-scale sense-tagged cor-
pus. Therefore, any corpus-based approach
to WSD for such languages should consider
the following problems:
 There's no reliable and available sense-
tagged corpus.
 Most words are sense ambiguous.
 Annotating the large corpora requires
human experts, so that it is too expen-
sive.
Because it is expensive to construct sense-
tagged corpus or bilingual corpus, many re-
searchers tried to reduce the number of ex-
amples needed to learn WSD (Atsushi et al,
1998; Pedersen and Bruce, 1997). Atsushi et
al. (Atsushi et al, 1998) adopted a selec-
tive sampling method to use small number of
examples in training. They dened a train-
ing utility function to select examples with
minimum certainty, and at each training it-
eration the examples with less certainty were
saved in the example database. However, at
each iteration of training the similarity among
word property vectors must be calculated due
to their k-NN like implementation of training
utility.
While labeled examples obtained from a
sense-tagged corpus is expensive and time-
consuming, it is signicantly easier to ob-
tain the unlabeled examples. Yarowsky
(Yarowsky, 1995) presented, for the rst time,
the possibility that unlabeled examples can
be used for WSD. He used a learning algo-
rithm based on the local context under the
assumption that all instances of a word have
the same intended meaning within any xed
document and achieved good results with only
a few labeled examples and many unlabeled
ones. Nigam et al (Nigam et al, 2000) also
showed the unlabeled examples can enhance
the accuracy of text categorization.
Attribute Substance
GFUNC the grammatical function of w
PARENT the word of the node modied by w
SUBJECT whether or not PARENT of w has a subject
OBJECT whether or not PARENT of w has an object
NMODWORD the word of the noun modier of w
ADNWORD the head word of the adnominal phrase of w
ADNSUBJ whether or not the adnominal phrase of w has a subject
ADNOBJ whether or not the adnominal phrase of w has an object
Table 1: The properties used to distinguish the sense of an ambiguous Korean noun w.
In this paper, we present a new approach
to word sense disambiguation that is based
on selective sampling algorithm with commit-
tees. In this approach, the number of train-
ing examples is reduced, by determining by
weighted majority voting of multiple classi-
ers, whether a given training example should
be learned or not. The classiers of the com-
mittee are rst trained on a small set of la-
beled examples and the training set is aug-
mented by a large number of unlabeled exam-
ples. One might think that this has the pos-
sibility that the committee is misled by unla-
beled examples. But, the experimental results
conrm that the accuracy of WSD is increased
by using unlabeled examples when the mem-
bers of the committee are well trained with
labeled examples. We also theoretically show
that performance improvement is guaranteed
by a mild requirement, i.e., the base classi-
ers need to guess better than random selec-
tion. This is because the possibility misled by
unlabeled examples is reduced by integrating
outputs of multiple classiers. One advantage
of this method is that it eectively performs
WSD with only a small number of labeled ex-
amples and thus shows possibility of building
word sense disambiguators for the languages
which have no sense-tagged corpus.
The rest of this paper is organized as fol-
lows. Section 2 introduces the general proce-
dure for word sense disambiguation and the
necessity of unlabeled examples. Section 3 ex-
plains how the proposed method works using
both labeled and unlabeled examples. Section
4 presents the experimental results obtained
by using the KAIST raw corpus. Section 5
draws conclusions.
2 Word Sense Disambiguation
Let S 2 fs
1
; : : : ; s
k
g be the set of possible
senses of a word to be disambiguated. To
determine the sense of the word, we need
to consider the contextual properties. Let
x =< x
1
; : : : ; x
n
> be the vector for rep-
resenting selected contextual features. If we
have a classier f(x; ) parameterized with ,
then the sense of a word with property vec-
tor x can be determined by choosing the most
probable sense s

:
s

= argmax
s2S
f(x; ):
The parameters  are determined by training
the classier on a set of labeled examples, L =
f(x
1
; s
1
); : : : ; (x
N
; s
N
)g.
2.1 Property Sets
In general, the rst step of WSD is to extract
a set of contextual features. To select particu-
lar properties for Korean, the language of our
cencern, the following characteristics should
be considered:
 Korean is a partially free-order language.
The ordering information on the neigh-
bors of the ambiguous word, therefore,
does not give signicantly meaningful in-
formation in Korean.
 In Korean, ellipses appear very often
with a nominative case or objective case.
Therefore, it is di?cult to build a large
scale database of labeled examples with
case markers.
Considering both characteristics and re-
sults of previous work, we select eight prop-
erties for WSD of Korean nouns (Table 1).
Three of them (PARENT, NMODWORD,
ADNWORD) take morphological form as
their value, one (GFUNC) takes 11 values of
grammatical functions
1
, and others take only
true or false.
2.2 Unlabeled Data for WSD
Many researchers tried to develop automated
methods to reduce training cost in language
learning and found out that the cost can be
reduced by active learning which has control
over the training examples (Dagan and Engel-
son, 1997; Liere and Tadepalli, 1997; Zhang,
1994). Though the number of labeled exam-
ples needed is reduced by active learning, the
label of the selected examples must be given
by the human experts. Thus, active learn-
ing is still expensive and a method for auto-
matic labeling unlabeled examples is needed
to have the learner automatically gather in-
formation (Blum and Mitchell, 1998; Peder-
sen and Bruce, 1997; Yarowsky, 1995).
As the unlabeled examples can be obtained
with ease without human experts it makes
WSD robust. Yarowsky (Yarowsky, 1995)
presented the possibility of automatic label-
ing of training examples in WSD and achieved
good results with only a few labeled exam-
ples and many unlabeled examples. On the
other hand, Blum and Mitchell tried to clas-
sify Web pages, in which the description of
each example can be partitioned into distinct
views such as the words occurring on that
page and the words occurring in hyperlinks
(Blum and Mitchell, 1998). By using both
views together, they augmented a small set
of labeled examples with a lot of unlabeled
examples.
The unlabeled examples in WSD can pro-
vide information about the joint probability
1
These 11 grammatical functions are from
the parser, KEMTS (Korean-to-English Machine
Translation System) developed in Seoul National Uni-
versity, Korea.
distribution over properties but they also can
mislead the learner. However, the possibility
of being misled by the unlabeled examples is
reduced by the committee of classiers since
combining or integrating the outputs of sev-
eral classiers in general leads to improved
performance. This is why we use active learn-
ing with committees to select informative un-
labeled examples and label them.
3 Active Learning with
Committees for WSD
3.1 Active Learning Using Unlabeled
Examples
The algorithm for active learning using unla-
beled data is given in Figure 1. It takes two
sets of examples as inputs. A Set L is the one
with labeled examples and D = fx
1
; : : : ;x
T
g
is the one with unlabeled examples where x
i
is a property vector. First of all, the training
set L
(1)
j
(1  j  M) of labeled examples is
constructed for each base classier C
j
. This
is done by random resampling as in Bagging
(Breiman, 1996). Then, each base classier
C
j
is trained with the set of labeled examples
L
(1)
j
.
After the classiers are trained on labeled
examples, the training set is augmented by
the unlabeled examples. For each unlabeled
example x
t
2 D, each classier computes the
sense y
j
2 S which is the label associated with
it, where S is the set of possible sense of x
t
.
The distribution W over the base classi-
ers represents the importance weights. As
the distribution can be changed each iter-
ation, the distribution in iteration t is de-
noted by W
t
. The importance weight of clas-
sier C
j
under distribution W
t
is denoted by
W
t
(j). Initially, the base classiers have equal
weights, so that W
t
(j) = 1=M .
The sense of the unlabeled example x
t
is de-
termined by majority voting among C
j
's with
weight distribution W . Formally, the sense y
t
of x
t
is predicted by
y
t
(x
t
) = argmax
y2S
X
j:C
j
(x
t
)=y
W
t
(j):
If most classiers believe that y
t
is the correct
Given an unlabeled example set D = fx
1
; : : : ;x
T
g
and a labeled example set L
and a word sense set S 2 fs
1
; : : : ; s
k
g for x
i
,
Initialize W
1
(j) =
1
M
,
where M is the number of classiers in the
committee.
Resample L
(1)
j
from L for each classier C
j
,
where jL
(1)
j
j = jLj as done in Bagging.
Train base classier C
j
(1  j  M) from L
(1)
j
.
For t = 1; : : : ; T :
1. Each C
j
predicts the sense y
j
2 S for x
t
2 D.
Y =< y
1
; : : : ; y
M
>
2. Find the most likely sense y
t
from Y using
distribution W :
y
t
= argmax
y2S
X
j:C
j
(x
t
)=y
W
t
(j):
3. Set 
t
=
1 
t

t
, where

t
=
No. of C
j
's whose predictions are not y
t
M
:
4. If 
t
is larger than a certainty threshold ,
then update W
t
:
W
t+1
(j) =
W
t
(j)
Z
t



t
if y
j
= y
t
1 otherwise,
where Z
t
is a normalization constant.
5. Otherwise, every classier C
j
is restructured
from new training set L
(t+1)
j
:
L
(t+1)
j
= L
(t)
j
+ f(x
t
; y
t
)g:
Output the nal classier:
f(x) = argmax
y2S
X
j:C
j
(x)=y
W
T
(j):
Figure 1: The active learning algorithm
with committees using unlabeled examples for
WSD.
sense of x
t
, they need not learn x
t
because
this example makes no contribution to reduce
the variance over the distribution of exam-
ples. In this case, instead of learning the ex-
ample, the weight of each classier is updated
in such a way that the classiers whose pre-
dictions were correct get a higher importance
weight and the classiers whose predictions
were wrong get a lower importance weight
under the assumption that the correct sense
of x
t
is y
t
. This is done by multiplying the
weight of the classier whose prediction is y
t
by certainty 
t
. To ensure the updated W
t+1
form a distribution, W
t+1
is normalized by
constant Z
t
. Formally, the importance weight
is updated as follows:
W
t+1
=
W
t
(j)
Z
t



t
if y
j
= y
t
;
1 otherwise.
The certainty 
t
is computed from error 
t
.
Because we trust that the correct sense of x
t
is y
t
, the error 
t
is the ratio of the number of
classiers whose predictions are not y
t
. That
is, 
t
is computed as

t
=
1  
t

t
where 
t
is given as

t
=
No. of C
j
's whose predictions are not y
t
M
:
Note that the smaller 
t
, the larger the value
of 
t
. This implies that, if the sense of x
t
is certainly y
t
and a classier predicts it, a
higher weight is assigned to the classier. We
assume that most classiers believe that y
t
is
the sense of x
t
if the value of y
t
is larger than
a certainty threshold  which is set by trial-
and-error.
However, if the certainty is below the
threshold, the classiers need to learn the ex-
ample x
t
yet with belief that the sense of it
is y
t
. Therefore, the set of training examples,
L
(t)
j
, for the classier C
j
is expanded by
L
(t+1)
j
= L
(t)
j
+ f(x
t
; y
t
)g:
Then, each classier C
j
is restructured with
L
(t+1)
j
.
This process is repeated until the unlabeled
examples are exhausted. The sense of a new
example x is then determined by weighted
majority voting among the trained classiers:
f(x) = argmax
y2S
X
j:C
j
(x)=y
W
T
(j);
where W
T
(j) is the importance weight of clas-
sier C
j
after the learning process.
3.2 Theoretical Analysis
Previous studies show that using multiple
classiers rather than a single classier leads
to improved generalization (Breiman, 1996;
Freund et al, 1992) and learning algorithms
which use weak classiers can be boosted
into strong algorithms (Freund and Schapire,
1996). In addition, Littlestone and Warmuth
(Littlestone and Warmuth, 1994) showed that
the error of the weighted majority algorithm
is linearly bounded on that of the best mem-
ber when the weight of each classier is de-
termined by held-out examples.
The performance of the proposed method
depends on that of initial base classiers.
This is because it is highly possible for unla-
beled examples to mislead the learning algo-
rithm if they are poorly trained in their initial
state. However, if the accuracy of the initial
majority voting is larger than
1
2
, the proposed
method performs well as the following theo-
rem shows.
Theorem 1 Assume that every unlabeled
data x
t
is added to the set of training ex-
amples for all classiers and the importance
weights are not updated. Suppose that p
0
be
the probability that the initial classiers do
not make errors and 
t
(0  
t
 1) be the
probability by which the accuracy is increased
in adding one more correct example or de-
creased in adding one more incorrect example
at iteration t. If p
0

1
2
, the accuracy does
not decrease as a new unlabeled data is added
to the training data set.
Proof. The probability for the classiers
to predict the correct sense at iteration t = 1,
p
1
, is
p
1
= p
0
(p
0
+ 
0
) + (1  p
0
)(p
0
  
0
)
= p
0
(2
0
+ 1)  
0
because the accuracy can be increased or de-
creased by 
0
with the probability p
0
and
1   p
0
, respectively. Therefore, without loss
of generality, at iteration t = i+ 1, we have
p
i+1
= p
i
(2
i
+ 1)  
i
:
To ensure the accuracy does not decrease, the
condition p
i+1
 p
i
should be satised.
p
i+1
  p
i
= p
i
(2
i
+ 1)  
i
  p
i
= p
i
(2
i
)  
i
 0
) p
i

1
2
The theorem follows immediately from this
result. 
3.3 Decision Trees as Base Classiers
Although any kind of learning algorithms
which meet the conditions for Theorem 1 can
be used as base classiers, Quinlan's C4.5 re-
lease 8 (Quinlan, 1993) is used in this paper.
The main reason why decision trees are used
as base classiers is that there is a fast restruc-
turing algorithm for decision trees. Adding an
unlabeled example with a predicted label to
the existing set of training examples makes
the classiers restructured. Because the re-
structuring of classiers is time-consuming,
the proposed method is of little practical use
without an e?cient way to restructure. Ut-
go et al (Utgo et al, 1997) presented two
kinds of e?cient algorithms for restructuring
decision trees and showed experimentally that
their methods perform well with only small
restructuring cost.
We modied C4.5 so that word match-
ing is accomplished not by comparing mor-
phological forms but by calculating similar-
ity between words to tackle data-sparseness
problem. The similarity between two Ko-
rean words is measured by averaged distance
in WordNet of their English-translated words
(Kim and Kim, 1996).
Word No. of Senses No. of Examples Sense Percentage
pear 6.2%
ship 55.2%
bae 4 876
times 13.7%
stomach 24.9%
person 46.2%
bun 3 796 minute 50.8%
indignation 3.0%
the former 28.6%
jonja 2 350
electron 71.4%
bridge 30.9%
dari 2 498
leg 69.1%
Table 2: Various senses of Korean nouns used for the experiments and their distributions in
the corpus.
4 Experiments
4.1 Data Set
We used the KAIST Korean raw corpus
2
for
the experiments. The entire corpus consists
of 10 million words but we used in this pa-
per the corpus containing one million words
excluding the duplicated news articles. Ta-
ble 2 shows various senses of ambiguous Ko-
rean nouns considered and their sense distri-
butions. The percentage column in the table
denotes the ratio that the word is used with
the sense in the corpus. Therefore, we can
regard the maximum percentage as a lower
bound on the correct sense for each word.
4.2 Experimental Results
For the experiments, 15 base classiers are
used. If there is a tie in predicting senses,
the sense with the lowest order is chosen as
in (Breiman, 1996). For each noun, 90% of
the examples are used for training and the
remaining 10% are used for testing.
Table 3 shows the 10-fold cross validation
result of WSD experiments for nouns listed
in Table 2. The accuracy of the proposed
method shown in Table 3 is measured when
the accuracy is in its best for various ratios of
the number of labeled examples for base clas-
siers to total examples. The results show
2
This corpus is distributed by the Korea Termi-
nology Research Center for Language and Knowledge
Engineering.
that WSD by selective sampling with com-
mittees using both labeled and unlabeled ex-
amples is comparable to a single learner us-
ing all the labeled examples. In addition, the
method proposed in this paper achieves 26.3%
improvement over the lower bound for `bae',
41.5% for `bun', 22.1% for `jonja', and 4.2%
for `dari', which is 23.6% improvement on the
average. Especially, for `jonja' the proposed
method shows higher accuracy than the single
C4.5 trained on the whole labeled examples.
Figure 2 shows the performance improved
by using unlabeled examples. This gure
demonstrates that the proposed method out-
performs the one without using unlabeled ex-
amples. The initial learning in the gure
means that the committee is trained on la-
beled examples, but is not augmented by un-
labeled examples. The dierence between two
lines is the improved accuracy obtained by
using unlabeled examples. When the accu-
racy of the proposed method gets stabilized
for the rst time, the improved accuracy by
using unlabeled examples is 20.2% for `bae',
9.9% for `bun, 13.5% `jonja', and 13.4% for
`dari'. It should be mentioned that the results
also show that the accuracy of the proposed
method may be dropped when the classiers
are trained on too small a set of labeled data,
as is the case in the early stages of Figure 2.
However, in typical situations where the clas-
siers are trained on minimum training set
Using Partially Using All
Word
Labeled Data Labeled Data
Lower Bound
bae 81.5  7.7% 82.3%  5.9% 55.2%
bun 92.3  7.7% 94.3%  5.7% 50.8%
jonja 93.5  6.5% 90.6%  9.4% 71.4%
dari 73.3  14.2% 80.8  10.9% 69.1%
Average 85.2% 87.0% 61.6%
Table 3: The accuracy of WSD for Korean nouns by the proposed method.
size, this does not happen as the results of
other nouns show. In addition, we can nd in
this particular experiment that the accuracy
is always improved by using unlabeled exam-
ples if only about 22% of training examples,
on the average, are labeled in advance.
In Figure 2(a), it is interesting to observe
jumps in the accuracy curve. The jump ap-
pears because the unlabeled examples mislead
the classiers only when the classiers are
poorly trained, but they play an important
role as information to select senses when the
classiers are well trained on labeled exam-
ples. Other nouns show similar phenomena
though the percentage of labeled examples is
dierent when the accuracy gets at.
5 Conclusions
In this paper, we proposed a new method
for word sense disambiguation that is based
on unlabeled data. Using unlabeled data is
especially important in corpus-based natural
language processing because raw corpora are
ubiquitous while labeled data are expensive
to obtain. In a series of experiments on word
sense disambiguation of Korean nouns we ob-
served that the accuracy is improved up to
20.2% using only 32% of labeled data. This
implies, the learning model trained on a small
number of labeled data can be enhanced by
using additional unlabeled data. We also the-
oretically showed that the predictive accuracy
is always improved if the individual classiers
do better than random selection after being
trained on labeled data.
As the labels of unlabeled data are es-
timated by committees of multiple decision
trees, the burden of manual labeling is min-
imized by using unlabeled data. Thus, the
proposed method seems especially eective
and useful for the languages for which a large-
scale sense-tagged corpus is not available yet.
Another advantage of the proposed method
is that it can be applied to other kinds of
language learning problems such as POS-
tagging, PP attachment, and text classica-
tion. These problems are similar to word
sense disambiguation in the sense that unla-
beled raw data are abundant but labeled data
are limited and expensive to obtain.
Acknowledgements
This research was supported in part by the
Korean Ministry of Education under the
BK21 Program and by the Korean Ministry
of Information and Communication through
IITA under grant 98-199.
References
F. Atsushi, I. Kentaro, T. Takenobu, and
T. Hozumi. 1998. Selective sampling of ef-
fective example sentence sets for word sense
disambiguation. Computational Linguistics,
24(4):573{597.
A. Blum and T. Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In
Proceedings of COLT-98, pages 92{100.
L. Breiman. 1996. Bagging predictors. Machine
Learning, 24:123{140.
J.-M. Cho and G.-C. Kim. 1995. Korean verb
sense disambiguation using distributional infor-
mation from corpora. In Proceedings of Natural
Language Processing Pacic Rim Symposium,
pages 691{696.
I. Dagan and S. Engelson. 1997. Committee-
based sampling for training probabilistic classi-
50
55
60
65
70
75
80
85
10 20 30 40 50 60 70 80 90 100
Ac
cu
ra
cy
 (%
)
Ratio of The Number of Labeled Examples to The Number of Total Examples (%)
Initial Learning
The Proposed Method
(a) bae
40
50
60
70
80
90
10 20 30 40 50 60 70 80 90 100
Ac
cu
ra
cy
 (%
)
Ratio of The Number of Labeled Examples to The Number of Total Examples (%)
Initial Learning
The Proposed Method
(b) bun
20
30
40
50
60
70
80
90
10 20 30 40 50 60 70 80 90 100
Ac
cu
ra
cy
 (%
)
Ratio of The Number of Labeled Examples to The Number of Total Examples (%)
Initial Learning
The Proposed Method
(c) jonja
30
40
50
60
70
80
10 20 30 40 50 60 70 80 90 100
Ac
cu
ra
cy
 (%
)
Ratio of The Number of Labeled Examples to The Number of Total Examples (%)
Initial Learning
The Proposed Method
(d) dari
Figure 2: Improvement in accuracy by using unlabeled examples.
ers. In Proceedings of the Fourteenth Interna-
tional Conference on Machine Learning, pages
150{157.
Y. Freund and R. Schapire. 1996. Experiments
with a new boosting algorithm. In Proceedings
of the Thirteenth International Conference on
Machine Learning, pages 148{156.
Y. Freund, H. Seung, E. Shamir, and N. Tishby.
1992. Selective sampling with query by com-
mittee algorithm. In Proceedings of NIPS-92,
pages 483{490.
T. Hwee and H. Lee. 1996. Integrating multiple
knowledge sources to disambiguate word sense:
An exemplar-based approach. In Proceedings
of the 34th Annual Meeting of the ACL, pages
40{47.
Nari Kim and Y.-T. Kim. 1996. Ambiguity reso-
lution of korean sentence analysis and korean-
english transfer based on korean verb patterns.
Journal of KISS, 23(7):766{775. in Korean.
R. Liere and P. Tadepalli. 1997. Active learn-
ing with committees for text categorization. In
Proceedings of AAAI-97, pages 591{596.
N. Littlestone and M. Warmuth. 1994. The
weighted majority algorithm. Information and
Computation, 108(2):212{261.
K. Nigam, A. McCallum, S. Thrun, and
T. Mitchell. 2000. Learning to classify text
from labeled and unlabeled documents. Ma-
chine Learning, 39:1{32.
T. Pedersen and R. Bruce. 1997. Distinguishing
word senses in untagged text. In Proceedings of
the Second Conference on Empirical Methods in
Natural Language Processing, pages 399{401.
R. Quinlan. 1993. C4.5: Programs For Machine
Learning. Morgan Kaufmann Publishers.
P. Utgo, N. Berkman, and J. Clouse. 1997. De-
cision tree induction based on e?cient tree re-
structuring. Machine Learning, 29:5{44.
Y. Wilks andM. Stevenson. 1998. Word sense dis-
ambiguation using optimised combinations of
knowledge sources. In Proceedings of COLING-
ACL '98, pages 1398{1402.
D. Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In
Proceedings of the 33rd Annual Meeting of the
ACL, pages 189{196.
B.-T. Zhang. 1994. Accerlated learning by ac-
tive example selection. International Journal
of Neural Systems, 5(1):67{75.
