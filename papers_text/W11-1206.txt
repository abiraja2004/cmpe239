Bilingual Lexicon Extraction from Comparable Corpora as Metasearch
Amir Hazem and Emmanuel Morin
Universite? de Nantes,
LINA - UMR CNRS 6241
2 rue de la Houssinie`re,
BP 92208 44322 Nantes Cedex 03
amir.hazem@univ-nantes.fr
emmanuel.morin@univ-nantes.fr
Sebastian Pen?a Saldarriaga
1100 rue Notre-Dame Ouest,
Montre?al, Que?bec,
Canada H3C 1K3
spena@synchromedia.ca
Abstract
In this article we present a novel way of look-
ing at the problem of automatic acquisition
of pairs of translationally equivalent words
from comparable corpora. We first present
the standard and extended approaches tradi-
tionally dedicated to this task. We then re-
interpret the extended method, and motivate a
novel model to reformulate this approach in-
spired by the metasearch engines in informa-
tion retrieval. The empirical results show that
performances of our model are always better
than the baseline obtained with the extended
approach and also competitive with the stan-
dard approach.
1 Introduction
Bilingual lexicon extraction from comparable cor-
pora has received considerable attention since the
1990s (Rapp, 1995; Fung, 1998; Fung and Lo,
1998; Peters and Picchi, 1998; Rapp, 1999; Chiao
and Zweigenbaum, 2002a; De?jean et al, 2002;
Gaussier et al, 2004; Morin et al, 2007; Laroche
and Langlais, 2010, among others). This attention
has been motivated by the scarcity of parallel cor-
pora, especially for countries with only one official
language and for language pairs not involving En-
glish. Furthermore, as a parallel corpus is com-
prised of a pair of texts (a source text and a translated
text), the vocabulary appearing in the translated text
is highly influenced by the source text, especially in
technical domains. Consequently, comparable cor-
pora are considered by human translators to be more
trustworthy than parallel corpora (Bowker and Pear-
son, 2002). Comparable corpora are clearly of use
in the enrichment of bilingual dictionaries and the-
sauri (Chiao and Zweigenbaum, 2002b; De?jean et
al., 2002), and in the improvement of cross-language
information retrieval (Peters and Picchi, 1998).
According to (Fung, 1998), bilingual lexicon
extraction from comparable corpora can be ap-
proached as a problem of information retrieval (IR).
In this representation, the query would be the word
to be translated, and the documents to be found
would be the candidate translations of this word. In
the same way that as documents found, the candi-
date translations are ranked according to their rele-
vance (i.e. a document that best matches the query).
More precisely, in the standard approach dedicated
to bilingual lexicon extraction from comparable cor-
pora, a word to be translated is represented by a
vector context composed of the words that appear
in its lexical context. The candidate translations
for a word are obtained by comparing the translated
source context vector with the target context vectors
through a general bilingual dictionary. Using this
approach, good results on single word terms (SWTs)
can be obtained from large corpora of several million
words, with an accuracy of about 80% for the top 10-
20 proposed candidates (Fung and McKeown, 1997;
Rapp, 1999). Cao and Li (2002) have achieved 91%
accuracy for the top three candidates using the Web
as a comparable corpus. Results drop to 60% for
SWTs using specialized small size language cor-
pora (Chiao and Zweigenbaum, 2002a; De?jean and
Gaussier, 2002; Morin et al, 2007).
In order to avoid the insufficient coverage of the
bilingual dictionary required for the translation of
source context vectors, an extended approach has
35
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 35?43,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
been proposed (De?jean et al, 2002; Daille and
Morin, 2005). This approach can be seen as a query
reformulation process in IR for which similar words
are substituted for the word to be translated. These
similar words share the same lexical environments
as the word to be translated without appearing with
it. With the extended approach, (De?jean et al, 2002)
obtained for single French-English words 43% and
51% precision out of the ten and twenty first candi-
dates applied to a medical corpus of 100 000 words
(respectively 44% and 57% with the standard ap-
proach) and 79% and 84% precision on the ten and
twenty first candidates applied to a social science
corpus of 8 million words (respectively 35% and
42% with the standard approach). Within this con-
text, we want to show how metasearch engines can
be used for bilingual lexicon extraction from spe-
cialized comparable corpora. In particular, we will
focus on the use of different strategies to take full
advantage of similar words.
The remainder of this paper is organized as fol-
lows. Section 2 presents the standard and extended
approaches based on lexical context vectors dedi-
cated to word alignment from comparable corpora.
Section 3 describes our metasearch approach that
can be viewed as the combination of different search
engines. Section 4 describes the different linguistic
resources used in our experiments and evaluates the
contribution of the metasearch approach on the qual-
ity of bilingual terminology extraction through dif-
ferent experiments. Finally, Section 5 presents our
conclusions.
2 Related Work
In this section, we first describe the standard ap-
proach dedicated to word alignment from compara-
ble corpora. We then present an extension of this
approach.
2.1 Standard Approach
The main work in bilingual lexicon extraction from
comparable corpora is based on lexical context anal-
ysis and relies on the simple observation that a word
and its translation tend to appear in the same lexi-
cal contexts. The basis of this observation consists
in the identification of first-order affinities for each
source and target language: First-order affinities de-
scribe what other words are likely to be found in
the immediate vicinity of a given word (Grefenstette,
1994a, p. 279). These affinities can be represented
by context vectors, and each vector element repre-
sents a word which occurs within the window of
the word to be translated (for instance a seven-word
window approximates syntactical dependencies).
The implementation of this approach can be car-
ried out by applying the following four steps (Rapp,
1995; Fung and McKeown, 1997):
Context characterization
All the lexical units in the context of each lexical
unit i are collected, and their frequency in a window
of n words around i extracted. For each lexical unit
i of the source and the target languages, we obtain a
context vector i where each entry, ij , of the vector is
given by a function of the co-occurrences of units j
and i. Usually, association measures such as the mu-
tual information (Fano, 1961) or the log-likelihood
(Dunning, 1993) are used to define vector entries.
Vector transfer
The lexical units of the context vector i are trans-
lated using a bilingual dictionary. Whenever the
bilingual dictionary provides several translations for
a lexical unit, all the entries are considered but
weighted according to their frequency in the target
language. Lexical units with no entry in the dictio-
nary are discarded.
Target language vector matching
A similarity measure, sim(i, t), is used to score
each lexical unit, t, in the target language with re-
spect to the translated context vector, i. Usual mea-
sures of vector similarity include the cosine similar-
ity (Salton and Lesk, 1968) or the weighted jaccard
index (WJ) (Grefenstette, 1994b) for instance.
Candidate translation
The candidate translations of a lexical unit are the
target lexical units ranked following the similarity
score.
2.2 Extended Approach
The main shortcoming of the standard approach is
that its performance greatly relies on the coverage of
the bilingual dictionary. When the context vectors
36
mot Identify k
similar vectors
Source language Target language
mot word
mot word
mot word
mot word
mot word
...
...
word
word
word
Match vectors
in target language ...
Bilingual dictionary
Figure 1: Illustration of the extended approach.
are well translated, the translation retrieval rate in
the target language improves.
Although, the coverage of the bilingual dictionary
can be extended by using specialized dictionaries
or multilingual thesauri (Chiao and Zweigenbaum,
2003; De?jean et al, 2002), translation of context
vectors remains the core of the approach.
In order to be less dependent on the coverage
of the bilingual dictionary, De?jean and Gaussier
(2002) have proposed an extension to the standard
approach. The basic intuition of this approach is
that words sharing the same meaning will share
the same environments. The approach is based
on the identification of second-order affinities in
the source language: Second-order affinities show
which words share the same environments. Words
sharing second-order affinities need never appear
together themselves, but their environments are sim-
ilar (Grefenstette, 1994a, p. 280).
Generally speaking, a bilingual dictionary is a
bridge between two languages established by its en-
tries. The extended approach is based on this ob-
servation and avoids explicit translation of vectors
as shown in Figure 1. The implementation of this
extended approach can be carried out in four steps
where the first and last steps are identical to the stan-
dard approach (De?jean and Gaussier, 2002; Daille
and Morin, 2005):
Reformulation in the target language
For a lexical unit i to be translated, we identify
the k-nearest lexical units (k nlu), among the dic-
tionary entries corresponding to words in the source
language, according to sim(i, s). Each nlu is trans-
lated via the bilingual dictionary, and the vector in
the target language, s, corresponding to the transla-
tion is selected. If the bilingual dictionary provides
several translations for a given unit, s is given by
the union of the vectors corresponding to the trans-
lations. It is worth noting that the context vectors are
not translated directly, thus reducing the influence of
the dictionary.
Vector matching against reformulations
The similarity measure, sim(s, t), is used to score
each lexical unit, t, in the target language with re-
spect to the k nlu. The final score assigned to each
unit, t, in the target language is given by:
sim(i, t) = ?
s?kNLU
sim(i, s)? sim(s, t) (1)
An alternate scoring function has been proposed
by Daille and Morin (2005). The authors computed
the centroid vector of the k nlu, then scored target
units with respect to the centroid.
3 The Metasearch Approach
3.1 Motivations
The approach proposed by De?jean and Gaussier
(2002) implicitly introduces the problem of select-
ing a good k. Generally, the best choice of k depends
on the data. Although several heuristic techniques,
like cross-validation, can be used to select a good
value of k, it is usually defined empirically.
The application of the extended approach (EA) to
our data showed that the method is unstable with
respect to k. In fact, for values of k over 20, the
precision drops significantly. Furthermore, we can-
not ensure result stability within particular ranges of
37
values. Therefore, the value of k should be carefully
tuned.
Starting from the intuition that each nearest lexi-
cal unit (nlu) contributes to the characterization of a
lexical unit to be translated, our proposition aims at
providing an algorithm that gives a better precision
while ensuring higher stability with respect to the
number of nlu. Pushing the analogy of IR style ap-
proaches (Fung and Lo, 1998) a step further, we pro-
pose a novel way of looking at the problem of word
translation from comparable corpora that is concep-
tually simple: a metasearch problem.
In information retrieval, metasearch is the prob-
lem of combining different ranked lists, returned
by multiple search engines in response to a given
query, in such a way as to optimize the performance
of the combined ranking (Aslam and Montague,
2001). Since the k nlu result in k distinct rankings,
metasearch provides an appropriate framework for
exploiting information conveyed by the rankings.
In our model, we consider each list of a given nlu
as a response of a search engine independently from
the others. After collecting all the lists of the se-
lected nlu?s, we combine them to obtain the final
similarity score. It is worth noting that all the lists
are normalized to maximize in such a way the con-
tribution of each nlu. A good candidate is the one
that obtains the highest similarity score which is cal-
culated with respect to the selected k. If a given can-
didate has a high frequency in the corpus, it may be
similar not only to the selected nearest lexical units
(k), but also to other lexical units of the dictionary. If
the candidate is close to the selected nlu?s and also
close to other lexical units, we consider it as a po-
tential noise (the more neighbours a candidate has,
the more it?s likely to be considered as noise). We
thus weight the similarity score of a candidate by
taking into account this information. We compare
the distribution of the candidate with the k nlu and
also with all its neighbours. This leads us to sup-
pose that a good candidate should be closer to the
selected nlu?s than the rest of its neighbours, if it?s
not the case there is more chances for this candidate
to be a wrong translation.
3.2 Proposed Approach
In the following we will describe our extension
to the method proposed by De?jean and Gaussier
(2002). The notational conventions adopted are re-
viewed in Table 1. Elaborations of definitions will
be given when the notation is introduced. In all our
experiments both terms and lexical units are single
words.
Symbol Definition
l a list of a given lexical unit.
k the number of selected nearest lex-
ical units (lists).
freq(w, k) the number of lists (k) in which a
term appears.
n all the neighbours of a given term.
u all the lexical units of the dictio-
nary.
wl a term of a given list l.
s(wl) the score of the term w in the list l.
maxl the maximum score of a given list
l.
maxAll the maximum score of all the lists.
snorm(wl) the normalized score of term w in
the list l.
s(w) the final score of a term w.
?w the regulation parameter of the
term w.
Table 1: Notational conventions.
The first step of our method is to collect each
list of each nlu. The size of the list has its impor-
tance because it determines how many candidates
are close to a given nlu. We noticed from our ex-
periments that, if we choose lists with small sizes,
we should lose information and if we choose lists
with large sizes, we could keep more information
than necessary and this should be a potential noise,
so we consider that a good size of each list should
be between 100 and 200 terms according to our ex-
periments.
After collecting the lists, the second step is to nor-
malize the scores. Let us consider the equation 2 :
snorm(wl) = s(wl)? maxlmaxAll (2)
We justify this by a rationale derived from two
observations. First, scores in different rankings are
compatible since they are based on the same simi-
larity measure (i.e., on the same scale). The second
observations follows from the first: if max (l) 
38
max (m), then the system is more confident about
the scores of the list l than m.
Using scores as fusion criteria, we compute the
similarity score of a candidate by summing its scores
from each list of the selected nlu?s :
s(w) = ?w ?
?k
l=1 snorm(wl)?n
l=1 snorm(wl)
(3)
the weight ? is given by :
?w = freq(w, k)? (u? (k ? freq(w, k)))(u? freq(w, n)) (4)
The aim of this parameter is to give more con-
fidence to a term that occurs more often with the
selected nearest neighbours (k) than the rest of its
neighbours. We can not affirm that the best candi-
date is the one that follows this idea, but we can nev-
ertheless suppose that candidates that appear with a
high number of lexical units are less confident and
have higher chances to be wrong candidates (we can
consider those candidates as noise). So, ? allows us
to regulate the similarity score, it is used as a confi-
dent weight or a regulation parameter. We will refer
to this model as the multiple source (MS) model. We
also use our model without using ? and refer to it by
(LC), this allows us to show the impact of ? in our
results.
4 Experiments and Results
4.1 Linguistic Resources
We have selected the documents from the Elsevier
website1 in order to obtain a French-English spe-
cialized comparable corpus. The documents were
taken from the medical domain within the sub-
domain of ?breast cancer?. We have automatically
selected the documents published between 2001 and
2008 where the title or the keywords contain the
term ?cancer du sein? in French and ?breast can-
cer? in English. We thus collected 130 documents
in French and 118 in English and about 530,000
words for each language. The documents compris-
ing the French/English specialized comparable cor-
pus have been normalized through the following lin-
guistic pre-processing steps: tokenisation, part-of-
1www.elsevier.com
speech tagging, and lemmatisation. Next, the func-
tion words were removed and the words occurring
less than twice (i.e. hapax) in the French and the
English parts were discarded. Finally, the compara-
ble corpus comprised about 7,400 distinct words in
French and 8,200 in English.
The French-English bilingual dictionary required
for the translation phase was composed of dictionar-
ies that are freely available on the Web. It contains,
after linguistic pre-processing steps, 22,300 French
single words belonging to the general language with
an average of 1.6 translations per entry.
In bilingual terminology extraction from special-
ized comparable corpora, the terminology refer-
ence list required to evaluate the performance of
the alignment programs are often composed of 100
single-word terms (SWTs) (180 SWTs in (De?jean
and Gaussier, 2002), 95 SWTs in (Chiao and
Zweigenbaum, 2002a), and 100 SWTs in (Daille
and Morin, 2005)). To build our reference list,
we selected 400 French/English SWTs from the
UMLS2 meta-thesaurus and the Grand dictionnaire
terminologique3. We kept only the French/English
pair of SWTs which occur more than five times in
each part of the comparable corpus. As a result of
filtering, 122 French/English SWTs were extracted.
4.2 Experimental Setup
Three major parameters need to be set to the ex-
tended approach, namely the similarity measure, the
association measure defining the entry vectors and
the size of the window used to build the context vec-
tors. Laroche and Langlais (2010) carried out a com-
plete study about the influence of these parameters
on the quality of bilingual alignment.
As similarity measure, we chose to use the
weighted jaccard index:
sim(i, j) =
?
t min (it, jt)?
t max (it, jt)
(5)
The entries of the context vectors were deter-
mined by the log-likelihood (Dunning, 1993), and
we used a seven-word window since it approximates
syntactic dependencies. Other combinations of pa-
rameters were assessed but the previous parameters
turned out to give the best performance.
2http://www.nlm.nih.gov/research/umls
3http://www.granddictionnaire.com/
39
4.3 Results
To evaluate the performance of our method, we use
as a baseline, the extended approach (EA) proposed
by De?jean and Gaussier (2002). We compare this
baseline to the two metasearch strategies defined
in Section 3: the metasearch model without the
regulation parameter ? (LC); and the one which is
weighted by theta (MS). We also provide results ob-
tained with the standard approach (SA).
We first investigate the stability of the metasearch
strategies with respect to the number of nlu consid-
ered. Figure 2 show the precision at Top 20 as a
function of k.
1 10 20 30 40 50 600
10
20
30
40
50
60
70
Number of NLU
Pre
cisi
on
att
op
20
LC
MS
EA
SA
Figure 2: Precision at top 20 as a function of the number
of nlu.
In order to evaluate the contribution of the param-
eter ?, we chose to evaluate the metasearch method
starting from k = 4, this explains why the precision
is extremely low for low values of k. We further
considered that less than four occurrences of a term
in the whole lexical units lists can be considered as
noise. On the other side, we started from k = 1 for
the extended approach since it makes no use of the
parameter ?. Figure 2 shows that extended approach
reaches its best performance at k = 7 with a preci-
sion of 40.98%. Then, after k = 15 the precision
starts steadily decreasing as the value of k increases.
The metasearch strategy based only on similarity
scores shows better results than the baseline. For
every value of k ? 10, the LC model outperform the
extended approach. The best precision (48.36%) is
obtained at k = 14, and the curve corresponding to
the LC model remains above the baseline regardless
of the increasing value of the parameter k. The curve
corresponding to the MS model is always above the
(EA) for every value of k ? 10. The MS model
consistently improves the precision, and achieves its
best performance (60.65%) at k = 21.
We can notice from Figure 2 that the LC and MS
models outperform the baseline (EA). More impor-
tantly, these models exhibit a better stability of the
precision with respect to the k-nearest lexical units.
Although the performance decrease as the value of k
increases, it does not decrease as fast as in the base-
line approach.
For the sake of comparability, we also provide
results obtained with the standard approach (SA)
(56.55%) represented by a straight line as it is not
dependent on k. As we can see, the metasearch
approach (MS) outperforms the standard approach
for values of k bertween 20 and 30 and for greater
values of k the precision remains more or less al-
most the same as the standard approach (SA). Thus,
the metasearch model (MS) can be considered as a
competitive approach regarding to its results as it is
shown in the figure 2.
Finally, Figure 3 shows the contribution of each
nlu taken independently from the others. This con-
firms our intuition that each nlu contribute to the
characterization of a lexical unit to be translated, and
supports our idea that their combination can improve
the performances.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
5
10
15
20
25
lexical units
Prec
isio
nat
top
20
Figure 3: Precision at top 20 for each of the 20 nlu. The
precision is computed by taking the each nlu indepen-
dently from the others.
Figure 3 shows the top 20 of each nlu. Notice
40
that the nlu are ordered from the most similar to the
lexical unit to be translated to the less similar, and
that each one of the nearest lexical units contains
information that it is worth taking into account.
Although each nlu can only translate few terms,
by using the metasearch idea we are able to improve
the retrieval of translation equivalents. The main
idea of the metasearch paradigm is to take into ac-
count the information conveyed by all the k nlu, us-
ing either similarity scores, their behaviour with all
the neighbours, in order to improve the performance
of the alignment process.
Although significant improvements can be ob-
tained with the metasearch models (comparatively
to the EA and SA approach), especially concerning
precision stability with respect to the k nlu, we be-
lieve that we need to address the estimation of k be-
forehand. Rather than fixing the same k for all the
units to be translated, there is the possibility to adapt
an optimal value of k to each lexical unit, according
to some criteria which have to be determined.
Approachs Top 5 Top 10 Top 15 Top 20
SA 37.70 45.08 52.45 56.55
EA 21.31 31.14 36.88 40.98
MS 40.98 54.91 56.55 60.65
Table 2: Precision(%) at top 5, 10, 15, 20 for SA, EA and
MS.
Finally, we present in table 2 a comparison be-
tween SA, EA and MS for the top 5, 10, 15 and 20.
By choosing the best configuration of each method,
we can note that our method outperforms the others
in each top. In addition, for the top 10 our preci-
sion is very close to the precision of the standard ap-
proach (SA) at the top 20. we consider these results
as encouraging for future work.
4.4 Discussion
Our experiments show that the parameter k remains
the core of both EA and MS approaches. A good
selection of the nearest lexical units of a term guar-
antee to find the good translation. It is important to
say that EA and MS which are based on the k nlu?s
depends on the coverage of the terms to be trans-
lated. Indeed, these approaches face three cases :
firstly, if the frequency of the word to be translated
is high and the frequency of the good translation in
the target language is low, this means that the nearest
lexical units of the candidate word and its translation
are unbalanced. This leads us to face a lot of noise
because of the high frequency of the source word
that is over-represented by its nlu?s comparing to the
target word which is under-represented. Secondly,
we consider the inverse situation, which is: low fre-
quency of the source word and high frequency of
the target translation, here as well, we have both the
source and the target words that are unbalanced re-
garding to the selected nearest lexical units. The
third case, represents more or less the same distri-
bution of the frequencies of source candidate and
target good translation. This can be considered as
the most appropriate case to find the good transla-
tion by applying the approaches based on the nlu?s
(EA or MS). Our experiments show that our method
works well in all the cases by using the parameter ?
which regulate the similarity score by taken into ac-
count the distribution of the candidate according to
both : selected nlu?s and all its neighbours. In re-
sume, words to be translated as represented in case
one and two give more difficulties to be translated
because of their unbalanced distribution which leads
to an unbalanced nlu?s. Future works should con-
firm the possibility to adapt an optimal value of k
to each candidate to be translated, according to its
distribution with respect to its neighbours.
5 Conclusion
We have presented a novel way of looking at the
problem of bilingual lexical extraction from compa-
rable corpora based on the idea of metasearch en-
gines. We believe that our model is simple and
sound. Regarding the empirical results of our propo-
sition, performances of the multiple source model
on our dataset was better than the baseline proposed
by De?jean and Gaussier (2002), and also outper-
forms the standard approach for a certain range of
k. We believe that the most significant result is that
a new approach to finding single word translations
has been shown to be competitive. We hope that
this new paradigm can lead to insights that would
be unclear in other models. Preliminary tests in this
perspective show that using an appropriate value of
k for each word can improve the performance of the
lexical extraction process. Dealing with this prob-
41
lem is an interesting line for future research.
6 Acknowledgments
The research leading to these results has received
funding from the French National Research Agency
under grant ANR-08-CORD-013.
References
Javed A. Aslam and Mark Montague. 2001. Models for
Metasearch. In SIGIR ?01, proceedings of the 24th
Annual SIGIR Conference, pages 276?284.
Lynne Bowker and Jennifer Pearson. 2002. Working
with Specialized Language: A Practical Guide to Us-
ing Corpora. Routledge, London/New York.
Yunbo Cao and Hang Li. 2002. Base Noun Phrase Trans-
lation Using Web Data and the EM Algorithm. In
Proceedings of the 19th International Conference on
Computational Linguistics (COLING?02), pages 127?
133, Tapei, Taiwan.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002a.
Looking for candidate translational equivalents in spe-
cialized, comparable corpora. In Proceedings of the
19th International Conference on Computational Lin-
guistics (COLING?02), pages 1208?1212, Tapei, Tai-
wan.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002b.
Looking for French-English Translations in Compara-
ble Medical Corpora. Journal of the American Society
for Information Science, 8:150?154.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2003. The
Effect of a General Lexicon in Corpus-Based Identifi-
cation of French-English Medical Word Translations.
In Robert Baud, Marius Fieschi, Pierre Le Beux, and
Patrick Ruch, editors, The New Navigators: from Pro-
fessionals to Patients, Actes Medical Informatics Eu-
rope, volume 95 of Studies in Health Technology and
Informatics, pages 397?402, Amsterdam. IOS Press.
Be?atrice Daille and Emmanuel Morin. 2005. French-
English Terminology Extraction from Comparable
Corpora. In Proceedings of the 2nd International
Joint Conference on Natural Language Processing (IJ-
CLNP?05), pages 707?718, Jeju Island, Korea.
Herve? De?jean and E?ric Gaussier. 2002. Une nouvelle ap-
proche a` l?extraction de lexiques bilingues a` partir de
corpus comparables. Lexicometrica, Alignement lexi-
cal dans les corpus multilingues, pages 1?22.
Herve? De?jean, Fatia Sadat, and E?ric Gaussier. 2002.
An approach based on multilingual thesauri and model
combination for bilingual lexicon extraction. In Pro-
ceedings of the 19th International Conference on
Computational Linguistics (COLING?02), pages 218?
224, Tapei, Taiwan.
Ted Dunning. 1993. Accurate Methods for the Statistics
of Surprise and Coincidence. Computational Linguis-
tics, 19(1):61?74.
Robert M. Fano. 1961. Transmission of Information:
A Statistical Theory of Communications. MIT Press,
Cambridge, MA, USA.
Pascale Fung and Yuen Yee Lo. 1998. An ir approach for
translating new words from nonparallel, comparable
texts. In Proceedings of the 17th international con-
ference on Computational linguistics (COLING?98),
pages 414?420.
Pascale Fung and Kathleen McKeown. 1997. Find-
ing Terminology Translations from Non-parallel Cor-
pora. In Proceedings of the 5th Annual Workshop on
Very Large Corpora (VLC?97), pages 192?202, Hong
Kong.
Pascale Fung. 1998. A Statistical View on Bilingual Lex-
icon Extraction: From ParallelCorpora to Non-parallel
Corpora. In David Farwell, Laurie Gerber, and Eduard
Hovy, editors, Proceedings of the 3rd Conference of
the Association for Machine Translation in the Ameri-
cas (AMTA?98), pages 1?16, Langhorne, PA, USA.
E?ric Gaussier, Jean-Michel Renders, Irena Matveeva,
Cyril Goutte, and Herve? De?jean. 2004. A Geomet-
ric View on Bilingual Lexicon Extraction from Com-
parable Corpora. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL?04), pages 526?533, Barcelona, Spain.
Gregory Grefenstette. 1994a. Corpus-Derived First, Sec-
ond and Third-Order Word Affinities. In Proceedings
of the 6th Congress of the European Association for
Lexicography (EURALEX?94), pages 279?290, Ams-
terdam, The Netherlands.
Gregory Grefenstette. 1994b. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publisher,
Boston, MA, USA.
Audrey Laroche and Philippe Langlais. 2010. Re-
visiting Context-based Projection Methods for Term-
Translation Spotting in Comparable Corpora. In
Proceedings of the 23rd International Conference on
Computational Linguistics (COLING?10), pages 617?
625, Beijing, China.
Emmanuel Morin, Be?atrice Daille, Koichi Takeuchi, and
Kyo Kageura. 2007. Bilingual Terminology Mining ?
Using Brain, not brawn comparable corpora. In Pro-
ceedings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL?07), pages 664?
671, Prague, Czech Republic.
Carol Peters and Eugenio Picchi. 1998. Cross-language
information retrieval: A system for comparable cor-
pus querying. In Gregory Grefenstette, editor, Cross-
language information retrieval, chapter 7, pages 81?
90. Kluwer Academic Publishers.
42
Reinhard Rapp. 1995. Identify Word Translations in
Non-Parallel Texts. In Proceedings of the 35th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?95), pages 320?322, Boston, MA, USA.
Reinhard Rapp. 1999. Automatic Identification of Word
Translations from Unrelated English and German Cor-
pora. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics (ACL?99),
pages 519?526, College Park, MD, USA.
Gerard Salton and Michael E. Lesk. 1968. Computer
evaluation of indexing and text processing. Jour-
nal of the Association for Computational Machinery,
15(1):8?36.
43
