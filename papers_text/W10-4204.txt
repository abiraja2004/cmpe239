Hierarchical Reinforcement Learning for Adaptive Text Generation
Nina Dethlefs
University of Bremen, Germany
dethlefs@uni-bremen.de
Heriberto Cuaya?huitl
University of Bremen, Germany
heriberto@uni-bremen.de
Abstract
We present a novel approach to natural lan-
guage generation (NLG) that applies hierar-
chical reinforcement learning to text genera-
tion in the wayfinding domain. Our approach
aims to optimise the integration of NLG tasks
that are inherently different in nature, such
as decisions of content selection, text struc-
ture, user modelling, referring expression gen-
eration (REG), and surface realisation. It
also aims to capture existing interdependen-
cies between these areas. We apply hierar-
chical reinforcement learning to learn a gen-
eration policy that captures these interdepen-
dencies, and that can be transferred to other
NLG tasks. Our experimental results?in a
simulated environment?show that the learnt
wayfinding policy outperforms a baseline pol-
icy that takes reasonable actions but without
optimization.
1 Introduction
Automatic text generation involves a number of sub-
tasks. (Reiter and Dale, 1997) list the following as
core tasks of a complete NLG system: content se-
lection, discourse planning, sentence planning, sen-
tence aggregation, lexicalisation, referring expres-
sion generation and linguistic realisation. However,
decisions made for each of these core tasks are not
independent of each other. The value of one gen-
eration task can change the conditions of others,
as evidenced by studies in corpus linguistics, and
it can therefore be undesirable to treat them all as
isolated modules. In this paper, we focus on inter-
related decision making in the areas of content se-
lection, choice of text structure, referring expression
and surface form. Concretely, we generate route in-
structions that are tailored specifically towards dif-
ferent user types as well as different environmental
features. In addition, we aim to balance the degree
of variation and alignment in texts and produce lex-
ical and syntactic patterns of co-occurrence that re-
semble those of human texts of the same domain.
Evidence for the importance of this is provided by
(Halliday and Hasan, 1976) who note the way that
lexical cohesive ties contribute to text coherence as
well as by the theory of interactive alignment. Ac-
cording to (Pickering and Garrod, 2004) we would
expect significant traces of lexical and syntactic self-
alignment in texts.
Approaches to NLG in the past have been ei-
ther rule-based (Reiter and Dale, 1997) or statisti-
cal (Langkilde and Knight, 1998). However, the for-
mer relies on a large number of hand-crafted rules,
which makes it infeasible for controlling a large
number of interrelated variables. The latter typi-
cally requires training on a large corpus of the do-
main. While these approaches may be better suitable
for larger domains, for limited domains such as our
own, we propose to overcome these drawbacks by
applying Reinforcement Learning (RL)?with a hi-
erarchical approach. Previous work that has used RL
for NLG includes (Janarthanam and Lemon, 2009)
who employed it for alignment of referring expres-
sions based on user models. Also, (Lemon, 2008;
Rieser and Lemon, 2009) used RL for optimising
information presentation styles for search results.
While both approaches displayed significant effects
of adaptation, they focused on a single area of opti-
misation. For larger problems, however, such as the
one we are aiming to solve, flat RL will not be appli-
cable due to the large state space. We therefore sug-
gest to divide the problem into a number of subprob-
lems and apply hierarchical reinforcement learning
(HRL) (Barto and Mahadevan, 2003) to solve it.
We describe our problem in more detail in Sec-
tion 2, our proposed HRL architecture in Sections
3 and 4 and present some results in Section 5. We
show that our learnt policies outperform a baseline
that does not adapt to contextual features.
2 Generation tasks
Our experiments are all drawn from an indoor
navigation dialogue system which provides users
with route instructions in a university building and
is described in (Cuaya?huitl et al, 2010). We aim
to optimise generation within the areas of content
selection, text structure, referring expression gener-
ation and surface realisation.
Content Selection Content selection decisions
are subject to different user models. We distin-
guish users who are familiar with the navigation
environment and users who are not. In this way,
we can provide different routes for these users
corresponding to their particular information need.
Specifically, we provide more detail for unfamiliar
than familiar users by adding any or several of
the following: (a) landmarks at decision points,
(b) landmarks lying on long route segments, (c)
specifications of distance.
Text Structure Depending on the type of user
and the length of the route, we choose among three
different text generation strategies to ease the cogni-
tive load of the user. Examples of all strategies are
displayed in Table 1. All three types resulted from
an analysis of a corpus of 24 human-written driving
route instructions. We consider the first type (se-
quential) most appropriate for long or medium-long
routes and both types of user. The second type (tem-
poral) is appropriate for unfamiliar users and routes
of short or medium length. It divides the route into
an explicit sequence of consecutive actions. The
third type (schematic) is used in the remaining cases.
Referring Expression Generation We dis-
tinguish three types of referring expressions:
common names, familiar names and descriptions.
In this way, entities can be named according to
the users? prior knowledge. For example, one
and the same room can be called either ?the
student union room?, ?room A3530? or ?the room
right at the corner beside the entrance to the terrace?.
Surface Realisation For surface realisation, we
aim to generate texts that display a natural balance
of (self-)alignment and variation. While it is a rule
of writing that texts should typically contain varia-
tion of surface forms in order not to appear repetitive
and stylistically poor, there is evidence that humans
also get influenced by self-alignment processes dur-
ing language production. Specifically, (Garrod and
Anderson, 1987; Pickering and Garrod, 2004) ar-
gue that the same mental representations are used
during language production and comprehension, so
that alignment occurs regardless of whether the last
utterance was made by another person or by the
speaker him- or herself (for experimental evidence
see (Branigan et al, 2000; Bock, 1986)). We can
therefore hypothesise that coherent texts will, be-
sides variation, also display a certain degree of self-
alignment. In order to determine a proper balance
of alignment and variation, we computed the degree
of lexical repetition from our corpus of 24 human
route descriptions. This analysis was based on (Hirst
and St-Onge, 1998) who retrieve lexical chains from
texts by identifying a number of relations between
lexical items. We focus here exclusively on Hirst
& St-Onge?s ?extra-strong? relations, since these can
be computed from shallow properties of texts and do
not require a large corpus of the target domain. In
order to make a fair comparison between the human
texts and our own, we used a part-of-speech (POS)
tagger (Toutanova and Manning, 2000)1 to extract
those grammatical categories that we aim to control
within our framework, i.e. nouns, verbs, preposi-
tions, adjectives and adverbs. Based on these cat-
egories, we compute the proportion of tokens that
are members in lexical chains, the ?alignment score?
(AS), according to the following equation:
AS = Lexical tokens in chains
Total number of tokens ? 100. (1)
We obtained an average alignment score of 43.3%
for 24 human route instructions. In contrast, the
1http://nlp.stanford.edu/software/tagger.shtml
Table 1: Different text generation strategies for the same underlying route.
Type 1: Sequential Type 2: Temporal Type 3: Schematic
Turn around, and go straight First, turn around. Second, - Turn around.
to the glass door in front of go straight to the glass door - Go straight until the glass door in front
you. Turn right, then follow in front of you. Third, turn of you. (20 m)
the corridor until the lift. It right. Fourth, follow the - Turn right
will be on your left-hand corridor until the lift. It will - Follow the corridor until the lift. (20 m)
side. be on your left-hand side. - It will be on your left-hand side.
same number of instructions generated by Google
Maps yielded 78.7%, i.e. an almost double amount
of repetition. We will therefore train our agent
to generate texts with an about medium alignment
score.
3 Hierarchical Reinforcement Learning
for NLG
The idea of text generation as an optimization
problem is as follows: given a set of genera-
tion states, a set of actions, and an objective
reward function, an optimal generation strategy
maximizes the objective function by choosing the
actions leading to the highest reward for every
reached state. Such states describe the system?s
knowledge about the generation task (e.g. con-
tent selection, text structure, REG, surface realiza-
tion). The action set describes the system?s ca-
pabilities (e.g. expand sequential aggregation, ex-
pand schematic aggregation, expand lexical items,
etc.). The reward function assigns a numeric value
for each taken action. In this way, text generation
can be seen as a finite sequence of states, actions
and rewards {s0, a0, r1, s1, a1, ..., rt?1, st}, where
the goal is to find an optimal strategy automatically.
To do that we use hierarchical reinforcement learn-
ing in order to optimize a hierarchy of text genera-
tion policies rather than a single policy.
The hierarchy of RL agents consists of L lev-
els and N models per level, denoted as M = M ij ,
where j ? {0, ..., N ? 1} and i ? {0, ..., L ? 1}.
Each agent of the hierarchy is defined as a Semi-
Markov Decision Process (SMDP) consisting of a
4-tuple < Sij, Aij , T ij , Rij >. Sij is a set of states, Aij
is a set of actions, and T ij is a transition function that
determines the next state s? from the current state
s and the performed action a with a probability of
P (s?|s, a). Rij(s?, ? |s, a) is a reward function that
specifies the reward that an agent receives for taking
an action a in state s at time ? . Since SMDPs allow
for temporal abstraction, that is, actions may take a
variable number of time steps to complete, the ran-
dom variable ? represents this number of time steps.
Actions can be either primitive or composite. The
former yield single rewards, the latter (executed us-
ing a stack mechanism) correspond to SMDPs and
yield cumulative discounted rewards. The goal of
each SMDP is to find an optional policy ?? that max-
imises the reward for each visited state, according to
??ij(s) = arg maxa?A Q
?i
j(s, a). (2)
where Qij(s, a) specifies the expected cumulative re-
ward for executing action a in state s and then fol-
lowing ??. For learning a generation policy, we
use hierarchical Q-Learning (HSMQ) (Dietterich,
1999). The dynamics of SMDPs are as follows:
when an SMDP terminates its execution, it is popped
off the stack of models to execute, and control is
transferred to the next available SMDP in the stack,
and so on until popping off the root SMDP. An
SMDP terminates when it reaches one of its termi-
nal states. This algorithm is executed until the Q-
values of the root agent stabilize. The hierarchical
decomposition allows to find context-independent
policies with the advantages of policy reuse and fa-
cilitation for state-action abstraction. This hierarchi-
cal approach has been applied successfully to dia-
logue strategy learning (Cuayahuitl et al, 2010).
4 Experimental Setting
4.1 Hierarchy of SMDPs
The hierarchy consists of 15 agents. It is depicted
in Figure 1. The root agent is responsible for deter-
Figure 1: Hierarchy of agents for learning adaptive text generation strategies in the wayfinding domain
mining a route instruction type for a navigation situ-
ation. We distinguish turning, passing, locating, go-
ing and following instructions. It also chooses a text
generation strategy and the information structure of
the clause (i.e., marked or unmarked theme (Hall-
iday and Matthiessen, 2004)). Leaf agents are re-
sponsible for expanding constituents in which varia-
tion or alignment can occur, e.g. the choice of verb
or prepositional phrase.
4.2 State and action sets
We distinguish three kinds of state representations,
displayed in Table 2. The first (M010 and M10 ) en-
codes information on the spatial environment and
user type so that texts can be tailored towards these
variables. These variables play a major part in our
simulated environment (Section 5.1). The second
representation (M11 - M15 and M23 ) controls sentence
structure and ensures that all required constituents
for a message have been realised. The third (all re-
maining models) encodes variants of linguistic sur-
face structure and represents the degree of alignment
of all variants. We address the way that these align-
ment values are computed in Section 4.4. Actions
can be either primitive or composite. Whereas the
former expand a logical form directly, the latter cor-
respond to SMDPs at different levels of the hierar-
chy. All parent agents have both types of actions,
only the leaf agents have exclusively primitive ac-
tions. The set of primitive actions is displayed in Ta-
ble 2, all composite actions, corresponding to mod-
els, are shown in Figure 1. The average number of
state-action pairs for a model is |S ? A| = 77786.
While in the present work, the action set was de-
termined manually, future work can aim at learning
hierarchies of SMDPs automatically from data.
4.3 Prior Knowledge
Agents contain prior knowledge of two sorts. First,
the root agents and agents at the first level of the hi-
erarchy contain prior probabilities of executing cer-
tain actions. For example, given an unfamiliar user
and a long route, model M10 , text strategy, is initi-
ated with a higher probability of choosing a sequen-
tial text strategy than a schematic or temporal strat-
egy. Second, leaf agents of the hierarchy are initi-
ated with values of a hand-crafted language model.
These values indicate the probabilities of occurrence
of the different surface forms of the leaf agents listed
in Table 2. Both types of prior probabilities are used
by the reward functions described below.
4.4 Reward functions
We use two types of reward function, both of which
are directly motivated by the principles we stated in
Section 2. The first addresses interaction length (the
shorter the better) and the choice of actions tailored
towards the user model and spatial environment.
R =
?
?
?
0 for reaching the goal state
-10 for an already invoked subtask
p(a) otherwise
(3)
p(a) corresponds to the probability of the last ac-
tion given the current state, described above as prior
knowledge. The second reward function addresses
Table 2: State and action sets for learning adaptive text generation strategies in the wayfinding domain
Model State Variables Action Set
M00 text strategy (FV), info structure (FV), instruction (FV), expand text strategy (M10 ), turning (M23 ),
slot in focus(0=action, 1=landmark), user type(0=unfamiliar, going (M21 ), passing (M25 ), following (M22 ),
1=familiar) subtask termination(0=continue, 1=halt) locating instr.(M24 ), expand unmarked theme
M10 end(0=continue, 1=halt), text strategy (FV), route length expand schematic aggregation, expand sequen-
(0=short, 1=medium, 2=long), user type(0=unfam., 1=fam.) ce aggregation, expand temporal aggregation
M11 going vp (FV), limit (FV), SV expand going vp (M20 ), expand limit
M12 following vp (FV), SV, limit (FV) expand following vp (M21 ), expand limit
M13 turning location (FV), turning vp (FV), expand turning vp (M32 ), expand turning loc.,
SV, turning direction (FV) expand turning direction (M34 )
M14 np locatum (FV), locating vp (FV), expand np locatum, expand locating vp (M25 ),
static direction (FV), SV expand static dir. (M26 )
M15 np locatum (FV), passing vp (FV), SV, static direction (FV) expand pass. vp (M26 ), expand static dir. (M26 )
M20 vp go straight ahead, vp go straight, vp move straight ahead, Actions correspond to expansions of
vp walk straight ahead, vp walk straight (all AS) lexemes
M21 vp follow, vp go over, vp walk down, vp go down, Actions correspond to expansions of
vp go up, vp walk up, vp walk over (all AS) lexemes
M22 vp walk, vp veer , vp hang, vp bear (all AS), vp go, vp head, Actions correspond to expansions of
vp turn (all AS) lexemes
M23 identifiability(0=not id.,1=id.), user type(0=un-, expand relatum id., expand relatum, not id.,
fam.,1=fam,, relatum identifiability (FV), relatum name (FV) expand descriptive, expand common name
M24 pp nonphoric, pp nonphoric handedness, Actions correspond to expansions of
pp nonphoric poss, pp phoric pp nonphoric side (all AS) lexemes
M25 vp be, vp be located at, vp get to, vp see (all AS) Actions correspond to expansions of lexemes
M26 direction on, direction poss, direction to (all AS) Actions correspond to expansions of lexemes
M27 vp move past, vp pass, vp pass by, vp walk past (all AS) Actions correspond to expansions of lexemes
(FV = filling status): 0=unfilled, 1=filled. (SV = shared variables): the variables np actor (FV), relatum (FV),
sentence (FV) and information need (0=low, 1=high) are shared by several subagents; the same applies to their
corresponding expansion actions. (AS = alignment score): 0=unaligned, 1=low AS, 2=medium AS, 3=high AS.
the tradeoff between alignment and variation:
R =
?
?
?
0 for reaching the goal state
p(a) for medium alignment
-0.1 otherwise
(4)
Whilst the former reward function is used by the root
and models M10 - M15 and M22 , the latter is used by
models M20 - M21 and M23 - M27 . It rewards the agent
for a medium alignment score, which corresponds
to the score of typical human texts we computed
in Section 2. The alignment status of a constituent
is computed by the Constituent Alignment Score
(CAS) as follows, where MA stands for ?medium
alignment?.
CAS(a) = Count of occurrences(a)Occurences of a without MA (5)
From this score, we can determine the degree of
alignment of a constituent by assigning ?no align-
ment? for a constituent with a score of less than
0.25, ?low alignment? for a score between 0.25 and
0.5, ?medium alignment? for a score between 0.5 and
0.75 and ?high alignment? above. On the whole thus,
the agent?s task consists of finding a balance be-
tween choosing the most probable action given the
language model and choosing an action that aligns
with previous utterances.
5 Experiments and Results
5.1 Simulated Environment
The simulated environment encodes information on
the current user type (un-/familiar with the environ-
ment) and corresponding information need (low or
high), the length of the current route (short, medium-
long, long), the next action to perform (turn, go
straight, follow a path, pass a landmark or take note
of a salient landmark) and the current focus of at-
tention (the action to be performed or some salient
landmark nearby). Thus, there are five different state
variables with altogether 120 combinations, sam-
pled from a uniform distribution. This simple form
of stochastic behaviour is used in our simulated en-
vironment. Future work can consider inducing a
learning environment from data.
5.2 Comparison of learnt and baseline policies
In order to test our framework, we designed a sim-
ulated environment that simulates different naviga-
tional situations, routes of different lengths and dif-
ferent user types. We trained our HRL agent for
10.000 episodes with the following learning param-
eters: the step-size parameter ? was initiated with 1
and then reduced over time by ? = 11+t , t being the
time step. The discount rate parameter ? was 0.99
and the probability of random action ? was 0.01 (see
(Sutton and Barto, 1998) for details on these param-
eters). Figure 2 compares the learnt behaviour of
our agent with a baseline (averaged over 10 runs)
that chooses actions at random in models M10 and
M20 - M27 (i.e., the baseline does not adapt its text
strategy to user type or route length and neither per-
forms adaptation of referring expressions or align-
ment score). The user study reported in (Cuaya?huitl
et al, 2010) provided users with instruction using
this baseline generation behaviour. The fact that
users had a user satisfaction score of 90% indicates
that this is a sensible baseline, producing intelligi-
ble instructions. We can observe that after a certain
number of episodes, the performance of the trained
agent begins to stabilise and it consistently outper-
forms the baseline.
6 Example of generation
As an example, Figure 3 shows in detail the genera-
tion steps involved in producing the clause ?Follow
102 103 104
?65
?60
?55
?50
?45
?40
?35
?30
?25
Av
er
ag
e 
Re
wa
rd
Episodes
 
 
Learnt Behaviour
Baseline
Figure 2: Comparison of learnt and baseline behaviour in
the generation of route descriptions
the corridor until the copyroom? for an unfamiliar
user and a route of medium length. Generation starts
with the root agent in state (0,0,0,0,0,0), which in-
dicates that text strategy, info structure and instruc-
tion are unfilled slots, the slot in focus of the sen-
tence is an action, the status of subtask termination
is ?continue? and the user type is unfamiliar. After
the primitive action expand unmarked theme was
executed, the state is updated to (0,1,0,0,0,0), in-
dicating the filled slot. Next, the composite action
text strategy is executed, corresponding to model
M10 . The initial state (1,0,0) indicates a route
of medium length, an unfilled text strategy slot
and an unfamiliar user. After the primitive ac-
tion expand sequential text was chosen, the ter-
minal state is reached and control is returned to
the root agent. Here, the next action is follow-
ing instruction corresponding to model M12 . The
initial state (0,1,0,0,0,0) here indicates unfilled slots
for following vp, np actor, sentence, path, limit
and relatum, as well as a high information need
of the current user. The required constituents
are expanded in turn. First, the primitive actions
expand limit, expand np actor, expand s and ex-
pand path cause their respective slots in the state
representation to be filled. Next, the composite ac-
tion expand relatum is executed with an initial state
(0,1,0,0) representing an identifiable landmark, un-
filled slots for a determiner and a referring expres-
sion for the landmark and an unfamiliar user. Two
primitive actions, expand relatum identifiable and
expand relatum common name, cause the agent to
reach its terminal state. The generated referring ex-
pression thus treats the referenced entity as either
known or easily recoverable. Finally, model M21
executes the composite action expand following vp,
which is initialised with a number of variables cor-
responding to the alignment status of different verb
forms. Since this is the first time this agent is called,
none of them shows traces of alignment (i.e., all val-
ues are 0). Execution of the primitive action ex-
pand following vp causes the respective slot to be
updated and the agent to terminate. After this sub-
task, model M12 has also reached its terminal state
and control is returned to the root agent.
As a final step towards surface generation, all cho-
sen actions are transformed into an SPL (Kasper,
1989). The type ?following instruction? leads to the
initialisation of a semantically underspecified scaf-
fold of an SPL, all other actions serve to supplement
this scaffold to preselect specific syntactic structures
or lexical items. For example, the choice of ?ex-
pand following vp? leads to the lexical item ?fol-
low? being inserted. Similarly, the choice of ?ex-
pand path? leads to the insertion of ?the corridor?
into the SPL to indicate the path the user should fol-
low. ?expand limit?, in combination with the choice
of referring expression, leads to the insertion of the
PP ?until the copy room?. For generation of more
than one instruction, aggregation has to take place.
This is done by iterating over all instructions of a
text and inserting them into a larger SPL that re-
alises the aggregation. Finally, the constructed SPL
is passed to the KPML surface generator (Bateman,
1997) for string realisation.
7 Discussion
We have argued in this paper that HRL is an es-
pecially suited framework for generating texts that
are adaptive to different users, to environmental fea-
tures and properties of surface realisation such as
alignment and variation. While the former tasks ap-
pear intuitively likely to contribute to users? com-
prehension of texts, it is often not recognised that
the latter task can have the same effect. Differing
surface forms of identical concepts in texts without
motivation can lead to user confusion and deterio-
rate task success. This is supported by Clark?s ?prin-
ciple of contrast? (Clark, 1987), according to which
new expressions are only introduced into an interac-
tion when the speaker wishes to contrast them with
other entities already present in the discourse. Si-
miliarly, a study by (Clark and Wilkes-Gibbs, 1986)
showed that interlocutors tend to align their referring
expressions and thereby achieve more efficient and
successful dialogues. We tackled the integration of
different NLG tasks by applying HRL and presented
results, which showed to be promising. As an al-
ternative to RL, other machine learning approaches
may be conceivable. However, supervised learning
requires a large amount of training data, which may
not always be available, and may also produce un-
predictable behaviour in cases where a user deviates
from the behaviour covered by the corpus (Levin
et al, 2000). Both arguments are directly trans-
ferable to NLG. If an agent is able to act only on
grounds of what it has observed in a training cor-
pus, it will not be able to react flexibly to new state
representations. Moreover, it has been argued that
a corpus for NLG cannot be regarded as an equiv-
alent gold standard to the ones of other domains of
NLP (Belz and Reiter, 2006; Scott and Moore, 2006;
Viethen and Dale, 2006). The fact that an expres-
sion for a semantic concept does not appear in a cor-
pus does not mean that it is an unsuited or impos-
sible expression. Another alternative to pure RL is
to apply semi-learnt behaviour, which can be help-
ful for tasks with very large state-action spaces. In
this way, the state-action space is reduced to only
sensible state-action pairs by providing the agent
with prior knowledge of the domain. All remain-
ing behaviour continues to be learnt. (Cuaya?huitl,
2009) suggests such an approach for learning dia-
logue strategies, but again the principle is transfer-
able to NLG. While there is room for exploration
of different RL methods, it is clear that neither tra-
ditional rule-based accounts of generation, nor n-
gram-based generators can achieve the same flexible
generation behaviour given a large, and partially un-
known, number of state variables. Since state spaces
are typically very large, specifying rules for each
single condition is at best impractical. Especially for
tasks such as achieving a balanced alignment score,
as we have shown in this paper, decisions depend on
very fine-grained textual cues such as patterns of co-
occurrence which are hard to pin down accurately
by hand. On the other hand, statistical approaches
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
M00 (0, 0, 0, 0, 0, 0)
{action = expand unmarked
theme}
M00 (0, 1, 0, 0, 0, 0)
{action = text strategy}
?
?
M10 (1, 0, 0)
{action = expand sequential text}
M10 (1, 1, 0), (terminalstate)
?
?
M00 , (1, 1, 0, 0, 0, 0),
{action = following instruction}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
M21 (0, 1, 0, 0, 0, 0)
{action = expand limit}
M21 (0, 1, 1, 0, 0, 0)
{action = expand np actor}
M21 (0, 1, 1, 1, 0, 0)
{action = expand s}
M21 (0, 1, 1, 0, 1, 0)
{action = expand path}
M21 (0, 1, 1, 1, 1, 0)
{action = expand relatum}
?
?
?
?
?
?
?
?
?
M23 (0, 0, 0, 0)
{action = expand relatum
identifiable}
M23 (0, 1, 0, 0)
{action = expand relatum
common name}
M23 (0, 1, 1, 0), (terminalstate)
?
?
?
?
?
?
?
?
?
M21 (0, 1, 1, 1, 1, 0)
{action = expand following
vp}
?
?
?
?
M21 (0, 0, 0, 0, 0, 0, 0, 0, 0)
{action = follow}
M21 (1, 0, 0, 0, 0, 0, 0, 0, 0)
(terminalstate)
?
?
?
?
M21 (1, 1, 1, 1, 1, 1)(terminalstate)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
M00 (1, 1, 1, 0, 1, 0)(terminalstate)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 3: Example of generation for the clause ?Follow the corridor until the copy room?. This example shows decision
making for a single instruction, adaptation and alignment occurs over longer sequences of text.
to generation that are based on n-grams focus on the
frequency of constructions in a corpus without tak-
ing contextual variables such as user type or environ-
mental properties into account. Further, they share
the problem of supervised learning approaches dis-
cussed above, namely, that it can act only on grounds
of what it has observed in the past, and are not well
able to adapt to novel situations. For a more de-
tailed account of statistical and trainable approaches
to NLG as well as their advantages and drawbacks,
see (Lemon, 2008).
8 Conclusion
We presented a novel approach to text generation
that applies hierarchical reinforcement learning to
optimise the following interrelated NLG tasks: con-
tent selection, choice of text structure, referring ex-
pressions and surface structure. Generation deci-
sions in these areas were learnt based on three differ-
ent variables: the type of user, the properties of the
spatial environment and the proportion of alignment
and variation in texts. Based on a simulated envi-
ronment, we compared the results of different poli-
cies and demonstrated that the learnt policy outper-
forms a baseline that chooses actions without taking
contextual variables into account. Future work can
transfer our approach to different domains of appli-
cation or to other NLG tasks. In addition, our pre-
liminary simulation results should be confirmed in
an evaluation study with real users.
Acknowledgements
This work was partly supported by DFG SFB/TR8
?Spatial Cognition?.
References
Barto, A. G. and Mahadevan, S. (2003). Recent Ad-
vances in Hierarchical Reinforcement Learning. Dis-
crete Event Dynamic Systems, 13:2003.
Bateman, J. A. (1997). Enabling technology for multi-
lingual natural language generation: the KPML devel-
opment environment. Natural Language Engineering,
3(1):15?55.
Belz, A. and Reiter, E. (2006). Comparing automatic and
human evaluation of nlg systems. In In Proc. EACL06,
pages 313?320.
Bock, K. (1986). Syntactic persistence in language pro-
duction. Cognitive Psychology, 18.
Branigan, H. P., Pickering, M. J., and Cleland, A. (2000).
Syntactic coordination in dialogue. Cognition, 75.
Clark, E. (1987). The principle of contrast: A constraint
on language acquisition. In MacWhinney, B., edi-
tor, Mechanisms of Language Acquisition, pages 1?33.
Lawrence Erlbaum Assoc., Hillsdale, NJ.
Clark, H. H. and Wilkes-Gibbs, D. (1986). Referring as
a colloborative process. Cognition, 22.
Cuaya?huitl, H. (2009). Hierarchical Reinforcement
Learning for Spoken Dialogue Systems. PhD thesis,
School of Informatics, University of Edinburgh.
Cuaya?huitl, H., Dethlefs, N., Richter, K.-F., Tenbrink, T.,
and Bateman, J. (2010). A dialogue system for indoor
wayfinding using text-based natural language. In-
ternational Journal of Computational Linguistics and
Applications, ISSN 0976-0962.
Cuayahuitl, H., Renals, S., Lemon, O., and Shimodaira,
H. (2010). Evaluation of a hierarchical reinforcement
learning spoken dialogue system. Computer Speech
and Language, 24(2):395?429.
Dietterich, T. G. (1999). Hierarchical reinforcement
learning with the maxq value function decomposition.
Journal of Artificial Intelligence Research, 13:227?
303.
Garrod, S. and Anderson, A. (1987). Saying What You
Mean in Dialogue: A Study in conceptual and seman-
tic co-ordination. Cognition, 27.
Halliday, M. A. K. and Hasan, R. (1976). Cohesion in
English. Longman, London.
Halliday, M. A. K. and Matthiessen, C. M. I. M. (2004).
An Introduction to Functional Grammar. Edward
Arnold, London, 3rd edition.
Hirst, G. and St-Onge, D. (1998). Lexical chains as rep-
resentations of context for the detection and correction
of malapropisms. In Fellbaum, C., editor, WordNet:
An Electronic Database and Some of its Applications,
pages 305?332. MIT Press.
Janarthanam, S. and Lemon, O. (2009). Learning lexi-
cal alignment policies for generating referring expres-
sions in spoken dialogue systems. In ENLG ?09: Pro-
ceedings of the 12th European Workshop on Natural
Language Generation, pages 74?81, Morristown, NJ,
USA.
Kasper, R. (1989). SPL: A Sentence Plan Language for
text generation. Technical report, USC/ISI.
Langkilde, I. and Knight, K. (1998). Generation that ex-
ploits corpus-based statistical knowledge. In ACL-36:
Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics and 17th In-
ternational Conference on Computational Linguistics,
pages 704?710.
Lemon, O. (2008). Adaptive Natural Language Gener-
ation in Dialogue using Reinforcement Learning. In
SemDial.
Levin, E., Pieraccini, R., and Eckert, W. (2000). A
stochastic model of computer-human interaction for
learning dialogue strategies. IEEE Transactions on
Speech and Audio Processing, 8.
Pickering, M. J. and Garrod, S. (2004). Toward a mecha-
nistc psychology of dialog. Behavioral and Brain Sci-
ences, 27.
Reiter, E. and Dale, R. (1997). Building applied natural
language generation systems. Natural Language En-
gineering, 3(1):57?87.
Rieser, V. and Lemon, O. (2009). Natural language gen-
eration as planning under uncertainty for spoken dia-
logue systems. In EACL ?09: Proceedings of the 12th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 683?691,
Morristown, NJ, USA.
Scott, D. and Moore, J. (2006). An NLG evaluation com-
petition? eight reasons to be cautious. Technical re-
port.
Sutton, R. S. and Barto, A. G. (1998). Reinforcement
Learning: An Introduction. MIT Press, Cambridge,
MA, USA.
Toutanova, K. and Manning, C. D. (2000). Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger. In Proceedings of the 2000 Joint
SIGDAT conference on Empirical methods in natural
language processing and very large corpora, pages
63?70, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Viethen, J. and Dale, R. (2006). Towards the evaluation
of referring expression generation. In In Proceedings
of the 4th Australiasian Language Technology Work-
shop, pages 115?122.
