Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 57?61,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Active Learning for Constrained Dirichlet Process Mixture Models
Andreas Vlachos
Computer Laboratory
University of Cambridge
av308@cl.cam.ac.uk
Zoubin Ghahramani
Department of Engineering
University of Cambridge
zoubin@eng.cam.ac.uk
Ted Briscoe
Computer Laboratory
University of Cambridge
ejb@cl.cam.ac.uk
Abstract
Recent work applied Dirichlet Process
Mixture Models to the task of verb cluster-
ing, incorporating supervision in the form
of must-links and cannot-links constraints
between instances. In this work, we intro-
duce an active learning approach for con-
straint selection employing uncertainty-
based sampling. We achieve substantial
improvements over random selection on
two datasets.
1 Introduction
Bayesian non-parametric mixture models have the
attractive property that the number of components
used to model the data is not fixed in advance but
is determined by the model and the data. This
property is particularly interesting for NLP where
many tasks are aimed at discovering novel in-
formation. Recent work has applied such mod-
els to various tasks with promising results, e.g.
Teh (2006) and Cohn et al (2009).
Vlachos et al (2009) applied the basic model
of this class, the Dirichlet Process Mixture Model
(DPMM), to lexical-semantic verb clustering with
encouraging results. The task involves discov-
ering classes of verbs similar in terms of their
syntactic-semantic properties (e.g. MOTION class
for travel, walk, run, etc.). Such classes can pro-
vide important support for other tasks, such as
word sense disambiguation, parsing and seman-
tic role labeling. (Dang, 2004; Swier and Steven-
son, 2004) Although some fixed classifications are
available these are not comprehensive and are in-
adequate for specific domains.
Furthermore, Vlachos et al (2009) used a con-
strained version of the DPMM in order to guide
clustering towards some prior intuition or consid-
erations relevant to the specific task at hand. This
supervision was modelled as pairwise constraints
between instances and it informs the model of re-
lations between them that cannot be recovered by
the model on the basis of the feature representa-
tion used. Like other forms of supervision, these
constraints require manual annotation and it is im-
portant to maximize the benefits obtained from it.
Therefore it is natural to consider active learning
(Settles, 2009) in order to focus the supervision on
clusterings on which the model is uncertain.
In this work, we propose a simple yet effec-
tive active learning method employing uncertainty
based sampling. The effectiveness of the AL
method is demonstrated on two datasets, one of
which has multiple gold standards.
2 Constrained DPMMs for clustering
In DPMMs, the parameters of each component are
generated by a Dirichlet Process (DP) which can
be seen as a distribution over distributions. Each
instance, represented by its features, is generated
by the component it is assigned to. The compo-
nents discovered correspond to the clusters. The
prior probability of assigning an instance to a par-
ticular component is proportionate to the number
of instances already assigned to it, in other words,
the DPMM exhibits the ?rich get richer? prop-
erty. A popular metaphor to describe the DPMM
which exhibits an equivalent clustering property
is the Chinese Restaurant Process (CRP). Cus-
tomers (instances) arrive at a Chinese restaurant
which has an infinite number of tables (compo-
nents). Each customer sits at one of the tables that
is either occupied or vacant with popular tables at-
tracting more customers.
Following Navarro et al (2006), parameter es-
timation is performed using Gibbs sampling by
sampling the assignment zi of each instance xi
given all the others z?i and the data X:
P (zi = z|z?i, X) ?
p(zi = z|z?i)P (xi|zi = z,X?i) (1)
57
In Eq. 1 p(zi = z|z?i) is the CRP prior and
P (xi|zi = z,X?i) is the distribution that gener-
ates instance xi given it has been assigned to com-
ponent z. This sampling scheme is possible be-
cause the assignments in the model are exchange-
able, i.e. their order is not relevant.
The constrained version of the DPMM uses
pairwise constraints over instances in order to
adapt the clustering discovered. Following
Wagstaff & Cardie (2000), a pair of instances is
either linked together (must-link) or not (cannot-
link). For example, charge and run should form a
must-link if the aim is to cluster MOTION verbs
together, but they should form a cannot-link if we
are interested in BILL verbs. All links are as-
sumed to be consistent with each other. In order
to incorporate the constraints in the DPMM, the
Gibbs sampling scheme is modified so that must-
linked instances are generated by the same compo-
nent and cannot-linked instances always by differ-
ent ones. Following Vlachos et al (2009), for each
instance that does not belong to a linked-group, the
sampler is restricted to choose components that do
not contain instances cannot-linked with it. For
instances in a linked-group, their assignment is
sampled jointly, again taking into account their
cannot-links. This is performed by adding each
instance of the linked-group successively to the
same component. In terms of the CRP metaphor,
customers connected with must-links arrive at the
restaurant and choose a table jointly, respecting
their cannot-links with other customers.
3 Active Constraint Selection
In active learning, the model selects the supervi-
sion to be provided by a human expert. In the con-
text of the DPMMs, the model chooses a pair of
instances for which a must-link or a cannot-link
must be provided. To select the pair, we employ
the simple but effective idea of uncertainty based
sampling. We consider the most informative link
as that on which the model is most uncertain, more
formally the link between instances l?ij that maxi-
mizes the following entropy:
l?ij = argmaxi,j
H(zi = zj) (2)
If we consider clustering as binary classification of
links into must-links and cannot-links, it is equiv-
alent to selecting the pair with the highest label
entropy. During the sampling process used for
parameter inference, component assignments vary
between samples and the components themselves
are not identifiable, i.e. one cannot match the com-
ponents of one sample with those of another. Fur-
thermore, the conditional assignments estimated
during Gibbs sampling (Eq. 1) they do not capture
the uncertainty of the assignments z?i on which
they condition. Therefore, we resort to generating
a set of samples from the (possibly constrained)
DPMM and pick the link on which these sam-
ples maximally disagree, i.e. we approximate the
distribution in Eq. 2 with the probability that in-
stances i, j are in the same cluster or not. Thus,
in a given set of samples the most uncertain link
would be the one between two instances which are
in the same cluster in exactly half of these sam-
ples. Using multiple samples allows us to take into
account the uncertainty in the assignments of the
other instances, as well as the varying number of
components.
Compared to standard pool-based AL, when
clustering with constraints the possible links be-
tween two instances (ignoring transitivity) are
C(N, 2) = N(N ? 1)/2 (N is the size of the
dataset) and there is an equal number of candi-
date queries to be considered, as opposed to N
queries in a supervised classification task. Another
interesting difference is that the the AL process
can be initiated without any supervision, since the
DPMM is unsupervised. On the other hand, in
the standard AL scenario a (usually small) labelled
seed set is used. Therefore, we rely exclusively on
the model and the features to guide the constraint
selection process. If the model combined with the
features is not appropriate for the task then the
constraints chosen are unlikely to be useful.
4 Datasets and Evaluation
In our experiments we used two verb clustering
datasets, one from general English (Sun et al,
2008) and one from the biomedical domain (Ko-
rhonen et al, 2006). In both datasets the fea-
tures for each verb are its subcategorization frames
(SCFs) which capture the syntactic context in
which it occurs. They were acquired automati-
cally using a domain-independent statistical pars-
ing toolkit, RASP (Briscoe and Carroll, 2002), and
a classifier which identifies verbal SCFs. As a
consequence, they include some noise due to stan-
dard text processing and parsing errors and due to
the subtlety of the argument-adjunct distinction.
The general English dataset contains 204 verbs
58
belonging to 17 fine-grained classes in Levin?s
(Levin, 1993) taxonomy so that each class con-
tains 12 verbs. The biomedical dataset consists of
193 medium to high frequency verbs from a cor-
pus of 2230 full-text articles from 3 biomedical
journals. A team of linguists and biologists cre-
ated a three-level gold standard with 16, 34 and
50 classes. Both datasets were pre-processed us-
ing non-negative matrix factorization (Lin, 2007)
which decomposes a large sparse matrix into two
dense matrices (of lower dimensionality) with
non-negative values. In all experiments 35 dimen-
sions were kept. Preliminary experiments with
different number of dimensions kept did not affect
the performance substantially.
We evaluate our results using three informa-
tion theoretic measures: Variation of Informa-
tion (Meila?, 2007), V-measure (Rosenberg and
Hirschberg, 2007) and V-beta (Vlachos et al,
2009). All three assess the two desirable proper-
ties that a clustering should have with respect to
a gold standard, homogeneity and completeness.
Homogeneity reflects the degree to which each
cluster contains instances from a single class and
is defined as the conditional entropy of the class
distribution of the gold standard given the clus-
tering. Completeness reflects the degree to which
each class is contained in a single cluster and is de-
fined as the conditional entropy of clustering given
the class distribution in the gold standard. V-beta
balances these properties explicitly by taking into
account the ratio of the number of cluster discov-
ered over the number of classes in the gold stan-
dard. While an ideal clustering should have both
properties, naively improving one of them can be
harmful for the other. Compared to the more com-
monly used F-measure (Fung et al, 2003), these
measures have the advantage that they do not as-
sume a mapping between clusters and classes.
5 Experiments
We performed experiments in order to assess the
effectiveness of the AL algorithm for the con-
strained DPMM comparing it to random selection.
In each AL round, we run the Gibbs sampler for
the (constrained) DPMM five times, using 100 it-
erations for burn-in, draw 20 samples from each
run with 5 iterations lag between samples and se-
lect the most uncertain link to be labeled. Fol-
lowing Navarro et al (2006), the concentration
parameter is inferred from the data using Gibbs
sampling. The performances were averaged across
the collected samples. Random selection was re-
peated three times. The three levels of the biomed-
ical gold standard were used independently and to-
gether with the general English dataset result in
four experimental setups.
The comparison between AL and random se-
lection for each dataset is shown in graphs 1(a)-
1(d) using V-beta, noting that the observations
made hold with all evaluation metrics used. Con-
straints selected via AL improve the performance
rapidly. Indicatively, the performance reached us-
ing 1000 randomly chosen constraints is obtained
using only 110 actively selected ones in the bio-50
dataset. AL performance levels out in later stages
with performance superior to the one achieved us-
ing random selection with the same number of
constraints. The poor performance of random se-
lection is expected, since the unsupervised DPMM
predicts more than 90% of the binary links cor-
rectly. Another interesting observation is that, dur-
ing AL, homogeneity increased faster than com-
pleteness (graphs 1(g) and 1(h)). This suggests
that the features used lead the model towards finer-
grained clusters, which is further confirmed by
the fact that the highest scores on the biomedical
dataset are achieved when comparing against the
finest-grained version of the gold standard. While
it is possible to choose constraints to the model
that would increase completeness with respect to
the gold standard, we argue that this would not al-
low us to obtain obtain insights on the model and
the features used.
We also noticed that the choice of batch size
has a significant effect on the learning rate of the
model. This phenomenon occurs in varying de-
grees in many applications of AL. Manual inspec-
tion of the links chosen at each round revealed that
batches often contained links involving the same
instances. This is expected due to transitivity: if
the link between instances A and B is uncertain
but the link between instances B and C is certain,
then the link between A and C will be uncertain
too. While reducing the batch size leads to bet-
ter learning rates, it requires estimating the model
more often. In order to ameliorate this issue, af-
ter obtaining the label of the most uncertain link,
we remove the samples that disagreed with it and
re-calculate the uncertainty of the remaining links
given the remaining samples. This is repeated un-
til the intended batch size is reached. Thus, we
59
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250
V-
be
ta
links
active
random
(a) bio-16
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250
V-
be
ta
links
active
random
(b) bio-34
 0.72
 0.76
 0.8
 0.84
 0.88
 0  50  100  150  200  250
V-
be
ta
links
active
random
(c) bio-50
 0.55
 0.6
 0.65
 0.7
 0.75
 0  50  100  150  200  250
V-
be
ta
links
active
random
(d) gen. English
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250
V-
be
ta
links
active10
batch
(e) bio-16
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250
V-
be
ta
links
active10
batch
(f) bio-34
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250
V-
be
ta
links
hom
comp
(g) bio-50
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0  50  100  150  200  250
V-
be
ta
links
hom
comp
(h) gen. English
Figure 1: (a)-(d): Constrained DPMM learning curves comparing random selection and AL. (e),(f):
Batch selection comparison. (g),(h): Homogeneity and completeness curves during AL.
avoid selecting links involving the same instance,
unless their uncertainty was not reduced by the
constraints added. A consideration that arises is
that by reducing the number of samples used for
uncertainty estimation, progressively we are left
with fewer samples to rank the remaining links.
Each labeled link reduces the number of samples
approximately by half since the most uncertain
link is likely to be a must-link in half the sam-
ples and a cannot-lnk in the remaining half. As
a result, for a batch with size |B| the uncertainty
of the last link will be estimated using |S|/2|B|?1
samples. A crude solution would be to generate
enough samples for the desired batch size. How-
ever, obtaining a very large number of samples can
be computationally expensive. Therefore, we set a
threshold for the minimum number of samples to
be used to estimate the link uncertainty and when
it is reached, more samples are generated using the
constraints selected. In graphs 1(e) and 1(f) we
demonstrate the effectiveness of the batch selec-
tion method proposed (labeled ?batch?) compared
to naive batch selection (labeled ?active10?).
6 Discussion and Future Work
We presented an AL method for constrained DP-
MMs employing uncertainty based sampling. We
applied it to two different verb clustering datasets
with 4 gold standards in total and obtained very
good results compared to random selection. The
idea, while explored in the context of verb cluster-
ing with the constrained DPMM, is likely to be ap-
plicable to other models that can incorporate must-
links and cannot-links in MCMC sampling.
Most literature on AL for NLP considers super-
vised methods for classification or sequential tag-
ging. However, AL for clustering is a relatively
under-explored area. Klein et al (2002) incorpo-
rated actively selected constraints in hierarchical
agglomerative clustering. Basu et al (2006) have
applied AL to obtain must-links and cannot-links
however, the clustering framework used requires
the number of clusters to be known in advance
which restricts counter-intuitively the clustering
solutions that are discovered. Moreover, semi-
supervised clustering is a form of semi-supervised
learning and in this light, our approach is related
to the work of Zhu et al (2003).
With respect to the practical application of the
AL method suggested, it is worth noting that in all
our experiments the constraints were obtained for
the respective gold standard of the dataset at ques-
tion and consequently they are all consistent with
each other. However, this assumption might not
hold in case human experts are employed for the
same purpose. In order to use such feedback in the
framework suggested, it is necessary to filter the
constraints provided in order to obtain a consistent
subset. To this end, it would be interesting to in-
vestigate the potential of using ?soft? constraints,
i.e. constraints that are provided with relative con-
fidence.
60
References
Sugato Basu, Mikhail Bilenko, Arindam Banerjee,
and Raymond J. Mooney. 2006. Probabilis-
tic semi-supervised clustering with constraints. In
O. Chapelle, B. Schoelkopf, and A. Zien, edi-
tors, Semi-Supervised Learning, pages 73?102. MIT
Press.
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of the 3rd International Conference on Language
Resources and Evaluation, pages 1499?1504.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 548?556.
Hoa Trang Dang. 2004. Investigations into the role
of lexical semantics in word sense disambiguation.
Ph.D. thesis, University of Pennsylvania, Philadel-
phia, PA, USA.
Benjamin C. M. Fung, Ke Wang, and Martin Ester.
2003. Hierarchical document clustering using fre-
quent itemsets. In Proceedings of SIAM Interna-
tional Conference on Data Mining, pages 59?70.
Dan Klein, Sepandar D. Kamvar, and Christopher D.
Manning. 2002. From instance-level constraints to
space-level constraints: Making the most of prior
knowledge in data clustering. In Proceedings of the
Nineteenth International Conference on Machine
Learning, pages 307?314.
Anna Korhonen, Yuval Krymolowski, and Nigel Col-
lier. 2006. Automatic classification of verbs in
biomedical texts. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 345?352.
Beth Levin. 1993. English Verb Classes and Alter-
nations: a preliminary investigation. University of
Chicago Press, Chicago.
Chih-Jen Lin. 2007. Projected gradient methods for
nonnegative matrix factorization. Neural Compua-
tion, 19(10):2756?2779.
Marina Meila?. 2007. Comparing clusterings?an in-
formation based distance. Journal of Multivariate
Analysis, 98(5):873?895.
Daniel J. Navarro, Thomas L. Griffiths, Mark Steyvers,
and Michael D. Lee. 2006. Modeling individual dif-
ferences using Dirichlet processes. Journal of Math-
ematical Psychology, 50(2):101?122, April.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 410?420.
Burr Settles. 2009. Active learning literature survey.
Computer Sciences Technical Report 1648, Univer-
sity of Wisconsin?Madison.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
2008. Verb class discovery from rich syntactic data.
In Proceedings of the 9th International Conference
on Intelligent Text Processing and Computational
Linguistics.
Robert S. Swier and Suzanne Stevenson. 2004. Unsu-
pervised semantic role labelling. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 95?102.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 985?992, Sydney, Australia, July.
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. 2009. Unsupervised and Constrained
Dirichlet Process Mixture Models for Verb Cluster-
ing. In Proceedings of the EACL workshop on GEo-
metrical Models of Natural Language Semantics.
Kiri Wagstaff and Claire Cardie. 2000. Clustering
with instance-level constraints. In Proceedings of
the Seventeenth International Conference on Ma-
chine Learning, pages 1103?1110.
Xiaojin Zhu, John Lafferty, and Zoubin Ghahramani.
2003. Combining Active Learning and Semi-
Supervised Learning Using Gaussian Fields and
Harmonic Functions. In ICML workshop on The
Continuum from Labeled to Unlabeled Data in Ma-
chine Learning and Data Mining, pages 58?65.
61
