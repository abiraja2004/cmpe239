2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 641?645,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
On The Feasibility of Open Domain Referring Expression
Generation Using Large Scale Folksonomies
Fabia?n Pacheco Pablo Ariel Duboue?
Facultad de Matema?tica, Astronom??a y F??sica
Universidad Nacional de Co?rdoba
Co?rdoba, Argentina
Mart??n Ariel Dom??nguez
Abstract
Generating referring expressions has received
considerable attention in Natural Language
Generation. In recent years we start seeing
deployments of referring expression genera-
tors moving away from limited domains with
custom-made ontologies. In this work, we ex-
plore the feasibility of using large scale noisy
ontologies (folksonomies) for open domain
referring expression generation, an important
task for summarization by re-generation. Our
experiments on a fully annotated anaphora
resolution training set and a larger, volunteer-
submitted news corpus show that existing al-
gorithms are efficient enough to deal with
large scale ontologies but need to be extended
to deal with undefined values and some mea-
sure for information salience.
1 Introduction
Given an entity1 (the referent) and a set of com-
peting entities (the set of distractors), the task of
referring expression generation (REG) involves cre-
ating a mention to the referent so that, in the eyes
of the reader, it is clearly distinguishable from any
other entity in the set of distractors. In a traditional
generation pipeline, referring expression generation
happens at the sentence planning level. As a result,
its output is not a textual nugget but a description
employed later on by the surface realizer. In this pa-
per, we consider the output of the REG system to
?To whom correspondence should be addressed. Email:
pablo.duboue@gmail.com.
1Or set of entities, but not in this work.
be Definite Descriptions (DD) consisting of a set of
positive triples and a set of negative triples, enumer-
ating referent-related properties.
Since the seminal work by Dale and Re-
iter (1995), REG has received a lot of attention in the
Natural Language Generation (NLG) community.
However, most of the early work on REG has been
on traditional NLG systems, using custom-tailored
ontologies. In recent years (Belz et al, 2010) there
has been a shift towards what we term ?Open Do-
main Referring Expression Generation,? (OD REG),
that is, a REG task where the properties come from
a folksonomy, a large-scale volunteer-built ontology.
In particular, we are interested in changing
anaphoric references for entities appearing in sen-
tences drafted from different documents, as done
in multi-document summarization (Advaith et al,
2011). For example, consider the following sum-
mary excerpt2 as produced by Newsblaster (McKe-
own et al, 2002):
Thousands of cheering, flag-waving Palestinians gave
Palestinian Authority President Mahmoud Abbas an en-
thusiastic welcome in Ramallah on Sunday, as he told
them triumphantly that a ?Palestinian spring? had been
born following his speech to the United Nations last
week.3 The president pressed Israel, in unusually frank
terms, to reach a final peace agreement with the Pales-
tinians, citing the boundaries in place on the eve of the
June 1967 Arab-Israeli War as the starting point for ne-
2From http://newsblaster.cs.columbia.edu/archives/2011-10-
07-04-51-35/web/summaries/ 2011-10-07-04-51-35-011.html.
3After his stint at UN, Abbas is politically stronger than ever
(haaretz.com, 10/07/2011, 763 words).
641
gotiation about borders.4
Here the second sentence refers to U.S. presi-
dent Barack Obama and a referring expression of the
form ?U.S. president? should have been used. Such
expressions depend on the set of distractors present
in the text, a requirement that highlights the dynamic
nature of the problem. Our experiments extracted
thousands of complex cases (such as distinguishing
one musician from a set of five) which we used to
test existing algorithms against a folksonomy, dbPe-
dia5 (Bizer et al, 2009). This folksonomy contains
1.7M triples (for its English version) and has been
curated from Wikipedia.6
We performed two experiments: first we em-
ployed sets of distractors derived from a set of docu-
ments annotated with anaphora resolution informa-
tion (Hasler et al, 2006). We found that roughly
half of the entities annotated in the documents were
present in the folksonomy, which speaks of the feasi-
bility of using a folksonomy for OD REG, given the
fact that Wikipedia has strict notability requirements
for adding information. In the second experiment,
we obtained sets of distractors from Wikinews,7 a
service where volunteers submit news articles inter-
spersed with Wikipedia links. We leveraged said
links to assemble 40k referring expression tasks.
For algorithms, we employed Dale and Re-
iter (1995), Gardent (2002) and Full Brevity (FB)
(Bohnet, 2007). Our results show that the first two
algorithms produce results in a majority of the re-
ferring expression tasks, with the Dale and Reiter
algorithm being the most efficient and resilient of
the three. The results, however, are of mixed quality
and more research is needed to overcome two prob-
lems we have identified in our experiments: dealing
with undefined information in the folksonomy and
the need to incorporate a rough user model in the
form of information salience.
In the next section we briefly summarize the three
algorithms we employed in our experiments. In Sec-
tion 3, we describe the data employed. Section 4
contains the results of our experiments and subse-
quent analysis. We conclude discussing future work.
4Obama prods Mideast allies to embrace reform, make
peace (Washington Post, 10/07/2011, 371 words).
5http://dbpedia.org
6http://wikipedia.org
7http://wikinews.org
2 Referring Expression Generation (REG)
REG literature is vast and spans decades of work.
We picked three algorithms with the following
desiderata: all the algorithms can deal with single
entity referents (a significant amount of recent work
went into multi-entity referents) and we wanted to
showcase a classic algorithm (Dale and Reiter?s), an
algorithm generating negations (Gardent?s) and an
algorithm with a more exhaustive search of the solu-
tions space (Full Brevity). We very briefly describe
each of the algorithms in turn, where R is the refer-
ent, C is the set of distractors and P is a list of prop-
erties, triples in the form (entity, property, value),
describing R:
Dale and Reiter (1995). They assume the prop-
erties in P are ordered according to an established
criteria. Then the algorithm iterates over P , adding
each triple one at a time and removing from C all
entities ruled out by the new triple. Triples that do
not eliminate any new entities from C are ignored.
The algorithm terminates when C is empty.
Gardent (2002). The algorithm uses Constraint
Satisfaction Programming to solve two basic con-
straints: find a set of positive properties P+ and neg-
ative properties P?, such that all properties in P+
are true for the referent and all in P? are false, and
it is the smaller P+ ? P? such that for every c ? C
there exist a property in P+ that does not hold for c
or a property in P? that holds for c.8
Full Brevity (Bohnet, 2007). Starting from a
state E of the form (L,C, P ) with L = ? (selected
properties), it keeps these states into a queue, where
it loops until C = ?. In each loop it generates new
states (added to the end of the queue), as follows:
given a state E = (L,C, P ) for each p ? P , if p re-
moves elements rem from C, it adds (L? {p}, C ?
rem, P ? {p}), otherwise (L,C, P ? {p}).
3 Data
dbPedia. dbPedia (Bizer et al, 2009) is
an ontology curated from Wikipedia infoboxes,
small tables containing structured information at
the top of most Wikipedia pages. The ver-
sion employed in this paper (?Ontology Infobox
Properties?) contains 1,7520,158 triples. Each
8We employed the Choco CSP solver Java library:
http://www.emn.fr/z-info/choco-solver/.
642
Former [[New Mexico]] {{w|Governor of New
Mexico|governor}} {{w|Gary Johnson}} ended
his campaign for the {{w|Republican Party
(United States)|Republican Party}} (GOP)
presidential nomination to seek the backing
of the {{w|Libertarian Party (United
States)|Libertarian Party}} (LP).
Figure 1: Wikinews example, from http://en.wikinews.org
/wiki/U.S. presidential candidate Gary Johnson leaves GOP to vie for
the LP nom
entity is represented by a URI starting with
http://dbpedia.org/resource/ followed by
the name of its associated Wikipedia title. See the
next section for some example triples.
Pilot. While creating unambiguous descriptions
is the NLG task known as referring expression gen-
eration, its NLU counterpart is anaphora resolu-
tion. We took a hand-annotated corpus for training
anaphora resolution algorithms (Hasler et al, 2006)
consisting of 74 documents containing 239 corefer-
ence chains. Each of the chains is an entity that can
be used for our experiments, if the entity is in db-
Pedia and there are other suitable distractors in the
same document. We hand annotated each of those
239 coreference chains by type (person, organiza-
tion and location) and associated them to dbPedia
URIs for the ones we found on Wikipedia. We found
roughly half of the chains in dbPedia (106 out of
239, 44%). This percentage speaks of the coverage
of dbPedia for OD REG. However, only 16 docu-
ments contain multiple entities of the same type and
present in dbPedia, our pilot study criteria. These 16
documents result in the 16 tasks for our pilot. For a
large scale evaluation we turned to Wikinews.
Wikinews. Wikinews is a news service operated
as a wiki. As the news articles are interspersed
with interwiki links, multiple entities can be disam-
biguated as Wikipedia pages (which in turn are db-
Pedia URIs). For example, in Figure 1, both the Lib-
ertarian Party and Republican Party can be consid-
ered potential distractors, as both are organizations.
The Wikimedia Foundation makes a database
dump available for all Wikinews interwiki links (the
links in braces in the above example). If a page con-
tains more than one organization or person, we ex-
tracted the whole set of people (or organizations) as
a referring expression task. To see whether a URI
is a person or an organization we check for a birth
date or creation date, respectively. In this manner,
we obtained 4,230 tasks for people and 12,998 for
organizations. This is dataset is freely available.9
4 Results
Pilot. The 16 tasks were split into 40 runs (a task
spans n runs each, where n is the number of entities
in the task, by rotating through the different alterna-
tive pairs of referent / set of distractors). From these
tasks, Dale and Reiter produced no output 12 times
and FB Brevity was unable to produce a result in 23
times. Gardent produced output for every run. We
consider this an example of the increased expressive
power of negative descriptions (it included a nega-
tion in 25% of the runs). For the other two algo-
rithms, the lack of an unique triple differentiating
one entity from the set of distractors seemed to be
the main issue but there were multiple cases were FB
ran out of memory for its queue of candidate nodes.
With respect to execution timings, Dale and Re-
iter ran into some corner cases and took time com-
parable to Gardent?s algorithm. FB was 16 times
slower (we found this counter-intuitive, as Gardent?s
algorithm is more demanding). Therefore, two of
these algorithms were able to produce results using
large scale ontological information. As FB ran into
problems both in terms of execution time and failure
rates, we omitted it from the large scale experiments.
We adjusted the parameters for the algorithms on
this set to obtain the best possible quality output
given the data and the problem. As such, we do not
report quality assessments on the pilot data.
Wikinews. The tasks obtained from wikinews
contained a large number of entities per task (an av-
erage of 12 people per task) and therefore span a
large number of runs: 17,814 runs for people (from
4,230 tasks) and 44,080 for organizations (from
12,998 tasks).
On these large runs, execution time differences
are in line with our a priori expectations: the greedy
approach of Dale and Reiter is very fast10 with Gar-
dent?s more comprehensive search taking about 40
times more time. Dale and Reiter failure rate was
9
http://www.cs.famaf.unc.edu.ar/?pduboue/data/ also mirrored
at http://duboue.ca/data.
10Dale and Reiter takes less than 3? for the 44,080 runs for
organizations in a 2.3 GHz machine.
643
Referent Dale and Reiter Output Gardent Output
EB { (EB occupation Software Freedom Law Center) } { (EB occupation Software Freedom Law Center) }
LL { (LL birthPlace United States), (LL, occupation Harvard Law School) } { (LL birthPlace Rapid City, South Dakota) }
LT { (LT occupation Software engineer) } { (LT nationality Finnish American) }
Figure 2: Example output for the task: {?Eben Moglen? (EB), ?Lawrence Lessig? (LL), ?Linus Torvalds? (LT) }.
comparable or better than in the pilot (for organiza-
tions that are more mixed, it was slightly lower but
for people it was as low 2.8%). Gardent missed 2%
of the people (and only 54 organizations), employ-
ing negatives 14% of the time for people and 12% of
the time for organizations.
Evaluating referring expressions is hard. Efforts
to automate this task in NLG (Gatt et al, 2007)
have taken an approach similar to machine transla-
tion BLEU scores (Papinini et al, 2001), for exam-
ple, by asking multiple judges to produce referring
expressions for a given scenario. These settings usu-
ally involve images of physical objects and relate to
small ontologies. While such an approach could be
adapted to the Open Domain case, a major problem
is the need for the judges to be acquainted with some
of the less popular entities in the training set. At
this point in our research, we decided to analyze the
quality of a sample of the output ourselves. This
process involved consulting information about each
entity to determine the soundness of the result.
We looked at a random sample of 20 runs and an-
notated it by two authors, measuring a Cohen?s ? of
60% for annotating DD results and 79% for deter-
mining whether the folksonomy had enough infor-
mation to build a satisfactory DD. We then extended
the evaluation to 60 runs and annotated them by one
author. We found that Dale and Reiter produced a
satisfactory DD in 41.6% of the cases and Gardent
in 43.4% of the cases and that the folksonomy con-
tained enough information 81.6% of the time. Fig-
ure 2 shows some example output.
From the evaluation we learned that the default
ordering strategy employed by Dale and Reiter is
not stable across different types of people (compare:
politicians vs. musicians) or organizations. We also
saw that Gardent?s algorithm in many cases selected
a single triple with very little practical value (an ob-
scure fact about the entity) or a negative piece of in-
formation which is actually true for the referent but
it is a missing piece of information.
The first two problems can be solved by either fur-
ther subdividing the taxonomies of entities or (more
interestingly) by incorporating some measure about
the salience of each piece of information, a possibil-
ity which we will discuss next. The last issue can be
addressed by having some form of meaningful de-
fault value.
The negations produced by Gardent?s algorithm
highlighted errors on the folksonomy. For example,
when referring to China with distractors Peru and
Taiwan, it will produce ?the place where they do not
speak Chinese,? as China has the different Chinese
dialects spelled out on the folksonomy (and some
Peruvians do speak Chinese). Given these limita-
tions, we find the current results very encouraging
and we believe folksonomies can help focus on ro-
bust NLG for noisy (ontological) inputs.
5 Discussion
We have shown that by using a folksonomy it should
be possible to deploy traditional NLG referring ex-
pression generation algorithms in Open Domain
tasks. To fulfill this vision, three tasks remain:
Dealing with missing information. Some form of
smart default values are needed, we are considering
using a nearest-neighbor approach to find ontologi-
cal siblings which can provide such defaults.
Estimating salience of each piece of ontological
information. The importance for each triple has to
be obtained in a way consistent with the Open Do-
main nature of the task. For this problem, we believe
search engine salience can be of great help.
Transform the extracted triples into actual text.
This problem has received attention in the past. We
would like to explore traditional surface realizer
with a custom-made grammar.
Acknowledgments
We would like to thank the anonymous reviewers as
well as Annie Ying and Victoria Reggiardo.
644
References
Siddharthan Advaith, Nenkova Ani, and McKeown Kath-
leen. 2011. Information status distinctions and refer-
ring expressions: An empirical study of references to
people in news summaries. Computational Linguis-
tics, 37(4):811?842.
Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt.
2010. Generating referring expressions in context:
The grec task evaluation challenges. In Emiel Krah-
mer and Marit Theune, editors, Empirical Methods
in Natural Language Generation, volume 5790 of
Lecture Notes in Computer Science, pages 294?327.
Springer.
C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker,
R. Cyganiak, and S. Hellmann. 2009. DBpedia-a
crystallization point for the web of data. Web Seman-
tics: Science, Services and Agents on the World Wide
Web, 7(3):154?165.
B. Bohnet. 2007. is-fbn, is-fbs, is-iac: The adaptation
of two classic algorithms for the generation of refer-
ring expressions in order to produce expressions like
humans do. MT Summit XI, UCNLG+ MT, pages 84?
86.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the gricean maxims in the generation of refer-
ring expressions. Cognitive Science, 19(2):233?263.
C. Gardent. 2002. Generating minimal definite descrip-
tions. In Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics, pages 96?
103. Association for Computational Linguistics.
A. Gatt, I. Van Der Sluis, and K. Van Deemter. 2007.
Evaluating algorithms for the generation of referring
expressions using a balanced corpus. In Proceedings
of the Eleventh European Workshop on Natural Lan-
guage Generation, pages 49?56. Association for Com-
putational Linguistics.
L. Hasler, C. Orasan, and K. Naumann. 2006. NPs
for events: Experiments in coreference annotation. In
Proceedings of the 5th edition of the International
Conference on Language Resources and Evaluation
(LREC2006), pages 1167?1172.
Kathleen R. McKeown, R. Barzilay, D. Evans, V. Hatzi-
vassiloglou, J. L. Klavans, A. Nenkova, C. Sable,
B. Schiffman, and S. Sigelman. 2002. Tracking and
summarizing news on a daily basis with columbia?s
newsblaster. In Proc. of HLT.
Kishore Papinini, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. Technical report, IBM.
645
