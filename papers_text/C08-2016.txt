Coling 2008: Companion volume ? Posters and Demonstrations, pages 63?66
Manchester, August 2008
Exact Inference for Multi-label Classification using Sparse Graphical
Models
Yusuke Miyao? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Japan
?School of Computer Science, University of Manchester, UK
?National Center for Text Mining, UK
{yusuke,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper describes a parameter estima-
tion method for multi-label classification
that does not rely on approximate infer-
ence. It is known that multi-label clas-
sification involving label correlation fea-
tures is intractable, because the graphi-
cal model for this problem is a complete
graph. Our solution is to exploit the spar-
sity of features, and express a model struc-
ture for each object by using a sparse
graph. We can thereby apply the junc-
tion tree algorithm, allowing for efficient
exact inference on sparse graphs. Exper-
iments on three data sets for text catego-
rization demonstrated that our method in-
creases the accuracy for text categorization
with a reasonable cost.
1 Introduction
This paper describes an exact inference method
for multi-label classification (Schapire and Singer,
2000; Ghamrawi and McCallum, 2005), into
which label correlation features are incorporated.
In general, directly solving this problem is compu-
tationally intractable, because the graphical model
for this problem is a complete graph. Neverthe-
less, an important characteristic of this problem,
in particular for text categorization, is that only a
limited number of features are active; i.e., non-
zero, for a given object x. This sparsity of fea-
tures is a desirable characteristic, because we can
remove the edges of the graphical model when no
corresponding features are active. We can there-
fore expect that a graphical model for each object
is a sparse graph. When a graph is sparse, we
can apply the junction tree algorithm (Cowell et
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
al., 1999), allowing for efficient exact inference on
sparse graphs.
Our method is evaluated on three data sets for
text categorization; one is from clinical texts, and
the others are from newswire articles. We ob-
serve the trade-off between accuracy and training
cost, while changing the number of label correla-
tion features to be included.
2 Multi-label Classification
Given a set of labels, L = {l
1
, . . . , l
|L|
}, multi-
label classification is the task of assigning a sub-
set y ? L to a document x. In the framework of
statistical machine learning, this problem can be
formulated as a problem of maximizing a scoring
function ?:
y? = argmax
y
?(x, y) = argmax
y
?(f(x, y)). (1)
As is usually the case in statistical machine
learning, we represent a probabilistic event,
?x, y?, with a feature vector, f(x, y) =
?f
1
(x, y), . . . , f
|f |
(x, y)?. In text categorization,
most effective features represent a frequency of a
word w in a document; i.e.,
f
l,w
(x, y) =
{
c
x
(w) if l ? y,
0 otherwise,
where c
x
(w) is a frequency of w in x.
The most popular method for multi-label classi-
fication is to create |L| binary classifiers, each of
which determines whether or not to assign a single
label (Yang and Pedersen, 1997). However, since
the decision for each label is independent of the de-
cision for other labels, this method cannot be sen-
sitive to label correlations, or the tendency of label
cooccurrences.
A recent research effort has been devoted to
the modeling of label correlations. While a num-
ber of approaches have been proposed for deal-
ing with label correlations (see Tsoumakas and
63
Katakis (2007) for the comprehensive survey), the
intuitively-appealing method is to incorporate fea-
tures on two labels into the model (Ghamrawi and
McCallum, 2005). The following label correlation
feature indicates a cooccurrence of two labels and
a word:
f
l,l
?
,w
(x, y) =
{
c
x
(w) if l, l? ? y,
0 otherwise.
3 A Method for Exact Inference
A critical difficulty encountered in the model with
label correlation features is the computational cost
for training and decoding. When features on every
pair of labels are included in the model, its graph-
ical model becomes a complete graph, which in-
dicates that the exact inference for this model is
NP-hard. However, not all edges are necessary
in actual inference, because of the sparsity of fea-
tures. That is, we can remove edges between l and
l
? when no corresponding features are active; i.e.,
f
l,l
?
,w
(x, y) = 0 for all w. In text categorization,
when feature selection is performed, many edges
can be removed because of this characteristic.
Therefore, our idea is to enjoy this sparsity of
features. We construct a graphical model for each
document, and put edges only when one or more
features are active on the corresponding label pair.
When a graph is sparse, we can apply a method
for exact inference, such as the junction tree al-
gorithm (Cowell et al, 1999). The junction tree
algorithm is a generic algorithm for exact infer-
ence on any graphical model, and it allows for ef-
ficient inference on sparse graphs. The method
converts a graph into a junction tree, which is a
tree of cliques in the original graph. When we
have a junction tree for each document, we can
efficiently perform belief propagation in order to
compute argmax in Equation (1), or the marginal
probabilities of cliques and labels, necessary for
the parameter estimation of machine learning clas-
sifiers, including perceptrons (Collins, 2002), and
maximum entropy models (Berger et al, 1996).
The computational complexity of the inference on
junction trees is proportional to the exponential of
the tree width, which is the maximum number of
labels in a clique, minus one.
An essential idea of this method is that a graph-
ical model is constructed for each document. Even
when features are defined on all pairs of labels,
active features for a specific document are lim-
ited. When combined with feature selection, this
# train # test # labels card.
cmc2007 978 976 45 1.23
reuters10 6,490 2,545 10 1.10
reuters90 7,770 3,019 90 1.24
Table 1: Statistics of evaluation data sets
? ? c
cmc2007 1,000 10 0
reuters10 5,000 20 5
reuters90 5,000 80 5
Table 2: Parameters for evaluation data sets
method greatly increases the sparsity of the result-
ing graphs, which is key to efficiency.
A weakness of this method comes from the as-
sumption of feature sparseness. We are forced to
apply feature selection, which is considered effec-
tive in text categorization, but not necessarily for
other tasks. The design of features is also restricted
in order to ensure the sparsity of features.
4 Experiments
4.1 Experimental Settings
We evaluate our method for multi-label classifica-
tion using three data sets for text categorization.
Table 1 shows the statistics of these data. In this
table, ?card.? denotes the average number of la-
bels assigned to a document.
cmc2007 is a data set used in the Computa-
tional Medicine Center (CMC) Challenge 2007
(Pestian et al, 2007)1. This challenge aimed at
the assignment of ICD-9-CM codes, such as cough
and pneumonia, to clinical free texts. It should be
noted that this data is controlled, so that both train-
ing and test sets include the exact same label com-
binations, and the number of combinations is 90.
This indicates that this task can be solved as a clas-
sification of 90 classes. However, since this is an
unrealistic situation for actual applications, we do
not rely on this characteristic in this work.
reuters10 and reuters90 are taken from
the Reuters-21578 collection,2 which is a popu-
lar benchmark for text categorization. This text
collection consists of newswire articles, and each
document is assigned topic categories, such as
grain and ship. We split the data into training and
test sets, according to the so-called ModApte split.
1Available at http://www.computationalmedicine.org
2Available at http://www.daviddlewis.com/resources/
testcollections/reuters21578/
64
cmc2007
BPM ME
? micro-F1 sub. acc. micro-F1 sub. acc.
0 82.79 69.88 83.09 69.06
100 83.49 70.70 83.68 70.39
200 82.95 69.67 83.67 70.18
400 83.03 69.98 83.49 70.49
800 83.51 71.41 83.58 70.70
1600 83.10 70.49 83.56 71.00
3200 80.74 66.70 82.02 69.57
reuters10
BPM ME
? micro-F1 sub. acc. micro-F1 sub. acc.
0 94.23 89.71 93.71 88.76
500 94.22 89.98 93.80 89.19
1000 94.43 90.37 94.07 89.55
2000 94.46 90.61 94.04 89.94
4000 94.12 90.26 94.12 89.98
8000 94.14 90.61 94.50 90.81
16000 93.92 90.29 94.30 90.88
reuters90
BPM ME
? micro-F1 sub. acc. micro-F1 sub. acc.
0 84.07 77.91 86.83 79.50
500 84.96 78.27 86.89 79.66
1000 85.38 78.70 86.94 79.99
2000 85.73 79.79 86.55 79.93
4000 85.72 79.73 86.54 80.23
8000 85.90 80.19 86.77 80.39
16000 86.17 80.52 ? ?
Table 3: Accuracy for cmc2007, reuters10,
and reuters90
From this data, we create two data sets. The first
set, reuters10, is a subset of the ModApte split,
to which the 10 largest categories are assigned.
The other, reuters90, consists of documents
that are labeled by 90 categories, having at least
one document in each of the training and test sets.
In the following experiments, we run two ma-
chine learning classifiers: Bayes Point Machines
(BPM) (Herbrich et al, 2001), and the maximum
entropy model (ME) (Berger et al, 1996). For
BPM, we run 100 averaged perceptrons (Collins,
2002) with 10 iterations for each. For ME, the
orthant-wise quasi-Newton method (Andrew and
Gao, 2007) is applied, with the hyper parameter
for l
1
regularization fixed to 1.0.
We use word unigram features that represent the
frequency of a particular word in a target docu-
ment. We also use features that indicate the non-
existence of a word, which we found effective in
preliminary experiments; feature f
l,w?
(x, y) is 1 if
l ? y and w is not included in the document x.
Words are stemmed and number expressions are
normalized to a unique symbol. Words are not
used if they are included in the stopword list (322
cmc2007
? max. width avg. width time (sec.)
0 0 0.00 90
100 2 1.17 132
200 3 1.51 145
400 3 1.71 165
800 4 2.11 200
1600 5 2.93 427
3200 4 3.99 2280
reuters10
? max. width avg. width time (sec.)
0 0 0.00 787
500 2 1.72 1378
1000 3 2.00 1752
2000 4 2.16 2594
4000 6 2.90 7183
8000 6 4.22 21555
16000 6 5.67 116535
reuters90
? max. width avg. width time (sec.)
0 0 0.00 26172
500 5 1.74 28067
1000 6 2.24 38510
2000 6 3.22 42479
4000 8 3.68 60029
8000 14 4.56 153268
16000 17 6.39 ?
Table 4: Tree width and training time for
cmc2007, reuters10, and reuters90
words), or they occur fewer than a threshold, c, in
training data. We set c = 5 for reuters10 and
reuters90, following previous works (Gham-
rawi and McCallum, 2005), while c = 0 for
cmc2007, because the data is small.
These features are selected according to av-
eraged mutual information (information gain),
which is the most popular method in previous
works (Yang and Pedersen, 1997; Ghamrawi and
McCallum, 2005). For each label, features are
sorted according to this score, and top-ranked fea-
tures are included in the model. By preliminary
experiments, we fixed parameters, ? for word uni-
gram features and ? for non-existence features, for
each data set, as shown in Table 2.
The same method is applied to the selection of
label correlation features. In the following experi-
ments, we observe the accuracy and training time
by changing the threshold parameter ? for the se-
lection of label correlation features.
4.2 Results
Table 33 shows microaveraged F-scores (micro-
F1) and subset accuracies (sub. acc.) (Ghamrawi
and McCallum, 2005) while varying ?, the num-
3The experiment with ? = 16000 for ME was not per-
formed due to its cost (estimated time is approx. two weeks).
65
ber of label correlation features. In all data sets
and with all classifiers, the accuracy is increased
by incorporating label correlation features. The re-
sults also demonstrate that the accuracy saturates,
or even decreases, with large ?. This indicates that
the feature selection is necessary not only for ob-
taining efficiency, but also for higher accuracy.
Table 4 shows tree widths, and the time for the
training of the ME models. As shown, the graph-
ical model is represented effectively with sparse
graphs, even when the number of label correlation
features is increased. With these results, we can
conclude that our method can model label correla-
tions with a tractable cost.
The accuracy for cmc2007 is significantly bet-
ter than the results reported in Patrick et al (2007)
(micro-F1=81.1) in a similar setting, in which only
word unigram features are used. Our best result is
approaching the results of Crammer et al (2007)
(micro-F1=84.6), which exploits various linguisti-
cally motivated features. Numerous results have
been reported for reuters10, and most of them
report the microaveraged F-score around 91 to 94,
while our best result is comparable to the state-of-
the-art accuracy. For reuters90, Ghamrawi and
McCallum (2005) achieved an improvement in the
microaveraged F-score from 86.34 to 87.01, which
is comparable to our result.
5 Conclusion
This paper described a method for the exact infer-
ence for multi-label classification with label corre-
lation features. Experimental results on text cate-
gorization with the CMC challenge data and the
Reuters-21578 text collection demonstrated that
our method improves the accuracy for text cate-
gorization with a tractable cost. The availability
of exact inference enables us to apply various ma-
chine learning methods not yet investigated in this
paper, including support vector machines.
From the perspective of machine learning re-
search, feature selection methods should be recon-
sidered. While we used a feature selection method
that is widely accepted in text categorization re-
search, it has no direct connection with machine
learning models. Since feature selection methods
motivated by the optimization criteria of machine
learning models have been proposed (Riezler and
Vasserman, 2004), we expect that the integration
of our proposal with those methods will open up a
new framework for multi-label classification.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Grant-in-Aid for Young Scientists (MEXT,
Japan).
References
Andrew, G. and J. Gao. 2007. Scalable training of
l
1
-regularized log-linear models. In 24th Annual In-
ternational Conference on Machine Learning.
Berger, A. L., S. A. Della Pietra, and V. J. Della
Pietra. 1996. A maximum entropy approach to natu-
ral language processing. Computational Linguistics,
22(1):39?71.
Collins, M. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In 2002 Conference on
Empirical Methods in Natural Language Processing.
Cowell, R. G., A. P. Dawid, S. L. Lauritzen, and D. J.
Spiegelhalter. 1999. Probabilistic Networks and Ex-
pert Systems. Springer-Verlag, New York.
Crammer, K., M. Dredze, K. Ganchev, and P. P. Taluk-
dar. 2007. Automatic code assignment to medical
text. In BioNLP 2007, pages 129?136.
Ghamrawi, N. and A. McCallum. 2005. Collective
multi-label classification. In ACM 14th Conference
on Information and Knowledge Management.
Herbrich, R., T. Graepel, and C. Campbell. 2001.
Bayes point machines. Journal of Machine Learn-
ing Research, 1:245?279.
Patrick, J., Y. Zhang, and Y. Wang. 2007. Evaluat-
ing feature types for encoding clinical notes. In 10th
Conference of the Pacific Association for Computa-
tional Linguistics, pages 218?225.
Pestian, J. P., C. Brew, P. Matykiewicz, DJ Hovermale,
N. Johnson, K. B. Cohen, and W. Duch. 2007.
A shared task involving multi-label classification of
clinical free text. In BioNLP 2007, pages 97?104.
Riezler, S. and A. Vasserman. 2004. Gradient fea-
ture testing and l
1
regularization for maximum en-
tropy parsing. In 42nd Meeting of the Association
for Computational Linguistics.
Schapire, R. E. and Y. Singer. 2000. Boostexter: a
boosting-based system for text categorization. Ma-
chine Learning, 39(2/3):135?168.
Tsoumakas, G. and I. Katakis. 2007. Multi-label clas-
sification: an overview. Journal of Data Warehous-
ing and Mining, 3(3):1?13.
Yang, Y. and J. O. Pedersen. 1997. A comparative
study on feature selection in text categorization. In
14th International Conference on Machine Learn-
ing, pages 412?420.
66
