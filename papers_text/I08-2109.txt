Fast Computing Grammar-driven Convolution Tree Kernel for
Semantic Role Labeling
Wanxiang Che1?, Min Zhang2, Ai Ti Aw2, Chew Lim Tan3, Ting Liu1, Sheng Li1
1School of Computer Science and Technology
Harbin Institute of Technology, China 150001
{car,tliu}@ir.hit.edu.cn, lisheng@hit.edu.cn
2Institute for Infocomm Research
21 Heng Mui Keng Terrace, Singapore 119613
{mzhang,aaiti}@i2r.a-star.edu.sg
3School of Computing
National University of Singapore, Singapore 117543
tancl@comp.nus.edu.sg
Abstract
Grammar-driven convolution tree kernel
(GTK) has shown promising results for se-
mantic role labeling (SRL). However, the
time complexity of computing the GTK is
exponential in theory. In order to speed
up the computing process, we design two
fast grammar-driven convolution tree kernel
(FGTK) algorithms, which can compute the
GTK in polynomial time. Experimental re-
sults on the CoNLL-2005 SRL data show
that our two FGTK algorithms are much
faster than the GTK.
1 Introduction
Given a sentence, the task of semantic role labeling
(SRL) is to analyze the propositions expressed by
some target verbs or nouns and some constituents
of the sentence. In previous work, data-driven tech-
niques, including feature-based and kernel-based
learning methods, have been extensively studied for
SRL (Carreras and Ma`rquez, 2005).
Although feature-based methods are regarded as
the state-of-the-art methods and achieve much suc-
cess in SRL, kernel-based methods are more effec-
tive in capturing structured features than feature-
based methods. In the meanwhile, the syntactic
structure features hidden in a parse tree have been
suggested as an important feature for SRL and need
to be further explored in SRL (Gildea and Palmer,
2002; Punyakanok et al, 2005). Moschitti (2004)
?The work was mainly done when the author was a visiting
student at I2R
and Che et al (2006) are two reported work to use
convolution tree kernel (TK) methods (Collins and
Duffy, 2001) for SRL and has shown promising re-
sults. However, as a general learning algorithm, the
TK only carries out hard matching between two sub-
trees without considering any linguistic knowledge
in kernel design. To solve the above issue, Zhang
et al (2007) proposed a grammar-driven convolu-
tion tree kernel (GTK) for SRL. The GTK can uti-
lize more grammatical structure features via two
grammar-driven approximate matching mechanisms
over substructures and nodes. Experimental results
show that the GTK significantly outperforms the
TK (Zhang et al, 2007). Theoretically, the GTK
method is applicable to any problem that uses syn-
tax structure features and can be solved by the TK
methods, such as parsing, relation extraction, and so
on. In this paper, we use SRL as an application to
test our proposed algorithms.
Although the GTK shows promising results for
SRL, one big issue for the kernel is that it needs ex-
ponential time to compute the kernel function since
it need to explicitly list all the possible variations
of two sub-trees in kernel calculation (Zhang et al,
2007). Therefore, this method only works efficiently
on such kinds of datasets where there are not too
many optional nodes in production rule set. In order
to solve this computation issue, we propose two fast
algorithms to compute the GTK in polynomial time.
The remainder of the paper is organized as fol-
lows: Section 2 introduces the GTK. In Section 3,
we present our two fast algorithms for computing
the GTK. The experimental results are shown in Sec-
tion 4. Finally, we conclude our work in Section 5.
781
2 Grammar-driven Convolution Tree
Kernel
The GTK features with two grammar-driven ap-
proximate matching mechanisms over substructures
and nodes.
2.1 Grammar-driven Approximate Matching
Grammar-driven Approximate Substructure
Matching: the TK requires exact matching between
two phrase structures. For example, the two phrase
structures ?NP?DT JJ NN? (NP?a red car) and
?NP?DT NN? (NP?a car) are not identical, thus
they contribute nothing to the conventional kernel
although they share core syntactic structure property
and therefore should play the same semantic role
given a predicate. Zhang et al (2007) introduces
the concept of optional node to capture this phe-
nomenon. For example, in the production rule
?NP?DT [JJ] NP?, where [JJ] denotes an optional
node. Based on the concept of optional node, the
grammar-driven approximate substructure matching
mechanism is formulated as follows:
M(r1, r2) =
?
i,j
(IT (T ir1 , T jr2)? ?
ai+bj
1 ) (1)
where r1 is a production rule, representing a two-
layer sub-tree, and likewise for r2. T ir1 is the ith vari-
ation of the sub-tree r1 by removing one ore more
optional nodes, and likewise for T jr2 . IT (?, ?) is a bi-
nary function that is 1 iff the two sub-trees are iden-
tical and zero otherwise. ?1 (0 ? ?1 ? 1) is a small
penalty to penalize optional nodes. ai and bj stand
for the numbers of occurrence of removed optional
nodes in subtrees T ir1 and T jr2 , respectively.
M(r1, r2) returns the similarity (i.e., the kernel
value) between the two sub-trees r1 and r2 by sum-
ming up the similarities between all possible varia-
tions of the sub-trees.
Grammar-driven Approximate Node Match-
ing: the TK needs an exact matching between two
nodes. But, some similar POSs may represent simi-
lar roles, such as NN (dog) and NNS (dogs). Zhang
et al (2007) define some equivalent nodes that can
match each other with a small penalty ?2 (0 ? ?2 ?
1). This case is called node feature mutation. The
approximate node matching can be formulated as:
M(f1, f2) =
?
i,j
(If (f i1, f j2 )? ?ai+bj2 ) (2)
where f1 is a node feature, f i1 is the ith mutation of
f1 and ai is 0 iff f i1 and f1 are identical and 1 oth-
erwise, and likewise for f2 and bj . If (?, ?) is a func-
tion that is 1 iff the two features are identical and
zero otherwise. Eq. (2) sums over all combinations
of feature mutations as the node feature similarity.
2.2 The GTK
Given these two approximate matching mecha-
nisms, the GTK is defined by beginning with the
feature vector representation of a parse tree T as:
??(T ) = (#subtree1(T ), . . . ,#subtreen(T ))
where #subtreei(T ) is the occurrence number of
the ith sub-tree type (subtreei) in T . Now the GTKis defined as follows:
KG(T1, T2) = ???(T1),??(T2)?
=?i #subtreei(T1) ?#subtreei(T2)=?i((
?
n1?N1 I
?
subtreei(n1))
? (?n2?N2 I
?
subtreei(n2)))
=?n1?N1
?
n2?N2 ?
?(n1, n2)
(3)
where N1 and N2 are the sets of nodes in trees T1
and T2, respectively. I ?subtreei(n) is a function that
is ?a1 ??b2 iff there is a subtreei rooted at node n and
zero otherwise, where a and b are the numbers of
removed optional nodes and mutated node features,
respectively. ??(n1, n2) is the number of the com-
mon subtrees rooted at n1 and n2, i.e.,
??(n1, n2) =
?
i
I ?subtreei(n1) ? I ?subtreei(n2) (4)
??(n1, n2) can be further computed by the follow-
ing recursive rules:
R-A: if n1 and n2 are pre-terminals, then:
??(n1, n2) = ??M(f1, f2) (5)
where f1 and f2 are features of nodes n1 and n2
respectively, and M(f1, f2) is defined in Eq. (2),
which can be computed in linear time O(n), where
n is the number of feature mutations.
R-B: else if both n1 and n2 are the same non-terminals, then generate all variations of sub-trees
of depth one rooted at n1 and n2 (denoted by Tn1
782
and Tn2 respectively) by removing different optionalnodes, then:
??(n1, n2) = ??
?
i,j IT (T in1 , T jn2)? ?
ai+bj
1
??nc(n1,i)k=1 (1 + ??(ch(n1, i, k), ch(n2, j, k)))
(6)
where T in1 , T jn2 , IT (?, ?), ai and bj have been ex-
plained in Eq. (1). nc(n1, i) returns the number
of children of n1 in its ith subtree variation T in1 .
ch(n1, i, k) is the kth child of node n1 in its ith vari-
ation subtree T in1 , and likewise for ch(n2, j, k). ?
(0 < ? < 1) is the decay factor.
R-C: else ??(n1, n2) = 0
3 Fast Computation of the GTK
Clearly, directly computing Eq. (6) requires expo-
nential time, since it needs to sum up all possible
variations of the sub-trees with and without optional
nodes. For example, supposing n1 = ?A?a [b] c
[d]?, n2 = ?A?a b c?. To compute the Eq. (6), we
have to list all possible variations of n1 and n2?s sub-
trees, n1: ?A?a b c d?, ?A?a b c?, ?A?a c d?, ?A?a
c?; n2: ?A?a b c?. Unfortunately, Zhang et al
(2007) did not give any theoretical solution for the
issue of exponential computing time. In this paper,
we propose two algorithms to calculate it in polyno-
mial time. Firstly, we recast the issue of computing
Eq. (6) as a problem of finding common sub-trees
with and without optional nodes between two sub-
trees. Following this idea, we rewrite Eq. (6) as:
??(n1, n2) = ?? (1 +
lm?
p=lx
?p(cn1 , cn2)) (7)
where cn1 and cn2 are the child node sequences of
n1 and n2, ?p evaluates the number of common
sub-trees with exactly p children (at least including
all non-optional nodes) rooted at n1 and n2, lx =
max{np(cn1), np(cn2)} and np(?) is the number of
non-optional nodes, lm = min{l(cn1), l(cn2)}and
l(?) returns the number of children.
Now let?s study how to calculate ?p(cn1 , cn2) us-
ing dynamic programming algorithms. Here, we
present two dynamic programming algorithms to
compute it in polynomial time.
3.1 Fast Grammar-driven Convolution Tree
Kernel I (FGTK-I)
Our FGTK-I algorithm is motivated by the string
subsequence kernel (SSK) (Lodhi et al, 2002).
Given two child node sequences sx = cn1 andt = cn2 (x is the last child), the SSK uses the fol-lowing recursive formulas to evaluate the ?p:
??0(s, t) = 1, for all s, t,
??p(s, t) = 0, ifmin(|s|, |t|) < p, (8)
?p(s, t) = 0, ifmin(|s|, |t|) < p, (9)
??p(sx, t) = ????p(sx, t) +?
j:tj=x
(??p?1(s, t[1 : j ? 1]? ?|t|?j+2)),(10)
p = 1, . . . , n? 1,
?p(sx, t) = ?p(s, t) +?
j:tj=x
(??p?1(s, t[1 : j ? 1]? ?2)). (11)
where ??p is an auxiliary function since it is only
the interior gaps in the subsequences that are penal-
ized; ? is a decay factor only used in the SSK for
weighting each extra length unit. Lodhi et al (2002)
explained the correctness of the recursion defined
above.
Compared with the SSK kernel, the GTK has
three different features:
f1: In the GTK, only optional nodes can be
skipped while the SSK kernel allows any node skip-
ping;
f2: The GTK penalizes skipped optional nodes
only (including both interior and exterior skipped
nodes) while the SSK kernel weights the length of
subsequences (all interior skipped nodes are counted
in, but exterior nodes are ignored);
f3: The GTK needs to further calculate the num-
ber of common sub-trees rooted at each two match-
ing node pair x and t[j].
To reflect the three considerations, we modify the
SSK kernel as follows to calculate the GTK:
?0(s, t) = opt(s)? opt(t)? ?|s|+|t|1 , for all s, t, (12)
?p(s, t) = 0, ifmin(|s|, |t|) < p, (13)
?p(sx, t) = ?1 ??p(sx, t)? opt(x)
+
?
j:tj=x
(?p?1(s, t[1 : j ? 1])? ?|t|?j (14)
?opt(t[j + 1 : |t|])???(x, t[j])).
where opt(w) is a binary function, which is 0 if
non-optional nodes are found in the node sequence
w and 1 otherwise (f1); ?1 is the penalty to penalize
skipped optional nodes and the power of ?1 is the
number of skipped optional nodes (f2); ??(x, t[j])
is defined in Eq. (7) (f3). Now let us compare
783
the FGTK-I and SSK kernel algorithms. Based on
Eqs. (8), (9), (10) and (11), we introduce the opt(?)
function and the penalty ?1 into Eqs. (12), (13) and
(14), respectively. opt(?) is to ensure that in the
GTK only optional nodes are allowed to be skipped.
And only those skipped optional nodes are penal-
ized with ?1. Please note that Eqs. (10) and (11)
are merged into Eq. (14) because of the different
meaning of ? and ?1. From Eq. (8), we can see
that the current path in the recursive call will stop
and its value becomes zero once non-optional node
is skipped (when opt(w) = 0).
Let us use a sample of n1 = ?A?a [b] c [d]?, n2 =
?A?a b c? to exemplify how the FGTK-I algorithm
works. In Eq. (14)?s vocabulary, we have s = ?a [b]
c?, t = ?a b c?, x = ?[d]?, opt(x) = opt([d]) = 1,
p = 3. Then according to Eq (14), ?p(cn1 , cn2) can
be calculated recursively as Eq. (15) (Please refer to
the next page).
Finally, we have ?p(cn1 , cn2) = ?1 ???(a, a)?
??(b, b)???(c, c)
By means of the above algorithm, we can com-
pute the ??(n1, n2) in O(p|cn1 | ? |cn2 |2) (Lodhi et
al., 2002). This means that the worst case complex-
ity of the FGTK-I is O(p?3|N1| ? |N2|2), where ? is
the maximum branching factor of the two trees.
3.2 Fast Grammar-driven Convolution Tree
Kernel II (FGTK-II)
Our FGTK-II algorithm is motivated by the partial
trees (PTs) kernel (Moschitti, 2006). The PT kernel
algorithm uses the following recursive formulas to
evaluate ?p(cn1 , cn2):
?p(cn1 , cn2) =
|cn1 |?
i=1
|cn2 |?
j=1
??p(cn1 [1 : i], cn2 [1 : j]) (16)
where cn1 [1 : i] and cn2 [1 : j] are the child sub-sequences of cn1 and cn2 from 1 to i and from 1to j, respectively. Given two child node sequences
s1a = cn1 [1 : i] and s2b = cn2 [1 : j] (a and b are
the last children), the PT kernel computes ??p(?, ?) as
follows:
??p(s1a, s2b) =
{
?2??(a, b)Dp(|s1|, |s2|) if a = b
0 else (17)
where ??(a, b) is defined in Eq. (7) and Dp is recur-
sively defined as follows:
Dp(k, l) = ??p?1(s1[1 : k], s2[1 : l])
+?Dp(k, l ? 1) + ?Dp(k ? 1, l) (18)
??2Dp(k ? 1, l ? 1)
D1(k, l) = 1, for all k, l (19)
where ? used in Eqs. (17) and (18) is a factor to
penalize the length of the child sequences.
Compared with the PT kernel, the GTK has two
different features which are the same as f1 and f2
when defining the FGTK-I.
To reflect the two considerations, based on the PT
kernel algorithm, we define another fast algorithm
of computing the GTK as follows:
?p(cn1 , cn2 ) =
? |cn1 |
i=1
? |cn2 |
j=1 ??p(cn1 [1 : i], cn2 [1 : j])
?opt(cn1 [i+ 1 : |cn1 |])?opt(cn2 [j + 1 : |cn2 |])
??|cn1 |?i+|cn2 |?j1
(20)
??p(s1a, s2b) =
{ ??(a, b)Dp(|s1|, |s2|) if a = b
0 else (21)
Dp(k, l) = ??p?1(s1[1 : k], s2[1 : l])
+?1Dp(k, l ? 1)? opt(s2[l]) (22)
+?1Dp(k ? 1, l)? opt(s1[k])
??21Dp(k ? 1, l ? 1)? opt(s1[k])? opt(s2[l])
D1(k, l) = ?k+l1 ? opt(s1[1 : k])? opt(s2[1 : l]), (23)
for all k, l
??p(s1, s2) = 0, if min(|s1|, |s2|) < p (24)
where opt(w) and ?1 are the same as them in the
FGTK-I.
Now let us compare the FGTK-II and the PT al-
gorithms. Based on Eqs. (16), (18) and (19), we in-
troduce the opt(?) function and the penalty ?1 into
Eqs. (20), (22) and (23), respectively. This is to
ensure that in the GTK only optional nodes are al-
lowed to be skipped and only those skipped optional
nodes are penalized. In addition, compared with
Eq. (17), the penalty ?2 is removed in Eq. (21) in
view that our kernel only penalizes skipped nodes.
Moreover, Eq. (24) is only for fast computing. Fi-
nally, the same as the FGTK-I, in the FGTK-II the
current path in a recursive call will stop and its value
becomes zero once non-optional node is skipped
(when opt(w) = 0). Here, we still can use an ex-
ample to derivate the process of the algorithm step
by step as that for FGTK-I algorithm. Due to space
limitation, here, we do not illustrate it in detail.
By means of the above algorithms, we can com-
pute the ??(n1, n2) in O(p|cn1 | ? |cn2 |) (Moschitti,
784
?p(cn1 , cn2 ) = ?p(?a [b] c [d]? , ?a b c?)
= ?1 ??p(?a [b] c?, ?a b c?) + 0 //Since x * t, the second term is 0
= ?1 ? (0 + ?p?1(?a [b]?, ?a b?)? ?3?31 ???(c, c)) //Since opt(?c?) = 0, the first term is 0
= ?1 ???(c, c)? (0 + ?p?2(?a?, ?a b?)? ?2?21 ???(b, b)) //Since p? 1 > |?a?|,?p?2(?a?, ?a b?) = 0
= ?1 ???(c, c)? (0 + ??(a, a)???(b, b)) //?p?2(?a?, ?a?) = ??(a, a)
(15)
2006). This means that the worst complexity of the
FGTK-II is O(p?2|N1| ? |N2|). It is faster than the
FGTK-I?s O(p?3|N1| ? |N2|2) in theory. Please note
that the average ? in natural language parse trees is
very small and the overall complexity of the FGTKs
can be further reduced by avoiding the computation
of node pairs with different labels (Moschitti, 2006).
4 Experiments
4.1 Experimental Setting
Data: We use the CoNLL-2005 SRL shared task
data (Carreras and Ma`rquez, 2005) as our experi-
mental corpus.
Classifier: SVM (Vapnik, 1998) is selected as our
classifier. In the FGTKs implementation, we mod-
ified the binary Tree Kernels in SVM-Light Tool
(SVM-Light-TK) (Moschitti, 2006) to a grammar-
driven one that encodes the GTK and the two fast dy-
namic algorithms inside the well-known SVM-Light
tool (Joachims, 2002). The parameters are the same
as Zhang et al (2007).
Kernel Setup: We use Che et al (2006)?s hybrid
convolution tree kernel (the best-reported method
for kernel-based SRL) as our baseline kernel. It is
defined as Khybrid = ?Kpath + (1 ? ?)Kcs (0 ?
? ? 1)1. Here, we use the GTK to compute the
Kpath and the Kcs.
In the training data (WSJ sections 02-21), we get
4,734 production rules which appear at least 5 times.
Finally, we use 1,404 rules with optional nodes for
the approximate structure matching. For the node
approximate matching, we use the same equivalent
node sets as Zhang et al (2007).
4.2 Experimental Results
We use 30,000 instances (a subset of the entire train-
ing set) as our training set to compare the different
kernel computing algorithms 2. All experiments are
1Kpath and Kcs are two TKs to describe predicate-
argument link features and argument syntactic structure fea-
tures, respectively. For details, please refer to (Che et al, 2006).
2There are about 450,000 identification instances are ex-
tracted from training data.
conducted on a PC with CPU 2.8GH and memory
1G. Fig. 1 reports the experimental results, where
training curves (time vs. # of instances) of five
kernels are illustrated, namely the TK, the FGTK-
I, the FGTK-II, the GTK and a polynomial kernel
(only for reference). It clearly demonstrates that our
FGTKs are faster than the GTK algorithm as ex-
pected. However, the improvement seems not so
significant. This is not surprising as there are only
30.4% rules (1,404 out of 4,734)3 that have optional
nodes and most of them have only one optional
node4. Therefore, in this case, it is not time con-
suming to list all the possible sub-tree variations and
sum them up. Let us study this issue from computa-
tional complexity viewpoint. Suppose all rules have
exactly one optional node. This means each rule can
only generate two variations. Therefore computing
Eq. (6) is only 4 times (2*2) slower than the GTK
in this case. In other words, we can say that given
the constraint that there is only one optional node
in one rule, the time complexity of the GTK is also
O(|N1| ? |N2|) 5, where N1 and N2 are the numbers
of tree nodes, the same as the TK.
12000
6000
8000
10000
Train
ing T
ime (
S) GTKFGTK-I
2000
4000Tra
ining
 Time
 (S)
FGTK-IITKPoly
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
Number of Training Instances (103)
Figure 1: Training time comparison among different
kernels with rule set having less optional nodes.
Moreover, Fig 1 shows that the FGTK-II is faster
than the FGTK-I. This is reasonable since as dis-
3The percentage is even smaller if we consider all produc-
tion (it becomes 14.4% (1,404 out of 9,700)).
4There are 1.6 optional nodes in each rule averagely.
5Indeed it is O(4 ? |N1| ? |N2|). The parameter 4 is omitted
when discussing time complexity.
785
cussed in Subsection 3.2, the FGTK-I?s time com-
plexity is O(p?3|N1| ? |N2|2) while the FGTK-II?s is
O(p?2|N1| ? |N2|).
40000
45000
20000
25000
30000
35000
Train
ing T
ime (
S) GTKFGTK-I
0
5000
10000
15000Trai
ning 
Time
 (S)
FGTK-IITKPoly
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
Number of Training Instances (103)
Figure 2: Training time comparison among different
kernels with rule set having more optional nodes.
To further verify the efficiency of our proposed
algorithm, we conduct another experiment. Here we
use the same setting as that in Fig 1 except that we
randomly add more optional nodes in more produc-
tion rules. Table 1 reports the statistics on the two
rule set. Similar to Fig 1, Fig 2 compares the train-
ing time of different algorithms. We can see that
Fig 2 convincingly justify that our algorithms are
much faster than the GTK when the experimental
data has more optional nodes and rules.
Table 1: The rule set comparison between two ex-
periments.
# rules # rule with at
least optional
nodes
# op-
tional
nodes
# average op-
tional nodes per
rule
Exp1 4,734 1,404 2,242 1.6
Exp2 4,734 4,520 10,451 2.3
5 Conclusion
The GTK is a generalization of the TK, which can
capture more linguistic grammar knowledge into the
later and thereby achieve better performance. How-
ever, a biggest issue for the GTK is its comput-
ing speed, which needs exponential time in the-
ory. Therefore, in this paper we design two fast
grammar-driven convolution tree kennel (FGTK-I
and II) algorithms which can compute the GTK in
polynomial time. The experimental results show that
the FGTKs are much faster than the GTK when data
set has more optional nodes. We conclude that our
fast algorithms enable the GTK kernel to easily scale
to larger dataset. Besides the GTK, the idea of our
fast algorithms can be easily used into other similar
problems.
To further our study, we will use the FGTK algo-
rithms for other natural language processing prob-
lems, such as word sense disambiguation, syntactic
parsing, and so on.
References
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 shared task: Semantic role label-
ing. In Proceedings of CoNLL-2005, pages 152?164.
Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.
2006. A hybrid convolution tree kernel for seman-
tic role labeling. In Proceedings of the COLING/ACL
2006, Sydney, Australia, July.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of NIPS-
2001.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In Pro-
ceedings of ACL-2002, pages 239?246.
Thorsten Joachims. 2002. Learning to Classify Text Us-
ing Support Vector Machines: Methods, Theory and
Algorithms. Kluwer Academic Publishers.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learning
Research, 2:419?444.
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow statistic parsing. In Proceedings of
ACL-2004, pages 335?342.
Alessandro Moschitti. 2006. Syntactic kernels for natu-
ral language learning: the semantic role labeling case.
In Proceedings of the HHLT-NAACL-2006, June.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2005.
The necessity of syntactic parsing for semantic role la-
beling. In Proceedings of IJCAI-2005.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
Wiley.
Min Zhang, Wanxiang Che, Aiti Aw, Chew Lim Tan,
Guodong Zhou, Ting Liu, and Sheng Li. 2007. A
grammar-driven convolution tree kernel for semantic
role classification. In Proceedings of ACL-2007, pages
200?207.
786
