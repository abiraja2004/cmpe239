Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 573?581,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Type-Based MCMC
Percy Liang
UC Berkeley
pliang@cs.berkeley.edu
Michael I. Jordan
UC Berkeley
jordan@cs.berkeley.edu
Dan Klein
UC Berkeley
klein@cs.berkeley.edu
Abstract
Most existing algorithms for learning latent-
variable models?such as EM and existing
Gibbs samplers?are token-based, meaning
that they update the variables associated with
one sentence at a time. The incremental na-
ture of these methods makes them suscepti-
ble to local optima/slow mixing. In this paper,
we introduce a type-based sampler, which up-
dates a block of variables, identified by a type,
which spans multiple sentences. We show im-
provements on part-of-speech induction, word
segmentation, and learning tree-substitution
grammars.
1 Introduction
A long-standing challenge in NLP is the unsu-
pervised induction of linguistic structures, for ex-
ample, grammars from raw sentences or lexicons
from phoneme sequences. A fundamental property
of these unsupervised learning problems is multi-
modality. In grammar induction, for example, we
could analyze subject-verb-object sequences as ei-
ther ((subject verb) object) (mode 1) or (subject
(verb object)) (mode 2).
Multimodality causes problems for token-based
procedures that update variables for one example at
a time. In EM, for example, if the parameters al-
ready assign high probability to the ((subject verb)
object) analysis, re-analyzing the sentences in E-step
only reinforces the analysis, resulting in EM getting
stuck in a local optimum. In (collapsed) Gibbs sam-
pling, if all sentences are already analyzed as ((sub-
ject verb) object), sampling a sentence conditioned
2 1 2 2 1
2 2 2 1 2
1 2 1 2 2
2 1 2 2 1
2 2 2 1 2
1 2 1 2 2
2 1 2 2 1
2 2 2 1 2
1 2 1 2 2
(a) token-based (b) sentence-based (c) type-based
Figure 1: Consider a dataset of 3 sentences, each of
length 5. Each variable is labeled with a type (1 or 2). The
unshaded variables are the ones that are updated jointly
by a sampler. The token-based sampler updates the vari-
able for one token at a time (a). The sentence-based sam-
pler updates all variables in a sentence, thus dealing with
intra-sentential dependencies (b). The type-based sam-
pler updates all variables of a particular type (1 in this ex-
ample), thus dealing with dependencies due to common
parameters (c).
on all others will most likely not change its analysis,
resulting in slow mixing.
To combat the problems associated with token-
based algorithms, we propose a new sampling algo-
rithm that operates on types. Our sampler would, for
example, be able to change all occurrences of ((sub-
ject verb) object) to (subject (verb object)) in one
step. These type-based operations are reminiscent of
the type-based grammar operations of early chunk-
merge systems (Wolff, 1988; Stolcke and Omohun-
dro, 1994), but we work within a sampling frame-
work for increased robustness.
In NLP, perhaps the the most simple and popu-
lar sampler is the token-based Gibbs sampler,1 used
in Goldwater et al (2006), Goldwater and Griffiths
(2007), and many others. By sampling only one
1In NLP, this is sometimes referred to as simply the col-
lapsed Gibbs sampler.
573
variable at a time, this sampler is prone to slow mix-
ing due to the strong coupling between variables.
A general remedy is to sample blocks of coupled
variables. For example, the sentence-based sampler
samples all the variables associated with a sentence
at once (e.g., the entire tag sequence). However, this
blocking does not deal with the strong type-based
coupling (e.g., all instances of a word should be
tagged similarly). The type-based sampler we will
present is designed exactly to tackle this coupling,
which we argue is stronger and more important to
deal with in unsupervised learning. Figure 1 depicts
the updates made by each of the three samplers.
We tested our sampler on three models: a
Bayesian HMM for part-of-speech induction (Gold-
water and Griffiths, 2007), a nonparametric
Bayesian model for word segmentation (Goldwater
et al, 2006), and a nonparametric Bayesian model of
tree substitution grammars (Cohn et al, 2009; Post
and Gildea, 2009). Empirically, we find that type-
based sampling improves performance and is less
sensitive to initialization (Section 5).
2 Basic Idea via a Motivating Example
The key technical problem we solve in this paper is
finding a block of variables which are both highly
coupled and yet tractable to sample jointly. This
section illustrates the main idea behind type-based
sampling on a small word segmentation example.
Suppose our dataset x consists of n occurrences
of the sequence a b. Our goal is infer z =
(z1, . . . , zn), where zi = 0 if the sequence is one
word ab, and zi = 1 if the sequence is two, a
and b. We can model this situation with a simple
generative model: for each i = 1, . . . , n, gener-
ate one or two words with equal probability. Each
word is drawn independently based on probabilities
? = (?a, ?b, ?ab) which we endow with a uniform
prior ? ? Dirichlet(1, 1, 1).
We marginalize out ? to get the following standard
expression (Goldwater et al, 2009):
p(z | x) ?
1(m)1(m)1(n?m)
3(n+m)
def
= g(m), (1)
where m =
?n
i=1 zi is the number of two-word se-
quences and a(k) = a(a + 1) ? ? ? (a + k ? 1) is the
200 400 600 8001000
m
-1411.4
-1060.3
-709.1
-358.0
-6.8
log
g(m
)
2 4 6 8 10
iteration
200
400
600
800
1000
m
Token
Type
(a) bimodal posterior (b) sampling run
Figure 2: (a) The posterior (1) is sharply bimodal (note
the log-scale). (b) A run of the token-based and type-
based samplers. We initialize both samplers with m = n
(n = 1000). The type-based sampler mixes instantly
(in fact, it makes independent draws from the posterior)
whereas the token-based sampler requires five passes
through the data before finding the high probability re-
gion m u 0.
ascending factorial.2 Figure 2(a) depicts the result-
ing bimodal posterior.
A token-based sampler chooses one zi to update
according to the posterior p(zi | z?i,x). To illus-
trate the mixing problem, consider the case where
m = n, i.e., all sequences are analyzed as two
words. From (1), we can verify that p(zi = 0 |
z?i,x) = O( 1n). When n = 1000, this means that
there is only a 0.002 probability of setting zi = 0,
a very unlikely but necessary first step to take to es-
cape this local optimum. Indeed, Figure 2(b) shows
how the token-based sampler requires five passes
over the data to finally escape.
Type-based sampling completely eradicates the
local optimum problem in this example. Let us take
a closer look at (1). Note that p(z | x) only depends
on a single integer m, which only takes one of n+ 1
values, not on the particular z. This shows that the
zis are exchangeable. There are
(n
m
)
possible val-
ues of z satisfying m =
?
i zi, each with the same
probability g(m). Summing, we get:
p(m | x) ?
?
z:m=
P
i zi
p(x, z) =
(
n
m
)
g(m). (2)
A sampling strategy falls out naturally: First, sample
the number m via (2). Conditioned on m, choose
2The ascending factorial function arises from marginaliz-
ing Dirichlet distributions and is responsible the rich-gets-richer
phenomenon: the larger n is, more we gain by increasing it.
574
the particular z uniformly out of the
(n
m
)
possibili-
ties. Figure 2(b) shows the effectiveness of this type-
based sampler.
This simple example exposes the fundamental
challenge of multimodality in unsupervised learn-
ing. Both m = 0 and m = n are modes due to the
rich-gets-richer property which arises by virtue of
all n examples sharing the same parameters ?. This
sharing is a double-edged sword: It provides us with
clustering structure but also makes inference hard.
Even though m = n is much worse (by a factor ex-
ponential in n) than m = 0, a na??ve algorithm can
easily have trouble escaping m = n.
3 Setup
We will now present the type-based sampler in full
generality. Our sampler is applicable to any model
which is built out of local multinomial choices,
where each multinomial has a Dirichlet process prior
(a Dirichlet prior if the number of choices is finite).
This includes most probabilistic models in NLP (ex-
cluding ones built from log-linear features).
As we develop the sampler, we will pro-
vide concrete examples for the Bayesian hidden
Markov model (HMM), the Dirichlet process uni-
gram segmentation model (USM) (Goldwater et al,
2006), and the probabilistic tree-substitution gram-
mar (PTSG) (Cohn et al, 2009; Post and Gildea,
2009).
3.1 Model parameters
A model is specified by a collection of multino-
mial parameters ? = {?r}r?R, where R is an in-
dex set. Each vector ?r specifies a distribution over
outcomes: outcome o has probability ?ro.
? HMM: Let K is the number of states. The set
R = {(q, k) : q ? {T,E}, k = 1, . . . ,K}
indexes the K transition distributions {?(T,k)}
(each over outcomes {1, . . . ,K}) and K emis-
sion distributions {?(E,k)} (each over the set of
words).
? USM: R = {0}, and ?0 is a distribution over (an
infinite number of) words.
? PTSG: R is the set of grammar symbols, and
each ?r is a distribution over labeled tree frag-
ments with root label r.
R index set for parameters
? = {?r}r?R multinomial parameters
? = {?r}r?R base distributions (fixed)
S set of sites
b = {bs}s?S binary variables (to be sampled)
z latent structure (set of choices)
z?s choices not depending on site s
zs:b choices after setting bs = b
?zs:b zs:b\z?s: new choices from bs = b
S ? S sites selected for sampling
m # sites in S assigned bs = 1
n = {nro} counts (sufficient statistics of z)
Table 1: Notation used in this paper. Note that there is a
one-to-one mapping between z and (b,x). The informa-
tion relevant for evaluating the likelihood is n. We use
the following parallel notation: n?s = n(z?s),ns:b =
n(zs:b),?ns = n(?zs).
3.2 Choice representation of latent structure z
We represent the latent structure z as a set of local
choices:3
? HMM: z contains elements of the form
(T, i, a, b), denoting a transition from state
a at position i to state b at position i + 1; and
(E, i, a, w), denoting an emission of word w
from state a at position i.
? USM: z contains elements of the form (i, w), de-
noting the generation of word w at character po-
sition i extending to position i+ |w| ? 1.
? PTSG: z contains elements of the form (x, t), de-
noting the generation of tree fragment t rooted at
node x.
The choices z are connected to the parameters ?
as follows: p(z | ?) =
?
z?z ?z.r,z.o. Each choice
z ? z is identified with some z.r ? R and out-
come z.o. Intuitively, choice z was made by drawing
drawing z.o from the multinomial distribution ?z.r.
3.3 Prior
We place a Dirichlet process prior on ?r (Dirichlet
prior for finite outcome spaces): ?r ? DP(?r, ?r),
where ?r is a concentration parameter and ?r is a
fixed base distribution.
3We assume that z contains both a latent part and the ob-
served input x, i.e., x is a deterministic function of z.
575
Let nro(z) = |{z ? z : z.r = r, z.o = o}| be the
number of draws from ?r resulting in outcome o, and
nr? =
?
o nro be the number of times ?r was drawn
from. Let n(z) = {nro(z)} denote the vector of
sufficient statistics associated with choices z. When
it is clear from context, we simply write n for n(z).
Using these sufficient statistics, we can write p(z |
?) =
?
r,o ?
nro(z)
ro .
We now marginalize out ? using Dirichlet-
multinomial conjugacy, producing the following ex-
pression for the likelihood:
p(z) =
?
r?R
?
o (?ro?ro)
(nro(z))
?r(nr?(z))
, (3)
where a(k) = a(a+1) ? ? ? (a+k?1) is the ascending
factorial. (3) is the distribution that we will use for
sampling.
4 Type-Based Sampling
Having described the setup of the model, we now
turn to posterior inference of p(z | x).
4.1 Binary Representation
We first define a new representation of the latent
structure based on binary variables b so that there is
a bijection between z and (b,x); z was used to de-
fine the model, b will be used for inference. We will
use b to exploit the ideas from Section 2. Specifi-
cally, let b = {bs}s?S be a collection of binary vari-
ables indexed by a set of sites S.
? HMM: If the HMM hasK = 2 states, S is the set
of positions in the sequence. For each s ? S , bs
is the hidden state at s. The extension to general
K is considered at the end of Section 4.4.
? USM: S is the set of non-final positions in the
sequence. For each s ? S , bs denotes whether
a word boundary exists between positions s and
s+ 1.
? PTSG: S is the set of internal nodes in the parse
tree. For s ? S, bs denotes whether a tree frag-
ment is rooted at node s.
For each site s ? S, let zs:0 and zs:1 denote the
choices associated with the structures obtained by
setting the binary variable bs = 0 and bs = 1, re-
spectively. Define z?s
def
= zs:0 ? zs:1 to be the set
of choices that do not depend on the value of bs, and
n?s
def
= n(z?s) be the corresponding counts.
? HMM: z?s includes all but the transitions into
and out of the state at s plus the emission at s.
? USM: z?s includes all except the word ending at
s and the one starting at s+ 1 if there is a bound-
ary (bs = 1); except the word covering s if no
boundary exists (bs = 0).
? PTSG: z?s includes all except the tree fragment
rooted at node s and the one with leaf s if bs = 1;
except the single fragment containing s if bs = 0.
4.2 Sampling One Site
A token-based sampler considers one site s at a time.
Specifically, we evaluate the likelihoods of zs:0 and
zs:1 according to (3) and sample bs with probability
proportional to the likelihoods. Intuitively, this can
be accomplished by removing choices that depend
on bs (resulting in z?s), evaluating the likelihood re-
sulting from setting bs to 0 or 1, and then adding the
appropriate choices back in.
More formally, let ?zs:b
def
= zs:b\z?s be the new
choices that would be added if we set bs = b ?
{0, 1}, and let ?ns:b
def
= n(?zs:b) be the corre-
sponding counts. With this notation, we can write
the posterior as follows:
p(bs = b | b\bs) ? (4)
?
r?R
?
o (?ro?ro + n
?s
ro )
(?ns:bro )
(?r + n
?s
r? )
(?ns:br? )
.
The form of the conditional (4) follows from the
joint (3) via two properties: additivity of counts
(ns:b = n?s + ?ns:b) and a simple property of as-
cending factorials (a(k+?) = a(k)(a+ k)(?)).
In practice, most of the entries of ?ns:b are zero.
For the HMM, ns:bro would be nonzero only for
the transitions into the new state (b) at position s
(zs?1 ? b), transitions out of that state (b? zs+1),
and emissions from that state (b? xs).
4.3 Sampling Multiple Sites
We would like to sample multiple sites jointly as in
Section 2, but we cannot choose any arbitrary subset
S ? S, as the likelihood will in general depend on
the exact assignment of bS
def
= {bs}s?S , of which
576
a b c a a b c a b c b
(a) USM
1 1 2 2 1 1 2 2
a b a b c b b e
(b) HMM
a
b
a a
b c
d e
c
d
b c
e
a b
(c) PTSG
Figure 3: The type-based sampler jointly samples all vari-
ables at a set of sites S (in green boxes). Sites in S are
chosen based on types (denoted in red). (a) HMM: two
sites have the same type if they have the same previous
and next states and emit the same word; they conflict un-
less separated by at least one position. (b) USM: two sites
have the same type if they are both of the form ab|c or
abc; note that occurrences of the same letters with other
segmentations do not match the type. (c) PTSG: analo-
gous to the USM, only for tree rather than sequences.
there are an exponential number. To exploit the ex-
changeability property in Section 2, we need to find
sites which look ?the same? from the model?s point
of view, that is, the likelihood only depends on bS
via m
def
=
?
s?S bs.
To do this, we need to define two notions, type and
conflict. We say sites s and s? have the same type if
the counts added by setting either bs or bs? are the
same, that is, ?ns:b = ?ns
?:b for b ? {0, 1}. This
motivates the following definition of the type of site
s with respect to z:
t(z, s)
def
= (?ns:0,?ns:1), (5)
We say that s and s? have the same type if t(z, s) =
t(z, s?). Note that the actual choices added (?zs:b
and ?zs
?:b) are in general different as s and s? cor-
respond to different parts of the latent structure, but
the model only depends on counts and is indifferent
to this. Figure 3 shows examples of same-type sites
for our three models.
However, even if all sites in S have the same
type, we still cannot sample bS jointly, since chang-
ing one bs might change the type of another site s?;
indeed, this dependence is reflected in (5), which
shows that types depend on z. For example, s, s? ?
S conflict when s? = s + 1 in the HMM or when
s and s? are boundaries of one segment (USM) or
one tree fragment (PTSG). Therefore, one additional
concept is necessary: We say two sites s and s? con-
flict if there is some choice that depends on both bs
and bs? ; formally, (z\z?s) ? (z\z?s
?
) 6= ?.
Our key mathematical result is as follows:
Proposition 1 For any set S ? S of non-conflicting
sites with the same type,
p(bS | b\bS) ? g(m) (6)
p(m | b\bS) ?
(
|S|
m
)
g(m), (7)
for some easily computable g(m), where m =
?
s?S bs.
We will derive g(m) shortly, but first note from
(6) that the likelihood for a particular setting of bS
depends on bS only via m as desired. (7) sums
over all
(|S|
m
)
settings of bS with m =
?
s?S bs.
The algorithmic consequences of this result is that
to sample bS , we can first compute (7) for each
m ? {0, . . . , |S|}, sample m according to the nor-
malized distribution, and then choose the actual bS
uniformly subject to m.
Let us now derive g(m) by generalizing (4).
Imagine removing all sites S and their dependent
choices and adding in choices corresponding to
some assignment bS . Since all sites in S are non-
conflicting and of the same type, the count contribu-
tion ?ns:b is the same for every s ? S (i.e., sites
in S are exchangeable). Therefore, the likelihood
of the new assignment bS depends only on the new
counts:
?nS:m
def
= m?ns:1 + (|S| ?m)?ns:0. (8)
Using these new counts in place of the ones in (4),
we get the following expression:
g(m) =
?
r?R
?
o (?ro?ro + nro(z
?S))
(?nS:mro )
?r + nr?(z?S)
(?nS:mr? )
. (9)
4.4 Full Algorithm
Thus far, we have shown how to sample bS given
a set S ? S of non-conflicting sites with the same
type. To complete the description of the type-based
577
Type-Based Sampler
for each iteration t = 1, . . . , T :
?for each pivot site s0 ? S:
??S ? TB(z, s0) (S is the type block centered at s0)
??decrement n and remove from z based on bS
??sample m according to (7)
??sample M ? S with |M | = m uniformly at random
??set bs = I[s ?M ] for each s ? S
??increment n and add to z accordingly
Figure 4: Pseudocode for the general type-based sampler.
We operate in the binary variable representation b of z.
Each step, we jointly sample |S| variables (of the same
type).
sampler, we need to specify how to choose S. Our
general strategy is to first choose a pivot site s0 ? S
uniformly at random and then set S = TB(z, s0) for
some function TB. Call S the type block centered at
s0. The following two criteria on TB are sufficient
for a valid sampler: (A) s0 ? S, and (B) the type
blocks are stable, which means that if we change bS
to any b?S (resulting in a new z
?), the type block cen-
tered at s0 with respect to z? does not change (that
is, TB(z?, s0) = S). (A) ensures ergodicity; (B),
reversibility.
Now we define TB as follows: First set S = {s0}.
Next, loop through all sites s ? S with the same type
as s0 in some fixed order, adding s to S if it does
not conflict with any sites already in S. Figure 4
provides the pseudocode for the full algorithm.
Formally, this sampler cycles over |S| transition
kernels, one for each pivot site. Each kernel (in-
dexed by s0 ? S) defines a blocked Gibbs move,
i.e. sampling from p(bTB(z,s0) | ? ? ? ).
Efficient Implementation There are two oper-
ations we must perform efficiently: (A) looping
through sites with the same type as the pivot site s0,
and (B) checking whether such a site s conflicts with
any site in S. We can perform (B) in O(1) time by
checking if any element of ?zs:bs has already been
removed; if so, there is a conflict and we skip s. To
do (A) efficiently, we maintain a hash table mapping
type t to a doubly-linked list of sites with type t.
There is anO(1) cost for maintaining this data struc-
ture: When we add or remove a site s, we just need
to add or remove neighboring sites s? from their re-
spective linked lists, since their types depend on bs.
For example, in the HMM, when we remove site s,
we also remove sites s?1 and s+1.
For the USM, we use a simpler solution: main-
tain a hash table mapping each word w to a list of
positions where w occurs. Suppose site (position) s
straddles words a and b. Then, to perform (A), we
retrieve the list of positions where a, b, and ab occur,
intersecting the a and b lists to obtain a list of posi-
tions where a b occurs. While this intersection is
often much smaller than the pre-intersected lists, we
found in practice that the smaller amount of book-
keeping balanced out the extra time spent intersect-
ing. We used a similar strategy for the PTSG, which
significantly reduces the amount of bookkeeping.
Skip Approximation Large type blocks mean
larger moves. However, such a block S is also sam-
pled more frequently?once for every choice of a
pivot site s0 ? S. However, we found that empir-
ically, bS changes very infrequently. To eliminate
this apparent waste, we use the following approxi-
mation of our sampler: do not consider s0 ? S as
a pivot site if s0 belongs to some block which was
already sampled in the current iteration. This way,
each site is considered roughly once per iteration.4
Sampling Non-Binary Representations We can
sample in models without a natural binary represen-
tation (e.g., HMMs with with more than two states)
by considering random binary slices. Specifically,
suppose bs ? {1, . . . ,K} for each site s ? S .
We modify Figure 4 as follows: After choosing a
pivot site s0 ? S , let k = bs0 and choose k
? uni-
formly from {1, . . . ,K}. Only include sites in one
of these two states by re-defining the type block to
be S = {s ? TB(z, s0) : bs ? {k, k?}}, and sam-
ple bS restricted to these two states by drawing from
p(bS | bS ? {k, k?}|S|, ? ? ? ). By choosing a random
k? each time, we allow b to reach any point in the
space, thus achieving ergodicity just by using these
binary restrictions.
5 Experiments
We now compare our proposed type-based sampler
to various alternatives, evaluating on marginal like-
4A site could be sampled more than once if it belonged to
more than one type block during the iteration (recall that types
depend on z and thus could change during sampling).
578
lihood (3) and accuracy for our three models:
? HMM: We learned a K = 45 state HMM on
the Wall Street Journal (WSJ) portion of the Penn
Treebank (49208 sentences, 45 tags) for part-of-
speech induction. We fixed ?r to 0.1 and ?r to
uniform for all r.
For accuracy, we used the standard metric based
on greedy mapping, where each state is mapped
to the POS tag that maximizes the number of cor-
rect matches (Haghighi and Klein, 2006). We did
not use a tagging dictionary.
? USM: We learned a USM model on the
Bernstein-Ratner corpus from the CHILDES
database used in Goldwater et al (2006) (9790
sentences) for word segmentation. We fixed ?0 to
0.1. The base distribution ?0 penalizes the length
of words (see Goldwater et al (2009) for details).
For accuracy, we used word token F1.
? PTSG: We learned a PTSG model on sections 2?
21 of the WSJ treebank.5 For accuracy, we used
EVALB parsing F1 on section 22.6 Note this is a
supervised task with latent-variables, whereas the
other two are purely unsupervised.
5.1 Basic Comparison
Figure 5(a)?(c) compares the likelihood and accu-
racy (we use the term accuracy loosely to also in-
clude F1). The initial observation is that the type-
based sampler (TYPE) outperforms the token-based
sampler (TOKEN) across all three models on both
metrics.
We further evaluated the PTSG on parsing. Our
standard treebank PCFG estimated using maximum
likelihood obtained 79% F1. TOKEN obtained an F1
of 82.2%, and TYPE obtained a comparable F1 of
83.2%. Running the PTSG for longer continued to
5Following Petrov et al (2006), we performed an initial pre-
processing step on the trees involving Markovization, binariza-
tion, and collapsing of unary chains; words occurring once are
replaced with one of 50 ?unknown word? tokens, using base
distributions {?r} that penalize the size of trees, and sampling
the hyperparameters (see Cohn et al (2009) for details).
6To evaluate, we created a grammar where the rule proba-
bilities are the mean values under the PTSG distribution: this
involves taking a weighted combination (based on the concen-
tration parameters) of the rule counts from the PTSG samples
and the PCFG-derived base distribution. We used the decoder
of DeNero et al (2009) to parse.
improve the likelihood but actually hurt parsing ac-
curacy, suggesting that the PTSG model is overfit-
ting.
To better understand the gains from TYPE
over TOKEN, we consider three other alterna-
tive samplers. First, annealing (TOKENanneal) is
a commonly-used technique to improve mixing,
where (3) is raised to some inverse temperature.7
In Figure 5(a)?(c), we see that unlike TYPE,
TOKENanneal does not improve over TOKEN uni-
formly: it hurts for the HMM, improves slightly for
the USM, and makes no difference for the PTSG. Al-
though annealing does increase mobility of the sam-
pler, this mobility is undirected, whereas type-based
sampling increases mobility in purely model-driven
directions.
Unlike past work that operated on types (Wolff,
1988; Brown et al, 1992; Stolcke and Omohun-
dro, 1994), type-based sampling makes stochastic
choices, and moreover, these choices are reversible.
Is this stochasticity important? To answer this, we
consider a variant of TYPE, TYPEgreedy: instead
of sampling from (7), TYPEgreedy considers a type
block S and sets bs to 0 for all s ? S if p(bS =
(0, . . . , 0) | ? ? ? ) > p(bS = (1, . . . , 1) | ? ? ? ); else
it sets bs to 1 for all s ? S. From Figure 5(a)?(c),
we see that greediness is disastrous for the HMM,
hurts a little for USM, and makes no difference on
the PTSG. These results show that stochasticity can
indeed be important.
We consider another block sampler, SENTENCE,
which uses dynamic programming to sample all
variables in a sentence (using Metropolis-Hastings
to correct for intra-sentential type-level coupling).
For USM, we see that SENTENCE performs worse
than TYPE and is comparable to TOKEN, suggesting
that type-based dependencies are stronger and more
important to deal with than intra-sentential depen-
dencies.
5.2 Initialization
We initialized all samplers as follows: For the USM
and PTSG, for each site s, we place a boundary (set
bs = 1) with probability ?. For the HMM, we set bs
to state 1 with probability ? and a random state with
7We started with a temperature of 10 and gradually de-
creased it to 1 during the first half of the run, and kept it at 1
thereafter.
579
3 6 9 12
time (hr.)
-1.1e7
-0.9e7
-9.1e6
-7.9e6
-6.7e6
log
-lik
elih
ood
3 6 9 12
time (hr.)
0.1
0.2
0.4
0.5
0.6
acc
ura
cy
2 4 6 8
time (min.)
-3.7e5
-3.2e5
-2.8e5
-2.4e5
-1.9e5
log
-lik
elih
ood Token
Tokenanneal
Typegreedy
Type
Sentence
2 4 6 8
time (min.)
0.1
0.2
0.4
0.5
0.6
F 1
3 6 9 12
time (hr.)
-6.2e6
-6.0e6
-5.8e6
-5.7e6
-5.5e6
log
-lik
elih
ood
(a) HMM (b) USM (c) PTSG
0.2 0.4 0.6 0.8 1.0
?
-7.1e6
-7.0e6
-6.9e6
-6.8e6
-6.7e6
log
-lik
elih
ood
0.2 0.4 0.6 0.8 1.0
?
0.2
0.3
0.4
0.5
0.6
acc
ura
cy
0.2 0.4 0.6 0.8 1.0
?
-3.5e5
-3.1e5
-2.7e5
-2.3e5
-1.9e5
log
-lik
elih
ood
0.2 0.4 0.6 0.8 1.0
?
0.2
0.3
0.4
0.5
0.6
F 1
0.2 0.4 0.6 0.8 1.0
?
-5.7e6
-5.6e6
-5.6e6
-5.5e6
-5.5e6
log
-lik
elih
ood
(d) HMM (e) USM (f) PTSG
Figure 5: (a)?(c): Log-likelihood and accuracy over time. TYPE performs the best. Relative to TYPE, TYPEgreedy
tends to hurt performance. TOKEN generally works worse. Relative to TOKEN, TOKENanneal produces mixed results.
SENTENCE behaves like TOKEN. (d)?(f): Effect of initialization. The metrics were applied to the current sample after
15 hours for the HMM and PTSG and 10 minutes for the USM. TYPE generally prefers larger ? and outperform the
other samplers.
probability 1 ? ?. Results in Figure 5(a)?(c) were
obtained by setting ? to maximize likelihood.
Since samplers tend to be sensitive to initializa-
tion, it is important to explore the effect of initial-
ization (parametrized by ? ? [0, 1]). Figure 5(d)?(f)
shows that TYPE is consistently the best, whereas
other samplers can underperform TYPE by a large
margin. Note that TYPE favors ? = 1 in general.
This setting maximizes the number of initial types,
and thus creates larger type blocks and thus enables
larger moves. Larger type blocks also mean more
dependencies that TOKEN is unable to deal with.
6 Related Work and Discussion
Block sampling, on which our work is built, is a clas-
sical idea, but is used restrictively since sampling
large blocks is computationally expensive. Past
work for clustering models maintained tractabil-
ity by using Metropolis-Hastings proposals (Dahl,
2003) or introducing auxiliary variables (Swendsen
and Wang, 1987; Liang et al, 2007). In contrast,
our type-based sampler simply identifies tractable
blocks based on exchangeability.
Other methods for learning latent-variable models
include EM, variational approximations, and uncol-
lapsed samplers. All of these methods maintain dis-
tributions over (or settings of) the latent variables of
the model and update the representation iteratively
(see Gao and Johnson (2008) for an overview in the
context of POS induction). However, these methods
are at the core all token-based, since they only up-
date variables in a single example at a time.8
Blocking variables by type?the key idea of
this paper?is a fundamental departure from token-
based methods. Though type-based changes have
also been proposed (Brown et al, 1992; Stolcke and
Omohundro, 1994), these methods operated greed-
ily, and in Section 5.1, we saw that being greedy led
to more brittle results. By working in a sampling
framework, we were able bring type-based changes
to fruition.
8While EM technically updates all distributions over latent
variables in the E-step, this update is performed conditioned on
model parameters; it is this coupling (made more explicit in
collapsed samplers) that makes EM susceptible to local optima.
580
References
P. F. Brown, V. J. D. Pietra, P. V. deSouza, J. C. Lai, and
R. L. Mercer. 1992. Class-based n-gram models of
natural language. Computational Linguistics, 18:467?
479.
T. Cohn, S. Goldwater, and P. Blunsom. 2009. Inducing
compact but accurate tree-substitution grammars. In
North American Association for Computational Lin-
guistics (NAACL), pages 548?556.
D. B. Dahl. 2003. An improved merge-split sampler for
conjugate Dirichlet process mixture models. Techni-
cal report, Department of Statistics, University of Wis-
consin.
J. DeNero, M. Bansal, A. Pauls, and D. Klein. 2009.
Efficient parsing for transducer grammars. In North
American Association for Computational Linguistics
(NAACL), pages 227?235.
J. Gao and M. Johnson. 2008. A comparison of
Bayesian estimators for unsupervised hidden Markov
model POS taggers. In Empirical Methods in Natural
Language Processing (EMNLP), pages 344?352.
S. Goldwater and T. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Association for Computational Linguistics (ACL).
S. Goldwater, T. Griffiths, and M. Johnson. 2006. Con-
textual dependencies in unsupervised word segmenta-
tion. In International Conference on Computational
Linguistics and Association for Computational Lin-
guistics (COLING/ACL).
S. Goldwater, T. Griffiths, and M. Johnson. 2009. A
Bayesian framework for word segmentation: Explor-
ing the effects of context. Cognition, 112:21?54.
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In North American Associ-
ation for Computational Linguistics (NAACL), pages
320?327.
P. Liang, M. I. Jordan, and B. Taskar. 2007. A
permutation-augmented sampler for Dirichlet process
mixture models. In International Conference on Ma-
chine Learning (ICML).
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In International Conference on Computa-
tional Linguistics and Association for Computational
Linguistics (COLING/ACL), pages 433?440.
M. Post and D. Gildea. 2009. Bayesian learning of a
tree substitution grammar. In Association for Com-
putational Linguistics and International Joint Confer-
ence on Natural Language Processing (ACL-IJCNLP).
A. Stolcke and S. Omohundro. 1994. Inducing prob-
abilistic grammars by Bayesian model merging. In
International Colloquium on Grammatical Inference
and Applications, pages 106?118.
R. H. Swendsen and J. S. Wang. 1987. Nonuniversal
critical dynamics in MC simulations. Physics Review
Letters, 58:86?88.
J. G. Wolff. 1988. Learning syntax and meanings
through optimization and distributional analysis. In
Categories and processes in language acquisition,
pages 179?215.
581
