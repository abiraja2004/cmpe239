JEP-TALN-RECITAL 2012, Atelier DEFT 2012: D?fi Fouille de Textes, pages 61?68,
Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCP
Participation du LINA ? DEFT 2012
Florian Boudin Amir Hazem Nicolas Hernandez Prajol Shrestha
Universit? de Nantes
pr?nom.nom@univ-nantes.fr
R?SUM?
Cet article pr?sente la participation de l??quipe TALN du LINA au d?fi fouille de textes (DEFT)
2012. D?velopp? sp?cifiquement pour la seconde piste du d?fi, notre syst?me combine les sorties
de trois diff?rentes m?thodes d?extraction de mots cl?s. Notre syst?me s?est class? ? la 2i e`me place
sur un total de 9 syst?mes avec une f-mesure de 21,3%.
ABSTRACT
LINA at DEFT 2012
This article presents the participation of the TALN group at LINA to the d?fi fouille de textes
(DEFT) 2012. Developed specifically for the second task, our system combines the outputs of
three different keyword extraction methods. Our system ranked 2nd out of 9 systems with a
f-measure of 21,3%.
MOTS-CL?S : extraction de mots cl?s, deft 2012, combinaison de m?thodes.
KEYWORDS: keyword extraction, deft 2012, combining methods.
1 Introduction
L?indexation automatique consiste ? identifier un ensemble de mots cl?s (e.g. mots, termes) qui
d?crit le contenu d?un document. Les mots cl?s peuvent ensuite ?tre utilis?s, entre autres, pour
faciliter la recherche d?information ou la navigation dans les collections de documents. L??dition
2012 du d?fi fouille de textes (DEFT) porte sur l?extraction automatique de mots cl?s ? partir
d?articles scientifiques parus dans le domaine des Sciences Humaines et Sociales (SHS).
L?objectif du d?fi est de retrouver, ? partir du contenu des documents (i.e. articles scientifiques),
les mots cl?s qui ont pu ?tre choisis par les auteurs. Deux diff?rentes pistes ont ?t? propos?es.
La premi?re piste consiste ? identifier dans une terminologie, les mots cl?s qui ont ?t? assign?s
aux documents. Cette terminologie regroupe l?ensemble des mots cl?s utilis?s dans la collection.
La seconde piste, de prime abord plus complexe, consiste ? extraire les mots cl?s directement ?
partir du contenu des documents. Cet article d?crit notre participation ? la seconde piste du d?fi.
Le reste de cet article est organis? comme suit. La section 2 d?crit l?ensemble de donn?es utilis?
pour la campagne d??valuation. La section 3 pr?sente les diff?rentes m?thodes que nous avons
d?velopp?es sp?cifiquement pour la seconde piste du d?fi. Nous d?crivons ensuite en section
4 nos r?sultats exp?rimentaux avant de pr?senter les m?thodes que nous avons test?es et qui
61
qui ont eu un impact nul ou n?gatif sur les r?sultats. La section 6 conclut cet article et donne
quelques perspectives de travaux futurs.
2 Description de la campagne DEFT 2012
L?ensemble de documents utilis? pour le d?fi 2012 est constitu? de 234 articles scientifiques parus
dans le domaine des SHS. Ces articles ont ?t? publi?s entre 2001 et 2008 dans quatre revues
diff?rentes. L?ensemble d?apprentissage contient 60% des documents (soit 141 articles), et celui
de test contient les 40% restants (soit 93 articles). La r?partition des quatre diff?rentes revues
dans les deux ensembles est uniforme.
Du point de vue technique, les articles sont au format XML. Ils sont structur?s en deux parties : le
r?sum? et le corps de l?article. Chaque article contient ?galement le nombre de mots cl?s indexant
son contenu. Les mots cl?s assign?s ? chaque article sont disponibles pour chacun des articles de
l?ensemble d?entra?nement.
Les syst?mes participant au d?fi sont ?valu?s ? l?aide des mesures classiques de pr?cision, rappel
et f-mesure. Pour chaque article, les mots cl?s g?n?r?s par les syst?mes sont compar?s aux mots
cl?s de r?f?rence (assign?s par les auteurs). Afin de limiter les probl?mes li?s aux diff?rentes
variations orthographiques, plusieurs traitements de normalisation (i.e. normalisation de la casse
et lemmatisation) sont appliqu?s au pr?alable aux mots cl?s. Chaque participant peut soumettre
jusqu?? trois ex?cutions par piste.
La liste ci-dessous pr?sente quelques unes des difficult?s que nous avons identifi?es dans les
articles de l?ensemble d?entra?nement.
? Articles diff?rents ayant le m?me r?sum?, e.g. les articles as_2002_007048ar et as_2002_
007053ar.
? Contenu des articles dans des langues diff?rentes et/ou m?lang?es, e.g. fran?ais et anglais
dans ttr_2008_037494ar, espagnol dans meta_2005_019927ar.
? Contenu des articles tr?s bruit? avec des probl?mes de ponctuation, de caract?res unicodes et
de segmentation en paragraphes, e.g. ci-dessous un extrait de l?article meta_2005_019840ar.
<p>Ce langage est au c.ur des pr?occupations des juristes , qui
nous rappellent r?guli?rement </p>
<p>que le droit est affaire de mots. Et cela dans tout l.univers
du droit , vers quelque c?t? que l.on se</p>
<p>tourne , dans le monde juridique anglophone - o?, pour
Mellinkoff (1963& amp;#x00A0 ;: vii), &amp;#x00A0;The law is a</p>
<p></p>
<p>dans son ensemble , la technique juridique aboutit , pour la
plus grande part , ? une question de</p>
<p>terminologie&amp;# x00A0;. Chacun pourra le v?rifier par la
consultation d.ouvrages parmi les plus r?cents et</p>
62
3 Approches
Les diff?rentes m?thodes que nous avons d?velopp?es utilisent le mot comme unit? principale.
Nous avons donc appliqu? un ensemble commun de pr?-traitements aux documents : segmenta-
tion en phrases, d?coupage en mots et ?tiquetage morpho-syntaxique. L?information structurelle
pr?sente dans chacun des documents (i.e. r?sum?, corps de l?article et paragraphes) est pr?serv?e.
Chaque paragraphe est segment? en phrases en utilisant la m?thode PUNKT de d?tection de
changement de phrases (Kiss et Strunk, 2006) mise en ?uvre dans la bo?te ? outils NLTK (Bird et
Loper, 2004). La tokenisation des phrases est effectu?e avec un outil d?velopp? en interne utili-
sant le lexique des formes fl?chies du fran?ais (lefff)1 pour l?indentification des unit?s lexicales
complexes (e.g. mots compos?s). L??tiquetage morpho-syntaxique est obtenu ? l?aide du Stanford
POS Tagger (Toutanova et al, 2003)2 entrain?e sur le French Treebank (Abeill? et al, 2003).
3.1 Syst?me 1
Ce syst?me est bas? sur du T F? IDF et trois r?gles issues du corpus d?apprentissage. La principale
question qui se pose ici est : qu?est ce qu?un mot cl? ? ou autrement dit, qu?est ce qui fait qu?un
terme a plus de chances d??tre un mot cl? qu?un autre ?
En analysant les documents du corpus d?apprentissage, nous avons relev? trois particularit?s li?es
aux mots cl?s. La premi?re concerne leur localisation dans les documents. Chaque document ?tant
divis? en deux parties qui sont : le r?sum? (ABSTRACT) et le corps du document (BODY), nous
nous sommes donc int?ress?s ? la position des mots cl?s par rapport ? ce d?coupage. Nous avons
pu constater qu?un terme apparaissant ? la fois dans le r?sum? et dans le corps du document avait
plus de chances d??tre un mot cl?. Ainsi, nous avons utilis? cette information comme premi?re
r?gle de notre syst?me (nous appellerons cette r?gle : R1). Deux strat?gies utilisant cette r?gle
ont ?t? adopt?es, la premi?re consiste ? ne s?lectionner que des termes qui apparaissent ? la
fois dans le r?sum? et dans le corps du document (nous appellerons cette strat?gie : S1), la
deuxi?me consiste ? donner la priorit? aux termes respectant la strat?gie S1 en utilisant une
pond?ration par un param?tre ? fix? empiriquement (nous appellerons cette strat?gie : S2).
Les diff?rents tests conduits ont montr? que l?utilisation de la strat?gie S1 donnait de meilleurs
r?sultats que l?utilisation de la strat?gie S2. Intuitivement, nous aurions tendance ? penser le
contraire (la strat?gie S2 devrait ?tre meilleur que S1), car ?liminer des termes n?apparaissant
que dans le r?sum? ou que dans le corps du document nous ferait sans doute perdre des mots
cl?s. L?explication et que la strat?gie S1 corrige sans doute les faiblesses de notre syst?me qui
renverrait plus de faux positifs que de vrais n?gatifs.
La deuxi?me particularit? relative aux n-grammes, d?coule de la question suivante : est ce qu?un
terme simple (1-gramme) a plus de chances d??tre un mot cl? qu?un terme compos? (n-grammes
avec n > 1) ? De part le corpus d?apprentissage nous avons pu constater qu?il y avait 70% de
termes simples et 30% de termes compos?s. Ainsi, nous avons voulu donner une plus grande
importance aux termes simples extraits par notre syst?me. De la m?me mani?re que pour la
strat?gie S2, nous avons introduit un param?tre de pond?ration ? afin de prioriser les termes
simples (nous appellerons cette r?gle R2).
1
http://www.labri.fr/perso/clement/lefff/2Nous utilison la version 3.1.0 avec les param?tres par d?faut.
63
La troisi?me particularit? rel?ve de la simple observation que la quasi totalit? des mots cl?s
?taient soit des noms, soit des adjectifs. ? partir de cette constatation, nous avons introduit une
troisi?me r?gle (R3) qui filtre les verbes.
3.2 Syst?me 2
Ce syst?me repose sur l?exploitation d?un existant ? savoir l?approche KEA (Keyphrase Extraction
Algorithm) de (Witten et al, 1999). KEA permet d?une part de mod?liser les expressions significa-
tives (compos?es d?un ou plusieurs mots) du contenu de textes ? l?aide de textes et d?expressions
cl?s associ?es et d?autre part d?extraire les expressions cl?s d?une collection de textes ? l?aide
d?une mod?lisation construite a priori. L?approche utilise un classifieur bay?sien na?f pour calculer
un score de probabilit? de chaque expression cl? candidate. La construction requiert un ensemble
d?expressions cl?s class?es positivement pour chaque texte du corpus d?apprentissage. L?extraction
se r?alise sur un corpus de domaine similaire au domaine du corpus d?apprentissage.
Les phases de mod?lisation ou d?extraction des expressions cl?s fonctionnent toutes deux ? la
suite de deux phases ?l?mentaires : l?extraction de candidats et le calcul de traits descriptifs
des candidats. Les candidats s?obtiennent par extraction de n-grammes de taille pr?d?finie ne
d?butant pas et ne finissant pas par un mot outil.
Les traits utilis?s pour d?crire chaque candidat au sein d?un document sont les suivants : le
T F ? IDF (mesure de sp?cificit? du candidat pour le document), la position de la premi?re
occurrence (pourcentage du texte pr?c?dent l?occurrence), le nombre de mots qui compose le
candidat. Les candidats ayant un haut T F ? IDF , apparaissant au d?but d?un texte et comptant
le plus de mots sont ainsi consid?r?s comme ?tant de bons descripteurs du contenu d?un texte.
Les derni?res ?volutions de KEA permettent d?exploiter des lexiques contr?l?s de type th?saurus
dans la construction de la mod?lisation (Medelyan et Witten, 2006).
Nous n?avons pas exploit? de ressources ext?rieures de type lexiques contr?l?s dans la construction
de notre mod?lisation. Nous avons utilis? la version 5.0 de l?impl?mentation de KEA3 disponible
sous licence GNU ; en pratique nous avons utilis? les fonctionnalit?s d?extraction ?libre? pr?sentes
d?s la version 3.0. Les fonctionnalit?s d?velopp?es ult?rieurement concernent l?exploitation de
lexiques contr?l?s. Nos candidats ?taient au maximum de taille 5. Nous avons exploit? le corpus
d?apprentissage fourni pour la seconde piste pour construire notre mod?lisation. Chaque texte
(r?sum? et corps) a ?t? consid?r? comme une unit? documentaire.
L?approche KEA est facilement portable ? diff?rentes langues du fait qu?elle n?cessite peu de
ressources. En particulier elle ne requiert pas une pr?-analyse syntaxique pour s?lectionner des
candidats. Nous avons n?anmoins port? une certaine attention ? nos traitements pr?liminaires
et nous avons constat? qu?une pr?-segmentation en token mots ainsi que l?utilisation d?une liste
multilingue de mots outils augmentaient la qualit? de l?extraction des expressions cl?s lorsque
nous ?valuions l?approche par validation crois?e sur le corpus d?apprentissage. Concernant la
liste des mots outils, nous avons fusionn? les listes fournies par KEA pour le fran?ais, l?anglais et
l?espagnol. Nous l?avons compl?t?e des formes des mots outils pouvant subir une ?lision du e final
en fran?ais (e.g. ?le? s?est vu compl?t? de la forme ?l??, de m?me pour ?lorsque? avec ?lorsqu??. . .).
Ces formes ?taient en effet reconnues par notre segmenteur en mots.
3
http://www.nzdl.org/Kea
64
3.3 Syst?me 3
Ce syst?me est bas? sur une approche par classification supervis?e. La t?che d?extraction de mots
cl?s est ici consid?r?e comme une t?che de classification binaire. La premi?re ?tape consiste ?
g?n?rer tous les mots cl?s candidats ? partir du document. Pour ce faire, nous commen?ons par
extraire tous les n-grammes de mots jusqu?? n = 4. Des contraintes syntaxiques sont ensuite
utilis?es pour filtrer les candidats. Ainsi, seuls les n-grammes compos?s uniquement de noms,
d?adjectifs et de mots outils (except? en premier/dernier mot du n-gramme) sont gard?s.
Pour chaque candidat, nous calculons les traits suivants :
? Poids T F ? IDF
? Nombre de mots du n-gramme
? Patron syntaxique du n-gramme (e.g. ?Nom Adjectif?)
? Position relative de la premi?re occurence dans le document
? Section(s) o? apparait le n-gramme (r?sum?, corps ou les deux)
? Nombre de documents de la collection dans lesquels le n-gramme apparait
? Score de saillance dans l?arbre de d?pendances de coh?sion lexicale du texte (voir ci-dessous)
Nous construisons ce que nous appelons un ?arbre de d?pendances de coh?sion lexicale? selon
une approche d?crite par Choi ? la section 6.3.1. de sa th?se (Choi, 2002). Une d?pendance
est pr?suppos?e exister entre deux phrases cons?cutives si celles-ci ont des mots en commun ;
l?hypoth?se est de consid?rer la seconde phrase comme une ?laboration de la premi?re. En
pratique, notre algorithme ne reconna?t pas syst?matiquement une relation de d?pendance
entre deux phrases cons?cutives qui partagent des mots en commun. En effet notre algorithme
recherche, pour chaque phrase du texte, la phrase la plus haute dans la cha?ne de d?pendance
de la phrase pr?c?dente avec laquelle elle partage des mots en commun. L?arbre est construit
en prenant le texte dans son ensemble (r?sum? et corps) pr?alablement lemmatis?. Un score de
saillance est calcul? pour chaque phrase en fonction du nombre de ses d?pendances (directes
et transitives) normalis? par le nombre de d?pendances maximal qu?une phrase peut avoir sur
le texte donn?. Chaque expression candidate h?rite alors du score de la phrase o? apparait sa
premi?re occurence.
Nous utilisons la combinaison par vote de trois algorithmes de classification disponibles dans la
boite ? outils Weka (Hall et al, 2009) : NaiveBayes, J48 et RandomForest. Les mots-cl?s candidats
sont ensuite tri?s selon leurs scores de pr?diction.
3.4 Combinaison des syst?mes
Les trois syst?mes que nous avons d?velopp?s utilisent diff?rentes m?thodes pour capturer
l?importance d?un mot cl? par rapport ? un document. Une combinaison des sorties de ces
derniers est donc pertinente.
Nous disposons pour chaque document, de trois listes pond?r?es de mots cl?s. La m?thode la
plus simple consisterait ? utiliser la somme des scores des trois syst?mes. Cependant, les scores
calcul?s par chacun des syst?mes ne sont pas directement comparables. ? la place du score, nous
utilisons pour chaque mot cl? candidat, l?inverse de son rang dans la liste ordonn?e.
Deux strat?gies de combinaison ont ?t? utilis?es. La premi?re, COMBI1 consiste ? assigner la
65
somme de l?inverse des rangs d?un mot cl? dans les listes ordonn?es des trois syst?mes. Pour la
seconde strat?gie, COMBI2, nous ne consid?rons que les mots cl?s apparaissant dans les sorties
des trois syst?mes. L?id?e est de filtrer les mots cl?s consid?r?s comme important par seulement
un ou deux des trois syst?mes.
4 R?sultats
Nous pr?sentons dans cette section les r?sultats officiels de la campagne DEFT 2012. Nous
avons soumis trois ex?cutions pour chacune des deux pistes. Pour la premi?re piste, nous avons
simplement utilis? le Syst?me 1 (d?crit dans la section 3.1) et filtr? les mots cl?s candidats ? l?aide
de la terminologie. Le nombre de mots cl?s retourn?s est fix? ? 7 pour la premi?re ex?cution et ?
6 pour les deux autres. Les trois configurations utilisent la r?gle R3.
La premi?re ex?cution utilise la r?gle R2 (? = 0,6). La seconde ex?cution utilise la r?gle R2
(? = 0, 65). La troisi?me ex?cution utilise la r?gle R1 avec la strat?gie S2 (?= 0, 65) et la r?gle
R2 (? = 0,65). Pour la seconde piste, nous avons soumis les ex?cutions de deux combinaisons
(COMBI1 et COMBI2) ainsi que du syst?me 3 (d?crit dans la section 3.3). Le nombre de mots cl?s
retourn?s est fix? ? 130% du nombre de mots cl?s de r?f?rence pour COMBI1 et COMBI2 et ?
110% pour le syst?me 3. Ces nombres permettent d?obtenir les meilleurs r?sultats sur l?ensemble
d?entra?nement.
La table 1 pr?sente les r?sultats de nos trois ex?cutions pour la premi?re piste. Les r?sultats
obtenus par les trois ex?cutions sont moins bons que ceux obtenus sur l?ensemble d?entrainement
(f-mesure=0,44 pour la premi?re ex?cution). Nous constatons que la variation du rappel sur les
trois ex?cutions est faible. La chute de la pr?cision pour la troisi?me ex?cution s?explique par
l?application de la r?gle R1 qui limite le nombre de candidats possibles.
Syst?me Pr?cision Rappel f-mesure
1 0,3812 0,4004 0,3906
2 0,3759 0,3948 0,3851
3 0,3343 0,4097 0,3682
TAB. 1 ? R?sultats de nos trois ex?cutions pour la premi?re piste.
La table 2 montre les r?sultats de nos trois ex?cutions pour la seconde piste. Nous pouvons voir
que la performance de COMBI2 est largement en dessous de COMBI1. Nous avions constat? le
ph?nom?ne inverse sur les donn?es d?entra?nement. Ceci est du au fait que le nombre de mots
cl?s retourn?s par COMBI2 peut dans certains cas ?tre inf?rieur au seuil que nous avons fix?. En
effet, l?intersection des listes des 100 meilleurs mots cl?s candidats de chaque syst?me est tr?s
restrainte pour quelque uns des documents de l?ensemble de test. Nous constatons que les scores
du syst?me 3, ayant obtenu les meilleurs r?sultats sur l?ensemble d?entra?nement parmis nos trois
syst?mes, sont faibles en comparaison des deux combinaisons. Ce r?sultat semble indiquer un
probl?me de sur-entra?nement et illustre bien l?utilit? de la combinaison.
La table 3 pr?sente, pour chacune des deux pistes, le classement des diff?rentes ?quipes sur la
base de la meilleure soumission. Notre soumission est class?e au rang 5 sur 10 pour la premi?re
66
Syst?me Pr?cision Rappel f-mesure
COMBI1 0,1949 0,2355 0,2133
COMBI2 0,1788 0,2128 0,1943
Syst?me 3 0,1643 0,1880 0,1753
TAB. 2 ? R?sultats de nos trois ex?cutions pour la seconde piste.
piste et au rang 2 sur 9 pour la seconde piste. Les r?sultats obtenus par l??quipe 16 sont bien
au dessus de toutes les autres ?quipes et montrent qu?une marge de progression importante est
possible pour notre syst?me.
Rang Piste 1 Piste 2
1 ?quipe 16 (0,9488) ?quipe 16 (0,5874)
2 ?quipe 05 (0,7475) ?quipe 06 (0,2133)
3 ?quipe 04 (0,4417) ?quipe 05 (0,2087)
4 ?quipe 02 (0,3985) ?quipe 02 (0,1921)
5 ?quipe 06 (0,3906) ?quipe 01 (0,1901)
6 ?quipe 01 (0,2737) ?quipe 13 (0,1632)
7 ?quipe 13 (0,1378) ?quipe 04 (0,1270)
8 ?quipe 17 (0,1079) ?quipe 17 (0,0895)
9 ?quipe 03 (0,0857) ?quipe 03 (0,0785)
10 ?quipe 18 (0,0428) -
TAB. 3 ? Classement de DEFT 2012 sur la base de la meilleure soumission de chaque ?quipe pour
chacune des deux pistes. Notre classement est indiqu? en gras (?quipe 06).
5 Ce qui n?a pas march?
Nous d?crivons ici les m?thodes qui ont eu un impact nul ou n?gatif sur les r?sultats.
Traits ayant un impact n?gatif sur la performance du syst?me 3 : la dispersion d?un mot cl?
dans le document, mots appartenant ? des phrases contenant des citations, noms des auteurs les
plus cit?s dans le document (sp?cifique aux articles commen?ant par ?as?).
Suppression de la redondance : nous avons constat? un niveau de redondance important
des mots cl?s dans les sorties de nos syst?mes. Par exemple, les mots cl?s ?jardins collectifs?,
?jardins? et ?collectifs? sont tous les trois pr?sents dans le top 10, ce qui fait baisser le rappel.
Plusieurs strat?gies ont ?t? exp?riment?es pour supprimer cette redondance (e.g. suppression
d?un n-gramme si tous les mots qui le composent sont ?galement pr?sents parmi les 10 meilleurs
candidats). Une d?gradation des r?sultats est cependant observ?e indiquant que la strat?gie ?
adopter est d?pendante des documents.
Mod?le de pond?ration ? base de graphe : nous avons impl?ment? l?approche propos?e
dans (Mihalcea et Tarau, 2004). Il s?agit de repr?senter chaque document sous la forme d?un
67
graphe de mots connect?s par des relations de co-occurrences. Des algorithmes de centralit? sont
ensuite appliqu?s pour extraire les mots les plus caract?ristiques. Les r?sultats obtenus par cette
m?thode sont inf?rieurs ? ceux obtenus ? l?aide d?une pond?ration par la mesure T F ? IDF .
6 Conclusions
Nous avons d?crit la participation du LINA ? DEFT 2012. Notre syst?me est le r?sultat de la
combinaison des sorties de trois diff?rentes m?thodes d?extraction de mots cl?s. Les r?sultats
obtenus par ce dernier sont toujours meilleurs que ceux obtenus par chacune des trois m?thodes
individuellement. Pour la seconde piste, notre syst?me s?est class? ? la 2i e`me place sur un total de
9 syst?mes avec une f-mesure de 21,3%.
La strat?gie que nous avons employ?e pour combiner les sorties des diff?rentes m?thodes n?est
cependant pas optimale. Nous envisageons d??tendre ce travail en proposant d?autres strat?gies
comme par exemple l?utilisation d?un meta-classifieur.
R?f?rences
ABEILL?, A., CL?MENT, L. et TOUSSENEL, F. (2003). Building a treebank for French. Treebanks :
building and using parsed corpora, pages 165?188.
BIRD, S. et LOPER, E. (2004). NLTK : The natural language toolkit. In ACL, Barcelone, Espagne.
CHOI, F. Y. Y. (2002). Content-based Text Navigation. Th?se de doctorat, Department of Computer
Science, University of Manchester.
HALL, M., FRANK, E., HOLMES, G., PFAHRINGER, B., REUTEMANN, P. et WITTEN, I. (2009). The weka
data mining software : an update. ACM SIGKDD Explorations Newsletter, 11(1):10?18.
KISS, T. et STRUNK, J. (2006). Unsupervised multilingual sentence boundary detection. Compu-
tational Linguistics, 32(4):485?525.
MEDELYAN, O. et WITTEN, I. H. (2006). Thesaurus based automatic keyphrase indexing. In
Proceedings of the 6th ACM/IEEE-CS joint conference on Digital libraries, JCDL ?06, pages 296?297,
New York, NY, USA. ACM.
MIHALCEA, R. et TARAU, P. (2004). Textrank : Bringing order into texts. In LIN, D. et WU,
D., ?diteurs : Proceedings of EMNLP 2004, pages 404?411, Barcelona, Spain. Association for
Computational Linguistics.
TOUTANOVA, K., KLEIN, D., MANNING, C. et SINGER, Y. (2003). Feature-rich part-of-speech tagging
with a cyclic dependency network. In Proceedings of the 3rd Conference of the North American
Chapter of the ACL (NAACL 2003), pages 173?180. Association for Computational Linguistics.
WITTEN, I. H., PAYNTER, G. W., FRANK, E., GUTWIN, C. et NEVILL-MANNING, C. G. (1999). Kea :
Practical automatic keyphrase extraction. CoRR, cs.DL/9902007.
68
