Linear Encodings of Linguistic Analyses 
Samuel  S. Epste in  
Bel l  Communicat ions  Research  
445 South Street, 2Q-350 
Morr is town,  NJ  07960-1910 
USA 
epstein@ f lash.bel lcore.com 
1. Introduction 
Natural languages contain families of expressions such 
that the number of readings of an expression is an 
exponential function of the length of the expression. 
Two well-known cases involve prepositional phrase 
attachment and coordination. Other cases discussed 
below involve anaphora and relative operator scope. 
For example, in 
(l) John said that Bill said that ... that Harry said 
that he thinks that he thinks that ... that he thinks 
that it is raining. 
each he can refer to any of John, Bill ..... Harly? Thus if 
(1) contains n names and m occurrences of he, this 
sentence has n m readings (assuming that all anaphoric 
relationships are intrasentential). We discuss below 
families of expressions whose ambiguities grow as 
various exponential functions (factorial, Fibonacci, 
Catalan) of expression length. 
It appears, then, that exhaustive linear-time 
processing of natural language is impossible. An 
exponentially ong answer cannot be produced in linear 
time. 2 On the other hand, human processing of natural 
language seems to be at least linearly fast. The ready 
explanation for this is that people do not recover all 
readings of ambiguous expressions. This is clearly 
correct, as far as it goes. 
This paper shows how to encode in linear space the 
exponentially arge numbers of readings associated with 
various families of expressions. The availability of 
these encodings removes an apparent obstacle to 
exhaustive analyses of these expressions in linear time. 
The encodings may thus be useful for practical 
computational purposes. They may also provide a better 
1. (1) is of course highly unnatural in a sense. However, it effectively 
isolates for study a phenomenon that is intrinsic to natural 
language. Similar observations apply to the examples below. 
2. It is of course also the case that an exponentially long answer 
caunot be produced in polynomial time. If the problem cannot be 
reformulated so that answers are not exponentially long, the 
question of tractability does not arise. See \[Garey and Johnson 79\] 
and \[Barton, Berwick, and Ristad 87\] for related iscussions. 
basis than exponential-space encodings for explanations 
of how humans process language. 
For each of the linguistic constructions discussed in 
this paper, there is a simple program that generates 
analyses of the construction. If there are no constraints 
on what counts as a linguistic analysis, then a 
specification of a program, which requires constant 
space, together with a specification of an input 
expression, which requires linear space, could count as a 
linear encoding of an analysis of the input. Intuitively, 
there is a vast qualitative divide between a 
(program,input) pair on one hand, and, for instance, a
forest of constituent structure trees on the other hand. 
More generally, a question arises of how to distinguish 
analyses from procedures that yield analyses. This 
paper will not attempt to answer this question 
definitively. The analyses presented in Sections 2 - 4 all 
satisfy a notion of "legal" analysis that excludes 
(program,input) pairs. Sections 2 and 3 discuss 
polynomial space analyses. Section 4 adds a 
representational device to the repertory of Sections 2 
and 3, so that linear space analyses are possible. Section 
5 infol~mally discusses a variety of issues, including the 
distinction between analysis and procedure. 
2. Analyses in Conjunctive Normal 
Form 
Assume that example (1) involves no ambiguities except 
for antecedents of pronouns. Assume further that the 
length of the analysis of (1), aside from the specification 
of antecedents of pronouns, grows linearly) Let the 
proposition q comprise all aspects of the analysis of (1), 
aside from specifications of antecedents of pronouns. 
Let the proposition p.. comprise the specification that 
the j-th name in (1I 'J is the antecedent of the i-th 
occurrence of he. (For example, Pl,2 comprises the 
3. These assumptions, and similar assumptions for other examples 
below, permit a briefer discussion than would otherwise be 
possible. Reservations about these assumptions do not affect the 
substance of the discussion. Our concern with (1) focuses on 
exponentially growing possibilities for assigning antecedents o
pronouns. 
108 1 
specification that Bill is the antecedent of the most 
shallowly embedded he.) Let n be the number of names 
in (1) and let m be the number of occun'ences of he. 
Then an exhaustive analysis of (1) can take the 
following form: 
(l-a) (q & Pl,~ & P2,1 & "'" & Pro,1 ) v 
(q & Pl,1 & P2,I & "'" & Pro-l,1 & Pra,2 ) v 
(q & Pl,n & P2,n & "'" & Pm,n )
(l-a), which contains n m disjuncts, is in Disjunctive 
Normal Form (DNF). Each disjunct fully specifies a 
possible interpretation of (1). It is an implicit 
assumption in much of the literature that the proper form 
for linguistic analyses is DNF. An analysis in DNF 
amounts to a listing of possible global interpretations. 
(l-a) is logically equivalent to the following 
:aatement in Conjunctive Normal Form (CNF): 
(l-b) q & (Pl,1 v Pl,2 v ... v Pl.n ) & 
(P2,1 v P2,2 v ... v P2,n )
(3) the block in the box on the table ... in the 
kitchen 
As \[Church and Patil 82\] discuss, examples like (3) are 
similar to other structures with systematic attachment 
ambiguities, such as coordination structures. While the 
number of readings of (3)4is thus exponential in the 
length of (3), (3) has an O(n ) length analysis in CNF as 
follows: 
(3-a) q & (Pl,0) & 
(3~a-2) 
(P2,0 v P2,1 ) & 
(3-a-3) 
(P3,0 v P3,1 v P3,2 ) & 
(P3,1 zo P2,1 ) & 
(3-a-4) 
(P4,0 v P4,1 v P4,2 v P4,3 ) & 
(P4,1 D P2,l ) & 
(P4,1 D (P3,I v P3,2)) & 
(P4,2 D P3,2 ) & 
(Pro,1 v Pro.2 v ... v pm,n )
(l-b) contains m+l conjuncts. The length of an 
exhaustive analysis of (1) is exponential in the number 
of pronouns in (1) when the analysis is given in DNF, 
but linear in the number of pronouns when the analysis 
is given in CNF. However, (l--b) is not linear in the 
length of (1), because each of m conjuncts contains n 
clisjuncts, so that a total of mxn literals is required to 
specify anaphoric possibilities. 
The following example has an analysis in DNF that 
grows as the factorial of the length of the input: 
(2) John told Bill that Tom told him that Fred told 
him tha! ... that Jim told him that Harry told him 
that it is raining. 
The first occurrence of him can have John or Bill as 
antecedent. The second occurrence of him can have 
John or Bill or Tom as antecedent, and so on. (2) has an 
obvious analysis in CNF whose length is a quadratic 
function of the length of the input, namely 
(2:a) q & (PL1 v Pl 2 ) &; 
(P2,1 vP2 ,2v ,  -~2,3 )&  
(Pro,1 v Pro,2 v ... v Pm,m+l ) 
where the notation follows the same conventions as in 
(l-a,b). 
The number of readings for the following noun 
phrase grows as the Catalan of the number of 
prepositional phrases: 
(3-a-k) 
(Pk,0 v Pk,1 V ... V Pk,k_l ) &; 
(3-a-k,1) 
(Pk,l D P2,1 ) & 
(Pk,l D (P3,1 v P3,2)) & 
(Pk,1 D (Pk-l,l v Pk-l,2 v ... v Pk_l,k.2)) & 
(3-a-k,m) 
(Pk,m ~ Pm+l,m ) & 
(Pk,,n D (Pm+2,m v Pm+2,m+l)) &
(Pk,m D (Pk-l,m V Pk-l,m+l V ... V Pk_l,k_2)) & 
In (3-a), Pi' comprises the specification that constituent i 
, J  . . . .  attaches to constmmnt j, where the block ~s constituent 0, 
in the box is constituent 1, on the table is constituent 2, 
and so on. Constituent k must attach to some constituent 
that lies to its left. If constituent k attaches to 
2 109 
constituent m, then the constituents between constituent 
m and constituent k cannot attach to constituents to the 
left of constituent m. 4 
For each pair (k,m), the number of atoms in (3-a- 
k,m) is fl(k,m) = ,~ ' i .  fl(k,m) is quadratic in k. For 
each k, then, the number of atoms in (3-a-k) is f2(k) = 
k+ l (k , i ) ,  a cubic function in k. The number of 
atoms in (3-a) (excluding atoms hidden in q) is thus 
=f2( i ) ,  a quartic function in n. (3-a) is certainly not 
the most compressed CNF analysis of (3). It is, 
however, easy to describe. 
Given an exhaustive analysis in DNF, choosing a 
global interpretation requires exactly one operation of 
selecting a disjunct. Foi" (l-b) and (2-a), choosing a 
global interpretation requires a number of selections that 
is linear in the length of the input. I am aware of no 
other reason for preferring DNF to CNF for analyses of 
examples like (1) and (2). In favor of preferring CNF 
there is the practical advantage of polynomial-space 
output, with its implications for speed of processing. 
There is also the possibility of more accurate 
psycholinguistic modeling. It seems likely that people 
make decisions on antecedents for pronouns in examples 
like (1) and (2) locally, on a pronoun-by-pronoun basis, 
and that they do not choose among global analyses. 5 In 
contrast, the conjuncts of (3-a) clearly do not correspond 
one-to-one with processing decisions. Section 4 
discusses an analysis of (3) whose components may 
correspond to local decisions on attachment sites. 
3. Encodings with non-atomic 
propositional constants 
It is possible to get a cubic length analysis of (3) by 
introducing constants tbr non-atomic propositions. For 
m<k, let r. be v Pk_l.k_2 ). K,m (Pk-I m+l V Pk-I m+2 v ... 
Then (3-a-k,m) is equ3)alent to: ' 
(3-b-k,m) (Pk m D Pm+l m ) & 
(Pk,~n D (Pm+2,m v Pm+2,m+t )) & 
(Pkm D (Pk 2 m V rk. 1 m) ) 
(Pklm D (Pk~l',m v ,'k,n~)) 
Of course, the space required to define the r km must 
figure in the space required to encode an analys\[s of (3) 
along the lines of (3-b-k,m). rk,m_ l -= (Pk-l,m v rk,m) , so 
4. (Pl ~ (P2 v ... v pj)) is equivalent to(-'Pl v P2 v ... v pj), SO that 
(3-a) is in CNF. 
5. This is not to suggest that people produce an exhaustive analysis in 
CNF prior to choosing a reading. The hypothesis rather that 
fragments of a CNF representation are produced (in some sense) 
during processing. 
it requires quadratic space to define all the rk. m. A 
revised version of (3) with (3-b-k,m) in place of (3-a- 
k,m) throughout requires cubic space. 6
Tree representations of single readings for examples 
like (3) may be viewed as follows: edges correspond to 
atomic propositions that comprise specifications like 
"constituent i attaches to constituent j" or "constituent i 
projects to constituent j.,,7 A non-terminal node A 
corresponds to a constituent, but also corresponds to the 
conjunction of the atomic propositions that correspond 
to edges that A dominates. Thus the root node of the 
tree corresponds to a proposition that comprises a full 
specification of constituent structure. 
The situation is essentially the same \['or shared 
forests. (\[Tomita 87\] discusses shared forests and 
packed shared forests.) Edges ill shared forests 
correspond to atomic propositions, and non-terminal 
nodes correspond to non-atomic propositions. To 
extend this perspective, shared forests compress the 
information in non-shared forests by exploiting the 
introduction of constants for non-atomic propositions. 
In a shared forest, the subtree that a node dominates is 
written only once. In effect, then, a constant is 
introduced that represents the conjunction that 
corresponds to the node. This constant is a constituent 
of the fornmlas that correspond to superior nodes. 
While shared forests are more compressed than 
unshared forests, the number of nodes in the shared 
forest representation of (3) is still exponential in the 
length of (3). 
In a packed shared forest, a packed node that does 
not dominate any packed nodes corresponds to a 
disjunction of conjunctions of atomic propositions. 
Packed nodes that dominate other packed nodes 
correspond to disjunctions of conjunctions of atomic and 
non-atomic propositions. In effect, for each node 
(packed or non-packed), a constant is introduced that 
abbreviates the formula that corresponds to the node. 
Exploitation of constants for non-atomic propositions 
pemfits more significant compression for packed shared 
forests than for shared forests. The packed root node of 
a packed shared forest for (3) cotxesponds to a 
disjunction of conjunctions whose size in atoms is 
exponential in the length of (3). However, the number 
of nodes of a packed shared forest for (3) goes up as the 
square of the length of (3). The number of edges of the 
packed shared forest (a more authentic measure of the 
size of the forest) goes up as the cube of the length. 
6. Further compression is possible if we allow quantification over 
subscript indices. However, quantification over artifacts of 
representation may uncontroversially involve crossing the divide 
between analysis and procedure. 
7. Details of constituent s ructure are not relevant to the discussion 
here. For example, we will not distinguish "X attaches to V" from 
"X attaches toVP." 
2 
110 3 
4. Encodings that introduce 
structural constants 
A linear length encoding of an analysis of (1) is possible 
if we use the constant A = {John, Bill . . . . .  Harry} in the 
encoding as follows: 
(l-c) q & (antecedent(pronoun l) e A) & 
(antecedent(pronoun 2) e A) & 
(antecedent(pronounm) e A) 
Note that "x ~ Y" is short-hand for the disjunction of the 
statements "x = y," where y ranges over Y, so that (l-c) 
is not very different from (l-b). Examples below 
involve tYeer use of constants that correspond to sets of 
linguistic entities, I will call such constants "structural." 
A linear analysis of (2) is possible if we introduce 
constants A 1 . . . . .  A , where A. = {John, Bill}, A;  = A 1 
u {Tom}, A 3 = A 2 ~ {Fred}, ..l., Am = Am-I U {Jim}: 
(2-b) q & 
(matecedent(pronoun t) E A1) & 
(antecedent(pronoun2) ~ A 2) & 
quantifier Qi takes scope over Qi-I to its immediate left, 
then the quantifier Qi+l to the immediate right of Qi 
cannot take scope over Qi" (See \[Epstein 88\] for a 
discussion of relative operator scope.) It follows that the 
number of relative operator scope readings for (4) grows 
as the Fibonacci of the length of (4). 8 However, a linear 
encoding of an exhaustive analysis of (4) is as follows: 
(4-a) q & 
\[((Q1 > Q2 ) & (L\] = Q2)) v 
((Q2 > Q1 ) & (L1 = T))\[ & 
\[Q1 > Q3 \] & 
\[((L 1 > Q3 ) & (L 2 = Q3)) v 
((Q3 > L1) & (L2 = T))\] & 
\[Qk-2 > Qk+l \] & 
\[Qk-t > Qk+l \] & 
\[((Lk_ 1 > Qk+l ) & (L k = Qk+l)) v 
((Qk+l > Lk-1) & (Lk = T))\] & 
(antecedent(pronounm) E Am) 
Because A. can be defined in terms of Ai_l, only linear 
1 
space is required to define these constants. It is 
convenient o mix definitions of constants with other 
aspects of the encoding of (2), as follows: 
(2-c) q & A a = {John, Bill} & 
(antecedent(pronounl) ~ A i & 
A 2 = (A 1 u {Tom})) & 
(antecedent(pronoun2) E A 2 & 
A 3 = (A 2 u {Fred})) & 
Here q represents aspects of the analysis of (4) aside 
from the specification of relative operator scope, and Qi 
represents the i-th quantifier in (4), reading from the left. 
The L. are introduced constants corresponding to 
1 
quantifiers that can have lower scope than some more 
deeply embedded quantifier. "Q > Q." means that Q. i 
has higher scope than Qj. For all Q, ' '~ < T" is true and 
"Q > T" is false. 9 Note that if we delete from (4-a) 
propositions that assign values to introduced constants, 
such as "(L l = Q2)," the resulting statement is in CNF. 
Section 3 discussed cubic length analyses of (3) with 
propositional constants. (3) has a linear analysis with 
structural constants as follows: 
(antecedent(pronoun .) ~ A . & 
I 11 -2  1111-1 
A m = (Am. 1 u {Jim})) & 
(antecedent(pronoun m) ~ A m ) 
For (2), the introduction of structural constants permits a 
linear encoding. For the following example, the 
introduction of structural constants likewise permits a 
linear encoding: 
(4) Many teachers expect several students to expect 
many teachers to expect several students to ... to 
expect many teachers to expect several students to 
read some book. 
Each quantifier in (4) can take scope over the quantifier 
to its immediate left (if any), and can take scope over the 
quantifier to its immediate right (if any). However, if a 
8. The most deeply embedded clause in (4) has 2 possible relative 
scope readings. The second most deeply embedded clansc in (4) 
has 3 possible relative scope readings (many>several>some, 
many>some>several several>many>some). Let S. be the k-th 
. . . .  k most deeply embedded clause m (4). (S k ts immediately embedded 
in S..;) '  Given that' S_ has a total ofn (relative operator) scope 
read~l~s, and that S ~tas a total of m scope readings then the 
? k - !  " " ' subject of S. can take scope over all the quantifiers in S 
? k+\[ , . k '  accounting f6r n global readings over S..,. Alternatively, the 
subject of S can take scope over the subj\[~ of S Then both 
. k k+. l "  these subjects take scope over all the qu'mtifiers m S . The 
k- l  . second alternative thus accounts for m additional global readings 
over  Sk+ 1. 
9. (4-a) does not explicitly state, for example that Q > Q. but this 
t . 3 '  fact can be derived from (4-a) through apphcatmn of the 
transitivity of relative operator scope? Generally speaking, 
linguistic representations don't explicitly include all their 
consequences. 
4 111 
(3-c) q & 
\[(ap(PP1) = NP) & (AP 1 = NP) & 
(RE 1 = {NP, PP1})I & 
\[(ap(PP2) e REI) & (AP 2 = ap(PP2)) & 
(RE 2 = (RE 1 q" AP2) u {PP2})\] & 
(5-a) q & 
\[(ap(PPt) e {VP, NP}) & (AP t = ap(PP1)) & 
(RE~ = {VP, NP} T AP~) & 
(OG 1 = {VP}- {API))\] & 
\[(ap(PP2) E REI) & (AP 2 = ap(PP2)) & 
(RE 2 = (RE l $ AP2) u {PP2} ) & 
(OG 2 = OG t - {AP2})\] & 
\[(ap(PP k) ~ REk. 1) & (AP k = ap(PPk) & 
(RE k = (REk_ 1 1" APk) u {PPk})\] & 
Here q represents aspects of the analysis of (3) aside 
from the specification of attachment points for the 
prepositional phrases. The desired solutions consist of 
specifications of attachment possibilities, stated in the 
form "ap(PPk) e X" ("attachment point of the k-th PP is 
one of the elements of X' ) in (3~c). The AP k and RE K 
are introduced constants. AP k is the attachment point ot 
10 o PPk" RE k represents the right edge of a constituent 
structure tree for the string consisting of the block and 
the first k PP's. (3-c) is in a sort of relaxed CNF, as 
discussed above in connection with (4-a), and in Section 
5 below. "T" in (3-c) is defined so that RE TAP = {AP} 
u {X ~ RE I X precedes AP}. (When PPk to the right 
of PP. attaches above PP., PP. is not in the right edge of 
1 1 l . 
the resulting structure, and is unavadable for attachment 
by material to the right of PPk.) 
As for (3), the number of readings of the following 
exmnple (from \[Church and Patil 82\]) grows as the 
Catalan of the number of prepositional phrases: 
(5) Put the block in the box on the table ... in the 
kitchen. 
However, there is an important difference between (3) 
and (5). In (3), any number of PP's can attach to the 
block, any number of PP's can attach to the box, and so 
on, No NP in (3) requires complements. (Dr the box 
must attach to the block, but only because the block is 
the only NP that lies to the left of in the box.) In (5), on 
the other hand, put requires one NP argument and one 
PP argument, and cannot accept any other 
complements. 11 An analysis of (5) along the lines of (3- 
c) would incorrectly include readings where more than 
one PP attaches to put, and readings where no PP 
attaches to put. A linear analysis of (5) is as follows: 
10. "PP attaches toPP " really means that PP attaches tothe object of 
i . .  k . i . . the preposmon head of PP. Thts usage permits a brtefer 
? k . discussion than would otherwise be possible. 
1 l. This characterization of put is not strictly speaking correct, but the 
necessary qualifications are irrelevant to the discussion here. 
\[(ap(PP k) E REk_ l) & (AP k = ap(PPk) & 
(RE k = (REk. 1 T APk) L.) {PPk} ) & 
(OG k = OGk. 1 - {APk})\] & 
\[(ap(PPn) E OGn_ l \[\] REn.1)\] 
(5-a) is similar to (3-c), but includes the additional 
constants OG OG is the open (theta-)gnd for the 
k"  k 
substructure corresponding to put the block followed by 
the first k PP's. OG k is either {VP}, if none of the first k 
PP's is attached to V, or is empty Non-empty OG 
" k 
indicates that for each constituent X in OG., some PP., 
. . K 1 
k<i~n, must attach to X. \[\] in (5-a) is defined so that 
A \[\] B is equal to A if A is non-empty, and is otherwise 
equal to B. The final conjunct in (5-a) captures the 
requirement that if none of the first n-1 PP's attaches to 
put, then the final PP must attach to this verb. 
5. Issues 
The example constructions presented above illustrate a 
variety of abstract cases. In (1), local ambiguities are 
independent of each other. The assignment of an 
antecedent o a pronoun in (1) does not affect the 
possibilities of antecedent assignment for other 
pronouns in (1). An analysis of (1) in CNF need not 
include more than one appearance of any literal. (2) is 
similar to (1) in this respect. In (4), local ambiguities 
are interdependent, but local ambiguity possibilities 
depend only on ambiguity possibilities in neighboring 
clauses. There is thus a bound on how many 
ambiguities can interact. In (3), on the other hand, there 
is no such bound. Choosing an attachment site for PP. 
. . . .  J affects the attachment posslbdlttes for PP ..... no matter 
how large k is. (5) is similar to (3), but ats*~ involves a 
global filter associated with the verb put. (3-c) and (5-a) 
employ a richer repertory of operators on structural 
constants (-, , q') than is found in (l-c), (2-b), (2-c), and 
(4-a). 
( l-b) may qualify relatively easily as an exhaustive 
analysis of (1), according to a common conception of 
what constitutes an analysis. (3-c), on the other hand, 
appears to have some of the ealxnarks of a procedure. 
The similarity of introduced constants to local variables 
is obvious. In particular, the constants AP; and RE; of 
112 c 5 
(3-c) conld be replaced with two local variables AP and 
RE that receive destructive assignment. (3--c) also 
en'@oys the operators "$" and "~", which might be 
regarded as corresponding toprocedures. Whether (3--c) 
is an analysis or a procedure for computing analyses is 
ultimately a matter of selecting a definition for 
"linguistic analysis." 
Criteria Io ta  successful definition of "linguistic 
analysis" might appeal to psychological reality. One 
possible requirement is that components of analyses 
correspond to partial analyses built during human 
processing. When definitions of constants (assignments 
to local variables) are blended into what is otherwise a
CNF formula, as in (2-c), (3-c), (4-a), and (5-a), the 
result might be called "relaxed CNF." Somewhat more 
precisely, suppose that a formula in "relaxed CNF" is a 
conjunction of "relaxed isjunctions," where a "relaxed 
disjunction" is the conjunction of a "generalized 
disjunction" with an "assignment formula." A 
"generalized disjunction" rnay be either a disjunction of 
atoms, or a statement of the form "x c A." An 
"assignment formula" is a conjunction of statements hat 
assign values to constants. Given such a relaxed CNF 
formula as an exhaustive analysis, obtaining an analysis 
of a single reading requires for' each generalized 
disjunction the choice of a disjunct or the selection of an 
element. Such single--reading analyses may be produced 
by deterministic variants of non-detemfinistic processing 
models that produce exhaustive analyses in relaxed 
CNF. Relaxed CNF is compatible with a variety of 
processing models. For example, a component of the 
form "x e A" might be produced before components 
that specify the corrtents of A. 
Recent work on Kolmogorov complexity might 
provide alternative criteria for the definition of 
"linguistic analysis." (\[Li and Vitffnyi 89\] is a recent 
survey ol' work on Kolmogorov complexity.) In 
particular, notions of time-bounded algorithmic 
complexity, such as the "logical depth" of \[Bennett 88\], 
may be relevant. Following a third alternative, a 
satisfactory definition of "analysis" may involve a 
correspondence principle, along lhe lollowing lines: 
N 13" i t  every component of a l%al analysis specifically 
mentions one or more components of the input. For this 
to work, "component" and "mention" themselves require 
appropriate definitions. Arbitrary fragments of analyses 
cannot count as "components." Mention" should be 
transitive. 
Analyses in relaxed CNF may be more compatible 
with principle-based grammars than are tree-based 
analyses. (\[Chomsky 81\] is the seminal work on 
principle-based grammars.) Constituent structure does 
not occupy as central a place in the principleobased 
paradigm as in other' grammatical paradigms. Each 
generalized isjunction (or its deterministic counterpart) 
supplies a piece of information about tire analyzed 
expression. Assignment formulas capture logical 
dependencies among thcse pieces. Each of the examples 
in this oar~er illustrates a sinele Nlenomenon. Relaxed 
CNF can also capture interactions among ptmn~mmr~a. 
Analyses like (3-c) are probably less easily readable 
than packed shared forests. Full analyses that specify 
constituent structure information together with relative 
operator scope information, information on anaphora, 
and so on, will be even less readable. It may be possible 
to devise a more graphically oriented notation for linear 
encodings of linguistic analyses. 
Wlmtever cozrception of "linguistic analysis" may 
ultimately prove most useful, it seems clear that working 
with relaxed Conjunctive Norrnal Form has advantages 
over working with Disjunctive Normal Form lor 
computational pplications. Relaxed CNF also appears 
to have advantages over DNF for psycholinguistic 
modeling. Introduced constants (local variables) have 
obvious utility in implementations. They may also play 
a role in human processing of language. In particular, as 
human processing proceeds, explicit details of 
previously encountered structure may recede into the 
background yet remain accessible. 
Acknowledgments 
1 am indebted for comments and discussions to Steven 
Abney, Yves Caseau, and Andrew Ogielski. 
Responsibility tbr errors is entirely mine. 
References 
E. Barton, R. Berwick, and E. Ristad, Computational 
Complexity and Natural Ixmguage, M_IT Press, 
Cmnbridge, Massachusetts, 1987. 
C. Bennett, "Logical Depth and Physical Complexity," 
in R. Herken (ed.), The Universal J~ring Machine; A 
Half-Century Survey, pp. 227-258, Oxford University 
Press, Oxford, 1988. 
N. Chomsky, Lectures on Government and Binding, 
Foris Publications, Dordrecht, 1981. 
K. Church and R. Patil, "Coping with Syntactic 
Ambiguity or How to Put the Block in the Box on the 
Table," American Journal of Computational 
Linguistics, 8:3-4, pp. 139-:149, 1982. 
S. Epstein, "Principle-Based Interpretation of Natural 
Language Quantifiers," Proceedings of the Seventh 
National Conference on Artificial Intelligence (AAAI- 
88), pp. 718~723, 1988. 
M. Garey and D. Johnson, Computers and Intractability, 
W. H. Freeman, San Francisco, 1979. 
M. Li and P. Vitgnyi, Kolmogorov Complexity and Its 
Applications (Revised Version), Report CS-R8901, 
Centre tor Mathematics and Computer Science, 
Amsterdam, 1989. 
M. Tomita, "An Efficient Augmented-Context-Free 
Parsing Algorithm," Computational Linguistics, 13:1-2, 
pp. 31-46, 1987. 
6 113  
