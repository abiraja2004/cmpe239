Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 187?195,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
The Benefits of a Model of Annotation
Rebecca J. Passonneau
Center for Computational Learning Systems
Columbia University
becky@ccls.columbia.edu
Bob Carpenter
Department of Statistics
Columbia University
carp@alias-i.com
Abstract
This paper presents a case study of a
difficult and important categorical anno-
tation task (word sense) to demonstrate
a probabilistic annotation model applied
to crowdsourced data. It is argued that
standard (chance-adjusted) agreement lev-
els are neither necessary nor sufficient
to ensure high quality gold standard la-
bels. Compared to conventional agree-
ment measures, application of an annota-
tion model to instances with crowdsourced
labels yields higher quality labels at lower
cost.
1 Introduction
The quality of annotated data for computational
linguistics is generally assumed to be good enough
if a few annotators can be shown to be consistent
with one another. Metrics such as pairwise agree-
ment and agreement coefficients measure consis-
tency among annotators. These descriptive statis-
tics do not support inferences about corpus quality
or annotator accuracy, and the absolute values one
should aim for are debatable, as in the review by
Artstein and Poesio (2008). We argue that high
chance-adjusted inter-annotator agreement is nei-
ther necessary nor sufficient to ensure high qual-
ity gold-standard labels. Agreement measures re-
veal little about differences among annotators, and
nothing about the certainty of the true label, given
the observed labels from annotators. In contrast, a
probabilistic model of annotation supports statis-
tical inferences about the quality of the observed
and inferred labels.
This paper presents a case study of a particu-
larly thorny annotation task that is of widespread
interest, namely word-sense annotation. The items
that were annotated are occurrences of selected
words in their sentence contexts, and the annota-
tion labels are WordNet senses (Fellbaum, 1998).
The annotations, collected through crowdsourc-
ing, consist of one WordNet sense for each item
from up to twenty-five different annotators, giv-
ing each word instance a large set of labels. Note
that application of an annotation model does not
require this many labels for each item, and crowd-
sourced annotation data does not require a prob-
abilistic model. This case study, however, does
demonstrate a mutual benefit.
A highly certain ground truth label for each an-
notated instance is the ultimate goal of data anno-
tation. Many issues, however, make this compli-
cated for word sense annotation. The number of
different senses defined for a word varies across
lexical resources, and pairs of senses within a sin-
gle sense inventory are not equally distinct (Ide
and Wilks, 2006; Erk and McCarthy, 2009). A
previous annotation effort using WordNet sense la-
bels demonstrates a great deal of variation across
words (Passonneau et al, 2012b). On over 116
words, chance-adjusted agreement ranged from
very high to chance levels. As a result, the ground
truth labels for many words are questionable. On a
random subset of 45 of the same words, the crowd-
sourced data presented here (available as noted be-
low) yields a certainty measure for each ground
truth label indicating high certainty for most in-
stances.
2 Chance-Adjusted Agreement
Current best practice for collecting and curating
annotated data involves iteration over four steps,
or variations of them: 1) design or redesign the
annotation task, 2) write or revise guidelines in-
187
structing annotators how to carry out the task, pos-
sibly with some training, 3) have two or more an-
notators work independently to annotate a sample
of data, and 4) measure the interannotator agree-
ment on the data sample. Once the desired agree-
ment has been obtained, a gold standard dataset
is created where each item is annotated by one
annotator. As noted in the introduction, how
much agreement is sufficient has been much dis-
cussed (Artstein and Poesio, 2008; di Eugenio and
Glass, 2004; di Eugenio, 2000; Bruce and Wiebe,
1998). The quality of the gold standard is not ex-
plicitly measured. Nor is the accuracy of the an-
notators. Since there are many ways to be inaccu-
rate, and only one way to be accurate, it is assumed
that if annotators agree, then the annotation must
be accurate. This is often but not always correct.
If two annotators do not agree well, this method
does not identify whether one annotator is more
accurate than the other. For the individual items
they disagree on, no information is gained about
the true label.
To get a high level sense of the limitations of
agreement metrics, we briefly discuss how they
are computed and what they tell us. For a com-
mon notation, let i ? 1:I represent the set of all
items, j ? 1:J all the annotators, k ? 1:K all the
label classes in a categorical labeling scheme (e.g.,
word senses), and yi,j ? 1:K the observed labels
from annotator j for item i (assuming every anno-
tator labels every item exactly once; we relax this
restriction later).
Agreement: Pairwise agreement Am,n between
two annotators m,n ? 1:J is defined as the pro-
portion of items 1:I for which the annotators sup-
plied the same label,
Am,n = 1I
?I
i=1 I(yi,m = yi,n),
where the indicator function I(s) = 1 if s is true
and 0 otherwise. Am,n is thus the maximum like-
lihood estimate that annotator m and n will agree.
Pairwise agreement can be extended to the en-
tire pool of annotators by averaging over all
(J
2
)
pairs,
A = 1
(J2)
?J
m=1
?J
n=m+1Am,n.
By construction, Am,n ? [0, 1] and A ? [0, 1].
Pairwise agreement does not take into account
the proportion of observed annotation values from
1:K. As a simple expected chance of agreement, it
provides little information about the resulting data
quality.
Chance-Adjusted Agreement: An agreement
coefficient, such as Cohen?s ? (Cohen, 1960) or
Krippendorff?s ? (Krippendorff, 1980), measures
the proportion of observed agreements that are
above the proportion expected by chance. Given
an estimate Am,n of the probability that two an-
notators m,n ? 1:J will agree on a label and
an estimate of the probability Cm,n that they
will agree by chance, the chance-adjusted inter-
annotator agreement coefficient IAm,n ? [?1, 1]
is defined by
IAm,n =
Am,n?Cm,n
1?Cm,n
.
For Cohen?s ? statistic, chance agreement is de-
fined to take into account the prevalence of the
individual labels in 1:K. Specifically, it is de-
fined to be the probability that a pair of labels
drawn at random for two annotators agrees. There
are two common ways to define this draw. The
first assumes each annotator draws uniformly at
random from her set of labels. Letting ?j,k =
1
I
?I
i=1 I(yi,j = k) be the proportion of the label k
in annotator j?s labels, this notion of chance agree-
ment for a pair of annotators m,n is estimated as
the sum over 1:K of the products of their propor-
tions ?:
Cm,n =
?K
k=1 ?m,k ? ?n,k.
Another computation of chance agreement in wide
use assumes each annotator draws uniformly at
random from the pooled set of labels from all an-
notators (Krippendorff, 1980). Letting ?k be the
proportion of label k in the entire set of labels, this
alternative estimate, C ?m,n =
?K
k=1 ?
2
k, does not
depend on the identity of the annotators m and n.
An inter-annotator agreement statistic like ?
suffers from multiple shortcomings. (1) Agree-
ment statistics are intrinsically pairwise, although
one can compare to a voted consensus or aver-
age over multiple pairwise agreements. (2) In
agreement-based analyses, two wrongs make a
right; if two annotators both make the same mis-
take, they agree. If annotators are 80% accurate
on a binary task, chance agreement on the wrong
category occurs at a 4% rate. (3) Chance-adjusted
agreement reduces to simple agreement as chance
agreement approaches zero. When chance agree-
ment is high, even high-accuracy annotators can
188
have low chance-adjusted agreement. For ex-
ample, in a binary task with 95% prevalence of
one category, two 90% accurate annotators have
a chance-adjusted agreement of 0.9?(.95
2+.052)
1?(.952+.052) =
?.053. Thus high chance-adjusted inter-annotator
agreement is not a necessary condition for a high-
quality corpus. (4) Inter-annotator agreement
statistics implicitly assume annotators are unbi-
ased; if they are biased in the same direction, as we
show they are for the sense data considered here,
then agreement is an overestimate of their accu-
racy. In the extreme case, in a binary labeling task,
two adversarial annotators who always provide the
wrong answer have a chance-adjusted agreement
of 100%. (5) Item-level effects such as difficulty
can inflate levels of agreement-in-error. For ex-
ample, hard-to-identify names in a named-entity
corpus have correlated false negatives among an-
notators, leading to higher agreement-in-error than
would otherwise be expected. (6) Inter-annotator
agreement statistics are rarely computed with con-
fidence intervals, which can be quite wide even
under optimistic assumptions of no annotator bias
or item-level effects. In a sample of MASC word
sense data, 100 annotations by 80% accurate an-
notators produce a 95% interval for accuracy of
+/- 6%. Agreement statistics have even wider er-
ror bounds. This introduces enough uncertainty to
span the rather arbitrary decision boundaries for
acceptable agreement.
Model-Based Inference: In contrast to agreement
metrics, application of a model of annotation can
provide information about the certainty of param-
eter estimates. The model of annotation presented
in the next section includes as parameters the true
categories of items in the corpus, and also the
prevalence of each label in the corpus and each
annotator?s accuracies and biases by category.
3 A Probabilistic Annotation Model
A probabilistic model provides a recipe to ran-
domly ?generate? a dataset from a set of model
parameters and constants.1 The utility of a math-
ematical model lies in its ability to support mean-
ingful inferences from data, such as the true preva-
lence of a category. Here we apply the probabilis-
tic model of annotation introduced in (Dawid and
Skene, 1979); space does not permit detailed dis-
1In a Bayesian setting, the model parameters are them-
selves modeled as randomly generated from a prior distribu-
tion.
n iin jjn yn
1 1 1 4
2 1 3 1
3 192 17 5
...
...
...
...
Table 1: Table of annotations y indexed by word
instance ii and annotator jj.
cussion here of the inference process (this will be
provided in a separate paper that is currently in
preparation). Dawid and Skene used their model
to determine a consensus among patient histories
taken by multiple doctors. We use it to estimate
the consensus judgement of category labels based
on word sense annotations provided by multiple
Mechanical Turkers. Inference is driven by accu-
racies and biases estimated for each annotator on
a per-category basis.
Let K be the number of possible labels or cate-
gories for an item, I the number of items to anno-
tate, J the number of annotators, and N the total
number of labels provided by annotators, where
each annotator may label each instance zero or
more times. Each annotation is a tuple consist-
ing of an item ii ? 1:I , an annotator jj ? 1:J ,
and a label y ? 1:K. As illustrated in Table 1, we
assemble the annotations in a database-like table
where each row is an annotation, and the values in
each column are indices over the item, annotator,
and label. For example, the first two rows show
that on item 1, annotators 1 and 3 assigned labels
4 and 1, respectively. The third row says that for
item 192 annotator 17 provided label 5.
Dawid and Skene?s model includes parameters
? zi ? 1:K for the true category of item i,
? pik ? [0, 1] for the probability that an item is
of category k, subject to
?K
k=1 pik = 1, and
? ?j,k,k? ? [0, 1] for the probabilty that annota-
tor j will assign the label k? to an item whose
true category is k, subject to
?K
k?=1 ?j,k,k? =
1.
The generative model first selects the true cate-
gory for item i according to the prevalence of cat-
egories, which is given by a Categorical distribu-
tion,2
zi ? Categorical(pi).
2The probability of n successes inm trials has a binomial
distribution, with each trial (m=1) having a Bernoulli dis-
tribution. Data with more than two values has a multinomial
189
Word Pos Senses ? Agreement
curious adj 3 0.94 0.97
late adj 7 0.84 0.89
high adj 7 0.77 0.91
different adj 4 0.13 0.60
severe adj 6 0.05 0.32
normal adj 4 0.02 0.38
strike noun 7 0.89 0.93
officer noun 4 0.85 0.91
player noun 5 0.83 0.93
date noun 8 0.48 0.58
island noun 2 0.10 0.78
success noun 4 0.09 0.39
combination noun 7 0.04 0.73
entitle verb 3 0.99 0.99
mature verb 6 0.86 0.96
rule verb 7 0.85 0.90
add verb 6 0.55 0.72
help verb 8 0.26 0.58
transfer verb 9 0.22 0.42
ask verb 7 0.10 0.37
justify verb 5 0.04 0.82
Table 2: Agreement results for MASC words with
the three highest and lowest ? scores, by part of
speech, along with additional words discussed in
the text (boldface).
The observed labels yn are generated based on
annotator jj[n]?s responses ?jj[n], z[ii[n]] to items
ii[n] whose true category is zz[ii[n]],
yn ? Categorical(?jj[n], z[ii[n]]).
We use additively smoothed maximum likelihood
estimation (MLE) to stabilize inference. This is
equivalent to maximum a posteriori (MAP) estima-
tion in a Bayesian model with Dirichlet priors,
?j,k ? Dirichlet(?k) pi ? Dirichlet(?).
The unsmoothed MLE is equivalent to the MAP es-
timate when ?k and ? are unit vectors. For our
experiments, we added a tiny fractional count to
unit vectors, corresponding to a very small degree
of additive smoothing applied to the MLE.
4 MASC Word Sense Sentence Corpus
MASC (Manually Annotated SubCorpus) is a very
heterogeneous 500,000 word subset of the Open
American National Corpus (OANC) with 16 types
of annotation.3 MASC contains a separate word
sense sentence corpus for 116 words nearly evenly
distribution (a generalization of the binomial). Each trial then
results in one of k outcomes with a categorical distribution.
3Both corpora are available from http://www.anc.
org. The crowdsourced MASC words and labels will also
be available for download.
balanced among nouns, adjectives and verbs (Pas-
sonneau et al, 2012a). Each sentence is drawn
from the MASC corpus, and exemplifies a partic-
ular word form annotated for a WordNet sense.
To motivate our aim, which is to compare MASC
word sense annotations with the annotations we
collected through crowdsourcing, we review the
MASC word sense corpus and some of its limita-
tions.
College students from Vassar, Barnard, and
Columbia were trained to carry out the MASC word
sense annotation (Passonneau et al, 2012a). Most
annotators stayed with the project for two to three
years. Along with general training in the anno-
tation process, annotators trained for each word
on a sample of fifty sentences to become famil-
iar with the sense inventory through discussion
with Christiane Fellbaum, one of the designers
of WordNet, and if needed, to revise the sense
inventory for inclusion in subsequent releases of
WordNet. After the pre-annotation sample, an-
notators worked independently to label 1,000 sen-
tences for each word using an annotation tool that
presented the WordNet senses and example us-
ages, plus four variants of none of the above. Pas-
sonneau et al describe the training and annotation
tools in (2012b; 2012a). For each word, 100 of the
total sentences were annotated by three or four an-
notators for assessment of inter-annotator reliabil-
ity using pairwise agreement and Krippendorff?s
?.
The MASC agreement measures varied widely
across words. Table 2 shows for each part of
speech the words with the three highest and three
lowest ? scores, along with additional words ex-
emplified below (boldface).4 The ? values in col-
umn 2 range from a high of 0.99 (for entitle, verb,
3 senses) to a low of 0.02 (normal, adjective, 3
senses). Pairwise agreement (column 3) has simi-
larly wide variation. Passonneau et al (2012b) ar-
gue that the differences were due in part to the dif-
ferent words: each word is a new annotation task.
The MASC project deviated from the best prac-
tices described in section 2 in that there was no
iteration to achieve some threshold of agreement.
All annotators, however, had at least two phases
of training. Table 2 illustrates that annotators can
agree on words with many senses, but at the same
time, there are many words with low agreement.
4This table differs from a similar one Passonneau et al
give in (2012b) due to completion of more words and other
updates.
190
Even with high agreement, the measures reported
in Table 2 provide no information about word in-
stance quality.
5 Crowdsourced Word Sense Annotation
Amazon Mechanical Turk is a venue for crowd-
sourcing tasks that is used extensively in the NLP
community (Callison-Burch and Dredze, 2010).
Human Intelligence Tasks (HITs) are presented to
turkers by requesters. For our task, we used 45
randomly selected MASC words, with the same
sentences and WordNet senses the trained MASC
annotators used. Given our 1,000 instances per
word, for a category whose prevalence is as low
as 0.10 (100 examples expected), the 95% interval
for observed examples, assuming examples are in-
dependent, will be 0.10 ? 0.06. One of our future
goals for this data is to build item difficulty into the
annotation model, so we collected 20 to 25 labels
per item to get reasonable confidence intervals for
the true label. This will also sharpen our estimates
of the true category significantly, as estimated er-
ror goes down as 1/
?
n with n independent anno-
tations; confidence intervals must be expanded as
correlation among annotator responses increases
due to annotator bias or item-level effects such as
difficulty or subject matter.
In each HIT, turkers were presented with ten
sentences for each word, with the word?s senses
listed below each sentence. Each HIT had a short
paragraph of instructions indicating that turkers
could expect their time per HIT to decrease as their
familiarity with a word?s senses increased (we
wanted multiple annotations per turker per word
for tighter estimates of annotator accuracies and
biases).
To insure a high proportion of instances with
high quality inferred labels, we piloted the HIT de-
sign and payment regimen with two trials of two
and three words each, and discussed both with
turkers on the Turker Nation message board. The
final procedure and payment were as follows. To
avoid spam workers, we required turkers to have
a 98% lifetime approval rating and to have suc-
cessfully completed 20,000 HITs. Our HITs were
automatically approved after fifteen minutes. We
considered manual approval and programming a
more sophisticated approval procedure, but both
were deemed too onerous given the scope of
our task. Instead, we monitored performance of
turkers across HITs by comparing each individ-
ual turker?s labels to the current majority labels.
Turkers with very poor performance were warned
to take more care, or be blocked from doing fur-
ther HITs. Of 228 turkers, five were blocked, with
one subsequently unblocked. The blocked turker
data is included in our analyses and in the full
dataset, which will be released in the near future;
the model-based approach to annotation is effec-
tive at adjusting for inaccurate annotators.
6 Annotator Accuracy and Bias
Through maximum likelihood estimation of the
parameters of the Dawid and Skene model, an-
notators? accuracies and error biases can be esti-
mated. Figure 1a) shows confusion matrices in the
form of heatmaps that plot annotator responses by
the estimated true labels for four of the 57 annota-
tors who contributed labels for add-v (the affixes
-v and -n represent part of speech). This word
had a reliability of ?=0.56 for four trained MASC
annotators on 100 sentences and pairwise agree-
ment=0.73. Figure 1b) shows heatmaps for four of
the 49 annotators on help-v, which had a reliability
of ?=0.26 for the MASC annotators, with pairwise
agreement=0.58. As indicated in the figure keys,
darker cells have higher probabilities. Perfect ac-
curacy of annotator responses (agreement with the
inferred reference label) would yield black squares
on the diagonal, with all the off-diagonal squares
in white.
The two figures show that the turkers were
generally more accurate on add-v than on help-
v, which is consistent with the differences in the
MASC agreement on these two words. In contrast
to the knowledge gained from agreement metrics,
inference based on the annotation model provides
estimates of bias towards specific category values.
Figure 1a shows the bias of these annotators to
overuse WordNet sense 1 for help-v; bias appears
in the plots as an uneven distribution of grey boxes
off the main diagonal. Further, there were no as-
signments of senses 6 or 8 for this word. The fig-
ures provide a succinct visual summary that there
were more differences across the four annotators
for help-v than for add-v, with more bias towards
overuse of not only sense 1, but also senses 2 (an-
notators 8 and 41) and 3 (annotator 9). When an-
notator 8 uses sense 1, the true label is often sense
6, thus illustrating how annotators provide infor-
mation about the true label even from inaccurate
responses.
191
(a) Four of 57 annotators for add-v
(b) Four of 49 annotators for help-v
Figure 1: Heatmaps of annotators? accuracies and biases
For the 45 words, average accuracies per word
ranged from 0.05 to 0.86, with most words show-
ing a large spread. Examination of accuracies by
sense shows that accuracy was often highest for
the more frequent senses. Accuracy for add-v
ranged from 0.25 to 0.73, but was 0.90 for sense
1, 0.79 for sense 2, and much lower for senses
6 (0.29) and 7 (0.19). For help-v, accuracy was
best on sense 1 (0.73), which was also the most
frequent, but it was also quite good on sense 4
(0.64), which was much less frequent. Accuracies
on senses of help-v ranged from 0.11 (senses 5, 7,
and other) to 0.73 (sense 1).
7 Estimates for Prevalence and Labels
That the Dawid and Skene model allows an-
notators to have distinct biases and accuracies
should match the intuitions of anyone who has
performed annotation or collected annotated data.
The power of their parameterization, however,
shows up in the estimates their model yields for
category prevalence (rate of each category) and for
the true labels on each instance. Figure 2 con-
trasts five ways to estimate the sense prevalence
of MASC words, two of which are based on models
estimated via MLE. The MLE estimates each have
an associated probability, thus a degree of cer-
tainty, with more certain estimates derived from
the larger sets of crowdsourced labels (AMT MLE).
MASC Freq is a simple ratio. Majority voted labels
tend to be superior to single labels, but do not take
annotators? biases into account.
The plots for the four words in Figure 2 are or-
dered by their ? scores from four trained MASC
annotators (see Table 2). There is a slight trend
for the various estimates to diverge less on words
where agreement is higher. The notable result,
however, is that for each word, the plot demon-
strates one or more senses where the AMT MLE es-
timate differs markedly from all other estimates.
For add-v, the AMT MLE estimate for sense 1 is
much lower (0.51) than any of the other measures
(0.61-0.64). For date-n, the AMT MLE estimate for
sense 4 is much closer to the other estimates than
AMT Maj, which sugggests that some AMT an-
notators are baised against sense 4. The AMT MLE
estimates for senses 6 and 7 are quite distinct. For
help-v, the AMT MLE estimates for senses 1 and 6
are also very distinct. For ask-v, there are more
differences across all estimates for senses 2 and 4,
with the AMT MLE estimate neither the highest nor
the lowest.
The estimates of label quality on each item are
perhaps the strongest reason for turning to model-
based approaches to assess annotated data. For the
same four words discussed above, Table 3 shows
the proportion of all instances that had an esti-
mated true label where the label probability was
greater than or equal to 0.99. For these words with
? scores ranging from 0.10 (ask-v) to 0.55 (add-v),
the proportion of very high quality inferred true
labels ranges from 81% to 94%. Even for help-
v, of the remaining 19% of instances, 13% have
probabilities greater than 0.75. Table 3 also shows
192
0.00
0.10
0.20
0.30
0.40
0.50
0.60
Other Sense 1 Sense 2 Sense 3 Sense 4 Sense 5 Sense 6
add-v MASC Freq
MASC Maj
MASC MLE
AMT Maj
AMT MLE
(a) add-v (? = 0.55, agreement=0.72)
0.00
0.10
0.20
0.30
0.40
0.50
0.60
Other Sense
1
Sense
2
Sense
3
Sense Sense
5
Sense
6
Sense
7
Sense
8
date -n MASC Freq
MASC Maj
MASC MLE
AMT Maj
AMT MLE
(b) date-n (? = 0.48, agreement=0.58)
0.00
0.10
0.20
0.30
0.40
0.50
0.60
Other Sense
1
Sense
2
Sense
3
Sense
4
Sense
5
Sense
6
Sense
7
Sense
8
hel p -v MASC Freq
MASC Maj
MASC MLE
AMT Maj
AMT MLE
(c) help-v (? = 0.26, agreement=0.58)
0.00
0.10
0.20
0.30
0.40
0.50
0.60
Other Sense 1 Sense 2 Sense 3 Sense 4 Sense 5 Sense 6 Sense 7
ask -v MASC Freq
MASC Maj
MASC MLE
AMT Maj
AMT MLE
(d) ask-v (? = 0.10, agreement=0.37)
Figure 2: Prevalence estimates for 4 MASC words; (MASC Freq) frequency of each sense in ? 1, 000
singly-annotated instances from the trained MASC annotators; (MASC Maj) frequency of majority vote
sense in ?100 instances annotated by four trained MASC annotators; (MASC MLE) estimated probability
of each sense in the same 100 instances annotated by four MASC annotators, using MLE; (AMT Maj)
frequency of each majority vote sense for ? 1000 instances annotated by ? 25 turkers; (AMT MLE)
estimated probability of each sense in the same ?1000 instances annotated by ?25 turkers, using MLE
Sense k ? 0.99 Prop.
0 9 0.01
1 461 0.48
2 135 0.14
3 107 0.11
4 50 0.05
5 50 0.05
6 93 0.10
SubTot 905 0.94
Rest 62 0.06
(a) add-v: 94%
Sense k ? 0.99 Prop.
0 19 0.02
1 68 0.07
2 19 0.02
3 83 0.09
4 173 0.18
5 190 0.20
6 133 0.14
7 236 0.25
8 5 0.01
SubTot 926 0.97
Rest 33 0.03
(b) date-n: 97%
Sense k ? 0.99 Prop.
0 0 0.00
1 279 0.30
2 82 0.09
3 201 0.21
4 24 0.03
5 0 0.00
6 169 0.18
7 0 0.00
8 5 0.01
SubTot 760 0.81
Rest 180 0.19
(c) help-v: 81%
Sense k ? 0.99 Prop.
0 6 0.01
1 348 0.36
2 177 0.18
3 9 0.01
4 251 0.26
5 0 0
6 0 0
7 6 0.01
8 6 0.01
SubTot 803 0.83
Rest 163 0.17
(d) ask-v: 83%
Table 3: Proportion of high quality labels per word
193
that the high quality labels for each word are dis-
tributed across many of the senses. Of the 45
words studied here, 22 had ? scores less than 0.50
from the trained annotators. For 42 of the same
45 words, 80% of the inferred true labels have a
probability higher than 0.99.
In contrast to current best practices, an annota-
tion model yields far more information about the
most essential aspect of annotation efforts, namely
how much uncertainty is associated with each gold
standard label, and how the uncertainty is dis-
tributed across other possible label categories for
each instance. An equally important benefit comes
from a comparison of the cost per gold standard
label. Over the course of a five-year period that
included development of the infrastructure, the
undergraduates who annotated MASC words were
paid an estimated total of $80,000 for 116 words
? 1000 sentences per word, which comes to a unit
cost of $0.70 per ground truth label. In a 12 month
period with 6 months devoted to infrastructure and
trial runs, we paid 224 turkers a total of $15,000
for 45 words? 1000 sentences per word, for a unit
cost of $0.33 per ground truth label. In short, the
AMT data cost less than half the trained annotator
data.
8 Related Work
The model proposed by Dawid and Skene (1979)
comes out of a long practice in epidemiology
to develop gold-standard estimation. Albert and
Dodd (2008) give a relevant discussion of dis-
ease prevalence estimation adjusted for accuracy
and bias of diagnostic tests. Like Dawid and
Skene (1979), Smyth (1995) used unsupervised
methods to model human annotation of craters on
images of Venus. In the NLP literature, Bruce
and Wiebe (1999) and Snow et al (2008) use
gold-standard data to estimate Dawid and Skene?s
model via maximum likelihood; Snow et al show
that combining noisy crowdsourced annotations
produced data of equal quality to five distinct pub-
lished gold standards. Rzhetsky et al (2009) and
Whitehill et al (2009) estimate annotation mod-
els without gold-standard supervision, but nei-
ther models annotator biases, which are criti-
cal for estimating true labels. Klebanov and
Beigman (2009) discuss censoring uncertain items
from gold-standard corpora. Sheng et al (2008)
apply similar models to actively select the next la-
bel to elicit from annotators. Smyth et al (1995),
Rogers et al (2010), and Raykar et al (2010)
all discuss the advantages of learning and evalu-
ation with probabilistically annotated corpora. By
now crowdsourcing is so widespread that NAACL
2010 sponsored a workshop on ?Creating Speech
and Language Data With Amazons Mechanical
Turk? and in 2011, TREC added a crowdsourcing
track.
9 Conclusion
The case study of word sense annotation presented
here demonstrates that in comparison to current
practice for assessment of annotated corpora, an
annotation model applied to crowdsourced labels
provides more knowledge and higher quality gold
standard labels at lower cost. Those who would
use the corpus for training benefit because they
can differentiate high from low confidence la-
bels. Cross-site evaluations of word sense dis-
ambiguation systems could benefit because there
are more evaluation options. Where the most
probable label is relatively uncertain, systems can
be penalized less for an incorrect but close re-
sponse (e.g., log loss). Systems that produce sense
rankings for each instance could be scored us-
ing metrics that compare probability distributions,
such as Kullbach-Leibler divergence (Resnik and
Yarowsky, 2000). Wider use of annotation mod-
els should lead to more confidence from users in
corpora for training or evaluation.
Acknowledgments
The first author was partially supported by from
NSF CRI 0708952 and CRI 1059312, and the
second by NSF CNS-1205516 and DOE DE-
SC0002099. We thank Shreya Prasad for data
collection, Mitzi Morris for feedback on the paper,
Marilyn Walker for advice on Mechanical Turk,
and Nancy Ide, Keith Suderman, Tim Brown and
Mitzi Morris for help with the sentence data.
References
Paul S. Albert and Lori E. Dodd. 2008. On esti-
mating diagnostic accuracy from studies with mul-
tiple raters and partial gold standard evaluation.
Journal of the American Statistical Association,
103(481):61?73.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
194
Rebecca F. Bruce and Janyce M. Wiebe. 1998. Word-
sense distinguishability and inter-coder agreement.
In Proceedings of Empirical Methods in Natural
Language Processing.
Rebecca F. Bruce and Janyce M. Wiebe. 1999. Recog-
nizing subjectivity: a case study of manual tagging.
Natural Language Engineering, 1(1):1?16.
Chris Callison-Burch and Mark Dredze. 2010. Cre-
ating speech and language data with Amazon?s Me-
chanical Turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, pages 1?12.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20:37?46.
A. P. Dawid and A. M. Skene. 1979. Maximum likeli-
hood estimation of observer error-rates using the EM
algorithm. Journal of the Royal Statistical Society.
Series C (Applied Statistics), 28(1):20?28.
Barbara di Eugenio and Michael Glass. 2004. The
kappa statistic: A second look. Computational Lin-
guistics, 30(1):95?101.
Barbara di Eugenio. 2000. On the usage of kappa
to evaluate agreement on coding tasks. In Proceed-
ings of the Second International Conference on Lan-
guage Resources and Evaluation (LREC).
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Nancy Ide and Yorick Wilks. 2006. Making sense
about sense. In Word Sense Disambiguation: Al-
gorithms and Applications, pages 47?74. Springer
Verlag.
Beata Beigman Klebanov and Eyal Beigman. 2009.
From annotator agreement to noise models. Com-
putational Linguistics, 35(4):495?503.
Klaus Krippendorff. 1980. Content analysis: An in-
troduction to its methodology. Sage Publications,
Beverly Hills, CA.
Rebecca J. Passonneau, Collin F. Baker, Christiane
Fellbaum, and Nancy Ide. 2012a. The MASC
word sense corpus. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet Uur Doan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12), Is-
tanbul, Turkey. European Language Resources As-
sociation (ELRA).
Rebecca J. Passonneau, Vikas Bhardwaj, Ansaf Salleb-
Aouissi, and Nancy Ide. 2012b. Multiplicity and
word sense: evaluating and learning from multi-
ply labeled word sense annotations. Language Re-
sources and Evaluation, 46(2):219?252.
Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-
ardo Hermosillo Valadez, Charles Florin, Luca Bo-
goni, and Linda Moy. 2010. Learning from crowds.
Journal of Machine Learning Research, 11:1297?
1322.
Philip Resnik and David Yarowsky. 2000. Distinguish-
ing systems and distinguishing senses: New evalua-
tion methods for word sense disambiguation. Natu-
ral Language Engineering, 5(3):113?133.
Simon Rogers, Mark Girolami, and Tamara Polajnar.
2010. Semi-parametric analysis of multi-rater data.
Statistical Computing, 20:317?334.
Andrey Rzhetsky, Hagit Shatkay, and W. John Wilbur.
2009. How to get the most out of your curation ef-
fort. PLoS Computational Biology, 5(5):1?13.
Victor S. Sheng, Foster Provost, and Panagiotis G.
Ipeirotis. 2008. Get another label? improving data
quality and data mining using multiple, noisy label-
ers. In Proceedings of the Fourteenth ACM Inter-
national Conference on Knowledge Discovery and
Data Mining (KDD).
Padhraic Smyth, Usama Fayyad, Michael Burl, Pietro
Perona, and Pierre Baldi. 1995. Inferring ground
truth from subjectively-labeled images of Venus. In
Advances in Neural Information Processing Systems
7, pages 1085?1092. MIT Press.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but
is it good? evaluating non-expert annotations for
natural language tasks. In Proceedings of Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 254?263, Honolulu.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels
from labelers of unknown expertise. In Proceedings
of the 24th Annual Conference on Advances in Neu-
ral Information Processing Systems.
195
