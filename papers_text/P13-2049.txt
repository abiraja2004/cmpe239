Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 273?277,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Building Japanese Textual Entailment Specialized Data Sets
for Inference of Basic Sentence Relations
Kimi Kaneko ? Yusuke Miyao ? Daisuke Bekki ?
? Ochanomizu University, Tokyo, Japan
? National Institute of Informatics, Tokyo, Japan
? {kaneko.kimi | bekki}@is.ocha.ac.jp
? yusuke@nii.ac.jp
Abstract
This paper proposes a methodology for
generating specialized Japanese data sets
for textual entailment, which consists of
pairs decomposed into basic sentence rela-
tions. We experimented with our method-
ology over a number of pairs taken from
the RITE-2 data set. We compared
our methodology with existing studies
in terms of agreement, frequencies and
times, and we evaluated its validity by in-
vestigating recognition accuracy.
1 Introduction
In recognizing textual entailment (RTE), auto-
mated systems assess whether a human reader
would consider that, given a snippet of text t1 and
some unspecified (but restricted) world knowl-
edge, a second snippet of text t2 is true. An ex-
ample is given below.
Ex. 1) Example of a sentence pair for RTE
? Label: Y
? t1: Shakespeare wrote Hamlet and Macbeth.
? t2: Shakespeare is the author of Hamlet.
?Label? on line 1 shows whether textual entail-
ment (TE) holds between t1 and t2. The pair is
labeled ?Y? if the pair exhibits TE and ?N? other-
wise.
It is difficult for computers to make such as-
sessments because pairs have multiple interrelated
basic sentence relations (BSRs, for detailed in-
formation on BSRs, see section 3). Recognizing
each BSRs in pairs exactly is difficult for com-
puters. Therefore, we should generate special-
ized data sets consisting of t1-t2 pairs decomposed
into BSRs and a methodology for generating such
data sets since such data and methodologies for
Japanese are unavailable at present.
This paper proposes a methodology for gener-
ating specialized Japanese data sets for TE that
consist of monothematic t1-t2 pairs (i.e., pairs in
which only one BSR relevant to the entailment
relation is highlighted and isolated). In addition,
we compare our methodology with existing stud-
ies and analyze its validity.
2 Existing Studies
Sammons et al(2010) point out that it is necessary
to establish a methodology for decomposing pairs
into chains of BSRs, and that establishing such
methodology will enable understanding of how
other existing studies can be combined to solve
problems in natural language processing and iden-
tification of currently unsolvable problems. Sam-
mons et al experimented with their methodology
over the RTE-5 data set and showed that the recog-
nition accuracy of a system trained with their spe-
cialized data set was higher than that of the system
trained with the original data set. In addition, Ben-
tivogli et al(2010) proposed a methodology for
classifying more details than was possible in the
study by Sammons et al.
However, these studies were based on only En-
glish data sets. In this regard, the word-order
rules and the grammar of many languages (such
as Japanese) are different from those of English.
We thus cannot assess the validity of methodolo-
gies for any Japanese data set because each lan-
guage has different usages. Therefore, it is neces-
sary to assess the validity of such methodologies
with specialized Japanese data sets.
Kotani et al (2008) generated specialized
Japanese data sets for RTE that were designed
such that each pair included only one BSR. How-
ever, in that approach the data set is generated ar-
tificially, and BSRs between pairs of real world
texts cannot be analyzed.
We develop our methodology by generating
specialized data sets from a collection of pairs
from RITE-21 binary class (BC) subtask data sets
containing sentences from Wikipedia. RITE-2 is
273
an evaluation-based workshop focusing on RTE.
Four subtasks are available in RITE-2, one of
which is the BC subtask whereby systems assess
whether there is TE between t1 and t2. The rea-
son why we apply our methodology to part of the
RITE-2 BC subtask data set is that we can con-
sider the validity of the methodology in view of
the recognition accuracy by using the data sets
generated in RITE-2 tasks, and that we can an-
alyze BSRs in real texts by using sentence pairs
extracted from Wikipedia.
3 Methodology
In this study, we extended and refined the method-
ology defined in Bentivogli et al(2010) and devel-
oped a methodology for generating Japanese data
sets broken down into BSRs and non-BSRs as de-
fined below.
Basic sentence relations (BSRs):
? Lexical: Synonymy, Hypernymy, Entailment,
Meronymy;
? Phrasal: Synonymy, Hypernymy, Entailment,
Meronymy, Nominalization, Corference;
? Syntactic: Scrambling, Case alteration, Modi-
fier, Transparent head, Clause, List, Apposi-
tion, Relative clause;
? Reasoning: Temporal, Spatial, Quantity, Im-
plicit relation, Inference;
Non-basic sentence relations (non-BSRs)?
? Disagreement: Lexical, Phrasal, Modal, Mod-
ifier, Temporal, Spatial, Quantity;
Mainly, we used relations defined in Bentivogli
et al(2010) and divided Synonymy, Hypernymy,
Entailment and Meronymy into Lexical and
Phrasal. The differences between our study and
Bentivogli et al(2010) are as follows. Demonymy
and Statements in Bentivogli et al(2010) were
not considered in our study because they were
not necessary for Japanese data sets. In addi-
tion, Scrambling, Entailment, Disagreement:
temporal, Disagreement: spatial and Disagree-
ment: quantity were newly added in our study.
Scrambling is a rule for changing the order of
phrases and clauses. Entailment is a rule whereby
the latter sentence is true whenever the former is
true (e.g., ?divorce?? ?marry?). Entailment is a
rule different from Synonymy, Hypernymy and
Meronymy.
The rules for decomposition are schematized as
follows:
1http://www.cl.ecei.tohoku.ac.jp/rite2/doku.php
? Break down pairs into BSRs in order to bring
t1 close to t2 gradually, as the interpretation
of the converted sentence becomes wider
? Label each pair of BSRs or non-BSRs
such that each pair is decomposed to ensure
that there are not multiple BSRs
An example is shown below, where the underlined
parts represent the revised points.
t1? ???????? ????? ? ????? ????
Shakespearenom Hamlet com Macbethacc writepast?Shakespeare wrote Hamlet and Macbeth.?[List] ???????? ?????? ????
Shakespearenom Hamletacc writepast?Shakespeare wrote Hamlet.?
t2?[Synonymy] ???????? ?????? ?? ????
?phrasal Shakespearenom Hamletgen authorcomp becop?Shakespeare is the author of Hamlet.?
Table 1: Example of a pair with TE
An example of a pair without TE is shown below.
t1? ?????? ???????? ???
Bulgarianom Eurasia.continentdat becop?Bulgaria is on the Eurasian continent.?
[Entailment] ?????? ???? ????
? phrasal Bulgarianom continental.statecomp becop?Bulgaria is a continental state.?
t2?[Disagreement] ?????? ?? ????
?lexical Bulgarianom island.countrycomp becop?Bulgaria is an island country.?
Table 2: Example of a pair without TE (Part 1)
To facilitate TE assessments like Table 3, non-
BSR labels were used in decomposing pairs. In
addition, we allowed labels to be used several
times when some BSRs in a pair are related to ?N?
assessments.
t1? ?????? ???????? ???
Bulgarianom Eurasia.continentdat becop?Bulgaria is on the Eurasian continent.?
[Disagreement] ?????? ???????? ???
?modal Bulgarianom Eurasia.continentdat becop?neg?Bulgaria is not on the Eurasian continent.?
t2?[Synonymy] ?????? ?????? ?????
?lexical Bulgarianom Europedat belongcop?neg?Bulgaria does not belong to Europe.?
Table 3: Example of a pair without TE (Part 2)
As mentioned above, the idea here is to decom-
pose pairs in order to bring t1 closer to t2, the
latter of which in principle has a wider semantic
scope. We prohibited the conversion of t2 because
it was possible to decompose the pairs such that
they could be true even if there was no TE. Never-
theless, since it is sometimes easier to convert t2,
274
we allowed the conversion of t2 in only the case
that t1 contradicted t2 and the scope of t2 did not
overlap with that of t1 even if t2 was converted and
TE would be unchanged. An example in case that
we allowed to convert t2 is shown below. Bold-
faced types in Table 4 shows that it becomes easy
to compare t1 with t2 by converting to t2.
t1? ??? ?????? ???????
Tomnom today breakfastacc eatpast?neg?Tom didn?t eat breakfast today.?
[Scrambling] ??? ??? ??? ???????
today Tomnom breakfastacc eatpast?neg?Today, Tom didn?t eat breakfast.?
t2? ??? ??? ??? ????
this.morning Tomnom breadacc eatpast?This morning, Tom ate bread and salad.?
[Entailment] ??? ??? ??? ????
?phrasal today Tomnom breakfastacc eatpast?Today, Tom ate breakfast.?
[Disagreement] ?????????????
?modal ?Today, Tom ate breakfast.?
Table 4: Example of conversion of t2
4 Results
4.1 Comparison with Existing Studies
We applied our methodology to 173 pairs from the
RITE-2 BC subtask data set. The pairs were de-
composed by one annotator, and the decomposed
pairs were assigned labels by two annotators. Dur-
ing labeling, we used the labels presented in Sec-
tion 3 and ?unknown? in cases where pairs could
not be labeled. Our methodology was developed
based on 112 pairs, and by using the other 61 pairs,
we evaluated the inter-annotator agreement as well
as the frequencies and times of decomposition.
The agreement for 241 monothematic pairs gen-
erated from 61 pairs amounted to 0.83 and was
computed as follows. The kappa coefficient for
them amounted 0.81.
Agreement = ?Agreed?? labels/Total 2
Bentivogli et al (2010) reported an agreement
rate of 0.78, although they computed the agree-
ment by using the Dice coefficient (Dice, 1945),
and therefore the results are not directly compara-
ble to ours. Nevertheless, the close values suggest
2Because the ?Agreed? pairs were clear to be classi-
fied as ?Agreed?, where ?Total? is the number of pairs la-
beled ?Agreed? subtracted from the number of labeled pairs.
?Agreed? labels is the number of pairs labeled ?Agreed? sub-
tract from the number of pairs with the same label assigned
by the two annotators.
that our methodology is comparable to that in Ben-
tivogli?s study in terms of agreement.
Table 5 shows the distribution of monothematic
pairs with respect to original Y/N pairs.
Or
igin
alp
air
s Monothematic pairs
Y N Total
Y (32) 116 ? 116
N (29) 96 29 125
Total (61) 212 29 241
Table 5: Distribution of monothematic pairs with
respect to original Y/N pairs
When the methodology was applied to 61 pairs,
a total of 241 and an average of 3.95 monothe-
matic pairs were derived. The average was slightly
greater than the 2.98 reported in (Bentivogli et al,
2010). For pairs originally labeled ?Y? and ?N?, an
average of 3.62 and 3.31 monothematic pairs were
derived, respectively. Both average values were
slightly higher than the values of 3.03 and 2.80 re-
ported in (Bentivogli et al, 2010). On the basis of
the small differences between the average values
in our study and those in (Bentivogli et al, 2010),
we are justified in saying that our methodology is
valid.
Table 6 3 shows the distribution of BSRs in t1-
t2 pairs in an existing study and the present study.
We can see from Table 6 thatCorferencewas seen
more frequently in Bentivogli?s study than in our
study, while Entailment and Scrambling were
seen more frequently in our study. This demon-
strates that differences between languages are rele-
vant to the distribution and classification of BSRs.
An average of 5 and 4 original pairs were de-
composed per hour in our study and Bentivogli?s
study, respectively. This indicates that the com-
plexity of our methodology is not much different
from that in Bentivogli et al(2010).
4.2 Evaluation of Accuracy in BSR
In the RITE-2 formal run4, 15 teams used our spe-
cialized data set for the evaluation of their systems.
Table 7 shows the average of F1 scores5 for each
BSR.
Scrambling and Modifier yielded high scores
(close to 90%). The score of List was also
3Because ?lexical? and ?phrasal? are classified together
in Bentivogli et al(2010), they are not shown separately in
Table 6.
4In RITE-2, data generated by our methodology were re-
leased as ?unit test data?.
5The traditional F1 score is the harmonic mean of preci-
sion and recall.
275
BSR Monothematic pairsBentivogli et al Present studyTotal Y N Total Y NSynonymy 25 22 3 45 45 0Hypernymy 5 3 2 5 5 0Entailment - - - 44 44 0Meronymy 7 4 3 1 1 0Nominalization 9 9 0 1 1 0Corference 49 48 1 3 3 0Scrambling - - - 15 15 0Case alteration 7 5 2 7 7 0Modifier 25 15 10 42 42 0Transparent head 6 6 0 1 1 0Clause 5 4 1 14 14 0List 1 1 0 3 3 0Apposition 3 2 1 1 1 0Relative clause 1 1 0 8 8 0Temporal 2 1 1 1 1 0Spatial 1 1 0 1 1 0Quantity 6 0 6 0 0 0Implicit relation 7 7 0 18 18 0Inference 40 26 14 2 2 0Disagreement: lexical/phrasal 3 0 3 27 0 27Disagreement: modal 1 0 1 1 0 1Disagreement: temporal - - - 1 0 1Disagreement: spatial - - - 0 0 0Disagreement: quantity - - - 0 0 0Demonymy 1 1 0 - - -Statements 1 1 0 - - -total 205 157 48 241 212 29
Table 6: Distribution of BSRs in t1-t2 pairs in an
existing study and in the present study using our
methodology
BSR F1(%) Monothematic MissPairsScrambling 89.6 15 4Modifier 88.8 42 0List 88.6 3 0Temporal 85.7 1 1Relative clause 85.4 8 2Clause 85.0 14 2Hypernymy: lexical 85.0 5 1Disagreement: phrasal 80.1 25 0Case alteration 79.9 7 2Synonymy: lexical 79.7 9 6Transparent head 78.6 1 2Implicit relation 75.7 18 2Synonymy: phrasal 73.6 36 9Corference 70.9 3 1Entailment: phrasal 70.2 44 7Disagreement: lexical 69.0 2 0Meronymy: lexical 64.3 1 1Nominalization 64.3 1 0Apposition 50.0 1 1Spatial 50.0 1 1Inference 40.5 2 2Disagreement: modal 35.7 1 0Disagreement: temporal 28.6 1 1Total - 241 41
Table 7: Average F1 scores in BSR and frequen-
cies of misclassifications by annotators
nearly 90%, although the data sets included only
3 instances. These scores were high because
pairs with these BSRs are easily recognized in
terms of syntactic structure. By contrast, Dis-
agreement: temporal, Disagreement: modal,
Inference, Spatial and Apposition yielded low
scores (less than 50%). The scores of Disagree-
ment: lexical, Nominalization and Disagree-
ment: Meronymy were about 50-70%. BSRs
that yielded scores of less than 70% occurred less
than 3 times, and those that yielded scores of not
more than 70% occurred 3 times or more, except
for Temporal and Transparent head. Therefore,
the frequencies of BSRs are related to F1 scores,
and we should consider how to build systems that
recognize infrequent BSRs accurately. In addi-
tion, F1 scores in Synonymy: phrasal and En-
tailment: phrasal are low, although these are la-
beled frequently. This is one possible direction of
future work.
Table 7 also shows the number of pairs in BSR
to which the two annotators assigned different la-
bels. For example, one annotator labeled t2 [Ap-
position] while the other labeled t2 [Spatial] in
the following pair:
Ex. 2) Example of a pair for RTE
? t1: Tokyo, the capital of Japan, is in Asia.
? t2: The capital of Japan is in Asia.
We can see from Table 7 that the F1 scores for
BSRs, which are often assessed as different by dif-
ferent people, are generally low, except for several
labels, such as Synonymy: lexical and Scram-
bling. For this reason, we can conjecture that
cases in which computers experience difficulty de-
termining the correct labels are correlated with
cases in which humans also experience such dif-
ficulty.
5 Conclusions
This paper presented a methodology for generat-
ing Japanese data sets broken down into BSRs
and Non-BSRs, and we conducted experiments in
which we applied our methodology to 61 pairs
extracted from the RITE-2 BC subtask data set.
We compared our method with that of Bentivogli
et al(2010) in terms of agreement as well as
frequencies and times of decomposition, and we
obtained similar results. This demonstrated that
our methodology is as feasible as Bentivogli et
al.(2010) and that differences between languages
emerge only as the different sets of labels and the
different distributions of BSRs. In addition, 241
monothematic pairs were recognized by comput-
ers, and we showed that both the frequencies of
BSRs and the rate of misclassification by humans
are relevant to F1 scores.
Decomposition patterns were not empirically
compared in the present study and will be investi-
gated in future work. We will also develop an RTE
inference system by using our specialized data set.
276
References
Bentivogli, L., Cabrio, E., Dagan, I, Giampiccolo, D.,
Leggio, M. L., Magnini,B. 2010. Building Textual
Entailment Specialized Data Sets: a Methodology
for Isolating Linguistic Phenomena Relevant to In-
ference. In Proceedings of LREC 2010, Valletta,
Malta.
Dagan, I, Glickman, O., Magnini, B. 2005. Recog-
nizing Textual Entailment Challenge. In Proc. of
the First PASCAL Challenges Workshop on RTE.
Southampton, U.K.
Kotani, M., Shibata, T., Nakata, T, Kurohashi, S. 2008.
Building Textual Entailment Japanese Data Sets and
Recognizing Reasoning Relations Based on Syn-
onymy Acquired Automatically. In Proceedings of
the 14th Annual Meeting of the Association for Nat-
ural Language Processing, Tokyo, Japan.
Magnini, B., Cabrio, E. 2009. Combining Special-
izedd Entailment Engines. In Proceedings of LTC
?09. Poznan, Poland.
Dice, L. R. 1945. Measures of the amount of ecologic
association between species. Ecology, 26(3):297-
302.
Mark Sammons, V.G.Vinod Vydiswaran, Dan Roth.
2010. ?Ask not what textual entailment can do for
you...?. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, Uppsala, Sweden, pp. 1199-1208.
277
