Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 355?362, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Identifying Sources of Opinions with Conditional Random Fields and
Extraction Patterns
Yejin Choi and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{ychoi,cardie}@cs.cornell.edu
Ellen Riloff and Siddharth Patwardhan
School of Computing
University of Utah
Salt Lake City, UT 84112
{riloff,sidd}@cs.utah.edu
Abstract
Recent systems have been developed for
sentiment classification, opinion recogni-
tion, and opinion analysis (e.g., detect-
ing polarity and strength). We pursue an-
other aspect of opinion analysis: identi-
fying the sources of opinions, emotions,
and sentiments. We view this problem as
an information extraction task and adopt
a hybrid approach that combines Con-
ditional Random Fields (Lafferty et al,
2001) and a variation of AutoSlog (Riloff,
1996a). While CRFs model source iden-
tification as a sequence tagging task, Au-
toSlog learns extraction patterns. Our re-
sults show that the combination of these
two methods performs better than either
one alone. The resulting system identifies
opinion sources with 79.3% precision and
59.5% recall using a head noun matching
measure, and 81.2% precision and 60.6%
recall using an overlap measure.
1 Introduction
In recent years, there has been a great deal of in-
terest in methods for automatically identifying opin-
ions, emotions, and sentiments in text. Much of
this research explores sentiment classification, a text
categorization task in which the goal is to classify
a document as having positive or negative polar-
ity (e.g., Das and Chen (2001), Pang et al (2002),
Turney (2002), Dave et al (2003), Pang and Lee
(2004)). Other research efforts analyze opinion ex-
pressions at the sentence level or below to recog-
nize opinions, their polarity, and their strength (e.g.,
Dave et al (2003), Pang and Lee (2004), Wilson et
al. (2004), Yu and Hatzivassiloglou (2003), Wiebe
and Riloff (2005)). Many applications could ben-
efit from these opinion analyzers, including prod-
uct reputation tracking (e.g., Morinaga et al (2002),
Yi et al (2003)), opinion-oriented summarization
(e.g., Cardie et al (2004)), and question answering
(e.g., Bethard et al (2004), Yu and Hatzivassiloglou
(2003)).
We focus here on another aspect of opinion
analysis: automatically identifying the sources of
the opinions. Identifying opinion sources will
be especially critical for opinion-oriented question-
answering systems (e.g., systems that answer ques-
tions of the form ?How does [X] feel about [Y]??)
and opinion-oriented summarization systems, both
of which need to distinguish the opinions of one
source from those of another.1
The goal of our research is to identify direct and
indirect sources of opinions, emotions, sentiments,
and other private states that are expressed in text.
To illustrate the nature of this problem, consider the
examples below:
S1: Taiwan-born voters favoring independence...
1In related work, we investigate methods to identify the
opinion expressions (e.g., Riloff and Wiebe (2003), Wiebe and
Riloff (2005), Wilson et al (2005)) and the nesting structure
of sources (e.g., Breck and Cardie (2004)). The target of each
opinion, i.e., what the opinion is directed towards, is currently
being annotated manually for our corpus.
355
S2: According to the report, the human rights
record in China is horrendous.
S3: International officers believe that the EU will
prevail.
S4: International officers said US officials want the
EU to prevail.
In S1, the phrase ?Taiwan-born voters? is the di-
rect (i.e., first-hand) source of the ?favoring? sen-
timent. In S2, ?the report? is the direct source of
the opinion about China?s human rights record. In
S3, ?International officers? are the direct source of
an opinion regarding the EU. The same phrase in
S4, however, denotes an indirect (i.e., second-hand,
third-hand, etc.) source of an opinion whose direct
source is ?US officials?.
In this paper, we view source identification as an
information extraction task and tackle the problem
using sequence tagging and pattern matching tech-
niques simultaneously. Using syntactic, semantic,
and orthographic lexical features, dependency parse
features, and opinion recognition features, we train a
linear-chain Conditional Random Field (CRF) (Laf-
ferty et al, 2001) to identify opinion sources. In ad-
dition, we employ features based on automatically
learned extraction patterns and perform feature in-
duction on the CRF model.
We evaluate our hybrid approach using the NRRC
corpus (Wiebe et al, 2005), which is manually
annotated with direct and indirect opinion source
information. Experimental results show that the
CRF model performs well, and that both the extrac-
tion patterns and feature induction produce perfor-
mance gains. The resulting system identifies opinion
sources with 79.3% precision and 59.5% recall us-
ing a head noun matching measure, and 81.2% pre-
cision and 60.6% recall using an overlap measure.
2 The Big Picture
The goal of information extraction (IE) systems is
to extract information about events, including the
participants of the events. This task goes beyond
Named Entity recognition (e.g., Bikel et al (1997))
because it requires the recognition of role relation-
ships. For example, an IE system that extracts in-
formation about corporate acquisitions must distin-
guish between the company that is doing the acquir-
ing and the company that is being acquired. Sim-
ilarly, an IE system that extracts information about
terrorism must distinguish between the person who
is the perpetrator and the person who is the victim.
We hypothesized that IE techniques would be well-
suited for source identification because an opinion
statement can be viewed as a kind of speech event
with the source as the agent.
We investigate two very different learning-based
methods from information extraction for the prob-
lem of opinion source identification: graphical mod-
els and extraction pattern learning. In particular, we
consider Conditional Random Fields (Lafferty et al,
2001) and a variation of AutoSlog (Riloff, 1996a).
CRFs have been used successfully for Named En-
tity recognition (e.g., McCallum and Li (2003),
Sarawagi and Cohen (2004)), and AutoSlog has per-
formed well on information extraction tasks in sev-
eral domains (Riloff, 1996a). While CRFs treat
source identification as a sequence tagging task, Au-
toSlog views the problem as a pattern-matching task,
acquiring symbolic patterns that rely on both the
syntax and lexical semantics of a sentence. We hy-
pothesized that a combination of the two techniques
would perform better than either one alone.
Section 3 describes the CRF approach to identify-
ing opinion sources and the features that the system
uses. Section 4 then presents a new variation of Au-
toSlog, AutoSlog-SE, which generates IE patterns to
extract sources. Section 5 describes the hybrid sys-
tem: we encode the IE patterns as additional features
in the CRF model. Finally, Section 6 presents our
experimental results and error analysis.
3 Semantic Tagging via Conditional
Random Fields
We defined the problem of opinion source identifi-
cation as a sequence tagging task via CRFs as fol-
lows. Given a sequence of tokens, x = x1x2...xn,
we need to generate a sequence of tags, or labels,
y = y1y2...yn. We define the set of possible label
values as ?S?, ?T?, ?-?, where ?S? is the first to-
ken (or Start) of a source, ?T? is a non-initial token
(i.e., a conTinuation) of a source, and ?-? is a token
that is not part of any source.2
A detailed description of CRFs can be found in
2This is equivalent to the IOB tagging scheme used in syn-
tactic chunkers (Ramshaw and Marcus, 1995).
356
Lafferty et al (2001). For our sequence tagging
problem, we create a linear-chain CRF based on
an undirected graph G = (V,E), where V is the
set of random variables Y = {Yi|1 ? i ? n},
one for each of n tokens in an input sentence;
and E = {(Yi?1, Yi)|1 < i ? n} is the set
of n ? 1 edges forming a linear chain. For each
sentence x, we define a non-negative clique poten-
tial exp(
?K
k=1 ?kfk(yi?1, yi, x)) for each edge, and
exp(?K?k=1 ??kf ?k(yi, x)) for each node, where fk(...)
is a binary feature indicator function, ?k is a weight
assigned for each feature function, and K and K ?
are the number of features defined for edges and
nodes respectively. Following Lafferty et al (2001),
the conditional probability of a sequence of labels y
given a sequence of tokens x is:
P (y|x) = 1Zx
exp
?
X
i,k
?k fk(yi?1, yi, x)+
X
i,k
??k f ?k(yi, x)
?
(1)
Zx =
X
y
exp
?
X
i,k
?k fk(yi?1, yi, x) +
X
i,k
??k f ?k(yi, x)
?
(2)
where Zx is a normalization constant for each
x. Given the training data D, a set of sen-
tences paired with their correct ?ST-? source la-
bel sequences, the parameters of the model are
trained to maximize the conditional log-likelihood
?
(x,y)?D P (y|x). For inference, given a sentence x
in the test data, the tagging sequence y is given by
argmaxy?P (y?|x).
3.1 Features
To develop features, we considered three properties
of opinion sources. First, the sources of opinions are
mostly noun phrases. Second, the source phrases
should be semantic entities that can bear or express
opinions. Third, the source phrases should be di-
rectly related to an opinion expression. When con-
sidering only the first and second criteria, this task
reduces to named entity recognition. Because of the
third condition, however, the task requires the recog-
nition of opinion expressions and a more sophisti-
cated encoding of sentence structure to capture re-
lationships between source phrases and opinion ex-
pressions.
With these properties in mind, we define the fol-
lowing features for each token/word xi in an input
sentence. For pedagogical reasons, we will describe
some of the features as being multi-valued or cate-
gorical features. In practice, however, all features
are binarized for the CRF model.
Capitalization features We use two boolean fea-
tures to represent the capitalization of a word:
all-capital, initial-capital.
Part-of-speech features Based on the lexical cat-
egories produced by GATE (Cunningham et al,
2002), each token xi is classified into one of a set
of coarse part-of-speech tags: noun, verb, adverb,
wh-word, determiner, punctuation, etc. We do the
same for neighboring words in a [?2,+2] window
in order to assist noun phrase segmentation.
Opinion lexicon features For each token xi, we in-
clude a binary feature that indicates whether or not
the word is in our opinion lexicon ? a set of words
that indicate the presence of an opinion. We do the
same for neighboring words in a [?1,+1] window.
Additionally, we include for xi a feature that in-
dicates the opinion subclass associated with xi, if
available from the lexicon. (e.g., ?bless? is clas-
sified as ?moderately subjective? according to the
lexicon, while ?accuse? and ?berate? are classified
more specifically as ?judgments?.) The lexicon is
initially populated with approximately 500 opinion
words 3 from (Wiebe et al, 2002), and then aug-
mented with opinion words identified in the training
data. The training data contains manually produced
phrase-level annotations for all expressions of opin-
ions, emotions, etc. (Wiebe et al, 2005). We col-
lected all content words that occurred in the training
set such that at least 50% of their occurrences were
in opinion annotations.
Dependency tree features For each token xi, we
create features based on the parse tree produced by
the Collins (1999) dependency parser. The purpose
of the features is to (1) encode structural informa-
tion, and (2) indicate whether xi is involved in any
grammatical relations with an opinion word. Two
pre-processing steps are required before features can
be constructed:
3Some words are drawn from Levin (1993); others are from
Framenet lemmas (Baker et al 1998) associated with commu-
nication verbs.
357
1. Syntactic chunking. We traverse the depen-
dency tree using breadth-first search to identify
and group syntactically related nodes, produc-
ing a flatter, more concise tree. Each syntac-
tic ?chunk? is also assigned a grammatical role
(e.g., subject, object, verb modifier, time,
location, of-pp, by-pp) based on its con-
stituents. Possessives (e.g., ?Clinton?s idea?)
and the phrase ?according to X? are handled as
special cases in the chunking process.
2. Opinion word propagation. Although the
opinion lexicon contains only content words
and no multi-word phrases, actual opinions of-
ten comprise an entire phrase, e.g., ?is really
willing? or ?in my opinion?. As a result, we
mark as an opinion the entire chunk that con-
tains an opinion word. This allows each token
in the chunk to act as an opinion word for fea-
ture encoding.
After syntactic chunking and opinion word propa-
gation, we create the following dependency tree fea-
tures for each token xi:
? the grammatical role of its chunk
? the grammatical role of xi?1?s chunk
? whether the parent chunk includes an opinion
word
? whether xi?s chunk is in an argument position
with respect to the parent chunk
? whether xi represents a constituent boundary
Semantic class features We use 7 binary fea-
tures to encode the semantic class of each word
xi: authority, government, human, media,
organization or company, proper name,
and other. The other class captures 13 seman-
tic classes that cannot be sources, such as vehicle
and time.
Semantic class information is derived from named
entity and semantic class labels assigned to xi by the
Sundance shallow parser (Riloff, 2004). Sundance
uses named entity recognition rules to label noun
phrases as belonging to named entity classes, and
assigns semantic tags to individual words based on
a semantic dictionary. Table 1 shows the hierarchy
that Sundance uses for semantic classes associated
with opinion sources. Sundance is also used to rec-
ognize and instantiate the source extraction patterns
PROPER NAMEAUTHORITY LOCATION
CITY
COUNTRY
PLANET
PROVINCE
PERSON NAME
PERSON DESC
NATIONALITY
TITLE
COMPANY
GOVERNMENT
MEDIA
ORGANIZATION
HUMAN
SOURCE
Figure 1: The semantic hierarchy for opinion
sources
that are learned by AutoSlog-SE, which is described
in the next section.
4 Semantic Tagging via Extraction
Patterns
We also learn patterns to extract opinion sources us-
ing a statistical adaptation of the AutoSlog IE learn-
ing algorithm. AutoSlog (Riloff, 1996a) is a super-
vised extraction pattern learner that takes a train-
ing corpus of texts and their associated answer keys
as input. A set of heuristics looks at the context
surrounding each answer and proposes a lexico-
syntactic pattern to extract that answer from the text.
The heuristics are not perfect, however, so the result-
ing set of patterns needs to be manually reviewed by
a person.
In order to build a fully automatic system that
does not depend on manual review, we combined
AutoSlog?s heuristics with statistics from the an-
notated training data to create a fully automatic
supervised learner. We will refer to this learner
as AutoSlog-SE (Statistically Enhanced variation
of AutoSlog). AutoSlog-SE?s learning process has
three steps:
Step 1: AutoSlog?s heuristics are applied to every
noun phrase (NP) in the training corpus. This
generates a set of extraction patterns that, col-
lectively, can extract every NP in the training
corpus.
Step 2: The learned patterns are augmented with
selectional restrictions that semantically con-
strain the types of noun phrases that are legiti-
mate extractions for opinion sources. We used
358
the semantic classes shown in Figure 1 as se-
lectional restrictions.
Step 3: The patterns are applied to the training cor-
pus and statistics are gathered about their ex-
tractions. We count the number of extrac-
tions that match annotations in the corpus (cor-
rect extractions) and the number of extractions
that do not match annotations (incorrect extrac-
tions). These counts are then used to estimate
the probability that the pattern will extract an
opinion source in new texts:
P (source | patterni) =
correct sources
correct sources + incorrect sources
This learning process generates a set of extraction
patterns coupled with probabilities. In the next sec-
tion, we explain how these extraction patterns are
represented as features in the CRF model.
5 Extraction Pattern Features for the CRF
The extraction patterns provide two kinds of infor-
mation. SourcePatt indicates whether a word
activates any source extraction pattern. For exam-
ple, the word ?complained? activates the pattern
?<subj> complained? because it anchors the ex-
pression. SourceExtr indicates whether a word is
extracted by any source pattern. For example, in the
sentence ?President Jacques Chirac frequently com-
plained about France?s economy?, the words ?Pres-
ident?, ?Jacques?, and ?Chirac? would all be ex-
tracted by the ?<subj> complained? pattern.
Each extraction pattern has frequency and prob-
ability values produced by AutoSlog-SE, hence we
create four IE pattern-based features for each token
xi: SourcePatt-Freq, SourceExtr-Freq,
SourcePatt-Prob, and SourceExtr-Prob,
where the frequency values are divided into three
ranges: {0, 1, 2+} and the probability values are di-
vided into five ranges of equal size.
6 Experiments
We used the Multi-Perspective Question Answering
(MPQA) corpus4 for our experiments. This corpus
4The MPQA corpus can be freely obtained at
http://nrrc.mitre.org/NRRC/publications.htm.
consists of 535 documents that have been manu-
ally annotated with opinion-related information in-
cluding direct and indirect sources. We used 135
documents as a tuning set for model development
and feature engineering, and used the remaining 400
documents for evaluation, performing 10-fold cross
validation. These texts are English language ver-
sions of articles that come from many countries and
cover many topics.5
We evaluate performance using 3 measures: over-
lap match (OL), head match (HM), and exact match
(EM). OL is a lenient measure that considers an ex-
traction to be correct if it overlaps with any of the an-
notated words. HM is a more conservative measure
that considers an extraction to be correct if its head
matches the head of the annotated source. We report
these somewhat loose measures because the annota-
tors vary in where they place the exact boundaries
of a source. EM is the strictest measure that requires
an exact match between the extracted words and the
annotated words. We use three evaluation metrics:
recall, precision, and F-measure with recall and pre-
cision equally weighted.
6.1 Baselines
We developed three baseline systems to assess the
difficulty of our task. Baseline-1 labels as sources
all phrases that belong to the semantic categories
authority, government, human, media,
organization or company, proper name.
Table 1 shows that the precision is poor, suggest-
ing that the third condition described in Section 3.1
(opinion recognition) does play an important role in
source identification. The recall is much higher but
still limited due to sources that fall outside of the se-
mantic categories or are not recognized as belong-
ing to these categories. Baseline-2 labels a noun
phrase as a source if any of the following are true:
(1) the NP is the subject of a verb phrase containing
an opinion word, (2) the NP follows ?according to?,
(3) the NP contains a possessive and is preceded by
an opinion word, or (4) the NP follows ?by? and at-
taches to an opinion word. Baseline-2?s heuristics
are designed to address the first and the third condi-
tions in Section 3.1. Table 1 shows that Baseline-2
is substantially better than Baseline-1. Baseline-3
5This data was obtained from the Foreign Broadcast Infor-
mation Service (FBIS), a U.S. government agency.
359
Recall Prec F1
OL 77.3 28.8 42.0
Baseline-1 HM 71.4 28.6 40.8
EM 65.4 20.9 31.7
OL 62.4 60.5 61.4
Baseline-2 HM 59.7 58.2 58.9
EM 50.8 48.9 49.8
OL 49.9 72.6 59.2
Baseline-3 HM 47.4 72.5 57.3
EM 44.3 58.2 50.3
OL 48.5 81.3 60.8
Extraction Patterns HM 46.9 78.5 58.7
EM 41.9 70.2 52.5
CRF: OL 56.1 81.0 66.3
basic features HM 55.1 79.2 65.0
EM 50.0 72.4 59.2
CRF: OL 59.1 82.4 68.9
basic + IE pattern HM 58.1 80.5 67.5
features EM 52.5 73.3 61.2
CRF-FI: OL 57.7 80.7 67.3
basic features HM 56.8 78.8 66.0
EM 51.7 72.4 60.3
CRF-FI: OL 60.6 81.2 69.4
basic + IE pattern HM 59.5 79.3 68.0
features EM 54.1 72.7 62.0
Table 1: Source identification performance table
labels a noun phrase as a source if it satisfies both
Baseline-1 and Baseline-2?s conditions (this should
satisfy all three conditions described in Section 3.1).
As shown in Table 1, the precision of this approach
is the best of the three baselines, but the recall is the
lowest.
6.2 Extraction Pattern Experiment
We evaluated the performance of the learned extrac-
tion patterns on the source identification task. The
learned patterns were applied to the test data and
the extracted sources were scored against the manual
annotations.6 Table 1 shows that the extraction pat-
terns produced lower recall than the baselines, but
with considerably higher precision. These results
show that the extraction patterns alone can identify
6These results were obtained using the patterns that had a
probability > .50 and frequency > 1.
nearly half of the opinion sources with good accu-
racy.
6.3 CRF Experiments
We developed our CRF model using the MALLET
code from McCallum (2002). For training, we used
a Gaussian prior of 0.25, selected based on the tun-
ing data. We evaluate the CRF using the basic fea-
tures from Section 3, both with and without the IE
pattern features from Section 5. Table 1 shows that
the CRF with basic features outperforms all of the
baselines as well as the extraction patterns, achiev-
ing an F-measure of 66.3 using the OL measure,
65.0 using the HM measure, and 59.2 using the
EM measure. Adding the IE pattern features fur-
ther increases performance, boosting recall by about
3 points for all of the measures and slightly increas-
ing precision as well.
CRF with feature induction. One limitation of
log-linear function models like CRFs is that they
cannot form a decision boundary from conjunctions
of existing features, unless conjunctions are explic-
itly given as part of the feature vector. For the
task of identifying opinion sources, we observed
that the model could benefit from conjunctive fea-
tures. For instance, instead of using two separate
features, HUMAN and PARENT-CHUNK-INCLUDES-
OPINION-EXPRESSION, the conjunction of the two
is more informative.
For this reason, we applied the CRF feature in-
duction approach introduced by McCallum (2003).
As shown in Table 1, where CRF-FI stands for the
CRF model with feature induction, we see consis-
tent improvements by automatically generating con-
junctive features. The final system, which com-
bines the basic features, the IE pattern features,
and feature induction achieves an F-measure of 69.4
(recall=60.6%, precision=81.2%) for the OL mea-
sure, an F-measure of 68.0 (recall=59.5%, preci-
sion=79.3%) for the HM measure, and an F-measure
of 62.0 (recall=54.1%, precision=72.7%) for the EM
measure.
6.4 Error Analysis
An analysis of the errors indicated some common
mistakes:
? Some errors resulted from error propagation in
360
our subsystems. Errors from the sentence bound-
ary detector in GATE (Cunningham et al, 2002)
were especially problematic because they caused
the Collins parser to fail, resulting in no depen-
dency tree information.
? Some errors were due to complex and unusual
sentence structure, which our rather simple fea-
ture encoding for CRF could not capture well.
? Some errors were due to the limited coverage of
the opinion lexicon. We failed to recognize some
cases when idiomatic or vague expressions were
used to express opinions.
Below are some examples of errors that we found
interesting. Doubly underlined phrases indicate in-
correctly extracted sources (either false positives
or false negatives). Opinion words are singly
underlined.
False positives:
(1) Actually, these three countries do have one common
denominator, i.e., that their values and policies do not
agree with those of the United States and none of them
are on good terms with the United States.
(2) Perhaps this is why Fidel Castro has not spoken out
against what might go on in Guantanamo.
In (1), ?their values and policies? seems like a rea-
sonable phrase to extract, but the annotation does not
mark this as a source, perhaps because it is some-
what abstract. In (2), ?spoken out? is negated, which
means that the verb phrase does not bear an opinion,
but our system failed to recognize the negation.
False negatives:
(3) And for this reason, too, they have a moral duty to
speak out, as Swedish Foreign Minister Anna Lindh,
among others, did yesterday.
(4) In particular, Iran and Iraq are at loggerheads with
each other to this day.
Example (3) involves a complex sentence structure
that our system could not deal with. (4) involves an
uncommon opinion expression that our system did
not recognize.
7 Related Work
To our knowledge, our research is the first to auto-
matically identify opinion sources using the MPQA
opinion annotation scheme. The most closely re-
lated work on opinion analysis is Bethard et al
(2004), who use machine learning techniques to
identify propositional opinions and their holders
(sources). However, their work is more limited
in scope than ours in several ways. Their work
only addresses propositional opinions, which are
?localized in the propositional argument? of cer-
tain verbs such as ?believe? or ?realize?. In con-
trast, our work aims to find sources for all opinions,
emotions, and sentiments, including those that are
not related to a verb at all. Furthermore, Berthard
et al?s task definition only requires the identifica-
tion of direct sources, while our task requires the
identification of both direct and indirect sources.
Bethard et al evaluate their system on manually
annotated FrameNet (Baker et al, 1998) and Prop-
Bank (Palmer et al, 2005) sentences and achieve
48% recall with 57% precision.
Our IE pattern learner can be viewed as a cross
between AutoSlog (Riloff, 1996a) and AutoSlog-
TS (Riloff, 1996b). AutoSlog is a supervised learner
that requires annotated training data but does not
compute statistics. AutoSlog-TS is a weakly super-
vised learner that does not require annotated data
but generates coarse statistics that measure each pat-
tern?s correlation with relevant and irrelevant docu-
ments. Consequently, the patterns learned by both
AutoSlog and AutoSlog-TS need to be manually re-
viewed by a person to achieve good accuracy. In
contrast, our IE learner, AutoSlog-SE, computes
statistics directly from the annotated training data,
creating a fully automatic variation of AutoSlog.
8 Conclusion
We have described a hybrid approach to the problem
of extracting sources of opinions in text. We cast
this problem as an information extraction task, using
both CRFs and extraction patterns. Our research is
the first to identify both direct and indirect sources
for all types of opinions, emotions, and sentiments.
Directions for future work include trying to in-
crease recall by identifying relationships between
opinions and sources that cross sentence boundaries,
and relationships between multiple opinion expres-
sions by the same source. For example, the fact that
a coreferring noun phrase was marked as a source
in one sentence could be a useful clue for extracting
the source from another sentence. The probability or
the strength of an opinion expression may also play
a useful role in encouraging or suppressing source
extraction.
361
9 Acknowledgments
We thank the reviewers for their many helpful com-
ments, and the Cornell NLP group for their advice
and suggestions for improvement. This work was
supported by the Advanced Research and Develop-
ment Activity (ARDA), by NSF Grants IIS-0208028
and IIS-0208985, and by the Xerox Foundation.
References
C. Baker, C. Fillmore & J. Lowe. 1998. The Berkeley
FrameNet Project. In Proceedings of the COLING-ACL.
S. Bethard, H. Yu, A. Thornton, V. Hativassiloglou & D. Juraf-
sky. 2004. Automatic extraction of opinion propositions and
their holders. In Proceedings of AAAI Spring Symposium on
Exploring Attitude and Affect in Text.
D. Bikel, S. Miller, R. Schwartz & R. Weischedel. 1997.
Nymble: A High-Performance Learning Name-Finder. In
Proceedings of the Fifth Conference on Applied Natural Lan-
guage Processing.
E. Breck & C. Cardie. 2004. Playing the Telephone Game:
Determining the Hierarchical Structure of Perspective and
Speech Expressions. In Proceedings of 20th International
Conference on Computational Linguistics.
C. Cardie, J. Wiebe, T. Wilson & D. Litman. 2004. Low-
level annotations and summary representations of opinions
for multiperspective QA. In New Directions in Question An-
swering. AAAI Press/MIT Press.
M. Collins. 1999. Head-driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
H. Cunningham, D. Maynard, K. Bontcheva & V. Tablan. 2002.
GATE: A Framework and Graphical Development Environ-
ment for Robust NLP Tools and Applications. In Proceed-
ings of the 40th Anniversary Meeting of the Association for
Computational Linguistics.
S. Das & M. Chen. 2001. Yahoo for amazon: Extracting market
sentiment from stock message boards. In Proceedings of the
8th Asia Pacific Finance Association Annual Conference.
K. Dave, S. Lawrence & D. Pennock. 2003. Mining the peanut
gallery: Opinion extraction and semantic classification of
product reviews. In International World Wide Web Confer-
ence.
J. Lafferty, A. K. McCallum & F. Pereira. 2001. Conditional
Random Fields: Probabilistic Models for Segmenting and
Labeling Sequence Data. In Proceedings of 18th Interna-
tional Conference on Machine Learning.
B. Levin. 1993. English Verb Classes and Alternations: A
Preliminary Investigation. University of Chicago Press.
A. K. McCallum. 2002. MALLET: A Machine Learning for
Language Toolkit. http://mallet.cs.umass.edu.
A. K. McCallum. 2003. Efficiently Inducing Features of Con-
ditional Random Fields. In Conference on Uncertainty in
Artificial Intelligence.
A. K. McCallum & W. Li. 2003. Early Results for Named
Entity Recognition with Conditional Random Fields, Feature
Induction and Web-Enhanced Lexicons. In Conference on
Natural Language Learning.
S. Morinaga, K. Yamanishi, K. Tateishi & T. Fukushima 2002.
Mining Product Reputations on the Web. In Proceedings of
the 8th Internatinal Conference on Knowledge Discover and
Data Mining.
M. Palmer, D. Gildea & P. Kingsbury. 2005. The Proposition
Bank: An Annotated Corpus of Semantic Roles. In Compu-
tational Linguistics 31.
B. Pang, L. Lee & S. Vaithyanathan. 2002. Thumbs up? sen-
timent classification using machine learning techniques. In
Proceedings of the 2002 Conference on Empirical Methods
in Natural Language Processing.
B. Pang & L. Lee. 2004. A sentimental education: Sentiment
analysis using subjectivity summarization based on mini-
mum cuts. In Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics.
L. A. Ramshaw & M. P. Marcus. 1995. Nymble: A High-
Performance Learning Name-Finder. In Proceedings of the
3rd Workshop on Very Large Corpora.
E. Riloff. 1996a. An Empirical Study of Automated Dictionary
Construction for Information Extraction in Three Domains.
In Artificial Intelligence, Vol. 85.
E. Riloff. 1996b. Automatically Generating Extraction Patterns
from Untagged Text. In Proceedings of the 13th National
Conference on Artificial Intelligence.
E. Riloff & J. Wiebe. 2003. Learning extraction patterns for
subjective expressions. In Proceesings of 2003 Conference
on Empirical Methods in Natural Language Processing.
E. Riloff & W. Phillips. 2004. An Introduction to the Sun-
dance and AutoSlog Systems Technical Report UUCS-04-
015, School of Computing, University of Utah.
S. Sarawagi & W. W. Cohen. 2004. Semi-Markov Condi-
tional Random Fields for Information Extraction 18th An-
nual Conference on Neural Information Processing Systems.
P. Turney. 2002. Thumbs up or thumbs down? semantic orien-
tation applied to unsupervised classification of reviews. In
Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics.
T. Wilson, J. Wiebe & R. Hwa. 2004. Just how mad are you?
finding strong and weak opinion clauses. In Proceedings of
the 9th National Conference on Artificial Intelligence.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler, J. Wiebe,
Y. Choi, C. Cardie, E. Riloff & S. Patwardhan. 2005. Opin-
ionFinder: A system for subjectivity analysis. Demonstra-
tion Description in Conference on Empirical Methods in
Natural Language Processing.
J. Yi, T. Nasukawa, R. Bunescu & W. Niblack. 2003. Sentiment
Analyzer: Extracting Sentiments about a Given Topic using
Natural Language Processing Techniques. In Proceedings of
the 3rd IEEE International Conference on Data Mining.
H. Yu & V. Hatzivassiloglou. 2003. Towards answering opin-
ion questions: Separating facts from opinions and identify-
ing the polarity of opinion sentences. In Proceedings of the
Conference on Empirical Methods in Natural Language Pro-
cessing.
J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis, B. Fraser,
D. Litman, D. Pierce, E. Riloff & T. Wilson. 2002. NRRC
Summer Workshop on Multiple-Perspective Question An-
swering: Final Report.
J. Wiebe & E. Riloff. 2005. Creating subjective and objective
sentence classifiers from unannotated texts. Sixth Interna-
tional Conference on Intelligent Text Processing and Com-
putational Linguistics.
J. Wiebe, T. Wilson & C. Cardie. 2005. Annotating expressions
of opinions and emotions in language. Language Resources
and Evaluation, 1(2).
362
