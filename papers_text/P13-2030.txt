Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 166?170,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
An Improved MDL-Based Compression Algorithm for
Unsupervised Word Segmentation
Ruey-Cheng Chen
National Taiwan University
1 Roosevelt Rd. Sec. 4
Taipei 106, Taiwan
rueycheng@turing.csie.ntu.edu.tw
Abstract
We study the mathematical properties of
a recently proposed MDL-based unsuper-
vised word segmentation algorithm, called
regularized compression. Our analysis
shows that its objective function can be
efficiently approximated using the nega-
tive empirical pointwise mutual informa-
tion. The proposed extension improves the
baseline performance in both efficiency
and accuracy on a standard benchmark.
1 Introduction
Hierarchical Bayes methods have been main-
stream in unsupervised word segmentation since
the dawn of hierarchical Dirichlet process (Gold-
water et al, 2009) and adaptors grammar (Johnson
and Goldwater, 2009). Despite this wide recog-
nition, they are also notoriously computational
prohibitive and have limited adoption on larger
corpora. While much effort has been directed
to mitigating this issue within the Bayes frame-
work (Borschinger and Johnson, 2011), many
have found minimum description length (MDL)
based methods more promising in addressing the
scalability problem.
MDL-based methods (Rissanen, 1978) rely on
underlying search algorithms to segment the text
in as many possible ways and use description
length to decide which to output. As differ-
ent algorithms explore different trajectories in
the search space, segmentation accuracy depends
largely on the search coverage. Early work in this
line focused more on existing segmentation algo-
rithm, such as branching entropy (Tanaka-Ishii,
2005; Zhikov et al, 2010) and bootstrap voting ex-
perts (Hewlett and Cohen, 2009; Hewlett and Co-
hen, 2011). A recent study (Chen et al, 2012) on
a compression-based algorithm, regularized com-
pression, has achieved comparable performance
result to hierarchical Bayes methods.
Along this line, in this paper we present a novel
extension to the regularized compressor algorithm.
We propose a lower-bound approximate to the
original objective and show that, through analy-
sis and experimentation, this amendment improves
segmentation performance and runtime efficiency.
2 Regularized Compression
The dynamics behind regularized compression is
similar to digram coding (Witten et al, 1999). One
first breaks the text down to a sequence of char-
acters (W0) and then works from that represen-
tation up in an agglomerative fashion, iteratively
removing word boundaries between the two se-
lected word types. Hence, a new sequence Wi
is created in the i-th iteration by merging all the
occurrences of some selected bigram (x, y) in the
original sequence Wi?1. Unlike in digram cod-
ing, where the most frequent pair of word types is
always selected, in regularized compression a spe-
cialized decision criterion is used to balance com-
pression rate and vocabulary complexity:
min. ??f(x, y) + |Wi?1|?H?(Wi?1,Wi)
s.t. either x or y is a character
f(x, y) > nms.
Here, the criterion is written slightly differ-
ently. Note that f(x, y) is the bigram fre-
quency, |Wi?1| the sequence length of Wi?1, and
?H?(Wi?1,Wi) = H?(Wi)? H?(Wi?1) is the dif-
ference between the empirical Shannon entropy
measured on Wi and Wi?1, using maximum like-
lihood estimates. Specifically, this empirical esti-
mate H?(W ) for a sequence W corresponds to:
log |W | ? 1|W |
?
x:types
f(x) log f(x).
For this equation to work, one needs to estimate
other model parameters. See Chen et al (2012)
for a comprehensive treatment.
166
f(x) f(y) f(z) |W |
Wi?1 k l 0 N
Wi k ?m l ?m m N ?m
Table 1: The change between iterations in word
frequency and sequence length in regularized
compression. In the new sequence Wi, each oc-
currence of the x-y bigram is replaced with a new
(conceptually unseen) word z. This has an effect
of reducing the number of words in the sequence.
3 Change in Description Length
The second term of the aforementioned objective
is in fact an approximate to the change in descrip-
tion length. This is made obvious by coding up a
sequence W using the Shannon code, with which
the description length ofW is equal to |W |H?(W ).
Here, the change in description length between se-
quences Wi?1 and Wi is written as:
?L = |Wi|H?(W )? |Wi?1|H?(Wi?1). (1)
Let us focus on this equation. Suppose that the
original sequence Wi?1 is N -word long, the se-
lected word type pair x and y each occurs k and l
times, respectively, and altogether x-y bigram oc-
curs m times in Wi?1. In the new sequence Wi,
each of the m bigrams is replaced with an un-
seen word z = xy. These altogether have reduced
the sequence length by m. The end result is that
compression moves probability masses from one
place to the other, causing a change in descrip-
tion length. See Table 1 for a summary to this
exchange.
Now, as we expand Equation (1) and reorganize
the remaining, we find that:
?L = (N ?m) log(N ?m)?N logN
+ k log k ? (k ?m) log(k ?m)
+ l log l ? (l ?m) log(l ?m)
+ 0 log 0?m logm
(2)
Note that each line in Equation (2) is of the form
x1 log x1 ? x2 log x2 for some x1, x2 ? 0. We
exploit this pattern and derive a bound for ?L
through analysis. Consider g(x) = x log x. Since
g??(x) > 0 for x ? 0, by the Taylor series we have
the following relations for any x1, x2 ? 0:
g(x1)? g(x2) ? (x1 ? x2)g?(x1),
g(x1)? g(x2) ? (x1 ? x2)g?(x2).
Plugging these into Equation (2), we have:
m log (k ?m)(l ?m)Nm ? ?L ? ?. (3)
The lower bound1 at the left-hand side is a best-
case estimate. As our aim is to minimize ?L, we
use this quantity to serve as an approximate.
4 Proposed Method
Based on this finding, we propose the following
two variations (see Figure 1) for the regularized
compression framework:
? G1: Replacing the second term in the origi-
nal objective with the lower bound in Equa-
tion (3). The new objective function is writ-
ten out as Equation (4).
? G2: Same as G1 except that the lower bound
is divided by f(x, y) beforehand. The nor-
malized lower bound approximates the per-
word change in description length, as shown
in Equation (5). With this variation, the func-
tion remains in a scalarized form as the orig-
inal does.
We use the following procedure to compute de-
scription length. Given a word sequence W , we
write out all the induced word types (say, M types
in total) entry by entry as a character sequence, de-
noted as C. Then the overall description length is:
|W |H?(W ) + |C|H?(C) + M ? 12 log |W |. (6)
Three free parameters, ?, ?, and nms remain to
be estimated. A detailed treatment on parameter
estimation is given in the following paragraphs.
Trade-off ? This parameter controls the bal-
ance between compression rate and vocabulary
complexity. Throughout this experiment, we es-
timated this parameter using MDL-based grid
search. Multiple search runs at different granular-
ity levels were employed as necessary.
Compression rate ? This is the minimum
threshold value for compression rate. The com-
pressor algorithm would go on as many iteration
as possible until the overall compression rate (i.e.,
1Sharp-eyed readers may have noticed the similarity be-
tween the lower bound and the negative (empirical) point-
wise mutual information. In fact, when f(z) > 0 in Wi?1, it
can be shown that limm?0 ?L/m converges to the empirical
pointwise mutual information (proof omitted here).
167
G1 ? f(x, y)
(
log (f(x)? f(x, y))(f(y)? f(x, y))|Wi?1|f(x, y)
? ?
)
(4)
G2 ? ??f(x, y) + log
(f(x)? f(x, y))(f(y)? f(x, y))
|Wi?1|f(x, y)
(5)
Figure 1: The two newly-proposed objective functions.
word/character ratio) is lower than ?. Setting this
value to 0 forces the compressor to go on until
no more can be done. In this paper, we exper-
imented with predetermined ? values as well as
those learned from MDL-based grid search.
Minimum support nms We simply followed the
suggested setting nms = 3 (Chen et al, 2012).
5 Evaluation
5.1 Setup
In the experiment, we tested our methods on
Brent?s derivation of the Bernstein-Ratner cor-
pus (Brent and Cartwright, 1996; Bernstein-
Ratner, 1987). This dataset is distributed via the
CHILDES project (MacWhinney and Snow, 1990)
and has been commonly used as a standard bench-
mark for phonetic segmentation. Our baseline
method is the original regularized compressor al-
gorithm (Chen et al, 2012). In our experiment, we
considered the following three search settings for
finding the model parameters:
(a) Fix ? to 0 and vary ? to find the best value (in
the sense of description length);
(b) Fix ? to the best value found in setting (a)
and vary ?;
(c) Set ? to a heuristic value 0.37 (Chen et al,
2012) and vary ?.
Settings (a) and (b) can be seen as running a
stochastic grid searcher one round for each param-
eter2. Note that we tested (c) here only to compare
with the best baseline setting.
5.2 Result
Table 2 summarizes the result for each objective
and each search setting. The best (?, ?) pair for
2A more formal way to estimate both ? and ? is to run
a stochastic searcher that varies between settings (a) and (b),
fixing the best value found in the previous run. Here, for
simplicity, we leave this to future work.
Run P R F
Baseline 76.9 81.6 79.2
G1 (a) ? : 0.030 76.4 79.9 78.1
G1 (b) ? : 0.38 73.4 80.2 76.8
G1 (c) ? : 0.010 75.7 80.4 78.0
G2 (a) ? : 0.002 82.1 80.0 81.0
G2 (b) ? : 0.36 79.1 81.7 80.4
G2 (c) ? : 0.004 79.3 84.2 81.7
Table 2: The performance result on the Bernstein-
Ratner corpus. Segmentation performance is mea-
sured using word-level precision (P), recall (R),
and F-measure (F).
G1 is (0.03, 0.38) and the best for G2 is (0.002,
0.36). On one hand, the performance ofG1 is con-
sistently inferior to the baseline across all settings.
Although approximation error was one possible
cause, we noticed that the compression process
was no longer properly regularized, since f(x, y)
and the ?L estimate in the objective are intermin-
gled. In this case, adjusting ? has little effect in
balancing compression rate and complexity.
The second objective G2, on the other hand,
did not suffer as much from the aforementioned
lack of regularization. We found that, in all three
settings, G2 outperforms the baseline by 1 to 2
percentage points in F-measure. The best perfor-
mance result achieved by G2 in our experiment is
81.7 in word-level F-measure, although this was
obtained from search setting (c), using a heuristic
? value 0.37. It is interesting to note that G1 (b)
and G2 (b) also gave very close estimates to this
heuristic value. Nevertheless, it remains an open
issue whether there is a connection between the
optimal ? value and the true word/token ratio (?
0.35 for Bernstein-Ratner corpus).
The result has led us to conclude that MDL-
based grid search is efficient in optimizing seg-
mentation accuracy. Minimization of descrip-
tion length is in general aligned with perfor-
mance improvement, although under finer gran-
ularity MDL-based search may not be as effec-
168
Method P R F
Adaptors grammar, colloc3-syllable Johnson and Goldwater (2009) 86.1 88.4 87.2
Regularized compression + MDL, G2 (b) ? 79.1 81.7 80.4
Regularized compression + MDL Chen et al (2012) 76.9 81.6 79.2
Adaptors grammar, colloc Johnson and Goldwater (2009) 78.4 75.7 77.1
Particle filter, unigram Bo?rschinger and Johnson (2012) ? ? 77.1
Regularized compression + MDL, G1 (b) ? 73.4 80.2 76.8
Bootstrap voting experts + MDL Hewlett and Cohen (2011) 79.3 73.4 76.2
Nested Pitman-Yor process, bigram Mochihashi et al (2009) 74.8 76.7 75.7
Branching entropy + MDL Zhikov et al (2010) 76.3 74.5 75.4
Particle filter, bigram Bo?rschinger and Johnson (2012) ? ? 74.5
Hierarchical Dirichlet process Goldwater et al (2009) 75.2 69.6 72.3
Table 3: The performance chart on the Bernstein-Ratner corpus, in descending order of word-level F-
measure. We deliberately reproduced the results for adaptors grammar and regularized compression. The
other measurements came directly from the literature.
tive. In our experiment, search setting (b) won
out on description length for both objectives, while
the best performance was in fact achieved by the
others. It would be interesting to confirm this
by studying the correlation between description
length and word-level F-measure.
In Table 3, we summarize many published re-
sults for segmentation methods ever tested on the
Bernstein-Ratner corpus. Of the proposed meth-
ods, we include only setting (b) since it is more
general than the others. From Table 3, we find that
the performance of G2 (b) is competitive to other
state-of-the-art hierarchical Bayesian models and
MDL methods, though it still lags 7 percentage
points behind the best result achieved by adap-
tors grammar with colloc3-syllable. We also com-
pare adaptors grammar to regularized compressor
on average running time, which is shown in Ta-
ble 4. On our test machine, it took roughly 15
hours for one instance of adaptors grammar with
colloc3-syllable to run to the finish. Yet an im-
proved regularized compressor could deliver the
result in merely 1.25 second. In other words, even
in an 100 ? 100 grid search, the regularized com-
pressor algorithm can still finish 4 to 5 times ear-
lier than one single adaptors grammar instance.
6 Concluding Remarks
In this paper, we derive a new lower-bound ap-
proximate to the objective function used in the
regularized compression algorithm. As computing
the approximate no longer relies on the change in
lexicon entropy, the new compressor algorithm is
made more efficient than the original. Besides run-
Method Time (s)
Adaptors grammar, colloc3-syllable 53826
Adaptors grammar, colloc 10498
Regularized compressor 1.51
Regularized compressor, G1 (b) 0.60
Regularized compressor, G2 (b) 1.25
Table 4: The average running time in seconds on
the Bernstein-Ratner corpus for adaptors grammar
(per fold, based on trace output) and regularized
compressors, tested on an Intel Xeon 2.5GHz 8-
core machine with 8GB RAM.
time efficiency, our experiment result also shows
improved performance. Using MDL alone, one
proposed method outperforms the original regu-
larized compressor (Chen et al, 2012) in preci-
sion by 2 percentage points and in F-measure by 1.
Its performance is only second to the state of the
art, achieved by adaptors grammar with colloc3-
syllable (Johnson and Goldwater, 2009).
A natural extension of this work is to repro-
duce this result on some other word segmenta-
tion benchmarks, specifically those in other Asian
languages (Emerson, 2005; Zhikov et al, 2010).
Furthermore, it would be interesting to investigate
stochastic optimization techniques for regularized
compression that simultaneously fit both ? and ?.
We believe this would be the key to adapt the al-
gorithm to larger datasets.
Acknowledgments
We thank the anonymous reviewers for their valu-
able feedback.
169
References
Nan Bernstein-Ratner. 1987. The phonology of parent
child speech. Children?s language, 6:159?174.
Benjamin Borschinger and Mark Johnson. 2011. A
particle filter algorithm for bayesian word segmen-
tation. In Proceedings of the Australasian Language
Technology Association Workshop 2011, pages 10?
18, Canberra, Australia, December.
Benjamin Bo?rschinger and Mark Johnson. 2012. Us-
ing rejuvenation to improve particle filtering for
bayesian word segmentation. In Proceedings of the
50th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers),
pages 85?89, Jeju Island, Korea, July. Association
for Computational Linguistics.
Michael R. Brent and Timothy A. Cartwright. 1996.
Distributional regularity and phonotactic constraints
are useful for segmentation. In Cognition, pages 93?
125.
Ruey-Cheng Chen, Chiung-Min Tsai, and Jieh Hsiang.
2012. A regularized compression method to unsu-
pervised word segmentation. In Proceedings of the
Twelfth Meeting of the Special Interest Group on
Computational Morphology and Phonology, SIG-
MORPHON ?12, pages 26?34, Montreal, Canada.
Association for Computational Linguistics.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing, volume 133. Jeju Island, Korea.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21?54, July.
Daniel Hewlett and Paul Cohen. 2009. Bootstrap vot-
ing experts. In Proceedings of the 21st international
jont conference on Artifical intelligence, IJCAI?09,
pages 1071?1076, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Daniel Hewlett and Paul Cohen. 2011. Fully unsuper-
vised word segmentation with BVE and MDL. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
?11, pages 540?545, Portland, Oregon. Association
for Computational Linguistics.
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, NAACL ?09, pages
317?325, Boulder, Colorado. Association for Com-
putational Linguistics.
Brian MacWhinney and Catherine Snow. 1990. The
child language data exchange system: an update.
Journal of child language, 17(2):457?472, June.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested Pitman-Yor language modeling.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1 - Volume 1, ACL ?09, pages
100?108, Suntec, Singapore. Association for Com-
putational Linguistics.
Jorma Rissanen. 1978. Modeling by shortest data de-
scription. Automatica, 14(5):465?471, September.
Kumiko Tanaka-Ishii. 2005. Entropy as an indica-
tor of context boundaries: An experiment using a
web search engine. In Robert Dale, Kam-Fai Wong,
Jian Su, and Oi Kwong, editors, Natural Language
Processing IJCNLP 2005, volume 3651 of Lecture
Notes in Computer Science, chapter 9, pages 93?
105. Springer Berlin / Heidelberg, Berlin, Heidel-
berg.
Ian H. Witten, Alistair Moffat, and Timothy C. Bell.
1999. Managing gigabytes (2nd ed.): compressing
and indexing documents and images. Morgan Kauf-
mann Publishers Inc., San Francisco, CA, USA.
Valentin Zhikov, Hiroya Takamura, and Manabu Oku-
mura. 2010. An efficient algorithm for unsuper-
vised word segmentation with branching entropy
and MDL. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?10, pages 832?842, Cambridge,
Massachusetts. Association for Computational Lin-
guistics.
170
