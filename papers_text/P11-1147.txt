Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1466?1475,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Discovery of Domain-Specific Knowledge from Text
Dirk Hovy, Chunliang Zhang, Eduard Hovy
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
{dirkh, czheng, hovy}@isi.edu
Anselmo Pen?as
UNED NLP and IR Group
Juan del Rosal 16
28040 Madrid, Spain
anselmo@lsi.uned.es
Abstract
Learning by Reading (LbR) aims at enabling
machines to acquire knowledge from and rea-
son about textual input. This requires knowl-
edge about the domain structure (such as en-
tities, classes, and actions) in order to do in-
ference. We present a method to infer this im-
plicit knowledge from unlabeled text. Unlike
previous approaches, we use automatically ex-
tracted classes with a probability distribution
over entities to allow for context-sensitive la-
beling. From a corpus of 1.4m sentences, we
learn about 250k simple propositions about
American football in the form of predicate-
argument structures like ?quarterbacks throw
passes to receivers?. Using several statisti-
cal measures, we show that our model is able
to generalize and explain the data statistically
significantly better than various baseline ap-
proaches. Human subjects judged up to 96.6%
of the resulting propositions to be sensible.
The classes and probabilistic model can be
used in textual enrichment to improve the per-
formance of LbR end-to-end systems.
1 Introduction
The goal of Learning by Reading (LbR) is to enable
a computer to learn about a new domain and then
to reason about it in order to perform such tasks as
question answering, threat assessment, and explana-
tion (Strassel et al, 2010). This requires joint efforts
from Information Extraction, Knowledge Represen-
tation, and logical inference. All these steps depend
on the system having access to basic, often unstated,
foundational knowledge about the domain.
Most documents, however, do not explicitly men-
tion this information in the text, but assume basic
background knowledge about the domain, such as
positions (?quarterback?), titles (?winner?), or ac-
tions (?throw?) for sports game reports. Without
this knowledge, the text will not make sense to the
reader, despite being well-formed English. Luckily,
the information is often implicitly contained in the
document or can be inferred from similar texts.
Our system automatically acquires domain-
specific knowledge (classes and actions) from large
amounts of unlabeled data, and trains a probabilis-
tic model to determine and apply the most infor-
mative classes (quarterback, etc.) at appropriate
levels of generality for unseen data. E.g., from
sentences such as ?Steve Young threw a pass to
Michael Holt?, ?Quarterback Steve Young finished
strong?, and ?Michael Holt, the receiver, left early?
we can learn the classes quarterback and receiver,
and the proposition ?quarterbacks throw passes to
receivers?.
We will thus assume that the implicit knowl-
edge comes in two forms: actions in the form of
predicate-argument structures, and classes as part of
the source data. Our task is to identify and extract
both. Since LbR systems must quickly adapt and
scale well to new domains, we need to be able to
work with large amounts of data and minimal super-
vision. Our approach produces simple propositions
about the domain (see Figure 1 for examples of ac-
tual propositions learned by our system).
American football was the first official evaluation
domain in the DARPA-sponsored Machine Reading
program, and provides the background for a number
1466
of LbR systems (Mulkar-Mehta et al, 2010). Sports
is particularly amenable, since it usually follows a
finite, explicit set of rules. Due to its popularity,
results are easy to evaluate with lay subjects, and
game reports, databases, etc. provide a large amount
of data. The same need for basic knowledge appears
in all domains, though. In music, musicians play in-
struments, in electronics, components constitute cir-
cuits, circuits use electricity, etc.
Teams beat teams
Teams play teams
Quarterbacks throw passes
Teams win games
Teams defeat teams
Receivers catch passes
Quarterbacks complete passes
Quarterbacks throw passes to receivers
Teams play games
Teams lose games
Figure 1: The ten most frequent propositions discovered
by our system for the American football domain
Our approach differs from verb-argument identi-
fication or Named Entity (NE) tagging in several re-
spects. While previous work on verb-argument se-
lection (Pardo et al, 2006; Fan et al, 2010) uses
fixed sets of classes, we cannot know a priori how
many and which classes we will encounter. We
therefore provide a way to derive the appropriate
classes automatically and include a probability dis-
tribution for each of them. Our approach is thus
less restricted and can learn context-dependent, fine-
grained, domain-specific propositions. While a NE-
tagged corpus could produce a general proposition
like ?PERSON throws to PERSON?, our method
enables us to distinguish the arguments and learn
?quarterback throws to receiver? for American foot-
ball and ?outfielder throws to third base? for base-
ball. While in NE tagging each word has only one
correct tag in a given context, we have hierarchical
classes: an entity can be correctly labeled as a player
or a quarterback (and possibly many more classes),
depending on the context. By taking context into
account, we are also able to label each sentence in-
dividually and account for unseen entities without
using external resources.
Our contributions are:
? we use unsupervised learning to train a model
that makes use of automatically extracted
classes to uncover implicit knowledge in the
form of predicate-argument propositions
? we evaluate the explanatory power, generaliza-
tion capability, and sensibility of the proposi-
tions using both statistical measures and human
judges, and compare them to several baselines
? we provide a model and a set of propositions
that can be used to improve the performance
of end-to-end LbR systems via textual enrich-
ment.
2 Methods
INPUT:
Steve Young threw a pass to Michael Holt
1. PARSE INPUT:
2. JOIN NAMES, EXTRACT PREDICATES:
NVN: Steve_Young throw pass 
NVNPN: Steve_Young throw pass to Michael_Holt
3. DECODE TO INFER PROPOSITIONS:
QUARTERBACK throw pass
QUARTERBACK throw pass to RECEIVER
Steve/NNP
Young/NNP
throw/VBD
pass/NN
a/DT
to/TO
Michael/NNP
Holt/NNP
nsubj
dobj
prep
nn
nn
pobjdet
Steve_Young    threw      a         pass       to    Michael_Holt
s1 s2 x1 s3 s4 s5
p1 p2 p3 p4 p5
quarterback      throw                pass          to         receiver
Figure 2: Illustrated example of different processing steps
Our running example will be ?Steve Young threw
a pass to Michael Holt?. This is an instance of the
underlying proposition ?quarterbacks throw passes
to receivers?, which is not explicitly stated in the
data. A proposition is thus a more general state-
ment about the domain than the sentences it de-
rives. It contains domain-specific classes (quarter-
back, receiver), as well as lexical items (?throw?,
?pass?). In order to reproduce the proposition,
given the input sentences, our system has to not
only identify the classes, but also learn when to
1467
abstract away from the lexical form to the ap-
propriate class and when to keep it (cf. Figure
2, step 3). To facilitate extraction, we focus on
propositions with the following predicate-argument
structures: NOUN-VERB-NOUN (e.g., ?quarter-
backs throw passes?), or NOUN-VERB-NOUN-
PREPOSITION-NOUN (e.g., ?quarterbacks throw
passes to receivers?. There is nothing, though, that
prevents the use of other types of structures as well.
We do not restrict the verbs we consider (Pardo et
al., 2006; Ritter et al, 2010)), which extracts a high
number of hapax structures.
Given a sentence, we want to find the most likely
class for each word and thereby derive the most
likely proposition. Similar to Pardo et al (2006), we
assume the observed data was produced by a process
that generates the proposition and then transforms
the classes into a sentence, possibly adding addi-
tional words. We model this as a Hidden Markov
Model (HMM) with bigram transitions (see Section
2.3) and use the EM algorithm (Dempster et al,
1977) to train it on the observed data, with smooth-
ing to prevent overfitting.
2.1 Data
We use a corpus of about 33k texts on Ameri-
can football, extracted from the New York Times
(Sandhaus, 2008). To identify the articles, we rely
on the provided ?football? keyword classifier. The
resulting corpus comprises 1, 359, 709 sentences
from game reports, background stories, and opin-
ion pieces. In a first step, we parse all documents
with the Stanford dependency parser (De Marneffe
et al, 2006) (see Figure 2, step 1). The output
is lemmatized (collapsing ?throws?, ?threw?, etc.,
into ?throw?), and marked for various dependen-
cies (nsubj, amod, etc.). This enables us to ex-
tract the predicate argument structure, like subject-
verb-object, or additional prepositional phrases (see
Figure 2, step 2). These structures help to sim-
plify the model by discarding additional words like
modifiers, determiners, etc., which are not essen-
tial to the proposition. The same approach is used
by (Brody, 2007). We also concatenate multi-
word names (identified by sequences of NNPs) with
an underscore to form a single token (?Steve/NNP
Young/NNP?? ?Steve Young?).
2.2 Deriving Classes
To derive the classes used for entities, we do not re-
strict ourselves to a fixed set, but derive a domain-
specific set directly from the data. This step is per-
formed simultaneously with the corpus generation
described above. We utilize three syntactic construc-
tions to identify classes, namely nominal modifiers,
copula verbs, and appositions, see below. This is
similar in nature to Hearst?s lexico-syntactic patterns
(Hearst, 1992) and other approaches that derive IS-
A relations from text. While we find it straightfor-
ward to collect classes for entities in this way, we
did not find similar patterns for verbs. Given a suit-
able mechanism, however, these could be incorpo-
rated into our framework as well.
Nominal modifier are common nouns (labeled
NN) that precede proper nouns (labeled NNP), as in
?quarterback/NN Steve/NNP Young/NNP?, where
?quarterback? is the nominal modifier of ?Steve
Young?. Similar information can be gained from ap-
positions (e.g., ?Steve Young, the quarterback of his
team, said...?), and copula verbs (?Steve Young is
the quarterback of the 49ers?). We extract those co-
occurrences and store the proper nouns as entities
and the common nouns as their possible classes. For
each pair of class and entity, we collect counts over
the corpus to derive probability distributions.
Entities for which we do not find any of the above
patterns in our corpus are marked ?UNK?. These
entities are instantiated with the 20 most frequent
classes. All other (non-entity) words (including
verbs) have only their identity as class (i.e., ?pass?
remains ?pass?).
The average number of classes per entity is 6.87.
The total number of distinct classes for entities is
63, 942. This is a huge number to model in our state
space.1 Instead of manually choosing a subset of the
classes we extracted, we defer the task of finding the
best set to the model.
We note, however, that the distribution of classes
for each entity is highly skewed. Due to the unsuper-
vised nature of the extraction process, many of the
extracted classes are hapaxes and/or random noise.
Most entities have only a small number of applicable
classes (a football player usually has one main posi-
1NE taggers usually use a set of only a few dozen classes at
most.
1468
tion, and a few additional roles, such as star, team-
mate, etc.). We handle this by limiting the number of
classes considered to 3 per entity. This constraint re-
duces the total number of distinct classes to 26, 165,
and the average number of classes per entity to 2.53.
The reduction makes for a more tractable model size
without losing too much information. The class al-
phabet is still several magnitudes larger than that for
NE or POS tagging. Alternatively, one could use ex-
ternal resources such as Wikipedia, Yago (Suchanek
et al, 2007), or WordNet++ (Ponzetto and Navigli,
2010) to select the most appropriate classes for each
entity. This is likely to have a positive effect on the
quality of the applicable classes and merits further
research. Here, we focus on the possibilities of a
self-contained system without recurrence to outside
resources.
The number of classes we consider for each entity
also influences the number of possible propositions:
if we consider exactly one class per entity, there will
be little overlap between sentences, and thus no gen-
eralization possible?the model will produce many
distinct propositions. If, on the other hand, we used
only one class for all entities, there will be similar-
ities between many sentences?the model will pro-
duce very few distinct propositions.
2.3 Probabilistic Model
INPUT:
Steve Young threw a pass to Michael Holt
PARSE:
INSTANCES:
Steve_Young throw pass 
Steve_Young throw pass to Michael_Holt
PROPOSITIONS:
Quarterback throw pass
Quarterback throw pass to receiver
Steve
Young
throw
pass
a
to
Michael
Holt
nsubj
dobj
prep
nn
nn
pobjdet
Steve_Young    threw      a         pass       to    Michael_Holt
s1 s2 x1 s3 s4 s5
p1 p2 p3 p4 p5
quarterback      throw                pass          to         receiver
Figure 3: Graphical model for the running example
We use a generative noisy-channel model to cap-
ture the joint probability of input sentences and their
underlying proposition. Our generative story of how
a sentence s (with words s1, ..., sn) was generated
assumes that a proposition p is generated as a se-
quence of classes p1, ..., pn, with transition proba-
bilities P (pi|pi?1). Each class pi generates a word
si with probability P (si|pi). We allow additional
words x in the sentence which do not depend on any
class in the proposition and are thus generated inde-
pendently with P (x) (cf. model in Figure 3).
Since we observe the co-occurrence counts of
classes and entities in the data, we can fix the emis-
sion parameter P (s|p) in our HMM. Further, we do
not want to generate sentences from propositions, so
we can omit the step that adds the additional words
x in our model. The removal of these words is re-
flected by the preprocessing step that extracts the
structure (cf. Section 2.1).
Our model is thus defined as
P (s,p) =P (p1) ?
n?
i=1
(
P (pi|pi?1) ? P (si|pi)
)
(1)
where si, pi denote the ith word of sentence s and
proposition p, respectively.
3 Evaluation
We want to evaluate how well our model predicts
the data, and how sensible the resulting propositions
are. We define a good model as one that generalizes
well and produces semantically useful propositions.
We encounter two problems. First, since we de-
rive the classes in a data-driven way, we have no
gold standard data available for comparison. Sec-
ond, there is no accepted evaluation measure for this
kind of task. Ultimately, we would like to evaluate
our model externally, such as measuring its impact
on performance of a LbR system. In the absence
thereof, we resort to several complementary mea-
sures, as well as performing an annotation task. We
derive evaluation criteria as follows. A model gener-
alizes well if it can cover (?explain?) all the sentences
in the corpus with a few propositions. This requires
a measure of generality. However, while a proposi-
tion such as ?PERSON does THING?, has excellent
generality, it possesses no discriminating power. We
also need the propositions to partition the sentences
into clusters of semantic similarity, to support effec-
tive inference. This requires a measure of distribu-
tion. Maximal distribution, achieved by assigning
every sentence to a different proposition, however,
is not useful either. We need to find an appropri-
ate level of generality within which the sentences
are clustered into propositions for the best overall
groupings to support inference.
To assess the learned model, we apply the mea-
sures of generalization, entropy, and perplexity (see
1469
Sections 3.2, 3.3, and 3.4). These measures can be
used to compare different systems. We do not at-
tempt to weight or combine the different measures,
but present each in its own right.
Further, to assess label accuracy, we use Ama-
zon?s Mechanical Turk annotators to judge the sen-
sibility of the propositions produced by each sys-
tem (Section 3.5). We reason that if our system
learned to infer the correct classes, then the resulting
propositions should constitute true, general state-
ments about that domain, and thus be judged as sen-
sible.2 This approach allows the effective annotation
of sufficient amounts of data for an evaluation (first
described for NLP in (Snow et al, 2008)).
3.1 Evaluation Data
With the trained model, we use Viterbi decoding to
extract the best class sequence for each example in
the data. This translates the original corpus sen-
tences into propositions. See steps 2 and 3 in Figure
2.
We create two baseline systems from the same
corpus, one which uses the most frequent class
(MFC) for each entity, and another one which uses
a class picked at random from the applicable classes
of each entity.
Ultimately, we are interested in labeling unseen
data from the same domain with the correct class,
so we evaluate separately on the full corpus and
the subset of sentences that contain unknown enti-
ties (i.e., entities for which no class information was
available in the corpus, cf. Section 2.2).
For the latter case, we select all examples con-
taining at least one unknown entity (labeled UNK),
resulting in a subset of 41, 897 sentences, and repeat
the evaluation steps described above. Here, we have
to consider a much larger set of possible classes per
entity (the 20 overall most frequent classes). The
MFC baseline for these cases is the most frequent
of the 20 classes for UNK tokens, while the random
baseline chooses randomly from that set.
3.2 Generalization
Generalization measures how widely applicable the
produced propositions are. A completely lexical ap-
2Unfortunately, if judged insensible, we can not infer
whether our model used the wrong class despite better options,
or whether we simply have not learned the correct label.
entropy
Page 1
full data set
unknown entities
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.04 0.01
0.12 0.09
0.25
0.66
Generalization
random
MFC
model
Figure 4: Generalization of models on the data sets
proach, at one extreme, would turn each sentence
into a separate proposition, thus achieving a gener-
alization of 0%. At the other extreme, a model that
produces only one proposition would generalize ex-
tremely well (but would fail to explain the data in
any meaningful way). Both are of course not desir-
able.
We define generalization as
g = 1?
|propositions|
|sentences|
(2)
The results in Figure 4 show that our model is
capable of abstracting away from the lexical form,
achieving a generalization rate of 25% for the full
data set. The baseline approaches do significantly
worse, since they are unable to detect similarities
between lexically different examples, and thus cre-
ate more propositions. Using a two-tailed t-test, the
difference between our model and each baseline is
statistically significant at p < .001.
Generalization on the unknown entity data set is
even higher (65.84%). The difference between the
model and the baselines is again statistically signif-
icant at p < .001. MFC always chooses the same
class for UNK, regardless of context, and performs
much worse. The random baseline chooses between
20 classes per entity instead of 3, and is thus even
less general.
3.3 Normalized Entropy
Entropy is used in information theory to measure
how predictable data is. 0 means the data is com-
pletely predictable. The higher the entropy of our
propositions, the less well they explain the data. We
are looking for models with low entropy. The ex-
treme case of only one proposition has 0 entropy:
1470
entropy
Page 1
full data set
unknown entities
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
1.00 1.000.99 0.99
0.89
0.50
Normalized Entropy
random
MFC
model
Figure 5: Entropy of models on the data sets
we know exactly which sentences are produced by
the proposition.
Entropy is directly influenced by the number of
propositions used by a system.3 In order to compare
different models, we thus define normalized entropy
as
HN =
?
n?
i=0
Pi ? logPi
log n
(3)
where Pi is the coverage of the proposition, or the
percentage of sentences explained by it, and n is the
number of distinct propositions.
The entropy of our model on the full data set is
relatively high with 0.89, see Figure 5. The best
entropy we can hope to achieve given the number
of propositions and sentences is actually 0.80 (by
concentrating the maximum probability mass in one
proposition). The model thus does not perform as
badly as the number might suggest. The entropy of
our model on unseen data is better, with 0.50 (best
possible: 0.41). This might be due to the fact that
we considered more classes for UNK than for regu-
lar entities.
3.4 Perplexity
Since we assume that propositions are valid sen-
tences in our domain, good propositions should have
a higher probability than bad propositions in a lan-
guage model. We can compute this using perplex-
3Note that how many classes we consider per entity influ-
ences how many propositions are produced (cf. Section 2.2),
and thus indirectly puts a bound on entropy.
entropy
Page 1
full data set unknown entities
50.00
51.00
52.00
53.00
54.00
55.00
56.00
57.00
58.00
59.00
60.00 59.52
57.0357.03 57.1556.84
54.92
Perplexity
random
MFC
model
Figure 6: Perplexity of models on the data sets
ity:4
perplexity(data) = 2
? logP (data)
n (4)
where P (data) is the product of the proposition
probabilities, and n is the number of propositions.
We use the uni-, bi-, and trigram counts of the
GoogleGrams corpus (Brants and Franz, 2006) with
simple interpolation to compute the probability of
each proposition.
The results in Figure 6 indicate that the proposi-
tions found by the model are preferable to the ones
found by the baselines. As would be expected, the
sensibility judgements for MFC and model5 (Tables
1 and 2, Section 3.5) are perfectly anti-correlated
(correlation coefficient ?1) with the perplexity for
these systems in each data set. However, due to the
small sample size, this should be interpreted cau-
tiously.
3.5 Sensibility and Label Accuracy
In unsupervised training, the model with the best
data likelihood does not necessarily produce the best
label accuracy. We evaluate label accuracy by pre-
senting subjects with the propositions we obtained
from the Viterbi decoding of the corpus, and ask
them to rate their sensibility. We compare the dif-
ferent systems by computing sensibility as the per-
centage of propositions judged sensible for each sys-
tem. Since the underlying probability distributions
are quite different, we weight the sensibility judge-
ment for each proposition by the likelihood of that
proposition. We report results for both aggregate
4Perplexity also quantifies the uncertainty of the resulting
propositions, where 0 perplexity means no uncertainty.
5We did not collect sensibility judgements for the random
baseline.
1471
accuracy
Page 1
System
90.16 92.13 69.35 70.57 88.84 90.37
94.28 96.55 70.93 70.45 93.06 95.16
100 most frequent random combined
Data set agg maj agg maj agg maj
full
baseline
model
Table 1: Percentage of propositions derived from labeling the full data set that were judged sensible
accuracy
Page 1
System
51.92 51.51 32.39 28.21 50.39 49.66
66.00 69.57 48.14 41.74 64.83 67.76
100 most frequent random combined
Data set agg maj agg maj agg maj
unknown
baseline
model
Table 2: Percentage of propositions derived from labeling unknown entities that were judged sensible
sensibility (using the total number of individual an-
swers), and majority sensibility, where each propo-
sition is scored according to the majority of annota-
tors? decisions.
The model and baseline propositions for the full
data set are both judged highly sensible, achieving
accuracies of 96.6% and 92.1% (cf. Table 1). While
our model did slightly better, the differences are not
statistically significant when using a two-tailed test.
The propositions produced by the model from un-
known entities are less sensible (67.8%), albeit still
significantly above chance level, and the baseline
propositions for the same data set (p < 0.01). Only
49.7% propositions of the baseline were judged sen-
sible (cf. Table 2).
3.5.1 Annotation Task
Our model finds 250, 169 distinct propositions,
the MFC baseline 293, 028. We thus have to restrict
ourselves to a subset in order to judge their sensi-
bility. For each system, we sample the 100 most
frequent propositions and 100 random propositions
found for both the full data set and the unknown enti-
ties6 and have 10 annotators rate each proposition as
sensible or insensible. To identify and omit bad an-
notators (?spammers?), we use the method described
in Section 3.5.2, and measure inter-annotator agree-
ment as described in Section 3.5.3. The details of
this evaluation are given below, the results can be
found in Tables 1 and 2.
The 200 propositions from each of the four sys-
6We omit the random baseline here due to size issues, and
because it is not likely to produce any informative comparison.
tems (model and baseline on both full and unknown
data set), contain 696 distinct propositions. We
break these up into 70 batches (Amazon Turk an-
notation HIT pages) of ten propositions each. For
each proposition, we request 10 annotators. Overall,
148 different annotators participated in our annota-
tion. The annotators are asked to state whether each
proposition represents a sensible statement about
American Football or not. A proposition like ?Quar-
terbacks can throw passes to receivers? should make
sense, while ?Coaches can intercept teams? does
not. To ensure that annotators judge sensibility and
not grammaticality, we format each proposition the
same way, namely pluralizing the nouns and adding
?can? before the verb. In addition, annotators can
state whether a proposition sounds odd, seems un-
grammatical, is a valid sentence, but against the
rules (e.g., ?Coaches can hit players?) or whether
they do not understand it.
3.5.2 Spammers
Some (albeit few) annotators on Mechanical Turk
try to complete tasks as quickly as possible with-
out paying attention to the actual requirements, in-
troducing noise into the data. We have to identify
these spammers before the evaluation. One way is
to include tests. Annotators that fail these tests will
be excluded. We use a repetition (first and last ques-
tion are the same), and a truism (annotators answer-
ing ?no? either do not know about football or just
answered randomly). Alternatively, we can assume
that good annotators, who are the majority, will ex-
hibit similar behavior to one another, while spam-
1472
mers exhibit a deviant answer pattern. To identify
those outliers, we compare each annotator?s agree-
ment to the others and exclude those whose agree-
ment falls more than one standard deviation below
the average overall agreement.
We find that both methods produce similar results.
The first method requires more careful planning, and
the resulting set of annotators still has to be checked
for outliers. The second method has the advantage
that it requires no additional questions. It includes
the risk, though, that one selects a set of bad annota-
tors solely because they agree with one another.
3.5.3 Agreement
agreement
Page 1
0.88 0.76 0.82
? 0.45 0.50 0.48
0.66 0.53 0.58
measure 100 most frequent random combined
agreement
G-index
Table 3: Agreement measures for different samples
We use inter-annotator agreement to quantify the
reliability of the judgments. Apart from the simple
agreement measure, which records how often an-
notators choose the same value for an item, there
are several statistics that qualify this measure by ad-
justing for other factors. One frequently used mea-
sure, Cohen?s ?, has the disadvantage that if there
is prevalence of one answer, ? will be low (or even
negative), despite high agreement (Feinstein and Ci-
cchetti, 1990). This phenomenon, known as the ?
paradox, is a result of the formula?s adjustment for
chance agreement. As shown by Gwet (2008), the
true level of actual chance agreement is realistically
not as high as computed, resulting in the counterin-
tuitive results. We include it for comparative rea-
sons. Another statistic, the G-index (Holley and
Guilford, 1964), avoids the paradox. It assumes that
expected agreement is a function of the number of
choices rather than chance. It uses the same general
formula as ?,
(Pa ? Pe)
(1? Pe)
(5)
where Pa is the actual raw agreement measured, and
Pe is the expected agreement. The difference with
? is that Pe for the G-index is defined as Pe = 1/q,
where q is the number of available categories, in-
stead of expected chance agreement. Under most
conditions, G and ? are equivalent, but in the case
of high raw agreement and few categories, G gives a
more accurate estimation of the agreement. We thus
report raw agreement, ?, and G-index.
Despite early spammer detection, there are still
outliers in the final data, which have to be accounted
for when calculating agreement. We take the same
approach as in the statistical spammer detection and
delete outliers that are more than one standard devi-
ation below the rest of the annotators? average.
The raw agreement for both samples combined is
0.82, G = 0.58, and ? = 0.48. The numbers show
that there is reasonably high agreement on the label
accuracy.
4 Related Research
The approach we describe is similar in nature to un-
supervised verb argument selection/selectional pref-
erences and semantic role labeling, yet goes be-
yond it in several ways. For semantic role label-
ing (Gildea and Jurafsky, 2002; Fleischman et al,
2003), classes have been derived from FrameNet
(Baker et al, 1998). For verb argument detec-
tion, classes are either semi-manually derived from
a repository like WordNet, or from NE taggers
(Pardo et al, 2006; Fan et al, 2010). This allows
for domain-independent systems, but limits the ap-
proach to a fixed set of oftentimes rather inappropri-
ate classes. In contrast, we derive the level of gran-
ularity directly from the data.
Pre-tagging the data with NE classes before train-
ing comes at a cost. It lumps entities together which
can have very different classes (i.e., all people be-
come labeled as PERSON), effectively allowing only
one class per entity. Etzioni et al (2005) resolve the
problem with a web-based approach that learns hi-
erarchies of the NE classes in an unsupervised man-
ner. We do not enforce a taxonomy, but include sta-
tistical knowledge about the distribution of possible
classes over each entity by incorporating a prior dis-
tribution P (class, entity). This enables us to gen-
eralize from the lexical form without restricting our-
selves to one class per entity, which helps to bet-
ter fit the data. In addition, we can distinguish sev-
eral classes for each entity, depending on the context
1473
(e.g., winner vs. quarterback). Ritter et al (2010)
also use an unsupervised model to derive selectional
predicates from unlabeled text. They do not assign
classes altogether, but group similar predicates and
arguments into unlabeled clusters using LDA. Brody
(2007) uses a very similar methodology to establish
relations between clauses and sentences, by cluster-
ing simplified propositions.
Pen?as and Hovy (2010) employ syntactic patterns
to derive classes from unlabeled data in the context
of LbR. They consider a wider range of syntactic
structures, but do not include a probabilistic model
to label new data.
5 Conclusion
We use an unsupervised model to infer domain-
specific classes from a corpus of 1.4m unlabeled
sentences, and applied them to learn 250k propo-
sitions about American football. Unlike previous
approaches, we use automatically extracted classes
with a probability distribution over entities to al-
low for context-sensitive selection of appropriate
classes.
We evaluate both the model qualities and sensibil-
ity of the resulting propositions. Several measures
show that the model has good explanatory power and
generalizes well, significantly outperforming two
baseline approaches, especially where the possible
classes of an entity can only be inferred from the
context.
Human subjects on Amazon?s Mechanical Turk
judged up to 96.6% of the propositions for the full
data set, and 67.8% for data containing unseen enti-
ties as sensible. Inter-annotator agreement was rea-
sonably high (agreement = 0.82, G = 0.58, ? =
0.48).
The probabilistic model and the extracted propo-
sitions can be used to enrich texts and support post-
parsing inference for question answering. We are
currently applying our method to other domains.
Acknowledgements
We would like to thank David Chiang, Victoria Fos-
sum, Daniel Marcu, and Stephen Tratz, as well as the
anonymous ACL reviewers for comments and sug-
gestions to improve the paper. Research supported
in part by Air Force Contract FA8750-09-C-0172
under the DARPA Machine Reading Program.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of the 17th international conference on Computa-
tional linguistics-Volume 1, pages 86?90. Association
for Computational Linguistics Morristown, NJ, USA.
Thorsten Brants and Alex Franz, editors. 2006. The
Google Web 1T 5-gram Corpus Version 1.1. Number
LDC2006T13. Linguistic Data Consortium, Philadel-
phia.
Samuel Brody. 2007. Clustering Clauses for High-
Level Relation Detection: An Information-theoretic
Approach. In Annual Meeting-Association for Com-
putational Linguistics, volume 45, page 448.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC 2006. Citeseer.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), 39(1):1?38.
Oren Etzioni, Michael Cafarella, Doug. Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web: An exper-
imental study. Artificial Intelligence, 165(1):91?134.
James Fan, David Ferrucci, David Gondek, and Aditya
Kalyanpur. 2010. Prismatic: Inducing knowledge
from a large scale lexicalized relation resource. In
Proceedings of the NAACL HLT 2010 First Interna-
tional Workshop on Formalisms and Methodology for
Learning by Reading, pages 122?127, Los Angeles,
California, June. Association for Computational Lin-
guistics.
Alvan R. Feinstein and Domenic V. Cicchetti. 1990.
High agreement but low kappa: I. the problems of
two paradoxes. Journal of Clinical Epidemiology,
43(6):543?549.
Michael Fleischman, Namhee Kwon, and Eduard Hovy.
2003. Maximum entropy models for FrameNet classi-
fication. In Proceedings of EMNLP, volume 3.
Danies Gildea and Dan Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Kilem Li Gwet. 2008. Computing inter-rater reliabil-
ity and its variance in the presence of high agreement.
British Journal of Mathematical and Statistical Psy-
chology, 61(1):29?48.
1474
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th conference on Computational linguistics-Volume
2, pages 539?545. Association for Computational Lin-
guistics.
Jasper Wilson Holley and Joy Paul Guilford. 1964. A
Note on the G-Index of Agreement. Educational and
Psychological Measurement, 24(4):749.
Rutu Mulkar-Mehta, James Allen, Jerry Hobbs, Eduard
Hovy, Bernardo Magnini, and Christopher Manning,
editors. 2010. Proceedings of the NAACL HLT
2010 First International Workshop on Formalisms and
Methodology for Learning by Reading. Association
for Computational Linguistics, Los Angeles, Califor-
nia, June.
Thiago Pardo, Daniel Marcu, and Maria Nunes. 2006.
Unsupervised Learning of Verb Argument Structures.
Computational Linguistics and Intelligent Text Pro-
cessing, pages 59?70.
Anselmo Pen?as and Eduard Hovy. 2010. Semantic en-
richment of text with background knowledge. In Pro-
ceedings of the NAACL HLT 2010 First International
Workshop on Formalisms and Methodology for Learn-
ing by Reading, pages 15?23, Los Angeles, California,
June. Association for Computational Linguistics.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich Word Sense Disambiguation rivaling
supervised systems. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1522?1531. Association for Computational
Linguistics.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet alocation method for selectional preferences.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 424?434,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Evan Sandhaus, editor. 2008. The New York Times Anno-
tated Corpus. Number LDC2008T19. Linguistic Data
Consortium, Philadelphia.
Rion Snow, Brendan O?Connor, Dan Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 254?263. Association for Computational Lin-
guistics.
Stephanie Strassel, Dan Adams, Henry Goldberg,
Jonathan Herr, Ron Keesing, Daniel Oblinger, Heather
Simpson, Robert Schrag, and Jonathan Wright. 2010.
The DARPA Machine Reading Program-Encouraging
Linguistic and Reasoning Research with a Series of
Reading Tasks. In Proceedings of LREC 2010.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In Proceedings of the 16th international conference on
World Wide Web, pages 697?706. ACM.
1475
