Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 73?81,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Active Learning for Coreference Resolution
Timothy A. Miller and Dmitriy Dligach and Guergana K. Savova
Children?s Hospital Boston
and Harvard Medical School
300 Longwood Ave.
Enders 141
Boston, MA 02115, USA
{Timothy.Miller,Dmitriy.Dligach,Guergana.Savova}@childrens.harvard.edu
Abstract
Active learning can lower the cost of anno-
tation for some natural language processing
tasks by using a classifier to select informa-
tive instances to send to human annotators. It
has worked well in cases where the training in-
stances are selected one at a time and require
minimal context for annotation. However,
coreference annotations often require some
context and the traditional active learning ap-
proach may not be feasible. In this work we
explore various active learning methods for
coreference resolution that fit more realisti-
cally into coreference annotation workflows.
1 Introduction
Coreference resolution is the task of deciding which
entity mentions in a text refer to the same entity.
Solving this problem is an important part of the
larger task of natural language understanding in gen-
eral. The clinical domain offers specific tasks where
it is easy to see that correctly resolving coreference
is important. For example, one important task in the
clinical domain is template filling for the Clinical El-
ements Model (CEM).1 This task involves extracting
various pieces of information about an entity and fit-
ting the information into a standard data structure
that can be reasoned about. An example CEM tem-
plate is that for Disease with attributes for Body Lo-
cation, Associated Sign or Symptom, Subject, Nega-
tion, Uncertainty, and Severity. Since a given entity
may have many different attributes and relations, it
1http://intermountainhealthcare.org/cem
may be mentioned multiple times in a text. Coref-
erence resolution is important for this task because
it must be known that all the attributes and relations
apply to the same entity so that a single CEM tem-
plate is filled in for an entity, rather than creating a
new template for each mention of the entity.
2 Background
2.1 Coreference Resolution
Space does not permit a thorough review of coref-
erence resolution, but recent publications covered
the history and current state of the art for both the
general domain and the clinical domain (Ng, 2010;
Pradhan et al, 2011; Zheng et al, 2011).
The system used here (Zheng et al, 2012) is
an end-to-end coreference resolution system, mean-
ing that the algorithm receives no gold standard in-
formation about mentions, named entity types, or
any linguistic information. The coreference res-
olution system is a module of the clinical Tex-
tual Analysis and Knowledge Extraction System
(cTAKES) (Savova et al, 2010) that is trained on
clinical data. It takes advantage of named entity
recognition (NER) and categorization to detect en-
tity mentions, and uses several cTAKES modules
as feature generators, including the NER module,
a constituency parser module, and a part of speech
tagging module.
The system architecture is based on the pairwise
discriminative classification approach to the coref-
erence resolution problem. In that paradigm, pairs
of mentions are classified as coreferent or not, and
then some reconciliation must be done on all of the
73
links so that there are no conflicts in the clusters.
The system uses support vector machines (SVMs)
as the pairwise classifiers, and conflicts are avoided
by only allowing an anaphor to link with one an-
tecedent, specifically that antecedent the classifier
links with the highest probability.
There are separate pairwise classifiers for named
entity and pronominal anaphor types. In the domain
of clinical narratives, person mentions and personal
pronouns in particular are not especially challeng-
ing ? the vast majority of person mentions are the
patient. In addition, pronoun mentions, while im-
portant, are relatively rare. Thus we are primarily
interested in named entity coreference classification,
and we use that classifier as the basis of the work de-
scribed here.
The feature set of this system is similar to that
used by Ng and Cardie (2002). That system in-
cludes features based on surface form of the men-
tions, shallow syntactic information, and lexical se-
mantics from WordNet. The system used here has
a similar feature set but uses Unified Medical Lan-
guage System (UMLS)2 semantic features as it is
intended for clinical text, and also incorporates sev-
eral syntactic features extracted from constituency
parses extracted from cTAKES.
To generate training data for active learning simu-
lations, mention detection is run first (cTAKES con-
tains a rule-based NER system) to find named en-
tities and a constituency parser situates entities in
a syntax tree). For each entity found, the system
works backwards through all other mentions within
a ten sentence window. For each candidate anaphor-
antecedent pair, a feature vector is extracted using
the features briefly described above.
2.2 Active Learning
Active Learning (AL) is a popular approach to se-
lecting unlabeled data for annotation (Settles, 2010)
that can potentially lead to drastic reductions in the
amount of annotation that is necessary for train-
ing an accurate statistical classifier. Unlike passive
learning, where the data is sampled for annotation
randomly, AL delegates data selection to the clas-
sifier. AL is an iterative process that operates by
first training a classifier on a small sample of the
2http://www.nlm.nih.gov/research/umls/
data known as the seed examples. The classifier
is subsequently applied to a pool of unlabeled data
with the purpose of selecting additional examples
the classifier views as informative. The selected data
is annotated and the cycle is repeated, allowing the
learner to quickly refine the decision boundary be-
tween classes. One common approach to assessing
the informativeness is uncertainty sampling (Lewis
and Gale, 1994; Schein and Ungar, 2007), in which
the learner requests a label for the instance it is most
uncertain how to label. In this work, we base our
instance selection on the distance to the SVM de-
cision boundary (Tong and Koller, 2002), assuming
that informative instances tend to concentrate near
the boundary.
Most AL work focuses on instance selection
where the unit of selection is one instance repre-
sented as a feature vector. In this paper we also
attempt document selection, where the unit of se-
lection is a document, typically containing multi-
ple coreference pairs each represented as a feature
vector. The most obvious way to extend a sin-
gle instance informativeness metric to the document
scenario is to aggregate the informativeness scores.
Several uncertainty metrics have been proposed that
follow that route to adapt single instance selection
to multiple instance scenarios (Settles et al, 2008;
Tomanek et al, 2009). We borrow some of these
metrics and propose several new ones.
To the best of our knowledge only one work
exists that explores AL for coreference resolution.
Gasperin (2009) experiments with an instance based
approach in which batches of anaphoric pairs are se-
lected on each iteration of AL. In these experiments,
AL did not outperform the passive learning baseline,
probably due to selecting batches of large size.
3 Active Learning Configurations
3.1 Instance Selection
The first active learning model we considered selects
individual training instances ? putatively coreferent
mention pairs. This method is quite easy to simu-
late, and follows naturally from most of the theo-
retical active learning literature, but it has the draw-
back of being seemingly unrealistic as an annotation
paradigm. That is, since coreference can span across
an entire document, it is probably not practical to
74
have a human expert annotate only a single instance
at a time when a given instance may require many
sentences of reading in order to contextualize the in-
stance and properly label it. Moreover, even if such
an annotation scheme proved viable, it may result
in an annotated corpus that is only valuable for one
type of coreference system architecture.
Nonetheless, active learning for coreference at the
instance level is still useful. First, since this method
most closely follows the successful active learning
literature by using the smallest discrete problems, it
can serve as a proof of concept for active learning
in the coreference task ? if it does not work well at
this level, it probably will not work at the document
level. Previous results (Gasperin, 2009) have shown
that certain multiple instance methods do not work
for coreference resolution, so testing on smaller se-
lection sizes first can ensure that active learning is
even viable at that scale. In addition, though in-
stance selection may not be feasible for real world
annotations, individual instances and metrics for se-
lecting them are usually used as building blocks for
more complex methods. In order for this to be pos-
sible it must be shown that the instances themselves
have some value.
3.2 Document Selection
Active learning with document selection is a much
more realistic representation of conventional anno-
tation methods. Conventionally, a set of documents
is selected, and each document is annotated exhaus-
tively for coreference (Pradhan et al, 2011; Savova
et al, 2011). Document selection fits into this work-
flow very naturally, by selecting the next document
to annotate exhaustively based on some metric of
which document has the best instances. In theory,
this method can save annotation time by only anno-
tating the most valuable documents.
Document selection is somewhat similar to the
concept of batch-mode active learning, wherein
multiple instances are selected at once, though
batch-mode learning is usually intended to solve a
different problem, that of an asymmetry between
classifier training speed and annotation speed (Set-
tles, 2010). A more important difference is that doc-
ument selection requires that all of the instances in
the batch must come from the same document. Thus,
one might expect a priori that document selection
for active learning will not perform as well as in-
stance selection. However, it is possible that even
smaller gains will be valuable for improving annota-
tion time, and the more robust nature of a corpus an-
notated in such a way will make the long term bene-
fits worthwhile.
In this work, we propose several metrics for se-
lecting documents to annotate, all of which are
based on instance level uncertainty. In the fol-
lowing descriptions, D is the set of documents, d
is a single document, d? is the selected document,
Instances(d) is a function which returns the set of
pair instances in document d, i is an instance, dist(i)
is a function which returns the distance of instance i
from the classification boundary, and I is the indica-
tor function, which takes the value 1 if its argument
is true and 0 otherwise. Note that high uncertainty
occurs when Abs(dist(i)) approaches 0.
? Best instance ? This method uses the un-
certainty sampling criteria on instances, and
selects the document containing the in-
stance the classifier is least certain about.
d? = argmin
d?D
[mini?Instances(d)Abs(dist(i))]
? Highest average uncertainty ? This method
computes the average uncertainty of all
instances in a document, and selects the
document with the highest average uncertainty.
d? = argmin
d?D
1
|Instances(d)|
?
i?Instances(d)Abs(dist(i))
? Least bad example ? This method uses
uncertainty sampling criteria to find the
document whose most certain example is
least certain, in other words the document
whose most useless example is least useless.
d? = argmin
d?D
maxi?Instances(d)Abs(dist(i))
? Narrow band ? This method creates an un-
certainty band around the discriminating
boundary and selects the document with
the most examples inside that narrow band.
d? = argmax
d?D
?
i?Instances(d) I(Abs(dist(i) < 0.2))
? Smallest spread ? This method computes the
distance between the least certain and most
certain instances and selects the document
minimizing that distance.
75
d? = argmin
d?D
[maxi?Instances(d)(Abs(dist(i)))?
mini?Instances(d)(Abs(dist(i)))]
? Most positives ? This method totals the
number of positive predicted instances
in each document and selects the doc-
ument with the most positive instances.
d? = argmax
d?D
?
i?Instances(d) I(dist(i) > 0)
? Positive ratio ? This method calculates
the percentage of positive predicted in-
stances in each document and selects the
document with the highest percentage.
d? = argmax
d?D
?
i?Instances(d) I(dist(i)>0)
|Instances(d)|
Many of these are straightforward adaptations of
the instance uncertainty criteria, but others deserve
a bit more explanation. The most positives and pos-
itive ratio metrics are based on the observation that
the corpus is somewhat imbalanced ? for every posi-
tive instance there are roughly 20 negative instances.
These metrics try to account for the possibility that
instance selection focuses on positive instances. The
average uncertainty is an obvious attempt to turn in-
stance metrics into document metrics, but narrow
band and smallest spread metrics attempt to do the
same thing while accounting for skew in the distri-
bution of ?good? and ?bad? instances.
3.3 Document-Inertial Instance Selection
One of the biggest impracticalities of instance se-
lection is that labeling any given instance may re-
quire reading a fair amount of the document, since
the antecedent and anaphor can be quite far apart.
Thus, any time savings accumulated by only anno-
tating an instance is reduced since the reading time
per instance is probably increased.
It is also possible that document selection goes
too far in the other direction, and requires too
many useless instances to be annotated to achieve
gains. Therefore, we propose a hybrid method of
document-inertial instance selection which attempts
to combine aspects of instance selection and docu-
ment selection.
This method uses instance selection criteria to se-
lect new instances, but will look inside the current
document for a new instance within an uncertainty
threshold rather than selecting the most uncertain in-
stance in the entire training set. Sticking with the
same document for several instances in a row can
potentially solve the real world annotation problem
that marking up each instance requires some knowl-
edge of the document context. Instead, the context
learned by selecting one instance can be retained if
useful for annotating the next selected instance from
the same document.
This also preserves one of the biggest advantages
of instance selection, that of re-training the model
after every selected instance. In batch-mode selec-
tion and document selection, many instances are se-
lected according to criteria based on the same model
starting point. As a result, the selected instances
may be redundant and document scores based on
accumulated instance scores may not reflect reality.
Re-training the model between selected instances
prevents redundant instances from being selected.
4 Evaluation
Evaluations of the active learning models described
above took place in a simulation context. In active
learning simulations, a labeled data set is used, and
the unlabeled pool is simulated by ignoring or ?cov-
ering? the labels for part of the data until the selec-
tion algorithm selects a new instance for annotation.
After selection the next data point is simply put into
the training data and its label is uncovered.
The data set used was the Ontology Development
and Information Extraction (ODIE) corpus (Savova
et al, 2011) used in the 2011 i2b2/VA Challenge on
coreference resolution.3 We used a set of 64 docu-
ments from the training set of the Mayo Clinic notes
for our simulations.
Instances were created by using the training
pipeline from the coreference system described in
Section 2.1. As previously mentioned, this work
uses the named entity anaphor classifier as it con-
tains the most data points. This training set resulted
in 6820 instances, with 311 positive instances and
6509 negative instances. Baseline ten-fold cross val-
idation performance on this data set using an SVM
with RBF kernel is an F-score of 0.48.
Simulations are performed using ten fold cross-
validation. First, each data point is assigned to one
3https://www.i2b2.org/NLP/Coreference/
76
of ten folds (this is done randomly to avoid any auto-
correlation issues). Then, for each iteration, one fold
is made the seed data, another fold is the validation
data, and the remainder are the unlabeled pool. Ini-
tially the labeled training data contains only the seed
data set. The model is trained on the labeled train-
ing data, tested on the validation set, then used to
select the next data point from the pool data set. The
selected data point is then removed from the pool
and added to the training data with its gold stan-
dard label(s), and the process repeats until the pool
of unlabeled data is empty. Performance is averaged
across folds to minimize the effects of randomness
in seed and validation set selection. Typically, active
learning is compared to a baseline of passive learn-
ing where the next data point to be labeled is selected
from the unlabeled pool data set randomly.
4.1 Instance Selection Experiments
Instance selection simulations follow the general
template above, with each instance (representing
a putative antecedent-anaphor pair) randomly as-
signed to a fold. After scoring on the validation set,
uncertainty sampling is used to select a single in-
stance from the unlabeled pool, and that instance is
added to the training set.
Figure 1 shows the results of active learning using
uncertainty selection on instances versus using pas-
sive learning (random selection). This makes it clear
that if the classifier is allowed to choose the data, top
performance can be achieved much faster than if the
data is presented in random order. Specifically, the
performance for uncertainty selection levels off at
around 500 instances into the active learning, out of
a pool set of around 5500 instances. In contrast, the
passive learning baseline takes basically the entire
dataset to reach the same performance.
This is essentially a proof of concept that there is
such a thing as a ?better? or ?worse? instance when
it comes to training a classifier for coreference. We
take this as a validation for attempting a document
selection experiment, with many metrics using in-
stance uncertainty as a building block.
4.2 Document Selection Experiments
Document selection follows similarly to the instance
selection above. The main difference is that instead
of assigning pair vectors to folds, we assign docu-
0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Active vs. Passive Learning on Pairwise Named Entity Coreference
Number of instances
F?
sc
or
e
 
 
Random (Passive)
Uncertainty Sampling
Figure 1: Instance selection simulation results. The x-
axis is number of instances and the y-axis is ten-fold av-
eraged f-score of the pairwise named entity classifier.
ments to folds. To make a selection, each instance is
labeled according to the model, document level met-
rics described in Section 3.2 are computed per docu-
ment, and the document is selected which optimizes
the metric being evaluated. All of that document?s
instances and labels are added to the training data,
and the process repeats as before.
The results of these experiments are divided into
two plots for visual clarity. Figure 2 shows the
results of these experiments, roughly divided into
those that work as well as a random baseline (left)
and those that seem to work worse than a random
baseline (right). The best performing metrics (on
the left side of the figure) are Positive Ratio, Least
Worst,Highest Average, and Narrow Band, although
none of these performs noticeably better than ran-
dom. The remaining metrics (on the right) seem
to do worse than random, taking more instances to
reach the peak performance near the end.
The performance of document selection suggests
that it may not be a viable means of active learn-
ing. This may be due to a model of data distribution
in which useful instances are distributed very uni-
formly throughout the corpus. In this case, an aver-
age document will only have 8?10 useful instances
and many times as many that are not useful.
This was investigated by follow-up experiments
on the instance selection which kept track of which
77
0 1000 2000 3000 4000 5000 6000 7000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Number of instances
F?
sc
or
e
Document?level active learning
 
 
Passive
Least worst
Highest average
Pos/neg ratio
Narrow Band
0 1000 2000 3000 4000 5000 6000 7000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Number of instances
F?
sc
or
e
Document?level active learning
 
 
Passive
Best example
Most positives
Smallest spread
Figure 2: Two sets of document selection experiments.
document each instance came from. The experi-
ments tracked the first 500 instances only, which is
roughly the number of instances shown in Figure 1
to reach peak performance. Figure 3 (left) shows
a histogram with document indices on the x-axis
and normalized instance counts on the y-axis. The
counts are normalized by total number of document
vectors. In other words, we wanted to show whether
there was a distinction between ?good? documents
containing lots of good instances and ?bad? docu-
ments with few good instances.
The figure shows a few spikes, but most docu-
ments have approximately 10% of their instances
sampled, and all but one document has at least one
instance selected. Further investigation shows that
the spikes in the figure are from shorter documents.
Since shorter documents have few instances overall
but always at least one positive instance, they will be
biased to have a higher ratio of positive to negative
instances. If positive instances are more uncertain
(which may be the case due to the class imbalance),
then shorter documents will have more selected in-
stances per unit length.
We performed another follow-up experiment
along these lines using the histogram as a measure
of document value. In this experiment, we took the
normalized histogram, selected documents from it in
order of normalized number of items selected, and
used that as a document selection technique. Ob-
viously this would be ?cheating? if used as a metric
for document selection, but it can serve as a check on
the viability of document selection. If the results are
better than passive document selection, then there is
some hope that a document level metric based on the
uncertainty of its instances can be successful.
In fact, the right plot on Figure 3 shows that the
?cheating? method of document selection still does
not look any better than random document selection.
4.3 Document-Inertial Instance Selection
Experiments
The experiments for document-inertial instance se-
lection were patterned after the instance selection
paradigm. However, each instance was bundled with
metadata representing the document from which it
came. In the first selection, the algorithm selects the
most uncertain instance, and the document it comes
from is recorded. For subsequent selections, the
document which contained the previously selected
instance is given priority when looking for a new
instance. Specifically, each instance in that docu-
ment is classified, and the confidence is compared
against a threshold. If the document contains in-
stances meeting the threshold, the most uncertain in-
stance was selected. After each instance, the model
is retrained as in normal instance selection, and the
new model is used in the next iteration of the selec-
tion algorithm. For these experiments, the threshold
is set at 0.75, where the distance between the classi-
fication boundary and the margin is 1.0.
Figure 4 shows the performance of this algorithm
compared to passive and uncertainty sampling. Per-
78
0 10 20 30 40 50 60
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Normalized document selection counts
Document index
%
 o
f v
ec
to
rs
 s
el
ec
te
d
0 1000 2000 3000 4000 5000 6000 7000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Number of instances
F?
sc
or
e
Document?level active learning
 
 
Passive
Cheating
Figure 3: Left: Percentage of instances selected from each document. Right: Performance of a document selection
algorithm that can ?cheat? and select the document with the highest proportion of good instances.
0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Active vs. Passive Learning on Pairwise Named Entity Coreference
Number of instances
F?
sc
or
e
 
 
Random (Passive)
Uncertainty Sampling
Sticky Instance Sampling
Figure 4: Document-inertial instance selection results.
formance using this algorithm is clearly better than
passive learning and is similar to standard uncer-
tainty selection ignoring document constraints.
5 Discussion and Conclusion
The results of these experiments paint a complex
picture of the way active learning works for this do-
main and model combination. The first experiments
with uncertainty selection indicate that the number
of instances required to achieve classifier perfor-
mance can be compressed. Selecting and training
on all the good instances first leads to much faster
convergence to the asymptotic performance of the
classifier given the features and data set.
Attempting to extend this result to document se-
lection met with mediocre results. Even the best per-
forming of seven attempted algorithms seems to be
about the same as random document selection. One
can interpret these results in different ways.
The most pessimistic interpretation is that docu-
ment selection simply requires too many useless in-
stances to be annotated, good instances are spread
too evenly, and so document selection will never be
meaningfully faster than random selection. This in-
terpretation seems to be supported by experiments
showing that even if document selection uses a
?cheating? algorithm to select the documents with
the highest proportion of good instances it still does
not beat a passive baseline.
One can also interpret these results to inspire fur-
ther work, first by noting that all of the selection
techniques attempt to build on the instance selec-
tion metrics. While our document selection metrics
were more sophisticated than simply taking the n-
best instances, Settles (2010) notes that some suc-
cessful batch mode techniques explicitly account for
diversity in the selections, which we do not. In ad-
dition, one could argue that our experiments were
unduly constrained by the small number of docu-
ments available in the unlabeled pool, and that with
a larger unlabeled pool, one would eventually en-
counter documents with many good instances. This
may be true, but may be difficult in practice as clin-
ical notes often need to be manually de-identified
79
before any research use, and so it is not simply a
matter of querying all records in an entire electronic
medical record system.
The document-inertial instance selection showed
that the increase in training speed can be main-
tained without switching documents for every in-
stance. This suggests that while good training in-
stances may be uniformly distributed, it is usually
possible to find multiple good enough instances in
the current document, and they can be found despite
not selecting instances in the exact best order that
plain instance selection would suggest.
Future work is mainly concerned with real world
applicability. Document level active learning can
probably be ruled out as being non-beneficial despite
being the easiest to work into annotation work flows.
Instance level selection is very efficient in achieving
classifier performance but the least practical.
Document-inertial seems to provide some com-
promise. It does not completely solve the prob-
lems of instance selection, however, as annotation
will still not be complete if done exactly as simu-
lated here. In addition, the assumption of savings
is based on a model that each instance takes a con-
stant amount of time to annotate. This assumption is
probably true for tasks like word sense disambigua-
tion, where an annotator can be presented one in-
stance at a time with little context. However, a better
model of annotation for tasks like coreference is that
there is a constant amount of time required for read-
ing and understanding the context of a document,
then a constant amount of time on top of that per
instance.While modeling annotation time may pro-
vide some insight, it will probably be most effective
to undertake empirical annotation experiments to in-
vestigate whether document-inertial instance selec-
tion actually provides a valuable time savings.
The final discussion point is that of producing
complete document annotations. For coreference
systems following the pairwise discriminative ap-
proach as in that described in Section 2.1, a corpus
annotated instance by instance is useful. However,
many recent approaches do some form of document-
level clustering or explicit coreference chain build-
ing, and are not natively able to handle incompletely
annotated documents.4
4Other recent unsupervised graphical model approaches us-
Future work will investigate this issue by quan-
tifying the value of complete gold standard annota-
tions versus the partial annotations that may be pro-
duced using document-inertial instance selection.
One way of doing this is in simulation, by training
a model on the 500 good instances that document-
inertial instance selection selects, and then classify-
ing the rest of the training instances using that model
to create a ?diluted? gold standard. Then, a model
trained on the diluted gold standard will be used
to classify the validation set and performance com-
pared to the version trained on the full gold standard
corpus. Similar experiments can be performed using
other systems. The logic here is that if an instance
was not in the top 10% of difficult instances it can be
classified with high certainty. The fact that positive
instances are rare and tend to be most uncertain is a
point in favor of this approach ? after all, high accu-
racy can be obtained by guessing in favor of negative
once the positive instances are labeled. On the other
hand, if document-inertial instance selection simply
amounts to labeling of positive instances, it may not
result in substantial time savings.
In conclusion, this work has shown that instance
selection works for coreference resolution, intro-
duced several metrics for document selection, and
proposed a hybrid selection approach that preserves
the benefits of instance selection while offering the
potential of being applicable to real annotation. This
work can benefit the natural language processing
community by providing practical methods for in-
creasing the speed of coreference annotation.
Acknowledgments
The project described was supported by award
number NLM RC1LM010608, the Strategic Health
IT Advanced Research Projects (SHARP) Program
(90TR002) administered by the Office of the Na-
tional Coordinator for Health Information Technol-
ogy, and Integrating Informatics and Biology to the
Bedside (i2b2) NCBO U54LM008748. The content
is solely the responsibility of the authors and does
not necessarily represent the official views of the
NLM/NIH/ONC.
ing Gibbs sampling (Haghighi and Klein, 2007) may be able to
incorporate partially annotated documents in semi-supervised
training.
80
References
Caroline Gasperin. 2009. Active learning for anaphora
resolution. In Proceedings of the NAACL HLT Work-
shop on Active Learning for Natural Language Pro-
cessing, pages 1?8.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, pages
848?855.
David D. Lewis andWilliam A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of the ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, pages 3?12.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL).
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics (ACL-10).
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 shared task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the 15th Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?27.
Guergana K. Savova, James J. Masanz, Philip V. Ogren,
Jiaping Zheng, Sunghwan Sohn, Karin C. Kipper-
Schuler, and Christopher G. Chute. 2010. Mayo
clinical text analysis and knowledge extraction sys-
tem (cTAKES): architecture, component evaluation
and applications. J Am Med Inform Assoc, 17(5):507?
513.
Guergana K. Savova, Wendy W. Chapman, Jiaping
Zheng, and Rebecca S. Crowley. 2011. Anaphoric
relations in the clinical narrative: corpus creation. J
Am Med Inform Assoc, 18:459?465.
A.I. Schein and L.H. Ungar. 2007. Active learning for
logistic regression: an evaluation. Machine Learning,
68(3):235?265.
B. Settles, M. Craven, and S. Ray. 2008. Multiple-
instance active learning. Advances in Neural Informa-
tion Processing Systems (NIPS), 20:1289?1296.
Burr Settles. 2010. Active learning literature survey.
Technical report, University of Wisconsin?Madison.
Katrin Tomanek, Florian Laws, Udo Hahn, and Hinrich
Schu?tze. 2009. On proper unit selection in active
learning: co-selection effects for named entity recog-
nition. In HLT ?09: Proceedings of the NAACL HLT
2009 Workshop on Active Learning for Natural Lan-
guage Processing, pages 9?17, Morristown, NJ, USA.
Association for Computational Linguistics.
S. Tong and D. Koller. 2002. Support vector machine
active learning with applications to text classification.
The Journal of Machine Learning Research, 2:45?66.
Jiaping Zheng, Wendy Webber Chapman, Rebecca S.
Crowley, and Guergana K. Savova. 2011. Coreference
resolution: A review of general methodologies and ap-
plications in the clinical domain. Journal of Biomedi-
cal Informatics, 44:1113?1122.
Jiaping Zheng, Wendy W Chapman, Timothy A Miller,
Chen Lin, Rebecca S Crowley, and Guergana K
Savova. 2012. A system for coreference resolution for
the clinical narrative. Journal of the American Medi-
cal Informatics Association.
81
