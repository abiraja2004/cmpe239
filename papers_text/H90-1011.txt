Performing Integrated Syntactic and Semantic Parsing 
Using Classification 
Robert T. Kasper and Eduard H. Hovy 
Information Sciences Institute of USC 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
Introduction 
This paper describes a particular approach to parsing 
that utilizes recent advances in unification-based parsing 
and in classification-based knowledge representation. As 
unification-based grammatical frameworks are extended to 
handle richer descriptions of linguistic information, they be- 
gin to share many of the properties that have been developed 
in KL-ONE-Iike knowledge representation systems. This 
commonality suggests that some of the classification-based 
representation techniques can be applied to unification-based 
linguistic descriptions. This merging supports the integration 
of semantic and syntactic information i to the same system, 
simultaneously subject o the same types of processes, in an 
efficient manner. The result is expected to be more efficient 
parsing due to the increased organization of knowledge. 
The use of a KL-ONE style representation for parsing and 
semantic interpretation was first explored in the PSI-KLONE 
system \[2\], in which parsing is characterized asan inference 
process called incremental description refinement. The key 
idea underlying this process is that a description of an object 
can become increasingly more specific as additional features 
are learned from multiple knowledge sources, which is es- 
sentially the same idea that underlies most unification-based 
approaches. Bobrow and Webber identified four crucial ca- 
pabilities that a representational system should have in order 
to support he process of incremental description refinement. 
These capabilities, not all available to Bobrow and Webber 
i n  1980, have recently been developed in the Loom knowl- 
edge representation system \[ 12\] and hence nable the practical 
development of the new parsing method. They are: 
1. What properties of a structured object provide sufficient 
information toguarantee the applicability of a description 
to (some portion of) that object- -  i.e., criteriality condi- 
tions. Loom provides a separation of definition (neces- 
sary and sufficient conditions) and constraints (implied 
features). 
2. What mappings are possible between classes of relations 
- -  e.g., how functional relationships between syntac- 
tic constituents map onto semantic relationships. This 
is not part of Loom, but can be captured in the inter- 
relationships between a syntax-oriented grammar and a 
semantics-oriented concept taxonomy. 
3. Which pairs of descriptions are mutually incompatible 
- -  i.e., which cannot both apply to a single individual. 
Loom provides more complete inference of disjointness 
than previous ystems in the KL-ONE family. 
4. Which sub-categorizations of descriptions are exhaus- 
tive - -  i.e., at least one of the subcategories applies to 
anything to which the more general description applies. 
Loom provides inference with respect to coverings, im- 
plemented by disjunctive descriptions. 
This paper outlines how a parser can be built using Loom's 
classifier as the primary inference operation. It first describes 
the process of unification, then points out similarities between 
unification and classification, then describes the process of 
parsing using the classifier with an example. The integration 
of semantic and syntactic information i to the same system is 
discussed. Finally, the efficiency benefits of the new method 
are mentioned. 
Constraints in Unification-based Gram- 
mars 
A variety of current approaches to parsing in computational 
linguistics emphasize declarative r presentations of grammar 
with logical constraints stated in terms of feature and cate- 
gory structures. These approaches have collectively become 
known as the unification-based grammars, because unification 
is commonly used as the primary operation for building and 
combining feature structures. Some of the simplest of these 
grammatical frameworks, as exemplified by the PATR-II sys- 
tem \[16\], state constraints on features entirely in terms of sets 
of unifications that must be simultaneously satisfied whenever 
a grammatical rule is used. In such systems all constraints on 
a rule or lexical item are interpreted conjunctively. Many 
of the more recent frameworks also use other general logical 
connectives, such as disjunction, egation and implication, in 
their representation f constraints. The utility of such logical 
constraints i abundantly illustrated by linguistic models, in- 
cluding Systemic Grammar (SG) \[5\] and Head Driven Phrase 
Structure Grammar (HPSG) \[14\], and by computational tools 
such as Functional Unification Grammar (FUG) \[11\]. For 
example, SG and FUG even use disjunctive alternations of 
features, instead of structural rules, as the primary units of 
grammatical organization. While the intuitive interpretation 
of these logical constraints i  rather straightforward, and they 
54 
are quite natural for linguists to formulate, large-scale imple- 
mentations of them have typically involved finding a balance 
between expressive power and computational efficiency, not 
an easy task. 
Some difficulties can be expected in developing a system 
for computing with disjunctive and negative feature con- 
straints, because it has been established that common op- 
erations on such descriptions, uch as unification and sub- 
sumption, are NP-complete and require exponential time in 
the worst case \[15\]. The most common and obvious way to 
deal with disjunctive constraints i to expand the grammat- 
ical description to disjunctive normal form (DNF) during a 
pre-processing step, thereby eliminating disjunction from the 
rules that are actually used by the parser. Though this method 
works reasonably well for small grammars, it turns out to be 
unsatisfactory for larger grammars. 
It is possible to avoid exponential expansion for most prac- 
tical grammars, and several unification algorithms for disjunc- 
tive feature descriptions have been developed inrecent years: 
\[6, 10, 4\]. The latter two algorithms allow general disjunc- 
tive descriptions, and avoid expansion to DNF by exploiting 
logical equivalences between descriptions to produce normal 
forms that allow a more compact representation. Kasper's 
algorithm is based on a normal form that divides each de- 
scription into definite and indefinite components. The definite 
component contains no disjunction, and the indefinite compo- 
nent contains a list of disjunctions that must be satisfied. The 
algorithm of Eisele and Doerre uses a different normal form 
that guarantees the detection of any inconsistencies during the 
normalization process by selectively expanding disjunctions 
that might possibly interact with other information i  the de- 
scription. The Kasper algorithm was first implemented asan 
extension to the unification algorithm of the PATR-II parser, 
and it has been further developed to handle conditional de- 
scriptions and a limited type of negation \[8\]. These extensions 
to PATR-II have been used to construct an experimental parser 
for systemic grammars \[9\], which has been tested with a large 
grammar of English called Nigel, which is part of the Penman 
language generation system \[13\]. 
Although these methods for processing complex feature 
constraints are generally much more efficient han expansion 
to DNF, they still have several significant sources of ineffi- 
ciency: 
1. a large amount of structure must be copied in order to 
guarantee correct unification; 
2. consistency checks are required between components of
a description that do not share any features in common, 
because unification cannot determine whether any de- 
pendencies xist between two structures without actually 
unifying them; 
3. repeated computations are often required over sub- 
expressions of descriptions, because the results of prior 
unifications (and compatibility ests) are not saved. 
These sources of inefficiency are not unique to one method 
of parsing with disjunctive descriptions; imilar shortcom- 
ings are commonly reported for most unification-based sys- 
tems. The unification literature contains everal techniques 
for reducing the amount of copying by structure sharing, but 
these techniques appear to solve only part of the problem. A 
more general approach to improving the efficiency of unifi- 
cation may be available by adopting methods that are used in 
classification-based ystems. 
Classification-based Knowledge 
Representation 
Unfortunately, as a grammar (or knowledge base) grows in 
size and complexity, it becomes increasingly less efficient 
to simply unify partial descriptions of constituents with a 
description from the grammar (as in most unification-based 
frameworks). Instead, it becomes preferable to classify each 
partial description of a constituent with respect to the objects 
that are defined by the grammar, exploiting known relation- 
ships between components of the grammatical description. 
Since the components of the grammar are known before pars- 
ing commences, various relationships, uch as subsumption 
and compatibility, can be used to construct a lattice of gram- 
matical objects, eliminating the need to derive those relation- 
ships repeatedly atparse time. 
The KL-ONE family of knowledge representation sys- 
tems is based on an explicit logical formalization of many 
of the constructs that have been explored in semantic net- 
works and frame-based representation systems. They or- 
ganize information about objects and the relations between 
them into hierarchies according to specificity, with more spe- 
cific objects placed below more general ones. For example, 
a hierarchy of English word classes would probably contain 
Verbs, Transitive-Verbs a a subclass of Verbs, and the word 
"like" as an instance of Transitive-Verbs. Each hierarchy is 
a subsumption-ordered lattice based upon logical properties 
that can be deduced from the definitions of objects and the 
facts known about them. In these systems, classification is
the operation that places a new class or object into the lattice 
according to the subsumption order. A primary benefit of 
classification is that it organizes large collections of knowl- 
edge in such a way that properties shared by many objects 
need only be represented once, yet they can still be efficiently 
accessed by inheritance. 
KL-ONE and similar frameworks have been used for se- 
mantic interpretation in some natural language processing sys- 
tems \[18\], but usually in a way that is quite separate from the 
grammatical parsing process (an exception is the aforemen- 
tioned PSI-KLONE system). Recent research indicates that 
it may be advantageous to make use of a classification-based 
framework for processing rammatical knowledge as well. 
Many formal properties are shared by the feature descrip- 
tions used in unification-based grammars and the termino- 
logical definitions used in KL-ONE. Generally speaking, lin- 
guistic categories correspond to concepts, and their features 
(or attributes) correspond to binary relations in the knowl- 
edge representation system. The similarity between these 
two types of descriptions has been most clearly documented 
by Smotka \[17\] in his development of a logic that integrates a 
significant combination oftheir expressive capabilities. Many 
theoretical results have also been based on the observation that 
55 
feature structures can be implicitly organized into a subsump- 
tion lattice of types according to their information content. 
In most unification-based systems the lattice is not explicitly 
constructed, but a classification-based system can be used to 
place the feature structures of a grammar and lexicon into 
a structure-sharing lattice, potentially improving both space 
and time efficiency. 
Despite the underlying similarities between the KL-ONE 
framework and unification-based grammars, there are signifi- 
cant differences in the expressive capabilities that are usually 
provided. In particular, the knowledge representation systems 
typically have general constraints on relations with multiple 
values, whereas most unification-based systems do not pro- 
vide a direct representation for features with set values. On 
the other hand, complex logical constraints involving dis- 
junction and negation have been more extensively developed 
in unification-based systems than in classification-based sys- 
tems. The Loom system \[12\], which has been developed at 
USC/ISI, appears to be the first in the KL-ONE family to 
have included general disjunction and negation in its concept 
definition language. The implementation f classification for 
disjunctive concepts has been based on several refinements of
a strategy that was originally developed for unification with 
disjunctive feature descriptions \[10\]. The implementation 
of classification for concepts defined by negation is still in 
progress. With these extensions, the Loom system is able to 
handle a much fuller range of constraints hat have been used 
in actual linguistic descriptions of feature structures. 
An Experiment in Classification-based 
Parsing 
In order to explore a strategy for parsing based on classifica- 
tion, we have to represent Penman's grammar in Loom and 
replace the existing unification component of our parser \[9\] 
with activations of Loom's classifier. Motivating this action 
are two primary goals: 
1. to investigate the extent o which classification can be 
used to organize the knowledge contained in linguistic 
descriptions so that it can be more efficiently accessed 
during the parsing process; 
2. to develop a suitable architecture for integrating seman- 
tic information into the parsing process, in a way that 
knowledge specific to application domains does not have 
to be re-organized for parsing. 
It is straightforward to convert he feature constraints of 
the grammar into a set of definitions that can be processed by 
Loom, because of the underlying correspondences between 
Loom's concept definitions and linguistic feature descriptions 
that we have already described. It is also straightforward 
to perform an operation that is equivalent to the unification 
of feature structures within Loom. This is accomplished by 
forming an object having a type that is defined as the conjunc- 
tion of the types corresponding to the feature structures. Con- 
junction of types yields a type which captures the unification 
of descriptions in a non-destructive way. Loom also supports 
merging of instances, corresponding to destructive unifica- 
tion, which is necessary inorder to satisfy feature quivalence 
constraints. When two instances are merged, the resulting in- 
stance has a type which is a conjunction of the types of the 
two original instances. 
A disjunction is represented in Loom by an object hat gen- 
eralizes (i.e., classifies above) each of the disjuncts. It is 
important to note that a disjunctive description entails more 
than a simple generalization. It is possible to satisfy a gener- 
alization without satisfying any of the disjuncts, but in order 
to satisfy a disjunction, an object must satisfy one of the dis- 
juncts. 
Instead of unifying a partial description of a constituent 
with a grammatical description, we classify the description of 
the constituent with respect to an object-oriented representa- 
tion of the grammar, in which each object stores information 
and constraints associated with a particular type of grammati- 
cal constituent. The classifier determines which grammatical 
classes the constituent instantiates, and the constraints asso- 
ciated with these classes can be used to give a more complete 
(grammatical, semantic, pragmatic) description of the con- 
stituent. 
Classification provides a way of decomposing a large de- 
' scription into types, and organizing these types into a lattice 
so that they can be efficiently searched. Thus, it can provide 
a more efficient mechanism for unifying large descriptions. 
Two immediate benefits of the lattice representation are: 
1. the descriptions ofthe grammar do not need to be copied 
each time that they are unified with a constituent, and 
2. each constituent does not need to have an explicit repre- 
sentation of a complete set of its grammatical features, 
because many of these are entailed by its type. 
Classification has the effect of abstracting frequently used 
combinations of features into a type hierarchy. By factoring 
descriptions of types of objects out of the feature structures 
that represent constituents, it is often possible to reduce the 
numbers of features that need to be unified (recursively) when 
the constituent is used as a role-filler (in multiple parses), 
because the type encapsulates restrictions on those features. 
A Simple Example 
In an example, consider how classification with respect to a 
simple grammar may be used in parsing the sentence: David 
likes computers. Assume that a lexical/morphological ana- 
lyzer gives the following type membership information for 
each word: 
David: Noun. 
computers: Noun. 
likes: Verb Trans i t ive Present. 
Also assume that a rather simple context-free grammar can 
be used to recognize possible constituents, and that it can be 
annotated to assign grammatical functions 1. In the example 
1 Using the classification-based approach outlined here, itis theoretically 
possible to perform the parsing completely using only classification. How- 
ever, such a parser would have to examine all substrings ofthe input in order 
56 
sentence, this grammar proposes a constituent c with the type 
Cl au s e and the following grammatical functions: 
sub jec t  : dav id  
process  : l i kes  
dob jec t  : computers  
This initial description of the constituent, c, is then given 
to the classifier, which deduces the most specific types that it 
belongs to. The classifier begins by considering types that are 
directly below the initial type, i.e., Int rans -C lause  and 
Trans -C lause .  The definition of I n t rans -C lause  
states that it is a Clause  with a process  of type 
In t rans i t i ve .  This definition is not satisfied by c, be- 
cause it does not have a process of type In t rans i t i ve .  
Next, the classifier considers T rans -C lause ,  which has a 
definition stating that it is a C lause  with a process  of 
type Trans i t i ve .  This definition is satisfied by c. In ad- 
dition, T rans -C lause  has a constraint: it implies the type 
Act ive  OR P as  s ive ,  which means thatany object which 
is a member of Trans -C lause  must also be a member of 
Act ive  OR Pass ive  (that is, Ac t ive  and Pass ive  
form a disjoint covering of Trans-Clause) .  Therefore, 
Ac t ive  OR Pass ive  is added to the list of types that c 
belongs to. 
BecauseAct ive OR Passiveisadisjunction,itispos- 
sible to infer membership in one of the disjuncts by prov- 
ing incompatibility with all other disjuncts, c is compatible 
with all of the constraints of Ac t ive ,  but it is not compat- 
ible with the constraints of Pass ive :  it has a process  
of type Present ,  Pass ive  requires a process  of type 
PastPart ,  and the types Present  and PastPar t  spe- 
ciaiize the disjoint types, F in i te  and Nonf in i te .  By  
eliminating the P a s s ive  disjunct from consideration, mem- 
bership in the Act ive  disjunct can be inferred. Ac t ive  is 
the most specific type that can be inferred for c, because it 
specializes all other types that c belongs to (and there are no 
more specific types defined in this simple example). 
As a consequence of acquiring membership in the type 
Act ive ,  c inherits all constraints that are associated with 
Act ive .  These constraints require that the ac tor  and 
sub  j ec t  roles are identical (i.e., that the values of these two 
roles should be unified), and that the goa l  and dob jec t  
roles are identical. Satisfying these constraints yields the 
following information about he roles of c: 
actor  : dav id  
goa l  : computers  
Thus, given the initial assumption that c is a clause 
with particular constituents filling the grammatical functions 
proces  s, sub jec t  and dob  ject, classification deduces: 
1. a more specific type: that c is an active clause; 
to find all possible constituents, unless ufficient constraints onconstituent 
ordering can be applied early enough inthe parsing process. By performing 
a sha/low structural parse before starting the deep classification-based parse, 
one gains a large improvement in efficiency, because even askeletal context- 
free grammar can provide the basic segmentation of the input sentence into 
its major constituents. Thus, a simple context-free parsing component was 
used for this purpose with success inthe prototype system. 
2. values for previously unspecified roles: actor  and 
goal. 
The classifier uses the lattice representation f defined types 
to guide its search for types that are satisfied by a given object. 
It does not need to consider any types that fall below a type 
that the object is known not to specialize, such as all types 
below In t rans -C lause  and Pass ive  for the object c. 
The power of using this kind of classification scheme may 
be further exploited by associating semantic and pragmatic 
constraints with each grammatical type, in addition to the 
grammatical constraints which have been illustrated. 
Integrating Semantic Information into 
the Parsing Process 
One of the greatest advantages of this method of parsing is 
the possibility of performing integrated semantic and syntactic 
processing. KL-ONE systems uch as Loom were tradition- 
ally developed to represent semantic information, and with 
the inclusion of syntactic information as required for the work 
described here, both types of knowledge reside in the same 
system and are accessible to a single classification process. 
The advantages for processing are clear. By being able op- 
portunistically toaccess both semantic and syntactic knowl- 
edge at any point during the process, the parser can resolve 
ambiguities sooner than in the traditional pipeline model, in 
which syntactic parsing is completed before semantic parsing 
commences. Many of the structural ambiguities that arise 
during parsing are only resolvable by semantic knowledge, 
and pipeline parsers have to maintain all the syntactic possi- 
bilities until the semantic parsing phase. Non-pipelineparsers 
have to perform acomplex interweaving of semantic and syn- 
tactic processing, requiring increased bookkeeping and more 
complex system architecture. In the method outlined in this 
paper, the parser's ingle call to the classifier will result in the 
most appropriate information - -  both semantic and syntactic 
- -  being found and being reconciled, if possible by the normal 
action of the classifier. 
Another benefit is the increased portability provided by a 
knowledge representation paradigm used in the Penman sys- 
tem. In order to achieve greater portability, Penman contains 
a general taxonomic ontology of concepts called the Upper 
Model \[ 1\], under which the concepts from various application 
domains are subordinated. By inheriting information from 
the Upper Model, domain concepts can be handled appropri- 
ately by the Penman language generator without he generator 
ever having to be explicitly informed of their individual na- 
ture. Similarly, the parser can exploit inherited Upper Model 
information when trying to place words appropriately into 
structures. More information can be found in \[7\]. 
Efficiency Considerations 
The classification-based architecture used by Loom solves 
a whole class of related efficiency problems by explicitly 
constructing and maintaining a subsumption-ordered lattice 
with inheritance. In particular, it may provide substantial 
improvements for some of the abovementioned sources of 
57  
inefficiency that have been observed with unification-based 
parsers: 
Structure Sharing: In most unification-based parsers, it is 
necessary tomake new copies of the feature structures that are 
associated with lexical items or grammatical rules whenever 
they are used in building a description of a sentence (or one 
of its constituents). In a classification-based ystem the entire 
structure does not need to be copied, because the description 
of a constituent can contain pointers to the classes of objects 
that it instantiates. This representation not only saves space, 
but it also allows the parser to make use of information that has 
already been precomputed ( uring the classification process) 
for classes of objects in the grammar and lexicon. Hence 
the organization of descriptions into a lattice automatically 
provides agreat amount of structure sharing. 
Indexing Dependencies: Theprocess ofclassification also 
keeps track of dependencies between different objects, elimi- 
nating the need for checking consistency between components 
of a description that have no features in common. In effect, an 
index is incrementally constructed from features to descrip- 
tions that contain them. This contrasts with most unification- 
based systems, in which feature structures are represented by 
directed graphs (or by first order terms, as in Prolog). 
Avoiding Redundant Computations: With un-typed fea- 
ture structures, each unification isperformed on a pair of struc- 
tures without reference to any stored knowledge, i.e., there is 
no way for simple unification to use the results of previous 
unification and subsumption computations, even if many ob- 
jects with identical features have already been unified. By 
explicitly representing the types of objects in a lattice, infor- 
mation can be stored for classes of objects, making it possible 
to avoid repeated computations for multiple objects having 
the same type (or any more specific type). Thus the first 
time a component of a description is classified, it is placed 
into the lattice containing all other descriptions in the knowl- 
edge base. Since the lattice xplicitly represents he types of 
objects, it makes full-depth consistency checks unnecessary 
between objects that are known to be in a subsumption rela- 
tionship, and subsumption (success) and consistency (failure) 
tests only need be computed once for all objects that belong 
to the same types. 
Using Classification as a Grammar Compiler: In sum- 
mary, classification can be seen as providing a capability 
similar to that provided by compilers in programming sys- 
tems. Although a simpler unification-based system may 
provide acceptable results with somewhat less overhead 
than a classification-based approach on a limited scale, a 
classification-based ystem is almost certainly to be preferable 
for applications that are necessarily knowledge-intensive. 
Concluding Remarks 
This work is part of an effort to provide the Penman sys- 
tem at ISI/USC with full natural language input and output 
capabilities. An experimental prototype of this parser using 
unification and a feature structure representation f part of 
Penman's grammar has been completed successfully. Most 
of the work in constructing a parser using the classification- 
based architecture of Loom and to reproduce the functionality 
of the unification-based system, now operating on the whole 
of the grammar, has been completed. 
If successful, this experiment should enable a comparison 
of classification and unification as mechanisms for parsing. 
The classification scheme appears to provide a way of sub- 
stantially reducing several of the most general sources of inef- 
ficiency that are observed in current unification-based parsers. 
However, this conjecture needs to be examined by performing 
experiments with several real grammars and applications. 
In addition to providing an efficient engine for processing 
the constraints of linguistic feature descriptions, we also ex- 
pect this type of information organization to provide a strong 
basis for integrating semantic knowledge and knowledge spe- 
cific to particular applications into the parsing process. 
Acknowledgment 
This research was sponsored in part by the United States 
Defense Advanced Research Projects Agency under contract 
MDA903-87-C-641 and in part by the United States Air Force 
Office of Scientific Research under contract F49620-87-C- 
0005. The opinions expressed here are solely those of the 
authors. 
Thanks to Bob MacGregor for many cooperative discus- 
sions and help with a partial implementation f these ideas 
using Loom. 
References 
\[1\] Bateman, J.A., Kasper, R.T., Moore, J.D., Whitney, R.A. 
A General Organization of Knowledge for Natural Lan- 
guage Processing: The Penman Upper Model. USCBSI 
Technical Report, Marina del Rey, 1990. 
\[2\] Bobrow, Robert and Webber, Bonnie. Knowledge Rep- 
resentation for Syntactic/Semantic Processing. In Pro- 
ceedings of AAAI-80, The First National Conference on 
Artificial Intelligence, Stanford, CA, August 1980. 
\[3\] Brachman, Ronald and Schmolze, James. An Overview 
of the KL-ONE Knowledge Representation System. 
Cognitive Science, Vol. 9:2, 1985. 
\[4\] Eisele, Andreas and Doerre, Jocben. Unification of Dis- 
junctive Feature Descriptions. InProceedings ofthe 26th 
Annual Meeting of the Association for Computational 
Linguistics, Buffalo, NY, June 1988. 
\[5\] Halliday, Michael. System and Function in Language. 
Kress G., (ed.), Oxford University Press, 1976. 
\[6\] Karttunen, Lauri. Features and Values. In Proceedings 
of the lOth International Conference on Computational 
Linguistics: COLING 84, Stanford, CA, July 1984. 
\[7\] Kasper, Robert. A Flexible Interface for Linking Ap- 
plications to Penman's Sentence Generator. In Proceed- 
ings of the DARPA Workshop on Speech and Natural 
Language, Philadelphia, PA, February 1989. 
58 
[8] Kasper, Robert. Conditional Descriptions in Functional 
Unification Grammar. In Proceedings ofthe 26th Annual 
Meeting of the Association for Computational Linguis- 
tics, Buffalo, NY, June 1988. 
[9] Kasper, Robert. An Experimental Parser for Systemic 
Grammars. In Proceedings of the 12th International 
Conference on Computational Linguistics, Budapest, 
August 1988. 
[10] Kasper, Robert. A Unification Method for Disjunctive 
Feature Descriptions. In Proceedings of the 25th An- 
nual Meeting of the Association for Computational Lin- 
guistics, Stanford, CA, July 1987. Also available as 
USC/Information Sciences Institute Reprint RS-87 -187. 
[11] Kay, Martin. Parsing in Functional Unification Gram- 
mar. In Natural Language Parsing, Dowty D., Karttunen 
L., and Zwicky A. (eds.), Cambridge University Press, 
1985. 
[12] MacGregor, Robert. A Deductive Pattern Matcher. In 
Proceedings ofAAAI-88, The Sixth National Conference 
on Artificial Intelligence, St. Paul, MN, August 1988. 
[13] The Penman Project. The Penman Primer, User Guide, 
Reference Manual, and Nigel Manual. System docu- 
mentation, USC/ISI Technical Report, Marina del Rey, 
1989. 
[14] Pollard, Carl and Sag, Ivan. Information Based Syntax. 
CSLI Lecture Notes Number 13, University of Chicago 
Press, 1987. 
[ 15] Rounds, William and Kasper, Robert. A Complete Log- 
ical Calculus for Record Structures Representing Lin- 
guistic Information. In Proceedings ofthe IEEE Sympo- 
sium on Logic in Computer Science, Cambridge, MA, 
June 1986. 
[16] Shieber, Stuart. The Design of a Computer Language 
for Linguistic Information. In Proceedings ofthe Tenth 
International Conference on ComputationalLinguistics: 
COLING 84, Stanford, CA, July 1984. 
[17] Smolka, Gert. A Feature Logic with Subsorts. LILOG 
Report 33, IBM Deutschland, Stuttgart, West Germany, 
May 1988. 
[18] Sondheimer, Norman K., Weischedel, Ralph M. and Bo- 
brow, Robert J. Semantic Interpretation Using KL-ONE. 
In Proceedings ofthe Tenth International Conference on 
Computational Linguistics: COLING 84, Stanford, CA, 
July 1984. 
59 
