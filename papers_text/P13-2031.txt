Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 171?176,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Co-regularizing character-based and word-based models for
semi-supervised Chinese word segmentation
Xiaodong Zeng? Derek F. Wong? Lidia S. Chao? Isabel Trancoso?
?Department of Computer and Information Science, University of Macau
?INESC-ID / Instituto Superior Te?cnico, Lisboa, Portugal
nlp2ct.samuel@gmail.com, {derekfw, lidiasc}@umac.mo,
isabel.trancoso@inesc-id.pt
Abstract
This paper presents a semi-supervised
Chinese word segmentation (CWS) ap-
proach that co-regularizes character-based
and word-based models. Similarly to
multi-view learning, the ?segmentation
agreements? between the two differen-
t types of view are used to overcome the
scarcity of the label information on unla-
beled data. The proposed approach train-
s a character-based and word-based mod-
el on labeled data, respectively, as the ini-
tial models. Then, the two models are con-
stantly updated using unlabeled examples,
where the learning objective is maximiz-
ing their segmentation agreements. The a-
greements are regarded as a set of valuable
constraints for regularizing the learning of
both models on unlabeled data. The seg-
mentation for an input sentence is decod-
ed by using a joint scoring function com-
bining the two induced models. The e-
valuation on the Chinese tree bank reveals
that our model results in better gains over
the state-of-the-art semi-supervised mod-
els reported in the literature.
1 Introduction
Chinese word segmentation (CWS) is a critical
and a necessary initial procedure with respect to
the majority of high-level Chinese language pro-
cessing tasks such as syntax parsing, informa-
tion extraction and machine translation, since Chi-
nese scripts are written in continuous characters
without explicit word boundaries. Although su-
pervised CWS models (Xue, 2003; Zhao et al,
2006; Zhang and Clark, 2007; Sun, 2011) pro-
posed in the past years showed some reasonably
accurate results, the outstanding problem is that
they rely heavily on a large amount of labeled da-
ta. However, the production of segmented Chi-
nese texts is time-consuming and expensive, since
hand-labeling individual words and word bound-
aries is very hard (Jiao et al, 2006). So, one can-
not rely only on the manually segmented data to
build an everlasting model. This naturally pro-
vides motivation for using easily accessible raw
texts to enhance supervised CWS models, in semi-
supervised approaches. In the past years, however,
few semi-supervised CWS models have been pro-
posed. Xu et al (2008) described a Bayesian semi-
supervised model by considering the segmentation
as the hidden variable in machine translation. Sun
and Xu (2011) enhanced the segmentation result-
s by interpolating the statistics-based features de-
rived from unlabeled data to a CRFs model. An-
other similar trial via ?feature engineering? was
conducted by Wang et al (2011).
The crux of solving semi-supervised learning
problem is the learning on unlabeled data. In-
spired by multi-view learning that exploits redun-
dant views of the same input data (Ganchev et
al., 2008), this paper proposes a semi-supervised
CWS model of co-regularizing from two dif-
ferent views (intrinsically two different models),
character-based and word-based, on unlabeled da-
ta. The motivation comes from that the two types
of model exhibit different strengths and they are
mutually complementary (Sun, 2010; Wang et al,
2010). The proposed approach begins by train-
ing a character-based and word-based model on
labeled data respectively, and then both models
are regularized from each view by their segmen-
tation agreements, i.e., the identical outputs, of
unlabeled data. This paper introduces segmenta-
tion agreements as gainful knowledge for guiding
the learning on the texts without label information.
Moreover, in order to better combine the strengths
of the two models, the proposed approach uses a
joint scoring function in a log-linear combination
form for the decoding in the segmentation phase.
171
2 Segmentation Models
There are two classes of CWS models: character-
based and word-based. This section briefly re-
views two supervised models in these categories,
a character-based CRFs model, and a word-based
Perceptrons model, which are used in our ap-
proach.
2.1 Character-based CRFs Model
Character-based models treat word segmentation
as a sequence labeling problem, assigning label-
s to the characters in a sentence indicating their
positions in a word. A 4 tag-set is used in this
paper: B (beginning), M (middle), E (end) and
S (single character). Xue (2003) first proposed
the use of CRFs model (Lafferty et al, 2001) in
character-based CWS. Let x = (x1x2...x|x|) ? X
denote a sentence, where each character and y =
(y1y2...y|y|) ? Y denote a tag sequence, yi ? T
being the tag assigned to xi. The goal is to achieve
a label sequence with the best score in the form,
p?c(y|x) =
1
Z(x; ?c)
exp{f(x, y) ? ?c} (1)
where Z(x; ?c) is a partition function that normal-
izes the exponential form to be a probability distri-
bution, and f(x, y) are arbitrary feature functions.
The aim of CRFs is to estimate the weight param-
eters ?c that maximizes the conditional likelihood
of the training data:
??c = argmax
?c
l?
i=1
log p?c(yi|xi)? ???c?22 (2)
where ???c?22 is a regularizer on parameters to
limit overfitting on rare features and avoid degen-
eracy in the case of correlated features. In this
paper, this objective function is optimized by s-
tochastic gradient method. For the decoding, the
Viterbi algorithm is employed.
2.2 Word-based Perceptrons Model
Word-based models read a input sentence from left
to right and predict whether the current piece of
continuous characters is a word. After one word
is identified, the method moves on and searches
for a next possible word. Zhang and Clark (2007)
first proposed a word-based segmentation mod-
el using a discriminative Perceptrons algorithm.
Given a sentence x, let us denote a possible seg-
mented sentence as w ? w, and the function that
enumerates a set of segmentation candidates as
GEN:w = GEN(x) for x. The objective is to
maximize the following problem for all sentences:
??w = argmax
w=GEN(x)
|w|?
i=1
?(x,wi) ? ?w (3)
where it maps the segmented sentencew to a glob-
al feature vector ? and denotes ?w as its cor-
responding weight parameters. The parameter-
s ?w can be estimated by using the Perceptron-
s method (Collins, 2002) or other online learning
algorithms, e.g., Passive Aggressive (Crammer et
al., 2006). For the decoding, a beam search decod-
ing method (Zhang and Clark, 2007) is used.
2.3 Comparison Between Both Models
Character-based and word-based models present
different behaviors and each one has its own
strengths and weakness. Sun (2010) carried out a
thorough survey that includes theoretical and em-
pirical comparisons from four aspects. Here, two
critical properties of the two models supporting
the co-regularization in this study are highlight-
ed. Character-based models present better predic-
tion ability for new words, since they lay more
emphasis on the internal structure of a word and
thereby express more nonlinearity. On the oth-
er side, it is easier to define the word-level fea-
tures in word-based models. Hence, these models
have a greater representational power and conse-
quently better recognition performance for in-of-
vocabulary (IV) words.
3 Semi-supervised Learning via
Co-regularizing Both Models
As mentioned earlier, the primary challenge of
semi-supervised CWS concentrates on the unla-
beled data. Obviously, the learning on unlabeled
data does not come for ?free?. Very often, it is
necessary to discover certain gainful information,
e.g., label constraints of unlabeled data, that is in-
corporated to guide the learner toward a desired
solution. In our approach, we believe that the seg-
mentation agreements (? 3.1) from two differen-
t views, character-based and word-based models,
can be such gainful information. Since each of the
models has its own merits, their consensuses signi-
fy high confidence segmentations. This naturally
leads to a new learning objective that maximizes
segmentation agreements between two models on
unlabeled data.
172
This study proposes a co-regularized CWS
model based on character-based and word-based
models, built on a small amount of segmented sen-
tences (labeled data) and a large amount of raw
sentences (unlabeled data). The model induction
process is described in Algorithm 1: given labeled
dataset Dl and unlabeled dataset Du, the first t-
wo steps are training a CRFs (character-based) and
Perceptrons (word-based) model on the labeled
data Dl , respectively. Then, the parameters of
both models are continually updated using unla-
beled examples in a learning cycle. At each iter-
ation, the raw sentences in Du are segmented by
current character-based model ?c and word-based
model ?w. Meanwhile, all the segmentation agree-
ments A are collected (? 3.1). Afterwards, the
agreements A are used as a set of constraints to
bias the learning of CRFs (? 3.2) and Perceptron
(? 3.3) on the unlabeled data. The convergence
criterion is the occurrence of a reduction of seg-
mentation agreements or reaching the maximum
number of learning iterations. In the final segmen-
tation phase, given a raw sentence, the decoding
requires both induced models (? 3.4) in measuring
a segmentation score.
Algorithm 1 Co-regularized CWS model induction
Require: n labeled sentencesDl;m unlabeled sentencesDu
Ensure: ?c and ?w
1: ?0c ? crf train(Dl)
2: ?0w ? perceptron train(Dl)
3: for t = 1...Tmax do
4: At ? agree(Du, ?t?1c , ?t?1w )
5: ?tc ? crf train constraints(Du,At, ?t?1c )
6: ?tw ? perceptron train constraints(Du,At, ?t?1w )
7: end for
3.1 Agreements Between Two Models
Given a raw sentence, e.g., ?????????
?????(I am watching the opening ceremony
of the Olympics in Beijing.)?, the two segmenta-
tions shown in Figure 1 are the predictions from
a character-based and word-based model. The
segmentation agreements between the two mod-
els correspond to the identical words. In this ex-
ample, the five words, i.e. ?? (I)?, ??? (Bei-
jing)?, ?? (watch)?, ???? (opening ceremony)?
and ??(.)?, are the agreements.
3.2 CRFs with Constraints
For the character-based model, this paper fol-
lows (Ta?ckstro?m et al, 2013) to incorporate the
segmentation agreements into CRFs. The main
idea is to constrain the size of the tag sequence
lattice according to the agreements for achieving
simplified learning. Figure 2 demonstrates an ex-
ample of the constrained lattice, where the bold
node represents that a definitive tag derived from
the agreements is assigned to the current charac-
ter, e.g., ?? (I)? has only one possible tag ?S?
because both models segmented it to a word with
a single character. Here, if the lattice of all admis-
sible tag sequences for the sentence x is denoted
as Y(x), the constrained lattice can be defined by
Y?(x, y?), where y? refers to tags inferred from the
agreements. Thus, the objective function on unla-
beled data is modeled as:
???c = argmax
?c
m?
i=1
log p?c(Y?(xi, y?i)|xi)? ???c?22
(4)
It is a marginal conditional probability given by
the total probability of all tag sequences consistent
with the constrained lattice Y?(x, y?). This objec-
tive can be optimized by using LBFGS-B (Zhu et
al., 1997), a generic quasi-Newton gradient-based
optimizer.
Figure 1: The segmentations given by character-
based and word-based model, where the words in
?2? refer to the segmentation agreements.
Figure 2: The constrained lattice representation
for a given sentence, ????????????
???.
3.3 Perceptrons with Constraints
For the word-based model, this study incorporates
segmentation agreements by a modified parame-
ter update criterion in Perceptrons online training,
as shown in Algorithm 2. Because there are no
?gold segmentations? for unlabeled sentences, the
output sentence predicted by the current model is
compared with the agreements instead of the ?an-
swers? in the supervised case. At each parameter
173
update iteration k, each raw sentence xu is decod-
ed with the current model into a segmentation zu.
If the words in output zu do not match the agree-
ments A(xu) of the current sentence xu, the pa-
rameters are updated by adding the global feature
vector of the current training example with the a-
greements and subtracting the global feature vec-
tor of the decoder output, as described in lines 3
and 4 of Algorithm 2.
Algorithm 2 Parameter update in word-based model
1: for k = 1...K, u = 1...m do
2: calculate zu = argmax
w=GEN(x)
?|w|
i=1 ?(xu, wi) ? ?k?1w
3: if zu 6= A(xu)
4: ?kw = ?k?1w + ?(A(xu))? ?(zu)
5: end for
3.4 The Joint Score Function for Decoding
There are two co-regularized models as results of
the previous induction steps. An intuitive idea is
that both induced models are combined to conduct
the segmentation, for the sake of integrating their
strengths. This paper employs a log-linear inter-
polation combination (Bishop, 2006) to formulate
a joint scoring function based on character-based
and word-based models in the decoding:
Score(w) = ? ? log(p?c(y|x))
+(1? ?) ? log(?(x,w) ? ?w) (5)
where the two terms of the logarithm are the s-
cores of character-based and word-based model-
s, respectively, for a given segmentation w. This
composite function uses a parameter ? to weight
the contributions of the two models. The ? value
is tuned using the development data.
4 Experiment
4.1 Setting
The experimental data is taken from the Chinese
tree bank (CTB). In order to make a fair compar-
ison with the state-of-the-art results, the versions
of CTB-5, CTB-6, and CTB-7 are used for the e-
valuation. The training, development and testing
sets are defined according to the previous works.
For CTB-5, the data split from (Jiang et al, 2008)
is employed. For CTB-6, the same data split as
recommended in the CTB-6 official document is
used. For CTB-7, the datasets are formed accord-
ing to the way in (Wang et al, 2011). The cor-
responding statistic information on these data s-
plits is reported in Table 1. The unlabeled data in
our experiments is from the XIN CMN portion of
Chinese Gigaword 2.0. The articles published in
1991-1993 and 1999-2004 are used as unlabeled
data, with 204 million words.
The feature templates in (Zhao et al, 2006)
and (Zhang and Clark, 2007) are used in train-ing
the CRFs model and Perceptrons model, respec-
tively. The experimental platform is implement-
ed based on two popular toolkits: CRF++ (Kudo,
2005) and Zpar (Zhang and Clark, 2011).
Data #Sent-train
#Sent-
dev
#Sent-
test
OOV-
dev
OOV-
test
CTB-5 18,089 350 348 0.0811 0.0347
CTB-6 23,420 2,079 2,796 0.0545 0.0557
CTB-7 31,131 10,136 10,180 0.0549 0.0521
Table 1: Statistics of CTB-5, CTB-6 and CTB-7
data.
4.2 Main Results
The development sets are mainly used to tune the
values of the weight factor ? in Equation 5. We
evaluated the performance (F-score) of our model
on the three development sets by using differen-
t ? values, where ? is progressively increased in
steps of 0.1 (0 < ? < 1.0). The best performed
settings of ? for CTB-5, CTB-6 and CTB-7 on de-
velopment data are 0.7, 0.6 and 0.6, respectively.
With the chosen parameters, the test data is used
to measure the final performance.
Table 2 shows the F-score results of word seg-
mentation on CTB-5, CTB-6 and CTB-7 testing
sets. The line of ?ours? reports the performance
of our semi-supervised model with the tuned pa-
rameters. We first compare it with the supervised
?baseline? method which joints character-based
and word-based model trained only on the training
set1. It can be observed that our semi-supervised
model is able to benefit from unlabeled data and
greatly improves the results over the supervised
baseline. We also compare our model with two
state-of-the-art semi-supervised methods of Wang
?11 (Wang et al, 2011) and Sun ?11 (Sun and X-
u, 2011). The performance scores of Wang ?11 are
directly taken from their paper, while the results of
Sun ?11 are obtained, using the program provided
by the author, on the same experimental data. The
1The ?baseline? uses a different training configuration so
that the ? values in the decoding are also need to be tuned on
the development sets. The tuned ? values are {0.6, 0.6, 0.5}
for CTB-5, CTB-6 and CTB-7.
174
bold scores indicate that our model does achieve
significant gains over these two semi-supervised
models. This outcome can further reveal that us-
ing the agreements from these two views to regu-
larize the learning can effectively guide the mod-
el toward a better solution. The third compari-
son candidate is Hatori ?12 (Hatori et al, 2012)
which reported the best performance in the litera-
ture on these three testing sets. It is a supervised
joint model of word segmentation, POS tagging
and dependency parsing. Impressively, our model
still outperforms Hatori ?12 on all three datasets.
Although there is only a 0.01 increase on CTB-5,
it can be seen as a significant improvement when
considering Hatori ?12 employs much richer train-
ing resources, i.e., sentences tagged with syntactic
information.
Method CTB-5 CTB-6 CTB-7
Ours 98.27 96.33 96.72
Baseline 97.58 94.71 94.87
Wang ?11 98.11 95.79 95.65
Sun ?11 98.04 95.44 95.34
Hatori ?12 98.26 96.18 96.07
Table 2: F-score (%) results of five CWS models
on CTB-5, CTB-6 and CTB-7.
5 Conclusion
This paper proposed an alternative semi-
supervised CWS model that co-regularizes a
character- and word-based model by using their
segmentation agreements on unlabeled data. We
perform the agreements as valuable knowledge
for the regularization. The experiment results
reveal that this learning mechanism results in a
positive effect to the segmentation performance.
Acknowledgments
The authors are grateful to the Science and Tech-
nology Development Fund of Macau and the Re-
search Committee of the University of Macau for
the funding support for our research, under the ref-
erence No. 017/2009/A and MYRG076(Y1-L2)-
FST13-WF. The authors also wish to thank the
anonymous reviewers for many helpful comments.
References
Christopher M. Bishop. 2006. Pattern recognition and
machine learning.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1-8, Philadelphia, USA.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of ma-chine
learning research, 7:551-585.
Kuzman Ganchev, Joao Graca, John Blitzer, and Ben
Taskar. 2008. Multi-View Learning over Struc-
tured and Non-Identical Outputs. In Proceedings of
CUAI, pages 204-211, Helsinki, Finland.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2012. Incremental Joint Approach
to Word Segmentation, POS Tagging, and Depen-
dency Parsing in Chinese. In Proceedings of ACL,
pages 1045-1053, Jeju, Republic of Korea.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Liu.
2008. A Cascaded Linear Model for Joint Chinese
Word Segmentation and Part-of-Speech Tagging. In
Proceedings of ACL, pages 897-904, Columbus, O-
hio.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic Adaptation of Annotation Standards: Chi-
nese Word Segmentation and POS Tagging - A Case
Study. In Proceedings of ACL and the 4th IJCNLP
of the AFNLP, pages 522-530, Suntec, Singapore.
Feng Jiao, Shaojun Wang and Chi-Hoon Lee. 2006.
Semi-supervised conditional random fields for im-
proved sequence segmentation and labeling. In Pro-
ceedings of ACL and the 4th IJCNLP of the AFNLP,
pages 209-216, Strouds-burg, PA, USA.
Taku Kudo. 2005. CRF++: Yet another CRF toolkit.
Software available at http://crfpp.sourceforge. net.
John Lafferty, Andrew McCallum, and Fernando Pe-
reira. 2001. Conditional Random Field: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of ICML, pages 282-
289, Williams College, USA.
Weiwei Sun. 2001. Word-based and character-based
word segmentation models: comparison and com-
bination. In Proceedings of COLING, pages 1211-
1219, Bejing, China.
Weiwei Sun. 2011. A stacked sub-word model for
joint Chinese word segmentation and part-of-speech
tagging. In Proceedings of ACL, pages 1385-1394,
Portland, Oregon.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
word segmentation using unlabeled data. In Pro-
ceedings of EMNLP, pages 970-979, Scotland, UK.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan M-
cDonald, and Joakim Nivre. 2013. Token and Type
Constraints for Cross-Lingual Part-of-Speech Tag-
ging. In Transactions of the Association for Compu-
tational Linguistics, 1:1-12.
175
Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010.
A Character-Based Joint Model for Chinese Word
Segmentation. In Proceedings of COLING, pages
1173-1181, Bejing, China.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Torisawa.
2011. Improving Chinese word segmentation and
POS tagging with semi-supervised methods using
large auto-analyzed data. In Proceedings of IJC-
NLP, pages 309-317, Hyderabad, India.
Jia Xu, Jianfeng Gao, Kristina Toutanova and Her-
mann Ney. 2008. Bayesian semi-supervised chinese
word segmentation for statistical machine transla-
tion. In Proceedings of COLING, pages 1017-1024,
Manchester, UK.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29-48.
Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation using a word-based perceptron algorithm. In
Proceedings of ACL, pages 840-847, Prague, Czech
Republic.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using
a single discriminative model. In Proceedings of
EMNLP, pages 843-852, Massachusetts, USA.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105-151.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006. Effective tag set selection in Chinese
word segmentation via conditional random field
modeling. In Proceedings of PACLIC, pages 87-94,
Wuhan, China.
Ciyou Zhu, Richard H. Byrd, Peihuang Lu, and Jorge
Nocedal. 2006. L-BFGS-B: Fortran subroutines for
large scale bound constrained optimization. ACM
Transactions on Mathematical Software, 23:550-
560.
176
