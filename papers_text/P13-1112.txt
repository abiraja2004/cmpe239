Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1137?1147,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Reconstructing an Indo-European Family Tree
from Non-native English texts
Ryo Nagata1,2 Edward Whittaker3
1Konan University / Kobe, Japan
2LIMSI-CNRS / Orsay, France
3Inferret Limited / Northampton, England
nagata-acl@hyogo-u.ac.jp, ed@inferret.co.uk
Abstract
Mother tongue interference is the phe-
nomenon where linguistic systems of a
mother tongue are transferred to another
language. Although there has been plenty
of work on mother tongue interference,
very little is known about how strongly
it is transferred to another language and
about what relation there is across mother
tongues. To address these questions,
this paper explores and visualizes mother
tongue interference preserved in English
texts written by Indo-European language
speakers. This paper further explores lin-
guistic features that explain why certain
relations are preserved in English writing,
and which contribute to related tasks such
as native language identification.
1 Introduction
Transfer of linguistic systems of a mother tongue
to another language, namely mother tongue inter-
ference, is often observable in the writing of non-
native speakers. The reader may be able to deter-
mine the mother tongue of the writer of the fol-
lowing sentence from the underlined article error:
The alien wouldn?t use my spaceship but
the hers.
The answer would probably be French or Span-
ish; the definite article is allowed to modify pos-
sessive pronouns in these languages, and the us-
age is sometimes negatively transferred to English
writing. Researchers such as Swan and Smith
(2001), Aarts and Granger (1998), Davidsen-
Nielsen and Harder (2001), and Altenberg and
Tapper (1998) work on mother tongue interfer-
ence to reveal overused/underused words, part of
speech (POS), or grammatical items.
In contrast, very little is known about how
strongly mother tongue interference is transferred
to another language and about what relation there
is across mother tongues. At one extreme, one
could argue that it is so strongly transferred to
texts in another language that the linguistic rela-
tions between mother tongues are perfectly pre-
served in the texts. At the other extreme, one
can counter it, arguing that other features such as
non-nativeness are more influential than mother
tongue interference. One possible reason for this
is that a large part of the distinctive language sys-
tems of a mother tongue may be eliminated when
transferred to another language from a speaker?s
mother tongue. For example, Slavic languages
have a rich inflectional case system (e.g., Czech
has seven inflectional cases) whereas French does
not. However, the difference in the richness cannot
be transferred into English because English has al-
most no inflectional case system. Thus, one can-
not determine the mother tongue of a given non-
native text from the inflectional case. A similar
argument can be made about some parts of gen-
der, tense, and aspect systems. Besides, Wong and
Dras (2009) show that there are no significant dif-
ferences, between mother tongues, in the misuse
of certain syntactic features such as subject-verb
agreement that have different tendencies depend-
ing on their mother tongues. Considering these,
one could not be so sure which argument is cor-
rect. In any case, to the best of our knowledge, no
one has yet answered this question.
In view of this background, we take the first step
in addressing this question. We hypothesize that:
Hypothesis: Mother tongue interference is so
strong that the relations in a language fam-
ily are preserved in texts written in another
language.
In other words, mother tongue interference is so
strong that one can reconstruct a language fam-
1137
ily tree from non-native texts. One of the major
contributions of this work is to reveal and visual-
ize a language family tree preserved in non-native
texts, by examining the hypothesis. This becomes
important in native language identification1 which
is useful for improving grammatical error correc-
tion systems (Chodorow et al, 2010) or for pro-
viding more targeted feedback to language learn-
ers. As we will see in Sect. 6, this paper reveals
several crucial findings that contribute to improv-
ing native language identification. In addition, this
paper shows that the findings could contribute to
reconstruction of language family trees (Enright
and Kondrak, 2011; Gray and Atkinson, 2003;
Barbanc?on et al, 2007; Batagelj et al, 1992;
Nakhleh et al, 2005), which is one of the central
tasks in historical linguistics.
The rest of this paper is structured as follows.
Sect. 2 introduces the basic approach of this work.
Sect. 3 discusses the methods in detail. Sect. 4 de-
scribes experiments conducted to investigate the
hypothesis. Sect. 5 discusses the experimental re-
sults. Sect. 6 discusses implications for work in
related domains.
2 Approach
To examine the hypothesis, we reconstruct a
language family tree from English texts writ-
ten by non-native speakers of English whose
mother tongue is one of the Indo-European lan-
guages (Beekes, 2011; Ramat and Ramat, 2006).
If the reconstructed tree is sufficiently similar to
the original Indo-European family tree, it will sup-
port the hypothesis. If not, it suggests that some
features other than mother tongue interference are
more influential.
The approach we use for reconstructing a lan-
guage family tree is to apply agglomerative hi-
erarchical clustering (Han and Kamber, 2006) to
English texts written by non-native speakers. Re-
searchers have already performed related work
on reconstructing language family trees. For in-
stance, Kroeber and Chrie?tien (1937) and Ellega?rd
(1959) proposed statistical methods for measuring
the similarity metric between languages. More re-
cently, Batagelj et al (1992) and Kita (1999) pro-
posed methods for reconstructing language fam-
ily trees using clustering. Among them, the
1Recently, native language identification has drawn the at-
tention of NLP researchers. For instance, a shared task on
native language identification took place at an NAACL-HLT
2013 workshop.
most related method is that of Kita (1999). In
his method, a variety of languages are modeled
by their spelling systems (i.e., character-based
n-gram language models). Then, agglomera-
tive hierarchical clustering is applied to the lan-
guage models to reconstruct a language family
tree. The similarity used for clustering is based on
a divergence-like distance between two language
models that was originally proposed by Juang and
Rabiner (1985). This method is purely data-driven
and does not require human expert knowledge for
the selection of linguistic features.
Our work closely follows Kita?s work. How-
ever, it should be emphasized that there is a signif-
icant difference between the two. Kita?s work (and
other previous work) targets clustering of a variety
of languages whereas our work tries to reconstruct
a language family tree preserved in non-native En-
glish. This significant difference prevents us from
directly applying techniques in the literature to our
task. For instance, Batagelj et al (1992) use basic
vocabularies such as belly in English and ventre in
French to measure similarity between languages.
Obviously, this does not work on our task; belly is
belly in English writing whoever writes it. Kita?s
method is also likely not to work well because all
texts in our task share the same spelling system
(i.e., English spelling). Although spelling is some-
times influenced by mother tongues, it involves a
lot more including overuse, underuse, and misuse
of lexical, grammatical, and syntactic systems.
To solve the problem, this work adopts a word-
based language model in the expectation that word
sequences reflect mother tongue interference. At
the same time, its simple application would cause
a serious side effect. It would reflect the topics
of given texts rather than mother tongue interfer-
ence. Unfortunately, there exists no such English
corpus that covers a variety of language speakers
with uniform topics; moreover the availability of
non-native corpora is still somewhat limited. This
also means that available non-native corpora may
be too small to train reliable word-based language
models. The next section describes two methods
(language model-based and vector-based), which
address these problems.
3 Methods
3.1 Language Model-based Method
To begin with, let us define the following symbols
used in the methods. Let Di be a set of English
1138
texts where i denotes a mother tongue i. Similarly,
let Mi be a language model trained using Di.
To solve the problems pointed out in Sect. 2, we
use an n-gram language model based on a mixture
of word and POS tokens instead of a simple word-
based language model. In this language model,
content words in n-grams are replaced with their
corresponding POS tags. This greatly decreases
the influence of the topics of texts, as desired. It
also decreases the number of parameters in the
language model.
To build the language model, the following
three preprocessing steps are applied to Di. First,
texts in Di are split into sentences. Second, each
sentence is tokenized, POS-tagged, and mapped
entirely to lowercase. For instance, the first ex-
ample sentence in Sect. 1 would give:
the/DT alien/NN would/MD not/RB
use/VB my/PRP$ spaceship/NN but/CC
the/DT hers/PRP ./.
Finally, words are replaced with their correspond-
ing POS tags; for the following words, word to-
kens are used as their corresponding POS tags:
coordinating conjunctions, determiners, preposi-
tions, modals, predeterminers, possessives, pro-
nouns, question adverbs. Also, proper nouns are
treated as common nouns. At this point, the spe-
cial POS tags BOS and EOS are added at the begin-
ning and end of each sentence, respectively. For
instance, the above example would result in the
following word/POS sequence:
BOS the NN would RB VB my NN but
the hers . EOS
Note that the content of the original sentence is far
from clear while reflecting mother tongue interfer-
ence, especially in the hers.
Now, the language model Mi can be built from
Di. We set n = 3 (i.e., trigram language model)
following Kita?s work and use Kneser-Ney (KN)
smoothing (Kneser and Ney, 1995) to estimate its
conditional probabilities.
With Mi and Di, we can naturally apply Kita?s
method to our task. The clustering algorithm used
is agglomerative hierarchical clustering with the
average linkage method. The distance2 between
two language models is measured as follows. The
2It is not a distance in a mathematical sense. However,
we will use the term distance following the convention in the
literature.
probability that Mi generates Di is calculated by
Pr(Di|Mi). Note that
Pr(Di|Mi) ?
Pr(w1,i) Pr(w2,i|w1,i)
?
|Di|?
t=3
Pr(wt,i|wt?2,i, wt?1,i) (1)
where wt,i and |Di| denote the tth token in Di and
the number of tokens in Di, respectively, since we
use the trigram language model. Then, the dis-
tance from Mi to Mj is defined by
d(Mi ? Mj) =
1
|Dj |
log Pr(Dj |Mj)Pr(Dj |Mi)
. (2)
In other words, the distance is determined based
on the ratio of the probabilities that each lan-
guage model generates the language data. Because
d(Mi ? Mj) and d(Mj ? Mi) are not symmet-
rical, we define the distance between Mi and Mj
to be their average:
d(Mi,Mj)=
d(Mi ? Mj)+d(Mj ? Mi)
2 . (3)
Equation (3) is used to calculate the distance be-
tween two language models for clustering.
To sum up, the procedure of the language fam-
ily tree construction method is as follows: (i) Pre-
process each Di; (ii) Build Mi from Di; (iii) Cal-
culate the distances between the language models;
(iv) Cluster the language data using the distances;
(v) Output the result as a language family tree.
3.2 Vector-based Method
We also examine a vector-based method for lan-
guage family tree reconstruction. As we will see
in Sect. 5, this method allows us to interpret clus-
tering results more easily than with the language
model-based method while both result in similar
language family trees.
In this method, Di is modeled by a vector. The
vector is constructed based on the relative frequen-
cies of trigrams. As a consequence, the distance
is naturally defined by the Euclidean distance be-
tween two vectors. The clustering procedure is the
same as for the language model-based method ex-
cept that Mi is vector-based and that the distance
metric is Euclidean.
1139
4 Experiments
We selected the ICLE corpus v.2 (Granger et al,
2009) as the target language data. It consists of
English essays written by a wide variety of non-
native speakers of English. Among them, the 11
shown in Table 1 are of Indo-European languages.
Accordingly, we selected the subcorpora of the 11
languages in the experiments. Before the exper-
iments, we preprocessed the corpus data to con-
trol the experimental conditions. Because some of
the writers had more than one native language, we
excluded essays that did not meet the following
three conditions: (i) the writer has only one na-
tive language; (ii) the writer has only one language
at home; (iii) the two languages in (i) and (ii) are
the same as the native language of the subcorpus
to which the essay belongs3. After the selection,
markup tags such as essay IDs were removed from
the corpus data. Also, the symbols ? and ? were
unified into ?4. For reference, we also used na-
tive English (British and American university stu-
dents? essays in the LOCNESS corpus5) and two
sets of Japanese English (ICLE and the NICE cor-
pus (Sugiura et al, 2007)). Table 1 shows the
statistics on the corpus data.
Performance of POS tagging is an important
factor in our methods because they are based on
word/POS sequences. Existing POS taggers might
not perform well on non-native English texts be-
cause they are normally developed to analyze na-
tive English texts. Considering this, we tested
CRFTagger6 on non-native English texts contain-
ing various grammatical errors before the exper-
iments (Nagata et al, 2011). It turned out that
CRFTagger achieved an accuracy of 0.932 (com-
pared to 0.970 on native texts). Although it did not
perform as well as on native texts, it still achieved
a fair accuracy. Accordingly, we decided to use it
in our experiments.
Then, we generated cluster trees from the cor-
pus data using the methods described in Sect. 3.
3For example, because of (iii), essays written by native
speakers of Swedish in the Finnish subcorpus were excluded
from the experiments. This is because they were collected in
Finland and might be influenced by Finnish.
4The symbol ? is sometimes used for ? (e.g., I?m).
5The LOCNESS corpus is a corpus of native En-
glish essays made up of British pupils? essays, British
university students? essays, and American university
students? essays: https://www.uclouvain.be/
en-cecl-locness.html
6Xuan-Hieu Phan, ?CRFTagger: CRF English POS
Tagger,? http://crftagger.sourceforge.net/,
2006.
Native language # of essays # of tokens
Bulgarian 294 219,551
Czech 220 205,264
Dutch 244 240,861
French 273 202,439
German 395 236,841
Italian 346 219,581
Norwegian 290 218,056
Polish 354 251,074
Russian 255 236,748
Spanish 237 211,343
Swedish 301 268,361
English 298 294,357
Japanese1 (ICLE) 171 224,534
Japanese2 (NICE) 340 130,156
Total 4,018 3,159,166
Table 1: Statistics on target corpora.
We used the Kyoto Language Modeling toolkit7
to build language models from the corpus data.
We removed n-grams that appeared less than five
times8 in each subcorpus in the language mod-
els. Similarly, we implemented the vector-based
method with trigrams using the same frequency
cutoff (but without smoothing).
Fig. 1 shows the experimental results. The
tree at the top is the Indo-European family tree
drawn based on the figure shown in Crystal
(1997). It shows that the 11 languages are divided
into three groups: Italic, Germanic, and Slavic
branches. The second and third trees are the clus-
ter trees generated by the language model-based
and vector-based methods, respectively. The num-
ber at each branching node denotes in which step
the two clusters were merged.
The experimental results strongly support the
hypothesis we made in Sect. 1. Fig. 1 reveals
that the language model-based method correctly
groups the 11 Englishes into the Italic, Ger-
manic, and Slavic branches. It first merges
Norwegian-English and Swedish-English into a
cluster. The two languages belong to the North
Germanic branch of the Germanic branch and
thus are closely related. Subsequently, the lan-
guage model-based method correctly merges the
other languages into the three branches. A dif-
7The Kyoto Language Modeling toolkit: http://www.
phontron.com/kylm/
8We found that the results were not sensitive to the value
of frequency cutoff so long as we set it to a small number.
1140
Polish
Italic Germanic Slavic
13
6
7
8
9
10
BulgarianSwedishFrench Spanish Norwegian CzechItalian RussianDutchGerman
French 
English
Spanish 
English
Italian 
English
Swedish 
English
Norwegian 
English
Dutch 
English
German
English
Polish 
English
Bulgarian 
English
Czech
English
Russian 
English
2 45
Indo-European family tree
Cluster tree generated by  LM-based method
13
4
7
6
8
10
French English Spanish English Italian English Swedish EnglishNorwegian EnglishDutch English German English Polish EnglishBulgarian EnglishCzech English Russian English
2 5
9 Cluster tree generated by vector-based clustering
Figure 1: Experimental results.
ference between its cluster tree and the Indo-
European family tree is that there are some mis-
matches within the Germanic and Slavic branches.
While the difference exists, the method strongly
distinguishes the three branches from one an-
other. The third tree shows that the vector-based
method behaves similarly while it mistakenly at-
taches Polish-English into an independent branch.
From these results, we can say that mother tongue
interference is transferred into the 11 Englishes,
strongly enough for reconstructing its language
family tree, which we propose calling the inter-
language Indo-European family tree in English.
Fig. 2 shows the experimental results with na-
tive and Japanese Englishes. It shows that the
same interlanguage Indo-European family tree
was reconstructed as before. More interestingly,
native English was detached from the interlan-
guage Indo-European family tree contrary to the
expectation that it would be attached to the Ger-
manic branch because English is of course a mem-
ber of the Germanic branch. This implies that
non-nativeness common to the 11 Englishes is
more influential than the intrafamily distance is9;
9Admittedly, we need further investigation to confirm this
argument especially because we applied CRFTagger, which is
developed to analyze native English, to both non-native and
native Englishes, which might affect the results.
Interlanguage Indo-European family tree Other family
JapaneseEnglish1 JapaneseEnglish2
3
Native English
12 13
ACL 2013
Figure 2: Experimental results with native and
Japanese Englishes.
otherwise, native English would be included in
the German branch. Fig. 2 also shows that the
two sets of Japanese English were merged into
a cluster and that it was the most distant in the
whole tree. This shows that the interfamily dis-
tance is the most influential factor. Based on
these results, we can further hypothesize as fol-
lows: interfamily distance > non-nativeness >
intrafamily distance.
5 Discussion
To get a better understanding of the interlanguage
Indo-European family tree, we further explore lin-
guistic features that explain well the above phe-
nomena. When we analyze the experimental re-
sults, however, some problems arise. It is al-
most impossible to find someone who has a good
knowledge of the 11 languages and their mother
language interference in English writing. Besides,
there are a large number of language pairs to com-
pare. Thus, we need an efficient and effective way
to analyze the experimental results.
To address these problems, we did the follow-
ing. First, we focused on only a few Englishes
out of the 11. Because one of the authors had
some knowledge of French, we selected French-
English as the main target. This naturally made
us select the other Italic Englishes as its counter-
parts. Also, because we had access to a native
speaker of Russian who had a good knowledge of
English, we included Russian-English in our fo-
cus. We analyzed these Englishes and then ex-
amined whether the findings obtained apply to the
other Englishes or not. Second, we used a method
for extracting interesting trigrams from the cor-
pus data. The method compares three out of the
11 corpora (for example, French-, Spanish-, and
Russian-Englishes). If we remove instances of a
trigram from each set, the clustering tree involving
1141
the three may change. For example, the removal
of but the hers may result in a cluster tree merg-
ing French- and Russian-Englishes before French-
and Spanish-Englishes. Even if it does not change,
the distances may change in that direction. We an-
alyzed what trigrams had contributed to the clus-
tering results with this approach.
To formalize this approach, we will denote a tri-
gram by t. We will also denote its relative fre-
quency in the language data Di by rti. Then, the
change in the distances caused by the removal of t
from Di, Dj , and Dk is quantified by
s = (rtk ? rti)2 ? (rtj ? rti)2 (4)
in the vector-based method. The quantity (rtk ?
rti)2 is directly related to the decrease in the dis-
tance between Di and Dk and similarly, (rtj ?
rti)2 to that between Di and Dj in the vector-
based method. Thus, the greater s is, the higher the
chance that the cluster tree changes. Therefore, we
can obtain a list of interesting trigrams by sorting
them according to s. We could do a similar calcu-
lation in the language model-based method using
the conditional probabilities. However, it requires
a more complicated calculation. Accordingly, we
limit ourselves to the vector-based method in this
analysis, noting that both methods generated sim-
ilar cluster trees.
Table 2 shows the top 15 interesting trigrams
where Di, Dj , and Dk are French-, Spanish-, and
Russian-Englishes, respectively. Note that s is
multiplied by 106 and r is in % for readability. The
list reveals that many of the trigrams contain the
article a or the. Interestingly, their frequencies are
similar in French-English and Spanish-English,
and both are higher than in Russian-English. This
corresponds to the fact that French and Spanish
have articles whereas Russian does not. Actu-
ally, the same argument can be made about the
other Italic and Slavic Englishes (e.g., the JJ NN:
Italian-English 0.82; Polish-English 0.72)10. An
exception is that of trigrams containing the definite
article in Bulgarian-English; it tends to be higher
in Bulgarian-English than in the other Slavic En-
glishes. Surprisingly and interestingly, however, it
reflects the fact that Bulgarian does have the def-
inite article but not the indefinite article (e.g., the
JJ NN: 0.82; a JJ NN: 0.60 in Bulgarian-English).
10Due to the space limitation, other lists were not included
in this paper but are available at http://web.hyogo-u.
ac.jp/nagata/acl/.
Table 3 shows that the differences in article
use exist even between the Italic and Germanic
branches despite the fact that both have the in-
definite and definite articles. The list still con-
tains a number of trigrams containing articles. For
a better understanding of this, we looked further
into the distribution of articles in the corpus data.
It turns out that the distribution almost perfectly
groups the 11 Englishes into the corresponding
branches as shown in Fig. 3. The overall use of
articles is less frequent in the Slavic-Englishes.
The definite article is used more frequently in the
Italic-Englishes than in the Germanic Englishes
(except for Dutch-English). We speculate that
this is perhaps because the Italic languages have a
wider usage of the definite article such as its modi-
fication of possessive pronouns and proper nouns.
The Japanese Englishes form another group (this
is also true for the following findings). This corre-
sponds to the fact that the Japanese language does
not have an article system similar to that of En-
glish.
s Trigram t rti rtj rtk
5.14 the NN of 1.01 0.98 0.78
4.38 a JJ NN 0.85 0.77 0.62
2.74 the JJ NN 0.87 0.86 0.71
2.30 NN of the 0.49 0.52 0.33
1.64 . . . 0.22 0.12 0.05
1.56 NNS . EOS 0.77 0.70 0.92
1.31 NNS and NNS 0.09 0.13 0.21
1.25 BOS RB , 0.25 0.22 0.14
1.22 of the NN 0.42 0.44 0.30
1.17 VBZ to VB 0.26 0.22 0.14
1.09 BOS i VBP 0.07 0.05 0.17
1.03 NN of NN 0.74 0.70 0.63
0.88 NN of JJ 0.15 0.15 0.25
0.67 the JJ NNS 0.28 0.28 0.20
0.65 NN to VB 0.40 0.38 0.31
Table 2: Interesting trigrams (French- (Di),
Spanish- (Dj), and Russian- (Dk) Englishes).
Another interesting trigram, though not as ob-
vious as article use, is NN of NN, which ranks
12th and 2nd in Table 2 and 3, respectively. In the
Italic Englishes, the trigram is more frequent than
the other non-native Englishes as shown in Fig. 4.
This corresponds to the fact that noun-noun com-
pounds are less common in the Italic languages
than in English and that instead, the of -phrase (NN
of NN) is preferred (Swan and Smith, 2001). For
1142
s Trigram t rti rtj rtk
21.49 the NN of 1.01 0.98 0.54
5.70 NN of NN 0.74 0.70 0.50
3.26 NN of the 0.49 0.52 0.30
3.10 the JJ NN 0.87 0.86 0.70
2.62 . . . 0.22 0.12 0.03
1.53 of the NN 0.42 0.44 0.29
1.50 NN , NN 0.30 0.30 0.18
1.50 BOS i VBP 0.07 0.05 0.19
0.85 NNS and NNS 0.09 0.13 0.19
0.81 JJ NN of 0.40 0.39 0.31
0.68 . . EOS 0.13 0.06 0.02
0.63 a JJ NN 0.85 0.77 0.73
0.63 RB . EOS 0.21 0.16 0.31
0.56 NN , the 0.16 0.16 0.08
0.50 NN of a 0.17 0.09 0.06
Table 3: Interesting trigrams (French- (Di),
Spanish- (Dj), and Swedish- (Dk) Englishes).
instance, orange juice is expressed as juice of or-
ange in the Italic languages (e.g., jus d?orange in
French). In contrast, noun-noun compounds or
similar constructions are more common in Russian
and Swedish. As a result, NN of NN becomes rel-
atively frequent in the Italic Englishes. Fig. 4 also
shows that its distribution roughly groups the 11
Englishes into the three branches. Therefore, the
way noun phrases (NPs) are constructed is a clue
to how the three branches were clustered.
This finding in turn reveals that the consecu-
tive repetitions of nouns occur less in the Italic
Englishes. In other words, the length tends to
be shorter than in the others where we define
the length as the number of consecutive repeti-
tions of common nouns (for example, the length
of orange juice is one because a noun is con-
secutively repeated once). To see if this is true,
we calculated the average length for each English.
Fig. 5 shows that the average length roughly dis-
tinguishes the Italic Englishes from the other non-
native Englishes; French-English is the shortest,
which is explained by the discussion above, while
Dutch- and German-Englishes are longest, which
may correspond to the fact that they have a prefer-
ence for noun-noun compounds as Snyder (1996)
argues. For instance, German allows the concate-
nated form as in Orangensaft (equivalently or-
angejuice). This tendency in the length of noun-
noun compounds provides us with a crucial insight
for native language identification, which we will
 2
 3
 4
 5
 6
 1  1.5  2  2.5  3
R
el
at
iv
e 
fre
qu
en
cy
 o
f d
ef
in
ite
 a
rti
cle
 (%
)
Relative frequency of indefinite article (%)
Bulgarian
Czech
Dutch
French
German
Italian
Norwegian
Polish
Russian
Spanish
Swedish
English
Japanese1
Japanese2
Italic
Germanic
Slavic
Japanese
Figure 3: Distribution of articles.
 0
 0.5  1
Relative frequency of NN of NN (%)
French
Italian
Spanish
Italic
Polish
Russian
Bulgarian
Czech
Slavic
English
Dutch
Swedish
German
Norwegian
Germanic
Japanese1
Japanese2 Japanese
Figure 4: Relative frequency of NN of NN in each
corpus (%).
come back to in Sect. 6.
The trigrams BOS RB , in Table 2 and RB . EOS
in Table 3 imply that there might also be a certain
pattern in adverb position in the 11 Englishes (they
roughly correspond to adverbs at the beginning
and end of sentences). Fig. 6 shows an insight into
this. The horizontal and vertical axes correspond
to the ratio of adverbs at the beginning and the end
of sentences, respectively. It turns out that the Ger-
man Englishes form a group. So do the Italic En-
glishes although it is less dense. In contrast, the
Slavic Englishes are scattered. However, the ra-
tios give a clue to how to distinguish Slavic En-
glishes from the others when combined with other
1143
 0
 0.1
Average length of noun-noun compounds
French
Italian
Spanish
Italic
Bulgarian
Czech
Russian
Polish
Slavic
Swedish
Norwegian
German
Dutch
English
Germanic
Japanese1
Japanese2 Japanese
Figure 5: Average length of noun-noun com-
pounds in each corpus.
 5
 10
 15  20  25  30
R
at
io
 o
f a
dv
er
bs
 a
t t
he
 e
nd
 (%
)
Ratio of adverbs at the beginning (%) 
Bulgarian
Czech
Polish
Russian
Dutch
German
Norwegian
Swedish
French
ItalianSpanish
English
Japanese1
Japanese2
Italic
Germanic
Slavic
Japanese
Figure 6: Distribution of adverb position.
trigrams. For instance, although Polish-English
is located in the middle of Swedish-English and
Bulgarian-English in the distribution of articles
(in Fig. 3), the ratios tell us that Polish-English is
much nearer to Bulgarian-English.
6 Implications for Work in Related
Domains
Researchers including Wong and Dras (2009),
Wong et al (2011; 2012), and Koppel et al (2005)
work on native language identification and show
that machine learning-based methods are effec-
tive. Wong and Dras (2009) propose using infor-
mation about grammatical errors such as errors in
determiners to achieve better performance while
they show that its use does not improve the perfor-
mance, contrary to the expectation. Related to this,
other researchers (Koppel and Ordan, 2011; van
Halteren, 2008) show that machine learning-based
methods can also predict the source language of
a given translated text although it should be em-
phasized that it is a different task from native lan-
guage identification because translation is not typ-
ically performed by non-native speakers but rather
native speakers of the target language11.
The experimental results show that n-grams
containing articles are predictive for identify-
ing native languages. This indicates that they
should be used in the native language identifi-
cation task. Importantly, all n-grams contain-
ing articles should be used in the classifier unlike
the previous methods that are based only on n-
grams containing article errors. Besides, no ar-
ticles should be explicitly coded in n-grams for
taking the overuse/underuse of articles into con-
sideration. We can achieve this by adding a spe-
cial symbol such as ? to the beginning of each NP
whose head noun is a common noun and that has
no determiner in it as in ?I like ? orange juice.?
In addition, the length of noun-noun com-
pounds and the position of adverbs should also
be considered in native language identification. In
particular, the former can be modeled by the Pois-
son distribution as follows. The Poisson distribu-
tion gives the probability of the number of events
occurring in a fixed time. In our case, the number
of events in a fixed time corresponds to the num-
ber of consecutive repetitions of common nouns in
NPs, which in turn corresponds to the length. To
be precise, the probability of a noun-noun com-
pound with length l is given by
Pr(l) = ?
l
l! e
??, (5)
where ? corresponds to the average length. Fig. 7
shows that the observed values in the French-
English data very closely fit the theoretical proba-
11For comparison, we conducted a pilot study where we
reconstructed a language family tree from English texts
in European Parliament Proceedings Parallel Corpus (Eu-
roparl) (Koehn, 2011). It turned out that the reconstructed
tree was different from the canonical tree (available at http:
//web.hyogo-u.ac.jp/nagata/acl/). However,
we need further investigation to confirm it because each sub-
corpus in Europarl is variable in many dimensions includ-
ing its size and style (e.g., overuse of certain phrases such as
ladies and gentlemen).
1144
 0
 0.5
 1
 0  1  2  3
Pr
ob
ab
ilit
y
Length of noun-noun compound
Theoretical
Observed
Figure 7: Distribution of noun-noun compound
length for French-English.
bilities given by Equation (5)12. This holds for the
other Englishes although we cannot show them be-
cause of the space limitation. Consequently, Equa-
tion (5) should be useful in native language identi-
fication. Fortunately, it can be naturally integrated
into existing classifiers.
In the domain of historical linguistics, re-
searchers have used computational and corpus-
based methods for reconstructing language fam-
ily trees. Some (Enright and Kondrak, 2011;
Gray and Atkinson, 2003; Barbanc?on et al, 2007;
Batagelj et al, 1992; Nakhleh et al, 2005) ap-
ply clustering techniques to the task of language
family tree reconstruction. Others (Kita, 1999;
Rama and Singh, 2009) use corpus statistics for
the same purpose. These methods reconstruct lan-
guage family trees based on linguistic features that
exist within words including lexical, phonological,
and morphological features.
The experimental results in this paper suggest
the possibility of the use of non-native texts for re-
constructing language family trees. It allows us to
use linguistic features that exist between words, as
seen in our methods, which has been difficult with
previous methods. Language involves the features
between words such as phrase construction and
syntax as well as the features within words and
thus they should both be considered in reconstruc-
12The theoretical and observed values are so close that it
is difficult to distinguish between the two lines in Fig. 7. For
example, Pr(l = 1) = 0.0303 while the corresponding ob-
served value is 0.0299.
tion of language family trees.
7 Conclusions
In this paper, we have shown that mother tongue
interference is so strong that the relations be-
tween members of the Indo-European language
family are preserved in English texts written by
Indo-European language speakers. To show this,
we have used clustering to reconstruct a lan-
guage family tree from 11 sets of non-native
English texts. It turned out that the recon-
structed tree correctly groups them into the Italic,
Germanic, and Slavic branches of the Indo-
European family tree. Based on the resulting
trees, we have then hypothesized that the fol-
lowing relation holds in mother tongue interfer-
ence: interfamily distance > non-nativeness >
intrafamily distance. We have further explored
several intriguing linguistic features that play an
important role in mother tongue interference: (i)
article use, (ii) NP construction, and (iii) adverb
position, which provide several insights for im-
proving the tasks of native language identification
and language family tree reconstruction.
Acknowledgments
This work was partly supported by the Digiteo for-
eign guest project. We would like to thank the
three anonymous reviewers and the following per-
sons for their useful comments on this paper: Ko-
taro Funakoshi, Mitsuaki Hayase, Atsuo Kawai,
Robert Ladig, Graham Neubig, Vera Sheinman,
Hiroya Takamura, David Valmorin, Mikko Vile-
nius.
References
Jan Aarts and Sylviane Granger, 1998. Tag sequences
in learner corpora: a key to interlanguage gram-
mar and discourse, pages 132?141. Longman, New
York.
Bengt Altenberg and Marie Tapper, 1998. The use of
adverbial connectors in advanced Swedish learners?
written English, pages 80?93. Longman, New York.
Franc?ois Barbanc?on, Tandy Warnow, Steven N. Evans,
Donald Ringe, and Luay Nakhleh. 2007. An exper-
imental study comparing linguistic phylogenetic re-
construction methods. Statistics Technical Reports,
page 732.
Vladimir Batagelj, Tomaz? Pisanski, and Dami-
jana Kerz?ic?. 1992. Automatic clustering of lan-
guages. Computational Linguistics, 18(3):339?352.
1145
Robert S.P. Beekes. 2011. Comparative Indo-
European Linguistics: An Introduction (2nd ed.).
John Benjamins Publishing Company, Amsterdam.
Martin Chodorow, Michael Gamon, and Joel R.
Tetreault. 2010. The utility of article and prepo-
sition error correction systems for English language
learners: feedback and assessment. Language Test-
ing, 27(3):419?436.
David Crystal. 1997. The Cambridge Encyclopedia of
Language (2nd ed.). Cambridge University Press,
Cambridge.
Niels Davidsen-Nielsen and Peter Harder, 2001.
Speakers of Scandinavian languages: Danish, Nor-
wegian, Swedish, pages 21?36. Cambridge Univer-
sity Press, Cambridge.
Alvar Ellega?rd. 1959. Statistical measurement of lin-
guistic relationship. Language, 35(2):131?156.
Jessica Enright and Grzegorz Kondrak. 2011. The ap-
plication of chordal graphs to inferring phylogenetic
trees of languages. In Proc. of 5th International
Joint Conference on Natural Language Processing,
pages 8?13.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English v2. Presses universitaires de Lou-
vain, Louvain.
Russell D. Gray and Quentin D. Atkinson. 2003.
Language-tree divergence times support the Ana-
tolian theory of Indo-European origin. Nature,
426:435?438.
Jiawei Han and Micheline Kamber. 2006. Data Min-
ing: Concepts and Techniques (2nd Ed.). Morgan
Kaufmann Publishers, San Francisco.
Bing-Hwang Juang and Lawrence R. Rabiner. 1985.
A probabilistic distance measure for hidden Markov
models. AT&T Technical Journal, 64(2):391?408.
Kenji Kita. 1999. Automatic clustering of languages
based on probabilistic models. Journal of Quantita-
tive Linguistics, 6(2):167?171.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Proc. of International Conference on Acoustics,
Speech, and Signal Processing, volume 1, pages
181?184.
Philipp Koehn. 2011. Europarl: A parallel corpus for
statistical machine translation. In Proc. of 10th Ma-
chine Translation Summit, pages 79?86.
Moshe Koppel and Noam Ordan. 2011. Translationese
and its dialects. In Proc. of 49th Annual Meeting
of the Association for Computational Linguistics,
pages 1318?1326.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005. Determining an author?s native language by
mining a text for errors. In Proc. of 11th ACM
SIGKDD International Conference on Knowledge
Discovery in Data Mining, pages 624?628.
Alfred L. Kroeber and Charles D. Chrie?tien. 1937.
Quantitative classification of Indo-European lan-
guages. Language, 13(2):83?103.
Ryo Nagata, Edward Whittaker, and Vera Shein-
man. 2011. Creating a manually error-tagged and
shallow-parsed learner corpus. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 1210?1219.
Luay Nakhleh, Tandy Warnow, Don Ringe, and
Steven N. Evans. 2005. A comparison of phyloge-
netic reconstruction methods on an Indo-European
dataset. Transactions of the Philological Society,
103(2):171?192.
Taraka Rama and Anil Kumar Singh. 2009. From bag
of languages to family trees from noisy corpus. In
Proc. of Recent Advances in Natural Language Pro-
cessing, pages 355?359.
Anna Giacalone Ramat and Paolo Ramat, 2006. The
Indo-European Languages. Routledge, New York.
William Snyder. 1996. The acquisitional role of the
syntax-morphology interface: Morphological com-
pounds and syntactic complex predicates. In Proc.
of Annual Boston University Conference on Lan-
guage Development, volume 2, pages 728?735.
Masatoshi Sugiura, Masumi Narita, Tomomi Ishida,
Tatsuya Sakaue, Remi Murao, and Kyoko Muraki.
2007. A discriminant analysis of non-native speak-
ers and native speakers of English. In Proc. of Cor-
pus Linguistics Conference CL2007, pages 84?89.
Michael Swan and Bernard Smith. 2001. Learner En-
glish (2nd Ed.). Cambridge University Press, Cam-
bridge.
Hans van Halteren. 2008. Source language markers
in EUROPARL translations. In Proc. of 22nd Inter-
national Conference on Computational Linguistics,
pages 937?944.
Sze-Meng J. Wong and Mark Dras. 2009. Con-
trastive analysis and native language identification.
In Proc. Australasian Language Technology Work-
shop, pages 53?61.
Sze-Meng J. Wong, Mark Dras, and Mark Johnson.
2011. Exploiting parse structures for native lan-
guage identification. In Proc. Conference on Em-
pirical Methods in Natural Language Processing,
pages 1600?1611.
Sze-Meng J. Wong, Mark Dras, and Mark Johnson.
2012. Exploring adaptor grammars for native lan-
guage identification. In Proc. Joint Conference on
1146
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 699?709.
1147
