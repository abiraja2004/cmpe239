Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 37?46,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
An Analysis of Memory-based Processing Costs using Incremental
Deep Syntactic Dependency Parsing?
Marten van Schijndel
The Ohio State University
vanschm@ling.osu.edu
Luan Nguyen
University of Minnesota
lnguyen@cs.umn.edu
William Schuler
The Ohio State University
schuler@ling.osu.edu
Abstract
Reading experiments using naturalistic
stimuli have shown unanticipated facili-
tations for completing center embeddings
when frequency effects are factored out.
To eliminate possible confounds due to
surface structure, this paper introduces a
processing model based on deep syntac-
tic dependencies. Results on eye-tracking
data indicate that completing deep syntac-
tic embeddings yields significantly more
facilitation than completing surface em-
beddings.
1 Introduction
Self-paced reading and eye-tracking experiments
have often been used to support theories about
inhibitory effects of working memory operations
in sentence processing (Just and Carpenter, 1992;
Gibson, 2000; Lewis and Vasishth, 2005), but it
is possible that many of these effects can be ex-
plained by frequency (Jurafsky, 1996; Hale, 2001;
Karlsson, 2007). Experiments on large naturalis-
tic text corpora (Demberg and Keller, 2008; Wu et
al., 2010; van Schijndel and Schuler, 2013) have
shown significant memory effects at the ends of
center embeddings when frequency measures have
been included as separate factors, but these mem-
ory effects have been facilitatory rather than in-
hibitory.
Some of the memory-based measures that pro-
duce these facilitatory effects (Wu et al, 2010; van
Schijndel and Schuler, 2013) are defined in terms
of initiation and integration of connected compo-
nents of syntactic structure,1 with the presumption
?*Thanks to Micha Elsner and three anonymous review-
ers for their feedback. This work was funded by an Ohio State
University Department of Linguistics Targeted Investment
for Excellence (TIE) grant for collaborative interdisciplinary
projects conducted during the academic year 2012?13.
1Graph theoretically, the set of connected components
that referents that belong to the same connected
component may cue one another using content-
based features, while those that do not must rely
on noisier temporal features that just encode how
recently a referent was accessed. These measures,
based on left-corner parsing processes (Johnson-
Laird, 1983; Abney and Johnson, 1991), abstract
counts of unsatisfied dependencies from noun or
verb referents (Gibson, 2000) to cover all syntactic
dependencies, motivated by observations of Dem-
berg and Keller (2008) and Kwon et al (2010) of
the inadequacies of Gibson?s narrower measure.
But these experiments use naturalistic stimuli
without constrained manipulations and therefore
might be susceptible to confounds. It is possible
that the purely phrase-structure-based connected
components used previously may ignore some in-
tegration costs associated with filler-gap construc-
tions, making them an unsuitable generalization of
Gibson-style dependencies. It is also possible that
the facilitatory effect for integration operations in
naturally-occurring stimuli may be driven by syn-
tactic center embeddings that arise from modifiers
(e.g. The CEO sold [[the shares] of the com-
pany]), which do not require any dependencies
to be deferred, but which might be systematically
under-predicted by frequency measures, produc-
ing a confound with memory measures when fre-
quency measures are residualized out.
In order to eliminate possible confounds due to
exclusion of unbounded dependencies in filler-gap
constructions, this paper evaluates a processing
model that calculates connected components on
deep syntactic dependency structures rather than
surface phrase structure trees. This model ac-
counts unattached fillers and gaps as belonging
to separate connected components, and therefore
performs additional initiation and integration op-
of a graph ?V, E? is the set of maximal subsets of
it {?V1, E1?, ?V2, E2?, ...} such that any pair of vertices in
each Vi can be connected by edges in the corresponding Ei.
37
a) Noun Phrase
Relative Clause
Sentence w. Gap
Verb Phrase w. Gap
Sentence w. Gap
millionsstole
say
officials
who
Noun Phrase
personthe
b)
i1
i2 i3
i4
i5
i6
i7
1
2
1
2
1
1
say
officials
stole
who
person
millionsthe
0
0
0
0
0
00
Figure 1: Graphical representation of (a) a single
connected component of surface syntactic phrase
structure corresponding to (b) two connected com-
ponents of deep syntactic dependency structure for
the noun phrase the person who officials say stole
millions, prior to the word say. Connections es-
tablished prior to the word say are shown in black;
subsequent connections are shown in gray.
erations in filler-gap constructions as hypothesized
by Gibson (2000) and others. Then, in order to
control for possible confounds due to modifier-
induced center embedding, this refined model is
applied to two partitions of an eye-tracking cor-
pus (Kennedy et al, 2003): one consisting of sen-
tences containing only non-modifier center em-
beddings, in which dependencies are deferred, and
the other consisting of sentences containing no
center embeddings or containing center embed-
dings arising from attachment of final modifiers,
in which no dependencies are deferred. Processing
this partitioned corpus with deep syntactic con-
nected components reveals a significant increase
in facilitation in the non-modifier partition, which
lends credibility to the observation of negative
integration cost in processing naturally-occurring
sentences.
2 Connected Components
The experiments described in this paper evalu-
ate whether inhibition and facilitation in reading
correlate with operations in a hierarchic sequen-
tial prediction model that initiate and integrate
connected components of hypothesized syntactic
structure during incremental parsing. The model
used in these experiments refines previous con-
nected component models by allowing fillers and
gaps to occur in separate connected components
of a deep syntactic dependency graph (Mel?c?uk,
1988; Kintsch, 1988), even when they belong to
the same connected component when defined on
surface structure.
For example, the surface syntactic phrase struc-
ture and deep syntactic dependency structure for
the noun phrase the person who officials say stole
millions are shown in Figure 1.2 Notice that af-
ter the word officials, there is only one connected
component of surface syntactic phrase structure
(from the root noun phrase to the verb phrase with
gap), but two disjoint connected components of
deep syntactic dependency structure (one ending
at i3, and another at i5). Only the deep syntactic
dependency structure corresponds to familiar (Just
and Carpenter, 1992; Gibson, 1998) notions of
how memory is used to store deferred dependen-
cies in filler-gap constructions. The next section
will describe a generalized categorial grammar,
which (i) can be viewed as context-free, to seed a
latent-variable probabilistic context-free grammar
to accurately derive parses of filler-gap construc-
tions, and (ii) can be viewed as a deep syntactic
dependency grammar, defining dependencies for
connected components in terms of function appli-
cations.
3 Generalized Categorial Grammar
In order to evaluate memory effects for hypothe-
sizing unbounded dependencies between referents
of fillers and referents of clauses containing gaps,
a memory-based processor must define connected
components in terms of deep syntactic dependen-
cies (including unbounded dependencies) rather
than in terms of surface syntactic phrase structure
trees. To do this, at least some phrase structure
edges must be removed from the set of connec-
tions that define a connected component.
Because these unbounded dependencies are not
represented locally in the original Treebank for-
mat, probabilities for operations on these modified
2Following Mel?c?uk (1988) and Kintsch (1988),
the graphical dependency structure adopted here uses
positionally-defined labels (?0? for the predicate label, ?1?
for the first argument ahead of a predicate, ?2? for the last
argument behind, etc.) but includes unbounded dependen-
cies between referents of fillers and referents of clauses
containing gaps. It is assumed that semantically-labeled
structures would be isomorphic to the structures defined
here, but would generalize across alternations such as active
and passive constructions, for example.
38
connected components are trained on a corpus an-
notated with generalized categorial grammar de-
pendencies for ?gap? arguments at all categories
that subsume a gap (Nguyen et al, 2012). This
representation is similar to the HPSG-like repre-
sentation used by Hale (2001) and Lewis and Va-
sishth (2005), but has a naturally-defined depen-
dency structure on which to calculate connected
components. This generalized categorial grammar
is then used to identify the first sign that introduces
a gap, at which point a deep syntactic connected
component containing the filler can be encoded
(stored), and a separate deep syntactic connected
component for a clause containing a gap can be
initiated.
A generalized categorial grammar (Bach, 1981)
consists of a set U of primitive category types;
a set O of type-constructing operators allowing a
recursive definition of a set of categories C =def
U ? (C ? O ? C); a set X of vocabulary items;
a mapping M from vocabulary items in X to se-
mantic functions with category types in C; and
a set R of inference rules for deriving functions
with category types inC from other functions with
category types in C. Nguyen et al (2012) use
primitive category types for clause types (e.g. V
for finite verb-headed clause, N for noun phrase
or nominal clause, D for determiners and pos-
sessive clauses, etc.), and use the generalized set
of type-constructing operators to characterize not
only function application dependencies between
arguments immediately ahead of and behind a
functor (-a and -b, corresponding to ?\? and ?/? in
Ajdukiewicz-Bar-Hillel categorial grammars), but
also long-distance dependencies between fillers
and categories subsuming gaps (-g), dependencies
between relative pronouns and antecedent modif-
icands of relative clauses (-r), and dependencies
between interrogative pronouns and their argu-
ments (-i), which remain unsatisfied in derivations
but function to distinguish categories for content
and polar questions. A lexicon can then be de-
fined in M to introduce lexical dependencies and
obligatory pronominal dependencies using num-
bered functions for predicates and deep syntactic
arguments, for example:
the ? (?i (0 i)=the) : D
person ? (?i (0 i)=person) : N-aD
who ? (?k i (0 i)=who ? (1 i)=k) : N-rN
officials ? (?i (0 i)=officials) : N
the
D
person
N-aD
N Aa
who
N-rN
officials
N
say
V-aN-bV
stole
V-aN-bN
millions
N
V-aN Ae
V-gN Ga
V-aN-gN
Ag
V-gN Ac
V-rN Fc
N R
Figure 2: Example categorization of the noun
phrase the person who officials say stole millions.
say ? (?i (0 i)=say) : V-aN-bV
stole ? (?i (0 i)=stole) : V-aN-bN
millions ? (?i (0 i)=millions) : N
Inference rules in R are then defined to com-
pose arguments and modifiers and propagate gaps.
Arguments g of type d ahead of functors h of
type c-ad are composed by passing non-local de-
pendencies ? ? {-g, -i, -r} ? C from premises to
conclusion in all combinations:
g:d h: c-ad ? ( fc-ad g h): c (Aa)
g:d? h: c-ad ? ?k ( fc-ad (g k) h): c? (Ab)
g:d h: c-ad? ? ?k ( fc-ad g (h k)): c? (Ac)
g:d? h: c-ad? ? ?k ( fc-ad (g k) (h k)): c? (Ad)
Similar rules compose arguments behind functors:
g: c-bd h:d ? ( fc-bd g h): c (Ae)
g: c-bd? h:d ? ?k ( fc-bd (g k) h): c? (Af)
g: c-bd h:d? ? ?k ( fc-bd g (h k)): c? (Ag)
g: c-bd? h:d? ? ?k ( fc-bd (g k) (h k)): c? (Ah)
These rules use composition functions fc-ad
and fc-bd for initial and final arguments, which de-
fine dependency edges numbered v from referents
of predicate functors i to referents of arguments j,
where v is the number of unsatisfied arguments
?1...?v ? {-a, -b} ?C in a category label:
fu?1..v?1-ac
def= ?g h i ? j (v i)= j ? (g j) ? (h i) (1a)
fu?1..v?1-bc
def= ?g h i ? j (v i)= j ? (g i) ? (h j) (1b)
R also contains inference rules to compose mod-
ifier functors g of type u-ad ahead of modifi-
cands h of type d:
g: u-ad h:c ? ( fIM g h):c (Ma)
g: u-ad? h:c ? ?k ( fIM (g k) h):c? (Mb)
g: u-ad h:c? ? ?k ( fIM g (h k)):c? (Mc)
39
?i1 j1.. i? j? ... ? (g?:c/d { j?} i?) xt
?i1 j1.. i? ... ? ((g? f ):c i?)
xt ? f :d (?Fa)
?i1 j1.. i? j? ... ? (g?:c/d { j?} i?) xt
?i1 j1.. i? j?i?+1 ... ? (g?:c/d { j?} i?) ? ( f :e i?+1)
xt ? f :e (+Fa)
?i1 j1.. i??1 j??1i? ... ? (g?:d i?)
?i1 j1.. i? j? ... ? (( f g?):c/e { j?} i?)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
g:d h:e ? ( f g h):c or
g:d h:e ? ?k( f (g k) h):c or
g:d h:e ? ?k( f g (h k)):c or
g:d h:e ? ?k( f (g k) (h k)):c
(?La)
?i1 j1.. i??1 j??1i? ... ? (g??1:a/c { j??1} i??1) ? (g?:d i?)
?i1 j1.. i??1 j??1 ... ? (g??1 ? ( f g?):a/e { j??1} i??1)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
g:d h:e ? ( f g h):c or
g:d h:e ? ?k( f (g k) h):c or
g:d h:e ? ?k( f g (h k)):c or
g:d h:e ? ?k( f (g k) (h k)):c
(+La)
Figure 3: Basic processing productions of a right-corner parser.
g: u-ad? h:c? ? ?k ( fIM (g k) (h k)):c? (Md)
or for modifier functors behind a modificand:
g:c h: u-ad ? ( fFM g h):c (Me)
g:c? h: u-ad ? ?k ( fFM (g k) h):c? (Mf)
g:c h: u-ad? ? ?k ( fFM g (h k)):c? (Mg)
g:c? h: u-ad? ? ?k ( fFM (g k) (h k)):c? (Mh)
These rules use composition functions fIM and fFM
for initial and final modifiers, which define depen-
dency edges numbered ?1? from referents of mod-
ifier functors i to referents of modificands j:
fIM
def= ?g h j ?i (1 i)= j ? (g i) ? (h j) (2a)
fFM
def= ?g h j ?i (1 i)= j ? (g j) ? (h i) (2b)
R also contains inference rules for hypothesiz-
ing gaps -gd for arguments and modifiers:3
g: c-ad ? ?k ( fc-ad {k} g): c-gd (Ga)
g: c-bd ? ?k ( fc-ad {k} g): c-gd (Gb)
g:c ? ?k ( fIM {k} g):c-gd (Gc)
and for attaching fillers e, d-re, d-ie as gaps -gd:
g:e h: c-gd ? ?i ? j (g i) ? (h i j):e (Fa)
g:d-re h: c-gd ? ?k j ?i (g k i) ? (h i j): c-re (Fb)
g:d-ie h: c-gd ? ?k j ?i (g k i) ? (h i j): c-ie (Fc)
3Since these unary inferences perform no explicit compo-
sition, they are defined to use only initial versions composi-
tion functions fc-ad and fIM.
and for attaching modificands as antecedents of
relative pronouns:
g:e h:c-rd ? ?i ? j (g i) ? (h i j):e (R)
An example derivation of the noun phrase the per-
son who officials say stole millions using these
rules is shown in Figure 2. The semantic expres-
sion produced by this derivation consists of a con-
junction of terms defining the edges in the graph
shown in Figure 1b.
This GCG formulation captures many of the in-
sights of the HPSG-like context-free filler-gap no-
tation used by Hale (2001) or Lewis and Vasishth
(2005): inference rules with adjacent premises can
be cast as context-free grammars and weighted us-
ing probabilities, which allow experiments to cal-
culate frequency measures for syntactic construc-
tions. Applying a latent variable PCFG trainer
(Petrov et al, 2006) to this formulation was shown
to yield state-of-the-art accuracy for recovery of
unbounded dependencies (Nguyen et al, 2012).
Moreover, the functor-argument dependencies in
a GCG define deep syntactic dependency graphs
for all derivations, which can be used in incremen-
tal parsing to calculate connected components for
memory-based measures.
4 Incremental Processing
In order to obtain measures of memory opera-
tions used in incremental processing, these GCG
inference rules are combined into a set of parser
40
?i1 j1.. in jn.. i? j? ... ? (gn:y/z? { jn} in) ? ... ? (g?:c/d { j?} i?) xt
?i1 j1.. in jn.. i? ... ? (gn:y/z? { jn} in) ? ... ? ((g?( f ?{ jn} f )):c i?)
xt ? ?k( f ?{k} f ):d
(?Fb)
?i1 j1.. in jn.. i? j? ... ? (gn:y/z? { jn} in) ? ... ? (g?:c/d { j?} i?) xt
?i1 j1.. in jn.. i? j?i?+1 ... ? (gn:y/z? { jn} in) ? ... ? (g?:c/d { j?} i?) ? (( f ?{ jn} f ):e i?+1)
xt ? ?k( f ?{k} f ):e
(+Fb)
?i1 j1.. in jn.. i??1 j??1i? ... ? (gn:y/z? { jn} in) ? ... ? (g?:d i?)
?i1 j1.. in jn.. i? j? ... ? (gn:y/z? { jn} in) ? ... ? (( f g?) ? ( f ?{ jn}):c?/e { j?} i?)
g:d h:e ? ?k( f g ( f ?{k} h)):c? (?Lb)
?i1 j1.. in jn.. i??1 j??1i? ... ? (gn:y/z? { jn} in) ? ... ? (g??1:a/c? { j??1} i??1) ? (g?:d i?)
?i1 j1.. in jn.. i??1 j??1 ... ? (gn:y/z? { jn} in) ? ... ? (g??1 ? ( f g?) ? ( f ?{ jn}):a/e { j??1} i??1)
g:d h:e ? ?k( f g ( f ?{k} h)):c? (+Lb)
Figure 4: Additional processing productions for attaching a referent of a filler jn as the referent of a gap.
productions, similar to those of the ?right corner?
parser of van Schijndel and Schuler (2013), ex-
cept that instead of recognizing shallow hierarchi-
cal sequences of connected components of surface
structure, the parser recognizes shallow hierarchi-
cal sequences of connected components of deep
syntactic dependencies. This parser exploits the
observation (van Schijndel et al, in press) that left-
corner parsers and their variants do not need to ini-
tiate or integrate more than one connected compo-
nent at each word. These two operations are then
augmented with rules to introduce fillers and at-
tach fillers as gaps.
This parser is defined on incomplete connected
component states which consist of an active sign
(with a semantic referent and syntactic form or
category) lacking an awaited sign (also with a ref-
erent and category) yet to come. Semantic func-
tions of active and awaited signs are simplified to
denote only sets of referents, with gap arguments
(?k) stripped off and handled by separate con-
nected components. Incomplete connected com-
ponents, therefore, always denote semantic func-
tions from sets of referents to sets of referents.
This paper will notate semantic functions of
connected components using variables g and h, in-
complete connected component categories as c/d
(consisting of an active sign of category c and an
awaited sign of category d), and associations be-
tween them as g:c/d. The semantic representa-
tion used here is simply a deep syntactic depen-
dency structure, so a connected component func-
tion is satisfied if it holds for some output ref-
erent i given input referent j. This can be no-
tated ?i j (g:c/d { j} i), where the set { j} is equiva-
lent to (? j? j?= j). Connected component functions
that have a common referent j can then be com-
posed into larger connected components:4
?i jk (g { j} i) ? (h {k} j) ? ?i j (g?h {k} i) (3)
Hierarchies of ? connected compo-
nents can be represented as conjunctions:
?i1 j1... i? j? (g1:c1/d1 { j1} i1) ? ... ? (g?:c?/d? { j?} i?).
This allows constraints such as unbounded depen-
dencies between referents of fillers and referents
of clauses containing gaps to be specified across
connected components by simply plugging vari-
ables for filler referents into argument positions
for gaps.
A nondeterministic incremental parser can now
be defined as a deductive system, given an input
sequence consisting of an initial connected com-
ponent state of category T/T, corresponding to an
existing discourse context, followed by a sequence
of observations x1, x2, . . . , processed in time order.
As each xt is encountered, it is connected to an ex-
isting connected component or it introduces a new
disjoint component using the productions shown
in Figures 3, 4, and 5.
4These are connected components of dependency struc-
ture resulting from one or more composition functions being
composed, with each function?s output as the previous func-
tion?s second argument. This uses a standard definition of
function composition: (( f ? g) x) = ( f (g x)).
41
?i1 j1.. i??1 j??1i? ... ? (g?:d i?)
?i1 j1.. i? j? ... ? (( f g?) ? (?h k i (h k)):a/e? { j?} i?)
g:d h:e? ? ( f g h):c (?Lc)
?i1 j1.. i??1 j??1i? ... ? (g??1:a/c { j??1} i??1) ? (g?:d i?)
?i1 j1.. i??1 j??1 ... ? (g??1 ? ( f g?) ? (?h k i (h k)):a/e? { j??1} i??1)
g:d h:e? ? ( f g h):c (+Lc)
?i1 j1.. i? j? ... ? (g??1:c/d? { j??1} i??1) ? (g?:d?/e { j?} i?)
?i1 j1.. i??1 j??1 ... ? (g??1 ? (?h i? j(h j)) ? g?:c/e { j??1} i??1)
(+N)
Figure 5: Additional processing productions for hypothesizing filler-gap attachment.
Operations on dependencies that can be derived
from surface structure (see Figure 3) are taken
directly from van Schijndel and Schuler (2013).
First, if an observation xt can immediately fill
the awaited sign of the last connected component
g?:c/d, it is hypothesized to do so, turning this
incomplete connected component into a complete
connected component (g? f ):c (Production ?Fa); or
if the observation can serve as an initial sub-sign
of this awaited sign, it is hypothesized to form a
new complete sign f :e in a new component with xt
as its first observation (Production +Fa). Then,
if either of these resulting complete signs g?:d
can immediately attach as an initial child of the
awaited sign of the most recent connected com-
ponent g??1:a/c, it is hypothesized to merge and
extend this connected component, with xt as the
last observation of the completed connected com-
ponent (Production +La); or if it can serve as an
initial sub-sign of this awaited sign, it is hypoth-
esized to remain disjoint and form its own con-
nected component (Production ?La). The side
conditions of La productions are defined to unpack
gap propagation (instances of ?k that distinguish
rules Aa?h and Ma?h) from the inference rules
in Section 3, because this functionality will be re-
placed with direct substitution of referent variables
into subordinate semantic functions, below.
The Nguyen et al (2012) GCG was defined
to pass up unbounded dependencies, but in in-
cremental deep syntactic dependency processing,
unbounded dependencies are accounted as sepa-
rate connected components. When hypothesizing
an unbounded dependency, the processing model
simply cues the active sign of a previous connected
component containing a filler without completing
the current connected component. The four +F,
?F, +L, and ?L operations are therefore combined
with applications of unary rules Ga?c for hypoth-
esizing referents as fillers for gaps (providing f ?
in the equations in Figure 4). Productions ?Fb
and +Fb fill gaps in initial children, and Produc-
tions ?Lb and +Lb fill gaps in final children. Note
that the Fb and Lb productions apply to the same
types of antecedents as Fa and La productions re-
spectively, so members of these two sets of pro-
ductions cannot be applied together.
Applications of rules Fa?c and R for introduc-
ing fillers are applied to store fillers as existentially
quantified variable values in Lc productions (see
Figure 5). These Lc productions apply to the same
type of antecedent as La and Lb productions, so
these also cannot be applied together.
Finally, connected components separated by
gaps which are no longer hypothesized (?) are
reattached by a +N production. This +N pro-
duction may then be paired with a ?N production
which yields its antecedent unchanged as a conse-
quent. These N productions apply to antecedents
and consequents of the same type, so they may be
applied together with one F and one L production,
but since the +N production removes in its conse-
quent a ? argument required in its antecedent, it
may not apply more than once in succession (and
applying the ?N production more than once in suc-
cession has no effect).
An incremental derivation of the noun phrase
the person who officials say stole millions, using
these productions, is shown in Figure 6.
5 Evaluation
The F, L, and N productions defined in the pre-
vious section can be made probabilistic by first
computing a probabilistic context-free grammar
(PCFG) from a tree-annotated corpus, then trans-
forming that PCFG model into a model of prob-
abilities over incremental parsing operations us-
ing a grammar transform (Schuler, 2009). This
allows the intermediate PCFG to be optimized us-
ing an existing PCFG-based latent variable trainer
42
?i0 (.. :T/T {i0} i0) the
?i0 i2 (.. :T/T {i0} i0) ? (.. :N/N-aD {i2} i2)
+Fa,?La,?N
person
?i0 i2 (.. :T/T {i0} i0) ? (.. :N/V-rN {i2} i2)
?Fa,?La,?N
who
?i0 i2 i3 (.. :T/T {i0} i0) ? (.. :N/V-gN {i3} i2)
+Fa,+Lc,?N
officials
?i0 i2 i3 i5 (.. :T/T {i0} i0) ? (.. :N/V-gN {i3} i2) ? (.. :V-gN/V-aN-gN {i5} i5)
+Fa,?La,?N
say
?i0 i2 i6 (.. :T/T {i0} i0) ? (.. :N/V-aN {i6} i2)
+Fb,+La,+N
stole
?i0 i2 i7 (.. :T/T {i0} i0) ? (.. :N/N {i7} i2)
+Fa,+La,?N
millions
?i0 (.. :T/T {i0} i0)
?Fa,+La,?N
Figure 6: Derivation of the person who officials say stole millions, showing connected components with
unique referent variables (calculated according to the equations in Section 4). Semantic functions are
abbreviated to ?..? for readability. This derivation yields the following lexical relations: (0 i1)=the,
(0 i2)=person, (0 i3)=who, (0 i4)=officials, (0 i5)=say, (0 i6)=stole, (0 i7)=millions, and the following
argument relations: (1 i2)=i1, (1 i3)=i2, (1 i5)=i4, (2 i5)=i6, (1 i6)=i3, (2 i6)=i7.
(Petrov et al, 2006). When applied to the output
of this trainer, this transform has been shown to
produce comparable accuracy to that of the origi-
nal Petrov et al (2006) CKY parser (van Schijn-
del et al, 2012). The transform used in these ex-
periments diverges from that of Schuler (2009), in
that the probability associated with introducing a
gap in a filler-gap construction is reallocated from
a ?F?L operation to a +F?L operation (to encode
the previously most subordinate connected com-
ponent with the filler as its awaited sign and be-
gin a new disjoint connected component), and the
probability associated with resolving such a gap is
reallocated from an implicit ?N operation to a +N
operation (to integrate the connected component
containing the gap with that containing the filler).
In order to verify that the modifications to the
transform correctly reallocate probability mass for
gap operations, the goodness of fit to reading
times of a model using this modified transform
is compared against the publicly-available base-
line model from van Schijndel and Schuler (2013),
which uses the original Schuler (2009) transform.5
To ensure a valid comparison, both parsers are
trained on a GCG-reannotated version of the Wall
Street Journal portion of the Penn Treebank (Mar-
cus et al, 1993) before being fit to reading times
using linear mixed-effects models (Baayen et al,
2008).6 This evaluation focuses on the process-
ing that can be done up to a given point in a sen-
tence. In human subjects, this processing includes
both immediate lexical access and regressions that
5The models used here also use random slopes to reduce
their variance, which makes them less anticonservative.
6The models are built using lmer from the lme4R package
(Bates et al, 2011; R Development Core Team, 2010).
aid in the integration of new information, so the
reading times of interest in this evaluation are log-
transformed go-past durations.7
The first and last word of each line in the
Dundee corpus, words not observed at least 5
times in the WSJ training corpus, and fixations af-
ter long saccades (>4 words) are omitted from the
evaluation to filter out wrap-up effects, parser in-
accuracies, and inattention and track loss of the
eyetracker. The following predictors are centered
and used in each baseline model: sentence posi-
tion, word length, whether or not the previous or
next word were fixated upon, and unigram and bi-
gram probabilities.8 Then each of the following
predictors is residualized off each baseline before
being centered and added to it to help residualize
the next factor: length of the go-past region, cumu-
lative total surprisal, total surprisal (Hale, 2001),
and cumulative entropy reduction (Hale, 2003).9
All 2-way interactions between these effects are
7Go-past durations are calculated by summing all fixa-
tions in a region of text, including regressions, until a new
region is fixated, which accounts for additional processing
that may take place after initial lexical access, but before the
next region is processed. For example, if one region ends at
word 5 in a sentence, and the next fixation lands on word 8,
then the go-past region consists of words 6-8 while go-past
duration sums all fixations until a fixation occurs after word
8. Log-transforming eye movements and fixations may make
their distributions more normal (Stephen and Mirman, 2010)
and does not substantially affect the results of this paper.
8For the n-gram model, this study uses the Brown corpus
(Francis and Kucera, 1979), the WSJ Sections 02-21 (Mar-
cus et al, 1993), the written portion of the British National
Corpus (BNC Consortium, 2007), and the Dundee corpus
(Kennedy et al, 2003) smoothed with modified Kneser-Ney
(Chen and Goodman, 1998) in SRILM (Stolcke, 2002).
9Non-cumulative metrics are calculated from the final
word of the go-past region; cumulative metrics are summed
over the go-past region.
43
included as predictors along with the predictors
from the previous go-past region (to account for
spillover effects). Finally, each model has sub-
ject and item random intercepts added in addition
to by-subject random slopes (cumulative total sur-
prisal, whether the previous word was fixated, and
length of the go-past region) and is fit to centered
log-transformed go-past durations.10
The Akaike Information Criterion (AIC)
indicates that the gap-reallocating model
(AIC = 128,605) provides a better fit to reading
times than the original model (AIC = 128,619).11
As described in Section 1, previous findings of
negative integration cost may be due to a confound
whereby center-embedded constructions caused
by modifiers, which do not require deep syntac-
tic dependencies to be deferred, may be driving
the effect. Under this hypothesis, embeddings
that do not arise from final adjunction of mod-
ifiers (henceforth canonical embeddings) should
yield a positive integration cost as found by Gib-
son (2000).
To investigate this potential confound, the
Dundee corpus is partitioned into two parts. First,
the model described in this paper is used to anno-
tate the Dundee corpus. From this annotated cor-
pus, all sentences are collected that contain canon-
ical embeddings and lack modifier-induced em-
beddings.12 This produces two corpora: one con-
sisting entirely of canonical center-embeddings
such as those used in self-paced reading exper-
iments with findings of positive integration cost
(e.g. Gibson 2000), the other consisting of the
remainder of the Dundee corpus, which contains
sentences with canonical embeddings but also in-
cludes modifier-caused embeddings.
The coefficient estimates for integration oper-
ations (?F+L and +N) on each of these corpora
are then calculated using the baseline described
above. To ensure embeddings are driving any ob-
served effect rather than sentence wrap-up effects,
the first and last words of each sentence are ex-
cluded from both data sets. Integration cost is
measured by the amount of probability mass the
parser allocates to ?F+L and +N operations, accu-
10Each fixed effect that has an absolute t-value greater than
10 when included in a random-intercepts only model is added
as a random slope by-subject.
11The relative likelihood of the original model to the gap-
sensitive model is 0.0009 (n = 151,331), which suggests the
improvement is significant.
12Modifier-induced embeddings are found by looking for
embeddings that arise from inference rules Ma-h in Section 3.
Model coeff std err t-score
Canonical -0.040 0.010 -4.05
Other -0.017 0.004 -4.20
Table 1: Fixed effect estimates for integration cost
when used to fit reading times over two partitions
of the Dundee corpus: one containing only canon-
ical center embeddings and the other composed of
the rest of the sentences in the corpus.
mulated over each go-past region, and this cost is
added as a fixed effect and as a random slope by
subject to the mixed model described earlier.13
The fixed effect estimate for cumulative inte-
gration cost from fitting each corpus is shown
in Table 1. Application of Welch?s t-test shows
that the difference between the estimated distri-
butions of these two parameters is highly signif-
icant (p < 0.0001).14 The strong negative corre-
lation of integration cost to reading times in the
purely canonical corpus suggests canonical (non-
modifier) integrations contribute to the finding of
negative integration cost.
6 Conclusion
This paper has introduced an incremental parser
capable of using GCG dependencies to distinguish
between surface syntactic embeddings and deep
syntactic embeddings. This parser was shown to
obtain a better fit to reading times than a surface-
syntactic parser and was used to parse the Dundee
eye-tracking corpus in two partitions: one consist-
ing of canonical embeddings that require deferred
dependencies and the other consisting of sentences
containing no center embeddings or center em-
beddings arising from the attachment of clause-
final modifiers, in which no dependencies are de-
ferred. Using linear mixed effects models, com-
pletion (integration) of canonical center embed-
dings was found to be significantly more nega-
tively correlated with reading times than comple-
tion of non-canonical embeddings. These results
suggest that the negative integration cost observed
in eye-tracking studies is at least partially due to
deep syntactic dependencies and not due to con-
founds related to surface forms.
13Integration cost is residualized off the baseline before be-
ing centered and added as a fixed effect.
14Integration cost is significant as a fixed effect (p = 0.001)
in both partitions: canonical (n = 16,174 durations) and
non-canonical (n = 131,297 durations).
44
References
Steven P. Abney and Mark Johnson. 1991. Memory
requirements and local ambiguities of parsing strate-
gies. J. Psycholinguistic Research, 20(3):233?250.
R. Harald Baayen, D. J. Davidson, and Douglas M.
Bates. 2008. Mixed-effects modeling with crossed
random effects for subjects and items. Journal of
Memory and Language, 59:390?412.
Emmon Bach. 1981. Discontinuous constituents in
generalized categorial grammars. Proceedings of
the Annual Meeting of the Northeast Linguistic So-
ciety (NELS), 11:1?12.
Douglas Bates, Martin Maechler, and Ben Bolker,
2011. lme4: Linear mixed-effects models using S4
classes.
BNC Consortium. 2007. The british national corpus.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical report, Harvard University.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193?210.
W. Nelson Francis and Henry Kucera. 1979. The
brown corpus: A standard corpus of present-day
edited american english.
Edward Gibson. 1998. Linguistic complexity: Local-
ity of syntactic dependencies. Cognition, 68(1):1?
76.
Edward Gibson. 2000. The dependency locality the-
ory: A distance-based theory of linguistic complex-
ity. In Image, language, brain: Papers from the first
mind articulation project symposium, pages 95?126,
Cambridge, MA. MIT Press.
John Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Proceedings of the sec-
ond meeting of the North American chapter of the
Association for Computational Linguistics, pages
159?166, Pittsburgh, PA.
John Hale. 2003. Grammar, Uncertainty and Sentence
Processing. Ph.D. thesis, Cognitive Science, The
Johns Hopkins University.
Philip N. Johnson-Laird. 1983. Mental models: to-
wards a cognitive science of language, inference,
and consciousness. Harvard University Press, Cam-
bridge, MA, USA.
Daniel Jurafsky. 1996. A probabilistic model of lexical
and syntactic access and disambiguation. Cognitive
Science: A Multidisciplinary Journal, 20(2):137?
194.
Marcel Adam Just and Patricia A. Carpenter. 1992. A
capacity theory of comprehension: Individual differ-
ences in working memory. Psychological Review,
99:122?149.
Fred Karlsson. 2007. Constraints on multiple center-
embedding of clauses. Journal of Linguistics,
43:365?392.
Alan Kennedy, James Pynte, and Robin Hill. 2003.
The Dundee corpus. In Proceedings of the 12th Eu-
ropean conference on eye movement.
Walter Kintsch. 1988. The role of knowledge in dis-
course comprehension: A construction-integration
model. Psychological review, 95(2):163?182.
Nayoung Kwon, Yoonhyoung Lee, Peter C. Gordon,
Robert Kluender, and Maria Polinsky. 2010. Cog-
nitive and linguistic factors affecting subject/object
asymmetry: An eye-tracking study of pre-nominal
relative clauses in korean. Language, 86(3):561.
Richard L. Lewis and Shravan Vasishth. 2005.
An activation-based model of sentence processing
as skilled memory retrieval. Cognitive Science,
29(3):375?419.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Igor Mel?c?uk. 1988. Dependency syntax: theory and
practice. State University of NY Press, Albany.
Luan Nguyen, Marten van Schijndel, and William
Schuler. 2012. Accurate unbounded dependency
recovery using generalized categorial grammars. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING ?12), Mumbai,
India.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
44th Annual Meeting of the Association for Compu-
tational Linguistics (COLING/ACL?06).
R Development Core Team, 2010. R: A Language and
Environment for Statistical Computing. R Foun-
dation for Statistical Computing, Vienna, Austria.
ISBN 3-900051-07-0.
William Schuler. 2009. Parsing with a bounded stack
using a model-based right-corner transform. In Pro-
ceedings of NAACL/HLT 2009, NAACL ?09, pages
344?352, Boulder, Colorado. Association for Com-
putational Linguistics.
Damian G. Stephen and Daniel Mirman. 2010. Inter-
actions dominate the dynamics of visual cognition.
Cognition, 115(1):154?165.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Seventh International Confer-
ence on Spoken Language Processing.
Marten van Schijndel and William Schuler. 2013. An
analysis of frequency- and recency-based processing
costs. In Proceedings of NAACL-HLT 2013. Associ-
ation for Computational Linguistics.
45
Marten van Schijndel, Andy Exley, and William
Schuler. 2012. Connectionist-inspired incremental
PCFG parsing. In Proceedings of CMCL 2012. As-
sociation for Computational Linguistics.
Marten van Schijndel, Andy Exley, and William
Schuler. in press. A model of language processing
as hierarchic sequential prediction. Topics in Cogni-
tive Science.
Stephen Wu, Asaf Bachrach, Carlos Cardenas, and
William Schuler. 2010. Complexity metrics in an
incremental right-corner parser. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics (ACL?10), pages 1189?1198.
46
