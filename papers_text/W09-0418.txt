Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 105?109,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
NICT@WMT09:
Model Adaptation and Transliteration for Spanish-English SMT
Michael Paul, Andrew Finch and Eiichiro Sumita
Language Translation Group
MASTAR Project
National Institute of Information and Communications Technology
Michael.Paul@nict.go.jp
Abstract
This paper describes the NICT statis-
tical machine translation (SMT) system
used for the WMT 2009 Shared Task
(WMT09) evaluation. We participated in
the Spanish-English translation task. The
focus of this year?s participation was to in-
vestigate model adaptation and transliter-
ation techniques in order to improve the
translation quality of the baseline phrase-
based SMT system.
1 Introduction
This paper describes the NICT statistical machine
translation (SMT) system used for the shared
task of the Fourth Workshop on Statistical Ma-
chine Translation. We participated in the Spanish-
English translation task under the Constrained
Condition. For the training of the SMT engines,
we used two parallel Spanish-English corpora pro-
vided by the organizers: the Europarl (EP) cor-
pus (Koehn, 2005), which consists of 1.4M paral-
lel sentences extracted from the proceedings of the
European Parliament, and the News Commentary
(NC) corpus (Callison-Burch et al, 2008), which
consists of 74K parallel sentences taken from ma-
jor news outlets like BBC, Der Spiegel, and Le
Monde.
In order to adapt SMT systems to a specific do-
main, recent research focuses on model adapta-
tion techniques that adjust their parameters based
on information about the evaluation domain (Fos-
ter and Kuhn, 2007; Finch and Sumita, 2008a).
Statistical models can be trained on in-domain
and out-of-domain data sets and combined at
run-time using probabilistic weighting between
domain-specific statistical models. As the official
WMT09 evaluation testset consists of documents
taken from the news domain, we applied statistical
model adaptation techniques to combine transla-
tion models (tm), language models (lm) and dis-
tortion models (dm) trained on (a) the in-domain
NC corpus and (b) the out-of-domain EP corpus
(cf. Section 2).
One major problem in the given translation task
was the large amount of out-of-vocabulary (OOV)
words, i.e., source language words that do not oc-
cur in the training corpus. For unknown words, no
translation entry is available in the statistical trans-
lation model (phrase-table). As a result, these
OOV words cannot be translated. Dealing with
languages with a rich morphology like Spanish
and having a limited amount of bilingual resources
make this problem even more severe.
There have been several efforts in dealing with
OOV words to improve translation quality. In ad-
dition to parallel text corpora, external bilingual
dictionaries can be exploited to reduce the OOV
problem (Okuma et al, 2007). However, these ap-
proaches depend on the coverage of the utilized
external dictionaries.
Data sparseness problems due to inflectional
variations were previously addressed by applying
word transformations using stemming or lemmati-
zation (Popovic and Ney, 2005; Gupta and Fed-
erico, 2006). A tight integration of morpho-
syntactic information into the translation model
was proposed by (Koehn and Hoang, 2007) where
lemma and morphological information are trans-
lated separately, and this information is combined
on the output side to generate the translation.
However, these approaches still suffer from the
data sparseness problem, since lemmata and in-
flectional forms never seen in the training corpus
cannot be translated.
In order to generate translations for unknown
words, previous approaches focused on translit-
eration methods, where a sequence of charac-
ters is mapped from one writing system into an-
other. For example, in order to translate names and
technical terms, (Knight and Graehl, 1997) intro-
duced a probabilistic model that replaces Japanese
105
katakana1 words with phonetically equivalent En-
glish words. More recently, (Finch and Sumita,
2008b) proposed a transliteration method that is
based directly on techniques developed for phrase-
based SMT, and transforms a character sequence
from one language into another in a subword-
level, character-based manner. We extend this ap-
proach by exploiting the phrase-table of the base-
line SMT system to train a phrase-based translit-
eration model that generates English translations
of Spanish OOV words as described in Section 3.
The effects of the proposed techniques are investi-
gated in detail in Section 4.
2 Model Adaptation
Phrase-based statistical machine translation en-
gines use multiple statistical models to generate a
translation hypothesis in which (1) the translation
model ensures that the source phrases and the se-
lected target phrases are appropriate translations of
each other, (2) the language model ensures that the
target language is fluent, (3) the distortion model
controls the reordering of the input sentence, and
(4) the word penalty ensures that the translations
do not become too long or too short. During de-
coding, all model scores are weighted and com-
bined to find the most likely translation hypothesis
for a given input sentence (Koehn et al, 2007).
In order to adapt SMT systems to a specific do-
main, separate statistical models can be trained
on parallel text corpora taken from the respec-
tive domain (in-domain) and additional out-of-
domain language resources. The models are then
combined using mixture modeling (Hastie et al,
2001), i.e., each model is weighted according to
its fit with in-domain development data sets and
the linear combination of the respective scores is
used to find the best translation hypothesis during
the decoding of unseen input sentences.
In this paper, the above model adaptation tech-
nique is applied to combine the NC and the EP
language resources provided by the organizers
for the Spanish-English translation task. As the
WMT09 evaluation testset consists of documents
taken from the news domain, we used the NC cor-
pus to train the in-domain models and the EP cor-
pus to train the out-of-domain component models.
Using mixture modeling, the above mentioned sta-
tistical models are combined where each compo-
nent model is optimized separately. Weight opti-
1A special syllabary alphabet used to write down foreign
names or loan words.
mization is carried out using a simple grid-search
method. At each point on the grid of weight pa-
rameter values, the translation quality of the com-
bined weighted component models is evaluated for
development data sets taken from (a) the NC cor-
pus and (b) from the EP corpus.
3 Transliteration
Source language input words that cannot be trans-
lated by the standard phrase-based SMT mod-
els are either left untranslated or simply removed
from the translation output. Common examples
are named entities such as personal names or tech-
nical terms, but also include content words like
common nouns or verbs that are not covered by the
training data. Such unknown occurrences could
benefit from being transliterated into the MT sys-
tem?s output during translation of orthographically
related languages like Spanish and English.
In this paper, we apply a phrase-based translit-
eration approach similar to the one proposed in
(Finch and Sumita, 2008b). The transliteration
method is based directly on techniques developed
for phrase-based SMT and treats the task of trans-
forming a character sequence from one language
into another as a character-level translation pro-
cess. In contrast to (Finch and Sumita, 2008b)
where external dictionaries and inter-language
links in Wikipedia2 are utilized, the translitera-
tion training examples used for the experiments in
Section 4 are extracted directly from the phrase-
table of the baseline SMT systems trained on the
provided data sets. For each phrase-table entry,
corresponding word pairs are identified according
to a string similarity measure based on the edit-
distance (Wagner, 1974) that is defined as the sum
of the costs of insertion, deletion, and substitution
operations required to map one character sequence
into the other and can be calculated by a dynamic
programming technique (Cormen et al, 1989). In
order to reduce noise in the training data, only
word pairs whose word length and similarity are
above a pre-defined threshold are utilized for the
training of the transliteration model.
The obtained transliteration model is applied as
a post-process filter to the SMT decoding process,
i.e.. all source language words that could not be
translated using the SMT engine are replaced with
the corresponding transliterated word forms in or-
der to obtain the final translation output.
2http://www.wikipedia.org
106
4 Experiments
The effects of model adaptation and translitera-
tion techniques were evaluated using the Spanish-
English language resources summarized in Ta-
ble 1. In addition, the characteristics of this year?s
testset are given in Table 2. The sentence length
is given as the average number of words per sen-
tence. The OOV word figures give the percentage
of words in the evaluation data set that do not ap-
pear in the NC/EP training data. In order to get an
idea how difficult the translation task may be, we
also calculated the language perplexity of the re-
spective evaluation data sets according to 5-gram
target language models trained on the NC/EP data
sets.
Concerning the development sets, the news-
dev2009 data taken from the same news sources
as the evaluation set of the shared task was used
for the tuning of the SMT engines, and the de-
vtest2006 data taken from the EP corpus was used
for system parameter optimization. For the evalua-
tion of the proposed methods, we used the testsets
of the Second Workshop on SMT (nc-test2007 for
NC and test2007 for EP). All data sets were case-
sensitive with punctuation marks tokenized.
The numbers in Table 1 indicate that the char-
acteristics of this year?s testset differ largely from
testsets of previous evaluation campaigns. The
NC devset (2,438/1,378 OOVs) contains twice
as many untranslatable Spanish words as the
NC evalset (1,168/73 OOVs) and the EP devset
(912/63 OOVs). In addition, the high language
perplexity figures for this year?s testset show that
the translation quality output for both baseline sys-
tems is expected to be much lower than those for
the EP evaluation data sets. In this paper, transla-
tion quality is evaluated according to (1) the BLEU
metrics which calculates the geometric mean of n-
gram precision by the system output with respect
to reference translations (Papineni et al, 2002),
and (2) the METEOR metrics that calculates uni-
gram overlaps between translations (Banerjee and
Lavie, 2005). Scores of both metrics range be-
tween 0 (worst) and 1 (best) and are displayed in
percent figures.
4.1 Baseline
Our baseline system is a fairly typical phrase-
based machine translation system (Finch and
Sumita, 2008a) built within the framework of a
feature-based exponential model containing the
following features:
Table 1: Language Resources
Corpus Train Dev Eval
NC Spanish sentences 74K 2,001 2,007
words 2,048K 49,116 56,081
vocab 61K 9,047 8,638
length 27.6 24.5 27.9
OOV (%) ? 5.2 / 2.9 1.4 / 0.9
English sentences 74K 2,001 2,007
words 1,795K 46,524 49,693
vocab 47K 8,110 7,541
length 24.2 23.2 24.8
OOV (%) ? 5.2 / 2.9 1.2 / 0.9
perplexity ? 349 / 381 348 / 458
EP Spanish sentences 1,404K 1,861 2,000
words 41,003K 50,216 61,293
vocab 170K 7,422 8,251
length 29.2 27.0 30.6
OOV (%) ? 2.4 / 0.1 2.4 / 0.2
English sentences 1,404K 1,861 2,000
words 39,354K 48,663 59,145
vocab 121K 5,869 6,428
length 28.0 26.1 29.6
OOV (%) ? 1.8 / 0.1 1.9 / 0.1
perplexity ? 210 / 72 305 / 125
Table 2: Testset 2009
Corpus Test
NC Spanish sentences 3,027
words 80,591
vocab 12,616
length 26.6
? Source-target phrase translation probability
? Inverse phrase translation probability
? Source-target lexical weighting probability
? Inverse lexical weighting probability
? Phrase penalty
? Language model probability
? Lexical reordering probability
? Simple distance-based distortion model
? Word penalty
For the training of the statistical models, stan-
dard word alignment (GIZA++ (Och and Ney,
2003)) and language modeling (SRILM (Stolcke,
2002)) tools were used. We used 5-gram lan-
guage models trained with modified Knesser-Ney
smoothing. The language models were trained on
the target side of the provided training corpora.
Minimum error rate training (MERT) with respect
to BLEU score was used to tune the decoder?s pa-
rameters, and performed using the technique pro-
posed in (Och, 2003). For the translation, the in-
house multi-stack phrase-based decoder CleopA-
TRa was used.
The automatic evaluation scores of the baseline
systems trained on (a) only the NC corpus and (b)
only on the EP corpus are summarized in Table 3.
107
Table 3: Baseline Performance
NC Eval EP Eval
BLEU METEOR BLEU METEOR
baseline 17.56 40.52 33.00 56.50
4.2 Effects of Model Adaptation
In order to investigate the effect of model adapta-
tion, each model component was optimized sep-
arately using the method described in Section 2.
Table 4 summarizes the automatic evaluation re-
sults for various model combinations. The combi-
nation of NC and EP models using equal weights
achieves only a slight improvement for the NC
task (BLEU: +0.4%, METEOR: +0.4%), but a
large improvement for the EP task (BLEU: +1.0%,
METEOR: +1.7%). Weight optimization further
improves all translation tasks where the highest
evaluation scores are achieved when the optimized
weights for all statistical models are used. In total,
model adaptation gains 1.1% and 1.3% in BLEU
and 0.8% and 1.8% in METEOR for the NC and
EP translation tasks, respectively.
Table 4: Effects of Model Adaptation
weight NC Eval EP Eval
optimization BLEU METEOR BLEU METEOR
? 17.92 40.72 34.00 58.20
tm 18.13 40.95 34.05 58.23
tm+lm 18.25 41.23 34.12 58.22
tm+dm 18.36 41.06 34.24 58.34
tm+lm+dm 18.65 41.35 34.35 58.36
4.3 Effects of Transliteration
In order to investigate the effects of translitera-
tion, we trained three different transliteration us-
ing the phrase-table of the baseline systems trained
on (a) only the NC corpus, (b) only the EP cor-
pus, and (c) on the merged corpus (NC+EP). The
performance of these phrase-based transliteration
models is evaluated for 2000 randomly selected
transliteration examples. Table 5 summarizes the
haracter-based automatic evaluation scores for the
word error rate (WER) metrics, i.e., the edit dis-
tance between the system output and the closest
reference translation (Niessen et al, 2000), as well
as the BLEU and METEOR metrics. The best
performance is achieved when training examples
from both domains are exploit to transliterate un-
known Spanish words into English. Therefore, the
NC+EP transliteration model was applied to the
translation outputs of all mixture models described
in Section 4.2.
The effects of the transliteration post-process
are summarized in Table 6. Transliteration consis-
Table 5: Transliteration Performance
Training character-based
Data WER BLEU METEOR
NC 13.10 83.62 86.74
EP 11.76 85.93 87.89
NC+EP 11.72 86.08 87.89
tently improves the translation quality of all mix-
ture models, although the gains obtained for the
NC task (BLEU: +1.3%, METEOR: +1.3%) are
much larger than those for the EP task (BLEU:
+0.1%, METEOR: +0.2%) which is due to the
larger amount of untranslatable words in the NC
evaluation data set.
Table 6: Effects of Transliteration
weight NC Eval EP Eval
optimization BLEU METEOR BLEU METEOR
tm 19.14 42.39 34.11 58.46
tm+lm 19.46 42.65 34.16 58.44
tm+dm 19.77 42.35 34.38 58.57
tm+lm+dm 19.95 42.64 34.48 58.60
4.4 WMT09 Testset Results
Based on the automatic evaluation results pre-
sented in the previous sections, we selected the
SMT engine based on the tm+lm+dm weights op-
timized on the NC devset as the primary run for
our testset run submission. All other model weight
combinations were submitted as contrastive runs.
The BLEU scores of these runs are listed in Ta-
ble 7 and confirm the results obtained for the
above experiments, i.e., the best performing sys-
tem is the one based on the mixture models us-
ing separately optimized weights in combination
with the transliteration of untranslatable Span-
ish words using the phrase-based transliteration
model trained on all available language resources.
Table 7: Testset 2009 Performance
weight NC Eval EP Eval
optimization BLEU BLEU
tm 21.07 20.81
tm+lm 20.95 20.59
tm+dm 21.45 21.32
tm+lm+dm 21.67? 21.27
5 Conclusion
The work for this year?s shared task focused on
the task of effectively utilizing out-of-domain lan-
guage resources and handling OOV words to im-
prove translation quality. Overall our experi-
ments show that the incorporation of mixture mod-
els and phrase-based transliteration techniques
largely out-performed standard phrase-based SMT
engines gaining a total of 2.4% in BLEU and 2.1%
in METEOR for the news domain.
108
References
S. Banerjee and A. Lavie. 2005. METEOR: An Auto-
matic Metric for MT Evaluation with Improved Cor-
relation with Human Judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Eval-
uation Measures for MT, pages 65?72, Ann Arbor,
Michigan.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2008. Further Meta-Evaluation of Ma-
chine Translation. In Proceedings of the 3rd Work-
shop on SMT, pages 70?106, Columbus, Ohio.
H. Cormen, C. Leiserson, and L. Rivest. 1989. Intro-
duction to Algorithms. MIT Press.
A. Finch and E. Sumita. 2008a. Dynamic Model Inter-
polation for Statistical Machine Translation. In Pro-
ceedings of the 3rd Workshop on SMT, pages 208?
215, Columbus, Ohio.
A. Finch and E. Sumita. 2008b. Phrase-based Ma-
chine Transliteration. In Proceedings of the IJC-
NLP, pages 13?18, Hyderabad, India.
G. Foster and R. Kuhn. 2007. Mixture-Model Adapta-
tion for SMT. In Proceedings of the 2nd Workshop
on SMT, pages 128?135, Prague, Czech Republic.
D. Gupta and M. Federico. 2006. Exploiting Word
Transformation in SMT from Spanish to English. In
Proceedings of the EAMT, pages 75?80, Oslo, Nor-
way.
T. Hastie, R. Tibshirani, and J. Friedman. 2001. The
Elements of Statistical Learning. Springer, New
York.
K. Knight and J. Graehl. 1997. Machine Translitera-
tion. In Proceedings of the 35th ACL, pages 128?
135, Madrid, Spain.
P. Koehn and H. Hoang. 2007. Factored Transla-
tion Models. In Proceedings of the EMNLP-CoNLL,
pages 868?876, Prague, Czech Republic.
P. Koehn, F.J. Och, and D. Marcu. 2007. Statisti-
cal Phrase-Based Translation. In Proceedings of the
HLT-NAACL, pages 127?133, Edmonton, Canada.
P. Koehn. 2005. Europarl: A Parallel Corpus for Sta-
tistical Machine Translation. In Proceedings of the
MT Summit X, pages 79?86, Phuket, Thailand.
S. Niessen, F.J. Och, G. Leusch, and H. Ney. 2000.
An Evaluation Tool for Machine Translation: Fast
Evaluation for MT Research. In Proc. of the 2nd
LREC, pages 39?45, Athens, Greece.
F.J. Och and H. Ney. 2003. A Systematic Comparison
of Various Statistical Alignment Models. Computa-
tional Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training in Sta-
tistical Machine Translation. In Proceedings of the
41st ACL, pages 160?167, Sapporo, Japan.
H. Okuma, H. Yamamoto, and E. Sumita. 2007. In-
troducing Translation Dictionary into phrase-based
SMT. In Proceedings of MT Summit XI, pages 361?
368, Copenhagen, Denmark.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of the 40th ACL,
pages 311?318, Philadelphia, USA.
M. Popovic and H. Ney. 2005. Exploiting Phrasal Lex-
ica and Additional Morpho-synatctic Language Re-
sources for SMT with Scarce Training Data. In Pro-
ceedings of the EAMT, pages 212?218, Budapest,
Hungary.
A. Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of ICSLP, pages
901?904, Denver.
R.W. Wagner. 1974. The string-to-string correction
problem. Journal of the ACM, 21(1):169?173.
109
