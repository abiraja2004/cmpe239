Proceedings of the 8th Workshop on Asian Language Resources, pages 129?136,
Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language Processing
A Supervised Learning based Chunking in Thai  
using Categorial Grammar 
Thepchai Supnithi, Peerachet Porkaew, 
Taneth Ruangrajitpakorn, Kanokorn 
Trakultaweekool 
Human Language Technology, 
National Electronics and Computer  
Technology Center 
{thepchai.sup, peera-
chet.por, taneth.rua, ka-
nokorn.tra}@nectec.or.th 
 
Chanon Onman, Asanee Kaw-
trakul  
Department of Computer Engineer-
ing, Kasetsart University and  
National Electronics and Computer  
Technology Center 
 
chanon.onman@gmail.com, 
asanee.kaw@nectec.or.th 
Abstract 
One of the challenging problems in Thai 
NLP is to manage a problem on a syn-
tactical analysis of a long sentence.  
This paper applies conditional random 
field and categorical grammar to devel-
op a chunking method, which can group 
words into larger unit. Based on the ex-
periment, we found the impressive re-
sults. We gain around 74.17% on sen-
tence level chunking. Furthermore we 
got a more correct parsed tree based on 
our technique. Around 50% of tree can 
be added. Finally, we solved the prob-
lem on implicit sentential NP which is 
one of the difficult Thai language pro-
cessing.  58.65% of sentential NP is cor-
rectly detected. 
1 Introduction 
Recently, many languages applied chunking, or 
shallow parsing, using supervised learning ap-
proaches. Basili (1999) utilized clause boundary 
recognition for shallow parsing. Osborne (2000) 
and McCallum et al (2000) applied Maximum 
Entropy tagger for chunking. Lafferty (2001) 
proposed Conditional Random Fields for se-
quence labeling. CRF can be recognized as a 
generative model that is able to reach global 
optimum while other sequential classifiers focus 
on making the best local decision. Sha and Pe-
reira (2003) compared CRF to other supervised 
learning in CoNLL task. They achieved results 
better than other approaches. Molina et al 
(2002) improved the accuracy of HMM-based 
shallow parser by introducing the specialized 
HMMs. 
In Thai language processing, many research-
es focus on fundamental level of NLP, such as 
word segmentation, POS tagging. For example, 
Kruengkrai et al (2006) introduced CRF for 
word segmentation and POS tagging trained 
over Orchid corpus (Sornlertlamvanich et al, 
1998.). However, the number of tagged texts in 
Orchid is specific on a technical report, which is 
difficult to be applied to other domains such as 
news, document, etc. Furthermore, very little 
researches on other fundamental tools, such as 
chunking, unknown word detection and parser, 
have been done. Pengphon et al (2002) ana-
lyzed chunks of noun phrase in Thai for infor-
mation retrieval task. All researches assume that 
sentence segmentation has been primarily done 
in corpus. Since Thai has no explicit sentence 
boundary, defining a concrete concept of sen-
tence break is extremely difficult. 
Most sentence segmentation researches con-
centrate on "space" and apply to Orchid corpus 
(Meknavin 1987, Pradit 2002). Because of am-
biguities on using space, the accuracy is not im-
pressive when we apply into a real application. 
Let consider the following paragraph which 
is a practical usage from news: 
129
"??????????????????????????????? ????????????????????????????????????
??????????????? | ?????????????????  | ??????????????????????????"  
lit: ?The red shirts have put bunkers around 
the assembly area and put oil and tires. The 
traffic is opened normally.? 
We found that three events are described in 
this paragraph. We found that both the first and 
second event do not contain a subject. The third 
event does not semantically relate to the previ-
ous two events. With a literal translation to Eng-
lish, the first and second can be combined into 
one sentence; however, the third events should 
be separated. 
As we survey in BEST corpus (Kosawat 
2009), a ten-million word Thai segmented cor-
pus. It contains twelve genres. The number of 
word in sentence is varied from one word to 
2,633 words and the average word per line is 
40.07 words. Considering to a News domain, 
which is the most practical usage in BEST, we 
found that the number of words are ranged from 
one to 415 words, and the average word length 
in sentence is 53.20. It is obvious that there is a 
heavy burden load for parser when these long 
texts are applied. 
Example 1:  
   ??                  ???                 ????????               ??                ???????????? ?
man(n) drive(v)   taxi(n)  find(v)   wallet(n) 
 
lit1: A man drove a taxi and found a wallet. 
lit2: A taxi chauffeur found a wallet. 
Example 2: 
   ???            ??          ????      ??????              ?????               ?????? 
should will must    can    develop(v) country(n) 
 
lit: possibly have to develop country. 
 
Figure 1. Examples of compounds in Thai 
Two issues are raised in this paper. The first 
question is "How to separate a long paragraph 
into a larger unit than word effectively?" We are 
looking at the possibility of combining words 
into a larger grain size. It enables the system to 
understand the complicate structure in Thai as 
explained in the example. Chunking approach in 
this paper is closely similar to the work of Sha 
and Pereira (2003). Second question is "How to 
analyze the compound noun structure in Thai?" 
Thai allows a compound construction for a noun 
and its structures can be either a sequence of 
nouns or a combination of nouns and verbs. The 
second structure is unique since the word order 
is as same as a word order of a sentence. We 
call this compound noun structure as a ?senten-
tial NP?. 
Let us exemplify some Thai examples related to 
compound word and serial construction problem 
in Figure 1. The example 1 shows a sentence 
which contains a combination of nouns and 
verbs. It can be ambiguously represented into 
two structures. The first alternative is that this 
sentence shows an evidence of a serial verb 
construction. The first word serves as a subject 
of the two following predicates. Another alter-
native is that the first three word can be formed 
together as a compound noun and they refer to 
?a taxi driver? which serve as a subject of the 
following verb and noun. The second alternative 
is more commonly used in practical language. 
However, to set the ?N V N? pattern as a noun 
can be very ambiguous since in the example 1 
can be formed a sentential NP from either the 
first three words or the last three words. 
From the Example 2, an auxiliary verb serial-
ization is represented. It is a combination of 
auxiliary verbs and verb. The word order is 
shown in Aux Aux Aux Aux V N sequence. 
The given examples show complex cases that 
require chunking to reduce an ambiguity while 
Thai text is applied into a syntactical analysis 
such as parsing. Moreover, there is more chance 
to get a syntactically incorrect result from either 
rule-based parser or statistical parser with a high 
amount of word per input. 
This paper is organized as follows. Section 2 
explains Thai categorial grammar. Section 3 
130
illustrates CRF, which is supervised method 
applied in this work.  Section 4 explains the 
methodology and experiment framework. Sec-
tion 5 shows experiments setting and result. 
Section 6 shows discussion. Conclusion and 
future work are illustrated in section 7. 
2 Linguistic Knowledge 
2.1 Categorial Grammar 
Categorial grammar (Aka. CG or classical cate-
gorial grammar) (Ajdukiewicz, 1935; Bar-
Hillel, 1953; Carpenter, 1992; Buszkowski, 
1998; Steedman, 2000) is formalism in natural 
language syntax motivated by the principle of 
constitutionality and organized according to the 
syntactic elements. The syntactic elements are 
categorised in terms of their ability to combine 
with one another to form larger constituents as 
functions or according to a function-argument 
relationship. All syntactic categories in CG are 
distinguished by a syntactic category identifying 
them as one of the following two types: 
1. Argument: this type is a basic category, 
such as s (sentence) and np (noun 
phrase).  
2. Functor (or function category): this cat-
egory type is a combination of argu-
ment and operator(s) '/' and '\'. Functor 
is marked to a complex constituent to 
assist argument to complete sentence 
such as s\np (intransitive verb) requires 
noun phrase from the left side to com-
plete a sentence. 
CG captures the same information by associ-
ating a functional type or category with all 
grammatical entities. The notation ?/? is a 
rightward-combining functor over a domain of ? 
into a range of ?. The notation ?\? is a leftward-
combining functor over ? into ?. ? and ? are 
both argument syntactic categories 
(Hockenmaier and Steedman, 2002; Baldridge 
and Kruijff, 2003). 
The basic concept is to find the core of the 
combination and replace the grammatical modi-
fier and complement with set of categories 
based on the same concept with fractions. For 
example, intransitive verb is needed to combine 
with a subject to complete a sentence therefore 
intransitive verb is written as s\np which means  
Figure 2 Example of Thai CG-parsed Tree. 
it needs a noun phrase from the left side to  
complete a sentence. If there is a noun phrase 
exists on the left side, the rule of fraction can-
cellation is applied as np*s\np = s. With CG, 
each constituent is annotated with its own syn-
tactic category as its function in text. Currently 
there are 79 categories in Thai. An example of 
CG derivation from Thai is shown in Figure 2.  
2.2 CG-Set 
CG-Set are used as a feature when no CG are 
tagged to the input. We aim to apply our chunk-
er to a real world application. Therefore, in case 
that we have only sentence without CG tags, we 
will use CG-Set instead.           
Cat-
Set 
Index 
Cat-Set Member 
0 np ????????? 
2 s\np/pp,s\np/np,s\np/pp/np,s\np ????, ???? 
3 
(np\np)/(np\np), 
((s\np)\(s\np))/spnum, 
np, 
(np\np)\num,np\num, 
(np\np)/spnum, 
((s\np)\(s\np))\num 
????, 
?????? 
62 (s\np)\(s\np),s\s ??'?, ??'?, ??? 
134 np/(s\np), 
np/((s\np)/np) ???, ???? 
Table 1 Example of CG-Set  
131
The concept of CG-Set is to group words that 
their all possible CGs are equivalent to the 
other. Therefore every word will be assigned to 
only one CG-Set. By using CG-Set we use the 
lookup table for tagging the input. Table 1 
shows examples of CG-set. Currently, there are 
183 CG set. 
3 Conditional Random Field (CRF) 
CRF is an undirected graph model in which 
each vertex represents a random variable whose 
distribution is to be inferred, and edge 
represents a dependency between two random 
variables. It is a supervised framework for 
labeling a sequence data such as POS tagging 
and chunking. Let X  is a random variable of 
observed input sequence, such as sequence of 
words, and Y is a random variable of label 
sequence corresponding to X , such as sequence 
of POS or CG. The most probable label 
sequence ( y? ) can be obtain by 
                     )|(maxarg? xypy =  
 Where nxxxx ,...,, 21= and nyyyy ,...,, 21=  
)|( xyp  is the conditional probability 
distribution of a label sequence given by an 
input sequence. CRF defines )|( xyp as 
                  ?
?
??
?
?= ?
=
n
i
ixyF
Z
xyP
1
),,(exp1)|(  
where ( )? ? == y ni ixyFZ 1 ),,(exp  is a 
normalization factor over all state sequences. 
),,( ixyF  is the global feature vector of CRF 
for sequence x and y at position i . ),,( ixyF  
can be calculated by using summation of local 
features. 
?? += ?
j
jj
i
iiii tyxgtyyfixyF ),,(),,(),,( 1 ??
Each local feature consists of transition feature 
function ),,( 1 tyyf iii ?  and per-state feature 
function ),,( tyxg j . Where i? and j? are 
weight vectors of transition feature function and 
per-state feature function respectively.  
The parameter of CRF can be calculated by 
maximizing the likelihood function on the 
training data. Viterbi algorithm is normally 
applied for searching the most suitable output. 
4 Methodology 
Figure 3 shows the methodology of our 
experiments. To prepare the training set, we 
start with our corpus annotated with CG tag. 
Then, each sentence in the corpus was parsed by 
Figure 3 Experimental Framework 
132
our Thai CG parser, developed by GLR tech-
technique. However, not all sentences can be 
parsed successfully due to the complexity of the 
sentence. We kept parsable sentences and 
unparsable sentences separately. The parsable 
sentences were selected to be the training set.  
There are four features ? surface, CG, CG-set 
and chunk marker ? in our experiments. CRF is 
applied using 5-fold cross validation over 
combination of these features. Accuracy in term 
of averaged precision and recall are reported. 
We select the best model from the experiment 
to implement the chunker. To investigate 
performance of the chunker, we feed the 
unparsable sentences to the chunker and 
evaluate them manually.  
After that, the sentences which are correctly 
chunked will be sent to our Thai CG parser. We 
calculate the number of successfully-parsed 
sentences and the number of correct chunks. 
5 Experiment Settings and Results 
5.1 Experiment on chunking 
5.1.1 Experiment setting 
To develop chunker, we apply CG Dictionary 
and CG tagged corpus as input. Four features 
are provided to CRF. Surface is a word surface. 
CG is a categorial grammar of the word. CG-set 
is a combination of CG of the word. IOB 
represents a method to mark chunk in a 
sentence. "I" means "inner" which represents 
the word within the chunk. "O" means "outside" 
which represents the word outside the chunk. 
"B" means "boundary" which represents the 
word as a boundary position. It accompanied 
with five chunk types. "NP" stands for noun 
phrase, "VP" stands for verb phrase, "PP" stands 
for preposition phrase, "ADVP" stands for 
adverb phrase and S-BAR stands for 
complementizer that link two phrases.  
Surface and CG-set are developed from CG 
dictionary. CG is retrieved from CG tagged 
corpus. IOB is developed by parsing tree. We 
apply Thai CG parser to obtain the parsed tree. 
Figure 4 shows an example of our prepared 
data. We provide 4,201 sentences as a training 
data in CRF to obtain a chunked model. In this 
experiment, we use 5-fold cross validation to 
evaluation the model in term of F-measure.  
surface cg_set cg chunk_label 
?? 74 s/s/np B-ADVP 
??? 3 np I-ADVP 
?? 180 (np\np)/(s\np) I-ADVP 
?? ? 54 (s\np)/(s\np) I-ADVP 
???? 7 s\np I-ADVP 
???? 130 ((s/s)\(s/s))/(s/s) I-ADVP 
?? 74 s/s/np I-ADVP 
??????? 0 np I-ADVP 
??? 0 np B-NP 
??? 8 s\np/np B-VP 
???'? 0 np B-NP 
?? 148 (s\np)/(s\np) B-VP 
???????? 2 s\np I-VP 
Figure 4 An example of prepared data 
 
Table 2 Chunking accuracy of each chunk 
133
  
5.1.2 Experiment result 
From Table 2, considering on chunk based lev-
el, we found that CG gives the best result 
among surface, CG-set, CG and their combina-
tion. The average on three types in terms of F-
measure is 86.20.  When we analyze infor-
mation in detail, we found that NP, VP and PP 
show the same results. Using CG shows the F-
measure for each of them, 81.15, 90.96 and 
99.56 respectively.   
From Table 3, considering in both word level 
and sentence level, we got the similar results, 
CG gives the best results. F-measure is 93.24 in 
word level and 74.17 in sentence level. This 
shows the evidence that CG plays an important 
role to improve the accuracy on chunking. 
5.2 Experiment on parsing 
5.2.1 Experiment setting 
We investigate the improvement of parsing con-
sidering unparsable sentences.  There are 14,885 
unparsable sentences from our CG parser. These 
sentences are inputted in chunked model to ob-
tain a chunked corpus. We manually evaluate 
the results by linguist. Linguists evaluate the 
chunked output in three types. 0 means incorrect 
chunk. 1 means correct chunk and 2 represents a 
special case for Thai NP, a sentential NP. 
5.2.2 Experiment result 
From the experiment, we got an impressive re-
sult. We found that 11,698 sentences (78.59%) 
are changed from unparsable to parsable sen-
tence. Only 3,187 (21.41%) are unparsable.  We 
manually evaluate the parsable sentence by ran-
domly select 7,369 sentences. Linguists found 
3,689 correct sentences (50.06%). In addition, 
we investigate the number of parsable chunk 
calculated from the parsable result and found 
37,743 correct chunks from 47,718 chunks 
(78.47%).  We also classified chunk into three 
types NN VP and PP and gain the accuracy in 
each type 79.14% ,74.66% and 92.57% respec-
tively. 
6 Discussion 
6.1 Error analysis 
From the experiment results, we found the fol-
lowing errors. 
6.1.1 Chunking Type missing 
Some chunk missing types are found in experi-
ment results. For example, [PP ?????? (rec-
ord)][NP ????????????????? (character about)]. [PP 
Table 3 Chunking accuracy based on  
word and sentence. 
Figure 4 An Example of sentential NP 
134
?????? (record)] should be defined as VP instead 
of PP. 
6.1.2 Over-grouping 
In the sentence ?[VP ?? ? (Using)][NP 
(medicine)][VP ????? (treat) ][NP ???????????' ?????
?????? (each disease have to)][PP ??? (follow) ]
[NP ???????????????? ?(doctor?s instruction)] ?, we 
found that ?NP ???????????' ??????????? (each disease 
have to) ? has over-grouping. IT is necessary to 
breakdown to NP ???????????' ?(each disease)  and  
VP ??????????(have to). The reason of this error is 
due to allow the sentential structure NP VP NP, 
and then NP and VP are combined. 
6.1.3 Sentential NP 
We investigated the number of sentential NP. If 
the number of chunk equal to 1, sentence should 
not be recognized as NP. Other cases are de-
fined as NP. We found that 929 from 1,584 sen-
tences (58.65 % of sentences) are correct sen-
tential NP. This evidence shows the impressive 
results to solve implicit NP in Thai. Figure 4 
shows an example of sentential NP.  
6.1.4 CG-set  
Since CG-set is another representation of word 
and can only detect from CG dictionary. It is 
very easy to develop a tag sequence using CG-
set. We found that CG-set is more powerful than 
surface. It might be another alternative for less 
language resource situation. 
6.2 The Effect of Linguistic Knowledge on 
chunking 
Since CG is formalism in natural language syn-
tax motivated by the principle of constitutionali-
ty and organised according to the syntactic ele-
ments, we would like to find out whether lin-
guistic knowledge effects to the model. We 
grouped 89 categorial grammars into 17 groups, 
called CG-17.  
It is categorized into Noun, Prep, Noun 
Modifier, Number modifier for noun, Number 
modifier for verb, Number, Clause Marker, 
Verb with no argument, Verb with 1 argument, 
Verb with 2 or more arguments, Prefix noun, 
Prefix predicate, Prefix predicate modifier, 
Noun linker, Predicate Modification, Predicate 
linker, and Sentence Modifier.  
We found that F-measure is slightly improved 
from 74.17% to 75.06%. This shows the evi-
dence that if we carefully categorized data based 
on linguistics viewpoint, it may improve more 
accuracy.  
7 Conclusions and Future Work 
In this paper, we stated Thai language problems 
on the long sentence pattern and find the novel 
method to chunk sentence into smaller unit, 
which larger than word. We concluded that us-
ing CRF accompanied with categorical grammar 
show the impressive results. The accuracy of 
chunking in sentence level is 74.17%. We are 
possible to collect 50% more on correct tree. 
This technique enables us to solve the implicit 
sentential NP problem. With our technique, we 
found 58% of implicit sentential NP. In the fu-
ture work, there are several issues to be im-
proved. First, we have to trade-off between 
over-grouping problem and implicit sentential 
problem. Second, we plan to consider ADVP, 
SBAR, which has a very small size of data. It is 
not adequate to train for a good result. Finally, 
we plan to apply more linguistics knowledge to 
assist more accuracy. 
References 
Abney S., and Tenny C., editors, 1991. Parsing 
by chunks, Priciple-based Parsing. Kluwer 
Academic Publishers. 
Awasthi P., Rao D., Ravindram B., 2006. Part 
of Speech Tagging and Chunking with HMM 
and CRF, Proceeding of the NLPAI Machine 
Learning Competition. 
Basili R., Pazienza T., and Massio F., 1999. 
Lexicalizing a shallow parser, Proceedings of 
135
Traitement Automatique du Langage Naturel 
1999. Corgese, Corsica. 
Charoenporn Thatsanee, Sornlertlamvanich Vi-
rach,  and Isahara Hitoshi. 1997. Building A 
Large Thai Text Corpus - Part-Of-Speech 
Tagged Corpus: ORCHID. Proceedings of 
Natural Language Processing Pacific Rim 
Symposium. 
Kosawat Krit, Boriboon Monthika, Chootrakool 
Patcharika, Chotimongkol Ananlada, Klaithin 
Supon, Kongyoung Sarawoot, Kriengket 
Kanyanut, Phaholphinyo Sitthaa, Puroda-
kananda Sumonmas,Thanakulwarapas 
Tipraporn, and Wutiwiwatchai Chai. 2009. 
BEST 2009: Thai Word Segmentation Soft-
ware Contest. The Eigth International Sym-
posium on Natural Language Processing  : 
83-88. 
Kruengkrai C., Sornlertlumvanich V., Isahara H, 
2006. A Conditional Random Field Frame-
work for Thai Morphological Analysis, Pro-
ceedings of 5th International Conference on 
Language Resources and Evaluation (LREC-
2006). 
Kudo T., and Matsumoto Y., 2001. Chunking 
with support vector machines, Proceeding of 
NAACL. 
Lafferty J., McCallum A., and Pereira F., 2001. 
Conditional Random Fields : Probabilistic 
models for segmenting and labeling sequence 
data. In Proceeding of ICML-01, 282-289. 
McCallum A., Freitag D., and Pereira F. 2000. 
Maximum entropy markov model for infor-
mation extraction and segmentation. Pro-
ceedings of ICML. 
Molina A., and Pla F., 2002. Shallow Parsing 
using Specialized HMMs, Journal of Machine 
Learning Research 2,595-613 
Nguyen L. Minh, Nguyen H. Thao, and Nguyen 
P., Thai. 2009. An Empirical Study of Viet-
namese Noun Phrase Chunking with Discrim-
inative Sequence Models, Proceedings of the 
7th Workshop on Asian Language Resources, 
ACL-IJCNLP 2009,9-16 
Osborne M. 2000. Shallow Parsing as Part-of-
Speech Tagging. Proceedings of CoNLL-
2000 and LLL-2000, Lisbon, Portugal. 
Pengphon N., Kawtrakul A., Suktarachan M., 
2002. Word Formation Approach to Noun 
Phrase Analysis for Thai,  Proceedings of 
SNLP2002. 
Sha F. and Pereira F., 2003. Shallow Parsing 
with Conditional Random Fields, Proceeding 
of HLT-NAACL. 
 
136
