Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 48?54,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Cross-lingual comparison between distributionally determined
word similarity networks
Olof Go?rnerup
Swedish Institute of Computer Science
(SICS)
164 29 Kista, Sweden
olofg@sics.se
Jussi Karlgren
Swedish Institute of Computer Science
(SICS)
164 29 Kista, Sweden
jussi@sics.se
Abstract
As an initial effort to identify universal
and language-specific factors that influ-
ence the behavior of distributional models,
we have formulated a distributionally de-
termined word similarity network model,
implemented it for eleven different lan-
guages, and compared the resulting net-
works. In the model, vertices constitute
words and two words are linked if they oc-
cur in similar contexts. The model is found
to capture clear isomorphisms across lan-
guages in terms of syntactic and semantic
classes, as well as functional categories of
abstract discourse markers. Language spe-
cific morphology is found to be a dominat-
ing factor for the accuracy of the model.
1 Introduction
This work takes as its point of departure the fact
that most studies of the distributional character of
terms in language are language specific. A model
or technique?either geometric (Deerwester et al,
1990; Finch and Chater, 1992; Lund and Burgess,
1996; Letsche and Berry, 1997; Kanerva et al,
2000) or graph based (i Cancho and Sole?, 2001;
Widdows and Dorow, 2002; Biemann, 2006)?
that works quite well for one language may not be
suitable for other languages. A general question
of interest is then: What strengths and weaknesses
of distributional models are universal and what are
language specific?
In this paper we approach this question by for-
mulating a distributionally based network model,
apply the model on eleven different languages, and
then compare the resulting networks. We com-
pare the networks both in terms of global statisti-
cal properties and local structures of word-to-word
relations of linguistic relevance. More specif-
ically, the generated networks constitute words
(vertices) that are connected with edges if they
are observed to occur in similar contexts. The
networks are derived from the Europarl corpus
(Koehn, 2005)?the annotated proceedings of the
European parliament during 1996-2006. This is
a parallel corpus that covers Danish, Dutch, En-
glish, Finnish, French, German, Greek, Italian,
Portuguese, Spanish and Swedish.
The objective of this paper is not to provide a
extensive comparison of how distributional net-
work models perform in specific applications for
specific languages, for instance in terms of bench-
mark performance, but rather to, firstly, demon-
strate the expressive strength of distributionally
based network models and, secondly, to highlight
fundamental similarities and differences between
languages that these models are capable of captur-
ing (and fail in capturing).
2 Methods
We consider a base case where a context is defined
as the preceding and subsequent words of a focus
word. Word order matters and so a context forms
a word pair. Consider for instance the following
sentence1:
Ladies and gentlemen, once again, we
see it is essential for Members to bring
their voting cards along on a Monday.
Here the focus word essential occurs in the con-
text is ? for, the word bring in the context to ?
their etcetera (the asterisk ? denotes an interme-
diate focus word). Since a context occurs with a
word with a certain probability, each word wi is
associated with a probability distribution of con-
texts:
Pi = {Pr[wpwiws|wi]}wp,ws?W , (1)
1Quoting Nicole Fontaine, president of the European Par-
liament 1999-2001, from the first session of year 2000.
48
where W denotes the set of all words and
Pr[wpwiws|wi] is the conditional probability that
context wp ?ws occurrs, given that the focus word
is wi. In practice, we estimate Pi by counting
the occurring contexts of wi and then normalizing
the counts. Context counts, in turn, were derived
from trigram counts. No pre-processing, such as
stemming, was performed prior to collecting the
trigrams.
2.1 Similarity measure
If two words have similar context distributions,
they are assumed to have a similar function in
the language. For instance, it is reasonable to as-
sume that the word ?salt? to a higher degree occurs
in similar contexts as ?pepper? compared to, say,
?friendly?. One could imagine that a narrow 1+1
neighborhood only captures fundamental syntactic
agreement between words, which has also been ar-
gued in the literature (Sahlgren, 2006). However,
as we will see below, the intermediate two-word
context also captures richer word relationships.
We measure the degree of similarity by com-
paring the respective context distributions. This
can be done in a number of ways. For example,
as the Euclidian distance (also known as L2 diver-
gence), the Harmonic mean, Spearman?s rank cor-
relation coefficient and the Jensen-Shannon diver-
gence (information radius). Here we quantify the
difference between two words wi and wj , denoted
dij , by the variational distance (or L1 divergence)
between their corresponding context distributions
Pi and Pj :
dij =
?
c?C
|Pi(X = c)? Pj(X = c)|, (2)
where X is a stochastic variable drawn from C,
which is the set of contexts that either wi or wj
occur in. 0 ? dij ? 2, where dij = 0 if
the two distributions are identical and dij = 2
if the words do not share any contexts at all. It
is not obvious that the variational distance is the
best choice of measure. However, we chose to
employ it since it is a well-established and well-
understood statistical measure; since it is straight-
forward and fast to calculate; and since it appears
to be robust. To compare, we have also tested
to employ the Jensen-Shannon divergence (a sym-
metrized and smoothed version of Kullback infor-
mation) and acquire very similar results as those
presented here. In fact, this is expected since the
two measures are found to be approximately lin-
early related in this context. However, for the two
first reasons listed above, the variational distance
is our divergence measure of choice in this study.
2.2 Network representation
A set of words and their similarity relations are
naturally interpreted as a weighted and undirected
network. The vertices then constitute words and
two vertices are linked by an edge if their corre-
sponding words wi and wj have overlapping con-
text sets. The strength of the links vary depend-
ing on the respective degrees of word similarities.
Here the edge between two words wi and wj?s
is weighted with wij = 2 ? dij (note again that
maxij dij = 2) since a large word difference im-
plies a weak link and vice versa.
In our experiment we consider the 3000 most
common words, excluding the 19 first ones, in
each language. To keep the data more manage-
able during analysis we employ various thresh-
olds. Firstly, we only consider context words that
occur five times or more. As formed by the re-
maining context words, we then only consider tri-
grams that occur three times or more. This allows
us to cut away a large chunk of the data. We have
tested to vary these thresholds and the resulting
networks are found to have very similar statisti-
cal properties, even though the networks differ by
a large number of very weak edges.
3 Results
3.1 Degree distributions
The degree gi of a vertex i is defined as the sum
of weights of the edges of the vertex: gi =
?wij .
The degree distribution of a network may provide
valuable statistical information about the networks
structure. For the word networks, Figure 1, the de-
gree distributions are all found to be highly right-
skewed and have longer tails than expected from
random graphs (Erdo?s and Re?nyi, 1959). This
characteristics is often observed in complex net-
works, which typically also are scale-free (New-
man, 2003). Interestingly, the word similarity net-
works are not scale-free as their degree distribu-
tions do no obey power-laws: Pr(g) ? g?? for
some exponent ?. Instead, the degree distributions
of each word network appears to lay somewhere
between a power-law distribution and an exponen-
tial distribution (Pr(g) ? e?g/?). However, due
to quite noisy statistics it is difficult to reliably
49
measure and characterize the tails in the word net-
works. Note that there appears to be a bump in
the distributions for some languages at around de-
gree 60, but again, this may be due to noise and
more data is required before we can draw any con-
clusions. Note also that the degree distribution of
Finnish stands out: Finnish words typically have
less or weaker links than words in the other lan-
guages. This is reasonably in view of the special
morphological character of Finnish compared to
Indo-European languages (see below).
3.2 Community structures
The acquired networks display interesting global
structures that emerge from the local and pair-
wise word to word relations. Each network form
a single strongly connected component. In other
words, any vertex can be reached by any other ver-
tex and so there is always a path of ?associations?
between any two words. Furthermore, all word
networks have significant community structures;
vertices are organized into groups, where there are
higher densities of edges within groups than be-
tween them. The strength of community structure
can be quantified as follows (Newman and Gir-
van, 2004): Let {vi}ni=1 be a partition of the set
of vertices into n groups, ri the fraction of edge
weights that are internal to vi (i.e. the sum of in-
ternal weights over the sum of all weights in the
network), and si the fraction of edge weights of
the edges starting in vi. The modularity strength is
then defined as
Q =
n?
i=1
(ri ? s2i ). (3)
Q constitutes the fraction of edge weights given
by edges in the network that link vertices within
the same communities, minus the expected value
of the same quantity in a random network with the
same community assignments (i.e. the same ver-
tex set partition). There are several algorithms that
aim to find the community structure of a network
by maximizing Q. Here we use an agglomerative
clustering method by Clauset (2005), which works
as follows: Initialize by assigning each vertex to
its own cluster. Then successively merge clusters
such that the positive change of Q is maximized.
The procedure is repeated as long as Q increases.
Typically Q is close to 0 for random partitions
and indicates strong community structure when
approaching its maximum 1. In practice Q is typi-
cally within the range 0.3 to 0.7, also for highly
modular networks (Newman and Girvan, 2004).
As can be seen in Table 1, all networks are highly
modular, although the degree of modularity varies
between languages. Greek in particular stands out.
However, the reason for this remains an open ques-
tion that requires further investigations.
Dutch 0.43 Swedish 0.58
German 0.43 French 0.63
Spanish 0.48 Finnish 0.68
Portuguese 0.51 Italian 0.68
English 0.53 Greek 0.78
Danish 0.55
Table 1: Community modularity.
Communities become more apparent when
edges are pruned by a threshold as they crystal-
ize into isolated subgraphs. This is exemplified
for English in Figure 2.
4 Discussion
We examine the resulting graphs and show in this
section through some example subgraphs how fea-
tures of human language emerge as charactersitics
of the model.
4.1 Morphology matters
Morphology is a determining and observable char-
acteristic of several languages. For the purposes
of distributional study of linguistic items, mor-
phological variation is problematic, since it splits
one lexical item into several surface realisations,
requiring more data to perform reliable and ro-
bust statistical analysis. Of the languages stud-
ied in this experiment, Finnish stands out atypi-
cal through its morphological characteristics. In
theory, Finnish nouns can take more than 2 000
surface forms, through more than 12 cases in sin-
gular and plural as well as possessive suffixes
and clitic particles (Linden and Pirinen, 2009),
and while in practice something between six and
twelve forms suffice to cover about 80 per cent
of the variation (Kettunen, 2007) this is still an
order of magnitude more variation than in typi-
cal Indo-European languages such as the others
in this sample. This variation is evident in Fig-
ure 1?Finnish behaves differently than the Indo-
European languages in the sample: as each word
is split in several other surface forms, its links to
other forms will be weaker. Morphological anal-
ysis, transforming surface forms to base forms
50
 0
 200
 400
 600
 800
 1000
 1200
 1400
 1600
 1800
 0  50  100  150  200  250  300  350
de
gre
e c
ou
nt
degree
Danish
German
Greek
English
Spanish
Finnish
French
Italian
Dutch
Portuguese
Swedish
Figure 1: Degree histograms of word similarity networks.
 # %
 #%
l' activit?
l' adh?sion
l' adoption
l' am?lioration
l' application
l' augmentation
l' ?galit?l' ?laboration
l' ?tablissement
l' ?valuationl' examen
l' ex?cution
l' existence l' harmonisation
l' histoire
l' interdiction
l' intervention
l' introduction
l' octroi l' ouverture
l' utilisation
l adh?sion l adoption l application l ?galit?
l ?largissement
l ensemble
l utilisation
Figure 3: French definite nouns clustered.
would strengthen those links.
In practice, the data sparsity caused by mor-
phological variation causes semantically homoge-
nous classes to be split. Even for languages such
as English and French, with very little data varia-
tion we find examples where morphological varia-
tion causes divergence as seen in Figure 3, where
French nouns in definite form are clustered. It is
not surprising that certain nouns in definite form
assume similar roles in text, but the neatness of
the graph is a striking exposition of this fact.
These problems could have been avoided with
better preprocessing?simple such processing in
the case of English and French, and considerably
more complex but feasible in the case of Finnish?
but are retained in the present example as proxies
for the difficulties typical of processing unknown
languages. Our methodology is robust even in
face of shoddy preprocessing and no knowledge
of the morphological basis of the target language.
In general, as a typological fact, it is reasonable to
assume that morphological variation is offset for
the language user in a greater freedom in choice of
word order. This would seem to cause a great deal
of problems for an approach such as the present
one, since it relies on the sequential organisation
of symbols in the signal. However, it is observ-
able that languages with free word order have pre-
ferred unmarked arrangements for their sentence
structure, and thus we find stable relationships in
the data even for Finnish, although weaker than for
the other languages examined.
4.2 Syntactic classes
Previous studies have shown that a narrow con-
text window of one neighour to the left and one
neighbour to the right such as the one used in
the present experiments retrieves syntactic rela-
tionships (Sahlgren, 2006). We find several such
examples in the graphs. In Figure 2 we can see
subgraphs with past participles, auxiliary verbs,
progressive verbs, person names.
4.3 Semantic classes
Some of the subgraphs we find are models of clear
semantic family resemblance as shown in Fig-
ure 4. This provides us with a good argument for
blurring the artificial distinction between syntax
and semantics. Word classes are defined by their
meaning and usage alike; the a priori distinction
between classification by function such as auxil-
iary verbs given above and classification by mean-
ing such months and places given here is not fruit-
ful. We expect to be able to provide much more in-
51
adapt
addition
address
afghanistanalbania
algeria
amend
amsterdam
asia
assess
auditors
austria barcelona
belarus
belgium
berlin
bosnia
britain
brussels
bulgaria
burma
chechnya
china
clarify
clarity
coherence
competitiveness
condemn
coordinate
copenhagencorruption
croatia
cuba
culture
cyprus
define
denmark
discuss
dublin
efficiency
eliminate
employers
emu
enhance
europol
examine
expression
finland
flexibility
france
fraud
fulfil
geneva
germany
globalisation
gmos
greece
harmonisation
harmonization
helsinki
implement
improve
incorporate
india
indonesia
innovation
inquiry
iran
iraq
ireland
israel
issue
italy
japan
kind
kosovo
liberalisation
londonluxembourg
macedonia
maintain
matter
men
ministers
monitor
afternoon
evening morning
morocco
movement
nato
nice
olaf
openness
oppose
order
origin
overcome
palestine
articles
paragraphparagraphs
parliament'
people'
poland
politics
portugal
poverty
preserve
presidents
abolition
absence
adoption
aim
appointment
background
chairman
completion
concept
continuationcreation
definition
establishment
event
face
grounds
inclusion
introduction
leader
light
middle
occasion
possibility promotion
prospect
purpose
purposes
pursue
racism
ratify
regulate
reinforce
reject
relation
religion
remove
replace
resolve
retain
romania
russia
safeguard
science
considering
debating
discussing
examining aims
fails
refers
seeks
april
december
february
january
july
june
march
november
october
september
serbia
slovakia
covers
includes
involvesreflects barroso
prodi
santer
solana
solve
sort
spain
sport
strasbourg
strengthen
addressed
applied
completed
condemned
conducted
created
debated
discussed
encouraged
established
examined
financed
funded
guaranteedimplemented
improved
introduced
justified
maintained
monitored
promoted
protected
pursued
regulated resolved
respected
solved
strengthened
confirmed
emphasised
highlighted noted
stressed
subject
subsidiarity
afraid
delighted
glad
happy
pleased
sorry
sure
sweden
austrian belgian
danish
dutch
finnish
french
german
greek
irish
italianportuguese
spanish swedish
tackle
achieving
addressing applying
changing
defending
defining
guaranteeing
implementingmaintaining
protecting
safeguarding supporting
tackling
tampere
terrorism
believe
feel
hopethink
foremost
hence
secondly
thirdly
torture
tourism
transparency
treat
turkey
five
four
seven
ten
three
two
type
ukraine
acknowledge
emphasise
emphasize
highlight
raise
realise
recognise
recognize
reiterate
remember
stress
underline
unemployment
apparentevidentobvious
regrettable
true
unfortunate
union'
appearbelongfailrefuse seem
ask inviteremind urge
americans
balkans
bureau
cap
citizen
courts
ecb
euro
factsgreens
ground
igc
internet
netherlands
ombudsman
palestinians
railways roadss
treaties
ukusa
vienna
crucial
essential
unacceptable
vital
able entitled
going
prepared supposed
willing
intend
want wish
continues intends
wants
wishes
withdraw
children
consumers employeesfarmers
fishermen
forests
immigrants
individuals
journalists
minorities
ngos
producers
refugeessmesstudents
women
workers
must
should
will
would
days
months
weeks
years
zimbabwe
Figure 2: English. Network involving edges with weights w ? 0.85. For sake of clarity, only subgraphs
with three or more words are shown. Note that the threshold 0.85 is used only for the visualization. The
full network consists of the 3000 most common words in English, excluding the 19 most common ones.
52
avril
d?cembre
f?vrier
janvier
juillet
juin
mai
mars
novembre
octobre
septembre
april
december
februari
januari
juli juni
maj
mars
november
oktoberseptember
enhance
improve
preserve
reinforce
safeguard strengthen
corruption fraud
poverty
racism
terrorism
torture
Figure 4: Examples of semantically homogenous
classes in English, French and Swedish.
formed classification schemes than the traditional
?parts of speech? if we define classes by their dis-
tributional qualities rather than by the ?content?
they ?represent?, schemes which will cut across
the function-topic distinction.
4.4 Abstract discourse markers are a
functional category
Further, several subgraphs have clear collections
of discourse markers of various types where the
terms are markers of informational organisation in
the text, as exemplified in Figure 5.
5 Conclusions
This preliminary experiment supports future stud-
ies to build knowledge structures across lan-
guages, using distributional isomorphism between
linguistic material in translated or even compara-
ble corpora, on several levels of abstraction, from
function words, to semantic classes, to discourse
markers. The isomorphism across the languages
is clear and incontrovertible; this will allow us to
continue experiments using collections of multi-
lingual materials, even for languages with rela-
tively little technological support. Previous stud-
ies show that knowledge structures of this type
that are created in one language show consider-
able isomorphism to knowledge structures created
in another language if the corpora are comparable
(Holmlund et al, 2005). Holmlund et alshow how
translation equivalences can be established using
ep?ilem?tt?
kuitenkin
luonnollisesti
selv?stikin
siis
tietenkin
tietysti
todellakin
toki
varmasti
f?rmodligen
f?rvisso
givetvis
s?kerligen
sannerligen
sj?lvfallet
uppenbarligen
visserligen
Figure 5: Examples of discourse functional classes
in Swedish and Finnish. The terms in the two sub-
graphs are discourse markers and correspond to
English ?certainly?, ?possibly?, ?evidently?, ?nat-
urally?, ?absolutely?, ?hence? and similar terms.
two semantic networks automatically created in
two languages by providing a relatively limited set
of equivalence relations in a translation lexicon.
This study supports those findings.
The results presented here display the potential
of distributionally derived network representations
of word similarities. Although geometric (vector
based) and probabilistic models have proven vi-
able in various applications, they are limited by
the fact that word or term relations are constrained
by the geometric (often Euclidian) space in which
they live. Network representations are richer in
the sense that they are not bound by the same con-
straints. For instance, a polyseme word (?may? for
example) can have strong links to two other words
(?might? and ?September? for example), where
the two other words are completely unrelated. In
an Euclidean space this relation is not possible due
to the triangle inequality. It is possible to em-
bed a network in a geometric space, but this re-
quires a very high dimensionality which makes the
representation both cumbersome and inefficient in
terms of computation and memory. This has been
addressed by coarse graining or dimension reduc-
tion, for example by means of singular value de-
53
composition (Deerwester et al, 1990; Letsche and
Berry, 1997; Kanerva et al, 2000), which results
in information loss. This can be problematic, in
particular since distributional models often face
data sparsity due to the curse of dimensionality.
In a network representation, such dimension re-
duction is not necessary and so potentially impor-
tant information about word or term relations is
retained.
The experiments presented here also show the
potential of moving from a purely probabilistic
model of term occurrence, or a bare distributional
model such as those typically presented using a
geometric metaphor, in that it affords the possibil-
ity of abstract categories inferred from the primary
distributional data. This will give the possibility
of further utilising the results in studies, e.g. for
learning syntactic or functional categories in more
complex constructional models of linguistic form.
Automatically establishing lexically and function-
ally coherent classes in this manner will have bear-
ing on future project goals of automatically learn-
ing syntactic and semantic roles of words in lan-
guage. This target is today typically pursued rely-
ing on traditional lexical categories which are not
necessarily the most salient ones in view of actual
distributional characteristics of words.
Acknowledgments: OG was supported by Johan
and Jacob So?derberg?s Foundation. JK was sup-
ported by the Swedish Research Council.
References
Chris. Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In Proceedings
of the HLT-NAACL-06 Workshop on Textgraphs-06,
New York, USA.
Aaron Clauset. 2005. Finding local community struc-
ture in networks. Physical Review E, 72:026132.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41:391?
407.
Pal Erdo?s and Alfre?d Re?nyi. 1959. On random graphs.
Publications Mathematicae, 6:290.
Steven Finch and Nick Chater. 1992. Bootstrap-
ping syntactic categories. In Proceedings of the
Fourteenth Annual Conference of the Cognitive Sci-
ence Society, pages 820?825, Bloomington, IN.
Lawrence Erlbaum.
Jon Holmlund, Magnus Sahlgren, and Jussi Karlgren.
2005. Creating bilingual lexica using reference
wordlists for alignment of monolingual semantic
vector spaces. In Proceedings of 15th Nordic Con-
ference of Computational Linguistics.
Ramon Ferrer i Cancho and Ricard V. Sole?. 2001. The
small world of human language. Proceedings of the
Royal Society of London. Series B, Biological Sci-
ences, 268:2261?2266.
Pentti Kanerva, Jan Kristoferson, and Anders Holst.
2000. Random indexing of text samples for latent
semantic analysis. In Proceedings of the 22nd An-
nual Conference of the Cognitive Science Society,
pages 103?6.
Kimmo Kettunen. 2007. Management of keyword
variation with frequency based generation of word
forms in ir. In Proceedings of SIGIR 2007.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Todd Letsche and Michael Berry. 1997. Large-scale
information retrieval with latent semantic indexing.
Information Sciences, 100(1-4):105?137.
Krister Linden and Tommi Pirinen. 2009. Weighting
finite-state morphological analyzers using hfst tools.
In Proceedings of the Finite-State Methods and Nat-
ural Language Processing. Pretoria, South Africa.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers, 28(2):203?208.
Mark Newman and Michelle Girvan. 2004. Find-
ing and evaluating community structure in networks.
Physical Review E, 69(2).
Mark E. J. Newman. 2003. The structure and function
of complex networks. SIAM Review, 45(2):167?
256.
Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. PhD Dissertation, De-
partment of Linguistics, Stockholm University.
Dominic Widdows and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. In In
19th International Conference on Computational
Linguistics, pages 1093?1099.
54
