Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 25?30,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Joint memory-based learning of syntactic and semantic dependencies
in multiple languages
Roser Morante, Vincent Van Asch
CNTS - Language Technology Group
University of Antwerp
Prinsstraat 13
B-2000 Antwerpen, Belgium
{Roser.Morante,Vincent.VanAsch}@ua.ac.be
Antal van den Bosch
Tilburg University
Tilburg centre for Creative Computing
P.O. Box 90153
NL-5000 LE Tilburg, The Netherlands
Antal.vdnBosch@uvt.nl
Abstract
In this paper we present a system submitted to the
CoNLL Shared Task 2009 performing the identifi-
cation and labeling of syntactic and semantic depen-
dencies in multiple languages. Dependencies are
truly jointly learned, i.e. as if they were a single
task. The system works in two phases: a classifica-
tion phase in which three classifiers predict different
types of information, and a ranking phase in which
the output of the classifiers is combined.
1 Introduction
In this paper we present the machine learning system
submitted to the CoNLL Shared Task 2009 (Hajic?
et al, 2009). The task is an extension to multi-
ple languages (Burchardt et al, 2006; Hajic? et al,
2006; Kawahara et al, 2002; Palmer and Xue, 2009;
Surdeanu et al, 2008; Taule? et al, 2008) of the
CoNLL Shared Task 2008, combining the identifica-
tion and labeling of syntactic dependencies and se-
mantic roles. Our system is a joint-learning system
tested in the ?closed? challenge, i.e. without making
use of external resources.
Our system operates in two phases: a classifica-
tion phase in which three memory-based classifiers
predict different types of information, and a rank-
ing phase in which the output of the classifiers is
combined by ranking the predictions. Semantic and
syntactic dependencies are jointly learned and pro-
cessed. In the task description no precise defini-
tion is given of joint learning. We consider that a
joint-learning system is one in which semantic and
syntactic dependencies are learned and processed
jointly as a single task. In our system this is achieved
by fully merging semantic and syntactic dependen-
cies at the word level as the first step.
One direct consequence of merging the two tasks,
is that the class space becomes more complex;
the number of classes increases. Many machine-
learning approaches do not scale well to larger class
spaces in terms of efficiency and computer resource
requirements. Memory-based learning is a noted ex-
ception, as it is largely insensitive to the number of
classes in terms of efficiency. This is the primary
reason for using memory-based learning. Memory-
based language processing (Daelemans and van den
Bosch, 2005) is based on the idea that NLP prob-
lems can be solved by storing solved examples of the
problem in their literal form in memory, and apply-
ing similarity-based reasoning on these examples in
order to solve new ones. Memory-based algorithms
have been previously applied to semantic role la-
beling and parsing separately (Morante et al, 2008;
Canisius and Tjong Kim Sang, 2007).
We briefly discuss the issue of true joint learning
of two tasks in Section 2. The system is described
in Section 3, Section 4 presents and discusses the
results, and in Section 5 we put forward some con-
clusions and future research.
2 Joint learning
When two tasks share the same feature space, there
is the natural option to merge them and consider the
merge as a single task. The merging of two tasks
will typically lead to an increase in the number of
classes, and generally a more complex class space.
In practice, if two combined tasks are to some ex-
25
tent related, the increase will tend to be less than the
product of the number of classes in the two original
tasks, as classes from both tasks will tend to cor-
relate. Yet, even a mild increase of the number of
classes leads to a further fragmentation of the class
space, and thus to less training examples per class
label. Joint learning can therefore only lead to posi-
tive results if the data sparsity effect of the fragmen-
tation of the class space is counter-balanced by an
improved learnability.
Here, we treat the syntactic and semantic tasks as
one and the same task. At the word level, we merge
the class labels of the two tasks into single labels,
and present the classifiers with these labels. Further
on in our system, as we describe in the next sec-
tion, we do make use of the compositionality of the
labels, as the semantic and syntactic output spaces
represented two different types of structure.
3 System description
The joint system that we submitted works in
two phases: a classification phase in which three
memory-based classifiers predict different aspects of
joint syntactic and semantic labeling, and a ranking
phase in which the output of the classifiers is com-
bined. Additionally, a memory-based classifier is
used for predicate sense disambiguation. As a first
step, before generating the instances of the classi-
fiers we merge the semantic and syntactic dependen-
cies into single labels. The merged version of the
dependencies from an example sentence is shown
in Table 1, where column MERGED DEPs contains
all the dependencies of a token separated by a blank
space expressed in labels with the following format:
PHEAD::PDEPREL:APRED.
3.1 Phase 1: Classification
In the classification phase, three classifiers predict
different local aspects of the global output structure.
The classifiers have been optimized for English, by
training on the full training set and testing on the
development set; these optimized settings were then
used for the other six languages. We experimented
with manually selected parameters and with param-
eters selected by a genetic algorithm, but the param-
eters found by the genetic algorithm did not yield
better results than the manually selected parameters.
N Token Merged Dependencies
1 Housing 2::NMOD:A1
2 starts 2:: :A2 3::SBJ: 4:: :A1 6:: :A1 13:: :A0
3 are 0::ROOT:
4 expected 3::VC:
5 to 4::OPRD:C-A1
6 quicken 5::IM:
7 a 8::NMOD:
8 bit 6::OBJ:A2
9 from 6::ADV:A3
10 August 13::NMOD:AM-TMP
11 ?s 10::SUFFIX:
12 annual 13::NMOD:AM-TMP
13 pace 9::PMOD:
14 of 13::NMOD:A2
15 1,350,000 16::NMOD:
16 units 14::PMOD:
17 . 3::P:
Table 1: Example sentence with merged depen-
dency labels.
3.1.1 Classifier 1: Pairwise semantic and
syntact dependencies
Classifier 1 predicts the merged semantic and syn-
tactic dependencies that hold between two tokens.
Instances represent combinations of pairs of tokens
within a sentence. Each token is combined with all
other tokens in the sentence. The class predicted is
the PDEPREL:APRED label. The amount of classes
per language is shown in Table 2 (?Classifier 1?).
Number of classes
Lang. Classifier 1 Classifier 2
Cat 111 111
Chi 309 1209
Cze 395 1221
Eng 351 1957
Ger 152 300
Jap 103 505
Spa 124 124
Table 2: Number of classes per language predicted
by Classifiers 1 and 2.
We use an IB1 memory?based algorithm as im-
plemented in TiMBL (version 6.1.2) 1, a memory-
based classifier based on the k-nearest neighbor
1TiMBL: http://ilk.uvt.nl/timbl
26
rule. The IB1 algorithm was parameterised by us-
ing modified value difference as the similarity met-
ric, gain ratio for feature weighting, using 11 k-
nearest neighbors, and weighting the class vote of
neighbors as a function of their inverse linear dis-
tance. Because of time limitations we used TRIBL
for Czech and Chinese to produce the official results,
although we also provide postevaluation results pro-
duced with IB1. TRIBL is a hybrid combination
of IB1 and IGTREE, a fast decision-tree approxi-
mation of k-NN (Daelemans and van den Bosch,
2005), trading off fast decision-tree lookup on the
most important features (in our experiments, five)
with slower k-NN classification on the remaining
features.
The features2 used by this classifier are:
? The word, lemma, POS and FILLPRED3 of the to-
ken, the combined token and of two tokens before
and after token and combined token.
? POS and FILLPRED of the third token before and
after token and combined token.
? Distance between token and combined token, loca-
tion of token in relation to combined token.
Because data are skewed towards the NONE
class, we downsampled the training instances so that
there would be a negative instance for every positive
instance. Instances with the NONE class to be kept
were randomly selected.
3.1.2 Classifier 2: Per-token relations
Classifier 2 predicts the labels of the dependency
relations of a token with its syntactic and/or seman-
tic head(s). Instances represent a token. As an ex-
ample, the instance that represents token 2 in Table 1
would have as class: :A2-SBJ: - :A1- :A1- :A0.
The amount of classes per language is shown in Ta-
ble 2 under ?Classifier 2?. The number of classes
exceeds 1,000 for Chinese, Czech, and English.
The features used by the classifier are the word,
lemma, POS and FILLPRED of the token and two
tokens before and after the token. We use the IB1
memory?based algorithm parameterised in the same
way as Classifier 1.
2POS refers to predicted part-of-speech and lemma to pre-
dicted lemma in the description of features for all classifiers.
3The FILLPRED column has value Y if a token is a predi-
cate.
3.1.3 Classifier 3: Pairwise detection of a
relation
Classifier 3 is a binary classifier that predicts
whether two tokens have a dependency relation. In-
stance representation follows the same scheme as
with Classifier 1. We use the IGTREE algorithm as
implemented in TiMBL. The data are also skewed
towards the NONE class, so we downsampled the
training instances so that there would be a negative
instance for every four positive instances.
The features used by this classifier are:
? The word, lemma, POS and FILLPRED of the to-
ken, of the combined token, and of two tokens be-
fore and after the token.
? Word and lemma of two tokens before and after
combined token.
? Distance between token and combined token.
3.1.4 Results
The results of the Classifiers are presented in Ta-
ble 3. The performance of Classifiers 1 and 3 is sim-
ilar across languages, whereas the scores for Clas-
sifier 2 are lower for Chinese, Czech and English.
This can be explained by the fact that the number of
classes that Classifier 2 predicts for these languages
is significantly higher.
Lang. C1 C2 C3
Cat 94.77 86.30 97.96
Chi 92.10 70.11 95.47
Cze 87.33 67.87 93.88
Eng 94.17 76.16 95.37
Ger 92.76 83.23 93.77
Jap 91.55 81.22 96.75
Spa 94.76 84.40 96.39
Table 3: Micro F1 scores per classifier (C) and per
language.
Training times for the three classifiers were rea-
sonably short, as is to be expected with memory-
based classification. With English, C2 takes just
over two minutes to train, and C3 half a minute. C1
takes 8 hours and 18 minutes, due to the much larger
amount of examples and features.
3.2 Phase 2: Ranking
The classifier that is at the root of generating the
desired output (dependency graphs and semantic
27
role assignments) is Classifier 1, which predicts the
merged semantic and syntactic dependencies that
hold between two tokens (PDEPREL:APRED la-
bels). If this classifier would be able to predict the
dependencies with 100% accuracy, no further pro-
cessing would be necessary. Naturally, however, the
classifier predicts incorrect dependencies to a certain
degree, and does not provide a graph in wich all to-
kens have at least a syntactic head. It achieves 51.3%
labeled macro F1. The ranking phase improves this
performance. This is done in three steps: (i) ranking
the predictions of Classifier 1; (ii) constructing an
intermediate dependency tree, and (iii) adding extra
semantic dependencies to the tree.
3.2.1 Ranking predictions of Classifier 1
In order to disambiguate between all possible de-
pendencies predicted by this classifier, the system
applies ranking rules. It analyses the dependency
relations that have been predicted for a token with
its potential parents in the sentence and ranks them.
For example, for a sentence with 10 tokens, the sys-
tem would make 10 predictions per token. The pre-
dictions are first ranked by entropy of the class dis-
tribution for that prediction, then using the output of
Classifier 2, and next using the output of Classifier 3.
Ranking by entropy In order to compute entropy
we use the (inverse-linear) distance-weighted class
label distributions among the nearest neighbors that
Classifier 1 was able to find. For example, the pre-
diction for an instance can be: { NONE (2.74),
NMOD: (0.48) }. We can compute the entropy for
this instance using the formula in (1):
?
n?
i=1
P (labeli)log2(P (labeli)) (1)
with
- n: the total number of different labels in the distri-
bution, and
- P (labeli): the weight of label ithe total sum of the weights in the distribution
The system ranks the prediction with the lowest
entropy in position 1, while the prediction with the
highest entropy is ranked in the last position. The
rationale behind this is that the lower the entropy,
the more certain the classifier is about the predicted
dependency. Table 4 lists the first six heads for the
predicate word ?starts? ranked by entropy (cf. Ta-
ble 1).
Head Predicted label Distribution Entropy
Housing NONE { NONE (8.51) } 0.0
expected :A1 { :A1 (5.64) } 0.0
to NONE { NONE (4.74) } 0.0
quicken :A0 { :A0 (4.13), :A1 (0.18), :A2 (0.31) } 0.56
are NONE { NONE (2.56), SBJ: (0.52) } 0.65
starts :A0 { :A0 (7.90), :A1 (0.61), :A2 (1.50) } 0.93
Table 4: Output of Classifier 1 for the first six heads
of ?starts?, ranked by entropy.
On the development data for English, applying
this rule causes a marked error reduction of 26.5%
on labeled macro F1: from 51.3% to 64.2%.
Ranking by Classifier 2 The next ranking step is
performed by using the predictions of Classifier 2,
i.e. the estimated labels of the dependency rela-
tions of a token with its syntactic and/or semantic
head(s). The system ranks the predictions that are
not in the set of possible dependencies predicted by
Classifier 2 at the bottom of the ranked list.
Head Predicted label Distribution Entropy
expected :A1 { :A1 (5.64) } 0.0
Housing NONE { NONE (8.51) } 0.0
to NONE { NONE (4.74) } 0.0
quicken :A0 { :A0 (4.13), :A1 (0.18), :A2 (0.31) } 0.56
are NONE { NONE (2.56), SBJ: (0.52) } 0.65
starts :A0 { :A0 (7.90), :A1 (0.61), :A2 (1.50) } 0.93
Table 5: Output of Classifier 1 for the first six heads
of ?starts?. Ranked by entropy and Classifier 2.
Because this is done after ranking by entropy, the
instances with the lowest entropy are still at the top
of the list. Table 5 displays the re-ranked six heads
of ?starts?, given that Classifier 2 has predicted that
possible relations to heads are SBJ:A1 and :A1, and
given that only ?expected? is associated with one of
these two relations.
On the development data for English, applying
this rule induces a 9.0% error reduction on labeled
macro F1: from 64.2% to 67.4%.
Ranking by Classifier 3 The final ranking step
makes use of Classifier 3, which predicts the rela-
tion that holds between two tokens. The dependency
relations predicted by Classifier 1 that are not con-
firmed by Classifier 3 predicting that a relation exists
are moved to the end of the ranked list. Table 6 lists
the resulting ranked list. On the development data
for English, applying this rule yields another 5.2%
28
error reduction on labeled macro F1: from 67.4% to
69.1%.
Head Predicted label Distribution Entropy
expected :A1 { :A1 (5.64) } 0.0
quicken :A0 { :A0 (4.13), :A1 (0.18), :A2 (0.31) } 0.56
starts :A0 { :A0 (7.90), :A1 (0.61), :A2 (1.50) } 0.93
Housing NONE { NONE (8.51) } 0.0
to NONE { NONE (4.74) } 0.0
are NONE { NONE (2.56), SBJ: (0.52) } 0.65
Table 6: Output of Classifier 1 for the first six heads
of ?starts?. Ranked by entropy, Classifier 2, and
Classifier 3.
3.2.2 Construction of the intermediate
dependency tree
After ranking the predictions of Classifier 1, the
system selects a syntactic head for every token. This
is motivated by the fact that every token has one and
only one syntactic head. The system selects the pre-
diction with the best ranking that has in the PDE-
PREL part a value different than ? ?.
The intermediate tree can have more than one root
or no root at all. To make sure that every sentence
has one and only one root we apply some extra rules.
If the sentence does not have a token with a root la-
bel, the system checks the distributions of Classi-
fier 1. The token with the rootlabel in its distribution
that is the head of the biggest number of tokens is
taken as root. If the intermediate tree has more than
one root, the last root is taken as root. The other root
tokens get the label with a syntax part (PDEPREL)
that has the highest score in the distribution of Clas-
sifier 1.
The product of this step is a tree in which ev-
ery token is uniquely linked to a syntactic head.
Because syntactic and semantic dependencies have
been linked, the tree contains also semantic depen-
dencies. However, the tree is missing the purely se-
mantic dependencies. The next step adds these rela-
tions to the dependency tree.
3.2.3 Adding extra semantic dependencies
In order to find the tokens that have only a seman-
tic relation with a predicate, the system analyses for
each predicate (i.e. tokens marked with Y in FILL-
PRED) the list of predictions made by Classifier 1
and selects the predictions in which the PDEPREL
part of the label is ? ? and the APRED part of the
label is different than ? ?. On the development data
for English, applying this rule produces a 6.7% er-
ror reduction on labeled macro F1: from 69.1% to
71.1%.
3.3 Predicate sense disambiguation
Predicate sense disambiguation is performed by a
classifier per language that predicts the sense of the
predicate, except for Japanese, as with that language
the lemma is taken as the sense. We use the IGTREE
algorithm. Instances represent predicates and the
features used are the word, lemma and POS of the
predicate, and the lemma and POS of two tokens be-
fore and after the predicate. The results per language
are presented in Table 7.
Lang. Cat Chi Cze Eng Ger Spa
F1 82.40 94.85 87.84 93.64 73.57 81.13
Table 7: Micro F1 for the predicate sense disam-
biguation.
4 Overall results
The system was developed by training on the train-
ing set provided by the task organisers and testing
on the development set. The final results were ob-
tained by testing on the testing set. Table 8 shows
the global results of the system for syntactic and se-
mantic dependencies.
Lang. F1 Precision Recall
Cat 73.75 74.91 72.63
Chi 67.16 68.09 66.26
Chi* 67.79 68.70 66.89
Cze 60.50 62.55 58.58
Cze* 68.68 70.38 67.07
Eng 78.19 79.69 76.74
Ger 67.51 69.52 65.62
Jap 77.75 81.91 73.98
Spa 70.78 71.34 70.22
Av. 70.81 72.57 69.15
Table 8: Macro F1, precision and recall for all de-
pendencies per language. Postevaluation results are
marked with *.
Table 9 shows the scores of syntactic and seman-
tic dependencies in isolation.
29
Syntax Semantics
Lang. LA F1 Precision Recall
Cat 77.33 70.14 72.49 67.94
Chi 67.58 66.71 68.59 64.93
Chi* 67.92 67.63 69.48 65.86
Cze 49.41 71.49 75.68 67.75
Cze* 60.03 77.28 80.73 74.11
Eng 80.35 75.97 79.04 73.13
Ger 73.88 61.01 65.15 57.36
Jap 86.17 68.82 77.66 61.80
Spa 73.07 68.48 69.62 67.38
Av. 72.54 68.95 72.60 65.76
Table 9: Labeled attachment (LA) score for syntac-
tic dependencies and Macro F1, precision and recall
of semantic dependencies per language. Postevalua-
tion results are marked with *.
5 Conclusions
In this paper we presented the system that we sub-
mitted to the ?closed? challenge of the CoNLL
Shared Task 2009. We observe fairly low scores,
which can be possibly improved for all languages by
making use of the available morpho-syntactic fea-
tures, which we did not use in the present system,
by optimising the classifiers per language, and by
improving the reranking algorithm. We also ob-
serve a relatively low recall on the semantic task as
compared to overall recall, indicating that syntactic
dependencies are identified with a better precision-
recall balance. A logical continuation of this study
is to compare joint learning to learning syntactic and
semantic dependencies in isolation, using the same
architecture. Only then will we be able to put for-
ward conclusions about the performance of a joint
learning system versus the performance of a system
that learns syntax and semantics independently.
Acknowledgments
This study was made possible through financial sup-
port from the University of Antwerp (GOA project
BIOGRAPH), and from the Netherlands Organisa-
tion for Scientific Research.
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
S. Canisius and E. Tjong Kim Sang. 2007. A con-
straint satisfaction approach to dependency parsing.
In Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 1124?1128.
W. Daelemans and A. van den Bosch. 2005. Memory-
based language processing. Cambridge University
Press, Cambridge, UK.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
R. Morante, W. Daelemans, and V. Van Asch. 2008. A
combined memory-based semantic role labeler of en-
glish. In Proc. of the CoNLL 2008, pages 208?212,
Manchester, UK.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008).
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
30
