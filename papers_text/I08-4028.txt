An Improved CRF based Chinese Language Processing System for SIGHAN
Bakeoff 2007
Xihong Wu, Xiaojun Lin, Xinhao Wang, Chunyao Wu, Yaozhong Zhang and Dianhai Yu
Speech and Hearing Research Center
State Key Laboratory of Machine Perception,
Peking University, China, 100871
{wxh,linxj,wangxh,wucy,zhangyaoz,yudh}@cis.pku.edu.cn
Abstract
This paper describes three systems: the
Chinese word segmentation (WS) system,
the named entity recognition (NER) sys-
tem and the Part-of-Speech tagging (POS)
system, which are submitted to the Fourth
International Chinese Language Processing
Bakeoff. Here, Conditional Random Fields
(CRFs) are employed as the primary mod-
els. For the WS and NER tracks, the n-
gram language model is incorporated in our
CRFs based systems in order to take into ac-
count the higher level language information.
Furthermore, to improve the performances
of our submitted systems, a transformation-
based learning (TBL) technique is adopted
for post-processing.
1 Introduction
Among 24 closed and open tracks in this bakeoff, we
participated in 23 tracks, except the open NER track
of MSRA. Our systems are ranked 1st in 6 tracks,
and get close to the top level in several other tracks.
Recently, Maximum Entropy model(ME) and
CRFs (Low et al, 2005)(Tseng et al, 2005) (Hai
Zhao et al, 2006) turned out to be promising in natu-
ral language processing tracks, and obtain excellent
performances on most of the test corpora of Bake-
off 2005 and Bakeoff 2006. Compared to the gen-
erative models, like HMM, the primary advantage
of CRFs is that it relaxes the independence assump-
tions, which makes it able to handle multiple inter-
acting features between observation elements (Wal-
lach et al, 2004).
However, the ME and CRFs emphasize the rela-
tion of the basic units of sequence, like the Chinese
characters in these tracks. While, the higher level
information, like the relationship of the words is ig-
nored. From this point of view, the n-gram language
model is incorporated in our CRFs based systems in
order to cover the word level language information.
Based on several pilot-experimental results, we
found that the tagging errors always follow some
patterns. In order to find those error patterns and cor-
rect the similar errors, we integrated the TBL post-
processor in our systems. In addition, extra train-
ing data, which is transformed from People Daily
Corpus (Shiwen Yu et al, 2000) with some auto-
extracted transition rules, is used in each corpus for
the open tracks of WS.
The remainder of this paper is organized as fol-
lows. The scheme of our three developed systems
are described in section 2, 3 and 4, respectively. In
section 5, evaluation results based on these systems
are enumerated and discussed. Finally some conclu-
sions are drawn in section 6.
2 Word Segmentation
The WS system mainly consists of three compo-
nents, CRFs, n-gram language model and post-
processing strategies.
2.1 Conditional Random Fields
Conditional Random Fields, as the statistical se-
quence labeling models, achieve great success in
natural language processing, such as chunking (Fei
Sha et al, 2003) and word segmentation (Hai Zhao
et al, 2006). Different from traditional generative
155
Sixth SIGHAN Workshop on Chinese Language Processing
model, CRFs relax the constraint of the indepen-
dence assumptions, and therefore turn out to be more
suitable for natural language tasks.
CRFs model the conditional distribution p(Y |X)
of the labels Y given the observations X directly
with the formulation:
P?(Y |X) = 1Z(X)exp{
?
c?C
?
k
?kfk(Yc, X, c)}
(1)
Y is the label sequence, X is the observation se-
quence, Z(X) is a normalization term, fk is a fea-
ture function, and c is the set of cliques in Graphic.
In our tasks, C = {(yi?1, yi)}, X is the Chinese
character sequence of a sentence.
To label a Chinese character, we need to define
the label tags. Here we have six types of tags ac-
cording to character position in a word (Hai Zhao et
al., 2006):
tag = {B1, B2, B3, I, E, S}
?B1, B2, B3, I, E? represent the first, second, third,
continue, and end character positions in a multi-
character word, and ?S? is the single-character word
tag.
The unigram feature templates used here are:
Cn (n = ?2,?1, 0, 1, 2)
CnCn+1 (n = ?2,?1, 0)
CnCn+1Cn+2 (n = ?1)
Where C0 refers to the current character and
C?n(Cn) is the nth character to the left(right) of the
current character. We also use the basic bigram fea-
ture template which denotes the dependency on the
previous tag and current tag.
2.2 Multi-Model Integration
In order to integrate multi-model information, we
use a log-linear model(Och et al, 2002) to compute
the posterior probability:
Pr (W |C) = p?M1 (W |C)
= exp[
?M
m=1 ?mhm(W,C)]
?
W ? exp[
?M
m=1 ?mhm(W ?, C)]
(2)
Where W is the word sequence, and C is the char-
acter sequence. The decision rule here is:
W0 = argmaxW {Pr(W |C)}
= argmaxW {
M
?
m=1
?mhm(W,C)} (3)
The parameters ?M1 of this model can be opti-
mized by standard approaches, such as the Mini-
mum Error Rate Training used in machine transla-
tion (Och, 2003). In fact, the CRFs approach is
a special case of this framework when we define
M = 1 and use the following feature function:
h1(W,C) = logP?(Y |X) (4)
In our approach, the logarithms of the scores gen-
erated by the two kinds of models are used as feature
functions:
h1(W,C) = logPcrf (W,C)
= log
?
w
i
P?(wi|C) (5)
h2(W,C) = logPlm(W ) (6)
The first feature function(Eq.5) comes from CRFs.
Instead of computing the score of the whole la-
bel sequence Y with character sequence X through
P?(Y |X) directly, we try to get the posterior prob-
ability of a sub-sequence to be tagged as one whole
word P?(wi|C). Then we combine all the score of
words together. The second feature function(Eq.6)
comes from n-gram language model, which aims to
catch the words information.
The log-linear model with the feature functions
described above allows the dynamic programming
search algorithm for efficient decoding. The system
generates the word lattice with posterior probability
P?(wi|C). Then the best word sequence is searched
on the word lattice with the decision rule(Eq.3).
Since arbitrary sub-sequence can be viewed as a
candidate word in word lattice, we need to deal with
the problem of OOV words. The unigram of an OOV
word is estimated as:
Unigram(OOV Word) = pl (7)
where p is the minimal value of unigram scores in
the language model; l is the length of the OOV
word, which is used as a punishment factor to
avoid overemphasizing the long OOV words (Xin-
hao Wang et al, 2006).
2.3 Post-Processing Strategies
The division and combination rule, which has been
proved to be useful in our system of Bakeoff 2006
(Xinhao Wang et al, 2006), is adopted for the post-
processing in the system.
156
Sixth SIGHAN Workshop on Chinese Language Processing
2.4 Training Data Transition
For the WS open tracks, the unique difference from
closed tracks is that the additional training data is
supplemented for model refinement.
For the Simplified Chinese tracks, the additional
training data are collected from People Daily Cor-
pus with a set of auto-extracted transition rules. This
process is performed in a heuristic strategy and con-
tains five steps as follows:
(1) Segment the raw People Daily texts with the cor-
responding system for the closed track of each cor-
pus.
(2) Compare the result of step 1 with People Daily
Corpus to get the conflict pairs. For example,
{pair1: ??? vs. ???}
(Zhemin Jiang)
{pair2: ??? vs. ???}
(catch with two hands)
In each pair, the left phrase follows the People Daily
Corpus segmentation guideline, while the right one
is the phrase obtained from step 1.
(3) Divide the pairs into two sets: the first set con-
tains the pairs with right phrase appearing in the tar-
get training data; the other pairs are in the second
set.
(4) Select sentences which contain the left phrase of
the pairs in the second set from People Daily Cor-
pus.
(5) Transform these selected sentences by replacing
their phrase in the left side of the pair in the first set
to the right one. This is used as our transition rules.
3 Named Entity Recognition
The named entity recognition track is viewed as a
character sequence tagging problem in our NER sys-
tem and the log-linear model mentioned above is
employed again to integrate multi-model informa-
tion. To find the error patterns and correct them,
a TBL strategy is then used in the post-processing
module.
3.1 Model Description
In this NER track, we employe the log-linear model
and use the logarithms of the scores generated by the
two types of models as feature functions. Besides
CRFs, another model is the class-based n-gram lan-
guage model:
h1(Y, X) = logPcrf (Y, X)
= logP?(Y |X) (8)
h2(Y, X) = logPclm(Y, X) (9)
Y is the label sequence and X is the character se-
quence.
CRFs are used to generate the N-best tagging re-
sults with the scores of whole label sequence Y on
character sequence X by P?(Y |X). And then, the
log-linear model is used to reorder the N-best tag-
ging results by integrating the CRFs score and the
class-based n-gram language model score together.
CRFs
In this track, one Chinese character is labeled by
a tag of ten classes, which denoting the beginning,
continue, ending character of a specified named en-
tity or a non-entity character. There are three types
of named entities in these tracks, including person
name, location name and organization name.
In CRFs, the basic features used here are:
Cn (n = ?2,?1, 0, 1, 2)
CnCn+1 (n = ?2,?1, 0, 1)
CnCn+2 (n = ?1)
Besides basic unigram features, the bigram transi-
tion features considering the previous tag is adopted
with template Cn (n = ?2,?1, 0, 1, 2).
Class-Based N-gram Language Model
For the class-based n-gram language model, we
define that each character is a single class, while
each type of named entity is viewed as a single class.
With the character sequence and label sequence, the
class sequence can be generated. Take this sentence
for instance:
???????????
(But Ibrahimov is not satisfied)
Table 1 shows its class sequence. Class-based n-
gram language model can be trained with class se-
quence.
3.2 TBL
Since the analysis on our experiments shows that the
tagging errors always follow some patterns in NER
track, TBL strategy is adopted in our system to find
these patterns and correct the similar errors.
157
Sixth SIGHAN Workshop on Chinese Language Processing
character sequence ? ? ? ? ? ? ? ? ? ? ?
label sequence N Per-B Per-C Per-C Per-C Per-C Per-E N N N N
class sequence ? PERSON ? ? ? ?
Table 1: A class sequence example
Transformation-based learning is a symbolic ma-
chine learning method, introduced by (Eric Brill,
1995). The main idea in TBL is to generate a set of
transformation rules that can correct tagging errors
produced by the initial process.
There are four main procedures in our TBL
framework: An initial state assignment which is op-
erated by the system we described above; a set of al-
lowable templates for rules, ranging from words in
a 3 positions windows and name entity information
in a 3-word window with their combinations consid-
ered, and rules which are learned according to the
tagging differences between training data and results
generated by our system, at last, those rules are in-
troduced to correct similar errors.
4 POS Tagging
The POS tagging track is to assign the part-of-
speech sequence for the correctly segmented word
sequence. In our system, for the CTB corpus, the
CRFs are adopted; however for the other four cor-
pora, considering the limitations of resources and
time, the ME model is adopted. To improve the per-
formance of ME model, the POS tag of the previous
word is taken as a feature and the dynamic program-
ming strategy is used in decoding.
In the closed track, the features include the basic
features and their combined features. Firstly the pre-
vious and next words of the current word are taken
as the basic features. Secondly, based on the anal-
ysis of the OOV words, the first and last characters
of the current word, as well as the length of the cur-
rent word are proven to be effective features for the
OOV POS. Furthermore since the long distance con-
straint word may impact the POS of current word
(Yan Zhao et al, 2006), in the open track, a Chi-
nese parser is imported and the word depended on
the current word is extracted as feature.
5 Experiments and Results
We have participated in 23 tracks, except the open
NER track of MSRA. CRFs, ME model and n-gram
language model are adopted in these systems. Our
implementation uses the CRF++ package1 provided
by Taku Kudo, the Maximum Entropy Toolkit2 pro-
vided by Zhang Le, and the SRILM Toolkit provided
by Andreas Stolcke (Andreas Stolcke et al, 2002).
5.1 Chinese Word Segmentation
In the closed tracks, CRFs and bigram language
model are trained on the given training data for each
corpus. In order to integrate these two models, it is
necessary to train the corresponding parameter ?M1
with Minimum Error Rate Training approache based
on a development data. Since the development data
is not provided in this bakeoff, a ten-fold cross val-
idation approach is employed to implement the pa-
rameter training. A set of parameters can be trained
independently, and then the mean value is calculated
as the estimation of each parameter.
Table 2 gives the results of our WS system for
closed tracks.
baseline +LM +LM+Post
CTB 94.7 94.7 94.8
NCC 92.6 92.4 92.9
SXU 94.7 95.7 95.8
CITYU 92.9 93.7 93.9
CKIP 93.2 93.7 93.7
Table 2: Word segmentation performance on F-
value with different approach for the closed tracks
In the open tracks, as we do not have enough time
to finish the parameter estimation on the new data,
our system adopt the same parameters ?M1 used in
closed tracks. The unique difference from closed
1http://chasen.org/taku/software/CRF++
2http://homepages.inf.ed.ac.uk/s0450736/maxent
toolkit.html
158
Sixth SIGHAN Workshop on Chinese Language Processing
tracks is that extra training data is added for each
corpus to improve the performance. For the Sim-
plified Chinese tracks, additional data comes from
People Daily Corpus which is transformed by our
transition strategy. At the same time, for the Tra-
ditional Chinese tracks, additional data comes from
the training and testing data used in the early Bake-
off. However, we implement two systems for the
CTB open track. The system (a) takes the training
and testing data used in the early Bakeoff as addi-
tional data, and System (b) takes the translated Peo-
ple Daily Corpus as additional data. Table 3 gives
the results of our open WS system.
baseline +LM +LM+Post
CTB(a) 99.2 99.2 99.3
CTB(b) 95.6 95.1 97.0
NCC 93.7 93.0 92.9
SXU 96.4 87.0 95.8
CITYU 95.8 90.6 91.0
CKIP 94.5 94.8 95.1
Table 3: Word segmentation performance on F-
value with different approach for the open tracks
The result shows that the system performance is
sensitive to the parameters ?M1 . Although we train
the useful parameter for closed tracks, it plays a bad
role in open tracks as we do not adapt it for the ad-
ditional training data.
5.2 Named Entity Recognition
In the closed NER tracks, CRFs and class-based tri-
gram language model are trained on the given train-
ing data for each corpus. The same approach em-
ployed in the WS tracks is adopted to train the corre-
sponding parameter ?M1 in our NER systems. Mean-
while, the TBL rules trained via five-fold cross val-
idation approach are also used in post-processing
procedure. Table 4 reports the results of our closed
NER system.
5.3 POS Tagging
The experiments show that the CRFs/ME method is
superior to the TBL method, and the concurrent er-
rors for these two methods are less than 60%. There-
fore we adopted TBL to correct the output results
of CRFs/ME: If the output tags of CRFs/ME and
baseline +LM +LM+Post
MSRA 89.3 89.7 89.9
CITYU 79.3 80.6 80.5
Table 4: Named entity recognition F-value through
different approaches for the closed tracks
TBL are not consistent and the output probability
of CRFs/ME is below a certain threshold, the TBL
results are fixed. Here the 90% of the training set
is taken as the training data and remained 10% is
separated as the development data to get the thresh-
old, which is 0.60 for the CRFs, and 0.90 for the
ME. In addition, the POS tagged corpus of the Chi-
nese Treebank 5.0 from LDC is added to the training
data for CTB open track. In our system, the Berke-
ley Parser (Slav Petrov et al, 2006) is adopted to
obtain the long distance constraint words. The per-
formance achieved by the methods described above
on each corpus are reported in Table 5.
CRFs/ME CRFs/ME
CRFs/ME TBL +TBL +TBL
+Syntax
CTIYU 88.7 87.7 89.1 89.0
CKIP 91.8 91.4 92.2 92.1
CTB 94.0 92.7 94.3 96.5
NCC 94.6 94.3 94.9 95.0
PKU 93.5 93.2 94.0 94.1
Table 5: POS tagging performance on total-accuracy
with different approach
6 Conclusion
In this paper, we have briefly described our systems
participating in the Bakeoff 2007. In the WS and
NER systems, the log-linear model is adopted to in-
tegrate CRFs and language model, which improves
the system performances effectively. At the same
time, system integration approach used in the POS
system also proves its validity. In addition, a heuris-
tic strategy is imported to generate additional train-
ing data for the open WS tracks. Finally, several
post-processing strategies are used to further im-
prove our systems.
159
Sixth SIGHAN Workshop on Chinese Language Processing
References
Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo. 2005.
A Maximum Entropy Approach to Chinese Word Seg-
mentation. Proceedings of the Fourth SIGHAN Work-
shop on Chinese Language Processing. pp. 161-164.
Jeju Island, Korea.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, Christopher Manning. 2005. A Conditional
Random Field Word Segmenter for Sighan Bakeoff
2005. Proceedings of the Fourth SIGHAN Workshop
on Chinese Language Processing. pp. 168-171. Jeju
Island, Korea.
Hai Zhao, Chang-Ning Huang and Mu Li. 2006. An
Improved Chinese Word Segmentation System with
Conditional Random Field. Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Processing.
pp. 162-165. Sydney, Australia.
Hanna M. Wallach. 2004. Conditional Random Fields:
An Introduction. Technical Report, UPenn CIS TR
MS-CIS-04-21.
Shiwen Yu, Xuefeng Zhu and Huiming Duan. 2000.
Specification of large-scale modern Chinese corpus.
Proceedings of ICMLP?2001. pp. 18-24. Urumqi,
China.
Fei Sha and Fernando Pereira. 2003. Shallow Parsing
with Conditional Random Fields. Proceedings of Hu-
man Language Technology/NAACL. pp. 213-220. Ed-
monton, Canada.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL). pp. 295-302. Philadelphia, PA.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. Proceedings of
the 41th Annual Meeting of the Association for Com-
putational Linguistics (ACL). pp. 160-167. Sapporo,
Japan.
Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, Xi-
hong Wu. 2006. Chinese Word Segmentation with
Maximum Entropy and N-gram Language Model. the
Fifth SIGHAN Workshop on Chinese Language Pro-
cessing. pp. 138-141. Sydney, Australia.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case study
in Part-of-Speech tagging. Computational Lingusitics.
21(4).
Yan Zhao, Xiaolong Wang, Bingquan Liu, and Yi Guan.
2006. Fusion of Clustering Trigger-Pair Features for
POS Tagging Based on Maximum Entropy Model.
Journal of Computer Research and Development.
43(2). pp. 268-274.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. Proceedings of International
Conference on Spoken Language Processing. pp. 901-
904. Denver, Colorado.
Slav Petrov, Leon Barrett, Romain Thibaux and Dan
Klein. 2006. Learning Accurate, Compact, and Inter-
pretable Tree Annotation. Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th annual meeting of the ACL. pp. 433-440.
Sydney, Australia.
160
Sixth SIGHAN Workshop on Chinese Language Processing
