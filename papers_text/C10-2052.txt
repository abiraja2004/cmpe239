Coling 2010: Poster Volume, pages 454?462,
Beijing, August 2010
What?s in a Preposition?
Dimensions of Sense Disambiguation for an Interesting Word Class
Dirk Hovy, Stephen Tratz, and Eduard Hovy
Information Sciences Institute
University of Southern California
{dirkh, stratz, hovy}@isi.edu
Abstract
Choosing the right parameters for a word
sense disambiguation task is critical to
the success of the experiments. We ex-
plore this idea for prepositions, an of-
ten overlooked word class. We examine
the parameters that must be considered in
preposition disambiguation, namely con-
text, features, and granularity. Doing
so delivers an increased performance that
significantly improves over two state-of-
the-art systems, and shows potential for
improving other word sense disambigua-
tion tasks. We report accuracies of 91.8%
and 84.8% for coarse and fine-grained
preposition sense disambiguation, respec-
tively.
1 Introduction
Ambiguity is one of the central topics in NLP. A
substantial amount of work has been devoted to
disambiguating prepositional attachment, words,
and names. Prepositions, as with most other word
types, are ambiguous. For example, the word in
can assume both temporal (?in May?) and spatial
(?in the US?) meanings, as well as others, less
easily classifiable (?in that vein?). Prepositions
typically have more senses than nouns or verbs
(Litkowski and Hargraves, 2005), making them
difficult to disambiguate.
Preposition sense disambiguation (PSD) has
many potential uses. For example, due to the
relational nature of prepositions, disambiguating
their senses can help with all-word sense disam-
biguation. In machine translation, different senses
of the same English preposition often correspond
to different translations in the foreign language.
Thus, disambiguating prepositions correctly may
help improve translation quality.1 Coarse-grained
PSD can also be valuable for information extrac-
tion, where the sense acts as a label. In a recent
study, Hwang et al (2010) identified preposition
related features, among them the coarse-grained
PP labels used here, as the most informative fea-
ture in identifying caused-motion constructions.
Understanding the constraints that hold for prepo-
sitional constructions could help improve PP at-
tachment in parsing, one of the most frequent
sources of parse errors.
Several papers have successfully addressed
PSD with a variety of different approaches (Rudz-
icz and Mokhov, 2003; O?Hara and Wiebe, 2003;
Ye and Baldwin, 2007; O?Hara and Wiebe, 2009;
Tratz and Hovy, 2009). However, while it is often
possible to increase accuracy by using a differ-
ent classifier and/or more features, adding more
features creates two problems: a) it can lead to
overfitting, and b) while possibly improving ac-
curacy, it is not always clear where this improve-
ment comes from and which features are actually
informative. While parameter studies exist for
general word sense disambiguation (WSD) tasks
(Yarowsky and Florian, 2002), and PSD accuracy
has been steadily increasing, there has been no
exploration of the parameters of prepositions to
guide engineering decisions.
We go beyond simply improving accuracy to
analyze various parameters in order to determine
which ones are actually informative. We explore
the different options for context and feature se-
1See (Chan et al, 2007) for the relevance of word sense
disambiguation and (Chiang et al, 2009) for the role of
prepositions in MT.
454
lection, the influence of different preprocessing
methods, and different levels of sense granular-
ity. Using the resulting parameters in a Maximum
Entropy classifier, we are able to improve signif-
icantly over existing results. The general outline
we present can potentially be extended to other
word classes and improve WSD in general.
2 Related Work
Rudzicz and Mokhov (2003) use syntactic and
lexical features from the governor and the preposi-
tion itself in coarse-grained PP classification with
decision heuristics. They reach an average F-
measure of 89% for four classes. This shows that
using a very small context can be effective. How-
ever, they did not include the object of the prepo-
sition and used only lexical features for classifi-
cation. Their results vary widely for the different
classes.
O?Hara and Wiebe (2003) made use of a win-
dow size of five words and features from the
Penn Treebank (PTB) (Marcus et al, 1993) and
FrameNet (Baker et al, 1998) to classify prepo-
sitions. They show that using high level fea-
tures, such as semantic roles, significantly aid dis-
ambiguation. They caution that using colloca-
tions and neighboring words indiscriminately may
yield high accuracy, but has the risk of overfit-
ting. O?Hara and Wiebe (2009) show compar-
isons of various semantic repositories as labels for
PSD approaches. They also provide some results
for PTB-based coarse-grained senses, using a five-
word window for lexical and hypernym features in
a decision tree classifier.
SemEval 2007 (Litkowski and Hargraves,
2007) included a task for fine-grained PSD (more
than 290 senses). The best participating system,
that of Ye and Baldwin (2007), extracted part-of-
speech and WordNet (Fellbaum, 1998) features
using a word window of seven words in a Max-
imum Entropy classifier. Tratz and Hovy (2009)
present a higher-performing system using a set of
20 positions that are syntactically related to the
preposition instead of a fixed window size.
Though using a variety of different extraction
methods, contexts, and feature words, none of
these approaches explores the optimal configura-
tions for PSD.
3 Theoretical Background
The following parameters are applicable to other
word classes as well. We will demonstrate their
effectiveness for prepositions.
Analyzing the syntactic elements of preposi-
tional phrases, one discovers three recurring ele-
ments that exhibit syntactic dependencies and de-
fine a prepositional phrase. The first one is the
governing word (usually a noun, verb, or adjec-
tive)2, the preposition itself, and the object of the
preposition.
Prepositional phrases can be fronted (?In May,
prices dropped by 5%?), so that the governor (in
this case the verb ?drop?) occurs later in the sen-
tence. Similarly, the object can be fronted (con-
sider ?a dessert to die for?).
In the simplest version, we can do classification
based only on the preposition and the governor or
object alone.3 Furthermore, directly neighboring
words can influence the preposition, mostly two-
word prepositions such as ?out of? or ?because
of?.
To extract the words discussed above, one can
either employ a fixed window size, (which has
to be large enough to capture the words), or se-
lect them based on heuristics or parsing informa-
tion. The governor and object can be hard to ex-
tract if they are fronted, since they do not occur in
their unusual positions relative to the preposition.
While syntactically related words improve over
fixed-window-size approaches (Tratz and Hovy,
2009), it is not clear which words contribute most.
There should be an optimal context, i.e., the small-
est set of words that achieves the best accuracy. It
has to be large enough to capture all relevant infor-
mation, but small enough to avoid noise words.4
We surmise that earlier approaches were not uti-
lizing that optimal context, but rather include a lot
of noise.
Depending on the task, different levels of sense
granularity may be used. Fewer senses increase
the likelihood of correct classification, but may in-
2We will refer to the governing word, irrespective of
class, as governor.
3Basing classification on the preposition alone is not fea-
sible, because of the very polysemy we try to resolve.
4It is not obvious how much information a sister-PP can
provide, or the subject of the superordinate clause.
455
correctly conflate prepositions. A finer granular-
ity can help distinguish nuances and better fit the
different contexts. However, it might suffer from
sparse data.
4 Experimental Setup
We explore the different context types (fixed win-
dow size vs. selective), the influence of the words
in that context, and the preprocessing method
(heuristics vs. parsing) on both coarse and fine-
grained disambiguation. We use a most-frequent-
sense baseline. In addition, we compare to the
state-of-the-art systems for both types of granu-
larity (O?Hara and Wiebe, 2009; Tratz and Hovy,
2009). Their results show what has been achieved
so far in terms of accuracy, and serve as a second
measure for comparison beyond the baseline.
4.1 Model
We use the MALLET implementation (McCal-
lum, 2002) of a Maximum Entropy classifier
(Berger et al, 1996) to construct our models. This
classifier was also used by two state-of-the-art
systems (Ye and Baldwin, 2007; Tratz and Hovy,
2009). For fine-grained PSD, we train a separate
model for each preposition due to the high num-
ber of possible classes for each individual prepo-
sition. For coarse-grained PSD, we use a single
model for all prepositions, because they all share
the same classes.
4.2 Data
We use two different data sets from existing re-
sources for coarse and fine-grained PSD to make
our results as comparable to previous work as pos-
sible.
For the coarse-grained disambiguation, we use
data from the POS tagged version of the Wall
Street Journal (WSJ) section of the Penn Tree-
Bank. A subset of the prepositional phrases in
this corpus is labelled with a set of seven classes:
beneficial (BNF), direction (DIR), extent (EXT),
location (LOC), manner (MNR), purpose (PRP),
and temporal (TMP). We extract only those prepo-
sitions that head a PP labelled with such a class
(N = 35, 917). The distribution of classes is
highly skewed (cf. Figure 1). We compare the
PTB class distrib
Page 1
LOC TMP DIR MNR PRP EXT BNF
0
2000
4000
6000
8000
10000
12000
14000
16000
18000 16995
10332
5414
1781 1071 280 44
classes
fre
qu
en
cy
Figure 1: Distribution of Class Labels in the WSJ
Section of the Penn TreeBank.
results of this task to the findings of O?Hara and
Wiebe (2009).
For the fine-grained task, we use data from
the SemEval 2007 workshop (Litkowski and Har-
graves, 2007), separate XML files for the 34 most
frequent English prepositions, comprising 16, 557
training and 8096 test sentences, each instance
containing one example of the respective prepo-
sition. Each preposition has between two and 25
senses (9.76 on average) as defined by The Prepo-
sition Project (Litkowski and Hargraves, 2005).
We compare our results directly to the findings
from Tratz and Hovy (2009). As in the original
workshop task, we train and test on separate sets.
5 Results
In this section we show experimental results for
the influence of word extraction method (parsing
vs. POS-based heuristics), context, and feature se-
lection on accuracy. Each section compares the
results for both coarse and fine-grained granular-
ity. Accuracy for the coarse-grained task is in all
experiments higher than for the fine-grained one.
5.1 Word Extraction
In order to analyze the impact of the extraction
method, we compare parsing versus POS-based
heuristics for word extraction.
Both O?Hara and Wiebe (2009) and Tratz and
Hovy (2009) use constituency parsers to prepro-
cess the data. However, parsing accuracy varies,
456
and the problem of PP attachment ambiguity in-
creases the likelihood of wrong extractions. This
is especially troublesome in the present case,
where we focus on prepositions.5 We use the
MALT parser (Nivre et al, 2007), a state-of-the-
art dependency parser, to extract the governor and
object.
The alternative is a POS-based heuristics ap-
proach. The only preprocessing step needed is
POS tagging of the data, for which we used the
system of Shen et al (2007). We then use simple
heuristics to locate the prepositions and their re-
lated words. In order to determine the governor
in the absence of constituent phrases, we consider
the possible governing noun, verb, and adjective.
The object of the preposition is extracted as first
noun phrase head to the right. This approach is
faster than parsing, but has problems with long-
range dependencies and fronting of the PP (e.g.,
the PP appearing earlier in the sentence than its
governor). word selection
Page 1
MALT 84.4 94.0
84.8 90.9
84.8 91.8
extraction method fine coarse
Heuristics
MALT + Heuristics
Table 1: Accuracies (%) for Word-Extraction Us-
ing MALT Parser or Heuristics.
Interestingly, the extraction method does not
significantly affect the final score for fine-grained
PSD (see Table 1). The high score achieved when
using the MALT parse for coarse-grained PSD
can be explained by the fact that the parser was
originally trained on that data set. The good re-
sults we see when using heuristics-based extrac-
tion only, however, means we can achieve high-
accuracy PSD even without parsing.
5.2 Context
We compare the effects of fixed window size ver-
sus syntactically related words as context. Table 2
shows the results for the different types and sizes
of contexts.6
5Rudzicz and Mokhov (2003) actually motivate their
work as a means to achieve better PP attachment resolution.
6See also (Yarowsky and Florian, 2002) for experiments
on the effect of varying window size for WSD.
context
Page 1
91.6 80.4
92.0 81.4
91.6 79.8
91.0 78.7
80.7 78.9
94.2 56.9
94.0 84.8
Context coarse fine
2-word window
3-word window
4-word window
5-word window
Governor, prep
Prep, object
Governor, prep, object
Table 2: Accuracies (%) for Different Context
Types and Sizes
The results show that the approach using both
governor and object is the most accurate one. Of
the fixed-window-size approaches, three words to
either side works best. This does not necessarily
reflect a general property of that window size, but
can be explained by the fact that most governors
and objects occur within this window size.7 This
dista ce can vary from corpus to corpus, so win-
dow size would have to be determined individu-
ally for each task. The difference between using
governor and preposition versus preposition and
object between coarse and fine-grained classifica-
tion might reflect the annotation process: while
Litkowski and Hargraves (2007) selected exam-
ples based on a search for governors8, most anno-
tators in the PTB may have based their decision
of the PP label on the object that occurs in it. We
conclude that syntactically related words present a
better context for classification than fixed window
sizes.
5.3 Features
Having established the context we want to use, we
now turn to the details of extracting the feature
words from that context.9 Using higher-level fea-
tures instead of lexical ones helps accounting for
sparse training data (given an infinite amount of
data, we would not need to take any higher-level
7Based on such statistics, O?Hara and Wiebe (2003) ac-
tually set their window size to 5.
8Personal communication.
9As one reviewer pointed out, these two dimensions are
highly interrelated and influence each other. To examine the
effects, we keep one dimension constant while varying the
other.
457
features into account, since every case would be
covered). Compare O?Hara and Wiebe (2009).
Following the prepocessing, we use a set of
rules to select the feature words, and then gen-
erate feature values from them using a variety
of feature-generating functions.10 The word-
selection rules are listed below.
Word-Selection Rules
? Governor from the MALT parse
? Object from the MALT parse
? Heuristically determined object of the prepo-
sition
? First verb to the left of the preposition
? First verb/noun/adjective to the left of the
preposition
? Union of (First verb to the left, First
verb/noun/adjective to the left)
? First word to the left
The feature-generating functions, many of
which utilize WordNet (Fellbaum, 1998), are
listed below. To conserve space, curly braces are
used to represent multiple functions in a single
line. The name of each feature is the combination
of the word-selection rule and the output from the
feature-generating function.
WordNet-based Features
? {Hypernyms, Synonyms} for {1st, all}
sense(s) of the word
? All terms in the definitions (?glosses?) of the
word
? Lexicographer file names for the word
? Lists of all link types (e.g., meronym links)
associated with the word
? Part-of-speech indicators for the existence of
NN/VB/JJ/RB entries for the word
? All sentence frames for the word
? All {part, member, substance}-of holonyms
for the word
? All sentence frames for the word
Other Features
? Indicator that the word-finding rule found a
word
10Some words may be selected by multiple word-selection
rules. For example, the governor of the preposition may
be identified by the Governor from MALT parse rule, first
noun/verb/adjective to left, and the first word to the left rule.
? Capitalization indicator
? {Lemma, surface form} of the word
? Part-of-speech tag for the word
? General POS tag for the word (e.g. NNS?
NN, VBZ? VB)
? The {first, last} {two, three} letters of each
word
? Indicators for suffix types (e.g., de-
adjectival, de-nominal [non]agentive,
de-verbal [non]agentive)
? Indicators for a wide variety of other affixes
including those related to degree, number, or-
der, etc. (e.g., ultra-, poly-, post-)
? Roget?s Thesaurus divisions for the word
To establish the impact of each feature word on
the outcome, we use leave-one-out and only-one
evaluation.11 The results can be found in Table 3.
A word that does not perform well as the only at-
tribute may still be important in conjunction with
others. Conversely, leaving out a word may not
hurt performance, despite being a good single at-
tribute. word selection
Page 1
Word LOO LOO
92.1 80.1 84.3 78.9
93.4 94.2 84.9 56.3
92.0 77.9 85.0 62.1
92.1 78.7 84.3 78.5
92.1 78.4 84.5 81.0
92.0 78.8 84.4 77.2
91.9 93.0 84.9 56.8
91.8 ? 84.8 ?
coarse fine
Only Only
MALT governor
MALT object
Heuristics VB to left
Heur. NN/VB/ADJ to left
Heur. Governor Union
Heuristics word to left
Heuristics object
none
Table 3: Accuracies (%) for Leave-One-
Out (LOO) and Only-One Word-Extraction-Rule
Evaluation. none includes all words and serves for
comparison. Important words reduce accuracy for
LOO, but rank high when used as only rule.
Independent of the extraction method (MALT
parser or POS-based heuristics), the governor is
the most informative word. Combining several
heuristics to locate the governor is the best sin-
gle feature for fine-grained classification. The rule
looking only for a governing verb fails to account
11Since the feature words are not independent of one an-
other, neither of the two measures is decisive on its own.
458
full both
Page 1
Total Total Total Total
? ? 6 100.0 125 90.4 53 47.2
364 94.0 5 80.0 ? ? 74 93.2
23 69.6 78 65.4 ? ? 1 0.0
151 96.7 87 79.3 ? ? 7 71.4
53 79.2 841 92.5 of 1478 87.9 71 64.8
92 92.4 16 43.8 76 84.2 28 75.0
173 96.0 45 71.1 441 81.4 2287 90.8
? ? 5 80.0 58 91.4 15 53.3
? ? 58 70.7 out ? ? 90 68.9
50 80.0 358 93.9 ? ? 62 90.3
? ? 1 0.0 98 79.6 417 89.4
155 69.0 107 86.0 ? ? 6 83.3
84 100.0 232 84.5 per ? ? 3 100.0
? ? 2 50.0 82 65.9 ? ?
367 86.4 3078 92.0 ? ? 449 94.4
? ? 5 100.0 ? ? 2 0.0
? ? 420 91.7 208 48.1 364 69.0
20 90.0 384 83.3 ? ? 62 93.5
68 77.9 65 87.7 ? ? 3 100.0
? ? 94 71.3 to 572 89.7 3166 97.5
28 78.6 11 72.7 ? ? 55 65.5
29 100.0 4 100.0 102 97.1 2 100.0
? ? 1 0.0 ? ? 604 91.4
102 94.1 98 84.7 ? ? 2 50.0
? ? 45 64.4 ? ? 208 94.2
248 88.3 1341 87.5 up ? ? 20 75.0
down 153 81.7 16 56.2 ? ? 23 73.9
39 87.2 547 92.1 via ? ? 22 40.9
? ? 1 0.0 ? ? 1 100.0
478 82.4 1455 84.5 ? ? 3 33.3
578 85.5 1712 90.5 578 84.4 272 69.5
in 688 77.0 15706 95.0 ? ? 213 96.2
38 73.7 24 91.7 ? ? 69 63.8
297 86.2 415 80.0
Overall 8096 84.8 35917 91.8
fine coarse fine coarse
Prep Acc Acc Prep Acc Acc
aboard like
about near
above nearest
across next
after
against off
along on
alongside onto
amid
among outside
amongst over
around past
as
astride round
at since
atop than
because through
before throughout
behind till
below
beneath toward
beside towards
besides under
between underneath
beyond until
by
upon
during
except whether
for while
from with
within
inside without
into
Table 4: Accuracies (%) for Coarse and Fine-Grained PSD, Using MALT and Heuristics. Sorted by
preposition.
for noun governors, which consequently leads to
a slight improvement when left out.
Curiously, the word directly to the left is a bet-
ter single feature than the object (for fine-grained
classification). Leaving either of them out in-
creases accuracy, which implies that their infor-
mation can be covered by other words.
459
coarse both 2009
Page 1
Most Frequent Sense
f1 f1 f1
LOC 71.8 97.4 82.6 90.8 93.2 92.0 94.7 96.4 95.6
TMP 77.5 39.4 52.3 84.5 85.2 84.8 94.6 94.6 94.6
DIR 91.6 94.2 92.8 95.6 96.5 96.1 94.6 94.5 94.5
MNR 69.9 43.2 53.4 82.6 55.8 66.1 83.3 75.0 78.9
PRP 78.2 48.8 60.1 79.3 70.1 74.4 90.6 83.8 87.1
EXT 0.0 0.0 0.0 81.7 84.6 82.9 87.5 82.1 84.7
BNF 0.0 0.0 0.0 ? ? ? 75.0 34.1 46.9
O'Hara/Wiebe 2009 10-fold CV
Class prec rec prec rec prec rec
Table 5: Precision, Recall and F1 Results (%) for Coarse-Grained Classification. Comparison to O?Hara
and Wiebe (2009). Classes ordered by frequency
5.4 Comparison with Related Work
To situate our experimental results within the
body of work on PSD, we compare them to both
a most-frequent-sense baseline and existing work
for both granularities (see Table 6). The results
use a syntactically selective context of preposi-
tion, governor, object, and word to the left as
determined by combined extraction information
(POS tagging and parsing).
accuracies
Page 1
75.8 39.6  
89.3* 78.3**
93.9 84.8  
coarse fine
Baseline
Related Work
Our system
Table 6: Accuracies (%) for Different Classifi-
cations. Comparison with O?Hara and Wiebe
(2009)*, and Tratz and Hovy (2009)**.
Our system easily exceeds the baseline for both
coarse and fine-grained PSD (see Table 6). Com-
parison with related work shows that we achieve
an improvement of 6.5% over Tratz and Hovy
(2009), which is significant at p < .0001, and
of 4.5% over O?Hara and Wiebe (2009), which is
significant at p < .0001.
A detailed overview over all prepositions for
frequencies and accuracies of both coarse and
fine-grained PSD can be found in Table 4.
In addition to overall accuracy, O?Hara and
Wiebe (2009) also measure precision, recall and
F-measure for the different classes. They omitted
BNF because it is so infrequent. Due to different
training data and models, the two systems are not
strictly comparable, yet they provide a sense of
the general task difficulty. See Table 5. We note
that both systems perform better than the most-
frequent-sense baseline. DIR is reliably classified
using the baseline, while EXT and BNF are never
selected for any preposition. Our method adds
considerably to the scores for most classes. The
low score for BNF is mainly due to the low num-
ber of instances in the data, which is why it was
excluded by O?Hara and Wiebe (2009).
6 Conclusion
To get maximal accuracy in disambiguating
prepositions?and also other word classes?one
needs to consider context, features, and granular-
ity. We presented an evaluation of these parame-
ters for preposition sense disambiguation (PSD).
We find that selective context is better than
fixed window size. Within the context for prepo-
sitions, the governor (head of the NP or VP gov-
erning the preposition), the object of the prepo-
sition (i.e., head of the NP to the right), and the
word directly to the left of the preposition have
the highest influence.12 This corroborates the lin-
guistic intuition that close mutual constraints hold
between the elements of the PP. Each word syn-
tactically and semantically restricts the choice of
the other elements. Combining different extrac-
tion methods (POS-based heuristics and depen-
dency parsing) works better than either one in iso-
lation, though high accuracy can be achieved just
using heuristics. The impact of context and fea-
tures varies somewhat for different granularities.
12These will likely differ for other word classes.
460
Not surprisingly, we see higher scores for coarser
granularity than for the more fine-grained one.
We measured success in accuracy, precision, re-
call, and F-measure, and compared our results to
a most-frequent-sense baseline and existing work.
We were able to improve over state-of-the-art sys-
tems in both coarse and fine-grained PSD, achiev-
ing accuracies of 91.8% and 84.8% respectively.
Acknowledgements
The authors would like to thank Steve DeNeefe,
Victoria Fossum, and Zornitsa Kozareva for com-
ments and suggestions. StephenTratz is supported
by a National Defense Science and Engineering
fellowship.
References
Baker, C.F., C.J. Fillmore, and J.B. Lowe. 1998.
The Berkeley FrameNet Project. In Proceedings of
the 17th international conference on Computational
linguistics-Volume 1, pages 86?90. Association for
Computational Linguistics Morristown, NJ, USA.
Berger, A.L., V.J. Della Pietra, and S.A. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39?71.
Chan, Y.S., H.T. Ng, and D. Chiang. 2007. Word sense
disambiguation improves statistical machine trans-
lation. In Annual Meeting ? Association For Com-
putational Linguistics, volume 45, pages 33?40.
Chiang, D., K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 218?226, Boulder, Colorado, June.
Association for Computational Linguistics.
Fellbaum, C. 1998. WordNet: an electronic lexical
database. MIT Press USA.
Hwang, J. D., R. D. Nielsen, and M. Palmer. 2010.
Towards a domain independent semantics: Enhanc-
ing semantic representation with construction gram-
mar. In Proceedings of the NAACL HLT Workshop
on Extracting and Using Constructions in Computa-
tional Linguistics, pages 1?8, Los Angeles, Califor-
nia, June. Association for Computational Linguis-
tics.
Litkowski, K. and O. Hargraves. 2005. The preposi-
tion project. ACL-SIGSEM Workshop on ?The Lin-
guistic Dimensions of Prepositions and Their Use in
Computational Linguistic Formalisms and Applica-
tions?, pages 171?179.
Litkowski, K. and O. Hargraves. 2007. SemEval-2007
Task 06: Word-Sense Disambiguation of Preposi-
tions. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
Marcus, M.P., M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: the Penn TreeBank. Computational Linguis-
tics, 19(2):313?330.
McCallum, A.K. 2002. MALLET: A Machine Learn-
ing for Language Toolkit. 2002. http://mallet. cs.
umass. edu.
Nivre, J., J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(02):95?135.
O?Hara, T. and J. Wiebe. 2003. Preposition semantic
classification via Penn Treebank and FrameNet. In
Proceedings of CoNLL, pages 79?86.
O?Hara, T. and J. Wiebe. 2009. Exploiting seman-
tic role resources for preposition disambiguation.
Computational Linguistics, 35(2):151?184.
Rudzicz, F. and S. A. Mokhov. 2003. To-
wards a heuristic categorization of prepo-
sitional phrases in english with word-
net. Technical report, Cornell University,
arxiv1.library.cornell.edu/abs/1002.1095-
?context=cs.
Shen, L., G. Satta, and A. Joshi. 2007. Guided learn-
ing for bidirectional sequence classification. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, volume 45, pages
760?767.
Tratz, S. and D. Hovy. 2009. Disambiguation of
preposition sense using linguistically motivated fea-
tures. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Companion Volume: Student Re-
search Workshop and Doctoral Consortium, pages
96?100, Boulder, Colorado, June. Association for
Computational Linguistics.
Yarowsky, D. and R. Florian. 2002. Evaluating sense
disambiguation across diverse parameter spaces.
Natural Language Engineering, 8(4):293?310.
461
Ye, P. and T. Baldwin. 2007. MELB-YB: Preposition
Sense Disambiguation Using Rich Semantic Fea-
tures. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
462
