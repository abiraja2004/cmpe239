MOVEMENT IN ACTIVE PRODUCTION NETWORKS 
Mark A. Jones 
Alan S. Driacoll 
AT&T Bell Laboratories 
Murray Hill, New Jersey 07974 
ABSTRACT 
We describe how movement is handled in a class of 
computational devices called active production networks 
(APNs). The APN model is a parallel, activation-basod 
framework that ha= been applied to other aspects of 
natural language processing. The model is briefly defined, 
the notation and mechanism for movement is explained, 
and then several examples are given which illustrate how 
various conditions on movement can naturally be explained 
in terms of limitations of the APN device. 
I. INTRODUCTION 
Movement is an important phenomenon in natural 
languages. Recently, proposals uch as Gazdar's dcrivod 
rules (Gazdar, 1982) and Pereira's extraposition grammars 
(Pereirao 1983) have attemptod to find minimal extensions 
to the context-free framework that would allow the descrip- 
tion of movement. In this paper, we describe a class of 
computational devices for natural language processing. 
called active production networks (APNs), and explore 
how certain kinds of movement are handled. In particular. 
we are concerned with left extraposition, such as Subject- 
auxiliary Inversion. Wh-movement, and NP holes in rela- 
tive clauses, in these cos?s, the extraposod constituent 
leaves a trace which is insertod at a later point in the pro- 
cessing. This paper builds on the research reported in 
Jones (1983) and Jones (forthcoming). 
7,. ACTIVE PRODUCTION NgrwoP J~ 
7..1 Tim i~vk~ 
Our contention is that only a class of parallel devices 
will prove to be powerful enough to allow broad contextual 
priming, to pursue alternative hypotheses, and to explain 
the paradox that the performance of a sequential system 
often degrades with new knowledge, whereas human per- 
formance usually improves with learning and experience. = 
There are a number of new parallel processing (connection- 
? st) models which are sympathetic to this view--Anderson 
(1983). Feldman and Ballard (1982), Waltz and Pollack 
(1985). McClelland and Rumelhart (1981, 1982), and 
Fahlman. Hinton and Sejnowski (1983). 
Many of the connection?st models use iterative relaxa- 
tion techniques with networks containing excitatory and 
inhibitory links. They have primarily been used as best-fit 
categorizers in large recognition spaces, and it is not yet 
clear how they will implement the rule-governed behavior 
of parsers or problem solvers. Rule-based systems need a 
strong notion of an operating state, and they depend 
heavily on appropriate variable binding schemes for opera- 
tions such as matching (e.g.. unification) and recurs?on. 
The APN model directly supports a rule-based interpreta- 
tion, while retaining much of the general flavor of 
I. 1"be htmmm li~ity to L:mrfofm mlpatztmmtlly e?patm,m opmltmm =alia s 
~ y  ~,  imt'alkd loud,mum remforou this b?fid. 
connection?sin. An active production network is a rule- 
oriented, distributed processing system based on the follow- 
ing principles: 
1. Each node in the network executes a uniform activa- 
tion algorithm and assumes states in response to mes- 
sage (,such as expectation, inhibition, and activation) 
that arrive locally; the node can, in turn, relay mes- 
sages, initiate messages, and spawn new instances to 
process message activity. Although the patterns that 
define a node's behavior may be quite idiosyncratic or 
spocializod, the algorithm that interprets the pattern 
is the same for each node in the network. 
2. Messages are relatively simple. They have an associ- 
ated time, strength, and purpose (e.g., to post an 
expectation). They do not encode complex structures 
such as entire binding lists, parse trees, feature lists, 
or meaning representations, z Consequently, no struc- 
ture is explicitly built; the "result" of a computation 
consists entirely of the activation trace and the new 
state of the network. 
Figure I gives an artificial', but comprehensive example 
of an APN grammar in graphical form. The grammar 
generates the strings--a, b. acd. ace. bed. bee. fg and g l -  
and illustrates mapy of the pattern language features and 
grammar writing paradigms. The network responds to 
$ourcex which activate the network at its leaves. Activa- 
tion messages spread '*upward" through the network. At 
conjunctive nodes (seq and and), expectation messages are 
posted for the legal continuations of the pattern; inhibition 
messages are sent down previous links when new activa- 
tions are recorded. 
P J~ 
Figure i. A Sample APN 
In parsing applications, partially instantiatcd nodes are 
viewed as phrase structure rules whose next constituent is 
expected. The sources primarily arise from exogenous 
2. For ? sit'tatar ?oaaectioaett vnew, ~ F?ldman sad B#llard (1982) or 
Waltz ted Pollack (198S). A compemoa or markor patuns, value 
Imaan I ?ad uoreltricted melmzle pinball =yttm=t= i  ipvea ia Fahlmnm, 
Hlalal lad Scjnowl~ (IgS)). 
161 
strobings of the network by external inputs. In generation 
or problem solving applications, partially instantiated nodes 
are viewed as partially satisfied goals which have out.stand- 
ing subgoaLs whine solutions are de=ired. The source= in 
this case are endogenously generated. The compatibility of 
the=e two views not only allows the same network to be 
used for both parsing and generation, but also permits 
procesu~ to share in the interaction of internal and exter- 
nal sources of information. This compatibility, somewhat 
surprisingly, turned out to be crucial to our treatment of 
movement, but it is aLso clearly desirable for other aspects 
of natural anguage processing in which parsing and prob- 
lem solving interact (e.8., referenco resolution and infer- 
en(~P.). 
Each node in an APN is defined by a pattern, written 
in the pattern language of Figure 2. A pattern describes 
the me=age= to which a node rmponds, and the new mes- 
sage= and internal state= that are produced. Each subpat- 
tern of the form ($ v binding-put) in the pattern for node 
N is a variable binding site; a variable binding takes place 
when an instance of a node in binding-gat activates a 
reference to variable v of node N. Implicitly, a pattern 
defines the set of state= and. state transitions for a node. 
The ? (optiouality), + (repetition) and ? (optional repeti- 
tion) operators do not extend the expressiveness of the 
language, but have been added for convenience. They can 
be replaced in preprocessin8 by equivalent expre&sions, j 
Formal semantic definitions of the m_~_~$e passing 
behavior for each primitive operator have been specified. 
pattern ::-- binding-site 
(seq pattern ...) 
(and pattern ...) 
(or pattern ...) 
(? pattern) 
(+ binding.site) 
(. binding-site) 
binding-site ::-- ($ vat binding-pattern) 
binding.pauern ::-- node 
I (and binding-pattern ...) 
I (or binding-pattern ...) 
Figure 7.. The APN Pattern Language 
An important distinction that the pattern language 
makes is in the synchronicity* of activation signals. The 
pattern (and ($ vl X) ($ v2 \]'3) require= that the activa- 
tion from X and F emanate from distinct network sources, 
while the pattern ($ v (and X I"3) insists that instances of 
X and Y are activated from the same source. In the 
\]. The enact chore= o( cq~s'acors in the pattern tan |up  it  t matewhat 
~at= mine from the =!~=m~attma of the APN maciaa~. 
4, - r~  ?nulreat APN model allocate= ~ telueatmUy. The ten= 
$yllgiteomlclly reflC~lt th l  fact th l l  t\[~ ~ kicl~Uly o4 r t~ i ~  
m~se= can be Ioc~y COmlm '.,,I f~m tlm=r tiuw ~f ~ T I~ 
u k in |  u the ,ctJvuua= pmau= rims \[=== a~ugb to coacli~aa the network 
bmmi ,  mai my, m0 scuvatmm. Alua'aaUvety, a,:Uvalma mela l~ covid 
emV t l~ mmr?~ ideatiW =t as a4di,t*...-t l~ram, et~n. ia t l~  csm. m=Jme 
aeunt iom cam a*su.hq~ ~ at t t ' '  prom t'~h, ~ e( tit iaaemmltal  
cxp~?ume ~mvtm,,_,~._ F.= re~l~ iy  illlequndeut i , i.. o,m'lap 
may nm po~ a p~Vlem. 
graphical representation f an APN, synchrony is indicated 
by a short tail above the subpattern expression; the 
definition of U in Figure I illustrates both conventions: 
(and ($ vl (and TI)) ($ v2 S)). 
2.3 Am F..~m~ 
Figure 3 shows the stages in parsing the string acd. An 
exogenous source Exog-srcO first activates a, which is not 
currently supported by a source and, hence, is in an inac- 
tive state. The activation of an inactive or inhibited node 
give= rise to a new instance (nO) to record the binding. 
The instance is effectively a new node in the network, and 
derives its pattern from the spawning node. The activation 
spreads upward to the other instances hown in Figure 
3(a). The labels on each node indicate the current activa- 
tion level, repreu:nted as an integer between 0 and 9, 
inclusive. 
PO(9) 
qo(9) c 
I I 
aO(9)  I 
I 
Exog-~rc0(9) \[Exog-srcJ 
(a) trace structure after a 
po(4) 
Q0 c0(4) s 
aO I T 
Exog-src0 Exog-srcl(9) d e f 
Exog-src 
(b) trace structure after ac 
pO(9)  
Q0 cO S0(9) 
I Exog-src0 Exog-srcl d0(9) 
J 
Exog-src2(9) 
(c) trace structzure after acd 
\ [~p le  3, Stalp=l in Parsing acd 
162 
The activation of a node causes its pattern to be 
4re)instantiated and a variable to be (re)bound. For exam- 
pie. in the activation of RO, the pattern (seq ($ vi Q) (5 
v2 c'9) is replaced by (seq ($ vi (or Q QO)) ($ v2 c)). and 
the variable vl is bound to (20. For simplicity, only the 
active links are shown in Figure 3. RO posts an expecta- 
tion message for node C which can further its pattern. 
The source Exog-secO is said to be supporting the activa- 
tion of nodea nO. QO. RO and PO above it, and the expecta- 
tions or inhibitions that are generated by these nodes. For 
the current paper we will assume that exogenous sources 
remain fully on for the duration of the sentenco, s 
In Figure 3(b), another exogenous ource Exog-srcl 
activates c, which furthers the pattern for RO. RO sends an 
inhibition message to QO, posts expectations for S, and 
relays an activation message to P0, which rebind~ its vari- 
able to RO and a~umes a new activation value. Figure 
3(c) shows the final situation after d has been activated. 
The synchronous conjunction of SO is satisfied hy TO and 
dO. RO is fully satisfied (activation value of 9), and PO is 
re-satisfied. 
1,4 Gramm~ Writbql P~Ulpm 
The APN in Figure I illustrates everal grammar writ- 
ing paradigms. The situation in which an initial prefix 
string (a or b) satisfies a constituent (P), but can be fol- 
lowed by optional suffix strings (cd or ce) occurs frequently 
in natural language grammars. For example, noun phrase 
heads in English have optional prenominal and postnominal 
modifiers. The synchronous disjunction at P allows the 
local role of a or b to change, while preserving its interpre- 
tation as part of a P. It is also simple to encode optional 
prefixes. 
Another common situation in natural language gram- 
mars is specialization of a constituent based on some inter- 
hal feature. Noun phrases in English, for exampl?, can be 
specialized hy case; verb phrases can be specialized as par- 
ticipial, tensed or infinitive. In Figure l, node S is a spe. 
cialization which represents "Ts with d-ness or e-ness, but 
not f-heSS.'" The specialization is constructed by a synchro- 
nous conjunction of features that arise from subtrees some- 
where below the node to be specialized. 
The APN model also provides for node outputs to he 
partitioned into independent classes for the purl~s?~ ,~)f the 
activation algorithm. The nodes in the classes form levels 
in the network and represent orthogonal systems of 
classification. The cascading of expectations from dilfcrent 
I~els can implement context-sensitive b haviors uch as 
feature agreement and s':mantic sclectionai restrictiops. 
This is described in Jones (forthcoming). In the next sec- 
tion, we will introduce a grammar writing paradigm to 
represent movement, another type of non..context-fre? 
behavior. 
$. It is interertins to sp~'ulatc: on the oOm~lUamC~ o( vsr~w relauua~q of 
~hiu ?al~m~l~Oe. Fundam,mt~l limitatmm in the allocatm of ~ may 
be reJalod to limiuUmna in sluart term memory (~r buff're space in 
dc'tl~iatMi? zzleJ?l~ I?? Matctul, 19BO). Lin|uilti? ?emmzinUl ~ on 
OoQM~tlt~l? IcqtStb oou~ be col=ted tO ~r l~ daca),. ~ |yntlcli? 
Mlzdca path bebav~? mJlbl be rclltad to accc.h=Itad iowr~ decay r.atmmd 
by inbibitioo from ? ~up~ml l  bypmbmia. Anythin$ mum than ? f ,~m~ 
iJ ~t t t re  at ,hi= 
3. MOVI~W..NT 
From the APN perspective, movement (limited here to 
left-extrapnsition) ecessitates the endogenous reactivation 
of a trace that was created earlier in the process. To cap.. 
ture the trace so that expectations for its reactivation can 
be posted, we use the following type of rule: (seq (5 vl ... 
X... ) ($ v2 ... (and X X-see Y) ...). When an instance, 
XO, first activatea this rule, vl is bound to XO; the second 
occurrence X in the rule is constrained to match instances 
of XO, and expectations for XO, X-see and Y are created. 
No new exogenous source can satisfy the synchronous con- 
junction; only an endogenous X.src can. The rule is simi- 
lar to the notion of an X followed by a Y with an X hole in 
it (cf. Gazdar, 1982). 
NP- t  rae l l  CNP V V \] 
I ? .~ .<>.~ 7 p . . .  / I ~ .~e t~ N ~ ran  cnasecl / 
I a the  ? .~mOU. .  ~ / 
Figure 4. A Grammar for Relative Clauses 
Figure 4 defines a grammar with an NP hoic in a rela- 
tive clause; other type, s of \[eft-extraposition are handled 
analogously. Our treatment of relatives is adapted from 
C'homsky and Lasnik (1977). The movement rule for S is: 
(seq ($ vl (and Cutup Re/ (or Exog.src PRO-src)) ($ v2 
(and Rel Rel.src S))). The rule restricts the first instance 
of Re/ to arise either from an exogenous relative pronoun 
such as which or from an endogenously generated (phono- 
logically null) pronoun PRO. The second variable is 
satisfied when Rei,src simultaneously reactivates a trace of 
the Rel instance and inserts an NP-tracc into an S. 
It is instructive to consider how phonologically null pro- 
nouns are inserted before we discuss how movement occurs 
by trace insertion. The phrase, \[NP the mouse \ [~ PRO=" 
that ...\]\], illustrates how a relative pronoun PRO is 
inserted. Figure 5(a) shows the network after parsing the 
cat. When the complementizer that appears next in the 
input, PRO-src receives inhibition (marked by downward 
arrows in Figure 5(b)) from Rel.CompO. Non-exogenous 
163 
sources uch as PRO-src and Rel.src are activated in con- 
texts in which they are expected and then receive inhibi- 
tion. Figure 5(c) shows the resulting network after PRO- 
src has been activated, The inserted pronoun behaves pre- 
cisely as an input pronoun with respect to subsequent 
movement. 
The trace generation ecessary for movement uses the 
same insertion mechanism described above. Figures 6(a)- 
(d) illustrate various stages in parsing the phraso, \[/vp the 
cat \[~" whichi \[$ tl ranll\], in Figure 6(a), after parsing 
the cat which, synchronous expectations are posted for an 
S which contains a reactivation of the RelO trace by Rel. 
see. The signal sent to S by Rei.src will be in the form of 
an NP (through NP-trace). 
Figure 6(b) shows how the input of ran produces inhi- 
bition on Rei-src from SI. The inhibition on Rei-src 
caus~ it to activate (just as in the null pronoun insertion) 
to try to satisfy the current contextual expectations. Fig- 
ure 6(c) shows the network after Rel-src has activated to 
supply the trace. The only remaining problem is that 
Rel-src is actively inhibiting itself through .~0. 6 When 
Rel-src activates again, new instances are created for the 
inhibited nodes as they are re-activated; the uninhibited 
nodes are simply rebound. The final structure is shown in 
Figure 6(d). 
it is interesting that the network automatically enforces 
the restriction that the relative pronoun, complementizer 
and subject of the embedded sentence cannot all be miss- 
ing. PRO must be generated before its trace can be 
inserted as the subject. Furthermore. since expectations 
are strongest for the first link of a sequence, expectations 
will be much weaker for the VP in the relative clause 
(under S under S") than for the top-level VP under SO. 
The fact that the device blocks certai'n structures, 
without explicit weli-formedness constraints, is quite 
significant. Wherever possible, we would like to account 
for the complexity of the data through the composite 
behavior of a universal device and a simple, general gram- 
mar. We consider the description of a device which embo- 
dies the appropriate principles more parsimonious than a 
list of complex conditions and filters, and, to the extent 
that its architecture is independently motivated by proc,'ss- 
ink (i.e.. performance) considerations, of greater thcorctical 
interestf 
As we have seen, certain interpretations can be 
suppressed by expectations from elsewhere in the network. 
Furthermore, the occurrence of traces and empty consti- 
tuents is severely constrained because they must be sup- 
plied by endogenous ources, which can only suppurt a sin- 
tie constituent at any given time. For NP movement, 
these two properties of the device, taken together. 
elfectively enforce Ross's Complex NP Constraint (Ross. 
1967), which states that, "No element contained in a 
6. Another ,~sy o4" rut?inS thi,J iJ that the noa~ynchroetM:ity of the two 
vanaMea in the I~ttern hat ~ viohtted. The wdt-inhibittoa f ? murcg 
ocgtwt in othcnr conteat~ in the APN ft'tnM:lmek eve? for egolgno~t 
toMt~eL Is net,aerita hat contai? leJ't.rm;urtiv? cyr.t~ or ,endmSl~tm 
tttaghn~nta (e.S.. PP lUaghfl~'ltt), tett-iahibltioa C ll Ifiu naturally U
the t~ult at nemum~ me-de~rmiaim~ ae.tctivatioe f ? ~\[-inhil~t~ 
mum d'egUvety Ixorgtva the aea-tyarJumigity ~ pmuwnt. 
?. 1"I~ work 4 Margin (1980) iain tJ~tm~&l~t. 
sentence dominated by an NP  with a lexLcal head noun 
may be moved out of that NP  by a transformation." 
To see why this constraint is enforced, consider the two 
kinds of sentences that an NP  with a lexical head noun 
might dominate. If the embedded sentence is a relative 
clause, as in. \[pip the rat \[~" whichl \ [$ the cat \[~" whichj 
\[S fj chased/I\] \]  likes fish\]J\], then Rel.src cannot support 
both traces. If the embedded sentence is a noun comple- 
ment (not shown in Figure 4). as in. \[NP the rat \[~" 
whichi \[S he read a report \[~" that \ [$ the cat chased 
fl\]\]\]\]\], then there is only one trace in the intended 
interpretation, but there is nondeterminlsm during parsing 
between the noun complement and the relative clause 
interpretation. The interference aus?,, the trace to be 
bound to the innermost relative pronoun in the relative 
clause interpretation.' Thus, the combined properties of 
the device and grammar consistently block those structures 
which violate the Complex NP Constraint. Our prelim- 
inary findings for other types of movement (e.g., Subject- 
auxiliary Inversion, Wh-movement, and Raising) indicate 
that they also have natural APN explanations. 
4. IMPLF.aMENTATION 8ml Fu'ruRg DIMF.CrlONS 
Although the re.torch described in this summary is pri- 
marily of a theoretic nature, the basic ideas involved in 
using APNs for recognition and generation are being 
implemented and tested in Zetalisp on a Symbolics Lisp 
Machine. We have also hand-simulated data on movement 
from the literature to design the theory and algorithms 
presented in this paper. We are currently designing net- 
works for a broad coverage syntactic grammar of English 
and for additional, cascaded levels for NP role mapping 
and case frames. The model has aLso been adapted as a 
general, context-driven problem solver, although more work 
remains to be done. 
We are considering ways of integrating iterative relaxa- 
tion techniques with the rule-based framework of APNs. 
This is particularly necessary in helping the network to 
identify expectation coalitions. In Figure 5(a), for exam- 
pie. there should be virtually no expectations for Rel-src, 
since it cannot satisfy any of the dominating synchronous 
conjunctions. Some type of non-activating feedback from 
the sources eems to be necessary. 
S. SUI~ARY 
Recent linguistic theories have attempted to induce 
general principles (e.g., CNPC. Subjacency, and the Struc- 
ture Preserving Hypothesis) from the detailed structural 
descriptions of earlier transformational theories (Chomsky, 
1981), Our research can be viewed as an attempt tu 
induce the machine that embodies theae principles. In this 
paper, we have described a class of candidate machine~, 
called active production networks, and outlined how they 
handle movement as a natural way in which machine and 
grammar interact. 
The APN framework was initially developed as a plau- 
sible cognitive model for language processing, which would 
have real-time processing behavior, and extensive 
8. Uhle tO r~-.~,-~.--i ?oeskJs~ttmsJ wb~t rg~lto tO e.lp~t~om q~t~nfftb~ t J r ?~  
tm heud ia s ~.tr tlmt ~ nemmtg. 
164 
so(4) 
NPO(9) VP 
CNPO (9 )  "'" 
o s 
theO 
I 
Exog-s rcO 
cat0(9)  ~ " " 
| L T Comn 
Exog-sr?l (9) I r~  
/ .h4Ch PRO% \ that  for 
(a) trace structure after the cat 
So(4) 
NPO(9)  
I 
CNPO(9) 
NO - -~  
VP 
OetO 
? I \  ' /..=,~ Rel -Com?O (4 )  
Re I ~ - - - ~ ~  4=er0(9)  
I 
(b) trace structure after the cat ... that 
NPO(4) vP 
/ 
CNPO(4) 
Oat0  NO 
/ I 
tneO catO 
I l ExOg-SrCO ~x Og-S?'C !
SO(4) 
/ _ ~ ~, -Ico~,oo ~ 9 ) ~ ~"  --. ~ NP- t  r 'ace CNP 
Re lO(9)  I \[ Comglement tzerO I / .I.. 
\1 ,.'..o J 
/ \[ p~o-src( )\[ g- 
(c) trace structure after the cat PRO that 
.Figure 5. Relative Pronoun Insertion 
contextual processing and learning capabilities based on a 
formal notion of expectations. That movement also seems 
naturally expressible in a way that is consistent with 
current linguistic theories i  quite intriguing. 
REFERENCES 
Anderson, J. R. (1983). The Architecture of Cognition, 
Harvard University Press, Cambridge. 
Chomsky. N. (1981). Lectures on Government and Bind- 
ing. Foris Publications, Dordrecht. 
Chomsky, N. and Lasnik, H. (1977). "Filters and Con- 
trol," Linguistic Inquiry g, 425-504. 
Fahlman, S. E. (1979). NETL" A System for Represent- 
ing and Using Real-World Knowledge. MIT Press, Cam- 
bridge. 
Fahlman, S. E., Hinton, G. E. and Sejnowski, T. J. 
(1983). "Massively Parallel Architectures for Ah NFTL, 
Thistle, and Boltzmann Machines," AAAI.83 Conference 
Proceedings. 
Feldman. J, A. and Ballard, D. It. (1982). "Connection- 
ist Models and Their Properties," Cognitive Science 6, 
205-254. 
Gazdar, G. (1982). "Phrase Structure Grammar," The 
Nature of Syntactic Representation, Jacubson and Pullum, 
eds., Reidel, Boston, 131 -186. 
Jones. M. A.. (1983). "Activation-Based Parsi.g." 8th 
IJCAI, Karlsruhe, W. Germany, 678-682. 
Jones, M.A. (forthcoming). submitted for publication. 
Marcus. M. P. (1980). A Theory of S),ntactic Recogni. 
lion for Natural L,znguage, M IT Press, Cambridge. 
Pereira. F. (1983). "Logic for Natural Language 
Analysis," technical report 275, SRI International. Menlo 
Park. 
Ross, J. R. (1967). Constraints on Variables.in Syntax, 
unpublished Ph.D. thesis, MIT, Cambridge. 
Waltz. D. L. and Pollack, J. B. (1985). "Massively 
Parallel Parsing: A Strongly Interactive Model of Natural 
Language Interpretation," Cognitive Science, 9, 51-74. 
165 
SO(2) 
VP NPO(4) / 
CNPO(4) 
OetO NO 
tfleO 
i I I'~c? . 'P?(9.~ 1 s 
I i~tchO( t )  ? l l - -~  r iC l  CNI 
SO(2) 
NPO(4 ~*~='~''`mr~~ " VP / 
CNPO(4) 
OetO NO $0(4) 
. I r~=o~ ~ i ~ , (4 )  
Sxog l i fe0  Exoo-src l  I "~4#" / ~ I ~=lo / ~ .  ~ v=o(9) 
\[ li" / / t 1 
I wntchO /N I - - IP IC i  VO(9) 
,.o.-..<~.,/.; ~!?o<,, 
(a) trace structure after ihe cat which (b) trace structure after the cat which ... ran 
NP0(9) VP / 
CNP0(9) 
0et0 NO SO(9} 
~..o~,,oo ~,o:.,<, '4~ I .o,<.~oo 
/ I I i l ;~o  / . . -< . . /< .o ( , ,  ~ ~,o ,~y ,:?o 
S0(9) 
NP0{9) VP / 
CNPO(9) 
..---'7 
0et0 NO S0(9) 
1 
/ cato tneO 
I wn~c~O =n4cn00(9~NP- t race0(9)  v0 
~:  \ / /  , Exog ranO 
I 
Exog-Sr?3 pe l - s rc (9 ) |  Exog-src3 
(c) trace structure just after the cat which t ran (d) final trace structure 
l;igwe 6. Parsin8 Rclativc Clauses 
166 
