Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 190?199,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Evaluating a Semantic Network Automatically Constructed from Lexical
Co-occurrence on a Word Sense Disambiguation Task
Sean Szumlanski
Department of EECS
University of Central Florida
seansz@cs.ucf.edu
Fernando Gomez
Department of EECS
University of Central Florida
gomez@eecs.ucf.edu
Abstract
We describe the extension and objective eval-
uation of a network1 of semantically related
noun senses (or concepts) that has been au-
tomatically acquired by analyzing lexical co-
occurrence in Wikipedia. The acquisition pro-
cess makes no use of the metadata or links
that have been manually built into the ency-
clopedia, and nouns in the network are auto-
matically disambiguated to their correspond-
ing noun senses without supervision. For
this task, we use the noun sense inventory of
WordNet 3.0. Thus, this work can be con-
ceived of as augmenting the WordNet noun
ontologywith unweighted, undirected related-
to edges between synsets. Our network con-
tains 208,832 such edges.
We evaluate our network?s performance on a
word sense disambiguation (WSD) task and
show: a) the network is competitive with
WordNet when used as a stand-alone knowl-
edge source for two WSD algorithms; b) com-
bining our network with WordNet achieves
disambiguation results that exceed the perfor-
mance of either resource individually; and c)
our network outperforms a similar resource
that has been automatically derived from se-
mantic annotations in the Wikipedia corpus.
1 Introduction
A growing interest in using semantic relatedness in
word sense disambiguation (WSD) tasks has spurred
investigations into the limitations of the WordNet
ontology (Fellbaum, 1998) for this purpose. Al-
though WordNet comprises a rich set of semantic
1http://www.cs.ucf.edu/? seansz/sem
links between word senses (or concepts), indicat-
ing semantic similarity through subsumptive hyper-
nymic and hyponymic relations (among others), it
lacks a general indication of semantic relatedness.
We present a semantic network that is automat-
ically acquired from lexical co-occurrence in Wi-
kipedia, and indicates general semantic relatedness
between noun senses in WordNet 3.0. In this work,
the discovery of relatedness is a context-sparse affair
that takes place in absentia of the semantic annota-
tions of Wikipedia, such as inter-article links, entries
in disambiguation pages, the title of the article from
which a sentence is extracted, and so on.
We released an earlier version of such a network
that was limited by the fact that only relationships
involving at least one monosemous noun had been
included, and it was not evaluated on a WSD task
(Szumlanski and Gomez, 2010).
In contrast, the network we present here has relat-
edness data for over 4,500 polysemous noun targets
and 3,000 monosemous noun targets, each of which
are related to an average of 27.5 distinct noun senses.
It consists of 208,832 undirected edges ? a 181% in-
crease in size over the previous network. The result
is a semantic network that has reached maturity and,
as we will show, can be successfully applied to a
WSD task.
This paper proceeds as follows. In the next sec-
tion (Section 2), we discuss related work. We then
give an overview of the method we use to con-
struct our network (Sections 3 and 4). The network
is evaluated through its application to a WSD task
(Sections 5?7), where we compare its performance
to WordNet and another automatically acquired se-
mantic network called WordNet++ (Ponzetto and
Navigli, 2010). A discussion follows (Section 8),
190
and we present our conclusions in Section 9.
2 Related Work
Our work bears strong relation to WordNet++
(henceforth WN++), which is constructed automat-
ically from the semantic annotations in Wikipedia
(Ponzetto and Navigli, 2010).2 Links in WN++ are
established between words whose articles link to one
another. For example, the article on astronomy in
Wikipedia links to the article on celestial naviga-
tion, so we find an edge from astronomy#n#1 to
celestial navigation#n#1 in WN++.3 The nouns re-
lated in WN++ are disambiguated automatically us-
ing further semantic annotation data from Wikipe-
dia, including sense labels, the titles of other pages
linked to by any two related nouns, and the folk-
sonomic categories to which articles belong. These
serve as context words that are compared with con-
text words from various WordNet relations in or-
der to map the nouns to their appropriate WordNet
senses. The resulting resource contains 1,902,859
unique edges between noun senses.
Augmenting the structure of Wikipedia itself has
been the subject of research as well, and involves
the discovery of relations between articles. Mihal-
cea and Csomai (2007), for example, added links
between Wikipedia pages after automatically iden-
tifying keywords in each article and disambiguating
those words to their appropriate Wikipedia concepts
(article titles), while Ponzetto and Navigli (2009)
used graph theoretic approaches to augment the tax-
onomic organization of Wikipedia articles.
In terms of automatically discovering semantic re-
lations, many pattern-based approaches have been
used to extract specific types of relations from large
corpora, e.g., hyponymy, meronymy, and synonymy
(Hearst, 1992; Pantel and Pennacchiotti, 2006).
Approaches based on distributional similarity have
been applied toward the same end (Harris, 1985;
Gorman and Curran, 2006), and there are sev-
eral approaches that rely on the underlying struc-
ture of WordNet or Wikipedia to measure the re-
latedness between two concepts or nouns quantita-
tively (Hughes and Ramage, 2007; Gabrilovich and
2http://lcl.uniroma1.it/wordnetplusplus
3The notation astronomy#n#1 refers to sense 1 (#1) of the
noun (#n) ?astronomy? in WordNet. Other parts of speech are
denoted by #v (verbs), #a (adjectives), or #r (adverbs).
Markovitch, 2007; Zaragoza et al, 2007; Patward-
han and Pedersen, 2006; Strube and Ponzetto, 2006;
Budanitsky and Hirst, 2006; Resnik, 1995).
Other quantitative approaches have leveraged the
large amounts of data available on the Web to dis-
cover relatedness. Notably, Agirre and de Lacalle
(2004) employed web queries to associate WordNet
synsets with representative context words, known as
topic signatures. Cuadros and Rigau (2008) have
used these data to construct four KnowNets, seman-
tic knowledge bases derived by disambiguating the
top 5, 10, 15, and 20 nouns, respectively, from the
topic signatures of Agirre and de Lacalle.
3 Automatic Acquisition of the Semantic
Network
The semantic network is automatically acquired in
three distinct stages (Szumlanski and Gomez, 2010):
(1) quantitative measurement of relatedness between
nouns that co-occur in a large corpus; (2) categori-
cal determination of whether the quantitative mea-
sure indicates strong and mutual semantic related-
ness between a given pair of nouns; and (3) unsuper-
vised disambiguation of all the nouns that are found
to be semantically related. We provide an overview
of each of these steps below (Sections 3.1?3.3), and
then discuss how we have expanded this method-
ology to create a more complete semantic network
(Section 4).
3.1 Quantitatively measuring relatedness from
lexical co-occurrence
We first measure the semantic relatedness, or re-
lational strength, of a target, t, to one of its co-
occurring nouns, or co-targets, c, with the following
asymmetric function:
Srel(t, c) = P (t|c)P (c|t)log
P (c|t)
P (c)
where P (c|t) is the relative frequency of c among all
nouns co-occurring with t, and vice versa for P (t|c).
P (c) is the relative frequency of c among all nouns
occurring in the corpus. For these values, we rely on
lexical co-occurrence data extracted from Wikipe-
dia. Co-occurrence is considered intra-sententially
(as opposed to co-occurrence in entire articles or
paragraphs, or co-occurrence within variable-sized
windows of context).
191
This function essentially measures the degree to
which an occurrence of t in a sentence predicts the
co-occurrence of c. It is an adaptation of Resnik?s
(1999) selectional association measure.
Table 1 shows the results of applying this function
to the co-targets of ?yoga? and ?meditation.?
Target (t): yoga Target (t): meditation
Co-target (c) Srel Co-target (c) Srel
hatha yoga .1801 yoga .0707
asana .0761 mindfulness .0415
meditation .0673 contemplation .0165
bhakti .0508 prayer .0139
raja .0410 practice .0068
tantra .0148 technique .0060
yogi .0132 mantra .0053
karma .0125 relaxation .0048
posture .0104 retreat .0047
aerobics .0093 enlightenment .0031
tai chi .0089 monk .0025
exercise .0036 posture .0024
practice .0032 breathing .0017
instructor .0031 - - - - - - - - - - - - -
- - - - - - - - - - - - - exercise .0015
guru .0027 teaching .0014
massage .0026 practitioner .0014
exercise .0019 ascetic .0014
...
...
...
...
Table 1: The most strongly related co-targets of ?yoga?
and ?meditation,? sorted by decreasing value of relational
strength (Srel). Nouns above dashed lines are the top 5%
of the target?s most strongly related co-targets.
3.2 Establishing categorical relatedness
We then use a mutual relatedness algorithm to as-
certain whether two nouns are semantically related
by determining whether the nouns under considera-
tion reciprocate a high degree of relatedness to one
another. It proceeds as follows:
For some target noun of interest, t, let Cx(t) be
the set of the top x% of t?s co-targets as sorted by
Srel(t, c). For each c ? Cx(t), if we have t ? Cx(c),
then we say that t and c are categorically related and
add the noun pair (t, c) to our semantic network. We
then increment x by one and repeat the process: for
every c ? Cx+1(t) such that (t, c) is not already in
our network, we look for t in Cx+1(c), and add (t, c)
to our network if we find it. This process continues
until we have incremented x some number of times
without adding any new relations to the semantic
network. We then take the symmetric closure of the
network, so that if (t, c) is in the network, (c, t) is, as
well. (That is, the relation is considered undirected.)
Consider, for example, the nouns in Table 1.
Given the target ?yoga,? we might first examine the
top 5% of its most strongly related co-targets (an ar-
bitrary initial threshold chosen simply for illustra-
tive purposes). In this case, we have all the nouns
above the dashed line: C5(yoga) = {hatha yoga,
asana, meditation, bhakti, raja, tantra, yogi, karma,
posture, aerobics, tai chi, exercise, practice, instruc-
tor}. The algorithm then searches C5(hatha yoga),
C5(asana), and so on, for ?yoga,? adding a new re-
lation to the network every time ?yoga? is found.
Thus, we can see by the inclusion of ?yoga? in
C5(meditation) (all nouns above the dashed line in
the second column of Table 1), that the pair (yoga,
meditation) will be included in the network.
This reliance on mutual relatedness ensures that
only noun pairs exhibiting strong semantic related-
ness are admitted to the network.
3.3 Disambiguation
Disambiguation of the resulting noun-noun pairs is
the product of majority-rules voting by the following
three algorithms.
Subsumption. The most frequently occurring
immediate hypernyms of all nouns related to our
target are permitted to disambiguate the polyse-
mous nouns. This is useful because of the semantic
clustering that tends to occur among related nouns.
(E.g., ?astronomer? is related to several terms cat-
egorized as celestial bodies in WordNet, such as
?planet,? ?star,? ?minor planet,? and ?quasar.?)
Glosses. Senses of polysemous co-targets with
occurrences of monosemous co-targets in their
glosses are preferentially taken as the intended
meanings of the polysemous nouns. Monosemous
co-targets are matched directly, or by suffix replace-
ment. (E.g., ?biology? can be matched by the oc-
currence of ?biologist? in a gloss, ?engineering? by
?engineers,? and so on.)
Selectional Preferences. This method associates
a numerical score with all superordinate synsets
from the WordNet noun ontology that categorize
192
the monosemous nouns related to a target. For
example, the noun ?unicorn? strongly predicts re-
lated nouns categorized as monsters (monster#1)4
and mythical beings (mythical being#1) in Word-
Net. These selectional preferences are applied to
polysemous co-targets in decreasing order of their
relational strength to the target noun. A polysemous
noun is disambiguated to the first sense or senses
subsumed by one of these selectional preferences.
For example, ?phoenix,? as it relates to ?unicorn,? is
disambiguated to phoenix#3 in WordNet (the fiery
bird that is reborn from its own ashes) by virtue of
its subsumption by mythical being#1.
4 Creating a More Complete Network
A shortcoming of our previously released network is
that it lacked concept-level relations between pairs
of polysemous nouns.
When humans encounter a pair of ambiguous but
closely related words, like bus?horn, we automat-
ically disambiguate to the automobile and the car
horn, as opposed to a computer?s front-side bus or
a rhinoceros?s horn. The human ability to perform
this disambiguation stems from the fact that human
semantic memory relates not just individual words,
but specific concepts denoted by those words. But if
our goal is to establish such a link in our computa-
tional model of semantic relatedness, then we can-
not rely on the link to perform that disambiguation
for us; another approach is called for.
One reasonable approach (the one taken in our
previous work) is to go where the problem no
longer exists ? to relationships that involve at
least one monosemous noun. Monosemous-to-
monosemous noun relationships require no disam-
biguation. Monosemous-to-polysemous noun rela-
tionships, on the other hand, require that only one
noun be disambiguated. This ameliorates our prob-
lem tremendously, because the monosemous noun
in the pair anchors the polysemous noun in an un-
ambiguous context where disambiguation can more
readily take place. That context includes all the
nouns related to our monosemous noun, which,
through their transitive relatedness to the polyse-
mous noun in question, can assist in the act of disam-
4We sometimes drop the part of speech from our word sense
notation for brevity, but only in the case of noun senses.
biguation vis-a`-vis the algorithms described in Sec-
tion 3.3.
Consider, in contrast, the polysemous ?batter,?
which can refer to the baseball player or the cake
batter. The algorithm for discovering semantic relat-
edness yields several nouns related to each of these
senses of ?batter? (see Table 2). If we wish to dis-
ambiguate the pair (batter, cake), we are left with the
question: which of the nouns in Table 2 should we
take as contextual anchors for the disambiguation?
baking fastball inning strike
ball flour outfielder strikeout
base glove pancake swing
baseball hitter pitch tempura
bat home plate pudding umpire
cake home run runner waffle
dugout infielder shortstop
Table 2: An excerpt of some of the nouns related to ?bat-
ter? by the algorithm for automatic acquisition.
In considering this question, it is important to note
that although the ontological categories that sub-
sume the nouns related to ?batter? exhibit greater
entropy than we usually observe among the terms
related to a monosemous noun, clear delineations
still exist. For example, Figure 1 shows the clusters
that form as we consider shared hypernymic rela-
tionships between all senses of the nouns related to
?batter? (gray nodes in the graph). We see that many
of the nouns related to ?batter? have senses catego-
rized by food#1, cake#3, pitch#2, ballplayer#1, or
equipment#1 ? the heads of five distinct clusters by
semantic similarity.
It is worth noting that some nouns related to ?bat-
ter? (such as ?baking,? ?swing,? and ?umpire?) do
not fall into any of these semantic clusters. In these
cases, the WordNet glosses serve as our primary
tool for disambiguation. (For example, the glosses
of both swing#8 and umpire#1 include mention of
?baseball,? which is also related to ?batter.?)
Conversely, some of the polysemous nouns in our
example have senses that join semantic clusters un-
intendedly. For instance, cake#2 (?[a] small flat
mass of chopped food,? according to WordNet) falls
under the cluster headed by food#1. Although this is
potentially problematic, cake#2 is discarded in this
particular case in favor of cake#3 (the baked good),
193
which has a greater mass because of its subsump-
tion of waffle#1 and pancake#1, and is indeed the
intended meaning of ?cake? as it relates to ?batter.?
Another example of unintended cluster member-
ship comes from bat#4 (the cricket bat), which is
categorized by sports equipment#1. In contrast, the
baseball bat does not have its own entry in WordNet,
and the most reasonable sense choice, bat#5 (?a club
used for hitting a ball in various games?), is cate-
gorized as a stick (stick#1), and not as equipment,
sports equipment, or game equipment.
These unintended cluster memberships are bound
to cause minor errors in our disambiguation efforts.
However, our analysis reveals that we do not find
such high entropy among the relatives of a polyse-
mous noun that the semantic clustering effect (which
is necessary for the success of the disambiguation
algorithms described above in Section 3.3) is dimin-
ished. Thus, to construct our network, we apply the
disambiguation algorithms described above, with
the following modification: when confronted with
a pair of semantically related polysemous nouns,
we apply the disambiguation mechanism described
above in both directions, and then fuse the results to-
gether. So, in one direction, the various baked goods
related to ?batter? help us to properly disambiguate
?cake? to cake#3 in WordNet, yielding the pair (bat-
ter, cake#3). A similar scenario yields the pair (cake,
batter#2) when disambiguating in the other direc-
tion, and we fuse the results together into the prop-
erly disambiguated pair (batter#2, cake#3).
Using this method, we have automatically created
a semantic network that has 208,832 pairs of related
noun senses ? the most extensive semantic network
between WordNet noun senses to be derived auto-
matically from a simple lexical co-occurrence mea-
sure. For the remainder of this paper, we will refer
to our network as the Szumlanski-Gomez network
(SGN).
5 Coarse-Grained WSD Experiments
To evaluate our semantic network, and to provide
fair comparison to related work, we take our cue
from Ponzetto and Navigli (2010), who evaluated
the performance of WN++ on the SemEval-2007
(Navigli et al, 2007) coarse-grained all-words WSD
task using extended gloss overlaps (Banerjee and
entity#1
food#1
cake#2
equipment#1
ballplayer#1
pitch#2
cake#3
dessert#1
tempura#1
game_equipment#1
sports_equipment#1
runner#4
fielder#1
hitter#1
fastball#1
strike#5
waffle#1
pancake#1
pudding#2
pudding#3
ball#1
infielder#1
outfielder#1
baseball#2
bat#4
base#3
glove#1
glove#3
shortstop#1
centerfielder#1
Figure 1: A partial view of the WordNet graph, showing
senses of nouns related to ?batter? (gray nodes) and inter-
mediary concepts (white nodes) that connect them to the
root of the taxonomy through hypernymic relationships.
Pedersen, 2003) and the graph-based degree central-
ity algorithm (Navigli and Lapata, 2010).
In this particular SemEval task, we are presented
with 237 sentences in which lemmatized target
words have been flagged for disambiguation. In our
experiments, we disambiguate nouns only (as did
Ponzetto and Navigli), since both SGN (our net-
work) and WN++ relate only concepts denoted by
nouns, and no other parts of speech. In our exper-
imental setup, each sentence is considered in isola-
tion from the rest, and all lemmatized content words
in a sentence are provided to the disambiguation
algorithms; the verbs, adjectives, and adverbs, al-
though we do not resolve their senses, lend addi-
tional context to the disambiguation algorithms.
The coarse-grained nature of the SemEval-2007
task provides that there may be more than one ac-
ceptable sense assignment for many of the targets. In
the coarse-grained setting, an algorithm?s sense as-
signment is considered correct when it appears in the
list of acceptable senses for the given target word.
The algorithms below both allow for multiple dis-
ambiguation results to be returned in the event of a
tie. In these cases (although they are rare), we adopt
the approach of Banerjee and Pedersen (2003), who
award partial credit and discredit proportionally for
all the senses returned by the algorithm.
194
6 Extended Gloss Overlaps (ExtLesk)
The first disambiguation algorithm we employ is
the extended gloss overlaps measure (henceforth
ExtLesk) of Banerjee and Pedersen (2003), which
is an extension of the Lesk (1986) gloss overlap
measure. Loosely speaking, the algorithm disam-
biguates a target noun by maximizing the overlap
(number of words in common) between the glosses
of word senses related5 to the target?s noun senses
and those related to all context words (all lemma-
tized targets in the sentence under consideration
other than the target itself). The sense with the great-
est overlap is selected as the intended meaning of a
target noun.
In the event of a tie, multiple senses may be se-
lected. ExtLesk does not attempt to perform sense
assignment if the score for every sense of a target
noun is zero, except when dealing with a monose-
mous noun, in which case we default to the only
sense possible.
6.1 Results
We have run ExtLesk on the SemEval-2007 task us-
ing five combinations of semantic resources: Word-
Net only, SGN (our semantic network) only, SGN
and WordNet combined (that is, the union of all
links contained in both networks), WN++ only, and
WN++ combined withWordNet. We include the tra-
ditional baselines of most frequent sense (MFS) as-
signment and random sense assignment for compari-
son, and measure precision (number of correct sense
assignments divided by the number of attempted
sense assignments), recall (number of correct sense
assignments divided by the number of target nouns
to be disambiguated), and the harmonic mean of the
two, F1, defined as:
F1 =
2 ? precision ? recall
precision + recall
We present our results in Table 3, and offer the
following observations. Firstly, SGN as a stand-
alone network rivals the performance of WordNet.
This is particularly impressive given the fact that
5We use all relations available in WordNet, as well as a
related-to relation derived from the links in our semantic net-
work.
Resource P R F1
WordNet 78.80 74.82 76.76
SGN 78.64 72.82 75.62
SGN and WordNet 82.35 78.11 80.18
WN++ 74.67 61.87 67.67
WN++ and WordNet 77.35 73.38 75.31
MFS Baseline 77.40 77.40 77.40
Random Baseline 63.50 63.50 63.50
Table 3: ExtLesk disambiguation results on the SemEval-
2007 all-words coarse-grained WSD task (nouns only).
the edges in SGN were derived automatically from
a simple lexical co-occurrence measure.
Equally impressive is the ability of SGN and
WordNet, when used in combination, to achieve re-
sults that exceed what either network is able to ac-
complish as a stand-alone knowledge source. When
combined, we see improvements of 3.42% and
4.56% over WordNet and SGN as stand-alone re-
sources, respectively. It is also only with these re-
sources combined that we are able to outperform the
MFS baseline, and we do so by 2.78%.6
In contrast, WN++ fails to perform as a stand-
alone resource, falling behind the MFS baseline by
9.73%.7 Of all the resources tested, WN++ yields
the lowest results. When combined with WordNet,
WN++ actually diminishes the ability of WordNet to
perform on this WSD task by 1.45%. We defer our
discussion of factors impacting the performance of
WN++ to Section 8 (Discussion).
7 WSD with Degree Centrality
Degree centrality is a graph-based measure of se-
mantic relatedness (Navigli and Lapata, 2010) in
which we search through a semantic network for
paths of length l ? maxLength between all sense
nodes for all lemmas in our context. The edges along
all such paths are added to a new graph, G?, and for
each target noun to be disambiguated, the sense node
with the greatest number of incident edges (highest
vertex degree) in G? is taken as its intended sense.
6Other systems have obtained better results on the same
dataset, but we focus only on SGN and WN++ because our aim
is to compare the resources themselves.
7Ponzetto and Navigli (2010) report results of F1 = 68.3 and
72.0 for WN and WN++ as stand-alone resources. Space con-
siderations prevent us from discussing this disparity in detail.
195
In these graphs, nodes represent synsets, as op-
posed to instantiating separate nodes for different
members of the same synset and allowing edges to
be constructed between them. We include all lem-
mas from a sentence in our context, but only return
disambiguation results for the nouns.
With SGN and WN++, the implementation of this
algorithm is straightforward. We initiate a breadth-
first search (BFS)8 at each target sense node in the
network, and proceed through ?maxLength+12 ? itera-
tions of spreading activation. Whenever the tendrils
of this spreading activation from one target sense
node in the graph connect to those of another,9 we
add the path between the nodes to our new graph, G?,
potentially incrementing the degree of the involved
target sense nodes in G? as we do so.
Because BFS is an admissible algorithm (guaran-
teed to find the shortest path from an initial state to
a goal), it provides a computationally efficient ap-
proach to finding all paths between all target nodes.
Also, because any node on a path of length l ?
maxLength between two target nodes is at most
? l2? nodes removed from at least one of those tar-
get sense nodes, we only need to perform a BFS of
depth ?maxLength+12 ? from every target sense node
in order to guarantee that every such path between
them will be discovered. Since the time complexity
of BFS is exponential with respect to the depth of
the search, cutting this depth in half (in comparison
to performing a BFS of depth maxLength) greatly
reduces the running time of our algorithm.
We take the same approach in traversing the
WordNet noun graph, using all possible sense re-
lations as edges. In keeping with the approach of
Navigli and Lapata (2010), an edge is also induced
between synsets if the gloss of one synset contains a
monosemous content word. For example, the gloss
for leprechaun#n#1, ?a mischievous elf in Irish folk-
lore,? contains the monosemous noun ?folklore;?
thus, we have an edge between leprechaun#n#1 and
8This is in contrast to the DFS implementation of Navigli
and Lapata (2010), so for the sake of posterity, we expound
upon our approach in this section.
9When maxLength is odd, this requires an additional
check to ensure that the intersection is not taking place at a node
that is exactly ?maxLength+12 ? degrees removed from each of
the two target nodes it is connecting, as this would result in a
path with overall length maxLength + 1 between the target
nodes.
folklore#n#1 in the WordNet graph.
Once we have our new graph, G?, constructed in
this manner, the vertex degree is considered an in-
dication of the semantic relatedness of a particular
synset to all other lemmas in our context. For each
target noun, we use its sense node with the highest
degree in G? for sense assignment.
7.1 Results
We have tested the degree centrality algorithm with
the following combinations of semantic resources:
WordNet, SGN, WN++, Refined WN++, SGN and
WordNet combined, and Refined WN++ and Word-
Net combined. (Refined WN++ consists of 79,422
of WN++?s strongest relations, and was created in an
unsupervised setting by Ponzetto and Navigli specif-
ically for use with degree centrality when they dis-
covered that WN++ had too many weak relations to
perform well with the algorithm.)
We have observed that the performance of de-
gree centrality rapidly levels off as maxLength
increases. Ponzetto and Lapata (2010) also re-
port this so-called ?plateau? effect, and employ a
maxLength of 6 in their experiments, despite find-
ing that results level off around maxLength = 4.
We, too, find that performance levels off around
maxLength = 4 in almost all cases, and so only
continue up to maxLength = 5.
We find that, in all cases tested, degree centrality
is unable to outperform the MFS baseline (with re-
spect to F1) (see Table 4). SGN and WN++ exhibit
comparable performance with this algorithm, with
maximum F1 values of 68.4% (maxLength = 2)
and 67.3% (maxLength = 3?5), respectively. Nei-
ther achieves the performance of WordNet with de-
gree centrality (F1 = 74.0%), which underperforms
the MFS baseline (F1 = 77.4%) by 3.4%.10 Ponzetto
and Navigli (2010) reported that only performing
sense assignment when the max degree exceeded an
empirically derived but non-disclosed threshold im-
proved performance, but we have found that imple-
menting such a threshold universally lowers results
for all resources we tested with degree centrality.
10Although Ponzetto and Navigli (2010) reported similar re-
sults with WordNet (F1 = 74.5), we have been unable to repro-
duce their results using Refined WN++, either combined with
WordNet (F1 = 79.4) or as a stand-alone resource (F1 = 57.4).
196
The lowest performance using degree central-
ity comes from Refined WN++ as a stand-alone
resource. We attribute this to the fact that Re-
fined WN++ is so semantically sparse. On average,
noun senses in Refined WN++ are related to only
3.42 other noun senses, while those in WN++ and
SGN relate to an average of 44.59 and 10.92 noun
senses, respectively. Accordingly, the success of Re-
fined WN++ and WordNet combined is attributable
mostly to the success of WordNet as a stand-alone
resource; as maxLength increases, the contribu-
tions made by the sparse Refined WN++ network
rapidly become negligible in comparison to those
provided by the WordNet ontology.
l P R F1 P R F1
WordNet SGN
1 96.9 16.8 28.6 79.7 32.9 46.6
2 77.6 45.1 57.0 72.0 64.6 68.4
3 76.7 65.6 70.7 68.7 63.5 66.0
4 76.9 71.0 73.9 68.0 63.9 65.9
5 76.6 71.6 74.0 68.0 64.2 66.1
SGN & WN WN++
1 77.4 52.4 62.5 87.2 23.5 37.1
2 74.7 70.7 72.7 71.6 60.2 65.4
3 70.3 67.1 68.7 70.7 64.3 67.3
4 70.5 67.4 68.9 70.4 64.5 67.3
5 70.1 67.0 68.5 70.4 64.5 67.3
WN++refined WN
++
refined& WN
1 98.3 15.3 26.5 83.3 31.2 45.4
2 91.4 23.4 37.3 77.5 66.6 71.6
3 88.7 29.9 44.7 77.6 73.6 75.5
4 83.7 32.3 46.7 74.7 71.4 73.0
5 80.2 35.3 49.0 74.7 71.4 73.0
MFS Baseline Random Baseline
77.4 77.4 77.4 63.5 63.5 63.5
Table 4: Degree centrality disambiguation results on
the SemEval-2007 all-words coarse-grained WSD task
(nouns only). l is maximum path length.
8 Discussion
The fact that the performance of degree centrality
quickly plateaus hints at the root cause of its weak
performance compared to ExtLesk and the MFS
baseline. As the maximum path length is increased
in a dense semantic network, all possible edges from
our target sense nodes rapidly find themselves in-
volved with paths to other target sense nodes. This is
particularly true of WN++ (notice its rapid and sta-
ble convergence), where certain ?sticky? nodes form
bridges between seemingly unrelated concepts. For
example, the frequent appearance of ?United States?
in Wikipedia articles, and its tendency to be linked
to the United States Wikipage when it occurs, causes
the term to serve as a bridge between such diverse
concepts as automaton#2 and burrito#1, which one
would typically expect to be far removed from one
another in a model of semantic relatedness.
Nonetheless, the degree centrality algorithm has
no difficulty finding short paths between target sense
nodes when traversing any of the semantic networks
we tested. In fact, we have discovered that as the
results of degree centrality converge, they approach
the performance obtained by foregoing the algo-
rithm altogether and simply disambiguating each
noun to the sense with the most edges in the net-
work (regardless of whether those edges ultimately
connect two word senses from the disambiguation
context). The expected values of convergence at-
tained by defaulting to the most semantically well-
connected sense of each target noun in each network
are F1 = 66.3%, 67.5%, and 74.6% for SGN,WN++,
and WordNet, respectively ? remarkably close to the
experimentally derived degree centrality results of
F1 = 66.1%, 67.3%, and 74.0%.
9 Conclusion
We have constructed a semantic network of related
noun senses automatically from intra-sentential lex-
ical co-occurrence data, and shown that on a WSD
task, it outperforms a similar resource, WN++,
which is derived from the rich set of semantic anno-
tations available in the Wikipedia corpus. Our net-
work has also shown competitive performance with
the WordNet ontology onWSD, and when combined
with WordNet, improves disambiguation results in
a coarse-grained setting using the ExtLesk disam-
biguation algorithm.
Acknowledgments
This research was supported in part by the
NASA Engineering and Safety Center under
Grant/Cooperative Agreement NNX08AJ98A.
197
References
Eneko Agirre and Oier Lopez de Lacalle. 2004. Pub-
licly available topic signatures for all WordNet nom-
inal senses. In Proceedings of the 4th International
Conference on Language Resources and Evaluations
(LREC ?04), pages 1123?1126, Lisbon, Portugal.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of the 18th International Joint Confer-
ence on Artificial Intelligence (IJCAI ?03), pages 805?
810, Acapulco, Mexico.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating WordNet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32(1):13?47.
Montse Cuadros and German Rigau. 2008. KnowNet:
building a large net of knowledge from the web. In
Proceedings of the 22nd International Conference on
Computational Linguistics (COLING ?08), pages 161?
168, Manchester, UK. Association for Computational
Linguistics.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness usingWikipedia-based ex-
plicit semantic analysis. In Proceedings of the 20th In-
ternational Joint Conference on Artificial Intelligence
(IJCAI ?07), pages 1606?1611, Hyderabad, India.
James Gorman and James R. Curran. 2006. Scaling dis-
tributional similarity to large corpora. In Proceedings
of the 21st InternationalConference on Computational
Linguistics and the 44th Annual Meeting of the Asso-
ciation for Computational Linguistics (COLING-ACL
?06), pages 361?368, Sydney, Australia. Association
for Computational Linguistics.
Zellig S. Harris. 1985. Distributional structure. In J. J.
Katz, editor, The Philosophy of Linguistics, pages 26?
47. Oxford University Press.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th International Conference on Computational
Linguistics (COLING ?92), pages 539?545, Nantes,
France.
Thad Hughes and Daniel Ramage. 2007. Lexical se-
mantic relatedness with random graph walks. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-CoNLL
?07), pages 581?589, Prague, Czech Republic. Asso-
ciation for Computational Linguistics.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proceedings of the
5th Annual International Conference on Systems Doc-
umentation (SIGDOC ?86), pages 24?26, Toronto, On-
tario, Canada. ACM.
RadaMihalcea and Andras Csomai. 2007. Wikify!: link-
ing documents to encyclopedic knowledge. In Pro-
ceedings of the 16th ACM Conference on Information
and Knowledge Management (CIKM ?07), pages 233?
242, Lisbon, Portugal. ACM.
Roberto Navigli and Mirella Lapata. 2010. An exper-
imental study of graph connectivity for unsupervised
word sense disambiguation. IEEE Transactions on
Pattern Analysis andMachine Intelligence, 32(4):678?
692.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. SemEval-2007 Task 07: coarse-grained
English all-words task. In Proceedings of the 4th In-
ternational Workshop on Semantic Evaluations (Sem-
Eval ?07), pages 30?35, Prague, Czech Republic. As-
sociation for Computational Linguistics.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL ?06), pages
113?120, Sydney, Australia. Association for Compu-
tational Linguistics.
Siddharth Patwardhan and Ted Pedersen. 2006. Using
WordNet-based context vectors to estimate the seman-
tic relatedness of concepts. In Proceedings of the 11th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics Workshop on Mak-
ing Sense of Sense, pages 1?8, Trento, Italy.
Simone Paolo Ponzetto and Roberto Navigli. 2009.
Large-scale taxonomy mapping for restructuring and
integrating Wikipedia. In Proceedings of the 21st In-
ternational Joint Conference on Artifical Intelligence
(IJCAI ?09), pages 2083?2088, Pasadena, CA.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?10), pages 1522?1531, Uppsala, Sweden.
Association for Computational Linguistics.
Philip Resnik. 1995. Using information content to eval-
uate semantic similarity in a taxonomy. In Proceed-
ings of the 14th International Joint Conference on Ar-
tificial Intelligence (IJCAI ?95), pages 448?453, Mon-
treal, QC.
Philip Resnik. 1999. Semantic similarity in a taxonomy:
an information-based measure and its application to
problems of ambiguity in natural language. Journal
of Artificial Intelligence Research, 11:95?130.
198
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using wi-
kipedia. In Proceedings of the 21st National Confer-
ence on Artificial Intelligence (AAAI ?06), pages 1419?
1424, Boston, MA. AAAI Press.
Sean Szumlanski and Fernando Gomez. 2010. Auto-
matically acquiring a semantic network of related con-
cepts. In Proceedings of the 19th ACM Conference on
Information and KnowledgeManagement (CIKM ?10),
pages 19?28, Toronto, Ontario, Canada. ACM.
Hugo Zaragoza, Henning Rode, Peter Mika, Jordi Atse-
rias, Massimiliano Ciaramita, and Giuseppe Attardi.
2007. Ranking very many typed entities on Wikipe-
dia. In Proceedings of the 16th ACM Conference on
Information and KnowledgeManagement (CIKM ?07),
pages 1015?1018, Lisbon, Portugal. ACM.
199
