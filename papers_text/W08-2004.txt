Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 25?32
Manchester, August 2008
Encoding Tree Pair-based Graphs in Learning Algorithms:
the Textual Entailment Recognition Case
Alessandro Moschitti
DISI, University of Trento
Via Sommarive 14
38100 POVO (TN) - Italy
moschitti@dit.unitn.it
Fabio Massimo Zanzotto
DISP, University of Rome ?Tor Vergata?
Via del Politecnico 1
00133 Roma, Italy
zanzotto@info.uniroma2.it
Abstract
In this paper, we provide a statistical ma-
chine learning representation of textual en-
tailment via syntactic graphs constituted
by tree pairs. We show that the natural way
of representing the syntactic relations be-
tween text and hypothesis consists in the
huge feature space of all possible syntac-
tic tree fragment pairs, which can only be
managed using kernel methods. Experi-
ments with Support Vector Machines and
our new kernels for paired trees show the
validity of our interpretation.
1 Introduction
Recently, a lot of valuable work on the recogni-
tion of textual entailment (RTE) has been carried
out (Bar Haim et al, 2006). The aim is to detect
implications between sentences like:
T
1
? H
1
T
1
?Wanadoo bought KStones?
H
1
?Wanadoo owns KStones?
where T
1
and H
1
stand for text and hypothesis, re-
spectively.
Several models, ranging from the simple lexi-
cal similarity between T and H to advanced Logic
Form Representations, have been proposed (Cor-
ley and Mihalcea, 2005; Glickman and Dagan,
2004; de Salvo Braz et al, 2005; Bos and Mark-
ert, 2005). However, since a linguistic theory able
to analytically show how to computationally solve
the RTE problem has not been developed yet, to
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
design accurate systems, we should rely upon the
application of machine learning. In this perspec-
tive, TE training examples have to be represented
in terms of statistical feature distributions. These
typically consist in word sequences (along with
their lexical similarity) and the syntactic structures
of both text and hypothesis (e.g. their parse trees).
The interesting aspect with respect to other natural
language problems is that, in TE, features useful
at describing an example are composed by pairs of
features from Text and Hypothesis.
For example, using a word representation, a text
and hypothesis pair, ?T,H?, can be represented
by the sequences of words of the two sentences,
i.e. ?t
1
, .., t
n
? and ?h
1
, .., h
m
?, respectively. If we
carry out a blind and complete statistical correla-
tion analysis of the two sequences, the entailment
property would be described by the set of subse-
quence pairs from T and H , i.e. the set R =
{?s
t
, s
h
? : s
t
= ?t
i
1
, .., t
i
l
?, s
h
= ?h
j
1
, .., h
j
r
?, l ?
n, r ? m}. The relation set R constitutes a
naive and complete representation of the example
?T,H? in the feature space {?v,w? : v,w ? V ?},
where V is the corpus vocabulary1 .
Although the above representation is correct and
complete from a statistically point of view, it suf-
fers from two practical drawbacks: (a) it is expo-
nential in V and (b) it is subject to high degree of
data sparseness which may prevent to carry out ef-
fective learning. The traditional solution for this
problem relates to consider the syntactic structure
of word sequences which provides their general-
ization.
The use of syntactic trees poses the problem
of representing structures in learning algorithms.
1
V
? is larger than the actual space, which is the one of
all possible subsequences with gaps, i.e. it only contains all
possible concatenations of words respecting their order.
25
For this purpose, kernel methods, and in partic-
ular tree kernels allow for representing trees in
terms of all possible subtrees (Collins and Duffy,
2002). Unfortunately, the representation in entail-
ment recognition problems requires the definition
of kernels over graphs constituted by tree pairs,
which are in general different from kernels applied
to single trees. In (Zanzotto and Moschitti, 2006),
this has been addressed by introducing semantic
links (placeholders) between text and hypothesis
parse trees and evaluating two distinct tree ker-
nels for the trees of texts and for those of hypothe-
ses. In order to make such disjoint kernel combi-
nation effective, all possible assignments between
the placeholders of the first and the second en-
tailment pair were generated causing a remarkable
slowdown.
In this paper, we describe the feature space of
all possible tree fragment pairs and we show that it
can be evaluated with a much simpler kernel than
the one used in previous work, both in terms of
design and computational complexity. Moreover,
the experiments on the RTE datasets show that our
proposed kernel provides higher accuracy than the
simple union of tree kernel spaces.
2 Fragments of Tree Pair-based Graphs
The previous section has pointed out that RTE can
be seen as a relational problem between word se-
quences of Text and Hypothesis. The syntactic
structures embedded in such sequences can be gen-
eralized by natural language grammars. Such gen-
eralization is very important since it is evident that
entailment cases depend on the syntactic structures
of Text and Hypothesis. More specifically, the set
R described in the previous section can be ex-
tended and generalized by considering syntactic
derivations2 that generate word sequences in the
training examples. This corresponds to the follow-
ing set of tree fragment pairs:
R
?
= {??
t
, ?
h
? : ?
t
? F(T ), ?
h
? F(H)}, (1)
where F(?) indicates the set of tree fragments of a
parse tree (i.e. the one of the text T or of the hy-
pothesis H). R? contains less sparse relations than
R. For instance, given T
1
and H
1
of the previous
section, we would have the following relational de-
scription:
2By cutting derivation at different depth, different degrees
of generalization can be obtained.
R
?
=
{
?
NP
NNP
,
NP
NNP
? , ?
S
NP VP
,
S
NP VP
? ,
?
S
NP
NNP
VP
VBP
bought
NP
NNP
,
S
NP
NNP
VP
VBP
owns
NP
NNP
? ,
?
VP
VBP
bought
NP
NNP
,
VP
VBP
owns
NP
NNP
? , ..
}
These features (relational pairs) generalize the
entailment property, e.g. the pair ?[VP [VBP bought] [NP]],
[VP [VBP own] [NP]]? generalizes many word sequences,
i.e. those external to the verbal phrases and inter-
nal to the NPs.
We can improve this space by adding semantic
links between the tree fragments. Such links
or placeholders have been firstly proposed in
(Zanzotto and Moschitti, 2006). A placeholder
assigned to a node of ?
t
and a node of ?
h
states
that such nodes dominate the same (or similar) in-
formation. In particular, placeholders are assigned
to nodes whose words t
i
in T are equal, similar, or
semantically dependent on words h
j
in H . Using
placeholders, we obtain a richer fragment pair
based representation that we call R?p, exemplified
hereafter:
{
?
S
NP
NNP X
VP
VBP
bought
NP
NNP Y
,
S
NP
NNP X
VP
VBP
owns
NP
NNP Y
?
, ?
S
NP VP
VBP
bought
NP
NNP Y
,
S
NP VP
VBP
owns
NP
NNP Y
?
, ?
S
NP VP
,
S
NP VP
? , ...
}
The placeholders (or variables) indicated with
X and Y specify that the NNPs labeled by
the same variables dominate similar or identical
words. Therefore, an automatic algorithm that
assigns placeholders to semantically similar con-
stituents is needed. Moreover, although R?p con-
tains more semantic and less sparse features than
26
both R? and R, its cardinality is still exponential in
the number of the words of T and H . This means
that standard machine learning algorithms cannot
be applied. In contrast, tree kernels (Collins and
Duffy, 2002) can be used to efficiently generate
the huge space of tree fragments but, to generate
the space of pairs of tree fragments, a new kernel
function has to be defined.
The next section provides a solution to both
problems. i.e. an algorithm for placeholders as-
signments and for the computation of paired tree
kernels which generates R? and R?p representa-
tions.
F
(
VP
V
book
NP
D
a
N
flight
)
=
{
VP
V NP
D
a
N
flight
,
VP
V NP
D N
,
NP
D
a
N
flight
,
NP
D
a
N ,
NP
D N
flight
,
NP
D N
,
N
flight
, . . .
}
Figure 1: A syntactic parse tree.
3 Kernels over Semantic Tree Pair-based
Graphs
The previous section has shown that placeholders
enrich a tree-based graph with relational informa-
tion, which, in turn, can be captured by means
of word semantic similarities sim
w
(w
t
, w
h
), e.g.
(Corley and Mihalcea, 2005; Glickman et al,
2005). More specifically, we use a two-step greedy
algorithm to anchor the content words (verbs,
nouns, adjectives, and adverbs) in the hypothesis
W
H
to words in the text W
T
.
In the first step, each word w
h
in W
H
is con-
nected to all words w
t
in W
T
that have the max-
imum similarity sim
w
(w
t
, w
h
) with it (more than
one w
t
can have the maximum similarity with w
h
).
As result, we have a set of anchors A ? W
T
?W
H
.
sim
w
(w
t
, w
h
) is computed by means of three tech-
niques:
1. Two words are maximally similar if they have
the same surface form w
t
= w
h
.
2. Otherwise, WordNet (Miller, 1995) similari-
ties (as in (Corley and Mihalcea, 2005)) and
different relation between words such as verb
entailment and derivational morphology are
applied.
3. The edit distance measure is finally used to
capture the similarity between words that are
missed by the previous analysis (for mis-
spelling errors or for the lack of derivational
forms in WordNet).
In the second step, we select the final anchor set
A
?
? A, such that ?w
t
(or w
h
) ?!?w
t
, w
h
? ? A
?
.
The selection is based on a simple greedy algo-
rithm that given two pairs ?w
t
, w
h
? and ?w?
t
, w
h
?
to be selected and a pair ?s
t
, s
h
? already selected,
considers word proximity (in terms of number of
words) between w
t
and s
t
and between w?
t
and s
t
;
the nearest word will be chosen.
Once the graph has been enriched with seman-
tic information we need to represent it in the learn-
ing algorithm; for this purpose, an interesting ap-
proach is based on kernel methods. Since the con-
sidered graphs are composed by only two trees, we
can carried out a simplified computation of a graph
kernel based on tree kernel pairs.
3.1 Tree Kernels
Tree Kernels (e.g. see NLP applications in (Giu-
glea and Moschitti, 2006; Zanzotto and Moschitti,
2006; Moschitti et al, 2007; Moschitti et al,
2006; Moschitti and Bejan, 2004)) represent trees
in terms of their substructures (fragments) which
are mapped into feature vector spaces, e.g. ?n.
The kernel function measures the similarity be-
tween two trees by counting the number of their
common fragments. For example, Figure 1 shows
some substructures for the parse tree of the sen-
tence "book a flight". The main advantage of
tree kernels is that, to compute the substructures
shared by two trees ?
1
and ?
2
, the whole fragment
space is not used. In the following, we report the
formal definition presented in (Collins and Duffy,
2002).
Given the set of fragments {f
1
, f
2
, ..} = F , the
indicator function I
i
(n) is equal 1 if the target f
i
is
rooted at node n and 0 otherwise. A tree kernel is
then defined as:
TK(?
1
, ?
2
) =
?
n
1
?N
?
1
?
n
2
?N
?
2
?(n
1
, n
2
) (2)
where N
?
1
and N
?
2
are the sets of the ?
1
?s and ?
2
?s
27
nodes, respectively and
?(n
1
, n
2
) =
|F|
?
i=1
I
i
(n
1
)I
i
(n
2
)
The latter is equal to the number of common frag-
ments rooted in the n
1
and n
2
nodes and ? can be
evaluated with the following algorithm:
1. if the productions at n
1
and n
2
are different
then ?(n
1
, n
2
) = 0;
2. if the productions at n
1
and n
2
are the
same, and n
1
and n
2
have only leaf children
(i.e. they are pre-terminals symbols) then
?(n
1
, n
2
) = 1;
3. if the productions at n
1
and n
2
are the same,
and n
1
and n
2
are not pre-terminals then
?(n
1
, n
2
) =
nc(n
1
)
?
j=1
(1 + ?(c
j
n
1
, c
j
n
2
)) (3)
where nc(n
1
) is the number of the children of
n
1
and cj
n
is the j-th child of the node n. Note
that since the productions are the same, nc(n
1
) =
nc(n
2
).
Additionally, we add the decay factor ? by mod-
ifying steps (2) and (3) as follows3:
2. ?(n
1
, n
2
) = ?,
3. ?(n
1
, n
2
) = ?
nc(n
1
)
?
j=1
(1 + ?(c
j
n
1
, c
j
n
2
)).
The computational complexity of Eq. 2 is
O(|N
?
1
| ? |N
?
2
|) although the average running
time tends to be linear (Moschitti, 2006).
3.2 Tree-based Graph Kernels
The above tree kernel function can be applied to
the parse trees of two texts or those of the two hy-
potheses to measure their similarity in terms of the
shared fragments. If we sum the contributions of
the two kernels (for texts and for hypotheses) as
proposed in (Zanzotto and Moschitti, 2006), we
just obtain the feature space of the union of the
fragments which is completely different from the
space of the tree fragments pairs, i.e. R? . Note
that the union space is not useful to describe which
3To have a similarity score between 0 and 1, we also ap-
ply the normalization in the kernel space, i.e. K?(?
1
, ?
2
) =
TK(?
1
,?
2
)
?
TK(?
1
,?
1
)?TK(?
2
,?
2
)
.
grammatical and lexical property is at the same
time held by T and H to trig the implication.
Therefore to generate the space of the frag-
ment pairs we need to define the kernel between
two pairs of entailment examples ?T
1
,H
1
? and
?T
2
,H
2
? as
K
p
(?T
1
,H
1
?, ?T
2
,H
2
?) =
=
?
n
1
?T
1
?
n
2
?T
2
?
n
3
?H
1
?
n
4
?H
2
?(n
1
, n
2
, n
3
, n
4
),
where ? evaluates the number of subtrees rooted
in n
1
and n
2
combined with those rooted in n
3
and
n
4
. More specifically, each fragment rooted into
the nodes of the two texts? trees is combined with
each fragment rooted in the two hypotheses? trees.
Now, since the number of subtrees rooted in the
texts is independent of the number of trees rooted
in the hypotheses,
?(n
1
, n
2
, n
3
, n
4
) = ?(n
1
, n
2
)?(n
3
, n
4
).
Therefore, we can rewrite K
p
as:
K
p
(?T
1
,H
1
?, ?T
2
,H
2
?) =
=
?
n
1
?T
1
?
n
2
?T
2
?
n
3
?H
1
?
n
4
?H
2
?(n
1
, n
2
)?(n
3
, n
4
) =
=
?
n
1
?T
1
?
n
2
?T
2
?(n
1
, n
2
)
?
n
3
?H
1
?
n
4
?H
2
?(n
3
, n
4
) =
= K
t
(T
1
, T
2
)?K
t
(H
1
,H
2
).
(4)
This result shows that the natural kernel to rep-
resent textual entailment sentences is the kernel
product, which corresponds to the set of all possi-
ble syntactic fragment pairs. Note that, such kernel
can be also used to evaluate the space of fragment
pairs for trees enriched with relational information,
i.e. by placeholders.
4 Approximated Graph Kernel
The feature space described in the previous sec-
tion correctly encodes the fragment pairs. How-
ever, such huge space may result inadequate also
for algorithms such as SVMs, which are in general
robust to many irrelevant features. An approxima-
tion of the fragment pair space is given by the ker-
nel described in (Zanzotto and Moschitti, 2006).
Hereafter we illustrate its main points.
First, tree kernels applied to two texts or two hy-
potheses match identical fragments. When place-
holders are added to trees, the labeled fragments
28
are matched only if the basic fragments and the
assigned placeholders match. This means that
we should use the same placeholders for all texts
and all hypotheses of the corpus. Moreover, they
should be assigned in a way that similar syntac-
tic structures and similar relational information be-
tween two entailment pairs can be matched, i.e.
same placeholders should be assigned to the po-
tentially similar fragments.
Second, the above task cannot be carried out at
pre-processing time, i.e. when placeholders are
assigned to trees. At the running time, instead,
we can look at the comparing trees and make a
more consistent decision on the type and order of
placeholders. Although, there may be several ap-
proaches to accomplish this task, we apply a basic
heuristic which is very intuitive:
Choose the placeholder assignment that maxi-
mizes the tree kernel function over all possible cor-
respondences
More formally, let A and A? be the placeholder sets
of ?T,H? and ?T ?,H ??, respectively, without loss
of generality, we consider |A| ? |A?| and we align
a subset of A to A?. The best alignment is the one
that maximizes the syntactic and lexical overlap-
ping of the two subtrees induced by the aligned set
of anchors. By calling C the set of all bijective
mappings from S ? A, with |S| = |A?|, to A?,
an element c ? C is a substitution function. We
define the best alignment c
max
the one determined
by
c
max
= argmax
c?C
(TK(t(T, c), t(T
?
, i))+
TK(t(H, c), t(H
?
, i)),
where (1) t(?, c) returns the syntactic tree enriched
with placeholders replaced by means of the sub-
stitution c, (2) i is the identity substitution and (3)
TK(?
1
, ?
2
) is a tree kernel function (e.g. the one
specified by Eq. 2) applied to the two trees ?
1
and
?
2
.
At the same time, the desired similarity value
to be used in the learning algorithm is given
by the kernel sum: TK(t(T, c
max
), t(T
?
, i)) +
TK(t(H, c
max
), t(H
?
, i)), i.e. by solving the fol-
lowing optimization problem:
K
s
(?T,H?, ?T
?
,H
?
?) =
max
c?C
(TK(t(T, c), t(T
?
, i))+
TK(t(H, c), t(H
?
, i)),
(5)
For example, let us compare the following two
pairs (T
1
,H
1
) and (T
2
,H
2
) in Fig. 2.
To assign the placeholders 1 , 2 and 3 of
(T
2
,H
2
) to those of (T
1
,H
1
), i.e. X and Y , we
need to maximize the similarity between the two
texts? trees and between the two hypotheses? trees.
It is straightforward to derive that X=1 and Y=3 al-
low more substructures (i.e. large part of the trees)
to be identical, e.g. [S [NP 1 X VP]] , [VP [VBP
NP 3 Y ]], [S [NP 1 X VP [VBP NP 3 Y ]]].
Finally, it should be noted that, (a)
K
s
(?T,H?, ?T
?
,H
?
?) is a symmetric function
since the set of derivation C are always computed
with respect to the pair that has the largest anchor
set and (b) it is not a valid kernel as the max
function does not in general produce valid kernels.
However, in (Haasdonk, 2005), it is shown that
when kernel functions are not positive semidef-
inite like in this case, SVMs still solve a data
separation problem in pseudo Euclidean spaces.
The drawback is that the solution may be only a
local optimum. Nevertheless, such solution can
still be valuable as the problem is modeled with a
very rich feature space.
Regarding the computational complexity, run-
ning the above kernel on a large training set may
result very expensive. To overcome this drawback,
in (Moschitti and Zanzotto, 2007), it has been de-
signed an algorithm to factorize the evaluation of
tree subparts with respect to the different substitu-
tion. The resulting speed-up makes the application
of such kernel feasible for datasets of ten of thou-
sands of instances.
5 Experiments
The aim of the experiments is to show that the
space of tree fragment pairs is the most effective
to represent Tree Pair-based Graphs for the design
of Textual Entailment classifiers.
5.1 Experimental Setup
To compare our model with previous work we
implemented the following kernels in SVM-light
(Joachims, 1999):
? K
s
(e
1
, e
2
) = K
t
(T
1
, T
2
) + K
t
(H
1
,H
2
),
where e
1
= ?T
1
,H
1
? and e
2
= ?T
2
,H
2
?
are two text and hypothesis pairs and K
t
is
the syntactic tree kernel (Collins and Duffy,
2002) presented in the previous section.
? K
p
(e
1
, e
2
) = K
t
(T
1
, T
2
) ? K
t
(H
1
,H
2
),
which (as shown in the previous sections) en-
29
T1
? H
1
S
NP X
NNP X
Wanadoo
VP
VBP
bought
NP Y
NNP Y
KStones
S
NP X
NNP X
Wanadoo
VP
VBP
owns
NP Y
NNP Y
KStones
T
2
? H
2
S
NP 1
NP 1
DT
the
NN 1
president
PP 2
IN
of
NP 2
NNP 2
Miramax
VP
VBP
bought
NP 3
DT
a
NN 3
castle
S
NP 1
NP 1
DT
the
NN 1
president
PP 2
IN
of
NP 2
NNP 2
Miramax
VP
VBP
own
NP 3
DT
a
NN 3
castle
Figure 2: The problem of finding the correct mapping between placeholders
codes the tree fragment pairs with and with-
out placeholders.
? K
max
(e
1
, e
2
) = max
c?C
(
K
t
(?
c
(T
1
), ?
c
(T
2
))+
K
t
(?
c
(H
1
), ?
c
(H
2
))
)
, where c is a possi-
ble placeholder assignment which connects
nodes from the first pair with those of the sec-
ond pair and ?
c
(?) transforms trees according
to c.
? K
pmx
(e
1
, e
2
) = max
c?C
(
K
t
(?
c
(T
1
), ?
c
(T
2
))?
K
t
(?
c
(H
1
), ?
c
(H
2
))
)
.
Note that K
max
is the kernel proposed in (Zanzotto
and Moschitti, 2006) and K
pmx
is a hybrid kernel
based on the maximum K
p
, which uses the space
of tree fragment pairs. For all the above kernels,
we set the default cost factor and trade-off param-
eters and we set ? to 0.4.
To experiment with entailment relations, we
used the data sets made available by the first (Da-
gan et al, 2005) and second (Bar Haim et al, 2006)
Recognizing Textual Entailment Challenge. These
corpora are divided in the development sets D1
and D2 and the test sets T1 and T2. D1 contains
567 examples whereas T1, D2 and T2 all have the
same size, i.e. 800 instances. Each example is an
ordered pair of texts for which the entailment rela-
tion has to be decided.
5.2 Evaluation and Discussion
Table 1 shows the results of the above kernels
on the split used for the RTE competitions. The
first column reports the kernel model. The second
and third columns illustrate the model accuracy for
RTE1 whereas column 4 and 5 show the accuracy
for RTE2. Moreover, ? P indicates the use of stan-
dard syntactic trees and P the use of trees enriched
with placeholders. We note that:
First, the space of tree fragment pairs, gener-
ated by K
p
improves the one generated by K
s
(i.e.
the simple union of the fragments of texts and hy-
potheses) of 4 (58.9% vs 54.9%) and 0.9 (53.5%
vs 52.6%) points on RTE1 and RTE2, respectively.
This suggests that the fragment pairs are more ef-
fective for encoding the syntactic rules describing
the entailment concept.
Second, on RTE1, the introduction of placehold-
ers does not improve K
p
or K
s
suggesting that for
their correct exploitation an extension of the space
of tree fragment pairs should be modeled.
Third, on RTE2, the impact of placeholders
seems more important but only K
max
and K
s
are able to fully exploit their semantic contribu-
tion. A possible explanation is that in order to
use the set of all possible assignments (required by
K
max
), we needed to prune the ?too large? syntac-
tic trees as also suggested in (Zanzotto and Mos-
chitti, 2006). This may have negatively biased the
statistical distribution of tree fragment pairs.
Finally, although we show that K
p
is better
30
Kernels RTE1 RTE2
? P P ? P P
K
s
54.9 50.0 52.6 59.5
K
p
58.9 55.5 53.5 56.0
K
max
- 58.25 - 61.0
K
pmx
- 50.0 - 56.8
Table 1: Accuracy of different kernel models using
(P) and not using (? P) placeholder information on
RTE1 and RTE2.
suited for RTE than the other kernels, its accuracy
is lower than the state-of-the-art in RTE. This is be-
cause the latter uses additional models like the lex-
ical similarity between text and hypothesis, which
greatly improve accuracy.
6 Conclusion
In this paper, we have provided a statistical ma-
chine learning representation of textual entailment
via syntactic graphs constituted by tree pairs. We
have analytically shown that the natural way of
representing the syntactic relations between text
and hypothesis in learning algorithms consists in
the huge feature space of all possible syntactic tree
fragment pairs, which can only be managed using
kernel methods.
Therefore, we used tree kernels, which allow for
representing trees in terms of all possible subtrees.
More specifically, we defined a new model for the
entailment recognition problems, which requires
the definition of kernels over graphs constituted by
tree pairs. These are in general different from ker-
nels applied to single trees. We also studied an-
other alternative solution which concerns the use
of semantic links (placeholders) between text and
hypothesis parse trees (to form relevant semantic
fragment pairs) and the evaluation of two distinct
tree kernels for the trees of texts and for those of
hypotheses. In order to make such disjoint kernel
combination effective, all possible assignments be-
tween the placeholders of the first and the second
entailment pair have to be generated (causing a re-
markable slowdown).
Our experiments on the RTE datasets show that
our proposed kernel may provide higher accuracy
than the simple union of tree kernel spaces with a
much simpler and faster algorithm. Future work
will be devoted to make the tree fragment pair
space more effective, e.g. by using smaller and ac-
curate tree representation for text and hypothesis.
Acknowledgments
We would like to thank the anonymous reviewers
for their professional and competent reviews and
for their invaluable suggestions.
Alessandro Moschitti would like to thank the Eu-
ropean Union project, LUNA (spoken Language
UNderstanding in multilinguAl communication
systems) contract n 33549 for supporting part of
his research.
References
Bar Haim, Roy, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The II PASCAL RTE challenge.
In PASCAL Challenges Workshop, Venice, Italy.
Bos, Johan and Katja Markert. 2005. Recognising
textual entailment with logical inference. In Pro-
ceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Nat-
ural Language Processing, pages 628?635, Vancou-
ver, British Columbia, Canada, October. Association
for Computational Linguistics.
Collins, Michael and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of ACL02.
Corley, Courtney and Rada Mihalcea. 2005. Measur-
ing the semantic similarity of texts. In Proc. of the
ACL Workshop on Empirical Modeling of Semantic
Equivalence and Entailment, pages 13?18, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Dagan, Ido, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL RTE challenge. In PASCAL
Challenges Workshop, Southampton, U.K.
de Salvo Braz, R., R. Girju, V. Punyakanok, D. Roth,
and M. Sammons. 2005. An inference model for se-
mantic entailment in natural language. In Proceed-
ings of AAAI, pages 1678?1679.
Giuglea, Ana-Maria and Alessandro Moschitti. 2006.
Semantic role labeling via framenet, verbnet and
propbank. In Proceedings of Coling-ACL, Sydney,
Australia.
Glickman, Oren and Ido Dagan. 2004. Probabilistic
textual entailment: Generic applied modeling of lan-
guage variability. In Proceedings of the Workshop on
Learning Methods for Text Understanding and Min-
ing, Grenoble, France.
Glickman, Oren, Ido Dagan, and Moshe Koppel. 2005.
Web based probabilistic textual entailment. In Pro-
ceedings of the 1st Pascal Challenge Workshop,
Southampton, UK.
31
Haasdonk, Bernard. 2005. Feature space interpretation
of SVMs with indefinite kernels. IEEE Trans Pat-
tern Anal Mach Intell, 27(4):482?92, Apr.
Joachims, Thorsten. 1999. Making large-scale svm
learning practical. In Schlkopf, B., C. Burges, and
A. Smola, editors, Advances in Kernel Methods-
Support Vector Learning. MIT Press.
Miller, George A. 1995. WordNet: A lexical
database for English. Communications of the ACM,
38(11):39?41, November.
Moschitti, Alessandro and Cosmin Adrian Bejan.
2004. A semantic kernel for predicate argument
classification. In CoNLL-2004, USA.
Moschitti, A. and F. Zanzotto. 2007. Fast and effective
kernels for relational learning from texts. In Ghahra-
mani, Zoubin, editor, Proceedings of the 24th An-
nual International Conference on Machine Learning
(ICML 2007).
Moschitti, Alessandro, Daniele Pighin, and Roberto
Basili. 2006. Semantic Role Labeling via Tree Ker-
nel Joint Inference. In Proceedings of CoNLL-X.
Moschitti, Alessandro, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for question
answer classification. In Proceedings ACL, Prague,
Czech Republic.
Moschitti, Alessandro. 2006. Efficient convolution
kernels for dependency and constituent syntactic
trees. In ECML?06.
Zanzotto, Fabio Massimo and Alessandro Moschitti.
2006. Automatic learning of textual entailments
with cross-pair similarities. In Proceedings of the
21st Coling and 44th ACL, pages 401?408, Sydney,
Australia, July.
32
