Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 585?592
Manchester, August 2008
Random Restarts in Minimum Error Rate Training for Statistical
Machine Translation
Robert C. Moore and Chris Quirk
Microsoft Research
Redmond, WA 98052, USA
bobmoore@microsoft.com, chrisq@microsoft.com
Abstract
Och?s (2003) minimum error rate training
(MERT) procedure is the most commonly
used method for training feature weights in
statistical machine translation (SMT) mod-
els. The use of multiple randomized start-
ing points in MERT is a well-established
practice, although there seems to be no
published systematic study of its bene-
fits. We compare several ways of perform-
ing random restarts with MERT. We find
that all of our random restart methods out-
perform MERT without random restarts,
and we develop some refinements of ran-
dom restarts that are superior to the most
common approach with regard to resulting
model quality and training time.
1 Introduction
Och (2003) introduced minimum error rate train-
ing (MERT) for optimizing feature weights in sta-
tistical machine translation (SMT) models, and
demonstrated that it produced higher translation
quality scores than maximizing the conditional
likelihood of a maximum entropy model using the
same features. Och?s method performs a series
of one-dimensional optimizations of the feature
weight vector, using an innovative line search that
takes advantage of special properties of the map-
ping from sets of feature weights to the resulting
translation quality measurement. Och?s line search
is guaranteed to find a global optimum, whereas
more general line search methods are guaranteed
only to find a local optimum.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Global optimization along one dimension at a
time, however, does not insure global optimization
in all dimensions. Hence, as Och briefly men-
tions, ?to avoid finding a poor local optimum,?
MERT can be augmented by trying multiple ran-
dom starting points for each optimization search
within the overall algorithm. However, we are not
aware of any published study of the effects of ran-
dom restarts in the MERT optimization search.
We first compare a variant of Och?s method with
and without multiple starting points for the op-
timization search, selecting initial starting points
randomly according to a uniform distribution. We
find that using multiple random restarts can sub-
stantially improve the resulting model in terms of
translation quality as measured by the BLEU met-
ric, but that training time also increases substan-
tially. We next try selecting starting points by a
random walk from the last local optimum reached,
rather than by sampling from a uniform distrib-
ution. We find this provides a slight additional
improvement in BLEU score, and is significantly
faster, although still slower than training without
random restarts.
Finally we look at two methods for speeding up
training by pruning the set of hypotheses consid-
ered. We find that, under some circumstances, this
can speed up training so that it takes very little
additional time compared to the original method
without restarts, with no significant reduction in
BLEU score compared to the best training methods
in our experiments.
2 Och?s MERT procedure
While minimum error rate training for SMT is
theoretically possible by directly applying gen-
eral numerical optimization techniques, such as
the downhill simplex method or Powell?s method
585
(Press, 2002), naive use of these techniques would
involve repeated translation of the training sen-
tences using hundreds or thousands of combina-
tions of feature weights, which is clearly impracti-
cal given the speed of most SMT decoders.
Och?s optimization method saves expensive
SMT decoding time by generating lists of n-best
translation hypotheses, and their feature values ac-
cording to the SMT model, and then optimizing
feature weights just with respect to those hypothe-
ses. In this way, as many different feature weight
settings as necessary can be explored without re-
running the decoder. The translation quality mea-
surement for the training corpus can be estimated
for a given point in feature weight space by find-
ing the highest scoring translation hypothesis, out
of the current set of hypotheses, for each sentence
in the training set. This is typically orders of mag-
nitude faster than re-running the decoder for each
combination of feature weights.
Since the resulting feature weights are opti-
mized only for one particular set of translation hy-
potheses, the decoder may actually produce differ-
ent results when run with those weights. There-
fore Och iterates the process, re-running the de-
coder with the optimized feature weights to pro-
duce new sets of n-best translation hypotheses,
merging these with the previous sets of hypothe-
ses, and re-optimizing the feature weights relative
to the expanded hypothesis sets. This process is re-
peated until no more new hypotheses are obtained
for any sentence in the training set.
Another innovation by Och is a method of nu-
merical optimization that takes advantage of the
fact that, while translation quality metrics may
have continous values, they are always applied to
the discrete outputs of a translation decoder. This
means that any measure of translation quality can
change with variation in feature weights only at
discrete points where the decoder output changes.
Och takes advantage of this through an efficient
procedure for finding all the points along a one-
dimensional line in feature weight space at which
the highest scoring translation hypothesis changes,
given the current set of hypotheses for a particu-
lar sentence. By merging the lists of such points
for all sentences in the training set, he finds all
the points at which the highest scoring hypothesis
changes for any training sentence.
Finding the optimal value of the feature weights
along the line being optimized then requires sim-
ply evaluating the translation quality metric for
each range of values for the feature weights be-
tween two such consecutive points. This can be
done efficiently by tracking incremental changes
in the sufficient statistics for the translation qual-
ity metric as we iterate through the points where
things change. Och uses this procedure as a line
search method in an iterative optimization proce-
dure, until no additional improvement in the trans-
lation quality metric is obtained, given the current
sets of translation hypotheses.
3 Optimization with Random Restarts
Although Och?s line search is globally optimal,
this is not sufficient to guarantee that a series of
line searches will find the globally optimal com-
bination of all feature weights. To avoid getting
stuck at an inferior local optimum during MERT, it
is usual to perform multiple optimization searches
over each expanded set of translation hypotheses
starting from different initial points. Typically, one
of these points is the best point found while op-
timizing over the previous set of translation hy-
potheses.1 Additional starting points are then se-
lected by independently choosing initial values for
each feature weight according to a uniform distri-
bution over a fixed interval, say ?1.0 to +1.0. The
best point reached, starting from either the previ-
ous optimum or one of the random restart points,
is selected as the optimum for the current set of hy-
potheses. This widely-used procedure is described
by Koehn et al (2007, p. 50).
3.1 Preliminary evaluation
In our first experiments, we compared a variant
of Och?s MERT procedure with and without ran-
dom restarts as described above. For our training
and test data we used the English-French subset
of the Europarl corpus provided for the shared task
(Koehn and Monz, 2006) at the Statistical Machine
Translation workshop held in conjunction with the
2006 HLT-NAACL conference. We built a stan-
dard baseline phrasal SMT system, as described
by Koehn et al (2003), for translating from Eng-
lish to French (E-to-F), using the word alignments
and French target language model provided by the
workshop organizers.
We trained a model with the standard eight fea-
tures: E-to-F and F-to-E phrase translation log
1Since additional hypotheses have been added, initiating
an optimization search from this point on the new set of hy-
potheses will often lead to a higher local optimum.
586
probabilities, E-to-F and F-to-E phrase translation
lexical scores, French language model log proba-
bilities, phrase pair count, French word count, and
distortion score. Feature weight optimization was
performed on the designated 2000-sentence-pair
development set, and the resulting feature weights
were evaluated on the designated 2000-sentence-
pair development test set, using the BLEU-4 metric
with one reference translation per sentence.
At each decoding iteration we generated the
100-best translation hypotheses found by our
phrasal SMT decoder. To generate the initial
100-best list, we applied the policy of setting the
weights for features we expected to be positively
correlated with BLEU to 1, the weights for fea-
tures we expected to be negatively correlated with
BLEU to ?1, and the remaining weights to 0. In
this case, we set the initial distortion score weight
to ?1, the phrase count weight to 0, and all other
feature weights to 1.
We made a common modification of the MERT
procedure described by Och, by replacing Pow-
ell?s method (Press, 2002) for selecting the direc-
tions in which to search the feature weight space,
with simple co-ordinate ascent?following Koehn
et al (2007)?repeatedly optimizing one feature
weight at a time while holding the others fixed, un-
til all feature weights are at optimum values, given
the values of the other feature weights. Powell?s
method is not designed to reach a better local op-
timum than co-ordinate ascent, but does have con-
vergence guarantees under certain idealized condi-
tions. However, we have observed informally that,
in MERT, co-ordinate ascent always seems to con-
verge relatively quickly, with Powell?s method of-
fering no clear advantage.
We also modified Och?s termination test slightly.
As noted above, Och suggests terminating the
overall procedure when n-best decoding fails to
produce any hypotheses that have not already been
seen. Without random restarts, this will guarantee
convergence because the last set of feature weights
selected will still be a local optimum.2 However,
if we go through the coordinate ascent procedure
without finding a better set of feature weights, then
we do not have to perform the last iteration of n-
best decoding, because it will necessarily produce
the same n-best lists as the previous iteration, as
2With random restarts, there can be no guarantee of con-
vergence, unless we have a true global optimization method,
or we enumerate all possible hypotheses permitted by the
model.
long as the decoder is deterministic. Thus we can
terminate the overall procedure if either we either
fail to generate any new hypotheses in n-best de-
coding, or the optimium set of feature weights does
not change in the coordinate ascent phase of train-
ing. In fact, we relax the termination test a bit
more than this, and terminate if no feature weight
changes by more than 1.0%.
Without random restarts, we found that MERT
converged in 8 decoding iterations, with the result-
ing model producing a BLEU score of 31.12 on
the development test set. For the experiment with
random restarts, after each iteration of 100-best
decoding, we searched from 20 initial points, 19
points selected by uniform sampling over the in-
terval [?1, 1] for each feature weight, plus the op-
timum point found for the previous set of hypothe-
ses. This procedure converged in 10 decoding it-
erations, with a BLEU score of 32.02 on the de-
velopment test set, an improvement of 0.90 BLEU,
compared to MERT without random restarts.
While the difference in BLEU score with and
without random restarts was substantial, training
with random restarts took much longer. With
our phrasal decoder and our MERT implementa-
tion, optimizing feature weights took 3894 seconds
without random restarts and 12690 seconds with
random restarts.3 We therefore asked the question
whether there was some other way to invest extra
time in training feature weights that might be just
as effective as performing random restarts. The ob-
vious thing to try is using larger n-best lists, so we
re-ran the training without random restarts, using
n-best lists of 200 and 300.
Using n-best lists of 200 produced a noticeable
improvement in training without restarts, converg-
ing in 9 decoding iterations taking 7877 seconds,
and producing a BLEU score of 31.83 on the de-
velopment test set. Using n-best lists of 300 con-
verged in 8 decoding iterations taking 8973 sec-
onds, but the BLEU score on the development test
set fell back to 31.16. Thus, simply increasing the
size of the n-best list does not seem to be a reli-
able method for improving the results obtained by
MERT without random restarts.
4 Random Walk Restarts
In the procedure described above, the initial values
for each feature weight are independently sampled
3Timings are for single-threaded execution using a desk-
top PC with 3.60 GHz Intel Xeon processors.
587
from a uniform distribution over the range [?1, 1].
We have observed anecdotally, however, that if
the selected starting point itself produces a BLEU
score much below the best we have seen so far, co-
ordinate ascent search is very unlikely to take us to
a point that is better than the current best. In order
to bias the selection of restarting points towards
better scores, we select starting points by random
walk from the ending point of the last coordinate
ascent search.
The idea is to perform a series of cautious steps
in feature weight space guided by training set
BLEU. We begin the walk at the ending point of the
last coordinate ascent search; let us call this point
~w
(0)
. Each step updates the feature weights in a
manner inspired by Metropolis-Hastings sampling
(Hastings, 1970). Starting from the current feature
weight vector ~w(i), we sample a small update from
a multivariate Gaussian distribution with mean of
0 and diagonal covariance matrix ?2I . This update
is added to the current value to produce a new po-
tential feature weight vector. The BLEU scores for
the old and the new feature weight vector are com-
pared. The new feature weight vector is always
accepted if the BLEU score on the training set is
improved; however if the BLEU score drops, the
new vector is accepted with a probability that de-
pends on how close the new BLEU score is to the
previous one. After a fixed number of steps, the
walk is terminated, and we produce a value to use
as the initial point for the next round of coordinate
ascent.
There are several wrinkles, however. First, we
prefer that the scores not fall substantially during
the random walk. Therefore we establish a base-
line value of m = BLEU(~w(0)) ? 0.005 (i.e., 1/2
BLEU point below the initial value) and do not al-
low a step to go below this baseline value. To en-
sure this, each step progresses as follows:
~
d
(i)
? GAUSSIAN(0, ?2I)
~v
(i)
= ~w
(i)
+
~
d
(i)
u
(i)
? UNIFORM(0, 1)
~w
(i+1)
=
?
?
?
~v
(i) if BLEU(~v
(i)
)?m
BLEU(~w(i))?m ? u
(i)
~w
(i) otherwise.
With this update rule, we know that ~w(i+1) will
never go below m, since the initial value is not be-
low m, and any step moving below m will result
in a negative ratio and therefore not be accepted.
So far, ?2 is left as a free parameter. An ini-
tial value of 0.001 performs well in our experi-
ence, though in general it may result in steps that
are consistently too small (so that only a very lo-
cal neighborhood is explored) or too large (so that
the vast majority of steps are rejected). Therefore
we devote the first half of the steps to ?burn-in?;
that is, tuning the variance parameter so that ap-
proximately 60% of the steps are accepted. During
burn-in, we compute the acceptance rate after each
step. If it is less than 60%, we multiply ?2 by 0.99;
if greater, we multiply by 1.01.
The final twist is in selection of the point used
for the next iteration of coordinate ascent. Rather
than using the final point of the random walk ~w(n),
we return the feature weight vector that achieved
the highest BLEU score after burn-in: ~w? =
argmax
~w
(i)
,n/2<i?n
BLEU(~w). This ensures that
the new feature weight vector has a relatively high
objective function value yet is likely very different
from the initial point.
4.1 Preliminary evaluation
To evaluate the random walk selection procedure,
we used a similar experimental set-up to the previ-
ous one, testing on the 2006 English-French Eu-
roparl corpus, using n-best lists of 100, and 20
starting points for each coordinate ascent search?
one being the best point found for the previous
hypothesis set, and the other 19 selected by our
random walk procedure. We set the number of
steps to be used in each random walk to 500. This
procedure converged in 6 decoding iterations tak-
ing 8458 seconds, with a BLEU score of 32.13 on
the development test set. This is an improvement
of 0.11 BLEU over the uniform random restart
method, and it also took only 67% as much time.
The speed up was due to the fact that random walk
method converged in 2 fewer decoding iterations,
although the average time per iteration was greater
(1410 seconds vs. 1269 seconds) because of the
extra time needed for the random walk.
5 Hypothesis Set Pruning
MERT with random walk restarts seems to pro-
duce better models than either MERT with uniform
random restarts or with no restarts, but it is still
slower than MERT with no restarts by more than
a factor of 2. The difference between 3894 sec-
onds (1.08 hours) and 8458 seconds (2.35 hours) to
optimize feature weights may not seem important,
given how long the rest of the process of build-
588
ing and training an SMT system takes; however,
to truly optimize an SMT system would actually
require performing feature weight training many
times to find optimum values of hyper-parameters
such as maximum phrase size and distortion limit.
This kind of optimization is rarely done for every
small model change, because of how long feature
weight optimization takes; so it seems well worth
the effort to speed up the optimization process as
much as possible.
To try to speed up the feature weight optimiza-
tion process, we have tried pruning the set of hy-
potheses that MERT is applied to. The time taken
by the random walk and coordinate ascent phases
of MERT with random walk restarts is roughly lin-
ear in the number of translation hypotheses exam-
ined. In the experiment described in Section 4.1,
after the first 100-best decoding iteration there
were 196,319 hypotheses in the n-best lists, and
MERT took 347 seconds. After merging all hy-
potheses from 6 iterations of 100-best decoding
there were 800,580 hypotheses, and MERT took
1380 seconds.
We conjectured that a large proportion of these
hypotheses are both low scoring according to most
submodels and low in measured translation qual-
ity, so that omitting them would make little differ-
ence to the feature weight optimization optimiza-
tion process.4 We attempt to identify such hy-
potheses by extracting some additional informa-
tion from Och?s line search procedure.
Och?s line search procedure takes note of every
hypothesis that is the highest scoring hypothesis
for a particular sentence for some value of the fea-
ture weight being optimized by the line search.
The hypotheses that are never the highest scoring
hypothesis for any combination of feature values
explored effectively play no role in the MERT pro-
cedure. We conjectured that hypotheses that are
never selected as potentially highest scoring in a
particular round of MERT could be pruned from
the hypothesis set without adversely affecting the
quality of the feature weights eventually produced.
We tested two implementations of this type of
hypothesis pruning. In the more conservative im-
plementation, after each decoding iteration, we
note all the hypotheses that are ever ?touched?
(i.e., ever the highest scoring) during the coordi-
nate ascent search either from the initial starting
4Hypotheses that are of low translation quality, but high
scoring according to some submodels, need to be retained so
that the feature weights are tuned to avoid selecting them.
point or from one of the random restarts. Any hy-
pothesis that is never touched is pruned from the
sets of hypotheses that are merged with the results
of subsequent n-best decoding iterations. We refer
to this as ?post-restart? pruning.
In the more aggressive implementation, after
each decoding iteration, we note all the hypothe-
ses that are touched during the coordinate ascent
search from the initial starting point. The hypothe-
ses that are not touched are pruned from the hy-
pothesis set before any random restarts. We refer
to this as ?pre-restart? pruning.
5.1 Preliminary evaluation
We evaluated both post- and pre-restart pruning
with random walk restarts, under the same condi-
tions used to evaluate random walk restarts without
pruning. With post-restart pruning, feature weight
training converged in 8 decoding iterations taking
7790 seconds, with a BLEU score of 32.14 on the
development test set. The last set of restarts of
MERT had 276,134 hypotheses to consider, a re-
duction of more than 65% compared to no prun-
ing. With pre-restart pruning, feature weight train-
ing converged in 7 decoding iterations taking 4556
seconds, with a BLEU score of 32.17 on the devel-
opment test set. The last set of restarts of MERT
had only 64,346 hypotheses to consider, a reduc-
tion of 92% compared to no pruning.
Neither method of pruning degraded translation
quality as measured by BLEU; in fact, BLEU scores
increased by a trivial amount with pruning. Post-
restart pruning speeded up training only slightly,
primarily because it took more decoding iterations
to converge. Time per decoding iteration was re-
duced from 1409 seconds to 974 seconds. Pre-
restart pruning was substantially faster overall, as
well as in terms of time per decoding iteration,
which was 650 seconds.
Additional insight into the differences between
feature weight optimization methods can be gained
by evaluating the feature weight sets produced af-
ter each decoding iteration. Figure 1 plots the
BLEU score obtained on the development test set
as a function of the cumulative time taken to pro-
duce the corresponding feature weights, for each
of the training runs we have described so far.
We observe a clear gap between the results
obtained from random walk restarts and those
from either uniform random restarts or no restarts.
Note in particular that, although the uniform ran-
589
Restart Pruning Num Num Decoding Total MERT Dev-test Conf
Method Method Starts N-best Iterations Seconds Seconds BLEU Level
none none 1 100 8 3894 276 31.12 > 0.999
none none 1 300 8 8973 1173 31.17 > 0.999
none none 1 200 9 7877 717 31.83 0.999
uniform rand none 5 100 7 4294 917 31.95 0.993
uniform rand none 30 100 11 19345 13306 31.98 0.995
uniform rand none 20 100 10 12690 7613 32.02 0.984
uniform rand none 10 100 10 9059 3898 32.02 0.984
random walk pre-restart 30 100 12 9963 3016 32.04 0.999
random walk pre-restart 5 100 11 7696 619 32.10 0.962
random walk none 30 100 7 14055 10887 32.10 0.959
random walk pre-restart 10 100 14 8581 1254 32.10 0.986
random walk post-restart 10 100 9 6985 2236 32.11 0.938
random walk none 20 100 6 8458 5766 32.13 0.909
random walk post-restart 20 100 8 7790 3965 32.14 0.857
random walk none 10 100 8 8338 4280 32.15 0.840
random walk pre-restart 20 100 7 4556 1179 32.17 0.712
random walk post-restart 5 100 10 6103 1114 32.18 0.794
random walk post-restart 30 100 8 9811 6218 32.20 0.554
random walk none 5 100 10 7741 3047 32.21 0.000
Table 1: Extended Evaluation Results.
dom restart method eventually comes within 0.15
BLEU points of the best result using random walk
restarts, it takes far longer to get there. The uni-
form restart run produces noticably inferior BLEU
scores until just before convergence, while with the
random walk method, the BLEU score increases
quite quickly and then stays essentially flat for sev-
eral iterations before convergence.
We also note that there appears to be less real
difference among our three variations on random
walk restarts than there might seem to be from
their times to convergence. Although pre-restart
pruning was much faster to convergence than ei-
ther of the other variants, all three reached approx-
imately the same BLEU score in the same amount
of time, if intermediate points are considered. This
suggests that our convergence test, while more lib-
eral than Och?s, still may be more conservative
than necessary when using random walk restarts.
6 Extended Evaluation
We now extend the previous evaluations in two
ways. First, we repeat all the experiments on opti-
mization with 20 starting points, using 5, 10, and
30 starting points, to see whether we can trade off
training time for translation quality by changing
that parameter setting, and if so, whether any set-
tings seem clearly better than others.
Second, we note that different optimization
methods lead to convergence at different numbers
of decoding iterations. This means that which
method produces the shortest total training time
will depend on the relative time taken by n-best
decoding and the MERT procedure itself (includ-
ing the random walk selection procedure, if that
is used). By co-incidence, these times happened
to be roughly comparable in our experiments.5 In
a situation where decoding is much slower than
MERT, however, the main determinant of overall
training time would be how many decoding iter-
ations are needed. On the other hand, if decod-
ing was made much faster, say, through algorith-
mic improvements or by using a compute cluster,
total training time would be dominated by MERT
proper. We therefore report number of decoding it-
erations to convergence and pure MERT time (ex-
cluding decoding and hypothesis set merging) for
each of our experiments, in addition to total feature
weight training time.
Table 1 reports these three measures of com-
5In our complete set of training experiments encompass-
ing 187 decoding iterations, decoding averaged 521 seconds
per iteration, and MERT (excluding decoding and hypothesis
set merging) averaged 421 seconds per (decoding) iteration.
590
puational effort, plus BLEU score on the devel-
opment test set, sorted by ascending BLEU score,
for 19 variations on MERT: 3 n-best list sizes for
MERT without restarts, and 4 different numbers
of restarts for 4 different versions of MERT with
restarts (uniform random selection, random walk
selection without pruning, random walk selection
with post-restart pruning, and random walk selec-
tion with pre-restart pruning).
The final column of Table 1 is a confidence
score reflecting the estimated probability that the
translation model produced (at convergence) by
the MERT variant for that row of the table is not
as good in terms of BLEU score as the variant that
yielded the highest BLEU score (at convergence)
that we observed in these experiments. These
probabilities were estimated by Koehn?s (2004)
paired bootstrap resampling method, run for at
least 100,000 samples per comparison.
The 11 models obtaining a BLEU score of 31.10
or less are all estimated to be at least 95% likely to
have worse translation quality than the best scor-
ing model. We therefore dismiss these models
from further consideration,6 including all models
trained without random restarts, as well as all mod-
els trained with uniform random restarts, leaving
only models trained with random walk restarts.
With random walk restarts, post-restart prun-
ing remains under consideration at all numbers
of restarts tried. For random walk restarts with-
out pruning, only the model produced by 30 start-
ing points has been eliminated, and for pre-restart
pruning, only the model produced by 20 starts re-
mains under consideration. This suggests that pre-
restart pruning may be too aggressive and, thus,
overly sensitive to the number of restarts.
To get a better picture of the remaining 8 mod-
els, see Figures 2?4. Despite convergence times
ranging from 4456 to 9811 seconds, in Figure 2 we
observe that, if feature weights after each decoding
iteration are considered, the relationships between
training time and BLEU score are remarkably sim-
ilar. In Figure 3, BLEU score varies considerably
up to 3 decoding iterations, but above that, BLEU
scores are very close, and almost flat. In fact, we
see almost no benefit from more than 7 decoding
iterations for any model.
Finally, Figure 4 shows some noticeable differ-
ences between random walk variants in respect to
6We estimate the joint probability that these 11 models are
all worse than our best scoring model to be 0.882, by multi-
plying the confidence scores for all these models.
MERT time proper. Thus, while the choice of ran-
dom walk variant chosen may matter little if de-
coding is slow, it seems that it can have an im-
portant impact if decoding is fast. If we com-
bine the results shown here with the observation
from Figure 3 that there seems to be no benefit to
trying more than 7 decoding iterations, it appears
that perhaps the best trade-off between translation
quality and training time would be obtained by us-
ing post-restart pruning, with 5 starting points per
decoding iteration, cutting off training after 7 iter-
ations. This took a total of 4009 seconds to train,
compared to 7741 seconds for the highest scoring
model on the development test set considered in
Table 1 (produced by random walk restarts with no
pruning, 5 starting points per decoding iteration, at
convergence after 10 iterations).
To validate the proposal to use the suggested
faster training procedure, we compared the two
models under discussion on the 2000 in-domain
sentence pairs for the designated final test set for
our corpus. The model produced by the sug-
gested training procedure resulted in a BLEU score
of 31.92, with the model that scored highest on
the development test set scoring an insignificantly
worse 31.89. In contast, the highest scoring model
of the three trained with no restarts produced a
BLEU score of 31.55 on the final test set, which
was worse than either of the random walk methods
evaluated on the final test set, at confidence levels
exceeding 0.996 according to the paired bootstrap
resampling test.
7 Conclusions
We believe that our results show very convinc-
ingly that using random restarts in MERT im-
proves the BLEU scores produced by the result-
ing models. They also seem to show that starting
point selection by random walk is slightly superior
to uniform random selection. Finally, our exper-
iments suggest that time to carry out MERT can
be significantly reduced by using as few as 5 start-
ing points per decoding iteration, performing post-
restart pruning of hypothesis sets, and cutting off
training after a fixed number of decoding iterations
(perhaps 7) rather than waiting for convergence.
References
Hastings, W. Keith. 1970. Monte Carlo sampling
methods using Markov chains and their applica-
tions. Biometrika 57: 97?109.
591
Koehn, Philipp, and Christof Monz. 2006. Manual
and automatic evaluation of machine translation
between European languages. In Proceedings
of the Workshop on Statistical Machine Trans-
lation, New York City, USA, 102?121.
Koehn, Philipp, Franz Josep Och, and Daniel
Marcu. 2003. Statistical Phrase-Based Trans-
lation. In Proceedings of Human Language
Technology Conference of the North American
Chapter of the Association for Computational
Linguistics, Edmondton, Alberta, Canada, 127?
133.
Koehn, Philipp. 2004. Statistical significance
tests for machine translation evaluation. In Pro-
ceedings of the 2004 Conference on Empiri-
cal Methods in Natural Language Processing,
Barcelona, Spain, 388?395.
Koehn, Philipp, et al 2007. Open Source Toolkit
for Statistical Machine Translation: Factored
Translation Models and Confusion Network De-
coding. Final Report of the 2006 Language En-
gineering Workshop, Johns Hopkins University,
Center for Speech and Language Processing.
Och, Franz Josef. 2003. Minimum error rate train-
ing in statistical machine translation. In Pro-
ceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, Sapporo,
Japan, 160?167.
Press, William H., et al 2002. Numerical Recipes
in C++. Cambridge University Press, Cam-
bridge, UK.
30
31
32
33
0 2000 4000 6000 8000 10000 12000 14000
B
L
E
U
s
c
o
r
e
total time in seconds
Figure 1
random walk, pre-
restart pruning, 20 
iterationsrandom walk, post-
restart pruning, 20 
iterationsrandom walk, no 
pruning, 20 iterations
uniform random 
restarts, 20 iterations
no restarts, n-best = 100
no restarts, n-best = 200
no restarts, n-best = 300
30
31
32
33
0 5000 10000
B
L
E
U
s
c
o
r
e
total time in seconds
Figure 2
random walk, pre-restart 
pruning, 20 iterations
random walk, post-
restart pruning, 5 
iterations
random walk, post-
restart pruning, 10 
iterations
random walk, post-
restart pruning, 20 
iterations
random walk, post-
restart pruning, 30 
iterations
random walk, no 
pruning, 5 iterations
random walk, no 
pruning, 10 iterations
random walk, no 
pruning, 20 iterations
30
31
32
33
0 1 2 3 4 5 6 7 8 9 10 11
B
L
E
U
s
c
o
r
e
decoding iterations
Figure 3
random walk, pre-restart 
pruning, 20 iterations
random walk, post-
restart pruning, 5 
iterations
random walk, post-
restart pruning, 10 
iterations
random walk, post-
restart pruning, 20 
iterations
random walk, post-
restart pruning, 30 
iterations
random walk, no 
pruning, 5 iterations
random walk, no 
pruning, 10 iterations
random walk, no 
pruning, 20 iterations
30
31
32
33
0 1000 2000 3000 4000 5000 6000 7000
B
L
E
U
s
c
o
r
e
MERT time in seconds
Figure 4
random walk, pre-restart 
pruning, 20 iterations
random walk, post-restart 
pruning, 5 iterations
random walk, post-restart 
pruning, 10 iterations
random walk, post-restart 
pruning, 20 iterations
random walk, post-restart 
pruning, 30 iterations
random walk, no 
pruning, 5 iterations
random walk, no 
pruning, 10 iterations
random walk, no 
pruning, 20 iterations
592
