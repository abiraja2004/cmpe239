Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 30?40,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Really? Well. Apparently Bootstrapping Improves the Performance of
Sarcasm and Nastiness Classifiers for Online Dialogue
Stephanie Lukin
Natural Language and Dialogue Systems
University of California, Santa Cruz
1156 High Street, Santa Cruz, CA 95064
slukin@soe.ucsc.edu
Marilyn Walker
Natural Language and Dialogue Systems
University of California, Santa Cruz
1156 High Street, Santa Cruz, CA 95064
maw@soe.ucsc.edu
Abstract
More and more of the information on the web
is dialogic, from Facebook newsfeeds, to fo-
rum conversations, to comment threads on
news articles. In contrast to traditional, mono-
logic Natural Language Processing resources
such as news, highly social dialogue is fre-
quent in social media, making it a challenging
context for NLP. This paper tests a bootstrap-
ping method, originally proposed in a mono-
logic domain, to train classifiers to identify
two different types of subjective language in
dialogue: sarcasm and nastiness. We explore
two methods of developing linguistic indica-
tors to be used in a first level classifier aimed
at maximizing precision at the expense of re-
call. The best performing classifier for the first
phase achieves 54% precision and 38% recall
for sarcastic utterances. We then use general
syntactic patterns from previous work to cre-
ate more general sarcasm indicators, improv-
ing precision to 62% and recall to 52%. To
further test the generality of the method, we
then apply it to bootstrapping a classifier for
nastiness dialogic acts. Our first phase, using
crowdsourced nasty indicators, achieves 58%
precision and 49% recall, which increases to
75% precision and 62% recall when we boot-
strap over the first level with generalized syn-
tactic patterns.
1 Introduction
More and more of the information on the web is
dialogic, from Facebook newsfeeds, to forum con-
versations, to comment threads on news articles. In
contrast to traditional, monologic Natural Language
Processing resources such as news, highly social di-
alogue is very frequent in social media, as illustrated
in the snippets in Fig. 1 from the publicly avail-
able Internet Argument Corpus (IAC) (Walker et al,
Quote Q, Response R Sarc Nasty
Q1: I jsut voted. sorry if some people actu-
ally have, you know, LIVES and don?t sit around
all day on debate forums to cater to some athe-
ists posts that he thiks they should drop every-
thing for. emoticon-rolleyes emoticon-rolleyes
emoticon-rolleyes As to the rest of your post, well,
from your attitude I can tell you are not Christian
in the least. Therefore I am content in knowing
where people that spew garbage like this will end
up in the End.
R1: No, let me guess . . . er . . . McDonalds. No,
Disneyland. Am I getting closer?
1 -3.6
Q2: The key issue is that once children are born
they are not physically dependent on a particular
individual.
R2 Really? Well, when I have a kid, I?ll be sure to
just leave it in the woods, since it can apparently
care for itself.
1 -1
Q3: okay, well i think that you are just finding
reasons to go against Him. I think that you had
some bad experiances when you were younger or
a while ago that made you turn on God. You are
looking for reasons, not very good ones i might
add, to convince people.....either way, God loves
you. :)
R3: Here come the Christians, thinking they can
know everything by guessing, and commiting the
genetic fallacy left and right.
0.8 -3.4
Figure 1: Sample Quote/Response Pairs from
4forums.com with Mechanical Turk annotations
for Sarcasm and Nasty/Nice. Highly negative values
of Nasty/Nice indicate strong nastiness and sarcasm is
indicated by values near 1.
2012). Utterances are frequently sarcastic, e.g., Re-
ally? Well, when I have a kid, I?ll be sure to just
leave it in the woods, since it can apparently care
for itself (R2 in Fig. 1 as well as Q1 and R1), and are
often nasty, e.g. Here come the Christians, thinking
they can know everything by guessing, and commit-
ing the genetic fallacy left and right (R3 in Fig. 1).
Note also the frequent use of dialogue specific dis-
course cues, e.g. the use of No in R1, Really? Well
in R2, and okay, well in Q3 in Fig. 1 (Fox Tree
and Schrock, 1999; Bryant and Fox Tree, 2002; Fox
Tree, 2010).
30
The IAC comes with annotations of different
types of social language categories including sarcas-
tic vs not sarcastic, nasty vs nice, rational vs emo-
tional and respectful vs insulting. Using a conser-
vative threshold of agreement amongst the annota-
tors, an analysis of 10,003 Quote/Response pairs
(Q/R pairs) from the 4forums portion of IAC sug-
gests that social subjective language is fairly fre-
quent: about 12% of posts are sarcastic, 23% are
emotional, and 12% are insulting or nasty. We select
sarcastic and nasty dialogic turns to test our method
on more than one type of subjective language and
explore issues of generalization; we do not claim any
relationship between these types of social language
in this work.
Despite their frequency, expanding this corpus of
sarcastic or nasty utterances at scale is expensive:
human annotation of 100% of the corpus would be
needed to identify 12% more examples of sarcasm
or nastiness. An explanation of how utterances are
annotated in IAC is detailed in Sec. 2.
Our aim in this paper is to explore whether it is
possible to extend a method for bootstrapping a clas-
sifier for monologic, subjective sentences proposed
by Riloff & Wiebe, henceforth R&W (Riloff and
Wiebe, 2003; Thelen and Riloff, 2002), to automat-
ically find sarcastic and nasty utterances in unanno-
tated online dialogues. Sec. 3 provides an overview
of R&W?s bootstrapping method. To apply boot-
strapping, we:
1. Explore two different methods for identifying
cue words and phrases in two types of subjec-
tive language in dialogues: sarcasm and nasty
(Sec. 4);
2. Use the learned indicators to train a sarcastic
(nasty) dialogue act classifier that maximizes
precision at the expense of recall (Sec. 5);
3. Use the classified utterances to learn general
syntactic extraction patterns from the sarcastic
(nasty) utterances (Sec. 6);
4. Bootstrap this process on unannotated text to
learn new extraction patterns to use for classifi-
cation.
We show that the Extraction Pattern Learner im-
proves the precision of our sarcasm classifier by
17% and the recall by 24%, and improves the pre-
cision of the nastiness classifier by 14% and recall
by 13%. We discuss previous work in Sec. 2 and
compare to ours in Sec. 7 where we also summarize
our results and discuss future work.
2 Previous Work
IAC provides labels for sarcasm and nastiness that
were collected with Mechanical Turk on Q/R pairs
such as those in Fig. 1. Seven Turkers per Q/R pair
answered a binary annotation question for sarcasm
Is the respondent using sarcasm? (0,1) and a scalar
annotation question for nastiness Is the respondent
attempting to be nice or is their attitude fairly nasty?
(-5 nasty . . . 5 nice). We selected turns from IAC
Table 1 with sarcasm averages above 0.5, and nasty
averages below -1 and nice above 1. Fig. 1 included
example nastiness and sarcasm values.
Previous work on the automatic identification
of sarcasm has focused on Twitter using the
#sarcasm (Gonza?lez-Iba?n?ez et al, 2011) and
#irony (Reyes et al, 2012) tags and a combined
variety of tags and smileys (Davidov et al, 2010).
Another popular domain examines Amazon product
reviews looking for irony (Reyes and Rosso, 2011),
sarcasm (Tsur et al, 2010), and a corpus collec-
tion for sarcasm (Filatova, 2012). (Carvalho et al,
2009) looks for irony in comments in online newpa-
pers which can have a thread-like structure. This
primary focus on monologic venues suggests that
sarcasm and irony can be detected with a relatively
high precision but have a different structure from di-
alogues (Fox Tree and Schrock, 1999; Bryant and
Fox Tree, 2002; Fox Tree, 2010), posing the ques-
tion, can we generalize from monologic to dialogic
structures? Each of these works use methods in-
cluding LIWC unigrams, affect, polarity, punctua-
tion and more, and achieve on average a precision of
75% or accuracy of between 45% and 85%.
Automatically identifying offensive utterances is
also of interest. Previous work includes identifying
flames in emails (Spertus, 1997) and other messag-
ing interfaces (Razavi et al, 2010), identifying in-
sults in Twitter (Xiang et al, 2012), as well as com-
ments from new sites (Sood et al, 2011). These
approaches achieve an accuracy between 64% and
83% using a variety of approaches. The accuracies
for nasty utterances has a much smaller spread and
higher average than sarcasm accuracies. This sug-
gests that nasty language may be easier to identify
than sarcastic language.
3 Method Overview
Our method for bootstrapping a classifier for sarcas-
tic (nasty) dialogue acts uses R&W?s model adapted
to our data as illustrated for sarcasm in Fig. 2. The
31
Figure 2: Bootstrapping Flow for Classifying Subjective
Dialogue Acts, shown for sarcasm, but identical for nas-
tiness.
overall idea of the method is to find reliable cues and
then generalize. The top of Fig. 2 specifies the input
to the method as an unannotated corpus of opinion
dialogues, to illustrate the long term aim of building
a large corpus of the phenomenon of interest with-
out human annotation. Although the bootstrapping
method assumes that the input is unannotated text,
we first need utterances that are already labeled for
sarcasm (nastiness) to train it. Table 1 specifies how
we break down into datasets the annotations on the
utterances in IAC for our various experiments.
The left circle of Fig. 2 reflects the assump-
tion that there are Sarcasm or Nasty Cues that can
identify the category of interest with high preci-
sion (R&W call this the ?Known Subjective Vocab-
ulary?). The aim of first developing a high preci-
sion classifier, at the expense of recall, is to select
utterances that are reliably of the category of inter-
est from unannotated text. This is needed to ensure
that the generalization step of ?Extraction Pattern
Learner? does not introduce too much noise.
R&W did not need to develop a ?Known Sub-
jective Vocabulary? because previous work provided
one (Wilson et al, 2005; Wiebe et al, 1999; Wiebe
et al, 2003). Thus, our first question with applying
R&W?s method to our data was whether or not it is
possible to develop a reliable set of Sarcasm (Nas-
tiness) Cues (O1 below). Two factors suggest that
it might not be. First, R&W?s method assumes that
the cues are in the utterance to be classified, but it
has been claimed that sarcasm (1) is context depen-
dent, and (2) requires world knowledge to recognize,
SARCASM #sarc #notsarc total
MT exp dev 617 NA 617
HP train 1407 1404 2811
HP dev test 1614 1614 3228
PE eval 1616 1616 3232
All 5254 4635 9889
NASTY #nasty #nice total
MT exp dev 510 NA 510
HP train 1147 1147 2294
HP dev test 691 691 1382
PE eval 691 691 1382
All 3039 2529 5568
Table 1: How utterances annotated for sarcasm (top) and
nastiness (bottom) in IAC were used. MT = Mechanical
Turk experimental development set. HP train = utter-
ances used to test whether combinations of cues could be
used to develop a High precision classifier. HP dev test
= ?Unannotated Text Collection? in Fig. 2. PE eval =
utterances used to train the Pattern Classifier.
at least in many cases. Second, sarcasm is exhibited
by a wide range of different forms and with differ-
ent dialogue strategies such as jocularity, understate-
ment and hyberbole (Gibbs, 2000; Eisterhold et al,
2006; Bryant and Fox Tree, 2002; Filatova, 2012).
In Sec. 4 we devise and test two different methods
for acquiring a set of Sarcasm (Nastiness) Cues on
particular development sets of dialogue turns called
the ?MT exp dev? in Table 1.
The boxes labeled ?High Precision Sarcastic Post
Classifier? and ?High Precision Not Sarcastic Post
Classifier? in Fig. 2 involves using the Sarcasm
(Nastiness) Cues in simple combinations that max-
imize precision at the expense of recall. R&W
found cue combinations that yielded a High Preci-
sion Classifier (HP Classifier) with 90% precision
and 32% recall on their dataset. We discuss our test
of these steps in Sec. 5 on the ?HP train? develop-
ment sets in Table 1 to estimate parameters for the
High Precision classifier, and then test the HP classi-
fier with these parameters on the test dataset labeled
?HP dev test? in Table 1.
R&W?s Pattern Based classifier increased recall
to 40% while losing very little precision. The open
question with applying R&W?s method to our data,
was whether the cues that we discovered, by what-
ever method, would work at high enough precision
to support generalization (O2 below). In Sec. 6 we
32
describe how we use the ?PE eval? development set
(Table 1) to estimate parameters for the Extraction
Pattern Learner, and then test the Pattern Based Sar-
castic (Nasty) Post classifier on the newly classified
utterances from the dataset labeled ?HP dev test?
(Table 1). Our final open question was whether the
extraction patterns from R&W, which worked well
for news text, would work on social dialogue (O3
below). Thus our experiments address the following
open questions as to whether R&W?s bootstrapping
method improves classifiers for sarcasm and nasti-
ness in online dialogues:
? (O1) Can we develop a ?known sarcastic
(nasty) vocabulary?? The LH circle of Fig. 2
illustrates that we use two different methods to
identify Sarcasm Cues. Because we have ut-
terances labeled as sarcastic, we compare a sta-
tistical method that extracts important features
automatically from utterances, with a method
that has a human in the loop, asking annotators
to select phrases that are good indicators of sar-
casm (nastiness) (Sec. 5);
? (O2) If we can develop a reliable set of sarcasm
(nastiness) cues, is it then possible to develop
an HP classifier? Will our precision be high
enough? Is the fact that sarcasm is often con-
text dependent an issue? (Sec. 5);
? (O3) Will the extraction patterns used in
R&W?s work allow us to generalize sarcasm
cues from the HP Classifiers? Are R&W?s pat-
terns general enough to work well for dialogue
and social language? (Sec. 6).
4 Sarcasm and Nastiness Cues
Because there is no prior ?Known Sarcastic Vocabu-
lary? we pilot two different methods for discovering
lexical cues to sarcasm and nastiness, and experi-
ment with combinations of cues that could yield a
high precision classifier (Gianfortoni et al, 2011).
The first method uses ?2 to measure whether a word
or phrase is statistically indicative of sarcasm (nasti-
ness) in the development sets labeled ?MT exp dev?
(Table 1). This method, a priori, seems reasonable
because it is likely that if you have a large enough
set of utterances labeled as sarcastic, you could be
able to automatically learn a set of reliable cues for
sarcasm.
The second method introduces a step of human
annotation. We ask Turkers to identify sarcastic
(nasty) indicators in utterances (the open question
unigram
?2 MT IA FREQ
right ah .95 2
oh relevant .85 2
we amazing .80 2
same haha .75 2
all yea .73 3
them thanks .68 6
mean oh .56 56
bigram
?2 MT IA FREQ
the same oh really .83 2
mean like oh yeah .79 2
trying to so sure .75 2
that you no way .72 3
oh yeah get real .70 2
I think oh no .66 4
we should you claim .65 2
trigram
?2 MT IA FREQ
you mean to I get it .97 3
mean to tell I?m so sure .65 2
have to worry then of course .65 2
sounds like a are you saying .60 2
to deal with well if you .55 2
I know I go for it .52 2
you mean to oh, sorry .50 2
Table 2: Mechanical Turk (MT) and ?2 indicators for
Sarcasm
O1) from the development set ?MT exp dev? (Ta-
ble 1). Turkers were presented with utterances pre-
viously labeled sarcastic or nasty in IAC by 7 dif-
ferent Turkers, and were told ?In a previous study,
these responses were identified as being sarcastic by
3 out of 4 Turkers. For each quote/response pair,
we will ask you to identify sarcastic or potentially
sarcastic phrases in the response?. The Turkers then
selected words or phrases from the response they be-
lieved could lead someone to believing the utterance
was sarcastic or nasty. These utterances were not
used again in further experiments. This crowdsourc-
ing method is similar to (Filatova, 2012), but where
their data is monologic, ours is dialogic.
4.1 Results from Indicator Cues
Sarcasm is known to be highly variable in form, and
to depend, in some cases, on context for its inter-
pretation (Sperber and Wilson, 1981; Gibbs, 2000;
Bryant and Fox Tree, 2002). We conducted an ini-
tial pilot on 100 of the 617 sarcastic utterances in
33
unigram
?2 MT IA FREQ
like idiot .90 3
them unfounded .85 2
too babbling .80 2
oh lie .72 11
mean selfish .70 2
just nonsense .69 9
make hurt .67 3
bigram
?2 MT IA FREQ
of the don?t expect .95 2
you mean get your .90 2
yes, you?re an .85 2
oh, what?s your .77 4
you are prove it .77 3
like a get real .75 2
I think what else .70 2
trigram
?2 MT IA FREQ
to tell me get your sick .75 2
would deny a your ignorance is .70 2
like that? make up your .70 2
mean to tell do you really .70 2
sounds like a do you actually .65 2
you mean to doesn?t make it .63 3
to deal with what?s your point .60 2
Table 3: Mechanical Turk (MT) and ?2 indicators for
Nasty
Figure 3: Interannotator Agreement for sarcasm trigrams
the development set ?MT exp dev? to see if this was
necessarily the case in our dialogues. (Snow et al,
2008) measures the quality of Mechanical Turk an-
notations on common NLP tasks by comparing them
to a gold standard. Pearson?s correlation coefficient
shows that very few Mechanical Turk annotators
were required to beat the gold standard data, often
less than 5. Because our sarcasm task does not have
gold standard data, we ask 100 annotators to partic-
ipate in the pilot. Fig. 3 plots the average interan-
notator agreement (ITA) as a function of the number
of annotators, computed using Pearson correlation
counts, for 40 annotators and for trigrams which re-
quire more data to converge. In all cases (unigrams,
bigrams, trigrams) ITA plateaus at around 20 anno-
tators and is about 90% with 10 annotators, showing
that the Mechanical Turk tasks are well formed and
there is high agreement. Thus we elicited only 10
annotations for the remainder of the sarcastic and all
the nasty utterances from the development set ?MT
exp dev?.
We begin to form our ?known sarcastic vocab-
ulary? from these indicators, (open question O1).
Each MT indicator has a FREQ (frequency): the
number of times each indicator appears in the train-
ing set; and an IA (interannotator agreement): how
many annotators agreed that each indicator was sar-
castic or nasty. Table 2 shows the best unigrams,
bigrams, and trigrams from the ?2 test and from the
sarcasm Mechanical Turk experiment and Table 3
shows the results from the nasty experiment. We
compare the MT indicators to the ?2 indicators as
part of investigating open question O1.
As a pure statistical method, ?2 can pick out
things humans might not. For example, if it just hap-
pened that the word ?we? only occurs in sarcastic
utterances in the development set, then ?2 will se-
lect it as a strong sarcastic word (row 3 of Table 2).
However, no human would recognize this word as
corresponding to sarcasm. ?2 could easily be over-
trained if the ?MT exp dev? development set is not
large enough to eliminate such general words from
consideration, ?MT exp dev? only has 617 sarcastic
utterances and 510 nasty utterances (Table 1).
Words that the annotators select as indicators
(columns labeled MT in Table 2 and Table 3) are
much more easily identifiable although they do not
appear as often. For example, the IA of 0.95 for ?ah?
in Table 2 means that of all the annotators who saw
?ah? in the utterance they annotated, 95% selected it
to be sarcastic. However the FREQ of 2 means that
?ah? only appeared in 2 utterances in the ?MT exp
dev? development set.
We test whether any of the methods for select-
ing indicators provide reliable cues that generalize
to a larger dataset in Sec. 5. The parameters that
we estimate on the development sets are exactly how
frequent (compared to a ?1) and how reliable (com-
34
pared to a ?2) a cue has to be to be useful in R&W?s
bootstrapping method.
5 High-Precision Classifiers
R&W use their ?known subjective vocabulary? to
train a High Precision classifier. R&W?s HP classi-
fier searches for exact surface matches of the sub-
jective indicators and classifies utterances as sub-
jective if two subjective indicators are present. We
follow similar guidelines to train HP Sarcasm and
Nasty Classifiers. To test open question O1, we
use a development set called ?HP train? (Table 1)
to test three methods for measuring the ?goodness?
of an indicator that could serve as a high precision
cue: (1) interannotator agreement based on anno-
tators consensus from Mechanical Turk, on the as-
sumption that the number of annotators that select
a cue indicates its strength and reliability (IA fea-
tures); (2) percent sarcastic (nasty) and frequency
statistics in the HP train dataset as R&W do (percent
features); and (3) the ?2 percent sarcastic (nasty)
and frequency statistics (?2 features).
The IA features use the MT indicators and the IA
and FREQ calculations introduced in Sec. 4 (see
Tables 2 and 3). First, we select indicators such
that ?1 <= FREQ where ?1 is a set of possible
thresholds. Then we introduce two new parameters
? and ? to divide the indicators into three ?good-
ness? groups that reflect interannotator agreement.
indicatorstrength =
{
weak if 0 ? IA < ?
medium if ? ? IA < ?
strong if ? ? IA < 1
For IA features, an utterance is classified as sar-
castic if it contains at least one strong or two medium
indicators. Other conditions were piloted. We first
hypothesized that weak cues might be a way of
classifying ?not sarcastic? utterances. But HP train
showed that both sarcastic and not sarcastic utter-
ances contain weak indicators yielding no informa-
tion gain. The same is true for Nasty?s counter-
class Nice. Thus we specify that counter-class utter-
ances must have no strong indicators or at most one
medium indicator. In contrast, R&W?s counter-class
classifier looks for a maximum of one subjective in-
dicator.
The percent features also rely on the FREQ of
each MT indicator, subject to a ?1 threshold, as
well as the percentage of the time they occur in
a sarcastic utterance (%SARC) or nasty utterance
(%NASTY). We select indicators with various pa-
rameters for ?1 and ?2 ? %SARC. At least two in-
dicators must be present and above the thresholds to
be classified and we exhaust all combinations. Less
than two indicators are needed to be classified as the
counter-class, as in R&W.
Finally, the ?2 features use the same method as
percent features only using the ?2 indicators instead
of the MT indicators.
After determining which parameter settings per-
forms the best for each feature set, we ran the HP
classifiers, using each feature set and the best param-
eters, on the test set labeled ?HP dev test?. The HP
Classifiers classify the utterances that it is confident
on, and leave others unlabeled.
5.1 Results from High Precision Classifiers
The HP Sarcasm and Nasty Classifiers were trained
on the three feature sets with the following parame-
ters: IA features we exhaust all combinations of ? =
[.70, .75, .80, .85, .90, .95, 1.00], ? = [.35, .40, .45,
.50, .55, .60, .65, .7], and ?1 = [2, 4, 6, 8, 10]; for the
percent features and ?2 features we again exhaust ?1
= [2, 4, 6, 8, 10] and ?2 = [.55, .60, .65, .70, .75, .80,
.85, .90, .95, 1.00].
Tables 4 and 5 show a subset of the experiments
with each feature set. We want to select parame-
ters that maximize precision without sacrificing too
much recall. Of course, the parameters that yield
the highest precision also have the lowest recall, e.g.
Sarcasm percent features, parameters ?1 = 4 and
?2 = 0.75 achieve 92% precision but the recall is
1% (Table 4), and Nasty percent features with pa-
rameters ?1 = 8 and ?2 = 0.8 achieves 98% preci-
sion but a recall of 3% (Table 5). On the other end of
the spectrum, the parameters that achieve the highest
recall yield a precision equivalent to random chance.
Examining the parameter combinations in Ta-
bles 4 and 5 shows that percent features do better
than IA features in all cases in terms of precision.
Compare the block of results labeled % in Tables 4
and 5 with the IA and ?2 blocks for column P. Nasty
appears to be easier to identify than Sarcasm, espe-
cially using the percent features. The performance
of the ?2 features is comparable to that of percent
features for sarcasm, but lower than percent features
for Nasty.
The best parameters selected from each feature
set are shown in the PARAMS column of Table 6.
With the indicators learned from these parameters,
we run the Classifiers on the test set labeled ?HP
35
SARC PARAMS P R N (tp)
% ?1 =4, ?2 =.55 62% 55% 768
4, .6 72% 32% 458
4, .65 84% 12% 170
4, .75 92% 1% 23
IA ?1 =2, ? =.90, ? =.35 51% 73% 1,026
2, .95, .55 62% 13% 189
2, .9, .55 54% 34% 472
4, .75, .5 64% 7% 102
4, .75, .6 78% 1% 22
?2 ?1 =8, ?2 =.55 59% 64% 893
8, .6 67% 31% 434
8, .65 70% 12% 170
8, .75 93% 1% 14
Table 4: Sarcasm Train results; P: precision, R: recall, tp:
true positive classifications
NASTY PARAMS P R N (tp)
% ?1 =2, ?2 =.55 65% 69% 798
4, .65 80% 44% 509
8, .75 95% 11% 125
8, .8 98% 3% 45
IA ?1 =2, ? =.95, ? =.35 50% 96% 1,126
2, .95, .45 60% 59% 693
4, .75, .45 60% 50% 580
2, .7, .55 73% 12% 149
2, .9, .65 85% 1% 17
?2 ?1 =2, ?2 =.55 73% 15% 187
2, .65 78% 8% 104
2, .7 86% 3% 32
Table 5: Nasty Train results; P: precision, R: recall, tp:
true positive classifications
dev test? (Table 1). The performance on test set ?HP
dev test? (Table 6) is worse than on the training set
(Tables 4 and 5). However we conclude that both
the % and ?2 features provide candidates for sar-
casm (nastiness) cues that are high enough precision
(open question O2) to be used in the Extraction Pat-
tern Learner (Sec. 6), even if Sarcasm is more con-
text dependent than Nastiness.
PARAMS P R F
Sarc % ?1 =4, ?2 =.55 54% 38% 0.46
Sarc IA ?1 =2, ? =.95, ? =.55 56% 11% 0.34
Sarc ?2 ?1 =8, ?2 =.60 60% 19% 0.40
Nasty % ?1 =2, ?2 =.55 58% 49% 0.54
Nasty IA ?1 =2, ? =.95, ? =.45 53% 35% 0.44
Nasty ?2 ?1 =2, ?2 =.55 74% 14% 0.44
Table 6: HP Dev test results; PARAMS: the best pa-
rameters for each feature set P: precision, R: recall, F:
f-measure
6 Extraction Patterns
R&W?s Pattern Extractor searches for instances of
the 13 templates in the first column of Table 7 in ut-
terances classified by the HP Classifier. We reim-
plement this; an example of each pattern as in-
stantiated in test set ?HP dev test? for our data is
shown in the second column of Table 7. The tem-
plate <subj> active-verb <dobj> matches ut-
terances where a subject is followed by an active
verb and a direct object. However, these matches
are not limited to exact surface matches as the HP
Classifiers required, e.g. this pattern would match
the phrase ?have a problem?. Table 10 in the Ap-
pendix provides example utterances from IAC that
match the instantiated template patterns. For exam-
ple, the excerpt from the first row in Table 10 ?It
is quite strange to encounter someone in this day
and age who lacks any knowledge whatsoever of the
mechanism of adaptation since it was explained 150
years ago? matches the <subj> passive-verb
pattern. It appears 2 times (FREQ) in the test set
and is sarcastic both times (%SARC is 100%). Row
11 in Table 10 shows an utterance matching the
active-verb prep <np> pattern with the phrase
?At the time of the Constitution there weren?t ex-
actly vast suburbs that could be prowled by thieves
looking for an open window?. This phrase appears
14 times (FREQ) in the test set and is sarcastic
(%SARC) 92% of the time it appears.
Synactic Form Example Pattern
<subj> passive-verb <subj> was explained
<subj> active-verb <subj> appears
<subj> active-verb dobj <subj> have problem
<subj> verb infinitive <subj> have to do
<subj> aux noun <subj> is nothing
active-verb <dobj> gives <dobj>
infinitive <dobj> to force <dobj>
verb infinitive <dobj> want to take <dobj>
noun aux <dobj> fact is <dobj>
noun prep <np> argument against <np>
active-verb prep <np> looking for <np>
passive-verb prep <np> was put in <np>
infinitive prep <np> to go to <np>
Table 7: Syntactic Templates and Examples of Patterns
that were Learned for Sarcasm. Table. 10 in the Appendix
provides example posts that instantiate these patterns.
The Pattern Based Classifiers are trained on a de-
velopment set labeled ?PE eval? (Table 1). Utter-
ances from this development set are not used again
36
Figure 4: Recall vs. Precision for Sarcasm PE eval
in any further experiments. Patterns are extracted
from the dataset and we again compute FREQ and
%SARC and %NASTY for each pattern subject to
?1 ? FREQ and ?2 ? %SARC or % NASTY.
Classifications are made if at least two patterns are
present and both are above the specified ?1 and ?2,
as in R&W. Also following R&W, we do not learn
?not sarcastic? or ?nice? patterns.
To test the Pattern Based Classifiers, we use as in-
put the classifications made by the HP Classifiers.
Using the predicted labels from the classifiers as the
true labels, the patterns from test set ?HP test dev?
are extracted and compared to those patterns found
in development set ?PE eval?. We have two feature
sets for both sarcasm and nastiness: one using the
predictions from the MT indicators in the HP clas-
sifier (percent features) and another using those in-
stances from the ?2 features.
6.1 Results from Pattern Classifier
The Pattern Classifiers classify an utterance as Sar-
castic (Nasty) if at least two patterns are present and
above the thresholds ?1 and ?2, exhausting all com-
binations of ?1 = [2, 4, 6, 8, 10] and ?2 = [.55, .60,
.65, .70, .75, .80, .85, .90, .95, 1.00]. The counter-
classes are predicted when the utterance contains
less than two patterns. The exhaustive classifica-
tions are first made using the utterances in the de-
velopment set labeled ?PE eval?. Fig. 4 shows the
precision and recall trade-off for ?1 = [2, 10] and all
?2 values on sarcasm development set?PE eval?. As
recall increases, precision drops. By including pat-
terns that only appear 2 times, we get better recall.
Limiting ?1 to 10 yields fewer patterns and lower
recall.
Table 8 shows the results for various parameters.
The PE dev dataset learned a total of 1,896 sarcas-
tic extraction patterns above a minimum threshold of
?1 < 2 and ?2 < 0.55, and similarly 847 nasty ex-
traction patterns. Training on development set ?PE
dev? yields high precision and good recall. To se-
lect the best parameters, we again look for a balance
between precision and recall. Both Classifiers have
very high precision. In the end, we select parame-
ters that have a better recall than the best parame-
ter from the HP Classifiers which is recall = 38%
for sarcasm and recall = 49% for nastiness. The
best parameters and their test results are shown in
Table 9.
PARAMS P R F N (tp)
SARC ?1 =2, ?2 =.60 65% 49% 0.57 792
2, .65 71% 44% 0.58 717
2, .70 80% 38% 0.59 616
2, 1.0 97% 24% 0.60 382
NASTY ?1 =2, ?2 =.65 71% 49% 0.60 335
2, .75 83% 42% 0.62 289
2, .90 96% 30% 0.63 209
Table 8: Pattern Classification Training; P: precision, R:
recall, F: F-measure, tp: true positive classifications
The Pattern Classifiers are tested on ?HP dev test?
with the labels predicted by our HP Classifiers, thus
we have two different sets of classifications for both
Sarcasm and Nastiness: percent features and ?2 fea-
tures. Overall, the Pattern Classification performs
better on Nasty than Sarcasm. Also, the percent fea-
tures yield better results than ?2 features, possibly
because the precision for ?2 is high from the HP
Classifiers, but the recall is very low. We believe
that ?2 selects statistically predictive indicators that
are tuned to the dataset, rather than general. Having
a human in the loop guarantees more general fea-
tures from a smaller dataset. Whether this remains
true on the size as the dataset increases to 1000 or
more is unknown. We conclude that R&W?s patterns
generalize well on our Sarcasm and Nasty datasets
(open question O3), but suspect that there may be
better syntactic patterns for bootstrapping sarcasm
and nastiness, e.g. involving cue words or semantic
categories of words rather than syntactic categories,
as we discuss in Sec. 7.
This process can be repeated by taking the newly
classified utterances from the Pattern Based Clas-
sifiers, then applying the Pattern Extractor to learn
new patterns from the newly classified data. This
37
PARAMS P R F
Sarc % ?1 =2, ?2 =.70 62% 52% 0.57
Sarc ?2 ?1 =2, ?2 =.70 31% 58% 0.45
Nasty % ?1 =2, ?2 =.65 75% 62% 0.69
Nasty ?2 ?1 =2, ?2 =.65 30% 70% 0.50
Table 9: The results for Pattern Classification on HP dev
test dataset ; PARAMS: the best parameters for each fea-
ture set P: precision, R: recall, F: f-measure
can be repeated for multiple iterations. We leave this
for future work.
7 Discussion and Future Work
In this work, we apply a bootstrapping method to
train classifiers to identify particular types of subjec-
tive utterances in online dialogues. First we create
a suite of linguistic indicators for sarcasm and nas-
tiness using crowdsourcing techniques. Our crowd-
sourcing method is similar to (Filatova, 2012). From
these new linguistic indicators we construct a classi-
fier following previous work on bootstrapping sub-
jectivity classifiers (Riloff and Wiebe, 2003; Thelen
and Riloff, 2002). We compare the performance of
the High Precision Classifier that was trained based
on statistical measures against one that keeps human
annotators in the loop, and find that Classifiers us-
ing statistically selected indicators appear to be over-
trained on the development set because they do not
generalize well. This first phase achieves 54% preci-
sion and 38% recall for sarcastic utterances using the
human selected indicators. If we bootstrap by using
syntactic patterns to create more general sarcasm in-
dicators from the utterances identified as sarcastic in
the first phase, we achieve a higher precision of 62%
and recall of 52%.
We apply the same method to bootstrapping a
classifier for nastiness dialogic acts. Our first phase,
using crowdsourced nasty indicators, achieves 58%
precision and 49% recall, which increases to 75%
precision and 62% recall when we bootstrap with
syntactic patterns, possibly suggesting that nastiness
(insults) are less nuanced and easier to detect than
sarcasm.
Previous work claims that recognition of sarcasm
(1) depends on knowledge of the speaker, (2) world
knowledge, or (3) use of context (Gibbs, 2000; Eis-
terhold et al, 2006; Bryant and Fox Tree, 2002;
Carvalho et al, 2009). While we also believe that
certain types of subjective language cannot be de-
termined from cue words alone, our Pattern Based
Classifiers, based on syntactic patterns, still achieves
high precision and recall. In comparison to previous
monologic works whose sarcasm precision is about
75%, ours is not quite as good with 62%. While the
nasty works do not report precision, we believe that
they are comparable to the 64% - 83% accuracy with
our precision of 75%.
Open question O3 was whether R&W?s patterns
are fine tuned to subjective utterances in news. How-
ever R&W?s patterns improve both precision and re-
call of our Sarcastic and Nasty classifiers. In fu-
ture work however, we would like to test whether
semantic categories of words rather than syntactic
categories would perform even better for our prob-
lem, e.g. Linguistic Inquiry and Word Count cat-
egories. Looking again at row 1 in Table 10, ?It
is quite strange to encounter someone in this day
and age who lacks any knowledge whatsoever of the
mechanism of adaptation since it was explained 150
years ago?, the word ?quite? matches the ?cogmech?
and ?tentative? categories, which might be interest-
ing to generalize to sarcasm. In row 11 ?At the time
of the Constitution there weren?t exactly vast sub-
urbs that could be prowled by thieves looking for an
open window?, the phrase ?weren?t exactly? could
also match the LIWC categories ?cogmech? and ?cer-
tain? or, more specifically, certainty negated.
We also plan to extend this work to other cate-
gories of subjective dialogue acts, e.g. emotional
and respectful as mentioned in the Introduction, and
to expand our corpus of subjective dialogue acts. We
will experiment with performing more than one iter-
ation of the bootstrapping process (R&W complete
two iterations) as well as create a Hybrid Classifier
combining the subjective cues and patterns into a
single Classifier that itself can be bootstrapped.
Finally, we would like to extend our method to
different dialogue domains to see if the classifiers
trained on our sarcastic and nasty indicators would
achieve similar results or if different social media
sites have their own style of displaying sarcasm or
nastiness not comparable to those in forum debates.
References
G.A. Bryant and J.E. Fox Tree. 2002. Recognizing ver-
bal irony in spontaneous speech. Metaphor and sym-
bol, 17(2):99?119.
P. Carvalho, L. Sarmento, M.J. Silva, and E. de Oliveira.
2009. Clues for detecting irony in user-generated con-
38
tents: oh...!! it?s so easy;-). In Proc. of the 1st inter-
national CIKM workshop on Topic-sentiment analysis
for mass opinion, p. 53?56. ACM.
D. Davidov, O. Tsur, and A. Rappoport. 2010. Semi-
supervised recognition of sarcastic sentences in twitter
and amazon. In Proc. of the Fourteenth Conference on
Computational Natural Language Learning, p. 107?
116. Association for Computational Linguistics.
J. Eisterhold, S. Attardo, and D. Boxer. 2006. Reactions
to irony in discourse: Evidence for the least disruption
principle. Journal of Pragmatics, 38(8):1239?1256.
E. Filatova. 2012. Irony and sarcasm: Corpus genera-
tion and analysis using crowdsourcing. In Language
Resources and Evaluation Conference, LREC2012.
J.E. Fox Tree and J.C. Schrock. 1999. Discourse Mark-
ers in Spontaneous Speech: Oh What a Difference
an Oh Makes. Journal of Memory and Language,
40(2):280?295.
J. E. Fox Tree. 2010. Discourse markers across speak-
ers and settings. Language and Linguistics Compass,
3(1):1?13.
P. Gianfortoni, D. Adamson, and C.P. Rose?. 2011. Mod-
eling of stylistic variation in social media with stretchy
patterns. In Proc. of the First Workshop on Algo-
rithms and Resources for Modelling of Dialects and
Language Varieties, p. 49?59. ACL.
R.W. Gibbs. 2000. Irony in talk among friends.
Metaphor and Symbol, 15(1):5?27.
R. Gonza?lez-Iba?n?ez, S. Muresan, and N. Wacholder.
2011. Identifying sarcasm in twitter: a closer look.
In Proc. of the 49th Annual Meeting of the ACL: Hu-
man Language Technologies: short papers, volume 2,
p. 581?586.
A. Razavi, D. Inkpen, S. Uritsky, and S. Matwin. 2010.
Offensive language detection using multi-level classi-
fication. Advances in Artificial Intelligence, p. 16?27.
A. Reyes and P. Rosso. 2011. Mining subjective knowl-
edge from customer reviews: a specific case of irony
detection. In Proc. of the 2nd Workshop on Computa-
tional Approaches to Subjectivity and Sentiment Anal-
ysis (WASSA 2.011), ACL, p. 118?124.
A. Reyes, P. Rosso, and D. Buscaldi. 2012. From humor
recognition to irony detection: The figurative language
of social media. Data & Knowledge Engineering.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proc. of the 2003
conference on Empirical methods in Natural Lan-
guage Processing-V. 10, p. 105?112. ACL.
R. Snow, B. O?Conner, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast?but is it good?: evaluating non-expert
annotations for natural language tasks In Proc. of
the Conference on Empirical Methods in Natural Lan-
guage Processing, p. 254?263. ACM.
S.O. Sood, E.F. Churchill, and J. Antin. 2011. Auto-
matic identification of personal insults on social news
sites. Journal of the American Society for Information
Science and Technology.
Dan Sperber and Deidre Wilson. 1981. Irony and the
use-mention distinction. In Peter Cole, editor, Radical
Pragmatics, p. 295?318. Academic Press, N.Y.
E. Spertus. 1997. Smokey: Automatic recognition of
hostile messages. In Proc. of the National Conference
on Artificial Intelligence, p. 1058?1065.
M. Thelen and E. Riloff. 2002. A bootstrapping method
for learning semantic lexicons using extraction pattern
contexts. In Proc. of the ACL-02 conference on Empir-
ical methods in natural language processing-Volume
10, p. 214?221. ACL.
O. Tsur, D. Davidov, and A. Rappoport. 2010. Icwsm?
a great catchy name: Semi-supervised recognition of
sarcastic sentences in online product reviews. In Proc.
of the fourth international AAAI conference on we-
blogs and social media, p. 162?169.
Marilyn Walker, Pranav Anand, , Robert Abbott, and
Jean E. Fox Tree. 2012. A corpus for research on
deliberation and debate. In Language Resources and
Evaluation Conference, LREC2012.
J.M. Wiebe, R.F. Bruce, and T.P. O?Hara. 1999. Devel-
opment and use of a gold-standard data set for subjec-
tivity classifications. In Proc. of the 37th annual meet-
ing of the Association for Computational Linguistics,
p. 246?253. ACL.
J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis,
B. Fraser, D. Litman, D. Pierce, E. Riloff, T. Wilson,
et al 2003. Recognizing and organizing opinions ex-
pressed in the world press. In Working Notes-New Di-
rections in Question Answering (AAAI Spring Sympo-
sium Series).
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Pat-
wardhan. 2005. Opinionfinder: A system for subjec-
tivity analysis. In Proc. of HLT/EMNLP on Interactive
Demonstrations, p. 34?35. ACL.
G. Xiang, B. Fan, L. Wang, J. Hong, and C. Rose. 2012.
Detecting offensive tweets via topical feature discov-
ery over a large scale twitter corpus. In Proc. of
the 21st ACM international conference on Information
and knowledge management, p. 1980?1984. ACM.
8 Appendix A. Instances of Learned
Patterns
39
Pattern Instance FREQ %SARC Example Utterance
<subj> was explained 2 100% Well, I incorrectly assumed that anyone attempting to enter the discus-
sion would at least have a grasp of the most fundamental principles. It
is quite strange to encounter someone in this day and age who lacks any
knowledge whatsoever of the mechanism of adaptation since it was ex-
plained 150 years ago.
<subj> appears 1 94% It appears this thread has been attacked by the ?line item ? poster.
<subj> have problem 4 50% I see your point, Iangb but I?m not about to be leaving before you?ve had
a chance to respond. I won?t be ?leaving ? at all. You challenged me
to produce an argument, so I?m going to produce my argument. I will
then summarize the argument, and you can respond to it and we can then
discuss / debate those specifics that you have a problem with.
<subj> have to do 15 86% How does purchasing a house have to do with abortion? Ok, so what if
the kid wants to have the baby and the adults want to get rid of it? What
if the adults want her to have the baby and the kid wants to get rid of it?
You would force the kid to have a child (that doesn?t seem responsible at
all), or you would force the kid to abort her child (thereby taking away
her son or daughter). Both of those decisions don?t sound very consitent
or responsible. The decision is best left up to the person that is pregnant,
regardless of their age.
<subj> is nothing 10 90% Even though there is nothing but ad hoc answers to the questions, cre-
ationists touted the book as ?proof ? that Noahs? ark was possible. They
never seem to notice that no one has ever tried to build and float an ark.
They prefer to put the money into creation museums and amusement
parks.
gives <dobj> 25 88% Just knowing that there are many Senators and Congressmen who would
like to abolish gun rights gives credence to the fact that government could
actually try to limit or ban the 2nd Amendment in the future.
to force <dobj> 9 89% And I just say that it would be unjust and unfair of you to force meta-
physical belief systems of your own which constitute religious belief
upon your follows who may believe otherwise than you. Get pregnant
and treat your fetus as a full person if you wish, nobody will force you
to abort it. Let others follow their own beliefs differing or the same.
Otherwise you attempt to obtain justice by doing injustice
want to take <dobj> 5 80% How far do you want to take the preemptive strike thing? Should we
make it illegal for people to gather in public in groups of two or larger
because anything else might be considered a violent mob assembly for
the basis of creating terror and chaos?
fact is <dobj> 6 83% No, the fact is PP was founded by an avowed racist and staunch supporter
of Eugenics.
argument against <np> 4 75% Perhaps I am too attached to this particular debate that you are having
but if you actually have a sensible argument against gay marriage then
please give it your best shot here. I look forward to reading your com-
ments.
looking for <np> 14 92% At the time of the Constitution there weren?t exactly vast suburbs that
could be prowled by thieves looking for an open window.
was put in <np> 3 66% You got it wrong Daewoo. The ban was put in place by the 1986 Firearm
Owners Protection Act, designed to correct the erronius Gun Control Act
of 1968. The machinegun ban provision was slipped in at the last minute,
during a time when those that would oppose it werent? there to debate it.
to go to <np> 8 63% Yes that would solve the problem wouldn?t it,worked the first time
around,I say that because we (U.S.)are compared to the wild west. But
be they whites,Blacks,Reds,or pi** purple shoot a few that try to detain
or threaten you, yeah I think they will back off unless they are prepared
to go to war.
Table 10: Sarcastic patterns and example instances
40
