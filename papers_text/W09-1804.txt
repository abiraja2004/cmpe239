Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 28?35,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A New Objective Function for Word Alignment
Tugba Bodrumlu Kevin Knight Sujith Ravi
Information Sciences Institute & Computer Science Department
University of Southern California
bodrumlu@usc.edu, knight@isi.edu, sravi@isi.edu
Abstract
We develop a new objective function for word
alignment that measures the size of the bilin-
gual dictionary induced by an alignment. A
word alignment that results in a small dictio-
nary is preferred over one that results in a large
dictionary. In order to search for the align-
ment that minimizes this objective, we cast the
problem as an integer linear program. We then
extend our objective function to align corpora
at the sub-word level, which we demonstrate
on a small Turkish-English corpus.
1 Introduction
Word alignment is the problem of annotating a bilin-
gual text with links connecting words that have the
same meanings. Figure 1 shows sample input for
a word aligner (Knight, 1997). After analyzing the
text, we may conclude, for example, that sprok cor-
responds to dat in the first sentence pair.
Word alignment has several downstream con-
sumers. One is machine translation, where pro-
grams extract translation rules from word-aligned
corpora (Och and Ney, 2004; Galley et al, 2004;
Chiang, 2007; Quirk et al, 2005). Other down-
stream processes exploit dictionaries derived by
alignment, in order to translate queries in cross-
lingual IR (Scho?nhofen et al, 2008) or re-score can-
didate translation outputs (Och et al, 2004).
Many methods of automatic alignment have been
proposed. Probabilistic generative models like IBM
1-5 (Brown et al, 1993), HMM (Vogel et al, 1996),
ITG (Wu, 1997), and LEAF (Fraser and Marcu,
2007) define formulas for P(f | e) or P(e, f), with
ok-voon ororok sprok
at-voon bichat dat
erok sprok izok hihok ghirok
totat dat arrat vat hilat
ok-drubel ok-voon anok plok sprok
at-drubel at-voon pippat rrat dat
ok-voon anok drok brok jok 
at-voon krat pippat sat lat 
wiwok farok izok stok
totat jjat quat cat 
lalok sprok izok jok stok
wat dat krat quat cat 
lalok farok ororok lalok sprok izok enemok
wat jjat bichat wat dat vat eneat 
lalok brok anok plok nok 
iat lat pippat rrat nnat
wiwok nok izok kantok ok-yurp
totat nnat quat oloat at-yurp
lalok mok nok yorok ghirok clok
wat nnat gat mat bat hilat
lalok nok crrrok hihok yorok zanzanok
wat nnat arrat mat zanzanat 
lalok rarok nok izok hihok mok
wat nnat forat arrat vat gat 
Figure 1: Word alignment exercise (Knight, 1997).
28
hidden alignment variables. EM algorithms estimate
dictionary and other probabilities in order to maxi-
mize those quantities. One can then ask for Viterbi
alignments that maximize P(alignment | e, f). Dis-
criminative models, e.g. (Taskar et al, 2005), in-
stead set parameters to maximize alignment accu-
racy against a hand-aligned development set. EMD
training (Fraser and Marcu, 2006) combines genera-
tive and discriminative elements.
Low accuracy is a weakness for all systems. Most
practitioners still use 1990s algorithms to align their
data. It stands to reason that we have not yet seen
the last word in alignment models.
In this paper, we develop a new objective function
for alignment, inspired by watching people manu-
ally solve the alignment exercise of Figure 1. When
people attack this problem, we find that once they
create a bilingual dictionary entry, they like to re-
use that entry as much as possible. Previous ma-
chine aligners emulate this to some degree, but they
are not explicitly programmed to do so.
We also address another weakness of current
aligners: they only align full words. With few ex-
ceptions, e.g. (Zhang et al, 2003; Snyder and Barzi-
lay, 2008), aligners do not operate at the sub-word
level, making them much less useful for agglutina-
tive languages such as Turkish.
Our present contributions are as follows:
? We offer a simple new objective function that
scores a corpus alignment based on how many
distinct bilingual word pairs it contains.
? We use an integer programming solver to
carry out optimization and corpus alignment.
? We extend the system to perform sub-
word alignment, which we demonstrate on a
Turkish-English corpus.
The results in this paper constitute a proof of con-
cept of these ideas, executed on small corpora. We
conclude by listing future directions.
2 New Objective Function for Alignment
We search for the legal alignment that minimizes the
size of the induced bilingual dictionary. By dictio-
nary size, we mean the number of distinct word-
pairs linked in the corpus alignment. We can im-
mediately investigate how different alignments stack
up, according to this objective function. Figure 2
garcia and associates 
garcia y asociados
his associates are not strong
sus asociados no son fuertes
carlos garcia has three associates 
carlos garcia tiene tres asociados
garcia has a company also
garcia tambien tiene una empresa
its clients are angry
sus clientes estan enfadados
the associates are also angry
los asociados tambien estan enfadados
the clients and the associates are enemies
los clientes y los asociados son enemigos
the company has three groups
la empresa tiene tres grupos
its groups are in europe
sus grupos estan en europa
the modern groups sell strong pharmaceuticals
los grupos modernos venden medicinas fuertes
the groups do not sell zenzanine
los grupos no venden zanzanina
the small groups are not modern
los grupos pequenos no son modernos
Figure 2: Gold alignment. The induced bilingual dic-
tionary has 28 distinct entries, including garcia/garcia,
are/son, are/estan, not/no, has/tiene, etc.
29
garcia and associates 
garcia y asociados
his associates are not strong
sus asociados no son fuertes
carlos garcia has three associates 
carlos garcia tiene tres asociados
garcia has a company also
garcia tambien tiene una empresa
its clients are angry
sus clientes estan enfadados
the associates are also angry
los asociados tambien estan enfadados
the clients and the associates are enemies
los clientes y los asociados son enemigos
the company has three groups
la empresa tiene tres grupos
its groups are in europe
sus grupos estan en europa
the modern groups sell strong pharmaceuticals
los grupos modernos venden medicinas fuertes
the groups do not sell zenzanine
los grupos no venden zanzanina
the small groups are not modern
los grupos pequenos no son modernos
Figure 3: IP alignment. The induced bilingual dictionary
has 28 distinct entries.
shows the gold alignment for the corpus in Figure 1
(displayed here as English-Spanish), which results
in 28 distinct bilingual dictionary entries. By con-
trast, a monotone alignment induces 39 distinct en-
tries, due to less re-use.
Next we look at how to automatically rifle through
all legal alignments to find the one with the best
score. What is a legal alignment? For now, we con-
sider it to be one where:
? Every foreign word is aligned exactly once
(Brown et al, 1993).
? Every English word has either 0 or 1 align-
ments (Melamed, 1997).
We formulate our integer program (IP) as follows.
We set up two types of binary variables:
? Alignment link variables. If link-i-j-k = 1, that
means in sentence pair i, the foreign word at
position j aligns to the English words at posi-
tion k.
? Bilingual dictionary variables. If dict-f-e = 1,
that means word pair (f, e) is ?in? the dictio-
nary.
We constrain the values of link variables to sat-
isfy the two alignment conditions listed earlier. We
also require that if link-i-j-k = 1 (i.e., we?ve decided
on an alignment link), then dict-fij -eik should also
equal 1 (the linked words are recorded as a dictio-
nary entry).1 We do not require the converse?just
because a word pair is available in the dictionary, the
aligner does not have to link every instance of that
word pair. For example, if an English sentence has
two the tokens, and its Spanish translation has two
la tokens, we should not require that all four links
be active?in fact, this would conflict with the 1-1
link constraints and render the integer program un-
solvable. The IP reads as follows:
minimize:?
f,e dict-f-e
subject to:
?i,j
?
k link-i-j-k = 1
?i,k
?
j link-i-j-k ? 1
?i,j,k link-i-j-k ? dict-fij -eik
On our Spanish-English corpus, the cplex2 solver
obtains a minimal objective function value of 28. To
1fij is the jth foreign word in the ith sentence pair.
2www.ilog.com/products/cplex
30
get the second-best alignment, we add a constraint
to our IP requiring the sum of the n variables active
in the previous solution to be less than n, and we
re-run cplex. This forces cplex to choose different
variable settings on the second go-round. We repeat
this procedure to get an ordered list of alignments.3
We find that there are 8 distinct solutions that
yield the same objective function value of 28. Fig-
ure 3 shows one of these. This alignment is not bad,
considering that word-order information is not en-
coded in the IP. We can now compare several align-
ments in terms of both dictionary size and alignment
accuracy. For accuracy, we represent each alignment
as a set of tuples < i, j, k >, where i is the sentence
pair, j is a foreign index, and k is an English index.
We use these tuples to calculate a balanced f-score
against the gold alignment tuples.4
Method Dict size f-score
Gold 28 100.0
Monotone 39 68.9
IBM-1 (Brown et al, 1993) 30 80.3
IBM-4 (Brown et al, 1993) 29 86.9
IP 28 95.9
The last line shows an average f-score over the 8 tied
IP solutions.
Figure 4 further investigates the connection be-
tween our objective function and alignment accu-
racy. We sample up to 10 alignments at each of
several objective function values v, by first adding
a constraint that dict variables add to exactly v, then
iterating the n-best list procedure above. We stop
when we have 10 solutions, or when cplex fails to
find another solution with value v. In this figure, we
see a clear relationship between the objective func-
tion and alignment accuracy?minimizing the for-
mer is a good way to maximize the latter.
3This method not only suppresses the IP solutions generated
so far, but it suppresses additional solutions as well. In partic-
ular, it suppresses solutions in which all link and dict variables
have the same values as in some previous solution, but some
additional dict variables are flipped to 1. We consider this a fea-
ture rather than a bug, as it ensures that all alignments in the
n-best list are unique. For what we report in this paper, we only
create n-best lists whose elements possess the same objective
function value, so the issue does not arise.
4P = proportion of proposed links that are in gold,
R = proportion of gold links that are proposed, and f-
score = 2PR/(P+R).
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
A
ve
ra
ge
 a
lig
nm
en
t a
cc
ur
ac
y 
 (f
-s
co
re
)
Number of bilingual dictionary entries in solution 
 (objective function value)
28 30 32 34 36 38 40 42 44 46
Figure 4: Relationship between IP objective (x-axis =
size of induced bilingual dictionary) and alignment ac-
curacy (y-axis = f-score).
Turkish English
yururum i walk
yururler they walk
Figure 5: Two Turkish-English sentence pairs.
3 Sub-Word Alignment
We now turn to alignment at the sub-word level.
Agglutinative languages like Turkish present chal-
lenges for many standard NLP techniques. An ag-
glutinative language can express in a single word
(e.g., yurumuyoruz) what might require many words
in another language (e.g., we are not walking).
Naively breaking on whitespace results in a very
large vocabulary for Turkish, and it ignores the
multi-morpheme structure inside Turkish words.
Consider the tiny Turkish-English corpus in Fig-
ure 5. Even a non-Turkish speaker might plausi-
bly align yurur to walk, um to I, and ler to they.
However, none of the popular machine aligners
is able to do this, since they align at the whole-
word level. Designers of translation systems some-
times employ language-specific word breakers be-
fore alignment, though these are hard to build and
maintain, and they are usually not only language-
specific, but also language-pair-specific. Good un-
31
supervised monolingual morpheme segmenters are
also available (Goldsmith, 2001; Creutz and Lagus,
2005), though again, these do not do joint inference
of alignment and word segmentation.
We extend our objective function straightfor-
wardly to sub-word alignment. To test our exten-
sion, we construct a Turkish-English corpus of 1616
sentence pairs. We first manually construct a regu-
lar tree grammar (RTG) (Gecseg and Steinby, 1984)
for a fragment of English. This grammar produces
English trees; it has 86 rules, 26 states, and 53 ter-
minals (English words). We then construct a tree-to-
string transducer (Rounds, 1970) that converts En-
glish trees into Turkish character strings, including
space. Because it does not explicitly enumerate the
Turkish vocabulary, this transducer can output a very
large number of distinct Turkish words (i.e., charac-
ter sequences preceded and followed by space). This
transducer has 177 rules, 18 states, and 23 termi-
nals (Turkish characters). RTG generation produces
English trees that the transducer converts to Turk-
ish, both via the tree automata toolkit Tiburon (May
and Knight, 2006). From this, we obtain a parallel
Turkish-English corpus. A fragment of the corpus is
shown in Figure 6. Because we will concentrate on
finding Turkish sub-words, we manually break off
the English sub-word -ing, by rule, as seen in the
last line of the figure.
This is a small corpus, but good for demonstrat-
ing our concept. By automatically tracing the inter-
nal operation of the tree transducer, we also produce
a gold alignment for the corpus. We use the gold
alignment to tabulate the number of morphemes per
Turkish word:
n % Turkish types % Turkish tokens
with n morphemes with n morphemes
1 23.1% 35.5%
2 63.5% 61.6%
3 13.4% 2.9%
Naturally, these statistics imply that standard whole-
word aligners will fail. By inspecting the corpus, we
find that 26.8 is the maximium f-score available to
whole-word alignment methods.
Now we adjust our IP formulation. We broaden
the definition of legal alignment to include breaking
any foreign word (token) into one or more sub-word
(tokens). Each resulting sub-word token is aligned
to exactly one English word token, and every En-
glish word aligns to 0 or 1 foreign sub-words. Our
dict-f-e variables now relate Turkish sub-words to
English words. The first sentence pair in Figure 5
would have previously contributed two dict vari-
ables; now it contributes 44, including things like
dict-uru-walk. We consider an alignment to be a set
of tuples < i, j1, j2, k >, where j1 and j2 are start
and end indices into the foreign character string. We
create align-i-j1-j2-k variables that connect Turkish
character spans with English word indices. Align-
ment variables constrain dictionary variables as be-
fore, i.e., an alignment link can only ?turn on? when
licensed by the dictionary.
We previously constrained every Turkish word to
align to something. However, we do not want ev-
ery Turkish character span to align?only the spans
explicitly chosen in our word segmentation. So we
introduce span-i-j1-j2 variables to indicate segmen-
tation decisions. Only when span-i-j1-j2 = 1 do we
require
?
k align-i-j1-j2-k = 1.
For a coherent segmentation, the set of active span
variables must cover all Turkish letter tokens in the
corpus, and no pair of spans may overlap each other.
To implement these constraints, we create a lattice
where each node represents a Turkish index, and
each transition corresponds to a span variable. In a
coherent segmentation, the sum of all span variables
entering an lattice-internal node equals the sum of
all span variables leaving that node. If the sum of
all variables leaving the start node equals 1, then we
are guaranteed a left-to-right path through the lat-
tice, i.e., a coherent choice of 0 and 1 values for span
variables.
The IP reads as follows:
minimize:?
f,e dict-f-e
subject to:
?i,j1,j2
?
k align-i-j1-j2-k = span-i-j1-j2
?i,k
?
j1,j2 align-i-j1-j2-k ? 1
?i,j1,j2,k align-i-j1-j2-k ? dict-fi,j1,j2-ei,k
?i,j
?
j3 span-i-j3-j =
?
j3 span-i-j-j3
?i,w
?
j>w span-i-w-j = 1
(w ranges over Turkish word start indices)
With our simple objective function, we obtain an
f-score of 61.4 against the gold standard. Sample
gold and IP alignments are shown in Figure 7.
32
Turkish English
onlari gordum i saw them
gidecekler they will go
onu stadyumda gordum i saw him in the stadium
ogretmenlerim tiyatroya yurudu my teachers walked to the theatre
cocuklar yurudu the girls walked
babam restorana gidiyor my father is walk ing to the restaurant
. . . . . .
Figure 6: A Turkish-English corpus produced by an English grammar pipelined with an English-to-Turkish tree-to-
string transducer.
you   go   to   his   office
onun ofisi- -ne   gider- -sin
Gold alignment IP sub-word alignment
you   go   to   his   office
onun ofisi- -ne   gider- -sin
my  teachers  ran  to  their  house
ogretmenler- -im onlarin evi- -ne  kostu
my  teachers  ran  to  their  house
ogretmenler- -im onlarin evi- -ne  kostu
i  saw  him
onu gordu- -m
i  saw  him
onu gordu- -m
we  go  to  the  theatre
tiyatro- -ya gider- -iz
we  go  to  the  theatre
tiyatro- -ya gider- -iz
they  walked  to  the  store
magaza- -ya yurudu- -ler
they  walked  to  the  store
magaza- -ya yurudu- -ler
my aunt goes to their house
hala- -m  onlarin evi- -ne  gider
my aunt goes to their house
hal- -am  onlarin evi- -ne  gider
1.
2.
3.
5.
6.
4.
Figure 7: Sample gold and (initial) IP sub-word alignments on our Turkish-English corpus. Dashes indicate where the
IP search has decided to break Turkish words in the process of aligning. For examples, the word magazaya has been
broken into magaza- and -ya.
33
The last two incorrect alignments in the figure
are instructive. The system has decided to align
English the to the Turkish noun morphemes tiyatro
and magaza, and to leave English nouns theatre and
store unaligned. This is a tie-break decision. It is
equally good for the objective function to leave the
unaligned instead?either way, there are two rele-
vant dictionary entries.
We fix this problem by introducing a special
NULL Turkish token, and by modifying the IP to re-
quire every English token to align (either to NULL
or something else). This introduces a cost for fail-
ing to align an English token x to Turkish, because
a new x/NULL dictionary entry will have to be cre-
ated. (The NULL token itself is unconstrained in
how many tokens it may align to.)
Under this scheme, the last two incorrect align-
ments in Figure 7 induce four relevant dictio-
nary entries (the/tiyatro, the/magaza, theatre/NULL,
store/NULL) while the gold alignment induces only
three (the/NULL, theatre/tiyatro, store/magaza), be-
cause the/NULL is re-used. The gold alignment is
therefore now preferred by the IP optimizer. There
is a rippling effect, causing the system to correct
many other decisions as well. This revision raises
the alignment f-score from 61.4 to 83.4.
The following table summarizes our alignment re-
sults. In the table, ?Dict? refers to the size of the
induced dictionary, and ?Sub-words? refers to the
number of induced Turkish sub-word tokens.
Method Dict Sub-words f-score
Gold (sub-word) 67 8102 100.0
Monotone (word) 512 4851 5.5
IBM-1 (word) 220 4851 21.6
IBM-4 (word) 230 4851 20.3
IP (word) 107 4851 20.1
IP (sub-word, 60 7418 61.4
initial)
IP (sub-word, 65 8105 83.4
revised)
Our search for an optimal IP solution is not fast.
It takes 1-5 hours to perform sub-word alignment on
the Turkish-English corpus. Of course, if we wanted
to obtain optimal alignments under IBM Model 4,
that would also be expensive, in fact NP-complete
(Raghavendra and Maji, 2006). Practical Model 4
systems therefore make substantial search approxi-
mations (Brown et al, 1993).
4 Related Work
(Zhang et al, 2003) and (Wu, 1997) tackle the prob-
lem of segmenting Chinese while aligning it to En-
glish. (Snyder and Barzilay, 2008) use multilingual
data to compute segmentations of Arabic, Hebrew,
Aramaic, and English. Their method uses IBM mod-
els to bootstrap alignments, and they measure the re-
sulting segmentation accuracy.
(Taskar et al, 2005) cast their alignment model as
a minimum cost quadratic flow problem, for which
optimal alignments can be computed with off-the-
shelf optimizers. Alignment in the modified model
of (Lacoste-Julien et al, 2006) can be mapped to a
quadratic assignment problem and solved with linear
programming tools. In that work, linear program-
ming is not only used for alignment, but also for
training weights for the discriminative model. These
weights are trained on a manually-aligned subset of
the parallel data. One important ?mega? feature for
the discriminative model is the score assigned by an
IBM model, which must be separately trained on the
full parallel data. Our work differs in two ways: (1)
our training is unsupervised, requiring no manually
aligned data, and (2) we do not bootstrap off IBM
models. (DeNero and Klein, 2008) gives an integer
linear programming formulation of another align-
ment model based on phrases. There, integer pro-
gramming is used only for alignment, not for learn-
ing parameter values.
5 Conclusions and Future Work
We have presented a novel objective function for
alignment, and we have applied it to whole-word and
sub-word alignment problems. Preliminary results
look good, especially given that new objective func-
tion is simpler than those previously proposed. The
integer programming framework makes the model
easy to implement, and its optimal behavior frees us
from worrying about search errors.
We believe there are good future possibilities for
this work:
? Extend legal alignments to cover n-to-m
and discontinuous cases. While morpheme-
to-morpheme alignment is more frequently a
34
1-to-1 affair than word-to-word alignment is,
the 1-to-1 assumption is not justified in either
case.
? Develop new components for the IP objec-
tive. Our current objective function makes no
reference to word order, so if the same word
appears twice in a sentence, a tie-break en-
sues.
? Establish complexity bounds for optimiz-
ing dictionary size. We conjecture that opti-
mal alignment according to our model is NP-
complete in the size of the corpus.
? Develop a fast, approximate alignment al-
gorithm for our model.
? Test on large-scale bilingual corpora.
Acknowledgments
This work was partially supported under DARPA
GALE, Contract No. HR0011-06-C-0022.
References
P. Brown, V. Della Pietra, S. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational linguis-
tics, 19(2).
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
M. Creutz and K. Lagus. 2005. Inducing the morpho-
logical lexicon of a natural language from unannotated
text. In Proc. AKRR.
J. DeNero and D. Klein. 2008. The complexity of phrase
alignment problems. In Proc. ACL.
A. Fraser and D. Marcu. 2006. Semi-supervised training
for statistical word alignment. In Proc. ACL-COLING.
A. Fraser and D. Marcu. 2007. Getting the structure right
for word alignment: LEAF. In Proc. EMNLP-CoNLL.
M. Galley, M. Hopkins, K. Knight, and D Marcu. 2004.
What?s in a translation rule. In Proc. NAACL-HLT.
F. Gecseg and M. Steinby. 1984. Tree automata.
Akademiai Kiado.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, 27(2).
K. Knight. 1997. Automating knowledge acquisition for
machine translation. AI Magazine, 18(4).
S. Lacoste-Julien, B. Taskar, D. Klein, and M. Jordan.
2006. Word alignment via quadratic assignment. In
Proc. HLT-NAACL.
J. May and K. Knight. 2006. Tiburon: A weighted tree
automata toolkit. In Proc. CIAA.
I. D. Melamed. 1997. A word-to-word model of transla-
tional equivalence. In Proc. ACL.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Computa-
tional Linguistics, 30(4).
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord
of features for statistical machine translation. In Proc.
HLT-NAACL.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proc. ACL.
U. Raghavendra and H. K. Maji. 2006. Computational
complexity of statistical machine translation. In Proc.
EACL.
W. Rounds. 1970. Mappings and grammars on trees.
Theory of Computing Systems, 4(3).
P. Scho?nhofen, A. Benczu?r, I. B??ro?, and K. Csaloga?ny,
2008. Cross-language retrieval with wikipedia.
Springer.
B. Snyder and R. Barzilay. 2008. Unsupervised multi-
lingual learning for morphological segmentation. In
Proc. ACL.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A dis-
criminative matching approach to word alignment. In
Proc. EMNLP.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proc. ACL.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3).
Y. Zhang, S. Vogel, and A. Waibel. 2003. Integrated
phrase segmentation and alignment algorithm for sta-
tistical machine translation. In Proc. Intl. Conf. on
NLP and KE.
35
