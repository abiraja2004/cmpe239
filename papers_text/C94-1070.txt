THE "WHITEBOARD" ARCHITECTURE:  
A WAY TO INTEGRATE HETEROGENEOUS COMPONENTS OF NLP SYSTEMS 
Christian Boitet Mark Scligman 
GETA, IMAG (UJF & CNRS), 
150 rue de la Chimie, BP 53 
38041 Grenoble Cedex 9, Fr,'mce 
Christian .Boitet@imag. fr 
NI'R Interpreting Teleconununicafions Research Labs 
2-2 ltikari-dai, Seika-cho, Somku-gun 
Kyoto 619-02, Japan 
seligman@it i. atr. co. jp 
ABSTRACT 
We present a new softw,'u'e architecture for NLP systems 
made of heterogeneous components, and demonstrate an 
architectural prototype we have built at ATR iu the 
context of Speech Translation. 
KEYWORDS: Distributed NLP systems, Software 
architectures, Whiteboard. 
INTRODUCTION 
Speech translation systems must integrate components 
handling speech recognition, machine translation and 
speech synthesis. Speech recognition often uses special 
hardware. More components may be added in the future, for 
task understanding, multimodal interaction, etc. In more 
traditional NLP systems, such ,'cq MT systems for written 
texts, there is also a trend towards distributing various 
tasks on various machines. 
Sequential ,architectures \[10, 11\] offer ,an easy solntion, 
but lead to loss of information and lack of robustness. On 
the other hand, reports on experimenls with blackboard 
architectures \[16, 13, 20\] show they also have problems. 
We ,are exploring an intermediate architecture, in which 
components are integrated under a coordinator, may be 
written in various programming languages, may use their 
own data structures and algorithms, and may run in parallel 
on different machines. The coordinator maintains in a 
whiteboard an image of the input and output data structures 
of each component, at a suitable level of detail. The 
whitehoard fosters reuse of partial results and avoids 
wasteful recomputation. Each component process is 
encapsulated in a manager, which transforms it inlo a 
server, commuuicating with external clients (including the 
coordinator) via a system of mailboxes. Managers handle 
the conversions between internal (server) and external 
(client) data formats. This protocol enhances modularity 
and clarity, because one needs to to explicitly and 
completely declare fl~e appearance of the partial results of 
the components on the whileboard. 
Managers may also make batch components appear :is 
incremental components by delivering outputs in a 
piecewise fashion, thus taking a first step towards ystems 
simulating simultaneous translation. 
We have prc~luced a rudimentary architectural prototype, 
KASUGA, to demonstrate he above ideas. 
In fl~e first section, our four main guidelines ,are detailed: 
(1) record overall progress of components in a whiteboard; 
(2) let a coordinator schedule the work of components; 
(3) encapsnlate components in managers; and (4) use the 
managers to simulate Incremental Processing. In the 
second section, some high-level aspects of the KASUGA 
prototype ,are first described, and a simple demonstration is 
discnssed, in which incremental speech translation is 
simulated. Lower-level details are then giveu on some 
internal aspects. 
I. T I lE  WI I1TEBOARD ARCHITECTURE 
1. Record overall progress in a whitelmard 
The whiteboard ,architecture is inspired by the chart 
architecture of the MIND system \[8\] and later systems or 
formalisms for NLP \[1, 5\], as well as by the blackbo~u'd 
architecture, first introduced in HEARSAY-II \[6, 13\] for 
speech recognition, l lowever, there is a significant 
difference: tile components do not access the whiteboard, 
and need not even know of its existence. 
There are 2 main problems with the sequential pproach. 
? Pl: loss of information 
If components ,are simply concatenated, as in Asnra 
\[10, 11\], it is difficult for them to share partial results. 
Information is lost at subsystem interfaces and work 
has to be duplicated. For example, the cited system 
uses an LR parser to drive speech recognition; but 
syntactic structures found are discarded when 
recognition candidates are passed to MT. Complete 
reparsing is thus needed. 
? P2: lack of robustness 
Communication difficulties between subsystems may 
also dmnage robusmess. During reparsing for MT in 
ASURA, if no well-formed sentences are found, partied 
syntactic structures are discarded before semantic 
analysis; thus there is no chauce to tr,'mslate partially, 
or to use semantic inlonnation to complete the parse. 
The pure blackboard approach solves P1, but not P2, and 
introduces four other problems. 
? P3: control of concurrent access 
In principle, all components are allowed to access the 
blackboard: complex protection and synchronization 
mechanisms must be included, and fast components 
may be considerably slowed down by having to wait 
for permission to read or write. 
? P4: commnnication overloads 
The amount of information exchanged may I~ large. I1" 
components rnn on different machines, such :is is 
often the case for speech-related componeuts, and may 
be the case for Example-Based MT con~ponents in the 
future, commmfication overloads may annihilate the 
bcuciit of using spcckdized or distributed hardware. 
? P5: efficiency problems 
As components compute directly on the blackbo,'u'd, it 
is a compromise by necessity, and can not offer the 
optimal kind of data structure h~r each component. 
? P6: debugging problems 
These ,are due to the complexity of writing each 
component with the complete blaekbo,'u'd in mind, and 
to the parallel nature of the whole computation. 
In the "whiteboard" approach, the global data structure is 
hidden from the components, and accessed only by a 
"coordinator". (The whiteboard rawing is expanded later.) 
426 
"1he "Whiteboard" Architecture: a way to integrate... Boitet & Seligman, COLING-94 
(.~Olllporlell\[ 
I 
(~OlllpOIl(3ll\[ 
2 
COlllpollent3, \] 
o l 
Figure l: the "whitelmard" arc'h#ect,~re 
This simple change makes it possible to avo id  problems 
P3-P6. It has also at least two good points: 
- It encourages developers to clearly define andpublish 
what their inputs and outputs are, at least to the level of 
detail necessary to represent them in the whiteboard. 
- The whiteboard can be the central place where graphical 
interfaces are developed to allow for e~Lsy inspection, at 
v,'u'ious leve ls  o f  det~fil. 
As long as an NLP system uses a central record accessed 
ouly by a "coordinator' and hidden fi'om the "comlxmeuts", 
it cau be said to use a whiteboard architecture. It remains 
open what dala structures the whiteboard itself should use. 
As in \[21, we suggest the use of a time-aligned lattice, in 
which several types of nodes can be distinguished. In 
stating our preference for lattices, we must first distinguish 
them from grids, and then distinguish true lattices from 2 
types of quasi-lattice, charts and Q-graphs (fig. 2 & 3). 
Sent4 
NP8 
NPI NP2 
\]e~.~W ~ ..=..__.41~_ ~ O  _.._._._ii~ tornove and ~lrO und fur4 t,~k 
I I NPS I I I I./pa Senti = ' ~ = 1 \[ NP4 _ ,..- 
I Senl2 
Sent3 
r~oruove tiller cap and ground fuel tank 
Fig. 2: chart built on a syntactically ambiguous a'et/tence 
N(l~ht,N(S), 
N(can,N(P},. V(spackle,P(3pI)..,'k.=. 
A(Iklht,N(S ,~  N(can, NtS}. ,.} V{spar kle, P(3sffi ...~ 
G{M).f} ~, - - 
. V(spaflde,lnf,..) 
G(M}.. / 
I 4(light,N(S), N(ean, N(S), IV(sparkle, 
N(PauI,PN, k(light,N(S,Pl I V(can,T(P), I lN(soa,~, ~ ' AV(slight..,L~ 
n ~ ~ iP"-' ~ ~ O 
Nb(sfl),Poss} I GIM,F}) ~ N(S),G(M}) , 
I v(light''') ! I N~,,.,l~. I IN! spark~' 
I G(M),..) N(P),G{M)) 
N(light,N(S), V (c in,T( P~ ,=~....e V(spathle,lnf,.,) 
G(M)..,\] ~V(mod)) 
Paul's ligh! can(s) sparldo(s) slightty 
Fig. 3: A Q-graph for a phonetically ambiguous sentence 
Grids have no arcs, but nodes Co,Tesponding to time 
spans. A ncxle N spanning It132\] is implicitly connected 
to another node N' spanning \[t'l,t'2\] iff its time span 
begins earlier 01 gt'l ), ends strictly earlier (t2<t'2), and the 
respective sp,'ms (a) are not too far apart anti (b) don't 
overlap tc~) much (t2-max-gap_<t'l ~t2+max-ovorlap). max-  
gap  and max-over lap are gapping and overlapping 
threshokts \[12\]. Because t2<t'2, there can be no cycles. 
ht a lattice, by contrast, nodes and arcs are explicit. 
Cycles are also forhiddcn, and there must be a unique first 
node and a unique last node. 
(;rids have often been used in NLP. l"or example, Ihe 
output of the phonetic omponent of Kt~AL \[121 was a 
word grid, and certain speech recognition programs at NI'R 
l~r(?luce phoneme grids 1. In gener~d, each uc~le bears a time 
span, a label, and a score. Grids can also be used to 
represent an input text obtained by scanning abad original, 
or a stenotypy tape \[9\], and to implement some working 
structures (like flint of the Cocke algorithm). 
llowever, we will require explicit arcs in order to 
explicitly model possible sequences, sometimes with 
associated information concerning sequence probability. 
Thus mw grkls am insufficient for our whiteboards. 
Two kinds of quasi-lattices have been used extensively, in
two wtrietics. First, chart structures have origi,mlly been 
intr(~luccd by M. Kay in the MIND system around 1965 
\[8\], In a ch:ut, as understood tt~lay (Kay's charts were more 
general), the nodes ,are arranged in a row, so that there is 
always a path between any two given nodes. The arcs bear 
the information (label, score), not the nodes. Ch\[u'ls are 
also used by many unification-based natural hmguage 
analyzers \[141. 
Chart structures are unsuitable for represcnting restflts on 
a whiteboard, however, because they are tmable to 
represent alternate sequences. Consider the alternate word 
sequences of Figure 4. It is not possible to arr.'mge the 
words in a single mw so that all and only the proper 
sequences can be read out, 
1 1 it if you came I would like you to come emly tomorrow earlier 
Figure 4: A sentence with alternate formtdations 
A second type of quasi-lattice is the Q-graphs of \[15\] and 
their exteasiou \[17\], the basic data structure for text 
representation in tile METI~,O \[14\] aud TAUM-Aviation \[71 
systems. A Q-graph is a loop-free graph wilh a tmique 
entry node and a uni(lue exit node. As iu charts, the 
inlonnalion is carried on the arcs. It cousisls in labeled or 
atmotaled trees. As there may be no l)ath between two 
nixies, Q-graphs can indeed faithfully represent alternate 
sequences like those of Figure 4. But in this case it is 
necess;uy to use, on more thau one arc, identical abels 
referring to the same span of the input. For representation 
on a whitcl?mrd, such duplication is a drawback. 
To simplify bookkeeping and visual presentation, we 
prefer a representation in which a given label referring to a 
given span appeaJw in only one place. A true lattice, like 
flint of Figure 5, makes this possible. 
"lhe decomposition of the laltice in htyers seems natural, 
aud leads to more clarity. Fach layer contains results of 
1115, 16\]. By contrast, tile IIWIM \[20\] system used a 
"phonetic lattice" on which an extended ATN operated. 
427 
The "Whiteboard" Architecture: a way to integrate... Boitet & Seligmcm, COLING-94 
one component, selected to the "appropriate level of 
detail". Its time-aligned character makes it possible to 
organize it in such a way that everything which has been 
computed on a certain time interval at a certain layer may 
be found in the same region. Each layer has three 
dimensions, time, depth and label (or "class"). A node at 
position (i,j,k) corresponds to the input segment of length 
j ending at time i and is of label k. All realizations of label 
k corresponding to this segment are to be packed in this 
node, and all nodes corresponding toapproximately equ,'d 
input segments am thus geometrically clustered. 
In other words, ambiguities are packed so that dynamic 
programming techniques may be applied on direct images 
of the whiteboard. Figure 6 gives an ex,'unple, Where the 
main NP has been obtained in two ways. 
G Q III II 
% 
Figure 5: A word lattice (representing a sentence with alternate fornudations 
Arcs may optionally be augmented with activation or realistic hoice of layers, however. 
inhibition weights, so that ideas from the fast-developing 
lield of neural networks may be applied. 
language u~q~ 
layers layers 
Figure 6: The whiteboard as a factorizing data structure 
The true lattice, then, is our preferred structure for the 
whiteboard. 
We said that the whiteboard could be a central place for 
transp,'u'ent inspection, at suitable levels of detail. We use 
the notion of "shaded nodes" for this. 
- "White" nodes are the real nodes of the lattice. They 
contain results of the computation of the component 
associated with their layer: a white node contains at 
least a label, legal in its layer, such as NP, AP, 
CARDP,  VP... in the example above, and possibly 
more complex information, as allowed by the 
declaration of the layer in the whitelx~ard. 
- "Grey" nodes may be added to show how the white 
nodes have been constructed. They don't belong to the 
lattice structure proper. In the example above, they 
stand for rule instances, with the possibility of m-->n 
rules. In other cases, they may be used to show the 
correspondences between odes ot two layers. 
?-@ .?.--@ 
Figure 7: White and grey nodes corresponding to rule Rn: 
X1 X2...Xp -> Y1 Y2...Yq 
- "Black" nodes may be used to represent finer steps in 
the computation of the component, e.g. to reflect he 
active edges of a chart parser. 
Whiteboard la rers are organized in a lc, op-li'ee dependency 
graph. Non-linguistic as well as 
linguistic information can be recorded in 
appropriate layers. For example, in a 
multimodal context, the syntactic 
analyzer might use selected information 
from a map layer, where pointing, etc. 
could be recorded. Interlayer 
dependencies should be decl~u'ed, with 
associated constraints, stating for 
instance that only nodes with certain 
labels can be related to other layers. 
Ilere is an illustration of that idea, 
wilhout any pretense to propose a 
~Ot\] ll|ellU 
layer layer 
Figure 8: A hierarchy of layers in an hypothetical 
whiteboard for a nndtimodal NLP ,~'ystem 
2. Let a coordin'ator schedule tile components 
In its simplest form, a coordinator only transmits the 
results of a component to Ihe next component(s). 
l lowever, it is in a position to carry out global strategies 
by filtering low-ranking hypotheses and transmitting only 
the most promising part of a whitcboard layer to its 
processiug component. Further, if certain components 
make uselhl predictions, the coordinator can pass these to 
other components as constraints, ,along with input. 
3. Encapsulate components in managers 
Developers of components should be free to choose and 
vary their algorithms, data structures, programming 
languages, and possibly hardware (especially so lor speech- 
related components). Our approach is to encapsulate 
existing components in managers, which hide them and 
transform them into servers. This strategy has the furlher 
adv,'mtage of avoiding any direct call between coordinator 
and components. To plug in a new component, one just 
writes a new manager, a good part of which is generic. 
428 
The "Whiteboard" Architecture: a way to integrate... Boitet & Seligman, COLING-94 
A m,'mager has a request box where clients send requests 
to open or close connections. A connection consists of a 
pair of in and out mailboxes, with associated locks, mid is 
opened with certain paraneters, uch as its sleep time and 
codes indicating pre-agreed import and export formats. The 
coordinator puts work to do into in-boxes aid gets results 
in corresponding out-boxes. 
As illustrated in Figure 1 above, a client can open more 
than one connection with the sane manager. For exanple, 
au on-line dictionary might be called for displaying 
"progressive" word for word translation, as well as for 
,'mswering ternfinological requests by a human interpretcr 
supervising several dialogues and l~ddng over if needed. 
And a malager can in principle have several clients. 
llowever, this potential is not used in KASUGA. 
4. Simulate incrementa l  processing 
In real life, simullanexms interpretation is often preferred 
over consecutive interpretation: although it may be less 
exact, one is not forced to wait, and one can react even 
before the end of tile speaker's utterance. Incremental 
processing will thus be an iinportant aspect of future 
machine interpretation systems. For instance, a sem.'mlic 
processor might begin working on the syntactic structures 
hypothesized for early parts of an utterance while later 
parts ,are still being syntactically an,'dyzed \[19\]. 
Even if a component (e.g., a W cun'ently existing speech 
recognizer) has to get to file end of the utterance before 
producing any result, its nmnager may still m;tke its 
processing appear incremental, by delivering its result 
piecewise and iu the desired order. I lence, this organiz'~tion 
makes it possible to siintfiate future incremental 
components. 
11. T I lE  KASUGA PROTOTYI 'E  
1. External level 
The coordinator (KAS.COORD) is writtcn in KEK TM, au 
object-oriented xpert system shell with excellent interface- 
building tools. The whiteboard is declared ill KEF\]s object 
language. KEE itself is written ill Common lisp. 
Three components are inw/lved: 
- speech recognition (SP.REC) providing :t 3-level grid, 
progrmnmcd in C \[15\]; 
- ish'md-driven syntactic hart-parsing (SYNT.AN) 
deriving words and higher-level syntactic units, 
programned in C; 
- word-for-word translation (WW.TRANS) at file word 
level, written in C aid running on another machine. 
The tanagers are written in Lisp, ,'rod run independently, 
in three Unix processes. Each manager ,and the c(gmlinator 
can rat  in different Unix shells. Although WW.TRANS is 
already accessible as a server on a distant machine, we had 
to create a manager lbr it to get the intended behavior. 
With only these components, it is possible to produce a 
simple demonstration in which incremental speech 
translation is simulated and the transparency gained by 
using a whiteboard is illustrated. The phonemes produced 
by SP.REC are assembled into words and phrases by 
SYNT.AN. As this goes on, WW.TRANS produces 
possible word-for-word translations, which are presented on 
screen ,'u,~ a word lattice. 
KASUGA's whiteboard has only three layers: phonemes; 
source words and phrases; and equivalent target words. At 
the first layer, the phoneme lattice is represented with 
phonemes in nodes. At the second layer, we retain only the 
complete substructures produced by SYNT.AN, that is, the 
inactive exlges. Phonemes used in these slructures appear 
again at that layer. 
In KEE, we define a class of NODES, with subclasses 
WHITE.NODES, GREY.NODES, PIlON.LAYI~P,.NOI)ES, aud 
SYNT. I,AYER. NODES in tile syntactic htycr. NODES have a 
generic display method, and subclasses have specialized 
variants (e.g., the placing of white nodes depends on their 
time interval, while that of grey nodes depends on that of 
the white nodes they cermet0. 
2. Internal level 
When a manager eceives a Make.Conuection request 
frola a client, it creates an in box and an out box (and 
associated locks, used to prevent interference between 
components), through which information is p.'~ssed to and 
from the client. The Make.Connection request includes 
codes showing in which format(s) the client is expecting 
to deposit data in the iu box and read data from tile out 
box, lbr that connection. 
Mlhough data transfer could be programmed more 
efficiently, e.g. nsing lhfix sockets, our method is more 
general, as it uses only the file system, and we believe its 
overhead will be negligible in comparison with tile 
processing times required by the compouents. 
Ikn each out box, the client (KASUGA) actbatcs a reader 
process and tile relewmt mauagcr actiwttes a writer process. 
Conversely, for each in box, tile client activates at writer 
process and the manager activates a reader process. A zeader 
process wakes up regul:uly and checks whether its mailbox 
is both non-empty and nnlocked. If so, it locks the 
mailbox; reads ils contents; empties tile mailbox; unlocks 
it; and goes to sleep again. A writer process, by 
comparison, wakes up regul:uly and checks whether its 
mailbox is both empty and unlocked. If so, it locks the 
box, fills it with appropriate data, unlocks it, and goes 
back to sleep. For example, the writer associated with 
SYNT.AN will deposit in the appropriate out box the 
image of all tile inactive arcs created since the lm;t deposit. 
SItI?,EC provides, lor each of 40 prerecorded bunsetsu 
(elementary phrase), a set of about 25 phoneme malrices, 
one for each phoneme. A malrix cell contains the score for 
a given phoneme with a given begimfiug/ending speech 
frane pair. These nmtrices are then compared, and 3 other 
inatrices are computed. The tnp-scoring ln:llrix contains in 
each cell the tnl~-scnring phone and its score for Ihe 
corresponding begimliug/cnd. The 2nd-scoring a~d 3rd- 
scoring matrices are computed sinfilarly. These three 
mauices are used to build the first layer of the whiteboard. 
To build the whilcboard's second layer, an ishmd-driven 
clmrt parser is used, where the matrices are cousklered as 
initialized charts. The over:dl best-scoring cell in the top 
matrix is established as the only anchor, and hi-directional 
searching is carried out wilhin the (handset) limits set by 
max-gap and max-overlap. A CFG written by J. llosaka 
for tile ASURA demos is now used as is. Parsing results 
are convertcd to syntact  Jc:. \] a t< 5 ce. N (by Olt\[ char t -  
to -  la t t i ce  filter) and brought into KEF~. 
Then an image lattice, ww. la  t t \] c e. N, is comptlted as 
the whiteboard's third layer, using a C-based ou-tine J-l{ 
dictionary. Each lexieal syntactic node gives rise to oue 
Fmglish word for each meafing. For example, ~ gives 
yes, yes-sir, the-lungs, ashes, etc. 
Layers of the whiteboard are represented by KEF, 
"planes". We can move planes rehtlive to e\[ich olher; ztx~m 
429 
The "Whiteboard" Architecture: a way to integrate... Boitet & Seligman, COLING-94 
in various ways; put various information in the nodes 
(label, rule responsible, id, time span, score); exp,'md the 
nodes; open & close the nodes selectively. And we can 
color the nodes according to their score. It is possible to 
show or hide various parts of the whiteboard. In Figure 9, 
the first layer, the time grid, the lattice lines, and the 
initial/final lattice nodes have been hidden. Alternatively, 
we could hide constnlction (dotted) lines, rule boxes, label 
boxes, etc. The view of any part of the whiteboard can he 
changed for emphasis: one can for instance interactively 
select only the nodes above a certain confidence threshold. 
Overall processing can be inten'upted for examination. 
WW.lattice,N 
| ,~. ~ I start II 
I , I I ? I',,,,ose,s.,l 
II I 
I I " - - "o  I ~ .  I 
o~ # %% 
O' 
Figure 9: a view of KASUGA ' s whiteboard 
If this architecture is to be further developed in the future, 
one could use instead of KEE a general-purpose, portable 
interface building toolkit in order to avoid the oved~ead ,'rod 
overspecialization ,associated with using a complete xpert 
system shell. 
KAS.COORD writes and reads data to and from the 
managers in a LISP-like format, and handles the 
transformation i to KEE's internal fornmt. Each manager 
translates back ,and forth between that format and wbatever 
format its associated component happens to be using. 
Ilence, formats must be precisely defined. For inst,'mce, the 
edges produced by the speech recognizer are of the form 
(begin end phoneme score). The nodes and edges of the 
conesponding phoneme layer in the whiteboard are of Ihe 
form (node-id begin end phoneme score (in-arcs) (out-arcs)), 
with ares being of the form (are-id origin extremity weight). 
CONCLUSION 
Although the concept of the whiteboard architecture Ires 
emerged in the context of rese,-u'ch in Speech Translation, 
it can be useful in other areas of NLP. It has already been 
used, in a prelimin,'u'y form, in dialogue-b~sed MT \[3\]: the 
tasks are distributed between the authoring stations and an 
MT server, m~d the coordinator maintains in a unique data 
structure all intermediate stages of processing of all units 
of translation. 
The whiteboard ,architecture might be used with profit in 
all situations where it is important o integrate new or 
existing components, e.g. to build generic environments 
for developing heterogeneous NLP systems. Researchers 
would thereby gain twice: by getting a clearer view of 
what they (and others) ,are doing; and by being able to use 
generic interlace tools provided by the coordinator for 
debugging and illustrating purposes. 
ACKNOWI ,EDGMENTS 
We are grateful to M. Fiorenthm from Intellicorp, Inc. 
and K. Kurokawa from CSK, Inc., for providing a demo 
copy of KEE TM and valuable technical support; to Dr. 
Y. Yamazaki, President of ATP,-IT, and T. Morimoto, 
l lead of Dept. 4, for their support and encouragement; to 
II. Singer, T. ltayashi, Y. Kitagawa, I\[. Kashioka, and 
J. Hosaka, for their help in developing the components; 
and to K. II. Loken-Kim, for sfimtflating discussions and 
proposing the term "whitelx~ard". 
REFERENCES 
\[1\] Barnett J., Knight K., Mani I. & Rich E. 
119901 Knowledge and Natural Language Processing. 
Comm. ACM, 33/8, 50-71. 
\[2\] Boltet C. (19881 Representation and Computation 
of Units of Translation for Machine Interpretation of Spoken 
Texts. Comp. & AI, 6, 505--546. 
\[3\] Baiter C. & Blanchon 11. (1993) Dialogue-Based 
MT for Monolingual Authors and the LIDIA project. Proc. 
NLPRS'93, Fukuoka, 208--222. 
\[4\] Chandloux J. & Gu~rard M.q*'. (1981) METEO: 
un sysltSme ?t l'tCpreuve du temps. META, 1, 17--22. 
\[5\] Colmeraner A. (1970) Les systdmes-Q, un 
formalisme pour analyser et synthdtiser des phrases sur 
ordinateur. TAIJM, Univ. de MontrSal, dec. 1970. 
\[61 Erman L. D. & Lesser V. R. (19801 771e Hearsay- 
H Speech Understanding System : A Tutorial. ht "Trends in 
Speech Recognition", W.A. Lea, ed., Prcntice-lhdl, 361-381. 
\[7i lsabelle P. & Bourbeau L. (1984) TAUM- 
A VIA770N: its technical features and some experbnental 
results. Comp. Ling., I1/I, 18 27. 
\[8\] Kay M. (1973) 77~e MIND system. In "Courant 
Computer Science Symposium 8: Natural Language 
Processing", R. Rustin, ed., Algorithntics Press, 155-188. 
\[9\] M6rlaldo 1|. (1988) Multilevel decoding for Vety- 
Large-Size-Dictionaly speech recognition. IBM Journal of 
R&D, 32/2, March 1988, 227-237. 
\[101 Morlmoto T., St, zuki M., Takezawa T., Klknl 
G.-(., Nagata M. & Tnmokiyo M. (19921 A Spoken 
Language Translation System: SL-TRANS2. Proc. COLING- 
92, Nantes, vol. 3/4, 1048--1052. 
\[11\] Morlmoto T., Takezawa T., Yato F., 
Sagayama S., Tashlro T., Nagata M. & al. (1993) 
ATR's Speech Translation System: ASURA. EuroSpeecb'93. 
\[12\] Qnlnton P. (19811) Contribution h la 
reconnaissance d la parole. Utilisation tie mdthodes 
heuristiques pour la reconnaissance ti phrases. TbSse d'Etat, 
Univ. de Rennes, 239 p. 
\[13\] Reddy R. (19811) Machine Models of Speech 
l'erception. In "Perception and Production of Fluent Speech", 
Cole, ed., Erlbaum, N.J., 215-242. 
\[141 Schleber S. M. (19861 An it, troduction to 
unification-based approaches togrammar. CSLI Lect. Notes 4. 
\[15\] Singer II. & Sagayama S. (19921 Matrix Parser 
and its Application to 11MM-based Speech Recognition. 
IEICE, 111/SP92-76, I)SP92-61, 21-26. 
\[16\] Singer I1. & Sagayama S. (1992) Matrix 
Parsing applied to TDNN-based Speech Recognition. Japanese 
Journal of Speech Processing, 1992/3, 89-90. 
\[17\] Stewart G. (1975) Manuel du langage REZO. 
TAUM, Univ. de Montrfal, 120 p. 
\[18\] Tomlta M. (19911) 77~e Generalized LR 
Parser/Compiler V8-4 : a Software Package for Praclical NL 
l'rojects. Proc. COLIN(;-90, vol. 1/3, 59-63. 
\[19\] Waldster W. (1993) Planning Multimodal 
Discourse. Proc. ACL-93, Columbus, Ohio, 95-96. 
\[20\] Wolf J. J. & Woods W. A. (19811) 771e IIWIM 
Speech Understanding System. In "Trends in Speech 
Recognition", W. A. Lea, ed., Prentice-llall, 316-339. 
-0 -0 -0 -0 -0 -0 -0 -0 -0 -0 -  
430 
