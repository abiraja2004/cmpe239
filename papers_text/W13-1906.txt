Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 45?53,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Unsupervised Linguistically-Driven Reliable Dependency Parses
Detection and Self-Training for Adaptation to the Biomedical Domain
Felice Dell?Orletta, Giulia Venturi, Simonetta Montemagni
Istituto di Linguistica Computazionale ?Antonio Zampolli? (ILC?CNR)
Via G. Moruzzi, 1 ? Pisa (Italy)
{felice.dellorletta,giulia.venturi,simonetta.montemagni}@ilc.cnr.it
Abstract
In this paper, a new self?training method
for domain adaptation is illustrated, where
the selection of reliable parses is car-
ried out by an unsupervised linguistically?
driven algorithm, ULISSE. The method
has been tested on biomedical texts with
results showing a significant improve-
ment with respect to considered baselines,
which demonstrates its ability to capture
both reliability of parses and domain?
specificity of linguistic constructions.
1 Introduction
As firstly demonstrated by (Gildea, 2001), pars-
ing systems have a drop of accuracy when tested
against domain corpora outside of the data from
which they were trained. This is a real prob-
lem in the biomedical domain where, due to the
rapidly expanding body of biomedical literature,
the need for increasingly sophisticated and effi-
cient biomedical text mining systems is becom-
ing more and more pressing. In particular, the ex-
istence of natural language parsers reliably deal-
ing with biomedical texts represents the prerequi-
ste for identifying and extracting knowledge em-
bedded in them. Over the last years, this prob-
lem has been tackled within the biomedical NLP
community from different perspectives. The de-
velopment of a domain?specific annotated corpus,
i.e. the Genia Treebank (Tateisi, Yakushiji, Ohta,
& Tsujii, 2005), played a key role by providing a
sound basis for empirical performance evaluation
as well as training of parsers. On the other hand,
several attempts have been made to adapt general
parsers to the biomedical domain. First experi-
ments in this direction are reported in (Clegg &
Shepherd, 2005) who first compared the perfor-
mance of three different parsers against the Ge-
nia treebank and a sample of the Penn Treebank
(PTB) (Mitchell P. Marcus & Santorini, 1993) in
order to carry out an inter?domain analysis of
the typology of errors made by each parser and
demonstrated that by integrating the output of the
three parsers they achieved statistically significant
performance gains. Three different methods of
parser adaptation for the biomedical domain have
been proposed by (Lease & Charniak, 2005) who,
starting from the results of unknown word rate
experiments carried out on the Genia treebank,
adapted a PTB?trained parser by improving the
Part?Of?Speech tagging accuracy and by relying
on an external domain?specific lexicon. More re-
cently, (McClosky, Charniak, & Johnson, 2010)
and (Plank & van Noord, 2011) devised adaptation
methods based on domain similarity measures. In
particular, both of them adopted lexical similar-
ity measures to automatically select from an anno-
tated collection of texts those training data which
is more relevant, i.e. lexically closer, to adapt the
parser to the target domain.
A variety of semi?supervised approaches,
where unlabeled data is used in addition to labeled
training data, have been recently proposed in the
literature in order to adapt parsing systems to new
domains. Among these approaches, the last few
years have seen a growing interest in self?training
for domain adaptation, i.e. a method for using au-
tomatically annotated data from a target domain
when training supervised models. Self?training
methods proposed so far mainly differ at the level
of the selection of parse trees to be added to the
in?domain gold trees as further training data. De-
pending on whether or not external supervised
classifiers are used to select the parses to be added
to the gold?training set, two types of methods are
envisaged in the literature. The first is the case,
among others, of: (Kawahara & Uchimoto, 2008),
using a machine learning classifier to predict the
reliability of parses on the basis of different fea-
ture types; or (Sagae & Tsujii, 2007), selecting
45
identical analyses for the same sentence within the
output of different parsing models trained on the
same dataset; or (McClosky, Charniak, & John-
son, 2006), using a discriminative reranker against
the output of a n?best generative parser for select-
ing the best parse for each sentence to be used
as further training data. Yet, due to the fact that
several supervised classifiers are resorted to for
improving the base supervised parser, this class
of methods cannot be seen as a genuine istance
of self?training. The second type of methods is
exemplified, among others, by (Reichart & Rap-
poport, 2007) who use the whole set of automat-
ically analyzed sentences, and by (McClosky &
Charniak, 2008) and (Sagae, 2010) who add dif-
ferent amounts of automatically parsed data with-
out any selection strategy. Note that (McClosky
& Charniak, 2008) tested their self?training ap-
proach on the Genia Treebank: they self?trained
a PTB?trained costituency parser using a random
selection of Medline abstracts.
In this paper, we address the second scenario
with a main novelty: we use an unsupervised ap-
proach to select reliable parses from automatically
parsed target domain texts to be combined with the
gold?training set. Two unsupervised algorithms
have been proposed so far in the literature for
selecting reliable parses, namely: PUPA (POS?
based Unsupervised Parse Assessment Algorithm)
(Reichart & Rappoport, 2009) and ULISSE (Un-
supervised LInguiStically?driven Selection of dE-
pendency parses) (Dell?Orletta, Venturi, & Mon-
temagni, 2011). Both algorithms assign a qual-
ity score to each parse tree based on statistics
collected from a large automatically parsed cor-
pus, with a main difference: whereas PUPA oper-
ates on costituency trees and uses statistics about
sequences of part?of?speech tags, ULISSE uses
statistics about linguistic features checked against
dependency?based representations. The self?
training strategy presented in this paper is based
on an augmented version of ULISSE. The reasons
for this choice are twofold: if on the one hand
ULISSE appears to outperform PUPA (namely, a
dependency?based version of PUPA implemented
in (Dell?Orletta et al, 2011)), on the other hand the
linguistically?driven nature of ULISSE makes our
self?training strategy for domain adaptation able
to capture reliable parses which are also represen-
tative of the syntactic peculiarities of the target do-
main.
After introducing the in? and out?domain cor-
pora used in this study (Section 2), we discuss
the results of the multi?level linguistic analysis of
these corpora carried out (Section 3) with a view
to identifying the main features differentiating the
biomedical language from ordinary language. In
Section 4, the algorithm used to select reliable
parses from automatically parsed domain?specific
texts is described. In Section 5 the proposed self?
training method is illustrated, followed by a dis-
cussion of achieved results (Section 6).
2 Corpora
Used domain corpora include i) the two out?
domain datasets used for the ?Domain Adaptation
Track? of the CoNLL 2007 Shared Task (Nivre
et al, 2007) and ii) the dependency?based version
of the Genia Treebank (Tateisi et al, 2005). The
CoNLL 2007 datasets are represented by chemical
(CHEM) and biomedical abstracts (BIO), made of
5,001 tokens (195 sentences) and of 5,017 tokens
(200 sentences) respectively. The dependency?
based version of Genia includes?493k tokens and
?18k sentences which was generated by convert-
ing the PTB version of Genia created by Illes Solt1
using the (Johansson & Nugues, 2007) tool with
the -conll2007 option to produce annotations in
line with the CoNLL 2007 data set2. As unla-
belled data, we used the datasets distributed in the
framework of the CoNLL 2007 Domain Adapta-
tion Track. For CHEM the set of unlabelled data
consists of 10,482,247 tokens (396,128 sentences)
and for BIO of 9,776,890 tokens (375,421 sen-
tences). For the experiments using Genia as test
set, we used the BIO unlabelled data. This was
possible due to the fact that both the Genia Tree-
bank and the BIO dataset consist of biomedical
abstracts extracted (though using different query
terms) from PubMed.com.
As in?domain training data we have used the
CoNLL 2007 dependency?based version of Sec-
tions 2?11 of the Wall Street Journal (WSJ) par-
tition of the Penn Treebank (PTB), for a total of
447,000 tokens and about 18,600 sentences. For
testing, we used the subset of Section 23 of WSJ
consisting of 5,003 tokens (214 sentences).
All corpora have been morpho?syntactically
tagged and lemmatized by a customized version
1http://categorizer.tmit.bme.hu/?illes/genia ptb/
2In order to be fully compliant with the PTB PoS tagset,
we changed the PoS label of all punctuation marks.
46
of the pos?tagger described in (Dell?Orletta, n.d.)
and dependency parsed by the DeSR parser us-
ing Multi?Layer Perceptron (MLP) as learning al-
gorithm (Attardi, Dell?Orletta, Simi, & Turian,
n.d.), a state?of?the?art linear?time Shift?Reduce
dependency parser following a ?stepwise? ap-
proach (Buchholz & Marsi, 2006).
3 Linguistic analysis of biomedical
abstrats vs newspaper articles
For the specific concerns of this study, we carried
out a comparative linguistic analysis of four dif-
ferent corpora, taken as representative of ordinary
language and biomedical language. In each case,
we took into account a gold (i.e. manually an-
notated) corpus, and an unlabelled corpus, which
was automatically annotated. By comparing the
results obtained with respect to gold and automat-
ically annotated texts, we intend to demonstrate
the reliability of features extracted from automat-
ically annotated texts. As data representative of
ordinary language we took i) the whole WSJ sec-
tion of the Penn Treebank3 including 39,285,425
tokens (1,625,606 sentences) and ii) the sections
2?11 of the WSJ. For what concerns the biomed-
ical domain, we relied on the Genia Treebank in
order to guarantee comparability of the results of
our linguistic analysis with previous studies car-
ried out on this reference corpus. As automatically
annotated data we used the corpus of biomedical
abstract (BIO) distributed as out?domain dataset
used for the ?Domain Adaptation Track? of the
CoNLL 2007 Shared Task.
In order to get evidence of the differences hold-
ing between the WSJ newspaper articles and the
selected biomedical abstracts, the four corpora
have been compared with respect to a wide typol-
ogy of features (i.e. raw text, lexical, morpho?
syntactic and syntactic). Let us start from raw
text features, in particular from average sen-
tence length (calculated as the average number
of words per sentence): as Figure 1 shows, both
the corpus of automatically parsed newspaper ar-
ticles (WSJ unlab) and the manually annotated
one (WSJ gold) contain shorter sentences with re-
spect to both the automatically parsed biomedi-
cal abstrats (BIO unlab) and the manually anno-
tated ones (Genia gold), a result which is in line
3This corpus represents to the unlabelled data set dis-
tributed for the CoNLL 2007 Shared Task on Dependency
Parsing, domain adaptation track.
Figure 1: Average sentence length in biomedical
and newspaper corpora.
with (Clegg & Shepherd, 2005) findings. When
we focus on the lexical level, BIO unlab and
Genia gold appear to have quite a similar per-
centage of lexical items which is not contained
in WSJ gold (23.13% and 26.14% respectively)
while the out?of?vocabulary rate of WSJ unlab
is much lower, i.e. 8.69%. Similar results were
recorded by (Lease & Charniak, 2005) who report
the unknown word rate for various genres of tec-
nical literature.
Let us focus now on the morpho?syntactic level.
If we consider the distribution of nouns, verbs
and adjectives, three features typically represent-
ing stylistic markers associated with different lin-
guistic varieties (Biber & Conrad, 2009), it can
be noticed (see Figures 2(a) and 2(c)) that the
biomedical abstracts contain a higher percentage
of nouns and adjectives while showing a signif-
icantly lower percentage of verbs (Figure 2(b)).
The syntactic counterpart of the different distri-
bution of morpho?syntactic categories can be ob-
served in Table 1, reporting the percentage distri-
bution of the first ten Parts?of?Speech dependency
triplets occurring in the biomedical and newspaper
corpora: each triplet is described as the sequence
of the PoS of a dependent and a head linked by a
depedency arc, by also considering the PoS of the
head father. It turned out that biomedical abstracts
are characterized by nominal dependency triplets,
e.g. two nouns and a preposition (NN?NN?IN)
or noun, preposition, noun (NN?IN?NN) or ad-
jective, noun and preposition (JJ?NN?IN), which
occur more frequently than verbal triplets, such
as determiner, noun and verb (DT?NN?VBZ) or a
verbal root (.?VBD?ROOT)4. Interestingly, in Ge-
nia gold no verbal triplet occurs within the top ten
triplets, which cover the 21% of the total amount
4We named ?ROOT? the artificial root node.
47
(a) Distribution of Nouns (b) Distribution of Verbs (c) Distribution of Adjectives
Figure 2: Distribution of some Parts?of?Speech in biomedical and newspaper corpora.
of triplets occurring in the corpus. By contrast,
the same top ten triplets represent only ?11% in
WSJ gold, testifying the wider variety of syntac-
tic constructions occurring in newspaper articles
with respect to texts of the biomedical domain.
This is also proved by the total amount of differ-
ent PoS dependency triplets occurring in the two
gold datasets, i.e. 7,827 in WSJ gold and 5,064
in Genia gold even though the Genia Treebank is
?50,000?tokens bigger.
Further differences can be observed at a deeper
syntactic level of analysis. This is the case of the
average depth of embedded complement ?chains?
governed by a nominal head. Figure 3(a) shows
that biomedical abstracts are characterized by an
average depth which is higher than the one ob-
served in newspaper articles. A similar trend
can be observed for what concerns the distribu-
tion of ?chains? by depth. In Figure 3(b) shows
that WSJ unlab and WSJ gold ?chains?, on the one
hand, and BIO unlab and Genia gold ?chains?,
on the other hand, overlap. The corpora have
also been compared with respect to i) the av-
erage length of dependency links, measured in
terms of the words occurring between the syntac-
tic head and the dependent (excluding punctua-
tion marks), and ii) the average depth of the whole
parse tree, calculated in terms of the longest path
from the root of the dependency tree to a leaf. In
both cases it can be noted that i) the biomedical
abstracts contain much longer dependency links
than newswire texts (Figure 3(c)) and ii) the av-
erage depth of BIO unlab and Genia gold parse
trees is higher than in the case of the WSJ unlab
and WSJ gold (Figure 3(d)). A further distin-
guishing feature of the biomedical abstracts con-
cerns the average depth of ?chains? of embed-
ded subordinate clauses, calculated here by taking
into account both clausal arguments and comple-
ments linked to a verbal sentence root. As Figure
3(e) shows, both BIO unlab and Genia gold have
shorter ?chains? with respect to the ones contained
in the newspaper articles. Interestingly, a careful
analysis of the distributions by depth of ?chains?
of embedded subordinate clauses shows that the
biomedical abstracts appear to have i) a higher oc-
currence of ?chains? including just one subordinate
clause and ii) a lower percentage of deep ?chains?
with respect to newswire texts. Finally, we com-
pared the two types of corpora with respect to the
distribution of verbal roots. The biomedical ab-
stracts resulted to be characterised by a lower per-
centage of verbal roots with respect to newspaper
articles (see Figure 3(f)). This is in line with the
distribution of verbs as well as of the nominal de-
pendency triplets observed in the biomedical ab-
stracts at the morpho?syntactic level of analysis.
Interestingly, the results obtained with respect
to automatically parsed and manually annotated
data show similar trends for both considered in?
and out?domain corpora, thus demonstrating the
reliability of features monitored against automat-
ically annotated data. In what follows, we will
show how detected linguistic peculiarities can be
exploited in a domain adaptation scenario.
4 Linguistically?driven Unsupervised
Ranking of Parses for Self?training
In the self?training approach illustrated in this pa-
per, the selection of parses from the automatically
annotated target domain dataset is guided by an
augmented version of ULISSE, an unsupervised
linguistically?driven algorithm to select reliable
parses from the output of dependency annotated
texts (Dell?Orletta et al, 2011) which has shown
a good performance for two different languages
48
WSJ gold WSJ unlab Genia gold BIO unlab
Triplet % Freq Triplet % Freq Triplet % Freq Triplet % Freq
DT-NN-IN 2.03 DT-NN-IN 1.72 NN-NN-IN 3.66 DT-NN-IN 2.87
.-VBD-ROOT 1.61 .-VBD-ROOT 1.30 NN-IN-NN 2.93 NN-IN-NN 2.39
NN-IN-NN 1.11 JJ-NN-IN 0.99 DT-NN-IN 2.48 JJ-NN-IN 2.08
JJ-NN-IN 1.10 NN-IN-NN 0.97 JJ-NN-IN 1.96 NN-NN-IN 1.73
.-VBZ-ROOT 1.09 NNP-NNP-IN 0.87 NN-NNS-IN 1.88 IN-NN-IN 1.72
NNP-NNP-IN 0.95 DT-NN-VBD 0.85 JJ-NNS-IN 1.77 JJ-NNS-IN 1.36
DT-NN-VBZ 0.89 NN-VBD-ROOT 0.80 IN-NN-IN 1.65 .-VBD-ROOT 1.33
DT-NN-VBD 0.87 JJ-NNS-IN 0.79 NN-CC-IN 1.64 NNS-IN-NN 1.13
JJ-NNS-IN 0.87 NNP-NNP-VBD 0.78 NNS-IN-NN 1.56 NNP-NN-IN 1.03
IN-NN-IN 0.87 .-VBZ-ROOT 0.75 NN-NN-CC 1.47 NN-IN-VBN 0.93
Table 1: Frequency distribution of the first ten Parts?of?Speech dependency triplets in biomedical and
newspaper corpora.
(a) Depth of embedded complement
?chains?
(b) Distribution of embedded com-
plement ?chains? by depth
(c) Length of dependency links
(d) Parse tree depth (e) Depth of embedded subordinate
clauses ?chains?
(f) Distribution of verbal roots
Figure 3: Syntactic features in biomedical and newspaper corpora.
(English and Italian) against the output of two su-
pervised parsers (MST, (McDonald, Lerman, &
Pereira, 2006) and DeSR, (Attardi, 2006)) se-
lected for their behavioral differences (McDonald
& Nivre, 2007). ULISSE assigns to each depen-
dency tree a score quantifying its reliability based
on a wide range of linguistic features. After col-
lecting statistics about selected features from a
corpus of automatically parsed sentences, for each
newly parsed sentence ULISSE computes a relia-
bility score using the previously extracted feature
statistics. In its reliability assessment, ULISSE ex-
ploits both global and local features, where global
features (listed in Table 2 and discussed in Sec-
tion 3) are computed with respect to each sen-
tence and averaged over all sentences in the cor-
pus, and the local features with respect to indi-
vidual dependency links and averaged over all of
them. Local features include the plausibility of
a dependency link calculated by considering se-
lected features of the dependent and its govern-
ing head as well as of the head father: whereas
in ULISSE the selected features were circum-
scribed to part?of?speech information (so?called
?ArcPOSFeat? feature), in this version of the al-
gorithm a new local feature has been introduced,
named ?ArcLemmaFeat?, which exploits lemma
information. ?ArcPOSFeat? is able to capture the
different distribution of PoS dependency triplets
(see Table 1), along with the type of dependency
link, while the newly introduced ?ArcLemmaFeat?
is meant to capture the lexical peculiarities of the
target domain (see Section 3). As demonstrated
in (Dell?Orletta et al, 2011), both global and lo-
49
cal linguistic features contribute to the selection
of reliable parses. Due to the typology of linguis-
tic features underlying ULISSE, selected reliable
parses typically include domain?specific construc-
tions. This peculiarity of the ULISSE algorithm
turned out to be particularly useful to maximize
the self?training effect in improving the parsing
performance in a domain adaptation scenario.
The reliability score assigned by this augmented
version of ULISSE to newly parsed sentences re-
sults from a combination of the weights associ-
ated with individual features, both global and local
ones. In this study, the reliability score was com-
puted as a simple product of the individual feature
weights: in this way, one low weight feature is suf-
ficient to qualify a parse as low quality and thus to
exclude it from the self?training dataset5.
Feature
Parse tree depth
Embedded complement ?chains? headed by a noun
- Average depth
- Distribution by depth
Verbal roots
Arity of verbal predicates
- Distribution by arity
Subordinate vs main clauses
- Relative ordering of subordinate clauses with respect to the main clause
- Average depth of ?chains? of embedded subordinate clauses
- Distribution of embedded subordinate clauses ?chains? by depth
Length of dependency links
Table 2: Global features underlying ULISSE.
5 Experimental set?up
In the reported experiments, we used the DeSR
parser. Its performance using the proposed domain
adaptation strategy was tested against i) the two
out?domain datasets distributed for the ?Domain
Adaptation Track? of the CoNLL 2007 Shared
Task and ii) the dependency?based version of the
Genia Treebank, described in Section 2. For test-
ing purposes, we selected from the dependency?
based version of the Genia Treebank sentences
with a maximum length of 39 tokens (for a total
of 375,912 tokens and 15,623 sentences).
Results achieved with respect to the CHEM and
BIO test sets were evaluated in terms of ?Labelled
Attachment Score? (LAS), whereas for Genia the
only possible evaluation was in terms of ?Un-
labelled Attachment Score? (UAS). This follows
from the fact that, as reported by Illes, this ver-
sion of Genia is annotated with a Penn Treebank?
style phrase?structure, where a number of func-
tional tags are missing: this influences the type
5See (Dell?Orletta et al, 2011) for a detailed description
of the quality score computation.
Test corpus LAS UAS
PTB 86.09% 87.29%
CHEM 78.50% 81.10%
BIO 78.65% 79.97%
GENIA n/a 80.25%
Table 3: The BASE model tested on PTB, CHEM,
BIO and GENIA.
of evaluation which can be carried out against the
Genia test set.
Achieved results were compared with two base-
lines, represented by: i) the Baseline model
(BASE), i.e. the parsing model trained on the PTB
training set only; ii) the Random Selection (RS) of
parses from automatically parsed out?domain cor-
pora, calculated as the mean of a 10?fold cross?
validation process. As proved by (Sagae, 2010)
and by (McClosky & Charniak, 2008) for the
biomedical domain, the latter represents a strong
unsupervised baseline showing a significant accu-
racy improvement which was obtained by adding
incremental amounts of automatically parsed out?
domain data to the training dataset without any se-
lection strategy.
The experiments we carried out to test the ef-
fectiveness of our self?training strategy were or-
ganised as follows. ULISSE and the baseline al-
gorithms were used to produce different rankings
of parses of the unlabelled target domain corpora.
From the top of these rankings different pools of
parses were selected to be used for training. In
particular, two different sets of experiments were
carried out, namely: i) using only automatically
parsed data as training corpus and ii) combining
automatically parsed data with the PTB training
set. For each set of experiments, different amounts
of unlabelled data were used to create the self?
training models.
6 Results
Table 3 reports the results of the BASE model
tested on PTB, CHEM, BIO and GENIA. When
applied without adaptation to the out?domain
CHEM, BIO and GENIA test sets, the BASE pars-
ing model has a drop of about 7.5% of LAS in both
CHEM and BIO cases. For what concerns UAS,
the drop is about 6% for CHEM and about 7% for
BIO and GENIA.
The results of the performed experiments are
shown in Figures 4 and 5, where each plot re-
ports the accuracy scores (LAS and UAS respec-
tively) of the self?trained parser using the ULISSE
50
(a) LAS for CHEM without PTB training set. (b) LAS for BIO without the PTB training set.
(c) LAS for CHEM with the PTB training set. (d) LAS for BIO with the PTB training set.
Figure 4: LAS of the different self?training models in the two sets of performed experiments.
(a) UAS for GENIA without the PTB training set. (b) UAS for GENIA with the PTB training set.
Figure 5: UAS of the different self?training models for GENIA.
algorithm (henceforth, ULISSE?Stp) and of the
baseline models (RS and BASE). The parser ac-
curacy was computed with respect to different
amounts of automatically parsed data which were
used to create the self?trained parsing model. For
this purpose, we considered six different pools of
250k, 450k, 900k, 1000k, 1350k and 1500k to-
kens. Plots are organized by experiment type: i.e.
the results in subfigures 4(a), 4(b) and 5(a) are
achieved by using only automatically parsed data
as training corpus, whereas those reported in the
other subfigures refer to models trained on auto-
matically parsed data combined with PTB. Note
that in all figures the line named best?RS repre-
sents the best RS score for each pool of k tokens in
the 10?fold cross?validation process.
For what concerns BIO and CHEM, in the first
set of experiments ULISSE?Stp turned out to be
the best self?training algorithm: this is always the
case for CHEM (see subfigure 4(a)), whereas for
BIO (see subfigure 4(b)) it outperforms all base-
lines only when pools of tokens >= 900k are
added. When 900k tokens are used, ULISSE?Stp
allows a LAS improvement of 0.81% on CHEM
and of 0.61% on BIO with respect to RS, and
of 0.62% on CHEM and of 0.48% on BIO with
respect to BASE. It is interesting to note that
ULISSE?Stp using only automatically parsed data
for training achieves better results than BASE (us-
ing the PTB gold training set): to our knowledge,
a similar result has never been reported in the lit-
erature. The behaviour is similar also when the
51
experiments are evaluated in terms of UAS6.
The results achieved in the first set of experi-
ments carried out on the GENIA test set (see 5(a))
differ significantly from what we observed for
CHEM and BIO: in this case, the BASE model ap-
pears to outperform all the other algorithms, with
the ULISSE?Stp algorithm being however more
effective than the RS baselines.
Figures 4(c), 4(d) and 5(b) report the results of
the second set of experiments, i.e. those carried
out by also including PTB in the training set. Note
that in these plots the RS+PTB lines represent the
score of the parser models trained on the pools
of tokens used to obtain the best?RS line com-
bined with the PTB gold training set. It can be
observed that the ULISSE?Stp+PTB self?training
model outperforms all baselines for CHEM, BIO
and GENIA for all the different sizes of pools of
selected tokens. For each pool of parsed data, Ta-
ble 4 records the improvement and the error reduc-
tion observed with respect to the BASE model.
Pool of tokens CHEM Err. red. BIO Err. red. GENIA Err. red.
250k 0.8 3.72 0.76 3.55 0.97 4.91
450k 1.1 5.12 0.54 2.53 1.52 7.7
900k 1.14 5.3 1.02 4.77 1.31 6.63
1000k 0.8 3.72 1.56 7.29 1.2 6.08
1350k 0.4 1.49 1.46 6.82 0.94 4.76
1500k 0.78 3.62 0.75 3.37 0.66 3.34
Table 4: % improvement of ULISSE?Stp+PTB vs
BASE reported in terms of LAS for CHEM and
BIO and of UAS for GENIA.
Differently from (Sagae, 2010) (with a
constituency?based parser), in this set of experi-
ments the self?training approach based on random
selection of sentences (i.e. the best?RS+PTB
baseline) doesn?t achieve any improvement with
respect to the BASE model with only minor
exceptions (observed e.g. with 250k and 450k
pools of added tokens for CHEM and with 250k
for GENIA). Moreover, even when the best?RS
LAS is higher than the ULISSE?Stp score (e.g.
in the first pools of k of Figure 4(b)), ULISSE?
Stp+PTB turns out to be more effective than
the best?RS+PTB baseline (Figure 4(d)). These
results may follow from the fact that ULISSE?Stp
is able to capture not only reliable parses but also,
and more significantly here, parses which reflect
the syntactic peculiarities of the target domain.
Table 5 shows the results of the different
ULISSE?Stp+PTB models tested on the PTB test
6In this paper, for CHEM and BIO experiments we report
only the LAS scores since this is the standard evaluation met-
ric for dependency parsing.
set: no LAS improvement is observed with respect
to the results obtained with the BASE model, i.e.
86.09% (see Table 3). This result is in line with
(McClosky et al, 2010) and (Plank & van No-
ord, 2011) who proved that parsers trained on the
union of gold corpora belonging to different do-
mains achieve a lower accuracy with respect to the
same parsers trained on data belonging to a sin-
gle target domain. Last but not least, it should be
noted that the performance of ULISSE?Stp across
the experiments carried out with pools of automat-
ically parsed tokens of different sizes is in line
with the behaviour of the ULISSE ranking algo-
rithm (Dell?Orletta et al, 2011), where increas-
ingly wider top lists of parsed tokens show de-
creasing LAS scores. This helps explaining why
the performance of ULISSE?Stp starts decreasing
after a certain threshold when wider top?lists of
tokens are added to the parser training data.
Pool of tokens CHEM BIO
250k 83.53 85.55
450k 85.53 86.01
900k 85.95 84.79
1000k 86.03 85.45
1350k 85.49 85.71
1500k 85.67 86.39
Table 5: ULISSE?Stp+PTB on PTB test set with
automatically parsed data.
Conclusion
In this paper we explored a new self?training
method for domain adaptation where the selec-
tion of reliable parses within automatically an-
notated texts is carried out by an unsupervised
linguistically?driven algorithm, ULISSE. Results
achieved for the CoNLL 2007 datasets as well
as for the larger test set represented by GENIA
show a significant improvement with respect to
considered baselines. This demonstrates a two?
fold property of ULISSE, namely its reliablity
and effectiveness both in capturing peculiarities
of biomedical texts, and in selecting high quality
parses. Thanks to these properties the proposed
self?training method is able to improve the parser
performances when tested in an out?domain sce-
nario. The same approach could in principle be
applied to deal with biomedical sub?domain varia-
tion: as reported by (Lippincott, Se?aghdha, & Ko-
rhonen, 2011), biomedical texts belonging to dif-
ferent sub?domains do vary along many linguistic
dimensions, with a potential negative impact on
biomedical NLP tools.
52
References
Attardi, G. (2006). Experiments with a multi-
language non-projective dependency parser.
In Proceedings of CoNLL-X ?06 (pp. 166?
170). New York City, New York.
Attardi, G., Dell?Orletta, F., Simi, M., & Turian, J.
(n.d.). Accurate dependency parsing with a
stacked multilayer perceptron. In Proceed-
ings of EVALITA 2009.
Biber, D., & Conrad, S. (2009). Genre, regis-
ter, style. Cambridge: Cambridge Univer-
sity Press.
Buchholz, S., & Marsi, E. (2006). CoNLL-
X shared task on multilingual dependency
parsing. In Proceedings of CoNLL 2006.
Clegg, A. B., & Shepherd, A. J. (2005). Evalu-
ating and integrating treebank parsers on a
biomedical corpus. In In proceedings of the
ACL 2005 workshop on software.
Dell?Orletta, F. (n.d.). Ensemble system for
part-of-speech tagging. In Proceedings of
EVALITA 2009.
Dell?Orletta, F., Venturi, G., & Montemagni, S.
(2011). Ulisse: an unsupervised algorithm
for detecting reliable dependency parses. In
Proceedings of CoNLL 2011 (pp. 115?124).
Portland, Oregon.
Gildea, D. (2001). Corpus variation and parser
performance. In Proceedings of EMNLP
2001 (p. 167-202). Pittsburgh, PA.
Johansson, R., & Nugues, P. (2007). Ex-
tended constituent-to-dependency conver-
sion for english. In Proceedings of NODAL-
IDA 2007. Tartu, Estonia.
Kawahara, D., & Uchimoto, K. (2008). Learning
reliability of parses for domain adaptation
of dependency parsing. In Proceedings of
IJCNLP 2008 (pp. 709?714).
Lease, M., & Charniak, E. (2005). Parsing
biomedical literature. In Proceedings of
the second international joint conference on
natural language processing (IJCNLP-05),
Jeju Island, Korea (pp. 58?69).
Lippincott, T., Se?aghdha, D. O?., & Korhonen, A.
(2011). Exploring subdomain variation in
biomedical language. BMC Bioinformatics,
12, 212?233.
McClosky, D., & Charniak, E. (2008). Self-
training for biomedical parsing. In Proceed-
ings of ACL?HLT 2008 (pp. 101?104).
McClosky, D., Charniak, E., & Johnson, M.
(2006). Reranking and self-training for
parser adaptation. In Proceedings of ACL
2006 (pp. 337?344). Sydney, Australia.
McClosky, D., Charniak, E., & Johnson, M.
(2010). Automatic domain adaptation for
parsing. In Proceedings of NAACL?HLT
2010 (pp. 28?36). Los Angeles, California.
McDonald, R., Lerman, K., & Pereira, F. (2006).
Multilingual dependency analysis with a
two-stage discriminative parser. In Proceed-
ings of CoNLL 2006.
McDonald, R., & Nivre, J. (2007). Character-
izing the errors of data-driven dependency
parsing models. In Proceedings of EMNLP-
CoNLL 2007 (p. 122-131).
Mitchell P. Marcus, M. A. M., & Santorini, B.
(1993). Building a large annotated corpus
of english: the penn treebank. Comput. Lin-
guist., 19(2), 313?330.
Nivre, J., Hall, J., Ku?bler, S., McDonald, R., Nils-
son, J., Riedel, S., & Yuret, D. (2007).
The conll 2007 shared task on dependency
parsing. In Proceedings of EMNLP-CoNLL
2007 (pp. 915?932).
Plank, B., & van Noord, G. (2011). Effective mea-
sures of domain similarity for parsing. In
Proceedings of ACL 2011 (pp. 1566?1576).
Portland, Oregon.
Reichart, R., & Rappoport, A. (2007). Self?
training for enhancement and domain adap-
tation of statistical parsers trained on small
datasets. In Proceedings of ACL 2007 (pp.
616?623).
Reichart, R., & Rappoport, A. (2009). Automatic
selection of high quality parses created by a
fully unsupervised parser. In Proceedings of
CoNLL 2009 (pp. 156?164).
Sagae, K. (2010). Self-training without reranking
for parser domain adaptation and its impact
on semantic role labeling. In Proceedings
of the 2010 workshop on domain adaptation
for natural language processing (DANLP
2010) (pp. 37?44). Uppsala, Sweden.
Sagae, K., & Tsujii, J. (2007). Dependency pars-
ing and domain adaptation with lr models
and parser ensemble. In Proceedings of
EMNLP?CoNLL 2007 (pp. 1044?1050).
Tateisi, Y., Yakushiji, A., Ohta, T., & Tsujii, J.
(2005). Syntax annotation for the GENIA
corpus. In Proceedings of IJCNLP?05 (pp.
222?227). Jeju Island, Korea.
53
