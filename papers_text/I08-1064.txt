Projection-based Acquisition of a Temporal Labeller
Kathrin Spreyer?
Department of Linguistics
University of Potsdam
Germany
spreyer@uni-potsdam.de
Anette Frank
Dept. of Computational Linguistics
University of Heidelberg
Germany
frank@cl.uni-heidelberg.de
Abstract
We present a cross-lingual projection frame-
work for temporal annotations. Auto-
matically obtained TimeML annotations in
the English portion of a parallel corpus
are transferred to the German translation
along a word alignment. Direct projection
augmented with shallow heuristic knowl-
edge outperforms the uninformed baseline
by 6.64% F
1
-measure for events, and by
17.93% for time expressions. Subsequent
training of statistical classifiers on the (im-
perfect) projected annotations significantly
boosts precision by up to 31% to 83.95% and
89.52%, respectively.
1 Introduction
In recent years, supervised machine learning has be-
come the standard approach to obtain robust and
wide-coverage NLP tools. But manually annotated
training data is a scarce and expensive resource. An-
notation projection (Yarowsky and Ngai, 2001) aims
at overcoming this resource bottleneck by scaling
conceptually monolingual resources and tools to a
multilingual level: annotations in existing monolin-
gual corpora are transferred to a different language
along the word alignment to a parallel corpus.
In this paper, we present a projection framework
for temporal annotations. The TimeML specifica-
tion language (Pustejovsky et al, 2003a) defines an
annotation scheme for time expressions (timex for
? The first author was affiliated with Saarland University
(Saarbru?cken, Germany) at the time of writing.
John [met]
event
Mary [last night]
timex
.
John [traf]
event
Mary [gestern Abend]
timex
.
Figure 1: Annotation projection.
short) and events, and there are tools for the auto-
matic TimeML annotation of English text (Verha-
gen et al, 2005). Similar rule-based systems exist
for Spanish and Italian (Saquete et al, 2006). How-
ever, such resources are restricted to a handful of
languages.
We employ the existing TimeML labellers to an-
notate the English portion of a parallel corpus, and
automatically project the annotations to the word-
aligned German translation. Fig. 1 shows a simple
example. The English sentence contains an event
and a timex annotation. The event-denoting verb met
is aligned with the German traf, hence the latter also
receives the event tag. Likewise, the components of
the multi-word timex last night align with German
gestern and abend, respectively, and the timex tag is
transferred to the expression gestern abend.
Projection-based approaches to multilingual an-
notation have proven adequate in various domains,
including part-of-speech tagging (Yarowsky and
Ngai, 2001), NP-bracketing (Yarowsky et al, 2001),
dependency analysis (Hwa et al, 2005), and role se-
mantic analysis (Pado? and Lapata, 2006). To our
knowledge, the present proposal is the first to apply
projection algorithms to temporal annotations.
489
Cross-lingually projected information is typically
noisy, due to errors in the source annotations as
well as in the word alignment. Moreover, success-
ful projection relies on the direct correspondence
assumption (DCA, Hwa et al (2002)) which de-
mands that the annotations in the source text be
homomorphous with those in its (literal) transla-
tion. The DCA has been found to hold, to a sub-
stantial degree, for the above mentioned domains.
The results we report here show that it can also
be confirmed for temporal annotations in English
and German. Yet, we cannot preclude divergence
from translational correspondence; on the contrary,
it occurs routinely and to a certain extent systemat-
ically (Dorr, 1994). We employ two different tech-
niques to filter noise. Firstly, the projection process
is equipped with (partly language-specific) knowl-
edge for a principled account of typical alignment
errors and cross-language discrepancies in the reali-
sation of events and timexes (section 3.2). Secondly,
we apply aggressive data engineering techniques to
the noisy projections and use them to train statistical
classifiers which generalise beyond the noise (sec-
tion 5).
The paper is structured as follows. Section 2
gives an overview of the TimeML specification lan-
guage and compatible annotation tools. Section 3
presents our projection models for temporal annota-
tions, which are evaluated in section 4. Section 5
describes how we induce temporal labellers for Ger-
man from the projected annotations; section 6 con-
cludes.
2 Temporal Annotation
2.1 The TimeML Specification Language
The TimeML specification language (Pustejovsky
et al, 2003a)1 and annotation framework emerged
from the TERQAS workshop2 in the context of the
ARDA AQUAINT programme. The goal of the pro-
gramme is the development of question answering
(QA) systems which index content rather than plain
keywords. Semantic indexing based on the identifi-
cation of named entities in free text is an established
1A standardised version ISO-TimeML is in preparation, cf.
Schiffrin and Bunt (2006).
2See http://www.timeml.org/site/terqas/in
dex.html
method in QA and related applications. Recent years
have also seen advances in relation extraction, a vari-
ant of event identification, albeit restricted in terms
of coverage: the majority of systems addressing
the task use a pre-defined set of?typically domain-
specific?templates. In contrast, TimeML models
events in a domain-independent manner and pro-
vides principled definitions for various event classes.
Besides the identification of events, it addresses their
relative ordering and anchoring in time by integrat-
ing timexes in the annotation. The major contri-
bution of TimeML is the explicit representation of
dependencies (so-called links) between timexes and
events.
Unlike traditional accounts of events (e.g.,
Vendler (1967)), TimeML adopts a very broad
notion of eventualities as ?situations that happen
or occur? and ?states or circumstances in which
something obtains or holds true? (Pustejovsky et
al., 2003a); besides verbs, this definition includes
event nominals such as accident, and stative mod-
ifiers (prepared, on board). Events are annotated
with EVENT tags. TimeML postulates seven event
classes: REPORTING, PERCEPTION, ASPECTUAL, I-
ACTION, I-STATE, STATE, and OCCURRENCE. For
definitions of the individual classes, the reader is re-
ferred to Saur?? et al (2005b).
Explicit timexes are marked by the TIMEX3 tag.
It is modelled on the basis of Setzer?s (2001) TIMEX
tag and the TIDES TIMEX2 annotation (Ferro et al,
2005). Timexes are classified into four types: dates,
times, durations, and sets.
Events and timexes are interrelated by three kinds
of links: temporal, aspectual, and subordinating.
Here, we consider only subordinating links (slinks).
Slinks explicate event modalities, which are of cru-
cial importance when reasoning about the certainty
and factuality of propositions conveyed by event-
denoting expressions; they are thus directly rel-
evant to QA and information extraction applica-
tions. Slinks relate events in modal, factive, counter-
factive, evidential, negative evidential, or condi-
tional relationships, and can be triggered by lexical
or structural cues.
2.2 Automatic Labellers for English
The basis of any projection architecture are high-
quality annotations of the source (English) portion
490
e ? E temporal entity
l ? E ? E (subordination) link
ws ? Ws, wt ? Wt source/target words
al ? Al : Ws ? Wt word alignment
As ? as : E ? 2Ws source annotation
At ? at : projected target
(E ? As ? Al) ? 2Wt annotation
Table 1: Notational conventions.
of the parallel corpus. However, given that the pro-
jected annotations are to provide enough data for
training a target language labeller (section 5), man-
ual annotation is not an option. Instead, we use the
TARSQI tools for automatic TimeML annotation of
English text (Verhagen et al, 2005). They have been
modelled and evaluated on the basis of the Time-
Bank (Pustejovsky et al, 2003b), yet for the most
part rely on hand-crafted rules. To obtain a full tem-
poral annotation, the modules are combined in a cas-
cade. We are using the components for timex recog-
nition and normalisation (Mani and Wilson, 2000),
event extraction (Saur?? et al, 2005a), and identifica-
tion of modal contexts (Saur?? et al, 2006).3
3 Informed Projection
3.1 The Core Algorithm
Recall that TimeML represents temporal entities
with EVENT and TIMEX3 tags which are anchored
to words in the text. Slinks, on the other hand, are
not anchored in the text directly, but rather relate
temporal entities. The projection of links is there-
fore entirely determined by the projection of the en-
tities they are defined on (see Table 1 for the nota-
tion used throughout this paper): a link l = (e, e?)
in the source annotation as projects to the target an-
notation at iff both e and e? project to non-empty
sequences of words. The projection of the enti-
ties e, e? themselves, however, is a non-trivial task.
3TARSQI also comprises a component that introduces tem-
poral links (Mani et al, 2003); we are not using it here because
the output includes the entire tlink closure. Although Mani et al
(2006) use the links introduced by closure to boost the amount
of training data for a tlink classifier, this technique is not suit-
able for our learning task since the closure might easily propa-
gate errors in the automatic annotations.
a.. . . [ ws ]e . . . b. . . . [ ws ]e . . .
. . . [ wt ]e . . . . . . [ wtj wtj+1 ]e . . .
c. . . . [ wsi wsi+1 ]e . . .
. . . [ wtj wtj+1 wtj+2 ]e . . .
Figure 2: Projection scenarios: (a) single-word 1-to-
1, (b) single-word 1-to-many, (c) multi-word.
a. [ . . . ]e b. [ . . . ]e . . .[ . . . ]e?
wtj?2 wtj?1 wtj wtj+1 wt
Figure 3: Problematic projection scenarios: (a) non-
contiguous aligned span, (b) rivalling tags.
Given a temporal entity e covering a sequence as(e)
of tokens in the source annotation, the projection
model needs to determine the extent at(e, as, al) of
e in the target annotation, based on the word align-
ment al . Possible projection scenarios are depicted
in Fig. 2. In the simplest case (Fig. 2a), e spans a
single word ws which aligns with exactly one word
wt in the target sentence. In this case, the model
predicts e to project to wt. A single tagged word
with 1-to-many alignments (as in Fig. 2b) requires
a more thorough inspection of the aligned words. If
they form a contiguous sequence, e can be projected
onto the entire sequence as a multi-word unit. This
is problematic in a scenario such as the one shown in
Fig. 3a, where the aligned words do not form a con-
tiguous sequence. There are various strategies, de-
scribed in section 3.2, to deal with non-contiguous
cases. For the moment, we can adopt a conservative
approach which categorically blocks discontinuous
projections. Finally, Fig. 2c illustrates the projec-
tion of an entity spanning multiple words. Here, the
model composes the projection span of e from the
alignment contribution of each individual word ws
covered by e. Again, the final extent of the projected
entity is required to be contiguous.
With any of these scenarios, a problem arises
when two distinct entities e and e? in the source an-
491
1. project(as, al ):
2. at,C = ?
3. for each entity e defined by as:
4. at,C(e, as, al) =
SC
ws?as(e) proj(ws, e, as, al)
5. for each link l = (e, e?) defined over as:
6. if at,C(e, as, al) 6= ? and at,C(e?, as, al) 6= ?
7. then define l to hold for at,C
8. return at,C
where
proj(ws, e, as, al) = {wt ? Wt | (ws, wt) ? al ?
?e? ? as. e? 6= e ? wt 6? at,C(e?, as, al)}
and
[C
S =
?
S
S :
S
S is convex
? : otherwise
Figure 4: The projection algorithm.
notation have conflicting projection extents, that is,
when at(e, as, al) ? at(e?, as, al ) 6= ?. This is il-
lustrated in Fig. 3b. The easiest strategy to resolve
conflicts like these is to pick an arbitrary entity and
privilege it for projection to the target word(s) wt in
question. All other rivalling entities e? project onto
their remaining target words at(e?, as, al) \ {wt}.
Pseudocode for this word-based projection of
temporal annotations is provided in Fig. 4.
3.2 Incorporating Additional Knowledge
The projection model described so far is extremely
susceptible to errors in the word alignment. Re-
lated efforts (Hwa et al, 2005; Pado? and Lapata,
2006) have already suggested that additional lin-
guistic information can have considerable impact on
the quality of the projected annotations. We there-
fore augment the baseline model with several shal-
low heuristics encoding linguistic or else topologi-
cal constraints for the choice of words to project to.
Linguistically motivated filters refer to the part-of-
speech (POS) tags of words in the target language
sentence, whereas topological criteria investigate the
alignment topology.
Linguistic constraints. Following Pado? and La-
pata (2006), we implement a filter which discards
alignments to non-content words, for two reasons:
(i) alignment algorithms are known to perform
poorly on non-content words, and (ii) events as
well as timexes are necessarily content-bearing and
hence unlikely to be realised by non-content words.
This non-content (NC) filter is defined in terms of
POS tags and affects conjunctions, prepositions and
punctuation. In the context of temporal annotations,
we extend the scope of the filter such that it effec-
tively applies to all word classes that we deem un-
likely to occur as part of a temporal entity. There-
fore, the NC filter is actually defined stronger for
events than for timexes, in that it further blocks
projection of events to pronouns, whereas pronouns
may be part of a timex such as jeden Freitag ?ev-
ery Friday?. Moreover, events prohibit the projec-
tion to adverbs; this restriction is motivated by the
fact that events in English are frequently translated
in German as adverbials which lack an event read-
ing (cf. head switching translations like prefer to X
vs. German lieber X ?rather X?). We also devise an
unknown word filter: it applies to words for which
no lemma could be identified in the preprocessing
stage. Projection to unknown words is prohibited
unless the alignment is supported bidirectionally.
The strictness concerning unknown words is due to
the empirical observation that alignments which in-
volve such words are frequently incorrect.
In order to adhere to the TimeML specification, a
simple transformation ensures that articles and con-
tracted prepositions such as am ?on the? are included
in the extent of timexes. Another heuristics is de-
signed to remedy alignment errors involving auxil-
iary and modal verbs, which are not to be annotated
as events. If an event aligns to more than one word,
then this filter singles out the main verb or noun and
discards auxiliaries.
Topological constraints. In section 3.1, we de-
scribed a conservative projection principle which re-
jects the transfer of annotations to non-contiguous
sequences. That model sets an unnecessarily modest
upper bound on recall; but giving up the contiguity
requirement entirely is not sensible either, since it is
indeed highly unlikely for temporal entities to be re-
alised discontinuously in either source or target lan-
guage (noun phrase cohesion, Yarowsky and Ngai
(2001)). Based on these observations, we propose
two refined models which manipulate the projected
annotation span so as to ensure contiguity. One
492
model identifies and discards outlier alignments,
which actively violate contiguity; the other one adds
missing alignments, which form gaps. Technically,
both models establish convexity in non-convex sets.
Hence, we first have to come up with a backbone
model which is less restrictive than the baseline, so
that the convexation models will have a basis to op-
erate on. A possible backbone model at,0 is pro-
vided in (1).
(1) at,0(e, as, al) =
?
ws?as(e)
proj(ws, e, as, al )
This model simply gathers all words aligned with
any word covered by e in the source annotation, ir-
respective of contiguity in the resulting sequence of
words. Discarding outlier alignments is then for-
malised as a reduction of at,0?s output to (one of)
its greatest convex subset(s) (GCS). Let us call this
model at,GCS. In terms of a linear sequence of
words, at,GCS chooses the longest contiguous sub-
sequence. The GCS-model thus serves a filtering
purpose similar to the NC filter. However, whereas
the latter discards single alignment links on linguis-
tic grounds, the former is motivated by topological
properties of the alignment as a whole.
The second model, which fills gaps in the word
alignment, constructs the convex hull of at,0 (cf.
Pado? and Lapata (2005)). We will refer to this model
as at,CH. The example in (2) illustrates both models.
(2)
[ . . . ]e
?C : ?
GCS : {1, 2}
1 2 3 4 5 CH : {1, 2, 3, 4, 5}
Here, entity e aligns to the non-contiguous token
sequence [1, 2, 5], or equivalently, the non-convex
set {1, 2, 5}(= at,0(e)). The conservative base-
line at,C rejects the projection altogether, whereas
at,GCS projects to the tokens 1 and 2. The additional
padding introduced by the convex hull (at,CH) fur-
ther extends the projected extent to {1, 2, 3, 4, 5}.
Alignment selection. Although bi-alignments are
known to exhibit high precision (Koehn et al, 2003),
in the face of sparse annotations we use unidirec-
tional alignments as a fallback, as has been proposed
in the context of phrase-based machine translation
(Koehn et al, 2003; Tillmann, 2003). Furthermore,
we follow Hwa et al (2005) in imposing a limit on
the maximum number of words that a single word
may align to.
4 Experiments
Our evaluation setup consists of experiments con-
ducted on the English-German portion of the Eu-
roparl corpus (Koehn, 2005); specifically, we work
with the preprocessed and word-aligned version
used in Pado? and Lapata (2006): the source-target
and target-source word alignments were automati-
cally established by GIZA++ (Och and Ney, 2003),
and their intersection achieves a precision of 98.6%
and a recall of 52.9% (Pado?, 2007). The preprocess-
ing consisted of automatic POS tagging and lemma-
tisation.
To assess the quality of the TimeML projec-
tions, we put aside and manually annotated a de-
velopment set of 101 and a test set of 236 bi-
sentences.4 All remaining data (approx. 960K bi-
sentences) was used for training (section 5). We
report the weighted macro average over all possi-
ble subclasses of timexes/events, and consider only
exact matches. The TARSQI annotations exhibit
an F
1
-measure of 80.56% (timex), 84.64% (events),
and 43.32% (slinks) when evaluated against the En-
glish gold standard.
In order to assess the usefulness of the linguis-
tic and topological parameters presented in section
3.2, we determined the best performing combination
of parameters on the development set. Not surpris-
ingly, event and timex models benefit from the var-
ious heuristics to different degrees. While the pro-
jection of events can benefit from the NC filter, the
projection of timexes is rather hampered by it. In-
stead, it exploits the flexibility of the GCS convexa-
tion model together with a conservative limit of 2 on
per-word alignments. In the underlying data sample
of 101 sentences, the English-to-German alignment
direction appears to be most accurate for timexes.
Table 2 shows the results of evaluating the optimised
models on the test set, along with the baseline from
section 3.1 and a ?full? model which activates all
4The unconventional balance of test and development data is
due to the fact that a large portion of the annotated data became
available only after the parameter estimation phase.
493
events slinks time expressions
model prec recall F prec recall F prec recall F
timex-optimised 48.53 33.73 39.80 30.09 10.71 15.80 71.01 52.76 60.54
event-optimised 50.94 44.23 47.34 30.96 14.29 19.55 56.55 42.52 48.54
combined 50.98 44.36 47.44 30.96 14.29 19.55 71.75 52.76 60.80
baseline 52.26 33.46 40.80 26.98 10.71 15.34 49.53 37.80 42.87
full 51.10 40.42 45.14 29.95 13.57 18.68 73.74 54.33 62.56
Table 2: Performance of projection models over test data.
[. . .] must today decide [. . .]: [. . .] (108723)
[. . .] hat heute u?ber
1
[. . .] zu entscheiden, na?mlich u?ber
2
[. . .]
APPR VVINF APPR
Figure 5: Amending alignment errors.
heuristics. The results confirm our initial assump-
tion that linguistic and topological knowledge does
indeed improve the quality of the projected annota-
tions. The model which combines the optimal set-
tings for timexes and events outperforms the un-
informed baseline by 17.93% (timexes) and 6.64%
(events) F
1
-measure. However, exploration of the
model space on the basis of the (larger and thus pre-
sumably more representative) test set shows that the
optimised models do not generalise well. The test
set-optimised model activates all linguistic heuris-
tics, and employs at,CH convexation. For events,
projection considers bi-alignments with a fallback to
unidirectional alignments, preferably from English
to German; timex projection considers all alignment
links. This test set-optimised model, which we will
use to project the training instances for the maxi-
mum entropy classifier, achieves an F
1
-measure of
48.82% (53.15% precision) for events and 62.04%
(73.74% precision) for timexes.5
With these settings, our projection model is ca-
pable of repairing alignment errors, as shown in
Fig. 5, where the automatic word alignments are rep-
resented as arrows. The conservative baseline con-
sidering only bidirectional alignments discards all
5The model actually includes an additional strategy to ad-
just event and timex class labels on the basis of designated
FrameNet frames; the reader is referred to Spreyer (2007), ch.
4.5 for details.
event timex
data prec recall prec recall
all 53.15 45.14 73.74 53.54
best 75% 54.81 47.06 74.61 62.82
Table 3: Correlation between alignment probability
and projection quality.
alignments but the (incorrect) one to u?ber
1
. The op-
timised model, on the other hand, does not exclude
any alignments in the first place; the faulty align-
ments to u?ber
1
and u?ber
2
are discarded on linguistic
grounds by the NC filter, and only the correct align-
ment to entscheiden remains for projection.
5 Robust Induction
The projected annotations, although noisy, can be
exploited to train a temporal labeller for German.
As Yarowsky and Ngai (2001) demonstrate for POS
tagging, aggressive filtering techniques applied to
vast amounts of (potentially noisy) training data are
capable of distilling relatively high-quality data sets,
which may then serve as input to machine learn-
ing algorithms. Yarowsky and Ngai (2001) use the
Model-3 alignment score as an indicator for the
quality of (i) the alignment, and therefore (ii) the
projection. In the present study, discarding 25% of
the sentences based on this criterion leads to gains
in both recall and precision (Table 3). In accor-
dance with the TimeML definition, we further re-
strict training instances on the basis of POS tags by
basically re-applying the NC filter (section 3.2). But
even so, the proportion of positive and negative in-
stances remains heavily skewed?an issue which we
will address below by formulating a 2-phase classi-
494
prec recall F F
model event slink
1-step 83.48 32.58 46.87 17.01
1-step unk 83.88 32.19 46.53 16.87
2-step 83.95 34.44 48.84 19.06
2-step unk 84.21 34.30 48.75 19.06
timex
1-step 87.77 49.11 62.98
1-step unk 87.22 49.55 63.20
2-step 89.52 51.79 65.62
2-step unk 88.68 50.89 64.67
Table 4: Classifier performance over test data.
fication task.
The remaining instances6 are converted to feature
vectors encoding standard lexical and grammatical
features such as (lower case) lemma, POS, govern-
ing prepositions, verbal dependents, etc.7 For slink
instances, we further encode the syntactic subordi-
nation path (if any) between the two events.
We trained 4 classifiers,8 with and without
smoothing with artificial unknowns (Collins, 2003),
and as a 1-step versus a 2-step decision in which
instances are first discriminated by a binary classi-
fier, so that only positive instances are passed on to
be classified for a subclass. The performance of the
various classifiers is given in Table 4. Although the
overall F
1
-measure does not notably differ from that
achieved by direct projection, we observe a drastic
gain in precision, albeit at the cost of recall. With
almost 84% and 90% precision, this is an ideal start-
ing point for a bootstrapping procedure.
6 Discussion and Future Work
Clearly, the?essentially unsupervised?projection
framework presented here does not produce state-
of-the-art annotations. But it does provide an inex-
6Note that slink instances are constructed for event pairs, as
opposed to event and timex instances, which are constructed for
individual words.
7The grammatical features have been extracted from analy-
ses of the German ParGram LFG grammar (Rohrer and Forst,
2006).
8We used the opennlp.maxent package,
http://maxent.sourceforge.net/.
pensive and largely language-independent basis (a)
for manual correction, and (b) for bootstrapping al-
gorithms. In the future, we will investigate how
weakly supervised machine learning techniques like
co-training (Blum and Mitchell, 1998) could further
enhance projection, e.g. taking into account a third
language in a triangulation setting (Kay, 1997).
Acknowledgements
We would like to thank Sebastian Pado? for provid-
ing us with the aligned Europarl data, Inderjeet Mani
and Marc Verhagen for access to the TARSQI tools,
and James Pustejovsky for clarification of TimeML
issues. We would also like to thank the three anony-
mous reviewers for helpful comments.
References
Avrim Blum and Tom Mitchell. 1998. Combining La-
beled and Unlabeled Data with Co-Training. In Pro-
ceedings of the 1998 Conference on Computational
Learning Theory, pages 92?100, July.
Michael Collins. 2003. Head-Driven Statistical Mod-
els for Natural Language Parsing. Computational Lin-
guistics, 29(4):589?637, December.
Bonnie J. Dorr. 1994. Machine Translation Divergences:
A Formal Description and Proposed Solution. Com-
putational Linguistics, 20(4):597?635.
Lisa Ferro, Laurie Gerber, Inderjeet Mani, Beth Sund-
heim, and George Wilson, 2005. TIDES 2005 Stan-
dard for the Annotation of Temporal Expressions,
September.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan
Kolak. 2002. Evaluating Translational Correspon-
dence using Annotation Projection. In Proceedings of
ACL-2002, Philadelphia, PA.
R. Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas,
and Okan Kolak. 2005. Bootstrapping Parsers via
Syntactic Projection across Parallel Texts. Natural
Language Engineering, 11(3):311?325.
Martin Kay. 1997. The Proper Place of Men and Ma-
chines in Language Translation. Machine Translation,
12(1-2):3?23.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of HLT/NAACL 2003, pages 127?133.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of the
MT Summit 2005.
495
Inderjeet Mani and George Wilson. 2000. Robust Tem-
poral Processing of News. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics (ACL-2000), pages 69?76, Hong Kong.
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.
2003. Inferring Temporal Ordering of Events in News.
In Proceedings of the Human Language Technology
Conference (HLT-NAACL-2003). Short paper.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine
Learning of Temporal Relations. In Proceedings of
ACL/COLING 2006, pages 753?760, Sydney, Aus-
tralia.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Sebastian Pado? and Mirella Lapata. 2005. Cross-lingual
projection of role-semantic information. In Proceed-
ings of HLT/EMNLP 2005, Vancouver, BC.
Sebastian Pado? and Mirella. Lapata. 2006. Optimal con-
stituent alignment with edge covers for semantic pro-
jection. In Proceedings of ACL-COLING 2006, Syd-
ney, Australia.
Sebastian Pado?. 2007. Cross-Lingual Annotation Pro-
jection Models for Role-Semantic Information. Ph.D.
thesis, Saarland University, Saarbru?cken, Germany.
James Pustejovsky, Jose? Castan?o, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Graham
Katz. 2003a. TimeML: Robust Specification of Event
and Temporal Expressions in Text. In Proceedings
of the Fifth International Workshop on Computational
Semantics.
James Pustejovsky, Patrick Hanks, Roser Saur??, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro, and
Marcia Lazo. 2003b. The TimeBank Corpus. In Pro-
ceedings of Corpus Linguistics, pages 647?656.
Christian Rohrer and Martin Forst. 2006. Improving
coverage and parsing quality of a large-scale LFG for
German. In Proceedings of LREC 2006, pages 2206?
2211, Genoa, Italy, May.
Estela Saquete, Patricio Mart??nez-Barco, Rafael Mun?oz,
Matteo Negri, Manuela Speranza, and Rachele Sprug-
noli. 2006. Multilingual Extension of a Temporal
Expression Normalizer using Annotated Corpora. In
Proceedings of the EACL 2006 Workshop on Cross-
Language Knowledge Induction, Trento, Italy, April.
Roser Saur??, Robert Knippen, Marc Verhagen, and
James Pustejovsky. 2005a. Evita: A Robust Event
Recognizer For QA Systems. In Proceedings of
HLT/EMNLP 2005, pages 700?707.
Roser Saur??, Jessica Littman, Bob Knippen, Robert
Gaizauskas, Andrea Setzer, and James Pustejovsky,
2005b. TimeML Annotation Guidelines Version 1.2.1,
October.
Roser Saur??, Marc Verhagen, and James Pustejovsky.
2006. SlinkET: A Partial Modal Parser for Events. In
Proceedings of LREC-2006, Genova, Italy, May. To
appear.
Amanda Schiffrin and Harry Bunt. 2006. Defining a
preliminary set of interoperable semantic descriptors.
Technical Report D4.2, INRIA-Loria, Nancy, France,
August.
Andrea Setzer. 2001. Temporal Information in Newswire
Articles: an Annotation Scheme and Corpus Study.
Ph.D. thesis, University of Sheffield, Sheffield, UK.
Kathrin Spreyer. 2007. Projecting Temporal Annotations
Across Languages. Diploma thesis, Saarland Univer-
sity, Saarbru?cken, Germany.
Christoph Tillmann. 2003. A Projection Extension Algo-
rithm for Statistical Machine Translation. In Michael
Collins and Mark Steedman, editors, Proceedings of
the 2003 Conference on Empirical Methods in Natural
Language Processing (EMNLP-2003), pages 1?8.
Zeno Vendler, 1967. Linguistics in Philosophy, chapter
Verbs and Times, pages 97?121. Cornell University
Press, Ithaca, NY.
Marc Verhagen, Inderjeet Mani, Roser Sauri, Robert
Knippen, Jessica Littman, and James Pustejovsky.
2005. Automating Temporal Annotation with
TARSQI. In Proceedings of the ACL-2005.
David Yarowsky and Grace Ngai. 2001. Inducing Mul-
tilingual POS Taggers and NP Bracketers via Robust
Projection across Aligned Corpora. In Proceedings of
NAACL-2001, pages 200?207.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing Multilingual Text Analysis Tools via
Robust Projection across Aligned Corpora. In Pro-
ceedings of HLT 2001, First International Conference
on Human Language Technology Research.
496
