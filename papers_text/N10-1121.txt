Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 795?803,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Convolution Kernels for Opinion Holder Extraction
Michael Wiegand and Dietrich Klakow
Spoken Language Systems
Saarland University
D-66123 Saarbru?cken, Germany
{Michael.Wiegand|Dietrich.Klakow}@lsv.uni-saarland.de
Abstract
Opinion holder extraction is one of the impor-
tant subtasks in sentiment analysis. The ef-
fective detection of an opinion holder depends
on the consideration of various cues on vari-
ous levels of representation, though they are
hard to formulate explicitly as features. In this
work, we propose to use convolution kernels
for that task which identify meaningful frag-
ments of sequences or trees by themselves.
We not only investigate how different levels
of information can be effectively combined
in different kernels but also examine how the
scope of these kernels should be chosen. In
general relation extraction, the two candidate
entities thought to be involved in a relation are
commonly chosen to be the boundaries of se-
quences and trees. The definition of bound-
aries in opinion holder extraction, however, is
less straightforward since there might be sev-
eral expressions beside the candidate opinion
holder to be eligible for being a boundary.
1 Introduction
In recent years, there has been a growing interest
in the automatic detection of opinionated content
in natural language text. One of the more impor-
tant tasks in sentiment analysis is the extraction of
opinion holders. Opinion holder extraction is one
of the critical components of an opinion question-
answering system (i.e. systems which automatically
answer opinion questions, such as ?What does [X]
like about [Y]??). Such systems need to be able to
distinguish which entities in a candidate answer sen-
tence are the sources of opinions (= opinion holder)
and which are the targets.
On other NLP tasks, in particular, on relation extrac-
tion, there has been much work on convolution ker-
nels, i.e. kernel functions exploiting huge amounts
of features without an explicit feature representa-
tion. Previous research on that task has shown that
convolution kernels, such as sequence and tree ker-
nels, are quite effective when compared to manual
feature engineering (Moschitti, 2008; Bunescu and
Mooney, 2005; Nguyen et al, 2009). In order to
effectively use convolution kernels, it is often nec-
essary to choose appropriate substructures of a sen-
tence rather than represent the sentence as a whole
structure (Bunescu and Mooney, 2005; Zhang et al,
2006; Moschitti, 2008). As for tree kernels, for ex-
ample, one typically chooses the syntactic subtree
immediately enclosing two entities potentially ex-
pressing a specific relation in a given sentence. The
opinion holder detection task is different from this
scenario. There can be several cues within a sen-
tence to indicate the presence of a genuine opinion
holder and these cues need not be member of a par-
ticular word group, e.g. they can be opinion words
(see Sentences 1-3), communication words, such as
maintained in Sentence 2, or other lexical cues, such
as according in Sentence 3.
1. The U.S. commanders consideropinion the prisoners to be un-
lawful combatantsopinion as opposed to prisoners of war.
2. During the summit, Koizumi maintainedcommunication a
clear-cut collaborative stanceopinion towards the U.S. and em-
phasized that the President was objectiveopinion and circum-
spect.
3. Accordingcue to Fernandez, it was the worst mistakeopinion in
the history of the Argentine economy.
795
Thus, the definition of boundaries of the structures
for the convolution kernels is less straightforward in
opinion holder extraction.
The aim of this paper is to explore in how far convo-
lution kernels can be beneficial for effective opinion
holder detection. We are not only interested in how
far different kernel types contribute to this extraction
task but we also contrast the performance of these
kernels with a manually designed feature set used
as a standard vector kernel. Finally, we also exam-
ine the effectiveness of expanding word sequences
or syntactic trees by additional prior knowledge.
2 Related Work
Choi et al (2005) examine opinion holder extraction
using CRFs with various manually defined linguis-
tic features and patterns automatically learnt by the
AutoSlog system (Riloff, 1996). The linguistic fea-
tures focus on named-entity information and syntac-
tic relations to opinion words. In this paper, we use
very similar settings. The features presented in Kim
and Hovy (2005) and Bloom et al (2007) resemble
very much Choi et al (2005). Bloom et al (2007)
also consider communication words to be predictive
cues for opinion holders.
Kim and Hovy (2006) and Bethard et al (2005) ex-
plore the usefulness of semantic roles provided by
FrameNet (Fillmore et al, 2003) for both opinion
holder and opinion target extraction. Due to data
sparseness, Kim and Hovy (2006) expand FrameNet
data by using an unsupervised clustering algorithm.
Choi et al (2006) is an extension of Choi et al
(2005) in that opinion holder extraction is learnt
jointly with opinion detection. This requires that
opinion expressions and their relations to opinion
holders are annotated in the training data. Seman-
tic roles are also taken as a potential source of in-
formation. In our work, we deliberately work with
minimal annotation and, thus, do not consider any
labeled opinion expressions and relations to opinion
holders in the training data. We exclusively rely on
entities marked as opinion holders. In many practi-
cal situations, the annotation beyond opinion holder
labeling is too expensive.
Complex convolution kernels have been success-
fully applied to various NLP tasks, such as rela-
tion extraction (Bunescu and Mooney, 2005; Zhang
et al, 2006; Nguyen et al, 2009), question an-
swering (Zhang and Lee, 2003; Moschitti, 2008),
and semantic role labeling (Moschitti et al, 2008).
In all these tasks, they offer competitive perfor-
mance to manually designed feature sets. Bunescu
and Mooney (2005) combine different sequence ker-
nels encoding different contexts of candidate en-
tities in a sentence. They argue that several ker-
nels encoding different contexts are more effective
than just using one kernel with one specific context.
We build on that idea and compare various scopes
eligible for opinion holder extraction. Moschitti
(2008) and Nguyen et al (2009) suggest that differ-
ent kinds of information, such as word sequences,
part-of-speech tags, syntactic and semantic informa-
tion should be contained in separate convolution ker-
nels. We also adhere to this notion.
3 Data
As labeled data, we use the sentiment annotation of
the MPQA 2.0 corpus1. Opinion holders are not ex-
plicitly labeled as such. However sources of pri-
vate states and subjective speech events (Wiebe et
al., 2003) are a fairly good approximation of the
task. Previous work (Choi et al, 2005; Kim and
Hovy, 2005; Choi et al, 2006) uses similar approxi-
mations.
4 Method
In this work, we consider all noun phrases (NPs)
as possible candidate opinion holders. Therefore,
the set of all data instances is the set of the NPs
within the MPQA 2.0 corpus. Each NP is labeled
as to whether it is a genuine opinion holder or not.
Throughout this section, we will use Sentence 2
from Section 1 as an example.
4.1 The Different Levels of Representation
Several levels of representation are important for
opinion holder extraction. Table 1 lists all the dif-
ferent levels that are used in this work. Generalized
sequences employ named-entity tags, an OPINION
tag for opinion words and a COMM tag for com-
munication words2. Thus, in a generalized word se-
1www.cs.pitt.edu/mpqa/databaserelease
2Note that all candidate tokens are reduced to one generic
CAND token. Thus, we hope to account for data sparseness in
796
quence (WRDGN ) a word is replaced by a general-
ized token whereas in a generalized part-of-speech
sequence (POSGN ) a part-of-speech tag is replaced.
For augmented constituent trees (CONSTAUG), the
same sources of information are used. The differ-
ence to generalizing sequences is that instead of re-
placing words by generalized tokens, we add a node
in the syntax tree with a generalized token so that it
dominates the pertaining leaf node (see also nodes
marked with AUG in Figure 2). All sources used for
this type of generalization are known to be predictive
for opinion holder classification (Choi et al, 2005;
Kim and Hovy, 2005; Choi et al, 2006; Kim and
Hovy, 2006; Bloom et al, 2007).
Note that the grammatical relation paths, i.e.
GRAMWRD and GRAMPOS , can only be applied
in case there is another expression in the focus in
addition to the candidate of the data instance itself,
e.g. the nearest opinion expression to the candidate.
Section 4.4 explains in detail how this is done.
Predicate-argument structures (PAS) are repre-
sented by PropBank trees (Kingsbury and Palmer,
2002).
4.2 Support Vector Machines and Kernel
Methods
Support Vector Machines (SVMs) are one of the
most robust supervised machine learning techniques
in which training data instances ~x are separated by a
hyperplane H(~x) = ~w ? ~x + b = 0 where w ? Rn
and b ? R. One advantage of SVMs is that ker-
nel methods can be applied which map the data to
other feature spaces in which they can be separated
more easily. Given a feature function ? : O ? R,
where O is the set of the objects, the kernel trick
allows the decision hyperplane to be rewritten as:
H(~x) =
(
?
i=1...l
yi?i~xi
)
? ~x + b =
?
i=1...l
yi?i~xi ? ~x+ b =
?
i=1...l
yi?i? (oi) ? ? (o) + b
where yi is equal to 1 for positive and ?1 for
negative examples, ?i ? R with ?i ? 0, oi?i ?
{1, . . . , l} are the training instances and the product
K(oi, o) = ??(oi) ? ?(o)? is the kernel function as-
sociated with the mapping ?.
case there are several tokens making up the candidate.
4.3 Sequence and Tree Kernels
A sequence kernel (SK) measures the similarity
of two sequences by counting the number of com-
mon subsequences. We use the kernel by Taylor
and Christianini (2004) which has the advantage that
it also considers subsequences of the original se-
quence with some elements missing. The extent of
these gaps in a sequence is suitably reflected by a
weighting function incorporated into the kernel.
Tree kernels (TKs) represent trees by their sub-
structures. The feature space of these substructures,
or fragments, is mapped onto a vector space. The
kernel function computes the similarity of pairs of
trees by counting the number of common fragments.
In this work, we evaluate two tree kernels: Subset
Tree Kernel (STK) (Collins and Duffy, 2002) and
Partial Tree Kernel (PTKbasic) (Moschitti, 2006).
In STK , a tree fragment can be any set of nodes
and edges of the original tree provided that every
node has either all or none of its children. This con-
straint makes that kind of kernel well-suited for con-
stituency trees which have been generated by con-
text free grammars since the constraint corresponds
to the restriction that no grammatical rule must be
broken. For example, STK enforces that a subtree,
such as [VP [VBZ, NP]], cannot be matched with
[VP [VBZ]] since the latter VP node only possesses
one of the children of the former.
PTKbasic is more flexible since the constraint
of STK on nodes is relaxed. This makes this
type of tree kernel less suitable for constituency
trees. We, therefore, apply it only to trees
representing predicate-argument structures (PAS)
(see Figure 1). Note that a data instance is
represented by a set of those structures3 rather
than a single structure. Thus, the actual partial
tree kernel function we use for this task, PTK ,
sums over all possible pairs PASl and PASm of
two data instances xi and xj: PTK(xi, xj) =
?
PASl?xi
?
PASm?xj
PTKbasic(PASl, PASm).
To summarize, Table 2 lists the different kernel
types we use coupled with the suitable levels of rep-
resentation. This choice of pairing has already been
motivated and empirically proven suitable on other
3i.e. all predicate-argument structures of a sentence in which
the head of the candidate opinion holder occurs
797
Type Description Example
WRD sequence of words During the summit , KoizumiCAND maintained a clear-cut
collaborative stance . . .
WRDGN sequence of generalized words During the summit , CAND COMM OPINION . . .
POS part-of-speech sequence IN DET NN PUNC CAND VBD DET JJ JJ NN . . .
POSGN generalized part-of-speech sequence IN DET NN PUNC CAND COMM OPINION . . .
CONST constituency tree see Figure 2 without nodes marked AUG
CONSTAUG augmented constituency tree see Figure 2
GRAMWRD grammatical relation path labels with words KoizumiCAND NSUBJ? maintained DOBJ? stance
GRAMPOS grammatical relation path labels with part-of-speech tags CAND NSUBJ? VBD DOBJ? NN
PAS predicate argument structures see Figure 1(a)
PASAUG augmented predicate argument structures see Figure 1(b)
Table 1: The different levels of representation.
(a) plain
(b) augmented
Figure 1: Predicate-argument structures (PAS).
tasks (Moschitti, 2008; Nguyen et al, 2009).
Type Description Levels of Representation
SK Sequential Kernel WRD(GN) , POS(GN),
GRAMWRD , GRAMPOS
STK Subset Tree Kernel CONST(AUG)
PTK Partial Tree Kernel PAS
V K Vector Kernel not restricted
Table 2: The different types of kernels.
4.4 The Different Scopes
We argue that using the entire word sequence or syn-
tax tree of the sentence in which a candidate opinion
holder is situated to represent a data instance pro-
duces too large structures for a convolution kernel.
Since a classifier based on convolution kernels has
to derive meaningful features by itself, the larger
these structures are, the more likely noise is included
in the model. Previous work in relation extraction
has also shown that the usage of more focused sub-
structures, e.g. the smallest subtree containing the
two candidate entities of a relation, is more effec-
tive (Zhang et al, 2006). Unfortunately, in our task
there is only one explicit entity we know of for each
data instance which is the candidate opinion holder.
However, there are several indicative cues within the
context of the candidate which might be considered
important. We identify three different cues being the
nearest predicate, i.e. full verb or nominalization,
opinion word and communication word4. For each
of these expressions, we define a scope where the
boundaries are the candidate opinion holder and the
pertaining cue. Given these scopes, we can define
resulting subsequences/subtrees and combine them.
We further add two background scopes, one being
the semantic scope of the candidate opinion holder
and the entire sentence. As semantic scope we con-
sider the subclause in which a candidate opinion
holder is situated5 .
Figure 2 illustrates the different scopes. Abbre-
viations are explained in Table 3. As already men-
tioned in Section 4.1 for grammatical relation paths,
a second expression in addition to the candidate
opinion holder is required. These expressions can be
derived from the different scopes, i.e. for PRED it
4These three expressions may coincide but do not have to.
5Typically, the subtree representing a subclause has the clos-
est S node dominating the candidate opinion holder as the root
node and it contains only those nodes from the original sentence
parse which are also dominated by that S node and whose path
to that node does not contain another S node.
798
is the nearest predicate to the candidate, for OP it is
the nearest opinion word and for COMM it is the
nearest communication word. For the background
scopes SEM and SENT , however, there is no sec-
ond expression in focus. Therefore, grammatical re-
lation paths cannot be defined for these scopes.
Type Description
PRED scope with the boundaries being the candidate opinion
holder and the nearest predicate
OP scope with the boundaries being the candidate opinion
holder and nearest opinion word
COMM scope with the boundaries being the candidate opinion
holder and the nearest communication word
SEM semantic scope of the candidate opinion holder, i.e.
subclause containing the candidate
SENT entire sentence in which in the opinion holder occurs
Table 3: The different types of scope.
4.5 Manually Designed Feature Set for a
Standard Vector Kernel
In addition to the different types of convolution ker-
nels, we also define an explicit feature set for a vec-
tor kernel (V K). Many of these features mainly de-
scribe properties of the relation between the candi-
date and the nearest predicate6 since in our initial
experiments the nearest predicate has always been
the strongest cue. Adding these types of features
for other cues, e.g. the nearest opinion or commu-
nication word, only resulted in a decrease in perfor-
mance. Table 4 lists all the features we use. Note
that this manual feature set employs all those sources
of information which are also exploited by the con-
volution kernels. Some of the information contained
in the convolution kernels can, however, only be rep-
resented in a more simplified fashion when using
a manual feature set. For example, the first PAS
in Figure 1(a) is converted to just the pair of pred-
icate and argument representing the candidate (i.e.
REL:maintain A0:Koizumi). The entire PAS is not
used since it would create too sparse features. Con-
volution kernels can cope with fairly complex struc-
tures as input since they internally match substruc-
tures. Manual features are less flexible since they do
not account for partial matches.
6We select the nearest predicate by using the syntactic parse
tree. Thus, we hope to select the predicate which syntactically
headword/governing category of CAND
is CAND capitalized/a person?
is CAND subj|dobj|iobj|pobj of OPINION/COMM?
is CAND preceded by according to? (Choi et al, 2005)
does CAND contain possessive and is followed by OPIN-
ION/COMM? (Choi et al, 2005)
is CAND preceded by by which is attached to OPINION/COMM?
(Choi et al, 2005)
predicate-argument pairs in which CAND occurs
lemma/part-of-speech tag/subcategorization frame/voice of nearest
predicate
is nearest predicate OPINION/COMM?
does CAND precede/follow nearest predicate?
words between nearest predicate and CAND (bag of words)
part-of-speech sequence between nearest predicate and CAND
constituency path/grammatical relation path from predicate to
CAND
Table 4: Manually designed feature set.
5 Experiments
We used 400 documents of the MPQA corpus for
five-fold crossvalidation and 133 documents as a de-
velopment set. We report statistical significance on
the basis of a paired t-test using 0.05 as the signif-
icance level. All experiments were done with the
SVM-Light-TK toolkit7. We evaluated on the basis
of exact phrase matching. We set the trade-off pa-
rameter j = 5 for all feature sets. For the manual
feature set we used a polynomial kernel of third de-
gree. These two critical parameters were tuned on
the development set. As far as the sequence and
tree kernels are concerned, we used the parameter
settings from Moschitti (2008), i.e. ? = 0.4 and
? = 0.4. Kernels were combined using plain sum-
mation. The documents were parsed using the Stan-
ford Parser (Klein and Manning, 2003). Named-
entity information was obtained by the Stanford tag-
ger (Finkel et al, 2005). Semantic roles were ob-
tained by using the parser by Zhang et al (2008).
Opinion expressions were identified using the Sub-
jectivity Lexicon from the MPQA project (Wil-
son et al, 2005). Communication words were ob-
tained by using the Appraisal Lexicon (Bloom et al,
2007). Nominalizations were recognized by looking
relates to the candidate opinion holder.
7available at disi.unitn.it/moschitti
799
Figure 2: Illustration of the different scopes on a CONSTAUG; nodes belonging to the candidate opinion holder are
marked with CAND.
up nouns in NOMLEX (Macleod et al, 1998).
5.1 Notation
Each kernel is represented as a triple
?levelOfRepresentation (Table 1), Scope (Table 3), typeOfKernel
(Table 2)?, e.g. ?CONST, SENT, STK? is a Subset
Tree Kernel of a constituency parse having the
scope of the entire sentence. Note that not all com-
binations of these three parameters are meaningful.
In the following, we will just focus on important
and effective combinations. The kernel composed
of manually designed features is denoted by just
V K . The kernel composed of predicate-argument
structures is denoted by ?PAS, SENT,PTK?.
5.2 Vector Kernel (VK)
The first line in Table 7 displays the result of the
vector kernel using a manually designed feature set.
It should be interpreted as a baseline. Due to the
high class imbalance we will focus on the compari-
son of F(1)-Score throughout this paper rather than
accuracy which is fairly biased on this data set. The
F-Score of this classifier is at 56.16%.
5.3 Sequence Kernels (SKs)
For both sequence and tree kernels we need to find
out what the best scope is, whether it is worthwhile
to combine different scopes and what different lay-
ers of representation can be usefully combined.
The upper part of Table 5 lists the results of simple
word kernels using the different scopes. The perfor-
mance of the kernels using individual scopes varies
greatly. The best scope is PRED (1), the second
best is SEM (2). The good performance of PRED
does not come as a surprise since the sequence is the
smallest among the different scopes, so this scope is
least affected by data sparseness. Moreover, this re-
sult is consistent with our initial experiments on the
manual feature set (see Section 4.5).
Using different combinations of the word se-
quence kernels shows that PRED and SEM (6)
are a good combination, whereas OP , COMM ,
and SENT (7;8;9) do not positively contribute to
the overall performance which is consistent which
the individual scope evaluation. Apparently, these
scopes capture less linguistically relevant structure.
The next part of Table 5 shows the contribution of
POS kernels when added to WRD kernels. Adding
the corresponding POS kernel to the WRD kernel
with PRED scope (10) results in an improvement
by more than 5% in F-Score. We get another im-
provement by approx. 3% when the corresponding
SEM kernels (11) are added. This suggests that
POS is an effective generalization and that the two
scopes PRED and SEM are complementary.
For the GRAMWRD kernel, the PRED scope
(12) is again most effective. We assume that this ker-
nel most likely expresses meaningful syntactic rela-
tionships for our task. Adding the GRAMPOS ker-
nel (14) gives another boost by almost 4%.
Generalized sequence kernels are important.
800
Adding the corresponding WRDGN kernels to the
WRD kernel with PRED and SEM scope results
in an improvement from 47.77% (1) to 53.00% (15)
which is a bit less than the combination of WRD
and POS(GN) kernels (16). However, these types of
kernels seem to be complementary since their com-
bination provides an F-Score of 56.06% (17). This
kernel combination already performs on a par with
the manually designed vector kernel though less in-
formation is taken into consideration.
Finally, the best combination of sequence ker-
nels (18) comprises WRD, WRDGN , POS, and
POSGN kernels with PRED and SEM scope
combined with a GRAMWRD and a GRAMPOS
kernel with PRED scope. The performance of
58.70% significantly outperforms the vector kernel.
5.4 Tree Kernels (TKs)
Table 6 shows the results of the different tree ker-
nels. The table is divided into two halves. The
left half (A) are plain tree kernels, whereas the right
half (B) are the augmented tree kernels. As far as
CONST kernels are concerned, there is a system-
atic improvement by approximately 2% using tree
augmentation. This proves that further non-syntactic
knowledge added to the tree itself results in an im-
proved F-Score. However, tree augmentation does
not have any impact on the PAS kernels.
The overall performance of the tree kernels shows
that they are much more expressive than sequence
kernels. For instance, in order to obtain the same
performance as of ?CONSTAUG, PRED,STK?
(19B), i.e. a single kernel with an F-Score 56.52, it
requires several sequence kernels, hence much more
effort. The performance of the different CONST
kernels relative to each other resembles the results
of the WRD kernels. The best scope is PRED
(19). By far the worst performance is obtained by
the SENT scope (23). The combination of PRED
and SEM scope achieves an F-Score of 59.67%
(25B) which is already slightly better than the best
configuration of sequence kernels (18).
The performance of the PAS kernel (28A) with
an F-Score of 53.51% is slightly worse than the best
single plain CONST kernel (19A). The PAS ker-
nel and the CONST kernels are complementary,
since their best combination (29B) achieves an F-
Score of 61.67% which is significantly better than
Combination Acc. Prec. Rec. F1
VK 93.63 53.28 59.37 56.16
best SKs 94.21 57.64 59.81 58.70
best TKs 94.16 56.18 68.36 61.67?
VK + best SKs 94.34 58.44 61.27 59.82?
VK + best TKs 94.33 57.41 68.03 62.27?
best SKs + best TKs 94.49 59.22 63.96 61.49?
VK + best SKs + best TKs 94.53 59.10 66.57 62.61??
Table 7: Results of kernel combinations (?: significantly
better than best SKs; ?: significantly better than best TKs;
all convolution kernels are significantly better than VK).
the best combination of CONST kernels (25B) or
sequence kernels (18).
5.5 Combinations
Table 7 lists the results of the different kernel type
combinations. If VK is added to the best TKs, the
best SKs, or both, a slight increase in F-Score is
achieved. The best performance with an F-Score of
62.61% is obtained by combining all kernels.
6 Conclusion
In this paper, we compared convolution kernels for
opinion holder extraction. We showed that, in gen-
eral, a combination of two scopes, namely the scope
immediately encompassing the candidate opinion
holder and its nearest predicate and the subclause
containing the candidate opinion holder provide best
performance. Tree kernels containing constituency
parse information and semantic roles achieve better
performance than sequence kernels or vector kernels
using a manually designed feature set. Best perfor-
mance is achieved if all kernels are combined.
Acknowledgements
Michael Wiegand was funded by the German research
council DFG through the International Research Training
Group ?IRTG? between Saarland University and Univer-
sity of Edinburgh.
The authors would like to thank Yi Zhang for pro-
cessing the MPQA corpus with his semantic-role label-
ing system, the researchers from the MPQA project for
helping to create an opinion holder corpus, and, in partic-
ular, Alessandro Moschitti for insightful comments and
suggestions.
801
ID Kernel Acc. Prec. Rec. F1
1 ?WRD, PRED, SK? 93.25 51.08 42.29 46.26
2 ?WRD, OP, SK? 92.77 46.38 32.52 38.21
3 ?WRD, COMM, SK? 92.42 43.70 35.99 39.46
4 ?WRD, SEM,SK? 93.16 50.32 34.65 41.04
5 ?WRD, SENT, SK? 90.60 29.90 27.29 28.53
6 ?WRD, PRED, SK? + ?WRD, SEM,SK? 93.78 56.55 41.36 47.77
7
P
j?{PRED,OP,COMM}?WRD, j,SK? 93.55 54.26 39.50 45.71
8
P
j?Scopes\SENT ?WRD, j, SK? 93.82 57.21 40.28 47.26
9
P
j?Scopes?WRD, j, SK? 93.63 55.15 39.52 46.03
10 ?WRD, PRED, SK? + ?POS, PRED, SK? 93.03 49.39 53.53 51.37
11
P
i?{PRED,SEM} (?WRD, i, SK? + ?POS, i, SK?) 93.86 55.60 53.22 54.38
12
P
i?{PRED,SEM}?WRD, i, SK? + ?GRAMWRD , PRED, SK? 94.01 58.19 45.88 51.29
13
P
i?{PRED,SEM}?WRD, i, SK? +
P
j?{PRED,OP,COMM}?GRAMWRD , j, SK? 93.83 56.28 45.64 50.40
14
X
i?{PRED,SEM}
?WRD, i, SK?+?GRAMWRD, PRED, SK?+?GRAMPOS, PRED, SK? 93.98 56.59 53.92 55.21
15
P
i?{PRED,SEM} (?WRD, i, SK? + ?WRDGN , i, SK?) 93.97 57.08 49.46 53.00
16
P
i?{PRED,SEM} (?WRD, i, SK? + ?POSGN , i, SK?) 93.97 56.60 52.42 54.42
17
X
i?{PRED,SEM}
(?WRD, i, SK? + ?WRDGN , i, SK? + ?POS, i, SK? + ?POSGN , i, SK?) 93.85 55.16 57.00 56.06
18
X
i?{PRED,SEM}
(?WRD, i, SK? + ?WRDGN , i, SK? + ?POS, i, SK? + ?POSGN , i, SK?) 94.21 57.64 59.81 58.70
+?GRAMWRD , PRED, SK? + ?GRAMPOS , PRED, SK?
Table 5: Results of the different sequence kernels.
A B
i = CONST, j = PAS i = CONSTAUG, j = PASAUG
ID Kernel Acc. Prec. Rec. F1 Acc. Prec. Rec. F1
19 ?i, PRED, STK? 92.89 48.68 62.34 54.67 93.12 49.99 65.04 56.52
20 ?i, OP,STK? 93.04 49.49 54.71 51.96 93.27 50.93 59.06 54.68
21 ?i, COMM,STK? 92.76 47.79 55.89 51.50 92.96 49.03 58.85 53.47
22 ?i, SEM,STK? 93.70 54.40 52.13 53.23 93.90 55.47 56.59 56.03
23 ?i, SENT,STK? 92.42 44.34 39.92 41.99 92.50 45.20 42.40 43.74
24
P
k?{PRED,OP,COMM}?i, k, STK? 93.62 53.26 60.05 56.44 93.77 54.06 63.21 58.26
25
P
k?{PRED,SEM}?i, k, STK? 93.90 55.26 59.50 57.30 94.13 56.57 63.12 59.67
26
P
k?Scopes\SENT ?i, k, STK? 94.09 56.65 59.68 58.11 94.21 57.21 62.61 59.80
27
P
k?Scopes?i, k, STK? 94.14 57.41 57.88 57.63 94.29 58.11 61.10 59.56
28 ?j, SENT, PTK? 92.11 45.02 69.96 53.51 91.92 44.27 67.39 53.43
29
X
k?{PRED,SEM}
?i, k, STK?+?PAS,SENT, PTK? 94.05 55.68 66.01 60.40 94.16 56.18 68.36 61.67
30
X
k?Scopes\SENT
?i, k, STK? + ?PAS,SENT, PTK? 94.30 57.95 62.62 60.19 94.36 58.07 64.94 61.31
Table 6: Results of the different tree kernels.
802
References
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2005. Extracting
Opinion Propositions and Opinion Holders using Syn-
tactic and Lexical Cues. In Computing Attitude and
Affect in Text: Theory and Applications. Springer.
Kenneth Bloom, Sterling Stein, and Shlomo Argamon.
2007. Appraisal Extraction for News Opinion Analy-
sis at NTCIR-6. In Proceedings of NTCIR-6 Workshop
Meeting, Tokyo, Japan.
Razvan C. Bunescu and Raymond J. Mooney. 2005.
Subsequence Kernels for Relation Extraction. In Pro-
ceedings of the Conference on Neural Information
Processing Systems (NIPS), Vancouver, Canada.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying Sources of Opinions
with Conditional Random Fields and Extraction Pat-
terns. In Proceedings of the Conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing (HLT/EMNLP), Vancouver,
Canada.
Yejin Choi, Eric Breck, and Claire Cardie. 2006.
Joint Extraction of Entities and Relations for Opin-
ion Recognition. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Sydney, Australia.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging. In Proceedings
of the Annual Meeting of the Association for Compu-
tational Linguistics (ACL), Philadelphia, USA.
Charles. J. Fillmore, Christopher R. Johnson, and
Miriam R. Petruck. 2003. Background to FrameNet.
International Journal of Lexicography, 16:235 ? 250.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Information
into Information Extraction Systems by Gibbs Sam-
pling. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL), Ann
Arbor, USA.
Soo-Min Kim and Eduard Hovy. 2005. Identifying
Opinion Holders for Question Answering in Opin-
ion Texts. In Proceedings of AAAI-05 Workshop
on Question Answering in Restricted Domains, Pitts-
burgh, USA.
Soo-Min Kim and Eduard Hovy. 2006. Extracting Opin-
ions, Opinion Holders, and Topics Expressed in On-
line News Media Text. In Proceedings of the ACL
Workshop on Sentiment and Subjectivity in Text, Syd-
ney, Australia.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
Bank to PropBank. In Proceedings of the 3rd Confer-
ence on Language Resources and Evaluation (LREC),
Las Palmas, Spain.
Dan Klein and Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics (ACL), Sapporo, Japan.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. NOMLEX:
A Lexicon of Nominalizations. In Proceedings of EU-
RALEX, Lie`ge, Belgium.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree Kernels for Semantic Role Label-
ing. Computational Linguistics, 34(2):193 ? 224.
Alessandro Moschitti. 2006. Efficient Convolution Ker-
nels for Dependency and Constituent Syntactic Trees.
In Proceedings of the 17th European Conference on
Machine Learning (ECML), Berlin, Germany.
Alessandro Moschitti. 2008. Kernel Methods, Syn-
tax and Semantics for Relational Text Categorization.
In Proceedings of the Conference on Information and
Knowledge Management (CIKM), Napa Valley, USA.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution Kernels on
Constituent, Dependency and Sequential Structures
for Relation Extraction. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP), Singapore.
Ellen Riloff. 1996. An Empirical Study of Automated
Dictionary Construction for Information Extraction.
Artificial Intelligence, 85.
John Taylor and Nello Christianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2003.
Annotating Expressions of Opinions and Emotions in
Language. Language Resources and Evaluation, 1:2.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
level Sentiment Analysis. In Proceedings of Hu-
man Language Technologies Conference/Conference
on Empirical Methods in Natural Language Process-
ing (HLT/EMNLP), Vancouver, Canada.
Dell Zhang and Wee Sun Lee. 2003. Question Classifi-
cation using Support Vector Machines. In Proceedings
of the ACM Special Interest Group on Information Re-
trieval (SIGIR), Toronto, Canada.
Min Zhang, Jie Zhang, and Jian Su. 2006. Explor-
ing Syntactic Features for Relation Extraction using a
Convolution Tree Kernel. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL (HLT/NAACL), New
York City, USA.
Yi Zhang, Rui Wang, and Hans Uszkoreit. 2008. Hy-
brid Learning of Dependency Structures from Het-
erogeneous Linguistic Resources. In Proceedings of
the Conference on Computational Natural Language
Learning (CoNLL), Manchester, United Kingdom.
803
