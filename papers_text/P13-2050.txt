Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 278?282,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Building Comparable Corpora Based on Bilingual LDA Model 
 Zede Zhu 
University of Science and Technology 
of China, Institute of Intelligent Ma-
chines Chinese Academy of Sciences 
Hefei, China 
zhuzede@mail.ustc.edu.cn 
Miao Li, Lei Chen, Zhenxin Yang 
Institute of Intelligent Machines Chinese 
Academy of Sciences 
Hefei, China 
mli@iim.ac.cn,alan.cl@163.com, 
xinzyang@mail.ustc.edu.cn 
 
  
Abstract 
Comparable corpora are important basic re-
sources in cross-language information pro-
cessing. However, the existing methods of 
building comparable corpora, which use inter-
translate words and relative features, cannot 
evaluate the topical relation between document 
pairs. This paper adopts the bilingual LDA 
model to predict the topical structures of the 
documents and proposes three algorithms of 
document similarity in different languages. 
Experiments show that the novel method can 
obtain similar documents with consistent top-
ics own better adaptability and stability per-
formance.  
1 Introduction 
Comparable corpora can be mined fine-grained 
translation equivalents, such as bilingual termi-
nologies, named entities and parallel sentences, 
to support the bilingual lexicography, statistical 
machine translation and cross-language infor-
mation retrieval (AbduI-Rauf et al, 2009). Com-
parable corpora are defined as pairs of monolin-
gual corpora selected according to the criteria of 
content similarity but non-direct translation in 
different languages, which reduces limitation of 
matching source language and target language 
documents. Thus comparable corpora have the 
advantage over parallel corpora in which they are 
more up-to-date, abundant and accessible (Ji, 
2009). 
Many works, which focused on the exploita-
tion of building comparable corpora, were pro-
posed in the past years. Tao et al (2005) ac-
quired comparable corpora based on the truth 
that terms are inter-translation in different lan-
guages if they have similar frequency correlation 
at the same time periods. Talvensaari et al (2007) 
extracted appropriate keywords from the source 
language documents and translated them into the 
target language, which were regarded as the que-
ry words to retrieve similar target documents. 
Thuy et al (2009) analyzed document similarity 
based on the publication dates, linguistic inde-
pendent units, bilingual dictionaries and word 
frequency distributions. Otero et al (2010) took 
advantage of the translation equivalents inserted 
in Wikipedia by means of interlanguage links to 
extract similar articles. Bo et al (2010) proposed 
a comparability measure based on the expecta-
tion of finding the translation for each word.  
The above studies rely on the high coverage of 
the original bilingual knowledge and a specific 
data source together with the translation vocabu-
laries, co-occurrence information and language 
links. However, the severest problem is that they 
cannot understand semantic information. The 
new studies seek to match similar documents on 
topic level to solve the traditional problems. Pre-
iss (2012) transformed the source language topi-
cal model to the target language and classified 
probability distribution of topics in the same lan-
guage, whose shortcoming is that the effect of 
model translation seriously hampers the compa-
rable corpora quality. Ni et al (2009) adapted 
monolingual topic model to bilingual topic mod-
el in which the documents of a concept unit in 
different languages were assumed to share iden-
tical topic distribution. Bilingual topic model is 
widely adopted to mine translation equivalents 
from multi-language documents (Mimno et al, 
2009; Ivan et al, 2011).  
Based on the bilingual topic model, this paper 
predicts the topical structure of documents in 
different languages and calculates the similarity 
of topics over documents to build comparable 
corpora. The paper concretely includes: 1) Intro-
duce the Bilingual LDA (Latent Dirichlet Alloca-
tion) model  which builds comparable corpora 
and improves the efficiency of matching similar 
documents; 2) Design a novel method of TFIDF 
(Topic Frequency-Inverse Document Frequency) 
to enhance the distinguishing ability of topics 
from different documents; 3) Propose a tailored 
278
method of conditional probability to calculate 
document similarity; 4) Address a language-
independent study which isn?t limited to a par-
ticular data source in any language. 
2 Bilingual LDA Model 
2.1 Standard LDA 
LDA model (Blei et al, 2003) represents the la-
tent topic of the document distribution by Di-
richlet distribution with a K-dimensional implicit 
random variable, which is transformed into a 
complete generative model when ?  is exerted to 
Dirichlet distribution (Griffiths et al, 2004) 
(Shown in Fig. 1), 
? m? ,m n? ,m n? ?
[1, ]mn N?
[1, ]m M?
k?
[1, ]k K?  
Figure 1: Standard LDA model 
where ? and ?  denote the parameters distribut-
ed by Dirichlet; K denotes the topic numbers; k?  denotes the vocabulary probability distribution in 
the topic k; M denotes the document number; m?  denotes the topic probability distribution in the 
document m; Nm denotes the length of m; ,m n?  and ?m,n denote the topic and the word in m re-spectively. 
2.2 Bilingual LDA 
Bilingual LDA is a bilingual extension of a 
standard LDA model. It takes advantage of the 
document alignment which shares the same topic 
distribution m?  and uses different word distribu-
tions for each topic (Shown in Fig. 2), where S 
and T denote source language and target lan-
guage respectively.  
?
,
S
m n?
,
T
m n?
,
S
m n?
,
T
m n? Tk?
S
k? S?
m?
[1, ]m M?
[1, ]S Smn N?
[1, ]T Tmn N? [1, ]k K?
T?
 
Figure 2: Bilingual LDA model 
For each language l ( { , }l S T? ), ,lm n? and 
,
l
m n? are drawn using , ( | )l lm n n mP ?? ??  and 
, ,( | , )l l l lm n n m nP? ? ??? . 
Giving the comparable corpora M, the distri-
bution ,k v?  can be obtained by sampling a new 
token as word v from a topic k. For new collec-
tion of documents M? , keeping ,k v? , the distri-
bution ,lm k? ? of sampling a topic k from document 
m?  can be obtained as follows: 
( )
, ( )
1
( | )
( )
l
l
l
k
kl m
k Km k k
kmk
nP m
n
??
?
?
?? ? ?
??
?
?
?
? ,     (1) 
where ( )lkmn?  denotes the total number of times that the document m?  is assigned to the topic k. 
3 Building comparable corpora  
Based on the bilingual LDA model, building 
comparable corpora includes several steps to 
generate the bilingual topic model ,k v?  from the given bilingual corpora, predict the topic distri-bution ,lm k? ? of the new documents, calculate the 
similarity of documents and select the largest 
similar document pairs. The key step is that the 
document similarity is calculated to align the 
source language document Sm?  with relevant 
target language document Tm? .  
As one general way of expressing similarity, 
the Kullback-Leibler (KL) Divergence is adopted 
to measure the document similarity by topic dis-
tributions ? ,Sm k? and ? ,Tm k? as follows: 
? ? ?? ?, , ,1
( , ) [ ( | ), ( | )]
log . (2)S S T
S T S T
KLK
m k m k m kk
Sim m m KL P m P m
? ? ?
?
? ? ?
? ?? ? ?? ??
? ? ? ?
 
The remainder section focuses on other two 
methods of calculating document similarity. 
3.1 Cosine Similarity 
The similarity between Sm? and Tm? can be meas-
ured by Topic Frequency-Inverse Document 
Frequency. It gives high weights to the topic 
which appears frequently in a specific document 
and rarely appears in other documents. Then the 
relation between ,SmTFIDF ?? and ,TmTFIDF ??  is 
measured by Cosine Similarity (CS). 
Similar to Term Frequency-Inverse Document 
Frequency (Manning et al,1999), Topic Fre-
quency (TF) denoting frequency of topic ?  for 
the document lm? is denoted by ( | )lP m? ? . Given 
a constant value? , Inverse Document Frequency 
(IDF) is defined as the total number of docu-
ments M? divided by the number of documents 
279
: ( | )l lm P m ?? ?? ?  containing a particular topic, and then taking the logarithm, which is calculat-
ed as follows: 
log1 : ( | )l l
MIDF m P m ?? ? ? ?
?
? ? .      (3) 
The TFIDF is calculated as follows: 
*
( | ) log1 : ( | )
l
l l
TFIDF TF IDF
MP m m P m ?
?
? ? ? ? ?
?
? ? ?
.   (4) 
Thus, the TFIDF score of the topic k over 
document lm?  is given by: 
,
,
,
( | ) log 1 : ( | )
log . (5)1 :
l
l
l
m k
l
k l l
k
m k l
m k
TFIDF
MP m m P m
M
m
?
? ? ?
? ? ? ? ?
? ? ?
?
?
?
?
? ? ?
?
?  
The similarity between Sm? and Tm? is given by: 
, ,
, ,1
2 2
, ,1 1
( , ) ( , )
. (6)
S T
S T
S T
S T
CS m mK
m k m kk
K K
m k m kk k
Sim m m Cos TFIDF TFIDF
TFIDF TFIDF
TFIDF TFIDF
? ?
?
? ?
?
?
?
? ?
? ?
? ?
? ?
? ?
 3.2 Conditional Probability 
The similarity between Sm? and Tm? is defined as 
the Conditional Probability (CP) of documents 
( | )T SP m m? ?  that Tm? will be generated as a re-
sponse to the cue Sm? . 
( )P ?  as prior topic distribution is assumed a 
uniform distribution and satisfied the condition 
( ) ( )kP P? ? ? . According to the total probabil-
ity formula, the document Tm? is given as: 
1
1
( ) ( | ) ( )
( ) ( | ).
KT T
k k
kK T
k
k
P m P m P
P P m
?
?
? ? ?
? ? ?
?
?
? ?
?          (7)
 
Based on the Bayesian formula, the probabil-
ity that a given topic ?  is assigned to a particu-
lar target language document Tm?  is expressed: 
1
( | ) ( | ) ( ) ( )
= ( | ) ( | ).
T T T
KT T
k
k
P m P m P m P
P Z m P m
?
? ?? ? ? ?? ?
??
? ? ?
? ?   (8)
 
The sum of all probabilities 
1
( | )
K T
k
k
P m
?
?? ?
 that all topics ?  are assigned to a particular doc-
ument Tm?  is a constant? , thus equation (8) is 
converted as follows: 
( | ) ( | )T TP m P m? ? ? ?? ? .             (9) 
According to the total probability formula, the 
similarity between Sm? and Tm?  is given by: 
? ?
1
1
, ,1
(10)
( , ) ( | )
[ ( | ) ( | )]
[ ( | ) ( | )]
[ ].S T
S T T S
CPK T S
k k
k K T S
k k
kK
m k m kk
Sim m m P m m
P m P m
P m P m
? ?
?
?
?
?
? ? ?
? ? ? ?
? ?
?
?
?
? ? ? ?
? ?
? ?  
4 Experiments and analysis 
4.1 Datasets and Evaluation 
The experiments are conducted on two sets of 
Chinese-English comparable corpora. The first 
dataset is news corpora with 3254 comparable 
document pairs, from which 200 pairs are ran-
domly selected as the test dataset News-Test and 
the remainder is the training dataset News-Train. 
The second dataset contains 8317 bilingual Wik-
ipedia entry pairs, from which 200 pairs are ran-
domly selected as the test dataset Wiki-Test and 
the remainder is the training dataset Wiki-Train. 
Then News-Train and Wiki-Train are merged 
into the training dataset NW-Train. And the 
hand-labeled gold standard namely NW-Test is 
composed of News-Test and Wiki-Test.  
Braschler et al (1998) used five levels of rele-
vance to assess the alignments as follows: Same 
Story, Related Story, Shared Aspect, Common 
Terminology and Unrelated. The paper selects 
the documents with Same Story and Related Sto-
ry as comparable corpora. Let Cp be the compa-
rable corpora in the building result and Cl be the 
comparable corpora in the labeled result. The 
Precision (P), Recall (R) and F-measure (F) are 
defined as: 
= ,p l p l
lp
C C C CP R CC ?
? ? , 2PRF P R? ? . (11) 
4.2 Results and analysis 
Two groups of validation experiments are set 
with sampling frequency of 1000, parameter ?  
280
of 50/K, parameter ?  of 0.01 and topic number K of 600. 
Group 1: Different data source 
We learn bilingual LDA models by taking differ-
ent training datasets. The performance of three 
approaches (KL, CS and CP) is examined on dif-
ferent test datasets. Tab. 1 demonstrates these 
results with the winners for each algorithm in 
bold. 
Train Test KL CS CP P F P F P F 
News News 0.62 0.52 0.73 0.59 0.69 0.56
News Wiki 0.60 0.47 0.68 0.56 0.66 0.52
Wiki News 0.61 0.48 0.71 0.58 0.68 0.55
Wiki Wiki 0.63 0.50 0.75 0.60 0.71 0.59
NW NW 0.66 0.55 0.76 0.62 0.73 0.60
Table 1: Sensitivity of Data Source 
The results indicate the robustness and effec-
tiveness of these algorithms. The performance of 
algorithms on Wiki-Train is much better than 
News-Train. The main reason is that Wiki-Train 
is an extensive snapshot of human knowledge 
which can cover most topics talked in News-
Train. The probability of vocabularies among the 
test dataset which have not appeared in the train-
ing data is very low. And then the document top-
ic can effectively concentrate all the vocabular-
ies? expressions. The topic model slightly faces 
with the problem of knowledge migration issue, 
so the performance of the topic model trained by 
Wiki-Train shows a slight decline in the experi-
ments on News-Test.  
CS shows the strongest performance among 
the three algorithms to recognize the document 
pairs with similar topics. CP has almost equiva-
lent performance with CS. Comparing the equa-
tion (5) and (6) with (10), we can find out that 
CP is similar to a simplified CS. CP can improve 
the operating efficiency and decrease the perfor-
mance. The performance achieved by KL is the 
weakest and there is a large gap between KL and 
others. In addition, the shortage of KL is that 
when the exchange between the source language 
and the target language documents takes place, 
different evaluations will occur in the same doc-
ument pairs. 
Group 2: Existing Methods Comparison 
We adopt the NW-Train and NW-Test as training 
set and test set respectively, and utilize the CS 
algorithm to calculate the document similarity to 
verify the excellence of methods in the study. 
Then we compare its performance with the exist-
ing representative approaches proposed by Thuy 
et al (2009) and Preiss (2012) (Shown in Tab. 2).  
Algorithm P R F 
Thuy 0.45 0.32 0.37 
Preiss 0.67 0.44 0.53 
CS 0.76 0.53 0.62 
Table 2: Existing Methods Comparison 
The table shows CS outperforms other algo-
rithms, which indicates that bilingual LDA is 
valid to construct comparable corpora. Thuy et al 
(2009) matches similar documents in the view of 
inter-translated vocabulary and co-occurrence 
information features, which cannot understand 
the content effectively. Preiss (2012) uses mono-
lingual training dataset to generate topic model 
and translates source language topic model into 
target language topic model respectively. Yet the 
translation accuracy constrains the matching ef-
fectiveness of similar documents, and the cosine 
similarity is directly used to calculate document-
topic similarity failing to highlight the topic con-
tributions of different documents. 
5 Conclusion  
This study proposes a new method of using bi-
lingual topic to match similar documents. When 
CS is used to match the documents, TFIDF is 
proposed to enhance the topic discrepancies 
among different documents. The method of CP is 
also addressed to measure document similarity. 
Experimental results show that the matching 
algorithm is superior to the existing algorithms. 
It can utilize comprehensively large scales of 
document information in training set to avoid the 
information deficiency of the document itself and 
over-reliance on bilingual knowledge. The algo-
rithm makes the document match on the basis of 
understanding the document. This study does not 
calculate similar contents existed in the monolin-
gual documents. However, a large number of 
documents in the same language describe the 
same event. We intend to incorporate monolin-
gual document similarity into bilingual topics 
analysis to match multi-documents in different 
languages perfectly. 
Acknowledgments 
The work is supported by the National Natural 
Science Foundation of China under No. 
61070099 and the project of MSR-CNIC Win-
dows Azure Theme. 
281
References  
AbduI-Rauf S, Schwenk H. On the use of comparable 
corpora to improve SMT perfor-
mance[C]//Proceedings of the 12th Conference of 
the European Chapter of the Association for Com-
putational Linguistics. Association for Computa-
tional Linguistics, 2009: 16-23. 
Ji H. Mining name translations from comparable cor-
pora by creating bilingual information networks[C] 
// Proceedings of BUCC 2009. Suntec, Singapore, 
2009: 34-37. 
Braschler M, Schauble P. Multilingual Information 
Retrieval based on document alignment tech-
niques[C] // Proceedings of the Second European 
Conference on Research and Advanced Technolo-
gy for Digital Libraries. Heraklion, Greece. 1998: 
183-197. 
Tao Tao, Chengxiang Zhai. Mining comparable bilin-
gual text corpora for cross-language information 
integration[C] // Proceedings of ACM SIGKDD, 
Chicago, Illinois, USA. 2005:691-696. 
Talvensaari T, Laurikkala J, Jarvelin K, et al Creating 
and Exploiting a Comparable Corpus in Cross-
Language Information Retrieval[J]. ACM Transac-
tions on Information Systems. 2007, 25(1): 322-
334. 
Thuy Vu, Ai Ti Aw, Min Zhang. Feature-based meth-
od for document alignment in comparable news 
corpora[C] // Proceedings of the 12th Conference 
of the European Chapter of the ACL, Athens, 
Greece. 2009: 843-851. 
Otero P G, L?opez I G. Wikipedia as Multilingual 
Source of Comparable Corpora[C] // Proceedings 
of the 3rd Workshop on BUCC, LREC2010. Malta. 
2010: 21-25. 
Li B, Gaussier E. Improving corpus comparability for 
bilingual lexicon extraction from comparable cor-
pora[C]//Proceedings of the 23rd International 
Conference on Computational Linguistics. Associ-
ation for Computational Linguistics, 2010: 644-652. 
Judita Preiss. Identifying Comparable Corpora Using 
LDA[C]//2012 Conference of the North American 
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies. Mon-
tre?al, Canada, June 3-8, 2012: 558-562. 
Mimno D, Wallach H, Naradowsky J et al Polylin-
gual topic models[C]//Proceedings of the EMNLP. 
Singapore, 2009: 880-889. 
Vulic I, De Smet W, Moens M F, et al Identifying 
word translations from comparable corpora using 
latent topic models[C]//Proceedings of ACL. 2011: 
479-484. 
Ni X, Sun J T, Hu J, et al Mining multilingual topics 
from wikipedia[C]//Proceedings of the 18th inter-
national conference on World wide web. ACM, 
2009: 1155-1156. 
Blei D M, Ng A Y, Jordan M I. Latent dirichlet alo-
cation[J]. the Journal of machine Learning research, 
2003, 3: 993-1022. 
Griffiths T L, Steyvers M. Finding scientific topics[J]. 
Proceedings of the National academy of Sciences 
of the United States of America, 2004, 101: 5228-
5235. 
Manning C D, Sch?tze H. Foundations of statistical 
natural language processing[M]. MIT press, 1999. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
282
