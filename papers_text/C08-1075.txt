Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 593?600
Manchester, August 2008
Robust Similarity Measures for Named Entities Matching
Erwan Moreau1
Institut Te?le?com ParisTech
& LTCI CNRS
erwan.moreau@enst.fr
Franc?ois Yvon
Univ. Paris Sud
& LIMSI CNRS
yvon@limsi.fr
Olivier Cappe?
Institut Te?le?com ParisTech
& LTCI CNRS
cappe@enst.fr
Abstract
Matching coreferent named entities with-
out prior knowledge requires good similar-
ity measures. Soft-TFIDF is a fine-grained
measure which performs well in this task.
We propose to enhance this kind of met-
rics, through a generic model in which
measures may be mixed, and show experi-
mentally the relevance of this approach.
1 Introduction
In this paper, we study the problem of matching
coreferent named entities (NE in short) in text col-
lections, focusing primarily on orthographic vari-
ations in nominal groups (we do not handle the
case of pronominal references). Identifying textual
variations in entities is useful in many text min-
ing and/or information retrieval tasks (see for ex-
ample (Pouliquen et al, 2006)). As described in
the literature (e.g. (Christen, 2006)), textual dif-
ferences between entities are due to various rea-
sons: typographical errors, names written in dif-
ferent ways (with/without first name/title, etc.),
abbreviations, lack of precision in organization
names, transliterations, etc. For example, one
wants ?Mr. Rumyantsev? to match with ?Alexan-
der Rumyanstev? but not with ?Mr. Ryabev?.
Here we do not address the related problem of dis-
ambiguation2 (e.g. knowing whether a given oc-
currence of ?George Bush? refers to the 41st or
43rd president of the USA), because it is techni-
cally very different from the matching problem.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1Now at LIPN - Univ. Paris 13 & UMR CNRS 7030.
2Which is essential in the Web People Search task.
There are different ways to tackle the problem
of NE matching: the first and certainly most reli-
able one consists in studying the specific features
of the data, and then use any available tool to de-
sign a specialized method for the matching task.
This approach will generally take advantage of
language-specific (e.g. in (Freeman et al, 2006))
and domain-specific knowledge, of any external
resources (e.g. database, names dictionaries, etc.),
and of any information about the entities to pro-
cess, e.g. their type (person name, organization,
etc.), or internal structure (e.g. in (Prager et al,
2007)). In such an in-depth approach, supervised
learning is helpful: it has been used for example
in a database context3 in (Bilenko et al, 2003), but
this approach requires labeled data which is usu-
ally costly. All those data specific appproaches
would necessitate some sort of human expertise.
The second approach is the robust one: we
propose here to try to match any kind of NE,
extracted from ?real world? (potentially noisy)
sources, without any kind of prior knowledge4.
One looks for coreferent NE, whatever their type,
source, language5 or quality6. Such robust simi-
larity methods may be useful for a lot of generic
tasks, in which maximum accuracy is not the main
criterion, or simply where the required resources
are not available.
The literature on string comparison metrics is
abundant, containing both general techniques and
3The matching task is quite different in this framework,
because one observes records (structured information).
4In this kind of knowledge are included the need for hand-
tuning parameters or defining specific thresholds.
5Actually we have only studied English and French (our
approach is neither ?multilingual?, in the sense that it is not
specific to multilingual documents).
6In particular, this task clearly depends on the NE recog-
nition step, which may introduce errors.
593
more linguistically motivated measures, see e.g.
(Cohen et al, 2003) for a review. From a bird?s eye
view, these measures can be sorted in two classes:
?Sequential character-based methods? and ?Bag-
of-words methods?7. Both classes show relevant
results, but do not capture the same kind of simi-
larity. In a robust approach for NE matching, one
needs a more fine-grained method, which performs
at least as well as bag-of-words methods, without
ignoring coreferent pairs that such methods miss.
A first attempt in this direction was introduced
in (Cohen et al, 2003), in the form of a measure
called Soft-TFIDF. We will show that this measure
has theoretical pitfalls and a few practical draw-
backs. Nevertheless, Soft-TFIDF outperforms the
better standard string similarity measures in the
NE matching task. That is why we propose to gen-
eralize and improve its principle, and show exper-
imentally that this approach is relevant.
In section 2 we introduce standard similar-
ity measures and enhance the definition of Soft-
TFIDF. Then we define a generic model in which
similarity measures may be combined (section 3).
Finally, section 4 shows that experiments with two
different corpora validate our approach.
2 Approximate matching methods
We present below some of the main string similar-
ity measures used to match named entities (Chris-
ten, 2006; Cohen et al, 2003; Bilenko et al, 2003).
2.1 Classical metrics
2.1.1 Sequential character based methods
Levenshtein edit distance. This well-known dis-
tance metric d represents the minimum number
of insertions, deletions or substitutions needed to
transform a string x into another string y. For ex-
ample, d(kitten, sitting) = 3 (k 7? s, e 7? i,
? 7? g). The corresponding normalized similarity
measure is defined as s = 1 ? d/max(|x|, |y|). A
lot of variants and/or improvements exist (Navarro,
2001), among which:
? Damerau. One basic edit operation is added:
a transposition consists in swapping two
characters;
? Needleman-Wunch. Basic edit operation
costs are parameterized: G is the cost of a gap
7We omit measures based on phonetic similarity such
as Soundex, because they are language-specific and/or type-
specific (person names).
(insertion or deletion), and there is a function
cost(c, c
?
) which gives the cost of substituting
c with c? for any pair of characters (c, c?).
Jaro metric (Winkler, 1999). This measure is
based on the number and the order of common
characters. Given two strings x = a
1
. . . a
n
and
y = b
1
. . . b
m
, let H = min(n,m)/2: a
i
is in com-
mon with y if there exists b
j
in y such that a
i
= b
j
and i ? H ? j ? i + H . Let x? = a?
1
. . . a
?
n
?
(resp. y? = b?
1
. . . b
?
m
?
) be the sequence of charac-
ters from x (resp. y) in common with y (resp. x),
in the order they appear in x (resp. y). Any posi-
tion i such that a?
i
6= b
?
i
is called a transposition.
Let T be the number of transpositions between x?
and y? divided by 2:
Jaro(x, y) =
1
3
?
(
|x
?
|
|x|
+
|y
?
|
|y|
+
|y
?
|?T
|y
?
|
)
2.1.2 Bag-of-words methods
With these methods, each NE is represented as
a set of features (generally words or characters n-
grams8). Let X = {x
i
}
1?i?n
and Y = {y
i
}
1?i?m
be the sets representing the entities x, y. Simplest
measures only count the number of elements in
common9, e.g:
Overlap(x, y) =
|X ? Y |
min(|X|, |Y |)
Some more subtle techniques are based on a
vector representation of entities x and y, which
may take into account parameters that are are
not included in the sets themselves. Let A =
(a
1
, . . . , a
|?|
) and B = (b
1
, . . . , b
|?|
) be such vec-
tors10, the widely used cosine similarity is:
cos(A,B) =
?
|?|
i=1
a
i
b
i
?
?
|?|
i=1
a
2
i
?
?
|?|
i=1
b
2
i
Traditionally, TF-IDF weights are used in
vectors (Term Frequency-Inverse Document Fre-
quency). In the NE case, this value represents the
importance each feature w (e.g. word) has for an
entity x belonging to the set E of entities:
tf(w, x) =
n
w,x
?
w
?
??
n
w
?
,x
, idf(w) = log
|E|
|{x ? E|w ? x}|
,
tfidf(w, x) = tf(w, x) ? idf(w).
with n
w,x
the number of times w appears in x.
Thus the similarity score is CosTFIDF(x, y) =
Cos(A,B), where each a
i
(resp. b
i
) in A (resp. in
B) is tfidf(w
i
, x) (resp. tfidf(w
i
, y)).
8In the remaining the term n-grams is always used for
characters n-grams.
9
|E| denotes the number of elements in E.
10
? is the vocabulary, containing all possible features.
594
2.2 Special measures for NE matching
Experiments show that sequential character-based
measures catch mainly coreferent pairs of long NE
that differ only by a few characters. Bag-of-words
methods suit better to the NE matching problem,
since they are more flexible about word order and
position. But a lot of coreferent pairs can not be
identified by such measures, because of small dif-
ferences between words: for example, ?Director
ElBaradei? and ?Director-General ElBareidi? is
out of reach for such methods. That is why ?sec-
ond level? measures are relevant: their principle is
to apply a sub-measure sim? to all pairs of words
between the two NE and to compute a final score
based on these values. This approach is possible
because NE generally contain only a few words.
Monge-Elkan measure belongs to this category:
it simply computes the average of the better pairs
of words according to the sub-measure:
sim(x, y) =
1
n
n
?
i=1
m
max
j=1
(sim
?
(x
i
, y
j
)).
But experiments show that Monge-Elkan does
not perform well. Actually, its very simple behav-
ior favors too much short entities, because averag-
ing penalizes a lot every non-matching word.
A more elaborated measure is proposed in (Co-
hen et al, 2003): Soft-TFIDF is intended precisely
to take advantage of the good results obtained with
Cosine/TFIDF, without automatically discarding
words which are not strictly identical. The original
definition is the following: let CLOSE(?,X, Y )
be the set of words w ? X such that there ex-
ists a word v ? Y such that sim?(w, v) > ?. Let
N(w, Y ) = max({sim
?
(w, v)|v ? Y }). For any
w ? CLOSE(?,X, Y ), let
S
w,X,Y
= weight(w,X) ? weight(w, Y ) ?N(w, Y ),
where weight(w,Z) = tfidf(w,Z)?
?
w?Z
tfidf(w,Z)
2
.
Finally,
SoftTFIDF(X,Y ) =
?
w?CLOSE(?,X,Y )
S
w,X,Y
.
This definition is not entirely correct, be-
cause weight(w, Y ) = 0 if w /? Y (in other
words, w must appear in both X and Y , thus
SoftTFIDF(X,Y ) would always be equal to
CosTFIDF(X,Y )). We propose instead the fol-
lowing corrected definition, which corresponds to
the implementation the authors provided in the
package SecondString11:
11http://secondstring.sourceforge.net
Let CLOSEST(?,w,Z) = {v ? Z | ?v? ? Z :
sim
?
(w, v) ? sim
?
(w, v
?
) ? sim
?
(w, v) > ?}.
SoftTFIDF(X,Y ) =
?
w?X
weight(w,X) ? ?
w,Y
,
where ?
w,Z
= 0 if CLOSEST(?,w,Z) = ?, and
?
w,Z
= weight(w
?
, Z) ? sim
?
(w,w
?
) otherwise,
with12 w? ? CLOSEST(?,w,Z).
As one may see, SoftTFIDF relies on the same
principle than Monge-Elkan: for each word x
i
in the first entity, find a word y
j
in the second
one that maximizes sim?(x
i
, y
j
). Therefore, these
measures have both the drawback not to be sym-
metric. Furthermore, there is another theoretical
pitfall with SoftTFIDF: in Monge-Elkan, the fi-
nal score is simply normalized in [0, 1] using the
average among words of the first entity. Accord-
ing to the principle of the Cosine angle of TF-
IDF-weighted vectors, SoftTFIDF uses both vec-
tors norms. However the way words are ?approx-
imately matched? does not forbid the matching of
a given word in the second entity twice: in this
case, normalization is wrong because this word is
counted only once in the norm of the second vec-
tor. Consequently there is a potential overflow: ac-
tually it is not hard to find simple examples where
the final score is greater than 1, even if this case is
unlikely with real NE and a high threshold ?.
3 Generalizing Soft-TFIDF
3.1 A unifying framework for similarity
measures
We propose to formalize similarity measures in the
generic model below. This model is intended to
define, compare and possibly mix different kinds
of measures. The underlying idea is simply that
most measures may be viewed as a process follow-
ing different steps: representation as a sequence of
features13 (e.g. tokenization), alignment and a way
to compute the final score. We propose to define a
similarity measure sim through these three steps,
each of them is modeled as a function14:
Representation. Given a set F of features, let
features(e) = ?a
1
, . . . , a
n
? be a function that as-
12If |CLOSEST(?, w, Z)| > 1, pick any such w? in the
set. In the case of matching words between NE, this should
almost never happen.
13We use the word feature for the sake of generality.
14Of course, alternative definitions may be relevant. In par-
ticular one may wish to allow the alignment function to return
a set of graphs instead of only one. In the same way, one may
wish to add a special vertex ? to the graph, in order to repre-
sent the fact that a feature is not matched by adding an edge
between this feature and ?.
595
signs an (ordered) sequence of features to any en-
tity e (a
i
? F for any i). Features may be of any
kind (e.g. characters, words, n-grams, or even con-
textual elements of the entity) ;
Alignment. Given a function simF : F 2 7? R
which defines similarity between any pair of fea-
tures, let algn(?a
1
, . . . , a
n
?, ?a
?
1
, . . . , a
?
n
?
?) = G
be a function which assigns a graph G to any pair
of features sequences. G = (V,E) is a bipartite
weighted graph where:
? The set of vertices is V = A ? A?, where
A and A? are the partitions defined as A =
{v
1
, . . . , v
n
} and A? = {v?
1
, . . . , v
?
n
?
}. Each
v
i
(resp. v?
i
) represents (the position of) the
corresponding feature a
i
(resp. a?
i
) ;
? The set of weighted edges is E =
{(v
i
j
, v
?
i
?
j
, s
j
)}
1?j?|E|
, where v
i
j
? A,
v
?
i
?
j
? A
?
. Weights s
j
generally depend on
sim
F
(a
i
j
, a
?
i
?
j
).
Scoring. Finally sim = score(G), where score
assigns a real value (possibly normalized in [0, 1])
to the alignment G.
The representation step is not particularly origi-
nal, since different kinds of representation have al-
ready been used both with sequential methods and
?bag-of-features? methods. However our model
also entails an alignment step, which does not exist
with bag-of-features methods. Actually, the align-
ment is implicit with such methods, and we will
show that making it visible is essential in the case
of NE matching.
In the remaining of this paper we will only con-
sider normalized metrics (scores belong to [0, 1]).
3.2 Revisiting classical similarity measures
Measures presented in section 2 may be defined
within the model presented above. This mod-
elization is only intended to provide a theoretical
viewpoint on the measures: for all practical pur-
poses, standard implementations are clearly more
efficient. Below we do not detail the represen-
tation step, because there is no difficulty with it,
and also because it is interesting to consider that
any measure may be used with different kinds
of features, as we will show in the next section.
Let S = ?a
1
, . . . , a
n
? = features(e) and S? =
?a
?
1
, . . . , a
?
n
?
? = features(e
?
) for any pair of enti-
ties (e, e?).
3.2.1 Levenshtein-like similarity
The function align
lev
(S, S
?
) is defined in the
following way: let G
lev
be the set of all graphs
G = (V,E) such that any pair of edges
(v
i
j
, v
?
i
?
j
, s
j
), (v
i
k
, v
?
i
?
k
, s
k
) ? E satisfies (i
j
<
i
k
? i
?
j
< i
?
k
) ? (i
j
> i
k
? i
?
j
> i
?
k
). This
constraint ensures that the sequential order of fea-
tures is respected15 , and that no feature may be
matched twice. In the simplest form of Leven-
shtein16, simF (a, b) = 1 if a = b and 0 otherwise:
for any (v
i
j
, v
?
i
?
j
, s
j
) ? E, s
j
= sim
F
(a
i
j
, a
?
i
?
j
).
Let
sim(G) = M ?n
g
?cost
g
?|E|+
?
(v
i
j
,v
?
i
?
j
,s
j
)?E
s
j
,
where M = max(n, n?) and n
g
is the number of
vertices that are not connected (i.e. the number of
inserted or deleted words). cost
g
= 1 in the simple
Levenshtein form, but may be a parameter in the
Needleman-Wunch variant (gap cost). In brief, the
principle in this definition is to count the positions
where no edit operation is needed: thus maximiz-
ing sim(G) is equivalent to minimizing the cost of
an alignment:
align
lev
(S, S
?
) = G, where G is any graph such
that sim(G) = max({sim(G?)|G? ? G
lev
}).
Finally, the function score
lev
is simply defined
as score
lev
(G) = sim(G)/max(n, n
?
). It is not
hard to see that this definition is equivalent to the
usual one (see section 2): basically, the graph rep-
resents the concept called trace in (Wagner and
Fischer, 1974), except that the cost function is ?re-
versed? to become a similarity function.
Figure 1: Example of Levenshtein alignment
k
i
t
t
s
i
t
t
i
n
g
e
n
0
1
1
1
0
1
Suppose cost
g
= 1:
sim(G) = M ?n
g
?|E|+
?
e
j
?E
s
j
sim(G) = 7 ? 1 ? 6 + 4
sim(G) = 4
score
lev
(G) = 4/7.
3.2.2 Bag of features
For all simple measures using only sets of fea-
tures, the function align
bag
(S, S
?
) is defined in
the following way: let G be the set of all graphs
15Constraints are a bit more complex for Damerau.
16In the Needleman-Wunch variant, simF should depend
on the cost function, e.g.: simF (a, b) = 1? cost(a, b).
596
G = (V,E) such that if (v
i
j
, v
?
i
?
j
, s
j
) ? E then
a
i
j
= a
?
i
?
j
(equivalently simF (a
i
j
, a
?
i
?
j
) = 1). Now
let once(G) be the set of all G ? G such that
any pair of edges (v
i
j
, v
?
i
?
j
, s
j
), (v
i
k
, v
?
i
?
k
, s
k
) ? E
satisfies i
j
6= i
k
? i
?
j
6= i
?
k
(at most one match
for each feature), and a
i
j
6= a
i
k
(a feature oc-
curring several times is matched only once). Let
sim(G) =
?
(v
i
j
,v
?
i
?
j
,s
j
)?E
s
j
for any G = (V,E).
align
bag
(S, S
?
) = G, where G is any graph such
that sim(G) = max({sim(G?) |G? ? once(G)}).
Since all weights are equal to 1, one may show
that sim(G) = |S ? S?| for any G ? once(G).
Thus the score function is simply used for nor-
malization, depending on the given measure: for
example, score
overlap
(G) =
sim(G)
min(n, n
?
)
.
3.2.3 Soft-TFIDF
The case of Cosine measure with TFIDF
weighted vectors is a bit different. Here we define
the SoftTFIDF version: let algn
soft
(S, S
?
) be the
graph G = (V,E) defined as17 (v
i
j
, v
?
i
?
j
, s
j
) ? E if
and only if a?
i
?
j
= select(CLOSEST(?, a
i
j
, S
?
)),
where CLOSEST is the function defined in sec-
tion 2 and select(E) is a function returning the
first element in E if |E| > 0, and is undefined
otherwise18. For any such edge, the weight s
j
is
s
j
= sim
F
(a
i
j
, a
?
i
?
j
) ?
idf(a
i
j
)
n
?
idf(a
?
i
?
j
)
n
?
.
Once again, let sim(G) =
?
(v
i
j
,v
?
i
?
j
,s
j
)?E
s
j
.
score
soft
(G) = sim(G)/(?S|| ? ?S
?
?), where
??a
1
, . . . , a
n
?|| =
?
?
?
?
n
?
i=1
(
idf(a
i
)
n
)
2
.
Although it is not explicitly used in this defini-
tion, term frequency is taken into account through
the number of edges: suppose a given term t ap-
pears m times in S and m? times in S?, all m ver-
tices corresponding to t in A (the partition repre-
senting S) will be connected to all m? vertices cor-
responding to t in A?. Thus there will be m ? m?
edges, which is exactly the unnormalized product
17In the simple case of CosTFIDF, the condition would be:
(v
i
j
, v
?
i
?
j
, s
j
) ? E if and only if a
i
j
= a
?
i
?
j
. In other words,
all identical features (and only they) are connected.
18
?the first element? means that select(E) may return any
e ? E, provided the same element is always returned for the
same set.
of term frequencies tf(t, S) ? tf(t, S?) ?n ?n?. Thus
summing m ? m? times idf(t)/n ? idf(t)/n? in
sim(G) is equal to tfidf(t, S) ? tfidf(t, S?) (nor-
malization is computed in the same way).
3.3 Meta-Levenshtein: Soft-TFIDF with
Levenshtein alignment
We have shown in part 2.2 that there are some
pitfalls in Soft-TFIDF, especially in the way the
alignment is computed: no symmetry, possible
score overflow. But experiments show that tak-
ing words IDF into account increases performance,
and that Soft-TFIDF, i.e. the possible matching
of words that are not strictly identical, increases
performance (see section 4). That is why improv-
ing this kind of measure is interesting. Follow-
ing the model we proposed above, we propose to
mix the cosine-like similarity used in Soft-TFIDF
with a Levenshtein-like alignment. The following
measure, called Meta-Levenshtein (ML for short),
takes IDFs into account but is not a bag-of-features
metrics.
Let us define align
ML
in the following way: let
G
ML
be defined exactly as the set of graphs G
lev
(see part 3.2.1), except that weights are defined as
in the case of Soft-TFIDF: for any G = (V,E) ?
G
lev
and for any edge (v
i
j
, v
?
i
?
j
, s
j
) ? E, let
s
j
= sim
F
(a
i
j
, a
?
i
?
j
) ?
idf(a
i
j
)
n
?
idf(a
?
i
?
j
)
n
?
.
Let sim(G) =
?
(v
i
j
,v
?
i
?
j
,s
j
)?E
s
j
, and
align
ML
(S, S
?
) = G, where G is such that
sim(G) = max({sim(G
?
) |G
?
? G
ML
}). Finally,
score
ML
(G) = sim(G)/(?S|| ? ?S
?
?).
Compared to Soft-TFIDF, ML solves the prob-
lem of symmetry (ML(S, S?) = ML(S?, S)), and
also the potential overflow, because no feature may
be matched twice (see fig. 2). Of course, the align-
ment is less flexible in ML, since it must satisfy the
sequential order of features. Practically, this mea-
sure may be efficiently implemented in the same
way as Levenshtein similarity, including option-
ally the Damerau extension for transpositions. We
have also tested a simple variant with possible ex-
tended transpositions, i.e. cases like ABC com-
pared to CA, where both C and A are matched.
3.4 Recursive combinations for NE matching
One of the points we want to emphasize through
the generic framework presented above is the mod-
597
Figure 2: Soft-TFIDF vs. ML alignment
With sim(A,D) ? ?, and sim(C,E) ? sim(B,E) ? ?:
A
C
B
A
D
E
F
Soft-TFIDF
A
C
B
A
D
E
F
ML
ularity of similarity measures. Our viewpoint is
that traditional measures may be seen not only in
their original context, but also as modular param-
eterized functions. The first application of such a
definition is already in use in the form of measures
like Monge-Elkan or Soft-TFIDF, which rely on
some sub-measure to compare words inside NEs.
But we will show that modularity is also useful
at a lower level: measures concerning words may
rely on similarity between (for example) n-grams,
and even at this restricted level numerous possible
kinds of similarity may be used.
Moreover, from the viewpoint of applications it
is not very costly to compute similarities between
n-grams and even between words. The number
of n-grams is clearly bounded, and the number of
words is not so high because there are only about 2
words by entity in average, and overall some words
appear very often in entities19.
4 Experiments
4.1 Data
Two corpora were used. Both contain mainly news
and press articles, collected from various interna-
tional sources. The first one, called ?Iran Nu-
clear Threat? (INT in short), is in English and
was extracted from the NTI (Nuclear Threat Ini-
tiative) web site20. It is 236,000 words long. Our
second corpus, called ?French Speaking Medias?
(FSM in short), is 856,000 words long. It was ex-
tracted from a regular crawling of a set of French-
speaking international newspapers web sites dur-
ing a short time-frame (in July 2007). GATE21
was used as the named entities recognizer for INT,
whereas Arisem22 performed the tagging of NEs
19In the corpora we studied, 1172 NE (resp. 2533) contain
1107 distinct words (resp. 2785).
20http://www.nti.org
21http://gate.ac.uk
22http://www.arisem.com
for FSM. Recognition errors23 appear in both cor-
pora, but significantly less in FSM. We restricted
the sets of NEs to those recognized as locations,
organizations and persons, and decided to work
only on entities appearing at least twice. Finally
for INT (resp. FSM) we obtain 1,588 distinct
NE (resp. 3,278) accounting altogether for 33,147
(resp. 23,725) occurrences.
Of course, it would be too costly to manually
label as match (positive) or non-match (negative)
the whole set containing n ? (n ? 1)/2 pairs, for
the observed values of n. The approach consist-
ing in labeling only a randomly chosen subset of
pairs is ineffective, because of the disproportion
between the number of negative and positive pairs
(less than 0.1%). Therefore we tried to find all pos-
itive pairs, assuming the remaining lot are nega-
tive. Practically, the labeling step was based only
on the best pairs as identified by a large set of
measures24. The guidelines we used for labeling
are the following: any incomplete, over-tagged or
simply wrongly recognized NE is discarded. Then
remaining pairs are classified as positive (corefer-
ent), negative (non-coreferent), or ?don?t know?25.
Corpus Discarded Pos. Neg. Don?t know
INT 416 / 1,588 764 2,821 302
FSM 745 / 3,278 741 32,348 419
According to our initial hypotheses, all non-
tagged pairs are considered as negative in the ex-
periments below. ?Don?t know? pairs are ignored.
As a further note, about 20% of the pairs are not
orthographically similar (e.g. acronyms and their
expansion): these pairs are out of reach of our tech-
niques, and would require additional knowledge.
4.2 Observations
4.2.1 Taking IDF into account
To evaluate the contribution of IDF26 in scor-
ing the coreference degree between NE, let us ob-
23Mainly truncated entities, over-tagged entities, and com-
mon nouns beginning with a capital letter.
24This is a potential methodological bias, but we hope to
have kept its effect as low as possible: the measures we used
are quite diverse and do not assign good scores to the same
pairs; therefore, for each measure, we expect that the poten-
tial misses (false negatives) will be matched by some other
measure, thus allowing a fair evaluation of its performance.
A few positive pairs are manually added (mainly acronyms).
25All ambiguous cases, mainly due to some missing preci-
sion (e.g. ?Ministry of Foreign Affairs? and ?Russian Min-
istry of Foreign Affairs?), and more rarely homonymy (e.g.
?Lebedev? and ?[Valery|Oleg] Lebedev?)
26It may be noticed that the Term Frequency in TFIDF is
rarely important, since a given word appear almost always
only once in a NE.
598
serve the differences among best scored pairs for
measures Bag-of-words Cosine and Cosine over
TFIDF weighted vectors. For example, the for-
mer will assign 0.5 to pair ?Prime Minister Tony
Blair?/?Blair? (from corpus INT), whereas the
latter gives 0.61. As expected, IDF weights lighten
the effect of non-informative words and strengthen
important words. In both corpora, The F1-measure
for TFIDF Cosine is about 10 points (in average)
better than for Bag-of-words Cosine (see fig. 3).
4.2.2 Soft-TFIDF problems: normalization,
threshold and sub-measure
As we have explained in section 2.2, the Soft-
TFIDF measure (Cohen et al, 2003) may suffer
from normalization problems. This is probably
the reason why the authors seem to use it parsi-
moniously, i.e. only in the case words are very
close (which is verified using a high threshold
?). Indeed, problems occur when the sub-measure
and/or the threshold are not carefully chosen, caus-
ing performances drop: using Jaro measure with
a very low threshold (0.2 here), performances
are even worst than Bag-of-words cosine (see fig.
3). This is due to the double matching problem:
for example, pair ?Tehran Times (Tehran)?/?Inter
Press Service? (from INT) is scored more than 1.0
because ?Tehran? matches ?Inter? twice: even
with a low score as a coefficient, ?Inter? has a
high IDF compared to ?Press? and ?Service?, so
counting it twice makes normalization wrong.
However, this problem may be solved by choos-
ing a more adequate sub-measure: experiments
show that using the CosTFIDF measure with bi-
grams or trigrams outperforms standard CosT-
FIDF. Of course, there are some positive pairs
that are found ?later? by Soft-TFIDF, since it may
only increase score. But the ?soft? comparison
brings back to the top ranked pairs a lot of positive
ones. In both corpora, the best sub-measure found
is CosTFIDF with trigrams. ?Mohamed ElBa-
radei?/?Director Mohammad ElBaradei? (INT)
or ?Chine?/?China? (FSM) are typical positive
pairs found by this measure but not by standard
CosTFIDF. Here no threshold is needed anymore
because the sub-measure has been chosen with
care, depending on the data, in order to avoid the
normalization problem. This is clearly a drawback
for Soft-TFIDF: it may perform well, but only with
hand-tuning sub-measure and/or threshold.
4.2.3 Beyond Soft-TFIDF: (recursive) ML
In the FSM corpus, replacing Soft-TFIDF with
(simple) Meta-Levenshtein at the word level does
not decrease performance, even though the align-
ment is more constrained in the latter case. Us-
ing the same sub-measure to compare words (tri-
grams CosTFIDF), it does neither increase perfor-
mance. A few positive pairs are missed in the INT
corpus, due to the more flexible word order in En-
glish: ?U.S. State Department?/?US Department
of State? is such an example (12 among 764 are
concerned). This problem is easily solved with the
ML variant with extended transposition (see part
3.3): in both corpora, there are no positive pairs
requiring more than a gap of one word in the align-
ment. Thus this measure is not only performant but
also robust, since it does not need any hand-tuning.
As a second step, we want to improve results
by selecting a more fine-grained sub-measure. We
have tried several ideas, such as using different
kinds of n-grams similarity inside the words sim-
ilarity measure. Firstly, trigrams performed bet-
ter than bigrams or simple characters. Secondly,
the best trigrams similarity method found is actu-
ally very simple: it consists in using CosTFIDF
computed on the trigrams contexts, i.e. the set of
closest27 trigrams of all occurrences of the given
trigram. Unsurprisingly, good scores are generally
obtained for pairs of trigrams that have common
characters. But it seems that this approach also
enhances robustness, because it finds similarities
between ?close characters?: in the French corpus,
one observes quite good scores between trigrams
containing an accentuated version and the non ac-
centuated version of the same character. Further-
more, some character encoding errors are some-
how corrected this way28. This is possibly the rea-
son why the improvement of results is better in
FSM than in INT (see table 1).
Finally, using also ML to compute similarity
between words29 yields the best results. This
means that compared to the simple CosTFIDF sub-
measure, one does not compare bags of trigrams
but ordered sequences of trigrams30.
27We have tried different window sizes for such contexts,
from 2 to 10 trigrams long: performances were approximately
the same. We only consider trigrams found in the entities.
28For example, the ?? in the name ?Lugovo??? appears also in
FSM as i, as y, as a`, and is sometimes deleted.
29i.e. not only between sequences of words: in this case
ML is run between trigrams at the word level, and then an-
other time between words at the NE level.
30It is hard to tell whether it is the sequential alignment or
599
Figure 3: F1-Measures for FSM (percentages)
 0
 20
 40
 60
 80
 100
 0  500  1000  1500  2000  2500
F1
-M
ea
su
re
n best scored pairs (considered as positive)
Bag of words Cosine
Cosine TFIDF (words)
Soft-TFIDF (Jaro)
Soft-TFIDF (TFIDF 3g)
ML (ML/contexts 3g)
Example: for Cosine TFIDF with words, if the threshold is
set in such a way that (only) the 1000 top ranked pairs are
classified as positive, then the F1-measure is around 60%.
Table 1: Best F1-measures (percentages)
INT FSM
Measure F1 P R F1 P R
Cosine 51.6 63.2 43.6 59.5 76.2 48.7
CosTFIDF 62.6 71.7 55.6 69.9 84.2 59.8
Soft TFIDF/3g 68.6 74.2 63.9 73.1 79.8 67.6
ML/ML-context 70.6 72.6 68.7 77.0 82.5 72.2
P/R: Corresponding Precision/Recall.
4.3 Global results
Results are synthesized in table 1, which is based
on the maximum F1-measure for each measure.
One observes that F1-measure is 3 to 6 points bet-
ter for Soft-TFIDF than for standard TF-IDF, and
that our measure still increases F1-measure by 2
(INT) to 4 points (FSM). Results show that its
contribution consists mainly in improving the re-
call, which means that our measure is able to catch
more positive pairs than Soft-TFIDF: for exam-
ple, the pair ?Fatah Al Islam?/ ?Fateh el-Islam?
(FSM) is scored 0.54 by SoftTFIDF and 0.70 by
ML. Our measure remains the best for all values of
n in fig. 3, and results are similar for F0.5-measure
and F2-measure: thus, irrespective of specific ap-
plication needs which may favor precision or re-
call, ML seems preferable.
5 Conclusion
In conclusion, we have proposed a generic model
to show that similarity measures may be combined
in numerous ways. We have tested such a combi-
nation, based on Soft-TFIDF, which performs bet-
the ?right? use of the trigrams sub-measure which is responsi-
ble for the improvement, since the only possible comparison
at this level is Soft-TFIDF.
ter than all existing similarity metrics on two cor-
pora. Our measure is robust, since it does not rely
on any kind of prior knowledge. Thus it may be
easily used, in particular in applications where NE
matching is useful but is not the essential task.
Acknowledgements
This work has been funded by the National Project
Cap Digital - Infom@gic. We thank Lo??s Rigouste
(Pertimm) and Nicolas Dessaigne and Aure?lie Mi-
geotte (Arisem) for providing us with the anno-
tated French corpus.
References
Bilenko, Mikhail, Raymond J. Mooney, William W.
Cohen, Pradeep Ravikumar, and Stephen E. Fien-
berg. 2003. Adaptive name matching in information
integration. IEEE Intelligent Systems, 18(5):16?23.
Christen, Peter. 2006. A comparison of personal name
matching: Techniques and practical issues. Techni-
cal Report TR-CS-06-02, Department of Computer
Science, The Australian National University, Can-
berra 0200 ACT, Australia, September.
Cohen, William W., Pradeep Ravikumar, and
Stephen E. Fienberg. 2003. A comparison of
string distance metrics for name-matching tasks. In
Kambhampati, Subbarao and Craig A. Knoblock,
editors, Proceedings of IJCAI-03 Workshop on
Information Integration on the Web (IIWeb-03),
August 9-10, 2003, Acapulco, Mexico, pages 73?78.
Freeman, Andrew, Sherri L. Condon, and Christopher
Ackerman. 2006. Cross linguistic name matching
in English and Arabic. In Moore, Robert C., Jeff A.
Bilmes, Jennifer Chu-Carroll, and Mark Sanderson,
editors, Proc. HLT-NAACL.
Navarro, Gonzalo. 2001. A guided tour to approximate
string matching. ACM Comput. Surv., 33(1):31?88.
Pouliquen, Bruno, Ralf Steinberger, Camelia Ignat,
Irina Temnikova, Anna Widiger, Wajdi Zaghouani,
and Jan Zizka. 2006. Multilingual person name
recognition and transliteration. CORELA - Cogni-
tion, Representation, Langage.
Prager, John, Sarah Luger, and Jennifer Chu-Carroll.
2007. Type nanotheories: a framework for term
comparison. In Proceedings of CIKM ?07, pages
701?710, New York, NY, USA. ACM.
Wagner, R. and M. Fischer. 1974. The string-to-string
correction problem. JACM, 21(1):168?173.
Winkler, W. E. 1999. The state of record linkage
and current research problems. Technical Report
RR99/04, US Bureau of the Census.
600
