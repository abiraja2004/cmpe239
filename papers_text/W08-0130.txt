Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 198?207,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Making Grammar-Based Generation Easier to Deploy in Dialogue Systems
David DeVault and David Traum and Ron Artstein
USC Institute for Creative Technologies
13274 Fiji Way
Marina del Rey, CA 90292
{devault,traum,artstein}@ict.usc.edu
Abstract
We present a development pipeline and asso-
ciated algorithms designed to make grammar-
based generation easier to deploy in imple-
mented dialogue systems. Our approach real-
izes a practical trade-off between the capabili-
ties of a system?s generation component and
the authoring and maintenance burdens im-
posed on the generation content author for a
deployed system. To evaluate our approach,
we performed a human rating study with sys-
tem builders who work on a common large-
scale spoken dialogue system. Our results
demonstrate the viability of our approach and
illustrate authoring/performance trade-offs be-
tween hand-authored text, our grammar-based
approach, and a competing shallow statistical
NLG technique.
1 Introduction
This paper gives an overview of a new example-
based generation technique that is designed to make
grammar-based generation easier to deploy in dia-
logue systems. Dialogue systems present several
specific requirements for a practical generation com-
ponent. First, the generator needs to be fast enough
to support real-time interaction with a human user.
Second, the generator must provide adequate cover-
age for the meanings the dialogue system needs to
express. What counts as ?adequate? can vary be-
tween systems, since the high-level purpose of a di-
alogue system can affect priorities regarding output
fluency, fidelity to the requested meaning, variety
of alternative outputs, and tolerance for generation
failures. Third, developing the necessary resources
for the generation component should be relatively
straightforward in terms of time and expertise re-
quired. This is especially important since dialogue
systems are complex systems with significant devel-
opment costs. Finally, it should be relatively easy
for the dialogue manager to formulate a generation
request in the format required by the generator.
Together, these requirements can reduce the at-
tractiveness of grammar-based generation when
compared to simpler template-based or canned text
output solutions. In terms of speed, off-the-
shelf, wide-coverage grammar-based realizers such
as FUF/SURGE (Elhadad, 1991) can be too slow for
real-time interaction (Callaway, 2003).
In terms of adequacy of coverage, in principle,
grammar-based generation offers significant advan-
tages over template-based or canned text output by
providing productive coverage and greater variety.
However, realizing these advantages can require sig-
nificant development costs. Specifying the neces-
sary connections between lexico-syntactic resources
and the flat, domain-specific semantic representa-
tions that are typically available in implemented sys-
tems is a subtle, labor-intensive, and knowledge-
intensive process for which attractive methodologies
do not yet exist (Reiter et al, 2003).
One strategy is to hand-build an application-
specific grammar. However, in our experience,
this process requires a painstaking, time-consuming
effort by a developer who has detailed linguistic
knowledge as well as detailed domain knowledge,
and the resulting coverage is inevitably limited.
Wide-coverage generators that aim for applicabil-
198
ity across application domains (White et al, 2007;
Zhong and Stent, 2005; Langkilde-Geary, 2002;
Langkilde and Knight, 1998; Elhadad, 1991) pro-
vide a grammar (or language model) for free. How-
ever, it is harder to tailor output to the desired word-
ing and style for a specific dialogue system, and
these generators demand a specific input format that
is otherwise foreign to an existing dialogue system.
Unfortunately, in our experience, the development
burden of implementing the translation between the
system?s available meaning representations and the
generator?s required input format is quite substan-
tial. Indeed, implementing the translation might re-
quire as much effort as would be required to build a
simple custom generator; cf. (Callaway, 2003; Buse-
mann and Horacek, 1998). This development cost is
exacerbated when a dialogue system?s native mean-
ing representation scheme is under revision.
In this paper, we survey a new example-based ap-
proach (DeVault et al, 2008) that we have devel-
oped in order to mitigate these difficulties, so that
grammar-based generation can be deployed more
widely in implemented dialogue systems. Our de-
velopment pipeline requires a system developer to
create a set of training examples which directly
connect desired output texts to available applica-
tion semantic forms. This is achieved through a
streamlined authoring task that does not require de-
tailed linguistic knowledge. Our approach then
processes these training examples to automatically
construct all the resources needed for a fast, high-
quality, run-time grammar-based generation compo-
nent. We evaluate this approach using a pre-existing
spoken dialogue system. Our results demonstrate
the viability of the approach and illustrate author-
ing/performance trade-offs between hand-authored
text, our grammar-based approach, and a competing
shallow statistical NLG technique.
2 Background and Motivation
The generation approach set out in this paper has
been developed in the context of a research pro-
gram aimed at creating interactive virtual humans
for social training purposes (Swartout et al, 2006).
Virtual humans are embodied conversational agents
that play the role of people in simulations or games.
They interact with human users and other virtual hu-
Figure 1: Doctor Perez.
mans using spoken language and non-verbal behav-
ior such as eye gaze, gesture, and facial displays.
The case study we present here is the genera-
tion of output utterances for a particular virtual hu-
man, Doctor Perez (see Figure 1), who is designed
to teach negotiation skills in a multi-modal, multi-
party, non-team dialogue setting (Traum et al, 2005;
Traum et al, 2008). The human trainee who talks
to the doctor plays the role of a U.S. Army captain
named Captain Kirk. We summarize Doctor Perez?s
generation requirements as follows.
In order to support compelling real-time conver-
sation and effective training, the generator must be
able to identify an utterance for Doctor Perez to use
within approximately 200ms on modern hardware.
Doctor Perez has a relatively rich internal men-
tal state including beliefs, goals, plans, and emo-
tions. As Doctor Perez attempts to achieve his con-
versational goals, his utterances need to take a va-
riety of syntactic forms, including simple declar-
ative sentences, various modal constructions relat-
ing to hypothetical actions or plans, yes/no and wh-
questions, and abbreviated dialogue forms such as
elliptical clarification and repair requests, ground-
ing, and turn-taking utterances. Doctor Perez cur-
rently uses about 200 distinct output utterances in
the course of his dialogues.
Doctor Perez is designed to simulate a non-native
English speaker, so highly fluent output is not a ne-
cessity; indeed, a small degree of disfluency is even
desirable in order to increase the realism of talking
to a non-native speaker.
Finally, in reasoning about user utterances, dia-
logue management, and generation, Doctor Perez
199
26
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
addressee captain-kirk
dialogue-act
2
6
4
addressee captain-kirk
type assign-turn
actor doctor-perez
3
7
5
speech-act
2
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
actor doctor-perez
addressee captain-kirk
action assert
content
2
6
6
6
6
6
6
6
6
4
type state
polarity negative
time present
attribute resourceAttribute
value medical-supplies
object-id market
3
7
7
7
7
7
7
7
7
5
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
addressee captain-kirk
dialogue-act.addressee captain-kirk
dialogue-act.type assign-turn
dialogue-act.actor doctor-perez
speech-act.actor doctor-perez
speech-act.addressee captain-kirk
speech-act.action assert
speech-act.content.type state
speech-act.content.polarity negative
speech-act.content.time present
speech-act.content.attribute resourceAttribute
speech-act.content.value medical-supplies
speech-act.content.object-id market
(a) Attribute-value matrix (b) Corresponding frame
Figure 2: An example of Doctor Perez?s representations for utterance semantics: Doctor Perez tells the captain that
there are no medical supplies at the market.
exploits an existing semantic representation scheme
that has been utilized in a family of virtual humans.
This scheme uses an attribute-value matrix (AVM)
representation to describe an utterance as a set of
core speech acts and other dialogue acts. Speech
acts generally have semantic contents that describe
propositions and questions about states and actions
in the domain, as well as other features such as po-
larity and modality. See (Traum, 2003) for some
more details and examples of this representation.
For ease of interprocess communication, and certain
kinds of statistical processing, this AVM structure is
linearized so that each non-recursive terminal value
is paired with a path from the root to the final at-
tribute. Thus, the AVM in Figure 2(a) is represented
as the ?frame? in Figure 2(b).
Because the internal representations that make up
Doctor Perez?s mental state are under constant de-
velopment, the exact frames that are sent to the gen-
eration component change frequently as new rea-
soning capabilities are added and existing capabil-
ities are reorganized. Additionally, while only hun-
dreds of frames currently arise in actual dialogues,
the number of potential frames is orders of magni-
tude larger, and it is difficult to predict in advance
which frames might occur.
In this setting, over a period of years, a number
of different approaches to natural language gener-
ation have been implemented and tested, including
hand-authored canned text, domain specific hand-
built grammar-based generators (e.g., (Traum et al,
2003)), shallow statistical generation techniques,
and the grammar-based approach presented in this
paper. We now turn to the details of our approach.
3 Technical Approach
Our approach builds on recently developed tech-
niques in statistical parsing, lexicalized syntax mod-
eling, generation with lexicalized grammars, and
search optimization to automatically construct all
the resources needed for a high-quality run-time
generation component.
The approach involves three primary steps: spec-
ification of training examples, grammar induction,
and search optimization. In this section, we present
the format that training examples take and then sum-
marize the subsequent automatic processing steps.
Due to space limitations, we omit the full details
of these automatic processing steps, and refer the
reader to (DeVault et al, 2008) for additional details.
3.1 Specification of Training Examples
Each training example in our approach speci-
fies a target output utterance (string), its syn-
tax, and a set of links between substrings within
the utterance and system semantic representa-
tions. Formally, a training example takes the form
(u, syntax(u), semantics(u)). We will illustrate
this format using the training example in Figure 3.
In this example, the generation content author
200
Utterance we don?t have medical supplies here captain
Syntax
cat: SA??
cat: S??
cat: NP??
pos: PRP??
we
cat: VP??
pos: AUX??
do
pos: RB??
n?t
cat: VP??
pos: AUX??
have
cat: NP??
pos: JJ??
medical
pos: NNS??
supplies
cat: ADVP??
pos: RB??
here
cat: NP??
pos: NN??
captain
Semantics
we do n?t . . . . . . . . .
{
speech-act.action = assert
speech-act.content.polarity = negative
have . . . . . . . . . . . . . speech-act.content.attribute = resourceAttribute
medical supplies . . speech-act.content.value = medical-supplies
here . . . . . . . . . . . . . speech-act.content.object-id = market
captain . . . . . . . . . .
?
?
?
addressee = captain-kirk
dialogue-act.addressee = captain-kirk
speech-act.addressee = captain-kirk
Figure 3: A generation training example for Doctor Perez.
suggests the output utterance u = we don?t have
medical supplies here captain. Each utterance u is
accompanied by syntax(u), a syntactic analysis in
Penn Treebank format (Marcus et al, 1994). In this
example, the syntax is a hand-corrected version of
the output of the Charniak parser (Charniak, 2001;
Charniak, 2005) on this sentence; we discuss this
hand correction in Section 4.
To represent the meaning of utterances, our ap-
proach assumes that the system provides some set
M = {m1, ...,mj} of semantic representations.
The meaning of any individual utterance is then
identified with some subset of M . For Doctor Perez,
M comprises the 232 distinct key-value pairs that
appear in the system?s various generation frames. In
this example, the utterance?s meaning is captured by
the 8 key-value pairs indicated in the figure.
Our approach requires the generation content
author to link these 8 key-value pairs to con-
tiguous surface expressions within the utterance.
The technique is flexible about which surface ex-
pressions are chosen (e.g. they need not corre-
spond to constituent boundaries); however, they do
need to be compatible with the way the syntactic
analysis tokenizes the utterance, as follows. Let
t(u) = ?t1, ..., tn? be the terminals in the syn-
tactic analysis, in left-to-right order. Formally,
semantics(u) = {(s1,M1), ..., (sk,Mk)}, where
t(u) = s1@ ? ? ?@sk (with @ denoting concatena-
tion), and where Mi ? M for all i ? 1..k. In this
example, the surface expression we don?t, which to-
kenizes as ?we,do,n?t?, is connected to key-values
that indicate a negative polarity assertion.
This training example format has two features that
are crucial to our approach. First, the semantics of
an utterance is specified independently of its syntax.
This greatly reduces the amount of linguistic exper-
tise a generation content author needs to have. It
also allows making changes to the underlying syn-
tax without having to re-author the semantic links.
Second, the assignment of semantic representa-
tions to surface expressions must span the entire ut-
terance. No words or expressions can be viewed as
?meaningless?. This is essential because, otherwise,
the semantically motivated search algorithm used in
generation has no basis on which to include those
particular expressions when it constructs its output
utterance. Many systems, including Doctor Perez,
lack some of the internal representations that would
be necessary to specify semantics down to the lex-
ical level. An important feature of our approach is
that it allows an arbitrary semantic granularity to be
employed, by mapping the representations available
in the system to appropriate multi-word chunks.
201
3.2 Automatic Grammar Induction and Search
Optimization
The first processing step is to induce a productive
grammar from the training examples. We adopt the
probabilistic tree-adjoining grammar (PTAG) for-
malism and grammar induction technique of (Chi-
ang, 2003). We induce our grammar from training
examples such as Figure 3 using heuristic rules to
assign derivations to the examples, as in (Chiang,
2003). Once derivations have been assigned, sub-
trees within the training example syntax are incre-
mentally detached. This process yields the reusable
linguistic resources in the grammar, as well as the
statistical model needed to compute operation prob-
abilities when the grammar is later used in genera-
tion. Figure 5 in the Appendix illustrates this pro-
cess by presenting the linguistic resources inferred
from the training example of Figure 3.
Our approach uses this induced grammar to treat
generation as a search problem: given a desired se-
mantic representation M ? ? M , use the grammar
to incrementally construct an output utterance u that
expressesM ?. We treat generation as anytime search
by accruing multiple goal states up until a specified
timeout (200ms for Doctor Perez) and returning a
list of alternative outputs ranked by their derivation
probabilities.
The search space created by a grammar induced
in this way is too large to be searched exhaustively
in most applications. The second step of automated
processing, then, uses the training examples to learn
an effective search policy so that good output sen-
tences can be found in a reasonable time frame. The
solution we have developed employs a beam search
strategy that uses weighted features to rank alterna-
tive grammatical expansions at each step. Our al-
gorithm for selecting features and weights is based
on the search optimization algorithm of (Daum?
and Marcu, 2005), which decides to update feature
weights when mistakes are made during search on
training examples. We use the boosting approach of
(Collins and Koo, 2005) to perform feature selection
and identify good weight values.
4 Empirical Evaluation
In the introduction, we identified run-time speed, ad-
equacy of coverage, authoring burdens, and NLG re-
quest specification as important factors in the selec-
tion of a technology for a dialogue system?s NLG
component. In this section, we evaluate our tech-
nique along these four dimensions.
Hand-authored utterances. We collected a sam-
ple of 220 instances of frames that Doctor Perez?s
dialogue manager had requested of the generation
component in previous dialogues with users. Some
frames occurred more than once in this sample.
Each frame was associated with a single hand-
authored utterance. Some of these utterances arose
in human role plays for Doctor Perez; some were
written by a script writer; others were authored
by system builders to provide coverage for specific
frames. All were reviewed by a system builder for
appropriateness to the corresponding frame.
Training. We used these 220 (frame, utterance)
examples to evaluate both our approach and a shal-
low statistical method called sentence retriever (dis-
cussed below). We randomly split the examples
into 198 training and 22 test examples; we used the
same train/test split for our approach and sentence
retriever.
To train our approach, we constructed training ex-
amples in the format specified in Section 3.1. Syntax
posed an interesting problem, because the Charniak
parser frequently produces erroneous syntactic anal-
yses for utterances in Doctor Perez?s domain, but it
was not obvious how detrimental these errors would
be to overall generated output. We therefore con-
structed two alternative sets of training examples ?
one where the syntax of each utterance was the un-
corrected output of the Charniak parser, and another
where the parser output was corrected by hand (the
syntax in Figure 3 above is the corrected version).
Hand correction of parser output requires consider-
able linguistic expertise, so uncorrected output rep-
resents a substantial reduction in authoring burden.
The connections between surface expressions and
frame key-value pairs were identical in both uncor-
rected and corrected training sets, since they are in-
dependent of the syntax. For each training set, we
trained our generator on the 198 training examples.
We then generated a single (highest-ranked) utter-
ance for each example in both the test and training
sets. The generator sometimes failed to find a suc-
cessful utterance within the 200ms timeout; the suc-
cess rate of our generator was 95% for training ex-
202
amples and 80% for test examples. The successful
utterances were rated by our judges.
Sentence retriever is based on the cross-
language information retrieval techniques described
in (Leuski et al, 2006), and is currently in use for
Doctor Perez?s NLG problem. Sentence retriever
does not exploit any hierarchical syntactic analy-
sis of utterances. Instead, sentence retriever views
NLG as an information retrieval task in which a set
of training utterances are the ?documents? to be re-
trieved, and the frame to be expressed is the query.
At run-time, the algorithm functions essentially as a
classifier: it uses a relative entropy metric to select
the highest ranking training utterance for the frame
that Doctor Perez wishes to express. This approach
has been used because it is to some extent robust
against changes in internal semantic representations,
and against minor deficiencies in the training corpus,
but as with a canned text approach, it requires each
utterance to be hand-authored before it can be used
in dialogue. We trained sentence retriever on the 198
training examples, and used it to generate a single
(highest-ranked) utterance for each example in both
the test and training sets. Sentence retriever?s suc-
cess rate was 96% for training examples and 90%
for test examples. The successful utterances were
rated by our judges.
Figure 7 in the Appendix illustrates the alternative
utterances that were produced for a frame present in
the test data but not in the training data.
Run-time speed. Both our approach and sentence
retriever run within the available 200ms window.
Adequacy of Coverage. To assess output quality,
we conducted a study in which 5 human judges gave
overall quality ratings for various utterances Doctor
Perez might use to express specific semantic frames.
In total, judges rated 494 different utterances which
were produced in several conditions: hand-authored
(for the relevant frame), generated by our approach,
and sentence retriever.
We asked our 5 judges to rate each of the 494 ut-
terances, in relation to the specific frame for which
it was produced, on a single 1 (?very bad?) to 5
(?very good?) scale. Since ratings need to incorpo-
rate accuracy with respect to the frame, our judges
had to be able to read the raw system semantic rep-
resentations. This meant we could only use judges
who were deeply familiar with the dialogue system;
however, the main developer of the new generation
algorithms (the first author) did not participate as
a judge. Judges were blind to the conditions un-
der which utterances were produced. The judges
rated the utterances using a custom-built application
which presented a single frame together with 1 to 6
candidate utterances for that frame. The rating inter-
face is shown in Figure 6 in the Appendix. The order
of candidate utterances for each frame was random-
ized, and the order in which frames appeared was
randomized for each judge.
The judges were instructed to incorporate both
fluency and accuracy with respect to the frame into
a single overall rating for each utterance. While it
is possible to have human judges rate fluency and
accuracy independently, ratings of fluency alone are
not particularly helpful in evaluating Doctor Perez?s
generation component, since for Doctor Perez, a cer-
tain degree of disfluency can contribute to believ-
ability (as noted in Section 2). We therefore asked
judges to make an overall assessment of output qual-
ity for the Doctor Perez character.
The judges achieved a reliability of ? = 0.708
(Krippendorff, 1980); this value shows that agree-
ment is well above chance, and allows for tentative
conclusions. Agreement between subsets of judges
ranged from ? = 0.802 for the most concordant pair
of judges to ? = 0.593 for the most discordant pair.
We also performed an ANOVA comparing three
conditions (generated, retrieved and hand-authored
utterances) across the five judges; we found sig-
nificant main effects of condition (F (2, 3107) =
55, p < 0.001) and judge (F (4, 3107) = 17, p <
0.001), but no significant interaction (F (8, 3107) =
0.55, p > 0.8). We therefore conclude that the indi-
vidual differences among the judges do not affect the
comparison of utterances across the different condi-
tions, so we will report the rest of the evaluation on
the mean ratings per utterance.
Due to the large number of factors and the dif-
ferences in the number of utterances correspond-
ing to each condition, we ran a small number
of planned comparisons. The distribution of rat-
ings across utterances is not normal; to validate
our results we accompanied each t-test by a non-
parametric Wilcoxon rank sum test, and signifi-
cance always fell in the same general range. We
found a significant difference between generated
203
Generated (N = 90)
Sentence retriever (N = 100)
Rating
Fr
eq
u
en
cy
(%
)
0
10
20
30
40
1 2 3 4 5
Figure 4: Observed ratings of generated (uncorrected
syntax) vs. retrieved sentences for test examples.
output for all examples, retrieved output for all ex-
amples, and hand-authored utterances (F (2, 622) =
16, p < 0.001); however, subsequent t-tests show
that all of this difference is due to the fact that hand-
authored utterances (mean rating 4.4) are better than
retrieved (t(376) = 3.7, p < 0.001) and gener-
ated (t(388) = 5.9, p < 0.001) utterances, whereas
the difference between generated (mean rating 3.8)
and retrieved (mean rating 4.0) is non-significant
(t(385) = 1.6, p > 0.1).
Figure 4 shows the observed rating frequencies
of sentence retriever (mean 3.0) and our approach
(mean 3.6) on the test examples. While this data
does not show a significant difference, it suggests
that retriever?s selected sentences are most fre-
quently either very bad or very good; this reflects
the fact that the classification algorithm retrieves
highly fluent hand-authored text which is sometimes
semantically very incorrect. (Figure 7 in the Ap-
pendix provides such an example, in which a re-
trieved sentence has the wrong polarity.) The qual-
ity of our generated output, by comparison, appears
more graded, with very good quality the most fre-
quent outcome and lower qualities less frequent. In
a system where there is a low tolerance for very
bad quality output, generated output would likely be
considered preferable to retrieved output.
In terms of generation failures, our approach had
poorer coverage of test examples than sentence re-
triever (80% vs. 90%). Note however that in this
study, our approach only delivered an output if it
could completely cover the requested frame. In the
future, we believe coverage could be improved, with
perhaps some reduction in quality, by allowing out-
puts that only partially cover requested frames.
In terms of output variety, in this initial study our
judges rated only the highest ranked output gener-
ated or retrieved for each frame. However, we ob-
served that our generator frequently finds several al-
ternative utterances of relatively high quality (see
Figure 7); thus our approach offers another poten-
tial advantage in output variety.
Authoring burdens. Both canned text and sen-
tence retriever require only frames and correspond-
ing output sentences as input. In our approach, syn-
tax and semantic links are additionally needed. We
compared the use of corrected vs. uncorrected syn-
tax in training. Surprisingly, we found no significant
difference between generated output trained on cor-
rected and uncorrected syntax (t(29) = 0.056, p >
0.9 on test items, t(498) = ?1.1, p > 0.2 on all
items). This is a substantial win in terms of reduced
authoring burden for our approach.
If uncorrected syntax is used, the additional bur-
den of our approach lies only in specifying the se-
mantic links. For the 220 examples in this study,
one system builder specified these links in about 6
hours. We present a detailed cost/benefit analysis of
this effort in (DeVault et al, 2008).
NLG request specification. Both our approach
and sentence retriever accept the dialogue manager?s
native semantic representation for NLG as input.
Summary. In exchange for a slightly increased
authoring burden, our approach yields a generation
component that generalizes to unseen test problems
relatively gracefully, and does not suffer from the
frequent very bad output or the necessity to author
every utterance that comes with canned text or a
competing statistical classification technique.
5 Conclusion and Future Work
In this paper we have presented an approach to spec-
ifying domain-specific, grammar-based generation
by example. The method reduces the authoring bur-
den associated with developing a grammar-based
NLG component for an existing dialogue system.
We have argued that the method delivers relatively
high-quality, domain-specific output without requir-
ing that content authors possess detailed linguistic
knowledge. In future work, we will study the perfor-
204
mance of our approach as the size of the training set
grows, and assess what specific weaknesses or prob-
lematic disfluencies, if any, our human rating study
identifies in output generated by our technique. Fi-
nally, we intend to evaluate the performance of our
generation approach within the context of the com-
plete, running Doctor Perez agent.
Acknowledgments
Thanks to Arno Hartholt, Susan Robinson, Thomas
Russ, Chung-chieh Shan, and Matthew Stone. This
work was sponsored by the U.S. Army Research,
Development, and Engineering Command (RDE-
COM), and the content does not necessarily reflect
the position or the policy of the Government, and no
official endorsement should be inferred.
References
Stephen Busemann and Helmut Horacek. 1998. A flex-
ible shallow approach to text generation. In Proceed-
ings of INLG, pages 238?247.
Charles B. Callaway. 2003. Evaluating coverage for
large symbolic NLG grammars. Proceedings of the
International Joint Conferences on Artificial Intelli-
gence.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In ACL ?01: Proceedings of the
39th Annual Meeting on Association for Computa-
tional Linguistics, pages 124?131, Morristown, NJ,
USA. Association for Computational Linguistics.
Eugene Charniak. 2005.
ftp://ftp.cs.brown.edu/pub/nlparser/
parser05Aug16.tar.gz.
David Chiang. 2003. Statistical parsing with an auto-
matically extracted tree adjoining grammar. In Rens
Bod, Remko Scha, and Khalil Sima?an, editors, Data
Oriented Parsing, pages 299?316. CSLI Publications,
Stanford.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25?70.
Hal Daum?, III and Daniel Marcu. 2005. Learning as
search optimization: approximate large margin meth-
ods for structured prediction. In ICML ?05: Proceed-
ings of the 22nd international conference on Machine
learning, pages 169?176, New York, NY, USA. ACM.
David DeVault, David Traum, and Ron Artstein. 2008.
Practical grammar-based NLG from examples. In
Fifth International Natural Language Generation
Conference (INLG).
Michael Elhadad. 1991. FUF: the universal unifier user
manual version 5.0. Technical Report CUCS-038-91.
Klaus Krippendorff, 1980. Content Analysis: An Intro-
duction to Its Methodology, chapter 12, pages 129?
154. Sage, Beverly Hills, CA.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
COLING-ACL, pages 704?710.
I. Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sen-
tence generator.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective question
answering characters. In The 7th SIGdial Workshop
on Discourse and Dialogue.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
E. Reiter, S. Sripada, and R. Robertson. 2003. Acquir-
ing correct knowledge for natural language generation.
Journal of Artificial Intelligence Research, 18:491?
516.
William Swartout, Jonathan Gratch, Randall W. Hill, Ed-
uard Hovy, Stacy Marsella, Jeff Rickel, and David
Traum. 2006. Toward virtual humans. AI Mag.,
27(2):96?108.
David Traum, Michael Fleischman, and Eduard Hovy.
2003. Nl generation for virtual humans in a complex
social environment. In Working Notes AAAI Spring
Symposium on Natural Language Generation in Spo-
ken and Written Dialogue, March.
David Traum, William Swartout, Jonathan Gratch,
Stacy Marsella, Patrick Kenny, Eduard Hovy, Shri
Narayanan, Ed Fast, Bilyana Martinovski, Rahul
Baghat, Susan Robinson, Andrew Marshall, Dagen
Wang, Sudeep Gandhe, and Anton Leuski. 2005.
Dealing with doctors: A virtual human for non-team
interaction. In SIGdial.
D. R. Traum, W. Swartout, J Gratch, and S Marsella.
2008. A virtual human dialogue model for non-team
interaction. In Laila Dybkjaer and Wolfgang Minker,
editors, Recent Trends in Discourse and Dialogue.
Springer.
David Traum. 2003. Semantics and pragmatics of ques-
tions and answers for dialogue agents. In proceedings
of the International Workshop on Computational Se-
mantics, pages 380?394, January.
Michael White, Rajakrishnan Rajkumar, and Scott Mar-
tin. 2007. Towards broad coverage surface realiza-
tion with CCG. In Proc. of the Workshop on Using
Corpora for NLG: Language Generation and Machine
Translation (UCNLG+MT).
Huayan Zhong and Amanda Stent. 2005. Building
surface realizers automatically from corpora using
general-purpose tools. In Proc. Corpus Linguistics ?05
Workshop on Using Corpora for Natural Language
Generation.
205
syntax:
cat: SA??
fin: other,?? cat: S
cat: NP,?? apr: VBP,
apn: other??
pos: PRP??
we
fin: yes,?? cat: VP
apn: other,?? pos: VBP
do
pos: RB??
n?t
fin: yes,?? cat: VP,
gra: obj1??
fin: yes,?? cat: VP,
gra: obj1??
pos: VBP??
have
cat: NP,?? gra: obj1
operations: initial tree comp
semantics: speech-act.action = assert
speech-act.content.polarity = negative
speech-act.content.attribute = resourceAttribute
syntax:
cat: NP,?? apr: VBP,
gra: obj1,?? apn: other
pos: JJ??
medical
pos: NNS??
supplies
cat: ADVP,?? gra: adj
pos: RB??
here
cat: NP,?? apr: VBZ,
gra: adj,?? apn: 3ps
pos: NN??
captain
operations: comp left/right adjunction left/right adjunction
semantics: speech-act.content.value =
medical-supplies
speech-act.content.object-id =
market
addressee = captain-kirk
dialogue-act.addressee =
captain-kirk
speech-act.addressee =
captain-kirk
Figure 5: The linguistic resources automatically inferred from the training example in Figure 3.
Figure 6: Human rating interface.
206
Input semantic form
addressee captain-kirk
dialogue-act.actor doctor-perez
dialogue-act.addressee captain-kirk
dialogue-act.type assign-turn
speech-act.action assert
speech-act.actor doctor-perez
speech-act.addressee captain-kirk
speech-act.content.attribute acceptableAttribute
speech-act.content.object-id clinic
speech-act.content.time present
speech-act.content.type state
speech-act.content.value yes
Outputs
Hand-authored
the clinic is acceptable captain
Generated (uncorrected syntax)
Rank Time (ms)
1 16 the clinic is up to standard captain
2 94 the clinic is acceptable captain
3 78 the clinic should be in acceptable condition captain
4 16 the clinic downtown is currently acceptable captain
5 78 the clinic should agree in an acceptable condition captain
Generated (corrected syntax)
Rank Time (ms)
1 47 it is necessary that the clinic be in good condition captain
2 31 i think that the clinic be in good condition captain
3 62 captain this wont work unless the clinic be in good condition
Sentence retriever
the clinic downtown is not in an acceptable condition captain
Figure 7: The utterances generated for a single test example by different evaluation conditions. Generated outputs
whose rank (determined by derivation probability) was higher than 1 were not rated in the evaluation reported in this
paper, but are included here to suggest the potential of our approach to provide a variety of alternative outputs for the
same requested semantic form. Note how the output of sentence retriever has the opposite meaning to that of the input
frame.
207
