Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1179?1189,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Disentangling Chat with Local Coherence Models
Micha Elsner
School of Informatics
University of Edinburgh
melsner0@gmail.com
Eugene Charniak
Department of Computer Science
Brown University, Providence, RI 02912
ec@cs.brown.edu
Abstract
We evaluate several popular models of local
discourse coherence for domain and task gen-
erality by applying them to chat disentangle-
ment. Using experiments on synthetic multi-
party conversations, we show that most mod-
els transfer well from text to dialogue. Co-
herence models improve results overall when
good parses and topic models are available,
and on a constrained task for real chat data.
1 Introduction
One property of a well-written document is coher-
ence, the way each sentence ts into its context? sen-
tences should be interpretable in light of what has
come before, and in turn make it possible to inter-
pret what comes after. Models of coherence have
primarily been used for text-based generation tasks:
ordering units of text for multidocument summariza-
tion or inserting new text into an existing article.
In general, the corpora used consist of informative
writing, and the tasks used for evaluation consider
different ways of reordering the same set of textual
units. But the theoretical concept of coherence goes
beyond both this domain and this task setting? and
so should coherence models.
This paper evaluates a variety of local coher-
ence models on the task of chat disentanglement or
?threading?: separating a transcript of a multiparty
interaction into independent conversations
1
. Such
simultaneous conversations occur in internet chat
1
A public implementation is available via https://
bitbucket.org/melsner/browncoherence.
rooms, and on shared voice channels such as push-
to-talk radio. In these situations, a single, correctly
disentangled, conversational thread will be coherent,
since the speakers involved understand the normal
rules of discourse, but the transcript as a whole will
not be. Thus, a good model of coherence should be
able to disentangle sentences as well as order them.
There are several differences between disentan-
glement and the newswire sentence-ordering tasks
typically used to evaluate coherence models. Inter-
net chat comes from a different domain, one where
topics vary widely and no reliable syntactic annota-
tions are available. The disentanglement task mea-
sures different capabilities of a model, since it com-
pares documents that are not permuted versions of
one another. Finally, full disentanglement requires
a large-scale search, which is computationally dif-
cult. We move toward disentanglement in stages,
carrying out a series of experiments to measure the
contribution of each of these factors.
As an intermediary between newswire and inter-
net chat, we adopt the SWITCHBOARD (SWBD) cor-
pus. SWBD contains recorded telephone conversa-
tions with known topics and hand-annotated parse
trees; this allows us to control for the performance
of our parser and other informational resources. To
compare the two algorithmic settings, we use SWBD
for ordering experiments, and also articially entan-
gle pairs of telephone dialogues to create synthetic
transcripts which we can disentangle. Finally, we
present results on actual internet chat corpora.
On synthetic SWBD transcripts, local coherence
models improve performance considerably over our
baseline model, Elsner and Charniak (2008b). On
1179
internet chat, we continue to do better on a con-
strained disentanglement task, though so far, we are
unable to apply these improvements to the full task.
We suspect that, with better low-level annotation
tools for the chat domain and a good way of integrat-
ing prior information, our improvements on SWBD
could transfer fully to IRC chat.
2 Related work
There is extensive previous work on coherence mod-
els for text ordering; we describe several specic
models below, in section 2. This study focuses on
models of local coherence, which relate text to its
immediate context. There has also been work on
global coherence, the structure of a document as a
whole (Chen et al, 2009; Eisenstein and Barzilay,
2008; Barzilay and Lee, 2004), typically modeled
in terms of sequential topics. We avoid using them
here, because we do not believe topic sequences are
predictable in conversation and because such models
tend to be algorithmically cumbersome.
In addition to text ordering, local coherence mod-
els have also been used to score the uency of texts
written by humans or produced by machine (Pitler
and Nenkova, 2008; Lapata, 2006; Miltsakaki and
Kukich, 2004). Like disentanglement, these tasks
provide an algorithmic setting that differs from or-
dering, and so can demonstrate previously unknown
weaknesses in models. However, the target genre is
still informative writing, so they reveal little about
cross-domain exibility.
The task of disentanglement or ?threading? for
internet chat was introduced by Shen et al (2006).
Elsner and Charniak (2008b) created the publicly
available #LINUX corpus; the best published re-
sults on this corpus are those of Wang and Oard
(2009). These two studies use overlapping unigrams
to measure similarity between two sentences; Wang
and Oard (2009) use a message expansion tech-
nique to incorporate context beyond a single sen-
tence. Unigram overlaps are used to model coher-
ence, but more sophisticated methods using syntax
(Lapata and Barzilay, 2005) or lexical features (La-
pata, 2003) often outperform them on ordering tasks.
This study compares several of these methods with
Elsner and Charniak (2008b), which we use as a
baseline because there is a publicly available imple-
mentation
2
.
Adams (2008) also created and released a disen-
tanglement corpus. They use LDA (Blei et al, 2001)
to discover latent topics in their corpus, then measur-
ing similarity by looking for shared topics. These
features fail to improve their performance, which is
puzzling in light of the success of topic modeling for
other coherence and segmentation problems (Eisen-
stein and Barzilay, 2008; Foltz et al, 1998). The
results of this study suggest that topic models can
help with disentanglement, but that it is difcult to
nd useful topics for IRC chat.
A few studies have attempted to disentangle con-
versational speech (Aoki et al, 2003; Aoki et al,
2006), mostly using temporal features. For the most
part, however, this research has focused on auditory
processing in the context of the cocktail party prob-
lem, the task of attending to a specic speaker in
a noisy room (Haykin and Chen, 2005). Utterance
content has some inuence on what the listener per-
ceives, but only for extremely salient cues such as
the listener's name (Moray, 1959), so cocktail party
research does not typically use lexical models.
3 Models
In this section, we briey describe the models we in-
tend to evaluate. Most of them are drawn from pre-
vious work; one, the topical entity grid, is a novel
extension of the entity grid. For the experiments be-
low, we train the models on SWBD, sometimes aug-
mented with a larger set of automatically parsed con-
versations from the FISHER corpus. Since the two
corpora are quite similar, FISHER is a useful source
for extra data; McClosky et al (2010) uses it for
this purpose in parsing experiments. (We continue
to use SWBD/FISHER even for experiments on IRC,
because we do not have enough disentangled train-
ing data to learn lexical relationships.)
3.1 Entity grid
The entity grid (Lapata and Barzilay, 2005; Barzilay
and Lapata, 2005) is an attempt to model some prin-
ciples of Centering Theory (Grosz et al, 1995) in a
statistical manner. It represents a document in terms
of entities and their syntactic roles: subject (S), ob-
ject (O), other (X) and not present (-). In each new
2
cs.brown.edu/
?
melsner
1180
utterance, the grid predicts the role in which each
entity will appear, given its history of roles in the
previous sentences, plus a salience feature counting
the total number of times the entity occurs. For in-
stance, for an entity which is the subject of sentence
1, the object of sentence 2, and occurs four times in
total, the grid predicts its role in sentence 3 accord-
ing to the conditional P (jS;O; sal = 4).
As in previous work, we treat each noun in a doc-
ument as denoting a single entity, rather than using
a coreference technique to attempt to resolve them.
In our development experiments, we noticed that
coreferent nouns often occur farther apart in conver-
sation than in newswire, since they are frequently
referred to by pronouns and deictics in the interim.
Therefore, we extend the history to six previous ut-
terances. For robustness with this long history, we
model the conditional probabilities using multilabel
logistic regression rather than maximum likelihood.
This requires the assumption of a linear model, but
makes the estimator less vulnerable to overtting
due to sparsity, increasing performance by about 2%
in development experiments.
3.2 Topical entity grid
This model is a variant of the generative entity
grid, intended to take into account topical informa-
tion. To create the topical entity grid, we learn a
set of topic-to-word distributions for our corpus us-
ing LDA (Blei et al, 2001)
3
with 200 latent top-
ics. This model embeds our vocabulary in a low-
dimensional space: we represent each word w as
the vector of topic probabilities p(t
i
jw). We ex-
perimented with several ways to measure relation-
ships between words in this space, starting with the
standard cosine. However, the cosine can depend on
small variations in probability (for instance, if w has
most of its mass in dimension 1, then it is sensitive
to the exact weight of v for topic 1, even if this es-
sentially never happens).
To control for this tendency, we instead use the
magnitude of the dimension of greatest similarity:
sim(w; v) = max
i
min(w
i
; v
i
)
Tomodel coherence, we generalize the binary his-
3
www.cs.princeton.edu/
?
blei/
topicmodeling.html
tory features of the standard entity grid, which de-
tect, for example, whether entity e is the subject of
the previous sentence. In the topical entity grid, we
instead compute a real-valued feature which sums
up the similarity between entity e and the subject(s)
of the previous sentence.
These features can detect a transition like: ?The
House voted yesterday. The Senate will consider the
bill today.?. If ?House? and ?Senate? have a high
similarity, then the feature will have a high value,
predicting that ?Senate? is a good subject for the cur-
rent sentence. As in the previous section, we learn
the conditional probabilities with logistic regression;
we train in parallel by splitting the data and averag-
ing (Mann et al, 2009). The topics are trained on
FISHER, and on NANC for news.
3.3 IBM-1
The IBM translation model was rst considered for
coherence by Soricut and Marcu (2006), although a
less probabilistically elegant version was proposed
earlier (Lapata, 2003). This model attempts to gen-
erate the content words of the next sentence by trans-
lating them from the words of the previous sentence,
plus a null word; thus, it will learn alignments be-
tween pairs of words that tend to occur in adjacent
sentences. We learn parameters on the FISHER cor-
pus, and on NANC for news.
3.4 Pronouns
The use of a generative pronoun resolver for co-
herence modeling originates in Elsner and Char-
niak (2008a). That paper used a supervised model
(Ge et al, 1998), but we adapt a newer, unsuper-
vised model which they also make publicly available
(Charniak and Elsner, 2009)
4
. They model each pro-
noun as generated by an antecedent somewhere in
the previous two sentences. If a good antecedent is
found, the probability of the pronoun's occurrence
will be high; otherwise, the probability is low, sig-
naling that the text is less coherent because the pro-
noun is hard to interpret correctly.
We use the model as distributed for news text. For
conversation, we adapt it by running a few iterations
of their EM training algorithm on the FISHER data.
4
bllip.cs.brown.edu/resources.shtml\
#software
1181
3.5 Discourse-newness
Building on work from summarization (Nenkova
and McKeown, 2003) and coreference resolution
(Poesio et al, 2005), Elsner and Charniak (2008a)
use a model which recognizes discourse-new versus
old NPs as a coherence model. For instance, the
model can learn that ?President Barack Obama? is
a more likely rst reference than ?Obama?. Follow-
ing their work, we score discourse-newness with a
maximum-entropy classier using syntactic features
counting different types of NP modiers, and we use
NP head identity as a proxy for coreference.
3.6 Chat-specic features
Most disentanglement models use non-linguistic in-
formation alongside lexical features; in fact, times-
tamps and speaker identities are usually better cues
than words are. We capture three essential non-
linguistic features using simple generative models.
The rst feature is the time gap between one utter-
ance and the next within the same thread. Consistent
short gaps are a sign of normal turn-taking behavior;
long pauses do occur, but much more rarely (Aoki et
al., 2003). We round all time gaps to the nearest sec-
ond and model the distribution of time gaps using a
histogram, choosing bucket sizes adaptively so that
each bucket contains at least four datapoints.
The second feature is speaker identity; conver-
sations usually involve a small subset of the to-
tal number of speakers, and a few core speakers
make most of the utterances. We model the distri-
bution of speakers in each conversation using a Chi-
nese Restaurant Process (CRP) (Aldous, 1985) (tun-
ing the dispersion  to maximize development pe-
formance). The CRP's ?rich-get-richer? dynamics
capture our intuitions, favoring conversations domi-
nated by a few vociferous speakers.
Finally, we model name mentioning. Speakers
in IRC chat often use their addressee's names to co-
ordinate the chat (O'Neill and Martin, 2003), and
this is a powerful source of information (Elsner and
Charniak, 2008b). Our model classies each utter-
ance into either the start or continuation of a conver-
sational turn, by checking if the previous utterance
had the same speaker. Given this status, it computes
probabilities for three outcomes: no name mention,
a mention of someone who has previously spoken
in the conversation, or a mention of someone else.
(The third option is extremely rare; this accounts
for most of the model's predictive power). We learn
these probabilities from IRC training data.
3.7 Model combination
To combine these different models, we adopt the
log-linear framework of Soricut and Marcu (2006).
Here, each model P
i
is assigned a weight 
i
, and the
combined score P (d) is proportional to:
X
i

i
log(P
i
(d))
The weights  can be learned discriminatively,
maximizing the probability of d relative to a task-
specic contrast set. For ordering experiments, the
contrast set is a single random permutation of d; we
explain the training regime for disentanglement be-
low, in subsection 4.1.
4 Comparing orderings of SWBD
To measure the differences in performance caused
by moving from news to a conversational domain,
we rst compare our models on an ordering task,
discrimination (Barzilay and Lapata, 2005; Karama-
nis et al, 2004). In this task, we take an original
document and randomly permute its sentences, cre-
ating an articial incoherent document. We then test
to see if our model prefers the coherent original.
For SWBD, rather than compare permutations
of the individual utterances, we permute conversa-
tional turns (sets of consecutive utterances by each
speaker), since turns are natural discourse units in
conversation. We take documents numbered 2000?
3999 as training/development and the remainder as
test, yielding 505 training and 153 test documents;
we evaluate 20 permutations per document. As a
comparison, we also show results for the same mod-
els on WSJ, using the train-test split from Elsner and
Charniak (2008a); the test set is sections 14-24, to-
talling 1004 documents.
Purandare and Litman (2008) carry out similar ex-
periments on distinguishing permuted SWBD doc-
uments, using lexical and WordNet features in a
model similar to Lapata (2003). Their accuracy for
this task (which they call ?switch-hard?) is roughly
68%.
1182
WSJ SWBD
EGrid 76.4z 86.0
Topical EGrid 71.8z 70.9z
IBM-1 77.2z 84.9y
Pronouns 69.6z 71.7z
Disc-new 72.3z 55.0z
Combined 81.9 88.4
-EGrid 81.0 87.5
-Topical EGrid 82.2 90.5
-IBM-1 79.0z 88.9
-Pronouns 81.3 88.5
-Disc-new 82.2 88.4
Table 1: Discrimination F scores on news and dialogue.
z indicates a signicant difference from the combined
model at p=.01 and y at p=.05.
In Table 1, we show the results for individual
models, for the combined model, and ablation re-
sults for mixtures without each component. WSJ is
more difcult than SWBD overall because, on av-
erage, news articles are shorter than SWBD con-
versations. Short documents are harder, because
permuting disrupts them less. The best SWBD re-
sult is 91%; the best WSJ result is 82% (both for
mixtures without the topical entity grid). The WSJ
result is state-of-the-art for the dataset, improving
slightly on Elsner and Charniak (2008a) at 81%. We
test results for signicance using the non-parametric
Mann-Whitney U test.
Controlling for the fact that discrimination is eas-
ier on SWBD, most of the individual models perform
similarly in both corpora. The strongest models in
both cases are the entity grid and IBM-1 (at about
77% for news, 85% for dialogue). Pronouns and the
topical entity grid are weaker. The major outlier is
the discourse-new model, whose performance drops
from 72% for news to only 55%, just above chance,
for conversation.
The model combination results show that all the
models are quite closely correlated, since leaving
out any single model does not degrade the combi-
nation very much (only one of the ablations is sig-
nicantly worse than the combination). The most
critical in news is IBM-1 (decreasing performance
by 3% when removed); in conversation, it is the
entity grid (decreasing by about 1%). The topical
entity grid actually has a (nonsignicant) negative
impact on combined performance, implying that its
predictive power in this setting comes mainly from
information that other models also capture, but that
it is noisier and less reliable. In each domain, the
combined models outperform the best single model,
showing the information provided by the weaker
models is not completely redundant.
Overall, these results suggest that most previ-
ously proposed local coherence models are domain-
general; they work on conversation as well as
news. The exception is the discourse-newness
model, which benets most from the specic con-
ventions of a written style. Full names with titles
(like ?President Barack Obama?) are more common
in news, while conversation tends to involve fewer
completely unfamiliar entities and more cases of
bridging reference, in which grounding information
is given implicitly (Nissim, 2006). Due to its poor
performance, we omit the discourse-newness model
in our remaining experiments.
5 Disentangling SWBD
We now turn to the task of disentanglement, test-
ing whether models that are good at ordering also
do well in this new setting. We would like to hold
the domain constant, but we do not have any disen-
tanglement data recorded from naturally occurring
speech, so we create synthetic instances by merging
pairs of SWBD dialogues. Doing so creates an arti-
cial transcript in which two pairs of people appear
to be talking simultaneously over a shared channel.
The situation is somewhat contrived in that each
pair of speakers converses only with each other,
never breaking into the other pair's dialogue and
rarely using devices like name mentioning to make
it clear who they are addressing. Since this makes
speaker identity a perfect cue for disentanglement,
we do not use it in this section. The only chat-
specic model we use is time.
Because we are not using speaker information, we
remove all utterances which do not contain a noun
before constructing synthetic transcripts? these are
mostly backchannels like ?Yeah?. Such utterances
cannot be correctly assigned by our coherence mod-
els, which deal with content; we suspect most of
them could be dealt with by associating them with
the nearest utterance from the same speaker.
1183
Once the backchannels are stripped, we can cre-
ate a synthetic transcript. For each dialogue, we rst
simulate timestamps by sampling the number of sec-
onds between each utterance and the next from a dis-
cretized Gaussian: bN(0; 2:5)c. The interleaving of
the conversations is dictated by the timestamps. We
truncate the longer conversation at the length of the
shorter; this ensures a baseline score of 50% for the
degenerate model that assigns all utterances to the
same conversation.
We create synthetic instances of two types? those
where the two entangled conversations had differ-
ent topical prompts and those where they were the
same. (Each dialogue in SWBD focuses on a prese-
lected topic, such as shing or movies.) We entangle
dialogues from our ordering development set to use
for mixture training and validation; for testing, we
use 100 instances of each type, constructed from di-
alogues in our test set.
When disentangling, we treat each thread as inde-
pendent of the others. In other words, the probability
of the entire transcript is the product of the probabil-
ities of the component threads. Our objective is to
nd the set of threads maximizing this. As a com-
parison, we use the model of Elsner and Charniak
(2008b) as a baseline. To make their implementa-
tion comparable to ours, in this section we constrain
it to nd only two threads.
5.1 Disentangling a single utterance
Our rst disentanglement task is to correctly assign
a single utterance, given the true structure of the rest
of the transcript. For each utterance, we compare
two versions of the transcript, the original, and a
version where it is swapped into the other thread.
Our accuracy measures how often our models prefer
the original. Unlike full-scale disentanglement, this
task does not require a computationally demanding
search, so it is possible to run experiments quickly.
We also use it to train our mixture models for disen-
tanglement, by construct a training example for each
utterance i in our training transcripts. Since the El-
sner and Charniak (2008b) model maximizes a cor-
relation clustering objective which sums up indepen-
dent edge weights, we can also use it to disentangle
a single sentence efciently.
Our results are shown in Table 2. Again, re-
sults for individual models are above the line, then
Different Same Avg.
EGrid 80.2 72.9 76.6
Topical EGrid 81.7 73.3 77.5
IBM-1 70.4 66.7 68.5
Pronouns 53.1 50.1 51.6
Time 58.5 57.4 57.9
Combined 86.8 79.6 83.2
-EGrid 86.0 79.1 82.6
-Topical EGrid 85.2 78.7 81.9
-IBM-1 86.2 78.7 82.4
-Pronouns 86.8 79.4 83.1
-Time 84.5 76.7 80.6
E+C `08 78.2 73.5 75.8
Table 2: Average accuracy for disentanglement of a sin-
gle utterance on 200 synthetic multiparty conversations
from SWBD test.
our combined model, and nally ablation results for
mixtures omitting a single model. The results show
that, for a pair of dialogues that differ in topic, our
best model can assign a single sentence with 87%
accuracy. For the same topic, the accuracy is 80%.
In each case, these results improve on (Elsner and
Charniak, 2008b), which scores 78% and 74%.
Changing to this new task has a substantial im-
pact on performance. The topical model, which per-
formed poorly for ordering, is actually stronger than
the entity grid in this setting. IBM-1 underperforms
either grid model (69% to 77%); on ordering, it was
nearly as good (85% to 86%).
Despite their ordering performance of 72%, pro-
nouns are essentially useless for this task, at 52%.
This decline is due partly to domain, and partly
to task setting. Although SWBD contains more
pronominals than WSJ, many of them are rst
and second-person pronouns or deictics, which our
model does not attempt to resolve. Since the disen-
tanglement task involves moving only a single sen-
tence, if moving this sentence does not sever a re-
solvable pronoun from its antecedent, the model will
be unable to make a good decision.
As before, the ablation results show that all the
models are quite correlated, since removing any sin-
gle model from the mixture causes only a small de-
crease in performance. The largest drop (83% to
81%) is caused by removing time; though time is
a weak model on its own, it is completely orthogo-
1184
nal to the other models, since unlike them, it does
not depend on the words in the sentences.
Comparing results between ?different topic? and
?same topic? instances shows that ?same topic? is
harder? by about 7% for the combined model. The
IBM model has a relatively small gap of 3.7%, and
in the ablation results, removing it causes a larger
drop in performance for ?same? than ?different?;
this suggests it is somewhat more robust to similar-
ity in topic than entity grids.
Disentanglement accuracy is hard to predict given
ordering performance; the two tasks plainly make
different demands on models. One difference is that
the models which use longer histories (the two entity
grids) remain strong, while the models considering
only one or two previous sentences (IBM and pro-
nouns) do not do as well. Since the changes being
considered here affect only a single sentence, while
permutation affects the entire transcript, more his-
tory may help by making the model more sensitive
to small changes.
5.2 Disentangling an entire transcript
We now turn to the task of disentangling an entire
transcript at once. This is a practical task, motivated
by applications such as search and information re-
trieval. However, it is more difcult than assign-
ing only a single utterance, because decisions are
interrelated? an error on one utterance may cause
a cascade of poor decisions further down. It is also
computationally harder.
We use tabu search (Glover and Laguna, 1997) to
nd a good solution. The search repeatedly nds and
moves the utterance which would most improve the
model score if swapped from one thread to the other.
Unlike greedy search, tabu search is constrained not
to repeat a solution that it has recently visited; this
forces it to keep exploring when it reaches a local
maximum. We run 500 iterations of tabu search
(usually nding the rst local maximum after about
100) and return the best solution found.
We measure performance with one-to-one over-
lap, which maps the two clusters to the two gold
dialogues, then measures percent correct
5
. Our re-
sults (Table 3) show that, for transcripts with dif-
ferent topics, our disentanglement has 68% over-
5
The other popular metrics, F and loc
3
, are correlated.
Different Same Avg.
EGrid 60.3 57.1 58.7
Topical EGrid 62.3 56.8 59.6
IBM-1 56.5 55.2 55.9
Pronouns 54.5 54.4 54.4
Time 55.4 53.8 54.6
Combined 67.9 59.8 63.9
E+C `08 59.1 57.4 58.3
Table 3: One-to-one overlap between disentanglement re-
sults and truth on 200 synthetic multiparty conversations
from SWBD test.
lap with truth, extracting about two thirds of the
structure correctly; this is substantially better than
Elsner and Charniak (2008b), which scores 59%.
Where the entangled conversations have the same
topic, performance is lower, about 60%, but still bet-
ter than the comparison model with 57%. Since cor-
relations with the previous section are fairly reliable,
and the disentanglement procedure is computation-
ally intensive, we omit ablation experiments.
As we expect, full disentanglement is more dif-
cult than single-sentence disentanglement (com-
bined scores drop by about 20%), but the single-
sentence task is a good predictor of relative perfor-
mance. Entity grid models do best, the IBM model
remains useful, but less so than for discrimination,
and pronouns are very weak. The IBM model per-
forms similarly under both metrics (56% and 57%),
while other models perform worse on loc
3
. This
supports our suggestion that IBM's decline in per-
formance from ordering is indeed due to its using a
single sentence history; it is still capable of getting
local structures right, but misses global ones.
6 IRC data
In this section, we move from synthetic data to
real multiparty discourse recorded from internet chat
rooms. We use two datasets: the #LINUX corpus
(Elsner and Charniak, 2008b), and three larger cor-
pora, #IPHONE, #PHYSICS and #PYTHON (Adams,
2008). We use the 1000-line ?development? sec-
tion of #LINUX for tuning our mixture models and
the 800-line ?test? section for development experi-
ments. We reserve the Adams (2008) corpora for
testing; together, they consist of 19581 lines of chat,
with each section containing 500 to 1000 lines.
1185
Chat-specic 74.0
+EGrid 79.3
+Topical EGrid 76.8
+IBM-1 76.3
+Pronouns 73.9
+EGrid/Topic/IBM-1 78.3
E+C `08b 76.4
Table 4: Accuracy for single utterance disentanglement,
averaged over annotations of 800 lines of #LINUX data.
In order to use syntactic models like the entity
grid, we parse the transcripts using (McClosky et
al., 2006). Performance is bad, although the parser
does identify most of the NPs; poor results are typi-
cal for a standard parser on chat (Foster, 2010). We
postprocess the parse trees to retag ?lol?, ?haha? and
?yes? as UH (rather than NN, NNP and JJ).
In this section, we use all three of our chat-
specic models (sec. 2.0.6; time, speaker andmen-
tion) as a baseline. This baseline is relatively strong,
so we evaluate our other models in combination with
it.
6.1 Disentangling a single sentence
As before, we show results on correctly disentan-
gling a single sentence, given the correct structure
of the rest of the transcript. We average perfor-
mance on each transcript over the different annota-
tions, then average the transcripts, weighing them by
length to give each utterance equal weight.
Table 4 gives results on our development corpus,
#LINUX. Our best result, for the chat-specic fea-
tures plus entity grid, is 79%, improving on the com-
parison model, Elsner and Charniak (2008b), which
gets 76%. (Although the table only presents an av-
erage over all annotations of the dataset, this model
is also more accurate for each individual annota-
tor than the comparison model.) We then ran the
same model, chat-specic features plus entity grid,
on the test corpora from Adams (2008). These re-
sults (Table 5) are also better than Elsner and Char-
niak (2008b), at an average of 93% over 89%.
As pointed out in Elsner and Charniak (2008b),
the chat-specic features are quite powerful in this
domain, and it is hard to improve over them. Elsner
and Charniak (2008b), which has simple lexical fea-
tures, mostly based on unigram overlap, increases
#IPHONE #PHYSICS #PYTHON
+EGrid 92.3 96.6 91.1
E+C `08b 89.0 90.2 88.4
Table 5: Average accuracy for disentanglement of a sin-
gle utterance for 19581 total lines from Adams (2008).
performance over baseline by 2%. Both IBM and
the topical entity grid achieve similar gains. The en-
tity grid does better, increasing performance to 79%.
Pronouns, as before for SWBD, are useless.
We believe that the entity grid's good perfor-
mance here is due mostly to two factors: its use of
a long history, and its lack of lexicalization. The
grid looks at the previous six sentences, which dif-
ferentiates it from the IBM model and from Elsner
and Charniak (2008b), which treats each pair of sen-
tences independently. Using this long history helps
to distinguish important nouns from unimportant
ones better than frequency alone. We suspect that
our lexicalized models, IBM and the topical entity
grid, are hampered by poor parameter settings, since
their parameters were learned on FISHER rather than
IRC chat. In particular, we believe this explains why
the topical entity grid, which slightly outperformed
the entity grid on SWBD, is much worse here.
6.2 Full disentanglement
Running our tabu search algorithm on the full disen-
tanglement task yields disappointing results. Accu-
racies on the #LINUX dataset are not only worse than
previous work, but also worse than simple baselines
like creating one thread for each speaker. The model
nds far too many threads? it detects over 300, when
the true number is about 81 (averaging over annota-
tions). This appears to be related to biases in our
chat-specic models as well as in the entity grid;
the time model (which generates gaps between adja-
cent sentences) and the speaker model (which uses
a CRP) both assign probability 1 to single-utterance
conversations. The entity grid also has a bias toward
short conversations, because unseen entities are em-
pirically more likely to occur toward the beginning
of a conversation than in the middle.
A major weakness in our model is that we aim
only to maximize coherence of the individual con-
versations, with no prior on the likely length or num-
ber of conversations that will appear in the tran-
1186
script. This allows the model to create far too many
conversations. Integrating a prior into our frame-
work is not straightforward because we currently
train our mixture to maximize single-utterance dis-
entanglement performance, and the prior is not use-
ful for this task.
We experimented with xing parts of the tran-
script to the solution obtained by Elsner and Char-
niak (2008b), then using tabu search to ll in the
gaps. This constrains the number of conversations
and their approximate positions. With this structure
in place, we were able to obtain scores comparable
to Elsner and Charniak (2008b), but not improve-
ments. It appears that our performance increase on
single-sentence disentanglement does not transfer to
this task because of cascading errors and the neces-
sity of using external constraints.
7 Conclusions
We demonstrate that several popular models of lo-
cal coherence transfer well to the conversational do-
main, suggesting that they do indeed capture coher-
ence in general rather than specic conventions of
newswire text. However, their performance across
tasks is not as stable; in particular, models which
use less history information are worse for disentan-
glement.
Our results study suggest that while sophisticated
coherence models can potentially contribute to dis-
entanglement, they would benet greatly from im-
proved low-level resources for internet chat. Bet-
ter parsing, or at least NP chunking, would help for
models like the entity grid which rely on syntactic
role information. Larger training sets, or some kind
of transfer learning, could improve the learning of
topics and other lexical parameters. In particular,
our results on SWBD data conrm the conjecture of
(Adams, 2008) that LDA topic modeling is in prin-
ciple a useful tool for disentanglement? we believe a
topic-based model could also work on IRC chat, but
would require a better set of extracted topics. With
better parameters for these models and the integra-
tion of a prior, we believe that our good performance
on SWBD and single-utterance disentanglement for
IRC can be extended to full-scale disentanglement
of IRC.
Acknowledgements
We are extremely grateful to Regina Barzilay, Mark
Johnson, Rebecca Mason, Ben Swanson and Neal
Fox for their comments, to Craig Martell for the
NPS chat datasets and to three anonymous review-
ers. This work was funded by a Google Fellowship
for Natural Language Processing.
References
Paige H. Adams. 2008. Conversation Thread Extraction
and Topic Detection in Text-based Chat. Ph.D. thesis,
Naval Postgraduate School.
David Aldous. 1985. Exchangeability and related top-
ics. In Ecole d'Ete de Probabilities de Saint-Flour
XIII 1983, pages 1?198. Springer.
Paul M. Aoki, Matthew Romaine, Margaret H. Szyman-
ski, James D. Thornton, Daniel Wilson, and Allison
Woodruff. 2003. The mad hatter's cocktail party: a
social mobile audio space supporting multiple simul-
taneous conversations. In CHI '03: Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 425?432, New York, NY, USA. ACM
Press.
Paul M. Aoki, Margaret H. Szymanski, Luke D.
Plurkowski, James D. Thornton, Allison Woodruff,
and Weilie Yi. 2006. Where's the ?party? in ?multi-
party??: analyzing the structure of small-group socia-
ble talk. In CSCW '06: Proceedings of the 2006 20th
anniversary conference on Computer supported coop-
erative work, pages 393?402, New York, NY, USA.
ACM Press.
Regina Barzilay and Mirella Lapata. 2005. Modeling lo-
cal coherence: an entity-based approach. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL'05).
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004: Proceedings of the Main Conference, pages
113?120.
David Blei, Andrew Y. Ng, and Michael I. Jordan.
2001. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:2003.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
EACL, Athens, Greece.
Harr Chen, S.R.K. Branavan, Regina Barzilay, and
David R. Karger. 2009. Global models of document
structure using latent permutations. In Proceedings
of Human Language Technologies: The 2009 Annual
1187
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 371?
379, Boulder, Colorado, June. Association for Com-
putational Linguistics.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In EMNLP, pages
334?343.
Micha Elsner and Eugene Charniak. 2008a.
Coreference-inspired coherence modeling. In
Proceedings of ACL-08: HLT, Short Papers, pages
41?44, Columbus, Ohio, June. Association for
Computational Linguistics.
Micha Elsner and Eugene Charniak. 2008b. You talk-
ing to me? a corpus and algorithm for conversation
disentanglement. In Proceedings of ACL-08: HLT,
pages 834?842, Columbus, Ohio, June. Association
for Computational Linguistics.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence with
latent semantic analysis. Discourse Processes,
25(2&3):285?307.
Jennifer Foster. 2010. ?cba to check the spelling?: In-
vestigating parser performance on discussion forum
posts. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
381?384, Los Angeles, California, June. Association
for Computational Linguistics.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A sta-
tistical approach to anaphora resolution. In Proceed-
ings of the Sixth Workshop on Very Large Corpora,
pages 161?171, Orlando, Florida. Harcourt Brace.
Fred Glover and Manuel Laguna. 1997. Tabu Search.
University of Colorado at Boulder.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Simon Haykin and Zhe Chen. 2005. The Cocktail Party
Problem. Neural Computation, 17(9):1875?1902.
Nikiforos Karamanis, Massimo Poesio, Chris Mellish,
and Jon Oberlander. 2004. Evaluating centering-
based metrics of coherence. In ACL, pages 391?398.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In IJCAI, pages 1085?1090.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the annual meeting of ACL, 2003.
Mirella Lapata. 2006. Automatic evaluation of informa-
tion ordering: Kendall's tau. Computational Linguis-
tics, 32(4):1?14.
Gideon Mann, Ryan McDonald, Mehryar Mohri, Nathan
Silberman, and Dan Walker. 2009. Efcient large-
scale distributed training of conditional maximum en-
tropy models. In Y. Bengio, D. Schuurmans, J. Laf-
ferty, C. K. I. Williams, and A. Culotta, editors, Ad-
vances in Neural Information Processing Systems 22,
pages 1231?1239.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, Main Conference, pages 152?159.
David McClosky, Eugene Charniak, and Mark Johnson.
2010. Automatic domain adaptation for parsing. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 28?36,
Los Angeles, California, June. Association for Com-
putational Linguistics.
Eleni Miltsakaki and K. Kukich. 2004. Evaluation of text
coherence for electronic essay scoring systems. Nat.
Lang. Eng., 10(1):25?55.
Neville Moray. 1959. Attention in dichotic listening: Af-
fective cues and the inuence of instructions. Quar-
terly Journal of Experimental Psychology, 11(1):56?
60.
Ani Nenkova and Kathleen McKeown. 2003. Refer-
ences to named entities: a corpus study. In NAACL
'03, pages 70?72.
Malvina Nissim. 2006. Learning information status of
discourse entities. In Proceedings of EMNLP, pages
94?102, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Jacki O'Neill and David Martin. 2003. Text chat in ac-
tion. In GROUP '03: Proceedings of the 2003 inter-
national ACM SIGGROUP conference on Supporting
group work, pages 40?49, New York, NY, USA. ACM
Press.
Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unied framework for predicting text qual-
ity. In Proceedings of the 2008 Conference on Empir-
ical Methods in Natural Language Processing, pages
186?195, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Massimo Poesio, Mijail Alexandrov-Kabadjov, Renata
Vieira, Rodrigo Goulart, and Olga Uryupina. 2005.
Does discourse-new detection help denite description
resolution? In Proceedings of the Sixth International
Workshop on Computational Semantics, Tillburg.
Amruta Purandare and Diane J. Litman. 2008. Analyz-
ing dialog coherence using transition patterns in lexi-
cal and semantic features. In FLAIRS Conference'08,
pages 195?200.
Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.
2006. Thread detection in dynamic text message
1188
streams. In SIGIR '06: Proceedings of the 29th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 35?42,
New York, NY, USA. ACM.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of the Association for Computational Lin-
guistics Conference (ACL-2006).
Lidan Wang and Douglas W. Oard. 2009. Context-based
message expansion for disentanglement of interleaved
text conversations. In Proceedings of NAACL-09.
1189
