Generat ing  from a Deep Structure * 
Claire Gardent 
Universitfi Blaise Pascal- Clermont Ferrand (France) 
and University of Edinburgh 
2 Buccleuch Place 
Edinburgh EH8 9LW (Scotland) 
Agnes Plainfossfi 
Laboratoires de Marcoussis 
Route de Nozay 
91460 Marcoussis (France) 
ABSTRACT:  Noncanonical semantic representations 
are representations which cannot be derived by some 
grammar G although they are semanticMly equivalent to 
representations which can be derived by G. This paper 
presents a generation Mgorithm which deals with non- 
canonical input. The proposed approach also enhances 
portabil ity and language independence in that (i) linguis- 
tic decisions made by independent modules (e.g., planner, 
transfer component) can be communicated to the gener- 
ater in a naturM way and (ii) the same algorithm coupled 
with different grammars will yield sentences in the cor- 
responding languages. 
1 Introduct ion 
not derivable under G. Representations that cannot be 
derived under a grammar G are said to be noncanonical 
with respect to G. 
In this paper, we present a generation algorithm for Uni- 
fication Categorial Grammar \[6\] ( , ca )  which addresses 
the problem raised by non-canonicM input (section 2 and 
3). An interesting upshot of the strategy we propose is 
that it allows :\['or language independent generation. Sec- 
tion 4 il lustrates this point by considering how, starting 
from one semantic representation, two sentences can be 
generated: one in English and one in French. Section 5 
relates our work to previous proposals by Van Noord and 
Shieber. 
Two major requirements on a generator is that it be cor- 
rect and complete. A generator is said to be correct if 
given two semantic representations R1 and R.~ which are 
not semantically equivalent, R1 and R2 do not generate 
the same string. A generator is said to be complete if 
any two senmn tically equivalent representations generate 
the same set of strings. 
An obvious case of incompleteness occurs when the gen- 
erator fails to terminate on some well-defined input. An- 
other less obvious cause for incompleteness can be ex- 
plMned as follows. Consider a grammar G and its as- 
sociated semantic representation language L. It is often 
the case that syntactically different strings of L will have 
equivalent semantics. A simple case in point is the se- 
mantic equivalence holding between ? A ? and ? A ? in 
e.g. propositional ogic. On the other hand, it is also 
often the case that the same grammar G will not derive 
for a given string all the formulae which may represent 
its meaning. From the point of view of generation, this 
means that given two semanticMly equivalent represen- 
tations R1 and R2 there is always a possibility that R1 
generates a string S but that R2 doesn't because R2 is 
*The work reported here has been carried out ,as part of the 
ESPRIT project P393 ACORD on "The Construction and In- 
terrogation of Knowledge-B~es using Natural Language Text 
and Graphics". It is the result of joint work with Michael 
Reape of the Center for Cognitive Science, University of Ed- 
inburgh (Scotland, UK). 
2 Generating from a deep 
structure 
It is sometimes the case that a grammar will assign to a 
string several possible derivations with equivalent semen- 
ties. This phenomena is particularly acute in categorial 
grammars \[2\] and is referred to in the l iterature as that 
of spurious ambiguity. In grammars where the semantics 
is built by unification, the syntactic differences holding 
between these equivalent semantics resides in the relative 
ordering of the subformulae within the formula. That is, 
there is a direct relationship between the syntactic shape 
of a semantic formula and the derivational history of the 
corresponding string. Consequently, a given formula will 
be non-canonk:al wrt to a particular grammar G if the 
relative sequencing of its subformulae does not reflect a 
possible derivation in C .  Hence, to allow for generation 
from non-canonical input, we need to abstract away from 
the derivational information reflected in the linear order- 
ing of the input formula. Three major alternatives come 
to mind. First, we could try to generate all sentences 
whose semantics are logically equivalent o the input se- 
mantics. In uc(~, this means that generation is carried 
out with the two additional ogical axioms of associativ- 
ity and commutativity.  However, this solution produces a 
search space factorial in the number of conjunction s and 
must thus be rejected as computational ly intractable. 
1 127 
The second possibility is to define a semantic represen- 
tation language for which all well-formed formulas are 
in normal form. This approach is essentially unavailable 
to any grammar framework in which the semantics of a 
given expression results from the unification of partially 
specified semantic representations because normal forms 
can only be defined on languages with fully instantiated 
formulae. 
A third possibility consists in generating from an alterna- 
tive representation i.e. one that is related to but not iden- 
tical with the semantic representation used by the gram- 
mar. This is what we chose to do. The alternative rep- 
resentation we adopted is closely related to D-structure 
in cn theory where D-structure is a level of syntactic 
structure which mirrors semantic functor-argument de- 
pendencies. Syntactic information is encoded in terms 
of schematic X theory familiar from modern generative 
grammar. The deep structures (DS) we generate from 
consist of four types: heads, complements, modifiers and 
specifiers (we follow LEG f-structure and ucc subcate- 
gorisation structure in treating subjects as ordinary com- 
plements rather than specifiers of clauses) whereby Spec- 
ifiers are of the form: spec i f ier (Semant ics ,  Head). 
That is, they specify their own semantics and the prop- 
erties of their head. In contrast, Heads are of the 
form: head(Semantics, ArgList, AdjunctList) .  That 
is, they specify their own head semantics and a list of 
arguments and adjuncts which are also either specifier 
or head structures. All of these structures also allow the 
encoding of syntactic requirements onarguments and ad- 
juncts. 
The use of DSs has two other consequences. First, by 
allowing for the association of syntactic with semantic 
information, D-structures offer a way to mediate the re- 
sults of linguistic decisions made by an eventual planner 
to the generator. This may be useful. For instance, NP 
planning could be accounted for. In the present context, 
a planner is any system which given some information 
about what to say will return some decision about how 
to say it. For instance, if we want to expre,~s the fact 
that Jon runs, the planner will have to decide on how 
to refer to Jon, i.e. it could decide to describe him us- 
ing a complex NP as in 'the man with the red scarf who 
stands negt to Irene', or a pronoun e.g. 'he' or simply 
his name i.e. '.Ion'. The point is that the syntactic deci- 
sion made by the planner must be communicated to the 
generator. Since DSs contain syntactic information, they 
are a good candidate for the necessary interface between 
planner and generator. 
A second advantage of DSs is that because they are lan- 
guage independent, they allow for language independent 
generation. That is, for any acceptable input deep struc- 
ture, the algorithm presented below will generate .g., 
a French sentence if coupled with a UCG grammar for 
French and an English sentence if coupled with a coo 
grammar for English. This is only possible because the 
input deep structure the generation algorithm relies on 
is both sufficiently abstract o be language-independent 
and general enough that it can be mapped onto language 
dependent surface syntactic structures. Language inde- 
pendent generation is discussed in more detail in section 
4. 
In relation with the problem raised by non-canonical in- 
put, an important property of DSs is that they contain 
no indication of either surface syntactic order of the com- 
plements and adjuncts or of the relative scope of quan- 
tifiers occurring in either complements or modifiers. In- 
stead, thematic dependencies between subformulae are 
kept track of by the X schema where no reference is made 
to derivational history. The generator is thus free to real- 
ize both scope and surface syntactic structure in any way 
which is consistent with the deep structure specification 
and the particular grammar used. The reader might ob- 
ject to this elimination of scope distinctions. However, 
within UOG any scope distinctions which are produced by 
the individual grammars or as a result of some seman- 
tics construction process are in fact artefactual. Further- 
more, it might reasonably be argued that it should be 
possible to generate all possible scopes. This is typically 
done with quantifier shifting rules. Our solution is simply 
not to specify scope. 
An immediate consequence of using DSs is that non- 
canonical input is no longer a problem. The reason for 
this simply is that the generation algorithm no longer 
relies on the assumption that the input semantic repre- 
sentation i8 canonical i.e. derivable under the grammar 
used. Rather, the assumption is that the input will be 
some well-formed DS that will contain all the information 
contained in the corresponding semantics but none of the 
information embodied in the linear ordering of the for- 
mula about the derivational history of the corresponding 
string. 
3 The basic a lgor i thm 
3.1 A br ie f  in t roduct ion  to  UCG 
In UOO the basic linguistic unit is a sign which includes 
phonological, syntactic, semantic and ordering informa- 
tion. In the sequel, a sign will be represented either by 
a complex feature structure or as Pho :Synt :Sera: Drder. 
The phonological field of a sign contains its orthographic 
string. The syntactic field is categorial i.e. it can be 
either basic (e.g s,np,n etc) or complex in which case, 
it will be of the form C/S ign where C is a syntactic 
field and Sign is a sign. Moreover, any basic category 
can be assigned some morphosyntactic information. For 
instance s\[fin\] denotes the category sentence with mor- 
pholog-y feature value: finite. The semantic field contains 
the semantics ofthe expression whereby the semantic rep- 
resentation language is a linear version of Discourse Rep- 
resentation Theory in which each condition is preceded 
by a sorted variable called the index. As in most unifi- 
cation based grammars, the semantics of any expression 
results from the unification of the semantics of its sub- 
parts. Finally, the Order field is a binary feature with 
value either pre or post which constrains the applicabil- 
ity of grammar rules. 
Grammar ules in uco are of two types: binary and 
128 2 
unary. Binary rules include forward and backward func- 
tional application. These are stated below. 
Pho : (Synt /S ign) :Sem:0rder ,  Sign 
- -> Pho:Synt :Sem:Order  
if the order value of Sign is pro 
Sign, Pho:(Synt/Sign):Sem:Order 
--> Pho:Synt :Sem:0rder  
if the order value of Sign i8 post 
Unary rules are of the form c~ --+ fl where c, and fl are 
signs. Unary rules are used for the treatment of un- 
bounded dependencies, syntactic forms of type-raising 
and subcategorlsation for optional modifiers. 
aetive(SignO,Active), 
apply(SignOoActive,Result), 
retrleve(DS,SubDS,NewDS), 
generate(SubDS, Active), 
reduee(Result,Sign,NewDS). 
The algorithm presented above makes many simplifying 
assumptions which are incompatible with a wide coverage 
uoc  grammar.  To produce a complete generator with re- 
spect to uoo we need to extend the basic algorithm to 
account for type-raised NPs, identity semantic functors, 
lexical modifiers and unary rules. For more details on the 
general content of these extensions ee Ill. For their im- 
plementation cf. the listing of the generation algorithm 
given in the appendix. 
3.2 A sketch of the a lgor i thm 
Following work by \[11, \[5\] and \[3\], the algorithm we 
present here follows a mixed top-down and bottom-up 
strategy. 
The generation process starts with a deep structure DS 
and a sign Sign whose syntax embodies the goal cate- 
gory (e.g. sentence(finite)), get deepstr info extracts 
from the deep structure some semantic (Sere) and syntac- 
tic (Synt) information on the next sign to be generated. 
c reate  s ign  creates a new sign Sign0 on the basis of Sem 
and Syn~. Lexlcal look-up on Sign0 returns a sign with 
instantiated syntax and phonology. The call to reduce 
ensures that this lexical sign is reduced to the goal sign 
Sign in the process instantiating the generated string. 
generate(DS.  Sign) :-  
get_dsepst r_ in fo (DS, \ [Synt ,Sem\] ,RestQfDS) ,  
create sign(Synt,Sem,SignO), 
lexical(SignO), 
reduce(SignO,Sign0Rest0fDS). 
There are two main ways of reducing a sign Sign0 to a 
goalsign Sign. The base case occurs when Sign0 unifies 
with Sign and the deep-structure is empty i.e. all the 
input semantic material has been made use of in gener- 
ating the result string. The recursive case occurs when 
Sign0 is a syntactic functor. If the syntax of Sign0 is 
of the form Resu l t /Ac t ive ,  we apply Resu l t /Ac t ive  to 
Act ive  thus getting a new sign Resul t .  re t r ieve  non- 
deterministically retrieves from the current deep struc- 
ture DS, a substructure SubDS and returns the remaining 
deep-structure NewDS. The argument Act ive  is then gen- 
erated on the basis of the extracted sub..structure SubDS 
with a new goal sign whose syntax is that predicted by 
the syntactic functor Sign0. The resulting sign Resu l t  
is recursively reduced to the original go,~l sign Sign. 
reduce (Sign. S ign,  \[ \[\] 0 \[\] \] ) . 
reduce (SignO, Sign, DS) :- 
4 Bi l ingual Generat ion 
Consider the following synonymous entences. 
a The mou.,m misses the cat 
b Le chat manque ~l la souris 
(Lit. the cat misses to the mouse) 
(1) 
There are two main differences between (la) and (lb).  
First, a NP (the mouse) translates to a PP ( g~ la souria). 
Second, a structural transfer occurs i.e. the object NP in 
( la) becomes a subject in ( lb) and vice-versa. For the 
generator described above, this poses no particular prob- 
lem. Because DSs encode thematic rather than grammat-  
ical dependencies, tructural  transfer is no issue. Further, 
since at DS all arguments are represented as NPs x, the 
generation of ( la) is straightforward. Generating (lb) 
is a little more intricate but results naturally from the 
interaction of the generator with the grammar =. Note 
that if the PP  were represented as such in the DS, then 
generation would fail for the English sentence. This sug- 
gests that the deep structures we generate from offer the 
right level of abstraction for generation to be possible in 
several anguages. 
The case of structural  transfer i l lustrated in (1) is a good 
example of the problems that occur with generators that 
are unable to deal with non-canonical input. To il lustrate 
this consider the following situation. Suppose that given 
two grammars, one for Engl lsh(G~) and one for French 
(GF), ( la) and (lb) each have one unique derivation with 
resulting semantics as in (2). 
a The(mouse(m),  the(cat(c), miss(re,c)) 
b The(cat(c),  the(mouse(m), tulsa(re,c))) 
(2) 
Furthermore, suppose (3a) is non-canonlcal with respect 
to C,~ (i.e. (an) is not derivable under C,~) and (3b) is 
non-canonic.M wrt GE. For any generator G that cannot 
deal with non-canonical input, this means that G cannot 
be used in a system where parsing occurs on one language 
IThis is in accordance with the view that prepositions oc? 
curing within argumental PPs have no semantic ontent. 
2For more details on the generation of subcategorised PPs 
within UCG see \[1}. 
3 129  ' 
and generation on another. More to the point, if G is cou- 
pled with the grammar GE, then it will fail to generate 
when given (2b) as input - and similarly when coupled 
with GF and given input (2a). To understand why deep 
structures allow for grammar independent generation, 
let us first examine why tradit ional top-down/bottom- 
up generators uch as the one described in \[1\] fail on 
non-canonical input. 3 Consider the case where we try 
to generate under Gs  the English sentence in ( la) from 
the semantic (2b) and- as already mentioned- (2b) is 
non-canonical wrt GE. The main steps of the genera- 
tion process will be as follows. 4 Suppose the goal sign is 
SignO with category s\[fin\]. First, a sign Sig~l is created 
whose semantics i as in (2b). Lexical access on Signl re- 
turns the sign for 'the'. On the basis of the syntactic and 
semantic predictions made by Signl, the sign Sign2 for 
'cat' is then generated. Reduction of Signl with Sign2 
yields a new sign Sign3 with phonology 'the cat'and syn- 
tax C/(C/np) 5. In turn, Sign3 makes some predictions 
which lead to the generation of a new sign Sign4 with 
syntax C/(C/np) and phonology 'the mouse'. Finally, 
on the basis of Sign4, the sign Sign5 for 'miss' is gen- 
erated. At this point in generating, the two signs in (3) 
must combine to reduce to a sign with category C/np. 
pho:~i~) 
synt : C/ 
pho : Wa 
synt:C/ 
pho:themouse 
syat:np 
sem :m.mouse(m) 
order:Of 
sem:VP 
order:Of 
order :02  
pho : misses 
I pho : Wa 
synt :s/ synt : np\[nom\] 
sere : m.NPI 
order : pre 
sem : m.miss(m,c) 
order : 03 
pho : Wb 
/ sr t: npb  \] 
sere : c.NP2 
order : post 
But under ti~e UCG rules of combination (see 3.1), these 
two signs cannot combine because of the unification clash 
occuring between the semantics of the accusative NP in 
the verbal sign (c.NP2) and that of the NP sign within 
aNote that in this case, reduction to normal form is no 
longer a possible solution even if we were able to define a 
normal form for our semantic representation language. For 
suppose that (2a) is the normal form, then (lb) is not derivable 
and if (2b) is, then (la) is not derivable. 
4For more information on the details of the generation pro- 
cedure, see \[1\]. 
~For the sake of clarity, the syntactic part of Sign3 is here 
simplified in that non-syntactic fields (Phonology, Semantics 
etc.) are omitted. Note also that in UCG, NPs are typer- 
aised i.e they are assigned the syntactic ategory C/(C/np) 
as opposed\[ to just np. 
the sign for 'the mouse' (m.mouse(m)). Hence generation 
fails. Consider now how the problem is dealt with when 
generating from deep structures. Rather than being as 
indicated in (2b), the input to the generator is 6 
head(miss(m, e), 
\[specifier(the, head(moose(m), \[\], \[\])), 
specifier(the, head(cat(e), \[l, \[l))\] 
\[\]) 
(3) 
Roughly, generation will proceed as follows. Suppose the 
goal sign SignO has category s\[fin\]. First, the semantics 
corresponding to the head of the clause (i.e. mi,Js(m, c)) 
is extracted from (3) and a sign Signl is created with 
semantics miss(re, c). Lexical access on Signl returns 
the sign given in (3) above. Signl must then be re- 
duced to SignO with category s\[fin\]. At this stage, the 
remaining DS is \[specifler(the, head(mouse(m), \[\],\[\])), 
speci/ier(the, head(cat(c), \[l, \[\]))\] Togenerate the first ar- 
gument 
of Signl, we then have the choice between generating 
on the basis of specifier(the, head(mouse(m), \[\],\[\])) or 
of specifier(the, head(cat(c), \[\], \[1)) 7 As demonstrated 
above, if we generate the sign for 'the mouse' first, re- 
duction cannot apply and generation will fail. But here, 
failure is only temporary and on backtracking, the sign 
for 'the cat' will eventually be generated; it will then 
reduce with Signl to generate Sign2 with phonology 
'misses the cat'. At this point, the remaining DS will 
be \[specifier(the, head(mouse(m), \[\],\[\]))\]. This will trig- 
ger the generation of Sign3 with phonology 'the mouse' 
which will then combine with Sign2 to reduce to SignO 
with resulting phonology 'the mouse misses the cat'. 
To generate the French sentence 'Is chat manque h la 
8ouris ', the same generation process applies but this time 
in connection with GF and in a reverse order i.e. the sign 
for 'Is souris'(the mouse) is generated before the sign cor- 
responding to the NP 'Is chat' (the cat). Further, because 
in the French lexicon 'manque' (miss) subcategorises for 
a dative NP, the preposition ~ is generated and combined 
with the sign for 'Is souris' before reduction of the thus 
obtained PP  with the verb. Because DSs make no as- 
sumption about the linear ordering of the constituents 
to be generated, the problem raised by non-canonicity 
simply does not arise. 
5 Compar i sons  w i th  Re \ ] \ [a ted  
Research  
To compare our algorithm with previous work, we first 
show how it can be amended to phrase structure gram- 
mars. Consider the following extension to reduce. 
reduce(SignO, Sign, DS) :- 
rule(Morn, SignO, Kids), 
6For the sake of simplicity, the syntactic information usu- 
ally contained in the deep structures input to the generator is
here omitted. 
7cf. the non-determinism of the retr ieve predicate. 
130 4 
generate_s i s te rs  (Kids,  DS, NewDS), 
reduce(gem, Sign, NewDS). 
gene: t : 'a te_s is ters( \ [ \ ]  , DS, DS). 
gene:t:afie_sisfiers(\[HIT\], DS, NewDS) : -  
index (tI, Idx) ,  
me I; eh ( Idx,  DS, SubDS, NewDS 1 ) ,  
generate(SubDS,  H), 
generate  s i s te rs (T ,  NewDS1, llewDS). 
This clause is very similar in structure to the second 
clause of reduce,  the main difference being that the new 
claus(, makes fewer assumptions about the feature struo- 
tures being manipulated, ru le  enmnerates rules of the 
grammar, its first argument representing the mother con. 
stitu~ut, its second the head daughter and its third a list 
of non-head daughters which are to be recursively gener- 
ated by the predicate generate  s i s te rs .  The behaviour 
of this clause is just like that of the clause for reduce 
which implements the uc(; rules of function application. 
On tire basis of the generated lexical sign Sign0 an ap- 
plical.ion of the rule is hypothesised and we then attempt 
to prove that ru\]e application will lead to a new sign gem 
whiel, reduces to the original goal Sign. 
Having generalised our basic algorithm to phrase struc- 
ture ~ran\]mars, we can now compare it to previous work 
by \[5} and \[3\] 
Van Iqoord's Bottom-Up Generator (BUG) is very similar 
in structure to our basic algorkhm. Closer examination 
of the. two programs however eveals two differences. The 
first is that daugthers in a rule are separated into those 
that })recede the semantic head and those that follow it. 
The ,'.econd more meaningful difference involves the use 
of a ' l ink' predicate implementing the transitive closure 
of the semantic head relation over the grammar rules. 
The link predicate is similar in purpose to reachibility 
table~ in parsing algorithms and contributes to reducing 
the search space by producing some syntactic information 
on the sign to be generated. Itowever, such a predicate is 
of litt.le use when generating with a categorial grammar 
in particular and with any strongly lexicalist linguistic 
theory in general since in these, the grammar rules are 
extremely schematised. Their information content is so 
impoverished that the computat ion of and resort to a link 
predicate cannot be expected to reduce the search space 
in an/meaningf l f l  way. In the algorithm presented above 
however~ this shortcoming is redressed by exploiting the 
syntactic information contained in the deep-structure we 
start from. 
In \[5\], Shieber et al present a "semantic-head-driven" 
generation algorithm that is closely related to van No- 
ord's. In contrast to Van Noord's algorithm however, 
this ~dgorithm also operate on grammars violating the 
sema~dic head con.~traint (SHC) according to which any 
sema~tic representation is a further instantiation of the 
semantic representation of one of i~s constituents called 
the semantic head. This is achieved as follows. First, a 
distlnction is made between chain--rules and non-chain- 
rules whereby non-chain-rules are used to introduce se- 
mantic material syncategorematically. The distinction 
between the two types of rules can be sketched as fol- 
lows. 
i. Chain-rule (Sem, lhs --> Head(Sem), Sisters) 
2. Non-Chain-rule (Sem, lhs(Sem) --> Daughters) 
(1) indicates that given a semantic Sere, a chain rule 
will be such that Sere unifies with the head daughter's 
semantics whilst (2) shows that non-chMn-rules are such 
that the input semantics must unify with the semantics of 
the lhs of the rule. The intuition is that non-chain-rules 
will help find the lowest node in the derivation tree whose 
semantics unify with the input semantics. Furthermore, 
the top-down base case for non-chain-rules corresponds to 
the case in which the lhs of the rule has no non-terminal 
daughters i.e. to lexieal look up. Consider now the top 
call to generate. 
generate(Root )  :-  
non_chain ru le (Root ,P ivot ,P .hs ) ,  
generate  rhs(Rhs)0 
connect (P ivot ,Root ) .  
Two cases obtain with regard to the applicatlon of the 
non- cha in - ru le  predicate. Either the base case occurs 
and lexical look-up takes place exactly as in our algo- 
rithm or a non-chain-rule is triggered top-down before 
the conatituents in the rhs are generated by a recursive 
call to generate .  Hence the solution to the introduction 
of syncategorematic material  is essentially a reintroduc- 
tion of the top-down generation strategy. The result is 
that there is no guarantee that the algorithm will termi- 
nate. This point seems to have been overlooked in \[5 t. 
Therefore, the extension may be of less utility than it ap- 
pears to be at first sight although it may well be the case 
for linguistically motivated grammars that termination 
problems never arise. 
6 Fur ther  Research  
The general backtracking reghne characterising the al- 
gorithm presented in this paper means that  failure at a 
first attempt to generate might induce the recomputa- 
tion of partial results. Perhaps the use of a chart could 
contribute to enhance generation efficiency. Ii1 relation 
to \[4\] where chart edges contain no ordering information, 
it would be interesting to investigate whether during the 
generation process some ordering information can be re- 
covered. That is, whether the chart could be constructed 
in such a way that t:he relative positioning of edges mir- 
rors the knowledge mbodied in the grammar about lin- 
ear precedence within and between constituents. In this 
way, only the relevant part of the chart would need to be 
looked up before attempting to build a new edge. 
The algorithm described above is implemented ill CPro- 
log on a Sun4 and constitutes part of the generation cola- 
5 131  
ponent in the ACORD prototype. The generator can be 
coupled with either a UCG grammar for French or one 
for English thus generating either French or English sen- 
tences. 
References  
\[l\] Calder,  J., Reape,  M. and Zeevat, H. \[1989\] 
An Algorithm for Generation in Unification Cate- 
gorial Grammar. In Proceedings of the Fourth Con- 
ference of the European Chapter of the Association 
for Computational Linguistics, University of Manch- 
ester Institute of Science and Technology, Manch- 
ester, England, 10-12 April, 1989, 233-240. 
\[2\] Gardent, C., Bes, G., Jurle,P.F. and 
Baschung,  K. \[1989\] Efficient P;~rsing for French. 
In Proceedings of the 27th annual meeting of the As- 
sociation for Computational Linguistics, University 
of British Columbia. Vancouver, 26-29 June 1989, 
280-287. 
\[3\] van Noord,  G. \[1989\] BUG: A Directed Bottom 
Up Generator for Unification Based Formalisms. 
Manuscript. Department of Linguistics, University 
of Utrecht, M~rch 14, 1989. 
\[4\] Shieber, S. \[1988\] A Uniform Architecture for 
Parsing and Generation. In Proceedings of the 12th 
International Conference on Computational Linguis- 
tics, Budapest, 22-27 August, 1988, 614-619. 
\[5\] Shieber, S., van Noord ,  G., Moore,  R. and 
Pereira,  F .C.N.  \[1989\] A Semantic-Head-Driven 
Generation Algorithm for Unification-B,nsed For- 
malisms. In Proceedings of the 27th Annual Meet- 
ing of the Association/or Computational Linguistics. 
University of British Columbi.% Vancouver, British 
Columbia, Canada, 26-29 June, 1989, 7-17. 
\[61 Zeevat H., Klein, E. and Calder,  J. \[19871 An 
Introduction to Unification C~tegorial Grammar. In 
Haddock, N.J., Klein, E. and Morrill, G. (eds.) Edin- 
burgh Working Papers in Cognitive Science, Volume 
1: Categorial Grammar, Unification Grammar and 
Parsing. 
Listing of the whole program 
(Low level procedures have been omitted) 
generate(DeepStro Sign) :- 
get_deepstr_info(DeepStr?\[Synt. Sem\],RestOfDeepStr). 
create_sign(Synt?Sem?SignO), 
lexical(Sign0), 
reduce(SignO,Sign,Rest0fDeepStr). 
reduce(SignoSigno \[ \[\] o \[\] \] ). 
reduce(SignO?Sign? DeepStr) :- 
active(SignO,Active), 
apply(SignO?Active,Reault), 
retrieve(DeepStr,SubDeepStr,NewDeepStr)? 
generate(SubDeepStr, Active), 
reduce(ResultoSign?NewDeepStr). 
reduce(SignO, Sign. DeepStr) :- 
transform(SignO, Signl, DeepStr, NewDeepStr) 
reduce(Signl? Sign, NewDeepStr). 
Identity Semantic Fun(torn 
transform(Sign,NewSign,DeepStr,DeepStr) :- 
not_idsign(Sign). 
create_id_functor(IdSemFctor, Sign), 
identity(IdSemFctor), 
apply(NewSign. IdSemFctor0Sign)? 
defreeze_order(AdjSign, Sign, NewSign) 
% Lexical Adjtmcts 
traneform(Sign,NewSign0DS,NewDS) :- 
create_lexical_adjunct(Sign, 
ASign0DS,NewDS,DS2) o 
generer(DS2, ASign), 
apply(NewSign, ASign, Sign). 
Type-raise Verbs to C/(C/NP) 
transform(Sign,NewSign,DS,NewDS) :- 
type_raise vb to_np(Sign, RaisedSign), 
getjub deepstr(Sign0 DS, SubDS, NewDS) 
generer(SubDS, RaisedSign), 
apply(NewSign, RaisedSign, Sign) 
132  
Unary rules 
transform(Sign,NewSign?DeepStr,DeepStr) 
unary_rule(NewSign,Sign). 
Identity Semantic Functor 
(Case marking Prepositions) 
transform(Sign,NewSign,DeepStr,DeepStr) 
active(Sign,VB), 
active(VB, NP), 
category(NP, np) o 
create_id_prep(Np,PREP)? 
identity(PREP), 
