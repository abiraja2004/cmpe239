THE BOUNDARIES OF LANGUAGE GENERATION 
Neil M. Goldman 
USC - Information Sciences Inst itute 
I. INTRODUCTION 
In this paper I would like to address 
several basical ly independent issues 
relating to the processes of natural 
language generat ion (NLG) and research on 
model ing these processes. In the subsect ion 
"Paradigms for Generation" I maintain that, 
viewed at a moderately  abstract level, the 
vast major i ty of current research in this 
area falls into a single model and focuses 
on the "tail end" of the language generat ion 
process. The di f ference between indiv idual  
models seems to be based on di f fer ing 
assumptions or convict ions regarding the 
nature of "pre-generative" aspects of 
language use. 
The subsect ion "Conceptual Generat ion" 
describes the part icular ized version of this 
basic model within which I work. The 
assumptions under ly ing this approach and the 
aspects of language generat ion which it 
attempts to account for are stressed. 
The discussion of "Generat ion and 
Understanding" addresses the question of why 
a heavy bias can be seen in the volume of 
work (at least in the fields of 
computat ional  l inguist ics and Art i f ic ia l  
Intel l igence) on language understanding as 
opposed to language generation. A related 
quest ion - whether the two parts of language 
processing are suff ic ient ly dif ferent to 
warrant independent stature - is d iscussed 
briefly. 
The conclus ion of the paper points up 
some of the areas of inquiry which have 
scarcely been touched, yet which must be 
developed before we can claim to have a 
model of the overal l  process of language 
generation. 
II. PARADIGMS FOR GENERATION 
A stra ight forward interpretat ion of the 
term "natural  language generat ion" would 
al low that phrase to encompass all processes 
which contr ibute to the product ion of a 
natural  language expression, E, from some 
context, C. This leads to a "demonic" 
picture of generat ion as i l lustrated in 
Figure I. Certain contexts produced by 
non-generat ive processes in a model 
containing a NLG component tr igger that 
component. The language generator, in 
addit ion to producing E, must alter the 
context suf f ic ient ly  to inhibit the 
reproduct ion of E ad infinitum. 
While this picture is suf f ic ient ly  
general to encompass virtual ly any proposed 
generat ive model, it is so non-commital  that 
it does l itt le to expl icate NLG. The 
quest ion is merely resolved into two 
subissues: 
(I) What const i tutes an NLG-act ivat ing 
context? 
74 
(2) What processes and knowledge are 
needed to produce an appropr iate E 
in such a context? 
Now in fact (I) has not been addressed 
as a serious problem in most work on 
generation. The act ivat ing context has 
almost universal ly  been the existence of 
some " information to be communicated" in a 
d ist inguished cell in the context. Any 
process which "stores" into this cell 
immediately awakens the generator which 
proceeds to produce a natural  language 
encoding of that information. Context 
a l terat ion by the generator consists s imply 
of erasing the special  cell. 
The paradigm which has evolved out of 
this decision is depicted in Figure 2. 
Models based on this paradigm 
dif ferent iated primari ly by: 
are 
(I) The representat ions used for 
messages to be encoded by the 
generator. 
(2) The degree to which the generat ion 
box interacts with the context 
(context-sensit iv i ty of 
generation). 
The predominant formalisms for 
representing messages are: 
(a (partial.) specif icat ion of 
syntactic structure <I> 
(b semantic networks (consisting 
of case relations between 
semantic objects) <3,6> 
(c conceptual networks <2> 
The dividing line between semantic and 
conceptual networks is not clear-cut. The 
intended dist inct ion is that conceptual  
objects and conceptual relat ions are 
divorced from natural language, whereas 
semantic nets are constructed to represent 
meaning in terms of objects and relat ions 
speci f ical ly  designed for (some particular) 
natural language. 
Presumably one reason for separat ing 
i the select ion of a message from the task of 
encoding that message into natural language 
was to free research on "generation" from 
the necessity of deal ing with context. But 
in recent years our generation models have 
l become more and more context 
sensit ive - this is true at least of NLG 
models which treat message encoding as a 
subpart of some larger task. Some of these 
contextual considerat ions appear to be 
independent of any part icular target 
language - e.g., consultat ion of context in 
determining which features of an object 
should be mentioned in its descr ipt ion 
<7> - while others depend on detai led 
knowledge of the target language - e.g., the 
choice of verbs and nouns to be used in 
describing events <2>. The increased use of 
context is done to effect a more "natural" 
encoding of the message rather than simply a 
"legal" encoding. In this respect there are 
implicit in such NLG models certain 
assumptions about the use of context in 
language understanding. This matter will be 
elaborated somewhat later. 
The set of processes and knowledge 
needed to encode a message depends heavi ly 
on the message representat ion chosen. This 
existence of a formal grammar as the 
repository of syntactic knowledge about the 
target language has become standard 
practice; transit ion network grammars are 
representat ive of the current state of the 
art in this respect. The progression from 
syntactic to semantic to conceptual  
representat ions entails the use of 
progressively more knowledge about language 
and communicat ion.  A semantic net 
representat ion may need a theory of semantic 
cases and rules for mapping these into 
surface cases  of the target language;' a 
! 75 
conceptual representat ion requires complex 
rules and extensive data to choose 
appropriate words for the construct ion of 
the target language expression. 
III. CONCEPTUAL GENERATION 
My own work in NLG falls within the 
paradigm described above under the 
assumption that the message to be expressed 
is avai lable only in a conceptual 
representation. This means that neither the 
words of the target language (English) nor 
the syntactic structures appropriate for 
encoding the message are init ial ly avai lable 
to the generator. They must be deduced from 
the information content of the message. 
(Actually one exception to this claim is 
clearly present in the model - an init ial  
presumption is made that the message is 
encodeable  as a single English sentence. 
This is an unwarrented assumption which 
hides a potent ia l ly  s ignif icant problem.) 
Once actual words have been selected and 
organized into an Engl ish-speci f ic  syntactic 
structure, knowledge of Engl ish 
"l inearization" rules - e.g., that 
adject ives precede the nouns they 
modify - are used to produce a surface 
string. This knowledge is contained in an 
AFSTN grammar and uti l ized by a method 
introduced by Simmons and Slocum <6>. 
By working from a conceptual  
representation, a generator assumes the 
burden of account ing for two aspects of 
language product ion general ly ignored in 
other models. The first of these is word 
selection, which is accounted for by a 
pattern matching mechanism, namely decision 
trees (discr imination nets). In order to 
account for the select ion of appropr iate 
words, it is necessary to presume that the 
generator has extensive access to contextual  
information as well as access to inferent ial  
capabi l i t ies and bel ief structures. The 
second aspect of generation which must be 
addressed in the l inguistic encoding of 
conceptual  graphs concerns the expression of 
meaning by structure in addit ion to i t s  
expression by individual  words. The case 
framework of verbs is one source of 
knowledge which deals with structural  
encoding - e.g., in Engl ish the recipient of 
an object can be encoded as a syntactic 
SUBJECT if the verb "receive" is used to 
describe the transmission of that object. 
Other forms of structural  encoding are not 
determined by verb-related rules - e.g., 
that the construct ion <container> OF 
<contents> can be used in Engl ish to express 
the re lat ionship between a container and the 
object(s) it contains. 
The generat ion algor ithm demonstrates a 
mixture of data-dr iven and goal -dr iven 
behavior. In addit ion to the init ial  
goal - "generate a SENTENCE expressing the 
meaning of the given graph" - choices made 
in the course of generat ion set up 
sub-goals - e.g., "express the RECIPIENT of 
a t ransmiss ion and make it the SUBJECT of 
the structure being built." The conceptual  
content of the message, however, drives the 
select ion of a verb for the Engl ish sentence 
and the construct ion of "optional" 
structural  segments. 
The choice of conceptual structures as 
a base for NLG was not made because of any 
part icular designed (or accidental)  
suitabi l i ty of conceptual graphs for this 
purpose. Indeed it would be possible to 
alter the representat ions in ways which 
would s impl i fy the task of generation. But, 
if a NLG model is to be uti l ized as a means 
of t ransmitt ing information from a machine 
to a human, then the construct ion of that 
information is a prerequis ite of its 
encoding. More importantly, for uses of 
generation in " intel l igent" systems, the 
construct ion of the information is the most 
t ime-consuming process. For this reason 
conceptual  structures are designed to 
faci l i tate inference and memory integrat ion 
capacity <5> - if necessary, at the expense 
of ease of l inguistic analysis and 
generation. 
IV. UNDERSTANDING AND GENERATION 
For several years there has been a 
strong emphasis on the problem of 
understanding in computat ional  models, and 
relat ively l ittle on problem of generation. 
In the proceedings of the past two 
Internat ional  Joint Conferences on 
Art i f ic ia l  Intel l igence, for example, we 
find eight papers deal ing with the analysis 
of natural  language, three descr ibing both 
analyt ic and generat ive portions of language 
processing systems, and none devoted mainly 
to NLG. At least two reasons for this bias 
are discernable: 
(i) Resolut ion of ambiguity, long 
recognized as one of the central  
problems of language 
understanding, relies for its 
solut ion on capabi l i t ies - l imited 
inference, expectation, hypothesis 
generat ion and test ing - required 
by other "intel l igent" behavior. 
As long as language generat ion was 
viewed as basical ly a matter of 
codi fy ing l inguistic knowledge, it 
appeared far less relevant to the 
AI community than did analysis. 
(2) For those with a pragmatic bent 
the lack of symmetry between 
requirements of an analyzer and 
those of a generator made research 
on understanding of paramount 
importance. That is, for a given 
domain of discourse, a machine can 
afford to express information 
ut i l iz ing a l imited vocabulary and 
with l imited syntactic var iety 
without placing an unacceptable 
burden on a human conversant; to 
ask a human to adhere to 
equivalent l imitat ions in his own 
language production could prohibit  
the conduct of any interact ive 
dialogue (at least without 
extensive training). 
Furthermore, there exist a great 
many tasks which are current ly or 
76 
will soon be within the capacity 
of computers and which could be 
useful ly extended by a natural 
language "front end" - i.e., an 
analyzer. Corresponding needs for 
a natural language "back end" are I 
harder to find, perhaps because we I 
are so accustomed to using our m 
machines in the computat ion of 
numerical  or tabular data, which 
is seldom enhanced by expression 
in natural language. 
Being of a pragmatic bent myself, at 
least in spirit, I think the bias toward 
analysis is justif ied. But I expect that as 
the boundaries of generat ion are pushed back 
and more work is done on the semantic 
aspects of generation, the view of _ 
"analysis" and "generation" as disparate I 
endeavors wil l  change considerably. I see I far more Commonal i ty than disparity in the 
two enterprises. Both require much the same 
capacity for inference and deduction, albeit 
for dif ferent purposes. The knowledge of ? 
the syntactic structure of a language needed i to  understand that language is also needed 
to generate it, although the organizat ion of 
that knowledge may be different. A similar i 
condit ion holds for knowledge of word I meanings and mappings from syntactic 
structure to semantic or conceptual  
relations. Still, I do not bel ieve we are 
ready for, or should even be str iving for, a I single representat ion and organizat ion of 
this knowledge which would permit its being 
shared by both analyt ic and generat ive 
processes. But a good deal of the fruits of i 
research can be shared. I 
V. NEW DIRECTIONS | 
It seems to me that there exist several I 
problem areas in the development of a 
complete theory of language generat ion which 
have scarcely been touched. Some of these I 
could be, and possibly are being, prof i tably I addressed already; others seem to involve 
extremely long range goals. Into the latter 
category I would put the issue of message 
select ion referred to earlier. A theory I capable of account ing for message select ion 
in a general context would need a thorough 
mot ivat ional  model, probably of both the 
informat ion producing mechanism and the I 
human information "consumer'. Fortunately,  ~ I  
adequate heur ist ics for message select ion 
are much easier to develop for task specif ic 
domains, so lack of a general theory is not I f  
l ikely to hinder either research on or 
appl icat ion of language generat ion 
techniques. 
However, a great deal can be done in I 
the short range on the use of context in I generation: (I) as it relates to the 
determinat ion of message encoding, and (2) 
in the modi f icat ion of context in ways which 
affect later analysis, generation, and I reasoning processes. 
Another frontier of research is in the 
communicat ive rather than l inguist ic aspects I 
of NLG. "Message selection" has been used I in this paper to refer to the choice of 
! 
information to be conveyed to a human. The 
nature of human communication is such that 
it is generally necessary only to transmit a 
subpart of the totality of that message. 
Context and the understanding mechanisms of 
the information consumer are capable of 
filling in much vague or omitted 
information. Winograd's heuristic for 
describing toy blocks addresses precisely 
this issue - it amounts to an implicit model 
procedurally encoded in a generation 
program, of a process for finding the 
intended referent of an object description. 
While I would not push for incorporating 
explicit models of understanding in our 
generation models, I believe much could be 
gained by the addition of further implicit 
knowledge of this sort. 
BIBLIOGRAPHY 
<I> Friedman, J., A COMPUTER MODEL OF 
TRANSFORMATIONAL GRAMMAR, American 
Elsevier, New York, 1971 
<2> Goldman, N., "Sentence Paraphrasing 
from a Conceptual Base," CACM Vol. 18, 
No. 2, February 1975. 
<3> Klein, S., et. al., "Automatic Novel 
Writing: A Status Report," University 
of Wisconsin TR 186, August 1973. 
<4> Riesbeck, C., "Computational 
Understanding of Natural Language Using 
Context', AIM-238, Computer Science 
Department, Stanford University, 
Stanford, California. 
<5> Schank, R., et. al., "Inference and 
Paraphrase by Computer", Journal of the 
ACM, July 1975. 
<6> Simmons, R., and Slocum, J., 
"Generating English Discourse from 
Semantic Networks", CACM, Vol. 15, No. 
10, October 1972. 
<7> Winograd, T., "Procedures as. a 
Representation for Data in a Computer 
Program for Understanding Natural 
Language", TR-84, M. I. T. Project 
MAC, February 1971. 
77 
input  
. . . . . .  > 
st imul i  
IA 
N 
A <--I 
L I 
Y I 
s ---I 
I 
S < . . . .  
. . . . .  > 
I 
. . . . . . . . . .  I 
C O N T E X T  I< 
G 
E 
N 
E 
R 
A 
T 
I 
0 
N 
F igure  I 
input  
. . . . . .  > 
s t imu l i  
IA 
N 
A <-- 
L 
Y 
S - - -  
I 
S < . . . .  
- - . - ->  
I MESSAGE 
I SELECT ION 
IA  
l__ 
I I 
, I 
I l t----~J 
I . . . .  
C O N T E X T  
- - - -> 
I e rase  
< 
G 
E 
N 
E 
R 
A 
T 
I 
0 
N 
F igure  2 
Natura l  
. . . . . .  > 
Language 
Natura l  
Language 
" I  
1 
I 
I 
I 
I 
I 
I 
l 
I 
I 
I 
i 
I 
! 
I 
I 
I 
! 
