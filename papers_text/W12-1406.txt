JEP-TALN-RECITAL 2012, Atelier ILADI 2012: Interactions Langagi?res pour personnes Ag?es Dans les habitats Intelligents, pages 49?59,
Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCP
Contribution ? l??tude de la variabilit? de la voix des
personnes ?g?es en reconnaissance automatique de la parole
Fr?d?ric Aman, Michel Vacher, Solange Rossato, Fran?ois Portet
Laboratoire d?Informatique de Grenoble (UMR 5217), ?quipe GETALP
41 avenue des Math?matiques,
BP 53 - 38041 Grenoble Cedex 9 - France
{frederic.aman, michel.vacher, solange.rossato, francois.portet}@imag.fr
R?SUM?
L?utilisation de la reconnaissance vocale pour l?assistance ? la vie autonome se heurte ? la
difficult? d?utilisation des syst?mes de RAP qui ne sont pas pr?vus ? la base pour la voix ?g?e.
Pour caract?riser les diff?rences de comportement d?un syst?me de reconnaissance entre les
personnes ?g?es et non-?g?es, nous avons ?tudi? quels sont les phon?mes les moins bien reconnus
en nous basant sur le corpus AD80 que nous avons enregistr?. Les r?sultats montrent que certains
phon?mes tels que les plosives sont plus sp?cifiquement affect?s par l??ge. De plus nous avons
recueilli le corpus sp?cifique ERES38 afin d?adapter les mod?les acoustiques, avec pour r?sultat
une diminution du taux d?erreur de mot de 15%. Malgr? la grande variabilit? des performances,
nous avons caract?ris? comment la baisse des performances du syst?me de reconnaissance
automatique de la parole peut ?tre corr?l?e avec la baisse d?autonomie des personnes ?g?es.
ABSTRACT
Contribution to the study of elderly people?s voice variability in automatic speech recogni-
tion
Using speech recognition to support ambient assisted living is impeded by the difficulty of
using ASR systems that are not provided for the elderly voice. To characterize these differences
in speech recognition performance, we studied phoneme categories which lead to the lowest
recognition rate in the elderly speakers with respect to the younger ones based on the AD80
corpus that we recorded. The results showed that some phonemes (such as plosives) are more
specifically affected by age than others. Moreover, we collected the specific ERES38 corpus to
adapt the ASR acoustic model to the elderly population which resulted in a 15% decrease of
the word error rate. Despite a great variability of performances, we characterized how lower
performance of ASR systems can be correlated to the autonomy degradation of elderly people.
MOTS-CL?S : reconnaissance automatique de parole, voix des personnes ?g?es, adaptation
acoustique, assistance ? la vie autonome.
KEYWORDS: automatic speech recognition, ageing voice, acoustic adaptation, ambient assisted
living.
49
1 Introduction
L?assistance ? la vie autonome (ou Ambient Assisted Living - AAL) est devenu un enjeu important
?tant donn? la part croissante de la population ?g?e dans les pays industrialis?s. Par contre,
il reste encore ? l?heure actuelle assez peu de projets qui prennent en compte une interaction
vocale de la part de l?utilisateur, par exemple une commande vocale en domotique. Les personnes
qui b?n?ficieraient le plus de ces technologies seraient les personnes en perte d?autonomie, ?tant
donn? que le langage naturel est le moyen le plus spontan? de communication.
Dans ce contexte, le projet CIRDO 1 auquel participe le LIG vise ? favoriser l?autonomie et la prise
en charge des personnes ?g?es par les aidants au travers d?e-lio, un produit de t?l?lien social
augment? et automatis?. e-lio est un syst?me de communication en visiophonie s?adaptant au
degr? d?autonomie de son utilisateur. e-lio permet l?acc?s ? de nombreux services interactifs :
visiophonie, t?l?phonie, messages, partage de photos, rappels automatiques, appels d?urgence,
agenda partag?, plateforme domotique, entra?nement de la m?moire et de l?attention, etc.
L?objectif du projet CIRDO est d?y int?grer un syst?me de Reconnaissance Automatique de la
Parole (RAP) qui inclura une d?tection des signaux de d?tresse ainsi que des commandes vocales
en compl?ment de la t?l?commande.
L?utilisation de la reconnaissance vocale pour l?assistance ? la vie autonome se heurte ? la difficult?
d?utilisation des syst?mes de RAP qui ne sont pas pr?vus ? la base pour la voix ?g?e. En effet, du
fait de certaines caract?ristiques sp?cifiques de la voix ?g?e, un travail d?adaptation des syst?mes
de RAP a d? ?tre r?alis?. De fait, la parole ?g?e se caract?rise notamment par des tremblements
de la voix, une production impr?cise des consonnes, et une articulation plus lente (Ryan et
Burk, 1974). Du point de vue anatomique, des ?tudes ont montr? des d?g?n?rescences li?es
? l??ge avec une atrophie des cordes vocales, une calcification des cartilages du larynx, et des
changements dans la musculature du larynx (Takeda et al, 2000; Mueller et al, 1984). De
plus, des changements dans les capacit?s de contr?le moteur de la voix au niveau cognitif
modifient la production de la parole tout au long de la vie (Hooper et Cralidis, 2009). D?autres
?tudes (Georgila et al, 2008) ont montr? que lors de l?interaction avec un syst?me de dialogue -
incluant une RAP et une synth?se vocale - les personnes ?g?es utilisent, par rapport aux personnes
jeunes, un vocabulaire plus riche et des phrases plus longues, et emploient plus fr?quement des
expressions d?interaction sociale telles que "au revoir" ou "merci", comme s?il s?agissait d?une
interaction humain/humain. Du fait que les mod?les acoustiques des syst?mes de RAP sont
appris majoritairement sur de la voix non-?g?e, on observe donc une augmentation significative
du taux d?erreurs de mots pour la voix des personnes ?g?es par rapport ? la voix des adultes
non-?g?s (Baba et al, 2004; Vipperla et al, 2008, 2010; Aman et al, 2012).
Afin d?am?liorer le module de d?codage acoustico-phon?tique dans un syst?me de RAP et de
l?adapter ? la voix des personnes ?g?es, une premi?re analyse a consist? ? ?tudier les phon?mes
qui ?taient mal reconnus pour les personnes ?g?es. Cette analyse, pr?sent?e dans la section 2, a
permis d?extraire les phon?mes qui semblent plus probl?matiques ? reconna?tre que d?autres lors
du d?codage acoustico-phon?tique. Nous avons r?alis? une adaptation du mod?le acoustique,
d?taill?e en section 3. Nous montrons en section 4 que l??ge n?est pas le facteur d?terminant sur
la baisse des performances du syst?me de RAP, mais que le niveau de d?pendance de la personne
?g?e joue un r?le important. Nous concluons et pr?sentons les perspectives de recherche en
section 5.
1. http://liris.cnrs.fr/cirdo/
50
2 D?termination des phon?mes difficiles ? reconna?tre
2.1 Le corpus de test AD80
Afin d??valuer le comportement du syst?me de RAP sur la voix ?g?e, nous avons utilis? le
corpus Anondin-D?tresse 80 (AD80). Ce corpus, enregistr? par le laboratoire LIG, est sp?cifique
au domaine de la domotique et ? la d?tection d?appels de d?tresse. Ce corpus est constitu? de
l?enregistrement de 57 locuteurs (22 hommes et 35 femmes) ?g?s de 20 ? 94 ans. Il a ?t? demand?
aux participants de lire une liste de 126 ?nonc?s courts de la vie quotidienne ou caract?ristiques
d?un appel de d?tresse (ex : "Il fait chaud" ou "Aidez-moi"). Dans le cadre de l?application
envisag?e, nous cherchons essentiellement ? reconna?tre ce type d??nonc?s correspondant ? des
ordres domotiques ou des appels ? l?aide.
Le corpus AD80 est constitu? de deux groupes :
- Le groupe voix non-?g?es, constitu? de 21 locuteurs ?g?s de 20 ? 65 ans enregistr?s ? Grenoble
dans notre laboratoire en 2004. Tous les locuteurs ?taient actifs professionnellement ou ?tudiants.
Cette premi?re partie a ?t? enregistr?e lors d??tudes relatives ? la reconnaissance d?appels de
d?tresse dans un Habitat Intelligent pour la Sant? (HIS) (Vacher et al, 2006), ce qui explique
pourquoi il s?agit d??nonc?s courts. Le texte de ces ?nonc?s a ensuite ?t? utilis? pour des
?valuations en milieu r?el en parole distante dans un appartement ?quip? de microphones (Vacher
et al, 2008).
- Le groupe voix ?g?es, constitu? de 36 locuteurs ?g?s de 62 ? 94 ans enregistr?s dans un h?pital
et ? domicile ? Grenoble en 2010 ainsi que dans un centre de r?tablissement et dans une maison
de retraite dans le d?partement du Gard en 2012. Les locuteurs ?taient ? la retraite, et pour
certains en situation de d?pendance.
Au final, le corpus AD80 est form? de 6 848 phrases annot?es, avec 2 heures et 3 minutes
d?enregistrements audio (cf. Table 1).
Corpus AD80 Nombre locuteurs Age min-max Dur?e Nombre phrases
Groupe voix non-?g?es 21 20-65 38min 2646
Groupe voix ?g?es 36 62-94 1h25min 4202
Total 57 20-94 2h03min 6848
TABLE 1 ? Caract?ristiques du corpus AD80
2.2 Le syst?me de RAP
Le syst?me de RAP choisi pour notre ?tude est Sphinx3 (Seymore et al, 1998). Ce d?codeur
utilise un mod?le acoustique d?pendant du contexte avec cha?nes de Markov cach?es 3 ?tats. Les
vecteurs acoustiques sont compos?s de 13 coefficients MFCC, le delta et le double delta de chaque
coefficient. Le mod?le acoustique que nous utilisons a ?t? entra?n? sur le corpus BREF120 (Lamel
et al, 1991) qui est compos? de 100 heures de parole annot?es enregistr?es aupr?s de 120
locuteurs fran?ais. Nous avons appel? ce mod?le le mod?le acoustique g?n?rique.
Un mod?le de langage sp?cialis? a ?t? utilis?. Ce mod?le a ?t? construit ? partir des transcriptions
51
des phrases du corpus AD80, dont il a r?sult? un mod?le de langage restreint, de type trigramme,
avec un vocabulaire d?environ 160 mots.
2.3 Comparaison des taux d?erreurs de mots et des scores d?alignements
forc?s entre les groupes voix ?g?es et voix non-?g?es
Afin d??valuer l?effet de la voix ?g?e sur la performance de la RAP en utilisant le mod?le acoustique
g?n?rique, nous avons compar? le taux d?erreurs de mots (ou Word Error Rate - WER) entre les
groupes. Nous avons obtenu un WER de 7,33% pour le d?codage sur le groupe voix non-?g?es, et
un WER de 27,56% pour le d?codage sur le groupe voix ?g?es. Ainsi, nous avons observ? une
importante d?gradation du syst?me de RAP pour la voix des personnes ?g?es, avec une diff?rence
absolue de 20,23%.
Pour aller plus loin dans l?analyse, nous avons r?alis? un alignement forc? sur ces deux groupes.
L?alignement forc? consiste ? convertir les transcriptions de r?f?rence en suites de phon?mes
cal?s sur les donn?es audio en utilisant un dictionnaire phon?tique. L?alignement forc? a permis
d?obtenir les scores d?alignement par phon?me. Ceux-ci sont des scores de vraisemblance d?appar-
tenance au phon?me normalement prononc? pour la portion de signal consid?r?e. Ce score peut
?tre interpr?t? comme une proximit? avec la prononciation "standard", mod?lis?e par le mod?le
acoustique g?n?rique. Le score exprime le logarithme d?une vraisemblance, il est inf?rieur ou ?gal
? z?ro, et plus il est faible, plus le phon?me associ? est ?loign? du mod?le acoustique.
Les scores sont pr?sent?s sur la Figure 1 selon la cat?gorie phon?mique.
Plosivesnon-vois?es
Plosivesvois?es Fricativesnon-vois?es
Fricativesvois?es Nasalesetliquides
Voyellesferm?es Voyellesmoyennes Voyellesouvertes Voyellesnasales
?3
?2
?1
0
?104
Sco
red
?alig
nem
ent
forc
? Groupe voix non-?g?es Groupe voix ?g?es
FIGURE 1 ? Score d?alignement forc? par cat?gorie phon?mique avec le mod?le acoustique
g?n?rique pour les groupes voix non-?g?es et voix ?g?es
Pour le groupe voix non-?g?es, certains phon?mes montrent des valeurs plus faibles du score
d?alignement, tels que les plosives ou les voyelles ouvertes. D?autres sons, ? l?inverse, sont plus
proches des repr?sentations de mod?le acoustique : les fricatives.
Pour le groupe voix ?g?es, les scores d?alignement sont moins ?lev?s que ceux obtenus pour le
groupe voix non-?g?es. En effet, le vieillissement provoque une moins bonne ma?trise du syst?me
52
articulatoire, et la prononciation des personnes ?g?es s??loigne donc du mod?le acoustique. Les
consonnes plosives et les voyelles nasales sont les phon?mes n?cessitant le plus de contr?le
moteur et sont les plus difficiles ? articuler. Ceci se traduit au niveau des scores d?alignement, les
plus bas sont obtenus pour les consonnes plosives et les voyelles nasales.
Les ?carts de scores les plus importants par cat?gorie phon?mique sur le groupe voix ?g?es par
rapport au groupe voix non-?g?es ont permis de caract?riser quels sont les phon?mes posant le
plus de probl?mes pour la RAP des voix ?g?es. Les diff?rences relatives de scores observ?es entre
les deux groupes ont ?t? calcul?es. Les cat?gories phon?miques sont par ordre descendant de
diff?rence : les voyelles nasales (-117,00%), les consonnes fricatives non-vois?es (-110,56%),
les consonnes plosives non-vois?es (-105,72%), les consonnes fricatives vois?es (-87,86%), les
consonnes plosives vois?es (-83,29%), les voyelles moyennes (-63,74%), les voyelles ouvertes
(-53,21%), les voyelles ferm?es (-45,52%), et les consonnes nasales et liquides (-42,65%). En
se basant sur les diff?rences relatives, les cat?gories phon?miques les plus affect?es pour la
parole ?g?e sont les voyelles nasales, et les consonnes fricatives et plosives. Aussi, nous pouvons
noter que globalement les consonnes sont plus affect?es que les voyelles. De plus, l?absence de
voisement est le principal facteur de d?gradation, suivie par la modalit? de r?alisation (plosives
et fricatives). Ainsi, il serait possible que, en ce qui concerne les personnes ?g?es, les consonnes
non vois?es soient plus proches des consonnes vois?es.
Ces r?sultats sont similaires ? ceux obtenus par (Privat et al, 2004) qui ont trouv? une d?gradation
de la RAP entre voix ?g?e et voix non-?g?e avec une diff?rence relative tr?s proche de celle que
nous avons obtenue pour chaque cat?gorie phon?mique, except? pour les consonnes nasales et
liquides et pour les voyelles nasales o? leur syst?me ?tait moins performant que le notre.
3 Adaptation acoustique
3.1 Recueil du nouveau corpus ERES38
?tant donn?e la baisse de performance du syst?me de RAP pour la voix ?g?es, nous avons
enregistr? un nouveau corpus de parole de personnes ?g?es en vue de l?am?lioration du mod?le
acoustique gr?ce ? une m?thode d?adaptation acoustique. Les principales ?tapes de l??tude sont
r?sum?es Figure 2.
           
           
           



      
      
      
      




        
        
        
        




Enregistrement corpus
Evaluation
ERES38AD80 Voix ?g?e Voix non??g?e
Adaptation acoustique
FIGURE 2 ? Principales ?tapes de l??tude
Le corpus constitu? est un ensemble d?entretiens. Chaque entrevue met en relation une per-
53
sonne ?g?e avec deux exp?rimentateurs dont l?un se fait l?interlocuteur privil?gi?. Le mat?riel
utilis? pour les enregistrements ?tait un enregistreur TASCAM DR-100 avec un microphone ?
condensateur unidirectionnel plac? ? proximit? et en direction de la personne ?g?e. Une premi?re
partie introductive permet de r?cup?rer les informations personnelles ainsi que les habitudes
linguistiques du locuteur. Cette phase d?habituation avec le mat?riel d?enregistrement permet
d??tablir le passage vers une parole un peu plus informelle et spontan?e pour recueillir le r?cit de
vie de la personne, incluant une description des activit?s quotidiennes et de leur habitat, un r?cit
d?accidents ?ventuels et des anecdotes. Une activit? de lecture est ?galement propos?e lors de cet
entretien. Le support choisi est un article de jardinage cr?? par les exp?rimentateurs dans le but
de cibler les phon?mes probl?matiques. Les plosives et fricatives non vois?es ont ?t? introduites
de fa?on ? se retrouver en contexte /a/, /i/ et /u/.
Le corpus est constitu? de 17 heures et 44 minutes d?enregistrements (cf. Table 2) avec 24
locuteurs (16 femmes et 8 hommes) dont l??ge varie de 68 ? 98 ans, incluant 48 minutes de
lectures par 22 locuteurs. Ces locuteurs sont issus de structures sp?cifiques pour personnes ?g?es,
foyers logements ou maisons de retraite. Les entretiens ont ?t? effectu?s avec des personnes plus
ou moins autonomes, sans d?ficience cognitive, parfois avec de s?rieuses difficult?s motrices,
mais sans handicap lourd.
Corpus ERES38 Nombre locuteurs Age min-max Dur?e Nombre phrases
Lecture de texte 22 68-98 16h56min 300
Parole spontan?e 24 68-98 48min 7300
Total 24 68-98 17h44min 7600
TABLE 2 ? Caract?ristiques du corpus ERES38
Les enregistrements des entretiens ont commenc? ? ?tre transcrits, et toutes les lectures ont
?t? transcrites et v?rifi?es. Ces donn?es annot?es et structur?es constituent le corpus Entretiens
RESidences 38 (ERES38).
3.2 Adaptation MLLR
La m?thode d?adaptation de r?gression lin?aire du maximum de vraisemblance (Maximum Likeli-
hood Linear Regression ou MLLR) a ?t? utilis?e pour adapter le mod?le acoustique g?n?rique, appris
sur BREF120, ? la voix des personnes ?g?es. L?adaptation a ?t? faite globalement avec l?ensemble
des lectures du corpus ERES38. Nous avons ainsi obtenu un nouveau mod?le acoustique appel?
mod?le acoustique adapt? par MLLR.
A partir du corpus AD80, un premier d?codage a ?t? fait sur le groupe voix ?g?es en utilisant le
mod?le acoustique g?n?rique. Puis un second d?codage a ?t? effectu? sur ce m?me groupe avec le
mod?le acoustique adapt? par MLLR. Le but ?tait de voir dans quelle mesure est la diff?rence de
WER avec l?utilisation de l?un ou l?autre des mod?les, avec l?hypoth?se que le WER avec le groupe
voix ?g?es issu du d?codage avec le mod?le acoustique adapt? par MLLR serait proche du WER
avec le groupe voix non-?g?es issu du d?codage avec le mod?le acoustique g?n?rique. En outre,
nous avons r?alis? un d?codage avec le mod?le acoustique adapt? par MLLR sur le groupe voix
non-?g?es afin de tester la sp?cificit? de l?adaptation.
54
La Figure 3 montre que l?utilisation du mod?le acoustique adapt? par MLLR a permis de r?duire le
WER pour tous les locuteurs du groupe voix ?g?es. Avec l?adaptation MLLR globale, le WER est
de 11,95%. Compar? au WER de 27,56% sans adaptation, la diff?rence absolue est de -15,61%
(diff?rence relative de -56,65%). D?un point de vue applicatif, cela montre que l?on peut utiliser
une base de parole ?g?e pour l?adaptation MLLR dont les locuteurs sont diff?rents de ceux de la
base de test. Cela d?montre que les voix des personnes ?g?es ont des caract?ristiques propres
communes. De plus, nous voyons que l?utilisation d?un corpus de petite taille (48 minutes de
lecture par 22 locuteurs du corpus ERES38) pour l?adaptation MLLR globale est suffisante pour
donner une am?lioration significative avec un WER de 11,95%, proche du WER de 7,33% trouv?
dans le cas du d?codage de la parole non-?g?e.
0
10
20
30
40
0 10 20 30 40 50 60 70 80
WE
R a
pr?
s ad
apt
atio
n a
cou
stiq
ue 
WER avant adaptation acoustique 
FIGURE 3 ? WER avec adaptation acoustique MLLR en fonction du WER sans adaptation. La droite
repr?sente la fonction identit?
De plus, nous avons obtenu un WER de 10,39% lors du d?codage sur le groupe voix non-?g?es
avec le mod?le acoustique adapt? par MLLR. Cela repr?sente une d?gradation relative de 43,75%
compar?e au WER de 7,33% pour le d?codage avec le mod?le acoustique g?n?rique sur le m?me
groupe. Il est donc convenu que le mod?le acoustique adapt? par MLLR est sp?cifique ? la RAP sur
la population ?g?e.
4 Relation entre l?autonomie des personnes ?g?es et la RAP
Le WER issu du d?codage sur le groupe voix ?g?es avec le mod?le acoustique adapt? par MLLR est
repr?sent? en fonction de l??ge sur la Figure 4. Nous observons par la dispersion des points des
locuteurs repr?sent?s que l??ge est un mauvais indicateur du WER. De plus, nous avons calcul?
la corr?lation de Pearson entre les variables "WER apr?s adaptation acoustique" et "Age". Nous
avons trouv? un score de corr?lation de -0,053 (p = 0,759%) prouvant que le WER et l??ge
ne sont pas corr?l?s. En cons?quence, nous avons cherch? si d?autres param?tres relatifs ? la
d?pendance peuvent ?tre des indicateurs de la performance du syst?me de RAP.
Nous avons fait l?hypoth?se que la d?gradation physique et psychique affecte la production de
la parole et donc les performances de la RAP. Nous avons pris pour r?f?rence un test national
relatif ? l?autonomie des personnes ?g?es : la grille AGGIR (Autonomie G?rontologie Groupes
55
0
5
10
15
20
25
30
35
40
60 65 70 75 80 85 90 95
W
ER
 ap
r?s
 ad
ap
tat
ion
 ac
ou
sti
qu
e 
Age  
FIGURE 4 ? WER apr?s adaptation acoustique MLLR en fonction de l??ge, avec la droite correspon-
dant ? la r?gression lin?aire
Iso-Ressources) 2. Pour 29 locuteurs ?g?s en ?tablissement que nous avions enregistr?s pour la
constitution du corpus AD80, nous avons compl?t? pour chacun d?entre eux une grille AGGIR
avec l?aide du personnel soignant.
La grille AGGIR est un outil d??valuation du degr? de perte d?autonomie et de d?pendance, en
terme de d?gradation physique et psychique, pour l?attribution de l?Allocation Personnalis?e
d?Autonomie (APA), qui est une aide financi?re pour les personnes ?g?es d?pendantes en France.
L??valuation est faite ? partir de 17 variables. Dix variables se r?f?rent ? la perte d?autonomie
physique et psychique : coh?rence, orientation, toilette, habillage, alimentation, ?limination,
transferts (se lever, se coucher, s?asseoir), d?placement ? l?int?rieur, d?placement ? l?ext?rieur, et
communication ? distance. Sept variables se rapportent ? la perte d?autonomie domestique et
sociale : gestion personnelle de son budget et de ses biens, cuisine, m?nage, transports, achats,
suivi du traitement, et activit?s de temps libre. Chaque variable est cod?e par A (fait seul), B (fait
partiellement) ou C (ne fait pas). Le score GIR (Groupe Iso-Ressources) est calcul? ? partir des
variables afin de classer les personnes ?g?es dans un des six groupes : de GIR 1 (d?pendance
totale) ? GIR 6 (autonomie totale). Les personnes class?es de GIR 1 ? GIR 4 sont autoris?es ?
recevoir une aide financi?re selon leur degr? de d?pendance.
Nous avons regard? si le score GIR est repr?sentatif de la performance de la RAP. Le WER des 29
participants test?s sont repr?sent?s en fonction de leur score GIR en Figure 5. Quatre locuteurs
sont GIR 2, deux locuteurs sont GIR 3, quinze locuteurs sont GIR 4 et huit locuteurs sont GIR 6.
Aucun locuteur n?est repr?sent? en GIR 1 et GIR 5. Nous observons en Figure 5 que le score GIR
pourrait avoir une influence sur le WER, avec une baisse du WER en fonction de l?augmentation
du score GIR, sauf pour GIR 2.
Du fait du faible nombre de locuteurs en GIR 2 et GIR 3, nous avons rassembl? ces deux groupes
en un groupe nomm? GIR 2-3. Puis nous avons avons r?alis? une ANOVA sur GIR 2-3, GIR 4
et GIR 6 pour v?rifier si le score GIR pourrait avoir un effet significatif sur le WER. Nous avons
trouv? les r?sultats suivants : F(DDL, DDLer ror) = F(2, 26) = 3,7 ; p < 0.05%, prouvant qu?il y aau moins une distribution dont la moyenne diff?re des autres moyennes. Nous avons r?alis? un
2. http ://vosdroits.service-public.fr/F1229.xhtml
56
05
10
15
20
25
30
35
40
GIR 2 GIR 3 GIR 4 GIR 6
W
ER
 ap
r?
s a
da
pta
tio
n a
co
us
tiq
ue
 
Score GIR 
FIGURE 5 ? WER avec adaptation acoustique MLLR en fonction du score GIR
test post-hoc de Bonferroni afin de caract?riser quels groupes sont significativement diff?rents de
quels autres groupes. Le test post-hoc a r?v?l? que les groupes extr?mes GIR 2-3 et GIR 6 ont un
effet significativement diff?rent sur le WER, et que GIR 4 n?a pas une influence significativement
diff?rente sur le WER par rapport aux autres groupes.
De plus, nous avons r?alis? une ?tude pr?liminaire (que nous d?taillerons dans un prochain
article) sur les corr?lations entre le WER et chacun des 17 param?tres de la grille AGGIR. Il
semblerait que les param?tres concernant le contr?le moteur des membres sup?rieurs et la
continence pourrait ?tre les plus corr?l?s au WER. En effet, ces param?tres pourraient ?tre
repr?sentatifs d?une d?gradation physique g?n?ralis?e avanc?e affectant ?galement le contr?le
de la voix, et donc diminueraient la performance du syst?me de RAP. Une perspective pourrait
?tre de permettre une pr?diction du WER en se basant sur les caract?ristiques de d?pendance
des personnes ?g?es.
5 Conclusion
Cet article pr?sente notre ?tude sur le comportement d?un syst?me de RAP vis-?-vis de la voix ?g?e.
?tant donn? l?absence de corpus contenant de la voix de personnes ?g?es de langue fran?aise
utilisable pour la cr?ation ou l?adaptation des mod?les, nous avons proc?d? ? l?enregistrement
du corpus AD80. A partir de ce corpus, nous avons analys? quels ?taient les phon?mes pour
la voix ?g?e posant le plus probl?me au syst?me de RAP. Nous avons pu d?terminer que leur
?loignement par rapport ? la prononciation mod?lis?e par les mod?les acoustiques provoque une
augmentation du WER du syst?me de RAP, avec une diff?rence absolue entre voix non-?g?e et
?g?e de 20,23%. Ensuite, nous avons proc?d? ? l?adaptation du mod?le acoustique g?n?rique ? la
voix des personnes ?g?es, gr?ce ? la m?thode d?adaptation MLLR, ? partir du corpus ERES38. Le
cas de l?adaptation MLLR globale est int?ressante car avec moins d?une heure d?enregistrements, ?
partir de locuteurs diff?rents des locuteurs de test, nous avons obtenu des taux d?erreurs de mots
proches du cas d?une reconnaissance avec le mod?le acoustique g?n?rique de parole non-?g?e,
avec un WER de 11,95%, contre 27,56% avant adaptation. De plus, nous avons montr? que le
WER n?est pas corr?l? avec l??ge mais pourrait ?tre corr?l? avec le niveau de d?pendance de la
57
personne ?g?e du fait d?une d?gradation physique g?n?rale. La continuation de notre travail sera
de r?aliser une adaptation au locuteur et de montrer comment les diff?rents param?tres de la
grille AGGIR sont corr?l?s au WER. La pr?diction du comportement du syst?me de RAP permettra
de faciliter l?utilisation de ces nouvelles technologies dans la vie quotidienne des personnes ?g?es
d?pendantes. Aussi, nous allons ?tudier dans quelle mesure les sons non verbaux (inspirations,
bruits de bouche) ainsi que les h?sitations et les d?fauts d?articulation sont plus fr?quents chez
les personnes ?g?es, et une prochaine ?tape de notre travail sera de prendre en consid?ration ces
ph?nom?nes pour la construction des mod?les acoustiques et de langage.
Remerciements
Cette ?tude a ?t? financ?e par l?Agence Nationale de la Recherche dans le cadre du projet
CIRDO-Recherche Industrielle (ANR-2010-TECS-012). Nous remercions particuli?rement Remus
Dugheanu, Juline le Grand, Yuko Sasa, Claude Aynaud et Quentin Lefol pour leur active contri-
bution, ainsi que les diff?rentes personnes ?g?es et le personnel soignant qui ont accept? de
participer aux enregistrements.
R?f?rences
AMAN, F., VACHER, M., ROSSATO, S., DUGHEANU, R., PORTET, F., LE GRAND, J. et SASA, Y. (2012).
?tude de la performance des mod?les acoustiques pour des voix de personnes ?g?es en vue de
l?adaptation des syst?mes de RAP. In JEP, Grenoble, France.
BABA, A., YOSHIZAWA, S., YAMADA, M., LEE, A. et SHIKANO, K. (2004). Acoustic models of the
elderly for large-vocabulary continuous speech recognition. Electronics and Communications in
Japan, Part 2, 87:49?57.
GEORGILA, K., WOLTERS, M., KARAISKOS, V., KRONENTHAL, M., LOGIE, R., MAYO, N., MOORE, J.
et WATSON, M. (2008). A fully annotated corpus for studying the effect of cognitive ageing
on users? interactions with spoken dialogue systems. In Proceedings of the 6th International
Conference on Language Resources and Evaluation, Marrakech, Morocco.
HOOPER, C. et CRALIDIS, A. (2009). Normal changes in the speech of older adults : You?ve still
got what it takes ; it just takes a little longer ! Perspectives on Gerontology, 14:47?56.
LAMEL, L., GAUVAIN, J. et ESKENAZI, M. (1991). BREF, a large vocabulary spoken corpus for
french. In Proceedings of EUROSPEECH 91, volume 2, pages 505?508, Geneva, Switzerland.
MUELLER, P., SWEENEY, R. et BARIBEAU, L. (1984). Acoustic and morphologic study of the
senescent voice. Ear, Nose, and Throat Journal, 63:71?75.
PRIVAT, R., VIGOUROUX, N. et TRUILLET, P. (2004). Etude de l?effet du vieillissement sur les
productions langagi?res et sur les performances en reconnaissance automatique de la parole.
Revue Parole, 31-32:281?318.
RYAN, W. et BURK, K. (1974). Perceptual and acoustic correlates in the speech of males. Journal
of Communication Disorders, 7:181?192.
SEYMORE, K., STANLEY, C., DOH, S., ESKENAZI, M., GOUVEA, E., RAJ, B., RAVISHANKAR, M., RO-
SENFELD, R., SIEGLER, M., STERN, R. et THAYER, E. (1998). The 1997 CMU Sphinx-3 English
58
broadcast news transcription system. In DARPA Broadcast News Transcription and Understanding
Workshop, Lansdowne, VA, USA.
TAKEDA, N., THOMAS, G. et LUDLOW, C. (2000). Aging effects on motor units in the human
thyroarytenoid muscle. Laryngoscope, 110:1018?1025.
VACHER, M., FLEURY, A., SERIGNAT, J., NOURY, N. et GLASSON, H. (2008). Preliminary Evaluation
of Speech/Sound Recognition for Telemedicine Application in a Real Environment. In 9th Inter-
national Conference on Speech Science and Speech Technology (InterSpeech 2008), volume 1, pages
496?499, Brisbane Convention & Exhibition Centre (BCEC), Brisbane (Australia). Australasian
Speech Science and Technology Association (ASSTA).
VACHER, M., SERIGNAT, J.-F., CHAILLOL, S., ISTRATE, D. et POPESCU, V. (2006). Speech and Sound
Use in Remote Monitoring System for Health Care. In Faculty of INFORMATICS, M. U., ?diteur :
9th International Conference on Text, Speech and Dialogue (TSD 2006), volume 4148 de LNCS -
LNAI, pages 711?718, Faculty of Informatics, Masaryk University, Brno (Czech Republic).
VIPPERLA, R., RENALS, S. et FRANKEL, J. (2008). Longitudinal study of ASR performance on
ageing voices. Interspeech, pages 2550?2553.
VIPPERLA, R., RENALS, S. et FRANKEL, J. (2010). Ageing voices : The Effect of Changes in Voice
Parameters on ASR Performance. EURASIP Journal on Audio, Speech, and Music Processing.
59

