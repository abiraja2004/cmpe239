Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 94?101,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
A Hybrid Word Alignment Model for Phrase-Based Statistical Ma-
chine Translation 
 
 
Santanu Pal*, Sudip Kumar Naskar? and Sivaji Bandyopadhyay* 
*Department of Computer Science & Engineering 
Jadavpur University, Kolkata, India 
santanu.pal.ju@gmail.com, sivaji_cse_ju@yahoo.com 
?
 Department of Computer & System Sciences 
Visva-Bharati University, Santiniketan, India 
sudip.naskar@gmail.com 
 
  
 
Abstract 
This paper proposes a hybrid word alignment 
model for Phrase-Based Statistical Machine 
translation (PB-SMT). The proposed hybrid 
alignment model provides most informative 
alignment links which are offered by both un-
supervised and semi-supervised word align-
ment models. Two unsupervised word align-
ment models (GIZA++ and Berkeley aligner) 
and a rule based aligner are combined togeth-
er. The rule based aligner only aligns named 
entities (NEs) and chunks. The NEs are 
aligned through transliteration using a joint 
source-channel model. Chunks are aligned 
employing a bootstrapping approach by trans-
lating the source chunks into the target lan-
guage using a baseline PB-SMT system and 
subsequently validating the target chunks us-
ing a fuzzy matching technique against the 
target corpus. All the experiments are carried 
out after single-tokenizing the multi-word 
NEs.  Our best system provided significant 
improvements over the baseline as measured 
by BLEU.  
1 Introduction 
Word alignment is the backbone of PB-SMT sys-
tem or any data driven approaches to Machine 
Translation (MT) and it has received a lot of at-
tention in the area of statistical machine transla-
tion (SMT) (Brown et al, 1993; Och and Ney, 
2003; Koehn et al, 2003). Word alignment is not 
an end task in itself and is usually used as an in-
termediate step in SMT. Word alignment is de-
fined as the detection of corresponding alignment 
of words from parallel sentences that are transla-
tion of each other. Statistical machine translation 
usually suffers from many-to-many word links 
which existing statistical word alignment algo-
rithms can not handle well.  
The unsupervised word alignment models are 
based on IBM models 1?5 (Brown et al, 1993) 
and the HMM model (Ney and Vogel, 1996; Och 
and Ney, 2003). Models 3, 4 and 5 are based on 
fertility based models which are asymmetric. To 
improve alignment quality, the Berkeley Aligner 
is based on the symmetric property by intersect-
ing alignments induced in each translation direc-
tion. 
In the present work, we propose improvement 
of word alignment quality by combining three 
word alignment tables (i) GIZA++ alignment (ii) 
Berkeley Alignment and (iii) rule based align-
ment. Our objective is to perceive the effective-
ness of the Hybrid model in word alignment by 
improving the quality of translation in the SMT 
system. In the present work, we have implement-
ed a rule based alignment model by considering 
several types of chunks which are automatically 
extracted on the source side. Each individual 
source chunk is translated using a baseline PB-
SMT system and validated with the target chunks 
on the target side. The validated source-target 
chunks are added in the rule based alignment 
table. Work has been carried out into three direc-
tions: (i) three alignment tables are combined 
together by taking their union; (ii) extra align-
ment pairs are added into the alignment table. 
This is a well-known practice in domain adapta-
tion in SMT (Eck et al, 2004; Wu et al, 2008); 
(iii) the alignment table is updated through semi-
supervised alignment technique. 
94
The remainder of the paper is organized as fol-
lows. Section 2 discusses related work. The pro-
posed hybrid word alignment model is described 
in Section 3. Section 4 presents the tools and re-
sources used for the various experiments. Section 
5 includes the results obtained, together with 
some analysis. Section 6 concludes and provides 
avenues for further work. 
2 Related Works  
Zhou et al (2004) proposed a multi lingual filter-
ing algorithm that generates bilingual chunk 
alignment from Chinese-English parallel corpus. 
The algorithm has three steps, first, from the par-
allel corpus; the most frequent bilingual chunks 
are extracted. Secondly, the participating chunks 
for alignments are combined into a cluster and 
finally one English chunk is generated corre-
sponding to a Chinese chunk by analyzing the 
highest co-occurrences of English chunks. Bilin-
gual knowledge can be extracted using chunk 
alignment (Zhou et. al., 2004). Pal et, al. (2012) 
proposed a bootstrapping method for chunk 
alignment; they used an SMT based model for 
chunk translation and then aligned the source-
target chunk pairs after validating the translated 
chunk. Ma et. al. (2007) simplified the task of 
automatic word alignment as several consecutive 
words together correspond to a single word in the 
opposite language by using the word aligner it-
self, i.e., by bootstrapping on its output. A Max-
imum Entropy model based approach for Eng-
lish?Chinese NE alignment which significantly 
outperforms IBM Model4 and HMM has been 
proposed by Feng et al (2004). They considered 
4 features: translation score, transliteration score, 
source NE and target NE's co-occurrence score 
and the distortion score for distinguishing identi-
cal NEs in the same sentence. Moore (2003) pre-
sented an approach where capitalization cues 
have been used for identifying NEs on the Eng-
lish side. Statistical techniques are applied to de-
cide which portion of the target language corre-
sponds to the specified English NE, for simulta-
neous NE identification and translation. 
To improve the learning process of unlabeled 
data using labeled data (Chapelle et al, 2006), 
the semi-supervised learning method is the most 
useful learning technique. Semi-supervised 
learning is a broader area of Machine Learning. 
Researchers have begun to explore semi-
supervised word alignment models that use both 
labeled and unlabeled data. Fraser and Marcu 
(2006) proposed a semi-supervised training algo-
rithm. The weighting parameters are learned 
from discriminative error training on labeled da-
ta, and the parameters are estimated by maxi-
mum-likelihood EM training on unlabeled data. 
They have also used a log-linear model which is 
trained on the available labeled data to improve 
performance. Interpolating human alignments 
with automatic alignments has been proposed by 
Callison-Burch et al (2004), where the align-
ments of higher quality have gained much higher 
weight than the lower-quality alignments. Wu et 
al. (2006) have developed two separate models 
of standard EM algorithm which learn separately 
from both labeled and unlabeled data. Two mod-
els are then interpolated as a learner in the semi-
supervised Ada-Boost algorithm to improve 
word alignment. Ambati et al (2010) proposed 
active learning query strategies to identify highly 
uncertain or most informative alignment links 
under an unsupervised word alignment model. 
Intuitively, multiword NEs on the source and 
the target sides should be both aligned in the par-
allel corpus and translated as a whole. However, 
in the state-of-the-art PB-SMT systems, the con-
stituents of multiword NE are marked and 
aligned as parts of consecutive phrases, since 
PB-SMT (or any other approaches to SMT) does 
not generally treat multiword NEs as special to-
kens. This is the motivations behind considering 
NEs for special treatment in this work by con-
verting into single tokens that makes sure that 
PB-SMT also treats them as a whole 
Another problem with SMT systems is the er-
roneous word alignment. Sometimes some words 
are not translated in the SMT output sentence 
because of the mapping to NULL token or erro-
neous mapping during word alignment. Verb 
phrase translation also creates major problems. 
The words inside verb phrases are generally not 
aligned one-to-one; the alignments of the words 
inside source and target verb phrases are mostly 
many-to-many particularly so for the English?
Bengali language pair.  
The first objective of the present work is to see 
how single tokenization and alignment of NEs on 
both the sides affects the overall MT quality. The 
second objective is to see whether Hybrid word 
alignment model of both unsupervised and semi-
supervised techniques enhance the quality of 
translation in the SMT system rather than the 
single tokenized NE level parallel corpus applied 
to the hybrid model.  
We carried out the experiments on English?
Bengali translation task. Bengali shows high 
morphological richness at lexical level. Lan-
95
guage resources in Bengali are not widely avail-
able. 
3 Hybrid Word Alignment Model 
The hybrid word alignment model is described as 
the combination of three word alignment models 
as follows: 
3.1 Word Alignment Using GIZA++ 
GIZA++ (Och and Ney, 2003) is a statistical 
word alignment tool which incorporates all the 
IBM 1-5 models. GIZA++ facilitates fast devel-
opment of statistical machine translation (SMT) 
systems. In case of low-resource language pairs 
the quality of word alignments is typically quite 
low and it also deviates from the independence 
assumptions made by the generative models. 
Although huge amount of parallel data enables 
the model parameters to acquire better estimation, 
a large number of language pairs still lacks from 
the unavailability of sizeable amount of parallel 
data. GIZA++ has some draw-backs. It allows at 
most one source word to be aligned with each 
foreign word. To resolve this issue, some tech-
niques have already been applied such as: the 
parallel corpus is aligned bidirectionally; then the 
two alignment tables are reconciled using differ-
ent heuristics e.g., intersection, union, and most 
recently grow-diagonal-final and grow-diagonal-
final-and heuristics have been applied. In spite of 
these heuristics, the word alignment quality for 
low-resource language pairs is still low and calls 
for further improvement. We describe our ap-
proach of improving word alignment quality in 
the following three subsections. 
3.2 Word Alignment Using Berkley Aligner 
The recent advancements in word alignment is 
implemented in Berkeley Aligner (Liang et al, 
2006) which allows both unsupervised and su-
pervised approach to align word from parallel 
corpus. We initially train the parallel corpus us-
ing unsupervised technique. We make a few 
manual corrections to the alignment table pro-
duced by the unsupervised aligner. Then we ap-
ply this corrected alignment table as gold stand-
ard training data for the supervised aligner. The 
Berkeley aligner is an extension of the Cross Ex-
pectation Maximization word aligner. Berkeley 
aligner is a very useful word aligner because it 
allows for supervised training, enabling us to 
derive knowledge from already aligned parallel 
corpus or we can use the same corpus by updat-
ing the alignments using some rule based meth-
ods. Our approach deals with the latter case. The 
supervised technique of Berkeley aligner helps 
us to align those words which could not be 
aligned by rule based word aligner.  
3.3 Rule Based Word Alignment 
The proposed Rule based aligner aligns Named 
Entities (NEs) and chunks. For NE alignment, 
we first identify NEs from the source side (i.e. 
English) using Stanford NER.  The NEs on the 
target side (i.e. Bengali) are identified using a 
method described in (Ekbal and Bandyopadhyay, 
2009). The accuracy of the Bengali Named Enti-
ty recognizers (NER) is much poorer compared 
to that of English NER due to several reasons: (i) 
there is no capitalization cue for NEs in Bengali; 
(ii) most of the common nouns in Bengali are 
frequently used as proper nouns; (iii) suffixes 
(case markers, plural markers, emphasizers, 
specifiers) get attached to proper names as well 
in Bengali. Bengali shallow parser 1  has been 
used to improve the performance of NE identifi-
cation by considering proper names as NE.  
Therefore, NER and shallow parser are jointly 
employed to detect NEs from the Bengali sen-
tences. The source NEs are then transliterated 
using a modified joint source-channel model 
(Ekbal et al, 2006) and aligned to their target 
side equivalents following the approach of Pal et 
al. (2010). The target side equivalents NEs are 
transformed into canonical form after omitting 
their ?matras?. Similarly Bengali NEs are also 
transformed into canonical forms as Bengali NEs 
may differ in their choice of matras (vowel mod-
ifiers). The transliterated NEs are then matched 
with the corresponding parallel target NEs and 
finally we align the NEs if match is found.   
After identification of multiword NEs on both 
sides, we pre-processed the corpus by replacing 
space with the underscore character (?_?). We 
have used underscore (?_?) instead of hyphen (?-
?) since there already exists some hyphenated 
words in the corpus.  The use of the underscore 
(?_?) character also facilitates to de-tokenize the 
single-tokenized NEs after decoding. 
For chunk alignment, the source sentences of 
the parallel corpus are parsed using Stanford 
POS tagger. The chunks of the sentences are ex-
tracted using CRF chunker2. The chunker detects 
the boundaries of noun, verb, adjective, adverb 
                                                 
1 
http://ltrc.iiit.ac.in/showfile.php?filename=downloads/shallo
w_parser.php 
2 http://crfchunker.sourceforge.net/ 
96
and prepositional chunks from the sentences. In 
case of prepositional phrase chunks, we have 
taken a special attention: we have expanded the 
prepositional phrase chunk by examining a single 
noun chunk followed by a preposition or a series 
of noun chunks separated by conjunctions such 
as 'comma', 'and' etc.  For each individual chunk, 
the head word is identified. Similarly target side 
sentences are parsed using a shallow parser. The 
individual target side Bengali chunks are extract-
ed from the parsed sentences.  The head words 
for all individual chunks on the target side are 
also marked. If the translated head word of a 
source chunk matches with the headword of a 
target chunk then we hypothesize that these two 
chunks are translations of each other.  
The extracted source chunks are translated us-
ing a baseline SMT model trained on the same 
corpus. The translated chunks are validated 
against the target chunks found in the corre-
sponding target sentence. During the validation 
process, if any match is found between the trans-
lated chunk and a target chunk then the source 
chunk is directly aligned with the original target 
chunk. Otherwise, the source chunk is ignored in 
the current iteration for any possible alignment 
and is considered in the next iterations. 
 
 
 
 
 
 
Figure 1.a: Rule based alignments 
 
 
 
 
 
 
Figure 1.b: Gold standard alignments 
 
Figure 1: Establishing alignments through Rule 
based methods. 
 
The extracted chunks on the source side may 
not have a one to one correspondence with the 
target side chunks. The alignment validation pro-
cess is focused on the proper identification of the 
head words and not between the translated 
source chunk and target chunk. The matching 
process has been carried out using a fuzzy 
matching technique. If both sides contain only 
one chunk after aligning the remaining chunks 
then the alignment is trivial. After aligning the 
individual chunks, we also establish word align-
ments between the matching words in those 
aligned chunks. Thus we get a sentence level 
source-target word alignment table.  
Figure 1 shows how word alignments are es-
tablished between a source-target sentence pair 
using the rule based method. Figure 1.a shows 
the alignments obtained through rule based 
method. The solid links are established through 
transliteration (for NEs) and translation. The dot-
ted arrows are also probable candidates for intra-
chunk word alignments; however they are not 
considered in the present work. Figure 1.b shows 
the gold standard alignments for this sentence 
pair.  
3.4  Hybrid Word alignment Model  
The hybrid word alignment method combines 
three different kinds of word alignments ? Gi-
za++ word alignment with grow-diag-final-and 
(GDFA) heuristic, Berkeley aligner and rule 
based aligner. We have followed two different 
strategies to combine the three different word 
alignment tables.  
 
Union 
In the union method all the alignment tables are 
united together and duplicate entries are removed. 
 
ADD additional Alignments  
In this method we consider either of the align-
ments generated by GIZA++ GDFA (A1) or 
Berkeley aligner (A2) as the standard alignment 
as the rule based aligner fails to align all words 
in the parallel sentences. From the three set of 
alignments A1, A2 and A3, we propose an 
alignment combination method as described in 
algorithm 1. 
 
ALGORITHM: 1 
 
Step 1: Choose either A1 or A2 as the standard 
alignment (SA). 
Step 2: Correct the alignments in SA using the 
alignment table of A3. 
Step 3: if A2 is considered as SA then find addi-
tional alignment from A1 and A3 using intersec-
tion method (A1?A3) otherwise find additional 
alignment from A2 and A3 (using A2?A3).   
Step 4: Add additional entries with SA. 
[Jaipur] [golapi sohor name] [porichito] [.] 
[Jaipur] [is known] [as [Pink City]] [.] 
 
[Jaipur] [golapi sohor name] [porichito] [.] 
[Jaipur] [is known] [as [Pink City]] [.] 
 
97
3.5 Berkeley Semi-supervised Alignment 
The correctness of the alignments is verified by 
manually checking the performance of the vari-
ous alignment system. We start with the com-
bined alignment table which is produced by Al-
gorithm 1. Iinitially, we take a subset of the 
alignments by manually inspecting from the 
combined alignment table. Then we train the 
Barkley supervised aligner with this labeled data. 
A subset of the unlabeled data from the com-
bined alignment table is tested with the super-
vised model. The output is then added as addi-
tional labeled training data for the supervised 
training method for the next iteration. Using this 
bootstrapping approach, the amount of labeled 
training data for the supervised aligner is gradu-
ally increased. The process is continued until 
there are no more unlabelled training data. In this 
way we tune the whole alignment table for the 
entire parallel corpus. The process is carried out 
in a semi-supervised manner. 
4 Tools and resources Used  
A sentence-aligned English-Bengali parallel cor-
pus containing 23,492 parallel sentences from 
the travel and tourism domain has been used in 
the present work. The corpus has been collected 
from the consortium-mode project ?Development 
of English to Indian Languages Machine Trans-
lation (EILMT) System - Phase II? 3. The Stan-
ford Parser4 and CRF chunker5 have been used 
for identifying chunks and Stanford NER has 
been used to identify named entities in the source 
side of the parallel corpus.  
The target side (Bengali) sentences are parsed 
by using the tools obtained from the consortium 
mode project ?Development of Indian Language 
to Indian Language Machine Translation (IL-
ILMT) System - Phase II6?. 
The effectiveness of the present work has been 
tested by using the standard log-linear PB-SMT 
model as our baseline system: phrase-extraction 
heuristics described in (Koehn et al, 2003), , 
MERT (minimum-error-rate training) (Och, 
2003) on a held-out development set, target 
                                                 
3  The EILMT project is funded by the Department of Elec-
tronics and Information Technology (DEITY), Ministry of 
Communications and Information Technology (MCIT), 
Government of India. 
4 http://nlp.stanford.edu/software/lex-parser.shtml 
5 http://crfchunker.sourceforge.net/ 
6   The IL-ILMT project is funded by the Department of 
Electronics and Information Technology (DEITY), Ministry 
of Communications and Information Technology (MCIT), 
Government of India. 
language model trained using SRILM toolkit 
(Stolcke, 2002) with Kneser-Ney smoothing 
(Kneser and Ney, 1995) and the Moses decoder 
(Koehn et al, 2007) have been used in the 
present study. 
5 Experiments and Results 
We have randomly selected 500 sentences each 
for the development set and the test set from the 
initial parallel corpus. The rest are considered as 
the training corpus. The training corpus was fil-
tered with the maximum allowable sentence 
length of 100 words and sentence length ratio of 
1:2 (either way). Finally the training corpus con-
tained 22,492 sentences. In addition to the target 
side of the parallel corpus, a monolingual Benga-
li corpus containing 488,026 words from the 
tourism domain was used for building the target 
language model. We experimented with different 
n-gram settings for the language model and the 
maximum phrase length and found that a 4-gram 
language model and a maximum phrase length of 
7 produced the optimum baseline result. We car-
ried out the rest of the experiments using these 
settings. 
We experimented with the system  over 
various combinations of word alignment models. 
Our hypothesis focuses mainly on the theme that 
proper alignment of words will result in 
improvement of the system performance in terms 
of translation quality.  
141,821 chunks were identified from the 
source corpus, of which 96,438 (68%) chunks 
were aligned by the system. 39,931 and 28,107 
NEs were identified from the source and target 
sides of the parallel corpus respectively, of which 
22,273 NEs are unique in English and 22,010 
NEs in Bengali. A total of 14,023 NEs have been 
aligned through transliteration.  
The experiments have been carried out with 
various experimental settings: (i) single 
tokenization of NEs on both sides of the parallel 
corpus, (ii) using Berkeley Aligner with 
unsupervised training, (iii) union of the three 
alignment models: rule based, GIZA++ with 
GDFA and Berkeley Alignment, (iv) 
hybridization of the three alignment models and 
(v) supervised Berkeley Aligner. Eextrinsic 
evaluation was carried out on the MT quality 
using BLEU (Papineni et al, 2002) and NIST 
(Doddington, 2002). 
98
 
 
Experiment Exp 
no. 
BLEU NIST 
Baseline system using GIZA++ with GDFA 1 10.92 4.13 
PB-SMT system using Berkeley Aligner 2 11.42 4.16 
Union of all Alignments 3 11.12 4.14 
PB-SMT System with Hybrid Alignment by considering (a) 
GIZA++ as the standard alignment) (b) Berkeley alignment 
as the standard alignment) 
4a? 15.38 4.30 
4b? 15.92 4.36 
Single tokenized NE + Exp 1 5 11.68 4.17 
Single tokenized NE + Exp 2 6 11.82 4.19 
Single tokenized NE + (a) Exp 4a (b) Exp 4b 7a? 16.58 4.45 
7b? 17.12 4.49 
PB-SMT System with semi-supervised Berkeley Aligner + 
Single tokenized NE 
8? 20.87 4.71 
 
Table: 1 Evaluation results for different experimental setups. (The ??? marked systems produce statis-
tically significant improvements on BLEU over the baseline system) 
 
 
The baseline system (Exp 1) is the state-of-art 
PB-SMT system where GIZA++ with grow-diag-
final-and has been used as the word alignment 
model. Experiment 2 provides better results than 
experiment 1 which signifies that Berkeley 
Aligner performs better than GIZA++ for the 
English-Bengali translation task. The union of all 
thee alignments (Exp 3) provides better scores 
than the baseline; however it cannot beat the re-
sults obtained with the Berkeley Aligner alone. 
Hybrid alignment model with GIZA++ as the  
standard alignment (Exp 4a) produces statistical-
ly significant improvements over the baseline. 
Similarly the use of Berkeley Aligner as the 
standard alignment for hybrid alignment model 
(Exp 4b) also results in statistically significant 
improvements over Exp 2. These two experi-
ments (Exp 4a and 4b) demonstrate the effec-
tiveness of the hybrid alignment model. It is to 
be noticed that hybrid alignment model works 
better with the Berkeley Aligner than with 
GIZA++. 
Single-tokenization of the NEs (Exp 5, 6, 7a 
and 7b) improves the system performance to 
some extent over the corresponding experiments 
without single-tokenization (Exp 1, 2, 4a and 
4b); however, these improvements are not statis-
tically significant. The Berkeley semi-supervised 
alignment method using a bootstrapping ap-
proach together with single-tokenization of NEs 
provided the overall best performance in terms of 
both BLEU and NIST and the corresponding im-
provement is statistically significant on BLEU 
over rest of the experiments. 
6 Conclusion and Future Work 
The paper proposes a hybrid word alignment 
model for PB-SMT. The paper also shows how 
effective pre-processing of NEs in the parallel 
corpus and direct incorporation of their align-
ment in the word alignment model can improve 
SMT system performance. In data driven ap-
proaches to MT, specifically for scarce resource 
data, this approach can help to upgrade the state-
of-art machine translation quality as well as the 
word alignment quality. . The hybrid model with 
the use of the semi-supervised technique of the 
Berkeley word aligner in a bootstrapping manner, 
together with single tokenization of NEs, pro-
vides substantial improvements (9.95 BLEU 
points absolute, 91.1% relative) over the base-
line. On manual inspection of the output we 
found that our best system provides more accu-
99
rate lexical choice as well as better word order-
ing than the baseline system.  
As future work we would like to explore how 
to get the best out of multiple word alignments. 
Furthermore, integrating the knowledge about 
multi-word expressions into the word alignment 
models is another future direction for this work. 
 
Acknowledgement 
 
The work has been carried out with support from 
the project ?Development of English to Indian 
Languages Machine Translation (EILMT) Sys-
tem - Phase II? funded by Department of Infor-
mation Technology, Government of India. 
References  
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment. 
In ACL-44: Proceedings of the 21st International 
Conference on Computational Linguistics and the 
44th annual meeting of the Association for Compu-
tational Linguistics (ACL-2006), Morristown, NJ, 
USA. pages 769?776. 
Brown, Peter F., Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
mathematics of statistical machine translation: pa-
rameter estimation. Computational Linguistics, 
19(2):263-311. 
Chris Callison-Burch, David Talbot, and Miles Os-
borne. 2004. Statistical machine translation with 
word- and sentence-aligned parallel corpora. In 
ACL 2004, page 175, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics. 
Dempster, A.P., N.M. Laird, and D.B. Rubin. 1977). 
Maximum Likelihood from Incomplete Data via 
the EM Algorithm. Journal of the Royal Statistical 
Society, Series B (Methodological) 39 (1): 1?38. 
Doddington, George. 2002. Automatic evaluation of 
machine translation quality using n-gram cooccur-
rence statistics. In Proceedings of the Second In-
ternational Conference on Human Language Tech-
nology Research (HLT-2002), San Diego, CA, pp. 
128-132. 
Eck, Matthias, Stephan Vogel, and Alex Waibel. 
2004. Improving statistical machine translation in 
the medical domain using the Unified Medical 
Language System. In Proc. of the 20th Internation-
al Conference on Computational Linguistics (COL-
ING 2004), Geneva, Switzerland, pp. 792-798. 
Ekbal, Asif, and Sivaji Bandyopadhyay. 2008. Maxi-
mum Entropy Approach for Named Entity Recog-
nition in Indian Languages. International Journal 
for Computer Processing of Languages (IJCPOL), 
Vol. 21 (3), 205-237. 
Ekbal, Asif, and Sivaji Bandyopadhyay. 2009. Voted 
NER system using appropriate unlabeled data. In 
proceedings of the ACL-IJCNLP-2009 Named En-
tities Workshop (NEWS 2009), Suntec, Singapore, 
pp.202-210. 
Feng, Donghui, Yajuan Lv, and Ming Zhou. 2004. A 
new approach for English-Chinese named entity 
alignment. In Proceedings of the 2004 Conference 
on Empirical Methods in Natural Language Pro-
cessing (EMNLP-2004), Barcelona, Spain, pp. 
372-379. 
Feng, Donghui, Yajuan Lv, and Ming Zhou. 2004. A 
new approach for English-Chinese named entity 
alignment. In Proceedings of the 2004 Conference 
on Empirical Methods in Natural Language Pro-
cessing (EMNLP-2004), Barcelona, Spain, pp. 
372-379. 
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment 
models. Computational Linguistics, pages 19?51. 
Huang, Fei, Stephan Vogel, and Alex Waibel. 2003. 
Automatic extraction of named entity translingual 
equivalence based on multi-feature cost minimiza-
tion. In Proceedings of the ACL-2003 Workshop 
on Multilingual and Mixed-language Named Entity 
Recognition, 2003, Sapporo, Japan, pp. 9-16. 
HuaWu, HaifengWang, and Zhanyi Liu. 2006. Boost-
ing statistical word alignment using labeled and un-
labeled data. In Proceedings of the COLING/ACL 
on Main conference poster sessions, pages 913?
920, Morristown, NJ, USA. Association for Com-
putational Linguistics.  
Kneser, Reinhard, and Hermann Ney. 1995. Improved 
backing-off for m-gram language modeling. In 
Proceedings of the IEEE Internation Conference on 
Acoustics, Speech, and Signal Processing 
(ICASSP), vol. 1, pp. 181?184. Detroit, MI. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003: conference com-
bining Human Language Technology conference 
series and the North American Chapter of the As-
sociation for Computational Linguistics conference 
series,  Edmonton, Canada, pp. 48-54. 
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Ber-
toldi, Brooke Cowan, Wade Shen, Christine Mo-
ran, Richard Zens, Chris Dyer, Ond?ej Bojar, Alex-
andra Constantin, and Evan Herbst. 2007. Moses: 
open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual meeting of 
the Association for Computational Linguistics 
(ACL 2007): Proceedings of demo and poster ses-
sions, Prague, Czech Republic, pp. 177-180. 
Koehn, Philipp. 2004. Statistical significance tests for 
machine translation evaluation. In  EMNLP-2004: 
100
Proceedings of the 2004 Conference on Empirical 
Methods in Natural Language Processing, 25-26 
July 2004, Barcelona, Spain, pp 388-395. 
O. Chapelle, B. Sch?olkopf, and A. Zien, editors. 
2006. Semi-Supervised Learning. MIT Press, 
Cambridge, MA. 
Och, Franz J. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of 
the 41st Annual Meeting of the Association for 
Computational Linguistics (ACL-2003), Sapporo, 
Japan, pp. 160-167. 
Pal, Santanu, Sivaji Bandyopadhyay. 2012, ?Boot-
strapping Chunk Alignment in Phrase-Based Sta-
tistical Machine Translation?, Joint Workshop on 
Exploiting Synergies between Information Retriev-
al and Machine Translation (ESIRMT) and Hybrid 
Approaches to Machine Translation (HyTra), 
EACL-2012, Avignon, France, pp. 93-100 . 
Pal, Santanu., Sudip Kumar Naskar, Pavel Pecina, 
Sivaji Bandyopadhyay and Andy Way. 2010, Han-
dling Named Entities and Compound Verbs in 
Phrase-Based Statistical Machine Translation, In 
proc. of the workshop on Multiword expression: 
from theory to application (MWE-2010), The 23rd 
International conference of computational linguis-
tics (Coling 2010),Beijing, Chaina, pp. 46-54. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. In Proceed-
ings of the 40th Annual Meeting of the Association 
for Computational Linguistics (ACL-2002), Phila-
delphia, PA, pp. 311-318. 
Percy Liang, Ben Taskar, Dan Klein. 2006.  6th Pro-
ceedings of the main conference on Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association of Computational 
Linguistics, HLT-NAACL-2006, Pages 104-111 
Stolcke, A. SRILM?An Extensible Language Mod-
eling Toolkit. Proc. Intl. Conf. on Spoken Lan-
guage Processing, vol. 2, pp. 901?904, Denver 
(2002). 
Vamshi Ambati, Stephan Vogel, Jaime Carbonell. 
2010, 10th Proceedings of the NAACL HLT 2010 
Workshop on Active Learning for Natural Lan-
guage Processing (ALNLP-2010), Pages 10-17. 
Vogel, Stephan, Hermann Ney, and Christoph Till-
mann. 1996. HMM-based word alignment in statis-
tical translation. In Proc. of the 16th International 
Conference on Computational Linguistics (COL-
ING 1996), Copenhagen, pp. 836-841. 
Wu, Hua Haifeng Wang, and Chengqing Zong. 2008. 
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proc. of the 22nd International Conference 
on Computational Linguistics (COLING 2008),  
Manchester, UK, pp. 993-1000. 
X. Zhu. 2005. Semi-Supervised Learning Literature 
Survey. Technical Report 1530, Computer Scienc-
es, University of Wisconsin-Madison. 
http://www.cs.wisc.edu/_jerryzhu/pub/ssl_survey.p
df. 
 
101
