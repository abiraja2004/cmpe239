Response Generation in Collaborative Negotiation* 
J enn i fe r  Chu-Car ro l l  and Sandra  Carber ry  
Department of  Computer  and Information Sciences 
University of  Delaware 
Newark, DE 19716, USA 
E-marl: {jchu,carberry} @cis.udel.edu 
Abst rac t  
In collaborative planning activities, since the 
agents are autonomous and heterogeneous, it 
is inevitable that conflicts arise in their beliefs 
during the planning process. In cases where 
such conflicts are relevant to the t~t~k at hand, 
the agents hould engage in collaborative ne- 
gotiation as an attempt to square away the dis- 
crepancies in their beliefs. This paper presents 
a computational strategy for detecting conflicts 
regarding proposed beliefs and for engaging 
in collaborative n gotiation to resolve the con- 
flicts that warrant resolution. Our model is 
capable of selecting the most effective aspect 
to address in its pursuit of conflict resolution i  
cases where multiple conflicts arise, and of se- 
lecting appropriate evidence to justify the need 
for such modification. Furthermore, by cap- 
turing the negotiation process in a recursive 
Propose-Evaluate.Modify cycle of actions, our 
model can successfully handle embedded ne- 
gotiation subdialogues. 
1 In t roduct ion  
In collaborative consultation dialogues, the consultant 
and the executing agent collaborate on developing a plan 
to achieve the executing agent's domain goal. Since 
agents are autonomous and heterogeneous, it isinevitable 
that conflicts in their beliefs arise during the planning pro- 
cess. In such cases, collaborative agents hould attempt 
to square away (Joshi, 1982) the conflicts by engaging in 
collaborative negotiation to determine what should con- 
stitute their shared plan of actions and shared beliefs. 
Collaborative negotiation differs from non-collaborative 
negotiation and argum_entation mainly in the attitude of 
the participants, since collaborative agents are not self- 
centered, but act in a way as to benefit he agents as 
This material isbased upon work supported by the National 
Science Foundation under Grant No. IRI-9122026. 
a group. Thus, when facing a conflict, a collaborative 
agent should not automatically reject a belief with which 
she does not agree; instead, she should evaluate the belief 
and the evidence provided to her and adopt the belief if the 
evidence isconvincing. On the other hand, if the evalua- 
tion indicates that the agent should maintain her original 
belief, she should attempt to provide sufficient justifica- 
tion to convince the other agent o adopt his belief if the 
belief is relevant to the task at hand. 
This paper presents a model for engaging in collabo- 
rative negoa~ion to resolve conflicts in agents' beliefs 
about domain knowledge. Our model 1) detects con- 
flicts in beliefs and initiates a negotiation subdialogue 
only when the conflict is relevant to the current a.~k, 2) 
selects the most effective aspect to address in its pursuit 
of conflict resolution when multiple conflicts exist, 3) 
selects appropriate evidence to justify the system's pro- 
posed modification of the user's beliefs, and 4) captures 
the negotiation process in a recursive Propose-Evaluate- 
Mod/fy cycle of actions, thus enabling the system to han- 
dle embedded negotiation sulxlialognes. 
2 Related Work 
Researchers have studied the analysis and generation of 
arguments (Birnbaum et al, 1980; Reichman, 1981; Co- 
hen, 1987; Sycara, 1989; Quilici, 1992; Maybury, 1993); 
however, agents engaging in argumentative dialogues are 
solely interested in winning an argument and thus ex- 
hibit different behavior f om collaborative agents. Sidner 
(1992; 1994) formulated an artificial anguage for mod- 
eling collaborative discourse using propo~acceptance 
and proposal/rejection sequences; however, her work 
is descriptive and does not specify response generation 
strategies for agents involved in collaborative interac- 
tions. 
Webber and Joshi (1982) have noted the importance of
a cooperative system providing support for its responses. 
They identified strategies that a system can adopt in justi- 
fying its beliefs; however, they did not specify the criteria 
under which each of these strategies should be selected. 
136 
Walker (1994) described amethod of determining when 
to include optional warrants to justify a claim based on 
factors uch as communication cost, inference cost, and 
cost of memory retrieval. However, her model focuses on 
determining when to include informationally redundant 
utterances, whereas our model determines whether or not 
justification is needed for a claim to be convincing and, ff 
so, selects appropriate evidence from the system's private 
beliefs to support he claim. 
Caswey et al (Cawsey et al, 1993; Logan et al, 
1994) introduced the idea of utilizing a belief revision 
mechanism (Galliers, 1992) to predict whether a set of 
evidence is sufficient o change a user's existing belief 
and to generate responses for information retrieval di- 
alogues in a library domain. They argued that in the 
library dialogues they analyzed, "in no cases does ne- 
gotiation extend beyond the initial belief conflict and its 
immediate r solution:' (Logan et al, 1994, page 141). 
However, our analysis of naturally-occurring consultation 
dialogues (Columbia University Transcripts, 1985; SRI 
Transcripts, 1992) shows that in other domains conflict 
resolution does extend beyond asingle exchange of con- 
flicting befiefs; therefore we employ a re, cursive model 
for collaboration that captures extended negotiation and 
represents he structure of the discourse. Furthermore, 
their system deals with a single conflict, while our model 
selects a focus in its pursuit of conflict resolution when 
multiple conflicts arise. In addition, we provide aprocess 
for selecting among multiple possible pieces of evidence. 
3 Features  of  Col laborat ive Negot iat ion 
Collaborative negoti~ion occurs when conflicts arise 
among agents developing a shared plan 1 during collab- 
orative planning. A collaborative agent is driven by the 
goal of developing a plan that best satisfies the interests of 
all the agents as a group, instead of one that maximizes his 
own interest. This results in several distinctive f atures of 
collaborative negotiation: 1) A collaborative agent does 
not insist on winning an argument, and may change his 
beliefs ff another agent presents convincing justification 
for an opposing belief. This differentiates collaborative 
negotiation from argumentation (Birnbaum et al, 1980; 
Reichman, 1981; Cohen, 1987; Quilici, 1992). 2) Agents 
involved in collaborative negotiation are open and hon- 
est with one another; they will not deliberately present 
false information toother agents, present information i
such a way as to mislead the other agents, or strategi- 
cally hold back information from other agents for later 
use. This distinguishes collaborative negotiation from 
non-collaborative n gotiation such as labor negotiation 
(Sycara, 1989). 3) Collaborative agents are interested in
1The notion of shared plan has been used in (Grosz and 
Sidner, 1990; Allen, 1991). 
others' beliefs in order to decide whether to revise their 
own beliefs so as to come to agreement (Chu-Carroll and 
Carberry, 1995). Although agents involvedin argumenta- 
tion and non-collaborative negotiation take other agents' 
beliefs into consideration, they do so mainly to find weak 
points in their opponents' beliefs and attack them to win 
the argument. 
In our earlier work, we built on Sidner's pro- 
posal/acceptance and proposal/rejection sequences (Sit- 
net, 1994) and developed a model tha? captures collabo- 
rative planning processes in a Propose-Evaluate-Modify 
cycle of actions (Chu-Carroll and Carberry, 1994). This 
model views co l l~t ive  planning as agent A propos- 
ing a set of actions and beliefs to be i~ted  into the 
plan being developed, agent B evaluating the pro- 
posal to determine whether or not he accepts the proposal 
and, ff not, agent B proposing aset of modifications toA's 
original proposal. The proposed modifications will again 
be evaluated by A, and if conflicts arise, she may propose 
modifications to B's previously proposed modifications, 
resulting in a recursive process. However, our research 
did not specify, in cases where multiple conflicts arise, 
how an agent should identify which pm of an unaccept~ 
proposal to address or how to select evidence to support 
the proposed modification. This paper extends that work 
by i~t ing  into the modification process aslrategy 
to determine the aspect of the proposal that he agent will 
address in her pursuit of conflict resolution, as well as 
a means of selecting appropriate evidence to justify the 
need for such modification. 
4 Response Generation in Collaborative 
Negotiation 
In order to capture the agents' intentions conveyed by 
their utterances, our model of collaborative negotiation 
utilizes an enhanced version of the dialogue model de- 
scribed in (Lambert and Carberry, 1991) to represent 
the current status of the interaction. The enhanced di- 
alogue model has four levels: the domain level which 
consists of the domain plan being constructed for the 
user's later execution, the problem-solving level which 
contains the actions being performed toconstruct the do- 
n~n plan, the belief level which consists of the mutual 
beliefs pursued uring the planning process in order to 
further the problem-solving intentions, and the discourse 
level which contains the communicative actions initiated 
to achieve the mutual beliefs (Chu-Carroll and Carberry, 
1994). This paper focuses on the evaluation and mod- 
ification of proposed beliefs, and details a strategy for 
engaging incollaborative negotiations. 
137 
4.1 Evaluating Proposed Beliefs 
Our system maintains a set of beliefs about he domain 
and about the user's beliefs. Associated with each be- 
lief is a strength that represents he agent's confidence 
in holding that belief. We model the strength of a belief 
using endorsements, which are explicit records of factors 
that affect one's certainty in a hypothesis (Cohen, 1985), 
following (Galliers, 1992; Logan et al, 1994). Our en- 
dorsements are based on the semantics of the utterance 
used to convey abefief, the level of expertise of the agent 
conveying the belief, stereotypical knowledge, tc. 
The belief level of the dialogue model consists of mu- 
tual beliefs proposed by the agents' discourse actions. 
When an agent proposes a new belief and gives (optional) 
supporting evidence for it, this set of proposed beliefs is 
represented asa belief tree, where the belief represented 
by a child node is intended to support that represented by
its parent. The root nodes of these belief trees (rap-level 
beliefs) contribute to problem-solving actions and thus 
affect he domain plan being developed. Given a set of 
newly proposed beliefs, the system must decide whether 
to accept the proposal or m initiate anegotiation dialogue 
to resolve conflicts. The evaluation of proposed beliefs 
starts at the leaf nodes of the proposed belief trees since 
acceptance of a piece of proposed evidence may affect ac- 
ceptance of the parent belief it is intended to support. The 
process continues until the top-level proposed beliefs are 
evaluated. Conflict resolution strategies are invoked only 
if the top-level proposed beliefs are not accepted because 
if collaborative agents agree on a belief relevant to the 
domain plan being constructed, it is irrelevant whether 
they agree on the evidence for that belief (Young et al, 
1994). 
In determining whether to accept a proposed befief 
or evidential relationship, the evaluator first constructs 
an evidence set containing the system's evidence thin 
supports or attacks _bcl and the evidence accepted by 
the system that was proposed by the user as support for 
-bel. Each piece of evidence contains abelief _beli, and 
an evidential relationship supports(.beli,-bel). Follow- 
ing Walker's weakest link assumption (Walker, 1992) the 
strength of the evidence is the weaker of the strength of 
the belief and the strength of the evidential relationship. 
The evaluator then employs a simplified version of Gal- 
liers' belief revision mechanism 2 (Galliers, 1992; Logan 
et al, 1994) to compare the strengths of the evidence that 
supports and attacks _bel. If the strength of one set of evi- 
dence strongly outweighs that of the other, the decision to 
accept or reject.bel iseasily made. However, if the differ- 
ence in their strengths does not exceed apre-determined 
2For details on how our model determines the acceptance of 
a belief using the ranking of endorsements proposed by GaUiers, 
see (Chu-Carroll, 1995). 
..v.~ ..e~......n.~q.h..x~ ........................... ., 
~." -~ MB~3tSt-Teaches(Smith~I)) \] 
a ; 1 ~q.  , i\[MB~J,S,O.-S~,~KS,~th,n~,a ) ) ~, - . .  -. 
Dlsc~rse Level ", i ......... : ............................................................... ". "d 
"" "\[ lnf~J,S,~Teache~(Smi~ I i ,', 
\[Tell('O,S,-Teaches(Smith,AI))\] \[Address-Acceplance ~i ~' 
\[ I~?'m(U,S,O"-S~ic~(Smith,~= Ye'O) k~" 
\[ TeU(U,S,On-S~,t,~(Smith,~xt y~0) I 
,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  J 
Dr. Smith is not teaching AL 
Dr. Smith is going on sablmutical next year. 
Figure 1: Belief and Discourse Levels for (2) and (3) 
threshold, the evaluator has insufficient information to 
determine whether to adopt _bel and therefore will ini- 
tiate an information-sharing subdialogue (Cho-Carmll 
and Carberry, 1995) to share information with the user 
so that each of them can knowiedgably re-evaluate he 
user's original proposal. If, during infommtion-sharing, 
the user provides convincing support for a belief whose 
negation is held by the system, the system may adopt he 
belief after the re-evaluation process, thus resolving the 
conflict without negotiation. 
4.1.1 Example 
To illustrate the evaluation of proposed beliefs, con- 
sider the following uttermmes: 
(1) S: 1 think Dr. Smith is teaching AI  next 
semester. 
(2) U: Dr. Smith is not teaching AL 
(3) He is going on sabbatical next year. 
Figure 1 shows the belief and discourse levels of 
the dialogue model that captures utterances (2) and 
(3). The belief evaluation process will start with 
the belief at the leaf node of the proposed belief 
txee, On.Sabbatical(Smith, next year)). The system 
will first gather its evidence pe~aining to the belief, 
which includes I) a warranted belief ~ that Dr. Smith 
has postponed his sabbatical until 1997 (Postponed- 
Sabbatical(Smith, J997)), 2) a warranted belief that 
Dr. Smith postponing his sabbatical until 1997 sup- 
ports the belief that he is not going on sabbatical 
next year (supports(Postponed-Sabbatical(Smith,1997), 
-~On-SabbaticaI(Smith, next year)), 3) a strong belief 
that Dr. Smith will not be a visitor at IBM next year 
(-~visitor(Smith, IBM, next year)), and 4) a warranted 
belief that Dr. Smith not being a visitor at IBM next 
aThe strength of a belief is classified as: warranted, strong, 
or weak, based on the endorsement of the belief. 
138 
year supports the belief that he is not going on sab- 
batical next year (supports(-~visitor(Smith, IBM, next 
year), -,On-Sabbatical(Smith, next year)), perhaps be- 
cause Dr. Smith has expressed his desire to spend his sab- 
batical only at IBM). The belief revision mechanism will 
then be invoked to determine the system's belief about 
On-Sabbatical(Smith, next year) based on the system's 
own evidence and the user's statement. Since beliefs (1) 
and (2) above constitute a warranted piece of evidence 
against the proposed belief and beliefs (3) and (4) consti- 
tute a strong piece of evidence against it, the system will 
not accept On-Sabbatical(Smith, next year). 
The system believes that being on sabbatical implies a 
faculty member is not teaching any courses; thus the pro- 
posed evidential relationship will be accepted. However, 
the system will not accept he top-level proposed belief, 
-,Teaches(Smith, A/), since the system has a prior belief 
to the contrary (as expressed inutterance ( 1)) and the only 
evidence provided by the user was an implication whose 
antecedent was not accepted. 
4.2 Modifying Unaccepted Proposals 
The collaborative planning principle in (Whittak~ and 
Stenton, 1988; Walker, 1992) suggests that "conversants 
must provide vidence of a detected discrepancy in belief 
as soon as possible." Thus, once an agent detects a rele- 
vant conflict, she must notify the other agent of the con- 
flict and initiate a negotiation subdialogne toresolve i t - -  
to do otherwise is to fail in her responsibility as a collab- 
orative agent. We capture the attempt to resolve a con- 
flict with the problem-solving action Modify-Proposal, 
whose goal is to modify the proposal to a form that will 
potentially be accepted by both agents. When applied to 
belief modification, Modify-Proposal has two specializa- 
tions: Correct-Node, for when a proposed belief is not 
accepted, and Correct-Relation, for when a proposed ev- 
idential relationship is not accepted. Figure 2 shows the 
problem-solving recipes 4 for Correct-Node and its subac- 
tion, Modify-Node, that is responsible for the actual mod- 
ification of the proposal. The applicability conditions 5 of 
Correct-Node specify that the action can only be invoked 
when _sl believes that _node is not acceptable while _s2 
believes that it is (when _sl and _s2 disagree about he 
proposed belief represented by ..node). However, since 
this is a collaborative interaction, the actual modification 
can only be performed when both ..sl and _s2 believe that 
_node is not acceptable w that is, the conflict between 
_sl and .s2 must have been resolved. This is captured by 
4A recipe (Pollack, 1986) is a template for performing ac- 
tions. It contains the applicabifity conditions for performing an 
action, the subactions comprising the body of an action, etc. 
SApplicabflity conditions are conditions that must already 
be satisfied in order for an action to be reasonable to pursue, 
whereas an agent can try to achieve unsatisfied preconditions. 
Action: 
~y~: 
Appl Cond: 
Const: 
Body: 
Goal: 
Action: 
~ype: 
Appi Cond: 
Precond: 
Body: 
Goal: 
Figure 2: 
Correct-Node(_s I, .s2, .propow, d) 
Decomposition 
believe(_s 1,--acceptable(..node)) 
believe(_s2, acceptable(_node)) 
error-in-plan(_node,..proposed) 
Modify-Node(..s l,_s2,_proposed,..node) 
Insert-Correction(.s 1, ..s2, _proposed) 
accoptable(_proposed) 
Modify-Node(..s I ,..s2,.4noposed,.suxle) 
Specialization 
believe( .s1, .-,acceptable( ...node ) ) 
believe(.s2,-,acceptable(_node)) 
Remove-Node(_sl,_s2,_proposed,..node) 
Alter-Node(.s l,_s2,.proposed,.node) 
mod~ed(.proposed) 
The Correct-Node and Modify-Node Recipes 
the applicability condition and precondition of Mod/fy- 
Node. ~ attempt to satisfy the precondition causes the 
system to post as a mutual belief to be achieved the belief 
that ..node is not acceptable, leading the system to adopt 
discourse actions to change _s2's beliefs, thus initiating a
collaborative negotiation subdialogne, e 
4.2,1 Selecting the Focus of Modification 
When multiple conflicts arise between the system and 
the user regarding the user's proposal, the system must 
identify the aspect of the proposal on which it should fo- 
cus in its pursuit of conflict resolution. For example, in 
the case where Correct-Node isselected as the specializa- 
tion of Modify-Proposal, the system must determine how 
the parameter node in Correct-Node should be instanti- 
ated. The goal of the modification process is to resolve 
the agents' conflicts regarding the unaccepted top-level 
proposed beliefs. For each such belief, the system could 
provide vidence against the befief itself, address the un- 
accepted evidence proposed by the user to eliminate the 
user's justification for the belief, or both. Since collab- 
orative agents are expected to engage in effective and 
efficient dialogues, the system should address the unac- 
cepted belief that it predicts will most quickly resolve 
the top-level conflict. Therefore, for each unaccepted 
top-level belief, our process for selecting the focus of 
modificatkm involves two steps: identifying acandidate 
foci tree from the proposed belief tree, and selecting a
eThis subdialogue is considered an interrupt by Whittaker, 
Stenton, and Walker (Whittaker and Stenton, 1988; Walker and 
Whittaker, 1990), initiated to negotiate he truth of a piece of in- 
formation. However, the utterances they classify as interrupts 
include not only our negotiation subdialogues, generated for 
the purpose of modifying a proposal, but also clarification sub- 
dialogues, and information-sharing subdialogues (Chu-Carroll 
and Carberry, 1995), which we contend should be part of the 
evaluation process. 
139 
focus from the candidate foci tree using the heuristic "at- 
tack the belief(s) that will most likely resolve the conflict 
about he top-level belief." A candidate loci tree contains 
the pieces of evidence in a proposed belief tree which, if 
disbelieved by the user, might change the user's view of 
the unaccepted top-level proposed belief (the root node 
of that belief tree). It is identified by performing a depth- 
first search on the proposed belief tree. When a node 
is visited, both the belief and the evidential relationship 
between it and its parent are examined. If  both the be- 
lief and relationship were accepted by the evaluator, the 
search on the current branch will terminate, since once the 
system accepts abelief, it is irrelevant whether it accepts 
the user's support for that belief (Young et al, 1994). 
Otherwise, this piece of evidence will be included in the 
candidate loci tree and the system will continue to search 
through the evidence in the belief tree proposed as support 
for the unaccepted belief and/or evidential relationship. 
Once a candidate foci tree is identified, the system 
should select he focus of modification based on the like- 
lihood of each choice changing the user's belief about 
the top-level belief. Figure 3 shows our algorithm for 
this selection process. Given an unaccept~ belief (.bel) 
and the beliefs proposed to support it, Select-Focus. 
Modification will annotate_bel with 1) its focus of mod- 
ification (.bel.focus), which contains aset of beliefs (.bel 
and/or its descendents) which, if disbelieved by the user, 
are predicted to cause him to disbelieve _bel, and 2) the 
system's evidence against_bel itself (_hel.s-attack). 
Select-Focus-Modification determines whether to at- 
tack _bel's supporting evidence separately, thereby elim- 
inating the user's reasons for holding ..b?l, to at ta~ ..bel 
itself, or both. However, in evainating the effectiveness of 
attacking the proposed evidence for.bel, the system must 
determine whether or not it is possible to successfully re- 
fute a piece of evidence (i.e., whether or not the system 
believes that sufficient evidence is available to convince 
the user that a piece of proposed evidence is invalid), and 
if so, whether it is mote effective to attack the evidence it- 
self or its support. Thus the algorithm recursively applies 
itself to the evidence proposed as support for _bel which 
was not accepted by the system (step 3). In this recursive 
process, the algorithm annotates each unaccepted belief 
or evidential relationship roposed to support _bel with 
its focus of modification (-beli.focus) and the system's 
evidence against it (_beli.s-attack). _bell.focus contains 
the beliefs selected to be addressed in order to change the 
user's belief about ..beli, and its value will be nil if the 
system predicts that insufficient evidence is available to 
change the user's belief about -bell. 
Based on the information obtained in step 3, Select. 
Focus-Modification decides whether to attack the evi- 
dence proposed to support _bel, or _bel itself (step 4). 
Its preference is to address the unaccepted evidence, be- 
Select  .Focus-Modlf lcat lon(_bel) :  
1. _bel.u-evid +-- system's beliefs about he user's evidence 
pertaining to _bel 
_bel.s-attack 4- system's own evidence against _bel 
2. If _bel is a leaf node in the candidate foci tree, 
2.1 If Predict(_bel, _bel.u-evid + _bel.s-attack) = -~_bel 
then _bel.focus ,-- .bel; return 
2.2 Else .bel.focus t -  nil; return 
3. Select focus for each of .bel's children in the candidate 
foci tree, .belx ..... ..bel,~: 
3.1 If supports(_beli,_bel) is accepted but .beli is not, 
Select-Focus-Modlficatioa(.bel~ ). 
3.2 Else if .beli is accepted but supports(_beli,.bel) is 
not, Sdect-Focus-Modlficatlon(.beli,.bel). 
3.3 Else Select-Focu-Modificatioa(.bel~) and Select- 
Focus-Modification( supports(_beli ,.bel)) 
4. Choose between attacking the Woposed evidence for .bel 
and attacking ..bel itself: 
4.1 eand-set ~-- {..beli I.beli E unaccepted user evidence 
for _bel A ..beli.focus ~ nil} 
4.2 //Check if addressing _bol's unaccepted evidence is 
suffu:ient 
If Predkt(.bel, _bel.u-evid - cand-set) = --,.~l (i.e., 
the user's disbelief in all unaecepted vidence which 
. the system can refute will cause him to reject _bel), 
min-set ~- Select-Mtu-Set(_bel,cand-set) 
..bel.focus ~- U_bel~ ?_min-set ..beli.focus 
4.3 //Check if addressing .bel itself is s~fcient 
Else if Predlct(.bel, ..bel.u-evid + .bel.s-attack) =
-,.bel (i.e., the system's evidence against .bel will 
cause the user to reject _bel), 
.bel.focus ~-- .bel 
4.4 //Check if addressing both .l~el and its unaccepted 
evidence is s~Ofcient 
Else if Predkt(..bel, _bel.s-attaek + .bel.u-evid - 
canal-set) =-,_bet, 
rain-set +-- Select-Mln-Set(.beL cand-set +_bel) 
.bel.focus +-- U.beli~dnin-set ..beli.focus U .bel 
4.5 Else _bel.focus +-- nil 
Figure 3: Selecting the Focus of Modification 
cause McKeown's focusing rules suggest that continuing 
a newly introduced topic (about which there is more to be 
said) is preferable to returning to a previous topic OVIcK- 
cown, 1985). Thus the algorithm first considers whether 
or not attacking the user's upport for ..bel is sufficient to 
convince him of--,-bel (step 4.2). It does so by gathering 
(in cand-set) evidence proposed by the user as direct sup- 
port for _bel but which was not accepted by the system 
and which the system predicts it can successfully refute 
(i.e., =beli.focus i not nil). The algorithm then hypothe- 
sizes that the user has changed his mind about each belief 
in cand-set and predicts how this will affect the user's 
belief about .bel (step 4.2). I f  the user is predicted to ac- 
cept --,..bel under this hypothesis, the algorithm invokes 
Select-Min-Set to select a minimum subset of cand-set as 
the unaccepted beliefs that it would actually pursue, and 
the focus of modification (..bel.focus) will be the union of 
140 
the focus for each of the beliefs in this minimum subset. 
If attacking the evidence for _bel does not appear to 
be sufficient to convince the user of -~_bel, the algorithm 
checks whether directly attacking _bel will accomplish 
this goal. If providing evidence directly against _bel is 
predicted to be successful, then the focus of modifica- 
tion is _bcl itself (step 4.3). If directly attacking _bel 
is also predicted to fail, the algorithm considers the ef- 
fect of attacking both ..bel and its unaccepted proposed 
evidence by combining the previous two prediction pro- 
cesses (step 4.4). If the combined evidence is still pre- 
dicted to fail, the system does not have sufficient evidence 
to change the user's view of_bel; thus, the focus of mod- 
ification for .bel is nil (step 4.5). 7 Notice that steps 2 and 
4 of the algorithm invoke a function, Predict, that makes 
use of the belief revision mechanism (Galliers, 1992) dis- 
cussed in Section 4.1 to predict he user's acceptance or
unacceptance of..bel based on the system's knowledge of 
the user's beliefs and the evidence that could be presented 
to him (Logan et al, 1994). The result of Select-Focus- 
Modification is a set of user beliefs (in _bel.focus) that 
need to be modified in order to change the user's belief 
about he unaccepted top-level belief. Thus, the negations 
of these beliefs will be posted by the system as mutual 
beliefs to be achieved in order to perform the Mod/fy 
actions. 
4.2.2 Selecting Justification for a Claim 
Studies in communication a d social psychology have 
shown that evidence improves the persuasiveness of a 
message (Luchok and McCroskey, 1978; Reynolds and 
Burgoon, 1983; Petty and Cacioppo, 1984; Hampie, 
1985). Research on the quantity of evidence indicates 
that there is no optimal amount of evidence, but that the 
use of high-quality evidence is consistent with persua- 
sive effects (Reinard, 1988). On the other hand, Cn'ice's 
maxim of quantity (Grice, 1975) specifies that one should 
not contribute more information than is required, s Thus, 
it is important that a collaborative agent selects uffmient 
and effective, but not excessive, evidence to justify an 
intended mutual belief. 
To convince the user ofa belief,_bel, our system selects 
appropriate justification by identifying beliefs that could 
7In collaborative dialogues, an agent should reject a pro- 
posal only ff she has strong evidence against i . When an agent 
does not have sufficient information to determine the accep- 
tance of a proposal, she should initiate an information-sharing 
subdialogue to share information with the other agent and re- 
evaluate the proposal (Chu-Carroll and Carberry, 1995). Thus, 
further esearch isneeded to determine whether or not the focus 
of modification for a rejected belief will ever be nil in collabo- 
rative dialogues. 
sWalker (1994) has shown the importance of IRU's Odor- 
mationally Redundant Utterances) in efficient discourse. We 
leave including appropriate IRU's for future work. 
be used to support_bel and applying filtering heuristics to 
them. The system must first determine wbether justifica- 
tion for_bel is needed by predicting whether or not merely 
informing the user of _bel will be sufficient to convince 
him of _bel. If so, no justification will be presented. If 
justification is predicted to be necessary, the system will 
first construct the justification chains that could be used 
to support _bel. For each piece of evidence t~t  could 
be used to directly support ..bel, the system first predicts 
whether the user will accept he evidence without justi- 
fication. If the user is predicted not to accept apiece of 
evidence (evidi), the system will augment the evidence to 
be presented tothe user by posting evidi as a mutual be- 
lief to be achieved, and selecting propositions that could 
serve as justification for it. This results in a recursive 
process that returns a chain of belief justifications that 
could be used to support.bel. 
Once a set of beliefs forming justification chains is 
identified, the system must then select from this set those 
belief chains which, when presented to the user, are pre- 
dicted to convince the user of .bel. Our system will first 
construct a singleton set for each such justification chain 
and select he sets containing justification which, when 
presented, is predicted to convince the user of _bel. If 
no single justification chain is predicted to be sufficient 
to change the nser's beliefs, new sets will be constructed 
by combining the single justification chains, and the se- 
lection ~ is repeated. This will produce a set of 
possible candidate justification chains, and three heuris- 
tics will then be applied to select from among them. The 
first heuristic prefers evidence inwhich the system is most 
confident since high-quality evidence produces more at- 
titude change than any other evidence form (Luchok and 
McCroskey, 1978). Furthermore, the system can better 
justify a belief in which it has high confidence should the 
user not accept it. The second heuristic prefers evidence 
that is novel to the user, since studies have shown that ev- 
idence is most persuasive ff it is previously unknown to 
the hearer (Wyer, 1970; Morley, 1987). The third heuris- 
tic is based on C.nice's maxim of quantity and prefers 
justification chains that contain the fewest beliefs. 
4.2.3 Example 
After the evaluation of the di~ogue model in Figure 1, 
Modify-Proposal is invoked because the top-level pro- 
posed belief is not accepted. In selecting the focus of 
modification, the system will first identify the candidate 
foci tree and then invoke the Select-Focus-Modification 
algorithm on the belief at the root node of the candidate 
foci tree. The candidate foci tree will be identical to the 
proposed belief tree in Figure 1 since both the top-level 
proposed belief and its proposed evidence were rejected 
during the evaluation process. This indicates that the fo- 
cus of modification could be either -~Teaches(Smith,AI) 
141 
or On-Sabbatical(Smith, next year) (since the evidential 
relationship between them was accepted). When Select- 
Focus-Modification is applied to --,Teaches(Smith,Al), 
the algorithm will first be recursively invoked on On- 
Sabbatical(Smith, next year) to determine the focus for 
modifying the child belief (step 3.1 in Figure 3). Since 
the system has two pieces of evidence against On- 
Sabbatical(Smith, next year), 1) a warranted piece of 
evidence containing Postponed-Sabbatical(Smittg1997) 
and supports( Postponed-Sabbatical(Smith,1997),-,On- 
Sabbatical(Smith, next year)), and 2) a strong 
piece of evidence containing --,visitor(Smith, IBM, next 
year) and supports(-,visitor(Smith, IBM next year),-,On- 
Sabbatical(Smith, next year)), the evidence is pre- 
dicted to be sufficient to change the user's be- 
lief in On-Sabbatical(Smith, next year), and hence 
-,Teaches(Smith, A1); thus, the focus of modification will 
be On-Sabbatical(Smith, next year). The Correct-Node 
specialization of Modify-Proposal will be invoked since 
the focus of modification is a belief, and in order to sat- 
isfy the precondition f Modify.Node (Figure 2), MB( S, U, 
-~ On-Sabbatical(Smith, next year)) will be posted as a mu- 
tual belief to be achieved. 
Since the user has a warranted belief in On- 
Sabbatical(Smith, next year) ('indicated by the seman- 
tic form of utterance (3)), the system will predict h~ 
merely informing the user of the intended mutual belief 
is not sufficient to change his belief; therefore R will 
select justificatkm from the two available pieces of evi- 
dence supporting -,On.Sabbatical(Smith, next year) pre- 
sented earlier. The system will predict hat either piece 
of evidence combined with the proposed mutual belief 
is sufficient to change the user's belief; thus, the filter- 
ing heuristics are applied. The first heuristic will cause 
the system to select Postponed.Sabbatical(Smith, 1997) 
and supports(Postponed-Sabbatical(Smith, 1997),-,On- 
Sabbatical(Smith, next year)) as support, since it is the 
evidence in which the system is more confident. 
The system will try to establish the mutual beliefs 9as 
an attempt to satisfy the precondition of Modify-Node. 
This will cause the system to invoke Inform cKscourse 
actions to generate he following utterances: 
(4) S: Dr. Smith is not going on sabbatical next 
year. 
(5) He postponed his sabbatical until 199Z 
If the user accepts the system's utterances, thus satisfy- 
ing the precondition that he conflict be resolved, Modify- 
Node can be performed and changes made to the original 
proposed beliefs. Otherwise, the user may propose mod- 
9Only MB( S, U, Postponed-Sabbatical( Smith, 1997)) will be 
proposed as justification because the system believes that he 
evidential relationship needed to complete he inference is held 
by a stereotypical user. 
ifications to the system's proposed modifications, result- 
ing in an embedded negotiation sub4iaJogue. 
5 Conc lus ion 
This paper has presented a computational strategy for en- 
gaging in collaborative negotiation tosquare away con- 
flicts in agents' beliefs. The model captures features 
specific to collaborative negotiation. It also suppom ef- 
fective and efficient dialogues by identifying the focus of 
modification based on its predicted success in resolving 
the conflict about he top-level belief and by using heuris- 
tics motivated by research in social psychology toselect 
a set of evidence to justify the proposed modification of
beliefs. Furthermore, by capturing collaborative negoti- 
ation in a cycle of Propose-Evaluate-Modify actions, the 
evaluation and modification processes can be applied re, 
cursively to capture mbedded negotiation subdialogues. 
Acknowledgments  
Discussions with Candy Sidner, Stephanie Elzer, and 
Kathy McCoy have been very helpful in the development 
of this work. Comments from the anonymous reviewers 
have also been very useful in preparing the final version 
of this paper. 
References 
James Allen. 1991. Discourse structure in the TRAINS 
project. In Darpa Speech and Natural Language Work- 
shop. 
Lawrence Birnb~nm; Margot Flowexs, and Rod McGuire. 
1980. Towards an AI model of argumentation. In 
Proceedings of the National Conference on Artificial 
Intelligence, pages 313-315. 
Alison Cawsey, Julia Galliers, Brian Logan, Steven 
Reec.e, and Karen Sparck Jones. 1993. Revising be- 
fiefs and intentions: A unified framework for agent 
interaction. In The Ninth Biennial Conference of the 
Society for the Study of Artificial Intelligence and Sim- 
ulation of Behaviour, pages 130-139. 
Jennifer Chu-Carroll and Sandra Carberry. 1994. A plan- 
based model for response generation i collaborative 
task-oriented dialogues. In Proceedings of the Twelfth 
National Conference on Artificial Intelligence, pages 
799-805. 
Jennifer Chu-Carroll and Sandra Carberry. 1995. Gener- 
ating information-sharing subdialognes in expert-user 
consultation. In Proceedings of the 14th International 
Joint Conference on Artificial Intelligence. To appear. 
Jennifer Chu-Carroll. 1995. A Plan-BasedModelforRe- 
sponse Generation in Collaborative Consultation Di. 
alogues. Ph.D. thesis, University of Delaware. Forth- 
coming. 
Paul R. Cohen. 1985. Heuristic Reasoning about Un- 
certainty: An Artificial Intelligence Approach. Pitman 
Publishing Company. 
142 
Robin Cohen. 1987. Analyzing the structure of argu- 
mentative discourse. ComputationalLinguistcis, 13(1- 
2): 11-24, January-June. 
Columbia University Transcripts. 1985. Transcripts de- 
rived from audiotape conversations made at Columbia 
University, New York, NY. Provided by Kathleen 
McKeown. 
Julia R. Galliers. 1992. Autonomous belief revision and 
communication. I  Gardenfors, editor, BeliefRevision. 
Cambridge University Press. 
H. Paul Grice. 1975. Logic and conversation. In Peter 
Cole and Jerry L. Morgan, editors, Syntax and Seman- 
tics 3: Speech Acts, pages 41-58. Academic Press, 
Inc., New York. 
Barbara J. Grosz and Caadace L. Sidner. 1990. Plans 
for discourse. In Cohen, Morgan, and Pollack, editors, 
Intentions in Communication, chapter 20, pages 417- 
444. MIT Press. 
Dale Hample. 1985. Refinements on the cognitive model 
of argument: Concreteness, involvement and group 
scores. The Western Journal of Speech Communica- 
tion, 49:267-285. 
Aravind K. Joshi. 1982. Mutual beliefs in question- 
answer systems. In N.V. Smith, editor, Mutual Knowl- 
edge, chapter 4, pages 181-197. Academic Press. 
Lynn Lambert and Sandra Carberry. 1991. A tripartite 
plan-based model of dialogue. In Proceedings of the 
29th Annual Meeting of the Association for Computa- 
tional Linguistics, pages 47-54. 
Brian Logan, Steven Reece, Alison Cawsey, Julia Gal- 
tiers, and Karen Sparck Jones. 1994. Belief revision 
and dialogue management in information retrieval. 
Technical Report 339, University of Cambridge, Com- 
puter Lalx)ratory. 
Joseph A. Luchok and James C. McCroskey. 1978. The 
effect of quality of evidence on attitude change and 
source credibility. The Southern Speech Communica- 
tion Journal, 43:371-383. 
Mark T. Maybury. 1993. Communicative acts for gen- 
erating natural anguage arguments. In Proceedings 
of the National Conference on Artificial Intelligence, 
pages 357-364. 
Kathleen R. McKeown. 1985. Text Generation : Using 
Discourse Strategies and Focus Constraints to Gen- 
erate Natural Language Text. Cambridge University 
Press. 
DonaldD. Morley. 1987. Subjective message constructs: 
A theory of persuasion. Conmmnication Monographs, 
54:183-203. 
Richard E. Petty and John T. Cacioppo. 1984. The ef- 
fects of involvement on responses toargument quantity 
and quality: Central and peripheral routes to persua- 
sion. Journal of Personality and Social Psychology, 
46(1):69-81. 
Martha E. Pollack. 1986. A model of plan inference 
that distinguishes between the beliefs of actors and ob- 
servers. In Proceedings of the 24th Annual Meeting of 
the Association for Computational Linguistics, pages 
207-214. 
Alex Quilici. 1992. Arguing about planning alternatives. 
In Proceedings of the 14th International Conference 
on Computational Linguistics, pages 906-910. 
Rachel Reichman. 1981. Modeling informal debates. In 
Proceedings of the 7th International Joint Conference 
on Artificial Intelligence, pages 19-24. 
John C. Reinard. 1988. The empirical study of the per- 
suasive ffects of evidence, the status after fifty years of 
research. Human Communication Research, 15(1):3- 
59. 
Rodaey A. Reynolds and Michael Burgoon. 1983. Be- 
lief processing, reasoning, and evidence. In Bostrom, 
editor, Communication Yearbook 7, chapter 4, pages 
83-104. Sage Publications. 
Candace L. Sidner. 1992. Using discourse to negotiate 
in collaborative activity: An artificial language. In 
AAA-92 Workshop: Cooperation Among Heteroge- 
neous Intelligent Systems, pages 121-128. 
Candace L. Sidner. 1994. An artificial discourse lan- 
guage for collaborative n gotiation. In Proceedings of 
the Twelfth National Conference on Artificial Intelli- 
gence, pages 814-819. 
SKI Transcripts. 1992. Transcripts derived from audio- 
tape conversations made at SRI International, Menlo 
Park, CA. Prepared by Jacqueline Kowtko under the 
direction of Patti Price. 
Katia Sycara. 1989. Argumentation: Planning other 
agents' plans. In Proceedings of the l l th International 
Joint Conference on Artificial Intelligence, pages 517- 
523. 
Marflyn Walker and Steve Whinak~. 1990. Mixed ini- 
tiative in dialogue: An investigation i to discourse seg- 
mentation. In Proceedings of the 28th Annual Meet- 
ing of the Association for Computational Linguistics, 
pages 70-78. 
Marilyn A. Walker. 1992. Redundancy in collaborative 
dialogue. In Proceedings of the 15th International 
Conference on Computational Linguistics, pages 345- 
351. 
Marilyn A. Walker. 1994. Discourse and deliberation: 
Testing a collaborative strategy. In Proceedings of 
the 15th International Conference on Computational 
Linguistics. 
Bonnie Webber and Atavind Joshi. 1982. Taking the 
initiative in natural anguage data base interactions: 
Justifying why. In Proceedings of COLING-82, pages 
413-418. 
Steve Whittaker and Phil Stenton. 1988. Cues and con- 
trol in expert-client dialogues. In Proceedings of the 
26th Annual Meeting of the Association for Computa- 
tional Linguistics, pages 123-130. 
Robert S. Wyer, Jr. 1970. Information redundancy, in- 
consistency, and novelty and their role in impression 
formation. Journal of Experimental Social Psychol- 
ogy, 6:111-127. 
R. Michael Young, Johanna D. Moore, and Martha E. 
Pollack. 1994. Towards a principled representation 
of discourse plans. In Proceedings of the Sixteenth 
Annual Meeting of the Cognitive Science Society, pages 
946-951. 
143 
