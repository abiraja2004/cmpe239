ARGUING ABOUT PLANNING ALTERNATIVES 
ALEX QUILICI 
Depar tment  of E lec t r i ca l  Eng ineer ing  
2540 Dole Street ,  Ho lmes  Hal l  455 
Univers i ty  of Hawai i  at Maaoa  
Honolu lu,  HI ,  96822 
Abstract  
In discourse processing, two major problems are un- 
derstanding the underlying connections between suc- 
ce~ive dialog utterances and deciding on the content 
of a coherent dialog response. Thin paper presents a
computational model of these tasks for a restricted 
class of argumentative dialogs. In these dialogs, each 
response presents a belief that justifies or contradicts 
another belief presented or inferred earlier in the di- 
alog. Understanding a response involves relating a 
stated belief to these earlier beliefs, and producing a
response involves electing a belief to justify and de- 
ciding upon the set of beliefs to provide as its justifi- 
cation. Our approach is knowledge baaed, using gen- 
eral, common-sense justification rules to recognize 
how a belief in being justified and to form new justifi- 
cations for beliefs. This approach provides the ability 
to recognize and respond to never before seen belief 
justifications, a necessary capability for any system 
that participates indialogs involving disagreements. 
1 In t roduct ion  
In discourse processing, two major problems are un- 
derstanding the underlying connections between suc- 
cessive dialog responses and deciding on the content 
of a coherent dialog response. This paper presents 
an initial model that accomplinhes these tasks for one 
class of argumentative dialogs. In this class, each di- 
alog respouse presents a belief that justifies or con- 
tradicts a belief provided earlier in the dialog. 
The following dialog fragment is an example: 
(1) TIDY: The members of the AI lab should clean 
it themselves. 
(2) ScguPPY: But that interferes with doing re- 
search. 
(3) TIDY: There's no other way to keep it clean. 
(4) SCRUFf'Y: We can pay a janitor to keep it 
clean. 
(5) TIDY: We need money to pay a janitor. 
(6) SCRUFFY: We can transfer the money from 
the salary fund. 
(7) TIDY: But doing that interferes with paying 
the lab members. 
(8) SCRUVFY: It's more desirable to have a clean 
lab than to pay the lab members. 
Each response states one or more plan-oriented be- 
liefs, usually as part of a short chain of reeanning 
justifying or contradicting a belief provided earlier 
in the dialog. 
In (1), TIDY begins by stating a belief: the lab 
members hould execute the plan of cleaning the lab. 
In (2), SCRUFFY responds with a belief that the 
lab members executing this plan interferes with their 
doing research. This belief justifies SCRUFFY~s un- 
stated belief that the lab members hould not exe- 
cute the plan of cleaning the lab, which contradicts 
TIvY's stated belief in (1). SCRUFPY's underlying 
reasoning is that the lab members houldn't clean 
the lab because it interferes with their executing the 
more desirable plan of doing research. 
In (3), TIDY presents belief that there's no al- 
ternative plan for keeping the lab clean. This belief 
justifies TIDY's belief in (1). TIDY's underlying rea- 
soning is that the lab members hould clean the lab 
because it's the best plan for the goal of keeping the 
lab clean, and it's the best plan because it's the only 
plan that achieves the goal. 
Finally, in (4), Scs.uFta'y states a belief that pay- 
ing a janitor achieves the goal of keeping the lab 
clean. This contradicts TIDY's stated belief in (3). 
It also justifies a belief that the lab members clean- 
ing the lab isn't the best plan for keeping the lab 
clean, which contradict~ one of the beliefs inferred 
from (3). SCRUFFY's reasoning is that paying a jan- 
itor is a more desirable plan that achieves thin goal. 
The remaining responses follow the same pattern. 
Understanding responses like these involves relating 
a stated belief to beliefs appearing earlier in the di- 
alog. That requires inferring the participant's un- 
derlying reasoning chain and the beliefs it justifies. 
Producing these responses involves electing a belief 
to justify and deciding upon the set of beliefs to pro- 
vide as its justification. That requires constructing 
an appropriate reasoning chain that justifies holding 
any unshared beliefs. 
Our focus in this paper is on an initial method 
for representing, recognising, and producing the be- 
lief justifications underlying dialog responses that 
provide coherent defenses of why beliefs are held. 
ACRES DE COLING-92, NANTES, 23-28 AOl~'r 1992 9 0 6 PROC. OF COLING-92, NANTES, AUG. 23-28. 1992 
The behavior modeled it limited in several signifi- 
cant ways. FirJt, we do not try to recognite when 
an trguer's response contradicts one of his earlier e- 
sponses, such as the contradiction between (2) and 
(8), nor do we try to avoid producing such responses. 
Second, we do not try to recngnise or make use 
of high-level arguing strategies, uch as reductio ed 
ab*urdum. Third, we restrict ourselves to a small 
clam of beliefs involving planning. Finally, we start 
with representatin~ of beliefs and ignore the linguis- 
tic issues involved in turning responses into be\]ida. 
Clearly, all these limitations must eventually be exi- 
dressed in order to produce a more realistic model of 
debate. Our belief, however, it that an initial model 
of the process of rccognising and producing belief 
justifications i a useful and necessary first step. 
2 Our Approach 
Our approach to these tasks rests on a simple as- 
sumption: Dialog participants jusLif~ beliefs with in- 
stantialions of general, common-sense justification 
ra/es. For plan-oriented beliefs, a justification rule 
corresponds to a planning heuristic that's based 
solely on structural features of plans in general, not 
on characteristics of specifc plans themselves. 
The first few responses in this dialog illustrate sev- 
eral justification rules. In (2), SCRUI~F'? uses the 
rule: O~e re.on wh~ a plan shouldn'~ be ezecuted is 
that it conflicts with assenting a more desirable plan. 
Similarly, in (3), TXDY chains together a pair of these 
rules: One reason why a plan should be ezecuted is 
that it's the be,t plan/or achieving a goal, and One 
reason why a plan il the be,t plan for a goal is that 
if'# the onl~ plan that achieves the goal. 
Given our assumption, understanding a response 
it equivalent to recogniting which justification rules 
were chained together and instantiated to form it, 
determining which belief to address in a response it 
equivalent to determining which beliefs in a chain of 
instantiated justification rules axe not shared, and 
producing a justification is equivalent o selecting 
and instantiating justification rules with beliefs from 
memory. 
We make this assumption for two reasons. First, 
dialog participants hould be able to understand and 
respond to never before seen belief justifications. 
That suggests applying general knowledge, such as 
our jtmtification rules, to analyse and produce spe- 
cific juJtifications, as that knowledge is likely to be 
shared by different participants, even if they hold dif- 
ferent beliefs about specific courses of action. And 
second, dialog parlieipants should abo be able io use 
the same knowledge for different foJks. That sug- 
gests that arguments about planning should use the 
Msne knowledge as planning itsel? The justifies- 
tion rules for plan-oriented belief1 describe knowl- 
edge that a planner would aim find nsdul in welectlng 
or constructing new plans. 
Our approach diffem in two ways fzom previons 
modeh of participating in dialogs. First, the*? mod- 
els emphe~ised plan recognition: the task of recog- 
nising and inferring the underlying plans and goalJ 
of a dialog paxtlcipant \[4, 10, 17, 18, 2\]. They view 
utternnces as providing steps in plans (typically by 
describing oals or actions) and tie them together 
by inferring an underlying plan. But in an argument 
not only must the participant's plans and goals be in- 
ferred, but alto their underlying belie/s about those 
plans and goals. Our approach suggests a model that 
infers these beliefs as a natural consequence of trying 
to understand connections between successive diMog 
utterances. In contrast, existing approaches to in- 
ferring participant beliefs take a stated belief and 
try to reason about possible justifications for it \[12, 
9\]. Previous models have also tended to view provid- 
ing a dialog response solely as a part of the question 
answering process. In contrast, our approach sug- 
gests that responses arise as a natural consequence 
of trying to integrate newly-encountered beliefs with 
current beliefs in memory, and trying to understand 
any contradictions that result. 
3 Justif ication Rules 
The argumentative dialogs we've examined have two 
types of plan-oriented beliefs: facts61 and evalus- 
flee \[1\]. Factual beliefs are objective judgements 
about planning relationships, uch as whether a plan 
has a particular effect or enablement. They repre- 
sent the planning knowledge held by moat previous 
plan-understanding and plan-constructing systems. 
Evaluative beliefs, on the other hand, are subjec- 
tive judgements about planning relationJhipe, such 
as whether or not a plan should be executed. Al- 
though these beliefs have generally been ignored by 
previous systems, they are crucial to participating in 
arguments involving plan-oriented beliefs. 
Our assumption is there exists a small set of jus- 
tification rules for each planning relationship. Each 
rule is represented as an abstract configuration of 
planning relationships that, when instantiated, pro- 
vides a reason for holding a particular belief. For 
example, the rule that a plan shouldn't be executed 
if it conflicts with a preferred plan is represented as: 
IF interforso(occtur(P) .occtn'(P')) tlID 
favoxa(occu~r(P'),occ~(p)) 
THEN ought (not (occu.~ OF))) 
That is, a plan shouldn't be executed if (1) it inter- 
fereB with another plan, and (2) that plan is preferred 
to it. Figure 1 lists our current justification rules for 
ACRES DE COLING-92. NANTES. 23-28 AO~r 1992 9 0 7 I~ROC. OV COLING-92. NANTES. AUG. 23-28. 1992 
\]~tee~'ot~ why execuginl~ plan X/n desirable: 
X iJ the be~t plt~ for g g0al. 
Executing X h aa enablemeat for n goal. 
_Re. a spas why execntinl~ plan X.IS undesirable: 
X conflicts with a more desirable plan. 
X has an uadefirable ffect. 
X h~ an undefirable nablemeat. 
Remtoua why plan X iJ the best plan for n ~oa\]: 
X hi the only plaza that achiev~ the goal. 
No plan more desirable than X achieves the goal. 
Re~oas why plan X is not the best plan for a goal: 
X hat an unachievable ensblement. 
X's execution is undesirable. 
Some more desirable plan schieve~ the goal. 
Rettsons why plan X is more desirable than plan Y: 
X heat a desirable ffect that Y doesn't have. 
X doesn't have an undesirable effect that Y h~. 
X doesn't have an undesirable enablement that Y has. 
Y conflicts with a more desirable plan and X doesn't. 
X i* an enablement of a mote desirable plan than Y. 
X has an effect more des~nble than Y. 
Re~ons why achieving oal G is undesirable: 
The only plan for achieving G in undesirable. 
Achieving G has an undesirable effect S. 
Reasons why achieving oal G i~ desirable: 
Achieving G in an enablement for another goal. 
Not achieving G has an undesirable effect S. 
Figure 1: Justification rules. 
evaluative beliefs (~ee \[13\] tbr representational de- 
tails and criteria for dedding what is a reasonable 
justification rule). These rul?~ were abstracted from 
examining a variety of different plan-oriented argu- 
mentative dialogs. 
The power of these justification rules comes from 
their generality: A single rule can be instantiated in 
different ways to provide justifications for different 
beliefs. In (2), SCRUFFY USes the above rule to jus- 
tify a belief that the lab members houldn't clean 
the lab themselves. In (7), TIDY uses the same rule 
to justify a belief that the lab members shouldn't 
transfer money front the salary fnnd. Here, TIDY's 
justification is that tranderring the money interferes 
with the more desirable plan of paying researchers. 
4 Recognizing Justifications 
The proee~ of understanding a dialog response is 
modeled as a forwar&chaining search for a chain of 
instantiated justification rules that (1) contains the 
user~s tated belief, and (2) jastifies an earlier dialog 
belief or its negation. 
We briefly illustrate this proce~ by showing how 
SCRUt'FY understands TIDe's response in (3). The 
input belief is that the lab members denning the lab 
is the only plan that achieves the goal of keeping the 
lab clean. This belief matches an antecedent in a 
pair of justification rules, so the process begins by 
inetantiating these rules, resulting in pair of possible 
justification chains that contain TIDY's stated belief: 
(1) the lab members cleaning is the beef plast for ~ep- 
lag the lab clean becalst it's the only pianist keeping 
the lab clean, and (2) the lab shonldntl ~ kept c/cart 
because the only plan for that goal is the wades~ble 
plan of having the lab members cleaning iL 
Neither justification directly relates to the dialog, 
so the next step is to determine which one to pursue 
further, and whether either can be el iminated from 
further consideration. Here, the second justif ication 
contains a belief that the lab members  cleaning the 
lab is undesirable, which contradicts TIDY's stated 
belief in (1). Applying the heuristic "D/aeard any 
potential justification containing beliefs that contra- 
dict the speaker's earlier beliefs" leaves only the first 
justification to pursue further. It 's consequent in the 
antecedent of a single justification rule, and instan- 
tinting tiffs rule leads to this justification chain: the 
lab members should clean the lab because their elear~. 
lag the lab is the best plan for the goal of keeping the 
lab clear* because it's the only plan for keeping tlAe lab 
clean. The justified belief is TIDY's belief in (1), so 
the process stops. 
In general, the understanding proceu it more com- 
plex, since justification rules may not be completely 
instantiated by a single antecedent, and may there- 
fore need to be further iastantiated from beliefs in 
the dialog context and memory. There ahm may be 
many possible chains to pursue even e~ter heuristi- 
cally discarding some of them, requiring the tree of 
other heuristics to determine which path to follow, 
such as "Pursue the reasoning chain whidt eoltains 
the most beliefs found in the dialog eontea~.  
5 Selecting A Belief To Justify 
After recognizing a participant's reasoning chain, it 's 
necessary to select a belief to justify as a response. 
This task involves determining which beliefs are not 
shared, and selecting the negation of one of  tho~ 
beliefs to justify. 
An intuitive notion of agreement is that  a belief 
is shared if it it's found in memory or can be justi- 
fied, and it's not shared if its negation it found in 
memory or can be justified. But this notion is com- 
putationally expensive, since it could conceivably in. 
volvo trying to justify all the beliefs in the lmrtie- 
ipant'a reasoning chain, as well as their negatinas. 
As ml alternative, our model determines whether a 
belief is shared by searching memory for the belief 
and its negation and, if that fails, applying a small 
Acrl~s DE COLING-gZ NarcH~s, 23-28 Ao(rf 1992 9 0 8 PROC. OF COLING-92. NANTES. AUG. 23-28, 1992 
set of agreement heuristics. One such heuristic is 
"Assume a belief is sassed if a justil~ling geaera//za- 
lion is found in tattooer. So, for exanlpie, if the 
belief "keep everything clean" is found in memory, 
the belief *keep the AI lab clean ~ is considered to he 
shared. If no agreement heuristic applies, the belief 
is simply marked as Uunknown". 
After determining whether each belief in the par- 
ticipant's reasoning chain is shared, the model first 
searches for an existing justification for an unshared 
belief's negation. If that fails, it then tries to create 
a new justification for an unshared belief's negation. 
And if that fails, it tries to create a new justifica- 
tion for the negation of one of the unknown beliefs. 
This way existing justifications are presented before 
an attempt is made to construct new ones. If none of 
these steps succeed, the assumption is that the rea- 
Boning chain is shared, and an attempt is made to 
form a new justification for the belief it contradicts. 
Thus, the belief our model addresses in a response 
arises from trying to discover whether or not it agrees 
with another participant's reasoning. 
6 Forming Justi f ications 
To form a new justification for a belief, our model 
performs a backward chaining search fo~" a chain of 
justification rules that justify the given belief and 
that can be iustantiated with beliefs from memory. 
We briefly illustrate this process by showing how 
SCRU~'Fy forms the response in (2). The belief to 
justify is that it's not desirable to have the lab mem- 
bers clean the lab. The first step is to instantiate the 
justification rules that have this belief as their conse- 
quent. That results in several possible justifications: 
(1) there's an undesirable nablement of cleaning the 
lab, (2) there's an undesirable effecf of cleaning the 
lab, or (3) the lab members cleaning the lab conflicts 
with a more desirable action. 
The next step is to try to fully iastantiate one of 
these rules. Applying the heuristic "Pursue the most 
instantiafed justification rule" suggests working on 
the last rule. Here, SCRUFFY instantiates it with a 
belief from memory that research is more desirable 
than cleaning. Once a rule is instantiated, it's neces- 
sary to verify that the beliefs it contains are shared. 
Here, that involves verifying that cleaning conflicts 
with research. It does, so the instantiated rule can 
be presented an the response. 
In general, the process is more complex than out- 
lined here, since not all of the belief in an iustantiated 
justification rule may be shared, and there may be 
several ways to instantiate a particular ule. Those 
rules containing unknown beliefs require further jus- 
tification, while those rules containing unshared be~ 
l ids can be discarded. 
7 Background 
The closest related system is ABDUL/ILANA \[8\], 
which debated the responsibility mad cause for hlstot- 
ical events. It focused on the complementary prob- 
lem of recogniling and providing episodic justifi?~ 
tions, rather than justifications b~ed on the rel~. 
tionships between different plans. 
There are several models for recognising the r?- 
lationship between argument propositions. Cobea's 
\[5\] taken each new belief and checks it for a justifi- 
cation relationship with a subset of the previnusly- 
stated belief~ determined through the use of dip 
sing structure and clue words. That model 
tureen the existence of an evidence oracle capable 
of determining whether a justification relationship 
holds between may pair of beliefs. Our model ira. 
plements this oracle for a particular clam of plan- 
oriented belief justifications. OpFkt \[3\] recogniset bo. 
lief justifications in editorials almut economic plan- 
ning through the use of argument units, a knowb 
edge structure that can be viewed as complex cow 
figurations of justification rules. The approaches 
are complementary, just as scripts \[7\] and plans \[6, 
18\] are both useful methods for recognising the cam 
nections between events in a narrative. 
Several systems have concentrated on producing 
belief justifications. Our own earlier work \[14, 15, 
16\] used a primitive form of j~tstification rules for 
factual beliefs as a template for producing corre~ 
tive responses for user misconceptions. Our current 
model extends this work to use these rules in both 
understanding and responding, and provides addi- 
tional rules for evaluative beliefs. 
ROMPER \[11\] providas justifications for belidk 
about an object's class or attributes. But it profides 
these justifications purely by template matching, not 
by constructing more general reasoning chains. 
8 Current  Status  
We've completely implemented the model di~umed 
in this paper. The program is written in Quintu~ 
Prolog and runs on an l IP /APOLLO workstation. 
Its input is a representation for a stated participant 
belief, and its output is a representation for m, up. 
propriate response. It currently includes 30 justitka~ 
tips rules and over 400 beliefs about various plans. 
We've used the program to participate in short ar-. 
gumentative dialogs in two disparate domains: day- 
to~day planning in the A! lab, and removing and 
recovering files in UNIX. We're currently using it to 
experiment with different heuristics for controlling 
the search process involved in rer.ognisbtg and c~u.- 
strutting these reasoning ch~in~. 
Our xxmdcl he~ eevt.L'~l /~ey Ib~dt~tion~ ~e e~e c,~_dy 
AcrEs DE COLING-92, NAN1q~, 23-28 hofrr 1992 9 0 9 P toc t  1: COLING.9"~ ~qt, l,~'n!s, Aut;. 23-28, 1992 
now starting to addrem. First, it views plans as 
atomic units and comiders only a small set of "all or 
nothing" plan-oriented beliefs. This means it can't 
produce or understand justifications involving atel~ 
in a plan, conditional planning relationships, or be- 
liefs not directly involving plans. Second, our model 
can understand only those responses that jnstify an 
earlier belief. It can't, for example, understand a re- 
sponse that contradicts an inferred justification for 
an earlier belief. These more complex relationships 
can be represented using juetificntinn rules, but our 
model must be extended to recognise them. Third, 
our model is reactive rather than initiatory: it pro- 
duces respon~ only when there's n perceived in-. 
agreement. It needs to be extended to know why its 
in an argument, and to be aware of the underlying 
goals of the other argument participants. 
9 Conclus ions 
Previous dialog models have focused primarily on 
recognising a participant's plans and goals. But to 
participate in an argument i 's also necessary to rec- 
ognize when participants are providing beliefs about 
their planl and goals and how they're justifying these 
beliefs. It's also necessary to be able to determine 
which beliefs require further justification and to for- 
mulate justifications for these beliefs. This paper 
suggests a knowledge-based approach for these tasks. 
Our approach has several attractive features. 
Firs L it builds It model of many relevant but un- 
stated participant beliefs as a side-effect of trying 
to relate their utterance to the dialog. Second, it 
decides which belief to address in n response as a 
natural consequence of trying to understand why it 
disagrees with another participant's belief. Third, it 
understands belief jnstifieations using the same gen- 
eral, common-sense planning knowledge that it uses 
to formulate them. Finally, it suggests how never be- 
fore seen belief justiflcatinns can be understood, so 
long as they were formed from general justification 
rules known to the participants. That ability is cru- 
cial for participating in dialogs whose participants 
hold differing beliefs. 
References 
\[1\] R. Abelson. Differences Between Beliefs and 
Knowledge Systems. Cognitive Science 3, 1979. 
\[2\] J.F. Allen and C.R. Perranlt, Analysing Inten- 
tion in Utterances. Artificial Intelligence 15, 1980. 
\[3\] S.J. Alvarado. Understanding Editorial Tezt: 
A Computer Model of A~umsnt Comprehension. 
Kluwer, Boston, MA, 1990. 
\[4\] S. Carberry. Modeling the Uxr ' -  Plans and 
Goals. Computational Li~q~isties, 14(3), 1988. 
\[5\] R. Cohen. Analysing the Structure of Argu- 
mentstive Discourse. Compwfational Linlmistiosj 
13(1), 1987. 
\[6\] M.G. Dyer. ln,-depCt U~derrlanding: A Corn- 
pater Model of Narrative Comprehension. MIT 
Press, Cambridge, MAj 1983. 
\[7\] M.G. Dyer, R.E. Cullingford, and S.5. Alvare~lo. 
Script~. In Encyclopedia of Artificial InteSigeuce, 
John Wiley, NY, NY, 1987. 
\[8\] M. Flowers, R. McGuire, and L. Birnbanm. Ad- 
versary Arguments and the Logic of Personal At- 
tacks. In Strategies/or Natural Language Procen- 
ing. Lawrence Erlbaum, Hill~iale, NJ, 1982. 
\[9\] R. Kaas. Building n User Model Implicitly from a 
Cooperative Advisory Dialog. User Modeling and 
User-Adapted Inieraclion~ 1(3), 1991. 
\[19\] D.J. Litman and J.F. Allen. A Plan Recognition 
Model for Subdialoguea in Conversatinns. C0gn/- 
live Science, 11, 1987. 
\[11\] K. McCoy. Reasoning on n Highlighted User 
Model to Respond to Misconceptions. Comp~la- 
tional Linguistics, 14(3), 1988. 
\[12\] M. Pollack. Inferring Domain Plans iB 
Question-Answering. PhD Thesis, University of 
Pennsylvania, Philadelphia, PA, 1986. 
\[13\] A. Qnilici. The Csrre~ion Machine: A Com- 
puter Model Of Rscogniu'ng and Producing Belief 
Jnstifications in Argumentative Dialogs. PhD The- 
sis, University of California, LA, CA, 1991. 
{14\] A. Quilici. Participating In Plan-Oriented Di- 
alogs. In Proceedings o/tAe l~h Conference of tits 
Cognitive Science 5ocietj, Boston, MA, 1990. 
\[15\] A. Qnilici. The Correction Machine: Formulat- 
ing Explanations for User Misconceptions. In Pro- 
ceedings of ~he 1989 International Joint Confer- 
ence on Artificial Intelligence, Detroit, MI, 1989. 
\[16\] A. Quilici, M.G. Dyer~ and M. FlowerL Recog- 
nising and Responding to Plan-oriented Miscon- 
ceptions. Computational Linguistics, 14(3), 1988. 
\[17\] C. L. Sidner. Plan Parsing For Intended Re- 
sponse Recognition in Discoume. Compugatiomal 
Intelligence, 1(1), 1985. 
\[18\] IL Wileneky. Planning and Understanding. Ad- 
dison Wesley, Reading, MA, 1983. 
Ac'r~ DE COLING-92. NANTES, 23-28 AOfrr 1992 9 1 0 PROC. OF COLING-92, NANI'ES. AUG. 23-28. 1992 
