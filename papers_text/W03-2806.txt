Intrinsic versus Extrinsic Evaluations of Parsing Systems
Diego Molla?
Centre for Language Technology
Department of Computing
Macquarie University
Sydney, NSW 2109, Australia
diego@ics.mq.edu.au
Ben Hutchinson
Division of Informatics
University of Edinburgh
Edinburgh EH8 9LW, United Kingdom
B.Hutchinson@sms.ed.ac.uk
Abstract
A wide range of parser and/or grammar
evaluation methods have been reported
in the literature. However, in most cases
these evaluations take the parsers in-
dependently (intrinsic evaluations), and
only in a few cases has the effect
of different parsers in real applications
been measured (extrinsic evaluations).
This paper compares two evaluations
of the Link Grammar parser and the
Conexor Functional Dependency Gram-
mar parser. The parsing systems, de-
spite both being dependency-based, re-
turn different types of dependencies,
making a direct comparison impossi-
ble. In the intrinsic evaluation, the accu-
racy of the parsers is compared indepen-
dently by converting the dependencies
into grammatical relations and using the
methodology of Carroll et al (1998) for
parser comparison. In the extrinsic eval-
uation, the parsers? impact in a practi-
cal application is compared within the
context of answer extraction. The dif-
ferences in the results are significant.
1 Introduction
Parsing is a principal stage in many natural lan-
guage processing (NLP) systems. A good parser is
expected to return an accurate syntactic structure
of a sentence. This structure is typically forwarded
to other modules so that they can work with un-
ambiguous and well-defined structures represent-
ing the sentences. It is to be expected that the
performance of an NLP system quickly degrades
if the parsing system returns incorrect syntactic
structures, and therefore an evaluation of parsing
coverage and accuracy is important.
According to Galliers and Sparck Jones (1993),
there are two main criteria in performance evalua-
tion: ?Intrinsic criteria are those relating to a sys-
tem?s objective, extrinsic criteria those relating to
its function i.e. to its role in relation to its setup?s
purpose.? (Galliers and Sparck Jones, 1993, p22).
Thus, an intrinsic evaluation of a parser would
analyse the accuracy of the results returned by the
parser as a stand-alone system, whereas an ex-
trinsic evaluation would analyse the impact of the
parser within the context of a broader NLP appli-
cation.
There are currently several parsing
systems that attempt to achieve a wide
coverage of the English language (such
as those developed by Collins (1996),
Ja?rvinen and Tapanainen (1997), and
Sleator and Temperley (1993)). There is also
substantial literature on parsing evaluation (see,
for example, work by Sutcliffe et al (1996),
Black (1996), Carroll et al (1998), and
Bangalore et al (1998)). Recently there has
been a shift from constituency-based (e.g. count-
ing crossing brackets (Black et al, 1991)) to
dependency-based evaluation (Lin, 1995; Carroll
et al, 1998). Those evaluation methodologies
typically focus on comparisons of stand-alone
parsers (intrinsic evaluations). In this paper we
report on the comparison between an intrinsic
evaluation and an evaluation of the impact of
the parser in a real application (an extrinsic
evaluation).
We have chosen answer extraction as an exam-
ple of a practical application within which to test
the parsing systems. In particular, the extrinsic
evaluation uses ExtrAns, an answer extraction sys-
tem that operates over Unix manual pages (Molla?
et al, 2000). The two grammar systems to com-
pare are Link Grammar (Sleator and Temperley,
1993) and the Conexor Functional Dependency
Grammar parser (Tapanainen and Ja?rvinen, 1997)
(henceforth referred to as Conexor FDG). These
parsing systems were chosen because both include
a dependency-based parser and a comprehensive
grammar of English. However, the structures re-
turned are so different that a direct comparison be-
tween them is not straightforward. In Section 2 we
review the main differences between Link Gram-
mar and Conexor FDG. In Section 3 we present
the intrinsic comparison of parsers, and in Sec-
tion 4 we comment on the extrinsic comparison
within the context of answer extraction. The re-
sults of the evaluations are discussed in Section 5.
2 Link Grammar and Conexor FDG
Link Grammar (Sleator and Temperley, 1993) is
a grammar theory that is strongly dependency-
based. A freely available parsing system that im-
plements the Link Grammar theory has been de-
veloped at Carnegie Mellon University. The pars-
ing system includes an extensive grammar and lex-
icon and has a wide coverage of the English lan-
guage. Conexor FDG (Tapanainen and Ja?rvinen,
1997) is a commercial parser and grammar, based
on the theory of Functional Dependency Gram-
mar, and was originally developed at the Univer-
sity of Helsinki.
Despite both being dependency-based, there are
substantial differences between the structures re-
turned by the two parsers. Figure 1 shows Link
Grammar?s output for a sample sentence, and Fig-
ure 2 shows the dependency structure returned
by Conexor FDG for comparison. Table 1 ex-
plains the dependency types used in the depen-
dency structures of the figures.
The differences between the dependency struc-
tures returned by Link Grammar 2.1 and Conexor
FDG 3.6 can be summarised as follows.
Direction of dependency: Link Grammar?s
?links?, although similar to true dependencies, do
not state which participant is the head and which
is the dependent. However, Link Grammar uses
different link types for head-right links and head-
left links, so this information can be recovered.
Conexor FDG always indicates the direction of the
dependence.
Clausal heads: Link Grammar generally
chooses the front-most element to be the head
of a clause, rather than the main verb. This is
true of both matrix and subordinate clauses, as
exemplified by the Wd and R links in Figure 1.
Conexor FDG follows the orthodox convention of
choosing the main verb as the head of the clause.
Graph structures: Link Grammar?s links com-
bine dependencies at the surface-syntactic and
deep-syntactic levels (e.g., the link Bs, which
links a noun modified by a subject-type relative
clause to the relative clause?s head verb, in Fig-
ure 1 indicates a deep-syntactic dependency). The
resulting structures are graphs rather than trees.
An example is shown in Figure 1, where the noun
man modified by a relative clause is linked to both
the complementiser and the head verb of the rela-
tive clause.
Conjunctions: Our version of Link Grammar
analyses a coordinating conjunction as the head of
a coordinated phrase (Figure 1). This is a modifi-
cation of Link Grammar?s default behaviour which
returns a list of parses, one parse per conjunct.
However in Conexor FDG?s analyses the head will
be either the first or the last conjunct, depending
on whether the coordinated phrase?s head lies to
the left or to the right (Figure 2).
Dependency types: Link Grammar uses a set of
about 90 link types and many subtypes, which ad-
dress very specific syntactic constructions (e.g. the
link type EB connects adverbs to forms of be be-
fore a noun phrase or prepositional phrase: He
is APPARENTLY a good programmer). On the
other hand, Conexor FDG uses a set of 32 de-
///// the man.n that came.v ate.v bananas.n and apples.n with a fork.n1
Wd
Ds
Ss
Bs
R RS
MVp
O^ Js
Ds
Figure 1: Output of Link Grammar.
///// the man that came ate bananas and apples with a fork
 main <
>det
> subj 
 mod<
>subj
 ins <
obj< cc<
 cc <  pcomp<
>det
Figure 2: Dependency structure returned by Conexor FDG.
pendency relations, ranging from traditional gram-
matical functions (e.g. subject, object), to specific
types of modifiers (e.g. frequency, duration, loca-
tion).
Both Conexor FDG and Link Grammar also
return non-dependency information. For Link
Grammar, this consists of some word class in-
formation, shown as suffixes in Figure 1. For
Conexor FDG, the base form morphological in-
formation of each word is returned, along with a
?functional? tag or morpho-syntactic function and
a ?surface syntactic? tag for each word.1
3 Intrinsic Evaluations
Given that both parses are dependency-based, in-
trinsic evaluations that are based on constituency
structures (e.g. (Black et al, 1991)) are hard
to perform. Dependency-based evaluations are
not easy either: directly comparing dependency
graphs (as suggested by Lin (1995), for exam-
ple) becomes difficult given the differences be-
tween the structures returned by the Link Gram-
mar parser and Conexor FDG. We there-
fore need an approach that is independent from
the format of the parser output. Following
Carroll et al (1998) we use grammatical relations
to compare the accuracy of Link Grammar and
Conexor FDG. Carroll et al (1998) propose a set
of twenty parser-independent grammatical rela-
tions arranged in a hierarchy representing differ-
ent degrees of specificity. Four relations from the
hierarchy are shown in Table 2. The arguments to
1See (Ja?rvinen and Tapanainen, 1997) for more informa-
tion on the output from Conexor FDG.
each relation specify a head, a dependent, and pos-
sibly an initial grammatical relation (in the case
of SUBJ in passive sentences, for example) or the
?type?, which specifies the word introducing the
dependent (in the case of XCOMP).
For example, the grammatical relations of the
sentence the man that came ate bananas and ap-
ples with a fork without asking has the following
relations:
SUBJ(eat,man, ),
OBJ(eat,banana),
OBJ(eat,apple),
MOD(fork,eat,with),
SUBJ(come,man, ),
MOD(that,man,come),
XCOMP(without,eat,ask)
The terms ?head? and ?dependent? used
by Carroll et al (1998) to refer to the arguments
of grammatical relations should not be con-
fused with the similar terms in the theory of
dependency grammar. Grammatical relations
and dependency arcs represent different phe-
nomena. An example should suffice to illustrate
the difference; consider The man that came ate
bananas and apples with a fork. In dependency
grammar a unique head is assigned to each word,
for example the head of man is ate. However
man is the dependent of more than one gram-
matical relation, namely SUBJ(eat,man, )
and SUBJ(come,man, ). Furthermore, in
dependency grammar a word can have at most
one dependent of each argument type, and so ate
can have at most one object, for example. But
Link Grammar Conexor FDG
Name Description Name Description
Bs Singular external object of relative clause cc Coordination
Ds Singular determiner det Determiner
Js Singular object of a preposition ins <not documented>
MVp Verb-modifying preposition main Main element
O? Object mod General post-modifier
R Relative clause obj Object
RS Part of subject-type relative clause pcomp Prepositional complement
Ss Singular subject subj Subject
Wd Declarative sentence
Table 1: Some of the dependency types used by Link Grammar and Conexor FDG.
Relation Description
SUBJ(head, dependent, initial gr) Subject
OBJ(head, dependent) Object
XCOMP(type, head, dependent) Clausal complement without an overt subject
MOD(type, head, dependent) Modifier
Table 2: Grammatical relations used in the intrinsic evaluation.
the same is not true for grammatical relations,
and we get both OBJ(eat,banana) and
OBJ(eat,apple).
3.1 Accuracy
Our intrinsic evaluation began on the assumption
that grammatical relations could be deduced from
the dependency structures returned by the parsers.
In practise, however, this deduction process is not
always straightforward; for example complexity
arises when arguments are shared across clauses.
In addition, Link Grammar?s analysis of the front-
most elements as clausal heads complicates the
grammatical relation deduction when there are
modifying clauses.
An existing corpus of 500 sentences/10,000
words annotated with grammatical relations was
used for the evaluation (Carroll et al, 1999). We
restricted the evaluation to just the four relations
shown in Table 2. This decision had two motiva-
tions. Firstly, since the dependency parsers? out-
put did not recognise some distinctions made in
the hierarchy of relations, it did not make sense to
test these distinctions. Secondly, we wanted the
deduction of grammatical relations to be as simple
a process as possible, to minimise the chance of
introducing errors. This second consideration also
led us to purposefully ignore the sharing of argu-
ments induced by control verbs, as this could not
always be deduced reliably. Since this was done
for both parsers the comparison remains meaning-
ful.
Algorithms for producing grammatical relations
from Link Grammar and Conexor FDG output
were developed and implemented. The results of
parsing the corpus are shown in Table 3. Since
Conexor FDG returns one parse per sentence only
and Link Grammar returns all parses ranked, the
first (i.e. the best) parse returned by Link Gram-
mar was used in the intrinsic evaluation.
The table shows significantly lower values of
recall and precision for Link Grammar. This is
partly due to the fact that Link Grammar?s links
often do not connect the head of the clause, as we
have seen with the Wd link in Figure 1.
3.2 Speed
Link Grammar took 1,212 seconds to parse the
10,000 word corpus, while Conexor FDG took
20.5 seconds. This difference is due partly to the
fact that Link Grammar finds and returns multiple
(and often many) alternative parses. For example,
With Link
Grammar
With
Conexor
FDG
Precision SUBJ 50.3% 73.6%
OBJ 48.5% 84.8%
XCOMP 62.2% 76.2%
MOD 57.2% 63.7%
Average 54.6% 74.6%
Recall SUBJ 39.1% 64.5%
OBJ 50% 53.4%
XCOMP 32.1% 64.7%
MOD 53.7% 56.2%
Average 43.7% 59.7%
Table 3: Accuracy of identification of grammatical
relations.
Link Grammar found a total of 410,509 parses of
the 505 corpus sentences.
4 Extrinsic Evaluations
It is important to know not only the accuracy of
a parser but how possible parsing errors affect the
success of an NLP application. This is the goal of
an extrinsic evaluation, where the system is eval-
uated in relation to the embedding setup. Using
answer extraction as an example of an NLP appli-
cation, we compared the performance of the Link
Grammar system and Conexor FDG.
4.1 Answer Extraction and ExtrAns
The fundamental goal of Answer Extraction (AE)
is to locate those exact phrases of unedited text
documents that answer a query worded in nat-
ural language. AE has received much attention
recently, as the increasingly active Question An-
swering track in TREC demonstrates (Voorhees,
2001b; Voorhees, 2001a).
ExtrAns is an answer extraction system that
operates over UNIX manual pages (Molla? et al,
2000). A core process in ExtrAns is the produc-
tion of semantic information in the shape of logi-
cal forms for each sentence of each manual page,
as well as the user query. These logical forms are
designed so that they can be derived from any sen-
tence (using robust approaches to treat very com-
plex or ungrammatical sentences), and they are op-
timised for NLP tasks that involve the semantic
comparison of sentences, such as AE.
ExtrAns? logical forms are called minimal log-
ical forms (MLFs) because they encode the mini-
mum information required for effective answer ex-
traction. In particular, only the main dependencies
between the verb and arguments are expressed,
plus modifier and adjunct relations. Thus, com-
plex quantification, tense and aspect, temporal re-
lations, plurality, and modality are not expressed.
The MLFs use reification to achieve flat expres-
sions, very much in the line of Davidson (1967),
Hobbs (1985), and Copestake et al (1997). In the
current implementation only reification to objects,
eventualities (events or states), and properties is
applied. For example, the MLF of the sentence cp
will quickly copy files is:
holds(e4),
object(cp,o1,[x1]),
object(s command,o2,[x1]),
evt(s copy,e4,[x1,x6]),
object(s file,o3,[x6]),
prop(quickly,p3,[e4]).
In other words, there is an entity x1 which rep-
resents an object of type command;2 there is an
entity x6 (a file); there is an entity e4, which rep-
resents a copying event where the first argument
is x1 and the second argument is x6; there is an
entity p3which states that e4 is done quickly, and
the event e4, that is, the copying, holds.
ExtrAns finds the answers to the questions by
converting the MLFs of the questions into Prolog
queries and then running Prolog?s default resolu-
tion mechanism to find those MLFs that can prove
the question.
This default search procedure is called the syn-
onym mode since ExtrAns uses a small WordNet-
style thesaurus (Fellbaum, 1998) to convert all the
synonyms into a synonym representative. Extr-
Ans also has an approximate mode which, be-
sides normalising all synonyms, scores all docu-
ment sentences on the basis of the maximum num-
ber of predicates that unify between the MLFs of
the query and the answer candidate (Molla? et al,
2000). If all query predicates can be matched then
2ExtrAns uses additional domain knowledge to infer that
cp is a command.
the approximate mode returns exactly the same an-
swers as the synonym mode.
4.2 The Comparison
Ideally, answer extraction systems should be eval-
uated according to how successful they are in help-
ing users to complete their tasks. The use of the
system will therefore depend on such factors as
how many potential answers the user is presented
with at a time, the way these potential answers are
ranked, how many potential answers the user is
prepared to read while searching for an actual an-
swer, and so on. These issues, though important,
are beyond the scope of the present evaluation. In
this evaluation we focus solely on the relevance of
the set of results returned by ExtrAns.
4.2.1 Method
Resources from a previous evaluation of Extr-
Ans (Molla? et al, 2000) were re-used for this eval-
uation. These resources were: a) a collection of
500 man pages, and b) a test set of 26 queries and
relevant answers found in the 500 manual pages.
The careful and labour-intensive construction of
the test set gives us confidence that practically all
relevant answers to each query are present in the
test set. The queries themselves were selected ac-
cording to the following criteria:
? There must be at least one answer in the man-
ual page collection.
? The query asks how to perform a particular
action, or how a particular command works.
? The query is simple, i.e. it asks only one
question.
The manual pages were parsed using Conexor
FDG and Link Grammar. The latter has a param-
eter for outputting either all parses found, or just
the best parse found, and both parameter settings
were used. The queries were then parsed by both
parsers and their logical forms were used to search
the respective databases. The experiment was re-
peated using both the synonym and approximate
search modes.
Parser Precision4 Recall F-score
Conexor FDG 55.8% 8.9% 0.074
LG?best 49.7% 11.4% 0.099
LG?all 50.9% 13.1% 0.120
Table 4: Averages per query in synonym mode.
Parser Precision4 Recall F-score
Conexor FDG 28.3% 21.9% 0.177
LG?best 31.8% 15.8% 0.150
LG?all 40.5% 20.5% 0.183
Table 5: Averages per query in approximate mode.
4.2.2 Results
Precision, Recall and the F-score (with Preci-
sion and Recall equally weighted) for each query
were calculated.3 When no results were returned
for a query the precision could not be calculated,
but the F-score is equal to zero. The results are
shown in Tables 4 and 5. The number of times the
results for a query contained no relevant answers
are shown in Table 6.
The tables show that the approximate mode
gives better results than the synonym mode. This
is to be expected, since the synonym mode returns
exact matches only and therefore some questions
may not produce any results. For those questions,
recall and F would be zero. In fact, the number of
questions without answers in the synonym mode
is so large that the comparison between Conexor
FDG and Link Grammar becomes unreliable in
this mode. In this discussion, therefore, we will
focus on the approximate mode.
The results returned by Link Grammar when all
parses are considered are significantly better than
when only the first (i.e. the best) parse is consid-
3F was calculated using the expression
F = 2 ? |returned and relevant||returned| + |relevant|
which is equivalent to the usual formulation (with ? = 1):
F = (?2 + 1) ? Precision ? Recall?2Precision + Recall
4Average over queries for which precision is defined, i.e.
when the number of returns is non-zero.
Parser Search mode No
results
returned
Nothing
relevant
returned
Con. FDG Synonym 20 20
Con. FDG Approximate 0 8
LG?best Synonym 16 18
LG?best Approximate 1 11
LG?all Synonym 15 18
LG?all Approximate 4 12
Table 6: Numbers of times no relevant answers
were found.
ered. This shows that, in the answer extraction
task, it is better to use the logical forms of all
possible sentence interpretations. Recall increases
and, remarkably, precision increases as well. This
means that the system is more likely to include
new relevant answers when all parses are consid-
ered.
In many applications it is more practical to con-
sider one parse only. Conexor FDG, for example,
returns one parse only, and the parsing speed com-
parison (Section 3.2) shows an important differ-
ence in parsing time. If we compare Conexor FDG
with Link Grammar set to return just the best parse
? since Conexor FDG returns one parse only, this
is the fairest comparison ? we can see that recall
of the system using Conexor FDG is higher than
that of the system using Link Grammar, while re-
taining similar precision.
5 Discussion
The fairest extrinsic comparison between Conexor
FDG and Link Grammar is the one that uses the
best parse returned by Link Grammar, and the an-
swer extraction method follows the approximate
mode. With these settings, Conexor FDG pro-
duces better results than Link Grammar. However,
the results of the extrinsic comparison are far less
dramatic than those of the intrinsic comparison,
specially in the precision figures.
One reason for the difference in the results is
that the intrinsic evaluation compares grammatical
relation accuracy, whereas the answer extraction
system used in the extrinsic evaluation uses logi-
cal forms. A preliminary inspection of the gram-
matical relations and logical forms of questions
and correct answers shows that high overlap of
grammatical relations does not translate into high
overlap of logical forms. A reason for this differ-
ence is that the semantic interpreters used in the
extrinsic evaluation explore exhaustively the de-
pendency structures returned by both parsing sys-
tems and they try to recover as much information
as possible. In contrast with this, the generators of
grammatical relations used in the intrinsic evalua-
tion provide the most direct mapping from depen-
dency structures to grammatical relations. For ex-
ample, typically a dependency structure would not
show a long dependency like the subject of come
in the sentence John wanted Mary to come:
John wanted.v Mary to.o come.v
Ss
TOo
Os I
As a result, the grammatical relations would not
show the subject of come. However, the subject
of come can be traced by following several de-
pendencies (I, TOo and Os above) and ExtrAns?
semantic interpreters do follow these dependen-
cies. In other words, the semantic interpreters
use more information than what is directly en-
coded in the dependency structures. Therefore,
the logical forms contain richer information than
the grammatical relations. We decided not to op-
timise the grammatical relations used in our eval-
uation because we wanted to test the expressivity
of the inherent grammars. It would be question-
able whether we should recover more information
than what is directly expressed. After all, provided
that the parse contains all the words in the origi-
nal order, we can theoretically ignore the sentence
structure and still recover all the information.
6 Summary and Further Work
We have performed intrinsic evaluations of
parsers and extrinsic evaluations within the
context of answer extraction. These evaluations
strengthen Galliers and Sparck Jones (1993)?s
claim that intrinsic evaluations are of very limited
value. In particular, our evaluations show that
intrinsic evaluations may provide results that
are distorted with respect to the most intuitive
purpose of a parsing system: to deliver syntactic
structures to subsequent modules of practical NLP
systems. There is a clear need for frameworks for
extrinsic evaluations of parsers for different NLP
applications.
Further research to confirm this conclusion will
be to try and minimise the occurrence of vari-
ables in the experiments by using the same corpus
for both the intrinsic and the extrinsic evaluations
and/or by using an answer extraction system that
operates on the level of grammatical relations in-
stead of MLFs. Additional further research will
be the use of other intrinsic evaluation methodolo-
gies and extrinsic evaluations within the context of
various other embedding setups.
Acknowledgement
This research is supported by the Macquarie Uni-
versity New Staff grant MUNS?9601/0069.
References
Srinivas Bangalore, Anoop Sarkar, Christine Doran,
and Beth Ann Hockey. 1998. Grammar & parser
evaluation in the XTAG project. In Proc. Workshop
on the Evaluation of Parsing Systems, LREC98.
Ezra Black, S.P. Abney, D. Flickinger, C. Gdaniec,
R. Grisham, P. Harrison, D. Hindle, R. Ingria,
F. Jelinek, J. Klavans, M. Liberman, M.P. Mar-
cus, S. Roukos, B. Santorini, and T. Strzalkowski.
1991. A procedure for quantitatively comparing the
syntactic coverage of English grammars. In Proc.
DARPA Speech and Natural Language Workshop,
pages 306?311, Pacific Grove, CA. Morgan Kauf-
mann.
Ezra Black. 1996. Evaluation of broad-coverage
natural-language parsers. In Ronald A. Cole, Joseph
Mariani, Hans Uszkoreit, Annie Zaenen, and Victor
Zue, editors, Survey of the State of the Art in Hu-
man Language Technology, pages 488?490. CSLU,
Oregon Graduate Institute.
John Carroll, Ted Briscoe, and Antonio Sanfilippo.
1998. Parser evaluation: a survey and a new pro-
posal. In Proc. LREC98.
John Carroll, G. Minnen, and T. Briscoe. 1999. Corpus
annotation for parser evaluation.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proc.
ACL. Santa Cruz.
Ann Copestake, Dan Flickinger, and Ivan A. Sag.
1997. Minimal recursion semantics: an introduc-
tion. Technical report, CSLI, Stanford University,
Stanford, CA.
Donald Davidson. 1967. The logical form of action
sentences. In Nicholas Rescher, editor, The Logic of
Decision and Action, pages 81?120. Univ. of Pitts-
burgh Press.
Christiane Fellbaum. 1998. Wordnet: Introduction. In
Christiane Fellbaum, editor, WordNet: an electronic
lexical database, Language, Speech, and Communi-
cation, pages 1?19. MIT Press, Cambrige, MA.
Julia R. Galliers and Karen Sparck Jones. 1993. Evalu-
ating natural language processing systems. Techni-
cal Report TR-291, Computer Laboratory, Univer-
sity of Cambridge.
Jerry R. Hobbs. 1985. Ontological promiscuity. In
Proc. ACL?85, pages 61?69. University of Chicago,
Association for Computational Linguistics.
Timo Ja?rvinen and Pasi Tapanainen. 1997. A depen-
dency parser for english. Technical Report TR-1,
Department of Linguistics, University of Helsinki,
Helsinki.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proc. IJCAI-
95, pages 1420?1425, Montreal, Canada.
Diego Molla?, Rolf Schwitter, Michael Hess, and
Rachel Fournier. 2000. Extrans, an answer extrac-
tion system. T.A.L., 41(2):495?522.
Daniel D. Sleator and Davy Temperley. 1993. Parsing
English with a link grammar. In Proc. Third Inter-
national Workshop on Parsing Technologies, pages
277?292.
Richard F. E. Sutcliffe, Heinz-Detlev Koch, and An-
nette McElligott, editors. 1996. Industrial Parsing
of Software Manuals. Rodopi, Amsterdam.
Pasi Tapanainen and Timo Ja?rvinen. 1997. A non-
projective dependency parser. In Procs. ANLP-97.
ACL.
Ellen M. Voorhees. 2001a. Overview of the TREC
2001 question answering track. In Ellen M.
Voorhees and Donna K. Harman, editors, Proc.
TREC-10, number 500-250 in NIST Special Publi-
cation. NIST.
Ellen M. Voorhees. 2001b. The TREC question
answering track. Natural Language Engineering,
7(4):361?378.
