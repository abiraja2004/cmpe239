Memory-Based Morphological Analysis 
Antal van den  Bosch  and Wal ter  Dae lemans  
ILK / Computational Linguistics 
Tilburg University 
{antalb,walter}@kub.nl} 
Abst rac t  
We present a general architecture for efficient 
and deterministic morphological nalysis based 
on memory-based learning, and apply it to 
morphological nalysis of Dutch. The system 
makes direct mappings from letters in context 
to rich categories that encode morphological 
boundaries, syntactic lass labels, and spelling 
changes. Both precision and recall of labeled 
morphemes are over 84% on held-out dictionary 
test words and estimated to be over 93% in free 
text. 
1 In t roduct ion  
Morphological analysis is an essential compo- 
nent in language ngineering applications rang- 
ing from spelling error correction to machine 
translation. Performing a full morphological 
analysis of a wordform is usually regarded as a 
segmentation f the word into morphemes, com- 
bined with an analysis of the interaction of these 
morphemes that determine the syntactic lass 
of the wordform as a whole. The complexity of 
wordform morphology varies widely among the 
world's languages, but is regarded quite high 
even in the relatively simple cases, such as En- 
glish. Many wordforms in English and other 
western languages contain ambiguities in their 
morphological composition that can be quite in- 
tricate. General classes of linguistic knowledge 
that are usually assumed to play a role in this 
disambiguation process are knowledge of (i) the 
morphemes of a language, (ii) the morphotac- 
tics, i.e., constraints on how morphemes are al- 
lowed to attach, and (iii) spelling changes that 
can occur due to morpheme attachment. 
State-of-the art systems for morphological 
analysis of wordforms are usually based on 
two-level finite-state transducers (FSTS, Kosken- 
niemi (1983)). Even with the availability of 
sophisticated evelopment tools, the cost and 
complexity of hand-crafting two-level rules is 
high, and the representation of concatenative 
compound morphology with continuation lexi- 
cons is difficult. As in parsing, there is a trade- 
off between coverage and spurious ambiguity in 
these systems: the more sophisticated the rules 
become, the more needless ambiguity they in- 
troduce. 
In this paper we present a learning approach 
which models morphological analysis (includ- 
ing compounding) of complex wordforms as se- 
quences of classification tasks. Our model, 
MBMA (Memory-Based Morphological Analy- 
sis), is a memory-based learning system (Stan- 
fill and Waltz, 1986; Daelemans et al, 1997). 
Memory-based learning is a class of induc- 
tive, supervised machine learning algorithms 
that learn by storing examples of a task in 
memory. Computational effort is invested on 
a "call-by-need" basis for solving new exam- 
ples (henceforth called instances) of the same 
task. When new instances are presented to a 
memory-based learner, it searches for the best- 
matching instances in memory, according to a 
task-dependent similarity metric. When it has 
found the best matches (the nearest neighbors), 
it transfers their solution (classification, label) 
to the new instance. Memory-based learn- 
ing has been shown to be quite adequate for 
various natural-language processing tasks such 
as stress assignment (Daelemans et al, 1994), 
grapheme-phoneme conversion (Daelemans and 
Van den Bosch, 1996; Van den Bosch, 1997), 
and part-of-speech tagging (Daelemans et al, 
1996b). 
The paper is structured as follows. First, we 
give a brief overview of Dutch morphology in 
Section 2. We then turn to a description of 
MBMA in Section 3. In Section 4 we present 
285 
the experimental outcomes of our study with 
MBMA. Section 5 summarizes our findings, re- 
ports briefly on a partial study of English show- 
ing that the approach is applicable to other lan- 
guages, and lists our conclusions. 
2 Dutch  Morpho logy  
The processes of Dutch morphology include 
inflection, derivation, and compounding. In- 
flection of verbs, adjectives, and nouns is 
mostly achieved by suffixation, but a circum- 
fix also occurs in the Dutch past participle (e.g. 
ge+werk+t as the past participle of verb werken, 
to work). Irregular inflectional morphology is 
due to relics of ablaut (vowel change) and to 
suppletion (mixing of different roots in inflec- 
tional paradigms). Processes of derivation in 
Dutch morphology occur by means of prefixa- 
tion and suffixation. Derivation can change the 
syntactic lass of wordforms. Compounding in
Dutch is concatenative (as in German and Scan- 
dinavian languages)' words can be strung to- 
gether almost unlimitedly, with only a few mor- 
photactic constraints, e.g., rechtsinformatica- 
toepassingen (applications of computer science 
in Law). In general, a complex wordform inher- 
its its syntactic properties from its right-most 
part (the head). Several spelling changes occur: 
apart from the closed set of spelling changes due 
to irregular morphology, a number of spelling 
changes is predictably due to morphological 
context. The spelling of long vowels varies be- 
tween double and single (e.g. ik loop, I run, 
versus wii Iop+en, we run); the spelling of root- 
final consonants can be doubled (e.g. ik stop, 
I stop, versus wij stopp+en, we stop); there is 
variation between s and z and f and v (e.g. huis, 
house, versus huizen, houses). Finally, between 
the parts of a compound, a linking morpheme 
may appear (e.g. staat+s+loterij, state lottery). 
For a detailed iscussion of morphological phe- 
nomena in Dutch, see De Haas and Trommelen 
(1993). Previous approaches to Dutch morpho- 
logical analysis have been based on finite-state 
transducers (e.g., XEROX'es morphological n- 
alyzer), or on parsing with context-free word 
grammars interleaved with exploration of pos- 
sible spelling changes (e.g. Heemskerk and van 
Heuven (1993); or see Heemskerk (1993) for a 
probabilistic variant). 
3 Applying memory-based learning 
to morphological ana lys i s  
Most linguistic problems can be seen as,context- 
sensitive mappings from one representation to
another (e.g., from text to speech; from a se- 
quence of spelling words to a parse tree; from 
a parse tree to logical form, from source lan- 
guage to target language, etc.) (Daelemans, 
1995). This is also the case for morphologi- 
cal analysis. Memory-based learning algorithms 
can learn mappings (classifications) if a suffi- 
cient number of instances of these mappings is 
presented to them. 
We drew our instances from the CELEX lex- 
ical data base (Baayen et al, 1993). CELEX 
contains a large lexical data base of Dutch word- 
forms, and features a full morphological naly- 
sis for 247,415 of them. We took each wordform 
and its associated analysis, and created task in- 
stances using a windowing method (Sejnowski 
and Rosenberg, 1987). Windowing transforms 
each wordform into as many instances as it has 
letters. Each example focuses on one letter, 
and includes a fixed number of left and right 
neighbor letters, chosen here to be five. Con- 
sequently, each instance spans eleven letters, 
which is also the average word length in the 
CELEX data base. Moreover, we estimated 
from exploratory data analysis that this con- 
text would contain enough information to allow 
for adequate disambiguation. 
To illustrate the construction of instances, 
Table 1 displays the 15 instances derived from 
the Dutch example word abnormaliteiten (ab- 
normalities) and their associated classes. The 
class of the first instance is "A+Da", which 
says that (i) the morpheme starting in a is an 
adjective ("A") 1, and (ii) an a was deleted at 
the end ("+Da"). The coding thus tells that 
the first morpheme is the adjective abnorrnaal. 
The second morpheme, iteit, has class "N_A,". 
This complex tag indicates that when iteit at- 
taches right to an adjective (encoded by "A,"), 
the new combination becomes a noun ("N_"). 
Finally, the third morpheme is en, which is a 
plural inflection (labeled "m" in CELEX). This 
way we generated an instance base of 2,727,462 
1CELEX features ten syntactic tags: noun (N), adjec- 
tive (A), quantifier/numeral (Q), verb (V), article (D), 
pronoun (O), adverb (B), preposition (P), conjunction 
(C), interjection (J), and abbreviation (X). 
286 
instances. Within these instances, 2422 differ- 
ent class labels occur. The most frequently oc- 
curring class label is "0", occurring in 72.5% of 
all instances. The three most frequent non-null 
labels are "N" (6.9%), "V" (3.6%), and "m" 
(1.6%). Most class labels combine a syntactic 
or inflectional tag with a spelling change, and 
generally have a low frequency. 
When a wordform is listed in CELEX as hav- 
ing more than one possible morphological la- 
beling (e.g., a morpheme may be N or V, the 
inflection -en may be plural for nouns or infini- 
tive for verbs), these labels are joined into am- 
biguous classes ("N/V") and the first generated 
example is labeled with this ambiguous class. 
Ambiguity in syntactic and inflectional tags oc- 
curs in 3.6% of all morphemes in our CELEX 
data. 
The  memory-based  learning algorithm used 
within MBMA is ml -m (Daelemans and Van  
den Bosch, 1992; Dae lemans et al, 1997), an 
extension of IBI (Aha et al, 1991). IBI-IG con- 
structs a data base of instances in memory  dur- 
ing learning. New instances are classified by 
IBI-IG by matching them to all instances in 
the instance base, and calculating with each 
match  the distance between the new instance 
X and the memory  instance Y, A (X~Y)  ---- 
~-\]n W(fi)~(xi,yi),  where W(f i )  is the weight i----1 
of the ith feature, and 5(x~, Yi) is the distance 
between the values of the ith feature in in- 
stances X and Y. When the values of the in- 
stance features are symbolic, as with our linguis- 
tic tasks, the simple overlap distance function 
5 is used: 5(xi,yi) = 0 i f  xi = Yi, else 1. The 
(most frequently occurring) classification of the 
memory instance Y with the smallest A(X,  Y) 
is then taken as the classification of X. 
The weighting function W(f i )  computes for 
each feature, over the full instance base, its 
information gain, a function from information 
theory; cf. Quinlan (1986). In short, the infor- 
mation gain of a feature expresses its relative 
importance compared to the other features in 
performing the mapping from input to classi- 
fication. When information gain is used in the 
similarity function, instances that match on im- 
portant features are regarded as more alike than 
instances that match on unimportant features. 
In our experiments, we are primarily inter- 
ested in the generalization accuracy of trained 
models, i.e., the ability of these models to use 
their accumulated knowledge to classify new 
instances that were not in the training mate- 
rial. A method that gives a good estimate 
of the generalization performance of an algo- 
rithm on a given instance base, is 10-fold cross- 
validation (Weiss and Kulikowski, 1991). This 
method generates on the basis of an instance 
base 10 subsequent partitionings into a training 
set (90%) and a test set (10%), resulting in 10 
experiments. 
4 Exper iments :  MBMA of  Dutch  
word forms 
As described, we performed 10-fold cross vali- 
dation experiments in an experimental matrix 
in which MBMA is applied to the full instance 
base, using a context width of five left and right 
context letters. We structure the presentation 
of the experimental outcomes as follows. First, 
we give the generalization accuracies on test in- 
stances and test words obtained in the exper- 
iments, including measurements of generaliza- 
tion accuracy when class labels are interpreted 
at lower levels of granularity. While the latter 
measures give a rough idea of system accuracy, 
more insight is provided by two additional anal- 
yses. First, precision and recall rates of mor- 
phemes are given. We then provide prediction 
accuracies of syntactic word classes. Finally, we 
provide estimations on free-text accuracies. 
4.1 Genera l i za t ion  accurac ies  
The percentages of correctly classified test in- 
stances are displayed in the top line of Table 2, 
showing an error in test instances of about 4.1% 
(which is markedly better than the baseline r- 
ror of 27.5% when guessing the most frequent 
class "0"), which translates in an error at the 
word level of about 35%. The output of MBMA 
can also be viewed at lower levels of granularity. 
We have analyzed MBMA's output at the three 
following lower granularity levels: 
1. Only decide, per letter, whether a seg- 
mentation occurs at that letter, and if so, 
whether it marks the start of a derivational 
stem or an inflection. This can be derived 
straightforwardly from the full-task class 
labeling. 
2. Only decide, per letter, whether a segmen- 
tation occurs at that letter. Again, this can 
287 
instance 
number 
1 
2 
3 
4 
left 
context 
- -  - a 
_ _ a b 
5 _ a b n 
6 a b n o 
7 b n o r 
8 n o r m 
o r m a 
10 r m a I 
11 rn a I i 
12 
13 
14 
15 
a I i t 
I i t e 
i t e i 
t e i t 
I fOCUS 
letter I 
a 
a b 
b n 
n o 
o r 
r m 
m a 
a I 
I i 
i t 
t e 
e i 
i t 
t e 
e n 
right 
context TASK 
b n o r m A+Da 
n o r m a 0 
o r m a I 0 
r m a I i 0 
m a I i t 0 
a I i t e 0 
I i t e i 0 
i t e i t 0 
t e i t e N_A,  
e i t e n 0 
i t e n _ 0 
_ 0 
_ 0 
_ m 
_ 0 
t e n _ 
e n n 
Table 1: Instances with morphological nalysis classifications derived from abnormaliteiten, ana- 
lyzed as \[abnormaal\]A\[iteit\]N_A,\[en\]m. 
be derived straightforwardly. This task im- 
plements egmentation of a complex word 
form into morphemes. 
3. Only check whether the desired spelling 
change is predicted correctly. Because of 
the irregularity of many spelling changes 
this is a hard task. 
The results from these analyses are displayed 
in Table 2 under the top line. First, Ta- 
ble 2 shows that performance on the lower- 
granularity tasks that exclude detailed syntac- 
tic labeling and spelling-change prediction is 
about 1.1% on test instances, and roughly 10% 
on test words. Second, making the distinction 
between inflections and other morphemes i  al- 
most as easy as just determining whether there 
is a boundary at all. Third, the relatively low 
score on correctly predicted spelling changes, 
80.95%, indicates that it is particularly hard 
to generalize from stored instances of spelling 
changes to new ones. This is in accordance with 
the common linguistic view on spelling-change 
exceptions. When, for instance, a past-tense 
form of a verb involves a real exception (e.g., 
the past tense of Dutch brengen,  to bring, is 
bracht ) ,  it is often the case that this exception is
confined to generalize to only a few other exam- 
ples of the same verb (b rachten ,  gebracht )  and 
not to any other word that is not derived from 
the same stem, while the memory-based learn- 
ing approach is not aware of such constraints. 
A post-processing step that checks whether the 
proposed morphemes are also listed in a mor- 
pheme lexicon would correct many of these er- 
rors, but has not been included here. 
4.2  P rec i s ion  and  recal l  of  morphemes  
Precision is the percentage of morphemes pre- 
dicted by MBMA that is actually a morpheme 
in the target analysis; recall is the percentage 
of morphemes in the target analysis that are 
also predicted by MBMA. Precision and recall 
of morphemes can again be computed at differ- 
ent levels of granularity. Table 3 displays these 
computed values. The results show that both 
precision and recall of fully-labeled morphemes 
within test words are relatively low. It comes 
as no surprise that the level of 84% recalled 
fully labeled morphemes, including spelling in- 
formation, is not much higher than the level of 
80% correctly recalled spelling changes (see Ta- 
ble 2). When word-class information, type of 
inflection, and spelling changes are discarded, 
precision and recall of basic segment types be- 
comes quite accurate: over 94%. 
288 
instances words 
class labeling granularity labeling example % :t: % + 
full morphological nalysis \[abnormaai\]A\[iteit\]N_A,\[en\]m 95.88 0.04 64.63 0.24 
derivation/inflection \[abnormal\]deriv\[iteit\]deriv\[en\]in/l 98.83 0.02 89.62 0.17 
segmentation \[abnormal\]\[iteit\]\[en\] 98.97 0.02 90.69 0.02 
spelling changes +Da 80.95 0.40 
Table 2: Generalization accuracies in terms of the percentage of correctly classified test instances 
and words, with standard eviations (+) of MBMA applied to full Dutch morphological nalysis and 
three lower-granularity tasks derived from MBMA's full output. The example word abnormaliteiten 
is shown according to the different labeling granularities, and only its single spelling change at the 
bottom line). 
precision recall 
task variation (%) (%) 
full morphological nalysis 84.33 83.76 
derivation/inflection 94.72 94.07 
segmentation 94.83 94.18 
Table 3: Precision and recall of morphemes, de- 
rived from the classification output of MBMA 
applied to the full task and two lower- 
granularity variations of Dutch morphological 
analysis, using a context width of five left and 
right letters. 
4.3 Pred ic t ing  the syntact ic  class of 
word forms 
Since MBMA predicts the syntactic label of 
morphemes, and since complex Dutch word- 
forms generally inherit their syntactic proper- 
ties from their right-most morpheme, MBMA's 
syntactic labeling can be used to predict the 
syntactic lass of the full wordform. When ac- 
curate, this functionality can be an asset in han- 
dling unknown words in part-of-speech tagging 
systems. The results, displayed in Table 4, show 
that about 91.2% of all test words are assigned 
the exact tag they also have in CELEX (includ- 
ing ambiguous tags such as "N/V" - 1.3% word- 
forms in the CELEX dataset have an ambiguous 
syntactic tag). When MBMA's output is also 
considered correct if it predicts at least one out 
of the possible tags listed in CELEX, the accu- 
racy on test words is 91.6%. These accuracies 
compare favorably with a related (yet strictly 
incomparable) approach that predicts the word 
class from the (ambiguous) part-of-speech tags 
of the two surrounding words, the first letter, 
and the final three letters of Dutch words, viz. 
71.6% on unknown words in texts (Daelemans 
et al, 1996a). 
!syntactic lass correct test words 
prediction words (%) -4- 
!exact 91.24 0.21 
exact or among alternatives 91.60 0.21 
Table 4: Average prediction accuracies (with 
standard deviations) of MBMA on syntactic 
classes of test words. The top line displays exact 
matches with CELEX tags; the bottom line also 
includes predictions that are among CELEX al- 
ternatives. 
4.4 Free text  es t imat ion  
Although some of the above-mentioned accu- 
racy results, especially the precision and recall 
of fully-labeled morphemes, eem not very high, 
they should be seen in the context of the test 
they are derived from: they stem from held-out 
portions of dictionary words. In texts sampled 
from real-life usage, words are typically smaller 
and morphologically ess complex, and a rela- 
tively small set of words re-occurs very often. 
It is therefore relevant for our study to have 
an estimate of the performance of MBMA on 
real texts. We generate such an estimate fol- 
lowing these considerations: New, unseen text 
is bound to contain a lot of words that are in the 
245,000 CELEX data base, but also some number 
of unknown words. The morphological naly- 
ses of known words are simply retrieved by the 
memory-based learner from memory. Due to 
some ambiguity in the class labeling in the data 
base itself, retrieval accuracy will be somewhat 
289 
below 100%. The morphological nalyses of un- 
known words are assumed to be as accurate as 
was tested in the above-mentioned experiments: 
they can be said to be of the type of dictionary 
words in the 10% held-out test sets of 10-fold 
cross validation experiments. CELEX bases its 
wordform frequency information on word counts 
made on the 42,380,000-words Dutch INL cor- 
pus. 5.06% of these wordforms are wordform 
tokens that occur only once. We assume that 
this can be extrapolated to the estimate that 
in real texts, 5% of the words do not occur 
in the 245,000 words of the CELEX data base. 
Therefore, a sensible estimate of the accura- 
cies of memory-based learners on real text is a 
weighted sum of accuracies comprised of 95% of 
the reproduction accuracy (i.e, the error on the 
training set itself), and 5% of the generalization 
accuracy as reported earlier. 
Table 5 summarizes the estimated generaliza- 
tion accuracy results computed on the results 
of MBMA. First, the percentages of correct in- 
stances and words are estimated to be above 
98% for the full task; in terms of words, it is es- 
t imated that 84% of all words are fully correctly 
analyzed. When lower-granularity classification 
tasks are discerned, accuracies on words are es- 
timated to exceed 96% (on instances, less than 
1% errors are estimated). Moreover, precision 
and recall of morphemes on the full task are 
estimated to be above 93%. A considerable sur- 
plus is obtained by memory retrieval in the es- 
timated percentage of correct spelling changes: 
93%. Finally, the prediction of the syntactic 
tags of wordforms would be about 97% accord- 
ing to this estimate. 
We briefly note that Heemskerk (1993) re- 
ports a correct word score of 92% on free text 
test material yielded by the probabilistic mor- 
phological analyzer MORPA. MORPA segments 
wordforms, decides whether a morpheme is a 
stem, an affix or an inflection, detects pelling 
changes, and assigns a syntactic tag to the word- 
form. We have not made a conversion of our 
output to Heemskerk's (1993). Moreover, a 
proper comparison would demand the same test 
data, but we believe that the 92% corresponds 
roughly to our MBMA estimates of 97.2% correct 
syntactic tags, 93.1% correct spelling changes, 
and 96.7% correctly segmented words. 
Estimate 
correct instances, full task 
correct words, full task 
98.4% 
84.2% 
correct instances, derivation/inflection 99.6% 
correct words, derivation/inflection 96.7% 
correct instances, segmentation 
correct words, segmentation 
99.6% 
96.7% 
precision of fully-labeled morphemes 93.6% 
recall of fully-labeled morphemes 93.2% 
precision of deriv./intl, morphemes 98.5% 
recall of deriv./inft, morphemes 98.0% 
precision of segments 98.5% 
recall of segments 97.9% 
correct spelling changes 
correct syntactic wordform t a ~  
Table 5: Estimations of accuracies on real text, 
derived from the generalization accuracies of 
MBMA on full Dutch morphological analysis. 
5 Conc lus ions  
We have demonstrated the applicability of 
memory-based learning to morphological nal- 
ysis, by reformulating the problem as a classi- 
fication task in which letter sequences are clas- 
sifted as marking different ypes of morpheme 
boundaries. The generalization performance of 
memory-based learning algorithms to the task 
is encouraging, given that the tests are done 
on held-out (dictionary) words. Estimates of 
free-text performance give indications of high 
accuracies: 84.6% correct fully-analyzed words 
(64.6% on unseen words), and 96.7% correctly 
segmented and coarsely-labeled words (about 
90% for unseen words). Precision and recall 
of fully-labeled morphemes i  estimated in real 
texts to be over 93% (about 84% for unseen 
words). Finally, the prediction of (possibly am- 
biguous) syntactic classes of unknown word- 
forms in the test material was shown to be 
91.2% correct; the corresponding free-text es- 
timate is 97.2% correctly-tagged wordforms. 
In comparison with the traditional approach, 
which is not immune to costly hand-crafting and 
spurious ambiguity, the memory-based learning 
approach applied to a reformulation ofthe prob- 
lem as a classification task of the segmentation 
type, has a number of advantages: 
290 
? it presupposes no more linguistic knowl- 
edge than explicitly present in the cor- 
pus used for training, i.e., it avoids a 
knowledge-acquisition bottleneck; 
? it is language-independent, as it functions 
on any morphologically analyzed corpus in 
any language; 
? learning is automatic and fast; 
? processing is deterministic, non-recurrent 
(i.e., it does not retry analysis generation) 
and fast, and is only linearly related to the 
length of the wordform being processed. 
The language-independence of the approach 
can be illustrated by means of the following par- 
tial results on MBMA of English. We performed 
experiments on 75,745 English wordforms from 
CELEX and predicted the lower-granularity 
tasks of predicting morpheme boundaries (Van 
den Bosch et al, 1996). Experiments yielded 
88.0% correctly segmented test words when de- 
ciding only on the location of morpheme bound- 
aries, and 85.6% correctly segmented test words 
discerning between derivational nd inflectional 
morphemes. Both results are roughly compa- 
rable to the 90% reported here (but note the 
difference in training set size). 
A possible limitation of the approach may 
be the fact that it cannot return more than 
one possible segmentation for a wordform. E.g. 
the compound word kwartslagen can be inter- 
preted as either kwart+slagen (quarter turns) 
or kwarts+lagen (quartz layers). The memory- 
based approach would select one segmentation. 
However, true segmentation ambiguity of this 
type is very rare in Dutch. Labeling ambigu- 
ity occurs more often (3.6% of all morphemes), 
and the current approach simply produces am- 
biguous tags. However, it is possible for our 
approach to return distributions of possible 
classes, if desired, as well as it is possible to "un- 
pack" ambiguous labeling into lists of possible 
morphological nalyses of a wordform. If, for 
example, MBMA's output for the word bakken 
(bake, an infinitive or plural verb form, or bins, 
a plural noun) would be \[bak\]v/N\[en\]tm/i/m, 
then this output could be expanded unambigu- 
ously into the noun analysis \[bak\]N\[en\]m (plu- 
ral) and the two verb readings \[bak\]y\[en\]i (in- 
finitive) and \[bak\]y\[en\]tm (present ense plu- 
ral). 
Points of future research are comparisons 
with other morphological analyzers and lem- 
matizers; applications of MBMA to other lan- 
guages (particularly those with radically differ- 
ent morphologies); and qualitative analyses of 
MBMA's output in relation with linguistic pre- 
dictions of errors and markedness ofexceptions. 
Acknowledgements  
This research was done in the context of 
the "Induction of Linguistic Knowledge" (ILK) 
research programme, supported partially by 
the Netherlands Organization for Scientific Re- 
search (NWO). The authors wish to thank Ton 
Weijters and the members of the Tilburg ILK 
group for stimulating discussions. A demonstra- 
tion version of the morphological nalysis sys- 
tem for Dutch is available via ILK's homepage 
http : / / i l k .  kub. nl. 
References  
D. W. Aha, D. Kibler, and M. Albert. 1991. 
Instance-based learning algorithms. Machine 
Learning, 6:37-66. 
R. H. Baayen, R. Piepenbrock, and H. van Rijn. 
1993. The CELEX lexical data base on CD- 
ROM. Linguistic Data Consortium, Philadel- 
phia, PA. 
W. Daelemans and A. Van den Bosch. 1992. 
Generalisation performance of backpropaga- 
tion learning on a syllabification task. In 
M. F. J. Drossaers and A. Nijholt, editors, 
Proc. of TWLT3: Connectionism and Nat- 
ural Language Processing, pages 27-37, En- 
schede. Twente University. 
W. Daelemans and A. Van den Bosch. 
1996. Language-independent da a-oriented 
grapheme-to-phoneme conversion. In J. P. H. 
Van Santen, R. W. Sproat, J. P. Olive, and 
J. Hirschberg, editors, Progress in Speech 
Processing, pages 77-89. Springer-Verlag, 
Berlin. 
W. Daelemans, S. Gillis, and G. Durieux. 
1994. The acquisition of stress: a data- 
oriented approach. Computational Linguis- 
tics, 20(3):421-451. 
W. Daelemans, J. Zavrel, and P. Berck. 
1996a. Part-of-speech tagging for Dutch with 
MBT, a memory-based tagger generator. In 
K. van der Meer, editor, Informatieweten- 
schap 1996, Wetenschappelijke bijdrage aan 
291 
de Vierde Interdisciplinaire Onderzoekscon- 
ferentie In,formatiewetenchap, ages 33-40, 
The Netherlands. TU Delft. 
W. Daelemans, J. Zavrel, P. Berck, and S. Gillis. 
1996b. MBT: A memory-based part of speech 
tagger generator. In E. Ejerhed and I. Dagan, 
editors, Proc. of Fourth Workshop on Very 
Large Corpora, pages 14-27. ACL SIGDAT. 
W. Daelemans, A. Van den Bosch, and A. Weij- 
ters. 1997. IGwree: using trees for com- 
pression and classification in lazy learning 
algorithms. Artificial Intelligence Review, 
11:407-423, 
W. Daelemans. 1995. Memory-based lexical ac- 
quisition and processing. In P. Steffens, ed- 
itor, Machine Translation and the Lexicon, 
Lecture Notes in Artificial Intelligence, pages 
85-98. Springer-Verlag, Berlin. 
W. De Haas and M. Trommelen. 1993. Mor- 
,fologisch andboek van her Nederlands: Een 
overzicht van de woordvorming. SDU, 's 
Gravenhage, The Netherlands. 
J. Heemskerk and V. van Heuven. 1993. 
MORPA: A morpheme lexicon-based mor- 
phological parser. In V. van Heuven and 
L. Pols, editors, Analysis and synthesis o,f 
speech; Strategic research towards high-quality 
speech generation, pages 67-85. Mouton de 
Gruyter, Berlin. 
J. Heemskerk. 1993. A probabilistic ontext- 
free grammar for disambiguation i  morpho- 
logical parsing. In Proceedings of the 6th Con- 
ference of the EACL, pages 183-192. 
K. Koskenniemi. 1983. Two-level morphol- 
ogy: a general computational model -for word- 
-form recognition and production. Ph.D. the- 
sis, University of Helsinki. 
J.R. Quinlan. 1986. Induction of Decision 
Trees. Machine Learning, 1:81-206. 
T. J. Sejnowski and C. S. Rosenberg. 1987. Par- 
allel networks that learn to pronounce English 
text. Complex Systems, 1:145-168. 
C. Stanfill and D. Waltz. 1986. Toward 
memory-based reasoning. Communications 
o,f the ACM, 29(12):1213-1228, December. 
A. Van den Bosch, W. Daelemans, and A. Weij- 
ters. 1996. Morphological analysis as classi- 
fication: an inductive-learning approach. In 
K. Ofiazer and H. Somers, editors, Proceed- 
ings of the Second International Con,ference 
on New Methods in Natural Language Pro- 
cessing, NeMLaP-P, Ankara, Turkey, pages 
79-89. 
A. Van den Bosch. 1997. Learning to pro- 
nounce written words: A study in inductive 
language learning. Ph.D. thesis, Universiteit 
Maastricht. 
S. Weiss and C. Kulikowski. 1991. Computer 
systems that learn. San Mateo, CA: Morgan 
Kaufmann. 
292 
