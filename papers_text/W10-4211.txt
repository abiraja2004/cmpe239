Generating and Validating Abstracts of Meeting Conversations: a User
Study
Gabriel Murray
gabrielm@cs.ubc.ca
Giuseppe Carenini
carenini@cs.ubc.ca
Department of Computer Science, University of British Columbia
Vancouver, Canada
Raymond Ng
rng@cs.ubc.ca
Abstract
In this paper we present a complete sys-
tem for automatically generating natural
language abstracts of meeting conversa-
tions. This system is comprised of com-
ponents relating to interpretation of the
meeting documents according to a meet-
ing ontology, transformation or content
selection from that source representation
to a summary representation, and gener-
ation of new summary text. In a forma-
tive user study, we compare this approach
to gold-standard human abstracts and ex-
tracts to gauge the usefulness of the dif-
ferent summary types for browsing meet-
ing conversations. We find that our auto-
matically generated summaries are ranked
significantly higher than human-selected
extracts on coherence and usability crite-
ria. More generally, users demonstrate a
strong preference for abstract-style sum-
maries over extracts.
1 Introduction
The most common solution to the task of summa-
rizing spoken and written data is sentence (or ut-
terance) extraction, where binary sentence classi-
fication yields a cut-and-paste summary compris-
ing informative sentences from the document con-
catenated in a new, condensed document. Such
extractive approaches have dominated the field of
automatic summarization for decades, in large part
because extractive systems do not require a natu-
ral language generation (NLG) component since
the summary sentences are simply lifted from the
source document.
Extrinsic evaluations have shown that, while ex-
tractive summaries may be less coherent than hu-
man abstracts, users still find them to be valuable
tools for browsing documents (He et al, 1999;
McKeown et al, 2005; Murray et al, 2009). How-
ever, these previous evaluations also illustrate that
concise abstracts are generally preferred by users
and lead to higher objective task scores. A weak-
ness of typical extractive summaries is that the end
user does not know why the extracted sentences
are important; exploring the original sentence con-
text may be the only way to resolve this uncer-
tainty. And if the input source document consists
of noisy, unstructured text such as ungrammatical,
disfluent multi-party speech, then the resultant ex-
tract is likely to be noisy and unstructured as well.
Herein we describe a complete and fully auto-
matic system for generating abstract summaries
of meeting conversations. Our abstractor maps
input sentences to a meeting ontology, generates
messages that abstract over multiple sentences,
selects the most informative messages, and ulti-
mately generates new text to describe these rele-
vant messages at a high level. We conduct a user
study where participants must browse a meeting
conversation within a very constrained timeframe,
having a summary at their disposal. We compare
our automatic abstracts with human abstracts and
extracts and find that our abstract summaries sig-
nificantly outperform extracts in terms of coher-
ence and usability according to human ratings. In
general, users rate abstract-style summaries much
more highly than extracts for these conversations.
2 Related Research
Automatic summarizaton has been described as
consisting of interpretation, transformation and
generation (Jones, 1999). Popular approaches to
text extraction essentially collapse interpretation
and transformation into one step, with genera-
tion either being ignored or consisting of post-
processing techniques such as sentence compres-
sion (Knight and Marcu, 2000; Clarke and Lapata,
2006) or sentence merging (Barzilay and McKe-
own, 2005). In contrast, in this work we clearly
separate interpretation from transformation and in-
corporate an NLG component to generate new text
to describe meeting conversations.
While extraction remains the most common ap-
proach to text summarization, one application in
which abstractive summarization is widely used is
data-to-text generation. Summarization is critical
for data-to-text generation because the amount of
collected data may be massive. Examples of such
applications include the summarization of inten-
sive care unit data in the medical domain (Portet
et al, 2009) and data from gas turbine sensors (Yu
et al, 2007). Our approach is similar except that
our input is text data in the form of conversations.
We otherwise utilize a very similar architecture of
pattern recognition, pattern abstraction, pattern
selection and summary generation.
Kleinbauer et al (2007) carry out topic-based
meeting abstraction. Our systems differ in two
major respects: their summarization process uses
human gold-standard annotations of topic seg-
ments, topic labels and content items from the on-
tology, while our summarizer is fully automatic;
secondly, the ontology they used is specific not
just to meetings but to the AMI scenario meetings
(Carletta et al, 2005), while our ontology applies
to conversations in general, allowing our approach
to be extended to emails, blogs, etc.
In this work we conduct a user study where par-
ticipants use summaries to browse meeting tran-
scripts. Some previous work has compared ex-
tracts and abstracts for the task of a decision au-
dit (Murray et al, 2009) , finding that human ab-
stracts are a challenging gold-standard in terms
of enabling participants to work quickly and cor-
rectly identify the relevant information. For that
task, automatic extracts and the semi-automatic
abstracts of Kleinbauer et al (2007) were found
to be competitive with one another in terms of
user satisfaction and resultant task scores. Other
research on comparing extracts and abstracts has
found that an automatic abstractor outperforms a
generic extractor in the domains of technical ar-
ticles (Saggion and Lapalme, 2002) and evalua-
tive reviews (Carenini and Cheung, 2008), and that
human-written abstracts were rated best overall.
3 Interpretation - Ontology Mapping
Source document interpretation in our system re-
lies on a general conversation ontology. The on-
tology is written in OWL/RDF and contains upper-
level classes such as Participant, Entity, Utterance,
and DialogueAct. When additional information is
available about participant roles in a given domain,
Participant subclasses such as ProjectManager can
be utilized. Object properties connect instances of
ontology classes; for example, the following entry
in the ontology states that the object property has-
Speaker has an instance of Utterance as its domain
and an instance of Participant as its range.
<owl:ObjectProperty rdf:about="#hasSpeaker">
<rdfs:range rdf:resource="#Participant"/>
<rdfs:domain rdf:resource="#Utterance"/>
</owl:ObjectProperty>
The DialogueAct class has subclasses cor-
responding to a variety of sentence-level phe-
nomena: decisions, actions, problems, positive-
subjective sentences, negative-subjective sen-
tences and general extractive sentences (important
sentences that may not match the other categories).
Utterance instances are connected to DialogueAct
subclasses through an object property hasDAType.
A single utterance may correspond to more than
one DialogueAct; for example, it may represent
both a positive-subjective sentence and a decision.
Our current definition of Entity instances is
simple. The entities in a conversation are noun
phrases with mid-range document frequency. This
is similar to the definition of concept proposed by
Xie et al (2009), where n-grams are weighted
by tf.idf scores, except that we use noun phrases
rather than any n-grams because we want to refer
to the entities in the generated text. We use mid-
range document frequency instead of idf (Church
and Gale, 1995), where the entities occur in be-
tween 10% and 90% of the documents in the col-
lection. We do not currently attempt coreference
resolution for entities; recent work has investi-
gated coreference resolution for multi-party dia-
logues (Muller, 2007; Gupta et al, 2007), but the
challenge of resolution on such noisy data is high-
lighted by low accuracy (e.g. F-measure of 21.21)
compared with using well-formed text.
We map sentences to our ontology classes by
building numerous supervised classifiers trained
on labeled decision sentences, action sentences,
etc. A general extractive classifier is also trained
on sentences simply labeled as important. We give
a specific example of the ontology mapping using
the following excerpt from the AMI corpus, with
entities italicized and resulting sentence classifica-
tions shown in bold:
? A: And you two are going to work together
on a prototype using modelling clay. [action]
? A: You?ll get specific instructions from your
personal coach. [action]
? C: Cool. [positive-subjective]
? A: Um did we decide on a chip? [decision]
? A: Let?s go with a simple chip. [decision,
positive-subjective]
The ontology is populated by adding all of
the sentence entities as instances of the Entity
class, all of the participants as instances of the
Participant class (or its subclasses such as Pro-
jectManager when these are represented), and all
of the utterances as instances of Utterance with
their associated hasDAType properties indicating
the utterance-level phenomena of interest. Here
we show a sample Utterance instance:
<Utterance rdf:about="#ES2014a.B.dact.37">
<hasSpeaker rdf:resource="#IndustrialDesigner"/>
<hasDAType rdf:resource="#PositiveSubjective"/>
<begTime>456.58</begTime>
<endTime>458.832</endTime>
</Utterance>
3.1 Feature Set
The interpretation component as just described re-
lies on supervised classifiers for the detection of
items such as decisions, actions, and problems.
This component uses general features that are ap-
plicable to any conversation domain. The first set
of features we use for this ontology mapping are
features relating to conversational structure. They
include sentence length, sentence position in the
conversation and in the current turn, pause-style
features, lexical cohesion, centroid scores, and
features that measure how terms cluster between
conversation participants and conversation turns.
While these features have been found to work
well for generic extractive summarization (Murray
and Carenini, 2008), we use additional features
for capturing the more specific sentence-level phe-
nomena of this research. These include character
trigrams, word bigrams, part-of-speech bigrams,
word pairs, part-of-speech pairs, and varying in-
stantiation n-grams, described in more detail in
(Murray et al, 2010). After removing features
that occur fewer than five times, we end up with
218,957 total features.
3.2 Message Generation
Rather than merely classifying individual sen-
tences as decisions, action items, and so on, we
also aim to detect larger patterns ? or messages
? within the meeting. For example, a given par-
ticipant may repeatedly make positive comments
about an entity throughout the meeting, or may
give contrasting opinions of an entity. In or-
der to determine which messages are essential for
summarizing meetings, three human judges con-
ducted a detailed analysis of four development
set meetings. They first independently examined
previously-written human abstracts for the meet-
ings to identify which messages were present in
the summaries. In the second step, the judges met
together to decide on a final message set. This
resulted in a set of messages common to all the
meetings and agreed upon by all the judges. The
messages that our summarizer will automatically
generate are defined as follows:
? OpeningMessage and ClosingMessage: Briefly de-
scribes opening/closing of the meeting
? RepeatedPositiveMessage and RepeatedNegativeMes-
sage: Describes a participant making positive/negative
statements about a giv en entity
? ActionItemsMessage: Indicates that a participant has
action items relating to some entity
? DecisionMessage: Indicates that a participant was in-
volved in a decision-making process regarding some
entity
? ProblemMessage: Indicates that a participant repeat-
edly discussed problems or issues about some entity
? GeneralDiscussionMessage: Indicates that a partici-
pant repeatedly discussed a given entity
Message generation takes as input the ontology
mapping described in the previous section, and
outputs a set of messages for a particular meeting.
This is done by identifying pairs of Participants
and Entities that repeatedly co-occur with the var-
ious sentence-level predictions. For example, if
the project manager repeatedly discusses the inter-
face using utterances that are classified as positive-
subjective, a RepeatedPositiveMessage is gener-
ated for that Participant-Entity pair. Messages are
generated in a similar fashion for all other mes-
sage types except for the opening and closing mes-
sages. These latter two messages are created sim-
ply by identifying which participants were most
active in the introductory and concluding portions
of the meeting and generating messages that de-
scribe that participant opening or closing the meet-
ing.
Messages types are defined within the OWL on-
tology, and the ontology is populated with mes-
sage instances for each meeting. The following
message describes the Marketing Expert making
a decision concerning the television, and lists the
relevant sentences contained by that decision mes-
sage.
<DecisionMessage rdf:about="#dec9">
<messageSource rdf:resource="#MarketingExpert"/>
<messageTarget rdf:resource="#television"/>
<containsUtterance rdf:resource="#ES2014a.D.dact.55"/>
<containsUtterance rdf:resource="#ES2014a.D.dact.63"/>
</DecisionMessage>
4 Transformation - ILP Content
Selection for Messages
Having detected all the messages for a given meet-
ing conversation, we now turn to the task of
transforming the source representation to a sum-
mary representation, which involves identifying
the most informative messages for which we will
generate text. We choose an integer linear pro-
gramming (ILP) approach to message selection.
ILP has previously been used for sentence selec-
tion in an extractive framework. Xie et al (2009)
used ILP to create a summary by maximizing a
global objective function combining sentence and
entity weights. Our method is similar except that
we are selecting messages based on optimizing
an objective function combining message and sen-
tence weights:
maximize (1??)?
?
i
wisi +??
?
j
ujmj (1)
subject to
?
i
lisi < L (2)
where wi is the score for sentence i, uj is the
score for message j, si is a binary variable in-
dicating whether sentence i is selected, mj is a
binary variable indicating whether message j is
selected, li is the length of sentence i and L is
the desired summary length. The ? term is used
to balance sentence and message weights. Our
sentence weight wi is the sum of all the poste-
rior probabilities for sentence i derived from the
various sentence-level classifiers. In other words,
sentences are weighted highly if they correspond
to multiple object properties in the ontology. To
continue the example from Section 3, the sen-
tence Let?s go with the simple chip will be highly
weighted because it represents both a decision and
a positive-subjective opinion. The message score
uj is the number of sentences contained by the
message j. For instance, the DecisionMessage
at the end of Section 3.2 contains two sentences.
We can create a higher level of abstraction in our
summaries if we select messages which contain
numerous utterances. Similar to how sentences
and concepts are combined in the previous ILP ex-
traction approach (Xie et al, 2009; Gillick et al,
2009), messages and sentences are tied together by
two additional constraints:
?
j
mjoij ? si ?i (3)
mjoij ? si ?ij (4)
where oij is the occurence of sentence i in mes-
sage j. These constraints state that a sentence can
only be selected if it occurs in a message that is
selected, and that a message can only be selected
if all of its sentences have also been selected.
For these initial experiments, ? is set to 0.5. The
summary length L is set to 15% of the conver-
sation word count. Note that this is a constraint
on the length of the selected utterances; we ad-
ditionally place a length constraint on the gener-
ated summary described in the following section.
The reason for both types of length constraint is to
avoid creating an abstract that is linked to a great
many conversation utterances but is very brief and
likely to be vague and uninformative.
5 Summary Generation
The generation component of our system fol-
lows the standard pipeline architecture (Reiter and
Dale, 2000), comprised of a text planner, a micro-
planner and a realizer. We describe each of these
in turn.
5.1 Text Planning
The input to the document planner is an ontol-
ogy which contains the selected messages from
the content selection stage. We take a top-
down, schema-based approach to document plan-
ning (Reiter and Dale, 2000). This method is ef-
fective for summaries with a canonical structure,
as is the case with meetings. There are three high-
level schemas invoked in order: opening mes-
sages, body messages, and closing messages. For
the body of the summary, messages are retrieved
from the ontology using SPARQL, an SQL-style
query language for ontologies, and are clustered
according to entities. Entities are temporally or-
dered according to their average timestamp in the
meeting. In the overall document plan tree struc-
ture, the body plan is comprised of document sub-
plans for each entity, and the document sub-plan
for each entity is comprised of document sub-
plans for each message type. The output of the
document planner is a tree structure with messages
as its leaves and document plans for its internal
nodes. Our text planner is implemented within the
Jena semantic web programming framework1.
5.2 Microplanning
The microplanner takes the document plan as in-
put and performs two operations: aggregation and
generation of referring expressions.
5.2.1 Aggregation
There are several possibilities for aggregation in
this domain, such as aggregating over participants,
entities and message types. The analysis of our
four development set meetings revealed that ag-
gregation over meeting participants is quite com-
mon in human abstracts, so our system supports
such aggregation. This involves combining mes-
sages that differ in participants but share a com-
mon entity and message type; for example, if there
are two RepeatedPositiveMessage instances about
the user interface, one with the project manager
as the source and one with the industrial designer
as the source, a single RepeatedPositiveMessage
instance is created that contains two sources. We
do not aggregate over entities for the sole reason
that the text planner already clustered messages
according to entity. The entity clustering is in-
tended to give the summary a more coherent struc-
ture but has the effect of prohibiting aggregation
over entities.
5.2.2 Referring Expressions
To reduce redundancy in our generated abstracts,
we generate alternative referring expressions when
a participant or an entity is mentioned multiple
times in sequence. For participants, this means
the generation of a personal pronoun. For entities,
rather than referring repeatedly to, e.g., the remote
control, we generate expressions such as that issue
or this matter.
5.3 Realization
The text realizer takes the output of the microplan-
ner and generates a textual summary of a meet-
ing. This is accomplished by first associating ele-
ments of the ontology with linguistic annotations.
For example, participants are associated with a
noun phrase denoting their role, such as the project
manager. Since entities were defined simply as
noun phrases with mid-frequency IDF scores, an
entity instance is associated with that noun phrase.
Messages themselves are associated with verbs,
1to be made publicly available upon publicaton
subject templates and object templates. For exam-
ple, instances of DecisionMessage are associated
with the verb make, have a subject template set to
the noun phrase of the message source, and have
an object template [NP a decision PP [concern-
ing ]] where the object of the prepositional
phrase is the noun phrase associated with the mes-
sage target.
To give a concrete example, consider the fol-
lowing decision message:
<DecisionMessage rdf:about="#dec9">
<rdf:type rdf:resource="&owl;Thing"/>
<hasVerb>make</hasVerb>
<hasCompl>a decision</hasCompl>
<messageSource rdf:resource="#MarketingExpert"/>
<messageSource rdf:resource="#ProjectManager"/>
<messageTarget rdf:resource="#television"/>
<containsUtterance rdf:resource="#ES2014a.D.dact.55"/>
<containsUtterance rdf:resource="#ES2014a.D.dact.63"/>
</DecisionMessage>
There are two message sources,
ProjectManager and MarketingExpert,
and one message target, television. The
subjects of the message are set to be the noun
phrases associated with the marketing expert and
the project manager, while the object template is
filled with the noun phrase the television. This
message is realized as The project manager and
the marketing expert made a decision about the
television.
For our realizer we use simpleNLG2. We tra-
verse the document plan output by the microplan-
ner and generate a sentence for each message leaf.
A new paragraph is created when both the message
type and target of the current message are different
than the message type and target for the previous
message.
6 Task-Based User Study
We carried out a formative user study in order to
inform this early work on automatic conversation
abstraction. This task required participants to re-
view meeting conversations within a short time-
frame, having a summary at their disposal. We
compared human abstracts and extracts with our
automatically generated abstracts. The interpre-
tation component and a preliminary version of
the transformation component have already been
tested in previous work (Murray et al, 2010). The
sentence-level classifiers were found to perform
well according to the area under the receiver op-
erator characteristic (AUROC) metric, which eva-
lutes the true-positive/false-positive ratio as the
2http://www.csd.abdn.ac.uk/?ereiter/simplenlg/
posterior threshold is varied, with scores ranging
from 0.76 for subjective sentences to 0.92 for ac-
tion item sentences. In the following, we focus
on the formative evaluation of the complete sys-
tem. We first describe the corpus we used, then
the materials, participants and procedure. Finally
we discuss the study results.
6.1 AMI Meeting Corpus
For our meeting summarization experiments, we
use the scenario portion of the AMI corpus (Car-
letta et al, 2005), where groups of four partici-
pants take part in a series of four meetings and
play roles within a fictitious company. There are
140 of these meetings in total. For the sum-
mary annotation, annotators wrote abstract sum-
maries of each meeting and extracted sentences
that best conveyed or supported the information
in the abstracts. The human-authored abstracts
each contain a general abstract summary and three
subsections for ?decisions,? ?actions? and ?prob-
lems? from the meeting. A many-to-many map-
ping between transcript sentences and sentences
from the human abstract was obtained for each an-
notator. Approximately 13% of the total transcript
sentences are ultimately labeled as extracted sen-
tences. A sentence is considered a decision item
if it is linked to the decision portion of the ab-
stract, and action and problem sentences are de-
rived similarly. We additionally use subjectivity
and polarity annotations for the AMI corpus (Wil-
son, 2008).
6.2 Materials, Participants and Procedures
We selected five AMI meetings for this user study,
with each stage of the four-stage AMI scenario
represented. The meetings average approximately
500 sentences each. We included the follow-
ing three types of summaries for each meeting:
(EH) gold-standard human extracts, (AH) gold-
standard human abstracts described in Section
6.1, and (AA) the automatic abstracts output by
our abstractor. All three conditions feature man-
ual transcriptions of the conversation. Each sum-
mary contains links to the sentences in the meet-
ing transcript. For extracts, this is a one-to-one
mapping. For the two abstract conditions, this can
be a many-to-many mapping between abstract sen-
tences and transcript sentences.
Participants were given instructions to browse
each meeting in order to understand the gist of
the meeting, taking no longer than 15 minutes per
meeting. They were asked to consider the sce-
nario in which they were a company employee
who wanted to quickly review a previous meet-
ing by using a browsing interface designed for this
task. Figure 1 shows the browsing interface for
meeting IS1001d with an automatically generated
abstract on the left-hand side and the transcript on
the right. In the screenshot, the user has clicked
the abstract sentence The industrial designer made
a decision on the cost and has been linked to a
transcript utterance, highlighted in yellow, which
reads Also for the cost, we should only put one bat-
tery in it. Notice that this output is not entirely cor-
rect, as the decision pertained to the battery, which
impacted the cost. This sentence was generated
because the entity cost appeared in several deci-
sion sentences.
The time constraint meant that it was not fea-
sible to simply read the entire transcript straight
through. Participants were free to adopt whatever
browsing strategy suited them, including skim-
ming the transcript and using the summary as they
saw fit. Upon finishing their review of each meet-
ing, participants were asked to rate their level of
agreement or disagreement on several Likert-style
statements relating to the difficulty of the task and
the usefulness of the summary. There were six
statements to be evaluated on a 1-5 scale, with
1 indicating strong disagreement and 5 indicating
strong agreement:
? Q1: I understood the overall content of the discussion.
? Q2: It required a lot of effort to review the meeting in
the allotted time.
? Q3: The summary was coherent and readable.
? Q4: The information in the summary was relevant.
? Q5: The summary was useful for navigating the dis-
cussion.
? Q6: The summary was missing relevant information.
Participants were also asked if there was any-
thing they would have liked to have seen in the
summary, and whether they had any general com-
ments on the summary.
We recruited 19 participants in total, with each
receiving financial reimbursement for their partic-
ipation. Each participant saw one summary per
meeting and rated every summary condition dur-
ing the experiment. We varied the order of the
meetings and summary conditions. With 19 sub-
jects, three summary conditions and six Likert
statements, we collected a total of 342 user judg-
ments. To ensure fair comparison between the
three summary types, we limit summary length to
Figure 1: Summary Interface
be equal to the length of the human abstract for
each meeting. This ranges from approximately
190 to 350 words per meeting summary.
6.2.1 Results and Discussion
Participants took approximately 12 minutes on av-
erage to review each meeting, slightly shorter than
the maximum allotted fifteen minutes.
Figure 2 shows the average ratings for each
summary condition on each Likert statement. For
Q1, which concerns general comprehension of
the meeting discussion, condition AH (human
abstracts) is rated significantly higher than EH
(human extracts) and AA (automatic abstracts)
(p=0.0016 and p=0.0119 according to t-test, re-
spectively). However, for the other statement that
addresses the overall task, Q2, AA is rated best
overall. Note that for Q2 a lower score is better.
While there are no significantly differences on this
criterion, it is a compelling finding that automatic
abstracts can greatly reduce the effort required for
reviewing the meeting, at a level comparable to
human abstracts.
Q3 concerns coherence and readability. Condi-
tion AH is significantly better than both EH and
AA (p<0.0001 and p=0.0321). Our condition AA
is also significantly better than the extractive con-
dition EH (p=0.0196). In the introduction we men-
tioned that a potential weakness of extractive sum-
maries is that coherence and readability decrease
when sentences are removed from their original
contexts, and that extracts of noisy, unstructured
source documents will tend to be noisy and un-
structured as well. These ratings confirm that ex-
tracts are not rated well on coherence and readabil-
ity.
Q4 concerns the perceived relevance of the
summary. Condition AH is again significantly bet-
ter than EH and AH (both p<0.0001). AA is rated
substantially higher than EH on summary rele-
vance, but not at a significant level.
Q5 is a key question because it directly ad-
dresses the issue of summary usability for such a
task. Condition AH is significantly better than EH
and AA (both p<0.0001), but we also find that AA
is significantly better than EH (p=0.0476). Ex-
tracts have an average score of only 2.37 out of
5, compared with 3.21 and 4.63 for automatic and
human abstracts, respectively. For quickly review-
ing a meeting conversation, abstracts are much
more useful than extracts.
Q6 indicates whether the summaries were miss-
ing any relevant information. As with Q2, a lower
score is better. Condition AH is significantly bet-
ter than EH and AA (p<0.0001 and p=0.0179),
while AA is better than EH with marginal signif-
icance (p=0.0778). This indicates that our auto-
matic abstracts were better at containing all the
relevant information than were human-selected
extracts.
All participants gave written answers to the
open-ended questions, yielding insights into the
strengths and weaknesses of the different sum-
mary types. Regarding the automatic abstracts
(AA), the most common criticisms were that the
 0
 1
 2
 3
 4
 5
Q1 - Understood Meeting
Q2 - Required Effort**
Q3 - Summary Coherent
Q4 - Summary Relevant
Q5 - Summary Useful
Q6 - Summary Missing Info**
Av
era
ge 
Us
er 
Ra
ting
s
Human AbstractsAuto AbstractsHuman Extracts
Figure 2: User Ratings (** indicates lower score
is better)
summaries are too vague (e.g. ?more concrete
would help?) and that the phrasing can be repet-
itive. There is a potential many-to-many map-
ping between abstract sentences and transcript
sentences, and some participants felt that it was
unnecessarily redundant to be linked to the same
transcript sentence more than once (e.g. ?quite a
few repetitive citations?). Several participants felt
that the sentences regarding positive-subjective
and negative-subjective opinions were overstated
and that the actual opinions were either more sub-
tle or neutral. One participant wrote that these sen-
tences constituted ?a lot of bias in the summary.?
On the positive side, several participants consid-
ered the links between abstract sentences and tran-
script sentences to be very helpful, e.g. ?it re-
ally linked to the transcript well? and ?I like how
the summary has links connected to the transcript.
Easier to follow-up on the meeting w/ the aid of
the summary.? One participant particularly liked
the subjectivity-oriented sentences: ?Lifting some
of the positive/negative from the discussion into
the summary can mean the discussion does not
even need to be included to get understanding.?
The written comments on the extractive condi-
tion (EH) were almost wholly negative. Many par-
ticipants felt that the extracts did not even con-
stitute a summary or that a cut-and-paste from
the transcript does not make a sufficient summary
(e.g. ?The summary was not helpful @ all be-
cause it?s just cut from the transcript?, ?All copy
and paste not a summary?, ?Not very clear sum-
mary - looked like the transcript?, and ?No ef-
fort was made in the summary to put things into
context?). Interestingly, several participants criti-
cized the extracts for not containing the most im-
portant sentences from the transcript despite these
being human-selected extracts, demonstrating that
a good summary is a subjective matter.
The comments on human abstracts (AH) were
generally very positive, e.g. ?easy to follow?, ?it
was good, clear?, and ?I could?ve just read the
summary and still understood the bulk of the meet-
ing?s content.? The most frequent negative criti-
cisms were that the abstract sentences sometimes
contained too many links to the transcript (?mas-
sive amount of links look daunting?), and that the
summaries were sometimes too vague (?perhaps
some points from the discussion can be included,
instead of just having topic outlines?, ?[want] spe-
cific details?). It is interesting to observe that this
latter criticism is shared between human abstracts
and our automatic abstracts. When generalizing
over the source document, details are sometimes
sacrificed.
7 Conclusion
We have presented a system for automatically gen-
erating abstracts of meeting conversations. This
summarizer relies on first mapping sentences to
a conversation ontology representing phenomena
such as decisions, action items and sentiment, then
identifying message patterns that abstract over
multiple sentences. We select the most informa-
tive messages through an ILP optimization ap-
proach, aggregate messages, and finally generate
text describing all of the selected messages. A
formative user study shows that, overall, our auto-
matic abstractive summaries rate very well in com-
parison with human extracts, particularly regard-
ing readability, coherence and usefulness. The
automatic abstracts are also significantly better in
terms of containing all of the relevant information
(Q6), and it is impressive that an automatic ab-
stractor substantially outperforms human-selected
content on such a metric. In future work we aim
to bridge the performance gap between automatic
and human abstracts by identifying more specific
messages and reducing redundancy in the sentence
mapping. We plan to improve the NLG output by
introducing more linguistic variety and better text
structuring. We are also investigating the impact
of ASR transcripts on abstracts and extracts, with
encouraging early results.
Acknowledgments Thanks to Nicholas Fitzgerald for
work on implementing the top-down planner.
References
R. Barzilay and K. McKeown. 2005. Sentence fusion
for multidocument news summarization. Computa-
tional Linguistics, 31(3):297?328.
G. Carenini and JCK Cheung. 2008. Extractive vs.
nlg-based abstractive summarization of evaluative
text: The effect of corpus controveriality. In Proc.
of the 5th International Natural Generation Confer-
ence.
J. Carletta, S. Ashby, S. Bourban, M. Flynn,
M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos,
W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln,
A. Lisowska, I. McCowan, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI meeting corpus: A pre-
announcement. In Proc. of MLMI 2005, Edinburgh,
UK, pages 28?39.
K. Church and W. Gale. 1995. Inverse document fre-
quency IDF: A measure of deviation from poisson.
In Proc. of the Third Workshop on Very Large Cor-
pora, pages 121?130.
J. Clarke and M. Lapata. 2006. Constraint-based
sentence compression: An integer programming ap-
proach. In Proc. of COLING/ACL 2006, pages 144?
151.
D. Gillick, K. Riedhammer, B. Favre, and D. Hakkani-
Tu?r. 2009. A global optimization framework for
meeting summarization. In Proc. of ICASSP 2009,
Taipei, Taiwan.
S. Gupta, J. Niekrasz, M. Purver, and D. Jurafsky.
2007. Resolving ?You? in multi-party dialog. In
Proc. of SIGdial 2007, Antwerp, Belgium.
L. He, E. Sanocki, A. Gupta, and J. Grudin. 1999.
Auto-summarization of audio-video presentations.
In Proc. of ACM MULTIMEDIA ?99, Orlando, FL,
USA, pages 489?498.
K. Spa?rck Jones. 1999. Automatic summarizing: Fac-
tors and directions. In I. Mani and M. Maybury,
editors, Advances in Automatic Text Summarization,
pages 1?12. MITP.
T. Kleinbauer, S. Becker, and T. Becker. 2007. Com-
bining multiple information layers for the automatic
generation of indicative meeting abstracts. In Proc.
of ENLG 2007, Dagstuhl, Germany.
K. Knight and D. Marcu. 2000. Statistics-based sum-
marization - step one: Sentence compression. In
Proc. of AAAI 2000, Austin, Texas, USA, pages 703?
710.
K. McKeown, J. Hirschberg, M. Galley, and S. Maskey.
2005. From text to speech summarization. In Proc.
of ICASSP 2005, Philadelphia, USA, pages 997?
1000.
C. Muller. 2007. Resolving It, This and That in un-
restricted multi-party dialog. In Proc. of ACL 2007,
Prague, Czech Republic.
G. Murray and G. Carenini. 2008. Summarizing spo-
ken and written conversations. In Proc. of EMNLP
2008, Honolulu, HI, USA.
G. Murray, T. Kleinbauer, P. Poller, S. Renals,
T. Becker, and J. Kilgour. 2009. Extrinsic sum-
marization evaluation: A decision audit task. ACM
Transactions on SLP, 6(2).
G. Murray, G. Carenini, and R. Ng. 2010. Interpre-
tation and transformation for abstracting conversa-
tions. In Proc. of NAACL 2010, Los Angeles, USA.
F. Portet, E. Reiter, A. Gatt, J. Hunter, S. Sripada,
Y. Freer, and C. Sykes. 2009. Automatic gener-
ation of textual summaries from neonatal intensive
care data. Artificial Intelligence, 173:789?816.
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University
Press, Cambridge, GB.
H. Saggion and G. Lapalme. 2002. Generat-
ing indicative-informative summaries with sumum.
Computational Linguistics, 28(4):497?526.
T. Wilson. 2008. Annotating subjective content in
meetings. In Proc. of LREC 2008, Marrakech, Mo-
rocco.
S. Xie, B. Favre, D. Hakkani-Tu?r, and Y. Liu. 2009.
Leveraging sentence weights in a concept-based op-
timization framework for extractive meeting sum-
marization. In Proc. of Interspeech 2009, Brighton,
England.
J. Yu, E. Reiter, J. Hunter, and C. Mellish. 2007.
Choosing the content of textual summaries of large
time-series data sets. Journal of Natural Language
Engineering, 13:25?49.
