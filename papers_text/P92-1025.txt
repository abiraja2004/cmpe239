MODEL ING NEGOTIAT ION SUBDIALOGUES 1 
Lynn Lamber t  and Sandra Carber ry  
Depar tment  of Computer  and Informat ion Sciences 
Univers i ty  of Delaware 
Newark,  Delaware 19716, USA 
email : lambert~cis, udel. edu, carberry@cis, udel. edu 
Abst ract  
This paper presents a plan-based model that han- 
dles negotiation subdialogues by inferring both the 
communicative actions that people pursue when 
speaking and the beliefs underlying these actions. 
We contend that recognizing the complex dis- 
course actions pursued in negotiation subdialogues 
(e.g., expressing doubt) requires both a multi- 
strength belief model and a process model that 
combines different knowledge sources in a unified 
framework. We show how our model identifies the 
structure of negotiation subdialogues, including 
recognizing expressions of doubt, implicit accep- 
tance of communicated propositions, and negotia- 
tion subdialogues mbedded within other negotia- 
tion subdialogues. 
1 In t roduct ion  
Since negotiation is an integral part of 
multi-agent activity, a robust natural anguage un- 
derstanding system must be able to handle subdi- 
alogues in which participants negotiate what has 
been claimed in order to try to come to some 
agreement about those claims. To handle such 
dialogues, the system must be able to recognize 
when a dialogue participant has initiated a nego- 
tiation subdialogue and why the participant began 
the negotiation (i.e., what beliefs led the partici- 
pant to start the negotiation). This paper presents 
a plan-based model of task-oriented interactions 
that assimilates negotiation subdialogues by in- 
ferring both the communicative actions that peo- 
ple pursue when speaking and the beliefs under- 
lying these actions. We will argue that recogniz- 
ing the complex discourse actions pursued in ne- 
gotiation subdialogues (e.g., expressing doubt) re- 
quires both a multi-strength belief model and a 
processing strategy that combines different knowl- 
edge sources in a unified framework, and we will 
show how our model incorporates these and rec- 
ognizes the structure of negotiation subdialogues. 
2 P rev ious  Work  
Several researchers have built argument un- 
derstanding systems, but none of these has ad- 
dressed participants coming to an agreement or 
mutual belief about a particular situation, ei- 
ther because the arguments were only monologues 
1 This work is being supported by the National Science 
Foundation under Grant No. IRI-9122026. The Govern- 
ment has certain rights in this material. 
(Cohen, 1987; Cohen and Young, 1991), or be- 
cause they assumed that dialogue participants do 
not change their minds (Flowers, McGuire and 
Birnbaum, 1982; Quilici, 1991). Others have ex- 
amined more cooperative dialogues. Clark and 
Schaefer (1989) contend that utterances must be 
grounded, or understood, by both parties, but they 
do not address conflicts in belief, only lack of un- 
derstanding. Walker (1991) has shown that evi- 
dence is often provided to ensure both understand- 
ing and believing an utterance, but she does not 
address recognizing lack of belief or lack of under- 
standing. Reichman (1981) outlines a model for 
informal debate, but does not provide a detailed 
computational mechanism for recognizing the role 
of each utterance in a debate. 
In previous work (Lambert and Carberry, 
1991), we described a tripartite plan-based model 
of dialogue that recognizes and differentiates three 
different kinds of actions: domain, problem- 
solving, and discourse. Domain actions relate to 
performing tasks in a given domain. We are mod- 
eling cooperative dialogues in which one agent 
has a domain goal and is working with another 
helpful, more expert agent to determine what do- 
main actions to perform in order to accomplish 
this goal. Many researchers (Allen, 1979; Car- 
berry, 1987; Goodman and Litman, 1992; Pol- 
lack, 1990; Sidner, 1985) have shown that recog- 
nition of domain plans and goals gives a system 
the ability to address many difficult problems in 
understanding. Problem-solving actions relate to 
how the two dialogue participants are going about 
building a plan to achieve the planning agent's 
domain goal. Ramshaw, Litman, and Wilensky 
(Ramshaw, 1991; Litman and Allen, 1987; Wilen- 
sky, 1981) have noted the need for recognizing 
problem-solving actions. Discourse actions are the 
communicative actions that people perform in say- 
ing something, e.g., asking a question or express- 
ing doubt. Recognition of discourse actions pro- 
vides expectations for subsequent utterances, and 
explains the purpose of an utterance and how it 
should be interpreted. 
Our system's knowledge about how to per- 
form actions is contained in a library of discourse, 
problem-solving, and domain recipes (Pollack, 
1990). Although domain recipes are not mutually 
known by the participants (Pollack, 1990), how to 
communicate and how to solve problems are corn- 
193 
Discourse Recipe-C3:{_agent1 informs _agent~ of_prop} 
Action: Inform(_agentl, _agent2, _prop) 
Recipe-type: Decomposition 
App Cond: believe(_agentl, _prop, \[C:C\]) 
believe(_agentl, believe(_agent2, _prop, \[CN:S\]), \[0:C\]) 
Body: Tell(_agent 1, _agent2, _prop) 
Address-Believability(_agent2, _agentl, _prop) 
Effects: believe(_agent2, want(_agentl, believe(_agent2, _prop, \[C:C\])), \[C:C\]) 
Goal: believe(_agent2, _prop, \[C:C\]) 
Discourse Recipe-C2: 
{_agent1 expresses doubt to _agent2 about _propI because _agent1 believes _prop~ to be true} 
Action: Express-Doubt(_agentl, _agent2, _propl, _prop2, _rule) 
Recipe-type: Decomposition 
App Cond: believe(_agentl, _prop2, \[W:S\]) 
believe(_agentl, believe(_agent2, _propl, \[S:C\]), \[S:C\]) 
believe(_agentl, ((_prop2 A _rule) ::~ -,_propl), \[S:C\]) 
believe(_agentl, _rule, \[S:C\]) 
in-focus(_propl)) 
Body: Convey- Uncertain- Belief(_ agent 1, _agent 2, _prop2) 
Address-Q-Acceptanee(_agent2, _agentl, _prop2) 
Effects: believe(_agent2, believe(_agentl, _propl, \[SN:W2~\]), \[S:C\]) 
believe(_agent2, want(_agentl, Resolve-Conflict(_agent2, _agentl, _propl, _prop2)), \[S:C\]) 
Goal: want(_agent2, Resolve-Conflict(_agent2, _agentl, _propl, _prop2)) 
Figure 1. Two Sample Discourse Recipes 
men skills that people use in a wide variety of 
contexts, so the system can assume that knowl- 
edge about discourse and problem-solving recipes 
is shared knowledge. Figure 1 contains two dis- 
course recipes. Our representation of a recipe in- 
cludes a header giving the name of the recipe and 
the action that it accomplishes, preconditions, ap- 
plicability conditions, constraints, a body, effects, 
and a goal. Constraints limit the allowable instan- 
tiation of variables in each of the components of 
a recipe (Litman and Allen, 1987). Applicability 
conditions (Carberry, 1987) represent conditions 
that must be satisfied in order for the recipe to 
be reasonable to apply in the given situation and, 
in the case of many of our discourse recipes, the 
applicability conditions capture beliefs that the di- 
alogue participants must hold. Especially in the 
case of discourse recipes, the goals and effects are 
likely to be different. This allows us to differen- 
tiate between ilIocutionary and perlocutionary ef- 
fects and to capture the notion that one can, for 
example, perform an inform act without the hearer 
adopting the communicated proposition. 2 
As actions are inferred by our process 
model, a structure of the discourse is built which is 
referred to as the Dialogue Model, or DM. In the 
DM, discourse, problem-solving, and domain ac- 
tions are each modeled on a separate level. Within 
each of these levels, actions may contribute to 
other actions in the dialogue, and this is captured 
with specialization (Kautz and Allen, 1986), sub- 
2Consider, for example, someone saying "I in.formed you 
of X but you wouldn't believe me." 
action, and enablement arcs. Thus, actions at each 
level form a tree structure in which each node rep- 
resents an action that a participant is performing 
and the children of a node represent actions pur- 
sued in order to contribute to the parent action. 
By using a tree structure to model actions at each 
level and by allowing the tree structures to grow at 
the root as well as at the leaves, we are able to in- 
crementally recognize discourse, problem-solving, 
and domain intentions, and can recognize the re- 
lationship among several utterances that are all 
part of the same higher-level discourse act even 
when that act cannot be recognized from the first 
utterance alone. Other advantages of our tripar- 
tite model are discussed in Lambert and Carberry. 
(1991). 
An action on one level in the DM may also 
contribute to an action on an immediately higher 
level. For example, discourse actions may be ex- 
ecuted in order to obtain the information eces- 
sary for performing a problem-solving action and 
problem-solving actions may be executed in order 
to construct a domain plan. We capture this with 
links between actions on adjacent levels of the DM. 
Figure 2 gives a DM built by our proto- 
type system whose implementation is currently be- 
ing expanded to include belief ascription and use 
of linguistic information. It shows that a ques- 
tion has been asked and answered, that this ques- 
tion/answer pair contributes to the higher-level 
discourse action of obtaining information about 
what course Dr. Smith is teaching, that this dis- 
course action enables the problem-solving action 
Of instantiating a parameter in a Learn-Material  
194 
Domain  Leve l  
? " * ' ? ?*??? ' ? ' ? ' ? ' ? ? "????***?? ' ? ;  -0-~.  = Enable Arc 
i I .Ta~.Co~:s,.  =o,,,,=) I ,.' 
~" . . . . . . . . . . . .  ~t . . . . . . . . . . . . . . .  ? ~ = Subact ion  Arc 
Prob lem-So lv ln_C l  Leve l  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
~ooooo*****o****~**********ooo**~*o******o  oo  * o*  
? | ? 
I Build-Plan(Sl, $2, Take-C0urse(S1, _course)) I | 
\[ ? i t IInstamiate-Vars(Sl, 2, Learn-Matertal(S1, _course, Dr. Smith)) \[ o 
? ' t ' : 
? T 0 ? e 
? I I 0 ,0 l* Instamiate-Single-Var(Sl, S2 _course, Learn-Material(S1, _course, Dr. Smith)) \] 
~ o .o to .oo********oo .o .oo .o .oo . . *~**oo***ooo*ooo*o~u=ooomo*moooo**oooooeoooo  -? 
Discourse Leve l  * 
! , \] Obtain-Info-Ref(Sl, S2, course, Teaches(Dr. S..__mith, _course)) I 
$2, Teaches(Dr. Smith, IAnswer-Ref(S2, SI-, course, Teaches(Dr. Smith, I course. I course), Teaches(Dr. Smith, Arch)) I 
I RexlUest(Sl, $2, Inf0rm-Rcf(S2, I 
Sl, _ course, Teaches(Dr. Smith, course)) I 
I t 
$ 
I Inform(S2, SI,. Teaches(Dr. Smith, Arch))\] 
? 
\[ * Tell(S2, SI, Teaches(Dr. Smith, Arch)) J 
? 
\[ \[ Surface-WH-Quesd0n(Sl, 2, Inform-Ref I 
\[ \[ ($2, SI, _course, Teaches(Dr. Smith, _course)) \[ Surface-lnf0rm(S20 SI, Teaches(Dr. Smith, Arch)) 
o ?oooo~ooooo  =ooooo  ~?oo~o~o~*o~****=* ? *** ? ? * * ? ? * **  ? o* ? ? ? ? * ? oooo*~ooo ? *o*o*ooo**oo**oo ? *** ?o*~moooo~ oo* ?  
! 
E \[ 
t 
i 
, i  
Figure 2. Dialogue Model for two utterances 
action, and that this problem-solving action con- 
tributes to the problem-solving action of building 
a plan ill order to perform the domain action of 
taking a course. 
The work described in this paper uses our 
tripartite model, but addresses the recognition of 
discourse actions and their use in the modeling of 
negotiation subdialogues. 
3 Discourse Actions and Implicit 
Acceptance 
One of the most important aspects of as- 
similating dialogue is the recognition of discourse 
actions and the role that an utterance plays with 
respect o the rest of the dialogue. For example, 
in (3), if S1 believes that each course has a sin- 
gle instructor, then S1 is expressing doubt at the 
proposition conveyed in (2). But in another con- 
text, (3) might simply be asking for verification. 
(1) SI: What is Dr. Smith teaching? 
(2) $2: Dr. Smith is teaching Architecture. 
(3) SI: Isu't Dr. Browa teaching Architecture? 
Unless a natural language system is able to iden- 
tify the role that an utterance is intended to play 
in a dialogue, the system will not be able to gener- 
ate cooperative responses which address the par- 
ticipants' goals. 
In addition to recognizing discourse ac- 
tions, it is also necessary for a cooperative sys- 
tem to recognize a user's changing beliefs as the 
dialogue progresses. Allen's representation of an 
Inform speech act (Allen, 1979) assumed that a 
listener adopted the communicated proposition. 
Clearly, listeners do not adopt everything they 
are told (e.g., (3) indicates that S1 does not im- 
mediately accept that Dr. Smith is teaching Ar- 
chitecture). Perrault's persistence model of belief 
(Perrault, 1990) assumed that a listener adopted 
the communicated proposition unless the listener 
had conflicting beliefs. Since Perrault's model as- 
sumes that people's beliefs persist, it cannot ac- 
count for S1 eventually accepting the proposition 
that Dr. Smith is teaching Architecture. We show 
in Section 6 how our model overcomes this limita- 
tion. 
Our investigation of naturally occurring di- 
alogues indicates that listeners are not passive par- 
ticipants, but instead assimilate each utterance 
into a dialogue in a multi-step acceptance phase. 
For statements, 3 a listener first attempts to un- 
derstand the utterance because if the utterance is 
not understood, then nothing else about it can be 
determined. Second, the listener determines if the 
utterance is consistent with the listener's beliefs; 
and finally, the listener determines the appropri- 
ateness of the utterance to the current context. 
Since we are assuming that people are engaged 
in a cooperative dialogue, a listener must indicate 
when the listener does not understand, believe, or 
consider elevant a particular utterance, address- 
ing understandability first, then believability, then 
relevance. We model this acceptance process by 
including acceptance actions in the body of many 
of our discourse recipes. For example, the actions 
the body of an Inform recipe (see Figure 1) are: 
il)n the speaker (_agentl) tells the listener (_agent2) 
3Questions must also be accepted and assimilated into 
a dialogue, but we axe concentrating on statements here. 
195 
the proposition that the speaker wants the listener 
to believe (_prop); and 2) the listener and speaker 
address believability by discussing whatever is nec- 
essary in order for the listener and speaker to come 
to an agreement about what the speaker said. 4 
This second action, and the subactions executed 
as part of performing it, account for subdialogues 
which address the believability of the proposition 
communicated in the Inform action. Similar ac- 
ceptance actions appear in other discourse recipes. 
The Tell action has a body containing a Surface- 
Inform action and an Address-Understanding ac-
tion; the latter enables both participants to ensure 
that the utterance has been understood. 
The combination of the inclusion of accep- 
tance actions in our discourse recipes and the or- 
dered manner in which people address acceptance 
allows our model to recognize the implicit accep- 
tance of discourse actions. For example, Figure 2 
presents the DM derived from utterances (1) and 
(2), with the current focus of attention on the dis- 
course level, the Tell action, marked with an aster- 
isk. In attempting to assimilate (3) into this DM, 
the system first tries to interpret (3) as address- 
ing the understanding of (2) (i.e., as part of the 
Tell action which is the current focus of attention 
in Figure 2). Since a satisfactory interpretation is 
not found, the system next tries to relate (3) to the 
Inform action in Figure 2, trying to interpret (3) 
as addressing the believability of (2). The system 
finds that the best interpretation of (3) is that of 
expressing doubt at (2), thus confirming the hy- 
pothesis that (3) is addressing the believability of 
(2). This recognition of (3) as contributing to the 
Inform action in Figure 2 indicates that S1 has 
implicitly indicated understanding by passing up 
the opportunity to address understanding in the 
Tell action that appears in the DM and instead 
moving to a relevant higher-level discourse action, 
thus conveying that the Tell action has been suc- 
cessful. 
4 Recogn iz ing  Be l ie fs  
In the dialogue in the preceding section, in 
order for $1 to use the proposition communicated 
in (3) to express doubt at the proposition conveyed 
in (2), $1 must believe 
(a) that Dr. Brown teaches Architecture; 
(b) that $2 believes that Dr. Smith is 
teaching Architecture; and 
(c) that Dr. Brown teaching Architecture is 
an indication that Dr. Smith does not 
teach Architecture. 
We capture these beliefs in the applicability condi- 
tions for an Express-Doubt discourse act (see Fig- 
ure 1). In order for the system to recognize (3) 
4This  is where our  model  differs f rom Al len's and  Per- 
rault 's ;  we allow the l istener to adopt ,  reject, or negoti-  
ate the speaker 's  c laims, which might  result  in the l istener 
eventua l ly  adopt ing  the speakers  claims, the l istener chang- 
ing the mind  of the speaker,  or both  agreeing to disagree. 
a~s an expression of doubt, it nmst come to be- 
lieve that these applicability conditions are satis- 
fied. The system's evidence that S1 believes (a) 
is provided by Sl's utterance, (3). But (3) does 
not state that Dr. Brown teaches Architecture; 
instead, Sl uses a negative yes-no question to ask 
whether or not Dr. Brown teaches Architecture. 
The surface form of this utterance indicates that 
S1 thinks that Dr. Brown teaches Architecture 
but is not sure of it. Thus, from the surface form 
of utterance (3), a listener can attribute to Sl an 
uncertain belief in the proposition that Dr. Brown 
teaches Architecture. 
This recognition of uncertain beliefs is an 
important part of recognizing complex discourse 
actions such as expressing doubt. If the system 
were limited to recognizing only lack of belief and 
belief, then yes-no questions would have to be in- 
terpreted as conveying lack of belief about the 
queried proposition, since a question in a cooper- 
ative consultation setting would not be felicitous 
if the speaker already knew the answer. Thus it 
would be impossible to attribute (a) to S1 from a 
question such as (3). And without this belief at- 
tribution, it would not be possible to recognize 
expressions of doubt. Furthermore, the system 
must be able to differentiate between expressions 
of doubt and objections; since we are assuming 
that people are engaged in a cooperative dialogue 
and communicate beliefs that they intend to be 
recognized, if S1 were certain of both (a) and (c), 
then S1 would object to (2), not simply express 
doubt at it. In summary, the surface form of ut- 
terances is one way that speakers convey belief. 
But these surface forms convey more than just be- 
lief and disbelief; they convey multiple strengths 
of belief, the recognition of which is necessary for 
identifying whether an agent holds the requisite 
beliefs for some discourse actions. 
We maintain a belief model for each partic- 
ipant which captures these multiple strengths of 
belief. We contend that at least three strengths 
of belief must be represented: certain belief (a be- 
lief strength of C); strong but uncertain belief, as 
in (3) above (a belief strength of S); and a weak 
belief, as in I think that Dr. C might be an edu- 
cation instructor (a belief strength of W). There- 
fore, our model maintains three degrees of belief, 
three degrees of disbelief (indicated by attaching 
a subscript of N, such as SN to represent strong 
disbelief and WN to represent weak disbelief), and 
one degree indicating no belief about a proposition 
(a belief strength of 0). 5 Our belief model uses 
belief intervals to specify the range of strengths 
5Others  (Walker, 1991; Gall iers, 1991) have also argued 
for mult ip le s t rengths  of belief, bas ing the s t rength  of belief 
on the amount  and kind of evidence avai lable for that  be- 
lief. We have not  invest igated how much evidence is needed 
for an  agent  to have a part icu lar  amount  of confidence in 
a belief; our  work has  concentrated on recogniz ing how the 
s t rength  of belief is communicated  in a discourse and the 
impact  that  the different belief s t rengths  have on the recog- 
nit ion of discourse acts.  
196 
within which an agent's beliefs are thought o fall, 
and our discourse recipes use belief intervals to 
specify the range of strengths that an agent's be- 
liefs may assume. Intervals uch as \[bi:bj\] spec- 
ify a strength of belief within bi and bj, inclu- 
sive. For example, the goal of the Inform recipe 
in Figure 1, (believe(..agent2, _prop, \[C:C\])), 
is that _agentl be certain that _prop is true; on the 
other hand, believe(_agentl, _prop, \[W:C\]), 
means that _agent I must have some belief in _prop. 
In order to recognize other beliefs, such as 
(b) and (c), it is necessary to use more informa- 
tion than just a speaker's utterances. For exam- 
ple, $2 might attribute (c) to $1 because $2 be- 
lieves that most people think that only one pro- 
fessor teaches each course. Our system incorpo- 
rates these commonly held beliefs by maintaining 
a model of a stereotypical user whose beliefs may 
be attributed to the user during the conversation 
as appropriate. People also communicate their be- 
liefs by their acceptance (explicit and implicit) and 
non-acceptance of other people's actions. Thus, 
explicit or implicit acceptance of discourse actions 
provides another mechanism for updating the be- 
lief model: when an action is recognized as suc- 
cessful, we update our model of the user's beliefs 
with the effects and goals of the completed ac- 
tion. For example, in determining whether (3) is 
expressing doubt at (2), thereby implicitly indi- 
cating that (2) has been understood and that the 
Tell action has therefore been successful, the sys- 
tem tentatively hypothesizes that the effects and 
goals of the Tell action hold, the goal being that 
$1 believes that $2 believes that Dr. Smith is 
teaching Architecture (belief (b) above). If the 
system determines that tiffs Express-Doubt infer- 
ence is the most coherent interpretation f (3), it 
attributes the hypothesized beliefs to S1. So, our 
model captures many of the ways in which people 
infer beliefs: 1) from the surface form of utter- 
ances; 2) from stereotype models; and 3) from ac- 
ceptance (explicit or implicit) or non-acceptance 
of previous actions. 
5 Combin ing  Knowledge  Sources  
Grosz and Sidner (1986) contend that mod- 
eling discourse requires integrating different kinds 
of knowledge in a unified framework in order to 
constrain the possible role that an utterance might 
be serving. We use three kinds of knowledge, 
1) contextual information provided by previous 
utterances; 2) world knowledge; and 3) the lin- 
guistic information contained in each utterance. 
Contextual knowledge in our model is captured by 
the DM and the current focus of attention within 
it. The system's world knowledge contains facts 
about the world, the system's beliefs (including 
its beliefs about a stereotypical user's beliefs), and 
knowledge about how to go about performing dis- 
course, problem-solving, and domain actions. The 
linguistic knowledge that we exploit includes the 
surface form of the utterance, which conveys be- 
liefs and the strength of belief, as discussed in the 
preceding section, and linguistic lue words. Cer- 
tain words often suggest what type of discourse 
action the speaker might be pursuing (Litman and 
Allen, 1987; Hinkelman, 1989). For example, the 
linguistic lue please suggests a request discourse 
act (Hinkelman, 1989) while the clue word but sug- 
gests a non-acceptance discourse act. Our model 
takes these linguistic clues into consideration i
identifying the discourse acts performed by an ut- 
terance. 
Our investigation ofnaturally occurring di- 
alogues indicates that listeners use a combination 
of information to determine what a speaker is try- 
ing to do in saying something. For example, S2's 
world knowledge of commonly held beliefs enabled 
$2 to determine that $1 probably believes (c), and 
therefore infer that $1 was expressing doubt at (2). 
However, $1 might have said (4) instead of (3). 
(4) But didn't Dr. Smith win a teaching award? 
It is not likely that $2 would think that people typ- 
ically believe that Dr. Smith winning a teaching 
award implies that she is not teaching Architec- 
ture. However, $2 would probably still recognize 
(4) as an expression of doubt because the linguis- 
tic clue but suggests that (4) may be some sort of 
non-acceptance action, there is nothing to suggest 
that S1 does not believe that Dr. Smith winning a 
teaching award implies that she is not teaching Ar- 
chitecture, and no other interpretation seems more 
coherent. Since linguistic knowledge is present, 
less evidence is needed from world knowledge to 
recognize the discourse actions being performed 
(Grosz and Sidner, 1986). 
In our model, if a new utterance contributes 
to a discourse action already in the DM, then there 
must be an inference path from the utterance that 
links the utterance up to the current ree structure 
on the discourse level. This inference path will 
contain an action that determines the relationship 
of the utterance to the DM by introducing new 
parameters for which there are many possible in- 
stantiations, but which must be instantiated based 
on values from the DM in order for the path to ter- 
minate with an action already in the DM. We will 
refer to such actions as e-actions ince we contend 
that there must be evidence to support he infer- 
ence of these actions. By substituting values from 
the DM that are not present in the semantic repre- 
sentation of the utterance for the new parameters 
in e-actions, we are hypothesizing a relationship 
between the new utterance and the existing dis- 
course level of the DM. 
Express-Doubt is an example of an e-action 
(Figure 1). From the speaker's conveying uncer- 
tain belief in the proposition _prop2, plan chain- 
ing suggests that the speaker might be expressing 
doubt at some proposition _propl, and from this 
Express-Doubt action, further plan chaining may 
suggest a sequence of actions terminating at an 
Inform action already in the DM. The ability of 
_propl to unify with the proposition that was con- 
veyed by the Inform action (and _rule to unify 
197 
with a rule in the system's world knowledge) is 
not sufficient o justify inferring that the current 
utterance contributes to an Express-Doubt action 
which contributes to an Inform action; more evi- 
dence is needed. This is further discussed in Lam- 
bert and Carberry (1992). 
Thus we need evidence for including e- 
actions on an inference path. The required evi- 
dence for e-actions may be provided by linguistic 
knowledge that suggests certain discourse actions 
(e.g., the evidence that (4) is expressing doubt) 
or may be provided by world knowledge that in- 
dicates that the applicability conditions for a par- 
ticular action hold (e.g., the evidence that (3) is 
expressing doubt). 
Our model combines these different knowl- 
edge sources in our plan recognition algorithm. 
From the semantic representation f an utterance, 
higher level actions are inferred using plan infer- 
ence rules (Allen, 1979). If the applicability condi- 
tions for an inferred action are not plausible, this 
action is rejected. If the applicability conditions 
are plausible, then the beliefs contained in them 
are temporarily ascribed to the user (if an infer- 
ence line containing this action is later adopted as 
the correct interpretation, these applicability con- 
ditions are added to the belief model of the user). 
The focus of attention and focusing heuristics (dis- 
cussed in Lambert and Carberry (1991)) order 
these sequences of inferred actions, or inference 
lines, in terms of coherence. For those inference 
lines with an e-action, linguistic clues are checked 
to determine if the action is suggested by linguistic 
knowledge, and world knowledge is checked to de- 
termine if there is evidence that the applicability 
conditions for the e-action hold. If there is world 
and linguistic evidence for the e-action of one or 
more inference lines, the inference line that is clos- 
est to the focus of attention (i.e., the most contex- 
tually coherent) is chosen. Otherwise, if there is 
world or linguistic evidence for the e-action of one 
or more inference lines, again the inference line 
that is closest to the focus of attention is chosen. 
Otherwise, there is no evidence for the e-action in 
any inference line, so the inference line that is clos- 
est to the current focus of attention and contains 
no e-action is chosen. 
6 Example  
The following example, an expansion of ut- 
terances (1), (2), and (3) from Section 3, illustrates 
how our model handles 1) implicit and explicit ac- 
ceptance; 2) negotiation subdialogues embedded 
within other negotiation subdialogues; 3) expres- 
sions of doubt at both immediately preceding and 
earlier utterances; and 4) multiple expressions of 
doubt at the same proposition. We will concen- 
trate on how Sl 's utterances are understood and 
assimilated into the DM. 
(5) $1: What is Dr. Smith teaching? 
(6) S2: Dr. Smith is teaching Architecture. 
(7) SI: Isn't Dr. Brown teaching Architecture? 
(8) $2: No. 
(9) Dr. Brown is on sabbatical. 
(10) SI: But didn't 1see him on campus 
yesterday? 
(11) $2: Yes. 
(12) He was giving a University colloquium. 
(13) SI: OK. 
(14) But isn't Dr. Smith a theory person? 
The inferencing for utterances similar to (5) 
and (6) is discussed in depth in Lambert and Car- 
berry (1992), and the resultant DM is given in 
Figure 2. No clarification or justification of the 
Request action or of the content of the question has 
been addressed by either S1 or $2, and $2 has pro- 
vided a relevant answer, so both parties have im- 
plicitly indicated (Clark and Schaefer, 1989) that 
they think that S1 has made a reasonable and un- 
derstandable r quest in asking the question in (5). 
The surface form of (7) suggests that S1 
thinks that Dr. Brown is teaching Architecture, 
but isn't certain of it. This belief is entered 
into the system's model of Sl 's beliefs. This sur- 
face question is one way to Convey-Uncertain- 
Belief. As discussed in Section 3, the most coher- 
ent interpretation of (7) based on focusing heuris- 
tics, addressing the understandability of (6), is 
rejected (because there is not evidence to sup- 
port this inference), so the system tries to relate 
(7) to the Inform action in (6); that is, the sys- 
tem tries to interpret (7) as addressing the believ- 
ability of (6). Plan chaining determines that the 
Convey-Uncertain-Belief action could be part of 
an Express-Doubt action which could be part of 
an Address-Unacceptance action which could be 
an action in an Address-Believability discourse ac- 
tion which could in turn be an action in the In- 
form action of (6). Express-Doubt is an e-action 
because the action header introduces new argu- 
ments that have not appeared previously on the 
inference path (see Figure 1). Since there is evi- 
dence from world knowledge that the applicability 
conditions hold for interpreting (7) as an expres- 
sion of doubt and since there is no other evidence 
for any other e-action, the system infers that this 
is the correct interpretation and stops. Thus, (7) 
is interpreted as an Express-Doubt action. S2's re- 
sponse in (8) and (9) indicates that $2 is trying to 
resolve $1 and S2's conflicting beliefs. The struc- 
ture that the DM has built after these utterances 
is contained in Figure 3, 6 above the numbers (5) - 
(9). 
The Surface-Neg-YN-Question in utterance 
(10) is one way to Convey-Uneerlain-Belief. The 
linguistic clue but suggests that S1 is execut- 
6 For space reasons, only inferencing of discourse actions 
will be discussed here, and only action names on the dis- 
course level are shown; the problem-solvlng and domain 
levels are as shown in Figure 2. 
198 
(5) (6) Resolve-Conflict 
Surface-Neg YN-Question \]
(7) 
(9) 
Figure 3. Discourse Level of DM 
|Address-UnacCeptance I 
\[Express-Doubt I 
\[YN-Question J 
(14) 
i 
I 
I 
t 
'eft/on 
Ibgue 
r 
(10) (11) (12) t" 
for Dialogue in Section 6 
ing a non-acceptance discourse action; this non- 
acceptance action might be addressing either (9) 
or (6). Focusing heuristics uggest hat the most 
likely candidate is the Inform act attempted in 
(9), and plan chaining suggests that the Convey- 
Uncertain-Belief could be part of an Express- 
Doubt action which in turn could be part of an 
Address-Unacceptance action which could be part 
of an Address-Believability action which could be 
part of the Inform action in (9). Again, there is 
evidence that the applicability conditions for the 
e-action (tile Express-Doubt action) hold: world 
knowledge indicates that a typical user believes 
that professors who are on sabbatical are not on 
campus. Thus, there is both linguistic and world 
knowledge giving evidence for the Express-Doubt 
action (and no other e-action has both linguistic 
and world knowledge vidence), so (10) is inter- 
preted as expressing doubt at (9). 
In (11) and (12), $2 clears up the confu- 
sion that S1 has expressed in (10), by telling S1 
that the rule that people on sabbatical are not 
on campus does not hold in this case. In (13), 
S1 indicates explicit acceptance of the previously 
communicated proposition, so the system is able 
to determine that S1 has accepted S2's response in 
12). This additional negotiation, utterances (10)- 
13), illustrates our model's handling of negotia- 
tion subdialogues embedded within other negoti- 
ation subdialogues. The subtree contained within 
the dashed lines in Figure 3 shows the structure 
of this embedded negotiation subdialogue. 
The linguistic clue but in (14) then again 
suggests non-acceptance. Since (12) has been ex- 
plicitly accepted, (14) could be expressing non- 
acceptance of the information conveyed in either 
(9) or (6). Focusing heuristics uggest hat (14) 
is most likely expressing doubt at (9). World 
knowledge, however, provides no evidence that the 
applicability conditions hold for (14) expressing 
doubt at (9). Thus, there is evidence from lin- 
guistic knowledge for this inference, but not from 
world knowledge. The system's tereotype model 
does indicate, however, that it is typically believed 
that faculty only teach courses in their field and 
that Architecture and Theory are different fields. 
So in this case, the system's world knowledge pro- 
vides evidence that Dr. Smith being a theory 
person is an indication that Dr. Smith does not 
teach Architecture. Therefore, the system inter- 
prets (14) as again expressing doubt at (6) because 
there is evidence for this inference from both world 
and linguistic knowledge. The system infers there- 
fore that S1 has implicitly accepted the statement 
in (9), that Dr. Smith is on sabbatical. Thus, the 
system is able to recognize and assimilate a second 
expression of doubt at the proposition conveyed in 
6). The DM for the discourse level of the entire 
ialogue is given in Figure 3. 
199 
7 Conclusion 
We have presented a plan-based model that 
handles cooperative negotiation subdialogues by 
inferring both the communicative actions that 
people pursue when speaking and the beliefs un- 
derlying these actions. Beliefs, and the strength of 
those beliefs, are recognized from the surface form 
of utterances and from the explicit and implicit ac- 
ceptance of previous utterances. Our model com- 
bines linguistic, contextual, and world knowledge 
in a unified framework that enables recognition 
not only of when an agent is negotiating a con- 
flict between the agent's beliefs and the preceding 
dialogue but also which part of the dialogue the 
agent's beliefs conflict with. Since negotiation is
an integral part of multi-agent activity, our model 
addresses an important aspect of cooperative in- 
teraction and communication. 
Re ferences  
Allen, James F. (1979). A Plan-Based Approach 
to Speech Act Recognition. PhD thesis, Uni- 
versity of Toronto, Toronto, Ontario, Canada. 
Carberry, Sandra (1987). Pragmatic Modeling: 
Toward a Robust Natural Language Interface. 
Computational Intelligence, 3, 117-136. 
Clark, tlerbert and Schaefer, Edward (1989). Con- 
tributing to Discourse. Cognitive Science, 
259-294. 
Cohen, Robin (1987). Analyzing the Structure 
of Argumentative Discourse. Computational 
Linguistics, 13(1-2), 11-24. 
Cohen, Robin and Young, Mark A. (1991). Deter- 
mining Intended Evidence Relations in Natu- 
ral Language Arguments. Computational In- 
telligence, 7, 110-118. 
Flowers, Margot, McGuire, Rod, and Birnbaum, 
Lawrence (1982). Adversary Arguments and 
the Logic of Personal Attack. In W. Lehn- 
eft and M. Ringle (Eds.), Strategies for Natu- 
ral Language Processing (pp. 275-294). Hills- 
dage, New Jersey: Lawrence Erlbaum Assoc. 
Galliers, Julia R. (1991). Belief Revision and a 
Theory of Communication. Technical Report 
193, University of Cambridge, Cambridge, 
England. 
Goodman, Bradley A. and Litman, Diane J. 
(1992). On the Interaction between Plan 
Recognition and Intelligent Interfaces. User 
Modeling and User-Adapted Interaction, 2, 
83-115. 
Grosz, Barbara and Sidner, Candace (1986). At- 
tention, Intention, and the Structure of Dis- 
course. Computational Linguistics, le(3), 
175-204. 
Hinkelman, Elizabeth (1989). Two Constraints on 
Speech Act Ambiguity. In Proceedings of the 
27th Annual Meeting of the ACL (pp. 212- 
219), Vancouver, Canada. 
Kautz, Henry and Allen, James (1986). General- 
ized Plan Recognition. In Proceedings of the 
Fifth National Conference on Artificial Intel- 
li.gence (pp. 32-37), Philadelphia, Pennsylva- 
nia. 
Lambert, Lynn and Carberry, Sandra (1991). A 
Tripartite Plan-based Model of Dialogue. In 
Proceedings of the 29th Annual Meeting of the 
ACL (pp. 47-54), Berkeley, CA. 
Lambert, Lynn and Carberry, Sandra (1992). Us- 
ing Linguistic, World, and Contextual Knowl- 
edge in a Plan Recognition Model of Dia- 
logue. In Proceedings of COLING-92, Nantes, 
France. To appear. 
Litman, Diane and Allen, James (1987). A Plan 
Recognition Model for Subdialogues in Con- 
versation. Cognitive Science, 11, 163-200. 
Perrault, Raymond (1990). An Application of De- 
fault Logic to Speech Act Theory. In P. Co- 
hen, J. Morgan, and M. Pollack (Eds.), Inten- 
tions in Communication (pp. 161-185). Cam- 
bridge, Massachusetts: MIT Press. 
Pollack, Martha (1990). Plans as Complex Men- 
tal Attitudes. In P. R. Cohen, J. Morgan, and 
M. E. Pollack (Eds.), Intentions in Commu- 
nication (pp 77-104). MIT Press. 
Quilici, Alexander (1991). The Correction Ma- 
chine: A Computer Model of Recognizing and 
Producing Belief Justifications in Argumenta- 
tive Dialogs. PhD thesis, Department ofCom- 
puter Science, University of California t Los 
Angeles, Los Angeles, California. 
Ramshaw, Lance A. (1991). A Three-Level Model 
for Plan Exploration. In Proceedings of the 
29th Annual Meeting of the ACL (pp. 36-46), 
Berkeley, California. 
Reichman, Rachel (1981). Modeling Informal De- 
bates. In Proceedings of the 1981 Interna- 
tional Joint Conference on Artificial Intelli- 
gence (pp. 19-24), Vancouver, B.C. IJCAI. 
Sidner, Candace L. (1985). Plan Parsing for In- 
tended Response Recognition in Discourse. 
Computational Intelligence, 1, 1-10. 
Walker, Marilyn (1991). Redundancy in Collabo- 
rative Dialogue. Presented at The AAAI Fall 
Symposium: Discourse Structure in Natural 
Language Understanding and Generation (pp. 
124-129), Asilomar, CA. 
Wilensky, Robert (1981). Meta-Planning: Rep- 
resenting and Using Knowledge About Plan- 
ning in Problem Solving and Natural Lan- 
guage Understanding. Cognitive Science, 5, 
197-233. 
200 
