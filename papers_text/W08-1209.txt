Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 58?65
Manchester, August 2008
An Agreement Measure for Determining Inter-Annotator Reliability of
Human Judgements on Affective Text
Plaban Kr. Bhowmick, Pabitra Mitra, Anupam Basu
Department of Computer Science and Engineering,
Indian Institute of Technology, Kharagpur, India ? 721302
{plaban,pabitra,anupam}@cse.iitkgp.ernet.in
Abstract
An affective text may be judged to be-
long to multiple affect categories as it may
evoke different affects with varying degree
of intensity. For affect classification of
text, it is often required to annotate text
corpus with affect categories. This task
is often performed by a number of hu-
man judges. This paper presents a new
agreement measure inspired by Kappa co-
efficient to compute inter-annotator relia-
bility when the annotators have freedom
to categorize a text into more than one
class. The extended reliability coefficient
has been applied to measure the quality of
an affective text corpus. An analysis of
the factors that influence corpus quality has
been provided.
1 Introduction
The accuracy of a supervised machine learning
task primarily depends on the annotation quality of
the data, that is used for training and cross valida-
tion. Reliability of annotation is a key requirement
for the usability of an annotated corpus. Inconsis-
tency or noisy annotation may lead to the degrada-
tion of performances of supervised learning algo-
rithms. The data annotated by a single annotator
may be prone to error and hence an unreliable one.
This also holds for annotating an affective corpus,
which is highly dependent on the mental state of
the subject. The recent trend in corpus develop-
ment in NLP is to annotate corpus by more than
one annotators independently. In corpus statistics,
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
the corpus reliability is measured by coefficient of
agreement. The coefficients of agreement are ap-
plied to corpus for various goals like measuring re-
liability, validity and stability of corpus (Artstein
and Poesio, 2008).
Jacob Cohen (Cohen, 1960) introduced Kappa
statistics as a coefficient of agreement for nom-
inal scales. The Kappa coefficient measures the
proportion of observed agreement over the agree-
ment by chance and the maximum agreement at-
tainable over chance agreement considering pair-
wise agreement. Later Fleiss (Fleiss, 1981) pro-
posed an extension to measure agreement in ordi-
nal scale data.
Cohen?s Kappa has been widely used in vari-
ous research areas. Because of its simplicity and
robustness, it has become a popular approach for
agreement measurement in the area of electron-
ics (Jung, 2003), geographical informatics (Hagen,
2003), medical (Hripcsak and Heitjan, 2002), and
many more domains.
There are other variants of Kappa like agree-
ment measures (Carletta, 1996). Scott?s pi (Scott,
1955) was introduced to measure agreement in sur-
vey research. Kappa and pi measures differ in the
way they determine the chance related agreements.
pi-like coefficients determine the chance agreement
among arbitrary coders, while ?-like coefficients
treats the chance of agreement among the coders
who produced the reliability data (Artstein and
Poesio, 2008).
One of the drawbacks of pi and Kappa like coef-
ficients except Fleiss? Kappa (Fleiss, 1981) is that
they treat all kinds of disagreements in the same
manner. Krippendorff?s ? (Krippendorff, 1980) is
a reliability measure which treats different kind of
disagreements separately by introducing a notion
of distance between two categories. It offers a way
58
to measure agreement in nominal, interval, ordinal
and ratio scale data.
Reliability assessment of corpus is an impor-
tant issue in corpus driven natural language pro-
cessing and the existing reliability measures have
been used in various corpus development tasks.
For example, Kappa coefficient has been used
in developing parts of speech corpus (Mieskes
and Strube, 2006), dialogue act tagging efforts
like MapTask (Carletta et al, 1997) and Switch-
board (Stolke et al, 1997), subjectivity tagging
task (Bruce and Wiebe, 1999) and many more.
The pi and ? coefficients measure the reliabil-
ity of the annotation task where a data item can
be annotated with one category. (Rosenberg and
Binkowski, 2004) puts an effort towards measur-
ing corpus reliability for multiply labeled data
points. In this measure, the annotators are allowed
to mark one data point with at most two classes,
one of which is primary and other is secondary.
This measure was used to determine the reliability
of a email corpus where emails are assigned with
primary and secondary labels from a set of email
types.
Affect recognition from text is a recent and
promising subarea of natural language process-
ing. The task is to classify text segments into ap-
propriate affect categories. The supervised ma-
chine learning techniques, which requires a reli-
able annotated corpus, may be applied for solv-
ing the problem. In general, a blend of emotions
is common in both verbal and non-verbal com-
munication. Unlike conventional annotation tasks
like POS corpus development, where one data item
may belong to only one category, in affective text
corpus, a data item may be fuzzy and may belong
to multiple affect categories. For example, the fol-
lowing sentence may belong to disgust and sad
category since it may evoke both the emotions to
different degrees of intensity.
A young married woman was burnt to
death allegedly by her in-laws for dowry.
This property makes the existing agreement mea-
sures inapplicable for determining agreement in
emotional corpus. Craggs and Wood (2004)
adopted a categorical scheme for annotating emo-
tion in affective text dialogue. They claimed to ad-
dress the problem of agreement measurement for
the data set where one data item may belong to
more than one category using an extension of Krip-
pendorff?s ?. But the details of the extension is yet
to be disseminated.
In this paper, we propose a new agreement mea-
sure for multiclass annotation which we denote by
A
m
. The new measure is then applied to an affec-
tive text corpus to
? Assess Reliability: To test whether the corpus
can be used for developing computational af-
fect recognizer.
? Determine Gold Standard: To define a gold
standard that will be used to test the accuracy
of the affect recognizer.
In section 2, we describe the affective text cor-
pus and the annotation scheme. In section 3, we
propose a new reliability measure (A
m
) for mul-
ticlass annotated data. In section 4, we provide
an algorithm to determine gold standard data from
the annotation and in section 5, we discuss about
applying A
m
measure to the corpus developed by
us and some observations related to the annotation.
2 Affective Text Corpus and Annotation
Scheme
The affective text corpus collected by us consists
of 1000 sentences extracted from Times of India
news archive1. The sentences were collected from
headlines as well as articles belonging to political,
social, sports and entertainment domain.
Selection of affect categories is a very crucial
and important decision problem due to the follow-
ing reasons.
? The affect categories should be applicable to
the considered genre.
? The affect categories should be identifiable
from language.
? The categories should be unambiguous.
We shall try to validate these points based on the
results obtained, after applying the our extended
measure on the text corpus with respect to a set of
selected basic emotional categories.
Basic emotions are those for which the respec-
tive expressions across culture, ethnicity, age, sex,
social structure are invariant (Ortony and Turner,
1990). But unfortunately, there is a long per-
sistent debate among the psychologists regarding
1http://timesofindia.indiatimes.com/archive.cms
59
the number of basic emotional categories (Ortony
and Turner, 1990). One of the theories behind
the basic emotions is that they are biologically
primitive because they possess evolutionary signif-
icance related to the basic needs for the survival of
the species (Plutchik, 1980). The universality of
recognition of emotions from distinctive facial ex-
pressions is an indirect technique to establish the
basic emotions (Darwin, 1965).
Six basic affect categories (Ekman, Friesen and
Ellsworth, 1982) have been considered in emotion
recognition from speech (Song et al, 2004), fa-
cial expression (Pantic and Rothkrantz, 2000). Our
annotation scheme considers six basic emotions,
namely, Anger, Disgust, Fear, Happiness, Sadness,
Surprise as specified by Ekman for affect recogni-
tion in text corpus.
The annotation scheme considers the following
points:
? Two types are sentences are collected for an-
notation.
? Direct Affective Sentence: Here, the
agent present in the sentence is experi-
encing a set of emotions, which are ex-
plicit in the sentence. For example, in
the following sentence Indian support-
ers are the agents experiencing a disgust
emotion.
Indian supporters are disgusted
about players? performances in
the World Cup.
? Indirect Affective Sentence: Here, the
reader of the sentence is experiencing a
set of emotions. In the following sen-
tence, the reader is experiencing a dis-
gust emotion because the event of ac-
cepting bribe, is an indecent act carried
out by responsible agents like Top offi-
cials.
Top officials are held for accept-
ing bribe from a poor villager.
? A sentence may trigger multiple emotions si-
multaneously. So, one annotator may classify
a sentence to more than one affective cate-
gories.
? For each emotion, the keywords that trigger
the particular emotion are marked.
? For each emotion, the events or objects that
trigger the concerned emotion are marked.
Here, we aim at measuring the agreement in an-
notation. The focus is to measure the agreement
in annotation pattern rather than the agreement in
individual emotional classes.
3 Proposed Agreement Measure
To overcome the shortcomings of existing relia-
bility measures mentioned earlier, we propose A
m
measure, which is an agreement measure for cor-
pus annotation task considering multiclass classifi-
cation. We present the notion of agreement below.
3.1 Notion of Paired Agreement
In order to allow for multiple labels, we calculate
agreement between all the pairs of possible labels.
Let C1 and C2 be two affect categories, e.g., anger
and disgust. Let <C1, C2> denote the category
pair. An annotator?s assignment of labels can be
represented as a pair of binary choices for each cat-
egory pair <C1, C2>, namely, < 0, 0 >, < 0, 1 >,
< 1, 0 >, and < 1, 1 >. It should be noted that the
proposed metric considers the non-inclusion in a
category by an annotator pair as an agreement.
For an item, two annotators U1 and U2 are said
to agree on <C1, C2> if the following conditions
hold.
U1.C1 = U2.C1
U1.C2 = U2.C2
where U
i
.C
j
signifies that the value for C
j
for an-
notator U
i
and the value may either be 1 or 0. For
example, if one coder marks an item with anger
and another with disgust, they would disagree on
the pairs that include these labels, but still agree
that the item does not express happiness and sad-
ness.
3.2 A
m
Agreement Measure
With the notion of paired agreement discussed ear-
lier, the observed agreement(P
o
) is the proportion
of items the annotators agreed on the category
pairs and the expected agreement(P
e
) is the pro-
portion of items for which agreement is expected
by chance when the items are randomly. Follow-
ing the line of Cohen?s Kappa (Cohen, 1960), A
m
is defined as the proportion of agreement after ex-
pected or chance agreement is removed from con-
sideration and is given by
A
m
=
P
o
? P
e
1? P
e
(1)
60
When P
o
equals P
e
, A
m
value is computed to
be 0, which signifies no non-random agreement
among the annotators. An A
m
value of 1, the
upper limit of A
m
, indicates a perfect agreement
among the annotators. We define P
o
and P
e
as
follows.
Observed Agreement (P
o
):
Let I be the number of items, C is the number of
categories and U is the number of annotators and
S be the set of all category pairs with cardinality
(
C
2
)
. The total agreement on a category pair p
for an item i is n
ip
, the number of annotator pairs
who agree on p for i.
The average agreement on a category pair p for
an item i is n
ip
divided by the total number of an-
notator pairs and is given by
P
ip
=
1
(
U
2
)
n
ip
(2)
The average agreement for the item i is the mean
of P
ip
over all category pairs and is given by
P
i
=
1
(
C
2
)(
U
2
)
?
p?S
n
ip
(3)
The observed agreement is the average agreement
over all the item and is given by
P
o
=
1
I
I
?
i=1
P
i
=
1
I
(
C
2
)(
U
2
)
I
?
i=1
?
p?S
n
ip
(4)
=
4
IC(C ? 1)U(U ? 1)
I
?
i=1
?
p?S
n
ip
Expected Agreement (P
e
):
The expected agreement is defined as the agree-
ment among the annotators when they assign the
items to a set of categories randomly. However,
since we are considering the agreement on cate-
gory pairs, we consider the expected agreement
to be the expectation that the annotators agree on
a category pair. For a category pair, four possible
assignment combinations constitute a set which is
given by
G = {[0 0], [0 1], [1 1]}.
It is to be noted that the combinations [0 1] and [1
0] are clubbed to one element as they are symmet-
ric to each other. Let ?P (p
g
|u) be the overall pro-
portion of items assigned with assignment combi-
nation g ? G to category pair p ? S by annotator
u and n
p
g
u
be the total number of assignments of
items by annotator u with assignment combination
g to category pair p. Then ?P (p
g
|u) is given by
?
P (p
g
|u) =
n
p
g
u
I
(5)
For an item, the probability that two arbitrary
coders agree with the same assignment combina-
tion in a category pair is the joint probability of
individual coders making this assignments inde-
pendently. For two annotators u
x
and u
y
the joint
probability is given by ?P (p
g
|u
x
)
?
P (pg|u
y
). The
probability that two arbitrary annotators agree on
a category pair p with assignment combination g
is the average over all annotator pairs belonging to
W , the set of annotator pairs and is given by
?
P (p
g
) =
1
(
U
2
)
?
(u
x
,u
y
)?W
?
P (p
g
|u
x
)
?
P (p
g
|u
y
)
(6)
The probability that two arbitrary annotators agree
on a category pair for all assignment combinations
is given by
?
P (p) =
?
pg?G
?
P (p
g
) (7)
The chance agreement is calculated by taking
average over all category pairs.
P
e
=
1
(
C
2
)
?
p?S
?
P (p) (8)
The A
m
measure may be calculated based on the
expressions of P
o
and P
e
as given in Equation 4
and Equation 8 to compute the reliability of anno-
tation with respect to multiclass annotation.
4 Gold Standard Determination
Gold standard data is used as a reference data set
for various goals like
? Building reliable classifier
61
? Determine the performance of a classifier
To attach a set of labels to a data item in the gold
standard data, we assign the majority decision
label to an item. Let n
O
be the number of annota-
tors, who have assigned an item i into category C
and n
?
annotators have decided not to assign the
same item into that category. Then i is assigned to
C if n
O
> n
?
; otherwise it is not assigned to that
category.
Algorithm 1: Algorithm for determining gold
standard data
Input: Set of I items annotated into C
categories by U annotators
Output: Gold standard data
foreach annotator u ? U do
?
u
? 0;
end
foreach item i ? I do
foreach category c ? C do
? = set of annotators who have
assigned i in category c;
? = set of annotators who have not
assigned i in category c;
if cardinality(?)>cardinality(?) then
assign label c to i;
?
j
? ?
j
+ 1 where j ? ?;
end
else if cardinality(?)<cardinality(?)
then
do not assign label c to i;
?
j
? ?
j
+ 1 where j ? ?;
end
else if
?
?
? >
?
?
? then
assign label c to i;
end
end
end
If n
O
= n
?
, then we resolve the tie based on the
performances of the annotators in previous assign-
ments. We assign an expert coder index(?) to each
annotator and it is updated based on the agreement
of their judgments over the corpus. There are two
cases when the ? values are incremented
? If the item is assigned to a category in the gold
standard data, the ? values are incremented
for those annotators who have assigned the
item into that category.
? If the item is not assigned to a category in
the gold standard data, the ? values are in-
cremented for those annotators who have not
assigned the item into that category.
If n
O
and n
?
are equal for an item, we make use
of the ? values for deciding upon the assignment of
the item to the category in concern. We assign the
item into that category if the combined ? values of
the annotators who have assigned the item into that
category is greater than the combined ? values of
the annotators who have not assigned the item into
that category, i.e.,
n
O
?
i=1
?
i
>
n
?
?
j=1
?
j
The algorithm for determining gold standard
data is given in Algorithm 1.
5 Experimental Results
We applied the proposed A
m
measure to estimate
the quality of the affective corpus described in sec-
tion 2. Below we present the annotation experi-
ment followed by some relevant analysis.
5.1 Annotation Experiment
Ten human judges with the same social back-
ground participated in the study, assigning affec-
tive categories to sentences independently of one
another. The annotators were provided with the
annotation instructions and they were trained with
some sentences not belonging to the corpus. The
annotation was performed with the help of a web
based annotation interface2. The corpus consists
of 1000 sentences. Three of judges were able to
complete the task within 20 days. In this paper,
we report the result of applying the measure with
data provided by three annotators without consid-
ering the incomplete annotations. Distribution of
the sentences across the affective categories for the
three judges is given in Figure 1.
5.2 Analysis of Corpus Quality
The corpus was evaluated in terms of the proposed
measure. Some of the relevant observations are
presented below.
? Agreement Value: Different agreement val-
ues related to A
m
measure are given in Ta-
ble 1. We present A
m
values for all the anno-
tator pairs in Table 2.
2http://www.mla.iitkgp.ernet.in/Annotation/index.php
62
Figure 1: Distribution of sentences for three
judges.
Agreement A
m
Value
Observed Agreement(P
o
) 0.878
Chance Agreement(P
e
) 0.534
A
m
0.738
Table 1: Agreement values for the affective text
corpus.
Annotator Pair P
o
P
e
A
m
Value
1-2 0.858 0.526 0.702
1-3 0.868 0.54 0.713
2-3 0.884 0.531 0.752
Table 2: Annotator pairwise A
m
values.
? Agreement Study: Table 3 provides the dis-
tribution of the sentences against individual
observed agreement values. It is observed
Observed Agreement No. of Sentences
0.0 < A
0
? 0.2 14
0.2 < A
0
? 0.4 73
0.4 < A
0
? 0.7 198
0.7 < A
0
? 1.0 715
Table 3: Distribution of the sentences over ob-
served agreement.
that 71.5% of the corpus belongs to [0.7 1.0]
range of observed agreement and among this
bulk portion of the corpus, the annotators as-
sign 78.6% of the sentences into a single cat-
egory. This is due to the existence of a domi-
nant emotion in a sentence and in most of the
cases, the sentence contains enough clues to
decode it. For the non-dominant emotions in
a sentence, ambiguity has been found while
decoding.
? Disagreement Study: In Table 4, we present
the category wise disagreement for all the an-
notator pairs. From the disagreement table it
is evident that the categories with maximum
number of disagreements are anger, disgust
and fear. The emotions which are close to
each other in the evaluation-activation space
are inherently ambiguous. For example,
anger and disgust are close to each other in
the evaluation-activation space. So, ambigu-
ity between these categories will be higher
compared to other pairs. If [a b] is the pair, we
count the number of cases where one annota-
tor categorized one item into [a -] pattern and
other annotator classified the same item into
[- b] pattern. In Table 5, we provide the con-
fusion between two affective categories for all
annotator pairs. This confusion matrix is a
symmetric one. So, we have provided only
the upper triangular matrix.
In Figure 2, we provide ambiguity counts of
the affective category pairs. It can be ob-
Figure 2: Category pair wise disagreement
(A=Anger, D=Disgust, F=Fear, H=Happiness,
S=Sadness and Su=Surprise).
served that anger, disgust and fear are asso-
ciated with three topmost ambiguous pairs.
5.3 Gold Standard for Affective Text Corpus
To determine the gold standard corpus, we have
applied majority decision label based approach
discussed in section 4 on the judgements provided
by only three annotators. However, as the num-
ber of annotators is much less in the current study,
the determined gold standard corpus may not have
63
Anger Disgust Fear Happiness Sadness Surprise
1-2 68 94 74 64 74 45
1-3 74 86 105 57 54 45
2-3 65 49 58 22 50 20
Total 207 229 273 143 178 110
Table 4: Categorywise disagreement for the annotator pairs.
Anger Disgust Fear Happiness Sadness Surprise
Anger - 39 28 11 22 7
Disgust - - 28 6 24 13
Fear - - - 2 24 12
Happiness - - - - 18 8
Sadness - - - - - 9
Surprise - - - - - -
Table 5: Confusion matrix for category pairs.
much significance. Here, we report the result
of applying the gold standard determination algo-
rithm on the data provided by three annotators.
The distribution of sentences over the affective cat-
egories is depicted in Figure 3.
Figure 3: Distribution of sentences in gold stan-
dard corpus.
6 Conclusion and Future Work
Measuring the reliability of the affective text cor-
pus where one single item may be classified into
more than one single category is a complex task.
In this paper, we have provided a new coefficient
to measure reliability in multiclass annotation task
by incorporating pairwise agreement in affective
class pairs. The measure yields an agreement value
0.72, when applied to an annotated corpus pro-
vided by three users. This considerable agreement
value indicates that the affect categories consid-
ered for annotation may be applicable to the news
genre.
We are in process of collecting annotated corpus
from more annotators which will ensure a statisti-
cally significant result. According to the disagree-
ment study presented in section 5.2, confusions
between specific emotions is most likely between
categories which are adjacent in the activation-
evaluation space. The models of annotator agree-
ment which use weights for different types of dis-
agreement will be interesting for future study. The
direct and indirect affective sentences have not
been treated separately in this study. The algo-
rithm for determination of gold standard requires
more details investigation as simple majority vot-
ing may not be sufficient for highly subjective data
like emotion.
Acknowledgement
Plaban Kr. Bhowmick is partially supported by
Microsoft Corporation, USA and Media Lab Asia,
India. The authors are thankful to the reviewers for
their detailed suggestions regarding the work.
References
Artstein, Ron and Massimo Poesio. 2008. Inter-coder
Agreement for Computational Linguistics. Compu-
tational Linguistics.
Bruce, Rebecca F. and Janyce M. Wiebe 1999. Rec-
64
ognizing Subjectivity: A Case Study of Manual Tag-
ging. Natural Language Engineering. 1(1):1-16.
Carletta, Jean. 1996. Assessing Agreement on Classi-
fication Tasks: The Kappa Statistic. Computational
Linguistics. 22(21):249-254.
Carletta, Jean, Isard .A, Isard S., Jacqueline C. Kowtko,
Gwyneth D. Sneddon, and Anne H. Anderson. 1997.
The Reliability of a Dialogue Structure Coding
Scheme. Computational Linguistics. 23(1):13-32.
Cohen, Jacob. 1960. A Coefficient of Agreement
for Nominal Scales. Educational and Psychological
Measurement. 20(1):37-46.
Craggs Richard and Mary M. Wood. 2004. A Categori-
cal Annotation Scheme for Emotion in the Linguistic
Content of Dialogue. Tutorial and Research Work-
shop, Affective Dialogue Systems. Kloster Irsee, 89-
100.
Darwin, Charles. 1965. The Expression of Emotions in
Man and Animals.. Chicago: University of Chicago
Press. (Original work published 1872)
Ekman, Paul., Friesen W. V., and Ellsworth P. 1982.
What Emotion Categories or Dimensions can Ob-
servers Judge from Facial Behavior? Emotion in
the human face, Cambridge University Press. pages
39-55, New York.
Fleiss, Joseph L. 1981. Statistical Methods for Rates
and Proportions. Wiley. second ed., New York.
Hagen-Zanker, Alex. 2003. Fuzzy Set Approach to
Assessing Similarity of Categorical Maps. Interna-
tional Journal for Geographical Information Science.
17(3):235-249.
Hripcsak, George and Daniel F. Heitjan. 2002. Mea-
suring Agreement in Medical Informatics Reliabil-
ity Studies. Journal of Biomedical Informatics.
35(2):99-110.
Jung, Ho-Won. 2003. Evaluating Interrater Agreement
in SPICE-based Assessments. Computer Standards
& Interfaces. 25(5):477-499.
Krippendorff, Klaus 1980. Content Analysis: An Intro-
duction to its Methodology. Sage Publications. Bev-
erley Hills, CA.
Mieskes, Margot and Michael Strube. 2006. Part-of-
Speech Tagging of Transcribed Speech. Proceedings
of International Conference on Language Resources
and Evaluation. GENOA
Ortony, Andrew and Terence J. Turner. 1990. What?s
Basic About Basic Emotions?. Psychological Re-
view. 97(3):315-331.
Pantic, Maja and Leon Rothkrantz. 2000. Automatic
Analysis of Facial Expressions: The State of the Art.
IEEE Transactions on Pattern Analysis and Machine
Intelligence. 22(12):1424-1445.
Plutchik, Robert 1980. A General Psychoevolutionary
Theory of Emotion. Emotion: Theory, research, and
experience: Vol. 1. Theories of emotion. Academic
Press, New York, 3-33.
Rosenberg, Andrew, and Ed Binkowski. 2004. Aug-
menting the Kappa Statistic to Determine Interanno-
tator Reliability for Multiply Labeled Data Points.
In Proceedings of North American Chapter of the
Association for Computational Linguistics. Boston,
77-80.
Scott, William A. 1955. Reliability of Content Anal-
ysis: The Case of Nominal Scale Coding. Public
Opinion Quarterly. 19(3):321-325.
Song, Mingli, Chun Chen, Jiajun Bu, and Mingyu You.
2004. Speech Emotion Recognition and Intensity Es-
timation. Internation Conference on Computational
Science and its Applications. Perugia, 406-413.
Stolcke A., Ries K., Coccaro N., Shriberg E., Bates R.,
Jurafsky .D, Taylor P., Martin C. Van-Ess-Dykema,
and Meteer .M. 1997. Dialogue Act Modeling
for Automatic Tagging and Recognition of Con-
versational Speech. Computational Linguistics.
26(3):339-371.
65
