Using model-theoretic semantic interpretation to guide statistical
parsing and word recognition in a spoken language interface

William Schuler
Department of Computer and Information Science
University of Pennsylvania
200 S. 33rd Street
Philadelphia, PA 19104
schuler@linc.cis.upenn.edu
Abstract
This paper describes an extension of the se-
mantic grammars used in conventional sta-
tistical spoken language interfaces to allow
the probabilities of derived analyses to be
conditioned on the meanings or denotations
of input utterances in the context of an
interface's underlying application environ-
ment or world model. Since these denota-
tions will be used to guide disambiguation
in interactive applications, they must be ef-
ciently shared among the many possible
analyses that may be assigned to an input
utterance. This paper therefore presents a
formal restriction on the scope of variables
in a semantic grammar which guarantees
that the denotations of all possible analy-
ses of an input utterance can be calculated
in polynomial time, without undue con-
straints on the expressivity of the derived
semantics. Empirical tests show that this
model-theoretic interpretation yields a sta-
tistically signicant improvement on stan-
dard measures of parsing accuracy over a
baseline grammar not conditioned on deno-
tations.
1 Introduction
The development of speaker-independent mixed-
initiative speech interfaces, in which users not only
answer questions but also ask questions and give
instructions, is currently limited by the perfor-
mance of language models based largely on word co-
occurrences. Even under ideal circumstances, with
large application-specic corpora on which to train,

The author would like to thank David Chiang, Karin
Kipper, and three anonymous reviewers for particularly
helpful comments on this material. This work was sup-
ported by NSF grant EIA 0224417.
conventional language models are not su?ciently
predictive to correctly analyze a wide variety of in-
puts from a wide variety of speakers, such as might
be encountered in a general-purpose interface for di-
recting robots, o?ce assistants, or other agents with
complex capabilities. Such tasks may involve unla-
beled objects that must be precisely described, and a
wider range of actions than a standard database in-
terface would require (which also must be precisely
described), introducing a great deal of ambiguity into
input processing.
This paper therefore explores the use of a statis-
tical model of language conditioned on the mean-
ings or denotations of input utterances in the context
of an interface's underlying application environment
or world model. This use of model-theoretic inter-
pretation represents an important extension to the
`semantic grammars' used in existing statistical spo-
ken language interfaces, which rely on co-occurrences
among lexically-determined semantic classes and slot
llers (Miller et al, 1996), in that the probability
of an analysis is now also conditioned on the exis-
tence of denoted entities and relations in the world
model. The advantage of the interpretation-based
disambiguation advanced here is that the probabil-
ity of generating, for example, the noun phrase `the
lemon next to the safe' can be more reliably esti-
mated from the frequency with which noun phrases
have non-empty denotations { given the fact that
`the lemon next to the safe' does indeed denote some-
thing in the world model { than it can from the rel-
atively sparse co-occurrences of frame labels such as
lemon and next-to, or of next-to and safe.
Since there are exponentially many word strings
attributable to any utterance, and an exponential
(Catalan-order) number of possible parse tree anal-
yses attributable to any string of words, this use
of model-theoretic interpretation for disambiguation
must involve some kind of sharing of partial results
between competing analyses if interpretation is to be
performed on large numbers of possible analyses in a
practical interactive application. This paper there-
fore also presents a formal restriction on the scope of
variables in a semantic grammar (without untoward
constraints on the expressivity of the derived seman-
tics) which guarantees that the denotations of all
possible analyses of an input utterance can be calcu-
lated in polynomial time. Empirical tests show that
this use of model-theoretic interpretation in disam-
biguation yields a statistically signicant improve-
ment on standard measures of parsing accuracy over
a baseline grammar not conditioned on denotations.
2 Model-theoretic interpretation
In order to determine whether a user's directions de-
note entities and relations that exist in the world
model { and of course, in order to execute those
directions once they are disambiguated { it is nec-
essary to precisely represent the meanings of input
utterances.
Semantic grammars of the sort employed in cur-
rent spoken language interfaces for ight reservation
tasks (Miller et al, 1996; Sene et al, 1998) asso-
ciate fragments of logical { typically relational alge-
bra { expressions with recursive transition networks
encoding lexicalized rules in a context-free grammar
(the independent probabilities of these rules can then
be estimated from a training corpus and multiplied
together to give a probability for any given analysis).
In ight reservation systems, these associated seman-
tic expressions usually designate entities through a
xed set of constant symbols used as proper names
(e.g. for cities and numbered ights); but in applica-
tions with unlabeled (perhaps visually-represented)
environments, entities must be described by pred-
icating one or more modiers over some variable,
narrowing the set of potential referents by specify-
ing colors, spatial locations, etc., until only the de-
sired entity or entities remain. A semantic grammar
for interacting with this kind of unlabeled environ-
ment might contain the following rules, using vari-
ables x
1
::: x
n
(over entities in the world model) in
the associated logical expressions:
VP ! VP PP : x
1
::: x
n
: $1(x
1
::: x
m
)^
$2(x
1
; x
m+1
::: x
n
)
VP ! hold NP : x
1
:Hold (Agent; x
1
) ^ $2(x
1
)
NP ! a glass : x
1
:Glass(x
1
)
PP ! under NP : x
1
x
2
:Under(x
1
; x
2
) ^ $2(x
2
)
NP ! the faucet : x
1
:Faucet(x
1
)
in which m and n are integers and 0  m  n. Each
lambda expression x
1
::: x
n
:  indicates a function
from a tuple of entities he
1
::: e
n
i to a truth value
dened by the remainder of the expression  (sub-
VP ! VP PP
x
1
::: x
n=2
: $1(x
1
::: x
m=1
) ^ $2(x
1
; x
m+1
::: x
n
)
fhf
1
; g
1
i; hf
2
; g
2
i; : : : g
VP ! hold NP
x
1
: H(A;x
1
) ^ $2(x
1
)
fg
1
; g
2
; : : : g
hold NP ! . . .
x
1
: G(x
1
)
fg
1
; g
2
; : : : g
a glass
PP ! under NP
x
1
x
2
: U(x
1
; x
2
) ^ $2(x
2
)
fhf
1
; g
1
i; hf
2
; g
2
i; : : : g
under NP ! . . .
x
1
: F (x
1
)
ff
1
; f
2
; : : : g
the faucet
Figure 1: Semantic grammar derivation showing the
associated semantics and denotation of each con-
stituent. Entire rules are shown at each step in the
derivation in order to make the semantic associations
explicit.
stituting e
1
::: e
n
for x
1
::: x
n
), which denotes a set of
tuples satisfying , drawn from E
n
(where E is the
set of entities in the world model).
The pseudo-variables $1; $2; : : : in this notation in-
dicate the sites at which the semantic expressions
associated with each rule's nonterminal symbols are
to compose (the numbers correspond to the relative
positions of the symbols on the right hand side of
each rule, numbered from left to right). Semantic ex-
pressions for complete sentences are then formed by
composing the sub-expressions associated with each
rule at the appropriate sites.
1
Figure 1 shows the above rules assembled in a
derivation of the sentence `hold a glass under the
faucet.' The denotation annotated beneath each con-
stituent is simply the set of variable assignments
(for each free variable) that satisfy the constituent's
semantics. These denotations exactly capture the
meaning (in a given world model) of the assem-
bled semantic expressions dominated by each con-
stituent, regardless of how many sub-expressions are
subsumed by that constituent, and can therefore be
shared among competing analyses in lieu of the se-
mantic expression itself, as a partial result in model-
theoretic interpretation.
2.1 Variable scope
Note, however, that the adjunction of the preposi-
tional phrase modier `under the faucet' adds an-
other free variable (x
2
) to the semantics of the verb
1
This use of pseudo-variables is intended to resemble
that of the unix program `yacc,' which has a similar pur-
pose (associating syntax with semantics in constructing
compilers for programming languages).
VP ! VP PP
x
1
::: x
n=1
: $1(x
1
::: x
m=0
) ^ $2(x
1
; x
m+1
::: x
n
)
VP ! hold NP
Q
x
1
: H(A;x
1
) ^ $2(x
1
)
hold NP ! . . .
PP ! under NP
x
1
: Q
x
2
: U(x
1
; x
2
) ^ $2(x
2
)
under NP ! . . .
Figure 2: Derivation with minimal scoping. The
variable x
1
in the semantic expression associated
with the prepositional phrase `under the faucet' can-
not be identied with the variable in the verb phrase.
phrase, and therefore another factor of jEj to the car-
dinality of its denotation. Moreover, under this kind
of global scoping, if additional prepositional phrases
are adjoined, they would each contribute yet an-
other free variable, increasing the complexity of the
denotation by an additional factor of jEj, making
the shared interpretation of such structures poten-
tially exponential on the length of the input. This
proliferation of free variables means that the vari-
ables introduced by the noun phrases in an utter-
ance, such as `hold a glass under the faucet,' can-
not all be given global scope, as in Figure 1. On
the other hand, the variables introduced by quanti-
ed noun phrases cannot be bound as soon as the
noun phrases are composed, as in Figure 2, because
these variables may need to be used in modiers
composed in subsequent (higher) rule applications.
Fortunately, if these non-immediate variable scop-
ing arrangements are expressed structurally, as dom-
inance relationships in the elementary tree struc-
tures of some grammar, then a structural restriction
on this grammar can be enforced that preserves as
many non-immediate scoping arrangements as possi-
ble while still preventing an unbounded proliferation
of free variables.
The correct scoping arrangements (e.g. for the sen-
tence `hold a glass under the faucet,' shown Fig-
ure 3) can be expressed using ordered sets of parse
rules grouped together in such a way as to allow
other structural material to intervene. In this case,
a group would include a rule for composing a verb
and a noun phrase with some associated predicate,
and one or more rules for binding each of the pred-
icate's variables in a quantier somewhere above it
(thereby ensuring that these rules always occur to-
gether with the quantier rules dominating the pred-
icate rule), while still allowing rules adjoining prepo-
sitional phrase modiers to apply in between them
(so that variables in their associated predicates can
VP ! VP
x
2
::: x
n=1
: Q
x
1
: $1(x
1
::: x
n
)
fhig
VP ! VP PP
x
1
::: x
n=1
: $1(x
1
::: x
m=1
) ^ $2(x
1
; x
m+1
::: x
n
)
fg
1
; g
2
; : : : g
VP ! hold NP
x
1
: H(A;x
1
) ^ $2(x
1
)
fg
1
; g
2
; : : : g
hold NP ! . . .
PP ! PP
x
2
::: x
n
: Q
x
1
: $1(x
1
::: x
n
)
fg
1
; g
2
; : : : g
PP ! under NP
x
1
x
2
: U(x
2
; x
1
) ^ $2(x
1
)
fhf
1
; g
1
i; hf
2
; g
2
i; : : : g
under NP ! . . .
Figure 3: Derivation with desired scoping.
be bound by the same quantiers).
These `grouped rules' can be formalized using a
tree-rewriting system whose elementary trees can
subsume several ordered CFG rule applications (or
steps in a context-free derivation), as shown in Fig-
ure 4. Each such elementary tree contains a rule
(node) associated with a logical predicate and rules
(nodes) associated with quantiers binding each of
the predicate's variables. These trees are then com-
posed by rewriting operations (dotted lines), which
split them up and either insert them between or iden-
tify them with (if demarcated with dashed lines) the
rules in another elementary tree { in this case, the
elementary tree anchored by the word `under.' These
trees are considered elementary in order to exclude
the possibility of generating derivations that contain
unbound variables or quantiers over unused vari-
ables, which would have no intuitive meaning. The
composition operations will be presented in further
detail in Section 2.2.
2.2 Semantic composition as tree-rewriting
A general class of rewriting systems can be dened
using sets of allowable expansions of some type of
object to incorporate zero or more other instances
of the same type of object, each of which is simi-
larly expandable. Such a system can generate ar-
bitrarily complex structure by recursively expand-
ing or `rewriting' each new object, concluding with a
set of zero-expansions at the frontier. For example,
a context-free grammar may be cast as a rewriting
system whose objects are strings, and whose allow-
able expansions are its grammar productions, each
of which expands or rewrites a certain string as a set
VP ! VP
x
2
::: x
n
: Q
x
1
: $1(x
1
::: x
n
)
VP ! hold NP
x
1
::: x
n
: $1(x
1
::: x
n
) ^ $2(x
1
)
V ! hold
x
1
:Hold(A; x
1
)
NP ! . . .
. . .
1
VP ! VP
x
2
::: x
n
: Q
x
1
: $1(x
1
::: x
n
)
VP ! VP PP
x
1
::: x
n
: $1(x
1
::: x
m
) ^ $2(x
1
; x
m+1
::: x
n
)
VP
1
PP ! PP
x
2
::: x
n
: Q
x
1
: $1(x
1
::: x
n
)
PP ! P NP
x
1
::: x
n
: $1(x
1
::: x
n
) ^ $2(x
1
)
P ! under
x
1
x
2
:Under(x
2
; x
1
)
NP
2
1
2
PP ! PP
x
2
::: x
n
: Q
x
1
: $1(x
1
::: x
n
)
NP ! Q N
$2
Q ! a N ! faucet
x
1
:Faucet(x
1
)
Figure 4: Complete elementary tree for `under' showing argument insertion sites.
of zero or more sub-strings arranged around certain
`elementary' strings contributing terminal symbols.
A class of tree-rewriting systems can similarly
be dened as rewriting systems whose objects are
trees, and whose allowable expansions are produc-
tions (similar to context-free productions), each of
which rewrite a tree A as some function f applied
to zero or more sub-trees A
1
; : : : A
s
; s  0 arranged
around some `elementary' tree structure dened by
f (Pollard, 1984; Weir, 1988):
A ! f(A
1
; : : : A
s
) (1)
This elementary tree structure can be used to ex-
press the dominance relationship between a logical
predicate and the quantiers that bind its variables
(which must be preserved in any meaningful derived
structure); but in order to allow the same instance
of a quantier to bind variables in more than one
predicate, the rewriting productions of such a se-
mantic tree-rewriting system must allow expanded
subtrees to identify parts of their structure (speci-
cally, the parts containing quantiers) with parts of
each other's structure, and with that of their host
elementary tree.
In particular, a rewriting production in such a sys-
tem would rewrite a tree A as an elementary tree 
0
with a set of sub-trees A
1
; : : : A
s
inserted into it, each
of which is rst partitioned into a set of contiguous
components (in order to isolate particular quantier
nodes and other kinds of sub-structure) using a tree
partition function g at some sequence of split points
h#
i1
,...#
ic
i
i, which are node addresses in A
i
(the rst
of which simply species the root).
2
The resulting se-
quence of partitioned components of each expanded
2
The node addresses encode a path from the root of
sub-tree are then inserted into 
0
at a correspond-
ing sequence of insertion site addresses h
i1
,... 
ic
i
i
dened by the rewriting function f :
f(A
1
; : : : A
s
) =

0
[h
11
,... 
1c
1
i; g
#
11
,...#
1c
1
(A
1
)] : : :
[h
s1
,... 
sc
s
i; g
#
s1
,...#
sc
s
(A
s
)] (2)
Since each address can only host a single inserted
component, any components from dierent sub-tree
arguments of f that are assigned to the same inser-
tion site address are constrained to be identical in or-
der for the production to apply. Additionally, some
addresses may be `pre-lled' as part of the elemen-
tary structure dened in f , and therefore may also
be identied with components of sub-tree arguments
of f that are inserted at the same address.
Figure 4 shows the set of insertion sites (designated
with boxed indices) for each argument of an elemen-
tary tree anchored by `under.' The sites labeled 1 ,
associated with the rst argument sub-tree (in this
case, the tree anchored by `hold'), indicate that it is
composed by partitioning it into three components,
each dominating or dominated by the others, the low-
est of which is inserted at the terminal node labeled
`VP,' the middle of which is identied with a pre-
lled component (delimited by dashed lines), con-
taining the quantier node labeled `VP ! VP,' and
the uppermost of which (empty in the gure) is in-
serted at the root, while preserving the relative dom-
inance relationships among the nodes in both trees.
Similarly, sites labeled 2 , associated with the sec-
ond argument sub-tree (for the noun phrase comple-
the tree in which every address i species the i
th
child
of the node at the end of path .
ment to the preposition), indicate that it is composed
by partitioning it into two components { again, one
dominating the other { the lowest of which is inserted
at the terminal node labeled `NP,' and the uppermost
of which is identied with another pre-lled compo-
nent containing the quantier node labeled `PP !
PP,' again preserving the relative dominance rela-
tionships among the nodes in both trees.
2.3 Shared interpretation
Recall the problem of unbounded variable prolif-
eration described in Section 2.1. The advantage
of using a tree-rewriting system to model semantic
composition is that such systems allow the appli-
cation of well-studied restrictions to limit their re-
cursive capacity to generate structural descriptions
(in this case, to limit the unbounded overlapping
of quantier-variable dependencies that can produce
unlimited numbers of free variables at certain steps in
a derivation), without limiting the multi-level struc-
ture of their elementary trees, used here for captur-
ing the well-formedness constraint that a predicate
be dominated by its variables' quantiers.
One such restriction, based on the regular form
restriction dened for tree adjoining grammars
(Rogers, 1994), prohibits a grammar from allowing
any cycle of elementary trees, each intervening inside
a spine (a path connecting the insertion sites of any
argument) of the next. This restriction is dened
below:
Denition 2.1 Let a spine in an elementary tree be
the path of nodes (or object-level rule applications)
connecting all insertion site addresses of the same
argument.
Denition 2.2 A grammar G is in regular form if a
directed acyclic graph hV;Ei can be drawn with ver-
tices v
H
; v
A
2 V corresponding to partitioned ele-
mentary trees of G (partitioned as described above),
and directed edges hv
H
; v
A
i 2 E  V  V from each
vertex v
H
, corresponding to a partitioned elementary
tree that can host an argument, to each vertex v
A
,
corresponding to a partitioned elementary tree that
can function as its argument, whose partition inter-
sects its spine at any place other than the top node
in the spine.
This restriction ensures that there will be no un-
bounded `pumping' of intervening tree structure in
any derivation, so there will never be an unbounded
amount of unrecognized tree structure to keep track
of at any step in a bottom-up parse, so the number
of possible descriptions of each sub-span of the in-
put will be bounded by some constant. It is called a
`regular form' restriction because it ensures that the
set of root-to-leaf paths in any derived structure will
form a regular language.
A CKY-style parser can now be built that rec-
ognizes each context-free rule in an elementary tree
from the bottom up, storing in order the unrecog-
nized rules that lie above it in the elementary tree
(as well as any remaining rules from any composed
sub-trees) as a kind of promissory note. The fact
that any regular-form grammar has a regular path
set means that only a nite number of states will be
required to keep track of this promised, unrecognized
structure in a bottom-up traversal, so the parser will
have the usual O(n
3
) complexity (times a constant
equal to the nite number of possible unrecognized
structures).
Moreover, since the parser can recognize any string
derivable by such a grammar, it can create a shared
forest representation of every possible analysis of a
given input by annotating every possible applica-
tion of parse rules that could be used in the deriva-
tion of each constituent (Billot and Lang, 1989).
This polynomial-sized shared forest representation
can then be interpreted determine which constituents
denote entities and relations in the world model, in
order to allow model-theoretic semantic information
to guide disambiguation decisions in parsing.
Finally, the regular form restriction also has the
important eect of ensuring that the number of un-
recognized quantier nodes at any step in a bottom-
up analysis { and therefore the number of free vari-
ables in any word or phrase constituent of a parse { is
also bounded by some constant, which limits the size
of any constituent's denotation to a polynomial or-
der of E , the number of entities in the environment.
The interpretation of any shared forest derived by
this kind of regular-form tree-rewriting system can
therefore be calculated in worst-case polynomial time
on E .
A denotation-annotated shared forest for the noun
phrase `the girl with the hat behind the counter' is
shown in Figure 5, using the noun and preposition
trees from Figure 4, with alternative applications of
parse rules represented as circles below each derived
constituent. This shared structure subsumes two
competing analyses: one containing the noun phrase
`the girl with the hat', denoting the entity g
1
, and
the other containing the noun phrase `the hat be-
hind the counter', which does not denote anything
in the world model. Assuming that noun phrases
rarely occur with empty denotations in the training
data, the parse containing the phrase `the girl with
the hat' will be preferred, because there is indeed a
girl with a hat in the world model.
This formalism has similarities with two ex-
NP ! girl
x
1
:Girl(x
1
)
fg
1
; g
2
; g
3
g
P ! with
x
1
x
2
:With(x
2
; x
1
)
fhh
1
; g
1
i; hh
2
; b
1
ig
NP ! hat
x
1
:Hat(x
1
)
fh
1
; h
2
; h
3
; h
4
g
P ! behind
x
1
x
2
:Behind(x
2
; x
1
)
fhc
1
; g
1
ig
NP ! counter
x
1
:Counter(x
1
)
fc
1
; c
2
g
PP ! P NP
x
1
::: x
n=2
: $1(x
1
::: x
n
) ^ $2(x
1
)
fhh
1
; g
1
i; hh
2
; b
1
ig
PP ! PP
x
2
::: x
n=2
: Q
x
1
: $1(x
1
::: x
n
)
fg
1
; b
1
g
PP ! P NP
x
1
::: x
n=2
: $1(x
1
::: x
n
) ^ $2(x
1
)
fhc
1
; g
1
ig
PP ! PP
x
2
::: x
n=2
: Q
x
1
: $1(x
1
::: x
n
)
fg
1
g
NP ! NP PP
x
1
::: x
n=1
: $1(x
1
::: x
m=1
) ^ $2(x
1
; x
m+1
::: x
n
)
fg
1
g
NP ! NP PP
x
1
::: x
n=1
: $1(x
1
::: x
m=1
) ^ $2(x
1
; x
m+1
::: x
n
)
;
PP ! P NP
x
1
::: x
n=2
: $1(x
1
::: x
n
) ^ $2(x
1
)
;
PP ! PP
x
2
::: x
n=2
: Q
x
1
: $1(x
1
::: x
n
)
;
NP ! NP PP
x
1
::: x
n=1
: $1(x
1
::: x
m=1
) ^ $2(x
1
; x
m+1
::: x
n
)
; or fg
1
g
Figure 5: Shared forest for `the girl with the hat behind the counter.'
tensions of tree-adjoining grammar (Joshi, 1985),
namely multi-component tree adjoining grammar
(Becker et al, 1991) and description tree substitu-
tion grammar (Rambow et al, 1995), and indeed
represents something of a combination of the two:
1. Like description tree substitution grammars,
but unlike multi-component TAGs, it allows
trees to be partitioned into any desired set of
contiguous components during composition,
2. Like multi-component TAGs, but unlike descrip-
tion tree substitution grammars, it allows the
specication of particular insertion sites within
elementary trees, and
3. Unlike both, it allow duplication of structure
(which is used for merging quantiers from dif-
ferent elementary trees).
The use of lambda calculus functions to dene de-
composable meanings for input sentences draws on
traditions of Church (1940) and Montague (1973),
but this approach diers from the Montagovian sys-
tem by introducing explicit limits on computational
complexity (in order to allow tractable disambigua-
tion).
This approach to semantics is very similar to that
described by Shieber (1994), in which syntactic and
semantic expressions are assembled synchronously
using paired tree-adjoining grammars with isomor-
phic derivations, except that in this approach the
derived structures are isomorphic as well, hence the
reduction of synchronous tree pairs to semantically-
annotated syntax trees. This isomorphism restric-
tion on derived trees reduces the number of quantier
scoping congurations that can be assigned to any
given input (most of which are unlikely to be used
in a practical application), but its relative parsimony
allows syntactically ambiguous inputs to be seman-
tically interpreted in a shared forest representation
in worst-case polynomial time. The interleaving of
semantic evaluation and parsing for the purpose of
disambiguation also has much in common with that
of Dowding et al (1994), except that in this case,
constituents are not only semantically type-checked,
but are also fully interpreted each time they are pro-
posed. There are also commonalities between the un-
derspecied semantic representation of structurally-
ambiguous elementary tree constituents in a shared
forest and the underspecied semantic representa-
tion of (e.g. quantier) scope ambiguity described
by Reyle (1993).
3
3 Evaluation
The contribution of this model-theoretic semantic in-
formation toward disambiguation was evaluated on
a set of directions to animated agents collected in a
controlled but spatially complex 3-D simulated en-
vironment (of children running a lemonade stand).
In order to avoid priming them towards particu-
lar linguistic constructions, subjects were shown un-
narrated animations of computer-simulated agents
performing dierent tasks in this environment (pick-
ing fruit, operating a juicer, and exchanging lemon-
ade for money), which were described only as the `de-
sired behavior' of each agent. The subjects were then
asked to direct the agents, using their own words, to
perform the desired behaviors as shown.
340 utterances were collected and annotated with
brackets and elementary tree node addresses as de-
scribed in Section 2.2, for use as training data and
as gold standard data in testing. Some sample direc-
tions are shown in Figure 6. Most elementary trees
were extracted, with some simplications for parsing
e?ciency, from an existing broad-coverage grammar
resource (XTAG Research Group, 1998), but some
elementary trees for multi-word expressions had to
be created anew. In all, a complete annotation of this
corpus required a grammar of 68 elementary trees
and a lexicon of 288 lexicalizations (that is, words
or sets of words with indivisible semantics, forming
the anchors of a given elementary tree). Each lex-
icalization was then assigned a semantic expression
describing the intended geometric relation or class of
objects in the simulated 3-D environment.
4
The interface was tested on the rst 100 collected
utterances, and the parsing model was trained on
the remaining utterances. The presence or absence
of a denotation of each constituent was added to the
label of each constituent in the denotation-sensitive
parsing model (for example, statistics were collected
for the frequency of `NP:  ! NP:+ PP:+' events,
meaning a noun phrase that does not denote any-
3
Denotation of competing applications of parse rules
can be unioned (though this eectively treats ambiguity
as a form of disjunction), or stored separately to some
nitie beam (though some globally preferable but locally
dispreferred structures would be lost).
4
Here it was assumed that the intention of the user
was to direct the agent to perform the actions shown in
the `desired behavior' animation.
Walk towards the tree where you see a yellow lemon
on the ground.
Pick up the lemon.
Place the lemon in the pool.
Take the dollar bill from the person in front of you.
Walk to the left towards the big black cube.
Figure 6: Sample utterances from collected corpus.
thing in the environment expands to a noun phrase
and a prepositional phrase that do have a denota-
tion in the environment), whereas the baseline sys-
tem used a parsing model conditioned on only con-
stituent labels (for example, `NP ! NP PP' events).
The entire word lattice output of the speech recog-
nizer was fed directly into the parser, so as to al-
low the model-theoretic semantic information to be
brought to bear on word recognition ambiguity as
well as on structural ambiguity in parsing.
Since any derivation of elementary trees uniquely
denes a semantic expression at each node, the task
of evaluating this kind of semantic analysis is reduced
to the familiar task of evaluating a the accuracy of
a labeled bracketing (labeled with elementary tree
names and node addresses). Here, the standard mea-
sures of labeled precision and recall are used. Note
that there may be multiple possible bracketings for
each gold standard tree in a given word lattice that
dier only in the start and end frames of the com-
ponent words. Since neither the baseline nor test
parsing models are sensitive to the start and end
frames of the component words, the gold standard
bracketing is simply assumed to use the most likely
frame segmentation in the word lattice that yields
the correct word sequence.
The results of the experiment are summarized
below. The environment-based model shows a
statistically signicant (p<.05) improvement of 3
points in labeled recall, a 12% reduction in error.
Most of the improvement can be attributed to the
denotation-sensitive parser dispreferring noun phrase
constituents with mis-attached modiers, which do
not denote anything in the world model.
Model LR LP
baseline model 82% 78%
baseline + denotation bit 85% 81%
4 Conclusion
This paper has described an extension of the seman-
tic grammars used in conventional spoken language
interfaces to allow the probabilities of derived anal-
yses to be conditioned on the results of a model-
theoretic interpretation. In particular, a formal re-
striction was presented on the scope of variables in a
semantic grammar which guarantees that the deno-
tations of all possible analyses of an input utterance
can be calculated in polynomial time, without un-
due constraints on the expressivity of the derived
semantics. Empirical tests show that this model-
theoretic interpretation yields a statistically signif-
icant improvement on standard measures of parsing
accuracy over a baseline grammar not conditioned
on denotations.
References
Tilman Becker, Aravind Joshi, and Owen Rambow.
1991. Long distance scrambling and tree adjoining
grammars. In Fifth Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL'91), pages 21{26.
Sylvie Billot and Bernard Lang. 1989. The structure
of shared forests in ambiguous parsing. In Proceed-
ings of the 27
th
Annual Meeting of the Association
for Computational Linguistics (ACL '89), pages
143{151.
Alonzo Church. 1940. A formulation of the sim-
ple theory of types. Journal of Symbolic Logic,
5(2):56{68.
John Dowding, Robert Moore, Francois Andery, and
Douglas Moran. 1994. Interleaving syntax and se-
mantics in an e?cient bottom-up parser. In Pro-
ceedings of the 32nd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL'94).
Aravind K. Joshi. 1985. How much context sensi-
tivity is necessary for characterizing structural de-
scriptions: Tree adjoining grammars. In L. Kart-
tunen D. Dowty and A. Zwicky, editors, Natu-
ral language parsing: Psychological, computational
and theoretical perspectives, pages 206{250. Cam-
bridge University Press, Cambridge, U.K.
Scott Miller, David Stallard, Robert Bobrow, and
Richard Schwartz. 1996. A fully statistical ap-
proach to natural language interfaces. In Pro-
ceedings of the 34th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL'96),
pages 55{61.
Richard Montague. 1973. The proper treatment
of quantication in ordinary English. In J. Hin-
tikka, J.M.E. Moravcsik, and P. Suppes, editors,
Approaches to Natural Langauge, pages 221{242.
D. Riedel, Dordrecht. Reprinted in R. H. Thoma-
son ed., Formal Philosophy, Yale University Press,
1994.
Carl Pollard. 1984. Generalized phrase structure
grammars, head grammars and natural langauge.
Ph.D. thesis, Stanford University.
Owen Rambow, David Weir, and K. Vijay-Shanker.
1995. D-tree grammars. In Proceedings of the 33rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL '95).
Uwe Reyle. 1993. Dealing with ambiguities by un-
derspecication: Construction, representation and
deduction. Journal of Semantics, 10:123{179.
James Rogers. 1994. Capturing CFLs with tree ad-
joining grammars. In Proceedings of the 32nd An-
nual Meeting of the Association for Computational
Linguistics (ACL '94).
Stephanie Sene, Ed Hurley, Raymond Lau, Chris-
tine Pao, Philipp Schmid, and Victor Zue. 1998.
Galaxy-II: a reference architecture for conversa-
tional system development. In Proceedings of the
5th International Conference on Spoken Language
Processing (ICSLP '98), Sydney, Australia.
Stuart M. Shieber. 1994. Restricting the weak-
generative capability of synchronous tree adjoining
grammars. Computational Intelligence, 10(4).
David Weir. 1988. Characterizing mildly context-
sensitive grammar formalisms. Ph.D. thesis, De-
partment of Computer and Information Science,
University of Pennsylvania.
XTAG Research Group. 1998. A lexicalized tree
adjoining grammar for english. Technical report,
IRCS, University of Pennsylvania.
