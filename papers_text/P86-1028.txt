FORUM ON CONNECTIONISM 
Connectionist Models for Natura l  Language Processing 
David L. Waltz 
Thinking Machines Corporation 
245 First Street 
Cambridge, MA 02142 
and 
Program in Linguistics and Cognitive Science 
Brandeis University 
Brown 125 
Waltham, MA 02254 
PANEL IST  STATEMENT 
After an almost twenty year lull, there has been a 
dramatic upsurge of interest in massively parallel models for 
computation, descendants of perceptron and pandemonium 
models, now dubbed 'connectionist models.' Much of the 
connectionist research has focused on models for natural an- 
guage processing. There have been three main reasons for 
this increase in interest: 
1. Scientific adequacy of the models 
2. The availability of fine-grained parallel hardware 
to run the models 
3. The demonstration of powerful connectionist 
learning models. 
The scientific adequacy of models based on a small num- 
ber of coarse-grained primitives (e.g. conceptual dependency), 
popular in AI during the 70's, has been called into question 
and substantially replaced by a current emphasis in much of 
computational linguistics on lexicalist models (i.e., ones which 
use words for representing concepts or meanings). However, 
few people can doubt that words are too coarse, that they 
have structure and properties and features. Connectionist 
models offer very fine granularity; they can capture such 
detail in a manner that still allows for tractable computation. 
Such models also promise to make the integration of syntac- 
tic, semantic, pragmatic, and memory models simpler and 
more transparent. 
Fine-grained hardware, such as the Connection Machine, 
can allow models with millions of active elements, full 
vocabularies, and rapid throughput, as well as powerful near- 
term connectionist applications based on the use of associa- 
tive memory and hardware support for interprocessor com- 
munication. Meanwhile, connectionist learning models, such 
as the Boltzmann Machine and its descendant, he backward 
error propagation model, have demonstrated surprising 
power in learning concepts from example; as for instance in 
Sejnowski's NETtalk, which learned the pronunciation rules 
for English from examples. The future promises yet more 
surprising results as the concepts in even more radical 
models, such as Minsky's Society of Minds model, are 
digested and as new, even more powerful hardware becomes 
available. 
185 
