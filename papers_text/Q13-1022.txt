Transactions of the Association for Computational Linguistics, 1 (2013) 267?278. Action Editor: Brian Roark.
Submitted 3/2013; Published 7/2013. c?2013 Association for Computational Linguistics.
Efficient Parsing for Head-Split Dependency Trees
Giorgio Satta
Dept. of Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Marco Kuhlmann
Dept. of Linguistics and Philology
Uppsala University, Sweden
marco.kuhlmann@lingfil.uu.se
Abstract
Head splitting techniques have been success-
fully exploited to improve the asymptotic
runtime of parsing algorithms for project-
ive dependency trees, under the arc-factored
model. In this article we extend these tech-
niques to a class of non-projective dependency
trees, called well-nested dependency trees with
block-degree at most 2, which has been previ-
ously investigated in the literature. We define a
structural property that allows head splitting for
these trees, and present two algorithms that im-
prove over the runtime of existing algorithms
at no significant loss in coverage.
1 Introduction
Much of the recent work on dependency parsing has
been aimed at finding a good balance between ac-
curacy and efficiency. For one end of the spectrum,
Eisner (1997) showed that the highest-scoring pro-
jective dependency tree under an arc-factored model
can be computed in timeO.n3/, where n is the length
of the input string. Later work has focused on mak-
ing projective parsing viable under more expressive
models (Carreras, 2007; Koo and Collins, 2010).
At the same time, it has been observed that for
many standard data sets, the coverage of projective
trees is far from complete (Kuhlmann and Nivre,
2006), which has led to an interest in parsing al-
gorithms for non-projective trees. While non-project-
ive parsing under an arc-factored model can be done
in time O.n2/ (McDonald et al, 2005), parsing with
more informed models is intractable (McDonald and
Satta, 2007). This has led several authors to investig-
ate ?mildly non-projective? classes of trees, with the
goal of achieving a balance between expressiveness
and complexity (Kuhlmann and Nivre, 2006).
In this article we focus on a class of mildly non-
projective dependency structures called well-nested
dependency trees with block-degree at most 2. This
class was first introduced by Bodirsky et al (2005),
who showed that it corresponds, in a natural way, to
the class of derivation trees of lexicalized tree-adjoin-
ing grammars (Joshi and Schabes, 1997). While there
are linguistic arguments against the restriction to this
class (Maier and Lichte, 2011; Chen-Main and Joshi,
2010), Kuhlmann and Nivre (2006) found that it has
excellent coverage on standard data sets. Assum-
ing an arc-factored model, well-nested dependency
trees with block-degree  2 can be parsed in time
O.n7/ using the algorithm of Go?mez-Rodr??guez et
al. (2011). Recently, Pitler et al (2012) have shown
that if an additional restriction called 1-inherit is im-
posed, parsing can be done in time O.n6/, without
any additional loss in coverage on standard data sets.
Standard context-free parsing methods, when adap-
ted to the parsing of projective trees, provide O.n5/
time complexity. The O.n3/ time result reported by
Eisner (1997) has been obtained by exploiting more
sophisticated dynamic programming techniques that
?split? dependency trees at the position of their heads,
in order to save bookkeeping. Splitting techniques
have also been exploited to speed up parsing time
for other lexicalized formalisms, such as bilexical
context-free grammars and head automata (Eisner
and Satta, 1999). However, to our knowledge no at-
tempt has been made in the literature to extend these
techniques to non-projective dependency parsing.
In this article we leverage the central idea from
Eisner?s algorithm and extend it to the class of well-
nested dependency trees with block-degree at most 2.
267
We introduce a structural property, called head-split,
that allows us to split these trees at the positions of
their heads. The property is restrictive, meaning that
it reduces the class of trees that can be generated.
However, we show that the restriction to head-split
trees comes at no significant loss in coverage, and it
allows parsing in timeO.n6/, an asymptotic improve-
ment of one order of magnitude over the algorithm
by Go?mez-Rodr??guez et al (2011) for the unrestric-
ted class. We also show that restricting the class of
head-split trees by imposing the already mentioned
1-inherit property does not cause any additional loss
in coverage, and that parsing for the combined class
is possible in time O.n5/, one order of magnitude
faster than the algorithm by Pitler et al (2012) for
the 1-inherit class without the head-split condition.
The above results have consequences also for the
parsing of other related formalisms, such as the
already mentioned lexicalized tree-adjoining gram-
mars. This will be discussed in the final section.
2 Head Splitting
To introduce the basic idea of this article, we briefly
discuss in this section two well-known algorithms for
computing the set of all projective dependency trees
for a given input sentence: the na??ve, CKY-style
algorithm, and the improved algorithm with head
splitting, in the version of Eisner and Satta (1999).1
CKY parsing The CKY-style algorithm works in
a pure bottom-up way, building dependency trees
by combining subtrees. Assuming an input string
w D a1    an, n  1, each subtree t is represented
by means of a finite signature ?i; j; h?, called item,
where i; j are the boundary positions of t ?s span over
w and h is the position of t?s root. This is the only
information we need in order to combine subtrees
under the arc-factored model. Note that the number
of possible signatures is O.n3/.
The main step of the algorithm is displayed in
Figure 1(a). Here we introduce the graphical conven-
tion, used throughout this article, of representing a
subtree by a shaded area, with an horizontal line in-
dicating the spanned fragment of the input string, and
of marking the position of the head by a bullet. The
illustrated step attaches a tree with signature ?k; j; d ?
1Eisner (1997) describes a slightly different algorithm.
(a)
ah ad
i k j
)
ah ad
i j
(b)
ah ad
k
)
ah ad
(c)
ah ad
j
)
ah ad
j
Figure 1: Basic steps for (a) the CKY-style algorithm
and (b, c) the head splitting algorithm.
as a dependent of a tree with signature ?i; k; h?. There
can be O.n5/ instantiations of this step, and this is
also the running time of the algorithm.
Eisner?s algorithm Eisner and Satta (1999) im-
prove over the CKY algorithm by reducing the num-
ber of position records in an item. They do this by
?splitting? each tree into a left and a right fragment,
so that the head is always placed at one of the two
boundary positions of a fragment, as opposed to be-
ing placed at an internal position. In this way items
need only two indices. Left and right fragments can
be processed independently, and merged afterwards.
Let us consider a right fragment t with head ah.
Attachment at t of a right dependent tree with head
ad is now performed in two steps. The first step at-
taches a left fragment with head ad , as in Figure 1(b).
This results in a new type of fragment/item that has
both heads ah and ad placed at its boundaries. The
second step attaches a right fragment with head ad ,
as in Figure 1(c). The number of possible instanti-
ations of these steps, and the asymptotic runtime of
the algorithm, is O.n3/.
In this article we extend the splitting technique to
the class of well-nested dependency trees with block-
degree at most 2. This amounts to defining a fac-
torization for these trees into fragments, each with
its own head at one of its boundary positions, along
with some unfolding of the attachment operation into
intermediate steps. While for projective trees head
splitting can be done without any loss in coverage,
for the extended class head splitting turns out to be
a proper restriction. The empirical relevance of this
will be discussed in ?7.
268
3 Head-Split Trees
In this section we introduce the class of well-nested
dependency trees with block-degree at most 2, and
define the subclass of head-split dependency trees.
3.1 Preliminaries
For non-negative integers i; j we write ?i; j ? to de-
note the set fi; iC1; : : : ; j g; when i > j , ?i; j ? is the
empty set. For a string w D a1    an, where n  1
and each ai is a lexical token, and for i; j 2 ?0; n?
with i  j , we write wi;j to denote the substring
aiC1    aj of w; wi;i is the empty string.
A dependency tree t over w is a directed tree
whose nodes are a subset of the tokens ai in w and
whose arcs encode a dependency relation between
two nodes. We write ai ! aj to denote the arc
.ai ; aj / in t ; here, the node ai is the head, and the
node aj is the dependent. If each token ai , i 2 ?1; n?,
is a node of t , then t is called complete. Sometimes
we write tai to emphasize that tree t is rooted in node
ai . If ai is a node of t , we also write t ?ai ? to denote
the subtree of t composed by node ai as its root and
all of its descendant nodes.
The nodes of t uniquely identify a set of max-
imal substrings of w, that is, substrings separated
by tokens not in t . The sequence of such substrings,
ordered from left to right, is the yield of t , written
yd.t/. Let ai be some node of t . The block-degree
of ai in t , written bd.ai ; t /, is defined as the number
of string components of yd.t ?ai ?/. The block-degree
of t , written bd.t/, is the maximal block-degree of
its nodes. Tree t is non-projective if bd.t/ > 1.
Tree t is well-nested if, for each node ai of t and for
every pair of outgoing dependencies ai ! ad1 and
ai ! ad2 , the string components of yd.t ?ad1 ?/ and
yd.t ?ad2 ?/ do not ?interleave? in w. More precisely,
it is required that, if some component of yd.t ?adi ?/,
i 2 ?1; 2?, occurs in w in between two components
s1; s2 of yd.t ?adj ?/, j 2 ?1; 2? and j ? i , then allcomponents of yd.t ?adi ?/ occur in between s1; s2.
Throughout this article, whenever we consider a
dependency tree t we always implicitly assume that
t is over w, that t has block-degree at most 2, and
that t is well-nested. Let tai be such a tree, with
bd.ai ; tai / D 2. We call the portion of w in between
the two substrings of yd.tai / the gap of tai , denoted
by gap.tai /.
ah ad4ad3ad2ad1
m.tah/
Figure 2: Example of a node ah with block-degree 2 in a
non-projective, well-nested dependency tree tah . Integerm.tah/, defined in ?3.2, is also marked.
Example 1 Figure 2 schematically depicts a well-
nested tree tah with block-degree 2; we have marked
the root node ah and its dependent nodes adi . Foreach node adi , a shaded area highlights t ?adi ?. Wehave bd.ah; tah/ D bd.ad1 ; tah/ D bd.ad4 ; tah/ D
2 and bd.ad2 ; tah/ D bd.ad3 ; tah/ D 1. 
3.2 The Head-Split Property
We say that a dependency tree t has the head-split
property if it satisfies the following condition. Let
ah ! ad be any dependency in t with bd.ah; t / D
bd.ad ; t / D 2. Whenever gap.t ?ad ?/ contains ah, it
must also contain gap.t ?ah?/. Intuitively, this means
that if yd.t ?ad ?/ ?crosses over? the lexical token ah in
w, then yd.t ?ad ?/ must also ?cross over? gap.t ?ah?/.
Example 2 Dependency ah ! ad1 in Figure 3 viol-
ates the head-split condition, since yd.t ?ad1 ?/ crosses
over the lexical token ah inw, but does not cross over
gap.t ?ah?/. The remaining outgoing dependencies of
ah trivially satisfy the head-split condition, since the
child nodes have block-degree 1. 
Let tah be a dependency tree satisfying the head-
split property and with bd.ah; tah/ D 2. We specify
below a construction that ?splits? tah with respect to
the position of the head ah in yd.tah/, resulting in
two dependency trees sharing the root ah and having
all of the remaining nodes forming two disjoint sets.
Furthermore, the resulting trees have block-degree at
most 2.
ahad1 ad2 ad3
Figure 3: Arc ah ! ad1 violates the head-split condition.
269
(a)
ah ad4ad3
(b)
ahad2ad1 m.tah/
Figure 4: Lower tree (a) and upper tree (b) fragments for
the dependency tree in Figure 2.
Let yd.tah/ D hwi;j ; wp;qi and assume that ah
is placed within wi;j . (A symmetric construction
should be used in case ah is placed withinwp;q .) The
mirror image of ah with respect to gap.tah/, written
m.tah/, is the largest integer in ?p; q? such that there
are no dependencies linking nodes in wi;h 1 and
nodes in wp;m.tah / and there are no dependencieslinking nodes in wh;j and nodes in wm.tah /;q . It isnot hard to see that such an integer always exists,
since tah is well-nested.
We classify every dependent ad of ah as being
an ?upper? dependent or a ?lower? dependent of
ah, according to the following conditions: (i) If
d 2 ?i; h   1? [ ?m.tah/C 1; q?, then ad is an upper
dependent of ah. (ii) If d 2 ?hC 1; j ? [ ?p;m.tah/?,
then ad is a lower dependent of ah.
The upper tree of tah is the dependency tree
rooted in ah and composed of all dependencies
ah ! ad in tah with ad an upper dependent of
ah, along with all subtrees tah ?ad ? rooted in those
dependents. Similarly, the lower tree of tah is the
dependency tree rooted in ah and composed of all
dependencies ah ! ad in tah with ad a lower de-
pendent of ah, along with all subtrees tah ?ad ? rooted
in those dependents. As a general convention, in this
article we write tU;ah and tL;ah to denote the upper
and the lower trees of tah , respectively. Note that, in
some degenerate cases, the set of lower or upper de-
pendents may be empty; then tU;ah or tL;ah consists
of the root node ah only.
Example 3 Consider the tree tah displayed in Fig-
ure 2. Integer m.tah/ denotes the boundary between
the right component of yd.tah ?ad4 ?/ and the right
component of yd.tah ?ad1 ?/. Nodes ad3 and ad4 are
lower dependents, and nodes ad1 and ad2 are upper
dependents. Trees tL;ah and tU;ah are displayed in
Figure 4 (a) and (b), respectively. 
The importance of the head-split property can be
informally explained as follows. Let ah ! ad be a
dependency in tah . When we take apart the upper and
the lower trees of tah , the entire subtree tah ?ad ? ends
up in either of these two fragments. This allows us to
represent upper and lower fragments for some head
independently of the other, and to freely recombine
them. More formally, our algorithms will make use
of the following three properties, stated here without
any formal proof:
P1 Trees tU;ah and tL;ah are well-nested, have block-
degree  2, and satisfy the head-split property.
P2 Trees tU;ah and tL;ah have their head ah always
placed at one of the boundaries in their yields.
P3 Let t 0U;ah and t 00L;ah be the upper and lower treesof distinct trees t 0ah and t 00ah , respectively. If m.t 0ah/ Dm.t 00ah/, then there exists a tree tah such that tU;ah D
t 0U;ah and tL;ah D t 00L;ah .
4 Parsing Items
Let w D a1    an, n  1, be the input string. We
need to compactly represent trees that span substrings
of w by recording only the information that is needed
to combine these trees into larger trees during the
parsing process. We do this by associating each
tree with a signature, called item, which is a tuple
?i; j ; p; q; h?X , where h 2 ?1; n? identifies the token
ah, i; j with 0  i  j  n identify a substringwi;j ,
and p; q with j < p  q  n identify a substring
wp;q . We also use the special setting p D q D  .
The intended meaning is that each item repres-
ents some tree tah . If p; q ?   then yd.tah/ D
hwi;j ; wp;qi. If p; q D   then
yd.tah/ D
8
<?
:?
hwi;j i if h 2 ?i C 1; j ?
hwh;h; wi;j i if h < i
hwi;j ; wh;hi if h > j C 1
The two cases h < i and h > j C 1 above will
be used when the root node ah of tah has not yet
collected all of its dependents.
Note that h 2 fi; j C 1g is not used in the
definition of item. This is meant to avoid differ-
ent items representing the same dependency tree,
270
which is undesired for the specification of our al-
gorithm. As an example, items ?i; j ; ; ; i C 1?X
and ?i C 1; j ; ; ; i C 1?X both represent a depend-
ency tree taiC1 with yd.taiC1/ D hwi;j i. This and
other similar cases are avoided by the ban against
h 2 fi; j C 1g, which amounts to imposing some
normal form for items. In our example, only item
?i; j ; ; ; i C 1?X is a valid signature.
Finally, we distinguish among several item types,
indicated by the value of subscript X . These types
are specific to each parsing algorithm, and will be
defined in later sections.
5 Parsing of Head-Split Trees
We present in this section our first tabular algorithm
for computing the set of all dependency trees for an
input sentence w that have the head-split property,
under the arc-factored model. Recall that tai denotes
a tree with root ai , and tL;ai and tU;ai are the lower
and upper trees of tai . The steps of the algorithm
are specified by means of deduction rules over items,
following the approach of Shieber et al (1995).
5.1 Basic Idea
Our algorithm builds trees step by step, by attaching
a tree tah0 as a dependent of a tree tah and creatingthe new dependency ah ! ah0 . Computationally,
the worst case for this operation is when both tah
and tah0 have a gap; then, for each tree we need tokeep a record of the four boundaries, along with the
position of the head, as done by Go?mez-Rodr??guez et
al. (2011). However, if we are interested in parsing
trees that satisfy the head-split property, we can avoid
representing a tree with a gap by means of a single
item. We instead follow the general idea of ?2 for
projective parsing, and use different items for the
upper and the lower trees of the source tree.
When we need to attach tah0 as an upper dependentof tah , defined as in ?3.2, we perform two consecutive
steps. First, we attach tL;ah0 to tU;ah , resulting in anew intermediate tree t1. As a second step, we attach
tU;ah0 to t1, resulting in a new tree t2 which is tU;ahwith tah0 attached as an upper dependent, as desired.Both steps are depicted in Figure 5; here we introduce
the convention of indicating tree grouping through
a dashed line. A symmetric procedure can be used
to attach tah0 as a lower dependent to tL;ah . The
ah
tU;ah
ah0
tL;ah0
+
t1
(a)
t1
ah0 ah
ah0
tU;ah0
+
t2
(b)
Figure 5: Two step attachment of tah0 at tU;ah : (a) attach-ment of tL;ah0 ; (b) attachment of tU;ah0 .
correctness of the two step approach follows from
properties P1 and P3 in ?3.2.
By property P2 in ?3.2, in both steps above the
lexical heads ah and ah0 can be read from the bound-
aries of the involved trees. Then these steps can be
implemented more efficiently than the na??ve method
of attaching tah0 to tah in a single step. A more de-tailed computational analysis will be provided in ?5.7.
To simplify the presentation, we restrict the use of
head splitting to trees with a gap and parse trees with
no gap with the na??ve method; this does not affect
the computational complexity.
5.2 Item Types
We distinguish five different types of items, indicated
by the subscriptX 2 f0;L;U; =L; =U g, as described
in what follows.
 If X D 0, we have p D q D   and yd.ah/ is
specified as in ?4.
 If X D L, we use the item to represent some
lower tree. We have therefore p; q ?   and
h 2 fi C 1; qg.
 If X D U , we use the item to represent some
upper tree. We have therefore p; q ?   and
h 2 fj; p C 1g.
 If X D =L or X D =U , we use the item to
represent some intermediate step in the parsing
process, in which only the lower or upper tree of
some dependent has been collected by the head
ah, and we are still missing the upper (=U ) or
the lower (=L) tree.
271
We further specialize symbol =U by writing =U<
(=U>) to indicate that the missing upper tree should
have its head to the left (right) of its gap. We also use
=L< and =L> with a similar meaning.
5.3 Item Normal Form
It could happen that our algorithm produces items of
type 0 that do not satisfy the normal form condition
discussed in ?4. To avoid this problem, we assume
that every item of type 0 that is produced by the
algorithm is converted into an equivalent normal form
item, by means of the following rules:
?i; j ; ; ; i ?0
?i   1; j ; ; ; i ?0 (1)
?i; j ; ; ; j C 1?0
?i; j C 1; ; ; j C 1?0 (2)
5.4 Items of Type 0
We start with deduction rules that produce items of
type 0. As already mentioned, we do not apply the
head splitting technique in this case.
The next rule creates trees with a single node, rep-
resenting the head, and no dependents. The rule is
actually an axiom (there is no antecedent) and the
statement i 2 ?1; n? is a side condition.
?i   1; i ; ; ; i ?0
?
i 2 ?1; n? (3)
The next rule takes a tree headed in ah0 and makes
it a dependent of a new head ah. This rule imple-
ments what has been called the ?hook trick?. The first
side condition enforces that the tree headed in ah0
has collected all of its dependents, as discussed in ?4.
The second side condition enforces that no cycle is
created. We also write ah ! ah0 to indicate that a
new dependency is created in the parse forest.
?i; j ; ; ; h0?0
?i; j ; ; ; h?0
8
<
:
h0 2 ?i C 1; j ?
h 62 ?i C 1; j ?
ah ! ah0
(4)
The next two rules combine gap-free dependents
of the same head ah.
?i; k; ; ; h?0 ?k; j ; ; ; h?0
?i; j ; ; ; h?0 (5)
?i; h; ; ; h?0 ?h   1; j ; ; ; h?0
?i; j ; ; ; h?0 (6)
We need the special case in (6) to deal with the con-
catenation of two items that share the head ah at the
concatenation point. Observe the apparent mismatch
in step (6) between index h in the first antecedent
and index h   1 in the second antecedent. This is
because in our normal form, both the first and the
second antecedent have already incorporated a copy
of the shared head ah.
The next two rules collect a dependent of ah that
wraps around the dependents that have already been
collected. As already discussed, this operation is
performed by two successive steps: We first collect
the lower tree and then the upper tree. We present
the case in which the shared head of the two trees is
placed at the left of the gap. The case in which the
head is placed at the right of the gap is symmetric.
?i 0; j 0; ; ; h?0
?i; i 0; j 0; j ; i C 1?L
?i; j ; ; ; h?=U<

h 62 ?i C 1; i 0?
[ ?j 0 C 1; j ?
(7)
?i 0; j 0; ; ; h?=U<
?i; i 0 C 1; j 0; j ; i 0 C 1?U
?i; j ; ; ; h?0
8
<
:
h 62 ?i C 1; i 0 C 1?
[ ?j 0 C 1; j ?
ah ! ai 0C1
(8)
Again, there is an overlap in rule (8) between the
two antecedents, due to the fact that both items have
already incorporated copies of the same head.
5.5 Items of Type U
We now consider the deduction rules that are needed
to process upper trees. Throughout this subsection
we assume that the head of the upper tree is placed at
the left of the gap. The other case is symmetric. The
next rule creates an upper tree with a single node, rep-
resenting its head, and no dependents. We construct
an item for all possible right gap boundaries j .
?i   1; i ; j; j ; i ?U

i 2 ?1; n?
j 2 ?i C 1; n?
(9)
The next rule adds to an upper tree a group of new
dependents that do not have any gap. We present the
case in which the new dependents are placed at the
left of the gap of the upper tree.
?i; i 0; ; ; j ?0 ?i 0; j ; p; q; j ?U
?i; j ; p; q; j ?U (10)
272
The next two rules collect a new dependent that
wraps around the upper tree. Again, this operation is
performed by two successive steps: We first collect
the lower tree, then the upper tree. We present the
case in which the shared head of the two trees is
placed at the left of the gap.
?i 0; j ; p; q0; j ?U ?i; i 0; q0; q; i C 1?L
?i; j ; p; q; j ?=U< (11)
?i 0; j ; p; q0; j ?=U<
?i; i 0 C 1; q0; q; i 0 C 1?U
?i; j ; p; q; j ?U
?
aj ! ai 0C1 (12)
5.6 Items of Type L
So far we have always expanded items (type 0 or U )
at their external boundaries. When dealing with lower
trees, we have to reverse this strategy and expand
items (type L) at their internal boundaries. Apart
from this difference, the deduction rules below are
entirely symmetric to those in ?5.5. Again, we as-
sume that the head of the lower tree is placed at
the left of the gap, the other case being symmetric.
Our first rule creates a lower tree with a single node,
representing its head. We blindly guess the right
boundary of the gap of such a tree.
?i   1; i ; j; j ; i ?L

i 2 ?1; n?
j 2 ?i C 1; n?
(13)
The next rule adds to a lower tree a group of new
dependents that do not have any gap. We present the
case in which the new dependents are placed at the
left of the gap of the lower tree.
?j 0; j ; ; ; i C 1?0 ?i; j 0; p; q; i C 1?L
?i; j ; p; q; i C 1?L (14)
The next two rules collect a new dependent with
a gap and embed it within the gap of our lower tree,
creating a new dependency. Again, this operation is
performed by two successive steps, and we present
the case in which the common head of the lower and
upper trees that are embedded is placed at the left of
the gap, the other case being symmetric.
?i; j 0; p0; q; i C 1?L ?j 0; j ; p; p0; j ?U
?i; j ; p; q; i C 1?=L< (15)
?i; j 0; p0; q; i C 1?=L<
?j 0   1; j ; p; p0; j 0?L
?i; j ; p; q; i C 1?L
?
aiC1 ! aj 0 (16)
ahad1 ad2 ad3 ad4 ad5
tU;ah tLL;ah tLR;ah
Figure 6: Node ah satisfies both the 1-inherit and head-
split conditions. Accordingly, tree tah can be split intothree fragments tU;ah , tLL;ah and tLR;ah .
5.7 Runtime
The algorithm runs in time O.n6/, where n is the
length of the input sentence. The worst case is due
to deduction rules that combine two items, each of
which represents trees with one gap. For instance,
rule (11) involves six free indices ranging over ?1; n?,
and thus could be instantiated O.n6/ many times. If
the head-split property does not hold, attachment of a
dependent in one step results in time O.n7/, as seen
for instance in Go?mez-Rodr??guez et al (2011).
6 Parsing of 1-Inherit Head-Split Trees
In this section we specialize the parsing algorithm
of ?5 to a new, more efficient algorithm for a restric-
ted class of trees.
6.1 1-Inherit Head-Split Trees
Pitler et al (2012) introduce a restriction on well-nes-
ted dependency trees with block-degree at most 2.
A tree t satisfies the 1-inherit property if, for every
node ah in t with bd.ah; t / D 2, there is at most
one dependency ah ! ad such that gap.t ?ad ?/
contains gap.t ?ah?/. Informally, this means that
yd.t ?ad ?/ ?crosses over? gap.t ?ah?/, and we say that
ad ?inherits? the gap of ah. In this section we in-
vestigate the parsing of head-split trees that also have
the 1-inherit property.
Example 4 Figure 6 shows a head node ah along
with dependents adi , satisfying the head-split condi-tion. Only tad1 has its yield crossing over gap.tah/.Thus ah also satisfies the 1-inherit condition. 
6.2 Basic Idea
Let tah be some tree satisfying both the head-split
property and the 1-inherit propery. Assume that the
dependent node ad which inherits the gap of tah
is placed within tU;ah . This means that, for every
273
dependency ah ! ad in tL;ah , yd.t ?ad ?/ does not
cross over gap.tL;ah/. Then we can further split
tL;ah into two trees, both with root ah. We call these
two trees the lower-left tree, written tLL;ah , and the
lower-right tree, written tLR;ah ; see again Figure 6.
The basic idea behind our algorithm is to split tah
into three dependency trees tU;ah , tLL;ah and tLR;ah ,
all sharing the same root ah. This means that tah
can be attached to an existing tree through three suc-
cessive steps, each processing one of the three trees
above. The correctness of this procedure follows
from a straightforward extension of properties P1 and
P3 from ?3.2, stating that the tree fragments tU;ah ,
tLL;ah and tLR;ah can be represented and processed
one independently of the others, and freely combined
if certain conditions are satisfied by their yields.
In case ad is placed within tL;ah , we introduce
the upper-left and the upper-right trees, written
tUL;ah and tUR;ah , and apply a similar idea.
6.3 Item Types
When processing an attachment, the order in which
the algorithm assembles the three tree fragments of
tah defined in ?6.2 is not always the same. Such an
order is chosen on the basis of where the head ah
and the dependent ad inheriting the gap are placed
within the involved trees. As a consequence, in our
algorithm we need to represent several intermediate
parsing states. Besides the item types from ?5.2, we
therefore need additional types. The specification
of these new item types is rather technical, and is
therefore delayed until we introduce the relevant de-
duction rules.
6.4 Items of Type 0
We start with the deduction rules for parsing of
trees tLL;ah and tLR;ah ; trees tUL;ah and tUR;ah can be
treated symmetrically. The yields of tLL;ah and tLR;ah
have the form specified in ?4 for the case p D q D  .
We can therefore use items of type 0 to parse these
trees, adopting a strategy similar to the one in ?5.4.
The main difference is that, when a tree tah0 with agap is attached as a dependent to the head ah, we
use three consecutive steps, each processing a single
fragment of tah0 . We assume below that tah0 can besplit into trees tU;ah0 , tLL;ah0 and tLR;ah0 , the othercase can be treated in a similar way.
We use rules (3), (4) and (5) from ?5.4. Since in
ad ah
????
1
????
2
????
3
????
4
tad tU;ad tLL;ad tLR;ad
Figure 7: Tree tU;ah is decomposed into tad and subtreescovering substrings i , i 2 ?1; 4?. Tree tad is in turndecomposed into three fragments (trees tLL;ad , tLR;ad ,and tU;ad in this example).
the trees tLL;ah and tLR;ah the head is never placed in
the middle of the yield, rule (6) is not needed now
and it can safely be discarded. Rule (7), attaching
a lower tree, needs to be replaced by two new rules,
processing a lower-left and a lower-right tree. We
assume here that the common head of these trees is
placed at the left boundary of the lower-left tree; we
leave out the symmetric case.
?i; i 0; ; ; i C 1?0
?i 0; j ; ; ; h?0
?i; j ; ; ; h?=LR<
?
h 62 ?i C 1; i 0? (17)
?j 0; j ; ; ; i C 1?0
?i; j 0; ; ; h?=LR<
?i; j ; ; ; h?=U<
?
h 62 ?j 0 C 1; j ? (18)
The first antecedent in (17) encodes a lower-left tree
with its head at the left boundary. The consequent
item has then the new type =LR<, meaning that a
lower-right tree is missing that must have its head
at the left. The first antecedent in (18) provides the
missing lower-right tree, having the same head as
the already incorporated lower-left tree. After these
rules are applied, rule (8) from ?5.4 can be applied
to the consequent item of (18). This completes the
attachment of a ?wrapping? dependent of ah, with the
incorporation of the missing upper tree and with the
construction of the new dependency.
6.5 Items of Type U
We now assume that node ad is realized within
tU;ah , so that tah can be split into trees tU;ah , tLL;ah
and tLR;ah . We provide deduction rules to parse of
tU;ah ; this is the most involved part of the algorithm.
In case ad is realized within tL;ah , tah must be
split into tL;ah , tUL;ah and tUR;ah , and a symmetrical
strategy can be applied to parse tL;ah .
274
ad ah
1 2 3 4
tU;ad
tLL;ad tLR;ad
rule (19)rule (20)
Figure 8: Decomposition of tU;ah as in Figure 7, withhighlighted application of rules (19) and (20).
We start by observing that yd.tad / splitsyd.tU;ah/ into at most four substrings i ; see Fig-
ure 7.2 Because of the well-nested property, within
the tree tU;ah each dependent of ah other than ad
has a yield that is entirely placed within one of the
i ?s substrings. This means that each substring i
can be parsed independently of the other substrings.
As a first step in the process of parsing tU;ah , we
parse each substring i . We do this following the
parsing strategy specified in ?6.4. As a second step,
we assume that each of the three fragments resulting
from the decomposition of tree tad has already beenparsed; see again Figure 7. We then ?merge? these
three fragments and the trees for segments i ?s into
a complete parse tree representing tU;ah . This is
described in detail in what follows.
We assume that ah is placed at the left of the gap
of tU;ah (the right case being symmetrical) and we
distinguish four cases, depending on the two ways in
which tad can be split, and the two side positions ofthe head ad with respect to gap.tad /.
Case 1 We assume that tad can be split into trees
tU;ad , tLL;ad , tLR;ad , and the head ad is placedat the left of gap.tad /; see again Figure 7.
Rule (19) below combines tLL;ad with a parse forsegment 2, which has its head ah placed at its right
boundary; see Figure 8 for a graphical representation
of rule (19). The result is an item of the new type HH.
This item is used to represent an intermediate tree
fragment with root of block-degree 1, where both the
left and the right boundaries are heads; a dependency
2According to our definition of m.tah/ in ?3.2, 3 is alwaysthe empty string. However, here we deal with the general formu-
lation of the problem in order to claim in ?8 that our algorithm
can be directly adapted to parse some subclasses of lexicalized
tree-adjoining grammars.
ah
ad
1 2 3 4
tU;ad
tLL;ad
tLR;ad
rule (22) rule (23)
Figure 9: Decomposition of tU;ah as in Figure 7, withhighlighted application of rules (22) and (23).
between these heads will be constructed later.
?i; i 0; ; ; i C 1?0 ?i 0; j ; ; ; j ?0
?i; j ; ; ; j ?HH (19)
Rule (20) combines tU;ad with a type 0 item rep-resenting tLR;ad ; see again Figure 8. Note that thiscombination operation expands an upper tree at one
of its internal boundaries, something that was not
possible with the rules specified in ?5.5.
?i; j ; p0; q; j ?U ?p; p0; ; ; j ?0
?i; j ; p; q; j ?U (20)
Finally, we combine the consequents of (19)
and (20), and process the dependency that was left
pending in the item of type HH.
?i; j 0; p; q; j 0?U
?j 0   1; j ; ; ; j ?HH
?i; j ; p; q; j ?U
?
aj ! aj 0 (21)
After the above steps, parsing of tU;ah can be com-
pleted by combining item ?i; j ; p; q; j ?U from (21)
with items of type 0 representing parses for the sub-
strings 1, 3 and 4.
Case 2 We assume that tad can be split into trees
tU;ad , tLL;ad , tLR;ad , and the head ad is placedat the right of gap.tad /, as depicted in Figure 9.
Rule (22) below, graphically represented in Fig-
ure 9, combines tU;ad with a type 0 item represent-ing tLL;ad . This can be viewed as the symmetricversion of rule (20) of Case 1, expanding an upper
tree at one of its internal boundaries.
?i; j 0; p; q; p C 1?U ?j 0; j ; ; ; p C 1?0
?i; j ; p; q; p C 1?U (22)
275
Arabic Czech Danish Dutch Portuguese Swedish
Number of trees 1,460 72,703 5,190 13,349 9,071 11,042
WN2 O.n7/ 1,458 99.9% 72,321 99.5% 5,175 99.7% 12,896 96.6% 8,650 95.4% 10,955 99.2%
Classes considered in this paper
WN2 + HS O.n6/ 1,457 99.8% 72,182 99.3% 5,174 99.7% 12,774 95.7% 8,648 95.3% 10,951 99.2%
WN2 + HS + 1I O.n5/ 1,457 99.8% 72,182 99.3% 5,174 99.7% 12,774 95.7% 8,648 95.3% 10,951 99.2%
Classes considered by Pitler et al (2012)
WN2 + 1I O.n6/ 1,458 99.9% 72,321 99.5% 5,175 99.7% 12,896 96.6% 8,650 95.4% 10,955 99.2%
WN2 + 0I O.n5/ 1,394 95.5% 70,695 97.2% 4,985 96.1% 12,068 90.4% 8,481 93.5% 10,787 97.7%
Projective O.n3/ 1,297 88.8% 55,872 76.8% 4,379 84.4% 8,484 63.6% 7,353 81.1% 9,963 90.2%
Table 1: Coverage of various classes of dependency trees on the training sets used in the CoNLL-X shared task (WN2 =
well-nested, block-degree  2; HS = head-split; 1I = 1-inherit; 0I = 0-inherit, ?gap-minding?)
Next, we combine the result of (22) with a parse for
substring 2. The result is an item of the new type
=LR>. This item is used to represent an intermediate
tree fragment that is missing a lower-right tree with
its head at the right. In this fragment, two heads
are left pending, and a dependency relation will be
eventually established between them.
?i; j 0; p; q; p C 1?U ?j 0; j ; ; ; j ?0
?i; j ; p; q; j ?=LR> (23)
The next rule combines the consequent item of (23)
with a tree tLR;ad having its head at the right bound-ary, and processes the dependency that was left
pending in the =LR> item.
?i; j ; p0; q; j ?=LR>
?p; p0 C 1; ; ; p0 C 1?0
?i; j ; p; q; j ?U
?
aj ! ap0C1 (24)
After the above rules, parsing of tU;ah continues by
combining the consequent item ?i; j ; p; q; j ?U from
rule (24) with items representing parses for the sub-
strings 1, 3 and 4.
Cases 3 and 4 We informally discuss the cases in
which tad can be split into trees tL;ad , tUL;ad ,
tUR;ad , for both positions of the head ad with re-spect to gap.tad /. In both cases we can adopt astrategy similar to the one of Case 2.
We first expand tL;ad externally, at the side op-posite to the head ad , with a tree fragment tUL;ador tUR;ad , similarly to rule (22) of Case 2. Thisresults in a new fragment t1. Next, we merge t1
with a parse for 2 containing the head ah, similarly
to rule (23) of Case 2. This results in a new frag-
ment t2 where a dependency relation involving the
heads ad and ah is left pending. Finally, we merge
t2 with a missing tree tUL;ad or tUR;ad , and pro-cess the pending dependency, similarly to rule (24).
One should contrast this strategy with the alternative
strategy adopted in Case 1, where the fragment of
tad having block-degree 2 cannot be merged with aparse for the segment containing the head ah (2 in
Case 1), because of an intervening fragment of tadwith block-degree 1 (tLL;ad in Case 1).Finally, if there is no node ad in tU;ah that inherits
the gap of ah, we can split tU;ah into two dependency
trees, as we have done for tL;ah in ?6.2, and parse
the two fragments using the strategy of ?6.4.
6.6 Runtime
Our algorithm runs in time O.n5/, where n is the
length of the input sentence. The reason of the im-
provement with respect to the O.n6/ result of ?5 is
that we no longer have deduction rules where both
antecedents represent trees with a gap. In the new al-
gorithm, the worst case is due to rules where only one
antecedent has a gap. This leads to rules involving a
maximum of five indices, ranging over ?1; n?. These
rules can be instantiated in O.n5/ ways.
7 Empirical Coverage
We have seen that the restriction to head-split de-
pendency trees enables us to parse these trees one
order of magnitude faster than the class of well-nes-
ted dependency trees with block-degree at most 2.
276
In connection with the 1-inherit property, this even
increases to two orders of magnitude. However, as
already stated in ?2, this improvement is paid for by
a loss in coverage; for instance, trees of the form
shown in Figure 3 cannot be parsed any longer.
7.1 Quantitative Evaluation
In order to assess the empirical loss in coverage that
the restriction to head-split trees incurs, we evaluated
the coverage of several classes of dependency trees
on standard data sets. Following Pitler et al (2012),
we report in Table 1 figures for the training sets of
six languages used in the CoNLL-X shared task on
dependency parsing (Buchholz and Marsi, 2006). As
we can see, the O.n6/ class of head-split trees has
only slightly lower coverage on this data than the
baseline class of well-nested dependency trees with
block-degree at most 2. The losses are up to 0.2
percentage points on five of the six languages, and 0.9
points on the Dutch data. Our even more restricted
O.n5/ class of 1-inherit head-split trees has the same
coverage as ourO.n6/ class, which is expected given
the results of Pitler et al (2012): Their O.n6/ class
of 1-inherit trees has exactly the same coverage as
the baseline (and thereby more coverage than our
O.n6/ class). Interestingly though, their O.n5/ class
of ?gap-minding? trees has a significantly smaller
coverage than our O.n5/ class. We conclude that
our class seems to strike a good balance between
expressiveness and parsing complexity.
7.2 Qualitative Evaluation
While the original motivation behind introducing the
head-split property was to improve parsing complex-
ity, it is interesting to also discuss the linguistic relev-
ance of this property. A first inspection of the struc-
tures that violate the head-split property revealed that
many such violations disappear if one ignores gaps
caused by punctuation. Some decisions about what
nodes should function as the heads of punctuation
symbols lead to more gaps than others. In order to
quantify the implications of this, we recomputed the
coverage of the class of head-split trees on data sets
where we first removed all punctuation. The results
are given in Table 2. We restrict ourselves to the five
native dependency treebanks used in the CoNLL-X
shared task, ignoring treebanks that have been con-
verted from phrase structure representations.
Arabic Czech Danish Slovene Turkish
with 1 139 1 2 2
without 1 46 0 0 2
Table 2: Violations against the head-split property (relative
to the class of well-nested trees with block-degree  2)
with and without punctuation.
We see that when we remove punctuation from
the sentences, the number of violations against the
head-split property at most decreases. For Danish
and Slovene, removing punctuation even has the ef-
fect that all well-nested dependency trees with block-
degree at most 2 become head-split. Overall, the
absolute numbers of violations are extremely small?
except for Czech, where we have 139 violations with
and 46 without punctuation. A closer inspection of
the Czech sentences reveals that many of these fea-
ture rather complex coordinations. Indeed, out of
the 46 violations in the punctuation-free data, only 9
remain when one ignores those with coordination.
For the remaining ones, we have not been able to
identify any clear patterns.
8 Concluding Remarks
In this article we have extended head splitting tech-
niques, originally developed for parsing of projective
dependency trees, to two subclasses of well-nested
dependency trees with block-degree at most 2. We
have improved over the asymptotic runtime of two
existing algorithms, at no significant loss in coverage.
With the same goal of improving parsing efficiency
for subclasses of non-projective trees, in very recent
work Pitler et al (2013) have proposed an O.n4/
time algorithm for a subclass of non-projective trees
that are not well-nested, using an approach that is
orthogonal to the one we have explored here.
Other than for dependency parsing, our results
have also implications for mildly context-sensitive
phrase structure formalisms. In particular, the al-
gorithm of ?5 can be adapted to parse a subclass
of lexicalized tree-adjoining grammars, improving
the result by Eisner and Satta (2000) from O.n7/ to
O.n6/. Similarly, the algorithm of ?6 can be adapted
to parse a lexicalized version of the tree-adjoining
grammars investigated by Satta and Schuler (1998),
improving a na??ve O.n7/ algorithm to O.n5/.
277
References
Manuel Bodirsky, Marco Kuhlmann, and Mathias Mo?hl.
2005. Well-nested drawings as models of syntactic
structure. In Proceedings of the 10th Conference on
Formal Grammar (FG) and Ninth Meeting on Mathem-
atics of Language (MOL), pages 195?203, Edinburgh,
UK.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computational
Natural Language Learning (CoNLL), pages 149?164,
New York, USA.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 957?961, Prague, Czech Republic.
Joan Chen-Main and Aravind K. Joshi. 2010. Unavoid-
able ill-nestedness in natural language and the adequacy
of tree local-MCTAG induced dependency structures.
In Proceedings of the Tenth International Conference
on Tree Adjoining Grammars and Related Formalisms
(TAG+), New Haven, USA.
Jason Eisner and Giorgio Satta. 1999. Efficient parsing
for bilexical context-free grammars and Head Auto-
maton Grammars. In Proceedings of the 37th Annual
Meeting of the Association for Computational Linguist-
ics (ACL), pages 457?464, College Park, MD, USA.
Jason Eisner and Giorgio Satta. 2000. A faster parsing
algorithm for lexicalized Tree-Adjoining Grammars. In
Proceedings of the Fifth Workshop on Tree Adjoining
Grammars and Related Formalisms (TAG+), pages 14?
19, Paris, France.
Jason Eisner. 1997. Bilexical grammars and a cubic-time
probabilistic parser. In Proceedings of the Fifth Inter-
national Workshop on Parsing Technologies (IWPT),
pages 54?65, Cambridge, MA, USA.
Carlos Go?mez-Rodr??guez, John Carroll, and David J. Weir.
2011. Dependency parsing schemata and mildly non-
projective dependency parsing. Computational Lin-
guistics, 37(3):541?586.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
Adjoining Grammars. In Grzegorz Rozenberg and
Arto Salomaa, editors, Handbook of Formal Languages,
volume 3, pages 69?123. Springer.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 1?11, Uppsala, Sweden.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly non-
projective dependency structures. In Proceedings of
the 21st International Conference on Computational
Linguistics (COLING) and 44th Annual Meeting of the
Association for Computational Linguistics (ACL) Main
Conference Poster Sessions, pages 507?514, Sydney,
Australia.
Wolfgang Maier and Timm Lichte. 2011. Characteriz-
ing discontinuity in constituent treebanks. In Philippe
de Groote, Markus Egg, and Laura Kallmeyer, editors,
Formal Grammar. 14th International Conference, FG
2009, Bordeaux, France, July 25?26, 2009, Revised
Selected Papers, volume 5591 of Lecture Notes in Com-
puter Science, pages 167?182. Springer.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency pars-
ing. In Proceedings of the Tenth International Confer-
ence on Parsing Technologies (IWPT), pages 121?132,
Prague, Czech Republic.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan
Hajic?. 2005. Non-projective dependency parsing using
spanning tree algorithms. In Human Language Techno-
logy Conference (HLT) and Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 523?530, Vancouver, Canada.
Emily Pitler, Sampath Kannan, and Mitchell Marcus.
2012. Dynamic programming for higher order parsing
of gap-minding trees. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Language
Processing (EMNLP) and Computational Natural Lan-
guage Learning (CoNLL), pages 478?488, Jeju Island,
Republic of Korea.
Emily Pitler, Sampath Kannan, and Mitchell Marcus.
2013. Finding optimal 1-endpoint-crossing trees.
Transactions of the Association for Computational Lin-
guistics.
Giorgio Satta and William Schuler. 1998. Restrictions on
tree adjoining languages. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics (ACL) and 17th International Conference
on Computational Linguistics (COLING), pages 1176?
1182, Montre?al, Canada.
Stuart M. Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive pars-
ing. Journal of Logic Programming, 24(1?2):3?36.
278
