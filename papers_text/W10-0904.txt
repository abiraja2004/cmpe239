Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 24?33,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Large Scale Relation Detection?
Chris Welty and James Fan and David Gondek and Andrew Schlaikjer
IBM Watson Research Center ? 19 Skyline Drive ? Hawthorne, NY 10532, USA
{welty, fanj, dgondek, ahschlai}@us.ibm.com
Abstract
We present a technique for reading sentences
and producing sets of hypothetical relations
that the sentence may be expressing. The
technique uses large amounts of instance-level
background knowledge about the relations in
order to gather statistics on the various ways
the relation may be expressed in language, and
was inspired by the observation that half of the
linguistic forms used to express relations oc-
cur very infrequently and are simply not con-
sidered by systems that use too few seed ex-
amples. Some very early experiments are pre-
sented that show promising results.
1 Introduction
We are building a system that learns to read in a new
domain by applying a novel combination of natural
language processing, machine learning, knowledge
representation and reasoning, information retrieval,
data mining, etc. techniques in an integrated way.
Central to our approach is the view that all parts of
the system should be able to interact during any level
of processing, rather than a pipeline view in which
certain parts of the system only take as input the re-
sults of other parts, and thus cannot influence those
results. In this paper we discuss a particular case
of that idea, using large knowledge bases hand in
hand with natural language processing to improve
the quality of relation detection. Ultimately we de-
fine reading as representing natural language text in
? Research supported in part by DARPA MRP Grant
FA8750-09-C0172
a way that integrates background knowledge and in-
ference, and thus are doing the relation detection
to better integrate text with pre-existing knowledge,
however that should not (and does not) prevent us
from using what knowledge we have to influence
that integration along the way.
2 Background
The most obvious points of interaction between NLP
and KR systems are named entity tagging and other
forms of type instance extraction. The second ma-
jor point of interaction is relation extraction, and
while there are many kinds of relations that may
be detected (e.g. syntactic relations such as modi-
fiers and verb subject/object, equivalence relations
like coreference or nicknames, event frame relations
such as participants, etc.), the kind of relations that
reading systems need to extract to support domain-
specific reasoning tasks are relations that are known
to be expressed in supporting knowledge-bases. We
call these relations semantic relations in this paper.
Compared to entity and type detection, extraction
of semantic relations is significantly harder. In our
work on bridging the NLP-KR gap, we have ob-
served several aspects of what makes this task dif-
ficult, which we discuss below.
2.1 Keep reading
Humans do not read and understand text by first rec-
ognizing named entities, giving them types, and then
finding a small fixed set of relations between them.
Rather, humans start with the first sentence and build
up a representation of what they read that expands
and is refined during reading. Furthermore, humans
24
do not ?populate databases? by reading; knowledge
is not only a product of reading, it is an integral part
of it. We require knowledge during reading in order
to understand what we read.
One of the central tenets of our machine reading
system is the notion that reading is not performed on
sentences in isolation. Often, problems in NLP can
be resolved by simply waiting for the next sentence,
or remembering the results from the previous, and
incorporating background or domain specific knowl-
edge. This includes parse ambiguity, coreference,
typing of named entities, etc. We call this the Keep
Reading principle.
Keep reading applies to relation extraction as
well. Most relation extraction systems are imple-
mented such that a single interpretation is forced
on a sentence, based only on features of the sen-
tence itself. In fact, this has been a shortcoming
of many NLP systems in the past. However, when
you apply the Keep Reading principle, multiple hy-
potheses from different parts of the NLP pipeline are
maintained, and decisions are deferred until there is
enough evidence to make a high confidence choice
between competing hypotheses. Knowledge, such
as those entities already known to participate in a
relation and how that relation was expressed, can
and should be part of that evidence. We will present
many examples of the principle in subsequent sec-
tions.
2.2 Expressing relations in language
Due to the flexibility and expressive power of nat-
ural language, a specific type of semantic relation
can usually be expressed in language in a myriad
of ways. In addition, semantic relations are of-
ten implied by the expression of other relations.
For example, all of the following sentences more
or less express the same relation between an actor
and a movie: (1) ?Elijah wood starred in Lord of
the Rings: The Fellowship of the Ring?, (2) ?Lord
of the Rings: The Fellowship of the Ring?s Elijah
Wood, ...?, and(3) ?Elijah Wood?s coming of age
was clearly his portrayal of the dedicated and noble
hobbit that led the eponymous fellowship from the
first episode of the Lord of the Rings trilogy.? No
human reader would have any trouble recognizing
the relation, but clearly this variability of expression
presents a major problem for machine reading sys-
tems.
To get an empirical sense of the variability of nat-
ural language used to express a relation, we stud-
ied a few semantic relations and found sentences
that expressed that relation, extracted simple pat-
terns to account for how the relation is expressed
between two arguments, mainly by removing the re-
lation arguments (e.g. ?Elijah Wood? and ?Lord of
the Rings: The Fellowship of the Ring? above) and
replacing them with variables. We then counted the
number of times each pattern was used to express
the relation, producing a recognizable very long tail
shown in Figure 1 for the top 50 patterns expressing
the acted-in-movie relation in 17k sentences. More
sophisticated pattern generalization (as discussed in
later sections) would significantly fatten the head,
bringing it closer to the traditional 50% of the area
under the curve, but no amount of generalization
will eliminate the tail. The patterns become increas-
ingly esoteric, such as ?The movie Death Becomes
Her features a brief sequence in which Bruce Willis
and Goldie Hawn?s characters plan Meryl Streep?s
character?s death by sending her car off of a cliff
on Mulholland Drive,? or ?The best known Hawk-
sian woman is probably Lauren Bacall, who iconi-
cally played the type opposite Humphrey Bogart in
To Have and Have Not and The Big Sleep.?
2.3 What relations matter
We do not consider relation extraction to be an end
in and of itself, but rather as a component in larger
systems that perform some task requiring interoper-
ation between language- and knowledge-based com-
ponents. Such larger tasks can include question
answering, medical diagnosis, intelligence analysis,
museum curation, etc. These tasks have evaluation
criteria that go beyond measuring relation extraction
results. The first step in applying relation detection
to these larger tasks is analysis to determine what
relations matter for the task and domain.
There are a number of manual and semi-automatic
ways to perform such analysis. Repeating the
theme of this paper, which is to use pre-existing
knowledge-bases as resources, we performed this
analysis using freebase and a set of 20k question-
answer pairs representing our task domain. For each
question, we formed tuples of each entity name in
the question (QNE) with the answer, and found all
25
 0
100
200
300
400
500
600
700
800
900
1000
Figure 1: Pattern frequency for acted-in-movie relation for 17k sentences.
 
0
10
20
30
40
50
60
70
80
Figure 2: Relative frequency for top 50 relations in 20K question-answer pairs.
the relations in the KB connecting the entities. We
kept a count for each relation of how often it con-
nected a QNE to an answer. Of course we don?t ac-
tually know for sure that the relation is the one being
asked, but the intuition is that if the amount of data
is big enough, you will have at least a ranked list of
which relations are the most frequent.
Figure 2 shows the ranking for the top 50 rela-
tions. Note that, even when restricted to the top 50
relations, the graph has no head, it is basically all
tail; The top 50 relations cover about 15% of the do-
main. In smaller, manual attempts to determine the
most frequent relations in our domain, we had a sim-
ilar result. What this means is that supporting even
the top 50 relations with perfect recall covers about
15% of the questions. It is possible, of course, to
narrow the domain and restrict the relations that can
be queried?this is what database systems do. For
reading, however, the results are the same. A read-
ing system requires the ability to recognize hundreds
of relations to have any significant impact on under-
standing.
2.4 Multi-relation learning on many seeds
The results shown in Figure 1 and Figure 2 con-
firmed much of the analysis and experiences we?d
had in the past trying to apply relation extraction in
the traditional way to natural language problems like
26
question answering, building concept graphs from
intelligence reports, semantic search, etc. Either by
training machine learning algorithms on manually
annotated data or by manually crafting finite-state
transducers, relation detection is faced by this two-
fold problem: the per-relation extraction hits a wall
around 50% recall, and each relation itself occurs
infrequently in the data.
This apparent futility of relation extraction led us
to rethink our approach. First of all, the very long
tail for relation patterns led us to consider how to
pick up the tail. We concluded that to do so would
require many more examples of the relation, but
where can we get them? In the world of linked-data,
huge instance-centered knowledge-bases are rapidly
growing and spreading on the semantic web1. Re-
sources like DBPedia, Freebase, IMDB, Geonames,
the Gene Ontology, etc., are making available RDF-
based data about a number of domains. These
sources of structured knowledge can provide a large
number of seed tuples for many different relations.
This is discussed further below.
Furthermore, the all-tail nature of relation cover-
age led us to consider performing relation extraction
on multiple relations at once. Some promising re-
sults on multi-relation learning have already been re-
ported in (Carlson et al, 2009), and the data sources
mentioned above give us many more than just the
handful of seed instances used in those experiments.
The idea of learning multiple relations at once also
fits with our keep reading principle - multiple rela-
tion hypotheses may be annotated between the same
arguments, with further evidence helping to disam-
biguate them.
3 Approach
One common approach to relation extraction is to
start with seed tuples and find sentences that con-
tain mentions of both elements of the tuple. From
each such sentence a pattern is generated using at
minimum universal generalization (replace the tuple
elements with variables), though adding any form of
generalization here can significantly improve recall.
Finally, evaluate the patterns by applying them to
text and evaluating the precision and recall of the tu-
ples extracted by the patterns. Our approach, called
1http://linkeddata.org/
Large Scale Relation Detection (LSRD), differs in
three important ways:
1. We start with a knowledge-base containing a
large number (thousands to millions) of tuples
encoding relation instances of various types.
Our hypothesis is that only a large number of
examples can possibly account for the long tail.
2. We do not learn one relation at a time, but
rather, associate a pattern with a set of relations
whose tuples appear in that pattern. Thus, when
a pattern is matched to a sentence during read-
ing, each relation in its set of associated rela-
tions is posited as a hypothetical interpretation
of the sentence, to be supported or refuted by
further reading.
3. We use the knowledge-base as an oracle to de-
termine negative examples of a relation. As
a result the technique is semi-supervised; it
requires no human intervention but does re-
quire reliable knowledge-bases as input?these
knowledge-bases are readily available today.
Many relation extraction techniques depend on a
prior step of named entity recognition (NER) and
typing, in order to identify potential arguments.
However, this limits recall to the recall of the NER
step. In our approach patterns can match on any
noun phrase, and typing of these NPs is simply an-
other form of evidence.
All this means our approach is not relation extrac-
tion per se, it typically does not make conclusions
about a relation in a sentence, but extracts hypothe-
ses to be resolved by other parts of our reading sys-
tem.
In the following sections, we elaborate on the
technique and some details of the current implemen-
tation.
3.1 Basic pipeline
The two principle inputs are a corpus and a
knowledge-base (KB). For the experiments below,
we used the English Gigaword corpus2 extended
with Wikipedia and other news sources, and IMDB,
DBPedia, and Freebase KBs, as shown. The intent is
2http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2003T05
27
to run against a web-scale corpus and larger linked-
data sets.
Input documents are sentence delimited, tok-
enized and parsed. The technique can benefit dra-
matically from coreference resolution, however in
the experiments shown, this was not present. For
each pair of proper names in a sentence, the names
are looked up in the KB, and if they are related,
a pattern is extracted from the sentence. At min-
imum, pattern extraction should replace the names
with variables. Depending on how patterns are ex-
tracted, one pattern may be extracted per sentence,
or one pattern may be extracted per pair of proper
names in the sentence. Each pattern is associated
with all the relations known in the KB between the
two proper names. If the pattern has been extracted
before, the two are merged by incrementing the as-
sociated relation counts. This phase, called pattern
induction, is repeated for the entire corpus, resulting
in a large set of patterns, each pattern associated with
relations. For each ?pattern, relation? pair, there is a
count of the number of times that pattern appeared
in the corpus with names that are in the relation ac-
cording to the KB.
The pattern induction phase results in positive
counts, i.e. the number of times a pattern appeared
in the corpus with named entities known to be re-
lated in the KB. However, the induction phase does
not exhaustively count the number of times each pat-
tern appears in the corpus, as a pattern may appear
with entities that are not known in the KB, or are not
known to be related. The second phase, called pat-
tern training, goes through the entire corpus again,
trying to match induced patterns to sentences, bind-
ing any noun phrase to the pattern variables. Some
attempt is made to resolve the noun phrase to some-
thing (most obviously, a name) that can be looked
up in the KB, and for each relation associated with
the pattern, if the two names are not in the relation
according to the KB, the negative count for that re-
lation in the matched pattern is incremented. The
result of the pattern training phase is an updated set
of ?pattern, relation? pairs with negative counts.
The following example illustrates the basic pro-
cessing. During induction, this sentence is encoun-
tered:
Tom Cruise and co-star Nicole Kidman
appeared together at the premier.
The proper names ?Tom Cruise? and ?Nicole Kid-
man? are recognized and looked up in the KB. We
find instances in the KB with those names, and the
following relations: coStar(Tom Cruise,
Nicole Kidman); marriedTo(Tom
Cruise, Nicole Kidman). We extract a
pattern p1: ?x and co-star ?y appeared
together at the premier in which all the
names have been replace by variables, and the
associations <p1, costar, 1, 0> and <p1,
marriedTo, 1, 0> with positive counts and
zero negative counts. Over the entire corpus, we?d
expect the pattern to appear a few times and end
up with final positive counts like <p1, coStar,
14, 0> and <p1, marriedTo, 2, 0>, in-
dicating the pattern p1 appeared 14 times in the
corpus between names known to participate in the
coStar relation, and twice between names known
to participate in the marriedTo relation. During
training, the following sentence is encountered that
matches p1:
Tom Hanks and co-star Daryl Hannah ap-
peared together at the premier.
The names ?Tom Hanks? and ?Daryl Hannah?
are looked up in the KB and in this case only
the relation coStar is found between them, so the
marriedTo association is updated with a negative
count: <p1, marriedTo, 2, -1>. Over the
entire corpus, we?d expect the counts to be some-
thing like <p1, costar, 14, -6> and <p1,
marriedTo, 2, -18>.
This is a very simple example and it is difficult to
see the value of the pattern training phase, as it may
appear the negative counts could be collected during
the induction phase. There are several reasons why
this is not so. First of all, since the first phase only
induces patterns between proper names that appear
and are related within the KB, a sentence in the cor-
pus matching the pattern would be missed if it did
not meet that criteria but was encountered before the
pattern was induced. Secondly, for reasons that are
beyond the scope of this paper, having to do with
our Keep Reading principle, the second phase does
slightly more general matching: note that it matches
noun phrases instead of proper nouns.
28
3.2 Candidate-instance matching
An obvious part of the process in both phases is
taking strings from text and matching them against
names or labels in the KB. We refer to the strings in
the sentences as candidate arguments or simply can-
didates, and refer to instances in the KB as entities
with associated attributes. For simplicity of discus-
sion we will assume all KBs are in RDF, and thus
all KB instances are nodes in a graph with unique
identifiers (URIs) and arcs connecting them to other
instances or primitive values (strings, numbers, etc.).
A set of specially designated arcs, called labels, con-
nect instances to strings that are understood to name
the instances. The reverse lookup of entity identi-
fiers via names referred to in the previous section
requires searching for the labels that match a string
found in a sentence and returning the instance iden-
tifier.
This step is so obvious it belies the difficultly of
the matching process and is often overlooked, how-
ever in our experiments we have found candidate-
instance matching to be a significant source of error.
Problems include having many instances with the
same or lexically similar names, slight variations in
spelling especially with non-English names, inflex-
ibility or inefficiency in string matching in KB im-
plementations, etc. In some of our sources, names
are also encoded as URLs. In the case of movie
and book titles-two of the domains we experimented
with-the titles seem almost as if they were designed
specifically to befuddle attempts to automatically
recognize them. Just about every English word is a
book or movie title, including ?It?, ?Them?, ?And?,
etc., many years are titles, and just about every num-
ber under 1000. Longer titles are difficult as well,
since simple lexical variations can prevent matching
from succeeding, e.g. the Shakespeare play, A Mid-
summer Night?s Dream appears often as Midsummer
Night?s Dream, A Midsummer Night Dream, and oc-
casionally, in context, just Dream. When titles are
not distinguished or delimited somehow, they can
confuse parsing which may fail to recognize them as
noun phrases. We eventually had to build dictionar-
ies of multi-word titles to help parsing, but of course
that was imperfect as well.
The problems go beyond the analogous ones in
coreference resolution as the sources and technology
themselves are different. The problems are severe
enough that the candidate-instance matching prob-
lem contributes the most, of all components in this
pipeline, to precision and recall failures. We have
observed recall drops of as much as 15% and preci-
sion drops of 10% due to candidate-instance match-
ing.
This problem has been studied somewhat in the
literature, especially in the area of database record
matching and coreference resolution (Michelson and
Knoblock, 2007), but the experiments presented be-
low use rudimentary solutions and would benefit
significantly from improvements; it is important to
acknowledge that the problem exists and is not as
trivial as it appears at first glance.
3.3 Pattern representation
The basic approach accommodates any pattern rep-
resentation, and in fact we can accommodate non
pattern-based learning approaches, such as CRFs, as
the primary hypothesis is principally concerned with
the number of seed examples (scaling up initial set
of examples is important). Thus far we have only
experimented with two pattern representations: sim-
ple lexical patterns in which the known arguments
are replaced in the sentence by variables (as shown
in the example above), and patterns based on the
spanning tree between the two arguments in a de-
pendency parse, again with the known arguments re-
placed by variables. In our initial design we down-
played the importance of the pattern representation
and especially generalization, with the belief that
very large scale would remove the need to general-
ize. However, our initial experiments suggest that
good pattern generalization would have a signifi-
cant impact on recall, without negative impact on
precision, which agrees with findings in the litera-
ture (Pantel and Pennacchiotti, 2006). Thus, these
early results only employ rudimentary pattern gen-
eralization techniques, though this is an area we in-
tend to improve. We discuss some more details of
the lack of generalization below.
4 Experiment
In this section we present a set of very early proof of
concept experiments performed using drastic simpli-
fications of the LSRD design. We began, in fact, by
29
Relation Prec Rec F1 Tuples Seeds
imdb:actedIn 46.3 45.8 0.46 9M 30K
frb:authorOf 23.4 27.5 0.25 2M 2M
imdb:directorOf 22.8 22.4 0.22 700K 700K
frb:parentOf 68.2 8.6 0.16 10K 10K
Table 1: Precision and recall vs. number of tuples used
for 4 freebase relations.
using single-relation experiments, despite the cen-
trality of multiple hypotheses to our reading system,
in order to facilitate evaluation and understanding of
the technique. Our main focus was to gather data
to support (or refute) the hypothesis that more re-
lation examples would matter during pattern induc-
tion, and that using the KB as an oracle for training
would work. Clearly, no KB is complete to begin
with, and candidate-instance matching errors drop
apparent coverage further, so we intended to explore
the degree to which the KB?s coverage of the relation
impacted performance. To accomplish this, we ex-
amined four relations with different coverage char-
acteristics in the KB.
4.1 Setup and results
The first relation we tried was the acted-in-show
relation from IMDB; for convenience we refer to
it as imdb:actedIn. An IMDB show is a movie,
TV episode, or series. This relation has over 9M
<actor, show> tuples, and its coverage was
complete as far as we were able to determine. How-
ever, the version we used did not have a lot of name
variations for actors. The second relation was the
author-of relation from Freebase (frb:authorOf ),
with roughly 2M <author, written-work>
tuples. The third relation was the director-of-
movie relation from IMDB (imdb:directorOf ), with
700k <director,movie> tuples. The fourth
relation was the parent-of relation from Free-
base (frb:parentOf ), with roughly 10K <parent,
child> tuples (mostly biblical and entertainment).
Results are shown in Table 1.
The imdb:actedIn experiment was performed on
the first version of the system that ran on 1 CPU and,
due to resource constraints, was not able to use more
than 30K seed tuples for the rule induction phase.
However, the full KB (9M relation instances) was
available for the training phase. With some man-
ual effort, we selected tuples (actor-movie pairs) of
popular actors and movies that we expected to ap-
pear most frequently in the corpus. In the other ex-
periments, the full tuple set was available for both
phases, but 2M tuples was the limit for the size of
the KB in the implementation. With these promising
preliminary results, we expect a full implementation
to accommodate up to 1B tuples or more.
The evaluation was performed in decreasing de-
grees of rigor. The imdb:actedIn experiment was run
against 20K sentences with roughly 1000 actor in
movie relations and checked by hand. For the other
three, the same sentences were used, but the ground
truth was generated in a semi-automatic way by re-
using the LSRD assumption that a sentence con-
taining tuples in the relation expresses the relation,
and then spot-checked manually. Thus the evalua-
tion for these three experiments favors the LSRD ap-
proach, though spot checking revealed it is the pre-
cision and not the recall that benefits most from this,
and all the recall problems in the ground truth (i.e.
sentences that did express the relation but were not
in the ground truth) were due to candidate-instance
matching problems. An additional idiosyncrasy in
the evaluation is that the sentences in the ground
truth were actually questions, in which one of the
arguments to the relation was the answer. Since
the patterns were induced and trained on statements,
there is a mismatch in style which also significantly
impacts recall. Thus the precision and recall num-
bers should not be taken as general performance, but
are useful only relative to each other.
4.2 Discussion
The results are promising, and we are continuing the
work with a scalable implementation. Overall, the
results seem to show a clear correlation between the
number of seed tuples and relation extraction recall.
However, the results do not as clearly support the
many examples hypothesis as it may seem. When
an actor and a film that actor starred in are men-
tioned in a sentence, it is very often the case that the
sentence expresses that relation. However, this was
less likely in the case of the parent-of relation, and
as we considered other relations, we found a wide
degree of variation. The borders relation between
two countries, for example, is on the other extreme
from actor-in-movie. Bordering nations often wage
30
war, trade, suspend relations, deport refugees, sup-
port, oppose, etc. each other, so finding the two na-
tions in a sentence together is not highly indicative
of one relation or another. The director-of-movie re-
lation was closer to acted-in-movie in this regard,
and author-of a bit below that. The obvious next step
to gather more data on the many examples hypoth-
esis is to run the experiments with one relation, in-
creasing the number of tuples with each experiment
and observing the change in precision and recall.
The recall results do not seem particularly strik-
ing, though these experiments do not include pat-
tern generalization (other than what a dependency
parse provides) or coreference, use a small corpus,
and poor candidate-instance matching. Further, as
noted above there were other idiosyncrasies in the
evaluation that make them only useful for relative
comparison, not as general results.
Many of the patterns induced, especially for
the acted-in-movie relation, were highly lexical,
using e.g. parenthesis or other punctuation to
signal the relation. For example, a common
pattern was actor-name (movie-name), or
movie-name: actor-name, e.g. ?Leonardo
DiCaprio (Titanic) was considering accepting the
role as Anakin Skywalker,? or ?Titanic: Leonardo
DiCaprio and Kate Blanchett steam up the silver
screen against the backdrop of the infamous disas-
ter.? Clearly patterns like this rely heavily on the
context and typing to work. In general the pattern
?x (?y) is not reliable for the actor-in-movie re-
lation unless you know ?x is an actor and ?y is a
movie. However, some patterns, like ?x appears
in the screen epic ?y is highly indicative
of the relation without the types at all - in fact it is
so high precision it could be used to infer the types
of ?x and ?y if they were not known. This seems
to fit extremely well in our larger reading system,
in which the pattern itself provides one form of evi-
dence to be combined with others, but was not a part
of our evaluation.
One of the most important things to general-
ize in the patterns we observed was dates. If
patterns like, actor-name appears in the
1994 screen epic movie-name could have
been generalized to actor-name appears in
the date screen epic movie-name, re-
call would have been boosted significantly. As it
stood in these experiments, everything but the argu-
ments had to match. Similarly, many relations often
appear in lists, and our patterns were not able to gen-
eralize that away. For example the sentence, ?Mark
Hamill appeared in Star Wars, Star Wars: The Em-
pire Strikes Back, and Star Wars: The Return of the
Jedi,? causes three patterns to be induced; in each,
one of the movies is replaced by a variable in the
pattern and the other two are required to be present.
Then of course all this needs to be combined, so that
the sentence, ?Indiana Jones and the Last Crusade is
a 1989 adventure film directed by Steven Spielberg
and starring Harrison Ford, Sean Connery, Denholm
Elliott and Julian Glover,? would generate a pattern
that would get the right arguments out of ?Titanic
is a 1997 epic film directed by James Cameron and
starring Leonardo DiCaprio, Kate Winslett, Kathy
Bates and Bill Paxon.? At the moment the former
sentence generates four patterns that require the di-
rector and dates to be exactly the same.
Some articles in the corpus were biographies
which were rich with relation content but also with
pervasive anaphora, name abbreviations, and other
coreference manifestations that severely hampered
induction and evaluation.
5 Related work
Early work in semi-supervised learning techniques
such as co-training and multi-view learning (Blum
and Mitchell, 1998) laid much of the ground work
for subsequent experiments in bootstrapped learn-
ing for various NLP tasks, including named entity
detection (Craven et al, 2000; Etzioni et al, 2005)
and document classification (Nigam et al, 2006).
This work?s pattern induction technique also repre-
sents a semi-supervised approach, here applied to
relation learning, and at face value is similar in mo-
tivation to many of the other reported experiments
in large scale relation learning (Banko and Etzioni,
2008; Yates and Etzioni, 2009; Carlson et al, 2009;
Carlson et al, 2010). However, previous techniques
generally rely on a small set of example relation in-
stances and/or patterns, whereas here we explicitly
require a larger source of relation instances for pat-
tern induction and training. This allows us to better
evaluate the precision of all learned patterns across
multiple relation types, as well as improve coverage
31
of the pattern space for any given relation.
Another fundamental aspect of our approach lies
in the fact that we attempt to learn many relations
simultaneously. Previously, (Whitelaw et al, 2008)
found that such a joint learning approach was use-
ful for large-scale named entity detection, and we
expect to see this result carry over to the relation ex-
traction task. (Carlson et al, 2010) also describes
relation learning in a multi-task learning framework,
and attempts to optimize various constraints posited
across all relation classes.
Examples of the use of negative evidence
for learning the strength of associations between
learned patterns and relation classes as proposed
here has not been reported in prior work to our
knowledge. A number of multi-class learning tech-
niques require negative examples in order to prop-
erly learn discriminative features of positive class
instances. To address this requirement, a number of
approaches have been suggested in the literature for
selection or generation of negative class instances.
For example, sampling from the positive instances
of other classes, randomly perturbing known pos-
itive instances, or breaking known semantic con-
straints of the positive class (e.g. positing multiple
state capitols for the same state). With this work,
we treat our existing RDF store as an oracle, and as-
sume it is sufficiently comprehensive that it allows
estimation of negative evidence for all target relation
classes simultaneously.
The first (induction) phase of LSRD is very simi-
lar to PORE (Wang et al, 2007) (Dolby et al, 2009;
Gabrilovich and Markovitch, 2007) and (Nguyen
et al, 2007), in which positive examples were ex-
tracted from Wikipedia infoboxes. These also bear
striking similarity to (Agichtein and Gravano, 2000),
and all suffer from a significantly smaller number of
seed examples. Indeed, its not using a database of
specific tuples that distinguishes LSRD, but that it
uses so many; the scale of the induction in LSRD
is designed to capture far less frequent patterns by
using significantly more seeds
In (Ramakrishnan et al, 2006) the same intu-
ition is captured that knowledge of the structure of
a database should be employed when trying to inter-
pret text, though again the three basic hypotheses of
LSRD are not supported.
In (Huang et al, 2004), a similar phenomenon to
what we observed with the acted-in-movie relation
was reported in which the chances of a protein in-
teraction relation being expressed in a sentence are
already quite high if two proteins are mentioned in
that sentence.
6 Conclusion
We have presented an approach for Large Scale Re-
lation Detection (LSRD) that is intended to be used
within a machine reading system as a source of hy-
pothetical interpretations of input sentences in natu-
ral language. The interpretations produced are se-
mantic relations between named arguments in the
sentences, and they are produced by using a large
knowledge source to generate many possible pat-
terns for expressing the relations known by that
source.
We have specifically targeted the technique at the
problem that the frequency of patterns occurring in
text that express a particular relation has a very long
tail (see Figure 1), and without enough seed exam-
ples the extremely infrequent expressions of the re-
lation will never be found and learned. Further, we
do not commit to any learning strategy at this stage
of processing, rather we simply produce counts, for
each relation, of how often a particular pattern pro-
duces tuples that are in that relation, and how of-
ten it doesn?t. These counts are simply used as ev-
idence for different possible interpretations, which
can be supported or refuted by other components in
the reading system, such as type detection.
We presented some very early results which while
promising are not conclusive. There were many
idiosyncrasies in the evaluation that made the re-
sults meaningful only with respect to other experi-
ments that were evaluated the same way. In addi-
tion, the evaluation was done at a component level,
as if the technique were a traditional relation extrac-
tion component, which ignores one of its primary
differentiators?that it produces sets of hypothetical
interpretations. Instead, the evaluation was done
only on the top hypothesis independent of other evi-
dence.
Despite these problems, the intuitions behind
LSRD still seem to us valid, and we are investing in a
truly large scale implementation that will overcome
the problems discussed here and can provide more
32
valid evidence to support or refute the hypotheses
LSRD is based on:
1. A large number of examples can account for the
long tail in relation expression;
2. Producing sets of hypothetical interpretations
of the sentence, to be supported or refuted by
further reading, works better than producing
one;
3. Using existing, large, linked-data knowledge-
bases as oracles can be effective in relation de-
tection.
References
[Agichtein and Gravano2000] E. Agichtein and L. Gra-
vano. 2000. Snowball: extracting relations from large
plain-text collections. In Proceedings of the 5th ACM
Conference on Digital Libraries, pages 85?94, San
Antonio, Texas, United States, June. ACM.
[Banko and Etzioni2008] Michele Banko and Oren Et-
zioni. 2008. The tradeoffs between open and tradi-
tional relation extraction. In Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics.
[Blum and Mitchell1998] A. Blum and T. Mitchell. 1998.
Combining labeled and unlabeled data with co-
training. In Proceedings of the 1998 Conference on
Computational Learning Theory.
[Carlson et al2009] A. Carlson, J. Betteridge, E. R. Hr-
uschka Jr., and T. M. Mitchell. 2009. Coupling semi-
supervised learning of categories and relations. In
Proceedings of the NAACL HLT 2009 Workshop on
Semi-supervised Learning for Natural Language Pro-
cessing.
[Carlson et al2010] A. Carlson, J. Betteridge, R. C.
Wang, E. R. Hruschka Jr., and T. M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of the 3rd ACM International
Conference on Web Search and Data Mining.
[Craven et al2000] Mark Craven, Dan DiPasquo, Dayne
Freitag, Andrew McCallum, Tom Mitchell, Kamal
Nigam, and Sean Slattery. 2000. Learning to construct
knowledge bases from the World Wide Web. Artificial
Intelligence, 118(1?2):69?113.
[Dolby et al2009] Julian Dolby, Achille Fokoue, Aditya
Kalyanpur, Edith Schonberg, and Kavitha Srinivas.
2009. Extracting enterprise vocabularies using linked
open data. In Proceedings of the 8th International Se-
mantic Web Conference.
[Etzioni et al2005] Oren Etzioni, Michael Cafarella,
Doug Downey, Ana-Maria Popescu, Tal Shaked,
Stephen Soderland, Daniel S. Weld, and Alexander
Yates. 2005. Unsupervised named-entity extraction
from the web: An experimental study. Artificial Intel-
ligence, 165(1):91?134, June.
[Gabrilovich and Markovitch2007] Evgeniy Gabrilovich
and Shaul Markovitch. 2007. Computing seman-
tic relatedness using wikipedia-based explicit seman-
tic analysis. In IJCAI.
[Huang et al2004] Minlie Huang, Xiaoyan Zhu, Yu Hao,
Donald G. Payan, Kunbin Qu, and Ming Li. 2004.
Discovering patterns to extract protein-protein interac-
tions from full texts. Bioinformatics, 20(18).
[Michelson and Knoblock2007] Matthew Michelson and
Craig A. Knoblock. 2007. Mining heterogeneous
transformations for record linkage. In Proceedings of
the 6th International Workshop on Information Inte-
gration on the Web, pages 68?73.
[Nguyen et al2007] Dat P. Nguyen, Yutaka Matsuo, ,
and Mitsuru Ishizuka. 2007. Exploiting syntactic
and semantic information for relation extraction from
wikipedia. In IJCAI.
[Nigam et al2006] K. Nigam, A. McCallum, , and
T. Mitchell, 2006. Semi-Supervised Learning, chapter
Semi-Supervised Text Classification Using EM. MIT
Press.
[Pantel and Pennacchiotti2006] Patrick Pantel and Marco
Pennacchiotti. 2006. Espresso: Leveraging generic
patterns for automatically harvesting semantic rela-
tions. In Proceedings of the 21st international Confer-
ence on Computational Linguistics and the 44th An-
nual Meeting of the Association For Computational
Linguistics, Sydney, Australia, July.
[Ramakrishnan et al2006] Cartic Ramakrishnan, Krys J.
Kochut, and Amit P. Sheth. 2006. A framework for
schema-driven relationship discovery from unstruc-
tured text. In ISWC.
[Wang et al2007] Gang Wang, Yong Yu, and Haiping
Zhu. 2007. PORE: Positive-only relation extraction
from wikipedia text. In ISWC.
[Whitelaw et al2008] C. Whitelaw, A. Kehlenbeck,
N. Petrovic, , and L. Ungar. 2008. Web-scale named
entity recognition. In Proceeding of the 17th ACM
Conference on information and Knowledge Manage-
ment, pages 123?132, Napa Valley, California, USA,
October. ACM.
[Yates and Etzioni2009] Alexander Yates and Oren Et-
zioni. 2009. Unsupervised methods for determining
object and relation synonyms on the web. Artificial
Intelligence, 34:255?296.
33
