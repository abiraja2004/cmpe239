From Structured Prediction to Inverse Reinforcement Learning
Hal Daume? III
School of Computing, University of Utah
and UMIACS, University of Maryland
me@hal3.name
1 Introduction
Machine learning is all about making predictions;
language is full of complex rich structure. Struc-
tured prediction marries these two. However,
structured prediction isn?t always enough: some-
times the world throws even more complex data
at us, and we need reinforcement learning tech-
niques. This tutorial is all about the how and the
why of structured prediction and inverse reinforce-
ment learning (aka inverse optimal control): par-
ticipants should walk away comfortable that they
could implement many structured prediction and
IRL algorithms, and have a sense of which ones
might work for which problems.
2 Content Overview
The first half of the tutorial will cover the ?ba-
sics? of structured prediction: the structured per-
ceptron and Magerman?s incremental parsing al-
gorithm. It will then build up to more advanced al-
gorithms that are shockingly reminiscent of these
simple approaches: maximum margin techniques
and search-based structured prediction.
The second half of the tutorial will ask the ques-
tion: what happens when our standard assump-
tions about our data are violated? This is what
leads us into the world of reinforcement learning
(the basics of which we?ll cover) and then to in-
verse reinforcement learning and inverse optimal
control.
Throughout the tutorial, we will see exam-
ples ranging from simple (part of speech tagging,
named entity recognition, etc.) through complex
(parsing, machine translation).
The tutorial does not assume attendees know
anything about structured prediction or reinforce-
ment learning (though it will hopefully be inter-
esting even to those who know some!), but does
assume some knowledge of simple machine learn-
ing (eg., binary classification).
3 Tutorial Outline
Part I: Structured prediction
? What is structured prediction?
? Refresher on binary classification
? What does it mean to learn?
? Linear models for classification
? Batch versus stochastic optimization
? From perceptron to structured perceptron
? Linear models for structured prediction
? The ?argmax? problem
? From perceptron to margins
? Search-based structured prediction
? Training classifiers to make parsing de-
cisions
? Searn and generalizations
Part II: Inverse reinforcement learning
? Refersher on reinforcement learning
? Markov decision processes
? Q learning
? Inverse optimal control and A* search
? Maximum margin planning
? Learning to search
? Apprenticeship learning
? Open problems
References
See http://www.cs.utah.edu/
?suresh/mediawiki/index.php/MLRG/
spring10.
