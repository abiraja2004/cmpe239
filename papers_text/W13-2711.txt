Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 84?89,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Using Comparable Collections of Historical Texts for Building a
Diachronic Dictionary for Spelling Normalization
Marilisa Amoia
Saarland University
m.amoial@mx.uni-saarland.de
Jose Manuel Martinez
Saarland University
j.martinez@mx.uni-saarland.de
Abstract
In this paper, we argue that compara-
ble collections of historical written re-
sources can help overcoming typical chal-
lenges posed by heritage texts enhanc-
ing spelling normalization, POS-tagging
and subsequent diachronic linguistic anal-
yses. Thus, we present a comparable cor-
pus of historical German recipes and show
how such a comparable text collection to-
gether with the application of innovative
MT inspired strategies allow us (i) to ad-
dress the word form normalization prob-
lem and (ii) to automatically generate a
diachronic dictionary of spelling variants.
Such a diachronic dictionary can be used
both for spelling normalization and for ex-
tracting new ?translation? (word forma-
tion/change) rules for diachronic spelling
variants. Moreover, our approach can be
applied virtually to any diachronic collec-
tion of texts regardless of the time span
they represent. A first evaluation shows
that our approach compares well with
state-of-art approaches.
1 Introduction
The study of heritage documents has been one
of the regular sources of knowledge in the Hu-
manities, specially in history-related disciplines.
The last years have witnessed an increased interest
in approaches combining NLP and corpus-based
techniques in the Humanities (Piotrowski, 2012)
as they can provide new insights and/or a more
consistent and reliable account of findings.
Until recently, research efforts have been fo-
cused on building diachronic corpora (e.g. Old
Bailey Online project (Hitchcock et al, 2012)
and its follow-up, the Old Bailey Corpus (Huber,
2007), the Bonn Corpus of Early New High Ger-
man (Diel et al, 2002) or the GerManC (Scheible
et al, 2011b) for German and many others). Such
resources are generally annotated with shallow
metadata (e.g. year of publication, author, ge-
ographical location) for allowing fast retrieval.
However, the annotation of richer linguistic and
semantic information still poses a series of chal-
lenges that have to be overcome, such as (i)
the noise introduced by deviant linguistic data
(spelling/orthography variation, lack of sentence
boundaries, etc.) typical of this kind of material,
due to the lack of standardized writing conventions
in terms of words and punctuation and hence (ii)
the higher error rates obtained when applying stan-
dard NLP methods.
Further, standardization of spelling variation in
historical texts can be broken down at least into
two subproblems:
1. the old word forms often differ from the mod-
ern orthography of the same items. Consider,
for instance, the diachronic variants of the
third person singular of present tense of the
verb werden in German (which means ?be-
come? as full verb, or is used as auxiliary
verb to build the future): wirt, wirdt, wirdet
vs wird; (Piotrowski, 2012) and
2. the denomination of certain objects may re-
sult completely different from that used in
the modern language due to historical reasons
(e.g. adoption of foreign language terms,
semantic shift). Consider, as an example,
the German historical/modern variants of the
word lemon (e.g. Limonie/Zitrone) or of the
word woman (e.g. Weib/Frau).
Previous approaches to spelling normalization
of historical texts have focused on the first sub-
problem. Two main strategies that have been ap-
plied:
? a rule based strategy, in which the transla-
tion of historical variants into modern forms
84
is performed on the ground of manually writ-
ten or semi-automatically gathered rules (cf.
(Pilz et al, 2008), (Bollmann et al, 2011));
? a string similarity strategy, in which a semi-
automatic attempt is made to link histori-
cal variants with modern dictionary entries
following string similarity (cf. (Giusti et
al., 2007), (Kunstmann and Stein, 2007),
(Dipper, 2010), (Hendrickx and Marquilhas,
2011), (Gotscharek et al, 2011)) or pho-
netic conflation strategies (cf. (Koolen et al,
2006), (Jurish, 2008) ).
These approaches have the disadvantage of end-
ing up relying on a time-specific dictionary of vari-
ants, e.g. they can cope with variants realized in
texts stemming from the same period of time for
which they have been created but may result inap-
propriate for texts belonging to other time spans.
Moreover, to our knowledge, there is currently
no approach to spelling normalization that can ad-
dress successfully the second subproblem stated
above ? the recognition of paraphrastic variations
realized as completely different strings or consist-
ing of semantic shifts.
As it has been often noted, the problem of stan-
dardizing diachronic variants can be understood as
a translation operation, where instead of translat-
ing between two different languages, translation
takes place between two diachronic varieties of the
same language. Inspired by experiments done for
interlinguistic translation (Rapp et al, 2012), the
idea is to use diachronic comparable corpora to
automatically produce a dictionary of diachronic
spelling variants even including semantic shifts,
regardless of the historical variants at stake.
In short, we first build a comparable histori-
cal corpus made up of recipe repertoires published
in the German language during the Early Modern
Age along with a contemporary comparable cor-
pus. Second, we address the problem of recog-
nizing and translating different variants by relay-
ing on MT techniques based on string similarity as
well as on semantic similarity measures. Finally,
we automatically extract a diachronic dictionary
of spelling and semantic variants which also pro-
vides a canonical form.
This paper is organized as follows. Section
2 presents the comparable corpus of German
recipes. Section 3 describes the approach used for
generating the dictionary of diachronic spelling
variants. Section 4 shows the results of a prelimi-
nary evaluation. Finally, in Section 5 we conclude
by discussing some final remarks.
2 The Historical Comparable Corpus of
German Recipes
The text collection encoded in our corpus spans
two hundred years and includes samples from 14
cook books written in German between 1569 and
1729. The core of the recipe corpus was compiled
as part of a PhD work in the field of Translation
Studies (cf. (Wurm, 2007)). This corpus has been
aligned resulting into two comparable corpora:
? a historical comparable dataset algned at
recipe level providing multiple versions of
the same dish across the time span of the core
corpus;
? a contemporary comparable dataset provid-
ing contemporary German versions for each
recipe.
In order to produce the historical comparable com-
ponent we proceeded in the following way:
? first, we classified the core recipes by main
ingredient and cooking method (e.g. chicken,
roast). These two parameters form the crite-
ria to consider the recipes aligned, then;
? we collected as many as possible diachronic
versions/variants of the same recipe by also
searching online resources providing collec-
tions of historical texts.
The historical component of the corpus (core and
comparable) contains a total of 430 recipes and
about 45.000 tokens. This dataset constitutes the
object of study for subsequent research, provid-
ing a representative sample of German during the
Early Modern Age in this specific domain. More-
over, language and genre evolution can be traced
thanks to its comparable nature.
Regarding the compilation of the contemporary
German comparable corpus, we collected a set of
recipes belonging to the same register but repre-
senting contemporary German language. These
recipes were collected from Internet sources and
filtered by geographical criteria (only the ones cat-
egorized as belonging to the cuisine of German
speaking regions were selected). The corpus con-
tains around 1500 recipes and over 500.000 to-
kens, which have been also aligned with their
85
Figure 1: A text excerpt from Wecker 1679.
comparable historical counterparts according to
the same parameters explained above. This sub-
set alows not only to compare historical recipes
with their modern versions but also to use them as
a reference corpus to extract standard word forms.
2.1 Digitization Strategy
The corpus has been manually transcribed. The
transcription can be regarded as a diplomatic one,
since it tries to preserve as many features of the
original as possible. Some standardization has
been performed at punctuation and hyphenation
level but no spellchecking or word separation has
been carried out. The corpus is encoded in UTF-8
and we have used a TEI-compatible XML format
to store both text and metadata.
2.2 Annotations
The corpus currently includes some shallow
semantic annotation describing text structure
(e.g. recipe, title, and body) and providing a
basic classification of recipes based on the main
ingredient and recipe type. The figure 2 below
shows an example of semantic annotation.
3 Building a Diachronic Dictionary of
Spelling Variants
Our spelling normalization strategy aims at solv-
ing both subproblems discussed in the Introduc-
tion. In order to extract the mapping between
diachronic variants by also capturing paraphrases
and semantic shifts, we apply two different strate-
gies one based on string similarity and the other
based on semantic similarity measures.
Our workflow can be summarized as follows:
1. In a first step, we relay on clustering tech-
niques based on string similarity measures
<recipe id=?26? author=?Deckhardt? year=?1611?
language=?german? ingredient=?Erdbeere?
cookingMethod=?Mus?>
<title> Ein Erdbeermuhs zumachen. < /title>
<body> <seg type=?newline?>
Nimb Erdbeer
</seg>
<seg type=?newline?/ >
treibe es durch mit Weine
</seg>
<seg type=?newline?>
thue Zucker darein
</seg>
<seg type=?newline?>
darnach man es gerne su?sse haben wil
</seg>
...
< /body>
< /recipe>
Figure 2: Comparable diachronic corpus: an ex-
ample of annotation.
to identify a set of diachronic variations of
the same word form. In this phase, cluster-
ing corresponds to the extraction of ?similar
strings?.
2. In the second step, we address the problem of
finding semantic variants, i.e. those variants
that are not realized as similar strings by ap-
plying paraphrase recognition techniques to
identify different denominations of the same
object.
3. Finally, we integrate the results of both
phases and generate a dictionary of di-
achronic variants, that is used to extract the
normalized spelling for each word in the cor-
pus. We assume that the normalized word
form corresponds to the most modern variant
found in the dictionary.
3.1 String Similarity
In the first step, we extract comparable recipes
from different decades and from the corpus of
modern recipes. Then we apply clustering tech-
niques to find spelling variations. The fact that
we use comparable texts for clustering, should re-
duces the errors as all tokens come from similar
terminological fields.
We apply agglomerative hierarchical clustering
as implemented in the R statistical programming
environment with the average agglomeration
method. As a string similarity measure, we use
the standard Levenshtein edit distance as imple-
mented in the R package Biostrings. In order to
86
build the dictionary, we select clusters that have
a string similarity greater than 65%. Figure 3
shows an example of diachronic dictionary entries
generated with this approach.
Hu?hner: Hu?ner 1574, Hu?nern 1574,
hu?ner 1574, Hu?nner 1611
und: vnd 1569, vnnd 1569,
vnd 1679, und 1698
magsts: magst 1574, magstu 1602,
magst 1679
lasst: lassen 1679, lassets 1682,
la?ssets 1715
Muscatenblu?h: Muscatblu? 1569, Mus-
catenblu?h 1715
Figure 3: Diachronic Dictionary.
For each list of diachronic variants gathered at
this point, we extracted the most recent variant and
used it as normalized form.
3.2 Semantic similarity
In order to cluster paraphrastic variants and se-
mantic shifts, we apply a slightly modified ver-
sion of Lin?s algorithm (Lin, 1998) based on the
assumption that words sharing similar contexts
should have similar semantics. Contrary to Lin, in
our approach we do not perform any dependency
analysis of the corpus data and compute semantic
similarity between strings simply in terms of the
mutual information of trigrams.
The semantic similarity strategy we imple-
mented can be summarized as follows:
? We start by generating a list of trigrams from
the corpus.
? We assign to each pair of tokens in the corpus
a value for their mutual information.
? We assign to each pair of tokens in the corpus
a value for their similarity.
? For each token in the corpus, we extract the N
most similar tokens and take the most modern
one as the normalized form.
The mutual information I for a pair of tokens t1
and t2 is defined as:
I(t1, t2) = log ?t1,?,t2???,?,???t1,?,????,?,t2? , with
? t1, ?, t2 ? the frequency of the occurrence of
the trigram t1,*,t2 in the corpus, ? ?, ?, ? ? the
total number of trigrams in the corpus, ? t1, ?, ? ?
the number of trigrams with t1 as first token and
? ?, ?, t2 ? the number of trigrams with t2 as last
token.
Semantic similarity between tokens is defined
in terms of their mutual information:
sim(t1, t2) =
?
Tt1?Tt2
I(t1,?)+I(t2,?)
?
I(t1,?)+
?
I(w2,?)
,
with Tt1 = {(v, w) : I(t1.w) > 0} and Tt2 =
{(v, w) : I(t2.w) > 0}, the sets of token pairs
that form trigrams with t1 or t2 as first element
and such that they have positive mutual informa-
tion values.
4 Evaluation
In order to evaluate the performance of our nor-
malization strategy, we extracted a subset of
recipes from the corpus for testing purposes. This
subcorpus includes 32 comparable recipes on how
to roast a chicken that have been written in a time
period ranging from 1569 to 1800 reaching a size
of 7103 words (about 8% of whole corpus). We
take as reference the results yielded by TreeTag-
ger1 (Schmid, 1994), the state-of-art POS-tagger
for German, regarding lemmatization and POS-
tagging.
First, we tagged the subcorpus on the non-
normalized word forms. The performance of POS-
tagging, in this case, is around 80%, which is
higher than the one observed in similar experi-
ments (cf. (Scheible et al, 2011a)) on other his-
torical corpora of German. We believe the reason
for this is the relative syntactic simplicity of recipe
texts in comparison to other kind of texts (dramas,
sermons, letters, scientific or legal texts).
The tagger?s poor performance is due to the ex-
istence of lexical items unknown to the system
(around 27%), on the one hand, and the high in-
consistency of the spelling, on the other hand.
Our strategy to circumvent this problem consists
of providing a modern word form to all histori-
cal word variants that we obtained from the pre-
viously discussed diachronic dictionary. We ex-
pected, that after the two normalization steps dis-
cussed in Section 3, the performance of the tag-
ging process should improve.
1The TreeTagger was trained on the Tu?Ba-D/Z treebank.
Its performance is about 97.4% on newspaper texts and 78%
on texts containing unknown words.
87
Strategy Lemma POS
no-norm 73% 80%
string-similarity 81% 81.4%
semantic similarity 82.5% 82%
Table 1: Evaluation Results.
Therefore, we repeated the experiment, first, on
the test subcorpus normalized by using the di-
achronic dictionary generated with first normaliza-
tion strategy, i.e. the one based on string similarity
measure and, second, on the normalized version
obtained after using the second strategy based on
semantic similarity.
Table 1 summarizes the results of a preliminary
evaluation of our strategy.
After string similarity normalization, the tagger
was able to identify all lemmas except for 1358
tokens (19% of unknown tokens). While POS-
tagging improved to 81.4%.
The semantic similarity step improved the per-
formance of lemmatization and POS reaching
82.5% and 82% respectively.
Despite the fact that our experiments refer to
very few data and to a restricted domain, we be-
lieve they are promising and show that our strat-
egy, the integration of string similarity and seman-
tic similarity measures can lead to a high quality
automatic spelling normalization and outperform
state-of-art approaches.
5 Conclusion
In this paper we have presented a comparable
corpus of historical German recipes and shown
that such comparable resources can help remov-
ing sources of noise typical of these text types
that hinder standard NLP manipulation of such
material. The old German recipes corpus is, to
our knowledge, one of the first attempts2 to build
a comparable historical corpus of German. The
corpus is accessible through a web interface and
allows sophisticated queries according to differ-
ent levels of annotation: 1) historical word forms;
2) modern normalized forms; 3) lemmas on top
of normalized forms; 4) part-of-speech, and, last
but not least; 5) semantics, namely main ingre-
dient and cooking method. Further, we describe
an innovative strategy for word form normaliza-
2We are aware of only one similar project (Bartsch et al,
2011) aimed at building a comparable corpus of German texts
for three main periods Old High, Middle High and Early New
High German. However, those corpora are not yet available.
tion that integrate string similarity measure with
semantic similarity thereby being able to cope not
only with formal spelling variations but also with
paraphrastic variations and semantic shift. More-
over, this method can be applied to any compara-
ble diachronic corpus, regardless of the time span
at stake. A preliminary evaluation has shown that
such a strategy may outperform state-of-art ap-
proaches.
References
Nina Bartsch, Stefanie Dipper, Birgit Herbers, Sarah
Kwekkeboom, Klaus-Peter Wegera, Lars Eschke,
Thomas Klein, and Elke Weber. 2011. An-
notiertes Referenzkorpus Mittelhochdeutsch (1050-
1350). Poster session at the 33rd annual meeting of
the German Linguistic Society (DGfS-2011) (Ab-
stract, Poster) .
Marcel Bollmann, Florian Petran, and Stefanie Dip-
per. 2011. Applying Rule-Based Normalization to
Different Types of Historical Texts ? An Evalua-
tion. In Proceedings of the 5th Language & Tech-
nology Conference: Human Language Technologies
as a Challenge for Computer Science and Linguis-
tics, Poznan?, November.
Marcel Diel, Bernhard Fisseni, Winfried Lenders,
and Hans-Christian Schmitz. 2002. XML-
Kodierung des Bonner Fru?hneuhochdeutschkorpus.
IKP-Arbeitsbericht NF 02, Bonn .
Stefanie Dipper. 2010. Pos-tagging of historical
language data: First experiments. In Semantic
Approaches in Natural Language Processing. Pro-
ceedings of the 10th Conference on Natural Lan-
guage Processing (KONVENS-10), pages 117?121,
Saarbru?cken.
Rafael Giusti, Arnaldo Candido Jr, Marcelo Muniz,
L??via Cucatto, and Sandra Alu??sio. 2007. Auto-
matic Detection of Spelling Variation in Historical
Corpus : An Application to Build a Brazilian Por-
tuguese Spelling Variants Dictionary. In Proceed-
ings of the Corpus Linguistics Conference, pages 1?
20.
A. Gotscharek, U. Reffle, C. Ringlstetter, K. U. Schulz,
and A. Neumann. 2011. Towards information re-
trieval on historical document collections: the role
of matching procedures and special lexica. IJDAR,
14(2):159?171.
Iris Hendrickx and Rita Marquilhas. 2011. From old
texts to modern spellings: an experiment in auto-
matic normalisation. Journal for Language Technol-
ogy and Computational Linguistics, 26(2):65?76.
Tim Hitchcock, Robert Shoemaker, Clive Emsley,
Sharon Howard, and Jamie McLaughliin. 2012.
The Old Bailey Proceedings Online, 1674-1913
(version 7.0).
Magnus Huber. 2007. The Old Bailey Proceed-
ings, 1674-1834. Evaluating and annotating a cor-
pus of 18th- and 19th-century spoken English. In
88
Meurman-Solin. Anneli and Arja Nurmi, editors,
Annotating Variation and Change, volume 1. Re-
search Unit for Variation, Contacts and Change
in English (VARIENG), University of Helsinki,
Helsinki.
Bryan Jurish. 2008. Finding canonical forms
for historical German text. In Angelika Storrer,
Alexander Geyken, Alexander Siebert, and Kay-
Michael Wu?rzner, editors, Text Resources and Lex-
ical Knowledge. Selected Papers from the 9th Con-
ference on Natural Language Processing KONVENS
2008, pages 27?38. Mouton de Gruyter, Berlin /
New York.
Marijn Koolen, Frans Adriaans, Jaap Kamps, and
Maarten de Rijke. 2006. A cross-language ap-
proach to historic document retrieval. In Mounia
Lalmas, Andy MacFarlane, Stefan Rueger, Anasta-
sios Tombros, Theodora Tsikrika, Alexei Yavlinsky,
editor, Advances in Information Retrieval, volume
3936, pages 407?419. Lecture Notes in Computer
Science, Berlin/Heidelberg: Springer.
Pierre Kunstmann and Achum Stein. 2007. Le
Nouveau Corpus d?Amsterdam. In Pierre Kunst-
mann Achim Stein, editor, Le Nouveau Corpus
d?Amsterdam. Actes de l?atelier de Lauterbad, 23-
26 fe?vrier 2006, pages 9?27. Stuttgart, Germany:
Steiner.
Vladimir I. Levenshtein. 1965. Binary codes capa-
ble of correcting deletions, insertions, and reversals.
Doklady Akademii Nauk SSSR, 163(4):845?848.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words.
Thomas Pilz, Andrea Ernst-Gerlach, Sebastian Kemp-
ken, Paul Rayson, and Dawn Archer. 2008. The
Identification of Spelling Variants in English and
German Historical Texts: Manual or Automatic?
Literary and Linguistic Computing, 23(1):65?72,
April.
Michael Piotrowski. 2012. Natural Language Pro-
cessing for Historical Texts, volume 5. Morgan &
Claypool Publishers, September.
Reinhard Rapp, Serge Sharoff, and Bogdan Babych.
2012. Identifying word translations from compa-
rable documents without a seed lexicon. In LREC,
pages 460?466.
Silke Scheible, Richard J. Whitt, Martin Durrell, and
Paul Bennett. 2011a. Evaluating an f-the-shelfS-
tagger on Early Modern German text. In Proceed-
ings of the 5th ACL-HLT Workshop on Language
Technology for Cultural Heritage, Social Sciences,
and Humanities, number June, pages 19?23, Port-
land, Oregon. Association for Computational Lin-
guistics.
Silke Scheible, Richard J. Whitt, Martin Durrell, and
Paul Bennett. 2011b. A gold standard corpus
of early modern german. In Linguistic Annotation
Workshop, pages 124?128.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, Manchester, UK.
Andrea Wurm. 2007. Translatorische
Wirkung: ein Beitrag zum Versta?ndnis von
U?bersetzungsgeschichte als Kulturgeschichte am
Beispiel deutscher U?bersetzungen franzo?sischer
Kochbu?cher in der Fru?hen Neuzeit. Ph.D. thesis,
Universita?t des Saarlandes, Saarbru?cken.
89
