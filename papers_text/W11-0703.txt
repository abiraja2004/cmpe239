Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 12?19,
Portland, Oregon, 23 June 2011. c?2011 Association for Computational Linguistics
What pushes their buttons? Predicting comment polarity from the content
of political blog posts
Ramnath Balasubramanyan
Language Technologies Institute
Carnegie Mellon University
rbalasub@cs.cmu.edu
William W. Cohen
Machine Learning Department
Carnegie Mellon University
wcohen@cs.cmu.edu
Doug Pierce and David P. Redlawsk
Political Science Department
Rutgers University
drpierce@eden.rutgers.edu, redlawsk@rutgers.edu
Abstract
Political blogs as a form of social media al-
low for an uniquely interactive form of politi-
cal discourse. This is especially evident in fo-
cused blogs with a strong ideological identity.
We investigate techniques to identify topics
within the context of the community, which
when discussed in a blog post evoke a dis-
cernible positive or negative collective opin-
ion from readers who respond to posts in com-
ments. This is done by using computational
methods to assign sentiment polarity to blog
comments and learning community specific
models that summarize issues tackled by blogs
and predict the polarity based on the topics
discussed in a blog post.
1 Introduction
Recent work in political psychology has made it
clear that political decision-making is strongly influ-
enced by emotion. For instance, (Lodge and Taber,
2000) propose a theory of ?motivated reasoning?, in
which political information is processed in a way
that is determined, in part, by a quickly-computed
emotional react to that information. Strong exper-
imental evidence for motivated reasoning (some-
times called ?hot cognition?) exists (Huang and
Price, 2001); (Redlawsk, 2002); (Redlawsk, 2006);
(Isbell et al, 2006). However, despite some recent
proposals (Kim et al, 2008) it is unclear how to
computationally model a person?s emotional reac-
tion to news, and how to collect the data necessary
to fit such a model. One problem is that emotional
reactions are different for different people - a fact ex-
ploited in the use of political ?code words? intended
to invoke a reaction in only a particular subset of the
electorate (a technique sometimes called ?dog whis-
tle politics?).
In this paper, we evaluate the use of machine
learning methods to predict how members of a spe-
cific political community will emotionally reaction
to different types of news. More specifically, we use
a dataset of widely read (?A-list?) political blogs,
and attempt to predict the aggregate sentiment in the
comment section of blogs, as a function of the tex-
tual content of the blog posting. In this paper, we
consider only predicting polarity (positive and neg-
ative feeling). In contrast to work done traditionally
in sentiment analysis which focuses on determining
the sentiment expressed in text, in this work, we fo-
cus on the task of predicting the sentiment that a
block of text will evoke in readers, expressed in the
comment section, as a response to the blog post.
This task is related to, but distinct from, several
other studies that have been made using comments
and discussions in political communities, or analy-
sis of sentiment in comments - (Yano et al, 2009),
(O?Connor et al, 2010), (Tumasjan et al, 2010).
Below we discuss the methods used to address the
various parts of this task. First, we evaluate two
methods to automatically determine the comment
polarity: SentiWordNet (Baccianella and Sebastiani,
2010) a general purpose resource that assigns sen-
timent scores to entries in WordNet, and an auto-
12
mated corpus-specific technique based on pointwise
mutual information. The quality of the polarity as-
sessments by these techniques are made by compar-
ing them to hand annotated assessments on a small
number of blog posts. Second, we consider two
methods for predicting comment polarity from post
content: support vector machine classification, and
sLDA, a topic-modeling-based approach. Finally,
we demonstrate that emotional reactions are indeed
community-specific, compare the accuracy of this
approach to the more traditional approach of pre-
dicting sentiment of a text from the text itself, and
present our conclusions.
2 Data
In this study, we use a collection of blog posts from
five blogs: Carpetbagger(CB)1, Daily Kos(DK)2,
Matthew Yglesias(MY)3, Red State(RS)4, and Right
Wing News(RWN)5, that focus on American politics
made available by (Yano et al, 2009). The posts
were collected during November 2007 to October
2008, which preceded the US presidential elections
held in November 2008. The blogs included in the
dataset vary in political idealogy with blogs like
Daily Kos that are Democrat-leaning and blogs like
Red State tending to be much more conservative.
Since we are interested in studying the responses
to blog posts, the corpus only contains posts where
there have been at least one comment in the six days
after the post was published. It is important to note
that only the text in the blog posts and comments are
used in this study. All non-textual information like
pictures, hyperlinks, videos etc. are discarded. In
terms of text processing, for each blog, a vocabulary
is created consisting of all terms that occur at least
5 times in the blog. Stopwords are eliminated us-
ing a standard stopword list. Each blog post is then
represented as a bag of words from the post. Table
2 shows statistics of the datasets. Each dataset is
studied separately for the most part in the rest of the
paper.
1http://www.thecarpetbaggerreport.com
2http://www.dailykos.com/
3http://yglesias.thinkprogress.org/
4http://www.redstate.com/
5http://rightwingnews.com/
3 Labelling comments with sentiment
polarity
The first step in understanding the nature of posts
that evoke emotional responses is to get a measure of
the polarity in the sentiment expressed in the com-
ments section of a blog post. The measure indicates
the ability of the issues in the blog post and its treat-
ment, to evoke strong emotions in readers.
3.1 SentiWordNet
In the first stage of the study, we use SentiWord-
Net (Baccianella and Sebastiani, 2010) which as-
sociates a large number of words in WordNet with
a positive, negative and objective score (summing
up to 1). Firstly, all the comments for a blog post
in the comment section are aggregated and for the
words in the comments that are found in SentiWord-
Net, the net positive and negative scores are com-
puted. Since SentiWordNet entries are associated
with word senses and because we don?t perform
word sense disambiguation, the SentiWordNet po-
larity of the most dominant word sense is used for
words in the comment section. The sentiment in the
comment section is deemed to be positive if the net
positive score exceeds the negative score and nega-
tive otherwise. Therefore, each blog post is now as-
sociated with a binary response variable indicating
the polarity of the sentiment expressed in the com-
ments.
3.2 Using pointwise mutual information
A second technique to determine the sentiment po-
larity of comments uses the principle of pointwise
mutual information (PMI)(Turney, 2002). We first
construct a seed list of positive and negative words
by choosing the 100 topmost positive and negative
words from SentiWordNet and manually eliminat-
ing words from this list that don?t pertain to senti-
ment in our context. (Appendix A has the list of
seed words used.) This seed list is used to construct
a larger set of positive and negative words by com-
puting the PMI of the words in the seed lists with
every other word in the vocabulary. It?s important
to note that this list is constructed for the specific
corpus that we work with. Because every blog is
processed separately, we construct a different senti-
ment word list for each blog based on the statistics
13
Blog Pol align-
ment
#posts Vocabulary size Avg
#words
per post
Avg #com-
ments per
post
Avg
#words per
comment
section
Carpetbagger
(CB)
liberal 1201 4998 170 31 1306
Daily Kos (DK) liberal 2597 6400 103 198 3883
Matthew Ygle-
sias (MY)
liberal 1813 4010 69 35 1420
Red State (RS) conservative 2357 8029 158 28 806
Right Wing Na-
tion (RWN)
conservative 1184 6205 185 33 1015
Table 1: Dataset statistics
of word occurences. Words in the vocabulary are
ranked by the difference in the average of the PMI
with positive and negative seed words. The top 1000
words in the resultant sorted list are treated as pos-
itive words and the bottom 1000 words as negative
words. The comment section of every post is tagged
with a positive or negative polarity as in the previous
section by computing the total positive and negative
word counts.
Using the same seed word list, the procedure is
performed separately for each blog resulting in sen-
timent polarity lists that are particular to the com-
munity and idealogy associated with each blog. It
should be noted that while this method provides bet-
ter estimates of comment sentiment polarity (as seen
in Section 4), it involves more manual work in con-
structing a seed set than the SentiWordNet method
which does not require any manual effort.
3.3 Human labels
As a third method that is accurate but expensive, we
manually labeled comments from approximately 30
blog posts from each blog, with a positive or neg-
ative label. The guideline in labeling was to deter-
mine if the sentiment in the comment section was
positive or negative to the subject of the post. The
chief intention of this exercise is to determine the
quality of the polarity assessments of the SentiWord-
Net and PMI methods. While it is possible to di-
rectly use the assessments and train a classifier, the
performance of the classifier will be limited by the
very small number of training examples (30 instead
of thousands of examples). The accuracy of the two
Blog SentiWordNet accuracy PMI accuracy
CB 0.56 0.78
DK 0.54 0.72
MY 0.61 0.83
RS 0.54 0.74
RWN 0.64 0.84
Table 2: Measuring accuracy of automatic comment po-
larity detection
automatic methods to determine comment polarity
is shown in Table 2
The better accuracy of the PMI method can be ex-
plained by the fact that SentiWordNet is a general
purpose list that is not customized for the domain
which tends to make it noisy for text in the politi-
cal domain. The PMI technique corresponds more
closely with the human labels but it requires a little
human effort in building the initial seed list of posi-
tive and negative words.
4 Predicting sentiment from blog content
We now address the problem of using machine
learning techniques to predict the polarity of the
comments based on the blog post contents.
4.1 SVM
Firstly, we use support vector machines (SVM) to
perform classification. We frame the classification
task as follows: The input features to the classifier
are the words in the blog post i.e each blog post is
treated as a bag of words and the output variable is
the binary comment polarity computed in the previ-
14
SentiWordNet PMI
Blog SVM sLDA SVM sLDA
cb 0.56 0.58 0.79 0.79
dk 0.61 0.64 0.75 0.77
my 0.67 0.59 0.87 0.87
rs 0.53 0.55 0.74 0.76
rwn 0.57 0.59 0.90 0.90
Table 3: Accuracy: Using blog posts to predict comment
sentiment polarity
ous section. For our experiments, we used the SVM-
Light package 6 with a simple linear kernel and eval-
uated the classifier using 10 fold cross validation.
Table 3 shows the accuracy of the classifier for the
different blogs and polarity measuring schemes. The
errors in classification can be attributed in part to
the inherent difficulty of the task due to the noise of
the polarity labeling schemes and in part due to the
difficulty in obtaining a signal to predict comment
polarity from the body of the post.
4.2 Supervised LDA
Next, we use Supervised LDA (sLDA) (Blei and
McAuliffe, 2008) to do the classification. sLDA is
a model that is an extension of Latent Dirichlet Al-
location (LDA) (Blei et al, 2003) that models each
document as having an output variable in addition to
the document contents. The output variable in the
classification case is modeled as an output of a lo-
gistic regression model that uses the posterior topic
distribution of the LDA model as features. In this
task, the output variable is +1 or -1 depending on
the polarity of the comment section. In the experi-
ments with sLDA, we set the number of topics as 15
after experimenting with a range of topics and use
10-fold cross validation. The number of topics is set
lower than it usually is with topic modeling, due to
the relatively short length and small number of doc-
uments.
The advantage of sLDA in this task is that we in-
duce topics from the bodies of the blog posts that
serve to characterize the different issues that each
blog addresses. In addition, the logistic regres-
sion parameters indicate how each topic influences
the output variable. Table 4 shows the top 1 or 2
6http://svmlight.joachims.org/
topics with the highest negative and positive logis-
tic regression coefficients for each blog. Inspect-
ing the top words of the topics confirms our no-
tions of the kinds of issues that appeal to the read-
ers of each of the blogs. For instance, in the top-
ics induced from Daily Kos, a very liberal leaning
blog, we see that the most negative topic (i.e. the
topic that contributes the most to potential nega-
tive comments) talks about the Bush adminstration
and Vice President Cheney, which was and remains
quite unpopular with people from the left. The other
negative topic concerns the war in Iraq which was
also very unpopular within people whose beliefs are
liberal-leaning. The most positive topic seemingly
focuses on campaign funding. Our conjecture for
the high comment polarity is the great success in the
then Democratic candidate Obama?s fund raising at-
tempts during the presidential campaign. In the sec-
ond blog, Right Wing News, which is a conservative
blog, we see a different picture. The most negative
topic deals with Islam and Muslim people which are
issues that have tended to evoke negative reactions
from certain sections of people with conservative
political beliefs. Global warming also evoked nega-
tive comments which is consistent with the conser-
vative viewpoint that there isn?t evidence to suggest
that greenhouse gases cause global warming. The
most positive topic seems to be about anti-abortion
issues which is an issue that frequently pops up in
conservative political discourse. Topics from the
other blogs also seem to be in line with the standard
positions taken by liberal and conservatives on lead-
ing issues in US politics like taxation, immigration,
public health and the presidential campaign which
was in full flow at the time the data was collected.
Table 3 shows the accuracy of sLDA in predict-
ing the comment polarity based on the blog posts.
It can be seen from the table that sLDA performs
marginally better than SVM when trained on blog
posts, even though documents are now represented
in the lower dimensional topic space in contrast to
the high dimensional word space that was used with
SVM. sLDA provides the additional advantage of
providing an overall summary of the corpus via the
topic tables it induces.
15
Blog Topic words Topic co-efficient
CB
* bush president news administration house white officials report fox government
office military department public cheney john journal week pentagon national
-0.79
* huckabee giuliani romney mccain republican presidential religious campaign gop
john party candidate mitt rudy mike conservative thompson support paul candidates
0.48
DK
* bush administration congress law government court house intelligence white ex-
ecutive committee time cheney federal course national act president congressional
information
-1.54
* iraq war bush troops news military american president iraqi starts maine cheers
days jeers mccain moreville rightnow day americans people
-0.60
* money health campaign foster energy district million people nrcc dccc care elec-
tion time bill change funds don global federal economy
0.62
MY
* iraq war american military iraqi government people troops bush security united
forces world country surge presence political force maliki afghanistan
-0.50
* people care health don public immigration college political education school is-
sue insurance social system policy real lot isn actually sense
1.05
RS
* economy market people financial economic markets money world rate rates fed-
eral mortgage government credit prices price term inflation reserve oil
-0.30
* tax government taxes money economic care people spending million jobs ameri-
can energy health increase pay economy private free federal business
0.61
RWN
* people muslim world country war american law muslims time police america
rights free peace death city islamic government freedom united
-0.68
* democrats warming global vote election obama energy democratic change votes
climate people john gore political gas don voters party bill
-0.39
* people life women woman time own little love person children world live read
believe god isn school feel mean
0.47
Table 4: Topics from sLDA and weights
SentiWordNet PMI
Blog SVM sLDA SVM sLDA
cb 0.66 0.56 0.79 0.79
dk 0.72 0.59 0.74 0.73
my 0.64 0.61 0.87 0.89
rs 0.65 0.57 0.75 0.80
rwn 0.65 0.60 0.90 0.90
Table 5: Accuracy: Using comments to predict comment
sentiment polarity
4.3 Using comments to predict comment
polarity
In the previous experiments we were using the bod-
ies of the blog posts to predict comment polarity.
There are multiple factors which make this a diffi-
cult task. One major factor is the difficulty of learn-
ing potentially noisy labels using automatic meth-
ods. More interestingly, we operate under the hy-
pothesis that there is signal about comment polarity
in the bodies of the blog posts. To test this hypoth-
esis, we train classifiers on the comment sections
themselves to predict comment polarity. This serves
to eliminate the effect of our hypothesis and focus
on the inherent difficulty in learning the noisy la-
bels. Table 5 shows the results of these experiments.
We see that once again, sLDA results are compara-
ble to the accuracies reported by SVM and that PMI
labels are less noisier than the labels obtained using
16
Evaluating Trained on DK Trained on RWN
DK 0.75/0.77 0.61/0.62
RWN 0.74/0.71 0.90/0.90
Table 6: Cross blog results: Accuracy using SVM/sLDA
SentiWordNet. More importantly, we note that the
accuracy in predicting the comment polarity while
higher than the accuracy in predicting the polarity
from blog posts, is not significantly higher which
strongly suggests that blog posts have quite a bit of
information regarding comment polarity.
4.4 Cross blog experiments
The effect of the nature of the blog on the classifier is
examined by training models on the blog posts from
a conservative blog (RWN) using PMI-determined
polarities as targets and by testing the model by run-
ning liberal blog data (from DK) through it. Simi-
larly, we test RWN blog entries by training it on a
classifier trained on DK posts. The results of the ex-
periments are in Table 6. For easy reference, the
table also includes the accuracies when blogs are
trained using posts from the same blog (obtained
from Table 3). We see that the accuracy in predict-
ing polarity degrades when blog posts are tested on
a classifier trained on posts from a blog of opposite
political affiliation. These results indicate that emo-
tion is tied to the blog and community that one is
involved in.
4.5 Conclusion
We addressed the task of predicting the emotional
response that is induced in political discourses. To
this end, we tackled the tasks of determining the sen-
timent polarity of comments in blogs and the task of
predicting the polarity based on the content of the
blog post. Our approach also characterized the is-
sues talked about in specific blog communities. Our
experiments show that the community specific PMI
method provides a more accurate picture of the sen-
timent in comments than the generic SentiWordNet
technique. We also see that the context of the com-
munity is key as seen in the poor performance of
models trained on blogs from one end of the politi-
cal spectrum in predicting the polarity of responses
to blog posts in communities on the other end of the
spectrum.
References
Andrea Esuli Stefano Baccianella and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Language
Resources Association (ELRA).
David Blei and Jon McAuliffe, 2008. Supervised Topic
Models, pages 121?128. MIT Press, Cambridge, MA.
D. M Blei, A. Y Ng, and M. I Jordan. 2003. Latent
dirichlet alocation. The Journal of Machine Learning
Research, 3:993?1022.
Li-Ning Huang and Vincent Price. 2001. Motivations,
goals, information search, and memory about political
candidates. Political Psychology, 22(4):pp. 665?692.
Linda M. Isbell, Victor C. Ottati, and Kathleen C. Bruns.
2006. Affect and politics: Effects on judgment, pro-
cessing, and information seeking. In David Redlawsk,
editor, Feeling Politics: Emotion in Political Infor-
mation Processing. Palgrave Macmillan, New York,
USA.
Sung-youn Kim, Charles S. Taber, and Milton Lodge.
2008. A Computational Model of the Citizen as Mo-
tivated Reasoner: Modeling the Dynamics of the 2000
Presidential Election. SSRN eLibrary.
Milton Lodge and Charles Taber, 2000. Three Steps
toward a Theory of Motivated Political Reasoning.
Cambridge University Press.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010. From
tweets to polls: Linking text sentiment to public opin-
ion time series. In Proceedings of the International
AAAI Conference on Weblogs and Social Media.
David P. Redlawsk. 2002. Hot cognition or cool con-
sideration? testing the effects of motivated reasoning
on political decision making. The Journal of Politics,
64(04):1021?1044.
David Redlawsk. 2006. Motivated reasoning, affect, and
the role of memory in voter decision-making. In David
Redlawsk, editor, Feeling Politics: Emotion in Politi-
cal Information Processing. Palgrave Macmillan, New
York, USA.
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-
ner, and Isabell M. Welpe. 2010. Predicting elec-
tions with twitter: What 140 characters reveal about
political sentiment. In William W. Cohen and Samuel
Gosling, editors, ICWSM. The AAAI Press.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, ACL ?02, pages 417?424, Stroudsburg, PA, USA.
Association for Computational Linguistics.
17
Tae. Yano, William. Cohen, and Noah A. Smith. 2009.
Predicting response to political blog posts with topic
models. In Proceedings of the North American Associ-
ation for Computational Linguistics Human Language
Technologies Conference.
Appendix A
18
Positive wonderfulness, admirableness, admirability, wonderful, admirable, top-flight, splendid, first-class, fantabu-
lous, excellent, good, balmy, mild, ennoble, dignified, amuse, agree, do good, benefit, vest, prefer, placate,
pacify, mollify, lenify, gentle, conciliate, assuage, appease, filigree, dazzle, admiringly, character, preem-
inence, note, eminence, distinction, radiance, amiability, bonheur, worship, adoration, divination, music,
euphony, judiciousness, essentialness, essentiality, gain, crispness, urbanity, courtesy, decency, modesty,
dedication, integrity, honourableness, honorableness, honor, goodness, good, morality, urbanity, tasteful-
ness, elegance, elegance, healthfulness, nutritiveness, nutritiousness, wholesomeness, fineness, choiceness,
loveliness, fairness, comeliness, beauteousness, picturesqueness, bluffness, good nature, character, props,
joke, jocularity, jest, worthy, salubrious, healthy, virtuous, esthetic, artistic, aesthetic, spiffing, superlative,
sterling, greatest, superb, brilliant, boss, banner, olympian, majestic, straightarrow, wide-eyed, round-eyed,
dewy-eyed, childlike, righteous, answerable, nice, decent, diffident, respected, reputable, self-respecting,
self-respectful, dignified, constructive, sweet, fabulous, fab, charming, admirable, idyllic, idealized, ide-
alised, ennobling, dignifying, nice, incumbent, clean, lucky, intellectual, formidable, awing, awful, awe-
some, awe-inspiring, amazing, important, joking, jocular, jocose, jesting, amicable, kind, genial, therapeu-
tic, sanative, remedial, healing, curative, gracious, gainly, goody-goody, good, superb, solid, good, inspired,
elysian, divine, worthy, quaint, discerning, golden, fortunate, blest, blessed, courteous, thorough, exhaus-
tive, better, benign, pretty, piquant, engaging, attractive, well, veracious, right, grace, goodwill, belong,
accommodate, serve, merit, deserve, shine, radiate, glow, beam, disillusion, disenchant, proclaim, laud,
glorify, extol, exalt, cheer, consider, purify, enervate, recuperate, amusingly, dearly, dear, affectionately,
thoroughly, soundly, well, simply, time, posterboard, fettle, mildness, clemency, successfulness, prosper-
ity, wellbeing, well-being, upbeat, wholeness, haleness, purity, pureness, innocence, antithesis, serendipity,
superordinate, superior, possible, pleaser, idolizer, idoliser, amoralist
Negative tawdry, shoddy, cheapjack, scrimy, unsound, unfit, bad, sorry, sad, pitiful, lamentable, distressing, de-
plorable, abject, unfortunate, inauspicious, humbug, trouble, inconvenience, disoblige, bother, smell, stink,
reek, twinge, sting, prick, burn, sting, burn, bite, desensitize, desensitise, resent, begrudge, pity, compassion-
ate, abreact, agonize, agonise, muddy, settle, moan, groan, impugn, repudiate, deny, reject, disapprove, snub,
repel, rebuff, sting, stick, disapprove, refute, rebut, controvert, foul, curdle, smite, afflict, ease, comfort, ail,
inflame, woefully, sadly, lamentably, deplorably, hard, unluckily, unfortunately, regrettably, alas, worst,
throe, woe, suffering, inconvenience, incommodiousness, solacement, solace, dyspnoea, dyspnea, throe,
shrew, ruffian, rowdy, roughneck, hooligan, bully, plonk, sullenness, moroseness, glumness, moodiness,
malignity, malevolence, guilt, sorrow, ruefulness, rue, regret, dolour, dolor, dolefulness, gloating, gloat,
weakness, self-torture, self-torment, suffering, hurt, distress, torment, curse, straits, pass, head, excoriation,
canard, scurrility, billingsgate, scribble, scrawl, scratch, prejudice, preconception, bias, pill, onus, load, in-
cumbrance, encumbrance, burden, poignancy, pathos, penalty, badness, bad, fault, demerit, hardness, moldi-
ness, harshness, cruelty, cruelness, spitefulness, spite, nastiness, cattiness, bitchiness, malice, malevolency,
malevolence, heinousness, barbarousness, barbarity, atrocity, atrociousness, illegitimacy, unnaturalness, dis-
agreeableness, incongruousness, incongruity, ruggedness, hardness, unneighborliness, unfriendliness, dis-
agreeableness, sadness, lugubriousness, gloominess, shlock, schlock, dreck, mongrel, bastard, shenanigan,
roguishness, roguery, rascality, mischievousness, mischief-making, mischief, deviltry, devilry, devilment,
shitwork, overexertion, overacting, hamming, shlep, schlep, worst, upset, scrofulous, sick, ill, sheltered,
occult, trashy, rubbishy, undivided, worried, upset, disturbed, distressed, disquieted, troubled, unmanage-
able, uncontrollable, mussy, messy, unsympathetic, invalidating, disconfirming, wretched, woeful, miser-
able, execrable, deplorable, bush-league, bush, tinny, sleazy, punk, crummy, chintzy, cheesy, cheap, bum,
inferior, indifferent, lowly, humble, insufficient, deficient, insubordinate, cross-grained, contrarious, spas-
tic, spasmodic, convulsive, unaccepted, unacceptable, nonstandard, unsound, asocial, antisocial, feigned,
broken-down, vicious, reprehensible, deplorable, criminal, condemnable, notorious, infamous, ill-famed,
untreated, modified, limited, unmixed, unmingled, sheer, plain, cretinous, negative, imponderable, vexing,
maddening, infuriating, exasperating, ungrateful, sore, painful, afflictive, harsh, unpeaceable, unforbearing,
unpainted, underivative, scurrilous, opprobrious, abusive, verminous, outrageous, horrific, horrid, hideous,
creepy, pestilent, pernicious, deadly, baneful, paranormal, grotty, nasty, awful, transcendental, preternatural,
otherworldly, nonnatural, simulated, imitation, faux, false, fake, substitute, ersatz, strong, smart, wicked,
terrible, severe, unpitying, ruthless, remorseless, pitiless, unlikeable, unlikable, unmourned, unlamented,
rough, harsh, woeful, woebegone, lugubrious, heartsick, heartbroken, brokenhearted, bitter
Table 7: Seed words used in the PMI technique
19
