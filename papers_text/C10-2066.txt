Coling 2010: Poster Volume, pages 579?587,
Beijing, August 2010
Automatic Allocation of Training Data for Rapid Prototyping
of Speech Understanding based on Multiple Model Combination
Kazunori Komatani? Masaki Katsumaru? Mikio Nakano?
Kotaro Funakoshi? Tetsuya Ogata? Hiroshi G. Okuno?
? Graduate School of Informatics, Kyoto University
{komatani,katumaru,ogata,okuno}@kuis.kyoto-u.ac.jp
? Honda Research Institute Japan Co., Ltd.
{nakano,funakoshi}@jp.honda-ri.com
Abstract
The optimal choice of speech understand-
ing method depends on the amount of
training data available in rapid prototyp-
ing. A statistical method is ultimately
chosen, but it is not clear at which point
in the increase in training data a statisti-
cal method become effective. Our frame-
work combines multiple automatic speech
recognition (ASR) and language under-
standing (LU) modules to provide a set
of speech understanding results and se-
lects the best result among them. The
issue is how to allocate training data to
statistical modules and the selection mod-
ule in order to avoid overfitting in training
and obtain better performance. This paper
presents an automatic training data alloca-
tion method that is based on the change
in the coefficients of the logistic regres-
sion functions used in the selection mod-
ule. Experimental evaluation showed that
our allocation method outperformed base-
line methods that use a single ASR mod-
ule and a single LU module at every point
while training data increase.
1 Introduction
Speech understanding in spoken dialogue systems
is the process of extracting a semantic represen-
tation from a user?s speech. That is, it consists
of automatic speech recognition (ASR) and lan-
guage understanding (LU). Because vocabularies
and language expressions depend on individual
systems, it needs to be constructed for each sys-
tem, and accordingly, training data are required
for each. To collect more real training data, which
will lead to higher performance, it is more desir-
able to use a prototype system than that based on
the Wizard-of-Oz (WoZ) method where real ASR
errors cannot be observed, and to use a more ac-
curate speech understanding module. That is, in
the bootstrapping phase, spoken dialogue systems
need to operate before sufficient real data have
been collected.
We have been addressing the issue of rapid pro-
totyping on the basis of the ?Multiple Language
model for ASR and Multiple language Under-
standing (MLMU)? framework (Katsumaru et al,
2009). In MLMU, the most reliable speech un-
derstanding result is selected from candidates pro-
duced by various combinations of multiple ASR
and LU modules using hand-crafted grammar and
statistical models. A grammar-based method is
still effective at an early stage of system devel-
opment because it does not require training data;
Schapire et al (2005) also incorporated human-
crafted prior knowledge into their boosting al-
gorithm. By combining multiple understanding
modules, complementary results can be obtained
by different kinds of ASR and LU modules.
We propose a novel method to allocate avail-
able training data to statistical modules when the
amount of training data increases. The training
data need to be allocated adaptively because there
are several modules to be trained, and they would
cause overfitting without data allocation. There
are speech understanding modules that have lan-
guage models (LMs) for ASR and LU models
579
(LUMs), and a selection module that selects the
most reliable speech understanding result from
multiple candidates in the MLMU framework.
When the amount of available training data is
small, and an LUM and the selection module are
trained on the same data set, they are trained un-
der a closed-set condition, and thus the training
data for the selection module include too many
correct understanding results. In such cases, the
data need to be divided into subdata sets to avoid
overfitting. On the other hand, when the amount
of available training data is large, so that overfit-
ting does not occur, all available data should be
used to train each statistical module to prepare as
much training data as possible.
We therefore develop a method for switching
data allocation policies. More specifically, two
points are automatically determined at which sta-
tistical modules with more parameters start to be
trained. As a result, better overall performance
is achieved at every point while the amount of
training data increases, compared with all combi-
nations of a single ASR module and a single LU
module.
2 Related Work
It is important to consider the amount of available
training data when designing a speech understand-
ing module. Many statistical LU methods have
been studied, e.g., (Wang and Acero, 2006; Jeong
and Lee, 2006; Raymond and Riccardi, 2007;
Hahn et al, 2008; Dinarelli et al, 2009). They
generally outperform grammar-based LU meth-
ods when a sufficient amount of training data is
available; but sufficient training data are not nec-
essarily available during rapid prototyping. Sev-
eral LU methods were constructed using a small
amount of training data (Fukubayashi et al, 2008;
Dinarelli et al, 2009). Fukubayashi et al (2008)
constructed an LU method based on the weighted
finite state transducer (WFST), in which filler
transitions accepting arbitrary inputs and transi-
tion weights were added to a hand-crafted FST.
This method is placed between a grammar-based
method and a statistical method because a sta-
tistically selected weighting scheme is applied
to a hand-crafted grammar model. Therefore,
the amount of training data can be smaller com-
pared with general statistical LU methods, but this
method does not outperform them when plenty of
training data are available. Dinarelli et al (2009)
used a generative model for which overfitting is
less prone to occur than discriminative models
when the amount of training data is small, but
they did not use a grammar-based model, which is
expected to achieve reasonable performance even
when the amount of training data is very small.
Raymond et al (2007) compared the perfor-
mances of statistical LU methods for various
amounts of training data. They used a statis-
tical finite-state transducer (SFST) as a genera-
tive model and a support vector machine (SVM)
and conditional random fields (CRF) as discrim-
inative models. The generative model was more
effective when the amount of data was small,
and the discriminative models were more effec-
tive when it was large. This shows that the perfor-
mance of an LUmethod depends on the amount of
training data available, and therefore, LU meth-
ods need to be switched automatically. Wang et
al. (2002) developed a two-stage speech under-
standing method by applying statistical methods
first and then grammatical rules. They also ex-
amined the performance of the statistical methods
at their first stage for various amounts of train-
ing data and confirmed that the performance is not
very high when a small amount of data is used.
Schapire et al (2005) showed that accuracy
of call classification in spoken dialogue systems
improved by incorporating hand-crafted prior
knowledge into their boosting algorithm. Their
idea is the same as ours in that they improve the
system?s performance by using hand-crafted hu-
man knowledge while only a small amount of
training data is available. We furthermore solve
the data allocation problem because there are mul-
tiple statistical models to be trained in speech
understanding, while their call classification has
only one statistical model.
3 MLMU Framework
MLMU is the framework for selecting the most
reliable speech understanding result from multi-
ple speech understanding modules (Katsumaru et
al., 2009). In this paper, we furthermore adapt the
selection module to the amount of available train-
580
LU model
#1
Language
model #1
LU
modules
ASR
modules
Result:
1
CM
N
CM
MN
CM
?
i
i
CMmaxarg
N
1
Selection module
LU
results
ASR
results
Utterance
ASR: automatic speech recognition
LU: language understanding
CM: confidence measure
M ?
M ?
Speech understanding
Language
model #2
Language
model #N
LU model
#2
LU model
#M
Logistic
regression #1
Logistic
regression #N
Logistic
regression #
Figure 1: Overview of speech understanding framework MLMU
ing data. More specifically, the allocation policy
of training data is changed and thus appropriate
LMs and LUMs are selected as its result.
An overview of MLMU is shown in Figure 1.
MLMU uses multiple LMs for ASR and multi-
ple LUMs and selects the most reliable speech un-
derstanding result from all combinations of them.
We denote a speech understanding module as SUi
(i = 1, . . . , n). Its result is a semantic representa-
tion consisting of a set of concepts. The concept is
either a semantic slot and its value or an utterance
type. Note that n = N ? M , when N LMs and
M LUMs are used. The confidence measure per
utterance for a result of i-th speech understanding
module SUi is denoted as CMi. The speech un-
derstanding result having the highest confidence
measure is selected as the final result for the ut-
terance. That is, the result is the output of SUm
where m = argmaxi CMi.
The confidence measure is calculated by logis-
tic regression based on the features of each speech
understanding result. A logistic regression func-
tion is constructed for each speech understanding
module SUi:
CMi =
1
1 + e?(ai1Fi1+...+ai7Fi7+bi) . (1)
Parameters ai1, . . . , ai7 and bi are determined by
using training data. In the training phase, teacher
signal 1 is given when a speech understanding re-
sult is completely correct; that is, when no error is
contained in the result. Otherwise, 0 is given. We
use seven features, Fi1, Fi2, . . . , Fi7, as indepen-
dent variables. Each feature value is normalized
Table 1: Features of speech understanding result
obtained from SUi
Fi1: Acoustic score normalized by utterance length
Fi2: Difference between Fi1 and normalized acoustic
scores of verification ASR
Fi3: Average concept CM in understanding result
Fi4: Minimum concept CM in understanding result
Fi5: Number of concepts in understanding result
Fi6: Whether any understanding result is obtained
Fi7: Whether understanding result is yes/no
CM: confidence measure
so as to make its mean zero and its variance one.
The features used are listed in Table 1. Com-
pared with those used in our previous paper (Kat-
sumaru et al, 2009), we deleted ones that were
highly correlated with other features and added
ones regarding content of the speech understand-
ing results. Features Fi1 and Fi2 are obtained
from an ASR result. Another ASR with a gen-
eral large vocabulary LM is executed for verifying
the i-th ASR result. Fi2 is the difference between
its score and Fi1 (Komatani et al, 2007). These
two features represent the reliability of the ASR
result. Fi3 and Fi4 are calculated for each concept
in the LU result on the basis of the posterior prob-
ability of the 10-best ASR candidates (Komatani
and Kawahara, 2000). Fi5 is the number of con-
cepts in the LU result. This feature is effective be-
cause the LU results of lengthy utterances tend to
be erroneous in a grammar-based LU. Fi6 repre-
sents the case when an ASR result is not accepted
by the subsequent LU module. In such cases, no
speech understanding result is obtained, which is
581
U1: It is June ninth.
ASR result:
- grammar ?It is June ninth.?
- N-gram ?It is June noon and?
LU result:
- grammar + FST ?month:6 day:9 type:refer-time?
- N-gram + WFST ?month:6 type:refer-time?
U2: I will borrow it on twentieth.
(Underlined part is out-of-grammar.)
ASR result:
- grammar ?Around two pm on twentieth.?
- N-gram ?Around two at ten on twentieth.?
LU result:
- grammar + FST ?day:20 hour:14 type:refer-time?
- N-gram + WFST ?day:20 type:refer-time?
Combination of LM and LUM is denoted as ?LM+LUM?.
Figure 2: Example of speech understanding re-
sults in MLMU framework
regarded as an error. Fi7 is added because affirma-
tive and negative responses, typically ?Yes? and
?No?, tend to be correctly recognized and under-
stood.
Figure 2 depicts an example when multiple
ASRs based on LMs and multiple LUs are used.
In short, the correct speech understanding result is
obtained from a different combination of LMs and
LUMs.
4 Automatic Allocation of Training Data
Using Change in Coefficients
The training data need to be allocated to the
speech understanding modules (i.e., statistical LM
and statistical LUM) and the selection module. If
more data are allocated to the ASR and LU mod-
ules, the performances of these modules are im-
proved, but the overall performance is degraded
because of the low performance of the selection
module. On the other hand, even if more training
data are allocated to the selection module, the per-
formance of each ASR and LU module remains
low.
4.1 Allocation Policy
We focus on the convergence of the logistic re-
gression functions when the amount of training
data increases. The convergence is defined as
the change in their coefficients, which will appear
later as Equation 2, and determines two points
1. All data are used to
train selection modules
2. Data are allocated to SU
and selection modules
3. Data are
not divided
No No
Yes
Yes
Selection module 
first converges?
No over-fitting
occurs?
Amount of training data increases
SU: speech understanding
Figure 3: Flowchart of data allocation
during the increase in training data, and thus three
phases are defined. The flowchart of data alloca-
tion is depicted in Figure 3. The three phases are
explained below.
In the first phase, the first priority is given to
the selection module. This is because the lo-
gistic regression functions used in the selection
module converge with relatively less training data
than those in the statistical ASR and LU mod-
ules for speech understanding; there are eight pa-
rameters for each logistic regression function as
shown in Equation 1, far fewer than for other sta-
tistical models such as N-gram and CRF. The out-
put from a speech understanding module that em-
ploys grammar-based LM and LUM would be the
most reliable in many cases because its perfor-
mance is better than that of other statistical mod-
ules when a very small amount of training data is
available. As a result, equivalent or better perfor-
mance would be achieved than methods using a
single ASR module and a single LU module.
In the second phase, the training data are also
allocated to the speech understanding modules af-
ter the selection module converges. This aims
to improve the performance of the speech under-
standing modules by allocating as much training
data to them as possible. The amount of train-
ing data is fixed in this phase to the amount al-
located to the selection module determined in the
first phase. The remaining data are used to train
the speech understanding modules.
When the performances of all the speech under-
standing modules stabilize, the allocation phase
proceeds to the third one. After this point, we
hypothesize that overfitting does not occur in this
phase because plenty of training data are avail-
able. All available data are used to train all mod-
582
ules without dividing the data in this phase.
4.2 Determining When to Switch Allocation
Policies
Automatic switching from one phase to the next
requires the determination of two points in the
number of training utterances: when the selec-
tion module first converges (konlysel) and when
the speech understanding modules all become sta-
ble (knodiv). These points are determined by fo-
cusing on the changes in the coefficients of the
logistic regression functions when the number of
utterances used as training data increases. We ob-
serve the sum of the changes in the coefficients of
the functions and then identify the points at which
the changes converge. The points are determined
individually by the following algorithm.
Step 1 Construct two logistic regression func-
tions for speech understanding module SUi
by using k and (k + ?k) utterances out of
kmax utterances, where kmax is the amount
of training data available.
Step 2 Calculate the change in coefficients from
the two logistic regression functions by
?i(k) =
?
j
|aij(k + ?k) ? aij(k)|
+|bi(k + ?k) ? bi(k)|, (2)
where aij(k) and bi(k) denote the param-
eters of the logistic regression functions,
shown in Equation 1, for speech understand-
ing module SUi, when k utterances are used
to train the functions.
Step 3 If ?i(k) becomes smaller than threshold
?, consider that the training of the functions
has converged, and record this k as the point
of convergence. If not, return to Step 1 after
k ? k + ?k.
The ?k is the minimum unit of training data con-
taining various utterances. We set it as the number
of utterances in one dialogue session, whose aver-
age was 17. Threshold ? was set to 8, which corre-
sponds to the number of parameters in the logistic
regression functions. No experiments were con-
ducted to determine if better performance could
be achieved with other choices of ?1.
The first point, konlysel, is determined using the
speech understanding module that uses no training
data. Specifically, we used ?grammar+FST? as
method SUi. Here, ?LM+LUM? denotes a com-
bination of LM for ASR and LUM. If the func-
tion converges at k utterances, we set konlysel to
k and fix the k utterances as training data used by
the selection module. The remaining (kmax ? k)
utterances are allocated to the speech understand-
ing modules, that is, the LMs and LUMs. Note
that if k becomes equal to kmax before ?i con-
verges, all training data are allocated to the selec-
tion module; that is, no data are allocated to the
LMs and LUMs. In this case, no output is ob-
tained from statistical speech understanding mod-
ules, and only outputs from the grammar-based
modules are used.
The second point, knodiv , is determined on the
basis of the speech understanding module that
needs the largest amount of data for training. The
amount of data needed depends on the number of
parameters. Specifically, we used ?N-gram+CRF?
as SUi in Equation 2. If the function converges,
we hypothesize that the performance of all the
speech understanding modules stabilize and thus
overfitting does not occur. We then stop the divi-
sion of training data, and use all available data to
train the statistical modules.
5 Experimental Evaluation
5.1 Target Data and Implementation
We used a data set previously collected through
actual dialogues with a rent-a-car reservation sys-
tem (Nakano et al, 2007) with 39 participants.
Each participant performed 8 dialogue sessions,
and 5900 utterances were collected in total. Out
of these utterances, we used 5240 for which the
automatic voice activity detection (VAD) results
agreed with manual annotation. We divided the
utterances into two sets: 2121 with 16 participants
as training data and 3119 with 23 participants as
the test data.
1We do not think the value is very critical after seeing the
results shown in Figure 4.
583
We constructed another rent-a-car reservation
system to evaluate our allocation method. The
system included two language models (LMs)
and four language understanding models (LUMs).
That is, eight speech understanding results in total
were obtained. The two LMs were a grammar-
based LM (?grammar?, hereafter) and a domain-
specific statistical LM (?N-gram?). The grammar
model was described by hand to be equivalent to
the FST model used in LU. The N-gram model
was a class 3-gram and was trained on a tran-
scription of the available training data. The vo-
cabulary size was 281 for the grammar model and
420 for the N-gram model when all the training
data were used. The ASR accuracies of the gram-
mar and N-gram models were 67.8% and 90.5%
for the training data and 66.3% and 85.0% for the
test data when all the training data were used. We
used Julius (ver. 4.1.2) as the speech recognizer
and a gender-independent phonetic-tied mixture
model as the acoustic model (Kawahara et al,
2004). We also used a domain-independent statis-
tical LM with a vocabulary size of 60250, which
was trained on Web documents (Kawahara et al,
2004), as the verification model.
The four LUMs were a finite-state transducer
(FST) model, a weighted FST (WFST) model,
a keyphrase-extractor (Extractor) model, and a
conditional random fields (CRF) model. In the
FST-based LUM, the FST was constructed by
hand. The WFST-based LUM is based on the
method developed by Fukubayashi et al (2008).
The WFSTs were constructed by using the MIT
FST Toolkit (Hetherington, 2004). The weight-
ing scheme used for the test data was selected by
using training data (Fukubayashi et al, 2008). In
the extractor-based LUM, as many parts as pos-
sible in the ASR result were simply transformed
into concepts. As the CRF-based LUM, we used
open-source software, CRF++2, to construct the
LUM. As its features, we use a word in the ASR
result, its first character, its last character, and the
ASR confidence of the word. Its parameters were
estimated by using training data.
The metric used for speech understanding per-
formance was concept understanding accuracy,
2http://crfpp.sourceforge.net/
Table 2: Absolute degradation in oracle accuracy
when each module was removed
Case (A) (B)
With all modules (%) 86.6 90.1
w/o grammar ASR -12.0 -1.1
w/o N-gram ASR -6.1 -7.7
w/o FST LUM -0.4 0.0
w/o WFST LUM -1.2 -0.5
w/o Extractor LUM -0.1 0.0
w/o CRF LUM -0.6 -3.7
(w/o FST & Extractor LUMs) -1.0 -0.1
(A): 141 utterances with 1 participant
(B): 2121 utterances with 16 participants
defined as
1 ? SUB + INS + DEL
no. of concepts in correct results,
where SUB, INS, and DEL denote the numbers of
substitution, insertion, and deletion errors.
5.2 Effectiveness of Using Multiple LMs and
LUMs
We investigated how much the performance of our
framework degraded when one ASR or LU mod-
ule was removed. We used the oracle accuracies,
i.e., when the most appropriate result was selected
by hand. The result reveals the contribution of
each ASR and LU module to the performance of
the framework. A module is regarded as more im-
portant when the accuracy is degraded more when
it is removed than when another one is removed.
Two cases (A) and (B) were defined: when the
amount of available training data was (A) small
and (B) large. We used 141 utterances with 1 par-
ticipant for case (A) and 2121 utterances with 16
participants for case (B). The results are shown in
Table 2.
When a small amount of training data was
available (case (A)), the accuracy was degraded by
12.0 points when the grammar-based ASRmodule
was removed and 6.1 points when the N-gram-
based ASR module was removed. The accuracy
was thus degraded substantially when either ASR
module was removed. This indicates that the two
ASR modules work complementarily.
584
020
40
60
80
100
120
140
160
180
200
0 100 200 300 400 500
C
h
a
n
g
e
s
 
i
n
 
c
o
e
f
f
i
c
i
e
n
t
s
Number of training utterances available
(a) grammar+FST
0
20
40
60
80
100
120
140
160
180
200
0 100 200 300 400 500
C
h
a
n
g
e
s
 
i
n
 
c
o
e
f
f
i
c
i
e
n
t
s
Number of training utterances available
(b) N-gram+CRF
Figure 4: Change in the sum of coefficients ?i when amount of training data increases (?LM+LUM?
denotes combination of LM and LUM)
On the other hand, when a large amount of
training data was available (case (B)), the ac-
curacy was degraded by 1.1 points when the
grammar-based ASR was removed. This means
that it became less important when there are
plenty of training data because the coverage of the
N-gram-based ASR became wider. In short, espe-
cially when the amount of training data is smaller,
speech understanding modules based on a hand-
crafted grammar are more important because of
the low performance of statistical modules.
Concerning the LUMs, the accuracy was de-
graded when any of the LUM modules was re-
moved when a small amount of training data was
available. When a large amount of training data
was available, the module based on CRF in par-
ticular became more important.
5.3 Results and Evaluation of Automatic
Allocation
Figure 4 shows the change in the sum of the co-
efficients, ?i, with the increase in the amount of
training data. In Figure 4(a), the change was very
large while the amount of training data was small,
and decreased dramatically and converged around
one hundred utterances. By applying ? (=8) to ?i,
we set 111 utterances as the first point, konlysel,
up to which all the training data are allocated to
the selection module, as described in Section 4.1.
Similarly, from the results shown in Figure 4(b),
we set 207 utterances as the second point, knodiv,
from which the training data are not divided.
To evaluate our method for allocating training
55
60
65
70
75
80
85
90
50 100 200 400 800 1600
C
o
n
c
e
p
t
 
u
n
d
e
r
s
t
a
n
d
i
n
g
 
a
c
c
u
r
a
c
y
 
[
%
]
Number of training utterances available
Our method
Na?ve allocation
No division
Figure 5: Results of allocation methods
data, we compared it with two baseline methods:
? No-division method: All data available at
each point were used to train both the speech
understanding modules and the selection
module. That is, the same data set was used
to train them.
? Naive-allocation method: Training data
available at each point were allocated equally
to the speech understanding modules and the
selection module.
As shown in Figure 5, our method had the best
concept understanding accuracy when the amount
of training data was small, that is, up to about
278 utterances. This indicates that our method for
allocating the available training data is effective
when the amount of training data is small.
This result is explained more specifically by us-
585
Table 3: Concept understanding accuracy for 141
utterances
Accuracy (%)
Our method 77.9
Naive allocation 73.5
No division 74.1
ing the case in which 141 utterances were used as
the training data. 111 (= konlysel) were secured to
train the selection module and 30 utterances were
allocated to train the speech understanding mod-
ules. As shown in Table 3, the accuracy with our
method was 3.8 points higher than that with the
no-division baseline method. This was achieved
by avoiding the overfitting of the logistic regres-
sion functions; i.e., the data input to the functions
became similar to the test data due to allocation,
so the concept understanding accuracy for the test
set was improved. The accuracy with our method
was 4.4 points higher than that with the naive al-
location baseline method. This was because the
amount of training data allocated to the selection
module was less than our method, and accordingly
the selection module was not trained sufficiently.
5.4 Comparison with methods using a single
ASR and a single LU
Figure 6 plots concept understanding accuracy
with our method against baseline methods using
a single ASR module and a single LU module for
various amounts of training data. Each module for
comparison was constructed by using all available
training data at each point while training data in-
creased; i.e., the same condition as our method.
The accuracies of only three speech understand-
ing modules are shown in the figure, out of the
eight obtained by combining two LMs for ASR
and four LUMs. These three are the ones with the
highest accuracies while the amount of training
data increased. Our method switched the alloca-
tion phase at 111 and 207 utterances, as described
in Section 5.3.
Our method performed equivalently or better
than all baseline methods even when only a small
amount of training data was available. As a result,
our method outperformed all the baseline methods
55
60
65
70
75
80
85
50 100 200 400 800 1600
C
o
n
c
e
p
t
 
u
n
d
e
r
s
t
a
n
d
i
n
g
 
a
c
c
u
r
a
c
y
 
[
%
]
Number of training utterances available
our method
grammar+FST
N-gram+WFST
N-gram+CRF
Figure 6: Comparison with baseline methods us-
ing single speech understanding
at every point while training data increase.
6 Conclusion
We developed a method to automatically allo-
cate training data to statistical modules so as to
avoid performance degradation caused by overfit-
ting. Experimental evaluation showed that speech
understanding accuracies achieved by our method
were equivalent or better than the baseline meth-
ods based on all combinations of a single ASR
module and a single LU module at every point
while training data increase. This includes a case
when a very small amount of training data is avail-
able. We also showed empirically that the training
data should be allocated while an amount of train-
ing data is not sufficient. Our method allocated
available training data on the basis of our alloca-
tion policy described in Section 4.1, and outper-
formed the two baselines where the training data
were equivalently allocated and not allocated.
When plenty of training data were available,
there was no difference between our method and
the speech understanding method that requires the
most training data, i.e., N-gram+CRF, as shown in
Figure 6. It is possible that our method combin-
ing multiple speech understanding modules would
outperform it as Schapire et al (2005) reported.
In their data, there were some examples that only
a hand-crafted rules can parse. Including such a
task as more complicated language understanding
grammar is required, verification of our method in
other tasks is one of the future works.
586
References
Dinarelli, Marco, Alessandro Moschitti, and Giuseppe
Riccardi. 2009. Re-Ranking Models for Spoken
Language Understanding. In Proc. European Chap-
ter of the Association for Computational Linguistics
(EACL), pages 202?210.
Fukubayashi, Yuichiro, Kazunori Komatani, Mikio
Nakano, Kotaro Funakoshi, Hiroshi Tsujino, Tet-
suya Ogata, and Hiroshi G. Okuno. 2008. Rapid
prototyping of robust language understanding mod-
ules for spoken dialogue systems. In Proc. Interna-
tional Joint Conference on Natural Language Pro-
cessing (IJCNLP), pages 210?216.
Hahn, Stefan, Patrick Lehnen, and Hermann Ney.
2008. System Combination for Spoken Language
Understanding. In Proc. Annual Conference of the
International Speech Communication Association
(INTERSPEECH), pages 236?239.
Hetherington, Lee. 2004. The MIT Finite-State Trans-
ducer Toolkit for Speech and Language Processing.
In Proc. Int?l Conf. Spoken Language Processing
(ICSLP), pages 2609?2612.
Jeong, Minwoo and Gary Geunbae Lee. 2006. Ex-
ploiting non-local features for spoken language un-
derstanding. In Proc. COLING/ACL 2006 Main
Conference Poster Sessions, pages 412?419.
Katsumaru, Masaki, Mikio Nakano, Kazunori Ko-
matani, Kotaro Funakoshi, Tetsuya Ogata, and Hi-
roshi G. Okuno. 2009. Improving speech un-
derstanding accuracy with limited training data us-
ing multiple language models and multiple under-
standing models. In Proc. Annual Conference of
the International Speech Communication Associa-
tion (INTERSPEECH), pages 2735?2738.
Kawahara, Tatsuya, Akinobu Lee, Kazuya Takeda,
Katsunobu Itou, and Kiyohiro Shikano. 2004. Re-
cent progress of open-source LVCSR engine Julius
and Japanese model repository. In Proc. Int?l Conf.
Spoken Language Processing (ICSLP), pages 3069?
3072.
Komatani, Kazunori and Tatsuya Kawahara. 2000.
Flexible mixed-initiative dialogue management us-
ing concept-level confidence measures of speech
recognizer output. In Proc. Int?l Conf. Computa-
tional Linguistics (COLING), pages 467?473.
Komatani, Kazunori, Yuichiro Fukubayashi, Tetsuya
Ogata, and Hiroshi G. Okuno. 2007. Introducing
utterance verification in spoken dialogue system to
improve dynamic help generation for novice users.
In Proc. 8th SIGdial Workshop on Discourse and
Dialogue, pages 202?205.
Nakano, Mikio, Yuka Nagano, Kotaro Funakoshi,
Toshihiko Ito, Kenji Araki, Yuji Hasegawa, and Hi-
roshi Tsujino. 2007. Analysis of user reactions to
turn-taking failures in spoken dialogue systems. In
Proc. 8th SIGdial Workshop on Discourse and Dia-
logue, pages 120?123.
Raymond, Christian and Giuseppe Riccardi. 2007.
Generative and Discriminative Algorithms for Spo-
ken Language Understanding. In Proc. Annual
Conference of the International Speech Communi-
cation Association (INTERSPEECH), pages 1605?
1608.
Shapire, Robert E., Marie Rochery, Mazin Rahim, and
Narendra Gupta. 2005. Boosting with prior knowl-
edge for call classification. IEEE Trans. on Speech
and Audio Processing, 13(2):174?181.
Wang, Ye-Yi and Alex Acero. 2006. Discrimina-
tive models for spoken language understanding. In
Proc. Int?l Conf. Spoken Language Processing (IN-
TERSPEECH), pages 2426?2429.
Wang, Ye-Yi, Alex Acero, Ciprian Chelba, Brendan
Frey, and Leon Wong. 2002. Combination of Sta-
tistical and Rule-based Approaches for Spoken Lan-
guage Understanding. In Proc. Int?l Conf. Spoken
Language Processing (ICSLP), pages 609?612.
587
