Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 216?223,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Incremental Decoding for Phrase-based Statistical Machine Translation
Baskaran Sankaran, Ajeet Grewal and Anoop Sarkar
School of Computing Science
Simon Fraser University
8888 University Drive
Burnaby BC. V5A 2Y1. Canada
{baskaran, asg10, anoop}@cs.sfu.ca
Abstract
In this paper we focus on the incremental
decoding for a statistical phrase-based ma-
chine translation system. In incremental
decoding, translations are generated incre-
mentally for every word typed by a user,
instead of waiting for the entire sentence
as input. We introduce a novel modifi-
cation to the beam-search decoding algo-
rithm for phrase-based MT to address this
issue, aimed at efficient computation of fu-
ture costs and avoiding search errors. Our
objective is to do a faster translation dur-
ing incremental decoding without signifi-
cant reduction in the translation quality.
1 Introduction
Statistical Machine Translation has matured sig-
nificantly in the past decade and half, resulting in
the proliferation of several web-based and com-
mercial translation services. Most of these ser-
vices work on sentence or document level, where
a user enters a sentence or chooses a document
for translation, which are then translated by the
servers. Translation in such typical scenarios is
still offline in the sense that the user input and
translation happen sequentially without any inter-
action between the two phases.
In this paper we study decoding for SMT with
the constraint that translations are to be gener-
ated incrementally for every word typed in by the
user. Such a translation service can be used for
language learning, where the user is fluent in the
target language and experiments with many differ-
ent source language sentences interactively, or in
real-time translation environments such as speech-
speech translation or translation during interactive
chats.
We use a phrase-based decoder similar to
Moses (Koehn et al, 2007) and propose novel
modifications in the decoding algorithm to tackle
incremental decoding. Our system maintains a
partial decoder state at every stage and uses it
while decoding for each newly added word. As
the decoder has access only to the partial sentence
at every stage, the future costs change with ev-
ery additional word and this has to be taken into
account while continuing from an existing partial
decoder state. Another major issue is that as incre-
mental decoding is provided new input one word
at at time, some of the entries that were pruned out
at an earlier decoder state might later turn out to
better candidates resulting in search errors com-
pared to decoding the entire sentence at once. It
is to be noted that, the search error problem is re-
lated to the inability to compute full future cost
in incremental decoding. Our proposed modifica-
tions address these twin challenges and allow for
efficient incremental decoding.
2 Incremental Decoding
2.1 Beam Search for Phrase-based SMT
In this section we review the usual beam search de-
coder for phrase-based MT because we present our
modifications for incremental decoding using the
same notation. Beam search decoding for phrase-
based SMT (Koehn, 2004) begins by collecting
the translation options from the phrase table for all
possible phrases of a given input sentence and pre-
computes the future cost for all possible contigu-
ous sequences in the sentence. The pseudo-code
for the usual beam-search decoding algorithm is
illustrated in Algorithm 1.
The decoder creates n bins for storing hypothe-
ses grouped by the number of source words cov-
ered. Starting from a null hypothesis in bin 0, the
decoder iterates through bins 1 though n filling
them with new hypotheses by extending the en-
tries in the earlier bins.
A hypothesis contains the target words gener-
ated (e), the source positions translated so far (f )
commonly known as coverage set and the score
of the current translation (p) computed by the
weighted log-linear combination of different fea-
ture functions. It also contains a back-pointer to
216
Algorithm 1 Phrase-based Decoder pseudocode
(Koehn, 2004)
1: Given: sentence Sn: s1s2...sn of length n
2: Pre-compute future costs for all contiguous
sequences
3: Initialize bins bi where i = 1 . . . n
4: Create initial hypothesis: {e : (), f : (), p :
1.0}
5: for i = 1 to n do
6: for hyp ? bi do
7: for newHyp that extends hyp do
8: nf := num src words covered by
newHyp
9: Add newHyp to bin bnf
10: Prune bin bnf using future costs
11: Find best hypothesis in bn
12: Output best path that leads to best hypothesis
its parent hypothesis in the previous state and other
information used for pruning and computing cost
in later iterations.
As a new hypothesis is generated by extending
an existing hypothesis with a new phrase pair, de-
coder updates the associated information such as
coverage set, the target words generated, future
cost (for translating rest of the source words) and
its translation score. For example, consider Span-
ish to English translation: for the source sentence
Maria no daba una bofetada, the hypothesis {e :
(Mary), f : (1), p : 0.534} which is the hypoth-
esis that covers Maria can be extended to a new
hypothesis {e : (Mary, slap), f : (1, 3, 4, 5), p :
0.043} by choosing a new phrase pair (daba una
bofetada, slap) covering the source phrases Maria
and daba una bofetada. The probability score is
obtained by weighted log-linear sum of the fea-
tures of the phrases contained in the derivation so
far.
An important aspect of beam search decoding
is the pruning away of low-scoring hypotheses in
each bin to reduce the search space and thus mak-
ing the decoding faster. To do this effectively,
beam search decoding uses the future cost of a hy-
pothesis together with its current cost. The future
cost is an estimate of the translation cost of the
input words that are yet to be translated, and is
typically pre-computed for all possible contiguous
sequences in the input sentence before the decod-
ing step. The future cost prevents the any hypothe-
ses that are low-scoring, but potentially promising,
from being pruned.
2.2 Incremental Decoder - Challenges
Our goal for the incremental decoder (ID) is to
generate output translations incrementally for par-
tial phrases as the source sentence is being input
by the user. We assume white-space to be the word
delimiter and the partial sentence is decoded for
every encounter of the space character. We further
assume the return key to mark end-of-sentence
(EOS) and use it to compute language model score
for the entire sentence.
As we noted above, future costs cannot be pre-
computed as in regular decoding because the com-
plete input sentence is not known while decod-
ing incrementally. Thus the incremental decoder
can only use a partial future cost until the EOS
is reached. The partial future cost could result
in some of the potentially better candidates being
pruned away in earlier stages. This leads to search
errors and result in lower translation quality.
2.3 Approach
We use a modified beam search for incremental
decoding (ID) and the two key modifications are
aimed at addressing the issues of future cost and
search errors. Beam search for ID begins with
a single bin for the first word and more bins are
added as the sentence is completed by the user.
Our approach requires that the decoder states for
the partial source sentence can be stored in a way
that allows efficient retrieval. It also maintains a
current decoder state, which includes all the bins
and the hypotheses contained in them, all pertain-
ing to the present sentence.
At each step ID goes through a pre-process
phase, where it recomputes the partial future costs
for all the spans accounting for the new word and
updates the current decoder state with new partial
future costs. It then generates new hypotheses into
all the earlier bins and in the newly created us-
ing any new phrases (resulting from the new word
added by the user) not used earlier.
Algorithm 2 shows the pseudocode of our incre-
mental decoder. Given a partial sentence Si1 ID
starts with the pre-process phase illustrated sepa-
rately in algorithm 3. We use Ptype(l) to denote
phrases of length l words and Htype to denote the
set of hypotheses; in both cases type correspond to
either old or new, indicating if it was not known in
the previous decoding state or not.
1we use Si and si to denote a i word partial sentence and
ith word in a (partial) sentence respectively
217
Algorithm 2 Incremental Decoder pseudocode
1: Input: (partial) sentence Sp: s1s2...si?1si
with ls words where si is the new word
2: PreProcess(Sp) (Algorithm 3)
3: for every bin bj in (1 . . . i) do
4: Update future cost and cover set ? Hold
5: Add any new phrase of length bj (subject to
d)
6: for bin bk in (bj?MaxPhrLen . . . bj?1) do
7: Generate Hnew for bj by extending:
8: every Hold with every other Pnew(bj ?
bk)
9: every Hnew with every other Pany(bj ?
bk)
10: Prune bin bj
Algorithm 3 PreProcess subroutine
1: Input: partial sentence Sp of length ls
2: Retrieve partial decoder object for Sp?1
3: Identify possible Pnew (subject to Max-
PhrLen)
4: Recompute fc for all spans in 1...ls
5: for every Pnew in local phrase table do
6: Load translation options to table
7: for every Pold in local phrase table do
8: Update fc with the recomputed cost
Given Si, the pre-process phase extracts the new
set of phrases (Pnew) for the ith word and adds
them to the existing phrases (Pold). It then recom-
putes the future-cost (fc) for all the contiguous se-
quences in the partial input and updates existing
entries in the local copy of phrase table with new
fc.
In decoding phase, ID generates new hypothe-
ses in two ways: i) by extending the existing hy-
potheses Hold in the previous decoder state Si?1
with new phrases Pnew and ii) by generating new
hypotheses Hnew that are unknown in the previous
state.
The main difference between incremental de-
coding and regular beam-search decoding is inside
the two ?for? loops corresponding to lines 3? 9 in
algorithm 2. In the outer loop each of the existing
hypotheses are updated to reflect the recomputed
fc and coverage set. Any new phrases belonging
to the current bin are also added to it2.
2Based on our implementation of lazier cube pruning they
are added to a priority queue, the contents of which are
flushed into the bin at the end of inner for-loop and before
the pruning step
Hypothesis surfaces
P. Queue
Hypstack
hyp 2hyp 1
A single surface
     
     
     
     
     





Figure 1: Illustration of Lazier Cube Pruning
The inner for-loop corresponds to the extension
of hypotheses sets (grouped by same coverage set)
to generate new hypotheses. Here a distinction is
made between hypotheses Hold corresponding to
previous decoder state Sp?1 and hypotheses Hnew
resulting from the addition of word si. Hold is ex-
tended only using the newly found phrases Pnew,
whereas the newer hypotheses are processed as in
regular beam-search.
2.4 Lazier Cube Pruning
We have adapted the pervasive lazy algorithm
(or ?lazier cube pruning?) proposed originally for
Hiero-style systems by (Pust and Knight, 2009)
for our phrase-based system. This step corre-
sponds to the lines 5?9 of algorithm 2 and allows
us to only generate as many hypotheses as speci-
fied by the configurable parameters, beam size and
beam threshold. Figure 1 illustrates the process of
lazier cube pruning for a single bin.
At the highest level it uses a priority queue,
which is populated by the different hyper-edges
or surfaces3, each corresponding to a pair of hy-
potheses that are being merged to create a new
hypothesis. New hypotheses are generated iter-
atively, such that the hypothesis with the highest
score is chosen in each iteration from among dif-
ferent hyper-edges bundles.
However, this will lead to search errors as have
been observed earlier. Any hyper-edge that has
been discarded due to poor score in an early stage
might later become a better candidate. The prob-
lem worsens further when using smaller beam
sizes (for interactive decoding in real-time set-
tings, we even consider a beam size of 3). In
3Unlike Hiero-style systems, only two hypotheses are
merged in a phrase-based system and hence the term surface
218
the next section, we introduce the idea of delayed
pruning to reduce search errors.
3 Delayed Pruning
Delayed pruning (DP) in our decoder was inspired
by the well known fable about the race between
a tortoise and a hare. If the decoding is consid-
ered to be a race between competing candidate hy-
potheses with the winner being the best hypothe-
sis for Viterbi decoding or among the top-n candi-
dates for n-best decoding.4
In this analogy, a hypothesis having a poor
score, might just be a tortoise having a slow start
(due to a bad estimate of the true future cost for
what the user intends to type in the future) as op-
posed to a high scoring hare in the same state.
Pruning such hypotheses early on is not risk-free
and might result in search errors. We hypothe-
size that, given enough chance it might improve its
score and move ahead of a hare in terms of trans-
lation score.
We implement DP by relaxing the lazier cube
pruning step to generate a small, fixed number
of hypotheses for coverage sets that are not rep-
resented in the priority queue and place them in
the bin. These hypotheses are distinct from the
usual top-k derivations. Thus, the resulting bin
will have entries from all possible hyper-edge bun-
dles. Though this reduces the search error prob-
lem, it leads to increasing number of possibilities
to be explored at later stages with vast majority
of them being worse hypotheses that should be
pruned away.
We use a two level strategy of delay and then
prune, to avoid such exponentially increasing
search space and at the same time to reduce search
error. At the delay level, the idea is to delay the
pruning for few promising tortoises, instead of re-
taining a fixed number of hypotheses from all un-
represented hyper-edges. We use the normalized
language model scores of the top-hypotheses in
each hyper-edge that is not represented in cube
pruning and based on a threshold (which is ob-
tained using a development test set), we selec-
tively choose few hyper-edge bundles and gen-
erate a small number (typically 1-3) of hypothe-
ses from each of them and flag them as tortoises.
4The analogy is used to compare two or more hypotheses
in terms of their translation scores and not speed. Though our
objective is faster incremental decoding, we use the analogy
here to compare the scores.
These tortoises are extended minimally at each it-
eration subject to their normalized LM score.
While this significantly reduces the total num-
ber of hypotheses at initial bins, many of these
tortoises might not show improvement even after
several bins. Thus at the prune level, we prune out
tortoises that does not improve beyond a threshold
number of bins called race course limit. The race
course limit signifies the number of steps a tortoise
has in order to get into the decoder beam.
When a tortoise improves in score and breaks
into the beam during cube pruning, it is de-
flagged as a tortoise and enters the regular decod-
ing stream. We found DP to be effective in reduc-
ing the search error for incremental decoder in our
experiments.
4 Evaluation and Discussion
The evaluation was performed using our own im-
plementation of the beam-search decoding algo-
rithms. The architecture of our system is similar
to Moses, which we also use for training and for
minimum error rate training (MERT) of the log-
linear model for translation (Och, 2003; Koehn et
al., 2007). Our features include 7 standard phrase-
based features: 4 translation model features, i.e.
p(f |e), p(e|f), plex(f |e) and plex(e|f), where e
and f are target and source phrases respectively;
features for phrase penalty, word penalty and lan-
guage model, and we do not include the reorder-
ing feature. We used Giza++ and Moses respec-
tively for aligning the sentences and training the
system. The decoder was written in Java and in-
cludes cube pruning (Huang and Chiang, 2007)
and lazier cube pruning (Pust and Knight, 2009)
functionalities as part of the decoder. Our de-
coder supports both regular beam search (similar
to Moses) and incremental decoding.
In our experiments we experimented various ap-
proaches for storing partial decoder states includ-
ing memcache and transactional persistence using
JDBM but found that the serialization and deseri-
alization of decoder objects directly into and from
the memory to work better in terms of speed and
memory requirements. The partial object is re-
trieved and deserialized from the memory when
required by the incremental decoder.
We evaluated the incremental decoder for trans-
lations between French and English (in both direc-
tions). We used the Workshop on Machine Trans-
lation shared task (WMT07) dataset for training,
219
optimizing and testing. The system was trained us-
ing Moses and the feature weights were optimized
using MERT. To benchmark our Java decoder, we
compare it with Moses by running it in regular
beam search mode. The Moses systems were also
optimized separately on the WMT07 devsets.
Apart from comparing our decoder with Moses
in regular beam search, we also compared the in-
cremental decoding with regular regular beam us-
ing our decoder. To make it comparable with
incremental decoding, we used the regular beam
search to re-decode the sentence fragments for ev-
ery additional word in the input sentence. We
measured the following parameters in our empir-
ical analysis: translation quality (as measured by
BLEU (Papineni et al, 2002) and TER (Snover et
al., 2006)), search errors and translation speed. Fi-
nally, we also measured the effect of different race
course limits on BLEU and decoding speed for in-
cremental decoding.
4.1 Benchmarking our decoder
In this section we compare our decoder with
Moses for regular beam search decoding. Table 1
gives the BLEU and TER for the two language
pairs. Our decoder implementation compares
favourably with Moses for Fr-En: the slightly bet-
ter BLEU and TER for our decoder in Fr-En is
possibly due to the minor differences in the con-
figuration settings. For En-Fr translation, Moses
performs better in both metrics. There are differ-
ences in the beam size between the two decoders,
in our system the beam size is set to 100 compared
to the default value of 1000 (the cube pruning pop
limit) in Moses; we are planning to explore this
and remove any other differences between them.
However based on our understanding of the Moses
implementation and our experiments, we believe
our decoder to be comparable in accuracy with the
Moses implementation. The numbers in the bold-
face are statistically significant at 95% confidence
interval.
4.2 Re-decoding v.s. Incremental decoding
We test our hypothesis that incremental decod-
ing can benefit by using partial decoder states for
decoding every additional word in the input sen-
tence. In order to do this, we run our incremen-
tal decoder in both regular beam search mode and
in incremental decoding mode. In regular beam
search mode, we forced the beam search decoder
to re-decode the sentence fragments for every ad-
ditional word and in incremental decoding mode,
we used the partial decoding states to incremen-
tally decode lastly added word. We then compare
the BLEU and TER scores between them to vali-
date our hypothesis.
We further test effectiveness of delayed prun-
ing (DP) in incremental decoding by comparing
it to the case where we turn off the DP. For in-
cremental decoding, we set the beam size and the
race course limit (for DP) to be 3. Additionally,
we used a threshold of?2.0 (in log-scale) for nor-
malized LM in the delay phase of DP, which was
obtained by testing on a separate development test
set.
We would like to highlight two observations
from the results in Table 2. First the regular beam
search indicate possible search errors due to the
small beam size (cube pruning pop limit) and the
BLEU scores has decreased by 0.56 for Fr-En
and by over 2.5 for En-Fr, than the scores cor-
responding to a beam size of 100 shown in Ta-
ble 1. Secondly, we find the incremental decoding
to perform better for the same beam size. How-
ever, incremental decoding without delay pruning
still seems to incur search errors when compared
with the regular decoding with a larger beam. De-
layed pruning alleviates this issue and improves
the BLEU and TER significantly. This we believe,
is mainly because the strategy to delay the pruning
retains the potentially better partial hypotheses for
every coverage set. It should be noted that results
in Table 2 pertain only to our decoder implemen-
tation and not with Moses.
We now give a comparative note between our
approach and the pruning strategy in regular beam
search. Delaying the hypothesis pruning is the im-
portant aspect in our approach to incremental de-
coding. In the case of regular beam search, the
hypotheses are pruned when they fall out of the
beam and the idea is to have a larger beam size
to avoid the early pruning of potentially good can-
didates. With the advent of cube pruning (Huang
and Chiang, 2007), the ?cube pruning pop limit?
(in Moses) determines the number of hypotheses
retained in each stack. In both the cases, it is pos-
sible that some of the coverage sets go unrepre-
sented in the stack due to poor candidate scores.
This is not desirable in the incremental decoding
setting as this might lead to search errors while
decoding a partial sentence.
Additionally, Moses offers an option (cube
220
Decoder
Fr-En En-Fr
BLEU TER BLEU TER
Moses 26.98 0.551 27.24 0.610
Our decoder 27.53 0.541 26.96 0.657
Table 1: Regular beam search: Moses v.s. Our decoder
Decoder
Fr-En En-Fr
BLEU TER BLEU TER
Re-decode w/ beam search 26.96 0.548 24.33 0.635
ID w/o delay pruning 27.01 0.547 25.00 0.618
ID w/ delay pruning 27.62 0.545 25.45 0.616
Table 2: BLEU and TER: Re-decoding v.s. Incremental Decoding (ID)
pruning diversity) to control the number of hy-
potheses generated for each coverage set (though
set to ?0? by default). It might be possible to use
this in conjunction with cube pruning pop limit as
an alternative to our delayed pruning in the incre-
mental decoding setting (with the risk of combina-
torial explosion in the search space).
In contrast, the delayed pruning not only avoids
search errors but also provides a dynamically man-
ageable search space (refer section 4.2.2) by re-
taining the best of the potential candidates. In a
practical scenario like real-time translation of in-
ternet chat, translation speed is an important con-
sideration. Furthermore, it is better to avoid large
number of candidates and generate only few best
ones, as only the top few translations will be used
by the system. Thus we believe our delayed prun-
ing approach to be a principled pruning strategy
that combines the different factors in an elegant
framework.
4.2.1 Search Errors
As BLEU only indirectly indicates the number
of search errors made by algorithm, we used a
more direct way of quantifying the search errors
incurred by the ID in comparison to regular beam
search. We define the search error to be the differ-
ence between the translation scores of the best hy-
potheses produced by the ID and the regular beam
search and then compute the mean squared error
(MSE) for the entire test set. We use this method
to compare ID in the two settings of delayed prun-
ing being turned off (using a smaller beam size
of 3 to simulate the requirements of near instanta-
neous translations in real-time environments) and
delayed pruning turned on. We compare the model
score in these cases with the model score for the
best result obtained from the regular beam search
decoder (using a larger beam of size 100).
Direction
Beam search against
Incremental Decoding
w/o DP w/ DP
Fr-En 0.3823 0.3235
En-Fr 1.1559 0.6755
Table 3: Search Errors in Incremental Decoding
The results are shown in Table 3 and as can be
clearly seen, ID shows much lesser mean square
error with the DP turned on than when it is turned
off. Together the BLEU and TER numbers and
the mean square search error show that delayed
pruning is useful in the incremental decoding set-
ting. Comparing the En-Fr and Fr-En results show
that the two language pairs show slightly different
characteristics but the experiments in both direc-
tions support our overall conclusions.
4.2.2 Speed
In this experiment, we set out to evaluate the
ID against the regular beam-search in which sen-
tence fragments are incrementally decoded for ad-
ditional words. In order compare with the in-
cremental decoder, we modified the regular de-
coder to decode the partial phrases, so that it re-
decodes the partial phrase from the scratch instead
of reusing the earlier state.
We ran the timing experiments on a Dell ma-
chine with an Intel Core i7 processor and 12 GB
memory, clocking 2.67 GHz and running Linux
(CentOS 5.3). We measured the time taken for de-
coding the fragment with every word added and
221
averaged it first over the sentence and then the en-
tire test set. The average time (in msecs) includes
the future cost computation for both. We also mea-
sured the average number of hypotheses for every
bin at the end of decoding a complete sentence,
which was also averaged over the test set.
The results in Table 4 show that the incremen-
tal decoder was significantly faster than the beam
search in re-decoding mode almost by a factor of
9 in the best case (for Fr-En). The speedup is pri-
marily due to two factors, i) computing the future
cost for the new phrases as opposed to computing
it for all the phrases and ii) using partial decoder
states without having to re-generate hypotheses
through the cube pruning step and the latencies
associated with computing LM scores for them.
The addition of delayed pruning slowed down the
speed at most by 7 msecs (for En-Fr). In addition,
delayed pruning can be seen generating far more
hypotheses than the other two cases. Clearly, this
is because of the delay in pruning the tortoises un-
til the race course limit. Even with such signifi-
cantly large number of hypotheses being retained
for every bin, DP results in improved speed (over
re-decoding from scratch) and better performance
by avoiding search errors (compared to the incre-
mental decoder that does not use DP).
4.3 Effect of Race course limit
Table 5 shows the effect of different race course
limits on translation quality measured using
BLEU. We generally expect the race course limit
to behave similar to the beam size as they both al-
low more hypotheses in the bin thereby reducing
search error although at the expense of increasing
decoding time.
However, in our experiments for Fr-En, we did
not find significant variations in BLEU for differ-
ent race course limits. This could be due to the
absence of long distance re-orderings between En-
glish and French and that the smallest race course
limit of 3 is sufficient for capturing all cases of lo-
cal re-ordering. As expected, we find the decoding
speed to slightly decrease and the average number
of hypotheses per bin to increase with the increas-
ing race course limit.
5 Related Work
Google5 does seem to perform incremental decod-
ing, but the underlying algorithms are not public
5translate.google.com
knowledge. They may be simply re-translating the
input each time using a fast decoder or re-using
prior decoder states as we do here.
Intereactive translation using text prediction
strategies have been studied well (Foster et al,
1997; Foster et al, 2002; Och et al, 2003). They
all attempt to interactively help the human user in
the postediting process, by suggesting completion
of the word/phrase based on the user accepted pre-
fix and the source sentece. Incremental feedback
is part of Caitra (Koehn, 2009) an interactive tool
for human-aided MT and works on a similar set-
ting to interactive MT. In Caitra, the source text
is pre-translated first and during the interactions it
dynamically generates user suggestions.
Our incremental decoder work differs from
these text prediction based approaches, in the
sense that the input text is not available to the de-
coder beforehand and the decoding is being done
dynamically for every source word as opposed to
generating suggestions dynamically for complet-
ing target sentece.
6 Conclusion and Future Work
We presented a modified beam search algorithm
for an efficient incremental decoder (ID), which
will allow translations to be generated incremen-
tally for every word typed by a user, instead of
waiting for the entire sentence as input by reusing
the partial decoder state. Our proposed modifica-
tions help us to efficiently compute partial future
costs in the incremental setting. We introduced the
notion of delayed pruning (DP) to avoid search
errors in incremental decoding. We showed that
reusing the partial decoder states is faster than re-
decoding the input from the scratch every time a
new word is typed by the user. Our exhaustive ex-
periments further demonstrated DP to be highly
effective in avoiding search errors under the in-
cremental decoding setting. In our experiments in
this paper we used a very tight beam size; in fu-
ture work, we would like to explore the tradeoff
between speed, accuracy and the utility of delayed
pruning by varying the beam size in our experi-
ments.
References
George Foster, Pierre Isabelle, and Pierre Plamondon.
1997. Target-text mediated interactive machine
translation. Machine Translation, 12(1/2):175?194.
222
Decoder
Fr-En En-Fr
Avg time Avg Hyp/ bin Avg time Avg Hyp/ bin
Re-decode 724.46 2.21 130.29 2.32
ID w/o DP 84.85 2.89 27.58 2.89
ID w/ DP 87.01 85.11 34.35 60.46
Table 4: Speed: Re-decoding v.s. Incremental Decoding (ID)
Race Fr-En En-Fr
Course
BLEU Avg time Avg Hyp/ bin BLEU Avg time Avg Hyp/ bin
Limit
3 26.75 87.83 85.11 25.39 36.15 75.03
4 26.77 91.14 86.35 25.37 36.21 77.69
5 26.77 90.81 86.52 25.37 36.25 78.47
6 26.77 95.91 86.56 25.37 37.34 78.71
7 26.77 91.67 86.57 25.37 36.26 78.81
Table 5: Effect of different race course limits
George Foster, Philippe Langlais, and Guy Lapalme.
2002. User-friendly text prediction for translators.
In EMNLP ?02: Proceedings of the ACL-02 con-
ference on Empirical methods in natural language
processing, pages 148?155, Morristown, NJ, USA.
Association for Computational Linguistics.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
144?151, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Philipp Koehn. 2004. Pharaoh: A beam search
decoder for phrase-based statistical machine trans-
lation models. In Robert E. Frederking and
Kathryn Taylor, editors, AMTA, volume 3265 of Lec-
ture Notes in Computer Science, pages 115?124.
Springer.
Philipp Koehn. 2009. A web-based interactive com-
puter aided translation tool. In In Proceedings of
ACL-IJCNLP 2009: Software Demonstrations, Sun-
tec, Singapore, August.
Franz Josef Och, Richard Zens, and Hermann Ney.
2003. Efficient search for interactive statistical ma-
chine translation. In EACL ?03: Proceedings of the
tenth conference on European chapter of the Asso-
ciation for Computational Linguistics, pages 387?
393, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. ACL.
Michael Pust and Kevin Knight. 2009. Faster mt
decoding through pervasive laziness. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Short Papers, pages 141?144,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas: AMTA 2006.
223
