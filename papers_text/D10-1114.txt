Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1173?1182,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Mixture Model with Sharing for Lexical Semantics
Joseph Reisinger
Department of Computer Science
University of Texas at Austin
1616 Guadalupe, Suite 2.408
Austin, TX, 78701
joeraii@cs.utexas.edu
Raymond Mooney
Department of Computer Science
University of Texas at Austin
1616 Guadalupe, Suite 2.408
Austin, TX, 78701
mooney@cs.utexas.edu
Abstract
We introduce tiered clustering, a mixture
model capable of accounting for varying de-
grees of shared (context-independent) fea-
ture structure, and demonstrate its applicabil-
ity to inferring distributed representations of
word meaning. Common tasks in lexical se-
mantics such as word relatedness or selec-
tional preference can benefit from modeling
such structure: Polysemous word usage is of-
ten governed by some common background
metaphoric usage (e.g. the senses of line or
run), and likewise modeling the selectional
preference of verbs relies on identifying com-
monalities shared by their typical arguments.
Tiered clustering can also be viewed as a form
of soft feature selection, where features that do
not contribute meaningfully to the clustering
can be excluded. We demonstrate the applica-
bility of tiered clustering, highlighting partic-
ular cases where modeling shared structure is
beneficial and where it can be detrimental.
1 Introduction
Word meaning can be represented as high-
dimensional vectors inhabiting a common space
whose dimensions capture semantic or syntactic
properties of interest (e.g. Erk and Pado, 2008;
Lowe, 2001). Such vector-space representations of
meaning induce measures of word similarity that can
be tuned to correlate well with judgements made
by humans. Previous work has focused on de-
signing feature representations and semantic spaces
that capture salient properties of word meaning (e.g.
Curran, 2004; Gabrilovich and Markovitch, 2007;
Landauer and Dumais, 1997), often leveraging the
distributional hypothesis, i.e. that similar words ap-
pear in similar contexts (Miller and Charles, 1991;
Pereira et al, 1993).
Since vector-space representations are con-
structed at the lexical level, they conflate multiple
word meanings into the same vector, e.g. collaps-
ing occurrences of bankinstitution and bankriver. Meth-
ods such as Clustering by Committee (Pantel, 2003)
and multi-prototype representations (Reisinger and
Mooney, 2010) address this issue by perform-
ing word-sense disambiguation across word occur-
rences, and then building meaning vectors from
the disambiguated words. Such approaches can
readily capture the structure of homonymous words
with several unrelated meanings (e.g. bat and club),
but are not suitable for representing the common
metaphor structure found in highly polysemous
words such as line or run.
In this paper, we introduce tiered clustering, a
novel probabilistic model of the shared structure
often neglected in clustering problems. Tiered
clustering performs soft feature selection, allocat-
ing features between a Dirichlet Process cluster-
ing model and a background model consisting of
a single component. The background model ac-
counts for features commonly shared by all occur-
rences (i.e. context-independent feature variation),
while the clustering model accounts for variation
in word usage (i.e. context-dependent variation, or
word senses; Table 1).
Using the tiered clustering model, we derive a
multi-prototype representation capable of capturing
varying degrees of sharing between word senses,
and demonstrate its effectiveness in lexical seman-
tic tasks where such sharing is desirable. In partic-
ular we show that tiered clustering outperforms the
multi-prototype approach for (1) selectional prefer-
ence (Resnik, 1997; Pantel et al, 2007), i.e. predict-
1173
ing the typical filler of an argument slot of a verb,
and (2) word-relatedness in the presence of highly
polysemous words. The former case exhibits a high
degree of explicit structure, especially for more se-
lectionally restrictive verbs (e.g. the set of things that
can be eaten or can shoot).
The remainder of the paper is organized as fol-
lows: Section 2 gives relevant background on the
methods compared, Section 3 outlines the multi-
prototype model based on the Dirichlet Process mix-
ture model, Section 4 derives the tiered cluster-
ing model, Section 5 discusses similarity metrics,
Section 6 details the experimental setup and in-
cludes a micro-analysis of feature selection, Section
7 presents results applying tiered clustering to word
relatedness and selectional preference, Section 8 dis-
cusses future work, and Section 9 concludes.
2 Background
Models of the attributional similarity of concepts,
i.e. the degree to which concepts overlap based on
their attributes (Turney, 2006), are commonly imple-
mented using vector-spaces derived from (1) word
collocations (Schu?tze, 1998), directly leveraging the
distributional hypothesis (Miller and Charles, 1991),
(2) syntactic relations (Pado? and Lapata, 2007), (3)
structured corpora (e.g. Gabrilovich and Markovitch
(2007)) or (4) latent semantic spaces (Finkelstein
et al, 2001; Landauer and Dumais, 1997). Such
models can be evaluated based on their correlation
with human-reported lexical similarity judgements
using e.g. the WordSim-353 collection (Finkelstein
et al, 2001). Distributional methods exhibit a high
degree of scalability (Gorman and Curran, 2006) and
have been applied broadly in information retrieval
(Manning et al, 2008), large-scale taxonomy induc-
tion (Snow et al, 2006), and knowledge acquisition
(Van Durme and Pas?ca, 2008).
Reisinger and Mooney (2010) introduced a multi-
prototype approach to vector-space lexical seman-
tics where individual words are represented as col-
lections of ?prototype? vectors. This representation
is capable of accounting for homonymy and poly-
semy, as well as other forms of variation in word
usage, like similar context-dependent methods (Erk
and Pado, 2008). The set of vectors for a word
is determined by unsupervised word sense discov-
ery (Schu?tze, 1998), which clusters the contexts in
which a word appears. Average prototype vectors
LIFE
all, about, life, would, death
my, you, real, your, about
spent, years, rest, lived, last
sentenced, imprisonment, sentence, prison
insurance, peer, Baron, member, company
Guru, Rabbi, Baba, la, teachings
RADIO
station, radio, stations, television
amateur, frequency, waves, system
show, host, personality, American
song, single, released, airplay
operator, contact, communications, message
WIZARD
evil, powerful, magic, wizard
Merlin, King, Arthur, Arthurian
fairy, wicked, scene, tale
Harry, Potter, Voldemort, Dumbledore
STOCK
stock, all, other, company, new
market, crash, markets, price, prices
housing, breeding, fish, water, horses
car, racing, cars, NASCAR, race, engine
card, cards, player, pile, game, paper
rolling, locomotives, line, new, railway
Table 1: Example tiered clustering representation of
words with varying degrees of polysemy. Each boxed
set shows the most common background (shared) fea-
tures, and each prototype captures one thematic usage
of the word. For example, wizard is broken up into a
background cluster describing features common to all us-
ages of the word (e.g., magic and evil) and several genre-
specific usages (e.g. Merlin, fairy tales and Harry Potter).
are then computed separately for each cluster, pro-
ducing a distributed representation for each word.
Distributional methods have also proven to be a
powerful approach to modeling selectional prefer-
ence (Pado? et al, 2007; Pantel et al, 2007), rivaling
methods based on existing semantic resources such
as WordNet (Clark and Weir, 2002; Resnik, 1997)
and FrameNet (Pado?, 2007) and performing nearly
as well as supervised methods (Herdag?delen and Ba-
roni, 2009). Selectional preference has been shown
to be useful for, e.g., resolving ambiguous attach-
ments (Hindle and Rooth, 1991), word sense disam-
biguation (McCarthy and Carroll, 2003) and seman-
tic role labeling (Gildea and Jurafsky, 2002).
3 Multi-Prototype Models
Representing words as mixtures over several pro-
totypes has proven to be a powerful approach to
1174
vector-space lexical semantics (Pantel, 2003; Pantel
et al, 2007; Reisinger and Mooney, 2010). In this
section we briefly introduce a version of the multi-
prototype model based on the Dirichlet Process Mix-
ture Model (DPMM), capable of inferring automat-
ically the number of prototypes necessary for each
word (Rasmussen, 2000). Similarity between two
DPMM word-representations is then computed as a
function of their cluster centroids (?5), instead of the
centroid of all the word?s occurrences.
Multiple prototypes for each word w are gener-
ated by clustering feature vectors vpcq derived from
each occurrence c P Cpwq in a large textual cor-
pus and collecting the resulting cluster centroids
pikpwq, k P r1,Kws. This approach is commonly
employed in unsupervised word sense discovery;
however, we do not assume that clusters correspond
to word senses. Rather, we only rely on clusters to
capture meaningful variation in word usage.
Instead of assuming all words can be repre-
sented by the same number of clusters, we allocate
representational flexibility dynamically using the
DPMM. The DPMM is an infinite capacity model
capable of assigning data to a variable, but finite
number of clusters Kw, with probability of assign-
ment to cluster k proportional to the number of data
points previously assigned to k. A single parameter
? controls the degree of smoothing, producing more
uniform clusterings as ? ? 8. Using this model,
the number of clusters no longer needs to be fixed
a priori, allowing the model to allocate expressivity
dynamically to concepts with richer structure. Such
a model naturally allows the word representation to
allocate additional capacity for highly polysemous
words, with the number of clusters growing loga-
rithmically with the number of occurrences. The
DPMM has been used for rational models of con-
cept organization (Sanborn et al, 2006), but to our
knowledge has not yet been applied directly to lexi-
cal semantics.
4 Tiered Clustering
Tiered clustering allocates features between two
submodels: a (context-dependent) DPMM and a sin-
gle (context-independent) background component.
This model is similar structurally to the feature se-
lective clustering model proposed by Law et al
(2002). However, instead of allocating entire feature
dimensions between model and background compo-
 
 
?
z
?
D w
 
w
?
!
?
background
!
c
 
?
 
?
 
?
clusters
d
Figure 1: Plate diagram for the tiered clustering model
with cluster indicators drawn from the Chinese Restau-
rant Process.
nents, assignment is done at the level of individual
feature occurrences, much like topic assignment in
Latent Dirichlet Allocation (LDA; Griffiths et al,
2007). At a high level, the tiered model can be
viewed as a combination of a multi-prototype model
and a single-prototype back-off model. However,
by leveraging both representations in a joint frame-
work, uninformative features can be removed from
the clustering, resulting in more semantically tight
clusters.
Concretely, each word occurrence wd first selects
a cluster ?d from the DPMM; then each feature wi,d
is generated from either the background model?back
or the selected cluster ?d, determined by the tier
indicator zi,d. The full generative model for tiered
clustering is given by
?d|?  Betap?q d P D,
?d|?, G0  DPp?, G0q d P D,
?back|?back  Dirichletp?backq
zi,d|?d  Bernoullip?dq i P |wd|,
wi,d|?d, zi,d 
$
'
'
&
'
'
%
Multp?backq
pzi,d  1q
Multp?dq
potherwiseq
i P |wd|,
where ? controls the per-data tier distribution
smoothing and ? controls the uniformity of the DP
cluster allocation. The DP is parameterized by a
base measure G0, controlling the per-cluster term
distribution smoothing; which use a Dirichlet with
hyperparameter ?, as is common (Figure 1).
Since the background topic is shared across all oc-
currences, it can account for features with context-
independent variance, such as stop words and other
high-frequency noise, as well as the central tendency
of the collection (Table 1). Furthermore, it is possi-
ble to put an asymmetric prior on ?, yielding more
fine-grained control over the assumed uniformity of
the occurrence of noisy features, unlike in the model
proposed by Law et al (2002).
1175
Although exact posterior inference is intractable
in this model, we derive an efficient collapsed Gibbs
sampler via analogy to LDA (Appendix 1).
5 Measuring Semantic Similarity
Due to its richer representational structure, comput-
ing similarity in the multi-prototype model is less
straightforward than in the single prototype case.
Reisinger and Mooney (2010) found that simply av-
eraging all similarity scores over all pairs of proto-
types (sampled from the cluster distributions) per-
forms reasonably well and is robust to noise. Given
two words w and w1, this AvgSim metric is
AvgSimpw,w1q def
1
KwKw1
Kw?
j1
Kw1
?
k1
dppikpwq, pijpw
1
qq
Kw andKw1 are the number of clusters for w and w1
respectively, and dp, q is a standard distributional
similarity measure (e.g. cosine distance). As cluster
sizes become more uniform, AvgSim tends towards
the single prototype similarity,1 hence the effective-
ness of AvgSim stems from boosting the influence
of small clusters.
Tiered clustering representations offer more pos-
sibilities for computing semantic similarity than
multi-prototype, as the background prototype can be
treated separately from the other prototypes. We
make use of a simple sum of the distance between
the two background components, and the AvgSim
of the two sets of clustering components.
6 Experimental Setup
6.1 Corpus
Word occurrence statistics are collected from a snap-
shot of English Wikipedia taken on Sept. 29th, 2009.
Wikitext markup is removed, as are articles with
fewer than 100 words, leaving 2.8M articles with a
total of 2.05B words. Wikipedia was chosen due to
its semantic breadth.
6.2 Evaluation Methodology
We evaluate the tiered clustering model on two prob-
lems from lexical semantics: word relatedness and
selectional preference. For the word relatedness
1This can be problematic for certain clustering methods
that specify uniform priors over cluster sizes; however the
DPMM naturally exhibits a linear decay in cluster sizes with
the Er# clusters of size M s  ?{M .
Rating distribution
WS-3530.0
0.5
1.0
Evocation Pado
Sense count distribution
WS-3530
3
10
80
Evocation Pado
Figure 2: (top) The distribution of ratings (scaled [0,1])
on WS-353, WN-Evocation and Pado? datasets. (bottom)
The distribution of sense counts for each data set (log-
domain), collected from WordNet 3.0.
evaluation, we compared the predicted similarity of
word pairs from each model to two collections of hu-
man similarity judgements: WordSim-353 (Finkel-
stein et al, 2001) and the Princeton Evocation rela-
tions (WN-Evocation, Ma et al, 2009).
WS-353 contains between 13 and 16 human sim-
ilarity judgements for each of 353 word pairs, rated
on a 1?10 integer scale. WN-Evocation is signif-
icantly larger than WS-353, containing over 100k
similarity comparisons collected from trained hu-
man raters. Comparisons are assigned to only 3-
5 human raters on average and contain a signifi-
cantly higher fraction of zero- and low-similarity
items than WS-353 (Figure 2), reflecting more ac-
curately real-world lexical semantics settings. In our
experiments we discard all comparisons with fewer
than 5 ratings and then sample 10% of the remain-
ing pairs uniformly at random, resulting in a test set
with 1317 comparisons.
For selectional preference, we employ the Pado?
dataset, which contains 211 verb-noun pairs with
human similarity judgements for how plausible the
noun is for each argument of the verb (2 arguments
per verb, corresponding roughly to subject and ob-
ject). Results are averaged across 20 raters; typical
inter-rater agreement is ?  0.7 (Pado? et al, 2007).
In all cases correlation with human judgements
is computed using Spearman?s nonparametric rank
correlation (?) with average human judgements
1176
(Agirre et al, 2009).
6.3 Feature Representation
In the following analyses we confine ourselves to
representing word occurrences using unordered un-
igrams collected from a window of size T10 cen-
tered around the occurrence, represented using tf-idf
weighting. Feature vectors are pruned to a fixed
length f , discarding all but the highest-weight fea-
tures (f is selected via empirical validation, as de-
scribed in the next section). Finally, semantic simi-
larity between word pairs is computed using cosine
distance (`2-normalized dot-product).2
6.4 Feature Pruning
Feature pruning is one of the most significant factors
in obtaining high correlation with human similarity
judgements using vector-space models, and has been
suggested as one way to improve sense disambigua-
tion for polysemous verbs (Xue et al, 2006). In this
section, we calibrate the single prototype and multi-
prototype methods on WS-353, reaching the limit
of human and oracle performance and demonstrat-
ing robust performance gains even with semanti-
cally impoverished features. In particular we obtain
?0.75 correlation on WS-353 using only unigram
collocations and ?0.77 using a fixed-K multi-
prototype representation (Figure 3; Reisinger and
Mooney, 2010). This result rivals average human
performance, obtaining correlation near that of the
supervised oracle approach of Agirre et al (2009).
The optimal pruning cutoff depends on the fea-
ture weighting and number of prototypes as well as
the feature representation. t-test and ?2 features are
most robust to feature noise and perform well even
with no pruning; tf-idf yields the best results but is
most sensitive to the pruning parameter (Figure 3).
As the number of features increases, more pruning
is required to combat feature noise.
Figure 4 breaks down the similarity pairs into four
quantiles for each data set and then shows corre-
lation separately for each quantile. In general the
more polarized data quantiles (1 and 4) have higher
correlation, indicating that fine-grained distinctions
2(Parameter robustness) We observe lower correlations on
average for T25 and T5 and therefore observe T10 to
be near-optimal. Substituting weighted Jaccard similarity for
cosine does not significantly affect the results in this paper.
0
0.4
0.8
0
0.4
0.8
S
p
e
a
r
m
a
n
'
s
 
?
 
0.7
0.0
-0.2
unpruned pruned (best)
Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4
human
Single-p
Multi-p
ESA
Figure 4: Correlation results on WS-353 broken down
over quantiles in the human ratings. Quantile ranges are
shown in Figure 2. In general ratings for highly sim-
ilar (dissimilar) pairs are more predictable (quantiles 1
and 4) than middle similarity pairs (quantiles 2, 3). ESA
shows results for a more semantically rich feature set de-
rived using Explicit Semantic Analysis (Gabrilovich and
Markovitch, 2007).
in semantic distance are easier for those sets.3 Fea-
ture pruning improves correlations in quantiles 2?4
while reducing correlation in quantile 1 (lowest sim-
ilarity). This result is to be expected as more fea-
tures are necessary to make fine-grained distinctions
between dissimilar pairs.
7 Results
We evaluate four models: (1) the standard single-
prototype approach, (2) the DPMM multi-prototype
approach outlined in ?3, (3) a simple combina-
tion of the multi-prototype and single-prototype ap-
proaches (MP+SP)4 and (4) the tiered clustering ap-
proach (?4). Each data set is divided into 5 quan-
tiles based on per-pair average sense counts,5 col-
lected from WordNet 3.0 (Fellbaum, 1998); ex-
amples of pairs in the high-polysemy quantile are
shown in Table 2. Unless otherwise specified,
both DPMM multi-prototype and tiered clustering
3The fact that the per-quantile correlation is significantly
lower than the full correlation e.g. in the human case indicates
that fine-grained ordering (within quantile) is more difficult than
coarse-grained (between quantile).
4(MP+SP) Tiered clustering?s ability to model both shared
and idiosyncratic structure can be easily approximated by us-
ing the single prototype model as the shared component and
multi-prototype model as the clustering. However, unlike in the
tiered model, all features are assigned to both components. We
demonstrate that this simplification actually hurts performance.
5Despite many skewed pairs (e.g. line has 36 senses while
insurance has 3), we found that arithmetic average and geomet-
ric average perform the same.
1177
00.4
0.8
0
0.4
0.8
0
0.4
0.8
0
0.4
0.8
K=1 K=10 K=50 tf-idf cosine, K=1,10,50
S
p
e
a
r
m
a
n
'
s
 
?
 
0.0
0.8
# of features
all10k5k2k1k5002001002010
# of features
all10k5k2k1k5002001002010
# of features
all10k5k2k1k5002001002010
# of features
all10k5k2k1k5002001002010
tf-idf
ttest
?2
tf
K=50
K=10
K=1
tf-idf
ttest
?2
tf
tf-idf
ttest
?2
tf
// //
// //
Figure 3: Effects of feature pruning and representation on WS-353 correlation broken down across multi-prototype
representation size. In general tf-idf features are the most sensitive to pruning level, yielding the highest correlation for
moderate levels of pruning and significantly lower correlation than other representations without pruning. The optimal
amount of pruning varies with the number of prototypes used, with fewer features being optimal for more clusters.
Bars show 95% confidence intervals.
WordSim-353
stock-live, start-match, line-insurance, game-
round, street-place, company-stock
Evocation
break-fire, clear-pass, take-call, break-tin,
charge-charge, run-heat, social-play
Pado?
see-drop, see-return, hit-stock, raise-bank, see-
face, raise-firm, raise-question
Table 2: Examples of highly polysemous pairs from each
data set using sense counts from WordNet.
use symmetric Dirichlet hyperparameters, ?0.1,
?0.1, and tiered clustering uses?10 for the back-
ground/clustering allocation smoother.
7.1 WordSim-353
Correlation results for WS-353 are shown in Table
3. In general the approaches incorporating multiple
prototypes outperform single prototype (?  0.768
vs. ?  0.734). The tiered clustering model does not
significantly outperform either the multi-prototype
or MP+SP models on the full set, but yields signifi-
cantly higher correlation on the high-polysemy set.
The tiered model generates more clusters than
DPMM multi-prototype (27.2 vs. 14.8), despite us-
ing the same hyperparameter settings: Since words
commonly shared across clusters have been allo-
cated to the background component, the cluster
components have less overlap and hence the model
naturally allocates more clusters.
Examples of the tiered clusterings for several
Method ?  100 ErCs background
Single prototype 73.40.5 1.0 -
high polysemy 76.00.9 1.0 -
Multi-prototype 76.80.4 14.8 -
high polysemy 79.31.3 12.5 -
MP+SP 75.40.5 14.8 -
high polysemy 80.11.0 12.5 -
Tiered 76.90.5 27.2 43.0%
high polysemy 83.11.0 24.2 43.0%
Table 3: Spearman?s correlation on the WS-353 data set.
All refers to the full set of pairs, high polysemy refers to
the top 20% of pairs, ranked by sense count. ErCs is the
average number of clusters employed by each method and
background is the average percentage of features allo-
cated by the tiered model to the background cluster. 95%
confidence intervals are computed via bootstrapping.
words from WS-353 are shown in Table 1 and corre-
sponding clusters from the multi-prototype approach
are shown in Table 4. In general the background
component does indeed capture commonalities be-
tween all the sense clusters (e.g. all wizards use
magic) and hence the tiered clusters are more se-
mantically pure. This effect is most visible in the-
matically polysemous words, e.g. radio and wizard.
7.2 Evocation
Compared to WS-353, the WN-Evocation pair set
is sampled more uniformly from English word pairs
and hence contains a significantly larger fraction of
unrelated words, reflecting the fact that word sim-
1178
LIFE
my, you, real, about, your, would
years, spent, rest, lived, last
sentenced, imprisonment, sentence, prison
years, cycle, life, all, expectancy, other
all, life, way, people, human, social, many
RADIO
station, FM, broadcasting, format, AM
radio, station, stations, amateur,
show, station, host, program, radio
stations, song, single, released, airplay
station, operator, radio, equipment, contact
WIZARD
evil, magic, powerful, named, world
Merlin, King, Arthur, powerful, court
spells, magic, cast, wizard, spell, witch
Harry, Dresden, series, Potter, character
STOCK
market, price, stock, company, value, crash
housing, breeding, all, large, stock, many
car, racing, company, cars, summer, NASCAR
stock, extended, folded, card, barrel, cards
rolling, locomotives, new, character, line
Table 4: Example DPMM multi-prototype representation
of words with varying degrees of polysemy. Compared to
the tiered clustering results in Table 1 the multi-prototype
clusters are significantly less pure for thematically poly-
semous words such as radio and wizard.
ilarity is a sparse relation (Figure 2 top). Further-
more, it contains proportionally more highly polyse-
mous words relative to WS-353 (Figure 2 bottom).
On WN-Evocation, the single prototype and
multi-prototype do not differ significantly in terms
of correlation (?0.198 and ?0.201 respectively;
Table 5), while SP+MP yields significantly lower
correlation (?0.176), and the tiered model yields
significantly higher correlation (?0.224). Restrict-
ing to the top 20% of pairs with highest human
similarity judgements yields similar outcomes, with
single prototype, multi-prototype and SP+MP sta-
tistically indistinguishable (?0.239, ?0.227 and
?0.235), and tiered clustering yielding signifi-
cantly higher correlation (?0.277). Likewise tiered
clustering achieves the most significant gains on the
high polysemy subset.
7.3 Selectional Preference
Tiered clustering is a natural model for verb selec-
tional preference, especially for more selectionally
restrictive verbs: the set of words that appear in a
particular argument slot naturally have some kind of
Method ?  100 ErCs background
Single prototype 19.80.6 1.0 -
high similarity 23.91.1 1.0 -
high polysemy 11.51.2 1.0 -
Multi-prototype 20.10.5 14.8 -
high similarity 22.71.2 14.1 -
high polysemy 13.01.3 13.2 -
MP+SP 17.60.5 14.8 -
high similarity 23.51.2 14.1 -
high polysemy 11.41.0 13.2 -
Tiered 22.40.6 29.7 46.6%
high similarity 27.71.3 29.9 47.2%
high polysemy 15.41.1 27.4 46.6%
Table 5: Spearman?s correlation on the Evocation data
set. The high similarity subset contains the top 20% of
pairs sorted by average rater score.
Method ?  100 ErCs background
Single prototype 25.80.8 1.0 -
high polysemy 17.31.7 1.0 -
Multi-prototype 20.21.0 18.5 -
high polysemy 14.12.4 17.4 -
MP+SP 19.71.0 18.5 -
high polysemy 10.52.5 17.4 -
Tiered 29.41.0 37.9 41.7%
high polysemy 28.52.4 37.4 43.2%
Table 6: Spearman?s correlation on the Pado? data set.
commonality (i.e. they can be eaten or can promise).
The background component of the tiered clustering
model can capture such general argument structure.
We model each verb argument slot in the Pado? set
with a separate tiered clustering model, separating
terms co-occurring with the target verb according to
which slot they fill.
On the Pado? set, the performance of the DPMM
multi-prototype approach breaks down and it yields
significantly lower correlation with human norms
than the single prototype (?0.202 vs. ?0.258;
Table 6), due to its inability to capture the shared
structure among verb arguments. Furthermore com-
bining with the single prototype does not signif-
icantly change its performance (?0.197). Mov-
ing to the tiered model, however, yields significant
improvements in correlation over the other models
(?0.294), primarily improving correlation in the
case of highly polysemous verbs and arguments.
1179
8 Discussion and Future Work
We have demonstrated a novel model for dis-
tributional lexical semantics capable of capturing
both shared (context-independent) and idiosyncratic
(context-dependent) structure in a set of word occur-
rences. The benefits of this tiered model were most
pronounced on a selectional preference task, where
there is significant shared structure imposed by con-
ditioning on the verb. Although our results on the
Pado? are not state of the art,6 we believe this to be
due to the impoverished vector-space design; tiered
clustering can be applied to more expressive vec-
tor spaces, such as those incorporating dependency
parse and FrameNet features.
One potential explanation for the superior perfor-
mance of the tiered model vs. the DPMM multi-
prototype model is simply that it allocates more
clusters to represent each word (Reisinger and
Mooney, 2010). However, we find that decreas-
ing the hyperparameter ? (decreasing vocabulary
smoothing and hence increasing the effective num-
ber of clusters) beyond ?  0.1 actually hurts multi-
prototype performance. The additional clusters do
not provide more semantic content due to significant
background similarity.
Finally, the DPMM multi-prototype and tiered
clustering models allocate clusters based on the vari-
ance of the underlying data set. We observe a neg-
ative correlation (?0.33) between the number of
clusters allocated by the DPMM and the number of
word senses found in WordNet. This result is most
likely due to our use of unigram context window
features, which induce clustering based on thematic
rather than syntactic differences. Investigating this
issue is future work.
(Future Work) The word similarity experiments
can be expanded by breaking pairs down further into
highly homonymous and highly polysemous pairs,
using e.g. WordNet to determine how closely related
the senses are. With this data it would be interest-
ing to validate the hypothesis that the percentage of
features allocated to the background cluster is corre-
lated with the degree of homonymy.
The basic tiered clustering can be extended with
additional background tiers, allocating more expres-
sivity to model background feature variation. This
class of models covers the spectrum between a pure
6E.g., Pado? et al (2007) report ?0.515 on the same data.
topic model (all background tiers) and a pure clus-
tering model and may be reasonable when there is
believed to be more background structure (e.g. when
jointly modeling all verb arguments). Furthermore,
it is straightforward to extend the model to a two-
tier, two-clustering structure capable of additionally
accounting for commonalities between arguments.
Applying more principled feature selection ap-
proaches to vector-space lexical semantics may
yield more significant performance gains. Towards
this end we are currently evaluating two classes of
approaches for setting pruning parameters per-word
instead of globally: (1) subspace clustering, i.e.
unsupervised feature selection (e.g., Parsons et al,
2004) and (2) multiple clustering, i.e. finding fea-
ture partitions that lead to disparate clusterings (e.g.,
Shafto et al, 2006).
9 Conclusions
This paper introduced a simple probabilistic model
of tiered clustering inspired by feature selective
clustering that leverages feature exchangeability to
allocate data features between a clustering model
and shared component. The ability to model back-
ground variation, or shared structure, is shown to be
beneficial for modeling words with high polysemy,
yielding increased correlation with human similarity
judgements modeling word relatedness and selec-
tional preference. Furthermore, the tiered clustering
model is shown to significantly outperform related
models, yielding qualitatively more precise clusters.
Acknowledgments
Thanks to Yinon Bentor and Bryan Silverthorn for
many illuminating discussions. This work was sup-
ported by an NSF Graduate Research Fellowship to
the first author, and a Google Research Award.
A Collapsed Gibbs Sampler
In order to sample efficiently from this model, we
leverage the Chinese Restaurant Process represen-
tation of the DP (cf., Aldous, 1985), introducing a
per-word-occurrence cluster indicator cd. Word oc-
currence features are then drawn from a combination
of a single cluster component indicated by cd and the
background topic.
By exploiting conjugacy, the latent variables ?, ?
and ?d can be integrated out, yielding an efficient
1180
collapsed Gibbs sampler. The likelihood of word
occurrence d is given by
P pwd|z, cd,?q 
?
i
P pwi,d|?cdq
?pzd,i0qP pwi,d|?noiseq
?pzd,i1q.
Hence, this model can be viewed as a two-topic
variant of LDA with the addition of a per-word-
occurrence (i.e. document) cluster indicator.7 The
update rule for the latent tier indicator z is similar
to the update rule for 2-topic LDA, with the back-
ground component as the first topic and the second
topic being determined by the per-word-occurrence
cluster indicator c.
We can efficiently approximate ppz|wq via Gibbs
sampling, which requires the complete conditional
posteriors for all zi,d. These are
P pzi,d  t|z
pi,dq,w, ?, ?q 
n
pwi,dq
t   ?
?
wpn
pwq
t   ?q
npdqt   ?
?
jpn
pdq
j   ?q
.
where z
pi,dq is shorthand for the set ztzi,du, n
pwq
t
is the number of occurrences of wordw in topic t not
counting wi,d and n
pdq
t is the number of features in
occurrence d assigned to topic t, not counting wi,d.
Likewise sampling the cluster indicators condi-
tioned on the data ppcd|w, cd, ?, ?q decomposes
into the DP posterior over cluster assignments
and the cluster-conditional Multinomial-Dirichlet
word-occurrence likelihood ppcd|w, cd, ?, ?q 
ppcd|cd, ?qppwd|wd, c, z, ?q given by
P pcd  kold|cd, ?, ?q9

mpdqk
mpdq

  ?

looooooomooooooon
ppcd|cd,?q
Cp? ??n pdqk  
??n pdq

qq
Cp? ??n pdqk q
loooooooooooooomoooooooooooooon
ppwd|wd,c,z,?q
P pcd  knew|cd, ?, ?q9
?
mpdq

  ?
Cp? ??n pdq

q
Cp?q
where mpdqk is the number of occurrences as-
signed to k not including d, ??n pdqk is the vector of
counts of words from occurrence wd assigned to
7Effectively, the tiered clustering model is a special case of
the nested Chinese Restaurant Process with the tree depth fixed
to two (Blei et al, 2003).
cluster k (i.e. words with zi,d  0) and Cpq is
the normalizing constant for the Dirichlet Cpaq 
?p
?m
j1 ajq
1?m
j1 ?pajq operating over vectors
of counts a.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distri-
butional and Wordnet-based approaches. In Proc.
of NAACL-HLT-09, pages 19?27.
David J. Aldous. 1985. Exchangeability and related
topics. In E?cole d?e?te? de probabilite?s de Saint-
Flour, XIII?1983, volume 1117, pages 1?198.
Springer, Berlin.
David Blei, Thomas Griffiths, Michael Jordan, and
Joshua Tenenbaum. 2003. Hierarchical topic
models and the nested Chinese restaurant process.
In Proc. NIPS-2003.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2):187?206.
James Richard Curran. 2004. From Distributional
to Semantic Similarity. Ph.D. thesis, University
of Edinburgh. College of Science.
Katrin Erk and Sebastian Pado. 2008. A structured
vector space model for word meaning in context.
In Proceedings of EMNLP 2008.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database and Some of its Ap-
plications. MIT Press.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: the
concept revisited. In Proc. of WWW 2001.
Evgeniy Gabrilovich and Shaul Markovitch.
2007. Computing semantic relatedness using
Wikipedia-based explicit semantic analysis. In
Proc. of IJCAI-07, pages 1606?1611.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Lin-
guistics, 28(3):245?288.
James Gorman and James R. Curran. 2006. Scaling
distributional similarity to large corpora. In Proc.
of ACL 2006.
1181
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representa-
tion. Psychological Review, 114:2007.
Amac? Herdag?delen and Marco Baroni. 2009. Bag-
pack: A general framework to represent semantic
relations. In Proc. of GEMS 2009.
Donald Hindle and Mats Rooth. 1991. Structural
ambiguity and lexical relations. In Proc. of ACL
1991.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic anal-
ysis theory of acquisition, induction and repre-
sentation of knowledge. Psychological Review,
104(2):211?240.
Martin H. C. Law, Anil K. Jain, and Ma?rio A. T.
Figueiredo. 2002. Feature selection in mixture-
based clustering. In Proc. of NIPS 2002.
Will Lowe. 2001. Towards a theory of semantic
space. In Proceedings of the 23rd Annual Meeting
of the Cognitive Science Society, pages 576?581.
Xiaojuan Ma, Jordan Boyd-Graber, Sonya S.
Nikolova, and Perry Cook. 2009. Speaking
through pictures: Images vs. icons. In ACM Con-
ference on Computers and Accessibility.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to Informa-
tion Retrieval. Cambridge University Press.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Com-
putational Linguistics, 29(4):639?654.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Lan-
guage and Cognitive Processes, 6(1):1?28.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Computational Linguistics,
33(2):161?199.
Sebastian Pado?, Ulrike Pado?, and Katrin Erk. 2007.
Flexible, corpus-based modelling of human plau-
sibility judgements. In Proc. of EMNLP 2007.
Ulrike Pado?. 2007. The Integration of Syntax and Se-
mantic Plausibility in a Wide-Coverage Model of
Sentence Processing. Ph.D. thesis, Saarland Uni-
versity, Saarbru?cken.
Patrick Pantel, Rahul Bhagat, Timothy Chklovski,
and Eduard Hovy. 2007. ISP: Learning inferen-
tial selectional preferences. In In Proceedings of
NAACL 2007.
Patrick Andre Pantel. 2003. Clustering by commit-
tee. Ph.D. thesis, Edmonton, Alta., Canada.
Lance Parsons, Ehtesham Haque, and Huan Liu.
2004. Subspace clustering for high dimensional
data: A review. SIGKDD Explor. Newsl., 6(1).
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of English words.
In Proc. of ACL 1993.
Carl E. Rasmussen. 2000. The infinite Gaussian
mixture model. In Advances in Neural Informa-
tion Processing Systems. MIT Press.
Joseph Reisinger and Raymond Mooney. 2010.
Multi-prototype vector-space models of word
meaning. In Proc. of NAACL 2010.
Philip Resnik. 1997. Selectional preference and
sense disambiguation. In Proceedings of ACL
SIGLEX Workshop on Tagging Text with Lexical
Semantics, pages 52?57. ACL.
Adam N. Sanborn, Thomas L. Griffiths, and
Daniel J. Navarro. 2006. A more rational model
of categorization. In Proceedings of the 28th An-
nual Conference of the Cognitive Science Society.
Hinrich Schu?tze. 1998. Automatic word sense
discrimination. Computational Linguistics,
24(1):97?123.
Patrick Shafto, Charles Kemp, Vikash Mansinghka,
Matthew Gordon, and Joshua B. Tenenbaum.
2006. Learning cross-cutting systems of cate-
gories. In Proc. CogSci 2006.
Rion Snow, Daniel Jurafsky, and Andrew Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proc. of ACL 2006.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379?416.
Benjamin Van Durme and Marius Pas?ca. 2008.
Finding cars, goddesses and enzymes:
Parametrizable acquisition of labeled instances
for open-domain information extraction. In Proc.
of AAAI 2008.
Nianwen Xue, Jinying Chen, and Martha Palmer.
2006. Aligning features with sense distinction di-
mensions. In Proc. of COLING/ACL 2006.
1182
