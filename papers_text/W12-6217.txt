Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 99?107,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
Finite-state acoustic and translation model composition in statistical
speech translation: empirical assessment
Alicia Pe?rez(1), M. Ine?s Torres(2)
(1)Dep. Computer Languages and Systems
(2)Dep. Electricidad y Electro?nica
University of the Basque Country UPV/EHU
Bilbao (Spain)
(1)alicia.perez@ehu.es
(2)manes.torres@ehu.es
Francisco Casacuberta
Instituto Tecnolo?gico de Informa?tica
Universidad Polite?cnica de Valencia
Valencia (Spain)
fcn@iti.upv.es
Abstract
Speech translation can be tackled by
means of the so-called decoupled ap-
proach: a speech recognition system fol-
lowed by a text translation system. The
major drawback of this two-pass decod-
ing approach lies in the fact that the trans-
lation system has to cope with the er-
rors derived from the speech recognition
system. There is hardly any cooperation
between the acoustic and the translation
knowledge sources. There is a line of re-
search focusing on alternatives to imple-
ment speech translation efficiently: rang-
ing from semi-decoupled to tightly in-
tegrated approaches. The goal of inte-
gration is to make acoustic and transla-
tion models cooperate in the underlying
decision problem. That is, the transla-
tion is built by virtue of the joint ac-
tion of both models. As a side-advantage
of the integrated approaches, the transla-
tion is obtained in a single-pass decod-
ing strategy. The aim of this paper is
to assess the quality of the hypotheses
explored within different speech transla-
tion approaches. Evidence of the perfor-
mance is given through experimental re-
sults on a limited-domain task.
1 Introduction
Statistical speech translation (SST) was typ-
ically implemented as a pair of consecutive
steps in the so-called decoupled approach: with
an automatic speech recognition (ASR) system
placed before to a text-to-text translation sys-
tem. This approach involves two independent
decision processes: first, getting the most likely
string in the source language and next, get-
ting the expected translation into the target lan-
guage. Since the ASR system is not an ideal
device it might make mistakes. Hence, the text
translation system would have to manage with
the transcription errors. Being the translation
models (TMs) trained with positive samples of
well-formed source strings, they are very sensi-
tive to ill-formed strings in the source language.
Hence, it seems ambitious for TMs to aspire to
cope with both well and ill formed sentences in
the source language.
1.1 Related work
Regarding the coupling of acoustic and trans-
lation models, there are some contributions in
the literature that propose the use of semi-
decoupled approaches. On the one hand, in
(Zhang et al, 2004), SST is carried out by
99
an ASR placed before a TM with an addi-
tional stage that would re-score the obtained hy-
potheses within a log-linear framework gather-
ing features from both the ASR system (lexicon
and language model) and the TM (eg. distor-
tion, fertility) and also additional features (POS,
length etc.).
On the other hand, in (Quan et al, 2005), the
N-best hypotheses derived from an ASR sys-
tem were next translated by a TM, finally, a last
stage would re-score the hypotheses and make
a choice. Within the list of the N-best hypothe-
ses typically a number of them include some n-
grams that are identical, hence, the list results to
be an inefficient means of storing data. Alterna-
tively, in (Zhou et al, 2007) the search space
extracted from the ASR system, represented as
a word-graph (WG), was next explored by a TM
following a multilayer search algorithm.
Still, a further approach can be assumed
in order to make the graph-decoding com-
putationally cheaper, that is, confusion net-
works (Bertoldi et al, 2007). Confusion-
networks implement a linear approach of the
word-graphs, however, as a result, dummy hy-
potheses might be introduced and probabili-
ties mis-computed. Confusion networks traded
off between the accuracy and storage ability of
word-graphs for decoding time. Indeed, in (Ma-
tusov and Ney, 2011) an efficient means of do-
ing the decoding with confusion networks was
presented. Note that these approaches follow a
two-pass decoding strategy.
The aforementioned approaches imple-
mented phrase-based TMs within a log-linear
framework. In this context, in (Casacuberta
et al, 2008) a fully integrated approach was
examined. Under this approach, the translation
was carried out in a single-pass decoding,
involving a single decision process in which
acoustic and translations models cooperated.
This integration paradigm, was earlier proposed
in (Vidal, 1997), showing that a single-pass
decoding was enough to carry out SST.
Finally, in (Pe?rez et al, 2010) several SST de-
coding approaches including decoupled, N-best
lists and integrated were compared. Neverthe-
less, the paper focused on the potential scope of
the approaches, comparing the theoretical upper
threshold of their performance.
1.2 Contribution
All the models assessed in this work relay upon
exactly the same acoustic and translation mod-
els. It is the combination of them on which
we are focusing. In brief, the aim of this pa-
per is to compare different approaches to carry
out speech translation decoding. The compari-
son is carried out using exactly the same under-
lying acoustic and translation models in order
to allow to make a fair comparison of the abil-
ities inherent to the decoding strategy. Apart
from the decoupled and semi-decoupled strate-
gies we also focus on the fully-integrated ap-
proach. While the fully integrated approach al-
lows to provide the most-likely hypothesis, we
explored a variant: an integrated architecture
with a re-scoring LM that provided alternatives
derived from the integrated approach and used
re-scoring to make the final decision. Not only
an oracle-evaluation is provided as an upper-
threshold of the experiments but also an experi-
mental set-up to give empirical evidence.
The paper is arranged as follows: Section 2
introduces the formulation of statistical speech
translation (SST); Section 3 describes differ-
ent approaches to put into practice SST, plac-
ing emphasis on the assumptions behind each
of them. Section 4 is devoted to assess experi-
mentally the performance of each approach. Fi-
nally, in Section 5 the concussions drawn from
the experiments are summarized.
100
2 Statistical speech translation
The goal of speech translation, formulated un-
der the probabilistic framework, is to find the
most likely string in the target language (?t)
given the spoken utterance in the source lan-
guage. Speech signal in the source language
is characterized in terms of an array of acoustic
features in the source language, x. The decision
problem involved is formulated as follows:
t? = arg max
t
P (t|x) (1)
In this context, the text transcription in the
source language (denoted as s) is introduced as
a hidden variable and Bayes? rule applied:
t? = arg max
t
?
s
P (x|s, t)P (s, t) (2)
Assuming P (x|s, t) ? P (x|s), and using the
maximum term involved in the sum as an ap-
proach to the sum itself for the sake of compu-
tational affordability, we yield to:
t? ? arg max
t
max
s
P (x|s)P (s, t) (3)
As a result, the expected translation is built
relying upon both a translation model (P (s, t))
and an acoustic model in the source language
(P (x|s)). This approach requires the joint co-
operation of both models to implement the de-
cision problem since the maximum over s con-
cerns both of them.
2.1 Involved models
Being the goal of this paper to compare differ-
ent techniques to combine acoustic and trans-
lation models, it is important to keep constant
the underlying models while varying the strate-
gies to combine them. Before to delve into the
composition strategies and due to the fact that
some combination strategies are based on the
finite-state topology of the models, a summary
of the relevant features of the underlying mod-
els is given in this section.
2.1.1 Translation model
The translation model used in this work
to tackle all the approaches consists of a
stochastic finite-state transducer (SFST) en-
compassing phrases in the source and tar-
get languages together with a probability of
joint occurrence. The SFST (T ) is a tuple
T = ??,?, Q, q0, R, F, P ?, where:
? is a finite set of input symbols;
? is a finite set of output symbols;
Q is a finite set of states;
q0 ? Q is the initial state;
R ? Q ? ?+ ? ?? ? Q is a set of transi-
tions. (q, s?, t?, q?) ? R, represents a tran-
sition from the state q ? Q to the state
q? ? Q, with the source phrase s? ? ?+ and
producing the substring t? ? ??, where t?
might consist of zero or more target words
(|t?| ? 0);
F : Q? [0, 1] is a final state probability;
P : R? [0, 1] is a transition probability;
Subject to the stochastic constraint:
?q ? Q F (q) +
?
s?,t?,q?
P (q, s?, t?, q?) = 1 (4)
For further reading on formulation and prop-
erties of these machines turn to (Vidal et al,
2005).
The SFST can be understood as a statistical
bi-language implemented by means of finite-
state regular grammar (Casacuberta and Vidal,
2004) (in the same way as a stochastic finite-
state automaton can be used to model a sin-
gle language): A = ??, Q, q0, R, F, P ?, being
? ? ?+ ??? a finite-set of bilingual-phrases.
Likewise, bilingual n-gram models can be in-
ferred in practice (Marin?o et al, 2006).
101
2.1.2 Acoustic models
The acoustic model consists of a mapping of
text-transcriptions of lexical units in the source
language and their acoustic representation. That
comprises the composition of: 1) a lexical
model consisting of a mapping between the tex-
tual representation with their phone-like repre-
sentation in terms of a left-to-right sequence;
and 2) an inventory of phone-like units con-
sists of a typical three-state hidden Markov
model (Rabiner, 1989). Thus, acoustic model
lays on the composition of two finite-state mod-
els (depicted in Figure 1).
/T/ /j/ /e/ /l/ /o/
cielo
(a) Phonetic representation of a text lexical unit
/T/
/j/
/e/
(b) HMM phone-like units
Figure 1: Acoustic model requires composing
phone-like units within phonetic representation
of lexical units.
3 Decoding strategies
In the previous section the formulation of SST
was summarized. Let us now turn into prac-
tice and show the different strategies explored
to combine acoustic and translation models to
tackle SST. The approaches accounted are: de-
coupled, semi-decoupled and integrated archi-
tectures. While the former two are imple-
mentable by virtue of alternative TMs, the latter
is achieved thanks to the integration allowed by
finite-state framework. Thus, in order to com-
pare the combination rather than the TMs them-
selves, all of the combinations shall be put in
practice using the same SFST as TM.
3.1 Decoupled approach
Possibly the most widely used approach to
tackle speech translation is the so-called serial,
cascade or decoupled approach. It consists of
a text-to-text translation system placed after an
ASR system. This process is formally stated as:
t? ? arg max
t
max
s
P (x|s)P (s)P (t|s) (5)
In practice, previous expression is imple-
mented in two independent stages as follows:
1st stage: an ASR system would find the
most likely transcription (?s):
s? ? arg max
s
P (x|s)P (s) (6)
2nd stage next, given the expected string in
the source language (?s), a TM would find the
most likely translation:
t? ? arg max
t
P (t|?s) = arg max
t
P (?s, t) (7)
The TM involved in eq.(7) can be based on
either posterior or joint-probability as the dif-
ference between both of them is a normaliza-
tion term that does not intervene in the maxi-
mization process. The second stage has to cope
with expected transcription of speech (?s) which
does not necessarily convey the exact reference
source string (s). That is, the ASR might intro-
duce errors in the source string to be translated
in the next stage. However, the TMs are typ-
ically trained with correct source-target pairs.
Thus, transcription errors are seldom foreseen
even in models including smoothing (Martin et
al., 1999). In addition, TMs are extremely sen-
sitive to the errors in the input, in particular to
substitutions (Vilar et al, 2006).
This architecture represents a suboptimal
means of contending with SST as referred in
eq. (3). This approach barely takes advantage of
the involved knowledge sources, namely, acous-
tic and translation models.
102
3.2 Semi-Decoupled approach
Occasionally, the most probable translation
does not result to be the most accurate one with
respect to a given reference. That is, it might
happen that hypotheses with a slightly lower
probability than that of the expected hypothesis
turn to be more similar to the reference than the
expected hypothesis. This happens due to sev-
eral factors, amongst others, due to the sparsity
of the data with which the model was trained.
In brief, some sort of disparity between the
probability of the hypotheses and their quality
might arise in practice. The semi-decoupled ap-
proach arose to address this issue. Hence, rather
than translating a single transcription hypothe-
sis, a number of them are provided by the ASR
to the TM, and it is the latter that makes the de-
cision giving as a result the most likely transla-
tion. The decoupled approach is implemented
in two steps, and so is it the semi-decoupled ap-
proach. Details on the process are as follows:
1st stage: for a given utterance in the source
language, an ASR system, laying on source
acoustic model and source language model
(LM), would provide a search sub-space. This
sub-space is traced in the search process for the
most likely transcription of speech but without
getting rid of other highly probable hypotheses.
For what us concern, this sub-space is rep-
resented in terms of a graph of words in the
source language (S). The word-graph gath-
ers the hypotheses with a probability within a
threshold with respect to the optimal hypothesis
at each time-frame as it was formulated in (Ney
et al, 1997). The obtained graph is an acyclic
directed graph where the nodes are associated
with word-prefixes of a variable length, and the
edges join the word sequences allowed in the
recognition process with an associated recogni-
tion probability. The edges consist of the acous-
tic and language model probabilities as the ASR
system handles throughout the trellis.
2nd stage: translating the hypotheses within
S (the graph derived in the 1st stage) allows to
take into account alternative translations for the
given spoken utterance. The searching space
being explored is limited by the source strings
conveyed by S . The combination of the recog-
nition probability with the translation probabil-
ity results in a score that accounts both recogni-
tion and translation likelihood:
t? ? arg max
t
max
s?S
P (s)P (s, t) (8)
Thus, acoustic and translation models would
one re-score the other.
All in all, this semi-decoupled approach re-
sults in an extension of the decoupled one.
It accounts alternative transcriptions of speech
in an attempt to get good quality transcrip-
tions (rather than the most probable transcrip-
tion as in the case of the decoupled approach).
Amongst all the transcriptions, those with high
quality are expected to provide the best quality
in the target language. That is, by avoiding er-
rors derived from the transcription process, the
TM should perform better, and thus get transla-
tions of higher quality. Note that finally, a single
translation hypothesis is selected. To do so, the
highest combined probability is accounted.
3.3 Fully-integrated approach
Finite-state framework (by contrast to other
frameworks) makes a tight composition of mod-
els possible. In our case, of acoustic and trans-
lation finite-state models. The fully-integrated
approach, proposed in (Vidal, 1997), encfom-
passed acoustic and translation models within a
single model. To develop the fully-integrated
approach a finite-state acoustic model on the
source language (A) providing the text tran-
scription of a given acoustic utterance (A :
103
X ? S) can be composed with a text transla-
tion model (T ) that provides the translation of a
given text in the source language (T : S ? T )
and give as a result a transducer (Z = A ? T )
that would render acoustic utterances in the
source language to strings in the target lan-
guage. For the sake of efficiency in terms of
spatial cost, the models are integrated on-the-fly
in the same manner as it is done in ASR (Ca-
seiro and Trancoso, 2006).
The way in which integrated architecture ap-
proaches eq. (3) is looking for the most-likely
source-target translation pair as follows:
?(s, t) = arg max
(s,t)
P (s, t)P (x|s) (9)
That is, the search is driven by bilingual phrases
made up of acoustic elements in the source
language integrated within bilingual phrases of
words together with target phrases.
Then, the expected translation would simply
be approached as the target projection of ?(s, t),
the expected source-target string (also known
as the lower projection); and likewise, the ex-
pected transcription is obtained as a side-result
by the source projection (aka upper projection).
It is well-worth mentioning that this approach
implements fairly the eq. (3) without further
assumptions rather than those made in the de-
coding stage such as Viterbi-like decoding with
beam-search. All in all, acoustic and translation
models cooperate to find the expected transla-
tion. Moreover, it is carried out in a single-pass
decoding strategy by contrast to either decou-
pled or semi-decoupled approaches.
3.4 Integrated WG and re-scoring LM
The fully-integrated approach looks for the
single-best hypothesis within the integrated
acoustic-and-translation network. Following
the reasoning of Section 3.2, the most likely
path together with other locally close paths in
the integrated searching space can be extracted
and arranged in terms of a word graph. While
the WG derived in Section 3.2 was in source
language, this one would be bilingual.
Given a bilingual WG, the lower-side net
(WG.l) can be extracted keeping the topol-
ogy and the associated probability distributions
while getting rid of the input string of each tran-
sition, this gives as a result the projection of
the WG in the target language. Next, a target
language model (LM) would help to make the
choice for the most likely hypothesis amongst
those in the WG.l.
t? ? arg max
t
PWG.l(t)PLM (t) (10)
In other words, while in Section 3.2 the trans-
lation model was used to re-score alternative
transcriptions of speech whereas in this ap-
proach a target language models re-scores al-
ternative translations provided by the bilingual
WG. Note that this approach, as well as the
semi-decoupled one, entail a two-pass decoding
strategy. Both rely upon two models: the for-
mer focused on the source language WG, this
one focuses on the target language WG.
4 Experiments
The aim of this section is to assess empir-
ically the performance each of the four ap-
proaches previously introduced: decoupled,
semi-decoupled, fully-integrated and integrated
WG with re-scoring LM. The four approaches
differ on the decoding strategy implemented to
sort out the decision problem, but all of them
rely on the very same knowledge sources (that
is, the same acoustic and translation model).
The main features of the corpus used to carry
out the experimental layout are summarized in
Table 1. The training set was used to infer the
104
TM consisting of an SFST and the test set to as-
sess the SST decoding approaches. The test set
consisted of 500 training-independent pairs dif-
ferent each other, each of them was uttered by
at least 3 speakers.
Spanish Basque
Tr
ain Sentences 15,000Running words 191,000 187,000
Vocabulary 702 1,135
Te
st Sentences 1,800
Hours of speech 3.0 3.5
Table 1: Main features of the Meteus corpus.
The performance of each experiment is as-
sessed through well-known evaluation met-
rics, namely: bilingual evaluation under-study
(BLEU) (Papineni et al, 2002), word error-rate
(WER), translation edit rate (TER).
4.1 Results
The obtained results are given in Table 2. The
performance of the most-likely or single-best
translation derived by either decoupled or fully-
integrated architectures is shown in the first row
of Tables 2a and 2b respectively. The per-
formance of the semi-decoupled and integrated
WG with re-scoring LM is shown in the sec-
ond row. The highest performance achievable
by both the semi decoupled approach and the
integrated WG with re-scoring LM is given in
the third row. To do so, an oracle evaluation of
the alternatives was carried out and the score as-
sociated to the best choice achievable was given
as in (Pe?rez et al, 2010). Since the oracle evalu-
ation provides an upper threshold of the quality
achievable, the scope of each decoupled or in-
tegrated approaches can be assessed regardless
of the underlying decoding algorithms and ap-
proaches. The highest performance achievable
is reflected in the last row of Tables 2a and 2b.
4.2 Discussion
While the results with two-pass decoding strate-
gies (either decoupled or semi-decoupled ap-
proach) require an ASR engine, integrated ap-
proaches have the ability to get both the source
string together with its translation. This is why
we have make a distinction between ASR-WER
in the former and source-WER in the latter.
Nevertheless, our aim focuses on translation
rather than on recognition.
The results show that semi-decoupled ap-
proach outperforms the decoupled one. Simi-
larly, the approach based on the integrated WG
with the re-scoring target LM outperforms the
integrated approach. As a result, exploring dif-
ferent hypotheses and making the selection with
a second model allows to make refined deci-
sions. On the other hand, comparing the first
row of the Table 2a with the first row of the Ta-
ble 2b (or equally the second row of the former
with the second row of the latter), we conclude
that slightly better performance can be obtained
with the integrated approach.
Finally, comparing the third row of both Ta-
ble 2a and Table 2b, the conclusion is that the
eventual quality of the hypotheses within the in-
tegrated approach are significantly better than
those in the semi-decoupled approaches. That
is, what we can learn is that the integrated de-
coding strategy keeps much better hypotheses
than the semi-decoupled one throughout the de-
coding process. Still, while good quality hy-
potheses exist within the integrated approach,
the re-scoring with a target LM used to select
a single hypothesis from the entire network has
not resulted in getting the best possible hypoth-
esis. Oracle evaluation shows that the integrated
approach offers a leeway to achieve improve-
ments in the quality, yet, alternative strategies
have to be explored.
105
ASR target
WER BLEU WER TER
D 1-best 7.9 40.8 50.3 47.7
SD 7.9 42.2 47.6 44.7
SD tgt-oracle 7.5 57.6 36.2 32.8
(a) Decoupled and semi-decoupled
source target
WER BLEU WER TER
I 1-best 9.6 40.9 49.6 46.8
I WG + LM 9.3 42.6 46.7 43.9
I tgt-oracle 6.6 64.0 32.2 28.5
(b) Integrated and integrated WG with LM
Table 2: Assessment of SST approaches decoupled (2a) and integrated (2b) respectively.
5 Conclusions
Different approaches to cope with the SST de-
coding methodology were explored, namely,
decoupled approach, semi-decoupled approach,
fully-integrated approach and integrated ap-
proach with a re-scoring LM. The first two fol-
low a two-pass decoding strategy and focus on
exploring alternatives in the source language;
while the integrated one follows a single-pass
decoding and present tight cooperation between
acoustic and translation models.
All the experimental layouts used exactly the
same translation and acoustic models differing
only on the methodology used to overcome the
decision problem. In this way, we can assert
that the differences lay on the decoding strate-
gies rather than on the models themselves. Note
that implementing all the models in terms of
finite-state models allows to build both decou-
pled and integrated approaches.
Both decoupled and integrated decoding ap-
proaches aim at finding the most-likely transla-
tion under different assumptions. Occasionally,
the most probable translation does not result to
be the most accurate one with respect to a given
reference. On account of this, we turned to ana-
lyzing alternatives and making use of re-scoring
techniques on both approaches in an attempt
to make the most accurate hypothesis emerge.
This resulted in semi-decoupled and integrated-
WG with re-scoring target LM approaches.
What we can learn from the experiments
is that integrating the models allow to keep
good quality hypotheses in the decoding pro-
cess. Nevertheless, the re-scoring model has
not resulted in being able to make the most of
the integrated approach. In other words, there
are better quality hypotheses within the word-
graph rather than that selected by the re-scoring
target LM. Hence, further work should be fo-
cused on other means of selecting hypotheses
from the integrated word-graph.
However, undoubtedly significantly better
performance can be reached from the inte-
grated decoding strategy than from the semi-
decoupled one. It seems as though knowledge
sources modeling the syntactic differences be-
tween source and target languages should be
tackled in order to improve the performance,
particularly in our case, a strategy for further
work could go on the line of the recently tack-
led approach (Durrani et al, 2011).
Acknowledgments
This work was partially funded by the
Spanish Ministry of Science and Innovation:
through the T??mpano (TIN2011-28169-C05-04)
and iTrans2 (TIN2009-14511) projects; also
through MIPRCV (CSD2007-00018) project
within the Consolider-Ingenio 2010 program;
by the Basque Government to PR&ST research
group (GIC10/158, IT375-10), and by the Gen-
eralitat Valenciana under grants ALMPR (Prom-
eteo/2009/01) and GV/2010/067.
106
References
[Bertoldi et al2007] N. Bertoldi, R. Zens, and M.
Federico. 2008. Efficient speech translation by
confusion network decoding. IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing, pg. 1696?1705
[Casacuberta and Vidal2004] F. Casacuberta and E.
Vidal. 2004. Machine translation with in-
ferred stochastic finite-state transducers. Compu-
tational Linguistics, 30(2): pg. 205?225.
[Casacuberta et al2008] F. Casacuberta, M. Fed-
erico, H. Ney, and E. Vidal. 2008. Recent efforts
in spoken language translation. IEEE Signal Pro-
cessing Magazine, 25(3): pg. 80?88.
[Caseiro and Trancoso2006] D. Caseiro and I. Tran-
coso. 2006. A specialized on-the-fly algo-
rithm for lexicon and language model composi-
tion. IEEE Transactions on Audio, Speech &
Language Processing, 14(4): pg. 1281?1291.
[Durrani et al2011] N. Durrani, H. Schmid, and A.
Fraser. 2011. A joint sequence translation model
with integrated reordering. In 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pg. 1045?
1054
[Marin?o et al2006] J. B. Marin?o, R. E. Banchs, J. M.
Crego, A. de Gispert, P. Lambert, J. A. R. Fonol-
losa, and M. R. Costa-jussa`. 2006. N-gram-
based machine translation. Computational Lin-
guistics, 32(4): pg. 527?549
[Martin et al1999] S. C. Martin, H. Ney, and
J. Zaplo. 1999. Smoothing methods in maxi-
mum entropy language modeling. IEEE Interna-
tional Conference on Acoustics, Speech, and Sig-
nal Processing , vol. 1, pg. 545?548
[Matusov and Ney2011] E. Matusov and H. Ney.
2011. Lattice-based ASR-MT interface for
speech translation. IEEE Transactions on Audio,
Speech, and Language Processing, 19(4): pg. 721
?732
[Ney et al1997] H. Ney, S. Ortmanns, and I. Lin-
dam. 1997. Extensions to the word graph method
for large vocabulary continuous speech recogni-
tion. IEEE International Conference on Acous-
tics, Speech, and Signal Processing, vol. 3, pg.
1791 ?1794
[Papineni et al2002] K. Papineni, S. Roukos, T.
Ward, and W.-J. Zhu. 2002. Bleu: a method for
automatic evaluation of machine translation. An-
nual Meeting on Association for Computational
Linguistics, pg. 311?318
[Pe?rez et al2010] A. Pe?rez, M. I. Torres, and F.
Casacuberta. 2010. Potential scope of a fully-
integrated architecture for speech translation. An-
nual Conference of the European Association for
Machine Translation, pg. 1?8
[Quan et al2005] V. H. Quan, M. Federico, and M.
Cettolo. 2005. Integrated n-best re-ranking for
spoken language translation. European Conver-
ence on Speech Communication and Technology,
Interspeech, pg. 3181?3184.
[Rabiner1989] L.R. Rabiner. 1989. A tutorial on
hidden markov models and selected applications
in speech recognition. Proceedings of the IEEE,
77(2): pg. 257?286
[Vidal et al2005] E. Vidal, F. Thollard, C. de la
Higuera, F. Casacuberta, and R. C. Carrasco.
2005. Probabilistic finite-state machines - part II.
IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 27(7): pg. 1026?1039
[Vidal1997] E. Vidal. 1997. Finite-state speech-to-
speech translation. International Conference on
Acoustic, Speech and Signal Processing, vol. 1,
pg. 111?114
[Vilar et al2006] David Vilar, Jia Xu, Luis Fernando
D?Haro, and H. Ney. 2006. Error Analysis of
Machine Translation Output. International Con-
ference on Language Resources and Evaluation,
pg. 697?702
[Zhang et al2004] R. Zhang, G. Kikui, H. Ya-
mamoto, T. Watanabe, F. Soong, and W. K. Lo.
2004. A unified approach in speech-to-speech
translation: integrating features of speech recog-
nition and machine translation. International
Conference on Computational Linguistics, pg.
1168-1174
[Zhou et al2007] B. Zhou, L. Besacier, and Y. Gao.
2007. On efficient coupling of ASR and SMT for
speech translation. IEEE International Confer-
ence on Acoustics, Speech and Signal Processing,
vol. 4, pg. 101?104
107
