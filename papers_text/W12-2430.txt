Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 240?243,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Boosting the protein name recognition performance
by bootstrapping on selected text
Yue Wang and Jin-Dong Kim
Database Center for Life Science,
Research Organization of Information and Systems
2-11-16 Yayoi, Bunkyo-ku, Tokyo, Japan 113-0032
{wang,jdkim}@dbcls.rois.ac.jp
Abstract
When only a small amount of manually anno-
tated data is available, application of a boot-
strapping method is often considered to com-
pensate for the lack of sufcient training ma-
terial for a machine-learning method. The
paper reports a series of experimental results
of bootstrapping for protein name recogni-
tion. The results show that the performance
changes signicantly according to the choice
of text collection where the training samples
to bootstrap, and that an improvement can be
obtained only with a well chosen text collec-
tion.
1 Introduction
While machine learning-based approaches are be-
coming more and more popular for the development
of natural language processing (NLP) systems, cor-
pora with annotation are regarded as a critical re-
source for the training process. Nonetheless, the cre-
ation of corpus annotation is an expensive and time-
consuming work (Cohen et al, 2005), and it is of-
ten the case that lack of sufcient annotation hinders
the development of NLP systems. Bootstrapping
method (Becker et al, 2005; Vlachos and Gasperin,
2006) can be considered as a way to automatically
inate the amount of corpus annotation to comple-
ment the lack of sufcient annotation.
In this study, we report the experimental results on
the effect of bootstrapping for the training of protein
name recognizers, particularly in the situation when
we have only a small amount of corpus annotations.
In summary, we begin with a small corpus with
manual annotation for protein names. A named en-
tity tagger trained on the small corpus is applied to
a big collection of text, to obtain more annotation.
We hope the newly created annotation to be precise
enough so that the training of a protein tagger can
benet from the increased training material.
We assume that the accuracy of a bootstrapping
method (Ng, 2004) depends on two factors: the ac-
curacy of the bootstrap tagger itself and the similar-
ity of the text to the original corpus. While accuracy
of the bootstrap tagger may be maximized by nd-
ing the optimal parameters of the applied machine
learning method, the choice of text where the origi-
nal annotations will bootstrap may also be a critical
factor for the success of the bootstrapping method.
Experimental results presented in this paper con-
rm that we can get a improvement by using a boot-
strapping method with a well chosen collection of
texts.
The paper is organized as follows. Section 2 intro-
duces the two datasets used in this paper. Following
that, in Section 3, we briey introduce the experi-
ments performed in our research. The experimental
results are demonstrated in Section 4. The research
is concluded in Section 5 and in the meanwhile, fu-
ture work is discussed.
2 Datasets
2.1 The cyanobacteria genome database
Cyanobacteria are prokaryotic organisms that have
served as important model organisms for studying
oxygenic photosynthesis and have played a signi-
240
cant role in the Earthfs history as primary producers
of atmospheric oxygen (Nakao et al, 2010).
The cyanobacteria genome database (abbreviated
to CyanoBase
1
) includes the annotations to the
PubMed text. In total, 39 species of the cyanobacte-
ria are covered in the CyanoBase.
In our cyanobacteria data (henceforth, the Kazusa
data for short), 270 abstracts were annotated by two
independent annotators. We take the entities, about
which both of the annotators agreed with each other.
In total, there are 1,101 entities in 2,630 sentences.
The Kazusa data was split equally into three sub-
sets and the subsets were used in turn as the training,
development and testing sets in the experiments.
2.2 The BioCreative data
The BioCreative data, which was used for the
BioCreative II gene mention task
2
, is described as
the tagged gene/protein names in the PubMed text.
The training set is used in the research, and totally
there are 15,000 sentences in the dataset.
Unlike other datasets, the BioCreative data was
designed to contain sentences both with and without
protein names, in a variety of contexts. Since the
collection is made to explicitly compile positive and
negative examples for protein recognition, there is a
chance that the sample of text is not comprehensive,
and gray-zone expressions may be missed.
The reason that we chose the BioCreative data
for the bootstrapping is that, the BioCreative data
(henceforth, the BC2 data for short) is the collection
for the purpose of training and evaluation of protein
name taggers.
3 Experiment summary
In the following experiments, the NERSuite
3
, a
named entity tagger based on Conditional Random
Fields (CRFs) (Lafferty et al, 2001; Sutton and Mc-
Callum, 2007), is used. The NERSuite is executable
open-source and serves as a machine learning sys-
tem for named entity recognition (NER). The sigma
value for the L2-regularization is optimizable and in
our experiments, we tune the sigma value between
10?1 to 104.
1
http://genome.kazusa.or.jp/cyanobase
2
http://www.biocreative.org/
3
http://nersuite.nlplab.org/
As mentioned in Section 2.1, the three subsets of
Kazusa data are used for training, tuning and testing
purposes, in turn. We experimented with all the six
combinations.
Experiments were performed to compare three
different strategies. First, with the baseline strat-
egy, the protein tagger is trained only on the Kazusa
training set. The sigma value is optimized on the
tuning set, and the performance is evaluated on the
test set. It is the most typical strategy particularly
when it is believed there is a sufcient training ma-
terial.
Second, with the bootstrapping strategy, the
Kazusa training set is used as the seed data. A tag-
ger for bootstrapping (bootstrap tagger, hereafter) is
trained on the seed data, and applied to the BC2 data
to bootstrap the training examples. Another pro-
tein tagger (application tagger) is then trained on the
bootstrapped BC2 data together with the seed data.
The Kazusa tuning set is used to optimize the two
sigma values for the two protein taggers, and the
performance is evaluated on the test set. With this
strategy, we wish the bootstrapped examples com-
plement the lack of sufcient training examples.
Experiment Seed BT BT+SS
E1 368 647 647 (1,103)
E2 368 647 647 (1,103)
E3 366 759 759 (1,200)
E4 366 769 590 (1,056)
E5 367 882 558 (1,068)
E6 367 558 558 (1,068)
Table 1: The number of positive examples used in each
experiment. The ?BT? column shows the number of posi-
tive examples obtained by the bootstrapping in the 15,000
BC2 sentences. In the last column, the gures in paren-
theses are the number of the selected sentences.
Third, the bootstrapping with sentence selection
strategy is almost the same with the bootstrapping
strategy, except that the second tagger is trained after
the non-relevant sentences are ltered out from the
BC2 data. Here, non-relevant sentences mean those
that are not tagged by the the bootstrap tagger. With
this strategy, we wish an improvement with the boot-
strapping by removing noisy data. Table 1 shows the
number of the seed and bootstrapped examples used
for the three strategies. It is observed that the seed
241
Training Tuning Testing Baseline BT BT+SS
E1 A B C 63.7/29.2/40.0 [102] 61.3/25.9/36.4 [104-101] 61.7/38.2/47.1 [104-104]
E2 A C B 65.2/36.9/47.1 [103] 67.7/35.0/46.1 [104-101] 61.7/46.7/53.2 [104-104]
E3 B C A 75.3/36.4/49.1 [102] 75.2/31.3/44.2 [102-101] 67.1/40.0/50.1 [102-101]
E4 B A C 68.5/33.8/45.3 [102] 70.2/28.9/40.9 [104-101] 66.7/36.5/47.2 [101-102]
E5 C B A 77.7/35.1/48.3 [101] 71.8/27.7/40.0 [104-102] 70.9/38.3/49.7 [100-101]
E6 C A B 73.0/39.1/50.9 [101] 76.1/32.2/45.3 [100-102] 67.7/41.8/51.7 [100-102]
Table 2: Experimental results of using the Kazusa and BC2 data (Precision/Recall/F-score). ?BT? and ?SS? represent
the bootstrapping and sentence selection strategies, respectively. The gures in square brackets are the sigma values
optimized in the experiments.
annotation bootstrap only on a small portion of the
BC2 data set, e.g., 1,103 vs. 15,000 sentences in the
case of E1 (less than 10%), suggesting that a large
portion of the data set may be irrelevant to the origi-
nal data set.
4 Experimental results
The experimental results of all the six combinations
are shown in Table 2. The use of the three subsets,
denoted by A, B, C, of the Kazusa data set for train-
ing, tuning and testing in each experiment is spec-
ied in ?training?, ?tuning? and ?testing? columns.
The results of the baseline strategy that uses only
the Kazusa data are shown in the ?baseline? column,
whereas the results with the bootstrapping methods
with and without sentence selection are shown in the
last two columns. As explained in Section 3, the
sigma values are optimized using the tuning set for
each experiment. Note that for bootstrapping, we
need two sigma values for the bootstrapping tagger
and the application tagger. See section 3.
The performance of named entity recognition is
measured in terms of precision, recall and F-score.
For matching criterion, in order to avoid underesti-
mation, instead of the exact matching, system per-
formance is evaluated under a soft matching, the
overlapping matching criterion. That is, if any part
of the annotated protein/gene names is recognized
by the NER tagger, we will regard that as a correct
answer.
4.1 Results with the bootstrapping strategy
Comparing the two columns, ?baseline? and ?BT?,
we observe that the use of bootstrapping may lead
to a degradation of the performance. Note that the
sigma values are optimized on the development set
for each experiment, and the text for bootstrapping
is BC2 corpus which is expected to be similar to the
Kazusa corpus, but still it is observed that the boot-
strapping does not work, suggesting that the text col-
lection may not yet similar enough.
4.2 Results with bootstrapping with sentence
selection
Comparing the last column (the ?BT+SS? column)
to the ?baseline? column, we observe that the appli-
cation of the bootstrapping method with sentence se-
lection consistently improves the performance. The
improvement is sometimes signicant, e.g., 7.1% of
difference in F-score in the case of E1, but some-
times not, e.g., only 0.8% in the case of E6, but the
performance is improved in the every experiments.
The results conrm our assumption that the choice
of text for bootstrapping is important, and that the
sentence selection is a stable method for the choice
of text.
5 Conclusion and future work
In order to compensate for the lack of sufcient
training data for a CRF-based protein name recog-
nizer, the potential of a bootstrapping method has
been explored through a series of experiments. The
BC2 data was chosen for the bootstrapping as the
data set was one collected for protein name recogni-
tion.
Our initial experiment showed that the seed anno-
tations bootstrapped only on a very small portion of
the BC2 data set, suggesting that a big portion of the
data set might be less relevant to the seed corpus.
From a series of experiments, it was observed that
the performance of protein name recognition was al-
ways improved with bootstrapping by selecting only
242
the sentences where the seed annotations bootstrap,
and by using them as an additional training data.
The goal was to be able to predict more possible
protein mentions (recall) at a relatively satisfactory
level of the quality (precision). The experimental
results suggest us, in order to achieve the goal, the
choice of text collection is important for the success
of the use of a bootstrapping method.
For the future work, we would like to take use of
the original annotations in the BC2 data. A ltering
strategy (Wang, 2010) will be performed. Instead of
completely using the output of the Kazusa-trained
tagger, we compare the output of the Kazusa-trained
tagger with the BioCreative annotations. If the en-
tity is recognized by the tagger and also annotated
in the BioCreative data, then the annotation to this
entity will be kept. The entity will be regarded as
a true positive according to the BioCreative annota-
tions. Otherwise, we will remove the annotation to
the entity from the BioCreative annotations.
Further, we also would like to combine the boot-
strapping with the ltering. Besides keeping the true
positives, we also want to include some false pos-
itives from the bootstrapping. Because these false
positives helps in improving the recall, when the tag-
ger is applied to the Kazusa testing subset. To dis-
criminate this strategy from the bootstrapping and
ltering strategies, different sigma value should be
used.
Acknowledgement
We thank Shinobu Okamoto for providing the
Kazusa data and for many useful discussion. This
work was supported by the ?Integrated Database
Project? funded by the Ministry of Education, Cul-
ture, Sports, Science and Technology (MEXT) of
Japan.
References
K. Bretonnel Cohen, Lynne Fox, Philip Ogren and
Lawrence Hunter. 2005. Empirical data on corpus
design and usage in biomedical natural language pro-
cessing. Proceedings of the AMIA Annual Symposium,
38?45.
Markus Becker, Ben Hachey, Beatrice Alex, Claire
Grover. 2005. Optimising Selective Sampling for
Bootstrapping Named Entity Recognition. Proceed-
ings of the Workshop on Learning with Multiple Views,
5?11.
Andreas Vlachos and Caroline Gasperin. 2006. Boot-
strapping and Evaluating Named Entity Recognition
in the Biomedical domain. Proceedings of the BioNLP
Workshop, 138?145.
Andrew Ng. 2004. Feature selection, L1 vs. L2 regu-
larization, and rotational invariance. Proceedings of
the 21st International Conference on Machine Learn-
ing (ICML).
Mitsuteru Nakao, Shinobu Okamoto, Mitsuyo Kohara,
Tsunakazu Fujishiro, Takatomo Fujisawa, Shusei
Sato, Satoshi Tabata, Takakazu Kaneko and Yasukazu
Nakamura. 2010. CyanoBase: the cyanobacteria
genome database update 2010. Nucleic Acids Re-
search, 38:D379?D381.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. Pro-
ceedings of the 18th International Conference on Ma-
chine Learning, 282?289.
Charles Sutton and Andrew McCallum. 2007. An Intro-
duction to Conditional Random Fields for Relational
Learning. Introduction to Statistical Relational Learn-
ing, MIT Press.
Yue Wang. 2010. Developing Robust Protein Name
Recognizers Based on a Comparative Analysis of Pro-
tein Annotations in Different Corpora. University of
Tokyo, Japan, PhD Thesis.
243
