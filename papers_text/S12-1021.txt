First Joint Conference on Lexical and Computational Semantics (*SEM), pages 132?141,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
An Unsupervised Ranking Model for Noun-Noun Compositionality
Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman
Department of Computer Science
University of Oxford
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
{karl.moritz.hermann,phil.blunsom,stephen.pulman}@cs.ox.ac.uk
Abstract
We propose an unsupervised system that
learns continuous degrees of lexicality for
noun-noun compounds, beating a strong base-
line on several tasks. We demonstrate that the
distributional representations of compounds
and their parts can be used to learn a fine-
grained representation of semantic contribu-
tion. Finally, we argue such a representation
captures compositionality better than the cur-
rent status-quo which treats compositionality
as a binary classification problem.
1 Introduction
A Multiword Expressions (MWE) can be defined as
a sequence of words whose meaning cannot nec-
essarily be derived from the meaning of the words
making up that sequence, for example:
Rat Race ? self-defeating or pointless pursuit1
MWEs are considered a ?key problem for the de-
velopment of large-scale, linguistically sound nat-
ural language processing technology? (Sag et al,
2002). The challenge posed by MWEs is three-
fold, consisting of MWE identification, classifica-
tion and interpretation. Following the identification
of a MWE, it needs to be established whether the
expression should be treated as lexical (idiomatic)
or as compositional. The final step, learning the se-
mantics of the MWE, strongly depends on this deci-
sion.
1Definition taken from Wikipedia, and clearly not recover-
able if one only knows the meaning of the words ?rat? and ?race?.
The problem posed by MWEs is considered hard,
but at the same time it is highly relevant and inter-
esting. MWEs occur frequently in language and in-
terpreting them correctly would directly improve re-
sults in a number of tasks in NLP such as translation
and parsing (Korkontzelos and Manandhar, 2010).
By extension this makes deciding the lexicality of
MWEs an important challenge for various fields in-
cluding machine translation, question answering and
information retrieval. In this paper we discuss com-
positionality with respect to noun-noun compounds.
Most Computational Linguistics literature treats
compositionality as a binary problem, classifying
compounds as either lexical or compositional. We
show that this approach is too simplistic and argue
for the real-valued treatment of compositionality.
We propose two unsupervised models that learn
compositionality rankings for compounds, placing
them on a scale between lexical and compositional
extremes. We develop a fine-grained representa-
tion of compositionality using a novel generative ap-
proach that models context as generated by com-
pound constituents. This representation differenti-
ates between the semantic contribution of both com-
pound constituents as well as the compound itself.
Comparing it with existing work in the field, we
demonstrate the competitiveness of our approach.
We evaluate on an existing corpus of noun com-
pounds with ranked compositionality data, as well
as on a large corpus with a binary annotation for lex-
ical and compositional compounds. We analyse the
impact of data sparsity and propose an interpolation
approximation which significantly reduces the effect
of sparsity on model performance.
132
2 Related Work
Interpreting MWEs is a difficult task as ?compound
nouns can be freely constructed? (Spa?rck Jones,
1985), and are thus able to proliferate infinitely. At
the same time, semantic composition can take many
different forms, making uniform interpretation of
compounds impossible (Zanzotto et al, 2010).
Most current work on MWEs focuses on inter-
preting compounds and sidesteps the task of deter-
mining whether a compound is compositional in the
first place (Butnariu et al, 2010; Kim and Baldwin,
2008). Such methods, aimed at learning the seman-
tics of compounds, can roughly be divided into two
major strands of research.
One group relies on data intensive methods to ex-
tract semantics vectors from large corpora (Baroni
and Zamparelli, 2010; Zanzotto et al, 2010; Gies-
brecht, 2009). The focus of these approaches is to
develop methods for composing the vectors of un-
igrams into a semantic vector representing a com-
pound. Some of the work in this area touches on the
issue of lexicality, as models learning distributional
representations of MWEs ideally would first estab-
lish whether a given MWE is compositional or not
(Mitchell and Lapata, 2010).
The other group are knowledge intensive ap-
proaches collecting linguistic features (Kim and
Baldwin, 2005; Korkontzelos and Manandhar,
2009). Tratz and Hovy (2010), for instance, train
a classifier for noun compound interpretation on a
large set of WORDNET and Thesaurus features.
Combined approaches include Kim and Baldwin
(2008), who interpret noun compounds by extrapo-
lating their semantics from observations where the
two nouns forming a compound are in an intransi-
tive relationship. For example extracting the phrase
?the family owns a car? from the training data would
help learn that the compound ?family car? describes
a POSSESSOR-OWNED/POSSESSED relationship.
Some of these supervised classifiers include lexi-
cality as a classification option, considering it jointly
with the actual compound interpretation.
Next to the work on MWE interpretation there has
been some work focused on determining lexicality
in its own right (Reddy et al, 2011; Bu et al, 2010;
Kim and Baldwin, 2007).
One possibility is to exploit special properties of
lexical MWEs such as high statistical association
of their constituents (Pedersen, 2011) or syntactic
rigidity (Fazly et al, 2009; McCarthy et al, 2007).
However, these approaches are limited in their ap-
plicability to compound nouns (Reddy et al, 2011).
Another method is to compare the semantics of
a compound and its constituents to decide com-
positionality. The approaches used to determine
those semantics can again be divided into knowl-
edge intensive and data-driven methods. Depending
on the chosen representation of semantics these ap-
proaches can either be used for supervised classifiers
or together with a distance metric comparing vector
space representations of semantics. In a binary set-
ting, a threshold would then be applied to the result
of that distance function (Korkontzelos and Man-
andhar, 2009). In a real-valued setting the distance
metric itself can be used as a measure for compo-
sitionality (Reddy et al, 2011). Related to the vec-
tor space based models, some research focuses on
improving the distance metrics used to compare in-
duced semantics (Bu et al, 2010).
3 Methodology
English noun-noun compounds are majority left-
branching (Lauer, 1995), with a head (the second
element), modified by an attributive noun (first el-
ement). For example:
Ground Floor ? The floor of a building at or near-
est ground level.2
In this paper, we will use the terms attributive noun
(AN) and head noun (HN) to refer to the first and
second noun in a noun compound.
3.1 Real-Valued Representation
Lexicality of MWEs is frequently treated as a bi-
nary property (Tratz and Hovy, 2010; O? Se?aghdha,
2007). We argue that lexicality should instead be
treated as a graded property, as most compound se-
mantics exhibit a mixture of compositional and lexi-
cal influences. For example, ?cocktail dress? derives
a large part of its semantics from ?dress?, but the
compound also contributes an idiosyncratic element
to its meaning.
2Definition from http://www.thefreedictionary.com
133
We define lexicality as the degree to which id-
iosyncrasy contributes to a compound?s semantics.
Inversely phrased, the compositionality of a com-
pound can be defined as the degree to which its sense
is related to the senses of its constituents.3
This graded representation follows Spa?rck Jones
(1985), who argued that ?it is not possible to main-
tain a principled distinction between lexicalised and
non-lexicalised compounds?. Some recent work
also supports this view (Reddy et al, 2011; Bu et
al., 2010; Baldwin, 2006). From a practical per-
spective, a real-valued representation of composi-
tionality should help improve interpretation of com-
pounds. This is especially true when factoring in the
respective semantic contributions of its parts.
3.2 Context Generation
According to the distributional hypothesis, the se-
mantics of a lexical item can be expressed by its
context. We apply this hypothesis to the problem of
noun compound compositionality by using a genera-
tive model on compound context. Our model allows
context to be generated by the compound itself or by
either one of its constituents. By learning which el-
ement of the compound generates which part of its
context we effectively determine the semantic con-
tribution of each element. This in turn gives us a
fine-grained, graded representation of a compound?s
lexicality.
4 Corpora for Evaluation
4.1 Ranked Corpus ? REDDY
As we want to evaluate our models? ability to learn
lexicality as a real-valued property, we require an
annotated data set of noun compounds ranked by
lexicality. To the best of our knowledge the only
such data set was developed by Reddy et al (2011).
This data set contains 90 distinct noun compounds
with real-valued gold standard scores ranking from
0 (lexical) to 5 (compositional). The compounds
are nearly linearly distributed across the [0;5] range,
with inter annotator agreement (Spearman?s ?) of
3For example, the meaning of ?gravy train? has hardly any
relation to either ?gravy? or ?train?. Its semantics are thus highly
dependent on the compound in its own right. On the other end
of the spectrum, ?climate change? is significantly related to both
?climate? and ?change?, contributing little inherent semantics to
its overall meaning.
0.522. We refer to this data set and evaluation as
REDDY throughout this paper.
4.2 Binary Corpora ? TRATZ
We also apply our models to a second, binary classi-
fication task. Tratz and Hovy (2010) compiled a data
set for noun compound interpretation, which classi-
fies noun compounds based on their internal struc-
ture. We use this corpus to extract lexical and com-
positional noun compounds.
After some pre-processing4 the data set contains
18,858 compositional and 118 lexical noun com-
pounds. We believe this to more accurately represent
the real world distribution of lexical and composi-
tional noun compounds: Tratz and Hovy (2010) ex-
tracted noun compounds from several large corpora
including the Wall Street Journal section of the Penn
Treebank, thus obtaining a reasonable approxima-
tion of real world occurrence. Other collections of
noun compounds (O? Se?aghdha, 2007) feature sim-
ilar proportions of lexical and compositional noun
compounds.
The large bias towards compositional noun com-
pounds does not support the status-quo of treating
compositionality as a binary property. As discussed
earlier, we assume that most compounds have a
compositional as well as a lexical element. While
the compositional aspect may be larger for most
compounds this alone does not suffice as a reason
to disregard the lexical element contained in these
compounds.
In order to evaluate our system on the TRATZ
data, we use receiving operator characteristic (ROC)
curves. ROC analysis enables us to evaluate a rank-
ing model without setting an artificial threshold for
the compositionality/lexicality decision.
5 Baseline Approach
We develop a set of advanced baselines related to
the semi-supervised models presented by Reddy et
al. (2011). We define the context K of a noun com-
pound as all words in all sentences the compound
appears in. From this we calculate distributional
representations of a compound (c = ?a, h?) and its
constituent elements a, h. We refer to these repre-
sentations as ~c for the compound and ~a, ~h for the
4We removed trigrams from the data set.
134
Name ? r ?
ADD w.Sac + (1? w).Shc .323 .567
MULT Sac.Shc .379 .551
MIN min(Sac, Shc) .343 .550
MAX max(Sac, Shc) .299 .505
COMB w1.Sac+w2.Shc+w3.Sac.Shc .366 .556
Table 1: Results of COSLEX with different operators on
the REDDY data set, reporting Pearson?s r and Spear-
man?s ? correlations. Weights for operators ADD (w =
0.3) and COMB (w = ?0.3, 0.1, 0.6?) are manually opti-
mised. Values range from -1 (negative correlation) to +1
(perfect correlation) with 0 describing random data.
attributive and head noun, respectively. We can cal-
culate the cosine similarity based lexicality score
(COSLEX) by combining the cosine similarity of the
compound?s distribution with each of its two con-
stituents (Reddy et al, 2011).
Sac = sim(~a,~c)
Shc = sim(~h,~c)
COSLEX(c) = Sac ? Shc
We evaluate a number of alternative operators ? for
combining Sac and Shc. Results for this baseline
on the REDDY corpus are in Table 1,5 with weights
wi on the combination operators manually optimised
for Spearman?s ? on that data set. In effect this
renders this baseline into a supervised approach, so
we would expect it to perform very well. We use
the best performing operators (ADD with w = 0.3,
MULT) as baselines for this paper.
6 Generative Models
We exploit the distributional hypothesis to model
the semantic contribution of the different elements
of a noun compound. For this, we require a sys-
tem that treats a noun compound as a vector of three
semantics-bearing units: the compound itself, its
head and its attributive noun. This system should
then model the relationship between the context of
the compound and these three units, deciding which
of them is responsible for each context element.
5Reddy et al (2011) report higher figures on our baseline
models. The differences are attributed to differences in training
data and parametrization.
6.1 3-way Compound Mixture
We model a corpus D of tuples d = {c, k1, ..., kn}.
Each tuple d contains a noun compound c = ?a, h?
and its context words K = (k1, ..., kn). We use vo-
cabularies Vc for noun compounds, Va for attributive
nouns, Vh for head nouns and Vk for context.
We condition our generative model on the noun
compounds. Given an observation d of a compound
c, we generate each context word in two steps. First,
we choose one of the compounds three elements6 to
generate the next context word. Second, we gener-
ate a new context word conditioned on that element.
Formally, the context is generated as follows.
We draw three multinomial parameters ?c, ?a
and ?h from Dirichlet distributions with parameters
?c, ?a and ?h. ?c represents the distribution over
context words Vk given compound c. ?a and ?h
are distributions over Vk given attributive noun a and
head noun h, respectively. These three distributions
form the mixture components of our model.
A fourth multinomial parameter ?z , drawn from
a Dirichlet distribution with parameter ?z , controls
the distribution over the mixture components. ?z is
specific to each compound c, so multiple observa-
tions of the same compound share this parameter.
For each context word we draw a mixture compo-
nent zc,i ? {c?, a?, h?} from the multinomial distribu-
tion with parameter ?z . zc,i determines which dis-
tribution the context word itself will be drawn from.
Finally, we draw the context word:
?i: ki | ?
{zc,i} ? Multi(?{zc,i})
Thus, for each observation of a compound noun we
have a vector zc = ?z1, ..., zn? detailing how its
context words were created either by the compound
itself or by one of its constituents. To determine lex-
icality, we are interested in learning the multinomial
parameter ?z , which describes to what extent the
compound and its constituents contribute to the gen-
eration of the context (i.e. semantics). We can ap-
proximate ?z from the vector zc.
We define the lexicality score Lex(c) for a com-
pound as the percentage of context words created by
6The compound itself, its attributive noun and its head noun
135
Figure 1: Plate diagram illustrating the MULT-CMPD
model with context words ki drawn from a mixture model
with three components controlled by zi.
the compound and not one of its constituents:
Lex(c) = p(z=c?|?a, h?), (1)
where c = ?a, h?
Figure 1 shows a plate diagram of this model, which
we will refer to as MULT-CMPD.
One hypothesis encoded in model MULT-CMPD
is that deciding which part of a compound (the com-
pound itself, the head or the attributive noun) gen-
erates context is a single decision. An alternative
representation could treat this as a two-step process,
which we encode in a second model BIN-CMPD.
The intuition behind the BIN-CMPD model is that
there are two distinct decisions. First, whether a
compound is compositional or not. Second, whether
(in the compositional case) its semantics stem from
its head or attributive noun
Where MULT-CMPD uses a three component mix-
ture to determine which multinomial distribution to
use, BIN-CMPD uses two cascaded binary mixtures
(see Figure 2). The BIN-CMPD model first chooses
whether to treat a compound as compositional or
lexical. If the compound is determined as composi-
tional, a second binary mixture determines whether
to generate a context word using the attributive (?a)
or head multinomial (?h). For the lexical case, the
model remains unchanged.
Figure 2: Schematic description of compositional-
ity/lexicality decision for models MULT-CMPD and BIN-
CMPD.
Model r ?
COSLEX (ADD) .323 .567
COSLEX (MULT) .379 .551
MULT-CMPD .141 .435
BIN-CMPD .168 .410
Table 2: Results on the REDDY data set, reporting Pear-
son?s r and Spearman?s ? correlations. Values range from
-1 (negative correlation) to +1 (perfect correlation).
6.1.1 Inference and Sampling
We use Gibbs sampling to learn the vectors z for
each instance d, integrating out the parameters ?x.
We train our models on the British National Corpus
(BNC), extracting all noun-noun compounds from a
parsed version of the corpus.
In order to speed up convergence of the sampler,
we use simulated annealing over the first 20 iter-
ations (Kirkpatrick et al, 1983), helping the ran-
domly initialised model reach a mode faster. We re-
port results using marginal distributions after a fur-
ther 130 iterations, excluding the counts of the an-
nealing stage.
6.1.2 Evaluation
We evaluate our two models on the REDDY data
set by comparing its scores for lexicality (Lex(c))
with the annotated gold standard. The aim of this
evaluation is to determine how accurately the mod-
els can capture gradual distinctions in lexicality. The
ROC analysis on the TRATZ data set furthermore in-
forms us how precise the models are at distinguish-
ing lexical from compositional compounds.
Results of the REDDY evaluation are in Table 2.
We use Spearman?s ? to measure the monotonic cor-
relation of our data to the gold standard. Pearson?s r
additionally captures the linear relationship between
the data, taking into account the relative differences
in Lex(c) scores among noun compounds.
136
Figure 3: ROC analysis of models MULT-CMPD and
BIN-CMPD versus the best COSLEX baseline (ADD) on
the TRATZ data set
While both models, BIN-CMPD and MULT-
CMPD, clearly learn a correlation with lexical-
ity rankings, they underperform the strong, semi-
supervised COSLEX baselines described earlier in
this paper. The second evaluation, on the binary
TRATZ data set shows a different picture (see Fig-
ure 3). The best COSLEX baseline (ADD with
w = 0.2) fails to outperform random choice on this
task. Both generative models clearly beat COSLEX
on this task, with MULT-CMPD in particular per-
forming very well for low sensitivity.
There is no clear distinction in performance be-
tween the two generative approaches. Further anal-
ysis might help us to separate the two more clearly,
and we will continue using both models throughout
this paper.
It is important to note the different performance of
the generative models vs. the cosine similarity ap-
proach on two tasks. The REDDY data set has a
nearly linear distribution of compositionality scores,
while the TRATZ data set is overwhelmingly com-
positional, which more closely represents the real
world distribution of compounds. The poor perfor-
mance of the cosine similarity approach (COSLEX)
on the TRATZ evaluation suggests the limitations
of this approach when applied to more realistic data
such as this data set. An additional explanation for
the semi-supervised baseline?s poorer result is that
the effect of parameter tuning decreases on larger
data.
Investigating the errors made by the models
MULT-CMPD and BIN-CMPD gives rise to a number
of possible explanations for their performance. The
most promising lead is related to data sparsity, with
many of the evaluated noun-noun compounds only
appearing once or twice in the corpus. This makes it
harder for our generative approach to learn sensible
context distributions for these instances.
We will next investigate how to reduce the effects
encountered by sparsity.
6.2 Interpolation
Working on problems related to non-unigram data,
sparsity is a frequently encountered problem. As al-
ready explored in the previous section, this is also
the case for our generative models of lexicality.
It would be possible to use an even larger training
corpus, but there are limitations as to what extent
this is possible. The BNC, containing 100 million
words, is already one of the largest corpora regu-
larly used in Computational Linguistics. However,
adding more data in an unsupervised sense is un-
likely to significantly improve results (Brants et al,
2007).
Alternatively, it would be possible to add spe-
cific training data that included the noun compounds
from the evaluation data sets. This would, how-
ever, compromise the unsupervised nature of our ap-
proach, and it thus not an option either.
In this paper, we will instead focus on extenuat-
ing the effects of data sparsity through other unsu-
pervised means. For this purpose we investigate in-
terpolating on a larger set of noun compounds.
Kim and Baldwin (2007) observed that seman-
tic similarity of verb-particle compounds correlates
with their lexicality. We extend this observation for
noun compounds, hypothesising that the lexicality
of similar words will be similar. We combine this
with the assumption that noun compounds sharing a
constituent are likely to be semantically similar (Ko-
rkontzelos and Manandhar, 2009).
Using this idea, we can approximate the lexical-
ity of a given compound with the lexicality scores of
all compounds sharing either of its constituents. So
far we have calculated the lexicality of a given com-
pound using the formula Lex(c) in Equation 1. The
formula Clex(c) in Equation 2 averages the lexical-
ity scores of a compound with those of its related
137
Function and Model r ?
COSLEX (ADD) .323 .567
COSLEX (MULT) .379 .551
Lex(c)
MULT-CMPD .141 .435
BIN-CMPD .168 .410
Clex(c)
MULT-CMPD .357 .596
BIN-CMPD .400 .592
Ilex(c)
MULT-CMPD .422 .621
BIN-CMPD .538 .623
Table 3: Results on the REDDY data set, reporting
Pearson?s r and Spearman?s ? correlations, comparing
Ilex(c) and Clec(c) interpolations with Lex(c).
compounds. As p(z=1|?a, h?) directly influences
both p(z=1|?a, ??) and p(z=1|??, h?), we can also
consider dropping it from the approximation such as
in Equation 3. This approach trades some specificity
in favour of reducing sparsity, as we observe more
instances of such related compounds than of a par-
ticular noun compound itself only.
Lex(c) ? Clex(c) (2)
Clex(c) =
p(z=1|?a, ??) + p(z=1|??, h?) + p(z=1|?a, h?)
3
,
where c = ?a, h?
Lex(c) ? Ilex(c) (3)
Ilex(c) =
p(z=1|?a, ??) + p(z=1|??, h?)
2
,
where c = ?a, h?
Both formulations enable us to better deal with
sparse data as decisions are made based on a wider
range of observations. At the same time, we avoid a
loss of specificity as the models and scores are still
highly dependent on the individual noun compound.
We avoid introducing additional degrees of free-
dom by using uniform weights only. However, it
would be simple to turn this approach into a semi-
supervised model by tuning the weights for the dif-
ferent probabilities involved in calculating Clex(c)
and Lex(c). That approach would be comparable to
the operators used on our COSLEX baselines.
Results on the REDDY data set using Clex(c)
and Ilex(c) are in Table 3. Figure 4 shows the im-
pact of these approximations on the Tratz data for
the BIN-CMPD model. These interpolations suggest
strong improvements in performance. It should es-
pecially be noted that Ilex(c) consistently outper-
forms Clex(c), which indicates the strength of the
Figure 4: ROC analysis of model BIN-CMPD on the
TRATZ data set, comparing Ilex(c) and Clec(c) inter-
polations with Lex(c).
related-compound probabilities over the individual
compound probabilities.
These results confirm our suspicion that sparsity
was a major factor affecting our models? perfor-
mance. Furthermore, they strengthen our hypothe-
sis about the relatedness of semantic similarity and
lexicality and demonstrate a sensible approach for
exploiting this relationship.
7 Analysis
We use this section for qualitative evaluation, com-
plementing the quantitative evaluation in the previ-
ous sections. The purpose of the qualitative evalu-
ation is to better understand exactly what it is our
models are learning.
Table 5 lists the compounds that model BIN-
CMPD considers the most lexical and the most com-
positional. The list of compounds with the high lex-
icality scores is dominated by proper nouns such as
countries, companies and persons. This is in line
with expectation as compounds of proper nouns are
fully lexical. Removing proper nouns (also in Table
5), we get a slightly more ambiguous list. For exam-
ple, ?study design? is not considered a lexical com-
pound, but rather a highly institutionalized, com-
positional MWE (Sag et al, 2002). Using Lex(c)
?study design? is ranked as such, so this appears to
be a case where interpolation has a negative impact.
In this paper we argued for a finer grained analysis
of compositionality, taking into account the differ-
138
Context of ?flea market? generated by
flea market flea market
canal, wall, incline,
campsite
stall, Paris, sale,
Saturday, week,
Sunday, quarter,
damage, change
barter, souvenir,
launderette,
Lamine, Canet,
Kouyate, Plage
Context of ?night owl? generated by
night owl night owl
court, fee, guest,
early, day, Baden,
membership, life,
game
waive, player,
Halikarnas, bar,
bird, unbooked,
Vienna
adventurous
Context of ?memory lane? generated by
memory lane memory lane
take, story, about,
tell, real, glimpse,
Britain, reminis-
cence
village, protection,
drive, catwalk,
plant
war, justify, bill,
Campbell, rude-
boys
Context of ?melting pot? generated by
melting pot melting pot
forest, racial,
caribbean, plan,
programme, real-
ity, arrangement
in, into, put, polit-
ical, community,
prepare
ethnic, greatest,
drawing, liaise,
pan-european,
myth
Table 4: Overview over context words generated by model BIN-CMPD. We list a selection of words predominately
generated by each of the mixture components of the given noun-noun compound.
Most Compositional
labour union, tax authority, health council,
market counterparty, employment policy
Most Lexical
study design, family motto, wood shaving,
avoidance behaviour, smash hit
Most Lexical (including Proper Nouns)
Vo Quy, Bonito Oliva, Mamur Zapt, Evander
Holyfield, Saudi Arabia
Table 5: Top lexical and compositional nouns for the
BIN-CMPD model using Ilex(c)
ent impact of both constituents. We tried to achieve
this by modelling a compound?s context as gener-
ated from its various semantic constituents. Table 4
highlights the impact of this method for a number
of noun compounds, showing which context words
were predominately generated by each constituent.
Due to the nature of the context used, some of
the links are semantically not obvious (e.g. the rela-
tionship between owls and Vienna). In some cases
the semantic contribution of the parts is more clearly
separated, such as the contributions of ?memory? and
?lane? to the semantics of ?memory lane?. In sum-
mary, these examples clearly suggest that our mod-
els learn to associate context with compound ele-
ments and that this association is an informed one.
8 Conclusion
We proposed a novel approach for learning lexicality
scores for noun compounds and empirically demon-
strated the feasiblity of this approach. Using a gen-
erative model we were able to beat a strong, semi-
supervised baseline with an unsupervised model.
We discussed the issue of data sparsity in depth
and proposed several approaches for overcoming
this problem. Focusing on unsupervised approaches,
we demonstrated how interpolation can be used to
tackle sparsity. The two interpolation methods that
we implemented helped us to strongly improve over-
all model performance. Our empirical evaluation of
interpolation metricsClex(c) and Ilex(c) also gives
credence to the hypothesis that lexicality is related to
semantic similarity.
On the theoretical side, we offered further support
to the real-valued treatment of lexicality.
Further work will include using larger training
corpora. While the BNC is a popular corpus in Com-
putational Linguistics, it proved to be too small to
learn sensible representations for a number of com-
pounds encountered in the test data. Using larger
corpora will also allow us to further study and re-
duce the sparsity issues encountered.
To study the relationship between constituent and
compound compositionality in greater depth, we
will also investigate alternative approaches for in-
terpolation. Similarity measures that consider the
semantic relevance of individual context elements
should also be considered as a next step.
Another obvious source of future work is to ap-
ply our approach to general collocations beyond the
special case of noun compounds only.
Acknowledgments
The authors would like to acknowledge the use of
the Oxford Supercomputing Centre (OSC) in carry-
ing out this work.
139
References
Timothy Baldwin. 2006. Compositionality and mul-
tiword expressions: Six of one, half a dozen of the
other? In Proceedings of the Workshop on Multiword
Expressions: Identifying and Exploiting Underlying
Properties, page 1, Sydney, Australia. Association for
Computational Linguistics.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 1183?1193, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large Language Mod-
els in Machine Translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 858?
867.
Fan Bu, Xiaoyan Zhu, and Ming Li. 2010. Measuring
the non-compositionality of multiword expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics, COLING ?10, pages 116?
124, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-
muid O?. Se?aghdha, Stan Szpakowicz, and Tony Veale.
2010. Semeval-2010 task 9: The interpretation of
noun compounds using paraphrasing verbs and prepo-
sitions. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, SemEval ?10, pages 39?
44, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification
of idiomatic expressions. Computational Linguistics,
35(1):61?103.
Eugenie Giesbrecht. 2009. In search of semantic com-
positionality in vector spaces. In Proceedings of the
17th International Conference on Conceptual Struc-
tures: Conceptual Structures: Leveraging Semantic
Technologies, ICCS ?09, pages 173?184, Berlin, Hei-
delberg. Springer-Verlag.
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using wordnet simi-
larity. In In Proceedings of the 2nd International Joint
Conference on Natural Language Processing, Jeju Is-
land, South Korea, 1113, pages 945?956.
Su Nam Kim and Timothy Baldwin. 2007. Detect-
ing compositionality of English verb-particle construc-
tions using semantic similarity. In Proceedings of the
7th Meeting of the Pacific Association for Computa-
tional Linguistics, PACLING ?07, pages 40?48.
Su Nam Kim and Timothy Baldwin. 2008. An unsu-
pervised approach to interpreting noun compounds. In
Natural Language Processing and Knowledge Engi-
neering, 2008. NLP-KE ?08. International Conference
on, pages 1?7.
S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. 1983.
Optimization by simulated annealing. Science,
220(4598):671?680.
Ioannis Korkontzelos and Suresh Manandhar. 2009. De-
tecting compositionality in multi-word expressions.
In Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, ACLShort ?09, pages 65?68, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ioannis Korkontzelos and Suresh Manandhar. 2010. Can
recognising multiword expressions improve shallow
parsing? In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ?10, pages 636?644, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Mark Lauer. 1995. Corpus statistics meet the noun com-
pound: some empirical results. In Proceedings of
the 33rd annual meeting on Association for Compu-
tational Linguistics, ACL ?95, pages 47?54, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Diana McCarthy, Sriram Venkatapathy, and Aravind
Joshi. 2007. Detecting compositionality of verb-
object combinations using selectional preferences. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 369?379, Prague, Czech Republic. As-
sociation for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Diarmuid O? Se?aghdha. 2007. Annotating and learning
compound noun semantics. In Proceedings of the 45th
Annual Meeting of the ACL: Student Research Work-
shop, ACL ?07, pages 73?78, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Ted Pedersen. 2011. Identifying collocations to mea-
sure compositionality: shared task system description.
In Proceedings of the Workshop on Distributional Se-
mantics and Compositionality, DiSCo ?11, pages 33?
37, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in com-
140
pound nouns. In Proceedings of The 5th Interna-
tional Joint Conference on Natural Language Process-
ing 2011 (IJCNLP 2011), Chiang Mai, Thailand.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for NLP. In In Proc.
of the 3rd International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing-
2002, pages 1?15.
Karen Spa?rck Jones. 1985. Compound noun interpre-
tation problems. In Frank Fallside and William A.
Woods, editors, Computer speech processing, pages
363?381. Prentice Hall International (UK) Ltd., Hert-
fordshire, UK, UK.
Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, ACL ?10, pages 678?687, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional dis-
tributional semantics. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, COLING ?10, pages 1263?1271, Stroudsburg,
PA, USA. Association for Computational Linguistics.
141
