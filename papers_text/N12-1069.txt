2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 577?581,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Concavity and Initialization for Unsupervised Dependency Parsing
Kevin Gimpel and Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{kgimpel,nasmith}@cs.cmu.edu
Abstract
We investigate models for unsupervised learn-
ing with concave log-likelihood functions. We
begin with the most well-known example,
IBM Model 1 for word alignment (Brown
et al, 1993) and analyze its properties, dis-
cussing why other models for unsupervised
learning are so seldom concave. We then
present concave models for dependency gram-
mar induction and validate them experimen-
tally. We find our concave models to be effec-
tive initializers for the dependency model of
Klein and Manning (2004) and show that we
can encode linguistic knowledge in them for
improved performance.
1 Introduction
In NLP, unsupervised learning typically implies op-
timization of a ?bumpy? objective function riddled
with local maxima. However, one exception is IBM
Model 1 (Brown et al, 1993) for word alignment,
which is the only model commonly used for unsu-
pervised learning in NLP that has a concave log-
likelihood function.1 For other models, such as
those used in unsupervised part-of-speech tagging
and grammar induction, and indeed for more sophis-
ticated word alignment models, the log-likelihood
function maximized by EM is non-concave. As a
result, researchers are obligated to consider initial-
ization in addition to model design (Klein and Man-
ning, 2004; Goldberg et al, 2008).
For example, consider the dependency grammar
induction results shown in Table 1 when training the
1It is not strictly concave (Toutanova and Galley, 2011).
widely used dependency model with valence (DMV;
Klein and Manning, 2004). Using uniform distri-
butions for initialization (UNIF) results in an accu-
racy of 17.6% on the test set, well below the base-
line of attaching each word to its right neighbor
(ATTACHRIGHT, 31.7%). Furthermore, when using
a set of 50 random initializers (RAND), the standard
deviation of the accuracy is an alarming 8.3%.
In light of this sensitivity to initialization, it is
compelling to consider unsupervised models with
concave log-likelihood functions, which may pro-
vide stable, data-supported initializers for more
complex models. In this paper, we explore the issues
involved with such an expedition and elucidate the
limitations of such models for unsupervised NLP.
We then present simple concave models for depen-
dency grammar induction that are easy to implement
and offer efficient optimization. We also show how
linguistic knowledge can be encoded without sacri-
ficing concavity. Using our models to initialize the
DMV, we find that they lead to an improvement in
average accuracy across 18 languages.
2 IBM Model 1 and Concavity
IBM Model 1 is a conditional model of a target-
language sentence e of length m and an alignment
a given a source-language sentence f of length l.
The generation of m is assumed to occur with some
(inconsequential) uniform probability . The align-
ment vector a, a hidden variable, has an entry for
each element of e that contains the index in f of
the aligned word. These entries are used to define
which translation parameters t(ej | faj ) are active.
Model 1 assumes that the probability of the ith ele-
577
ment in a, denoted a(i | j, l,m), is simply a uni-
form distribution over all l source words plus the
null word. These assumptions result in the follow-
ing log-likelihood for a sentence pair ?f , e? under
Model 1 (marginalizing a):
log p(e | f) = log (l+1)m+
?m
j=1 log
?l
i=0 t(ej | fi)
(1)
The only parameters to be learned in the model are
t = {t(e | f)}e,f . Since a parameter is concave in
itself, the sum of concave functions is concave, and
the log of a concave function is concave, Eq. 1 is
concave in t (Brown et al, 1993).
IBM Model 2 involves a slight change to Model
1 in which the probability of a word link depends
on the word positions. However, this change renders
it no longer concave. Consider the log-likelihood
function for Model 2:
log +
?m
j=1 log
?l
i=0 t(ej | fi) ?a(i | j, l,m) (2)
Eq. 2 is not concave in the parameters t(ej | fi) and
a(i | j, l,m) because a product is neither convex nor
concave in its vector of operands. This can be shown
by computing the Hessian matrix of f(x, y) = xy
and showing that it is indefinite.
In general, concavity is lost when the log-
likelihood function contains a product of model pa-
rameters enclosed within a log
?
. If the sum is not
present, the log can be used to separate the prod-
uct of parameters, making the function concave. It
can also be shown that a ?featurized? version (Berg-
Kirkpatrick et al, 2010) of Model 1 is not con-
cave. More generally, any non-concave function en-
closed within log
?
will cause the log-likelihood
function to be non-concave, though there are few
other non-concave functions with a probabilistic se-
mantics than those just discussed.
3 Concave, Unsupervised Models
Nearly every other model used for unsupervised
learning in NLP has a non-concave log-likelihood
function. We now proceed to describe the conditions
necessary to develop concave models for two tasks.
3.1 Part-of-Speech Tagging
Consider a standard first-order hidden Markov
model for POS tagging. Letting y denote the tag
sequence for a sentence e with m tokens, the single-
example log-likelihood is:
log
?
y p(stop | ym)
?m
j=1 p(yj | yj?1) ? p(ej | yj)
(3)
where y0 is a designated ?start? symbol. Unlike IBM
Models 1 and 2, we cannot reverse the order of the
summation and product here because the transition
parameters p(yj | yj?1) cause each tag decision to
affect its neighbors. Therefore, Eq. 3 is non-concave
due to the presence of a product within a log
?
.
However, if the tag transition probabilities p(yj |
yj?1) are all constants and also do not depend on
the previous tag yj?1, then we can rewrite Eq. 3 as
the following concave log-likelihood function (using
C(y) to denote a constant function of tag y, e.g., a
fixed tag prior distribution):
logC(stop) + log
?m
j=1
?
yj
C(yj) ? p(ej | yj)
Lacking any transition modeling power, this model
appears weak for POS tagging. However, we note
that we can add additional conditioning information
to the p(ej | yj) distributions and retain concavity,
such as nearby words and tag dictionary informa-
tion. We speculate that such a model might learn
useful patterns about local contexts and provide an
initializer for unsupervised part-of-speech tagging.
3.2 Dependency Grammar Induction
To develop dependency grammar induction models,
we begin with a version of Model 1 in which a sen-
tence e is generated from a copy of itself (denoted
e?): log p(e | e?)
= log (m+1)m +
?m
j=1 log
?m
i=0,i 6=j c(ej | e
?
i) (4)
If a word ej is ?aligned? to e?0, ej is a root. This
is a simple child-generation model with no tree con-
straint. In order to preserve concavity, we are forbid-
den from conditioning on other parent-child assign-
ments or including any sort of larger constraints.
However, we can condition the child distributions
on additional information about e? since it is fully
observed. This conditioning information may in-
clude the direction of the edge, its distance, and
any properties about the words in the sentence. We
found that conditioning on direction improved per-
formance: we rewrite the c distributions as c(ej |
e?i, sign(j ? i)) and denote this model by CCV1.
578
We note that we can also include constraints in the
sum over possible parents and still preserve concav-
ity. Naseem et al (2010) found that adding parent-
child constraints to a grammar induction system can
improve performance dramatically. We employ one
simple rule: roots are likely to be verbs.2 We mod-
ify CCV1 to restrict the summation over parents to
exclude e?0 if the child word is not a verb.
3 We only
employ this restriction during EM learning for sen-
tences containing at least one verb. For sentences
without verbs, we allow all words to be the root. We
denote this model by CCV2.
In related work, Brody (2010) also developed
grammar induction models based on the IBM word
alignment models. However, while our goal is to
develop concave models, Brody employed Bayesian
nonparametrics in his version of Model 1, which
makes the model non-concave.
4 Experiments
We ran experiments to determine how well our con-
cave grammar induction models CCV1 and CCV2 can
perform on their own and when used as initializers
for the DMV (Klein and Manning, 2004). The DMV
is a generative model of POS tag sequences and pro-
jective dependency trees over them. It is the foun-
dation of most state-of-the-art unsupervised gram-
mar induction models (several of which are listed in
Tab. 1). The model includes multinomial distribu-
tions for generating each POS tag given its parent
and the direction of generation: where ei is the par-
ent POS tag and ej the child tag, these distributions
take the form c(ej | ei, sign(j ? i)), analogous to
the distributions used in our concave models. The
DMV also has multinomial distributions for decid-
ing whether to stop or continue generating children
in each direction considering whether any children
have already been generated in that direction.
The majority of researchers use the original ini-
tializer from Klein and Manning (2004), denoted
here K&M. K&M is a deterministic harmonic initial-
izer that sets parent-child token affinities inversely
2This is similar to the rule used by Marec?ek and Z?abokrtsky?
(2011) with empirical success.
3As verbs, we take all tags that map to V in the universal tag
mappings from Petrov et al (2012). Thus, to apply this con-
straint to a new language, one would have to produce a similar
tag mapping or identify verb tags through manual inspection.
Train ? 10 Train ? 20
Test Test
Model Init. ?10 ?? ?10 ??
ATTRIGHT N/A 38.4 31.7 38.4 31.7
CCV1 UNIF 31.4 25.6 31.0 23.7
CCV2 UNIF 43.1 28.6 43.9 27.1
UNIF 21.3 17.6 21.3 16.4
RAND? 41.0 31.8 - -
DMV K&M 44.1 32.9 51.9 37.8
CCV1 45.3 30.9 53.9 36.7
CCV2 54.3 43.0 64.3 53.1
Shared LN K&M 61.3 41.4
L-EVG RAND? 68.8 -
Feature DMV K&M 63.0 -
LexTSG-DMV K&M 67.7 55.7
Posterior Reg. K&M 64.3 53.3
Punc/UTags K&M? - 59.1?
Table 1: English attachment accuracies on Section 23, for
short sentences (?10 words) and all (??). We include
selected results on this same test set: Shared LN = Cohen
and Smith (2009), L-EVG = Headden III et al (2009),
Feature DMV = Berg-Kirkpatrick et al (2010), LexTSG-
DMV = Blunsom and Cohn (2010), Posterior Reg. =
Gillenwater et al (2010), Punc/UTags = Spitkovsky et
al. (2011a). K&M? is from Spitkovsky et al (2011b).
?Accuracies are averages over 50 random initializers;
? = 10.9 for test sentences ? 10 and 8.3 for all. ?Used
many random initializers with unsupervised run selec-
tion. ?Used staged training with sentences ? 45 words.
proportional to their distances, then normalizes to
obtain probability distributions. K&M is often de-
scribed as corresponding to an initial E step for an
unspecified model that favors short attachments.
Procedure We run EM for our concave models for
100 iterations. We evaluate the learned models di-
rectly as parsers on the test data and also use them
to initialize the DMV. When using them directly as
parsers, we use dynamic programming to ensure that
a valid tree is recovered. When using the concave
models as initializers for the DMV, we copy the c
parameters over directly since they appear in both
models. We do not have the stop/continue parame-
ters in our concave models, so we simply initialize
them uniformly for the DMV. We train each DMV
for 200 iterations and use minimum Bayes risk de-
coding with the final model on the test data. We use
several initializers for training the DMV, including
the uniform initializer (UNIF), K&M, and our trained
concave models CCV1 and CCV2.
579
Init. eu bg ca zh cs da nl en de el hu
UNIF 24/21 32/26 27/29 44/40 32/30 24/19 21/21 21/18 31/24 37/32 23/18
K&M 32/26 48/40 24/25 38/33 31/29 34/23 39/33 44/33 47/37 50/41 23/20
CCV1 22/21 34/27 44/51 46/45 33/31 19/14 24/24 45/31 46/31 51/45 32/28
CCV2 26/25 34/26 29/35 46/44 50/40 29/18 50/43 54/43 49/33 50/45 60/46
it ja pt sl es sv tr avg. accuracy avg. log-likelihood
UNIF 31/24 35/30 49/36 20/20 29/24 26/22 33/30 29.8 / 25.7 -15.05
K&M 32/24 39/31 44/28 33/27 19/11 46/33 39/36 36.7 / 29.4 -14.84
CCV1 34/25 42/27 50/38 30/25 41/33 45/33 37/29 37.5 / 30.9 -14.93
CCV2 55/48 49/31 50/38 22/21 57/50 46/32 31/22 43.7 / 35.5 -14.45
Table 2: Test set attachment accuracies for 18 languages; first number in each cell is accuracy for sentences ? 10
words and second is for all sentences. For training, sentences ? 10 words from each treebank were used. In order,
languages are Basque, Bulgarian, Catalan, Chinese, Czech, Danish, Dutch, English, German, Greek, Hungarian,
Italian, Japanese, Portuguese, Slovenian, Spanish, Swedish, and Turkish.
Data We use data prepared for the CoNLL
2006/07 shared tasks (Buchholz and Marsi, 2006;
Nivre et al, 2007).4 We follow standard practice
in removing punctuation and using short sentences
(? 10 or ? 20 words) for training. For all experi-
ments, we train on separate data from that used for
testing and use gold POS tags for both training and
testing. We report accuracy on (i) test set sentences
?10 words and (ii) all sentences from the test set.
Results Results for English are shown in Tab. 1.
We train on ?2?21 and test on ?23 in the Penn Tree-
bank. The constraint on sentence roots helps a great
deal, as CCV2 by itself is competitive with the DMV
when testing on short sentences. The true benefit of
the concave models, however, appears when using
them as initializers. The DMV initialized with CCV2
achieves a substantial improvement over all others.
When training on sentences of length ? 20 words
(bold), the performance even rivals that of several
more sophisticated models shown in the table, de-
spite only using the DMV with a different initializer.
Tab. 2 shows results for 18 languages. On av-
erage, CCV2 performs best and CCV1 does at least
as well as K&M. This shows that a simple, concave
model can be as effective as a state-of-the-art hand-
designed initializer (K&M), and that concave mod-
els can encode linguistic knowledge to further im-
prove performance.
4In some cases, we did not use official CoNLL test sets but
instead took the training data and reserved the first 80% of the
sentences for training, the next 10% for development, and the
final 10% as our test set; dataset details are omitted for space
but are the same as those given by Cohen (2011).
Average log-likelihoods (micro-averaged across
sentences) achieved by EM training are shown in the
final column of Tab. 2. CCV2 leads to substantially-
higher likelihoods than the other initializers, sug-
gesting that the verb-root constraint is helping EM
to find better local optima.5
5 Discussion
Staged training has been shown to help unsupervised
learning in the past, from early work in grammar in-
duction (Lari and Young, 1990) and word alignment
(Brown et al, 1993) to more recent work in depen-
dency grammar induction (Spitkovsky et al, 2010).
While we do not yet offer a generic procedure for
extracting a concave approximation from any model
for unsupervised learning, our results contribute evi-
dence in favor of the general methodology of staged
training in unsupervised learning, and provide a sim-
ple and powerful initialization method for depen-
dency grammar induction.
Acknowledgments
We thank Shay Cohen, Dipanjan Das, Val Spitkovsky,
and members of the ARK research group for helpful com-
ments that improved this paper. This research was sup-
ported in part by the NSF through grant IIS-0915187, the
U. S. Army Research Laboratory and the U. S. Army Re-
search Office under contract/grant number W911NF-10-
1-0533, and Sandia National Laboratories (fellowship to
K. Gimpel).
5However, while CCV1 leads to a higher average accuracy
than K&M, the latter reaches slightly higher likelihood, sug-
gesting that the success of the concave initializers is only par-
tially due to reaching high training likelihood.
580
References
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In Proc. of NAACL.
P. Blunsom and T. Cohn. 2010. Unsupervised induction
of tree substitution grammars for dependency parsing.
In Proc. of EMNLP.
S. Brody. 2010. It depends on the translation: Unsu-
pervised dependency parsing via word alignment. In
Proc. of EMNLP.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
S. Cohen and N. A. Smith. 2009. Shared logistic normal
distributions for soft parameter tying in unsupervised
grammar induction. In Proc. of NAACL.
S. Cohen. 2011. Computational Learning of Probabilis-
tic Grammars in the Unsupervised Setting. Ph.D. the-
sis, Carnegie Mellon University.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, , and
B. Taskar. 2010. Posterior sparsity in unsupervised
dependency parsing. Journal of Machine Learning
Research.
Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM
can find pretty good HMM POS-taggers (when given
a good start). In Proc. of ACL.
W. Headden III, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with
richer contexts and smoothing. In Proc. of NAACL.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In Proc. of ACL.
K. Lari and S. J. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer Speech and Language, 4:35?56.
D. Marec?ek and Z. Z?abokrtsky?. 2011. Gibbs sampling
with treeness constraint in unsupervised dependency
parsing. In Proc. of Workshop on Robust Unsuper-
vised and Semisupervised Methods in Natural Lan-
guage Processing.
T. Naseem, H. Chen, R. Barzilay, and M. Johnson. 2010.
Using universal linguistic knowledge to guide gram-
mar induction. In Proc. of EMNLP.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc. of
CoNLL.
S. Petrov, D. Das, and R. McDonald. 2012. A universal
part-of-speech tagset. In Proc. of LREC.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010.
From Baby Steps to Leapfrog: How ?Less is More?
in unsupervised dependency parsing. In Proc. of
NAACL-HLT.
V. I. Spitkovsky, H. Alshawi, A. X. Chang, and D. Juraf-
sky. 2011a. Unsupervised dependency parsing with-
out gold part-of-speech tags. In Proc. of EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b.
Punctuation: Making a point in unsupervised depen-
dency parsing. In Proc. of CoNLL.
K. Toutanova and M. Galley. 2011. Why initialization
matters for IBM Model 1: Multiple optima and non-
strict convexity. In Proc. of ACL.
581
