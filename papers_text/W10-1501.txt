Proceedings of the NAACL HLT 2010 Sixth Web as Corpus Workshop, pages 1?7,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
NoWaC: a large web-based corpus for Norwegian
Emiliano Guevara
Tekstlab,
Institute for Linguistics and Nordic Studies,
University of Oslo
e.r.guevara@iln.uio.no
Abstract
In this paper we introduce the first version
of noWaC, a large web-based corpus of Bok-
m?l Norwegian currently containing about
700 million tokens. The corpus has been
built by crawling, downloading and process-
ing web documents in the .no top-level in-
ternet domain. The procedure used to col-
lect the noWaC corpus is largely based on
the techniques described by Ferraresi et al
(2008). In brief, first a set of ?seed? URLs
containing documents in the target language
is collected by sending queries to commer-
cial search engines (Google and Yahoo). The
obtained seeds (overall 6900 URLs) are then
used to start a crawling job using the Heritrix
web-crawler limited to the .no domain. The
downloaded documents are then processed in
various ways in order to build a linguistic cor-
pus (e.g. filtering by document size, language
identification, duplicate and near duplicate de-
tection, etc.).
1 Introduction and motivations
The development, training and testing of NLP tools
requires suitable electronic sources of linguistic data
(corpora, lexica, treebanks, ontological databases,
etc.), which demand a great deal of work in or-
der to be built and are, very often copyright pro-
tected. Furthermore, the ever growing importance of
heavily data-intensive NLP techniques for strategic
tasks such as machine translation and information
retrieval, has created the additional requirement that
these electronic resources be very large and general
in scope.
Since most of the current work in NLP is carried
out with data from the economically most impact-
ing languages (and especially with English data),
an amazing wealth of tools and resources is avail-
able for them. However, researchers interested in
?smaller? languages (whether by the number of
speakers or by their market relevance in the NLP
industry) must struggle to transfer and adapt the
available technologies because the suitable sources
of data are lacking. Using the web as corpus is a
promising option for the latter case, since it can pro-
vide with reasonably large and reliable amounts of
data in a relatively short time and with a very low
production cost.
In this paper we present the first version of
noWaC, a large web-based corpus of Bokm?l Nor-
wegian, a language with a limited web presence,
built by crawling the .no internet top level domain.
The computational procedure used to collect the
noWaC corpus is by and large based on the tech-
niques described by Ferraresi et al (2008). Our
initiative was originally aimed at collecting a 1.5?2
billion word general-purpose corpus comparable to
the corpora made available by the WaCky initiative
(http://wacky.sslmit.unibo.it). How-
ever, carrying out this project on a language with a
relatively small online presence such as Bokm?l has
lead to results which differ from previously reported
similar projects. In its current, first version, noWaC
contains about 700 million tokens.
1
1.1 Norwegian: linguistic situation and
available corpora
Norway is a country with a population of ca. 4.8 mil-
lion inhabitants that has two official national written
standards: Bokm?l and Nynorsk (respectively, ?book
language? and ?new Norwegian?). Of the two stan-
dards, Bokm?l is the most widely used, being ac-
tively written by about 85% of the country?s popu-
lation (cf. http://www.sprakrad.no/ for de-
tailed up to date statistics). The two written stan-
dards are extremely similar, especially from the
point of view of their orthography. In addition, Nor-
way recognizes a number of regional minority lan-
guages (the largest of which, North Sami, has ca.
15,000 speakers).
While the written language is generally standard-
ized, the spoken language in Norway is not, and
using one?s dialect in any occasion is tolerated and
even encouraged. This tolerance is rapidly extend-
ing to informal writing, especially in modern means
of communication and media such as internet fo-
rums, social networks, etc.
There is a fairly large number of corpora of the
Norwegian language, both spoken and written (in
both standards). However, most of them are of a
limited size (under 50 million words, cf. http://
www.hf.uio.no/tekstlab/ for an overview).
To our knowledge, the largest existing written cor-
pus of Norwegian is the Norsk Aviskorpus (Hofland
2000, cf. http://avis.uib.no/), an expand-
ing newspaper-based corpus currently containing
700 million words. However, the Norsk Aviskor-
pus is only available though a dedicated web inter-
face for non commercial use, and advanced research
tasks cannot be freely carried out on its contents.
Even though we have only worked on building a
web corpus for Bokm?l Norwegian, we intend to
apply the same procedures to create web-corpora
also for Nynorsk and North Sami, thus covering the
whole spectrum of written languages in Norway.
1.2 Obtaining legal clearance
The legal status of openly accessible web-
documents is not clear. In practice, when one
visits a web page with a browsing program, an
electronic exact copy of the remote document is
created locally; this logically implies that any online
document must be, at least to a certain extent,
copyright-free if it is to be visited/viewed at all.
This is a major difference with respect to other types
of documents (e.g. printed materials, films, music
records) which cannot be copied at all.
However, when building a web corpus, we do not
only wish to visit (i.e. download) web documents,
but we would like to process them in various ways,
index them and, finally, make them available to other
researchers and users in general. All of this would
ideally require clearance from the copyright holders
of each single document in the corpus, something
which is simply impossible to realize for corpora
that contain millions of different documents.1
In short, web corpora are, from the legal point of
view, still a very dark spot in the field of computa-
tional linguistics. In most countries, there is simply
no legal background to refer to, and the internet is a
sort of no-man?s land.
Norway is a special case: while the law explicitly
protects online content as intellectual property,
there is rather new piece of legislation in Forskrift
til ?ndsverkloven av 21.12 2001 nr. 1563, ? 1-4
that allows universities and other research insti-
tutions to ask for permission from the Ministry
of Culture and Church in order to use copyright
protected documents for research purposes that
do not cause conflict with the right holders?
own use or their economic interests (cf. http:
//www.lovdata.no/cgi-wift/ldles?
ltdoc=/for/ff-20011221-1563.html).
We have been officially granted this permission for
this project, and we can proudly say that noWaC
is a totally legal and recognized initiative. The
results of this work will be legally made available
free of charge for research (i.e. non commercial)
purposes. NoWaC will be distributed in association
with the WaCky initiative and also directly from the
University of Oslo.
1Search engines are in a clear contradiction to the copyright
policies in most countries: they crawl, download and index bil-
lions of documents with no clearance whatsoever, and also re-
distribute whole copies of the cached documents.
2
2 Building a corpus of Bokm?l by
web-crawling
2.1 Methods and tools
In this project we decided to follow the methods
used to build the WaCky corpora, and to use the re-
lated tools as much as possible (e.g. the BootCaT
tools). In particular, we tried to reproduce the pro-
cedures described by Ferraresi et al (2008) and Ba-
roni et al (2009). The methodology has already
produced web-corpora ranging from 1.7 to 2.6 bil-
lion tokens (German, Italian, British English). How-
ever, most of the steps needed some adaptation, fine-
tuning and some extra programming. In particu-
lar, given the relatively complex linguistic situation
in Norway, a step dedicated to document language
identification was added.
In short, the building and processing chain used
for noWaC comprises the following steps:
1. Extraction of list of mid-frequency Bokm?l
words from Wikipedia and building query
strings
2. Retrieval of seed URLs from search engines by
sending automated queries, limited to the .no
top-level domain
3. Crawling the web using the seed URLS, limited
to the .no top-level domain
4. Removing HTML boilerplate and filtering doc-
uments by size
5. Removing duplicate and near-duplicate docu-
ments
6. Language identification and filtering
7. Tokenisation
8. POS-tagging
At the time of writing, the first version of noWaC is
being POS-tagged and will be made available in the
course of the next weeks.
2.2 Retrieving seed URLs from search engines
We started by obtaining the Wikipedia text dumps
for Bokm?l Norwegian and related languages
(Nynorsk, Danish, Swedish and Icelandic) and se-
lecting the 2000 most frequent words that are unique
to Bokm?l. We then sent queries of 2 randomly
selected Bokm?l words though search engine APIs
(Google and Yahoo!). A maximum of ten seed
URLs were saved for each query, and the retrieved
URLs were collapsed in a single list of root URLs,
deduplicated and filtered, only keeping those in the
.no top level domain.
After one week of automated queries (limited
to 1000 queries per day per search engine by the
respective APIs) we had about 6900 filtered seed
URLs.
2.3 Crawling
We used the Heritrix open-source, web-scale
crawler (http://crawler.archive.org/)
seeded with the 6900 URLs we obtained to tra-
verse the internet .no domain and to download
only HTML documents (all other document types
were discarded from the archive). We instructed
the crawler to use a multi-threaded breadth-first
strategy, and to follow a very strict politeness policy,
respecting all robots.txt exclusion directives
while downloading pages at a moderate rate (90
second pause before retrying any URL) in order not
to disrupt normal website activity.
The final crawling job was stopped after 15 days.
In this period of time, a total size of 1 terabyte
was crawled, with approximately 90 million URLs
being processed by the crawler. Circa 17 million
HTML documents were downloaded, adding up to
an overall archive size of 550 gigabytes. Only
about 13.5 million documents were successfully re-
trieved pages (the rest consisting of various ?page
not found? replies and other server-side error mes-
sages).
The documents in the archive were filtered by
size, keeping only those documents that were be-
tween 5Kb and 200Kb in size (following Ferraresi
et al 2008 and Baroni et al 2009). This resulted
in a reduced archive of 11.4 million documents for
post-processing.
2.4 Post-processing: removing HTML
boilerplate and de-duplication
At this point of the process, the archive contained
raw HTML documents, still very far from being a
linguistic corpus. We used the BootCaT toolkit (Ba-
roni and Bernardini 2004, cf. http://sslmit.
unibo.it/~baroni/bootcat.html) to per-
form the major operations to clean our archive.
First, every document was processed with the
HTML boilerplate removal tool in order to select
3
only the linguistically interesting portions of text
while removing all HTML, Javascript and CSS code
and non-linguistic material (made mainly of HTML
tags, visual formatting, tables, navigation links, etc.)
Then, the archive was processed with the dupli-
cate and near-duplicate detecting script in the the
BootCaT toolkit, based on a 5-gram model. This is
a very drastic strategy leading to a huge reduction in
the number of kept documents: any two documents
sharing more than 1/25 5-grams were considered du-
plicates, and both documents were discarded. The
overall number of documents in the archive went
down from 11.40 to 1.17 million after duplicate re-
moval.2
2.5 Language identification and filtering
The complex linguistic situation in Norway makes
us expect that the Norwegian internet be at least a
bilingual domain (Bokm?l and Nynorsk). In addi-
tion, we also expect a number of other languages to
be present to a lesser degree.
We used Damir Cavar?s tri-gram algorithm
for language identification (cf. http://ling.
unizd.hr/~dcavar/LID/), training 16 lan-
guage models onWikipedia text from languages that
are closely related to, or that have contact with Bok-
m?l (Bokm?l, Danish, Dutch, English, Faeroese,
Finnish, French, German, Icelandic, Italian, North-
ern Sami, Nynorsk, Polish, Russian, Spanish and
Swedish). The best models were trained on 1Mb
of random Wikipedia lines and evaluated against a
database of one hundred 5 Kb article excerpts for
each language. The models performed very well,
often approaching 100% accuracy; however, the ex-
tremely similar orthography of Bokm?l and Nynorsk
make them the most difficult pair of languages to
spot for the system, one being often misclassified as
the other. In any case, our results were relatively
good: Bokm?l Precision = 1.00, Recall = 0.89, F-
measure = 0.94, Nynorsk Precision = 0.90, Recall =
1.00, F-measure = 0.95.
The language identifying filter was applied on a
document basis, recognizing about 3 out of 4 docu-
2As pointed out by an anonymous reviewer, this drastic re-
duction in number of documents may be due to faults in the
boilerplate removal phase, leading to 5-grams of HTML or sim-
ilar code counting as real text. We are aware of this issue, and
the future versions of noWaC will be revised to this effect.
ments as Bokm?l:
? 72.25% Bokm?l
? 16.00% Nynorsk
? 05.80% English
? 02.43% Danish
? 01.95% Swedish
This filter produced another sensible drop in the
overall number of kept documents: from 1.17 to 0.85
million.
2.6 POS-tagging and lemmatization
At the time of writing noWaC is in the process of be-
ing POS-tagged. This is not at all an easy task, since
the best and most widely used tagger for Norwe-
gian (the Oslo-Bergen tagger, cf. Hagen et al 2000)
is available as a binary distribution which, besides
not being open to modifications, is fairly slow and
does not handle large text files. A number of statisti-
cal taggers have been trained, but we are still unde-
cided about which system to use because the avail-
able training materials for Bokm?l are rather lim-
ited (about 120,000 words). The tagging accuracy
we have obtained so far is still not comparable to
the state-of-the-art (94.32% with TnT, 94.40% with
SVMT). In addition, we are also working on creat-
ing a large list of tagged lemmas to be used with
noWaC. We estimate that a final POS-tagged and
lemmatized version of the corpus will be available
in the next few weeks (in any case, before the WAC6
workshop).
3 Comparing results
While it is still too early for us to carry out a
fully fledged qualitative evaluation of noWaC, we
are able to compare our results with previous pub-
lished work, especially with the WaCky corpora we
tried to emulate.
3.1 NoWaC and the WaCky corpora
As we stated above, we tried to follow the WaCky
methodology as closely as possible, in the hopes that
we could obtain a very large corpus (we aimed at
collecting above 1 billion tokens). However, even
though our crawling job produced a much bigger
initial archive than those reported for German, Ital-
ian and British English in Baroni et al (2009), and
4
even though after document size filtering was ap-
plied our archive contained roughly twice as many
documents, our final figures (number of tokens and
number of documents) only amount to about half the
size reported for the WaCky corpora (cf. table 1).
In particular, we observe that the most significant
drop in size and in number of documents took place
during the detection of duplicate and near-duplicate
documents (drastically dropping from 11.4 million
documents to 1.17 million documents after duplicate
filtering). This indicates that, even if a huge num-
ber of documents in Bokm?l Norwegian are present
in the internet, a large portion of them must be
machine generated content containing repeated n-
grams that the duplicate removal tool successfully
identifies and discards.3
These figures, although unexpected by us, may
actually have a reasonable explanation. If we con-
sider that Bokm?l Norwegian has about 4.8 million
potential content authors (assuming that every Nor-
wegian inhabitant is able to produce web documents
in Bokm?l), and given that our final corpus contains
0.85 million documents, this means that we have so
far sampled roughly one document every five poten-
tial writers: as good as it may sound, it is a highly
unrealistic projection, and a great deal of noise and
possibly also machine generated content must still
be present in the corpus. The duplicate removal tools
are only helping us understand that a speaker com-
munity can only produce a limited amount of lin-
guistically relevant online content. We leave the in-
teresting task of estimating the size of this content
and its growth rate for further research. The Norwe-
gian case, being a relatively small but highly devel-
oped information society, might prove to be a good
starting point.
3.2 Scaling noWaC: how much Bokm?l is
there? How much did we get?
The question arises immediately. We want to know
how representative our corpus is, in spite of the fact
that we now know that it must still contain a great
deal of noise and that a great deal of documents were
plausibly not produced by human speakers.
To this effect, we applied the scaling factors
3Although we are aware that the process of duplicate re-
moval in noWaC must be refined further, constituting in itself
an interesting research area.
methodology used by Kilgarrif (2007) to estimate
the size of the Italian and German internet on the
basis of the WaCky corpora. The method consists
in comparing document hits for a sample of mid-
frequency words in Google and in our corpus before
and after duplicate removal. The method assumes
that Google does indeed apply duplicate removal to
some extent, though less drastically than we have.
Cf. table 2 for some example figures.
From this document hit comparison, two scaling
factors are extracted. The scaling ratio tells us how
much smaller our corpus is compared to the Google
index for Norwegian (including duplicates and non-
running-text). The duplicate ratio gives us an idea
of how much duplicated material was found in our
archive.
Since we do not know exactly how much dupli-
cate detection Google performs, we will multiply
the duplicate ratio by a weight of 0.1, 0.25 and 0.5
(these weights, in turn, assume that Google discards
10 times less, 4 times less and half what our dupli-
cate removal has done ? the latter hypothesis is used
by Kilgarriff 2007).
? Scaling ratio (average):
Google frq. / noWaC raw frq. = 24.9
? Duplicate ratio (average):
noWaC raw frq. / dedup. frq. = 7.8
We can then multiply the number of tokens in our
final cleaned corpus by the scaling ratio and by the
duplicate ratio (weighted) in order to obtain a rough
estimate of how much Norwegian text is contained
in the Google index. We can also estimate howmuch
of this amount is present in noWaC. Cf. table 3.
Using exactly the same procedure as Kilgarrif
(2007) leads us to conclude that noWaC should
contain over 15% of the Bokm?l text indexed by
Google. A much more restrictive estimate gives us
about 3%. More precise estimates are extremely
difficult to make, and these results should be taken
only as rough approximations. In any case, noWaC
certainly is a reasonably representative web-corpus
containing between 3% and 15% of all the currently
indexed online Bokm?l (Kilgarriff reports an esti-
mate of 3% for German and 7% for Italian in the
WaCky corpora).
5
deWaC itWaC ukWaC noWaC
N. of seed pairs 1,653 1,000 2,000 1,000
N. of seed URLs 8,626 5,231 6,528 6,891
Raw crawl size 398GB 379GB 351GB 550GB
Size after document size filter 20GB 19GB 19GB 22GB
N. of docs after document size filter 4.86M 4.43M 5.69M 11.4M
Size after near-duplicate filter 13GB 10GB 12GB 5GB
N. of docs after near-duplicate filter 1.75M 1.87M 2.69M 1.17M
N. of docs after lang-ID ? ? ? 0.85M
N. of tokens 1.27 Bn 1.58 Bn 1.91 Bn 0.69 Bn
N. of types 9.3M 3.6M 3.8M 6.0M
Table 1: Figure comparison of noWaC and the published WaCky corpora (German, Italian and British English data
from Baroni et al 2009)
Word Google frq. noWaC raw frq. noWaC dedup. frq.
bilavgifter 33700 1637 314
mekanikk 82900 3266 661
musikkpris 16700 570 171
Table 2: Sample of Google and noWaC document frequencies before and after duplicate removal.
noWaC Scaling ratio Dup. ratio (weight) Google estimate % in noWaC
0.78 (0.10) 21.8 bn 3.15%
0.69 bn 24.9 1.97 (0.25) 8.7 bn 7.89%
3.94 (0.50) 4.3 bn 15.79%
Table 3: Estimating the size of the Bokm?l Norwegian internet as indexed by Google in three different settings (method
from Kilgarriff 2007)
4 Concluding remarks
Building large web-corpora for languages with a
relatively small internet presence and with a lim-
ited speaker population presents problems and chal-
lenges that have not been found in previous work.
In particular, the amount of data that can be col-
lected with similar efforts is considerably smaller. In
our experience, following as closely as possible the
WaCky corpora methodology yielded a corpus that
is roughly between one half and one third the size of
the published comparable Italian, German and En-
glish corpora.
In any case, the experience has been very success-
ful so far, and the first version of the noWaC cor-
pus is about the same size than the largest currently
available corpus of Norwegian (i.e. Norske Avisko-
rpus, 700 million tokens), and it has been created in
just a minimal fraction of the time it took to build it.
Furthermore, the scaling experiments showed that
noWaC is a very representative web-corpus contain-
ing a significant portion of all the online content in
Bokm?l Norwegian, in spite of our extremely drastic
cleaning and filtering strategies.
There is clearly a great margin for improvement
in almost every processing step we applied in this
work. And there is clearly a lot to be done in or-
der to qualitatively assess the created corpus. In the
future, we intend to pursue this activity by carrying
out an even greater crawling job in order to obtain a
larger corpus, possibly containing over 1 billion to-
kens. Moreover, we shall reproduce this corpus cre-
ation process with the remaining two largest written
languages of Norway, Nynorsk and North Sami. All
of these resources will soon be publicly and freely
available both for the general public and for the re-
search community.
6
Acknowledgements
Building noWaC has been possible thanks to NO-
TUR advanced user support and assistance from the
Research Computing Services group (Vitenskapelig
Databehandling) at USIT, University of Oslo. Many
thanks are due to Eros Zanchetta (U. of Bologna),
Adriano Ferraresi (U. of Bologna) and Marco Ba-
roni (U. of Trento) and two anonymous reviewers
for their helpful comments and help.
References
M. Baroni and S. Bernardini. 2004. Bootcat: Bootstrap-
ping corpora and terms from the web. In Proceedings
of LREC 2004, pages 1313?1316, Lisbon. ELDA.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226, 09.
David Crystal. 2001. Language and the Internet. Cam-
bridge University Press, Cambridge.
A. Ferraresi, E. Zanchetta, M. Baroni, and S. Bernardini.
2008. Introducing and evaluating ukWaC, a very large
web-derived corpus of English. In Proceedings of the
WAC4 Workshop at LREC 2008.
R. Ghani, R. Jones, and D. Mladenic. 2001. Mining the
web to create minority language corpora. In Proceed-
ings of the tenth international conference on Informa-
tion and knowledge management, pages 279?286.
Johannessen J.B. N?klestad A. Hagen, K. 2000. A
constraint-based tagger for norwegian. Odense Work-
ing Papers in Language and Communication, 19(I).
Knut Hofland. 2000. A self-expanding corpus based on
newspapers on the web. In Proceedings of the Sec-
ond International Language Resources and Evaluation
Conference, Paris. European Language Resources As-
sociation.
Adam Kilgarriff and Marco Baroni, editors. 2006. Pro-
ceedings of the 2nd International Workshop on theWeb
as Corpus (EACL 2006 SIGWACWorkshop). Associa-
tion for Computational Linguistics, East Stroudsburg,
PA.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the special issue on the web as corpus.
Computational Linguistics, 29(3):333?348.
A. Kilgarriff. 2007. Googleology is bad science. Com-
putational Linguistics, 33(1):147?151.
S. Sharoff. 2005. Open-source corpora: Using the net to
fish for linguistic data. International Journal of Cor-
pus Linguistics, (11):435?462.
7
