Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 61?69,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
NE Tagging for Urdu based on Bootstrap POS Learning 
 
 
Smruthi Mukund Rohini K. Srihari 
Dept. of Computer Science and Engineering Dept. of Computer Science and Engineering 
University at Buffalo, SUNY University at Buffalo, SUNY 
Amherst, NY, USA Amherst, NY, USA 
smukund@buffalo.edu rohini@cedar.buffalo.edu 
 
 
 
 
 
 
Abstract 
Part of Speech (POS) tagging and Named Ent-
ity (NE) tagging have become important com-
ponents of effective text analysis. In this 
paper, we propose a bootstrapped model that 
involves four levels of text processing for Ur-
du. We show that increasing the training data 
for POS learning by applying bootstrapping 
techniques improves NE tagging results. Our 
model overcomes the limitation imposed by 
the availability of limited ground truth data 
required for training a learning model. Both 
our POS tagging and NE tagging models are 
based on the Conditional Random Field 
(CRF) learning approach. To further enhance 
the performance, grammar rules and lexicon 
lookups are applied on the final output to cor-
rect any spurious tag assignments. We also 
propose a model for word boundary segmen-
tation where a bigram HMM model is trained 
for character transitions among all positions in 
each word. The generated words are further 
processed using a probabilistic language mod-
el. All models use a hybrid approach that 
combines statistical models with hand crafted 
grammar rules. 
1 Introduction 
The work here is motivated by a desire to under-
stand human sentiment and social behavior through 
analysis of verbal communication. Newspapers 
reflect the collective sentiments and emotions of 
the people and in turn the society to which they 
cater to. Not only do they portray an event that has 
taken place as is, but they also reveal details about 
the intensity of fear, imagination, happiness and 
other emotions that people express in relation to 
that event.  Newspaper write ups, when analyzed 
over these factors - emotions, reactions and beha-
vior - can give a broader perspective on the culture, 
beliefs and the extent to which the people in the 
region are tolerant towards other religions. Our 
final goal is to automate this kind of behavioral 
analysis on newspaper articles for the Urdu lan-
guage. Annotated corpus that tag six basic human 
emotions, ?happy?, ?fear?, ?sad?, ?surprise?, ?an-
ger? and ?disgust?, based on the code book devel-
oped using the MPQA standards as guideline, is 
currently being developed.  Articles from two lead-
ing Urdu newswires, BBC Urdu1 and Jung Daily2 
form our corpus.  
In order to achieve our goal, it was required to 
generate the basic tools needed for efficient text 
analysis. This includes NE tagging and its precur-
sor, POS tagging. However, Urdu, despite being 
spoken by over 100 million people, (Gordon, 
2005) is still a less privileged language when it 
comes to the availability of resources on the inter-
net. Developing tools for a language with limited 
resources is a challenge, but necessary, as the vo-
lume of Urdu text on the internet is rising. Huda 
(2001) shows that Urdu has now gained impor-
tance on the web, making it the right time to tackle 
these issues. 
It is useful to first examine some basic proper-
ties of Urdu and how they affect the cascade of 
NLP steps in text analysis. Urdu has the nastaleeq 
and nasq style of writing that is similar to Arabic 
                                                           
1 http://www.bbc.co.uk/urdu/ 
2 http://www.jang.net/urdu/ 
61
and flows from right to left (Ahmad et al, 2001). It 
also adopts some of its vocabulary from Arabic. 
However, the grammar and semantics of the lan-
guage is similar to Hindi and this makes it very 
different from Arabic. For effective text analysis, a 
thorough syntactic and semantic understanding of 
the language is required. Detailed grammatical 
analysis provided by Platts (1909) and Schmidt 
(1999) can be used for this purpose. The first step 
in the information retrieval pipeline is tokeniza-
tion. Unlike English, where the word delimiter is 
mostly a space, Urdu is more complex. There are 
space insertion as well as space deletion problems. 
This makes tokenization a difficult task. The word 
segmentation model that we propose here com-
bines the statistical approach that considers bigram 
transition of characters based on their positions in a 
word and morphological rules with lexicon loo-
kups. 
 POS tagging comes next in the NLP text analy-
sis pipeline. The accuracy of the tagging model 
varies, depending on the tagsets used and the do-
main of the ground truth data. There are two main 
tagsets designed for Urdu, the CRULP tagset3 and 
the U1-tagset (Hardie 2003). The U1-tagset, re-
leased as a part of EMILLE4 corpus, is based on 
the EAGLES standards (Leech and Wilson 1999). 
We decided to use the standards proposed by 
CRULP for the following reasons. 
 
1. The tagset, though not as detailed as the 
one proposed in U1-tagset, covers all the 
basic requirements needed to achieve our 
final goal. 
2. The tagged corpus provided by CRULP is 
newswire material, similar to our final 
corpus. 
 
A person, when asked to identify an NE tagged 
word in a sentence would typically try to first find 
the word associated with a proper noun or a noun, 
and then assign a suitable NE tag based on the con-
text. A similar approach is used in our model, 
where the learning happens on the data that is POS 
tagged as well as NE tagged. Features are learnt 
from the POS tags as well as the NE tags. The final 
output of our complete model returns the POS tags 
                                                           
3 
http://www.crulp.org/Downloads/ling_resources/parallelcorpu
s/Urdu POS Tagset.pdf 
4 http://www.emille.lancs.ac.uk/ 
and NE tags associated with each word. Since we 
have limited data for training both the POS as well 
as the NE models, we propose a technique called 
bootstrapping that helps in maximizing the learn-
ing for efficient tagging. 
The remainder of the paper is organized as fol-
lows. Section 2 discusses the resources assimilated 
for the work followed by tokenization and word 
segmentation in Section 3. Section 4 gives a de-
tailed explanation of our model starting with a 
brief introduction of the learning approach used. 
Rules used for POS tagging and NE tagging are 
mentioned in subsections of Section 4. Section 5 
presents the results and Section 6 concludes the 
paper. In each section, wherever relevant, previous 
work and drawbacks are presented. 
2 Resources  
Based on the style of writing for Urdu, different 
encoding standards have been proposed. Urdu 
Zabta Takthi - the national standard code page for 
Urdu and Unicode - international standard for mul-
tilingual characters are the two proposed and wide-
ly used encoding standards. BBC Urdu and Jung 
Daily are both encoded with Unicode standards 
and are good sources of data. The availability of 
online resources for Urdu is not as extensive as 
other Asian languages like Chinese and Hindi. 
However, Hussain (2008) has done a good job in 
assimilating most of the resources available on the 
internet. The lexicon provided as a part of the 
EMILLE (2003) data set for Urdu has about 
200,000 words. CRL5 has released a lexicon of 
8000 words as a part of their Urdu data collection. 
They also provide an NE tagged data set mostly 
used for morphological analysis. The lexicon in-
cludes POS information as well. CRULP6 has also 
provided a lexicon of 149,466 words that contains 
places, organizations and names of people. As part 
of the Urdu morphological analyzer provided by 
Humayoun (2007), a lexicon of about 4,500 unique 
words is made available. There are a few Urdu-
English dictionaries available online and the first 
online dictionary, compiled by Siddiqi (2008), 
provides about 24,000 words with their meanings 
in English.  
Getting all the resources into one single compi-
lation is a challenge. These resources were brought 
                                                           
5 http://crl.nmsu.edu/Resources/lang_res/urdu.html 
6 http://www.crulp.org/software/ling_resources/wordlist.htm 
62
together and suitably compiled into a format that 
can be easily processed by Semantex (Srihari, 
2008), a text extraction platform provided by Janya 
Inc7. Lists of places, organizations and names of 
famous personalities in Pakistan were also com-
piled using the Urdu-Wikipedia8 and NationalMas-
ter9. A list of most common names in Pakistan was 
composed by retrieving data from the various 
name databases available on the internet.   
The word segmentation model uses the Urdu 
corpus released by CRULP as the training data. 
This dataset is well segmented. POS tagging model 
uses data provided by CRULP and NE tagging 
model uses data provided by CRL. 
3 Word Segmentation and Tokenization  
Urdu is a language that has both the space inser-
tion and space deletion problems. The Urdu word 
segmentation problem as mentioned by Durrani 
(2007) is triggered by its orthographic rules and 
confusions about the definition of a word. Durrani 
summarizes effectively, all the problems associated 
with Urdu word segmentation. Of all the different 
techniques explored to achieve this objective, tra-
ditional techniques like longest and maximum 
matching depend mostly on the availability of a 
lexicon that holds all the morphological forms of a 
word. Such a lexicon is difficult to obtain. It is 
shown by Theeramunkong et al, (2001), that for a 
Thai segmentation system, the efficiency drops 
considerably (from 97% to 82%) making this ap-
proach highly lexicon dependent.  
Statistical based techniques have applied proba-
bilistic models to solve the problem of word seg-
mentation. Bigram and trigram models are most 
commonly employed. Using feature based tech-
niques for POS tagging is also very common. 
These techniques overcome the limitations of sta-
tistical models by considering the context around 
the word for specific words and collocations. There 
are other models that generate segments by consi-
dering word level collation as well as syllable level 
collocation.  
However, for a language like Urdu, a model that 
is purely statistical will fail to yield good segmen-
tation results. A mixed model that considers the 
morphological as well as semantic features of the 
                                                           
7 http://www.janyainc.com/ 
8 http://ur.wikipedia.com/wiki/ 
9 http://www.nationmaster.com/index.php 
language facilitates better performance as shown 
by Durrani (2007) where the word segmentation 
model uses a lexicon for proper nouns and a statis-
tical model that trains over the n-gram probability 
of morphemes. Maximum matching technique is 
used to generate word boundaries of the ortho-
graphic words that are formed and these are later 
verified using the POS information. The segments 
thus generated are ranked and the best ones are 
accepted. Statistical models that consider character 
based, syllable based and word based probabilities 
have shown to perform reasonably well. The Thai 
segmentation problem was solved by Pornprasert-
kul (1994) using the character based approach. In 
our model, we use a combination of character 
based statistical approach and grammar rules with 
lexicon lookups to generate word boundaries. 
Urdu segmentation problem can be looked at as 
an issue of inserting spaces between characters. All 
letters in Urdu, with a few exceptions, have three 
forms - initial, medial and final. (We do not con-
sider the detached form for word formation). 
Words are written by joining the letters together 
and based on the position of the letter in the word, 
suitable forms are applied. This property of word 
formation is the crux of our model. The bigram 
probability of occurrences of each of these charac-
ters, based on their positions, is obtained by train-
ing over a properly segmented training set. For 
unknown characters, unknown character models 
for all the three position of occurrences are also 
trained. The probability of word occurrence is 
noted. Along with this, a lexicon rich enough to 
hold all possible common words is maintained. 
However, this lexicon does not contain proper 
nouns. A new incoming sentence that is not seg-
mented correctly is taken and suitable word boun-
daries are generated by using a combination of 
morphological rules, lexicon lookups, bigram word 
probabilities and bigram HMM character model. 
The following probabilities are estimated and max-
imized at character level using the Viterbi algo-
rithm. The following are the calculated 
probabilities:  
 
(i) )|( )(1)( initialkmedialk chchP ? - is the prob-
bility of character k being in medial 
form given character k-1 is in initial 
form. 
63
(ii) )|( )(1)( initialkfinalk chchP ? - is the proba-
bility of character k being in final form 
given character k-1 is in initial form. 
(iii) )|( )(1)( medialkfinalk chchP ?  - is the proba-
bility of character k being in final form 
given character k-1 is in medial form. 
(iv) )|( )(1)( medialkmedialk chchP ? - is the proba-
bility of character k being in medial 
form given character k-1 is in medial 
form. 
(v) )|( )(1)( finalkinitialk chchP ? - is the proba-
bility of character k being in initial 
form given character k-1 is in final 
form. 
 
Each word thus formed successfully is then veri-
fied for morphological correctness. If the word is 
not valid morphologically, then the window is 
moved back over 3 characters and at every step the 
validity of occurrence of the word is noted. Simi-
larly, the window is moved 3 characters ahead and 
the validity of the word is verified. All words 
formed successfully are taken and further 
processed using a language model that considers 
the bigram occurrence for each word. The un-
known word probability is considered here as well. 
The word with maximum probability is taken as 
valid in the given context.  
Let >< 321 www  be the word formed by the 
moving window. Then, the word selected, ws, is 
given by 
 
(vi) 
??
??
?
??
??
?
=
)(|)(
)(|)(
)(|)(
max
3
2
1
prev
prev
prev
s
wPwP
wPwP
wPwP
w  
where wprev  is the previous word. 
 
It is also noted that the number of times a transi-
tion happens from a syllable set with consonants to 
a syllable set with vowels, in a word, is no longer 
than four in most cases as noted below. This factor 
is also considered for terminating the Viterbi algo-
rithm for each word.  
 
 Ir | aad | ah - three transitions 
 
Some of the morphological rules considered 
while deciding the word boundaries are given be-
low. Word boundary is formed when  
1. The word ends with ''?? - un Gunna 
2. The character transitions over to digits 
3. Punctuations marks are encountered ('-' is 
also included) 
4. No two 'ye' - choti ye come back to back 
5. No characters occur in detached form un-
less they are initials or abbreviations fol-
lowed by a period 
6. If current character is 'alif' and the pre-
vious character is 'ee' - bari ye then the 
word boundary occurs after 'alif' 
Some of the drawbacks seen in this model are 
mainly on account of improper identification of 
proper nouns. If a proper noun is not well seg-
mented, the error propagates through the sentence 
and typically the next two or three words fail to get 
segmented correctly. Also, in Urdu, some words 
can be written in more than one ways. This mostly 
depends on the diacritics and ambiguity between 
bari and choti 'ye'. The training data as well as the 
test data were not normalized before training. The 
model shows a precision of 83%. We realized that 
the efficiency of this model can be improved if 
phoneme level transitions were taken into consid-
eration. Training has to be increased over more 
proper nouns and a lexicon for proper nouns loo-
kup has to be maintained. Diacritics that are typi-
cally used for beautification should be removed. 
Words across the documents need to be normalized 
to one accepted format to assure uniqueness.  This 
involves considerable amount of work and hence, 
in order to prevent the propagation of error into the 
NLP text analysis pipeline, we decided to test our 
subsequent models using pre-segmented data, in-
dependent of our word segmentation model. 
4 Learning Approaches  
A Conditional Random Field (CRF), is an undi-
rected graphical model used for sequential learn-
ing. The tasks of POS tagging and NE tagging are 
both sequential learning tasks and hence this learn-
ing approach is a reasonable choice. What follows 
is a brief outline about CRF. Interested readers are 
referred to Lafferty et al, (2001), for more infor-
mation on CRF.  
4.1 Conditional Random Fields (CRF) 
64
A linear chain CRF defines a single log-linear 
probabilistic distribution over the possible tag se-
quences y for a sentence x 
??
= =
?=
T
t
K
k
tttkk xyytfxZ
xyp
1 1
1 ),,,(exp)(
1)|( ?  
where  fk(t, yt, yt-1, xt) is typically a binary function 
indicating the presence of feature k, ?k is the weight 
of the feature, and Z(x) is a normalization function. 
? ??
= =
?=
y
T
t
K
k
tttkk xyytfxZ
1 1
1 ),,,(exp)( ?  
This modeling allows us to define features on 
states (the POS/NE tags) and edges (pairs of adja-
cent POS/NE tags) combined with observations 
(eg. words and POS tags for NE estimation). The 
weights of the features are determined such that 
they maximize the conditional log-likelihood of the 
training data:  
( )? == i ii xypL 1 )()( )|(log)( ?? .  
For the actual implementation, CRF++10, an 
open source tool that uses the CRF learning algo-
rithm is used. The L-BFGS algorithm11 is used for 
optimization. 
4.2 %E Tagging using POS information 
POS tagging is a precursor for all text analysis 
tasks. Assigning POS tags to words without any 
ambiguity depends on contextual information and 
extracting this information is a challenge. For a 
language like English, several techniques have 
been proposed that can be broadly classified into 
statistical, rule based and hybrid approaches (Ek-
bal, 2007). The general consensus is that ap-
proaches like MEMM and HMM, that work well 
for Hindi, would work well for Urdu as well, since 
Urdu is grammatically similar to Hindi (Platts, 
1909).  However, the linguistic and morphological 
rules used in the post processing steps differ from 
Hindi because of Urdu?s borrowed vocabulary and 
                                                           
10 http://crfpp.sourceforge.net/ 
11 http://www.mcs.anl.gov/index.php 
style of writing from Arabic. Also, the requirement 
for such models to work well is the availability of 
large training data. 
Building NE recognizers for languages like Ur-
du is difficult as there are no concepts like capitali-
zation of characters. Also, most names of people 
have specific meanings associated with them and 
can easily be found in a dictionary with different 
associated meanings. Various learning approaches 
have been proposed for this task, HMM based 
learning approach (Bikel et al, 1999), Maximum 
Entropy Approach (Borthwick, 1999) and CRF 
approach (McCallum, 2003) are the most popular. 
Ashish et al, (2009) show an SVM based approach 
also works well for such tasks. To overcome the 
problem of limited data availability, we present a 
method to increase the amount of training data that 
is available, by using a technique called bootstrap-
ping. 
We do not have a training corpus that is manual-
ly tagged for both POS and NE. Our training data 
consists of two different datasets. The dataset used 
for POS tagging is provided by CRULP and is 
tagged using their tagset. The dataset used for NE 
tagging is provided by CRL as a part of their Urdu 
resource package. The CRL tagset consists of 
LOCATION, PERSON, ORGANIZATION, DATE 
and TIME tags. We use only the first three tags in 
this work. 
Our aim is to achieve effective POS tagging and 
NE tagging by maximizing the use of the available 
training data. The CRULP dataset (which we call 
datasetPOS) is a corpus of 150,000 words that are 
only POS tagged and the CRL dataset (which we 
call datasetNE) is a corpus of 50,000 words that are 
only NE tagged. First, we trained a CRF model on 
datasetNE that uses only the NE information to per-
form NE recognition. This one stage model was 
not effective due to the sparseness of the NE tags 
in the dataset. The model requires more data while 
training. The obvious and frequently tried ap-
proach (Thamar, 2004) is to use the POS informa-
tion.  
Figure 1 shows a two stage model that uses POS 
information to perform NE tagging. The first stage 
POSA performs POS tagging by using a CRF 
trained model to assign POS tags to each word in a 
sentence of datasetNE. The second stage NEA per-
forms NE tagging by using another CRF trained 
model that uses both the POS information as well 
65
as the NE information, to perform effective NE 
tagging. 
 
 
Figure 1. Two stage model for NE tagging using POS 
information 
 
However, although the accuracy of NE tagging 
improved over the one stage model, there was 
scope for further improvement. It is obvious that 
all the NE tagged words should have the proper 
noun (NNP) POS tag associated. But, when POS 
tags were generated for the NE tagged ground truth 
data in datasetNE, most of the words were either 
tagged as adjectives (JJ) or common nouns (NN).  
Most tags that come after case markers (CM) were 
adjectives (JJ) in the training data. Very few ac-
counted for proper nouns after case markers. This 
adversely affected the NE tagger output. It was 
also noticed that the POS tagger tagged most of the 
proper nouns (NNP) as common nouns (NN) be-
cause of the sparseness of the proper noun tag in 
the POS ground truth data set datasetPOS. This ob-
servation made us look to bootstrapping techniques 
for effective learning.  
We propose a four stage model as shown in Fig-
ure 2, for NE tagging. Three of the stages are 
trained using the CRF learning approach and one 
stage uses a rule based approach.  All four stages 
are trained using unigram features on tags and 
words and bigram features on tags. The POS 
tagged dataset, datasetPOS, consists of words and 
associated POS tags and the NE tagged dataset, 
datasetNE, consists of words and associated NE 
tags. We divide both datasets into training and test-
ing partitions. datasetPOS is divided into trainsetPOS 
and testsetPOS and datasetNE is divided into train-
setNE and testsetNE. 
 
 
Figure 2. Four stage model for NE tagging using POS 
information with bootstrapping 
 
In the model shown in Figure 2, POSA stage is a 
CRF based stage that is trained using trainsetPOS. 
Once trained, the POSA stage takes as input a sen-
tence and generates the associated POS tag for 
each word in that sentence.  
In order to increase the NNP tag associations to 
improve NE tagging, we generate POS tags for the 
NE training data in trainsetNE using the POSA 
stage. The POS tags generated at the POSA stage 
are called POSint. The POScorrection stage takes as 
input trainsetNE along with its associated POS tags, 
POSint. At this stage, correction rules - that change 
the POS tags of NE associated words to proper 
noun (NNP), assign Case Markers (CM) before 
and after the NE tags and verify proper tagging of 
Cardinals (CD) - are applied. The corrected POS 
tags are called POScorrected. A consolidated POS 
training set consisting of entries from both train-
setPOS and trainsetNE (with POScorrected generated as 
output from the POScorrection stage) is used to train 
the CRF based POSB stage. This stage is the final 
POS tagging stage. Test data consisting of sen-
tences (words) from testsetNE is sent as input to 
stage POSB and the output generated at stage POSB 
is the POS tag associated with each input word of a 
sentence. The NEB stage is a CRF based NE tagger 
that is trained on a dataset consisting of word and 
associated NE tags from trainsetNE and associated 
POS tags from POScorrected. This stage learns from 
the POS information and the NE information pro-
vided in the training data. Once trained, the NEB 
stage takes as input words from testsetNE and asso-
ciated POS tags (obtained at stage POSB) and ge-
nerates NE tags. 
The domain we are interested in is newswire 
material, and these articles are written in the ?jour-
66
nalistic? or ?news writing? style12. The articles are 
objective and follow a Subject-Object-Verb struc-
ture. Related information is usually presented with-
in close sentence proximity. This makes it possible 
to hand-craft grammar rules for the discovery of 
NE tags with fine granularity. The final POS 
tagged and NE tagged data generated as outputs at 
stage POSB and stage NEB respectively of the four 
stage model, are processed using rules and lexicon 
lookups to further improve the overall tagging ac-
curacy of the model. Rules used are mostly domain 
specific. The rules were applied to the model using 
Semantex. 
4.3 Rules for POS Tagging 
1. Our model tags all the Question Words 
(QW) like ????? - kya as pronoun (PR). All 
such occurrences are assigned QW tag. 
2. If the word is ????? ? kya and the previous 
tag is an adjective (JJ) and the next tag is a 
phrase marker (PM) then assign a light 
verb tag (VBL) else assign a verb (VB) tag 
to the word. 
3. It was observed that there were spurious 
instances of proper nouns getting tagged as 
nouns. In order to correct this error, if a 
word ends with any of the characters 
shown below, and the word was tagged as 
a noun, then the tag on the word was 
changed to a proper noun.  
?%?, ??? ,???, ?()?, ?*+?, 
?,-?, ????, ?  0*- ?, ???? 
4. All valid cardinals were tagged as nouns or 
proper nouns by the model. This was re-
solved by looking for a digit in the string.  
4.4 Rules for %E Tagging 
1. Words like ?????? (court), ??????? (bu-
reau), ????? (army) etc. are looked up. If 
there are any nouns or proper nouns above 
these within a window of two, then the tag 
on this word is ORGANIZATION. 
2. Words like ??????? (organization), ?????? 
are marked ORGANIZATION if the pre-
vious word is a proper noun. 
3. Lexicon look up for names of places is per-
formed and the POS tag of the next word 
that is found is checked. If this tag is a 
                                                           
12 http://en.wikipedia.org/wiki/News_writing 
Case Marker (CM) with a feminine gend-
er, like ???? (main) or ?????, then the 
word is marked with a LOCATION tag. 
4. If a proper noun that is selected ends with 
a suffix ?pur?, ?bad, ?dad? and has the 
same constraint as mentioned in rule 3, 
then the LOCATION tag is assigned to it 
as well. 
5 Results 
The NE tagging performance, for both the two 
stage model and the four stage model, are eva-
luated using Precision (P), Recall (R) and F-Score 
(FS) metrics, the equations for which are given 
below. 
(vii) NEs  taggedof No.
NEs taggedcorrectly  of No. P =  
(viii) setin test  NEs of no. Total
NEs  taggedof No.R =  
(ix) 
 PR
RPFS +=
2  
 
We performed a 10 fold cross validation test to 
determine the performance of the model. The data-
set is divided into 10 subsets of approximately 
equal size. One subset is withheld for testing and 
the remaining 9 subsets are used for training. This 
process is repeated for all 10 subsets and an aver-
age result is computed. The 10 fold validation test 
for NE tagging was performed for both the two 
stage as well as the four stage models. 
 
Set P R FS P R FS
1 48.09 73.25 58.06 60.54 78.7 68.44
2 38.94 72.42 50.65 60.29 80.46 68.93
3 56.98 74.38 64.53 60.54 79.74 68.83
4 38.44 78.05 51.51 60.54 80.79 69.21
5 32.29 75.91 45.31 60.79 80.34 69.21
6 44.82 88.02 59.4 59.31 79.93 68.09
7 45.75 69.75 55.26 61.04 81.73 69.89
8 43.52 71.5 54.11 60.05 80.36 68.74
9 44.64 81.97 57.8 59.93 81.09 68.92
10 44.17 78.18 56.45 60.67 79.22 68.72
Avg 43.764 76.343 55.308 60.37 80.236 68.898
Four Stage ModelTwo Stage Model
 
Table 1. NE tagging results for the two stage and four 
stage models 
 
It can be seen from Table 1 that the four stage 
model outperforms the two stage model with the 
67
average F-Score being 55.31% for the two stage 
model and 68.89% for the four stage model. 
Table 2 shows the POS tagging results for stages 
POSA and POSB. The POSB stage performs margi-
nally better than the POSA stage. 
 
Set P Set P
1 84.38 1 83.97
2 89.32 2 89.84
3 88.09 3 88.48
4 89.45 4 89.66
5 89.66 5 89.76
6 90.57 6 90.63
7 81.1 7 89.24
8 89.47 8 89.5
9 89 9 89.12
10 89.12 10 89.25
Avg 88.016 Avg 88.945
POSB ResultsPOSA Results
 
Table 2. POS tagging results for the two stage (POSA) 
and four stage (POSB) models 
 
Although for POS tagging, the improvement is 
not very significant between the two models, tags 
like light verbs (VBLI), auxiliary verbs (AUXA 
and AUXT), adjectives (JJ), demonstratives (DM) 
and nouns (NN, NNC, NNCM, NNCR) get tagged 
with higher accuracy in the four stage model as 
shown in Table 3. This improvement becomes evi-
dent in the NE test set. Unfortunately, since this 
data has no associated POS tagged ground truth, 
the results cannot be quantified. The trainsetPOS 
training data had very few instances of proper 
nouns (NNP) occurring after case markers (CM) 
and so most of the proper nouns were getting 
tagged as either adjectives (JJ) or common nouns 
(NN). After providing more training data to stage 
POSB, the model could effectively learn proper 
nouns. Spurious tagging of adjectives (JJ) and 
common nouns (NN) reduced while more proper 
nouns (NNP, NNPC) were tagged accurately and 
this allowed the NE stage to apply its learning effi-
ciently to the NE test set thereby improving the NE 
tagging results.  
The two stage model tagged 238 NE tagged 
words as proper nouns out of 403 NE words. The 
four stage model tagged 340 NE tagged words as 
proper nouns out of 403 NE words. The four stage 
model shows an improvement of 25.3% over the 
two stage model. The results reported for NE and 
POS tagging models are without considering rules 
or lexicon lookups. 
 
Tag FS Tag FS
AUXA 0.801 AUXA 0.816
AUXT 0.872 AUXT 0.898
DM 0.48 DM 0.521
JJ 0.751 JJ 0.765
NN 0.85 NN 0.858
NNC 0.537 NNC 0.549
NNCM 0.909 NNCM 0.923
NNCR 0.496 NNCR 0.51
RB 0.785 RB 0.834
VBLI 0.67 VBLI 0.693
VBT 0.553 VBT 0.586
POSA Output POSB Output
 
Table 3. POS tagging results for stages POSA and POSB 
 
In order to further improve the POS tagged re-
sults and NE tagged results, the rules mentioned in 
sections 4.3 and 4.4 and lexicon lookups were ap-
plied. Table 4 shows the result for NE tagging with 
an overall F-Score of 74.67% 
 
Tag P R FS
LOCATION 0.78 0.793 0.786
ORGANIZATION 0.775 0.731 0.752
PERSON 0.894 0.595 0.714
NEA Output
 
Table 4. NE tagging results after applying rules for test 
results in Table 1 
6. Conclusion and Future Work  
This work was undertaken as a precursor to 
achieve our final objective as discussed in Section 
1. The basic idea here is to increase the size of the 
available training data, by using bootstrapping, so 
as to maximize learning for NE tagging. The pro-
posed four stage model shows an F-Score of 68.9% 
for NE tagging which is much higher than that ob-
tained by the simple two stage model. 
A lot of avenues remain to be explored to fur-
ther improve the performance of the model. One 
approach would be to use the bootstrapping tech-
nique for NE data as well. However, the rules re-
quired can be complicated. More hand crafted rules 
and detailed lexicon lookups can result in better 
NE tagging. We have also noticed certain ambigui-
ties in tagging PERSON and LOCATION. Rules 
that resolve this ambiguity can be explored. 
68
References  
Raymond G. Gordon Jr. (ed.). 2005. Ethnologue: Lan-
guages of the World, Fifteenth edition. Dallas, TX.: 
SIL International 
Kashif Huda. 2001. An Overview of Urdu on the Web. 
 Annual of Urdu Studies Vol 20. 
Zaheer Ahmad, Jehanzeb Khan Orakzai, Inam Shamsh-
er, Awais Adnan. 2007. Urdu astaleeq Character 
Recognition. Proceedings of World Academy of 
Science, Engineering and Technology. Volume 26, 
ISSN 2070-3740. 
John T. Platts. 1967. A grammar of the Hindustani or 
Urdu language.  Munshiram Manoharlal Delhi. 
R. L. Schmidt. 1999. Urdu: an essential grammar. 
London: Routledge. 
Sarmad Hussain. 2008. Resources for Urdu Language 
Processing. The 6th Workshop on Asian Language 
Resources. 
P. Baker, A. Hardie, T. McEnery, B.D. Jayaram. 2003. 
Corpus Data for South Asian Language Processing. 
Proceedings of the 10th Annual Workshop for South 
Asian Language Processing, EACL. 
M. Humayoun, H. Hammarstrm, A. Ranta. 2007. Urdu 
Morphology, Orthography and Lexicon Extraction. 
CAASL-2: The Second Workshop on Computational 
Approaches to Arabic Script-based Languages, LSA 
2007 Linguistic Institute, Stanford University. 
Waseem Siddiqi, Shahab Alam. 2008. Online Urdu-
English and English-Urdu dictionary. 
N. Durrani. 2007. Typology of Word and Automatic 
Word Segmentation in Urdu Text Corpus. National 
University of Computer and Emerging Sciences, La-
hore, Pakistan. 
T. Theeramunkong, S. Usanavasin. 2001. on-
Dictionary Based Thai Word Segmentation Using 
decision trees. In proceedings of the First Interna-
tional Conference on Human Language Technology 
Research, San Diego, California, USA. 
A. Pornprasertkul. 1994. Thai Syntactic Analysis. Ph.D 
Thesis, Asian Institute of Technology. 
Ismat Javed. 1981.  ??  ????? ????. Taraqqi Urdu Bureau, 
New Delhi. 
Abdul M. Haq. 1987.  ????  ??? ? ???. Amjuman-e-
Taraqqi Urdu (Hindi). 
Hassan Sajjad. 2007. Statistical Part of Speech Tagger 
for Urdu. National University of Computer and 
Emerging Sciences, Lahore, Pakistan. 
John D. Lafferty, Andrew McCallum, Fernando C.N. 
Pereira. 2001. Conditional Random Fields: Probabi-
listicModels for Segmenting and Labeling Sequence 
Data. Proceedings of the Eighteenth International 
Conference on Machine Learning, pp. 282-289. 
John Chen. 2006. How to use Sequence Tagger. Seman-
tex Documentation, Janya Inc. 
Bikel, D.M., Schwartz, R.L., Weischedel, R.M.1999. 
An Algorithm that Learns What?s in a ame. Ma-
chine Learning 34(1-3), pp. 211?231. 
Borthwick, A. 1999. Maximum Entropy Approach to 
amed Entity Recognition. PhD thesis, New York 
University. 
McCallum, A., Li, W. 2003. Early results for amed 
Entity Recognition with Conditional Random Fields, 
Feature Induction and Web-enhanced Lexicons. In 
Proceedings of CoNLL. 
A. Hardie. 2003. Developing a tagset for automated 
part-of-speech tagging in Urdu. Department of Lin-
guistics and Modern English Language, University 
of Lancaster. 
Leech, G and Wilson, A. 1999. Standards for tagsets. 
Edited version of EAGLES Recommendations for the 
Morphosyntactic Annotation of Corpora. In van Hal-
teren, H (ed.) Syntactic wordclass tagging. Dor-
drecht: Kluwer Academic Publishers. 
Awaghad Ashish Krishnarao, Himanshu Gahlot, Amit 
Srinet and D. S. Kushwaha.  2009. A Comparative 
Study of amed Entity Recognition for Hindi Using 
Sequential Learning Algorithms. In IEEE Interna-
tional Advance Computing Conference (IACC '09), 
Thapar University, India. March 6-7. 
Thamar Solario. 2004. Improvement of amed Entity 
Tagging by Machine Learning, Technical Report 
CCC-04-004, Coordinacin de Ciencias Computatio-
nales. 
Ekbal, A. and Bandyopadhyay, S. 2007. A Hidden 
Markov Model Based amed Entity Recognition Sys-
tem: Bengali and Hindi as Case Studies. Springer 
LNCS, Vol. 4815, pp. 545. 
R. K. Srihari, W. Li, C. Niu and T. Cornell,"InfoXtract: 
A Customizable Intermediate Level Information Ex-
traction Engine," Journal of atural Language En-
gineering, Cambridge U. Press, 14(1), 2008, pp..33-
69. 
 
69
