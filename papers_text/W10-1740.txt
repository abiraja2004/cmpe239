Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 276?281,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Adaptive Model Weighting and Transductive Regression for
Predicting Best System Combinations
Ergun Bic?ici
Koc? University
34450 Sariyer, Istanbul, Turkey
ebicici@ku.edu.tr
S. Serdar Kozat
Koc? University
34450 Sariyer, Istanbul, Turkey
skozat@ku.edu.tr
Abstract
We analyze adaptive model weight-
ing techniques for reranking using in-
stance scores obtained by L1 regular-
ized transductive regression. Compet-
itive statistical machine translation is
an on-line learning technique for se-
quential translation tasks where we
try to select the best among com-
peting statistical machine translators.
The competitive predictor assigns a
probability per model weighted by
the sequential performance. We de-
fine additive, multiplicative, and loss-
based weight updates with exponential
loss functions for competitive statisti-
cal machine translation. Without any
pre-knowledge of the performance of
the translation models, we succeed in
achieving the performance of the best
model in all systems and surpass their
performance in most of the language
pairs we considered.
1 Introduction
When seen as independent instances, system
combination task can be solved with a sequen-
tial learning algorithm. Online learning algo-
rithms enable us to benefit from previous good
model choices to estimate the next best model.
We use transductive regression based machine
translation model to estimate the scores for
each sentence.
We analyze adaptive model weighting tech-
niques for system combination when the com-
peting translators are SMT models. We use
separate model weights weighted by the se-
quential performance. We use additive, mul-
tiplicative, or loss based weight updates to
update model weights. Without any pre-
knowledge of the performance of the transla-
tion models, we are able to achieve the per-
formance of the best model in all systems and
we can surpass its performance as well as the
regression based machine translation?s perfor-
mance.
The next section reviews the transductive
regression approach for machine translation,
which we use to obtain instance scores. In sec-
tion 3 we present competitive statistical ma-
chine translation model for solving sequential
translation tasks with competing translation
models. Section 4 presents our results and ex-
periments and the last section gives a sum-
mary of our contributions.
2 Transductive Regression Based
Machine Translation
Transduction uses test instances, which can
sometimes be accessible at training time, to
learn specific models tailored towards the test
set. Transduction has computational advan-
tages since we are not using the full train-
ing set and a smaller set of constraints exist
to satisfy. Transductive regression based ma-
chine translation (TRegMT) aims to reduce
the computational burden of the regression ap-
proach by reducing the dimensionality of the
training set and the feature set and also im-
prove the translation quality by using trans-
duction.
Regression Based Machine Translation:
Let n training instances be represented as
(x1,y1), . . . , (xn,yn) ? X
??Y ?, where (xi,yi)
corresponds to a pair of source and target lan-
guage token sequences. Our goal is to find
a mapping f : X? ? Y ? that can convert a
given set of source tokens to a set of target to-
kens that share the same meaning in the target
language.
276
We use feature mappers ?X : X? ?
FX = RNX and ?Y : Y ? ? FY =
RNY to represent the training set. Then,
MX ? RNX?n and MY ? RNY ?n such that
MX = [?X(x1), . . . ,?X(xn)] and MY =
[?Y (y1), . . . ,?Y (yn)]. The ridge regression
solution using L2 regularization is found as:
HL2 = arg min
H?RNY ?NX
?MY ?HMX ?
2
F +??H?
2
F . (1)
Two main challenges of the regression based
machine translation (RegMT) approach are
learning the regression function, g : X? ?
FY , and solving the pre-image problem, which,
given the features of the estimated target
string sequence, g(x) = ?Y (y?), attempts to
find y ? Y ?: f(x) = arg miny?Y ? ||g(x) ?
?Y (y)||2. Pre-image calculation involves a
search over possible translations minimizing
the cost function:
f(x) = arg min
y?Y ?
??Y (y)?H?X(x)?
2 . (2)
We use n-spectrum weighted word feature
mappers (Taylor and Cristianini, 2004) which
consider all word sequences up to order n.
L1 Regularized Regression for Learning:
HL2 is not a sparse solution as most of the co-
efficients remain non-zero. L1 norm behaves
both as a feature selection technique and a
method for reducing coefficient values.
HL1 = arg min
H?RNY ?NX
?MY ?HMX ?
2
F +??H?1 .(3)
Equation 3 presents the lasso (least absolute
shrinkage and selection operator) (Tibshirani,
1996) solution where the regularization term
is defined as ?H?1=
?
i,j |Hi,j |. We use for-
ward stagewise regression (FSR) (Hastie et
al., 2006) and quadratic programming (QP) to
find HL1 . The details of the TRegMT model
can be read in a separate submission to the
translation task (Bicici and Yuret, 2010).
3 Competitive Statistical Machine
Translation
We develop the Competitive Statistical Ma-
chine Translation (CSMT) framework for se-
quential translation tasks when the compet-
ing models are statistical machine translators.
CSMT uses the output of different translation
models to achieve a translation performance
that surpasses the translation performance of
all of the component models or achieves the
performance of the best.
CSMT uses online learning to update the
weights used for estimating the best perform-
ing translation model. Competitive predictor
assigns a weight per model estimated by their
sequential performance. At each step, m com-
ponent translation models are executed in par-
allel over the input source sentence sequence
and the loss lp[n] of model p at observation
n is calculated by comparing the desired data
y[n] with the output of model p, y?p[n]. CSMT
model selects a model based on the weights
and the performance of the selected model as
well as the remaining models to adaptively up-
date the weights given for each model. This
corresponds to learning in full information set-
ting where we have access to the loss for each
action (Blum and Mansour, 2007). CSMT
learning involves two main steps: estimation
and weight update:
y?c[n] = E(w[n],x[n]), (estimation)
lp[n] = y[n]? y?p[n], (instance loss)
Lp[n] =
?n
i=1 lp[i]
2, (cumulative loss)
w[n+ 1] = U(w[n], y?c[n],L[n]), (update)
(4)
where w[n] = (w1[n], . . . , wm[n]) for m mod-
els, Lp is the cumulative squared loss of model
p, L[n] stores cumulative and instance losses,
and y?c[n] is the competitive model estimated
for instance n. The learning problem is finding
an adaptive w that minimizes the cumulative
squared error with appropriate estimation and
update methods.
Related Work: Multistage adaptive filter-
ing (Kozat and Singer, 2002) combines the
output of multiple adaptive filters to outper-
form the best among them where the first
stage executes models in parallel and the sec-
ond stage updates parameters using the per-
formance of the combined prediction, y?c[n].
Macherey and Och (2007) investigate different
approaches for system combination including
candidate selection that maximize a weighted
combination of BLEU scores among different
system outputs. Their system uses a fixed
weight vector trained on the development set
277
to be multiplied with instance BLEU scores.
3.1 Estimating the Best Performing
Translation Model
We use additive, multiplicative, or loss based
updates to estimate model weights. We
measure instance loss with trLoss(y[i], y?p[i]),
which is a function that returns the transla-
tion performance of the output translation of
model p with respect to the reference transla-
tion at instance i. 1-BLEU (Papineni et al,
2001) is one such function with outputs in the
range [0, 1]. Cumulative squared loss of the
p-th translation model is defined as:
Lp[n] =
n?
i=1
trLoss(y[i], y?p[i])
2. (5)
We use exponentially re-weighted prediction to
estimate model performances, which uses ex-
ponentially re-weighted losses based on the
outputs of the m different translation models.
We define the additive exponential weight
update as follows:
wp[n+ 1] =
wp[n] + e?? lp[n]
m?
k=1
(
wk[n] + e
?? lk[n]
)
, (6)
where ? > 0 is the learning rate and the de-
nominator is used for normalization. The up-
date amount, e?? lp[n] is 1 when lp[n] = 0 and it
approaches zero with increasing instance loss.
Perceptrons, gradient descent, and Widrow-
Huff learning have additive weight updates.
We define the multiplicative exponential
weight update as follows:
wp[n+ 1] = wp[n]?
e?? lp[n]
2
m?
k=1
wk[n] e
?? lk[n]2
, (7)
where we use the squared instance loss. Equa-
tion 7 is similar to the update of Weighted Ma-
jority Algorithm (Littlestone and Warmuth,
1992) where the weights of the models that
make a mistake are multiplied by a fixed ?
such that 0 ? ? < 1.
We use Bayesian Information Criterion
(BIC) as a loss based re-weighting technique.
Assuming that instance losses are normally
distributed with variance ?2, BIC score is ob-
tained as (Hastie et al, 2009):
BICp[n] =
Lp[n]
?2
+ dp log(n), (8)
where ?2 is estimated by the average of model
sample variances of squared instance loss and
dp is the number of parameters used in model p
which we assume to be the same for all models;
therefore we can discard the second term. The
model with the minimum BIC value becomes
the one with the highest posterior probability
where the posterior probability of model p can
be estimated as (Hastie et al, 2009):
wp[n+ 1] =
e?
1
2BICp[n]
m?
k=1
e?
1
2BICk[n]
. (9)
The posterior probabilities become model
weights and we basically forget about the pre-
vious weights, whose information is presum-
ably contained in the cumulative loss, Lp. We
define multiplicative re-weighting with BIC
scores as follows:
wp[n+ 1] = wp[n]?
e?
1
2BICp
m?
k=1
wk[n] e
? 12BICk
. (10)
Model selection: We use stochastic or de-
terministic selection to choose the competitive
model for each instance. Deterministic choice
randomly selects among the maximum scor-
ing models with minimum translation length
whereas stochastic choice draws model p with
probability proportional to wp[n]. Random-
ization with the stochastic model selection
decreases expected mistake bounds in the
weighted majority algorithm (Littlestone and
Warmuth, 1992; Blum, 1996).
Auer et al (2002) show that optimal fixed
learning rate for the weighted majority algo-
rithm is found as ?[n] =
?
m/L?[n] where
L?[n] = min1?i?m Li[n], which requires prior
knowledge of the cumulative losses. We use
? =
?
m/(0.05n) for constant ?.
4 Experiments and Discussion
We perform experiments on the system com-
bination task for the English-German (en-
de), German-English (de-en), English-French
278
(en-fr), English-Spanish (en-es), and English-
Czech (en-cz ) language pairs using the trans-
lation outputs for all the competing systems
provided in WMT10. We experiment in a sim-
ulated online learning setting where only the
scores obtained from the TRegMT system are
used during both tuning and testing. We do
not use reference translations in measuring in-
stance performance in this simulated setting
for the results we obtain be in line with sys-
tem combination challenge?s goals.
4.1 Datasets
We use the training set provided in WMT10 to
index and select transductive instances from.
The challenge split the test set for the transla-
tion task of 2489 sentences into a tuning set of
455 sentences and a test set with the remain-
ing 2034 sentences. Translation outputs for
each system is given in a separate file and the
number of system outputs per translation pair
varies. We have tokenized and lowercased each
of the system outputs and combined these in
a single N -best file per language pair. We use
BLEU (Papineni et al, 2001) and NIST (Dod-
dington, 2002) evaluation metrics for measur-
ing the performance of translations automati-
cally.
4.2 Reranking Scores
The problem we are solving is online learn-
ing with prior information, which comes from
the comparative BLEU scores, LM scores, and
TRegMT scores at each step n. The scoring
functions are explained below:
1. TRegMT: Transductive regression based
machine translation scores as found by
Equation 2. We use the TRegMT scores
obtained by the FSR model.
2. CBLEU: Comparative BLEU scores we
obtain by measuring the average BLEU
performance of each translation relative
to the other systems? translations in the
N -best list.
3. LM: We calculate 5-gram language model
scores for each translation using the lan-
guage model trained over the target cor-
pus provided in the translation task.
To make things simpler, we use a single prior
TRegMT system score linearly combining the
three scores mentioned with weights learned
on the tuning set. The overall TRegMT sys-
tem score for instance n, model i is referred as
TRegScorei[n].
Since we do not have access to the refer-
ence translations nor to the translation model
scores each system obtained for each sentence,
we estimate translation model performance by
measuring the average BLEU performance of
each translation relative to other translations
in the N -best list. Thus, each possible transla-
tion in the N -best list is BLEU scored against
other translations and the average of these
scores is selected as the CBLEU score for the
sentence. Sentence level BLEU score calcula-
tion avoids singularities in n-gram precisions
by taking the maximum of the match count
and 12|si| for |si| denoting the length of the
source sentence si as used in (Macherey and
Och, 2007).
4.3 Adaptive Model Weighting
We initialize model weights to 1/m for all
models, which are updated after each instance
according to the losses based on the TRegMT
model. Table 1 presents the performance
of the algorithms on the en-de development
set. We have measured their performances
with stochastic (stoc.) or deterministic (det.)
model selection when using only the weights or
mixture weights obtained when instance scores
are also considered. Mixture weights are ob-
tained as: wi[n] = wi[n] TRegScorei[n], for
instance n, model i.
Baseline performance obtained with random
selection has .1407 BLEU and 4.9832 NIST
scores. TRegMT model obtains a performance
of .1661 BLEU and 5.3283 NIST with rerank-
ing. The best model performance among the
12 en-de translation models has .1644 BLEU
and 5.2647 NIST scores. Therefore, by using
TRegMT score, we are able to achieve better
scores.
Not all of the settings are meaningful. For
instance, stochastic model selection is used for
algorithms having multiplicative weight up-
dates. This is reflected in the Table 1 by low
performance on the additive and BIC models.
Similarly, using mixture weights may not re-
sult in better scores for algorithms with multi-
plicative updates, which resulted in decreased
279
Additive Multiplicative BIC BIC Weighting
Setting BLEU NIST BLEU NIST BLEU NIST BLEU NIST
Stoc., W .1419 5.0016 ?.003 .1528 5.1710 ?.001 .1442 5.0468 .1568 ?.001 5.2052 ?.005
Stoc., M .1415 5.0001 .1525 5.1601 ?.001 .1459 5.0619 ?.004 .1566 ?.001 5.2030 ?.006
Det., W .1644 5.3208 .1638 5.2571 .1638 5.2542 .1646 5.2535
Det., M .1643 5.3173 .1536 5.1756 .1530 5.1871 .1507 5.1973
Table 1: Performances of the algorithms on the development set over 100 repetitions. W:
Weights, M: Mixture.
performance in Table 1. Decreased perfor-
mance with BIC hints that we may use other
techniques for mixture weights.
Table 2 presents reranking results on all of
the language pairs we considered with the ran-
dom, TRegMT, and CSMT models. Random
model score lists the random model perfor-
mance selected among the competing trans-
lations randomly and it can be used as a
baseline. Best model score lists the perfor-
mance of the best model performance. CSMT
models are named with the weighting model
used (Add for additive, Mul for multiplicative,
BICW for BIC weighting), model selection
technique (S for stochastic, D for determinis-
tic), and mixtures model (W for using only
weights, M for using mixture weights) with
hyphens in between. Our challenge submis-
sion is given in the last row of Table 2 where
we used multiplicative exponential weight up-
dates, deterministic model selection, and only
the weights during model selection. For the
challenge results, we initialized the weights to
the weights obtained in the development set.
We have presented scores that are better
than or close to the best model in bold. We
observe that the additive model performs the
best by achieving the performance of the best
competing translation model and performing
better than the best in most of the language
pairs. For the en-de language pair, addi-
tive model score achieves even better than the
TRegMT model, which is used for evaluating
instance scores.
5 Contributions
We have analyzed adaptive model weighting
techniques for system combination when the
competing translators are statistical machine
translation models. We defined additive, mul-
tiplicative, and loss-based weight updates with
exponential loss functions for the competitive
statistical machine translation framework.
Competitive SMT via adaptive weighting of
various translators is shown to be a powerful
technique for sequential translation tasks. We
have demonstrated its use in the system com-
bination task by using the instance scores ob-
tained by the TRegMT model. Without any
pre-knowledge of the performance of the trans-
lation models, we have been able to achieve the
performance of the best model in all systems
and we are able to surpass its performance as
well as TRegMT?s performance with the addi-
tive model.
Acknowledgments
The research reported here was supported in
part by the Scientific and Technological Re-
search Council of Turkey (TUBITAK). The
first author would like to thank Deniz Yuret
for helpful discussions and for guidance and
support during the term of this research.
References
Auer, Cesa-Bianchi, and Gentile. 2002. Adaptive
and self-confident on-line learning algorithms.
JCSS: Journal of Computer and System Sci-
ences, 64.
Ergun Bicici and Deniz Yuret. 2010. L1 regular-
ized regression for reranking and system combi-
nation in machine translation. In Proceedings of
the ACL 2010 Joint Fifth Workshop on Statis-
tical Machine Translation and Metrics MATR,
Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
Avrim Blum and Yishay Mansour. 2007. Learn-
ing, regret minimization and equilibria. In
Noam Nisan, Tim Roughgarden, Eva Tar-
dos, and Vijay V. Vazirani, editors, Algorith-
mic Game Theory (Cambridge University Press,
2007).
Avrim Blum. 1996. On-line algorithms in machine
learning. In In Proceedings of the Workshop on
On-Line Algorithms, Dagstuhl, pages 306?325.
Springer.
280
en-de de-en en-fr en-es en-cz
Model BLEU NIST BLEU NIST BLEU NIST BLEU NIST BLEU NIST
Random .1490 5.6555 .2088 6.4886 .2415 6.8948 .2648 7.2563 .1283 4.9238
Best model .1658 5.9610 .2408 6.9861 .2864 7.5272 .3047 7.7559 .1576 5.4480
TRegMT .1689 5.9638 .2357 6.9254 .2947 7.7107 .3049 7.8156 .1657 5.5632
Add-D-W .1697 5.9821 .2354 6.9175 .2948 7.7094 .3043 7.8093 .1642 5.5463
Add-D-M .1698 5.9824 .2353 6.9152 .2949 7.7103 .3044 7.8091 .1642 5.5461
Mul-S-W .1574 5.7564 .2161 6.5950 .2805 7.4599 .2961 .7.6870 .1572 5.4394
Mul-D-W .1618 5.8912 .2408 6.9854 .2847 7.5085 .2785 7.4133 .1612 5.5119
BIC-D-W .1614 5.8852 .2408 6.9853 .2842 7.5022 .2785 7.4132 .1623 5.5236
BIC-D-M .1580 5.7614 .2141 6.5597 .2791 7.4309 .2876 7.5138 .1577 5.4488
BICW-S-W .1621 5.8795 .2274 6.8142 .2802 7.4873 .2892 7.5569 .1565 5.4126
BICW-S-M .1618 5.8730 .2196 6.6493 .2806 7.4948 .2849 7.4845 .1561 5.4099
BICW-D-W .1648 5.9298 .2355 6.9112 .2807 7.4648 .2785 7.4134 .1534 5.3458
Challenge .1567 5.73 .2394 6.9627 .2758 7.4333 .3047 7.7559 .1641 5.5435
Table 2: CSMT results where bold corresponds to scores better than or close to the best model.
Underlined scores are better than both the TregMT model and the best model.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Human Language Tech-
nology Research, pages 138?145.
Trevor Hastie, Jonathan Taylor, Robert Tibshi-
rani, and Guenther Walther. 2006. Forward
stagewise regression and the monotone lasso.
Electronic Journal of Statistics, 1.
Trevor Hastie, Robert Tibshirani, and Jerome
Friedman. 2009. The Elements of Statistical
Learning: Data Mining, Inference and Predic-
tion. Springer-Verlag, 2nd edition.
S.S. Kozat and A.C. Singer. 2002. Further re-
sults in multistage adaptive filtering. ICASSP,
2:1329?1332.
Nick Littlestone and Manfred K. Warmuth. 1992.
The Weighted Majority Algorithm. Technical
Report UCSC-CRL-91-28, University of Califor-
nia, Santa Cruz, Jack Baskin School of Engi-
neering, October 26,.
Wolfgang Macherey and Franz J. Och. 2007. An
empirical study on computing consensus transla-
tions from multiple machine translation systems.
In EMNLP-CoNLL, pages 986?995.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. Bleu: a method for au-
tomatic evaluation of machine translation. In
ACL, pages 311?318. ACL.
J. Shawe Taylor and N. Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge
University Press.
Robert J. Tibshirani. 1996. Regression shrinkage
and selection via the lasso. Journal of the Royal
Statistical Society, Series B, 58(1):267?288.
281
