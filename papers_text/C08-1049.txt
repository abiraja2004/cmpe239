Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 385?392
Manchester, August 2008
Word Lattice Reranking for Chinese Word Segmentation and
Part-of-Speech Tagging
Wenbin Jiang ? ? Haitao Mi ? ? Qun Liu ?
?Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
?Graduate University of Chinese Academy of Sciences
Beijing, 100049, China
{jiangwenbin,htmi,liuqun}@ict.ac.cn
Abstract
In this paper, we describe a new rerank-
ing strategy named word lattice reranking,
for the task of joint Chinese word segmen-
tation and part-of-speech (POS) tagging.
As a derivation of the forest reranking
for parsing (Huang, 2008), this strategy
reranks on the pruned word lattice, which
potentially contains much more candidates
while using less storage, compared with
the traditional n-best list reranking. With a
perceptron classifier trained with local fea-
tures as the baseline, word lattice rerank-
ing performs reranking with non-local fea-
tures that can?t be easily incorporated into
the perceptron baseline. Experimental re-
sults show that, this strategy achieves im-
provement on both segmentation and POS
tagging, above the perceptron baseline and
the n-best list reranking.
1 Introduction
Recent work for Chinese word segmentation and
POS tagging pays much attention to discriminative
methods, such as Maximum Entropy Model (ME)
(Ratnaparkhi and Adwait, 1996), Conditional Ran-
dom Fields (CRFs) (Lafferty et al, 2001), percep-
tron training algorithm (Collins, 2002), etc. Com-
pared to generative ones such as Hidden Markov
Model (HMM) (Rabiner, 1989; Fine et al, 1998),
discriminative models have the advantage of flexi-
bility in representing features, and usually obtains
almost perfect accuracy in two tasks.
Originated by Xue and Shen (2003), the typ-
ical approach of discriminative models conducts
c
? 2008. Licensed to the Coling 2008 Organizing Com-
mittee for publication in Coling 2008 and for re-publishing in
any form or medium.
segmentation in a classification style, by assign-
ing each character a positional tag indicating its
relative position in the word. If we extend these
positional tags to include POS information, seg-
mentation and POS tagging can be performed by a
single pass under a unify classification framework
(Ng and Low, 2004). In the rest of the paper, we
call this operation mode Joint S&T. Experiments
of Ng and Low (2004) shown that, compared with
performing segmentation and POS tagging one at
a time, Joint S&T can achieve higher accuracy not
only on segmentation but also on POS tagging.
Besides the usual local features such as the
character-based ones (Xue and Shen, 2003; Ng
and Low, 2004), many non-local features related
to POSs or words can also be employed to improve
performance. However, as such features are gener-
ated dynamically during the decoding procedure,
incorporating these features directly into the clas-
sifier results in problems. First, the classifier?s fea-
ture space will grow much rapidly, which is apt to
overfit on training corpus. Second, the variance of
non-local features caused by the model evolution
during the training procedure will hurt the param-
eter tuning. Last but not the lest, since the cur-
rent predication relies on the results of prior predi-
cations, exact inference by dynamic programming
can?t be obtained, and then we have to maintain a
n-best candidate list at each considering position,
which also evokes the potential risk of depress-
ing the parameter tuning procedure. As a result,
many theoretically useful features such as higher-
order word- or POS- grams can not be utilized ef-
ficiently.
A widely used approach of using non-local
features is the well-known reranking technique,
which has been proved effective in many NLP
tasks, for instance, syntactic parsing and machine
385
v0
v
1
v
2
v
3
v
4
v
5
v
6
v
7
C
1
:e
C
2
:? C
3
:U
C
4
:/ C
5
:? C
6
:?
C
7
:Y
NN
VV
NN
M
NN
NN
NN
NN
VV
NN
NN
Figure 1: Pruned word lattice as directed graph. The character sequence we choose is ?e-?-U-/-
?-?-Y?. For clarity, we represent each subsequence-POS pair as a single edge, while ignore the
corresponding scores of the edges.
translation (Collins, 2000; Huang, 2008), etc. Es-
pecially, Huang (2008) reranked the packed for-
est, which contains exponentially many parses.
Inspired by his work, we propose word lattice
reranking, a strategy that reranks the pruned word
lattice outputted by a baseline classifier, rather than
only a n-best list. Word lattice, a directed graph as
shown in Figure 1, is a packed structure that can
represent many possibilities of segmentation and
POS tagging. Our experiments on the Penn Chi-
nese Treebank 5.0 show that, reranking on word
lattice gains obvious improvement over the base-
line classifier and the reranking on n-best list.
Compared against the baseline, we obtain an error
reduction of 11.9% on segmentation, and 16.3%
on Joint S&T.
2 Word Lattice
Formally, a word lattice L is a directed graph
?V,E?, where V is the node set, and E is the
edge set. Suppose the word lattice is for sentence
C
1:n
= C
1
..C
n
, node v
i
? V (i = 1..n ? 1) de-
notes the position between C
i
and C
i+1
, while v
0
before C
1
is the source node, and v
n
after C
n
is
the sink node. An edge e ? E departs from v
b
and
arrives at v
e
(0 ? b < e ? n), it covers a subse-
quence of C
1:n
, which is recognized as a possible
word. Considering Joint S&T, we label each edge
a POS tag to represent a word-POS pair. A series
of adjoining edges forms a path, and a path con-
necting the source node and the sink node is called
diameter, which indicates a specific pattern of seg-
mentation and POS tagging. For a diameter d, |d|
denotes the length of d, which is the count of edges
contained in this diameter. In Figure 1, the path
p
?
= v?
0
v
3
? v?
3
v
5
? v?
5
v
7
is a diameter, and
|p
?
| is 3.
2.1 Oracle Diameter in Lattice
Given a sentence s, its reference r and pruned
word lattice L generated by the baseline classi-
fier, the oracle diameter d? of L is define as the
diameter most similar to r. With F-measure as the
scoring function, we can identify d? using the al-
gorithm depicted in Algorithm 1, which is adapted
to lexical analysis from the forest oracle computa-
tion of Huang (2008).
Before describe this algorithm in detail, we de-
pict the key point for finding the oracle diameter.
Given the system?s output y and the reference y?,
using |y| and |y?| to denote word counts of them
respectively, and |y ? y?| to denote matched word
count of |y| and |y?|, F-measure can be computed
by:
F (y, y
?
) =
2PR
P + R
=
2|y ? y
?
|
|y| + |y
?
|
(1)
Here, P = |y?y
?
|
|y|
is precision, and R = |y?y
?
|
|y
?
|
is recall. Notice that F (y, y?) isn?t a linear func-
tion, we need access the largest |y ? y?| for each
possible |y| in order to determine the diameter with
maximum F , or another word, we should know the
maximum matched word count for each possible
diameter length.
The algorithm shown in Algorithm 1 works in
a dynamic programming manner. A table node
T [i, j] is defined for sequence span [i, j], and it has
a structure S to remember the best |y
i:j
? y
?
i:j
| for
each |y
i:j
|, as well as the back pointer for this best
choice. The for-loop in line 2 ? 14 processes for
each node T [i, j] in a shorter-span-first order. Line
3? 7 initialize T [i, j] according to the reference r
and the word lattice?s edge set L ?E. If there exists
an edge e in L ?E covering the span [i, j], then we
386
Algorithm 1 Oracle Diameter, U la Huang (2008,
Sec. 4.1).
1: Input: sentence s, reference r and lattice L
2: for [i, j] ? [1, |s|] in topological order do
3: if ?e ? L ? E s.t. e spans from i to j then
4: if e ? label exists in r then
5: T [i, j] ? S[1]? 1
6: else
7: T [i, j] ? S[1]? 0
8: for k s.t. T [i, k ? 1] and T [k, j] defined do
9: for p s.t. T [i, k ? 1] ? S[p] defined do
10: for q s.t. T [k, j] ? S[q] defined do
11: n? T [i, k ? 1] ? S[p] + T [k, j] ? S[q]
12: if n > T [i, j] ? S[p + q] then
13: T [i, j] ? S[p + q]? n
14: T [i, j] ? S[p + q] ? bp? ?k, p, q?
15: t? argmax
t
2?T [1,|s|]?S[t]
t+|r|
16: d? ? Tr(T [1, |s|] ? S[t].bp)
17: Output: oracle diameter: d?
define T [i, j], otherwise we leave this node unde-
fined. In the first situation, we initialize this node?s
S structure according to whether the word-POS
pair of e is in the reference (line 4?7). Line 8?14
update T [i, j]?s S structure using the S structures
from all possible child-node pair, T [i, k ? 1] and
T [k, j]. Especially, line 9? 10 enumerate all com-
binations of p and q, where p and q each repre-
sent a kind of diameter length in T [i, k ? 1] and
T [k, j]. Line 12 ? 14 refreshes the structure S
of node T [i, j] when necessary, and meanwhile,
a back pointer ?k, p, q? is also recorded. When
the dynamic programming procedure ends, we se-
lect the diameter length t of the top node T [1, |s|],
which maximizes the F-measure formula in line
15, then we use function Tr to find the oracle di-
ameter d? by tracing the back pointer bp.
2.2 Generation of the Word Lattice
We can generate the pruned word lattice using the
baseline classifier, with a slight modification. The
classifier conducts decoding by considering each
character in a left-to-right fashion. At each consid-
ering position i, the classifier enumerates all can-
didate results for subsequence C
1:i
, by attaching
each current candidate word-POS pair p to the tail
of each candidate result at p?s prior position, as
the endmost of the new generated candidate. We
give each p a score, which is the highest, among
all C
1:i
?s candidates that have p as their endmost.
Then we select N word-POS pairs with the high-
est scores, and insert them to the lattice?s edge set.
This approach of selecting edges implies that, for
the lattice?s node set, we generate a node v
i
at each
position i. Because N is the limitation on the count
Algorithm 2 Lattice generation algorithm.
1: Input: character sequence C
1:n
2: E ? ?
3: for i? 1 .. n do
4: cands? ?
5: for l? 1 .. min(i, K) do
6: w ? C
i?l+1:i
7: for t ? POS do
8: p? ?w, t?
9: p ? score? Eval(p)
10: s? p ? score + Best[i? l]
11: Best[i]? max(s,Best[i])
12: insert ?s, p? into cands
13: sort cands according to s
14: E ? E ? cands[1..N ] ? p
15: Output: edge set of lattice: E
of edges that point to the node at position i, we call
this pruning strategy in-degree pruning. The gen-
eration algorithm is shown in Algorithm 2.
Line 3 ? 14 consider each character C
i
in se-
quence, cands is used to keep the edges closing at
position i. Line 5 enumerates the candidate words
ending with C
i
and no longer than K, where K
is 20 in our experiments. Line 5 enumerates all
POS tags for the current candidate word w, where
POS denotes the POS tag set. Function Eval in
line 9 returns the score for word-POS pair p from
the baseline classifier. The array Best preserve the
score for sequence C
1:i
?s best labelling results. Af-
ter all possible word-POS pairs (or edges) consid-
ered, line 13? 14 select the N edges we want, and
add them to edge set E.
Though this pruning strategy seems relative
rough ? simple pruning for edge set while no
pruning for node set, we still achieve a promising
improvement by reranking on such lattices. We be-
lieve more elaborate pruning strategy will results
in more valuable pruned lattice.
3 Reranking
A unified framework can be applied to describing
reranking for both n-best list and pruned word lat-
tices (Collins, 2000; Huang, 2008). Given the can-
didate set cand(s) for sentence s, the reranker se-
lects the best item y? from cand(s):
y? = argmax
y?cand(s)
w ? f(y) (2)
For reranking n-best list, cand(s) is simply the set
of n best results from the baseline classifier. While
for reranking word lattice, cand(s) is the set of
all diameters that are impliedly built in the lattice.
w ? f(y) is the dot product between a feature vec-
tor f and a weight vector w, its value is used to
387
Algorithm 3 Perceptron training for reranking
1: Input: Training examples{cand(s
i
), y
?
i
}
N
i=1
2: w? 0
3: for t? 1 .. T do
4: for i? 1 .. N do
5: y? ? argmax
y?cand(s
i
)
w ? f(y)
6: if y? 6= y?
i
then
7: w? w + f(y?
i
)? f(y?)
8: Output: Parameters: w
Non-local Template Comment
W
0
T
0
current word-POS pair
W
?1
word 1-gram before W
0
T
0
T
?1
POS 1-gram before W
0
T
0
T
?2
T
?1
POS 2-gram before W
0
T
0
T
?3
T
?2
T
?1
POS 3-gram before W
0
T
0
Table 1: Non-local feature templates used for
reranking
rerank cand(s). Following usual practice in pars-
ing, the first feature f
1
(y) is specified as the score
outputted by the baseline classifier, and its value
is a real number. The other features are non-local
ones such as word- and POS- n-grams extracted
from candidates in n-best list (for n-best rerank-
ing) or diameters (for word lattice reranking), and
they are 0 ? 1 valued.
3.1 Training of the Reranker
We adopt the perceptron algorithm (Collins, 2002)
to train the reranker. as shown in Algorithm 3. We
use a simple refinement strategy of ?averaged pa-
rameters? of Collins (2002) to alleviate overfitting
on the training corpus and obtain more stable per-
formance.
For every training example {cand(s
i
), y
?
i
}, y
?
i
denotes the best candidate in cand(s
i
). For n-
best reranking, the best candidate is easy to find,
whereas for word lattice reranking, we should use
the algorithm in Algorithm 1 to determine the or-
acle diameter, which represents the best candidate
result.
3.2 Non-local Feature Templates
The non-local feature templates we use to train the
reranker are listed in Table 1. Notice that all fea-
tures generated from these templates don?t contain
?future? words or POS tags, it means that we only
use current or history word- or POS- n-grams to
evaluate the current considering word-POS pair.
Although it is possible to use ?future? information
in n-best list reranking, it?s not the same when we
rerank the pruned word lattice. As we have to tra-
verse the lattice topologically, we face difficulty in
Algorithm 4 Cube pruning for non-local features.
1: function CUBE(L)
2: for v ? L ? V in topological order do
3: NBEST(v)
4: return D
v
sink
[1]
5: procedure NBEST(v)
6: heap? ?
7: for v? topologically before v do
8: ?? all edges from v? to v
9: p? ?D
v
?
,??
10: ?p,1??score? Eval(p,1)
11: PUSH(?p,1?, heap)
12: HEAPIFY(heap)
13: buf ? ?
14: while |heap| > 0 and |buf | < N do
15: item? POP-MAX(heap)
16: append item to buf
17: PUSHSUCC(item, heap)
18: sort buf to D
v
19: procedure PUSHSUCC(?p, j?, heap)
20: p is ?vec
1
,vec
2
?
21: for i? 1..2 do
22: j? ? j+ bi
23: if |vec
i
| ? j
?
i
then
24: ?p, j???score? Eval(p, j?)
25: PUSH(?p, j??, heap)
utilizing the information ahead of the current con-
sidering node.
3.3 Reranking by Cube Pruning
Because of the non-local features such as word-
and POS- n-grams, the reranking procedure is sim-
ilar to machine translation decoding with inter-
grated language models, and should maintain a
list of N best candidates at each node of the lat-
tice. To speed up the procedure of obtaining the
N best candidates, following Huang (2008, Sec.
3.3), we adapt the cube pruning method from ma-
chine translation (Chiang, 2007; Huang and Chi-
ang 2007) which is based on efficient k-best pars-
ing algorithms (Huang and Chiang, 2005).
As shown in Algorithm 4, cube pruning works
topologically in the pruned word lattice, and main-
tains a list of N best derivations at each node.
When deducing a new derivation by attaching a
current word-POS pair to the tail of a antecedent
derivation, a function Eval is used to compute the
new derivation?s score (line 10 and 24). We use
a max-heap heap to hold the candidates for the
next-best derivation. Line 7 ? 11 initialize heap
to the set of top derivations along each deducing
source, the vector pair ?D
v
head
,??.Here, ? de-
notes the vector of current word-POS pairs, while
D
v
head
denotes the vector of N best derivations
at ??s antecedent node. Then at each iteration,
388
Non-lexical-target Instances
C
n
(n = ?2..2) C
?2
=e, C
?1
=?, C
0
=U, C
1
=/, C
2
=?
C
n
C
n+1
(n = ?2..1) C
?2
C
?1
=e?, C
?1
C
0
=?U, C
0
C
1
=U/, C
1
C
2
=/?
C
?1
C
1
C
?1
C
1
=?/
Lexical-target Instances
C
0
C
n
(n = ?2..2) C
0
C
?2
=Ue, C
0
C
?1
=U?, C
0
C
0
=UU, C
0
C
1
=U/, C
0
C
2
=U?
C
0
C
n
C
n+1
(n = ?2..1) C
0
C
?2
C
?1
=Ue?, C
0
C
?1
C
0
=U?U, C
0
C
0
C
1
=UU/, C
0
C
1
C
2
=U/?
C
0
C
?1
C
1
C
0
C
?1
C
1
= U?/
Table 2: Feature templates and instances. Suppose we consider the third character ?U? in the sequence
?e?U/??.
we pop the best derivation from heap (line 15),
and push its successors into heap (line 17), until
we get N derivations or heap is empty. In line 22
of function PUSHSUCC, j is a vector composed of
two index numbers, indicating the two candidates?
indexes in the two vectors of the deducing source
p, where the two candidates are selected to deduce
a new derivation. j? is a increment vector, whose
ith dimension is 1, while others are 0. As non-
local features (word- and POS- n-grams) are used
by function Eval to compute derivation?s score,
the derivations extracted from heap may be out of
order. So we use a buffer buf to keep extracted
derivations (line 16), then sort buf and put its first
N items to D
v
(line 18).
4 Baseline Perceptron Classifier
4.1 Joint S&T as Classification
Following Jiang et al (2008), we describe segmen-
tation and Joint S&T as below:
For a given Chinese sentence appearing as a
character sequence:
C
1:n
= C
1
C
2
.. C
n
the goal of segmentation is splitting the sequence
into several subsequences:
C
1:e
1
C
e
1
+1:e
2
.. C
e
m?1
+1:e
m
While in Joint S&T, each of these subsequences is
labelled a POS tag:
C
1:e
1
/t
1
C
e
1
+1:e
2
/t
2
.. C
e
m?1
+1:e
m
/t
m
Where C
i
(i = 1..n) denotes a character, C
l:r
(l ?
r) denotes the subsequence ranging from C
l
to C
r
,
and t
i
(i = 1..m,m ? n) denotes the POS tag of
C
e
i?1
+1:e
i
.
If we label each character a positional tag in-
dicating its relative position in an expected subse-
quence, we can obtain the segmentation result ac-
cordingly. As described in Ng and Low (2004) and
Jiang et al (2008), we use s indicating a single-
character word, while b, m and e indicating the be-
gin, middle and end of a word respectively. With
these positional tags, the segmentation transforms
to a classification problem. For Joint S&T, we
expand positional tags by attaching POS to their
tails as postfix. As each tag now contains both
positional- and POS- information, Joint S&T can
also be resolved in a classification style frame-
work. It means that, a subsequence is a word with
POS t, only if the positional part of the tag se-
quence conforms to s or bm?e pattern, and each
element in the POS part equals to t. For example,
a tag sequence b NN m NN e NN represents a
three-character word with POS tag NN .
4.2 Feature Templates
The features we use to build the classifier are gen-
erated from the templates of Ng and Low (2004).
For convenience of comparing with other, they
didn?t adopt the ones containing external knowl-
edge, such as punctuation information. All their
templates are shown in Table 2. C denotes a char-
acter, while its subscript indicates its position rela-
tive to the current considering character(it has the
subscript 0).
The table?s upper column lists the templates that
immediately from Ng and Low (2004). they
named these templates non-lexical-target because
predications derived from them can predicate with-
out considering the current character C
0
. Tem-
plates called lexical-target in the column below are
introduced by Jiang et al (2008). They are gener-
ated by adding an additional field C
0
to each non-
lexical-target template, so they can carry out pred-
ication not only according to the context, but also
according to the current character itself.
Notice that features derived from the templates
in Table 2 are all local features, which means all
features are determined only by the training in-
stances, and they can be generated before the train-
ing procedure.
389
Algorithm 5 Perceptron training algorithm.
1: Input: Training examples (x
i
, y
i
)
2: ?? 0
3: for t? 1 .. T do
4: for i? 1 .. N do
5: z
i
? argmax
z?GEN(x
i
)
?(x
i
, z) ? ?
6: if z
i
6= y
i
then
7: ?? ? +?(x
i
, y
i
)??(x
i
, z
i
)
8: Output: Parameters: ?
4.3 Training of the Classifier
Collins (2002)?s perceptron training algorithm
were adopted again, to learn a discriminative clas-
sifier, mapping from inputs x ? X to outputs
y ? Y . Here x is a character sequence, and y is
the sequence of classification result of each char-
acter in x. For segmentation, the classification re-
sult is a positional tag, while for Joint S&T, it is
an extended tag with POS information. X denotes
the set of character sequence, while Y denotes the
corresponding set of tag sequence.
According to Collins (2002), the function
GEN(x) generates all candidate tag sequences for
the character sequence x , the representation ?
maps each training example (x, y) ? X ? Y to
a feature vector ?(x, y) ? Rd, and the parameter
vector ? ? Rd is the weight vector corresponding
to the expected perceptron model?s feature space.
For a given input character sequence x, the mission
of the classifier is to find the tag sequence F (x)
satisfying:
F (x) = argmax
y?GEN(x)
?(x, y) ? ? (3)
The inner product ?(x, y) ? ? is the score of the
result y given x, it represents how much plausibly
we can label character sequence x as tag sequence
y. The training algorithm is depicted in Algorithm
5. We also use the ?averaged parameters? strategy
to alleviate overfitting.
5 Experiments
Our experiments are conducted on the Penn Chi-
nese Treebank 5.0 (CTB 5.0). Following usual
practice of Chinese parsing, we choose chapters
1?260 (18074 sentences) as the training set, chap-
ters 301? 325 (350 sentences) as the development
set, and chapters 271 ? 300 (348 sentences) as
the final test set. We report the performance of
the baseline classifier, and then compare the per-
formance of the word lattice reranking against the
 0.9
 0.91
 0.92
 0.93
 0.94
 0.95
 0.96
 0  1  2  3  4  5  6  7  8  9  10
F-
me
as
ur
e
number of iterations
Perceptron Learning Curves
Segmentation
Joint ST
Figure 2: Baseline averaged perceptron learning
curves for segmentation and Joint S&T.
n-best reranking, based on this baseline classifier.
For each experiment, we give accuracies on seg-
mentation and Joint S&T. Analogous to the situa-
tion in parsing, the accuracy of Joint S&T means
that, a word-POS is recognized only if both the
positional- and POS- tags are correctly labelled for
each character in the word?s span.
5.1 Baseline Perceptron Classifier
The perceptron classifier are trained on the train-
ing set using features generated from the templates
in Table 2, and the development set is used to
determine the best parameter vector. Figure 2
shows the learning curves for segmentation and
Joint S&T on the development set. We choose
the averaged parameter vector after 7 iterations for
the final test, this parameter vector achieves an F-
measure of 0.973 on segmentation, and 0.925 on
Joint S&T. Although the accuracy on segmentation
is quite high, it is obviously lower on Joint S&T.
Experiments of Ng and Low (2004) on CTB 3.0
also shown the similar trend, where they obtained
F-measure 0.952 on segmentation, and 0.919 on
Joint S&T.
5.2 Preparation for Reranking
For n-best reranking, we can easily generate n best
results for every training instance, by a modifica-
tion for the baseline classifier to hold n best can-
didates at each considering point. For word lattice
reranking, we use the algorithm in Algorithm 2 to
generate the pruned word lattice. Given a training
instance s
i
, its n best result list or pruned word
lattice is used as a reranking instance cand(s
i
),
the best candidate result (of the n best list) or or-
acle diameter (of the pruned word lattice) is the
reranking target y?
i
. We find the best result of the
n best results simply by computing each result?s
390
F-measure, and we determine the oracle diame-
ter of the pruned word lattice using the algorithm
depicted in Algorithm 1. All pairs of cand(s
i
)
and y?
i
deduced from the baseline model?s training
instances comprise the training set for reranking.
The development set and test set for reranking are
obtained in the same way. For the reranking train-
ing set {cand(s
i
), y
?
i
}
N
i=1
, {y
?
i
}
N
i=1
is called oracle
set, and the F-measure of {y?
i
}
N
i=1
against the ref-
erence set is called oracle F-measure. We use the
oracle F-measure indicating the utmost improve-
ment that an reranking algorithm can achieve.
5.3 Results and Analysis
The flows of the n-best list reranking and the
pruned word lattice reranking are similar to the
training procedure for the baseline classifier. The
training set for reranking is used to tune the param-
eter vector of the reranker, while the development
set for reranking is used to determine the optimal
number of iterations for the reranker?s training pro-
cedure.
We compare the performance of the word lat-
tice reranking against the n-best list reranking. Ta-
ble 3 shows the experimental results. The up-
per four rows are the experimental results for n-
best list reranking, while the four rows below are
for word lattice reranking. In n-best list rerank-
ing, with list size 20, the oracle F-measure on
Joint S&T is 0.9455, and the reranked F-measure
is 0.9280. When list size grows up to 50, the oracle
F-measure on Joint S&T jumps to 0.9552, while
the reranked F-measure becomes 0.9302. How-
ever, when n grows to 100, it brings tiny improve-
ment over the situation of n = 50. In word lat-
tice reranking, there is a trend similar to that in
n-best reranking, the performance difference be-
tween in degree = 2 and in degree = 5 is ob-
vious, whereas the setting in degree = 10 does
not obtain a notable improvement over the perfor-
mance of in degree = 5. We also notice that even
with a relative small in degree limitation, such as
in degree = 5, the oracle F-measures for seg-
mentation and Joint S&T both reach a quite high
level. This indicates the pruned word lattice con-
tains much more possibilities of segmentation and
tagging, compared to n-best list.
With the setting in degree = 5, the oracle F-
measure on Joint S&T reaches 0.9774, and the
reranked F-measure climbs to 0.9336. It achieves
an error reduction of 16.3% on Joint S&T, and an
error reduction of 11.9% on segmentation, over the
n-best Ora Seg Tst Seg Ora S&T Tst S&T
20 0.9827 0.9749 0.9455 0.9280
50 0.9903 0.9754 0.9552 0.9302
100 0.9907 0.9755 0.9558 0.9305
Degree Ora Seg Rnk Seg Ora S&T Rnk S&T
2 0.9898 0.9753 0.9549 0.9296
5 0.9927 0.9774 0.9768 0.9336
10 0.9934 0.9774 0.9779 0.9337
Table 3: Performance of n-best list reranking and
word lattice reranking. n-best: the size of the n-
best list for n-best list reranking; Degree: the in de-
gree limitation for word lattice reranking; Ora Seg:
oracle F-measure on segmentation of n-best lists or
word lattices; Ora S&T: oracle F-measure on Joint
S&T of n-best lists or word lattices; Rnk Seg: F-
measure on segmentation of reranked result; Rnk
S&T: F-measure on Joint S&T of reranked result
baseline classifier. While for n-best reranking with
setting n = 50, the Joint S&T?s error reduction is
6.9% , and the segmentation?s error reduction is
8.9%. We can see that reranking on pruned word
lattice is a practical method for segmentation and
POS tagging. Even with a much small data rep-
resentation, it obtains obvious advantage over the
n-best list reranking.
Comparing between the baseline and the two
reranking techniques, We find the non-local infor-
mation such as word- or POS- grams do improve
accuracy of segmentation and POS tagging, and
we also find the reranking technique is effective to
utilize these kinds of information. As even a small
scale n-best list or pruned word lattice can achieve
a rather high oracle F-measure, reranking tech-
nique, especially the word lattice reranking would
be a promising refining strategy for segmentation
and POS tagging. This is based on this viewpoint:
On the one hand, compared with the initial input
character sequence, the pruned word lattice has a
quite smaller search space while with a high ora-
cle F-measure, which enables us to conduct more
precise reranking over this search space to find the
best result. On the other hand, as the structure of
the search space is approximately outlined by the
topological directed architecture of pruned word
lattice, we have a much wider choice for feature se-
lection, which means that we would be able to uti-
lize not only features topologically before the cur-
rent considering position, just like those depicted
in Table 2 in section 4, but also information topo-
logically after it, for example the next word W
1
or
the next POS tag T
1
. We believe the pruned word
391
lattice reranking technique will obtain higher im-
provement, if we develop more precise reranking
algorithm and more appropriate features.
6 Conclusion
This paper describes a reranking strategy called
word lattice reranking. As a derivation of the for-
est reranking of Huang (2008), it performs rerank-
ing on pruned word lattice, instead of on n-best
list. Using word- and POS- gram information, this
reranking technique achieves an error reduction of
16.3% on Joint S&T, and 11.9% on segmentation,
over the baseline classifier, and it also outperforms
reranking on n-best list. It confirms that word lat-
tice reranking can effectively use non-local infor-
mation to select the best candidate result, from a
relative small representation structure while with a
quite high oracle F-measure. However, our rerank-
ing implementation is relative coarse, and it must
have many chances for improvement. In future
work, we will develop more precise pruning al-
gorithm for word lattice generation, to further cut
down the search space while maintaining the ora-
cle F-measure. We will also investigate the feature
selection strategy under the word lattice architec-
ture, for effective use of non-local information.
Acknowledgement
This work was supported by National Natural Sci-
ence Foundation of China, Contracts 60736014
and 60573188, and 863 State Key Project No.
2006AA010108. We show our special thanks to
Liang Huang for his valuable suggestions.
References
Collins, Michael. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
17th International Conference on Machine Learn-
ing, pages 175?182.
Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Empirical Methods in Natural Language Pro-
cessing Conference, pages 1?8, Philadelphia, USA.
Fine, Shai, Yoram Singer, and Naftali Tishby. 1998.
The hierarchical hidden markov model: Analysis
and applications. In Machine Learning, pages 32?
41.
Huang, Liang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proceedings
of the 46th Annual Meeting of the Association for
Computational Linguistics.
Jiang, Wenbin, Liang Huang, Yajuan Lv, and Qun Liu.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the 23rd International Con-
ference on Machine Learning, pages 282?289, Mas-
sachusetts, USA.
Ng, Hwee Tou and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
the Empirical Methods in Natural Language Pro-
cessing Conference.
Rabiner, Lawrence. R. 1989. A tutorial on hidden
markov models and selected applications in speech
recognition. In Proceedings of IEEE, pages 257?
286.
Ratnaparkhi and Adwait. 1996. A maximum entropy
part-of-speech tagger. In Proceedings of the Empir-
ical Methods in Natural Language Processing Con-
ference.
Xue, Nianwen and Libin Shen. 2003. Chinese word
segmentation as lmr tagging. In Proceedings of
SIGHAN Workshop.
392
