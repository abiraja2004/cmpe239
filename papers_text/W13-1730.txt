Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 232?241,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Feature Engineering in the NLI Shared Task 2013:
Charles University Submission Report
Barbora Hladka?, Martin Holub and Vincent Kr??z?
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Prague, Czech Republic
{hladka, holub,kriz}@ufal.mff.cuni.cz
Abstract
Our goal is to predict the first language (L1)
of English essays?s authors with the help of
the TOEFL11 corpus where L1, prompts (top-
ics) and proficiency levels are provided. Thus
we approach this task as a classification task
employing machine learning methods. Out
of key concepts of machine learning, we fo-
cus on feature engineering. We design fea-
tures across all the L1 languages not making
use of knowledge of prompt and proficiency
level. During system development, we experi-
mented with various techniques for feature fil-
tering and combination optimized with respect
to the notion of mutual information and infor-
mation gain. We trained four different SVM
models and combined them through majority
voting achieving accuracy 72.5%.
1 Introduction
Learner corpora are collections of texts written by
second language (L2) learners, e.g. English as L2
? ICLE (Granger et al, 2009), Lang-8 (Tajiri et al,
2012), Cambridge Learner Corpus,1 German as L2
? FALKO (Reznicek et al, 2012), Czech as L2 ?
CzeSL (Hana et al, 2010). They are a valuable
resource for second language acquisition research,
identifying typical difficulties of learners of a cer-
tain proficiency level (e.g. low/medium/high) or
learners of a certain native language (L1 learners of
L2). Research on the learner corpora does not con-
centrate on text collections only. Studying the er-
rors in learner language is undertaken in the form
1http://www.cambridge.org/gb/elt
of error annotation like in the projects (Hana et al,
2012), (Boyd et al, 2012), (Rozovskaya and Roth,
2010), (Tetreault and Chodorow, 2008). Once the
errors and other relevant data are recognized in the
learner corpora, automatic procedures for e.g. error
correction, author profiling, native language identi-
fication etc. can be designed.
Our attention is focused on the task of automatic
Native Language Identification (NLI), namely with
English as L2.
In this report, we summarize the involment of the
Charles University team in the first shared task in
NLI co-located with the 8th Workshop on Innova-
tive Use of NLP for Building Educational Appli-
cations in June 2013 in Atlanta, USA. The report
is organized as follows: we briefly review related
works in Section 2. The data sets to experiment with
are characterized in Section 3. Section 4 lists the
main concepts we pursue during the system devel-
opment. Our approach is entirely focused on feature
engineering and thus Section 5 is the most impor-
tant one. We present there our main motivation for
making such a decision, describe patterns according
to which the features are generated and techniques
that manipulate the features. We revise our ideas ex-
perimentally as documented in Section 6. In total,
we submitted five systems to the sub-task of closed-
training. In Sections 7 and 8, we describe these sys-
tems and discuss their results in detail. We summa-
rize our two month effort in the shared task in Sec-
tion 9.
232
2 Related work
We understand the task of native language identifica-
tion as a subtask of natural language processing and
we consider it as still a young task since the very
first attempt to address it occurred eight years ago in
2005, as evident from the literature, namely (Koppel
et al, 2005b), (Koppel et al, 2005a).
We appreciate all the previous work concerned
with the given topic but we focus on the latest three
papers only, all of them published at the 24th In-
ternational Conference on Computational Linguis-
tics held in December 2012 in Bombay, India,
namely (Brooke and Hirst, 2012), (Bykh and Meur-
ers, 2012), and (Tetreault et al, 2012). They provide
a comprehensive review of everything done since the
very first attempts. We do not want to replicate their
chapters. Rather, we summarize them from the as-
pects we consider the most important ones in any
machine learning system, namely the data, the fea-
ture design, the feature manipulation, and the ma-
chine learning methods - see Table 1.
3 Data sets
A new publicly available corpus of non-native En-
glish writing called TOEFL112 consists of essays on
eight different topics written by non-native speakers
of three proficiency levels (low/medium/high); the
essays? authors have 11 different native languages.
The corpus contains 1,100 essays per language with
an average of 348 word tokens per essay. A corpus
description and motivation to build such corpus can
be found in (Blanchard et al, 2013).
The texts from TOEFL11 were released for the
purpose of the shared task as three subsets, namely
Train for training, DevTest for testing while sys-
tem development, and EvalTest for final testing.
The texts were already tokenized and we processed
them with the Standford POS tagger (Toutanova et
al., 2003).
4 System settings
1. Task: Having a collection of English essays
written by non-native speakers, the goal is to
predict a native language of the essays? authors.
2Source: Derived from data provided by ETS. Copyright c?
2013 ETS. www.ets.org.
Languages L1 are known in advance. Since we
have a collection of English essays for which
L1 is known (TOEFL11) at our disposal, we
formulate this task as a classification task ad-
dressed by using supervised machine learning
methods.
2. Feature set: A setA = {A1, A2, ..., Am} ofm
features where m changes as we perform var-
ious feature combinations and filtering steps.
We prefer to work with binary features. We
do not include two extra features, proficiency
level and prompt, provided with the data. In
addition, we design features across all 11 lan-
guages, i.e. we do not design features sepa-
rately for a particular L1. Doing so, we ad-
dress the task of predicting L1 from the text
only, without any additional knowledge.
3. Input data: A set X of instances being texts
from TOEFL11 corpus represented as feature
vectors, x = ?x1, x2, ..., xm? ? X,xi ? Ai.
4. Output classes: A set C of L1 languages, C
= {ARA, CHIN, FRE, GER, HIN, ITA, JPN,
KOR, SPA, TEL, TUR}, |C| = 11.
5. True prediction: A set D = {< x, y >:
x ? X , y ? C}, |D| = 12, 100 and its pairwise
disjoint subsets Train, DevTest, EvalTest
where Train ? DevTest ? EvalTest = D,
|Train| = 9, 900, |DevTest| = 1, 100,
|EvalTest| = 1, 100.
6. Training data: Train ? DevTest. No other
type of training data is used.
7. Learning mechanism: Since we focus on fea-
ture engineering, we do not study appropriate-
ness of particular machine learning methods to
our task in details. Instead, reviewing the re-
lated works, we selected the Support Vector
Machine algorithm to experiment with.
8. Evaluation: 10-fold cross-validation with the
sample Train ? DevTest. Accuracy, Pre-
cision, Recall. Proficiency-based evaluation.
Topic-based evaluation.
233
PAPER DATA FEATURE FEATURE ML
DESIGN MANIPULATION METHOD
[1] Lang-8,
ICLE,
Cambridge
Learner
Corpus
function words, charac-
ter n-grams, POS n-grams,
POS/function n-grams, context-
free-grammar productions,
dependencies, word n-grams
frequency-based
feature selection
SVM, MaxEnt
[2] ICLE binary features spanning word-
based recurring n-grams, func-
tion words, recurring POS based
n-grams and combination of
them
no special feature
treatment
logistic regression
[3] ICLE,
TOEFL11
character n-grams, function
words, POS, spelling errors,
writing quality
no special feature
treatment
logistic regression
Table 1: A summary of latest related works [1](Brooke and Hirst, 2012), [2](Bykh and Meurers, 2012), [3](Tetreault
at al., 2012)
5 Feature engineering
We split the process of feature engineering into two
mutually interlinked steps. The first step aims at an
understanding of the task projected into features de-
scribing properties of entities we experiment with.
These experiments represent the second step where
we find out how the features interact with each other
and how they interact with a chosen machine learn-
ing algorithm.
We compose a feature family as a group of pat-
terns that are relevant for a particular task. The fea-
tures are then extracted from the data according to
them. Since we experiment with English texts writ-
ten by non-native speakers, we have to search for
specific and identifiable text properties, i.e. tenden-
cies of certain first language writers, based on the
errors caused by the difference between L1 and L2.
In addition, we look for phenomena that are not nec-
essarily incorrect in written English but they provide
clear evidence of characteristics typical for L1. Our
feature family is built from chunks of various length
in the texts, formally lexically and part-of-speech
based n-grams. In total, the feature family contains
eight patterns described in Table 2 - six for binary
features l,n,p,s1,s2,sp and two for continuous fea-
tures a,r. Outside the feature family, its patterns can
be combined into joint patterns, like l+sp, n+sp+r.
Considering the key issues of machine learning,
Figure 1: Feature engineering
we mainly pay attention to overfitting. We are aware
of many aspects that may cause overfitting, like
complexity of the model trained, noise in training
data, a small amount of training data. Features can
lead to overfitting as well, thus we address it us-
ing elaborated feature engineering visualised in Fig-
ure 1. We can see there the data components and the
process components having the features in common.
The scheme can be traced either with individual pat-
terns from the feature family or with joint patterns.
Both basic feature filtering and advanced feature
manipulation apply selected concepts from informa-
234
FEATURE DESCRIPTION EXAMPLES
FAMILY n=1,2,3
PATTERN
l n-grams of lemmas picture; to see; you, be, not
n n-grams of words picture; to see; you, are, not
p n-grams of function words and POS tags of content
words, i.e. nouns, verbs, adjectives, cardinal num-
bers
not; PRP; you, VBP; JJ, to, VB
s1 skipgrams of words: bigram wi?2, wi and trigrams
wi?3, wi?1, wi, wi?3, wi?2, wi extracted from a se-
quence of words wi?3 wi?2 wi?1 wi
you,not; able, see; to, see,in; to
things, in
s2 skipgrams of words: bigrams wi?3, wi, wi?4, wi
and trigrams wi?4, wi?3, wi, wi?4, wi?2, wi,
wi?4, wi?1, wi extracted from a sequence of words
wi?4 wi?3 wi?2 wi?1 wi
are,see; you,see; you,are,see;
you,able,see; you,to,see;
sp n-grams of function words and shrunken POS tags
of content words: POS tags N* are shrunken into a
tag N, V* into V, J* into J
not; PRP; you V; J to V
a relative frequency of POS tags and function words
r relative frequency of POS tags
Table 2: A feature family. Examples are taken from the file 498.txt, namely from the sentence You are not able to
see things in a big picture. tagged as follows: (You/you/PRP are/be/VBP not/not/RB able/able/JJ to/to/TO see/see/VB
things/thing/NNS in/in/IN a/a/DT big/big/JJ picture/picture/NN ././.)
tion theory.
5.1 Concepts from information theory
Consider a random variable A having two possible
values 0 and 1 where the probability of 1 is p and
0 is 1 ? p. A degree of uncertainty we deal with
when predicting the value of the variable depends
on p. If p is close to zero or one, then we are almost
confident about the value and our uncertainty is low.
If the values are equally likely (i.e. p = 0.5), our
uncertainty is maximal.
The entropy H(A) measures the uncertainty. In
other words, it quantifies the amount of information
needed to predict the value of the variable. The for-
mula 1 for the entropy treats variables with N ? 1
possible values.
H(A) = ?
N?
i=1
p(A = ai) log2 p(A = ai) (1)
The conditional entropy H(A|B) quantifies the
amount of information needed to predict the value
of the random variable A given that the value of an-
other random variable B is known, see Formula 2.
Then H(A|B) ? H(A) holds.
H(A|B) =
?
b?B
p(B = b)H(A|B = b) (2)
The amount H(A) ? H(A|B) by which H(A)
decreases reflects additional information about A
provided by B and is called mutual information
I(A;B) - see Formula 3. In other words, I(A;B)
quantifies the mutual dependence of two random
variables A and B.
I(A;B) = H(A)?H(A|B) (3)
Proceeding from statistics to machine learning,
independent random variables correspond to fea-
tures. Thus we can directly speak about the entropy
of a feature, the conditional entropy of a feature
given another feature and the mutual information of
two features.
235
Information gain of feature Ak - IG(Ak) - mea-
sures the expected reduction in entropy caused by
partitioning the data set Data according to the val-
ues of the feature Ak (Quinlan, 1987):
IG(Ak) = H(Data)?
c?
i=j
|Dvj |
|Data|
H(Dvj ), (4)
where Avk = {v1, v2, ..., vc} is a set of possible val-
ues of feature Ak and Dvi is a subset of Data con-
tainig instances with the feature value xk = vj .
C being a target feature, H(Data) = H(C).
Thus the mutual information between C and Ak -
I(C;Ak) - is the information gain of the feature Ak,
i.e.
I(C;Ak) = IG(Ak). (5)
All mentioned concepts are visualized in Figure 2
for our settings:
? Our target feature C has eleven possible val-
ues (i.e. L1 languages). These values are
uniformly distributed in the data D, thus
H(C) = ?
?11
i=1
1
11 log2
1
11 = log2 11
.=
3.46. Sample features (only for illustration)
A1, A2, A3, A4 ? A are binary features so
H(Ai) ? 1 < H(C) = 3.46, i = 1, ..., 4.
The circle areas correspond to the entropy of
features.
? The black areas correspond to mutual informa-
tion I(Ai;Ak).
? The striped areas correspond to the mutual in-
formation I(C;Ak) between C and Ak.
? Features A1 and A3 are independent, so
I(A1;A3) = 0.
? A2 has the highest mutual dependence with C,
? H(A2) = H(A3) and IG(A2) > IG(A3)
In addition to the concepts from information the-
ory, we introduce another measure to quantify fea-
tures: the document frequency of feature Ak ?
df(Ak) is the number of texts in which Ak occurs,
i.e. df(Ak) ? 0.
Figure 2: Information gain and mutual information visu-
alization
5.2 Discussion on features
We impose a fundamental requirement on features:
they should be both informative (i.e. useful for the
classification task) and robust (i.e. not sensitive to
training data). We control the criterion of being in-
formative by information gain maximization. The
criterion of being robust is quantified by document
frequency. If df(Ak) is high enough, then we can
expect that Ak will occur in test data frequently. We
propose two techniques to increase df : (i) filtering
out features with low df ; (ii) feature combination
driven by IG.
The fulfillment of both criteria is always depen-
dent on training data, i.e. the final feature set tends
to fit training data and our goal is to weaken this ten-
dency in order to get a more robust feature set. Both
basic feature filtering and advanced feature combi-
nation help us to address this issue.
5.3 Basic feature filtering
We obtained the feature setA0 by extracting features
according to the feature family patterns) from the
training data. Basic feature filtering removes fea-
tures from A0 in two steps that result in a primary
feature set A1:
1. Remove binary feature Ak if df(Ak) < ?df .
Remove continous feature Ak if
relative frequency(Ak) < ?rf or
df(relative frequency(Ak) ? ?rf ) < ?df .
2. Remove binary feature Ak if IG(Ak) ? ?IG.
236
5.4 Advanced feature manipulation
The process of advanced feature manipulation han-
dles m input features from the primary feature set
A1 in two different ways, filter them and combine
them, in order to generate a final feature set Af
ready to train the model:
? Filter them. We use Fast Correlation-Based
Filter (FCBF; (Fleuret, 2004), (Yu and Liu,
2003)) that addresses the correlation between
features. It first ranks the features accord-
ing to their information gain, i.e. IG(A1) ?
IG(A2) ? ... ? IG(Am). In the second step,
it iteratively removes any featureAk if there ex-
ists a feature Aj such that IG(Aj) ? IG(Ak)
and I(Ak;Aj) ? IG(Ak), i.e. Aj is bet-
ter as a predictor of C and Ak is more sim-
ilar to Aj than to C. In the situation visu-
alized in Figure 2, the feature A4 will be fil-
tered out because there is a featureA3 such that
IG(A3) ? IG(A4) and I(A3;A4) ? IG(A4)
? Combine them. We combine (COMB) binary
features using logical operations (AND, OR,
XOR, AND NOT, etc.) getting a new binary
feature.
For example, if we combine two features A1
and A2 using the OR operator, we get a new
binary feature Y = A1 OR A2 for which the
inequalities df(Y ) > df(A1) and df(Y ) >
df(A2) hold. Thus we get a feature that is
more robust than the two input features. To
know whether it is more informative, we need
to know how high IG(Y ) is with respect to
IG(A1) and IG(A2). Without loss of gen-
erality, assume that IG(A1) > IG(A2). If
IG(Y ) > IG(A1) > IG(A2), then Y is more
informative than A1 and A2, but both of these
features could be informative enough as well.
It depends on the threshold we set up for being
informative. We can easily iterate this process -
let Y1 = A1 ORA2 and Y2 = A3 ORA4. Then
we can combine Y3 = Y1 OR A5 or Y4 = Y1
OR Y2, etc.
Then, advanced feature manupilation runs ac-
cording to scenarios formed as a series of FCBF
and COMB, for example A1 ? FCBF ? COMB
? FCBF? Af or A1 ? COMB? FCBF? Af .
6 System development
During system development, we formulated hy-
potheses how to avoid overfitting and get features ro-
bust and informative enough. In parallel, we run the
experiments with parameters using which we con-
trolled this requirement.
Basic feature filtering We set the thresholds ?df ,
?IG, ?rf empirically to the values 4, 0 and 0.02, re-
spectively. Table 3 shows the changes in the size of
the initial feature set after the basic feature filtering.
It is evident that even such trivial filtering reduces
the number of features substantially.
FEATURE INITIAL AFTER AFTER
FAMILY FEATURE df IG
PATTERN SET FILTERING FILTERING
(i.e. |A0|) (i.e. |A1|)
l 2,078,105 156,722 2,827
n 2,411,516 163,939 2,840
p 1,116,986 161,681 2,467
s1 4,794,702 242,969 1,877
s2 7,632,011 382,881 4,566
sp 781,018 123,431 933
a 181 111 111
r 48 48 48
Table 3: Volumes of initial feature sets extracted from
Train ? DevTest (1st column). Volumes of primary
feature sets after basic filtering of A0 (3rd column)
.
Learning mechanisms Originally, we started
with two learning algorithms, Random Forests (RF)
and Support Vector Machines (SVM), running them
in the R system.3
The Random forests4 algorithm joins random-
ness with classification decision trees. They iterate
the process of two random selections and training a
decision tree k-times on a subset ofm features. Each
of them classifies a new input instance x and the
class with the most votes becomes the output class
of x.
Support Vector Machines (Vapnik, 1995) effi-
ciently perform both linear and non-linear classi-
fication employing different Kernel functions and
3http://www.r-project.org
4http://www.stat.berkeley.edu/?breiman/
237
avoiding the overfitting by two parameters, cost and
gamma.
We run a number of initial experiments with the
following settings: the feature family pattern n; the
basic feature filtering, RF with different values of
parameters k and m, SVM with different values of
parameters kernel, gamma and cost
Cross-validation on the data set Train performed
with SVM showed significantly better results than
those obtained with RF. We were quite suprised that
RF ran with low performance so that we decided
to stop experimenting with this algorithm. Step by
step, we added patterns into the feature family and
carried out experiments with SVM only on the data
set Train ? DevTest. We fixed the values of the
SVM parameters kernel, degree, gamma, cost after
several experiments as follows kernel = polynomial,
degree = 1, gamma = 0.0004, cost = 1. Then we
included the advanced feature manipulation into the
experiments according to the scenariosA1 ? FCBF
? COMB ? FCBF ? Af and A1 ? COMB ?
FCBF? Af . COMB was composed using the OR
operator only. Unfortunately, none of them outper-
formed the initial experiments with the basic filter-
ing only.
Table 4 contains candidates for the final submis-
sion. The highlighted candidates were finally se-
lected for the submission.
FEATURE CROSS-VALIDATION Acc (%)
PATTERNS on Train on DevTest
l + a 72.97 ? 0.76 71.09
n + a 72.45 ? 0.98 63.00
l + sp + a 72.00 ? 0.72 70.64
l+sp 71.09 ? 0.72 71.45
n+sp 70.38 ? 0.69 52.27
l 71.67 ? 0.57 70.18
n 71.27 ? 0.84 68.72
l+p 71.17 ? 2.41 71.27
n+s1 69.90 ? 1.04 66.72
n+s2 68.75 ? 1.50 67.63
n+s1+s2 67.97 ? 0.96 66.81
Table 4: Candidates for the final submission. Candidates
in bold were submitted.
MODEL FEATURE FAMILY Acc
PATTERN (%)
CUNI-closed-1 majority voting
of CUNI-closed-[2-5] 72.5
CUNI-closed-2 l+a 71.6
CUNI-closed-3 l+p 71.6
CUNI-closed-5 l+sp+a 71.1
CUNI-closed-4 l+sp 69.7
Table 5: An overview of models submitted.
MODEL Acc (%)
CUNI-closed-1 74.2
CUNI-closed-2 73.4
CUNI-closed-3 73.9
CUNI-closed-4 73.1
CUNI-closed-5 72.9
Table 6: Cross-validation results for all submitted CUNI-
closed systems.
7 Submission to the shared task
In total, we submitted five systems to the closed-
training sub-task - see their overview in Table 5. The
results correspond to our expectations that we made
based on the results of cross-validation presented in
Table 4. The best system, CUNI-closed-1, was the
outcome of majority voting of the remaining four
systems. The performance of this system per lan-
guage is presented in Table 7.
Table 6 reports accuracy results when doing 10-
fold cross-validation on Train ? DevTest. The
folds for this experiment were provided by the or-
ganizers to get more reliable comparison of the NLI
systems.
It is interesting to analyse the complementarity of
the CUNI-closed-[2-5] systems that affects the per-
formance of CUNI-closed-1. In Table 8, we list the
numerical characteristics of five possible situations
that can occur when comparing the outputs of two
systems i and j. Situations 2 and 3 capture how
complementary the systems are. The numbers for
our systems are presented in Table 9.
We grouped languages according to the thresholds
of F-measure. First we did it across the data, no mat-
ter what the proficiency level and prompt are - see
the first row of Table 10. Second we did grouping
238
Acc(%) P(%) R(%) F(%)
ARA 72 67 72 69,6
CHI 78 71 78 74,3
FRE 73 74 73 73,7
GER 83 83 83 83,0
HIN 75 68 75 71,4
ITA 83 85 83 83,8
JPN 70 65 70 67,6
KOR 64 70 64 67,0
SPA 66 70 66 68,0
TEL 68 72 68 69,7
TUR 65 72 65 68,4
Table 7: CUNI-closed-1 on EvalTest: Acc, P, R, F
1. the number of instances both systems pre-
dicted correctly;
2. the number of instances both systems pre-
dicted incorrectly;
3. the number of instances the systems pre-
dicted differently: i system correctly and j
system incorrectly;
4. the number of instance the systems pre-
dicted differently: i system incorrectly and j
system correctly;
5. the number of instances the systems pre-
dicted differently and both incorrectly.
Table 8: Pair of two systems i and j and their predictions.
pair of CUNI-closed-i
and CUNI-closed-j systems
2-3 2-4 2-5 3-4 3-5 4-5
1 707 717 745 701 710 732
2 161 215 242 183 181 250
3 81 71 43 87 78 35
4 81 50 37 66 72 50
5 70 47 33 63 59 33
Table 9: CUNI-closed-[2-5]: complementary rates.
? 90% ? 80% ? 70% < 70%
overall GER,
ITA
CHI,
FRE,
HIN
TEL,
ARA,
TUR,
SPA,
JPN,
KOR
high GER,
ITA
CHI,
HIN,
FRE
KOR,
TUR,
SPA,
TEL,
ARA,
JPN
medium ITA,
GER,
FRE,
TEL
CHI,
ARA,
SPA,
TUR
JPN,
KOR,
HIN
low GER ITA,
FRE,
JPN
ARA KOR,
TEL,
HIN,
TUR,
SPA,
CHI,
FRE
Table 10: CUNI-closed-1 on EvalTest: Groups of lan-
guages sorted according to F-measure w.r.t. proficiency
level.
for a particular proficiency level - see the remaining
rows in Table 10. We can see that both GER and
ITA are languages with the highest F-measure on all
levels. Third we grouped by a particular prompt -
see Table 11. We can see there diversed numbers for
L1 languages despite the fact that prompts are for-
mulated generally. Even more, we observe a topic
similarity between prompts P2, P3, and P8, between
P4 and P5, and between P1 and P7.
8 Future plans
In our future research, w want to elaborate ideas that
concern the feature engineering. We plan to work
with the feature family that we designed in our ini-
tial experiments. However, we will think about more
specific patterns in the essays, like the average count
of tokens/punctuation/capitalized nouns/articles per
sentence. As Table 12 shows, there is only one can-
didate, namely the number of tokens in sentence, to
be taken into considerations since there is the largest
difference between minimum and maximum.
We confronted Ken Lackman,5 an English
teacher, with the task of manual native language
identification by English teachers. He says: ?I think
5http://kenlackman.com
239
? 90% ? 80% ? 70% < 70%
P1 GER,
ITA
FRE,
HIN,
ARA,
TEL
CHI,
KOR,
TUR
SPA,
JPN
P2 GER,
FRE,
ITA,
TEL
ARA,
HIN,
JPN
SPA,
KOR,
CHI
TUR
P3 GER CHI,
KOR
HIN,
ITA
FRE,
JPN,
TUR,
ARA,
SPA,
TEL
P4 ITA CHI,
TUR,
HIN,
FRE
TEL,
SPA,
GER,
JPN,
ARA,
KOR
P5 ITA TUR,
JPN,
GER
FRE,
TEL,
KOR
HIN,
CHI,
SPA,
ARA
P6 ITA,
CHI,
SPA
KOR,
ARA,
JPN
HIN,
FRE,
TEL,
GER,
TUR
P7 ITA,
CHI,
TUR
SPA,
GER,
HIN,
FRE
ARA,
JPN,
KOR,
TEL
P8 ARA GER,
TEL,
SPA,
ITA
FRE
HIN,
KOR,
JPN,
TUR,
CHI
Table 11: CUNI-closed-1 on EvalTest: Groups of lan-
guages sorted according to F-measure w.r.t. prompt.
AVG COUNT Train
PER MIN (L1) - MAX (L1)
SENTENCE
TOKEN 18 (JPN) -25.8 (SPA)
PUNCTUATION 1.5 (HIN, TEL) - 2.1 (SPA)
CAPITALIZED 0.1 (CHI) - 0.3 (HIN)
NOUN
the 0.6 (KOR) - 1.2 (ITA, SPA, TEL)
a/an 0.3 (JPN, KOR) - 0.7 (ITA, SPA)
Table 12: Data counts on Train.
it?s quite possible to do but you would need a set of
guidelines to supply teachers with. The guidelines
would list tendancies of certain first language writ-
ers, based on errors caused by difference between
L1 and L2. For example, Germans tend to capital-
ize too many nouns, since there are far more nouns
capitalized in their language, Asians tend to leave
out articles and Arab students tend to use the verb
?to be? inappropriately before other verbs.? Look-
ing into the data, we observe the phenomena Ken is
speaking about, but the quantity of them is not sta-
tistically significant to distinguish L1s.
We formulate an idea of a bootstrapped feature
extraction that has not been published yet, at least
to our knowledge. Let us assume a set of opera-
tions that can be performed over a feature set (so far,
we have proposed two possible operations with the
features, filtering them out and their combinations).
Determining whether a condition to perform a given
operation holds is done on the high number of ran-
dom samples. If the condition holds on the majority
of them, then the operation is performed. The only
parameter that must be set up is the majority. In-
stead of setting a threshold that is adjusted for all the
features, bootstrapped feature extraction deals with
fitting the data individually for each feature.
9 Conclusion
It was the very first experience for our team to ad-
dress the task of NLI. We assess it as very stimu-
lating and we understand our participation as setting
the baseline for applying other ideas. An overall ta-
ble of results (Tetreault et al, 2013) for all the teams
involved in the NLI 2013 Shared Task shows that
there is still space for improvement of our baseline.
We really appreciate all the work done by the or-
ganizers. They?ve made an effort to prepare the
high-quality data and set up the framework by which
the use of various NLI systems can be reliably com-
pared.
Acknowledgments
The authors would like to thank Eva Hajic?ova? and
Jirka Hana for their valuable comments. We also
thank Ken Lackman and Leslie Ryan6 for sharing
6http://lesliestreet.cz
240
their teaching experience. This research was sup-
ported by the Czech Science Foundation, grant no.
P103/12/G084 and the Technology Agency of the
Czech Republic, grant no. TA02010182.
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Adriane Boyd, Marion Zepf, and Detmar Meurers. 2012.
Informing Determiner and Preposition Error Correc-
tion with Hierarchical Word Clustering. In Proceed-
ings of the 7th Workshop on Innovative Use of NLP
for Building Educational Applications (BEA7), pages
208?215, Montreal, Canada. Association for Compu-
tational Linguistics.
Julian Brooke and Graeme Hirst. 2012. Robust, Lexical-
ized Native Language Identification. In Proceedings
of COLING 2012, pages 391?408, Mumbai, India, De-
cember.
Serhiy Bykh and Detmar Meurers. 2012. Native Lan-
guage Identification using Recurring n-grams ? Inves-
tigating Abstraction and Domain Dependence. In Pro-
ceedings of COLING 2012, pages 425?440, Mumbai,
India, December.
F. Fleuret. 2004. Fast Binary Feature Selection with
Conditional Mutual Information. Journal of Machine
Learning Research (JMLR), 5:1531?1555.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English v2 (Handbook + CD-ROM). Presses
universitaires de Louvain, Louvain-la-Neuve.
Jirka Hana, Alexandr Rosen, Svatava S?kodova?, and
Barbora S?tindlova?. 2010. Error-tagged Learner Cor-
pus of Czech. In Proceedings of the Fourth Lin-
guistic Annotation Workshop (LAW IV), pages 11?
19, Stroudsburg, USA. Association for Computational
Linguistics.
Jirka Hana, Alexandr Rosen, Barbora S?tindlova?, and
Petr Ja?ger. 2012. Building a learner corpus. In
Proceedings of the 8th International Conference on
Language Resources and Evaluation (LREC 2012),
I?stanbul, Turkey. European Language Resources As-
sociation.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005a.
Automatically determining an anonymous author?s na-
tive language. Intelligence and Security Informatics,
pages 41?76.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005b.
Determining an author?s native language by mining
a text for errors. In Proceedings of the 11th ACM
SIGKDD, pages 624?628, Chicago, IL. ACM.
John Ross Quinlan. 1987. Simplifying decision trees.
International Journal of ManMachine Studies, 27,
221-234.
Marc Reznicek, Anke Ludeling, Cedric Krummes,
Franziska Schwantuschke, Maik Walter, Karin
Schmidt, Hagen Hirschmann, and Torsten Andreas.
2012. Das Falko-Handbuch. Korpusaufbau und
Annotationen Version 2.01. Technical report, Depart-
ment of German Studies and Linguistics, Humboldt
University, Berlin, Germany.
Alla Rozovskaya and Dan Roth. 2010. Annotating ESL
Errors: Challenges and Rewards. In Proceedings of
the NAACL HLT 2010 Fifth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 28?36, Los Angeles, California, June. Associ-
ation for Computational Linguistics.
Toshikazu Tajiri, Mamoru Komachi, and Yuji Mat-
sumoto. 2012. Tense and Aspect Error Correction for
ESL Learners Using Global Context. In In Proceed-
ings of the 50th ACL: Short Papers, pages 192?202.
Joel R. Tetreault and Martin Chodorow. 2008. Native
judgments of non-native usage: experiments in prepo-
sition error detection. In Proceedings of the Work-
shop on Human Judgements in Computational Lin-
guistics, HumanJudge ?08, pages 24?32, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native Tongues, Lost and
Found: Resources and Empirical Evaluations in Na-
tive Language Identification. In Proceedings of COL-
ING 2012, pages 2585?2602, Mumbai, India, Decem-
ber. The COLING 2012 Organizing Committee.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A Report on the First Native Language Identification
Shared Task. In Proceedings of the Eighth Workshop
on Innovative Use of NLP for Building Educational
Applications, Atlanta, GA, USA, June. Association for
Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
In Proceedings of HLT-NAACL 2003, pages 252?259.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag, New York.
L. Yu and H. Liu. 2003. Feature Selection for High-
Dimensional Data: A Fast Correlation-Based Filter
Solution. In Proceedings of The Twentieth Interna-
tional Conference on Machine Leaning (ICML-03),
pages 856?863, Washington, D.C., USA. Association
for Computational Linguistics.
241
