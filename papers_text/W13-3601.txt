Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1?12,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
The CoNLL-2013 Shared Task on Grammatical Error Correction
Hwee Tou Ng
Department of Computer Science
National University of Singapore
nght@comp.nus.edu.sg
Siew Mei Wu
Centre for English Language Communication
National University of Singapore
elcwusm@nus.edu.sg
Yuanbin Wu and Christian Hadiwinoto
Department of Computer Science
National University of Singapore
{wuyb,chrhad}@comp.nus.edu.sg
Joel Tetreault
Nuance Communications, Inc.
Joel.Tetreault@nuance.com
Abstract
The CoNLL-2013 shared task was devoted
to grammatical error correction. In this
paper, we give the task definition, present
the data sets, and describe the evaluation
metric and scorer used in the shared task.
We also give an overview of the various
approaches adopted by the participating
teams, and present the evaluation results.
1 Introduction
Grammatical error correction is the shared task
of the Seventeenth Conference on Computational
Natural Language Learning in 2013 (CoNLL-
2013). In this task, given an English essay written
by a learner of English as a second language, the
goal is to detect and correct the grammatical errors
present in the essay, and return the corrected essay.
This task has attracted much recent research in-
terest, with two shared tasks Helping Our Own
(HOO) 2011 and 2012 organized in the past two
years (Dale and Kilgarriff, 2011; Dale et al,
2012). In contrast to previous CoNLL shared tasks
which focused on particular subtasks of natural
language processing, such as named entity recog-
nition, semantic role labeling, dependency pars-
ing, or coreference resolution, grammatical error
correction aims at building a complete end-to-end
application. This task is challenging since for
many error types, current grammatical error cor-
rection systems do not achieve high performance
and much research is still needed. Also, tackling
this task has far-reaching impact, since it is esti-
mated that hundreds of millions of people world-
wide are learning English and they benefit directly
from an automated grammar checker.
The CoNLL-2013 shared task provides a forum
for participating teams to work on the same gram-
matical error correction task, with evaluation on
the same blind test set using the same evaluation
metric and scorer. This overview paper contains a
detailed description of the shared task, and is orga-
nized as follows. Section 2 provides the task def-
inition. Section 3 describes the annotated training
data provided and the blind test data. Section 4 de-
scribes the evaluation metric and the scorer. Sec-
tion 5 lists the participating teams and outlines the
approaches to grammatical error correction used
by the teams. Section 6 presents the results of the
shared task. Section 7 concludes the paper.
2 Task Definition
The goal of the CoNLL-2013 shared task is to
evaluate algorithms and systems for automati-
cally detecting and correcting grammatical errors
present in English essays written by second lan-
guage learners of English. Each participating
team is given training data manually annotated
with corrections of grammatical errors. The test
data consists of new, blind test essays. Prepro-
cessed test essays, which have been sentence-
segmented and tokenized, are also made available
to the participating teams. Each team is to submit
its system output consisting of the automatically
corrected essays, in sentence-segmented and tok-
enized form.
Grammatical errors consist of many different
types, including articles or determiners, preposi-
tions, noun form, verb form, subject-verb agree-
ment, pronouns, word choice, sentence structure,
punctuation, capitalization, etc. Of all the er-
ror types, determiners and prepositions are among
1
the most frequent errors made by learners of En-
glish. Not surprisingly, much published research
on grammatical error correction focuses on arti-
cle and preposition errors (Han et al, 2006; Ga-
mon, 2010; Rozovskaya and Roth, 2010; Tetreault
et al, 2010; Dahlmeier and Ng, 2011b), with rel-
atively less work on correcting word choice errors
(Dahlmeier and Ng, 2011a). Article and preposi-
tion errors were also the only error types featured
in the HOO 2012 shared task. Likewise, although
all error types were included in the HOO 2011
shared task, almost all participating teams dealt
with article and preposition errors only (besides
spelling and punctuation errors).
In the CoNLL-2013 shared task, it was felt
that the community is now ready to deal with
more error types, including noun number, verb
form, and subject-verb agreement, besides arti-
cles/determiners and prepositions. Table 1 shows
examples of the five error types in our shared task.
Since there are five error types in our shared task
compared to two in HOO 2012, there is a greater
chance of encountering multiple, interacting errors
in a sentence in our shared task. This increases the
complexity of our shared task relative to that of
HOO 2012. To illustrate, consider the following
sentence:
Although we have to admit some bad
effect which is brought by the new
technology, still the advantages of the
new technologies cannot be simply dis-
carded.
The noun number error effect needs to be corrected
(effect? effects). This necessitates the correction
of a subject-verb agreement error (is ? are). A
pipeline system in which corrections for subject-
verb agreement errors occur strictly before correc-
tions for noun number errors would not be able
to arrive at a fully corrected sentence for this ex-
ample. The ability to correct multiple, interacting
errors is thus necessary in our shared task. The re-
cent work of (Dahlmeier and Ng, 2012a), for ex-
ample, is designed to deal with multiple, interact-
ing errors.
Note that the essays in the training data and the
test essays naturally contain grammatical errors of
all types, beyond the five error types focused in our
shared task. In the automatically corrected essays
returned by a participating system, only correc-
tions necessary to correct errors of the five types
are made. The other errors are to be left uncor-
rected.
3 Data
This section describes the training and test data
released to each participating team in our shared
task.
3.1 Training Data
The training data provided in our shared task is
the NUCLE corpus, the NUS Corpus of Learner
English (Dahlmeier et al, 2013). As noted by
(Leacock et al, 2010), the lack of a manually an-
notated and corrected corpus of English learner
texts has been an impediment to progress in gram-
matical error correction, since it prevents com-
parative evaluations on a common benchmark test
data set. NUCLE was created precisely to fill this
void. It is a collection of 1,414 essays written
by students at the National University of Singa-
pore (NUS) who are non-native speakers of En-
glish. The essays were written in response to some
prompts, and they cover a wide range of topics,
such as environmental pollution, health care, etc.
The grammatical errors in these essays have been
hand-corrected by professional English instructors
at NUS. For each grammatical error instance, the
start and end character offsets of the erroneous text
span are marked, and the error type and the cor-
rection string are provided. Manual annotation is
carried out using a graphical user interface specif-
ically built for this purpose. The error annotations
are saved as stand-off annotations, in SGML for-
mat.
To illustrate, consider the following sentence at
the start of the first paragraph of an essay:
From past to the present, many impor-
tant innovations have surfaced.
There is an article/determiner error (past ? the
past) in this sentence. The error annotation, also
called correction or edit, in SGML format is
shown in Figure 1. start par (end par) de-
notes the paragraph ID of the start (end) of the er-
roneous text span (paragraph ID starts from 0 by
convention). start off (end off) denotes the
character offset of the start (end) of the erroneous
text span (again, character offset starts from 0 by
convention). The error tag is ArtOrDet, and the
correction string is the past.
2
Error tag Error type Example sentence Correction (edit)
ArtOrDet Article or determiner In late nineteenth century, there
was a severe air crash happening
at Miami international airport.
late ? the late
Prep Preposition Also tracking people is very
dangerous if it has been con-
trolled by bad men in a not good
purpose.
in ? for
Nn Noun number I think such powerful device
shall not be made easily avail-
able.
device ? devices
Vform Verb form However, it is an achievement as
it is an indication that our soci-
ety is progressed well and peo-
ple are living in better condi-
tions.
progressed ? progressing
SVA Subject-verb agreement People still prefers to bear the
risk and allow their pets to have
maximum freedom.
prefers ? prefer
Table 1: The five error types in our shared task.
<MISTAKE start par="0" start off="5" end par="0" end off="9">
<TYPE>ArtOrDet</TYPE>
<CORRECTION>the past</CORRECTION>
</MISTAKE>
Figure 1: An example error annotation.
The NUCLE corpus was first used in
(Dahlmeier and Ng, 2011b), and has been
publicly available for research purposes since
June 20111. All instances of grammatical errors
are annotated in NUCLE, and the errors are
classified into 27 error types (Dahlmeier et al,
2013).
To help participating teams in their prepara-
tion for the shared task, we also performed au-
tomatic preprocessing of the NUCLE corpus and
released the preprocessed form of NUCLE. The
preprocessing operations performed on the NU-
CLE essays include sentence segmentation and
word tokenization using the NLTK toolkit (Bird
et al, 2009), and part-of-speech (POS) tagging,
constituency and dependency tree parsing using
the Stanford parser (Klein and Manning, 2003;
de Marneffe et al, 2006). The error annotations,
which are originally at the character level, are
then mapped to error annotations at the word to-
ken level. Error annotations at the word token
1http://www.comp.nus.edu.sg/?nlp/corpora.html
level also facilitate scoring, as we will see in Sec-
tion 4, since our scorer operates by matching to-
kens. Note that although we released our own
preprocessed version of NUCLE, the participating
teams were however free to perform their own pre-
processing if they so preferred.
3.1.1 Revised version of NUCLE
NUCLE release version 2.3 was used in the
CoNLL-2013 shared task. In this version, 17 es-
says were removed from the first release of NU-
CLE since these essays were duplicates with mul-
tiple annotations.
In the original NUCLE corpus, there is not an
explicit preposition error type. Instead, prepo-
sition errors are part of the Wcip (wrong collo-
cation/idiom/preposition) and Rloc (local redun-
dancy) error types. The Wcip error type combines
errors concerning collocations, idioms, and prepo-
sitions together into one error type. The Rloc er-
ror type annotates extraneous words which are re-
dundant and should be removed, and they include
redundant articles, determiners, and prepositions.
3
Training data Test data
(NUCLE)
# essays 1,397 50
# sentences 57,151 1,381
# word tokens 1,161,567 29,207
Table 2: Statistics of training and test data.
In our shared task, in order to facilitate the detec-
tion and correction of article/determiner errors and
preposition errors, we performed automatic map-
ping of error types in the original NUCLE cor-
pus. The mapping relies on POS tags, constituent
parse trees, and error annotations at the word token
level. Specifically, we map the error types Wcip
and Rloc to Prep, Wci, ArtOrDet, and Rloc?.
Prepositions in the error type Wcip or Rloc are
mapped to a new error type Prep, and redundant
articles or determiners in the error type Rloc are
mapped to ArtOrDet. The remaining unaffected
Wcip errors are assigned the new error type Wci
and the remaining unaffected Rloc errors are as-
signed the new error type Rloc?. The code that
performs automatic error type mapping was also
provided to the participating teams.
The statistics of the NUCLE corpus (release 2.3
version) are shown in Table 2. The distribution
of errors among the five error types is shown in
Table 3. The newly added noun number error type
in our shared task accounts for the second highest
number of errors among the five error types. The
five error types in our shared task constitute 35%
of all grammatical errors in the training data, and
47% of all errors in the test data. These figures
support our choice of these five error types to be
the focus of our shared task, since they account
for a large percentage of all grammatical errors in
English learner essays.
While the NUCLE corpus is provided in our
shared task, participating teams are free to not use
NUCLE, or to use additional resources and tools
in building their grammatical error correction sys-
tems, as long as these resources and tools are pub-
licly available and not proprietary. For example,
participating teams are free to use the Cambridge
FCE corpus (Yannakoudakis et al, 2011; Nicholls,
2003) (the training data provided in HOO 2012
(Dale et al, 2012)) as additional training data.
Error tag Training % Test %
data data
(NUCLE)
ArtOrDet 6,658 14.8 690 19.9
Prep 2,404 5.3 312 9.0
Nn 3,779 8.4 396 11.4
Vform 1,453 3.2 122 3.5
SVA 1,527 3.4 124 3.6
5 types 15,821 35.1 1,644 47.4
all types 45,106 100.0 3,470 100.0
Table 3: Error type distribution of the training and
test data.
3.2 Test Data
25 NUS students, who are non-native speakers of
English, were recruited to write new essays to be
used as blind test data in the shared task. Each
student wrote two essays in response to the two
prompts shown in Table 4, one essay per prompt.
Essays written using the first prompt are present
in the NUCLE training data, while the second
prompt is a new prompt not used previously. As
a result, 50 test essays were collected. The statis-
tics of the test essays are shown in Table 2.
Error annotation on the test essays was carried
out by a native speaker of English who is a lecturer
at the NUS Centre for English Language Commu-
nication. The distribution of errors in the test es-
says among the five error types is shown in Ta-
ble 3. The test essays were then preprocessed in
the same manner as the NUCLE corpus. The pre-
processed test essays were released to the partici-
pating teams.
Unlike the test data used in HOO 2012 which
was proprietary and not available after the shared
task, the test essays and their error annotations in
the CoNLL-2013 shared task are freely available
after the shared task.
4 Evaluation Metric and Scorer
A grammatical error correction system is evalu-
ated by how well its proposed corrections or edits
match the gold-standard edits. An essay is first
sentence-segmented and tokenized before evalua-
tion is carried out on the essay. To illustrate, con-
sider the following tokenized sentence S written
by an English learner:
There is no a doubt, tracking system
4
ID Prompt
1 Surveillance technology such as RFID (radio-frequency identification) should not be used to
track people (e.g., human implants and RFID tags on people or products). Do you agree? Sup-
port your argument with concrete examples.
2 Population aging is a global phenomenon. Studies have shown that the current average life span
is over 65. Projections of the United Nations indicate that the population aged 60 or over in
developed and developing countries is increasing at 2% to 3% annually. Explain why rising life
expectancies can be considered both a challenge and an achievement.
Table 4: The two prompts used for the test essays.
has brought many benefits in this infor-
mation age .
The set of gold-standard edits of a human annota-
tor is g = {a doubt ? doubt, system ? systems,
has ? have}. Suppose the tokenized output sen-
tence H of a grammatical error correction system
given the above sentence is:
There is no doubt, tracking system has
brought many benefits in this informa-
tion age .
That is, the set of system edits is e = {a doubt
? doubt}. The performance of the grammatical
error correction system is measured by how well
the two sets g and e match, in the form of recall
R, precision P , and F1 measure: R = 1/3, P =
1/1, F1 = 2RP/(R + P ) = 1/2.
More generally, given a set of n sentences,
where gi is the set of gold-standard edits for sen-
tence i, and ei is the set of system edits for sen-
tence i, recall, precision, and F1 are defined as
follows:
R =
?n
i=1 |gi ? ei|?n
i=1 |gi|
(1)
P =
?n
i=1 |gi ? ei|?n
i=1 |ei|
(2)
F1 =
2?R? P
R + P
(3)
where the intersection between gi and ei for sen-
tence i is defined as
gi ? ei = {e ? ei|?g ? gi,match(g, e)} (4)
Evaluation by the HOO scorer (Dale and Kilgar-
riff, 2011) is based on computing recall, precision,
and F1 measure as defined above.
Note that there are multiple ways to specify a
set of gold-standard edits that denote the same cor-
rections. For example, in the above learner-written
sentence S, alternative but equivalent sets of gold-
standard edits are {a ? , system ? systems, has
? have}, {a ? , system has ? systems have},
etc. Given the same learner-written sentence S
and the same system output sentence H shown
above, one would expect a scorer to give the same
R,P, F1 scores regardless of which of the equiv-
alent sets of gold-standard edits is specified by an
annotator.
However, this is not the case with the HOO
scorer. This is because the HOO scorer uses
GNU wdiff2 to extract the differences between
the learner-written sentence S and the system out-
put sentence H to form a set of system edits.
Since in general there are multiple ways to spec-
ify a set of gold-standard edits that denote the
same corrections, the set of system edits com-
puted by the HOO scorer may not match the set of
gold-standard edits specified, leading to erroneous
scores. In the above example, the set of system
edits computed by the HOO scorer for S and H is
{a ? }. Given that the set of gold-standard edits
g is {a doubt ? doubt, system ? systems, has ?
have}, the scores computed by the HOO scorer are
R = P = F1 = 0, which are erroneous.
The MaxMatch (M2) scorer3 (Dahlmeier and
Ng, 2012b) was designed to overcome this limita-
tion of the HOO scorer. The key idea is that the
set of system edits automatically computed and
used in scoring should be the set that maximally
matches the set of gold-standard edits specified by
the annotator. The M2 scorer uses an efficient al-
gorithm to search for such a set of system edits
using an edit lattice. In the above example, given
S, H , and g, the M2 scorer is able to come up
with the best matching set of system edits e = {a
doubt ? doubt}, thus giving the correct scores
R = 1/3, P = 1/1, F1 = 1/2. We use the M2
2http://www.gnu.org/s/wdiff/
3http://www.comp.nus.edu.sg/?nlp/software.html
5
scorer in the CoNLL-2013 shared task.
The original M2 scorer implemented in
(Dahlmeier and Ng, 2012b) assumes that there
is one set of gold-standard edits gi for each
sentence i. However, it is often the case that
multiple alternative corrections are acceptable for
a sentence. As we allow participating teams to
submit alternative sets of gold-standard edits for
a sentence, we also extend the M2 scorer to deal
with multiple alternative sets of gold-standard
edits.
Based on Equations 1 and 2, Equation 3 can be
re-expressed as:
F1 =
2?
?n
i=1 |gi ? ei|?n
i=1 (|gi|+ |ei|)
(5)
To deal with multiple alternative sets of gold-
standard edits gi for a sentence i, the extended
M2 scorer chooses the gi that maximizes the cu-
mulative F1 score for sentences 1, . . . , i. Ties
are broken based on the following criteria: first
choose the gi that maximizes the numerator?n
i=1 |gi ? ei|, then choose the gi that minimizes
the denominator
?n
i=1 (|gi|+ |ei|), finally choose
the gi that appears first in the list of alternatives.
5 Approaches
54 teams registered to participate in the shared
task, out of which 17 teams submitted the output
of their grammatical error correction systems by
the deadline. These teams are listed in Table 5.
Each team is assigned a 3 to 4-letter team ID. In
the remainder of this paper, we will use the as-
signed team ID to refer to a participating team.
Every team submitted a system description paper
(the only exception is the SJT2 team).
Many different approaches are adopted by par-
ticipating teams in the CoNLL-2013 shared task,
and Table 6 summarizes these approaches. A com-
monly used approach in the shared task and in
grammatical error correction research in general
is to build a classifier for each error type. For ex-
ample, the classifier for noun number returns the
classes {singular, plural}, the classifier for article
returns the classes {a/an, the, }, etc. The classi-
fier for an error type may be learned from train-
ing examples encoding the surrounding context of
an error occurrence, or may be specified by deter-
ministic hand-crafted rules, or may be built using
a hybrid approach combining both machine learn-
ing and hand-crafted rules. These approaches are
denoted by M, R, and H respectively in Table 6.
The machine translation approach (denoted by
T in Table 6) to grammatical error correction
treats the task as ?translation? from bad English
to good English. Both phrase-based translation
and syntax-based translation approaches are used
by teams in the CoNLL-2013 shared task. An-
other related approach is the language modeling
approach (denoted by L in Table 6), in which
the probability of a learner sentence is compared
with the probability of a candidate corrected sen-
tence, based on a language model built from a
background corpus. The candidate correction is
chosen if it results in a corrected sentence with a
higher probability. In general, these approaches
are not mutually exclusive. For example, the
work of (Dahlmeier and Ng, 2012a; Yoshimoto et
al., 2013) includes elements of machine learning-
based classification, machine translation, and lan-
guage modeling approaches.
When different approaches are used to tackle
different error types by a system, we break down
the error types into different rows in Table 6, and
specify the approach used for each group of error
types. For instance, the HIT team uses a machine
learning approach to deal with article/determiner,
noun number, and preposition errors, and a rule-
based approach to deal with subject-verb agree-
ment and verb form errors. As such, the entry for
HIT is sub-divided into two rows, to make it clear
which particular error type is handled by which
approach.
Table 6 also shows the linguistic features used
by the participating teams, which include lexical
features (i.e., words, collocations, n-grams), parts-
of-speech (POS), constituency parses, dependency
parses, and semantic features (including semantic
role labels).
While all teams in the shared task use the NU-
CLE corpus, they are also allowed to use addi-
tional external resources (both corpora and tools)
so long as they are publicly available and not pro-
prietary. The external resources used by the teams
are also listed in Table 6.
6 Results
All submitted system output was evaluated using
the M2 scorer, based on the error annotations pro-
vided by our annotator. The recall (R), precision
(P ), and F1 measure of all teams are shown in Ta-
ble 7. The performance of the teams varies greatly,
6
Team ID Affiliation
CAMB University of Cambridge
HIT Harbin Institute of Technology
IITB Indian Institute of Technology, Bombay
KOR Korea University
NARA Nara Institute of Science and Technology
NTHU National Tsing Hua University
SAAR Saarland University
SJT1 Shanghai Jiao Tong University (Team #1)
SJT2 Shanghai Jiao Tong University (Team #2)
STAN Stanford University
STEL Stellenbosch University
SZEG University of Szeged
TILB Tilburg University
TOR University of Toronto
UAB Universitat Auto`noma de Barcelona
UIUC University of Illinois at Urbana-Champaign
UMC University of Macau
Table 5: The list of 17 participating teams.
Rank Team R P F1
1 UIUC 23.49 46.45 31.20
2 NTHU 26.35 23.80 25.01
3 HIT 16.56 35.65 22.61
4 NARA 18.62 27.39 22.17
5 UMC 17.53 28.49 21.70
6 STEL 13.33 27.00 17.85
7 SJT1 10.96 40.18 17.22
8 CAMB 10.10 39.15 16.06
9 IITB 4.99 28.18 8.48
10 STAN 4.69 25.50 7.92
11 TOR 4.81 17.67 7.56
12 KOR 3.71 43.88 6.85
13 TILB 7.24 6.25 6.71
14 SZEG 3.16 5.52 4.02
15 UAB 1.22 12.42 2.22
16 SAAR 1.10 27.69 2.11
17 SJT2 0.24 13.33 0.48
Table 7: Scores (in %) without alternative an-
swers.
from barely half a per cent to 31.20% for the top
team.
The nature of grammatical error correction is
such that multiple, different corrections are of-
ten acceptable. In order to allow the participating
teams to raise their disagreement with the original
gold-standard annotations provided by the anno-
tator, and not understate the performance of the
teams, we allow the teams to submit their pro-
posed alternative answers. This was also the prac-
tice adopted in HOO 2011 and HOO 2012. Specif-
ically, after the teams submitted their system out-
put and the error annotations on the test essays
were released, we allowed the teams to propose al-
ternative answers (gold-standard edits), to be sub-
mitted within four days after the initial error an-
notations were released. The same annotator who
provided the error annotations on the test essays
also judged the alternative answers proposed by
the teams, to ensure consistency. In all, five teams
(NTHU, STEL, TOR, UIUC, UMC) submitted al-
ternative answers.
The same submitted system output was then
evaluated using the extended M2 scorer, with the
original annotations augmented with the alterna-
tive answers. Table 8 shows the recall (R), preci-
sion (P ), and F1 measure of all teams under this
new evaluation setting.
The F1 measure of every team improves when
7
Te
am
E
rr
or
A
pp
ro
ac
h
D
es
cr
ip
ti
on
of
A
pp
ro
ac
h
L
in
gu
is
ti
c
F
ea
tu
re
s
E
xt
er
na
lR
es
ou
rc
es
C
A
M
B
A
N
P
S
V
T
fa
ct
or
ed
ph
ra
se
-b
as
ed
tr
an
sl
at
io
n
m
od
el
w
it
h
IR
S
T
la
ng
ua
ge
m
od
el
le
xi
ca
l,
P
O
S
C
am
br
id
ge
L
ea
rn
er
C
or
pu
s
H
IT
A
N
P
M
m
ax
im
um
en
tr
op
y
w
it
h
co
nfi
de
nc
e
tu
ni
ng
,
an
d
ge
ne
ti
c
al
go
-
ri
th
m
fo
r
fe
at
ur
e
se
le
ct
io
n
le
xi
ca
l,
P
O
S
,c
on
st
it
ue
nc
y
pa
rs
e,
de
pe
nd
en
cy
pa
rs
e,
se
m
an
ti
c
W
or
dN
et
,L
on
gm
an
di
ct
io
na
ry
S
V
R
ru
le
-b
as
ed
P
O
S
,d
ep
en
de
nc
y
pa
rs
e,
se
m
an
ti
c
II
T
B
A
N
M
m
ax
im
um
en
tr
op
y
le
xi
ca
l,
P
O
S
,n
ou
n
pr
op
er
ti
es
W
ik
ti
on
ar
y
S
R
ru
le
-b
as
ed
P
O
S
,c
on
st
it
ue
nc
y
pa
rs
e,
de
pe
nd
en
cy
pa
rs
e
K
O
R
A
N
P
M
m
ax
im
um
en
tr
op
y
le
xi
ca
l,
P
O
S
,h
ea
d-
m
od
ifi
er
,d
ep
en
de
nc
y
pa
rs
e
(n
on
e)
N
A
R
A
A
P
T
ph
ra
se
-b
as
ed
st
at
is
ti
ca
lm
ac
hi
ne
tr
an
sl
at
io
n
le
xi
ca
l
L
an
g-
8
N
M
ad
ap
tiv
e
re
gu
la
ri
za
ti
on
of
w
ei
gh
tv
ec
to
rs
le
xi
ca
l,
le
m
m
a,
co
ns
ti
tu
en
cy
pa
rs
e
G
ig
aw
or
d
S
V
L
tr
ee
le
t(
tr
ee
-b
as
ed
)
la
ng
ua
ge
m
od
el
le
xi
ca
l,
P
O
S
,c
on
st
it
ue
nc
y
pa
rs
e
P
en
n
T
re
eb
an
k,
G
ig
aw
or
d
N
T
H
U
A
N
P
V
L
n-
gr
am
-b
as
ed
an
d
de
pe
nd
en
cy
-b
as
ed
la
ng
ua
ge
m
od
el
le
xi
ca
l,
P
O
S
,c
on
st
it
ue
nc
y
pa
rs
e,
de
pe
nd
en
cy
pa
rs
e
G
oo
gl
e
W
eb
-1
T
S
A
A
R
A
M
m
ul
ti
-c
la
ss
S
V
M
an
d
na
iv
e
B
ay
es
le
xi
ca
l,
P
O
S
,c
on
st
it
ue
nc
y
pa
rs
e
C
M
U
P
ro
no
un
ci
ng
D
ic
ti
on
ar
y
S
R
ru
le
-b
as
ed
P
O
S
,d
ep
en
de
nc
y
pa
rs
e
S
JT
1
A
N
P
S
V
M
m
ax
im
um
en
tr
op
y
(w
it
h
L
M
po
st
-fi
lt
er
in
g)
le
xi
ca
l,
le
m
m
a,
P
O
S
,c
on
st
it
ue
nc
y
pa
rs
e,
de
pe
nd
en
cy
pa
rs
e
E
ur
op
ar
l
S
TA
N
A
N
P
S
V
H
E
ng
li
sh
R
es
ou
rc
e
G
ra
m
m
ar
(E
R
G
),
he
ad
-d
ri
ve
n
ph
ra
se
st
ru
c-
tu
re
,e
xt
en
de
d
w
it
h
ha
nd
-c
od
ed
m
al
-r
ul
es
le
xi
ca
l,
P
O
S
,c
on
st
it
ue
nc
y
pa
rs
e,
se
m
an
ti
c
E
ng
li
sh
R
es
ou
rc
e
G
ra
m
m
ar
S
T
E
L
A
N
P
S
V
T
tr
ee
-t
o-
st
ri
ng
w
it
h
G
H
K
M
tr
an
sd
uc
er
co
ns
ti
tu
en
cy
pa
rs
e
W
ik
ip
ed
ia
,W
or
dN
et
S
Z
E
G
A
N
M
m
ax
im
um
en
tr
op
y
L
F
G
,l
ex
ic
al
,c
on
st
it
ue
nc
y
pa
rs
e,
de
pe
nd
en
cy
pa
rs
e
(n
on
e)
T
IL
B
A
N
P
S
V
M
bi
na
ry
an
d
m
ul
ti
-c
la
ss
IG
T
re
e
le
xi
ca
l,
le
m
m
a,
P
O
S
G
oo
gl
e
W
eb
-1
T,
G
ig
aw
or
d
T
O
R
A
N
P
S
V
T
no
is
y
ch
an
ne
lm
od
el
in
vo
lv
in
g
tr
an
sf
or
m
at
io
n
of
si
ng
le
w
or
ds
le
xi
ca
l,
P
O
S
W
ik
ip
ed
ia
U
A
B
A
N
P
S
V
R
ru
le
-b
as
ed
le
xi
ca
l,
de
pe
nd
en
cy
pa
rs
e
To
p
25
0
un
co
un
ta
bl
e
no
un
s,
F
re
eL
in
g
m
or
ph
ol
og
ic
al
di
ct
io
na
ry
U
IU
C
A
N
P
S
V
M
A
:m
ul
ti
-c
la
ss
av
er
ag
ed
pe
rc
ep
tr
on
;o
th
er
s:
na
iv
e
B
ay
es
le
xi
ca
l,
P
O
S
,s
ha
ll
ow
pa
rs
e
G
oo
gl
e
W
eb
-1
T,
G
ig
aw
or
d
U
M
C
A
N
P
S
V
H
pi
pe
li
ne
:
ru
le
-b
as
ed
fi
lt
er
?
se
m
i-
su
pe
rv
is
ed
m
ul
ti
-c
la
ss
m
ax
im
um
en
tr
op
y
cl
as
si
fi
er
?
L
M
co
nfi
de
nc
e
sc
or
er
le
xi
ca
l,
P
O
S
,d
ep
en
de
nc
y
pa
rs
e
N
ew
s
co
rp
us
,J
M
yS
pe
ll
di
ct
io
na
ry
,G
oo
gl
e
W
eb
-1
T,
P
en
n
T
re
eb
an
k
Ta
bl
e
6:
P
ro
fi
le
of
th
e
pa
rt
ic
ip
at
in
g
te
am
s.
T
he
E
rr
or
co
lu
m
n
sh
ow
s
th
e
er
ro
r
ty
pe
,
w
he
re
ea
ch
le
tt
er
de
no
te
s
th
e
er
ro
r
ty
pe
be
gi
nn
in
g
w
it
h
th
at
in
it
ia
l
le
tt
er
.
T
he
A
pp
ro
ac
h
co
lu
m
n
sh
ow
s
th
e
ap
pr
oa
ch
ad
op
te
d
by
ea
ch
te
am
,s
om
et
im
es
br
ok
en
do
w
n
ac
co
rd
in
g
to
th
e
er
ro
r
ty
pe
:
H
de
no
te
s
a
hy
br
id
cl
as
si
fi
er
ap
pr
oa
ch
,L
de
no
te
s
a
la
ng
ua
ge
m
od
el
in
g-
ba
se
d
ap
pr
oa
ch
,M
de
no
te
s
a
m
ac
hi
ne
le
ar
ni
ng
-b
as
ed
cl
as
si
fi
er
ap
pr
oa
ch
,R
de
no
te
s
a
ru
le
-b
as
ed
cl
as
si
fi
er
(n
on
-m
ac
hi
ne
le
ar
ni
ng
)
ap
pr
oa
ch
,a
nd
T
de
no
te
s
a
m
ac
hi
ne
tr
an
sl
at
io
n
ap
pr
oa
ch
8
evaluated with alternative answers. Not surpris-
ingly, the teams which submitted alternative an-
swers tend to show the greatest improvements in
their F1 measure. Overall, the UIUC team (Ro-
zovskaya et al, 2013) achieves the best F1 mea-
sure, with a clear lead over the other teams in the
shared task, under both evaluation settings (with-
out and with alternative answers).
For future research which uses the test data of
the CoNLL-2013 shared task, we recommend that
evaluation be carried out in the setting that does
not use alternative answers, to ensure a fairer eval-
uation. This is because the scores of the teams
which submitted alternative answers tend to be
higher in a biased way when evaluated with alter-
native answers.
Rank Team R P F1
1 UIUC 31.87 62.19 42.14
2 NTHU 34.62 30.57 32.46
3 UMC 23.66 37.12 28.90
4 NARA 24.05 33.92 28.14
5 HIT 20.29 41.75 27.31
6 STEL 18.91 37.12 25.05
7 CAMB 14.19 52.11 22.30
8 SJT1 13.67 47.77 21.25
9 TOR 8.77 30.67 13.64
10 IITB 6.55 34.93 11.03
11 STAN 5.86 29.93 9.81
12 KOR 4.78 53.24 8.77
13 TILB 9.29 7.60 8.36
14 SZEG 4.07 6.67 5.06
15 UAB 1.81 17.39 3.28
16 SAAR 1.68 40.00 3.23
17 SJT2 0.33 16.67 0.64
Table 8: Scores (in %) with alternative answers.
We are also interested in the analysis of scores
of each of the five error types. To compute the
recall of an error type, we need to know the er-
ror type of each gold-standard edit, which is pro-
vided by the annotator. To compute the precision
of each error type, we need to know the error type
of each system edit, which however is not avail-
able since the submitted system output only con-
tains the corrected sentences with no indication of
the error type of the system edits.
In order to determine the error type of system
edits, we first perform POS tagging on the submit-
ted system output using the Stanford parser (Klein
andManning, 2003). We also make use of the POS
tags assigned in the preprocessed form of the test
essays. We then assign an error type to a system
edit based on the automatically determined POS
tags, as follows:
? ArtOrDet: The system edit involves a change
(insertion, deletion, or substitution) of words
tagged as article/determiner, i.e., DT or PDT.
? Prep: The system edit involves a change of
words tagged as preposition, i.e., IN or TO.
? Nn: The system edit involves a change of
words such that a word in the source string
is a singular noun (tagged as NN or NNP)
and a word in the replacement string is a plu-
ral noun (tagged as NNS or NNPS), or vice
versa. Since a word tagged as JJ (adjective)
can serve as a noun, a system edit that in-
volves a change of POS tags from JJ to one of
{NN, NNP, NNS, NNPS} or vice versa also
qualifies.
? Vform/SVA: The system edit involves a
change of words tagged as one of the verb
POS tags, i.e., VB, VBD, VBG, VBN, VBP,
and VBZ.
The verb form and subject-verb agreement error
types are grouped together into one category, since
it is difficult to automatically distinguish the two in
a reliable way.
The scores when distinguished by error type are
shown in Tables 9 and 10. Based on the F1 mea-
sure of each error type, the noun number error type
gives the highest scores, and preposition errors re-
main the most challenging error type to correct.
7 Conclusions
The CoNLL-2013 shared task saw the participa-
tion of 17 teams worldwide to evaluate their gram-
matical error correction systems on a common test
set, using a common evaluation metric and scorer.
The five error types included in the shared task
account for at least one-third to close to one-half
of all errors in English learners? essays. The best
system in the shared task achieves an F1 score of
42%, when it is scored with multiple acceptable
answers. There is still much room for improve-
ment, both in the accuracy of grammatical error
correction systems, and in the coverage of systems
to deal with a more comprehensive set of error
9
Te
am
A
rt
O
rD
et
P
re
p
N
n
V
fo
rm
/S
V
A
R
P
F
1
R
P
F
1
R
P
F
1
R
P
F
1
C
A
M
B
15
.0
7
38
.6
6
21
.6
9
3.
54
40
.7
4
6.
51
7.
58
55
.5
6
13
.3
3
8.
54
31
.8
2
13
.4
6
H
IT
24
.2
0
42
.8
2
30
.9
3
2.
89
28
.1
2
5.
25
17
.1
7
29
.6
9
21
.7
6
11
.3
8
26
.4
2
15
.9
1
II
T
B
1.
30
21
.4
3
2.
46
(n
ot
do
ne
)
9.
85
28
.6
8
14
.6
6
13
.8
2
30
.0
9
18
.9
4
K
O
R
4.
78
53
.2
3
8.
78
0.
32
4.
76
0.
60
6.
82
49
.0
9
11
.9
7
(n
ot
do
ne
)
N
A
R
A
20
.4
3
34
.0
6
25
.5
4
12
.5
4
29
.1
0
17
.5
3
16
.4
1
48
.8
7
24
.5
7
24
.8
0
14
.8
1
18
.5
4
N
T
H
U
21
.0
1
35
.8
0
26
.4
8
12
.8
6
12
.0
1
12
.4
2
45
.9
6
40
.9
0
43
.2
8
26
.8
3
12
.2
2
16
.7
9
S
A
A
R
0.
72
62
.5
0
1.
43
(n
ot
do
ne
)
(n
ot
do
ne
)
5.
28
23
.2
1
8.
61
S
JT
1
16
.8
1
47
.1
5
24
.7
9
1.
29
12
.5
0
2.
33
13
.6
4
42
.1
9
20
.6
1
2.
44
14
.6
3
4.
18
S
JT
2
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
1.
01
13
.3
3
1.
88
0.
00
0.
00
0.
00
S
TA
N
3.
91
20
.4
5
6.
57
0.
32
20
.0
0
0.
63
6.
06
29
.6
3
10
.0
6
10
.1
6
32
.0
5
15
.4
3
S
T
E
L
12
.6
1
27
.7
1
17
.3
3
9.
32
25
.6
6
13
.6
8
18
.1
8
46
.7
5
26
.1
8
12
.6
0
17
.6
1
14
.6
9
S
Z
E
G
1.
16
1.
70
1.
38
(n
ot
do
ne
)
11
.1
1
13
.6
2
12
.2
4
(n
ot
do
ne
)
T
IL
B
4.
49
4.
49
4.
49
10
.6
1
5.
07
6.
86
7.
07
21
.2
1
10
.6
1
10
.9
8
9.
57
10
.2
3
T
O
R
8.
55
25
.5
4
12
.8
1
2.
25
5.
38
3.
17
1.
77
31
.8
2
3.
35
2.
44
12
.2
4
4.
07
U
A
B
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
8.
13
12
.4
2
9.
83
U
IU
C
25
.6
5
47
.8
4
33
.4
0
4.
18
26
.5
3
7.
22
38
.3
8
52
.2
3
44
.2
5
17
.8
9
38
.9
4
24
.5
1
U
M
C
21
.0
1
30
.2
7
24
.8
1
1.
93
35
.2
9
3.
66
23
.2
3
27
.9
6
25
.3
8
18
.2
9
28
.8
5
22
.3
9
Ta
bl
e
9:
S
co
re
s
(i
n
%
)
w
it
ho
ut
al
te
rn
at
iv
e
an
sw
er
s,
di
st
in
gu
is
he
d
by
er
ro
r
ty
pe
.
If
a
te
am
in
di
ca
te
s
th
at
it
s
sy
st
em
do
es
no
th
an
dl
e
a
pa
rt
ic
ul
ar
er
ro
r
ty
pe
,i
ts
en
tr
y
fo
r
th
at
er
ro
r
ty
pe
is
m
ar
ke
d
as
?(
no
td
on
e)
?.
10
Te
am
A
rt
O
rD
et
P
re
p
N
n
V
fo
rm
/S
V
A
R
P
F
1
R
P
F
1
R
P
F
1
R
P
F
1
C
A
M
B
19
.6
2
49
.8
1
28
.1
5
5.
04
50
.0
0
9.
15
9.
50
69
.0
9
16
.7
0
16
.5
2
54
.4
1
25
.3
4
H
IT
27
.4
1
47
.4
4
34
.7
4
4.
58
37
.5
0
8.
16
19
.9
5
35
.3
7
25
.5
1
17
.9
0
38
.3
2
24
.4
0
II
T
B
1.
79
28
.5
7
3.
37
(n
ot
do
ne
)
11
.9
1
35
.2
9
17
.8
1
18
.6
7
36
.8
4
24
.7
8
K
O
R
5.
95
64
.5
2
10
.9
0
1.
53
19
.0
5
2.
83
7.
52
54
.5
5
13
.2
2
(n
ot
do
ne
)
N
A
R
A
25
.7
9
43
.3
4
32
.3
4
17
.6
0
34
.5
6
23
.3
3
19
.1
5
57
.0
4
28
.6
8
34
.6
2
19
.1
0
24
.6
2
N
T
H
U
25
.3
0
42
.5
4
31
.7
3
20
.4
5
16
.1
7
18
.0
6
52
.3
5
50
.8
7
51
.6
0
43
.0
3
19
.2
2
26
.5
7
S
A
A
R
1.
04
87
.5
0
2.
06
(n
ot
do
ne
)
(n
ot
do
ne
)
8.
60
33
.9
3
13
.7
2
S
JT
1
19
.6
5
54
.0
7
28
.8
2
1.
92
15
.6
2
3.
42
16
.4
6
52
.3
4
25
.0
5
4.
05
21
.9
5
6.
84
S
JT
2
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
1.
26
16
.6
7
2.
35
0.
00
0.
00
0.
00
S
TA
N
4.
91
24
.8
1
8.
20
0.
38
20
.0
0
0.
75
6.
78
33
.3
3
11
.2
7
13
.5
1
37
.9
7
19
.9
3
S
T
E
L
16
.2
3
35
.6
9
22
.3
1
12
.8
3
29
.8
2
17
.9
4
26
.0
5
70
.0
0
37
.9
7
20
.5
2
26
.5
5
23
.1
5
S
Z
E
G
1.
50
2.
13
1.
76
(n
ot
do
ne
)
12
.8
7
15
.9
5
14
.2
5
(n
ot
do
ne
)
T
IL
B
5.
78
5.
64
5.
71
13
.9
1
5.
68
8.
07
8.
25
24
.2
6
12
.3
1
16
.3
6
12
.7
7
14
.3
4
T
O
R
13
.1
0
39
.1
3
19
.6
3
5.
97
12
.3
1
8.
04
3.
52
63
.6
4
6.
67
8.
14
35
.2
9
13
.2
4
U
A
B
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
12
.6
1
17
.3
9
14
.6
2
U
IU
C
31
.9
9
59
.8
4
41
.6
9
8.
81
46
.9
4
14
.8
4
46
.8
8
70
.0
0
56
.1
5
28
.5
7
60
.7
1
38
.8
6
U
M
C
25
.8
8
36
.7
4
30
.3
7
3.
47
56
.2
5
6.
55
30
.6
1
38
.6
4
34
.1
6
26
.8
1
40
.1
3
32
.1
4
Ta
bl
e
10
:
S
co
re
s
(i
n
%
)
w
it
h
al
te
rn
at
iv
e
an
sw
er
s,
di
st
in
gu
is
he
d
by
er
ro
r
ty
pe
.
If
a
te
am
in
di
ca
te
s
th
at
it
s
sy
st
em
do
es
no
th
an
dl
e
a
pa
rt
ic
ul
ar
er
ro
r
ty
pe
,i
ts
en
tr
y
fo
r
th
at
er
ro
r
ty
pe
is
m
ar
ke
d
as
?(
no
td
on
e)
?.
11
types. The evaluation data sets and scorer used
in our shared task serve as a benchmark for future
research on grammatical error correction4.
Acknowledgments
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
References
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media.
Daniel Dahlmeier and Hwee Tou Ng. 2011a. Cor-
recting semantic collocation errors with L1-induced
paraphrases. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 107?117.
Daniel Dahlmeier and Hwee Tou Ng. 2011b. Gram-
matical error correction with alternating structure
optimization. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics, pages 915?923.
Daniel Dahlmeier and Hwee Tou Ng. 2012a. A beam-
search decoder for grammatical error correction. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
568?578.
Daniel Dahlmeier and Hwee Tou Ng. 2012b. Better
evaluation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
568?572.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
English: The NUS Corpus of Learner English. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 22?31.
Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 pilot shared task. In Proceed-
ings of the 13th EuropeanWorkshop on Natural Lan-
guage Generation, pages 242?249.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A report on the preposition and
determiner error correction shared task. In Proceed-
ings of the 7th Workshop on the Innovative Use of
NLP for Building Educational Applications, pages
54?62.
4http://www.comp.nus.edu.sg/?nlp/conll13st.html
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the Fifth Conference on Language
Resources and Evaluation, pages 449?454.
Michael Gamon. 2010. Using mostly native data to
correct errors in learners? writing: A meta-classifier
approach. In Proceedings of the Annual Meeting of
the North American Chapter of the Association for
Computational Linguistics, pages 163?171.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineer-
ing, 12(2):115?129.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423?430.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Morgan &
Claypool Publishers.
Diane Nicholls. 2003. The Cambridge Learner Cor-
pus: Error coding and analysis for lexicography and
ELT. In Proceedings of the Corpus Linguistics 2003
Conference, pages 572?581.
Alla Rozovskaya and Dan Roth. 2010. Generating
confusion sets for context-sensitive error correction.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
961?970.
Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,
and Dan Roth. 2013. The University of Illinois
system in the CoNLL-2013 shared task. In Pro-
ceedings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared Task.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selection
and error detection. In Proceedings of the ACL 2010
Conference Short Papers, pages 353?358.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading ESOL texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics, pages 180?189.
Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuzawa,
Keisuke Sakaguchi, Tomoya Mizumoto, Yuta
Hayashibe, Mamoru Komachi, and Yuji Matsumoto.
2013. NAIST at 2013 CoNLL grammatical error
correction shared task. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task.
12
