Using existing systems to supplement small amounts of
annotated grammatical relations training data
?
Alexander Yeh
Mitre Corp.
202 Burlington Rd.
Bedford, MA 01730
USA
asy@mitre.org
Abstract
Grammatical relationships (GRs)
form an important level of natu-
ral language processing, but dier-
ent sets of GRs are useful for dier-
ent purposes. Therefore, one may of-
ten only have time to obtain a small
training corpus with the desired GR
annotations. To boost the perfor-
mance from using such a small train-
ing corpus on a transformation rule
learner, we use existing systems that
nd related types of annotations.
1 Introduction
Grammatical relationships (GRs), which in-
clude arguments (e.g., subject and object) and
modiers, form an important level of natural
language processing. Examples of GRs in the
sentence
Today, my dog pushed the ball on the oor.
are pushed having the subject my dog, the
object the ball and the time modier To-
day, and the ball having the location modier
on (the oor). The resulting annotation is
my dog ?subj? pushed
on ?mod-loc? the ball
?
This paper reports on work performed at the
MITRE Corporation under the support of the MITRE
Sponsored Research Program. Marc Vilain provided
the motivation to nd GRs. Warren Grei suggested
using randomization-type techniques to determine sta-
tistical signicance. Sabine Buchholz and John Car-
roll ran their GR nding systems over our data for the
experiments. Jun Wu provided some helpful explana-
tions. Christine Doran and John Henderson provided
helpful editing. Three anonymous reviewers provided
helpful suggestions.
etc. GRs are the objects of study in rela-
tional grammar (Perlmutter, 1983). In the
SPARKLE project (Carroll et al, 1997), GRs
form the top layer of a three layer syntax
scheme. Many systems (e.g., the KERNEL
system (Palmer et al, 1993)) use GRs as an
intermediate form when determining the se-
mantics of syntactically parsed text. GRs are
often stored in structures similar to the F-
structures of lexical-functional grammar (Ka-
plan, 1994).
A complication is that dierent sets of GRs
are useful for dierent purposes. For exam-
ple, Ferro et al (1999) is interested in seman-
tic interpretation, and needs to dierentiate
between time, location and other modiers.
The SPARKLE project (Carroll et al, 1997),
on the other hand, does not dierentiate be-
tween these types of modiers. As has been
mentioned by John Carroll (personal commu-
nication), combining modier types together
is ne for information retrieval. Also, having
less dierentiation of the modiers can make
it easier to nd them (Ferro et al, 1999).
Furthermore, unless the desired set of GRs
matches the set aleady annotated in some
large training corpus,
1
one will have to either
manually write rules to nd the GRs, as done
in A?t-Mokhtar and Chanod (1997), or anno-
tate a new training corpus for the desired set.
Manually writing rules is expensive, as is an-
notating a large corpus.
Often, one may only have the resources to
produce a small annotated training set, and
many of the less common features of the set's
1
One example is a memory-based GR nder (Buch-
holz et al, 1999) that uses the GRs annotated in the
Penn Treebank (Marcus et al, 1993).
domain may not appear at all in that set.
In contrast are existing systems that perform
well (probably due to a large annotated train-
ing set or a set of carefully hand-crafted rules)
on related (but dierent) annotation stan-
dards. Such systems will cover many more
domain features, but because the annotation
standards are slightly dierent, some of those
features will be annotated in a dierent way
than in the small training and test set.
A way to try to combine the dierent advan-
tages of these small training data sets and ex-
isting systems which produce related annota-
tions is to use a sequence of two systems. We
rst use an existing annotation system which
can handle many of the less common features,
i.e., those which do not appear in the small
training set. We then train a second system
with that same small training set to take the
output of the rst system and correct for the
dierences in annotations. This approach was
used by Palmer (1997) for word segmentation.
Hwa (1999) describes a somewhat similar ap-
proach for nding parse brackets which com-
bines a fully annotated related training data
set and a large but incompletely annotated -
nal training data set. Both these works deal
with just one (word boundary) or two (start
and end parse bracket) annotation label types
and the same label types are used in both the
existing annotation system/training set and
the nal (small) training set. In compari-
son, our work handles many annotation la-
bel types, and the translation from the types
used in the existing annotation system to the
types in the small training set tends to be both
more complicated and most easily determined
by empirical means. Also, the type of baseline
score being improved upon is dierent. Our
work adds an existing system to improve the
rules learned, while Palmer (1997) adds rules
to improve an existing system's performance.
We use this related system/small training
set combination to improve the performance
of the transformation-based error-driven
learner described in Ferro et al (1999). So
far, this learner has started with a blank
initial labeling of the GRs. This paper
describes experiments where we replace this
blank initial labeling with the output from
an existing GR nder that is good at a
somewhat dierent set of GR annotations.
With each of the two existing GR nders that
we use, we obtained improved results, with
the improvement being more noticeable when
the training set is smaller.
We also nd that the existing GR nders
are quite uneven on how they improve the re-
sults. They each tend to concentrate on im-
proving the recovery of a few kinds of rela-
tions, leaving most of the other kinds alone.
We use this tendency to further boost the
learner's performance by using a merger of
these existing GR nders' output as the initial
labeling.
2 The Experiment
We now improve the performance of the
Ferro et al (1999) transformation rule
learner on a small annotated training set by
using an existing system to provide initial
GR annotations. This experiment is repeated
on two dierent existing systems, which
are reported in Buchholz et al (1999) and
Carroll et al (1999), respectively.
Both of these systems nd a somewhat
dierent set of GR annotations than the
one learned by the Ferro et al (1999) sys-
tem. For example, the Buchholz et al (1999)
system ignores verb complements of verbs
and is designed to look for relationships
to verbs and not GRs that exist between
nouns, etc. This system also handles
relative clauses dierently. For example,
in Miller, who organized ..., this system is
trained to indicate that who is the subject
of organized, while the Ferro et al (1999)
system is trained to indicate that Miller
is the subject of organized. As for the
Carroll et al (1999) system, among other
things, it does not distinguish between sub-
types of modiers such as time, location and
possessive. Also, both systems handle copu-
las (usually using the verb to be) dierently
than in Ferro et al (1999).
2.1 Experiment Set-Up
As described in Ferro et al (1999), the trans-
formation rule learner starts with a p-o-s
tagged corpus that has been chunked into
noun chunks, etc. The starting state also in-
cludes imperfect estimates of pp-attachments
and a blank set of initial GR annotations.
In these experiments, this blank initial set
is changed to be a translated version of the
annotations produced by an existing system.
This is how the existing system transmits
what it found to the rule learner. The set-
up for this experiment is shown in gure 1.
The four components with + signs are taken
out when one wants the transformation rule
learner to start with a blank set of initial GR
annotations.
The two arcs in that gure with a * indicate
where the translations occur. These transla-
tions of the annotations produced by the ex-
isting system are basically just an attempt to
map each type of annotation that it produces
to the most likely type of corresponding an-
notation used in the Ferro et al (1999) sys-
tem. For example, in our experiments, the
Buchholz et al (1999) system uses the anno-
tation np-sbj to indicate a subject, while the
Ferro et al (1999) system uses the annota-
tion subj. We create the mapping by ex-
amining the training set to be given to the
Ferro et al (1999) system. For each type of
relation e
i
output by the existing system when
given the training set text, we look at what
relation types (which t
k
's) co-occur with e
i
in
the training set. We look at the t
k
's with the
highest number of co-occurrences with that
e
i
. If that t
k
is unique (no ties for the highest
number of co-occurrences) and translating e
i
to that t
k
generates at least as many correct
annotations in the training set as false alarms,
then make that translation. Otherwise, trans-
late e
i
to no relation. This latter translation
is not uncommon. For example, in one run of
our experiments, 9% of the relation instances
in the training set were so translated, in an-
other run, 46% of the instances were so trans-
lated.
Some relations in the Carroll et al (1999)
system are between three or four elements.
These relations are each rst translated into
a set of two element sub-relations before the
examination process above is performed.
Even before applying the rules, the trans-
lations nd many of the desired annotations.
However, the rules can considerably improve
what is found. For example, in two of our
early experiments, the translations by them-
selves produced F-scores (explained below)
of about 40% to 50%. After the learned
rules were applied, those F-scores increased
to about 70%.
An alternative to performing translations is
to use the untranslated initial annotations as
an additional type of input to the rule sys-
tem. This alternative, which we have yet
to try, has the advantage of tting into the
transformation-based error-driven paradigm
(Brill and Resnik, 1994) more cleanly than
having a translation stage. However, this ad-
ditional type of input will also further slow-
down an already slow rule-learning module.
2.2 Overall Results
For our experiment, we use the same
1151 word (748 GR) test set used in
Ferro et al (1999), but for a training set, we
use only a subset of the 3299 word training set
used in Ferro et al (1999). This subset con-
tains 1391 (71%) of the 1963 GR instances in
the original training set. The overall results
for the test set are
Smaller Training Set, Overall Results
R P F ER
IaC 478 (63.9%) 77.2% 69.9% 7.7%
IaB 466 (62.3%) 78.1% 69.3% 5.8%
NI 448 (59.9%) 77.1% 67.4%
where row IaB is the result of using the rules
learned when the Buchholz et al (1999) sys-
tem's translated GR annotations are used
as the Initial Annotations, row IaC is the
similar result with the Carroll et al (1999)
system, and row NI is the result of using
the rules learned when No Initial GR an-
notations are used (the rule learner as run
in Ferro et al (1999)). R(ecall) is the num-
ber (and percentage) of the keys that are
recalled. P(recision) is the number of cor-
?
?
?
existing system
+

?
?
?
existing system
+
test set
?
?
?
?
??
??
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
??
?
?
?
?

?
?
?
??
?
?
?
?
?
?
??
?
small training set
rule learner
key GR annotations for small training set
*
*
rules
+
GR annotations
initial test
+
initial training
GR annotations
nal test
GR annotations
rule interpreter
Figure 1: Set-up to use an existing system to improve performance
rectly recalled keys divided by the num-
ber of GRs the system claims to exist.
F(-score) is the harmonic mean of recall (r)
and precision (p) percentages. It equals
2pr/(p + r). ER stands for Error Reduc-
tion. It indicates how much adding the ini-
tial annotations reduced the missing F-score,
where the missing F-score is 100%?F. ER=
100%?(F
IA
?F
NI
)/(100%?F
NI
), where F
NI
is the F-score for the NI row, and F
IA
is the
F-score for using the Initial Annotations of
interest. Here, the dierences in recall and F-
score between NI and either IaB or IaC (but
not between IaB and IaC) are statistically sig-
nicant. The dierences in precision is not.
2
In these results, most of the modest F-score
gain came from increasing recall.
One may note that the error reductions here
are smaller than Palmer (1997)'s error reduc-
tions. Besides being for dierent tasks (word
segmentation versus GRs), the reductions are
also computed using a dierent type of base-
line. In Palmer (1997), the baseline is how
well an existing system performs before the
rules are run. In this paper, the baseline is
the performance of the rules learned without
2
When comparing dierences in this paper, the
statistical signicance of the higher score being bet-
ter than the lower score is tested with a one-sided
test. Dierences deemed statistically signicant are
signicant at the 5% level. Dierences deemed non-
statistically signicant are not signicant at the 10%
level. For recall, we use a sign test for matched-pairs
(Harnett, 1982, Sec. 15.5). For precision and F-score,
a matched-pairs randomization test (Cohen, 1995,
Sec. 5.3) is used.
rst using an existing system. If we were to
use the same baseline as Palmer (1997), our
baseline would be an F of 37.5% for IaB and
52.6% for IaC. This would result in a much
higher ER of 51% and 36%, respectively.
We now repeat our experiment with the
full 1963 GR instance training set. These re-
sults indicate that as a small training set gets
larger, the overall results get better and the
initial annotations help less in improving the
overall results. So the initial annotations are
more helpful with smaller training sets. The
overall results on the test set are
Full Training Set, Overall Results
R P F ER
IaC 487 (65.1%) 79.7% 71.7% 6.3%
IaB 486 (65.0%) 76.5% 70.3% 1.7%
NI 476 (63.6%) 77.3% 69.8%
The dierences in recall, etc. between IaB and
NI are now small enough to be not statisti-
cally signicant. The dierences between IaC
and NI are statistically signicant,
3
but the
dierence in both the absolute F-score (1.9%
versus 2.5% with the smaller training set) and
ER (6.3% versus 7.7%) has decreased.
2.3 Results by Relation
The overall result of using an existing system
is a modest increase in F-score. However, this
increase is quite unevenly distributed, with a
3
The recall dierence is semi-signicant, being sig-
nicant at the 10% level.
few relation(s) having a large increase, and
most relations not having much of a change.
Dierent existing systems seem to have dier-
ent relations where most of the increase oc-
curs.
As an example, take the results of using
the Buchholz et al (1999) system on the 1391
GR instance training set. Many GRs, like pos-
sessive modier, are not aected by the added
initial annotations. Some GRs, like location
modier, do slightly better (as measured by
the F-score) with the added initial annota-
tions, but some, like subject, do better with-
out. With GRs like subject, some dierences
between the initial and desired annotations
may be too subtle for the Ferro et al (1999)
system to adjust for. Or those dierences may
be just due to chance, as the result dierences
in those GRs are not statistically signicant.
The GRs with statistically signicant result
dierences are the time and other
4
modiers,
where adding the initial annotations helps.
The time modier
5
results are quite dierent:
Smaller Training Set, Time Modiers
R P F ER
IaB 29 (64.4%) 80.6% 71.6% 53%
NI 14 (31.1%) 56.0% 40.0%
The dierence in the number recalled (15) for
this GR accounts for nearly the entire dier-
ence in the overall recall results (18). The re-
call, precision and F-score dierences are all
statistically signicant.
Similarly, when using the
Carroll et al (1999) system on this training
set, most GRs are not aected, while others
do slightly better. The only GR with a sta-
tistically signicant result dierence is object,
where again adding the initial annotations
helps:
Smaller Training Set, Object Relations
R P F ER
IaC 198 (79.5%) 79.5% 79.5% 17%
NI 179 (71.9%) 78.9% 75.2%
The dierence in the number recalled (19) for
this GR again accounts for most of the dif-
4
Modiers that do not fall into any of the subtypes
used, such as time, location, possessive, etc. Examples
of unused subtypes are purpose and modality.
5
There are 45 instances in the test set key.
ference in the overall recall results (30). The
recall and F-score dierences are statistically
signicant. The precision dierence is not.
As one changes from the smaller 1391 GR
instance training set to the larger 1963 GR
instance training set, these F-score improve-
ments become smaller. When using the
Buchholz et al (1999) system, the improve-
ment in the other modier is now no longer
statistically signicant. However, the time
modier F-score improvement stays statisti-
cally signicant:
Full Training Set, Time Modiers
R P F ER
IaB 29 (64.4%) 74.4% 69.0% 46%
NI 15 (33.3%) 57.7% 42.3%
When using the Carroll et al (1999) system,
the object F-score improvement stays statisti-
cally signicant:
Full Training Set, Object Relations
R P F ER
IaC 194 (77.9%) 85.1% 81.3% 16%
NI 188 (75.5%) 80.3% 77.8%
2.4 Combining Sets of Initial
Annotations
So the initial annotations from dierent ex-
isting systems tend to each concentrate on
improving the performance of dierent GR
types. From this observation, one may wonder
about combining the annotations from these
dierent systems in order to increase the per-
formance on all the GR types aected by those
dierent existing systems.
Various works (van Halteren et al, 1998;
Henderson and Brill, 1999; Wilkes and
Stevenson, 1998) on combining dierent sys-
tems exist. These works use one or both of
two types of schemes. One is to have the
dierent systems simply vote. However, this
does not really make use of the fact that dif-
ferent systems are better at handling dier-
ent GR types. The other approach uses a
combiner that takes the systems' output as
input and may perform such actions as de-
termining which system to use under which
circumstance. Unfortunately, this approach
needs extra training data to train such a com-
biner. Such data may be more useful when
used instead as additional training data for
the individual methods that one is consider-
ing to combine, especially when the systems
being combined were originally given a small
amount of training data.
To avoid the disadvantages of these existing
schemes, we came up with a third method.
We combine the existing related systems by
taking a union of their translated annota-
tions as the new initial GR annotation for
our system. We rerun rule learning on the
smaller (1391 GR instance) training set with
a Union of the Buchholz et al (1999) and
Carroll et al (1999) systems' translated GR
annotations. The overall results for the test
set are (shown in row IaU)
Smaller Training Set, Overall Results
R P F ER
IaU 496 (66.3%) 76.4% 71.0% 11%
IaC 478 (63.9%) 77.2% 69.9% 7.7%
IaB 466 (62.3%) 78.1% 69.3% 5.8%
NI 448 (59.9%) 77.1% 67.4%
where the other rows are as shown in Sec-
tion 2.2. Compared to the F-score with
using Carroll et al (1999) (IaC), the IaU
F-score is borderline statistically signi-
cantly better (11% signicance level). The
IaU F-score is statistically signicantly bet-
ter than the F-scores with either using
Buchholz et al (1999) (IaB) or not using any
initial annotations (NI).
As expected, most (42 of 48) of the overall
increase in recall going from NI to IaU comes
from increasing the recall of the object, time
modier and other modier relations, the re-
lations that IaC and IaB concentrate on. The
ER for object is 11% and for time modier is
56%.
When this combining approach is repeated
the full 1963 GR instance training set, the
overall results for the test set are
Full Training Set, Overall Results
R P F ER
IaU 502 (67.1%) 77.7% 72.0% 7.3%
IaC 487 (65.1%) 79.7% 71.7% 6.3%
IaB 486 (65.0%) 76.5% 70.3% 1.7%
NI 476 (63.6%) 77.3% 69.8%
Compared to the smaller training set results,
the dierence between IaU and IaC here is
smaller for both the absolute F-score (0.3%
versus 1.1%) and ER (1.0% versus 3.3%). In
fact, the F-score dierence is small enough to
not be statistically signicant. Given the pre-
vious results for IaC and IaB as a small train-
ing set gets larger, this is not surprising.
3 Discussion
GRs are important, but dierent sets of GRs
are useful for dierent purposes and dierent
systems are better at nding certain types of
GRs. Here, we have been looking at ways of
improving automatic GR nders when one has
only a small amount of data with the desired
GR annotations. In this paper, we improve
the performance of the Ferro et al (1999) GR
transformation rule learner by using existing
systems to nd related sets of GRs. The out-
put of these systems is used to supply ini-
tial sets of annotations for the rule learner.
We achieve modest gains with the existing
systems tried. When one examines the re-
sults, one notices that the gains tend to be
uneven, with a few GR types having large
gains, and the rest not being aected much.
The dierent systems concentrate on improv-
ing dierent GR types. We leverage this ten-
dency to make a further modest improvement
in the overall results by providing the rule
learner with the merged output of these ex-
isting systems. We have yet to try other ways
of combining the output of existing systems
that do not require extra training data. One
possibility is the example-based combiner in
Brill and Wu (1998, Sec. 3.2).
6
Furthermore,
nding additional existing systems to add to
the combination may further improve the re-
sults.
References
S. A?t-Mokhtar and J.-P. Chanod. 1997. Subject
and object dependency extraction using nite-
state transducers. In Proc. ACL workshop on
automatic information extraction and building
6
Based on the paper, we were unsure if extra train-
ing data is needed for this combiner. One of the au-
thors, Wu, has told us that extra data is not needed.
of lexical semantic resources for NLP applica-
tions, Madrid.
E. Brill and P. Resnik. 1994. A rule-based ap-
proach to prepositional phrase attachment dis-
ambiguation. In 15th International Conf. on
Computational Linguistics (COLING).
E. Brill and J. Wu. 1998. Classier combina-
tion for improved lexical disambiguation. In
COLING-ACL'98, pages 191195, Montr?al,
Canada.
S. Buchholz, J. Veenstra, and W. Daelemans.
1999. Cascaded grammatical relation assign-
ment. In Joint SIGDAT Conference on Empir-
ical Methods in NLP and Very Large Corpora
(EMNLP/VLC'99). cs.CL/9906004.
J. Carroll, T. Briscoe, N. Calzolari, S. Fed-
erici, S. Montemagni, V. Pirrelli, G. Grefen-
stette, A. Sanlippo, G. Carroll, and M. Rooth.
1997. Sparkle work package 1, spec-
ication of phrasal parsing, nal report.
Available at http://www.ilc.pi.cnr.it/-
sparkle/sparkle.htm, November.
J. Carroll, G. Minnen, and T. Briscoe. 1999.
Corpus annotation for parser evaluation. In
EACL99 workshop on Linguistically Interpreted
Corpora (LINC'99). cs.CL/9907013.
P. Cohen. 1995. Empirical Methods for Articial
Intelligence. MIT Press, Cambridge, MA, USA.
L. Ferro, M. Vilain, and A. Yeh. 1999. Learn-
ing transformation rules to nd grammatical
relations. In Computational natural language
learning (CoNLL-99), pages 4352. EACL'99
workshop, cs.CL/9906015.
D. Harnett. 1982. Statistical Methods. Addison-
Wesley Publishing Co., Reading, MA, USA,
third edition.
J. Henderson and E. Brill. 1999. Exploiting diver-
sity in natural language processing: combining
parsers. In Joint SIGDAT Conference on Em-
pirical Methods in NLP and Very Large Cor-
pora (EMNLP/VLC'99).
R. Hwa. 1999. Supervised grammar induction
using training data with limited constituent in-
formation. In ACL'99. cs.CL/9905001.
R. Kaplan. 1994. The formal architecture of
lexical-functional grammar. In M. Dalrymple,
R. Kaplan, J. Maxwell III, and A. Zaenen, ed-
itors, Formal issues in lexical-functional gram-
mar. Stanford University.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: the penn treebank. Computational Lin-
guistics, 19(2).
M. Palmer, R. Passonneau, C. Weir, and T. Finin.
1993. The kernel text understanding system.
Articial Intelligence, 63:1768.
D. Palmer. 1997. A trainable rule-based algo-
rithm for word segmentation. In Proceedings of
ACL/EACL97.
D. Perlmutter. 1983. Studies in Relational Gram-
mar 1. U. Chicago Press.
H. van Halteren, J. Zavrel, and W. Daelemans.
1998. Improving data driven wordclass tagging
by system combination. In COLING-ACL'98,
pages 491497, Montr?al, Canada.
Y. Wilkes and M. Stevenson. 1998. Word sense
disambiguation using optimized combinations
of knowledge sources. In COLING-ACL'98,
pages 13981402, Montr?al, Canada.
