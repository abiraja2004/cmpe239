INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 125?127,
Utica, May 2012. c?2012 Association for Computational Linguistics
Natural Language Generation for a Smart Biology Textbook
Eva Banik1, Eric Kow1, Vinay Chaudhri2, Nikhil Dinesh2, and Umangi Oza3
1{ebanik,kowey}@comp-ling.co.uk, Computational Linguistics Ltd, London, UK
2 {chaudhri,dinesh}@ai.sri.com, SRI International, Menlo Park, CA
3umangi.oza@evalueserve.com, Evaluserve, New Delhi, India
1 Application Context
In this demo paper we describe the natural lan-
guage generation component of an electronic
textbook application, called Inquire1. Inquire
interacts with a knowledge base which encodes
information from a biology textbook. The
application includes a question-understanding
module which allows students to ask questions
about the contents of the book, and a question-
answering module which retrieves the corre-
sponding answer from the knowledge base. The
task of the natural language generation mod-
ule is to present specific parts of the answer in
English. Our current generation pipeline han-
dles inputs that describe the biological func-
tions of entities, the steps of biological processes,
and the spatial relations between parts of enti-
ties. Our ultimate goal is to generate paragraph-
length texts from arbitrary paths in the knowl-
edge base. We describe here the natural lan-
guage generation pipeline and demonstrate the
inputs and generated texts. In the demo pre-
sentation we will show the textbook application
and the knowledge base authoring environment,
and provide an opportunity to interact with the
system.
2 The Knowledge Base
The knowledge base contains information from
a college-level biology textbook2, encoded by bi-
1The work described in this paper and presented in
the demo is funded by Vulcan Inc.
2 Reece et al 2010. Campbell biology. Pearson
Publishing.
ologists as part of project HALO at SRI3. The
core of the knowledge base is the CLIB ontol-
ogy4, which is extended with biology-specific in-
formation. The knowledge base encodes entity-
to-event relations (similar to thematic roles in
linguistics), event-to-event relations (discourse
relations), various property values and relations
between properties, spatial relations, cardinality
constraints, and roles that participants play in
events. The input to the generation pipeline is a
set of triples extracted from the biology knowl-
edge base. Currently our content selection in-
cludes either an event and the entities that par-
ticipate in the event, or a set of entities and
spatial relations between them.
3 Generation Grammar and Lexicon
Our generation grammar consists of a set of Tree
Adjoining Grammar (TAG) elementary trees.
Each tree is associated with either a single rela-
tion, or a set of relations in the knowledge base.
As an example, Fig 1 illustrates the mapping
between elementary trees and event participant
relations in the KB for the above input. We
currently associate up to three different elemen-
tary trees with each event and the connected
set of participant relations: an active senten-
tial tree, a passive sentential tree and a complex
noun phrase.
The knowledge base provides concept-to-word
3 Gunning Et al, 2010. Project halo update
progress toward digital aristotle. AI Magazine Fall:33-
58. See also http://www.projecthalo.com/
4http://www.cs.utexas.edu/users/mfkb/RKF/clib.html
125
Figure 1: The grammar of the surface realizer
mappings (a list of synonyms) for every concept,
and the words are used in the generation lexi-
con to anchor elementary TAG trees. Our gen-
eration grammar consists of a set of TAG tree
templates, which are defined as combinations of
tree fragments and are compiled using the XMG
metgrammar toolkit5.
These underspecified elementary trees are fur-
ther specified in the generation lexicon, which
maps concepts onto elementary tree templates,
and associates a word (an anchor) with the
tree, along with other idiosynchratic information
(e.g., preposition choice). We create a genera-
tion lexicon dynamically at run-time, by map-
ping tree templates onto concepts based on the
number and types of participants, and the lexi-
cal information associated with the event (e.g.,
the preposition requirements of the verb).
Concept names for entities are included in
the elementary trees as features on the corre-
sponding NP nodes. These features form part
of the input to the referring expression genera-
tion module, which looks up the concept name
5https://sourcesup.renater.fr/xmg/
in the concept-to-word mapping to obtain a list
of possible noun phrases.
4 Realization
Our natural language generation pipeline is cen-
tered around the GenI surface realizer6,7. The
set of triples yielded by content selection are first
aggregated and converted to GenI?s input for-
mat, a set of flat semantic literals. We then feed
this input to GenI to produce an underspecified
surface form in which referring expressions are
still underspecified:
NP is detach from NP resulting in NP at NP
NP detach from NP resulting in NP at NP
Detachment of NP from NP resulting in NP at NP
A post-processing module carries out refer-
ring expression generation and morphological re-
alization to produce the fully specified output.
6 Kow, Eric. 2007. Surface realisation: ambiguity
and determinism. Doctoral Dissertation, Universite de
Henri Poincare - Nancy 1.
7 Banik, Eva 2010. A minimalist approach to gen-
erating coherent texts. Phd thesis, Department of Com-
puting, The Open University
126
Question Answering & Reasoning Algorithms
Event Instance 
Content Selection
Set of triples
Input aggregation and conversion +Stylistic control
Knowledge Base
Realization with GenI
Morphology &referring expression generation
Semantic literals +input parameters
Ranking
Underspecified realizations
Linguistic Resources
Generation Lexicon
Grammar: Description of TAG tree templates
Concept-to-Wordmappings
Mapping of KB relationsto TAG tree templates
Morphological lexicon
Verb frames (preposition choice)
NLG Pipeline
Figure 2: Linguistic resources and the generation pipeline
Our referring expression realization algorithm
performs further semantic aggregation where
necessary to produce cardinals (?two chromo-
somes?), and decides on a suitable determiner
based on previous mentions of instance names
and subclasses in the discourse context (def-
inite/indefinite determiner, ?another? or ?the
same?). For the input shown in Fig 1, our sys-
tem will produce the following three realizations:
1. A sister chromatid detaches from another sister chro-
matid resulting in two chromosomes at a kinetochore.
2. A sister chromatid is detached from another sister
chromatid resulting in two chromosomes at a kinetochore.
3. Detachment of a sister chromatid from another sister
chromatid resulting in two chromosomes at a kinetochore
We rank the generated outputs based on their
linguistic properties using optimality theoretic
constraints (e.g., active sentences are ranked
above passive sentences), where each constraint
corresponds to a (set of) tree fragments that
contributed to building the tree that appears in
the output. Our system also allows for extra in-
put parameters to be sent to GenI to restrict the
set of generated outputs to fit a specific context
(e.g., syntactic type or focused discourse entity).
Our full natural language generation pipeline is
illustrated in Fig 2.
5 Future Work
We are currently working on extending the sys-
tem to handle more relations and other data
types in the knowledge base. This involves ex-
tending the grammar to new sentence types and
other linguistic constructions, and extending the
content selection module to return more triples
from the knowledge base. Our ultimate goal is
to be able to generate arbitrary ? but in some
sense well-formed ? paths from the knowledge
base as coherent paragraphs of text.
127
