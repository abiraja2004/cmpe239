R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 245 ? 256, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
A Method of Recognizing Entity and Relation 
Xinghua Fan1, 2 and Maosong Sun1 
1
 State Key Laboratory of Intelligent Technology and Systems,  
Tsinghua University, Beijing 100084, China 
fanxh@tsinghua.org.cn, sms@mail.tsinghua.edu.cn 
2
 State Intellectual Property Office of P.R. China, Beijing, 100088, China 
Abstract. The entity and relation recognition, i.e. (1) assigning semantic classes 
to entities in a sentence, and (2) determining the relations held between entities, 
is an important task in areas such as information extraction. Subtasks (1) and 
(2) are typically carried out sequentially, but this approach is problematic: the 
errors made in subtask (1) are propagated to subtask (2) with an accumulative 
effect; and, the information available only in subtask (2) cannot be used in sub-
task (1). To address this problem, we propose a method that allows subtasks (1) 
and (2) to be associated more closely with each other. The process is performed 
in three stages: firstly, employing two classifiers to do subtasks (1) and (2) in-
dependently; secondly, recognizing an entity by taking all the entities and rela-
tions into account, using a model called the Entity Relation Propagation Dia-
gram; thirdly, recognizing a relation based on the results of the preceding stage. 
The experiments show that the proposed method can improve the entity and re-
lation recognition in some degree. 
1   Introduction 
The entity and relation recognition, i.e. assigning semantic classes (e.g., person, or-
ganization and location) to entities in a sentence and determining the relations (e.g., 
born-in and employee-of) that hold between entities, is an important task in areas such 
as information extraction (IE) [1] [2] [3] [4], question answering (QA) [5] and story 
comprehension [6]. In a QA system, many questions concern the specific entities in 
some relations. For example, the question that ?Where was Poe born?? in TREC-9 
asks for the location entity in which Poe was born. In a typical IE task in constructing 
a job database from unstructured texts, the system are required to extract many mean-
ingful entities like titles and salary from the texts and to determine how these entities 
are associated with job positions. 
The task of recognizing entity and relation is usually treated as two separate sub-
tasks carried out sequentially: (1) to recognize entities using an entity recognizer, and 
(2) to determine the relations held between them. This approach has two shortcom-
ings. Firstly, the errors made in subtask (1) will be propagated to subtask (2) with an 
accumulative effect, leading to a loss in performance of relation recognition. For 
example, if ?Boston? is mislabeled as a person, it will never have chance to be classi-
fied as the location of Poe?s birthplace. Secondly, the information available only in 
246 X. Fan and M. Sun 
subtask (2) cannot be used for subtask (1). For example, if we feel difficult to  
determine whether the entity X is a person or not, but we can determine that there 
exists a relation born-in between X and China easily, it is obvious that we can claim 
that X must be a person. 
To address the problems described above, this paper presents a novel approach 
which allows subtasks (1) and (2) to be linked more closely together. The process is 
separated into three stages. Firstly, employing two classifiers to perform subtasks (1) 
and (2) independently. Secondly, recognizing an entity by taking all the entities and 
relations into account using a particularly designed model called the Entity Relation 
Propagation Diagram. And, thirdly, recognizing a relation based on the results of the 
preceding step. 
The rest of the paper is organized as follows. Section 2 defines the problem of en-
tity and relation recognition in a formal way. Section 3 describes the proposed method 
of recognizing entity and relation. Section 4 gives the experimental results. Section 5 
is the related work and comparison. Section 6 is conclusions. 
2   The Problem of Entity and Relation Recognition 
Conceptually, the entities and relations in a sentence can be viewed, while taking 
account of the mutual dependencies among them, as a labeled graph in Fig. 1. 
 
Fig. 1. Concept view of the entities and relations among them 
In Fig.1, a node represents an entity and a link denotes the relation held between 
two entities. The arrowhead of a link represents the direction of the relation. Each 
entity or relation has several attributes, which are structured as a list of the node or the 
edge. These attributes can be classified into two classes. Some of them that are easy to 
acquire, such as words in an entity and parts of speech of words in a context, are 
called local attributes; the others that are difficult to acquire, such as semantic classes 
of phrases and relations among them, are called decision attributes. The issue of entity 
and relation recognition is to determine a unique value for each decision attribute of 
all entities and relations, by considering the local attributes of them. To describe the 
problem in a formal way, we first give some basic definitions as follows. 
 A Method of Recognizing Entity and Relation 247 
Definition 1 (Entity). An entity can be a single word or a set of consecutive words 
with a predefined boundary. A sentence is a linked list, which consists of words and 
entities. Entities in a sentence are denoted as E1, E2 ? according to their order, with 
values ranging over a set of entity class CE. For example, the sentence in Fig. 2 has 
three entities: E1= ?Dole?, E2= ?Elizabeth? and E3= ?Salisbury, N.C.?. Note that it is 
not easy to determine the entity boundaries [7]. Here we assume that it has been 
solved and its output serves as the input to our model.  
 
Fig. 2. A sentence that have three entities 
Definition 2 (Relation). In this paper, we only consider the relation between two 
entities. An entity pair (Ei, Ej) represents a relation Rij from entity Ei and Ej, where Ei 
is the first argument and Ej is the second argument. Relation Rij takes its value that 
ranges over a set of relation class CR. Note that (Ei, Ej) is an ordered pair, and there 
exist two relations Rij =(Ei, Ej) and Rji =(Ej, Ei) between entities Ei and Ej. 
Definition 3 (Class). The class of an entity or relation is its decision attribute, which 
is one of the predefined class set and is unknown before being recognized. We denote 
the sets of predefined entity class and relation class as CE and CR respectively. CE has 
one special element other-ent, which represents any unlisted entity class. For algo-
rithmic reasons, we suppose all elements in CE are mutually exclusive. Similarly, CR 
also has one special element other-rel, which represents that the two involved entities 
are irrelevant or their relation class is undefined. For algorithmic reasons, we suppose 
all elements in CR are mutually exclusive. In fact, because the class of an entity or a 
relation is only a label that we want to predict, if an entity or a relation have more 
than one labels simultaneously, to satisfy the constraint that all elements in CE or CR 
are mutually exclusive, we can separate it into several cases and construct several 
predefined entity class sets and relation class sets.  
The classes of entities and relations in a sentence must satisfy some constraints. For 
example, if the class of entity E1, which is the first argument of relation R12, is a loca-
tion, then the class of relation R12 cannot be born-in because the class of the first ar-
gument in relation R12 has to be a person. 
Definition 4 (Constraint). A constraint is a 5-tuple ),,, R,( R21 ????? . The symbols 
are defined as follows. RCR ?  represents the class of relation R. E21 C, ??? repre-
sents the classes of the first argument Ei and the second argument Ej in the relation R 
respectively. ]1,0[?R?  is a real number that represents a joint conditional probability 
distribution }R|,Pr{ 21R ??? = . ]1,0[???  is a real number that represents a condi-
tional probability distribution },|RPr{ 21 ???? = . Note that R?  and ??  need not to 
be specified manually and can be learned from an annotated training dataset easily. 
248 X. Fan and M. Sun 
Definition 5 (Observation). We denote the observations of an entity and a relation in 
a sentence as OE and OR respectively. OE or OR represent all the ?known? local attrib-
utes of an entity or a relation, e.g., the spelling of a word, parts of speech, and  
semantic related attributes acquired from external resources such as WordNet. The  
observations OE and OR can be viewed as a random event, and 1}Pr{O}Pr{O RE ?=  
because OE and OR in a sentence are known.        
Based on the above definitions, the issue of entity and relation recognition can be 
described in a formal way as follows. Suppose in a sentence, the set of entity is {E1, 
E2 ? En}, the set of relation is {R12, R21, R13, R31, ?, R1n, Rn1, ?, Rn-1,n, Rn,n-1}, the 
predefined sets of entity class and relation class are CE ={e1, e2, ? em} and CR ={ r1, 
r2, ? rk} respectively, the observation of entity Ei is EiO , and the observation of rela-
tion Rij is RijO . n, m and k represent the number of entity, the number of the prede-
fined entity class and the number of the predefined relation class respectively. The 
problem is to search the most probable class assignment for each entity and each 
relation of interest, given the observations of all entities and relations. In other words, 
the problem is to solve the following two equations, using two kinds of constraint 
knowledge ???  ,R  and the interaction among entities and relations.  
} O ,O ,,O ,O , ,O ,O ,O, ,O ,O|ePr{E max arge R 1-nn,R n1,-nRn1R1nR21R12EnE2E1did LLL==  (1) 
} O ,O ,,O ,O , ,O ,O ,O, ,O ,O|rPr{R max argr R 1-nn,R n1,-nRn1R1nR21R12EnE2E1dijd LLL==  (2) 
In (1), d =1, 2, ?, m, and in (2), d=1, 2, ?, k. 
3   The Proposed Method 
Because the class assignment of a single entity or relation depends not only on local 
attributes itself, but also on those of all other entities and relations, the equations (1) 
and equation (2) cannot be solved directly. To simplify the problem, we present the 
following method consisting of three stages. Firstly, employ two classifiers to perform 
entity recognition and relation recognition independently. Their outputs are the condi-
tional probability distributions Pr{E| OE} and Pr{R|OR}, given the corresponding 
observations. Secondly, recognize an entity by taking account of all entities and rela-
tions, as computed in the previous step. This is achieved by using the model Entity 
Relation Propagation Diagram (ERPD). And, recognize a relation based on the results 
of the second step at last.       
In this paper, we concentrate on the processes at the second and the third stages, as-
suming that the process at the first stage is solved and its output are given to us as 
input. At the second stage, the aim of introducing ERPD is to estimate the conditional 
probability distribution 
 ERPD}|Pr{E  given the constraint R?  in Definition 5 and 
the sets {  }O|Pr{E Eii } and { }O|Pr{R R ijij } (i, j=1,?,n), as computed at the first 
stage. For the readability, suppose 
 ERPD}|Pr{E is given, the entity recognition 
equation (1) becomes the equation (3). 
 A Method of Recognizing Entity and Relation 249 
??
???
?=
>=
=
?
?
RV     ERPD}|ePr{E max arg
RV     }O|ePr{E max arg
e
dd
E
dd
i
i
i
 (3) 
where ?  is a threshold determined by the experiment. RV? [0, 1] is a real number, 
called the reliable value, representing the belief degree of the output of the entity 
recognizer at the first stage. Suppose the maximum value of the conditional probabil-
ity distribution  }O|Pr{E E is Vm and the second value is Vs, RV is defined as: 
sm
sm
VV
VVRV
+
?
=  (4) 
The reason of introducing RV is due to a fact that only for ambiguous entities, it is 
effective by taking the classes of all entities in a sentence into account. ?Reliable 
Value? measures whether an entity is ambiguous. 
At the third stage, the basic idea of recognizing a relation is to search the probable 
relation given its observation, under a condition of satisfying the constraints imposed 
by the results of entity recognition at the second stage. The relation recognition equa-
tion (2) becomes the equation (5). 
RR
k
k W}O|rPr{R max argr ?==  
??
???
=
>
=
 0}?,?|Pr{r if  0
0}?,?|Pr{r if   1
W
21
21
R  
(5) 
where 21,??  is the results of entity recognition at the second stage, },|Pr{r 21 ??  is 
constraint knowledge ??  in Definition 4, and WR is the weight of the constraint 
knowledge. 
In the following sections, we present ERPD and two algorithms to estimate the 
conditional probability distribution
 ERPD}|Pr{E . 
3.1   The Entity Relation Propagation Diagram 
To represent the mutual dependencies among entities and relations, a model named 
the Entity Relation Propagation Diagram that can deal with cycles, similar to the 
Causality Diagram [8][9] for the complex system fault diagnosis, is developed for 
entity and relation recognition. 
The classes of any two entities are dependent on each other through the relations 
between them, while taking account of the relations in between. For example, the 
class of entity Ei in Fig. 3 (a) depends on the classes of relations Rji between entities 
Ei and Ej, and the classes of relations Rij and Rji depend on the classes of entities  
Ei and Ej. This means that we can predict the class of a target entity according to the 
class of its neighboring entity, making use of the relations between them. We  
further introduce the relation reaction intensity to describe the prediction ability of 
this kind. 
250 X. Fan and M. Sun 
 
Fig. 3. Illustration of relation reaction 
Definition 6 (Relation Reaction Intensity). We denote the relation reaction intensity 
from entity Ei to entity Ej as Pij, which represents the ability that we guess the class of 
Ej if we know the class of its neighboring entity Ei and the relation Rij between them. 
The relation reaction intensity could be modeled using a condition probability distri-
bution Pij=Pr {Ej |Ei}. 
The element klijp of Pij represents the conditional probability Pr {Ej=el |Ei=ek}:  
?
=
=
====
====
N
1t ki
tijljkitij
kilj
kl
ij }ePr{E
}rR| eE ,e}Pr{ErPr{R}eE|eEPr{p  
according to Definition 5:  
}O|rPr{R}rPr{R Rijtijtij === , }O|ePr{E}ePr{E Eikiki ===  
Then, we have: 
 }O|ePr{E
}rR| eE ,e}Pr{EO|rPr{R
 
N
1t
E
iki
tijljki
R
ijtijkl
ij ?
=
=
====
=p  (6) 
where Rt Cr ? , N is the number of relations in relation class set. In equation (6), 
}rR| eE ,ePr{E tijljki === represents the constraint knowledge R?  among entities 
and relations. }O|rPr{R Rijtij =  and }O|ePr{E Eiki =  represent the outputs at the  
first stage.  
Definition 7 (Observation Reaction Intensity). We denote the observation reaction 
intensity as the conditional probability distribution }O|Pr{E E  of an entity class, 
given the observation, which is the output at the first stage. 
The Entity Relation Propagation Diagram (ERPD). is a directed diagram that 
allows cycles. As illustrated in Fig. 4, the symbols used in the ERPD are defined as 
follows. A circle node represents an event variable that can be any one from a set of 
mutually exclusive events, which all together cover the whole sample space. Here, an  
event variable represents an entity, an event represents a predefined entity class, and 
the whole sample space represents the set of predefined entity classes. Box node 
represents a basic event which is one of the independent sources of the associated 
event variable. Here, a basic event represents the observation of an entity. Directed 
arc represents a linkage event variable that may or may not enable an input event to 
cause the corresponding output event. The linkage event variable from an event  
 A Method of Recognizing Entity and Relation 251 
variable to another event variable represents the relation reaction intensity in Defini-
tion 6. And, the linkage event variable from a basic event to the corresponding event 
variable represents the observation reaction intensity in Definition 7. All arcs pointing 
to a node are in a logical OR relationship. 
 
Fig. 4. Illustration of the Entity Relation Propagation Diagram 
Now, we present two algorithms to compute the conditional probability distribu-
tion
 ERPD}|Pr{E , one is based on the entity relation propagation tree, and the other 
is the directed iteration algorithm on ERPD. 
3.2   The Entity Relation Propagation Tree 
The Entity Relation Propagation Tree (ERPT). is a tree decomposed from an 
ERPD, which represents the relation reaction propagation from all basic events to 
each event variable logically. Each event variable in the ERPD corresponds to an 
ERPT. For example, the ERPT of X1 in Fig. 4 is illustrated in Fig. 5. The symbols 
used in the ERPT are defined as follows. The root of the tree, denoted as Circle, is an 
event variable corresponding to the event variable in the ERPD. A leaf of the tree, 
denoted as Box, is a basic event corresponding to the basic event in the ERPD. The 
middle node of the tree, denoted as Diamond, is a logical OR gate variable, which is 
made from an event variable that has been expanded in the ERPD, and, the label in 
Diamond corresponds to the label of the expanded event variable. The directed arc of 
the tree corresponds to the linkage event variable in the ERPD. All arcs pointing to a 
node are in a logical OR relationship. The relation between the directed arc and the 
node linked to it is in logical AND relationship. 
To decompose an ERPD into entity relation propagation trees, firstly we decom-
pose the ERPD into mini node trees. Each event variables in the ERPD corresponds to 
a mini node tree, in which the root of the mini tree is the event variable in concern at 
present, and the leaves are composed of all neighboring basic events and event vari-
ables that are connected to the linkage event variables pointing to the top event vari-
ables. Secondly, expand a mini node tree into an entity relation propagation tree, i.e., 
the neighboring event variables in the mini node tree are replaced with their corre-
sponding mini trees. During expanding a node event variable, when there are loops, 
Rule BreakLoop is applied to break down the loops. 
252 X. Fan and M. Sun 
 
Fig. 5. Illustration of the entity relation propagation tree 
Rule BreakLoop. An event variable cannot propagate the relation reaction to itself. 
Rule 1 is derived from a law commonsense - one can attest that he is sinless. When such 
a loop is encountered, the descendant event variable, which is same as the head event 
variable of the loop, is treated as a null event variable, together with its connected link-
age event variable to be deleted.    
Compute the Conditional Probability Distribution in an ERPT. After an ERPD is 
decomposed into entity relation propagation trees, the conditional probability distribu-
tion  ERPD}|Pr{E becomes  ERPT}|Pr{E . When an event variable Xi has more than 
one input, these inputs will be in logic OR relationship, as defined in the ERPD. Since 
these inputs are independent, there exists such a case that one input causes Xi to be an 
instance kiX  while another input causes Xi to be an instance
l
iX , this would be impos-
sible because kiX  and 
l
iX  are exclusive. In the real world, the mechanism, in which 
iX  can response to more than one independent input properly, is very complicated 
and may vary from one case to another. To avoid this difficulty, a basic assumption is 
introduced.           
Assumption. When there is more than one input to Xi, each input will contribute a 
possibility to Xi. For each input, its contribution to this possibility equals to the prob-
ability that it causes Xi directly, as if the other inputs do not exist. The final possibility 
that Xi occurs is the sum of the possibilities from all inputs. 
Suppose an event variable X has m inputs, and the probability distributions of all 
linkage event variables, linked basic events or event variables are Pi and Pr {Xi} re-
spectively, i=1,2?m. Based on the above assumption, the formula for computing the 
probability distribution of X can be derived as: 
)
}Pr{X
}Pr{X
PNorm(
}Pr{X
}Pr{X
m
1i n
i
1
i
i
n
1
?
= ??
?
?
?
??
?
?
?
?=
??
?
?
?
??
?
?
?
MM  (7) 
 A Method of Recognizing Entity and Relation 253 
where, Norm () is a function that normalizes the vector in {}, and n is the state  
number of X. 
So, the probability distribution  ERPT}|Pr{E of the variable X in the correspond-
ing ERPT can be computed in the following steps. Firstly, to find the middle node 
sequence in the corresponding ERPT in the depth-first search; secondly, according to 
the sequence, for each middle node, equation (7) is applied to compute its probability 
distribution. In this procedure, the previous results can be used for the latter  
computation. 
3.3   The Directed Iteration Algorithm on ERPD 
The idea is to compute the probability distribution of the event variable on the ERPD 
directly, without decomposing the ERPD to some ERPTs. The aim is to avoid the 
computational complexity of using ERPT. This is achieved by adopting an iteration 
strategy, which is the same as that used in the loopy belief network [10]. 
The Directed Iteration Algorithm. is as follows: Firstly, only take the basic event as 
input, and initialize each event variable according to formula (7), i.e., assigning an 
initialized probability distribution to each event variable. Secondly, take the basic 
event and the probability distributions of all neighboring nodes computed in the pre-
vious step as input, and iterate to update the probability distributions of all nodes in 
ERPD in parallel according to formula (7). Thirdly, if none of the probability distribu-
tion of all nodes in ERPD in successive iterations changes larger than a small thresh-
old, the iteration is said to converge and then stops. 
4   Experiments 
Dataset. The dataset in our experiments is the same as the Roth?s dataset ?all? [11], 
which consists of 245 sentences that have the relation kill, 179 sentences that have the 
relation born-in and 502 sentences that have no relations. The predefined entity 
classes are other-ent, person and location, and the predefined relation classes are 
other-rel, kill and born-in. In fact, we use the results at the first stage in our method as 
the input, which are provided by W. Yih. 
Experiment Design. We compare five approaches in the experiments: Basic, Omnis-
cient, ERPD, ERPD* and BN. The Basic approach, which is a baseline, tests the per-
formance of the two classifiers at the first stage, which are learned from their local 
attributes independently. The Omniscient approach is similar to Basic, the only defer-
ence is that the classes of entities are exposed to relation classifier and vice versa. 
Note that it is certainly impossible to know the true classes of an entity and a relation 
in advance. The BN is the method based on the belief network, -- we follow the BN 
method according to the description in [11]. The ERPD is the proposed method based 
on ERPT, and the ERPD* is the proposed method based on the directed iteration 
algorithm. The threshold of RV is 0.4. 
Results. The experimental results are shown in Table 1. It can be seen from the table 
that 1) it is very difficult to improve the entity recognition because BN and Omnis-
cient almost do not improve the performance of Basic; 2) the proposed method can 
254 X. Fan and M. Sun 
improve the precision, which is thought of being more important than the recall for 
the task of recognizing entity; 3) the relation recognition can be improved if we can 
improve the entity recognition, as indicated by the comparisons of Basic, ERPD and 
Omniscient; 4) the proposed method can improve the relation recognition, and it per-
formance is almost equal to that of BN; 5) the performance of ERPD and ERPD* is 
almost equal, so the directly iteration algorithm is effective. 
Table 1. Experimental results 
 
5   Related Work and Comparison  
Targeting at the problems mentioned above, a method based on the belief network has 
been presented in [11], in which two subtasks are carried out simultaneously. Its pro-
cedure is as follows: firstly, two classifiers are trained for recognizing entities and 
relations independently and their outputs are treated as the conditional probability 
distributions for each entity and relation, given the observed data; secondly, this in-
formation together with the constraint knowledge among relations and entities are 
represented in a belief network [12] and are used to make global inferences for all 
entities and relations of interest. This method is denoted BN in our experiments.  
Although BN can block the error propagation from the entity recognizer to the rela-
tion classifier as well as improve the relation recognition, it cannot make use of the 
information, which is only available in relation recognition, to help entity recognition. 
Experiments show that BN cannot improve entity recognition. 
Comparing to BN, the proposed method in this paper can overcome the two short-
comings of it. Experiments show that it can not only improve the relation recognition, 
but also improve the precision of entity recognition. Moreover, the model ERPD 
could be more expressive enough than the belief network for the task of recognizing 
 A Method of Recognizing Entity and Relation 255 
entity and relation. It can represent the mutually dependences between entities and 
relations by introducing relation reaction intensity, and can deal with a loop without 
the limitation of directed acyclic diagram (DAG) in the belief network. At the same 
time, the proposed method can merge two kinds of constraint knowledge (i.e. 
???  and R  in Definition 4), but the method based on belief network can only use ?? . 
Finally, the proposed method has a high computation efficiency while using the di-
rected iteration algorithm. 
6   Conclusions 
The subtasks of entity recognition and relation recognition are typically carried out 
sequentially. This paper proposed an integrated approach that allows the two subtasks 
to be performed in a much closer way. Experimental results show that this method can 
improve the entity and relation recognition in some degree. 
In addition,  the Entity Relation Propagation Diagram (ERPD) is used to figure out 
the dependencies among entities and relations. It can also merge some constraint 
knowledge. Regarding to ERPD, two algorithms are further designed, one is based on 
the entity relation propagation tree, the other is the directed iteration algorithm on 
ERPD. The latter can be regarded as an approximation of the former with a higher 
computational efficiency. 
Acknowledgements 
We would like to express our deepest gratitude to Roth D. and Yih W. for making 
their dataset available for us. The research is supported in part by the National 863 
Project of China under grant number 2001AA114210-03, the National Natural Sci-
ence Foundation of China under grant number 60321002, and the Tsinghua-ALVIS 
Project co-sponsored by the National Natural Science Foundation of China under 
grant number 60520130299 and EU FP6. 
References 
1. Chinchor, N. MUC-7 Information Extraction Task Definition. In Proceeding of the Sev-
enth Message Understanding Conference (MUC-7), Appendices, 1998. 
2. Califf, M. and Mooney, R. Relational Learning of Pattern-match Rules for Information 
Extraction. In Proceedings of the Sixteenth National Conference on Artificial Intelligence 
and Eleventh Conference on Innovative Applications of Artificial Intelligence, 328-334, 
Orlando, Florida, USA, AAAI Press, 1999. 
3. Freitag, D. Machine Learning for Information Extraction in Informal Domains. Machine 
learning, 39(2/3): 169-202, 2000. 
4. Roth, D. and Yih, W. Relational Learning via Prepositional Algorithms: An Information 
Extraction Case Study. In Proceedings of the Seventeenth International Joint Conference on 
Artificial Intelligence, 1257-1263, Seattle, Washington, USA, Morgan Kaufmann, 2001. 
5. Voorhees, E. Overview of the Trec-9 Question Answering Track. In The Ninth Text Re-
trieval Conference (TREC-9), 71-80, 2000. 
256 X. Fan and M. Sun 
6. Hirschman, L., Light, M., Breck, E. and Burger, J. Deep Read: A Reading Comprehension 
System. In Proceedings of the 37th Annual Meeting of Association for Computational Lin-
guistics, 1999. 
7. Abney, S.P. Parsing by Chunks. In S. P. Abney, R. C. Berwick, and C. Tenny, editors, 
Principle-based parsing: Computation and Psycholinguistics, 257-278. Kluwer, Dordrecht, 
1991. 
8. Xinghua Fan. Causality Diagram Theory Research and Applying it to Fault Diagnosis of 
Complexity System, Ph.D. Dissertation of Chongqing University, P.R. China,  2002. 
9. Xinghua Fan, Zhang Qin, Sun Maosong, Huang Xiyue. Reasoning Algorithm in Multi-
Valued Causality Diagram, Chinese Journal of Computers, 26(3), 310-322, 2003. 
10. Murphy, K., Weiss, Y., and Jordan, M. Loopy Belief Propagation for Approximate Infer-
ence: An empirical study. In Proceeding of Uncertainty in AI, 467-475, 1999. 
11. Roth, D. and Yih, W. Probability Reasoning for Entity & Relation Recognition. In Pro-
ceedings of 20th International Conference on Computational Linguistics (COLING-02), 
835-841, 2002. 
12. Pearl, J. Probability Reasoning in Intelligence Systems. Morgan Kaufmann, 1988. 
