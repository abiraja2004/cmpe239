Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 163?168,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
SEMILAR: The Semantic Similarity Toolkit 
 
Vasile Rus, Mihai Lintean, Rajendra Banjade, Nobal Niraula, and Dan Stefanescu 
Department of Computer Science 
The University of Memphis 
Memphis, TN 38152 
{vrus,rbanjade,mclinten,nbnraula,dstfnscu}@memphis.edu 
 
 
Abstract 
We present in this paper SEMILAR, the SE-
Mantic simILARity toolkit. SEMILAR im-
plements a number of algorithms for assessing 
the semantic similarity between two texts. It is 
available as a Java library and as a Java 
standalone ap-plication offering GUI-based 
access to the implemented semantic similarity 
methods. Furthermore, it offers facilities for 
manual se-mantic similarity annotation by ex-
perts through its component SEMILAT (a 
SEMantic simILarity Annotation Tool). 
1 Introduction 
We present in this paper the design and im-
plementation of SEMILAR, the SEMantic 
simILARity toolkit. SEMILAR 
(www.semanticsimilarity.org) includes im-
plementations of a number of algorithms pro-
posed over the last decade or so to address 
various instances of the general problem of 
text-to-text semantic similarity. Semantic sim-
ilarity is an approach to language understand-
ing that is widely used in real applications. It 
is a practical alternative to the true under-
standing approach, which is intractable as it 
requires world knowledge, a yet to-be-solved 
problem in Artificial Intelligence. 
Text A: York had no problem with MTA?s in-
sisting the decision to shift funds had been within 
its legal rights. 
Text B: York had no problem with MTA?s say-
ing the decision to shift funds was within its 
powers. 
 
Given such two texts, the paraphrase identifi-
cation task is about automatically assessing 
whether Text A is a paraphrase of, i.e. has the 
same meaning as, Text B. The example above is 
a positive instance, meaning that Text A is a par-
aphrase of Text B and vice versa. 
The importance of semantic similarity in Nat-
ural Language Processing (NLP) is highlighted 
by the diversity of datasets and shared task eval-
uation campaigns (STECs) that have been pro-
posed over the last decade (Dolan, Quirk, and 
Brockett, 2004; McCarthy & McNamara, 2008; 
Agirre et al, 2012). These datasets include in-
stances from various applications.  Indeed, there 
is a need to identify and quantify semantic rela-
tions between texts in many applications. For 
instance, paraphrase identification, an instance of 
the semantic similarity problem, is an important 
step in a number of applications including Natu-
ral Language Generation, Question Answering, 
and dialogue-based Intelligent Tutoring Systems. 
In Natural Language Generation, paraphrases are 
a method to increase diversity of generated text 
(Iordanskaja et al 1991). In Question Answer-
ing, multiple answers that are paraphrases of 
each other could be considered as evidence for 
the correctness of the answer (Ibrahim et al 
2003). In Intelligent Tutoring Sys-tems (Rus et 
al., 2009; Lintean et al, 2010; Lintean, 2011), 
paraphrase identification is useful to assess 
whether students? articulated answers to deep 
questions (e.g. conceptual physics questions) are 
similar-to/paraphrases-of ideal answers. 
Generally, the problem of semantic similarity 
between two texts, denoted text A and text B, is 
defined as quantifying and identifying the pres-
ence of semantic relations between the two texts, 
e.g. to what extent text A has the same meaning 
as or is a paraphrase of text B (paraphrase rela-
tion; Dolan, Quirk, and Brockett, 2004). Other 
semantic relations that have been investigated 
systematically in the recent past are entailment, 
i.e. to what extent text A entails or logically in-
fers text B (Dagan, Glickman, & Magnini, 2004), 
and elaboration, i.e. is text B is an elaboration of 
text A? (McCarthy & McNamara, 2008). 
163
Semantic similarity can be broadly construed 
between texts of any size. Depending on the 
granularity of the texts, we can talk about the 
following fundamental text-to-text similarity 
problems: word-to-word similarity, phrase-to-
phrase similarity, sentence-to-sentence similari-
ty, paragraph-to-paragraph similarity, or docu-
ment-to-document similarity. Mixed combina-
tions are also possible such as assessing the simi-
larity of a word to a sentence or a sentence to a 
paragraph. For instance, in summarization it 
might be useful to assess how well a sentence 
summarizes an entire paragraph. 
2 Motivation 
The problem of word-to-word similarity has been 
extensively studied over the past decades and a 
word-to-word similarity library (WordNet Simi-
larity) has been developed by Pedersen and col-
leagues (Pedersen, Patwardhan, & Michelizzi, 
2004). 
Methods to assess the semantic similarity of 
larger texts, in particular sentences, have been 
proposed over the last decade (Corley and 
Mihalcea, 2005; Fernando & Stevenson, 2008; 
Rus, Lintean, Graesser, & McNamara 2009). 
Androutsopoulos & Malakasiotis (2010) com-
piled a survey of methods for paraphrasing and 
entailment semantic relation identification at sen-
tence level. Despite all the proposed methods to 
assess semantic similarity between two texts, no 
semantic similarity library or toolkit, similar to 
the WordNet library for word-to-word similarity, 
exists for larger texts. Given the importance of 
semantic similarity, there is an acute need for 
such a library and toolkit. The developed SEMI-
LAR library and toolkit presented here fulfill this 
need. 
In particular, the development of the semantic 
similarity toolkit SEMILAR has been motivated 
by the need for an integrated environment that 
would provide:  
 
? easy access to implementations of various 
semantic similarity approaches from the 
same user-friendly interface and/or library. 
? easy access to semantic similarity methods 
that work at different levels of text granulari-
ty: word-to-word, sentence-to-sentence, par-
agraph-to-paragraph, document-to-
document, or a combination (SEMILAR in-
tegrates word-to-word similarity measures). 
? authoring methods for semantic similarity. 
? a common environment for that allows sys-
tematic and fair comparison of semantic sim-
ilarity methods. 
? facilities to manually annotate texts with se-
mantic similarity relations using a graphical 
user interface that make such annotations 
easier for experts (this component is called 
SEMILAT component - a SEMantic similari-
ty Annotation Tool). 
 
SEMILAR is thus a one-stop-shop for investi-
gating, annotating, and authoring methods for the 
semantic similarity of texts of any level of granu-
larity. 
3 SEMILAR: The Semantic Similarity 
Toolkit 
The authors of the SEMILAR toolkit (see Figure 
1) have been involved in assessing the semantic 
Figure 1. Snapshot of SEMILAR. The Data View tab is shown. 
164
similarity of texts for more than a decade. During 
this time, they have conducted a careful require-
ments analysis for an integrated software toolkit 
that would integrate various methods for seman-
tic similarity assessment. The result of this effort 
is the prototype presented here. We briefly pre-
sent the components of SEMILAR next and then 
describe in more detail the core component of 
SEMILAR, i.e. the set of semantic similarity 
methods that are currently available. It should be 
noted that we are continuously adding new se-
mantic similarity methods and features to SEMI-
LAR. 
The SEMILAR toolkit includes the following 
components: project management; data view-
browsing-visualization; preprocessing (e.g., col-
location identification, part-of-speech tagging, 
phrase or dependency parsing, etc.), semantic 
similarity methods (word-level and sentence-
level), classification components for qualitative 
decision making with respect to textual semantic 
relations (na?ve Bayes, Decision Trees, Support 
Vector Machines, and Neural Network), kernel-
based methods (sequence kernels, word sequence 
kernels, and tree kernels; as of this writing, we 
are still implementing several other tree kernel 
methods); debugging and testing facilities for 
model selection; and annotation components (al-
lows domain expert to manually annotate texts 
with semantic relations using GUI-based facili-
ties; Rus et al, 2012). For space reasons, we only 
detail next the main algorithms in the core com-
ponent, i.e. the major text-to-text similarity algo-
rithms currently available in SEMILAR. 
4 The Semantic Similarity Methods 
Available in SEMILAR 
The core component of SEMILAR is a set of 
text-to-text semantic similarity methods. We 
have implemented methods that handle both uni-
directional similarity measures as well as bidirec-
tional similarity measures. For instance, the se-
mantic relation of entailment between two texts 
is unidirectional (a text T logically entails a hy-
pothesis text H but H does not entail T) while the 
paraphrase relation is bidirectional (text A has 
same meaning as text B and vice versa). 
Lexical Overlap. Given two texts, the sim-
plest method to assess their semantic similarity is 
to compute lexical overlap, i.e. how many words 
they have in common. There are many lexical 
overlap variations. Indeed, a closer look at lexi-
cal overlap reveals a number of parameters that 
turns the simple lexical overlap problem into a 
large space of possibilities. The parameters in-
clude preprocessing options (collocation detec-
tion, punctuation, stopword removal, etc.), filter-
ing options (all words, content words, etc.), 
weighting schemes (global vs. local weighting, 
binary weighting, etc.), and normalization factors 
(largest text, weighted average, etc.). A total of 
3,456 variants of lexical overlap can be generat-
ed by different parameter settings in SEMILAR. 
Lintean (2011) has shown that performance on 
lexical overlap methods on the tasks of para-
phrase identification and textual entailment tasks 
can vary significantly depending on the selected 
parameters. Some lexical overlap variations lead 
to performance results rivaling more sophisticat-
ed, state-of-the-art methods. 
It should be noted that the overlap category of 
methods can be extended to include N-gram 
overlap methods (see the N-gram overlap meth-
ods proposed by the Machine Translation com-
munity such as BLEU and METEOR). SEMI-
LAR offers bigram and unigram overlap methods 
including the BLEU and METEOR scores. 
A natural approach to text-to-text similarity 
methods is to rely on word-to-word similarity 
measures. Many of the methods presented next 
compute the similarity of larger texts using indi-
vidual word similarities. 
Mihalcea, Corley, & Strappavara (2006; 
MCS) proposed a greedy method based on word-
to-word similarity measures. For each word in 
text A (or B) the maximum similarity score to 
any word in the other text B (or A) is used. An 
idf-weighted average is then computed as shown 
in the equation below. 
 
)
}
2
{
)(
}
2
{
)}(*)
1
,(max{
}
1
{
)(
}
1
{
)}(*)
2
,(max{
(
2
1
)2,1(
?
?
?
?
?
?
?
?
?
?
Tw
widf
Tw
widfTwSim
Tw
widf
Tw
widfTwSim
TTsim
 
 
The word-to-word similarity function sim(w, 
T) in the equation above can be instantiated to 
any word-to-word similarity measure (e.g. 
WordNet similarities or Latent Semantic Analy-
sis). The vast majority of word-to-word similari-
ty measures that rely on WordNet are concept-to-
concept measures and to be able to use them one 
must map words in the input texts onto concepts 
in WordNet, i.e. word sense disambiguation 
(WSD) is needed. As of this writing, SEMILAR 
addresses the issue in two simple ways: (1) se-
165
lecting the most frequent sense for each word, 
which is sense #1 in WordNet, and (2) using all 
the senses for each word and then take the max-
imum (or average) of the relatedness scores for 
each pair of word senses. We label the former 
method as ONE (sense one), whereas the latter is 
labeled as ALL-MAX or ALL-AVG (all senses 
maximum score or all senses average score, re-
spectively). Furthermore, most WordNet-based 
measures only work within a part-of-speech cat-
egory, e.g. only between nouns. 
Other types of word-to-word measures, such 
as those based on Latent Semantic Analysis or 
Latent Dirichlet Allocation, do not have a word-
sense disambiguation challenge.  
Rus and Lintean (2012; Rus-Lintean-
Optimal Matching or ROM) proposed an opti-
mal solution for text-to-text similarity based on 
word-to-word similarity measures. The optimal 
lexical matching is based on the optimal assign-
ment problem, a fundamental combinatorial op-
timization problem which consists of finding a 
maximum weight matching in a weighted bipar-
tite graph.  
Given a weighted complete bipartite graph 
, where edge  has weight 
, the optimal assignment problem is to 
find a matching M from X to Y with maximum 
weight. 
A typical application is about assigning a 
group of workers, e.g. words in text A in our 
case, to a set of jobs (words in text B in our case) 
based on the expertise level, measured by 
, of each worker at each job. By adding 
dummy workers or jobs we may assume that X 
and Y have the same size, n, and can be viewed 
as   and Y = . 
In the semantic similarity case, the weight  
is the word-to-word similarity between a word x 
in text A and a word y in text B.  
The assignment problem can also be stated as 
finding a permutation  of {1, 2, 3, ? , n} for 
which  is maximum. Such an 
assignment is called optimum assignment. The 
Kuhn-Munkres algorithm (Kuhn, 1955) can find 
a solution to the optimum assignment problem in 
polynomial time. 
Rus and colleagues (Rus et al, 2009; Rus & 
Graesser, 2006; Rus-Syntax-Negation or RSN) 
used a lexical overlap component combined with 
syntactic overlap and negation handling to com-
pute an unidirectional subsumption score be-
tween two sentences, T (Text) and H (Hypothe-
sis), in entailment recognition and student input 
assessment in Intelligent Tutoring Systems. Each 
text is regarded as a graph with words as 
nodes/vertices and syntactic dependencies as 
edges. The subsumption score reflects how much 
a text is subsumed or contained by another. The 
equation below provides the overall subsumption 
score, which can be averaged both ways to com-
pute a similarity score, as opposed to just the 
subsumption score, between the two texts.  
 
2
)
_#
)1(1(
)
||
),(max
||
),(max
(),(
relneg
h
E
eHh
E
tEh
Ematch
eTtE
h
V
vHh
V
tVh
Vmatch
vTtV
HTsubsump
??
?
?
?
?
??
?
?
?
??
?
?
 
The lexical component can be used by itself 
(given a weight of 1 with the syntactic compo-
nent given a weight of 0) in which case the simi-
larity between the two texts is just a composi-
tional extension of word-to-word similarity 
measures. The match function in the equation 
can be any word-to-word similarity measure in-
cluding simple word match, WordNet similarity 
measures, LSA, or LDA-based similarity 
measures. 
Fernando and Stevenson (FST; 2008) pro-
posed a method in which similarities among all 
pairs of words are taken into account for compu-
ting the similarity of two texts. Each text is rep-
resented as a binary vector (1 ? the word occurs 
in the text; 0 ? the word does not occur in the 
text). They use a similarity matrix operator W 
that contains word-to-word similarities between 
any two words. 
||||
),( ??
??
?
??
ba
TbWa
basim
 
Each element wij represents the word-level 
semantic similarity between word ai in text A 
and word bj in text B. Any word-to-word seman-
tic similarity measure can be used. 
Lintean and Rus (2010; weighted-LSA or 
wLSA) extensively studied methods for semantic 
similarity based on Latent Semantic Analysis 
(LSA; Landauer et al, 2006). LSA represents 
words as vectors in a 300-500 dimensional LSA 
space. An LSA vector for larger texts can be de-
rived by vector algebra, e.g. by summing up the 
individual words? vectors. The similarity of two 
texts A and B can be computed using the cosine 
(normalized dot product) of their LSA vectors. 
Alternatively, the individual word vectors can be 
166
combined through weighted sums. Lintean and 
Rus (2010) experimented with a combination of 
3 local weights and 3 global weights. All these 
versions of LSA-based text-to-text similarity 
measures are available in SEMILAR. 
SEMILAR also includes a set of similarity 
measures based on the unsupervised method La-
tent Dirichlet Allocation (LDA; Blei, Ng, & 
Jordnan, 2003; Rus, Banjade, & Niraula, 
2013). LDA is a probabilistic generative model 
in which documents are viewed as distributions 
over a set of topics (?d - text d?s distribution over 
topics) and topics are distributions over words (?t 
? topic t?s distribution over words). That is, each 
word in a document is generated from a distribu-
tion over words that is specific to each topic. 
A first LDA-based semantic similarity meas-
ure among words would then be defined as a dot-
product between the corresponding vectors rep-
resenting the contributions of each word to a top-
ic (?t(w) ? represents the probability of word w 
in topic t). It should be noted that the contribu-
tions of each word to the topics does not consti-
tute a distribution, i.e. the sum of contributions is 
not 1. Assuming the number of topics T, then a 
simple word-to-word measure is defined by the 
formula below. 
 
  
 
 
 
More global text-to-text similarity measures could 
be defined in several ways as detailed next.  
Because in LDA a document is a distribution 
over topics, the similarity of two texts needs to 
be computed in terms of similarity of distribu-
tions. The Kullback-Leibler (KL) divergence 
defines a distance, or how dissimilar, two distri-
butions p and q are as in the formula below. 
 
 
 
 
 
If we replace p with ?d (text/document d?s dis-
tribution over topics) and q with ?c 
(text/document c?s distribution over topics) we 
obtain the KL distance between two documents 
(documents d and c in our example). The KL 
distance has two major problems. In case qi is 
zero KL is not defined. Then, KL is not symmet-
ric. The Information Radius measure (IR) solves 
these problems by considering the average of pi 
and qi as below. Also, the IR can be transformed 
into a symmetric similarity measure as in the fol-
lowing (Dagan, Lee, & Pereira, 1997): 
 
 
 
The Hellinger and Manhattan distances be-
tween two distributions are two other options 
that avoid the shortcomings of the KL distance. 
Both are options are implemented in SEMILAR. 
LDA similarity measures between two docu-
ments or texts c and d can also include similarity 
of topics. That is, the text-to-text similarity is 
obtained multiplying the similarities between the 
distribution over topics (?d and ?c) and distribu-
tion over words (?t1 and ?t2). The similarity of 
topics can be computed using the same methods 
illustrated above as the topics are distributions 
over words (for all the details see Rus, Banjade, 
& Niraula, 2013). 
The last semantic similarity method presented 
in this paper is based on the Quadratic Assign-
ment Problem (QAP). The QAP method aims at 
finding an optimal assignment from words in text 
A to words in text B, based on individual word-
to-word similarity measures, while simultaneous-
ly maximizing the match between the syntactic 
dependencies of the words. 
The Koopmans-Beckmann (1957) formulation 
of the QAP problem best fits this purpose. The 
goal of the original QAP formulation, in the do-
main of economic activity, was to minimize the 
objective function QAP shown below where ma-
trix F describes the flow between any two facili-
ties, matrix D indicates the distances between 
locations, and matrix B provides the cost of lo-
cating facilities to specific locations. F, D, and B 
are symmetric and non-negative. 
 
?? ??????
n
i
n
i iib
n
j jidjifBDFQAP 1 1 )(,1 )()(,),,(min ???
 
 
The fi,j term denotes the flow between facili-
ties i and j which are placed at locations ?(i) and 
?(j), respectively. The distance between these 
locations is d?(i)?(j). In our case, F and D describe 
dependencies between words in one sentence 
while B captures the word-to-word similarity 
between words in opposite sentences. Also, we 
have weighted each term in the above formula-
tion and instead of minimizing the sum we are 
maximizing it resulting in the formulation below.  
 
?? ???????
n
i
n
i iib
n
j jidjifBDFQAP 1 1 )(,)1(1 )()(,),,(max ?????
 
 
?
?
?? T
t
tt vwvwwwLDA
1
)()(),(2 ??
),(10),( dcIRqpSIM ???
?
?
? T
i i
i
i q
ppqpKL
1
log),(
167
5 Discussion and Conclusions 
The above methods were experimented with on 
various datasets for paraphrase, entailment, and 
elaboration. For paraphrase identification, the 
QAP method provides best accuracy results 
(=77.6%) on the test subset of the Microsoft Re-
search Paraphrase corpus, one of the largest par-
aphrase datasets. 
 Due to space constraints, we have not de-
scribed all the features available in SEMILAR. 
For a complete list of features, latest news, refer-
ences, and updates of the SEMILAR toolkit 
along with downloadable resources including 
software and data files, the reader can visit this 
link: www.semanticsimilarity.org. 
 
Acknowledgments 
This research was supported in part by Institute 
for Education Sciences under award 
R305A100875. Any opinions, findings, and con-
clusions or recommendations expressed in this 
material are solely the authors? and do not neces-
sarily reflect the views of the sponsoring agency. 
References  
Androutsopoulos, I. & Malakasiotis, P. 2010. A sur-
vey of paraphrasing and textual entailment meth-
ods. Journal of Artificial Intelligence Research, 
38:135-187. 
Agirre, E., Cer, D., Diab, M., & Gonzalez-Agirre, A. 
(2012). SemEval-2012 Task 6: A Pilot on Semantic 
Textual Similarity, First Joint Conference on Lexi-
cal and Computational Semantics (*SEM), Mon-
treal, Canada, June 7-8, 2012. 
Blei, D.M., Ng, A.Y., & Jordan, M.I. 2003. Latent 
dirichlet alocation, The Journal of Machine Learn-
ing Research 3, 993-1022. 
Corley, C., & Mihalcea, R. (2005). Measuring the 
Semantic Similarity of Texts. In Proceedings of the 
ACL Workshop on Empirical Modeling of Seman-
tic Equivalence and Entailment. Ann Arbor, MI. 
Dagan, I., Glickman, O., & Magnini, B. (2004). The 
PASCAL Recognising textual entailment Chal-
lenge. In Quinorero-Candela, J.; Dagan, I.; Magni-
ni, B.; d'Alche-Buc, F. (Eds.), Machine Learning 
Challenges. Lecture Notes in Computer Science, 
Vol. 3944, pp. 177-190, Springer, 2006. 
Dolan, B., Quirk, C., & Brockett, C. (2004). Unsuper-
vised construction of large paraphrase corpora: Ex-
ploiting massively parallel news sources. In Pro-
ceedings of the 20th International Conference on 
Computational Linguistics (COLING-2004), Gene-
va, Switzerland. 
Fernando, S. & Stevenson, M. (2008). A semantic 
similarity approach to paraphrase detec-
tion, Computational Linguistics UK (CLUK 2008) 
11th Annual Research Colloquium. 
Lintean, M., Moldovan, C., Rus, V., & McNamara D. 
(2010). The Role of Local and Global Weighting in 
Assessing the Semantic Similarity of Texts Using 
Latent Semantic Analysis. Proceedings of the 23rd 
International Florida Artificial Intelligence Re-
search Society Conference. Daytona Beach, FL. 
Lintean, M. (2011). Measuring Semantic Similarity: 
Representations and Methods, PhD Thesis, De-
partment of Computer Science, The University of 
Memphis, 2011.  
Ibrahim, A., Katz, B., & Lin, J. (2003). Extracting 
structural paraphrases from aligned monolingual 
corpora In Proceedings of the Second International 
Workshop on Paraphrasing, (ACL 2003). 
Iordanskaja, L., Kittredge, R., & Polgere, A. (1991). 
Natural Language Generation in Artificial Intelli-
gence and Computational Linguistics. Lexical se-
lection and paraphrase in a meaning-text genera-
tion model, Kluwer Academic. 
McCarthy, P.M. & McNamara, D.S. (2008). User-
Language Paraphrase Corpus Challenge 
https://umdrive.memphis.edu/pmmccrth/public/Par
aphraseCorpus/Paraphrase site.htm. Retrieved 
2/20/2010 online, 2009. 
Pedersen, T., Patwardhan, S., & Michelizzi, J. (2004). 
WordNet::Similarity - Measuring the Relatedness 
of Concepts, In the Proceedings of the Nineteenth 
National Conference on Artificial Intelligence 
(AAAI-04), pp. 1024-1025, July 25-29, 2004, San 
Jose, CA (Intelligent Systems Demonstration). 
Rus, V., Lintean M., Graesser, A.C., & McNamara, 
D.S. (2009). Assessing Student Paraphrases Using 
Lexical Semantics and Word Weighting. In Pro-
ceedings of the 14th International Conference on 
Artificial Intelligence in Education, Brighton, UK. 
Rus, V., Lintean, M., Moldovan, C., Baggett, W., 
Niraula, N., Morgan, B. (2012). The SIMILAR 
Corpus: A Resource to Foster the Qualitative Un-
derstanding of Semantic Similarity of Texts, In 
Semantic Relations II: Enhancing Resources and 
Applications, The 8th Language Resources and 
Evaluation Conference (LREC 2012), May 23-25, 
Instanbul, Turkey. 
Rus, V., Banjade, R., & Niraula, N. (2013). Similarity 
Measures based on Latent Dirichlet Allocation, 
The 14th International Conference on Intelligent 
Text Procesing and Computational Linguistics, 
March 24-30, 2013, Samos, Greece. 
 
168
