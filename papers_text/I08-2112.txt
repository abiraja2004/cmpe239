Learning Named Entity Hyponyms for Question Answering
Paul McNamee
JHU Applied Physics Laboratory
11100 Johns Hopkins Road
Laurel, MD 20723-6099, USA
paul.mcnamee@jhuapl.edu
Rion Snow
Stanford AI Laboratory
Stanford University
Stanford, CA 94305, USA
rion@cs.stanford.edu
Patrick Schone
Department of Defense
Fort George G. Meade, MD 20755-6000
pjschon@tycho.ncsc.mil
James Mayfield
JHU Applied Physics Laboratory
11100 Johns Hopkins Road
Laurel, MD 20723-6099, USA
james.mayfield@jhuapl.edu
Abstract
Lexical mismatch is a problem that con-
founds automatic question answering sys-
tems. While existing lexical ontologies such
as WordNet have been successfully used to
match verbal synonyms (e.g., beat and de-
feat) and common nouns (tennis is-a sport),
their coverage of proper nouns is less ex-
tensive. Question answering depends sub-
stantially on processing named entities, and
thus it would be of significant benefit if
lexical ontologies could be enhanced with
additional hypernymic (i.e., is-a) relations
that include proper nouns, such as Edward
Teach is-a pirate. We demonstrate how a re-
cently developed statistical approach to min-
ing such relations can be tailored to iden-
tify named entity hyponyms, and how as a
result, superior question answering perfor-
mance can be obtained. We ranked candi-
date hyponyms on 75 categories of named
entities and attained 53% mean average pre-
cision. On TREC QA data our method pro-
duces a 9% improvement in performance.
1 Introduction
To correctly extract answers, modern question an-
swering systems depend on matching words be-
tween questions and retrieved passages containing
answers. We are interested in learning hypernymic
(i.e., is-a) relations involving named entities because
we believe these can be exploited to improve a sig-
nificant class of questions.
For example, consider the following questions:
? What island produces Blue Mountain coffee?
? In which game show do participants compete
based on their knowledge of consumer prices?
? What villain is the nemesis of Dudley Do-
Right?
Knowledge that Jamaica is an island, that The Price
is Right is a game show, and that Snidely Whiplash
is a villain, is crucial to answering these questions.
Sometimes these relations are evident in the same
context as answers to questions, for example, in
?The island of Jamaica is the only producer of Blue
Mountain coffee?; however, ?Jamaica is the only
producer of Blue Mountain coffee? should be suf-
ficient, despite the fact that Jamaica is an island is
not observable from the sentence.
The dynamic nature of named entities (NEs)
makes it difficult to enumerate all of their evolv-
ing properties; thus manual creation and curation
of this information in a lexical resource such as
WordNet (Fellbaum, 1998) is problematic. Pasca
and Harabagiu discuss how insufficient coverage of
named entities impairs QA (2001). They write:
?Because WordNet was not designed
as an encyclopedia, the hyponyms of con-
cepts such as composer or poet are illus-
trations rather than an exhaustive list of
instances. For example, only twelve com-
poser names specialize the concept com-
poser ... Consequently, the enhancement
of WordNet with NE information could
help QA.?
799
The chief contribution of this study is demonstrat-
ing that an automatically mined knowledge base,
which naturally contains errors as well as correctly
distilled knowledge, can be used to improve QA per-
formance. In Section 2 we discuss prior work in
identifying hypernymic relations. We then explain
our methods for improved NE hyponym learning
and its evaluation (Section 3) and apply the relations
that are discovered to enhance question answering
(Section 4). Finally we discuss our results (Section
5) and present our conclusions (Section 6).
2 Hyponym Induction
We review several approaches to learning is-a rela-
tions.
2.1 Hearst Patterns
The seminal work in the field of hypernym learn-
ing was done by Hearst (1992). Her approach was
to identify discriminating lexico-syntactic patterns
that suggest hypernymic relations. For example, ?X,
such as Y?, as in ?elements, such as chlorine and
fluorine?.
2.2 KnowItAll
Etzioni et al developed a system, KnowItAll, that
does not require training examples and is broadly
applicable to a variety of classes (2005). Starting
with seed examples generated from high precision
generic patterns, the system identifies class-specific
lexical and part-of-speech patterns and builds a
Bayesian classifier for each category. KnowItAll
was used to learn hundreds of thousands of class
instances and clearly has potential for improving
QA; however, it would be difficult to reproduce the
approach because of information required for each
class (i.e., specifying synonyms such as town and
village for city) and because it relies on submitting a
large number of queries to a web search engine.
2.3 Query Logs
Pasca and Van Durme looked at learning entity class
membership for five high frequency classes (com-
pany, country, city, drug, and painter), using search
engine query logs (2007). They reported precision
at 50 instances between 0.50 and 0.82.
2.4 Dependency Patterns
Snow et al have described an approach with several
desirable properties: (1) it is weakly-supervised and
only requires examples of hypernym/hyponym rela-
tions and unannotated text; (2) the method is suit-
able for both common and rare categories; and, (3)
it achieves good performance without post filtering
using the Web (2005; 2006). Their method relies
on dependency parsing, a form of shallow parsing
where each word modifies a single parent word.
Hypernym/hyponym word pairs where the words1
belong to a single WordNet synset were identified
and served to generate training data in the follow-
ing way: making the assumption that when the two
words co-occur, evidence for the is-a relation is
present, sentences containing both terms were ex-
tracted from unlabeled text. The sentences were
parsed and paths between the nouns in the depen-
dency trees were calculated and used as features in a
supervised classifier for hypernymy.
3 Learning Named Entity Hyponyms
The present work follows the technique described
by Snow et al; however, we tailor the approach in
several ways. First, we replace the logistic regres-
sion model with a support vector machine (SVM-
Light). Second, we significantly increase the size
of training corpora to increase coverage. This ben-
eficially increases the density of training and test
vectors. Third, we include additional features not
based on dependency parses (e.g., morphology and
capitalization). Fourth, because we are specifically
interested in hypernymic relations involving named
entities, we use a bootstrapping phase where train-
ing data consisting primarily of common nouns are
used to make predictions and we then manually ex-
tract named entity hyponyms to augment the train-
ing data. A second learner is then trained using the
entity-enriched data.
3.1 Data
We rely on large amounts of text; in all our exper-
iments we worked with a corpus from the sources
given in Table 1. Sentences that presented difficul-
ties in parsing were removed and those remaining
1Throughout the paper, use of the term word is intended to
include named entities and other multiword expressions.
800
Table 1: Sources used for training and learning.
Size Sentences Genre
TREC Disks 4,5 81 MB 0.70 M Newswire
AQUAINT 1464 MB 12.17 M Newswire
Wikipedia (4/04) 357 MB 3.27 M Encyclopedia
Table 2: Characteristics of training sets.
Pos. Pairs Neg. Pairs Total Features
Baseline 7975 63093 162528
+NE 9331 63093 164298
+Feat 7975 63093 162804
were parsed with MINIPAR (Lin, 1998). We ex-
tracted 17.3 million noun pairs that co-occurred in
at least one sentence. All pairs were viewed as po-
tential hyper/hyponyms.
Our three experimental conditions are summa-
rized in Table 2. The baseline model used 71068
pairs as training data; it is comparable to the
weakly-supervised hypernym classifier of Snow et
al. (2005), which used only dependency parse fea-
tures, although here the corpus is larger. The entity-
enriched data extended the baseline training set by
adding positive examples. The +Feat model uses ad-
ditional features besides dependency paths.
3.2 Bootstrapping
Our synthetic data relies on hyper/hyponym pairs
drawn from WordNet, which is generally rich in
common nouns and lacking in proper nouns. But
certain lexical and syntactic features are more likely
to be predictive for NE hyponyms. For example, it
is uncommon to precede a named entity with an in-
definite article, and certain superlative adjectives are
more likely to be used to modify classes of entities
(e.g., ?the youngest coach?, ?the highest peak?). Ac-
cordingly we wanted to enrich our training data with
NE exemplars.
By manually reviewing highly ranked predictions
of the baseline system, we identified 1356 additional
pairs to augment the training data. This annotation
took about a person-day. We then rescanned the cor-
pus to build training vectors for these co-occurring
nouns to produce the +NE model vectors.
Table 3: Features considered for +Feat model.
Feature Comment
Hypernym con-
tained in hyponym
Sands Hotel is-a hotel
Length in chars /
words
Chars: 1-4, 5-8, 9-16, 17+
Words: 1, 2, 3, 4, 5, 6, 7+
Has preposition Treaty of Paris; Statue of Liberty
Common suffixes -ation, -ment, -ology, etc...
Figurative term Such as goal, basis, or problem
Abstract category Like person, location, amount
Contains digits Usually not a good hyponym
Day of week;
month of year
Indiscriminately co-occurs with
many nouns.
Presence and depth
in WordNet graph
Shallow hypernyms are unlikely to
have entity hyponyms. Presence in
WN suggests word is not an entity.
Lexname of 1st
synset in WordNet
Root classes like person, location,
quantity, and process.
Capitalization Helps identify entities.
Binned document
frequency
Partitioned by base 10 logs
3.3 Additional Features
The +Feat model incorporated an additional 276 bi-
nary features which are listed in Table 3. We consid-
ered other features such as the frequency of patterns
on the Web, but with over 17 million noun pairs this
was computationally infeasible.
3.4 Evaluation
To compare our different models we created a test
set of 75 categories. The classes are diverse and
include personal, corporate, geographic, political,
artistic, abstract, and consumer product entities.
From the top 100 responses of the different learn-
ers, a pool of candidate hyponyms was created, ran-
domly reordered, and judged by one of the authors.
To assess the quality of purported hyponyms we
used average precision, a measure in ranked infor-
mation retrieval evaluation, which combines preci-
sion and recall.
Table 4 gives average precision values for the
three models on 15 classes of mixed difficulty2. Per-
formance varies considerably based on the hyper-
nym category, and for a given category, by classifier.
N is the number of known correct instances found in
the pool that belong to a given category.
Aggregate performance, as mean average preci-
sion, was computed over all 75 categories and is
2These are not the highest performing classes
801
Table 4: Average precision on 15 categories.
N Baseline +NE +Feat
chemical element 78 0.9096 0.9781 0.8057
african country 48 0.8581 0.8521 0.4294
prep school 26 0.6990 0.7098 0.7924
oil company 132 0.6406 0.6342 0.7808
boxer 109 0.6249 0.6487 0.6773
sculptor 95 0.6108 0.6375 0.8634
cartoonist 58 0.5988 0.6109 0.7097
volcano 119 0.5687 0.5516 0.7722
horse race 23 0.4837 0.4962 0.7322
musical 80 0.4827 0.4270 0.3690
astronaut 114 0.4723 0.5912 0.5738
word processor 26 0.4437 0.4426 0.6207
chief justice 115 0.4029 0.4630 0.5955
perfume 43 0.2482 0.2400 0.5231
pirate 10 0.1885 0.3070 0.2282
Table 5: Mean average precision over 75 categories.
Baseline +NE +Feat
MAP 0.4801 0.5001 (+4.2%) 0.5320 (+10.8%)
given in Table 5. Both the +NE and +Feat models
yielded improvements that were statistically signif-
icant at a 99% confidence level. The +Feat model
gained 11% over the baseline condition. The maxi-
mum F-score for +Feat is 0.55 at 70% recall.
Mean average precision emphasizes precision at
low ranks, so to capture the error characteristics at
multiple operating points we present a precision-
recall graph in Figure 1. The +NE and +Feat models
both attain superior performance at all but the lowest
recall levels. For question answering this is impor-
tant because it is not known which entities will be
the focus of a question, so the ability to deeply mine
various entity classes is important.
Table 6 lists top responses for four categories.
3.5 Discussion
53% mean average precision seems good, but is it
good enough? For automated taxonomy construc-
tion precision of extracted hyponyms is critically
important; however, because we want to improve
question answering we prefer high recall and can
tolerate some mistakes. This is because only a small
set of passages that are likely to contain an answer
are examined in detail, and only from this subset
of passages do we need to reason about potential
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall Level
P
r
e
c
i
s
i
o
n
Feat
Ent
Baseline
Figure 1: Precision-recall graph for three classifiers.
hyponyms. In the next section we describe an ex-
periment which confirms that our learned entity hy-
ponyms are beneficial.
4 QA Experiments
4.1 QACTIS
To evaluate the usefulness of our learned NE hy-
ponyms for question answering, we used the QAC-
TIS system (Schone et al, 2005). QACTIS was
fielded at the 2004-2006 TREC QA evaluations and
placed fifth at the 2005 workshop. We worked with
a version of the software from July 2005.
QACTIS uses WordNet to improve matching of
question and document words, and a resource, the
Semantic Forest Dictionary (SFD), which contains
many hypernym/hyponym pairs. The SFD was pop-
ulated through both automatic and manual means
(Schone et al, 2005), and was updated based on
questions asked in TREC evaluations through 2004.
4.2 Experimental Setup
We used factoid questions from the TREC 2005-
2006 QA evaluations (Voorhees and Dang, 2005)
and measured performance with mean reciprocal
rank (MRR) and percent correct at rank 1.
All runs made use of WordNet 2.0, and we ex-
amined several other sources of hypernym knowl-
802
Table 6: Top responses for four categories using the +Feat model. Starred entries were judged incorrect.
Sculptor Horse Race Astronaut Perfume
1 Evelyn Beatrice Longman Tevis Cup Mark L Polansky * Avishag
2 Nancy Schon Kenilworth Park Gold Cup Richard O Covey Ptisenbon
3 Phidias Cox Plate George D Nelson Poeme
4 Stanley Brandon Kearl Grosser Bugatti Preis Guion Bluford Jr Parfums International
5 Andy Galsworthy Melbourne Cup Stephen S Oswald Topper Schroeder
6 Alexander Collin * Great Budda Hall Eileen Collins * Baccarin
7 Rachel Feinstein Travers Stakes Leopold Eyharts Pink Lady
8 Zurab K Tsereteli English Derby Daniel M Tani Blue Waltz
9 Bertel Thorvaldsen * Contrade Ronald Grabe WCW Nitro
10 Cildo Meireles Palio * Frank Poole Jicky
Table 7: Additional knowledge sources by size.
Classes Class Instances
Baseline 76 11,066
SFD 1,140 75,647
SWN 7,327 458,370
+Feat 44,703 1,868,393
edge. The baseline condition added a small subset
of the Semantic Forest Dictionary consisting of 76
classes seen in earlier TREC test sets (e.g., nation-
alities, occupations, presidents). We also tested: (1)
the full SFD; (2) a database from the Stanford Word-
net (SWN) project (Snow et al, 2006); and, (3) the
+Feat model discussed in Section 3. The number of
classes and entries of each is given in Table 7.
4.3 Results
We observed that each source of knowledge benefit-
ted questions that were incorrectly answered in the
baseline condition. Examples include learning a me-
teorite (Q84.1), a university (Q93.3), a chief oper-
ating officer (Q108.3), a political party (Q183.3), a
pyramid (Q186.4), and a movie (Q211.5).
In Table 8 we compare performance on questions
from the 2005 and 2006 test sets. We assessed
performance primarily on test questions that were
deemed likely to benefit from hyponym knowledge
? questions that had a readily discernible category
(e.g., ?What film ...?, ?In what country ...?) ? but we
also give results on the entire test set.
The WordNet-only run suffers a large decrease
compared to the baseline. This is expected because
WordNet lacks coverage of entities and the baseline
condition specifically populates common categories
of entities that have been observed in prior TREC
evaluations. Nonetheless, WordNet is useful to the
system because it addresses lexical mismatch that
does not involve entities.
The full SFD, the SWN, and the +Feat model
achieved 17%, 2%, and 9% improvements in answer
correctness, respectively. While no model had ex-
posure to the 2005-2006 TREC questions, the SFD
database was manually updated based on training
on the TREC-8 through TREC-2004 data sets. It
approximates an upper bound on gains attributable
to addition of hyponym knowledge: it has an un-
fair advantage over the other models because recent
question sets use similar categories to those in ear-
lier TRECs. Our +Feat model, which has no bias
towards TREC questions, realizes larger gains than
the SWN. This is probably at least in part because it
produced a more diverse set of classes and a signif-
icantly larger number of class instances. Compared
to the baseline condition the +Feat model sees a 7%
improvement in mean reciprocal rank and a 9% im-
provement in correct first answers; both results rep-
resent a doubling of performance compared to the
use of WordNet alne. We believe that these results
illustrate clear improvement attributable to automat-
ically learned hyponyms.
The rightmost columns in Table 8 reveal that the
magnitude of improvements, when measured over
all questions, is less. But the drop off is consistent
with the fact that only one third of questions have
clear need for entity knowledge.
5 Discussion
Although there is a significant body of work in auto-
mated ontology construction, few researchers have
examined the relationship between their methods
803
Table 8: QA Performance on TREC 2005 & 2006 Data
Hyponym-Relevant Subset (242) All Questions (734)
MRR % Correct MRR % Correct
WN-alone 0.189 (-45.6%) 12.8 (-51.6%) 0.243 (-29.0%) 18.26 (-30.9%)
Baseline 0.348 26.4 0.342 26.4
SFD 0.405 (+16.5%) 31.0 (+17.2%) 0.362 (+5.6%) 27.9 (+5.7%)
SWN 0.351 (+1.0%) 26.9 (+1.6%) 0.343 (+0.3%) 26.6 (+0.5%)
Feat 0.373 (+7.4%) 28.9 (+9.4%) 0.351 (+2.5%) 27.3 (+3.1%)
for knowledge discovery and improved question-
answering performance. One notable study was con-
ducted by Mann (2002). Our work differs in two
ways: (1) his method for identifying hyponyms was
based on a single syntactic pattern, and (2) he looked
at a comparatively simple task ? given a question
and one answer sentence containing the answer, ex-
tract the correct named entity answer.
Other attempts to deal with lexical mismatch in
automated QA include rescoring based on syntactic
variation (Cui et al, 2005) and identification of ver-
bal paraphrases (Lin and Pantel, 2001).
The main contribution of this paper is showing
that large-scale, weakly-supervised hyponym learn-
ing is capable of producing improvements in an end-
to-end QA system. In contrast, previous studies have
generally presented algorithmic advances and show-
cased sample results, but failed to demonstrate gains
in a realistic application. While the hypothesis that
discovering is-a relations for entities would improve
factoid QA is intuitive, we believe these experiments
are important because they show that automatically
distilled knowledge, even when containing errors
that would not be introduced by human ontologists,
is effective in question answering systems.
6 Conclusion
We have shown that highly accurate statistical learn-
ing of named entity hyponyms is feasible and that
bootstrapping and feature augmentation can signif-
icantly improve classifier accuracy. Mean aver-
age precision of 53% was attained on a set of 75
categories that included many fine-grained entity
classes. We also demonstrated that mining knowl-
edge about entities can be directly applied to ques-
tion answering, and we measured the benefit on
TREC QA data. On a subset of questions for
which NE hyponyms are likely to help we found that
learned hyponyms generated a 9% improvement in
performance compared to a strong baseline.
References
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-Seng
Chua. 2005. Question answering passage retrieval using
dependency relations. In SIGIR 2005, pages 400?407.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana M.
Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld,
and Alexander Yates. 2005. Unsupervised Named-Entity
Extraction from the Web: An Experimental Study. Artificial
Intelligence, 165(1):191?134.
Christine Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Marti A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In ACL 1992, pages 539?545.
Dekang Lin and Patrick Pantel. 2001. Discovery of inference
rules for question-answering. Natural Language Engineer-
ing, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of minipar.
In Workshop on the Evaluation of Parsing Systems.
Gideon S. Mann. 2002. Fine-grained proper noun ontolo-
gies for question answering. In COLING-02 on SEMANET,
pages 1?7.
Marius Pasca and Benjamin Van Durme. 2007. What you seek
is what you get: Extraction of class attributes from query
logs. In IJCAI-07, pages 2832?2837.
Marius Pasca and Sanda M. Harabagiu. 2001. The informa-
tive role of wordnet in open-domain question answering. In
Proceedings of the NAACL 2001 Workshop on WordNet and
Other Lexical Resources.
Patrick Schone, Gary Ciany, Paul McNamee, James Mayfield,
and Thomas Smith. 2005. QACTIS-based Question An-
swering at TREC 2005. In Proceedings of the 14th Text RE-
trieval Conference.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. Learn-
ing syntactic patterns for automatic hypernym discovery. In
NIPS 17.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006. Seman-
tic taxonomy induction from heterogenous evidence. In ACL
2006, pages 801?808.
804
