THE REASONER AND THE INFERENCER 
DON'T TALK MUCH TO EACH OTHER 
Robert P. Abelson 
Yale University 
In this paper, I wish to point out an 
important problem which has largely escaped 
notice in the field(s) of natural language 
processing~ Curiously, it is implicit in 
the topic title of this session, "Reasoning 
and Inference"~ The fact that two terms 
were used in the title instead of one 
implies a meaningful  dist inct ion between two 
different types of intel l igent processing~ 
The dict ionary definit ions of reasoning 
and inference are not much help in making a 
sharp distinction, but in actual use in 
art i f ic ia l  intel l igence work these are two 
quite different traditions, each laying 
claim to one of the terms~ Reasoning refers 
to the processing of abstract proposit ions 
to reach abstract conclusions~ Reasoning is 
formal; it is based on the use of logical or 
mathematical  rules, or special devices in a 
part icular problem-solving domain~ 
Inferencing, on the other hand, refers to 
the processing of concrete facts to reach 
concrete conclusions~ It is informal and 
"common-sensical"  (whatever we quite mean by 
that)~ It includes the curren~ vein of work 
on "frames" (Minsky, 1974; Winograd, 1975) 
or "scripts" (Schank & Abelson, 1975) or 
"conceptual overlays" (Rieger, 1975a) in 
which input patterns are matched to big 
memory structures from which further 
expectat ions are read out~ 
It is conceivable that one of these two 
approaches is "right" and the other "wrong". 
It is also conceivable that the two 
approaches are really the same, if only we 
were smart enough to see how~ Much more 
likely, mechanisms of (at least!) two rather 
different general types can coexist within 
the same intel l igent system, each taking 
over the processing burden in its 
appropr iate contexts~ 
The question I want to raise concerns 
th~ interface between reasonin~ and 
in~erencing~ I will init ial ly explore 
examples of this interface in the human 
mind, and then comment with respect to 
art i f ic ia l  intel l igence. My very tentative 
conclusions will be that psychological  
reasoning and inference processes are 
relat ively insulated from one another; that 
this has unfortunate, or at any rate 
pecul iar consequences for human thinking; 
that it is tempting to want to design 
art i f ic ia l  intel l igence systems wherein the 
reasoner and the inferencer talk to each 
other more, although it is very unclear how 
best to do this; thus this problem may 
constitute a major agenda item for 
art i f ic ia l  intel l igence in the next decade. 
M~xin~ abstract aD~ concrete information 
In the course of human affairs, there 
often seem to be cases where lip service is 
paid to abstract values (say, racial 
integration) but behavior is by contrast 
responsive to concrete concerns (say, being 
3. 
fearful for one's own children). As 
pol it ical  scientist Robert Dahl puts it, 
"lit is\] a common tendency ... of 
mankind ~ to qual i fy universals in 
appl icat ion while leaving them intact in 
rhetorics" On the face of it, this contrast 
appears hypocrit ical~ More charitably, 
however, it may in some cases represent a 
natural di f f iculty in applying abstract 
principles to concrete  cases~ Recent 
experimental  studies have demonstrated such 
a di f f iculty very strikingly. 
Several studies by Kahneman and Tversky 
(1973) have dealt with the use of abstract 
"base rate" information~ Subjects are given 
a personal background descript ion of an 
individual and are asked to rate the 
l ikel ihood that he chose one or another of 
two careers~ For example: 
Jack is a 45-year-old man~ He is 
married and-has four children~ He is 
general ly conservative, careful, and 
ambitious. He shows no interest in 
pol it ical  and social issues and spends 
most of his free time on his many 
hobbies which include home carpentry, 
sailing, and mathematical  puzzles. 
This background biography is written so as 
to suggest strongly the stereotypic 
inference that one occupation is much more 
l ikely than the other (above, engineer). 
Now suppose that base rate information 
is introduced. Some of the subjects are 
told that the given biography was randomly 
sampled from a populat ion of individuals 
with a fixed ratio of the two career 
choices, say 70% lawyers and 30% engineers. 
Rationally, this base rate information ought 
to make the more densely represented 
occupation more l ikely to be the correct 
character izat ion of any single individual.  
Yet it turns out that subjects given base 
rate information make occupational 
predict ions no different from subjects given 
no base rate information or contrary base 
rate information. That subjects might not 
understand the meaning of the base rate 
information cannot be the explanation of 
this irrational result. A separate group of 
subjects given only base rate information 
and no biography correctly applies the 
statist ical  odds from the population 
proportions to the single case. 
The explanation for this effect seems 
to be that the concrete impression from the 
biography subject ively wipes out the 
abstract statist ical information. It is as 
though the inferential  program scanning the 
biography as a single case has no way to 
borrow from or communicate with the 
reasoning program which scanned the base 
rate information. This explanation gains 
credence from the further f inding that the 
biography doesn't necessari ly have to be 
strongly stereotypic to wipe out the 
inf luence of base-rate information. 
At ~a recent conference at 
Carnegie-Mel lon on "Cognition and Social 
Behavior", papers by Abelson (1975a), 
Slovic, Fischhoff, and Lichtenstein (1975) 
and Nisbett, Borgida, Crandall, and Reed 
(1975) discussed several related phenomena. 
Nisbett et al gave a number of str iking 
experimental findings, and the fol lowing 
powerful anecdotal example: 
"Suppose you wish to buy a new car and 
have deczded that ... you want to 
purchase ... either a Volvo or a Saab. As a 
prudent and sensible buyer, you go to 
Consumer Reports, which informs you that the 
consensus of their experts is that the Volvo 
is mechanical ly superior, and the consensus 
of the readership is that the Volvo has the 
better repair record. Armed with this 
information, you decide to go and strike a 
bargain with the Volvo dealer before the 
week is out. In the interim, however, you 
go to a cocktail  party where you announce 
this intention to an acquaintance~ He 
reacts with disbel ief and alarm: "A Volvo! 
You've got to be kidding. My brother- in- law 
had a Volvo. First, that fancy fuel 
injection computer thing went out~ 250 
bucks. Next he started having trouble with 
the rear end~ Had to replace it. Then the 
transmission and the clutch~ Final ly he 
sold it in three years for junk~ The 
logical status of this information is that 
of the N of several hundred Volvo-owning 
Consumec Reports readers has been increased 
by one, and the mean frequency-of-repair  
record shifted up by an iota on three or 
four dimensions~ But anyone who maintains 
that he would reduce the encounter to such a 
net informational effect is either 
disingenuous .or lacking in the most 
elemental sel f -knowledge~" 
If we accept this example as indicative 
of a general phenomenon, it may seem to 
imply that people do not use statist ical  
information properly~ However, that's not 
quite it, because if the protagonist of the 
story had not gone to the cocktai l  party, he 
would have used the Consumer Report~ 
information (properly) to buy a Volvo~ 
Rather, like the experiment with the 
biographies, it appears that there is an 
inabi l i ty to translate information from one 
mode to the other~ If you have an 
abstract ion (a good statist ical  repair 
record) without direct episodes to 
instantiate it, and then in another context 
you are given an episode implying but not by 
itself proving the contrary abstraction, it 
is not so easy to put these pieces of 
information together. You cannot 
comfortably add the new episode to the 
instance set stored under the old 
abstraction; it's the only instance, and it 
doesn't support the abstraction. There is 
no commensurabi l i ty of information, either 
at a concrete or at an abstract level. How 
would a cleverly designed art i f ic ial  
knowledge system cope with such a case? I do 
not know. 
Consider another psychological  
phenomenon involving more reasoning than the 
above example, but stil l  pitt ing abstract 
against concrete information. In the lingo 
of social psychology's ,,attribution theory" 
(Jones, et al, 1972), both ,,consensus" 
information and ,,distinctiveness" 
information are useful in assigning causal 
responsib i l i ty  for an event~ Given a 
statement about an actor's response to a 
st imulus ("Mary ran away from the dog"), it 
is possible to locate the cause either 
pr imari ly  in the actor ("Mary was fearful") 
or in the stimulus ("The dog was 
fr ightening").  From simple reasoning 
(setting aside compl icat ing factors) it 
follows that if the given actor responds 
non-dist inct ively to other similar stimuli 
("Mary is afraid of other dogs") and there 
is non-consensus from other actors toward 
this stimulus ("No one else is afraid of 
this dog") then it is something about Mary 
which has caused the event. On the other 
hand, if the given actor responds 
dist inct ively to other stimuli ("Mary is not 
afraid of other dogs"), and there is 
consensus from other actors toward this 
st imulus ("Everyone else is afraid of this 
dog"), then it is something about the dog 
which has caused the event~ 
In a formal logical sense, consensus 
and dist inct iveness information are 
symmetric and equivalent~ However, it has 
been found in quest ionnaires posing events 
and asking for attr ibut ions (McArthur, 1972) 
that dist inct iveness information (what this 
actor does with other stimuli) is more 
powerful than consensus information (what 
other actors do with this st imulus)~ In 
fact, consensus information is cur iously 
weak in a number of contexts~ 
Miller, Gillen, Schenker, and Radlove 
(1973) asked college students to read the 
procedure section of the classic Mi lgram 
(1963) study of obedience, in which subjects 
were found wi l l ing to administer 
overwhelming doses of electric shock to 
another person~ Half the subjects were 
given the actual data of the Mi lgram study 
showing that all subjects administered 
substantial  shocks and a clear major i ty 
continued throughout the experiment, even 
beyond the point of apparent danger to the 
vict im's life~ The other half of the 
subjects were left with the naive--and 
typica l - -expectat ion that such behavior 
would be quite rare~ Then all subjects were 
asked to rate two individuals, both of whom 
had administered maximal shock, on eleven 
evaluat ively loaded trait dimensions such as 
warmth, aggressiveness,  etc~ 
Social psychologists have learned to 
interpret the apparent ly vicious behavior of 
Mi lgram's subjects as being due to the 
subtly very powerful s i tuat ional  pressures 
toward obedience, pressures with universal  
effect transcending personal values~ If 
everybody does something bad, it is not as 
logical to fault the given individual who 
does it as it would be if he were one of 
only a few individuals who did it~ 
Nevertheless, logic does not prevail  in this 
judgment~ The Mil ler et al subjects given 
the true consensus information judge the two 
shock-givers in v irtual ly equivalent ly 
wicked terms as do subjects not given this 
information~ Somehow subjects are not able 
to adjust their concretely moral ist ic  
interpretat ions to take account of the base 
rate of the behavior in question~ The value 
inference is impervious to stat ist ical  
4. 
reasoning. 
It may seem that these examples are 
special  in some way, and that perhaps the 
reasoner and the inferencer do converse in 
other contexts. I want to turn now to 
another, rather different context which the 
art i f ic ia l  intel l igence worker will find 
somewhat more familiar, but where the same 
puzzle arises. 
How abstraet ly should a planner plan? 
One of the vexing issues in art i f ie ial  
intel l igenee which everyone is aware of but 
not eager to discuss, is the lack of 
general i ty of problem contents. There is a 
kind of taeit agreement that it is OK for 
everyone to define his own arena or table 
top or eoneeptual eorpus, and do his thing 
within that context without worrying yet 
about extrapolation. 
I can understand the arguments for this 
permissive att i tude--the necessity to start 
somewhere, the danger of doing nothing by 
worrying about everything, and so on--yet 
somehow it stil l  bothers me. Thus I note 
that Terry Winograd's (1972) tour de foree 
with the "blocks world" is applauded for its 
cleverness, but no one to my knowledge has 
tried to general ize any of the,mechanisms to 
other problems. 
Yet other problems arise which seem 
somehow very closely related. For example, 
Charniak's (1975) paper at this conference 
raises questions about the "supermarket 
frame"~ The shopper must get items to the 
checkout counter with the sometime 
instrumental i ty of a shopping cart. It 
seems clear that there is an analogy between 
the planned transport of a grocery item to a 
checklist counter, enabled by the  prior 
placement of that item in a mobile cart, and 
the planned transfer of a block to an empty 
spot on a table top, enabled by the prior 
grasping of that block by a mobile robot 
arm. To be sure, there are some dif ferences 
in side constraints (Shrdlu's arm can only 
hold one thing at a time, while the cart can 
hold many), but it is the similar it ies which 
interest us, even at this very. simple 
analogic level~ 
Is there any way to take advantage of 
such similar it ies in the design of a more 
abstract problem understander? Yes, in my 
view. In a recent paper (Abelson, 1975b), I 
attempted to develop a set of intention 
primit ives as bui lding blocks for plans~ 
Each primit ive is an act package causing a 
state change. Each state change helps 
enable some later action, such that there is 
a chain or lattice of steps from the init ial 
states to the goal state the actor desires. 
These act primit ives we call 
"deltacts". They ~re dist inguished by the 
state they change, and are notated by the 
symbol ~ preceding a state name. There are 
nine deltacts in my system, designed 
(hopefully) such that they could be applied 
to almost any natural world content~ (Some 
of these deltacts make cameo appearances in 
a Sehank (1975) paper in this volume, 
5. 
embel l ished with his idea of planboxes, and 
with a new deltact added~ He and I have 
been. trying to work out ways to graft the 
deltacts into an expanded Conceptual  
Dependency formalism)~ For purposes of the 
present paper, it is not necessary to 
explain the entire set of deltacts, but 
merely to focus  on a portion of the 
machinery needed for the robot-arm and 
shopping basket cases~ 
Consider first the abstract goal (or 
subgoal) that an object X be located at a 
certain place Y. In our abstract system, 
the achievement o f  this goal requires the 
deltact ~PROX, a change in the proximity 
relat ions of an object. This deltact has 
five arguments: the actor A, the object X, 
the object's start ing place Z, the final 
place Y, and the means M~ In turn, this 
deltact has a set of enabling states, which 
may have to be achieved by other deltacts. 
The Abelson (1975b) paper considers the 
general case where the~PROX may be achieved 
via a "carrier system" (say, an airplane) 
run by agents other than the main actor. If 
we ignore this unnecessary compl icat ion 
(along with one other) here, the enabl ing 
states for ~PROX may be listed as follows, 
where "I" denotes an instrumental device 
(such as a shopping cart) used for means M~ 
a) PROX(A,Z)--the actor must be at the 
start ing point; 
b) PROX(X,Z)--the object is at the same 
starting point; 
c) HAVE(I ,A)- - the actor must have 
instrumental device; 
the 
d) UNIT(X, I ) - - the object must be in a 
"unit relation" with the instrumental  
device; 
e) OKFOR(I ,M)--the device must be in good 
condit ion for the transportat ion means~ 
These enablements are very general~ 
They must be satisf ied no matter what the 
content~ Perhaps the level of general i ty is 
too high, so that some of the states such as 
OKFOR may have a feel ing of kluge about them 
(as Rieger (1975b) wonders), but this is not 
a problem for other states such as UNIT 
which have rather clean properties. (There 
are two variants of UNIT: nested ("in", or 
"on"), and joined ("with"). Various nice 
axioms characterize objects in UNIT 
relation, for example, common fate from 
~PROX; transit iv ity of nesting, etc~) 
Concret iz ing these condit ions to the 
two applications: 
Robot arm moving block: The arm 
must be at the place where the 
block is; the arm must have its 
"hand" (satisfied by definit ion); 
the object must be put in unit 
relation with the hand; the hand 
must not be damaged (satisfied by 
definit ion. 
Shoppgr moving an item: The shopper 
must be at the place where the item 
is; the shopper must have (say) a 
cart; the item must be put in unit 
relation with the cart; the cart 
must be all right for whatever it 
is that carts do (roll)~ 
In placing these two appl ications in 
parallel under a common abstract rubric, we 
clearly see their s imilarit ies and 
differences. The robot problem is 
s impl i f ied because its hand is always there, 
undamaged~ To satisfy the remaining two 
states enabling the desired ~PROX,  it is 
necessary to get the arm to where the block 
is (another ~PROX, or in Winograd's terms, 
MOVE), and to join the arm to the block (a 
~UNIT  in our terms, or GRASP in the 
original)~ The supermarket problem is one 
more complex because the shopper may not 
have a cart, or may have a damaged one~ If 
he doesn't have one, he must execute a 
~HAVE, and if it is damaged, he might 
execute a ~OKFOR (fix it) or a ~HAVE of 
another~ Additionally, he must get to where 
the item is (~PROX) and nest the item in the 
cart (~UNIT)~ 
Each of these prior deltacts has its 
own abstract list of enablements~ For a 
~UNIT,  these include a couple of OKFORs 
which for grasping are not so klugey: for 
the robot, they map into the condit ions that 
the arm be empty and the block have a clear 
top. For a ~HAVE (which can be achieved 
either by taking the object -  or having 
someone give it to you), the enablements 
include other PROX and UNIT conditions, etc~ 
With a wel l -designed system of deltacts 
(which mine probably is not, because of 
various loopholes), it ought to be possible 
to have a planner (or an understander of the 
plans of others) reason appropr iately at a 
level well above the specif ic set of content 
bindings of a part icular problem frame or 
script~ Apart from the academic theoretical  
interest of such an abstract system, would 
it ever be useful to human or art i f ic ial  
intel l igence working in a given practical  
context? 
Well, suppose something went wrong in 
the execution of the usual plans in the 
context~ Suppose, for example, that 
Shrdlu's hand were damaged so that it 
couldn't grasp the smallest blocks--could it 
ever move them? Or suppose that the shopper 
absolutely couldn't find a cart~ 
In these cases, one could imagine the 
inferencer (say, of the shopper) contact ing 
the reasoner for help, "Do something! Get 
through your such-thats and Use a theorem or 
something~ I'm in trouble here without a 
cart~" And the reasoner might say, "Well 
let's see, you ask for an instrumental 
device such that you can have it and it's OK 
for forming unit connections with a number 
of these--what did you call them?--grocery 
items, in order to move them a short 
distance? Well, how big and heavy are these 
grocery items, and how soon do you want an 
answer?" And three minutes later the 
6. 
reasoner would come back and say, "Since 
it's an emergency, I' l l be more specif ic 
than I usually like to be~ I recommend 
something flat, with a big surface area, 
that you already have and that could be 
pushed or pulled by hand~ Have you got 
anything on your person?" 
"Well, I have this overcoat~" 
"Oh, good~ Yes, use your overcoat to pull 
the grocery items along the floor to 
the check-out counter~" 
"What?" (Horrified) "But the overcoat will 
get f i lthy!" 
"Sorry~ That's not my department~ That's 
a low-level inference~ I thought you 
wanted high- level  reasonings" 
An alternat ive fantasy is that the 
reasoner might be so busy working out 
abstract puzzles with its abstract 
mechanisms (say, running through a UNIT 
exercise on the Towers of Hanoi problem) 
that it wouldn't be interested in troubl ing 
with a si l ly appl ied problem~ "Go away and 
don't bother me! I'm thinking about the 
Towers of Hanoi~ What do I know from 
groceries?l"  
Reprise 
The argument I have stated as a devil 's 
advocate goes back at least to Wi l l iam 
James, writ ing in 1890 of the several 
"worlds" of the mind: 
"Every object we think of gets at last 
referred to one world or another~I t  
sett les into our bel ief as a 
common-sense object, a scientif ic 
object, an abstract object, a 
mythological  object, an object of 
someone's mistaken conception, or a 
madman's ~ object, and it reaches this 
s ta te~of ten  only after being hustled 
and bandied about amongst other 
objects until  it f inds some which will 
tolerate its presence and stand in 
relat ions to it which nothing 
contradicts~ The molecules and 
ether -waves of the scienti f ic world, 
for example, simply kick the object's 
warmth and color out, they refuse to 
have any relat ions with them~\[The\ ]  
world of classic myth takes up the 
winged horse ; .~the  world of abstract 
truth, the proposit ion that justice is 
kingly, though no actual king be just~ 
The various worlds themselves, 
however, appear (as aforesaid) to most 
men's minds in no very def in i te ly 
conceived relation to each other, and 
our attention, when it turns to one, 
is apt to drop the others for the time 
being out of its account~ 
Proposit ions concerning the different 
worlds are made from "different points 
of view'; and in this more or less 
chaotic state the consciousness of 
most thinkers remains to the end~ 
Each world whilst it is attended t_oo is 
real after its own fashion; only the 
reality lapses with the attention." 
(James, 1950, p~ 293)~ 
Is James right? If not, wherein? If so, 
is that how artificial intelligence--which 
possibly has design options not available to 
the human mind--would like to keep it? 
References 
Abelson, R~P~ Script processes in attitude 
formation and decision-making~ Presented 
at the Eleventh Carnegie Symposium on 
Cognition, Carnegie-Mellon University, 
April, 1975~ (a) 
Abelson, R~P~ Concepts for representing 
mundane reality in plans~ In D~ Bobrow 
and At Collins (Eds~), Representation 
and Understanding~ New York: Academic 
Press, 1975. (b) 
Charniak, Et Organization and inference in 
a frame-like system of common knowledge~ 
In Theoretical Issues in Natural Language 
Processing~ Proceedings of conference at 
the Massachusetts Institute of 
Technology, June, 1975t 
James, W~ (1890) Principles of Psychology. 
Vol~ 2~ New York: Dover ed'ition, 1950. 
Jones, E~E~, Kanouse, D~E~, Kelley, H.H~, 
Nisbett, RtE~, Valins, St, & Weiner, B~ 
Attribution: Perceiving the Causes of 
Hehavior. General Learning Press, 1972. 
Kahneman, D~, & Tversky, A~ On the 
psychology of prediction~ Psychological 
Review, 1973, 80, 237-251~ 
McArthur, LtA~ The how and what of why: 
Some determinants and consequences of 
causal attribution~ Journal of 
Personality and Social Psychology, 1972, 
22, 171-193t 
Milgram, S. Behavioral study of obedience. 
Journal of Abnormal and Social 
Psychology, 1963, 67, 371-378~ 
Miller, A~G~, Gillen, B., Schenker, C., & 
Radlove, S~ Perception of obedience to 
authority~ Proceedings. 81s . t  Annual 
Conventioq of the American Psychological 
Associatiqn, 1973, 127-128~ 
Minsky, M~ A framework for representing 
knowledge~ MIT AI Memo 306~ June, 1974~ 
Nisbett R.E~, Borgida, E~, Crandall, R~, & 
Reed, H~ Popular induction: Information 
is not necessarily informative. 
Presented at the Eleventh Carnegie 
Symposium on Cognition, Carnegie-Mellon 
University, April, 1975. 
Rieger, C~ Conceptual overlays: A mechanism 
for the interpretation of sentence 
meanings in context. To appear in the 
Proceedings of the 4th International 
Joint Conference on Artificial 
Intelligence~ Tbilisi, 1975. (a)~ 
7. 
Rieger, C~ The commonsense algorithm as a 
basis for computer models of human 
memory, inference, belief, and contextual 
language comprehension~ In Theoretical 
Issues in Natura\] Language P__roqessing~ 
Proceedings of conference at the 
Massachusetts Inst i tute  of Techn~logy, 
June, 1975~ (b) 
Schank, R.C. Using knowledge to understand~ 
In Theoretical Issues in Natural Language 
processing~ Proceedings of conference at 
the Massachusetts Institute of 
Technology, June, 1975. 
Schank, R~C~, & Abelson, R.P~ Scripts, 
plans, and knowledge~ Prepared for 
presentation at the 4th International 
Joint Conference on Artificial 
Intelligence. Tbilisi, 1975. 
Slovic, P~, Fischhoff, B~, & Lichtenstein, 
S. Cognitive processes and societal risk 
taking~ Presented at the Eleventh 
Carnegie Symposium on Cognition, 
Carnegie-Mellon University, April, 1975~ 
Winograd, 
Language. 
1972~ 
Tt 9nderstanding Natural 
New York: Academic Press, 
Winograd, T~ Frame representations and the 
declarative/procedural controversy~ In 
D~ Bobrow & A. Collins (Eds~), 
~epresentation and Understanding. New 
York: Academic Press, 1975~ 
