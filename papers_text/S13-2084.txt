Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 508?512, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
senti.ue-en: an approach for informally written short texts
in SemEval-2013 Sentiment Analysis task
Jose? Saias
DI - ECT - Universidade de E?vora
Rua Roma?o Ramalho, 59
7000-671 E?vora, Portugal
jsaias@uevora.pt
Hila?rio Fernandes
Cortex Intelligence
Rua Sebastia?o Mendes Bolas, 2 K
7005-872 E?vora, Portugal
hilario.fernandes@cortex-intelligence.com
Abstract
This article describes a Sentiment Analysis
(SA) system named senti.ue-en, built
for participation in SemEval-2013 Task 2, a
Twitter SA challenge. In both challenge sub-
tasks we used the same supervised machine
learning approach, including two classifiers in
pipeline, with 22 semantic oriented features,
such as polarized term presence and index,
and negation presence. Our system achieved
a better score on Task A (0.7413) than in the
Task B (0.4785). In the first subtask, there is
a better result for SMS than the obtained for
the more trained type of data, the tweets.
1 Introduction
This paper describes the participation of a group
led by Universidade de E?vora?s Computer Science
Department in SemEval-2013 Task 2 (Wilson et
al., 2013), using senti.ue-en system. Having
previous experience in NLP tasks, such as ques-
tion answering (Saias, 2010; Saias and Quaresma,
2012), this was the authors first attempt to imple-
ment a system for Sentiment Analysis (SA) in En-
glish language. We have a recent work (Fernandes,
2013) involving SA but it is geared towards Por-
tuguese language, and thought for regular text. It
was based on rules on the outcome of linguistic anal-
ysis, which did not work well for tweets, because the
morphosyntactic analyzer misses much, due to the
abundance of writing errors, symbols and abbrevia-
tions. Moreover, in that work we began by detecting
named entities and afterwards classify the sentiment
being expressed about them. For SemEval the goal
is different, being target-independent. In both A and
B subtasks, systems must work on sentiment polar-
ity, in a certain context or full message, but the target
entity (or the opinion topic) will not appear in the
output. Thus, we have decided that senti.ue-en
system would be implemented from scratch, for En-
glish language and according to the objectives of this
challenge, in particular the Task B.
2 Related Work
Microblogging and social networks are platforms
where people express opinions. In recent years
many papers have been published on social me-
dia content SA. Pang et al (2002) applied machine
learning based classifiers for sentiment classification
on movie reviews. Their experimental results using
Naive Bayes, Maximum Entropy, and Support Vec-
tor Machines (SVM) algorithms achieved best re-
sults with SVM and unigram presence as features.
Some target-dependent approaches are sensitive to
the entity that is receiving each sentiment. A sen-
tence can have a positive sentiment about an entity
and a negative for another. Such classification can
be performed with rules on the occurrence of nouns,
verbs and adjectives, as done in (Nasukawa and Yi,
2003). It is common to use parsers and part-of-
speech tagging. Barbosa and Feng (2010) explore
tweet writting details and meta-information in fea-
ture selection. Instead of using many unigrams as
features, the authors propose the use of 20 features
(related to POS tags, emoticons, upper case usage,
word polarity and negation), achieving faster train-
ing and test times. A two-phase approach first clas-
508
sifies messages as subjective and objective, and then
the polarity is classified as positive or negative for
tweets having subjectivity. Groot (2012) builds a
feature vector with polarized words and frequently
occurring words being taken as predictive for Twit-
ter messages. Supervised learning algorithms as
SVM and Naive Bayes are then used to create a pre-
diction model. The work (Gebremeskel, 2011) is fo-
cused on tweets about news. Authors report an ac-
curacy of 87.78% for a three-classed sentiment clas-
sification using unigram+bigram presence features
and Multinomial Naive Bayes classifier. In Jiang et
al. (2011) work, Twitter SA starts with a query, iden-
tifying a target, and classifies sentiment in the query
result tweets, related to that target. Instead of con-
sidering only the text of a tweet, their context-aware
approach also considers related tweets and target-
dependent features. With precise criteria for the con-
text of a tweet, authors seek to reduce ambiguity and
report performance gains.
3 Methodology
As in most systems described in the literature, in
this area, our senti.ue-en system is based on su-
pervised machine learning. To handle the data for-
mat, in the input and on the outcome of the system,
we chose to use Python and the Natural Language
Toolkit (NLTK), a platform with resources and pro-
gramming libraries suitable for linguistic processing
(Bird, 2006). Task A asks us to classify the senti-
ment in a word or phrase in the context of the mes-
sage to which it belongs. For Task B, we had to
classify the overall sentiment expressed in each mes-
sage. Since tweets are short messages, we early have
chosen to apply the same system for both tasks, ad-
mitting some possible difference in training or pa-
rameterization. As the fine control of the correspon-
dence between each sentiment expression and its tar-
get entity is not sought, Task A is treated as a spe-
cial case of Task B, and our system does not con-
sider the text around the expression to classify. The
organization prepared a message corpus for training
and another to be used as a development-time eval-
uation dataset. We merged the training corpus with
the development corpus, and our development test
set was dynamically formed by random selection
of instances for each class (positive, negative and
neutral). Some tweets were not downloaded prop-
erly. For message polarity classification, we ended
up with 9191 labeled messages, which we split into
training and test sets.
Text processing started with tokenization, that was
white space or punctuation based. Some experi-
ments also included lemmatization, done with the
NLTK WordNet Lemmatizer. In the first approach
to Task B, we applied the Naive Bayes classification
algorithm using term presence features. The test set
was formed by random selection of 200 instances
of each class. After several experiments with this
system configuration, the average accuracy for the 3
classes was close to 45%. Looking for better results,
instead of the bag-of-words approach, we chose a
smaller set of semantic oriented features:
? presence of polarized term
? overall value of sentiment in text
? negation presence
? negation before polarized expression
? presence of polarized task A n-grams
? overall value of polarized task A n-grams
? overall and presence of similar to Task A n-grams
? first and last index of polarized terms
Checking for the presence of positive and negative
polarized terms produces two features for each of
the three sentiment lexicons used by our system.
AFINN (Nielsen, 2011) is a sentiment lexicon con-
taining a list of English words rated between minus
five (negative) and plus five (positive). The words
have been manually labeled by Finn A?rup Nielsen,
from 2009 to 2011. SentiWordNet (Baccianella et
al., 2010) is a lexical resource for opinion mining
that assigns sentiment scores to each synset ofWord-
Net (Princeton University, 2010). After some exper-
imentation with this resource, we decided to apply a
threshold, disregarding terms whose score absolute
value is less than 0.3. Another sentiment lexicon,
from Liu et al (2005), derived from a work on online
customer reviews of products. The overall text sen-
timent value is calculated by adding the sentiment
value in each word. This is the way chosen to handle
more than one sentiment in a single tweet. Our sys-
tem creates a separated overall sentiment value fea-
ture for AFINN, SentiWordNet and Liu?s lexicons,
because each resource uses a different range of val-
ues. Each of these features is calculated by summing
the sentiment value in each word of the text clas-
sify. Detection of denial in the text also gave rise to
a feature. Thinking in cases like ?This meal was not
509
good?, we created features for the presence of denial
before positive and negative expressions, where the
adjective?s sentiment value is inverted by negation.
In these two features, an expression is polarized if
it is included in any of the sentiment lexicons. The
training corpus for Task A included words or phrases
marked as positive or negative. We created twomore
features to signal the presence of polarized words
or n-grams in the texts to be classified. To comple-
ment, another feature accounts for the overall Task
A polarized n-grams value, adding 1 for each posi-
tive occurrence and subtracting 1 every negative oc-
currence in the tweet. Because a term can arise in
inflected form, we added another three features to
assess the same on Task A data, but accepting varia-
tions in words or expressions. Using lemmatization
and synonyms, we seek more flexibility in n-gram
verification. The last four features identify the text
token index for the first and the last occurrence, for
each sentiment flavor, positive and negative, accord-
ing to any used sentiment lexicon. Emoticons are
present in sentiment lexicons, so it was not created a
specific feature for them.
Using these 22 features with Naive Bayes, the aver-
age overall accuracy was 60%. When analyzed by
class, the lower accuracy happens on neutral class,
near 50%. Accuracy por positive class was 68%,
and for negative it was 63%. For the next iteration,
the NLTK classifier was set up for Decision Tree al-
gorithm. After several runs, we noticed that while
the overall accuracy remained identical, the poorest
results came now for the negative class, having 54%
accuracy. The run average accuracy for classes pos-
itive and neutral, was respectively 59% and 64%. In
the latest evolution the system applies two classifiers
in sequence. Each tweet is first classified with Naive
Bayes. This creates a new feature for the second
classifier, which is considered along with the previ-
ous ones by the Decision Tree algorithm. This con-
figuration led us to the best overall accuracy in the
development stage, with 62%, and was the version
applied to Task B in constrained mode.
The unconstrained mode allowed systems to use ad-
ditional data for training. The IMDB dataset (Maas
et al, 2011) contains movie reviews with their asso-
ciated binary sentiment polarity labels. We chose a
subset of this corpus consisting of 500 positive and
500 negative reviews with less than 350 characters.
T Data Mode Positive Negative Neutral
A
sms
C 0.8079 0.8985 0.1130
U 0.8695 0.9206 0.1348
twitter
C 0.9190 0.8162 0.0588
U 0.9412 0.8411 0.0705
B
sms
C 0.4676 0.4356 0.7168
U 0.4625 0.4161 0.7293
twitter
C 0.6264 0.3996 0.5538
U 0.6036 0.3589 0.5621
Table 1: senti.ue-en precision in Tasks A and B
Sanders used a Naive Bayes classifier and token-
based feature extraction to create a corpus (Sanders,
2011) for SA on Twitter. We were able to discharge
only part of the corpus, from which we selected
250 positive tweets and the same number of neg-
ative ones. In unconstrained mode, senti.ue-en
has the same configuration, but uses extra instances
from these two corpus for training.
Task A is treated with the same mechanism. The
system classifies the sentiment for the text inside the
given boundaries. Because many of these cases have
a single word, our system uses a third extra corpus
for training in unconstrained mode. Each word on
AFINN lexicon is added to training set, with pos-
itive or negative class, depending on its sentiment
value.
4 Results
We submitted our system?s result for each of the
eight expected runs. Each run was a combination
of subtask (A or B), dataset (Twitter or SMS) and
training mode (constrained or unconstrained). After
the deadline for submission, the organization evalu-
ated the results. The precision in our system?s output
is indicated in Table 1. The use of more training in-
stances in unconstrained mode leads to an improve-
ment of precision in Task A, for all classes. In Task
B we notice the opposite effect, with a slight drop in
precision for positive and negative classes, and about
1% improvement in neutral class precision. We also
note that precision has lower values in neutral class
for Task A, whereas in Task B it is the class negative
that has the lowest precision.
Table 2 shows the recall obtained for the same re-
sults. This metric also shows a gain in Task A,
for positive and negative classes using unconstrained
mode. For subtask B, the constrained mode had bet-
510
T Data Mode Positive Negative Neutral
A
sms
C 0.5341 0.5453 0.6792
U 0.6471 0.6196 0.6730
twitter
C 0.4898 0.4958 0.7500
U 0.6203 0.5704 0.7000
B
sms
C 0.5711 0.3350 0.7061
U 0.5386 0.4594 0.6556
twitter
C 0.5515 0.3245 0.6555
U 0.5280 0.4359 0.5854
Table 2: senti.ue-en recall in Tasks A and B
T Data Mode Positive Negative Neutral
A
sms
C 0.6431 0.6787 0.1937
U 0.7420 0.7407 0.2246
twitter
C 0.6390 0.6169 0.1090
U 0.7478 0.6798 0.1281
B
sms
C 0.5142 0.3788 0.7114
U 0.4977 0.4367 0.6905
twitter
C 0.5866 0.3581 0.6004
U 0.5633 0.3937 0.5735
Table 3: senti.ue-en F-measure in Tasks A and B
ter recall for positive and neutral classes. But recall
varies in the opposite direction in the negative class
when using our extra training instances.
Using the F-measure metric to evaluate our results,
we get the values in Table 3. This balanced assess-
ment between precision and recall confirms the im-
provement of results in Task A when using the un-
constrained mode. We note, for Task B, a small loss
in unconstrained mode on positive class, but that is
outweighed by the gain on the negative class.
In SemEval-2013 Task 2, the participating systems
are ranked by their score. This corresponds to the
average F-measure in positive and negative classes.
Table 4 shows the score obtained by our system. The
score is in line with our forecasts in the Task A, but
below what we wanted in Task B. Looking at Table 3
we see that positive and negative classes? F-measure
values are substantially lower than the values for
neutral class, in Task B and in both constrained and
unconstrained mode. For Task B, most correct re-
sults were in the class less relevant for the score.
5 Conclusions
With our participation in SemEval-2013 Task 2 we
intended to build a real-time SA system for the En-
glish used nowadays in social media content. This
goal was achieved and we experienced the use of im-
T Data Mode Score
A
sms
C 0.6609
U 0.7413
twitter
C 0.6279
U 0.7138
B
sms
C 0.4465
U 0.4672
twitter
C 0.4724
U 0.4785
Table 4: senti.ue-en score
portant English linguistic resources to support this
task, such as corpora and sentiment lexicons.
We had some problems detected only after the close
of submission. Lemmatization did not always work
well. In ?last index of polarized term? feature, we
noticed a problem that ironically came precisely at
the version used to submit, where the last index
was counted from the start of text, and it should be
counted from the end.
We think that the difference in system performance
between Task A and Task B has to do with the
amount of noise present in the text. Because many
of the texts to classify in Task A had a single word
or a short phrase, the system was more likely to suc-
ceed. Another reason is the fact that our system has
not been tuned to maximize the score (F-measure in
positive and negative classes). During development
we took into account only the overall accuracy seen
in NLTK classifier result. Perhaps the overall system
performance may have been affected by our deci-
sion of merge the training and the development cor-
pus as training set. We used a class balanced set for
development-time evaluation, smaller than the given
development set, and the final test set had a different
class distribution (Wilson et al, 2013).
By reviewing the system, we feel that the classifica-
tion algorithms in the pipeline system should swap.
Now we would use first the Decision Tree classi-
fier, and after, receiving an extra feature, the Naive
Bayes classifier, which as mentioned in section 3,
suggested slightly better results for positive and neg-
ative classes. For the future, we intend to evolve the
system in order to become more precise and target-
aware. For the first part we need to review and evalu-
ate the actual contribution of the current features. As
for the second, we intend to introduce named entity
recognition, so that each sentiment can be associated
with its target entity.
511
References
Andrew L. Maas, Raymond Daly, Peter Pham, Dan
Huang, Andrew Ng and Christopher Potts. 2011.
Learning Word Vectors for Sentiment Analysis. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pp: 142-150. ACL. Portland, USA.
Bing Liu, Minqing Hu and Junsheng Cheng. 2005. Opin-
ion Observer: Analyzing and Comparing Opinions
on the Web. In Proceedings of the 14th International
World Wide Web conference (WWW-2005). Chiba,
Japan.
Bo Pang, Lillian Lee, Shivakumar Vaithyanathan. 2002.
Thumbs up? Sentiment Classification using Machine
Learning Techniques. In Proceedings of EMNLP. pp:
79?86.
Finn A?rup Nielsen. 2011. A New ANEW: Evalua-
tion of a Word List for Sentiment Analysis in Mi-
croblogs. In Proceedings, 1st Workshop on Mak-
ing Sense of Microposts (#MSM2011): Big things
come in small packages. pp: 93-98. Greece.
http://arxiv.org/abs/1103.2903
Gebrekirstos Gebremeskel. 2011. Sentiment Analysis of
Twitter Posts About news. Master?s thesis. University
of Malta.
Hila?rio Fernandes. 2013. Sentiment Detection and
Classification in Non Structured Information Sources.
Master?s thesis, ECT - Universidade de E?vora.
Jose? Saias. 2010. Contextualizac?a?o e Activac?a?o
Sema?ntica na Selecc?a?o de Resultados em Sistemas
de Pergunta-Resposta. PhD thesis, Universidade de
E?vora.
Jose? Saias and Paulo Quaresma. 2012. Di@ue in
clef2012: question answering approach to the multi-
ple choice qa4mre challenge. In Proceedings of CLEF
2012 Evaluation Labs and Workshop - Working Notes
Papers, Rome, Italy.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun
Zhao. 2011. Target-dependent twitter sentiment clas-
sification. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies - Volume 1, HLT ?11,
pages 151?160. Association for Computational Lin-
guistics. USA.
Luciano Barbosa and Junlan Feng. 2010. Robust Sen-
timent Detection on Twitter from Biased and Noisy
Data. Coling 2010. pages 36?44. Beijing.
Niek J. Sanders. 2011. Sanders-Twitter Sentiment Cor-
pus. Sanders Analytics LLC
Princeton University. 2010. ?About WordNet.? WordNet.
http://wordnet.princeton.edu
Roy de Groot. 2012. Data mining for tweet sentiment
classification. Master?s thesis, Faculty of Science -
Utrecht University.
Stefano Baccianella, Andrea Esuli and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
- LREC?10. European Language Resources Associa-
tion. Malta.
Steven Bird. 2006. NLTK: the natural language toolkit.
In Proceedings of the COLING?06/ACL on Interactive
presentation sessions. Australia. http://nltk.org
Tetsuya Nasukawa, Jeonghee Yi. 2003. Sentiment analy-
sis: capturing favorability using natural language pro-
cessing. In Proceedings of the 2nd International Con-
ference on Knowledge Capture(K-CAP). USA.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan
Ritter, Sara Rosenthal and Veselin Stoyanov. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter. In
Proceedings of the 7th International Workshop on Se-
mantic Evaluation. Association for Computation Lin-
guistics.
512
