Proceedings of NAACL HLT 2009: Short Papers, pages 29?32,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Large-scale Computation of Distributional Similarities for Queries
Enrique Alfonseca
Google Research
Zurich, Switzerland
ealfonseca@google.com
Keith Hall
Google Research
Zurich, Switzerland
kbhall@google.com
Silvana Hartmann
University of Stuttgart
Stuttgart, Germany
silvana.hartmann@ims.uni-stuttgart.de
Abstract
We present a large-scale, data-driven approach
to computing distributional similarity scores
for queries. We contrast this to recent web-
based techniques which either require the off-
line computation of complete phrase vectors,
or an expensive on-line interaction with a
search engine interface. Independent of the
computational advantages of our approach, we
show empirically that our technique is more
effective at ranking query alternatives that the
computationally more expensive technique of
using the results from a web search engine.
1 Introduction
Measuring the semantic similarity between queries
or, more generally, between pairs of very short texts,
is increasingly receiving attention due to its many
applications. An accurate metric of query simi-
larities is useful for query expansion, to improve
recall in Information Retrieval systems; for query
suggestion, to propose to the user related queries
that might help reach the desired information more
quickly; and for sponsored search, where advertisers
bid for keywords that may be different but semanti-
cally equivalent to user queries.
In this paper, we study the problem of measuring
similarity between queries using corpus-based unsu-
pervised methods. Given a query q, we would like
to rank all other queries according to their similarity
to q. The proposed approach compares favorably to
a state-of-the-art unsupervised system.
2 Related work
Distributional similarity methods model the similar-
ity or relatedness of words using a metric defined
over the set of contexts in which the words appear
(Firth, 1957). One of the most common representa-
tions for contexts is the vector space model (Salton
et al, 1975). This is the basic idea of approaches
such as (Grefenstette, 1992; Bordag, 2008; Lin,
1998; Riloff and Shepherd, 1997), with some varia-
tions; e.g., whether syntactic information is used ex-
plicitly, or which weight function is applied. Most of
the existing work has focused on similarity between
single words or syntactically-correct multiword ex-
pressions. In this work, we adapt these techniques
to calculate similarity metrics between pairs of com-
plete queries, which may or may not be syntactically
correct.
Other approaches for query similarity use sta-
tistical translation models (Riezler et al, 2008),
analysing search engine logs (Jones et al, 2006),
looking for different anchor texts pointing to the
same pages (Kraft and Zien, 2004), or replacing
query words with other words that have the high-
est pointwise mutual information (Terra and Clarke,
2004).
Sahami and Helman (Sahami and Heilman, 2006)
define a web kernel function for semantic similarity
based on the snippets of the search results returned
by the queries. The algorithm used is the following:
(a) Issue a query x to a search engine and collect
the set of n snippets returned by the search engine;
(b) Compute the tf?idf vector vi for each document
snippet di; (c) Truncate each vector to include its m
29
highest weighted terms; (d) Construct the centroid
of the L2-normalized vectors vi; (e) Calculate the
similarity of two queries as the dot product of their
L2-normalized vectors, i.e. as the cosine of both
vectors.
This work was followed up by Yih and Meek (Yih
and Meek, 2007), who combine the web kernel with
other simple metrics of similarity between word vec-
tors (Dice Coefficient, Jaccard Coefficient, Overlap,
Cosine, KL Divergence) in a machine learning sys-
tem to provide a ranking of similar queries.
3 Proposed method
Using a search engine to collect snippets (Sahami
and Heilman, 2006; Yih and Meek, 2007; Yih and
Meek, 2008) takes advantage of all the optimizations
performed by the retrieval engine (spelling correc-
tion, relevance scores, etc.), but it has several disad-
vantages: first, it is not repeatable, as the code un-
derlying search engines is in a constant state of flux;
secondly, it is usually very expensive to issue a large
number of search requests; sometimes the APIs pro-
vided limit the number of requests. In this section,
we describe a method which overcomes these draw-
backs. The distributional methods we propose for
calculating similarities between words and multi-
word expressions profit from the use of a large Web-
based corpus.
The contextual vectors for a query can be col-
lected by identifying the contexts in which the query
appears. Queries such as [buy a book] and [buy
some books] are supposed to appear close to simi-
lar context words in a bag-of-words model, and they
should have a high similarity. However, there are
two reasons why this would yield poor results:
First, as the length of the queries grows, the prob-
ability of finding exact queries in the corpus shrinks
quickly. As an example, when issuing the queries
[Lindsay Lohan pets] and [Britney Spears pets] to
Google enclosed in double quotes, we obtain only
6 and 760 results, respectively. These are too few
occurrences in order to collect meaningful statistics
about the contexts of the queries.
Secondly, many user queries are simply a concate-
nation of keywords with weak or no underlying syn-
tax. Therefore, even if they are popular queries, they
may not appear as such in well-formed text found
in web documents. For example, queries like [hol-
lywood dvd cheap], enclosed in double quotes, re-
trieve less than 10 results. Longer queries, such as
[hotel cheap new york fares], are still meaningful,
but do not appear frequently in web documents.
In order to use of distributional similarities in the
query setting, we propose the following method.
Given a query of interest p = [w1, w2, ..., wn]:
1. For each word wi collect all words that appear
close to wi in the web corpus (i.e., a bag-fo-
words models). Empirically we have chosen
all the words whose distance to wi is less or
equal to 3. This gives us a vector of context
words and frequencies for each of the words in
the query, ~vi = (fi1, fi2, ..., fi|V |), where |V | is
the size of the corpus vocabulary.
2. Represent the query p with a vector of words,
and the weight associated to each word is the
geometric mean of the frequencies for the word
in the original vectors:
~qv =
0
B
@
0
@
|n|Y
i=1
fi1
1
A
1
n
,
0
@
|n|Y
i=1
fi2
1
A
1
n
, ...,
0
@
|n|Y
i=1
fi|V |
1
A
1
n
1
C
A
3. Apply the ?2 test as a weighting function test to
measure whether the query and the contextual
feature are conditionally independent.
4. Given two queries, use the cosine between their
vectors to calculate their similarity.
The motivations for this approach are: the geo-
metric mean is a way to approximate a boolean AND
operation between the vectors, while at the same
time keeping track of the magnitude of the frequen-
cies. Therefore, if two queries only differ on a very
general word, e.g. [books] and either [buy books]
or [some books], the vector associated to the general
words (buy or some in the example) will have non-
zero values for most of the contextual features, be-
cause they are not topically constrained; and the vec-
tors for the queries will have similar sets of features
with non-zero values. Equally relevant, terms that
are closely related will appear in the proximity of a
similar set of words and will have similar vectors.
For example, if the two queries are Sir Arthur Co-
nan Doyle books and Sir Arthur Conan Doyle nov-
els, given that the vectors for books and novels are
expected to have similar features, these two queries
30
Contextual word acid fast bacteria Query
acidogenicity 11 6 4 6.41506
auramin 2 5 2 2.71441
bacillae 3 10 4 4.93242
carbolfuchsin 1 28 2 8.24257
dehydrogena 5 3 3 3.55689
diphtheroid 5 9 92 16.05709
fuchsine 42 3 4 7.95811
glycosilation 3 2 3 2.62074
Table 1: Example of context words for the query [acid fast bacteria].
will receive a high similarity score.
On the other hand, this combination also helps in
reducing word ambiguity. Consider the query bank
account; the bag-of-words vector for bank will con-
tain words related to the various senses of the word,
but when combining it to account only the terms that
belong to the financial domain and are shared be-
tween the two vectors will be included in the final
query vector.
Finally, we note that the geometric mean provides
a clean way to encode the pair-wise similarities of
the individual words of the phrase. One can inter-
pret the cosine similarity metric as the magnitude of
the vector constructed by the scalar product of the
individual vectors. Our approach scales this up by
taking the scalar product of the vectors for all words
in the phrase and then scaling them by the number of
words (i.e., the geometric mean). Instead of comput-
ing the magnitude of this vector, we use it to com-
pute similarities for the entire phrase.
As an example of the proposed procedure, Table 1
shows a random sample of the contextual features
collected for the words in the query [acid fast bac-
teria], and how the query?s vector is generated by
using the geometric mean of the frequencies of the
features in the vectors for the query words.
4 Experiments and results
4.1 Experimental settings
To collect the contextual features for words and
phrases, we have used a corpus of hundreds of mil-
lions of documents crawled from the Web in August
2008. An HTML parser is used to extract text and
non-English documents are discarded. After pro-
cess, the remaining corpus contains hundreds of bil-
lions of words.
As a source of keywords, we have used the top
0 1 2 3 4
0 280 95 14 1 0
1 108 86 65 4 0
2 11 47 83 16 0
3 1 2 17 45 2
4 0 0 1 1 2
Table 2: Confusion matrix for the pairs in the goldstandard. Rows
represent first rater scores, and columns second rater scores.
one and a half million English queries sent to the
Google search engine after being fully anonymized.
We have calculated the pairwise similarity between
all queries, which would potentially return 2.25 tril-
lion similarity scores, but in practice returns a much
smaller number as many pairs have non-overlapping
contexts.
As a baseline, we have used a new implementa-
tion of the Web Kernel similarity (Sahami and Heil-
man, 2006). The parameters are set the same as re-
ported in the paper with the exception of the snip-
pet size; in their study, the size was limited to 1,000
characters and in our system, the normal snippet re-
turned by Google is used (around 160 characters).
In order to evaluate our system, we prepared a
goldstandard set of query similarities. We have ran-
domly sampled 65 queries from our full dataset, and
obtained the top 20 suggestions from both the Sa-
hami system and the distributional similarities sys-
tem. Two human raters have rated the original query
and the union of the sets of suggestions, using the
same 5-point Likert scale that Sahami used. Table 2
shows the confusion matrix of scores between the
two raters. Most of the disagreements are between
the scores 0 and 1, which means that probably it was
not clear enough whether the queries were unrelated
or only slightly related. It is also noteworthy that
in this case, very few rewritten queries were clas-
sified as being better than the original, which also
suggests to us that probably we could remove the
topmost score from the classifications scale.
We have evaluated inter-judge agreement in the
following two ways: first, using the weighted Kappa
score, which has a value of 0.7111. Second, by
grouping the pairs judged as irrelevant or slightly
relevant (scores 0 and 1) as a class containing nega-
tive examples, and the pairs judged as very relevant,
equal or better (scores 2 through 4) as a class con-
taining positive examples. Using this two-class clas-
31
Method Prec@1 Prec@3 Prec@5 mAP AUC
Web Kernel 0.39 0.35 0.32 0.49 0.22
Unigrams 0.47 0.53 0.47 0.57 0.26
N-grams 0.70 0.57 0.52 0.71 0.54
Table 3: Results. mAP is mean average precision, and AUC is the
area under the precision/recall curve.
sification, Cohen?s Kappa score becomes 0.6171.
Both scores indicates substantial agreement amongst
the raters.
The data set thus collected is a ranked list of sug-
gestions for each query1, and can be used to evaluate
any other suggestion-ranking system.
4.2 Experiments and results
As an evolution of the distributional similarities
approach, we also implemented a second version
where the queries are chunked into phrases. The
motivation for the second version is that, in some
queries, like [new york cheap hotel], it makes sense
to handle new york as a single phrase with a sin-
gle associated context vector collected from the web
corpus. The list of valid n-grams is collected by
combining several metrics, e.g. whether Wikipedia
contains an entry with that name, or whether they
appear quoted in query logs. The queries are then
chunked greedily always preferring the longer n-
gram from our list.
Table 3 shows the results of trying both systems
on the same set of queries. The original system is
the one called Unigrams, and the one that chunks
the queries is the one called N-grams. The distri-
butional similarity approaches outperform the web-
based kernel on all the metrics, and chunking queries
shows a good improvement over using unigrams.
5 Conclusions
This paper extends the vector-space model of dis-
tributional similarities to query-to-query similarities
by combining different vectors using the geometric
mean. We show that using n-grams to chunk the
queries improves the results significantly. This out-
performs the web-based kernel method, a state-of-
the-art unsupervised query-to-query similarity tech-
nique, which is particularly relevant as the corpus-
based method does not benefit automatically from
1We plan to make it available to the research community.
search engine features.
References
S. Bordag. 2008. A Comparison of Co-occurrence and
Similarity Measures as Simulations of Context. Lec-
ture Notes in Computer Science, 4919:52.
J.R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. Studies in Linguistic Analysis, pages 1?32.
G. Grefenstette. 1992. Use of syntactic context to pro-
duce term association lists for text retrieval. In Pro-
ceedings of the 15th annual international ACM SI-
GIR conference on Research and development in infor-
mation retrieval, pages 89?97. ACM New York, NY,
USA.
R. Jones, B. Rey, O. Madani, andW. Greiner. 2006. Gen-
erating query substitutions. In Proceedings of the 15th
international conference on World Wide Web, pages
387?396. ACM New York, NY, USA.
Reiner Kraft and Jason Zien. 2004. Mining anchor text
for query refinement. In WWW ?04: Proceedings of
the 13th international conference on World Wide Web,
pages 666?674, New York, NY, USA. ACM.
D. Lin. 1998. Extracting Collocations from Text Cor-
pora. In First Workshop on Computational Terminol-
ogy, pages 57?63.
Stefan Riezler, Yi Liu, and Alexander Vasserman.
2008. Translating Queries into Snippets for Improved
Query Expansion. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(COLING?08).
E. Riloff and J. Shepherd. 1997. A corpus-based ap-
proach for building semantic lexicons. In Proceed-
ings of the Second Conference on Empirical Methods
in Natural Language Processing, pages 117?124. As-
sociation for Computational Linguistics.
M. Sahami and T.D. Heilman. 2006. A web-based ker-
nel function for measuring the similarity of short text
snippets. In Proceedings of the 15th international con-
ference on World Wide Web, pages 377?386.
G. Salton, A. Wong, and CS Yang. 1975. A vector space
model for automatic indexing. Communications of the
ACM, 18(11):613?620.
Egidio Terra and Charles L.A. Clarke. 2004. Scoring
missing terms in information retrieval tasks. In CIKM
?04: Proceedings of the thirteenth ACM international
conference on Information and knowledge manage-
ment, pages 50?58, New York, NY, USA. ACM.
W. Yih and C. Meek. 2007. Improving Similarity Mea-
sures for Short Segments of Text. In Proceedings of
the Natural Conference on Artificial Intelligence, vol-
ume 2, page 1489. Menlo Park, CA; Cambridge, MA;
London; AAAI Press; MIT Press; 1999.
W. Yih and C. Meek. 2008. Consistent Phrase Relevance
Measures. Data Mining and Audience Intelligence for
Advertising (ADKDD 2008), page 37.
32
