Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 58?67,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Using Sequence Kernels to identify Opinion Entities in Urdu 
 
 
Smruthi Mukund? and Debanjan Ghosh* Rohini K Srihari 
?SUNY at Buffalo, NY 
smukund@buffalo.edu 
*Thomson Reuters Corporate R&D 
debanjan.ghosh@thomsonreuters.com 
SUNY at Buffalo, NY 
rohini@cedar.buffalo.edu 
 
 
 
 
 
Abstract 
Automatic extraction of opinion holders 
and targets (together referred to as opinion 
entities) is an important subtask of senti-
ment analysis. In this work, we attempt to 
accurately extract opinion entities from 
Urdu newswire. Due to the lack of re-
sources required for training role labelers 
and dependency parsers (as in English) for 
Urdu, a more robust approach based on (i) 
generating candidate word sequences 
corresponding to opinion entities, and (ii) 
subsequently disambiguating these se-
quences as opinion holders or targets is 
presented. Detecting the boundaries of such 
candidate sequences in Urdu is very differ-
ent than in English since in Urdu, 
grammatical categories such as tense, 
gender and case are captured in word 
inflections. In this work, we exploit the 
morphological inflections associated with 
nouns and verbs to correctly identify 
sequence boundaries. Different levels of 
information that capture context are 
encoded to train standard linear and se-
quence kernels. To this end the best per-
formance obtained for opinion entity 
detection for Urdu sentiment analysis is 
58.06% F-Score using sequence kernels 
and 61.55% F-Score using a combination 
of sequence and linear kernels. 
1 Introduction 
Performing sentiment analysis on newswire da-
ta facilitates the development of systems capable 
of answering perspective questions like ?How did 
people react to the latest presidential speech?? and 
?Does General Musharraf support the Indo-Pak 
peace treaty??. The components involved in de-
veloping such systems require accurate identifica-
tion of opinion expressions and opinion entities. 
Several of the approaches proposed in the literature 
to automatically extract the opinion entities rely on 
the use of thematic role labels and dependency 
parsers to provide new lexical features for opinion 
words (Bethard et al, 2004). Semantic roles (SRL) 
also help to mark the semantic constituents (agent, 
theme, proposition) of a sentence. Such features 
are extremely valuable for a task like opinion en-
tity detection. 
English is a privileged language when it comes 
to the availability of resources needed to contribute 
features for opinion entity detection. There are 
other widely spoken, resource poor languages, 
which are still in the infantile stage of automatic 
natural language processing (NLP). Urdu is one 
such language. The main objective of our research 
is to provide a solution for opinion entity detection 
in the Urdu language. Despite Urdu lacking NLP 
resources required to contribute features similar to 
what works for the English language, the perform-
ance of our approach is comparable with English 
for this task (compared with the work of Weigand 
and Klalow, 2010 ~ 62.61% F1). The morphologi-
cal richness of the Urdu language enables us to 
extract features based on noun and verb inflections 
that effectively contribute to the opinion entity ex-
traction task. Most importantly, these features can 
be generalized to other Indic languages (Hindi, 
Bengali etc.) owing to the grammatical similarity 
between the languages. 
58
English has seen extensive use of sequence ker-
nels (string and tree kernels) for tasks such as rela-
tion extraction (Culotta and Sorensen, 2004) and 
semantic role labeling (Moschitti et al, 2008). But, 
the application of these kernels to a task like opin-
ion entity detection is scarcely explored (Weigand 
and Klalow, 2010). Moreover, existing works in 
English perform only opinion holder identification 
using these kernels. What makes our approach 
unique is that we use the power of sequence ker-
nels to simultaneously identify opinion holders and 
targets in the Urdu language. 
Sequence kernels allow efficient use of the 
learning algorithm exploiting massive number of 
features without the traditional explicit feature rep-
resentation (such as, Bag of Words). Often, in case 
of sequence kernels, the challenge lies in choosing 
meaningful subsequences as training samples in-
stead of utilizing the whole sequence. In Urdu 
newswire data, generating candidate sequences 
usable for training is complicated. Not only are the 
opinion entities diverse in that they can be con-
tained within noun phrases or clauses, the clues 
that help to identify these components can be con-
tained within any word group - speech events, 
opinion words, predicates and connectors. 
 
1 Pakistan ke swaat sarhad ke janoobi shahar Banno 
ka havayi adda zarayye ablaagk tavvju ka markaz 
ban gaya hai. 
[ Pakistan?s provincial border?s south city?s airbase 
has become the center of attraction for all reporters.] 
 
Here, the opinion target spans across four noun 
chunks, ? Pakistan?s | provincial border?s | south 
city?s | airbase ?. The case markers (connectors) 
?ke?and?ka ?  indicate the span. 
2  Habib miyan ka ghussa bad gaya aur wo apne aurat 
ko maara. 
[Habib miya?s anger increased and he hit his own 
wife.] 
 
Here, the gender (Masculine) inflection of the verb 
?maara? (hit) indicates that the agent performing 
this action is ? Habib miya?  (Masculine) 
3  Ansari ne kaha ? mere rayee mein Aamir Sohail eek 
badimaak aur Ziddi insaan hai?. 
[Ansari said, ? according to me Aamir Sohail is one 
crazy and stubborn man?] 
 
Here, cues similar to English such as ?mere rayee 
mein ? (according to)  indicate the opinion holder.  
Another interesting behavior here is the presence of 
nested opinion holders. ?kaha? (said)  indicates that 
this statement was made by Ansari only. 
4  Sutlan bahut khush tha, naseer key kaam se.  
[Sultan was very happy with Naseer?s work ] 
 
Here, the target of the expression ?khush? is after the 
verb ?khush tha?(was happy) ? SV O structure  
Table 1: Examples to outline the complexity of the task 
 
Another contributing factor is the free word or-
der of the Urdu language. Although the accepted 
form is SOV, there are several instances where the 
object comes after the verb or the object is before 
the subject. In Urdu newswire data, the average 
number of words in a sentence is 42 (Table 3). 
This generates a large number of candidate se-
quences that are not opinion entities, on account of 
which the data used for training is highly unbal-
anced. The lack of tools such as dependency pars-
ers makes boundary detection for Urdu different 
from English, which in turn makes opinion entity 
extraction a much harder task. Examples shown in 
table 1 illustrate the complexity of the task. 
One safe assumption that can be made for opin-
ion entities is that they are always contained in a 
phrase (or clause) that contains a noun (common 
noun, proper noun or pronoun), which is either the 
subject or the object of the predicate. Based on 
this, we generate candidate sequences by consider-
ing contextual information around noun phrases. In 
example 1 of Table 1, the subsequence that is gen-
erated will consider all four noun phrases ? Paki-
stan?s | provincial border?s | south city?s | 
airbase?  as a single group for opinion entity. 
We demonstrate that investigating postpositions 
to capture semantic relations between nouns and 
predicates is crucial in opinion entity identifica-
tion. Our approach shows encouraging perform-
ance. 
2  Related Work 
Choi et al, (2005) consider opinion entity iden-
tification as an information extraction task and the 
opinion holders are identified using a conditional 
random field (Lafferty et al, 2001) based se-
quence-labeling approach. Patterns are extracted 
using AutoSlog (Riloff et al, 2003). Bloom et al, 
(2006) use hand built lexicons for opinion entity 
identification. Their method is dependent on a 
combination of heuristic shallow parsing and de-
pendency parsing information. Kim and Hovy 
59
(2006) map the semantic frames of FrameNet 
(Baker et al, 1998) into opinion holder and target 
for adjectives and verbs to identify these compo-
nents.  Stoyanov and Cardie (2008) treat the task of 
identifying opinion holders and targets as a co-
reference resolution problem. Kim et al, (2008) 
used a set of communication words, appraisal 
words from Senti-WordNet (Esuli and Sebastiani, 
2006) and NLP tools such as NE taggers and syn-
tactic parsers to identify opinion holders accu-
rately. Kim and Hovy (2006) use structural 
features of the language to identify opinion enti-
ties. Their technique is based on syntactic path and 
dependency features along with heuristic features 
such as topic words and named entities. Weigand 
and Klalow (2010) use convolution kernels that 
use predicate argument structure and parse trees. 
For Urdu specifically, work in the area of clas-
sifying subjective and objective sentences is at-
tempted by Mukund and Srihari, (2010) using a 
vector space model. NLP tools that include POS 
taggers, shallow parser, NE tagger and morpho-
logical analyzer for Urdu is provided by Mukund 
et al, (2010). This is the only extensive work done 
for automating Urdu NLP, although other efforts to 
generate semantic role labels and dependency 
parsers are underway. 
3  Linguistic Analysis for Opinion Entities 
In this section we introduce the different cues 
used to capture the contextual information for cre-
ating candidate sequences in Urdu by exploiting 
the morphological richness of the language. 
Table 2: Case Inflections on Nouns  
Urdu is a head final language with post-
positional case markers. Some post-positions are 
associated with grammatical functions and some 
with specific roles associated with the meaning of 
verbs (Davison, 1999). Case markers play a very 
important role in determining the case inflections 
of nouns. The case inflections that are useful in the 
context of opinion entity detection are ?ergative?, 
?dative?, ?genitive?, ?instrumental?  and ?loca-
tive? . Table 2 outlines the constructs. 
Consider example 1 below. (a) is a case where 
? Ali ? is nominative. However, in (b) ? Ali?  is da-
tive. The case marker ?ko?  helps to identify sub-
jects of certain experiential and psychological 
predicates: sensations, psychological or mental 
states and obligation or compulsion. Such predi-
cates clearly require the subject to be sentient, and 
further, indicate that they are aected in some 
manner, correlating with the semantic properties 
ascribed to the dative?s primary use (Grimm, 
2007). 
 
E xample (1): 
(a)  Ali khush hua  (Ali became happy) 
(b)  Ali ko khushi hui (Ali became happy) 
E xample (2): 
(a) Sadaf kaam karne ki koshish karti hai ( Sadaf 
tries to do work)  
 
Semantic information in Urdu is encoded in a 
way that is very different from English. Aspect, 
tense and gender depend on the noun that a verb 
governs. Example 2 shows the dependency that 
verbs have on nouns without addressing the lin-
guistic details associated with complex predicates. 
In example 2, the verb ?karti?(do)  is feminine 
and the noun it governs ~ Sadaf  is also feminine. 
The doer for the predicate ?karti hai?(does)  is 
? Sadaf? and there exists a gender match. This 
shows that we can obtain strong features if we are 
able to accurately (i) identify the predicates, (ii) 
find the governing noun, and (iii) determine the 
gender. 
In this work, for the purpose of generating can-
didate sequences, we encompass the post-position 
responsible for case inflection in nouns, into the 
noun phrase and group the entire chunk as one sin-
gle candidate. In example 1, the dative inflection 
on ? Ali?  is due to the case marker ? ko?.  Here, ? Ali 
ko?  will always be considered together in all candi-
date sequences that this sentence generates. This 
Case Clitic 
Form 
Examples 
Ergative (ne) Ali  ne ghussa dikhaya ~ 
Ali showed anger  
Accusa-
tive 
(ko) Ali ko mainey maara ~ 
I hit Ali  
Dative (ko,ke) Similar to accusative 
Instru-
mental 
(se) Yeh kaam Ali  se hua ~ 
This work was done by 
Ali  
Genitive (ka, ke, ki) Ali ka ghussa, baap re 
baap! ~ Ali?s anger, oh 
my God!  
Locative (mein, par, 
tak, tale, 
talak) 
Ali mein ghussa zyaada 
hai ~ there is a lot of 
anger in Ali  
60
behavior can also be observed in example 1 of ta-
ble 1.  
We use SemantexTM (Srihari et al, 2008) - an 
end to end NLP framework for Urdu that provides 
POS, NE, shallow parser and morphological ana-
lyzer, to mark tense, mood, aspect, gender and 
number inflections of verbs and case inflections of 
nouns. For ease of parsing, we enclose dative and 
accusative inflected nouns and the respective case 
markers in a tag called POSSE S S . We also enclose 
locative, genitive and ergative inflections and case 
markers in a tag called DOER.  
4  Methodology 
Sequence boundaries are first constructed based 
on the POSSESS, DOER and NP (noun chunk) 
tags prioritized by the position of the tag while 
parsing. We refer to these chunks as ?candidates?  
as they are the possible opinion entity candidates. 
We generate candidate sequences by combining 
these candidates with opinion expressions (Mu-
kund and Srihari, 2010) and the predicates that 
contain or follow the expression words (~khushi in 
(b) of example 1 above). 
We evaluate our approach in two steps: 
(i) Boundary Detection - detecting opinion 
entities that contain both holders and tar-
gets 
(ii) Entity Disambiguation - disambiguating 
opinion holders from opinion targets 
In the following sections, we briefly describe 
our research methodology including sequence 
creation, choice of kernels and the challenges thus 
encountered. 
4.1  Data Set 
The data used for the experiments are newswire 
articles from BBC Urdu1 that are manually anno-
tated to reflect opinion holders, targets, and ex-
pressions (emotion bearing words).  
 
Number of  subjective sentences 824 
Average word length of each sentence 42 
Number of opinion holders  974 
Number of opinion targets 833 
Number of opinion expressions 894 
Table 3: Corpus Statistics 
 
                                                           
1 www.bbc.co.uk/urdu/ 
Table 3 summarizes the corpus statistics. The inter 
annotator agreement established between two an-
notators over 30 documents was found to be 0.85 
using Cohen?s Kappa score (averaged over all 
tags). The agreement is acceptable as tagging emo-
tions is a difficult and a personalized task. 
4.2  Support Vector Machines (SVM) and 
Kernel Methods 
SVMs belong to a class of supervised machine 
learning techniques that merge the nuances of sta-
tistical learning theory, kernel mapping and opti-
mization techniques to discover separating 
hyperplanes. Given a set of positive and negative 
data points, based on structural risk minimization, 
SVMs attempt to find not only a separating hyper-
plane that separates two categories (Vapnik and 
Kotz, 2006) but also maximize the boundary be-
tween them (maximal margin separation tech-
nique). In this work, we propose to use a variation 
of sequence kernels for opinion entity detection. 
4.3  Sequence Kernels 
The lack of parsers that capture dependencies in 
Urdu sentences inhibit the use of ?tree kernels? 
(Weigand and Klalow, 2010). In this work, we ex-
ploit the power of a set of sequence kernels known 
as ?gap sequence string kernels? (Lodhi et al, 
2002). These kernels provide numerical compari-
son of phrases as entire sequences rather than a 
probability at the chunk level. Gap sequence ker-
nels measure the similarity between two sequences 
(in this case a sequence of Urdu words) by count-
ing the number of common subsequences. Gaps 
between words are penalized with suitable use of 
decay factor to compensate for 
matches between lengthy word sequences. 
Formally, let  be the feature space over 
words. Consequently, we declare other disjoint 
feature spaces  (stem words, POS, 
chunks, gender inflections, etc.) 
and
.
For any two-feature 
vectors  let  compute the number 
of common features between s and t. Table 5 lists 
the features used to compute . 
Given two sequences, s and t and the kernel 
function  that calculates the number of 
61
weighted sparse subsequences of length n (say, 
n =2: bigram) common to both s and t, then 
is as shown in eq 1 (Bunescu and 
Mooney, 2005). 
 
(i,j,k are dimensions)                                ------ Eq 1. 
Generating correct sequences is a prior require-
ment for sequence kernels. For example, in the task 
of relation extraction, features included in the 
shortest path between the mentions of the two se-
quences (which hold the relation) play a decisive 
role (Bunescu and Mooney, 2005). Similarly, in 
the task of role labeling (SRL - Moschitti et al, 
2008), syntactic sub-trees containing the arguments 
are crucial in finding the correct associations. Our 
approach to create candidate sequences for opinion 
entity detection in Urdu is explained in the next 
section. 
4.4  Candidate Sequence Generation 
Each subjective sentence in Urdu contains sev-
eral noun phrases with one or more opinion ex-
pressions. The words that express opinions 
(expression words) can be contained within a verb 
predicate (if the predicate is complex) or precede 
the verb predicate. These subjective sentences are 
first pre-processed to mark the morphological in-
flections as mentioned in ?3. 
Table 4: Candidate Sequence Generation 
 
We define training candidate sequences as the 
shortest substring t which is a tuple that contains 
the candidate noun phrase (POSSESS, DOER or 
NP), an emotion expression and the closest predi-
cate. Table 4 outlines the steps taken to create the 
candidate sequences and figure 1 illustrates the 
different tuples for a sample sentence.  
Experiments conducted by Weigand and 
Klakow (2010) consider <candidate, predicate> 
and <candidate, expression> tuples. However, in 
Urdu the sense of expression and predicate are so 
tightly coupled (in many examples they subsume 
each other and hence inseparable), that specifically 
trying to gauge the influence of predicate and 
expression separately on candidates is impossible. 
There are three advantages in our approach to 
creating candidate sequences: (i) by pairing ex-
pressions with their nearest predicates, several un-
necessary candidate sequences are eliminated, (ii) 
phrases that do not contain nouns are automatically 
not considered (see RBP chunk in figure 1), and 
(iii) by considering only one candidate chunk at a 
time in generating the candidate sequence, we en-
sure that the sequence that is generated is short for 
better sequence kernel performance. 
 
4 . 4 .1  Linear Kernel features 
 
For linear kernels we define features explicitly 
based on the lexical relationship between the can-
didate and its context. Table 5 outlines the features 
used.  
 
Feature Sets and Description  
Set 1 
Baseline 
1. head word of candidate 
2. case marker contained within candidate? 
3. expression words  
4. head word of predicate 
5. POS sequence of predicate words 
6. # of NPs between candidate and emotion 
Set 2 7. the DOER 
8. expression right after candidate? 
Set 3  9. gender match between candidate and 
predicate 
10. predicate contains emotion words? 
Set 4  11. POS sequence of candidate 
Set 5   12. ?kah?  feature in the predicate 
13. locative feature? 
14. genitive feature on noun? 
Table 5: Linear Kernel Features 
 
 
 
 
1 A sentence is parsed to extract all likely candi-
date chunks ? POSSESS, DOER, NP in that 
order. 
2 <expression, predicate> t uples are first selected 
based on nearest neighbor rule : 
1. Predicates that are paired with the expres-
sion words either contain the expressions or 
follow the expressions.  
2. Stand alone predicates are simply ignored as 
they do not contribute to the holder identifi-
cation task (they contribute to either the sen-
tence topic or the reason for the emotion). 
3 For each candidate, 
<candidate, expression, predicate> tuples are 
generated without changing the word order.  
(Fig. 1 ? example candidates maintain the same 
word order) 
62
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: Illustration of candidate sequences 
 
4 . 4 .1  Sequence Kernel features 
 
Features commonly used for sequence kernels 
are based on words (such as character-based or 
word-based sequence kernels). In this work, we 
consider to be a feature space over Urdu words 
along with other disjoint features such as POS, 
gender, case inflections. In the kernel, however, for 
each combination (see table 6) the similarity 
matching function that computes the num-
ber of similar features remains the same. 
Table 6: Disjoint feature set for sequence kernels 
 
Sequence kernels are robust and can deal with 
complex structures. There are several overlapping 
features between the feature sets used for linear 
kernel and sequence kernel.  Consider the POS 
path information feature. This is an important fea-
ture for the linear kernel. However this feature  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
need not be explicitly mentioned for the sequence 
kernel as the model internally learns the path in-
formation. In addition, several Boolean features 
explicitly described for the linear kernel (2 and 13 
in table 5) are also learned automatically in the 
sequence kernel by matching subsequences. 
5  Experiments 
The data used for our experiments is explained 
in ?4.1. Figure 2 gives a flow diagram of the entire 
process. LIBSVM?s (Cha ng and Lin, 2001) linear 
kernel is trained using the manually coded features 
mentioned in table 5. We integrated our proposed 
sequence kernel with the same toolkit. This se-
quence kernel uses the features mentioned in table 
6 and the decay factor is set to 0.5. 
 
 
 
 
 
 
 
 
 
Figure 2: Overall Process 
KID Kernel Type 
1 word based kernel (baseline) 
2 word + POS (parts of speech) 
3 word + POS + chunk  
4 word + POS + chunk + gender inflection  
63
The candidate sequence generation algorithm gen-
erated 8,329 candidate sequences (contains all opi-
nion holders and targets ? table 3) that are used for 
training both the kernels. The data is parsed using 
SemantexTM to apply POS, chunk and morphology 
information. Our evaluation is based on the exact 
candidate boundary (whether the candidate is en-
closed in a POSSESS, DOER or NP chunk).All 
scores are averaged over a 5-fold cross validation 
set. 
5.1  Comparison of Kernels 
We apply both linear kernels (LK) and se-
quence kernels (SK) to identify the entities as well 
as disambiguate between the opinion holders and 
targets. Table 7 illustrates the baselines and the 
best results for boundary detection of opinion enti-
ties. ID 1 of table 7 represents the result of using 
LK with feature set 1 (table 5). We interpret this as 
our baseline result. The best F1 score for this clas-
sifier is 50.17%. 
Table 7: Boundary detection of Opinion Entities 
 
Table 8 compares various kernels and combina-
tions. Set 1 of table 8 shows the relative effect of 
feature sets for LK and how each set contributes to 
detecting opinion entity boundaries. Although sev-
eral features are inspired by similar classification 
techniques (features used for SRL and opinion 
mining by Choi et al, (2005) ~ set 1, table 5), the 
free word nature of Urdu language renders these 
features futile. Moreover, due to larger average 
length of each sentence and high occurrences of 
NPs (candidates) in each sentence, the number of 
candidate instances (our algorithm creates 10 se-
quences per sentence on average) is also very high 
as compared to any English corpus.  This makes 
the training corpus highly imbalanced. Interest-
ingly, when features like ? occurrence of postposi-
tions, ?kah? predicate, gender inflections etc. are 
used, classification improves (set 1, Feature set 
1,2,3,4,5, table 8). 
Table 8: Kernel Performance 
 
ID 3 of table 7 displays the baseline result for SK. 
Interestingly enough, the baseline F1 for SK is 
very close to the best LK performance. This shows 
the robustness of SK and its capability to learn 
complex substructures with only words. A se-
quence kernel considers all possible subsequence 
matching and therefore implements a concept of 
partial (fuzzy) matching. Because of its tendency 
to learn all fuzzy matches while penalizing the 
gaps between words intelligently, the performance 
of SK in general has better recall (Wang, 2008). To 
explain the recall situation, consider set 2 of table 
8. This illustrates the effect of disjoint feature 
scopes of each feature (POS, chunk, gender). Each 
feature adds up and expands the feature space of 
sequence kernel and allows fuzzy matching there-
by improving the recall. Hence KID 4 has almost 
20% recall gain over the baseline (SK baseline).  
However, in many cases, this fuzzy matching 
accumulates in wrong classification and lowers 
precision. A fairly straightforward approach to 
overcome this problem is to employ a high preci-
sion kernel in addition to sequence kernel. Another 
limitation of SK is its inability to capture complex 
I
D Kernel 
Features  
(table 
5/6 ) 
Prec. 
(% ) 
Rec. 
(% ) 
F1 
(% ) 
1 LK Baseline (Set 1) 39.58 51.49 44.75  
2 LK(best) Set 1, 2, 3, 4, 5 44.20 57.99 50.17  
3  SK Baseline (KID 1) 58.22 42.75 49.30  
4 SK (best) KID 4 54.00 62.79 58.06  
5 
Best LK 
+ best 
SK 
KID 4, 
Set 1, 2, 
3, 4, 5 
5 8 . 4 3  65 . 0 4  61.55  
Set Kernel KID Prec.  
(% )  
Rec.  
(% )  
F1 
(% )  
Baseline 
(Set 1) 
39.58  51.49  44.75  
Set 1,2 39.91  52.57  45.38  
Set 1, 2, 3 43.55  57.72  49.65  
Set 1,2,3,4 44.10  56.90  49.68  
 
 
 
 
1 
 
 
 
 
LK 
Feature set 
1,2,3,4,5 
4 4 . 2 0  57 . 9 9  50 .17  
Baseline - 
KID 1 
58.22 42.75  49.30  
KID 2 5 8 . 9 8  47.55  52.65 
KID 3 58.18  49.62  53.59  
 
 
2 
 
 
SK 
KID 4 54.00 6 2 . 7 9  58 . 0 6  
KID 1 + 
best LK 
51.44 6 8 . 8 9  58.90  
KID 2 + 
best LK 
5 9 .18  62.98  61.02 
KID 3 + 
best LK 
55.18 68.38  61.07  
 
 
 
3 
 
 
 
SK  +  
LK 
KID 4 + 
best LK 
58.43  65.04 61.55  
64
grammatical structure and dependencies making it 
highly dependent on only the order of the string 
sequence that is supplied. 
We also combine the similarity scores of SK 
and LK to obtain the benefits of both kernels. This 
permits SK to expand the feature space by natu-
rally adding structural features (POS, chunk) re-
sulting in high recall. At the same time, LK with 
strict features (such as the use of ?kah?  verb) or 
rigid word orders (several Boolean features) will 
help maintain acceptable precision. By summing 
the contribution of both kernels, we achieve an F1 
of 61.55% (Set 3, table 8), which is 17.8%, more 
(relative gain ? around 40%) than the LK baseline 
results (ID 1, table 7). 
 
Table 9: Opinion entity disa mbiguation for best features 
Our next sets of experiments are conducted to dis-
ambiguate opinion holders and targets. A large 
number of candidate sequences that are created are 
not candidates for opinion entities. This results in a 
huge imbalance in the data set. Jointly classify 
opinion holders, opinion targets and false candi-
dates with one model can be attempted if this im-
balance in the data set due to false candidates can 
be reduced. However, this has not been attempted 
in this work. In order to showcase the feasibility of 
our method, we train our model only on the gold 
standard candidate sequences that contain opinion 
entities for entity disambiguation. 
The two kernels are applied on just the two 
classes (opinion holder vs. opinion target). Com-
bined kernels identify holders with a 65.26% F1 
(table 9). However, LK performs best for target 
identification (61.23%). We believe that this is due 
to opinion holders and targets sharing similar syn-
tactic structures. Hence, the sequence information 
that SK learns affects accuracy but improves recall. 
6  Challenges 
Based on the error analysis, we observe some 
common mistakes and provide some examples. 
1. Mistakes resulting due to POS tagger and shal-
low chunker errors. 
2. Errors due to heuristic rules for morphological 
analysis. 
3.  Mistakes due to inaccurate identification of ex-
pression words by the subjectivity classifier. 
4. Errors due to complex and unusual sentence 
structures which the kernels failed to capture. 
 
Example (3):  
Is na-insaafi ka badla hamein zaroor layna chahiye. 
[ we  have to certainly take revenge for this injustice. ] 
E xample (4): 
Kya hum dayshadgardi ka shikar banna chahatein 
hai? 
[Do we  want to become victims of terrorism ? ] 
E xample (5): 
Jab secretary kisi aur say baat karke husthi hai, tho 
Pinto ko ghussa aata hai. 
[When the secretary talks to someone and laughs, 
Pinto  gets angry.] 
 
Example 3 is a false positive. The emotion is ?an-
ger?, indicated by ?na-insaafi ka badla? (revenge 
for injustice) and ?zaroor? (certainly) .  But only 
the second expression word is identified accu-
rately. The sequence kernel model determines na-
insaafi (injustice) to be the opinion holder when it 
is actually the reason for the emotion. However, it 
also identifies the correct opinion holder - hamein 
(we) .  Emotions associated with interrogative sen-
tences are not marked (example 4) as there exists 
no one word that captures the overall emotion. 
However, the subjectivity classifier identifies such 
sentences as subjective candidates. This results in 
false negatives for opinion entity detection. The 
target (secretary) in example 5, fails to be detected 
as no candidate sequence that we generate indi-
cates the noun ?secretary?  to be the target. We 
propose to address these issues in our future work. 
7  Conclusion 
We describe an approach to identify opinion en-
tities in Urdu using a combination of kernels. To 
the best of our knowledge this is the first attempt 
where such an approach is used to identify opinion 
entities in a language lacking the availability of 
resources for automatic text processing. The per-
formance for this task for Urdu is equivalent to the 
state of the art performance for English (Weigand 
and Klakow, 2010) on the same task.  
Kernel Opinion 
Entity 
Prec. 
(% ) 
Rec. 
(% ) 
F1 
(% ) 
Holder 58.71 66.67 62.44 LK 
(best) Target 6 5 . 5 3 57.48 61.23 
Holder 60.26 69.46 64.54 SK 
 Target 59.75 49.73 54.28 
Holder 62.90 6 9 . 81 65. 2 6 Both 
kernels Target 60.71 55.44 57.96 
65
References  
Collin F. Baker, Charles J. Fillmore, John B. Lowe. 
1998. The Berkeley FrameN et Project, Proceedings 
of the 17th international conference on Computa-
tional linguistics, August 10-14. Montreal, Quebec, 
Canada 
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios 
Hatzivassiloglou, and Dan Jurafsky. 2004. Automatic 
Extraction of Opinion Propositions and their Holders, 
AAAI Spring Symposium on Exploring Attitude and 
Affect in Text: Theories and Applications. 
Kenneth Bloom, Sterling Stein, and Shlomo Argamon. 
2007. Appraisal Extraction for News Opinion Analy-
sis at NTCIR-6. In Proceedings of NTCIR-6 Work-
shop Meeting, Tokyo, Japan. 
R. C. Bunescu and R. J. M ooney. 2005. A shortest path 
dependency kernel for relation extraction. In Pro-
ceedings of HLT/EMNLP. 
R. C. Bunescu and R. J.  Mooney. 2005. Subsequence 
Kernels for Relation Extraction. NIPS. Vancouver. 
December. 
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM: 
a library for support vector machines. Software 
available at http://www.cs ie.ntu.edu.tw/~cjlin/libsvm 
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth 
Patwardhan. 2005. Identifying Sources of Opinions 
with Conditional Random Fields and Extraction Pat-
terns. In Proceedings of the Conference on Human 
Language Technology and Empirical Methods in 
Natural Language Processing (HLT/EMNLP), Van-
couver, Canada.  
Aaron Culotta and Jeffery Sorensen. 2004. Dependency 
tree kernels for relation extraction. In Proceedings of 
the 42rd Annual Meeting of the Association for 
Computational Linguistics. pp. 423-429.   
Alice Davison. 1999. Syntax  and Morphology in Hindi 
and Urdu: A Lexical Resource.  University of Iowa.  
Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiWordNet: A publicly available lexical resource for 
opinion mining. In Proc of LREC. Vol 6, pp 417-422.  
Scott Gimm. 2007. Subject Ma rking in Hindi/Urdu: A 
Study in Case and Agency. ESSLLI Student Session. 
Malaga, Spain. 
Youngho Kim, Seaongchan Kim and Sun-Hyon 
Myaeng. 2008. Extracting Topic-related Opinions 
and their Targets in NTCIR-7. In Proceedings of the 
7th NTCIR Workshop Meeting. Tokyo. Japan. 
John Lafferty, Andrew McCa llum and F. Pereira. 2001. 
Conditional random fields: Probabilistic models for 
segmenting and labeling sequence data. In: Proc. 
18th International Conf. on Machine Learning, Mor-
gan Kaufmann, San Francisco, CA . pp. 282?289 
Huma Lodhi, Craig Saunders, John Shawe-Taylor, 
Nello Cristianini, Chris Watkins. 2002. Text 
classification using string kernels. J. Mach. Learn. 
Res. 2 (March 2002), 419-44. 
Kim, Soo-Min. and Eduard Hovy. 2006. Extracting 
Opinions, Opinion Holders, and Topics Expressed in 
Online News Media Text. In ACL Workshop on Sen-
timent and Subjectivity in Text. 
Alessandro Moschitti, Daniele Pighin, Roberto Basili. 
2008. Tree kernels for semantic role labeling. Com-
putational Linguistics. Vol 34, num 2, pp 193-224. 
Smruthi Mukund and Rohini K. Srihari. 2010. A Vector 
Space Model for Subjectivity Classification in Urdu 
aided by Co-Training, In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics, 
Beijing, China.  
Smruthi Mukund, Rohini K. Srihari and Erik Peterson. 
2010. An Information Extraction System for Urdu ? 
A Resource Poor Language. Special Issue on Infor-
mation Retrieval for Indian Languages. 
Ellen Riloff, Janyce Wiebe and Theresa Wilson. 2003. 
Learning subjective nouns using extraction pattern 
bootstrapping. In Proceedings of the Seventh Confer-
ence on Natural Language Learning (CoNLL-03). 
Rohini K. Srihari, W. Li, C. Niu, and T. Cornell. 2008. 
InfoXtract: A Customizable Intermediate Level In-
formation Extraction Engine, Journal of Natural Lan-
guage Engineering, Cambridge U. Press, 14(1), pp. 
33-69. 
Veselin Stoyanov and Claire Cardie. 2008.  Annotating 
Topic Opinions. In Proceedings of the Sixth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2008), Marrakech, Morocco. 
John Shawe-Taylor and Nello  Cristianni. 2004. Kernel 
methods for pattern analysis. Cambridge University 
Press. 
Mengqiu Wang. 2008. A Re-examination of Depend-
ency Path Kernels for Relation Extraction, In 
Proceedings of IJCNLP 2008. 
Michael Wiegand and Dietrich Klalow. 2010. Convolu-
tion kernels for opinion holder extraction. In Proc. of 
Human Language Technologies: The 2010 Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics. pp 795-
803, ACL 
66
Vladimir Vapnik, S.Kotz . 2006. Estimation of De-
pendences Based on Empirical Data. Springer,  510 
pages. 
67
