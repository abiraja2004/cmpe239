A SYNTACTIC F ILTER ON PRONOMINAL  ANAPHORA FOR SLOT GRAMMAR 
Shalom Lappin and Michael McCord 
IBM T.J. Watson Research Center 
P.O. Box 704 
Yorktown Heights, NY 10598 
E-mail: Lappin/McCord@yktvmh.bitnet 
ABS\]RACT 
We propose a syntactic falter for identifying 
non-coreferential pronoun-NP pairs within a 
sentence. The filter applies to the output of a 
Slot Grammar parser and is formulated m terms 
of the head-argument structures which the parser 
generates. It liandles control and unbounded e- 
pendency constructions without empty categories 
or binding chains, by virtue of the uniticational 
nature of the parser. The filter provides con- 
straints for a discourse semantics system, reducing 
the search domain to which the inference rules 
of the system's anaphora resolution component 
apply. 
1. INTRODUCTION 
In this paper we present an implemented al- 
gorithm which filters intra-sentential relations of 
referential dependence between pronouns and 
putative NP antecedents (both full and pronomi- 
nal NP's) for the syntactic representations pro- 
vided by an English Slot Grammar parser 
(McCord 1989b). For  each parse of a sentence, 
the algorithm provides a list o7 pronoun-NP pairs 
where referential dependence of the first element 
on the second is excluded by syntactic con- 
straints. The coverage of the filter has roughly 
the same extension as conditions B and C Of 
Chomsky's (1981, 1986) binding theory, tlow- 
ever, the formulation of the algorithm is sign!f" - 
icantly different from the conditions of the 
binding theory, and from proposed implementa- 
tions of its conditions. In particular, the filter 
formulates constraints on pronominal anaphora 
in terms of the head-argument structures provided 
by Slot Grammar syntactic representations rather 
than the configurational tree relations, partic- 
ularly c-command, .on which the binding theory 
relies. As a result, the statements ofthe algorithm 
apply straightforwardly, and without special pro- 
vision, to a wide variety of constructions which 
recently proposed implementations of the binding 
theory do not handle without additional devices. 
Like the Slot Grammar whose input it applies to, 
the algorithm runs in Prolog, and it is stated in 
essentially declarative t rms. 
In Section 2 we give a brief description of Slot 
Grammar, and the parser we are employing. The 
syntactic filter is presented in Section 3, first 
through a statement of six constraints, each of 
which is sufficient o rule out coreference, then 
through a detailed description of the algorithm 
which implements these constraints. We illus- 
trate the/algorithm with examples of the lists of 
non-corelerential p irs which it provides for par- 
ticular parses. In Section 4 we compare our ap- 
proach to other proposals for syntactic filtering 
of pronominal anapliora which have appeared in 
the literature. We discuss Ilobbs algorithm, and 
we take UP two recent implementations of the 
binding theory. Finally, in Section 5 we discuss 
the integration of our filter into other systems of 
anaphora resolution. We indicate how it can be 
combined with a VP anaphora lgorithm which 
we have recently completed. We also outline the 
incorporation of our algorithm into LODUS 
(Bemth 1989), a system for discourse represen- 
tation. 
2. SLOT GRAMMAR 
The original work on Slot Grammar was done 
around 1976-78 and appeared in (McCord 1980). 
Recently, a new version (McCord 1989b) was 
developed in a logic programming framework, in 
connection with fhe machine translation system 
LMT (McCord 1989a,c,d). 
Slot Grammar is lexicalist and is dependen- 
cy-oriented. Every phrase has a head word (with 
a given word sense and morphosyntactic fea- 
tures). The constituents of a phrase besides tile 
head word (also called the modifiers of the hcad) 
are obtained by "Idling" slots associated with the 
head. Slots are symbols like sub j, obj and iobj 
representing grammatical relations, and are asso- 
ciated with a word (sense) in two ways. The 
lexical entry for the word specifies a set of com- 
plement slots (corresponding to arguments of tile 
word sense in logical form); and the grammar 
specifies a set of ad/unct slots for each part of 
135 
speech. A complement slot can be filled at most 
once, and an adjunct slot can by default be filled 
any number of times. 
The phenomena treated by augmented phrase 
structure rules in some grammatical systems are 
treated modularly by several different ypes of 
rules in Slot Grammar. The most important type 
of rule is the (slot) filler rule, which gives condi- 
tions (expressed largely through unification) on 
the filler phrase and its relations to the higher 
phrase. 
Filler rules are stated (normally) without ref- 
erence to conditions on order among constitu- 
ents. But there are separately stated ordering 
rules, l Slot~head ordering rules state conditions 
on the position (left or fight) of the slot (fdler) 
relative to the head word. Slot~slot ordering rules 
place conditions on the relative left-to-right order 
of (the fillers of) two slots. 
A slot is obligatory (not optional) if it must 
be filled, either in the current phrase or in a raised 
~osition through left movement or coordination. 
djunct slots are always optional. Complement 
slots are optional by default, but they may be 
specified to be obligatory in a particular lexical 
entry, or they may be so specifiedin the grammar 
by obligatory slot rules. Such rules may be un- 
conditional or be conditional on the character- 
istics of the higher phrase. They also may specify 
that a slot is obligatory relative to the idling of 
another slot. For example, the direct object slot 
in English. may. be d.eclared obligatory on the 
conditmn that the indirect object slot is filled by 
a noun phrase. 
One aim of Slot Grammar is to develop a 
p, owerful language-independent module, a 
shell", which can be used together with lan- 
guage-dependent modules, reducing the effort of 
writing grammars for new languages. The Slot 
Grammar shell module includes the parser, which 
is a bottom-up chart parser. It also includes most 
of the treatment of coordination, unbounded e- 
pendencies, controlled subjects, and punctuation. 
And the shell contains a system for evaluating 
parses, extending tteidom's (1982)parse metric, 
which is used not only for ranking final parses but 
also for pruning away unlikely partial analyses 
during parsing, thus reducing the problem of 
parse space explosion. Parse evaluation expresses 
preferences for close attachment, for choice of 
complements over adjuncts, and for parallelism 
in coordination. 
Although the shell contains most of the treat- 
ment of the above .phenomena (coordination, 
etc.), a small part of their treatment is necessarily 
language-dependent. A (language-specific) gram- 
mar can include for instance (1) rules for coordi- 
nating feature structures that override the defaults 
in the shell; (2) declarations of slots (called ex- 
traposer slots) that allow left extraposition of 
other slots out oI their fdlers; (3) language-specific 
rules for punctuation that override defaults; and 
(4) language-specific controls over parse evalu- 
ation that override defaults. 
Currently, Slot Grammars are being devel- 
oped for English (ESG) by McCord, for Danish 
(DSG) by Arendse Bemth, and for German 
(GSG) by Ulrike Schwall. ESG uses the UDIC'F 
lexicon (Byrd 1983, Klavans and Wacholder 
1989) having over 60,000 lemmas, with an inter- 
face that produces lot frames. The fdter algo- 
r i thm has so far been successfully tested with 
ESG and GSG. (The adaptation to German was 
done by Ulrike Schwall.) 
The algorithm applies in a second pass to the 
parse output, so the important hing in the re- 
mainder of this section is to describe Slot Gram- 
mar syntactic analysis tructures. 
A syntactic structure is a tree; each node of 
the tree represents a phrase in the sentence and 
has a unique head word. Formally, a phrase is 
represented by a term 
phrase(X,H,Sense,Features, 
s IotFrame,Ext,Hods), 
where the components are as follows: (1) X is a 
logical variable called the marker of the phrase. 
U/aifications of the marker play a crucial role in 
the fdter algorithm. (2) H is an integer epres- 
enting the position of the head word o f the 
phrase. This integer identifies the phrase 
uniquely, and is used ha the fdter algorithm as the 
way of referring to phrases. (3) Sense is the 
word sense of the head word. (4) Features is 
the feature structure of the head word and of the 
phrase. It is a logic term (not an attribute-value 
list), which is generally rather sparse ha informa- 
tion, showing mainly the part of speech and in- 
flectional features of the head word. (5) 
5 l otFrame is the list of complement slots, each 
slot being ha the internal form 
s Iot(S iot,0b,X), where Slot is the slot name, 
0b shows whether it is an obligatory form of 
Slot, and X is the slot marker. The slot marker 
is unified (essentially) with the marker of the filler 
phrase when the slot is fdled, even remotely, as 
in left movement or coordination. Such unifica- 
tions are important for the filter algorithm. (6) 
Ext is the list of slots that have been extraposed 
or raised to the level of the current phrase. (7) 
The last component Hods represents he modifi- 
ers (daughters) of the phrase, and is of the form 
mods (LHods, RMods ) where LHods and RMods are 
Tile distinction between slot filler rules and ordering constraints parallels the difference between Immediate Do- 
minance Rules and Linear Precedence Rules in GPSG. See Gazdar et al(1985) for a characterization f ID and 
I,P rules in GPSG. See (McCord 1989b) for more discussion of the relation of Slot Grammar to other systems. 
136 
Who did John say wanted to try to find him? 
subj(n) 
top 
subj(n) 
auxcmp(inf(bare)) 
obj(fin) 
preinf 
comp(enlinfling) 
~ preinf obj(inf) 
obj(fin) 
who(X2) noun 
dol(Xl,X3,X4) verb 
John(X3) noun 
say(X4,X3,Xg,u) verb 
want(X9,X2,X2,Xl2) verb 
preinf(Xl2) preinf 
try(Xl2,X2,Xl3) verb 
preinf(Xl3) preinf 
find(Xl3,X2,Xl4,u,u) verb 
he(Xl4) noun 
Figure i. 
the lists of left modifiers and right modifiers, re- 
spectively. Each member of a modifier list is of 
the form Slot:Phrase where Slot is a slot and 
Phrase is a phrase which flUs Slot. Modifier 
lists reflect surface order, and a given slot may 
appear more than once (if it is an adjunct). Thus 
modifier lists are not attribute-value lists. 
In Figure 1, a sample parse tree is shown, 
displayed by a procedure that uses only one line 
per node and exhibits tree structure lines on the 
left. In this display, each line (representing a 
node) shows (1) the tree connection fines, (2) the 
slot filled by the node, (3) the word sensepredi- 
cation, and (4) the feature structure. The feature 
structure is abbreviated here by a display option, 
showin8 only the part of speech. The word sense 
predication consists of the sense name of the head 
word with the following arguments. The first ar- 
gument is the marker variable for the phrase 
(node) itself; it is like an event or state variable for 
verbs. The remaining arguments are the marker 
variables of the slots in the complement slot 
frame (u signifies "unbound"). As can be seen in 
the display, the complement arguments are uni- 
fied with the marker variables of the fdler com- 
plement phrases., Note that in the example the 
marker X2 ol the who phrase is unified with the 
subject variables of want, try, and find. 
(There are also some unifications created by ad- 
junct slot Idling, which will not be described 
here.) 
Forthe operation of the filter algorithm, there 
is a prelim~ary step in which pertinent informa- 
tion about the parse tree is represented in a man- 
ner more convenient for the algorithm. As 
indicated above, nodes (phrases) t\]lemselves are 
represented by the word numbers of their head 
words. Properties of phrases and relations be- 
tween them are represented by unit clauses 
(predications) involving these integers (and other 
data), which are asserted into the Prolog work- 
space. Because of this "dispersed" representation 
with a collection of unit clauses, the original 
phrase structure for the whole tree is first 
grounded (variables are bound to unique con- 
stants) before the unit clauses are created. 
As an example for this clausal representation, 
the clause has ar g (P, X) says that phrase P has X 
one of its arguments; i.e., X is the slot marker 
variable for one of the complement slots of P. 
For the above sample parse, then, we would get 
clauses 
hasarg(5,'X2'), hasarg(5,'Xl2'). 
as information about the "want' node (5). 
As another example, the clause 
phmarker(P,X) is added when phrase P has 
marker X. Thus for the above sample, we would 
get the unit clause 
phmarker(I,'X2'). 
An important predicate for the fdter algorithm 
is argm, defined by 
argm(P,Q) *- phmarker(P,X) & 
hasarg(Q,X). 
This says that phrase P is an argument of phrase 
Q. This includes remote arguments and con- 
trolled subjects, because of the unifications of 
marker variables performed by the Slot Grammar 
parser. Thus for the above parse, we would get 
argm(1,5), argm( 1,7). argm( I ,9). 
showing that 'who' is an argument of 'want', "try', 
and "find'. 
3. THE FILTER 
137 
A. 
A.I. 
B. 
B.I. 
C. 
C.l. 
a.  
b. 
C. 
d. 
e.  
?. 
C.2. 
C.2.1. 
C.2.2. 
C.3. 
D. 
D.I. 
E. 
E.I. 
Fo 
F.I 
The Filter Algorithm 
nonrefdep(P,Q) ~ refpair(P,Q) & ncorefpair(P,Q). 
refpair(P,Q) ~ pron(p) & noun(Q) & P=/Q. 
ncorefpair(P,Q) ~ nonagr(P,Q) &/. 
nonagr(P,Q) ~ numdif(p,Q) I typedif(P,Q) I persdif(P,Q). 
ncorefpair(P,Q) ~ proncom(P,Q) &/. 
proncom(P,Q) 
argm(P,H) & 
(argm(Q,H) &/ I 
-pron(Q) & 
cont(Q,H) & 
(-subclcont(Q,T) I gt(Q,p)) & 
(~det(Q) I gt(Q,P))). 
cont_i(P,Q) ~ argm(P,Q) I adjunct(P,Q). 
cont(P,Q) ~ cont_i(P,Q). 
cont(P,Q) ~ cont_i(P,R) & R=/Q & cont(R,Q). 
subclcont(P,Q) ~ subconj(Q) & cont(P,Q). 
ncorefpair(P,Q) ~ prepcom(Q,P) &/. 
prepcom(Q,P) ~ argm(Q,H) & adjunct(R,H) & prep(R) & argm(P,R). 
ncorefpair(P,Q) ~ npcom(P,Q) &/. 
npcom(Q,P) ~ adjunct(Q,H) & noun(H) & 
(argm(P,H) \[ 
adjunct(R,H) & prep(R) & argm(P,R)). 
ncorefpair(P,Q) ~ nppcom(P,Q) &/. 
nppcom(P,Q) ~ adjunct(P,H) & noun(H) & 
-pron(Q) & cont(Q,H). 
Figure 2. 
In preparation for stating the six constraints, 
we adopt the following definitions. The agree- 
ment features of an NP are its number, person 
and gender features. We will say that a phrase P 
is in the argument domain of a phrase N iff P an 
N are both arguments of the same head. We will 
also say that P i s  in the adjunct domain of N iff 
N is an argument of a head tt, P is the object of 
a preposition PREP, and PREP is an adjunct of 
It. P is in the NP domain of N iff N is the det- 
erminer of a noun Qand (i) P is an argument of 
Q, or (ii) P is the object of  a preposition PREP 
and Prep is an adjunct of Q. The six constraints 
are as follows. A pronoun P is not coreferential 
with a noun phrase N if any of the following 
conditions holds. 
I. P and N have incompatible agreement features. 
II. P is in the argument domain of N. 
III. P is in the adjunct domain of N. 
IV. P is an argument of a head H, N is not a 
pronoun, and N is contained in tt. 
V. P is in the NP domain of N. 
VI. P is the determiner of a noun Q, and N is 
contained in Q. 
The algorithm wlfich implements I-VI defines a 
predicate nonrefdep(P,q) wlfich is satisfied by 
a pair whose first element Is a pronoun and whose 
second element is an NP on which the pronoun 
cannot be taken as referentially dependent, by 
virtue of the syntactic relation between them. 
The main clauses of the algorithm are shown in 
Figure 2. 
Rule A specifies that the main goal 
nonrefdep(P,Q) is satisfied by <P ,Q> if this pair 
is a referential pair ( refpalr (P ,Q))  and a non- 
coreferential pair (neorefpair(P,Q)) .  A.1 de- 
frees a refpatr ,:P,Q> as one in which P is a 
pronoun, Q'is a noun (either pronominal or non- 
pronominal), and P and Q are distinct. Rules B, 
C, D, E, and F provide a disjunctive statement 
of the conditions under which the non-corefer- 
ence goal ncorefpair(P,Q) is satisfied, and so 
const,tute the core of the algorithm. Each of 
these rules concludes with a cut to prevent un- 
necessary backtracking which could generate 
looping. 
Rule B, together with B. I, identifies the con- 
ditions under which constraint I holds. In the 
following example sentences, the pairs consisting 
of the second and the first coindexed expressions 
in la-c (and in lc also the pair < T, 'she'> ) sat- 
isfy nonrefdep(P,Q) by virtue of rule B. 
la. John i said that they i came. 
138 
b. The woman i said that he i is funny. 
C. I i believe that she  i is competent. 
? " ? ~ ,  t ,  ? The algorithm Identifies they, John > as a 
nonrefdep pair in la, which entails that 'they, 
cannot be taken as coreferential with John. 
However, (the referent of) "John" could of course 
be part of the reference set of 'they, and in suit- 
able discourses LODUS could identify this possi- 
bility. 
Rule C states that <P ,Q> is a non-coreferential 
pl.~i.r, if it satisfies the pro ncom(P,Q) predicate. 
s holds under two conditions, corresponding 
to disjuncts C. 1.a-b and C.l.a,c-f. The first con- 
dition specifies that the pronoun P and its puta- 
tive antecedent Q are both arguments of the same 
phrasal head, and so implements constraint II. 
This rules out referential dependence in 2a-b. 
2a. Mary i likes her i. 
b. She i tikes her i. 
Given the fact that Slot Grammar unifies the ar- 
gument and adjunct variables of a head with the 
phrases which t'dl these variable positions, it will 
also exclude coreference in cases of control and 
unbounded ependency, as in 3a-c. 
3a. Jo l t .  seems to want to see hirn~.. 
b. Whi6h man i did he i see? - -  
e. This is the girl i. Johh said she i saw. 
The second disjunct C.l.a,c-f covers cases in 
which the pronoun is an argument which is 
higher up in the head-argument structure of the 
sentence than a non-pronominal noun. This dis- 
junct corresponds to condition IV. C.2-C.2.2 
provide a reeursive definition of containment 
within aphrase. This definition uses the relation 
of immediate containment, eont i (P ,Q), as the 
base of the recursion, where con~ i (P ,Q) holds 
if Q is either an argument or an adj'unct (modifier 
or determiner) of a head Q. The second disjunct 
blocks coreference in 4a-c. 
4a. He~ believes that the m.a% is amusing. 
b. Who i did he i say Johr~. hssed? 
c. This Is the man i he i said John i
wrote about. 
The wh-phrase in 4b and the head noun of the 
relative clause in 4c unify with variables in posi- 
tions contained within the phrase (more precise!y, 
the verb which heads the phrase) of which the 
pronoun is an argument. Therefore, the algo- 
rithm identifies these nouns as impossible ante- 
cedents of the pronoun. 
The two final conditions of the second dis- 
junct, C. 1 .e and C. l.f, describe cases in which the 
antecedent of a pronoun is contained in a pre- 
ceding adjunct clause, and cases in which the an- 
tecedent is the determiner of an NP which 
precedes a pronoun, respectively. These clauses 
prevent such structures from satisfying the non- 
coreference goal, and so permit referential de- 
pendence in 5a-b. 
5a. After John i sang, he i danced. 
b. Johni's motherlikes him i. 
Notice that because a determiner is an adjunct of 
an NP and not an argument of the verb of which 
the NP is an argument, rule C. 1 also permits co- 
reference in 6. 
6. His i mother likes John i. 
ltowever, C.l.a,c-e correctly excludes referential 
dependence in 7, where the pronoun is an argu- 
ment which is higher than a noun adjunct. 
7. He i likes Johni's mother. 
The algorithm permits backwards anaphora in 
cases like 8, where the pronoun is not an argu- 
ment of a phrase 14 to wtfich its antecedent Q bears 
the con t (Q, fl ) relation. 
8. After he i sang, John i danced. 
D-D.I block coreference between an NP 
which is the argument of a head H, and apronoun 
that is the object of a preposition heading a PP 
adjunct of 14, as in 9a-c. These rules implement 
constraint III. 
9a. Sam. i spoke about him i. 
b. She i sat near her i. 
C. Who i did he i ask for? 
Finally, E-E.I and F realize conditions V and 
VI, respectively, in NP internal non-coreference 
cases like 10a-c. 
10a. His i portrait of Jo .hnj. is interesting. 
b. JolL, i/s portrait of htrn i is interestmg. 
c. Hisi description of the portrait by John i
is interesting. 
Let us look at three examples of actual lists 
of pairs satisfying the nonrefdep redicate which 
the algorithm generates for particular parse trees 
of Slot Grammar. The items in each pair are 
identified by their words and word numbers, cor- 
responding to their sequential position in the 
stnng. 
When the sentence Who did John say 
wanted to try to find him? is ~ven to 
the system, the parse is as shown in Figure 1 
above, and the output of the filter is: 
Noncoref pairs: 
he.lO - who.l 
139 
Coreference ana lys i s  t ime = ii msec .  
Thus < "him','who' > is identified as a non-core- 
ferential pair, while coreference between 'John' 
and 'him is allowed. 
In Figure 3, the algorithm correctly lists 
< 'him ,'Bill > (6-3) as a non-coreferential pair, 
while permitting 'him' to take "John' as an ante- 
cedent. In Fi~c~ure 4, it correctly excludes corefer- 
ence between him and 'John' (he.6-John.1), and 
allows him to be referentially dependent upon 
"Bill'. 
John expected Bill to impress him. 
I 
I 
subj(n) John(X3) noun 
top expect(Xl,X3,X4,X5) verb 
obj Bill(X4) noun 
preinf preinf(X5) preinf 
comp(inf) impress(XS,X4,X6) verb 
obj he(X6) noun 
Noncoref pairs : 
he.6 - Bill.3 
Coreference analysis time = 5 msec.  
complement clause subiect, tlowever, in Figure 
4, the infinitival clause IS an adjunct of 'lectured' 
mid requires matrix subject control. 
4. EXISTING PROPOSALS FOR CON- 
STRAINING PRONOMINAL ANAPHORA 
We will discuss three suggestions which have 
been made in the computational literature for 
syntactically constraining the relationship be- 
tween a pronoun and its set of possible antece. 
dents intra-sententially. The first is Hobbs 
(1978) Algorithm, which performs a breadth-first, 
left-to-right search of the tree containing the pro- 
noun for possible antecedents. The search is re- 
stricted to paths above the first NP or S node 
containing the pronoun, and so the pronoun 
cannot be boundby an antecedent in its minimal 
governing category. If no antecedents are found 
within the same tree as the pronoun, the trees of 
the previous entences in the text are searched in 
order of proximity. There are two main .difficul- 
ties with this approach. First, it cannot be ap- 
plied to cases of  control in infinitival clauses, like 
those given in Figures 3 and 4, or to unbounded 
dependencies, like those in Figure 1 and in ex- 
amples 3b-c and 4b-c, without significant modifi- 
cation. 
Figure 3. 
John lectured Bill to impress him. 
! subj(n) John(X3) noun 
? top lecture(Xl,X3,X4) verb 
\[ obj Bill(X4) noun 
~ preinf preinf(X5) preinf 
vnfvp impress(X5,X3,X6) verb 
obj he(X6) noun 
Noncoref pairs: 
he.6 - John.l 
Coreference analysis time = 5 msec.  
Figure 4. 
It makes this distinction by virtue of the differ- 
ences between the roles of the two infinitival 
clauses in these sentences. In Fi~gtjre 3, the infin- 
itival clause is a complement o1 "expected, and 
this verb is marked for object control of the 
Second, the algorithm is inefficient in design 
and violates modularity by virtue of the fact that 
it computes both intra-sentential constraints on 
pronoriainal anaphora and inter-sentential nte- 
cedent possibilities each time it is invoked for a 
new pronoun in a tree. Our system computes the 
set ofpronoun-NP pairs for which coreference is 
syntactically excluded in a single pass on a parse 
tree. This set provides the input to a semantic- 
pragmatic discourse module which determines 
anaphora by inference and preference rules. 
The other two proposals are presented in 
Correa (1988), and in lngria and Stallard (1989). 
Both of these models are implementations oI 
Chomsky's Binding theory which make use of 
Government Binding type parsers. They employ 
essentially the same strategy. This involves com- 
puting the set of possible antecedents of an ana- 
phor as the NP s which c-command the anaphor 
within a minimal domain (its minimal govet:ning 
category). 2 The minimal domain of an NP is 
characterized asthe first S, or the first NP without 
a possessive subiect, in which it is contained. The 
possible intra-sentential antecedents of a pronoun 
are the set of NP's in the tree which are not in- 
cluded within this minimal domain. 
See Reinhart (1976) and (1983) for alternative definitions of c-command, and discussions of the role of this re- 
lation in determining the possibilities of anaphora. See Lappin (1985) for additional discussion of the connection 
between c-command and distinct varieties of pronominal anal3hora. See Chomsky (1981), (1986a) and (1986b) 
for alternative definitions of the notion 'government' and 'rain,real governing category'. 
140 
This approach does sustain modularity by 
computing the set of possible antecedents for all 
pronouns within a tree in a single pass operation, 
prior to the application of inter-sentential search 
procedures. The main difficulty with the model 
is that because constraints on pronominal ana- 
phora are stated entirely in terms of configura- 
tional relations of tree geometry, specifically, in 
terms of c-command and minimal dominating S
and NP domains, control and unbounded e- 
p endency structures can only be handled b~' ad- itional and fairly complex devices. It is 
necessary to generate mpty categories for PRO 
and trace in appropriate positions in parse trees. 
Additional algorithms must be invoked to specify 
the chains of  control (A-binding) for PRO, and 
operator (A )-binding for trace in order to link 
these categories to the constituents which bind 
them. The algorithm which computes possible 
antecedents for anaphors and pronouns must be 
formulated so that ii identifies the head of such a 
chain as non-coreferential with a pronoun or 
anaphor (in the sense of the Binding theory), if 
any element of the chain is excluded as a possible 
antecedent. 
Neither empty categories nor binding chains 
are required in our system. In Slot Grammar 
parse representations, wh-phrases, heads of rela- 
tive clauses, and NP's which control the subjects 
of inf'mitival clauses are unified with the variables 
corresponding to the roles they bind in argument 
positions. Tlierefore, the clauses of the algorithm 
apply to these constructions directly, and without 
additional devices or stipulations) 
5. THE INTEGRATION OF THE FILTER 
INTO OTHER SYSTEMS OF ANAPHORA 
RESOLUTION 
We have recently implemented an algorithm 
for the interpretation of intrasentential VP ana- 
phora structures like those in 1 la-c. 
1 l a. John arrived, and Mary did too. 
b. Bill read every book which Sam said 
he did. 
c. Max wrote a letter to Bill before Mary 
did to John. 
The VP anaphora lgorithm generates a second 
tree which copies the antecedent verb into the 
position of the head of the elliptical VP. It also 
lists the new arguments and adjuncts which the 
copied verb inhei'its from its antecedent. We have 
integrated our filter on pronominal anaphora into 
this algorithm, so that the filter applies to the in- 
terpreted trees which the algorithm generates. 
consider 
12. John likes to him, and Bill does too. 
If the \[dter applies to the parse of 11, it will 
identify only .< him, John'> as a non-corefer- 
ential pair, gwen that the pair <'him','Bill'> 
doesn t satisfy any of the conditions of the filter 
algorithm. Ilowever, when the filter is applied to 
the interpreted VP anaphora tree of 12, the filter 
algorithm correctly identifies both pronoun-NP 
pairs, as shown in the VP output of the algorithm 
for 12 given in Figure 5. 
John likes him, and Bi l ldoes too. 
Antecedent Verb-Elliptical Verb Pairs. 
like.2 - dol.7 
Elliptical Verb-New Argument Pairs. 
like.7 - he.3 
Interpreted VP anaphora tree. 
subj John(X9) noun 
~ iconj like(X8,X9,Xl0) verb 
obj he(Xl0) noun 
? top and(Xi,X8,Xll) verb ~ subj BilI(XI2) noun rconj like(Xll,Xl2,Xl0) verb 
vadv too(Xll) adv 
Non-Coreferential Pronoun-NP Pairs. 
he.3 - John.l, he.3 - Bill.6 
Coreference analysis time = 70 msec. 
Figure 5. 
Our filter also provides input to a discourse 
understanding system, LODUS, designed and 
implemented by A. Bernth, and described in 
(..Bernth 1988, 1989). LOI)US creates a single 
discourse structure from the analyses of the S|0t 
Grammar parser for several sentences. It inter- 
prets each sentence analysis in the context con- 
sisting of the discourse processed so far, together 
with domain knowledge, and it then embeds it 
into the discourse structure. The process of in- 
te.rpretation consists in applying rules of inference 
which encode semantic and pragmatic (know- 
In fact, a more complicated algorithm with approximately tile same coverage as our lilter can be formulated fi, r 
a parser which produces configurational surlhce trees wiulout empty categories and binding chains, if the parser 
provides deep grammatical roles at some level of representation. The first author has implemented such an al- 
gorithm for the PEG parser. For a general description of I'EG, see Jensen (1986). The current version of \['E(; 
provides information on deep grammatical roles by means of second pass rules which apply to the initial parse 
record structure. The algorithm employs both c-command and reference to deep grammatical roles. 
141 
ledge-based) relations among lexical items, and 
discourse structures. The fdter reduces the set oI 
possible antecedents which the anaphora resol- 
ution component of LODUS considers for pro- 
nouns. For example, this component will not 
consider 'the cat or that' as a .p, ossible antece- 
dents for either occurrence of it in the second 
sentence in 13, but only "the mouse' in the first 
sentence of this discourse. This is due to the fact 
that our fdter lists the excluded pairs together 
with the parse tree of the second sentence. 
13. The mouse ran in. 
The cat that saw it ate it. 
Thus, the fdter significantly reduces the search 
space which the anaphora resolution component 
of LODUS must process. The interface between 
our filter and LODUS embodies the sort of mo- 
dular interaction of syntactic and semantic-prag- 
matic components which we see as important to 
the successful operation and efficiency of any 
anaphora resolution system. 
ACKNOWLEDGMENTS 
We are grateful to Arendse Bemth, Martin 
Chodorow, and Wlodek Zadrozny for helpful 
comments and advice on proposals contained in
this paper. 
REFERENCES 
Bemth, A. (1988) Computational Discourse Se- 
mantics, Doctoral Dmsertation, U. Copenha- 
gen and IBM Research. 
Bemth, A. (1989) "Discourse Understanding In 
Lo~c", Proc. North American Conference on 
Logic Programming, pp. 755-771, MIT Press. 
Byrd, R. J. (1983) "Word Formation in Natural 
Language Processing Systems," Proceedings 
oflJCAI-VIII, pp. 704-706. 
Chomsky, N. (1981) Lectures on Government and 
Binding, Foils, Dordrecht. 
Chomsky, N. (1986a) Knowledge of Language: 
Its Nature, Origin, and Use, Praeger, New 
York. 
Chomsky, N. (1986b) Barriers, MIT Press, 
Cambridge, Mass. 
Correa, N. (1988) "A B'_m,,ding Rule for Govern- 
ment-Binding Parsing , COLING "88, Buda- 
pest, pp. 123-129. 
Gazdar, G., E. Klein, G. Pullum, and I. Sag, 
G1985) Generalized Phrase Structure 
rammar, Blackwell, Oxford. 
Heidorn, G. E. (1982) "Experience with an Easily 
Computed Metric for Ranking Alternative 
Parses," Proceedings ofAnnual ACL Meeting, 
1982, pp. 82-84. 
I tobbs, J. (1978) j'Resolving l'ronoun 
References", Lingua 44, pp. 311-338. 
Ingria, R. and D. Stallard (1989) "A Computa- 
tional Mechanism for Pronominal 
Reference", Proceedings of the 27th Annual 
Meeting of the Association for Computational 
Linguistics, Vancouver, pp. 262-271. 
Jensen, K. (,1986) "PEG: A Broad-Coverage 
Computatmnal Syntax of English," Technical 
Report, IBM T.J. Watson Research Center, 
Yorktown Heights, NY. 
Klavans, J. L. and Wacholder, N. (1989) "Doc- 
umentation of Features and Attributes in 
UDICT," Research Report RC14251, IBM 
T.J. Watson Research Center, Yorktown 
Heights, N.Y. 
Lappin, S. (1985) "Pronominal Binding and Co- 
reference", Theoretical Linguistics 12, pp. 
241-263. 
McCord, M. C. (1980) "Slot Grammars," Com- 
putational Linguistics, vol. 6, pp. 31-43. 
McCord, M. C. (1989a) "Design of LMT: A 
Prolog-based Machine Translation System," 
Computational Linguistics, vol. 15, pp. 33-52. 
McCord, M. C. (1989b) "A New Version of Slot 
Grammar," Research Report RC 14506, IBM 
Research Division, Yorktown Iteights, NY 
10598. 
McCord, M. C. (198%) "A New Version of the 
Machine Translation System LMT," to ap- 
pear in Proc. International Scientific Sympo- 
sium on Natural Language and Logic, Springer 
Lecture Notes in Computer Science, and in 
J. Literary and Linguistic Computing. 
McCord, M. C. (1989d) "LMT," Proceedings of
MT Summit II, pp. 94-99, Deutsche GeseU- 
schaft f'tir Dokumentation, Frankfurt. 
Reinhart, T. (1976) The Syntactic Domain of 
Anaphora, Doctoral Dissertation, MIT, Cam- 
bridge, Mass. 
Reinhart, T. (1983) Anaphora, Croom Ilelm, 
London. 
142 
