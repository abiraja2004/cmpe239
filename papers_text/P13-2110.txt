Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 623?627,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Lattice-based Framework for Joint Chinese Word Segmentation, 
POS Tagging and Parsing 
Zhiguo Wang1, Chengqing Zong1 and Nianwen Xue2 
1National Laboratory of Pattern Recognition, 
Institute of Automation, Chinese Academy of Sciences, Beijing, China, 100190 
2Computer Science Department, Brandeis University, Waltham, MA 02452 
{zgwang, cqzong}@nlpr.ia.ac.cn   xuen@brandeis.edu 
 
Abstract 
For the cascaded task of Chinese word seg-
mentation, POS tagging and parsing, the pipe-
line approach suffers from error propagation 
while the joint learning approach suffers from 
inefficient decoding due to the large combined 
search space. In this paper, we present a novel 
lattice-based framework in which a Chinese 
sentence is first segmented into a word lattice, 
and then a lattice-based POS tagger and a lat-
tice-based parser are used to process the lattice 
from two different viewpoints: sequential POS 
tagging and hierarchical tree building. A strat-
egy is designed to exploit the complementary 
strengths of the tagger and parser, and encour-
age them to predict agreed structures. Experi-
mental results on Chinese Treebank show that 
our lattice-based framework significantly im-
proves the accuracy of the three sub-tasks. 
1 Introduction 
Previous work on syntactic parsing generally 
assumes a processing pipeline where an input 
sentence is first tokenized, POS-tagged and then 
parsed (Collins, 1999; Charniak, 2000; Petrov 
and Klein, 2007). This approach works well for 
languages like English where automatic tokeni-
zation and POS tagging can be performed with 
high accuracy without the guidance of the high-
level syntactic structure. Such an approach, how-
ever, is not optimal for languages like Chinese 
where there are no natural delimiters for word 
boundaries, and word segmentation (or tokeniza-
tion) is a non-trivial research problem by itself. 
Errors in word segmentation would propagate to 
later processing stages such as POS tagging and 
syntactic parsing. More importantly, Chinese is a 
language that lacks the morphological clues that 
help determine the POS tag of a word. For ex-
ample, ??  (?investigate/investigation?) can 
either be a verb (?investigate?) or a noun (?inves-
tigation?), and there is no morphological varia-
tion between its verbal form and nominal form. 
This contributes to the relatively low accuracy 
(95% or below) in Chinese POS tagging when 
evaluated as a stand-alone task (Sun and Uszko-
reit, 2012), and the noun/verb ambiguity is a ma-
jor source of error.  
More recently, joint inference approaches 
have been proposed to address the shortcomings 
of the pipeline approach. Qian and Liu (2012) 
proposed a joint inference approach where syn-
tactic parsing can provide feedback to word 
segmentation and POS tagging and showed that 
the joint inference approach leads to improve-
ments in all three sub-tasks. However, a major 
challenge for joint inference approach is that the 
large combined search space makes efficient de-
coding and parameter estimation very hard.  
In this paper, we present a novel lattice-based 
framework for Chinese. An input Chinese sen-
tence is first segmented into a word lattice, 
which is a compact representation of a small set 
of high-quality word segmentations. Then, a lat-
tice-based POS tagger and a lattice-based parser 
are used to process the word lattice from two 
different viewpoints. We next employ the dual 
decomposition method to exploit the comple-
mentary strengths of the tagger and parser, and 
encourage them to predict agreed structures. Ex-
perimental results show that our lattice-based 
framework significantly improves the accuracies 
of the three sub-tasks  
2 The Lattice-based Framework 
Figure 1 gives the organization of the framework. 
There are four types of linguistic structures: a 
Chinese sentence, the word lattice, tagged word 
sequence and parse tree of the Chinese sentence. 
An example for each structure is provided in 
Figure 2. We can see that the terminals and pre-
terminals of a parse tree constitute a tagged word 
sequence. Therefore, we define a comparator 
between a tagged word sequence and a parse tree: 
if they contain the same word sequence and POS 
tags, they are equal, otherwise unequal. 
623
Figure 1 also shows the workflow of the 
framework. First, the Chinese sentence is seg-
mented into a word lattice using the word seg-
mentation system. Then the word lattice is fed 
into the lattice-based POS tagger to produce a 
tagged word sequence   and into the lattice-
based parser to separately produce a parse tree  . 
We then compare   with   to see whether they 
are equal. If they are equal, we output   as the 
final result. Otherwise, the guidance generator 
generates some guidance orders based on the 
difference between   and  , and guides the tag-
ger and the parser to process the lattice again. 
This procedure may iterate many times until the 
tagger and parser predict equal structures. 
 
 
The motivation to design such a framework is 
as follows. First, state-of-the-art word segmenta-
tion systems can now perform with high accura-
cy. We can easily get an F1 score greater than 
96%, and an oracle (upper bound) F1 score 
greater than 99%  for the word lattice (Jiang et 
al., 2008). Therefore, a word lattice provides us a 
good enough search space to allow sufficient 
interaction among word segmentation, POS tag-
ging and parsing systems. Second, both the lat-
tice-based POS tagger and the lattice-based pars-
er can select word segmentation from the word 
lattice and predict POS tags, but they do so from 
two different perspectives. The lattice-based POS 
tagger looks at a path in a word lattice as a se-
quence and performs sequence labeling based on 
linear local context, while the lattice-based pars-
er builds the parse trees in a hierarchical manner. 
They have different strengths with regard to 
word segmentation and POS tagging. We hypo-
thesize that exploring the complementary 
strengths of the tagger and parser would improve 
each of the sub-tasks. 
We build a character-based model (Xue, 2003) 
for the word segmentation system, and treat 
segmentation as a sequence labeling task, where 
each Chinese character is labeled with a tag. We 
use the tag set provided in Wang et al (2011) 
and use the same feature templates. We use the 
Maximum Entropy (ME) model to estimate the 
feature weights. To get a word lattice, we first 
generate N-best word segmentation results, and 
then compact the N-best lists into a word lattice 
by collapsing all the identical words into one 
edge. We also assign a probability to each edge, 
which is calculated by multiplying the tagging 
probabilities of each character in the word. 
    The goal of the lattice-based POS tagger is to 
predict a tagged word sequence   for an input 
word lattice  :   = argmax ?    ( ) ?  ( ) 
where     ( ) represents the set of all possible 
tagged word sequences derived from the word 
lattice  .  ( ) is used to map   onto a global fea-
ture vector, and   is the corresponding weight 
vector. We use the same non-local feature tem-
plates used in Jiang et al (2008) and a similar 
decoding algorithm. We use the perceptron algo-
rithm (Collins, 2002) for parameter estimation. 
Goldberg and Elhadad (2011) proposed a lat-
tice-based parser for Heberw based on the 
PCFG-LA model (Matsuzaki et al, 2005). We 
adopted their approach, but found the un-
weighted word lattice their parser takes as input 
to be ineffective for our Chinese experiments. 
Instead, we use a weighted lattice as input and 
weigh each edge in the lattice with the word 
probability. In our model, each syntactic catego-
ry   is split into multiple subcategories  [ ] by 
labeling a latent annotation  . Then, a parse tree 
????????????? 
Brown?s group will leave Shanghai to Guangzhou tonight. 
(a) Chinese Sentence 
 
 (b) Word Lattice 
?? ?????????
NR NRVVNRPNTP PU
??
NN
Brown .GuangzhougoShanghaileavetonightingroup  
(c) Tagged Word Sequence 
Brown
.
Guangzhou
go
Shanghai
leavetonight
ingroup
?? ?
?? ?
??
?
NR P
NT
NP
PP
VV
NR
NP
VP
PUNP
IP
VP
??
NN
NP NP
?
?
VV
NR
NP
VP
VP
 
(d) Parse Tree 
Figure 2: Linguistic structure examples. 
Chinese Sentence
Word Segmentation
Word Lattice
Lattice-based Parser Lattice-based POS Tagger
Guidance Generator
Parse Tree Tagged Word 
Sequence
The Final Parse Tree
No
Yes
Equal?
 
Figure 1: The lattice-based framework. 
624
  is refined into  [ ], where X is the latent an-
notation vector for all non-terminals in  . The 
probability of  [ ] is calculated as:  ( [ ]) =   ( [ ] ?  [ ] [ ]) ?  ( [ ] ?  )?  ( ) 
where the three terms are products of all syntac-
tic rule probabilities, lexical rule probabilities 
and word probabilities in  [ ] respectively. 
3 Combined Optimization Between The 
Lattice-based POS Tagger and The 
Lattice-based Parser  
We first define some variables to make it easier 
to compare a tagged word sequence   with a 
parse tree  . We define   as the set of all POS 
tags. For  , we define  ( ,  , )=1 if   contains a 
POS tag  ?   spanning from the i-th character 
to the j-th character, otherwise  ( ,  , ) = 0. We 
also define  ( ,  , #) = 1 if   contains the word 
spanning from the i-th character to the j-th cha-
racter, otherwise  ( ,  , #) = 0. Similarly, for  , 
we define  ( ,  , )=1 if   contains a POS tag  ?   spanning from the i-th character to the j-th 
character, otherwise  ( ,  ,  ) = 0. We also define  ( ,  , #)  = 1 if   contains the word spanning 
from the i-th character to the j-th character, oth-
erwise  ( ,  , #) = 0. Therefore,   and   are equal, 
only if  ( ,  ,  ) =  ( ,  ,  )  for all  ? [0,  ] ,  ? [ + 1,  ] and  ?  ? #, otherwise unequal. 
Our framework expects the tagger and the 
parser to predict equal structures and we formu-
late it as a constraint optimization problem:    ,   = argmax ,    ( ) +   ( ) 
Such that for all  ? [0, ] ,  ? [ + 1, ]  and  ?  ? #:  ( ,  ,  ) =  ( ,  , ) 
 
where   ( ) =  ?  ( )  is a scoring function 
from the viewpoint of the lattice-based POS tag-
ger, and   ( ) = log  ( ) is a scoring function 
from the viewpoint of the lattice-based parser.  
The dual decomposition (a special case of La-
grangian relaxation) method introduced in Ko-
modakis et al (2007) is suitable for this problem. 
Using this method, we solve the primal con-
straint optimization problem by optimizing the 
dual problem. First, we introduce a vector of La-
grange multipliers  ( ,  ,  )  for each equality 
constraint. Then, the Lagrangian is formulated as:  ( ,  ,  ) =   ( ) +   ( ) +   ( ,  , )( ( ,  ,  )?  ( ,  , )) , ,  
By grouping the terms that depend on   and  , 
we rewrite the Lagrangian as  ( , , ) =    ( ) +   ( ,  , ) ( ,  , ) , ,   +   ( )?  ( ,  , ) ( ,  , ) , ,   
Then, the dual objective is  ( ) = max ,  ( , , ) = max    ( ) +   ( ,  , ) ( ,  , ) , ,  + max    ( )?  ( ,  , ) ( ,  , ) , ,   
The dual problem is to find min  ( ). 
    We use the subgradient method (Boyd et al, 
2003) to minimize the dual. Following Rush et al 
(2010), we define the subgradient of   ( ) as:  ( ,  , ) =  ( ,  , )?  ( ,  ,  )  for all ( ,  , ) 
Then, adjust  ( ,  ,  ) as follows:   ( ,  , ) =  ( ,  , )?  ( ( ,  ,  )?  ( ,  , )) 
where  >0 is a step size. 
 
Algorithm 1 presents the subgradient method 
to solve the dual problem. The algorithm initia-
lizes the Lagrange multiplier values with 0 (line 
1) and then iterates many times. In each iteration, 
the algorithm finds the best   ( )  and   ( )  by 
running the lattice-based POS tagger (line 3) and 
the lattice-based parser (line 4). If   ( ) and    ( ) 
share the same tagged word sequence (line 5), 
then the algorithm returns the solution (line 6). 
Otherwise, the algorithm adjusts the Lagrange 
multiplier values based on the differences be-
tween    ( ) and   ( ) (line 8). A crucial point is 
that the argmax problems in line 3 and line 4 can 
be solved efficiently using the original decoding 
algorithms, because the Lagrange multiplier can 
be regarded as adjustments for lexical rule prob-
abilities and word probabilities.  
4 Experiments 
We conduct experiments on the Chinese Tree-
bank Version 5.0 and use the standard data split 
Algorithm 1: Combined Optimization 
1: Set  ( )( ,  , )=0, for all  ( ,  , ) 
2: For k=1 to K 
3:     ( ) ? argmax    ( ) + ?   (   )( ,  , ) ( ,  , )  , ,    
4:     ( ) ? argmax    ( )? ?   (   )( ,  ,  ) ( ,  ,  )  , ,   
5:   If  ( )( ,  ,  ) =  ( )( ,  ,  ) for all ( ,  ,  )  
6:      Return (  ( ),   ( )) 
7:   Else  
8:       ( )( ,  ,  ) =  (   )( ,  ,  ) ?  ( ( )( ,  ,  )?  ( )( ,  , ))  
 
625
(Petrov and Klein, 2007). The traditional evalua-
tion metrics for POS tagging and parsing are not 
suitable for the joint task. Following with Qian 
and Liu (2012), we redefine precision and recall 
by computing the span of a constituent based on 
character offsets rather than word offsets.  
4.1 Performance of the Basic Sub-systems 
We train the word segmentation system with 100 
iterations of the Maximum Entropy model using 
the OpenNLP toolkit. Table 1 shows the perfor-
mance. It shows that our word segmentation sys-
tem is comparable with the state-of-the-art sys-
tems and the upper bound F1 score of the word 
lattice exceeds 99.6%. This indicates that our 
word segmentation system can provide a good 
search space for the lattice-based POS tagger and 
the lattice-based parser. 
 
To train the lattice-based POS tagger, we gen-
erate the word lattice for each sentence in the 
training set using cross validation approach. We 
divide the entire training set into 18 folds on av-
erage (each fold contains 1,000 sentences). For 
each fold, we segment each sentence in the fold 
into a word lattice by compacting 20-best seg-
mentation list produced with a model trained on 
the other 17 folds. Then, we train the lattice-
based POS tagger with 20 iterations of the aver-
age perceptron algorithm. Table 2 presents the 
joint word segmentation and POS tagging per-
formance and shows that our lattice-based POS 
tagger obtains results that are comparable with 
state-of-the-art systems. 
 
We implement the lattice-based parser by 
modifying the Berkeley Parser, and train it with 
5 iterations of the split-merge-smooth strategy 
(Petrov et al, 2006). Table 3 shows the perfor-
mance, where the ?Pipeline Parser? represents 
the system taking one-best segmentation result 
from our word segmentation system as input and 
?Lattice-based Parser? represents the system tak-
ing the compacted word lattice as input. We find 
the lattice-based parser gets better performance 
than the pipeline system among all three sub-
tasks. 
 
4.2 Performance of the Framework 
For the lattice-based framework, we set the max-
imum iteration in Algorithm 1 as K = 20. The 
step size   is tuned on the development set and 
empirically set to be 0.8. Table 4 shows the pars-
ing performance on the test set. It shows that the 
lattice-based framework achieves improvement 
over the lattice-based parser alone among all 
three sub-tasks: 0.16 points for word segmenta-
tion, 1.19 points for POS tagging and 1.65 points 
for parsing. It also outperforms the lattice-based 
POS tagger by 0.65 points on POS tagging accu-
racy. Our lattice-based framework also improves 
over the best joint inference parsing system 
(Qian and Liu, 2012) by 0.57 points. 
 
5 Conclusion  
In this paper, we present a novel lattice-based 
framework for the cascaded task of Chinese 
word segmentation, POS tagging and parsing. 
We first segment a Chinese sentence into a word 
lattice, then process the lattice using a lattice-
based POS tagger and a lattice-based parser. We 
also design a strategy to exploit the complemen-
tary strengths of the tagger and the parser and 
encourage them to predict agreed structures. Ex-
perimental results show that the lattice-based 
framework significantly improves the accuracies 
of the three tasks. The parsing accuracy of the 
framework also outperforms the best joint pars-
ing system reported in the literature. 
  P R F 
(Qian and Liu, 
2012) 
 
Seg. 97.56 98.36 97.96 
POS 93.43 94.2 93.81 
Parse 83.03 82.66 82.85 
Lattice-based  
Framework 
Seg. 97.82 97.9 97.86 
POS 94.36 94.44 94.40 
Parse 83.34 83.5 83.42 
 Table 4: Lattice-based framework evaluation. 
  P R F 
Pipeline Parser 
 
Seg. 96.97 98.06 97.52 
POS 92.01 93.04 92.52 
Parse 80.86 81.47 81.17 
 
Lattice-based 
 Parser 
Seg. 97.73 97.66 97.70 
POS 93.24 93.18 93.21 
Parse 81.83 81.71 81.77 
 Table 3: Parsing evaluation. 
 P R F (Kruengkrai et al, 2009) 93.28 94.07 93.67 
(Zhang and Clark, 2010) - - 93.67 
(Qian and Liu, 2012) 93.1 93.96 93.53 
(Sun, 2011) - - 94.02 
Lattice-based POS tagger 93.64 93.87 93.75 
Table 2: POS tagging evaluation. 
  P R F 
(Kruengkrai et al, 2009) 97.46 98.29 97.87 
(Zhang and Clark, 2010) - - 97.78 
(Qian and Liu, 2012) 97.45 98.24 97.85 
(Sun, 2011) - - 98.17 
Our Word Seg. System 96.97 98.06 97.52 
Word Lattice Upper Bound 99.55 99.75 99.65 
Table 1: Word segmentation evaluation. 
626
Acknowledgments 
The research work has been funded by the Hi-
Tech Research and Development Program ("863" 
Program) of China under Grant No. 
2011AA01A207, 2012AA011101, and 
2012AA011102 and also supported by the Key 
Project of Knowledge Innovation Program of 
Chinese Academy of Sciences under Grant 
No.KGZD-EW-501. This work is also supported 
in part by the DAPRA via contract HR0011-11-
C-0145 entitled "Linguistic Resources for Multi-
lingual Processing". 
References  
S. Boyd, L. Xiao and A. Mutapcic. 2003. Subgradient 
methods. Lecture notes of EE392o, Stanford Uni-
versity. 
E. Charniak. 2000. A maximum?entropy?inspired 
parser. In NAACL ?00, page 132?139. 
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis, 
University of Pennsylvania. 
Michael Collins. 2002. Discriminative training me-
thods for hidden markov models: Theory and expe-
riments with perceptron algorithms. In Proc. of 
EMNLP2002, pages 1-8. 
Yoav Goldberg and Michael Elhadad. 2011. Joint 
Hebrew segmentation and parsing using a PCFG-
LA lattice parser. In Proc. of ACL2011. 
Wenbin Jiang, Haitao Mi and Qun Liu. 2008. Word 
lattice reranking for Chinese word segmentation 
and part-of-speech tagging. In Proc. of Coling 2008, 
pages 385-392. 
Komodakis, N., Paragios, N., and Tziritas, G. 2007. 
MRF optimization via dual decomposition: Mes-
sage-passing revisited. In ICCV 2007. 
C. Kruengkrai, K. Uchimoto, J. Kazama, Y. Wang, K. 
Torisawa and H. Isahara. 2009. An error-driven 
word-character hybrid model for joint Chinese 
word segmentation and POS tagging. In Proc. of 
ACL2009, pages 513-521. 
Takuya Matsuzaki, Yusuke Miyao and Jun'ichi Tsujii. 
2005. Probabilistic CFG with latent annotations. In 
Proc. of ACL2005, pages 75-82. 
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proc. of ACL2006, 
pages 433-440. 
Slav Petrov and Dan Klein. 2007. Improved inference 
for unlexicalized parsing. In Proc. of NAACL2007, 
pages 404-411. 
Xian Qian and Yang Liu. 2012. Joint Chinese Word 
segmentation, POS Tagging Parsing. In Proc. of 
EMNLP 2012, pages 501-511. 
Alexander M. Rush, David Sontag, Michael Collins 
and Tommi Jaakkola. 2010. On dual decomposi-
tion and linear programming relaxations for natural 
language processing. In Proc. of EMNLP2010, 
pages 1-11. 
Weiwei Sun. 2011. A stacked sub-word model for 
joint Chinese word segmentation and part-of-
speech tagging. In Proc. of ACL2011, pages 1385-
1394. 
Weiwei Sun and Hans Uszkoreit. Capturing paradig-
matic and syntagmatic lexical relations: Towards 
accurate Chinese part-of-speech tagging. In Proc. 
of ACL2012. 
Yiou Wang, Jun'ichi Kazama, Yoshimasa Tsuruoka, 
Wenliang Chen, Yujie Zhang and Kentaro Torisa-
wa. 2011. Improving Chinese word segmentation 
and POS tagging with semi-supervised methods us-
ing large auto-analyzed data. In Proc. of 
IJCNLP2011, pages 309-317. 
Nianwen Xue. 2003. Chinese word segmentation as 
character tagging. Computational Linguistics and 
Chinese Language Processing, 8 (1). pages 29-48. 
Yue Zhang and Stephen Clark. 2010. A fast decoder 
for joint word segmentation and POS-tagging using 
a single discriminative model. In Proc. of 
EMNLP2010, pages 843-852. 
627
