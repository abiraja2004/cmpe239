Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 335?340,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Modeling Wisdom of Crowds Using
Latent Mixture of Discriminative Experts
Derya Ozkan and Louis-Philippe Morency
Institute for Creative Technologies
University of Southern California
{ozkan,morency}@ict.usc.edu
Abstract
In many computational linguistic scenarios,
training labels are subjectives making it nec-
essary to acquire the opinions of multiple an-
notators/experts, which is referred to as ?wis-
dom of crowds?. In this paper, we propose a
new approach for modeling wisdom of crowds
based on the Latent Mixture of Discrimina-
tive Experts (LMDE) model that can automat-
ically learn the prototypical patterns and hid-
den dynamic among different experts. Experi-
ments show improvement over state-of-the-art
approaches on the task of listener backchannel
prediction in dyadic conversations.
1 Introduction
In many real life scenarios, it is hard to collect
the actual labels for training, because it is expen-
sive or the labeling is subjective. To address this
issue, a new direction of research appeared in the
last decade, taking full advantage of the ?wisdom of
crowds? (Surowiecki, 2004). In simple words, wis-
dom of crowds enables parallel acquisition of opin-
ions from multiple annotators/experts.
In this paper, we propose a new method to fuse
wisdom of crowds. Our approach is based on the
Latent Mixture of Discriminative Experts (LMDE)
model originally introduced for multimodal fu-
sion (Ozkan et al, 2010). In our Wisdom-LMDE
model, a discriminative expert is trained for each
crowd member. The key advantage of our compu-
tational model is that it can automatically discover
the prototypical patterns of experts and learn the dy-
namic between these patterns. An overview of our
approach is depicted in Figure 1.
We validate our model on the challenging task of
listener backchannel feedback prediction in dyadic
conversations. Backchannel feedback includes the
nods and paraverbals such as ?uh-huh? and ?mm-
hmm? that listeners produce as they are speaking.
Backchannels play a significant role in determining
the nature of a social exchange by showing rapport
and engagement (Gratch et al, 2007). When these
signals are positive, coordinated and reciprocated,
they can lead to feelings of rapport and promote
beneficial outcomes in diverse areas such as nego-
tiations and conflict resolution (Drolet and Morris,
2000), psychotherapeutic effectiveness (Tsui and
Schultz, 1985), improved test performance in class-
rooms (Fuchs, 1987) and improved quality of child
care (Burns, 1984). Supporting such fluid interac-
tions has become an important topic of virtual hu-
man research. In particular, backchannel feedback
has received considerable interest due to its perva-
siveness across languages and conversational con-
texts. By correctly predicting backchannel feed-
back, virtual agent and robots can have stronger
sense of rapport.
What makes backchannel prediction task well-
suited for our model is that listener feedback varies
between people and is often optional (listeners can
always decide to give feedback or not). A successful
computational model of backchannel must be able
to learn these variations among listeners. Wisdom-
LMDE is a generic approach designed to integrate
opinions from multiple listeners.
In our experiments, we validate the performance
of our approach using a dataset of 43 storytelling
dyadic interactions. Our analysis suggests three pro-
335
Latent Mixture of 
Discriminative Experts
h1 h2 h3 hn
y2y1 y3 yn
x1
x1
Wisdom of crowds
(listener backchannel)
Speaker
x1 x2 x3 xnPitch
Words
Gaze Look  at listener
h1
Time
Figure 1: Left: Our approach applied to backchannel prediction: (1) multiple listeners experience the same series of
stimuli (pre-recorded speakers) and (2) a Wisdom-LMDE model is learned using this wisdom of crowds, associating
one expert for each listener. Right: Baseline models used in our experiments: a) Conditional Random Fields (CRF),
b) Latent Dynamic Conditional Random Fields (LDCRF), c) CRF Mixture of Experts (no latent variable)
totypical patterns for backchannel feedback. By
automatically identifying these prototypical pat-
terns and learning the dynamic, our Wisdom-LMDE
model outperforms the previous approaches for lis-
tener backchannel prediction.
1.1 Previous Work
Several researchers have developed models to pre-
dict when backchannel should happen. Ward and
Tsukahara (2000) propose a unimodal approach
where backchannels are associated with a region of
low pitch lasting 110ms during speech. Nishimura et
al. (2007) present a unimodal decision-tree approach
for producing backchannels based on prosodic fea-
tures. Cathcart et al (2003) propose a unimodal
model based on pause duration and trigram part-of-
speech frequency.
Wisdom of crowds was first defined and used in
business world by Surowiecki (2004). Later, it has
been applied to other research areas as well. Raykar
et. al. (2010) proposed a probabilistic approach for
supervised learning tasks for which multiple annota-
tors provide labels but not an absolute gold standard.
Snow et. al. (2008) show that using non-expert la-
bels for training machine learning algorithms can be
as effective as using a gold standard annotation.
In this paper, we present a computational ap-
proach for listener backchannel prediction that ex-
ploits multiple listeners. Our model takes into ac-
count the differences in people?s reactions, and au-
tomatically learns the hidden structure among them.
The rest of the paper is organized as follows. In
Section 2, we present the wisdom acquisition pro-
cess. Then, we describe our Wisdom-LMDE model
in Section 3. Experimentals are presented in Sec-
tion 4. Finally, we conclude with discussions and
future works in Section 5.
2 Wisdom Acquisition
It is known that culture, age and gender affect peo-
ple?s nonverbal behaviors (Linda L. Carli and Loe-
ber, 1995; Matsumoto, 2006). Therefore, there
might be variations among people?s reactions even
when experiencing the same situation. To effi-
ciently acquire responses from multiple listeners, we
employ the Parasocial Consensus Sampling (PCS)
paradigm (Huang et al, 2010), which is based on the
theory that people behave similarly when interact-
ing through a media (e.g., video conference). Huang
et al (2010) showed that a virtual human driven by
PCS approach creates significantly more rapport and
is perceived as more believable than the virtual hu-
man driven by face-to-face interaction data (from ac-
tual listener). This result indicates that the parasocial
paradigm is a viable source of information for wis-
dom of crowds.
In practice, PCS is applied by having participants
watch pre-recorded speaker videos drawn from a
336
Listener1 Listener2 Listener3 Listener4 Listener5 Listener6 Listener7 Listener8 Listener9
pauselabel:subPOS:NN
POS:NNpauselabel:pmod
pausePOS:NNlabel:nmod
pausePOS:NNlow pitch
pausedirdist:L1low pitch
POS:NNpauselow pitch
Eyebrow updirdist:L8+POS:NN
eye gazedirdist:R1POS:JJ
lownesseye gazepause
Table 1: Most predictive features for each listener from our wisdom dataset. This analysis suggests three prototypical
patterns for backchannel feedback.
dyadic story-telling dataset. In our experiments,
we used 43 video-recorded dyadic interactions from
the RAPPORT1 dataset (Gratch et al, 2006). This
dataset was drawn from a study of face-to-face
narrative discourse (?quasi-monologic? storytelling).
The videos of the actual listeners were manually an-
notated for backchannel feedback. For PCS wis-
dom acquisition, we recruited 9 participants, who
were told to pretend they are an active listener and
press the keyboard whenever they felt like provid-
ing backchannel feedback. This provides us the re-
sponses from multiple listeners all interacting with
the same speaker, hence the wisdom necessary to
model the variability among listeners.
3 Modeling Wisdom of Crowds
Given the wisdom of multiple listeners, our goal is to
create a computational model of backchannel feed-
back. Although listener responses vary among indi-
viduals, we expect some patterns in these responses.
Therefore, we first analyze the most predictive fea-
tures for each listener and search for prototypical
patterns (in Section 3.1). Then, we present our
Wisdom-LMDE that allows to automatically learn
the hidden structure within listener responses.
3.1 Wisdom Analysis
We analyzed our wisdom data to see the most rel-
evant speaker features when predicting responses
from each individual listener. (The complete list of
speaker features are described in Section 4.1.) We
used a feature ranking scheme based on a sparse
regularization technique, as described in (Ozkan and
Morency, 2010). It allows us to identify the speaker
features most predictive of each listener backchan-
nel feedback. The top 3 features for all 9 listeners
are listed in Table 1.
This analysis suggests three prototypical patterns.
For the first 3 listeners, pause in speech and syntac-
1http://rapport.ict.usc.edu/
tic information (POS:NN) are more important. The
next 3 experts include a prosodic feature, low pitch,
which is coherent with earlier findings (Nishimura
et al, 2007; Ward and Tsukahara, 2000). It is inter-
esting to see that the last 3 experts incorporate visual
information when predicting backchannel feedback.
This is in line with Burgoon et al (Burgoon et al,
1995) work showing that speaker gestures are of-
ten correlated with listener feedback. These results
clearly suggest that variations be present among lis-
teners and some prototypical patterns may exist.
Based on these observations, we propose new com-
putational model for listener backchannel.
3.2 Computational Model: Wisdom-LMDE
The goals of our computational model are to au-
tomatically discover the prototypical patterns of
backchannel feedback and learn the dynamic be-
tween these patterns. This will allow the compu-
tational model to accurately predict the responses of
a new listener even if he/she changes her backchan-
nel patterns in the middle of the interaction. It will
also improve generalization by allowing mixtures of
these prototypical patterns.
To achieve these goals, we propose a variant of the
Latent Mixture of Discriminative Experts (Ozkan et
al., 2010) which takes full advantage of the wisdom
of crowds. Our Wisdom-LMDE model is based on
a two step process: a Conditional Random Field
(CRF, see Figure 1a) is learned for each wisdom
listener, and the outputs of these expert models are
used as input to a Latent Dynamic Conditional Ran-
dom Field (LDCRF, see Figure 1b) model, which is
capable of learning the hidden structure within the
experts. In our Wisdom-LMDE, each expert cor-
responds to a different listener from the wisdom of
crowds. More details about training and inference of
LMDE can be found in Ozkan et al (2010).
337
4 Experiments
To confirm the validity of our Wisdom-LMDE
model, we compare its performance with compu-
tational models previously proposed. As motivated
earlier, we focus our experiments on predicting lis-
tener backchannel since it is a well-suited task where
variability exists among listeners.
4.1 Multimodal Speaker Features
The speaker videos were transcribed and annotated
to extract the following features:
Lexical: Some studies have suggested an asso-
ciation between lexical features and listener feed-
back (Cathcart et al, 2003). Therefore, we use all
the words (i.e., unigrams) spoken by the speaker.
Syntactic structure: Using a CRF part-of-speech
(POS) tagger and a data-driven left-to-right shift-
reduce dependency parser (Sagae and Tsujii, 2007)
we extract four types of features from a syntactic de-
pendency structure corresponding to the utterance:
POS tags and grammatical function for each word,
POS tag of the syntactic head, distance and direction
from each word to its syntactic head.
Prosody: Prosody refers to the rhythm, pitch and
intonation of speech. Several studies have demon-
strated that listener feedback is correlated with
a speaker?s prosody (Ward and Tsukahara, 2000;
Nishimura et al, 2007). Following this, we use
downslope in pitch, pitch regions lower than 26th
percentile, drop/rise and fast drop/rise in energy of
speech, vowel volume, pause.
Visual gestures: Gestures performed by the speaker
are often correlated with listener feedback (Burgoon
et al, 1995). Eye gaze, in particular, has often been
implicated as eliciting listener feedback. Thus, we
encode the following contextual features: speaker
looking at listener, smiling, moving eyebrows up
and frowning.
Although our current method for extracting these
features requires that the entire utterance to be avail-
able for processing, this provides us with a first
step towards integrating information about syntac-
tic structure in multimodal prediction models. Many
of these features could in principle be computed in-
crementally with only a slight degradation in accu-
racy, with the exception of features that require de-
pendency links where a word?s syntactic head is to
the right of the word itself. We leave an investiga-
tion that examines only syntactic features that can be
produced incrementally in real time as future work.
4.2 Baseline Models
Consensus Classifier In our first baseline model, we
use consensus labels to train a CRF model, which
are constructed by a similar approach presented
in (Huang et al, 2010). The consensus threshold is
set to 3 (at least 3 listeners agree to give feedback at
a point) so that it contains approximately the same
number of head nods as the actual listener. See Fig-
ure 1 for a graphical representation of CRF model.
CRF Mixture of Experts To show the importance
of latent variable in our Wisdom-LMDE model, we
trained a CRF-based mixture of discriminative ex-
perts. This model is similar to the Logarithmic
Opinion Pool (LOP) CRF suggested by Smith et
al. (2005). Similar to our Wisdom-LMDE model,
the training is performed in two steps. A graphical
representation of a CRF Mixture of experts is given
in the Figure 1.
Actual Listener (AL) Classifiers This baseline model
consists of two models: CRF and LDCRF chains
(See Figure 1). To train these models, we use the
labels of the ?Actual Listeners? (AL) from the RAP-
PORT dataset.
Multimodal LMDE In this baseline model, we com-
pare our Wisdom LMDE to a multimodal LMDE,
where each expert refers to one of 5 different set of
multimodal features as presented in (Ozkan et al,
2010): lexical, prosodic, part-of-speech, syntactic,
and visual.
Random Classifier Our last baseline model is a ran-
dom backchannel generator as desribed by Ward
and Tsukahara (2000). This model randomly gener-
ates backchannels whenever some pre-defined con-
ditions in the prosody of the speech is purveyed.
4.3 Methodolgy
We performed hold-out testing on a randomly se-
lected subset of 10 interactions. The training set
contains the remaining 33 interactions. Model pa-
rameters were validated by using a 3-fold cross-
validation strategy on the training set. Regulariza-
338
Table 2: Comparison of our Wisdom-LMDE model with previously proposed models. The last column shows the
paired one tailed t-test results comparing Wisdom LMDE to each model.
tion values used are 10k for k = -1,0,..,3. Numbers
of hidden states used in the LDCRF models were
2, 3 and 4. We use the hCRF library2 for training
of CRFs and LDCRFs. Our Wisdom-LMDE model
was implemented in Matlab based on the hCRF li-
brary. Following (Morency et al, 2008), we use
an encoding dictionary to represent our features.
The performance is measured by using the F-score,
which is the weighted harmonic mean of precision
and recall. A backchannel is predicted correctly if
a peak happens during an actual listener backchan-
nel with high enough probability. The threshold was
selected automatically during validation.
4.4 Results and Discussion
Before reviewing the prediction results, is it impor-
tant to remember that backchannel feedback is an
optional phenomena, where the actual listener may
or may not decide on giving feedback (Ward and
Tsukahara, 2000). Therefore, results from predic-
tion tasks are expected to have lower accuracies as
opposed to recognition tasks where labels are di-
rectly observed (e.g., part-of-speech tagging).
Table 2 summarizes our experiments comparing
our Wisdom-LMDE model with state-of-the-art ap-
proaches for behavior prediction (see Section 4.2).
Our Wisdom-LMDE model achieves the best F1
score. Statistical t-test analysis show that Wisdom-
LMDE is significantly better than Consensus Clas-
sifier, AL Classifier (LDCRF), Multimodel LMDE
and Random Classifier.
The second best F1 score is achieved by CRF
Mixture of experts, which is the only model among
other baseline models that combines different lis-
tener labels in a late fusion manner. This result
2http://sourceforge.net/projects/hrcf/
supports our claim that wisdom of clouds improves
learning of prediction models. CRF Mixture model
is a linear combination of the experts, whereas
Wisdom-LMDE enables different weighting of ex-
perts at different point in time. By using hidden
states, Wisdom-LMDE can automatically learn the
prototypical patterns between listeners.
One really interesting result is that the optimal
number of hidden states in the Wisdom-LMDE
model (after cross-validation) is 3. This is coherent
with our qualitative analysis in Section 3.1, where
we observed 3 prototypical patterns.
5 Conclusions
In this paper, we proposed a new approach called
Wisdom-LMDE for modeling wisdom of crowds,
which automatically learns the hidden structure in
listener responses. We applied this method on
the task of listener backchannel feedback predic-
tion, and showed improvement over previous ap-
proaches. Both our qualitative analysis and exper-
imental results suggest that prototypical patterns ex-
ist when predicting listener backchannel feedback.
The Wisdom-LMDE is a generic model applicable
to multiple sequence labeling tasks (such as emotion
analysis and dialogue intent recognition), where la-
bels are subjective (i.e. small inter-coder reliability).
Acknowledgements
This material is based upon work supported by
the National Science Foundation under Grant No.
0917321 and the U.S. Army Research, Develop-
ment, and Engineering Command (RDE-COM).
The content does not necessarily reflect the position
or the policy of the Government, and no official en-
dorsement should be inferred.
339
References
Judee K. Burgoon, Lesa A. Stern, and Leesa Dillman.
1995. Interpersonal adaptation: Dyadic interaction
patterns. Cambridge University Press, Cambridge.
M. Burns. 1984. Rapport and relationships: The basis of
child care. Journal of Child Care, 4:47?57.
N. Cathcart, Jean Carletta, and Ewan Klein. 2003. A
shallow model of backchannel continuers in spoken
dialogue. In European Chapter of the Association for
Computational Linguistics. 51?58.
Aimee L. Drolet and Michael W. Morris. 2000. Rap-
port in conflict resolution: Accounting for how face-
to-face contact fosters mutual cooperation in mixed-
motive conflicts. Journal of Experimental Social Psy-
chology, 36(1):26?50.
D. Fuchs. 1987. Examiner familiarity effects on test per-
formance: Implications for training and practice. Top-
ics in Early Childhood Special Education, 7:90?104.
J. Gratch, A. Okhmatovskaia, F. Lamothe, S. Marsella,
M. Morales, R.J. Werf, and L.-P. Morency. 2006. Vir-
tual rapport. Proceedings of International Conference
on Intelligent Virtual Agents (IVA), Marina del Rey,
CA.
Jonathan Gratch, Ning Wang, Jillian Gerten, and Edward
Fast. 2007. Creating rapport with virtual agents. In
IVA.
L. Huang, L.-P. Morency, and J. Gratch:. 2010. Paraso-
cial consensus sampling: combining multiple perspec-
tives to learn virtual human behavior. In Interna-
tional Conference on Autonomous Agents and Multi-
agent Systems (AAMAS).
Suzanne J. LaFleur Linda L. Carli and Christopher C.
Loeber. 1995. Nonverbal behavior, gender, and influ-
ence. Journal of Personality and Social Psychology.
68, 1030-1041.
D. Matsumoto. 2006. Culture and Nonverbal Behav-
ior. The Sage Handbook of Nonverbal Communica-
tion, Sage Publications Inc.
L.-P. Morency, I. de Kok, and J. Gratch. 2008. Predict-
ing listener backchannels: A probabilistic multimodal
approach. In Proceedings of the Conference on Intel-
ligent Virutal Agents (IVA).
Ryota Nishimura, Norihide Kitaoka, and Seiichi Naka-
gawa. 2007. A spoken dialog system for chat-like
conversations considering response timing. Interna-
tional Conference on Text, Speech and Dialog. 599-
606.
D. Ozkan and L.-P. Morency. 2010. Concensus of self-
features for nonverbal behavior analysis. In Human
Behavior Understanding in conjucion with Interna-
tional Conference in Pattern Recognition.
D. Ozkan, K. Sagae, and L.-P. Morency. 2010. La-
tent mixture of discriminative experts for multimodal
prediction modeling. In International Conference on
Computational Linguistics (COLING).
Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-
ardo Hermosillo Valadez, Charles Florin, Luca Bo-
goni, Linda Moy, and David Blei. 2010. Learning
from crowds.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 1044?1050,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic
opinion pools for conditional random fields. In ACL,
pages 18?25.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2008.
Cheap and fast - but is it good? Evaluating non-expert
annotations for natural language tasks.
James Surowiecki. 2004. The Wisdom of Crowds: Why
the Many Are Smarter Than the Few and How Col-
lective Wisdom Shapes Business, Economies, Societies
and Nations. Doubleday.
P. Tsui and G.L. Schultz. 1985. Failure of rapport: Why
psychotheraputic engagement fails in the treatment of
asian clients. American Journal of Orthopsychiatry,
55:561?569.
N. Ward and W. Tsukahara. 2000. Prosodic fea-
tures which cue back-channel responses in english and
japanese. Journal of Pragmatics. 23, 1177?1207.
340
