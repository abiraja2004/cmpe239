Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 149?154,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The Cunei Machine Translation Platform for WMT ?10
Aaron B. Phillips
Carnegie Mellon
Pittsburgh, USA.
aphillips@cmu.edu
Abstract
This paper describes the Cunei Machine
Translation Platform and how it was used
in the WMT ?10 German to English and
Czech to English translation tasks.
1 The Cunei Machine Translation
Platform
The Cunei Machine Translation Platform (Phillips
and Brown, 2009) is open-source software
and freely available at http://www.cunei.
org/. Like Moses (Koehn et al, 2007) and
Joshua (Li et al, 2009), Cunei provides a statisti-
cal decoder that combines partial translations (ei-
ther phase pairs or grammar rules) in order to com-
pose a coherent sentence in the target language.
What makes Cunei unique is that it models the
translation task with a non-parametric model that
assesses the relevance of each translation instance.
The process begins by encoding in a lattice all
possible contiguous phrases from the input.1 For
each source phrase in the lattice, Cunei locates in-
stances of it in the corpus and then identifies the
aligned target phrase(s). This much is standard to
most data-driven MT systems. The typical step at
this stage is to model a phrase pair by computing
relative frequencies over the collection of transla-
tion instances. This model for the phrase pair will
never change and knowledge of the translation in-
stances can subsequently be discarded. In contrast
to using a phrase pair as the basic unit of modeling,
Cunei models each translation instance. A dis-
tance function, represented by a log-linear model,
scores the relevance of each translation instance.
Our model then sums the scores of translation in-
stances that predict the same target hypothesis.
The advantage of this approach is that it pro-
vides a flexible framework for novel sources of
1Cunei offers limited support for non-contiguous phrases,
similar in concept to grammar rules, but this setting was dis-
abled in our experiments.
information. The non-parametric model still uses
information gleaned over all translation instances,
but it permits us to define a distance function that
operates over one translation instance at a time.
This enables us to score a wide-variety of informa-
tion represented by the translation instance with
respect to the input and the target hypothesis un-
der consideration. For example, we could compute
how similar one translation instance?s parse tree or
morpho-syntactic information is to the input. Fur-
thermore, this information will vary throughout
the corpus with some translation instances exhibit-
ing higher similarity to the input. Our approach
captures that these instances are more relevant and
they will have a larger effect on the model. For
the WMT ?10 task, we exploited instance-specific
context and alignment features which will be dis-
cussed in more detail below.
1.1 Formalism
Cunei?s model is a hybrid between the approaches
of Statistical MT and Example-Based MT. A typ-
ical SMT model will score a phrase pair with
source s, target t, log features ?, and weights
? using a log-linear model, as shown in Equa-
tion 1 of Figure 1. There is no prototypical model
for EBMT, but Equation 2 demonstrates a reason-
able framework where evidence for the phrase pair
is accumulated over all instances of translation.
Each instance of translation from the corpus has
a source s? and target t?. In the most limited case
s = s? and t = t?, but typically an EBMT sys-
tem will have some notion of similarity and use
instances of translation that do not exactly match
the input.
Cunei?s model is defined in such a way that we
maintain the distance function ?(s, s?, t?, t) from
the EBMT model, but compute it in a much more
efficient manner. In particular, we remove the real-
space summation within a logarithm that makes it
impractical to tune model weights. However, our
149
score(s, t) =
?
k
?k?k(s, t) (1)
score(s, t) = ln
?
s?,t?
e
?
k ?k?k(s,s
?,t?,t) (2)
score(s, t) = ? +
?
k
?k
(?
(s?,t?)?C ?k(s, s
?, t?, t)e
?
i ?i?i(s,s
?,t?,t)
?
(s?,t?)?C e
?
i ?i?i(s,s
?,t?,t)
)
(3)
Figure 1: Translation model scores according to SMT (1), EBMT (2), and Cunei (2)
model preserves the first-order derivative of Equa-
tion 2, which is useful during optimization to lo-
cally approximate the hypothesis space. While the
inner term initially appears complex, it is simply
the expectation of each feature under the distribu-
tion of translation instances and can be efficiently
computed with an online update. Last, the in-
troduction of ?, a slack variable, is necessary to
additionally ensure that the score of this model
is equal to Equation 2. Specifying the model in
this manner ties together the two different mod-
eling approaches pursued by SMT and EBMT;
the SMT model of Equation 1 is merely a spe-
cial case of our model when the features for all
instances of a translation are constant such that
?k(s, s?, t?, t) = ?k(s, t) ?s?, t?.
Indeed, this distinction illuminates the primary
advantage of our model. Each feature is calcu-
lated particular to one translation instance in the
corpus and each translation instance is scored in-
dividually. The model is then responsible for ag-
gregating knowledge across multiple instances of
translation. Unlike the SMT model, our aggregate
model does not maintain feature independence.
Each instance of translation represents a joint set
of features. The higher the score of a translation
instance, the more all its features inform the ag-
gregate model. Thus, our model is biased toward
feature values that represent relevant translation
instances.
1.2 Context
Not all translations found in a corpus are equally
useful. Often, when dealing with data of vary-
ing quality, training a SMT system on all of the
data degrades performance. A common work-
around is to perform some sort of sub-sampling
that selects a small quantity of novel phrase pairs
from the large out-of-domain corpus such that they
do not overwhelm the number of phrase pairs ex-
tracted from the smaller in-domain corpus.
Instead of building our model from a heuristic
sub-sample, we utilize Cunei?s modeling approach
to explicitly identify the relevance of each transla-
tion instance. We add features to the model that
identify when a translation instance occurs within
the same context as the input. This permits us to
train on all available data by dynamically weight-
ing each instance of a translation.
First, we capture the broader context or genre of
a translation instance by comparing the document
in the corpus from which it was extracted to the
input document. These documents are modeled as
a bag of words, and we use common document-
level distance metrics from the field of information
retrieval. Specifically, we implement as features
document-level precision, recall, cosine distance
and Jensen-Shannon distance (Lin, 1991).
In order to capture local, intra-sentential con-
text, we compare the words immediately to the left
and right of each translation instance with the in-
put. We add one feature that counts the total num-
ber of adjacent words that match the input and a
second feature that penalizes translation instances
whose adjacent context only (or mostly) occurs in
one direction. As a variation on the same concept,
we also add four binary features that indicate when
a unigram or bigram match is present on the left or
right hand side.
The corpus in which an instance is located can
also substantially alter the style of a translation.
For example, both the German to English and the
Czech to English corpora consisted of in-domain
News Commenary and out-of-domain Europarl
text. When creating the index, Cunei stores the
name of the corpus that is associated with each
sentence. From this information we create a set
of binary features for each instance of translation
that indicate from which corpus the instance origi-
nated. The weights for these origin features can be
150
conceived as mixture weights specifying the rele-
vance of each corpus.
1.3 Alignment
After a match is found on the source-side of the
corpus, Cunei must determine the target phrase to
which it aligns. The phrase alignment is treated as
a hidden variable and not specified during train-
ing. Ideally, the full alignment process would
be carried out dynamically at run-time. Unfor-
tunately, even a simple word alignment such as
IBM Model-1 is too expensive. Instead, we run a
word aligner offline and our on-line phrase align-
ment computes features over the the word align-
ments. The phrase alignment features are then
components of the model for each translation in-
stance. While the calculations are not exactly the
same, conceptually this work is modeled after (Vo-
gel, 2005).
For each source-side match in the corpus, an
alignment matrix is loaded for the complete sen-
tence in which the match resides. This align-
ment matrix contains scores for all word corre-
spondences in the sentence pair and can be created
using GIZA++ (Och and Ney, 2003) or the Berke-
ley aligner (Liang et al, 2006). Intuitively, when a
source phrase is aligned to a target phrase, this im-
plies that the remainder of the source sentence that
is not specified by the source phrase is aligned to
the remainder of the target sentence not specified
by the target phrase. Separate features compute
the probability that the word alignments for to-
kens within the phrase are concentrated within the
phrase boundaries and that the word alignments
for tokens outside the phrase are concentrated out-
side the phrase boundaries. In addition, words
with no alignment links or weak alignments links
demonstrate uncertainty in modeling. To capture
this effect, we incorporate two more features that
count the number of uncertain alignments present
in the source phrase and the target phrase.
The features described above assess the phrase
alignment likelihood for a particular translation in-
stance. Because they operate over all the word
alignments present in a sentence, the alignment
scores are contextual and usually vary from in-
stance to instance. As the model weights change,
so too will the phrase alignment scores. Each
source phrase is modeled as having some proba-
bility of aligning to every possible target phrase
within a given sentence. However, it is not prac-
tical to compute all possible phrase alignments,
so we extract translation instances using only a
few high-scoring phrase alignments for each oc-
currence of a source phrase in the corpus.2 As dis-
cussed previously, these extracted translation in-
stances form the basic modeling unit in Cunei.
1.4 Optimization
Cunei?s built-in optimization code closely follows
the approach of (Smith and Eisner, 2006), which
minimizes the expectation of the loss function over
the distribution of translations present in the n-
best list. Following (Smith and Eisner, 2006), we
implemented log(BLEU) as the loss function such
that the objective function can be decomposed as
the expected value of BLEU?s brevity penalty and
the expected value of BLEU?s precision score.
The optimization process slowly anneals the dis-
tribution of the n-best list in order to avoid local
minima. This begins with a near uniform distribu-
tion of translations and eventually reaches a distri-
bution where, for each sentence, nearly all of the
probability mass resides on the top translation (and
corresponds closely with the actual 1-best BLEU
score). In addition, Cunei supports the ability to
decode sentences toward a particular set of refer-
ences. This is used to prime the optimization pro-
cess in the first iteration with high-scoring, obtain-
able translations.
2 The WMT ?10 Translation Task
For the WMT ?10 Translation Task we built two
systems. The first translated from German to En-
glish and was trained with the provided News
Commentary and Europarl (Koehn, 2005) corpora.
The second system translated from Czech to En-
glish and used the CzEng 0.9 corpus (Bojar and
Z?abokrtsky?, 2009), which is a collection of many
different texts and includes the Europarl. To val-
idate our results, we also trained a Moses system
with the same corpus, alignments, and language
model.
2.1 Corpus Preparation
A large number of hand-crafted regular expres-
sions were used to remove noise (control char-
acters, null bytes, etc.), normalize (hard spaces
vs. soft spaces, different forms of quotations,
2This is controlled by a score ratio that typically selects
2-6 translation instances per occurrence of a source phrase.
151
render XML codes as characters, etc.), and tok-
enize (abbreviations, numbers, punctuation, etc.).
However, these rules are fairly generic and appli-
cable to most Western languages. In particular,
we did not perform any morphologically-sensitive
segmentation. From the clean text we calculated
the expected word and character ratios between
the source language and the target language. Then
we proceeded to remove sentence pairs according
to the following heuristics:
? A sentence exceeded 125 words
? A sentence exceeded 1,000 characters
? The square of the difference between the
actual and expected words divided by the
square of the standard deviation exceeded 5
? The square of the difference between the ac-
tual and expected characters divided by the
square of the standard deviation exceeded 5
All of these processing routines are included as
part of the Cunei distribution and are configurable
options. An overview of the resulting corpora is
shown in Table 1.
Finally, we used the GIZA++ toolkit (Och and
Ney, 2003) to induce word alignments in both di-
rections for each language pair. The resulting cor-
pus and word alignments were provided to Moses
and Cunei for training. Each system used their
respective phrase extraction and model estimation
routines.
2.2 Language Model
We intentionally selected two language pairs that
translated into English so that we could share one
language model between them. We used the large
monolingual English News text made available
through the workshop and augmented this with
the Xinhua and AFP sections of the English Gi-
gaword corpus (Parker and others, 2009). In all,
approximately one billion words of English text
were fed to the SRILM toolkit (Stolcke, 2002) to
construct a single English 5-gram language model
with Kneser-Ney smoothing.
2.3 Experiments
The newswire evaluation sets from the prior two
years were selected as development data. 636 sen-
tences were sampled from WMT ?09 for tuning
and all 2,051 sentences from WMT ?08 were re-
served for testing. Finally, a blind evaluation was
also performed with the new WMT ?10 test set.
All systems were tuned toward BLEU (Papineni
et al, 2002) and all evaluation metrics were run
on lowercased, tokenized text.
The results in Table 2 and Table 3 show the per-
formance of Cunei3 against the Moses system we
also built with the same data. The first Cunei sys-
tem we built included all the alignment features
discussed in ?1.3. These per-instance alignment
features are essential to Cunei?s run-time phrase
extraction and cannot be disabled. The second,
and complete, system added to this all the context
features described in ?1.2. Cunei, in general, per-
forms significantly better than Moses in German
and is competitive with Moses in Czech. However,
we hoped to see a larger gain from the addition of
the context features.
In order to better understand our results and see
if there was greater potential for the context fea-
tures, we selectively added a few of the features at
a time to the German system. These experiments
are reported in Table 4. What is interesting here
is that most subsets of context features did better
than the whole and none degraded the baseline (at
least according to BLEU) on the test sets. We did
not expect a fully additive gain from the combina-
tion, as many of the context features do represent
different ways of capturing the same phenomena.
However, we were still surprised to find an appar-
ently detrimental interaction among the full set of
context features.
Theoretically adding new features should only
improve a system as a feature can always by ig-
nored by assigning it a weight of zero. How-
ever, new features expand the hypothesis space
and provide the model with more degrees of free-
dom which may make it easier to get stuck in lo-
cal minima. While the gradient-based, annealing
method for optimization that we use tends work
better than MERT (Och, 2003), it is still suscep-
tible to these issues. Indeed, the variation on the
tuning set?while relatively inconsequential?is ev-
idence that this is occurring and that we have not
found the global optimum. Further investigation is
necessary into the interaction between the context
features and techniques for robust optimization.
3These results have been updated since the official
WMT ?10 submission as a result of minor bug-fixes and code
improvements to Cunei.
152
German English Czech English
Tokens 41,245,188 43,064,069 63,776,164 72,325,831
Sentences 1574044 6181270
Table 1: Corpus Statistics
2.4 Conclusion
We used the Cunei Machine Translation Platform
to build German to English and Czech to English
systems for the WMT ?10 evaluation. In both
systems we experimented with per-instance align-
ment and context features. Our addition of the
context features resulted in only minor improve-
ment, but a deeper analysis of the individual fea-
tures suggests greater potential. Overall, Cunei
performed strongly in our evaluation against a
comparable Moses system. We acknowledge that
the actual features we selected are not particu-
larly novel. Instead, the importance of this work
is the simplicity with which instance-specific fea-
tures can be jointly modeled and integrated within
Cunei as a result of its unique modeling approach.
Acknowledgements
The author would like to thank Ralf Brown for
providing suggestions and feedback on this paper.
References
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009.
Czeng0.9: Large parallel treebank with rich annota-
tion. Prague Bulletin of Mathematical Linguistics,
92.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics, pages 177?
180, Prague, Czech Republic, June.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Machine Transla-
tion Summit X Proceedings (mts, 2005), pages 79?
86.
Zhifei Li, Chris Callison-Burch, Chris Dyer, San-
jeev Khudanpur, Lane Schwartz, Wren Thornton,
Jonathan Weese, and Omar Zaidan. 2009. Joshua:
An open source toolkit for parsing-based machine
translation. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, pages 135?139,
Athens, Greece, March.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 104?111, New York City,
USA, June.
Jianhua Lin. 1991. Divergence measures based on the
shannon entropy. IEEE Transactions on Information
Theory, 37(1):145?151, January.
2005. Phuket, Thailand, September.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
USA, July.
Robert Parker et al 2009. English gigaword fourth
edition.
Aaron B. Phillips and Ralf D. Brown. 2009. Cunei
machine translation platform: System description.
In Mikel L. Forcada and Andy Way, editors, Pro-
ceedings of the 3rd Workshop on Example-Based
Machine Translation, pages 29?36, Dublin, Ireland,
November.
David A. Smith and Jason Eisner. 2006. Minimum
risk annealing for training log-linear models. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 787?794, Sydney, Australia, July.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In 7th International Conference
on Spoken Language Processing, pages 901?904,
Denver, USA, September.
Stephan Vogel. 2005. Pesa: Phrase pair extraction as
sentence splitting. In Machine Translation Summit
X Proceedings (mts, 2005), pages 251?258.
153
D
ev
el
op
m
en
t
Tu
ni
ng
D
ev
el
op
m
en
t
Te
st
B
lin
d
Te
st
B
L
E
U
N
IS
T
M
et
eo
r
T
E
R
B
L
E
U
N
IS
T
M
et
eo
r
T
E
R
B
L
E
U
N
IS
T
M
et
eo
r
T
E
R
M
os
es
0.
19
16
5.
91
56
0.
52
86
0.
64
75
0.
20
46
6.
28
02
0.
53
30
0.
65
23
0.
20
97
6.
56
57
0.
55
91
0.
63
13
C
un
ei
w
it
h
A
li
gn
m
en
t
0.
20
18
5.
98
47
0.
53
26
0.
63
75
0.
21
25
6.
36
39
0.
53
42
0.
64
30
0.
22
10
6.
63
55
0.
55
73
0.
62
24
C
un
ei
w
it
h
A
li
gn
m
en
t&
C
on
te
xt
0.
20
22
6.
00
21
0.
53
31
0.
63
62
0.
21
27
6.
37
53
0.
53
44
0.
64
08
0.
22
14
6.
64
67
0.
55
75
0.
61
98
Ta
bl
e
2:
O
ve
rv
ie
w
of
G
er
m
an
to
E
ng
li
sh
E
va
lu
at
io
ns
D
ev
el
op
m
en
t
Tu
ni
ng
D
ev
el
op
m
en
t
Te
st
B
lin
d
Te
st
B
L
E
U
N
IS
T
M
et
eo
r
T
E
R
B
L
E
U
N
IS
T
M
et
eo
r
T
E
R
B
L
E
U
N
IS
T
M
et
eo
r
T
E
R
M
os
es
0.
21
41
6.
19
69
0.
55
36
0.
61
70
0.
20
41
6.
35
74
0.
53
61
0.
64
22
0.
22
97
6.
79
16
0.
56
17
0.
60
54
C
un
ei
w
it
h
A
li
gn
m
en
t
0.
22
06
6.
26
34
0.
55
55
0.
61
28
0.
20
58
6.
41
16
0.
54
25
0.
63
91
0.
22
91
6.
84
64
0.
56
65
0.
60
03
C
un
ei
w
it
h
A
li
gn
m
en
t&
C
on
te
xt
0.
21
70
6.
28
02
0.
55
67
0.
61
25
0.
20
65
6.
43
91
0.
53
98
0.
63
62
0.
23
15
6.
88
29
0.
56
76
0.
59
84
Ta
bl
e
3:
O
ve
rv
ie
w
of
C
ze
ch
to
E
ng
li
sh
E
va
lu
at
io
ns
D
ev
el
op
m
en
t
Tu
ni
ng
D
ev
el
op
m
en
t
Te
st
B
lin
d
Te
st
B
L
E
U
N
IS
T
M
et
eo
r
T
E
R
B
L
E
U
N
IS
T
M
et
eo
r
T
E
R
B
L
E
U
N
IS
T
M
et
eo
r
T
E
R
C
un
ei
0.
20
18
5.
98
47
0.
53
26
0.
63
75
0.
21
25
6.
36
39
0.
53
42
0.
64
30
0.
22
10
6.
63
55
0.
55
73
0.
62
24
+
O
ri
gi
ns
0.
20
10
6.
02
33
0.
53
70
0.
63
53
0.
21
50
6.
41
54
0.
53
61
0.
63
91
0.
22
21
6.
67
19
0.
56
09
0.
62
08
+
A
dj
ac
en
tL
en
gt
h
&
S
ke
w
0.
20
02
6.
00
80
0.
53
38
0.
64
02
0.
21
47
6.
41
83
0.
53
54
0.
64
31
0.
22
37
6.
73
36
0.
55
74
0.
61
72
+
A
dj
ac
en
tN
-g
ra
m
s
0.
20
11
5.
96
48
0.
53
10
0.
64
10
0.
21
37
6.
35
98
0.
53
29
0.
64
34
0.
22
35
6.
66
56
0.
55
64
0.
62
02
+
D
oc
C
os
in
e
&
JS
D
0.
19
87
5.
95
14
0.
53
05
0.
64
22
0.
21
34
6.
34
98
0.
53
24
0.
64
56
0.
22
28
6.
66
47
0.
55
79
0.
62
09
+
D
oc
P
re
ci
si
on
&
R
ec
al
l
0.
20
07
5.
97
64
0.
53
15
0.
63
76
0.
21
45
6.
39
84
0.
53
61
0.
64
10
0.
22
44
6.
69
00
0.
56
08
0.
62
06
Ta
bl
e
4:
B
re
ak
do
w
n
of
C
on
te
xt
F
ea
tu
re
s
in
G
er
m
an
to
E
ng
li
sh
154
