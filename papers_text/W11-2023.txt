Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 204?215,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Commitments to Preferences in Dialogue
Anais Cadilhac*, Nicholas Asher*, Farah Benamara*, Alex Lascarides**
*IRIT, University of Toulouse, **School of Informatics, University of Edinburgh
Abstract
We propose a method for modelling how dialogue
moves influence and are influenced by the agents?
preferences. We extract constraints on preferences
and dependencies among them, even when they are
expressed indirectly, by exploiting discourse struc-
ture. Our method relies on a study of 20 dia-
logues chosen at random from the Verbmobil cor-
pus. We then test the algorithms predictions against
the judgements of naive annotators on 3 random un-
seen dialogues. The average annotator-algorithm
agreement and the average inter-annotator agree-
ment show that our method is reliable.
1 Introduction
Dialogues are structured by various moves that the
participants make?e.g., answering questions, asking
follow-up questions, elaborating prior claims, and so
on. Such moves come with commitments to certain at-
titudes such as intentions and preferences. While map-
ping utterances to their underlying intentions is well
studied through the application of plan recognition tech-
niques (e.g., Grosz and Sidner (1990), Allen and Litman
(1987)), game-theoretic models of rationality generally
suggest that intentions result from a deliberation to find
the optimal tradeoff between one?s preferences and one?s
beliefs about possible outcomes (Rasmusen, 2007). So
mapping dialogue moves to preferences is an important
task: for instance, they are vital in decisions on how to
re-plan and repair should the agents? current plan fail, for
they inform the agents about the relative importance of
their various goals. Classical game theory, however, de-
mands a complete and cardinal representation of prefer-
ences for the optimal intention to be defined. This is not
realistic for modelling dialogue because agents often lack
complete information about preferences prior to talking:
they learn about the domain, each other?s preferences and
even their own preferences through dialogue exchange.
For instance, utterance (1) implies that the speaker wants
to go to the mall given that he wants to eat, but we do not
know his preferences over ?go to the mall? if he does not
want to eat.
(1) I want to go to the mall to eat something.
Existing formal models of dialogue content either do not
formalise a link between utterances and preferences (e.g.,
Ginzburg (to appear)), or they encode such links in a
typed feature structure, where desire is represented as a
feature that takes conjunctions of values as arguments
(e.g., Poesio and Traum (1998)), making the language
too restricted to express dependencies among preferences
of the kind we just described. Existing implemented
dialogue systems likewise typically represent goals as
simple combinations of values on certain information
?slots? (e.g., He and Young (2005), Lemon and Pietquin
(2007)); thus (1) yields a conjunction of preferences, to
go to the mall and to eat something. But such a system
could lead to suboptimal dialogue moves?e.g., to help
the speaker go to the mall even if he has already received
food.
What?s required, then, is a method for extracting par-
tial information about preferences and the dependencies
among them that are expressed in dialogue, perhaps indi-
rectly, and a method for exploiting that partial informa-
tion to identify the next optimal action. This paper pro-
poses a method for achieving these tasks by exploiting
discourse structure.
We exploited the corpus of Baldridge and Lascarides
(2005a), who annotated 100 randomly chosen sponta-
neous face-to-face dialogues from the Verbmobil cor-
pus (Wahlster, 2000) with their discourse structure ac-
cording to Segmented Discourse Representation Theory
(SDRT, Asher and Lascarides (2003))?these structures
represent the types of (relational) speech acts that the
agents perform. Here?s a typical fragment:
(2) a. A: Shall we meet sometime in the next
week?
b. A: What days are good for you?
c. B: Well, I have some free time on almost
every day except Fridays.
204
d. B: In fact, I?m busy on Thursday too.
e. A: So perhaps Monday?
Across the corpus, more than 30% of the discourse units
are either questions or assertions that help to elaborate a
plan to achieve the preferences revealed by a prior part
of the dialogue?these are marked respectively with the
discourse relations Q-Elab and Plan-Elab in SDRT, and
utterances (2b) and (2e) and the segments (2c) and (2d)
invoke these relations (see Section 2). Moreover, 10% of
the moves revise or correct prior preferences (like (2d)).
We will model the interaction between dialogue con-
tent and preferences in two steps. The first maps ut-
terances and their rhetorical connections into a partial
description of the agents? preferences. The mapping is
compositional and monotonic over the dialogue?s logi-
cal form (i.e., the description of preferences for an ex-
tended segment is defined in terms of and always sub-
sumes those for its subsegments): it exploits recursion
over discourse structure. The descriptions partially de-
scribe ceteris paribus preference nets or CP-nets with
Boolean variables (Boutilier et al, 2004). We chose CP-
nets over alternative logics of preferences, because they
provide a compact, computationally efficient, qualitative
and relational representation of preferences and their de-
pendencies, making them compatible with the kind of
partial information about preferences that utterances re-
veal. Our mapping from the logical form of dialogue
to partial descriptions of Boolean CP-nets proceeds in a
purely linguistic or domain independent way (e.g., it ig-
nores information such as Monday and Tuesday cannot
co-refer) and will therefore apply to dialogue generally
and not just Verbmobil.
In a second stage, we ?compress? and refine our descrip-
tion making use of constraints proper to CP-nets (e.g.,
that preference is transitive) and constraints provided by
the domain?in this case constraints about times and
places, as well as constraints from deep semantics. This
second step reduces the complexity of inferring which
CP-net(s) satisfy the partial description and allows us to
identify the minimal CP-net that satisfies the domain-
dependent description of preferences. We can thus ex-
ploit dependencies between dialogue moves and mental
states in a compact, efficient and intuitive way.
We start by motivating and describing the semantic repre-
sentation of dialogue from which our CP-net descriptions
and then our CP-nets will be constructed.
2 The Logical Form of Dialogue
Our starting point for representing dialogue con-
tent is SDRT. Like Hobbs et al (1993) and
Mann and Thompson (1987), it structures discourse
into units that are linked together with rhetorical re-
lations such as Explanation, Question Answer Pair
(QAP), Q-Elab, Plan-Elab, and so on. Logical forms
in SDRT consist of Segmented Discourse Representation
Structures (SDRSs). As defined in Asher and Lascarides
(2003), an SDRS is a set of labels representing discourse
units, and a mapping from each label to an SDRS-formula
representing its content?these formulas are based on
those for representing clauses or elementary discourse
units (EDUs) plus rhetorical relation symbols between
labels. Lascarides and Asher (2009) argue that to make
accurate predictions about acceptance and denial, both
of which can be implicated rather than linguistically
explicit, the logical form of dialogue should track each
agent?s commitments to content, including rhetorical
connections. They represent a dialogue turn (where turn
boundaries occur whenever the speaker changes) as a
set of SDRSs?one for each agent representing all his
current commitments, from the beginning of the dialogue
to the end of that turn. The representation of the dialogue
overall?a Dialogue SDRS or DSDRS?is that of each of
its turns. Each agent constructs the SDRSs for all other
agents as well as his own. For instance, (2) is assigned
the DSDRS in Table 1, with the content of the EDUs
omitted for reasons of space (see Lascarides and Asher
(2009) for details). We adopt a convention of indexing
the root label of the nth turn, spoken by agent d, as
nd; and pi : ? means that ? describes pi?s content (we?ll
sometimes also write ?pi to identify this description).
We now return to our example (2). Intuitively, (2a) com-
mits A to a preference for meeting next week but it does
so indirectly: the preference is not asserted, or equiva-
lently entailed at the level of content from the semantics
of Q-Elab(a,b). Accordingly, responding with "I do too"
(meaning "I want to meet next week too") is correctly pre-
dicted to be highly anomalous. A?s SDRS for turn 1 in Ta-
ble 1 commits him to the questions (2a) and (2b) because
Q-Elab is veridical: i.e. Q-Elab(a,b) entails the dynamic
conjunction ?a ??b. Since intuitively (2a) commits A to
the implicature that he prefers next week, our algorithm
for eliciting preferences from dialogue must ascribe this
preference to A on the basis of his move Q-Elab(a,b).
Furthermore,Q-Elab(a,b) entails that any answer to (2b)
must elaborate a plan to achieve the preference revealed
by (2a); this makes ?b paraphrasable as ?What days next
week are good for you??, which does not add new prefer-
ences.
B?s contribution in the second turn attaches to (2b) with
QAP and also Plan-Elab?he answers with a non-empty
extension for what days. Lascarides and Asher (2009) ar-
gue that this means that B is also committed to the illo-
cutionary contribution of (2b), as shown in Table 1 by
the addition of Q-Elab(a,b) to B?s SDRS. This addition
commits B also to the preference of meeting next week,
with his answer making the preferencemore precise: (2c)
reveals that B prefers any day except Friday; by linking
(2d) with Plan-Correction he retracts the preference for
Thursday. This compels A to revise his inferences about
205
Turn A?s SDRS B?s SDRS
1 pi1A : Q-Elab(a,b) /0
2 pi1A : Q-Elab(a,b) pi2B : Q-Elab(a,b)?QAP(b,pi)?Plan-Elab(b,pi)
pi : Plan-Correction(c,d)
3 pi3A : Q-Elab(a,b)?QAP(b,pi)?Plan-Elab(b,pi)? pi2B : Q-Elab(a,b)?QAP(b,pi)?Plan-Elab(b,pi)
Plan-Elab(pi,e) pi : Plan-Correction(c,d)
Table 1: The DSDRS for Dialogue (2).
B?s preference for meeting on Thursday. A?s Plan-Elab
move (2e) in the third turn reveals another preference for
Monday. This may not match his preferred day when the
dialogue started: perhaps that was Friday. He may con-
tinue to prefer that day. But engaging in dialogue can
compel agents to revise their commitments to preferences
as they learn about the domain and each other.
The above discussion of (2) exhibits how different types
of rhetorical relations between utterances rather than
Searle-like speech acts like question, construed as a prop-
erty of an utterance, are useful for encoding how pref-
erences evolve in a dialogue and how they relate to
one another. While the Grounding Acts dialogue model
(Poesio and Traum, 1998) and the Question Under Dis-
cussion (QUD) model (Ginzburg, to appear) both have
many attractive features, they do not encode as fine-
grained a taxonomy of types of speech acts and their se-
mantic effects as SDRT: in SDRT each rhetorical relation
is a different kind of (relational) speech act, so that, for
instance, the speech act of questioning is divided into the
distinct types Q-Elab, Plan-Correction, and others. For
the QUD model to encode such relations would require
implicit questions of all sorts of different types to be in-
cluded in the taxonomy, in which case the result may be
equivalent to the SDRT taxonomy of dialogue moves. We
have not explored this eventual equivalence here.
3 CP-nets and CP-net descriptions
A preference is standardly understood as an ordering by
an agent over outcomes; at the very least it entails a com-
parison between one entity and another (outcomes being
one sort of entity among others). As indicated in the in-
troduction, we are interested in an ordinal definition of
preferences, which consists in imposing an ordering over
all (relevant) possible outcomes. Among these outcomes,
some are acceptable for the agent, in the sense that the
agent is ready to act in such a way as to realize them;
and some outcomes are not acceptable. Amongst the ac-
ceptable outcomes, the agent will typically prefer some
to others. Our method does not try to determine the most
preferred outcome of an agent but follows rather the evo-
lution of their commitments to certain preferences as the
dialogue proceeds. To give an example, if an agent pro-
poses to meet on a certain day X and at a certain time Y,
we infer that among the agent?s acceptable outcomes is a
meeting on X at Y, even if this is not his most preferred
outcome (see earlier discussion of (2e)).
A CP-net (Boutilier et al, 2004) offers a compact rep-
resentation of preferences. It is a graphical model that
exploits conditional preferential independence so as to
structure the decision maker?s preferences under a ceteris
paribus assumption.
Although CP-nets generally consider variables with a fi-
nite range of values, to define the mapping from dialogue
turns to descriptions of CP-nets in a domain indepen-
dent and compositional way, we use Boolean proposi-
tional variables: each variable describes an action that an
agent can choose to perform, or not. We will then refine
the CP-net description by using domain-specific informa-
tion, transforming CP-nets with binary valued variables
to CP-nets with multiple valued variables. This reduces
the complexity of the evaluation of the CP-net by a large
factor.
More formally, let V be a finite set of propositional vari-
ables and LV the description language built from V via
Boolean connectives and the constants ? (true) and ?
(false). Formulas of LV are denoted by ?,?, etc. 2V is the
set of interpretations for V , and as usual for M ? 2V and
x?V , M gives the value true to x if x?M and false other-
wise. Where X ?V , let 2X be the set of X-interpretations.
X-interpretations are denoted by listing all variables of
X , with a ? symbol when the variable is set to false: e.g.,
where X = {a,b,d}, the X-interpretation M = {a,d} is
expressed as abd.
A preference relation  is a reflexive and transitive bi-
nary relation on 2V with strict preference ? defined in
the usual way (i.e., M  M? but M? 6 M). Note that
preference orderings are not necessarily complete, since
some candidates may not be comparable by a given agent.
An agent is said to be indifferent between two options
M,M? ? 2V , written M ?M?, if M M? and M? M.
As we stated earlier, CP-nets exploit conditional pref-
erential independence to compute a preferential ranking
over outcomes:
Definition 1 Let V be a set of propositional variables
and {X ,Y,Z} a partition of V . X is conditionally pref-
erentially independent of Y given Z if and only if ?z ?
2Z , ?x1,x2 ? 2X and ?y1,y2 ? 2Y we have: x1y1z 
206
x2y1z iff x1y2z x2y2z.
For each variable X , the agent specifies a set of parent
variables Pa(X) that can affect his preferences over the
values of X . Formally, X is conditionally preferentially
independent of V \ ({X}?Pa(X)). This is then used to
create the CP-net.
Definition 2 Let V be a set of propositional variables.
N = ?G ,T ? is a CP-net on V , where G is a directed
graph over V , and T is a set of Conditional Preference
Tables (CPTs) with indifference. That is, T = {CPT(X j):
X j ? V}, where CPT(X j) specifies for each instantiation
p ? 2Pa(X j) either x j ?p x j, x j ?p x j or x j ?p x j.
The following simple example illustrates these defini-
tions. Suppose our agent prefers to go from Paris to
Hong Kong by day rather than overnight. If he takes an
overnight trip, he prefers a non stop flight, but if he goes
by day, he prefers a flight with a stop. Figure 1 shows the
associated CP-net. The variable T stands for the prefer-
ence over the period of travel. Its values are Td for a day
trip and Tn for a night one. The variable St stands for the
preference over stops. Its values are S for a trip with stops
and S without.
T
St
CPT(T) = Td ? Tn
CPT(St) = Td : S ? S
Tn : S? S
Figure 1: Travel CP-net
With CP-nets defined, we proceed to a description lan-
guage for them. The description language formula w ?
y(CPT ) describes a CP-net where a CPT contains an en-
try of the form w ?p y for some possibly empty list of
parent variables p. A CP-net description is a set of such
formulas. The CP-net N |= x1, . . .xn : w? y(CPT ) iff the
CP-net N ?s CPT T contains an entry w?~u y?also writ-
ten~u :w? y?where x1, . . .xn figure in~u. Satisfaction of a
description formula by a CP-net yields a notion of logical
consequence between a CP-net descriptionD N and a de-
scription formula in the obvious way. Dialogue turns also
sometimes inform us that certain variables enter into pref-
erence statements. We?ll express the fact that the vari-
ables x1, . . . ,xn are associated with discourse constituent
pi by the formula x1, . . . ,xn(P(pi)), where P(pi) refers to
the partial description of the preferences expressed by the
discourse unit pi (see Section 4).
The description language allows us to impose constraints
on the CP-nets that agents commit to without specifying
the CP-net completely, as is required for utterances like
(1). In section 6, we describe how to construct a min-
imal CP-net from a satisfiable CP-net description. One
can then use the forward sweep procedure for outcome
optimisation (Boutilier et al, 2004). This is a proce-
dure of linear complexity, which consists in instantiating
variables following an order compatible with the graph,
choosing for each variable (one of) its preferred values
given the value of the parents.
4 From EDUs to Preferences
EDUs are described in SDRT using essentially Boolean
formulas over labels (Asher and Lascarides, 2003); thus
?(pi)??(pi) means that ? and ? describe aspects of pi?s
content. Not(pi1,pi)? ?(pi1) means that the logical form
of the EDU pi is of the form ?pi1 and that pi1 is described
by ?; so pi has the content ??. Our task is to map such
descriptions of content into descriptions of preferences.
Our preference descriptions will use Boolean connectives
and operators over preference entries (e.g., of the form
x ? y): namely, &,?, 7?, and a modal operator ?. The
rules below explain the semantics of preference opera-
tors (they are in effect defined in terms of the semantics
of buletic attitudes and Boolean connectives) and how
to recursively calculate preference descriptions from the
EDU?s logical structure.
Simple EDUs can provide atomic preference statements
(e.g., I want X or We need X). This means that with this
EDU the speaker commits to a preference for X . X will
typically involve a Boolean variable and a preference en-
try for its CPT. P(pi) is the label of the preference descrip-
tion associated with discourse unit pi. Hence for a sim-
ple EDU pi, we have X(P(pi)) as its description. Simple
EDUs also sometimes express preferences in an indirect
way (see (2a)).
More generally, P recursively exploits the logical struc-
ture of an EDU?s logical form to produce an EDU pref-
erence representation (EDUPR). For instance, since the
logical form of the EDU I want fish and wine features
conjunction, likewise so does its preference description:
?&?(P(pi)) means that among the preferences included
in pi, the agent prefers to have both ? and ? and prefers ei-
ther one if he can?t have both.1 We also have disjunctions
(let?s meet Thursday or Friday), and negations (I don?t
want to meet on Friday), whose preferences we?ll express
respectively as Thurs?Fri(P(pi)) and ?Fri(P(pi)).
Some EDUs express commitments to dependencies
among preferences. For example, in the sentence What
about Monday, in the afternoon?, there are two prefer-
ences: one for the day Monday, and, given the Monday
preference, one for the time afternoon (of Monday), at
least on one syntactic disambiguation. We represent this
dependency as Mon 7? Aft(P(pi)). Note that 7? is not
expressible with just Boolean operators. Finally, EDUs
can express commitment to preferences via free choice
1The full set of rules also includes a stronger conjunction ???(P(pi))
(the agent prefers both ? and ?, but is indifferent if he can?t have both).
207
modalities; I am free on Thursday, or?Thurs(P(pi)), tells
us that Thursday is a possible day to meet. ?? says that ?
is an acceptable outcome (as described earlier, this means
the agent is ready to act so as to realize an outcome that
entails ?). Thus, ??(pi) entails ?(pi), and ?-embedded
preferences obey reduction axioms permitting ? to be
eliminated when combined with other preference oper-
ators. But a ? preference statement does affect a prefer-
ence description when is is conjoined in Boolean fashion
with another ? preference statement in an EDU or com-
bined via a discourse relation like Continuation. This is
because ? is a free choice modality and obeys the equiv-
alence (3) below, which in turn yields a disjunctive pref-
erence ???(P(pi)) from what appeared to be a conjunc-
tion.2
(3) (??(P(pi))???(P(pi)))??(???)(P(pi))
The variables introduced by a discourse segment pi are
integrated into the CP-net description D N via the oper-
ation Commit(pi,D N ). The following seven rules cover
the different possible logical structures for the EDU pref-
erence representation. In the following, X ,Y,Z,W denote
propositional variables and ?, ? propositional formulas
from EDUPR. Var(?) are the variables in ?, and ?X
the preference relation describing CPT (X). Sat(?) (or
non-Sat(?)) is a conjunction of literals from Var(?) that
satisfy (or do not satisfy) ?. Sat(?)?X is the formula that
results from removing the conjunct with X from Sat(?).
1. Where X(P(pi)) (X is a variable of P(pi), e.g., I want
X), Commit(pi,D N ) adds the description D N |=
X ? X(CPT (X)).3
2. Where ?&?(P(pi)), Commit(pi,D N ) adds descrip-
tions as follows:
? For each X ?Var(?), addVar(?) to Pa(X) and
modifyCPT (X) as follows:
If Sati(?), Sat j(?) ? X (resp. X), then Sati(?),
Sat j(?)?X : X ? X (resp. X ? X), for all sat-
isfiers i and j.
? Similarly for each Y ?Var(?).
If ? and ? are literals X and Y we get: D N |= Y ?
Y (CPT (Y )) and D N |= X ? X(CPT (X)). Graph-
ically, this yields the following preference relation
(where one way arrows denote preference, two way
2We provide here the reduction axioms over preference descriptions
1. ?(?&?)(P(pi))? (?&?)(P(pi))
2. ?(? 7? ?)(P(pi))? (? 7? ?)(P(pi))
3. ?(???)? (?? ?)(P(pi))
4. ???(P(pi))???(P(pi))
3Given our description language semantics, this means that any
CP-net which satisfies the description D N contains a preference ta-
ble CPT (X) with an entry X ? X with at least one instantiation of the
variables in Pa(X).
arrows denote indifference or equal preference, and
no arrow means the options are incomparable):
XY
XY XY
XY
3. Where ?? ?(P(pi)) (the agent prefers to have at
least one of ? and ? satisfied). If ? and ? are X
and Y , we get:
? Var(X) ? Pa(Var(Y )) and D N |= X : Y ?
Y (CPT (Y )), D N |= X : Y ? Y (CPT (Y )).
? Var(Y ) ? Pa(Var(X)) and D N |= Y : X ?
X(CPT (X)), D N |= Y : X ? X(CPT (X)).
This corresponds to the following preference rela-
tion:
XY
XY XY XY
As before, the use of indifference allows us to find
the best outcomes (XY , XY and XY ) easily.
4. Where ? 7? ?(P(pi)) (the agent prefers that ? is sat-
isfied and if so that ? is also satisfied. If ? is not
satisfied, it is not possible to define preferences on
?). If ? and ? are X and Y , we get:
? D N |= X ? X(CPT (X))
? Var(X) ? Pa(Var(Y)) and
D N |= X : Y ? Y (CPT (Y )).
Note that this description is also produced by
Elab(pii,pi j) below where X(P(pii)) and Y (P(pi j))
(see rule 8). Thus the implication symbol 7? is a
"shortcut" in that it represents elaborations whose
arguments are in the same EDU.
5. Where ??(P(pi)) (the agent prefers a free choice of
?). Given the behaviour of?, this reduces to treating
?(P(pi)).
6. Where ??(P(pi)). We can apply rules 1-5 by con-
verting ?? into conjunctive normal form.
7. Where ?(P(pi))??(P(pi)), with ? and ? nonmodal,
we simply apply the rule for ? and that for ?.
5 From Discourse Structure to Preferences
We must now define how the agents? preferences, repre-
sented as a partial description of a CP-net, are built com-
positionally from the discourse structure over EDUs. The
constraints are different for different discourse relations,
reflecting the fact that the semantics of connections be-
tween segments influences how their preferences relate
to one another.
We will add rules for defining Commit over la-
bels pi whose content ?pi express rhetorical relations
R(pii,pi j)?indeed, we overload the notation and write
Commit(R(pii,pi j),D N ). Since Commit applies com-
positionally, starting with the EDUs and working up
208
the discourse structure towards the unique root la-
bel of the SDRS, we can assume in our definition of
Commit(R(pii,pi j),D N ) that the EDUPRs are already de-
fined. We give rules for all the relations in the Verbmobil
corpus, though we will be very brief with those that are
less prevalent. A complete example using our rules is in
appendix A.
IExplanation, Elab, Plan-Elab, Q-elab
IExplanation(pii, pi j): i.e., pi j?s preferences explain pii?s
(e.g., see (1), where P(pii) would be going to the mall
and P(pi j) is eating something). With Elab(pii, pi j) a
preference in pii is elaborated on or developed in pi j,
as in: I want wine. I want white wine. That is, a
preference for white wine depends on a preference for
wine. Plan-Elab(pii,pi j) means that pi j describes a plan
for achieving the preferences expressed by pii, and with
Q-Elab we have a similar dependence between prefer-
ences, but the second constituent is a question (so often
in practice this means preference commitments from pii
transfer from one agent to another).
Plan-Elab(pi j,pii), Elab(pi j,pii) and IExplanation(pii,pi j)
all follow the same two-step rule, and so from the point
of view of preference updates they are equivalent:
8. i Firstly, preference description D N is up-
dated according to P(pi j) by applying
Commit(pi j,D N ), if pi j expresses a new
preference. If not go to step (ii).
ii. Secondly, description D N is modified so that
each variable in P(pii) depends on each vari-
able in P(pi j): i.e., ?X ? Var(P(pii)), ?Y ?
Var(P(pi j)), Y ? Pa(X). Then, D N is enriched
according to P(pii), if pii expresses a preference.
If it does not, then end.
We now give some details concerning step (ii) above. To
this end, let ? denote a formula with SDRS description
predicates, ?? its corresponding boolean (preference) for-
mula and ?? its negation. Then for ?=Y , we define ??=Y
and ?? = Y ; for ? = Y 7? Z we define ?? = Y ? Z and
?? = Y ? Z; and for ? = Y ? Z and ? = Y&Z, we have
?? = Y ?Z and ?? = Y ?Z.
a. X(P(pii)) and ?(P(pi j)). The agent explains his pref-
erences on X by ?. So, if no preferences on X are
already defined, ? is a reason to prefer X . That is,
D N |= ??: X ? X(CPT (X)). However, it is not pos-
sible to define preferences on X if ? is false. If, on
the other hand, preferences on X are already defined,
the agent prefers X if ? is satisfied, and does not
modify his preferences otherwise?i.e.,?X ,??= X ?
X , ?X ,??=?X .4
4If we have ?X such that Z: X ? X , Z: X ? X , ?X ,?? represents
preferences defined by Z??? and Z???, whereas ?X ,?? represents pref-
erences defined by Z??? and Z???.
For ? = Y , if ?X is not already defined, we obtain
the following preference relation (no information on
the preference for X if Y is false makes XY and XY
incomparable):
XY
XYXY
XY
b. X?Z(P(pii)) and ?(P(pi j)). The agent explains his
preferences X?Z by ?: he wants to satisfy X or Z
if ? is satisfied.
First, we set Var(Z) ? Pa(Var(X)), Var(X) ?
Pa(Var(Z)). If ?X is not already defined, we have:
D N |= ?? ? Z: X ? X(CPT (X)), D N |= ?? ? Z:
X ? X(CPT (X)).
Otherwise, ?X ,??,Z= X ? X , ?X ,??,Z= X ? X ,
?X ,??,Z= ?X ,??,Z= ?X .
CPT (Z) is defined asCPT (X) by inverting X and Z.
For ? =Y , if ?X and?Z are not already defined, we
obtain the following preference relation (again, the
lack of preference information on X and Z when Y
is false yields incomparability among states whereY
is false):
XYZXYZ
XYZ
XYZ
XYZ
XYZ
XYZXYZ
c. X&Z(P(pii)) and ?(P(pi j)). The agent explains his
preferences on X&Z by ?.
? If ?X is not already defined, we have: D N |=
?? : X ? X(CPT (X)).
Otherwise, ?X ,??= X ? X , ?X ,??= ?X ,
? CPT (Z) is defined as CPT (X) by replacing X
by Z.
d. X 7? Z(P(pii)) and ?(P(pi j)). The agent explains his
preferences on X 7? Z by ?: he wants to satisfy X
and after Z if ? is satisfied.
If ?X is not already defined, we have D N |= ?? :
X ? X(CPT (X)) and we setVar(X)? Pa(Var(Z)).5
If ?Z is not yet defined, we have : D N |= ?? ?X :
Z ? Z(CPT (Z)), D N |= ???X : Z ? Z(CPT (Z)).
Else, ?Z,(???X)= Z ? Z, ?Z,(???X)= Z ? Z,
?Z,(???X)= ?Z,(???X)=?Z.
e. ?(P(pii)) and ?(P(pi j)). We can apply rules 8 by
decomposing ?.
5Otherwise, there is no need to modify ?X . This is what we call a
?partial elaboration?. Variables that were evoked since preferences on
X were introduced are parents of Z but not of X . For example, if an
agent commits to a preference for Monday then Afternoon, and later in
the discourse he commits to 2oclock, then Afternoon is 2oclock?s parent
but not Monday?s.
209
f. ?(?)(P(pii)) and ?(?)(P(pi j)). We treat this like a
free choice EDU (see rule 5).
g. ?(?)(P(pii)) and ?(P(pi j)), where ? is non modal.
We treat this like ?(P(pii)) and ?(P(pi j)) (see rule
8.e)
Let?s briefly look at how the rule changes for
Q-elabA(pi1,pi2) (where the subscript A identifies the
speaker of pi2):
9. Q-ElabA(pi1,pi2) implies that we update A?s CP-
net description D N by applying the rule for
Elab(pi1,pi2), where if pi2 expresses no preferences
on their own, we simply make the P(pi2) description
equal to the P(pi1) description. Thus A?s CP-net de-
scription is updated with the preferences expressed
by utterance pi1, regardless of who said pi1.
QAPAnswers to questions affect preferences in complex
ways:
10. The first case concerns yes/no questions and there
are two cases, depending on whether B replies yes
or no:
Yes QAPB(pi1,pi2) where pi2 is yes. B?s pref-
erence descriptions are updated by apply-
ing Commit(ElabB(pi1,pi2),D N ) (and so B?s
preference description include preferences ex-
pressed by pi1 and pi2).
No QAPB(pi1,pi2) where pi2 is no. If P(pi1)
and P(pi2) are consistent, then B?s pref-
erence descriptions are updated by ap-
plying CommitB(ElabB(pi1,pi2),D N );
otherwise, they are updated by applying
Commit(Correction(pi1,pi2),D N ) (see rule
13).
11. When pi1 is a wh-question and QAPB(pi1,pi2), B?s
preferences over variables in pi1 and pi2 are ex-
actly the same as the ones defined for a yes/no
question where the answer is yes. Variables in pi2
will refine preferences over variables in pi1. So,
B?s preference descriptions are updated by applying
CommitB(ElabB(pi1,pi2),D N ).
In previous rules, it is relatively clear how to update the
preference commitments. However, in some cases it?s not
clear what the answer in a QAP targets: in Could we meet
the 25 in the morning? No, I can?t., we do not know if
No is about the 25 and the morning, or only about the
morning. So, we define the following rule for managing
cases where the target is unknown :
12. If we know the target, we can change the description
of the CP-net. Otherwise, we wait to learn more.
Correction and Plan-Correction allow a speaker to rec-
tify a prior commitment to preferences. Self-corrections
also occur in the corpus: I could do it on the 27th. No I
can not make it on the 27th, sorry I have a seminar. Cor-
rection and Plan-Correction can have several effects on
the preferences. For instance, they can correct preference
entries. That is, given Correction(pi1,pi2), some variables
in P(pi1) are replaced by variables in P(pi2) (in the self-
correction example, every occurrence of 27 in P(pi1) is
replaced with 27 and vice versa). We have a set of rules
of the form X ?{Y1, . . . ,Ym}, which means that the vari-
able X ? Var(P(pi1)) is replaced by the set of variables
{Y1, . . . ,Ym} ? Var(P(pi2)). We assume that X can?t de-
pend on {Y1, . . . ,Ym} before the Correction is performed.
Then replacement proceeds as follows:
13. If Pa(X) = /0, we add the description D N |= Yk ?
Y k(CPT (Yk)) for all k ? {1, . . . ,m} and remove X ?
X(CPT (X)) (or X ? X(CPT (X))). Otherwise, we
replace every description ofCPT (X) with an equiv-
alent statement using Yk (to describe CPT (Yk)), for
all k ? {1, . . .m}.
The specific target of the correction behaves similarly to
the target of a QAP. In some cases we don?t know the
target, in which case we apply rule 12.
Plan-Correction can also lead to the modification of an
agent?s own plan because of other agent?s proposals. In
this case it corrects the list of parent variables on which
a preference depends. We call that list of variables the
operative variables. Once the operative variables are
changed, Plan-Correction can elaborate a plan if some
new preferences are expressed. For example, all agents
have agreed to meet next week, so in their CP-net descrip-
tion, there is the entry Week1?Week1. Then discussion
shows that their availabilities are not compatible and one
of them says "okay, that week is not going to work.". That
does not mean the agent prefersWeek1 toWeek1 because
both agreed on Week1 as preferable. Rather, Week1 has
been removed as an operative variable in the following
discourse segments. This leads us to the following rule:
14. For Plan-Correction(pi1,pi2) which corrects
the list of parent variables, the operative vari-
able list becomes the intersection of all Pa(X)
where X ? Var(P(pi1)). We can now apply
Commit(Plan-Elab(pi1,pi2),D N ), if P(pi2) contains
some new preferences ?. If the CPT affected by a
rule has no entry for the current operative variable
list O , then O : ? has to be added to D N .
Continuation, Contrast and Q-Cont pattern with the
rule for Elab. Alternation patterns with rule 8.b.6 Expla-
nation, Explanation*,Result, Qclar (clarification ques-
tion), Commentary, Summary and Acknowledgment
6The rule for Alternative questions like Do you want fish or chicken?
is a special case yielding ???(P(pi)), but we don?t offer details here.
210
either do nothing or have the same effect on preference
elicitation as Elab. Sometimes, adding these preferences
via the Elab rule may yield an unsatisfiable CP-net de-
scription, because an implicit correction is involved. If an
evaluation of the CP-net (see next section) is performed
after a processing of one of these rules shows that the
CP-net description is not satisfiable, then we apply the
rule 13, associated with Correction.
6 From Descriptions to Models
Each dialogue turn adds constraints monotonically to the
descriptions of the CP-nets to which the dialogue partic-
ipants commit. We have interpreted each new declared
variable in our rules as independent, which allows us to
give a domain independent description of preference elic-
itation. However, when it comes to evaluating a CP-net
description for satisfiability, we need to take into account
various axioms about preference (irreflexivity and transi-
tivity), and axioms for the domain of conversation: in our
case, temporal designations (Wednesdays are not Tues-
days and so on). This typically adds dependencies among
the variables in the description. In the case of the Verb-
mobil domain, since the variable Monday means essen-
tially "to meet on Monday", Monday implies Meet , and
this must be reflected via a dependency in the CP-net: we
must view the variable Meet as filling a hidden slot in
the variable Monday in the preference description, Meet :
Mon?Mon. This likewise allows us to fill in the negative
clauses of the CP-net description: we can now infer that
Meet : Mon ? Mon. These axioms also predict certain
preference descriptions to be unsatisfiable. For instance,
if we have Mon ?Mon, our axioms imply Mon ? Tues,
Mon ?Wed, etc. At this point we can calculate, ceteris
paribus, inconsistencies on afternoons and mornings of
particular days.
Domain knowledge also allows us to collapse Boolean
valued variables that all denote, say, days or times of the
day into multiple valued variables. So for instance, our
domain independent algorithm from dialogue moves to
preference descriptions might yield:
(4) Meet?31.01?30.01?02.02: am? am
Domain knowledge collapses all Boolean variables for
distinct days into one variable with values for days to get:
(5) Meet?02.02: am? pm
This leads to a sizeable reduction in the set of variables
that are used in the CP-net.
We can test any CP-net description for satisfiability by
turning the description formulas into CP-net entries. Our
description automatically produces a directed graph over
the parent variables. We have to check that the ? state-
ments form an irreflexive and transitive relation and that
each variable introduced into the CP-net has a preference
entry consistent given these constraints. If the description
does not yield a preference entry for a given variable X ,
we will add the indifference formula X ? X as the entry.
If our CP-net description meets these requirements, this
procedure yields a minimal CP-net. Testing for satisfia-
bility is useful in eliciting preferences from several dis-
course moves like Explanation, Qclar or Result, since in
the case of unsatisfiability, we will exploit the Correction
rule 13 with these moves.
7 Evaluation of the proposed method
We evaluate our method by testing it against the judg-
ments of three annotators on three randomly chosen un-
seen test dialogues from the Verbmobil corpus. The
test corpus contains 75 EDUs and the proportion of dis-
course relations is the same as in the corpus overall. The
three annotators were naive in the sense that they were
not familiar with preference representations and prefer-
ence reasoning strategies. For each dialogue segment,
we checked if the judges had the same intuitions that we
did on: (i) how commitments to preferences are extracted
from EDUs, and (ii) how preferences evolve through dia-
logue exchange.
The judges were given a manual with all the instructions
and definitions needed to make the annotations. For ex-
ample, the manual defined preference to be "a notion of
comparison between one thing at least one other". The
manual also instructs annotators to label each EDU with
the following four bits of information: (1) preferences
(if any) expressed in the EDU; (2) dependencies between
preferences expressed in the EDU; (3) dependencies be-
tween preferences in the current EDU and previous ones;
and (4) preference evolution (namely, the appearance of
a new factor that affects preferred outcomes, update to
preferences over values for an existing factor, and so on).
For each of these four components, example dialogues
were given for each type of decision they would need to
make, and instructions were given on the format in which
to code their judgements. Appendix A shows an example
of an annotated dialogue.
Table 2 presents results of the evaluation of (i). For each
EDU, we asked the annotator to list the preferences ex-
pressed in the EDU and we compared the preferences ex-
tracted by each judge with those extracted by our algo-
rithm. The triple (a, b, c) respectively indicates the pro-
portion of common preferences (two preference sets ?i
and ? j are common if (?i = ? j) or (?x ? ?i,y ? ? j ,x?
y)?for example, the preference MeetBefore2?MeetAt2
implies MeetAt2 ? MeetAt2), the proportion of prefer-
ences that one judge extracts and the other judge or our al-
gorithm misses and the proportion of preferences missed
by one judge and extracted by the other judge or by our
algorithm. The average annotator-algorithm agreement
(AAA) is 75.6% and the average inter-annotator agree-
211
Our algorithm J1 J2 J3 % of EDUs that commit to preferences
Our algorithm (83, 4, 13) (91, 0, 9) (91, 0, 9) 76%
J1 (83, 13, 4) (85, 7, 8) (91, 4, 5) 80%
J2 (91, 9, 0) (85, 8, 7) (92, 4, 4) 86%
J3 (91, 9, 0) (91, 5, 4) (92, 4, 4) 84%
Table 2: Evaluating how preferences are extracted from EDUs.
Our algorithm J1 J2 J3
Our algorithm (85, 71) (96, 100) (93, 86)
J1 (85, 71) (89, 71) (91, 86)
J2 (96, 100) (89, 71) (98, 86)
J3 (93, 86) (91, 86) (98, 86)
Table 3: Evaluating how preferences evolve through dialogue.
ment (IAA) is 77.9%; this shows that our method for ex-
tracting preferences from EDUs is reliable.
The evaluation (ii) proceeds as follows. For each EDU, we
ask the judge if the segment introduces new preferences
or if it updates, corrects or deletes preferences commited
in previous turns. As in (i), judges have to justify their
choices. Table 3 presents the preliminary results where
the couple (a,b) indicates respectively the proportion of
common elaborations (preference updates or new prefer-
ences) and the proportion of common corrections. Since
elaboration is also applied in case of other discourse re-
lations (e.g., Q-Elab), the measure a evaluates the rules
8, 9, 10 (yes) and 11. Similarly, the measure b evalu-
ates the rules 10 (no), 13 and 14. We obtain AAA=91%
IAA=92.7% for elaboration and AAA=85.7% IAA=81%
for correction.
8 Conclusion
We have proposed a compositional method for elicit-
ing preferences from dialogue consisting of a domain-
independent algorithm for constructing a partial CP-net
description of preferences, followed by a domain-specific
method for identifying the minimal CP-net satisfying the
partial description and domain constraints. The method
supports qualitative and partial information about prefer-
ences, with CP-nets benefiting from linear algorithms for
computing the optimal outcome from a set of preferences
and their dependencies. The need to compute intentions
from partially defined preferences is crucial in dialogue,
since preferences are acquired and change through dia-
logue exchange.
Our work partially confirms that CP-nets have a certain
naturalness, as the map from dialogue moves to prefer-
ences using the CP-net formalism is relatively intuitive.
The next step is to implement our method. This depends
on extracting discourse structure from text, which, though
difficult, is becoming increasingly tractable for simple
domains (Baldridge and Lascarides, 2005b). We plan to
extract CP-net descriptions from EDUs and to evaluate
these descriptions using "multi-valued variables" auto-
matically. We will then evaluate our method on a large
number of dialogues.
Our work here is also and more generally a first step to-
wards modelling the complex interaction between what
agents say, what their preferences are, and what they take
the preferences of other dialogue agents to be. It leads
to a conception of dialogue that?s more general than one
based purely on Gricean cooperative principles (Grice,
1975). On a purely Gricean approach, conversation is
cooperative in at least two ways: a basic level concern-
ing the conventions that govern linguistic meaning (ba-
sic cooperativity); and a level concerning shared attitudes
towards what is said, including shared intentions (con-
tent cooperativity). While basic cooperation is needed
for communication to work at all, content cooperativ-
ity involves strongly cooperative axioms like Coopera-
tivity (interlocutors normally adopt the speaker?s inten-
tions) (Allen and Litman, 1987, Grosz and Sidner, 1990,
Lochbaum, 1998). Our approach allows for divergent
preferences and divergent intentions, i.e. conversations
that aren?t based on content cooperativity. This will al-
low us to exploit information about conflicting agents?
preferences and game-theoretic techniques that are inher-
ent in the logics of CP-nets for computing optimal moves
(Bonzon, 2007). And in contrast to Franke et al (2009),
who analyse conversations where content cooperativity
doesn?t hold using a game-theoretic framework, our ap-
proach allows for partial and qualitative representations
of preferences rather than demanding complete and quan-
titative representations of them.
212
References
J. Allen and D. Litman. A plan recognition model for
subdialogues in conversations. Cognitive Science, 11
(2):163?200, 1987.
N. Asher and A. Lascarides. Logics of Conversation.
Cambridge University Press, 2003.
J. Baldridge and A. Lascarides. Annotating discourse
structures for robust semantic interpretation. In Pro-
ceedings of the Sixth International Workshop on Com-
putational Semantics (IWCS), Tilburg, The Nether-
lands, 2005a.
J. Baldridge and A. Lascarides. Probabilistic head-driven
parsing for discourse structure. In Proceedings of
the Ninth Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 96?103, 2005b.
E. Bonzon. Mod?lisation des Interactions entre Agents
Rationnels: les Jeux Bool?ens. PhD thesis, Universit?
Paul Sabatier, Toulouse, 2007.
C. Boutilier, R.I. Brafman, C. Domshlak, H.H. Hoos, and
David Poole. Cp-nets: A tool for representing and
reasoning with conditional ceteris paribus preference
statements. Journal of Artificial Intelligence Research,
21:135?191, 2004.
M. Franke, T. de Jager, and R. van Rooij. Relevance in
cooperation and conflict. Journal of Logic and Lan-
guage, 2009.
J. Ginzburg. The Interactive Stance: Meaning for Con-
versation. CSLI Publications, to appear.
H. P. Grice. Logic and conversation. In P. Cole and
J. L. Morgan, editors, Syntax and Semantics Volume
3: Speech Acts, pages 41?58. Academic Press, 1975.
B. Grosz and C. Sidner. Plans for discourse. In J. Mor-
gan P. R. Cohen and M. Pollack, editors, Intentions in
Communication, pages 365?388. MIT Press, 1990.
Y. He and S. Young. Spoken language understsanding
using the hidden vector state model. Speech Commu-
nication, 48(3-4):262?275, 2005.
J. R. Hobbs, M. Stickel, D. Appelt, and P. Martin. Inter-
pretation as abduction. Artificial Intelligence, 63(1?2):
69?142, 1993.
A. Lascarides and N. Asher. Agreement, disputes and
commitment in dialogue. Journal of Semantics, 26(2):
109?158, 2009.
O. Lemon and O. Pietquin. Machine learning for spoken
dialogue systems. In Interspeech, 2007.
K. E. Lochbaum. A collaborative planning model of in-
tentional structure. Computational Linguistics, 24(4):
525?572, 1998.
W. C. Mann and S. A. Thompson. Rhetorical structure
theory: A framework for the analysis of texts. Interna-
tional Pragmatics Association Papers in Pragmatics,
1:79?105, 1987.
M. Poesio and D. Traum. Towards an axiomatisation of
dialogue acts. In J. Hulstijn and A. Nijholt, editors,
Proceedings of the Twente Workshop on the Formal Se-
mantics and Pragmatics of Dialogue. 1998.
E. Rasmusen. Games and Information: An Introduction
to Game Theory. Blackwell Publishing, 2007.
W. Wahlster, editor. Verbmobil: Foundations of Speech-
to-Speech Translation. Springer, 2000.
213
Appendix A : Treatment of an example
We illustrate in this section how our rules work on an
example. Since this dialogue was also evaluated by our
judges (cf section 7), we give where relevant some details
on those annotations. The example is as follows:
(6) pi1. A: so, I guess we should have another meet-
ing
pi2. A: how long do you think it should be for.
pi3. B: well, I think we have quite a bit to talk
about.
pi4. B: maybe, two hours?
pi5. B: how does that sound.
pi6. A: deadly,
pi7. A: but, let us do it anyways.
pi8. B: okay, do you have any time next week?
pi9. B: I have got, afternoons on Tuesday and
Thursday.
pi10. A: I am out of Tuesday Wednesday Thurs-
day,
pi11. A: so, how about Monday or Friday
Table 4 is the DSDRS associated with (6).
Relation(pii, [pi j ? pik]) indicates that a rhetorical re-
lation holds between the segment pii and a segment
consisting of pi j, pi j+1, . . . , pik
pi1 provides an atomic preference. We apply the rule
1 and so CommitA(pi1,D N A) adds the description
D N A |=M ?M(CPT (M)) where M means Meet.
pi2 We have Q-Elab(pi1, pi2). A continues to commit to
M in pi2 and no new preferences are introduced by
pi2. We apply rule 9, which makes the P(pi2) de-
scription the same as P(pi1)?s.
pi3 is linked to pi2 with QAP. B accepts A?s preference
and we apply the rule 11 since pi2 is a wh-question.
Thus CommitB(ElabB(pi2,pi3),D N B) adds the de-
scription D N B |= M ?M(CPT (M)). It is interest-
ing to note that some judges consider that agent?s
utterance in pi3 indicates a preference towards "talk-
ing a long time" while other judges consider, as our
method predicts, that this segment does not convey
any preference.
pi4 is linked to pi3 by Q-Elab. B commits to a new
preference. We apply rule 9, rule 8 and then rule
8.a. The preference on the hour is now dependent
on the preference on meeting; i.e., D N B |= M :
2h ? 2h(CPT (2h)), where the variable 2h means
two hours.
pi5 is related to pi4 with the Q-Cont relation. We
then follow the same rule as the continued relation,
namely Q-Elab. We apply rule 9 which does not
change the CP-net description of B because pi5 does
not convey any preference.
pi6 is related to pi5 with QAP relation. In this case, it?s
not clear what is the QAP target and so we apply
rule 12: we wait to learn more and we do not change
B?s CP-net description.
All the Judges indicated that segments pi5 and pi6
are ambiguous and therefore hesitated to say if they
commit to preferences. For example in pi6, do we
have a preference for meeting more than 2 hours
or less than 2 hours? This indecision is compatible
with the predictions of rule 12.
pi7 A accepts B?s preference. We apply rule 9 and then
rule 8 to obtain:
D N A |=M ?M(CPT (M)),
D N A |=M : 2h? 2h(CPT (2h)).
pi8 is linked to pi7 by Q-Elab. B introduces a new pref-
erence for meeting next week.
We apply rule 9 and then 8 to obtain:
D N B |=M ?M(CPT (M)),
D N B |=M : 2h? 2h(CPT (2h)),
D N B |=M?2h :NW ?NW (CPT (NW ))where the
variable NW means next week.
pi9 is linked to pi8 by Plan-Elab. pi9 expresses com-
mitments to preference that already involve a
CP-net description. B introduces three prefer-
ences: one for meeting on Tuesday, the other
for meeting on Thursday and given the conjunc-
tion of preferences Tues ? Thurs, one for time
afternoon (of Tuesday and Thursday). That is,
((?(Tues)??(Thurs)) 7? Aft)(P(pi9)). We apply
the equivalence (3) and obtain :
(?(Tues?Thurs)? Aft)(P(pi9)).
Then, we apply rules 8.g, 8.b and 8.d. The CP-net
description of B is thus updated as follows:
D N B |= M ? 2h ? NW ? Tues : Thurs ?
Thurs(CPT (Thurs)),
D N B |= M ? 2h ? NW ? Tues : Thurs ?
Thurs(CPT (Thurs)),
D N B |= M ? 2h ? NW ? Thurs : Tues ?
Tues(CPT (Tues)),
D N B |= M ? 2h ? NW ? Thurs : Tues ?
Tues(CPT (Tues)),
D N B |= M ? 2h ? NW ? (Thurs ? Tues) : Aft ?
Aft(CPT (Aft)).
Most judges express here a preference ranking over
outcomes. For instance, if B elaborates by adding
the preference "I have got Monday morning too"
(as it is in the test corpus), some consider the rank-
ing "(Tuesday or Thursday afternoons) ? (Monday
214
Turn A?s SDRS B?s SDRS
1 pi1A : Q-Elab(pi1,pi2) /0
2 pi1A:is the same as in turn 1 pi2B : Q-Elab(pi1, [pi2?pi5])?QAP(pi2, [pi3?pi5])?
Q-Elab(pi3,pi)
pi : Q-Cont(pi4,pi5)
3 pi3A : Q-Elab(pi1, [pi2?pi7])?QAP(pi2, [pi3?pi7])? pi2B: is the same as in turn 2
Q-Elab(pi3, [pi4,pi7])?QAP(pi,pi?)
pi : Q-Cont(pi4,pi5),pi? : Contrast(pi6,pi7)
4 pi3A: is the same as in turn 3 pi4B : Q-Elab(pi1, [pi2?pi9])?QAP(pi2, [pi3?pi9])?
Q-Elab(pi3, [pi4?pi9])?QAP(pi, [pi6?pi9])?
Q-Elab(pi?,pi??)
pi : Q-Cont(pi4,pi5),pi? : Contrast(pi6,pi7)
pi?? : Plan-Elab(pi8,pi9)
5 pi5A : Q-Elab(pi1, [pi2?pi11])?QAP(pi2, [pi3?pi11])? pi4B: is the same as in turn 4
Q-Elab(pi3, [pi4?pi11])?QAP(pi, [pi6?pi11])?
Q-Elab(pi?, [pi8?pi11])?QAP(pi??,pi???)
pi : Q-Cont(pi4,pi5),pi? : Contrast(pi6,pi7)
pi?? : Plan-Elab(pi8,pi9),pi??? : Q-Elab(pi10,pi11)
Table 4: The DSDRS for Dialogue (6).
morning)? (other days)", while others consider the
ranking "(Tuesday or Thursday afternoon) or (Mon-
day morning)? (other days)". We did not treat such
preference ranking.
pi10 is related to pi9 by QAP where A answers no to B?s
question asked in pi8. We apply rule 10 (no). Since
Tues&Weds&Thurs(P(pi10)) is not consistent with
((?(Tues) ??(Thurs)) 7? Aft)(P(pi9)), we apply
CommitA(Correction(pi9,pi10),D N A), which adds
the preference Weds to A?s description and then
the rule 13 where Tues and Thurs are respectively
replaced by Tues and Thurs :
D N A |=M?2h?NW : Tues? Tues(CPT (Tues)),
D N A |= M ? 2h ? NW : Thurs ?
Thurs(CPT (Thurs)),
D N A |= M ? 2h ? NW : Weds ?
Weds(CPT (Weds)).
pi11 Finally, this segment is linked to pi10 with Q-Elab
where Mond?Fri(P(pi11)). We apply rules 9 and
8.b and update A?s CP-net description as follows:
D N A |=M?2h?NW ?Tues?Thurs?Weds?Fri :
Mond ?Mond(CPT (Mond)),
D N A |=M?2h?NW ?Tues?Thurs?Weds?Fri :
Mond ?Mond(CPT (Mond)),
D N A |= M ? 2h ? NW ? Tues ? Thurs ?Weds ?
Mond : Fri? Fri(CPT (Fri)),
D N A |= M ? 2h ? NW ? Tues ? Thurs ?Weds ?
Mond : Fri? Fri(CPT (Fri)).
The evaluation of this dialogue also reveals to what extent
naive annotators reason with binary (Monday preferred
to not Monday) or multi-valued variables (Monday pre-
ferred to Tuesday). Most judges use multi-valued vari-
ables to express the preference extracted from an EDU,
and the way in which our method exploits domain knowl-
edge to yield the minimal CP-net satisfying the descrip-
tion reflects this. In addition, some judges use a small
set of variables (for example the variable time of meeting
that groups together the notion of week, day, hours, etc.)
while others use a distinct variable for each preference.
Finally, we also noticed that judges do not describe the
same preference dependencies. For example, in:
(7) We could have lunch together and then have the
meeting from one to three?
some consider that the preference on having lunch is in-
dependent from the preference on the meeting (in this
case, they consider that the preference on the period one
to three is independent from the preference on meeting)
while others consider that the two preferences are depen-
dent.
215
