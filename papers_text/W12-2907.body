NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 47?55,
Montre?al, Canada, June 7?8, 2012. c?2012 Association for Computational Linguistics
Communication strategies for a computerized caregiver for individuals with
Alzheimer?s disease
Frank Rudzicz1,2, ? and Rozanne Wilson1 and Alex Mihailidis2 and Elizabeth Rochon1
1 Department of Speech-Language Pathology,
2 Department of Occupational Science and Occupational Therapy
University of Toronto
Toronto Canada
Carol Leonard
School of Rehabilitation Sciences
University of Ottawa
Ottawa Canada
Abstract
Currently, health care costs associated with
aging at home can be prohibitive if individ-
uals require continual or periodic supervision
or assistance because of Alzheimer?s disease.
These costs, normally associated with human
caregivers, can be mitigated to some extent
given automated systems that mimic some of
their functions. In this paper, we present in-
augural work towards producing a generic au-
tomated system that assists individuals with
Alzheimer?s to complete daily tasks using ver-
bal communication. Here, we show how to
improve rates of correct speech recognition
by preprocessing acoustic noise and by mod-
ifying the vocabulary according to the task.
We conclude by outlining current directions of
research including specialized grammars and
automatic detection of confusion.
1 Introduction
In the United States, approximately $100 billion are
spent annually on the direct and indirect care of in-
dividuals with Alzheimer?s disease (AD), the major-
ity of which is attributed to long-term institutional
care (Ernst et al., 1997). As the population ages, the
incidence of AD will double or triple, with Medi-
care costs alone reaching $189 billion in the US by
2015 (Bharucha et al., 2009). Given the growing
need to support this population, there is an increas-
ing interest in the design and development of tech-
nologies that support this population at home and
extend ones quality of life and autonomy (Mihailidis
et al., 2008).
?Contact: frank@cs.toronto.edu
Alzheimer?s disease is a type of progres-
sive neuro-degenerative dementia characterized by
marked declines in mental acuity, specifically in
cognitive, social, and functional capacity. A decline
in memory (short- and long-term), executive capac-
ity, visual-spacial reasoning, and linguistic ability
are all typical effects of AD (Cummings, 2004).
These declines make the completion of activities of
daily living (e.g., finances, preparing a meal) diffi-
cult and more severe declines often necessitate care-
giver assistance. Caregivers who assist individuals
with AD at home are common, but are often the pre-
cursor to placement in a long-term care (LTC) facil-
ity (Gaugler et al., 2009).
We are building systems that automate, where
possible, some of the support activities that currently
require family or formal (i.e., employed) caregivers.
Specifically, we are designing an intelligent dialog
component that can engage in two-way speech com-
munication with an individual in order to help guide
that individual towards the completion of certain
daily household tasks, including washing ones hands
and brushing ones teeth. A typical installation setup
in a bathroom, shown in figure 1, consists of video
cameras that track a user?s hands and the area in and
around the sink, as well as microphones, speakers,
and a screen that can display prompting informa-
tion. Similar installations are being tested in other
household rooms as part of the COACH project (Mi-
hailidis et al., 2008), according to the task; this is
an example of ambient intelligence in which tech-
nology embedded in the environment is sensitive to
the activities of the user with it (Spanoudakis et al.,
2010).
47
Our goal is to encode in software the kinds of
techniques used by caregivers to help their clients
achieve these activities; this includes automati-
cally identifying and recovering from breakdowns
in communication and flexibly adapting to the in-
dividual over time. Before such a system can be de-
ployed, the underlying models need to be adjusted
to the desired population and tasks. Similarly, the
speech output component would need to be pro-
grammed according to the vocabularies, grammars,
and dialog strategies used by caregivers. This paper
presents preliminary experiments towards dedicated
speech recognition for such a system. Evaluation
data were collected as part of a larger project exam-
ining the use of communication strategies by formal
caregivers while assisting residents with moderate to
severe AD during the completion of toothbrushing
(Wilson et al., 2012).
2 Background ? communication strategies
Automated communicative systems that are more
sensitive to the emotive and the mental states of their
users are often more successful than more neutral
conversational agents (Saini et al., 2005). In order to
be useful in practice, these communicative systems
need to mimic some of the techniques employed
by caregivers of individuals with AD. Often, these
caregivers are employed by local clinics or medical
institutions and are trained by those institutions in
ideal verbal communication strategies for use with
those having dementia (Hopper, 2001; Goldfarb and
Pietro, 2004). These include (Small et al., 2003) but
are not limited to:
1. Relatively slow rate of speech rate.
2. Verbatim repetition of misunderstood prompts.
3. Closed-ended questions (i.e., that elicit yes/no
responses).
4. Simple sentences with reduced syntactic com-
plexity.
5. Giving one question or one direction at a time.
6. Minimal use of pronouns.
These strategies, though often based on observa-
tional studies, are not necessarily based on quantita-
tive empirical research and may not be generalizable
across relevant populations. Indeed, Tomoeda et al.
(1990) showed that rates of speech that are too slow
(a) Environmental setup
(b) On-screen prompting
Figure 1: Setup and on-screen prompting for COACH.
The environment includes numerous sensors including
microphones and video cameras as well as a screen upon
which prompts can be displayed. In this example, the
user is prompted to lather their hands after having applied
soap. Images are copyright Intelligent Assistive Technol-
ogy and Systems Lab).
may interfere with comprehension if they introduce
48
problems of short-term retention of working mem-
ory. Small, Andersen, and Kempler (1997) showed
that paraphrased repetition is just as effective as ver-
batim repetition (indeed, syntactic variation of com-
mon semantics may assist comprehension). Further-
more, Rochon, Waters, and Caplan (2000) suggested
that the syntactic complexity of utterances is not
necessarily the only predictor of comprehension in
individuals with AD; rather, correct comprehension
of the semantics of sentences is inversely related to
the increasing number of propositions used ? it is
preferable to have as few clauses or core ideas as
possible, i.e., one-at-a-time.
Although not the empirical subject of this pa-
per, we are studying methods of automating the
resolution of communication breakdown. Much of
this work is based on the Trouble Source-Repair
(TSR) model in which difficulties in speaking, hear-
ing, or understanding are identified and repairs are
initiated and carried out (Schegloff, Jefferson, and
Sacks, 1977). Difficulties can arise in a number
of dimensions including phonological (i.e., mispro-
nunciation), morphological/syntactic (e.g., incorrect
agreement among constituents), semantic (e.g., dis-
turbances related to lexical access, word retrieval,
or word use), and discourse (i.e., misunderstanding
of topic, shared knowledge, or cohesion) (Orange,
Lubinsky, and Higginbotham, 1996). The major-
ity of TSR sequences involve self-correction of a
speaker?s own error, e.g., by repetition, elaboration,
or reduction of a troublesome utterance (Schegloff,
Jefferson, and Sacks, 1977). Orange, Lubinsky,
and Higginbotham (1996) showed that while 18%
of non-AD dyad utterances involved TSR, whereas
23.6% of early-stage AD dyads and 33% of middle-
stage AD dyads involved TSR. Of these, individu-
als with middle-stage AD exhibited more discourse-
related difficulties including inattention, failure to
track propositions and thematic information, and
deficits in working memory. The most common
repair initiators and repairs given communication
breakdown involved frequent ?wh-questions and hy-
potheses (e.g., ?Do you mean??). Conversational
partners of individuals with middle-stage AD initi-
ated repair less frequently than conversational part-
ners of control subjects, possibly aware of their de-
teriorating ability, or to avoid possible further con-
fusion. An alternative although very closely related
paradigm for measuring communication breakdown
is Trouble Indicating Behavior (TIB) in which the
confused participant implicitly or explicitly requests
aid. In a study of 7 seniors with moderate/severe de-
mentia and 3 with mild/moderate dementia, Watson
(1999) showed that there was a significant difference
in TIB use (? < 0.005) between individuals with
AD and the general population. Individuals with
AD are most likely to exhibit dysfluency, lack of up-
take in the dialog, metalinguistic comments (e.g., ?I
can?t think of the word?), neutral requests for repeti-
tion, whereas the general population are most likely
to exhibit hypothesis formation to resolve ambiguity
(e.g., ?Oh, so you mean that you had a good time??)
or requests for more information.
2.1 The task of handwashing
Our current work is based on a study completed by
Wilson et al. (2012) towards a systematic observa-
tional representation of communication behaviours
of formal caregivers assisting individuals with mod-
erate to severe AD during hand washing. In that
study, caregivers produced 1691 utterances, 78% of
which contained at least one communication strat-
egy. On average, 23.35 (? = 14.11) verbal strate-
gies and 7.81 (? = 5.13) non-verbal strategies were
used per session. The five most common communi-
cation strategies employed by caregivers are ranked
in table 1. The one proposition strategy refers to
using a single direction, request, or idea in the utter-
ance (e.g. ?turn the water on?). The closed-ended
question strategy refers to asking question with a
very limited, typically binary, response (e.g., ?can
you turn the taps on??) as opposed to questions elic-
iting a more elaborate response or the inclusion of
additional information. The encouraging comments
strategy refers to any verbal praise of the resident
(e.g., ?you are doing a good job?). The paraphrased
repetition strategy is the restatement of a misunder-
stood utterance using alternative syntactic or lexical
content (e.g., ?soap up your hands....please use soap
on your hands?). There was no significant difference
between the use of paraphrased and verbatim repe-
tition of misunderstood utterances. Caregivers also
reduced speech rate from an average baseline of 116
words per minute (s.d. 36.8) to an average of 36.5
words per minute (s.d. 19.8).
The least frequently used communication strate-
49
Number of occurrences % use of strategy Uses per session
Verbal strategy Overall Successful Overall Successful Mean SD
One proposition 619 441 35 36 8.6 6.7
Closed-ended question 215 148 12 12 3.0 3.0
Encouraging comments 180 148 10 12 2.9 2.5
Use of resident?s name 178 131 10 11 2.8 2.5
Paraphrased repetition 178 122 10 10 3.0 2.5
Table 1: Most frequent verbal communication strategies according to their number of occurrences in dyad communi-
cation. The % use of strategy is normalized across all strategies, most of which are not listed. These results are split
according to the total number of uses and the number of uses in successful resolution of a communication breakdown.
Mean (and standard deviation) of uses per session are given across caregivers. Adapted with permission from Wilson
et al. (2012).
gies employed by experienced caregivers involved
asking questions that required verification of a res-
ident?s request or response (e.g., ?do you mean
that you are finished??), explanation of current ac-
tions (e.g., ?I am turning on the taps for you?), and
open-ended questions (e.g., ?how do you wash your
hands??).
The most common non-verbal strategies em-
ployed by experienced caregivers were guided touch
(193 times, 122 of which were successful) in which
the caregiver physically assists the resident in the
completion of a task, demonstrating action (113
times. 72 of which were successful) in which an
action is illustrated or mimicked by the caregiver,
handing an object to the resident (107 times, 85 of
which were successful), and pointing to an object
(105 times, 95 of which were successful) in which
the direction to an object is visually indicated by
the caregiver. Some of these strategies may be em-
ployed by the proposed system; for example, videos
demonstrating an action may be displayed on the
screen shown in figure 1(a), which may replace to
some extent the mimicry by the caregiver. A pos-
sible replication of the fourth most common non-
verbal strategy may be to highlight the required ob-
ject with a flashing light, a spotlight, or by display-
ing it on screen; these solutions require tangential
technologies that are beyond the scope of this cur-
rent study, however.
3 Data
Our experiments are based on data collected by Wil-
son et al. (submitted) with individuals diagnosed
with moderate-to-severe AD who were recruited
from long-term care facilities (i.e., The Harold and
Grace Baker Centre and the Lakeside Long-Term
Care Centre) in Toronto. Participants had no pre-
vious history of stroke, depression, psychosis, alco-
holism, drug abuse, or physical aggression towards
caregivers. Updated measures of disease severity
were taken according to the Mini-Mental State Ex-
amination (Folstein, Folstein, and McHugh, 1975).
The average cognitive impairment among 7 individ-
uals classified as having severe AD (scores below
10/30) was 3.43 (? = 3.36) and among 6 individ-
uals classified as having moderate AD (scores be-
tween 10/30 and 19/30) was 15.8 (? = 4.07). The
average age of residents was 81.4 years with an aver-
age of 13.8 years of education and 3.1 years of resi-
dency at their respective LTC facility. Fifteen formal
caregivers participated in this study and were paired
with the residents (i.e., as dyads) during the comple-
tion of activities of daily living. All but one care-
giver were female and were comfortable with En-
glish. The average number of years of experience
working with AD patients was 12.87 (? = 9.61).
The toothbrushing task follows the protocol of the
handwashing task. In total, the data consists of 336
utterances by the residents and 2623 utterances by
their caregivers; this is manifested by residents utter-
ing 1012 words and caregivers uttering 12166 words
in total, using 747 unique terms. The toothbrushing
task consists of 9 subtasks, namely: 1) get brush and
paste, 2) put paste on brush, 3) turn on water, 4) wet
tooth brush, 5) brush teeth, 6) rinse mouth, 7) rinse
brush, 8) turn off water, 9) dry mouth.
These data were recorded as part of a large
project to study communication strategies of care-
givers rather than to study the acoustics of their
transactions with residents. As a result, the record-
50
ings were not of the highest acoustic quality; for
example, although the sampling rate and bit rate
were high (48 kHz and 384 kbps respectively), the
video camera used was placed relatively far from the
speakers, who generally faced away from the mi-
crophone towards the sink and running water. The
distribution of strategies employed by caregivers for
this task is the subject of ongoing work.
4 Experiments in speech recognition
Our first component of an automated caregiver
is the speech recognition subsystem. We test
two alternative systems, namely Carnegie Mellon?s
Sphinx framework and Microsoft?s Speech Plat-
form. Carnegie Mellon?s Sphinx framework (pock-
etsphinx, specifically) is an open-source speech
recognition system that uses traditional N -gram
language modeling, sub-phonetic acoustic hidden
Markov models (HMMs), Viterbi decoding and
lexical-tree structures (Lamere et al., 2003). Sphinx
includes tools to perform traditional Baum-Welch
estimation of acoustic models, but there were not
enough data for this purpose. The second ASR sys-
tem, Microsoft?s Speech Platform (version 11) is
less open but exposes the ability to vary the lexicon,
grammar, and semantics. Traditionally, Microsoft
has used continuous-density HMMs with 6000 tied
HMM states (senones), 20 Gaussians per state, and
Mel-cepstrum features (with delta and delta-delta).
Given the toothbrushing data described in section
3, two sets of experiments were devised to config-
ure these systems to the task. Specifically, we per-
form preprocessing of the acoustics to remove envi-
ronmental noise associated with toothbrushing and
adapt the lexica of the two systems, as described in
the following subsections.
4.1 Noise reduction
An emergent feature of the toothbrushing data is
very high levels of acoustic noise caused by the
running of water. In fact, the estimated signal-to-
noise ratio across utterances range from ?2.103 dB
to 7.63 dB, which is extremely low; for comparison
clean speech typically has an SNR of approximately
40 dB. Since the resident is likely to be situated close
to this source of the acoustic noise, it becomes im-
portant to isolate their speech in the incoming signal.
Speech enhancement involves the removal of
acoustic noise d(t) in a signal y(t), including am-
bient noise (e.g., running water, wind) and signal
degradation giving the clean ?source? signal x(t).
This involves an assumption that noise is strictly ad-
ditive, as in the formula:
y(t) = x(t) + d(t). (1)
Here, Yk, Xk, and Dk are the kth spectra of the
noisy observation y(t), source signal x(t), and un-
correlated noise signal d(t), respectively. Generally,
the spectral magnitude of a signal is more important
than its phase when assessing signal quality and per-
forming speech enhancement. Spectral subtraction
(SS), as the name suggests, subtracts an estimate of
the noisy spectrum from the measured signal (Boll,
1979; Martin, 2001), where the estimate of the noisy
signal is estimated from samples of the noise source
exclusively. That is, one has to learn estimates based
on pre-selected recordings of noise. We apply SS
speech enhancement given sample recordings of wa-
ter running. The second method of enhancement
we consider is the log-spectral amplitude estimator
(LSAE) which minimizes the mean squared error
(MMSE) of the log spectra given a model for the
source speech Xk = Ak exp(j?k), where Ak is the
spectral amplitude. The LSAE method is a modifi-
cation to the short-time spectral amplitude estima-
tor that attempts to find some estimate A?k that min-
imizes the distortion
E
[(
logAk ? log A?k
)2
]
, (2)
such that the log-spectral amplitude estimate is
A?k = exp (E [lnAk |Yk])
=
?k
1 + ?k
exp
(
1
2
? ?
vk
e?t
t
dt
)
Rk,
(3)
where ?k is the a priori SNR,Rk is the noisy spectral
amplitude, vk =
?k
1+?k
?k, and ?k is the a posteriori
SNR (Erkelens, Jensen, and Heusdens, 2007). Of-
ten this is based on a Gaussian model of noise, as
it is here (Ephraim and Malah, 1985). We enhance
our recordings by both the SS and LSAE methods.
Archetypal instances of typical, low, and (relatively)
high SNR waveform recordings and their enhanced
versions are shown in 4.1.
51
(a) Dyad1.1
(b) Dyad4.2
(c) Dyad11.1
Figure 2: Representative samples of toothbrushing data
audio. Figures show normalized amplitude over time for
signals cleaned by the LSAE method overlaid over the
larger-amplitude original signals.
We compare the effects of this enhanced audio
across two ASR systems. For the Sphinx system,
we use a continuous tristate HMM for each of the 40
phones from the CMU dictionary trained with audio
from the complete Wall Street Journal corpus and
the independent variable we changed was the num-
ber of Gaussians per state (n. ?). These parame-
ters are not exposed by the Microsoft speech system,
so we instead vary the minimum threshold of confi-
dence C ? [0..1] required to accept a word; in theory
lower values of C would result in more insertion er-
rors and higher values would result in more deletion
errors. For each system, we used a common dic-
tionary of 123, 611 unique words derived from the
Carnegie Mellon phonemic dictionary.
Table 2 shows the word error rate for each of
the two systems. Both the SS and LSAE methods
of speech enhancement result in significantly better
word error rates than with the original recordings at
the 99.9% level of confidence according to the one-
tailed paired t-test across both systems. The LSAE
method has significantly better word error rates than
the SS method at the 99% level of confidence with
this test. Although these high WERs are impractical
for a typical system, they are comparable to other re-
sults for speech recognition in very low-SNR envi-
ronments (Kim and Rose, 2003). Deng et al. (2000),
for example, describe an ASR system trained with
clean speech that has a WER of 87.11% given addi-
tive white noise for a resulting 5 dB SNR signal for
a comparable vocabulary of 5000 words. An inter-
esting observation is that even at the low confidence
threshold of C = 0.2, the number of insertion er-
rors did not increase dramatically relative to for the
higher values in the Microsoft system; only 4.0% of
all word errors were insertion errors at C = 0.2, and
2.7% of all word errors at C = 0.8.
Given Levenshtein alignments between annotated
target (reference) and hypothesis word sequences,
we separate word errors across residents and across
caregivers. Specifically, table 3 shows the propor-
tion of deletion and substitution word errors (relative
to totals for each system separately) across residents
and caregivers. This analysis aims to uncover dif-
ferences in rates of recognition between those with
AD and the more general population. For exam-
ple, 12.6% of deletion errors made by Sphinx were
words spoken by residents. It is not possible to at-
52
Word error rate %
Parameters Original SS LSAE
Sphinx
n. ? = 4 98.13 75.31 70.61
n. ? = 8 98.13 74.95 69.66
n. ? = 16 97.82 75.09 69.78
n. ? = 32 97.13 74.88 67.22
Microsoft
C = 0.8 97.67 73.59 67.11
C = 0.6 97.44 72.57 67.08
C = 0.4 96.85 71.78 66.54
C = 0.2 94.30 71.36 64.32
Table 2: Word error rates for the Sphinx and Microsoft
ASR systems according to their respective adjusted pa-
rameters, i.e., number of Gaussians per HMM state (n. ?)
and minimum confidence threshold (C). Results are given
on original recordings and waveforms enhanced by spec-
tral subraction (SS) and MMSE with log-spectral ampli-
tude estimates (LSAE).
tribute word insertion errors to either the resident or
caregiver, in general. If we assume that errors should
be distributed across residents and caregivers in the
same proportion as their respective total number of
words uttered, then we can compute the Pearson ?2
statistic of significance. Given that 7.68% of all
words were uttered by residents, the observed num-
ber of substitutions was significantly different than
the expected value at the 99% level of confidence
for both the Sphinx and Microsoft systems, but the
number of deletions was not significantly different
even at the 95% level of confidence. In either case,
however, substantially more errors are made propor-
tionally by residents than we might expect; this may
in part be caused by their relatively soft speech.
Proportion of errors
Sphinx Microsoft
Res. Careg. Res. Careg.
deletion 13.9 86.1 12.6 87.4
substitution 23.2 76.8 18.4 81.6
Table 3: Proportion of deletion and substitution errors
made by both (Res)idents and (Careg)ivers. Proportions
are relative to totals within each system.
4.2 Task-specific vocabulary
We limit the common vocabulary used in each
speech recognizer in order to be more specific to the
task. Specifically, we begin with the 747 words ut-
tered in the data as our most restricted vocabulary.
Then, we expand this vocabulary according to two
methods. The first method adds words that are se-
mantically similar to those already present. This
is performed by taking the most common sense for
each noun, verb, adjective, and adverb, then adding
each entry in the respective synonym sets accord-
ing to WordNet 3.0 (Miller, 1995). This results in
a vocabulary of 2890 words. At this point, we it-
eratively add increments of words at intervals of
10, 000 (up to 120, 000) by selecting random words
in the vocabulary and adding synonym sets for all
senses as well as antonyms, hypernyms, hyponyms,
meronyms, and holonyms. The result is a vocabu-
lary whose semantic domain becomes increasingly
generic. The second approach to adjusting the vo-
cabulary size is to add phonemic foils to more re-
stricted vocabularies. Specifically, as before, we be-
gin with the restricted 747 words observed in the
data but then add increments of new words that
are phonemically similar to existing words. This
is done exhaustively by selecting a random word
and searching for minimal phonemic misalignments
(i.e., edit distance) among out-of-vocabulary words
in the Carnegie Mellon phonemic dictionary. This
approach of adding decoy words is an attempt to
model increasing generalization of the systems. Ev-
ery vocabulary is translated into the format expected
by each recognizer so that each test involves a com-
mon set of words.
Word error rates are measured for each vocabu-
lary size across each ASR system and the manner in
which those vocabularies were constructed (seman-
tic or phonemic expansion). The results are shown
in figure 4.2 and are based on acoustics enhanced
by the LSAE method. Somewhat surprisingly, the
method used to alter the vocabulary did appear to
have a very large effect. Indeed, the WER across
the semantic and phonemic methods were correlated
at ? >= 0.99 across both ASR systems; there was
no significant difference between traces (within sys-
tem) even at the 60% level of confidence using the
two-tailed heteroscedastic t-test.
5 Ongoing work
This work represents the first phase of development
towards a complete communicative artificial care-
giver for the home. Here, we are focusing on the
53
102 103 104 105 10635
40
45
50
55
60
65
70
Vocabulary size
Wo
rd E
rror
 Ra
te (%
)
 
 
Sphinx ? Phonemic
Microsoft ? PhonemicSphinx ? Semantic
Microsoft ? Semantic
Figure 3: Word error rate versus size of vocabulary (log
scale) for each of the Sphinx and Microsoft ASR systems
according to whether the vocabularies were expanded by
semantic or phonemic similarity.
speech recognition component and have shown re-
ductions in error of up to 72% (Sphinx ASR with
n.? = 4) and 63.1% (Sphinx ASR), relative to base-
line rates of error. While significant, baseline er-
rors were so severe that other techniques will need
to be explored. We are now collecting additional
data by fixing the Microsoft Kinect sensor in the
environment, facing the resident; this is the default
configuration and may overcome some of the ob-
stacles present in our data. Specifically, the beam-
forming capabilities in the Kinect (generalizable to
other multi-microphone arrays) can isolate speech
events from ambient environmental noise (Balan and
Rosca, 2002). We are also collecting speech data for
a separate study in which individuals with AD are
placed before directional microphones and complete
tasks related to the perception of emotion.
As tasks can be broken down into non-linear (par-
tially ordered) sets of subtasks (e.g., replacing the
toothbrush is a subtask of toothbrushing), we are
specifying grammars ?by hand? specific to those sub-
tasks. Only some subset of all subtasks are possible
at any given time; e.g., one can only place tooth-
paste on the brush once both items have been re-
trieved. The possibility of these subtasks depend on
the state of the world which can only be estimated
through imperfect techniques ? typically computer
vision. Given the uncertainty of the state of the
world, we are integrating subtask-specific grammars
into a partially-observable Markov decision process
(POMDP). These grammars include the semantic
state variables of the world and break each task
down into a graph-structure of interdependent ac-
tions. Each ?action? is associated with its own gram-
mar subset of words and phrases that are likely to
be uttered during its performance, as well as a set
of prompts to be spoken by the system to aid the
user. Along these lines, we we will attempt to gen-
eralize the approach taken in section 4.2 to gener-
ate specific sub-vocabularies automatically for each
subtask. The relative weighting of words will be
modeled based on ongoing data collection.
Acknowledgments
This research was partially funded by Mitacs and
an operating grant from the Canadian Institutes of
Health Research and the American Alzheimer As-
sociation (ETAC program). The authors acknowl-
edge and thank the administrative staff, caregivers,
and residents at the Harold and Grace Baker Centre
and the Lakeside Long-Term Care Centre.
References
