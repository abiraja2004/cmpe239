Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 220?228,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Composing Simple Image Descriptions using Web-scale N-grams
Siming Li, Girish Kulkarni, Tamara L Berg, Alexander C Berg, and Yejin Choi
Department of Computer Science
Stony Brook University
NY 11794, USA
{silli, gkulkarni, tlberg, aberg, ychoi}@cs.stonybrook.edu
Abstract
Studying natural language, and especially how
people describe the world around them can
help us better understand the visual world. In
turn, it can also help us in the quest to generate
natural language that describes this world in a
human manner. We present a simple yet effec-
tive approach to automatically compose im-
age descriptions given computer vision based
inputs and using web-scale n-grams. Unlike
most previous work that summarizes or re-
trieves pre-existing text relevant to an image,
our method composes sentences entirely from
scratch. Experimental results indicate that it is
viable to generate simple textual descriptions
that are pertinent to the specific content of an
image, while permitting creativity in the de-
scription ? making for more human-like anno-
tations than previous approaches.
1 Introduction
Gaining a better understanding of natural language,
and especially natural language associated with im-
ages helps drive research in both computer vision
and natural language processing (e.g., Barnard et
al. (2003), Pastra et al (2003), Feng and Lapata
(2010b)). In this paper, we look at how to exploit
the enormous amount of textual data electronically
available today, web-scale n-gram data in particular,
in a simple yet highly effective approach to com-
pose image descriptions in natural language. Auto-
matic generation of image descriptions differs from
automatic image tagging (e.g., Leong et al (2010))
in that we aim to generate complex phrases or sen-
tences describing images rather than predicting in-
dividual words. These natural language descriptions
can be useful for a variety of applications, includ-
ing image retrieval, automatic video surveillance,
and providing image interpretations for visually im-
paired people.
Our work contrasts to most previous approaches
in four key aspects: first, we compose fresh sen-
tences from scratch, instead of retrieving (Farhadi et
al. (2010)), or summarizing existing text fragments
associated with an image (e.g., Aker and Gaizauskas
(2010), Feng and Lapata (2010a)). Second, we aim
to generate textual descriptions that are truthful to
the specific content of the image, whereas related
(but subtly different) work in automatic caption gen-
eration creates news-worthy text (Feng and Lapata
(2010a)) or encyclopedic text (Aker and Gaizauskas
(2010)) that is contextually relevant to the image, but
not closely pertinent to the specific content of the
image. Third, we aim to build a general image de-
scription method as compared to work that requires
domain specific hand-written grammar rules (Yao et
al. (2010)). Last, we allow for some creativity in
the generation process which produces more human-
like descriptions than a closely related, very recent
approach that drives annotation more directly from
computer vision inputs (Kulkarni et al, 2011).
In this work, we propose a novel surface realiza-
tion technique based on web-scale n-gram data. Our
approach consists of two steps: (n-gram) phrase se-
lection and (n-gram) phrase fusion. The first step
? phrase selection ? collects candidate phrases that
may be potentially useful for generating the descrip-
tion of a given image. This step naturally accom-
modates uncertainty in image recognition inputs as
220
Hairy goat under a tree 
Fluffy posturing sheep under a tree 
<furry;gray;brown,sheep>,by;near,<rusty;gray;green,tree> 
furry 
gray 
brown 
rusty 
gray 
green 
by 
near 
Figure 1: The big picture of our task to automatically
generate image description.
well as synonymous words and word re-ordering to
improve fluency. The second step ? phrase fusion
? finds the optimal compatible set of phrases us-
ing dynamic programming to compose a new (and
more complex) phrase that describes the image. We
compare the performance of our proposed approach
to three baselines based on conventional techniques:
language models, parsers, and templates.
Despite its simplicity, our approach is highly ef-
fective for composing image descriptions: it gen-
erates mostly appealing and presentable language,
while permitting creative writing at times (see Fig-
ure 5 for example results). We conclude from our
exploration that (1) it is viable to generate simple
textual descriptions that are germane to the specific
image content, and that (2) world knowledge implic-
itly encoded in natural language (e.g., web-scale n-
gram data) can help enhance image content recogni-
tion.
2 Image Recognition
Figure 1 depicts our system flow: a) an image is in-
put into our system, b) image recognition techniques
are used to extract visual content information, c) vi-
sual content is encoded as a set of triples, d) natural
language descriptions are generated.
In this section, we briefly describe the image
recognition system that extracts visual information
and encodes it as a set of triples. For a given image,
the image recognizer extracts objects, attributes and
spatial relationships among objects as follows:
1. Objects: including things (e.g., bird, bus, car)
and stuff (e.g., grass, water, sky, road) are de-
tected.
2. Visual attributes (e.g., feathered, black) are pre-
dicted for each object.
3. Spatial relationships (e.g., on, near, under) be-
tween objects are estimated.
In particular, object detectors are trained using state
of the art mixtures of multi-scale deformable parts
models (Felzenszwalb et al, 2010). Our set of
objects encompasses the 20 PASCAL 2010 object
challenge 1 categories as well as 4 additional cate-
gories for flower, laptop, tiger, and window trained
on images with associated bounding boxes from
Imagenet (Deng et al, 2009). Stuff detectors are
trained to detect regions corresponding to non-part
based object categories (sky, road, building, tree,
water, and grass) using linear SVMs trained on
the low level region features of (Farhadi et al,
2009). These are also trained on images with la-
beled bounding boxes from ImageNet and evaluated
at test time on a coarsely sampled grid of overlap-
ping square regions over whole images. Pixels in
any region with a classification probability above a
fixed threshold are treated as detections.
We select visual attribute characteristics that are
relevant to our object and stuff categories. Our at-
tribute terms include 21 visual modifiers ? adjec-
tives ? related to color (e.g. blue, gray), texture
(e.g. striped, furry), material (e.g. wooden, feath-
ered), general appearance (e.g. rusty, dirty, shiny),
and shape (e.g. rectangular) characteristics. The at-
tribute classifiers are trained on the low level fea-
tures of (Farhadi et al, 2009) using RBF kernel
SVMs. Preposition functions encoding spatial rela-
tionships between objects are hand designed to eval-
uate the spatial relationships between pairs of re-
gions in an image and provide a score for 16 prepo-
sitions (e.g., above, under, against, in etc).
1http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2010/
221
From these three types of visual output, we con-
struct a meaning representation of an image as a
set of triples (one triple for every pair of detected
objects). Each triple encodes a spatial relation be-
tween two objects in the following format: <<adj1,
obj1>, prep, <adj2, obj2>>. The generation pro-
cedure is elaborated in the following two sections.
3 Baseline Approaches to Surface
Realization
This section explores three baseline surface realiza-
tion approaches: language models (?3.1), random-
ized local search (?3.2), and template-based (?3.3).
Our best approach, phrase fusion using web-scale n-
grams follows in ?4.
3.1 Language Model Based Approach
For each triple, as described in ?2, we construct a
sentence. For instance, given the triple <<white,
cloud>, in, <blue, sky>>, we might generate
?There is a white cloud in the blue sky?.
We begin with a simple decoding scheme based
on language models. Let t be a triple, and let V t
be the set of words in t. We perform surface real-
ization by adding function words in-between words
in V t. As a concrete example, suppose we want to
determine whether to insert a function word x be-
tween a pair of words ? ? V t and ? ? V t. Then,
we need to compare the length-normalized probabil-
ity p?(?x?) with p?(??), where p? takes the n?th root
of the probability p for n-word sequences. We in-
sert the new function word x if p?(?x?) ? p?(??)
using the n-gram models, where the probability of
any given sequence w1, ..., wm is approximated by
p(w1, ..., wm) =
m?
i=1
p(wi|wi?(n?1), ..., wi?1)
Note that if we wish to reorder words in V t based on
n-gram based language models, then the decoding
problem becomes an instance of asymmetric trav-
eler?s salesman problem (NP-hard). For brevity, we
retain the original order of words in the given triple.
We later lift this restriction using the web-scale n-
gram based phrase fusion method introduced in ?4.
3.2 Randomized Local Search Approach
A much needed extension to the language model
based surface realization is incorporating parsers to
Begin Loop (until T iterations or convergence)
Choose a position i to revise at random
Choose an edit operation at random
If the edit yields a better score by LM and PCFG
Commit the edit
End Loop
Table 1: Pseudo code for a randomized local search ap-
proach. A possible edit operation includes insertion,
deletion, and replacement. The score of the current sen-
tence is determined by the multiplication LM-based prob-
ability and PCFG-based probability.
enforce long distance regularities for more gram-
matically correct generation. However, optimiz-
ing both language-model-based probabilities and
parser-based probabilities is intractable. Therefore,
we explore a randomized local search approach that
makes greedy revisions using both language models
and parsers. Randomized local search has been suc-
cessfully applied to intractable optimization prob-
lems in AI (e.g., Chisholm and Tadepalli (2002)) and
NLP (e.g., White and Cardie (2002)).
Table 1 shows the skeleton of the algorithm in our
study. Iterating through a loop, it chooses an edit
location and an edit operation (insert, delete, or re-
place) at random. If the edit yields a better score,
then we commit the edit, otherwise we jump to the
next iteration of the loop. We define the score as
score(X) = p?LM (X)p?PCFG(X)
where X is a given sentence (image description),
p?LM (X) is the length normalized probability of X
based on the language model, and p?PCFG(X) is the
length normalized probability of X based on the
probabilistic context free grammar (PCFG) model.
The loop is repeated until convergence or a fixed
number of iterations is reached. Note that this ap-
proach can be extended to simulated annealing to al-
low temporary downward steps to escape from local
maxima. We use the PCFG implementation of Klein
and Manning (2003).
3.3 Template Based Approach
The third approach is a template-based approach
with linguistic constraints, a technique that has of-
ten been used for various practical applications such
as summarization (Zhou and Hovy, 2004) and dia-
222
blue, bike  [2669]  blue, bicycle  [1365]  bike, blue  [1184]  blue, cycle  [324]  cycle, of, the, blue  [172]  cycle, blue  [158]  bicycle, blue  [154]  bike, in, blue  [98]  cycle, of, blue  [64]  bike, with, blue  [43]  
< < blue , bicycle >, near, < shiny , person > >  
bright, boy  [8092]  bright, child  [7840]  bright, girl  [6191]  bright, kid  [5873]  bright, person  [5461 ]  bright, man  [4936]  bright, woman  [2726]  bright, women  [1684]  lady, bright  [1360]  bright, men  [1050]  
person, operating, a, bicycle  [3409]  man, on, a, bicycle  [2842]  cycle, of, child  [2507]  bike, for, men  [2485]  person, riding, a, bicycle  [2118]  cycle, in, women  [1853]  bike, for, women  [1442]  boy, on, a, bicycle  [1378]  cycle, of, women  [1288]  man, on, a, bike  [1283]  
bright person operating a blue bicycle [2541 158 938 5] bright man on a blue bicycle [1914 83 72 88 0]  bright man on a blue bike [1690 24 78 07 2]  bright person riding a blue bicycle [157 881 332 70]  bright boy on a blue bicycle [1522 08 09 24 0]  blue bike for bright men [6964 08 82 50 ]  blue bike for bright women [648120 743 2]  blue cycle of bright child [6368 18 11 20 ]  blue cycle in bright women [1011 02 64 48 ]  
Figure 2: Illustration of phrase fusion composition al-
gorithm using web-scale n-grams. Numbers in square
brackets are n-gram frequencies.
logue systems (Channarukul et al, 2003). Because
the meaning representation produced by the image
recognition system has a fixed pattern of <<adj1,
obj1>, prep, <adj2, obj2>>, it can be templated as
?There is a [adj1] [obj1] [prep] the [adj2] [obj2].?
We also include templates that encode basic dis-
course constraints. For instance, the template that
generated the first sentences in Figure 3 and 4 is:
[PREFIX] [#(x1)] [x1], [#(x2)] [x2], ... and [#(xk)]
[xk], where xi is the name of an object (e.g. ?cow?),
#(xi) is the number of instances of xi (e.g. ?one?),
and PREFIX ? {?This picture shows?, ?This is a pic-
ture of?, etc}.
Although this approach can produce good looking
sentences in a limited domain, there are many limita-
tions. First, a template-based approach does not al-
low creative writing and produces somewhat stilted
prose. In particular, it cannot add interesting new
words, or replace existing content words with better
ones. In addition, such an approach does not allow
any reordering of words which might be necessary to
create a fluent sentence. Finally, hand-written rules
are domain-specific, and do not generalize well to
new domains.
4 Surface Realization by Phrase Fusion
using Web-scale N-gram
We now introduce an entirely different approach
that addresses the limitations of the conventional ap-
proaches discussed in ?3. This approach is based
on web-scale n-gram, also known as Google Web
1T data, which provides the frequency count of each
possible n-gram sequence for 1 ? n ? 5.
4.1 [Step I] ? Candidate Phrase Selection
We first define three different sets of phrases for each
given triple <<adj1, obj1>, prep, <adj2, obj2>>:
? O1 = {(x, f) | x is an n-gram phrase describ-
ing the first object using the words adj1 and
obj1, and f is the frequency of x}
? O2 = {(x, f) | x is an n-gram phrase describ-
ing the second object using the words adj2 and
obj2, and f is the frequency of x}
? R = {(x, f) | x is an n-gram describing the re-
lation between the two objects using the words
obj1 and obj2, and f is the frequency of x}
We find n-gram phrases for O1, O2, andR from the
Google Web 1T data. The search patterns for O1 is:
? [adj1] [?]n?2 [obj1]
? [obj1] [?]n?2 [adj1]
where [?] is a wildcard word, and [?]n?2 denotes
a sequence of n-2 number of wildcard words in a n-
gram sequence. For wildcards, we only allow a lim-
ited set of function words, and verbs in the gerund
form2 for reasons that will become clearer in the
next step ? phrase fusion in ?4.2.
Note that it is the second pattern that allows
interesting re-ordering of words in the final sen-
tence generation. For instance, suppose adj1=green,
obj1=person. Then it is more natural to generate
a phrase using the reverse pattern such as, ?person
in green? or ?person wearing green? than simply
concatenating adj1 and obj1 to generate ?green per-
son?. Similarly, given obj1=bicycle and obj2=man,
generating a phrase using the reverse pattern, e.g.,
?man with a bicycle? would be more natural than
?bicycle with a man?. Our hypothesis is that such
ordering preference is implicitly encoded in the
web-scale n-grams via frequencies.
It is worthwhile to note that our pattern matching
is case sensitive, and we only allow patterns that are
2We treat words with suffix ?ing? as gerund for simplicity.
223
Phrase Fusion Black bull eating fluffy grass Bright apartment in the blue sky Shaggy grass looking at the blue sky 
Local Search the black cow or even by the furry grass.  the shiny building up by the same blue sky. the furry grass be below over one  blue sky. 
Template   This picture shows one cow, one building, one grass and one sky. The black cow is by the shiny building, and by the furry grass, and by the blue sky. The shiny building is by the furry grass, and by the blue sky. The furry grass is below the blue sky.  
Simple decoding the black cow or by the furry grass. the shiny building up by the blue sky. the furry grass be below one  blue sky.  
Image Recognition Output as Tripes: <black;yellow;rusty,cow>,by;near;by,<furry;green;brown,grass>  <shiny;colorful;yellow,building>,by;near;by,<blue;clear;colorful,sky>  <furry;green;brown,grass>,below;beneath;by,<blue;clear;colorful,sky> 
Figure 3: Comparison of image descriptions
all lower-case. From our pilot study, we found that
n-grams with upper case characters are likely from
named entities, which distort the n-gram frequency
distribution that we rely on during the phrase fusion
phase. To further reduce noise, we also discard any
n-gram that contains a character that is not an alpha-
bet.
Accommodating Uncertainty We extend candi-
date phrase selection in order to cope with uncer-
tainty from the image recognition. In particular,
for each object detection obji, we include its top 3
predicted modifiers adji1, adji2, adji3 determined
by the attribute classifiers (see ?2) to expand the
set O1 and O2 accordingly. For instance, given
adji =(shiny or white) and obji = sheep, we can
consider both <shiny,sheep> and <white,sheep>
pairs to predict more compatible pairs of words.
Accommodating Synonyms Additionally, we
augment each modifier adji and each object name
obji with synonyms to further expand our sets
O1, O2, and R. These expanded sets of phrases
enable resulting generations that are more fluent
and creative.
4.2 [Step II] ? Phrase Fusion
Given the expanded sets of phrases O1, O2, and R
described above, we perform phrase fusion to gen-
erate simple image description. In this step, we find
the best combination of three phrases, (x?1, f?1) ?
O1, (x?2, f?2) ? O2, and (x?R, f?R) ? R as follows:
(x?1, x?2, x?R) = argmaxx1,x2,xRscore(x1, x2, xR) (1)
score(x1, x2, xR) = ?(x1)? ?(x2)? ?(xR) (2)
s.t. x?1 and x?R are compatible
& x?2 and x?R are compatible
Two phrases x?i and x?R are compatible if they share
the same object noun obji. We define the phrase-
level score function ?(?) as ?(xi) = fi using the
Google n-gram frequencies. The equation (2) can be
maximized using dynamic programming, by align-
ing the decision sequence as x?1 ? x?R ? x?2.
Once the best combination ? (x?1, x?2, x?R) is de-
termined, we fuse the phrases by replacing the word
obj1 in the phrase x?R with the corresponding phrase
x?1. Similarly, we replace the word obj2 in the phrase
x?R with the other corresponding phrase x?2. Because
the wildcard words ? [?] in ?4.1 allow only a lim-
ited set of function words and gerund, the resulting
phrase is highly likely to be grammatically correct.
Computational Efficiency One advantage of our
phrase fusion method is its efficiency. If we were
to attempt to re-order words with language mod-
els in a naive way, we would need to consider all
possible permutations of words ? an NP-hard prob-
lem (?3.1). However, our phrase fusion method is
clever in that it probes reordering only on selected
pairs of words, where reordering is likely to be use-
ful. In other words, our approach naturally ignores
most word pairs that do not require reordering and
has a time complexity of only O(K2n), where K is
the maximum number of candidate phrases of any
phrase type, and n is the number of phrase types in
each sentence. K can be kept as a small constant by
selecting K-best candidate phrases of each phrase
type. We set K = 10 in this paper.
5 Experimental Results
To construct the training corpus for language mod-
els, we crawled Wikipedia pages that describe our
object set. For evaluation, we use the UIUC PAS-
CAL sentence dataset3 which contains upto five
human-generated sentences that describing 1000 im-
ages. Note that all of the approaches presented in
3http://vision.cs.uiuc.edu/pascal-sentences/
224
Phrase fusion  shiny motorcycle nearby shiny motorcycle.   black women operating a shiny motorcycle.   bright boy on a shiny motorcycle.   girl showing pink on a shiny motorcycle.  
Local search the shiny motorbike or against the shiny motorbike. the shiny motorbike or by the black person. the shiny motorbike or by the shiny person. the shiny motorbike or by the pink person. 
Simple Decoding  the shiny motorbike or against the shiny motorbike. the shiny motorbike or by the black person. the shinny motorbike or by the shiny boy. the shiny motorbike or by the pink person. 
Template  This is a picture of two motorbikes, three persons, one building and one tree. The first shiny motorbike is against the second shiny motorbike, and by the first black person. The second shiny motorbike is by the first black person, and by the second shiny person, and by the third pink person. 
Image Recognition Output as Triples: < < shiny; black; rusty , motorbike >, against; by; in , < shiny; black; rusty , motorbike > > < < shiny; black; rusty , motorbike >, by; near; by , < black; shiny; rusty , person > > < < shiny; black; rusty , motorbike >, by; near; by , < pink; rusty; striped , person > > 
Figure 4: Comparison of image descriptions
Section 3 and 4 attempt to insert function words for
surface realization. In this work, we limit the choice
of function words to only those words that are likely
to be necessary in the final output.4 For instance, we
disallow function words such as ?who? or ?or?.
Before presenting evaluation results, we present
some samples of image descriptions generated by 4
different approaches in Figure 3 and 4. Notice that
only the PHRASE FUSION approach is able to in-
clude interesting and adequate verbs, such as ?eat-
ing? or ?looking? in Figure 3, and ?operating? in
Figure 4. Note that the choice of these action verbs
is based only on the co-occurrence statistics encoded
in n-grams, without relying on the vision compo-
nent that specializes in action recognition. These ex-
amples therefore demonstrate that world knowledge
implicitly encoded in natural language can help en-
hance image content recognition.
Automatic Evaluation: BLEU (Papineni et al,
2002) is a widely used metric for automatic eval-
uation of machine translation that measures the n-
gram precision of machine generated sentences with
respect to human generated sentences. Because our
task can be viewed as machine translation from im-
ages to text, BLEU (Papineni et al, 2002) may seem
4This limitation does not apply to TEMPLATE.
w/o w/ syn
LANGUAGE MODEL 0.094 0.106
TEMPLATE 0.087 0.096
LOCAL SEARCH 0.100 0.111
PHRASE FUSION (any best) 0.149 0.153
PHRASE FUSION (best w/ gerund) 0.146 0.149
Human 0.500 0.510
Table 2: Automatic Evaluation: BLEU measured at 1
Creativ. Fluency Relevan.
LANGUAGE MODEL 2.12 1.96 2.09
TEMPLATE 2.04 1.7 1.96
LOCAL SEARCH 2.21 1.96 2.04
PHRASE FUSION 1.86 1.97 2.11
Table 3: Human Evaluation: the scores range over 1 to 3,
where 1 is very good, 2 is ok, 3 is bad.
like a reasonable choice. However, there is larger
inherent variability in generating sentences from im-
ages than translating a sentence from one language
to another. In fact two people viewing the same pic-
ture may produce quite different descriptions. This
means BLEU could penalize many correctly gener-
ated sentences, and be poorly correlated with human
judgment of quality. Nevertheless we report BLEU
scores in absence of any other automatic evaluation
method that serves our needs perfectly.
The results are shown in Table 2 ? first column
shows BLEU score considering exact matches, sec-
ond column shows BLEU with full credit for syn-
onyms. To give a sense of upper bound and to see
some limitations of the BLEU score, we also com-
pute the BLEU score between human-generated sen-
tences by computing the BLEU score of the first hu-
man sentence with respect to the others.
There is one important factor to consider when in-
terpreting Table 2. The four approaches explored
in this paper are purposefully prolific writers in that
they generate many more sentences than the num-
ber of sentences in the image descriptions written by
humans (available in the UIUC PASCAL dataset).
In this work, we do not perform sentence selection
to reduce the number of sentences in the final out-
put. Rather, we focus on the quality of each gener-
ated sentence. The consequence of producing many
225
Way rusty the golden cow 
Golden cow in the golden sky 
Tree snowing black train 
Black train under the tree Rusty girl sitting at a white table White table in the clear sky 
Rusty girl living in the clear sky 
Blue path up in the clear sky 
Blue path to colored fishing boat 
Blue path up in the clear 
morning sky 
rusty chair for rusty dog.  
rusty dog under the rusty chair.  
rusty dog sitting in a rusty chair. 
Gray cat from a burning gray 
building 
Gray building with a gray cat. 
Gray building in the white sky 
 
Shaggy dog knotting hairy men 
Pink flowering plant the hairy dog 
Pink dog training shaggy dog 
Shaggy dog relaxing on a colored sofa 
 
black women hanging 
from a black tree.  
colored man in the tree. 
1 2 3 4 
5 
6 7 
8 
Figure 5: Sample image descriptions using PHRASE FUSION: some of the unexpected or poetic descriptions are
highlighted in boldface, and some of the interesting incorrect descriptions are underlined.
more sentences in our output is overall lower BLEU
scores, because BLEU precision penalizes spurious
repetitions of the same word, which necessarily oc-
curs when generating more sentences. This is not an
issue for comparing different approaches however,
as we generate the same number of sentences for
each method.
From Table 2, we find that our final approach ?
PHRASE FUSION based on web-scale n-grams per-
forms the best. Notice that there are two different
evaluations for PHRASE FUSION: the first one is
evaluated for the best combination of phrases (Equa-
tion (1)), while the second one is evaluated for the
best combination of phrases that contained at least
one gerund.
Human Evaluation: As mentioned earlier, BLEU
score has some drawbacks including obliviousness
to correctness of grammar and inability to evaluate
the creativity of a composition. To directly quantify
these aspects that could not be addressed by BLEU,
we perform human judgments on 120 instances for
the four proposed methods. Evaluators do not have
any computer vision or natural language generation
background.
We consider the following three aspects to eval-
uate the our image descriptions: creativity, fluency,
and relevance. For simplicity, human evaluators as-
sign one set of scores for each aspect per image. The
scores range from 1 to 3, where 1 is very good, 2 is
ok, and 3 is bad.5 The definition and guideline for
each aspect is:
[Creativity] How creative is the generated sen-
tence?
1 There is creativity either based on unexpected
words (in particular, verbs), or describing
things in a poetic way.
2 There is minor creativity based on re-ordering
words that appeared in the triple
3 None. Looks like a robot talking.
[Fluency] How grammatically correct is the gener-
ated sentence?
1 Mostly perfect English phrase or sentence.
2 There are some errors, but mostly comprehen-
sible.
3 Terrible.
[Relevance] How relevant is the generated descrip-
tion to the given image?
1 Very relevant.
2 Reasonably relevant.
3 Totally off.
5In our pilot study, human annotations on 160 instances
given by two evaluators were identical on 61% of the instances,
and close (difference ? 1) on 92%.
226
Table 3 shows the human evaluation results. In
terms of creativity, PHRASE FUSION achieves the
best score as expected. In terms of fluency and
relevance however, TEMPLATE achieves the best
scores, while PHRASE FUSION performs the second
best. Remember that TEMPLATE is based on hand-
engineered rules with discourse constraints, which
seems to appeal to evaluators more. It would be
straightforward to combine PHRASE FUSION with
TEMPLATE to improve the output of PHRASE FU-
SION with hand-engineered rules. However, our
goal in this paper is to investigate statistically moti-
vated approaches for generating image descriptions
that can address inherent limitations of hand-written
rules discussed in ?3.3.
Notice that the relevance score of TEMPLATE is
better than that of LANGUAGE MODEL, even though
both approaches generate descriptions that consist of
an almost identical set of words. This is presum-
ably because the output from LANGUAGE MODEL
contains grammatically incorrect sentences that are
not comprehendable enough to the evaluators. The
relevance score of PHRASE FUSION is also slightly
worse than that of TEMPLATE, presumably because
PHRASE FUSION often generates poetic or creative
expressions, as shown in Figure 5, which can be con-
sidered a deviation from the image content.
Error Analysis There are different sources of er-
rors. Some errors are due to mistakes in the origi-
nal visual recognition input. For example, in the 3rd
image in Figure 5, the color of sky is predicted to
be ?golden?. In the 4th image, the wall behind the
table is recognized as ?sky?, and in the 6th image,
the parrots are recognized as ?person?.
Other errors are from surface realization. For in-
stance, in the 8th image, PHRASE FUSION selects
the preposition ?under?, presumably because dogs
are typically under the chair rather than on the chair
according to Google n-gram statistics. In the 5th
image, an unexpected word ?burning? is selected to
make the resulting output idiosyncratic. Word sense
disambiguation sometimes causes a problem in sur-
face realization as well. In the 3rd image, the word
?way? is chosen to represent ?path? or ?street? by
the image recognizer. However, a different sense of
way ? ?very? ? is being used in the final output.
6 Related Work
There has been relatively limited work on automat-
ically generating natural language image descrip-
tions. Most work related to our study is discussed
in ?1, hence we highlight only those that are clos-
est to our work here. Yao et al (2010) present a
comprehensive system that generates image descrip-
tions using Head-driven phrase structure (HPSG)
grammar, which requires carefully written domain-
specific lexicalized grammar rules, and also de-
mands a very specific and complex meaning rep-
resentation scheme from the image processing. In
contrast, our approach handles images in the open-
domain more naturally using much simpler tech-
niques.
We use similar vision based inputs ? object detec-
tors, modifier classifiers, and prepositional functions
? to some very recent work on generating simple de-
scriptions for images (Kulkarni et al, 2011), but fo-
cus on improving the sentence generation method-
ology and produce descriptions that are more true
to human generated descriptions. Note that the
BLEU scores reported in their work of Kulkarni et
al. (2011) are not directly comparable to ours, as the
scale of the scores differs depending on the number
of sentences generated per image.
7 Conclusion
In this paper, we presented a novel surface realiza-
tion technique based on web-scale n-gram data to
automatically generate image description. Despite
its simplicity, our method is highly effective in gen-
erating mostly appealing and presentable language,
while permitting creative writing at times. We con-
clude from our study that it is viable to generate
simple textual descriptions that are germane to the
specific image content while also sometimes pro-
ducing almost poetic natural language. Furthermore,
we demonstrate that world knowledge implicitly en-
coded in natural language can help enhance image
content recognition.
Acknowledgments
This work is supported in part by NSF Faculty Early
Career Development (CAREER) Award #1054133.
227
References
A. Aker and R. Gaizauskas. 2010. Generating image
descriptions using dependency relational patterns. In
ACL.
K. Barnard, P. Duygulu, N. de Freitas, D. Forsyth,
D. Blei, and M. Jordan. 2003. Matching words and
pictures. JMLR, 3:1107?1135.
Songsak Channarukul, Susan W. McRoy, and Syed S.
Ali. 2003. Doghed: a template-based generator for
multimodal dialog systems targeting heterogeneous
devices. In NAACL.
Michael Chisholm and Prasad Tadepalli. 2002. Learning
decision rules by randomized iterative local search. In
ICML, pages 75?82.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. 2009. ImageNet: A Large-Scale Hierarchical Im-
age Database. In CVPR.
A. Farhadi, I. Endres, D. Hoiem, and D. A. Forsyth.
2009. Describing objects by their attributes. In CVPR.
A. Farhadi, M Hejrati, A. Sadeghi, P. Young,
C. Rashtchian, J. Hockenmaier, and D. A. Forsyth.
2010. Every picture tells a story: generating sentences
for images. In ECCV.
P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ra-
manan. 2010. Object detection with discriminatively
trained part based models. tPAMI, Sept.
Y. Feng and M. Lapata. 2010a. How many words is a
picture worth? automatic caption generation for news
images. In ACL.
Yansong Feng and Mirella Lapata. 2010b. Topic models
for image annotation and text illustration. In HLT.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, pages 423?430. Association for Computa-
tional Linguistics.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C Berg, and Tamara L Berg.
2011. Babytalk: Understanding and generating simple
image descriptions. In CVPR.
Chee Wee Leong, Rada Mihalcea, and Samer Hassan.
2010. Text mining for automatic image tagging. In
COLING.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation.
Katerina Pastra, Horacio Saggion, and Yorick Wilks.
2003. Nlp for indexing and retrieval of captioned pho-
tographs. In EACL.
Michael White and Claire Cardie. 2002. Selecting sen-
tences for multidocument summaries using random-
ized local search. In ACL Workshop on Automatic
Summarization.
B.Z. Yao, Xiong Yang, Liang Lin, Mun Wai Lee, and
Song-Chun Zhu. 2010. I2t: Image parsing to text de-
scription. Proc. IEEE, 98(8).
Liang Zhou and Eduard Hovy. 2004. Template-
filtered headline summarization. In Text Summariza-
tion Branches Out: Pr ACL-04 Wkshp, July.
228
