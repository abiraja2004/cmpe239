Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 398?404,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
LIMSI Submission for the WMT?13 Quality Estimation Task: an
Experiment with n-gram Posteriors
Anil Kumar Singh
LIMSI
Orsay, France
anil@limsi.fr
Guillaume Wisniewski
Universite? Paris Sud
LIMSI
Orsay, France
wisniews@limsi.fr
Franc?ois Yvon
Universite? Paris Sud
LIMSI
Orsay, France
yvon@limsi.fr
Abstract
This paper describes the machine learning
algorithm and the features used by LIMSI
for the Quality Estimation Shared Task.
Our submission mainly aims at evaluating
the usefulness for quality estimation of n-
gram posterior probabilities that quantify
the probability for a given n-gram to be
part of the system output.
1 Introduction
The dissemination of statistical machine transla-
tion (SMT) systems in the professional translation
industry is still limited by the lack of reliability of
SMT outputs, the quality of which varies to a great
extent. In this context, a critical piece of informa-
tion would be for MT systems to assess their out-
put translations with automatically derived quality
measures. This problem is the focus of a shared
task, the aim of which is to predict the quality
of a translation without knowing any human ref-
erence(s).
To the best of our knowledge, all approaches
so far have tackled quality estimation as a super-
vised learning problem (He et al, 2010; Soricut
and Echihabi, 2010; Specia et al, 2010; Specia,
2011). A wide variety of features have been pro-
posed, most of which can be described as loosely
?linguistic? features that describe the source sen-
tence, the target sentence and the association be-
tween them (Callison-Burch et al, 2012). Sur-
prisingly enough, information used by the decoder
to choose the best translation in the search space,
such as its internal scores, have hardly been con-
sidered and never proved to be useful. Indeed, it is
well-known that these scores are hard to interpret
and to compare across hypotheses. Furthermore,
mapping scores of a linear classifier (such as the
scores estimated by MERT) into consistent prob-
abilities is a difficult task (Platt, 2000; Lin et al,
2007).
This work aims at assessing whether informa-
tion extracted from the decoder search space can
help to predict the quality of a translation. Rather
than using directly the decoder score, we propose
to consider a finer level of information, the n-gram
posterior probabilities that quantifies the probabil-
ity for a given n-gram to be part of the system
output. These probabilities can be directly inter-
preted as the confidence the system has for a given
n-gram to be part of the translation. As they are
directly derived from the number of hypotheses in
the search space that contains this n-gram, these
probabilities might be more reliable than the ones
estimated from the decoder scores.
We first quickly review, in Section 2, the n-gram
posteriors introduced by (Gispert et al, 2013) and
explain how they can be used in the QE task; we
then describe, in Section 3 the different systems
that have developed for our participation in the
WMT?13 shared task on Quality Estimation and
assess their performance in Section 4.
2 n-gram Posterior Probabilities in SMT
Our contribution to the WMT?13 shared task on
quality estimation relies on n-gram posteriors. For
the sake of completeness, we will quickly formal-
ize this notion and summarize the method pro-
posed by (Gispert et al, 2013) to efficiently com-
pute them. We will then describe preliminary ex-
periments to assess their usefulness for predicting
the quality of a translation hypothesis.
2.1 Computing n-gram Posteriors
For a given source sentence F , the n-gram pos-
terior probabilities quantifies the probability for
a given n-gram to be part of the system output.
Their computation relies on all the hypotheses
considered by a SMT system during decoding: in-
tuitively, the more hypotheses a n-gram appears
in, the more confident the system is that this n-
gram is part of the ?correct? translation, and the
398
higher its posterior probability is. Formally, the
posterior of a given n-gram u is defined as:
P (u|E) =
?
(A,E)?E
?u(E) ? P (E,A|F )
where the sum runs over the translation hypothe-
ses contained in the search space E (generally rep-
resented as a lattice); ?u(E) has the value 1 if u
occurs in the translation hypothesis E and 0 oth-
erwise and P (E,A|F ) is the probability that the
source sentence F is translated by the hypothesis
E using a derivation A. Following (Gispert et al,
2013), this probability is estimated by applying a
soft-max function to the score of the decoder:
P (A,E|F ) = exp (??H(E,A, F ))?
(A?,E?)?E exp (H(E?, A?, F ))
where the decoder score H(E,A, F ) is typically
a linear combination of a handful of features, the
weights of which are estimated by MERT (Och,
2003).
n-gram posteriors therefore aggregate two
pieces of information: first, the number of paths in
the lattice (i.e. the number of translation hypothe-
ses of the search path) the n-gram appears in; sec-
ond, the decoder scores of these paths that can be
roughly interpreted as a quality of the path.
Computing P (u|E) requires to enumerate all n-
gram contained in E and to count the number of
paths in which this n-gram appears at least once.
An efficient method to perform this computation
in a single traversal of the lattice is described
in (Gispert et al, 2013). This algorithm has been
reimplemented1 to generate the posteriors used in
this work.
2.2 Analysis of n-gram Posteriors
Figure 1 represents the distribution of n-gram pos-
teriors on the training set of the task 1-1. This dis-
tribution is similar to the ones observed for task 1-
3 and for higher n-gram orders. It appears that, the
distribution is quite irregular and has two modes.
The minor modes corresponds to n-grams that ap-
pear in almost every translation hypotheses and
have posterior probability close to 1. Further anal-
yses show that these n-grams are mainly made of
stop words and of out-of-vocabulary words. The
major mode corresponds to very small n-gram
posteriors (less than 10?1) that the system has only
1Our implementation can be downloaded from http://
perso.limsi.fr/Individu/wisniews/.
a very small confidence in producing. The num-
ber of n-grams that have such a small posterior
suggests that most n-grams occur only in a small
number of paths.
0 5 10 15 20 250
.00
0.0
5
0.1
0
0.1
5
0.2
0
? logP (u|E)
De
nsi
ty
Figure 1: Distribution of the unigram posteriors
observed on the training set of the task 1-1
Using n-gram posteriors to predict the quality
of translation raises a representation issue: the
number of n-grams contained in a sentence varies
with the sentence length (and hence with the num-
ber of posteriors) but this information needs to be
represented in a fixed-length vector describing the
sentence. Similarly to what is usually done in the
quality estimation task, we chose to represent pos-
teriors probability by their histogram: for a given
n-gram order, each posterior is mapped to a bin;
each bin is then represented by a feature equal to
the number of n-gram posteriors it contains. To
account for the irregular distribution of posteriors,
bin breaks are chosen on the training set so as to
ensure that each bin contains the same number of
examples. In our experiments, we considered a
partition of the training data into 20 bins.
3 Systems Description
LIMSI has participated to the tasks 1-1 (predic-
tion of the hTER) and 1-3 (prediction of the post-
edition time). Similar features and learning algo-
rithms have been considered for the two tasks. We
will first quickly describe them before discussing
the specific development made for task 1-3.
399
3.1 Features
In addition to the features described in the previ-
ous section, 176 ?standard? features for quality es-
timation have been considered. The full list of fea-
tures we have considered is given in (Wisniewski
et al, 2013) and the features set can be down-
loaded from our website.2 These features can be
classified into four broad categories:
? Association Features: Measures of the qual-
ity of the ?association? between the source
and the target sentences like, for instance,
features derived from the IBM model 1
scores;
? Fluency Features: Measures of the ?fluency?
or the ?grammaticality? of the target sentence
such as features based on language model
scores;
? Surface Features: Surface features extracted
mainly from the source sentence such as
the number of words, the number of out-
of-vocabulary words or words that are not
aligned;
? Syntactic Features: some simple syntactic
features like the number of nouns, modifiers,
verbs, function words, WH-words, number
words, etc., in a sentence;
These features sets differ, in several ways, from
the baseline feature set provided by the shared task
organizers. First, in addition to features derived
from a language model, it also includes several
features based on large span continuous space lan-
guage models (Le et al, 2011). Such language
models have already proved their efficiency both
for the translation task (Le et al, 2012) and the
quality estimation task (Wisniewski et al, 2013).
Second, each feature was expanded into two ?nor-
malized forms? in which their value was divided
either by the source length or the target length
and, when relevant, into a ?ratio form? in which
the feature value computed on the target sentence
is divided by its value computed in the source sen-
tence. At the end, when all possible feature expan-
sions are considered, each example is described by
395 features.
2http://perso.limsi.fr/Individu/
wisniews/
3.2 Learning Methods
The main focus of this work is to study the rel-
evance of features for quality estimation; there-
fore, only very standard learning methods were
used in our work. For this year submission
both random forests (Breiman, 2001) and elas-
tic net regression (Zou and Hastie, 2005) have
been used. The capacity of random forests to take
into account complex interactions between fea-
tures has proved to be a key element in the re-
sults achieved in our experiments with last year
campaign datasets (Zhuang et al, 2012). As we
are considering a larger features set this year and
the number of examples is comparatively quite
small, we also considered elastic regression, a lin-
ear model trained with L1 and L2 priors as regu-
larizers, hoping that training a sparse model would
reduce the risk of overfitting.
In this study, we have used the implementation
provided by scikit-learn (Pedregosa et al,
2011). As detailed in Section 4.1, cross-validation
has been used to choose the hyper-parameters of
all regressors, namely the number of estimators,
the maximal depth of a tree and the minimum
number of examples in a leaf for the random
forests and the importance of the L1 and the L2
regularizers for the elastic net regressor.
3.3 System for Task 1-3
Like task 1-1, task 1-3 is a regression task that
aims at predicting the time needed to post-edit a
translation hypothesis. From a machine learning
point of view, this task differs from task 1-1 in
three aspects. First, the distributed training set
is much smaller: it is made of only 803 exam-
ples, which increases the risk of overfitting. Sec-
ond, contrary to hTER scores, post-edition time is
not normalized and the label of this task can take
any positive value. Finally and most importantly,
as shown in Figure 2, the label distributions es-
timated on the training set has a long tail which
indicates the presence of several outliers: in the
worse case, it took more than 18 minutes to cor-
rect a single sentence made of 35 words! Such
a long post-edition time most certainly indicates
that the corrector has been distracted when post-
editing the sentence rather than a true difficulty in
the post-edition.
These outliers have a large impact on training
and on testing, as their contributions to both MAE
400
0 200 400 600 800 1000 12000.
00
0
0.0
02
0.0
04
0.0
06
0.0
08
Post-edition time (s)
De
nsi
ty
Figure 2: Kernel density estimate of the post-
edition time distribution used as label in task 1-3.
and MSE,3 directly depends on label values and
can therefore be very large in the case of outliers.
For instance, a simple ridge regression with the
baseline features provided by the shared task or-
ganizer achieves a MAE of 42.641 ? 2.126 on
the test set. When all the examples having a la-
bel higher than 300 are removed from the training
set, the MAE drops to 41.843? 4.134. When out-
liers are removed from both the training and the
test sets, the MAE further drops to 32.803?1.673.
These observations indicate that special care must
be taken when collecting the data and that, maybe,
post-edition times should be clipped to provide a
more reliable estimation of the predictor perfor-
mance.
In the following (and in our submission) only
examples for which the post-edition time was less
than 300 seconds were considered.
4 Results
4.1 Experimental Setup
We have tested different combinations of features
and learning methods using a standard metric for
regression: Mean Absolute Error (MAE) defined
by:
MAE = 1n
n?
i=1
|y?i ? yi|
3The two standard loss functions used to train and evalu-
ate a regressor
where n is the number of examples, yi and y?i
the true label and predicted label of the ith exam-
ple. MAE can be understood as the averaged error
made in predicting the quality of a translation.
Performance of both task 1-1 and task 1-34 was
also evaluated by the Spearman rank correlation
coefficient ? that assesses how well the relation-
ship between two variables can be described using
a monotonic function. While the value of the cor-
relation coefficient is harder to interpret as it not
directly related to the value to predict, it can be
used to compare the performance achieved when
predicting different measures of the post-editing
effort. Indeed, several sentence-level (or docu-
ment level) annotation types can be used to reflect
translation quality (Specia, 2011), such as the time
needed to post-edit a translation hypothesis, the
hTER, or qualitative judgments as it was the case
for the shared task of WMT 2012. Comparing di-
rectly these different settings is complicated, since
each of them requires to optimize a different loss,
and even if the losses are the same, their actual
values will depend on the actual annotation to be
predicted (refer again to the discussion in (Specia,
2011, p5)). Using a metric that relies on the pre-
dicted rank of the example rather than the actual
value predicted allows us to directly compare the
performance achieved on the two tasks.
As the labels for the different tasks were not re-
leased before the evaluation, all the reported re-
sults are obtained on an ?internal? test set, made of
20% of the data released by the shared task or-
ganizers as ?training? data. The remaining data
were used to train the regressor in a 10 folds cross-
validation setting. In order to get reliable estimate
of our methods performances, we used bootstrap
resampling (Efron and Tibshirani, 1993) to com-
pute confidence intervals of the different scores:
10 random splits of the data into a training and
sets were generated; a regressor was then trained
and tested for each of these splits and the resulting
confidence intervals at 95% computed.
4.2 Results
Table 1and Table 2 contain the results achieved by
our different conditions. We used, as a baseline,
the set of 17 features released by the shared task
organizers.
It appears that the differences in MAE between
4The Spearman ? was an official metric only for task 1-
1. For reasons explained in this paragraph, we also used it to
evaluate our results for task 1-3.
401
the different configurations are always very small
and hardly significant. However, the variation of
the Spearman ? are much larger and the difference
observed are practically significant when the inter-
pretation scale of (Landis and Koch, 1977) is used.
We will therefore mainly consider ? in our discus-
sion.
For the two tasks 1-1 and 1-3, the features we
have designed allow us to significantly improve
prediction performance in comparison to the base-
line. For instance, for task 1-1, the correlation
is almost doubled when the features described in
Section 3.1 are used. As expected, random forests
are overfitting and did not manage to outperform
a simple linear classifier. That is why we only
used the elastic net method for our official submis-
sion. Including posterior probabilities in the fea-
ture set did not improve performance much (ex-
cept when only the baseline features are consid-
ered) and sometimes even hurt performance. This
might be caused by an overfitting problem, the
training set becoming too small when new features
are added. We are conducting further experiments
to explain this paradoxical observation.
Another interesting observation that can be
made looking at the results of Table 1 and Ta-
ble 2 is that the prediction of the post-edition time
seems to be easier than the prediction of the hTER:
using the same classifiers and the same features,
the performance for the former task is always far
better than the performance for the latter.
5 Conclusion
In this paper, we described our submission to the
WMT?13 shared task on quality estimation. We
have explored the use of posteriors probability,
hoping that information about the search space
could help in predicting the quality of a transla-
tion. Even if features derived from posterior prob-
abilities have shown to have only a very limited
impact, we managed to significantly improve the
baseline with a standard learning method and sim-
ple features. Further experiments are required to
understand the reasons of this failure.
Our results also highlight the need to continue
gathering high-quality resources to train and in-
vestigate quality estimation systems: even when
considering few features, our systems were prone
to overfitting. Developing more elaborated sys-
tems will therefore only be possible if more train-
ing resource is available. Our experiments also
stress that both the choice of the quality measure
(i.e. the quantity to predict) and of the evaluation
metrics for quality estimation are still open prob-
lems.
6 Acknowledgments
This work was partly supported by ANR
projects Trace (ANR-09-CORD-023) and Tran-
sread (ANR-12-CORD-0015).
References
Leo Breiman. 2001. Random forests. Mach. Learn.,
45(1):5?32, October.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
B. Efron and R. Tibshirani. 1993. An Introduction
to the Bootstrap. Chapman and Hall/CRC Mono-
graphs on Statistics and Applied Probability Series.
Chapman & Hall.
Adria` Gispert, Graeme Blackwood, Gonzalo Iglesias,
and William Byrne. 2013. N-gram posterior prob-
ability confidence measures for statistical machine
translation: an empirical study. Machine Transla-
tion, 27(2):85?114.
Yifan He, Yanjun Ma, Josef van Genabith, and Andy
Way. 2010. Bridging smt and tm with translation
recommendation. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 622?630, Uppsala, Sweden, July.
Association for Computational Linguistics.
R. J. Landis and G. G. Koch. 1977. The measurement
of observer agreement for categorical data. Biomet-
rics, 33(1):159?174.
Hai Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
Output Layer Neural Network Language Model.
In Proceedings of IEEE International Conference
on Acoustic, Speech and Signal Processing, pages
5524?5527, Prague, Czech Republic.
Hai-Son Le, Thomas Lavergne, Alexandre Al-
lauzen, Marianna Apidianaki, Li Gong, Aure?lien
Max, Artem Sokolov, Guillaume Wisniewski, and
Franc?ois Yvon. 2012. Limsi @ wmt12. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 330?337, Montre?al,
Canada, June. Association for Computational Lin-
guistics.
402
MAE ?
train test train test
Baseline Features
RandomForest 0.109? 0.013 0.130? 0.004 0.405? 0.008 0.314? 0.016
Elastic 0.127? 0.001 0.129? 0.003 0.336? 0.004 0.319? 0.015
?Linguistic? Features
RandomForest 0.082? 0.019 0.118? 0.003 0.689? 0.003 0.625? 0.009
Elastic 0.107? 0.004 0.115? 0.003 0.705? 0.009 0.660? 0.009
?Linguistic? Features + posteriors
RandomForest 0.088? 0.017 0.116? 0.003 0.694? 0.003 0.615? 0.014
Elastic 0.105? 0.006 0.114? 0.002 0.699? 0.007 0.662? 0.011
Table 1: Results for the task 1-1
MAE ?
train test train test
Baseline Features
RandomForest 25.145? 3.745 33.279? 1.687 0.669? 0.007 0.639? 0.017
Elastic 32.776? 0.795 33.702? 2.328 0.678? 0.006 0.657? 0.018
Baseline Features + Posteriors
RandomForest 33.707? 0.309 35.646? 0.889 0.674? 0.004 0.637? 0.017
Elastic 31.487? 0.261 32.922? 0.789 0.698? 0.004 0.681? 0.016
?Linguistic? Features
RandomForest 25.236? 4.400 33.017? 1.582 0.735? 0.007 0.666? 0.023
Elastic 28.706? 1.273 31.630? 1.612 0.760? 0.006 0.701? 0.017
?Linguistic? Features + Posteriors
RandomForest 22.951? 3.903 33.013? 1.514 0.741? 0.003 0.695? 0.013
Elastic 28.911? 1.020 31.865? 1.636 0.761? 0.008 0.710? 0.017
Table 2: Results for the task 1-3
403
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng.
2007. A note on platt?s probabilistic outputs for
support vector machines. Mach. Learn., 68(3):267?
276, October.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine Learn-
ing in Python . Journal of Machine Learning Re-
search, 12:2825?2830.
John C. Platt, 2000. Probabilities for SV Machines,
pages 61?74. MIT Press.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine translation evaluation versus quality esti-
mation. Machine Translation, 24(1):39?50, March.
Lucia Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In Pro-
ceedings of the 15th conference of EAMT, pages 73?
80, Leuven, Belgium.
Guillaume Wisniewski, Anil Kumar Singh, and
Franc?ois Yvon. 2013. Quality estimation for ma-
chine translation: Some lessons learned. Machine
Translation. accepted for publication.
Yong Zhuang, Guillaume Wisniewski, and Franc?ois
Yvon. 2012. Non-linear models for confidence es-
timation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 157?162,
Montre?al, Canada, June. Association for Computa-
tional Linguistics.
Hui Zou and Trevor Hastie. 2005. Regularization and
variable selection via the elastic net. Journal of the
Royal Statistical Society, Series B, 67:301?320.
404
