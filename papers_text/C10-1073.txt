Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 644?652,
Beijing, August 2010
Improving Corpus Comparability for Bilingual Lexicon Extraction from
Comparable Corpora
Bo Li, Eric Gaussier
Laboratoire d?Informatique de Grenoble (LIG)
Universite? de Grenoble
firstname.lastname@imag.fr
Abstract
Previous work on bilingual lexicon extrac-
tion from comparable corpora aimed at
finding a good representation for the usage
patterns of source and target words and at
comparing these patterns efficiently. In
this paper, we try to work it out in an-
other way: improving the quality of the
comparable corpus from which the bilin-
gual lexicon has to be extracted. To do
so, we propose a measure of comparabil-
ity and a strategy to improve the qual-
ity of a given corpus through an iterative
construction process. Our approach, be-
ing general, can be used with any existing
bilingual lexicon extraction method. We
show here that it leads to a significant im-
provement over standard bilingual lexicon
extraction methods.
1 Introduction
Bilingual dictionaries are an essential resource
in many multilingual natural language process-
ing (NLP) tasks such as machine translation (Och
and Ney, 2003) and cross-language information
retrieval (CLIR) (Ballesteros and Croft, 1997).
Hand-coded dictionaries are of high quality, but
expensive to build and researchers have tried,
since the end of the 1980s, to automatically
extract bilingual lexicons from parallel corpora
(see (Chen, 1993; Kay and Ro?scheisen, 1993;
Melamed, 1997a; Melamed, 1997b) for early
work). Parallel corpora are however difficult to
get at in several domains, and the majority of
bilingual collections are comparable and not par-
allel. Due to their low cost of acquisition, sev-
eral researchers have tried to exploit such com-
parable corpora for bilingual lexicon extraction
(Fung and McKeown, 1997; Fung and Yee, 1998;
Rapp, 1999; De?jean et al, 2002; Gaussier et al,
2004; Robitaille et al, 2006; Morin et al, 2007;
Yu and Tsujii, 2009). The notion of comparability
is however a loose one, and comparable corpora
range from lowly comparable ones to highly com-
parable ones and parallel ones. For data-driven
NLP techniques, using better corpora often leads
to better results, a fact which should be true for
the task of bilingual lexicon extraction. This point
has largely been ignored in previous work on the
subject. In this paper, we develop a well-founded
strategy to improve the quality of a comparable
corpus, so as to improve in turn the quality of the
bilingual lexicon extracted. To do so, we first pro-
pose a measure of comparability which we then
use in a method to improve the quality of the ex-
isting corpus.
The remainder of the paper is organized as fol-
lows: Section 2 introduces the experimental mate-
rials used for the different evaluations; compara-
bility measures are then presented and evaluated
in Section 3; in Section 4, we detail and evaluate
a strategy to improve the quality of a given corpus
while preserving its vocabulary; the method used
for bilingual lexicon extraction is then described
and evaluated in Section 5. Section 6 is then de-
voted to a discussion, prior to the conclusion given
in Section 7.
2 Experimental Materials
For the experiments reported here, several cor-
pora were used: the parallel English-French
Europarl corpus (Koehn, 2005), the TREC
644
(http://trec.nist.gov/) Associated Press corpus
(AP, English) and the corpora used in the
multilingual track of CLEF (http://www.clef-
campaign.org) which includes the Los Angeles
Times (LAT94, English), Glasgow Herald (GH95,
English), Le Monde (MON94, French), SDA
French 94 (SDA94, French) and SDA French 95
(SDA95, French). In addition to these exist-
ing corpora, two monolingual corpora from the
Wikipedia dump1 were built. For English, all
the articles below the root category Society with
a depth less than 42 were retained. For French,
all the articles with a depth less than 7 below the
category Socie?te? are extracted. As a result, the
English corpus Wiki-En consists of 367,918 doc-
uments and the French one Wiki-Fr consists of
378,297 documents.
The bilingual dictionary used in our experi-
ments is constructed from an online dictionary.
It consists of 33,372 distinct English words and
27,733 distinct French words, which constitutes
75,845 translation pairs. Standard preprocessing
steps: tokenization, POS-tagging and lemmatiza-
tion are performed on all the linguistic resources.
We will directly work on lemmatized forms of
content words (nouns, verbs, adjectives, adverbs).
3 Measuring Comparability
As far as we can tell, there are no practical mea-
sures with which we can judge the degree of com-
parability of a bilingual corpus. In this paper, we
propose a comparability measure based on the ex-
pectation of finding the translation for each word
in the corpus. The measure is light-weighted and
does not depend on complex resources like the
machine translation system. For convenience, the
following discussions will be made in the context
of the English-French comparable corpus.
3.1 The Comparability Measure
For the comparable corpus C, if we consider the
translation process from the English part Ce to the
1The Wikipedia dump files can be downloaded at
http://download.wikimedia.org. In this paper, we use the En-
glish dump file on July 13, 2009 and the French dump file on
July 7, 2009.
2There are several cycles in the category tree of
Wikipedia. It is thus necessary to define a threshold on the
depth to make the iterative process feasible.
French part Cf , a comparability measure Mef can
be defined on the basis of the expectation of find-
ing, for each English word we in the vocabulary
Cve of Ce, its translation in the vocabulary Cvf of Cf .
Let ? be a function indicating whether a transla-
tion from the translation set Tw of w is found in
the vocabulary Cv of a corpus C, i.e.:
?(w, Cv) =
{
1 iff Tw ? Cv 6= ?
0 else
Mef is then defined as:
Mef (Ce, Cf ) = E(?(w, Cvf )|w ? Cve )
=
?
w?Cve
?(w, Cvf ) ? Pr(w ? Cve )? ?? ?
Aw
= |C
v
e |
|Cve ? Dve |
?
w?Cve?Dve
Aw
where Dve is the English part of a given, inde-
pendent bilingual dictionaryD, and where the last
equality is based on the fact that, the compara-
ble corpus and the bilingual dictionary being in-
dependent of one another, the probability of find-
ing the translation in Cvf of a word w is the same
for w is in Cve ? Dve and in Cve\Dve 3. Furthermore,
the presence of common words suggests that one
should rely on a presence/absence criterion rather
than on the number of occurrences to avoid a bias
towards common words. Given the natural lan-
guage text, our evaluation will show that the sim-
ple presence/absence criterion can perform very
well. This leads to Pr(w ? Cve ) = 1/|Cve |, and
finally to:
Mef (Ce, Cf ) =
1
|Cve ? Dve |
?
w?Cve?Dve
?(w, Cvf )
This formula shows that Mef is actually the pro-
portion of English words translated in the French
part of the comparable corpus. Similarly, the
counterpart of Mef , Mfe, is defined as:
Mfe(Ce, Cf ) =
1
|Cvf ? Dvf |
?
w?Cvf?Dvf
?(w, Cve )
3The fact can be reliable only when a substantial part of
the corpus vocabulary is covered by the dictionary. Fortu-
nately, the constraint is satisfied in most applications where
the common but not the specialized corpora like the medical
corpora are involved.
645
and measures the proportion of French words in
Cvf translated in the English part of the compara-
ble corpus. A symmetric version of these mea-
sures is obtained by considering the proportion of
the words (both English and French) for which a
translation can be found in the corpus:
M(Ce, Cf )
=
?
w?Cve?Dve ?(w, C
v
f ) +
?
w?Cvf?Dvf ?(w, C
v
e )
|Cve ? Dve |+ |Cvf ? Dvf |
We now present an evaluation of these measures
on artificial test corpora.
3.2 Validation
In order to test the comparability measures, we de-
veloped gold-standard comparability scores from
the Europarl and AP corpora. We start from the
parallel corpus, Europarl, of which we degrade
the comparability by gradually importing some
documents from either Europarl or AP. Three
groups (Ga, Gb, Gc) of comparable corpora are
built in this fashion. Each group consists of test
corpora with a gold-standard comparability rang-
ing, arbitrarily, from 0 to 1 and corresponding to
the proportion of documents in ?parallel? transla-
tion. The first group Ga is built from Europarl
only. First, the Europarl corpus is split into 10
equal parts, leading to 10 parallel corpora (P1, P2,
. . . , P10) with a gold-standard comparability arbi-
trarily set to 1. Then for each parallel corpus, e.g.
Pi, we replace a certain proportion p of the En-
glish part with documents of the same size from
another parallel corpus Pj(j 6= i), producing the
new corpus P ?i with less comparability which is
the gold-standard comparability 1 ? p. For each
Pi, as p increases, we obtain several comparable
corpora with a decreasing gold-standard compara-
bility score. All the Pi and their descendant cor-
pora constitute the group Ga. The only difference
betweenGb andGa is that, inGb, the replacement
in Pi is done with documents from the AP cor-
pus and not from Europarl. In Gc, we start with
10 final, comparable corpora P ?i from Ga. These
corpora have a gold-standard comparability of 0
in Ga, and of 1 in Gc. Then each P ?i is further
degraded by replacing certain portions with docu-
ments from the AP corpus.
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
0.83
0.84
0.85
0.86
0.87
0.88
M
Gold-standard comparability scores
Scores from the comparability
 metric
Figure 1: Evolution of M wrt gold-standard
on the corpus group Gc (x-axis: gold-standard
scores, y-axis: M scores)
We then computed, for each comparable cor-
pus in each group, its comparability according
to one of the comparability measures. Figure 1
plots the measure M for ten comparable corpora
and their descendants from Gc, according to their
gold-standard comparability scores. As one can
note, the measure M is able to capture almost all
the differences in comparability and is strongly
correlated with the gold-standard. The correla-
tion between the different measures and the gold-
standard is finally computed with Pearson corre-
lation coefficient. The results obtained are listed
in Table 1. As one can note, Mfe performs worst
among the three measures, the reason being that
the process to construct Gb and Gc yields unbal-
anced bilingual corpora, the English section being
larger than the French one. Translations of French
words are still likely to be found in the English
corpus, even though the corpora are not compara-
ble. On all the 3 groups,M performs best and cor-
relates very well with the gold standard, meaning
that M was able to capture all the differences in
comparability artificially introduced in the degra-
dation process we have considered. This is the
measure we will retain in the following parts.
Mef Mfe M
Ga 0.897 0.770 0.936
Gb 0.955 0.190 0.979
Gc 0.940 -0.595 0.960
Table 1: Correlation scores for the different com-
parability measures on the 3 groups of corpora
646
Having established a measure for the degree of
comparability of bilingual corpora, we now turn
to the problem of improving the quality of com-
parable corpora.
4 Improving Corpus Quality
We here try to improve the quality of a given cor-
pus C, which we will refer to as the base corpus,
by extracting the highly comparable subpart CH
which is above a certain degree of comparability
? (Step 1), and by enriching the lowly comparable
part CL with texts from other sources (Step 2). As
we are interested in extracting information related
to the vocabulary of the base corpus, we want the
newly built corpus to contain a substantial part of
the base corpus. This can be achieved by preserv-
ing in Step 1 as many documents from the base
corpus as possible (e.g. by considering low values
of ?), and by using in step 2 sources close to the
base corpus.
4.1 Step 1: Extracting CH
The strategy consisting of building all the possible
sub-corpora of a given size from a given compa-
rable corpora is not realistic as soon as the num-
ber of documents making up the corpora is larger
than a few thousands. In such cases, better ways
for extracting subparts have to be designed. The
strategy we have adopted here aims at efficiently
extracting a subpart of C above a certain degree of
comparability and is based on the following prop-
erty.
Property 1. Let d1e and d2e (resp. d1f and d2f )
be two English (resp. French) documents from a
bilingual corpus C. We consider, as before, that
the bilingual dictionary D is independent from C.
Let (d1e ?, d1f ?) be such that: d1e ? ? d1e, d1f ? ? d1f ,
which means d1e ? is a subpart of d1e and d1f ? is a
subpart of d1f .
We assume:
(i) |d1e?d2e||d2e| =
|d1f?d2f |
|d2f |
(ii) Mef (d1e ?, d1f ) ?Mef (d2e, d2f )
Mfe(d1e, d1f
?) ?Mfe(d2e, d2f )
Then:
M(d2e, d2f ) ?M(d1e ? d2e, d1f ? d2f )
Proof [sketch]: Let B = (d1e ? d2e) ? Dve )\(d2e ?
Dve ). One can show, by exploiting condition (ii),
that:
?
w?B
?(w, d1f ? d2f ) ? |B|Mef (d2e, d2f )
and similarly that:
?
w?d2e?Dve
?(w, d1f ? d2f ) ? |d2e ? Dve |Mef (d2e, d2f )
Then exploiting condition (i), and the indepen-
dence between the corpus and the dictionary, one
arrives at:
?
w?(d1e?d2e)?Dve ?(w, d
1
f ? d2f )
|(d1e ? d2e) ? Dve |+ |(d1f ? d2f ) ? Dvf |
?
|d2e ? Dve |Mef (d2e, d2f )
|d2e ? Dve |+ |d2f ? Dvf |
The same development on Mfe completes the
proof. 2
Property 1 shows that one can incrementally ex-
tract from a bilingual corpus a subpart with a guar-
anteed minimum degree of comparability ? by it-
eratively adding new elements, provided (a) that
the new elements have a degree of comparability
of at least ? and (b) that they are less comparable
than the currently extracted subpart (conditions
(ii)). This strategy is described in Algorithm 1.
Since the degree of comparability is always above
a certain threshold and since the new documents
selected (d2e, d2f ) are the most comparable among
the remaining documents, condition (i) is likely
to be satisfied, as this condition states that the in-
crease in the vocabulary from the second docu-
ments to the union of the two is the same in both
languages. Similarly, considering new elements
by decreasing comparability scores is a necessary
step for the satisfaction of condition (ii), which
states that the current subpart should be uniformly
more comparable than the element to be added.
Hence, the conditions for property 1 to hold are
met in Algorithm 1, which finally yields a corpus
with a degree of comparability of at least ?.
4.2 Step 2: Enriching CL
This step tries to absorb knowledge from other
resources, which will be called external corpus,
647
Algorithm 1
Input:
English document set Cde of C
French document set Cdf of C
Threshold ?
Output:
CH , consisting of the English document set Se
and the French document set Sf
1: Initialize Se = ?,Sf = ?, temp = 0;
2: repeat
3: (de, df ) = argmax
de?Cde ,df?Cdf
M(de, df );
4: temp = max
de?Cde ,df?Cdf
M(de, df );
5: if temp ? ? then
6: Add de into Se and add df into Sf ;
7: Cde = Cde\de, Cdf = Cdf\df ;
8: end if
9: until Cde = ? or Cdf = ? or temp < ?
10: return CH ;
to enrich the lowly comparable part CL which is
the left part in C during the creation of CH . One
choice for obtaining the external corpus CT is to
fetch documents which are likely to be compara-
ble from the Internet. In this case, we first ex-
tract representative words for each document in
CL, translate them using the bilingual dictionary
and retrieve associated documents via a search en-
gine. An alternative approach is of course to use
existing bilingual corpora. Once CT has been con-
structed, the lowly comparable part CL can be en-
riched in exactly the same way as in section 4.1:
First, Algorithm 1 is used on the English part of
CL and the French part of CT to get the high-
quality document pairs. Then the French part of
CL is enriched with the English part of CT by the
same algorithm. All the high-quality document
pairs are then added to CH to constitute the final
result.
4.3 Validation
We use here GH95 and SDA95 as the base cor-
pus C0. In order to illustrate that the efficiency
of the proposed algorithm is not confined to a
specific external resource, we consider two ex-
ternal resources: (a) C1T made of LAT94, MON94
and SDA94, and (b) C2T consisting of Wiki-En and
Wiki-Fr. The number of documents in all the cor-
pora after elimination of short documents (< 30
words) is listed in Table 2.
C0 C1T C2T
English 55,989 109,476 367,918
French 42,463 87,086 378,297
Table 2: The size of the corpora in the experiments
For the extraction of the highly comparable part
CH from the base corpus C0, we set ? to 0.3
so as to extract a substantial subpart of C0. Af-
ter this step, corresponding to Algorithm 1, we
have 20,124 English-French document pairs in
CH . The second step is to enrich the lowly compa-
rable part CL of the base corpus documents from
the external resources C1T and C2T . The final cor-
pora we obtain consist of 46,996 document pairs
for C1 (with C1T ) and of 54,402 document pairs for
C2 (with C2T ), size similar to the one of C0. The
proportion of documents (columns ?D-e? and ?D-
f?), sentences (columns ?S-e? and ?S-f?) and vo-
cabulary (columns ?V-e? and ?V-f?) of C0 found
in C1 and C2 is given in Table 3. As one can note,
the final corpora obtained through the method pre-
sented above preserve most of the information
from the base corpus. Especially for the vocab-
ulary, the final corpora cover nearly all the vocab-
ulary of the base corpus. Considering the compa-
rability scores, the comparability of C1 is 0.912
and the one of C2 is 0.916. Both of them are
more comparable than the base corpus of which
the comparability is 0.882.
From these results of the intrinsic evaluation,
one can conclude that the strategy developed to
improve the corpus quality while preserving most
of its information is efficient: The corpora ob-
tained here, C1 and C2, are more comparable than
the base corpus C0 and preserve most of its infor-
mation. We now turn to the problem of extracting
bilingual lexicons from these corpora.
5 Bilingual Lexicon Extraction
Following standard practice in bilingual lexicon
extraction from comparable corpora, we rely on
the approach proposed by Fung and Yee (1998).
In this approach, each word w is represented as a
648
D-e D-f S-e S-f V-e V-f
C1 0.669 0.698 0.821 0.805 0.937 0.981
C2 0.785 0.719 0.893 0.807 0.968 0.987
Table 3: Proportion of documents, sentences and
vocabulary of C0 covered by the result corpora
context vector consisting of the weight a(wc) of
each context word wc, the context being extracted
from a window running through the corpus. Once
context vectors for English and French words have
been constructed, a general bilingual dictionaryD
can be used to bridge them by accumulating the
contributions from words that are translation of
each other. Standard similarity measures, as the
cosine or the Jaccard coefficient, can then be ap-
plied to compute the similarity between vectors.
For example, the cosine leads to:
sc(we, wf ) =
?
(wce,wcf )?D a(w
c
e)a(wcf )
???we? ? ???wf?
(1)
5.1 Using Algorithm 1 pseudo-Alignments
The process we have defined in the previous sec-
tion to improve the quality of a given corpus while
preserving its vocabulary makes use of highly
comparable document pairs, and thus provides
some loose alignments between the two corpora.
One can thus try to leverage the above approach
to bilingual lexicon extraction by re-weighting
sc(we, wf ) by a quantity which is large if we and
wf appear in many document pairs with a high
comparability score, and small otherwise. In this
section, we can not use the alignments in algo-
rithm 1 directly because the alignments in the
comparable corpus should not be 1 to 1 and we
did not try to find the precise 1 to 1 alignments in
algorithm 1.
Let ? be the threshold used in algorithm 1 to
construct the improved corpus and let ?(de, df )
be defined as:
?(de, df ) =
{
1 iff M(de, df ) ? ?
0 else
Let He (resp. Hf ) be the set of documents con-
taining word we (resp. wf ). We define the joint
probability of we and wf as being proportional
to the number of comparable document pairs they
belong to, where two documents are comparable
if their comparability score is above ?, that is:
p(we, wf ) ?
?
de?He,df?Hf
?(de, df )
The marginal probability p(we) can then be writ-
ten as:
p(we)?
?
wf?Cvf
p(we, wf )
?
?
de?He
?
df?Cdf
|df | ? ?(de, df )
Assuming that all df in Cdf have roughly the
same vocabulary size and all de have the same
number of comparable counterparts in Cdf , then
the marginal probability can be simplified as:
p(we) ? |He|. By resorting to the exponential
of the point-wise mutual information, one finally
obtains the following weight:
pi(we, wf ) =
p(we, wf )
p(we) ? p(wf )
? 1|He| ? |Hf |
?
de?He,df?Hf
?(de, df )
which has the desired property: It is large if the
two words appear in comparable document pairs
more often than chance would predict, and small
otherwise. We thus obtain the revised similarity
score for we and wf :
scr(we, wf ) = sc(we, wf ) ? pi(we, wf ) (2)
5.2 Validation
In order to measure the performance of the bilin-
gual lexicon extraction method presented above,
we divided the original dictionary into 2 parts:
10% of the English words (3,338 words) together
with their translations are randomly chosen and
used as the evaluation set, the remaining words
(30,034 words) being used to compute context
vectors and similarity between them. In this
study, the weight a(wc) used in the context vec-
tors (see above) are taken to be the tf-idf score
of wc: a(wc) = tf-idf(wc). English words not
649
present in Cve or with no translation in Cvf are ex-
cluded from the evaluation set. For each English
word in the evaluation set, all the French words
in Cvf are then ranked according to their similar-
ity with the English word (using either equation 1
or 2). To evaluate the quality of the lexicons ex-
tracted, we first retain for each English word its
N first translations, and then measure the preci-
sion of the lists obtained, which amounts in this
case to the proportion of lists containing the cor-
rect translation (in case of multiple translations, a
list is deemed to contain the correct translation as
soon as one of the possible translations is present).
This evaluation procedure has been used in pre-
vious work (e.g. (Gaussier et al, 2004)) and is
now standard for the evaluation of lexicons ex-
tracted from comparable corpora. In this study,
N is set to 20. Furthermore, several studies have
shown that it is easier to find the correct transla-
tions for frequent words than for infrequent ones
(Pekar et al, 2006). To take this fact into account,
we distinguished different frequency ranges to as-
sess the validity of our approach for all frequency
ranges. Words with frequency less than 100 are
defined as low-frequency words (WL), whereas
words with frequency larger than 400 are high-
frequency words (WH ), and words with frequency
in between are medium-frequency words (WM ).
We then tested the standard method based on
the cosine similarity (equation 1) on the corpora
C0, CH , C?H , C1 and C2. The results obtained are
displayed in Table 4, and correspond to columns
2-6. They show that the standard approach per-
forms significantly better on the improved corpora
C1/C2 than on the base corpus C0. The overall pre-
cision is increased by 5.3% on C1 (corresponding
to a relative increase of 26%) and 9.5% on C2 (cor-
responding to a relative increase of 51%), even
though the low-frequency words, which dominate
the overall precision, account for a higher pro-
portion in C1 (61.3%) and C2 (61.3%) than in
C0 (56.2%). For the medium and high frequency
words, the precision is increased by over 11% on
C1 and 16% on C2. As pointed out in other stud-
ies, the performance for the low-frequency words
is usually bad due to the lack of context informa-
tion. This explains the relatively small improve-
ment obtained here (only 2.2% on C1 and 6.7%
on C2). It should also be noticed that the perfor-
mance of the standard approach is better on C2
than on C1, which may be due to the fact that C2
is slightly larger than C1 and thus provides more
information or to the actual content of these cor-
pora. Lastly, if we consider the results on the cor-
pus CH which is produced by only choosing the
highly comparable part from C0, the overall preci-
sion is increased by only 1.9%, which might come
from the fact that the size of CH is less than half
the size of C0. We also notice the better results on
CH than on C?H of the same size which consists of
randomly choosing documents from C0.
The results obtained with the refined approach
making use of the comparable document pairs
found in the improved corpus (equation 2) are
also displayed in Table 4 (columns ?C1 new? and
?C2 new?). From these results, one can see that
the overall precision is further improved by 2.0%
on C1 and 2.3% on C2, compared with the stan-
dard approach. For all the low, medium and
high-frequency words, the precision has been im-
proved, which demonstrates that the information
obtained through the corpus enrichment process
contributes to improve the quality of the extracted
bilingual lexicons. Compared with the original
base corpus C0, the overall improvement of the
precision on both C1 and C2 with the refined ap-
proach is significant and important (respectively
corresponding to a relative improvement of 35%
and 62%), which also demonstrates that the effi-
ciency of the refined approach is not confined to a
specific external corpus.
6 Discussion
It is in a way useless to deploy bilingual lexicon
extraction techniques if translation equivalents are
not present in the corpus. This simple fact is at the
basis of our approach which consists in construct-
ing comparable corpora close to the original cor-
pus and which are more likely to contain transla-
tion equivalents as they have a guaranteed degree
of comparability. The pseudo-alignments identi-
fied in the construction process are then used to
leverage state-of-the-art bilingual lexicon extrac-
tion methods. This approach to bilingual lexicon
extraction from comparable corpora radically dif-
fers, to our knowledge, from previous approaches
650
C0 CH C?H C1 C2 C1 new > C1, > C0 C2 new > C2, > C0
WL 0.114 0.144 0.125 0.136 0.181 0.156 2.0%, 4.2% 0.205 2.4%, 9.1%
WM 0.233 0.313 0.270 0.345 0.401 0.369 2.4%, 3.6% 0.433 3.2%, 20.0%
WH 0.417 0.456 0.377 0.568 0.633 0.581 1.3%, 16.4% 0.643 1.0%, 22.6%
All 0.205 0.224 0.189 0.258 0.310 0.278 2.0%, 7.3% 0.333 2.3%, 12.8%
Table 4: Precision of the different approaches on different corpora
which are mainly variants of the standard method
proposed in (Fung and Yee, 1998) and (Rapp,
1999). For example, the method developed in
(De?jean et al, 2002) and (Chiao and Zweigen-
baum, 2002) involves a representation of dictio-
nary entries with context vectors onto which new
words are mapped. Pekar et al (2006) smooth
the context vectors used in the standard approach
in order to better deal with low frequency words.
A nice geometric interpretation of these processes
is proposed in (Gaussier et al, 2004), which fur-
thermore introduces variants based on Fisher ker-
nels, Canonical Correlation Analysis and a com-
bination of them, leading to an improvement of
the F1-score of 2% (from 0.14 to 0.16) when con-
sidering the top 20 candidates. In contrast, the ap-
proach we have developed yields an improvement
of 7% (from 0.13 to 0.20) of the F-1 score on C2,
again considering the top 20 candidates. More im-
portant, however, is the fact that the approach we
have developed can be used in conjunction with
any existing bilingual extraction method, as the
strategies for improving the corpus quality and the
re-weighting formula (equation 2) are general. We
will assess in the future whether substantial gains
are also attained with other methods.
Some studies have tried to extract subparts of
comparable corpora to complement existing par-
allel corpora. Munteanu (2004) thus developed a
maximum entropy classifier aiming at extracting
those sentence pairs which can be deemed paral-
lel. The step for choosing similar document pairs
in this work resembles some of our steps. How-
ever their work focuses on high quality and spe-
cific documents pairs, as opposed to the entire cor-
pus of guaranteed quality we want to build. In
this latter case, the cross-interaction between doc-
uments impacts the overall comparability score,
and new methods, as the one we have introduced,
need to be proposed. Similarly, Munteanu and
Marcu (2006) propose a method to extract sub-
sentential fragments from non-parallel corpora.
Again, the targeted elements are very specific
(parallel sentences or sub-sentences) and limited,
and the focus is put on a few sentences which can
be considered parallel. As already mentioned, we
rather focus here on building a new corpus which
preserves most of the information in the original
corpus. The construction process we have pre-
sented is theoretically justified and allows one to
preserve ca. 95% of the original vocabulary.
7 Conclusion
We have first introduced in this paper a compara-
bility measure based on the expectation of find-
ing translation word pairs in the corpus. We have
then designed a strategy to construct an improved
comparable corpus by (a) extracting a subpart of
the original corpus with a guaranteed compara-
bility level, and (b) by completing the remaining
subpart with external resources, in our case other
existing bilingual corpora. We have then shown
how the information obtained during the construc-
tion process could be used to improve state-of-
the-art bilingual lexicon extraction methods. We
have furthermore assessed the various steps of
our approach and shown: (a) that the compara-
bility measure we introduced captures variations
in the degree of comparability between corpora,
(b) that the construction process we introduced
leads to an improved corpus preserving most of
the original vocabulary, and (c) that the use of
pseudo-alignments through simple re-weighting
yields bilingual lexicons of higher quality.
Acknowledgements
This work was supported by the French National
Research Agency grant ANR-08-CORD-009.
651
References
Ballesteros, Lisa and W. Bruce Croft. 1997. Phrasal
translation and query expansion techniques for
cross-language information retrieval. In Proceed-
ings of the 20th ACM SIGIR, pages 84?91, Philadel-
phia, Pennsylvania, USA.
Chen, Stanley F. 1993. Aligning sentences in bilingual
corpora using lexical information. In Proceedings
of the 31st Annual Conference of the Association for
Computational Linguistics, pages 9?16, Columbus,
Ohio, USA.
Chiao, Yun-Chuang and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in
specialized, comparable corpora. In Proceedings
of the 19th International Conference on Computa-
tional Linguistics, pages 1?7, Taipei, Taiwan.
De?jean, Herve?, Eric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and
model combination for bilingual lexicon extraction.
In Proceedings of the 19th International Conference
on Computational Linguistics, pages 1?7, Taipei,
Taiwan.
Fung, Pascale and Kathleen McKeown. 1997. Finding
terminology translations from non-parallel corpora.
In Proceedings of the 5th Annual Workshop on Very
Large Corpora, pages 192?202, Hong Kong.
Fung, Pascale and Lo Yuen Yee. 1998. An IR ap-
proach for translating new words from nonparallel,
comparable texts. In Proceedings of the 17th inter-
national conference on Computational linguistics,
pages 414?420, Montreal, Quebec, Canada.
Gaussier, E., J.-M. Renders, I. Matveeva, C. Goutte,
and H. De?jean. 2004. A geometric view on bilin-
gual lexicon extraction from comparable corpora.
In Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics, pages
526?533, Barcelona, Spain.
Kay, Martin and Martin Ro?scheisen. 1993. Text-
translation alignment. Computational Linguistics,
19(1):121?142.
Koehn, Philipp. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit 2005.
Melamed, I. Dan. 1997a. A portable algorithm
for mapping bitext correspondence. In Proceed-
ings of the 35th Annual Meeting of the Association
for Computational Linguistics and the 8th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 305?312, Madrid,
Spain.
Melamed, I. Dan. 1997b. A word-to-word model
of translational equivalence. In Proceedings of the
35th Annual Meeting of the Association for Compu-
tational Linguistics and the 8th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 490?497, Madrid, Spain.
Morin, Emmanuel, Be?atrice Daille, Koichi Takeuchi,
and Kyo Kageura. 2007. Bilingual terminology
mining - using brain, not brawn comparable cor-
pora. In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics,
pages 664?671, Prague, Czech Republic.
Munteanu, Dragos Stefan and Daniel Marcu. 2006.
Extracting parallel sub-sentential fragments from
non-parallel corpora. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and the 44th annual meeting of the Association
for Computational Linguistics, pages 81?88, Syd-
ney, Australia.
Munteanu, Dragos Stefan, Alexander Fraser, and
Daniel Marcu. 2004. Improved machine translation
performance via parallel sentence extraction from
comparable corpora. In Proceedings of the HLT-
NAACL 2004, pages 265?272, Boston, MA., USA.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Pekar, Viktor, Ruslan Mitkov, Dimitar Blagoev, and
Andrea Mulloni. 2006. Finding translations for
low-frequency words in comparable corpora. Ma-
chine Translation, 20(4):247?266.
Rapp, Reinhard. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of the 37th Annual
Meeting of the Association for Computational Lin-
guistics, pages 519?526, College Park, Maryland,
USA.
Robitaille, Xavier, Yasuhiro Sasaki, Masatsugu
Tonoike, Satoshi Sato, and Takehito Utsuro. 2006.
Compiling French-Japanese terminologies from the
web. In Proceedings of the 11st Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 225?232, Trento, Italy.
Yu, Kun and Junichi Tsujii. 2009. Extracting bilin-
gual dictionary from comparable corpora with de-
pendency heterogeneity. In Proceedings of HLT-
NAACL 2009, pages 121?124, Boulder, Colorado,
USA.
652
