Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 28?37,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Efficient, correct, unsupervised learning of context-sensitive languages
Alexander Clark
Department of Computer Science
Royal Holloway, University of London
alexc@cs.rhul.ac.uk
Abstract
A central problem for NLP is grammar in-
duction: the development of unsupervised
learning algorithms for syntax. In this pa-
per we present a lattice-theoretic represen-
tation for natural language syntax, called
Distributional Lattice Grammars. These
representations are objective or empiri-
cist, based on a generalisation of distribu-
tional learning, and are capable of repre-
senting all regular languages, some but not
all context-free languages and some non-
context-free languages. We present a sim-
ple algorithm for learning these grammars
together with a complete self-contained
proof of the correctness and efficiency of
the algorithm.
1 Introduction
Grammar induction, or unsupervised learning of
syntax, no longer requires extensive justification
and motivation. Both from engineering and cog-
nitive/linguistic angles, it is a central challenge
for computational linguistics. However good al-
gorithms for this task are thin on the ground.
There are numerous heuristic algorithms, some of
which have had significant success in inducing
constituent structure (Klein and Manning, 2004).
There are algorithms with theoretical guarantees
as to their correctness ? such as for example
Bayesian algorithms for inducing PCFGs (John-
son, 2008), but such algorithms are inefficient: an
exponential search algorithm is hidden in the con-
vergence of the MCMC samplers. The efficient
algorithms that are actually used are heuristic ap-
proximations to the true posteriors. There are al-
gorithms like the Inside-Outside algorithm (Lari
and Young, 1990) which are guaranteed to con-
verge efficiently, but not necessarily to the right
answer: they converge to a local optimum that
may be, and in practice nearly always is very far
from the optimum. There are naive enumerative
algorithms that are correct, but involve exhaus-
tively enumerating all representations below a cer-
tain size (Horning, 1969). There are no correct
and efficient algorithms, as there are for parsing,
for example.
There is a reason for this: from a formal point
of view, the problem is intractably hard for the
standard representations in the Chomsky hierar-
chy. Abe and Warmuth (1992) showed that train-
ing stochastic regular grammars is hard; Angluin
and Kharitonov (1995) showed that regular gram-
mars cannot be learned even using queries; these
results obviously apply also to PCFGs and CFGs
as well as to the more complex representations
built by extending CFGs, such as TAGs and so
on. However, these results do not necessarily ap-
ply to other representations. Regular grammars
are not learnable, but deterministic finite automata
are learnable under various paradigms (Angluin,
1987). Thus it is possible to learn by changing to
representations that have better properties: in par-
ticular DFAs are learnable because they are ?ob-
jective?; there is a correspondence between the
structure of the language, (the residual languages)
and the representational primitives of the formal-
ism (the states) which is expressed by the Myhill-
Nerode theorem.
In this paper we study the learnability of a class
of representations that we call distributional lat-
tice grammars (DLGs). Lattice-based formalisms
were introduced by Clark et al (2008) and Clark
(2009) as context sensitive formalisms that are po-
tentially learnable. Clark et al (2008) established
a similar learnability result for a limited class of
context free languages. In Clark (2009), the ap-
proach was extended to a significantly larger class
but without an explicit learning algorithm. Most of
the building blocks are however in place, though
we need to make several modifications and ex-
28
tensions to get a clean result. Most importantly,
we need to replace the representation used there,
which naively could be exponential, with a lazy,
exemplar based model.
In this paper we present a simple algorithm
for the inference of these representations and
prove its correctness under the following learning
paradigm: we assume that as normal there is a sup-
ply of positive examples, and additionally that the
learner can query whether a string is in the lan-
guage or not (an oracle for membership queries).
We also prove that the algorithm is efficient in
the sense that it will use a polynomial amount of
computation and makes a polynomial number of
queries at each step.
The contributions of this paper are as follows:
after some basic discussion of distributional learn-
ing in Section 2, we define in Section 3 an
exemplar-based grammatical formalism which we
call Distributional Lattice Grammars. We then
give a learning algorithm under a reasonable learn-
ing paradigm, together with a self contained proof
in elementary terms (not presupposing any exten-
sive knowledge of lattice theory), of the correct-
ness of this algorithm.
2 Basic definitions
We now define our notation; we have a finite al-
phabet ?; let ?? be the set of all strings (the free
monoid) over ?, with ? the empty string. A (for-
mal) language is a subset of ??. We can concate-
nate two languagesA andB to getAB = {uv|u ?
A, b ? B}.
A context or environment, as it is called in struc-
turalist linguistics, is just an ordered pair of strings
that we write (l, r) where l and r refer to left and
right; l and r can be of any length. We can com-
bine a context (l, r) with a string u with a wrap-
ping operation that we write : so (l, r)  u is
defined to be lur. We will sometimes write f for
a context (l, r). There is a special context (?, ?):
(?, ?)  w = w. We will extend this to sets of
contexts and sets of strings in the natural way. We
will write Sub(w) = {u|?(l, r) : lur = w} for
the set of substrings of a string, and Con(w) =
{(l, r)|?u ? ?? : lur = w}.
For a given string w we can define the distribu-
tion of that string to be the set of all contexts that it
can appear in: CL(w) = {(l, r)|lwr ? L}, equiv-
alently {f |f  w ? L}. Clearly (?, ?) ? CL(w)
iff w ? L.
Distributional learning (Harris, 1954) as a tech-
nical term refers to learning techniques which
model directly or indirectly properties of the dis-
tribution of strings or words in a corpus or a lan-
guage. There are a number of reasons to take
distributional learning seriously: first, historically,
CFGs and other PSG formalisms were intended
to be learnable by distributional means. Chomsky
(2006) says (p. 172, footnote 15):
The concept of ?phrase structure
grammar? was explicitly designed to ex-
press the richest system that could rea-
sonably be expected to result from the
application of Harris-type procedures to
a corpus.
Second, empirically we know they work well at
least for lexical induction, (Schu?tze, 1993; Cur-
ran, 2003) and are a component of some imple-
mented unsupervised learning systems (Klein and
Manning, 2001). Linguists use them as one of the
key tests for constituent structure (Carnie, 2008),
and finally there is some psycholinguistic evidence
that children are sensitive to distributional struc-
ture, at least in artificial grammar learning tasks
(Saffran et al, 1996). These arguments together
suggest that distributional learning has a some-
what privileged status.
3 Lattice grammars
Clark (2009) presents the theory of lattice based
formalisms starting algebraically from the theory
of residuated lattices. Here we will largely ig-
nore this, and start from a straightforward com-
putational treatment. We start by defining the rep-
resentation.
Definition 1. Given a non-empty finite alphabet,
?, a distributional lattice grammar (DLG) is a 3-
tuple consisting of ?K,D,F ?, where F is a finite
subset of ?? ? ??, such that (?, ?) ? F , K is a
finite subset of ?? which contains ? and ?, and D
is a subset of (F KK).
K here can be thought of as a finite set of exem-
plars, which correspond to substrings or fragments
of the language. F is a set of contexts or fea-
tures, that we will use to define the distributional
properties of these exemplars; finally D is a set
of grammatical strings, the data; a finite subset of
the language. F KK using the notation above is
{luvr|u, v ? K, (l, r) ? F}. This is the finite part
of the language that we examine. If the language
29
we are modeling is L, then D = L ? (F KK).
Since ? ? K,K ? KK.
We define a concept to be an ordered pair ?S,C?
where S ? K and C ? F , which satisfies the fol-
lowing two conditions: first C  S ? D; that is
to say every string in S can be combined with any
context in C to give a grammatical string, and sec-
ondly they are maximal in that neither K nor F
can be increased without violating the first condi-
tion.
We define B(K,D,F ) to be the set of all such
concepts. We use theB symbol (Begriff ) to bring
out the links to Formal Concept Analysis (Ganter
and Wille, 1997; Davey and Priestley, 2002). This
lattice may contain exponentially many concepts,
but it is clearly finite, as the number of concepts is
less than min(2|F |, 2|K|).
There is an obvious partial order defined by
?S1, C1? ? ?S2, C2? iff S1 ? S2, Note that
S1 ? S2 iff C2 ? C1.
Given a set of strings S we can define a set of
contexts S? to be the set of contexts that appear
with every element of S.
S? = {(l, r) ? F : ?w ? S, lwr ? D}
Dually we can define for a set of contexts C the
set of strings C ? that occur with all of the elements
of C:
C ? = {w ? K : ?(l, r) ? C, lwr ? D}
The concepts ?S,C? are just the pairs that sat-
isfy S? = C and C ? = S; the two maps denoted
by ? are called the polar maps. For any S ? K,
S??? = S? and for any C ? F , C ??? = C ?. Thus we
can form a concept from any set of strings S ? K
by taking ?S??, S??; this is a concept as S??? = S?.
We will write this as C(S), and for any C ? F ,
we will write C(C) = ?C ?, C ???.
If S ? T then T ? ? S?, and S?? ? T ??. For any
set of strings S ? K, S ? S??.
One crucial concept here is the concept de-
fined by (?, ?) or equivalently by the set K ? D
which corresponds to all of the elements in the
language. We will denote this concept by L =
C({(?, ?)}) = C(K ?D).
We also define a meet operation by
?S1, C1? ? ?S2, C2? = ?S1 ? S2, (S1 ? S2)
??
This is the greatest lower bound of the two con-
cepts; this is a concept since if S??1 = S1 and
S??2 = S2 then (S1 ? S2)
?? = (S1 ? S2). Note that
this operation is associative and commutative. We
can also define a join operation dually; with these
operationsB(K,D,D) is a complete lattice.
So far we have only used strings in F K; we
now define a concatenation operation as follows.
?S1, C1? ? ?S2, C2? = ?(S1S2)
??, (S1S2)
????
Since S1 and S2 are subsets of K, S1S2 is a sub-
set of KK, but not necessarily of K. (S1S2)? is
the set of contexts shared by all elements of S1S2
and (S1S2)?? is the subset of K, not KK, that
has all of the contexts of (S1S2)?. (S1S2)??? might
be larger than (S1S2)?. We can also write this as
C((S1S2)?).
Both ? and ? are monotonic in the sense that if
X ? Y then X ? Z ? Y ? Z, Z ? X ? Z ? Y
and X ? Z ? Y ? Z. Note that all of these oper-
ations can be computed efficiently; using a perfect
hash, and a naive algorithm, we can do the polar
maps and ? operations in timeO(|K||F |), and the
concatenation in time O(|K|2|F |).
We now define the notion of derivation in this
representation. Given a string w we recursively
compute a concept for every substring of w; this
concept will approximate the distribution of the
string. We define ?G as a function from ?? to
B(K,D,F ); we define it recursively:
? If |w| ? 1, then ?G(w) = ?{w}??, {w}??
? If |w| > 1 then
?G(w) =
?
u,v??+:uv=w ?G(u) ? ?G(v)
The first step is well defined because all of the
strings of length at most 1 are already in K so
we can look them up directly. To clarify the sec-
ond step, if w = abc then ?G(abc) = ?G(a) ?
?G(bc) ? ?G(ab) ? ?G(c); we compute the string
from all possible non-trivial splits of the string
into a prefix and a suffix. By using a dynamic
programming table that stores the values of ?(u)
for all u ? Sub(w) we can compute this in time
O(|K|2|F ||w|3); this is just an elementary variant
of the CKY algorithm. We define the language de-
fined by the DLG G to be
L(G) = {w|?G(w) ? C({(?, ?)})}
That is to say, a string is in the language if we
predict that a string has the context (?, ?). We now
consider a trivial example: the Dyck language.
30
Example 1. Let L be the Dyck language (matched
parenthesis language) over ? = {a, b}, where
a corresponds to open bracket, and b to close
bracket. Define
? K = {?, a, b, ab}
? F = {(?, ?), (?, b), (a, ?)}.
? D = {?, ab, abab, aabb}
G = ?K,D,F ? is a DLG. We will now write down
the 5 elements of the lattice:
? > = ?K, ??
? ? = ??, F ?
? L = ?{?, ab}, {(?, ?)}?
? A = ?{a}, {(?, b)}?
? B = ?{b}, {(a, ?)}?
To compute the concatenation A ? B we first
compute {a}{b} = {ab}; we then compute {ab}?
which is {(?, ?)}, and {(?, ?)}? = {?, ab}, so
A ? B = L. Similarly to compute L ? L, we first
take {?, ab}{?, ab} = {?, ab, abab}. These all
have the context (?, ?), so the result is the con-
cept L. If we compute A ?A we get {a}{a} which
is {aa} which has no contexts so the result is >.
We have ?G(?) = L, ?G(a) = A,?G(b) = B.
Applying the recursive computation we can verify
that ?G(w) = L iff w ? L and so L(G) = L. We
can also see that D = L ? (F KK).
4 Search
In order to learn these grammars we need to find a
suitable set of contexts F , a suitable set of strings
K, and then work out which elements of F KK
are grammatical. So given a choice for K and F
it is easy to learn these models under a suitable
regime: the details of how we collect information
about D depend on the learning model.
The question is therefore whether it is easy
to find suitable sets, K and F . Because of the
way the formalism is designed, it transpires that
the search problem is entirely tractable. In or-
der to analyse the search space, we define two
maps between the lattices as K and F are in-
creased. We are going to augment our notation
slightly; we will write B(K,L, F ) for B(K,L ?
(FKK), F ) and similarly ?K,L, F ? for ?K,L?
(F KK), F ?. When we use the two polar maps
(such as C ?, S?), though we are dealing with more
than one lattice, there is no ambiguity as the maps
agree; we will when necessary explicitly restrict
the output (e.g. C ? ? J) to avoid confusion.
Definition 2. For any language L and any set of
contexts F ? G, and any sets of strings J ?
K ? ??. We define a map g from B(J, L, F ) to
B(K,L, F ) (from the smaller lattice to the larger
lattice) as g(?S,C?) = ?C ?, C?.
We also define a map f from B(K,L,G)
to B(K,L, F ), (from larger to smaller) as
f(?S,C?) = ?(C ? F )?, C ? F ?.
These two maps are defined in opposite direc-
tions: this is because of the duality of the lattice.
By defining them in this way, as we will see, we
can prove that these two maps have very similar
properties. We can verify that the outputs of these
maps are in fact concepts.
We now need to define two monotonicity lem-
mas: these lemmas are crucial to the success of
the formalism. We show that as we increase K
the language defined by the formalism decreases
monotonically, and that as we increase F the lan-
guage increases monotonically. There is some du-
plication in the proofs of the two lemmas; we
could prove them both from more abstract prop-
erties of the maps f, g which are what are called
residual maps, but we will do it directly.
Lemma 1. Given two lattices B(K,L, F ) and
B(K,L,G) where F ? G; For all X,Y ?
B(K,L,G) we have that
1. f(X) ? f(Y ) ? f(X ? Y )
2. f(X) ? f(Y ) ? f(X ? Y )
Proof. The proof is elementary but difficult to
read. We write X = ?SX , CX? and similarly for
Y . For part 1 of the lemma: Clearly (S?X ? F ) ?
S?X , so (S
?
X ? F )
? ? S??X = SX and the same for
SY . So (S?X ?F )
?(S?Y ?F )
? ? SXSY (as subsets
ofKK). So ((S?X?F )
?(S?Y ?F )
?)? ? (SXSY )? ?
(SXSY )???. Now by definition, f(X) ? f(Y ) is
C(Z) where Z = ((S?X ?F )
?(S?Y ?F )
?)? ?F and
f(X ?Y ) has the set of contexts ((SXSY )??? ?F ).
Therefore f(X ? Y ) has a bigger set of contexts
than f(X) ? f(Y ) and is thus a smaller concept.
For part 2: by definition f(X ? Y ) = ?((SX ?
Sy)? ? F )?, (SX ? Sy)? ? F ? and f(X) ? f(Y ) =
?(S?X?F )
??(S?y?F )
?, ((S?X?F )
??(S?y?F )
?)??F ?
Now S?X ? F ? S
?
X , so (since S
??
X = SX ) SX ?
(S?X?F )
?, and so SX?Sy ? (S?X?F )
??(S?y?F )
?.
31
So (SX ? Sy)? ? ((S?X ? F )
? ? (S?y ? F )
?) which
gives the result by comparing the context sets of
the two sides of the inequality.
Lemma 2. For any language L, and two sets of
contexts F ? G, and any K, if we have two DLGs
?K,L, F ? with map ?F : ?? ? B(K,L, F ) and
?K,L,G? with map ?G : ?? ? B(K,L,G) then
for all w, f(?G(w)) ? ?F (w).
Proof. By induction on the length of w; clearly
if |w| ? 1, f(?G(w)) = ?F (w). We now take
the inductive step; by definition, (suppressing the
definition of u, v in the meet)
f(?G(w)) = f(
?
u,v
?G(u) ? ?G(v))
By Lemma 1, part 2:
f(?G(w)) ?
?
u,v
f(?G(u) ? ?G(v))
By Lemma 1, part 1:
f(?G(w)) ?
?
u,v
f(?G(u)) ? f(?G(v))
By the inductive hypothesis we have f(?G(u)) ?
?F (u) and similarly for v and so by the mono-
tonicity of ? and ?:
f(?G(w)) ?
?
u,v
?F (u) ? ?F (v)
Since the right hand side is equal to ?F (w), the
proof is done.
It is then immediate that
Lemma 3. If F ? G then L(?K,L, F ?) ?
L(?K,L,G?),
Proof. If w ? L(?K,L, F ?), then ?F (w) ? L,
and so f(?G(w)) ? L and so ?G(w) has the con-
text (?, ?) and is thus in L(?K,L,G?).
We now prove the corresponding facts about g.
Lemma 4. For any J ? K and any conceptsX,Y
inB(J, L, F ), we have that
1. g(X) ? g(Y ) ? g(X ? Y )
2. g(X) ? g(Y ) ? g(X ? Y )
Proof. For the first part: Write X = ?SX , CX? as
before. Note that SX = C ?X ? J . SX ? C
?
X , so
SXSY ? C ?XC
?
Y , and so (SXSY )
?? ? (C ?XC
?
Y )
??,
and ((SXSY )?? ? J)? ? (C ?XC
?
Y )
???. By calcu-
lation g(X) ? g(Y ) = ?(C ?XC
?
Y )
??, (C ?XC
?
Y )
????
On the other hand, g(X ? Y ) = ?((SXSY )?? ?
J)??, ((SXSY )?? ? J)?? and so g(X ? Y ) is smaller
as it has a larger set of contexts.
For the second part: g(X) ? g(Y ) = ?C ?X ?
C ?Y , (C
?
X ? C
?
Y )
?? and g(X ? Y ) = ?(SX ?
SY )??, (SX ? SY )??. Since SX = C ?X ? J , SX ?
C ?X , so (SX ? SY ) ? C
?
X ? C
?
Y , and therefore
(SX ? SY )?? ? (C ?X ? C
?
Y )
?? = C ?X ? C
?
Y .
We now state and prove the monotonicity
lemma for g.
Lemma 5. For all J ? K ? ?????, and for all
strings w; we have that g(?J(w)) ? ?K(w).
Proof. By induction on length of w. Both J and
K include the basic elements of ? and ?. First
suppose |w| ? 1, then ?J(w) = ?(CL(w) ?
F )??J,CL(w)?F ?, and g(?J(w)) = ?(CL(w)?
F )?, CL(w) ? F ? which is equal to ?K(w).
Now suppose true for all w of length at most k,
and take some w of length k + 1. By definition of
?J :
g(?J(w)) = g
(
?
u,v
?J(u) ? ?J(v)
)
Next by Lemma 4, Part 2
g(?J(w)) ?
?
u,v
g(?J(u) ? ?J(v))
By Lemma 4, Part 1
g(?J(w)) ?
?
u,v
g(?J(u)) ? g(?J(v))
By the inductive hypothesis and monotonicity of ?
and ?:
g(?J(w)) ?
?
u,v
?K(u) ? ?K(v) = ?K(w)
Lemma 6. If J ? K then L(?J, L, F ?) ?
L(?K,L, F ?)
Proof. Suppose w ? L(?K,L, F ?). this means
that ?K(w) ? LK . therefore g(?J(w)) ?
Lk; which means that (?, ?) is in the concept
g(?J(w)), which means it is in the concept ?J(w),
and therefore w ? L(?J, L, F ?).
32
Given these two lemmas we can make the fol-
lowing observations. First, if we have a fixed L
and F , then as we increase K, the language will
decrease until it reaches a limit, which it will at-
tain after a finite limit.
Lemma 7. For all L, and finite context sets F ,
there is a finite K such that for all K2, K ? K2,
L(?K,L, F ?) = L(?K2, L, F ?).
Proof. We can define the latticeB(??, L, F ). De-
fine the following equivalence relation between
pairs of strings, where (u1, v1) ? (u2, v2) iff
C(u1) = C(u2) and C(v1) = C(v2) and C(u1v1) =
C(u2v2). The number of equivalence classes is
clearly finite. If K is sufficiently large that there is
a pair of strings (u, v) in K for each equivalence
class, then clearly the lattice defined by thisK will
be isomorphic toB(??, L, F ). Any superset ofK
will not change this lattice.
Moreover this language is unique for each L,F .
We will call this the limit language ofL,F , and we
will write it as L(???, L, F ?).
If F ? G, then L(???, L, F ?) ?
L(???, L,G?). Finally, we will show that
the limit languages never overgeneralise.
Lemma 8. For any L, and for any F ,
L(???, L, F ?) ? L.
Proof. Recall that C(w) = ?{w}??, {w}?? is the
real concept. If G is a limit grammar, we can
show that we always have ?G(w) > C(w), which
will give us the result immediately. First note
that C(u) ? C(v) ? C(uv), which is immedi-
ate by the definition of ?. We proceed, again,
by induction on the length of w. For |w| ? 1,
?G(w) = C(w). For the inductive step we have
?G(w) =
?
u,v ?G(u) ? ?G(v); by inductive hy-
pothesis we have that this must be more than
?
u,v C(u) ? C(v) >
?
u,v C(uv) = C(w)
5 Weak generative power
First we make the following observation: if we
consider an infinite variant of this, where we set
K = ?? and F = ?? ? ?? and D = L, we
can prove easily that, allowing infinite ?represen-
tations?, for any L, L(?K,D,F ?) = L. In this
infinite data limit, ? becomes associative, and the
structure ofB(K,D,F ) becomes a residuated lat-
tice, called the syntactic concept lattice of the lan-
guage L, B(L). This lattice is finite iff the lan-
guage is regular. The fact that this lattice now has
residuation operations suggest interesting links to
the theory of categorial grammar. It is the finite
case that interests us.
We will use LDLG to refer to the class of lan-
guages that are limit languages in the sense de-
fined above.
LDLG = {L|?F,L(??
?, L, F ?) = L}
Our focus in this paper is not on the language
theory: we present the following propositions.
First LDLG properly contains the class of regular
languages. Secondly LDLG contains some non-
context-free languages (Clark, 2009). Thirdly it
does not contain all context-free languages.
A natural question to ask is how to convert a
CFG into a DLG. This is in our view the wrong
question, as we are not interested in modeling
CFGs but modeling natural languages, but given
the status of CFGs as a default model for syn-
tactic structure, it will help to give a few exam-
ples, and a general mechanism. Consider a non-
terminal N in a CFG with start symbol S. We
can define C(N) = {(l, r)|S
?
? lNr} and the
yield Y (N) = {w|N
?
? w}. Clearly C(N) 
Y (N) ? L, but these are not necessarily maxi-
mal, and thus ?C(N), Y (N)? is not necessarily a
concept. Nonetheless in most cases, we can con-
struct a grammar where the non-terminals will cor-
respond to concepts, in this way.
The basic approach is this: for each non-
terminal, we identify a finite set of contexts that
will pick out only the set of strings generated
from that non-terminal: we find some set of con-
texts FN typically a subset of C(N) such that
Y (N) = {w|?(l, r) ? FN , lwr ? L}. We say
that we can contextually define this non-terminal
if there is such a finite set of contexts FN . If a
CFG in Chomsky normal form is such that every
non-terminal can be contextually defined then the
language defined by that grammar is in LDLG. If
we can do that, then the rest is trivial. We take
any set of features F that includes all of these FN ;
probably just F =
?
N FN ; we then pick a set of
strings K that is sufficiently large to rule out all
incorrect generalisations, and then define D to be
L ? (F KK).
Consider the language L = {anbncm|n,m ?
0} ? {ambncn|n,m ? 0}. L is a classic ex-
ample of an inherently ambiguous and thus non-
deterministic language.
The natural CFG in CNF for L has
non-terminals that generate the following
33
sets: {anbn|n ? 0}, {an+1bn|n ? 0},
{bncn|n ? 0}, {bncn+1|n ? 0}, {a?}
and {c?}. We note that the six contexts
(aa, bbc), (aa, bbbc), (abb, cc)(abbb, cc), (?, a)
and (c, ?) will define exactly these sets, in
the sense that the set of strings that oc-
cur in each context will be exactly the
corresponding set. We can also pick out
?, a, b, c with individual contexts. Let F =
{(?, ?), (aaabb, bccc), (aaabbc, ?), (?, abbccc),
(aaab, bccc), (aa, bbc), (aa, bbbc), (abb, cc),
(abbb, cc), (?, a), (c, ?)}. If we take a sufficiently
large set K, say ?, a, b, c, ab, aab, bc, bcc, abc, and
set D = L ? F KK, then we will have a DLG
for the language L. In this example, it is sufficient
to have one context per non-terminal. This is not
in general the case.
Consider L = {anbn|n ? 0} ? {anb2n|n ?
0}. Here we clearly need to identify sets of strings
corresponding to the two parts of this language,
but it is easy to see that no one context will suffice.
However, note that the first part is defined by the
two contexts (?, ?), (a, b) and the second by the
two contexts (?, ?), (a, bb). Thus it is sufficient to
have a set F that includes these four contexts, as
well as similar pairs for the other non-terminals in
the grammar, and some contexts to define a and b.
We can see that we will not always be able to do
this for every CFG. One fixable problem is if the
CFG has two separate non-terminals, M,N such
that C(M) ? C(N). If this is the case, then we
must have that Y (N) ? Y (M), If we pick a set
of contexts to define Y (N), then clearly any string
in Y (M) will also be picked out by the same con-
texts. If this is not the case, then we can clearly try
to rectify it by adding a rule N ? M which will
not change the language defined.
However, we cannot always pick out the non-
terminals with a finite set of contexts. Consider
the language L = {anb|n > 0} ? {ancm|m >
n > 0} defined in Clark et al (2008). Sup-
pose wlog that F contains no context (l, r) such
that |l| + |r| ? k. Then it is clear that we will
not be able to pick out b without also picking out
ck+1, since CL(ck+1) ? F ? CL(b) ? F . Thus
L, which is clearly context-free, is not in LDLG.
Luckily, this example is highly artificial and does
not correspond to any phenomena we are aware of
in linguistics.
In terms of representing natural languages, we
clearly will in many cases need more than one
context to pick out syntactically relevant groups
of strings. Using a very simplified example from
English, if we want to identify say singular noun
phrases, a context like (that is, ?) will not be suf-
ficient since as well as noun phrases we will also
have some adjective phrases. However if we in-
clude multiple contexts such as (?, is over there)
and so on, eventually we will be able to pick out
exactly the relevant set of strings. One of the
reasons we need to use a context sensitive repre-
sentation, is so that we can consider every possi-
ble combination of contexts simultaneously: this
would require an exponentially large context free
grammar.
6 Learning Model
In order to prove correctness of the learning algo-
rithm we will use a variant of Gold-style inductive
inference (Gold, 1967). Our choice of this rather
old-fashioned model requires justification. There
are two problems with learning ? the information
theoretic problems studied under VC-dimension
etc., and the computational complexity issues of
constructing a hypothesis from the data. In our
view, the latter problems are the key ones. Ac-
cordingly, we focus entirely on the efficiency is-
sue, and allow ourself a slightly unrealistic model;
see (Clark and Lappin, 2009) for arguments that
this is a plausible model.
We assume that we have a sequence of posi-
tive examples, and that we can query examples for
membership. Given a language L a presentation
for L is an infinite sequence of strings w1, w2, . . .
such that {wi|i ? N} = L. An algorithm receives
a sequence T and an oracle, and must produce a
hypothesis H at every step, using only a polyno-
mial number of queries to the membership oracle
? polynomial in the total size of the presentation.
It identifies in the limit the language L iff for ev-
ery presentation T of L there is a N such that for
all n > N Hn = HN , and L(HN ) = L. We say
it identifies in the limit a class of languages L iff
it identifies in the limit all L in L. We say that it
identifies the class in polynomial update time iff
there is a polynomial p, such that at each step the
model uses an amount of computation (and thus
also a number of queries) that is less than p(n, l),
where n is the number of strings and l is the max-
imum length of a string in the observed data. We
note that this is slightly too weak. It is possible
to produce vacuous enumerative algorithms that
34
can learn anything by only processing a logarith-
mically small prefix of the string (Pitt, 1989).
7 Learning Algorithm
We now define a simple learning algorithm, that
establishes learnability under this paradigm.
There is one minor technical detail we need to
deal with. We need to be able to tell when adding
a string to a lazy DLG will leave the grammar un-
changed. We use a slightly weaker test. Given
G1 = ?K,D,F ? we define as before the equiva-
lence relation between pairs of strings ofK, where
(u1, v1) ?G1 (u2, v2) iff CD(u1) = CD(u2) and
CD(v1) = CD(v2) and CD(u1v1) = CD(u2v2).
Note that CD(u) = {(l, r)|lur ? D}.
Given two grammars G1 = ?K,D,F ? and
G2 = ?K2, D2, F ? where K ? K2 and D ? D2
but F is unchanged, we say that these two are
indistinguishable iff the number of equivalence
classes of K ?K under ?G1 is equal to the num-
ber of equivalence classes of K2?K2 under?G2 .
This can clearly be computed efficiently using a
union-find algorithm, in time polynomial in |K|
and |F |. If they are indistinguishable then they de-
fine the same language.
7.1 Algorithm
Algorithm 1 presents the basic algorithm. At var-
ious points we compute sets of strings like (F 
KK)?L; these can be computed using the mem-
bership oracle.
First we prove that the program is efficient in
the sense that it runs in polynomial update time.
Lemma 9. There is a polynomial p, such that Al-
gorithm 1, for each wn, runs in time bounded by
p(n, l) where l is the maximum length of a string
in w1, . . . wn.
Proof. First we note that K,K2 and F are always
subsets of Sub(E)?? andCon(E), and thus both
|K| and |F | are bounded by nl(l+1)/2+ |?|+1.
Computing D is efficient as |F KK| is bounded
by |K|2|F |. We can compute ?G as mentioned
above in time |K|2|F |l3; distinguishability is as
observed earlier also polynomial.
Before we prove the correctness of the algo-
rithm we make some informal points. First, we
are learning under a rather pessimistic model ? the
positive examples may be chosen to confuse us,
so we cannot make any assumptions. Accordingly
we have to very crudely add all substrings and all
Algorithm 1: DLG learning algorithm
Data: Input strings S = {w1, w2 . . . , },
membership oracle O
Result: A sequence of DLGs G1, G2, . . .
K ? ? ? {?}, K2 = K ;
F ? {(?, ?)}, E = {} ;
D = (F KK) ? L ;
G = ?K,D,F ? ;
for wi do
E ? E ? {wi} ;
K2 ? K2 ? Sub(wi) ;
if there is some w ? E that is not in
L(G) then
F ? Con(E) ;
K ? K2 ;
D = (F KK) ? L ;
G = ?K,D,F ? ;
end
else
D2 ? (F K2K2) ? L ;
if ?K2, D2, F ? not indistinguishable
from ?K,D,F ? then
K ? K2 ;
D = (F KK) ? L ;
G = ?K,D,F ? ;
end
end
Output G;
end
contexts, rather than using sensible heuristics to
select frequent or likely ones.
Intuitively the algorithm works as follows: if we
observe a string not in our current hypothesis, then
we increase the set of contexts which will increase
the language defined. Since we only see positive
examples, we will never explicitly find out that our
hypothesis overgenerates, accordingly we always
add strings to a tester set K2 and see if this gives
us a more refined model. If this seems like it might
give a tighter hypothesis, then we increase K.
In what follows we will say that the hypothesis
at step n, Gn = ?Kn, Dn, Fn?, and the language
defined is Ln. We will assume that the target lan-
guage is some L ? LDLG and w1, . . . is a presen-
tation of L.
Lemma 10. Then there is a point n, and a finite set
of contexts F such that for all N > n, FN = F .,
and L(???, L, F ?) = L.
Proof. Since L ? LDLG there is some set of con-
35
texts G ? Con(L), such that L = L(???, L,G?).
Any superset ofG will define the correct limit lan-
guage. Let n be the smallest n such that G is a
subset of Con({w1, . . . , wn}). Consider Fn. If
Fn defines the correct limit language, then we will
never change F as the hypothesis will be a super-
set of the target. Otherwise it must define a subset
of the correct language. Then either there is some
N > n at which it has converged to the limit lan-
guage which will cause the first condition in the
loop to be satisfied and F will be increased to a
superset ofG, or F will be increased before it con-
verges, and thus the result holds.
Lemma 11. After F converges according to the
previous lemma, there is some n, such that for all
N > n, KN = Kn and L(?Kn, L, Fn?) = L.
Proof. let n0 be the convergence point of F ; for
all n > n0 the hypothesis will be a superset of
the target language; therefore the only change that
can happen is that K will increase. By definition
of the limit language, it must converge after a finite
number of examples.
Theorem 1. For every language L ? LDLG, and
every presentation of L, Algorithm 1 will converge
to a grammar G such that L(G) = L.
This result is immediate by the two preceding
lemmas.
8 Conclusion
We have presented an efficient, correct learning al-
gorithm for an interesting class of languages; this
is the first such learning result for a class of lan-
guages that is potentially large enough to describe
natural language.
The results presented here lack a couple of tech-
nical details to be completely convincing. In par-
ticular we would like to show that given a repre-
sentation of size n, we can learn once we have seen
a set of examples that is polynomially bounded by
n. This will be challenging, as the size of the K
we need to converge can be exponentially large
in F . We can construct DFAs where the num-
ber of congruence classes of the language is an
exponential function of the number of states. In
order to learn languages like this, we will need
to use a more efficient algorithm that can learn
even with ?insufficient? K: that is to say when
the lattice B(K,L, F ) has fewer elements that
B(KK,L, F ).
This algorithm can be implemented directly and
functions as expected on synthetic examples, but
would need modification to run efficiently on nat-
ural languages. In particular rather than consider-
ing whole contexts of the form (l, r) it would be
natural to restrict them just to a narrow window
of one or two words or tags on each side. Rather
than using a membership oracle, we could prob-
abilistically cluster the data in the table of counts
of strings in F  K. In practice we will have a
limited amount of data to work with and we can
control over-fitting in a principled way by control-
ling the relative size of K and F .
This formalism represents a process of anal-
ogy from stored examples, based on distributional
learning ? this is very plausible in terms of what
we know about cognitive processes, and is com-
patible with much non-Chomskyan theorizing in
linguistics (Blevins and Blevins, 2009). The class
of languages is a good fit to the class of natural
languages; it contains, as far as we can tell, all
standard examples of context free grammars, and
includes non-deterministic and inherently ambigu-
ous grammars. It is hard to say whether the class
is in fact large enough to represent natural lan-
guages; but then we don?t know that about any for-
malism, context-free or context-sensitive. All we
can say is that there are no phenomena that we are
aware of that don?t fit. Only large scale empirical
work can answer this question.
Ideologically these models are empiricist ? the
structure of the representation is based on the
structure of the data: this has to be a good thing
for computational modeling. By minimizing the
amount of hidden, unobservable structure, we can
improve learnability. Languages are enormously
complex, and it would be simplistic to try to re-
duce their acquisition to a few pages of mathe-
matics; nonetheless, we feel that the representa-
tions and grammar induction algorithms presented
in this paper could be a significant piece of the
puzzle.
36
References
N. Abe and M. K. Warmuth. 1992. On the computa-
tional complexity of approximating distributions by
probabilistic automata. Machine Learning, 9:205?
260.
D. Angluin and M. Kharitonov. 1995. When won?t
membership queries help? J. Comput. Syst. Sci.,
50:336?355.
D. Angluin. 1987. Learning regular sets from queries
and counterexamples. Information and Computa-
tion, 75(2):87?106.
James P. Blevins and Juliette Blevins. 2009. Analogy
in grammar: Form and acquisition. Oxford Univer-
sity Press.
A. Carnie. 2008. Constituent structure. Oxford Uni-
versity Press, USA.
Noam Chomsky. 2006. Language and mind. Cam-
bridge University Press, 3rd edition.
Alexander Clark and Shalom Lappin. 2009. Another
look at indirect negative evidence. In Proceedings of
the EACL Workshop on Cognitive Aspects of Com-
putational Language Acquisition, Athens, March.
Alexander Clark, Re?mi Eyraud, and Amaury Habrard.
2008. A polynomial algorithm for the inference of
context free languages. In Proceedings of Interna-
tional Colloquium on Grammatical Inference, pages
29?42. Springer, September.
Alexander Clark. 2009. A learnable representation
for syntax using residuated lattices. In Proceedings
of the 14th Conference on Formal Grammar, Bor-
deaux, France.
J.R. Curran. 2003. From distributional to semantic
similarity. Ph.D. thesis, University of Edinburgh.
B. A. Davey and H. A. Priestley. 2002. Introduction to
Lattices and Order. Cambridge University Press.
B. Ganter and R. Wille. 1997. Formal Concept Analy-
sis: Mathematical Foundations. Springer-Verlag.
E. M. Gold. 1967. Language identification in the limit.
Information and control, 10(5):447 ? 474.
Zellig Harris. 1954. Distributional structure. Word,
10(2-3):146?62.
J. J. Horning. 1969. A Study of Grammatical Infer-
ence. Ph.D. thesis, Stanford University, Computer
Science Department, California.
M. Johnson. 2008. Using adaptor grammars to identify
synergies in the unsupervised acquisition of linguis-
tic structure. In 46th Annual Meeting of the ACL,
pages 398?406.
Dan Klein and Chris Manning. 2001. Distribu-
tional phrase structure induction. In Proceedings of
CoNLL 2001, pages 113?121.
Dan Klein and Chris Manning. 2004. Corpus-based
induction of syntactic structure: Models of depen-
dency and constituency. In Proceedings of the 42nd
Annual Meeting of the ACL.
K. Lari and S. J. Young. 1990. The estimation of
stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Language,
4:35?56.
L. Pitt. 1989. Inductive inference, dfa?s, and computa-
tional complexity. In K. P. Jantke, editor, Analogical
and Inductive Inference, number 397 in LNAI, pages
18?44. Springer-Verglag.
J. R. Saffran, R. N. Aslin, and E. L. Newport. 1996.
Statistical learning by eight month old infants. Sci-
ence, 274:1926?1928.
Hinrich Schu?tze. 1993. Part of speech induction from
scratch. In Proceedings of the 31st annual meet-
ing of the Association for Computational Linguis-
tics, pages 251?258.
37
