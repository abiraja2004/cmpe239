Comput ing  Phrasa l - s igns  in HPSG pr ior  to Pars ing  
Kentaro  Tor i sawa and  aun ' i ch i  Tsu j i i  
l )c i )ar tmenl ;  of In format ,  ion S(:icncc, Un ivers i ty  of Tokyo ,  
l tongo  7-3-1, Bunkyo-ku ,  Tokyo ,  113, Japan  
{tor i sawa, tsu j i i}0 is .o .u - ' cokyo .ac . jp  
Abstract 
This t)ai)er deseril)es techniques to com- 
pile lexical entries in I IPSG (Pollard and 
Sag, 1.987; Poll;ml and Sag, 1993)-style 
grammar into a set of finite state au- 
tomata. The states in automat~L are 
possible signs derived fl'om h',xical en- 
tries and contaili nformation raised fl'om 
the lexical entries. The automatt~ are 
augmented with feature structures use(l 
by a partial unification routine and de- 
layed/frozen definite el;rose programs. 
1 introduction 
Our aim is to 1)uild an e, fli(:ient and robust \]tl)SG - 
based parser. I IPSG has 1)(;eu re, gar(led as a so- 
phisticated but fl:;tgile and inettieient ff~mwwork. 
However, its principle-based al'(:hitecture nables 
a parser to handle real world texts only by giv- 
ing concise core grammar, including principles and 
templates for lexicM entries, defau l t  lexical en- 
tries(Horiguchi et al, 1995). The architecture is
different fl'om those of eonvelltional unification- 
l)t~sed ff)rmMisms which require hundreds of CFG 
skelet;ons to t)arse real world texl;s. 
However, tiles(', design prin(:il)les of l l I 'SG have 
draw-backs in parsing cost. That is, signs/feature 
structures corresponding ~o non-termillal symbols 
ill CFG become vi,sible, only after applying t)l'inci- 
pies a l ld  ~t t)&rs0r has  to  Cl'e~4te ~(;&tlIl;C st l ' I lCt l l leS 
()lie by one using unification. \]in addition, identity 
checking of non-terlninM symbols used to elimi- 
nate spurious signs must be replaced with sub- 
sumption checking, which flHther detcrior~tes ef- 
fi('ien(:y. 
Our grammar eompih',r COlnputes keleta.l 1)~rt 
of possible phrasal-signs froln individual h!xical 
enl;l'ies prior to parsing, and generates a set of 
finite state automata from h'~xical entries to ;tvoid 
the above draw-I)acks. We call this operation Oil- 
l ine ra is ing and an automaton thus generated is 
called a Lexieal  Ent ry  Automaton  (LA).  its 
states corresponds to 1)art of sigl,s and each tran- 
sition between stales (;orrespon(ls to at)plication of 
a rule schema,  which is a nonqexical comt)onent 
of grammar. 
Our parsing algorithm adopts a two-i)hased 
l/arsing method. 
Phase  1 \]iottom-up (:hart-like 1)~rsing with LAs. 
Ilewriting lt,ule: 
MOTIIEI{(\[1\]), IIEAI)-I)TR (\[81) NON-IIEAt)-I)TI~(\[S\]) 
FS: 
sign \ [ '  ...... 't t 
Sy l l  b;I I } ) C ~)a, 
content\[4\] 
scm iIKlices \[3\] 
, ...... ~ . \] 
1 ...... l-dr,, \[s\] '~Y" \[ .~,l,,:~,: ( ,~ rl> 
S(! l l l  i nd ices  
indices \] J 
goals arg2 2 
arg2 
fl '(!t!z(~ 
Figur(' 1 : AlL e.xalnph; of a rule s(:hema. 
Phase  2 Computing part of feature, structures 
which cannot l)e (:omputcd at (;oml)ile-tinm. 
We call tile tbature structures that are repre- 
sented as states in automa,t;~ mtd are COml)uted 
at conlpih>time Core -s t ruc tures ,  and the fca- 
tllre strllCtlll'es whi(;h are t;o l)e (:Olnl)ut, ed in Phase 
2, Sub-s t ruc tures .  In l)h~sc 1 parsing, t~ (:ore- 
si, ructme. (:orr(,spond to a state in an \] ,A. The cost 
of comt)uting sub-structures at Phase 2 is inin- 
imized by Dependency  Ana lys i s  mM Par t ia l  
Uni f icat ion.  
Tile next section describes rule schcmtm~, cen- 
tral eompouents of the. formalism, and gives ~ def- 
inition of Definite Clause Programs. Section 3 de- 
scribes how to obtain LAs h'om lexical entries and 
how to perform the Phase i p;~rsing. Section 4 ex- 
plMns the Phase 2 Parsing algorithm. A parsing 
exmnple is ln'es(;iLted in Section 5. The effective- 
ness of our method is exeinplified with a series (117 
eXl)eriments in Section 6. 
2 Rule Sch(:nmta and Definite 
Claus( ;  l ) rograms 
Our fonmdism has only one type of compolmnt 
g-tS llOll-10xic&l (;ollll)Ollellt,q of  ~ra l l l l nar ,  i.e., rule 
schemata .  I An example is showll in Figure 1. A 
ruh' s(:henl;~ consists of the following two items. 
Ih l  ()Ill' cilrr(!ilt, sysLeil/~ rill(', s(;h(2mltt~ l'('. goltc.r- 
i~t('.(1 froul principh,.s and rewriting rules ~m(:ording (;c) 
;L SlmCifical, ion given by ~ progr~mmter. 
949  
ru le (R)  a rewriting rule without specific syntac- 
tic categories; 
fs(R)  a feature structure. 
A characteristic of HPSG is in the flexibility 
of principles which demands complex operations, 
such as append or subtraction of list-value feature 
structures. In our formalism, those operations are 
treated by a Definite Clause Program. A DCP can 
be seen as a logic program language whose argu- 
ments are feature structures. An auxiliary term, a 
query to a DCP augmenting a rule schema, is em- 
bedded in a feature structure of a rule schema as 
the value of goals .  The rule schema in the exam- 
pie has an auxiliary term, append(\[1\], [2\], \[3\] ).
The bottom-up application of the rule schema 
R is carried out as follows. First, two daugh- 
ter signs are substituted to the HEAD-DTP~ position 
and NOR-HEAD-DTI~ position of the rewriting rule 
rule(R). Then, the signs are unified with the 
head-dt r  value and the non-head-dt r  value of 
the feature structure of the schema, fs(R) .  Fi- 
nally, the auxiliary term for DCPs given in the 
schema is evaluated? 
Our definition of a DCP has a more operational 
flavor than that given by Carpenter(Carpenter, 
1992)? The definition is crucial to capture the cor- 
rectness of our method. 2
Def in i t ion 1 (DCP)  A definite clause program 
(DCP) is a finite set of feature structures, each of 
which has the following form. 
goals (HI 1-13) J 
next -s teps  \[ goaSs (1~o, B1 , ' ' ' ,  ~ ,  \[1\] ) \] 
a where 0 <_ n and H, Bo," ? ?, B,~ are feature struc- 
tures. 
A feature structure of the above form corre- 
sponds to a clause in Prolog. H, B0 , . . . ,B~ cor- 
responds to literals in Prolog. H is the head and 
Bo," ' ,  B,~ are literals in the body of a clause. 
Def in i t ion 2 (Execut ion  of DCP)  Execution 
of a DCP P for the query, 
C2~e~y = \[ go~Ss (qo, q~,'" qz) \] 
is a sequence of unification, 
Query U rl U r2 U ... Urn  
where ri = \[ (next -s teps)  i-1 Ci \], C, ? P or 
Ci = \[ goa ls  0 \]. /f  the execution is terminated, 
C~ must be unifiable with\[ goa ls  () \]. In this 
case, we call the sequence (r l , ' " , r ,~} a reso lu -  
t ion  sequence .  
2Though, through the rest of the paper, we treat 
the definition as if it were used in an actual implemen- 
tation, the actual implementation uses a more efficient 
method whose output is equivalent with the result ob- 
tained by the defiifition. 
a (H0,.- ' ,  H~, \[1\]) is an abbreviation of 
res t  " ' res t  \[1\] 
E21\[ ? N ~,bcat 0 
My colleague 
\[ sig,! ~d \] 
S 2 maJ subcat ( 
'ql ms.) V 
subcat (\[2\]NP) 
ln~i sutJcat (~ 
m a j  V 
subcat (\[lJNI', 
\[2\]NP) 
wrote a good paper 
Figure 3: A parsing example 
(next -s teps)  i-1 \ [goals of QueryHrl Hr2 H. ? ? H 
ri represents the goals which are to be solved in 
the steps following the i-th step. The goals are 
instantiated by the steps fi'om the first one to i-th 
one, through structure sharings. The result of ex- 
ecution in a Prolog-like sense appears in the query, 
Figure 2 is an example of execution for the query 
append( \ [a \ ] ,  \[b\] ,X) , whose definition is based 
on a standard definition of append in Prolog. 
Given this definition of DCPs, an application 
of a rule schema to two (laughter signs D1 and 
D2 can be expressed in the following form, where 
@1, r2 , . " ,  r,~} is a resolution sequence: 
M= \[ head-dtrnon_head_dtr D~D2 \]Ufs(R)U','tUr.2U...Ur,~ 
3 Lex ica l  Ent ry  Automata  
This section presents a Lexical  Ent ry  Automa-  
ton  (LA).  The ineifieiency of parsing in HPSG 
is due to the fact that what kind of constituents 
phrasal-signs would become is invisible until the 
whole sequence of applications of rule schemata 
is completed. Consider the parse tree in Figure 
3. The phrasal-signs $1 and $2 are invisible until 
a parser creates the feature structures describing 
them, using expensive unification. 
Our parsing method avoids this on-line con- 
struction of phrasal-signs by computing skeletal 
part of parse trees prior to parsing. \[n Figure 
3, our compiler generates $1 and $2 only from 
the lexical entry "wrote," without specifying the 
non-head daughters indicated by the triangles in 
Figure 3. Since the non-head aughters are token- 
identical with subcat  values of the lexical entry 
for "wrote", the obtained skeletal parse tree con- 
tains the information that St takes a noun phrase 
as object and $2 selects another noun-phrase. 
Then unifying those non-head daughters with ac- 
tual signs constructed from input, parsing can be 
done. An LA expresses a set of such skeletal parse 
trees. A state in an LA corresponds to a phrasal- 
sign suc h as Sj and $2. They are called core-  
s t ruc tures .  A transition arc is a domination link 
between a phrasal-sign and its head daughter, and 
its condition for transition on input is a non-head 
950 
Qt le ry  : 
P l"Ogt'iUll: 
I,'~xecut io t : 
Q10 (J2 =: 
C; I =: goals 
llext-steps 
goals 
1 ..... 
/ i~l'g, 1 (} 
/ lll'g~ \[2\] I\[\]\] , 
L ,,,.ga \[21 m,,,l.~ \[J\]\] 
next -s teps  
. . . . . .  
.-,,.~a \[ (!~'q\]\[l\[~i) \]} 
argl 4 \[ e-list \] 
arg'3 6 
&rgl  ( : goals arg2 
(21 I I ( ;2 U \[ nca:t-.sL,ep.s C I \] =: 
( goals 
( '2  = 
next-steps 
l\[7\]\[ (,-list 1} 
\[ ;~, \[\[{!\[)e list \]) 
\[ (2 31\[8\] \](\[q\[ ,, })) 
a,'g2 r , (f:~t\[ \ 4 \ ] ) \ [ \ [7 \ ]  
gl 'ga ( \ [3 J l \ [~})  
goals arg2 4 i\[r\] 
re'g3 
I\[r)\[ e-list \ ] )  
next -s teps  
/ ,u~J 4 \[ ,<i.~t \] goals / arg2 5 b 
.... xt-steps goals <) \] 
I\[r\]\[ ,!-li,~t \ ] )  \] 
Figure 2: An examl)le of DCP 's  execution 
daughter, such as signs tagged \[1\] and \[2\] in Fig- 
ure 3. Kasper c.t al. 1)resented an idea similar to 
this @l ine  raising in their work on HPSG-TAG 
compiler(Kasper et al, 1995). The difference, is 
that our algorithm is based ou substitution, not 
adjoining, Furthermore, it is not clear in their 
work how offl ine raising is used to improve ef\[i- 
cicncy of parsing. 
Before giving the definition of LAs, we detine 
the notion of a quasi-sign, which is part of a sign 
and constitutes l~As. 
Def in i t ion  3 (quas i - s ign(n) )  For a given inte- 
ger n, a fcatu,e structure S is a q'aasi-sign(n) 
if it has some of tile following four attributes: 
syn, sem, head-dt r ,non-head-dt r  and does 
not /Lave values for the paths (head-dt r  + 
non-head-dtr)"". 
A qua,.si-sign('n) cannot rel)resent a parse tree 
whose height is inore than n, while a sign can 
express a parse tree with any height. Tlm)ugh the 
rest of this 1)aper, we often extract a quasi-sig"n.(n) 
S from a sign or a quasi-sig',,(,n/) S' where '., < 
n'. This operation is denote(l by S' = c'x(S',,n). 
This means that 5' is equivMent to S' except ff)r 
the attr ibutes head-dt r  mM non-head-dt r  whose 
root is the (head-dtr + non-head-dtr)  '~ value in 
S'. Note that S and S' are completely different 
entities. In other words, S and S' pose different 
scopes on structure sharing tags, in addition, we 
also extract a feature structure F reached by a 
path or an attr ibute 1) in a feature structure IP'. 
We denote this by F = va l (F ' ,p )  and regard F 
and F'  as different entities. 
Def in i t ion 4 (Lex ica l  Ent ry  Auton,  a ton(LA) )  
A Lezical Entry Automaton is a tuplc (Q,A,qo} 
whel'e~ 
Q:  a set of states, where a .state is a 
quasi-sign(O). 
A : a ,set of transition arcs between states, where 
a transition arc is a tuple (qd, q .... N ,D ,R)  
where qd, q,. 6 Q, N is a quasi-sign(O), D is 
a quasi -s ign( I )  and R is a rule schema. 
qo : tile initial state, which corresponds to a lezi- 
cal erLtry. 
In a transition :-tt'(; < qd, q ..... N, D, 1~} , q,~ denotes 
the destination of the transition arc, and qd is the 
root of the arc. The N is a non-head daughter 
of a l)hrasal-sign, i.e., the destination state of the 
transition, and expresses the input condition for 
the transition. The D is used to represe, nt: the de- 
pendency 1)etween the nn)ther sign and the daugh- 
ters through structure sharings. This is called a 
Dependency  Feature  S t rueture(DFS)  of the 
transition arc, the role of which will be discussed 
in Section 4. 1~, is the rule schema used to create 
this arc. 
An LA is generated fl'om a lexieal entry l by the 
following recursive pro(:edure: 
1. Let; ,~; 1)e {/}, A be an eml)ty set and sd = / 
2. For ea(:h rule, schema 1~, and for each of its 
ea(:h resolution sequence (rl , . . . , 'r ,~} obtain, 
1) - \[ head-dt r  ,Sd \] 
uf.s(l~) u r, u .-. o r,~ 
and if l) is a 
feature structure, obtain s,, = ex(D,O) and 
N = ex(w~l(D, non-head-dt r ) ,  0). 
a. If D is a t~ature structure, 
? If the, re is a state s~,~ 6 S such that s',~ 
,s .... 4 let s,~ be s~,~. Otherwise, add s,,~ 
to 5". 
* If there is no T'r = \'/~"d, '~,,~"', N" ,  D",  1~) 
A such that .%~ ~ s{',~, s,z ~ sSl, N 
4For ~my feature structures f ~md f' ,  f ~ f '  iff 
f E f '~md f '  E f 
951 
Phase2-proc-dcp(e : dge); 
assume  = (1, r, S, Dep) 
return S U sub-structure(e) 
sub-structure(e : edge); 
assmne e = (l, r, S, Dep) 
If Dep = 4) 
then return sub(S), 
else 
for each (D, eh, e~, R) C Dep, 
assume that el~ = (lh, rh, Sh, Deph) 
and e~ : (In, r~, Sn, Dep,~) 
Sh := sub-structure(eh), 
S,~ := Sn U sub-strueture(e~) 
If neither of Sh and Sn is n i l ,  
s%bo :~ 
fv(dep(D) u sub(fs(R)), 
\[ head-dt r  Sh \] 
non-head-dtr S n ' 
rs) 
............................... (A)
for each resolution sequence 
,rd, 
sub := .sub0 LIT 1 I~ ? ? ? U Ti 
............................... (B)
If sub is not a feature structure or 
either of Sh or S~ is n i l ,  
then return n i l  
else return sub 
Figure 4: A recursive procedure for tile Phase 2 
N"  and D ~ D", then, add the tuple 
(s,t, s,,~, N, D, R) to A. 
4. If the new quasi-sign(O) (s,~) was added to 
S in the previous tep, let sd be s,~ and go to 
Step 2. 
When this terminates, (S, A, l) is the LA for 1. 
The major difference of Step 2 and the 
normal application of a rule schema is that 
non-head-dt r  values are not specified in Step 2. 
In spite of this underspecification, certain parts 
of the non-head-dt r  are instantiated because 
they are token-identicM with certain values of the 
head-d%r domain. By unifying non-head-dt r  
values with actual signs to be constructed fl'om in- 
put sentences, a parser can obtain parsing results. 
For more intuitive explanation, see (Torisawa and 
Tsujii, 1996). 
However, this simple LA generation algorithm 
has a termination problem. There are two poten- 
tial causes of non-termination. The first is the 
generative capacity of a feature structure of a rule 
schema, i.e., a rule schema can generate infinite 
variety of signs. The second is non-termination of
the execution of DCP in Step 2 because of lack of 
concrete non-head daughters. 
For the first case, consider a rule schema with 
the following feature structure. 
head-dt r  syn  \[ counter  \[1\] \] \] 
Then, this can generate an infinite se- 
quence of signs, each of which contains a part, 
\[ counter  <bar, ba, r , . . . ,bar)  l and is not equiv- 
alent to any previously generated sign. In order 
to resolve this difficulty, we apply tim res t r i c t ion  
(Shieber, 1985) to a rule schemata nd a lexical 
entry, and split the feature structure F = fs (R)  
of a rule schema R or a lexical entry F = l, 
into two, namely, core(F) and sub(F) such that 
F = core(F) U sub(F). The definition of the re- 
striction here is given as follows. 
Definition 5 (paths)  For arty node n in a fea- 
ture structure F, paths(n,F)  is a set of all the 
paths that reaches n from the root of F. 
Definition 6 (Restriction Schema) A 
restriction schema rs is a set of paths. 
Definition 7 (Res)  F '  = Res(F, rs) is a ma.~;i- 
real feature structure such that each node n in F ~ 
satisfies the following conditions. 
? The~ is a node no in f: such that 
paths(no,F) = path.s(n,F') and type('n) = 
t?tpe(no). 
? For any p C paths('n,F'), there is no path 
p,, 6 rs which prefixes p. 
Res eliminates the feature structure nodes 
which is specified by a restriction schema. For a 
certMn given restriction schema rs, eore(fs(l~,)) -= 
Res( fs (R) , rs )  and sub(fs(R)) is a mini- 
mM feature structure such that core( fs (R) )U 
sub(fs(R)) = fs(R) .  Tile nodes eliminated by 
Res must appear in sub(fs(R)).  In tile example, 
if we add (syn, counter} to a restriction schema 
and replace f s (R)  with eorc(fs(.R)) in the Mgo- 
rithm for generating LAs, the termination prob- 
lenl does not occur because LAs can contain a loop 
and equivMent signs are reduced to one state in 
LAs. The sub(fs(R)) contains the syn lcounter ,  
and the value is treated at Phase 2. 
The other problem, i.e., termination of DCPs, 
often occurs because of underspecification f the 
nork-head-dtr wines. Consider the rule schema 
in F igure 1. The append does not terminate at 
Phase 2 because the ind ices  value of non-head 
(laughters is \[ ? \]. (Consider the case of execut- 
ing append(X, (b),Y) in Prolog.) We introduce 
the .freeze Nnctor in Prolog which delays the 
evaluation of the second argument of the func- 
tors if the first arguruent is not instantiated. For 
instance, f reeze  (X, append(X, \ [b\] ,  Z) ) means to 
delay the ewfluation of append until X is instan- 
tinted. We introduce the functor in the following 
forln. 
goals arg2 (fl 
arg3 \ [~ 
freeze \] 
This means the resolution of this query is not 
performed if \[1\] is \[?\]. The delayed evaluation 
is considered later when tile non-head-dt r  val- 
ues are instantiated by an actual sign. Note that 
this change does not affect the discussion on the 
correctness of our parsing method, because the 
difference can be seen as only changes of order of 
unification. 
Now, tile two phases of our parsing algorithm 
can be described in more detail. 
Phase  1 : Enumerate possible parses or edges in 
a chart only with unifiability checking in a 
bottom-up chart-parsing like manner. 
952 
Phase  2 : For comt)leted parse trees, compute 
sub-structures by DFSs, ,sub(fs(R))  for each 
schema R and frozen 1)C1 ) programs. 
Note that, in \['has(; 1, unification is replaced 
with nnifiability checking, which is more efficient 
than unification in terlns of space an(l time. The 
intended side effect by unification, such as build- 
ing up logical forms in sere va lues ,  is COmlntted 
at Phase 2 only for the parse trees covering the 
whole input. 
a.1 Phase  1 Pars ing  
The Phase~ 1parsing algorithm is quite similar to a 
bottom-up chart parsing for CFG. The Mgorithm 
has a chart and edges. 
Def in i t ion 8 (edge)  An edge is a tupla 
(1, r ,  S, l)ep) where, 
? 1 and r arc. vertexes in the chart. 
? S is a slate of an LA. 
? .l)ep i.s a .set of tuples in the form of 
(D, eh, c,,, ll} wh, e, rc. eh a7%d Cn aTY; (:dges, \]) 
i.s a quasi-.sign(I) and R is a rule .schema. 
The intuition behind this definition is, 
? ?' l)lays the role of a non-/termimd in CFG, 
though it is actually a quasi-sign(O). 
? ch and e,~ denote a head daughter edge and a 
non-head daughter edge, respectively. 
? Dep represents the dependency of an 
edge and its daughter edges. Where 
(D, eh,c,~,l~} E Dcp, D is a DIeS of a tran- 
sition arc. Basi(:ally, Phase 1 parsing creates 
these tuples, and \])hase 2 parsing uses them. 
The Phase 1 parsing (:onsists of the folh)wing 
steps. Assume that a word in i nput  \]n~s a lexical 
entry L~ and that an LA (Q,;,A,,q~) generated 
fi'om Li is attached to the word: 
1. Create an edge li -= (j.i,ji + 1,q~,()) in the 
chart for each Li, for at)propriate .ji. 
2. For an edge e. 1 whose state is q~ in the chart, 
pick u t) an edge e2 which is adjacent to el 
and whose state is q~. 
3. For a transition arc (ql, q, N, D, ll), check if 
N is unifiable with q2. 
4. If the unifiability check is successful, find an 
edge (l = ('m,d,'n,d,q, Depd) strictly covering 
el and e2. 
5. if there is, replace d with a new edge 
(m,,,'na,q, Dep,z U {(D,c, ,eu,B)})  it) the. 
chart. 
6. Otherwise, create 
a new edge (Tn, n, q, {(D, el, e2, R)}) strictly 
covering el and e2. 
7. Go to steI) 2. 
4 Phase  2 Pars ing  
The algorithnl of Phase 2 parsing is given in 
Figure 4. The procedure sub-.structure is a re- 
cursive 1)rocedure which takes an edge as in- 
put and builds Ul) sub-structures, which is dif- 
fer'ential feature structures representing modifica- 
tions to core-structures, in a bottoln-U 1) nlanner. 
The obtained sub-structures are unified with core- 
structures when 1) the input edge covers a whole 
input or 2) the edge is a non-head daughter edge 
of sonm other edge. Note that the .~ub-struet'are 
treats sub( fs (R) ) ,  a feature structure eliminated 
l)y the restriction in the generation of LAs, (the 
(A) 1)art in Figure 4) and frozen goals of DCPs, 
by additional ewduation of DCPs. (the (B) part) 
Here, we use two techniques: ()tie is dependency 
analysis which is eml)odied by the function dep in 
Figure 4. The other is a partiM unification routine 
expressed by p_nn i fy  in the figure. 
The del)endency analysis is represented with 
the function, dep(F,'rs), where F is a DFS and 
rs is a restriction schema used in generation of 
LAs: 
Def in i t ion 9 (dep)  For a feature structure \["' 
and the. restriction schema r.s, F = dep(l  c~,r,s) 
is a maximal fc.atu're~ structure such O~,at any 'node 
'n in F sati,~fies the conjunction of th, e. following 
two conditions: 
t. There is a node n' in f i'' ,such, that 
v(tm.+, . ,  P)  - ~), ,m.,+, , ' ,F ' )  a,.Z t:,mc.(7,0 := 
typc(n').  
2. Where A) ha. = 'n or B) n,t is a descendan? 
of n, pa, ths(n,z,F)  contains a path. prefixed 
by one of (head-dtr), (non-head-dtr) and 
<goa:ts>. 
3. The diajunetion of the following three condi- 
tions is satisfied where A)  n,t = n or B) 'n(t 
is a descendant of n. 
? For .some p G pa, th, s(7t~l,F), there  i.s a 
path, p,,. E 'rs wh, ieh prefixes p. 
? Some p ~ p.,th,@n,t,F) is prefixed by 
(~m.,ls). 
? 7'here is no node 'n. in F .~'uch th, at 
i) there is paths Pi,7)'2 ~ paths('n<,., f;') 
such that Pi is prefixed by (syn) 07' 
(sere) aTtd P2 is 'p'r'efi;Le.d by (head-dtr) 
Or (non-head-dtr>, and i/) for a~ty p G 
paths(rid, F)  there is p,~ E path..s(n,~, F) 
which prefixes p. 
Roughly, dep eliminates 1) the descendant 
nodes of the node which apl)ears both in syn/sem 
domains and head-dt r /non-head-dt r  domains 
and 2) the nodes at)peering only in syn/sem do- 
mains, excet)t for the node which el)pears in 
s'ab(fs(\]?)) or goa ls  domains. In other words, 
it removes the feature structures that have I)een 
already raised to core-structures or other DFSs, 
ex(:ept for the structure sharings, and leaves those 
which will be required by DCPs or xub( fs (R) ) .  
p_uni f y( Fl , F.2 , r s ) is a partial unification rou- 
tine where Fl and F2 are feature structures, and 
rs is a restriction schema used in generation of 
LAs. l{oughly, it performs unification of F, and 
l'12 only for common part of Ft, F.2, and it pro- 
duces unified results only for the node 'n in Fl if 
s'nj is ~t descendant of 'n2 in a feaiure structur{~ l,'
i l l  'nt # n2, and the.r('. ~u:e paths  Pl 6 path, s(~,,l, \ [" )  ~Hld 
I)'2 E pa, th, s(n.2, l"),  nnd p2 l)r('.fixes p l .  
953 
phon "wrote" 
syn , o rv , \ ]  .... .... subcat <NI'\[1\],NP\[2\]) 
rein wrote sere content agent 
object 
indices 0 
Figure 5: A lexical entry for "wrote" 
$2 ? A State 
P b  T :N  T2:N S1 A Transition Arc 
T I :NP  (N denotes  
L a non-head-dtr.) 
Figure 6: The LA derived from "wrote" 
n has a counter part in F~. More precisely, it pro- 
duces the unification results for a nod(; n in Fj 
such that 
? there is a path p ~ paths(n, I~) such that the 
node reached by 1) is also defined in F2, or 
? there is a path p ~ paths(n, F1) prefixed by 
some p,, C rs or (goals). 
Note that a node is unified if its structure- 
shared part has a counter-I)art in F2. Intuitively, 
the routing produces unified results for the part of 
Fi instantiated by /7'2. The other part, that is not 
produced by p_unify, is not required at Phase 2 
because it is already computed in a state or DFSs 
in LAs when the LAs are generated. Then, a sign 
can be obtained by unifying a sub-structure and 
the corresponding core-structure. 
5 Example  
This section describes the parsing process of the 
sentence "My colleague wrote a good paper." The 
LA generated fronl the lexical entry for "wrote" 
in Figure 5 is given in Figure 6. The transition arc 
T1 between the states L and S1 is generated by 
the rule schema in Figure 1. Note thai; the query 
to DCP, freeze(\[1\], append(Ill, \[2\], \[3\])), is used to 
obtain union of indices values of daughters and the 
result is written to the indices values of the mother 
sign. During the generation of the transition arc, 
since the first argument of the query is \[ ? \], it is 
frozen. The core-structures arid the dependency- 
analyzed DFSs that augment he LA are shown 
in Figure 7. We assume that we do not use any 
restriction, i.e., for any lexical entry l and rule 
schenaata 2~, s,bb(1) ~-\[?1 and sub(fs(I{)) = \[?1. 
Note that, in the DFSs, the already raised fea- 
ture structures are eliminated and, that the DFS 
of the transition arc T contains the frozen query 
as the goals .  
Assmne that the noun phrases "My colleague" 
and '% good paper" are already recognized by a 
parser. At phase 1, they are checked if they are 
unifiable to the condition of transition arcs T1 and 
T2, i.e., the NPs which are non-head daughters 
$2 
syn 
senl 
5'1 
syn 
head \[ ..... ior V \] \] 
subcat 0 \] 
rehl wrote content agent ? 
object ? 
indices ? 
head \[ major V \] \] 
subcat (NP\[2\]) 
rein wrote senl content agent \] 
object 
indices A_ 
The dependency-anMyzed DFS of T2 
syn \[~ ..... l \[s\] \] 
seln 
indices \[a\] 
- synSign 1 \] .... \] 
h~ad-dtr \[a\] s~beat .\[qN;'\[r\]) 
Z 
\ [}  t f l l la jor  ..... syn subcttt ) non-head-dtr \[5\] 
sere indices 
goals argl 
arg2 
arg2 
The dependency-analyzed DFS of T1 
head 8\] 
syn subcat {/10\]?\[9\]}  
content \[6\] agent sere object 
indices \[3\] 
? sign 
sy 
1 ..... I-dtr \[3\] 9\]) 
..... \[ ic'::~itceet: t \[2 t (, \] 
syn \[ t subcat ) non-head-dtr \[5\] 
sere indices 1 ..... \]> freeze 1 
goals argl 
a,-~2 () 
arg2 
Nll\] 
N,I\] 
Figure 7: States and DFSs in tim LA in Figure 6 
The sub-structure for $2 
content | agen~ 1\]my_colleaque | 
..... \[ obje,:t \[2\]good_p.vJ4' J 
indices {\[llmy_collea.o . .. \[2\]good_paper) 
The sub-structure for S1 
sere \[ object \[1\]good_paper \] 
indices (\[1\]good_paper) 
The goals,head-dtr,non-head- dtr vMues are omitted. 
Figure 8: The sub-structures obtained in the pars- 
ing 
954 
__Parsing ~ i thn l  
Phase 1 only 
Pheuse 1 & Phase 2 
Phase 1 & Phase 2 
~ f - ~  
naive application of rule sche, mata 
naive application of rule s<:hentata 
~\['ylTe. -6f sent;ences 
(# of sentences) 
glT 70  - - - -  
<ufly successful~7~) 
enD. s ~ s ~  
Ass) 
onTy ~fuT- -  - 
_ (~)_  . . . . . . .  
19.2 
19.2 
18.8 
17.13 
3:\[.4 
Av 3n< e& 
1.25 ~L121_ 
3.00 ~1.65) 
85.09 
1093.22 ~2.1~ 
A bracketed t ime indicates non-(~(\] execution tim(< '\]'he Xl)erimeuts was l)(nformed on SparcStat ion 20 with 128 MI) I IAM 
Figure 9: Ext>eriments on a Japanese newsl)aper(Asahi Shinl)un) 
<)f $1 and $2. Since all l;he u,dfial)ility <:lwx:k- 
ings ,'/.1"o successful, Phase 1 parsing produ(:es the 
parse tree whose form is presented in Figure 3. 
The Phase 2 1)arsing produces the sub-structures 
in Figure 8. Note that the frozen goals are eval- 
uated and the ind ices  wdues have al)prot)riate 
values. A l)arsing result is obtaine{l by unifying 
the sub-structure for 5"2 with tim correspon<ling 
core -s t ruc t l l re .  
The amount of the feature stru<:ture nodes gen- 
erate(1 during t)arsing are r(~<lu(:e(1 :<m~t>are(l to 
the case of the naive at)l)lication of rule schemata 
presented in Section 2. The important point is 
that they contMn only either the part iu the 
DFSs that was instantiated by head daughters' 
sub-structures, and non-head daughters' core- 
structures and sub-structures, or the part that 
contributes to the DCP's exaluation. The feature 
structure that does not al)pear i ,  a sub-structure 
appears in the corresponding core-structure. Se, e 
Figure 7. Because of these 1)rot>erties, the correct- 
ness of our parsing nmthod is guaranteed. ('lbri- 
sawa and Tsujii, 1996). 
7 Conc lus ion  
We have lu'esented a two-phased t)arsing nlethod 
tor HPSG. In the first l)hase,, our 1)arser pro- 
duces parse trees using Lexical Entry Automnta 
compilcxl from lexical entries, in the second 
phase, only the feature structures whi<:h luust \])e 
(:ompute(\[ dynamically are (:omputed. As a re- 
suit, amount of the fl;ature structures unifie<l at 
1)arsing-time is reduce.d. We also showed the el'- 
feet of our optinfization te(:hniques by a series of 
exl)erinwats <m a real world text. 
\]t can l)e noticed that ea<:h transition arc of tim 
cOral)ileal l,As can be seen as a rewriting rule in 
CFG (or a dott;ed notation in a chart parser.) We 
belie.ve this can Ol)en the way to integrate severaJ 
n,et;hods deveh)l>ed for CI,'G, including the inside- 
outside algorithm tot grmmnar learning or disam 
biguation, into an HPSC, framework. We also 1)e- 
lieve that, by pursuing this direction for optimiz- 
ing ttl)SG parsers, we can reach the point whe.re 
grammar learning from corl)ora can be done with 
concise, and linguistically well-defined (:ore grant- 
Itt;tr.  
6 Ex I )e r iments  
We have implenmnted our parsing metho<l in 
Common Lisp Ol)je<:t Systen~. hnprovenmnt by 
our method has /)een measured on 70 ra.ndonfly 
selected Japanese sentences from a newsl)at)er 
(Asahi Shinbun). The used grammar (',onsists of 
just 5 rule schemata, which are generated fl'om 
principles and rewriting rules, aim 55 default lex- 
ical entries given for each part of speech, with 44 
manually tailored lexical entries. The total num- 
ber of states in the LAs compiled fl'oln them was 
1490. The grammar does not have a semantic 
part. The results arc. l)resented in Figure 9. Our 
grammar produ<:ed l>ossil)le parse trees for 43 sen- 
ten<'.es (61.4%). We compared the. execution time 
of our I)arsing method and a more naive algorithm, 
which l)erforms Phase 1 parsing with LAs and al)- 
plys rule s(:hemata to (:olnph'.ted pars<; trees in the 
naive way described in Se<:tion 2. As the. naive al- 
gorithm caused thrashing for storage in GC, it is 
pointless to compare those tigures simply. How- 
ever, it is obvious that our method is much fi~ster 
than the naive one. We could not measure the ex- 
ecution time for a totally naive algorithm which 
t)uilds parse trees without LAs because of Uwash- 
ing. 
References  
Bob Carl>enter. 1992. The Looi<: of "/\]qped F.a,t..,.+'. 
Str'ucturcs. Cambridge University Press. 
Keiko Horiguchi, Kentaro Torisawa, and Jun'ichi 
Tsujii. 1995. Automatic acquisition of cont(;nt 
words using an IIPSG.-based parser. In NL- 
1"1~S'95. 
Robert Kasper, Bernd Kiefer, Klaus Netter, and 
K. Vijay-Shanker. 1995. Compilation of I IPSG 
to TAG. In ACL 95. 
Carl Pollard and Ivan A. Sag. 1987. h~,fovmatio,,- 
Based Syntaz and Semau, ties Vol. 1. CSLI lec- 
ture notes  11o.1 3. 
(,'arl Pollard and Ivan A. Sag. 1993. lh..a.d- 
Driv<'.n Phrase Structure Grammar. University 
of Chicago Press an(l CSLI l)ul)li<:ations. 
Stuart C. Shieber. 1985. Using restri<:tion to 
extend I)arsing algorithms for conq)lex feature 
based formalisms. In A CL85. 
Kentaro Torisawa and Jun'ichi Tsujii. 1996. ()if- 
line raising, dei)endency analysis an{l l>artial 
unifieat;ion. In Third Iu, ternational Conference 
on HPSG. In the pr<)ceedings of TALN '96. 
955 
