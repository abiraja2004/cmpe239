Study of Practical Effectiveness for Machine Translation Using
Recursive Chain-link-type Learning
Hiroshi Echizen-ya
Dept. of Electronics and Information
Hokkai-Gakuen University
S 26-Jo, W 11-Chome, Chuo-ku
Sapporo, 064-0926 Japan
echi@eli.hokkai-s-u.ac.jp
Kenji Araki
Division of Electronics and Information
Hokkaido University
N 13-Jo, W 8-Chome, Kita-ku
Sapporo, 060-8628 Japan
araki@media.eng.hokudai.ac.jp
Yoshio Momouchi
Dept. of Electronics and Information
Hokkai-Gakuen University
S 26-Jo, W 11-Chome, Chuo-ku
Sapporo, 064-0926 Japan
momouchi@eli.hokkai-s-u.ac.jp
Koji Tochinai
Division of Business Administration
Hokkai-Gakuen University
4-Chome, Asahi-machi, Toyohira-ku
Sapporo, 060-8790 Japan
tochinai@econ.hokkai-s-u.ac.jp
Abstract
A number of machine translation systems based
on the learning algorithms are presented. These
methods acquire translation rules from pairs
of similar sentences in a bilingual text cor-
pora. This means that it is difficult for the
systems to acquire the translation rules from
sparse data. As a result, these methods require
large amounts of training data in order to ac-
quire high-quality translation rules. To over-
come this problem, we propose a method of ma-
chine translation using a Recursive Chain-link-
type Learning. In our new method, the system
can acquire many new high-quality translation
rules from sparse translation examples based on
already acquired translation rules. Therefore,
acquisition of new translation rules results in
the generation of more new translation rules.
Such a process of acquisition of translation rules
is like a linked chain. From the results of evalua-
tion experiments, we confirmed the effectiveness
of Recursive Chain-link-type Learning.
1 Introduction
Rule-Based Machine Translation(MT)(Hutchins
and Somers, 1992) requires large-scale knowl-
edge to analyze both source language(SL)
sentences and target language(TL) sentences.
Moreover, it is difficult for a developer to com-
pletely describe large-scale knowledge that can
analyze various linguistic phenomena. There-
fore, Rule-Based MT is time-consuming and
expensive. Statistical MT and Example-Based
MT have been proposed to overcome the dif-
ficulties of Rule-Based MT. These approaches
correspond to Corpus-Based approach. Corpus-
Based approach uses translation examples that
keep including linguistic knowledge. This
means that the system can improve the quality
of its translation only by adding new translation
examples. However, in Statistical MT(Brown
et al, 1990), large amounts of translation
examples are required in order to obtain
high-quality translation. Moreover, Example-
Based MT(Sato and Nagao, 1990; Watanabe
and Takeda, 1998; Brown, 2001; Carl, 2001)
which relies on various knowledge resources
results in the same difficulties as Rule-Based
MT. Therefore, Example-Based MT, which
automatically acquires the translation rules
from only bilingual text corpora, is very effec-
tive. However, existing Example-Based MT
systems using the learning algorithms require
large amounts of translation pairs to acquire
high-quality translation rules.
In Example-Based MT based on analogical
reasoning(Malavazos, 2000; Guvenir, 1998), the
different parts are replaced by variables to gen-
eralize translation examples as shown in (1) of
Figure 1. However, the number of different
parts of the two SL sentences must be same
as the number of different parts of the two TL
sentences. This means that the condition of ac-
quisition of translation rules is very strict be-
cause this method allows only n:n mappings in
the number of the different parts between the
SL sentences and the TL sentences. As a re-
sult, many translation rules cannot be acquired.
(McTait, 2001) generalizes both the different
parts and the common parts as shown in Fig-
ure 1(2). This means that (McTait, 2001) al-
lows m:n mappings in the number of the differ-
ent parts, or the number of the common parts.
However, it is difficult to acquire the translation
rules that correspond to the lexicon level. On
the other hand, we have proposed a method of
Machine Translation using Inductive Learning
with Genetic Algorithms(GA-ILMT)(Echizen-
ya et al, 1996). This method automatically
generates the similar translation examples from
only given translation examples by applying ge-
netic algorithms(Goldberg, 1989) as shown in
(3a) of Figure 1. Moreover, the system per-
forms Inductive Learning. By using Inductive
Learning, the abstract translation rules are ac-
quired by performing phased extraction of dif-
ferent parts as shown in Figure 1(3b) and (3c).
In all methods shown in Figure 1, the condi-
tion of acquisition of translation rules is that
two similar translation examples must exist. As
a result, the systems require large amounts of
translation examples.
We propose a method of MT using Recur-
sive Chain-link-type Learning as a method to
overcome the above problem. In our method,
the system acquires new translation rules from
sparse data using other already acquired trans-
lation rules. For example, first, translation
rule B is acquired by using translation rule A
when the translation rule A exists in the dictio-
nary. Moreover, translation rule C is acquired
by using the translation rule B. Such a pro-
cess of acquisition of translation rules is like a
chain where each ring is linked. Therefore, we
call this mechanism Recursive Chain-link-type
Learning(RCL). This method can effectively ac-
quire many translation rules from sparse data
without depending on the different parts of sim-
ilar translation pairs. In this paper, we describe
the effectiveness of RCL through evaluation ex-
periments.
2 Basic Idea
RCL is a method with an ability that automat-
ically acquires translation knowledge in a com-
puter without any analytical knowledge, such as
GA-ILMT. This is the ability to extract corre-
	
		 	

	

 	 !"#$ "$	

 	 !	"#$ "$
%	
		
!" 	#&!" 
'

	 '

!'

"#$"$'

(	
				)*	(++
(	

		 	&,
	 	&,
(%	
		
-	-&,! -	%		 -
(	
		
* - ,
	-! .	-
,
	
/&-	


	0-
123.)*4 2#	 	
56
! 	 	%		  
! 		 	%		 
! 7& -& 


8$ 	59:; ;<=;>;?@;ABC
8D   59:E; ;FGH;>;?@;ABC
1		*	
	4I	,
%#,,
#
 
8$
59:;J;FGH;>;?@;ABC
8D 
	59:E;J;<=;>;?@;ABC
1%	
		4I		%#3&.	
8$
59:;J;;>;?@;ABC
8$
59:;J; ;>;?@;ABC
8$
K+59:;J;K+;>;?@;ABC
1)	
		4I		%#
8! 
K+59";J;K+;>;?@;ABC
8# 
K+59"$;J;K+;>;?@;ABC
8K
K+59K;J;K+;>;?@;ABC
*	
	4I	,

	*	
	4I	,

89FGHLM8	9<=L
8$9:LM8D 9:EL
3&.	
Figure 1: Previous works.
sponding parts from pairs of objects with which
it corresponds. In this paper, we apply this abil-
ity to a translation example that consists of SL
and TL sentences. A system with RCL can ac-
quire translation rules from sparse translation
examples. Figure 2 shows how translation rules
are acquired using this method1.
Figure 2 shows the process where translation
rules B, C and D are acquired one after another
using RCL. In this paper, source parts are those
parts that are extracted from the SL sentences
of translation examples, and target parts are
those parts that are extracted from the TL sen-
tences of translation examples. Moreover, part
translation rules are pairs of source parts and
1In Figure 2, the use of a Greek character means that
all language characters correspond to unknown character
strings for a computer.



























	








































	
	
	
	




























	
	
	
	




































	











































 


!
	


"




#
$
%






















	
	
	
	
























	
	
	
	
























&
'
(
)
*
+
(
,
-
.
)
/
0
1
(
2
3
+
0
/
4
.
	
5
Pro
cess
&
'
(
)
*
+
(
,
-
.
)
/
0
1
(
2
3
+
0
/
4
.
	
6
&
'
(
)
*
+
(
,
-
.
)
/
0
1
(
2
3
+
0
/
4
.
	
7
4
.
	
5
Pro
cess
4
.
	
6
Pro
cess
4
.
	
7
 
!
"
#
$
#
"
!
%
&
'
!
#
(
)
%
$
"
*
'
+
$
,
 
!
"
#
$
#
"
!
%
&
'
!
#
(
)
%
$
"
*
'
+
$
,
 
!
"
#
$
#
"
!
%
&
'
!
#
(
)
%
$
"
*
'
+
$
,
 
!
"
#
$
#
"
!
%
&
'
!
#
(
)
%
$
"
*
'
+
$
,
-
+
%
#
+
%
.
+
$
#
"
!
%
&
'
!
#
(
)
%
$
$
$
$
"
*
'
+
$
/
-
+
%
#
+
%
.
+
$
#
"
!
%
&
'
!
#
(
)
%
$
$
$
$
"
*
'
+
$
/
-
+
%
#
+
%
.
+
$
#
"
!
%
&
'
!
#
(
)
%
$
$
$
$
"
*
'
+
$
/
-
+
%
#
+
%
.
+
$
#
"
!
%
&
'
!
#
(
)
%
$
$
$
$
"
*
'
+
$
/
 
!
"
#
$
#
"
!
%
&
'
!
#
(
)
%
$
"
*
'
+
$
0
 
!
"
#
$
#
"
!
%
&
'
!
#
(
)
%
$
"
*
'
+
$
0
 
!
"
#
$
#
"
!
%
&
'
!
#
(
)
%
$
"
*
'
+
$
0
 
!
"
#
$
#
"
!
%
&
'
!
#
(
)
%
$
"
*
'
+
$
0
-
+
%
#
+
%
.
+
$
#
"
!
%
&
'
!
#
(
)
%
$
"
*
'
+
$
1
-
+
%
#
+
%
.
+
$
#
"
!
%
&
'
!
#
(
)
%
$
"
*
'
+
$
1
-
+
%
#
+
%
.
+
$
#
"
!
%
&
'
!
#
(
)
%
$
"
*
'
+
$
1
-
+
%
#
+
%
.
+
$
#
"
!
%
&
'
!
#
(
)
%
$
"
*
'
+
$
1




-
)
*
"
.
+
$
2
!
"
#
-
)
*
"
.
+
$
2
!
"
#
-
)
*
"
.
+
$
2
!
"
#
-
)
*
"
.
+
$
2
!
"
#




3
!
"
4
+
#
$
2
!
"
#
3
!
"
4
+
#
$
2
!
"
#
3
!
"
4
+
#
$
2
!
"
#
3
!
"
4
+
#
$
2
!
"
#




3
"
!
%
&
'
!
#
(
)
%
$
"
*
'
+
3
"
!
%
&
'
!
#
(
)
%
$
"
*
'
+
3
"
!
%
&
'
!
#
(
)
%
$
"
*
'
+
3
"
!
%
&
'
!
#
(
)
%
$
"
*
'
+
F
igure
2:
Schem
a
in
process
of
acquisition
of
translation
rules
using
R
C
L
.
target
parts,
extracted
as
parts
like
translation
rules
A
and
C
in
F
igure
2.
Sentence
translation
rules
are
pairs
of
source
and
target
parts
ex-
tracted
as
sentences
like
translation
rules
B
and
D
in
F
igure
2.
O
n
the
other
hand,
translation
rules
that
are
used
as
starting
p
oints
in
the
ac-
quisition
process
of
translation
rules,
like
trans-
lation
rule
A
in
F
igure
2,
are
acquired
by
using
G
A
-IL
M
T
.
T
he
reason
b
eing
that
the
system
can
p
erform
translation
based
on
only
learning
ability
w
ithout
any
analyticalknow
ledge,by
us-
ing
G
A
-IL
M
T
and
R
C
L
.
A
system
w
ith
R
C
L
acquires
part
translation
rules
and
sentence
translation
rules
together.
A
s
a
result,
a
chain
reaction
causes
the
ac-
quisition
of
translation
rules.
In
the
process
N
o.1
of
F
igure
2,
translation
rule
A
has
infor-
m
ation
that
the
system
can
extract
?Z
?
from
the
SL
sentences
of
translation
exam
ples,
or
the
source
parts
oftranslation
rules,and
can
extract
?
??
from
the
T
L
sentences
or
the
target
parts.
T
herefore,
the
system
can
acquire
the
sentence
translation
rule
B
by
extracting
?Z
?
from
the
SL
sentence
of
translation
exam
ple
N
o.1
and
?
??
from
the
T
L
sentence
of
translation
exam
-
ple
N
o.1.
T
he
acquired
translation
rule
B
has
inform
ation
that
the
system
can
extract
from
the
right
of
?E
?
to
the
left
of
?H
?
in
the
SL
sentences
of
translation
exam
ples,
or
the
source
parts
of
translation
rules,
and
can
extract
from
the
right
of
?
??
to
the
left
of
?
??
in
the
T
L
sentences
or
the
target
parts.
B
y
using
this
in-
form
ation,
in
process
2,
the
system
can
acquire
part
translation
rule
C
?
N
?
?
?
?
?
by
extract-
ing
?N
?
?
from
the
SL
sentence
of
translation
exam
ple
N
o.2
and
?
?
?
?
from
the
T
L
sentence.
M
oreover,
in
process
3,
translation
rule
D
is
ac-
quired
based
on
translation
rule
C
.Such
process
is
p
erform
ed
by
deciding
the
com
m
on
and
dif-
ferent
parts
in
character
strings
of
translation
exam
ples
(A
raki
et
al.,
1995).
T
herefore,
the
system
p
ossesses
an
ability
to
decide
com
m
on
and
diff
erent
parts
b
etw
een
tw
o
ob
jects.
3
O
u
tlin
e










	












	



















	









	


















	









	









	



































	



















	









F
igure
3:
P
rocess
flow
.
F
igure
3
show
s
the
outline
of
an
E
nglish-to-
Japanese
M
T
system
w
ith
R
C
L
.F
irst,a
user
in-
puts
a
SL
sentence
in
E
nglish.
In
the
translation
process,the
system
generates
translation
results
using
translation
rules
acquired
in
the
learning
process.
T
he
user
then
proofreads
the
trans-
lated
sentences
checking
for
errors.
In
the
feed-
back
process,
the
system
evaluates
the
transla-
tion
rules
used
in
the
translation
process.
In
the
learning
process,
the
translation
rules
are
acquired
by
using
tw
o
learning
algorithm
s.
O
ne
is
G
A
-IL
M
T
,
the
other
is
R
C
L
.
T
hese
tw
o
al-
gorithm
s
w
ork
each
other.
N
am
ely,
the
trans-
lation
rules
acquired
by
G
A
-IL
M
T
are
used
in
R
C
L
,and
the
translation
rules
acquired
by
R
C
L
are
used
in
Inductive
L
earning
of
G
A
-IL
M
T
.
In
this
pap
er,w
e
im
plem
ented
a
new
system
based
on
F
igure
3
as
a
b
ootstrapping
system
,
and
w
e
then
evaluated
this
system
.
4 Process
4.1 Translation process
In the translation process, the system gener-
ates translation results using acquired transla-
tion rules. First, the system selects the sen-
tence translation rules that can be applied to
the SL sentence. Second, the system generates
the translation results by replacing the variables
in the sentence translation rules with the part
translation rules.
4.2 Feedback process
In the feedback process, the system evaluates
the translation rules used. First, the system
evaluates the translation rules without variables
by using the results of combinations between
the translation rules with variables and the
translation rules without variables(Echizen-ya
et al, 1996). Next, the system evaluates trans-
lation rules with variables by using the processes
of combinations between the translations rules
with variables and the translation rules without
variables(Echizen-ya et al, 2000). As a result,
the system increases the correct translation fre-
quencies, or the erroneous translation frequen-
cies, of the translation rules by using these eval-
uation methods for the translation rules.
4.3 Learning process
4.3.1 GA-ILMT
In this paper, by using the process of acquisi-
tion of translation rules in GA-ILMT, the sys-
tem acquires both sentence and part transla-
tion rules. These rules are then used as starting
points when the system performs RCL.
4.3.2 Recursive Chain-link-type
Learning(RCL)
In this section, we describe the process of acqui-
sition of translation rules using RCL. The de-
tails of the process of acquisition of part trans-
lation rules are as follows.
(1)The system selects translation examples
that have common parts with the sentence
translation rules.
(2)The system extracts the parts that corre-
spond to the variables in the source parts
and in the target parts of the sentence
translation rules from the SL sentences,
and the TL sentences of the translation ex-
amples.
(3)The system registers pairs, of the parts ex-
tracted from the SL sentences and the parts
extracted from the TL sentences, as the
part translation rules.
(4)The system gives the correct and erroneous
frequencies of sentence translation rules to
the acquired part translation rules.
Figure 42 shows an example of the acquisi-
tion of a part translation rule using the sentence
translation rule. In Figure 4, (thirty;30[sanju])
as the part translation rule is acquired be-
cause ?thirty? corresponds to the variable in
the source part of sentence translation rule and
?30[sanju]? corresponds to the variable in the
target part of sentence translation rule.
	
   

	  
		
 			
	
			
	
	!"#	 $%
	&' 
(			
)*+,-.-/012/+ -
---3-45-678-39:;<%
=>-
---9-?>-@A-B8-39:;<%
C			
#        $
#       $
Figure 4: Example of the acquisition of a part
translation rule using the sentence translation
rule.
The details of the process of acquisition of
sentence translation rules are as follows:
(1)The system selects the part translation rules
in which the source parts are included in
the SL sentences of the translation example
or in the source parts of sentence transla-
tion rules, and in which the target parts are
included in the TL sentences of the trans-
lation examples or in the target parts of
sentence translation rules.
(2)The system acquires new sentence transla-
tion rules by replacing the parts which are
same as the part translation rules with the
variables to the translation examples or the
sentence translation rules.
(3)The system gives the correct and erroneous
frequencies of the part translation rules to
the acquired sentence translation rules.
2Italics are the pronunciation in Japanese.
Figure 5 shows examples of the acquisition
of the sentence translation rules using the part
translation rules. In Figure 5, the system
acquires?It starts in @0 minutes.???/?
/@0/?/??/?/???/???[Sore wa @0 pun
tate ba hajimari masu.]?as a sentence trans-
lation rule by using the part translation rule
(thirty;30[sanju]) acquired in Figure 4, and?@1
starts in @0 minutes.?@1/?/@0/?/??/?/
???/???[@1 wa @0 pun tate ba hajimari
masu.]?as the sentence translation rule, that is
more abstracted, is acquired by using the part
translation rule (it;?? [sore]).
	 	


					
			
 

			 
		
		

 !"!!#!$%!&!'()!(*+
 !"!!#!$%!&!'()!(*+

			
,			
					
-		
-!"!!#!$%!&!'()!(*+
 		

!"!!#!$%!&!'()!(*+
.				
/	 	   
01
/	 
 	   
01
/ 	   
01
/	  	   
01
Figure 5: Examples of the acquisition of a sen-
tence translation rule using the part translation
rule.
5 Experiments for performance
evaluation
5.1 Experimental procedure
There are two kinds of data as experimental
data. One is learning data and the other is
evaluation data. In these experiments, 1,759
translation examples were used as learning data.
These translation examples were taken from
textbooks(Nihon Kyozai(1), 2001; Nihon Ky-
ozai(2), 2001; Hoyu Shuppan, 2001) for second-
grade junior high school students. As well,
1,097 translation examples were used as eval-
uation data. These translation examples were
taken from textbooks(Bunri, 2001; Sinko Shup-
pan, 2001) for second-grade junior high school
students. All of these translation examples were
processed by the method outlined in Figure 3.
The initial condition of the dictionary is empty.
Moreover, we used three other commercial Rule-
Based MT systems, comparing our system with
those systems. We call these three MT systems
A, B and C respectively.
5.2 Evaluation standards
The correct translation results are grouped into
two categories:
(1) The correct translation
This means that the translation results cor-
respond to the correct translation results
taken from textbooks respectively(Bunri,
2001; Sinko Shuppan, 2001).
(2) A correct translation which includes un-
known words
This means that the translation results
with substituted nouns or adjectives as
variables correspond to the correct trans-
lation results taken from textbooks respec-
tively(Bunri, 2001; Sinko Shuppan, 2001).
In this paper, the effective translation results
are the translation results that correspond to
(1) and (2), and the ineffective translation re-
sults are the translation results that do not cor-
respond to (1) and (2). Moreover, the effec-
tive translation rate is the rate of the effective
translation results in all the evaluation data.
The translation results are ranked when several
translation results are generated. The transla-
tion results using the translation rules whose
rate of correct translation frequency is high, are
ranked at the top. We evaluated the translation
results that are ranked from No.1 to No.3.
5.3 Experimental results and discussion
Table 1 shows examples of effective translation
results in our system with RCL. Table 2 shows
the results of comparative experiments of our
system and the three Rule-Based MT systems.
We excluded 309 SL sentences from 1,097 SL
sentences used as evaluation data in Table 2. In
our system, the 309 SL sentences became the in-
effective translation results because of a lack of
learning data. Therefore, the 309 SL sentences
are not inadequate as evaluation data. Table 2
shows the effective translation rates in 788 SL
sentences, which were left after excluding 309
SL sentences from the 1,097 SL sentences used
as evaluation data. In the other three Rule-
Based MT systems, the same 788 SL sentences
were used as evaluation data and the transla-
tion results which correspond to (1) and (2)
Table 1: Examples of effective translation results.
Examples of the correct translation results
SL sentences TL sentences
This bag was made in France. ??????????????[Kono baggu wa furansu sei desu.]
We went there to play ??????????????????????
baseball. [Watashi tachi wa yakyu wo suru tame soko e iki mashi ta.]
Examples of the correct translation results which includes the unknown words
SL sentences TL sentences
@0???????????????
Shall I take you to the [@0 e tsure te itte age masho ka?]
amusement park? ?@0 requires ???? [yuen chi]? which is equivalent for
?the noun ?the amusement park?.
@0????????????????????? [@0 kara
How far is it from Kyoto to hiroshima made dono kurai no kyori ga ari masu ka?]
Hiroshima? ?@0 requires ??? [kyoto]? which is equivalent for
?the noun ?Kyoto?.
described in section 5.2 were evaluated as the
correct translation results. The effective trans-
lation rate in the system with only GA-ILMT
was 45.1%. In Table 2, the effective translation
rate of system with RCL is almost the same as
the effective translation rates of system A, but
is higher than systems B and C.
Table 2: Results of comparative experiments.
Effective trans- Details
System lation rates (1) (2)
Our system 85.0% 41.6% 58.4%
system A 85.8% 84.0% 16.0%
system B 81.7% 83.7% 16.3%
system C 76.9% 82.7% 17.3%
Table 3: Comparison of effective translation
rates based on quality.
Effective trans- Details
System lation rates (1) (2)
Our system 73.7% 7.5% 52.5%
system A 70.3% 84.2% 15.8%
system B 63.8% 85.0% 15.0%
system C 58.7% 82.8% 17.2%
Moreover, we evaluated translation results
more strictly in terms of the quality of trans-
lation. Meaning that only translation results
that had almost the same character strings as
the correct translation results taken from the
textbooks(Bunri, 2001; Sinko Shuppan, 2001)
were effective translation results. For exam-
ple, ????? 10?????? [Sore wa yaku
juppun kakari masu.]? is an ineffective trans-
lation result because of the correct transla-
tion results for ?It takes about ten minutes.?
is ?? 10 ?????? [Yaku juppun kakari
masu.]? in textbook(Bunri, 2001; Sinko Shup-
pan, 2001). In this Japanese sentence, phrase
???? [sore wa]? results in needlessly long.
Therefore, we evaluate the translation results
that have different phrases to the correct trans-
lation results as the ineffective translation re-
sults in terms of the quality of translation. Ta-
ble 3 shows a comparison of effective translation
rates based on quality. In Table 3, we confirmed
that the system with RCL can generate more
high-quality translation results than the three
other Rule-Based MT systems.
In the system with RCL, the erroneous trans-
lation rules are also acquired like a linked chain.
For example, in Figure 2, the translation rules
B, C and D are acquired as the erroneous trans-
lation rules when the translation rule A is the
erroneous translation rule. Namely, a chain re-
action causes the acquisition of erroneous trans-
lation rules. In learning data, the rate of er-
roneous part translation rules to the acquired
part translation rules was 47.9%, and the rate
of erroneous sentence translation rules to the
acquired sentence translation rules was 38.2%.
However, such erroneous translation rules are
automatically decided as being erroneous trans-
lation rules in the feedback process resulting
from the ineffective translation results.
6 Conclusion
In existing Example-Based MT systems based
on learning algorithms, similar translation pairs
must exist to acquire high-quality translation
rules. This means that the systems require
large amounts of translation examples to ac-
quire high-quality translation rules. On the
other hand, a system with RCL can acquire
many new translation rules from sparse trans-
lation examples because it uses other already
acquired translation rules based on the learn-
ing algorithms described in section 2. As a re-
sult, the quality of the translation and the ef-
fective translation rate of our system is higher
than other Rule-Based MT systems. However,
our system still does not reach the level of a
practical MT system and requires more transla-
tion rules to realize the goal of a practical MT
system. Although our system is not a practical
enough MT system, the system can effectively
acquire the translation rules from sparse data
by using RCL. Therefore, we consider that the
quality of translation improves only by adding
new translation examples without the difficulty
of Rule-Based MT systems in which a developer
must completely describe large-scale knowledge.
In the future, we plan to add a mechanism
that effectively combines the acquired transla-
tion rules so that the system realizes the trans-
lation of practical SL sentences.
7 Acknowledgements
This work was partially supported by the
Grants from the High-Tech Research Cen-
ter of Hokkai-Gakuen University and a Gov-
ernment subsidy for aiding scientific research
(No.14658097) of the Ministry of Education,
Culture, Sports, Science and Technology of
Japan.
References
Hutchins, W. J and H. L. Somers. 1992. An
Introduction to Machine Translation. ACA-
DEMIC PRESS.
Sato, S and M. Nagao. 1990. Toward Memory-
based Translation. In proceedings of the Col-
ing?90.
Brown, P., J. Cocke, S. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mer-
cer and P. S. Roossin. 1990. A Statistical
Approach to Machine Translation. Computa-
tional Linguistics Vol.16, No.2.
Watanabe, H and K. Takeda. 1998. A Pattern-
based Machine Translation System Extended
by Example-based Processing. In proceedings
of the Coling-ACL?98.
Brown, R.D. 2001. Transfer-Rule Induction
for Example-Based Translation. In proceed-
ings of the Workshop on EBMT, MT Summit
VIII.
Carl, M. 2001. Inducing Translation Grammars
from Bracketed Alignments. In proceedings of
the Workshop on EBMT, MT Summit VIII.
Malavazos, C and S. Piperidis. 2000. Appli-
cation of Analogical Modelling to Example
Based Machine Translation. In proceedings of
the Coling2000.
Gu?venir, H.A and I. Cicekli. 1998. Learning
Translation Templates from Examples. Infor-
mation Systems Vol.23, No.6.
McTait, K. 2001. Linguistic Knowledge and
Complexity in an EBMT System Based on
Translation Patterns. In proceedings of the
Workshop on EBMT, MT Summit VIII.
Echizen-ya, H., K. Araki, Y. Momouchi and K.
Tochinai. 1996. Machine Translation Method
using Inductive Learning with Genetic Algo-
rithms. In proceedings of the Coling?96.
Goldberg, D. E. 1989. Genetic Algorithms in
Search, Optimization, and Machine Learning.
Addison-Wesley.
Araki, K., Y. Momouchi and K. Tochinai. 1995.
Evaluation for Adaptability of Kana-kanji
Translation of Non-segmented Japanese Kana
Sentences using Inductive Learning. In pro-
ceedings of the PACLING?95.
Echizen-ya, H., K. Araki, Y. Momouchi and
K. Tochinai. 2000 Effectiveness of Layering
Translation Rules based on Transition Net-
works in Machine Translation using Inductive
Learning with Genetic Algorithms. In pro-
ceedings of the MT and Multilingual Applica-
tions in the New Millennium.
Nihon-Kyozai(1). 2001. One World English
Course 1 new edition. Tokyo.
Nihon-Kyozai(2). 2001. One World English
Course 2 new edition. Tokyo.
Hoyu Shuppan. 2001. System English Course 2
new edition. Tokyo.
Bunri. 2001. Work English Course 2 new edi-
tion. Tokyo.
Sinko Shuppan 2001. Training English Course
2 new edition. Osaka.
