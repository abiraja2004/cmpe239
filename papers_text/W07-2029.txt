Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 145?148,
Prague, June 2007. c?2007 Association for Computational Linguistics
FBK-irst: Lexical Substitution Task Exploiting
Domain and Syntagmatic Coherence
Claudio Giuliano and Alfio Gliozzo and Carlo Strapparava
FBK-irst, I-38050, Povo, Trento, ITALY
{giuliano, gliozzo, strappa}@itc.it
Abstract
This paper summarizes FBK-irst participa-
tion at the lexical substitution task of the
SEMEVAL competition. We submitted two
different systems, both exploiting synonym
lists extracted from dictionaries. For each
word to be substituted, the systems rank the
associated synonym list according to a simi-
larity metric based on Latent Semantic Anal-
ysis and to the occurrences in the Web 1T
5-gram corpus, respectively. In particular,
the latter system achieves the state-of-the-art
performance, largely surpassing the baseline
proposed by the organizers.
1 Introduction
The lexical substitution (Glickman et al, 2006a) can
be regarded as a subtask of the lexical entailment,
in which for a given word in context the system is
asked to select an alternative word that can be re-
placed in that context preserving the meaning. Lex-
ical Entailment, and in particular lexical reference
(Glickman et al, 2006b)1 , is in turn a subtask of tex-
tual entailment, which is formally defined as a rela-
tionship between a coherent text T and a language
expression, the hypothesis H . T is said to entail H ,
denoted by T ? H , if the meaning of H can be in-
ferred from the meaning of T (Dagan et al, 2005;
Dagan and Glickman., 2004). Even though this no-
tion has been only recently proposed in the computa-
tional linguistics literature, it attracts more and more
attention due to the high generality of its settings and
to the usefulness of its (potential) applications.
1In the literature, slight variations of this problem have been
also referred to as sense matching (Dagan et al, 2006).
With respect to lexical entailment, the lexical sub-
stitution task has a more restrictive criterion. In
fact, two words can be substituted when meaning is
preserved, while the criterion for lexical entailment
is that the meaning of the thesis is implied by the
meaning of the hypothesis. The latter condition is in
general ensured by substituting either hyperonyms
or synonyms, while the former is more rigid because
only synonyms are in principle accepted.
Formally, in a lexical entailment task a system is
asked to decide whether the substitution of a par-
ticular term w with the term e in a coherent text
Hw = H lwHr generates a sentence He = H leHr
such that Hw ? He, where H l and Hr denote the
left and the right context of w, respectively. For
example, given the source word ?weapon? a system
may substitute it with the target synonym ?arm?, in
order to identify relevant texts that denote the sought
concept using the latter term.
A particular case of lexical entailment is recog-
nizing synonymy, where both Hw ? He and He ?
Hw hold. The lexical substitution task at SEMEVAL
addresses exactly this problem. The task is not easy
since lists of candidate entailed words are not pro-
vided by the organizers. Therefore the system is
asked first to identify a set of candidate words, and
then to select only those words that fit in a particu-
lar context. To promote unsupervised methods, the
organizers did not provide neither labeled data for
training nor dictionaries or list of synonyms explain-
ing the meanings of the entailing words.
In this paper, we describe our approach to the
Lexical Substitution task at SEMEVAL 2007. We
developed two different systems (named IRST1-lsa
and IRST2-syn in the official task ranking), both ex-
ploiting a common lists of synonyms extracted from
dictionaries (i.e. WordNet and the Oxford Dictio-
145
nary) and ranking them according to two different
criteria:
Domain Proximity: the similarity between each
candidate entailed word and the context of the
entailing word is estimated by means of a co-
sine between their corresponding vectors in the
LSA space.
Syntagmatic Coherence: querying a large corpus,
the system finds all occurrences of the target
sentence, in which the entailing word is substi-
tuted with each synonym, and it assigns scores
proportional to the occurrence frequencies.
Results show that both methods are effective. In
particular, the second method achieved the best per-
formance in the competition, defining the state-of-
the-art for the lexical substitution task.
2 Lexical Substitution Systems
The lexical substitution task is a textual entailment
subtask in which the system is asked to provide one
or more terms e ? E ? syn(w) that can be sub-
stituted to w in a particular context Hw = H lwHr
generating a sentence He = H leHr such that both
Hw ? He and He ? Hw hold, where syn(w) is the
set of synonyms lemmata obtained from all synset in
which w appears in WordNet and H l and Hr denote
the left and the right context of w, respectively.
The first step, common to both systems, consists
of determining the set of synonyms syn(w) for each
entailing word (see Section 2.1). Then, each system
ranks the extracted lists according to the criteria de-
scribed in Section 2.2 and 2.3.
2.1 Used Lexical Resources
For selecting the synonym candidates we used two
lexical repositories: WordNet 2.0 and the Oxford
American Writer Thesaurus (1st Edition). For each
target word, we simply collect all the synonyms for
all the word senses in both these resources.
We exploited two corpora for our systems: the
British National Corpus for acquiring the LSA space
for ranking with domain proximity measure (Sec-
tion 2.2) and the Web 1T 5-gram Version 1 corpus
from Google (distributed by Linguistic Data Consor-
tium)2 for ranking the proposed synonyms accord-
ing to syntagmatic coherence (Section 2.3).
2Available from http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2006T13.
No other resources were used and the sense rank-
ing in WordNet was not considered at all. Therefore
our system is fully unsupervised.
2.2 Domain Proximity
Semantic Domains are common areas of human dis-
cussion, such as Economics, Politics, Law (Magnini
et al, 2002). Semantic Domains can be described
by DMs (Gliozzo, 2005), by defining a set of term
clusters, each representing a Semantic Domain, i.e.
a set of terms having similar topics. A DM is repre-
sented by a k ? k? rectangular matrix D, containing
the domain relevance for each term with respect to
each domain.
DMs can be acquired from texts by exploiting
term clustering algorithms. The degree of associ-
ation among terms and clusters, estimated by the
learning algorithm, provides a domain relevance
function. For our experiments we adopted a clus-
tering strategy based on Latent Semantic Analy-
sis (LSA) (Deerwester et al, 1990), following the
methodology described in (Gliozzo, 2005).
The input of the LSA process is a Term by Docu-
ment matrix T of the frequencies in the whole cor-
pus for each term. In this work we indexed all lem-
matized terms. The so obtained matrix is then de-
composed by means of a Singular Value Decompo-
sition, identifying the principal components of T.
Once a DM has been defined by the matrix D, the
Domain Space is a k? dimensional space, in which
both texts and terms are associated to Domain Vec-
tors (DVs), i.e. vectors representing their domain
relevance with respect to each domain. The DV ~t?i
for the term ti ? V is the ith row of D, where
V = {t1, t2, . . . , tk} is the vocabulary of the cor-
pus. The DVs for texts are obtained by mapping the
document vectors ~dj , represented in the vector space
model, into the vectors ~d?j in the Domain Space, de-fined by
D(~dj) = ~dj(IIDFD) = ~d?j (1)
where IIDF is a diagonal matrix such that iIDFi,i =
IDF (wi) and IDF (wi) is the Inverse Document
Frequency of wi. The similarity among both texts
and terms in the Domain Space is then estimated by
the cosine operation.
To implement our lexical substitution criterion we
ranked the candidate entailed words according to
their domain proximity, following the intuition that
if two words can be substituted in a particular con-
text, then the entailed word should belong to the
146
same semantic domain of the context in which the
entailing word is located.
The intuition above can be modeled by estimating
the similarity in the LSA space between the pseudo
document, estimated by Equation 1, formed by all
the words in the context of the entailing word (i.e.
the union of H l and Hr), and each candidate en-
tailed word in syn(w).
2.3 Syntagmatic Coherence
The syntagmatic coherence criterion is based on the
following observation. If the entailing word w in
its context Hw = H lwHr is actually entailed by
a word e, then there exist some occurrences on the
WEB of the expression He = H leHr, obtained
by replacing the entailing word with the candidate
entailed word. This intuition can be easily imple-
mented by looking for occurrences of He in the Web
1T 5-gram Version 1 corpus.
Figure 1 presents pseudo-code for the synonym
scoring procedure. The procedure takes as input the
set of candidate entailed words E = syn(w) for the
entailing word w, the context Hw in which w oc-
curs, the length of the n-gram (2 6 n 6 5) and the
target word itself. For each candidate entailed word
ei, the procedure ngrams(Hw, w, ei, n) is invoked
to substitute w with ei in Hw, obtaining Hei , and re-turns the set Q of all n-grams containing ei. For ex-
ample, all 3-grams obtained replacing ?bright? with
the synonym ?intelligent? in the sentence ?He was
bright and independent and proud.? are ?He was in-
telligent?, ?was intelligent and? and ?intelligent and
independent?. The maximum number of n-grams
generated is ?5n=2 n. Each candidate synonym isthen assigned a score by summing all the frequen-
cies in the Web 1T corpus of the so generated n-
grams3. The set of synonyms is ranked according
the so obtained scores. However, candidates which
appear in longer n-grams are preferred to candidates
appearing in shorter ones. Therefore, the ranked list
contains first the candidate entailed words appearing
in 5-grams, if any, then those appearing in 4-grams,
and so on. For example, a candidate e1 that appears
only once in 5-grams is preferred to a candidate e2
that appears 1000 times in 4-grams. Note that this
strategy could lead to an output list with repetitions.
3Note that n-grams with frequency lower than 40 are not
present in the corpus.
1: Given E, the set of candidate synonyms
2: Given H , the context in which w occurs
3: Given n, the length of the n-gram
4: Given w, the word to be substituted
5: E? ? ?
6: for each ei in E do
7: Q? ngrams(H,w, ei, n)
8: scorei ? 0
9: for each qj in Q do
10: Get the frequency fj of qj
11: scorei ? scorei + fj
12: end for
13: if scorei > 0 then add the pair {scorei, ei}
in E?
14: end for
15: Return E?
Figure 1: The synonym scoring procedure
3 Evaluation
There are basically two scoring methodologies: (i)
BEST, which scores the best substitute for a given
item, and (ii) OOT, which scores for the best 10 sub-
stitutes for a given item, and systems do not benefit
from providing less responses4 .
BEST. Table 1 and 2 report the performance for the
domain proximity and syntagmatic coherence rank-
ing. Please note that in Table 2 we report both the
official score and a score that takes into account just
the first proposal of the systems, as the usual in-
terpretation of BEST score methodology would sug-
gest5.
OOT. Table 4 and 5 report the performance for the
domain proximity and syntagmatic coherence rank-
ing, scoring for the 10 best substitutes. The results
are quite good especially in the case of syntagmatic
coherence ranking.
Baselines. Table 3 displays the baselines respec-
tively for the BEST and OOT using WordNet 2.1
as calculated by the task organizers. They pro-
pose many baseline measures, but we report only the
4The task proposed a third scoring measure MW that scores
precision and recall for detection and identification of multi-
words in the input sentences. However our systems were not
designed for this functionality. For the details of all scoring
methodologies please refer to the task description documents.
5We misinterpreted that the official scorer divides anyway
the figures by the number of proposals. So for the competition
we submitted the oot result file without cutting the words after
the first one.
147
P R Mode P Mode R
all 8.06 8.06 13.09 13.09
Table 1: BEST results for LSA ranking (IRST1-lsa)
P R Mode P Mode R
all 12.93 12.91 20.33 20.33
all (official) 6.95 6.94 20.33 20.33
Table 2: BEST results for Syntagmatic ranking
(IRST2-syn)
WordNet one, as it is the higher scoring baseline. We
can observe that globally our systems perform quite
good with respect to the baselines.
4 Conclusion
In this paper we reported a detailed description of
the FBK-irst systems submitted to the Lexical En-
tailment task at the SEMEVAL 2007 evaluation cam-
paign. Our techniques are totally unsupervised, as
they do not require neither the availability of sense
tagged data nor an estimation of sense priors, not
considering the WordNet sense order information.
Results are quite good, as in general they signifi-
cantly outperform all the baselines proposed by the
organizers. In addition, the method based on syn-
tagmatic coherence estimated on the WEB outper-
forms, to our knowledge, the other systems sub-
mitted to the competition. For the future, we plan
to avoid the use of dictionaries by adopting term
similarity techniques to select the candidate entailed
words and to exploit this methodology in some spe-
cific applications such as taxonomy induction and
ontology population.
Acknowledgments
Claudio Giuliano is supported by the X-Media
project (http://www.x-media-project.
org), sponsored by the European Commission
as part of the Information Society Technologies
(IST) programme under EC grant number IST-FP6-
026978. Alfio Gliozzo is supported by FIRB-Israel
P R Mode P Mode R
WN BEST 9.95 9.95 15.28 15.28
WN OOT 29.70 29.35 40.57 40.57
Table 3: WordNet Baselines
P R Mode P Mode R
all 41.23 41.20 55.28 55.28
Table 4: OOT results for LSA ranking (IRST1-lsa)
P R Mode P Mode R
all 69.03 68.90 58.54 58.54
Table 5: OOT results for Syntagmatic ranking
(IRST2-syn)
research project N. RBIN045PXH.
References
I. Dagan and O. Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. In proceedings of the PASCAL Workshop
on Learning Methods for Text Understanding and Min-
ing, Grenoble.
I. Dagan, O. Glickman, and B. Magnini. 2005. The pas-
cal recognising textual entailment challenge. Proceed-
ings of the PASCAL Challenges Workshop on Recog-
nising Textual Entailment.
I. Dagan, O. Glickman, A. Gliozzo, E. Marmorshtein,
and C. Strapparava. 2006. Direct word sense match-
ing for lexical substitution. In Proceedings ACL-2006,
pages 449?456, Sydney, Australia, July.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society of Information
Science.
O. Glickman, I. Dagan, M. Keller, S. Bengio, and
W. Daelemans. 2006a. Investigating lexical substi-
tution scoring for subtitle generation tenth conference
on computational natural language learning. In Pro-
ceedings of CoNLL-2006.
O. Glickman, E. Shnarch, and I. Dagan. 2006b. Lexical
reference: a semantic matching subtask. In proceed-
ings of EMNLP 2006.
A. Gliozzo. 2005. Semantic Domains in Computa-
tional Linguistics. Ph.D. thesis, ITC-irst/University of
Trento.
B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.
2002. The role of domain information in word
sense disambiguation. Natural Language Engineer-
ing, 8(4):359?373.
148
