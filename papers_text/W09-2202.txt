Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 10?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Surrogate Learning -
From Feature Independence to Semi-Supervised Classification
Sriharsha Veeramachaneni and Ravi Kumar Kondadadi
Thomson Reuters Research and Development
Eagan, MN 55123, USA
[harsha.veeramachaneni,ravikumar.kondadadi]@thomsonreuters.com
Abstract
We consider the task of learning a classi-
fier from the feature space X to the set of
classes Y = {0, 1}, when the features can
be partitioned into class-conditionally inde-
pendent feature sets X1 and X2. We show
that the class-conditional independence can be
used to represent the original learning task
in terms of 1) learning a classifier from X2
to X1 (in the sense of estimating the prob-
ability P (x1|x2))and 2) learning the class-
conditional distribution of the feature set X1.
This fact can be exploited for semi-supervised
learning because the former task can be ac-
complished purely from unlabeled samples.
We present experimental evaluation of the idea
in two real world applications.
1 Introduction
Semi-supervised learning is said to occur when the
learner exploits (a presumably large quantity of) un-
labeled data to supplement a relatively small labeled
sample, for accurate induction. The high cost of la-
beled data and the simultaneous plenitude of unla-
beled data in many application domains, has led to
considerable interest in semi-supervised learning in
recent years (Chapelle et al, 2006).
We show a somewhat surprising consequence of
class-conditional feature independence that leads
to a principled and easily implementable semi-
supervised learning algorithm. When the feature set
can be partitioned into two class-conditionally in-
dependent sets, we show that the original learning
problem can be reformulated in terms of the problem
of learning a first predictor from one of the partitions
to the other, plus a second predictor from the latter
partition to class label. That is, the latter partition
acts as a surrogate for the class variable. Assum-
ing that the second predictor can be learned from
a relatively small labeled sample this results in an
effective semi-supervised algorithm, since the first
predictor can be learned from only unlabeled sam-
ples.
In the next section we present the simple yet in-
teresting result on which our semi-supervised learn-
ing algorithm (which we call surrogate learning) is
based. We present examples to clarify the intuition
behind the approach and present a special case of
our approach that is used in the applications section.
We then examine related ideas in previous work and
situate our algorithm among previous approaches
to semi-supervised learning. We present empirical
evaluation on two real world applications where the
required assumptions of our algorithm are satisfied.
2 Surrogate Learning
We consider the problem of learning a classifier
from the feature space X to the set of classes Y =
{0, 1}. Let the features be partitioned into X =
X1 ? X2. The random feature vector x ? X will be
represented correspondingly as x = (x1,x2). Since
we restrict our consideration to a two-class problem,
the construction of the classifier involves the esti-
mation of the probability P (y = 0|x1,x2) at every
point (x1,x2) ? X .
We make the following assumptions on the joint
probabilities of the classes and features.
10
1. P (x1,x2|y) = P (x1|y)P (x2|y) for y ?
{0, 1}. That is, the feature sets x1 and x2
are class-conditionally independent for both
classes. Note that, when X1 and X2 are one-
dimensional, this condition is identical to the
Naive Bayes assumption, although in general
our assumption is weaker.
2. P (x1|x2) 6= 0, P (x1|y) 6= 0 and P (x1|y =
0) 6= P (x1|y = 1). These assumptions are
to avoid divide-by-zero problems in the alge-
bra below. If x1 is a discrete valued random
variable and not irrelevant for the classification
task, these conditions are often satisfied.
We can now show that P (y = 0|x1,x2) can
be written as a function of P (x1|x2) and P (x1|y).
When we consider the quantity P (y,x1|x2), we
may derive the following.
P (y,x1|x2) = P (x1|y,x2)P (y|x2)
? P (y,x1|x2) = P (x1|y)P (y|x2)
(from the independence assumption)
? P (y|x1,x2)P (x1|x2) = P (x1|y)P (y|x2)
? P (y|x1,x2)P (x1|x2)P (x1|y) = P (y|x2) (1)
Since P (y = 0|x2) + P (y = 1|x2) = 1, Equa-
tion 1 implies
P (y = 0|x1,x2)P (x1|x2)
P (x1|y = 0) +
P (y = 1|x1,x2)P (x1|x2)
P (x1|y = 1) = 1
? P (y = 0|x1,x2)P (x1|x2)P (x1|y = 0) +
(1? P (y = 0|x1,x2))P (x1|x2)
P (x1|y = 1) = 1(2)
Solving Equation 2 for P (y = 0|x1,x2), we ob-
tain
P (y = 0|x1,x2) =
P (x1|y = 0)
P (x1|x2) ?
P (x1|y = 1)? P (x1|x2)
P (x1|y = 1)? P (x1|y = 0)(3)
We have succeeded in writing P (y = 0|x1,x2) as
a function of P (x1|x2) and P (x1|y). Although this
result was previously observed in a different context
by Abney in (Abney, 2002), he does not use it to
derive a semi-supervised learning algorithm. This
result can lead to a significant simplification of the
learning task when a large amount of unlabeled data
is available. The semi-supervised learning algorithm
involves the following two steps.
1. From unlabeled data learn a predictor from the
feature space X2 to the space X1 to predict
P (x1|x2). There is no restriction on the learner
that can be used as long as it outputs posterior
class probability estimates.
2. Estimate the quantity P (x1|y) from a labeled
samples. In case x1 is finite valued, this can
be done by just counting. If X1 has low car-
dinality the estimation problem requires very
few labeled samples. For example, if x1 is
binary, then estimating P (x1|y) involves esti-
mating just two Bernoulli probabilities.
Thus, we can decouple the prediction problem into
two separate tasks, one of which involves predict-
ing x1 from the remaining features. In other words,
x1 serves as a surrogate for the class label. Fur-
thermore, for the two steps above there is no neces-
sity for complete samples. The labeled examples can
have the feature x2 missing.
At test time, an input sample (x1,x2) is classified
by computing P (x1|y) and P (x1|x2) from the pre-
dictors obtained from training, and plugging these
values into Equation 3. Note that these two quanti-
ties are computed for the actual value of x1 taken by
the input sample.
The following example illustrates surrogate learn-
ing.
??????????????
Example 1
Consider the following variation on a problem
from (Duda et al, 2000) of classifying fish on a con-
veryor belt as either salmon (y = 0) or sea bass
(y = 1). The features describing the fish are x1,
a binary feature describing whether the fish is light
(x1 = 0) or dark (x1 = 1), and x2 describes the
length of the fish which is real-valued. Assume (un-
realistically) that P (x2|y), the class-conditional dis-
tribution of x2, the length for salmon is Gaussian,
11
and for the sea bass is Laplacian as shown in Fig-
ure 1.
?4 ?2 0 2 40
0.5
x2
P(x2|y=0) P(x2|y=1)
Figure 1: Class-conditional probability distributions of
the feature x2.
Because of the class-conditional feature in-
dependence assumption, the joint distribution
P (x1,x2,y) = P (x2|y)P (x1,y) can now be
completely specified by fixing the joint probabil-
ity P (x1,y). Let P (x1 = 0,y = 0) = 0.3,
P (x1 = 0,y = 1) = 0.1, P (x1 = 1,y = 0) = 0.2,
and P (x1 = 1,y = 1) = 0.4. I.e., a salmon is more
likely to be light than dark and a sea bass is more
likely to be dark than light.
The full joint distribution is depicted in Figure 2.
Also shown in Figure 2 are the conditional distribu-
tions P (x1 = 0|x2) and P (y = 0|x1,x2).
Assume that we build a predictor to decide be-
tween x1 = light and x1 = dark from the length us-
ing a data set of unlabeled fish. On a random salmon,
this predictor will most likely decide that x1 = light
(because, for a salmon, x1 = light is more likely
than x1 = dark, and similarly for a sea bass the
predictor often decides that x1 = dark. Conse-
quently the predictor provides information about the
true class label y. This can also be seen in the sim-
ilarities between the curves P (y = 0|x1,x2) to the
curve P (x1|x2) in Figure 2.
Another way to interpret the example is to note
that if a predictor for P (x1|x2) were built on only
the salmons then P (x1 = light|x2) will be a con-
stant value (0.6). Similarly the value of P (x1 =
light|x2) for sea basses will also be a constant value
(0.2). That is, the value of P (x1 = light|x2) for
a sample is a good predictor of its class. However,
?4 ?2 0 2 4
0.5
x1 = 0
x1 = 1
x2
P(x1=1,y=0,x2) P(x1=1,y=1,x2)
P(x1=0,y=1,x2)P(x1=0,y=0,x2)
P(y=0|x1=1,x2)
P(y=0|x1=0,x2)
P(x1=0|,x2)
Figure 2: The joint distributions and the posterior distri-
butions of the class y and the surrogate class x1.
surrogate learning builds the predictor P (x1|x2) on
unlabeled data from both types of fish and there-
fore additionally requires P (x1|y) to estimate the
boundary between the classes.
2.1 A Special Case
The independence assumptions made in the setting
above may seem too strong to hold in real problems,
especially because the feature sets are required to
be class-conditionally independent for both classes.
We now specialize the setting of the classification
problem to the one realized in the applications we
present later.
We still wish to learn a classifier from X = X1 ?
X2 to the set of classes Y = {0, 1}. We make the
following slightly modified assumptions.
1. x1 is a binary random variable. That is, X1 =
{0, 1}.
2. P (x1,x2|y = 0) = P (x1|y = 0)P (x2|y =
0). We require that the feature x1 be class-
conditionally independent of the remaining fea-
tures only for the class y = 0.
3. P (x1 = 0,y = 1) = 0. This assumption says
that x1 is a ?100% recall? feature for y = 11.
Assumption 3 simplifies the learning task to the
estimation of the probability P (y = 0|x1 = 1,x2)
for every point x2 ? X2. We can proceed as before
1This assumption can be seen to trivially enforce the inde-
pendence of the features for class y = 1.
12
to obtain the expression in Equation 3.
P (y = 0|x1 = 1,x2)
= P (x1 = 1|y = 0)P (x1 = 1|x2) . . .
. . . P (x1 = 1|y = 1)? P (x1 = 1|x2)P (x1 = 1|y = 1)? P (x1 = 1|y = 0)
= P (x1 = 1|y = 0)P (x1 = 1|x2) ?
1? P (x1 = 1|x2)
1? P (x1 = 1|y = 0)
= P (x1 = 1|y = 0)P (x1 = 1|x2) ?
P (x1 = 0|x2)
P (x1 = 0|y = 0)
= P (x1 = 1|y = 0)P (x1 = 0|y = 0) ?
P (x1 = 0|x2)
(1? P (x1 = 0|x2))(4)
Equation 4 shows that P (y = 0|x1 = 1,x2)
is a monotonically increasing function of P (x1 =
0|x2). This means that after we build a predictor
from X2 to X1, we only need to establish the thresh-
old on P (x1 = 0|x2) to yield the optimum classi-
fication between y = 0 and y = 1. Therefore the
learning proceeds as follows.
1. From unlabeled data learn a predictor from the
feature space X2 to the binary space X1 to pre-
dict the quantity P (x1|x2).
2. Use labeled sample to establish the thresh-
old on P (x1 = 0|x2) to achieve the desired
precision-recall trade-off for the original clas-
sification problem.
Because of our assumptions, for a sample from
class y = 0 it is impossible to predict whether
x1 = 0 or x1 = 1 better than random by looking
at the x2 feature, whereas a sample from the posi-
tive class always has x1 = 1. Therefore the samples
with x1 = 0 serve to delineate the positive exam-
ples among the samples with x1 = 1. We therefore
call the samples that have x1 = 1 as the target sam-
ples and those that have x1 = 0 as the background
samples.
3 Related Work
Although the idea of using unlabeled data to im-
prove classifier accuracy has been around for several
decades (Nagy and Shelton, 1966), semi-supervised
learning has received much attention recently due
to impressive results in some domains. The com-
pilation of chapters edited by Chappelle et al is an
excellent introduction to the various approaches to
semi-supervised learning, and the related practical
and theoretical issues (Chapelle et al, 2006).
Similar to our setup, co-training assumes that the
features can be split into two class-conditionally
independent sets or ?views? (Blum and Mitchell,
1998). Also assumed is the sufficiency of either
view for accurate classification. The co-training al-
gorithm iteratively uses the unlabeled data classified
with high confidence by the classifier on one view,
to generate labeled data for learning the classifier on
the other.
The intuition underlying co-training is that the er-
rors caused by the classifier on one view are inde-
pendent of the other view, hence can be conceived
as uniform2 noise added to the training examples
for the other view. Consequently, the number of la-
bel errors in a region in the feature space is propor-
tional to the number of samples in the region. If the
former classifier is reasonably accurate, the propor-
tionally distributed errors are ?washed out? by the
correctly labeled examples for the latter classifier.
Seeger showed that co-training can also be viewed
as an instance of the Expectation-Maximization al-
gorithm (Seeger, 2000).
The main distinction of surrogate learning from
co-training is the learning of a predictor from one
view to the other, as opposed to learning predictors
from both views to the class label. We can there-
fore eliminate the requirement that both views be
sufficiently informative for reasonably accurate pre-
diction. Furthermore, unlike co-training, surrogate
learning has no iterative component.
Ando and Zhang propose an algorithm to regu-
larize the hypothesis space by simultaneously con-
sidering multiple classification tasks on the same
feature space (Ando and Zhang, 2005). They then
use their so-called structural learning algorithm for
semi-supervised learning of one classification task,
by the artificial construction of ?related? problems
on unlabeled data. This is done by creating prob-
lems of predicting observable features of the data
and learning the structural regularization parame-
ters from these ?auxiliary? problems and unlabeled
data. More recently in (Ando and Zhang, 2007) they
2Whether or not a label is erroneous is independent of the
feature values of the latter view.
13
showed that, with conditionally independent feature
sets predicting from one set to the other allows the
construction of a feature representation that leads
to an effective semi-supervised learning algorithm.
Our approach directly operates on the original fea-
ture space and can be viewed another justification
for the algorithm in (Ando and Zhang, 2005).
Multiple Instance Learning (MIL) is a learning
setting where training data is provided as positive
and negative bags of samples (Dietterich et al,
1997). A negative bag contains only negative ex-
amples whereas a positive bag contains at least one
positive example. Surrogate learning can be viewed
as artificially constructing a MIL problem, with the
targets acting as one positive bag and the back-
grounds acting as one negative bag (Section 2.1).
The class-conditional feature independence assump-
tion for class y = 0 translates to the identical and
independent distribution of the negative samples in
both bags.
4 Two Applications
We applied the surrogate learning algorithm to the
problems of record linkage and paraphrase genera-
tion. As we shall see, the applications satisfy the
assumptions in our second (100% recall) setting.
4.1 Record Linkage/ Entity Resolution
Record linkage is the process of identification and
merging of records of the same entity in different
databases or the unification of records in a single
database, and constitutes an important component of
data management. The reader is referred to (Win-
kler, 1995) for an overview of the record linkage
problem, strategies and systems. In natural language
processing record linkage problems arise during res-
olution of entities found in natural language text to
a gazetteer.
Our problem consisted of merging each of ?
20000 physician records, which we call the update
database, to the record of the same physician in
a master database of ? 106 records. The update
database has fields that are absent in the master
database and vice versa. The fields in common in-
clude the name (first, last and middle initial), sev-
eral address fields, phone, specialty, and the year-
of-graduation. Although the last name and year-
of-graduation are consistent when present, the ad-
dress, specialty and phone fields have several incon-
sistencies owing to different ways of writing the ad-
dress, new addresses, different terms for the same
specialty, missing fields, etc. However, the name
and year alone are insufficient for disambiguation.
We had access to ? 500 manually matched update
records for training and evaluation (about 40 of these
update records were labeled as unmatchable due to
insufficient information).
The general approach to record linkage involves
two steps: 1) blocking, where a small set of can-
didate records is retrieved from the master record
database, which contains the correct match with
high probability, and 2) matching, where the fields
of the update records are compared to those of the
candidates for scoring and selecting the match. We
performed blocking by querying the master record
database with the last name from the update record.
Matching was done by scoring a feature vector of
similarities over the various fields. The feature val-
ues were either binary (verifying the equality of a
particular field in the update and a master record) or
continuous (some kind of normalized string edit dis-
tance between fields like street address, first name
etc.).
The surrogate learning solution to our matching
problem was set up as follows. We designated the
binary feature of equality of year of graduation3 as
the ?100% recall? feature x1, and the remaining fea-
tures are relegated to x2. The required conditions
for surrogate learning are satisfied because 1) in our
data it is highly unlikely for two records with differ-
ent year- of-graduation to belong to the same physi-
cian and 2) if it is known that the update record
and a master record belong to two different physi-
cians, then knowing that they have the same (or dif-
ferent) year-of-graduation provides no information
about the other features. Therefore all the feature
vectors with the binary feature indicating equality
of year-of-graduation are targets and the remaining
are backgrounds.
First, we used feature vectors obtained from the
records in all blocks from all 20000 update records
to estimate the probability P (x1|x2). We used lo-
3We believe that the equality of the middle intial would have
worked just as well for x1.
14
Table 1: Precision and Recall for record linkage.
Training Precision Recall
proportion
Surrogate 0.96 0.95
Supervised 0.5 0.96 0.94
Supervised 0.2 0.96 0.91
gistic regression for this prediction task. For learn-
ing the logistic regression parameters, we discarded
the feature vectors for which x1 was missing and
performed mean imputation for the missing values
of other features. Second, the probability P (x1 =
1|y = 0) (the probability that two different ran-
domly chosen physicians have the same year of
graduation) was estimated straightforwardly from
the counts of the different years-of-graduation in the
master record database.
These estimates were used to assign the score
P (y = 1|x1 = 1,x2) to the records in a block (cf.
Equation 4). The score of 0 is assigned to feature
vectors which have x1 = 0. The only caveat is cal-
culating the score for feature vectors that had miss-
ing x1. For such records we assign the score P (y =
1|x2) = P (y = 1|x1 = 1,x2)P (x1 = 1|x2). We
have estimates for both quantities on the right hand
side. The highest scoring record in each block was
flagged as a match if it exceeded some appropriate
threshold.
We compared the results of the surrogate learn-
ing approach to a supervised logistic regression
based matcher which used a portion of the manual
matches for training and the remaining for testing.
Table 1 shows the match precision and recall for
both the surrogate learning and the supervised ap-
proaches. For the supervised algorithm, we show the
results for the case where half the manually matched
records were used for training and half for testing,
as well as for the case where a fifth of the records of
training and the remaining four-fifths for testing. In
the latter case, every record participated in exactly
one training fold but in four test folds.
The results indicate that the surrogate learner per-
forms better matching by exploiting the unlabeled
data than the supervised learner with insufficient
training data. The results although not dramatic are
still promising, considering that the surrogate learn-
ing approach used none of the manually matched
records.
4.2 Paraphrase Generation for Event
Extraction
Sentence classification is often a preprocessing step
for event or relation extraction from text. One of the
challenges posed by sentence classification is the di-
versity in the language for expressing the same event
or relationship. We present a surrogate learning ap-
proach to generating paraphrases for expressing the
merger-acquisition (MA) event between two organi-
zations in financial news. Our goal is to find para-
phrase sentences for the MA event from an unla-
beled corpus of news articles, that might eventually
be used to train a sentence classifier that discrimi-
nates between MA and non-MA sentences.
We assume that the unlabeled sentence corpus is
time-stamped and named entity tagged with orga-
nizations. We further assume that a MA sentence
must mention at least two organizations. Our ap-
proach to generate paraphrases is the following. We
first extract all the so-called source sentences from
the corpus that match a few high-precision seed pat-
terns. An example of a seed pattern used for the
MA event is ?<ORG1> acquired<ORG2>? (where
<ORG1> and <ORG2> are place holders for
strings that have been tagged as organizations). An
example of a source sentence that matches the seed
is ?It was announced yesterday that <ORG>Google
Inc.<ORG> acquired <ORG>Youtube <ORG>?.
The purpose of the seed patterns is to produce pairs
of participant organizations in an MA event with
high precision.
We then extract every sentence in the corpus that
contains at least two organizations, such that at least
one of them matches an organization in the source
sentences, and has a time-stamp within a two month
time window of the matching source sentence. Of
this set of sentences, all that contain two or more or-
ganizations from the same source sentence are des-
ignated as target sentences, and the rest are desig-
nated as background sentences.
We speculate that since an organization is unlikely
to have a MA relationship with two different orga-
nizations in the same time period the backgrounds
are unlikely to contain MA sentences, and more-
over the language of the non-MA target sentences is
15
Table 2: Patterns used as seeds and the number of source
sentences matching each seed.
Seed pattern # of sources
1 <ORG> acquired <ORG> 57
2 <ORG> bought <ORG> 70
3 offer for <ORG> 287
4 to buy <ORG> 396
5 merger with <ORG> 294
indistinguishable from that of the background sen-
tences. To relate the approach to surrogate learning,
we note that the binary ?organization-pair equality?
feature (both organizations in the current sentence
being the same as those in a source sentence) serves
as the ?100% recall? feature x1. Word unigram, bi-
gram and trigram features were used as x2. This
setup satisfies the required conditions for surrogate
learning because 1) if a sentence is about MA, the
organization pair mentioned in it must be the same
as that in a source sentence, (i.e., if only one of the
organizations match those in a source sentence, the
sentence is unlikely to be about MA) and 2) if an un-
labeled sentence is non-MA, then knowing whether
or not it shares an organization with a source does
not provide any information about the language in
the sentence.
If the original unlabeled corpus is sufficiently
large, we expect the target set to cover most of the
paraphrases for the MA event but may contain many
non-MA sentences as well. The task of generating
paraphrases involves filtering the target sentences
that are non-MA and flagging the rest of the tar-
gets as paraphrases. This is done by constructing a
classifier between the targets and backgrounds. The
feature set used for this task was a bag of word un-
igrams, bigrams and trigrams, generated from the
sentences and selected by ranking the n-grams by
the divergence of their distributions in the targets
and backgrounds. A support vector machine (SVM)
was used to learn to classify between the targets and
backgrounds and the sentences were ranked accord-
ing to the score assigned by the SVM (which is a
proxy for P (x1 = 1|x2)). We then thresholded the
score to obtain the paraphrases.
Our approach is similar in principle to the ?Snow-
ball? system proposed in (Agichtein and Gravano,
2000) for relation extraction. Similar to us, ?Snow-
ball? looks for known participants in a relationship in
an unlabeled corpus, and uses the newly discovered
contexts to extract more participant tuples. How-
ever, unlike surrogate learning, which can use a rich
set of features for ranking the targets, ?Snowball?
scores the newly extracted contexts according to a
single feature value which is confidence measure
based only on the number of known participant tu-
ples that are found in the context.
Example 2 below lists some sentences to illustrate
the surrogate learning approach. Note that the tar-
gets may contain both MA and non-MA sentences
but the backgrounds are unlikely to be MA.
??????????????
Example 2
Seed Pattern
?offer for <ORG>?
Source Sentences
1. <ORG>US Airways<ORG> said Wednesday it will
increase its offer for <ORG>Delta<ORG>.
Target Sentences (SVM score)
1.<ORG>US Airways<ORG> were to combine with a
standalone <ORG>Delta<ORG>. (1.0008563)
2.<ORG>US Airways<ORG> argued that the nearly
$10 billion acquisition of <ORG>Delta<ORG> would
result in an efficiently run carrier that could offer low
fares to fliers. (0.99958149)
3.<ORG>US Airways<ORG> is asking
<ORG>Delta<ORG>?s official creditors commit-
tee to support postponing that hearing. (-0.99914371)
Background Sentences (SVM score)
1. The cities have made various overtures to
<ORG>US Airways<ORG>, including a promise
from <ORG>America West Airlines<ORG> and the
former <ORG>US Airways<ORG>. (0.99957752)
2. <ORG>US Airways<ORG> shares rose 8 cents
to close at $53.35 on the <ORG>New York Stock
Exchange<ORG>. (-0.99906444)
??????????????
We tested our algorithm on an unlabeled corpus of
approximately 700000 financial news articles. We
experimented with the five seed patterns shown in
Table 2. We extracted a total of 870 source sentences
from the five seeds. The number of source sentences
matching each of the seeds is also shown in Table 2.
Note that the numbers add to more than 870 because
it is possible for a source sentence to match more
than one seed.
The participants that were extracted from sources
16
Table 3: Precision/Recall of surrogate learning on the
MA paraphrase problem for various thresholds. The
baseline of using all the targets as paraphrases for MA
has a precision of 66% and a recall of 100%.
Threshold Precision Recall
0.0 0.83 0.94
-0.2 0.82 0.95
-0.8 0.79 0.99
corresponded to approximately 12000 target sen-
tences and approximately 120000 background sen-
tences. For the purpose of evaluation, 500 randomly
selected sentences from the targets were manually
checked leading to 330 being tagged as MA and the
remaining 170 as non-MA. This corresponds to a
66% precision of the targets.
We then ranked the targets according to the score
assigned by the SVM trained to classify between the
targets and backgrounds, and selected all the targets
above a threshold as paraphrases for MA. Table 3
presents the precision and recall on the 500 manu-
ally tagged sentences as the threshold varies. The
results indicate that our approach provides an effec-
tive way to rank the target sentences according to
their likelihood of being about MA.
To evaluate the capability of the method to find
paraphrases, we conducted five separate experi-
ments using each pattern in Table 2 individually as a
seed and counting the number of obtained sentences
containing each of the other patterns (using a thresh-
old of 0.0). These numbers are shown in the differ-
ent columns of Table 4. Although new patterns are
obtained, their distribution only roughly resembles
the original distribution in the corpus. We attribute
this to the correlation in the language used to de-
scribe a MA event based on its type (merger vs. ac-
quisition, hostile takeover vs. seeking a buyer, etc.).
Finally we used the paraphrases, which were
found by surrogate learning, to augment the train-
ing data for a MA sentence classifier and evaluated
its accuracy. We first built a SVM classifier only
on a portion of the labeled targets and classified the
remaining. This approach yielded an accuracy of
76% on the test set (with two-fold cross validation).
We then added all the targets scored above a thresh-
old by surrogate learning as positive examples (4000
Table 4: Number of sentences found by surrogate learn-
ing matching each of the remaining seed patterns, when
only one of the patterns was used as a seed. Each column
is for one experiment with the corresponding pattern used
as the seed. For example, when only the first pattern was
used as the seed, we obtained 18 sentences that match the
fourth pattern.
Seeds 1 2 3 4 5
1 2 2 5 1
2 5 6 7 5
3 4 6 152 103
4 18 16 93 57
5 3 9 195 57
positive sentences in all were added), and all the
backgrounds that scored below a low threshold as
negative examples (27000 sentences), to the training
data and repeated the two-fold cross validation. The
classifier learned on the augmented training data im-
proved the accuracy on the test data to 86% .
We believe that better designed features (than
word n-grams) will provide paraphrases with higher
precision and recall of the MA sentences found by
surrogate learning. To apply our approach to a new
event extraction problem, the design step also in-
volves the selection of the x1 feature such that the
targets and backgrounds satisfy our assumptions.
5 Conclusions
We presented surrogate learning ? an easily imple-
mentable semi-supervised learning algorithm that
can be applied when the features satisfy the required
independence assumptions. We presented two appli-
cations, showed how the assumptions are satisfied,
and presented empirical evidence for the efficacy of
our algorithm. We have also applied surrogate learn-
ing to problems in information retrieval and docu-
ment zoning. We expect that surrogate learning is
sufficiently general to be applied in many NLP ap-
plications, if the features are carefully designed. We
briefly note that a surrogate learning method based
on regression and requiring only mean independence
instead of full statistical independence can be de-
rived using techniques similar to those in Section 2
? this modification is closely related to the problem
and solution presented in (Quadrianto et al, 2008).
17
References
S. Abney. 2002. Bootstrapping. In In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 360?367.
E. Agichtein and L. Gravano. 2000. Snowball: Extract-
ing Relations from Large Plain-Text Collections. In
Proceedings of the 5th ACM International Conference
on Digital Libraries (ACM DL), pages 85?94, June,
2-7.
R. K. Ando and T. Zhang. 2005. A framework for learn-
ing predictive structures from multiple tasks and unla-
beled data. JMLR, 6:1817?1853.
R. K. Ando and T. Zhang. 2007. Two-view feature gen-
eration model for semi-supervised learning. In ICML,
pages 25?32.
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT, pages 92?
100.
O. Chapelle, B. Scho?lkopf, and A. Zien, editors. 2006.
Semi-Supervised Learning. MIT Press, Cambridge,
MA.
T. G. Dietterich, R. H. Lathrop, and T. Lozano-Perez.
1997. Solving the multiple instance problem with
axis-parallel rectangles. Artificial Intelligence, 89(1-
2):31?71.
R. O. Duda, P. E. Hart, and D. G. Stork. 2000. Pattern
Classification. Wiley-Interscience Publication.
G. Nagy and G. L. Shelton. 1966. Self-corrective charac-
ter recognition system. IEEE Trans. Information The-
ory, 12(2):215?222.
N. Quadrianto, A. J. Smola, T. S. Caetano, and Q. V. Le.
2008. Estimating labels from label proportions. In
ICML ?08: Proceedings of the 25th international con-
ference on Machine learning, pages 776?783.
M. Seeger. 2000. Input-dependent regularization
of conditional density models. Technical re-
port, Institute for ANC, Edinburgh, UK. See
http://www.dai.ed.ac.uk/?seeger/papers.html.
W. E. Winkler. 1995. Matching and record linkage. In
Business Survey Methods, pages 355?384. Wiley.
18
