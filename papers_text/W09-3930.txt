Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 206?215,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
What do We Know about Conversation Participants: Experiments on
Conversation Entailment
Chen Zhang Joyce Y. Chai
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824, USA
{zhangch6, jchai}@cse.msu.edu
Abstract
Given the increasing amount of conversa-
tion data, techniques to automatically ac-
quire information about conversation par-
ticipants have become more important.
Towards this goal, we investigate the prob-
lem of conversation entailment, a task
that determines whether a given conversa-
tion discourse entails a hypothesis about
the participants. This paper describes
the challenges related to conversation en-
tailment based on our collected data and
presents a probabilistic framework that in-
corporates conversation context in entail-
ment prediction. Our preliminary exper-
imental results have shown that conver-
sation context, in particular dialogue act,
plays an important role in conversation en-
tailment.
1 Introduction
Conversation is a joint activity between its partic-
ipants (Clark, 1996). Their goals and their under-
standing of mutual beliefs of each other shape the
linguistic discourse of conversation. In turn, this
linguistic discourse provides tremendous informa-
tion about conversation participants. Given the
increasing amount of available conversation data
(e.g., conversation scripts such as meeting scripts,
court records, and online chatting), an important
question is what do we know about conversation
participants? The capability to automatically ac-
quire such information can benefit many appli-
cations, for example, development of social net-
works and discovery of social dynamics.
Related to this question, previous work has de-
veloped techniques to extract profiling informa-
tion about participants from conversation inter-
views (Jing et al, 2007) and to automatically iden-
tify dynamics between conversation participants
such as agreement/disagreement from multiparty
meeting scripts (Galley et al, 2004). We approach
this question from a different angle as a conversa-
tion entailment problem: given a conversation dis-
course D and a hypothesis H concerning its par-
ticipant, the goal is to identify whether D entails
H. For instance, in the following example, the first
hypothesis can be entailed from the dialogue seg-
ment while the second hypothesis cannot.
Example 1:
Dialogue Segment:
A: And where about were you born?
B: Up in Person Country.
Hypothesis:
(1) B was born in Person Country.
(2) B lives in Person Country.
Inspired by textual entailment (Dagan et al,
2005; Bar-Haim et al, 2006; Giampiccolo et al,
2007), conversation entailment provides an inter-
mediate step towards acquiring information about
conversation participants. What we should know
or would like to know about a participant can be
rather open. The type of information needed about
participants is also application-dependent and dif-
ficult to generalize. In conversation entailment, we
will not face this problem since hypotheses can be
used to express any type of information about a
participant one might be interested in. Although
hypotheses are currently given in our investiga-
tion, they can potentially be automatically gener-
ated based on information needs and/or theories
on cognitive status/mental models of conversation
participants. The capability to make correct entail-
ment judgements based on these hypotheses will
benefit many applications such as information ex-
traction, question answering, and summarization.
As a first step in our investigation, we collected
a corpus of conversation entailment data from
nineteen human annotators. Our data showed that
conversation entailment is more challenging than
206
the textual entailment task due to unique charac-
teristics about conversation and conversational im-
plicature. To predict entailment, we developed a
probabilisitic framework that incorporates seman-
tic representation of conversation context. Our
preliminary experimental results have shown that
conversation context, in particular dialogue acts,
play an important role in conversation entailment.
2 Related Work
Recent work has applied different approaches
to acquire information about conversation par-
ticipants based on human-human conversation
scripts, for example, to extract profiling infor-
mation from conversation interviews (Jing et al,
2007) and to identify agreement/disagreement
between participants from multiparty meeting
scripts (Galley et al, 2004). In human-machine
conversation, inference about conversation partic-
ipants has been studied as a part of user modeling.
For example, earlier work has investigated infer-
ence of user intention from utterances to control
clarification dialogue (Horvitz and Paek, 2001)
and recognition of user emotion and attitude from
utterances for intelligent tutoring systems (Litman
and Forbes-Riley, 2006). In contrast to previous
work, we propose a new angle to address informa-
tion acquisition about conversation participants,
namely, through conversation entailment.
This work is inspired by a large body of recent
work on textual entailment initiated by the PAS-
CAL RTE Challenge (Dagan et al, 2005; Bar-
Haim et al, 2006; Giampiccolo et al, 2007). Nev-
ertheless, conversation discourse is very different
from written monologue discourse. The conversa-
tion discourse is shaped by the goals of its partici-
pants and their mutual beliefs. The key distinctive
features include turn-taking between participants,
grounding between participants, and different lin-
guistic phenomena of utterances (e.g., utterances
in conversation tend to be shorter, with disfluency,
and sometimes incomplete or ungrammatical). It
is the goal of this paper to explore how techniques
developed for textual entailment can be extended
to address these unique behaviors in conversation
entailment.
3 Experimental Data
The first step in our investigation is to collect en-
tailment data to help us better understand the prob-
lem and facilitate algorithm development and eval-
uation.
3.1 Data Collection Procedure
We selected 50 dialogues from the Switchboard
corpus (Godfrey and Holliman, 1997). In each of
these dialogues, two participants discuss a topic
of interest (e.g., sports activities, corporate cul-
ture, etc.). To focus our work on the entailment
problem, we use the transcribed scripts of the di-
alogues in our experiments. We also make use of
available annotations such as syntactic structures,
disfluency markers, and dialogue acts.
We had 15 volunteer annotators read the se-
lected dialogues and create hypotheses about par-
ticipants. As a result, a total of 1096 entailment
examples were created. Each example consists of
a snippet from the dialogue (referred to as dia-
logue segment in the rest of this paper), a hypothe-
sis statement, and a truth value indicating whether
the hypothesis can be inferred from the snippet
given the whole history of that dialogue session.
During annotation, we asked the annotators to pro-
vide balanced examples for each dialogue. That is,
roughly half of the hypotheses are truly entailed
and half are not. Special attention was given to
negative entailment examples. Since any arbitrary
hypotheses that are completely irrelevant can be
negative examples, a special criteria is enforced
that any negative examples should have a major-
ity word overlap with the snippet. In addition, in-
spired by previous work (Jing et al, 2007; Galley
et al, 2004), we particularly asked annotators to
provide hypotheses that address the profiling in-
formation of the participants, their opinions and
desires, as well as the dynamic communicative re-
lations between participants.
A recent study shows that for many NLP an-
notation tasks, the reliability of a small number
of non-expert annotations is on par with that of
an expert annotator (Snow et al, 2008). It also
found that for tasks such as affection recogni-
tion, an average of four non-expert labels per item
are capable of emulating expert-level label qual-
ity. Based on this finding, in our study the en-
tailment judgement for each example was further
independently annotated by four annotators (who
were not the original contributors of the hypothe-
ses). As a result, on average each entailment ex-
ample (i.e., a pair of snippet and hypothesis) re-
ceived five judgements.
207
Figure 1: Agreement histogram of entailment
judgements
3.2 Data and Examples
Figure 1 shows a histogram of the agreements of
collected judgements. It indicates that conversa-
tion entailment is in fact a quite difficult task even
for humans. Only 53% of all the examples (586
out of 1096) are agreed upon by all human annota-
tors. The disagreement between users sometimes
is caused by language ambiguity since conversa-
tion scripts are often short and without clear sen-
tence boundaries. For example,
Example 2:
Dialogue Segment:
A: Margaret Thatcher was prime minister, uh,
uh, in India, so many, uh, women are heads
of state.
Hypothesis:
A believes that Margaret Thatcher was prime
minister of India.
In the utterance of speaker A, the prepositional
phrase in India is ambiguous because it can either
be attached to the preceding sentence, which suffi-
ciently entails the hypothesis; or it can be attached
to the succeeding sentence, which leaves it unclear
which country A believes Margaret Thatcher was
prime minister of.
Difference in recognition and handling of con-
versational implicature is another issue that led to
disagreement among annotators. For example:
Example 3:
Dialogue Segment:
A: Um, I had a friend who had fixed some, uh,
chili, buffalo chili and, about a week before
we went to see the movie.
Hypothesis:
A ate some buffalo chili.
Example 4:
Dialogue Segment:
B: Um, I?ve visited the Wyoming area. I?m
not sure exactly where Dances with Wolves
was filmed.
Hypothesis:
B thinks Dances with Wolves was filmed in
Wyoming.
In the first example, a listener could assume
that A follows the maxim of relevance. Therefore,
a natural inference that makes ?fixing of buffalo
chili? relevant is that A ate the buffalo chili. Sim-
ilarly, in the second example, the speaker A men-
tions a visit to Wyoming, which can be considered
relevant to the filming place of DANCES WITH
WOLVES. Some annotators recognized such rele-
vance and some did not.
Given the discrepencies between annotators, we
selected 875 examples which have at least 75%
agreement among the judgements in our current
investigation. We further selected one-third of this
data (291 examples) as our development data. The
experiments reported in Section 5 are based on this
development set.
3.3 Types of Hypotheses
The hypotheses collected from our study can be
categorzied into the following four types:
Fact. Facts about the participants. This includes:
(1) profiling information about individual partici-
pants (e.g., occupation, birth place, etc.); (2) activ-
ities associated with individual participants (e.g.,
A bikes to work everyday); and (3) social rela-
tions between participants (e.g., A and B are co-
workers, A and B went to college together).
Belief. Participants? beliefs and opinions about the
physical world. Any statement about the physical
world in fact is a belief of the speaker. Technically,
the state of the physical world that involves the
speaker him/herself is also a type of belief. How-
ever, here we assume a statement about oneself is
true and is considered as a fact.
Desire. Participants? desire of certain actions or
outcomes (e.g., A wants to find a university job).
These desires represent the states of the world the
participant finds pleasant (although they could be
conflicting to each other).
Intent. Participants? deliberated intent, in partic-
ular communicative intention which captures the
intent from one participant on the other partici-
pant such as whether A agrees/disagrees with B
208
on some issue, whether A intends to convince B
on something, etc.
Most of these types are motivated by the Belief-
Desire-Intention (BDI) model, which represents
key mental states and reflects the thoughts of
a conversation participant. Desire is different
from intention. The former arises subconsciously
and the latter arise from rational deliberation that
takes into consideration desires and beliefs (Allen,
1995). The fact type represents the facts about
a participant. Both thoughts and facts are criti-
cal to characterize a participant and thus impor-
tant to serve many other downstream applications.
The above four types account for 47.1%, 34.0%,
10.7%, and 8.2% of our development set respec-
tively.
4 A Probabilistic Framework
Following previous work (Haghighi et al, 2005;
de Salvo Braz et al, 2005; MacCartney et al,
2006), we approach conversation entailment using
a probabilistic framework. To predict whether a
hypothesis statement H can be inferred from a di-
alogue segment D, we estimate the probability
P (D  H|D,H)
Suppose we have a representation of a dia-
logue segment D in m clauses d1, . . . , dm and a
representation of the hypothesis H in n clauses
h1, . . . , hn. Since a hypothesis is the conjunc-
tion of the decomposed clauses, whether it can be
inferred from a segment is equivalent to whether
all of its clauses can be inferred from the seg-
ment. We further simplify the problem by assum-
ing that whether a clause is entailed from a dia-
logue segment is conditionally independent from
other clauses. Note that this conditional indepen-
dence assumption is an over-simplification, but it
gets things started. Therefore:
P (D  H|D,H)
= P (d1 . . . dm  h1 . . . hn|d1, . . . , dm, h1, . . . , hn)
= P (D  h1, . . . , D  hn|D,h1, . . . , hn)
=
n?
j=1
P (D  hj |D = d1 . . . dm, hj)
=
n?
j=1
P (d1 . . . dm  hj |d1, . . . , dm, hj) (1)
If this likelihood is above a certain threshold
(e.g., 0.5 in our experiments), then H is consid-
ered as a true entailment from D.
Given this framework, two important questions
are: (1) how to represent and automatically create
the clauses from each pair of dialogue segment and
hypothesis; and (2) how to estimate probabilities
as shown in Equation 1?
4.1 Clause Representation
Our clause representation is inspired by previ-
ous work on textual entailment (Dagan et al,
2005; Bar-Haim et al, 2006; Giampiccolo et al,
2007). Clause representation has several advan-
tages. First, it can be acquired automatically from
a parse tree (e.g., dependency parser). Second,
it can be used to facilitate both logic-based rea-
soning as in (Tatu and Moldovan, 2005; Bos and
Markert, 2005; Raina et al, 2005) or probabilis-
tic reasoning as in (Haghighi et al, 2005; de
Salvo Braz et al, 2005; MacCartney et al, 2006).
The key difference between our work and previ-
ous work on textual entailment is the representa-
tion of conversation discourse, which has not been
considered in previous work but is important for
conversation entailment, as we will see later.
More specifically, a clause is made up by two
components: Term and Predicate.
Term: A term can be an entity or an event. An
entity refers to a person, a place, an organization,
or other real world entities. This follows the con-
cept of mention in the Automatic Content Extrac-
tion (ACE) evaluation (Doddington et al, 2004).
An event refers to an action or an activity. For
example, from the sentence ?John married Eva in
1940? we can identify an event of marriage. Fol-
lowing the neo-Davidsonian representation (Par-
sons, 1990), all the events are reified as terms in
our representation.
Predicate: A predicate represents either a prop-
erty (i.e., unary) for a term or a relation (i.e., bi-
nary) between two terms. For example, an entity
company has a property of Russian as in the phrase
?a Russian company? (i.e., Russian(company)).
An event visit has a property of recently (i.e.,
recently(visit)) as in the phrase ?visit Brazil re-
cently?. From the phrase ?Prime Minister re-
cently visited Brazil?, there are binary relations:
PrimeMinister is the subject of the event visit (i.e.,
subj(visit, Prime Minister)) and Brazil is the
object of the visit (i.e., obj(visit, Brazil)).
This representation is a direct conversion from
the dependency structure and can be used to rep-
resent the semantics of utterances in the dialogue
209
segments and the semantics of hypotheses. For ex-
ample,
Example 5:
Dialogue Segment:
B: Have you seen Sleeping with the Enemy?
A: No. I?ve heard that?s really great, though.
B: You have to go see that one.
Hypothesis:
B suggests A to watch Sleeping with the Enemy.
Appendix A shows the dependency structure of
the dialogue utterances and the hypothesis from
Example 5. Appendix B shows the correspond-
ing clause representation of the dialogue segment
and the hypothesis. Note that in this represen-
tation, you and I are replaced with the respec-
tive participants. Since the clauses are generated
based on parse trees, most relational predicates are
syntactic-driven.
To facilitate conversation entailment, we fur-
ther augment the representation of a dialogue seg-
ment by incorporating conversation context. Ap-
pendix C shows the augmented representation for
Example 5. It represents the following additional
information:
? Utterance: A group of pseudo terms u1,
u2, . . . are used to represent individual utter-
ances.
? Participant: A relational clause
speaker(?, ?) is used to represent the speaker
of this utterance, e.g., speaker(u1, B).
? Content: A relational clause content(?, ?) is
used to represent the content of an utterance
where the second term is the head of the ut-
terance as identified in the parsing structure.
e.g., content(u3, heard)
? Dialogue act: A relational clause act(?, ?)
is used to represent the dialogue act of the
speaker for a particular utterance. e.g.,
act(u2, no answer). A set of 42 dialogue
acts from the Switchboard annotation are
used here (Godfrey and Holliman, 1997).
? Utterance flow: A relational clause
follow(?, ?) is used to connect each pair of
adjacent utterances. e.g., follow(u2, u1).
We currently do not consider overlap in utter-
ances, but our representation can be modified
to handle this situation by introducing
additional predicates.
4.2 Entailment Prediction
Given the clause representation for a conversation
segment and a hypothesis, the next step is to make
an entailment prediction (as in Equation 1) based
on two models: an Alignment Model and an Infer-
ence Model.
4.2.1 Alignment Model
The alignment model is to find alignments (or
matches) between terms in the clause representa-
tion for a hypothesis and those in the clause rep-
resentation for a conversation segment. We define
an alignment as a mapping function g between a
term x in the dialogue segment and a term y in the
hypothesis. g(x, y) = 1 if x and y are aligned;
otherwise g(x, y) = 0. Note that a verb can be
aligned to a noun as in g(sell, sale) = 1. It is also
possible that there are multiple terms from the seg-
ment mapped to one term in the hypothesis, or vice
versa.
For any two terms x and y, the problem of pre-
dicting the alignment function g(x, y) can be for-
mulated as a binary classification problem. We
used several features to train the classifier, which
include whether x and y are the same (or have the
same stem), whether one term is an acronym of the
other, and their WordNet and distributional simi-
larities (Lin, 1998).
Given an augmented representation with con-
versation context (as in Appendix C), we also
align event terms in the hypothesis (e.g., suggest
in Example 5) to (pseudo) utterance terms in the
dialogue segment. We call it a pseudo alignment.
This is currently done by a set of rules which asso-
ciate event terms in the hypotheses with dialogue
acts. For example, the event term suggest may be
aligned to an utterance with dialogue act of opin-
ion. Appendix D gives a correct alignment for Ex-
ample 5, in which g(u4, x1) = 1 is a pseudo align-
ment.
4.2.2 Inference Model
As shown in Equation 1, to predict the infer-
ence of the entire hypothesis, we need to calculate
the probability that the dialogue segment entails
each clause from the hypothesis. More specifi-
cally, given a clause from the hypothesis hj , a set
of clauses from the dialogue segment d1, . . . , dm,
and an alignment function g between them derived
by the method described in Section 4.2.1, we pre-
dict whether d1, . . . , dm entails hj under the align-
ment g using two different classification models,
210
depending on whether hj is a property or a rela-
tion (i.e. whether it takes one argument (hj(?)) or
two arguments (hj(?, ?))):
Given a property clause from the hypothe-
sis, hj(x), we look for all the property clauses
in the dialogue segment that describes the
same term as x, i.e. a clause set D? =
{di(x?)|di(x?) ? D, g(x?, x) = 1}. Then we pre-
dict whether hj(x) can be inferred from the
clauses in D? by binary classification, using a set
of features similar to those used in the alignment
model.
Given a relational clause from the hypothe-
sis, hj(x, y), we look for the relation between
the counterparts of x and y in the dialogue seg-
ment. That is, we find the set of terms X ? =
{x?|x? ? D, g(x?, x) = 1} and the set of terms
Y ? = {y?|y? ? D, g(y?, y) = 1} and look for the
closest relation between these two sets of terms in
the dependency structure. If there is a path be-
tween any x? ? X ? and any y? ? Y ? in the de-
pendency structure with a length smaller than a
threshold ?L, we predict that hj(x, y) can be in-
ferred. Note that our current handling of the re-
lational clauses is rather simplified. It only cap-
tures whether two terms from an hypothesis are
connected by any relation in the dialogue segment.
Appendix E shows the inference procedure of
the four hypothesis clauses in Example 5. For
each relational clause hj(x, y), the shortest path
between the correspondingX ? and Y ? has a length
of 3 or less, so each of these four clauses is en-
tailed from the dialogue segment. Based on Equa-
tion 1 we can conclude that the overall hypothesis
is entailed.
We trained the alignment model and the in-
ference model (e.g.,the threshold ?L) based on
the development data provided by the PASCAL 3
challenges on textual entailment.
5 Experimental Results
To understand unique behaviors of conversation
entailment, we focused our current experiments
on the development dataset (see Section 3.2).
We are particularly interested in how the tech-
niques for textual entailment can be improved for
conversation entailment. To do so, we applied
our entailment framework on the test data of the
PASCAL-3 RTE Challenge (Giampiccolo et al,
2007). Among 800 testing examples, our ap-
proach achieved an accuracy of 60.6%. This re-
sult is on par with the performance of the me-
dian system of accuracy 61.8% (z-test, p=0.63) in
the PASCAL-3 RTE Challenge. Our current ap-
proach is very lean on the use of external knowl-
edge. Its competitive performance sets up a rea-
sonable baseline for our investigation on conversa-
tion entailment. This same system, modified to tai-
lor linguistic characteristics of conversation (e.g.,
removal of disfluency), was used as the baseline in
our experiments.
5.1 Event Alignment
To understand the effect of conversation context
in the event alignment, we compared two configu-
rations of alignment model for events. The first
configuration is based on the clause representa-
tion of semantics of utterances (as shown in Ap-
pendix B). This is the same configuration as used
in textual entailment. The second configuration
is based on representation of both semantics from
utterances and conversation context (as shown in
Appendix C). We evaluate how well each config-
uration aligns the event terms based on the pair-
wise alignment decision: for any event term tH in
the hypothesis and any term tD in the dialogue,
whether the model can correctly predict that the
two terms should be aligned.
Figure 2(a) shows the comparison of F-measure
between the two models. Depending on the thresh-
old of alignment prediction, the precision and re-
call of the prediction vary. When the thresh-
old is lower, the models tend to give more align-
ments, resulting in lower precision and higher re-
call. When the threshold is higher, the models tend
to give fewer alignments, thus resulting in higher
precision but lower recall. When the threshold
is around 0.5, the alignment reaches its best F-
measure. Regardless of what threshold is cho-
sen, the model based on both utterance and con-
text consistently works better. Figure 2(b) shows
the breakdown based on the types of hypothesis (at
threshold 0.5). The model that incorporates con-
versation context consistently performs better for
all types. Its improvement is particularly signifi-
cant for the intent type of hypothesis.
These results are not surprising. Many event
terms in hypotheses (e.g., suggest, think, etc.) do
not have their counterparts directly expressed in
utterances in the dialogue discourse. Only through
the modeling of dialog acts, these terms can be
aligned to potential pseudo terms in the dialogue
211
segment. For the fact type hypotheses, the event
terms in the hypotheses generally have their coun-
terparts in the dialogue discourse. That explains
why the improvement for the fact type using con-
versation context is minimal.
(a) Overall comparison on F-measure
(b) Comparison for different types of hypothesis
Figure 2: Experimental results on event alignment
5.2 Entailment Prediction
Given correct alignments, we further evaluated
entailment prediction based on three configura-
tions of the inference model: (1) the same infer-
ence model learned from the textual entailment
data and tested on the PASCAL-3 RTE Challenge
(Text); (2) an improved model incorporating a
number of features relevant to dialogues (espe-
cially syntactic structure of utterances) based on
representations without conversation context as in
Appendix B (+Dialogue); (3) a further improved
model based on augmented representations of con-
versation context and using dialogue acts during
the prediction of entailment as in Appendix C
(+Context).
System Acc Prec Recall F
Text 53.6% 71.6% 29.3% 41.6%
+Dialogue 58.4% 84.1% 32.3% 46.7%
+Context 67.7% 91.7% 47.0% 62.1%
Table 1: Experimental results on entailment pre-
diction
For each configuration we present two evalua-
tion metrics: an accuracy of the overall prediction
and a precision-recall measurement for the posi-
tive entailment examples. All the evaluations are
performed on our development data, which has
56.4% of positive examples and 43.6% of negative
examples.
The evaluations results are shown in Table 1.
The system learned from textual entailment per-
forms lower than the prediction based on the
majority class (56.4%). Incorporating syntactic
features of dialogues did better but the differ-
ence is not statistically significant. Incorporat-
ing conversation context, especially dialogue acts,
achieves significantly better performance (z-test,
p < 0.005).
Table 2 shows the comparison of the three con-
figurations based on different types of hypothesis.
As expected, the basic system trained on textual
entailment is not capable for any intent type of
hypotheses. Modeling conversation context with
dialogue acts improves inference for all types of
hypothesis, with most significant improvement for
the belief, desire, and intent types of hypothesis.
6 Conclusion
This paper describes our initial investigation on
conversation entailment to address information ac-
quisition about conversation participants. Since
there are so many variables involved in the pre-
diction, our experiments have been focused on a
set of development data where most of the features
are annotated. This allowed us to study the effect
of conversation context in both alignment and en-
tailment. Our future work will enhance the cur-
rent approach by training the models based on our
development data and evaluate them on the test-
ing data. Conversation entailment is an important
task. Although the current exercise is targeted to
process conversation scripts from human-human
conversation, it can potentially benefit human ma-
chine conversation by enabling automated agents
to gain better understanding of their conversation
212
Fact Belief Desire Intent
System Acc F Acc F Acc F Acc F
Text 58.4% 51.3% 52.5% 37.3% 51.6% 34.8% 33.3% 0
+Dialogue 68.6% 62.6% 53.5% 36.1% 48.4% 33.3% 33.3% 0
+Context 70.8% 64.9% 67.7% 62.8% 58.1% 47.8% 62.5% 60.9%
Table 2: Experimental results on entailment prediction for different types of hypotheses
partners.
Acknowledgments
This work was partially supported by IIS-0347548
and IIS-0840538 from the National Science Foun-
dation. We thank the anonymous reviewers for
their valuable comments and suggestions.
References
James Allen. 1995. Natural language understanding.
The Benjamin/Cummings Publishing Company, Inc.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second pascal recognising
textual entailment challenge. In Proceedings of the
Second PASCAL Challenges Workshop on Recognis-
ing Textual Entailment, Venice, Italy.
Johan Bos and Katja Markert. 2005. Recognising tex-
tual entailment with logical inference. In Proceed-
ings of HLT-EMNLP, pages 628?635.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. In PASCAL Challenges Workshop on
Recognising Textual Entailment.
Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-
yakanok, Dan Roth, and Mark Sammons. 2005. An
inference model for semantic entailment in natural
language. In Proceedings of AAAI.
G. Doddington, A. Mitchell, M. Przybocki, and
L. Ramshaw. 2004. The automatic content extrac-
tion (ace) programctasks, data, and evaluation. In
Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC).
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech:
Use of bayesian networks to model pragmatic de-
pendencies. In Proceedings of ACL, pages 669?676.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recogniz-
ing textual entailment challenge. In Proceedings of
the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing, pages 1?9.
John J. Godfrey and Edward Holliman. 1997.
Switchboard-1 Release 2. Linguistic Data Consor-
tium, Philadelphia.
Aria Haghighi, Andrew Ng, and Christopher Manning.
2005. Robust textual inference via graph matching.
In Proceedings of HLT-EMNLP, pages 387?394.
Eric Horvitz and Tim Paek. 2001. Harnessing mod-
els of users? goals to mediate clarification dialog in
spoken language systems. In Proceedings of the 8th
International Conference on User Modeling, pages
3?13.
Hongyan Jing, Nanda Kambhatla, and Salim Roukos.
2007. Extracting social networks and biographical
facts from conversational speech transcripts. In Pro-
ceedings of ACL, pages 1040?1047.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of International
Conference on Machine Learning, pages 296?304.
Diane Litman and Katherine Forbes-Riley. 2006. Rec-
ognizing student emotions and attitudes on the basis
of utterances in spoken tutoring dialogues with both
human and computer tutors. Speech Communica-
tion, 48(5):559?590.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Man-
ning. 2006. Learning to recognize features of valid
textual entailments. In Proceedings of HLT-NAACL,
pages 41?48.
Terence Parsons. 1990. Events in the Semantics of En-
glish. A Study in Subatomic Semantics. MIT Press.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning
and abductive reasoning. In Proceedings of AAAI,
pages 1099?1105.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural lan-
guage tasks. In Proceedings of EMNLP, pages 254?
263.
Marta Tatu and Dan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. In Pro-
ceedings of HLT-EMNLP, pages 371?378.
213
APPENDIX
A Dependency Structure of Dialogue Utterances and Hypothesis in Example 5
Di
alo
gu
e S
eg
m
en
t:
B:
 H
av
e y
ou
se
en
Sl
ee
pin
g w
ith
 th
e E
ne
my
?
A:
 N
o. 
I 'v
e h
ea
rd
tha
t's
 re
all
y g
rea
t, t
ho
ug
h.
B:
 Y
ou
ha
ve
to 
go
se
et
ha
t o
ne
.
x 1
A
x 2
x 3
A
x 4
x 5
x 6
tho
ug
h(
?)
A
x 7
x 8
x 9
x 10
B
A
x 1
x 2
x 3
Hy
po
th
es
is:
B
su
gg
es
ts
A
to 
wa
tch
Sl
ee
pin
g w
ith
 th
e E
ne
my
.
ter
ms
pr
ed
ica
tes
B Clause Representation of Dialogue Segment and Hypothesis for Example 5s
ub
j(x
1,B
), 
ob
j(x
1,A
), 
ob
j(x
1,x
2),
 ob
j(x
2,x
3)
x 1=
su
gg
es
ts,
 x 2
=w
atc
h, 
x 3=
Sle
ep
ing
 w
ith
 th
e E
ne
my
, A
, B
Hy
po
th
es
is:
su
bj(
x 7,
A)
, o
bj(
x 7,
x 8)
, o
bj(
x 8,
x 9)
, o
bj(
x 9,
x 10
)
x 7=
ha
ve
, x
8=
go
, x
9=
se
e, 
x 10
=o
ne
, A
B:
su
bj(
x 4,
A)
, o
bj(
x 4,
x 6)
, s
ub
j(x
6,x
5),
 th
ou
gh
(x 4
)
x 4=
ha
ve
 he
ar
d, 
x 5=
tha
t, 
x 6=
is 
re
all
yg
re
at,
 A
A:
su
bj(
x 2,
A)
, o
bj(
x 2,
x 3)
, a
ux
(x 2
,x 1
)
x 1=
ha
ve
, x
2=
se
en
, 
x 3=
Sle
ep
ing
 w
ith
 th
e E
ne
my
, A
B:
Cl
au
se
s
Te
rm
s
Di
alo
gu
e S
eg
m
en
t:
C Augmented Clause Representation of Dialogue Segment in Example 5s
pe
ak
er
(u
4,B
), 
co
nte
nt(
u 4
,x 7
), 
ac
t(u
4,o
pin
ion
), 
su
bj(
x 7,
A)
, o
bj(
x 7,
x 8)
, o
bj(
x 8,
x 9)
, o
bj(
x 9,
x 10
)
u 4
,x
7=
ha
ve
, x
8=
go
, x
9=
se
e, 
x 10
=o
ne
, A
, B
B:
fol
low
(u
2,u
1),
 fo
llo
w(
u 3
,u 2
), 
fol
low
(u
4,u
3)
sp
ea
ke
r(u
2,A
), 
co
nte
nt(
u 2
,-)
, a
ct(
u 2
,no
_a
ns
we
r),
 
sp
ea
ke
r(u
3,A
), 
co
nte
nt(
u 3
,x 4
), 
ac
t(u
3,s
tat
em
en
t),
su
bj(
x 4,
A)
, o
bj(
x 4,
x 6)
, s
ub
j(x
6,x
5),
 th
ou
gh
(x 4
)
u 2
,u
3,x
4=
ha
ve
 he
ar
d, 
x 5=
tha
t, 
x 6=
is 
re
all
yg
re
at,
 A
A:
sp
ea
ke
r(u
1,B
), 
co
nte
nt(
u 1
,x 2
), 
ac
t(u
1,w
h_
qu
es
tio
n)
, 
su
bj(
x 2,
A)
, o
bj(
x 2,
x 3)
, a
ux
(x 2
,x 1
)
u 1
,x
1=
ha
ve
, x
2=
se
en
, 
x 3=
Sle
ep
ing
 w
ith
 th
e E
ne
my
, A
, B
B:
Cl
au
se
s
Te
rm
s
Di
alo
gu
e S
eg
m
en
t (
wi
th
 co
nt
ex
t r
ep
re
se
nt
at
ion
):
214
D The Alignment for Example 5
x 2=
se
en
x 3=
Sle
ep
ing
 w
ith
 th
e E
ne
my
 
BA x 5=
tha
t
x 7=
ha
ve
u 4
: a
ct(
u 4
,op
ini
on
)
Di
alo
gu
e S
eg
m
en
t
x 1=
su
gg
es
ts
x 3=
Sle
ep
ing
 w
ith
 th
e E
ne
my
 
BA x 2=
wa
tch
Hy
po
th
es
is
x 4=
ha
ve
 he
ar
d
x 8=
go
x 9=
se
e
x 10
=o
ne
x 6=
is 
re
all
yg
re
at
u 3
: a
ct(
u 3
,st
ate
me
nt)
u 2
: a
ct(
u 2
,no
_a
ns
we
r)
u 1
: a
ct(
u 1
,w
h_
qu
es
tio
n)
x 1=
ha
ve
E The Prediction of Inference for the Hypothesis Clauses in Example 5
x 3,
 x 5
, 
x 10x 3
x 2,
 x 9x 2
AA
BB
ye
s
ye
s
ye
s
ye
s
Hy
po
the
sis
 C
lau
se
 
En
tai
led
?
1
3
2
1
Pa
th 
Le
ng
th
ob
j(x
9,x
10
)
co
nte
nt(
u 4
,x 7
), 
ob
j(x
7,x
8),
 
ob
j(x
8,x
9)
co
nte
nt(
u 4
,x 7
), 
su
bj(
x 7,
A)
sp
ea
ke
r(u
4,B
)
Sh
or
tes
t P
ath
 be
tw
ee
n t
he
 
Al
ign
ed
 T
erm
s i
n t
he
 
De
pe
nd
en
cy
 St
ru
ctu
re 
of
 
Di
alo
gu
e S
eg
me
nt
x 2,
 x 9
u 4
u 4
u 4
Al
ign
ed
 T
erm
s i
n t
he
 
Di
alo
gu
e S
eg
me
nt
x 2
x 1
x 1
x 1
Te
rm
s i
n t
his
 C
lau
se
rel
ati
on
rel
ati
on
rel
ati
on
rel
ati
on
Cl
au
se
 T
yp
e
ob
j(x
2,x
3)
ob
j(x
1,x
2)
ob
j(x
1,A
)
su
bj(
x 1,
B)
 
Hy
po
the
sis
 C
lau
se
215
