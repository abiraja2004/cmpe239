A Stochast ic  Approach t O Parsing 
Geoffrey Sampson 
Department of L inguist ics  & Phonet ics 
Univers i ty  of Leeds 
I. S imulated anneal ing (e.g. 
K i rkpatr ick et al 1983, Br idle & Moore 
1984, Ackley et al 1985) is a stochast ic 
computat ional  technique for f inding optimal 
solut ions to combinator ia l  problems for 
which the combinator ia l  explos ion phenomenon 
rules out the poss ib i l i ty  of systemat ica l ly  
examining each alternat ive.  It is current ly  
being appl ied to the pract ical  problem of 
opt imiz ing the physical  design of computer 
circuitry,  and to the theoret ical  problems 
of resolv ing patterns of auditory and visual  
st imulat ion into meaningfu l  arrangements of 
phonemes and three-d imensional  objects. 
Grammatica l  pars ing -- resolving unanalysed 
l inear sequences of words into meaningful  
grammatica l  structures -- can be regarded as 
a percept ion problem logical ly analogous to 
those just cited, and s imulated anneal ing 
holds great promise as a parsing technique. 
S imulated anneal ing can most 
d irect ly  be expla ined via a physical  
analogy. Consider the logical space of 
a l ternat iw~s in some large-scale 
combinator ia l  problem as a chunk of 
mountainous terrain, in which the a l t i tude 
of any point corresponds to the relat ive 
"goodness" of that part icular  solut ion (the 
lower, the better).  We want to f ind the 
lowest point, but there are far too many 
points for each to be considered separately. 
We might try to locate the low point by 
dropping a bal l  onto the terr i tory at random 
and hoping it wil l  roll down to the low 
point. This corresponds to randomly 
choosing a part icu lar  overal l  solut ion to 
the combinator ia l  problem, and then 
consider ing a series of modi f icat ions to 
indiv idual  components of the solution, 
adopt ing the modi f icat ions whenever they 
improve the overal l  solut ion and reject ing 
them otherwise. But the ball is very 
unl ike ly  to reach the low point. Much more 
probably, it wi l l  rol l  a short way downhi l l  
and qome to rest in a "local minimum", a 
place .where  all immediate moves are uphil l  
even though, some distance away, there are 
places much lower than the spot where the 
bal l  has halted. 
In this situation, a good way of 
improving hhe search technique would be to 
pick up the landscape and shake it, so that 
the bal l  does not invar iably roll downhi l l  
but sometimes bounces over obstruct ions.  
one would begin by shaking hard, so that 
even the highest  peaks can be cleared, and 
then gradual ly  reduce the ampl i tude of 
shaking so that the ball searches for 
lowness in terms of progress ive ly  f iner 
detail .  In computat ional  terms, rather than 
dec id ing whether  to adopt each of a series 
of modi f icat ions to the randomly-chosen 
init ial  posi t ion simply by reference to 
whether it y ields a gain or a loss, one 
decides by reference to whether it yields a 
loss that is greater  than a number whose 
magnitude tends to decrease as the process 
continues. This is s imulated anneal ing. 
Not all combinator ia l  problems are 
amenable to the anneal ing technique. If the 
des i red low point in the mountainous terrain 
of the analogy happened to be at the bottom 
of a deep, narrow mineshaft  sunk from a high 
place, anneal ing would not help to find it. 
But the logical geometry of many real- l i fe 
combinator ia l  phenomena is more like the 
geometry of natural  mountains, where there 
is a strong tendency for re lat ively 
low- ly ing points to be adjacent to many 
other re lat ively low-ly ing points. For such 
phenomena, s imulated anneal ing can be an 
eff ic ient way of arr iv ing at optimal 
solutions. 
2. The appl icabi l i ty  of anneal ing as 
a parsing technique presupposes a 
stat ist ical  approach to NL analysis which 
wil l  itself be unfami l iar  to many readers. 
At this point I must therefore digress from 
the anneal ing concept in order br ief ly  to 
descr ibe the stat is t ics-based NL research 
paradigm within which I am working, and to 
which s imulated anneal ing appears to offer 
an important contr ibut ion.  
Much current work in parsing, as 
represented in books such as King (1983), 
Jones & Wilks (1983), analyses input by 
systemat ica l ly  checking its propert ies 
against the var ious poss ib i l i t ies  a l lowed by 
a grammar which specif ies the language in 
quest ion as a wel l -def ined class of 
sentences. The grammar may be in the form 
of an ATN, a GPSG, or in some other form, 
and the checking process may operate in very 
diverse ways; but all these approaches have 
in common the not ion of a c learcut boundary 
between a (probably very r ich and complex) 
class of wel l - formed inputs, and other 
inputs which simply do not belong to the 
language under analysis.  There are major 
d i f f icu l t ies  in making such parsers work 
adequately  with authentic,  unedited input 
drawn from unrestr ic ted domains. NLs are so 
endless ly  d iverse and unpredictable  in the 
turns of phrase they display that many 
people f ind it extremely d i f f icu l t  to 
bel ieve that any sets of rules, no matter 
how complex, can fully def ine them. It 
151 
remains as true as it was sixty years ago 
that "All grammars leak" (Sapir 1921: 38). 
The Unit for Computer Research on 
the Engl ish Language (UCREL) of the 
Univers i ty  of Lancaster,  led by Geoffrey 
Leech and Roger Garside, with whom I remain 
associated since my recent move to Leeds, 
has made considerable progress in recent 
years in developing automatic rout ines which 
succeed in analys ing authent ic  text using 
techniques which do not assume the existence 
of a c learcut grammat ica l /ungrammat ica l  
d ist inct ion (cf. Garside et al: 
forthcoming).  The first major UCREL 
achievement was the CLAWS word-tagging 
system (see e.g. Atwel l  et al 1984). Since 
1982 CLAWS has been assigning part -o f -speech 
tags, drawn from a f inely-d i f ferent iated,  
134-member tag-set, to words of authent ic  
running Engl ish text (which are often 
grammat ica l ly  many ways ambiguous in 
isolation), with a consistent  accuracy level 
of 96-97% of words correct ly tagged -- a 
f igure which we bel ieve can be further 
improved by tuning the system in var ious 
ways. This is an achievement to which we 
have been unable, despite extensive 
enquir ies, to d iscover near rivals. 
The s igni f icant point about CLAWS 
is that it embodies no knowledge whatever  of 
the overal l  grammatical  archi tecture of 
Engl ish  sentences. Instead, it uses an 
empir ica l ly -der ived matr ix of relat ive 
t rans i t ion-probabi l i t ies  between pairs of 
adjacent tags, together with information 
enabl ing a set of candidate tags to be 
identi f ied for any given word taken in 
isolat ion (using rules which ?efer to the 
last letters of the word's spell ing, 
together with a list of c. 7200 exceptions).  
S impl i fy ing greatly, CLAWS works by forming 
all possible paths through sequences of tags 
which are candidates for the words of a 
sentence, and choosing the path for which 
the product of the successive transi t ion 
probabi l i t ies  is highest. As a matter of 
policy, no entr ies in the matr ix  of 
tag- t rans i t ion probabi l i t ies  are zero; we 
know as well  as other l inguists that fai lure 
to observe a part icular  t ransi t ion in our 
data does not imply that the transi t ion is 
"ungrammatical" ,  and therefore even 
unobserved transi t ions are ass igned a small 
posit ive probabi l i ty.  Thus it is true to 
say that the system "knows" nothing about 
Engl ish in the sense of drawing sharp 
d ist inct ions between grammatical  and 
ungrammatical  sequences; it deals 
exclusively in re lat ive l ikel ihoods. Yet 
this whol ly  "unintel l igent"  system works 
extremely well. It is easy to make CLAWS 
fail by inputt ing "trick" sentences of the 
kind often encountered in l inguist ics 
textbooks, but the lesson of CLAWS is that 
such sentences are notably rare in real 
life. 
We are current ly developing a 
CLAWS-l ike solut ion to the harder problem Of  
grammatical  parsing. We have bui lt  up a 
database of manual ly -parsed sentences, from 
152 
which we extract stat ist ics that al low a 
l ikel ihood measure to be determined for any 
logical ly possib le non- leaf  const i tuent  of a 
parse-tree. That is, given a pair ing of a 
mother- label  with a sequence of 
daughter- labels ,  say the pair <J, NN JJ P>, 
the l ikel ihood funct ion wil l  return a f igUre 
for the relat ive frequency with which (in 
this case) an adject ive phrase consists of 
s ingular common noun + adject ive + 
preposi t ional  phrase. (In the case quoted 
the l ikel ihood wil l  probably be low, but it 
ought not to be zero: I selected the 
example after encounter ing,  in a book opened 
at random, the adject ive phrase under l ined 
in "the value obtained must be a#s ignmen~ 
compat ib le  with the ~fKP_q of  the var iable 
...".) We assume, I believe- with 
just i f icat ion,  that with only minor special  
provisos the l ikel ihood of a full parse-t ree 
can be ident i f ied with a simple funct ion of 
the l ikel ihood of each of its non- leaf 
nodes. 
3. The most d irect  way to imitate the 
CLAWS technique in the pars ing domain would 
be to generate all poss ib le t ree-st ructures  
for a given sentence taken as a sequence of 
word-tags,  and all possible label l ings of 
each of those structures, and choose the 
tree whose overal l  p laus ib i l i ty  f igure is 
highest. Unl ike in the case of 
word-tagging,  however, for parsing this 
approach is whol ly  impractical.  The average 
sentence in our database is about 22 words 
long, and the set of nonterminal  symbols 
recognized by our parsing scheme has a lmost  
thirty members; the number of a l ternat ive 
log ica l ly -poss ib le  label led tree structures 
having 22 terminal  nodes is astronomical .  I 
have therefore begun to exper iment with 
s imulated anneal ing as  a solut ion to the 
problem. The grammatica l  stat ist ics in the 
exper iment descr ibed here are far cruder 
than would be needed for a fu l l -scale 
anneal ing parser, but init ial  results are 
nevertheless promising. 
4. To apply the anneal ing technique 
to the parsing problem, it is necessary: 
(i) to state a t ree-evaluat ion function; 
(ii) to def ine a class of local changes to 
trees, such that any log ica l ly -poss ib le  tree 
can be converted to any other by apply ing a 
series of changes drawn from the class (we 
cannot a l low the init ial  randomly-chosen 
tree to el iminate the poss ib i l i ty  of ever 
reaching some other tree which might be .the 
correct one); and (iii) to def ine an 
anneal ing schedule. 
Tree-eva luat ion  in my exper iment 
is based on stat ist ics of 
const i tuent -daughter  t rans i t ion frequencies:  
a const i tuent  label led A and having 
daughters label led B C D is g iven a value 
der ived from the observed frequencies of the 
transit ions A/\[B, A/BC, A/CD? A/D\]. (The 
functions which der ive node values from 
daughter - t rans i t ion  frequencies,  and tree 
values from node values, are more complex 
than simple averaging, which is 
unsat i s factory  because it too easi ly  al lows 
an indiv idual  "bad" value in a candidate 
parse- t ree to be of fset  by several  "good" 
values e lsewhere in the tree. I do not give 
deta i ls  of the funct ions current ly  used.) 
3in the experiment,  the stat ist ics 
referred to a very small set of 
broad ly -def ined node- labels ,  compris ing 14 
nonterminal  labels and 30 word-c lass  labels. 
Our database uses a pars ing scheme which 
recognizes d is t inct ions  much finer than this 
-- we have seen that 134 word-c lasses  are 
d ist inguished,  and nonterminal  labels can 
include subcategory symbols which in theory 
permit on the order of 60,000 d ist inct  
labels~ However, most of this information 
was d iscarded for the sake of s impl i fy ing 
the pi lot  exper iment.  
For point (ii) above, \[\[ def ine a 
possible move as fol lows. Given a 
parse-tree~ select a node other than the 
root at random. Disconnect  it from its 
mother? It wi l l  then be located with in an 
"arch" of nodes whose left and r ight bases 
are respect ive ly  the last word before and 
the f irst word after the d isconnected 
const i tuent,  and whose "keystone" is the 
lowest node dominat ing both of those words. 
Choose at random e i ther  any node located on 
the arch other than the two bases, or any 
l ink between two nodes in the arch? In the 
former case, attach the d isconnected node to 
the chosen node as an extra daughter,  and 
re label  the new mother  of the d isconnected 
node with a randomly-chosen label. In the 
latter case, create a new node on the chosen 
link between exist ing nodes, label it with a 
randomly-chosen label, and attach the 
d isconnected node to it as a sister of the 
node at the lower end of the chosen link. 
In either case, if the ex-mother  of the 
d isconnected  node is left with only one 
daughter,  t l l en  delete the ex-mother by 
merging its upward and downward l inks (if 
the ex-mother is the root node, its 
remain ing (laughter becomes the new root and 
i s  accord ing ly  re label led S). It is easy to 
show that any tree can be der ived from any 
other tree via a series of moves of this 
kind. 
There is no "magic formula" to 
determine the ideal anneal ing schedule for a 
given class of combinator ia l  opt imizat ion 
problems: this depends on the geometry of 
the logical space of poss ib i l i t ies ,  and has 
to be d iscovered by experiment? The 
requi rements  are that anneal ing must begin 
at a high enough "temperature" for the 
system to be thoroughly  "melted" (that is, 
the factor by which the negat iv i ty  of 
loca l ly -negat ive  moves is d iscounted must be 
suf f ic ient ly  large for moves to occur at 
random with no s igni f icant  bias towards 
loca l ly -pos i t ive  moves),  and "cool ing" must 
take place slowly enough for adequate 
searching of the poss ib i l i ty -space to occur. 
(If the init ial  temperature is 
unnecessar i ly  high, or cool ing unnecessar i ly  
protracted,  a penalty wi l l  be paid in extra 
process ing for l i tt le or no gain in u l t imate 
accuracy of search.) "Temperature" might be 
treated as a constant f igure which is added 
to the result  of subtract ing previous 
l ike l ihood-va lue from subsequent 
l ike l ihood-va lue in determin ing whether a 
move under cons iderat ion yields a net gain 
and is therefore adopted? What is usual, 
however, is to strengthen the analogy with 
thermodynamics  by adding, not a constant 
figure, but a f igure drawn randomly from a 
Gauss ian distr ibut ion,  with "temperature" 
standing for the standard deviat ion of the 
d istr ibut ion.  Thus, even at a high 
temperature it wil l  sometimes happen that a 
s l ight ly loca l ly -negat ive move is rejected, 
and even at a low temperature it will 
occas iona l ly  happen that a strongly 
loca l ly -negat ive move is accepted. 
(Local ly-pos i t ive and neutral  moves are 
always accepted at any temperature?)  
5. Let me i l lustrate by quot ing one 
of the f irst anneal ing runs carr ied out by 
the system, on a short sentence input as 
d j j n o v i d j n . 
Br ief  glosses for these symbols are: d, 
determiner;  j, adject ive; n, s ingular noun; 
o, modal verb or ~l_qo; v, main verb; i, 
preposit ion;  o, sentence- f ina l  punctuat ion 
mark. Thus the sequence stands for a 
sentence such as T~le ~ brown fox wil l  
j~m_~ over the laz Z ~\ ] i  This is of course 
an art i f ic ia l ly  simple example, and if 
authent ic  language were commonly as 
"wel l -behaved" as this then the case for 
using stochast ic  pars ing techniques would be 
weak. However,  notice that the technique 
embodies no concept of a contrast  between 
wel l - formed and deviant  strings, so that in 
pr inc ip\ ]e  it should be as easy to set an 
anneal ing parser to see\]< the "least 
implaus ib le"  analysis of a highly deviant  
input as to seek the correct analysis  of a 
thoroughly  wel l - formed input. The reason 
for beginning with a simple example is that 
I ant ic ipate that the per formance of the 
system wil l  become more sensit ive to detai ls  
of the evaluat ion funct ion and anneal ing 
schedule as inputs become more complex and 
less wel l - formed,  and at present I have only 
begun to explore a l ternat ive evaluat ion 
funct ions and anneal ing schedules. 
The schedule used for the run to 
be i l lustrated was as fol lows. The 
structure in i t ia l ly  ass igned to the str ing 
was the "flat" structure in which each word 
is an immediate const i tuent  of the root: 
\[S dj jnovidjn.  \] This tree is ass igned the 
value -2.26 by the t ree-evaluat ion function. 
The init ial  temperature (standard deviat ion 
of the Gaussian) was I. The temperature was 
reduced by 3% after every f i f t ieth 
success ive attempt to change the tree~ The 
system was deemed to have "frozen" at the 
f irst temperature-drop at which each of the 
100 preceding attempts to change the tree 
ei ther had been re jected or left the value 
of the tree unchanged. 
153 
To give the reader a feeling for 
the way an annealing run proceeds, I display 
the situation reached after every hundredth 
attempted tree-change. On each line I show 
the temperature reached immediately before 
the drop which occurs at that point, the 
proportion of the last hundred attempted 
Attempts Temp. Changes Value 
100 
200 
300 
400 
5O0 
600 
700 
800 
900 
1000 
1100 
1200 
1300 
1400 
1500 
1600 
1700 
1800 
1900 
2000 
2100 
0.970  
0 .913  
0 ,859  
0 808 
0 ,760  
0 715 
0 673 
0 ,633  
0 ,596  
0 561 
0 ,527  
0 496 
0 467 
0 439 
0.413 
0.389 
0.366 
0344 
0 324 
0,305  
0287 
93 -1.31 
93 -1.52 
92 -1.02 
88 -2.30 
82 0.00976 
90 -0.536 
66 1.64 
69 -1.51 
73 -0.0562 
75 -0.984 
58 1.50 
66 -0.184 
63 0.668 
54 0.325 
57 0.760 
18 4.93 
8 5.21 
11 5.46 
7 5.70 
5 6.63 
5 6.63 
changes which were accepted, the value of 
the current tree, and the tree itself. 
Nonterminal symbols are represented by  
capitals written immediately after the 
opening bracket of the constituent they 
label; closing brackets are unlabelled. 
Current Tree 
\[Sdjj\[Rn\[Lo\[G\[Fvi\]\[J\[N\[Gdj\]n\].\]\]\]\]\] 
\[S\[Wd\[D\[Jjj\]no\]\]\[Fv\[Vidj\]\]\[Pn.\]\] 
\[S\[Pd\[Dj\[Jjn\]\]\[Nov\]i\[R\[Adj\]n\]\].\] 
\[Sd\[Dj\[V\[Fjn\]\[P\[V\[Gov\]\[Ji\[Ldj\]\]n\].\]\]\]\] 
\[S\[Vdj\]\[Njn\[W\[Tovidj\]n\]\].\] 
\[S\[N\[Jd\[T\[Pjj\]\[S\[N\[Tn\[Jov\]\]\[Tid\]\]j\]\]\]n\].\] 
\[S\[Sd\[Njj\[Nn\[No\[Vvi\]\]\]d\[Njn\]\]\].\] 
\[S\[N\[Nd\[Sjj\]\]no\]\[J\[A\[Tvi\]\[Tdjn\]\].\]\] 
\[S\[N\[D\[F\[Ad\[S\[Fjj\]\[Nno\]\]\]v\]\[Tid\]j\]n\].\] 
\[Sd\[D\[L\[Gj\[Njn\]\]\[Wov\]\]\[F\[F\[Lid\]jn\].\]\]\] 
\[S\[P\[Mdj\]\[Njn\]\[Jovi\]\[Ndj\]n\].\] 
\[S\[Ndjjn\]\[Novid\]j\[Mn.\]\] 
\[S\[N\[Dd\[Njj\]n\]ov\[Lidj\]n\].\] 
\[S\[Nd\[Njj\]n\]\[P\[Jo\[Rvi\[Sdj\]\]n\].\]\] 
\[S\[Nd\[V\[Njj\]n\]o\]\[S\[T\[Tvi\]\[Dd\[Njn\]\]\].\]\] 
\[S\[Ndjjn\[F\[Vov\]\[Pid\]\]\]\[Njn\].\] 
\[S\[Ndjjn\]\[F\[Vov\]\[Pi\[Nd\[Njn\]\]\]\].\] 
\[S\[N\[Ndjjn\]\[F\[Vov\]\[Pi\[Ndjn\]\]\]\].\] 
\[S\[Ndjjn\]\[F\[Vov\]\[Pi\[Ndjn\]\]\].\] 
\[S\[Ndjjn\]\[Vov\]\[Pi\[Ndjn\]\].\] 
\[S\[Ndjjn\]\[Vov\]\[Pi\[Ndjn\]\].\] 
On this run the system froze at 
temperature 0.287, after 2100 attempted 
changes of which 1173 were accepted. The 
structure attained at freezing is the 
correct structure for the input sequence, 
according to our parsing scheme. (The 
symbols N, P, V stand for noun phrase, 
prepositional phrase, and verb phrase -- the 
latter in our terms referring to a sequence 
of auxiliary and main verbs, not including 
object, complement, etc. We recognize no 
internal structure in a noun phrase such as 
the ~uick brown fox.) 
Not all runs of this pilot system 
have been as completely successful as this, 
though none have frozen on totally crazy 
trees. Yet the range of possibilities out 
of which the system has winnowed the correct 
analysis includes massive numbers of utterly 
crazy structures: note for instance how in 
the early stages of the run illustrated the 
system has considered a tree including a 
genitive phrase (G) consisting of a finite 
clause (F) followed by an adjective phrase 
(J) -- a constituent which linguistically 
makes no sense at all. Considering how many 
alternative logically-possible solutions are 
available to the system, a few thousand 
steps seems a small number by which to reach 
the correct solution or even its vicinity. 
Although at present some mistakes are made, 
there is plenty of scope for improving 
performance by refining the grammatical 
statistics and evaluation function, and 
modifying the annealing schedule. At this 
admittedly very early stage I regard the 
prospects for parsing by annealing as highly 
promising. 
154 
6. Simulated annealing appeals 
strongly to some writers (e.g. Bridle & 
Moore 1984: 315) as a model of psychological 
perception mechanisms. In the case of 
grammatical parsing, though, there is one 
respect in which the model presented so far 
is quite implausible psychologically: it 
ignores the left-to-right sequential manner 
in which humans procesS written as well as 
spoken language. There is a natural way to 
incorporate time into an annealing parser 
which not only is psychologically plausible 
but promises greatly to increase its 
efficiency as a practical automatic system. 
Rather than a whole sentence being 
submitted to the annealing system at once, 
in a "dynamic" annealing system parsing 
would proceed in a series of rounds. The 
input to the nth round would be an annealed 
parsing of the first n-1 words of the 
sentence, followed by the nth word; 
annealing would begin anew at melting 
temperature on this input. The opportunity 
for efficiency would arise from the fact 
that NL grammar only rarely forces the 
reader to backtrack -- the insight on which 
Mitchell Marcus's Parsifal system was 
founded (Marcus 1980). Marcus's strategy 
involved a total exclusion of backtracking 
from his central parsing system, with 
"garden path" sentences being handed over to 
a quite separate "higher level problem 
solver" for processing. However, Marcus's 
predictions about a sharp categorization of 
NL sentences into garden-paths and 
non-garden-paths have provoked considerable 
criticism. In a dynamic annealing parser, 
all parts of the curreDtr tree would at all 
stages be available to revision, but the 
relative rarity of the need for backtracking 
could be exploited by adding a bias to the 
function which randomly selects nodes for 
reconsideration, so that nodes are 
reconsidered less frequently as they become 
"older". Since the bulk of computing time 
in an annealing parser would undoubtedly be 
consumed in calculating gains and losses for 
candidate tree-changes, this system of 
concentrating the search for profitable 
tree-changes on the areas of the tree where 
such changes are most likely to be found 
could be a good means of saving processing 
time by reducing the total number of moves 
considered. 
7. A problem that will not have 
escaped the reader's attention is that I 
have discussed parsing purely in terms of 
finding surface parse-trees (which hapEens 
to be the task which the UCREL group are 
engaged on). It is not obvious how to 
extend the annealing approach so as to yield 
deep parses. However, there is nothing 
about simulated annealing that makes it 
intrinsically inapplicable to the task of 
deep parsing. What needs to be done is to 
define a class of logically-possible deep 
parse-trees and a class of moves between 
them, and to find an evaluation function 
which takes any pairing of a deep structure 
with a surface word-sequence into a 
likelihood-value. This task is very 
different in kind from the work currently 
done by theoretical linguists and AI 
researchers interested in underlying or 
logical grammar, who tend to have little 
time for statistical thinking, but that is 
not to say that the task is necessarily 
senseless or impossible. Deep parsing, if 
possible at all, will presumably need to 
exploit semantic/"inferencing" 
consideratJons as well as information about 
grammar in the narrow sense, but nothing 
says that these matters might not be built 
into the evaluation function. 
8 Finaiiy, i t  may be that annealing 
is:useless as a parsing techn ique because 
the geometry of NL parsing space is wrong. 
Perhaps the space of English parse-trees 
(whether surface or deep) resembles the 
Witwatersrand rather than the Cotswolds, 
being an upland plateau riddled with deep 
goldmines rather than a rolling landscape 
whose treasures lie exposed in valley 
bottoms. I conjecture that NLs are 
Cotswold-like rather than Rand-like, and 
that, if they were not, humans could not 
understand them. Only empirical research 
using authentic data can settle the 
question. 
REFERENCES 
Ackley, D.\[I., G.E. Hinton, & T.J. Sejnowski 
1 985 "A learning algorithm for 
Boltzmann machines". C_~nit ive 
Science 9.147-69. 
Atwell~ E.S., G.N. Leech, & R.G. Garside 
1984 "Analysis of the LOB Corpus: 
progress and prospects". In J. Aarts 
& W. Meijs, eds., Cor~ ~ t i c s .  
Rodopi. 
Bridle, J.S. & RoK. Moore 1984 "Boltzmann 
machines for speech pattern 
processing". Proceedings of the 
Institute of Acoustics vol. 6 pt. 4 
pp. 315-22. 
Garside, R.G., G.N. Leech, & G.R. Sampson, 
eds. Forthcoming. The Computational 
Analysis of Enqlish. Longman. 
Jones, K.S. & Y.A. Wilks, eds. 1983 
Automatic Natural Language Parsinq. 
Ellis Horwood. 
King, M., ed. 1983 Parsinq Natural 
L a n ~  Academic Press. 
Kirkpatrick, S., C.D. Gelatt, & M.P. Vecchi 
1983 "Optimization by simulated 
annealing". Science 220.671-80. 
Marcus, M.P. 1980 A Theor~ o_~f Syntaqtic 
Recognition for Natural Language. MIT 
Press. 
Sapir, E. 1921. La\[i u a ~  Harcourt, Brace 
& World. 
155 
