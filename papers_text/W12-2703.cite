Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3:1137?1155.
Yoshua Bengio. 2007. Learning Deep Architectures for
AI. Technical report, Universit e de Montreal.
S. F. Chen and J. Goodman. 1999. An empirical study of
smoothing techniques for language modeling. Com-
puter Speech and Language, 13(4).
Stanley F. Chen, Lidia Mangu, Bhuvana Ramabhadran,
Ruhi Sarikaya, and Abhinav Sethy. 2009. Scaling
shrinkage-based language models. In Proc. ASRU
2009, pages 299?304, Merano, Italy, December.
Stanley F. Chen. 2008. Performance prediction for expo-
nential language models. Technical Report RC 24671,
IBM Research Division.
George E. Dahl, Marc?Aurelio Ranzato, Abdel rah-
man Mohamed, and Geoffrey E. Hinton. 2010.
Phone Recognition with the Mean-Covariance Re-
stricted Boltzmann Machine. In Proc. NIPS.
Ahmad Emami. 2006. A neural syntactic language
model. Ph.D. thesis, Johns Hopkins University, Bal-
timore, MD, USA.
Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh.
2006. A Fast Learning Algorithm for Deep Belief
Nets. Neural Computation, 18:1527?1554.
H-K. J. Kuo, L. Mangu, A. Emami, I. Zitouni, and Y-
S. Lee. 2009. Syntactic features for Arabic speech
recognition. In Proc. ASRU 2009, pages 327 ? 332,
Merano, Italy.
Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cer-
nocky, and Sanjeev Khudanpur. 2010. Recurrent neu-
ral network based language model. In Proc. INTER-
SPEECH 2010, pages 1045?1048.
Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas
Burget, and Jan Cernocky. 2011a. Strategies for train-
ing large scale neural network language models. In
Proc. ASRU 2011, pages 196?201.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky, and Sanjeev Khudanpur. 2011b. Exten-
sions of recurrent neural network language model. In
Proc. ICASSP 2011, pages 5528?5531.
Andriy Mnih and Geoffrey Hinton. 2008. A scalable hi-
erarchical distributed language model. In Proc. NIPS.
Abdel-rahman Mohamed, George E. Dahl, and Geoffrey
Hinton. 2009. Deep belief networks for phone recog-
nition. In Proc. NIPS Workshop on Deep Learning for
Speech Recognition and Related Applications.
Frederic Morin and Yoshua Bengio. 2005. Hierarchical
probabilistic neural network language model. In Proc.
AISTATS05, pages 246?252.
Douglas B. Paul and Janet M. Baker. 1992. The de-
sign for the wall street journal-based csr corpus. In
Proc. DARPA Speech and Natural Language Work-
shop, page 357362.
Tara N. Sainath, Brian Kingsbury, and Bhuvana Ramab-
hadran. 2012. Improvements in Using Deep Belief
Networks for Large Vocabulary Continuous Speech
Recognition. Technical report, IBM, Speech and Lan-
guage Algorithms Group.
Ruhi Sarikaya, Mohamed Afify, and Brian Kingsbury.
2009. Tied-mixture language modeling in continuous
space. In HLT-NAACL, pages 459?467.
Holger Schwenk and Jean-Luc Gauvain. 2005. Training
neural network language models on very large corpora.
In Proc. HLT-EMNLP 2005, pages 201?208.
Holger Schwenk. 2007. Continuous space language
models. Comput. Speech Lang., 21(3):492?518, July.
Frank Seide, Gang Li, Xie Chen, and Dong Yu. 2011.
Feature Engineering in Context-Dependent Deep Neu-
ral Networks for Conversational Speech Transcription.
In Proc. ASRU.
Hagen Soltau, George. Saon, and Brian Kingsbury. 2010.
The IBM Attila speech recognition toolkit. In Proc.
IEEE Workshop on Spoken Language Technology,
pages 97?102.
Hai Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Francois Yvon. 2011. Structured out-
put layer neural network language model. In Pro-
ceedings of IEEE International Conference on Acous-
tic, Speech and Signal Processing, pages 5524?5527,
Prague, Czech Republic.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proceedings of DARPA
Broadcast News Transcription and Understanding
Workshop, pages 270 ? 274, Lansdowne, VA, USA.