Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 430?438,
Beijing, August 2010
A Structured Vector Space Model for Hidden Attribute Meaning
in Adjective-Noun Phrases
Matthias Hartung and Anette Frank
Computational Linguistics Department
Heidelberg University
{hartung, frank}@cl.uni-heidelberg.de
Abstract
We present an approach to model hid-
den attributes in the compositional se-
mantics of adjective-noun phrases in a
distributional model. For the represen-
tation of adjective meanings, we refor-
mulate the pattern-based approach for at-
tribute learning of Almuhareb (2006) in
a structured vector space model (VSM).
This model is complemented by a struc-
tured vector space representing attribute
dimensions of noun meanings. The com-
bination of these representations along the
lines of compositional semantic principles
exposes the underlying semantic relations
in adjective-noun phrases. We show that
our compositional VSM outperforms sim-
ple pattern-based approaches by circum-
venting their inherent sparsity problems.
1 Introduction
In formal semantic theory, the compositional se-
mantics of adjective-noun phrases can be modeled
in terms of selective binding (Pustejovsky, 1995),
i.e. the adjective selects one of possibly several
roles or attributes1 from the semantics of the noun.
(1) a. a blue car
b. COLOR(car)=blue
In this paper, we define a distributional frame-
work that models the compositional process un-
derlying the modification of nouns by adjectives.
1In the original statement of the theory, adjectives se-
lect qualia roles that can be considered as collections of at-
tributes.
We focus on property-denoting adjectives as they
are valuable for acquiring concept representations
for, e.g., ontology learning. An approach for au-
tomatic subclassification of property-denoting ad-
jectives is presented in Hartung and Frank (2010).
Our goal is to expose, for adjective-noun phrases
as in (1a), the attribute in the semantics of the
noun that is selected by the adjective, while not
being overtly realized on the syntactic level. The
semantic information we intend to capture for (1a)
is formalized in (1b).
Ideally, this kind of knowledge could be ex-
tracted from corpora by searching for patterns that
paraphrase (1a), e.g. the color of the car is blue.
However, linguistic patterns that explicitly relate
nouns, adjectives and attributes are very rare.
We avoid these sparsity issues by reducing
the triple r=?noun, attribute, adjective? that
encodes the relation illustrated in (1b) to tu-
ples r?=?noun, attribute? and r??=?attribute,
adjective?, as suggested by Turney and Pantel
(2010) for similar tasks. Both r? and r?? can be
observed much more frequently in text corpora
than r. Moreover, this enables us to model adjec-
tive and noun meanings as distinct semantic vec-
tors that are built over attributes as dimensions.
Based on these semantic representations, we make
use of vector composition operations in order to
reconstruct r from r? and r??. This, in turn, al-
lows us to infer complete noun-attribute-adjective
triples from individually acquired noun-attribute
and adjective-attribute representations.
The contributions of our work are as follows:
(i) We propose a framework for attribute selection
based on structured vector space models (VSM),
using as meaning dimensions attributes elicited
430
by adjectives; (ii) we complement this novel rep-
resentation of adjective meaning with structured
vectors for noun meanings similarly built on at-
tributes as meaning dimensions; (iii) we propose a
composition of these representations that mirrors
principles of compositional semantics in mapping
adjective-noun phrases to their corresponding on-
tological representation; (iv) we propose and eval-
uate several metrics for the selection of meaning-
ful components from vector representations.
2 Related Work
Adjective-noun meaning composition has not
been addressed in a distributional framework be-
fore (cf. Mitchell and Lapata (2008)). Our ap-
proach leans on related work on attribute learning
for ontology induction and recent work in distri-
butional semantics.
Attribute learning. Early approaches to at-
tribute learning include Hatzivassiloglou and
McKeown (1993), who cluster adjectives that de-
note values of the same attribute. A weakness
of their work is that the type of the attribute
cannot be made explicit. More recent attempts
to attribute learning from adjectives are Cimiano
(2006) and Almuhareb (2006). Cimiano uses at-
tributes as features to arrange sets of concepts in a
lattice. His approach to attribute acquisition har-
nesses adjectives that occur frequently as concept
modifiers in corpora. The association of adjec-
tives with their potential attributes is performed by
dictionary look-up in WordNet (Fellbaum, 1998).
Similarly, Almuhareb (2006) uses adjectives and
attributes as (independent) features for the pur-
pose of concept learning. He acquires adjective-
attribute pairs using a pattern-based approach.
As a major limitation, these approaches are
confined to adjective-attribute pairs. The poly-
semy of adjectives that can only be resolved in the
context of the modified noun is entirely neglected.
From a methodological point of view, our work
is similar to Almuhareb?s, as we will also build
on lexico-syntactic patterns for attribute selection.
However, we extend the task to involve nouns and
rephrase his approach in a distributional frame-
work based on the composition of structured vec-
tor representations.
Distributional semantics. We observe two re-
cent trends in distributional semantics research:
(i) The use of VSM tends to shift from mea-
suring unfocused semantic similarity to captur-
ing increasingly fine-grained semantic informa-
tion by incorporating more linguistic structure.
Following Baroni and Lenci (to appear), we re-
fer to such models as structured vector spaces.
(ii) Distributional methods are no longer confined
to word meaning, but are noticeably extended to
capture meaning on the phrase level. Prominent
examples for (i) are Pado? and Lapata (2007) and
Rothenha?usler and Schu?tze (2009) who use syn-
tactic dependencies rather than single word co-
occurrences as dimensions of semantic spaces.
Erk and Pado? (2008) extend this idea to the ar-
gument structure of verbs, while also accounting
for compositional meaning aspects by modelling
predication over arguments. Hence, their work is
also representative for (ii).
Baroni et al (2010) use lexico-syntactic pat-
terns to represent concepts in a structured VSM
whose dimensions are interpretable as empirical
manifestations of properties. We rely on similar
techniques for the acquisition of structured vec-
tors, whereas our work focusses on exposing the
hidden meaning dimensions involved in composi-
tional processes underlying concept modification.
The commonly adopted method for modelling
compositionality in VSM is vector composition
(Mitchell and Lapata, 2008; Widdows, 2008).
Showing the benefits of vector composition for
language modelling, Mitchell and Lapata (2009)
emphasize its potential to become a standard
method in NLP.
The approach pursued in this paper builds on
both lines of research sketched in (i) and (ii) in
that we model a specific meaning layer in the se-
mantics of adjectives and nouns in a structured
VSM. Vector composition is used to expose their
hidden meaning dimensions on the phrase level.
3 Structured Vector Representations for
Adjective-Noun Meaning
3.1 Motivation
Contrary to prior work, we model attribute selec-
tion as involving triples of nouns, attributes and
431
C
O
LO
R
D
IR
EC
TI
O
N
D
U
R
AT
IO
N
SH
A
PE
SI
ZE
SM
EL
L
SP
EE
D
TA
ST
E
TE
M
PE
R
AT
U
R
E
W
EI
G
H
T
ve 1 1 0 1 45 0 4 0 0 21
vb 14 38 2 20 26 0 45 0 0 20
ve ? vb 14 38 0 20 1170 0 180 0 0 420
ve + vb 15 39 2 21 71 0 49 0 0 41
Figure 1: Vectors for enormous (ve) and ball (vb)
adjectives, as in (2). The triple r can be bro-
ken down into tuples r? = ?noun, attribute? and
r?? = ?attribute, adjective?. Previous learning
approaches focussed on r? (Cimiano, 2006) or r??
(Almuhareb, 2006) only.
(2) a. a bluevalue carconcept
b. ATTR(concept) = value
In semantic composition of adjective-noun
compounds, the adjective (e.g. blue) contributes a
value for an attribute (here: COLOR) that charac-
terizes the concept evoked by the noun (e.g. car).
Thus, the attribute in (2) constitutes a ?hidden
variable? that is not overtly expressed in (2a), but
constitutes the central axis that relates r? and r??.
Structured vectors built on extraction patterns.
We model the semantics of adjectives and nouns
in a structured VSM that conveys the hidden re-
lationship in (2). The dimensions of the model
are defined by attributes, such as COLOR, SIZE
or SPEED, while the vector components are deter-
mined on the basis of carefully selected acquisi-
tion patterns that are tailored to capturing the par-
ticular semantic information of interest for r? and
r??. In this respect, lexico-syntactic patterns serve
a similar purpose as dependency relations in Pado?
and Lapata (2007) or Rothenha?usler and Schu?tze
(2009). The upper part of Fig. 1 displays exam-
ples of vectors we build for adjectives and nouns.
Composing vectors along hidden dimensions.
The fine granularity of lexico-syntactic patterns
that capture the triple r comes at the cost of their
sparsity when applied to corpus data. Therefore,
we construct separate vector representations for
r? and r??. Eventually, these representations are
joined by vector composition to reconstruct the
triple r. Apart from avoiding sparsity issues,
this compositional approach has several prospects
from a linguistic perspective as well.
Ambiguity and disambiguation. Building vec-
tors with attributes as meaning dimensions en-
ables us to model (i) ambiguity of adjectives with
regard to the attributes they select, and (ii) the dis-
ambiguation capacity of adjective and noun vec-
tors when considered jointly. Consider, for exam-
ple, the phrase enormous ball that is ambiguous
for two reasons: enormous may select a set of pos-
sible attributes (SIZE or WEIGHT, among others),
while ball elicits several attributes in accordance
with its different word senses2. As seen in Fig. 1,
these ambiguities are nicely captured by the sep-
arate vector representations for the adjective and
the noun (upper part); by composing these repre-
sentations, the ambiguity is resolved (lower part).
3.2 Building a VSM for Adjective-Noun
Meaning
In this section, we introduce the methods we ap-
ply in order to (i) acquire vector representations
for adjectives and nouns, (ii) select appropriate at-
tributes from them, and (iii) compose them.
3.2.1 Attribute Acquisition Patterns
We use the following patterns3 for the ac-
quisition of vectors capturing the tuple r?? =
?attribute, adjective?. Even though some of
these patterns (A1 and A4) match triples of nouns,
attributes and adjectives, we only use them for the
extraction of binary tuples (underlined), thus ab-
stracting from the modified noun.
(A1) ATTR of DT? NN is|was JJ
(A2) DT? RB? JJ ATTR
(A3) DT? JJ or JJ ATTR
(A4) DT? NN?s ATTR is|was JJ
(A5) is|was|are|were JJ in|of ATTR
To acquire noun vectors capturing the tuple
r? = ?noun, attribute?, we rely on the follow-
ing patterns. Again, we only extract pairs, as indi-
cated by the underlined elements.
(N1) NN with|without DT? RB? JJ? ATTR
(N2) DT ATTR of DT? RB? JJ? NN
(N3) DT NN?s RB? JJ? ATTR
(N4) NN has|had a|an RB? JJ? ATTR
2WordNet senses for the noun ball include, among others:
1. round object [...] in games; 2. solid projectile, 3. object
with a spherical shape, 4. people [at a] dance.
3Some of these patterns are taken from Almuhareb (2006)
and Sowa (2000). The descriptions rely on the Penn Tagset
(Marcus et al, 1999). ? marks optional elements.
432
3.2.2 Target Filtering
Some of the adjectives extracted by A1-A5 are
not property-denoting and thus represent noise.
This affects in particular pattern A2, which ex-
tracts adjectives like former or more, or relational
ones such as economic or geographic.
This problem may be addressed in different
ways: By target filtering, extractions can be
checked against a predicative pattern P1 that is
supposed to apply to property-denoting adjectives
only. Vectors that fail this test are suppressed.
(P1) DT NN is|was JJ
Alternatively, extractions obtained from low-
confidence patterns can be awarded reduced
weights by means of a pattern value function (de-
fined in 3.3; cf. Pantel and Pennacchiotti (2006)).
3.2.3 Attribute Selection
We intend to use the acquired vectors in order
to detect attributes that are implicit in adjective-
noun meaning. Therefore, we need a method
that selects appropriate attributes from each vec-
tor. While, in general, this task consists in dis-
tinguishing semantically meaningful dimensions
from noise, the requirements are different depend-
ing on whether attributes are to be selected from
adjective or noun vectors. This is illustrated in
Fig. 1, a typical configuration, with one vector
representing a typical property-denoting adjective
that exhibits relatively strong peaks on one or
more dimensions, whereas noun vectors show a
tendency for broad and flat distributions over their
dimensions. This suggests using a strict selection
function (choosing few very prominent dimen-
sions) for adjectives and a less restrictive one (li-
censing the inclusion of more dimensions of lower
relative prominence) for nouns. Moreover, we are
interested in finding a selection function that re-
lies on as few free parameters as possible in order
to avoid frequency or dimensionality effects.
MPC Selection (MPC). An obvious method
for attribute selection is to choose the most promi-
nent component from any vector (i.e., the highest
absolute value). If a vector exhibits several peaks,
all other components are rejected, their relative
importance notwithstanding. MPC obviously fails
to capture polysemy of targets, which affects ad-
jectives such as hot, in particular.
Threshold Selection (TSel). TSel recasts the
approach of Almuhareb (2006), in selecting all di-
mensions as attributes whose components exceed
a frequency threshold. This avoids the drawback
of MPC, but introduces a parameter that needs to
be optimized. Also, it is difficult to apply absolute
thresholds to composed vectors, as the range of
their components is subject to great variation, and
it is unclear whether the method will scale with
increased dimensionality.
Entropy Selection (ESel). In information the-
ory, entropy measures the average uncertainty in
a probability distribution (Manning and Schu?tze,
1999). We define the entropy H(v) of a
vector v=?v1, . . . , vn? over its components as
H(v) = ??ni=1 P (vi) log P (vi), where P (vi) =
vi/
?n
i=1 vi.
We use H(v) to assess the impact of singular
vector components on the overall entropy of the
vector: We expect entropy to detect components
that contribute noise, as opposed to those that con-
tribute important information.
We define an algorithm for entropy-based at-
tribute selection that returns a list of informa-
tive dimensions. The algorithm successively sup-
presses (combinations of) vector components one
by one. Given that a gain of entropy is equiva-
lent to a loss of information and vice versa, we as-
sume that every combination of components that
leads to an increase in entropy when being sup-
pressed is actually responsible for a substantial
amount of information. The algorithm includes a
back-off to MPC for the special case that a vector
contains a single peak (i.e., H(v) = 0), so that,
in principle, it should be applicable to vectors of
any kind. Vectors with very broad distributions
over their dimensions, however, pose a problem
to this method. For ball in Fig. 1, for instance, the
method does not select any dimension.
Median Selection (MSel). As a further method
we rely on the median m that can be informally
defined as the value that separates the upper from
the lower half of a distribution (Krengel, 2003).
It is less restrictive than MPC and TSel and over-
comes the particular drawback of ESel. Using this
measure, we choose all dimensions whose compo-
nents exceed m. Thus, for the vector representing
433
Pattern Label # Hits (Web) # Hits (ukWaC)
A1 2249 815
A2 36282 72737
A3 3370 1436
A4 ? 7672
A5 ? 3768
N1 ? 682
N2 ? 5073
N3 ? 953
N4 ? 56
Table 1: Number of pattern hits on the Web (Al-
muhareb, 2006) and on ukWaC
ball, WEIGHT, DIRECTION, SHAPE, SPEED and
SIZE are selected.
3.2.4 Vector Composition
We use vector composition as a hinge to com-
bine adjective and noun vectors in order to recon-
struct the triple r=?noun, attribute, adjective?.
Mitchell and Lapata (2008) distinguish two major
classes of vector composition operations, namely
multiplicative and additive operations, that can be
extended in various ways. We use their standard
definitions (denoted ? and +, henceforth). For
our task, we expect ? to perform best as it comes
closest to the linguistic function of intersective ad-
jectives, i.e. to select dimensions that are promi-
nent both for the adjective and the noun, whereas
+ basically blurs the vector components, as can
be seen in the lower part of Fig. 1.
3.3 Model Parameters
We follow Pado? and Lapata (2007) in defining a
semantic space as a matrix M = B?T relating a
set of target elements T to a set of basis elements
B. Further parameters and their instantiations we
use in our model are described below. We use p to
denote an individual lexico-syntactic pattern.
The basis elements of our VSM are nouns de-
noting attributes. For comparison, we use the at-
tributes selected by Almuhareb (2006): COLOR,
DIRECTION, DURATION, SHAPE, SIZE, SMELL,
SPEED, TASTE, TEMPERATURE, WEIGHT.
The context selection function cont(t) deter-
mines the set of patterns that contribute to the rep-
resentation of each target word t ? T . These are
the patterns A1-A5 and N1-N4 (cf. Section 3.2.1).
The target elements represented in the vector
space comprise all adjectives TA that match the
patterns A1 to A5 in the corpus, provided they ex-
ceed a frequency threshold n. During develop-
ment, n was set to 5 in order to filter noise.
As for the target nouns TN , we rely on a repre-
sentative dataset compiled by Almuhareb (2006).
It contains 402 nouns that are balanced with re-
gard to semantic class (according to the WordNet
supersenses), ambiguity and frequency.
As association measure that captures the
strength of the association between the elements
of B and T , we use raw frequency counts4 as ob-
tained from the PoS-tagged and lemmatized ver-
sion of the ukWaC corpus (Baroni et al, 2009).
Table 1 gives an overview of the number of hits
returned by these patterns.
The basis mapping function ? creates the di-
mensions of the semantic space by mapping each
extraction of a pattern p to the attribute it contains.
The pattern value function enables us to sub-
divide dimensions along particular patterns. We
experimented with two instantiations: pvconst
considers, for each dimension, all patterns, while
weighting them equally. pvf (p) awards the ex-
tractions of pattern p with weight 1, while setting
the weights for all patterns different from p to 0.
4 Experiments
We evaluate the performance of the structured
VSM on the task of inferring attributes from
adjective-noun phrases in three experiments: In
Exp1 and Exp2, we evaluate vector representa-
tions capturing r? and r?? independently of one an-
other. Exp3 investigates the selection of hidden
attributes from vector representations constructed
by composition of adjective and noun vectors.
We compare all results against different gold
standards. In Exp1, we follow Almuhareb (2006),
evaluating against WordNet 3.0. For Exp2 and
Exp3, we establish gold standards manually: For
Exp2, we construct a test set of nouns annotated
with their corresponding attributes. For Exp3, we
manually annotate adjective-noun phrases with
the attributes appropriate for the whole phrase. All
experiments are evaluated in terms of precision,
recall and F1 score.
4We experimented with the conditional probability ratio
proposed by Mitchell and Lapata (2009). As it performed
worse on our data, we did not consider it any further.
434
4.1 Exp1: Attribute Selection for Adjectives
The first experiment evaluates the performance of
structured vector representations on attribute se-
lection for adjectives. We compare this model
against a re-implementation of Almuhareb (2006).
Experimental settings and gold standard. To
reconstruct Almuhareb?s approach, we ran his pat-
terns A1-A3 on the ukWaC corpus. Table 1 shows
the number of hits when applied to the Web (Al-
muhareb, 2006) vs. ukWaC. A1 and A3 yield less
extractions on ukWaC as compared to the Web.5
We introduced two additional patterns, A4 and
A5, that contribute about 10,000 additional hits.
We adopted Almuhareb?s manually chosen thresh-
olds for attribute selection for A1-A3; for A4, A5
and a combination of all patterns, we manually se-
lected optimal thresholds.
We experiment with pvconst and all variants of
pvf (p) for pattern weighting (see sect. 3.3). For
attribute selection, we compare TSel (as used by
Almuhareb), ESel and MSel.
The gold standard consists of all adjectives that
are linked to at least one of the ten attributes
we consider by WordNet?s attribute relation
(1063 adjectives in total).
Evaluation results. Results for Exp1 are dis-
played in Table 2. The settings of pv are given in
the rows, the attribute selection methods (in com-
bination with target filtering6) in the columns.
The results for our re-implementation of Al-
muhareb?s individual patterns are comparable to
his original figures7, except for A3 that seems to
suffer from quantitative differences of the under-
lying data. Combining all patterns leads to an
improvement in precision over (our reconstruc-
tion of) Almuhareb?s best individual pattern when
TSel and target filtering are used in combina-
tion. MPC and MSel perform worse (not reported
here). As for target filtering, A1 and A3work best.
Both TSel and ESel benefit from the combina-
tion with the target filter, where the largest im-
provement (and the best overall result) is observ-
5The difference for A2 is an artifact of Almuhareb?s ex-
traction methodology.
6Regarding target filtering, we only report the best filter
pattern for each configuration.
7P(A1)=0.176, P(A2)=0.218, P(A3)=0.504
MPC ESel MSel
P R F P R F P R F
pvf (N1) 0.22 0.06 0.10 0.29 0.04 0.07 0.22 0.09 0.13
pvf (N2) 0.29 0.18 0.23 0.20 0.06 0.09 0.28 0.39 0.33
pvf (N3) 0.34 0.05 0.09 0.20 0.02 0.04 0.25 0.08 0.12
pvf (N4) 0.25 0.02 0.04 0.29 0.02 0.03 0.26 0.02 0.05
pvconst 0.29 0.18 0.22 0.20 0.06 0.09 0.28 0.43 0.34
Table 3: Evaluation results for Experiment 2
able for ESel on pattern A1 only. This is the
pattern that performs worst in Almuhareb?s orig-
inal setting. From this, we conclude that both
ESel and target filtering are valuable extensions
to pattern-based structured vector spaces if preci-
sion is in focus. This also underlines a finding
of Rothenha?usler and Schu?tze (2009) that VSMs
intended to convey specific semantic information
rather than mere similarity benefit primarily from
a linguistically adequate choice of contexts.
Similar to Almuhareb, recall is problematic.
Even though ESel leads to slight improvements,
the scores are far from satisfying. With Al-
muhareb, we note that this is mainly due to a
high number of extremely fine-grained adjectives
in WordNet that are rare in corpora.8
4.2 Exp2: Attribute Selection for Nouns
Exp2 evaluates the performance of attribute selec-
tion from noun vectors tailored to the tuple r??.
Construction of the gold standard. For eval-
uation, we created a gold standard by manually
annotating a set of nouns with attributes. This
gold standard builds on a random sample ex-
tracted from TN (cf. section 3.3). Running N1-
N4 on ukWaC returned semantic vectors for 216
concepts. From these, we randomly sampled 100
concepts that were manually annotated by three
human annotators.
The annotators were provided a matrix consist-
ing of the nouns and the set of ten attributes for
each noun. Their task was to remove all inappro-
priate attributes. They were free to decide how
many attributes to accept for each noun. In order
to deal with word sense ambiguity, the annotators
were instructed to consider all senses of a noun
and to retain every attribute that was acceptable
for at least one sense.
Inter-annotator agreement amounts to ?= 0.69
(Fleiss, 1971). Cases of disagreement were ad-
judicated by majority-voting. The gold standard
435
Almuhareb (reconstr.) VSM (TSel + Target Filter) VSM (ESel) VSM (ESel + Target Filter)
P R F Thr P R F Patt Thr P R F P R F Patt
pvf (A1) = 1 0.183 0.005 0.009 5 0.300 0.004 0.007 A3 5 0.231 0.045 0.076 0.519 0.035 0.065 A3
pvf (A2) = 1 0.207 0.039 0.067 50 0.300 0.033 0.059 A1 50 0.084 0.136 0.104 0.240 0.049 0.081 A3
pvf (A3) = 1 0.382 0.020 0.039 5 0.403 0.014 0.028 A1 5 0.192 0.059 0.090 0.375 0.027 0.050 A1
pvf (A4) = 1 0.301 0.020 0.036 A3 10 0.135 0.055 0.078 0.272 0.020 0.038 A1
pvf (A5) = 1 0.295 0.008 0.016 A3 24 0.105 0.056 0.073 0.315 0.024 0.045 A3
pvconst 0.420 0.024 0.046 A1 183 0.076 0.152 0.102 0.225 0.054 0.087 A3
Table 2: Evaluation results for Experiment 1
contains 424 attributes for 100 nouns.
Evaluation results. Results for Exp2 are given
in Table 3. Performance is lower in comparison to
Exp1. We hypothesize that the tuple r?? might not
be fully captured by overt linguistic patterns. This
needs further investigation in future research.
Against this background, MPC is relatively pre-
cise, but poor in terms of recall. ESel, being
designed to select more than one prominent di-
mension, counterintuitively fails to increase re-
call, suffering from the fact that many noun vec-
tors show a rather flat distribution without any
strong peak. MSel turns out to be most suitable
for this task: Its precision is comparable to MPC
(with N3 as an outlier), while recall is consider-
ably higher. Overall, these results indicate that at-
tribute selection for adjectives and nouns, though
similar, should be viewed as distinct tasks that re-
quire different attribute selection methods.
4.3 Exp3: Attribute Selection for
Adjective-Noun Phrases
In this experiment, we compose noun and adjec-
tive vectors in order to yield a new combined rep-
resentation. We investigate whether the seman-
tic information encoded by the components of this
new vector is sufficiently precise to disambiguate
the attribute dimensions of the original represen-
tations (see section 3.1) and, thus, to infer hidden
attributes from adjective-noun phrases (see (2)) as
advocated by Pustejovsky (1995).
Construction of the gold standard. For evalu-
ation, we created a manually annotated test set of
adjective-noun phrases. We selected a subset of
property-denoting adjectives that are appropriate
modifiers for the nouns from TN using the pred-
icative pattern P1 (see sect. 3) on ukWaC. This
8For instance: bluish-lilac, chartreuse or pink-lavender
as values of the attribute COLOR.
yielded 2085 adjective types that were further re-
duced to 386 by frequency filtering (n = 5). We
sampled our test set from all pairs in the carte-
sian product of the 386 adjectives and 216 nouns
(cf. Exp2) that occurred at least 5 times in a sub-
section of ukWaC. To ensure a sufficient number
of ambiguous adjectives in the test set, sampling
proceeded in two steps: First, we sampled four
nouns each for a manual selection of 15 adjectives
of all ambiguity levels in WordNet. This leads to
60 adjective-noun pairs. Second, another 40 pairs
were sampled fully automatically.
The test set was manually annotated by the
same annotators as in Exp2. They were asked to
remove all attributes that were not appropriate for
a given adjective-noun pair, either because it is not
appropriate for the noun or because it is not se-
lected by the adjective. Further instructions were
as in Exp2, in particular regarding ambiguity.
The overall agreement is ?=0.67. After adjudi-
cation by majority voting, the resulting gold stan-
dard contains 86 attributes for 76 pairs. 24 pairs
could not be assigned any attribute, either because
the adjective did not denote a property, as in pri-
vate investment, or the most appropriate attribute
was not offered, as in blue day or new house.
We evaluate the vector composition methods
discussed in section 3.2.4. Individual vectors for
the adjectives and nouns from the test pairs were
constructed using all patterns A1-A5 and N1-N4.
For attribute selection, we tested MPC, ESel and
MSel. The results are compared against three
baselines: BL-P implements a purely pattern-
based method, i.e. running the patterns that ex-
tract the triple r (A1, A4, N1, N3 and N4, with
JJ and NN instantiated accordingly) on the pairs
from the test set. BL-N and BL-Adj are back-offs
for vector composition, taking the respective noun
or adjective vector, as investigated in Exp1 and
Exp2, as surrogates for a composed vector.
436
MPC ESel MSel
P R F P R F P R F
? 0.60 0.58 0.59 0.63 0.46 0.54 0.27 0.72 0.39
+ 0.43 0.55 0.48 0.42 0.51 0.46 0.18 0.91 0.30
BL-Adj 0.44 0.60 0.50 0.51 0.63 0.57 0.23 0.83 0.36
BL-N 0.27 0.35 0.31 0.37 0.29 0.32 0.17 0.73 0.27
BL-P 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
Table 4: Evaluation results for Experiment 3
Evaluation results. Results are given in Table
4. Attribute selection based on the composition of
adjective and noun vectors yields a considerable
improvement of both precision and recall as com-
pared to the individual results obtained in Exp1
and Exp2. Comparing the results of Exp3 against
the baselines reveals two important aspects of our
work. First, the complete failure of BL-P9 un-
derlines the attractiveness of our method to build
structured vector representations from patterns of
reduced complexity. Second, vector composition
is suitable for selecting hidden attributes from
adjective-noun phrases that are jointly encoded
by adjective and noun vectors: Both composition
methods we tested outperform BL-N.
However, the choice of the composition method
matters: ? performs best with a maximum pre-
cision of 0.63. This confirms our expectation
that vector multiplication is a good approxima-
tion for attribute selection in adjective-noun se-
mantics. Being outperformed by BL-Adj in most
categories, + is less suited for this task.
All selection methods outperform BL-Adj in
precision. Comparing MPC and ESel, ESel
achieves better precision when combined with the
?-operator, while doing worse for recall. The
robust performance of MPC is not surprising as
the test set contains only ten adjective-noun pairs
that are still ambiguous with regard to the at-
tributes they elicit. The stronger performance of
the entropy-based method with the ?-operator is
mainly due to its accuracy on detecting false posi-
tives, in that it is able to return ?empty? selections.
In terms of precision, MSel did worse in general,
while recall is decent. This underlines that vector
composition generally promotes meaningful com-
ponents, but MSel is too inaccurate to select them.
Given the performance of the baselines and
the noun vectors in Exp2, we consider this a
very promising result for our approach to attribute
9The patterns used yield no hits for the test pairs at all.
selection from structured vector representations.
The results also corroborate the insufficiency of
previous approaches to attribute learning from ad-
jectives alone.
5 Conclusions and Outlook
We proposed a structured VSM as a framework
for inferring hidden attributes from the composi-
tional semantics of adjective-noun phrases.
By reconstructing Almuhareb (2006), we
showed that structured vector representations of
adjective meaning consistently outperform sim-
ple pattern-based learning, up to 13 pp. in preci-
sion. A combination of target filtering and pat-
tern weighting turned out to be effective here, by
selecting particulary meaningful lexico-syntactic
contexts and filtering adjectives that are not
property-denoting. Further studies need to inves-
tigate this phenomenon and its most appropriate
formulation in a vector space framework.
Moreover, the VSM offers a natural represen-
tation for sense ambiguity of adjectives. Compar-
ing attribute selection methods on adjective and
noun vectors shows that they are sensitive to the
distributional structure of the vectors, and need to
be chosen with care. Future work will investigate
these selection methods in high-dimensional vec-
tors spaces, by using larger sets of attributes.
Exp3 shows that the composition of pattern-
based adjective and noun vectors robustly reflects
aspects of meaning composition in adjective-noun
phrases, with attributes as a hidden dimension.
It also suggests that composition is effective in
disambiguation of adjective and noun meanings.
This hypothesis needs to be substantiated in fur-
ther experiments.
Finally, we showed that composition of vectors
representing complementary meaning aspects can
be beneficial to overcome sparsity effects. How-
ever, our compositional approach meets its lim-
its if the patterns capturing adjective and noun
meaning in isolation are too sparse to acquire suf-
ficiently populated vector components from cor-
pora. For future work, we envisage using vector
similarity to acquire structured vectors for infre-
quent targets from semantic spaces that convey
less linguistic structure to address these remain-
ing sparsity issues.
437
References
Almuhareb, Abdulrahman. 2006. Attributes in Lexi-
cal Acquisition. Ph.D. Dissertation, Department of
Computer Science, University of Essex.
Baroni, Marco and Alessandro Lenci. to appear.
Distributional Memory. A General Framework for
Corpus-based Semantics. Computational Linguis-
tics.
Baroni, Marco, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide web:
A collection of very large linguistically processed
web-crawled corpora. Journal of Language Re-
sources and Evaluation, 43(3):209?226.
Baroni, Marco, Brian Murphy, Eduard Barbu, and
Massimo Poesio. 2010. Strudel. A Corpus-based
Semantic Model of Based on Properties and Types.
Cognitive Science, 34:222?254.
Cimiano, Philipp. 2006. Ontology Learning and Pop-
ulation from Text. Algorithms, Evaluation and Ap-
plications. Springer.
Erk, Katrin and Sebastian Pado?. 2008. A Structured
Vector Space Model for Word Meaning in Context.
In Proceedings of EMNLP, Honolulu, HI.
Fellbaum, Christiane, editor. 1998. WordNet: An
Electronic Lexical Database. MIT Press, Cam-
bridge, Mass.
Fleiss, Joseph L. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76(5):378?382.
Hartung, Matthias and Anette Frank. 2010. A
Semi-supervised Type-based Classification of Ad-
jectives. Distinguishing Properties and Relations.
In Proceedings of the 7th International Conference
on Language Resources and Evaluation, Valletta,
Malta, May.
Hatzivassiloglou, Vasileios and Kathleen McKeown.
1993. Towards the Automatic Identification of Ad-
jectival Scales. Clustering Adjectives According to
Meaning. In Proceedings of the 31st Annual Meet-
ing of the Association of Computational Linguistics,
pages 172?182.
Krengel, Ulrich. 2003. Wahrscheinlichkeitstheorie
und Statistik. Vieweg, Wiesbaden.
Manning, Christopher D. and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. The MIT Press, Cambridge, Mas-
sachusetts.
Marcus, Mitchell P., Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3,
ldc99t42. CD-ROM. Philadelphia, Penn.: Linguis-
tic Data Consortium.
Mitchell, Jeff and Mirella Lapata. 2008. Vector-based
Models of Semantic Composition. In Proceedings
of ACL-08: HLT, pages 236?244, Columbus, Ohio,
June.
Mitchell, Jeff and Mirella Lapata. 2009. Lan-
guage Models Based on Semantic Composition. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, Singa-
pore, August 2009, pages 430?439, Singapore, Au-
gust.
Pado?, Sebastian and Mirella Lapata. 2007.
Dependency-based Construction of Semantic Space
Models. Computational Linguistics, 33:161?199.
Pantel, Patrick and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, Sydney,
Australia, 17?21 July 2006, pages 113?120.
Pustejovsky, James. 1995. The Generative Lexicon.
MIT Press, Cambridge, Mass.
Rothenha?usler, Klaus and Hinrich Schu?tze. 2009. Un-
supervised Classification with Dependency Based
Word Spaces. In Proceedings of the EACL Work-
shop on Geometrical Models of Natural Language
Semantics (GEMS), pages 17?24, Athens, Greece,
March.
Sowa, John F. 2000. Knowledge Representation.
Logical, Philosophical, and Computational Foun-
dations. Brooks Cole.
Turney, Peter D. and Patrick Pantel. 2010. From Fre-
quency to Meaning. Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Widdows, Dominic. 2008. Semantic Vector Products.
Some Initial Investigations. In Proceedings of the
2nd Conference on Quantum Interaction, Oxford,
UK, March.
438
