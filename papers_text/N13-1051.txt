Proceedings of NAACL-HLT 2013, pages 482?486,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Parameter Estimation for LDA-Frames
Jir??? Materna
Centre for Natural Language Processing
Faculty of Informatics, Masaryk University
Botanicka? 68a, 602 00, Brno, Czech Republic
xmaterna@fi.muni.cz
Abstract
LDA-frames is an unsupervised approach for
identifying semantic frames from semanti-
cally unlabeled text corpora, and seems to
be a useful competitor for manually created
databases of selectional preferences. The most
limiting property of the algorithm is such that
the number of frames and roles must be pre-
defined. In this paper we present a modifi-
cation of the LDA-frames algorithm allowing
the number of frames and roles to be deter-
mined automatically, based on the character
and size of training data.
1 Introduction
Semantic frames and valency lexicons are useful
lexical sources capturing semantic roles valid for
a set of lexical units. The structures of linked seman-
tic roles are called semantic frames. Linguists are
using them for their ability to describe an interface
between syntax and semantics. In practical natural
language processing applications, they can be used,
for instance, for the word sense disambiguation task
or in order to resolve ambiguities in syntactic analy-
sis of natural languages.
The lexicons of semantic frames or verb valencies
are mainly created manually or semi-automatically
by highly trained linguists. Manually created lex-
icons involve, for instance, a well-known lexi-
con of semantic frames FrameNet (Ruppenhofer
et al, 2006) or a lexicon of verb valencies VerbNet
(Schuler, 2006). These and other similar lexical re-
sources have many promising applications, but suf-
fer from several disadvantages:
? Creation of them requires manual work of
trained linguists which is very time-consuming
and expensive.
? Coverage of the resources is usually small or
limited to some specific domain.
? Most of the resources do not provide any
information about relative frequency of us-
age in corpora. For instance, both patterns
[Person] acquire [Physical object]
and [Person] acquire [Disease] reflect
correct usage of verb acquire, but the former
is much more frequent in English.
? Notion of semantic classes and frames is sub-
jectively biased when the frames are created
manually without corpus evidence.
In order to avoid those problems we proposed
a method for creating probabilistic semantic frames
called LDA-frames (Materna, 2012). The main idea
of LDA-frames is to generate the set of semantic
frames and roles automatically by maximizing pos-
terior probability of a probabilistic model on a syn-
tactically annotated training corpus. A semantic role
is represented as probability distribution over all its
realizations in the corpus, a semantic frame as a tu-
ple of semantic roles, each of them connected with
some grammatical relation. For every lexical unit
(a verb in case of computing verb valencies), a prob-
ability distribution over all semantic frames is gen-
erated, where the probability of a frame corresponds
to the relative frequency of usage in the corpus for
a given lexical unit. An example of LDA-frames
482
computed on the British National Corpus is avail-
able at the LDA-frames website1.
The original LDA-frames algorithm has two pa-
rameters that must be predefined ? number of frames
and number of roles ? which is the most limiting
property of the algorithm. A simple cross-validation
approach can be used in case of very small data.
However, real data is much bigger and it is not re-
commended to use such techniques. For example,
the inference on the British National Corpus using a
single core 2.4 GHz CPU takes several days to com-
pute one reasonable combination of parameters.
In this paper we present a non-parametric modifi-
cation of the LDA-frames algorithm allowing to de-
termine the parameters automatically, based on the
character and size of training data.
2 LDA-Frames
LDA-frames (Materna, 2012) is an unsupervised ap-
proach for identifying semantic frames from seman-
tically unlabeled text corpora. In the LDA-frames,
a frame is represented as a tuple of semantic roles,
each of them connected with a grammatical rela-
tion i.e. subject, object, modifier, etc. These frames
are related to a lexical unit via probability distribu-
tion. Every semantic role is represented as probabil-
ity distribution over its realizations.
The method of automatic identification of se-
mantic frames is based on probabilistic generative
process. Training data for the algorithm consists
of tuples of grammatical relation realizations ac-
quired using a dependency parser from the train-
ing corpus for every lexical unit. For example, sup-
pose that the goal is to generate semantic frames of
verbs from a corpus for grammatical relations sub-
ject and object. The training data for lexical unit
eat may look like {(peter, cake), (man,
breakfast), (dog, meat), ...}, where
the first component of the tuples corresponds to sub-
ject and the second to object.
In the generative process, each grammatical rela-
tion realization is treated as being generated from
a given semantic frame according to the realiza-
tion distribution of the corresponding semantic role.
Supposing the number of frames is given by param-
eter F , the number of semantic roles by R, the num-
1http://nlp.fi.muni.cz/projekty/lda-frames/
ber of slots (grammatical relations) by S and the size
of vocabulary is V . The realizations are generated as
follows.
For each lexical unit u ? {1, 2, . . . , U}:
1. Choose a frame distribution ?u from Dir(?).
2. For each lexical unit realization
t ? {1, 2, . . . , Tu} choose a frame fu,t from
Multinomial(?u), where fu,t ? {1, 2, . . . , F}.
3. For each slot s ? {1, 2, . . . , S} of frame
fu,t, generate a grammatical relation realiza-
tion wu,t,s from Multinomial(?rfu,t,s), where
rf,s is a projection (f, s) 7? r, which assigns
a semantic role for each slot s in frame f . The
multinomial distribution of realizations, sym-
bolized by ?r, for semantic role r is generated
from Dir(?).
The graphical model for LDA-Frames is shown
in figure 1. It is parametrized by hyperparameters of
prior distributions ? and ?, usually set by hand to
a value between 0.01 ? 0.1.
?
? f
r
w
U T
S
?
?F S
u u,t
f,s
u,t,s
r R
Figure 1: Graphical model for LDA-frames.
The inference is performed using the Collapsed
Gibbs sampling (Neal, 2000), where the ? and ? dis-
tributions are marginalized out of the equations. In
each iteration, latent variables fu,t and rf,s are sam-
pled as follows
P (fu,t|f?(u,t), r,w, ?, ?) ?
(fc?(u,t)fu,t,u + ?)
S?
s=1
wc?(u,t,s)wu,t,s,rfu,t,s + ?
wc?(u,t,s)?,rfu,t,s + V ?
(1)
483
P (rf,s|f , r
?(f,s),w, ?, ?) ?
V?
v=1
(
wc?(f,s)v,rf,s + ?
wc?(f,s)?,rf,s + V ?
)wcf,s,v
,
(2)
where fc?(u,t)f,u is the number of times frame f is
assigned to lexical unit u excluding (u, t), wc?(u,t,s)v,r
is the number of times word v is assigned to role
r excluding (u, t, s), and wcf,s,v is the number of
times word v is assigned to slot s in frame f . The
asterisk sign * stands for any value in its position.
After having all latent variables f and r inferred,
one can proceed to compute the lexical unit?frame
distribution and the semantic role?word distribution
using the following formulas:
?u =
fcf,u + ?
?
f fcf,u + F?
(3)
?r =
wcv,r + ?
?
v wcv,r + V ?
. (4)
3 Parameter Estimation
As one can see from the LDA-frames model, the
requirement is to define the number of frames and
roles in advance. It is not clear, however, how to se-
lect the best values that depend on several factors.
First of all, the number of frames and roles usually
increase with the growing size of training corpus. If
the training data is small and covers just a small pro-
portion of lexical unit usage patterns, the number of
semantic frames should be small as well. The pa-
rameters are also affected by the granularity of roles
and frames. One way to estimate the parameters au-
tomatically is to select those that maximize posterior
probability of the model given training data.
LDA-frames algorithm generates frames from the
Dirichlet distribution (DD) which requires a fixed
number of components. Similarly, the latent vari-
ables rf,s are chosen from a fixed set of semantic
roles. In order to be able to update the number of
frames and roles during the inference process, we
propose to add the Chinese restaurant process (CRP)
(Aldous, 1985) prior for the rf,s variables, and to re-
place the Dirichlet distribution the semantic frames
are generated from with the Dirichlet process (Fer-
guson, 1973).
3.1 Number of Semantic Roles
In the original version of the LDA-frames model,
the latent variables rf,s, representing semantic role
assignment for slot s in frame f , are chosen from
a fixed set of semantic roles without any prior distri-
bution. We propose to generate rf,s from the CRP,
which is a single parameter distribution over parti-
tions of integers. The generative process can be de-
scribed by using an analogy with a Chinese restau-
rant. Consider a restaurant with an infinite number
of tables, each of them associated with some dish,
and N customers choosing a table. The first cus-
tomer sits at the first table. The nth customer sits at
table t drawn from the following distribution
P (t = occupied table i) =
ni
? + n? 1
P (t = next unoccupied table) =
?
? + n? 1
,
(5)
where ni is the number of customers sitting at the
table i and ? > 0 is a concentration parameter which
controls how often a customer chooses a new table.
The seating plan makes a partition of the customers
(Aldous, 1985).
In the proposed modification of the LDA-frames
model, the dishes are replaced with the semantic role
numbers and customers with slots of frames. In the
model we use prior distribution ? corresponding to
the CRP with concentration parameter ?. The latent
variables rf,s are then sampled as follows
P (rf,s|f , r
?(f,s),w, ?, ?, ?) ?
(rc?(f,s)rf,s + ?)
V?
v=1
(
wc?(f,s)v,rf,s + ?
wc?(f,s)?,rf,s + V ?
)wcf,s,v
,
(6)
where rc?(f,s)r is the number of times role r is used
in any frame and slot excluding slot s in frame f .
Notice that the sampling space hasR+1 dimensions
with the probability of the last unseen component
proportional to
?
V?
v=1
1
V wcf,s,v
. (7)
3.2 Number of Semantic Frames
Estimating the number of frames is a little bit more
complicated than the case of semantic roles. The
idea is to replace DD ?u with the Dirichlet process.
484
The Dirichlet process DP (?0, G0) is a stochastic
process that generates discrete probability distribu-
tions. It has two parameters, a base distribution G0
and a concentration parameter ?0 > 0. A sample
from the Dirichlet process (DP) is then
G =
??
k=1
?k??k , (8)
where ?k are independent random variables dis-
tributed according to G0, ??k is an atom at ?k, and
weights ?k are also random and dependent on the
parameter ?0 (Teh et al, 2006). Simply, DP is a dis-
tribution over some infinite and discrete distribu-
tions. It is the reason why DP is often used instead of
DD in order to avoid using a fixed number of com-
ponents.
The question, however, is how to make the sam-
pled frames shared between different lexical units.
We propose to generate base distributions of the
DPs from GEM distribution (Pitman, 2002) ? with
concentration parameter ?. The idea is inspired by
the Hierarchical Dirichlet Process (Teh et al, 2006)
used for topic modeling. The graphical model of the
non-parametric LDA-frames is shown in figure 2.
?
? f
r
w
U T S
?
?S
u u,t
f,s
u,t,s
r?
? ??
?
?
Figure 2: Graphical model for non-parametric LDA-
frames.
Since it is hard to integrate out the DP with base
distribution generated from GEM in this model, we
proceeded to sample ? separately (Porteous, 2010).
The base distribution proportions can be sampled
by simulating how new components are created for
fcf,u draws from DP with the concentration param-
eter ??f , which is a sequence of Bernoulli trials for
each u and f (Heinrich, 2011):
P (uf,u,r = 1) =
??f
??f + r ? 1
?r ? [1, fcf,u]
? ? Dir({uf}f , ?) with uf =
?
u
?
r
uf,u,r.
(9)
Finally, the latent variables fu,t are sampled as fol-
lows
P (fu,t|f?(u,t), r,w, ?, ?, ?) ?
(fc?(u,t)fu,t,u + ??f )
S?
s=1
wc?(u,t,s)wu,t,s,rfu,t,s + ?
wc?(u,t,s)?,rfu,t,s + V ?
.
(10)
4 Evaluation
The non-parametric algorithm was evaluated by
an experiment on a synthetic data set consisting
of 155 subject-object tuples. The training data
was generated randomly from a predefined set of 7
frames and 4 roles for 16 verbs using the following
algorithm. For every lexical unit u:
1. Choose a number of corpus realizations Nu ?
{5, . . . , 15} from the uniform distribution.
2. For each realization nu ? {1, . . . , Nu}, among
all permitted frames for lexical unit u, choose
a semantic frame fnu from the uniform distri-
bution.
3. For each frame fnu , generate a realization of all
its roles from the uniform distribution.
Each semantic role had 6 possible realizations on
average, some of them assigned to more than one se-
mantic role to reflect the character of real languages.
Since the data was generated artificially, we knew
the number of frames and roles, how the frames were
defined, and which frame and which role was re-
sponsible for generating each realization in the data.
We ran the non-parametric algorithm with hyper-
parameters ? = 5, ? = ? = 0.1, ? = 1.5. It has
been shown that the selection of hyperparameters
has little impact on the resulting frames when they
are in some reasonable range, thus, the hyperparam-
eters were chosen empirically by hand. The experi-
ment led to correct assignments of fu,t and rf,s after
56 iterations on average (based on 10 independent
runs of the algorithm).
485
In order to compare the non-parametric algorithm
with the original, we ran the original algorithm with
the same data that had the number of frames and
roles set to R ? {1 . . . 10}, F ? {1 . . . 20}, and
measured the perplexity of the data given to the
model after convergence. The perplexities for all
settings are shown in figure 3. The lowest perplexity
was reached with F = 7, R = 4 and had the same
value as the case of the non-parametric algorithm.
The fu,t and rf,s assignments were correct as well.
Figure 3: Perplexities for different values of F and R.
We also ran the non-parametric algorithm with the
same hyperparameters on real data (1.4 millions of
subject-object tuples) acquired from the British Na-
tional Corpus2 using the Stanford Parser (de Marn-
effe et al, 2006). The algorithm reached the opti-
mal perplexity with 427 frames and 144 roles. This
experiment has been performed only for illustrating
the algorithm on real data. Because of long running
time of the algorithm on such huge data set, we did
not perform the same experiments as with the case
of the small synthetic data.
5 Conclusion
In this paper we presented a method for estimat-
ing the number of frames and roles for the LDA-
frames model. The idea is based on using the Chi-
nese Restaurant Process and the Dirichlet Process
instead of the Dirichlet Distributions and selecting
such parameters that maximize the posterior proba-
bility of the model for given training data. An ex-
periment showed that the non-parametric algorithm
2http://www.natcorp.ox.ac.uk
infers correct values of both the number of frames
and roles on a synthetic data set.
Acknowledgments
This work has been partly supported by the Min-
istry of Education of the Czech Republic under the
project LINDAT-Clarin LM2010013.
References
Aldous, D. J. (1985). Exchangeability and Related Top-
ics. E?cole d?E?te? de Probabilite?s de Saint-Flour XIII ?
1983, 1117:1 ? 198.
de Marneffe, M.-C., MacCartney, B., and Manning, C. D.
(2006). Generating Typed Dependency Parses from
Phrase Structure Parses. In The International Confer-
ence on Language Resources and Evaluation (LREC)
2006.
Ferguson, T. S. (1973). A Bayesian Analysis of Some
Nonparametric Problems. The Annals of Statistics,
1:209 ? 230.
Heinrich, G. (2011). ?Infinite LDA? ? Implementing the
HDP with Minimum Code complexity. Technical re-
port.
Materna, J. (2012). LDA-Frames: An Unsupervised Ap-
proach to Generating Semantic Frames. In Gelbukh,
A., editor, Proceedings of the 13th International Con-
ference CICLing 2012, Part I, pages 376?387, New
Delhi, India. Springer Berlin / Heidelberg.
Neal, R. M. (2000). Markov Chain Sampling Methods
for Dirichlet Process Mixture Models. Journal of com-
putational and graphical statistics, 9(2):249?265.
Pitman, J. (2002). Combinatorial Stochastic Processes.
Lecture Notes for St. Flour Summer School.
Porteous, I. (2010). Networks of mixture blocks for non
parametric bayesian models with applications. PhD
thesis, University of California.
Ruppenhofer, J., Ellsworth, M., Petruck, M. R. L.,
Johnson, C. R., and Scheffczyk, J. (2006).
FrameNet II: Extended Theory and Practice.
http://www.icsi.berkeley.edu/framenet.
Schuler, K. K. (2006). VerbNet: A Broad-Coverage,
Comprehensive Verb Lexicon. PhD thesis, University
of Pennsylvania.
Teh, Y. W., Jordan, M. I., Beal, M. J., and Blei, D. M.
(2006). Hierarchical Dirichlet processes . Journal
of the American Statistical Association, 101:1566 ?
1581.
486
