Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 984?992,
Beijing, August 2010
Multi-Document Summarization via
the Minimum Dominating Set
Chao Shen and Tao Li
School of Computing and Information Sciences
Florida Internation University
{cshen001|taoli}@cs.fiu.edu
Abstract
Multi-document summarization has
been an important problem in infor-
mation retrieval. It aims to dis-
till the most important information
from a set of documents to gener-
ate a compressed summary. Given
a sentence graph generated from a
set of documents where vertices repre-
sent sentences and edges indicate that
the corresponding vertices are simi-
lar, the extracted summary can be de-
scribed using the idea of graph dom-
ination. In this paper, we propose
a new principled and versatile frame-
work for multi-document summariza-
tion using the minimum dominating
set. We show that four well-known
summarization tasks including generic,
query-focused, update, and compara-
tive summarization can be modeled as
different variations derived from the
proposed framework. Approximation
algorithms for performing summariza-
tion are also proposed and empirical
experiments are conducted to demon-
strate the effectiveness of our proposed
framework.
1 Introduction
As a fundamental and effective tool for docu-
ment understanding and organization, multi-
document summarization enables better infor-
mation services by creating concise and infor-
mative reports for a large collection of doc-
uments. Specifically, in multi-document sum-
marization, given a set of documents as input,
the goal is to produce a condensation (i.e.,
a generated summary) of the content of the
entire input set (Jurafsky and Martin, 2008).
The generated summary can be generic where
it simply gives the important information con-
tained in the input documents without any
particular information needs or query/topic-
focused where it is produced in response to a
user query or related to a topic or concern the
development of an event (Jurafsky and Mar-
tin, 2008; Mani, 2001).
Recently, new summarization tasks such as
update summarization (Dang and Owczarzak,
2008) and comparative summarization (Wang
et al, 2009a) have also been proposed. Up-
date summarization aims to generate short
summaries of recent documents to capture
new information different from earlier docu-
ments and comparative summarization aims
to summarize the differences between compa-
rable document groups.
In this paper, we propose a new principled
and versatile framework for multi-document
summarization using the minimum dominat-
ing set. Many known summarization tasks in-
cluding generic, query-focused, update, and
comparative summarization can be modeled
as different variations derived from the pro-
posed framework. The framework provides an
elegant basis to establish the connections be-
tween various summarization tasks while high-
lighting their differences.
In our framework, a sentence graph is first
generated from the input documents where
vertices represent sentences and edges indicate
that the corresponding vertices are similar. A
natural method for describing the extracted
summary is based on the idea of graph dom-
ination (Wu and Li, 2001). A dominating set
of a graph is a subset of vertices such that
every vertex in the graph is either in the sub-
set or adjacent to a vertex in the subset; and
984
a minimum dominating set is a dominating
set with the minimum size. The minimum
dominating set of the sentence graph can be
naturally used to describe the summary: it
is representative since each sentence is either
in the minimum dominating set or connected
to one sentence in the set; and it is with
minimal redundancy since the set is of mini-
mum size. Approximation algorithms are pro-
posed for performing summarization and em-
pirical experiments are conducted to demon-
strate the effectiveness of our proposed frame-
work. Though the dominating set problem has
been widely used in wireless networks, this pa-
per is the first work on using it for modeling
sentence extraction in document summariza-
tion.
The rest of the paper is organized as fol-
lows. In Section 2, we review the related work
about multi-document summarization and the
dominating set. After introducing the min-
imum dominating set problem in graph the-
ory in Section 3, we propose the minimum
dominating set based framework for multi-
document summarization and model the four
summarization tasks including generic, query-
focused, update, and comparative summariza-
tion in Section 4. Section 5 presents the exper-
imental results and analysis, and finally Sec-
tion 6 concludes the paper.
2 Related Work
Generic Summarization For generic sum-
marization, a saliency score is usually as-
signed to each sentence and then the sen-
tences are ranked according to the saliency
score. The scores are usually computed based
on a combination of statistical and linguistic
features. MEAD (Radev et al, 2004) is an
implementation of the centroid-based method
where the sentence scores are computed based
on sentence-level and inter-sentence features.
SumBasic (Nenkova and Vanderwende, 2005)
shows that the frequency of content words
alone can also lead good summarization re-
sults. Graph-based methods (Erkan and
Radev, 2004; Wan et al, 2007b) have also
been proposed to rank sentences or passages
based on the PageRank algorithm or its vari-
ants.
Query-Focused Summarization In
query-focused summarization, the informa-
tion of the given topic or query should be
incorporated into summarizers, and sentences
suiting the user?s declared information need
should be extracted. Many methods for
generic summarization can be extended to
incorporate the query information (Saggion
et al, 2003; Wei et al, 2008). Wan et al
(Wan et al, 2007a) make full use of both
the relationships among all the sentences in
the documents and relationship between the
given query and the sentences by manifold
ranking. Probability models have also been
proposed with different assumptions on the
generation process of the documents and
the queries (Daume? III and Marcu, 2006;
Haghighi and Vanderwende, 2009; Tang et
al., 2009).
Update Summarization and Compara-
tive Summarization Update summariza-
tion was introduced in Document Understand-
ing Conference (DUC) 2007 (Dang, 2007) and
was a main task of the summarization track in
Text Analysis Conference (TAC) 2008 (Dang
and Owczarzak, 2008). It is required to sum-
marize a set of documents under the assump-
tion that the reader has already read and
summarized the first set of documents as the
main summary. To produce the update sum-
mary, some strategies are required to avoid re-
dundant information which has already been
covered by the main summary. One of the
most frequently used methods for remov-
ing redundancy is Maximal Marginal Rele-
vance(MMR) (Goldstein et al, 2000). Com-
parative document summarization is proposed
by Wang et. al. (Wang et al, 2009a) to
summarize the differences between compara-
ble document groups. A sentence selection
approach is proposed in (Wang et al, 2009a)
to accurately discriminate the documents in
different groups modeled by the conditional
entropy.
985
The Dominating Set Many approxima-
tion algorithms have been developed for find-
ing minimum dominating set for a given
graph (Guha and Khuller, 1998; Thai et al,
2007). Kann (Kann, 1992) shows that the
minimum dominating set problem is equiv-
alent to set cover problem, which is a well-
known NP-hard problem. Dominating set has
been widely used for clustering in wireless net-
works (Chen and Liestman, 2002; Han and
Jia, 2007). It has been used to find topic
words for hierarchical summarization (Lawrie
et al, 2001), where a set of topic words is ex-
tracted as a dominating set of word graph. In
our work, we use the minimum dominating set
to formalize the sentence extraction for docu-
ment summarization.
3 The Minimum Dominating Set
Problem
Given a graph G =< V,E >, a dominating
set of G is a subset S of vertices with the
following property: each vertex of G is either
in the dominating set S, or is adjacent to some
vertices in S.
Problem 3.1. Given a graph G, the mini-
mum dominating set problem (MDS) is to find
a minimum size subset S of vertices, such that
S forms a dominating set.
MDS is closely related to the set cover prob-
lem (SC), a well-known NP-hard problem.
Problem 3.2. Given F , a finite collection
{S1, S2, . . . , Sn} of finite sets, the set cover
problem (SC) is to find the optimal solution
F ? = arg min
F ??F
|F ?| s.t.
?
S??F ?
S? =
?
S?F
S.
Theorem 3.3. There exists a pair of polyno-
mial time reduction between MDS and SC.
So, MDS is also NP-hard and it has been
shown that there are no approximate solutions
within c log |V |, for some c > 0 (Feige, 1998;
Raz and Safra, 1997).
3.1 An Approximation Algorithm
A greedy approximation algorithm for the SC
problem is described in (Johnson, 1973). Ba-
sically, at each stage, the greedy algorithm
chooses the set which contains the largest
number of uncovered elements.
Based on Theorem 3.3, we can obtain a
greedy approximation algorithm for MDS.
Starting from an empty set, if the current sub-
set of vertices is not the dominating set, a new
vertex which has the most number of the ad-
jacent vertices that are not adjacent to any
vertex in the current set will be added.
Proposition 3.4. The greedy algorithm ap-
proximates SC within 1 + ln s where s is the
size of the largest set.
It was shown in (Johnson, 1973) that the
approximation factor for the greedy algorithm
is no more thanH(s) , the s-th harmonic num-
ber:
H(s) =
s?
k=1
1
k ? ln s+ 1
Corollary 3.5. MDS has a approximation al-
gorithm within 1 + ln? where ? is the maxi-
mum degree of the graph.
Corollary 3.5 follows directly from Theo-
rem 3.3 and Proposition 3.4.
4 The Summarization Framework
4.1 Sentence Graph Generation
To perform multi-document summarization
via minimum dominating set, we need to first
construct a sentence graph in which each node
is a sentence in the document collection. In
our work, we represent the sentences as vec-
tors based on tf-isf, and then obtain the cosine
similarity for each pair of sentences. If the
similarity between a pair of sentences si and
sj is above a given threshold ?, then there is
an edge between si and sj .
For generic summarization, we use all sen-
tences for building the sentence graph. For
query-focused summarization, we only use the
sentences containing at least one term in the
query. In addition, when a query q is involved,
we assign each node si a weight, w(si) =
d(si, q) = 1 ? cos(si, q), to indicate the dis-
tance between the sentence and the query q.
After building the sentence graph, we can
formulate the summarization problem using
986
Generic Summary
(a)
Query-focused Summary
query
(b)
Updated Summary
C
1
C
2
(c)
Comparative Summary
Comparative Summary
Comparative Summary
C
2
C
1
C
3
(d)
Figure 1: Graphical illustrations of multi-document summarization via the minimum domi-
nating set. (a): The minimum dominating set is extracted as the generic summary. (b):The
minimum weighted dominating set is extracted as the query-based summary. (c):Vertices in
the right rectangle represent the first document set C1, and ones in the left represent the sec-
ond document set where update summary is generated. (d):Each rectangle represents a group
of documents. The vertices with rings are the dominating set for each group, while the solid
vertices are the complementary dominating set, which is extracted as comparative summaries.
the minimum dominating set. A graphical il-
lustration of the proposed framework is shown
in Figure 1.
4.2 Generic Summarization
Generic summarization is to extract the most
representative sentences to capture the impor-
tant content of the input documents. Without
taking into account the length limitation of
the summary, we can assume that the sum-
mary should represent all the sentences in the
document set (i.e., every sentence in the docu-
ment set should either be extracted or be sim-
ilar with one extracted sentence). Meanwhile,
a summary should also be as short as possi-
ble. Such summary of the input documents
under the assumption is exactly the minimum
dominating set of the sentence graph we con-
structed from the input documents in Section
4.1. Therefore the summarization problem
can be formulated as the minimum dominat-
ing set problem.
However, usually there is a length restric-
tion for generating the summary. Moreover,
the MDS is NP-hard as shown in Section 3.
Therefore, it is straightforward to use a greedy
approximation algorithm to construct a subset
of the dominating set as the final summary. In
the greedy approach, at each stage, a sentence
which is optimal according to the local crite-
ria will be extracted. Algorithm 1 describes
Algorithm 1 Algorithm for Generic Summariza-
tion
INPUT: G, W
OUTPUT: S
1: S = ?
2: T = ?
3: while L(S) < W and V (G)! = S do
4: for v ? V (G)? S do
5: s(v) = |{ADJ(v) ? T}|
6: v? = argmaxv s(v)
7: S = S ? {v?}
8: T = T ?ADJ(v?)
an approximation algorithm for generic sum-
marization. In Algorithm 1, G is the sen-
tence graph, L(S) is the length of the sum-
mary, W is the maximal length of the sum-
mary, and ADJ(v) = {v?|(v?, v) ? E(G)} is
the set of vertices which are adjacent to the
vertex v. A graphical illustration of generic
summarization using the minimum dominat-
ing set is shown in Figure 1(a).
4.3 Query-Focused Summarization
Letting G be the sentence graph constructed
in Section 4.1 and q be the query, the query-
focused summarization can be modeled as
D? = argminD?G
?
s?D d(s, q) (1)
s.t. D is a dominating set of G.
Note that d(s, q) can be viewed as the weight
of vertex in G. Here the summary length is
minimized implicitly, since if D? ? D, then
987
?
s?D? d(s, q) ?
?
s?D d(s, q). The problem
in Eq.(1) is exactly a variant of the minimum
dominating set problem, i.e., the minimum
weighted dominating set problem (MWDS).
Similar to MDS, MWDS can be reduced
from the weighted version of the SC problem.
In the weighted version of SC, each set has a
weight and the sum of weights of selected sets
needs to be minimized. To generate an ap-
proximate solution for the weighted SC prob-
lem, instead of choosing a set i maximizing
|SET (i)|, a set i minimizing w(i)|SET (i)| is cho-
sen, where SET (i) is composed of uncovered
elements in set i, and w(i) is the weight of set
i. The approximate solution has the same ap-
proximation ratio as that for MDS, as stated
by the following theorem (Chvatal, 1979).
Theorem 4.1. An approximate weighted
dominating set can be generated with a size at
most 1+log??|OPT |, where ? is the maximal
degree of the graph and OPT is the optimal
weighted dominating set.
Accordingly, from generic summarization to
query-focused summarization, we just need to
modify line 6 in Algorithm 1 to
v? = argmin
v
w(v)
s(v) , (2)
where w(v) is the weight of vertex v. A graph-
ical illustration of query-focused summariza-
tion using the minimum dominating set is
shown in Figure 1(b).
4.4 Update Summarization
Give a query q and two sets of documents C1
and C2, update summarization is to generate
a summary of C2 based on q, given C1. Firstly,
summary of C1, referred as D1 can be gener-
ated. Then, to generate the update summary
of C2, referred as D2, we assume D1 and D2
should represent all query related sentences in
C2, and length of D2 should be minimized.
Let G1 be the sentence graph for C1. First
we use the method described in Section 4.3 to
extract sentences from G1 to form D1. Then
we expand G1 to the whole graph G using the
second set of documents C2. G is then the
graph presentation of the document set in-
cluding C1 and C2. We can model the update
summary of C2 as
D? = argminD2
?
s?D2 w(s) (3)
s.t. D2 ?D1 is a dominating set of G.
Intuitively, we extract the smallest set of sen-
tences that are closely related to the query
from C2 to complete the partial dominating
set of G generated from D1. A graphical il-
lustration of update summarization using the
minimum dominating set is shown in Fig-
ure 1(c).
4.5 Comparative Summarization
Comparative document summarization aims
to summarize the differences among compara-
ble document groups. The summary produced
for each group should emphasize its difference
from other groups (Wang et al, 2009a).
We extend our method for update sum-
marization to generate the discriminant sum-
mary for each group of documents. Given N
groups of documents C1, C2, . . . , CN , we first
generate the sentence graphs G1, G2, . . . , GN ,
respectively. To generate the summary for
Ci, 1 ? i ? N , we view Ci as the update
of all other groups. To extract a new sen-
tence, only the one connected with the largest
number of sentences which have no represen-
tatives in any groups will be extracted. We
denote the extracted set as the complemen-
tary dominating set, since for each group we
obtain a subset of vertices dominating those
are not dominated by the dominating sets of
other groups. To perform comparative sum-
marization, we first extract the standard dom-
inating sets for G1, . . . , GN , respectively, de-
noted as D1, . . . , DN . Then we extract the
so-called complementary dominating set CDi
for Gi by continuing adding vertices in Gi to
find the dominating set of ?1?j?NGj given
D1, . . . ,Di?1,Di+1, . . . ,DN . A graphical il-
lustration of comparative summarization is
shown in Figure 1(d).
988
DUC04 DUC05 DUC06 TAC08 A TAC08 B
Type of Summarization Generic Topic-focused Topic-focused Topic-focused Update
#topics NA 50 50 48 48
#documents per topic 10 25-50 25 10 10
Summary length 665 bytes 250 words 250 words 100 words 100 words
Table 1: Brief description of the data set
5 Experiments
We have conducted experiments on all four
summarization tasks and our proposed meth-
ods based on the minimum dominating set
have outperformed many existing methods.
For the generic, topic-focused and update
summarization tasks, the experiments are per-
form the DUC data sets using ROUGE-2 and
ROUGE-SU (Lin and Hovy, 2003) as evalua-
tion measures. For comparative summariza-
tion, a case study as in (Wang et al, 2009a) is
performed. Table 1 shows the characteristics
of the data sets. We use DUC04 data set to
evaluate our method for generic summariza-
tion task and DUC05 and DUC06 data sets
for query-focused summarization task. The
data set for update summarization, (i.e. the
main task of TAC 2008 summarization track)
consists of 48 topics and 20 newswire articles
for each topic. The 20 articles are grouped
into two clusters. The task requires to pro-
duce 2 summaries, including the initial sum-
mary (TAC08 A) which is standard query-
focused summarization and the update sum-
mary (TAC08 B) under the assumption that
the reader has already read the first 10 docu-
ments.
We apply a 5-fold cross-validation proce-
dure to choose the threshold ? used for gener-
ating the sentence graph in our method.
5.1 Generic Summarization
We implement the following widely used or
recent published methods for generic summa-
rization as the baseline systems to compare
with our proposed method (denoted as MDS).
(1) Centroid: The method applies MEAD al-
gorithm (Radev et al, 2004) to extract sen-
tences according to the following three pa-
rameters: centroid value, positional value,
and first-sentence overlap. (2) LexPageR-
ank: The method first constructs a sentence
connectivity graph based on cosine similarity
and then selects important sentences based on
the concept of eigenvector centrality (Erkan
and Radev, 2004). (3) BSTM: A Bayesian
sentence-based topic model making use of
both the term-document and term-sentence
associations (Wang et al, 2009b).
Our method outperforms the simple Cen-
troid method and another graph-based Lex-
PageRank, and its performance is close to the
results of the Bayesian sentence-based topic
model and those of the best team in the DUC
competition. Note however that, like clus-
tering or topic based methods, BSTM needs
the topic number as the input, which usually
varies by different summarization tasks and is
hard to estimate.
5.2 Query-Focused Summarization
We compare our method (denoted as MWDS)
described in Section 4.3 with some recently
published systems. (1) TMR (Tang et al,
2009): incorporates the query information
into the topic model, and uses topic based
score and term frequency to estimate the im-
portance of the sentences. (2) SNMF (Wang
et al, 2008): calculates sentence-sentence
similarities by sentence-level semantic analy-
sis, clusters the sentences via symmetric non-
negative matrix factorization, and extracts
the sentences based on the clustering result.
(3) Wiki (Nastase, 2008): uses Wikipedia
as external knowledge to expand query and
builds the connection between the query and
the sentences in documents.
Table 3 presents the experimental compar-
ison of query-focused summarization on the
two datasets. From Table 3, we observe that
our method is comparable with these systems.
This is due to the good interpretation of the
summary extracted by our method, an ap-
989
DUC04
ROUGE-2 ROUGE-SU
DUC Best 0.09216 0.13233
Centroid 0.07379 0.12511
LexPageRank 0.08572 0.13097
BSTM 0.09010 0.13218
MDS 0.08934 0.13137
Table 2: Results on generic summariza-
tion.
DUC05 DUC06
ROUGE-2 ROUGE-SU ROUGE-2 ROUGE-SU
DUC Best 0.0725 0.1316 0.09510 0.15470
SNMF 0.06043 0.12298 0.08549 0.13981
TMR 0.07147 0.13038 0.09132 0.15037
Wiki 0.07074 0.13002 0.08091 0.14022
MWDS 0.07311 0.13061 0.09296 0.14797
Table 3: Results on query-focused summariza-
tion.
proximate minimal dominating set of the sen-
tence graph. On DUC05, our method achieves
the best result; and on DUC06, our method
outperforms all other systems except the best
team in DUC. Note that our method based
on the minimum dominating set is much sim-
pler than other systems. Our method only
depends on the distance to the query and has
only one parameter (i.e., the threshold ? in
generating the sentence graph).
 0.065
 0.07
 0.075
 0.08
 0.085
 0.09
 0.095
 0.05  0.1  0.15  0.2  0.25
R
O
UG
E-
2
Similarity threshold ?
DUC 06
DUC 05
Figure 2: ROUGE-2 vs. threshold ?
We also conduct experiments to empirically
evaluate the sensitivity of the threshold ?.
Figure 2 shows the ROUGE-2 curve of our
MWDS method on the two datasets when ?
varies from 0.04 to 0.26. When ? is small,
edges fail to represent the similarity of the sen-
tences, while if ? is too large, the graph will
be sparse. As ? is approximately in the range
of 0.1? 0.17, ROUGE-2 value becomes stable
and relatively high.
5.3 Update Summarization
Table 5 presents the experimental results on
update summarization. In Table 5, ?TAC
Best? and ?TAC Median? represent the best
and median results from the participants of
TAC 2008 summarization track in the two
tasks respectively according to the TAC 2008
report (Dang and Owczarzak, 2008). As seen
from the results, the ROUGE scores of our
methods are higher than the median results.
The good results of the best team typically
come from the fact that they utilize advanced
natural language processing (NLP) techniques
to resolve pronouns and other anaphoric ex-
pressions. Although we can spend more efforts
on the preprocessing or language processing
step, our goal here is to demonstrate the ef-
fectiveness of formalizing the update summa-
rization problem using the minimum dominat-
ing set and hence we do not utilize advanced
NLP techniques for preprocessing. The exper-
imental results demonstrate that our simple
update summarization method based on the
minimum dominating set can lead to compet-
itive performance for update summarization.
TAC08 A TAC08 B
ROUGE-2 ROUGE-
SU
ROUGE-2 ROUGE-
SU
TAC Best 0.1114 0.14298 0.10108 0.13669
TAC Median 0.08123 0.11975 0.06927 0.11046
MWDS 0.09012 0.12094 0.08117 0.11728
Table 5: Results on update summarization.
5.4 Comparative Summarization
We use the top six largest clusters of doc-
uments from TDT2 corpora to compare the
summary generated by different comparative
summarization methods. The topics of the six
document clusters are as follows: topic 1: Iraq
Issues; topic 2: Asia?s economic crisis; topic 3:
Lewinsky scandal; topic 4: Nagano Olympic
Games; topic 5: Nuclear Issues in Indian and
Pakistan; and topic 6: Jakarta Riot. From
each of the topics, 30 documents are extracted
990
Topic Complementary Dominating Set Discriminative Sentence Selection Dominating Set
1 ? ? ? U.S. Secretary of State
Madeleine Albright arrives to
consult on the stand-off between
the United Nations and Iraq.
the U.S. envoy to the United
Nations, Bill Richardson, ? ? ?
play down China?s refusal to sup-
port threats of military force
against Iraq
The United States and Britain
do not trust President Sad-
dam and wants cdotswarning
of serious consequences if Iraq
violates the accord.
2 Thailand?s currency, the
baht, dropped through a
key psychological level of ? ? ?
amid a regional sell-off sparked
by escalating social unrest in
Indonesia.
Earlier, driven largely by the de-
clining yen, South Korea?s
stock market fell by ? ? ? , while
the Nikkei 225 benchmark in-
dex dipped below 15,000 in the
morning ? ? ?
In the fourth quarter, IBM
Corp. earned $2.1 billion, up
3.4 percent from $2 billion a
year earlier.
3 ? ? ? attorneys representing Pres-
ident Clinton and Monica
Lewinsky.
The following night Isikoff ? ? ? ,
where he directly followed the
recitation of the top-10 list: ?Top
10 White House Jobs That
Sound Dirty.?
In Washington, Ken Starr?s
grand jury continued its inves-
tigation of theMonica Lewin-
sky matter.
4 Eight women and six men were
named Saturday night as the
first U.S. Olympic Snow-
board Team as their sport
gets set to make its debut in
Nagano, Japan.
this tunnel is finland?s cross coun-
try version of tokyo?s alpine ski
dome, and olympic skiers flock
from russia, ? ? ? , france and aus-
tria this past summer to work out
the kinks ? ? ?
If the skiers the men?s super-
G and the women?s downhill
on Saturday, they will be back
on schedule.
5 U.S. officials have announced
sanctions Washington will im-
pose on India and Pakistan
for conducting nuclear tests.
The sanctions would stop all for-
eign aid except for humanitarian
purposes, ban military sales to
India ? ? ?
And Pakistan?s prime min-
ister says his country will sign
the U.N.?s comprehensive
ban on nuclear tests if In-
dia does, too.
6 ? ? ? remain in force around
Jakarta, and at the Parliament
building where thousands of
students staged a sit-in Tues-
day ? ? ? .
?President Suharto has given
much to his country over the
past 30 years, raising Indone-
sia?s standing in the world ? ? ?
What were the students doing
at the time you were there, and
what was the reaction of the
students to the troops?
Table 4: A case study on comparative document summarization. Some unimportant words are skipped due to
the space limit. The bold font is used to annotate the phrases that are highly related with the topics, and italic
font is used to highlight the sentences that are not proper to be used in the summary.
randomly to produce a one-sentence summary.
For comparison purpose, we extract the sen-
tence with the maximal degree as the base-
line. Note that the baseline can be thought
as an approximation of the dominating set
using only one sentence. Table 4 shows the
summaries generated by our method (comple-
mentary dominating set (CDS)), discrimina-
tive sentence selection (DSS) (Wang et al,
2009a) and the baseline method. Our CDS
method can extract discriminative sentences
for all the topics. DSS can extract discrimina-
tive sentences for all the topics except topic 4.
Note that the sentence extracted by DSS for
topic 4 may be discriminative from other top-
ics, but it is deviated from the topic Nagano
Olympic Games. In addition, DSS tends to
select long sentences which should not be pre-
ferred for summarization purpose. The base-
line method may extract some general sen-
tences, such as the sentence for topic 2 and
topic 6 in Table 4.
6 Conclusion
In this paper, we propose a framework to
model the multi-document summarization us-
ing the minimum dominating set and show
that many well-known summarization tasks
can be formulated using the proposed frame-
work. The proposed framework leads to sim-
ple yet effective summarization methods. Ex-
perimental results show that our proposed
methods achieve good performance on several
multi-document document tasks.
7 Acknowledgements
This work is supported by NSF grants IIS-
0549280 and HRD-0833093.
991
References
Chen, Y.P. and A.L. Liestman. 2002. Approximating
minimum size weakly-connected dominating sets for
clustering mobile ad hoc networks. In Proceedings
of International Symposium on Mobile Ad hoc Net-
working & Computing. ACM.
Chvatal, V. 1979. A greedy heuristic for the set-
covering problem. Mathematics of operations re-
search, 4(3):233?235.
Dang, H.T. and K Owczarzak. 2008. Overview of the
TAC 2008 Update Summarization Task. In Pro-
ceedings of the Text Analysis Conference (TAC).
Dang, H.T. 2007. Overview of DUC 2007. In Docu-
ment Understanding Conference.
Daume? III, H. and D. Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of the ACL-
COLING.
Erkan, G. and D.R. Radev. 2004. Lexpagerank: Pres-
tige in multi-document text summarization. In Pro-
ceedings of EMNLP.
Feige, U. 1998. A threshold of lnn for approximating
set cover. Journal of the ACM (JACM), 45(4):634?
652.
Goldstein, J., V. Mittal, J. Carbonell, and
M. Kantrowitz. 2000. Multi-document summariza-
tion by sentence extraction. In NAACL-ANLP 2000
Workshop on Automatic summarization.
Guha, S. and S. Khuller. 1998. Approximation algo-
rithms for connected dominating sets. Algorithmica,
20(4):374?387.
Haghighi, A. and L. Vanderwende. 2009. Exploring
content models for multi-document summarization.
In Proceedings of HLT-NAACL.
Han, B. and W. Jia. 2007. Clustering wireless ad
hoc networks with weakly connected dominating
set. Journal of Parallel and Distributed Computing,
67(6):727?737.
Johnson, D.S. 1973. Approximation algorithms for
combinatorial problems. In Proceedings of STOC.
Jurafsky, D. and J.H. Martin. 2008. Speech and lan-
guage processing. Prentice Hall New York.
Kann, V. 1992. On the approximability of NP-
complete optimization problems. PhD thesis, De-
partment of Numerical Analysis and Computing
Science, Royal Institute of Technology, Stockholm.
Lawrie, D., W.B. Croft, and A. Rosenberg. 2001.
Finding topic words for hierarchical summarization.
In Proceedings of SIGIR.
Lin, C.Y. and E. Hovy. 2003. Automatic evaluation
of summaries using n-gram co-occurrence statistics.
In Proceedings of HLT-NAACL.
Mani, I. 2001. Automatic summarization. Computa-
tional Linguistics, 28(2).
Nastase, V. 2008. Topic-driven multi-document
summarization with encyclopedic knowledge and
spreading activation. In Proceedings of EMNLP.
Nenkova, A. and L. Vanderwende. 2005. The impact
of frequency on summarization. Microsoft Research,
Redmond, Washington, Tech. Rep. MSR-TR-2005-
101.
Radev, D.R., H. Jing, M. Stys?, and D. Tam. 2004.
Centroid-based summarization of multiple docu-
ments. Information Processing and Management,
40(6):919?938.
Raz, R. and S. Safra. 1997. A sub-constant error-
probability low-degree test, and a sub-constant
error-probability PCP characterization of NP. In
Proceedings of STOC.
Saggion, H., K. Bontcheva, and H. Cunningham. 2003.
Robust generic and query-based summarisation. In
Proceedings of EACL.
Tang, J., L. Yao, and D. Chen. 2009. Multi-topic
based Query-oriented Summarization. In Proceed-
ings of SDM.
Thai, M.T., N. Zhang, R. Tiwari, and X. Xu. 2007.
On approximation algorithms of k-connected m-
dominating sets in disk graphs. Theoretical Com-
puter Science, 385(1-3):49?59.
Wan, X., J. Yang, and J. Xiao. 2007a. Manifold-
ranking based topic-focused multi-document sum-
marization. In Proceedings of IJCAI.
Wan, X., J. Yang, and J. Xiao. 2007b. Towards an
iterative reinforcement approach for simultaneous
document summarization and keyword extraction.
In Proceedings of ACL.
Wang, D., T. Li, S. Zhu, and C. Ding. 2008. Multi-
document summarization via sentence-level seman-
tic analysis and symmetric matrix factorization. In
Proceedings of SIGIR.
Wang, D., S. Zhu, T. Li, and Y. Gong. 2009a. Com-
parative document summarization via discrimina-
tive sentence selection. In Proceeding of CIKM.
Wang, D., S. Zhu, T. Li, and Y. Gong. 2009b.
Multi-document summarization using sentence-
based topic models. In Proceedings of the ACL-
IJCNLP.
Wei, F., W. Li, Q. Lu, and Y. He. 2008. Query-
sensitive mutual reinforcement chain and its ap-
plication in query-oriented multi-document summa-
rization. In Proceedings of SIGIR.
Wu, J. and H. Li. 2001. A dominating-set-based rout-
ing scheme in ad hoc wireless networks. Telecom-
munication Systems, 18(1):13?36.
992
