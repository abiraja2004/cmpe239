Proceedings of the 6th Workshop on Statistical Machine Translation, pages 227?236,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Fuzzy Syntactic Reordering
for Phrase-based Statistical Machine Translation
Jacob Andreas and Nizar Habash and Owen Rambow
Center for Computational Learning Systems
Columbia University
jda2129@columbia.edu
{habash,rambow}@ccls.columbia.edu
Abstract
The quality of Arabic-English statistical ma-
chine translation often suffers as a result of
standard phrase-based SMT systems? inabil-
ity to perform long-range re-orderings, specif-
ically those needed to translate VSO-ordered
Arabic sentences. This problem is further ex-
acerbated by the low performance of Arabic
parsers on subject and subject span detection.
In this paper, we present two parse ?fuzzi-
fication? techniques which allow the transla-
tion system to select among a range of pos-
sible S?V re-orderings. With this approach,
we demonstrate a 0.3-point improvement in
BLEU score (69% of the maximum possible
using gold parses), and a corresponding im-
provement in the percentage of syntactically
well-formed subjects under a manual evalua-
tion.
1 Introduction
The question of how to effectively use phrase-based
statistical machine translation (PSMT) to translate
between language pairs which require long-range re-
ordering has attracted a great deal of interest in re-
cent years. The inability to capture long-range re-
ordering behaviors is a weakness inherent in PSMT
systems, which typically have only two mechanisms
to control the reordering between source and tar-
get language: (1) distortion penalties, which penal-
ize or forbid long-distance re-orderings in order to
reduce the search space explored by the decoder,
and (2) lexicalized reordering models, which cap-
ture the preferences of individual phrases to orient
themselves monotonically, reversed with their pre-
ceding phrases or discontinuously. Because both
of these mechanisms work at the phrase level, they
have proven very effective at capturing short-range
reordering behaviors, but unable to describe long
range movements; in fact, the distortion penalty ef-
fectively causes the translation system to not pre-
fer long-range re-orderings, even when they are as-
signed significantly higher probability by the lan-
guage model.
The problem is particularly acute in translating
from Arabic to English: Arabic sentences frequently
exhibit a VSO ordering (both VSO and SVO are
permitted in Arabic), while English permits only
an SVO order. Past research has shown that verb
anticipation and subject-span detection is a ma-
jor source of error when translating from Arabic
to English (Green et al, 2009; Bisazza and Fed-
erico, 2010). Unable to perform long-range reorder-
ing, PSMT frequently produces English sentences in
which verbs precede their subjects (sometimes with
?hallucinated? pronouns in front of them) or do not
appear at all. Intuitively, better handling of these re-
orderings has the potential to improve both accuracy
and fluency of translation.
In this paper, we present two parse fuzzification
techniques which allow the translation system to se-
lect among a range of possible S?V re-orderings.
With this approach, we demonstrate a 0.3-point im-
provement in BLEU score (69% of the maximum
possible using gold parses), and a corresponding im-
provement in the percentage of syntactically well-
formed subjects under a manual evaluation.
The rest of the paper is structured as follows. Sec-
tion 2 gives a review of research on this topic. Sec-
tion 3 motivates the approach discussed in Section 4.
227
Section 5 presents the results of a set of machine
translation experiments using the automatic metrics
BLEU (Papineni et al, 2002) and METEOR (Baner-
jee and Lavie, 2005), and a manual-evaluation of
subject integrity. Section 6 discusses our conclu-
sions and future plans.
2 Related Work
The general approach pursued in this paper?that
of using pre-ordering to improve translation output?
has been explored by many researchers. Most
work has focused on automatically learning reorder-
ing rules (Xia and McCord, 2004; Habash, 2007b;
Elming, 2008; Elming and Habash, 2009; Dyer
and Resnik, 2010). Xia and McCord (2004) de-
scribe an approach for translation from French to
English, where context-free constituency reordering
rules are acquired automatically using source and
target parses and word alignment. Elming (2008)
and Elming and Habash (2009) use a large set of
linguistic features to automatically learn reordering
rules for English-Danish and English-Arabic; the
rules are used to pre-order the input into a lattice
of variant orders. Habash (2007b) learns syntactic
reordering rules targeting Arabic-English word or-
der differences and integrated them as deterministic
preprocessing. He reports improvements in BLEU
compared to phrase-based SMT limited to mono-
tonic decoding, but these improvements do not hold
with distortion. He hypothesizes that parse errors
are responsible for lack of improvement. Dyer and
Resnik (2010) use an input forest structure to rep-
resent word-order alternatives and learn models for
long-range source reordering that maximize trans-
lation quality. Their results for Arabic-English are
negative.
In contrast to these approaches, Collins et al
(2005) apply six manually defined transformations
to German parse trees which yield an improvement
on a German-English translation task. In this paper,
we follow Collins et al (2005) and restrict ourselves
to handcrafted rules (in our case, actually a single
over-generating rule) motivated by linguistic under-
standing.
One major concern not addressed in any of the
aforementioned research on syntax-based reordering
is the fact that the quality of parsers for many lan-
guages is still quite poor. Collins et al (2005), for
example, assume that the parse trees they use are
correct. While the state-of-the-art in English pars-
ing is fairly good (though far from perfect), this
is not the case in other languages, where parsing
shows substantial error rates. Moreover, when at-
tempting to reorder so as to bring the source text
more grammatically in line with the target language,
a bad parse can be disastrous: moving parts of the
sentence that shouldn?t be moved, and introducing
more distortion error than it is able to correct. To ad-
dress the problem of noisy parse data, Bisazza and
Federico (2010) identify the subject using a chunker,
then fuzzify it, creating a lattice in which the transla-
tion system has a choice of several different paths,
corresponding to re-orderings of different subject
spans.
In investigating syntax-based reordering for Ara-
bic specifically, Carpuat et al (2010) show that a
syntax-driven reordering of the training data only
for the purpose of alignment improvement leads to
a substantial improvement in translation quality, but
do not report a corresponding improvement when re-
ordering test data in a similar fashion. Interestingly,
Bisazza and Federico (2010) report that fuzzy re-
ordering the test data improves MT output, suggest-
ing that fuzzification may be the mechanism neces-
sary to render reordering on test data useful. To the
best of our knowledge, nobody has yet used fuzzifi-
cation to correct the identified subject span of com-
plete Arabic dependency parses. Green et al (2009)
use a conditional random field sequence classifier
to detect Arabic noun phrase subjects in verb-initial
clauses achieving an F-score of 61.3%. They in-
tegrate their classifier?s decisions as additional fea-
tures in the Moses decoder (Koehn et al, 2007), but
do not show any gains.
The present work may be thought of as extending
the fuzzification explored by Bisazza and Federico
(2010) to the domain of full parsing?a combina-
tion, in some sense, of their approach with the work
of Carpuat et al (2010). The approach examined in
this paper differs from Collins et al (2005) in its use
of fuzzification, from Bisazza and Federico (2010)
in its use of a complete dependency parse, and from
Carpuat et al (2010) in its use of a reordered test set.
228
Figure 1: An example of a dependency tree of a Verb-Object-Subject Arabic sentence: 	?A??j. ? ??J
? @ ZA?? 	?AK
Q? @ 	Q?	?
J 	j 	j 	?? 	?
KPAJ
? +H. hz AlryAD msA? Alywm hjwmAn b+ syArtyn mfxxtyn ?Two car bombs shook Riyadh thisevening?. The predicted tree (on the left) shows an incorrect subject span (words 5-8).
Figure 1: An example of a dependency tree of a Verb-Object-Subject Arabic sentence: ??????? ? ????? ? ?? ? ??????? ? ????????? ?? ?? ??? ??????????? +?? hz AlryAD msA? Alywm hjwmAn b+ syArtyn mfxxtyn ?Two car bombs shook Riyadh thisevening?. The predicted tree (on the left) shows an incorrect subject span (words 5-8).
Gold Tree
VRB
??? hz1
?shook?
OBJ
PROP??????? ? AlryAD2
?RiyAdh?
MOD
NOM
?? ? msA?3
?evening?
IDF
NOM
????? ? Alywm4
?today?
SBJ
NOM??????? ? hjwmAn5
?two attacks?
MOD
PRT
+?? b+6
?with?
OBJ
NOM??????????? syArtyn7
?two cars?
MOD
NOM?????? ?? ?? ??? mfxxtyn8
?booby-trapped?
Predicted Tree
VRB
??? hz1
?shook?
MOD
PROP??????? ? AlryAD2
?RiyAdh?
MOD
NOM
?? ? msA?3
?evening?
IDF
NOM
????? ? Alywm4
?today?
SBJ
NOM??????? ? hjwmAn5
?two attacks?
MOD
PRT
+?? b+6
?with?
OBJ
NOM??????????? syArtyn7
?two cars?
MOD
NOM?????? ?? ?? ??? mfxxtyn8
?booby-trapped?
We focused on correcting the largest sources of er-
ror: incorrect span and false-positive subjects. As
false-positive subject corrections were already cap-
tured by providing a no-reorder option in the lattice,
only span errors needed additional correction.
In principle, spans can be marked incorrectly both
on their front and back ends; however, because left-
dependency is fairly uncommon in Arabic and hap-
pens in a limited number of predictable cases, the
system made so few errors in identifying the left
boundary of spans (1.8%) that it is not worth try-
ing to correct them. [A note on terminology: ?left?
and ?right? are used throughout this paper with ref-
erence to English word order. ?Left? should be un-
derstood to mean ?towards the beginning of the sen-
tence?, and ?right? to mean ?towards the end of the
sentence.?]
The question is thus how to correct the right edge
of spans assuming that label and attachment have
been predicted correctly. Span classifications can be
broken into three categories: those that are too long
(i.e. that have too many right descendants), too short
(i.e. that have too few right descendants), or correct
(so that the predicted tree has all the same descen-
dants as the gold tree). A comparison of gold and
predicted trees for MT05 was conducted, revealing
the following breakdown:
Type # %
Long 260 12.4%
Short 293 14.0%
Correct 1538 73.6%
Total 2091 100%
Table 1: Distribution of span errors
These numbers are quite low: roughly 3 out of
every 10 subjects identified in the corpus have their
spans incorrectly marked. This suggest that fuzzifi-
cation will provide room for improvement. But what
technique should we use to fuzzify the subjects?
To answer this question, we examined more
3 Motivation
While the VSO order is common at both the matrix
and non-matrix level in Arabic newswire text, ma-
trix VSO constructions are almost always reordered
in translation, while non-matrix VSO constructions
are frequently translated monotonically (they are in-
stead passivized or otherwise transformed in a fash-
ion that leaves them parallel to the source Arabic
text) (Carpuat et al, 2010). This reordering, as
noted in the introduction, is notoriously difficult for
phrase-based statistical machine translation systems
to capture. It is further exacerbated by the low
quality of Arabic parsing especially for subject span
identification (Green et al, 2009).
3.1 Reordering
We began by performing a series of reordering ex-
periments using gold-standard parses of the NIST
MT05 data set:1 (a) a baseline experiment with no
reordering, (b) an experiment which forced reorder-
ing on all matrix subjects, and (c) an experiment in
which the translation system was presented with a
lattice, in which one path contained the original sen-
tence and the other path contained the sentence with
the matrix subject reordered. The baseline system
produced a BLEU score of 47.13, forced reorder-
ing produced a BLEU score of 47.43, and optional
reordering produced a BLEU score of 47.55. These
results indicate that, given correct reordering bound-
aries, the translation quality can indeed be improved
with reordered test data. Furthermore, the improve-
ment noted above between the orced eordering and
optional reordering experiments, while small, indi-
cates that even with correct parses it is sometimes
preferable to leave the input sentence un-reordered.
This is consistent with Carpuat et al (2010)?s ob-
1The gold parses for NIST MT05 are part of the Columbia
Arabic Treeebank (CATiB) (Habash and Roth, 2009).
229
servation that even VS-ordered matrix verbs in Ara-
bic are sometimes translated monotonically into En-
glish (as, for example, in passive constructions). An
alternative explanation may be that since the train-
ing data itself is not re-ordered, it is plausible that
some re-ordering may cause otherwise good possi-
ble matches in the phrase table to not match any
more.
3.2 Parser Error
The problem of finding correct subject span bound-
aries for reordering, however, is a particularly dif-
ficult one. Both Habash (2007b) and Green et al
(2009) have noted previously that even state-of-
the-art Arabic dependency parsers tend to perform
poorly, and we would expect that incorrect bound-
aries would do more harm than good for translation.
In order to determine how to ?fix? these spans, it is
first necessary to understand the kinds of errors that
the parser makes. A set of predicted parses of the
NIST MT05 data was compared to the gold parses
of the same data set.
There are three categories of error the parser can
make in identifying subjects: labeling errors, attach-
ment errors and span errors. In labeling errors, the
parser either incorrectly marks a node SBJ when no
such label appears in the gold tree, or fails to identify
one of the gold-labeled SBJs. In attachment error,
the identified subject is marked as depending on the
wrong node. Finally, in span error, the descendants
assigned to a labeled SBJ are wrong. The distribu-
tion of parser errors in the NIST MT05 data is as
follows:
? Label errors: 19.8% of predicted subjects are
not gold subjects, and 19.1% of gold subjects
are not identified as predicted subjects.
? Attachment errors: 16.92% of gold subjects are
incorrectly attached in the predicted tree.
? Span errors: 26.4% of predicted subject spans
are incorrect.
In this paper, we focus on correcting the largest
sources of error: incorrect span and false-positive
subjects. We now provide further analysis of the
span errors.
In principle, spans can be marked incorrectly both
on their front and back ends; however, because left-
dependency is fairly uncommon in Arabic and hap-
pens in a limited number of predictable cases, the
parser made so few errors in identifying the left
boundary of spans (1.8%) that it is not worth trying
to correct them.2
The question is thus how to correct the right edge
of spans assuming that label and attachment have
been predicted correctly. Span classifications can
be broken into three categories: those that are too
long (i.e. that have too many right descendants), too
short (i.e. that have too few right descendants), or
correct (so that the predicted tree has all the same
descendants as the gold tree, without regard to their
syntactic structure). A comparison of gold and pre-
dicted trees for MT05 was conducted, revealing the
distribution shown in Table 1. We see that the 26.4%
of subjects with incorrect spans are roughly equally
divided between subjects that are too short and sub-
jects that are too long.
Type # %
Long 260 12.4%
Short 293 14.0%
Correct 1538 73.6%
Total 2091 100%
Table 1: Distribution of span errors in NIST MT05
To gain further insight into the nature of the sub-
ject span errors, we examined more closely the
26.4% of cases where the span is incorrectly labeled,
looking specifically at the ?difference box?: the set
of contiguous nodes that must be added to or re-
moved from the predicted span to bring it into agree-
ment with the gold span (see Fig. 1).3 Specifically,
we wished to know how many top-level constituents
required addition or removal to cover the entire dif-
ference. The smaller the number of top-level con-
stituents that needs to be added, the fewer reorder-
ing variations possible, and the better the expected
performance of the system.
Roughly 2% of these difference boxes are what
we might call ?pathological? cases: due to some se-
2A note on terminology: ?left? and ?right? are used through-
out this paper with reference to word order when using the Latin
alphabet. ?Left? should be understood to mean ?towards the be-
ginning of the sentence?, and ?right? to mean ?towards the end
of the sentence.?
3Arabic transliteration is presented in the Habash-Soudi-
Buckwalter scheme (Habash et al, 2007).
230
r2 
r1 
a1 
a2 sbj 
r2 
r1 
a1 
a2 
original 
verb 
+ 
+ 
_ 
_ 
Figure 2: A schematic representation of the fuzzification algorithm. The black node is the matrix subject, + indicates
that a node (and its descendants) can be added, ? indicates that a node (and its descendants) can be removed, and the
black brackets denote the boundaries of the candidate spans.
rious error in parsing, there is a constituent inside
the difference box with descendants outside the box.
These are algorithmically very difficult to correct as
they require us to either add a constituent and then
prune it, or remove a constituent and then reattach
some of its children; attempting to correct for this
possibility in all sentences will lead to a combinato-
rial explosion of possible parses. Fortunately, these
pathological cases make up a small enough portion
of the data set that they can be safely disregarded.
More promisingly, 66.5% of incorrect spans can
be corrected with the addition or removal of a single
constituent; in other words, the recall of span iden-
tification can be improved from 73.6% to 91.2% by
adding or removing at most one constituent at the
end of the parser?s identified span.
4 Approach
To improve translation of matrix subjects, we im-
plement fuzzy reordering by using a lattice-based
approach similar to Bisazza and Federico (2010) to
correct the matrix subject spans identified by a state-
of-the-art dependency parser (Marton et al, 2010).
Specifically, we take a twofold approach to fuzzy
reordering. First, we present the translation system
with both un-reordered and reordered options. This
is motivated by the observation that on gold parses,
optional reordering outperformed forced reordering
(Section 3.1). Second, we apply a fuzzification algo-
rithm to the reordered subject span, adding yet more
options to the lattice. This is motivated by the ob-
servation that the greatest source of parsing errors
in subjects is span errors (Section 3.2). We discuss
these two techniques in turn.
4.1 Optional Reordering
In keeping with results from the initial gold experi-
ments, we decided to generate a lattice identical to
that used for the optional-reordering experiment, in
which the translation system was presented with the
input sentence both un-reordered and reordered, us-
ing a predicted parse to perform the reordering.
4.2 Subject Span Fuzzification
The observation that 91.2% of spans can be recalled
with single-constituent modifications led very natu-
rally to the following fuzzification algorithm, which
is illustrated in Fig. 2:
1. For each matrix subject in the parse tree4, cre-
ate an empty list to hold fuzzified boundaries.
2. Original span: Add to the list the tuple (l, r, v),
where l is the index of the predicted span?s left-
most descendant, r is the index of the predicted
span?s rightmost descendant and v is the verb
4Allowance must be made for parsers which incorrectly
identify multiple subjects for the matrix verb.
231
that the predicted span attaches to. (This step
produces the span labeled ?original? in Fig. 2.)
3. Expansion: Add to the list all tuples of the form
(l, r+, v), where r+ is the index of the right-
most descendant of a node whose leftmost de-
scendant has index r + 1. (This step produces
the spans labeled ?a1? and ?a2? in Fig. 2.)
4. Contraction: Add to the list all tuples of the
form (l, r??1, v), where r? is the index of the
leftmost descendant of a node whose rightmost
descendant has index r. (This step produces the
spans labeled ?r1? and ?r2? in Fig. 2.)
5. Create the list of all valid combinations of
spans by taking the Cartesian product of all
the per-subject span lists, and rejecting all en-
tries in which two spans overlap. (This step ac-
counts for multiple subject cases.)
The result of this algorithm is a list of lists of tuples,
where each tuple defines a single reordering, and
each list of tuples defines a set of spans that must be
moved to the left of the matrix verb for one reorder-
ing. These re-orderings are then joined together to
form the final lattice. If a single-constituent correc-
tion to the span exists (except in the aforementioned
pathological and left-attachment cases), it is guaran-
teed to appear as one path through the lattice.
5 Evaluation
5.1 Experimental Setup
We used the open-source Moses PSMT toolkit
(Koehn et al, 2007). Training data was a newswire
(MSA-English) parallel text with 12M words on the
Arabic side (LDC2007E103)5 Sentences were re-
ordered only for alignment, following the approach
of Carpuat et al (2010). Parses were obtained using
a publicly available parser for Arabic (Marton et al,
2010). GIZA++ was used for word alignment (Och
and Ney, 2003) and phrase translations of up to 10
words are extracted in the Moses phrase table. The
same baseline phrase table was used in all experi-
ments.
The system?s language model was trained both on
the English portion of the training corpus and En-
glish Gigaword (Graff and Cieri, 2003). We used a
5All data is available from the Linguistic Data Consortium:
http://www.ldc.upenn.edu.
5-gram language model with modified Kneser-Ney
smoothing implemented using the SRILM toolkit
(Stolcke, 2002). Feature weights were tuned with
MERT (Och, 2003) to maximize BLEU on the NIST
MT06 corpus. MERT was done only for the baseline
system; these same weights were used for all exper-
iments to control for the effect of MERT instability.
In the future, we plan to experiment with approach-
specific optimization and to use recent published
suggestions on controlling for optimizer instability
(Clark et al, 2011).
English data was tokenized using simple
punctuation-based rules. Arabic data was seg-
mented with to the Arabic Treebank tokeniza-
tion scheme (Maamouri et al, 2004) using the
MADA+TOKAN morphological disambiguator and
tokenizer (Habash and Rambow, 2005; Habash,
2007a; Roth et al, 2008). The Arabic text was
also Alif/Ya normalized (Habash, 2010). MADA-
produced Arabic lemmas were used for word
alignment.
We compare four settings with predicted parses
(as opposed to the gold parse experiments discussed
in Section 3):
? BASE An un-reordered test set;
? FORCE A test set which forced reordering on
matrix verbs;
? OPT A test set with fuzzification through op-
tional reordering on matrix verbs; and
? SPAN A test set with fuzzification through op-
tional reordering on matrix verbs and through
fuzzification of the subject span according to
the algorithm shown in Section 4.2.
Each reordering corpus used Moses? lattice input
format (Dyer et al, 2008) (including the baselines,
which had only one path). Results are presented in
terms of the standard BLEU metric (Papineni et al,
2002), METEOR metric (Banerjee and Lavie, 2005)
and a manual evaluation targeting subject span trans-
lation correctness.
5.2 Automatic Evaluation Results
Table 2 presents the results for the experiments dis-
cussed above. Columns three and Four (Prec-1g
and Prec-4g) indicate the corresponding 1-gram and
4-gram (sub-BLEU) precision scores, respectively.
232
System BLEU Prec-1g Prec-4g METEOR
BASE 47.13 81.91 29.52 53.09
FORCE 47.03 81.78 29.52 53.11
OPT 47.42 81.88 30.04 53.22
SPAN 47.41 81.92 30.03 53.21
Table 2: Automatic evaluation results
Both OPT and SPAN showed a statistically signif-
icant improvement in BLEU score over BASE and
FORCE above the 95% level. Statistical signifi-
cance is computed using paired bootstrap resam-
pling (Koehn, 2004). The difference between OPT
and SPAN, however, was not statistically significant.
The relatively small difference in BLEU score be-
tween the baseline and gold reordering (Section 3:
baseline 47.13 and optional reordering 47.55) sug-
gests that we should expect at most a modest in-
crease in BLEU from improving the predicted trees.
The first key observation in these results is that
with a noisy parser, translation quality actually goes
down with forced reordering?the opposite of what
was observed in the gold experiment. By introduc-
ing either optional reordering or complete fuzzifi-
cation, however, BLEU score increases .3 past the
baseline to achieve nearly three quarters of the gain
obtained by optional reordering using the gold parse
(Section 3: baseline 47.13 and optional reordering
47.55). In other words, it is possible to compensate
for the parser noisiness without actually attempting
to correct spans: simply allowing the translation sys-
tem to fall back on an un-reordered input leads to a
significant gain in BLEU.
One possible explanation for this fact is that we
only ever correct for parses on the right-hand side?
the left sides are virtually always correct. Thus,
when we perform any reordering, even if the subject
span is not entirely perfect, we guarantee that we
bring at least one word from the sentence (and usu-
ally more) into alignment where it was out of align-
ment before; this obviously leads to better BLEU
n-gram scores along that boundary.
The general trend in these results is confirmed by
the results of a METEOR analysis, also provided in
Tab. 2. Again, both the OPT and SPAN systems
result exhibit comparable performance, and demon-
strate an improvement over the baseline.
The second observation is that introducing span
fuzzification did not improve over simple optional
reordering. There are a several reasons this could be
happening:
? The increased fluency and introduction of un-
seen phrases cancel each other out.
? All the gains that come from reordering occur
at the left; the presence or absence of correct
words at the right end is less important.
? Better sentences are proposed during the trans-
lation process, but they are not selected during
the final filtering stage.
? The sentences being output are actually better,
but the improvement is not captured by the au-
tomatic evaluation.
Further experiments will be necessary to determine
whether any of the first three possibilities is the case.
We next consider the fourth possibility in more de-
tail.
5.3 Manual Evaluation
We additionally conducted a manual evaluation to
examine how subject quality differed in fuzzified vs.
unfuzzified parses. Each sentence examined was as-
signed one of the six labels below. Examples are
with respect to the reference sentence ?Recep Tayyip
Erdogan announced that Turkey is strong.?
? MM: both verb and subject missing. ?Turkey
is strong.?
? MV: verb missing. ?Recep Tayyip Erdogan
Turkey is strong.?
? MS: subject missing. ?announced that Turkey
is strong.?
? SO: subject overlaps with verb. ?Recep an-
nounced Tayyip Erdogan Turkey is strong.?
? SI: verb precedes subject (as in Arabic). ?an-
nounced Recep Tayyip Erdogan that Turkey is
strong.?
? C: verb follows subject (as in English), i.e. the
correct ordering. ?Recep Tayyip Erdogan an-
nounced that Turkey is strong.? We also include
in this category sentences where the English
reference contains no verb (e.g. in newspaper
headlines).
233
System MM MS MV SI SO C M* S* C
BASE 8 13 11 9 3 53 33 12 53
OPT 7 11 10 5 5 61 28 10 61
SPAN 8 10 09 5 2 64 27 7 64
Table 3: Subject integrity analysis results. All numbers are %s.
By grouping some of these categories together, we
obtained the following label scheme:
? M*: MM, MV or MS, i.e. verb or subject is
missing.
? S*: SO or SI, i.e. word order is incorrect.
? C: as above.
280 sentences selected randomly from our test set
were evaluated, generating 461 unique output sen-
tences. Annotation was performed by two English
speakers, with 40 input sentences (68 unique out-
puts) annotated by both authors to collect agreement
statistics. For the complete label scheme, the an-
notators agreed on 86.8% of labels, with Cohen?s
? = 0.811. For the simple label scheme, the an-
notators agreed on 92.6% of labels, with ? = .883.
Results for the BASE, OPT and SPAN systems are
shown in Table 3. Each annotator?s labels were as-
signed a weight of .5 in the section that was jointly
annotated.
Again, both the OPT and SPAN systems display
statistically significant improvements over the base-
line system (p < 0.001). While the SPAN system
consistently displays better results than the OPT sys-
tem, the significance is low (p < .3). Statistical sig-
nificance was measured using the McNemar test of
statistical significance (McNemar, 1947).
These results thus agree with the BLEU score in
indicating that the OPT and SPAN systems are sub-
stantially better than the baseline, but statistically in-
distinguishable from each other. They further in-
dicate that most of the improvements in the OPT
system come from preventing dropped subjects or
verbs, while the improvements in the SPAN system
result in roughly equal proportion from preventing
word-dropping and ensuring correct ordering.
6 Conclusion & Future Work
We presented an approach for improving Arabic-
English PSMT using syntactic information from a
noisy parser. We demonstrated that translation qual-
ity goes down with forced reordering, but improves
with the introduction of either optional reordering
and subject span fuzzification. The BLEU score in-
creases by 0.3% absolute past the baseline achieve
nearly three quarters of the maximum possible gain
starting with gold parses. A detailed manual eval-
uation produces results generally consistent with
BLEU, but highlights the small improvements that
can be gained by subject span fuzzification.
In the future, we plan to explore a more sophis-
ticated approach to the lattice of re-orderings pre-
sented here. We would take into account the fact that
it is possible to suggest to the system that certain
re-orderings are less likely than others without re-
moving them from the search space completely. The
same can be done for the fuzzification task: while
we might wish to add additional fuzzification op-
tions, we also don?t want the correct choice to be
crowded out by too many alternatives.
Acknowledgements
We would like to thank Marine Carpuat, Yuval Mar-
ton, Amit Abbi and Ahmed El Kholy for helpful dis-
cussions and feedback. The second and third authors
were supported by the Defense Advanced Research
Projects Agency (DARPA) under GALE Contract
No HR0011-08-C-0110. Any opinions, findings and
conclusions or recommendations expressed in this
material are those of the authors and do not neces-
sarily reflect the views of DARPA.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Arbor,
Michigan.
234
Arianna Bisazza and Marcello Federico. 2010. Chunk-
based verb reordering in VSO sentences for Arabic-
English statistical machine translation. In Proceedings
of ACL 2010: Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR, Uppsala, Swe-
den.
Marine Carpuat, Yuval Marton, and Nizar Habash.
2010. Improving Arabic-to-English Statistical Ma-
chine Translation by Reordering Post-Verbal Subjects
for Alignment. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 178?183, Uppsala, Swe-
den, July.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statisti-
cal machine translation: Controlling for optimizer in-
stability. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 176?181, Port-
land, Oregon, USA.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 531?540, Ann Arbor, Michigan.
Chris Dyer and Philip Resnik. 2010. Context-free re-
ordering, finite-state translation. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 858?866, Los Angeles,
California.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-08: HLT, Columbus, Ohio.
Jakob Elming and Nizar Habash. 2009. Syntactic Re-
ordering for English-Arabic Phrase-Based Machine
Translation. In Proceedings of the EACL 2009 Work-
shop on Computational Approaches to Semitic Lan-
guages, pages 69?77, Athens, Greece, March.
J. Elming. 2008. Syntactic reordering integrated with
phrase-based smt. In Proceedings of the ACL Work-
shop on Syntax and Structure in Statistical Translation
(SSST-2).
David Graff and Christopher Cieri. 2003. English Gi-
gaword, LDC Catalog No.: LDC2003T05. Linguistic
Data Consortium, University of Pennsylvania.
Spence Green, Conal Sathi, and Christopher D. Man-
ning. 2009. NP Subject Detection in Verb-initial Ara-
bic Clauses. In Proceedings of the Third Workshop
on Computational Approaches to Arabic Script-based
Languages (CAASL3).
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphological
Disambiguation in One Fell Swoop. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 573?580, Ann
Arbor, Michigan.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
221?224, Suntec, Singapore.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den Bosch
and A. Soudi, editors, Arabic Computational Mor-
phology: Knowledge-based and Empirical Methods.
Springer.
Nizar Habash. 2007a. Arabic Morphological Repre-
sentations for Machine Translation. In A. van den
Bosch and A. Soudi, editors, Arabic Computational
Morphology: Knowledge-based and Empirical Meth-
ods. Springer.
Nizar Habash. 2007b. Syntactic preprocessing for statis-
tical machine translation. In Proceedings of the 11th
MT Summit.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-
pher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Christopher Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public.
Philipp Koehn. 2004. Statistical significance tests for-
machine translation evaluation. In Proceedings of the
Empirical Methods in Natural Language Processing
Conference (EMNLP?04), Barcelona, Spain.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109, Cairo, Egypt.
Yuval Marton, Nizar Habash, and Owen Rambow. 2010.
Improving Arabic Dependency Parsing with Lexical
and Inflectional Morphological Features. In Proceed-
ings of the NAACL HLT 2010 First Workshop on Sta-
tistical Parsing of Morphologically-Rich Languages,
pages 13?21, Los Angeles, CA, USA, June.
Quinn McNemar. 1947. Note on the sampling error of
the difference between correlated proportions or per-
centages. Psychometrika, 12(2):153?157.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
235
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proceedings
of the 41st Annual Conference of the Association for
Computational Linguistics, pages 160?167, Sapporo,
Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA.
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,
and Cynthia Rudin. 2008. Arabic Morphological Tag-
ging, Diacritization, and Lemmatization Using Lex-
eme Models and Feature Ranking. In Proceedings of
ACL-08: HLT, Short Papers, pages 117?120, Colum-
bus, Ohio.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing
(ICSLP), volume 2, pages 901?904, Denver, CO.
Fei Xia and Michael McCord. 2004. Improving a statis-
tical mt system with automatically learned rewrite pat-
terns. In Proceedings of the 20th International Confer-
ence on Computational Linguistics (COLING 2004),
pages 508?514, Geneva, Switzerland.
236
