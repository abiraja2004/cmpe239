Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 72?76,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Multilingual summarization system based on analyzing the discourse 
structure at MultiLing 2013 
 
 
Daniel Alexandru Anechitei 
?Al. I. Cuza? University of Iasi, 
 Faculty of Computer Science 
16,General Berthelot St., 700483, Iasi, 
Romania 
daniel.anechitei@info.uaic.ro 
Eugen Ignat 
?Al. I. Cuza? University of Iasi, 
 Faculty of Computer Science 
16,General Berthelot St., 700483, Iasi, 
Romania 
eugen.ignat@info.uaic.ro 
 
  
 
Abstract 
This paper describes the architecture of 
UAIC1?s Summarization system participating 
at MultiLing ? 2013. The architecture includes 
language independent text processing mod-
ules, but also modules that are adapted for one 
language or another. In our experiments, the 
languages under consideration are Bulgarian, 
German, Greek, English, and Romanian. Our 
method exploits the cohesion and coherence 
properties of texts to build discourse struc-
tures. The output of the parsing process is used 
to extract general summaries. 
1 Introduction 
Automatic text summarization is a well studied 
research area and has been active for many years. 
In this paper, we describe the automatic text 
summarization system implemented by UAIC for 
participation at MultiLing 2013 single document 
track. Our approach to summarization follows 
the one presented in (Anechitei et al, 2013). The 
summarization architecture that this system uses 
includes two main parts that can be viewed in 
Figure 1. The text is passed to the language pro-
cessing chain (LPC) which processes the data. 
As revealed from the figure each language has its 
own LPC. The LPC?s, acts as a prerequisite for 
the summarization meta tool (SMT). In this pa-
per we will focus more on the SMT engine, 
which is composed of four modules: anaphora 
resolution (AR), clause splitter (CS), discourse 
parser (DP) and the proper summarizer (SUM). 
The intermediate format between the modules 
consists of XML files. The summary of a text is 
                                                 
1 University ?Al. I. Cuza? of Iasi, Romania 
obtained as a sequence of discourse clauses ex-
tracted from the original text, after obtaining the 
discourse structure of the text and exploiting the 
cohesion and coherence properties. 
 
Figure 1: Summarization system architecture 
 
2 Language Processing Chains 
Every document is analyzed by the LPC in the 
following consecutive steps: sentence splitter, 
tokenizer, Part of Speech tagger, lemmatizer, 
Noun phrase extractor and Named entity recog-
nizer. All tools are self-contained and designed 
72
to work in a chain, i.e. the output of the previous 
component is the input for the next component. 
3 Anaphora Resolution  
Anaphora resolution is one of the key steps of 
the discourse parser, by resolving anaphoric pro-
nouns, automatically generated summaries may 
be more cohesive and, thus, more coherent. Cal-
culating scores for references and transitions 
would be impossible without the proper identifi-
cation of the co-referential chains. 
Anaphora resolution is defined in (Or?san et. 
al, 2008) as the process of resolving an anaphoric 
expression to the expression it refers to. The tool 
used for the anaphora resolution named RARE 
(Robust Anaphora Resolution Engine) uses the 
work done in (Cristea and Dima, 2001), where 
the process implies three layers (Figure 2): 
? The text layer, containing referential ex-
pressions(RE) as they appear in the dis-
course; 
? An intermediate layer (projection layer) 
that contains any specific information 
that can be extracted from the corre-
sponding referential expressions. 
? A semantic layer that contains descrip-
tions of the discourse entities (DE). Here 
the information contributed by chains of 
referential expressions is accumulated. 
 
 
 
Figure 2: Three layers representation of co-
referencing REs (Cristea and Dima, 2001) 
 
The core of the system is language independ-
ent, but in order to localize it to one language or 
another it requires specific resources. These spe-
cific resources are as follows: 
? constraints ? containing the rules that 
match the conditions between anaphor 
and antecedent; 
? stopwords ? containing a list of 
stopwords; 
? tagset ? implies a mapping from the 
tagset used in the input file to a more 
simplified tagset used by the system. 
? window ? here is defined the length of 
the window where the antecedent should 
be looked for by the system. 
 
The process of anaphora resolution runs as fol-
lows: The text is ?read? from the left to right. 
When a new NP is found, a new RE is created 
and contains the morphological, syntactic and 
semantic features. All the features are tested us-
ing the constraints and it is decided whether the 
RE introduces a new discourse entity, not men-
tioned before, or it revokes one already men-
tioned.  
4 Clause Splitter 
Numerous techniques are used to recognize 
clause boundaries for different languages, where 
some are rule based (Leffa, 1988), and others are 
hybrid methods, like in (Parven et al, 2011) and 
(Or?san, 2000), where the results of a machine 
learning algorithm, trained on an annotated cor-
pus, are processed by a shallow rule-based mod-
ule in order to improve the accuracy of the meth-
od. Our approach to discourse segmentation 
starts from the assumption that a clause is headed 
by a main verb, like ?go? or a verbal compound, 
like ?like to swim? (Ex.1). Verbs and verb com-
pounds are considered pivots and clause bounda-
ries are looked for in-between them.  
 
Ex. 1 <When I go to river>< I like to swim with my 
friends.> 
 
Verb compounds are sequences of more than 
one verb in which one is the main verb and the 
others are auxiliaries, infinitives, conjunctives 
that complement the main verb and the semantics 
of the main verb in context obliges to take the 
whole construction together. The CS module 
segments the input by applying a machine learn-
ing algorithm, to classify pairs of verbs as being 
or not compound verbs and, after that, applying 
rules and heuristics based on pattern matching or 
machine learning algorithms to identify the 
clause boundary. The exact place of a clause 
boundary between verbal phrases is best indicat-
ed by discourse markers. A discourse marker, 
like ?because? (Ex.1), or, simply, marker, is a 
word or a group of words having the function to 
signal a clause boundary and/or to signal a rhe-
torical relation between two text spans. 
  
Ex. 1 <Markers are good><because they can give 
information on boundaries and discourse structure.> 
 
73
When markers are missing, boundaries are found 
by statistical methods, which are trained on ex-
plicit annotations given in manually built files. 
Based on the manually annotated files, a training 
module extracts two models (one for the CS 
module and one for the DP module). These mod-
els incorporate patterns of use of markers used to 
decide the segmentation boundaries and also to 
identify rhetorical relations between spans of 
text. The clauses act as terminal nodes in the 
process of discourse parsing which is described 
below. 
5 Discourse Parser  
Discourse parsing is the process of building a 
hierarchical model of a discourse from its basic 
elements (sentences or clauses), as one would 
build a parse of a sentence from its words (Ban-
galore and Stent, 2009). Rhetorical Structure 
Theory (Mann and Thompson, 1988) is one of 
the most popular discourse theories. In RST a 
text segment assumes one of two roles in a rela-
tionship: the nucleus (N) or satellite (S). Nuclei 
express what is more essential to the understand-
ing of the narrative than the satellites. Our Dis-
course Parser uses a symbolic approach and pro-
duces discourse trees, which include nuclearity, 
but lacking rhetorical relation names: intermedi-
ate nodes in the discourse tree have no name and 
terminal nodes are elementary discourse units, 
mainly clauses. It adopts an incremental policy in 
developing the trees, on three levels (paragraphs, 
sentences and clauses) by consuming, recursive-
ly, one entire structure of an inferior level, by 
attaching the elementary discourse tree (edt) of 
the last structure to the already developed tree on 
the right frontier (Cristea and Webber, 1997). 
First, an edt of each sentence is produced using 
incremental parsing, by consuming each clause 
within the sentence. Secondly, the edt of the par-
agraph is produced by consuming each sentence 
within the paragraph. The same approach is used 
at discourse level by attaching the paragraph tree 
of each paragraph to the already developed tree. 
The criterion to guide the discourse parsing is 
represented by the principle of sequentiality 
(Marcu, 2000). The incremental discourse pars-
ing approach borrows the two operations used in 
(L)TAG (lexicalized tree-adjoining grammar) 
(Joshi and Schabes, 1997): adjunction and sub-
stitution. 
Adjunction operation (Figure 3) occurs only 
on the right frontier and it takes an initial or de-
veloping tree (D-treei-1), creating a new develop-
ing tree (D-treei) by combining D-treei-1 with an 
auxiliary tree (A-tree), by replacing the foot node 
with the cropped tree. This is done for each node 
on the right frontier resulting in multiple D-trees. 
Figure 3 depicts this idea. 
 
Figure 3: Adjunction operation 
 
Substitution operation (Figure 4) replaces a 
placed node on a terminal frontier, called substi-
tution node, with an auxiliary tree (Figure 14).  
 
 
Figure 4: Substitution operation 
 
The uses of different types of auxiliary trees 
(Figure 5) are determined by two factors: 
? the type of operation in which are used: 
alpha and beta are used only for adjunc-
tion operations and gamma and delta for 
substitution operations; 
? the auxiliary tree introduces or not an 
expectation: beta and gamma are auxilia-
ry trees that raise an expectation and al-
pha an delta are auxiliary trees which do 
not raise an expectation. 
 
 
Figure 5: Types of auxiliary trees 
74
At each parsing step there is a module which 
decides the type of the auxiliary tree between 
alpha, gamma, beta, delta (Anechitei et al, 
2013) together with the relations type (R1 and R2, 
which can be N_N, N_S or S_N; the notation 
express the nuclearity of the child nodes: left one 
and the right one) by analyzing the structure 
which is processed (clause, sentence or para-
graph). This module uses the compiled model 
described in previous section and doesn?t pro-
duce a unique auxiliary tree for each structure 
but rather a set of trees. 
At each level, the parser goes on with a forest 
of developing trees in parallel, ranking them by a 
global score (Figure 6) based on heuristics that 
are suggested by both Veins Theory (Cristea et 
al., 1998) and Centering Theory (Grosz et al, 
1995).  After normalizing the score for each heu-
ristic, the global score is computed by summing 
the score of one heuristic with the corresponding 
weight. The weights were established after a cal-
ibration process.  
 
Figure 6: Global score for each discourse tree 
 
The trees used at the next step are only the 
best ranked trees. The aim of this filtering step is 
to reduce the exponential explosion of the ob-
tained trees. For this task the threshold was set to 
five best trees from iteration to another and six 
(N=6) heuristics chosen in a way to maximize 
the coherence of the discourse structure and im-
plicitly the coherence of the summary. 
6 The Summarizer 
The mentioned system produces excerpt type 
summaries, which are summaries that copy con-
tiguous sequences of tokens from the original 
text. 
The structure of a discourse as a complete tree 
gives more information than properly needed (at 
least for summarization purpose). By exploiting 
the discourse structure, we expect to add cohe-
sion and coherence to our summaries. From the 
discourse structure we can extract three types of 
summaries: general summaries, entity focused 
summaries and clause focused summaries. For 
the summarization task we only extracted the 
general summary. The module that extracts the 
summaries (SUM) takes the tree of a discourse 
structure and produces a general summary, of a 
certain length, depending on the length of the 
computed vein (Cristea et al, 1998). As the task 
supposed summaries containing a maximum of 
250 words and the summaries the system was 
providing were always bigger, a new scoring sys-
tem was needed. This scoring system needed to 
shorten the summaries to under 250 words, yet 
keep as much coherence and cohesion as the sys-
tem provided. For this end the scoring system 
took all the clauses from the vein and scored 
them as follows: in each clause the noun phrases 
were found, for each noun phrase a coreferential 
score was given. These scores are added and 
computed for each clause. The clauses were sort-
ed and only the first N clauses were selected 
such as the maximum coherence was retained, 
where N is the number of the clauses so that the 
final summaries are below the word count 
threshold. The score for each noun phrase is giv-
en taking into account how big the coreference 
chain is. 
7 Conclusion and Results 
This year, the evaluation at MultiLing 2013 
was performed automatically using N-gram 
graph methods, which were interchangeable in 
the single document setting. Below we provide 
the results based on average NPowER  grades. 
 
Lang UAIC Mary-
land (I) 
Mary-
land (II) 
Mary-
land (II) 
Baseline 
BG 1.538 1.600 1.593 1.600 1.310 
DE 1.537 1.64 1.612 1.617 1.289 
EL 1.560 1.501 1.513 1.494 1.314 
EN 1.646 1.641 1.661 1.656 1.367 
RO 1.627 1.655 1.679 1.680 1.346 
 1.582 1.607 1.611 1.609 1.325 
Table 1: Table with results  
 
Table 1 shows the comparison between 
UAIC?s system and Maryland?s system, as it was 
the only other system, besides the baseline, that 
ran on the same 5 languages. Generally the re-
sults of both systems are close as the average 
figure shows. For our first participation the re-
sults are encouraging for this complex system, 
which has the possibility of running on multiple 
languages. Our future work should reside in the 
scorer of the summarizer, as the approach usually 
creates summaries bigger than 250 words. 
 
75
References  
Anechitei A. Daniel, Cristea Dan, Dimosthenis Ioan-
nidis, Ignat Eugen, Karagiozov Diman, Koeva 
Svetla, Kope? Mateusz and Vertan Cristina. 2013. 
Summarizing Short Texts Through a Discourse-
Centered Approach in a Multilingual Context. In 
Neustein, A., Markowitz, J.A. (eds.), Where Hu-
mans Meet Machines: Innovative Solutions to 
Knotty Natural Language Problems. Springer 
Verlag, Heidelberg/New York. 
Bangalore Srinivas and Stent J. Amanda. 2009. In-
cremental parsing models for dialog task structure, 
in Proceedings of the 12th Conference of the Euro-
pean Chapter of the Association for Computational 
Linguistics. 
Cristea Dan and Webber Bonnie. 1997. Expectations 
in incremental discourse processing. In Proceed-
ings of the 8th Conference of the European Chapter 
of the Association for Computational Linguistics. 
Cristea Dan, Ide Nancy and Romary Laurent. 1998: 
Veins Theory: A Model of Global Discourse Cohe-
sion and Coherence, in Proceedings of the 17th in-
ternational conference on Computational linguis-
tics. 
Cristea Dan and Dima E. Gabriela. 2001. An integrat-
ing framework for anaphora resolution. In Infor-
mation Science and Technology, Romanian Acad-
emy Publishing House, Bucharest, vol. 4, no. 3-4, p 
273-291. 
Grosz J. Barbara, Joshi K. Arvind and Weinstein 
Scott. 1995. Centering: A Framework for Model-
ling the Local Coherence of Discourse. Computa-
tional Linguistics, 21/2: 203-25. 
Joshi K. Aravind and Schabes Yves. 1997: Tree-
Adjoining Grammars. In G. Rozenberg and 
A.Salomaa, editors, Handbook of Formal lan-
guages. 
Leffa J. Vilson. 1988. Clause processing in complex 
sentences. In Proceedings of the First International 
Conference on Language Resource and Evaluation, 
volume 1, pages 937 ? 943, May 1998. 
Mann C. William and Thompson A. Sandra. 1988. 
Rhetorical structure theory: a theory of text organ-
ization. Text 8(3):243?281. 
Marcu Daniel. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. The MIT 
Press, November 2000. 
Orasan Constantin. 2000. A hybrid method for clause 
splitting in unrestricted English texts. In Proceed-
ings of ACIDCA, Corpora and Natural Language 
Processing, March 22-24, Monastir, Tunisia, pp. 
129 ? 134. 
Or?san Constantin, Cristea Dan, Mitkov Ruslan and 
Branco Antonio. 2008. Anaphora Resolution Exer-
cise ? An Overview. In Proceedings of LREC-2008, 
Marrakech, Morocco. 
Parveen Daraksha, Sanyal Ratna and Ansari Afreen. 
2011. Clause Boundary Identification using Classi-
fier and Clause Markers in Urdu Language. 
Polibits Research Journal on Computer Science, 
43, pp. 61-65. 
76
