Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 709?712,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Arabic Mention Detection: Toward Better Unit of Analysis
Yassine Benajiba
Center for Computational Learning Systems
Columbia University
ybenajiba@ccls.columbia.edu
Imed Zitouni
IBM T. J. Watson Research Center
izitouni@us.ibm.com
Abstract
We investigate in this paper the adequate unit
of analysis for Arabic Mention Detection. We
experiment different segmentation schemes
with various feature-sets. Results show that
when limited resources are available, models
built on morphologically segmented data out-
perform other models by up to 4F points. On
the other hand, when more resources extracted
from morphologically segmented data become
available, models built with Arabic TreeBank
style segmentation yield to better results. We
also show additional improvement by combin-
ing different segmentation schemes.
1 Introduction
This paper addresses an important and basic task of
information extraction: Mention Detection (MD)1:
the identification and classification of textual refer-
ences to objects/abstractions (i.e., mentions). These
mentions can be either named (e.g. Mohammed,
John), nominal (city, president) or pronominal (e.g.
he, she). For instance, in the sentence ?President
Obama said he will visit ...? there are three men-
tions: President, Obama and he. This is similar
to the Named Entity Recognition (NER) task with
the additional twist of also identifying nominal and
pronominal mentions. We formulate the mention de-
tection problem as a classification problem, by as-
signing to each token in the text a label, indicating
whether it starts a specific mention, is inside a spe-
cific mention, or is outside all mentions. The se-
lection of the unit of analysis is an important step
toward a better classification. When processing lan-
guages, such as English, using the word itself as the
1We adopt here the ACE nomenclature:
http://www.nist.gov/speech/tests/ace/index.html
unit of analysis (after separating punctuations) leads
to a good performance (Florian et al, 2004). For
other languages, such as Chinese, character is con-
sidered as the adequate unit of analysis (Jing et al,
2003). In this paper, we investigate different seg-
mentation schemes in order to define the best unit of
analysis for Arabic MD. Arabic adopts a very com-
plex morphology, i.e. each word is composed of zero
or more prefixes, one stem and zero or more suffixes.
Consequently, the Arabic data is sparser than other
languages, such as English, and it is necessary to
?segment? the words into several units of analysis in
order to achieve a good performance.
(Zitouni et al, 2005) used Arabic morphologically
segmented data and claimed to have very competi-
tive results in ACE 2003 and ACE 2004 data. On the
other hand, (Benajiba et al, 2008) report good re-
sults for Arabic NER on ACE 2003, 2004 and 2005
data using Arabic TreeBank (ATB) segmentation. In
all published works, authors do not mention a spe-
cific motivation for the segmentation scheme they
have adopted. Only for the Machine Translation
task, (Habash and Sadat, 2006) report several results
using different Arabic segmentation schemes. They
report that the best results were obtained when the
ATB-like segmentation was used. We explore here
the four known and linguistically-motivated sorts of
segmentation: punctuation separation, ATB, mor-
phological and character-level segmentations. To
our knowledge, this is the first paper which inves-
tigates different segmentation schemes to define the
unit of analysis which best fits Arabic MD.
2 Arabic Segmentation Schemes
Character-level Segmentation: considers that each
character is a separate token.
Morphological Segmentation : aims at segmenting
709
all affixes of a word. The morphological segmenta-
tion for the word I.

J??? @? (wAlmktb ? and the of-
fice)2 could be: ?I.

J??+ ?@+ ?? (w +Al +mktb).
Arabic TreeBank (ATB) segmentation : This seg-
mentation considers splitting the word into affixes
only if it projects an independent phrasal constituent
in the parse tree. As an example, in the word shown
above I.

J??? @?, the phrasal independent constituents
are: the conjunction ? (w ? and) and the noun
I.

J??? @ (Almktb ? the office). The morphological
segmentation of this word would lead to the follow-
ing parse tree:
S
HH
CONJ
w
NP
b
b
"
"
Al +mktb
Since the ?@ (Al, the definite article) is not an in-
dependent constituent, it is not considered for ATB
segmentation. Hence, for I.

J??? @?, the ATB segmen-
tation would be I.

J??? @+ ? (w +Almktb).
Punctuation separation : it consists of separating
the punctuation marks from the word.
Both ATB and morphological segmentation sys-
tems are based on weighted finite state transducers
(WFST). The decoder implements a general Bell-
man dynamic programming search for the best path
on a lattice of segmentation hypotheses that match
the input characters (Benajiba and Zitouni, 2009).
ATB and morphological segmentation systems have
a performance of 99.4 and 98.1 F-measure respec-
tively on ATB data.
The unit of analysis when doing classification de-
pends on the used segmentation. When using the
punctuation separation or character-based segmen-
tations, the unit of analysis is the word itself (with-
out the punctuation marks attached) or the character,
respectively. The ATB and morphological segmen-
tations are language specific and are based on dif-
ferent linguistic viewpoint. When using one of these
two segmentation schemes, the unit of analysis is the
morph (i.e. prefix, stem or suffix). Our goal in this
paper is to find the unit of analysis that fits best Ara-
bic MD.
2Throughout the paper, for each Arabic example we show
between parenthesis its transliteration and English translation
separated by ???.
3 Mention Detection System
As explained earlier, we consider the MD task as a
sequence classification problem where the class we
predict for each unit of analysis (i.e., token) is the
type of the entity which it refers to. We chose the
maximum entropy (MaxEnt) classifier that can in-
tegrate arbitrary types of information and make a
classification decision by aggregating all informa-
tion available for a given classification. For more
details about the system architecture, reader may re-
fer to (Zitouni et al, 2009). The features used in our
MD system can be divided into four categories:
Lexical Features: n-grams spanning the current to-
ken; both preceding and following it. A number of
n equal to 3 turned out to be a good choice.
Stem n-gram Features: stem trigram spanning the
current stem; both preceding and following it (Zi-
touni et al, 2005).
Syntactic Features: POS tags and shallow parsing
information in a ?2 window.
Features From Other Classifiers: outputs of MD
and NER taggers trained on other data-sets different
from the one we used here. They may identify types
of mentions different from the mentions of interest
in our task. For instance, such a tagger may identify
dates or occupation references (not used in our task),
among other types. Our hypothesis is that combin-
ing classifiers from diverse sources will boost per-
formance by injecting complementary information
into the mention detection models. We also use the
two previously assigned classification tags as addi-
tional feature.
4 Data
Experiments are conducted on the Arabic ACE 2007
data. Since the evaluation tests set are not publicly
available, we have split the publicly available train-
ing corpus into an 85%/15% data split. We use 323
documents (80, 000 words, 17, 634 mentions) for
training and 56 documents (18, 000 words, 3, 566
mentions) as a test set. We are interested in 7 types
of mentions: facility, Geo-Political Entity (GPE),
location, organization, person, vehicle and weapon.
We segmented the training and test set with four dif-
ferent styles building the following corpora:
Words: a corpus which is the result of running
punctuation separation;
ATBs: a corpus obtained by running punctuation
separation and ATB segmentation;
Mophs: a corpus where we conduct punctuation
separation and morphological segmentation;
Chars: a corpus where the original text is separated
710
into a sequence of characters.
When building MD systems on Words, ATBs,
Morphs and Chars, the unit of analysis is the word,
the ATB token, the morph and the character, respec-
tively.
5 Experiments
We show in this section the experimental results
when using Arabic MD system with different seg-
mentation schemes and different feature sets. We
explore in this paper four categories of features (c.f.
Section 3):
Lexf : lexical features;
Stemf : Lexf + morphological features;
Syntf : Stemf + syntactic features;
Semf : Syntf + output of other MD classifiers.
Lexf and Stemf features are directly extracted
from the appropriate corpus based on the used seg-
mentation style. This is different for Semf : we first
run classifiers on the morphologically segmented
data. Thereafter, we project those labels to other
corpora. This is because, we use classifiers initially
trained on morphologically segmented data such as
ACE 2003, 2004 and 2005 data. In such data, two
morphs belonging to the same word or ATB token
may have 2 different mentions. During transfer, a
token will have the label of the corresponding stem
in the morphologically segmented data. One moti-
vation to not re-train classifiers on each corpus sep-
arately is to be able to extract Semf features from
classifiers with similar performance.
Table 1: Results in terms of F-measure per feature-set and
segmentation scheme
Lexf Stemf Syntf Semf
Words 66.4 66.6 69.0 77.1
ATBs 70.1 69.8 72.1 79.0
Morphs 74.1 74.5 75.5 78.3
Chars 22.3 22.4 22.5 22.6
Results in Table 1 show that classifiers built on
ATBs and Morphs have shown to perform better
than classifiers trained on data with other segmenta-
tion styles. When the system uses character as the
unit of analysis, performance is poor. This is be-
cause the token itself becomes insignificant informa-
tion to the classifier. On the other hand, when only
punctuation separation is performed (Words), the
data is significantly sparse and the obtained results
achieves high F-measure (77.1) only when outputs
of other classifiers are used. As mentioned earlier,
classifiers used to extract those features are trained
on Morphs (less sparse), which explains their re-
markable positive impact since they resolve part of
the data sparseness problem in Words. When us-
ing full morphological segmentation, the data is less
sparse, which leads to less Out-Of-Vocabulary to-
kens (OOVs): the number of OOVs in the Morphs
data is 1,518 whereas it is 2,464 in the ATBs.
As an example, the word

?
	
JJ
?Q?@ (Alrhynp ? the
hostage), which is person mention in the training
data. This word is kept unchanged after ATB seg-
mentation and is segmented to ?

?+
	?
?P
+ ?@? (Al+
rhyn +p) in Morphs. In the development set the
same word appears in its dual form without defi-
nite article, i.e. 	?


J
	
J
?P. This word is unchanged in
ATBs and is segmented to ? 	?K
+
H+
	?
?P? (rhyn
+p +yn) in Morphs. For the model built on ATBs,
this word is an OOV, whereas for the model built
on Morphs the stem has been seen as part of a per-
son mention and consequently has a better chance
to tag it correctly. These phenomena are frequent,
which make the classifier trained on Morphs more
robust for such cases. Also, we observed that mod-
els trained on ATBs perform better on long span
mentions. We think this is because a model trained
on ATBs has access to larger context. One may
argue that a similar behavior of the model built on
the Morphs might be obtained if we use a wider
context window than the one used for ATBs in or-
der to have similar contextual information. In or-
der to confirm this statement, we have carried out a
set of experiments using all features over Morphs
data for a context window up to ?5/ + 5, the ob-
tained results show no improvement. Similar behav-
ior is observed when looking to results on identi-
fied named (Nam.), nominal (Nom.) and pronomi-
nal (Pro.) mentions on ATBs and Morphs (c.f. Ta-
ble 2); we remind the reader that NER is about rec-
ognizing named mentions. When limited resources
are available (e.g. Lexf , Stemf or Syntf ), we be-
lieve that it is more effective to morphologically seg-
ment the text (Morphs) as a pre-processing step.
The use of morph as a unit of analysis reduces the
data sparseness issue and at the same time allows
better context handling when compared to character.
On the other hand, when a larger set of resources
are available (e.g., Semf ), the use of the ATB to-
ken as a unit of analysis combined with morph-
based features leads to better performance (79.0 vs.
78.3 on Morphs). This is because (1) classifiers
trained on ATBs handle better the context and (2)
the use of morph-based features (output of classi-
711
fiers trained on morphologically segmented data) re-
moves some of the data sparseness from which clas-
sifiers trained on ATBs suffer. The obtained im-
provement in performance is statistically significant
when using the stratified bootstrap re-sampling sig-
nificance test (Noreen, 1989). We consider results
as statistically significant when p < 0.02, which is
the case in this paper. For an accurate MD system,
we think it is appropriate to benefit from ATBs to-
kens and Morphs. We investigate in the following
the combination of these two segmentation styles.
Table 2: Performance in terms of F-measure per level on
ATBs and Morphs
Seg. Lexf Stemf Syntf Semf
Nam.
ATBs 68.2 69.0 72.8 79.1
Morphs 73.4 73.8 75.3 78.7
Nom.
ATBs 65.6 64.6 66.9 75.8
Morphs 71.7 72.2 72.9 75.4
Pro.
ATBs 60.7 60.1 59.9 66.3
Morphs 63.0 67.2 65.7 65.1
5.1 Combination of ATB and Morph
We trained a model on ATBs that uses output of the
model trained on Morphs as additional information
(M2Af feature). We proceed similarly by training a
model on Morphs using output of the model trained
on ATBs (A2Mf feature). We have obtained the
features by a 15-way round-robin. Table 3 shows
the obtained results.
Table 3: Results in terms of F-measure of the combina-
tion experiments
Lexf Stemf Syntf Semf
ATBs 70.1 69.8 72.1 79.0
ATBs+M2Af 70.7 70.8 73.1 79.1
Morphs 74.1 74.5 75.5 78.3
Morphs+A2Mf 74.9 75.2 75.4 78.6
Results show a significant improvement for mod-
els that are trained on ATBs using information from
Morphs in addition to Lexf , Stemf and Syntf
features. This again confirms our claim that the use
of features from morphologically segmented text re-
duces the data sparseness and consequently leads to
better performance. For Semf features, only a 0.1
F-measure points have been gained. This is because
we are already using output of classifiers trained
on morphologically segmented data, which resolve
some of the data sparseness issue. The Morphs
side shows that the obtained performance when the
ATBs output is employed together with the Stemf
(75.2) is only 0.3 points below the performance of
the system using Syntf (75.5).
6 Conclusions
We have shown a comparative study aiming at defin-
ing the adequate unit of analysis for Arabic MD.
We conducted our study using four segmentation
schemes with four different feature-sets. Results
show that when only limited resources are available,
using morphological segmentation leads to the best
results. On the other hand, model trained on ATB
segmented data become more powerful and effective
when data sparseness is reduced by the use of other
classifier outputs trained on morphologically seg-
mented data. More improvement is obtained when
both segmentation styles are combined.
References
Y. Benajiba and I. Zitouni. 2009. Morphology-
based segmentation combination for arabic men-
tion detection. Special Issue on Arabic Nat-
ural Language Processing of ACM Transac-
tions on Asian Language Information Processing
(TALIP), 8(4).
Y. Benajiba, M. Diab, and P. Rosso. 2008. Arabic
named entity recognition using optimized feature
sets. In Proc. of EMNLP?08, pages 284?293.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing,
N. Kambhatla, X. Luo, N. Nicolov, and
S. Roukos. 2004. A statistical model for
multilingual entity detection and tracking. In
Proc.eedings of HLT-NAACL?04, pages 1?8.
N. Habash and F. Sadat. 2006. Combination of ara-
bic preprocessing schemes for statistical machine
translation. In Proceedings of ACL?06, pages 1?8.
H. Jing, R. Florian, X. Luo, T. Zhang, and A. Itty-
cheriah. 2003. HowtogetaChineseName(Entity):
Segmentation and combination issues. In Pro-
ceedings of EMNLP?03, pages 200?207.
E. W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses. John Wiley Sons.
I. Zitouni, J. Sorensen, X. Luo, and R. Florian.
2005. The impact of morphological stemming on
arabic mention detection and coreference resolu-
tion. In Proc. of the ACL Workshop on Compu-
tational Approaches to Semitic Languages, pages
63?70.
I. Zitouni, X. Luo, and R. Florian. 2009. A cascaded
approach to mention detection and chaining in
arabic. IEEE Transactions on Audio, Speech and
Language Processing, 17:935?944.
712
