Unificational Combinatory Categorial Grammar:
Combining Information Structure and Discourse Representations
Maarika Traat
The University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW,
United Kingdom,
M.Traat@ed.ac.uk
Johan Bos
The University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW,
United Kingdom,
jbos@inf.ed.ac.uk
Abstract
In this paper we present a grammar formalism that
combines the insights from Combinatory Categorial
Grammar with feature structure unification. We
show how information structure can be incorporated
with syntactic and semantic representations in a
principled way. We focus on the way theme, rheme,
and focus are integrated in the compositional se-
mantics, using Discourse Representation Theory as
first-order semantic theory. UCCG can be used for
parsing and generating prosodically annotated text,
and therefore has the potential to advance spoken
dialogue systems.
1 Introduction
The integration of information structure (the
way information is ?packaged? in a sentence)
in practical formalisms in computational lin-
guistics has long been ignored. There are two
main reasons for this: (1) formalisations of in-
formation structure often use variants of higher-
order logic to characterise its semantic impact
(Krifka, 1993; Kruijff-Korbayova, 1998; Steed-
man, 2000), which limits the use of inference
in practice (Blackburn and Bos, 2003); and (2)
the effect of information structure on the com-
positional semantics of an utterance is rarely
worked out in enough detail useful for compu-
tational implementation. On the other hand,
exploring information structure in spoken dia-
logue systems is becoming realistic now because
of the recent advances made in text-to-speech
synthesisers and automated speech recognisers
? hence there is a growing need for computa-
tional implementations of information structure
in grammar formalisms.
In this paper we present Unificational Com-
binatory Categorial Grammar (UCCG), which
integrates aspects of Combinatory Categorial
Grammar (Steedman, 2000), Unification Cat-
egorial Grammar (Zeevat, 1988; Calder et al,
1988), and Discourse Representation Theory
(Kamp and Reyle, 1993). It offers a compo-
sitional analysis of information structure, a se-
mantics compatible with first-order logic, and
a computational implementation for a fragment
of English, using unification in combining gram-
matical categories. As we will show, this makes
UCCG easy to implement, and allows us to in-
tegrate prosodic information in the semantics
in a transparent and systematic way. Although
based on first-order logic, we claim that UCCG
has enough expressive power to model informa-
tion structure such that it has the potential to
improve speech generation with context appro-
priate intonation in spoken dialogue systems.
2 Background
Categorial Grammars (CG) (Wood, 2000) are
lexicalised theories of grammar. The notion of
?category? refers to the functional type that is
associated with each entry in the lexicon which
determines the ability of a lexical item to com-
bine with other lexical items. CGs also have
a set of rules defining the syntactico-semantic
operations that can be performed on the cate-
gories.
Combinatory Categorial Grammar (CCG) is
a generalisation of CG (Steedman, 2000). While
the pure CG only involved functional applica-
tion rules for combining categories, CCG intro-
duces several additional combinatory rules for
both syntactic and semantic composition ? for-
ward and backward composition, and crossed
composition, as well as substitution rules. As
a result, CCG covers a wide range of linguis-
tic phenomena, including various kinds of coor-
dination. For building semantic representation
CCG uses the lambda calculus, although uni-
fication has been proposed as well (Steedman,
1990). Moreover, CCG has a built-in theory
of intonation and information structure (Steed-
man, 2000), that we will use as the basis for our
computational treatment of theme, rheme and
focus.
Unification Categorial Grammar (UCG) uses
Head-Driven Phrase Structure Grammar type
of feature structures, called signs, to represent
the categories of lexical items (Zeevat, 1988;
Calder et al, 1988). The directionality of the
attributes of a functor category is marked by
the features pre and post on its attributes rather
than by the directionality of the slashes as it
is done in CCG. In contrast to CCG, UCG
only uses forward and backward application
as means for combining categories. The use
of signs makes it straightforward to define the
syntax-semantic interface.
The formalism that we introduce in this pa-
per, UCCG, aims to marry the best parts of
CCG and UCG. Following UCG, we use signs
to represent the linguistic data, and both se-
mantics and syntax are built up simultaneously
via unification. From CCG we inherit the di-
rectional slash notation, the additional com-
binatory rules, and the analysis of intonation.
UCCG employs DRT (Kamp and Reyle, 1993)
with neo-davidsonian style event semantics as
semantic formalism, but extends the basic DRS
language to allow integration of prosodic infor-
mation in syntactic and semantic analysis.
3 Unificational CCG
3.1 Signs
UCCG makes use of feature structures called
signs in its linguistic description. There are two
types of signs: basic and complex signs. A basic
sign is a list of attributes or features describing
the syntactic and semantic characteristics of
a lexical expression, in the spirit of UCG.
We deviate from UCG in the way we define
complex signs, which is done recursively:
? If X and Y are signs then X/Y is a com-
plex sign.
? If X and Y are signs X\Y is a complex
sign.
? All basic and complex signs are signs.
A basic sign can have a varied number of
features, depending on the syntactic category
of the lexical expression the sign is character-
ising. There are three obligatory features any
sign must have, namely pho, cat and drs. pho
stands for the phonological form, cat for the
syntactic category of the lexical expression, and
drs for its semantical representation. Besides
the above three a sign can also have the follow-
ing features:1
? agr to mark the inflectional characteristics
of categories;
? var for discourse referents ranging over in-
dividuals;
? sit for discourse referents ranging over
eventualities (events or states).
In our notation inside the feature structures
we use the following convention: constants start
with a lower case letter, and variables start with
an upper case letter. The feature names are
written using small capitals. To make the fea-
ture structures more easily readable we narrow
the choice of possible variable names for each
type of variables:
? (pho) variables: W, W1, W2, etc.
? (agr) variables: A, A1, A2, etc.
? (drs) variables: D, D1, D2, etc.
? (sit) variables: E, E1, E2, etc.
? Discourse referents (var) use any other
capital letter with the preference for the
characters towards the end of the alphabet.
There are three kinds of basic signs in UCCG,
corresponding to the basic categories ? those
with cat feature sentence (s), those with cat
feature noun (n), and those with cat feature
verb phrase (vp). A basic sign for verb phrases
is shown in (1), and a complex sign for noun
phrases is shown in (2).
(1)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
pho: walks
cat: vp
agr: fin
var: X
sit: E
drs:
E
walk(E)
agent(E,X)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(2)
?
?
?
?
?
?
?
?
?
pho: every+man+W
cat: s
drs: X
man(X)
?D
?
?
?
?
?
?
?
?
?
/
?
?
?
?
?
?
?
?
pho: W
cat: vp
agr: fin
var: X
sit:
drs: D
?
?
?
?
?
?
?
?
1Depending of the needs of a specific application and
language for which a UCCG grammar is constructed
many more features could be introduced in basic signs.
The above examples illustrate the role of uni-
fication by creating a link between syntax and
semantics. UCCG explores the fact that the
same variables can be used at several different
levels. For example, the variables standing for
discourse referents serve as a link between syn-
tax and semantics ? the variable in the var
feature in the feature structure fits into its cor-
responding slot in the DRS in the drs feature.
We use this technique to integrate information
structure as well.
3.2 Categories
Each sign corresponds to a related CCG cate-
gory. The category of a basic sign is the value of
its cat feature. The category of a complex sign
it is made up of the cat feature values of all the
component parts of the complex sign, separated
by the slashes and brackets used in the complex
sign, resulting in a complex category. For in-
stance, the the syntactic category of the sign in
(1) is vp, and in (2) the category is s/vp. The
three basic categories used in UCCG are thus s,
n and vp, while all other categories are formed
by combining the above three, using backward
and forward slashes.
Note that noun phrase is not among the basic
categories. In UCCG We use its ?type-raised?
variant s/vp (corresponding to the CCG cate-
gory s/(s\np)). This choice is motivated by the
need to determine quantifier scope in the se-
mantics of quantified noun phrases. The some-
what unconventional basic category vp is a by-
product of the above.
3.3 Feature Values
In order to make it easier to refer to parts of
complex signs later, we introduce the following
terminology:
? X is the result of a sign X/Y or X\Y.
? Y is the argument of a sign X/Y or X\Y.
The value of the var and the sit features
is always a variable, while other features can
have a number of constant values. The pho fea-
ture holds the string value of the linguistic ex-
pression represented by the given feature struc-
ture. Presently, we use the orthographic form
of words. In basic signs the pho feature is filled
by lexical items, in complex signs it also con-
tains variables, which get constant values when
the complex sign is combined with its argument
signs. The pho feature in result parts of com-
plex signs is of the form:
. . . + W1 + word + W2 + . . .
where word is a lexical item, and W1 and W2
are variables that get values through unification
in the categorial combination process. The item
unifying with W1 precedes and the one unifying
with W2 follows the lexical item word. The ex-
act number and order of the variables the pho
feature contains depends on the category of the
given sign.
In the present implementation the agr fea-
ture is only used in connection with verb phrases
and can take constant values fin (finite) or non-
fin (non finite).
The drs feature, if it is not a variable itself,
holds a DRS corresponding to the semantics of
the lexical item(s) characterised by the given
sign. DRSs are constructed in a compositional
way using the var and sit features of the sign
to take care of predicate argument structure,
and the merge operator (;) to construct larger
DRSs from smaller ones. Merge-reduction is
used to eliminate merge operators introduced
in the composition process. This is also the
stage where discourse referents are renamed to
avoid accidental clashes of variables introduced
by unification (Blackburn and Bos, 2003).
3.4 The Combinatory Rules
Presently we have introduced the following four
CCG combinatory rules in UCCG: forward
application, backward application, forward
composition, and backward composition.
Other CCG combinatory rules could be intro-
duced equally easily should the need arise.
X/Y Y =? X
Forward application ?????>
Y X\Y =? X
Backward application <?????
X/Y Y/Z =? X/Z
Forward composition ???Comp>
Y\Z X\Y =? X\Z
Backward composition <Comp????
The rule boxes above are to be interpreted in
the following way: in the first row there is the
rule, on the left in the second row there is the
name of the rule and on the right the marking
for it as used in the derivations. The variables
X, Y and Z in the rules above stand for (basic
or complex) signs.
Some of the combinatory rules can be seen in
action on UCCG signs in Figures 1 to 3 below.
4 Adding Information Structure
By information structure we mean the way in-
formation is packaged in a sentence. We use the
terms theme and rheme as introduced by the
Prague circle of linguists. Theme is the central
question or topic the sentence is about, while
rheme is the novel contribution of the sentence.
In many languages, including English,
prosody is the main means of indicating the in-
formation structure of the sentence. In other
languages additional or alternative means may
be available, such as word order, and the use
of specific lexical items. Example (3) illustrates
the connection between information structure
and prosody in English.
(3) Who taught Alexander the Great?
[ARISTOTLE]rh [taught Alexander the Great.]th
?[Aristotle taught]th[ALEXANDER the GREAT.]rh
The lexical items in capital letters in (3) carry
the main rhematic accent of the sentence. As
illustrated by this example, the placement of
this accent determines whether the answer given
to the question is appropriate or not.
4.1 Information Structure in CCG
Steedman introduces information structure as
an integral part of the CCG formalism (Steed-
man, 2000). He argues that there is a specific
set of pitch accents in English that can accom-
pany theme, and another set that accompany
rheme, the most common theme pitch accent
being L+H* and the most common rheme pitch
accent being H*.2 The main pitch accent of the
intonational phrase combined with a boundary
tone gives us a complete intonational phrase.
There are various boundary tones, the most
frequently occurring ones being a low boundary
LL% and a rising boundary LH%. There is a
tendency for LH% to occur at the end of an
intonational phrase containing the theme pitch
accent L+H*, and for LL% to occur after the
rheme pitch accent H*.
According to the prosodical phrasing, CCG
provides different parses for the same string
of words, giving rise to different interpretation
with respect to information structure:
2The intonational notation used is due to Pierrehum-
bert (Pierrehumbert, 1980). According to her intona-
tional phrases are made up of the following components:
pitch accent(s), phrasal tone and boundary tone. In
Steedman?s (Steedman, 2000) representation the last two
have been joined together under the name ?boundary
tone?. L stands for low pitch, and H for high pitch.
(4) Anna
H* LL%
married Manny.
L+H* LH%
Anna
married Manny
(5) Anna
L+H*
married
LH%
Manny.
H* LL%
Anna married
Manny
Parsing according to intonational phrasing
in CCG is achieved in the following way: the
categories of lexical items can be either theme
marked by a theme accent, rheme marked by a
rheme accent, or unmarked (i.e., unaccented).
Theme and rheme marked categories can freely
combine with adjacent categories with the same
marking or adjacent categories with no intona-
tional marking. If a theme or rheme marked
category combines with an intonationally un-
marked category, the result category inherits
the themeness or rhemeness from the marked
category that participated in the combination
process.
While pitch accents are seen as properties
of words that carry them, boundary tones are
seen as individual lexical entries, and have their
own category of the form S$?\S$?/?, where
S$ is a variable that stands for any cate-
gory that is a function whose result is S (i.e.,
sentence), ? stands for phrase, ? for theme
and ? for rheme (Steedman, 2000). The ef-
fect this category achieves is copying the cat-
egory to its left it combines with, and replac-
ing its intonational marking by phrase. Phrase
marked categories can only combine with other
phrase marked categories, and hence avoid com-
bination over intonational phrase boundaries.
In other words, boundary tones function like
?stoppers? of theme and rheme categories, pre-
venting theme and rheme to be further spread
along sub-phrases of the sentence.
4.2 Information Structure in UCCG
When introducing prosodical and information-
structural features to UCCG we follow the the-
ory of CCG, with a few exeptions. As we
also aim to derive a computational implemen-
tation of UCCG in the form of a parser we need
to be concrete about how sign unification in
UCCG interacts with CCG?s theory of informa-
tion structure.
Adding intonation to UCCG raises several
problems, as combination of signs only via
straighforward unification is not possible any
more. We have to give prosodical signs the abil-
ity to alter prosodical feature values in the re-
sult signs they produce when combining with
a lexical sign. We will do this using recursive
unification?the details of this process will be
discussed in Sections 4.3 and 4.4.
Integrating information structure with the
UCCG sign representation brought along some
additions. Firstly, we introduce two new fea-
tures in the sign. The first of them is called inf
and expresses information structure. It can ei-
ther be a variable ? in the case of unmarked
expressions, or it can take the following values ?
(theme), ? (rheme), or ? (phrase). The second
newly introduced feature is foc. This feature
indicates focus, i.e. whether the particular word
carries a pitch accent or not. This feature is only
present on lexical signs.
The second change involves introducing in-
formation structural labels on DRS conditions
(except on those expressing the semantic roles of
verbs). The labels are of the form Cond:Inf Foc,
where Cond is a DRS condition, Inf stands for
the information-structure value (?, ?, or ?), and
Foc for the value of the focus (+ or ?). The
information-structure label in the DRS is tied
to the inf feature through the use of the same
variable, and gets its constant value from the
feature by unification.
4.3 Pitch Accents
CCG views pitch accents as properties of words
and introduces multiple entries for each lexical
item in the lexicon, whether it is theme marked,
rheme marked, or unmarked. We do not oppose
CCG?s view of pitch accents, but we chose a
slightly different approach in UCCG: pitch ac-
cents get similar treatment as boundary tones
? they are independent entries in the lexicon.
This way we avoid having to expand the lexi-
con. For instance, the lexical sign for the proper
name Manny is shown in (6).
(6)
?
?
?
?
?
?
?
?
?
pho: Manny+W
cat: s
inf: I
foc: F
drs:
X
manny(X):I F
;D
?
?
?
?
?
?
?
?
?
/
?
?
?
?
?
?
?
?
?
?
pho: W
cat: vp
var: X
sit: E
inf: I
foc: F
drs: D
?
?
?
?
?
?
?
?
?
?
Like all lexical signs, the sign in (6) shows
that the values for foc and inf are still unin-
stantiated. Once it combines with the sign for
a pitch accent, both of these features will get
instantiated. For example, (7) shows the result
of combining the above lexical sign with a the
sign for L+H*:
(7)
?
?
?
?
?
?
pho: Manny+W
cat: s
inf: ?
drs:
X
manny(X):?+
;D
?
?
?
?
?
?
/
?
?
?
?
?
?
?
?
pho: W
cat: vp
var: X
sit: E
inf: I
drs: D
?
?
?
?
?
?
?
?
Note that signs for pitch accents need to be
combined first with the signs of the lexical items
the accents appear on. Otherwise it would be
impossible to tell which item actually carries
the accent for larger phrases such as married
Manny H* LL% , where without the above men-
tioned constraint we could combine married and
Manny first to form the unit married Manny ,
and only then combine this two word unit with
the pitch accent. However, this is not what we
want, because this way we cannot determine
any more which of the two words was accented.
Note also, that the foc feature only appears in
lexical signs.
So what does a sign for pitch accents look
like? Borrowing from Steedman?s notation, the
sign for L+H* has the following format:
(8)
?
?
?
?
?
?
?
?
pho: W
cat: C
var: X
sit: E
inf: ?
drs: D
?
?
?
?
?
?
?
?
$ \
?
?
?
?
?
?
?
?
?
?
pho: W
cat: C
var: X
sit: E
inf: ?
foc: +
drs: D
?
?
?
?
?
?
?
?
?
?
$
The idea behind the sign in (8) is the fol-
lowing: the sign X$ stands for unification of X
with a (basic or complex) sign. In the case of
basic signs, ordinary unification on the level of
signs applies, in the case of complex signs, uni-
fication of S also applies to sub-signs. Through
unification of variables the information struc-
tural marking also finds its way to the DRS in
the form of labels on the appropriate DRS con-
ditions.
Combining the sign for ?Manny? (6) with the
sign for the theme accent ?L+H*? (8) results in
the unit ?Manny L+H*?, shown in (7). Notice
how through unification also the information
??
?
?
?
?
?
pho: married+W1
cat: vp
var: Z
sit: E
inf: I
drs: D1
?
?
?
?
?
?
?
/(
?
?
?
pho: W1+W2
cat: s
inf: I
drs: D1
?
?
?
/
?
?
?
?
?
?
?
?
?
?
?
?
?
pho: W2
cat: vp
var: Y
sit: E
inf: I
drs:
E
marry(E):I?
agent(E,Z)
patient(E,Y)
?
?
?
?
?
?
?
?
?
?
?
?
?
)
?
?
?
?
?
?
pho: Manny+W3
cat: s
inf: ?
drs: (
X
manny(X):?+
;D)
?
?
?
?
?
?
/
?
?
?
?
?
?
?
pho: W3
cat: vp
var: X
sit: E1
inf: ?
drs: D
?
?
?
?
?
?
?
?????????????????????????????????????????>
?
?
?
?
?
?
?
?
?
?
?
?
?
?
pho: married+Manny
cat: vp
var: Z
sit: E
inf: ?
drs: (
X
manny(X):?+
;
E
marry(E):??
agent(E,Z)
patient(E,X)
)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Derivation for married Manny H* using Forward Application
structural label of the DRS condition manny(X)
gets the value ?+ (theme and focus).
4.4 Boundary Tones
In essence the signs for boundary tones are sim-
ilar to the pitch accent signs, except that they
do not contain a foc feature in the argument
part. They take the following form:
(9)
?
?
?
?
?
?
?
?
pho: W
cat: C
var: X
sit: E
inf: ?
drs: D
?
?
?
?
?
?
?
?
$ \
?
?
?
?
?
?
?
?
pho: W
cat: C
var: X
sit: E
inf: ?
drs: D
?
?
?
?
?
?
?
?
$
As with pitch accents, when combining signs
of boundary tones the argument sign will unify
recursively with all sub-signs of the lexical sign,
effectively replacing the value of the inf feature
by ? (phrase).
Hence, the constant value ? for the inf fea-
ture only serves the purpose of keeping the
full intonational phrase from combining with
any other signs than similarly phrase marked
signs, and it has no impact on the semantics.
There are two signs for each boundary tone: one
that deals with boundary tones occurring at the
end of a rheme marked intonational phrase (as
shown above), and another one that deals with
boundary tones after themes.
We have restricted the variable in the argu-
ment part of the boundary signs to only be able
to combine with themes and rhemes, assum-
ing that in the case of unmarked themes (as
here is no pitch accent, there is no theme mark-
ing on the sign) we do not encounter boundary
tones after the theme part, and therefore we are
dealing with genuinely ambiguous information
structure. An unmarked theme will in our ap-
proach be automatically marked as part of the
rheme. For illustration of combining a lexical
sign with a boundary tone sign see Figure 2.
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
pho: married+Manny
cat: vp
var: Z
sit: E
inf: ?
drs:
X E
manny(X):?+
marry(E):??
agent(E,Z)
patient(E,X)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
pho: W
cat: C
var: X
sit: E
inf: ?
drs: D
?
?
?
?
?
?
?
$ \
?
?
?
?
?
?
?
pho: W
cat: C
var: X
sit: E
inf: ?
drs: D
?
?
?
?
?
?
?
$
?????????????????????<
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
pho: married+Manny
cat: vp
var: Z
sit: E
inf: ?
drs:
X E
manny(X):?+
marry(E):??
agent(E,Z)
patient(E,X)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 2: Derivation of married Manny H* LL%
using Backward Application
Finally, Figures 1 to 3 show a complete parse
of the prosodically marked sentence ?Anna
L+H* LH% married Manny H* LL%?. Due
to space considerations we omitted the two
initial steps that involve combining the sign
of ?Anna? with the sign of the theme accent
?L+H*? to form a new theme unit ?Anna
L+H*?, and then combining this unit with the
sign of the boundary tone ?LH%? to form the
full intonational phrase ?Anna L+H* LH%?.
(These steps are similar to the ones illustrated
in Figure 2.) Due to variable unification in
the features var and sit, while performing
the syntactic combination of the lexical signs,
we simultaneously construct the semantic
representation in the DRS.
?
?
?
?
?
?
pho: Anna+W
cat: s
inf: ?
drs:
Y
anna(Y):?+
;D
?
?
?
?
?
?
/
?
?
?
?
?
?
?
?
pho:W
cat:vp
var:Y
sit: E
inf: ?
drs:D
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
pho: married+Manny
cat: vp
var: Z
sit: E
inf: ?
drs:
X E
manny(X):?+
marry(E):??
agent(E,Z)
patient(E,X)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
????????????????????>
?
?
?
?
?
?
?
?
?
?
?
?
?
pho: Anna+married+Manny
cat: s
inf: ?
drs:
Y X E
anna(Y):?+
manny(X):?+
marry(E):??
agent(E,Y)
patient(E,X)
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 3: Derivation for ?Anna L+H* LH%
married Manny H* LL%?, using Forward Ap-
plication and Merge-Reduction
5 Conclusions and Future work
The present paper described the Unificational
Combinatory Categorial Grammar (UCCG)
formalism, which was developed bearing in
mind its future application in parsing and gen-
erating prosodically annotated text. One of the
key features of UCCG is the novel use of Dis-
course Representation Theory combined with
a theory of information structure. We believe
that UCCG has the potential to advance spoken
language dialogue systems, both in natural lan-
guage analysis and generation. Although cur-
rent automatic speech recognisers do not out-
put prosodic information, some of the state-of-
the-art speech synthesisers handle prosodically
annotated input strings.
We have implemented a UCCG parser for a
fragment of English that takes prosodically an-
notated strings as input and generates DRSs en-
riched with information structure. Future work
involves implementing a generation component
based on UCCG, evalating the expressive power
of UCCG with respect to information structure
on a selected corpus, and using the formalism
in existing spoken dialogue systems.
Acknowledgements
We would like to thank Frank Keller and Mark
Steedman for their comments on earlier versions
of this paper.
References
Patrick Blackburn and Johan Bos. 2003. Computa-
tional semantics. Theoria, 18(46):27?45.
Jonathan Calder, Ewan Klein, and Henk Zeevat.
1988. Unification categorial grammar: A con-
cise, extendable grammar for natural language
processing. In Proceedings of the 12th Interna-
tional Conerence on Computational Linguistics,
Budapest, August.
Hans Kamp and Uwe Reyle. 1993. From Discourse
to Logic. Kluwer Academic Publishers, London.
Manfred Krifka. 1993. Focus and Presupposition
in Dynamic Interpretation. Journal of Semantics,
10(4):269?300.
Ivana Kruijff-Korbayova. 1998. The Dynamic Po-
tential of Topic and Focus: A Praguian Approach
to Discourse Representation Theory. Ph.D. the-
sis, Faculty of Mathematics and Physics, Charles
University, Prague.
Janet Pierrehumbert. 1980. The Phonology and
Phonetics of English Intonation. Ph.D. thesis,
Massachusetts Institute of Technology, Blooming-
ton, IN. Published 1988 by Indiana University
Linguistics Club.
Mark Steedman. 1990. Gapping as constituent co-
ordination. Linguistics and Philosophy, 13.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, Massachusetts.
Mary McGee Wood. 2000. Syntax in categorial
grammar: An introduction for linguists. ESS-
LLI 2000, Birmingham, England. ESSLLI course-
book.
Henk Zeevat. 1988. Combining categorial grammar
and unification. In U.Reyle and C.Rohrer, ed-
itors, Natural Language Parsing and Linguistic
Theories. D.Reidel Publishing Company.
