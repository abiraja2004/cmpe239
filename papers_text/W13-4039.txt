Proceedings of the SIGDIAL 2013 Conference, pages 251?260,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Surface Text based Dialogue Models for Virtual Humans
Sudeep Gandhe and David Traum
USC Institute for Creative Technologies,
12015 Waterfront Drive, Playa Vista, CA 90094, USA
srgandhe@gmail.com, traum@ict.usc.edu
Abstract
We present virtual human dialogue mod-
els which primarily operate on the surface
text level and can be extended to incor-
porate additional information state annota-
tions such as topics or results from simpler
models. We compare these models with
previously proposed models as well as two
human-level upper baselines. The mod-
els are evaluated by collecting appropri-
ateness judgments from human judges for
responses generated for a set of fixed dia-
logue contexts. Our results show that the
best performing models achieve close to
human-level performance and require only
surface text dialogue transcripts to train.
1 Introduction
Virtual Humans (VH) are autonomous agents who
can play the role of humans in simulations (Rickel
and Johnson, 1999; Traum et al, 2005). For these
simulations to be convincing these agents must
have the ability to communicate with humans and
other agents using natural language. Like other di-
alogue system types, different architectures have
been proposed for virtual human dialogue sys-
tems. These architectures can afford different fea-
tures and require different sets of resources. E.g.,
an information state based architecture such as the
one used in SASO-ST (Traum et al, 2005) can
model detailed understanding of the task at hand
and progression of dialogue, but at the cost of re-
quiring resources such as information state update
rules and an annotated corpus or grammar to be
able to map surface text to dialogue acts.
For some virtual human dialogue genres such
as simple question-answering or some negotiation
domains, a simple model of dialogue progression
would suffice. In such a case we can build dia-
logue models that primarily operate on a surface
text level. These models only require surface text
dialogue transcripts as a resource, and don?t re-
quire expensive manual update rules, grammars,
or even extensive corpus annotation.
In this paper, we describe the construction and
evaluation of several models for engaging in dia-
logue by selecting an utterance that has been seen
previously in a corpus. We include one model that
has been used for this task previously (Gandhe and
Traum, 2007b), an adaptation of a model that has
been used in a similar manner, though on hand-
authored data sets, rather than data sets extracted
automatically from a corpus (Leuski and Traum,
2008), as well as a new set of models, using per-
ceptrons on surface text features as well as more
abstract information state annotations such as top-
ics. We also tackle the question of evaluating such
dialogue models manually as well as automati-
cally, starting with systematically analyzing var-
ious decisions involved in the evaluation process.
We situate our work with respect to previous eval-
uation methods.
2 Related Work
The task of a dialogue model is to formulate an
utterance given a dialogue context. There are
two approaches towards formulating an utterance:
Generation, where a response is compositionally
created from elements of the information state,
including the context of previous utterances, and
Selection, where a response is chosen from pre-
viously seen set of responses. In (Gandhe and
Traum, 2010), we examined the theoretical poten-
tial for the selection approach, looking at a wide
variety of domains, and evaluating based on sim-
ilarity between the actual utterance and the best
match in the previously seen corpus. We saw a
wide variance in scores across domains, both as to
the similarity scores and improvement of scores as
more data is considered. For task-oriented plan-
ning domains, such as Monroe (Stent, 2000) and
251
TRAINS (Heeman and Allen, 1994), as well as
open conversation in Switchboard (Godfrey et al,
1992), the performance was very low. On the other
hand, for more limited domains such as simple
question-answering (Leuski et al, 2006) or role-
play negotiation in a scenario, the performance
was high, with METEOR scores averaging over
0.8.
One possible selection criterion is to assume
that the most appropriate response is the most
probable response according to a model trained
on human-human dialogues. More formally, let
there be a dialogue ?u1, u2, . . . , ut?1, ut, . . . , uT ?,
where utterance ut appears in contextt =
?u1, u2, . . . , ut?1?. If we have a dialogue model
P estimated from the training corpus then the for-
mulated response uq for some unseen contextq is
given by,
ut = argmax
i
P (ui|contextt) ?ui ? Upossible
where Upossible is a set of all possible response ut-
terances. Ideally we would like to estimate a prob-
ability distribution P , but since it?s hard to esti-
mate and we only need argmax for this applica-
tion, we approximate P with a ranking function.
We can compare previous work within this frame-
work.
In our previous work (Gandhe and Traum,
2007a), we used context similarity as the rank-
ing function P (see section 3.1 for details). This
model is trained from in-domain surface text di-
alogue transcripts. Leuski et al (2006) model P
as cross-lingual relevance, where the task of se-
lecting an appropriate response is seen as cross-
lingual information retrieval where the response
utterance ut is the relevant document and the
contextt is treated as a query from different lan-
guage. This model has been applied to simple
question answering where context is the previous
utterance and the training data is manually anno-
tated question-answer pairs. DeVault et al (2011)
have proposed to use a multi-class classification
model (such as maximum entropy) for estimat-
ing P . Their method restricts the set Upossible
to a set of canonical utterances which represent
distinct dialogue acts. This allows for a limited
number of classes (|Upossible|) and also maximizes
the number of distinct contexts seen per utterance.
This model is also trained from manually anno-
tated utterance-context pairs and can additionally
use manually created utterance paraphrases.
Apart from the models discussed above which
have been mainly applied to dialogue domains
situated in a story context, there has been some
work in surface text based dialogue models for
open domains. Ritter et al (2011) use informa-
tion retrieval based and statistical machine trans-
lation (SMT) based approaches towards predicting
the next response in Twitter conversations. Also
Chatbots typically use surface text based process-
ing such as string transformations (e.g., AIML
rules (Wallace, 2003)). Such rules can also be
learned from a dialogue corpus (Abu Shawar and
Atwell, 2005). Systems employing SMT or string
transformation rules are formulating a response
by Generation approach and it can be frequently
ungrammatical or incoherent, unlike the selection
approach which will always pick something that
someone has once said (even though it might be
inappropriate in the current context).
3 Dialogue Models
3.1 Nearest Context
In previous work (Gandhe and Traum, 2007a), we
modeled P as,
P (ui|contextq) ? Sim(contexti, contextq)
where contexti is the context in which utterance
ui was seen in training corpus and Sim is con-
text similarity in a customized vector-space model.
The model restricts the set of possible response
utterances (Upossible) to the set of utterances ob-
served in the training data (Utrain). The context
is approximated using the previous two utterances
(one from each speaker). This model does not use
the contents of the utterance ui itself.
3.2 Cross-lingual Relevance Model
Leuski et al (2006) model P as a cross-lingual rel-
evance model. This model takes into account the
content of the utterance ui as well as the content of
the context. It does not impose any restriction on
Upossible, but in practice it is restricted to the set
of utterances in the training data. The model al-
lows the context to be composed of multiple fields,
each with its own weight. This allows us to ex-
tend the model where the context is approximated
by the previous two utterances. The weights need
to be learned using a held-out development set,
which presents a challenge in the case of multiple
fields (possible if we add more information state
annotations), modest amounts of training data and
252
non-availability of an automatic and reliable esti-
mate of the model?s performance. Here, for the
first time, we apply this model to automatically
extracted pairs of utterance-context and evaluate
it. For our model we used the implementation that
is available as a part of NPCEditor (Leuski and
Traum, 2011) and manually set the field weights
corresponding to the two previous utterances to be
equal (0.5).
3.3 Perceptron
As discussed earlier, the task of selecting the most
appropriate response can be viewed as multi-class
classification. But there are a couple of issues.
First, since we operate at the surface text level,
each unique response utterance will be labeled as
a separate class. The number of classes is the
number of unique utterances seen in the training
set, which is relatively large. As the training data
grows, the number of classes will increase. Sec-
ond, there are very few examples (on average a
single example) per class. We need a classifier that
can overcome these issues.
The perceptron algorithm and its variants ?
voted perceptron and averaged perceptron are
well known classification models (Freund and
Schapire, 1999). They have been extended for use
in various natural language processing tasks such
as part-of-speech tagging (Collins, 2002), pars-
ing (Collins, 2004) and discriminative language
modeling (Roark et al, 2007). Here we use the
averaged perceptron model for mapping from dia-
logue context to an appropriate response utterance.
Collins (2002) outlines the following four com-
ponents of a perceptron model:
? The training data. In our case it is a set of au-
tomatically extracted utterance-context pairs
{. . . , ?ui, contexti?, . . .}
? A function GEN(context) that enumerates a
set of all possible outputs (response utter-
ances) for any possible input (dialogue con-
text)
? A feature extraction function ? :
?u, context? ? Rd that is defined over
all possible pairings of response utterances
and dialogue contexts. d is the total number
of possible features.
? A parameter vector ?? ? Rd
Using such a perceptron model, the most appropri-
ate response utterance (ut) for the given dialogue
context (contextt) is given by,
uq = argmax
ui?GEN(context)
?(ui, contextq) ? ??
Algorithm 1 Perceptron Training Algorithm
Initialize: t? 0 ; ??0 ? 0
for iter = 1 to MAX ITER do
for i = 1 to N do
ri ? argmaxu?GEN(contexti) ?(u, contexti) ? ??t
if ri 6= ui then
??t+1 ? ??t + ?(ui, contexti) ?
?(ri, contexti)
else
??t+1 ? ??t
end if
t? t+ 1
end for
end for
return ??? (?t ??t)/(MAX ITER?N)
The parameter vector ?? is trained using the
training algorithm described in Algorithm 1. The
algorithm goes through the training data one in-
stance at a time. For every training instance, it
computes the best response utterance (ri) for the
context based on its current estimate of the param-
eter vector ??t. The algorithm changes the param-
eter vector only if it makes an error (ri 6= ui). The
update drives the parameter vector away from the
error (ri) and towards the correct output (ui). The
final parameter vector ?? is an average of all the in-
termediate ??t values. The averaging of parameter
vectors avoids overfitting.
The feature extraction function ? can list any
arbitrary features from the pair ?u, context?. We
consider information state annotations (ISt) along
with the surface text corresponding to the previous
two turns. The features could also include scores
computed from other models, such as those pre-
sented in sections 3.1 and 3.2. Figure 1 illustrates
an example context and utterance, and several fea-
tures. We examine several sets of features, Surface
text based features (?S), Retrieval model based
features (?R), and Topic based features (?T ).
Surface text based features (?S) are the fea-
tures extracted from the surface text of the previ-
ous utterances in the dialogue context (contextj)
and the response utterance (ui). ?S(d)(ux, uy) ex-
tracts surface text features from two utterances ? a
response utterance (ux) and an utterance (uy) from
the context that is (d) utterances away. There are
four types of features we extract:
253
? common term(d,w) features indicate the
number of times a wordw appears in both the
utterances. The total number of possible fea-
tures is O(|V |) and we select a small subset
of words (Selected common(d)) from the
vocabulary.
? The common term count(d) feature indi-
cates the number of words that appear in both
utterances.
? The unique common term count(d) fea-
ture indicates the number of unique words
that appear in both utterances.
? cross term(d,wx, wy) features indicate the
number of times the word wx appears in the
utterance ux and the word wy appears in the
utterance uy. The total possible number of
such cross features is very large (O(|V |2)),
where |V | is the utterance vocabulary size.
In order to keep the training tractable and
avoid overfitting, we select a small subset of
cross features (Selected cross(d)) from all
possible features.
In this model, we perform feature selection by
selecting the subsets Selected cross(d) and
Selected common(d). The training algorithm re-
quires evaluating the feature extraction (?S) func-
tion for all possible pairings of response utterances
and contexts. One simple feature selection crite-
rion is to allow the features only appearing in true
pairings of response utterance and context (i.e.
features from ?S(?ui, contextj?) ?i = j). The
subset Selected common(d) for common term
features is selected by extracting features from
only such true pairings.
For selecting cross term(d,wx, wy) features
we use only true pairings but we need to
reduce this subset even further. We im-
pose additional constraints based on the col-
lection frequency of lexical events such as,
cf(wx) > thresholdx, cf(wy) > thresholdy,
cf(?wx, wy?) > thresholdxy. Further reduction
in size of the selected subset of cross term fea-
tures is achieved by ranking the features using a
suitable ranking function and choosing the top n
features. In this model, we rank the cross term
features based on pointwise mutual-information
pmi(?wx, wy?) given by,
log p(?wx, wy?)p(wx)p(wy)
= log
(
#?wx,wy?
#??,??
)
(
#?wx,??
#??,??
)
?
(
#??,wy?
#??,??
)
Summing up, ?S(d)(ux, uy) =
{cross term(d,wx, wy) : wx ? ux?
wy ? uy ? ?wx, wy? ? Selected cross(d)}
? {common term(d,w) : w ? ux ?w ? uy ?
w ? Selected common(d)}
? {common term count(d)}
? {unique common term count(d)}
Retrieval model based features (?R) are
the scores computed in a fashion similar to
the Nearest Context model. Sim(ux, uy) is
a cosine similarity function for tf-idf weighted
vector space representations of utterances and
Sim(contexta, contextb) is the same function
from Nearest Context model. We define three fea-
tures,
? retrieval score =
|L|max
k=1
Sim(contextj , contextk) ? Sim(ui, uk)
? context sim@best utt match =
Sim(contextj , contextb)
where, b = |L|argmax
k=1
Sim(ui, uk)
? utt sim@best context match = Sim(ui, ub)
where, b = |L|argmax
k=1
Sim(contextj , contextk)
?R(?ui, contextj?) = {retrieval score,
context sim@best utt match,
utt sim@best context match}
Topic based feature (?T ) tracks the topic sim-
ilarity between the topic of the dialogue context
and the response utterance. A topic is marked
as mentioned if a set of keywords triggering that
topic have been previously mentioned in the dia-
logue. Each information state (IS) consists of a
topic signature which can be viewed as a boolean
vector representing mentions of topics.
?T (?ui, contextj?) = {topic similarity}
topic similarity = cosine(ISi, ISj)
where, ISi is the topic and is part of contexti
which is the context associated with the utterance
ui.
The perceptron model presented here allows
novel combinations of resources such as combin-
ing surface text transcripts with information state
annotations for tracking topics in the conversa-
tion. As compared to the generative cross-lingual
relevance model approach, the perceptron model
is a discriminative model. It is also a paramet-
ric model and the inference requires linear time
with respect to the size of candidate utterances
(|GEN(context)|) and the number of features (|??|).
Although, computing some of the features them-
selves (e.g., ?R features) requires linear time with
254
...
contextj [uj(?2)] Doctor you are the threat i need protection from you
[uj(?1)] Captain no no
you do you do not need protection from me
i am here to help you
uh what i would like to do is move your your clinic to a safer location
and uh give you money and medicine to help build it
utterance [ui] Doctor i have no way of moving
?S(?ui, contextj?) = { cross term(?2, ?moving?, ?need?) = 1,
common term(?2, ?i?) = 1,
common term count(?2) = 1, unique common term count(?2) = 1,
cross term(?1, ?moving?, ?give?) = 1,
common term(?1, ?i?) = 1, common term(?1, ?no?) = 1,
common term count(?1) = 2, unique common term count(?1) = 2,
retrieval score = 0.198, context sim@best utt match = 0.198,
utt sim@best context match = 0,
topic similarity = 0.667 }
Figure 1: Features extracted from a context (contextj) and a response utterance (ui)
respect to the size of the training data. The per-
ceptron model can rank an arbitrary set of utter-
ances given a dialogue context. But some of the
features (e.g., topic similarity) require that the
utterance ui(ui ? |GEN(context)|) be associated
with a known context (contexti). For all our mod-
els we use GEN(context) = Utrain.
We have implemented three different vari-
ants of the perceptron model based on the
choice of features used. Perceptron(surface)
model uses only surface text features (? =
?S). The other two models are Percep-
tron(surface+retrieval) where ? = ?S ? ?R and
Perceptron(surface+retrieval+topic) where ? =
?S ? ?R ? ?T .
Figure 2 shows a schematic representation of
these models along with the set of resources be-
ing used by each model. The figure also shows the
relationships between these models. The arrows
point from a less informative model to a more in-
formative model and the annotations on these ar-
rows indicate the additional information used.
4 Evaluation
For the experiments reported in this paper, we
used the human-human spoken dialogue corpus
collected for the project SASO-ST (Traum et al,
2005). In this scenario, the trainee acts as an
Army Captain negotiating with a simulated doc-
Figure 2: A schematic representation of imple-
mented unsupervised dialogue models and the re-
lationships between the information used by their
ranking functions.
tor to convince him to move his clinic to another
location. The corpus is a collection of 23 roleplay
dialogues and 13 WoZ dialogues lasting an aver-
age of 40 turns (a total of ? 1400 turns and ? 30k
words).
We perform a Static Context evalua-
tion (Gandhe, 2013). In Static Context evaluation,
all the dialogue models being evaluated receive
the same set of contexts as input. These dialogue
contexts are extracted from actual in-domain
255
human-human dialogues and are not affected by
the dialogue model being evaluated. For every
turn whose role is to be played by the system, we
predict the most appropriate response in place of
that turn given the dialogue context.
Since the goal for virtual humans is to be as
human-like as possible, a suitable evaluation met-
ric is how appropriate or human-like the responses
are for a given dialogue context. The evaluation
reported here employs human judges. We set up a
simple subjective 5-point likert scale for rating ap-
propriateness ? 1 being a very inappropriate non-
sensical response and 5 being a perfectly appropri-
ate response.
We built five dialogue models to play the role
of the doctor in SASO-ST domain, viz.: Near-
est Context (section 3.1), Cross-lingual Relevance
Model (section 3.2) and three perceptron models
(section 3.3) with different feature sets. These
dialogue models are evaluated using 5 in-domain
human-human dialogues from the training data (2
roleplay and 3 WoZ dialogues, referred to as test
dialogues). A dialogue model is trained in a leave-
one-out fashion where the training data consists of
all dialogues except the one test dialogue that is
being evaluated. A dialogue model trained in this
fashion is then used to predict the most appropri-
ate response for every context that appears in the
test dialogue. This process is repeated for each test
dialogue and for each dialogue model being evalu-
ated. In this evaluation setting, the actual response
utterance found in the original human-human dia-
logue may not belong to the set of utterances being
ranked by the dialogue model. We also compare
these five dialogue models with two human-level
upper baselines. Figure 4 in the appendix shows
some examples of utterances returned by a couple
of the models.
4.1 Human-level Upper Baselines
In order to establish an upper baseline for human-
level performance for the evaluation task, we con-
ducted a wizard data collection. We asked human
volunteers (wizards) to perform a similar task to
that performed by the dialogue models being eval-
uated. The wizard is presented with a set of ut-
terances (Utrain) and is asked to select a subset
from these that will be appropriate as a response
for the presented dialogue context. Compared to
this, the task of the dialogue model is to select
a single most appropriate response for the given
context.
DeVault et al (2011) carried out a similar wiz-
ard data collection but at the dialogue act level,
where wizards were asked to select only one re-
sponse dialogue act for each dialogue context.
Their findings suggest that there are several valid
response dialogue acts for a dialogue context. A
specific dialogue act can be realized in several
ways at the surface text level. For these reasons
we believe that for a given dialogue context there
are often several appropriate response utterances
at the surface text level. In our setting the dia-
logue models work at the surface text level and
hence the wizards were asked to select a subset of
surface text utterances that would be appropriate
responses. Each wizard was asked to select sev-
eral (ideally between five and ten, but always at
least one) appropriate responses for each dialogue
context. Four wizards participated in this data col-
lection with each wizard selecting responses for
the contexts from the same five human-human test
dialogues. The set of utterances to chose from
(Utrain) for every test dialogue was built in the
same leave-one-out fashion as used for evaluating
the implemented dialogue models.
There are a total of 89 dialogue contexts where
the next turn belongs to doctor. As expected, wiz-
ards frequently chose multiple utterances as ap-
propriate responses (mean = 7.80, min = 1, max
= 25).
This data collected from wizards is used to build
two human-level upper-baseline models for the
task of selecting a response utterance given a di-
alogue context:
Wizard Max Voted model returns the response
which gets the maximum number of votes
from the four wizards. Ties are broken
randomly.
Wizard Random model returns a random utter-
ance from the list of all utterances marked as
appropriate by one of the wizards.
4.2 Comparative Evaluation of Models
We performed a static context evaluation using
four judges for the above-mentioned two human-
level baselines (Wizard Random and Wizard Max
Voted) and five dialogue models (Nearest Con-
text, Cross-lingual Relevance Model and three
perceptron models), as described in section 3.3.
We tune the parameters used for the perceptron
256
models based on the automatic evaluation met-
ric, Weak Agreement (DeVault et al, 2011). Ac-
cording to this evaluation metric a response utter-
ance is judged as perfectly appropriate (a score
of 5) if any of the wizards chose this response
utterance for given context and inappropriate (a
score of 0) otherwise. The Perceptron(surface)
model was trained using 30 iterations, the Per-
ceptron(surface+retrieval) using 20 iterations,
and the Perceptron(surface+retrieval+topic) was
trained using 25 iterations. For all perceptron
models we used thresholdx = thresholdy =
thresholdxy = 3.
For a comparative evaluation of dialogue mod-
els, we need an evaluation setup where judges
could see the complete dialogue context along
with the response utterances generated by the di-
alogue models to be evaluated. In this setup, we
show all the response utterances next to each other
for easy comparison and we do not show the ac-
tual response utterance that was encountered in
the original human-human dialogue. We built a
web interface for collecting appropriateness rat-
ings that addresses the above requirements. Fig-
ure 3 shows the web interface used by the four
judges to evaluate the appropriateness of response
utterances for given dialogue context. The appro-
priateness was rated on the same scale of 1 to 5.
The original human-human dialogue (roleplay or
WoZ) is shown on the left hand side and the re-
sponse utterances from different dialogue models
are shown on the right hand side. In cases where
different dialogue models produce the same sur-
face text response only one candidate surface text
is shown to judge. Once the judge has rated all the
candidate responses they can proceed to the next
dialogue context. This setting allows for compar-
ative evaluation of different dialogue models. The
presentation order of responses from different di-
alogue models is randomized. Two of the judges
also performed the role of the wizards in our wiz-
ard data collection as outlined in section 4.1, but
the wizard data collection and the evaluation tasks
were separated by a period of over 3 months.
Table 1 shows the results of our compara-
tive evaluation for each judge and averaged over
all judges. We also computed inter-rater agree-
ment for individual ratings for all response ut-
terances using Krippendorff?s ? (Krippendorff,
2004). There were a total of n = 397 distinct
response utterances that were judged by the eval-
uators. The Krippendorff?s ? for all four judges
was 0.425 and it ranges from 0.359 to 0.495 for
different subsets of judges. The value of ? indi-
cates that the inter-rater agreement is substantially
above chance (? > 0), but indicates a fair amount
of disagreement, indicating that judging appropri-
ateness is a hard task even for human judges. Al-
though there is low inter-rater agreement at the
individual response utterance level there is high
agreement at the dialogue model level. Pearson?s
correlation between the average appropriateness
for different dialogue models ranges from 0.928
to 0.995 for different pairs of judges.
We performed a paired Wilcoxon test to check
for statistically significant differences in differ-
ent dialogue models. Wizard Max Voted is sig-
nificantly more appropriate than all other models
(p < 0.001). Wizard Random is significantly more
appropriate than Cross-lingual Relevance Model
(p < 0.05) and significantly more appropriate
than the three perceptron models as well as Near-
est Context model (p < 0.001). Cross-lingual
Relevance Model is significantly more appropri-
ate than Nearest Context (p < 0.01). All other
differences are not statistically significant at the 5
percent level.
We found that adding topic annotations did not
help. This is in contrast with previous observa-
tion (Gandhe and Traum, 2007b), where topic in-
formation helped when evaluation was performed
in Dynamic Context setting. In Dynamic Context
setting, the dialogue model is used in an online
fashion where the response utterances it generates
become part of the dialogue contexts with respect
to which the subsequent responses are predicted
and evaluated. The topic information ensures sys-
tematic progression of dialogue. But for static
context evaluation such help is not required as the
dialogue contexts are extracted from human hu-
man dialogues and are fixed.
5 Conclusion
In this paper we introduced dialogue models that
can be trained simply from in-domain surface
text dialogue transcripts. Some of these models
also allow for incorporating additional informa-
tion state features such as topics or results of sim-
pler models. We have evaluated the appropriate-
ness of responses and have compared these mod-
els with two human-level baselines. Evaluating
response appropriateness is highly subjective as
257
Figure 3: Screenshot of the user interface for static context comparative evaluation of dialogue models
Model #Utts Avg. appropriateness Appropriateness(All judges)
Judge 1 Judge 2 Judge 3 Judge 4 Avg stddev
Nearest Context 89 4.12 3.98 3.40 3.53 3.76 1.491
Perceptron(surface) 89 3.97 4.11 3.51 3.62 3.80 1.445
Perceptron
(surface+retrieval)
89 4.26 4.12 3.51 3.72 3.90 1.414
Perceptron
(surface+retrieval+topic)
89 4.21 4.09 3.51 3.57 3.85 1.433
Cross-lingual Relevance
Model
89 4.28 4.31 3.70 3.91 4.05 1.314
Wizard Random 89 4.55 4.55 4.03 4.16 4.32 1.153
Wizard Max Voted 89 4.76 4.84 4.40 4.52 4.63 0.806
Table 1: Offline comparative evaluation of dialogue models.
can be seen from the fact that utterances which
receive more wizard votes (Wizad Max Voted) re-
ceive significantly higher appropriateness ratings
than those which receive fewer votes (Wizard Ran-
dom). The performance of best performing dia-
logue models are close to human-level baselines.
In future we plan to use larger datasets which
should be easy, since no additional annotations are
required for training these dialogue models.
Acknowledgments
The effort described here has been sponsored by
the U.S. Army. Any opinions, content or informa-
tion presented does not necessarily reflect the posi-
tion or the policy of the United States Government,
and no official endorsement should be inferred.
References
Bayan Abu Shawar and Eric Atwell. 2005. Using cor-
pora in machine-learning chatbot systems. Interna-
tional Journal of Corpus Linguistics, 10:489?516.
258
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing - Volume 10, EMNLP
?02, pages 1?8, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Michael Collins, 2004. Parameter estimation for sta-
tistical parsing models: theory and practice of
distribution-free methods, pages 19?55. Kluwer
Academic Publishers, Norwell, MA, USA.
David DeVault, Anton Leuski, and Kenji Sagae. 2011.
Toward learning and evaluation of dialogue policies
with text examples. In Proceedings of the SIGDIAL
2011 Conference, pages 39?48, Portland, Oregon,
June. Association for Computational Linguistics.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Mach. Learn., 37:277?296, December.
Sudeep Gandhe and David Traum. 2007a. Creating
spoken dialogue characters from corpora without an-
notations. In Proceedings of Interspeech-07.
Sudeep Gandhe and David Traum. 2007b. First steps
towards dialogue modeling from an un-annotated
human-human corpus. In 5th Workshop on knowl-
edge and reasoning in practical dialogue systems,
Hyderabad, India.
Sudeep Gandhe and David Traum. 2010. I?ve said it
before, and i?ll say it again: an empirical investiga-
tion of the upper bound of the selection approach to
dialogue. In Proceedings of the 11th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue, SIGDIAL ?10, pages 245?248, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Sudeep Gandhe. 2013. Rapid prototyping and evalu-
ation of dialogue systems for virtual humans. Ph.D.
thesis, University of Southern California.
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
Switchboard: Telephone speech corpus for research
and development. In Proc. of ICASSP-92, pages
517?520.
P. A. Heeman and J. Allen. 1994. The TRAINS 93 di-
alogues. TRAINS Technical Note 94-2, Department
of Computer Science, University of Rochester.
Klaus Krippendorff. 2004. Content Analysis, An Intro-
duction to Its Methodology 2nd Edition. Sage Pub-
lications.
Anton Leuski and David Traum. 2008. A statistical
approach for text processing in virtual humans. In
Proccedings of 26th Army Science Conference.
Anton Leuski and David Traum. 2011. Npceditor:
Creating virtual human dialogue using information
retrieval techniques. AI Magazine, 32(2):42?56.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective ques-
tion answering characters. In Proceedings of the
7th SIGdial Workshop on Discourse and Dialogue,
pages 18?27, Sydney, Australia, July. Association
for Computational Linguistics.
Jeff Rickel and W. Lewis Johnson. 1999. Virtual hu-
mans for team training in virtual reality. In Proceed-
ings of the Ninth International Conference on Artifi-
cial Intelligence in Education, pages 578?585. IOS
Press.
Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 583?593, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Brian Roark, Murat Saraclar, and Michael Collins.
2007. Discriminative n-gram language modeling.
Comput. Speech Lang., 21:373?392, April.
A. J. Stent. 2000. The monroe corpus. Technical
Report 728, Computer Science Dept. University of
Rochester.
David Traum, William Swartout, Jonathan Gratch, and
Stacy Marsella. 2005. Virtual humans for non-
team interaction training. In AAMAS-05 Workshop
on Creating Bonds with Humanoids, July.
Richard Wallace. 2003. AIML Overview. ALICE A. I.
Foundation.
259
Original Human-Human Dialogue Model response
Captain hello how are you doing sir
Doctor what do you want
i have patients waiting for me
hello
what can i do for you mr
Captain sir i come by to offer you some assistance ah on
behalf of my commander
ah we want to try to support you in this hospital
and ah improving the health and welfare of the
civilians that you have under your care
Doctor well this is your opinion i see captain
is this a is this a suggestion from your
commander
Captain i am sorry sir
could you say again
Doctor well this is your opinion you are the threat
i need protection from you
(a) Perceptron(surface)
Original Human-Human Dialogue Model response
Captain hello doctor perez
Doctor hello hello
what was your name
captain dezois very nice to meet you
i am sorry but i am very busy today
so i only have a limited amount of time
what can i help you with
Captain i am captain xx
Doctor so
do you need help
ok
what do you want i have patients waiting for me
Captain yes
i have a very urgent matter to discuss
with you
Doctor are you injured pl please captain
i i am sorry to cut you off
but i really must uh go
i was in the middle of examining a patient
(b) Cross-lingual Relevance Model
Figure 4: Example interaction for the dialogue models in static context setting. The second column
shows the original human-human dialogue and the third column shows the dialogue model?s response
for the corresponding system turn.
260
