2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 263?273,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Every sensible extended top-down tree transducer
is a multi bottom-up tree transducer
Andreas Maletti?
Institute for Natural Language Processing, Universit?t Stuttgart
Pfaffenwaldring 5b, 70569 Stuttgart, Germany
andreas.maletti@ims.uni-stuttgart.de
Abstract
A tree transformation is sensible if the size of
each output tree is uniformly bounded by a
linear function in the size of the correspond-
ing input tree. Every sensible tree transfor-
mation computed by an arbitrary weighted ex-
tended top-down tree transducer can also be
computed by a weighted multi bottom-up tree
transducer. This further motivates weighted
multi bottom-up tree transducers as suitable
translation models for syntax-based machine
translation.
1 Introduction
Several different translation models are used in
syntax-based statistical machine translation. Koehn
(2010) presents an introduction to statistical ma-
chine translation, and Knight (2007) presents an
overview of syntax-based statistical machine trans-
lation. The oldest and best-studied tree transfor-
mation device is the top-down tree transducer of
Rounds (1970) and Thatcher (1970). G?cseg and
Steinby (1984) and F?l?p and Vogler (2009) present
the existing results on the unweighted and weighted
model, respectively. Knight (2007) promotes the
use of weighted extended top-down tree transduc-
ers (XTOP), which have also been implemented in
the toolkit TIBURON by May and Knight (2006)
[more detail is reported by May (2010)]. In the con-
text of bimorphisms, Arnold and Dauchet (1976) in-
vestigated XTOP, and Lilin (1978) and Arnold and
Dauchet (1982) investigated multi bottom-up tree
?The author was supported by the German Research Foun-
dation (DFG) grant MA 4959/1-1.
transducers (MBOT) [as k-morphisms]. Recently,
weighted XTOP and MBOT, which are the cen-
tral devices in this contribution, were investigated
by Maletti (2011a) in the context of statistical ma-
chine translation.
Several tree transformation devices are used as
translation models in statistical machine translation.
Chiang (2007) uses synchronous context-free gram-
mars, which force translations to be very similar
as observed by Eisner (2003) and Shieber (2004).
This deficiency is overcome by synchronous tree
substitution grammars, which are state-less linear
and nondeleting XTOP. Recently, Maletti (2010b)
proposed MBOT, and Zhang et al (2008b) and
Sun et al (2009) proposed the even more powerful
synchronous tree-sequence substitution grammars.
Those two models allow certain translation discon-
tinuities, and the former device also offers computa-
tional benefits over linear and nondeleting XTOP as
argued by Maletti (2010b).
The simplicity of XTOP makes them very appeal-
ing as translation models. In 2010 the ATANLP par-
ticipants [workshop at ACL] identified ?copying? as
the most exciting and promising feature of XTOP,
but unrestricted copying can lead to an undesirable
explosion of the size of the translation. According
to Engelfriet and Maneth (2003) a tree transforma-
tion has linear size-increase if the size of each output
tree is linearly bounded by the size of its correspond-
ing input tree. The author believes that this is a very
sensible restriction that intuitively makes sense and
at the same time suitably limits the copying power
of XTOP.
We show that every sensible tree transformation
263
that can be computed by an XTOP can also be com-
puted by an MBOT. For example, linear XTOP (i.e.,
no copying) compute only sensible tree transforma-
tions, and Maletti (2008) shows that for each linear
XTOP there exists an equivalent MBOT. Here, we
do not make any restrictions on the XTOP besides
some sanity conditions (see Section 3). In particu-
lar, we consider copying XTOP. If we accept the re-
striction to linear size-increase tree transformation,
then our main result further motivates MBOT as a
suitable translation model for syntax-based machine
translation because MBOT can implement each rea-
sonable (even copying) XTOP. In addition, our re-
sult allows us to show that each reasonable XTOP
preserves regularity under backward application. As
demonstrated by May et al (2010) backward appli-
cation is the standard application of XTOP in the
machine translation pipeline, and preservation of
regularity is the essential property for several of the
evaluation algorithms of May et al (2010).
2 Notation
We start by introducing our notation for trees, whose
nodes are labeled by elements of an alphabet ? and
a set V . However, only leaves can be labeled by
elements of V . For every set T , we let
?(T ) = {?(t1, . . . , tk) | ? ? ?, t1, . . . , tk ? T} ,
which contains all trees with a ?-labeled root
and direct successors in T . The set T?(V ) of
?-trees with V -leaves is the smallest set T such that
V ? ?(T ) ? T . We use X = {x1, x2, . . . } as a set
of formal variables.
Each node of the tree t ? T?(V ) is identified by
a position p ? N+, which is a sequence of posi-
tive integers. The root is at position ? (the empty
string), and the position ip with i ? N+ and p ? N?+
is the position p in the i-th direct subtree. The
set pos(t) contains all positions of t, and the size
of t is |t| = |pos(t)|. For each p ? pos(t), the label
of t at p is t(p). Given a set L ? ??V of labels, we
let posL(t) = {p ? pos(t) | t(p) ? L} be the posi-
tions with L-labels. We write posl(t) for pos{l}(t)
for each l ? L. Finally, we write t[u]p for the tree
obtained from t by replacing the subtree at position p
by the tree u ? T?(V ).
The following notions refer to the variables X .
The tree t ? T?(V ) [potentially V ? X = ?] is
S?
NP1
PP11
x1112
VP2
VBD21
ran211
RB22
away221
Figure 1: The tree t (with positions indicated as super-
scripts) is linear and var(t) = {x2}. The tree t[He]111 is
the same tree with x2 replaced by ?He?.
linear if every x ? X occurs at most once in t (i.e.,
|posx(t)| ? 1). Moreover,
var(t) = {x ? X | posx(t) 6= ?}
contains the variables that occur in t. A substitu-
tion ? is a mapping ? : X ? T?(V ). When applied
to t, it returns the tree t?, which is obtained from t
by replacing all occurrences of x ? X in t by ?(x).
Our notions for trees are illustrated in Figure 1.
Finally, we present weighted tree grammars
(WTG) as defined by F?l?p and Vogler (2009), who
defined it for arbitrary semirings as weight struc-
tures. In contrast, our weights are always nonneg-
ative reals, which form the semiring (R+,+, ?, 0, 1)
and are used in probabilistic grammars. For each
weight assignment f : T ? R+, we let
supp(f) = {t ? T | f(t) 6= 0} .
WTG offer an efficient representation of weighted
forests (i.e., set of weighted trees), which is even
more efficient than the packed forests of Mi et al
(2008) because they can be minimized efficiently us-
ing an algorithm of Maletti and Quernheim (2011).
In particular, WTG can share more than equivalent
subtrees and can even represent infinite sets of trees.
A WTG is a system G = (Q,?, q0, P,wt) with
? a finite set Q of states (nonterminals),
? an alphabet ? of symbols,
? a starting state q0 ? Q,
? a finite set P of productions q ? r, where
q ? Q and r ? T?(Q) \Q, and
? a mapping wt: P ? R+ that assigns produc-
tion weights.
Without loss of generality, we assume that we can
distinguish states and symbols (i.e., Q ? ? = ?).
For all ?, ? ? T?(Q) and a production ? = q ? r,
264
St1 VP
t2 t3
7?
S
t2 t1 t3
Figure 2: Example rotation. In principle, such rotations
are required in the translation from English to Arabic.
we write ? ??G ? if ? = ?[q]p and ? = ?[r]p, where
p is the lexicographically least element of posQ(?).
The WTG G generates the weighted tree lan-
guage LG : T? ? R+ such that
LG(t) =
?
n?N,?1,...,?n?P
q0?
?1
G ????
?n
G t
wt(?1) ? . . . ? wt(?n)
for every t ? T?. Each such language is regular, and
Reg(?) contains all those languages over the alpha-
bet ?. A thorough introduction to tree languages is
presented by G?cseg and Steinby (1984) and G?c-
seg and Steinby (1997) for the unweighted case and
by F?l?p and Vogler (2009) for the weighted case.
3 Extended top-down tree transducers
We start by introducing the main model of this
contribution. Extended top-down tree transducers
(XTOP) are a generalization of the top-down tree
transducers (TOP) of Rounds (1970) and Thatcher
(1970). XTOP allow rules with several (non-state
and non-variable) symbols in the left-hand side (as
in the rule of Figure 3), whereas a TOP rule contains
exactly one symbol in the left-hand side. Shieber
(2004) and Knight (2007) identified that this exten-
sion is essential for many NLP applications because
without it linear (i.e., non-copying) cannot compute
rotations (see Figure 2). In the form of bimorphisms
XTOP were investigated by Arnold and Dauchet
(1976) and Arnold and Dauchet (1982) in the 1970s,
and Knight (2007) invigorated research.
As demonstrated by Graehl et al (2009) the
most general XTOP model includes copying, dele-
tion, and regular look-ahead in the spirit of En-
gelfriet (1977). More powerful models (such as
synchronous tree-sequence substitution grammars
and multi bottom-up tree transducers) can handle
translation discontinuities naturally as evidenced
by Zhang et al (2008a) and Maletti (2011b), but
q0
S
x1 VP
x2 x3
?
S
qVB
x2
qNP
x1
qNP
x3
Figure 3: Example XTOP rule by Graehl et al (2008).
XTOP need copying and deletion to handle them.
Copying essentially allows an XTOP to translate
certain parts of the input several times and was iden-
tified by the ATANLP 2010 participants as one of the
most interesting and promising features of XTOP.
Currently, the look-ahead feature is not used in ma-
chine translation, but we need it later on in the theo-
retical development.
Given an alphabet Q and a set T , we let
Q[T ] = {q(t) | q ? Q, t ? T},
in which the root always has exactly one succes-
sor from T in contrast to Q(T ). We treat elements
of Q[T?(V )] as special trees of T??Q(V ). More-
over, we let 1??(t) = 1 for every t ? T?. XTOP
with regular look-ahead (XTOPR) were also stud-
ied by Knight and Graehl (2005) and Graehl et al
(2008). Formally, an XTOPR is a system
M = (Q,?,?, q0, R, c,wt)
with
? a finite set Q of states,
? alphabets ? and ? of input and output symbols,
? a starting state q0 ? Q,
? a finite set R of rules of the form ` ? r with
linear ` ? Q[T?(X)] and r ? T?(Q[var(`)]),
? c : R ? X ? Reg(?) assigns a regular look-
ahead to each deleted variable of a rule [i.e.,
c(` ? r, x) = 1?? for all ` ? r ? R and
x ? X \ (var(`) \ var(r))], and
? wt: R? R+ assigns rule weights.
The XTOPR M is linear [respectively, nondeleting]
if r is linear [respectively, var(`) = var(r)] for ev-
ery rule ` ? r ? R. It has no look-ahead (XTOP)
if c(?, x) = 1?? for all ? ? R and x ? X . Figure 3
shows a rule of a linear and nondeleting XTOP.
The look-ahead can be used to restrict rule appli-
cations. It can inspect subtrees that are deleted by a
265
uq0
S
t1
VP
t2 t3
??,.5M
u
S
qVB
t2
qNP
t1
qNP
t3
Figure 4: Rewrite step using rule ? of Figure 3.
rule application, so for each rule ? = ` ? r, we let
del(?) = var(`) \ var(r) be the set of deleted vari-
ables in ?. If we suppose that a variable x ? del(?)
matches to an input subtree t, then the weight of the
look-ahead c(?, x)(t), which we also write c?,x(t),
is applied to the derivation. If it is 0, then this look-
ahead essentially prohibits the application of ?. It is
important that the look-ahead is regular (i.e., there
exists a WTG accepting it). The toolkit TIBURON
by May and Knight (2006) implements XTOP to-
gether with a number of essential operations. Look-
ahead is not implemented in TIBURON, but it can
be simulated using a composition of two XTOP, in
which the first XTOP performs the look-ahead and
marks the results, so that the second XTOP can ac-
cess the look-ahead information.
As for WTG the semantics for the XTOPR
M = (Q,?,?, I, R, c,wt) is presented using
rewriting. Without loss of generality, we again sup-
pose that Q ? (? ??) = ?. Let ?, ? ? T?(Q[T?]),
w ? R+, and ? = ` ? r be a rule of R. We write
? ??,wM ? if there exists a substitution ? : X ? T?
such that
? ? = ?[`?]p,
? ? = ?[r?]p, and
? w = wt(?) ?
?
x?del(?) c?,x(x?),
where p ? posQ(?) is the lexicographically least
Q-labeled position in ?. Figure 4 illustrates a deriva-
tion step.
The XTOPR M computes a weighted tree trans-
formation by applying rewrite steps to the tree q0(t),
where t ? T? is the input tree, until an output
tree u ? T? has been produced. The weight of a
particular derivation is obtained by multiplying the
weights of the rewrite steps. The weight of the trans-
formation from t to u is obtained by summing all
weights of the derivations from q0(t) to u. For-
mally1, the weighted tree transformation computed
by M in state q ? Q is
? qM (t, u) =
?
n?N,?1,...,?n?R
q(t)?
?1,w1
M ????
?n,wn
M u
w1 ? . . . ? wn (1)
for every t ? T? and u ? T?. The XTOPR M
computes the weighted tree transformation ? q0M . Two
XTOPR M and N are equivalent, if ?M = ?N .
The sum (1) can be infinite, which we avoid by
simply requiring that all our XTOPR are produc-
ing, which means that r /? Q[X] for every rule
` ? r ? R.2 In a producing XTOPR each rule ap-
plication produces at least one output symbol, which
limits the number n of rule applications to the size of
the output tree u. A detailed exposition to XTOPR is
presented by Arnold and Dauchet (1982) and Graehl
et al (2009) for the unweighted case and by F?l?p
and Vogler (2009) for the weighted case.
Example 1. LetMex = (Q,?,?, q, R, c,wt) be the
nondeleting XTOP with
? Q = {q},
? ? = {?, ?, ?},
? the two rules
q(?)? ? (?)
q(?(x1))? ?(q(x1), q(x1)) (??)
? trivial look-ahead (i.e., c(?, x) = 1??), and
? wt(?) = 2 and wt(??) = 1.
The XTOPR Mex computes the tree transformation
that turns the input tree ?n(?) into the fully balanced
binary tree u of the same height with weight 2(2
n).
An example derivation is presented in Figure 5.
Unrestricted copying (as in Example 1) yields
very undesirable phenomena and is most likely not
needed in the machine translation task. In fact, it
is almost universally agreed that a translation model
should be ?linear-size increase?, which means that
1There is an additional restriction that is discussed in the
next paragraph.
2This is a convenience requirement. We can use other con-
ditions on the XTOPR or the used weight structures to guarantee
a well-defined semantics.
266
q?
?
?
??
?,1
Mex
?
q
?
?
q
?
?
??
?,1
Mex
?
?
q
?
q
?
q
?
?
??
?,1
Mex
?
?
q
?
q
?
?
q
?
q
?
??,2Mex
?
?
? q
?
?
q
?
q
?
??,2Mex ? ? ? ?
?,2
Mex
?
?
? ?
?
? ?
Figure 5: Example derivation using the XTOP Mex with weight 13 ? 24 = 16.
the size of each output tree should be linearly
bounded in the size of the corresponding input tree
according to Aho and Ullman (1971) and Engelfriet
and Maneth (2003).
Definition 2. A mapping ? : T? ? T? ? R+ is
linear-size increase if there exists an integer n ? N
such that |u| ? n ? |t| for all (t, u) ? supp(?).
An XTOPR M is sensible if ?M is linear-size in-
crease.
?Sensible? is not a syntactic property of an
XTOPR as it does not depend on the actual rules,
but only on its computed weighted tree transforma-
tion. The XTOP Mex of Example 1 is not sensible
because |u| = 2|t| ? 1 for every (t, u) ? ?Mex . In-
tuitively, the number of times that Mex can use the
copying rule ?? is not uniformly bounded.
We need an auxiliary result in the main part.
Let ? : T? ? T? ? R+ be a weighted tree
transformation. We need the weighted tree lan-
guage ??1(u) : T? ? R+ of input trees weighted
by their translation weight to a given output
tree u ? T?. Formally,
(
??1(u)
)
(t) = ?(t, u) for
every t ? T?.
Theorem 3. For every producing XTOPR M and
output tree u? ? T?, the weighted tree lan-
guage ??1M (u
?) is regular.
Proof sketch. We use some properties that are only
defined in the next sections (for proof economy). It
is recommended to skip this proof on the first read-
ing and revisit it later. Maletti (2010a) shows that
we can construct an XTOPR M ? such that
?M ?(t, u) =
{
?M (t, u) if u? = u
0 otherwise
for every t ? T? and u ? T?. This operation is
called ?output product? by Maletti (2010a). The ob-
tained XTOPR M ? is also producing, so we know
that M ? can take at most |u?| rewrite steps to de-
rive u?. Since M ? can only produce the output
tree u?, this also limits the total number of rule appli-
cations in any successful derivation. Consequently,
M ? can only apply a copying rule at most |u?| times,
which shows that M ? is finitely copying (see Def-
inition 8). By Theorem 11 we can implement M ?
by an equivalent MBOT M ?? (i.e., ?M ?? = ?M ? ;
see Section 5), for which we know by Theorem 14
of Maletti (2011a) that ??1M ??(u) = ?
?1
M ? (u) is regu-
lar.
Finally, let us illustrate the overall structure of our
arguments to show that every sensible XTOPR can
be implemented by an equivalent MBOT. We first
normalize the given XTOPR such that the seman-
tic property ?sensible? yields a syntactic property
called ?finitely copying? (see Section 4). In a second
step, we show that each finitely copying XTOPR can
be implemented by an equivalent MBOT (see Sec-
tion 5). Figure 6 illustrates these steps towards our
main result. In the final section, we derive some con-
sequences from our main result (see Section 6).
4 From sensible to finite copying
First, we adjust a normal form of Engelfriet and
Maneth (2003) to our needs. This section bor-
rows heavily from Aho and Ullman (1971) and En-
gelfriet and Maneth (2003), where ?sensible?
(unweighted) deterministic macro tree transduc-
ers (MAC) [see Engelfriet and Vogler (1985)] are
considered. Our setting is simpler on the one hand
because XTOPR do not have context parameters
as MAC, but more difficult on the other hand be-
cause we consider nondeterministic and weighted
transducers.
Intuitively, a sensible XTOPR cannot copy a lot
since the size of each output tree is linearly bounded
in the size of the corresponding input tree. However,
the actual presentation of the XTOPR M might con-
267
sensible XTOPR
sensible proper XTOPR
finitely copying XTOPR
linear and nondeleting MBOT
Figure 6: Overview of the proof steps.
tain rules that allow unbounded copying. This un-
bounded copying might not manifest due to the look-
ahead restrictions or due to the fact that those rules
cannot be used in a successful derivation. The pur-
pose of the normal form is the elimination of those
artifacts. To this end, we eliminate all states (except
the initial state) that can only produce finitely many
outputs. Such a state can simply be replaced by one
of the output trees that it can produce and an ad-
ditional look-ahead that checks whether the current
input tree indeed allows that translation (and inserts
the correct translation weight).
Normalized XTOPR are called ?proper?, and we
define this property next. For the rest of this section,
let M = (Q,?,?, q0, R, c,wt) be the considered
sensible XTOPR. Without loss of generality, we as-
sume that the state q0 does not occur in the right-
hand sides of rules. Moreover, we write ? ??M ? if
there exist nonzero weights w1, . . . , wn ? R+ \ {0}
and rules ?1, . . . , ?n ? R with
? ??1,w1M ? ? ? ?
?n,wn
M ? .
In essence, ? ??M ? means that M can transform ?
into ? (in the unweighted setting).
Definition 4. A state q ? Q is proper if there are in-
finitely many u? ? T? such that there exists a deriva-
tion
q0(t)?
?
M ?[q(s)]p ?
?
M u[u
?]p
where s, t ? T? are input trees, ? ? T?(Q[T?]),
p ? pos(?), and u ? T? is an output tree.
The derivation in Definition 4 is illustrated in Fig-
ure 7. In other words, a proper state is reachable
from the initial state and can transform infinitely
many input trees into infinitely many output trees.
The latter is an immediate consequence of Defini-
tion 4 since each input tree can be transformed into
only finitely many output trees due to sensibility.
The restriction includes the look-ahead (because we
require that the rewrite step weights are nonzero),
which might further restrict the input trees.
Example 5. The state q of the XTOP Mex is proper
because we already demonstrated that it can trans-
form infinitely many input trees into infinitely many
output trees.
The XTOPR M is proper if all its states except
the initial state q0 are proper. Next, we show that
each XTOPR can be transformed into an equivalent
proper XTOPR using a simplified version of the con-
struction of Lemma 5.4 by Engelfriet and Maneth
(2003). Mind that we generally assume that all con-
sidered XTOPR are producing.
Theorem 6. For every XTOPR there exists an equiv-
alent proper XTOPR.
Proof sketch. The construction is iterative. Sup-
pose that M is not yet proper. Then there exists
a state q ? Q, which can produce only finitely
many outputs U . It can be decided whether a state
is proper using Theorem 4.5 of Drewes and Engel-
friet (1998), and in case it is proper, the set U can
also be computed effectively. The cited theorem ap-
plies to unweighted XTOPR, but it can be applied
also in our setting because??M in Definition 4 dis-
regards weights. Now we consider each u ? U in-
dividually. Clearly, (? qM )
?1(u) is regular by The-
orem 3. For each u and each occurrence of q in
the right-hand side of a rule ? ? R of M , we cre-
ate a copy ?? of ?, in which the selected occur-
rence of q(x) is replaced by u and the new look-
ahead is c(??, x) = c(?, x) ? (? qM )
?1(u), which re-
stricts the input tree appropriately and includes the
adjustment of the weights. Since regular weighted
tree languages are closed under HADAMARD prod-
ucts [see F?l?p and Vogler (2009)], the look-ahead
c(?, x) ? (? qM )
?1(u) is again regular.
Essentially, we precompute the action of q as
much as possible, and immediately output one of
the finitely many output trees, check that the input
tree has the required shape using the look-ahead,
and charge the weight for the precomputed trans-
formation again using the look-ahead. This pro-
cess is done for each occurrence, so if a rule con-
tains two occurrences of q, then the process must be
268
q0
t ??M
...
...
q
s
??M
...
...
u?
Figure 7: Illustration of the derivation in Definition 4.
done twice to this rule. In this way, we eventually
purge all occurrences of q from the right-hand sides
of rules of M without changing the computed trans-
formation. Since q 6= q0 and q is now unreachable,
it is useless and can be deleted, which removes one
non-proper state. This process is repeated until all
states except the initial state q0 are proper.
Clearly, the construction of Theorem 6 applied
to a sensible XTOPR M yields a sensible proper
XTOPR M ? since the property ?sensible? refers to
the computed transformation and ?M = ?M ? . Let us
illustrate the construction on a small example.
Example 7. Let ? be the rule displayed in Figure 3,
and let us assume that the state qVB is not proper.
Moreover, suppose that qVB can yield the output
tree u and that we already computed the translation
options that yield u. Let t1, . . . , tn ? T? be those
translation options. Then we create the copy ??
q0(S(x1,VP(x2, x3)))? S(u, qNP(x1), qNP(x3))
of the rule ? with look-ahead c?(??, x) such that
c???,x(t) =
{
c?,x(t) if x 6= x2
? qVBM (t, u) if x = x2 .
In general, there can be infinitely many input
trees ti that translate to a selected output tree u, so
we cannot simply replace the variable in the left-
hand side by all the options for the input tree. This
is the reason why we use the look-ahead because the
set ??1M (u) is a regular weighted tree language.
From now on, we assume that the XTOPR M is
proper. Next, we want to invoke Theorem 7.1 of En-
gelfriet and Maneth (2003) to show that a proper
sensible XTOPR is finitely copying. Engelfriet and
Maneth (2003) present a formal definition of finite
copying, but we only present a high-level descrip-
tion of it.
Definition 8. The XTOPR M is finitely copying if
there is a copying bound n ? N such that no input
subtree is copied more than n times in any derivation
q(t)??M u with q ? Q, t ? T?, and u ? T?.
Example 9. The XTOP of Example 1 is not finitely
copying as the input subtree ? is copied 2n times if
the input tree is ?n(?). Clearly, this shows that there
is no uniform bound on the number of copies.
It is worth noting that the properties ?sensible? and
?finitely copying? are essentially unweighted prop-
erties. They largely disregard the weights and a
weighted XTOPR does have one of those properties
if and only if its associated unweighted XTOPR has
it. We now use this tight connection to lift Theo-
rem 7.1 of Engelfriet and Maneth (2003) from the
unweighted (and deterministic) case to the weighted
(and nondeterministic) case.
Theorem 10. If a proper XTOPR is sensible, then it
is finitely copying.
Proof. Let M be the input XTOPR. Since M is sen-
sible, its associated unweighted XTOPR N , which
is obtained by setting all weights to 1 and comput-
ing in the BOOLEAN semiring, is sensible. Conse-
quently,N is finitely copying by Theorem 7.1 of En-
gelfriet and Maneth (2003). Thus, also M is finitely
copying, which concludes the proof. We remark
that Theorem 7.1 of Engelfriet and Maneth (2003)
only applies to deterministic XTOPR, but the essen-
tial pumping argument, which is Lemma 6.2 of En-
gelfriet and Maneth (2003) also works for nonde-
terministic XTOPR. Essentially, the pumping argu-
ment shows the contraposition. If M is not finitely
copying, then M can copy a certain subtree an arbi-
trarily often. Due to the properness of M , all these
copies have an impact on the output tree, which
yields that its size grows beyond any uniform lin-
ear bound, which in turn demonstrates that M is not
sensible.
269
We showed that each sensible XTOPR can be im-
plemented by a finitely copying XTOPR via the con-
struction of the proper normal form. This approach
actually yields a characterization because finitely
copying XTOPR are trivially sensible by Theo-
rem 4.19 of Engelfriet and Maneth (2003).
5 From finite copying to an MBOT
We complete the argument by showing how to im-
plement a finitely copying XTOPR by a weighted
multi bottom-up tree transducer (MBOT). First, we
recall the MBOT, which was introduced by Arnold
and Dauchet (1982) and Lilin (1978) in the un-
weighted case. Engelfriet et al (2009) give an En-
glish presentation. We present the linear and non-
deleting MBOT of Engelfriet et al (2009).
A weighted multi bottom-up tree transducer is a
system M = (Q,?,?, F,R,wt) with
? an alphabet Q of states,
? alphabets ? and ? of input and output symbols,
? a set F ? Q of final states,
? a finite set R of rules of the form ` ? r where
` ? T?(Q(X)) and r ? Q(T?(X)) are linear
and var(`) = var(r), and
? wt: R? R+ assigning rule weights.
We now use T?(Q(X)) and Q(T?(X)) instead of
T?(Q[X]) and Q[T?(X)], which highlights the dif-
ference between XTOPR and MBOT. First, MBOT
are a bottom-up device, which yields that ? and ?
as well as ` and r exchange their place. More impor-
tantly, MBOT can use states with more than 1 suc-
cessor (e.g, Q(X) instead of Q[X]). An example
rule is displayed in Figure 8.
Let M = (Q,?,?, F,R,wt) be an MBOT such
thatQ?(???) = ?.3 We require that r /? Q(X) for
each rule ` ? r ? R to guarantee finite derivations
and thus a well-defined semantics.4 As before, we
present a rewrite semantics. Let ?, ? ? T?(Q(T?)),
and let ? = ` ? r be a rule. We write ? ??M ?
if there exists a substitution ? : X ? T? such that
? = ?[`?]p and ? = ?[r?]p, where p ? pos(?) be is
the lexicographically least reducible position in ?. A
rewrite step is illustrated in Figure 8.
3This restriction can always be achieved by renaming the
states.
4Again this could have been achieved with the help of other
conditions on the MBOT or the used weight structure.
The weighted tree transformation computed byM
in state q ? Q is
? qM (t, u1 ? ? ?uk) =
?
n?N,?1,...,?n?R
t?
?1
M ????
?n
M q(u1,...,uk)
wt(?1) ? . . . ? wt(?n)
for all t ? T? and u1, . . . , uk ? T?. The semantics
of M is ?M (t, u) =
?
q?F ?
q
M (t, u) for all t ? T?
and u ? T?.
We move to the last step for our main result,
in which we show how to implement each finitely
copying XTOPR by an MBOT using a weighted ver-
sion of the construction in Lemma 15 of Maletti
(2008). The computational benefits (binarization,
composition, efficient parsing, etc.) of MBOT over
XTOPR are described by Maletti (2011a).
Theorem 11. Every finitely copying XTOPR can be
implemented by an MBOT.
Proof sketch. We plan to utilize Theorem 18 of En-
gelfriet et al (2009), which proves the same state-
ment in the unweighted and deterministic case.
Again, the weights are not problematic, but we need
to remove the nondeterminism before we can apply
it. This is achieved by a decomposition into two
XTOPR. The first XTOPR annotates the input tree
with the rules that the second XTOPR is supposed to
use. Thus, the first XTOPR remains nondeterminis-
tic, but the second XTOPR, which simply executes
the annotated rules, is now deterministic. This stan-
dard approach due to Engelfriet (1975) is used in
many similar constructions.
Suppose that n is a copying bound for the input
XTOPR M , which means that no more than n rules
are applied to each input symbol. The first XTOPR
is actually a nondeterministic linear and nondeleting
XTOP that annotates each input tree symbol with ex-
actly n rules of M that are consistent with the state
behavior of M . Moreover, the annotation also pre-
scribes with which of n rules the processing should
continue at each subtree. Since we know all the rules
that will potentially be applied for a certain symbol,
we can make the assignment such that no annotated
rule is used twice in the same derivation. The de-
tails for this construction can be found in Lemma 15
of Maletti (2008).
In this way, we obtain a weighted linear and non-
deleting XTOP M1, which includes the look-ahead,
270
SqNP
x1
VP
qVB
x2
qNP
x3 x4
?
qS
S
x2 x1 x3
x4
t
S
qNP
u1
VP
qVB
u2
qNP
u3 u4
??M
t
qS
S
u2 u1 u3
u4
Figure 8: Example MBOT rule ? [left] and its use in a rewrite step [right].
and an unweighted deterministic XTOP M2. Only
the weight and look-ahead of rules that are actu-
ally executed are applied (e.g., although we anno-
tate n rules at the root symbol, we only execute the
first rule and thus only apply its weight and look-
ahead). The look-ahead of different rules is either
resolved (i.e., pushed to the next rules) or multi-
plied using the HADAMARD product [see F?l?p and
Vogler (2009)], which preserves regularity. This
process is also used by Seemann et al (2012). Now
we can use Theorem 4 of Maletti (2011a) to obtain
an MBOT N1 that is equivalent to M1. Similarly,
we can use Theorem 18 of Engelfriet et al (2009)
to obtain an MBOT N2 that is equivalent to M2.
Since MBOT are closed under composition by The-
orem 23 of Engelfriet et al (2009), we can compose
N1 andN2 to obtain a single MBOTN that is equiv-
alent to M .
Corollary 12. For every sensible producing XTOPR
there exists an equivalent MBOT.
Proof. Theorem 6 shows that there exists an equiva-
lent proper XTOPR, which must be finitely copying
by Theorem 10. This last fact allows us to construct
an equivalent MBOT by Theorem 11.
6 Preservation of regularity
Finally, we present an application of Corollary 12 to
solve an open problem. The translation model is of-
ten used in a backwards manner in a machine trans-
lation system as demonstrated, for example, by May
et al (2010), which means that an output tree is sup-
plied and the corresponding input trees are sought.
This starting output tree is typically the best parse
of the string that we want to translate. However, in-
stead of a single tree, we want to use all parses of
this sentence together with their parse scores. Those
parses form a regular weighted tree language, and
applying them backwards to the translation model
yields another weighted tree language L of corre-
sponding input trees. For an efficient representation
and efficient modification algorithms (such a k-best
extraction) we would like L to be regular. However,
F?l?p et al (2011) demonstrate that the backward
application of a regular weighted tree language to
an XTOPR is not necessarily regular. The counterex-
ample uses a variant of the XTOP of Example 1 and
is thus not sensible. Theorem 14 of Maletti (2011a)
shows that MBOT preserve regularity under back-
ward application.
Corollary 13. Sensible XTOPR preserve regularity
under backward application.
Conclusion
We demonstrated that each sensible XTOPR can be
implemented by an MBOT. The latter formalism of-
fers many computational advantages, so that the au-
thor believes that MBOT should be used instead of
XTOP. We used real number weights, but the author
believes that our results carry over to at least all zero-
sum and zero-divisor free semirings [see Hebisch
and Weinert (1998) and Golan (1999)], which are
semirings such that (i) a+ b = 0 implies a = 0 and
(ii) a ? b = 0 implies 0 ? {a, b}. Whether our results
hold in other semirings (such as the semiring of all
reals where ?1 + 1 = 0) remains an open question.
271
References
Alfred V. Aho and Jeffrey D. Ullman. 1971. Transla-
tions on a context-free grammar. Inform. and Control,
19(5):439?475.
Andr? Arnold and Max Dauchet. 1976. Bi-transductions
de for?ts. In Proc. 3th Int. Coll. Automata, Languages
and Programming, pages 74?86. University of Edin-
burgh.
Andr? Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d?arbres. Theoret. Comput. Sci.,
20(1):33?93.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Comput. Linguist., 33(2):201?228.
Frank Drewes and Joost Engelfriet. 1998. Decidability
of the finiteness of ranges of tree transductions. In-
form. and Comput., 145(1):1?50.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proc. 41st Ann.
Meeting Association for Computational Linguistics,
pages 205?208. Association for Computational Lin-
guistics.
Joost Engelfriet and Sebastian Maneth. 2003. Macro tree
translations of linear size increase are MSO definable.
SIAM J. Comput., 32(4):950?1006.
Joost Engelfriet and Heiko Vogler. 1985. Macro tree
transducers. J. Comput. System Sci., 31(1):71?146.
Joost Engelfriet, Eric Lilin, and Andreas Maletti. 2009.
Extended multi bottom-up tree transducers ? compo-
sition and decomposition. Acta Inform., 46(8):561?
590.
Joost Engelfriet. 1975. Bottom-up and top-down tree
transformations ? a comparison. Math. Systems The-
ory, 9(3):198?231.
Joost Engelfriet. 1977. Top-down tree transducers
with regular look-ahead. Math. Systems Theory,
10(1):289?303.
Zolt?n F?l?p and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Handbook
of Weighted Automata, EATCS Monographs on Theo-
ret. Comput. Sci., chapter 9, pages 313?403. Springer.
Zolt?n F?l?p, Andreas Maletti, and Heiko Vogler. 2011.
Weighted extended tree transducers. Fundam. Inform.,
111(2):163?202.
Ferenc G?cseg and Magnus Steinby. 1984. Tree Au-
tomata. Akad?miai Kiad?, Budapest.
Ferenc G?cseg and Magnus Steinby. 1997. Tree lan-
guages. In Grzegorz Rozenberg and Arto Salomaa,
editors, Handbook of Formal Languages, volume 3,
chapter 1, pages 1?68. Springer.
Jonathan S. Golan. 1999. Semirings and their Applica-
tions. Kluwer Academic, Dordrecht.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Comput. Linguist.,
34(3):391?427.
Jonathan Graehl, Mark Hopkins, Kevin Knight, and An-
dreas Maletti. 2009. The power of extended top-down
tree transducers. SIAM J. Comput., 39(2):410?430.
Udo Hebisch and Hanns J. Weinert. 1998. Semirings?
Algebraic Theory and Applications in Computer Sci-
ence. World Scientific.
Kevin Knight and Jonathan Graehl. 2005. An overview
of probabilistic tree transducers for natural language
processing. In Proc. 6th Int. Conf. Computational Lin-
guistics and Intelligent Text Processing, volume 3406
of LNCS, pages 1?24. Springer.
Kevin Knight. 2007. Capturing practical natu-
ral language transformations. Machine Translation,
21(2):121?133.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Eric Lilin. 1978. Une g?n?ralisation des transduc-
teurs d??tats finis d?arbres: les S-transducteurs. Th?se
3?me cycle, Universit? de Lille.
Andreas Maletti and Daniel Quernheim. 2011. Pushing
for weighted tree automata. In Proc. 36th Int. Symp.
Mathematical Foundations of Computer Science, vol-
ume 6907 of LNCS, pages 460?471. Springer.
Andreas Maletti. 2008. Compositions of extended top-
down tree transducers. Inform. and Comput., 206(9?
10):1187?1196.
Andreas Maletti. 2010a. Input and output products
for weighted extended top-down tree transducers. In
Proc. 14th Int. Conf. Developments in Language The-
ory, volume 6224 of LNCS, pages 316?327. Springer.
Andreas Maletti. 2010b. Why synchronous tree substitu-
tion grammars? In Proc. Human Language Technolo-
gies: Conf. North American Chapter of the ACL, pages
876?884. Association for Computational Linguistics.
Andreas Maletti. 2011a. An alternative to synchronous
tree substitution grammars. J. Natur. Lang. Engrg.,
17(2):221?242.
Andreas Maletti. 2011b. How to train your multi bottom-
up tree transducer. In Proc. 49th Ann. Meeting Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 825?834. Association for
Computational Linguistics.
Jonathan May and Kevin Knight. 2006. Tiburon: A
weighted tree automata toolkit. In Proc. 11th Int.
Conf. Implementation and Application of Automata,
volume 4094 of LNCS, pages 102?113. Springer.
Jonathan May, Kevin Knight, and Heiko Vogler. 2010.
Efficient inference through cascades of weighted tree
transducers. In Proc. 48th Ann. Meeting Association
for Computational Linguistics, pages 1058?1066. As-
sociation for Computational Linguistics.
272
Jonathan May. 2010. Weighted Tree Automata and
Transducers for Syntactic Natural Language Process-
ing. Ph.D. thesis, University of Southern California,
Los Angeles.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. 46th Ann. Meeting Associ-
ation for Computational Linguistics, pages 192?199.
Association for Computational Linguistics.
William C. Rounds. 1970. Mappings and grammars on
trees. Math. Systems Theory, 4(3):257?287.
Nina Seemann, Daniel Quernheim, Fabienne Braune, and
Andreas Maletti. 2012. Preservation of recognizabil-
ity for weighted linear extended top-down tree trans-
ducers. In Proc. 2nd Workshop Applications of Tree
Automata in Natural Language Processing, pages 1?
10. Association for Computational Linguistics.
Stuart M. Shieber. 2004. Synchronous grammars as tree
transducers. In Proc. 7th Int. Workshop Tree Adjoining
Grammars and Related Formalisms, pages 88?95.
Jun Sun, Min Zhang, and Chew Lim Tan. 2009. A
non-contiguous tree sequence alignment-based model
for statistical machine translation. In Proc. 47th Ann.
Meeting Association for Computational Linguistics,
pages 914?922. Association for Computational Lin-
guistics.
James W. Thatcher. 1970. Generalized2 sequential ma-
chine maps. J. Comput. System Sci., 4(4):339?367.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008a. A tree se-
quence alignment-based tree-to-tree translation model.
In Proc. 46th Ann. Meeting Association for Compu-
tational Linguistics, pages 559?567. Association for
Computational Linguistics.
Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, and
Sheng Li. 2008b. Grammar comparison study for
translational equivalence modeling and statistical ma-
chine translation. In Proc. 22nd Int. Conf. Computa-
tional Linguistics, pages 1097?1104. Association for
Computational Linguistics.
273
