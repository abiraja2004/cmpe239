Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 562?567, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
nlp.cs.aueb.gr: Two Stage Sentiment Analysis
Prodromos Malakasiotis, Rafael Michael Karampatsis
Konstantina Makrynioti and John Pavlopoulos
Department of Informatics
Athens University of Economics and Business
Patission 76, GR-104 34 Athens, Greece
Abstract
This paper describes the systems with which
we participated in the task Sentiment Analysis
in Twitter of SEMEVAL 2013 and specifically
the Message Polarity Classification. We used
a 2-stage pipeline approach employing a lin-
ear SVM classifier at each stage and several
features including BOW features, POS based
features and lexicon based features. We have
also experimented with Naive Bayes classi-
fiers trained with BOW features.
1 Introduction
During the last years, Twitter has become a very
popular microblogging service. Millions of users
publish messages every day, often expressing their
feelings or opinion about a variety of events, top-
ics, products, etc. Analysing this kind of content
has drawn the attention of many companies and re-
searchers, as it can lead to useful information for
fields, such as personalized marketing or social pro-
filing. The informal language, the spelling mis-
takes, the slang and special abbreviations that are
frequently used in tweets differentiate them from
traditional texts, such as articles or reviews, and
present new challenges for the task of sentiment
analysis.
The Message Polarity Classification is defined as
the task of deciding whether a message M conveys a
positive, negative or neutral sentiment. For instance
M1 below expresses a positive sentiment, M2 a neg-
ative one, while M3 has no sentiment at all.
M1: GREAT GAME GIRLS!! On to districts Monday
at Fox!! Thanks to the fans for coming out :)
M2: Firework just came on my tv and I just broke down
and sat and cried, I need help okay
M3: Going to a bulls game with Aaliyah & hope next
Thursday
As sentiment analysis in Twitter is a very recent
subject, it is certain that more research and improve-
ments are needed. This paper presents our approach
for the subtask of Message Polarity Classification
(Wilson et al, 2013) of SEMEVAL 2013. We used a
2-stage pipeline approach employing a linear SVM
classifier at each stage and several features includ-
ing bag of words (BOW) features, part-of-speech
(POS) based features and lexicon based features.
We have also experimented with Naive Bayes clas-
sifiers trained with BOW features.
The rest of the paper is organised as follows. Sec-
tion 2 provides a short analysis of the data used
while section 3 describes our approach. Section 4
describes the experiments we performed and the cor-
responding results and section 5 concludes and gives
hints for future work.
2 Data
Before we proceed with our system description we
briefly describe the data released by the organisers.
The training set consists of a set of IDs correspond-
ing to tweet messages, along with their annotations.
A message can be annotated as positive, negative
or neutral. In order to address privacy concerns,
rather than releasing the original Tweets, the organ-
isers chose to provide a python script for download-
ing the data. This resulted to different training sets
for the participants since tweets may often become
562
SEMEVAL STATS TRAIN (ours) TRAIN (official) Dev DEV (final) TEST (sms)
Positive 3280 37,57% 3640 37,59% 524 34,82% 575 34,76% 492 23,50%
Negative 1289 14,77% 1458 15,06% 308 20,47% 340 20,56% 394 18,82%
Neutral 4161 47,66% 4586 47,36% 673 44,72% 739 44,68% 1208 57,69%
TOTAL 8730 9684 1505 1654 2094
37,57% 
14,77% 
47,66% 
Training data class distribution 
Positive
Negative
Neutral
34,76% 
20,56% 
44,68% 
Development data class distribution 
Positive
Negative
Neutral
23,50% 
18,82% 57,69% 
Test data class distribution (sms) 
Positive
Negative
Neutral
41,23% 
15,76% 
43,01% 
Test data class distribution (twitter) 
Positive
Negative
Neutral
(a)
SEMEVAL STATS TRAIN (ours) TRAIN (official) Dev DEV (final) TEST (sms)
Positive 3280 37,57% 3640 37,59% 524 34,82% 575 34,76% 492 23,50%
Negative 1289 14,7 % 1458 15,06% 308 20,47% 340 20,56% 394 18, 2%
Neutral 4161 47,6 % 4586 47,36% 673 4 ,72% 739 4 ,68% 1208 57,69%
TOTAL 8730 9684 1505 1654 2094
37,57% 
14,7 % 
47,6 % 
Training data clas  distribution 
Positive
Negative
Neutral
34,76% 
20,56% 
4 ,68% 
Development data clas  distribution 
Posit ve
Negative
Neutral
23,50% 
18,82% 57,69% 
Test data class distribution (sms) 
Positive
Negative
Neutral
41,23% 
15,76% 
43,01% 
Test data clas  distribution (twit er) 
Posit ve
Negative
Neutral
(b)
Figure 1: Train and Development data class distribution.
unavailable due to a number of reasons. Concerning
the development and test sets the organisers down-
loaded and provided the tweets. 1 A first analysis
of the data indicates that they suffer from a class im-
balance problem. Specifically the training data we
have downloaded contain 8730 tweets (3280 posi-
tive, 1289 negative, 4161 neutral), while the devel-
opment set contains 1654 tweets (575 positive, 340
negative, 739 neutral). Figure 1 illustrates the prob-
lem on train and development sets.
3 System Overview
The system we propose is a 2?stage pipeline pro-
cedure employing SVM classifiers (Vapnik, 1998)
to detect whether each message M expresses pos-
itive, negative or no sentiment (figure 2). Specifi-
cally, during the first stage we attempt to detect if M
expresses a sentiment (positive or negative) or not.
If so, M is called ?subjective?, otherwise it is called
?objective? or ?neutral?.2 Each subjective message
is then classified in a second stage as ?positive? or
?negative?. Such a 2?stage approach has also been
suggested in (Pang and Lee, 2004) to improve sen-
timent classification of reviews by discarding objec-
tive sentences, in (Wilson et al, 2005a) for phrase-
level sentiment analysis, and in (Barbosa and Feng,
2010) for sentiment analysis on Twitter messages.
1A separate test set with SMS messages was also provided
by the organisers to measure performance of systems over other
types of message data. No training and development data were
provided for this set.
2Hereafter we will use the terms ?objective? and ?neutral?
interchangeably.
3.1 Data Preprocessing
Before we could proceed with feature engineering,
we performed several preprocessing steps. To be
more precise, a twitter specific tokeniser and part-
of-speech (POS) tagger (Ritter et al, 2011) were
used to obtain the tokens and the corresponding
POS tags which are necessary for a particular set
of features to be described later. In addition to these,
six lexicons, originating from Wilson?s (2005b) lexi-
con, were created. This lexicon contains expressions
that given a context (i.e., surrounding words) indi-
cate subjectivity. The expression that in most con-
text expresses sentiment is considered to be ?strong?
subjective, otherwise it is considered weak subjec-
tive (i.e., it has specific subjective usages). So, we
first split the lexicon in two smaller, one contain-
ing strong and one containing weak subjective ex-
pressions. Moreover, Wilson also reports the polar-
ity of each expression out of context (prior polarity)
which can be positive, negative or neutral. As a con-
sequence, we further split each of the two lexicons
into three smaller according to the prior polarity of
the expression, resulting to the following six lexi-
cons:
S+ : Contains strong subjective expressions with
positive prior polarity.
S? : Contains strong subjective expressions with
negative prior polarity.
S0 : Contains strong subjective expressions with
neutral prior polarity.
563
 Subjectivity detection 
SVM 
Polarity detection 
SVM 
Objective 
messages 
Messages 
Subjective 
messages 
Positive 
messages 
Negative 
messages 
Figure 2: Our 2?stage pipeline procedure.
W+ : Contains weak subjective expressions with
positive prior polarity.
W? : Contains weak subjective expressions with
negative prior polarity.
W0 : Contains weak subjective expressions with
neutral prior polarity.
Adding to these, three more lexicons were created,
one for each class (positive, negative, neutral). In
particular, we employed Chi Squared feature selec-
tion (Liu and Setiono, 1995) to obtain the 100 most
important tokens per class from the training set.
Very few tokens were manually erased to result to
the following three lexicons.
T+ : Contains the top-94 tokens appearing in posi-
tive tweets of the training set.
T? : Contains the top-96 tokens appearing in nega-
tive tweets of the training set.
T0 : Contains the top-94 tokens appearing in neutral
tweets of the training set.
The nine lexicons described above are used to cal-
culate precision (P (t, c)), recall (R(t, c)) and F ?
measure (F1(t, c)) of tokens appearing in a mes-
sage with respect to each class. Equations 1, 2 and 3
below provide the definitions of these metrics.
P (t, c) =
#tweets that contain token t and belong to class c
#tweets that contain token t
(1)
R(t, c) =
#tweets that contain token t and belong to class c
#tweets that belong to class c
(2)
F1(t, c) =
2 ? P (t, c) ? R(t, c)
P (t, c) + R(t, c)
(3)
3.2 Feature engineering
We employed three types of features, namely
boolean features, POS based features and lexicon
based features. Our goal is to build a system that is
not explicitly based on the vocabulary of the training
set, having therefore better generalisation capability.
3.2.1 Boolean features
Bag of words (BOW): These features indicate the
existence of specific tokens in a message. We
used feature selection with Info Gain to obtain
the 600 most informative tokens of the training
set and we then manually removed 19 of them
564
to result in 581 tokens. As a consequence we
get 581 features that can take a value of 1 if a
message contains the corresponding token and
0 otherwise.
Time and date: We observed that time and date of-
ten indicated events in the train data and such
messages tend to be objective. Therefore, we
added two more features to indicate if a mes-
sage contains time and/or date expressions.
Character repetition: Repetitive characters are of-
ten added to words by users, in order to give
emphasis or to express themselves more in-
tensely. As a consequence they indicate sub-
jectivity. So we added one more feature having
a value of 1 if a message contains words with
repeating characters and 0 otherwise.
Negation: Negation not only is a good subjectivity
indicator but it also may change the polarity of
a message. We therefore add 5 more features,
one indicating the existence of negation, and
the remaining four indicating the existence of
negation that precedes (in a distance of at most
5 tokens) words from lexicons S+, S?, W+ and
W?.
Hash-tags with sentiment: These features are im-
plemented by getting all the possible sub-
strings of the string after the symbol # and
checking if any of them match with any word
from S+, S?, W+ and W? (4 features). A
value of 1 means that a hash-tag containing a
word from the corresponding lexicon exists in
a message.
3.2.2 POS based features
Specific POS tags might be good indicators of
subjectivity or objectivity. For instance adjectives
often express sentiment (e.g., beautiful, frustrating)
while proper nouns are often reported in objective
messaged. We, therefore, added 10 more features
based on the following POS tags:
1. adjectives,
2. adverbs,
3. verbs,
4. nouns,
5. proper nouns,
6. urls,
7. interjections,
8. hash-tags,
9. happy emoticons, and
10. sad emoticons.
We then constructed our features as follows. For
each message we counted the occurrences of tokens
with these POS tags and we divided this number
with the number of tokens having any of these POS
tags. For instance if a message contains 2 adjectives,
1 adverb and 1 url then the features corresponding to
adjectives, adverbs and urls will have a value of 24 ,
1
4
and 14 respectively while all the remaining features
will be 0. These features can be thought of as a way
to express how much specific POS tags affect the
sentiment of a message.
Going a step further we calculate precision
(P (b, c)), recall (R(b, c)) and F ? measure
(F1(b, c)) of POS tags bigrams with respect to each
class (equations 4, 5 and 6 respectively).
P (b, c) =
#tweets that contain bigram b and belong to class c
#tweets that contain bigram b
(4)
R(b, c) =
#tweets that contain bigram b and belong to class c
#tweets that belong to class c
(5)
F1(b, c) =
2 ? P (b, c) ? R(b, c)
P (b, c) + R(b, c)
(6)
For each bigram (e.g., adjective-noun) in a mes-
sage we calculate F1(b, c) and then we use the aver-
age, the maximum and the minimum of these values
to create 9 additional features. We did not experi-
ment over measures that weight differently Precision
and Recall (e.g., Fb for b = 0.5) or with different
combinations (e.g., F1 and P ).
3.2.3 Lexicon based features
This set of features associates the words of the
lexicons described earlier with the three classes.
Given a message M , similarly to the equations 4 and
565
6 above, we calculate P (t, c) and F1(t, c) for every
token t ? M with respect to a lexicon. We then ob-
tain the maximum, minimum and average values of
P (t, c) and F1(t, c) in M . We note that the combi-
nation of P and F1 appeared to be the best in our
experiments while R(t, c) was not helpful and thus
was not used. Also, similarly to section 3.2.2 we
did not experiment over measures that weight differ-
ently Precision and Recall (e.g., Fb for b = 0.5). The
former metrics are calculated with three variations:
(a) Using words: The values of the metrics con-
sider only the words of the message.
(b) Using words and priors: The same as (a) but
adding to the calculated metrics a prior value.
This value is calculated on the entire lexicon,
and roughly speaking it is an indicator of how
much we can trust L to predict class c. In cases
that a token t of a message M does not appear
in a lexicon L the corresponding scores of the
metrics will be 0.
(c) Using words and their POS tags: The values
of the metrics consider the words of the message
along with their POS tags.
(d) Using words, their POS tags and priors: The
same as (c) but adding to the calculated metrics
an apriori value. The apriori value is calculated
in a similar manner as in (b) with the difference
that we consider the POS tags of the words as
well.
For case (a) we calculated minimum, maximum
and average values of P (t, c) and F1(t, c) with re-
spect to S+, S?, S0, W+, W? and W0 consider-
ing only the words of the message resulting to 108
features. Concerning case (b) we calculated average
P (t, c) and F1(t, c) with respect to S+, S?, S0, W+,
W? and W0, and average P (t, c) with respect to T+,
T? and T0 adding 45 more features. For case (c) we
calculated minimum, maximum and average P (t, c)
with respect to S+, S?, S0, W+, W? and W0 (54
features), and, finally, for case (d) we calculated av-
erage P (t, c) and F1(t, c) with respect to S+, S?,
S0, W+, W? and W0 to add 36 features.
Class F1
Positive 0.6496
Negative 0.4429
Neutral 0.7022
Average 0.5462
Table 1: F1 for development set.
4 Experiments
As stated earlier we use a 2?stage pipeline approach
to identify the sentiment of a message. Preliminary
experiments on the development data showed that
this approach is better than attempting to address the
problem in one stage during which a classifier must
classify a message as positive, negative or neutral.
To be more precise we used a Naive Bayes classifier
and BOW features using both 1?stage and 2?stage
approaches. Although we considered the 2?stage
approach with a Naive Bayes classifier as a baseline
system we used it to submit results for both twitter
and sms test sets.
Having concluded to the 2?stage approach we
employed for each stage an SVM classifier, fed with
the 855 features described in section 3.2.3 Both
SVMs use linear kernel and are tuned in order to
find the optimum C parameter. Observe that we use
the same set of features in both stages and let the
classifier learn the appropriate weights for each fea-
ture. During the first stage, the classifier is trained
on the entire training set after merging positive and
negative classes to one superclass, namely subjec-
tive. In the second stage, the classifier is trained only
on positive and negative tweets of the training and
is asked to determine whether the messages classi-
fied as subjective during the first stage are positive
or negative.
4.1 Results
In order to obtain the best set of features we trained
our system on the downloaded training data and
measured its performance on the provided develop-
ment data. Table 1 illustrates the F1 results on the
development set. A first observation is that there
is a considerable difference between the F1 of the
negative class and the other two, with the former be-
3We used the LIBLINEAR distribution (Fan et al, 2008)
566
Class F1
Positive 0.6854
Negative 0.4929
Neutral 0.7117
Average 0.5891
Table 2: F1 for twitter test set.
Class F1
Positive 0.6349
Negative 0.5131
Neutral 0.7785
Average 0.5740
Table 3: F1 for sms test set.
ing significantly decreased. This might be due to
the quite low number of negative tweets of the ini-
tial training set in comparison with the rest of the
classes. Therefore, the addition of 340 negative ex-
amples from the development set emerged from this
imbalance and proved to be effective as shown in ta-
ble 2 illustrating our results on the test set regarding
tweets. Unfortunately we were not able to submit
results with this system for the sms test set. How-
ever, we performed post-experiments after the gold
sms test set was released. The results shown on table
3 are similar to the ones obtained for the twitter test
set which means that our model has a good general-
isation ability.
5 Conclusion and future work
In this paper we presented our approach for the
Message Polarity Classification task of SEMEVAL
2013. We proposed a pipeline approach to detect
sentiment in two stages; first we discard objective
messages and then we classify subjective (i.e., car-
rying sentiment) ones as positive or negative. We
used SVMs with various extracted features for both
stages and although the system performed reason-
ably well, there is still much room for improvement.
A first problem that should be addressed is the dif-
ficulty in identifying negative messages. This was
mainly due to small number of tweets in the train-
ing data. This was somewhat alleviated by adding
the negative instances of the development data but
still our system reports lower results for this class as
compared to positive and neutral classes. More data
or better features is a possible improvement. An-
other issue that has not an obvious answer is how to
proceed in order to improve the 2?stage pipeline ap-
proach. Should we try and optimise each stage sepa-
rately or should we optimise the second stage taking
into consideration the results of the first stage?
References
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, COLING ?10,
pages 36?44, Beijing, China. Association for Compu-
tational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. The Journal of Machine
Learning Research, 9:1871?1874.
Huan Liu and Rudy Setiono. 1995. Chi2: Feature se-
lection and discretization of numeric attributes. In
Tools with Artificial Intelligence, 1995. Proceedings.,
Seventh International Conference on, pages 388?391.
IEEE.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, ACL ?04, Barcelona, Spain. As-
sociation for Computational Linguistics.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An experi-
mental study. In EMNLP, pages 1524?1534.
V. Vapnik. 1998. Statistical learning theory. John Wiley.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005a. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the confer-
ence on Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ?05,
pages 347?354, Vancouver, British Columbia, Canada.
Association for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on Human Language Technology and Empir-
ical Methods in Natural Language Processing, pages
347?354. Association for Computational Linguistics.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ?13, June.
567
