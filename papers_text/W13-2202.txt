Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 45?51,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Results of the WMT13 Metrics Shared Task
Matous? Macha?c?ek and Ondr?ej Bojar
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
machacekmatous@gmail.com and bojar@ufal.mff.cuni.cz
Abstract
This paper presents the results of the
WMT13 Metrics Shared Task. We asked
participants of this task to score the
outputs of the MT systems involved in
WMT13 Shared Translation Task. We
collected scores of 16 metrics from 8 re-
search groups. In addition to that we com-
puted scores of 5 standard metrics such as
BLEU, WER, PER as baselines. Collected
scores were evaluated in terms of system
level correlation (how well each metric?s
scores correlate with WMT13 official hu-
man scores) and in terms of segment level
correlation (how often a metric agrees with
humans in comparing two translations of a
particular sentence).
1 Introduction
Automatic machine translation metrics play a very
important role in the development of MT systems
and their evaluation. There are many different
metrics of diverse nature and one would like to
assess their quality. For this reason, the Metrics
Shared Task is held annually at the Workshop of
Statistical Machine Translation (Callison-Burch et
al., 2012). This year, the Metrics Task was run
by different organizers but the only visible change
is hopefully that the results of the task are pre-
sented in a separate paper instead of the main
WMT overview paper.
In this task, we asked metrics developers to
score the outputs of WMT13 Shared Translation
Task (Bojar et al, 2013). We have collected the
computed metrics? scores and use them to evalu-
ate quality of the metrics.
The systems? outputs, human judgements and
evaluated metrics are described in Section 2. The
quality of the metrics in terms of system level cor-
relation is reported in Section 3. Segment level
correlation is reported in Section 4.
2 Data
We used the translations of MT systems involved
in WMT13 Shared Translation Task together with
reference translations as the test set for the Metrics
Task. This dataset consists of 135 systems? out-
puts and 6 reference translations in 10 translation
directions (5 into English and 5 out of English).
Each system?s output and the reference translation
contain 3000 sentences. For more details please
see the WMT13 main overview paper (Bojar et al,
2013).
2.1 Manual MT Quality Judgements
During the WMT13 Translation Task a large scale
manual annotation was conducted to compare the
systems. We used these collected human judge-
ments for evaluating the automatic metrics.
The participants in the manual annotation were
asked to evaluate system outputs by ranking trans-
lated sentences relative to each other. For each
source segment that was included in the procedure,
the annotator was shown the outputs of five sys-
tems to which he or she was supposed to assign
ranks. Ties were allowed. Only sentences with 30
or less words were ranked by humans.
These collected rank labels were then used to
assign each system a score that reflects how high
that system was usually ranked by the annotators.
Please see the WMT13 main overview paper for
details on how this score is computed. You can
also find inter- and intra-annotator agreement esti-
mates there.
2.2 Participants of the Shared Task
Table 1 lists the participants of WMT13 Shared
Metrics Task, along with their metrics. We have
collected 16 metrics from a total of 8 research
groups.
In addition to that we have computed the fol-
lowing two groups of standard metrics as base-
lines:
45
Metrics Participant
METEOR Carnegie Mellon University (Denkowski and Lavie, 2011)
LEPOR, NLEPOR University of Macau (Han et al, 2013)
ACTA, ACTA5+6 Idiap Research Institute (Hajlaoui, 2013) (Hajlaoui and Popescu-Belis, 2013)
DEPREF-{ALIGN,EXACT} Dublin City University (Wu et al, 2013)
SIMPBLEU-{RECALL,PREC} University of Shefield (Song et al, 2013)
MEANT, UMEANT Hong Kong University of Science and Technology (Lo and Wu, 2013)
TERRORCAT German Research Center for Artificial Intelligence (Fishel, 2013)
LOGREGFSS, LOGREGNORM DFKI (Avramidis and Popovic?, 2013)
Table 1: Participants of WMT13 Metrics Shared Task
? Moses Scorer. Metrics BLEU (Papineni et
al., 2002), TER (Snover et al, 2006), WER,
PER and CDER (Leusch et al, 2006) were
computed using the Moses scorer which is
used in Moses model optimization. To tok-
enize the sentences we used the standard tok-
enizer script as available in Moses Toolkit. In
this paper we use the suffix *-MOSES to label
these metrics.
? Mteval. Metrics BLEU (Papineni et
al., 2002) and NIST (Doddington,
2002) were computed using the script
mteval-v13a.pl 1 which is used in
OpenMT Evaluation Campaign and includes
its own tokenization. We use *-MTEVAL
suffix to label these metrics. By default,
mteval assumes the text is in ASCII,
causing poor tokenization around curly
quotes. We run mteval in both the
default setting as well as with the flag
--international-tokenization
(marked *-INTL).
We have normalized all metrics? scores such
that better translations get higher scores.
3 System-Level Metric Analysis
We measured the quality of system-level metrics?
scores using the Spearman?s rank correlation coef-
ficient ?. For each direction of translation we con-
verted the official human scores into ranks. For
each metric, we converted the metric?s scores of
systems in a given direction into ranks. Since there
were no ties in the rankings, we used the simplified
formula to compute the Spearman?s ?:
? = 1? 6
?
d2i
n(n2 ? 1) (1)
1http://www.itl.nist.gov/iad/mig/
/tools/
where di is the difference between the human rank
and metric?s rank for system i and n is number of
systems. The possible values of ? range between
1 (where all systems are ranked in the same order)
and -1 (where the systems are ranked in the re-
verse order). A good metric produces rankings of
systems similar to human rankings. Since we have
normalized all metrics such that better translations
get higher score we consider metrics with values
of Spearman?s ? closer to 1 as better.
We also computed empirical confidences of
Spearman?s ? using bootstrap resampling. Since
we did not have direct access to participants? met-
rics (we received only metrics? scores for the com-
plete test sets without the ability to run them on
new sampled test sets), we varied the ?golden
truth? by sampling from human judgments. We
have bootstrapped 1000 new sets and used 95 %
confidence level to compute confidence intervals.
The Spearman?s ? correlation coefficient is
sometimes too harsh: If a metric disagrees with
humans in ranking two systems of a very similar
quality, the ? coefficient penalizes this equally as
if the systems were very distant in their quality.
Aware of how uncertain the golden ranks are in
general, we do not find the method very fair. We
thus also computed three following correlation co-
efficients besides the Spearman?s ?:
? Pearson?s correlation coefficient. This co-
efficient measures the strength of the linear
relationship between metric?s scores and hu-
man scores. In fact, Spearman?s ? is Pear-
son?s correlation coefficient applied to ranks.
? Correlation with systems? clusters. In the
Translation Task (Bojar et al, 2013), the
manual scores are also presented as clus-
ters of systems that can no longer be signifi-
cantly distinguished from one another given
the available judgements. (Please see the
WMT13 Overview paper for more details).
46
We take this cluster information as a ?rank
with ties? for each system and calculate its
Pearson?s correlation coefficient with each
metric?s scores.
? Correlation with systems? fuzzy ranks. For
a given system the fuzzy rank is computed
as an average of ranks of all systems which
are not significantly better or worse than the
given system. The Pearson?s correlation co-
efficient of a metric?s scores and systems?
fuzzy ranks is then computed.
You can find the system-level correlations for
translations into English in Table 2 and for transla-
tions out of English in Table 3. Each row in the ta-
bles contains correlations of a metric in each of the
examined translation directions. The metrics are
sorted by average Spearman?s ? correlation across
translation directions. The best results in each di-
rection are in bold.
As in previous years, a lot of metrics outper-
formed BLEU in system level correlation. The
metric which has on average the strongest corre-
lation in directions into English is METEOR. For
the out of English direction, SIMPBLEU-RECALL
has the highest system-level correlation. TER-
RORCAT achieved even a higher average corre-
lation but it did not participate in all language
pairs. The implementation of BLEU in mteval
is slightly better than the one in Moses scorer
(BLEU-MOSES). This confirms the known truth
that tokenization and other minor implementation
details can considerably influence a metric perfor-
mance.
4 Segment-Level Metric Analysis
We measured the quality of metrics? segment-
level scores using Kendall?s ? rank correlation
coefficient. For this we did not use the official
WMT13 human scores but we worked with raw
human judgements: For each translation direction
we extracted all pairwise comparisons where one
system?s translation of a particular segment was
judged to be (strictly) better than the other sys-
tem?s translation. Formally, this is a list of pairs
(a, b) where a segment translation a was ranked
better than translation b:
Pairs := {(a, b) | r(a) < r(b)} (2)
where r(?) is human rank. For a given metricm(?),
we then counted all concordant pairwise compar-
isons and all discordant pairwise comparisons. A
concordant pair is a pair of two translations of
the same segment in which the comparison of hu-
man ranks agree with the comparison of the met-
ric?s scores. A discordant pair is a pair in which
the comparison of human ranks disagrees with the
metric?s comparison. Note that we totally ignore
pairs where human ranks or metric?s scores are
tied. Formally:
Con := {(a, b) ? Pairs | m(a) > m(b)} (3)
Dis := {(a, b) ? Pairs | m(a) < m(b)} (4)
Finally the Kendall?s ? is computed using the fol-
lowing formula:
? = |Con| ? |Dis||Con|+ |Dis| (5)
The possible values of ? range between -1 (a met-
ric always predicted a different order than humans
did) and 1 (a metric always predicted the same or-
der as humans). Metrics with higher ? are better.
The final Kendall?s ?s are shown in Table 4
for directions into English and in Table 5 for di-
rections out of English. Each row in the tables
contains correlations of a metric in given direc-
tions. The metrics are sorted by average corre-
lation across the translation directions. Metrics
which did not compute scores for systems in all
directions are at the bottom of the tables.
You can see that in both categories, into and out
of English, the strongest correlated segment-level
metric is SIMPBLEU-RECALL.
4.1 Details on Kendall?s ?
The computation of Kendall?s ? has slightly
changed this year. In WMT12 Metrics Task
(Callison-Burch et al, 2012), the concordant pairs
were defined exactly as we do (Equation 3) but the
discordant pairs were defined differently: pairs in
which one system was ranked better by the human
annotator but in which the metric predicted a tie
were considered also as discordant:
Dis := {(a, b) ? Pairs | m(a) ? m(b)} (6)
We feel that for two translations a and b of a seg-
ment, where a is ranked better by humans, a metric
which produces equal scores for both translations
should not be penalized as much as a metric which
47
Co
rre
lat
ion
coe
ffic
ien
t
Sp
ear
ma
n?s
?
Co
rre
lat
ion
Co
effi
cie
nt
Pe
ars
on
?s
Cl
ust
ers
Fu
zzy
Ra
nk
s
Di
rec
tio
ns
fr-
en
de
-en
es-
en
cs-
en
ru
-en
Av
era
ge
Av
era
ge
Av
era
ge
Av
era
ge
Co
nsi
de
red
sys
tem
s
12
22
11
10
17
M
ET
EO
R
.9
84
?
.0
14
.9
61
?
.0
20
.97
9?
.0
24
.9
64
?
.0
27
.7
89
?
.0
40
.93
5?
.0
12
.95
0
.92
4
.93
6
DE
PR
EF
-A
LI
GN
.99
5?
.0
11
.96
6?
.0
18
.9
65
?
.0
31
.9
64
?
.0
23
.7
68
?
.0
41
.9
31
?
.0
12
.92
6
.90
9
.92
4
UM
EA
NT
.9
89
?
.0
11
.9
46
?
.0
18
.9
58
?
.0
28
.97
3?
.0
32
.7
75
?
.0
37
.9
28
?
.0
12
.90
9
.90
3
o.
93
0
M
EA
NT
.9
73
?
.0
14
.9
26
?
.0
21
.9
44
?
.0
38
.97
3?
.0
32
.7
65
?
.0
38
.9
16
?
.0
13
.90
1
.89
1
.91
8
SE
M
PO
S
.9
38
?
.0
14
.9
19
?
.0
28
.9
30
?
.0
31
.9
55
?
.0
18
.82
3?
.0
37
.9
13
?
.0
12
o.
93
4
o.
89
4
.90
1
DE
PR
EF
-EX
AC
T
.9
84
?
.0
11
.9
61
?
.0
17
.9
37
?
.0
38
.9
36
?
.0
27
.7
44
?
.0
46
.9
12
?
.0
15
o.
92
4
o.
89
2
.90
1
SIM
PB
LE
U-
RE
CA
LL
.9
78
?
.0
14
.9
36
?
.0
20
.9
23
?
.0
52
.9
09
?
.0
27
.7
98
?
.0
43
.9
09
?
.0
17
o.
92
3
.87
4
.88
6
BL
EU
-M
TE
VA
L-I
NT
L
.9
89
?
.0
14
.9
02
?
.0
17
.8
95
?
.0
49
.9
36
?
.0
32
.6
95
?
.0
42
.8
83
?
.0
15
.86
6
.84
3
.87
4
BL
EU
-M
TE
VA
L
.9
89
?
.0
14
.8
95
?
.0
20
.8
88
?
.0
45
.9
36
?
.0
32
.6
70
?
.0
41
.8
76
?
.0
15
.85
4
.83
5
.86
5
BL
EU
-M
OS
ES
.9
93
?
.0
14
.9
02
?
.0
17
.8
79
?
.0
51
.9
36
?
.0
36
.6
51
?
.0
41
.8
72
?
.0
16
o.
85
6
.82
6
.86
1
CD
ER
-M
OS
ES
.99
5?
.0
14
.8
77
?
.0
17
.8
88
?
.0
49
.9
27
?
.0
36
.6
59
?
.0
45
.8
69
?
.0
17
o.
87
7
o.
83
1
.85
9
SIM
PB
LE
U-
PR
EC
.9
89
?
.0
08
.8
46
?
.0
20
.8
32
?
.0
59
.9
18
?
.0
23
.7
04
?
.0
42
.8
58
?
.0
17
o.
87
1
.81
5
.84
7
NL
EP
OR
.9
45
?
.0
22
.9
49
?
.0
25
.8
25
?
.0
56
.8
45
?
.0
41
.7
05
?
.0
43
.8
54
?
.0
18
o.
86
7
.80
4
o.
85
3
LE
PO
R
V3
.10
0
.9
45
?
.0
19
.9
34
?
.0
27
.7
48
?
.0
77
.8
00
?
.0
36
.7
79
?
.0
41
.8
41
?
.0
20
o.
86
9
.78
0
o.
85
0
NI
ST
-M
TE
VA
L
.9
51
?
.0
19
.8
75
?
.0
22
.7
69
?
.0
77
.8
91
?
.0
27
.6
49
?
.0
45
.8
27
?
.0
20
.85
2
.77
4
.82
4
NI
ST
-M
TE
VA
L-I
NT
L
.9
51
?
.0
19
.8
75
?
.0
22
.7
62
?
.0
77
.8
82
?
.0
32
.6
58
?
.0
45
.8
26
?
.0
21
o.
85
6
.77
4
o.
82
6
TE
R-
M
OS
ES
.9
51
?
.0
19
.8
33
?
.0
23
.8
25
?
.0
77
.8
00
?
.0
36
.5
81
?
.0
45
.7
98
?
.0
21
.80
3
.73
3
.79
7
W
ER
-M
OS
ES
.9
51
?
.0
19
.6
72
?
.0
26
.7
97
?
.0
70
.7
55
?
.0
41
.5
91
?
.0
42
.7
53
?
.0
20
.78
5
.68
2
.74
9
PE
R-
M
OS
ES
.8
52
?
.0
27
.8
58
?
.0
25
.3
57
?
.0
91
.6
97
?
.0
43
.6
77
?
.0
40
.6
88
?
.0
24
.75
7
.63
7
.70
6
TE
RR
OR
CA
T
.9
84
?
.0
11
.9
61
?
.0
23
.9
72
?
.0
28
n/a
n/a
.97
2?
.0
12
.97
7
.95
8
.95
9
Ta
ble
2:
Sy
ste
m-
lev
el
co
rre
lat
ion
so
fa
uto
ma
tic
eva
lua
tio
nm
etr
ics
an
dt
he
offi
cia
lW
MT
hu
ma
ns
co
res
wh
en
tra
nsl
ati
ng
int
oE
ng
lis
h.
Th
es
ym
bo
l?
o?
ind
ica
tes
wh
ere
the
oth
er
ave
rag
es
are
ou
to
fs
eq
ue
nc
ec
om
pa
red
to
the
ma
in
Sp
ear
ma
n?s
?
ave
rag
e.
48
Co
rre
lat
ion
coe
ffic
ien
t
Sp
ear
ma
n?s
?
Co
rre
lat
ion
Co
effi
cie
nt
Pe
ars
on
?s
Cl
ust
ers
Fu
zzy
Ra
nk
s
Di
rec
tio
ns
en
-fr
en
-de
en
-es
en
-cs
en
-ru
Av
era
ge
Av
era
ge
Av
era
ge
Av
era
ge
Co
nsi
de
red
sys
tem
s
14
14
12
11
12
SIM
PB
LE
U-
RE
CA
LL
.9
24
?
.0
22
.92
5?
.0
20
.8
30
?
.0
47
.8
67
?
.0
31
.7
10
?
.0
53
.85
1?
.0
18
.84
4
.85
6
.84
9
LE
PO
R
V3
.10
0
.9
04
?
.0
34
.9
00
?
.0
27
.8
41
?
.0
49
.7
48
?
.0
56
.85
5?
.0
48
.8
50
?
.0
20
o.8
54
.83
3
.84
4
NI
ST
-M
TE
VA
L-I
NT
L
.92
9?
.0
32
.8
46
?
.0
29
.7
97
?
.0
60
.9
02
?
.0
45
.7
71
?
.0
48
.8
49
?
.0
20
.80
8
o.8
63
o.
84
5
CD
ER
-M
OS
ES
.9
21
?
.0
29
.8
67
?
.0
29
.85
7?
.0
58
.8
88
?
.0
24
.7
01
?
.0
59
.8
47
?
.0
19
.79
6
o.
86
1
.84
3
NL
EP
OR
.9
19
?
.0
28
.9
04
?
.0
27
.8
52
?
.0
49
.8
18
?
.0
45
.7
27
?
.0
64
.8
44
?
.0
21
o.
84
9
o.
84
6
.84
0
NI
ST
-M
TE
VA
L
.9
14
?
.0
34
.8
25
?
.0
30
.7
80
?
.0
66
.9
16
?
.0
31
.7
23
?
.0
48
.8
32
?
.0
21
.79
4
o.
85
1
.82
8
SIM
PB
LE
U-
PR
EC
.9
09
?
.0
26
.8
79
?
.0
25
.7
80
?
.0
71
.8
81
?
.0
35
.6
97
?
.0
51
.8
29
?
.0
20
o.
84
0
o.
85
2
.82
7
M
ET
EO
R
.9
24
?
.0
27
.8
79
?
.0
30
.7
80
?
.0
60
.93
7?
.0
24
.5
69
?
.0
66
.8
18
?
.0
22
o.
80
6
.82
5
.81
4
BL
EU
-M
TE
VA
L-I
NT
L
.9
17
?
.0
33
.8
32
?
.0
30
.7
64
?
.0
71
.8
95
?
.0
28
.6
57
?
.0
62
.8
13
?
.0
22
o.
80
2
.82
1
.80
8
BL
EU
-M
TE
VA
L
.8
95
?
.0
37
.7
86
?
.0
34
.7
64
?
.0
71
.8
95
?
.0
28
.6
31
?
.0
53
.7
94
?
.0
22
o.
79
9
.80
9
.79
0
TE
R-
M
OS
ES
.9
12
?
.0
38
.8
54
?
.0
32
.7
53
?
.0
66
.8
60
?
.0
59
.5
38
?
.0
68
.7
83
?
.0
23
.74
6
.80
6
.77
8
BL
EU
-M
OS
ES
.8
97
?
.0
34
.7
86
?
.0
34
.7
59
?
.0
78
.8
95
?
.0
28
.5
74
?
.0
57
.7
82
?
.0
22
o.
80
2
.79
2
o.
77
9
W
ER
-M
OS
ES
.9
14
?
.0
34
.8
25
?
.0
34
.7
14
?
.0
77
.8
60
?
.0
56
.5
52
?
.0
66
.7
73
?
.0
24
.73
7
o.
79
6
.76
6
PE
R-
M
OS
ES
.8
73
?
.0
40
.6
86
?
.0
45
.7
75
?
.0
47
.7
97
?
.0
49
.5
91
?
.0
62
.7
44
?
.0
24
o.
75
8
.74
7
.73
9
TE
RR
OR
CA
T
.92
9?
.0
22
.94
6?
.0
18
.91
2?
.0
41
n/a
n/a
.92
9?
.0
17
.95
2
.93
3
.92
3
SE
M
PO
S
n/a
n/a
n/a
.6
99
?
.0
45
n/a
.6
99
?
.0
45
.71
7
.61
5
.69
6
AC
TA
5?
6
.8
09
?
.0
46
-.5
26
?
.0
34
n/a
n/a
n/a
.1
41
?
.0
29
.16
6
.19
6
.17
6
AC
TA
.8
09
?
.0
46
-.5
26
?
.0
34
n/a
n/a
n/a
.1
41
?
.0
29
.16
6
.19
6
.17
6
Ta
ble
3:
Sy
ste
m-
lev
el
co
rre
lat
ion
so
fa
uto
ma
tic
eva
lua
tio
nm
etr
ics
an
dt
he
offi
cia
lW
MT
hu
ma
ns
co
res
wh
en
tra
nsl
ati
ng
ou
to
fE
ng
lis
h.
Th
es
ym
bo
l?
o?
ind
ica
tes
wh
ere
the
oth
er
ave
rag
es
are
ou
to
fs
eq
ue
nc
ec
om
pa
red
to
the
ma
in
Sp
ear
ma
n?s
?
ave
rag
e.
49
Directions fr-en de-en es-en cs-en ru-en Average
Extracted pairs 80741 128668 67832 85469 151422
SIMPBLEU-RECALL .193 .318 .279 .260 .234 .257
METEOR .178 .293 .236 .265 .239 .242
DEPREF-ALIGN .161 .267 .234 .228 .200 .218
DEPREF-EXACT .167 .263 .228 .227 .195 .216
SIMPBLEU-PREC .154 .236 .214 .208 .174 .197
NLEPOR .149 .240 .204 .176 .172 .188
SENTBLEU-MOSES .150 .218 .198 .197 .170 .187
LEPOR V3.100 .149 .221 .161 .187 .177 .179
UMEANT .101 .166 .144 .160 .108 .136
MEANT .101 .160 .145 .164 .109 .136
LOGREGFSS-33 n/a .272 n/a n/a n/a .272
LOGREGFSS-24 n/a .270 n/a n/a n/a .270
TERRORCAT .161 .298 .230 n/a n/a .230
Table 4: Segment-level Kendall?s ? correlations of automatic evaluation metrics and the official WMT
human judgements when translating into English.
Directions en-fr en-de en-es en-cs en-ru Average
Extracted pairs 100783 77286 60464 102842 87323
SIMPBLEU-RECALL .158 .085 .231 .065 .126 .133
SIMPBLEU-PREC .138 .065 .187 .055 .095 .108
METEOR .147 .049 .175 .058 .111 .108
SENTBLEU-MOSES .133 .047 .171 .052 .095 .100
LEPOR V3.100 .126 .058 .178 .023 .109 .099
NLEPOR .124 .048 .163 .048 .097 .096
LOGREGNORM-411 n/a n/a .136 n/a n/a .136
TERRORCAT .116 .074 .186 n/a n/a .125
LOGREGNORMSOFT-431 n/a n/a .033 n/a n/a .033
Table 5: Segment-level Kendall?s ? correlations of automatic evaluation metrics and the official WMT
human judgements when translating out of English.
50
strongly disagrees with humans. The method we
used this year does not harm metrics which often
estimate two segments as equally good.
5 Conclusion
We carried out WMT13 Metrics Shared Task in
which we assessed the quality of various au-
tomatic machine translation metrics. We used
the human judgements as collected for WMT13
Translation Task to compute system-level and
segment-level correlations with human scores.
While most of the metrics correlate very well
on the system-level, the segment-level correlations
are still rather poor. It was shown again this year
that a lot of metrics outperform BLEU, hopefully
one of them will attract a wider use at last.
Acknowledgements
This work was supported by the grants
P406/11/1499 of the Grant Agency of the
Czech Republic and FP7-ICT-2011-7-288487
(MosesCore) of the European Union.
References
Eleftherios Avramidis and Maja Popovic?. 2013. Ma-
chine learning methods for comparative and time-
oriented Quality Estimation of Machine Translation
output. In Proceedings of the Eight Workshop on
Statistical Machine Translation.
Ondr?ej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 workshop
on statistical machine translation. In Proceedings of
the Eight Workshop on Statistical Machine Transla-
tion.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Language
Technology Research, HLT ?02, pages 138?145, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Mark Fishel. 2013. Ranking Translations using Error
Analysis and Quality Estimation. In Proceedings of
the Eight Workshop on Statistical Machine Transla-
tion.
Najeh Hajlaoui and Andrei Popescu-Belis. 2013. As-
sessing the accuracy of discourse connective transla-
tions: Validation of an automatic metric. In 14th In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics, page 12. Uni-
versity of the Aegean, Springer, March.
Najeh Hajlaoui. 2013. Are ACT?s scores increasing
with better translation quality. In Proceedings of the
Eight Workshop on Statistical Machine Translation.
Aaron Li-Feng Han, Derek F. Wong, Lidia S. Chao,
Yi Lu, Liangye He, Yiming Wang, and Jiaji Zhou.
2013. A Description of Tunable Machine Transla-
tion Evaluation Systems in WMT13 Metrics Task.
In Proceedings of the Eight Workshop on Statistical
Machine Translation.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2006. Cder: Efficient mt evaluation using block
movements. In In Proceedings of EACL, pages 241?
248.
Chi-Kiu Lo and Dekai Wu. 2013. MEANT @
WMT2013 metrics evaluation. In Proceedings of
the Eight Workshop on Statistical Machine Transla-
tion.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. pages 311?318.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In In Proceedings of Association for Machine
Translation in the Americas, pages 223?231.
Xingyi Song, Trevor Cohn, and Lucia Specia. 2013.
BLEU deconstructed: Designing a better MT evalu-
ation metric. March.
Xiaofeng Wu, Hui Yu, and Qun Liu. 2013. DCU Par-
ticipation in WMT2013 Metrics Task. In Proceed-
ings of the Eight Workshop on Statistical Machine
Translation.
51
