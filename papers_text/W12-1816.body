NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 37?40,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Mining Search Query Logs for Spoken Language Understanding
Dilek Hakkani-Tu?r, Gokhan Tu?r, Asli Celikyilmaz
Microsoft, Mountain View, CA 94041, USA
dilek|gokhan.tur|asli@ieee.org
Abstract
In a spoken dialog system that can handle nat-
ural conversation between a human and a ma-
chine, spoken language understanding (SLU)
is a crucial component aiming at capturing
the key semantic components of utterances.
Building a robust SLU system is a challeng-
ing task due to variability in the usage of lan-
guage, need for labeled data, and requirements
to expand to new domains (movies, travel, fi-
nance, etc.). In this paper, we survey recent
research on bootstrapping or improving SLU
systems by using information mined or ex-
tracted from web search query logs, which
include (natural language) queries entered by
users as well as the links (web sites) they click
on. We focus on learning methods that help
unveiling hidden information in search query
logs via implicit crowd-sourcing.
1 Introduction
Building a robust spoken dialog system involves hu-
man language technologies to cooperate to answer
natural language (NL) user requests. First user?s
speech is recognized using an automatic speech
recognition (ASR) engine. Then a spoken language
understanding (SLU) engine extracts their meaning
to be sent to dialog manager for taking the appropri-
ate system action.
Three key tasks of an SLU system are domain
classification, intent determination and slot filling
(Tur and Mori, 2011). While the state-of-the-art
SLU systems rely on data-driven methods, collect-
ing and annotating naturally spoken utterances to
train the required statistical models is often costly
and time-consuming, representing a significant bar-
rier to deployment. However, previous work shows
that it may be possible to alleviate this hurdle by
leveraging the abundance of implicitly labeled web
search queries in search engines. Large-scale en-
gines, e.g., Bing or Google, log more than 100M
queries every day. Each logged query has an associ-
ated set of URLs that were clicked after the users en-
tered the query. This information can be valuable for
building more robust SLU components, therefore,
provide (noisy) supervision in training SLU mod-
els. Take domain detection problem: Two users who
enter different queries but click on the same URL
(www.hotels.com) would probably be searching for
concepts in the same domain (?hotels? in this case).
The use of click information obtained through
massive search query click logs has been the fo-
cus of previous research. Specifically, query logs
have been used for building more robust web search
and better information retrieval (Pantel and Fuxman,
2011; Li et al., 2008), improve personalization expe-
rience and understand social networking behaviors
(Wang et al., 2011), etc. The use of query logs in
spoken dialog research is fairly new. In this paper,
we will survey the recent research on utilizing the
search query logs to obtain more accurate and ro-
bust spoken dialog systems, focusing on the SLU.
Later in the discussion section, we will discuss the
implimications on the dialog models.
The paper is organized as follows: In ? 2, we
briefly describe query click logs. We then summa-
rize recent research papers to give a snapshot of how
user search queries are being used in ? 3, and how
information from click-through graphs (queries and
37
movie theaters 
in san bruno
regal sunset 
square cinemas
the majestic 
crest theater
cinema in rosewell
rave cinemas
in eastfield
Queries Clicked URLs
moviefone.com
movies.eventful.com
yelp.com
amctheaters.com
find cheap tickets
for inception
movie ticket deals
watch movie
deals for inception
Queries Clicked URLs
fandango.com
movietickets.com
movie.yahoo.com
ticketmakers.com
movieworld carslton
ticket prices
1783
20
530
32
24
549
46
121
Figure 1: A sample query click graph. The squared queries
are samples from training data which are natural language ut-
terances. Edges include click frequencies from query to link.
clicked links) are exploited to boost the SLU perfor-
mance. Lastly, we discuss possible future directions.
2 What are Query Click Logs (QCL)?
QCL are logs of unstructured text including both the
users queries sent to a search engine and the links
that the users clicked on from the list of sites re-
turned by that search engine. A common representa-
tion of such data is a bi-partite query-click graph as
shown in (Fig 1), where one set of nodes represents
queries, and the other set of nodes represents URLs,
and an edge is placed between two nodes represent-
ing a query q and a URL u, if at least one user who
typed the q clicked on u.
Traditionally, the edge of the click graph is
weighted based on the raw click frequency (number
of clicks) from a query to a URL. Some of the chal-
lenges in extracting useful information from QCL is
that the feature space is high dimensional (there are
thousands of url clicks linked to many queries), and
there are millions of queries logged daily.
3 Exploiting NL Search Queries for SLU
Previous work on web search has benefited from the
use of query click logs for improving query intent
classification. Li et al. use query click logs to de-
termine the domain of a query (typically keyword
search queries), and then infer the class member-
ships of unlabeled queries from those of the labeled
search queries using the URLs the users clicked (Li
et al., 2009; Li et al., 2008). QCL have been used to
extract named-entities to improve web search and ad
publishing experience (Hillard and Leggetter, 2010)
using (un)supervised learning methods on keyword
based search queries. Different from previous re-
search, in this paper we focus on recent research that
utilize NL search queries to boost the performance
of SLU components, i.e., domain detection, intent
determination, and slot filling.
In (Hakkani-Tur et al., 2011a), they use the search
query logs for domain classification by integrat-
ing noisy supervision into the semi-supervised la-
bel propagation algorithm, and sample high-quality
query click data. Specifically, they extract a set of
queries, whose users clicked on the URLs that are
related to their target domain categories. Then they
mine query click logs to get all instances of these
search queries and the set of links that were clicked
on by search engine users who entered the same
query. They compare two semi-supervised learn-
ing methods, self-training and label propagation, to
exploit the domain information obtained form the
URLs user have clicked on. The analysis indicate
that query sampling through semi-supervised learn-
ing enables extracting NL queries for use in domain
detection. They also argue that using raw queries
with and without the noisy labels in semi-supervised
learning reduces domain detection error rate by 20%
relative to supervised learning which uses only the
manually labeled examples.
The search queries found in click logs and the NL
spoken utterances are different in the sense that the
search queries are usually short and keyword based
compared to NL utterances that are longer and are
usually grammatical sentences (see Fig. 1). Hence,
in (Hakkani-Tur et al., 2012), they choose a statis-
tical machine translation (SMT) approach to search
query mining for SLU as sketched in Fig. 2. The
assumption is that, users typically have conceptual
intents underlying their requests when they inter-
act with web search engine or use a virtual assis-
tance system with built in SLU engine, e.g., ?avatar
awards? versus ?which awards did the movie avatar
win??. They translate NL queries into search queries
and mine similar search queries in QCL. They also
exploit QCL for bootstrapping domain detection
models, using only the NL queries hitting to seed
domain indicator URLs (Hakkani-Tur et al., 2011c).
Specifically, if one needs to detect a domain detector
for the hotels domain, the queries hitting hotels.com,
or tripadvisor.com, may be used to mine.
Query click logs have been explored for slot fill-
ing models as well. The slot filling models of SLU
38
Figure 2: Using natural language to query language translation
for mining query click logs.
aim to capture semantic components given the do-
main and a common way is to use gazetteer features
(dictionaries specific to domain such as movie-name
or actors in movie domain). In (Hillard et al., 2011),
they propose to mine and weight gazetteer entries
using query click logs. The gazetteer entries are
scored using a function of posterior probabilities for
that entry hitting a URL (compared to others URLs)
and for that URL being related to the target domain.
In such a schema the movie name ?gone with the
wind? gets higher score than the movie ?up?.
In (Tur et al., 2011), an unsupervised approach is
presented to implicitly annotate the training data us-
ing the QCL. Being unsupervised, this method auto-
matically populates gazetteers as opposed to man-
ually crafted gazetteers. Specifically they use an
abundant set of web search query logs with their
click information (see Fig. 1). They start by de-
tecting target URLs (such as imdb.com/title
for the movie names). Then they obtain a list
of entities and their target URLs (for example,
www.imdb.com/title/tt047723 can be the target URL
for the movie ?the count of monte carlo?. Then they
extract all queries hitting those links if they include
that entity. This method enables automatically ob-
taining annotated queries such as: ?review of the
hand? or ?mad men season one synopsis? (bold
terms are automatically discovered entities.)
4 Mining Click Graph Features for SLU
In the previous section, we presented examples of
recent research that use queries obtained from QCL
to bootstrap and improve SLU models. Note that
each query in QCL is linked to one or many web
sites (links), which indicate a certain feature of the
query (queries that the hotels.com linked are clicked
after they are entered might indicate hotels domain).
Such features extracted from QCL data (called click-
through features) has been demonstrated to signifi-
cantly improve the performance of ranking models
for Web search applications (Gao et al., 2009), es-
timating relations between entities and web search
queries (Pantel and Fuxman, 2011), etc.
In SLU research community, only recently the use
of click-through features has shown to improve the
performance of domain and intent of NL user utter-
ances. In one study (Hakkani-Tur et al., 2011b), in-
stead of mining more data to train a domain clas-
sifier with lexical features, they enrich their fea-
tures using the click-through features with the in-
tuition that the queries with similar click patterns
should be semantically similar. They search all the
NL utterances in the training data set amongst the
search queries. Once they obtain search queries,
they pull the list of clicked URLs and their frequen-
cies for each query which represent the click fea-
tures. To reduce the number of features, they ex-
tract only the base URLs (such as opentable.com or
wikipedia.com), as is commonly done in the web
search literature. T use the list of the 1000 most fre-
quently clicked base URLs for extracting classifica-
tion features (QCL features). For each input user
utterance, xj , they compute P (URLi|xj), where
i = 1..1000. They compute the click probability dis-
tribution distance between a query and the queries in
a target domain, Dk, using the KL divergence:
KLk = KL(P (URLi|xj)||P (URLi|Dk)) (1)
Thus, for a given domain Dk, the KLk and the do-
main with the lowest KL divergence are used as ad-
ditional features.
Although the click-through are demonstrated to
be beneficial for SLU models, such benefits, how-
ever, are severely limited by the data sparseness
problem, i.e., many queries and documents have no
or very few clicks. The SLU models thus cannot rely
strongly on click-through features. In (Celikyilmaz
et al., 2011), the sparsity issue of representing the
queries with click-through features are investigated.
They represent each unlabeled query from QCL as
39
a high dimensional sparse vector of click frequen-
cies. Since the true dimensionality of a query is un-
known (the number of clicks are infinitely many),
they utilize an unbounded factor analysis approach
and build an infinite dimensional latent factor anal-
ysis, namely the Indian Buffet Process (IBP) (Grif-
fiths and Ghahramani, 2005), specifically to model
the latent factor structure of the given set of queries.
They implement a graph summarization algorithm
to capture representative queries from a large set of
unlabeled queries that are similar to a rather smaller
set of labeled queries. They capture the latent factor
structure of the labeled queries via IBP and reduce
the dimensionality of the queries to manageable size
and collect additional queries in this latent factor
space. They use the new set of utterances boost the
intent detection performance of SLU models.
5 Discussions and Future Directions
This paper surveyed previous research on the usage
of the query click logs (the click through data) pro-
vide valuable statistics that can potentially improve
performance of the SLU models. We presented sev-
eral methods that has been used to extract infor-
mation in the form of additional vocabulary, unla-
beled utterances and hidden features to represent ut-
terances. The current research is only the beginning,
and most approaches such as query expansion, sen-
tence compression, etc. can be easily adopted for
dialog state update processes. Thus, the state-of-the
art in NL understanding can be improved by:
? clustering of URLs as well as queries for extract-
ing better features as well as to extend ontologies.
The search community has access to vast amounts
of search data that would benefit natural language
processing research,
? mining multi-lingual data for transferring dialog
systems from one language to others,
? mining information from search sessions, for ex-
ample, users rephrasing of their own search queries
for better results.
One issue that has been the topic of recent discus-
sions is the accessibility of QCL data to researchers.
Note that, QCL is not a crowd-source data that only
large web search organizations like Google or Mi-
crosoft Bing can mine and exploit for NL under-
standing, but various other forms may be imple-
mented by interested researchers by using a simple
web service or a mobile app (such as AT&T SpeakIt
or Dragon Go) or using a targeted search engine.
References
