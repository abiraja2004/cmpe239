Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 948?956,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Probabilistic Frame-Semantic Parsing
Dipanjan Das Nathan Schneider Desai Chen Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dipanjan@cs,nschneid@cs,desaic@andrew,nasmith@cs}.cmu.edu
Abstract
This paper contributes a formalization of
frame-semantic parsing as a structure predic-
tion problem and describes an implemented
parser that transforms an English sentence
into a frame-semantic representation. It finds
words that evoke FrameNet frames, selects
frames for them, and locates the arguments
for each frame. The system uses two feature-
based, discriminative probabilistic (log-linear)
models, one with latent variables to permit
disambiguation of new predicate words. The
parser is demonstrated to significantly outper-
form previously published results.
1 Introduction
FrameNet (Fillmore et al, 2003) is a rich linguistic
resource containing considerable information about
lexical and predicate-argument semantics in En-
glish. Grounded in the theory of frame semantics
(Fillmore, 1982), it suggests?but does not formally
define?a semantic representation that blends word-
sense disambiguation and semantic role labeling.
In this paper, we present a computational and
statistical model for frame-semantic parsing, the
problem of extracting from text semantic predicate-
argument structures such as those shown in Fig. 1.
We aim to predict a frame-semantic representation
as a structure, not as a pipeline of classifiers. We
use a probabilistic framework that cleanly integrates
the FrameNet lexicon and (currently very limited)
available training data. Although our models often
involve strong independence assumptions, the prob-
abilistic framework we adopt is highly amenable to
future extension through new features, relaxed in-
dependence assumptions, and semisupervised learn-
ing. Some novel aspects of our current approach
include a latent-variable model that permits disam-
biguation of words not in the FrameNet lexicon, a
unified model for finding and labeling arguments,
TRANSITIVE_
ACTION
Agent
Patient
Event
Cause
Place
Time
CAUSE_TO_
MAKE_NOISE
Agent
Sound_maker
Cause
Place
Time
MAKE_NOISE
Noisy_event
Sound
Sound_source
Place
Time
cough.v, gobble.v, 
ring.v, yodel.v, ...
blare.v, play.v, 
ring.v, toot.v, ...
?
Inheritance relation Causative_of relation
Excludes relation
Purpose
Figure 2. Partial illustration of frames, roles, and LUs
related to the CAUSE TO MAKE NOISE frame, from the
FrameNet lexicon. ?Core? roles are filled ovals. 8 addi-
tional roles of CAUSE TO MAKE NOISE are not shown.
and a precision-boosting constraint that forbids ar-
guments of the same predicate to overlap. Our parser
achieves the best published results to date on the
SemEval?07 FrameNet task (Baker et al, 2007).
2 Resources and Task
We consider frame-semantic parsing resources.
2.1 FrameNet Lexicon
The FrameNet lexicon is a taxonomy of manu-
ally identified general-purpose frames for English.1
Listed in the lexicon with each frame are several
lemmas (with part of speech) that can denote the
frame or some aspect of it?these are called lexi-
cal units (LUs). In a sentence, word or phrase to-
kens that evoke a frame are known as targets. The
set of LUs listed for a frame in FrameNet may not
be exhaustive; we may see a target in new data that
does not correspond to an LU for the frame it evokes.
Each frame definition also includes a set of frame el-
ements, or roles, corresponding to different aspects
of the concept represented by the frame, such as par-
ticipants, props, and attributes. We use the term ar-
1Like the SemEval?07 participants, we used FrameNet v. 1.3
(http://framenet.icsi.berkeley.edu).
948
bell.n
ring.v
there be.v
enough.a
LU
NOISE_MAKERS
SUFFICIENCY
Frame
EXISTENCE
CAUSE_TO_MAKE_NOISE
.bells
 
 
N_m
more than six of the eight
Sound_maker
Enabled_situation
ringtoringers
Item
enough
Entity
Agent
n'tarestillthereBut
Figure 1. A sentence from PropBank and the SemEval?07 training data, and a partial depiction of gold FrameNet
annotations. Each frame is a row below the sentence (ordered for readability). Thick lines indicate targets that evoke
frames; thin solid/dotted lines with labels indicate arguments. ?N m? under bells is short for the Noise maker role of
the NOISE MAKERS frame. The last row indicates that there. . . are is a discontinuous target. In PropBank, the verb
ring is the only annotated predicate for this sentence, and it is not related to other predicates with similar meanings.
FRAMENET LEXICON V. 1.3
lexical exemplars
entries counts coverage
8379 LUs 139K sentences, 3.1M words 70% LUs
795 frames 1 frame annotation / sentence 63% frames
7124 roles 285K overt arguments 56% roles
Table 1. Snapshot of lexicon entries and exemplar sen-
tences. Coverage indicates the fraction of types attested
in at least one exemplar.
gument to refer to a sequence of word tokens anno-
tated as filling a frame role. Fig. 1 shows an exam-
ple sentence from the training data with annotated
targets, LUs, frames, and role-argument pairs. The
FrameNet lexicon also provides information about
relations between frames and between roles (e.g.,
INHERITANCE). Fig. 2 shows a subset of the rela-
tions between three frames and their roles.
Accompanying most frame definitions in the
FrameNet lexicon is a set of lexicographic exemplar
sentences (primarily from the British National Cor-
pus) annotated for that frame. Typically chosen to il-
lustrate variation in argument realization patterns for
the frame in question, these sentences only contain
annotations for a single frame. We found that using
exemplar sentences directly to train our models hurt
performance as evaluated on SemEval?07 data, even
though the number of exemplar sentences is an order
of magnitude larger than the number of sentences in
our training set (?2.2). This is presumably because
the exemplars are neither representative as a sample
nor similar to the test data. Instead, we make use of
these exemplars in features (?4.2).
2.2 Data
Our training, development, and test sets consist
of documents annotated with frame-semantic struc-
tures for the SemEval?07 task, which we refer to col-
FULL-TEXT SemEval?07 data
ANNOTATIONS train dev test
Size (words sentences documents)
all 43.3K1.7K 22 6.3K 251 4 2.8K 120 3
ANC (travel) 3.9K 154 2 .8K 32 1 1.3K 67 1
NTI (bureaucratic) 32.2K1.2K 15 5.5K 219 3 1.5K 53 2
PropBank (news) 7.3K 325 5 0 0 0 0 0 0
Annotations (frames/word overt arguments/word)
all 0.23 0.39 0.22 0.37 0.37 0.65
Coverage of lexicon (% frames % roles % LUs)
all 64.1 27.4 21.0 34.0 10.2 7.3 29.3 7.7 4.9
Out-of-lexicon types (frames roles LUs)
all 14 69 71 2 4 2 39 99 189
Out-of-lexicon tokens (% frames % roles % LUs)
all 0.7 0.9 1.1 1.0 0.4 0.2 9.8 11.2 25.3
Table 2. Snapshot of the SemEval?07 annotated data.
lectively as the SemEval?07 data.2 For the most
part, the frames and roles used in annotating these
documents were defined in the FrameNet lexicon,
but there are some exceptions for which the annota-
tors defined supplementary frames and roles; these
are included in the possible output of our parser.
Table 2 provides a snapshot of the SemEval?07
data. We randomly selected three documents from
the original SemEval training data to create a devel-
opment set for tuning model hyperparameters. No-
tice that the test set contains more annotations per
word, both in terms of frames and arguments. More-
over, there are many more out-of-lexicon frame,
role, and LU types in the test set than in the training
set. This inconsistency in the data results in poor re-
call scores for all models trained on the given data
split, a problem we have not sought to address here.
2http://framenet.icsi.berkeley.edu/
semeval/FSSE.html
949
Preprocessing. We preprocess sentences in our
dataset with a standard set of annotations: POS
tags from MXPOST (Ratnaparkhi, 1996) and depen-
dency parses from the MST parser (McDonald et al,
2005) since manual syntactic parses are not available
for most of the FrameNet-annotated documents. We
used WordNet (Fellbaum, 1998) for lemmatization.
We also labeled each verb in the data as having AC-
TIVE or PASSIVE voice, using code from the SRL
system described by Johansson and Nugues (2008).
2.3 Task and Evaluation
Automatic annotations of frame-semantic structure
can be broken into three parts: (1) targets, the words
or phrases that evoke frames; (2) the frame type,
defined in the lexicon, evoked by each target; and
(3) the arguments, or spans of words that serve to
fill roles defined by each evoked frame. These cor-
respond to the three subtasks in our parser, each
described and evaluated in turn: target identifica-
tion (?3), frame identification (?4, not unlike word-
sense disambiguation), and argument identification
(?5, not unlike semantic role labeling).
The standard evaluation script from the
SemEval?07 shared task calculates precision,
recall, and F1-measure for frames and arguments;
it also provides a score that gives partial credit
for hypothesizing a frame related to the correct
one. We present precision, recall, and F1-measure
microaveraged across the test documents, report
labels-only matching scores (spans must match
exactly), and do not use named entity labels. More
details can be found in Baker et al (2007). For our
experiments, statistical significance is measured us-
ing a reimplementation of Dan Bikel?s randomized
parsing evaluation comparator.3
2.4 Baseline
A strong baseline for frame-semantic parsing is
the system presented by Johansson and Nugues
(2007, hereafter J&N?07), the best system in the
SemEval?07 shared task. For frame identifica-
tion, they used an SVM classifier to disambiguate
frames for known frame-evoking words. They used
WordNet synsets to extend the vocabulary of frame-
evoking words to cover unknown words, and then
3http://www.cis.upenn.edu/?dbikel/
software.html#comparator
TARGET IDENTIFICATION P R F1
Our technique (?3) 89.92 70.79 79.21
Baseline: J&N?07 87.87 67.11 76.10
Table 3. Target identification results for our system and
the baseline. Scores in bold denote significant improve-
ments over the baseline (p < 0.05).
used a collection of separate SVM classifiers?one
for each frame?to predict a single evoked frame for
each occurrence of a word in the extended set.
J&N?07 modeled the argument identification
problem by dividing it into two tasks: first, they
classified candidate spans as to whether they were
arguments or not; then they assigned roles to those
that were identified as arguments. Both phases used
SVMs. Thus, their formulation of the problem in-
volves a multitude of classifiers?whereas ours uses
two log-linear models, each with a single set of
weights, to find a full frame-semantic parse.
3 Target Identification
Target identification is the problem of deciding
which word tokens (or word token sequences) evoke
frames in a given sentence. In other semantic role
labeling schemes (e.g. PropBank), simple part-of-
speech criteria typically distinguish predicates from
non-predicates. But in frame semantics, verbs,
nouns, adjectives, and even prepositions can evoke
frames under certain conditions. One complication
is that semantically-impoverished support predi-
cates (such as make in make a request) do not
evoke frames in the context of a frame-evoking,
syntactially-dependent noun (request). Further-
more, only temporal, locative, and directional senses
of prepositions evoke frames.
We found that, because the test set is more com-
pletely annotated?that is, it boasts far more frames
per token than the training data (see Table 2)?
learned models did not generalize well and achieved
poor test recall. Instead, we followed J&N?07 in us-
ing a small set of rules to identify targets.
For a span to be a candidate target, it must ap-
pear (up to morphological variation) as a target in the
training data or the lexicon. We consider multiword
targets,4 unlike J&N?07 (though we do not consider
4There are 629 multiword LUs in the lexicon, and they cor-
respond to 4.8% of the targets in the training set; among them
are screw up.V, shoot the breeze.V, and weapon of mass de-
950
FRAME IDENTIFICATION exact frame matching partial frame matching
(?4) targets P R F1 P R F1
Frame identification (oracle targets) ? 60.21 60.21 60.21 74.21 74.21 74.21
Frame identification (predicted targets) auto ?3 69.75 54.91 61.44 77.51 61.03 68.29
Baseline: J&N?07 auto 66.22 50.57 57.34 73.86 56.41 63.97
Table 4. Frame identification results. Precision, recall, and F1 were evaluated under exact and partial frame matching;
see ?2.3. Bold indicates statistically significant results with respect to the baseline (p < 0.05).
discontinuous targets). Using rules from ?3.1.1 of
J&N?07, we further prune the list, with two modi-
fications: we prune all prepositions, including loca-
tive, temporal, and directional ones, but do not prune
support verbs. This is a conservative approach; our
automatic target identifier will never propose a target
that was not seen in the training data or FrameNet.
Results. Table 3 shows results on target identifica-
tion; our system gains 3 F1 points over the baseline.
4 Frame Identification
Given targets, the parser next identifies their frames.
4.1 Lexical units
FrameNet specifies a great deal of structural infor-
mation both within and among frames. For frame
identification we make use of frame-evoking lexical
units, the (lemmatized and POS-tagged) words and
phrases listed in the lexicon as referring to specific
frames. For example, listed with the BRAGGING
frame are 10 LUs, including boast.N, boast.V, boast-
ful.A, brag.V, and braggart.N. Of course, due to pol-
ysemy and homonymy, the same LU may be associ-
ated with multiple frames; for example, gobble.V is
listed under both the INGESTION and MAKE NOISE
frames. All targets in the exemplar sentences, and
most in our training and test data, correspond to
known LUs (see Table 2).
To incorporate frame-evoking expressions found
in the training data but not the lexicon?and to avoid
the possibility of lemmatization errors?our frame
identification model will incorporate, via a latent
variable, features based directly on exemplar and
training targets rather than LUs. Let L be the set of
(unlemmatized and automatically POS-tagged) tar-
gets found in the exemplar sentences of the lexi-
con and/or the sentences in our training set. Let
Lf ? L be the subset of these targets annotated as
struction.N. In the SemEval?07 training data, there are just 99
discontinuous multiword targets (1% of all targets).
evoking a particular frame f . Let Ll and Llf de-
note the lemmatized versions of L and Lf respec-
tively. Then, we write boasted.VBD ? LBRAGGING
and boast.VBD ? LlBRAGGING to indicate that this in-
flected verb boasted and its lemma boast have been
seen to evoke the BRAGGING frame. Significantly,
however, another target, such as toot your own horn,
might be used in other data to evoke this frame. We
thus face the additional hurdle of predicting frames
for unknown words.
The SemEval annotators created 47 new frames
not present in the lexicon, out of which 14 belonged
to our training set. We considered these with the 795
frames in the lexicon when parsing new data. Pre-
dicting new frames is a challenge not yet attempted
to our knowledge (including here). Note that the
scoring metric (?2.3) gives partial credit for related
frames (e.g., a more general frame from the lexicon).
4.2 Model
For a given sentence x with frame-evoking targets t,
let ti denote the ith target (a word sequence). Let tli
denote its lemma. We seek a list f = ?f1, . . . , fm?
of frames, one per target. In our model, the set of
candidate frames for ti is defined to include every
frame f such that tli ? L
l
f?or if t
l
i 6? L
l, then every
known frame (the latter condition applies for 4.7%
of the gold targets in the development set). In both
cases, we let Fi be the set of candidate frames for
the ith target in x.
To allow frame identification for targets whose
lemmas were seen in neither the exemplars nor the
training data, our model includes an additional vari-
able, `i. This variable ranges over the seen targets
in Lfi , which can be thought of as prototypes for
the expression of the frame. Importantly, frames are
predicted, but prototypes are summed over via the
latent variable. The prediction rule requires a prob-
abilistic model over frames for a target:
fi ? argmaxf?Fi
?
`?Lf
p(f, ` | ti,x) (1)
951
We adopt a conditional log-linear model: for f ? Fi
and ` ? Lf , p?(f, ` | ti,x) =
exp?>g(f, `, ti,x)
?
f ??Fi
?
`??Lf ?
exp?>g(f ?, `?, ti,x)
(2)
where ? are the model weights, and g is a vector-
valued feature function. This discriminative formu-
lation is very flexible, allowing for a variety of (pos-
sibly overlapping) features; e.g., a feature might re-
late a frame type to a prototype, represent a lexical-
semantic relationship between a prototype and a tar-
get, or encode part of the syntax of the sentence.
Previous work has exploited WordNet for better
coverage during frame identification (Johansson and
Nugues, 2007; Burchardt et al, 2005, e.g., by ex-
panding the set of targets using synsets), and others
have sought to extend the lexicon itself (see ?6). We
differ in our use of a latent variable to incorporate
lexical-semantic features in a discriminative model,
relating known lexical units to unknown words that
may evoke frames. Here we are able to take advan-
tage of the large inventory of partially-annotated ex-
emplar sentences.
Note that this model makes a strong independence
assumption: each frame is predicted independently
of all others in the document. In this way the model
is similar to J&N?07. However, ours is a single
conditional model that shares features and weights
across all targets, frames, and prototypes, whereas
the approach of J&N?07 consists of many separately
trained models. Moreover, our model is unique in
that it uses a latent variable to smooth over frames
for unknown or ambiguous LUs.
Frame identification features depend on the pre-
processed sentence x, the prototype ` and its
WordNet lexical-semantic relationship with the tar-
get ti, and of course the frame f . Our model instan-
tiates 662,020 binary features; see Das et al (2010).
4.3 Training
Given the training subset of the SemEval?07 data,
which is of the form
?
?x(j), t(j), f (j),A(j)?
?N
j=1
(N = 1663 is the number of sentences), we dis-
criminatively train the frame identification model by
maximizing the following log-likelihood:5
5We found no benefit on development data from using an L2
regularizer (zero-mean Gaussian prior).
max
?
N?
j=1
mj?
i=1
log
?
`?L
f
(j)
i
p?(f
(j)
i , ` | t
(j)
i ,x
(j)) (3)
Note that the training problem is non-convex be-
cause of the summed-out prototype latent variable
` for each frame. To calculate the objective func-
tion, we need to cope with a sum over frames and
prototypes for each target (see Eq. 2), often an ex-
pensive operation. We locally optimize the function
using a distributed implementation of L-BFGS. This
is the most expensive model that we train: with 100
CPUs, training takes several hours. (Decoding takes
only a few minutes on one CPU for the test set.)
4.4 Results
We evaluate the performance of our frame identifi-
cation model given gold-standard targets and auto-
matically identified targets (?3); see Table 4.
Given gold-standard targets, our model is able
to predict frames for lemmas not seen in training,
of which there are 210. The partial-match evalua-
tion gives our model some credit for 190 of these,
4 of which are exactly correct. The hidden vari-
able model, then, is finding related (but rarely exact)
frames for unknown target words. The net effect of
our conservative target identifier on F1 is actually
positive: the frame identifier is far more precise for
targets seen explicitly in training. Together, our tar-
get and frame identification outperform the baseline
by 4 F1 points. To compare the frame identification
stage in isolation with that of J&N?07, we ran our
frame identification model with the targets identified
by their system as input. With partial matching, our
model achieves a relative improvement of 0.6% F1
over J&N?07 (though this is not significant).
While our frame identification model thus per-
forms on par with the current state of the art for
this task, it improves upon J&N?s formulation of
the problem because it requires only a single model,
learns lexical-semantic features as part of that model
rather than requiring a preprocessing step to expand
the vocabulary of frame-evoking words, and is prob-
abilistic, which can facilitate global reasoning.
5 Argument Identification
Given a sentence x = ?x1, . . . , xn?, the set of tar-
gets t = ?t1, . . . , tm?, and a list of evoked frames
952
f = ?f1, . . . , fm? corresponding to each target, ar-
gument identification is the task of choosing which
of each fi?s roles are filled, and by which parts of x.
This task is most similar to the problem of semantic
role labeling, but uses frame-specific labels that are
richer than the PropBank annotations.
5.1 Model
Let Rfi = {r1, . . . , r|Rfi |} denote frame fi?s roles
(named frame element types) observed in an exem-
plar sentence and/or our training set. A subset of
each frame?s roles are marked as core roles; these
roles are conceptually and/or syntactically necessary
for any given use of the frame, though they need
not be overt in every sentence involving the frame.
These are roughly analogous to the core arguments
A0?A5 and AA in PropBank. Non-core roles?
analogous to the various AMs in PropBank?loosely
correspond to syntactic adjuncts, and carry broadly-
applicable information such as the time, place, or
purpose of an event. The lexicon imposes some
additional structure on roles, including relations to
other roles in the same or related frames, and se-
mantic types with respect to a small ontology (mark-
ing, for instance, that the entity filling the protag-
onist role must be sentient for frames of cogni-
tion). Fig. 2 illustrates some of the structural ele-
ments comprising the frame lexicon by considering
the CAUSE TO MAKE NOISE frame.
We identify a set S of spans that are candidates for
filling any role r ? Rfi . In principle, S could con-
tain any subsequence of x, but in this work we only
consider the set of contiguous spans that (a) contain
a single word or (b) comprise a valid subtree of a
word and all its descendants in the dependency parse
produced by the MST parser. This covers 81% of ar-
guments in the development data. The empty span
is also included in S, since some roles are not ex-
plicitly filled; in the development data, the average
number of roles an evoked frame defines is 6.7, but
the average number of overt arguments is only 1.7.6
In training, if a labeled argument is not a valid sub-
6In the annotated data, each core role is filled with one of
three types of null instantiations indicating how the role is con-
veyed implicitly. E.g., the imperative construction implicitly
designates a role as filled by the addressee, and the correspond-
ing filler is thus CNI (constructional null instantiation). In this
work we do not distinguish different types of null instantiations.
tree of the dependency parse, we add its span to S .
Let Ai denote the mapping of roles in Rfi to
spans in S. Our model makes a prediction for each
Ai(rk) (for all roles rk ? Rfi) using:
Ai(rk)? argmaxs?S p(s | rk, fi, ti,x) (4)
We use a conditional log-linear model over spans for
each role of each evoked frame:
p?(Ai(rk) = s | fi, ti,x) = (5)
exp?>h(s, rk, fi, ti,x)
?
s??S exp?
>h(s?, rk, fi, ti,x)
Note that our model chooses the span for each
role separately from the other roles and ignores all
frames except the frame the role belongs to. Our
model departs from the traditional SRL literature by
modeling the argument identification problem in a
single stage, rather than first classifying token spans
as arguments and then labeling them. A constraint
implicit in our formulation restricts each role to have
at most one overt argument, which is consistent with
96.5% of the role instances in the training data.
Out of the overt argument spans in the training
data, 12% are duplicates, having been used by some
previous frame in the sentence (supposing some ar-
bitrary ordering of frames). Our role-filling model,
unlike a sentence-global argument detection-and-
classification approach,7 permits this sort of argu-
ment sharing among frames. The incidence of span
overlap among frames is much higher; Fig. 1 illus-
trates a case with a high degree of overlap. Word
tokens belong to an average of 1.6 argument spans
each, including the quarter of words that do not be-
long to any argument.
Features for our log-linear model (Eq. 5) depend
on the preprocessed sentence x; the target t; a
role r of frame f ; and a candidate argument span
s ? S. Our model includes lexicalized and unlexi-
calized features considering aspects of the syntactic
parse (most notably the dependency path in the parse
from the target to the argument); voice; word order-
ing/overlap/distance of the argument with respect to
the target; and POS tags within and around the argu-
ment. Many features have a version specific to the
frame and role, plus a smoothed version incorporat-
ing the role name, but not the frame. These features
7J&N?07, like us, identify arguments for each target.
953
are fully enumerated in (Das et al, 2010); instanti-
ating them for our data yields 1,297,857 parameters.
5.2 Training
We train the argument identification model by:
max
?
N?
j=1
mj?
i=1
|R
f
(j)
i
|
?
k=1
log p?(A
(j)
i (rk) | f
(j)
i , t
(j)
i ,x
(j))
(6)
This objective function is concave, and we globally
optimize it using stochastic gradient ascent (Bottou,
2004). We train this model until the argument iden-
tification F1 score stops increasing on the develop-
ment data. Best results on this dataset were obtained
with a batch size of 2 and 23 passes through the data.
5.3 Approximate Joint Decoding
Na??ve prediction of roles using Eq. 4 may result
in overlap among arguments filling different roles
of a frame, since the argument identification model
fills each role independently of the others. We want
to enforce the constraint that two roles of a single
frame cannot be filled by overlapping spans. We dis-
allow illegal overlap using a 10000-hypothesis beam
search; the algorithm is given in (Das et al, 2010).
5.4 Results
Performance of the argument identification model
is presented in Table 5. The table shows how per-
formance varies given different types of perfect in-
put: correct targets, correct frames, and the set of
correct spans; correct targets and frames, with the
heuristically-constructed set of candidate spans; cor-
rect targets only, with model frames; and ultimately,
no oracle input (the full frame parsing scenario).
The first four rows of results isolate the argu-
ment identification task from the frame identifica-
tion task. Given gold targets and frames and an ora-
cle set of argument spans, our local model achieves
about 87% precision and 75% recall. Beam search
decoding to eliminate illegal argument assignments
within a frame (?5.3) further improves precision by
about 1.6%, with negligible harm to recall. Note
that 96.5% recall is possible under the constraint that
roles are not multiply-filled (?5.1); there is thus con-
siderable room for improvement with this constraint
in place. Joint prediction of each frame?s arguments
is worth exploring to capture correlations not en-
coded in our local models or joint decoding scheme.
The 15-point drop in recall when the heuristically-
built candidate argument set replaces the set of true
argument spans is unsurprising: an estimated 19% of
correct arguments are excluded because they are nei-
ther single words nor complete subtrees (see ?5.1).
Qualitatively, the problem of candidate span recall
seems to be largely due to syntactic parse errors.8
Still, the 10-point decrease in precision when using
the syntactic parse to determine candidate spans sug-
gests that the model has trouble discriminating be-
tween good and bad arguments, and that additional
feature engineering or jointly decoding arguments of
a sentence?s frames may be beneficial in this regard.
The fifth and sixth rows show the effect of auto-
matic frame identification on overall frame parsing
performance. There is a 22% decrease in F1 (18%
when partial credit is given for related frames), sug-
gesting that improved frame identification or joint
prediction of frames and arguments is likely to have
a sizeable impact on overall performance.
The final two rows of the table compare our full
model (target, frame, and argument identification)
with the baseline, showing significant improvement
of more than 4.4 F1 points for both exact and partial
frame matching. As with frame identification, we
compared the argument identification stage with that
of J&N?07 in isolation, using the automatically iden-
tified targets and frames from the latter as input to
our model. With partial frame matching, this gave us
an F1 score of 48.1% on the test set?significantly
better (p < 0.05) than 45.6%, the full parsing re-
sult from J&N?07. This indicates that our argument
identification model?which uses a single discrim-
inative model with a large number of features for
role filling (rather than argument labeling)?is more
powerful than the previous state of the art.
6 Related work
Since Gildea and Jurafsky (2002) pioneered statis-
tical semantic role labeling, a great deal of com-
8Note that, because of our labels-only evaluation scheme
(?2.3), arguments missing a word or containing an extra word
receive no credit. In fact, of the frame roles correctly predicted
as having an overt span, the correct span was predicted 66% of
the time, while 10% of the time the predicted starting and end-
ing boundaries of the span were off by a total of 1 or 2 words.
954
ARGUMENT IDENTIFICATION exact frame matching
targets frames spans decoding P R F1
Argument identifica-
tion (oracle spans)
? ? ? na??ve 86.61 75.11 80.45
? ? ? beam ?5.3 88.29 74.77 80.97
Argument identifica-
tion (full)
? ? model ?5 na??ve 77.43 60.76 68.09 partial frame matching
? ? model ?5 beam ?5.3 78.71 60.57 68.46 P R F1
Parsing (oracle targets) ? model ?4 model ?5 beam ?5.3 49.68 42.82 46.00 57.85 49.86 53.56
Parsing (full) auto ?3 model ?4 model ?5 beam ?5.3 58.08 38.76 46.49 62.76 41.89 50.24
Baseline: J&N?07 auto model model N/A 51.59 35.44 42.01 56.01 38.48 45.62
Table 5. Argument identification results. ? indicates that gold-standard labels were used for a given pipeline stage.
For full parsing, bolded scores indicate significant improvements relative to the baseline (p < 0.05).
putational work has investigated predicate-argument
structures for semantics. Briefly, we highlight some
relevant work, particularly research that has made
use of FrameNet. (Note that much related research
has focused on PropBank (Kingsbury and Palmer,
2002), a set of shallow predicate-argument annota-
tions for Wall Street Journal articles from the Penn
Treebank (Marcus et al, 1993); a recent issue of CL
(Ma`rquez et al, 2008) was devoted to the subject.)
Most work on frame-semantic role labeling has
made use of the exemplar sentences in the FrameNet
corpus (see ?2.1), each of which is annotated for a
single frame and its arguments. On the probabilis-
tic modeling front, Gildea and Jurafsky (2002) pre-
sented a discriminative model for arguments given
the frame; Thompson et al (2003) used a gener-
ative model for both the frame and its arguments;
and Fleischman et al (2003) first used maximum
entropy models to find and label arguments given
the frame. Shi and Mihalcea (2004) developed a
rule-based system to predict frames and their argu-
ments in text, and Erk and Pado? (2006) introduced
the Shalmaneser tool, which employs Na??ve Bayes
classifiers to do the same. Other FrameNet SRL
systems (Giuglea and Moschitti, 2006, for instance)
have used SVMs. Most of this work was done on an
older, smaller version of FrameNet.
Recent work on frame-semantic parsing?in
which sentences may contain multiple frames to be
recognized along with their arguments?has used
the SemEval?07 data (Baker et al, 2007). The LTH
system of Johansson and Nugues (2007), our base-
line (?2.4), performed the best in the SemEval?07
task. Matsubayashi et al (2009) trained a log-
linear model on the SemEval?07 data to evaluate
argument identification features exploiting various
types of taxonomic relations to generalize over roles.
A line of work has sought to extend the coverage
of FrameNet by exploiting VerbNet, WordNet, and
Wikipedia (Shi and Mihalcea, 2005; Giuglea and
Moschitti, 2006; Pennacchiotti et al, 2008; Tonelli
and Giuliano, 2009), and projecting entries and an-
notations within and across languages (Boas, 2002;
Fung and Chen, 2004; Pado? and Lapata, 2005;
Fu?rstenau and Lapata, 2009). Others have applied
frame-semantic structures to question answering,
paraphrase/entailment recognition, and information
extraction (Narayanan and Harabagiu, 2004; Shen
and Lapata, 2007; Pado? and Erk, 2005; Burchardt,
2006; Moschitti et al, 2003; Surdeanu et al, 2003).
7 Conclusion
We have provided a supervised model for rich
frame-semantic parsing, based on a combination
of knowledge from FrameNet, two probabilistic
models trained on SemEval?07 data, and expedi-
ent heuristics. Our system achieves improvements
over the state of the art at each stage of process-
ing and collectively, and is amenable to future ex-
tension. Our parser is available for download at
http://www.ark.cs.cmu.edu/SEMAFOR.
Acknowledgments
We thank Collin Baker, Katrin Erk, Richard Johansson,
and Nils Reiter for software, data, evaluation scripts, and
methodological details. We thank the reviewers, Alan
Black, Ric Crabbe, Michael Ellsworth, Rebecca Hwa,
Dan Klein, Russell Lee-Goldman, Dan Roth, Josef Rup-
penhofer, and members of the ARK group for helpful
comments. This work was supported by DARPA grant
NBCH-1080004, NSF grant IIS-0836431, and computa-
tional resources provided by Yahoo.
955
References
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-
2007 Task 19: frame semantic structure extraction. In
Proc. of SemEval.
H. C. Boas. 2002. Bilingual FrameNet dictionaries for
machine translation. In Proc. of LREC.
L. Bottou. 2004. Stochastic learning. In Advanced Lec-
tures on Machine Learning. Springer-Verlag.
A. Burchardt, K. Erk, and A. Frank. 2005. A WordNet
detour to FrameNet. In B. Fisseni, H.-C. Schmitz,
B. Schro?der, and P. Wagner, editors, Sprachtech-
nologie, mobile Kommunikation und linguistische Re-
sourcen, volume 8. Peter Lang.
A. Burchardt. 2006. Approaching textual entailment
with LFG and FrameNet frames. In Proc. of the Sec-
ond PASCAL RTE Challenge Workshop.
D. Das, N. Schneider, D. Chen, and N. A. Smith.
2010. SEMAFOR 1.0: A probabilistic frame-semantic
parser. Technical Report CMU-LTI-10-001, Carnegie
Mellon University.
K. Erk and S. Pado?. 2006. Shalmaneser - a toolchain for
shallow semantic parsing. In Proc. of LREC.
C. Fellbaum, editor. 1998. WordNet: an electronic lexi-
cal database. MIT Press, Cambridge, MA.
C. J. Fillmore, C. R. Johnson, and M. R.L. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16(3).
C. J. Fillmore. 1982. Frame semantics. In Linguistics in
the Morning Calm, pages 111?137. Hanshin Publish-
ing Co., Seoul, South Korea.
M. Fleischman, N. Kwon, and E. Hovy. 2003. Maximum
entropy models for FrameNet classification. In Proc.
of EMNLP.
P. Fung and B. Chen. 2004. BiFrameNet: bilin-
gual frame semantics resource construction by cross-
lingual induction. In Proc. of COLING.
H. Fu?rstenau and M. Lapata. 2009. Semi-supervised se-
mantic role labeling. In Proc. of EACL.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3).
A.-M. Giuglea and A. Moschitti. 2006. Shallow
semantic parsing based on FrameNet, VerbNet and
PropBank. In Proc. of ECAI 2006.
R. Johansson and P. Nugues. 2007. LTH: semantic struc-
ture extraction using nonprojective dependency trees.
In Proc. of SemEval.
R. Johansson and P. Nugues. 2008. Dependency-based
semantic role labeling of PropBank. In Proc. of
EMNLP.
P. Kingsbury and M. Palmer. 2002. From TreeBank to
PropBank. In Proc. of LREC.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
the Penn Treebank. Computational Linguistics, 19(2).
L. Ma`rquez, X. Carreras, K. C. Litkowski, and S. Steven-
son. 2008. Semantic role labeling: an introduction to
the special issue. Computational Linguistics, 34(2).
Y. Matsubayashi, N. Okazaki, and J. Tsujii. 2009. A
comparative study on generalization of semantic roles
in FrameNet. In Proc. of ACL-IJCNLP.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.
A. Moschitti, P. Mora?rescu, and S. M. Harabagiu. 2003.
Open-domain information extraction via automatic se-
mantic labeling. In Proc. of FLAIRS.
S. Narayanan and S. Harabagiu. 2004. Question answer-
ing based on semantic structures. In Proc. of COLING.
S. Pado? and K. Erk. 2005. To cause or not to cause:
cross-lingual semantic matching for paraphrase mod-
elling. In Proc. of the Cross-Language Knowledge In-
duction Workshop.
S. Pado? and M. Lapata. 2005. Cross-linguistic projec-
tion of role-semantic information. In Proc. of HLT-
EMNLP.
M. Pennacchiotti, D. De Cao, R. Basili, D. Croce, and
M. Roth. 2008. Automatic induction of FrameNet
lexical units. In Proc. of EMNLP.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. of EMNLP.
D. Shen and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proc. of EMNLP-
CoNLL.
L. Shi and R. Mihalcea. 2004. An algorithm for open
text semantic parsing. In Proc. of Workshop on Robust
Methods in Analysis of Natural Language Data.
L. Shi and R. Mihalcea. 2005. Putting pieces together:
combining FrameNet, VerbNet and WordNet for ro-
bust semantic parsing. In Computational Linguis-
tics and Intelligent Text Processing: Proc. of CICLing
2005. Springer-Verlag.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using predicate-argument structures for infor-
mation extraction. In Proc. of ACL.
C. A. Thompson, R. Levy, and C. D. Manning. 2003. A
generative model for semantic role labeling. In Proc.
of ECML.
S. Tonelli and C. Giuliano. 2009. Wikipedia as frame
information repository. In Proc. of EMNLP.
956
