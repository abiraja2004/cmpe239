Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 16?23,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Using WordNet and Semantic Similarity for Bilingual Terminology
Mining from Comparable Corpora
Dhouha Bouamor
CEA, LIST, Vision and
Content Engineering Laboratory,
91191 Gif-sur-Yvette CEDEX
France
dhouha.bouamor@cea.fr
Nasredine Semmar
CEA, LIST, Vision and Content
Engineering Laboratory,
91191 Gif-sur-Yvette
CEDEX France
nasredine.semmar@cea.fr
Pierre Zweigenbaum
LIMSI-CNRS,
F-91403 Orsay CEDEX
France
pz@limsi.fr
Abstract
This paper presents an extension of the
standard approach used for bilingual lex-
icon extraction from comparable corpora.
We study of the ambiguity problem re-
vealed by the seed bilingual dictionary
used to translate context vectors. For
this purpose, we augment the standard ap-
proach by a Word Sense Disambiguation
process relying on a WordNet-based se-
mantic similarity measure. The aim of
this process is to identify the translations
that are more likely to give the best rep-
resentation of words in the target lan-
guage. On two specialized French-English
comparable corpora, empirical experimen-
tal results show that the proposed method
consistently outperforms the standard ap-
proach.
1 Introduction
Bilingual lexicons play a vital role in many Natu-
ral Language Processing applications such as Ma-
chine Translation (Och and Ney, 2003) or Cross-
Language Information Retrieval (Shi, 2009). Re-
search on lexical extraction from multilingual cor-
pora have largely focused on parallel corpora. The
scarcity of such corpora in particular for special-
ized domains and for language pairs not involv-
ing English pushed researchers to investigate the
use of comparable corpora (Fung, 1998; Chiao
and Zweigenbaum, 2003). These corpora are com-
prised of texts which are not exact translation of
each other but share common features such as do-
main, genre, sampling period, etc.
The main work in this research area could be
seen as an extension of Harris?s distributional hy-
pothesis (Harris, 1954). It is based on the sim-
ple observation that a word and its translation are
likely to appear in similar contexts across lan-
guages (Rapp, 1995). Based on this assumption,
the alignment method, known as the standard ap-
proach builds and compares context vectors for
each word of the source and target languages.
A particularity of this approach is that, to enable
the comparison of context vectors, it requires the
existence of a seed bilingual dictionary to translate
source context vectors. The use of the bilingual
dictionary is problematic when a word has sev-
eral translations, whether they are synonymous or
polysemous. For instance, the French word action
can be translated into English as share, stock, law-
suit or deed. In such cases, it is difficult to iden-
tify in flat resources like bilingual dictionaries,
wherein entries are usually unweighted and un-
ordered, which translations are most relevant. The
standard approach considers all available trans-
lations and gives them the same importance in
the resulting translated context vectors indepen-
dently of the domain of interest and word ambigu-
ity. Thus, in the financial domain, translating ac-
tion into deed or lawsuit would probably introduce
noise in context vectors.
In this paper, we present a novel approach
which addresses the word ambiguity problem ne-
glected in the standard approach. We introduce a
use of a WordNet-based semantic similarity mea-
sure permitting the disambiguation of translated
context vectors. The basic intuition behind this
method is that instead of taking all translations
of each seed word to translate a context vector,
we only use the translations that are more likely
to give the best representation of the context vec-
tor in the target language. We test the method on
two specialized French-English comparable cor-
16
pora (financial and medical) and report improved
results, especially when many of the words in the
corpus are ambiguous.
The remainder of the paper is organized as fol-
lows: Section 2 presents the standard approach
and recalls in some details previous work address-
ing the task of bilingual lexicon extraction from
comparable corpora. In section 3 we present our
context disambiguation process. Before conclud-
ing and presenting directions for future work, we
describe in section 4 the experimental protocol we
followed and discuss the obtained results.
2 Bilingual lexicon extraction
2.1 Standard Approach
Most previous works addressing the task of bilin-
gual lexicon extraction from comparable corpora
are based on the standard approach (Fung, 1998;
Chiao and Zweigenbaum, 2002; Laroche and
Langlais, 2010). Formally, this approach is com-
posed of the following three steps:
1. Building context vectors: Vectors are first
extracted by identifying the words that appear
around the term to be translated S in a win-
dow of N words. Generally, an association
measure like the mutual information (Morin
and Daille, 2006), the log-likelihood (Morin
and Prochasson, 2011) or the Discounted
Odds-Ratio (Laroche and Langlais, 2010) are
employed to shape the context vectors.
2. Translation of context vectors: To enable
the comparison of source and target vectors,
source terms vectors are translated in the tar-
get language by using a seed bilingual dic-
tionary. Whenever it provides several trans-
lations for an element, all proposed transla-
tions are considered. Words not included in
the bilingual dictionary are simply ignored.
3. Comparison of source and target vectors:
Translated vectors are compared to target
ones using a similarity measure. The most
widely used is the cosine similarity, but
many authors have studied alternative metrics
such as the Weighted Jaccard index (Prochas-
son et al, 2009) or the City-Block dis-
tance (Rapp, 1999). According to similarity
values, a ranked list of translations for S is
obtained.
2.2 Related Work
Recent improvements of the standard approach are
based on the assumption that the more the con-
text vectors are representative, the better the bilin-
gual lexicon extraction is. Prochasson et al (2009)
used transliterated words and scientific compound
words as ?anchor points?. Giving these words
higher priority when comparing target vectors im-
proved bilingual lexicon extraction. In addition to
transliteration, Rubino and Linare`s (2011) com-
bined the contextual representation within a the-
matic one. The basic intuition of their work is that
a term and its translation share thematic similari-
ties. Hazem and Morin (2012) recently proposed a
method that filters the entries of the bilingual dic-
tionary based upon POS-tagging and domain rel-
evance criteria, but no improvements was demon-
strated.
Gaussier et al (2004) attempted to solve the
problem of different word ambiguities in the
source and target languages. They investigated a
number of techniques including canonical corre-
lation analysis and multilingual probabilistic la-
tent semantic analysis. The best results, with a
very small improvement were reported for a mixed
method. One important difference with Gaussier
et al (2004) is that they focus on words ambigu-
ities on source and target languages, whereas we
consider that it is sufficient to disambiguate only
translated source context vectors.
A large number of Word Sense Disambigua-
tion WSD techniques were previously proposed
in the literature. The most popular ones are those
that compute semantic similarity with the help
of existing thesauri such as WordNet (Fellbaum,
1998). This resource groups English words into
sets of synonyms called synsets, provides short,
general definitions and records various semantic
relations (hypernymy, meronymy, etc.) between
these synonym sets. This thesaurus has been ap-
plied to many tasks relying on word-based sim-
ilarity, including document (Hwang et al, 2011)
and image (Cho et al, 2007; Choi et al, 2012)
retrieval systems. In this work, we use this re-
source to derive a semantic similarity between lex-
ical units within the same context vector. To the
best of our knowledge, this is the first application
of WordNet to the task of bilingual lexicon extrac-
tion from comparable corpora.
17
  
Word?to?be?translated?(source?language)
Building?Context?Vector
Context?vector?  Translated?Context?vector
Bilingual?Dictionary WordNet
Disambiguated??Context?vector
Context?Vectors?(Target?language)
Figure 1: Overall architecture of the lexical extraction approach
3 Context Vector Disambiguation
The approach we propose includes the three steps
of the standard approach. As it was mentioned in
section 1, when lexical extraction applies to a spe-
cific domain, not all translations in the bilingual
dictionary are relevant for the target context vec-
tor representation. For this reason, we introduce
a WordNet-based WSD process that aims at im-
proving the adequacy of context vectors and there-
fore improve the results of the standard approach.
Figure 1 shows the overall architecture of the lexi-
cal extraction process. Once translated into the tar-
get language, the context vectors disambiguation
process intervenes. This process operates locally
on each context vector and aims at finding the
most prominent translations of polysemous words.
For this purpose, we use monosemic words as a
seed set of disambiguated words to infer the pol-
ysemous word?s translations senses. We hypoth-
esize that a word is monosemic if it is associated
to only one entry in the bilingual dictionary. We
checked this assumption by probing monosemic
entries of the bilingual dictionary against WordNet
and found that 95% of the entries are monosemic
in both resources.
Formally, we derive a semantic similarity value
between all the translations provided for each pol-
ysemous word by the bilingual dictionary and
all monosemic words appearing whithin the same
context vector. There is a relatively large number
of word-to-word similarity metrics that were pre-
viously proposed in the literature, ranging from
path-length measures computed on semantic net-
works, to metrics based on models of distribu-
tional similarity learned from large text collec-
tions. For simplicity, we use in this work, the Wu
and Palmer (1994) (WUP) path-length-based se-
mantic similarity measure. It was demonstrated by
(Lin, 1998) that this metric achieves good perfor-
mances among other measures. WUP computes a
score (equation 1) denoting how similar two word
senses are, based on the depth of the two synsets
(s1 and s2) in the WordNet taxonomy and that of
their Least Common Subsumer (LCS), i.e., the
most specific word that they share as an ancestor.
WupSim(s1, s2) =
2? depth(LCS)
depth(s1) + depth(s2)
(1)
In practice, since a word can belong to more
than one synset in WordNet, we determine the
semantic similarity between two words w1 and
w2 as the maximum WupSim between the synset
or the synsets that include the synsets(w1) and
synsets(w2) according to the following equation:
SemSim(w1, w2) = max{WupSim(s1, s2);
(s1, s2) ? synsets(w1)? synsets(w2)} (2)
18
Context Vector Translations Comparison Ave Sim
liquidite? liquidity ? ?
action
act SemSim(act,liquidity), SemSim(act,dividend) 0.2139
action SemSim(action,liquidity), SemSim(action,dividend) 0.4256
stock SemSim(stock,liquidity), SemSim(stock,dividend) 0.5236
deed SemSim(deed,liquidity), SemSim(deed,dividend) 0.1594
lawsuit SemSim(lawsuit,liquidity), SemSim(lawsuit,dividend) 0.1212
fact SemSim(fact,liquidity), SemSim(fact,dividend) 0.1934
operation SemSim(operation,liquidity), SemSim(operation,dividend) 0.2045
share SemSim(share,liquidity), SemSim(share,dividend) 0.5236
plot SemSim(plot,liquidity), SemSim(plot,dividend) 0.2011
dividende dividend ? ?
Table 1: Disambiguation of the context vector of the French term be?ne?fice [income] in the corporate
finance domain. liquidite? and dividende are monosemic and are used to infer the most similar translations
of the term action.
Then, to identify the most prominent translations
of each polysemous unit wp, an average similarity
is computed for each translation wjp of wp:
Ave Sim(wjp) =
?N
i=1 SemSim(wi, w
j
p)
N
(3)
where N is the total number of monosemic words
and SemSim is the similarity value of w
j
p and the
ith monosemic word. Hence, according to average
relatedness values Ave Sim(wjp), we obtain for
each polysemous word wp an ordered list of trans-
lations w1p . . . w
n
p . This allows us to select trans-
lations of words which are more salient than the
others to represent the word to be translated.
In Table 1, we present the results of the dis-
ambiguation process for the context vector of the
French term be?ne?fice in the corporate finance cor-
pus. This vector contains the words action, div-
idende, liquidite? and others. The bilingual dic-
tionary provides the following translations {act,
stock, action, deed, lawsuit, fact, operation, plot,
share} for the French polysemous word action.
We use the monosemic words dividende and liq-
uidite? to disambiguate the word action. From ob-
serving average similariy values (Ave Sim), we
notice that the words share and stock are on the
top of the list and therefore are most likely to rep-
resent the source word action in this context.
Corpus French English
Corporate finance 402, 486 756, 840
Breast cancer 396, 524 524, 805
Table 2: Comparable corpora sizes in term of
words.
4 Experiments and Results
4.1 Resources
4.1.1 Comparable corpora
We conducted our experiments on two French-
English comparable corpora specialized on
the corporate finance and the breast cancer
domains. Both corpora were extracted from
Wikipedia1. We consider the topic in the source
language (for instance finance des entreprises
[corporate finance]) as a query to Wikipedia
and extract all its sub-topics (i.e., sub-categories
in Wikipedia) to construct a domain-specific
category tree. A sample of the corporate fi-
nance sub-domain?s category tree is shown in
Figure 2. Then, based on the constructed tree,
we collect all Wikipedia pages belonging to one
of these categories and use inter-language links
to build the comparable corpus. Both corpora
were normalized through the following linguistic
preprocessing steps: tokenisation, part-of-speech
tagging, lemmatisation, and function word re-
moval. The resulting corpora2 sizes are given in
Table 2.
1http://dumps.wikimedia.org/
2Comparable corpora will be shared publicly
19
  
Finance?des?entreprise?[Corporate?Finance]
Analyse?Financi?re?[Financial?Analysis] Comptabilit??g?n?rale[Financial?accountancy] Indicateur?Financier[Financial?ratios]
Risque[Risk] Cr?dit[Credit]Actifs[Asset] Bilan[Balance?sheet] Salaire[Salary]Solde[Balance]
B?n?fice[profit] Revenu[Income]
...?...?
Figure 2: Wikipedia categories tree of the corporate finance sub-domain.
4.1.2 Bilingual dictionary
The bilingual dictionary used to translate context
vectors consists of an in-house manually revised
bilingual dictionary which contains about 120,000
entries belonging to the general domain. It is im-
portant to note that words on both corpora has on
average, 7 translations in the bilingual dictionary.
4.1.3 Evaluation list
In bilingual terminology extraction from compa-
rable corpora, a reference list is required to eval-
uate the performance of the alignment. Such
lists are usually composed of about 100 sin-
gle terms (Hazem and Morin, 2012; Chiao and
Zweigenbaum, 2002). Here, we created two refer-
ence lists3 for the corporate finance and the breast
cancer domains. The first list is composed of 125
single terms extracted from the glossary of bilin-
gual micro-finance terms4. The second list con-
tains 96 terms extracted from the French-English
MESH and the UMLS thesauri5. Note that refer-
ence terms pairs appear at least five times in each
part of both comparable corpora.
4.2 Experimental setup
Three other parameters need to be set up: (1) the
window size, (2) the association measure and the
(3) similarity measure. To define context vectors,
we use a seven-word window as it approximates
syntactic dependencies. Concerning the rest of the
3Reference lists will be shared publicly
4http://www.microfinance.lu/en/
5http://www.nlm.nih.gov/
parameters, we followed Laroche and Langlais
(2010) for their definition. The authors carried out
a complete study of the influence of these param-
eters on the bilingual alignment and showed that
the most effective configuration is to combine the
Discounted Log-Odds ratio (equation 4) with the
cosine similarity. The Discounted Log-Odds ratio
is defined as follows:
Odds-Ratiodisc = log
(O11 + 12)(O22 +
1
2)
(O12 + 12)(O21 +
1
2)
(4)
where Oij are the cells of the 2 ? 2 contingency
matrix of a token s co-occurring with the term S
within a given window size.
4.3 Results and discussion
It is difficult to compare results between different
studies published on bilingual lexicon extraction
from comparable corpora, because of difference
between (1) used corpora (in particular their con-
struction constraints and volume), (2) target do-
mains, and also (3) the coverage and relevance of
linguistic resources used for translation. To the
best of our knowledge, there is no common bench-
mark that can serve as a reference. For this reason,
we use the results of the standard approach (SA)
described in section 2.1 as a reference. We evalu-
ate the performance of both the SA and ours with
respect to TopN precision (PN ), recall (RN ) and
Mean Reciprocal Rank (MRR) (Voorhees, 1999).
Precision is the total number of correct translations
divided by the number of terms for which the sys-
tem gave at least one answer. Recall is equal to
20
a)
C
or
po
ra
te
F
in
an
ce
Method P1 P10 P20 R1 R10 R20 MRR
Standard Approach (SA) 0.046 0.140 0.186 0.040 0.120 0.160 0.064
WN-T1 0.065 0.196 0.261 0.056 0.168 0.224 0.089
WN-T2 0.102 0.252 0.308 0.080 0.216 0.264 0.122
WN-T3 0.102 0.242 0.327 0.088 0.208 0.280 0.122
WN-T4 0.112 0.224 0.299 0.090 0.190 0.250 0.124
WN-T5 0.093 0.205 0.280 0.080 0.176 0.240 0.110
WN-T6 0.084 0.205 0.233 0.072 0.176 0.200 0.094
WN-T7 0.074 0.177 0.242 0.064 0.152 0.208 0.090
b)
B
re
as
tC
an
ce
r
Method P1 P10 P20 R1 R10 R20 MRR
Standard Approach (SA) 0.342 0.542 0.585 0.250 0.395 0.427 0.314
WN-T1 0.257 0.500 0.571 0.187 0.364 0.416 0.257
WN-T2 0.314 0.614 0.671 0.229 0.447 0.489 0.313
WN-T3 0.342 0.628 0.671 0.250 0.458 0.489 0.342
WN-T4 0.342 0.571 0.642 0.250 0.416 0.468 0.332
WN-T5 0.357 0.571 0.657 0.260 0.416 0.479 0.348
WN-T6 0.357 0.571 0.652 0.260 0.416 0.468 0.347
WN-T7 0.357 0.585 0.657 0.260 0.427 0.479 0.339
Table 3: Precision, Recall at TopN (N=1,10,20) and MRR at Top20 for the two domains. In each column,
bold show best results. Underline show best results overall.
the ratio of correct translation to the total number
of terms. The MRR takes into account the rank
of the first good translation found for each entry.
Formally, it is defined as:
MRR =
1
Q
i=1?
|Q|
1
ranki
(5)
where Q is the total number of terms to be trans-
lated and ranki is the position of the first correct
translation in the translations candidates.
Our method provides a ranked list of transla-
tions for each polysemous word. A question that
arises here is whether we should introduce only
the best ranked translation in the context vector
or consider a larger number of words, especially
when a translations list contain synonyms (share
and stock in Table 1). For this reason, we take
into account in our experiments different number
of translations, noted WN-Ti, ranging from the
pivot translation (i = 1) to the seventh word in the
translations list. This choice is motivated by the
fact that words in both corpora have on average 7
translations in the bilingual dictionary. The base-
line (SA) uses all translations associated to each
entry in the bilingual dictionary. Table 3a displays
the results obtained for the corporate finance cor-
pus. The first substantial observation is that our
method which consists in disambiguating polyse-
mous words within context vectors consistently
outperforms the standard approach (SA) for all
configurations. The best MRR is reported when
for each polysemous word, we keep the most simi-
lar four translations (WN-T4) in the context vector
of the term to be translated. However, the highest
Top20 precision and recall are obtained by WN-
T3. Using the top three word translations in the
vector boosts the Top20 precision from 0.186 to
0.327 and the Top20 recall from 0.160 to 0.280.
Concerning the Breast Cancer corpus, slightly dif-
ferent results were obtained. As Table 3b show,
when the context vectors are totally disambiguated
(i.e. each source unit is translated by at most one
word in context vectors), all TopN precision, re-
call and MRR decrease. However, we report im-
provements against the SA in most other cases.
For WN-T5, we obtain the maximum MRR score
with an improvement of +0.034 over the SA. But,
as for the corporate finance corpus, the best Top20
precision and recall are reached by the WN-T3
method, with a gain of +0.082 in both Top10 and
Top20 precision and of about +0.06 in Top10 and
Top20 recall.
From observing result tables of both corporate
finance and breast cancer domains, we notice that
our approach performs better than the SA but with
different degrees. The improvements achieved in
21
Corpus Corpus PR Vectors PR
Corporate finance 41% 91, 6%
Breast cancer 47% 85, 1%
Table 4: Comparable corpora?s and context vec-
tor?s Polysemy Rates PR.
the corporate finance domain are higher than those
reported in the breast cancer domain. The reason
being that the vocabulary used in the breast cancer
corpus is more specific and therefore less ambigu-
ous than that used in corporate finance texts. The
results given in table 4 validate this assumption. In
this table, we give the polysemy rates of the com-
parable corpora (Corpus PR) and that of context
vectors (Vectors PR). PR indicates the percent-
age of words that are associated to more than one
translation in the bilingual dictionary. The results
show that breast cancer corpus is more polysemic
than that of the corporate finance. Nevertheless,
even if in both corpora, the candidates? context
vectors are highly polysemous, breast cancer?s
context vectors are less polysemous than those of
the corporate finance texts. In this corpus, 91, 6%
of the words used as entries to define context vec-
tors are polysemous. This shows that the ambi-
guity present in specialized comparable corpora
hampers bilingual lexicon extraction, and that dis-
ambiguation positively affects the overall results.
Even though the two corpora are fairly different
(subject and polysemy rate), the optimal Top20
precision and recall results are obtained when con-
sidering up to three most similar translations in
context vectors. This behavior shows that the dis-
ambiguation method is relatively robust to domain
change. We notice also that the addition of supple-
mentary translations, which are probably noisy in
the given domain, degrades the overall results but
remains greater than the SA.
5 Conclusion
We presented in this paper a novel method that
extends the standard approach used for bilin-
gual lexicon extraction from comparable corpora.
The proposed method disambiguates polysemous
words in context vectors and selects only the trans-
lations that are most relevant to the general con-
text of the corpus. Conducted experiments on two
highly polysemous specialized comparable cor-
pora show that integrating such process leads to
a better performance than the standard approach.
Although our initial experiments are positive, we
believe that they could be improved in a number
of ways. In addition to the metric defined by (Wu
and Palmer, 1994), we plan to apply other seman-
tic similarity and relatedness measures and com-
pare their performance. It would also be interest-
ing to mine much more larger comparable corpora
and focus on their quality as presented in (Li and
Gaussier, 2010). We want also to test our method
on bilingual lexicon extraction for a larger panel of
specialized corpora, where disambiguation meth-
ods are needed to prune translations that are irrel-
evant to the domain.
References
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in
specialized, comparable corpora. In Proceedings of
the 19th international conference on Computational
linguistics - Volume 2, COLING ?02, pages 1?5. As-
sociation for Computational Linguistics.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2003.
The effect of a general lexicon in corpus-based iden-
tification of french-english medical word transla-
tions. In Proceedings Medical Informatics Europe,
volume 95 of Studies in Health Technology and In-
formatics, pages 397?402, Amsterdam.
Miyoung Cho, Chang Choi, Hanil Kim, Jungpil Shin,
and PanKoo Kim. 2007. Efficient image retrieval
using conceptualization of annotated images. Lec-
ture Notes in Computer Science, pages 426?433.
Springer.
Dongjin Choi, Jungin Kim, Hayoung Kim, Myungg-
won Hwang, and Pankoo Kim. 2012. A method for
enhancing image retrieval based on annotation using
modified wup similarity in wordnet. In Proceed-
ings of the 11th WSEAS international conference
on Artificial Intelligence, Knowledge Engineering
and Data Bases, AIKED?12, pages 83?87, Stevens
Point, Wisconsin, USA. World Scientific and Engi-
neering Academy and Society (WSEAS).
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Pascale Fung. 1998. A statistical view on bilingual
lexicon extraction: From parallel corpora to non-
parallel corpora. In Parallel Text Processing, pages
1?17. Springer.
E?ric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herve? De?jean. 2004. A geometric
view on bilingual lexicon extraction from compara-
ble corpora. In ACL, pages 526?533.
Z.S. Harris. 1954. Distributional structure. Word.
22
Amir Hazem and Emmanuel Morin. 2012. Adap-
tive dictionary for bilingual lexicon extraction from
comparable corpora. In Proceedings, 8th interna-
tional conference on Language Resources and Eval-
uation (LREC), Istanbul, Turkey, May.
Myunggwon Hwang, Chang Choi, and Pankoo Kim.
2011. Automatic enrichment of semantic relation
network and its application to word sense disam-
biguation. IEEE Transactions on Knowledge and
Data Engineering, 23:845?858.
Audrey Laroche and Philippe Langlais. 2010. Re-
visiting context-based projection methods for term-
translation spotting in comparable corpora. In 23rd
International Conference on Computational Lin-
guistics (Coling 2010), pages 617?625, Beijing,
China, Aug.
Bo Li and E?ric Gaussier. 2010. Improving corpus
comparability for bilingual lexicon extraction from
comparable corpora. In 23rd International Confer-
ence on Computational Linguistics (Coling 2010),
Beijing, China, Aug.
Dekang Lin. 1998. An information-theoretic def-
inition of similarity. In Proceedings of the Fif-
teenth International Conference on Machine Learn-
ing, ICML ?98, pages 296?304, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Emmanuel Morin and Be?atrice Daille. 2006. Com-
parabilite? de corpus et fouille terminologique mul-
tilingue. In Traitement Automatique des Langues
(TAL).
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceed-
ings, 4th Workshop on Building and Using Compa-
rable Corpora (BUCC), page 27?34, Portland, Ore-
gon, USA.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Emmanuel Prochasson, Emmanuel Morin, and Kyo
Kageura. 2009. Anchor points for bilingual lexi-
con extraction from small comparable corpora. In
Proceedings, 12th Conference on Machine Transla-
tion Summit (MT Summit XII), page 284?291, Ot-
tawa, Ontario, Canada.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd an-
nual meeting on Association for Computational Lin-
guistics, ACL ?95, pages 320?322. Association for
Computational Linguistics.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th annual meet-
ing of the Association for Computational Linguistics
on Computational Linguistics, ACL ?99, pages 519?
526. Association for Computational Linguistics.
Raphae?l Rubino and Georges Linare`s. 2011. A multi-
view approach for term translation spotting. In
Computational Linguistics and Intelligent Text Pro-
cessing, Lecture Notes in Computer Science, pages
29?40.
Lei Shi. 2009. Adaptive web mining of bilingual
lexicons for cross language information retrieval.
In Proceedings of the 18th ACM conference on In-
formation and knowledge management, CIKM ?09,
pages 1561?1564, New York, NY, USA. ACM.
Ellen M. Voorhees. 1999. The trec-8 question an-
swering track report. In In Proceedings of TREC-8,
pages 77?82.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, ACL ?94, pages 133?138. Association
for Computational Linguistics.
23
