Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 386?391,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
LIG System for WMT13 QE Task: Investigating the Usefulness of
Features in Word Confidence Estimation for MT
Ngoc-Quang Luong Benjamin Lecouteux
LIG, Campus de Grenoble
41, Rue des Mathe?matiques,
UJF - BP53, F-38041 Grenoble Cedex 9, France
{ngoc-quang.luong,laurent.besacier,benjamin.lecouteux}@imag.fr
Laurent Besacier
Abstract
This paper presents the LIG?s systems
submitted for Task 2 of WMT13 Qual-
ity Estimation campaign. This is a
word confidence estimation (WCE) task
where each participant was asked to la-
bel each word in a translated text as
a binary ( Keep/Change) or multi-class
(Keep/Substitute/Delete) category. We in-
tegrate a number of features of various
types (system-based, lexical, syntactic and
semantic) into the conventional feature
set, for our baseline classifier training.
After the experiments with all features,
we deploy a ?Feature Selection? strategy
to keep only the best performing ones.
Then, a method that combines multiple
?weak? classifiers to build a strong ?com-
posite? classifier by taking advantage of
their complementarity is presented and ex-
perimented. We then select the best sys-
tems for submission and present the offi-
cial results obtained.
1 Introduction
Recently Statistical Machine Translation (SMT)
systems have shown impressive gains with many
fruitful results. While the outputs are more accept-
able, the end users still face the need to post edit
(or not) an automatic translation. Then, the issue
is to be able to accurately identify the correct parts
as well as detecting translation errors. If we fo-
cus on errors at the word level, the issue is called
Word-level Confidence Estimation (WCE).
In WMT 2013, a shared task about quality esti-
mation is proposed. This quality estimation task is
proposed at two levels: word-level and sentence-
level. Our work focuses on the word-level qual-
ity estimation (named Task 2). The objective is to
highlight words needing post-edition and to detect
parts of the sentence that are not reliable. For the
task 2, participants produce for each token a label
according to two sub-tasks:
? a binary classification: good (keep) or bad
(change) label
? a multi-class classification: the label refers to
the edit action needed for the token (i.e. keep,
delete or substitute).
Various approaches have been proposed for
WCE: Blatz et al (2003) combine several features
using neural network and naive Bayes learning al-
gorithms. One of the most effective feature combi-
nations is the Word Posterior Probability (WPP) as
proposed by Ueffing et al (2003) associated with
IBM-model based features (Blatz et al, 2004).
Ueffing and Ney (2005) propose an approach for
phrase-based translation models: a phrase is a se-
quence of contiguous words and is extracted from
word-aligned bilingual training corpus. The con-
fidence value of each word is then computed by
summing over all phrase pairs in which the tar-
get part contains this word. Xiong et al (2010)
integrate target word?s Part-Of-Speech (POS) and
train them by Maximum Entropy Model, allow-
ing significative gains compared to WPP features.
Other approaches are based on external features
(Soricut and Echihabi, 2010; Felice and Specia,
2012) allowing to deal with various MT systems
(e.g. statistical, rule based etc.).
In this paper, we propose to use both internal
and external features into a conditionnal random
fields (CRF) model to predict the label for each
word in the MT hypothesis. We organize the arti-
cle as follows: section 2 explains all the used fea-
tures. Section 3 presents our experimental settings
and the preliminary experiments. Section 4 ex-
plores a feature selection refinement and the sec-
tion 5 presents work using several classifiers asso-
ciated with a boosting decision. Finally we present
386
our systems submissions and propose some con-
clusions and perspectives.
2 Features
In this section, we list all 25 types of features for
building our classifier (see a list in Table 3). Some
of them are already used and described in detail in
our previous paper (Luong, 2012), where we deal
with French - English SMT Quality Estimation.
WMT13 was a good chance to re-investigate their
usefulness for another language pair: English-
Spanish, as well as to compare their contributions
with those from other teams. We categorize them
into two types: the conventional features, which
are proven to work efficiently in numerous CE
works and are inherited in our systems, and the
LIG features which are more specifically sug-
gested by us.
2.1 The conventional features
We describe below the conventional features we
used. They can be found in some previous papers
dealing with WCE.
? Target word features: the target word itself;
the bigram (trigram) it forms with one (two)
previous and one (two) following word(s); its
number of occurrences in the sentence.
? Source word features: all the source words
that align to the target one, represented in
BIO1 format.
? Source alignment context features: the com-
binations of the target word and one word be-
fore (left source context) or after (right source
context) the source word aligned to it.
? Target algnment context features: the com-
binations of the source word and each word
in the window ?2 (two before, two after) of
the target word.
? Target Word?s Posterior Probability (WPP).
? Backoff behaviour: a score assigned to the
word according to how many times the target
Language Model has to back-off in order to
assign a probability to the word sequence, as
described in (Raybaud et al, 2011).
1http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/
? Part-Of-Speech (POS) features (using Tree-
Tagger2 toolkit): The target word?s POS; the
source POS (POS of all source words aligned
to it); bigram and trigram sequences between
its POS and the POS of previous and follow-
ing words.
? Binary lexical features that indicate whether
the word is a: stop word (based on the stop
word list for target language), punctuation
symbol, proper name or numerical.
2.2 The LIG features
? Graph topology features: based on the N-best
list graph merged into a confusion network.
On this network, each word in the hypothesis
is labelled with its WPP, and belongs to one
confusion set. Every completed path passing
through all nodes in the network represents
one sentence in the N-best, and must con-
tain exactly one link from each confusion set.
Looking into a confusion set, we find some
useful indicators, including: the number of
alternative paths it contains (called Nodes),
and the distribution of posterior probabili-
ties tracked over all its words (most interest-
ing are maximum and minimum probabilities,
called Max and Min).
? Language Model (LM) features: the ?longest
target n-gram length? and ?longest source n-
gram length?(length of the longest sequence
created by the current target (source aligned)
word and its previous ones in the target
(source) LM). For example, with the tar-
get word wi: if the sequence wi?2wi?1wi
appears in the target LM but the sequence
wi?3wi?2wi?1wi does not, the n-gram value
for wi will be 3.
? The word?s constituent label and its depth in
the tree (or the distance between it and the
tree root) obtained from the constituent tree
as an output of the Berkeley parser (Petrov
and Klein, 2007) (trained over a Spanish tree-
bank: AnCora3).
? Occurrence in Google Translate hypothesis:
we check whether this target word appears in
the sentence generated by Google Translate
engine for the same source.
2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
3http://clic.ub.edu/corpus/en/ancora
387
? Polysemy Count: the number of senses of
each word given its POS can be a reliable in-
dicator for judging if it is the translation of
a particular source word. Here, we investi-
gate the polysemy characteristic in both tar-
get word and its aligned source word. For
source word (English), the number of senses
can be counted by applying a Perl exten-
sion named Lingua::WordNet4, which pro-
vides functions for manipulating the Word-
Net database. For target word (Spanish), we
employ BabelNet5 - a multilingual semantic
network that works similarly to WordNet but
covers more European languages, including
Spanish.
3 Experimental Setting and Preliminary
Experiment
The WMT13 organizers provide two bilingual
data sets, from English to Spanish: the training
and the test ones. The training set consists of
803 MT outputs, in which each token is anno-
tated with one appropriate label. In the binary
variant, the words are classified into ?K? (Keep)
or ?C? (Change) label, meanwhile in the multi-
class variant, they can belong to ?K? (Keep), ?S?
(Substitution) or ?D? (Deletion). The test set con-
tains 284 sentences where all the labels accompa-
nying words are hidden. For optimizing parame-
ters of the classifier, we extract 50 sentences from
the training set to form a development set. Since
a number of repetitive sentences are observed in
the original training set, the dev set was carefully
chosen to ensure that there is no overlap with the
new training set (753 sentences), keeping the tun-
ing process accurate. Some statistics about each
set can be found in Table 1.
Motivated by the idea of addressing WCE as
a sequence labeling task, we employ the Con-
ditional Random Fields (CRF) model (Lafferty
et al, 2001) and the corresponding WAPITI toolkit
(Lavergne et al, 2010) to train our classifier. First,
we experiment with the combination of all fea-
tures. For the multi-class system, WAPITI?s de-
fault configuration is applied to determine the la-
bel, i.e. label which has the highest score is as-
signed to word. In case of the binary system,
the classification task is then conducted multiple
times, corresponding to a threshold increase from
4http://search.cpan.org/dist/Lingua-Wordnet/Wordnet.pm
5http://babelnet.org
0.300 to 0.975 (step = 0.025). When threshold =
?, all words in the test set which the probability of
?K? class > ? will be labelled as ?K?, and oth-
erwise, ?C?. The values of Precision (Pr), Recall
(Rc) and F-score (F) for K and C label are tracked
along this threshold variation, allowing us to se-
lect the optimal threshold that yields the highest
Favg = F (K)+F (C)2 .
Results for the all-feature binary system
(ALL BIN) at the optimal threshold (0.500) and
the multi-class one (ALL MULT) at the default
threshold, obtained on our dev set, are shown
in Table 2. We can notice that with ALL BIN,
?K? label scores are very promising and ?C? la-
bel reaches acceptable performance. In case of
ALL MULT we obtain the almost similar above
performance for ?K? and ?S?, respectively, ex-
cept the disappointing scores for ?D? (which can
be explained by the fact that very few instances of
?D? words (4%) are observed in the training cor-
pus).
Data set Train Dev Test
#segments 753 50 284
#distinct segments 400 50 163
#words 18435 1306 7827
%K : %C 70: 30 77: 23 -
%K: %S: %D 70:26:4 77:19:4 -
Table 1: Statistics of training, dev and test sets
System Label Pr(%) Rc(%) F(%)
ALL BIN K 85.79 84.68 85.23
C 50.96 53.16 52.04
ALL MULT K 85.30 84.00 84.65
S 43.89 49.00 46.31
D 7.90 6.30 7.01
Table 2: Average Pr, Rc and F for labels of all-
feature binary and multi-class systems, obtained
on dev set.
4 Feature Selection
In order to improve the preliminary scores of all-
feature systems, we conduct a feature selection
which is based on the hypothesis that some fea-
tures may convey ?noise? rather than ?informa-
tion? and might be the obstacles weakening the
other ones. In order to prevent this drawback,
we propose a method to filter the best features
388
based on the ?Sequential Backward Selection? al-
gorithm6. We start from the full set of N features,
and in each step sequentially remove the most use-
less one. To do that, all subsets of (N-1) fea-
tures are considered and the subset that leads to
the best performance gives us the weakest feature
(not involved in the considered set). This proce-
dure is also called ?leave one out? in the litera-
ture. Obviously, the discarded feature is not con-
sidered in the following steps. We iterate the pro-
cess until there is only one remaining feature in
the set, and use the following score for compar-
ing systems: Favg(all) = Favg(K)+Favg(C)2 , where
Favg(K) and Favg(C) are the averaged F scores
for K and C label, respectively, when threshold
varies from 0.300 to 0.975. This strategy enables
us to sort the features in descending order of im-
portance, as displayed in Table 3. Figure 1 shows
the evolution of the performance as more and more
features are removed.
Rank Feature name Rank Feature name
1 Source POS 14? Distance to root
2? Occur in Google Trans. 15 Backoff behaviour
3? Nodes 16? Constituent label
4 Target POS 17 Proper name
5 WPP 18 Number of occurrences
6 Left source context 19? Min
7 Right target context 20? Max
8 Numeric 21 Left target context
9? Polysemy (target) 22? Polysemy (source)
10 Punctuation 23? Longest target gram length
11 Stop word 24? Longest source gram length
12 Right source context 25 Source Word
13 Target Word
Table 3: The rank of each feature (in term of use-
fulness) in the set. The symbol ?*? indicates our
proposed features.
Observations in 10-best and 10-worst perform-
ing features in Table 3 suggest that numerous fea-
tures extracted directly from SMT system itself
(source and target POS, alignment context infor-
mation, WPP, lexical properties: numeric, punc-
tuation) perform very well. Meanwhile, opposite
from what we expected, those from word statis-
tical knowledge sources (target and source lan-
guage models) are likely to be much less ben-
eficial. Besides, three of our proposed features
appear in top 10-best. More noticeable, among
them, the first-time-experimented feature ?Occur-
rence in Google Translation hypothesis? is the
most prominent (rank 2), implying that such an on-
line MT system can be a reliable reference channel
for predicting word quality.
6http://research.cs.tamu.edu/prism/lectures/pr/pr l11.pdf
Figure 1: Evolution of system performance
(Favg(all)) during Feature Selection process, ob-
tained on dev set
The above selection process also brings us the
best-performing feature set (Top 20 in Table 3).
The binary classifier built using this optimal sub-
set of features (FS BIN) reaches the optimal per-
formance at the threshold value of 0.475, and
slightly outperforms ALL BIN in terms of F scores
(0.46% better for ?K? and 0.69% better for ?C?).
We then use this set to build the multi-class one
(FS MULT) and the results are shown to be a
bit more effective compare to ALL MULT (0.37%
better for ?K?, 0.80% better for ?S? and 0.15%
better for ?D?). Detailed results of these two sys-
tems can be found in Table 4.
In addition, in Figure 1, when the size of fea-
ture set is small (from 1 to 7), we can observe
sharply the growth of system scores for both la-
bels. Nevertheless the scores seem to saturate as
the feature set increases from the 8 up to 25. This
phenomenon raises a hypothesis about the learn-
ing capability of our classifier when coping with
a large number of features, hence drives us to an
idea for improving the classification scores. This
idea is detailed in the next section.
System Label Pr(%) Rc(%) F(%)
FS BIN K 85.90 85.48 85.69
C 52.29 53.17 52.73
FS MULT K 85.05 85.00 85.02
S 45.36 49.00 47.11
D 9.1 5.9 7.16
Table 4: The Pr, Rc and F for labels of binary and
multi-class system built from Top 20 features, at
the optimal threshold value, obtained on dev set
389
5 Using Boosting technique to improve
the system?s performance
In this section, we try to answer to the following
question: if we build a number of ?weak? (or ?ba-
sic?) classifiers by using subsets of our features
and a machine learning algorithm (such as Boost-
ing), would we get a single ?strong? classifier?
When deploying this idea, our hope is that multi-
ple models can complement each other as one fea-
ture set might be specialized in a part of the data
where the others do not perform very well.
First, we prepare 23 feature subsets
(F1, F2, ..., F23) to train 23 basic classifiers,
in which: F1 contains all features, F2 is the Top
20 in Table 3 and Fi (i = 3..23) contains 9
randomly chosen features. Next, a 7-fold cross
validation is applied on our training set. We
divide it into 7 subsets (S1, S2, . . . , S7). Each
Si (i = 1..6) contains 100 sentences, and the
remaining 153 sentences constitute S7. In the
loop i (i = 1..7), Si is used as the test set and
the remaining data is trained with 23 feature
subsets. After each loop, we obtain the results
from 23 classifiers for each word in Si. Finally,
the concatenation of these results after 7 loops
gives us the training data for Boosting. Therefore,
the Boosting training file has 23 columns, each
represents the output of one basic classifier for
our training set. The detail of this algorithm is
described below:
Algorithm to build Boosting training data
for i :=1 to 7 do
begin
TrainSet(i) := ?Sk (k = 1..7, k 6= i)
TestSet(i) := Si
for j := 1 to 23 do
begin
Classifier Cj := Train TrainSet(i) with Fj
Result Rj := Use Cj to test Si
Column Pj := Extract the ?probability of word
to be G label? in Rj
end
Subset Di (23 columns) := {Pj} (j = 1..23)
end
Boosting training set D := ?Di (i = 1..7)
Next, the Bonzaiboost toolkit7 (which imple-
ments Boosting algorithm) is used for building
Boosting model. In the training command, we in-
voked: algorithm = ?AdaBoost?, and number of
iterations = 300. The Boosting test set is prepared
as follows: we train 23 feature subsets with the
training set to obtain 23 classifiers, then use them
7http://bonzaiboost.gforge.inria.fr/x1-20001
to test our dev set, finally extract the 23 probabil-
ity columns (like in the above pseudo code). In the
testing phase, similar to what we did in Section 4,
the Pr, Rc and F scores against threshold variation
for ?K? and ?C? labels are tracked, and those cor-
responding to the optimal threshold (0.575 in this
case) are represented in Table 5.
System Label Pr(%) Rc(%) F(%)
BOOST BIN K 86.65 84.45 85.54
C 51.99 56.48 54.15
Table 5: The Pr, Rc and F for labels of Boosting
binary classifier (BOOST BIN)
The scores suggest that using Boosting algo-
rithm on our CRF classifiers? output accounts
for an efficient way to make them predict better:
on the one side, we maintain the already good
achievement on K class (only 0.15% lost), on the
other side we gain 1.42% the performance in C
class. It is likely that Boosting enables different
models to better complement each other, in terms
of the later model becomes experts for instances
handled wrongly by the previous ones. Another
advantage is that Boosting algorithm weights each
model by its performance (rather than treating
them equally), so the strong models (come from
all features, top 20, etc.) can make more dominant
impacts than the rest.
6 Submissions and Official Results
After deploying several techniques to improve the
system?s prediction capability, we select two bests
of each variant (binary and multi-class) to sub-
mit. For the binary task, the submissions in-
clude: the Boosting (BOOST BIN) and the Top
20 (FS BIN) system. For the multi-class task, we
submit: the Top 20 (FS MULT) and the all-feature
(ALL MULT) one. Before the submission, the
training and dev sets were combined to re-train
the prediction models for FS BIN, FS MULT and
ALL MULT. Table 6 reports the official results
obtained by LIG at WMT 2013, task 2. We ob-
tained the best performance among 3 participants.
These results confirm that the feature selection
strategy is efficient (FS MULT slightly better than
ALL MULT) while the contribution of Boosting
is unclear (BOOST BIN better than FS BIN if F-
measure is considered but worse if Accuracy is
considered - the difference is not significant).
390
System Pr Rc F Acc
BOOST BIN 0.777882 0.884325 0.827696 0.737702
FS BIN 0.788483 0.864418 0.824706 0.738213
FS MULT - - - 0.720710
ALL MULT - - - 0.719177
Table 6: Official results of the submitted systems, obtained on test set
7 Discussion and Conclusion
In this paper, we describe the systems submitted
for Task 2 of WMT13 Quality Estimation cam-
paign. We cope with the prediction of quality
at word level, determining whether each word
is ?good? or ?bad? (in the binary variant), or is
?good?, or should be ?substitute? or ?delete? (in
the multi-class variant). Starting with the ex-
isting word features, we propose and add vari-
ous of novel ones to build the binary and multi-
class baseline classifier. The first experiment?s re-
sults show that precision and recall obtained in
?K? label (both in binary and multi-class sys-
tems) are very encouraging, and ?C? (or ?S?) la-
bel reaches acceptable performance. A feature se-
lection strategy is then deployed to enlighten the
valuable features, find out the best performing sub-
set. One more contribution we made is the proto-
col of applying Boosting algorithm, training mul-
tiple ?weak? classifiers, taking advantage of their
complementarity to get a ?stronger? one. These
techniques improve gradually the system scores
(measure with F score) and help us to choose the
most effective systems to classify the test set.
In the future, this work can be extended in the
following ways. Firstly, we take a deeper look into
linguistic features of word, such as the grammar
checker, dependency tree, semantic similarity, etc.
Besides, we would like to reinforce the segment-
level confidence assessment, which exploits the
context relation between surrounding words to
make the prediction more accurate. Moreover, a
methodology to evaluate the sentence confidence
relied on the word- and segment- level confidence
will be also deeply considered.
References
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. Confidence estimation for machine trans-
lation. Technical report, JHU/CLSP Summer Workshop,
2003.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. Confidence estimation for machine trans-
lation. In Proceedings of COLING 2004, pages 315?321,
Geneva, April 2004.
Mariano Felice and Lucia Specia. Linguistic features for
quality estimation. In Proceedings of the 7th Workshop on
Statistical Machine Translation, pages 96?103, Montreal,
Canada, June 7-8 2012.
John Lafferty, Andrew McCallum, and Fernando Pereira.
Conditional random fields: Probabilistic models for seg-
menting et labeling sequence data. In Proceedings of
ICML-01, pages 282?289, 2001.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon. Practi-
cal very large scale crfs. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics,
pages 504?513, 2010.
Ngoc-Quang Luong. Integrating lexical, syntactic and
system-based features to improve word confidence estima-
tion in smt. In Proceedings of JEP-TALN-RECITAL, vol-
ume 3 (RECITAL), pages 43?56, Grenoble, France, June
4-8 2012.
Slav Petrov and Dan Klein. Improved inference for unlexical-
ized parsing. In Proceedings of NAACL HLT 2007, pages
404?411, Rochester, NY, April 2007.
S. Raybaud, D. Langlois, and K. Sma?? li. ?this sentence is
wrong.? detecting errors in machine - translated sentences.
In Machine Translation, pages 1?34, 2011.
Radu Soricut and Abdessamad Echihabi. Trustrank: Inducing
trust in automatic translations via ranking. In Proceedings
of the 48th ACL (Association for Computational Linguis-
tics), pages 612?621, Uppsala, Sweden, July 2010.
Nicola Ueffing and Hermann Ney. Word-level confidence
estimation for machine translation using phrased-based
translation models. In Proceedings of Human Lan-
guage Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, pages 763?
770, Vancouver, 2005.
Nicola Ueffing, Klaus Macherey, and Hermann Ney. Con-
fidence measures for statistical machine translation. In
Proceedings of the MT Summit IX, pages 394?401, New
Orleans, LA, September 2003.
Deyi Xiong, Min Zhang, and Haizhou Li. Error detection
for statistical machine translation using linguistic features.
In Proceedings of the 48th Association for Computational
Linguistics, pages 604?611, Uppsala, Sweden, July 2010.
391
