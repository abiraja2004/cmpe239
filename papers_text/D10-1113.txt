Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1162?1172,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Measuring Distributional Similarity in Context
Georgiana Dinu
Department of Computational Linguistics
Saarland University
Saarbru?cken, Germany
dinu@coli.uni-sb.de
Mirella Lapata
School of Informatics
University of Edinburgh
Edinburgh, UK
mlap@inf.ed.ac.uk
Abstract
The computation of meaning similarity as
operationalized by vector-based models has
found widespread use in many tasks ranging
from the acquisition of synonyms and para-
phrases to word sense disambiguation and tex-
tual entailment. Vector-based models are typ-
ically directed at representing words in isola-
tion and thus best suited for measuring simi-
larity out of context. In his paper we propose
a probabilistic framework for measuring sim-
ilarity in context. Central to our approach is
the intuition that word meaning is represented
as a probability distribution over a set of la-
tent senses and is modulated by context. Ex-
perimental results on lexical substitution and
word similarity show that our algorithm out-
performs previously proposed models.
1 Introduction
The computation of meaning similarity as op-
erationalized by vector-based models has found
widespread use in many tasks within natural lan-
guage processing (NLP). These range from the ac-
quisition of synonyms (Grefenstette, 1994; Lin,
1998) and paraphrases (Lin and Pantel, 2001) to
word sense disambiguation (Schuetze, 1998), tex-
tual entailment (Clarke, 2009), and notably informa-
tion retrieval (Salton et al, 1975).
The popularity of vector-based models lies in
their unsupervised nature and ease of computation.
In their simplest incarnation, these models repre-
sent the meaning of each word as a point in a
high-dimensional space, where each component cor-
responds to some co-occurring contextual element
(Landauer and Dumais, 1997; McDonald, 2000;
Lund and Burgess, 1996). The advantage of taking
such a geometric approach is that the similarity of
word meanings can be easily quantified by measur-
ing their distance in the vector space, or the cosine
of the angle between them.
Vector-based models do not explicitly identify the
different senses of words and consequently repre-
sent their meaning invariably (i.e., irrespective of co-
occurring context). Consider for example the adjec-
tive heavy which we may associate with the gen-
eral meaning of ?dense? or ?massive?. However,
when attested in context, heavy may refer to an over-
weight person (e.g., She is short and heavy but she
has a heart of gold.) or an excessive cannabis user
(e.g., Some heavy users develop a psychological de-
pendence on cannabis.).
Recent work addresses this issue indirectly with
the development of specialized models that repre-
sent word meaning in context (Mitchell and Lap-
ata, 2008; Erk and Pado?, 2008; Thater et al, 2009).
These methods first extract typical co-occurrence
vectors representing a mixture of senses and then use
vector operations to either obtain contextualized rep-
resentations of a target word (Erk and Pado?, 2008)
or a representation for a set of words (Mitchell and
Lapata, 2009).
In this paper we propose a probabilistic frame-
work for representing word meaning and measuring
similarity in context. We model the meaning of iso-
lated words as a probability distribution over a set of
latent senses. This distribution reflects the a priori,
out-of-context likelihood of each sense. Because
sense ambiguity is taken into account directly in the
1162
vector construction process, contextualized meaning
can be modeled naturally as a change in the origi-
nal sense distribution. We evaluate our approach on
word similarity (Finkelstein et al, 2002) and lexical
substitution (McCarthy and Navigli, 2007) and show
improvements over competitive baselines.
In the remainder of this paper we give a brief
overview of related work, emphasizing vector-based
approaches that compute word meaning in context
(Section 2). Next, we present our probabilistic
framework and different instantiations thereof (Sec-
tions 3 and 4). Finally, we discuss our experimental
results (Sections 5 and 6) and conclude the paper
with future work.
2 Related work
Vector composition methods construct representa-
tions that go beyond individual words (e.g., for
phrases or sentences) and thus by default obtain
word meanings in context. Mitchell and Lapata
(2008) investigate several vector composition op-
erations for representing short sentences (consist-
ing of intransitive verbs and their subjects). They
show that models performing point-wise multiplica-
tion of component vectors outperform earlier pro-
posals based on vector addition (Landauer and Du-
mais, 1997; Kintsch, 2001). They argue that multi-
plication approximates the intersection of the mean-
ing of two vectors, whereas addition their union.
Mitchell and Lapata (2009) further show that their
models yield improvements in language modeling.
Erk and Pado? (2008) employ selectional prefer-
ences to contextualize occurrences of target words.
For example, the meaning of a verb in the presence
of its object is modeled as the multiplication of the
verb?s vector with the vector capturing the inverse
selectional preferences of the object; the latter are
computed as the centroid of the verbs that occur
with this object. Thater et al (2009) improve on this
model by representing verbs in a second order space,
while the representation for objects remains first or-
der. The meaning of a verb boils down to restricting
its vector to the features active in the argument noun
(i.e., dimensions with value larger than zero).
More recently, Reisinger and Mooney (2010)
present a method that uses clustering to pro-
duce multiple sense-specific vectors for each word.
Specifically, a word?s contexts are clustered to pro-
duce groups of similar context vectors. An aver-
age prototype vector is then computed separately
for each cluster, producing a set of vectors for each
word. These cluster vectors can be used to determine
the semantic similarity of both isolated words and
words in context. In the second case, the distance
between prototypes is weighted by the probability
that the context belongs to the prototype?s cluster.
Erk and Pado? (2010) propose an exemplar-based
model for capturing word meaning in context. In
contrast to the prototype-based approach, no cluster-
ing takes place, it is assumed that there are as many
senses as there are instances. The meaning of a word
in context is the set of exemplars most similar to it.
Unlike Reisinger and Mooney (2010) and Erk and
Pado? (2010) our model is probabilistic (we repre-
sent word meaning as a distribution over a set of la-
tent senses), which makes it easy to integrate and
combine with other systems via mixture or product
models. More importantly, our approach is concep-
tually simpler as we use a single vector representa-
tion for isolated words as well as for words in con-
text. A word?s different meanings are simply mod-
eled as changes in its sense distribution. We should
also point out that our approach is not tied to a spe-
cific sense induction method and can be used with
different variants of vector-space models.
3 Meaning Representation in Context
In this section we first describe how we represent
the meaning of individual words and then move on
to discuss our model of inducing meaning represen-
tations in context.
Observed Representations Most vector space
models in the literature perform computations on
a co-occurrence matrix where each row repre-
sents a target word, each column a document or
another neighboring word, and each entry their
co-occurrence frequency. The raw counts are typ-
ically mapped into the components of a vector in
some space using for example conditional probabil-
ity, the log-likelihood ratio or tf-idf weighting. Un-
der this representation, the similarity of word mean-
ings can be easily quantified by measuring their dis-
tance in the vector space, the cosine of the angle be-
tween them, or their scalar product.
1163
Our model assumes the same type of input data,
namely a co-occurrence matrix, where rows corre-
spond to target words and columns to context fea-
tures (e.g., co-occurring neighbors). Throughout
this paper we will use the notation ti with i : 1..I
to refer to a target word and cj with j : 1..J to refer
to context features. A cell (i, j) in the matrix rep-
resents the frequency of occurrence of target ti with
context feature cj over a corpus.
Meaning Representation over Latent Senses
We further assume that the target words ti i : 1...I
found in a corpus share a global set of meanings
or senses Z = {zk|k : 1...K}. And therefore the
meaning of individual target words can be described
as a distribution over this set of senses. More for-
mally, a target ti is represented by the following vec-
tor:
v(ti) = (P(z1|ti), ...,P(zK|ti)) (1)
where component P (z1|ti) is the probability of
sense z1 given target word ti, component P (z2|ti)
the probability of sense z2 given ti and so on.
The intuition behind such a representation is that
a target word can be described by a set of core mean-
ings and by the frequency with which these are at-
tested. Note that the representation in (1) is not
fixed but parametrized with respect to an input cor-
pus (i.e., it only reflects word usage as attested in
that corpus). The senses z1 . . . zK are latent and can
be seen as a means of reducing the dimensionality
of the original co-occurrence matrix.
Analogously, we can represent the meaning of a
target word given a context feature as:
v(ti, cj) = (P(z1|ti, cj), ...,P(zK|ti, cj)) (2)
Here, target ti is again represented as a distribution
over senses, but is now modulated by a specific con-
text cj which reflects actual word usage. This distri-
bution is more ?focused? compared to (1); the con-
text helps disambiguate the meaning of the target
word, and as a result fewer senses will share most
of the probability mass.
In order to create the context-aware representa-
tions defined in (2) we must estimate the proba-
bilities P (zk|ti, cj) which can be factorized as the
product of P (ti, zk), the joint probability of target ti
and latent sense zk, and P (cj |zk, ti), the conditional
probability of context cj given target ti and sense zk:
P (zk|ti, cj) =
P (ti, zk)P (cj |zk, ti)
?
k P (ti, zk)P (cj |zk, ti)
(3)
Problematically, the term P (cj |zk, ti) is difficult to
estimate since it implies learning a total number of
K ? I J-dimensional distributions. We will there-
fore make the simplifying assumption that target
words ti and context features cj are conditionally in-
dependent given sense zk:
P (zk|ti, cj) ?
P (zk|ti)P (cj |zk)
?
k P (zk|ti)P (cj |zk)
(4)
Although not true in general, the assumption is rela-
tively weak. We do not assume that words and con-
text features occur independently of each other, but
only that they are generated independently given an
assigned meaning. A variety of latent variable mod-
els can be used to obtain senses z1 . . . zK and es-
timate the distributions P (zk|ti) and P (cj |zk); we
give specific examples in Section 4.
Note that we abuse terminology here, as the
senses our models obtain are not lexicographic
meaning distinctions. Rather, they denote coarse-
grained senses or more generally topics attested in
the document collections our model is trained on.
Furthermore, the senses are not word-specific but
global (i.e., shared across all words) and modulated
either within or out of context probabilistically via
estimating P (zk|ti, cj) and P (zk|ti), respectively.
4 Parametrizations
The general framework outlined above can be
parametrized with respect to the input co-occurrence
matrix and the algorithm employed for inducing the
latent structure. Considerable latitude is available
when creating the co-occurrence matrix, especially
when defining its columns, i.e., the linguistic con-
texts a target word is attested with. These con-
texts can be a small number of words surrounding
the target word (Lund and Burgess, 1996; Lowe
and McDonald, 2000), entire paragraphs, documents
(Salton et al, 1975; Landauer and Dumais, 1997)
or even syntactic dependencies (Grefenstette, 1994;
Lin, 1998; Pado? and Lapata, 2007).
1164
Analogously, a number of probabilistic models
can be employed to induce the latent senses. Ex-
amples include Probabilistic Latent Semantic Anal-
ysis (PLSA, Hofmann (2001)), Probabilistic Prin-
cipal Components Analysis (Tipping and Bishop,
1999), non-negative matrix factorization (NMF, Lee
and Seung (2000)), and latent Dirichlet alocation
(LDA, Blei et al (2003)). We give a more detailed
description of the latter two models as we employ
them in our experiments.
Non-negative Matrix Factorization Non-
negative matrix factorization algorithms approx-
imate a non-negative input matrix V by two
non-negative factors W and H , under a given
loss function. W and H are reduced-dimensional
matrices and their product can be regarded as a
compressed form of the data in V :
VI,J ?WI,KHK,J (5)
where W is a basis vector matrix and H is an en-
coded matrix of the basis vectors in equation (5).
Several loss functions are possible, such as mean
squared error and Kullback-Leibler (KL) diver-
gence. In keeping with the formulation in Sec-
tion 3 we opt for a probabilistic interpretation of
NMF (Gaussier and Goutte, 2005; Ding et al, 2008)
and thus minimize the KL divergence between WH
and V .
min
?
i,j
(Vi,j log
Vi,j
WHi,j
? Vi,j +WHi,j) (6)
Specifically, we interpret matrix V as
Vij = P (ti, cj), and matrices W and H as P (ti, zk)
and P (cj |zk), respectively. We can also ob-
tain the following more detailed factorization:
P (ti, cj) =
?
k P (ti)P (zk|ti)P (cj |zk).
Le WH denote the factors in a NMF decom-
position of an input matrix V and B be a diag-
onal matrix with Bkk =
?
j Hkj . B
?1H gives a
row-normalized version of H . Similarly, given
matrix WB, we can define a diagonal matrix A,
with Aii =
?
k(WB)ik. A
?1WB row-normalizes
matrix WB. The factorization WH can now be re-
written as:
WH=AA?1WBB?1H=A(A?1WB)(B?1H)
which allows us to interpret A as P (ti), A?1WB
as P (zk|ti) and B?1H as P (cj |zk). These interpre-
tations are valid since the rows of A?1WB and of
B?1H sum to 1, matrix A is diagonal with trace 1
because elements in WH sum to 1, and all entries
are non-negative.
Latent Dirichlet Allocation LDA (Blei et al,
2003) is a probabilistic model of text generation.
Each document d is modeled as a distribution
over K topics, which are themselves characterized
by distributions over words. The individual words
in a document are generated by repeatedly sampling
a topic according to the topic distribution and then
sampling a single word from the chosen topic.
More formally, we first draw the mixing propor-
tion over topics ?d from a Dirichlet prior with pa-
rameters ?. Next, for each of the Nd words wdn in
document d, a topic zdn is first drawn from a multi-
nomial distribution with parameters ?dn. The prob-
ability of a word token w taking on value i given
that topic z = j is parametrized using a matrix ?
with bij = P (w = i|z = j). Integrating out ?d?s
and zdn?s, gives P (D|?, ?), the probability of a cor-
pus (or document collection):
M?
d=1
?
P (?d|?)
?
?
Nd?
n=1
?
zdn
P (zdn|?d)P (wdn|zdn, ?)
?
?d?d
The central computational problem in topic
modeling is to obtain the posterior distri-
bution P (?, z|w, ?, ?) of the hidden vari-
ables z = (z1, z2, . . . , zN ). given a docu-
ment w = (w1, w2, . . . , wN ). Although this
distribution is intractable in general, a variety
of approximate inference algorithms have been
proposed in the literature. We adopt the Gibbs
sampling procedure discussed in Griffiths and
Steyvers (2004). In this model, P (w = i|z = j) is
also a Dirichlet mixture (denoted ?) with symmetric
priors (denoted ?).
We use LDA to induce senses of target words
based on context words, and therefore each row ti
in the input matrix transforms into a document. The
frequency of ti occurring with context feature cj is
the number of times word cj is encountered in the
?document? associated with ti. We train the LDA
model on this data to obtain the ? and ? distribu-
1165
tions. ? gives the sense distributions of each tar-
get ti: ?ik = P (zk|ti) and ? the context-word dis-
tribution for each sense zk: ?kj = P (cj |zk).
5 Experimental Set-up
In this section we discuss the experiments we per-
formed in order to evaluate our model. We describe
the tasks on which it was applied, the corpora used
for model training and our evaluation methodology.
Tasks The probabilistic model presented in Sec-
tion 3 represents words via a set of induced senses.
We experimented with two types of semantic space
based on NMF and LDA and optimized parameters
for these models on a word similarity task. The
latter involves judging the similarity sim(ti, t?i) =
sim(v(ti), v(t?i)) of words ti and t
?
i out of context,
where v(ti) and v(t?i) are obtained from the output of
NMF or LDA, respectively. In our experiments we
used the data set of Finkelstein et al (2002). It con-
tains 353 pairs of words and their similarity scores
as perceived by human subjects.
The contextualized representations were next
evaluated on lexical substitution (McCarthy and
Navigli, 2007). The task requires systems to find
appropriate substitutes for target words occurring in
context. Typically, systems are given a set of substi-
tutes, and must produce a ranking such that appro-
priate substitutes are assigned a higher rank com-
pared to non-appropriate ones. We made use of the
SemEval 2007 Lexical Substitution Task benchmark
data set. It contains 200 target words, namely nouns,
verbs, adjectives and adverbs, each of which occurs
in 10 distinct sentential contexts. The total set con-
tains 2,000 sentences. Five human annotators were
asked to provide substitutes for these target words.
Table 1 gives an example of the adjective still and
its substitutes.
Following Erk and Pado? (2008), we pool together
the total set of substitutes for each target word.
Then, for each instance the model has to produce a
ranking for the total substitute set. We rank the can-
didate substitutes based on the similarity of the con-
textualized target and the out-of-context substitute,
sim(v(ti, cj), v(t?i)), where ti is the target word, cj a
context word and t?i a substitute. Contextualizing
just one of the words brings higher discriminative
power to the model rather than performing compar-
Sentences Substitutes
It is important to apply the
herbicide on a still day, be-
cause spray drift can kill
non-target plants.
calm (5) not-windy (1)
windless (1)
A movie is a visual docu-
ment comprised of a series
of still images.
motionless (3) unmov-
ing (2) fixed (1) sta-
tionary (1) static (1)
Table 1: Lexical substitution data example for the adjec-
tive still ; numbers in parentheses indicate the frequency
of the substitute.
isons with the target and its substitute embedded in
an identical context (see also Thater et al (2010) for
a similar observation).
Model Training All the models we experimented
with use identical input data, i.e., a bag-of-words
matrix extracted from the GigaWord collection of
news text. Rows in this matrix are target words and
columns are their co-occurring neighbors, within a
symmetric window of size 5. As context words, we
used a vocabulary of the 3,000 most frequent words
in the corpus.1
We implemented the classical NMF factorization
algorithm described in Lee and Seung (2000). The
input matrix was normalized so that all elements
summed to 1. We experimented with four dimen-
sions K: [600 ? 1000] with step size 200. We ran
the algorithm for 150 iterations to obtain factors W
and H which we further processes as described in
Section 4 to obtain the desired probability distribu-
tions. Since the only parameter of the NMF model
is the factorization dimension K, we performed two
independent runs with each K value and averaged
their predictions.
The parameters for the LDA model are the num-
ber of topicsK and Dirichlet priors ? and ?. We ex-
perimented with topics K: [600? 1400], again with
step size 200. We fixed ? to 0.01 and tested two val-
ues for ?: 2K (Porteous et al, 2008) and
50
K (Griffiths
and Steyvers, 2004). We used Gibbs sampling on
the ?document collection? obtained from the input
matrix and estimated the sense distributions as de-
scribed in Section 4. We ran the chains for 1000 iter-
1The GigaWord corpus contains 1.7B words; we scale down
all the counts by a factor of 70 to speed up the computation of
the LDA models. All models use this reduced size input data.
1166
ations and averaged over five iterations [600?1000]
at lag 100 (we observed no topic drift).
We measured similarity using the scalar prod-
uct, cosine, and inverse Jensen-Shannon (IJS) diver-
gence (see (7), (8), and (9), respectively):
sp(v, w) =< v,w >=
?
i
viwi (7)
cos(v, w) =
?v, w?
||v|| ||w||
(8)
IJS(v, w) =
1
JS(v,w)
(9)
JS(v,w) =
1
2
KL(v|m) +
1
2
KL(w|m) (10)
where m is a shorthand for 12(v + w) and
KL the Kullback-Leibler divergence, KL(v|w) =
?
i vilog(
vi
wi
).
Among the above similarity measures, the scalar
product has the most straightforward interpretation
as the probability of two targets sharing a common
meaning (i.e., the sum over all possible meanings).
The scalar product assigns 1 to a pair of identi-
cal vectors if and only if P (zi) = 1 for some i
and P (zj) = 0,?j 6= i. Thus, only fully disam-
biguated words receive a score of 1. Beyond similar-
ity, the measure also reflects how ?focused? the dis-
tributions in question are, as very ambiguous words
are unlikely to receive high scalar product values.
Given a set of context words, we contextualize the
target using one context word at a time and compute
the overall similarity score by multiplying the indi-
vidual scores.
Baselines Our baseline models for measuring sim-
ilarity out of context are Latent Semantic Analysis
(Landauer and Dumais, 1997) and a simple seman-
tic space without any dimensionality reduction.
For LSA, we computed the U?V SVD decompo-
sition of the original matrix to rank k = 1000. Any
decomposition of lower rank can be obtained from
this by setting rows and columns to 0. We evaluated
decompositions to ranks K: [200 ? 1000], at each
100 step. Similarity computations were performed
in the lower rank approximation matrix U?V , as
originally proposed in Deerwester et al (1990), and
in matrix U which maps the words into the concept
space. It is common to compute SVD decomposi-
tions on matrices to which prior weighting schemes
have been applied. We experimented with tf-idf
weighting and line normalization.
Our second baseline, the simple semantic space,
was based on the original input matrix on which
we applied several weighting schemes such as point-
wise mutual information, tf-idf, and line normaliza-
tion. Again, we measured similarity using cosine,
scalar product and inverse JS divergence. In addi-
tion, we also experimented with Lin?s (1998) simi-
larity measure:
lin(v, w) =
?
i?I(v)?I(w)(vi + wi)
?
i?I(v) vi +
?
l?I(w)wi
(11)
where the values in v and w are point-wise mutual
information, and I(?) gives the indices of positive
values in a vector.
Our baselines for contextualized similarity were
vector addition and vector multiplication which
we performed using the simple semantic space
(Mitchell and Lapata, 2008) and dimensionality
reduced representations obtained from NMF and
LDA. To create a ranking of the candidate substi-
tutes we compose the vector of the target with its
context and compare it with each substitute vector.
Given a set of context words, we contextualize the
target using each context word at a time and multi-
ply the individual scores.
Evaluation Method For the word similarity task
we used correlation analysis to examine the rela-
tionship between the human ratings and their cor-
responding vector-based similarity values. We re-
port Spearman?s ? correlations between the simi-
larity values provided by the models and the mean
participant similarity ratings in the Finkelstein et al
(2002) data set. For the lexical substitution task, we
compare the system ranking with the gold standard
ranking using Kendall?s ?b rank correlation (which is
adjusted for tied ranks). For all contextualized mod-
els we defined the context of a target word as the
words occurring within a symmetric context window
of size 5. We assess differences between models us-
ing stratified shuffling (Yeh, 2000).2
2Given two system outputs, the null hypothesis (i.e., that
the two predictions are indistinguishable) is tested by randomly
mixing the individual instances (in our case sentences) of the
two outputs. We ran a standard number of 10000 iterations.
1167
Model Spearman ?
SVS 38.35
LSA 49.43
NMF 52.99
LDA 53.39
LSAMIX 49.76
NMFMIX 51.62
LDAMIX 51.97
Table 2: Results on out of context word similarity using
a simple co-occurrence based vector space model (SVS),
latent semantic analysis, non-negative matrix factoriza-
tion and latent Dirichlet alocation as individual models
with the best parameter setting (LSA, NMF, LDA) and as
mixtures (LSAMIX, NMFMIX, LDAMIX).
6 Results
Word Similarity Our results on word similar-
ity are summarized in Table 2. The simple co-
occurrence based vector space (SVS) performed best
with tf-idf weighting and the cosine similarity mea-
sure. With regard to LSA, we obtained best re-
sults with initial line normalization of the matrix,
K = 600 dimensions, and the scalar product sim-
ilarity measure while performing computations in
matrix U . Both NMF and LDA models are generally
better with a larger number of senses. NMF yields
best performance with K = 1000 dimensions and
the scalar product similarity measure. The best LDA
model also uses the scalar product, has K = 1200
topics, and ? set to 50K .
Following Reisinger and Mooney (2010), we also
evaluated mixture models that combine the output
of models with varying parameter settings. For both
NMF and LDA we averaged the similarity scores re-
turned by all runs. For comparison, we also present
an LSA mixture model over the (best) middle in-
terval K values. As can be seen, the LSA model
improves slightly, whereas NMF and LDA perform
worse than their best individual models.3 Overall,
we observe that NMF and LDA yield significantly
(p < 0.01) better correlations than LSA and the sim-
3It is difficult to relate our results to Reisinger and Mooney
(2010), due to differences in the training data and the vector rep-
resentations it gives rise to. As a comparison, a baseline config-
uration with tf-idf weighting and the cosine similarity measure
yields a correlation of 0.38 with our data and 0.49 in Reisinger
and Mooney (2010).
Model Kendall?s ?b
SVS 11.05
Add-SVS 12.74
Add-NMF 12.85
Add-LDA 12.33
Mult-SVS 14.41
Mult-NMF 13.20
Mult-LDA 12.90
Cont-NMF 14.95
Cont-LDA 13.71
Cont-NMFMIX 16.01
Cont-LDAMIX 15.53
Table 3: Results on lexical substitution using a simple
semantic space model (SVS), additive and multiplicative
compositional models with vector representations based
on co-occurrences (Add-SVS, Mult-SVS), NMF (Add-
NMF, Mult-NMF), and LDA (Add-LDA, Mult-LDA) and
contextualized models based on NMF and LDA with the
best parameter setting (Cont-NMF, Cont-LDA) and as
mixtures (Cont-NMFMIX, Cont-LDAMIX).
ple semantic space, both as individual models and as
mixtures.
Lexical Substitution Our results on lexical sub-
stitution are shown in Table 3. As a baseline we
also report the performance of the simple semantic
space that does not use any contextual information.
This model returns the same ranking of the substi-
tute candidates for each instance, based solely on
their similarity with the target word. This is a rel-
atively competitive baseline as observed by Erk and
Pado? (2008) and Thater et al (2009).
We report results with contextualized NMF and
LDA as individual models (the best word similar-
ity settings) and as mixtures (as described above).
These are in turn compared against additive and
multiplicative compositional models. We imple-
mented an additive model with pmi weighting and
Lin?s similarity measure which is defined in an ad-
ditive fashion. The multiplicative model uses tf-
idf weighting and cosine similarity, which involves
multiplication of vector components. Other combi-
nations of weighting schemes and similarity mea-
sures delivered significantly lower results. We also
report results for these models when using the NMF
and LDA reduced representations.
1168
Model Adv Adj Noun Verb
SVS 22.47 14.38 09.52 7.98
Add-SVS 22.79 14.56 11.59 10.00
Mult-SVS 22.85 16.37 13.59 11.60
Cont-NMFMIX 26.13 17.10 15.16 14.18
Cont-LDAMIX 21.21 16.00 16.31 13.67
Table 4: Results on lexical substitution for different parts
of speech with a simple semantic space model (SVS), two
compositional models (Add-SVS, Mult-SVS), and con-
textualized mixture models with NMF and LDA (Cont-
NMFMIX, Cont-LDAMIX), using Kendall?s ?b correlation
coefficient.
All models significantly (p < 0.01) outperform
the context agnostic simple semantic space (see
SVS in Table 3). Mixture NMF and LDA mod-
els are significantly better than all variants of com-
positional models (p < 0.01); the individual mod-
els are numerically better, however the difference
is not statistically significant. We also find that the
multiplicative model using a simple semantic space
(Mult-SVS) is the best performing compositional
model, thus corroborating the results of Mitchell and
Lapata (2009). Interestingly, dimensionality compo-
sitional models. This indicates that the better results
we obtain are due to the probabilistic formulation of
our contextualized model as a whole rather than the
use of NMF or LDA. Finally, we observe that the
Cont-NMF model is slightly better than Cont-LDA,
however the difference is not statistically significant.
To allow comparison with previous results re-
ported on this data set, we also used the General-
ized Average Precision (GAP, Kishida (2005)) as an
evaluation measure. GAP takes into account the or-
der of candidates ranked correctly by a hypothetical
system, whereas average precision is only sensitive
to their relative position. The best performing mod-
els are Cont-NMFMIX and Cont-LDAMIX obtaining
a GAP of 42.7% and 42.9%, respectively. Erk and
Pado? (2010) report a GAP of 38.6% on this data set
with their best model.
Table 4 shows how the models perform across dif-
ferent parts of speech. While verbs and nouns seem
to be most difficult, we observe higher gains from
the use of contextualized models. Cont-LDAMIX
obtains approximately 7% absolute gain for nouns
and Cont-NMFMIX approximately 6% for verbs. All
Senses Word Distributions
TRAFFIC (0.18) road, traffic, highway, route, bridge
MUSIC (0.04) music, song, rock, band, dance, play
FAN (0.04) crowd, fan, people, wave, cheer, street
VEHICLE (0.04) car, truck, bus, train, driver, vehicle
Table 5: Induced senses of jam and five most likely words
given these senses using an LDA model; sense probabili-
ties are shown in parentheses.
contextualized models obtain smaller improvements
for adjectives. For adverbs most models do not im-
prove over the no-context setting, with the exception
Cont-NMFMIX.
Finally, we also qualitatively examined how the
context words influence the sense distributions of
target words using examples from the lexical sub-
stitution dataset and the output of an individual
Cont-LDA model. In many cases, a target word
starts with a distribution spread over a larger number
of senses, while a context word shifts this distribu-
tion to one majority sense. Consider, for instance,
the target noun jam in the following sentence:
(1) With their transcendent, improvisational jams
and Mayan-inspired sense of a higher, meta-
physical purpose, the band?s music delivers a
spiritual sustenance that has earned them a very
devoted core following.
Table 5 shows the out-of-context senses activated
for jam together with the five most likely words as-
sociated with them.4 Sense probabilities are also
shown in parentheses. As can be seen, initially two
traffic-related and two music-related senses are acti-
vated, however with low probabilities. In the pres-
ence of the context word band, we obtain a much
more ?focused? distribution, in which the MUSIC
sense has 0.88 probability. The system ranks riff
and gig as the most likely two substitutes for jam.
The gold annotation also lists session as a possible
substitute.
In a large number of cases, the target is only par-
tially disambiguated by a context word and this is
also reflected in the resulting distribution. An ex-
4Sense names are provided by the authors in an attempt to
best describe the clusters (i.e., topics for LDA) to which words
are assigned.
1169
ample is the word bug which initially has a distribu-
tion triggering the SOFTWARE (0.09, computer, soft-
ware, microsoft, windows) and DISEASE (0.06, dis-
ease, aids, virus, cause) senses. In the context of
client, bug remains ambiguous between the senses
SECRET-AGENCY (0.34, agent, secret, intelligence,
FBI) ) and SOFTWARE (0.29):
(2) We wanted to give our client more than just a
list of bugs and an invoice ? we wanted to
provide an audit trail of our work along with
meaningful productivity metrics.
There are also cases where the contextualized dis-
tributions are not correct, especially when senses are
domain specific. An example is the word function
occurring in its mathematical sense with the context
word distribution. However, the senses that are trig-
gered by this pair all relate to the ?service? sense of
function. This is a consequence of the newspaper
corpus we use, in which the mathematical sense of
function is rare. We also see several cases where
the target word and one of the context words are as-
signed senses that are locally correct, but invalid in
the larger context. In the following example:
(3) Check the shoulders so it hangs well, stops at
hips or below, and make sure the pants are long
enough.
The pair (check, shoulder) triggers senses IN-
JURY (0.81, injury, left, knee, shoulder) and
BALL-SPORTS (0.10, ball, shot, hit, throw). How-
ever, the sentential context ascribes a meaning that
is neither related to injury nor sports. This suggests
that our models could benefit from more principled
context feature aggregation.
Generally, verbs are not as good context words
as nouns. To give an example, we often encounter
the pair (let, know), used in the common ?inform?
meaning. The senses we obtain for this pair, are,
however, rather uninformative general verb classes:
{see, know, think, do} (0.57) and {go, say, do,
can} (0.20). This type of error can be eliminated in
a space where context features are designed to best
reflect the properties of the target words.
7 Conclusions
In this paper we have presented a general frame-
work for computing similarity in context. Key in this
framework is the representation of word meaning as
a distribution over a set of global senses where con-
textualized meaning is modeled as a change in this
distribution. The approach is conceptually simple,
the same vector representation is used for isolated
words and words in context without being tied to a
specific sense induction method or type of semantic
space.
We have illustrated two instantiations of this
framework using non-negative matrix factorization
and latent Dirichlet alocation for inducing the la-
tent structure, and shown experimentally that they
outperform previously proposed methods for mea-
suring similarity in context. Furthermore, both of
them benefit from mixing model predictions over a
set of different parameter choices, thus making pa-
rameter tuning redundant.
The directions for future work are many and var-
ied. Conceptually, we have defined our model in an
asymmetric fashion, i.e., by stipulating a difference
between target words and contextual features. How-
ever, in practice, we used vector representations that
do not distinguish the two: target words and con-
textual features are both words. This choice was
made to facilitate comparisons with the popular bag-
of-words vector space models. However, differen-
tiating target from context representations may be
beneficial particularly when the similarity compu-
tations are embedded within specific tasks such as
the acquisition of paraphrases, the recognition of en-
tailment relations, and thesaurus construction. Also
note that our model currently contextualizes target
words with respect to individual contexts. Ideally,
we would like to compute the collective influence of
several context words on the target. We plan to fur-
ther investigate how to select or to better aggregate
the entire set of features extracted from a context.
Acknowledgments The authors acknowledge the
support of the DFG (Dinu; International Re-
search Training Group ?Language Technology and
Cognitive Systems?) and EPSRC (Lapata; grant
GR/T04540/01).
1170
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of
the Workshop on Geometrical Models of Natural Lan-
guage Semantics, pages 112?119, Athens, Greece.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas, and Richard Harshman. 1990. Indexing by
latent semantic analysis. Journal of the American So-
ciety for Information Science, 41:391?407.
Chris Ding, Tao Li, and Wei Peng. 2008. On the equiv-
alence between non-negative matrix factorization and
probabilistic latent semantic indexing. Computational
Statistics & Data Analysis, 52(8):3913?3927.
Katrin Erk and Sabastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 897?906,
Honolulu, Hawaii.
Katrin Erk and Sebastian Pado?. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of the ACL 2010 Conference Short Papers, pages 92?
97, Uppsala, Sweden.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: the concept
revisited. ACM Transactions on Information Systems,
20(1):116?131.
Eric Gaussier and Cyril Goutte. 2005. Relation between
PLSA and NMF and implications. In Proceedings of
the 28th Annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 601?602, New York, NY.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl. 1):5228?5235.
Thomas Hofmann. 2001. Unsupervised learning by
probabilistic latent semantic analysis. Machine Learn-
ing, 41(2):177?196.
Walter Kintsch. 2001. Predication. Cognitive Science,
25:173?202.
Kazuaki Kishida. 2005. Property of average precision
and its generalization: An examination of evaluation
indicator for information retrieval experiments. NII
Technical Report.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to Plato?s problem: The latent semantic analysis
theory of acquisition, induction and representation of
knowledge. Psychological Review, 104(2):211?240.
Daniel D. Lee and H. Sebastian Seung. 2000. Algo-
rithms for non-negative matrix factorization. In NIPS,
pages 556?562.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):342?360.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the joint Annual
Meeting of the Association for Computational Linguis-
tics and International Conference on Computational
Linguistics, pages 768?774, Montre?al, Canada.
Will Lowe and Scott McDonald. 2000. The direct route:
Mediated priming in semantic space. In Proceedings
of the 22nd Annual Conference of the Cognitive Sci-
ence Society, pages 675?680, Philadelphia, PA.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments,
and Computers, 28:203?208.
Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 Task 10: English Lexical Substitution Task. In
Proceedings of SemEval, pages 48?53, Prague, Czech
Republic.
Scott McDonald. 2000. Environmental Determinants of
Lexical Processing Effort. Ph.D. thesis, University of
Edinburgh.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244, Columbus, Ohio.
Jeff Mitchell and Mirella Lapata. 2009. Language mod-
els based on semantic composition. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 430?439, Suntec, Singa-
pore.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161?199.
Ian Porteous, David Newman, Alexander Ihler, Arthur
Asuncion, Padhraic Smyth, and Max Welling. 2008.
Fast collapsed gibbs sampling for latent Dirichlet alo-
cation. In Proceeding of the 14th ACM SIGKDD inter-
national conference on Knowledge discovery and data
mining, pages 569?577, New York, NY.
Joseph Reisinger and Raymond J. Mooney. 2010. Multi-
prototype vector-space models of word meaning. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 109?
117, Los Angeles, California.
G Salton, A Wang, and C Yang. 1975. A vector-space
model for information retrieval. Journal of the Ameri-
can Society for Information Science, 18:613?620.
1171
Hinrich Schuetze. 1998. Automatic word sense discrim-
ination. Journal of Computational Linguistics, 24:97?
123.
Stefan Thater, Georgiana Dinu, and Manfred Pinkal.
2009. Ranking paraphrases in context. In Proceed-
ings of the 2009 Workshop on Applied Textual Infer-
ence, pages 44?47, Suntec, Singapore.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 948?957, Uppsala,
Sweden.
Michael E. Tipping and Chris M. Bishop. 1999. Prob-
abilistic principal component analysis. Journal of the
Royal Statistical Society, Series B, 61:611?622.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th Conference on Computational Linguistics,
pages 947?953, Saarbru?cken, Germany.
1172
