Handling Pragmatic Information 
With A Reversible Architecture 
Masato Ishizaki 
NTT Communications and Information Processing 
Laboratones 
1-2356, Take, Yokosuka, Kanagawa, 238-03 Japan 
E-mail: ishizaki%nttnly.ntt.jp@relay.cs.net 
Abstract 
This paper propo~s areversible architecture 
to handle not orily syntactic and semantic 
information but also pragmatic nformation. 
? I Existing architectures cannot represent prag- 
matic informatioil explicitly, and lack rea- 
soning capability;given insufficient informa- 
tion. I argue that he techniques ofplan rep- 
resentation and approximate r asoning are, 
in the enhanced argumentation system pro- 
posed here, effective for solving these prob- 
lems. 
1. Introduction 
Reversibility orbi-directionality of grammars 
seems to play a quite important role in natural 
language procesSing. It reduces the cost of 
constructing a grammar; we need to use only 
one grammar instead of two for parsing and 
generation. Cost ~here includes not only the 
making of grammar rules but also verifying 
the rules and the algorithms for parsing and 
generation. Reversibility differs from bi- 
directionality: the former equires the same 
mechanism and 'grammar for parsing and 
generation; the latter equires just the same 
grammar as shown Figure l(Noord, 1990). 
Pragmatic in:formation is not rigidly 
defined; rather it is thought of as information 
other than syntaqtic and semantic informa- 
tion. It is indispetlsable for explaining many 
linguistic pheno/nena from cleft sentences 
to discourse structures. As pragmatics annot 
restrain language in a strict manner like syn- 
tax, it must be processed differently; that is, 
a distinction must be made between con- 
straints that need to be fully satisfied and 
those that do not.'Plan representation seems 
to be appropriate for collecting different 
level information. A plan consists of precon 
ditions, constraints, plan expansion (usually 
termed the body), and effects. The relation- 
ship between preconditions and constraints 
parallels that between pragmatic and syntactic 
information. Thus, the difference between 
preconditions and constraints can be easily 
modeled. 
Handling pragmatic information clearly 
depends on assumption ofbelief: Generating 
referring expressions requires inferencing 
the hearer's belief (Appelt, 1985); Producing 
text requires the usage of a one-sided mutual 
beliefl(Moore t a1.,1989); the listener's 
inference about he speaker's belief greatly 
helps to resolve anaphora or to analyze the 
speaker's intention. In any case, belief be- 
comes a condition for further inference; how- 
ever, it is difficult if not impossible to confirm 
the assumed belief. Thus, a new mechanism 
based on a new architecture is needed. Ap- 
proximate reasoning (Elkan,1990) is stat- 
able for this purpose. Processing can con- 
"tinue, even if some preconditions are not 
fully satisfied; they are held as assump- 
tions 2. This approach seems to be very 
natural. For example, in conversations, the 
speaker should conceptualize the listener's 
1One-sided mutual belief is one half of 
mutual knowledge, soto speak, namely the set of 
those pieces of mutual knowledge that constitute 
the knowledge ofone speaker (Bunt, 1989: 60). 
2Since approximate r asoning can fail, 
assumptions must be held explicitly for further in- 
ference. Plan representation is adequate for that 
reason. 
119 
Strings Stri0gs 
Analyzer Forms 
Figure 1. Reversible And Bi-directional Architectures. 
understanding. In most conversations, how- 
ever, the speaker does not keep confirming 
the other's belief because this would disrupt 
the conversation flow. 
This paper describes areversible archi- 
tecture to handle not only syntactic and se- 
mantic information, but also pragmatic in- 
formation. Existing architectures cannot rep- 
resent pragmatic nformation explicitly, and 
lack reasoning capability given insufficient 
information. I argue that the techniques of 
plan representation a d approximate r ason- 
ing in the argumentation system introduced 
here are effective for solving these problems. 
First, the difficulties of existing architec- 
tures have in handling pragmatic nformation 
are described. Next, plan representation f 
linguistic information and approximate r a- 
soning are mentioned in the context of the 
argumentation system. Third, parsing and 
generation examples are shown. Finally, the 
problem of proposed ata structure, the de- 
composition of semantic representation, the 
role of syntactic information and the differ- 
ence between active and passive vocabulary 
are discussed. 
2. Existing Architectures For Re- 
versible And Bi-directional Gram- 
mar 
Shieber proposed auniform architecture for 
sentence parsing and generation based on 
the Early type deduction mechanism 
(Shieber,1988). He parametrized the archi- 
tecture with the initial condition, a priority 
function on lemmas, and a predicate x- 
pressing the concept of successful proof. 
Shieber emedied the inefficiency of the gen 
eration algorithm in his uniform architecture 
to introduce the concept of semantic head 
(Shieber et a1.,1989). Although Definite 
Clause Grammar (DCG) is reversible, its 
synthesis mode is inefficient. Dymetman 
and Strzalkowski approached the problem 
by compiling DCG into efficient analysis 
and synthesis programs (Dymetman et 
a1.,1988) (Strzalkowski,1990). The compi- 
lation is realized by changing oal ordering 
statically. Since Shieber's, Dymetman's and 
Strzalkowski's architectures are based on 
syntax deduction, they have difficulties in 
handling pragmatic nformation. 
Dependency propagation was suggested 
for parsing and generation in (Hasida et 
a1.,1987). His idea was developed using 
horn clauses imilar to PROLOG. The word 
dependency indicates the states where vari- 
ables are shared by constraints 3. Problem 
solving or parsing and generation can be 
modeled by resolving the dependencies. De- 
pendency resolution is executed by fold/un- 
fold transformations. Dependency propaga- 
tion is a very elegant mechanism for problem 
solving, but it seems to be difficult to repre- 
sent syntactic, semantic and pragmatic nfor- 
mation with indiscrete constraints. In addi- 
tion, to that, since dependency propagation 
is a kind of co-routine process, programs 
are very hard to debug and so constraints 
are tough to stipulate. 
Ait-Kaci's typed unification was applied 
to a reversible architecture (Emele and Zajac 
,1990). All features in sign are sorted and 
placed in hierarchical structures. Parsing and 
generation can be executed by rewriting the 
3Constraints are represented bythe usual 
PROLOG predicates. 
120 
I 
features into their most specific forms. Their 
mechanism greatly depends on the hierarchy 
of information, but with information other 
than syntactic, especially pragmatic, it is hard 
to construct the hierarchy. 
3. Introduction To the New Re- 
versible Architecture 
3.1. The Linguistic Objects 
We introduce the!linguistic object sign which 
incorporates syntactic, semantic and prag- 
matic information. Sign is represented by 
feature structures and consists of features 
phn, syn, sem and prag. Phn represents 
surface string information for words, phras- 
es and sentences. Syn stands for syntactic 
information like the part of speech and sub- 
categorization i formation using HPSG. 
HPSG inherits the fundamental properties 
of Generalized: Phrase Structure Gram- 
mar(GPSG). That is, HPSG makes use of 
a set of feature+value pairs, feature con- 
straints and unification to stipulate grammar 
instead of rewriting rules for terminal and 
nonterminal symbols. The major difference 
between HPSGi and GPSG is that subcat- 
egorizafion i formation is stored in lexical 
entries, instead of being stored in grammar 
rules (Pollard et a1.,1987,1990). Sere de- 
notes emantic nformation or logical forms. 
Logical forms are expressed by the semantic 
representation l~guage proposed by Gazdar 
(Gazdar et a1,1989). Since the language is a 
feature representation of Woods' representa- 
tion (Woods,1978), it has the advantages 
that it can represent quantifier scope ambigu- 
ities. It consistsi of the features qnt, var, 
rest, and body : qnt is for quantifier expres- 
sions; var is for variables bound by the 
qnt; rest is for restrictions of the var ; while 
body represents flae predication ofthe logical 
form. Prag deliiaeates pragmatic informa- 
tion. Pragmatic conditions are not necessarily 
true but are held a s assumptions. Uniqueness 
and novelty conditions in cleft sentences are 
instances of the conditions. 
m 
phn: "It was the girl that a boy loved" 
syn: pos: verb 
subcat:subc(\[\]) 
sem: qnt: indefinite 
var: X 
rest: argO: X 
pred: BOY 
body: qnt: definite 
var: Y 
rest: argO: Y 
pred: GIRL 
body: argO: X 
argl: Y 
pred: LOVED 
prag: \[novel(Y), unique(Y)\] 
- -  Sign 
Figure 2. Linguistic Object Example. 
Figure 2 shows an example of the 
linguistic object sign. The feature phn indi- 
cates that surface string is "It was a girl 
that the boy loved". Syn represents hat: 1) 
the part of speech is verb; 2) subcategoriza- 
tion information is satisfied. Sem shows 
that: 1) the quantifier at the top level is in- 
definite; 2) the property of the variable X is 
a boy; 3) the property of the variable Y 
bounded by the quantifier definite is a girl; 
4) the boy loved the girl. Prag mentions 
that the variable Y is constrained with 
uniquness and novelty conditions. 
3.2. The Plan Representation of 
Linguistic Objects 
To handle syntactic, semantic, and pragmatic 
information, our generator represents hem 
as plans. Plans are composed of precondi- 
tions, constraints, plan expansion, and ef- 
fects. Preconditions include pragmatic nfor- 
mation which are the criteria needed to select 
a plan. Constraints include syntactic ondi- 
tions such as the head feature principle and 
conditions on surface strings. Plan expan- 
sion contains ub-semantic expressions for 
effects, which are complete semantic ex- 
pressions. Constraints and preconditions are 
similar, but differ in that the former must be 
satisfied, but the latter are retained as as- 
121 
sumpfions if not satisfied. 
Figure 3 describes aplan relating to the 
semantic nformation LOVED. No precondi- 
tions exists because the expression "loved" 
has no pragmatic information. Constraints 
indicate that: 1) the part of speech equals 
verb; 2) the subcategofization nformation 
is subc(\[Sbj,Obj\]); 3) The sem features of 
precond: \[\] 
const: (Sign:syn:pos = verb), 
(Sign:syn:subcat = subc(\[Sbj,Obj\])), 
(ArgO = Sbj:sem), (Argl = Obj:sem), 
(Sign:phn = "loved") 
eplan: \[\] 
effect: argO: ArgO 
argl: Argl 
pred: LOVED 
Figure 3. Plan Example. 
Sbj and Obj are semantic arguments ofpred- 
icate LOVED; 4) the surface string is "loved". 
There is no plan expansion because lexical 
information does not need to be expanded. 
Effects mention semantic expression of 
LOVED. 
3.3. An Argumentation System 
For Planning 
A plan recognition scheme, named the argu- 
mentation system, was proposed by Kono- 
lige and Pollack (Konolige and Pollack 
,1989). It can defeasibly reason about belief 
and intention ascription 4, and can process 
preferences over candidate ascriptions. The 
framework is so general and powerful that 
it can perform other processes other than 
belief and intention ascription. For example, 
Shimazu has shown that it can model parsing 
mechanism 
The argumentation system consists of 
arguments. An argument is a relation between 
4 Defeasible r asoning and approximate 
reasoning are very similar, but differ in that: the 
former addresses the result after ule application; the 
latter considers just rule application. 
a set of propositions (the premises of the 
argument), and another set of propositions 
(the conclusion of the argument) (Konolige 
and Pollack,1989: 926). The system has a 
language containing the following operators: 
t(P) which indicates the truth of the proposi- 
tion P; beI(A,PF) which mentions that agent 
A believes plan fragment PF; int(A,PF) 
which shows that agent A intends plan frag- 
ment PF; exp(A,P) which means that agent 
A expects proposition P to be true; and 
by(Actexpl,Actexp2,Pexp) which signifies 
the complex plan fragment which consists 
of the action expression Actexp2, by doing 
action expression Actexpl while proposi- 
tional expression Pexp is true 5. The argu- 
ments to the operators, action expressions 
inform(S,H,Sem) and utter(S,H,Str), are in- 
troduced to mimic informing and uttering 
activities: the former is designated such that 
speaker S informs hearer H about semantic 
content Sem; the latter indicates peaker S
utters tring Str to hearer H. 
Plan expansion, effects and constraints 
mentioned in subsection 3.2 correspond to 
Actexpl, Actexp2 and Pexp, respectively. 
To represent the difference between precon- 
ditions and constraints, the operator by is 
revised to include preconditions a the fourth 
argument. Thus, the new operator 
by(Actexp 1,Actexp2,Pexp 1,Pexp2) is de- 
fined as the complex plan fragment, consist- 
ing of doing Actexp2 (effect(s)), by doing 
Actexpl (plan expansion) while Pexpl (con- 
straints) is true, and Pexp2 (precondition(s)) 
is true or held as assumptions. The plan in 
figure 2 was redefined by using axiom (1) 6. 
Axiom (2) shows another example cor- 
5Action expressions are formed from an 
action ame and parameters; Propositional expres- 
sions are formed from a property name and parameters. 
~ecause of space limitations, abbrevia- 
tions are used as necessary. For example, Pos is 
taken to mean the value of the features of part 
of speech of syntactic information f sign. 
122 
I ? 
responding to the context free grammar for 
a cleft sentence. 
Axiom (1): 
t(by(utter(S,H,"loved"), 
inform(S,H,LOVED), 
((Pos=verb), 
(Subcat=subc(\[Sbj,Obj\])), 
(Sbj:sem=Arg0), 
(Obj:semaArgl)), 
0) 
Axiom (2): 
r 
t(by((inform(S,H,LF17), 
inform(S,H,LF2S)), 
inform(S,H,LF9), 
((Pos=Pos2),(Sign 1 =Sbj), 
(Subcat=subc(\[\])), 
(Slash=sl(\[\])), 
(Slash2=s\[(\[Obj\])), 
(Phn="It was"+Phn 1 +"that" +Phn2)), 
(Prag))). 
7LF1 is designated as:
Signl: Sem: qnt: Q2 
var: Y 
rest: argO: Y 
pred: YRest. 
Z 
SLF2 is designated as:
Sign2: sere: qnt: Q1 
var: X 
rest: argO: X 
pred: XRest 
body: argO: ArgO 
argl: Argl 
pred: Pred. 
9LF designated as:
Sign: s m: qnt: Q1 
var: X 
rest: argO: X 
pred: XRest 
body: qnt: Q2 
var: Y 
rest: argO: Y 
pred: YRest 
body: argO: ArgO 
argl: Argl 
pred: Pred. 
Plan expansion and effects indicate that if 
speaker S wants to inform hearer H about 
LF, the speaker should inform the hearer 
about LF1 and LF2, while observing con- 
straints 1) -4). Constraints state that: 1) the 
part of speech of Sign equals one of Sign2; 
2) Subcategorized and slash information of 
Sign is nil; 3) Subcategorized information 
of Sign2 equals nil; 4) Slash information of 
Sign2 is equivalent toObj; 4) a surface string 
consists of the string "It was", the string 
relating to Signl, the string "that" and the 
string relating to Sign2. Other axioms which 
are necessary for explaining parsing and gen- 
eration examples are listed in the appendix. 
4. Reversibility In Proposed Ar- 
chitecture 
4.1. Sentence Parsing 
Parsing techniques were simulated using an 
argumentation system in (Shimazu,1990). 
Since he faithfully tried to model existing 
techniques, many parsing oriented terms 
such as complete and addition were intro- 
duced. This seems to be the cause of the 
difficulty he experienced in integrating pars- 
ing with other processes. 
Argument (a): 
belasc ~? 
t(P) ..... > bel(S,P). 
Argument (b): 
beI(S,by(PE,E,C,PR)),int(S,PE), 
exp(S,C),exp 1 (S,PR) 
by2' 
..... > int(S,by(PE,E,C,PR)), 
int(S,E). 
1?The expression above the arrow 
indicates the class of an argument. 
123 
Since syntactic, semantic and pragmatic n- 
formation can be represented with the new 
by relation, arguments (a) and (b) enable 
us to simulate parsing: (a) says that true 
propositions can be ascribed to a speaker's 
belief; (b) states that, if a speaker is assumed 
to believe that E is an effect of performing 
plan expansion PE, while constraint C is 
true and precondition PR is assumed to be 
true H, then it is plausible that his reason 
for doing PE is his intention to do E. 
Parsing is executed as follows: first, 
axioms whose constraints match an input 
word are collected; second, the axiom which 
satisfies the constraint is selected (precondi- 
tions are asserted); third, an effect, or se- 
mantic information is derived using an in- 
stance of argument (b); fourth, another in- 
stance of the argument isapplied to the effect 
and the effect which was already derived to 
obtain a new effect. If the application cannot 
proceed further, a new word is analyzed; 
Lastly, ff all words in a sentence are analyzed 
successfully, the execution is complete. 
Parsing is exactly the same as plan 
recognition in the sense of (Konolige and 
Pollack, 1989:925): 
Plan recognition is essentially a "bottom-up" 
recognition process, with global coherence 
used mostly as an evaluative measure, to 
eliminate ambiguous plan fragments that 
emerge.from local cues. 
Maximizing head elements can realize 
right association and minimal attachment, 
but handling semantic ambiguities, that is to 
clarify global coherence, is a further issue. 
4.2. Sentence Generation 
Generation can be simulated using arguments 
(a) and (c). (c) says that, if a speaker believes 
that E is an effect of performing plan expan- 
sion PE, while constraint C is true and pre- 
condition PR is assumed to be true, and he 
intends to do E, then it is plausible that his 
intention PE is to achieve E. 
HAction expression expl(A,P) 
means that agent A expects proposition P 
to be assumed to be true if not fully satisfied. 
Argument (c): 
bel(S,by(PE,E,C,PR)),int(S,E), 
exp(S ,C),exp 1 (S,PR) 
by3' 
..... > int(by(PE,E,C,PR)), 
int(S,PE) 
Generation is executed in a similar way 
to parsing except hat axioms are collected 
using semantic information and the result is 
a string 12. Figure 4 describes the generation 
process. The input linguistic object is equiv- 
alent o the object in Figure 2 whose surface 
string infoiTnation is parametefized. The gen- 
eration result is the input object with the 
instafiated surface string, that is Figure 1. 
In Figure 4, axiom (2) creates ubgoals 
related to the variable Y and others (cor- 
responding to (2) and (3)) because the se- 
mantic and pragmatic nformation ofthe input 
equals the effect and preconditions of the 
axiom. As the head features propagate to 
the linguistic object (2), execution addressing 
object (2) is preferred. The axiom (6) con- 
stnacts ubgoals by referring to the objects 
whose semantic information is related to the 
features qnt, vat, rest and the logical form 
concerned with the bound variable X. The 
head feature preference makes the generator 
execute axioms about object (4). This results 
in the axiom (1) of lexical information. Sim- 
ilar to the above process, the remaining sub- 
goals (5) and (2) are executed. Finally, the 
surface string "It was a girl that the boy 
loved" is obtained. 
5. Discussions 
As mentioned in (Emele and Zajac,1990), 
the proposed approach inevitably leads to 
the consequence that the data structure be- 
comes slightly complicated. However, due 
12Because of space limitation, action 
expressions inform and utter are omitted 
in the figure. 
124 
(2) 
02) I 
(14) 
(1)1 Sign:phn:"It syn:pos:verb sla h:sl(\[\]) ubcat:subc(\[D w "+Exp 1 + " t h a t " + E x p 2 , ~  
sem:qnt:indefinite 
Sign:phn:Expl 
syn:Sy/fl 
sem:qnhdefinite 
v~:Y 
rest:arg0:Y 
: pred:GIRL 
var:X 
rest:argO:X 
pred:BOY 
body:qnt:definite 
var:Y 
rest:argO:Y 
pred:GIRL 
body:argO:X 
argl:Y 
pred:LOVED 
prag: \[\] prag: \[novel(Y),unique(Y)\] 
Sign:phn:Expl 1 
syn:Synl i 
sem:qnt:definite 
var:Y 
ping:I\] 
I 
Sign:phn:"~e" 
syn:l~s:dei 
sem:qnt:definite 
var:Y 
prag:\[\] 
(11).l 
(13) I 
Sign:phn:Expl2 
syn:Synl 
sem:rest:arg0:Y 
prag:\[\] 
Sign:phn:"girl" 
syn:pos:noun 
sem:rest:arg0:Y 
prag:\[\] 
(5) 
Sign:phn:Exp211 
syn:Syn211 
sem:qnt:indefinite 
prag:\[\] 
(3) Sign:phn:Exp2 
syn:pos:verb 
subcat:subc(\]) 
slash:sl(\[Obj\]) 
sem:qnt:indefinite 
var:X 
rest:arg0:X 
pred:BOY 
body:arg0:X 
argl:Y 
pred:LOVED 
i 
(10~ Sign:phn:"a" 
il syn:pos:det 
| sem:qnt:indefinite 
:1 var:X 
ii prag:\[\] 
prag:\[\] 
pred:GIRL 
pred:GIRL 
Sign:phn:Exp21 (4) 
syn:Syn21 
sem:qnt:indefinite 
var:X 
rest:arg0:X 
pred:BOY 
prag:\[\] 
(6) 
,...-.-------""-N 
(7) I Sign:phn:Exp212 
\[ syn:Syn21 
I sem:rest:arg0:X 
var:X I pred:BOY 
\[ prag:\[\] 
I I 
(9) \[ Sign:phn:"boy" 
\[ syn:pos:noun 
\[ sem:rest:arg0~X 
I prag:\[\] pred.BOY 
Sign:phn:Exp22 
syn:pos:verb 
subcat:subc(\[Sbj,Obj\] 
sem:argO:X 
argl:Y 
pred:LOVED 
prag:\[\] 
I 
Sign:phn:"loved" 
syn:pos:verb 
subcat:subc(\[S bj,Obj\] 
sem:arg0:X 
arg 1 :Y 
pred:LOVED 
prag:\[\] 
Figure 4. Generation Example. 
125 
to the segregation of the structure such as 
the distinction between preconditions and 
constraints, the task of developing rules can 
be done independently. Thus, if you want, 
you can concentrate ondeveloping rammar 
rules irrespective of the pragmatic informa- 
tion. If desired, however, pragmatics can 
be used to precisely stipulate some linguistic 
phenomena (Delin, 1990). 
The semantic representation utilized here 
depends on the strong assumption that it 
can be systematically decomposed. It is ad- 
vantageous that the assumption supports 
symmetry as discussed in (Noord,1990) 
and naturally realizes semantic indexing 
which leads to efficient processing 
(Calder, 1989). However, it limits the repre- 
sentation capability for semantic processing 
(Shieber et a1.,1989). The problems of se- 
mantic representation are still difficult, so 
their study is an ongoing task. 
Syntactic information, or grammar 
rules in the paper is neutral in the sense 
that only one kind of rules, or axioms are 
sufficient for both parsing and generation; 
but their difference lies in their usage of 
information. In the case of parsing, syntactic 
information is used as a local cue to derive 
the semantic and pragmatic nformation. In
the case of generation, it is used to prevent 
the production of ungrammatical strings. 
This difference appears to mirror asymmetry 
between writing and reading (or speaking 
and hearing). The reading process lets un- 
known words be hypothesized by referring 
to neighboring words that are understood. 
Writing, on the other hand, is a process in 
which unknown words cannot be developed 
by examining adjacent words. Hypothesiz- 
ing is used both for parsing and generation, 
but its role in these processes i different. It 
is used to derive a coherent interpretation 
from all words in the parsing, while it is 
used to smooth the conversational (or text) 
flow in generation. The difference of the 
hypothesis use seems to be one of the factors 
in explaining this asymmetry. The proposed 
architecture is certain to provide a basis to 
examine this claim in the sense that it inte- 
grates linguistic processes with a reasoning 
mechanism. 
6. Conclusion 
This paper has proposed a reversible archi- 
tecture to handle not only syntactic and se- 
mantic information but also pragmatic nfor- 
mation. Existing architectures cannot repre- 
sent pragmatic information explicitly, and 
lack reasoning capability given insufficient 
information. I argue that the techniques of 
plan representation a d approximate r ason- 
ing in the enhanced argumentation system 
are effective for solving these problems. 
Acknowledgments 
The author wishes to extend his sincere grat- 
itude to Tsuneaki Kato and Yoshihiko Ha- 
yashi for discussing this architecture. He 
also thank Masanobu Higashida nd Yoichi 
Sakai for encouraging him to study this sub- 
ject. 
Appendix 
Axiom (3): 
t(by(utter(S,H,"the"), 
inform(S,H,definite), 
((Pos=det),(Adjacent=noun)), 
0)). 
Axiom (4): 
t(by(utter(S,H,"boy"), 
inform(S,H,BOY), 
(Pos=noun), 
0)). 
Axiom (5): 
t(by(utter(S,H,"a"), 
in form (S,H,indefinite), 
((Pos=det),(Adjacent=noun)), 
0)). 
126 
Axiom (6): 
t(by(utter(S,H,"girl"), 
inform(S,H,GIRL), 
(Pos=noun), 
0)). 
Axiom (7): 
t(by((inform(S,H,LF 11), 
inform(S,H,LF12)), 
inform(S,H,LF1), 
((Posl=Pos\]2), 
(Phn I =Phn 1 l+Phn 12)), 
0)). 
Axiom (8): 
t(by((inform(S,H,LF21), 
inform(S,H,LF22)), 
inform(S,H,LF2), 
((Pos2=Pos21), 
(Subcat2=~subc(\[\])), 
(Slash2=s!(\[Obj\])), 
(Subcat22~subc(\[Sbj,Obj\])), 
(Phn2=Phh21+Phn22)), 
0)). 
Bibliography 
(Appelt,1985i Douglas E. Appelt. 1985. 
"Planning English Sentences". Cambridge 
University Press. 
(Calder et a!:.,1989) Jonathan Calder, 
Mike Reape, and Henk Zeevat. 1989. "An 
Algorithm for Generation i  Unification Cat- 
egorial Gramme". Proceedings of the 4th 
Conference of the European Chapter of the 
Association for Computational Linguistics. 
Manchester, U.K.. 10-12, April. 233-240. 
(Delin,1990) J.L.Delin. 1990. "A Multi- 
Level Account 0f Cleft Constructions inDis- 
course". Proceedings of the 13th Interna- 
tional Conference on Computational Linguis- 
tics. Aug. 20-25. Helsinki, Finland. 83-88. 
(Dymetman et a1.,1989) Marc Dyrnet- 
man and Pierre Isabelle. 1989. "Reversible 
Logic Grammars For Machine Translation". 
Proceedings ofthe 2nd International Confer- 
ence on Theoretical nd Methodological Is-
sues In Machine Translation of Natural Lan- 
guages. Jun. 12-14. Pittsburgh, Pennsylva- 
nia, U.S.A.. 
(Elkan,1990) Charles Elkan. 1990. "In- 
cremental, Approximate Planning". Pro- 
ceedings of the 8th National Conference on 
Artificial Intelligence. Jul.29-Aug.3. Bos- 
ton, MA, U.S.A.. 145-150. 
(Emele and Zajac,1990) Martin Emele 
and Remi Zajac. 1990. "Typed Unification 
Grammars". Proceedings of the 13th Inter- 
national Conference on Computational Lin- 
guistics. Aug. 20-25. Helsinki, Finland. 
293-298. 
(Gazdar et al, 1989) Gerald Gazdar and 
Chris Mellish. 1989. "Natural Language 
Processing in PROLOG". Addison-Welsley 
Publishing Company. 
(Has ida ,1987)  Hasida Koiti. 1987. "De- 
pendency Propagation: A Unified Theory 
of Sentence Comprehension a d Genera- 
tion". Proceedings of the 10th International 
Joint Conference on Artificial Intelligence. 
Aug. 23-28. Milan, Italy. 664-670. 
(Konol ige and Po l lack ,1989)Kur t  
Konolige and Martha E. Pollack. 1989. "As- 
cribing Plans To Agents - Preliminary Report 
- . Proceedings of the 11th International 
Joint Conference on Artificial Intelligence. 
Aug. 20-25. Detroit, Michigan, U.S.A.. 
924-930. 
(Moore et a1,1989) JohannaD. Moore 
and Cecile Paris. 1989. "Planning Text For 
Advisory Dialogue". Proceedings of the 27th 
Annual Meeting of the Association for Com- 
putational Linguistics. Jun. 26-29. Van cou- 
ver, British Columbia, Canada. 203-211. 
(Pollard et a1.,1987) Carl Pollard and 
Ivan A. Sag. 1987. "An Information-Based 
Syntax and Semantics (Volume 1)". CSLI 
127 
Lecture Notes. Number 13. 
(Pollard et a!.,1990) Carl Pollard and 
Ivan A. Sag. 1990. "An Information-Based 
Syntax and Semantics (Volume 2)". ms. 
(Shieber,1988) Stuart M. Shieber. 1988. 
"A Uniform Architecture For Parsing and 
Generation". Proceedings of the 12th Inter- 
national Conference on Computational Lin- 
guistics. Aug. 22-27. Bonn, Germany. 614- 
619. 
(Shieber et a1.,1989) Stuart M. Shieber, 
Gertjan van Noord, Robert C. Moore, and 
Femando C.N. Perreira. 1989. "A Semantic- 
Head-Driven Generation Algorithms for 
Unification-Based Formalisms". Proceed- 
ings of the 27th Annual Meeting of the As- 
sociation for Computational Linguistics. 
Jun. 26-29. Van couver, British Columbia, 
Canada. 7-17. 
(Shimazu,1990) Akira Shimazu. 1990. 
"Japanese Sentence Analysis as Argumenta- 
tion". Proceedings of the 13th International 
Conference on Computational Linguistics. 
Aug. 20-25. Helsinki, Finland. 259-264. 
(Strzalkowski ,1990) Tomek Strza- 
lkowski. 1990. "How To Invert A Natural 
Language Parser Into An Efficient Generator. 
An Algorithm For Logic Grammars". Pro- 
ceedings of the 13th Intemational Conference 
on Computational Linguistics. Aug. 20-25. 
Helsinki, Finland. 347-352. 
(Woods,1978) W.A.Woods. 1978. "Se- 
mantics and Quantification i Natural Lan- 
guage Question Answering". Advances in 
Computers. Vol. 17. M.Yovits (ed.). Aca- 
demic Press. 
128 
