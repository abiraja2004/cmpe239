Proceedings of the 2010 Workshop on Applications of Tree Automata in Natural Language Processing, ACL 2010, pages 10?18,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
A Decoder for
Probabilistic Synchronous Tree Insertion Grammars
Steve DeNeefe ? and Kevin Knight ?
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292, USA
{sdeneefe,knight}@isi.edu
Heiko Vogler ?
Department of Computer Science
Technische Universita?t Dresden
D-01062 Dresden
Heiko.Vogler@tu-dresden.de
Abstract
Synchronous tree insertion grammars
(STIG) are formal models for syntax-
based machine translation. We formal-
ize a decoder for probabilistic STIG; the
decoder transforms every source-language
string into a target-language tree and cal-
culates the probability of this transforma-
tion.
1 Introduction
Tree adjoining grammars (TAG) were invented in
(Joshi et al 1975) in order to better character-
ize the string sets of natural languages1. One of
TAG?s important features is the ability to introduce
two related syntactic units in a single rule, then
push those two units arbitrarily far apart in sub-
sequent derivation steps. For machine translation
(MT) between two natural languages, each being
generated by a TAG, the derivations of the two
TAG may be synchronized (Abeille et al, 1990;
Shieber and Shabes, 1990) in the spirit of syntax-
directed transductions (Lewis and Stearns, 1968);
this results in synchronous TAG (STAG). Recently,
in (Nesson et al, 2005, 2006) probabilistic syn-
chronous tree insertion grammars (pSTIG) were
discussed as model of MT; a tree insertion gram-
mar is a particular TAG in which the parsing prob-
lem is solvable in cubic-time (Schabes and Wa-
ters, 1994). In (DeNeefe, 2009; DeNeefe and
Knight 2009) a decoder for pSTIG has been pro-
posed which transforms source-language strings
into (modifications of) derivation trees of the
pSTIG. Nowadays, large-scale linguistic STAG
rule bases are available.
In an independent tradition, the automata-
theoretic investigation of the translation of trees
? financially supported by NSF STAGES project, grant
#IIS-0908532.
? financially supported by DFG VO 1011/5-1.
1see (Joshi and Shabes, 1997) for a survey
led to the rich theory of tree transducers (Ge?cseg
and Steinby, 1984, 1997). Roughly speaking, a
tree transducer is a finite term rewriting system. If
each rewrite rule carries a probablity or, in gen-
eral, a weight from some semiring, then they are
weighted tree transducers (Maletti, 2006, 2006a;
Fu?lo?p and Vogler, 2009). Such weighted tree
transducers have also been used for the specifi-
cation of MT of natural languages (Yamada and
Knight, 2001; Knight and Graehl, 2005; Graehl et
al., 2008; Knight and May 2009).
Martin and Vere (1970) and Schreiber (1975)
established the first connections between the two
traditions; also Shieber (2004, 2006) and Maletti
(2008, 2010) investigated their relationship.
The problem addressed in this paper is the
decoding of source-language strings into target-
language trees where the transformation is de-
scribed by a pSTIG. Currently, this decoding re-
quires two steps: first, every source string is
translated into a derivation tree of the underly-
ing pSTIG (DeNeefe, 2009; DeNeefe and Knight
2009), and second, the derivation tree is trans-
formed into the target tree using an embedded tree
transducer (Shieber, 2006). We propose a trans-
ducer model, called a bottom-up tree adjoining
transducer, which performs this decoding in a sin-
gle step and, simultaneously, computes the prob-
abilities of its derivations. As a basis of our ap-
proach, we present a formal definition of pSTIG.
2 Preliminaries
For two sets ? and A, we let U?(A) be the set of
all (unranked) trees over ? in which also elements
of A may label leaves. We abbreviate U?(?) by
U?. We denote the set of positions, leaves, and
non-leaves of ? ? U? by pos(?) ? N?, lv(?), and
nlv(?), resp., where ? denotes the root of ? and
w.i denotes the ith child of position w; nlv(?) =
pos(?) \ lv(?). For a position w ? pos(?), the la-
bel of ? at w (resp., subtree of ? at w) is denoted
10
by ?(w) (resp., ?|w). If additionally ? ? U?(A),
then ?[?]w denotes the tree which is obtained from
? by replacing its subtree at w by ?. For every
? ? ? ?A, the set pos?(?) is the set of all those
positions w ? pos(?) such that ?(w) ? ?. Simi-
larly, we can define lv?(?) and nlv?(?). The yield
of ? is the sequence yield(?) ? (? ? A)? of sym-
bols that label the leaves from left to right.
If we associate with ? ? ? a rank k ? N, then
we require that in every tree ? ? U?(A) every ?-
labeled position has exactly k children.
3 Probabilistic STAG and STIG
First we will define probabilistic STAG, and sec-
ond, as a special case, probabilistic STIG.
Let N and T be two disjoint sets of, resp., non-
terminals and terminals. A substitution rule r is a
tuple (?s, ?t, V,W, P radj) where
? ?s, ?t ? UN (T ) (source and target tree) and
|lvN (?s)| = |lvN (?t)|,
? V ? lvN (?s)?lvN (?t) (substitution sites), V
is a one-to-one relation, and |V | = |lvN (?s)|,
? W ? nlvN (?s)?nlvN (?t) (potential adjoin-
ing sites), and
? P radj : W ? [0, 1] (adjoining probability).
An auxiliary rule r is a tuple (?s, ?t, V,W, ?, P radj)
where ?s, ?t,W , and P radj are defined as above and
? V is defined as above except that |V | =
|lvN (?s)| ? 1 and
? ? = (?s, ?t) ? lvN (?s)? lvN (?t) and neither
?s nor ?t occurs in any element of V ; more-
over, ?s(?) = ?s(?s) and ?t(?) = ?t(?t), and
?s 6= ? 6= ?t; the node ?s (and ?t) is called
the foot-node of ?s (resp., ?t).
An (elementary) rule is either a substitution rule
or an auxiliary rule. The root-category of a rule r
is the tuple (?s(?), ?t(?)), denoted by rc(r).
A probabilistic synchronous tree ad-
joining grammar (pSTAG) is a tuple
G = (N,T, (Ss, St),S,A, P ) such that N
and T are two disjoint sets (resp., of nonterminals
and terminals), (Ss, St) ? N?N (start nontermi-
nal), S and A are finite sets of, resp., substitution
rules and auxiliary rules, and P : S ? A ? [0, 1]
such that for every (A,B) ? N ?N ,
?
r?S
rc(r)=(A,B)
P (r) = 1 and
?
r?A
rc(r)=(A,B)
P (r) = 1
assuming that in each case the number of sum-
mands is not zero. In the following, let G always
denote an arbitrary pSTAG.
Ss
? A? A
?
a
r1??
St
B
?
B? ?a
P (r1) = 1
P r1adj(a) = .9
A
A ?
b,c
?
r2??
B
B
?
B
b
c ?
P (r2) = .4
P r2adj(b) = .2
P r2adj(c) = .6
A
A A
?
d
? e
r3??
B
? B
d,e
?
P (r3) = .6
P r3adj(d) = .3
P r3adj(e) = .8
A
?
r4??
B
?
P (r4) = .1
A
?
r5??
B
?
P (r5) = .9
Figure 1: The running example pSTAG G.
In Fig. 1 we show the rules of our running ex-
ample pSTAG, where the capital Roman letters are
the nonterminals and the small Greek letters are
the terminals. The substitution site (in rule r1) is
indicated by ?, and the potential adjoining sites are
denoted2 by a, b, c, d, and e. For instance, in for-
mal notation the rules r1 and r2 are written as fol-
lows:
r1 = (Ss(?,A,A(?)), St(B(?), B, ?), {?}, {a}, P r1adj)
where ? = (2, 2) and a = (3, 1), and
r2 = (A(A, ?), B(B(?), B), ?, {b, c}, ?, P r2adj)
where b = (?, ?), c = (?, 1), and ? = (1, 2).
In the derivation relation of G we will distin-
guish four types of steps:
1. substitution of a rule at a substitution site
(substitution),
2. deciding to turn a potential adjoining site into
an activated adjoining site (activation),
3. deciding to drop a potential adjoining site,
i.e., not to adjoin, (non-adjoining) and
4. adjoining of a rule at an activated adjoining
site (adjoining).
In the sentential forms (defined below) we will
maintain for every adjoining site w a two-valued
flag g(w) indicating whether w is a potential
(g(w) = p) or an activated site (g(w) = a).
The set of sentential forms ofG is the set SF(G)
of all tuples ? = (?s, ?t, V,W, g) with
2Their placement (as left or right index) does not play a
role yet, but will later when we introduce pSTIG.
11
? ?s, ?t ? UN (T ),
? V ? lvN (?s) ? lvN (?t) is a one-to-one rela-
tion, |V | = |lvN (?s)| = |lvN (?t)|,
? W ? nlvN (?s)? nlvN (?t), and
? g : W ? {p, a}.
The derivation relation (of G) is the binary
relation ? ? SF(G) ? SF(G) such that
for every ?1 = (?1s , ?1t , V1,W1, g1) and ?2 =
(?2s , ?2t , V2,W2, g2) we have ?1 ? ?2 iff one of
the following is true:
1. (substitution) there are w = (ws, wt) ? V1
and r = (?s, ?t, V,W, P radj) ? S such that
? (?1s (ws), ?1t (wt)) = rc(r),
? ?2s = ?1s [?s]ws and ?2t = ?1t [?t]wt ,
? V2 = (V1 \ {w}) ? w.V ,3
? W2 = W1 ? w.W , and
? g2 is the union of g1 and the set of pairs
(w.u, p) for every u ?W ;
this step is denoted by ?1
w,r=? ?2;
2. (activation) there is a w ? W1 with g1(w) =
p and (?1s , ?1t , V1,W1) = (?2s , ?2t , V2,W2),
and g2 is the same as g1 except that g2(w) =
a; this step is denoted by ?1
w=? ?2;
3. (non-adjoining) there is w ? W1 with
g1(w) = p and (?1s , ?1t , V1) = (?2s , ?2t , V2),
W2 = W1 \ {w}, and g2 is g1 restricted to
W2; this step is denoted by ?1
?w=? ?2;
4. (adjoining) there are w ? W1 with g1(w) =
a, and r = (?s, ?t, V,W, ?, P radj) ? A such
that, for w = (ws, wt),
? (?1s (ws), ?1t (wt)) = rc(r),
? ?2s = ?1s [? ?s]ws where ? ?s = ?s[?1s |ws ]?s ,
?2t = ?1t [? ?t]wt where ? ?t = ?t[?1t |wt ]?t ,
? V2 is the smallest set such that (i) for
every (us, ut) ? V1 we have (u?s, u?t) ? V2
where
u?s =
{
us if ws is not a prefix of us,
ws. ?s .u if us = ws.u for some u;
and u?t is obtained in the same way from ut,
wt, and ?t, and (ii) V2 contains w.V ;
? W2 is the smallest set such that (i) for every
(us, ut) ? W1 \ {w} we have (u?s, u?t) ?
W2 where u?s and u?t are obtained in the
same way as for V2, and g2(u?s, u?t) =
g1(us, ut) and (ii) W2 contains w.W and
g2(w.u) = p for every u ?W ;
this step is denoted by ?1
w,r=? ?2.
3w.V = {(ws.vs, wt.vt) | (vs, vt) ? V }
In Fig. 2 we show a derivation of our running
example pSTAG where activated adjoining sites
are indicated by surrounding circles, the other ad-
joining sites are potential.
Ss? ?? St?
substitution of
r1 at (?, ?)
=? P (r1) = 1
Ss
? A? A
?
a ??
St
B
?
B? ?a
substitution of
r4 at (2, 2)
=? P (r4) = .1
Ss
? A
?
A
?
a ??
St
B
?
B
?
?a
activation
at a = (3, 1)
=? P r1adj(a) = .9
Ss
? A
?
A
?
a ??
St
B
?
B
?
?a
adjoining of
r2 at a = (3, 1)
=? P (r2) = .4
Ss
? A
?
A
A
?
?
b,c ??
St
B
B
?
B
?
B
?
?b
c
non-adjoining
at c = (3, 1.1)
=? 1? P r2adj(c) = .4
Ss
? A
?
A
A
?
?
b ??
St
B
B
?
B
?
B
?
?b
non-adjoining
at b = (3, 1)
=? 1? P r2adj(b) = .8
Ss
? A
?
A
A
?
?
??
St
B
B
?
B
?
B
?
?
Figure 2: An example derivation with total proba-
bility 1? .1? .9? .4? .4? .8 = .01152.
The only initial sentential form is ?in =
(Ss, St, {(?, ?)}, ?, ?). A sentential form ? is final
if it has the form (?s, ?t, ?, ?, ?). Let ? ? SF(G).
A derivation (of ?) is a sequence d of the form
?0u1?1 . . . un?n with ?0 = ?in and n ? 0,
?i?1
ui? ?i for every 1 ? i ? n (and ?n = ?). We
12
denote ?n also by last(d), and the set of all deriva-
tions of ? (resp., derivations) by D(?) (resp., D).
We call d ? D successful if last(d) is final.
The tree transformation computed by G is
the relation ?G ? UN (T ) ? UN (T ) with
(?s, ?t) ? ?G iff there is a successful derivation
of (?s, ?t, ?, ?, ?).
Our definition of the probability of a deriva-
tion is based on the following observation.4 Let
d ? D(?) for some ? = (?s, ?t, V,W, g). Then,
for every w ? W , the rule which created w and
the corresponding local position in that rule can
be retrieved from d. Let us denote this rule by
r(d, ?, w) and the local position by l(d, ?, w).
Now let d be the derivation ?0u1?1 . . . un?n.
Then the probability of d is defined by
P (d) =
?
1?i?n
Pd(?i?1
ui? ?i)
where
1. (substitution) Pd(?i?1 w,r=? ?i) = P (r)
2. (activation)
Pd(?i?1 w=? ?i) = P r
?
adj(w?) where r? =
r(d, ?i?1, w) and w? = l(d, ?i?1, w)
3. (non-adjoining)
Pd(?i?1 ?w=? ?i) = 1 ? P r
?
adj(w?) where r?
and w? are defined as in the activation case
4. (adjoining)
Pd(?i?1
w,r=? ?i) = P (r).
In order to describe the generative model of
G, we impose a deterministic strategy sel on the
derivation relation in order to obtain, for every
sentential form, a probability distribution among
the follow-up sentential forms. A deterministic
derivation strategy is a mapping sel : SF(G) ?
(N? ? N?) ? {?} such that for every ? =
(?s, ?t, V,W, g) ? SF(G), we have that sel(?) ?
V ?W if V ?W 6= ?, and sel(?) = ? otherwise.
In other words, sel chooses the next site to operate
on. Then we define ?sel in the same way as ? but
in each of the cases we require that w = sel(?1).
Moreover, for every derivation d ? D, we denote
by next(d) the set of all derivations of the form
du? where last(d) u?sel ?.
The generative model of G comprises all the
generative stories of G. A generative story is a
tree t ? UD; the root of t is labeled by ?in. Let
w ? pos(t) and t(w) = d. Then either w is a
leaf, because we have stopped the generative story
4We note that a different definition occurs in (Nesson et
al., 2005, 2006).
at w, or w has |next(d)| children, each one repre-
sents exactly one possible decision about how to
extend d by a single derivation step (where their
order does not matter). Then, for every generative
story t, we have that
?
w?lv(t)
P (t(w)) = 1 .
We note that (D,next, ?) can be considered as
a discrete Markov chain (cf., e.g. (Baier et al,
2009)) where the initial probability distribution
? : D ? [0, 1] maps d = ?in to 1, and all the
other derivations to 0.
A probabilistic synchronous tree insertion
grammar (pSTIG) G is a pSTAG except that
for every rule r = (?s, ?t, V,W, P radj) or r =
(?s, ?t, V,W, ?, P radj) we have that
? if r ? A, then |lv(?s)| ? 2 and |lv(?t)| ? 2,
? for ? = (?s, ?t) we have that ?s is either the
rightmost leaf of ?s or its leftmost one; then
we call r, resp., L-auxiliary in the source and
R-auxiliary in the source; similarly, we re-
strict ?t; the source-spine of r (target-spine
of r) is the set of prefixes of ?s (resp., of ?t)
? W ? nlvN (?s)?{L,R}?nlvN (?t)?{L,R}
where the new components are the direction-
type of the potential adjoining site, and
? for every (ws, ?s, wt, ?t) ? W , if ws lies on
the source-spine of r and r is L-auxiliary (R-
auxiliary) in the source, then ?s = L (resp.,
?s = R), and corresponding restrictions hold
for the target component.
According to the four possibilities for the foot-
node ? we call r LL-, LR-, RL-, or RR-auxiliary.
The restriction for the probability distribution P of
G is modified such that for every (A,B) ? N?N
and x, y ? {L,R}:
?
r?A, rc(r)=(A,B)
r is xy?auxiliary
P (r) = 1 .
In the derivation relation of the pSTIG G we
will have to make sure that the direction-type of
the chosen adjoining site w matches with the type
of auxiliarity of the auxiliary rule. Again we as-
sume that the data structure SF(G) is enriched
such that for every potential adjoining site w of
? ? SF(G) we know its direction-type dir(w).
We define the derivation relation of the pSTIG
G to be the binary relation ?I ? SF(G)?SF(G)
such that we have ?1 ?I ?2 iff (i) ?1 ? ?2 and
13
(ii) if adjoining takes place atw, then the used aux-
iliary rule must be dir(w)-auxiliary. Since ?I is
a subset of ?, the concepts of derivation, success-
ful derivation, and tree transformation are defined
also for a pSTIG.
In fact, our running example pSTAG in Fig. 1 is
a pSTIG, where r2 and r3 are RL-auxiliary and
every potential adjoining site has direction-type
RL; the derivation shown in Fig. 2 is a pSTIG-
derivation.
4 Bottom-up tree adjoining transducer
Here we introduce the concept of a bottom-up tree
adjoining transducer (BUTAT) which will be used
to formalize a decoder for a pSTIG.
A BUTAT is a finite-state machine which trans-
lates strings into trees. The left-hand side of each
rule is a string over terminal symbols and state-
variable combinations. A variable is either a sub-
stitution variable or an adjoining variable; a substi-
tution variable (resp., adjoining variable) can have
an output tree (resp., output tree with foot node) as
value. Intuitively, each variable value is a transla-
tion of the string that has been reduced to the cor-
responding state. The right-hand side of a rule has
the form q(?) where q is a state and ? is an output
tree (with or without foot-node); ? may contain the
variables from the left-hand side of the rule. Each
rule has a probability p ? [0, 1].
In fact, BUTAT can be viewed as the string-
to-tree version of bottom-up tree transducers (En-
gelfriet, 1975; Gecseg and Steinby, 1984,1997) in
which, in addition to substitution, adjoining is al-
lowed.
Formally, we let X = {x1, x2, . . .} and F =
{f1, f2, . . .} be the sets of substitution variables
and adjoining variables, resp. Each substitu-
tion variable (resp., adjoining variable) has rank
0 (resp., 1). Thus when used in a tree, substitu-
tion variables are leaves, while adjoining variables
have a single child.
A bottom-up tree adjoining transducer (BU-
TAT) is a tuple M = (Q,?,?, Qf , R) where
? Q is a finite set (of states),
? ? is an alphabet (of input symbols), assuming
that Q ? ? = ?,
? ? is an alphabet (of output symbols),
? Qf ? Q (set of final states), and
? R is a finite set of rules of the form
?0 q1(z1) ?1 . . . qk(zk) ?k
p? q(?) (?)
where p ? [0, 1] (probability of (?)), k ? 0,
?0, ?1, . . . , ?k ? ??, q, q1, . . . , qk ? Q,
z1, . . . , zk ? X ? F , and ? ? RHS(k)
where RHS(k) is the set of all trees over
? ? {z1, . . . , zk} ? {?} in which the nullary
? occurs at most once.
The set of intermediate results of M is the set
IR(M) = {? | ? ? U?({?}), |pos{?}(?)| ? 1}
and the set of sentential forms of M is the set
SF(M) = (? ? {q(?) | q ? Q, ? ? IR(M)})?.
The derivation relation induced by M is the bi-
nary relation ? ? SF(M) ? SF(M) such that
for every ?1, ?2 ? SF(M) we define ?1 ? ?2 iff
there are ?, ?? ? SF(M), there is a rule of the form
(?) in R, and there are ?1, . . . , ?k ? IR(M) such
that:
? for every 1 ? i ? k: if zi ? X , then ?i does
not contain ?; if zi ? F , then ?i contains ?
exactly once,
? ?1 = ? ?0 q1(?1) ?1 . . . qk(?k) ?k ??, and
? ?2 = ? q(?(?)) ??
where ? is a function that replaces variables
in a right-hand side with their values (subtrees)
from the left-hand side of the rule. Formally,
? : RHS(k) ? IR(M) is defined as follows:
(i) for every ? = ?(?1, . . . , ?n) ? RHS(k), ? ?
?, we have ?(?) = ?(?(?1), . . . , ?(?n)),
(ii) (substitution) for every zi ? X , we have
?(zi) = ?i,
(iii) (adjoining) for every zi ? F and ? ?
RHS(k), we have ?(zi(?)) = ?i[?(?)]v
where v is the uniquely determined position
of ? in ?i, and
(iv) ?(?) = ?.
Clearly, the probablity of a rule carries over to
derivation steps that employ this rule. Since, as
usual, a derivation d is a sequence of derivation
steps, we let the probability of d be the product of
the probabilities of its steps.
The string-to-tree transformation computed by
M is the set ?M of all tuples (?, ?) ? ???U? such
that there is a derivation of the form ? ?? q(?) for
some q ? Qf .
5 Decoder for pSTIG
Now we construct the decoder dec(G) for a pSTIG
G that transforms source strings directly into tar-
get trees and simultaneously computes the proba-
bility of the corresponding derivation of G. This
decoder is formalized as a BUTAT.
Since dec(G) is a string-to-tree transducer, we
14
have to transform the source tree ?s of a rule r
into a left-hand side ? of a dec(G)-rule. This is
done similarly to (DeNeefe and Knight, 2009) by
traversing ?s via recursive descent using a map-
ping ? (see an example after Theorem 1); this
creates appropriate state-variable combinations for
all substitution sites and potential adjoining sites
of r. In particular, the source component of the
direction-type of a potential adjoining site deter-
mines the position of the corresponding combina-
tion in ?. If there are several potential adjoining
sites with the same source component, then we
create a ? for every permutation of these sites. The
right-hand side of a dec(G)-rule is obtained by
traversing the target tree ?t via recursive descent
using a mapping ?? and, whenever a nonterminal
with a potential adjoining site w is met, a new po-
sition labeled by fw is inserted.5 If there is more
than one potential adjoining site, then the set of
all those sites is ordered as in the left-hand side ?
from top to bottom.
Apart from these main rules we will employ
rules which implement the decision of whether or
not to turn a potential adjoining site w into an ac-
tivated adjoining site. Rules for the first purpose
just pass the already computed output tree through
from left to right, whereas rules for the second pur-
pose create for an empty left-hand side the output
tree ?.
We will use the state behavior of dec(G) in or-
der to check that (i) the nonterminals of a substi-
tution or potential adjoining site match the root-
category of the used rule, (ii) the direction-type
of an adjoining site matches the auxiliarity of the
chosen auxiliary rule, and (iii) the decisions of
whether or not to adjoin for each rule r of G are
kept separate.
Whereas each pair (?s, ?t) in the translation of
G is computed in a top-down way, starting at the
initial sentential form and substituting and adjoin-
ing to the present sentential form, dec(G) builds
?t in a bottom-up way. This change of direction is
legitimate, because adjoining is associative (Vijay-
Shanker and Weir, 1994), i.e., it leads to the same
result whether we first adjoin r2 to r1, and then
align r3 to the resulting tree, or first adjoin r3 to
r2, and then adjoin the resulting tree to r1.
In Fig. 3 we show some rules of the decoder
of our running example pSTIG and in Fig. 4 the
5We will allow variables to have structured indices that
are not elements of N. However, by applying a bijective re-
naming, we can always obtain rules of the form (?).
derivation of this decoder which correponds to the
derivation in Fig. 2.
Theorem 1. Let G be a pSTIG over N and T .
Then there is a BUTAT dec(G) such that for ev-
ery (?s, ?t) ? UN (T ) ? UN (T ) and p ? [0, 1] the
following two statements are equivalent:
1. there is a successful derivation of
(?s, ?t, ?, ?, ?) by G with probability p,
2. there is a derivation from yield(?s) to
[Ss, St](?t) by dec(G) with probability p.
PROOF. Let G = (N,T, [Ss, St],S,A, P ) be a
pSTIG. We will construct the BUTAT dec(G) =
(Q,T,N ?T, {[Ss, St]}, R) as follows (where the
mappings ? and ?? will be defined below):
? Q = [N ?N ] ? [N ?{L,R}?N ?{L,R}]
?{[r, w] | r ? A, w is an adjoining site of r},
? R is the smallest set R? of rules such
that for every r ? S ? A of the form
(?s, ?t, V,W, P radj) or (?s, ?t, V,W, ?, P radj):
? for every ? ? ?(?), if r ? S, then the
main rule
? P (r)? [?s(?), ?t(?)]
(
??(?)
)
is in R?, and if r ? A and r is ?s?t-
auxiliary, then the main rule
? P (r)? [?s(?), ?s, ?t(?), ?t]
(
??(?)
)
is in R?, and
? for every w = (ws, ?s, wt, ?t) ? W the
rules
qw
(
fw
) P radj(w)?? [r, w]
(
fw(?)
)
with qw = [?(ws), ?s, ?t(wt), ?t] for ac-
tivation at w, and the rule
?
1?P radj(w)?? [r, w](?)
for non-adjoining at w are in R?.
We define the mapping
? : pos(?s) ? P((T ?Q(X ? F ))?)
with Q(X ? F ) = {q(z) | q ? Q, z ? X ? F}
inductively on its argument as follows. Let w ?
pos(?s) and let w have n children.
(a) Let ?s(w) ? T . Then ?(w) = {?s(w)}.
15
(b) (substitution site) Let ?s(w) ? N and let
w? ? pos(?t) such that (w,w?) ? V . Then
?(w) = {[?s(w), ?t(w?)]
(
x(w,w?)
)
}.
(c) (adjoining site) Let ?s(w) ? N and let there
be an adjoining site in W with w as first
component. Then, we define ?(w) to be the
smallest set such that for every permutation
(u1, . . . , ul) (resp., (v1, . . . , vm)) of all the L-
adjoining (resp., R-adjoining) sites inW with
w as first component, the set6
J ? ?(w.1) ? . . . ? ?(w.n) ?K
is a subset of ?(w), where J = {u?1 . . . u?l}
and K = {v?m . . . v?1} and
u?i = [r, ui]
(
fui
)
and v?j = [r, vj ]
(
fvj
)
for 1 ? i ? l and 1 ? j ? m.
(d) Let ?s(w) ? N , w 6= ?, and let w be neither
the first component of a substitution site in V
nor the first component of an adjoining site in
W . Then
?(w) = ?(w.1) ? . . . ? ?(w.n) .
(e) Let w = ?. Then we define ?(w) = {?}.
For every ? ? ?(?), we define the mapping
?? : pos(?t) ? UN?F?X(T ? {?})
inductively on its argument as follows. Let
w ? pos(?t) and let w have n children.
(a) Let ?t(w) ? T . Then ??(w) = ?t(w).
(b) (substitution site) Let ?t(w) ? N and let
w? ? pos(?s) such that (w?, w) ? V . Then
??(w) = x(w?,w).
(c) (adjoining site) Let ?t(w) ? N and let there
be an adjoining site in W with w as third
component. Then let {u1, . . . , ul} ? W be
the set of all potential adjoining sites with w
as third component, and we define
??(w) = fu1(. . . ful(?) . . .)
where ? = ?t(w)(??(w.1), . . . , ??(w.n))
and the ui?s occur in ??(w) (from the root
towards the leaves) in exactly the same order
as they occur in ? (from left to right).
(d) Let ?t(w) ? N , w 6= ?, and let w be neither
the second component of a substitution site
in V nor the third component of an adjoining
site in W . Then
??(w) = ?t(w)(??(w.1), . . . , ??(w.n)).
6using the usual concatenation ? of formal languages
(e) Let w = ?. Then ??(w) = ?.
With dec(G) constructed as shown, for each
derivation of G there is a corresponding deriva-
tion of dec(G), with the same probability, and vice
versa. The derivations proceed in opposite direc-
tions. Each sentential form in one has an equiv-
alent sentential form in the other, and each step
of the derivations correspond. There is no space
to present the full proof, but let us give a slightly
more precise idea about the formal relationship be-
tween the derivations of G and dec(G).
In the usual way we can associate a deriva-
tion tree dt with every successful derivation d of
G. Assume that last(d) = (?s, ?t, ?, ?, ?), and
let Es and Et be the embedded tree transducers
(Shieber, 2006) associated with, respectively, the
source component and the target component of
G. Then it was shown in (Shieber, 2006) that
?Es(dt) = ?s and ?Et(dt) = ?t where ?E de-
notes the tree-to-tree transduction computed by an
embedded tree transducer E. Roughly speaking,
Es and Et reproduce the derivations of, respec-
tively, the source component and the target com-
ponent of G that are prescribed by dt. Thus, for
? = (??s, ??t, V,W, g), if ?in ??G ? and ? is a prefix
of d, then there is exactly one subtree dt[(w,w?)]
of dt associated with every (w,w?) ? V ? W ,
which prescribes how to continue at (w,w?) with
the reproduction of d. Having this in mind, we ob-
tain the sentential form of the dec(G)-derivation
which corresponds to ? by applying a modifica-
tion of ? to ? where the modification amounts to
replacing x(w,w?) and f(w,w?) by ?Et(dt[(w,w?)]);
note that ?Et(dt[(w,w?)]) might contain ?. 
As illustration of the construction in Theorem 1
let us apply the mappings ? and ?? to rule r2 of
Fig. 1, i.e., to r2 = (?s, ?t, ?, {b, c}, ?, P r2adj)
with ?s = A(A, ?), ?t = B(B(?), B),
b = (?,R, ?,L), c = (?,R, 1,L), and ? = (1, 2).
Let us calculate ?(?) on ?s. Due to (c),
?(?) = J ? ?(1) ? ?(2) ?K.
Since there are no L-adjoinings at ?, we have that
J = {?}. Since there are the R-adjoinings b and c
at ?, we have the two permutations (b, c) and (c, b).
(v1, v2) = (b, c): K = {[r2, c](fc)[r2, b](fb)}
(v1, v2) = (c, b): K = {[r2, b](fb)[r2, c](fc)}
Due to (e) and (a), we have that ?(1) = {?} and
?(2) = {?}, resp. Thus, ?(?) is the set:
{? [r2, c](fc) [r2, b](fb), ? [r2, b](fb) [r2, c](fc)}.
16
r1
(r1, a)
r2
(r2,?b) (r2,?c)
r4
?
[A,B]
x(2,2)
?
[r1, a]
fa
1??
[Ss, St]
St
f
B
?
x ?a (2,2)
[A,R, B,L]
fa
.9??
[r1, a]
f
?
a
?
[r2, b]
f b
[r2, c]
fc
.4??
[A,R, B,L]
f
B
f
B
?
?
b
c
? .8??
[r2, b]
?
? .4??
[r2, c]
?
? .1??
[A,B]
B
?
Figure 3: Some rules of the running example de-
coder.
Now let ? = ? [r2, b](fb) [r2, c](fc). Let us cal-
culate ??(?) on ?t.
??(?)
(c)= fb(B(??(1), ??(2)))
(c)= fb(B(fc(B(??(1.1))), ??(2)))
(a)= fb(B(fc(B(?)), ??(2)))
(e)= fb(B(fc(B(?)), ?))
Hence we obtain the rule
? [r2, b](fb) [r2, c](fc) ?
[A,R, B,L](fb(B(fc(B(?)), ?)))
which is also shown in Fig. 3.
? ? ? ?
? ? ? ?
[r2, b]
?
? ? ? ?
[r2, b]
?
[r2, c]
?
? ? ?
? ? ?
?
[A,B]
B
?
?
[A,R, B,L]
B
B
?
?
=?
=?
=?
=?
=?
[Ss, St]
St
B
B
?
B
?
B
?
?
prob. .8
prob. .4
prob. .4
prob. .9
prob. .1
=? prob. .1
[r1, a]
B
B
?
?
(r2,?b)
(r2,?c)
(r2, bc)
(r1, a)
r4
r1
[r1, a]
B
B
?
?
Figure 4: Derivation of the decoder corresponding
to the derivation in Fig. 2.
17
References
A. Abeille, Y. Schabes, A.K. Joshi. Using lexicalized
TAGs for machine translation. In Proceedings of
the 13th International Conference on Computational
Linguistics, volume 3, pp. 1?6, Helsinki, Finland,
1990.
C. Baier, M. Gro??er, F. Ciesinski. Model checking
linear-time properties of probabilistic systems. In
Handbook of Weighted Automata, Chapter 13, pp.
519?570, Springer, 2009.
S. DeNeefe. Tree adjoining machine translation. Ph.D.
thesis proposal, Univ. of Southern California, 2009.
S. DeNeefe, K. Knight. Synchronous tree adjoining
machine translation. In Proc. of Conf. Empirical
Methods in NLP, pp. 727?736, 2009.
J. Engelfriet. Bottom-up and top-down tree transfor-
mations ? a comparison. Math. Systems Theory,
9(3):198?231, 1975.
J. Engelfriet. Tree transducers and syntax-directed se-
mantics. In CAAP 1982: Lille, France, 1982.
A. Fujiyoshi, T. Kasai. Spinal-formed context-free tree
grammars. Theory of Computing Systems, 33:59?
83, 2000.
Z. Fu?lo?p, H. Vogler. Weighted tree automata and tree
transducers. In Handbook of Weighted Automata,
Chapter 9, pp. 313?403, Spinger, 2009.
F. Ge?cseg, M. Steinby. Tree Automata. Akade?miai
Kiado?, Budapest, 1984.
F. Ge?cseg, M. Steinby. Tree languages. In Handbook
of Formal Languages, volume 3, chapter 1, pages
1?68. Springer-Verlag, 1997.
J. Graehl, K. Knight, J. May. Training tree transducers.
Computational Linguistics, 34(3):391?427, 2008
A.K. Joshi, L.S. Levy, M. Takahashi. Tree adjunct
grammars. Journal of Computer and System Sci-
ences, 10(1):136?163, 1975.
A.K. Joshi, Y. Schabes. Tree-adjoining grammars. In
Handbook of Formal Languages. Chapter 2, pp. 69?
123, Springer-Verlag, 1997.
K. Knight, J. Graehl. An overview of probabilis-
tic tree transducers for natural language processing.
In Computational Linguistics and Intelligent Text
Processing, CICLing 2005, LNCS 3406, pp. 1?24,
Springer, 2005.
K. Knight, J. May. Applications of Weighted Au-
tomata in Natural Language Processing. In Hand-
book of Weighted Automata, Chapter 14, pp. 571?
596, Springer, 2009.
P.M. Lewis, R.E. Stearns. Syntax-directed transduc-
tions. Journal of the ACM, 15:465?488, 1968.
A. Maletti. Compositions of tree series transforma-
tions. Theoret. Comput. Sci., 366:248?271, 2006.
A. Maletti. The Power of Tree Series Transducers.
Ph.D. thesis, TU Dresden, Germany, 2006.
A. Maletti. Compositions of extended top-down
tree transducers. Information and Computation,
206:1187?1196, 2008.
A. Maletti. Why synchronous tree substitution gram-
mars? in Proc. 11th Conf. North American Chap-
ter of the Association of Computational Linguistics.
2010.
D.F. Martin and S.A. Vere. On syntax-directed trans-
ductions and tree transducers. In Ann. ACM Sympo-
sium on Theory of Computing, pp. 129?135, 1970.
R. Nesson, S.M. Shieber, and A. Rush. Induction
of probabilistic synchronous tree-insertion gram-
mars. Technical Report TR-20-05, Computer Sci-
ence Group, Harvard Univeristy, Cambridge, Mas-
sachusetts, 2005.
R. Nesson, S.M. Shieber, and A. Rush. Induction of
probabilistic synchronous tree-inserting grammars
for machine translation. In Proceedings of the 7th
Conference of the Association for Machine Transla-
tion in the Americas (AMTA 2006), 2006.
Y. Schabes, R.C. Waters. Tree insertion grammars:
a cubic-time, parsable formalism that lexicalizes
context-free grammar without changing the trees
produced. Computational Linguistics, 21:479?513,
1994.
P.P. Schreiber. Tree-transducers and syntax-connected
transductions. In Automata Theory and Formal
Languages, Lecture Notes in Computer Science 33,
pp. 202?208, Springer, 1975.
S.M. Shieber. Synchronous grammars and tree trans-
ducers. In Proc. 7th Workshop on Tree Adjoin-
ing Grammars and Related Formalisms, pp. 88?95,
2004.
S.M. Shieber. Unifying synchronous tree-adjoining
grammars and tree transducers via bimorphisms. In
Proc. 11th Conf. European Chapter of ACL, EACL
06, pp. 377?384, 2006.
S.M. Shieber, Y. Schabes. Synchronous tree-adjoining
grammars. In Proceedings of the 13th Interna-
tional Conference on Computational Linguistics,
volume 3, pp. 253?258, Helsinki, Finland, 1990.
K. Vijay-Shanker, D.J. Weir. The equivalence of four
extensions of context-free grammars. Mathematical
Systems Theory, 27:511?546, 1994.
K. Yamada and K. Knight. A syntax-based statistical
translation model. In Proc. of 39th Annual Meeting
of the Assoc. Computational Linguistics, pp. 523?
530, 2001.
18
