Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 41?51,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Multi-Word Unit Dependency Forest-based Translation Rule
Extraction
Hwidong Na Jong-Hyeok Lee
Department of Computer Science and Engineering
Pohang University of Science and Technology (POSTECH)
San 31 Hyoja Dong, Pohang, 790-784, Republic of Korea
{leona,jhlee}@postech.ac.kr
Abstract
Translation requires non-isomorphic
transformation from the source to the
target. However, non-isomorphism can
be reduced by learning multi-word units
(MWUs). We present a novel way of
representating sentence structure based
on MWUs, which are not necessarily
continuous word sequences. Our pro-
posed method builds a simpler structure
of MWUs than words using words
as vertices of a dependency structure.
Unlike previous studies, we collect
many alternative structures in a packed
forest. As an application of our proposed
method, we extract translation rules in
form of a source MWU-forest to the
target string, and verify the rule coverage
empirically. As a consequence, we
improve the rule coverage compare to a
previous work, while retaining the linear
asymptotic complexity.
1 Introduction
Syntax is the hierarchical structure of a natu-
ral language sentence. It is generally repre-
sented with tree structures using phrase struc-
ture grammar (PSG) or dependency grammar
Figure 1: A pair of sentences that require long
distance reordering (dashed line) and discontinuous
translation (thick line)
(DG). Although the state-of-the-art statistical
machine translation (SMT) paradigm is phrase-
based SMT (PBSMT), many researchers have
attempted to utilize syntax in SMT to over-
come the weaknesses of PBSMT. An emerging
paradigm alternative to PBSMT is syntax-based
SMT, which embeds the source and/or target
syntax in its translation model (TM). Utilizing
syntax in TM has two advantages over PBSMT.
The first advantage is that syntax eases global
reordering between the source and the target
language. Figure 1 shows that we need global
reordering in a complex real situation, where
a verbal phrase requires a long distance move-
ment. PBSMT often fails to handle global re-
ordering, for example, from subject-verb-object
(SVO) to SOV transformation where V should
be moved far away from the original position in
41
Table 1: Statistics of the corresponding target words
for the continuous word sequences in the source lan-
guage, or vice versa. C denotes consistent, O over-
lapped, D discontinuous, and N null.
Word Alignment C O D N
Manual 25 60 10 5
Automatic 20 55 15 5
the source language. This is because of the two
distance-based constraints in PBSMT: the dis-
tortion model cost and the distortion size limit.
For the distortion model cost, PBSMT sets zero
cost to the monotone translation and penalizes
the distorted translations as the distortion grows
larger. For the distortion size limit, a phrase can
only be moved from its original position within
a limit. Therefore, PBSMT fails to handle long
distance reordering. Syntax-based SMT man-
ages global reordering as structural transforma-
tion. Because reordering occurs at the sub-
structure level such as constituents or treelets
in syntax-based SMT, the transformation of the
sub-structure eventually yields the reordering of
the whole sentence.
The second advantage of using syntax in TM
is that syntax guides us to discontinuous trans-
lation patterns. Because PBSMT regards only
a continuous sequence of words as a transla-
tion pattern, it often fails to utilize many use-
ful discontinuous translation patterns. For ex-
ample, two discontinuous source words corre-
spond to a target word in Figure 1. In our in-
spection of the training corpus, a continuous
word sequence often corresponds to a set of
discontinuous words in the target language, or
vice versa (Table 1). Discontinuous translation
patterns frequently appear in many languages
(S?gaard and Kuhn, 2009). Syntax-based SMT
overcomes the limitations of PBSMT because it
finds discontinuous patterns along with the hier-
Figure 2: The maximum branching factor (BF) and
depth factor (DF) in a dependency tree in our corpus
archical structure. For example, the two discon-
tinuous source words have a head-dependent re-
lation (Figure 3). Especially with the depen-
dency tree, we can easily identify patterns that
have non-projectivity (Na et al, 2010). How-
ever, syntax-based patterns such as constituents
or treelets do not sufficiently cover various use-
ful patterns, even if we have the correct syn-
tactic analysis (Chiang, 2010). For this reason,
many researchers have proposed supplementary
patterns such as an intra/inter constituent or se-
quence of treelets (Galley et al, 2006; Shen et
al., 2008).
Unlike PSG, DG does not include non-
terminal symbols, which represent constituent
information. This makes DG simpler than PSG.
For instance, it directly associates syntatic role
with the structure, but introduces a difficulty in
syntax-based SMT. The branching factor of a
dependency tree becomes larger when a head
word dominates many dependents. We ob-
serve that the maximum branching factor of
an automatically parsed dependency tree ranges
widely, while most trees have depth under a cer-
tain degree (Figure 2). This indicates that we
have a horizontally flat dependency tree struc-
ture. The translation patterns extracted from the
42
flat dependency tree are also likely to be flat.
Unfortunately, the flat patterns are less appli-
cable at the decoding stage. When one of the
modifiers does not match, for instance, we fail
to apply the translation pattern. Therefore, we
need a more generally applicable representation
for syntax-based SMT using DG.
We propose a novel representation of DG that
regards a set of words as a unit of the depen-
dency relations, similar to (Ding, 2006; Wu et
al., 2009; Na et al, 2010). Unlike their work,
we consider many alternatives without prede-
fined units, and construct a packed forest of the
multi-word units (MWUs) from a dependency
tree. For brevity, we denote the forest based on
MWUs as an MWU-forest. Because all pos-
sible alternatives are exponentially many, we
give an efficient algorithm that enumerates the
k-best alternatives in section 3. As an appli-
cation, we extract translation patterns in form
of a source MWU-forest to the target string in
order to broaden the coverage of the extracted
patterns for syntax-based SMT in section 4. We
also report empirical results related to the use-
fulness of the extracted pattern in section 5. The
experimental results show that the MWU-forest
representation gives more applicable translation
patterns than the original word-based tree.
2 Related Work
Previous studies have proposed merging alter-
native analyses to deal with analysis errors for
two reasons: 1) the strongest alternative is not
necessarily the correct analysis, and 2) most
alternatives contain similar elements such as
common sub-trees. For segmentation alterna-
tives, Dyer et al (2008) proposed a word lattice
that represents exponentially large numbers of
segmentations of a source sentence, and inte-
grates reordering information into the lattice as
well. For parsing alternatives, Mi et al (2008)
suggested a packed forest that encodes alterna-
tive PSG derivations. Futher, Mi et al (2010)
combined the two approaches in order to bene-
fit from both.
The translation literature also shows that
translation requires non-isomorphic transfor-
mation from the source to the target. This yields
translation divergences such as head-switching
(Dorr, 1994). Ding and Palmer (2005) reported
that the percentage of the head-swapping cases
is 4.7%, and that of broken dependencies is
59.3% between Chinese and English. The large
amount of non-isomorphism, however, will be
reduced by learning MWUs such as elementary
trees (Eisner, 2003).
There are few studies that consider a depen-
dency structure based on MWUs. Ding (2006)
suggested a packed forest which consists of the
elementary trees, and described how to find
the best decomposition of the dependency tree.
However, Ding (2006) did not show how to de-
termine the MWUs and restrict them to form
a subgraph from a head. For opinion mining,
Wu et al (2009) also utilized a dependency
structure based on MWUs, although they re-
stricted MWUs with predefined relations. Na
et al (2010) proposed an MWU-based depen-
dency tree-to-string translation rule extraction,
but considered only one decomposition for ef-
ficiency. Our proposed method includes addi-
tional units over Ding?s method, such as a se-
quence of subgraphs within a packed forest. It
is also more general than Wu et al?s method
because it does not require any predefined re-
lations. We gain much better rule coverage
against Na et al?s method, while retaining linear
asymptotical computational time.
43
Figure 3: A dependency tree of the source sentence
in Figure 1
3 MWU-based Dependency Forest
There are two advantages when we use the
MWU-forest representaion with DG. First, we
express the discontinuous patterns in a vertex,
so that we can extract more useful translation
patterns beyond continuous ones for syntax-
based SMT. Second, an MWU-forest contains
many alternative structures which may be sim-
pler structures than the original tree in terms of
the branching factor and the maximum depth.
Wu et al (2009) utilized an MWU-tree to iden-
tify the product features in a sentence easily.
As in previous literature in syntax-based
SMT using DG, we only consider the well-
formed MWUs where an MWU is either a
treelet (a connected sub-graph), or a sequence
of treelets under a common head. In other
words, each vertex in an MWU-forest is either
?fixed on head? or ?floating with children?. The
formal definitions can be found in (Shen et al,
2008).
We propose encoding multiple dependency
structures based on MWUs into a hypergraph.
A hypergraph is a compact representation of
exponetially many variations in a polynomi-
nal space. Unlike PSG, DG does not have
Figure 4: An MWU-forest of Figure 3. The dashed
line indicates the alternative hyperedges.
non-terminals that represent the linguistically
motivated, intermediate structure such as noun
phrases and verb phrases. For this simplicity,
Tu et al (2010) proposed a dependency forest
as a hypergraph, regarding a word as a vertex
with a span that ranges for all its descendants.
The dependency forest offers tolerence of pars-
ing errors.
Our representation is different from the de-
pendency forest of Tu et al (2010) since a ver-
tex corresponds to multiple words as well as
words. Note that our representation is also
capable of incorporating multiple parse trees.
Therefore, MWU-forests will also be tolerant
of the parsing error if we provide multiple parse
trees. In this work, we concentrate on the ef-
fectiveness of MWUs, and hence utilize the
best dependency parse tree. Figure 4 shows an
MWU-forest of the dependency tree in Figure
3.
More formally, a hypergraph H = ?V,E?
consists of the vertices V and hyperedges
E. We assume that a length-J sentence has
a dependency graph which is single-headed,
44
acyclic, and rooted, i.e. hj is the index of the
head word of the j-th word, or 0 if the word is
the root. Each vertex v = {j|j ? [1, J ]} de-
notes a set of the indices of the words that satis-
fies the well-formed constraint. Each hyperedge
e = ?tails(e), head(e)? denotes a set of the de-
pendency relations between head(e) and ?v ?
tails(e). We include a special node v0 ? V
that denotes the dummy root of an MWU-forest.
Note that v0 does not appear in tails(e) for all
hyperedges. We denote |e| is the arity of hyper-
edge e, i.e. the number of tail nodes, and the
arity of a hypergraph is the maximum arity over
all hyperedges. Also, let ?(v) be the indices of
the words that the head lays out of the vertex,
i.e. ?(v) = {j|hj 6? v ? j ? v}, and ?(v) be
the indices of the direct dependent words of the
vertex, i.e. ?(v) = {j|hj ? v ? j 6? v}. Let
OUT (v) and IN(v) be the outgoing and in-
coming hyperedges of a vertex v, respectively.
It is challenging to weight the hyperedges
based on dependency grammar because a de-
pendency relation is a binary relation from a
head to a dependent. Tu et al (2010) assigned
a probability for each hyperedge based on the
score of the binary relation. We simply prefer
the hyperedges that have lower arity by scoring
as follows:
c(e) =
?
v?tails(e) |v|
|e|
p(e) = c(e)?
e??IN(head(e)) c(e?)
We convert a dependency tree into a hyper-
graph in two steps using the Inside-Outside al-
gorithm. Algorithm 1 shows the pseudo code
of our proposed method. At the first step, we
find the k-best incoming hyperedges for each
vertex (line 3-8), and compute the inside proba-
bility (line 9), in bottom-up order. At the sec-
ond step, we compute the outside probability
Algorithm 1 Build Forest
1: Initialize V
2: for v ? V in bottom-up order do
3: Create a chart C = |?(v)|2
4: for chart span [p, q] do
5: Initialize C[p, q] if ?v s.t. [p, q] = v or
?(v)
6: Combine C[p, i] and C[i + 1, q]
7: end for
8: Set IN(v) to the k-best in C[TOP ]
9: Set ?(v) as in Eq. 1
10: end for
11: for v ? V in top-down order do
12: Set ?(v) as in Eq. 2
13: end for
14: Prune out e if p(e) ? ?
15: return v0
(line 12) for each vertex in a top-down manner.
Finally we prune out less probable hyperedges
(line 14) similar to (Mi et al, 2008). The inside
and outside probabilities are defined as follows:
?(v) =
?
e?IN(v)
p(e)
?
d?tails(e)
?(d) (1)
where ?(v) = 1.0 if IN(v) = ?, and
?(v) =
?
h?OUT (v)
e?IN(head(h))
?(head(e))p(e)
|OUT (v)|
?
?
d?tails(e)\{v}
?(d) (2)
where ?(v) = 1.0 if OUT (v) = ?.
In practice, we restrict the number of words
in a vertex in the initialization (line 1). We ap-
proximate all possible alternative MWUs that
include each word as follows:
45
Figure 5: A sub-forest of Figure 4 with annotation
of aspan and cspan for each vertex. We omit the
span if it is not consistent.
? A horizontal vertex is a sequence of modi-
fiers for a common head word, and
? A vertical vertex is a path from a word to
one of the ancestors, and
? A combination of the horizontal vertices
and the vertical vertices, and
? A combination of the vertical vertices and
the vertical vertices.
The computational complexity of the initial-
izaion directly affects the complexity of the en-
tire procedure. For each word, generating the
horizontal vertices takes O(b2), and the vertical
vertices take O(bd?1), where b is the maximum
branching factor and d is the maximum depth
of a dependency tree. The two combinations
take O(bd+1) and O(b2(d?1)) time to initialize
the vertices. However, it takes O(mm+1) and
O(m2(m?1)) if we restrict the maximum num-
ber of the words in a vertex to a constant m.
Ding and Palmer (2005) insisted that the
Viterbi decoding of an MWU-forest takes lin-
ear time. In our case, we enumerate the k-best
incoming hyperedeges instead of the best one.
Because each enumeration takes O(k2|?(v)|3),
Table 2: The extracted rules in Figure 5. N denotes
the non-lexicalized rules with variables xi for each
v ? tails(e), and L denotes the lexicalized rule.
head(e) tails(e) rhs(?)
N
{3} {8} : x1 x1
{8} {4} : x1, {5} : x2 when x1 x2
{3, 8} {4, 5} : x1 when x1
{3, 8} {4} : x1, {5} : x2 when x1 x2
{4, 5} {6, 7} : x1 I?m in x1
{5} {6, 7} : x1 in x1
L
{6, 7}
N/A
the States
{4} I?m
{5} in
{4, 5} I?m in
{5, 6} in the State
{3, 8} When
the total time complexity also becomes linear
to the length of the sentence n similar to Ding
and Palmer (2005), i.e. O(|V |k2|?(v)|3), where
|V | = O(na2(a?1)) and a = min(m, b, d).
4 MWU-Forest-to-String Translation
Rule Extraction
As an application of our proposed MWU-forest,
we extract translation rules for syntax-based
SMT. Forest-based translation rule extraction
has been suggested by Mi and Huang (2008)
although their forest compacts the k-best PSG
trees. The extraction procedure is essentially
the same as Galley et al (2004), which iden-
tifies the cutting points (frontiers) and extracts
the sub-structures from a root to frontiers.
The situation changes in DG because DG
does not have intermediate representation. At
the dependency structure, a node corresponds
to two kinds of target spans. We borrow the
definitions of the aligned span (aspan), and the
covered span (cspan) from Na et al (2010), i.e.
46
? aspan(v) = [min(av),max(av)], and
? cspan(v) =
aspan(v)?d?tails(e)
e?IN(v)
cspan(d)
, where av = {i|j ? v ? (i, j) ? A}. Figure 5
shows aspans and cspans of a sub-forest of of
the MWU-forest in the previous example.
Each span type yields a different rule type:
aspan yields a lexicalized rule without any
variables, and cspan yields a non-lexicalized
rule with variables for the dependents of the
head word. For example, Table 2 shows the ex-
tracted rule in Figure 5.
In our MWU-forest, the rule extraction pro-
cedure is almost identical to a dependency
tree-to-string rule extraction except we regard
MWUs as vertices. Let fj and ei be the j-th
source and i-th target word, respectively. As an
MWU itself has a internal structure, a lexical
rule is a tree-to-string translation rule. There-
fore, a lexicalized rule is a pair of the source
words s and the target words t as follows:
s(v) = {fj |j ? v}
t(v) = {ei|i ? aspan(v)} (3)
In addition, we extract the non-lexicalized
rules from a hyperedge e to cspan of the
head(e). A non-lexicalized rule is a pair of the
source words in the vertices of a hyperedge and
the cspan of the target words with substitutions
of cspan(d) for each d ? tails(e). We abstract
d on the source with ?(d) for non-lexicalized
rules (row 2 in Table 2). We define the source
words s and the target words t as follows:
s(e) = {fj |j ? head(e) ? j ? ?(d)}
t(e) = {ei|i ? cspan(v) ? i 6? cspan(d)}
? {xi|d? xi} (4)
Algorithm 2 Extract Rules( H = ?V,E?)
1: ? = ?
2: for v ? V do
3: if aspan(v) is consistent then
4: ?? ? ? ? s(v) , t(v) ? as in Eq. 3
5: end if
6: if cspan(v) is consistent then
7: for e ? IN(v) do
8: if cspan(d)?d ? tails(e) then
9: ?? ?? ? s(e), t(e) ? as in Eq. 4
10: end if
11: end for
12: end if
13: end for
14: return ?
where d ? tails(e).
More formally, we extract a synchronous tree
substitution grammar (STSG) which regards the
MWUs as non-terminals.
Definition 1 A STSG using MWU (STSG-
MWU) is a 6-tuple G = ??S ,?T ,?,?, S, ??,
where:
? ?S and ?T are finite sets of terminals
(words, POSs, etc.) of the source and tar-
get languages, respectively.
? ? is a finite set of MWUs in the source
language, i.e. ? = {?S}+
? ? is a finite set of production rules
where a production rule ? : X ?
? lhs(?) , rhs(?), ? ?, which is a relation-
ship from ? to {x ? ?T } ?, where ? is the
bijective function from the source vertices
to the variables x in rhs(?). The asterisk
represents the Kleenstar operation, and
? S is the start symbol used to represent the
whole sentence, i.e. ?0 : S ? ? X , X ?.
47
For each type of span, we only extract the
rules if the target span has consistent word
alignments, i.e. span 6= ? ? ?i ? span,
{j|(i, j) ? A ? (i?, j) ? A s.t. i? 6? span} = ?.
Algorithm 2 shows the pseudo code of the
extraction. Because a vertex has aspan and
csapn, we extract a lexicalized rule (line 3-5)
and/or non-lexicalized rules (line 6-12) for each
vertex.
5 Experiment
We used the training corpus provided for the
DIALOG Task in IWSLT10 between Chinese
and English . The corpus is a collection of
30,033 sentence pairs and consists of dialogs in
travel situations (10,061) and parts of the BTEC
corpus (19,972). Details about the provided
corpus are described in (Paul, 2009). We used
the Stanford Parser 1 to obtain word-level de-
pendency structures of Chinese sentences, and
GIZA++ 2 to obtain word alignments of the
biligual corpus.
We extracted the SCFG-MWU from the
biligual corpus with word alignment. In or-
der to investigate the coverage of the extracted
rule, we counted the number of the recovered
sentences, i.e. counted if the extracted rule
for each sentence pair generates the target sen-
tence by combining the extracted rules. As we
collected many alternatives in an MWU-forest,
we wanted to determine the importance of each
source fragment. Mi and Huang (2008) penal-
ized a rule ? by the posterior probability of its
tree fragment lhs(?). This posterior probability
is also computed in the Inside-Outside fashion
that we used in Algorithm 1. Therefore, we re-
garded the fractional count of a rule ? as
1http://nlp.stanford.edu/software/lex-parser.shtml,
Version 1.6.4
2http://code.google.com/p/giza-pp/
Figure 6: The rule coverage according to the number
of the words in a vertex.
c(?) = ??(lhs(?))??(v0)
We prioritized the rule according to the frac-
tional count. The priority is used when we com-
bine the rules to restore the target sentence us-
ing the extracted rule for each sentence. We var-
ied the maximum size of a vertex m, and the
number of incoming hyperedges k. Figure 6
shows the emprical result.
6 Discussion
Figure 6 shows that we need MWU to broaden
the coverage of the extracted translation rules.
The rule coverage increases as the number of
words in an MWU increases, and almost con-
verges at m = 6. Our proposed method re-
cover around 75% of the sentences in the cor-
pus when we properly restrict m and k. This is
a great improvement over Na et al (2010), who
reported around 60% of the rule coverage with-
out the limitaion of the size of MWUs. They
only considered the best decomposition of the
dependency tree, while our proposed method
collects many alternative MWUs into an MWU-
forest. When we considered the best decom-
position (k = 1), the rule coverage dropped to
48
Figure 7: The frequency of the recovery according
to the length of the sentences in 1,000 sentences
around 65%. This can be viewed as an indirect
comparison between Na et al (2010) and our
proposed method in this corpus.
Figure 7 shows that the frequency of suc-
cess and failure in the recovery depends on the
length of the sentences. As the length of sen-
tences increase, the successful recovery occurs
less frequently. We investigated the reason of
failure in the longer sentences. As a result, the
two main sources of the failure are the word
alignment error and the dependency parsing er-
ror.
Our proposed method does not include all
translation rules in PBSMT because of the syn-
tactic constraint. Generally speaking, our pro-
posed method cannot deal with MWUs that do
not satisfy the well-formed constraint. How-
ever, ill-formed MWUs seems to be useful as
well. For example, our proposed method dose
not allow ill-formed vertices in an MWU-forest
as shown in Figure 8. This would be problem-
atic when we use an erroneuos parsing result.
Because dealing with parsing error has been
studied in literature, our proposed method has
the potential to improve thought future work.
Figure 8: An illustration of ill-formed MWUs
7 Conclusion
We have presented a way of representing sen-
tence structure using MWUs on DG. Because of
the absence of the intermdiate representation in
DG, we built a simpler structure of MWUs than
words using words as vertices of a dependency
structure. Unlike previous studies, we collected
many alternative structures using MWUs in a
packed forest, which is novel. We also ex-
tracted MWU-forest-to-string translation rules,
and verified the rule coverage empirically. As a
consequence, we improvemed the rule coverage
compared with a previous work, while retaining
the linear asymptotic complexity. We will ex-
pand our propose method to develop a syntax-
based SMT system in the future, and incoporate
the parsing error by considering multiple syn-
tactic analyses.
Acknowledgments
We appreciate the three anonymous reviewers.
This work was supported in part by the Korea
Science and Engineering Foundation (KOSEF)
grant funded by the Korean government (MEST
No. 2011-0003029), and in part by the BK 21
Project in 2011.
49
References
David Chiang. 2010. Learning to translate with
source and target syntax. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1443?1452, Upp-
sala, Sweden, July. Association for Computa-
tional Linguistics.
Yuan Ding and Martha Palmer. 2005. Machine
translation using probabilistic synchronous de-
pendency insertion grammars. In ACL ?05: Pro-
ceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 541?
548, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Yuan Ding. 2006. Machine Translation Using
Probabilistic Synchronous Dependency Insertion
Grammars. Ph.D. thesis, August.
Bonnie J. Dorr. 1994. Machine translation diver-
gences: a formal description and proposed solu-
tion. Comput. Linguist., 20:597?633, December.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice trans-
lation. In Proceedings of ACL-08: HLT, pages
1012?1020, Columbus, Ohio, June. Association
for Computational Linguistics.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In ACL ?03:
Proceedings of the 41st Annual Meeting on As-
sociation for Computational Linguistics, pages
205?208, Morristown, NJ, USA. Association for
Computational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a transla-
tion rule? In Daniel Marcu Susan Dumais
and Salim Roukos, editors, HLT-NAACL 2004:
Main Proceedings, pages 273?280, Boston, Mas-
sachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight,
Daniel Marcu, Steve DeNeefe, Wei Wang, and
Ignacio Thayer. 2006. Scalable inference and
training of context-rich syntactic translation mod-
els. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 961?968, Sydney, Aus-
tralia, July. Association for Computational Lin-
guistics.
Haitao Mi and Liang Huang. 2008. Forest-based
translation rule extraction. In Proceedings of the
2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 206?214, Hon-
olulu, Hawaii, October. Association for Compu-
tational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008.
Forest-based translation. In Proceedings of ACL-
08: HLT, pages 192?199, Columbus, Ohio, June.
Association for Computational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2010. Ma-
chine translation with lattices and forests. In
Coling 2010: Posters, pages 837?845, Beijing,
China, August. Coling 2010 Organizing Commit-
tee.
Hwidong Na, Jin-Ji Li, Yeha Lee, and Jong-Hyeok
Lee. 2010. A synchronous context free grammar
using dependency sequence for syntax-base sta-
tistical machine translation. In The Ninth Confer-
ence of the Association for Machine Translation
in the Americas (AMTA 2010), Denver, Colorado,
October.
Michael Paul. 2009. Overview of the iwslt 2009
evaluation campaign.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation
algorithm with a target dependency language
model. In Proceedings of ACL-08: HLT, pages
577?585, Columbus, Ohio, June. Association for
Computational Linguistics.
Anders S?gaard and Jonas Kuhn. 2009. Empirical
lower bounds on aligment error rates in syntax-
based machine translation. In Proceedings of the
Third Workshop on Syntax and Structure in Statis-
tical Translation (SSST-3) at NAACL HLT 2009,
pages 19?27, Boulder, Colorado, June. Associa-
tion for Computational Linguistics.
Zhaopeng Tu, Yang Liu, Young-Sook Hwang, Qun
Liu, and Shouxun Lin. 2010. Dependency for-
est for statistical machine translation. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pages
50
1092?1100, Beijing, China, August. Coling 2010
Organizing Committee.
Yuanbin Wu, Qi Zhang, Xuangjing Huang, and Lide
Wu. 2009. Phrase dependency parsing for opin-
ion mining. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 1533?1541, Singapore,
August. Association for Computational Linguis-
tics.
51
