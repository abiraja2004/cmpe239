Proceedings of the 6th Workshop on Statistical Machine Translation, pages 78?84,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
TESLA at WMT 2011: Translation Evaluation and Tunable Metric
Daniel Dahlmeier1 and Chang Liu2 and Hwee Tou Ng1,2
1NUS Graduate School for Integrative Sciences and Engineering
2Department of Computer Science, National University of Singapore
{danielhe,liuchan1,nght}@comp.nus.edu.sg
Abstract
This paper describes the submission from the
National University of Singapore to the WMT
2011 Shared Evaluation Task and the Tunable
Metric Task. Our entry is TESLA in three dif-
ferent configurations: TESLA-M, TESLA-F,
and the new TESLA-B.
1 Introduction
TESLA (Translation Evaluation of Sentences with
Linear-programming-based Analysis) was first pro-
posed in Liu et al (2010). The simplest variant,
TESLA-M (M stands for minimal), is based on N-
gram matching, and utilizes light-weight linguis-
tic analysis including lemmatization, part-of-speech
tagging, and WordNet synonym relations. TESLA-
B (B stands for basic) additionally takes advan-
tage of bilingual phrase tables to model phrase syn-
onyms. It is a new configuration proposed in this pa-
per. The most sophisticated configuration TESLA-F
(F stands for full) additionally uses language mod-
els and a ranking support vector machine instead of
simple averaging. TESLA-F was called TESLA in
Liu et al (2010). In this paper, we rationalize the
naming convention by using TESLA to refer to the
whole family of metrics.
The rest of this paper is organized as follows. Sec-
tions 2 to 4 describe the TESLA variants TESLA-M,
TESLA-B, and TESLA-F, respectively. Section 5
describes MT tuning with TESLA. Section 6 shows
experimental results for the evaluation and the tun-
able metric task. The last section concludes the pa-
per.
2 TESLA-M
The version of TESLA-M used in WMT 2011 is ex-
actly the same as in Liu et al (2010). The descrip-
tion is reproduced here for completeness.
We consider the task of evaluating machine trans-
lation systems in the direction of translating a source
language to a target language. We are given a refer-
ence translation produced by a professional human
translator and a machine-produced system transla-
tion. At the highest level, TESLA-M is the arith-
metic average of F-measures between bags of N-
grams (BNGs). A BNG is a multiset of weighted
N-grams. Mathematically, a BNG B consists of tu-
ples (bi, bWi ), where each bi is an N-gram and b
W
i is
a positive real number representing the weight of bi.
In the simplest case, a BNG contains every N-gram
in a translated sentence, and the weights are just the
counts of the respective N-grams. However, to em-
phasize the content words over the function words,
we discount the weight of an N-gram by a factor of
0.1 for every function word in the N-gram. We de-
cide whether a word is a function word based on its
POS tag.
In TESLA-M, the BNGs are extracted in the target
language, so we call them bags of target language
N-grams (BTNGs).
2.1 Similarity functions
To match two BNGs, we first need a similarity mea-
sure between N-grams. In this section, we define
the similarity measures used in our experiments. We
adopt the similarity measure from MaxSim (Chan
and Ng, 2008; Chan and Ng, 2009) as sms. For uni-
grams x and y,
78
? If lemma(x) = lemma(y), then sms = 1.
? Otherwise, let
a = I(synsets(x) overlap with synsets(y))
b = I(POS(x) = POS(y))
where I(?) is the indicator function, then sms =
(a + b)/2.
The synsets are obtained by querying WordNet
(Fellbaum, 1998). For languages other than English,
a synonym dictionary is used instead.
We define two other similarity functions between
unigrams:
slem(x, y) = I(lemma(x) = lemma(y))
spos(x, y) = I(POS(x) = POS(y))
All the three unigram similarity functions generalize
to N-grams in the same way. For two N-grams x =
x1,2,...,n and y = y1,2,...,n,
s(x, y) =
{
0 if ?i, s(xi, yi) = 0
1
n
?n
i=1 s(x
i, yi) otherwise
2.2 Matching two BNGs
Now we describe the procedure of matching two
BNGs. We take as input BNGs X and Y and a sim-
ilarity measure s. The i-th entry in X is xi and has
weight xWi (analogously for yj and y
W
j ).
Intuitively, we wish to align the entries of the two
BNGs in a way that maximizes the overall similar-
ity. An example matching problem for bigrams is
shown in Figure 1a, where the weight of each node
is shown, along with the hypothetical similarity for
each edge. Edges with a similarity of zero are not
shown. Note that for each function word, we dis-
count the weight by a factor of ten. The solution to
the matching problem is shown in Figure 1b, and the
overall similarity is 0.5 ? 0.01 + 0.8 ? 0.1 + 0.8 ?
0.1 = 0.165.
Mathematically, we formulate this as a (real-
valued) linear programming problem1. The vari-
ables are the allocated weights for the edges
w(xi, yj) ?i, j
1While integer linear programming is NP-complete, real-
valued linear programming can be solved efficiently.
w=1.0 w=0.1 w=0.1 w=0.1
w=0.01 w=0.1 w=0.1
s=0.1 s=0.8s=0.5 s=0.8
Good morning morning , , sir sir .
Hello , , Querrien Querrien .
s=0.4
(a) The matching problem
w=1.0 w=0.1 w=0.1 w=0.1
w=0.01 w=0.1 w=0.1
w=0.1w=0.01 w=0.1
s885Go8d m r o8d m r Gn nGimd imdG.
g,HH8Gn nGel,ddm, el,ddm, G.
(b) The solution
Figure 1: A BNG matching problem
We maximize
?
i,j
s(xi, yj)w(xi, yj)
subject to
w(xi, yj) ? 0 ?i, j
?
j
w(xi, yj) ? x
W
i ?i
?
i
w(xi, yj) ? y
W
j ?j
The value of the objective function is the overall
similarity S. Assuming X is the reference and Y
is the system translation, we have
Precision =
S
?
j y
W
j
Recall =
S
?
i x
W
i
The F-measure is derived from the precision and the
recall:
F =
Precision ? Recall
? ? Precision + (1 ? ?) ? Recall
In this work, we set ? = 0.8, following MaxSim.
The value gives more importance to the recall than
the precision.
79
If the similarity function is binary-valued and
transitive, such as slem and spos, then we
can use a much simpler and faster greedy
matching procedure: the best match is simply
?
g min(
?
xi=g
xWi ,
?
yi=g
yWi ).
2.3 Scoring
The TESLA-M sentence-level score for a reference
and a system translation is the arithmetic average of
the BTNG F-measures for unigrams, bigrams, and
trigrams based on similarity functions sms and spos.
We thus have 3 ? 2 = 6 BTNG F-measures for
TESLA-M.
We can compute a system-level score for a ma-
chine translation system by averaging its sentence-
level scores over the complete test set.
3 TESLA-B
TESLA-B uses the average of two types of F-
measures: (1) BTNG F-measures as in TESLA-M
and (2) F-measures between bags of N-grams in one
or more pivot languages, called bags of pivot lan-
guage N-grams (BPNGs), The rest of this section fo-
cuses on the generation of the BPNGs. Their match-
ing is done in the same way as described for BTNGs
in the previous section.
3.1 Phrase level semantic representation
Given a sentence-aligned bitext between the target
language and a pivot language, we can align the
text at the word level using well known tools such
as GIZA++ (Och and Ney, 2003) or the Berkeley
aligner (Liang et al, 2006; Haghighi et al, 2009).
We observe that the distribution of aligned
phrases in a pivot language can serve as a seman-
tic representation of a target language phrase. That
is, if two target language phrases are often aligned
to the same pivot language phrase, then they can be
inferred to be similar in meaning. Similar observa-
tions have been made by previous researchers (Ban-
nard and Callison-Burch, 2005; Callison-Burch et
al., 2006; Snover et al, 2009).
We note here two differences from WordNet syn-
onyms: (1) the relationship is not restricted to the
word level only, and (2) the relationship is not bi-
nary. The degree of similarity can be measured by
the percentage of overlap between the semantic rep-
resentations.
3.2 Segmenting a sentence into phrases
To extend the concept of this semantic representa-
tion of phrases to sentences, we segment a sentence
in the target language into phrases. Given a phrase
table, we can approximate the probability of a phrase
p by:
Pr(p) =
N(p)
?
p? N(p
?)
(1)
where N(?) is the count of a phrase in the phrase
table. We then define the likelihood of seg-
menting a sentence S into a sequence of phrases
(p1, p2, . . . , pn) by:
Pr(p1, p2, . . . , pn|S) =
1
Z(S)
n?
i=1
Pr(pi) (2)
where Z(S) is a normalizing constant. The segmen-
tation of S that maximizes the probability can be de-
termined efficiently using a dynamic programming
algorithm. The formula has a strong preference for
longer phrases, as every Pr(p) is a small fraction.
To deal with out-of-vocabulary (OOV) words, we
allow any single word w to be considered a phrase,
and if N(w) = 0, we set N(w) = 0.5 instead.
3.3 BPNGs as sentence level semantic
representation
Simply merging the phrase-level semantic represen-
tation is insufficient to produce a sensible sentence-
level semantic representation. As an example, we
consider two target language (English) sentences
segmented as follows:
1. ||| Hello , ||| Querrien ||| . |||
2. ||| Good morning , sir . |||
A naive comparison of the bags of aligned pivot lan-
guage (French) phrases would likely conclude that
the two sentences are completely unrelated, as the
bags of aligned phrases are likely to be completely
disjoint. We tackle this problem by constructing
a confusion network representation of the aligned
phrases, as shown in Figures 2 and 3. A confusion
network is a compact representation of a potentially
exponentially large number of weighted and likely
malformed French sentences. We can collect the N-
gram statistics of this ensemble of French sentences
80
w=1.=0s858G8od 
mrn0i858G8odg
,0HsseH18G8gdo d8G8gdo
Figure 2: A confusion network as a semantic repre-
sentation
w=1.=0s858G=1od 0s8m8r8nmi
Figure 3: A degenerate confusion network as a se-
mantic representation
efficiently from the confusion network representa-
tion. For example, the trigram Bonjour , Querrien 2
would receive a weight of 0.9 ? 1.0 = 0.9 in Fig-
ure 2. As with BTNGs, we discount the weight of an
N-gram by a factor of 0.1 for every function word in
the N-gram, so as to place more emphasis on the
content words.
The collection of all such N-grams and their cor-
responding weights forms the BPNG of a sentence.
The reference and system BPNGs are then matched
using the algorithm outlined in Section 2.2.
3.4 Scoring
The TESLA-B sentence-level score is a linear com-
bination of (1) BTNG F-measures for unigrams,
bigrams, and trigrams based on similarity func-
tions sms and spos, and (2) BPNG F-measures for
unigrams, bigrams, and trigrams based on sim-
ilarity functions slem and spos. We thus have
3 ? 2 F-measures from the BTNGs and 3 ? 2 ?
#pivot languages F-measures from the BPNGs. We
average the BTNG and BPNG scores to obtain
sBTNG and sBPNG, respectively. The sentence-
level TESLA-B score is then defined as 12(sBTNG +
sBPNG). The two-step averaging process prevents
the BPNG scores from overwhelming the BTNG
scores, especially when we have many pivot lan-
guages. The system-level TESLA-B score is the
arithmetic average of the sentence-level TESLA-B
scores.
2Note that an N-gram can span more than one segment.
4 TESLA-F
Unlike the simple arithmetic averages used in
TESLA-M and TESLA-B, TESLA-F uses a gen-
eral linear combination of three types of scores: (1)
BTNG F-measures as in TESLA-M and TESLA-B,
(2) BPNG F-measures as in TESLA-B, and (3) nor-
malized language model scores of the system trans-
lation, defined as 1n logP , where n is the length of
the translation, and P the language model probabil-
ity. The method of training the linear model depends
on the development data. In the case of WMT, the
development data is in the form of manual rankings,
so we train SVM rank (Joachims, 2006) on these in-
stances to build the linear model. In other scenarios,
some form of regression can be more appropriate.
The BTNG and BPNG scores are the same as
used in TESLA-B. In the WMT campaigns, we use
two language models, one generated from the Eu-
roparl dataset and one from the news-train dataset.
We thus have 3 ? 2 features from the BTNGs,
3 ? 2 ? #pivot languages features from the BPNGs,
and 2 features from the language models. Again, we
can compute system-level scores by averaging the
sentence-level scores.
4.1 Scaling of TESLA-F Scores
While machine translation evaluation is concerned
only with the relative order of the different trans-
lations but not with the absolute scores, there are
practical advantages in normalizing the evaluation
scores to a range between 0 and 1. For TESLA-M
and TESLA-B, this is already the case, since every
F-measure has a range of [0, 1] and so do their av-
erages. In contrast, the SVM rank -produced model
typically gives scores very close to zero.
To remedy that, we note that we have the free-
dom to scale and shift the linear SVM model with-
out changing the metric. We observe that the F-
measures have a range of [0, 1], and studying the
data reveals that [?15, 0] is a good approximation of
the range for normalized language model scores, for
all languages involved in the WMT campaign. Since
we know the range of values of an F-measure feature
(between 0 and 1) and assuming that the range of
the normalized LM score is between ?15 and 0, we
can find the maximum and minimum possible score
given the weights. Then we linearly scale the range
81
of scores from [min, max] to [0, 1]. We provide an
option of scaling TESLA-F scores in the new release
of TESLA.
5 MT tuning with TESLA
All variants of TESLA can be used for automatic
MT tuning using Z-MERT (Zaidan, 2009). Z-
MERT?s modular design makes it easy to integrate a
new metric. As TESLA already computes scores at
the sentence level, integrating TESLA into Z-MERT
was straightforward. First, we created a ?streaming?
version of each TESLA metric which reads trans-
lation candidates from standard input and prints the
sentence-level scores to standard output. This allows
Z-MERT to easily query the metric for sentence-
level scores during MT tuning. Second, we wrote
a Java wrapper that calls the TESLA code from Z-
MERT. The resulting metric can be used for MERT
tuning in the standard fashion. All that a user has
to do is to change the metric in the Z-MERT config-
uration file to TESLA. All the necessary code for
Z-MERT tuning is included in the new release of
TESLA.
6 Experiments
6.1 Evaluation Task
We evaluate TESLA using the publicly available
data from WMT 2009 for into-English and out-
of-English translation. The pivot language phrase
tables and language models are built using the
WMT 2009 training data. The SVM rank model for
TESLA-F is trained on manual rankings from WMT
2008. The results for TESLA-M and TESLA-F have
previously been reported in Liu et al (2010)3. We
add results for the new variant TESLA-B here.
Tables 1 and 2 show the sentence-level consis-
tency and system-level Spearman?s rank correlation,
respectively for into-English translation. For com-
parison, we include results for some of the best per-
forming metrics in WMT 2009. Tables 3 and 4 show
the same results for out-of-English translation. We
do not include the English-Czech language pair in
our experiments, as we unfortunately do not have
good linguistic resources for the Czech language.
3The English-Spanish system correlation differs from our
previous result after fixing a minor mistake in the language
model.
cz-en fr-en de-en es-en hu-en Overall
TESLA-M 0.60 0.61 0.61 0.59 0.63 0.61
TESLA-B 0.63 0.64 0.63 0.62 0.63 0.63
TESLA-F 0.63 0.65 0.64 0.62 0.66 0.63
ulc 0.63 0.64 0.64 0.61 0.60 0.63
maxsim 0.60 0.63 0.63 0.61 0.62 0.62
meteor-0.6 0.47 0.51 0.52 0.49 0.48 0.50
Table 1: Into-English sentence-level consistency on
WMT 2009 data
cz-en fr-en de-en es-en hu-en Avg
TESLA-M 1.00 0.86 0.85 0.99 0.66 0.87
TESLA-B 1.00 0.92 0.67 0.95 0.83 0.87
TESLA-F 1.00 0.92 0.68 0.94 0.94 0.90
ulc 1.00 0.92 0.78 0.86 0.60 0.83
maxsim 0.70 0.91 0.76 0.98 0.66 0.80
meteor-0.6 0.70 0.93 0.56 0.87 0.54 0.72
Table 2: Into-English system-level Spearman?s rank
correlation on WMT 2009 data
The new TESLA-B metric proves to be competi-
tive to its siblings and is often on par with the more
sophisticated TESLA-F metric. The exception is
the English-German language pair, where TESLA-
B has very low system-level correlation. We have
two possible explanations for this. First, the system-
level correlation is computed on a very small sample
size (the ranked list of MT systems). This makes the
system-level correlation score more volatile com-
pared to the sentence-level consistency score which
is computed on thousands of sentence pairs. Sec-
ond, German has a relatively free word order which
potentially makes word alignment and phrase table
extraction more noisy. Interestingly, all participating
metrics in WMT 2009 had low system-level correla-
tion for the English-German language pair.
en-fr en-de en-es Overall
TESLA-M 0.64 0.59 0.59 0.60
TESLA-B 0.65 0.59 0.60 0.61
TESLA-F 0.68 0.57 0.60 0.61
wpF 0.66 0.60 0.61 0.61
wpbleu 0.60 0.47 0.49 0.51
Table 3: Out-of-English sentence-level consistency
on WMT 2009 data
82
en-fr en-de en-es Avg
TESLA-M 0.93 0.86 0.79 0.86
TESLA-B 0.91 0.05 0.63 0.53
TESLA-F 0.85 0.78 0.67 0.77
wpF 0.90 -0.06 0.58 0.47
wpbleu 0.92 0.07 0.63 0.54
Table 4: Out-of-English system-level Spearman?s
rank correlation on WMT 2009 data
6.2 Tunable Metric Task
The goal of the new tunable metric task is to explore
MT tuning with metrics other than BLEU (Papineni
et al, 2002). To allow for a fair comparison, the
WMT organizers provided participants with a com-
plete Joshua MT system for an Urdu-English trans-
lation task. We tuned models for each variant of
TESLA, using Z-MERT in the default configuration
provided by the organizers. There are four reference
translations for each Urdu source sentence. The size
of the N-best list is set to 300.
For our own experiments, we randomly split the
development set into a development portion (781
sentences) and a held-out test portion (200 sen-
tences). We run the same Z-MERT tuning process
for each TESLA variant on this reduced develop-
ment set and evaluate the resulting models on the
held out test set. We include a model trained with
BLEU as an additional reference point. The results
are shown in Table 5. We observe that the model
trained with TESLA-F achieves the best results
when evaluated with any of the TESLA metrics, al-
though the differences between the scores are small.
We found that TESLA produces slightly longer
translations than BLEU: 22.4 words (TESLA-M),
21.7 words (TESLA-B), and 22.5 words (TESLA-
F), versus 18.7 words (BLEU). The average refer-
ence length is 19.8 words.
The official evaluation for the tunable metric task
is performed using manual rankings. The score of
a system is calculated as the percentage of times
the system is judged to be either better or equal
(score1) or strictly better (score2) compared to each
other system in pairwise comparisons. Although
we submit results for all TESLA variants, only our
primary submission TESLA-F is included in the
manual evaluation. The results for TESLA-F are
mixed. When evaluated with score1, TESLA-F is
Tune\Test BLEU TESLA-M TESLA-B TESLA-F
BLEU 0.2715 0.3756 0.3129 0.3920
TESLA-M 0.2279 0.4056 0.3279 0.3981
TESLA-B 0.2370 0.4001 0.3257 0.3977
TESLA-F 0.2432 0.4076 0.3299 0.4007
Table 5: Automatic evaluation scores on held out
test portion for the tunable metric task. The best re-
sult in each column is printed in bold.
ranked 7th out of 8 participating systems, but when
evaluated with score2, TESLA-F is ranked second
best. These findings differ from previous results
that we reported in Liu et al (2011) where MT
systems tuned with TESLA-M and TESLA-F con-
sistently outperform two other systems tuned with
BLEU and TER for translations from French, Ger-
man, and Spanish into English on the WMT 2010
news data set. A manual inspection of the references
in the tunable metric task shows that the translations
are of lower quality compared to the news data sets
used in WMT. As the SVM model in TESLA-F is
trained with rankings from WMT 2008, it is possible
that the model is less robust when applied to Urdu-
English translations. This could explain the mixed
performance of TESLA-F in the tunable metric task.
7 Conclusion
We introduce TESLA-B, a new variant of the
TESLA machine translation metric and present ex-
perimental results for all TESLA variants in the set-
ting of the WMT evaluation task and tunable met-
ric task. All TESLA variants are integrated into Z-
MERT for automatic machine translation tuning.
Acknowledgments
This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) adminis-
tered by the Media Development Authority (MDA)
of Singapore.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics.
83
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics.
Yee Seng Chan and Hwee Tou Ng. 2008. MaxSim:
A maximum similarity metric for machine translation
evaluation. In Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2009. MaxSim: Per-
formance and effects of translation fluency. Machine
Translation, 23(2?3):157?168.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In Proceedings of 47th Annual Meeting
of the Association for Computational Linguistics and
the 4th IJCNLP of the AFNLP.
Thorsten Joachims. 2006. Training linear SVMs in lin-
ear time. In Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. TESLA: Translation evaluation of sentences
with linear-programming-based analysis. In Proceed-
ings of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011.
Better evaluation metrics lead to better machine trans-
lation. In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with a
tunable MT metric. In Proceedings of of the Fourth
Workshop on Statistical Machine Translation.
Omar Zaidan. 2009. Z-MERT: A fully configurable open
source tool for minimum error rate training of machine
translation systems. The Prague Bulletin of Mathe-
matical Linguistics, 91:79?88.
84
