~YNTACTIC PROCESSING AND 
~UNCTIONAL SENTENCE PERSPECTIVE 
Martin Kay 
XEROX PARC 
This paper contains some ideas that 
nave occurred to me in the course of some 
prel iminary work on the notion of reversible 
grammar. In order to make it possible to 
generate and analyze sentences with the same 
grammar, represented in the same way, I was 
led to consider restr ict ions on the 
expressive power of the formal ism that would 
be acceptible only if the structures of 
sentences contained more information than 
American l inguisits have usual ly been 
prepared to admit. I hope to convey to you 
some of my surprise and delight in f inding 
that certain l inguists of the Prague school 
argue for the representat ion of this same 
information on altogether different grounds. 
I. REVERSIBLE GRAMMARS 
For me, a grammar is not so much a set 
of wel l - formedness condit ions on strings of 
words as a relation between strings of words 
(sentences) and structures. Usual ly it is 
construct ive in the sense that there is a 
device that interprets it to convert either 
structures into sentences (a generator) or 
sentences into structures (a parser). A 
grammar for which both a generator and a 
parser can be found is what I call 
reversible and, of course, what I am 
interested in is not so much part icular 
reversible grammars as a formal ism which 
guarantees reversibi l i ty for any grammar 
that adheres to it. Context- free grammars 
are clearly reversible in this sense and 
transformat ional  grammars are clearly not. 
Augmented Transit ion Networks (ATNs) are 
reversible provided some quite reasonable 
restr ict ions are placed on the operations 
that can be performed on registers. This is 
not to say that it is a tr ivial  matter to 
obtain a generator from an arbitrary ATN 
grammar. 
It goes w i thout  saying that the 
composit ion of a generator and a parser 
interpret ing the same reversible grammar on 
the same sentence or structure does not, in 
general, perform the identity 
transformation. Frequently, sentences are 
ambiguous and structures often correspond to 
sets of more than one 
paraphrase. Parsing fol lowed by generat ion 
of each member of the result ing set of 
structures will therefore yield the 
grammatical  paraphrases of the original  
under all grammatical  interpretat ions.  
The practical advantages of a 
reversible grammar are obvious. A computer 
program that engages in any kind of 
conversat ion must both parse and generate 
and it would be economical  to base both 
processes on the same grammar. But, to me, 
the theoret ical  appeal is stronger. It is 
plausible that we have something in our 
heads that fills the function I am ascr ibing 
to grammar, though I am not insensit ive to 
the claims of those who deny this. But it 
is a l together implausible that we have two 
such things, one for parsing and one for 
generation, essent ial ly unrelated to one 
another. 
A left -to-r ight generator is one that 
deposits words into the output str ing one 
after another, in left -to-r ight sequence. A 
left - to-r ight parser is one that examines 
the words of the input string one after 
another, from left to right. A 
left - to-r ight reversible grammar is one for 
which there is a left -to-r ight generator and 
a left - to-r ight parser. Once again, it is 
clear that context- free grammars, in the 
usual notation, meet the requirements.  ATN 
grammars probably do not. They certainly do 
not if we exclude the possib i l i ty  of 
ent ire ly reworking the grammar and 
present ing it in an ent irely new form to the 
generator. The kind of grammar I have in 
mind would require no such  reworking. 
Intuit ively, the notion is a simple one. It 
is very like an ATN in that it analyses 
sentences by moving through a network, 
examining the input string at each 
t rans i t ion  and, if the current symbol meets 
the condit ions specif ied for the transit ion, 
ass igning it to a register. The generator  
would also move through the network, making 
exactly the same transit ions, but deposit ing 
the contents of registers into the str ing at 
each step. 
2. THE PROCESSOR 
Generators and parsers for the kind of 
revers ible grammar I have in mind could be 
implemented in a great variety of ways. One 
of the simplest I know would be to use a 
version of the General Syntact ic Processor 
(GSP). GSP contains: 
(I) a grammar in the form of a 
transi t ion network, that is, a 
airected graph in which the 
wermissible transit ions between 
states are represented by arcs, each 
labeled with a more or less 
compl icated set of act ions which 
eetermine the appl icabi l i ty  of the 
arc and cause side effects, such as 
the assignment of values to 
re~isters, 
(2) an agenda of tasks to be carr ied 
out, 
(3) a chart, that is, a directed graph 
consist ing of ?ert iees and edges 
which represents the sentence being 
analyzed or generated together with 
1~s component parts--phrases,  
In~ermediate derivations,  or 
whatever--which,  together with the 
agenda, completely encapsulates the 
state of the entire processor at any 
glven point in its operation, 
(4) a set of schedul in~ rules whose job 
is to determine the order in which 
the tasks on  the agenda wil l  be 
carr ied out, ana 
(5) the interpreter ?~self. 
Edges in the chart are either complete 
or incomplete. Complete edges represent 
12. 
completely specif ied words or phrases and, 
if there is a path through the chart from 
one edge to another, it is because the f i r s t  
precedes the second in temporal, or 
left-to-r ight,  sequence. If there is no 
path from one to another, then they belong 
to alternat ive hypotheses about the 
structure of of the sentence. So, the 
sentence "they are f lying planes" has, let 
us say, two analyses, each consist ing of a 
noun phrase fol lowed by a verb phrase. But 
there is no path between the phrases in one 
analysis and those in the other. The verb 
phrase in one analysis consists of the verb 
"are" fol lowed by the noun phrase "flying 
planes", which are therefore adjacent on the 
same path, but there is no path from either 
of them to the verb phrase they make up 
because this is an alternative analysis of 
the same set of words. 
An incomplete edge represents part of a 
phrase together with an indicat ion of  what 
would have to be added to complete it. For 
example, an incomplete noun phrase might 
include the string "the big black" plus an 
indicat ion that a fo l lowing noun, possibly 
preceded by some more adjectives, would 
complete it. A special kind of incomplete 
edge is an empty edge. Empty edges are 
successors of themselves. In other words, 
they are always incident from and to the 
same vertex, ref lecting the fact that they 
represent no part of the sentence, but 
merely the potential for f inding some 
structural  component. The specif icat ion of 
how an incomplete edge can be completed 
takes the form of one or more arcs in the 
grammar, each paired with a direct ion-- left  
or right. If the direction is right, then 
complet ion can be achieved by fo l lowing 
sequences of arcs incident fro~ the given 
state; if it is left, then arcs incident to 
the state must be followed. 
The interpreter uses the schedul ing 
rules to chose an item on the agenda which 
it then carries out. If all the tasks that 
were ever put on the agenda in the course of 
generation or analysis were carr ied out in 
an arbitrary order, then all results that 
the grammar al lowed would be obtained sooner 
or later. The scheduling rules formalize 
strategie~ of one kind and another. They 
are presumably designed so as to shorten the 
time required to reach a result which is, in 
some sense, acceptable, at which time the 
remaining entries on the agenda can simply 
be abandoned. 
The typical  task is an attempt to apply 
an arc from the grammar to an edge in the 
chart. If the arc applies successfully, 
~ome new material  will be added to the 
chart. In generation, the new material  
typical ly consists of one or two new edges 
const i tut ing a sequence with the same end 
points as those of the initial edge. 
usually, no more than one of the newly 
introduced edges will be incomplete. Thus, 
there might be a task in which an arc was 
appl ied to the noun-phrase edge representing 
"the big black dog" and which resulted in 
the complete art icle "the" and the 
incomplete noun phrase "big black dog". In 
parsing, the task specif ies one or two 
i3. 
euges, one of which is incomplete. The idea 
is to  attempt to take the complete edge at 
least one step nearer to complet ion by 
incorporat ing the other edge. If only one 
edge is specif ied, then the new edge will 
have the same end points as the original, 
but wil l  presumably be di f ferent ly labeled. 
Within this version of GSP, 
top-to-bottom, left-to-r ight parsing, in the 
manner of an ATN parser, proceeds broadly as 
follows: 
I. Whenever, as the result of introducing 
a new edge into the chart, a new 
sequence consist ing of an incomplete 
eage fol lowed by a complete one comes 
into existance, put a new task on the 
agenda for each of the "category" arcs 
named on the incomplete edge. When 
one of these tasks is executed, the 
arc will be applied to the complete 
e e giving rise, if successful,  to a 
new edge~ complete or incomplete. 
2. Whenever a new incomplete edge is 
Introduced that names a "Pop" or 
"Jump" arc, create tasks that will 
cause these to be carried out. 
3. Place an empty sentence edge before 
the first word of the sentence. 
The process starts with step 3, which 
immediately causes a sequence of instances 
u~ steps I and 2. 
An incomplete edge represents a stack 
frame in the ATN processor. It is labeled 
with a set of registers and it spans a 
portion of the chart represent ing the part 
of the str ing so far analyzed. "Category" 
arcs are appl ied to an incomplete edge and a 
complete edge immediately to its right. If 
successful,  the result is a new incomplete 
edge. "Pop" arcs produce a complete edge, 
in exchange for an incomplete one. "Jump" 
arcs produce an incomplete edge in exchange 
for an incomplete edge, the dif ferences 
being in the arcs that specify how to 
proceed towards completion, and possibly in 
the labe l .  
It turns out that the mechanism of 
Fecursive calls that "Push" and "Pop" arcs 
provide for is embraced by the devices 
already described. Suppose that sentences 
are to be analyzed as consist ing of a noun 
phrase fol lowed by a verb phrase and that a 
noun phrase, say "the big black dog" has 
somehow been recognized at the beginning of 
the sentence. This means that there will be 
a complete edge representing this noun 
phrase and an incomplete sentence edge which 
has the same end points with an arc 
speci fy ing that a verb phrase with a 
singular, third-person verb, is to follow. 
Since the grammar contains a subnetwork 
giving the structure of verb phrases, an 
empty edge labeled with the category "verb 
phrase" is introduced fol lowing that 
incomplete sentence provided there is not 
one already there. In due course, this will 
presumably cause a complete verb phrase to 
appear. The general principle is this: 
whenever an incomplete edge specif ies a 
"category" arc for wnlch ~nere is a 
corresponding subnetwork, an empty edge is 
created fol lowing that one for each of the 
init ial arcs in the subnetwork in the hope 
that this will lead to the creation of a new 
complete edge that the "category" arc can be 
successful ly applied to. 
3. THE USE OF REGISTERS 
The principal problem with this simple 
plan, when applied to reversible grammars, 
is that the registers cannot be guaranteed 
to have the necessary contents at the time 
required. One of the strengths of the ATN 
formalism is that it al lows the parser to 
"change its mind". The canonical example is 
the passive construction. The first verb 
phrase in the sentence is assigned to the 
subject register. But when a passive 
verb--part of the verb "be" and a transit ive 
past part ic ip le--has been encountered, the 
contents of the subject register are simply 
transferred to the object register. If a 
"by" phrase follows, its object wil l  later 
go into the subject register. In this way, 
a great deal of backing up is avoided. 
In generat ing a passive sentence, it is 
clear that the first step cannot be to 
deposit the contents of the subject register 
in the first position. An alternat ive might 
be to decide which register to use in 
fi l l ing the first posit ion by examining a 
"voice" register, using the object instead 
of the subject register if its value is 
"passive". But this would require us to 
assign a value to the voice register in 
parsing before the relevant evidence is in. 
It would work only if the contents of the 
voice register were changed at the same time 
as the passive verb was recognized and the 
contents of the subject register were moved 
to the object register. It could indeed be 
made to work, but the solution is 
unsat isfactory because it does not reflect 
any general principle that carries over to 
other cases. More important, it violates a 
princple that must be regarded as 
fundamental for the achievement of 
revers ib i l i ty  in general, namely that each 
elementary operation that an arc in the 
grammar can specify must have two 
systematical ly  related interpretat ions,  for 
use in generation and parsing respectively. 
Another solution would be to assign the 
first noun phrase to a neutral register when 
it is encountered in parsing, and only to 
copy it into the subject or object registers 
when it was finally establ ished which one it 
belonged in. This neutral  register would 
have to be reflected direct ly in the 
structure assigned to the sentence because 
it would be from there that the first noun 
phrase in the sentence would have to be 
taken by the generator. One advantage of 
this scheme is that a passive marker ~ ,Id 
no longer be required in the structure of" 
passive sentences. Instead, the voice of a 
sentence would be determined by the 
generator on the asis o~ which 
register- -subject  or object- -had the same 
contents as the special neutral  register. 
The general principle behind this strategy 
is that ~he contents of a regi ster are never 
changed in the course ~f either generation 
14. 
or parsing. This is the solut ion I 
advocate. 
A name is needed for the neutral 
register, and topic, or theme, suggest 
themselves immediately. But consider the 
case of cleft sentences like "It was Brutus 
that ki l led Caesar" and "It was Caesar that 
Brutus ki l led" and assume, for the sake of 
the argument, that these are to be handled 
as main clauses and not by the 
re lat ive-c lause mechanism. Once again, the 
under ly ing grammatical  function of the first 
noun phrase is not known when the parser 
first encounters it. The problem can be 
solved by the same device, but of all the 
names one might choose for the neutral  
register, "topic" is least appropr iate in 
this instance. Something like focus or 
comment would be more to the point. What, 
then, of datives? Consider "He gave Fido a 
bone" and "He gave Fido to Mary". The 
problem here is the noun phrase fol lowing 
the verb. In neither case can we 
convinc ingly argue that it is either the 
topic or the focus of the sentence. 
4. FUNCTIONAL SENTENCE PERSPECTIVE 
The most sat isfying solut ion to these 
problems is to be found in the work of the 
Prague school of l inguists, part icular ly  
Mathesius, Firbas, Danes, and Sgall. The 
basic notion is that of the Funct ional  
Sentence Perspect ive according to which 
topic and focus are two regions in the scale 
of communicat ive dynamism along which each 
of the major const i tuents of a sentence are 
ordered. In the unmarked case, each 
succeding const ituent in the surface string 
has a higher degree of communicat ive 
dynamism. The point on the scale at which 
one passes from topic to focus may or may 
not be marked. In speech, special  stress 
can be used to mark any element as the 
focus; in wr i t ing ,  several devices like 
c left ing fill the same role. 
Communicat ive dynamism correlates with 
a number of other notions that are more 
famil iar in this part of the world. 
g lements that are low on this scale are the 
ones that are more contextual ly  bound, which 
is to say that they involve presupposi t ions 
about the preceding text. In "It was Brutus 
that ki l led Caesar", "that ki l led Caesar" is 
the topic and it c learly involves the 
presupposi t ion that someone ki l led Caesar. 
in an unmarked sentence, like "Brutus ki l led 
Caesar", it is not clear whether the 
dividing line between topic and comment 
~alls before or after the verb; there are 
nevertheless three degrees of communicat ive 
dynamism involved. 
According to this .iew, the di f ference 
between "He gave Fido to Mary" and "He gave 
Mary Fido" is not in what is topic and what 
is focus but simply in the posit ions that 
"Mary" and "Fido" occupy on the scale of 
communicat ive dynamism. Consider the 
sentences: 
(I) John did all the work, but they gave 
the reward to Bill. 
(2) John did all the work, but they gave 
Bill the reward. 
(3) They were so impressed with the work 
that they gave Bill a reward. 
(4) They were so impressed with the work 
that they gave a reward to Bill. 
I c laim that (2) and (4) are less natural 
than (I) and (3) when read with even 
intonation. Sentence (5), with underl in ing 
for stress, is, of course, quite natural, 
and (6) is questionable. 
(5) John did all the work, but they gave 
Bill the reward. 
(6) They were so impressed with the work 
that they gave a reward to Bill. 
The claim is simply that the last item 
carries the greatest communicat ive load, 
represents the most novel component of the 
sentence. 
This is consistent with the observat ion 
that dative movement is at best awkward when 
the direct object is a pronoun, as in 
(7) I gave him it. 
and it becomes more awkward when the 
indirect object is more ponderous, as in 
(8) I gave the man you said you had seen 
it. 
In fact, it is consistent with the 
observat ion that ponderous const ituents tend 
to be deferred, using such devices as 
extraposit lon.  It is in the nature of 
pronouns that they are contextual ly bound, 
and the complexity of large const ituents 
presumably comes directly from the fact that 
they tend to convey new information. 
What this suggests is a formalism in 
which the structure of a phrase is a list of 
attr ibutes named for grammatical  functions, 
whose values are words or other phrases. 
They are ordered so as to show there 
posit ions on the scale of communicat ive 
dynamism and there is provision for a marker 
to be introduced into the list expl ic it ly 
separat ing the topic from the focus. 
Consider ing only the sentence level, and 
s impl i fy ing greatly, this would give 
examples like the following, using " / "as  
the marker: 
\ [Subject: John Verb:gave Dir-obj :(the 
candy) Indir-obj:Mary\] => "John gave 
the candy to Mary" 
\[ Indir-obJ:Mary Verb:gave Dir-obj : ( the 
candy) Subject: John\] => "Mary was 
given the candy by John" 
\[Verb:gave Dir-obJ:(the candy) 
Indir-obJ:Mary / Subject: John\] => 
"It was John that gave Mary the 
candy" or "John gave Mary the candy" 
\[Subject: John Verb:gave Dir-obj :(the 
candy) / Indir-obj:Mary\] => "It was 
Mary that John gave the candy to" 
\[Subject: John Dir -obj : ( the candy) / 
Verb:gave Indir-obj :Mary\]  => "What 
John did with the candy was give it 
to Mary" 
The impl icat ions for reversible 
syntactic processing seem to be as follows: 
The famil iar set of registers, named for the 
most part for the names of grammatical  
functions, are supplemented by three others 
cal led topic, focus and, say, marker. 
Marker wil l  have a value only when the 
sentence is marked in the sense I have been 
us ing .  Topic and focus will contain ordered 
lists of elements. The structure of a 
passive sentence, for example, will be 
recognizable by the fact that it is unmarked 
and has a patient (dative, or whatever) as 
the first item on its topic list. The 
parser wil l  place the first noun phrase in a 
"standard" sentence on this list and only 
copy it into some other register later. The 
generator will ~ unload the first item into 
the str ing and decide later what form of 
verb to produce. 
The i l l - formedness of the ideas I have 
tried to present here is clear for all to 
see. I have so far acquired only the most 
tenuous grasp of what the Czech l inguists 
are doing and, while I should publ ic ly thank 
Petr Sgall for his patience in explaining it 
to me, it is only right that I should also 
appologise for the egregious errors I have 
doubtless been guilty of. But, whatever 
errors of detail  there may be, one important 
point will, I hope, remain. The notions of 
topic and focus are clearly well motivated 
in theoret ical  l inguistics, and the richer 
not ion of functional sentence perspective 
probably is also. I have been led to these 
same notions for purely technical  reasons 
ar is ing out of my desire to build a 
reversible syntactic processor. 
REFERENCES 
Firbas, J. "On Defining the Theme in 
Funct ional  Sentence Analysis", Travaux 
L inguist iques d_ee Pragu_ee, Vol. I, pp 
267-280, 1964. 
Kaplan, Ronald M. A General Syntactic 
Processor in Randall Rustin(ed.) "Natural 
Language Processing", New York, 
Algor i thmics Press, 1973. 
Mathesius, V. "Zur Satzperspekt ive in 
modernen English", Archiv fuer das 
Studium der neueren Sprachen und 
Literaturen, Vol. 155, pp. 202-210, 
1929. 
15. 
