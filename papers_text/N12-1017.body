2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 162?171,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
HyTER: Meaning-Equivalent Semantics for Translation Evaluation
Markus Dreyer
SDL Language Weaver
6060 Center Drive, Suite 150
Los Angeles, CA 90045, USA
mdreyer@sdl.com
Daniel Marcu
SDL Language Weaver
6060 Center Drive, Suite 150
Los Angeles, CA 90045, USA
dmarcu@sdl.com
Abstract
It is common knowledge that translation is
an ambiguous, 1-to-n mapping process, but
to date, our community has produced no em-
pirical estimates of this ambiguity. We have
developed an annotation tool that enables us
to create representations that compactly en-
code an exponential number of correct trans-
lations for a sentence. Our findings show that
naturally occurring sentences have billions of
translations. Having access to such large sets
of meaning-equivalent translations enables us
to develop a new metric, HyTER, for transla-
tion accuracy. We show that our metric pro-
vides better estimates of machine and human
translation accuracy than alternative evalua-
tion metrics.
1 Motivation
During the last decade, automatic evaluation met-
rics (Papineni et al., 2002; Snover et al., 2006; Lavie
and Denkowski, 2009) have helped researchers ac-
celerate the pace at which they improve machine
translation (MT) systems. And human-assisted met-
rics (Snover et al., 2006) have enabled and sup-
ported large-scale U.S. government sponsored pro-
grams, such as DARPA GALE (Olive et al., 2011).
However, these metrics have started to show signs of
wear and tear.
Automatic metrics are often criticized for provid-
ing non-intuitive scores ? few researchers can ex-
plain to casual users what a BLEU score of 27.9
means. And researchers have grown increasingly
concerned that automatic metrics have a strong bias
towards preferring statistical translation outputs; the
NIST (2008, 2010), MATR (Gao et al., 2010) and
WMT (Callison-Burch et al., 2011) evaluations held
during the last five years have provided ample ev-
idence that automatic metrics yield results that are
inconsistent with human evaluations when compar-
ing statistical, rule-based, and human outputs.
In contrast, human-informed metrics have other
deficiencies: they have large variance across human
judges (Bojar et al., 2011) and produce unstable re-
sults from one evaluation to another (Przybocki et
al., 2011). Because evaluation scores are not com-
puted automatically, systems developers cannot au-
tomatically tune to human-based metrics.
Table 1 summarizes the dimensions along which
evaluation metrics should do well and the strengths
and weaknesses of the automatic and human-
informed metrics proposed to date. Our goal is
to develop metrics that do well along all these di-
mensions. The fundamental insight on which our
research relies is that the failures of current auto-
matic metrics are not algorithmic: BLEU, Meteor,
TER (Translation Edit Rate), and other metrics ef-
ficiently and correctly compute informative distance
functions between a translation and one or more hu-
man references. We believe that these metrics fail
simply because they have access to sets of human
references that are too small. If we had access to
the set of all correct translations of a given sentence,
we could measure the minimum distance between a
translation and the set. When a translation is perfect,
it can be found in the set, so it requires no editing to
produce a perfect translation. Therefore, its score
should be zero. If the translation has errors, we can
162
Desiderata Auto. Manu. HyTER
Metric is intuitive N Y Y
Metric is computed automatically Y N Y
Metric is stable and reproducible from one evaluation to another Y N Y
Metric works equally well when comparing human and automatic outputs
and when comparing rule-based, statistical-based, and hybrid engines
N Y Y
System developers can tune to the metric Y N Y
Metric helps developers identify deficiencies of MT engines N N Y
Table 1: Desiderata of evaluation metrics: Current automatic and human metrics, proposed metric.
efficiently compute the minimum number of edits
(substitutions, deletions, insertions, moves) needed
to rewrite the translation into the ?closest? reference
in the set. Current automatic evaluation metrics do
not assign their best scores to most perfect transla-
tions because the set of references they use is too
small; their scores can therefore be perceived as less
intuitive.
Following these considerations, we developed an
annotation tool that enables one to efficiently create
an exponential number of correct translations for a
given sentence, and present a new evaluation met-
ric, HyTER, which efficiently exploits these mas-
sive reference networks. In the rest of the paper, we
first describe our annotation environment, process,
and meaning-equivalent representations that we cre-
ate (Section 2). We then present the HyTER met-
ric (Section 3). We show that this new metric pro-
vides better support than current metrics for machine
translation evaluation (Section 4) and human trans-
lation proficiency assessment (Section 5).
2 Annotating sentences with exponential
numbers of meaning equivalents
2.1 Annotation tool
We have developed a web-based annotation tool
that can be used to create a representation encoding
an exponential number of meaning equivalents
for a given sentence. The meaning equivalents
are constructed in a bottom-up fashion by typing
translation equivalents for larger and larger phrases.
For example, when building the meaning equiv-
alents for the Spanish phrase ?el primer ministro
italiano Silvio Berlusconi?, the annotator first types
in the meaning equivalents for ?primer ministro?
? ?prime-minister; PM; prime minister; head of
government; premier; etc.?; ?italiano? ? ?Italian?;
and ?Silvio Berlusconi? ? ?Silvio Berlusconi;
Berlusconi?. The tool creates a card that stores
all the alternative meanings for a phrase as a
determinized FSA and gives it a name in the target
language that is representative of the underly-
ing meaning-equivalent set: [PRIME-MINISTER],
[ITALIAN], and [SILVIO-BERLUSCONI]. Each base
card can be thought of expressing a semantic con-
cept. A combination of existing cards and additional
words can be subsequently used to create larger
meaning equivalents that cover increasingly larger
source sentence segments. For example, to create
the meaning equivalents for ?el primer ministro ital-
iano? one can drag-and-drop existing cards or type
in new words: ?the [ITALIAN] [PRIME-MINISTER];
the [PRIME-MINISTER] of Italy?; to create the
meaning equivalents for ?el primer ministro italiano
Silvio Berlusconi?, one can drag-and-drop and type:
?[SILVIO-BERLUSCONI] , [THE-ITALIAN-PRIME-
MINISTER]; [THE-ITALIAN-PRIME-MINISTER] ,
[SILVIO-BERLUSCONI]; [THE-ITALIAN-PRIME-
MINISTER] [SILVIO-BERLUSCONI] ?. All meaning
equivalents associated with a given card are ex-
panded and used when that card is re-used to create
larger meaning-equivalent sets.
The annotation tool supports, but does not en-
force, re-use of annotations created by other anno-
tators. The resulting meaning equivalents are stored
as recursive transition networks (RTNs), where each
card is a subnetwork; if needed, these non-cyclic
RTNs can be automatically expanded into finite-
state acceptors (FSAs, see Section 3).
2.2 Data and Annotation Protocols
Using the annotation tool, we have created meaning-
equivalent annotations for 102 Arabic and 102 Chi-
nese sentences ? a subset of the ?progress set? used
in the 2010 Open MT NIST evaluation (the average
163
sentence length was 24 words). For each sentence,
we had access to four human reference translations
produced by LDC and five MT system outputs,
which were selected by NIST to cover a variety of
system architectures (statistical, rule-based, hybrid)
and performances. For each MT output, we also had
access to sentence-level HTER scores (Snover et al.,
2006), which were produced by experienced LDC
annotators.
We have experimented with three annotation pro-
tocols:
? Ara-A2E and Chi-C2E: Foreign language natives
built English networks starting from foreign lan-
guage sentences.
? Eng-A2E and Eng-C2E: English natives built En-
glish networks starting from ?the best translation?
of a foreign language sentence, as identified by
NIST.
? Eng*-A2E and Eng*-C2E: English natives built
English networks starting from ?the best transla-
tion?, but had access to three additional, indepen-
dently produced human translations to boost their
creativity.
Each protocol was implemented independently by
at least three annotators. In general, annotators need
to be fluent in the target language, familiar with the
annotation tool we provide and careful not to gen-
erate incorrect paths, but they do not need to be lin-
guists.
2.3 Exploiting multiple annotations
For each sentence, we combine all networks that
were created by the different annotators. We eval-
uate two different combination methods, each of
which combines networks N1 and N2 of two anno-
tators (see an example in Figure 1):
(a) Standard union U(N1, N2): The standard
finite-state union operation combines N1 and N2
on the whole-network level. When traversing
U(N1, N2), one can follow a path that comes from
either N1 or N2.
(b) Source-phrase-level union SPU(N1, N2): As
an alternative, we introduce SPU, a more fine-
grained union which operates on sub-sentence seg-
ments. Here we exploit the fact that each annotator
explicitly aligned each of her various subnetworks
N1
the level of approval
was
close to
zero
the approval rate
practically
the approval level
was
close to
zero
the approval rate
about equal to
(a)
was
zero
the approval rate
the level of approval
the approval level
close to
practically
about equal to
(b)
N2
Figure 1: (a) Finite-state union versus (b) source-phrase-
level union (SPU). The former does not contain the path
?the approval level was practically zero?.
for a given sentence to a source span of that sen-
tence. Now for each pair of subnetworks (S1, S2)
from N1 and N2, we build their union if they are
compatible; two subnetworks S1, S2 are defined to
be compatible if they are aligned to the same source
span and have at least one path in common.
The purpose of SPU is to create new paths by mix-
ing paths from N1 and N2. In Figure 1, for example,
the path ?the approval level was practically zero? is
contained in the SPU, but not in the standard union.
We build SPUs using a dynamic programming al-
gorithm that builds subnetworks bottom-up, build-
ing unions of intermediate results. Two larger sub-
networks can be compatible only if their recursive
smaller subnetworks are compatible. Each SPU con-
tains at least all paths from the standard union.
2.4 Empirical findings
Now that we have described how we created partic-
ular networks for a given dataset, we describe some
empirical findings that characterize our annotation
process and the created networks.
Meaning-equivalent productivity. When we
compare the productivity of the three annotation
protocols in terms of the number of reference trans-
lations that they enable, we observe that target lan-
guage natives that have access to multiple human
references produce the largest networks. The me-
dian number of paths produced by one annotator un-
der the three protocols varies from 7.7 ? 105 paths
for Ara-A2E, to 1.4 ? 108 paths for Eng-A2E, to
5.9 ? 108 paths for Eng*-A2E; in Chinese, the me-
164
dians vary from 1.0? 105 for Chi-C2E, to 1.7? 108
for Eng-C2E, to 7.8? 109 for Eng*-C2E.
Protocol productivity. When we compare the
annotator time required by the three protocols, we
find that foreign language natives work faster ? they
need about 2 hours per sentence ? while target lan-
guage natives need 2.5 hours per sentence. Given
that target language natives build significantly larger
networks and that bilingual speakers are in shorter
supply than monolingual ones, we conclude that us-
ing target language annotators is more cost-effective
overall.
Exploiting multiple annotations. Overall, the me-
dian number of paths produced by a single annota-
tor for A2E is 1.5 ? 106, two annotators (randomly
picked per sentence) produce a median number of
4.7 ? 107 (Union), for all annotators together it is
2.1? 1010 (Union) and 2.1? 1011 (SPU). For C2E,
these numbers are 5.2? 106 (one), 1.1? 108 (two),
and 2.6?1010 (all, Union) and 8.5?1011 (all, SPU).
Number of annotators and annotation time. We
compute the minimum number of edits and length-
normalized distance scores required to rewrite ma-
chine and human translations into translations found
in the networks produced by one, two, and three
annotators. We find that the length-normalized dis-
tances do not vary by more than 1% when adding the
meaning equivalents produced by a third annotator.
We conclude that 2-3 annotators per sentence pro-
duce a sufficiently large set of alternative meaning
equivalents, which takes 4-7.5 hours. We are cur-
rently investigating alternative ways to create net-
works more efficiently.
Grammaticality. For each of the four human trans-
lation references and each of the five machine trans-
lation outputs (see Section 2.2), we algorithmically
find the closest path in the annotated networks of
meaning equivalents (see Section 3). We presented
the resulting 1836 closest paths extracted from the
networks (2 language pairs ?102 sentences ?9 hu-
man/machine translations) to three independent En-
glish speakers. We asked each English path to be
labeled as grammatical, grammatical-but-slightly-
odd, or non-grammatical. The metric is harsh: paths
such as ?he said that withdrawing US force with-
out promoting security would be cataclysmic? are
judged as non-grammatical by all three judges al-
though a simple rewrite of ?force? into ?forces?
would make this path grammatical. We found that
90% of the paths are judged as grammatical and
96% as grammatical or grammatical-but-slightly-
odd, by at least one annotator. We interpret these
results as positive: the annotation process leads to
some ungrammatical paths being created, but most
of the closest paths to human and machine outputs,
those that matter from an evaluation perspective, are
judged as correct by at least one judge.
Coverage. We found it somewhat disappoint-
ing that networks that encode billions of meaning-
equivalent translations for a given sentence do not
contain every independently produced human ref-
erence translation. The average length-normalized
edit distance (computed as described in Section 3)
between an independently produced human refer-
ence and the corresponding network is 19% for
Arabic-English and 34% for Chinese-English across
the entire corpus. Our analysis shows that about
half of the edits are explained by several non-
content words (?the?, ?of?, ?for?, ?their?, ?,?) be-
ing optional in certain contexts; several ?obvious?
equivalents not being part of the networks (?that??
?this?; ?so???accordingly?); and spelling alterna-
tives/errors (?rockstrom???rockstroem?). We hy-
pothesize that most of these ommissions/edits can be
detected automatically and dealt with in an appropri-
ate fashion. The rest of the edits would require more
sophisticated machinery, to figure out, for example,
that in a particular context pairs like ?with???and?
or ?that???therefore? are interchangeable.
Given that Chinese is significantly more under-
specified compared to Arabic and English, it is con-
sistent with our intuition to see that the average mini-
mal distance is higher between Chinese-English ref-
erences and their respective networks (34%) than
between Arabic-English references and their respec-
tive networks (19%).
3 Measuring translation quality with large
networks of meaning equivalents
In this section, we present HyTER (Hybrid Trans-
lation Edit Rate), a novel metric that makes use of
large reference networks.
165


	


	


	


	





	


	


	
 

	



	
	




	

	

  	
		
 		

<ts>
 
	

	

	





 


  


<root>
Reference RTN
Reordered
hypothesis

	




	



	

	



	

	


? ?
?
?
?
?
?
?
?
?
Levenshtein
lazy composition
?x
LS
H(x,Y)
Y
  	
 	
 
 Hypothesisx
convert
Figure 2: Defining the search space H(x,Y) through (lazy) composition. x is a translation hypothesis ?where train
station is?, Y contains all correct translations. ?x may be defined in various ways, here local reordering (k = 3) is
used.
HyTER is an automatically computed version of
HTER (Snover et al., 2006); HyTER computes the
minimum number of edits between a translation x
and an exponentially sized reference set Y , which is
encoded as a Recursive Transition Network (RTN).
Perfect translations have a HyTER score of 0.
General Setup. The unnormalized HyTER score
is defined as in equation (1) where ?x is a set of
permutations of the hypothesis x, d(x, x?) is the dis-
tance between x and a permutation x??typically
measured as the number of reordering moves be-
tween the two?and LS(x?, y) is the standard Lev-
enshtein distance (Levenshtein, 1966) between x?
and y, defined as the minimum number of insertions,
deletion, and substitutions. We normalize uhyter by
the number of words in the found closest path.
uhyter(x,Y) def= min
x???x,
y?Y
d(x, x?) + LS(x?, y) (1)
We treat this minimization problem as graph-
based search. The search space over which we mini-
mize is implicitly represented as the Recursive Tran-
sition Network H (see equation (2)), where ?x is
encoded as a weighted FSA that represents the set
of permutations of x with their associated distance
costs, and LS is the one-state Levenshtein trans-
ducer whose output weight for a string pair (x,y) is
the Levenshtein distance between x, and y, and the
symbol ? denotes composition. The model is de-
picted in Figure 2.
H(x,Y) def= ?x ? LS ? Y (2)
Permutations. We define an FSA ?x that allows
permutations according to certain constraints. Al-
lowing all permutations of the hypothesis x would
increase the search space to factorial size and make
inference NP-complete (Cormode and Muthukrish-
nan, 2007). We use local-window constraints (see,
e.g., Kanthak et al. (2005)), where words may move
within a fixed window of size k; these constraints
are of size O(n) with a constant factor k, where n is
the length of the translation hypothesis x.
Lazy Evaluation. For efficiency, we use lazy evalu-
ation when defining the search space H(x,Y). This
means we never explicitly compose ?x, LS, and Y .
Parts of the composition that our inference algorithm
does not explore are not constructed, saving compu-
tation time and memory. Permutation paths in ?x
are constructed on demand. Similarly, the reference
set Y is expanded on demand, and large parts may
remain unexpanded.1
Exact Inference. To compute uhyter(x,Y), we de-
fine the composition H(x,Y) and can apply any
shortest-path search algorithm (Mohri, 2002). We
found that using the A* algorithm (Hart et al., 1972)
was the most efficient; we devised an A* heuristic
similar to Karakos et al. (2008).
Runtime. Computing the HyTER score takes 30
ms per sentence on networks by single annotators
(combined all-annotator networks: 285 ms) if no
1These on-demand operations are supported by the OpenFst
library (Allauzen et al., 2007); specifically, to expand the RTNs
into FSAs we use the Replace operation.
166
Arabic-English Chinese-English
Metric Human mean Machine mean m/h Human mean Machine mean m/h
[100-0]-BLEU, 1 ref 59.90 69.14 1.15 71.86 84.34 1.17
[100-0]-BLEU, 3 refs 41.49 57.44 1.38 54.25 75.22 1.39
[100-0]-Meteor, 1 ref 60.13 65.70 1.09 66.81 73.66 1.10
[100-0]-Meteor, 3 refs 55.98 62.91 1.12 62.95 70.68 1.12
[100-0]-TERp, 1 ref 35.87 46.48 1.30 53.58 71.70 1.34
[100-0]-TERp, 3 refs 27.08 39.52 1.46 41.79 60.61 1.45
HyTER U 18.42 34.94 1.90 27.98 52.08 1.86
HyTER SPU 17.85 34.39 1.93 27.57 51.73 1.88
[100-0]-Likert 5.26 50.37 9.57 4.35 48.37 11.12
Table 2: Scores assigned to human versus machine translations, under various metrics. Each score is normalized to
range from 100 (worst) to 0 (perfect translation).
reordering is used. These numbers increase to 143
ms (1.5 secs) for local reordering with window size
3, and 533 ms (8 secs) for window size 5. Many
speedups for computing the score with reorderings
are possible, but we will see below that using re-
ordering does not give consistent improvements (Ta-
ble 3).
Output. As a by-product of computing the HyTER
score, one can obtain the closest path itself, for er-
ror analysis. It can be useful to separately count the
numbers of insertions, deletions, etc., and inspect
the types of error. For example, one may find that
a particular system output tends to be missing the fi-
nite verb of the sentence or that certain word choices
were incorrect.
4 Using meaning-equivalent networks for
machine translation evaluation
We now present experiments designed to measure
how well HyTER performs, compared to other eval-
uation metrics. For these experiments, we sample 82
of the 102 available sentences; 20 sentences are held
out for future use in optimizing our metric.
4.1 Differentiating human from machine
translation outputs
We score the set of human translations and machine
translations separately, using several popular met-
rics, with the goal of determining which metric per-
forms better at separating machine translations from
human translations. To ease comparisons across dif-
ferent metrics, we normalize all scores to a number
between 0 (best) and 100 (worst). Table 2 shows
the normalized mean scores for the machine trans-
lations and human translations under multiple au-
tomatic and one human evaluation metric (Likert).
The quotient of interest, m/h, is the mean score for
machine translations divided by the mean score for
the human translations: the higher this number, the
better a metric separates machine from human pro-
duced outputs.
Under HyTER, m/h is about 1.9, which shows
that the HyTER scores for machine translations are,
on average, almost twice as high as for human trans-
lations. Under Likert ? a score assigned by hu-
man annotators who compare pairs of sentences at
a time?, the quotient is higher, suggesting that hu-
man raters make stronger distinctions between hu-
man and machine translations. The quotient is lower
under the automatic metrics Meteor (Version 1.3,
(Denkowski and Lavie, 2011)), BLEU and TERp
(Snover et al., 2009). These results show that
HyTER separates machine from human translations
better than alternative metrics.
4.2 Ranking MT systems by quality
We rank the five machine translation systems ac-
cording to several widely used metrics (see Fig-
ure 3). Our results show that BLEU, Meteor and
TERp do not rank the systems in the same way
as HTER and humans do, while the HyTER met-
ric yields the correct ranking. Also, separation be-
tween the quality of the five systems is higher un-
der HyTER, HTER, and Likert than under alterna-
tive metrics.
4.3 Correlations with HTER
We know that current metrics (e.g., BLEU, Meteor,
TER) correlate well with HTER and human judg-
167
Arabic-English
Size Likert Meteor 1 Meteor 4 BLEU 1 BLEU 4 TERp 1 TERp 4 HyTER U (r5) HyTER SPU (r5)
1 .653 .529 .541 .512 .675 .452 .547 .643 (.661) .647 (.655)
2 .645 .614 .636 .544 .706 .599 .649 .733 (.741) .735 (.732)
4 .739 .782 .804 .710 .803 .782 .803 .827 (.840) .831 (.838)
8 .741 .809 .822 .757 .818 .796 .833 .827 (.828) .830 (.825)
16 .868 .840 .885 .815 .887 .824 .862 .888 (.890) .893 (.894)
32 .938 .945 .957 .920 .948 .930 .947 .938 (.935) .940 (.936)
64 .970 .973 .979 .964 .973 .966 .968 .964 (.960) .966 (.961)
Chinese-English
Size Likert Meteor 1 Meteor 4 BLEU 1 BLEU 4 TERp 1 TERp 4 HyTER U (r5) HyTER SPU (r5)
1 .713 .495 .557 .464 .608 .569 .594 .708 (.721) .668 (.681)
2 .706 .623 .673 .569 .655 .639 .651 .713 (.716) .702 (.701)
4 .800 .628 .750 .593 .734 .651 .726 .822 (.825) .820 (.814)
8 .810 .745 .778 .783 .808 .754 .754 .852 (.856) .854 (.845)
16 .881 .821 .887 .811 .884 .826 .844 .912 (.914) .914 (.908)
32 .915 .873 .918 .911 .930 .851 .911 .943 (.942) .941 (.937)
64 .950 .971 .976 .979 .973 .952 .970 .962 (.958) .958 (.957)
Table 3: Document-level correlations of various scores to HTER. Meteor, BLEU and TERp are shown with 1 and 4
references each, HyTER is shown with the two combination methods (U and SPU), and with reordering (r5).
!"#$% !!#!%
!&#'%
&"#'%
()#*%
'&%
!"%
!&%
&"%
&&%
("%
(&%
)% *% '% !% &%
!"#$%&'
!"#$%
!&#'%(!#)%
((#*%
"'#&%
!+%
!'%
(+%
('%
"+%
"'%
'+%
,% !% (% "% '%
!"#$%
!"#$%!&#'%
&"#$%&"#(%
$"#(%
!)%
!&%
&)%
&&%
$)%
$&%
')%
*% +% "% !% &%
!"#$%&'&
!"#$%!!#&%
'$#(%'"#)%
)!#*%
!+%
!'%
'+%
''%
)+%
)'%
,+%
*% $% "% !% '%
!"#$%&'()&
!"#$%"&#'%
"$#(%"$#)%
*&#(%
!'%
!+%
"'%
"+%
*'%
*+%
)''%
)% &% ,% $% +%
!"#$%&'&(#)&
!"#$%
!$#&%
'&#(%')#)%
$&#)%
!(%
!*%
'(%
'*%
$(%
$*%
+(%
,% "% )% &% *%
!"#$%&'&(#)*&
!!"!#!$"%#%&"'#%&"'#
%("'#
!&#
!)#
%&#
%)#
$&#
$)#
*&#
+# '# ,# (# )#
!"#"$%&'('%")*'
!"#$%
!!#!% !$#&% !$#'%
&(#!%
)"%
)!%
!"%
!!%
&"%
&!%
("%
*% +% '% )% !%
!"#$%&'&()*+&
Figure 3: Five MT systems (Chinese-English), scored by
8 different metrics. The x-axis shows the five systems, the
y-axis shows the [100-0] normalized scores, with 0 cor-
responding to a perfect translation. (Note that the scale
is similar in all eight graphs.) HTER and HyTER show a
similar pattern and similar ranking of the systems.
ments on large test corpora (Papineni et al., 2002;
Snover et al., 2006; Lavie and Denkowski, 2009).
We believe, however, that the field of MT will be
better served if researchers have access to metrics
that provide high correlation at the sentence level as
well. To this end, we estimate how well various met-
rics correlate with the Human TER (HTER) metric
for corpora of increasingly larger sizes.
Table 3 shows Pearson correlations between
HTER and various metrics for scoring documents
of size s =1, 2, 4, . . . , and 64 sentences. To get
more reliable results, we create 50 documents per
size s, where each document is created by select-
ing s sentences at random from the available 82 sen-
tences. For each document, there are 5 translations
from different systems, so we have 250 translated
documents per size. For each language and size, we
score the 250 documents under HTER and the other
metrics and report the Pearson correlation. Our re-
sults show that for large documents, all metrics cor-
relate well with HTER. However, as the sizes of the
documents decrease, and especially at the sentence
level, HyTER provides especially high correlation
with HTER as compared to the other metrics. As
a side note, we can see that using reordering when
computing the HyTER score does not give consis-
tently better results ? see the (r5) numbers, which
searched over hypothesis permutations within a lo-
cal window of size 5; this shows that most reorder-
ings are already captured in the networks. In all ex-
periments, we use BLEU with plus-one smoothing
(Lin and Och, 2004).
5 Using meaning-equivalent networks for
human translation evaluation
In this section, we present a use case for the HyTER
metric outside of machine translation.
168
5.1 Setup and problem
Language Testing units assess the translation profi-
ciency of thousands of applicants interested in per-
forming language translation work for the US Gov-
ernment. Job candidates typically take a written test
in which they are asked to translate four passages
(i.e., paragraphs) of increasing difficulty into En-
glish. The passages are at difficulty levels 2, 2+, 3,
and 4 on the Interagency Language Roundable (ILR)
scale.2 The translations produced by each candidate
are manually reviewed to identify mistranslation,
word choice, omission, addition, spelling, grammar,
register/tone, and meaning distortion errors. Each
passage is then assigned one of five labels: Success-
fully Matches the definition of a successful transla-
tion (SM); Mostly Matches the definition (MM); In-
termittently Matches (IM); Hardly Matches (HM);
Not Translated (NT) for anything where less than
50% of a passage is translated.
We have access to a set of more than 100 rules that
agencies practically use to assign each candidate an
ILR translation proficiency level: 0, 0+, 1, 1+, 2, 2+,
3, and 3+. For example, a candidate who produces
passages labeled as SM, SM, MM, IM for difficulty
levels 2, 2+, 3, and 4, respectively, is assigned an
ILR level of 2+.
We investigate whether the assessment process
described above can be automated. To this end, we
obtained the exam results of 195 candidates, where
each exam result consists of three passages trans-
lated into English by a candidate, as well as the
manual rating for each passage translation (i.e., the
gold labels SM, MM, IM, HM, or NT). 49 exam re-
sults are from a Chinese exam, 71 from a Russian
exam and 75 from a Spanish exam. The three pas-
sages in each exam are of difficulty levels 2, 2+, and
3; level 4 is not available in our data set. In each
exam result, the translations produced by each can-
didate are sentence-aligned to their respective for-
eign sentences. We applied the passage-to-ILR map-
ping rules described above to automatically create a
gold overall ILR assessment for each exam submis-
sion. Since the languages used here have only 3 pas-
sages each, some rules map to several different ILR
ratings. Table 4 shows the label distribution at the
2See http://www.govtilr.org/skills/
AdoptedILRTranslationGuidelines.htm.
Lang. 0 0+ 1 1+ 2 2+ 3 3+
Chi. 0.0 8.2 40.8 65.3 59.2 10.2 4.1 0.0
Rus. 0.0 2.8 12.7 42.3 60.6 46.5 25.4 5.6
Spa. 0.0 1.3 33.3 66.7 88.0 24.0 4.0 0.0
All 0.0 3.6 27.7 57.4 70.8 28.7 11.8 2.1
Table 4: Percentage of exams with ILR levels 0, 0+, . . . ,
3+ as gold labels. Multiple levels per exam possible.
ILR assessment level across all languages.
5.2 Experiments
We automatically assess the proficiency of candi-
dates who take a translation exam. We treat this as a
classification task where, for each translation of the
three passages, we predict the three passage assess-
ment labels as well as one overall ILR rating.
In support of our goal, we asked annotators to cre-
ate an English HyTER network for each foreign sen-
tence in the exams. These HyTER networks then
serve as English references for the candidate transla-
tions. The median number of paths in these HyTER
networks is 1.6? 106 paths/network.
In training, we observe a set of submitted exam
translations, each of which is annotated with three
passage-level ratings and one overall ILR rating.
We develop features (Section 5.3) that describe each
passage translation in its relation to the HyTER net-
works for the passage. We then train a classifier to
predict passage-level ratings given the passage-level
features that describe the candidate translation. As
classifier, we use a multi-class support-vector ma-
chine (SVM, Krammer and Singer (2001)). In de-
coding, we observe a set of exams without their rat-
ings, derive the features and use the trained SVM to
predict ratings of the passage translations. We then
derive an overall ILR rating based on the predicted
passage-level ratings. Since our dataset is small we
run 10-fold cross-validation.
5.3 Features
We define features describing a candidate?s transla-
tion with respect to the corresponding HyTER refer-
ence networks. Each of the feature values is com-
puted based on a passage translation as a whole,
rather than sentence-by-sentence. As features, we
use the HyTER score, as well as the number of in-
sertions, deletions, substitutions, and insertions-or-
deletions. We use these numbers normalized by the
169
Level Measure Baseline HyTER-enabled
All Accuracy 72.31 90.77
2 or better
Precision 85.62 82.11
Recall 84.93 98.63
F1 85.27 89.62
Table 5: Predicting final ILR ratings for candidate exams.
length of the passage, as well as unnormalized. We
also use n-gram precisions (for n=1,. . . ,20) as fea-
tures.
5.4 Results
We report the accuracy on predicting the overall ILR
rating of the 195 exams (Table 5). The results in 2
or better show how well we predict a performance
level of 2, 2+, 3 or 3+. It is important to retrieve
such relatively good exams with high recall, so that
a manual review QA process can confirm the choices
while avoid discarding qualified candidates. The re-
sults show that high recall is reached while preserv-
ing good precision. Since we have several possible
gold labels per exam, precision and recall are com-
puted similar to precision and recall in the NLP task
of word alignment. F1(P,R) = 2PRP+R is the har-
monic mean of precision and recall. The row All
shows the accuracy in predicting ILR performance
labels overall. As a baseline method we assign the
most frequent label per language; these are 1+ for
Chinese, and 2 for Russian and Spanish. The results
in Table 5 strongly suggest that the process of as-
signing a proficiency level to human translators can
be automated.
6 Discussion
We have introduced an annotation tool and process
that can be used to create meaning-equivalent net-
works that encode an exponential number of trans-
lations for a given sentence. We have shown that
these networks can be used as foundation for devel-
oping improved machine translation evaluation met-
rics and automating the evaluation of human trans-
lation proficiency. We plan to release the OpenMT
HyTER networks to the community after the 2012
NIST Open MT evaluation. We believe that our
meaning-equivalent networks can be used to support
interesting research programs in semantics, para-
phrase generation, natural language understanding,
generation, and machine translation.
Acknowledgments
We thank our colleagues and the anonymous review-
ers for their valuable feedback. This work was par-
tially sponsored by DARPA contract HR0011-11-
C-0150 and TSWG contract N41756-08-C-3020 to
Language Weaver Inc.
References
