THE COMMONSENSE ALGORITHM 
AS A BASIS FOR COMFUTER MODELS 
OF HUMAN MEMORY, INFERENCE, BELIEF 
AND CONTEXTUAL LANGUAGE COMPREHENSION 
Chuck Rieger 
Department of Computer Science 
Univers i ty of Maryland 
ABSTRACT 
The notion of a commonsense 
a lgor i thm is presented as a basic 
data structure for model ing human 
cognit ion. This data structure 
unif ies many current ideas about 
human memory and information 
process ing .  The structure is 
defined by specifying a set of 
proposed cognit ive primitive links 
which, when used to build up large 
structures of actions, states, 
statechanges and tendencies, 
provide an adequate formal ism for 
expressing human plans and 
activit ies, as well as general 
mechanisms and computer algorithms. 
The commonsense algor i thm is a type 
of framework (as Minsky has defined 
the term) for represent ing 
algor i thmic processes, hopeful ly 
the way humans do. 
I. INTRODUCTION AND MOTIVATION 
It is becoming increasingly evident to 
human intel l igence model bui lders and 
theorists that, in order to character ize 
human knowledge and bel ief as computer data 
structures and processes, it is necessary to 
deal with very large, expl ic i t ly  unif ied 
structures rather than smaller, ununif ied 
fragments. The reason for this seems to be 
that the or iginal  experiences which caused 
the structures and processes to exist in the 
first place come in chunks themselves; 
knowledge is never gained outside of some 
context, and in gaining some piece of 
knowledge X in context C, X and C become 
inseparable. This suggests that it is 
meaningless to model "a piece of knowledge" 
without regard for the larger structures of 
which it is a part. If our goal is to build 
a robot which behaves and perceives in 
manners s imi lar to a human, this means that 
the process by which the robot selects a 
piece of knowledge as being appl icable to 
the planning, executory, inferential  or 
interpret ive process at hand at the moment 
is a function not only of the specif ic 
problem, but also of the larger context in 
which that instance of planning, execution, 
inference or interpretat ion occurs. If, for 
example, our robot sees his friend with a 
wretched facial expression, the inference he 
makes about the reasons for his fr iend's 
misery wil l  reflect the larger picture of 
which he is aware at the time: his friend 
has just returned from a trip to purchase 
opera t ickets vs. his friend has Just eaten 
the cache of mushrooms col lected yesterday 
vs . . . . .  The same pervasiveness of context 
exists in the realm of the robot's 
interpretat ions of visual perceptions: the 
very same object (visible at eye level) wi l l  
18o 
be perceived out of the corner of his eye in 
one situation as the cyl indr ical  top of his 
electr ic coffee grinder (he is at home in 
his kitchen), but as the f lasher of a police 
car (he is speeding on the freeway) in 
another. This suggests that at every 
moment, some fairly large swatch of his 
knowledge about the world somehow has found 
its way to the foreground to exert its 
influence; as our robot moves about, 
swatches must fade in and out, sometimes 
coalescing, so that at any moment, just the 
right one is standing by to help guide acts 
of planning, infePence and perception. 
Marvin Minsky has captured this whole 
idea very neatly in his wide ly-c i rcu lated 
"Frames" paper \[MI\]. While this paper 
describes an overal l  approach to model ing 
human memory, inference and beliefs, we 
sti l l  lack any specif ic formulat ion of the 
ingredients which make up the large, 
expl ic i t ly -uni f ied structures which seem to 
underl ie many higher- level  human cognit ive 
functions. It is the purpose of this paper 
to define the notion "commonsense algor ithm" 
(CSA) and to propose the CSA as the basic 
Cognit ive structure which underl ies the 
human processes of planning, inference and 
contextual  interpretat ion of meaning. 
I do not have a complete theory yet: 
the intent of this paper is to record a 
memory dump of ideas accumulated over the 
past few months and to show how they can 
unify my past ideas on inference and memory, 
as well as the ideas of others. 
II. THE SCOPE OF THE CSA'S APPLICABIL ITY 
Most of human knowledge can be 
c lassi f ied as either static or dvnamic. For 
example, a person's static knowledge of an 
automobi le tells him its general physical  
shape, size, posit ion of steering wheel, 
wheels, engine, seats, etc.; these are the 
abstract aspects of a car which, a l though 
many differ in detail  from car to car, are 
inherent ly  unchanging. They are in essence 
the physical  def init ion of a car. On the 
other hand, a person s dynamic knowledge of 
a car tel ls him the functions of the various 
components and how and why to coordinate 
them when the car is appl ied to some goal. 
The static knowledge tells the person where 
to expect the steering wheel to be when he 
gets in; the dynamic knowledge tells him how 
to get in in the first place, and what to do 
with the wheel (and why) once he is in. For 
a robot immersed in a highly k inematic  
world -- physical ly, psychologica l ly  and 
social ly  -- a very large part of his bel iefs 
and knowledge must relate to dynamics: how 
he can effect changes in himself  and his 
world, and how he perceives other robots 
ef fect ing changes. It is the purpose of the 
CSA to capture the dynamics of the world in 
bel ief structures which are amenable to 
computer manipulat ion of plans, inference 
and contextual  interpretat ion.  
It should be stressed that the phrase 
"dynamics of the world" is intended in its 
broadest possible sense. As wil l  be 
e laborated upon in a later section, the 
I 
I 
I 
I 
I 
I 
i 
I 
i 
II 
I 
phrase is intended to encompass such 
seemingly diverse robot/human act ivit ies as: 
I. communicat ing with another 
robot/human (e.g., how to transfer 
information, insti l l  wants, 
convince, etc.) 
2. getting about in the world 
3. building things (both physical and 
mental) and understanding the 
operation of things already built 
by others 
4. conceiving, designing .and 
implementing computer programs and 
other commonsense algorithms (a 
special form of building) 
5. interpret ing sequences of 
perceptions (e.g., language 
utterances) in context 
6. making contextual ly meaningful  
inferences from perceptions 
I am convinced that all such dynamics 
of the world can and should be expressed in 
a uniform CSA formalism built around a 
relat ively small number of cognit ively 
primitive ingredients. 
III. EVOLUTION OF THE CSA IDEA 
The next section will define a CSA as a 
network- l ike structure consist ing of events 
tied together by primitive links. Taken as 
a whole, the CSA specif ies a process: how to 
get something done, how something works, 
etc. A computer scientist 's first reaction 
to this type of structure is "Oh yes, that's 
an AND/OR problem-reduct ion graph" (see 
Nilsson \[NI\] for example). Figure I shows 
an AND/OR graph for how to achieve the goal 
state "a McDonald's hamburger is in P's 
stomach." Edges with an arc through them 
specify AND successors of a node (subgoals, 
all of which achieved imply the parent node 
has been achieved); edges with no arc 
through them specify OR successors 
(subgoals, any one of which being suff ic ient 
to achieve the parent goal). 
AND/OR graphs have been demonstrated 
adequate in practice for guiding various 
aspects of problem-solving behavior in 
exist ing robots (see \[$I\] for example) .  
However, they are intuit ively not 
theoret ical ly  adequate structures for 
representing general knowledge of world 
dynamics: their principal def ic iency is that 
they are ad-hoc construct ions which express 
neither the implicit conceptual 
relat ionships among their components, nor 
the inherent types of their components. 
Because of this, there is no constraint on 
their organization, and this means that two 
AND/OR graphs which accomplish or model the 
same thing might bear very l ittle 
resemblance to one-another when in fact they 
are conceptual ly very similar. This may be 
little more than a nuisance in practice, but 
it is undesirable in principle because it 
makes learning, reasoning by analogy, 
sharing of subgoals, etc. ted ious  if not 
impossible in a general ized problem solver. 
A refinement of the notion of an AND/OR 
graph introduces the concepts of causal ity 
and enablement, and actions and states 
(statechanges); edges in the graph are 
dist inguished as either causal or enabling, 
the nodes are dist inguished as either 
actions or states, and the graph obeys the 
syntactic contraints: 
(a) actions cause states 
(b) states enable actions 
Bob Abelson \[AI\] was among the first to 
employ these histor ical ly very old concepts 
in the framework of a computer model of 
human belief, and since then, numerous 
computer-or iented systems of knowledge 
reDresentat ion (e.g., Schank's conceptual  
deDendency\[S2\] ,  Schmidt's models of personal  
causation \[$4\]), as well as systems of 
inference (Rieger \[RI\], Charniak \[CI\]) have 
found these four concepts to be vital to 
meaning representat ion and inference. In 
some sense, enablement, causality, states 
and actions seem to be cognitive primitives. 
Figure 2 is a ref inement of Figure I which 
makes explicit the nature of each node and 
each connect ing arc, and hence the 
underlying gross conceptual structure of the 
algorithm. 
While the inclusion of these four 
concepts (and their result ing syntactic 
constraints) in the basic paradigm makes for 
a theoret ical ly more coherent 
representation, the scheme is still too 
coarse to capture the kinds of detai led 
knowledge of algor ithms people possess. The 
fol lowing section proposes an extended 
framework of event types and event 
connectors based on these four notions and 
some others. These event types and 
connectors will be regarded as 
model -pr imit ives which hopeful ly are in 
correspondence with "psychological  
pr imit ives" in humans. 
IV. DEFINITION OF THE COMMONSENSE ALGORITHM 
In the new formalism, a CSA consists of 
nodes of five types: 
I. WANTS 
2. ACTIONS 
3. STATES 
4. STATECHANGES 
5. TENDENCIES 
The first four types are not new (see \[$3\] 
for example), and wil l  not be covered here 
beyond the fol lowing brief mention. A WANT 
is some goal state which is desired by a 
potential  actor. An action is something an 
(animate) actor does or can do: it is 
enabled by certain states (certain 
condit ions which must be true in order for 
the action to begin and/or proceed), and in 
turn causes other states (discrete) or 
statechanges (continuous) to occur. Actions 
are character ized by an actor, a 
model-pr imit ive action, a time aspect, a 
location aspect, and a conceptual case 
framework which is specif ic to each 
model -pr imit ive action. States are 
character ized by an object, an attribute, a 
181 
value and a time aspect; statechanges are 
character ized by an object, a continuous 
state scale (temperature, degree of anger, 
distance, etc.), a time aspect and beginning 
and end points on the scale. 
It is the notion of a tendency which is 
new and which serves to unify a class of 
problems which have been cont inual ly  
exper ienced in represent ing processes. 
Basical ly, a tendency is an actor less 
action. Tendencies are character ized by 
speci fy ing a set of enabling condit ions ,and 
a set of result states and/or statechanges. 
Whenever the enabling condit ions are 
satisf ied, the tendency, by some unspeci f ied 
means, causes the states and statechanges 
speci f ied as the tendency's results. Hence, 
a tendency may be regarded as a special  type 
of non-purposive action which must occur 
whenever all its enabling condit ions are 
satisf ied. Contrast ing the notion of a 
tendency with the notion of an action yields 
a rather compact def init ion of what makes a 
"vol i t ional" act ion vol it ional:  a vol i t ional  
act ion is an action which need not occur 
even though all its physical  enabl ing 
condit ions are met. The reason it may not 
occur is, of course, that the actor does not 
desire it to occur; tendencies have no such 
desires. 
The abstract notion of a tendency is 
meant to be general-purpose, to character ize 
a wide variety of phenomena which are not 
actions, but action-l ike. Examples of 
tendencies are: 
I. GRAVITY, PRESSURE, MAGNETISM, 
ATOMIC-FISSION, HEAT-FLOW, and the host 
of other physical principles. 
Commonsense GRAVITY might be captured 
as fol lows:** 
((TYPE . TENDENCY) 
(REFERENCE-NAME . GRAVITY) 
(ENABLEMENTS . (UNSUPPORTED OBJ) 
(LESSP (DISTANCE OBJ EARTH) 
(ORDERMILES) )  
(RESULTS . (STATECHANGE OBJ VELOCITY X 
X+d (LOC OBJ) 
(LOC EARTH))) 
2. human biological  functions: a tendency 
to GROW-HUNGRY, GROW-SLEEPY, GROW-OLDER 
(sole enabl ing condit ion is the passage 
of time!), GROW-LARGER, etc. For 
example: 
((TYPE . TENDENCY) 
(REFERENCE-NAME GROW-HUNGRY) 
(ENABLEMENTS . (iNOT (LOC NUTRIENTS 
STOMACH)) 
(DURATION * ORDERHOURS)))  
(RESULTS . (WANT P (INGEST P NUTRIENTS 
MOUTH STOMACH))))  
3. human psychological  functions: the 
tendency to GROW-LONELY, the tendency 
to FORGET, etc. For example: 
**The LISP notat ion ref lects some 
concurrent thinking on how a 
commonsense algor i thm system might 
actual ly be engineered. A forthcoming 
report wil l  describe progress toward 
implementing the ideas in this paper. 182 
((TYPE ? TENDENCY) 
(REFERENCE-NAME . GROW-LONELY) 
(ENABLEMENTS ((ALONE P) 
iDURATION * ORDERDAYS)) 
(RESULTS . (WANT P (COMMUNICATE P 
X)))) 
((TYPE . TENDENCY) 
(REFERENCE-NAME . FORGET) 
(ENABLEMENTS . (INHEAD ITEM P) 
((UNREFERENCED ITEM P) 
(DURATION * ORDER??))) 
(RESULTS (STATECHANGE ITEM 
REFERENCE-DIFFICULTY X X+d)) 
Tendencies, thus character ized, will 
play an important role in model ing 
algor i thmic processes via CSA's. In fact, 
adopt ing the notion of a tendency as a model 
pr imit ive points out a rather ubiquitous 
principle: humans spend a large amount of 
time in planning either how to overcome 
tendencies which stand in the way of their 
goals, or how to harness them at the proper 
times in place of an action (e.g., dropping 
the large rock on the coconut). Although a 
tendency 's  pr imary use is at the edge of the 
world model, where things happen simply 
because "that's the way things are", it will 
probably be desirable to have the abi l i ty to 
regard as tendencies things which in fact 
can be explained. Character iz ing something 
as a tendency even though it may be 
reduceable to further algor i thms is probably 
one tactic a human employs when confronted 
with the analysis of very complex, olny 
part ia l ly understood processes. Even though 
something ~ be further explained, the 
system of representat ion should al low that 
something to be treated as though it were a 
tendency. 
Tendencies have numerous aspects which 
wil l  require expl ic it  character izat ion in a 
computer model. Two such aspects relate to 
(I) the inherent rapidity with which a 
tendency exerts itself  and (2) the 
tendency's  periodicity,  if any. That is, 
how quickly does a person become hungry 
(slope of curve), how long does it take to 
forget something, how rapidly does an object 
accelerate, how fast does the water f low 
through the nozzle, etc.? If the tendency is 
periodic, what are the parameters descr ib ing 
its per iodic i ty? The primit ive CSA links 
descr ibed in the next section wil l  serve in 
part to capture such aspects, but they are 
not yet adequate. 
The CS~ nrimit ive L in~ Using these 
five event-types as bui lding blocks (WANTS, 
ACTIONS, STATES, STATECHANGES, TENDENCIES),  
the goal is to be able to express the 
dynamics of just about anything, be it a 
physical  device, a psychological  tactic 
employed by one person on another, how a 
person purchases a McDonald's  hamburger, or 
how a computer program functions or was 
constructed. There are 25 pr imit ive l inks 
in the current formulation. They will only 
be defined here, leaving Just i f icaion and 
details of their use for the examples which 
wil l  follow, and for subsequent papers on 
the subject. In the fo l lowing definit ions, 
W, A, S, SC and T wil l  stand for WANT, 
ACTION, STATE, STATECHANGE and TENDENCY, 
respectively.  
I 
I 
! 
l 
I 
I 
i 
I 
i 
I 
i 
I 
I 
I 
I 
i 
I 
I 
I 
TYPE I: ONE-SHOT CAUSALITY fA -r~ 
l Action A or tendency T causes state S. 7 The action or tendency  need occur only once; thereafter S will persist until altered by another j action or tendency. For any given S, there will 
ordinarily be numerous alternative A's or T's in the 
algorithmic base which would provide the one-shot 
causality. 
l TYPE 2: CONTINUOUS CAUSALITY 
I Action or tendency A,T's continuing existence 
continually causes state or statechange S,SC. 
Whether one-shot or continuous causality is 
required to maintain S or SC is both a function of S or 
? SC and its particular environment in a particular 
algorithm (i.e., what other tendencies and actions are 
i influencing it). Again, there will ordinarily be 
numerous actions or tendencies in the algorithmic base 
which could provide continuous causality for any given 
state or statechange. 
I TYPES 3,4: GATED ONE-SHOT AND CONTINUOUS CAUSALITY 
A,T causes S,SC either one-shot or continuously, \[ 
i providing that all states in \[S\] are satisfied. ! ~4- - '~  
The flow of causality cannot occur unless states 
specified by \[S\] exist. That is, even though A,T is 
occuring and there is a potential causal relationship ~r 
between A,T and S,SC, the relationship will not be 
i realized until the gating states become true. 
TYPE 5: ONE-SHOT ENABLEMENT 
l State S's one-time existence allows action A or 
tendency T to proceed. 
Thereafter, A,T's continuation is no longer a 
function of S. A,T will ordinarily have numerous 
one-shot enablements, in which case, all must be 
satisfied in order for A or T to proceed. ' 
State S's continued presence is requisite to 
action A's or tendency T's continuance. 
i S's removal causes A or T to halt. Any given A or 
T will ordinarily have numerous continuous enablements, 
in which case all must reamin true in order for A or T 
to proceed. 
TYPE 7: CAUSAL STATE COUPLING 
States $I, S2 or statechangges SCI, SC2 are 
causally coupled; because of this coupling, changes in 
$I or SCI are synonomous with changes in $2 or SC2. 
This link provides a way of capturing the relatedness 
of various aspects of the same situation. 
TYPE 8: GATED CAUSAL STATE COUPLING 
State $2 or statechange 
(causally coupled to) $I 
states in \[S\] are true. 
SC2 is synonymous with 
or SCI, provided that all 
This link is similar to ungated state coupling, except 
for the existence of factors which could disrupt the 
coupling. To illustrate, the flow of a fluid into a 
container (a statechange in location of the water) is 
synonymous with an increase in the amount of water in 
183 
the container (another statechange), but only providing 
that there is no souL~ce of exit from the container's 
bottom. 
TYPE 9,10,11,12: 
GATED/NON-GATED) 
BYPRODUCT (0NE-SHOT/CONTINUOUS, 
State S or statechange SC is a causal byproduct of 
action A, relative to goal state Sg or SCg. 
That is, the actor of A, wishing to achieve state 
Sg or statechange SCg also produces state S or 
statechange SC. The byproduct link 'is truly a causal 
link; what is and is not a by product must obviously 
relate to the motive of the actor in performing the 
action. Where gated, all states in \[S\] must be 
satisf ied in order for the byproduct to occur. 
TYPE 13: ORIGINAL INTENT 
Want W is the original  desire (goal state) of an 
actor. W is external to the CSA in that its origin is 
not expl icable within the CSA itself; it is the outside 
directive which motivated the invocation of some acton. 
Within an algor i thm for achieving some goal, 
mot ivat ions are explicable: every subaction is, by its 
nature, designed to produce subgoal states which, taken 
together, meet the original intent. 
TYPE 14: ACTION CONCURRENCY 
Actions AI , . . . ,An must be concurrent ly executed. 
This link will arise in the dynamics of an actual 
plan, rather than be stored or ig inal ly  in the 
algor ithmic base explicit ly. As plans evolve and the 
actor learns concurrency by rote, the link will begin 
to appear in the algor ithmic base as well. Action 
concurrency is nearly always caused by mult iple 
enabl ing states for some other action, all of which 
must be cont inual ly  present, or one-t ime synchronized 
as a col lect ion of one-shot enablements. 
TYPE 15: DYNAMIC ANTAGONISM 
State $I or statechange SCI is antagonist ic  to 
state $2 or statechange SC2 along some dimension. 
This link relates two states Or statechanges which 
are opposites in some sense; typical ly  the antagonism 
link wil l  make explicit the final link in some sort of 
feedback cycle in an algorithm. The link is hard to 
describe outside the context of an example; examples 
will appear in the next section. 
TYPE 16: MOTIVATING DYNAMIC ~NTAGONISM 
As with ordinary dynamic antagonism, $I, $2 are 
antagonist ic  states. Typically, $2 is required as an 
enabl ing state (continuous) for some action, but that 
action, or some other action, produces $I as a 
byproduct; this gives rise to the need for another 
correct ive action A which can suppress the byproduct, 
therby preserving the original  required enablement. 
This link is intended to capture the execution dynamics 
of a s ituation in which antagonist ic  states are 
expected to arise. That is, it wil l  provide a 
representat ion wherein antagonisms can be ant ic ipated 
in advance of the SCA's actual execution. An example 
of mot ivat ing dynamic antagonism is included in the 
next section. 
184 
II 
i 
I 
I 
i 
I 
! 
I 
i 
I 
I 
I 
i 
I 
I 
1 
I 
I 
I 
TYPE 17: GOAL-REALIZATION COUPL ING 
State S is an alternat ive way of expressing 
original goal W or subgoal Sg .  
This link suppl ies a way of speci fy ing terminat ion 
criteria for CSA's involving repretit ion. Its use is 
i l lustrated in one of the examples~ 
TYPE 18: COMPOUND GOAL STATE DEFINITION 
State S is a shorthand for expresing the set of 
goal states SI,. . . ,Sn. 
This link allows a "situation" to be character ized 
as a col lect ion of goal states. When all goal states 
are satisf ied, the situation is satisf ied. An example 
of a compound goal state would be: "get the kids ready 
for the car trip", where this means a set of things 
rather than one thing. 
TYPES 19,20,21,22: DISENABLEMENT 
GATED/NON-GATED) 
(ONE-SHOT/CONTINUOUS, 
Action A or tendency T one-shot /cont inual ly  causes 
state S or statechange SC not to exist. 
These four forms are shorthands for causal i ty in 
conjunct ion with antagonism. They will be pr incipal ly 
useful for represent ing acts of d isenabl ing unwanted 
tendencies. 
TYPE 23: REPETITION UNTIL THRESHOLD 
Action A or tendency T occurs repeatedly until 
state S becomes true. 
This link provides for the repeated appl icat ion of 
an action or tendency. Normally, the action or 
tendency will, d irect ly or indirectly, causal ly produce 
a statechange along some scale; this statechange will 
eventual ly threshold at state S. 
TYPE 24: INDUCEMENT 
State S's or statechange SC's existence induces 
want W in a potential  actor. 
Origins of wants can be expl ic i t ly  represented via 
this link. Typically, W wil l  be a stabe which is 
antagonist ic  to S or SC. For example ,  if the 
temperature is too high in the room, the want is that 
the temperature become lower; if the tendency, 
PRESSURE, has been enabled, al lowing blood to flow out 
of P's body, the induced want is that this tendency be 
disenabled, and hence that the antagonism of one of 
PRESSURE's enabl ing states start to exist. 
TYPE 25: OPTIMIZATION MARKER 
State S is an enabling condit ion for action A, and 
this re lat ionship makes possible an opt imizat ion during 
the execution of A in a part icular environment. 
When several actions arise in a plan, they may share 
enabling states. This means that when the plans are 
considered together, some of the states needed for one 
action may coincide with those needed for another. The 
opt imizat ion marker allows this phenomenon to be 
recorded. Its interpretat ion is: when state S becomes 
true, consider performing acton A~ because action A 
also has S as an enabling state. ~ denotes a savings. 
185 
D 
These are the commonsense algorithm 
primitive links. It is felt that they are 
conceptually independent enough of 
one-another so that unique algorithms will 
be forced into unique, or at least similar, 
representations under this formalism. 
Although it is the eventual intent of the 
theory to be able to capture all the nuances 
of intentional human problem-solving 
behavior, there is no real feeling yet for 
the completeness of this set of links in 
this regard; all that can be said now is 
that they do seem to suggest a reasonable 
approach to representing large classes of 
purposive human behavior. The adequacy of 
these primitives for representing devices 
and mechanisms, on the other hand, is easier 
to see, at least intuitively; the links seem 
to be adequate for some fairly complex 
"purposive" mechanisms. Accordingly, the 
first example of their use will be to 
characterize a mechanism very dear to most 
of us. 
V. EXAMPLES OF COMMONSENSE ALGORITHMS 
EXAMPLE I. Operation of g reverse-trap 
toilet \[Figure 3\] 
As a first test of the theory, the 
reverse-trap toilet is a relatively 
demanding mechanism. It is a complex 
feedback mechanism which is the product of 
some rather sophisticated human 
problem-solving. It is therefore 
interesting both in its own right and as a 
tangible manifestation of human-concocted 
causality and enablement. By one simple 
action, a complex sequence of tendencies is 
unleasehed; the sequence not only stops 
itself, but restores the system to its 
initial state, and does something useful in 
the process. 
The English description of the 
schematic of Figure 4 is as follows: The 
trip handle is pushed down, one-shot causing 
the flush-ball to be raised; this one-shot 
enables the tendency to float, in turn 
continually causing the float ball to remain 
raised. The float ball's being raised is 
synonomous with the flush valve being open, 
and this openness continuously enab les  the 
tendency of gravity to move water from the 
tank to the bowl beneath (as long as water 
remains in the tank, of course.) This 
movement of water is synonomous with two 
other state changes: a decrease of water 
height in the tank, and an increase of water 
height in the bowl. The increase of bowl 
water height thresholds when the water 
reaches waste channel lip level, at which 
time it begins providing continuous 
enablement for gravity to move the water 
into the waste channel; this movement 
thresholds when the channel fills, providing 
the beginning of continuous enablement of 
the tendency capillary action. This 
tendency, in turn, sustains the flow of 
water from the bowl to the waste channel, 
continually moving waste water into the 
sewer. This action ceases when the bowl 
becomes empty. Meanwhile, the tendency 
gravity is continually moving water from the 
tank to the bowl. This is synonomous with a 
186 
decrease in tank water height, and this 
decrease thresholds at point X, synonomous 
with the fresh water supply valve opening. 
This opening enables the tendency pressure 
to move water from the fresh water line into 
the tank; this is synonomous with an 
increase in tank water height, but only 
providing that the flush valve is closed 
(this will have to wait for the movement of 
waste from tank to bowl to cease). When the 
tank water height finally begins its 
increase, this increase will threshold at 
point X again, this time being synchronous 
with the ball cock supply valve's being 
closed, stopping the fresh water and hence 
the tank water height increase. At this 
point, the system has become quiescent 
again. (Note: in the actual simulation 
which will be performed, flow rates, or more 
generally, rates of statechanges, will be 
incorporated.) 
EXAMPLE 2. Sawing a hoard in half to 
decrease its length (Figure 5) 
Figure 5 is a bare-bones representation 
Of a purposive human process: sawing a board 
in two using a handsaw. This CSA 
il lustrates the concepts of motivating 
dynamic antagonsim, original intent and 
byproduct with respect to a goal. The 
schematic of Figure 5 is only a fragment of 
the larger algorithm; many enabling states 
and byproducts, as well as their 
compensatory actions have been omitted. In 
this CSA, the act of sawing for the purpose 
of decreasing the board's length produces, 
among others, the byproduct of the board's 
moving. Since a stationary board is a gate 
condition on the flow of causality from the 
sawing action to the statechange in cut 
depth, the two states joined by the 
motivating dynamic antagonsim link form an 
antagonistic pair, indicating in advance of 
actual execution that it will be necessary 
to perform a compensatory action: hold the 
wood down. If we were to illustrate more of 
this algorithm, it might be found that 
holding the wood down would require more 
hands than were available. This would 
provide another dynamic antagonsim which 
would motivate the engagement of another 
compensatory action, such as "call for 
help," "go to a vise," etc. 
It should again be pointed out that 
points of antagonism could alternatively be 
detected at the execution time of the CSA 
and compensatory solutions dynamically 
fabricated. This would likely occur via 
some sort of interrupt mechanism. But the 
antagonsim link allows for planning ahead 
(e.g. when two arbitrary algorithms are 
selected to accomplish a task, their 
coexistence will probably not always be 
without antagonism -- this allows the 
planning mechanism to anticipate and solve 
such antagonisms before execution). Also, 
after a successful plan involving 
antagonisms has actually been executed, this 
link provides a means of recording once and 
for all the compensating actions which were 
performed. 
I 
I 
I 
I 
I 
I 
I 
EXAMPLE 3. Vicious cycles <Figure 6) 
Consider tendencies such as fire and 
forgetfulness. Both roughly follow the 
paradigm: a tendency has state S as a 
continuous enablement, and produces the same 
state as continuous causality. Once 
started, such a system is self-sustaining. 
In the case of fire, a one-shot causing 
action causes a statechange in temperature 
which thresholds at the point of the 
material 's  combustion temperature; this 
enables the tendency to burn, which in turn 
produces as a continual byproduct heat, 
causing a vicious cycle. In forgetting, the 
tendency to forget X is enabled by not 
referencing X for periods of time; but as X 
grows more forgotten, it becomes less 
referenceable. Here, dynamic antagonism 
lies at the root of the vicious cycle. 
EXAMPLE 4. Descr ipt ion (synthesis) of 
computer algor i thm (Figure 7) 
Suppose the goal is to compute the 
average of a table of numbers, 
TABLE(1), . . . ,TABLE(n).  Figure 7 shows both 
how to conceive of the algor i thm and how the 
algor i thm wil l  actual ly run. As a computer 
algorithm, this is not as fully explicit as 
might be desired: it lacks explicit 
i teration and explicit termination cr i ter ion 
testing. These will have to be worked out 
before the theory adequately handles 
repetit ion. 
i 
Causal gating seems to play a central 
role in this sort of computer algorithm. 
Intuitively, this is the case because, 
though a computer instruct ion typical ly has 
no physical enabling condit ions (it could be 
issued at any time), desired effects can be 
achieved only by tying the syntax of 
instruct ion causal ity to the semantics of 
logical causality. For example, the flow of 
causal ity from the action "fetch location 
SUM to ACI" to the logical semantic state 
"partial sum in ACI" can take place only if 
location SUM logical ly contains the actual 
partial sum at that point! Otherwise, 
garbage is fetched. 
The relat ionships of certain types of 
causal gating and state coupling (e.g. the 
valve closing because the float has risen in 
the toilet tank) are not completely apparent 
yet. Perhaps state coupling is a shorthand 
for an implicit sequence of gated 
causal i t ies between two statechanges. On 
the other hand, state coupling between two 
states, as opposed to statechanges, seems to 
be a concept which is independent of gated 
causality. To i l lustrate; "a nail through 
two pieces of wood" (state I) has to be 
regarded as state-coupled to "the pieces of 
wood are joined" (state 2, a descr ipt ion of 
the same situation, but at a different 
level): 
187 
~WOODI ,  WOOD2~ 
- _L_  
T 
 ooo , 
JOINED ~/ 
In this type of situation, the state 
coupling concept is required at this level 
to stop the representat ion of some sort of 
inexpl icable "micro-causal i ty" when it 
transcends the model's knowledge of the 
world. 
VI. ALTERNATIVE ACTION SELECTION 
In looking at devices and simple 
processes such as sawing a board in half, 
there have been few choices; the causal ity 
and enablement are in a sense already built 
in or strongly prescribed. In a real 
planning environment on the other hand, 
there will ordinar i ly be numerous 
alternat ive actions which could causal ly 
produce some desired goal state, providing 
all gating condit ions could be met. For 
example, if the goal of a planner is to 
produce a statechange in his location to 
some specif ied point, the various subplans 
of walking, driving a car, hitching a ride, 
bicycling, taking a plane, etc. all suggest 
themselves as potential ly relevant, some 
more than others. The one the planner 
actual ly selects will be a function of more 
than Just the relative costs of each 
alternative; the select ion will also relate 
to the inherent appl icabi l i ty,  or 
reasonableness of the plan, based on the 
sDecif%cs of where his dest ination is 
relative to his current location, weather 
conditions, etc. Of course, all the 
relevant factors could eventual ly be 
discovered by s imulat ing each alternat ive 
plan before choosing, watching out for 
undesirable or suboptimal events. For 
example, in s imulat ing the walking, 
h i tchhik ing or bicycl ing plans, the planner 
finds himself  outside for potent ia l ly  long 
durations. Hence, if it is raining, the 
cost is judged high. If the distance is 
less than a mile, or is indoors, s imulat ion 
of the airplane plan leads to some absurdly 
high costs and perhaps some unsolvable 
antagonisms. Certainly, a degree of such 
forward simulat ion must occur in planning; 
however, it seems that the process of 
select ing among alternative actions is, 
intuit ively, more unif ied than just a 
col lect ion of forward simulations. 
For this reason, the model of CSA's 
incorporates the notion of a selector, 
denoted by the construction: 
SEL is a place where heuristics, as well as 
forward simulat ion control can reside, The 
heurist ics test relevant dimensions 
(e.g. distance, weather conditions, etc.) of 
the context in which the state or 
statechange is being sought (either for 
execution of some larger plan, or for 
interpret ing what another might do in some 
context). Based on the outcomes of such 
tests, the SELector chooses one alternat ive 
action as most reasonable. Currently, the 
selector function is imagined to exist 
"outside" the CAS formalism as 'an 
unrestr icted program which runs and decides. 
Eventually, since it is one goal of the CSA 
formalism to be able to represent arbitrary 
decision processes (these are, after all, 
just other algorithms), the SELector 
function should simply reference other CSA's 
which carry out the heurist ic testing. In 
other words, defer the " intel l igence" in 
select ing an alternat ive at this level to 
unintel l igent CSA's at the next level, and 
SO on .  
VII. LEVELS OF RESOLUTIONS IN CSA'S 
The algor ithmic content of a CSA can be 
described at many different levels of 
resolution. For example, the "action" "take 
a plane to San Francisco" is quite a bit 
higher in level and more abstract than the 
action "grasp a saw". In the former, the 
act of taking a plane somewhere is not 
really an action at all, but rather a 
descr ipt ion of an entire set of actions, 
themselves related in a CSA; "take a plane 
to San Francisco" is a high level surrogate 
for a low level col lect ion of true actions 
in the sense of actual ly performing physical 
movements, etc. in the real world (things 
like grasping a saw, reaching into pants 
pocket for some money, and so on). 
Another example of resolut ion level 
dif ferences relates to how enabling states 
for actions are characterized. For example, 
in (A2) Abelson employs the primit ive (OKFOR 
object appl ication),  as in (OKFOR AUTO 
TRAVEL). The quest ion here is, what is the 
relat ionship between this high level 
descr ipt ion of OKness and the specif ics of 
what OKFOR means for any given object? That 
is, for a car, OKFOR means "gas in tank", 
"tires inflated", "battery charged",. . . ,  
whereas (OKFOR TOILET FLUSHING) means quite 
a different set of things. The basic issue 
is: should the memory plan and interpret in 
the abstract realm of OKFORedness, then 
instant iate with details later, or must the 
details serve as the primary planning basis, 
with the abstract ideas being reserved for 
other higher level processes such as 
reasoning by analogy, general izat ion and so 
forth? There is probably no cut-and-dr ied 
answer; however, the tendency in a CSA 
system would be to favor the details over 
the abstract. But the CSA representat ion is 
intended to be f lexible enough to accomodate 
both the abstract and the concrete. The 
idea of state coupling is an i l lustrat ion of 
this. 
188 
VIII. THE THEORY HAS ONLY JUST BEGUN 
A later version of this paper will 
contain more examples of the CSA, including 
its use in language context problems. The 
theory is by no means complete; to 
i l lustrate: 
(I) Is there such a thing as gated  
enablement? The answer seems to be 
"yes", since it seems reasonable to 
regard enablement as a flow which 
can be cut off in much the same way 
as causality. Perhaps an example 
of gated enablement i s  when the 
horses begin their race at the 
racetrack: the start gate's being 
open is a one-shot enablement for 
the horse to run, but only if the 
horse is in the box to start with! 
If he's not in the box, the gate's 
posit ion isn't relevant as an 
enablement to run; its flow is 
severed. 
(2) What kinds of time and sequencing 
information need to be incorporated 
in the formal ism? For example, 
causal i ty can be either abrupt or 
gradual: taking medicine for an 
ulcer p rov ides  a conceptual ly  
gradual statechange in the 
stomach's condition, whereas 
surgery provides a conceptual ly  
abrupt cure! This suggests the need 
for c lass i fy ing statechanges on 
some discrete conceptual  scale. 
Another inadequacy of the present 
model is its inabi l i ty to specify 
time sequencing; adoption of some 
tradit ional  f lowchart concepts will 
probably prove adequate for this. 
(3) There is no convenient way to model 
decis ion-making processes on the 
part of the planner of a CSA. This 
wil l  have to be developed. 
IX. APPLICATIONS OF THE CSA 
On the brighter side, the CSA provides 
a unif ied basis for problem-so lv ing-re lated 
cognit ive models. Specif ical ly,  I believe 
it shores up, under one basic data 
structure, the ideas presented in my own 
past research in conceptual  memory and 
inference (RI,R2) and in conceptual  overlays 
(R3) which suggests a meaning context 
mechanism for language comprehension based 
around CSA's. I want to conclude by l ist ing 
ant ic ipated appl icaions of CSA's. The 
appl icat ions have been divided into two 
categories: general (those which are central  
to some major theoret ical  issues in language 
understanding and problem-solving),  and 
specif ic (those which provide some local 
insights into memory organization).  
General ApPl icat ions 
I. As the basis for active Rroblem-solv~ng 
The CSA suppl ies an algor ithmic format 
wherein plans can be conceived, synthesized 
and executed. One immediate goal of 
I 
I 
I 
I 
I 
I 
I 
! 
I 
i 
I 
i 
I 
research should be to construct a 
commonsense algorithm1 interpreter which 
could "execute" the contents of portions of 
its own CSA memory in order to effect 
actions of moving about, communicating, and 
so forth. 
2. As the basis for conceptual inference 
In (RI), which describes a theory of 
conceptual memory and inference, sixteen 
classes of conceptual inference were 
identi f ied as the logical foundation of'a 
language-based meaning comprehension system. 
Interest ingly enough (but not surprising), 
nine of those inference classes correspond 
directly to traversals of CAS primitive 
links. In the theory  of (RI), every 
language stimulus, represented in conceptual 
form via Schank's conceptual dependency 
notation (S2), was subjected to a 
spontaneous expansion in "inference space" 
along the sixteen dimensions corresponding 
to the sixteen inference classes. Making an 
inference in that model corresponds to 
identi fying each perception as a step in one 
or more CSA's, then expanding outward from 
those points along the CSA links 
breadth-f irst.  Although there is certainly 
a class of more goal-directed conceptual 
inference, this kind of spontaneous 
expansion seems necessary to general 
comprehension, and the CSA is a natural 
formalism to use. The nine classes of 
inference which relate directly to CSA links 
are: 
I. causative 
2. resultat ive 
3. motivat ional  
4. enablement 
5. function 
6. enablement-predict ion 
7. missing enablement 
8. intervention 
9. act ion-predict ion 
3. As the basis for the conceptual 
representat ion o_~f language. 
A very large percentage of what people 
communicate deals with algorithms, the how 
and why of their act ivit ies in the world. 
Schank's conceptual dependency framework 
does a good job at representing rather 
complex utterances which reference 
underlying actions, states and statechanges. 
This theory of CSA's extends this framework 
to accomodate larger chunks of experience 
and language to begin dealing with 
paragraphs and stories instead of isolated 
sentences. 
4. As the basis for model in~ mechanisms 
Every man-made mechanism, as well as 
every natural ly-occurr ing biological system, 
is rich in algorithmic content. As 
i l lustrated in a previous example, CSA's can 
do a respectable job at character iz ing 
complex servo- and feedback mechanisms. It 
is not hard to envision the CSA as a basis 
for physiological  models in such an 
appl icat ion as medical diagnosis. Since all 
biological  systems are purposively 
constructed mechanisms in the evolut ionary 
189 
sense, representing such mechanisms in terms 
of causality, enablement, byproducts, 
thresholds, etc. is quite meaningful.  
5. As a basis for model ing dynamic 
meaning context i__nn language 
comprehension and general perception 
(R3) describes an expectancy-based 
system cal led "conceptual overlays" which 
can impose high-level, contextual 
interpretat ions on sentences by consult ing 
its algor ithmic base. In that paradigm, 
some stimuli (i.e. meaning graphs result ing 
from a conceptual parser " which receives 
language utterances as input) act ivate 
action overlays, while other stimuli fit 
into previously act ivated action overlays. 
Since an overlay is a col lection of pointers 
to CSA's in the algor ithmic base which have 
been predicted as l ikely to occur next, to 
"fit into" is to identify subsequent input 
as steps in the various algor ithms actors 
have been predicted to engage. For example, 
knowing what the sentence "John asked Mary 
for the keys" means contextual ly is quite a 
bit more simply understanding the "picture" 
this utterance el icits (its conceptual  
dependency representation).  If we know that 
John was hungry: 
John hadn't eaten in days. 
John asked Mary for the car keys. 
we activate an overlay which expects that 
John will engage CSA's which will a l leviate 
his inferred hunger; needing car keys fits 
nicely as a continuous enablement in several 
of these algorithms. Of course, the v i r tue  
of such a system is that it allows the 
high- level  interpretat ion of a sentence to 
change as a function of its contextual  
environment: 
John had some hamburger stuck 
in his teeth. 
John asked Mary for the car keys. 
Change the expectancies, and the 
interpretat ion changes! 
6. As ~he basis for 
computer a l~or l thm synthesis 
~nd 
Since a computer algor ithm is a 
relat ively direct ref lect ion of a 
programmer's internal model of an 
algor i thmic process, it seems reasonable 
that both the processes of synthesis and 
final implementat ion be represented in the 
same terms as his internal model. The 
present theory only suggests an approach; it 
is not yet adequate for general computer 
algorithms. But it seems that the idea of a 
CSA might be very relevant to recent 
research in the area of automatic 
programming, at least as a basis of 
representation. 
7. As ~ basis o__ff g self-model 
If a CSA interpreter  can indeed be 
defined, and if indeed the CSA can 
eventual ly capture any computer algorithm, 
then creating a self -model amounts to 
specifying the CSA interpreter in terms of 
CSA's. For example, an act of communicat ion 
amounts to the communicat ion of enough 
referential  information (features of 
objects, times, etc.) to enable the 
comprehender to identify, in his own model, 
the concepts being communicated. The 
how-to-communicate algor i thm which the CSA 
interpreter employs could itself be a CSA. 
8. As a basis for invest igat ion 
of a lgor i thm learning 
If we posit the existence of a small 
set of primit ive CSA links and make the 
assumption that these are either part of the 
brain's hardware, or learned impl ic i t ly  as 
soon as the intel lect begins perceiving, we 
have a basis from which to study how a child 
learns world dynamics. For example, how, 
and at what point, does the toddler know 
that he must grasp the cup in an act of 
continuous enablement before he can lift it 
to his mouth, and how does he know it must 
be at his mouth before he can successful ly 
drink? Perhaps algorithmic knowledge 
develops from random exper imentat ion within 
the syntactic constraints imposed by the set 
of CSA primitive links. 
Specif ic CSA Appl icat ions 
I. For represent ing the functions of objects 
As with mechanisms, any man-made object 
is made for a purpose. Translated to CSA's, 
this means that part of every 
purpos ive ly-constructed object's def init ion 
is a set of pointers into the algor i thm base 
to CSA's in which the object occurs. This 
is true for all objects from pencils, to 
furnaces, to window shades, to a bauble 
which provided its constructor amusement, to 
newspapers. An object in memory can be 
completely character ized (in the abstract) 
by a set of intr insic features (shape, size, 
color, etc) and this set of pointers to 
CSA's. 
2. For represent ing people's professions 
To say (ISA JOHNI PLUMBER) skirts what 
it means to be a plumber. Rather, to be a 
plumber means to engage plumbing algor i thms 
as a principal source of income. Thus, a 
profession can be defined by a set of 
pointers to the CSA's which are 
character ist ic  of that profession. This 
makes it possible to observe someone at work 
and identi fy his profession, to compare 
professions, etc.; these would not be 
possible if CSA's were not the basis of 
representat ion.  
3. For detect ing and explaining 
anomalous situations and potential ly 
antagonist ic  states 
A person notices a l icense plate yearly 
st icker on upside down; a person notices two 
fire engines approaching an intersection, 
rushing to a fire; at the intersection, one 
turns left, the other turns right; a person 
notices that the rain that morning will 
190 
interfere with the picnic plans that 
afternoon. How do such situations get 
judged "anomalous", and how does the 
perceiver try to explain or cope with them? 
The answer undoubtedly relates to 
expectancies and a knowledge of a lgor i thms 
for putt ing things on one-another, gett ing 
somewhere in a hurry and antagonist ic states 
when eating outdoors. By playing experience 
against CSA's we discover things which would 
not otherwise be discovered. 
4. For f i l l ing in missing information 
If a person is perceiving in a noisy or 
incomplete environment, having CSA's 
avai lable to guide his interpretat ions of 
perceptions provides enough momentum to fill 
in miss ing details, scarcely notic ing their 
absence. If John is hammering a nail into 
the wall with his hand on the backswing, but 
the object in his hand is occluded, it 
requires very l ittle effort to surmise that 
it is a hammer. If we believe that Mary is 
going to McDonald's  to buy a hamburger, but 
she comes back into the house saying "It 
won't start", we have a pretty good idea 
"it" refers to the car. This appl icat ion of 
CSA's corresponds to the notion of a 
speci f icat ion inference in (RI). 
X. CONCLUSIONS 
Instead of a conclusion, I wil l  s imply 
state the order in which research along CSA 
lines should, and hopeful ly w i l l  at the 
University of Maryland, progress: 
I. Reimplementat ion of the conceptual  
overlays prototype system described in (R3) 
to ref lect the new CSA ideas and replace the 
ad-hoc AND/OR graph approach descr ibed in 
that report. 
2. Implementat ion of a mechanism simulator 
which could accept, in CSA terms, the 
def init ion of a complex mechanism 
(electronic circuit or toilet), s imulate it, 
respond to art i f ic ia l ly - induced 
malfunct ions,  and answer questions about the 
mechanism's cause and effect structure. 
3. Engineer ing of a new total conceptual  
memory, along the lines of the original  one 
of (RI), but incorporatng CSA's and the new 
idea of a tendency. This would involve 
re implement ing the inference mechanism and 
various searchers. 
4. Development of a CSA interpreter which 
could not only use CSA's as data structures 
in the various cognit ive processes, but also 
could execute them to drive itself. 
5. Applying CSA's to medical  diagnosis and 
automatic programming. 
6. Invest igat ing the problem of story 
comprehension via conceptual overlays and 
CSA's. Perhaps also invest igat ing 
generat ion of stories (e.g. the story of 
the trip to McDonald's) or the generat ion of 
a descr ipt ion of a complex electronic 
circuit, encoded as a CSA, in layman's 
terms. 
XI. ACKNOWLEDGEMENTS 
My thanks to the members of the 
Commonsense Algorithm Study Group at the 
University of Maryland: Bob Eberlein, Milt 
Grinberg, Bob Kirby, Phil London and Tom 
Skillman. They have provided considerable 
intellectual stimulation. We hope to 
continue as a group and eventually issue a 
working paper and computer system. 
REFERENCES 
(AI) Abelson, R., "The Structure of Belief 
Systems," in Schank and Colby (eds.), 
Computer Models of Thought and 
Language, W.H. Freeman, 1973 
(A2) Abelson, R., "Frames for Understanding 
Social Actions," Paper for Carbonell 
Conference, Pajarro Dunes CA, May 1974. 
(CI) Charniak, E., "Toward a Model of 
Children's Story Comprehension," 
Doctoral dissertation, M.I.T., AI 
TR-266, 1972. 
(MI) Minsky, M., "A Framework for 
Representing Knowledge," M.I.T. AI 
TR-306, 1974. 
(NI) Nilsson, N., Probelm Solving Methods i__nn 
Artificial Intelligence, McGraw Hill, 
1971. 
(RI) Rieger, C., "Conceptual Memory: A 
Theory and Computer Program for 
Processing the Meaning Content of 
Natural Language Utterances," Doctoral 
dissertation, Stanford Univ. AI Memo 
233, 1974. 
(R2) Rieger, C., "Understanding by 
Conceptual Inference," American Journal 
of Computational Linguistics (in 
press), 1975. (Also available as Univ. 
of Maryland Technical Report #353) 
(R3) Rieger, C., "Conceptual Overlays: A 
Mechanism for the Interpretation of 
Sentence Meaning in Context," to appear 
in Proceedings 4IJCAI 1975. (Also 
available as Univ. of Maryland 
Technical Report #354) 
($I) Sacerdoti, E., "Planning in a Hierarchy 
of Abstraction Spaces," in Proceedings 
3IJCAI 1973. 
($2) Schank, R., "Identifications of 
Conceptualizations Underlying Natural 
Language", in Schank and Colby, 
Computer Models of Thought and 
Language, W.H. Freeman, 1973. 
($3) Schank, R., Goldman, N., Rieger, C., 
and Riesbeck, C., "Primitive Concepts 
Underlying Verbs of Thought," Stanford 
Univ. AI Memo 162, 1972. 
($4) Schmidt, C., and D'Addamio, J., "A 
Model of the Common-Sense Theory of 
Intention and Personal Causation," 
Proceedings 3IJCAI, 1973. 
191 
Figure ! 
Unrestricted AND/OR graph for getting a 
McDonald's hamburger into stomach. 
e.,~\e. 
Figure 2 
Hamburger algorithm, with actions, states, 
causality and enablement explicit. 
6~6~r o~ ?uo#T~ 
L~t. ~A~If, Q.: IvPPI.Y 
LtN~- "r~ TA~V~ 
tank from ~ / /  
trip _# float 
~j~ / ~ J  ~anolellf~ ~ arm 
supply II1 dis- _ ~ ~ _ ~ ,  channel valve Ill harge I I I~  pe 
supply |~ flush 
stewOer plpe~_ ValeV ~ 
i t r ip~ f'~,refi I l 
- %\ ]   t~i/i 
c 
TAN K 
FIGURE 3 
A reverse-trap toilet. 
, i m,  , 
ll!ft 
I "~ Over.fl ow 
' ~ ' - ' )  p, pe 
? ~flush ball 
K 
~Bk~6 c~c~ sutp6y Fuu~ ~F t~b 
V~#~. 
v~Lv~ 
Bo~L. 1"o w~rs~n~ 
G,'Z~W, T7 
~.  w ~,~J~ 
wWr~ F~ 
15owu 6J~TF.~ H?16.~ 
o~rr~ L~P ~F wksl~ 
FIGURE 4 
O~eration of the reverse-trap toilet. 
192 
I 
I 
I 
I 
I 
I _ _  
I 
I 
I 
I 
I 
I 
I 
I 
OF I,~o0 
I (oe,,.,,~. (Co~t~UouS 
J r~  , 
WooO 
'Figure 5 
_ _ . + 
Sawing a board in half to decrease its length. 
193 
I T~,~,~'~.~ ~ I T;~- ~, .  ' k /~ ? o - I - - - - - "  I , T~ ~ .  | 
I 
I 
I 
COMBUSTION 
, I 
I 
| 
~~'"  ~ 
! 
! 
I 
FORGETFULNESS ! 
FIGURE 6 
Vicious Cycles. 
! 
I 
194 ! 
ts ~j  /tc 
2,? I~ 
~F-.O 
Nf.x.T P~ 
t~E~T TA~LF_ 
DJVII>( ,4c~. J ~c Z. 
l 
l l l l  
FIGURE 7 
Computer algorithm to compute the 
average of TABLE(1),...,TABLE(N) 
expressed as a commonsense algorithm. 
(NOTE: Initialization has not been shown. The assumptions are 
that AC3 begins with zero, that ACI begins with zero, 
and that N and TABLE(1),...,TABLE(N) exist in core.) 
