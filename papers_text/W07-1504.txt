Proceedings of the Linguistic Annotation Workshop, pages 25?32,
Prague, June 2007. c?2007 Association for Computational Linguistics
Associating Facial Displays with Syntactic Constituents for Generation
Mary Ellen Foster
Informatik VI: Robotics and Embedded Systems
Technical University of Munich
Boltzmannstra?e 3, 85748 Garching, Germany
foster@in.tum.de
Abstract
We present an annotated corpus of conversa-
tional facial displays designed to be used for
generation. The corpus is based on a record-
ing of a single speaker reading scripted out-
put in the domain of the target generation
system. The data in the corpus consists of
the syntactic derivation tree of each sentence
annotated with the full syntactic and prag-
matic context, as well as the eye and eye-
brow displays and rigid head motion used
by the the speaker. The behaviours of the
speaker show several contextual patterns,
many of which agree with previous findings
on conversational facial displays. The cor-
pus data has been used in several studies ex-
ploring different strategies for selecting fa-
cial displays for a synthetic talking head.
1 Introduction
An increasing number of systems designed to au-
tomatically generate linguistic and multimodal out-
put now make use of corpora to help in decision-
making (cf. Belz and Varges, 2005). Some imple-
mentations use corpora to help select output that is
grammatical or fluent; for example, Langkilde and
Knight (1998) and White (2006) both used n-gram
language models to guide stochastic surface realis-
ers. In other systems, corpora are used to make
decisions based on pragmatic factors such as the
reading level of the target user (Williams and Re-
iter, 2005) or the visual features of an object be-
ing described (Cassell et al, 2007). The latter type
of domain-specific contextual information is not of-
ten included in generally-available corpora. For this
reason, developers of generation systems that need
this type of information often create and make use
of application-specific corpora.
The easiest method of including the necessary
pragmatic information in a corpus is to base the cor-
pus on output generated in situations where the con-
textual factors are known; this eliminates the need to
annotate these factors explicitly. Stone et al (2004),
for example, created a multimodal corpus based on
the voice and body language of an actor performing
scripted output in the domain of the target genera-
tion system: an animated instructor character for a
snowboarding video game. The contextual informa-
tion in the corpus scripts included the move that the
player attempted in the game and the result of that
attempt. Similarly, van Deemter et al (2006) cre-
ated a corpus of multimodal referring expressions
produced in specific pragmatic contexts and used it
to compare several referring-expression generation
algorithms to human performance.
In this work, the task is to select facial displays
for an animated talking head to use while present-
ing output in the COMIC multimodal dialogue sys-
tem (Foster et al, 2005), which generates spoken
descriptions and comparisons of bathroom-tile op-
tions. The output of the COMIC text planner in-
cludes a range of information in addition to the text:
the syntactic derivation tree, the user?s evaluation
of the object being described, the information sta-
tus (new or old, contrastive) of each fact described,
and the predicted speech-synthesiser prosody. All of
this contextual information can be used to help select
25
appropriate facial displays to accompany the spo-
ken presentation; however?as in the other systems
mentioned above?this requires a corpus where the
full context for every facial display is known. To cre-
ate such a corpus, we recorded a speaker performing
scripted output in the domain of COMIC.
This paper is arranged as follows. In Section 2,
we first describe how the scripts for the corpus were
created and how the recording was made. Section 3
then presents the annotation scheme and the tool that
was used to perform the annotation, while Section 4
describes the measures that were taken to ensure that
the annotation was reliable. Section 5 then sum-
marises the high-level patterns that were found in the
displays annotated in the corpus and compares them
to other findings on conversational facial displays.
At the end of the section, we use the corpus data to
test two assumptions that were made in the annota-
tion scheme. After that, in Section 6, we describe
several experiments in which different methods of
using the data in this corpus to select facial displays
for a synthetic head have been compared. Finally,
in Section 7, we summarise the contributions of this
paper and draw some conclusions about the useful-
ness of this corpus for its intended task.
2 Recording
For this corpus, we recorded a single speaker read-
ing a set of 444 scripted sentences in the domain of
the COMIC multimodal dialogue system. The sen-
tences were generated by the full COMIC output-
generation process, which uses the OpenCCG sur-
face realiser (White, 2006) to create texts includ-
ing prosodic specifications for the speech synthe-
siser and incorporates information from the dialogue
history and a model of the user?s likes and dislikes.
Every node in the OpenCCG derivation tree for
each sentence in the script was initially annotated
with all of the available syntactic and pragmatic in-
formation from the output planner, including the fol-
lowing features:
? The user-model evaluation of the object being
described (positive or negative);
? Whether the fact being presented was previ-
ously mentioned in the discourse (as I said be-
fore, . . . ) or is new information;
?Although it's in the family style, the tiles are by Alessi Tiles.?
? although it's in the family style? although? it's in the family style? it? 's in the family style? 's? in the family style? in? the family style? the? family style? family? style
? the tiles are by Alessi Tiles? the tiles? the? tiles? are by Alessi Tiles? are? by Alessi Tiles? Alessi Tiles
User model: badClause: first
User model: goodClause: second
Accent: L+H*
Accent: H*
Figure 1: Annotated OpenCCG derivation tree
? Whether the fact is explicitly compared or con-
trasted with a feature of the previous tile design
(once again . . . but here . . . );
? Whether the node is in the first clause of a two-
clause sentence, in the second clause, or is an
only clause;1
? The surface string associated with the node;
? The surface string, with words replaced by se-
mantic classes or stems drawn from the gram-
mar (e.g., this design is classic becomes this
[mental-obj] be [style]); and
? Any pitch accents specified by the text planner.
Figure 1 illustrates the annotated OpenCCG
derivation tree for a sample sentence drawn from
the recording script. The annotations indicate that
every node in the first half of this sentence is associ-
ated with a negative user-model evaluation and is in
the first clause of a two-clause sentence, while every
node in the second half is linked to a positive eval-
uation and is in the second clause of the sentence.
The figure also shows the pitch accents selected by
the output planner according to Steedman?s (2000)
theory of information structure and intonation.
For the recording, the sentences in the script were
presented one at a time to the speaker; the presen-
1No sentence in the script had more than two clauses.
26
tation included both the linguistic content (with ac-
cented words highlighted) as well as the intended
pragmatic context. Each sentence was displayed in
a large font on a laptop computer directly in front
of the speaker, with the camera positioned directly
above the laptop to ensure that the speaker was look-
ing towards the camera at all times. The speaker was
instructed to read each sentence out loud as expres-
sively as possible into the camera.
3 Annotation
Once all of the sentences in the script had been
recorded as described in the preceding section, the
next step was to annotate the facial displays that oc-
curred. We first used Anvil (Kipp, 2004) to split
the video into individual clips corresponding to each
sentence. This section describes how the facial dis-
plays in each of the clips were then annotated.
3.1 Annotation scheme
We annotated the speaker?s facial displays by linking
each to the span of nodes in the OpenCCG derivation
tree with which it was temporally related. Making
cross-modal links at this level made it possible to
use the annotated information directly in the output-
generation process for the experiments described in
Section 6.
A display was associated with the full span of
words that it coincided with temporally, as follows.
If a single node in the derivation tree covered ex-
actly all of the relevant words, then the annotation
was placed on that node; if the words spanned by a
display did not coincide with a single node, it was
attached to the set of nodes that did span the neces-
sary words. For example, in the derivation shown in
Figure 1, the sequence the family style is associated
with a single node, so a motion temporally associ-
ated with that sequence would be attached to that
node. On the other hand, if there were a motion as-
sociated with the tiles are, it would be attached to
both the the tiles node and the are node.
The following were the features that were consid-
ered; for each feature, we note the corresponding
Action Unit (AU) from the well-known Facial Ac-
tion Coding System (Ekman et al, 2002).
? Eyebrows: up (AU 1+2) or down (AU 4)
? Eye squinting (AU 43)
Figure 2: Annotation tool
? Head nodding: up (AU 53) or down (AU 54)
? Head leaning: left (AU 55) or right (AU 56)
? Head turning: left (AU 57) or right (AU 58)
This set of displays was chosen based on a combi-
nation of three factors: the emphatic facial displays
documented in the literature, the capabilities of the
target talking head, and the actual displays of the
speaker during the recording session.
3.2 Annotation tool
The tool for the annotation was a custom-written
program that allowed the coder to play back a
recorded sentence at full speed or slowed down, and
to associate any combination of displays with any
node or set of nodes in the OpenCCG derivation tree
of the sentence. The tool also allowed the coder to
play back a proposed annotation sequence on a syn-
thetic talking head to verify that it was as close as
possible to the actual motions. Figure 2 shows a
screenshot of the annotation tool in use on the sen-
tence from Figure 1. In the screenshot, a left turn is
attached to the entire sentence (i.e., the root node),
while a series of nods is associated with single leaf
nodes in the first half of the sentence. The annotator
has already attached a brow raise to the word are in
the second half and is in the process of adding a nod
to the same word.
The output of the annotation tool is an XML doc-
ument including the original contextually-annotated
27
<node surf=" although it ?s in the family style the tiles are by Alessi_Tiles" LEAN="left"
sc=" although [pro3n] be in the [style] [abstraction] the [phys -obj] be by [manufacturer ]">
<node surf=" although it ?s in the family style" um="b" first ="y"
sc=" although [pro3n] be in the [style] [abstraction ]">
<node surf=" although" um="b" first ="y" NOD="down" />
<node surf="it ?s in the family style" um="b" first ="y"
sc="[ pro3n] be in the [style] [abstraction ]">
<node surf="it" stem=" pro3n" um="b" first ="y" NOD="down" />
<node surf="?s in the family style" um="b" first ="y" sc="be in the [style] [abstraction ]">
<node surf="?s" stem="be" um="b" first ="y" NOD="down" />
<node surf="in the family style" um="b" first ="y" sc="in the [style] [abstraction ]">
<node surf="in" um="b" first ="y" NOD="down" />
<node surf="the family style" um="b" first ="y" sc="the [style] [abstraction ]">
<node surf="the" um="b" first ="y" />
<node surf=" family style" um="b" first ="y" sc="[ style] [abstraction ]">
<node surf=" family" sc="[ style]" accent ="L+H*" um="b" first ="y" NOD="down" />
<node surf="style" sc="[ abstraction ]" um="b" first ="y" />
</node >
</node >
</node >
</node >
</node >
</node >
<node surf="the tiles are by Alessi_Tiles" um="g" first ="n"
sc="the [phys -obj] be by [manufacturer ]">
<node surf="the tiles" um="g" first ="n" sc="the [phys -obj]">
<node surf="the" um="g" first ="n" />
<node surf="tiles" sc="[phys -obj]" stem="tile" um="g" first ="n" />
</node >
<node surf="are by Alessi_Tiles" um="g" first ="n" sc="be by [manufacturer ]">
<node surf="are" stem="be" accent ="H*" um="g" first ="n" BROW="up" NOD="down" />
<node surf="by Alessi_Tiles" um="g" first ="n" sc="by [manufacturer ]">
<node surf="by" um="g" first ="n" />
<node surf=" Alessi_Tiles" sc="[ manufacturer ]" accent ="H*" um="g" first ="n" />
</node >
</node >
</node >
</node >
Figure 3: Annotated sentence from the corpus
OpenCCG derivation tree of each sentence, with
each node additionally labelled with a (possibly
empty) set of facial displays. Figure 3 shows the
fully-annotated version of the sentence from Fig-
ure 1. This document includes the contextual fea-
tures from the original tree, indicated by italics: ev-
ery node in the first subtree has um="b" and first="y",
while every node in the second subtree has um="g"
and first="n", while the accented items also have an
accent feature. Every node also specifies the string
generated by the subtree that it spans, both in its sur-
face form (surf) and with semantic-class and stem
replacement (sc). This tree also includes the facial
displays added by the coder in Figure 2, indicated
by underlining: (LEAN="left") attached to the root
node), a number of downward nods (NOD="down") on
individual words in the first half of the sentence, and
a nod accompanied by a brow raise (BROW="up") on
are near the end.
4 Reliability of the annotation
Several measures were taken to ensure that the an-
notation process was reliable. As the first step, two
independent coders each separately processed the
same set of 20 sentences, using an initial annotation
scheme. The outputs of these two coders were com-
pared, and the coders discussed the differences and
agreed on a revised scheme. One of these coders
then used the final scheme to process the entire set
of 444 sentences. As a further test of reliability, an
28
additional coder was instructed on the use of the an-
notation tool and scheme and used them to process
286 sentences (approximately 65% of the corpus).
To assess the degree of agreement between these
two coders, we used a version of the ? agreement
coefficient proposed by Artstein and Poesio (2005).
? is designed as a coefficient that is weighted, that
applies to multiple coders, and that uses a separate
probability distribution for each coder. Weighted
coefficients like ? permit degrees of agreement to
be measured, so that partial agreement is penalised
less severely than total disagreement. Like other
weighted coefficients, ? is based on the ratio be-
tween the observed and expected disagreement on
the corpus.
To use this coefficient, it is necessary to define
a measure that computes the distance between two
proposed annotations. In this case, to compute the
observed disagreement Do(S) on a sentence S, we
use a measure similar to that proposed by Passon-
neau (2004) for measuring agreement on set-valued
annotations. For each display proposed by each
coder on the sentence, we search for a correspond-
ing display proposed by the other coder?one with
the same value (e.g., a brow raise) and covering a
similar span of nodes. If both proposed exactly the
same display, that indicates no disagreement (0); if
one display covers a strict subset of the nodes cov-
ered by the other, that indicates minor disagreement
(13 ); if the nodes covered by the two proposals over-
lap, that is a more major disagreement (23 ); and if no
corresponding display can be found from the second
coder, then that indicates the maximum level of dis-
agreement (1). The total observed disagreement on
a sentence is the sum of the disagreement level for
each display proposed by each coder.
The expected disagreement De(S) for a sentence
S depends on the length of that sentence, as fol-
lows. We first use the corpus counts to compute
the probability of each coder assigning each pos-
sible facial display to word spans of all possible
lengths. We then use these probabilities to estimate
the likelihood of the two coders assigning identical,
super/subset, overlapping, or disjoint annotations to
the sentence, for each possible display. The total
expected disagreement for the sentence is the sum
of these probabilities across all displays, using the
same weights as the observed disagreement above.
The overall observed disagreement in the corpus
Do is the arithmetic mean of the disagreement on
each sentence; similarly, the overall expected dis-
agreement De is the mean of the expected disagree-
ment across all of the sentences. To compute the
value of ? for the output of the two coders, we sub-
tract the ratio of these two values from 1:
? = 1?
Do
De
As Artstein and Poesio (2005) point out, for
weighted measures such as ?, there is no signif-
icance test for agreement, and the actual value is
strongly affected by the distance metric that is se-
lected. However, ? values can be compared with
one another to assess degrees of agreement. The
overall ? value between the two coders on the full
set of 286 sentences processed by both was 0.561,
with ? values on individual facial displays ranging
from a high of 0.661 on nodding to a low of 0.285
on squinting (a very rare motion). To put these val-
ues into context, we computed ? on the set of 20
sentences processed by the final coder as part of the
training process (which are not included in the set
of 286). The overall ? value for these sentences is
0.231, with negative values for some of the individ-
ual displays. This demonstrates that the training pro-
cess had a positive effect on agreement.
5 Patterns in the corpus
We investigated the contextual features to see which
had the most significant effect on the facial displays
occurring on a node. To determine this, we used
multinomial logit regression to select the factors and
factor interactions that had the most significant ef-
fects on the distribution of each display; this form of
regression is appropriate when, as in this case, the
response variable is categorical. In this section, we
list the most significant factors and give a qualitative
description of the impact of each.
The single most influential contextual factor was
the user-model evaluation, which had an effect on all
of the facial displays. In positive user-model con-
texts, eyebrow raising and turning to the right were
relatively more frequent (Figure 4(a)); in negative
contexts, on the other hand, the rates of eyebrow
lowering, squinting, and leaning to the left were all
higher (Figure 4(b)). Other factors also affected the
29
(a) Positive (b) Negative
Figure 4: Characteristic facial displays for different user-model evaluations
distribution of facial displays. In the first half of
two-clause sentences, brow lowering was also more
frequent, as was upward nodding, while downward
nodding and right turns showed up more often in the
second clause of two-clause sentences. Nodding and
brow raising were both more frequent on nodes with
any sort of predicted pitch accent.
Several of these factors agree with previous find-
ings on conversational body language. The in-
creased frequency of nodding and brow raising on
accented words agrees with many previous stud-
ies: Ekman (1979), Cav? et al (1996), Graf et al
(2002), Keating et al (2003), Krahmer and Swerts
(2004), and Flecha-Garc?a (2006) all noted similar
displays on prosodically accented parts of the sen-
tence. The speaker?s tendency to move right on pos-
itive descriptions and left on negative descriptions
is also consistent with other findings. According
to the work of Davidson and colleagues (Davidson
and Irwin, 1999), emotion and affect processing are
asymmetrically organised in the human brain. The
right hemisphere is associated with negative affect
(and withdrawal behaviours), and the left with posi-
tive affect (and approach behaviours). Because both
perceptual and motor systems are contra-laterally or-
ganised, this means that higher levels of right hemi-
sphere activity are associated with attention being
oriented towards the left, while higher levels of left
hemisphere activity are associated with attention be-
ing oriented to the right; this fits with our speaker?s
pattern of movements.
The annotation scheme described here allowed a
display to be associated with any contiguous span of
words in the sentence. Annotators were encouraged
to use syntactic constituents wherever possible, but
also had the option to select multiple nodes where a
display did not correspond with a single constituent
in the derivation tree. Earlier versions of the annota-
tion scheme did not support this degree of flexibility,
so we used the patterns in the corpus to test whether
the modifications to the scheme were useful.
In a previous study using the same video record-
ings but a different, simpler scheme (Foster and
Oberlander, 2006), facial displays could only be as-
sociated with single leaf nodes (i.e., words); that is,
in the terminology of Ekman (1979), all motions
were considered to be batons rather than underlin-
ers. Based on the data in the current corpus, that
restriction was clearly unrealistic: the mean number
of nodes spanned by a display in the full corpus was
1.95, with a maximum of 15 and a standard devia-
tion of 2. The results were similar in the sub-corpus
produced by the final coder, in which the mean num-
ber of nodes spanned by a display was 2.25.
The annotation rules for this study did not ini-
tially permit displays to be associated with more
than nodes in the derivation tree. This capability
was added following inter-coder discussions after
the initial test annotation to deal with cases where
the speaker?s displays did not correspond to syntac-
tic constituents?for example, if the speaker raised
his eyebrows on the tiles are or some other such
non-standard constituent. The data in the annotated
corpus supports this modification. Approximately
6% of the annotations in the main corpus?165 of
2826?were attached to more than one node in the
derivation tree; for the final coder, 4.5% of annota-
tions were on multiple nodes.
30
6 Generation experiments
The primary reason for creating this corpus of fa-
cial displays was to use the resulting data to select
facial displays for the artificial talking head in the
COMIC multimodal dialogue system. Several dif-
ferent strategies have been implemented to use the
corpus data for this task, and a number of automated
and human evaluations have been carried out com-
paring the different implementations.
As described in the preceding section, the fac-
tor with the largest influence on the displays of
the recorded speaker was the user-model evaluation.
Two studies (Foster, 2007b) were carried out to test
the generality of the characteristic positive and neg-
ative displays (Figure 4). In the first study, users
were asked to identify the intended user-model po-
larity of a description presented by the talking head
based only on the facial displays. The participants
were generally able to recognise the characteristic
positive and negative facial displays; they also iden-
tified the displays intended to be neutral (nodding
alone) as positive, and tended to judge videos with
no facial displays to be negative. In the second study,
users? subjective preferences were gathered between
videos in which the user-model evaluation expressed
in speech was either consistent or inconsistent with
the facial displays. In this study, the participants
generally preferred the videos that showed consis-
tent content on the two output channels.
In another study (Foster and Oberlander, 2007),
two different data-driven strategies were imple-
mented that used the corpus data to select facial dis-
plays to accompany speech. One strategy always se-
lected the highest-probability option in all contexts,
while the other made a stochastic choice among all
of the options weighted by the corpus probabili-
ties. These two strategies were compared against
each other using both automated and human eval-
uation methods: the majority strategy scored more
highly on the automated cross-validation, while the
weighted strategy was strongly preferred by human
judges. The judges also preferred resynthesised ver-
sions of the original facial displays from the corpus
to the output of either of the generation strategies.
Two further human evaluation studies compared
the weighted data-driven generation strategy from
the preceding study to a rule-based strategy that
selected the most characteristic displays based
only on the user-model evaluation (Foster, 2007a).
When users? subjective judgements were gathered
as above, they had a mild preference for the out-
put of the weighted strategy over that of the rule-
based strategy. In a second study, videos generated
by the weighted strategy significantly decreased par-
ticipants? ability to select descriptions that were cor-
rectly tailored to a given set of user preferences,
while videos generated by the rule-based strategy
had no such impact.
7 Conclusions
We have described the collection and annotation of
an application-specific corpus of conversational fa-
cial displays. The designs of both the corpus and
the annotation scheme were driven by the needs of
a specific generation system, which makes use of a
range of pragmatic information while creating out-
put. To use this information to make corpus-based
decisions, it is necessary that the full context of ev-
ery utterance and facial display in the corpus be
available. Rather than adding this information to an
existing corpus, we chose?like Stone et al (2004)
and van Deemter et al (2006), for example?to cre-
ate a corpus based on known contexts so that the
full information for every sentence was known be-
fore the fact.
The final annotation scheme required each facial
display to be linked to the set of nodes in the syntac-
tic derivation tree of the sentence that exactly cov-
ered the words temporally associated with the dis-
play. Two coders separately processed the sentences
in the corpus; on the sentences processed by both
coders (about 65% of the corpus), the agreement as
measured by ? was 0.561.
A number of contextual factors had an influ-
ence on the displays used by the recorded speaker.
The single most influential factor was the user-
model evaluation of the object being described.
The speaker?s characteristic side-to-side motions on
these sentences agree with findings on the relation-
ship between brain hemispheres and affect. In ad-
dition, in user studies, human judges were reliably
able to identify the intended affect based on resyn-
thesised versions of these characteristic displays.
Other patterns in the data also agree with exist-
31
ing findings on facial displays: for example, the
speaker tended to nod and raise his eyebrows more
frequently on words with prosodic accents.
Several experiments have been performed in
which the annotated data from this corpus was used
to select the facial displays to accompany the out-
put of an animated talking head. These studies have
found interesting results on both the relationship be-
tween automated and human judgements of output
quality and the relative utility of rule-based and data-
driven approaches for selecting conversational facial
displays.
Acknowledgements
This research was supported by the EU projects
COMIC (IST-2001-32311) and JAST (FP6-003747-
IP). Thanks to Amy Isard, Ron Petrick, and Tom
Segler for annotation assistance, and to Jon Ober-
lander and the LAW reviewers for useful comments.
References
R. Artstein and M. Poesio. 2005. Kappa3 = alpha (or beta).
Technical Report CSM-437, University of Essex Department
of Computer Science.
A. Belz and S. Varges, editors. 2005. Corpus Linguistics 2005
Workshop on Using Corpora for Natural Language Genera-
tion. http://www.itri.brighton.ac.uk/ucnlg/ucnlg05/.
J. Cassell, S. Kopp, P. Tepper, K. Ferriman, and K. Striegnitz.
2007. Trading spaces: How humans and humanoids use
speech and gesture to give directions. In T. Nishida, edi-
tor, Engineering Approaches to Conversational Informatics.
Wiley. In press.
C. Cav?, I. Gua?tella, R. Bertrand, S. Santi, F. Harlay, and R. Es-
pesser. 1996. About the relationship between eyebrowmove-
ments and F0 variations. In Proceedings of the 4th Interna-
tional Conference on Spoken Language Processing (ICSLP
1996).
R. J. Davidson and W. Irwin. 1999. The functional neu-
roanatomy of emotion and affective style. Trends in Cog-
nitive Sciences, 3(1):11?21. doi:10.1016/S1364-6613(98)
01265-0.
K. van Deemter, I. van der Sluis, and A. Gatt. 2006. Building
a semantically transparent corpus for the generation of refer-
ring expressions. In Proceedings of the Fourth International
Natural Language Generation Conference, pages 130?132.
Sydney, Australia. ACL Anthology W06-1420.
P. Ekman. 1979. About brows: Emotional and conversational
signals. In M. von Cranach, K. Foppa, W. Lepenies, and
D. Ploog, editors, Human Ethology: Claims and limits of a
new discipline. Cambridge University Press.
P. Ekman, W. V. Friesen, and J. C. Hager. 2002. Facial Action
Coding System. A Human Face, Salt Lake City.
M. L. Flecha-Garc?a. 2006. Eyebrow raising in dialogue:
Discourse structure, utterance function, and pitch accents.
Ph.D. thesis, Department of Theoretical and Applied Lin-
guistics, University of Edinburgh.
M. E. Foster. 2007a. Comparing rule-based and data-driven se-
lection of facial displays. In Proceedings of the ACL 2007
Workshop on Embodied Language Processing.
M. E. Foster. 2007b. Generating embodied descriptions tailored
to user preferences. In submission.
M. E. Foster and J. Oberlander. 2006. Data-driven generation
of emphatic facial displays. In Proceedings of the 11th Con-
ference of the European Chapter of the Association for Com-
putational Linguistics (EACL 2006), pages 353?360. Trento,
Italy. ACL Anthology E06-1045.
M. E. Foster and J. Oberlander. 2007. Corpus-based generation
of conversational facial displays. In submission.
M. E. Foster, M. White, A. Setzer, and R. Catizone. 2005. Mul-
timodal generation in the COMIC dialogue system. In Pro-
ceedings of the ACL 2005 Demo Session. ACL Anthology
W06-1403.
H. Graf, E. Cosatto, V. Strom, and F. Huang. 2002. Visual
prosody: Facial movements accompanying speech. In Pro-
ceedings of the 5th IEEE International Conference on Auto-
matic Face and Gesture Recognition (FG 2002), pages 397?
401. doi:10.1109/AFGR.2002.1004186.
P. Keating, M. Baroni, S. Mattys, R. Scarborough, and A. Al-
wan. 2003. Optical phonetics and visual perception of lexi-
cal and phrasal stress in English. In Proceedings of the 15th
International Congress of Phonetic Sciences (ICPhS), pages
2071?2074.
M. Kipp. 2004. Gesture Generation by Imitation - From Hu-
man Behavior to Computer Character Animation. Disserta-
tion.com.
E. Krahmer and M. Swerts. 2004. More about brows: A cross-
linguistic study via analysis-by-synthesis. In C. Pelachaud
and Z. Ruttkay, editors, From Brows to Trust: Evaluating
Embodied Conversational Agents, pages 191?216. Kluwer.
doi:10.1007/1-4020-2730-3_7.
I. Langkilde and K. Knight. 1998. The practical value of n-
grams in generation. In Proceedings of the 9th International
Natural Language Generation Workshop (INLG 1998). ACL
Anthology W98-1426.
R. J. Passonneau. 2004. Computing reliability for coreference
annotation. In Proceedings, Fourth International Conference
on Language Resources and Evaluation (LREC 2004), vol-
ume 4, pages 1503?1506. Lisbon.
M. Steedman. 2000. Information structure and the syntax-
phonology interface. Linguistic Inquiry, 31(4):649?689.
doi:10.1162/002438900554505.
M. Stone, D. DeCarlo, I. Oh, C. Rodriguez, A. Lees, A. Stere,
and C. Bregler. 2004. Speaking with hands: Creating
animated conversational characters from recordings of hu-
man performance. ACM Transactions on Graphics (TOG),
23(3):506?513. doi:10.1145/1015706.1015753.
M. White. 2006. Efficient realization of coordinate struc-
tures in Combinatory Categorial Grammar. Research on
Language and Computation, 4(1):39?75. doi:10.1007/
s11168-006-9010-2.
S. Williams and E. Reiter. 2005. Deriving content selection
rules from a corpus of non-naturally occurring documents
for a novel NLG application. In Belz and Varges (2005).
32
