Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 1?6,
Avignon, France, April 23 - 24 2012. c?2012 Association for Computational Linguistics
Visualization of Linguistic Patterns
and
Uncovering Language History from Multilingual Resources
Miriam Butt1 Jelena Prokic?2 Thomas Mayer2 Michael Cysouw3
1Department of Linguistics, University of Konstanz
2Research Unit Quantitative Language Comparison, LMU Munich
3Research Center Deutscher Sprachatlas, Philipp University of Marburg
1 Introduction
The LINGVIS and UNCLH (Visualization of Lin-
guistic Patterns & Uncovering Language His-
tory from Multilingual Resources) were originally
conceived of as two separate workshops. Due to
perceived similarities in content, the two work-
shops were combined and organized jointly.
The overal aim of the joint workshop was to
explore how methods developed in computational
linguistics, statistics and computer science can
help linguists in exploring various language phe-
nomena. The workshop focused particularly on
two topics: 1) visualization of linguistic patterns
(LINGVIS); 2) usage of multilingual resources in
computational historical linguistics (UNCLH).
2 LINGVIS
The overall goal of the first half of the work-
shop was to bring together researchers work-
ing within the emerging subfield of computa-
tional linguistics ? using methods established
within Computer Science in the fields of Infor-
mation Visualization (InfoVis) and Visual Ana-
lytics in conjunction with methodology and anal-
yses from theoretical and computational linguis-
tics. Despite the fact that statistical methods for
language analysis have proliferated in the last
two decades, computational linguistics has so far
only marginally availed itself of techniques from
InfoVis and Visual Analytics (e.g., Honkela et
al. (1995); Neumann et al (2007); Collins et
al. (2009); Collins (2010); Mayer et al (2010a);
Mayer et al (2010b); Rohrdantz et al (2011)).
The need to integrate methods from InfoVis and
Visual Analytics arises particularly with respect
to situations in which the amount of data to be
analyzed is huge and the interactions between rel-
evant features are complex. Both of these situ-
ations hold for much of current (computational)
linguistic analysis. The usual methods of sta-
tistical analysis do not allow for quick and easy
grasp and interpretation of the patterns discovered
through statistical processing and an integration
of innovative visualization techniques has become
imperative.
The overall aim of the first half of the workshop
was thus to draw attention to this need and to the
newly emerging type of work that is beginning to
respond to the need. The workshop succeeded in
bringing together researchers interesting in com-
bining techniques and methodology from theo-
retical and computational linguistics with InfoVis
and Visual Analytics.
Three of the papers in the workshop focused
on the investigation and visualization of lexical
semantics. Rohrdantz et al present a diachronic
study of fairly recently coined derivational suf-
fixes (-gate, -geddon, -athon) as used in newspa-
per corpora across several languages. Their anal-
ysis is able to pin-point systematic differences in
contextual use as well as some first clues as to
how and why certain new coinages spread bet-
ter than others. Heylen et al point out that me-
thods such as those used in Rohrdantz et al,
while producing interesting results, are essentially
black boxes for the researchers ? it is not clear
exactly what is being calculated. Their paper
presents some first steps towards making the black
box more transparent. In particular, they take
a close look at individual tokens and their se-
mantic use with respect to Dutch synsets. Cru-
cially, they anticipate an interactive visualization
that will allow linguistically informed lexicogra-
1
phers to work with the available data and patterns.
A slightly different take on synset relations is pre-
sented by Lohk et al, who use visualization me-
thods to help identify errors in WordNets across
different languages.
Understanding differences and relatedness be-
tween languages or types of a language is the sub-
ject of another three papers. Littauer et al use
data from the WALS (World Atlas of Language
Structures; Dryer and Haspelmath (2011)) to
model language relatedness via heat maps. They
overcome two difficulties: one is the sparseness
of the WALS data; another is that WALS does
not directly contain information about possible ef-
fects of language contact. Littauer et al attempt
to model the latter by taking geographical infor-
mation about languages into account (neighboring
languages and their structure). A different kind
of language relatedness is investigated by Yan-
nakoudakis et al, who look at learner corpora and
develop tools that allow an assessment of learner
competence with respect to various linguistic fea-
tures found in the corpora. The number of rel-
evant features is large and many of them are in-
terdependent or interact. Thus, the amount and
complexity of the data present a classic case of
complex data sets that are virtually impossible to
analyze well without the application of visualiza-
tion methods. Finally, Lyding et al take academic
texts and investigate the use of modality across
academic registers and across time in order to
identify whether the academic language used in
different subfields (or adjacent fields) of an aca-
demic field has an effect on the language use of
that field.
3 UNCLH
The second half of the workshop focused on
the usage of multilingual resources in computa-
tional historical linguistics. In the past 20 years,
the application of quantitative methods in his-
torical linguistics has received increasing atten-
tion among linguists (Dunn et al, 2005; Heg-
garty et al, 2010; McMahon and McMahon,
2006), computational linguists (Kondrak, 2001;
Hall and Klein, 2010) and evolutionary anthropol-
ogists (Gray and Atkinson, 2003). Due to the ap-
plication of these quantitative methods, the field
of historical linguistics is undergoing a renais-
sance. One of the main problems that researchers
face is the limited amount of suitable compara-
tive data, often falling back on relatively restricted
?Swadesh type? wordlists. One solution is to use
synchronic data, like dictionaries or texts, which
are available for many languages. For example,
in Kondrak (2001), vocabularies of four Algo-
nquian languages were used in the task of au-
tomatic cognate identification. Another solution
employed by Snyder et al (2010) is to apply a
non-parametric Bayesian framework to two non-
parallel texts in the task of text deciphering. Al-
though very promising, these approaches have so
far only received modest attention. Thus, many
questions and challenges in the automatization
of language resources in computational historical
linguistics remain open and ripe for investigation.
In dialectological studies, there is a long tra-
dition, starting with Se?guy (1971), in which lan-
guage varieties are grouped together on the ba-
sis of their similarity with respect to certain prop-
erties. Later work in this area has incorporated
methods of string alignment for a quantitative
comparison of individual words to obtain an aver-
age measure of the similarity of languages. This
line of research became known as dialectome-
try. Unlike traditional dialectology which is based
on the analysis of individual items, dialectometry
shifts focus on the aggregate level of differences.
Most of the work done so far in dialectometry
is based on the carefully selected wordlists and
problems with the limited amount of suitable data
(i.e. computer readable and comparable across di-
alects) are also present in this field.
This workshop brings together researchers in-
terested in computational approaches that uncover
sound correspondences and sound changes, auto-
matic identification of cognates across languages
and language comparison based both on wordlists
and parallel texts. First, Wettig et al investigate
the sound correspondences in cognate sets in a
sample of Uralic languages. Then, List?s contri-
bution to the volume introduces a novel method
for automatic cognate detection in multilingual
wordlists which combines various previous ap-
proaches for string comparison. The paper by
Mayer & Cysouw presents a first step to use par-
allel texts for a quantitative comparison of lan-
guages. The papers by Scherrer and Prokic? et
al. both are in the spirit of the dialectometric line
of research. Further, Ja?ger reports on quantify-
ing language similarity via phonetic alignment of
core vocabulary items. Finally, some of the pa-
2
pers presented in this workshop deal with further
topics in quantitative language comparison, like
the application of phylogenetic methods in cre-
ole research in the paper by Daval-Markussen &
Bakker, and the study of the evolution of the Aus-
tralian kinship terms reported on in the paper by
McConvell & Dousset.
In the next section, we give a brief introduc-
tion into the papers presented in this workshop,
ordered according to the program of the oral pre-
sentations at the workshop.
4 Papers
Christian Rohrdantz, Andreas Niekler, Annette
Hautli, Miriam Butt and Daniel A. Keim (?Lex-
ical Semantics and Distribution of Suffixes ?
A Visual Analysis) present a quantitative cross-
linguistic investigation of the lexical semantic
content expressed by three suffixes originating in
English: -gate, -geddon and -athon. Using data
from newspapers, they look at the distribution and
lexical semantic usage of these morphemes across
several languages and also across time, with a
time-depth of 20 years for English. Using tech-
niques from InfoVis and Visual Analytics is cru-
cial for the analysis as the occurrence of these suf-
fixes in the available corpora is comparatively rare
and it is only by dint of processing and visualiz-
ing huge amounts of data that a clear pattern can
begin to emerge.
Kris Heylen, Dirk Speelman and Dirk Geer-
aerts (?Looking at Word Meaning. An Interac-
tive Visualization of Semantic Vector Spaces for
Dutch synsets?) focus on the pervasive use of Se-
mantic Vector Spaces (SVS) in statistical NLP
as a standard technique for the automatic mod-
eling of lexical semantics. They take on the
fact that while the method appears to work fairly
well (though they criticize the standardly avail-
able evaluation measures via some created gold
standard), it is in fact quite unclear how it captures
word meaning. That is, the standard technology
can be seen as a black box. In order to find a way
of providing some transparency to the method,
they explore the way an SVS structures the indi-
vidual occurrences of words with respect to the
occurrences of 476 Dutch nouns. These were
grouped into 214 synsets in previous work. This
paper looks at a token-by-token similarity matrix
in conjunction with a visualization that uses the
Google Chart Tools and compares the results with
previous work, especially in light of different uses
in different versions of Dutch.
Ahti Lohk, Kadri Vare and Leo Vo?handu
(?First Steps in Checking and Comparing Prince-
ton WordNet and Estonian WordNet?) use visu-
alization methods to compare two existing Word-
Nets (English and Estonian) in order to identify
errors and semantic inconsistencies that are a re-
sult of the manual coding. Their method opens
up a potentially interesting way of automatically
checking for inconsistencies and errors not only
at a fairly basic and surface level, but by work-
ing with the lexical semantic classification of the
words in question.
Richard Littauer, Rory Turnbull and Alexis
Palmer (?Visualizing Typological Relationships:
Plotting WALS with Heat Maps?) present a novel
way of visualizing relationships between lan-
guages. The paper is based on data extracted from
the World Atlas of Language Structures (WALS),
which is the most complete set of typological and
digitized data available to date, but which presents
two challenges: 1) it actually has very low cover-
age both in terms of languages represented and
in terms of feature description for each language;
2) areal effects are not coded for. While the au-
thors find a way to overcome the first challenge,
the paper?s real contribution lies in proposing a
method for overcoming the second challenge. In
particular, the typological data is filtered by geo-
graphical proximity and then displayed by means
of heat maps, which reflect the strength of similar-
ity between languages for different linguistic fea-
tures. Thus, the data should allow one to be able
to ascertain areal typological effects via a single
integrated visualization.
Helen Yannakoudakis, Ted Briscoe and
Theodora Alexopoulou (?Automatic Second
Language Acquisition Research: Integrating
Information Visualisation and Machine Learn-
ing?) look at yet another domain of application.
They show how data-driven approaches to
learner corpora can support Second Language
Acquisition (SLA) research when integrated
with visualization tools. Learner corpora are
interesting because their analysis requires a good
understanding of a complex set of interacting
linguistic features across corpora with different
distributional patterns (since each corpus po-
tentially diverges from the standard form of the
language by a different set of features). The paper
3
presents a visual user interface which supports
the investigation of a set of linguistic features
discriminating between pass and fail exam
scripts. The system displays directed graphs to
model interactions between features and supports
exploratory search over a set of learner texts.
A very useful result for SLA is the proposal
of a new method for empirically quantifying
the linguistic abilities that characterize different
levels of language learning.
Verena Lyding, Ekaterina Lapshinova-
Koltunski, Stefania Degaetano-Ortlieb, Henrik
Dittmann and Chris Culy (?Visualizing Linguistic
Evolution in Academic Discourse?) describe
methods for visualizing diachronic language
changes in academic writing. In particular, they
look at the use of modality across different aca-
demic subfields and investigate whether adjacent
subfields affect the use of language in a given
academic subfield. Their findings potentially
provide crucial information for further NLP tasks
such as automatic text classification.
Grzegorz Kondrak?s invited contribution
(?Similarity Patterns in Words?) sketches a num-
ber of the author?s research projects on diachronic
linguistics. He first discusses computational tech-
niques for implementing several steps of the
comparative method. These techniques include
algorithms that deal with a wide range of prob-
lems: pairwise and multiple string alignment,
calculation of phonetic similarity between two
strings, automatic extraction of recurrent sound
correspondences, quantification of semantic
similarity between two words, identification of
sets of cognates and building of phylogenetic
trees. In the second part, Kondrak sketches
several NLP projects that directly benefitted
from his research on diachronic linguistics:
statistical machine translation, word align-
ment, identification of confusable drug names,
transliteration, grapheme-to-phoneme conver-
sion, letter-phoneme alignment and mapping of
annotations.
Thomas Mayer and Michael Cysouw (?Lan-
guage Comparison through Sparse Multilingual
Word Alignment?) present a novel approach on
how to calculate similarities among languages
with the help of massively parallel texts. In-
stead of comparing languages pairwise they sug-
gest a simultaneous analysis of languages with re-
spect to their co-occurrence statistics for individ-
ual words on the sentence level. These statistics
are then used to group words into clusters which
are considered to be partial (or ?sparse?) align-
ments. These alignments then serve as the basis
for the similarity count where languages are taken
to be more similar the more words they share in
the various alignments, regardless of the actual
form of the words. In order to cope with the
computationally demanding multilingual analysis
they introduce a sparse matrix representation of
the co-occurrence statistics.
Yves Scherrer (?Recovering Dialect Geogra-
phy from an Unaligned Comparable Corpus?) pro-
poses a simple metric of dialect distance, based
on the ratio between identical word pairs and cog-
nate word pairs occurring in two texts. Scherrer
proceeds from a multidialectal corpus and applies
techniques from machine translation in order to
extract identical words and cognate words. The
dialect distance is defined as as function of the
number of cognate word pairs and identical word
pairs. Different variations of this metric are tested
on a corpus containing comparable texts from dif-
ferent Swiss German dialects and evaluated on the
basis of spatial autocorrelation measures.
Jelena Prokic?, C?ag?r? Co?ltekin and John Ner-
bonne (?Detecting Shibboleths?) propose a gen-
eralization of the well-known precision and re-
call scores to deal with the case of detecting dis-
tinctive, characteristic variants in dialect groups,
in case the analysis is based on numerical differ-
ence scores. This method starts from the data that
has already been divided into groups using clus-
ter analyses, correspondence analysis or any other
technique that can identify groups of language va-
rieties based on linguistic or extra-linguistic fac-
tors (e.g. geography or social properties). The
method seeks items that differ minimally within a
group but differ a great deal with respect to ele-
ments outside it. They demonstrate the effective-
ness of their approach using Dutch and German
dialect data, identifying those words that show
low variation within a given dialect area, and high
variation outside a given area.
Gerhard Ja?ger (?Estimating and Visualizing
Language Similarities Using Weighted Align-
ment and Force-Directed Graph Layout?) reports
several studies to quantify language similarity
via phonetic alignment of core vocabulary items
(taken from the Automated Similarity Judgement
Program data base). Ja?ger compares several string
4
comparison measures based on Levenshtein dis-
tance and based on Needleman-Wunsch similar-
ity score. He also tests two normalization func-
tions, one based on the average score and the
other based on the informatic theoretic similar-
ity measure. The pairwise similarity between all
languages are analyzed and visualized using the
CLANS software, a force directed graph layout
that does not assume an underlying tree structure
of the data.
Aymeric Daval-Markussen and Peter Bakker
(?Explorations in Creole Research with Phyloge-
netic Tools?) employ phylogenetic tools to inves-
tigate and visualize the relationship of creole lan-
guages to other (non-)creole languages on the ba-
sis of structural features. Using the morphosyn-
tactic features described in the monograph on
Comparative Creole Syntax (Holm and Patrick,
2007), they create phylogenetic trees and net-
works for the languages in the sample, which
show the similarity between the various languages
with respect to the grammatical features inves-
tigated. Their results lend support to the uni-
versalist approach which assumes that creoles
show creole-specific characteristics, possibly due
to restructuring universals. They also apply their
methodology to the comparison of creole lan-
guages to other languages, on the basis of typo-
logical features from the World Atlas of Language
Structures. Their findings confirm the hypothe-
sis that creole languages form a synchronically
distinguishable subgroup among the world?s lan-
guages.
Patrick McConvell and Laurent Dousset
(?Tracking the Dynamics of Kinship and So-
cial Category Terms with AustKin II?) give an
overview of their ongoing work on kinship and
social category terms in Australian languages.
They describe the AustKin I database which
allows for the reconstruction of older kinship
systems as well as the visualization of patterns
and changes. In particular, their method recon-
structs so-called ?Kariera? kinship systems for the
proto-languages in Australia. This supports ear-
lier hypotheses about the primordial world social
organization from which Dravidian-Kariera sys-
tems are considered to have evolved. They also
report on more recent work within the AustKin II
project which is devoted to the co-evoluation of
marriage and social category systems.
Hannes Wettig, Kirill Reshetnikov and Roman
Yangarber (?Using Context and Phonetic Fea-
tures in Models of Etymological Sound Change?)
present a novel method for a context-sensitive
alignment of cognate words, which relies on the
information theoretic concept of Minimum De-
scription Length to decide on the most compact
representation of the data given the model. Start-
ing with an initial random alignment for each
word pair, their algorithm iteratively rebuilds de-
cision trees for each feature and realigns the cor-
pus while monotonically decreasing the cost func-
tion until convergence. They also introduce a
novel test for the quality of the models where one
word pair is omitted from the training phase. The
rules that have been learned are then used to guess
one word from the other in the pair. The Lev-
enshtein distance of the correct and the guessed
word is then computed to give an idea of how
good the model actually learned the regularities
in the sound correspondences.
Johann-Mattis List (?LexStat: Automatic De-
tection of Cognates in Multilingual Wordlists?)
presents a new method for automatic cognate
detection in multilingual wordlists. He com-
bines different approaches to sequence compari-
son in historical linguistics and evolutionary bi-
ology into a new framework which closely mod-
els central aspects of the comparative method.
The input sequences, i.e. words, are converted to
sound classes and their sonority profiles are deter-
mined. In step 2, a permutation method is used to
create language specific scoring schemes. In step
3, the pairwise distances between all word pairs,
based on the language-specific scoring schemes,
are computed. In step 4, the sequences are clus-
tered into cognate sets whose average distance is
beyond a certain threshold. The method is tested
on 9 multilingual wordlists.
5 Final remarks
The breadth and depth of the research collected
in this workshop more than testify to the scope
and possibilities for applying new methods that
combine quantitative methods with not only a so-
phisticated linguistic understanding of language
phenomena, but also with visualization methods
coming out of the Computer Science fields of In-
foVis and Visual Analytics. The papers in the
workshop addressed how the emerging new body
of work can provide advances and new insights
for questions pertaining to theoretical linguistics
5
(lexical semantics, derivational morphology, his-
torical linguistics, dialectology and typology) and
applied linguistic fields such as second language
acquisition and statistical NLP.
6 Acknowledgments
We are indebted to the members of the pro-
gram committee of the workshop for their ef-
fort in thoroughly reviewing the papers: Quentin
Atkinson, Christopher Collins, Chris Culy, Dan
Dediu, Michael Dunn, Sheila Embleton, Simon
Greenhill, Harald Hammarstro?m, Annette Hautli,
Wilbert Heeringa, Gerhard Heyer, Eric Hol-
man, Gerhard Ja?ger, Daniel Keim, Tibor Kiss,
Jonas Kuhn, Anke Lu?deling, Steven Moran, John
Nerbonne, Gerald Penn, Don Ringe, Christian
Rohrdantz, Tandy Warnow, S?ren Wichmann.
We also thank the organizers of the EACL 2012
conference for their help in setting up the joint
workshop.
References
Christopher Collins, Sheelagh Carpendale, and Ger-
ald Penn. 2009. Docuburst: Visualizing document
content using language structure. Computer Graph-
ics Forum (Proceedings of Eurographics/IEEE-
VGTC Symposium on Visualization (EuroVis ?09)),
28(3):1039?1046.
Christopher Collins. 2010. Interactive Visualizations
of Natural Language. Ph.D. thesis, University of
Toronto.
Matthew S. Dryer and Martin Haspelmath, editors.
2011. The World Atlas of Language Structures On-
line. Max Planck Digital Library, Munich, 2011
edition.
Michael Dunn, Angela Terrill, Ger Resnik, Robert A.
Foley, and Stephen C. Levinson. 2005. Structural
phylogenetics and the reconstruction of ancient lan-
guage history. Science, 309(5743):2072?2075.
Russell Gray and Quentin Atkinson. 2003. Language-
tree divergence times support the Anatolian theory
of Indo-European origins. Nature, 426:435?439.
David LW Hall and Dan Klein. 2010. Finding cognate
groups using phylogenies. In Proceedings of the
Association for Computational Linguistics.
Paul Heggarty, Warren Maguire, and April McMahon.
2010. Splits or waves? trees or webs? how diver-
gence measures and network analysis can unravel
language histories. In Philosophical Transactions
of the Royal Society (B), volume 365, pages 3829?
3843.
John Holm and Peter L. Patrick, editors. 2007. Com-
parative Creole Syntax. London: Battlebridge.
Timo Honkela, Ville Pulkki, and Teuvo Kohonen.
1995. Contextual relations of words in grimm tales,
analyzed by self-organizing map. In Proceedings of
International Conference on Artificial Neural Net-
works (ICANN-95), pages 3?7.
Grzegorz Kondrak. 2001. Identifying cognates by
phonetic and semantic similarity. In Proceedings
of the North American Chapter of the Association
of Computational Linguistics.
Thomas Mayer, Christian Rohrdantz, Miriam Butt,
Frans Plank, and Daniel A. Keim. 2010a. Visualiz-
ing vowel harmony. Linguistic Issues in Language
Technology (LiLT), 2(4).
Thomas Mayer, Christian Rohrdantz, Frans Plank,
Peter Bak, Miriam Butt, and Daniel A. Keim.
2010b. Consonant co-occurrence in stems across
languages: Automatic analysis and visualization of
a phonotactic constraint. In Proceedings of the
2010 Workshop on NLP and Linguistics: Finding
the Common Ground, ACL 2010, pages 70?78.
April McMahon and Robert McMahon. 2006. Lan-
guage Classification by Numbers. OUP.
Petra Neumann, Annie Tat, Torre Zuk, and Shee-
lagh Carpendale. 2007. Keystrokes: Personaliz-
ing typed text with visualization. In Proceedings
of Eurographics IEEE VGTC Symposium on Visu-
alization.
Christian Rohrdantz, Annette Hautli, Thomas Mayer,
Miriam Butt, Daniel A. Keim, and Frans Plank.
2011. Towards tracking semantic change by visual
analytics. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics (Short Papers), pages 305?310. Portland, Ore-
gon.
Jean Se?guy. 1971. La relation entre la distance spa-
tiale et la distance lexicale. Revue de Linguistique
Romane, 35(138):335?357.
Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010. A statistical model for lost language deci-
pherment. In Proceedings of the Association for
Computational Linguistics.
6
