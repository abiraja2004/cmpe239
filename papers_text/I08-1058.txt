Multilingual Text Entry using Automatic Language Detection
Yo Ehara and Kumiko Tanaka-Ishii
Graduate School of Information Science and Technology, University of Tokyo
13F Akihabara Daibiru, 1-18-13 SotoKanda Chiyoda-ku, Tokyo, Japan
ehara@r.dl.itc.u-tokyo.ac.jp kumiko@i.u-tokyo.ac.jp
Abstract
Computer users increasingly need to pro-
duce text written in multiple languages.
However, typical computer interfaces re-
quire the user to change the text entry soft-
ware each time a different language is used.
This is cumbersome, especially when lan-
guage changes are frequent.
To solve this problem, we propose TypeAny,
a novel front-end interface that detects the
language of the user?s key entry and au-
tomatically dispatches the input to the ap-
propriate text entry system. Unlike previ-
ously reported methods, TypeAny can han-
dle more than two languages, and can easily
support any new language even if the avail-
able corpus is small.
When evaluating this method, we obtained
language detection accuracy of 96.7% when
an appropriate language had to be chosen
from among three languages. The number of
control actions needed to switch languages
was decreased over 93% when using Ty-
peAny rather than a conventional method.
1 Introduction
Globalization has increased the need to produce
multilingual text ? i.e., text written in more than
one language ? for many users. When producing
a text in a language other than English, a user has
to use text entry software corresponding to the other
language which will transform the user?s key stroke
sequences into text of the desired language. Such
software is usually called an input method engine
(IME) and is available for each widely used lan-
guage. When producing a multilingual text on a
typical computer interface, though, the user has to
switch IMEs every time the language changes in a
multilingual text. The control actions to choose an
appropriate IME are cumbersome, especially when
the language changes frequently within the text.
To solve this problem, we propose a front-end in-
terface called TypeAny. This interface detects the
language that the user is using to enter text and dy-
namically switches IMEs. Our system is situated
between the user key entry and various IMEs. Ty-
peAny largely frees the user from the need to exe-
cute control actions when switching languages.
The production of multilingual text involves three
kinds of key entry action:
? actions to enter text
? actions to control an IME1
? actions to switch IMEs
Regarding the first and second types, substantial
work has been done in the UI and NLP domain,
as summarized in (MacKenzie and Tanaka-Ishii,
2007). There has especially been much work re-
garding Chinese and Japanese because in these lan-
guages the number of actions of the second type is
closely related to the accuracy of conversion from
Romanized transcription to characters in each of
these languages, and this directly reflects the capa-
bility of the language model used.
1When using predictive methods such as completion, or
kana-kanji conversion in Japanese, the user has to indicate to
the IME when it should predict and which proposed candidate
to choose.
441
In contrast, this paper addresses the question of
how to decrease the need for the third type of action.
From the text entry viewpoint, this question has re-
ceived much less attention than the need to reduce
the number of actions of the second type. As far as
we know, this issue has only been directly addressed
by Chen et al (2000), who proposed integrating En-
glish entry into a Chinese input system rather than
implementing multilingual input.
Reports on ways to detect a change in the lan-
guage used are more abundant. (Murthy and Ku-
mar, 2006) studied the language identification prob-
lem based on small samples in several Indian lan-
guages when machine learning techniques are used.
Although they report a high accuracy for the method
they developed, their system handles switches be-
tween two Indian languages only. In contrast, Ty-
peAny can handle any number of languages mixed
within a text.
(Alex, 2005) addresses a related task, called for-
eign inclusion detection (FID). The task is to find
foreign (i.e., English) inclusions, such as foreign
noun compounds, within monolingual (i.e., Ger-
man) texts. Alex reported that the use of FID to
build a polyglot TTS synthesizer was also consid-
ered (Pfister and Romsdorfer, 2003), (Marcadet et
al., 2005). Recently, Alex used FID to improve pars-
ing accuracy (Alex et al, 2007). While FID relies
on large corpora and lexicons, our model requires
only small corpora since it incorporates the transi-
tion probabilities of language switching. Also, while
FID is specific to alphabetic languages, we made our
method language-independent by taking into consid-
eration the inclusion problem at the key entry level.
In the following, we introduce the design of Ty-
peAny, explain its underlying model, and report on
our evaluation of its effectiveness.
2 Design of TypeAny
Figure 1 shows an example of text written in En-
glish, Japanese and Russian. The strings shown be-
tween the lines indicate the Roman transcription of
Japanese and Russian words.
With a conventional computer interface, entering
the text shown in Figure 1 would require at least six
control actions since there are six switches between
languages: from English to Japanese and back, and
Figure 1: Example of Multilingual Text in English,
Japanese and Russian
Figure 2: System Structure
twice from English to Russian and back. Note that
such IME switches are also required even when the
text consists only of European languages. Each Eu-
ropean language has its own font and diacritic sys-
tem, which are realized by using IMEs.
TypeAny solves the problem of changing IME. It
is situated between the user?s key entry and various
IMEs as shown in the system architecture diagram
of Figure 2. The user?s key entry sequence is input
to our client software. The client sends the sequence
to the server which has the language identifier mod-
ule. This module detects the language of the key
sequence and then sends the key sequence to the ap-
propriate IME2. The selected IME then converts the
key entries into text of the detected language.
In our study, IMEs for European languages are
built using simple transliteration: e.g., ?[? typed in
an English keyboard is transliterated into ?u?? of Ger-
man. In contrast, the IMEs for Japanese and Chinese
2Precisely speaking, TypeAny detects keyboard layouts (i.e.,
Qwerty, Dvorak, Azerty, etc.) as well as the languages used.
442
Figure 3: Entry Flow
require a more complicated system because in these
languages there are several candidate transcriptions
for a key sequence. Fortunately, several existing
software resources can be used for this. We use An-
thy3 as the IME for Japanese. As for the IME for
Chinese, we used a simple word-based pinyin-hanzi
conversion system.
TypeAny restarts the language detection every
time a certain delimiter appears in the user?s key se-
quence. By using delimiters, the system can avoid
resorting to a combinatorial search to find the bor-
der between languages. Such delimiters naturally
occur in natural language texts. For example, in
the case of European languages, blank spaces are
used to delimit words and it is unlikely that two lan-
guages will be mixed within one word. In languages
such as Chinese and Japanese, blank spaces are typ-
ically used to indicate that the entry software should
perform conversion, thus guaranteeing that the se-
quence between two delimiters will consist of only
one language4. Therefore, assuming that a text frag-
ment between two delimiters is written in just one
language is natural for users. A text fragment be-
tween two delimiters is called a token in TypeAny.
An example of the TypeAny procedure to enter
3http://anthy.sourceforge.jp/
4Note that a token can consist of a sequence longer than a
word, since many types of conversion software allow the con-
version of multiple words at one time.
the text from Figure 15 is shown in Figure 3. In
each step in Figure 3, the text is entered in the first
line, where the token that the user is entering is high-
lighted. The language estimated from the token is
shown in the locale window shown below (called the
Status Window). Each step proceeds as follows.
(a) The initial state.
(b) The user first wants to type a token ?Some? in
English. When ?Som? is typed, the system
identifies that the entry is in English. The user
confirms this language by looking at the locale
window.
(c) The user finishes entering the token ?Some? and
when the user enters a blank space, the token
?Some? is confirmed as English text. TypeAny
restarts detection for the language of the next
token. The tokens up to and including ?offer?
are entered similarly to ?Some?.
(d) The user types in a token ?ikura? in Japanese.
The moment ?iku? is typed, ?iku? is identified
as Japanese, as is confirmed by the user through
the locale window.
(e) When the user finishes entering the token
?ikura? and types in a blank space, the se-
quence is sent to a Japanese IME to be con-
verted into ?ikura?, so that a Japanese text frag-
ment is obtained.
(f) Through conventional kana-kanji conversion,
5This case assumes use of the Qwerty keyboard.
443
Figure 4: When Detection Fails
the user can select the appropriate conversion
of ?ikura? among from candidates shown in the
Lookup Window and the token is confirmed.
TypeAny begins detecting the language of the
next token. The tokens between ?or? and ?Rus-
sian? are successfully identified as English in a
way similar to procedures (b) and (c).
(g) The key entry ?brhf? is the key sequence for the
Russian token whose English transliteration is
?ikra?.
(h) Since ?brhf? is identified as Russian, ?brhf? is
converted into Russian characters.
(i) The following word ?Caviar? is detected as En-
glish, as in (b) and (c).
As seen in this example, the user does not need to
take any action to switch IMEs to enter tokens of
different languages.
Two types of detection failure occur in TypeAny:
Failure A: the language should switch, but the new
language is incorrectly selected.
Failure B: the language should not switch, but Ty-
peAny misjudges that it should.
While conventional methods require a control ac-
tion every time the language switches, TypeAny re-
quires a control action only to correct such a failure.
Therefore, Failure A never increases the number
of control actions compared to that of conventional
methods. On the other hand, Failure B errors are
a concern as such failures might increase the num-
ber of control actions to beyond the number required
by a conventional method. Thus, the effectiveness
of introducing TypeAny depends on a trade-off be-
tween fewer control actions at language switching
points and potentially more control actions due to
Failure B errors. Our evaluation in ?4.2 shows that
the increase in the number of actions due to Failure
B errors is insignificant.
In the event of failures, the user can see that there
is a problem by watching the locale window and
then easily correct the language by pressing the TAB
key. For example, while ?in? was correctly judged
for our example, suppose it is incorrectly detected as
Japanese as shown in Figure 4(a). In this case, the
user can manually correct the locale by pressing the
TAB key once. The locale is then changed from Fig-
ure 4(a) (where ?in? is identified as Japanese), to (b)
where ?in? is identified as English.
Note that the language of some tokens will be am-
biguous. For example, the word ?sushi? can be both
English and Japanese because ?sushi? has almost be-
come an English word: many loan words share this
ambiguity. Another case is when diacritic marks are
considered: for example, the word ?fur? is usually
an English word, but some German users may wish
to use this word as ?fu?r? without diacritic marks.
Such a habit is widely seen among users of Euro-
pean languages. Some of this sort of ambiguity is
disambiguated by considering the context and by on-
line learning, which is incorporated in the detection
model as explained next.
3 Language Detection
3.1 Language Detection Model
We modeled the language detection as a hidden
Markov model (HMM) process whose states corre-
spond to languages and whose outputs correspond to
tokens from a language.
Here, the goal is to estimate the languages l?m1 by
maximizing P(lm1 , tm1 ), where l ? L denotes a lan-
guage in L, a set of languages, and t denotes a to-
ken6. By applying a hidden Markov model, the max-
imization of P(lm1 , tm1 ) is done as shown in Equa-
tion (1).
l?m1 = argmaxlm1 ?L
P(lm1 , tm1 )
= argmax
lm1 ?L
P(tm1 |lm1 )P(lm1 )
? argmax
lm1 ?L
( m?
i=1
P (ti|li)
)( m?
i=1
P (li|li?1i?k)
)
(1)
In the last transformation of Equation (1), it
is assumed that P(tm1 |lm1 ) ?
?m
i=1 P (ti|li) and
P(li|li?11 ) ? P (li|li?1i?k) for the first and the second
terms, respectively. In Equation (1), the first term
6Let tvu = (tu, tu+1, . . . , tv) be an ordered list consisting of
v ? u+ 1 elements for v ? u.
444
corresponds to the output probabilities and the sec-
ond term corresponds to the transition probabilities.
In a usual HMM process, a system finds the lan-
guage sequence (i.e., state sequence) lm1 that maxi-
mizes Equation (1) by typically using a Viterbi algo-
rithm. In our case, too, the system can estimate the
language sequence for a sequence of tokens. How-
ever, as discussed earlier, since it is unlikely that a
user enters a token consisting of multiple languages,
our system is designed only to estimate the language
of the latest token lm, supposing that the languages
of the previous lm?11 are correct.
In the following two sections, the estimation of
each term is explained.
3.2 Output Probabilities
The output probabilities P (ti|li) indicate the proba-
bilities of tokens in a monolingual corpus, and their
modeling has been substantially investigated in NLP.
Note that the estimation of P (ti|li) requires
monolingual corpora. If the corpora are large,
P (ti|li) is estimated from the token frequencies.
However, because large corpora are not always
available, especially for minor languages, P (ti|li)
is estimated using key entry sequence probabilities
based on n-grams (with maximum n being nmax) as
follows:
P (ti|li) = P (c|ti|1 |li) =
|ti|?
r=1
P (cr|cr?1r?nmax+1, li)(2)
In Equation (2), ti = c|ti|1 and |ti| is the length of ti
with respect to the key entry sequence. For exam-
ple, in the case of ti=?ikura?, |ti| = 5 and c1=?i?,
c2=?k?, c3=?u? and |ti|=5. Here, each probability
P (cr|cr?1r?nmax+1, li) needs to be smoothed.
Values of P (ti|li) are estimated from monolin-
gual corpora. If the corpora are large, P (ti|li) is
estimated from the token frequencies. However, be-
cause large corpora are not always available, espe-
cially for minor languages, P (ti|li) is estimated us-
ing smoothed character-based n-grams. Prediction
by Partial Matching, or PPM is adopted for this task,
since it naturally incorporates online learning and it
is effective in various NLP tasks as reported in (Tea-
han et al, 2000) and (Tanaka-Ishii, 2006). PPM
uses cr?nmax1 as a corpus for training. PPM is de-
signed to predict the next cr by estimating the nmax-
gram probability P (cr|cr?1r?nmax+1) using backing-
off techniques with regard to the current context.
Precisely, the probability is estimated as a weighted
sum of different (n + 1)-gram probabilities up to a
fixed nmax-gram as follows:
P (cr|cr?1r?nmax+1) =
nmax?1?
n=?1
wnpn(cr) (3)
The weights wn are determined through escape
probabilities. Depending on how the escape prob-
abilities are calculated, there are several PPM vari-
ants, which are named PPMA, PPMB, PPMC, and
so on. PPMC, the one that we have used, is also
known as Witten-Bell smoothing in the NLP field
(Manning and Schuetze, 1999). The escape proba-
bilities are defined as follows.
wn = (1? en)
ncont?
n?=n+1
en? (?1 ? n < ncont)(4)
wncont = (1? en)
Here, ncont is defined as the maximum n that satis-
fies Xn 6= 0. Let Xn be the number of cr?1r?n, xn be
the number of crr?n and qn be the number of differ-
ent keycodes followed by cr?1r?n found in cr?n?11 .
Using these notations, pn(cr) is defined as
pn(cr) = xnXn (5)
In PPMC, the escape probabilities are calculated as
en = qnXn + qn (6)
For further details, see (Bell et al, 1990).
3.3 Language Transition Probabilities
Only a small corpus is typically available to esti-
mate P (lm|lm?1m?kmax+1), where kmax is the longestk-gram in the language sequence to be considered.
Thus, the transition probability is estimated on-line,
making use of language that will be corrected inter-
actively by the user. For this on-line learning, we
adopted PPM as well as the output probabilities.
Note that a large kmax may reduce accuracy,
which is intuitively explained as follows. While
there is typically a high probability that the subse-
quent language will be the same as the current lan-
guage, it is unlikely that any language sequence will
have long regular patterns. Therefore, kmax should
be fixed according to this consideration.
445
 0
 20
 40
 60
 80
 100
 2  3  4  5  6  7  8
A
c
c
u
ra
c
y
 (%
)
Number of languages
PPM
ML
baseline
Figure 5: Detection Accuracy Test1
 75
 80
 85
 90
 95
 100
 2  3  4  5  6  7  8
A
c
c
u
ra
c
y
 (%
)
Number of languages
PPM
ML
baseline
Figure 6: Detection Accuracy Test2
4 Evaluation
We evaluated TypeAny with respect to two mea-
sures: language detection capability when using arti-
ficially generated multilingual corpora, and the num-
ber of required control actions when using actual
multilingual corpora.
4.1 Language Detection Accuracy
The ideal experiment would be to use actual multi-
lingual corpora for many language sets. However, it
is still difficult to collect a large amount of multilin-
gual corpora with adequate quality for the test data
of languages.
Therefore, we measured the language detection
accuracies using artificially generated multilingual
corpora by mixing monolingual corpora for every
combination varying from two to eight languages.
First, the following monolingual corpora were
collected: editions of the Mainichi newspaper in
2004 for Japanese, the Peking University corpus
for Chinese, and the Leipzig corpora (Biemann et
al., 2007) for English, French, German, Estonian,
Finnish and Turkish. The text of each of these cor-
pora was transformed into a sequence of key entries.
Two test sets, Test1 and Test2, were generated by
using different mixture rates. In Test1, languages
were mixed uniformly and randomly, whereas in
Test2 a major language accounted for 90% of the
text and the remaining 10% included different lan-
guages chosen uniformly and randomly. Test2 is
more realistic since a document is usually composed
in one major language.
The output and language transition probabilities
were estimated and smoothed using PPMC as de-
scribed in ?3. Since part of the target of the exper-
iment was to clarify the relation between learning
size and accuracy, the output probabilities and tran-
sition probabilities were not trained on-line while
the text was entered using PPMC, thus accuracy was
measured by fixing the language model at this initial
state. We used nmax = 5 for the output probability
and kmax = 1 for the transition probability since the
distribution of languages in the corpus was uniform
here as we generated it uniformly. (See formula (4)
in ?3.2).
A 10-fold cross validation was applied to the gen-
erated corpora. Each generated corpus was 111
Kbytes in size, consisting of a disjoint 100-Kbyte
training part and an 11-Kbyte testing part. The out-
put probabilities were trained using the 100-Kbyte
training part. The language transition probabilities
were trained using about 2000 tokens.
The results for Test1 and Test2 are shown in Fig-
ure 5 and Figure 6, respectively. The horizontal
axis shows the number of languages and the ver-
tical axis shows the detection accuracy. There are
three lines: PPM indicates that the transition proba-
bilities were trained by PPM; ML indicates that no
transition probability was used and the language was
detected using only output probabilities (maximum
likelihood only); Baseline is the accuracy when the
most frequent language is always selected.
446
As shown in Figure 5 (Test1), when the mix-
ture was uniform, the PPM performance was slightly
lower but very close to that of ML. This was because
PPM would be theoretically equivalent to ML with
infinite learning of language transition probabilities,
since languages were uniformly distributed in Test1.
These results show that our PPM for transition prob-
abilities learns this uniformity in Test1.
As shown in Figure 6, PPM clearly outperformed
ML in Test2. This was because ML has no way
to learn the transition probability, which was biased
with the major language being used 90% of the time.
This shows that the introduction of language transi-
tion probabilities accounts for higher performance.
Interestingly, ML falls below the baseline case when
more than three languages were used in Test2, a sit-
uation that has rarely been considered in previous
studies. This suggests that language detection using
only ML requires large corpora for learning to select
one appropriate language, and that this requirement
can be alleviated by using PPM.
Another finding is that the detection accuracy de-
pends on the language set. For example, the accu-
racy for language sets consisting of both French and
English tended to be lower than for other language
sets due to the spelling closeness between these two
languages. For example, the accuracy for test data
consisting of 90% English, 5% French and 5% Ger-
man was 94.4%. This is not surprising since the de-
tection was made only within a token (which cor-
responds to a word in European languages): natu-
rally there were many words whose language was
ambiguous within the test set. In contrast, high ac-
curacies were obtained for test sets consisting of lan-
guages more different in their nature. We obtained
97.5% accuracy for test data consisting of 90% En-
glish, 5% Finnish and 5% Turkish; this accuracy was
higher than the average for all test sets.
4.2 Number of Control Actions
The second evaluation was done to compare the
number of control actions needed to switch lan-
guages with TypeAny and with a conventional
method. As mentioned in ?1, three types of key-
board actions are used when entering text. Our work
7E.: English, J.: Japanese and C.: Chinese.
8http://en.wikitravel.org/
9http://en.wikipedia.org/
Table 1: Articles Used in the Decrease Test
article Article 1 Article 2
Foreign tokens 286 55
Total tokens 1725 5100
Inclusion ratio 16.6% 1.1%
languages E., J. E., J., C.7
content Introduction
of Japanese
phrases for
traveling
About tofu
(bean curd)
Source Wikitravel 8 Wikipedia 9
Table 2: Required Number of Control Actions
Article 1 Article 2
Conventional 572 110
Number of switches (100%) (100%)
Ours Failure A 2.8% 3.6%
Failure B 1.6% 2.7%
Total Failures 4.4% 6.3%
Decrease 95.6% 93.6%
only concerns the control action to switch language,
though, and the comparison in this section focuses
on this type of action.
This evaluation was done using two samples of
actual multilingual text collected from the Web. The
features of these samples are shown in the top block
of Table 1. In both cases, the major language was
English.
For each of these articles, the number of con-
trol actions required with the conventional method
and with TypeAny was measured. The conventional
method requires a control action every time the lan-
guage switches. For TypeAny, control actions are re-
quired only to correct language detection failures. In
both cases, the action required to switch languages
or correct the language was counted as one action.
For the language model, the output probabilities
were first trained using the 100-Kbyte monolingual
corpora collected for the previous evaluation. The
transition probabilities were not trained beforehand;
i.e., the system initially regarded the languages to be
uniformly distributed. Since this experiment was in-
tended to simulate a realistic case, both output and
447
transition probabilities were trained on-line using
PPMC while the text was entered. Here, both nmax
and kmax were set at 5.
The results are shown in Table 2. First, some de-
tection errors occurred for Article 2 because ?tofu?
was detected as Japanese at the beginning of entry,
even though it was used as an English word in the
original text. As noted at the end of ?2, such loan
words can cause errors. However, since our system
uses PPM and learns on-line, our system learned that
?tofu? had to be English, and such detection errors
occurred only at the beginning of the text.
Consequently, there was a substantial decrease in
the number of necessary control actions with Ty-
peAny, over 93%, for both articles. An especially
large decrease was observed for Article 2, even
though the text was almost all in English (98.9%).
There was only a small increase in the incidence rate
of Failure B for Article 2, so the total decrease in the
number of required actions was still large, putting
to rest the concern discussed in ?2. These results
demonstrate the effectiveness of our approach.
5 Conclusion
TypeAny is a novel multilingual text input interface
in which the languages used for entries are detected
automatically. We modeled the language detection
as an HMM process whose transition probabilities
are estimated by on-line learning through the PPM
method.
This system achieved language detection accu-
racy of 96.7% in an evaluation where it had to
choose the appropriate language from among three
languages with the major language accounting for
90% of the sample. In addition, the number of con-
trol actions required to switch IMEs was decreased
by over 93%. These results show the promise of our
system and suggest that it will work well under real-
istic circumstances.
An interesting objection might be raised to the
conclusions of this study: some users might find
it difficult to watch the locale window all the time
and prefer the conventional method despite having
to work with a large number of key types. We plan to
examine and clarify the cognitive load of such users
in our future work.
References
B. Alex, A. Dubey, and F. Keller. 2007. Using foreign in-
clusion detection to improve parsing performance. In
Proceedings of EMNLP-CoNLL, Prague, Czech, June.
B. Alex. 2005. An unsupervised system for identifying
English inclusions in German text. In Proceedings of
the ACL Student Research Workshop, pages 133?138,
Ann Arbor, Michigan, June. Association for Compu-
tational Linguistics.
T. C. Bell, J. G. Clear, and I. H. Witten. 1990. Text Com-
pression. Prentice-Hall, New Jersey.
C. Biemann, G. Heyer, U. Quasthoff, and M. Richter.
2007. The Leipzig corpora collection - monolingual
corpora of standard size. In Proceedings of Corpus
Linguistics, Birmingham, United Kingdom, July.
Z. Chen and K. Lee. 2000. A new statistical approach
to Chinese input. In The 38th Annual Meeting of the
Association for Computer Linguistics, pages 241?247,
Hong Kong, October.
I. S. MacKenzie and K. Tanaka-Ishii. 2007. Text Entry
Systems ?Mobility, Accessibility, Universality?. Mor-
gan Kaufmann.
C. D. Manning and H. Schuetze. 1999. Foundations of
Statistical Natural Language Processing. MIT Press.
J.-C. Marcadet, V. Fischer, and C. Waast-Richard. 2005.
A transformation-based learning approach to language
identification for mixed-lingual text-to-speech synthe-
sis. In Interspeech 2005 - ICSLP, pages 2249?2252,
Lisbon, Portugal.
K. N. Murthy and G. B. Kumar. 2006. Language identi-
fication from small text samples. Journal of Quantita-
tive Linguistics, 13:57?80.
B. Pfister and H. Romsdorfer. 2003. Mixed-lingual anal-
ysis for polyglot TTS synthesis. In Eurospeech, pages
2037?2040, Geneva, Switzerland.
K. Tanaka-Ishii. 2006. Word-based text entry techniques
using adaptive language models. Journal of Natural
Language Engineering, 13(1):51?74.
W. J. Teahan, Y. Wen, R. MacNab, and I. H. Witten.
2000. A compression-based algorithm for Chinese
word segmentation. In Computational Linguistics,
volume 26, pages 375?393.
448
