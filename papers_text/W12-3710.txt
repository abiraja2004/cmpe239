Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 61?69,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Unifying Local and Global Agreement and Disagreement Classification
in Online Debates
Jie Yin
CSIRO ICT Centre
NSW, Australia
jie.yin@csiro.au
Nalin Narang
University of New South Wales
NSW, Australia
nalinnarang@gmail.com
Paul Thomas
CSIRO ICT Centre
ACT, Australia
paul.thomas@csiro.au
Cecile Paris
CSIRO ICT Centre
NSW, Australia
cecile.paris@csiro.au
Abstract
Online debate forums provide a powerful
communication platform for individual users
to share information, exchange ideas and ex-
press opinions on a variety of topics. Under-
standing people?s opinions in such forums is
an important task as its results can be used
in many ways. It is, however, a challeng-
ing task because of the informal language use
and the dynamic nature of online conversa-
tions. In this paper, we propose a new method
for identifying participants? agreement or dis-
agreement on an issue by exploiting infor-
mation contained in each of the posts. Our
proposed method first regards each post in
its local context, then aggregates posts to es-
timate a participant?s overall position. We
have explored the use of sentiment, emotional
and durational features to improve the accu-
racy of automatic agreement and disagree-
ment classification. Our experimental results
have shown that aggregating local positions
over posts yields better performance than non-
aggregation baselines when identifying users?
global positions on an issue.
1 Introduction
With their increasing popularity, social media appli-
cations provide a powerful communication channel
for individuals to share information, exchange ideas
and express their opinions on a wide variety of top-
ics. An online debate is an open forum where a
participant starts a discussion by posting his opin-
ion on a particular topic, such as regional politics,
health or the military, while other participants state
their support or opposition by posting their opinions.
Understanding participants? opinions in online de-
bates has become an increasingly important task as
its results can be used in many ways. For example,
by analysing customers? online discussions, compa-
nies can better understand customers? reviews about
their products or services. For government agencies,
it could help gather public opinions about policies,
legislation, laws, or elections. For social science, it
can assist scientists to understand a breadth of social
phenomena from online observations of large num-
bers of individuals.
Despite the potentially wide range of applications,
understanding participants? positions in online de-
bates remains a difficult task. One reason is that
online conversations are very dynamic in nature.
Unlike spoken conversations (Thomas et al, 2006;
Wang et al, 2011), users in online debates are not
guaranteed to participate in a discussion at all times.
They may enter or exit the online discussion at any
point, so it is not appropriate to use models assuming
continued conversation. In addition, most discus-
sions in online debates are essentially dialogic; par-
ticipants could choose to implicitly respond to a pre-
vious post, or explicitly quote some content from an
earlier post and make a response. Therefore, an as-
sumption has to be made about what a participant?s
post is in response to, particularly when an explicit
quote is not present; in most cases, a post is assumed
to be in response to the most recent post in the thread
(Murakami and Raymond, 2010).
In this paper, we address the problem of detecting
users? positions with respect to the main topic in on-
line debates; we call this the global position of users
on an issue. It is inappropriate to identify each user?s
global position with respect to a main topic directly,
because most expressions of opinion are made not
61
for the main topic but for posts in a local context.
This poses a difficulty in directly building a global
classifier for agreement and disagreement. We illus-
trate this with the example below. Here, the topic of
the thread is ?Beijing starts gating, locking migrant
villages? and the discussion is started with a seed
post criticising the Chinese government1.
Seed post: I?m most sure there will be some
China sympathisers here justifying these ac-
tions imposed by the Communist Chinese gov-
ernment. . . .
Reply 1: Not really seeing a problem there.
From you article. They can come and go. Peo-
ple in my country pay hundreds of thousands
of pounds for security like that in their gated
communities..
Reply 2: So, you are OK with living in a Police
State? . . .
The author of Reply 1 argues that the Chinese pol-
icy is not as presented, and is in fact defensible. This
opposes the seed post, so that the author?s global po-
sition for the main topic is ?disagree?. The opin-
ion expressed in Reply 2, however, is not a response
to the seed post: it relates to Reply 1. It indicates
that the author of Reply 2 disagrees with the opinion
made in Reply 1, and thus indirectly implies agree-
ment with the seed post. From this example, we can
see that it is hard to infer the global position of Re-
ply 2?s author only from the text of their post. How-
ever, we can exploit information in the local context,
such as the relationship between Replies 1 and 2, to
indirectly infer the author?s opinion with regard to
the seed post.
Motivated by this observation, we propose a
three-step method for detecting participants? global
agreement or disagreement positions by exploiting
local information in the posts within the debate.
First, we build a local classifier to determine whether
a pair of posts agree with each other or not. Sec-
ond, we aggregate over posts for each pair of partic-
ipants in one discussion to determine whether they
agree with each other. Third, we infer the global po-
sitions of participants with respect to the main topic,
so that participants can be classified into two classes:
1Spelling of the posts is per original on the website.
agree and disagree. The advantage of our proposed
method is that it builds a unified framework which
enables the classification of participants? local and
global positions in online debates; the aggregation
of local estimates also tends to reduce error in the
global classification.
In order to evaluate the performance of our
method, we have conducted experiments on data sets
collected from two online debate forums. We have
explored the use of sentiment, emotional and du-
rational features for automatic agreement and dis-
agreement classification, and our feature analysis
suggests that they can significantly improve the per-
formance of baselines using only word features. Ex-
perimental results have also demonstrated that ag-
gregating local positions over posts yields better per-
formance for identifying users? global positions on
an issue.
The rest of the paper is organised as follows. Sec-
tion 2 discusses previous work on agreement and
disagreement classification. Section 3 presents our
proposed method for both local and global position
classification, which we validate in Section 4 with
experiments on two real-world data sets. Section 5
concludes the paper and discusses possible direc-
tions for future work.
2 Related Work
Previous work in automatic identification of agree-
ment and disagreement has mainly focused on
analysing conversational speech. Thomas et al
(2006) presented a method based on support vector
machines to determine whether the speeches made
by participants represent support or opposition to
proposed legislation, using transcripts of U.S. con-
gressional floor debates. This method showed that
the classification of participants? positions can be
improved by introducing the constraint that a sin-
gle speaker retains the same position during one
debate. Wang et al (2011) presented a condi-
tional random field based approach for detecting
agreement/disagreement between speakers in En-
glish broadcast conversations. Galley et al (2004)
proposed the use of Bayesian networks to model
pragmatic dependencies of previous agreement or
disagreement on the current utterance. These differ
from our work in that the speakers are assumed to
62
be present all the time during the conversation, and
therefore, user speech models can be built, and their
dependencies can be explored to facilitate agreement
and disagreement classification. Our aggregation
technique does, however, presuppose consistency of
opinions, in a similar way to Thomas et al (2006).
There has been other related work which aims
to analyse informal texts for opinion mining and
(dis)agreement classification in online discussions.
Agrawal et al (2003) described an observation that
reply-to activities always show disagreement with
previous authors in newsgroup discussions, and pre-
sented a clustering approach to group users into two
parties: support and opposition, based on reply-
to graphs between users. Murakami and Raymond
(2010) proposed a method for deriving simple rules
to extract opinion expressions from the content of
posts and then applied a similar graph clustering al-
gorithm for partitioning participants into supporting
and opposing parties. By combining both text and
link information, this approach was demonstrated to
outperform the method proposed by Agrawal et al
(2003). Due to the nature of clustering mechanisms,
the output of these methods are two user parties, in
each of which users most agree or disagree with each
other. However, users? positions in the two parties
do not necessarily correspond to the global position
with respect to the main issue in a debate, which
is our interest here. Balasubramanyan and Cohen
(2011) proposed a computational method to classify
sentiment polarity in blog comments and predict the
polarity based on the topics discussed in a blog post.
Finally, Somasundaran and Wiebe (2010) explored
the utility of sentiment and arguing opinions in ideo-
logical debates and applied a support vector machine
based approach for classifying stances of individual
posts. In our work, we focus on classifying people?s
global positions on a main issue by exploiting and
aggregating local positions expressed in individual
posts.
3 Our Proposed Method
To infer support or opposition positions with respect
to the seed post, we propose a three-step method.
First, we consider each post in its local context and
build a local classifier to classify each pair of posts
as agreeing with each other or not. Second, we ag-
gregate over posts for each pair of participants in
one discussion to determine whether they agree with
each other. Third, we infer global positions of par-
ticipants with respect to the seed post based on the
thread structure.
3.1 Classifying Local Positions between Posts
To classify local positions between posts, we need to
extract the reply-to pairs of posts from the threading
structure. The web forums we work with tend not to
present thread structure, so we consider two types
of reply-to relationships between individual posts.
When a post explicitly quotes the content from an
earlier post, we create an explicit link between the
post and the quoted post. When a post does not
contain a quote, we assume that it is a reply to the
preceding post, and thus create an implicit link be-
tween the two adjacent posts. After obtaining ex-
plicit/implicit links, we build a classifier to classify
each pair of posts as agreeing or disagreeing with
each other.
3.1.1 Features
To build a classifier for identifying local agree-
ment and disagreement, we explored different types
of features from individual posts with the aim to un-
derstand which have predictive power for our agree-
ment/disagreement classification task.
Words We extract unigram and bigram features
to capture the lexical information from each post.
Since many words are topic related and might be
used by both parties in a debate, we mainly use un-
igrams for adjectives, verbs and adverbs because
they have been demonstrated to possess discrimi-
native power for sentiment classification (Benamara
et al, 2007; Subrahmanian and Regorgiato, 2008).
Typical examples of such unigrams include ?agree?,
?glad?, ?indeed?, and ?wrong?. In addition, we ex-
tract bigrams to capture phrases expressing argu-
ments, for example, ?don?t think? and ?how odd?
could indicate disagreement, while ?I concur? could
indicate agreement.
Sentiment features In order to detect sentiment
opinions, we use a sentiment lexicon referred to as
SentiWordNet (Baccianella et al, 2010). This lexi-
con assigns a positive and negative score to a large
number of words in WordNet. For example, the
63
A
B
D
C
C
B
seed
(a) Estimate P (y|x) for each
post
A
B
D
CL(B,C)=disagree
L(A,B)=agreeL(A,C)=disagree
L(C,D)=disagree
(b) Aggregate these over
pairs of users to get local
agreement L(m,n)
A
B
D
Cagree disagree
agree
(c) Infer the global position
of each user by walking the
tree
Figure 1: Local agreement/disagreement and participants? global positions. We first estimate P (y|xi, xj), the prob-
ability of two posts xi and xj being in agreement or disagreement with each other, then aggregate over posts to
determine L(m,n), the position between two users. Finally, we infer the global position for any user by walking this
graph back to the seed.
word ?odd? has a positive score of 1.125, and a neg-
ative score of 1.625. To aggregate the sentiment
polarity of each post, we calculate the overall pos-
itive and negative scores for all the words that can
be found in SentiWordNet, and use these two sums
as two features for each post.
Emotional features We observe that personal
emotions could be a good indicator of agree-
ment/disagreement expression in online debates.
Therefore, we include a set of emotional features,
including occurrences of emoticons, number of cap-
ital letters, number of foul words, number of excla-
mation marks, and number of question marks con-
tained in a post. Intuitively, use of foul words might
be linked to emotion in a visceral way, which if used,
could be a sign of strong argument and disagree-
ment. The presence of question marks could be in-
dicative of disagreement, and the use of exclama-
tion marks and capital letters could be an emphasis
placed on opinions.
Durational features Inspired by conversation
analysis (Galley et al, 2004; Wang et al, 2011), we
extract durational features, such as the length of a
post in words and in characters. These features are
analogous to the ones used to capture the duration of
a speech for conversation analysis. Intuitively, peo-
ple tend to respond with a short post if they agree
with a previous opinion. Otherwise, when there is a
strong argument, people tend to use a longer post to
state and defend their own opinions. Moreover, we
also consider the time difference between adjacent
posts as additional features. Presumably, when a de-
bate is controversial, participants would be actively
involved in the discussions, and the thread would un-
fold quickly over time. Thus, the time difference be-
tween adjacent posts would be smaller in the debate.
3.1.2 Classification Model
We use logistic regression as the basic classi-
fier for local position classification because it has
been demonstrated to provide good predictive per-
formance across a range of text classification tasks,
such as document classification and sentiment anal-
ysis (Zhang and Oles, 2001; Pan et al, 2010). In ad-
dition to the predicted class, logistic regression can
also generate probabilities of class memberships,
64
which are quite useful in our case for aggregating
local positions between participants.
Formally, logistic regression estimates the condi-
tional probability of y given x in the form of
Pw(y = ?1|x) =
1
1 + e?ywTx
, (1)
where x is the feature vector, y is the class label, and
w ? Rn is the weight vector. Given the training data
{xi, yi}li=1, xi ? R
n, yi ? {1,?1}, we consider the
following form of regularised logistic regression
min
w
f(w) =
1
2
wTw + C
l?
i=1
log
(
1 + e?yiw
Txi
)
,
(2)
which aims to minimise the regularised negative log-
likelihood of the training data. Above, wTw/2 is
used as a regularisation term to achieve good gen-
eralisation abilities. Parameter C > 0 is a penalty
factor which controls the balance of the two terms
in Equation 2. The above optimisation problem can
be solved using different iterative methods, such as
conjugate gradient and Newton methods (Lin et al,
2008). As a result, an optimal estimate of w can be
obtained.
Given a representation of a post xm, we can use
Equation 1 to estimate its membership probabil-
ity of belonging to each class, P (agree|xm) and
P (disagree|xm), respectively.
3.2 Estimating Local Positions between
Participants
After obtaining local position between posts, this
step aims to aggregate over posts to determine
whether each pair of participants agree with each
other. The intuition is that, in one threaded dis-
cussion, most of the participants tend to retain their
positions in the course of their arguments. This as-
sumption holds for the ground-truth annotations we
have obtained in our data sets. Given local predic-
tions obtained from the previous step, we adopt the
weighted voting scheme to determine the local posi-
tion for each pair of participants. Specifically, given
a pair of users i and j, we aggregate over all the
reply-to posts between them to calculate the overall
agreement score r(i, j) as follows:
r(i, j) =
N(i,j)?
k=1
P (agree|xk)?
N(i,j)?
k=1
P (disagree|xk).
(3)
Here, N(i, j) denotes the number of post exchanges
between users i and j, and r(i, j) indicates the de-
gree of agreement between users i and j. Let L(i, j)
denote the local position between two users i and
j. If r(i, j) > 0, we have L(i, j) = agree, that is,
user i agrees with user j. Otherwise, if r(i, j) ? 0,
we have L(i, j) = disagree, that is, user i disagrees
with user j.
Let us consider the example in Figure 1(a) and
1(b). There are two posts exchanged between users
B and C. For each of these posts, two probabilities
of class membership can be obtained:
P (agree|x1) = 0.1, P (disagree|x1) = 0.9,
P (agree|x2) = 0.3, P (disagree|x2) = 0.7.
Then we can calculate the agreement score r(B,C)
between users B and C by aggregating over two
posts, that is, r(B,C) = (0.1+0.3)? (0.9+0.7) =
?1.2 < 0. We can conclude that user B dis-
agrees with user C in the threaded discussion and
that L(B,C) = disagree.
3.3 Identifying Participants? Global Positions
After estimating local positions between partici-
pants, we now can infer a participant?s global sup-
port or opposition position with regards to the seed
post. For this purpose, a thread structure must be
considered. A thread begins with a seed post, which
is further followed by other response posts. Of these
responses, many employ a quote mechanism to ex-
plicitly state which post they reply to, whereas oth-
ers are assumed to be in response to the most recent
post in the thread. We construct a tree-like thread
structure by examining all the posts in a thread and
determining the parent of each post. Then, travers-
ing through the thread structure from top to bottom
allows us to infer the global position of each user
with respect to the seed post. When there is more
than one path from the seed to a user, the shortest
path is used to infer the user?s global position on the
main issue.
We illustrate this inference process using Figure
1, an example thread with four users and six posts.
65
Let L(m,n) denote the local position between two
users m and n. In the figure, the local position be-
tween user B and user A (the author of the seed
post), L(A,B), is in agreement, while users B and
C, A and C, as well as C and D each disagree.
Walking the shortest path between D and the seed
in Figure 1(a), we have L(C,D) = disagree and
L(A,C) = disagree, so we can infer that the global
position between user D and user A is in agreement.
That is, user D agrees with the seed post. Had the
local position between user A and user C, L(A,C),
been in agreement, then we would have concluded
that user D disagrees with the seed post.
4 Experiments
In this section, we describe our experiments on two
real-world data sets and report our experimental re-
sults for local and global (dis)agreement classifica-
tion.
4.1 Data Sets
We used two data sets to evaluate our pro-
posed method in our experiments. They were
crawled from the U.S. Message Board (www.
usmessageboard.com) and the Political Forum
(www.politicalforum.com). The two data
sets are referred to as usmb and pf, respectively, in
our discussion. The detailed characteristics of the
two data sets are given in Table 1.
Table 1: Characteristics of data sets
usmb pf
# of threads 88 33
# of posts 818 170
# of participants 270 103
Mean # of posts per thread 9.3 5.2
Mean # of participants per thread 3.1 3.1
Mean # of posts per participant 3.0 1.7
For the evaluation, each post was labelled with
two annotations. The first was a global annotation
with respect to the thread?s seed post, and the other
was a local annotation with respect to the immediate
parent. Seed posts themselves were not annotated,
nor were they classified by our algorithms.
Global annotations were made by two postgrad-
uate students. Each was instructed to read all the
posts in a thread, then label each post with agree if
the author agreed with the seed post; disagree if they
disagreed; or neutral if opinions were mixed or un-
clear. The annotators used training data until they
reached 85% agreement, then annotated posts sepa-
rately. At no time were they allowed to confer. Lo-
cal annotations were reverse-engineered from these
global annotations. The ratio of posts annotated as
agree to those as disagree is about 2 to 1 on both
datasets.
For our proposed three-stage method, local an-
notations were taken as input to train the classi-
fier and then used as ground truth to evaluate the
performance of local agreement/disagreement clas-
sification, while the global annotations were only
used to evaluate our final accuracy of global agree-
ment/disagreement identification. In contrast, the
baseline classifiers that we compare against for
global classification were directly trained and evalu-
ated using global annotations.
4.2 Evaluation Metrics
We used two evaluation metrics to evaluate the per-
formance of agreement/disagreement classification.
The first metric is accuracy, which is computed as
the percentage of correctly classified examples over
all the test data:
accuracy =
|{x : x ? Dtest
?
h(x) = y}|
|Dtest|
,
where Dtest denotes the test data, y is the ground
truth annotation label and h(x) is the predicted class
label.
Accuracy can be biased in situations with un-
even division between classes, so we also evaluate
our classifiers with the F-measure. For each class
i ? {agree, disagree}, we first calculate precision
P (i) and recall R(i), and the F-measure is computed
as
F1(i) =
2P (i)R(i)
P (i) +R(i)
.
For our binary task, we report the average F-measure
over both classes.
4.3 Local Agree/Disagree Classification
In our experiments, we used the implementation
of L2-regularised logistic regression in Fan et al
(2008) as our local classifier. For each data set,
66
Table 2: Classification performance for local (dis)agreement
usmb pf
Accuracy F-measure Accuracy F-measure
Naive Bayes, all features 0.46 0.42 0.52 0.51
SVM, all features 0.56 0.60 0.55 0.52
Logistic regression, all features 0.62 0.65 0.68 0.77
Table 3: Feature analysis for local (dis)agreement using logistic regression
usmb pf
Accuracy F-measure Accuracy F-measure
words 0.50 0.55 0.55 0.63
words, sentiment 0.53 0.59 0.61 0.71
words, sentiment, emotional 0.54 0.51 0.55 0.65
words, sentiment, durational 0.58 0.61 0.64 0.72
words, sentiment, emotional, durational 0.62 0.65 0.68 0.77
we used 70% of posts as training and the other
30% were held out for testing. We compared reg-
ularised logistic regression against two baselines:
naive Bayes and support vector machines (SVMs),
which have been used for (dis)agreement classifica-
tion in previous works (Thomas et al, 2006; Soma-
sundaran and Wiebe, 2010). For SVMs, we used
the toolbox LIBSVM in Chang and Lin (2011) to
implement the classification and probability estima-
tion. We tuned the parameter C in regularised logis-
tic regression and SVM, using cross-validation on
the training data, and thereafter the optimal C was
used on the test data for evaluation.
Table 2 compares the local classification accuracy
of the three methods on data sets usmb and pf, re-
spectively. We can see from the table that logistic
regression outperforms naive Bayes and SVM on
the two evaluation metrics for local classification.
Although logistic regression and SVM have been
shown to yield comparable performance on some
text categorisation tasks Li and Yang (2003), in
our problem, regularised logistic regression was ob-
served to outperform SVM for local (dis)agreement
classification.
Experiments were also carried out to investigate
how the performance of local classification would be
changed by using different types of features. Table 3
shows the classification accuracy of logistic regres-
sion using different types of features on the two data
sets. We can see from the table that using both words
and sentiment features can improve the performance
as compared to using only words features. On the
usmb dataset, adding emotional features slightly im-
proves the accuracy but degrades F-measure, while
on the pf dataset, it degrades on accuracy and F-
measure. In addition, durational features substan-
tially improve the classification performance on the
two metrics. Overall, the highest classification ac-
curacy and F-measure can be achieved by using all
four types of features.
4.4 Global Support/Opposition Identification
We also conducted experiments to validate the ef-
fectiveness of our proposed method for global posi-
tion identification. Table 4 reports the performance
of global classification using the three methods on
the two data sets. Classifiers ?without aggregation?
were trained directly on global annotations, with-
out considering local positions at all; those ?with
aggregation? were developed with our three-stage
method, estimating global positions by aggregating
local positions L(m,n).
As before, logistic regression generally outper-
forms SVM or naive Bayes classifiers, although
SVM does well on usmb when aggregation (via
L(m,n)) is used. Although SVM scores well for
67
Table 4: Classification performance for global (dis)agreement
usmb pf
Accuracy F-measure Accuracy F-measure
Without aggregation
Naive Bayes, all features 0.42 0.41 0.48 0.47
SVM, all features 0.62 0.46 0.68 0.40
Logistic regression, all features 0.60 0.63 0.65 0.77
With aggregation
Naive Bayes, all features 0.54 0.67 0.65 0.70
SVM, all features 0.64 0.77 0.48 0.60
Logistic regression, all features 0.64 0.77 0.68 0.76
classification accuracy without aggregation, it has
degraded and classifies everything as the majority
class in these cases. The F-measure is correspond-
ingly poor due to a low recall. This observation is
consistent with the findings reported in Agrawal et
al. (2003).
In all cases ? bar logistic regression on the pf set
? aggregation of local classifications improves the
performance of global classification. This is more
marked in the usmb data set, which has slightly
more exchanges between each pair of users (mean
1.33 per pair per topic, vs. 1.19 for the pf data
set) and therefore more potential for aggregation.
We believe that this improvement is because local
classification is sometimes error prone, especially
when opinions are not expressed clearly in individ-
ual posts. If so, and assuming that users tend to re-
tain their stances within a debate, aggregation can
?wash out? local classification errors.
5 Conclusion and Future Work
In this paper, we have proposed a new method for
identifying participants? agreement or disagreement
on an issue by exploiting local information con-
tained in individual posts. Our proposed method
builds a unified framework which enables the clas-
sification of participants? local and global positions
in online debates. To evaluate the performance of
our proposed method, we conducted experiments on
two real-world data sets collected from two online
debate forums. Our experiments have shown that
regularised logistic regression is useful for this type
of task; it has a built-in automatic feature selection
by assigning a coefficient to each specific feature,
and directly estimates probabilities of class mem-
berships, which is quite useful for aggregating local
positions between users. Our feature analysis has
suggested that using sentiment, emotional and du-
rational features can significantly improve the per-
formance over only using word features. Experi-
mental results have also shown that, for identifying
users? global positions on an issue, aggregating lo-
cal positions over posts results in better performance
than no-aggregation baselines and that more benefit
seems to accrue as users exchange more posts.
We consider extending this work along several di-
rections. First, we would like to examine what other
factors would have predictive power in online de-
bates and thus could be utilised to improve the per-
formance of agreement/disagreement classification.
Second, we have so far focused on classifying users?
positions into two categories: agree and disagree.
However, there do exist a portion of posts falling into
the neutral category; that means posts/users do not
express any position towards an issue. We will ex-
plore how to extend our computational framework to
classify the neutral class. Finally, in online debates,
it is not uncommon to have off-topic or topic-drift
posts, especially for long threaded discussions. Off-
topic posts are the ones totally irrelevant to the main
issue being discussed, and topic-drift posts usually
exist when the topic of a debate has shifted over
time. Taking these posts into consideration would
increase the difficulty of automatic agreement and
disagreement classification, and therefore it is an-
other important issue we plan to investigate.
68
References
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social bahavior. In Pro-
ceedings of the 12th International World Wide Web
Conference, pages 529?535, Budapest, Hungary, May.
Stefano Baccianella, Andrea Esuli, , and Fabrizio Sebas-
tiani. 2010. SENTIWORDNET 3.0: An enhanced
lexical resource for sentiment analysis and opinion
mining. In Proceedings of the 7th Conference on In-
ternatinal Language Resources and Evaluation, pages
2200?2204, Valletta, Malta, May.
Ramnath Balasubramanyan and William W. Cohen.
2011. What pushes their buttons? Predicting com-
ment polarity from the content of political blog posts.
In Proceedings of the ACL Workshop on Language in
Social Media, pages 12?19, Porland, Oregon, USA,
June.
Farah Benamara, Carmine Cesarano, Antonio Picariello,
Diego Reforgiato, and V. S. Subrahmanian. 2007.
Sentiment analysis: Adjectives and adverbs are better
than adjectives alone. In Proceedings of the Interna-
tional AAAI Conference on Weblogs and Social Media,
Boulder, CO, USA, March.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2(27):1?
27.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech: Use
of Bayesian networks to model pragmatic dependen-
cies. In Proceedings of the 42nd Meeting of the Asso-
ciation for Computational Linguistics, pages 669?676,
Barcelona, Spain, July.
Fan Li and Yiming Yang. 2003. A loss function analy-
sis for classification methods in text categorisation. In
Proceedings of the 20th International Conference on
Machine Learning, pages 472?479, Washington, DC,
USA, July.
Chih-Jen Lin, Ruby C. Weng, and S. Sathiya Keerthi.
2008. Trust region Newton method for large-scale lo-
gistic regression. Journal of Machine Learning Re-
search, 9:627?650.
Akiko Murakami and Rudy Raymond. 2010. Support or
oppose? Classifying positions in online debates from
reply activities and opinion expressions. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics, pages 869?875, Beijing, China,
August.
Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain senti-
ment classification via spectral feature alignment. In
Proceedings of the 19th International World Wide Web
Conference, pages 751?760, Raleigh, NC, USA, April.
Swapna Somasundaran and Janyce Wiebe. 2010. Recog-
nizing stances in ideological on-line debates. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Com-
putational Approaches to Analysis and Generation of
Emotion in Text, pages 116?124, Los Angeles, CA,
USA, June.
V. S. Subrahmanian and Diego Regorgiato. 2008.
AVA: Adjective-verb-adverb combinations for senti-
ment analysis. Intelligent Systems, 23(4):43?50.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from con-
gressional floor-debate transcripts. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, pages 327?335, Sydney, Australia,
July.
Wen Wang, Sibel Yaman, Kristin Precoda, Colleen
Richey, and Geoffrey Raymond. 2011. Detection
of agreement and disagreement in broadcast conver-
sations. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics, pages
374?378, Porland, Oregon, USA, June.
Tong Zhang and Frank J. Oles. 2001. Text categorisation
based on regularised linear classification methods. In-
formation Retrieval, 4(1):5?31.
69
