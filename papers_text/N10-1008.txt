Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 64?72,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Dialogue-Oriented Review Summary Generation for Spoken Dialogue 
Recommendation Systems 
 
 
Jingjing Liu, Stephanie Seneff, Victor Zue 
MIT Computer Science & Artificial Intelligence Laboratory 
32 Vassar Street, Cambridge, MA 02139 
{jingl, seneff, zue}@csail.mit.edu 
 
 
 
Abstract 
In this paper we present an opinion summari-
zation technique in spoken dialogue systems. 
Opinion mining has been well studied for 
years, but very few have considered its appli-
cation in spoken dialogue systems. Review 
summarization, when applied to real dialogue 
systems, is much more complicated than pure 
text-based summarization. We conduct a sys-
tematic study on dialogue-system-oriented 
review analysis and propose a three-level 
framework for a recommendation dialogue 
system. In previous work we have explored a 
linguistic parsing approach to phrase extrac-
tion from reviews. In this paper we will de-
scribe an approach using statistical models 
such as decision trees and SVMs to select the 
most representative phrases from the ex-
tracted phrase set. We will also explain how 
to generate informative yet concise review 
summaries for dialogue purposes. Experimen-
tal results in the restaurant domain show that 
the proposed approach using decision tree al-
gorithms achieves an outperformance of 13% 
compared to SVM models and an improve-
ment of 36% over a heuristic rule baseline. 
Experiments also show that the decision-tree-
based phrase selection model can achieve ra-
ther reliable predictions on the phrase label, 
comparable to human judgment. The pro-
posed statistical approach is based on do-
main-independent learning features and can 
be extended to other domains effectively. 
1 Introduction 
Spoken dialogue systems are presently available 
for many purposes, such as weather inquiry (Zue 
et al, 2000), bus schedules and route guidance 
(Raux et al, 2003), customer service (Gorin et al, 
1997), and train timetable inquiry (Eckert et al, 
1993). These systems have been well developed 
for laboratory research, and some have become 
commercially viable. 
The next generation of intelligent dialogue sys-
tems is expected to go beyond factoid question 
answering and straightforward task fulfillment, by 
providing active assistance and subjective recom-
mendations, thus behaving more like human 
agents. For example, an intelligent dialogue sys-
tem may suggest which airline is a better choice, 
considering cost, flight duration, take-off time, 
available seats, etc.; or suggest which digital cam-
era is the most popular among teenagers or highest 
rated by professional photographers; or which res-
taurant is a perfect spot for a semi-formal business 
meeting or a romantic date. 
Luckily, there are enormous amounts of reviews 
published by general users on the web every day. 
These are perfect resources for providing subjec-
tive recommendations and collective opinions. If 
there exists a systematic framework that harvests 
these reviews from general users, extracts the es-
sence from the reviews and presents it appropriate-
ly in human-computer conversations, then we can 
enable dialogue systems to behave like a human 
shopping assistant, a travel agent, or a local friend 
who tells you where to find the best restaurant.  
Summarization from online reviews, therefore, 
plays an important role for such dialogue systems. 
There have been previous studies on review analy-
sis for text-based summarization systems (Mei et 
al., 2007; Titov and McDonald, 2008a; Branavan 
et al, 2008). Mixture models and topic models are 
used to predict the underlying topics of each doc-
ument and generate a phrase-level summary. An 
aspect rating on each facet is also automatically 
64
learned with statistical models (Snyder and Barzi-
lay, 2007; Titov and McDonald, 2008b; Baccia-
nella et al, 2009). These approaches are all very 
effective, and the review databases generated are 
well presented.  
So the first thought for developing a recom-
mendation dialogue system is to use such a cate-
gorized summary in a table-lookup fashion. For 
example, a dialogue system for restaurant recom-
mendations can look up a summary table as exem-
plified in Table 1, and generate a response 
utterance from each row: ?Restaurant A has good 
service and bad food; restaurant B has good ser-
vice and good food; restaurant C has great service 
and nice atmosphere; restaurant D has poor service 
and reasonable price.?  
 
Restaurant Summary 
A Good service, bad food, 
B Good service, good food 
C Great service, nice atmosphere 
D Poor service, reasonable price 
Table 1. A partial table of categorization-based review 
summaries. 
 
Such a dialogue system is, however, not very 
informative. First of all, there is too much redun-
dancy. Long utterances repeated in the same pat-
tern on the same topic are quite boring, and the 
information density is very low. Second, such a 
summary is too coarse-grained to be helpful. A 
user querying a restaurant recommendation system 
expects more fine-grained information such as 
house specials, wine selections and choices on 
desserts rather than just general ?good food.?  
In contrast to a ?text? summarization system, the 
textual space in a dialogue turn is often very li-
mited. Speech is inherently serial, and it cannot be 
skipped and scanned easily. A dialogue system 
which speaks long diatribes in each single conver-
sation turn would likely not be well received. We 
believe that the best unit of review summary for 
dialogue purposes is short succinct phrases (e.g., 
?amazing sauce? and ?romantic ambiance?) which 
are catalogued into categories to answer high-level 
questions such as ?How is the food at this restau-
rant?? or ?What about the atmosphere?? Also, 
domain-specific phrases are greatly needed to 
generate correct answers to specific queries, such 
as ?Which restaurants have fresh sushi?? and 
?Show me restaurants that serve fantastic cock-
tails.? 
Thus, the generally used correlated topic mod-
els, although very effective in text-based systems, 
are not quite suitable for interactive dialogue sys-
tems. The missing piece is an HCI-oriented (hu-
man computer interaction), fine-grained, 
informative yet concise review summarization. A 
good recommendation system should be able to 
provide sufficient yet specific information to help 
users choose a restaurant, a movie or a consumer 
product. Figure 1 shows an example of a conversa-
tion with a good recommendation system, which 1) 
provides a generalized yet high-density review 
summary in human-friendly dialogue; 2) provides 
both a coarse-grained summary (e.g., ?authentic 
food?) and fine-grained information such as house 
specialties.  
 
U: Are there any French restaurants in Cambridge? 
S: There are five French restaurants in Cambridge. 
Most of them are on Backbay Street. And most of 
them have attentive service. 
U: Show me the ones that have nice ambiance. 
S: Two of them have nice ambiance, Pierrot Bistrot 
Francais and Coriander. 
U: How about the food? 
S: Pierrot Bistrot Francais has authentic food, 
fresh salad, and absolutely delicious spaghetti 
carbonara. The food at Coriander is below aver-
age. It has very greasy chicken tikka masala and 
dry bread.  
Figure 1. Example of a conversation with a good rec-
ommendation dialogue system (?U? denotes the user 
and ?S? denotes the dialogue system. The responses to 
the user queries are produced by our system and the 
actual dialogue was an illustration of system capacities). 
 
Therefore, the task of developing recommenda-
tion dialogue systems is decomposed into three 
problems: 1) how to extract context-related phras-
es, both coarse-grained and fine-grained, from 
online reviews; 2) how to select a representative 
set from the extracted phrases to create an infor-
mative yet concise dialogue-oriented summary 
database; 3) how to generate human-friendly di-
alogue responses from the review summary data-
base.  
To tackle these problems, we propose a three-
level framework. In previous work (Liu and Seneff, 
2009), we explored the first level by proposing a 
linguistic parse-and-paraphrase paradigm for re-
65
view phrase extraction. In this paper, we address 
the second problem: dialogue-oriented review 
summary generation. We propose an automatic 
approach to classifying high/low informative 
phrases using statistical models. Experiments con-
ducted on a restaurant-domain dataset indicate that 
the proposed approach can predict phrase labels 
consistently with human judgment and can gener-
ate high-quality review summaries for dialogue 
purposes.  
The rest of the paper is organized as follows: 
Section 2 gives an overview of the three-level 
framework for recommendation dialogue systems. 
In Section 3, we explain the proposed approach to 
dialogue-oriented review summary generation. 
Section 4 provides a systematic evaluation of the 
proposed approach, and Section 5 gives a further 
discussion on the experimental results. Section 6 
summarizes the paper as well as pointing to future 
work. 
2 System Overview 
The three-level framework of a review-summary-
based recommendation dialogue system is shown 
in Figure 2. The bottom level is linguistic phrase 
extraction. In previous work (Liu and Seneff, 
2009), we employed a probabilistic lexicalized 
grammar to parse review sentences into a hierar-
chical representation, which we call a linguistic 
frame. From the linguistic frames, phrases are ex-
tracted by capturing a set of adjective-noun rela-
tionships. Adverbs and negations conjoined with 
the adjectives are also captured. We also calcu-
lated a numerical score for sentiment strength for 
each adjective and adverb, and further applied a 
cumulative offset model to assign a sentiment 
score to each phrase. 
The approach relies on linguistic features that 
are independent of frequency statistics; therefore it 
can retrieve very rare phrases such as ?very greasy 
chicken tikka masala? and ?absolutely delicious 
spaghetti carbonara?, which are very hard to derive 
from correlated topic models. Experimental results 
showed that the linguistic paradigm outperforms 
existing methods of phrase extraction which em-
ploy shallow parsing features (e.g., part-of-speech). 
The main contribution came from the linguistic 
frame, which preserves linguistic structure of a 
sentence by encoding different layers of semantic 
dependencies. This allows us to employ more so-
phisticated high-level linguistic features (e.g., long 
distance semantic dependencies) for phrase extrac-
tion. 
However, the linguistic approach fails to distin-
guish highly informative and relevant phrases 
from uninformative ones (e.g., ?drunken husband?, 
?whole staff?). To apply these extracted phrases 
within a recommendation dialogue system, we 
have to filter out low quality or irrelevant phrases 
and maintain a concise summary database. This is 
the second level: dialogue-oriented review sum-
mary generation.  
 
 
 
Figure 2. Three-level framework of review-based rec-
ommendation dialogue systems. 
 
The standard of highly informative and relevant 
phrases is a very subjective problem. To gain in-
sights on human judgment on this, the first two 
authors separately labeled a set of review-related 
phrases in a restaurant domain as ?good? and ?bad? 
summary phrases. We surveyed several subjects, 
all of whom indicated that, when querying a dialo-
gue system for information about a restaurant, 
they care much more about special dishes served 
in this restaurant than generic descriptions such as 
?good food.? This knowledge informed the annota-
tion task: to judge whether a phrase delivered by a 
dialogue recommendation system would be help-
66
ful for users to make a decision. Surprisingly, al-
though this is a difficult and subjective problem, 
the judgment from the two annotators is substan-
tially consistent. By examining the annotations we 
observed that phrases such as ?great value? and 
?good quality? are often treated as ?uninformative? 
as they are too common to be representative for a 
particular product, a restaurant or a movie. Phrases 
with neutral sentiment (e.g., ?green beans? and 
?whole staff?) are often considered as uninforma-
tive too. Phrases on specific topics such as house 
specialties (e.g., ?absolutely delicious spaghetti 
carbonara?) are what the annotators care about 
most and are often considered as highly relevant, 
even though they may have only been seen once in 
a large database.  
Driven by these criteria, from each phrase we 
extract a set of statistical features such as uni-
gram/bigram probabilities and sentiment features 
such as sentiment orientation degree of the phrase, 
as well as underlying semantic features (e.g., 
whether the topic of the phrase fits in a domain-
specific ontology). Classification models such as 
SVMs and decision tree algorithms are then 
trained on these features to automatically classify 
high/low informative phrases. Phrases identified 
as ?good? candidates are further pruned and cata-
logued to create concise summaries for dialogue 
purposes. 
After generating the review summary database, 
the third level is to modify the response generation 
component in dialogue systems to create genera-
lized and interactive conversations, as exemplified 
in Figure 1. The utterance from users is piped 
through speech recognition and language under-
standing. The meaning representation is then sent 
to the dialogue management component for re-
view-summary database lookup. A response is 
then generated by the language generation compo-
nent, and a speech utterance is generated by the 
synthesizer and sent back to the user. The dialogue 
system implementation is beyond the scope of this 
paper and will be discussed later in a separate pa-
per. 
3 Dialogue-oriented Review Summary 
Generation 
Given an inquiry from users, the answer from a 
recommendation system should be helpful and 
relevant. So the first task is to identify a phrase as 
?helpful? or not. The task of identifying a phrase as 
informative and relevant, therefore, is defined as a 
classification problem: 
 =  ? ? #? = ?  %#%&%=1          (1) 
where y is the label of a phrase, assigned as ?1? if 
the phrase is highly informative and relevant, and 
?-1? if the phrase is uninformative. #? is the feature 
vector extracted from the phrase, and  ?  is the 
coefficient vector.  
We employ statistical models such as SVMs 
(Joachims, 1998) and decision trees (Quinlan, 
1986) to train the classification model. For model 
learning, we employ a feature set including statis-
tical features, sentiment features and semantic 
features.  
Generally speaking, phrases with neutral senti-
ment are less informative than those with strong 
sentiment, either positive or negative. For example, 
?fried seafood appetizer?, ?baked halibut?, ?elec-
tronic bill? and ?red drink? do not indicate whether 
a restaurant is worth trying, as they did not express 
whether the fried seafood appetizer or the baked 
halibut are good or bad. Therefore, we take the 
sentiment score of each phrase generated from a 
cumulative offset model (Liu and Seneff, 2009) as 
a sentiment feature. Sentiment scores of phrases 
are exemplified in Table 2 (on a scale of 1 to 5).  
 
Phrase Sc. Phrase Sc. 
really welcoming 
atmosphere 
4.8 truly amazing flavor 4.6 
perfect portions  4.4 very tasty meat 4.3 
busy place 3.1 typical Italian restaurant 3.1 
a little bit high 
price 
2.2 pretty bad soup 1.8 
sloppy service 1.8 absolute worst service 1.4 
Table 2. Examples of sentiment scores of phrases. 
 
We also employ a set of statistical features for 
model training, such as the unigram probability of 
the adjective in a phrase, the unigram probability 
of the noun in a phrase, the unigram probability of 
the phrase and the bigram probability of the adjec-
tive-noun pair in a phrase.  
Statistical features, however, fail to reveal the 
underlying semantic meaning of phrases. For ex-
ample, phrases ?greasy chicken tikka masala? and 
?drunken husband? have the same n-gram proba-
bilities in our corpus (a single observation), but 
67
they should certainly not be treated as the same. 
To capture the semantic meanings of phrases, we 
first cluster the topics of phrases into generic se-
mantic categories. The language-model based al-
gorithm is given by: 
 
        '(() | (%) = ? '(() |+) ? '(+|(%)+?.  
                 =  ? '(+ ,())'(+) ?
'(+ ,(%)
'((%)+?.
  
                 =  1'((%)
? 1
'(+) ? '(+, ()) ? '(+, (%)+?.          (2) 
where A represents the set of all the adjectives in 
the corpus. We select a small set of initial topics 
with the highest frequency counts (e.g., ?food?, 
?service? and ?atmosphere?). For each of the other 
topics tc  (e.g., ?chicken?, ?waitress? and ?d?cor?), 
we calculate its similarity with each initial topic (% 
based on the bigram probability statistics. For 
those topics with conditional probability higher 
than a threshold for an initial topic (%, we assign 
them to the cluster of (%. We use this as a semantic 
feature, e.g., whether the topic of a phrase belongs 
to a generic semantic category. Table 3 gives some 
clustering examples. 
 
Category Relevant Topics 
food 
appetizer, beer, bread, fish, fries, ice 
cream, margaritas, menu, pizza, pasta, 
rib, roll, sauce, seafood, sandwich, 
steak, sushi, dessert, cocktail, brunch 
service waiter, staff, management, server, hostess, chef, bartender, waitstaff 
atmosphere d?cor, ambiance, music, vibe, setting, environment, crowd 
price bill, pricing, prices 
Table 3. Topic to semantic category clustering. 
 
This language-model-based method relies on 
bigram probability statistics and can well cluster 
highly frequent topics. Categories such as ?service? 
and ?atmosphere? contain very limited related top-
ics, most of which have high frequencies (e.g., 
?waiter?, ?staff?, ?ambiance? and ?vibe?). The cate-
gory ?food?, however, is very domain-specific and 
contains a very large vocabulary, from generic 
sub-categories such as ?sushi?, ?dessert? and 
?sandwich? as shown in the examples, to specific 
courses such as ?bosc pear bread pudding? and 
?herb roasted vermont pheasant wine cap mu-
shrooms?. These domain-specific topics have very 
low frequencies, yet they are very relevant and 
valuable. But many of them are discarded by the 
clustering. It would be a similar case in other do-
mains. For example, consumer products, movies 
and books all have domain-independent semantic 
categories (e.g., ?price? and ?released date?) and 
domain-specific categories (e.g., technical features 
of consumer products, casts of movies and authors 
of books). 
To recover these context-relevant topics, we 
employ domain context relations such as a con-
text-related ontology. A context-related ontology 
can be constructed from structured web resources 
such as online menus of restaurants, names of ac-
tors and actresses from movie databases, and spe-
cifications of products from online shops. An 
example of a partial online menu of a restaurant is 
shown in Figure 3. From these structured web re-
sources, we can build up a hierarchical ontology, 
based on which a set of semantic features can be 
extracted (e.g., whether a phrase contains a course 
name, or an actress?s name, or a dimension of 
technical features of a consumer product).  
 
Entree 
Roasted Pork Loin Wrapped In Bacon with watermelon and 
red onion salad spicy honey-mustard bbq sauce 
Spicy Halibut And Clam Roast with bacon braised greens, 
white beans and black trumpet mushrooms 
Parmesan and Caramelized Shallot Wrapper Style Ravi-
oli turnip greens and white truffle oil 
Herb Roasted Vermont Pheasant Wine Cap Mushrooms, 
Pearl Onions and Fava Beans 
Dessert 
Chocolate Tasting Plate of white chocolate bombe milk choc-
olate creme brul?e and dark chocolate flourless cake  
White Fruit Tasting Plate of warm apple strudel butterscotch, 
Bosc Pear bread pudding and toasted coconut panna cotta  
 
Entr?e Pork loin, bacon, watermelon, red onion 
salad, honey, mustard, bbq sauce 
Dessert  Chocolate, milk, cr?me brulee, cake 
Figure 3. Example of a partial online menu and an ex-
emplary ontology derived. 
 
After the classification, phrases identified as 
?highly informative and relevant? are clustered 
into different aspects according to the semantic 
category clustering and the hierarchical ontology. 
An average sentiment score for each aspect is then 
calculated:  
+/0(1() =
? 233?41
|41|
     (3) 
68
where 1(  represents the aspect s of entry t (e.g., a 
restaurant, a movie, or a consumer product), 41  
represents the set of phrases in the cluster of as-
pect s, and 23  represents the sentiment score of 
phrase j in the cluster. 
The set of phrases selected for one entry may 
come from several reviews on this single entry, 
and many of them may include the same noun 
(e.g., ?good fish?, ?not bad fish? and ?above-
average fish? for one restaurant). Thus, the next 
step is multi-phrase redundancy resolution. We 
select the phrase with a sentiment score closest to 
the average score of its cluster as the most repre-
sentative phrase on each topic:  
5 = +265%&3?4%(|23 ? +/0(1()|)    (4) 
where +/0(1()  represents the average sentiment 
score of aspect 1, 4%  represents the set of phrases 
on the same topic %  in the cluster 1 , and 23  
represents the sentiment score of phrase 3.  
This sequence of topic categorization, ontology 
construction, phrase pruning and redundancy eli-
mination leads to a summary database, which can 
be utilized for dialogue generation in spoken rec-
ommendation systems. A review summary data-
base entry generated by the proposed approaches 
is exemplified in Figure 4. 
 
{ restaurant "dali restaurant and tapas bar" 
     :atmosphere ( "wonderful evening", "cozy atmos- 
phere", "fun decor", "romantic date" ) 
     :atmosphere_rating "4.1" 
     :food ( "very fresh ingredients",  "tasty fish", 
"creative dishes",  "good sangria" ) 
     :food_rating "3.9"         
     :service ( "fast service" ) 
     :service_rating "3.9"    
     :general ("romantic restaurant","small space" ) 
     :general_rating "3.6"                 } 
Figure 4. Example of a review summary database entry 
generated by the proposed approaches. 
4 Experiments 
In this project, we substantiate the proposed ap-
proach in a restaurant domain for our spoken di-
alogue system (Gruenstein and Seneff, 2007), 
which is a web-based multimodal dialogue system 
allowing users to inquire about information about 
restaurants, museums, subways, etc. We harvested 
a data collection of 137,569 reviews on 24,043 
restaurants in 9 cities in the U.S. from an online 
restaurant evaluation website1. From the dataset, 
857,466 sentences were subjected to parse analysis; 
and a total of 434,372 phrases (114,369 unique 
ones) were extracted from the parsable subset 
(78.6%) of the sentences.  
Most pros/cons consist of well-formatted phras-
es; thus, we select 3,000 phrases extracted from 
pros/cons as training data. To generate a human 
judgment-consistent training set, we manually la-
bel the training samples with ?good? and ?bad? la-
bels. We then randomly select a subset of 3,000 
phrases extracted from review texts as the test set 
and label the phrases. The kappa agreement be-
tween two sets of annotations is 0.73, indicating 
substantial consistency. We use the two annotation 
sets as the ground truth. 
To extract context-related semantic features, we 
collect a large pool of well-formatted menus from 
an online resource2, which contains 16,141 restau-
rant menus. Based on the hierarchical structure of 
these collected menus, we build up a context-
related ontology and extract a set of semantic fea-
tures from the ontology, such as whether the topic 
of a phrase is on category-level (e.g., ?entr?e?, 
?dessert?, ?appetizers?, ?salad?), whether the topic 
is on course-level (e.g., ?Roasted Pork Loin?, ?Spi-
cy Halibut and Clam Roast?), and whether the top-
ic is on ingredient-level (e.g., ?beans?, ?chicken?, 
?mushrooms?, ?scallop?).  
We employ the three types of features as afore-
mentioned to train the SVMs and the decision tree 
models. To select the most valuable features for 
model training, we conducted a set of leave-one-
feature-out experiments for both the SVMs and the 
decision tree models. We found that all the fea-
tures except the adjective unigram probability 
contribute positively to model learning. From fur-
ther data analysis we observed that many phrases 
with popular adjectives have context-unrelated 
nouns, which makes the adjective unigram proba-
bility fail to become a dominant factor for phrase 
relevance. Using the adjective unigram probability 
as a learning feature will mislead the system into 
trusting an adjective that is common but has a poor 
bigram affinity to the noun in the phrase. Thus, we 
eliminate this feature for both the SVMs and the 
decision tree learning. 
                                                          
1 http://www.citysearch.com 
2 http://www.menupages.com  
69
 To evaluate the performance of the classifica-
tion models, we take a set of intuitively motivated 
heuristic rules as the baseline. Figure 5 gives the 
pseudo-code of the heuristic rule algorithm, which 
uses variations of all the features except the uni-
gram probability of adjectives. 
  
If(sentiment score of the phrase exists) 
if(sentiment score is within neutral range) label=-1; 
else    
if(phrase appeared in the training data) 
      if((3<frequency of phrase < 100))   label = 1; 
          else 
            if(frequency of phrase >= 100)   label = -1; 
              else    if(topic belongs to ontology)  label = 1; 
                        else   label = -1; 
       else 
            if(topic belongs to ontology)   label = 1; 
            else   label = -1; 
else 
if(phrase appeared in the training data) 
           if((3<frequency of phrase < 100))    
if(topic belongs to ontology)  label = 1; 
                   else   label = -1; 
           else 
              if(frequency of phrase >= 100)   label = -1; 
              else 
                   if(topic belongs to ontology)  label = 1; 
                   else   if(frequency of noun > 100) label = 1; 
                            else   label = -1; 
    else 
         if(topic belongs to ontology)  label = 1; 
         else     if(frequency of noun > 100)   label = 1; 
                    else   label = -1;                 
Figure 5. Pseudo-code of the heuristic rule algorithm. 
 
The performance of classification by different 
models is shown in Table 4. Although the heuris-
tic rule algorithm is complicated and involves hu-
man knowledge, the statistical models trained by 
SVMs and the decision tree algorithms both out-
perform the baseline significantly. The SVM mod-
el outperforms the baseline by 10.5% and 11.9% 
on the two annotation sets respectively. The deci-
sion tree model outperforms the baseline by 16.4% 
and 23.2% (average relative improvement of 36%), 
and it also outperforms the SVM model by 5.9% 
and 11.3% (average relative improvement of 13%). 
The classification model using the decision tree 
algorithm can achieve a precision of 77.9% and 
74.5% compared with the ground truth, which is 
quite comparable to human judgment (the preci-
sion of one annotation set based on the other is 
74%). This shows that the decision tree model can 
predict phrase labels as reliably as human judg-
ment. 
 
 Baseline SVM Decision tree 
Annotation 1 61.5% 72.0% 77.9% 
Annotation 2 51.3% 63.2% 74.5% 
Table 4. Precision of phrase classification using the 
heuristic rule baseline, the SVM model, and the deci-
sion tree algorithm. 
 
To gain further insight on the contributions of 
each feature set to the decision tree learning, Table 
5 gives the experimental results on leaving each 
feature out of model training. As shown, without 
semantic features, the precision is 70.6% and 65.4% 
on the two annotation sets, lower by 7.3% and 9.1% 
than the case of training the model with all the 
features (77.9% and 74.5%). This shows that the 
semantic features significantly contribute to the 
decision tree learning. 
 
Feature set A1 A2 
all features  77.9% 74.5% 
without bigram probability 
of adjective-noun pair 
56.6%  
(-21.3%) 
63.9%  
(-10.6%) 
without unigram probability 
of the phrase 
57.6%  
(-20.3%) 
64.3%  
(-10.2%) 
without unigram probability 
of the noun 
59.8%  
(-18.1%) 
67.8%  
(-6.7%) 
without sentiment score of 
the phrase 
63.4%  
(-14.5%) 
66.6%  
(-7.9%) 
without underlying semantic 
features  
70.6%  
(-7.3%) 
65.4%  
(-9.1%) 
Table 5. Performance of the decision tree model by 
leaving each feature out of model training (?A1? and 
?A2? represent the annotation set 1 and 2 respectively). 
 
The experimental results also show that the fea-
ture of bigram probability of the adjective-noun 
pair contributes the most to the model learning. 
Without this feature, the precision drops by 21.3% 
and 10.6%, reaching the lowest precision among 
all the leave-one-out experiments. This confirms 
our observation that although a single adjective is 
not dominant, the pair of the adjective and the 
noun that co-occurs with it plays an important role 
in the classification.  
The sentiment of phrases also plays an impor-
tant role. Without sentiment features, the precision 
70
drops to 63.4% and 66.6% respectively on the two 
annotations, decreasing by 14.5% and 7.9%. This 
shows that the sentiment features contribute sig-
nificantly to the classification.  
5 Discussions 
Experimental results show that the decision tree 
algorithm outperforms the SVMs on this particular 
classification problem, and it outperforms the heu-
ristic rule baseline significantly. Thus, although 
the identification of informativeness and relevance 
of phrases is a rather subjective problem, which is 
difficult to predict using only human knowledge, it 
can be well defined by decision trees. Part of the 
reason is that the decision tree algorithm can make 
better use of a combination of Boolean value fea-
tures (e.g., whether a topic belongs to a context-
related ontology) and continuous value features. 
Also, as the phrase classification task is very sub-
jective, it is very similar to a ?hierarchical if-else 
decision problem? in human cognition, where de-
cision tree algorithms can fit well. Figure 6 shows 
a partial simplified decision tree learned from our 
model, which can give an intuitive idea of the de-
cision tree models. 
6 Related Work 
Sentiment classification and opinion mining have 
been well studied for years. Most studies have fo-
cused on text-based systems, such as document-
level sentiment classification and sentence-level 
opinion aggregation (Turney, 2002; Pang et al, 
2002; Dave et al, 2003; Hu and Liu, 2004; Popes-
cu and Etzioni, 2005; Wilson et al, 2005; Zhuang 
et al, 2006; Kim and Hovy, 2006).  
There was a study conducted by Carenini et al 
in 2006, which proposed a combination of a sen-
tence extraction-based approach and a language 
generation-based approach for summarizing eva-
luative arguments. In our work, we utilize a lower-
level phrase-based extraction approach, which uti-
lizes high level linguistic features and syntactic 
structure to capture phrase patterns.  
There was also a study on using reviews to gen-
erate a dictionary of mappings between semantic 
representations and realizations of concepts for 
dialogue systems (Higashinaka et al, 2006; Higa-
shinaka, 2007). They also used the association 
between user ratings and reviews to capture se-
mantic-syntactic structure mappings. A set of fil-
tering rules was manually created to eliminate 
low-quality mappings. In our approach, we use an 
automatic approach to classifying high/low infor-
mative phrases. The learning features are domain-
independent with no hand-crafted rules, and can 
be extended to other domains effortlessly.  
7 Conclusions 
In this paper we proposed a three-level framework 
for review-based recommendation dialogue sys-
tems, including linguistic phrase extraction, dialo-
gue-oriented review summary generation, and 
human-friendly dialogue generation. The contribu-
tions of this paper are three-fold: 1) it identified 
and defined the research goal of utilizing opinion 
summarization for real human-computer conversa-
tion; 2) it formulated an evaluation methodology 
for high-density review summary for dialogue 
purposes; 3) it proposed an approach to automatic 
classification of high/low informative phrases us-
ing a decision tree model. Experimental results 
showed that the decision tree model significantly 
outperforms a heuristic rule baseline and the SVM 
model, and can resolve the phrase classification 
problem comparably to humans consistently. 
Future work will focus on: 1) applying the sen-
timent scoring model to noun/verb sentiment as-
sessment; 2) application of the review summary 
generation approach in other domains and other 
languages; 3) data collection on user engagement 
with our dialogue systems involving review-
summary evaluation. 
 
 
Figure 6. A partial simplified decision tree learned from 
our model. 
  
71
References  
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2009. Multi-facet Rating of Product Reviews. 
In Proceedings of European Conference on Informa-
tion Retrieval. 
S.R.K. Branavan, Harr Chen, Jacob Eisenstein, and 
Regina Barzilay. 2008. Learning document-level 
semantic properties from free-text annotations. In 
Proc. of ACL. 
Giuseppe Carenini, Raymond Ng, and Adam Pauls. 
2006. Multi-Document Summarization of Evaluative 
Text. In Proceedings of the Conference of the Euro-
pean Chapter of the Association for Computational 
Linguistics. 
Kushal Dave, Steve Lawrence, and David M. Pennock. 
2003. Mining the peanut gallery: opinion extraction 
and semantic classification of product reviews. In 
Proceedings of the International Conference on 
World Wide Web. 
W. Eckert, T. Kuhn, H. Niemann, S. Rieck, A. Scheuer, 
and E. G. Schukat-talamazzini. 1993. A Spoken Di-
alogue System for German Intercity Train Timetable 
Inquiries. In Proc. European Conf. on Speech Tech-
nology. 
Alexander Gruenstein and Stephanie Seneff. 2007. Re-
leasing a Multimodal Dialogue System into the 
Wild: User Support Mechanisms. In Proceedings of 
the 8th SIGdial Workshop on Discourse and Dialo-
gue, Antwerp, pages 111-119. 
A. L. Gorin, G. Riccardi and J. H. Wright. 1997. ?How 
may I help you?? Speech Communication, vol. 23, 
pp. 113?127.  
Ryuichiro Higashinaka, Rashmi Prasad and Marilyn 
Walker. 2006. Learning to Generate  
Naturalistic Utterances Using Reviews in Spoken 
Dialogue Systems. In Proceedings of COLING-ACL.  
Ryuichiro Higashinaka, Marilyn Walker and Rashmi 
Prasad. 2007. An Unsupervised Method  
for Learning Generation Dictionaries for Spoken Di-
alogue Systems by Mining User Reviews.  
Journal of ACM Transactions on Speech and Lan-
guage Processing. 
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 2004 
ACM SIGKDD international conference on Know-
ledge Discovery and Data mining. 
S.M. Kim and E.H. Hovy. 2006. Identifying and Ana-
lyzing Judgment Opinions. In Proc. of HLT/NAACL. 
Jingjing Liu and Stephanie Seneff. 2009. Review Sen-
timent Scoring via a Parse-and-Paraphrase Para-
digm. In proceedings of EMNLP. 
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, 
and ChengXiang Zhai. 2007. Topic Sentiment Mix-
ture: Modeling Facets and Opinions in Weblogs. In 
Proc. of WWW. 
Bo Pang, Lillian Lee, and S. Vaithyanathan. 2002. 
Thumbs up? Sentiment classification using machine 
learning techniques. In Proceedings of EMNLP. 
A.M. Popescu and O. Etzioni. 2005. Extracting product 
features and opinions from reviews. In Proceedings 
of EMNLP. 
JR Quinlan, 1986. Induction of decision trees. Machine 
learning, Springer-Netherlands. 
A. Raux, B. Langner, A. Black, and M. Eskenazi. 2003. 
LET'S GO: Improving Spoken Dialog Systems for 
the Elderly and Non-natives. In Proc. Eurospeech. 
Benjamin Snyder and Regina Barzilay. 2007. Multiple 
Aspect Ranking using the Good Grief Algorithm. In 
Proceedings of NAACL-HLT. 
Ivan Titov and Ryan McDonald. 2008a. Modeling On-
line Reviews with Multi-Grain Topic Models. In 
Proc. of WWW. 
Ivan Titov and Ryan McDonald. 2008b. A Joint Model 
of Text and Aspect Ratings for Sentiment Summari-
zation. In Proceedings of the Annual Conference of 
the Association for Computational Linguistics. 
Peter D. Turney. 2002. Thumbs up or thumbs down? 
Sentiment orientation applied to unsupervised classi-
fication of reviews. In Proceedings of the Annual 
Conference of the Association for Computational 
Linguistics. 
T. Joachims. 1998. Text categorization with support 
vector machines: Learning with many relevant fea-
tures. In Proc. of ECML, p. 137?142.  
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing Contextual Polarity in Phrase-Level Senti-
ment Analysis. In Proc. of HLT/EMNLP. 
Victor Zue, Stephanie Seneff, James Glass, Joseph Po-
lifroni, Christine Pao, Timothy J. Hazen, and Lee 
Hetherington. 2000. JUPITER: A Telephone-Based 
Conversational Interface for Weather Information. In 
IEEE Transactions on Speech and Audio Processing, 
Vol. 8 , No. 1.  
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie 
review mining and summarization. In Proceedings of 
the 15th ACM international conference on Informa-
tion and knowledge management. 
72
