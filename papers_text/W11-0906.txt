Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 37?45,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
Transition-based Semantic Role Labeling
Using Predicate Argument Clustering
Jinho D. Choi
Department of Computer Science
University of Colorado at Boulder
choijd@colorado.edu
Martha Palmer
Department of Linguistics
University of Colorado at Boulder
mpalmer@colorado.edu
Abstract
This paper suggests two ways of improving
semantic role labeling (SRL). First, we intro-
duce a novel transition-based SRL algorithm
that gives a quite different approach to SRL.
Our algorithm is inspired by shift-reduce pars-
ing and brings the advantages of the transition-
based approach to SRL. Second, we present
a self-learning clustering technique that effec-
tively improves labeling accuracy in the test
domain. For better generalization of the sta-
tistical models, we cluster verb predicates by
comparing their predicate argument structures
and apply the clustering information to the
final labeling decisions. All approaches are
evaluated on the CoNLL?09 English data. The
new algorithm shows comparable results to
another state-of-the-art system. The cluster-
ing technique improves labeling accuracy for
both in-domain and out-of-domain tasks.
1 Introduction
Semantic role labeling (SRL) has sparked much in-
terest in NLP (Shen and Lapata, 2007; Liu and
Gildea, 2010). Lately, dependency-based SRL has
shown advantages over constituent-based SRL (Jo-
hansson and Nugues, 2008). Two main benefits can
be found. First, dependency parsing is much faster
than constituent parsing, whereas constituent pars-
ing is usually considered to be a bottleneck to SRL in
terms of execution time. Second, dependency struc-
ture is more similar to predicate argument struc-
ture than phrase structure because it specifically de-
fines relations between a predicate and its arguments
with labeled arcs. Unlike constituent-based SRL
that maps phrases to semantic roles, dependency-
based SRL maps headwords to semantic roles be-
cause there is no phrasal node in dependency struc-
ture. This may lead to a concern about getting the
actual semantic chunks back, but Choi and Palmer
(2010) have shown that it is possible to recover the
original chunks from the headwords with minimal
loss, using a certain type of dependency structure.
Traditionally, either constituent or dependency-
based, semantic role labeling is done in two steps,
argument identification and classification (Gildea
and Jurafsky, 2002). This is from a general be-
lief that each step requires a different set of fea-
tures (Xue and Palmer, 2004), and training these
steps in a pipeline takes less time than training them
as a joint-inference task. However, recent machine
learning algorithms can deal with large scale vector
spaces without taking too much training time (Hsieh
et al, 2008). Furthermore, from our experience in
dependency parsing, handling these steps together
improves accuracy in identification as well as clas-
sification (unlabeled and labeled attachment scores
in dependency parsing). This motivates the develop-
ment of a new semantic role labeling algorithm that
treats these two steps as a joint inference task.
Our algorithm is inspired by shift-reduce pars-
ing (Nivre, 2008). The algorithm uses several transi-
tions to identify predicates and their arguments with
semantic roles. One big advantage of the transition-
based approach is that it can use previously identi-
fied arguments as features to predict the next argu-
ment. We apply this technique to our approach and
achieve comparable results to another state-of-the-
art system evaluated on the same data sets.
37
NO-PRED
( ?1 , ?2, j, ?3, [i|?4], A )? ( [?1|j], ?2, i, ?3, ?4 , A )
?j. oracle(j) 6= predicate
SHIFT
( ?1 , ?2, j, [i|?3], ?4, A )? ( [?2|j], [ ] , i, [ ] , ?3, A )
?j. oracle(j) = predicate ? ?1 = [ ] ? ?4 = [ ]
NO-ARC?
( [?1|i], ?2 , j, ?3, ?4, A )? ( ?1 , [i|?2], j, ?3, ?4, A )
?j. oracle(j) = predicate ? ?i.oracle(i, j) = {i 6? j}
NO-ARC?
( ?1, ?2, j, ?3 , [i|?4], A )? ( ?1, ?2, j, [?3|i], ?4 , A )
?j. oracle(j) = predicate ? ?i.oracle(i, j) = {j 6? i}
LEFT-ARC?L
( [?1|i], ?2 , j, ?3, ?4, A )? ( ?1 , [i|?2], j, ?3, ?4, A ? {i
L
? j} )
?j. oracle(j) = predicate ? ?i.oracle(i, j) = {i
L
? j}
RIGHT-ARC?L
( ?1, ?2, j, ?3 , [i|?4], A )? ( ?1, ?2, j, [?3|i], ?4 , A ? {j
L
? i} )
?j. oracle(j) = predicate ? ?i.oracle(i, j) = {j
L
? i}
Table 1: Transitions in our bidirectional top-down search algorithm. For each row, the first line shows a transition and
the second line shows preconditions of the transition.
For better generalization of the statistical models,
we apply a self-learning clustering technique. We
first cluster predicates in test data using automati-
cally generated predicate argument structures, then
cluster predicates in training data by using the previ-
ously found clusters as seeds. Our experiments show
that this technique improves labeling accuracy for
both in-domain and out-of-domain tasks.
2 Transition-based semantic role labeling
Dependency-based semantic role labeling can be
viewed as a special kind of dependency parsing in
the sense that both try to find relations between
word pairs. However, they are distinguished in two
major ways. First, unlike dependency parsing that
tries to find some kind of relation between any word
pair, semantic role labeling restricts its search only
to top-down relations between predicate and argu-
ment pairs. Second, dependency parsing requires
one head for each word, so the final output is a tree,
whereas semantic role labeling allows multiple pred-
icates for each argument. Thus, not all dependency
parsing algorithms, such as a maximum spanning
tree algorithm (Mcdonald and Pereira, 2006), can be
naively applied to semantic role labeling.
Some transition-based dependency parsing algo-
rithms have been adapted to semantic role labeling
and shown good results (Henderson et al, 2008;
Titov et al, 2009). However, these algorithms are
originally designed for dependency parsing, so are
not necessarily customized for semantic role label-
ing. Here, we present a novel transition-based algo-
rithm dedicated to semantic role labeling. The key
difference between this algorithm and most other
transition-based algorithms is in its directionality.
Given an identified predicate, this algorithm tries to
find top-down relations between the predicate and
the words on both left and right-hand sides, whereas
other transition-based algorithms would consider
words on either the left or the right-hand side, but
not both. This bidirectional top-down search makes
more sense for semantic role labeling because predi-
cates are always assumed to be the heads of their ar-
guments, an assumption that cannot be generalized
to dependency parsing, and arguments can appear
either side of the predicate.
Table 1 shows transitions used in our algorithm.
All parsing states are represented as tuples (?1, ?2,
p, ?3, ?4, A), where ?1..4 are lists of word indices
and p is either a word index of the current predi-
cate candidate or @ indicating no predicate candi-
date. ?1,4 contain indices to be compared with p and
?2,3 contain indices already compared with p. A is a
set of labeled arcs representing previously identified
arguments with respect to their predicates. ? and
? indicate parsing directions. L is a semantic role
label, and i, j represent indices of their correspond-
ing word tokens. The initial state is ([ ], [ ], 1, [ ],
[2, . . . , n], ?), where w1 and wn are the first and the
last words in a sentence, respectively. The final state
is (?1, ?2, @, [ ], [ ],A), i.e., the algorithm terminates
when there is no more predicate candidate left.
38
John
1
wants
2
to
3
buy
4
a
5
car
6
Root
0
SBJ
ROOT
OPRD
OBJ
IM
NMOD
A0 A1
A0 A1
Figure 1: An example of a dependency tree with semantic roles. The upper and lower arcs stand for syntactic and
semantic dependencies, respectively. SBJ, OBJ, OPRD, IM, NMOD stand for a subject, object, object predicative,
infinitive marker, and noun-modifier. A0, A1 stand for ARG0, ARG1 in PropBank (Palmer et al, 2005).
Transition ?1 ?2 p ?3 ?4 A
0 [ ] [ ] 1 [ ] [2..6] ?
1 NO-PRED [1] [ ] 2 [ ] [3..6]
2 LEFT-ARC [ ] [1] 2 [ ] [3..6] A ? {1?A0? 2}
3 RIGHT-ARC [ ] [1] 2 [3] [4..6] A ? {2 ?A1? 3}
4 NO-ARC [ ] [1] 2 [3..4] [5..6]
5 NO-ARC [ ] [1] 2 [3..5] [6]
6 NO-ARC [ ] [1] 2 [3..6] [ ]
7 SHIFT [1..2] [ ] 3 [ ] [4..6]
8 NO-PRED [1..3] [ ] 4 [ ] [5..6]
9 NO-ARC [1..2] [3] 4 [ ] [5..6]
10 NO-ARC [1] [2..3] 4 [ ] [5..6]
11 LEFT-ARC [ ] [1..3] 4 [ ] [5..6] A ? {1?A0? 4}
12 NO-ARC [ ] [1..3] 4 [5] [6]
13 RIGHT-ARC [ ] [1..3] 4 [5..6] [ ] A ? {4 ?A1? 6}
14 SHIFT [1..4] [ ] 5 [ ] [6]
15 NO-PRED [1..5] [ ] 6 [ ] [ ]
16 NO-PRED [1..6] [ ] @ [ ] [ ]
Table 2: Parsing states generated by our algorithm for the example in Figure 1.
The algorithm uses six kinds of transitions. NO-
PRED is performed when an oracle identifies wj as
not a predicate. All other transitions are performed
when wj is identified as a predicate. SHIFT is per-
formed when both ?1 and ?4 are empty, meaning
that there are no more argument candidates left for
the predicate wj . NO-ARC is performed when wi
is identified as not an argument of wj . LEFT-ARCL
and RIGHT-ARCL are performed when wi is identi-
fied as an argument of wj with a label L. These tran-
sitions can be performed in any order as long as their
preconditions are satisfied. For our experiments, we
use the following generalized sequence:
[ (NO-PRED)? ? (LEFT-ARC?L |NO-ARC
?)? ?
(RIGHT-ARC?L |NO-ARC
?)? ? SHIFT ]?
Notice that this algorithm does not take separate
steps for argument identification and classification.
By adding the NO-ARC transitions, we successfully
merge these two steps together without decrease in
labeling accuracy.1 Since each word can be a predi-
cate candidate and each predicate considers all other
words as argument candidates, a worst-case com-
plexity of the algorithm is O(n2). To reduce the
complexity, Zhao et al (2009) reformulated a prun-
ing algorithm introduced by Xue and Palmer (2004)
for dependency structure by considering only direct
dependents of a predicate and its ancestors as ar-
gument candidates. This pruning algorithm can be
easily applied to our algorithm: the oracle can pre-
filter such dependents and uses the information to
perform NO-ARC transitions without consulting sta-
tistical models.
1We also experimented with the traditional approach of
building separate classifiers for identification and classification,
which did not lead to better performance in our case.
39
Table 2 shows parsing states generated by our al-
gorithm. Our experiments show that this algorithm
gives comparable results against another state-of-
the-art system.
3 Predicate argument clustering
Some studies showed that verb clustering informa-
tion could improve performance in semantic role la-
beling (Gildea and Jurafsky, 2002; Pradhan et al,
2008). This is because semantic role labelers usually
perform worse on verbs not seen during training, for
which the clustering information can provide useful
features. Most previous studies used either bag-of-
words or syntactic structure to cluster verbs; how-
ever, this may or may not capture the nature of predi-
cate argument structure, which is more semantically
oriented. Thus, it is preferable to cluster verbs by
their predicate argument structures to get optimized
features for semantic role labeling.
In this section, we present a self-learning clus-
tering technique that effectively improves labeling
accuracy in the test domain. First, we perform se-
mantic role labeling on the test data using the algo-
rithm in Section 2. Next, we cluster verbs in the test
data using predicate argument structures generated
by our semantic role labeler (Section 3.2). Then, we
cluster verbs in the training data using the verb clus-
ters we found in the test data (Section 3.3). Finally,
we re-run our semantic role labeler on the test data
using the clustering information. Our experiments
show that this technique gives improvement to la-
beling accuracy for both in and out-of domain tasks.
3.1 Projecting predicate argument structure
into vector space
Before clustering, we need to project the predicate
argument structure of each verb into vector space.
Two kinds of features are used to represent these
vectors: semantic role labels and joined tags of
semantic role labels and their corresponding word
lemmas. Figure 2 shows vector representations of
predicate argument structures of verbs, want and
buy, in Figure 1.
Initially, all existing and non-existing features are
assigned with a value of 1 and 0, respectively. How-
ever, assigning equal values to all existing features
is not necessarily fair because some features have
want 1 1 1 1 00s 0s
buy 1 1 1 0 10s 0s
A0 A1 john:A0 to:A1 car:A1... ...Verb
Figure 2: Projecting the predicate argument structure of
each verb into vector space.
higher confidence, or are more important than the
others; e.g., ARG0 and ARG1 are generally predicted
with higher confidence than modifiers, nouns give
more important information than some other gram-
matical categories, etc. Instead, we assign each ex-
isting feature with a value computed by the follow-
ing equations:
s(lj |vi) =
1
1 + exp(?score(lj |vi))
s(mj , lj) =
{
1 (wj 6= noun)
exp( count(mj ,lj)?
?k count(mk,lk)
)
vi is the current verb, lj is the j?th label of vi, and
mj is lj?s corresponding lemma. score(lj |vi) is a
score of lj being a correct argument label of vi; this
is always 1 for training data and is provided by our
statistical models for test data. Thus, s(lj |vi) is an
approximated probability of lj being a correct argu-
ment label of vi, estimated by the logistic function.
s(mj , lj) is equal to 1 if wj is not a noun. If wj is
a noun, it gets a value ? 1 given a maximum likeli-
hood of mj being co-occurred with lj .2
With the vector representation, we can apply any
kind of clustering algorithm (Hofmann and Puzicha,
1998; Kamvar et al, 2002). For our experiments,
we use k-best hierarchical clustering for test data,
and k-means clustering for training data.
3.2 Clustering verbs in test data
Given automatically generated predicate argument
structures in the test data, we apply k-best hierar-
chical clustering; that is, a relaxation of classical hi-
erarchical agglomerative clustering (from now on,
HAC; Ward (1963)), to find verb clusters. Unlike
HAC that merges a pair of clusters at each iteration,
k-best hierarchical clustering merges k-best pairs at
2Assigning different weights for nouns resulted in more
meaningful clusters in our experiments. We will explore addi-
tional grammatical category specific weighting schemes in fu-
ture work.
40
each iteration (Lo et al, 2009). Instead of merging a
fixed number of k-clusters, we use a threshold to dy-
namically determine the top k-clusters. Our studies
indicate that this technique produces almost as fine-
grained clusters as HAC, yet converges much faster.
Our algorithm for k-best hierarchical clustering is
presented in Algorithm 1. thup is a threshold that de-
termines which k-best pairs are to be merged (in our
case, kup = 0.8). sim(ci, cj) is a similarity between
clusters ci and cj . For our experiments, we use co-
sine similarity with average-linkage. It is possible
that other kinds of similarity metrics would work
better, which we will explore as future work. Con-
ditions in line 15 ensure that each cluster is merged
with at most one other cluster at each iteration, and
conditions in line 17 force at least one cluster to
be merged with one other cluster at each iteration.
Thus, the algorithm is guaranteed to terminate after
at most (n? 1) iterations.
When the algorithm terminates, it returns a set of
one cluster with different hierarchical levels. For
our experiments, we set another threshold, thlow, for
early break-out: if there is no cluster pair whose sim-
ilarity is greater than thlow, we terminate the algo-
rithm (in our case, thlow = 0.7). A cluster set gen-
erated by this early break-out contains several unit
clusters that are not merged with any other cluster.
All of these unit clusters are discarded from the set
to improve set quality. This is reasonable because
our goal is not to cluster all verbs but to find a useful
set of verb clusters that can be mapped to verbs in
training data, which can lead to better performance
in semantic role labeling.
3.3 Clustering verbs in training data
Given the verb clusters we found in the test data,
we search for verbs that are similar to these clusters
in the training data. K-means clustering (Hartigan,
1975) is a natural choice for this case because we
already know k-number of center clusters to begin
with. Each verb in the training data is compared with
all verb clusters in the test data, and merged with the
cluster that gives the highest similarity. To maintain
the quality of the clusters, we use the same thresh-
old, thlow, to filter out verbs in the training data that
are not similar enough to any verb cluster in the test
data. By doing so, we keep only verbs that are more
likely to be helpful for semantic role labeling.
input : C = [c1, .., cn]: ci is a unit cluster.
thup ? R: threshold.
output: C? = [c1, .., cm]: cj is a unit or merged
cluster, where m ? n.
begin1
while |C| > 1 do2
L? list()3
for i ? [1, |C| ? 1] do4
for j ? [i+ 1, |C|] do5
t? (i, j, sim(ci, cj))6
L.add(t)7
end8
end9
descendingSortBySimilarity(L)10
S ? set()11
for k ? [1, |L|] do12
t? L.get(k)13
i? t(0); j ? t(1); sim? t(2)14
if i ? S or j ? S then15
continue16
if k = 1 or sim > thup then17
C.add(ci ? cj); S.add(i, j)18
C.remove(ci, cj)19
else20
break21
end22
end23
end24
end25
Algorithm 1: k-best hierarchical clustering.
4 Features
4.1 Baseline features
For a baseline approach, we use features similar to
ones used by Johansson and Nugues (2008). All fea-
tures are assumed to have dependency structures as
input. Table 3 shows n-gram feature templates used
for our experiments (f: form, m: lemma, p: POS tag,
d: dependency label). warg andwpred are the current
argument and predicate candidates. hd(w) stands for
the head of w, lm(w), rm(w) stand for the leftmost,
rightmost dependents of w, and ls(w), rs(w) stand
for the left-nearest, right-nearest siblings of w, with
respect to the dependency structures. Some of these
features can be presented as a joined feature; e.g., a
combination of warg?s POS tag and lemma.
41
Word tokens Features
warg, wpred f,m,p,d
warg?1, hd, lm, rm, ls, rs (warg) m,p
wpred?1, hd, lm, rm (wpred) m,p
Table 3: N -gram feature templates.
Besides the n-gram features, we use several struc-
tural features such as dependency label set, subcat-
egorization, POS path, dependency path, and depen-
dency depth. Dependency label set features are de-
rived by collecting all dependency labels of wpred?s
direct dependents. Unlike Johansson and Nugues,
we decompose subcategorization features into two
parts: one representing the left-hand side and the
other representing the right-hand side dependencies
of wpred. For the predicate wants in Figure 3, we
generate ??SBJ and ????OPRD as separate subcategoriza-
tion features.
wants
PRP:John TO:to
VB:buy
SBJ OPRD
IM
Figure 3: Dependency structure used for subcategoriza-
tion, path, and depth features.
We also decompose path features into two parts:
given the lowest common ancestor (LCA) of warg
and wpred, we generate path features from warg to
the LCA and from the LCA to wpred, separately.
For example, the predicate buy and the argument
John in Figure 3 have a LCA at wants, so we gen-
erate two sets of path features, {?PRP, ?TO?VB}
with POS tags, and {?SBJ, ?OPRD?IM} with depen-
dency labels. Such decompositions allow more gen-
eralization of those features; even if one part is not
matched to the current parsing state, the other part
can still participate as a feature. Throughout our
experiments, these generalized features give slightly
higher labeling accuracy than ungeneralized features
although they form a smaller feature space.
In addition, we apply dependency path features to
wpred?s highest verb chain, which often shares ar-
guments with the predicate (e.g., John is a shared
argument of the predicate buy and its highest verb
chain wants). To retrieve the highest verb chain, we
apply a simple heuristic presented below. The func-
tion getHighestVerbChain takes a predicate,
pred, as input and returns its highest verb chain,
vNode, as output. If there is no verb chain for the
predicate, it returns null instead. Note that this
heuristic is designed to work with dependency rela-
tions and labels described by the CoNLL?09 shared
task (Hajic? et al, 2009).
func getHighestVerbChain(pred)
vNode = pred;
regex = "CONJ|COORD|IM|OPRD|VC";
while (regex.matches(vNode.deprel))
vNode = vNode.head;
if (vNode != pred) return vNode;
else return null;
Dependency depth features are a reduced form of
path features. Instead of specifying POS tags or de-
pendency labels, we indicate paths with their depths.
For instance, John and buy in Figure 3 have a depen-
dency depth feature of ?1?2, which implies that the
depth between John and its LCA (wants) is 1, and
the depth between the LCA and buy is 2.
Finally, we use four kinds of binary features: if
warg is a syntactic head of wpred, if wpred is a syn-
tactic head ofwarg, ifwpred is a syntactic ancestor of
warg, and if wpred?s verb chain has a subject. Each
feature gets a value of 1 if true; otherwise, it gets a
value of 0.
4.2 Dynamic and clustering features
All dynamic features are derived by using previ-
ously identified arguments. Two kinds of dynamic
features are used for our experiments. One is a la-
bel of the very last predicted numbered argument of
wpred. For instance, the parsing state 3 in Table 2
uses a label A0 as a feature to make its prediction,
wants
A1
? to, and the parsing states 4 to 6 use a label
A1 as a feature to make their predictions, NO-ARC?s.
With this feature, the oracle can narrow down the
scope of expected arguments of wpred. The other is
a previously identified argument label of warg. The
existence of this feature implies that warg is already
identified as an argument of some other predicate.
For instance, when warg = John and wpred = buy in
Table 2, a label A0 is used as a feature to make the
prediction, John
A0
? buy, because John is already
identified as an A0 of wants.
42
Finally, we use wpred?s cluster ID as a feature. The
dynamic and clustering features combine a very
small portion of the entire feature set, but still give a
fair improvement to labeling accuracy.
5 Experiments
5.1 Corpora
All models are trained on Wall Street Journal sec-
tions 2-21 and developed on section 24 using auto-
matically generated lemmas and POS tags, as dis-
tributed by the CoNLL?09 shared task (Hajic? et al,
2009). CoNLL?09 data contains semantic roles for
both verb and noun predicates, for which we use
only ones related to verb predicates. Furthermore,
we do not include predicate sense classification as a
part of our task, which is rather a task of word sense
disambiguation than semantic role labeling.
For in-domain and out-of-domain evaluations,
WSJ section 23 and the Brown corpus are used, also
distributed by CoNLL?09. To retrieve automatically
generated dependency trees as input to our semantic
role labeler, we train our open source dependency
parser, called ClearParser3, on the training set and
run the parser on the evaluation sets. ClearParser
uses a transition-based dependency parsing algo-
rithm that gives near state-of-the-art results (Choi
and Palmer, 2011), and mirrors our SRL algorithm.
5.2 Statistical models
We use Liblinear L2-L1 SVM for learning; a linear
classification algorithm using L2 regularization and
L1 loss function. This algorithm is designed to han-
dle large scale data: it assumes the data to be lin-
early separable so does not use any kind of kernel
space (Hsieh et al, 2008). As a result, it significantly
reduces training time compared to typical SVM, yet
performs accurately. For our experiments, we use
the following learning parameters: c = 0.1 (cost),
e = 0.2 (termination criterion), B = 0 (bias).
Since predicate identification is already provided
in the CoNLL?09 data, we do not train NO-PRED.
SHIFT does not need to be trained in general be-
cause the preconditions of SHIFT can be checked
deterministically without consulting statistical mod-
els. NO-ARC? and LEFT-ARC?L are trained to-
gether using the one-vs-all method as are NO-ARC?
3http://code.google.com/p/clearparser/
and RIGHT-ARC?L . Even with multi-classifications,
it takes less than two minutes for the entire training
using Liblinear.
5.3 Accuracy comparisons
Tables 4 and 5 show accuracy comparisons between
three models evaluated on the WSJ and Brown cor-
pora, respectively. ?Baseline? uses the features de-
scribed in Section 4.1. ?+Dynamic? uses all baseline
features and the dynamic features described in Sec-
tion 4.2. ?+Cluster? uses all previous features and the
clustering feature. Even though our baseline system
already has high performance, each model shows an
improvement over its previous model (very slight
for ?+Cluster?). The improvement is greater for the
out-of-domain task, implying that the dynamic and
clustering features help more on new domains. The
differences between ?Baseline? and ?+Dynamic? are
statistically significant for both in and out-of domain
tasks (Wilcoxon signed-rank test, treating each sen-
tence as an individual event, p ? 0.025).
Task P R F1
Baseline
AI 92.57 88.44 90.46
AI+AC 87.20 83.31 85.21
+Dynamic
AI 92.38 88.76 90.54
AI+AC 87.33 83.91 85.59?
+Cluster
AI 92.62 88.90 90.72
AI+AC 87.43 83.92 85.64
JN (2008) AI+AC 88.46 83.55 85.93
Table 4: Labeling accuracies evaluated on the WSJ (P:
precision, R: recall, F1: F1-score, all in %). ?AI? and
?AC? stand for argument identification and argument clas-
sification, respectively.
Task P R F1
Baseline
AI 90.96 81.57 86.01
AI+AC 77.11 69.14 72.91
+Dynamic
AI 90.90 82.25 86.36
AI+AC 77.41 70.05 73.55?
+Cluster
AI 90.87 82.43 86.44
AI+AC 77.47 70.28 73.70
JN (2008) AI+AC 77.67 69.63 73.43
Table 5: Labeling accuracies evaluated on the Brown.
We also compare our results against another state-
of-the-art system. Unfortunately, no other system
43
has been evaluated with our exact environmental set-
tings. However, Johansson and Nugues (2008), who
showed state-of-the-art performance in CoNLL?08,
evaluated their system with settings very similar to
ours. Their task was exactly the same as ours;
given predicate identification, they evaluated their
dependency-based semantic role labeler for argu-
ment identification and classification on the WSJ
and Brown corpora, distributed by the CoNLL?05
shared task (Carreras and Ma`rquez, 2005). Since
the CoNLL?05 data was not dependency-based, they
applied heuristics to build dependency-based predi-
cate argument structures. Their converted data may
appear to be a bit different from the CoNLL?09 data
we use (e.g., hyphenated words are tokenized by the
hyphens in CoNLL?09 data whereas they are not in
CoNLL?05 data), but semantic role annotations on
headwords should look very similar.
Johansson and Nugues?s results are presented as
JN (2008) in Tables 4 and 5. Our final system shows
comparable results against this system. These re-
sults are meaningful in two ways. First, JN used a
graph-based dependency parsing algorithm that gave
higher parsing accuracy for these test sets than the
transition-based dependency parsing algorithm used
in ClearParser (about 0.9% better in labeled attach-
ment score). Even with poorer parse output, our SRL
system performed as well as theirs. Furthermore,
our system used only one set of features, which
makes the feature engineering easier than JN?s ap-
proach that used different sets of features for argu-
ment identification and classification.
6 Conclusion and future work
This paper makes two contributions. First, we in-
troduce a transition-based semantic role labeling al-
gorithm that shows comparable performance against
another state-of-the-art system. The new algorithm
takes advantage of using previous predictions as fea-
tures to make the next predictions. Second, we
suggest a self-learning clustering technique that im-
proves labeling accuracy slightly in both the do-
mains. The clustering technique shows potential for
improving performance in other new domains.
These preliminary results are promising; however,
there is still much room for improvement. Since our
algorithm is transition-based, many existing tech-
niques such as k-best ranking (Zhang and Clark,
2008) or dynamic programming (Huang and Sagae,
2010) designed to improve transition-based parsing
can be applied. We can also apply different kinds of
clustering algorithms to improve the quality of the
verb clusters. Furthermore, more features, such as
named entity tags or dependency labels, can be used
to form a better representation of feature vectors for
the clustering.
One of the strongest motivations for designing our
transition-based SRL system is to develop a joint-
inference system between dependency parsing and
semantic role labeling. Since we have already de-
veloped a dependency parser, ClearParser, based
on a parallel transition-based approach, it will be
straightforward to integrate this SRL system with the
parser. We will also explore the possiblity of adding
empty categories during semantic role labeling.
7 Related work
Nivre (2008) introduced several transition-based de-
pendency parsing algorithms that have been widely
used. Johansson and Nugues (2008) and Zhao
et al (2009) presented dependency-based semantic
role labelers showing state-of-the-art performance
for the CoNLL?08 and ?09 shared tasks in English.
Scheible (2010) clustered predicate argument struc-
tures using EM training and the MDL principle.
Wagner et al (2009) used predicate argument clus-
tering to improve verb sense disambiguation.
Acknowledgments
We gratefully acknowledge the support of the
National Science Foundation Grants CISE-IIS-
RI-0910992, Richer Representations for Machine
Translation, a subcontract from the Mayo Clinic and
Harvard Children?s Hospital based on a grant from
the ONC, 90TR0002/01, Strategic Health Advanced
Research Project Area 4: Natural Language Pro-
cessing, and a grant from the Defense Advanced
Research Projects Agency (DARPA/IPTO) under
the GALE program, DARPA/CMO Contract No.
HR0011-06-C-0022, subcontract from BBN, Inc.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the National Science Foundation.
44
References
X. Carreras and L. Ma`rquez. 2005. Introduction to the
conll-2005 shared task: semantic role labeling. In Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning.
J. D. Choi and M. Palmer. 2010. Retrieving correct se-
mantic boundaries in dependency structure. In Pro-
ceedings of ACL workshop on Linguistic Annotation.
J. D. Choi and M. Palmer. 2011. Getting the most out
of transition-based dependency parsing. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3).
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,
M. A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?,
J. S?te?pa?nek, P. Stran?a?k, M. Surdeanu, N. Xue, and
Y. Zhang. 2009. The conll-2009 shared task: Syntac-
tic and semantic dependencies in multiple languages.
In Proceedings of the 13th Conference on Computa-
tional Natural Language Learning: Shared Task.
J. A. Hartigan. 1975. Clustering Algorithms. New York:
John Wiley & Sons.
J. Henderson, P. Merlo, G. Musillo, and I. Titov. 2008.
A latent variable model of synchronous parsing for
syntactic and semantic dependencies. In Proceedings
of the Twelfth Conference on Computational Natural
Language Learning.
T. Hofmann and J. Puzicha. 1998. Statistical models for
co-occurrence data. Technical report, Massachusetts
Institute of Technology.
C. Hsieh, K. Chang, C. Lin, S. S. Keerthi, and S. Sun-
dararajan. 2008. A dual coordinate descent method
for large-scale linear svm. In Proceedings of the 25th
international conference on Machine learning.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics.
R. Johansson and P. Nugues. 2008. Dependency-based
semantic role labeling of PropBank. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing.
S. D. Kamvar, D. Klein, and C. D. Manning. 2002. Inter-
preting and extending classical agglomerative cluster-
ing algorithms using a model-based approach. In Pro-
ceedings of the 9th International Conference on Ma-
chine Learning.
D. Liu and D. Gildea. 2010. Semantic role features for
machine translation. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics.
C. Lo, J. Luo, and M. Shieh. 2009. Hardware/software
codesign of resource constrained real-time systems. In
Proceedings of the 5th International Conference on In-
formation Assurance and Security.
R. Mcdonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Pro-
ceedings of the Annual Meeting of the European Amer-
ican Chapter of the Association for Computational
Linguistics.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4).
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
S. Pradhan, W. Ward, and J. H. Martin. 2008. Towards
robust semantic role labeling. Computational Linguis-
tics: Special Issue on Semantic Role Labeling, 34(2).
C. Scheible. 2010. An evaluation of predicate argument
clustering using pseudo-disambiguation. In Proceed-
ings of the 7th conference on International Language
Resources and Evaluation.
D. Shen and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing and on Computational Natural Lan-
guage Learning.
I. Titov, J. Henderson, P. Merlo, and G. Musillo. 2009.
Online graph planarisation for synchronous parsing of
semantic and syntactic dependencies. In Proceedings
of the 21st International Joint Conference on Artificial
Intelligence.
W. Wagner, H. Schmid, and S. Schulte im Walde.
2009. Verb sense disambiguation using a predicate-
argument-clustering model. In Proceedings of the
CogSci Workshop on Distributional Semantics beyond
Concrete Concepts.
J. H. Ward. 1963. Hierarchical grouping to optimize an
objective function. Journal of the American Statistical
Association, 58(301).
N. Xue and M. Palmer. 2004. Calibrating features for se-
mantic role labeling. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing.
Y. Zhang and S. Clark. 2008. A tale of two parsers: in-
vestigating and combining graph-based and transition-
based dependency parsing using beam-search. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
H. Zhao, W. Chen, and C. Kit. 2009. Semantic depen-
dency parsing of NomBank and PropBank: An effi-
cient integrated approach via a large-scale feature se-
lection. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
45
