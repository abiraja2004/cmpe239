Coling 2010: Poster Volume, pages 338?346,
Beijing, August 2010
Comparing the performance of two TAG-based surface realisers using
controlled grammar traversal
Claire Gardent
CNRS/LORIA
claire.gardent@loria.fr
Benjamin Gottesman
acrolinx GmbH
ben.gottesman@acrolinx.com
Laura Perez-Beltrachini
Universite? Henri Poincare?/LORIA
laura.perez@loria.fr
Abstract
We present GENSEM, a tool for generat-
ing input semantic representations for two
sentence generators based on the same re-
versible Tree Adjoining Grammar. We
then show how GENSEM can be used
to produced large and controlled bench-
marks and test the relative performance of
these generators.
1 Introduction
Although computational grammars are mostly
used for parsing, they can also be used to gener-
ate sentences. This has been done, for instance,
to detect overgeneration by the grammar (Gardent
and Kow, 2007). Sentences that are generated but
are ungrammatical indicate flaws in the grammar.
This has also been done to test a parser (Neder-
hof, 1996; Purdom, 1972). Using the sentences
generated from the grammar ensures that the sen-
tences given to the parser are in the language it de-
fines. Hence a parse failure necessarily indicates
a flaw in the parser?s design as opposed to a lack
of coverage by the grammar.
Here we investigate a third option, namely, the
focused benchmarking of sentence realisers based
on reversible grammars, i.e. on grammars that can
be used both to produce sentences from a semantic
representation and semantic representations from
a sentence.
More specifically, we present a linguistically-
controlled grammar traversal algorithm for Tree
Adjoining Grammar (TAG) which, when applied
to a reversible TAG, permits producing arbitrarily
many of the semantic representations associated
by this TAG with the sentences it generates. We
then show that the semantic representations thus
produced can be used to compare the relative per-
formance of two sentence generators based on this
grammar.
Although the present paper concentrates on
Tree Adjoining Grammar realisers, it is worth
pointing out that the semantic representations pro-
duced could potentially be used to evaluate any
surface realiser whose input is a flat semantic for-
mula.
Section 2 discusses related work and motivates
the approach. Section 3 presents GENSEM, the
DCG-based grammar traversal algorithm we de-
veloped. We show, in particular, that the use of
a DCG permits controlling grammar traversal in
such a way as to systematically generate sets of se-
mantic representations covering certain computa-
tionally or linguistically interesting cases. Finally,
Section 4 reports on the benchmarking of two sur-
face realisers with respect to a GENSEM-produced
benchmark.
2 Motivations
Previous work on benchmark construction for
testing the performance of surface realisers falls
into two camps depending on whether or not the
realiser uses a reversible grammar, that is, a gram-
mar that can be used for both parsing and genera-
tion.
To test a surface realiser based on a large
reversible Head-Driven Phrase Structure Gram-
mar (HPSG), Carroll et al (1999) use a small
test set of two hand-constructed and 40 parsing-
derived cases to test the impact of intersective
modifiers1 on generation performance. More re-
cently, Carroll and Oepen (2005) present a perfor-
1As first noted by Brew (1992) and Kay (1996), given a
set of n modifiers all modifying the same structure, all pos-
sible intermediate structures will be constructed, i.e., 2n+1.
338
mance evaluation which uses as a benchmark the
set of semantic representations produced by pars-
ing 130 sentences from the Penn Treebank and
manually selecting the correct semantic represen-
tations. Finally, White (2004) profiles a CCG2-
based sentence realiser using two domain-focused
reversible CCGs to produce two test suites of 549
and 276 ? semantic formula, target sentence ?
pairs, respectively.
For realisers that are not based on a reversible
grammar, there are approaches which derive large
sets of realiser input from the Penn Treebank
(PTB). For example, Langkilde-Geary (2002)
proposes to translate the PTB annotations into a
format accepted by her sentence generator Halo-
gen. The output of this generator can then be au-
tomatically compared with the PTB sentence from
which the corresponding input was derived. Simi-
larly, Callaway (2003) builds an evaluation bench-
mark by transforming PTB trees into a format
suitable for the KPML realiser he uses.
In all of the above cases, the data is derived
from real world sentences, thereby exemplifying
?real world complexity?. If the corpus is large
enough (as in the case of the PTB), the data can
furthermore be expected to cover a broad range of
syntactic phenomena. Moreover, the data, being
derived from real world sentences, is not biased
towards system-specific capabilities. Nonethe-
less, there are also limits to these approaches.
First, they fail to support graduated perfor-
mance testing on constructs such as intersective
modifiers or lexical ambiguity, which are known
to be problematic for surface realisation.
Second, the construction of the benchmark is in
both cases time consuming. In the reversible ap-
proach, for each input sentence, the correct inter-
pretation must be manually selected from among
the semantic formulae produced by the parser. As
a side effect, the constructed benchmarks remain
relatively small (825 in the case of White (2004);
130 in Carroll and Oepen (2005)). In the case
of a benchmark derived by transformation from
a syntactically annotated corpus, the implemen-
tation of the converter is both time-intensive and
corpus-bound. For instance, Callaway (2003) re-
2Combinatory Categorial Grammar
ports that the implementation of such a proces-
sor for the SURGE realiser was the most time-
consuming part of the evaluation with the result-
ing component containing 4000 lines of code and
900 rules.
As we shall show in the following sections,
the GENSEM approach to benchmark construction
aims to address both of these shortcomings. By
using a DCG to implement grammar traversal, it
permits both a full automation of the benchmark
creation and some control over the type and the
distribution of the benchmark items.
3 GenSem
As mentioned above, GENSEM is a grammar
traversal algorithm for TAG. We first present the
specific TAG used for traversal, namely SEMX-
TAG (Alahverdzhieva, 2008) (section 3.1). We
then show how to automatically derive a DCG
that describes the derivation trees of this gram-
mar (section 3.2). Finally, we show how this DCG
encoding permits generating formulae while en-
abling control over the set of semantic representa-
tions to be produced (section 3.3).
3.1 SemXTAG
The SEMXTAG grammar used by GENSEM and
by the two surface realisers is a Feature-Based
Lexicalised Tree Adjoining Grammar augmented
with a unification-based semantics as described by
Gardent and Kallmeyer (2003). We briefly intro-
duce each of these components and describe the
grammar coverage.
FTAG. A Feature-based TAG (Vijay-Shanker
and Joshi, 1988) consists of a set of (auxil-
iary or initial) elementary trees and of two tree-
composition operations: substitution and adjunc-
tion. Initial trees are trees whose leaves are la-
belled with substitution nodes (marked with a
downarrow) or terminal categories. Auxiliary
trees are distinguished by a foot node (marked
with a star) whose category must be the same as
that of the root node. Substitution inserts a tree
onto a substitution node of some other tree while
adjunction inserts an auxiliary tree into a tree. In
an FTAG, the tree nodes are furthermore deco-
rated with two feature structures (called top and
339
bottom) which are unified during derivation as
follows. On substitution, the top of the substitu-
tion node is unified with the top of the root node
of the tree being substituted in. On adjunction,
the top of the root of the auxiliary tree is unified
with the top of the node where adjunction takes
place; and the bottom features of the foot node are
unified with the bottom features of this node. At
the end of a derivation, the top and bottom of all
nodes in the derived tree are unified. Finally, each
sentence derivation in an FTAG is associated with
both a derived tree representing the phrase struc-
ture of the sentence and a derivation tree record-
ing how the corresponding elementary trees were
combined to form the derived tree.
FTAG with semantics. To associate seman-
tic representations with natural language expres-
sions, the FTAG is modified as proposed by Gar-
dent and Kallmeyer (2003).
NPj
John
name(j,john)
Sb
NP?c VPba
Va
runs
run(a,c)
VPx
often VP*x
often(x)
? name(j,john), run(a,j), often(a)
Figure 1: Flat semantics for ?John often runs?
Each elementary tree is associated with a flat
semantic representation. For instance, in Fig-
ure 1,3 the trees for John, runs, and often are asso-
ciated with the semantics name(j,john), run(a,c),
and often(x), respectively. Importantly, the argu-
ments of a semantic functor are represented by
unification variables which occur both in the se-
mantic representation of this functor and on some
nodes of the associated syntactic tree. For in-
stance in Figure 1, the semantic index c occur-
ring in the semantic representation of runs also
occurs on the subject substitution node of the as-
sociated elementary tree. The value of semantic
arguments is determined by the unifications re-
sulting from adjunction and substitution. For in-
stance, the semantic index c in the tree for runs is
3Cx/Cx abbreviate a node with category C and a
top/bottom feature structure including the feature-value pair
{ index : x}.
unified during substitution with the semantic in-
dex labelling the root node of the tree for John.
As a result, the semantics of John often runs is
{name(j,john),run(a,j),often(a)}.
SemXTAG. SEMXTAG is an FTAG for En-
glish augmented with a unification-based compo-
sitional semantics of the type described above.
Its syntactic coverage approaches that of XTAG,
the FTAG developed for English by the XTAG
group (The XTAG Research Group, 2001). Like
this grammar, it contains around 1300 elementary
trees and covers auxiliaries, copula, raising and
small clause constructions, topicalization, relative
clauses, infinitives, gerunds, passives, adjuncts,
ditransitives and datives, ergatives, it-clefts, wh-
clefts, PRO constructions, noun-noun modifica-
tion, extraposition, sentential adjuncts, impera-
tives and resultatives.
3.2 Converting SemXTAG to a DCG
We would like to be able to traverse SEMXTAG in
order to generate semantic representations that are
licensed by it. In the DCG formalism, a grammar
is represented as a set of Prolog definite clauses,
and Prolog?s query mechanism provides built-in
grammar traversal. We take advantage of this by
deriving a DCG from SEMXTAG and then using
Prolog queries to generate semantic representa-
tions that are associated with sentences in the lan-
guage described by it.
Another advantage of the DCG formalism is
that arbitrary Prolog goals can be inserted into a
rule, to constrain when the rule applies or to bind
variables occurring in it. We use this to ground
derivations with lexical items, which are repre-
sented using Prolog assertions. We also use it to
control Prolog?s grammar traversal in such a way
as to generate sets of semantic formulae covering
certain computationally interesting cases (see sec-
tion 3.3).
Our algorithm for converting SEMXTAG to a
DCG is inspired by Schmitz and Le Roux (2008),
who derive from an FTAG a feature-based reg-
ular tree grammar (RTG) whose language is the
derivation trees of the FTAG. Indeed, in our im-
plementation, we derive a DCG from such an
RTG, thereby taking advantage of a SEMXTAG-
340
to-RTG converter previously implemented by Syl-
vain Schmitz.
TAG to RTG. In the conversion to RTG4, each
elementary tree in SEMXTAG is converted to a
rule that models the contribution of the tree to
a TAG derivation. A TAG derivation involves
the selection of an initial tree, which has some
nodes requiring substitution and some permitting
adjunction. Let us think of the potential adjunc-
tion sites as requiring, rather than permitting, ad-
junction, but such that the requirement can be sat-
isfied by ?null? adjunction. Inserting another tree
into this initial tree satisfies one of the substitution
or adjunction requirements, but introduces some
new requirements into the resulting tree, in the
form of its own substitution nodes and adjunction
sites.
Thus, intuitively, the RTG representation of a
SEMXTAG elementary tree is a rule that rewrites
the satisfied requirement as a local tree whose root
is a unique identifier of the tree and whose leaves
are the introduced requirements. A requirement
of a substitution or adjunction of a tree of root
category X is written as XS or XA, respectively.
Here, for example, is the translation to RTG of the
TAG tree (minus semantics) for runs in Figure 1,
using the word anchoring the tree as its identifier
(the superscripts abbreviate feature structures: b/t
refers to the bottom/top feature structure and the
upper case letters to the semantic index value, so
[idx : X] is abbreviated to X):
S[t:T ]S ? runs(S
[t:T,b:B]
A NP
[t:C]
S V P
[t:B,b:A]
A V
[t:A]
A )
The semantics of the SEMXTAG tree are carried
over as-is to the corresponding RTG rule. Fur-
ther, the feature structures labelling the nodes of
SEMXTAG trees are carried over to the RTG rules
so as to correctly interact with substitution and
adjunction (see Schmitz and Le Roux (2008) for
more details on this part of the conversion pro-
cess).
To account for the optionality of adjunction,
there are additional rules allowing any adjunction
4For a more precise description of the FTAG to RTG con-
version see Schmitz and Le Roux (2008).
requirement to be rewritten as the symbol ?, a ter-
minal symbol of the RTG.
The terminal symbols of the RTG are thus the
tree identifiers and the symbol ?, and its non-
terminals are XS and XA for each terminal or
non-terminal X of SEMXTAG.
RTG to DCG. Since the right-hand side of each
RTG rule is a local tree ? that is, a tree of depth no
more than one ? we can flatten each of them into
a list consisting of the root node followed by the
leaves without losing any structural information.
This is the insight underlying the RTG-to-DCG
conversion step. Each RTG rule is converted to
a DCG rule that is essentially identical except for
this flattening of the right-hand side. Here is the
translation to DCG of the RTG rule above5:
rule(s,init,Top,Bot,Sem;S;N;VP;V)
--> [runs],
{lexicon(runs,n0V,[run])},
rule(s,aux,Top,[B],S),
rule(np,init,[C],_,N),
rule(vp,aux,[B],[A],VP),
rule(v,aux,[A],_,V),
{Sem =.. [run,A,C]}.
We represent non-terminals of the DCG us-
ing the rule predicate, whose five (non-hidden)6
arguments, in order, are the category, the sub-
script (init for subscript S, aux for subscript
A), the top and bottom feature values, and the se-
mantics. Feature structures are represented us-
ing Prolog lists with a fixed argument position
for each attribute in the grammar (in this ex-
ample, only the index attribute). The semantics
associated with the left-hand-side symbol (here,
Sem;S;N;VP;V, with the semicolon represent-
ing semantic conjunction) are composed of the se-
mantics associated with this rule and those associ-
ated with each of the right-hand-side symbols.
The language of the resulting DCG is neither
the language of the RTG nor the language of
SEMXTAG, and indeed the language of the DCG
does not interest us but rather its derivation trees.
5In practice, the lexicon is factored out, so there is no rule
specifically for runs, but one for intransitive verbs (n0V) in
general. Each rule hooks into the lexicon, so that a given
invocation of a rule is grounded by a particular lexical item.
6The ?? > notation is syntactic sugar for the usual Pro-
log : ? definite clause notation with two hidden arguments
on each predicate. The hidden arguments jointly represent
the list of terminals dominated by the symbol.
341
These are correlated one-to-one with the trees in
the language described by the RTG, i.e. with the
derivation trees of SEMXTAG, and the latter can
be trivially reconstructed from the DCG deriva-
tions. From a SEMXTAG derivation tree, one can
compose the semantic representation of the asso-
ciated sentence, and in fact this semantic compo-
sition occurs as a side effect of a Prolog query
against the DCG, allowing semantic representa-
tions licensed by SEMXTAG to be returned as
query results.
We define a Prolog predicate for querying
against the DCG, as follows. Its one input argu-
ment, Cat, is the label of the root node of the
derivation tree (typically s), and its one output ar-
gument, Sem, is the semantic representation asso-
ciated with that tree7.
genSem(Cat,Sem) :-
rule(Cat,init,_,_,Sem,_,[]).
3.3 Control parameters
In order to give the users some control over the
sorts of semantic representations that they get
back from a query against the DCG, we augment
the DCG in such a way as to allow control over
the TAG family8 of the root tree in the derivation
tree, over the number and type of adjunctions in
the derivation, and over the depth of substitutions.
To implement control over the family is quite sim-
ple: we need merely to index the DCG rules by
family and modify the GENSEM call accordingly.
For instance, the above DCG rule becomes :
rule(s,init,Top,Bot,n0V,Sem;S;NP;VP;V)
--> [runs],
{lexicon(runs,n0V,[run])},
...
We implement restrictions on adjunctions by
adding an additional argument to the grammar
symbols, namely a vector of non-negative inte-
gers representing the number of non-null adjunc-
tions of each type that are in the derivation sub-
tree dominated by the symbol. By ?type? of ad-
junction, we mean the category of the adjunc-
7The 6th and 7th arguments of the rule call are the hidden
arguments needed by the DCG.
8TAG families group together trees which belong to-
gether, in particular, the trees associated with various real-
isation of a specific subcategorisation type. Thus, here the
notion of TAG family is equivalent to that of subcategorisa-
tion type.
tion site. In DCG terms, a non-null adjunction
of a category X is represented as the expansion of
an x/aux symbol other than as ?. So, for ex-
ample, a DCG symbol associated with the vec-
tor [1,0,0,0,0], where the five dimensions of
the vector correspond to the n, np, v, vp, and s
categories, respectively, dominates a subtree con-
taining exactly one n/aux symbol expanded by
a non-epsilon rule, and no other aux symbol ex-
panded by a non-epsilon rule. We link the vector
associated with the root of the derivation to the
query predicate.
We define a special predicate to handle the
divvying up of a mother node?s vector among the
daughters, taking advantage of the fact that the
DCG formalism permits the insertion of arbitrary
Prolog goals into a rule.
Finally, we add an additional argument to the
DCG rule and to the GENSEM?s call to control
the traversal depth with respect to the number of
substitutions applied. The overall depth of each
derivation is therefore constrained both by the
user defined adjunctions and substitution depth
constraints.
Our query predicate now has four input argu-
ments and one output argument:
genSem(Cat,Fam,[N,NP,V,VP,S],Dth,Sem):-
rule(Cat,init,_,_,Fam,
[N,NP,V,VP,S],Dth,Sem,_,[]).
4 Using GENSEM for benchmarking
We now show how GENSEM can be put to work
for comparing two TAG-based surface realisers,
namely GENI (Gardent and Kow, 2007) and RT-
GEN (Perez-Beltrachini, 2009). These two realis-
ers follow globally similar algorithms but differ in
several respects. We show how GENSEM can be
used to produce benchmarks that are tailored to
test hypotheses about how these differences might
impact performance. We then use this GENSEM-
generated benchmark to compare the performance
of the two realisers.
4.1 GenI and RTGen
Both GENI and RTGEN use the SEMXTAG gram-
mar described in section 3.1. Moreover, both re-
alisers follow an algorithm pipelining three main
phases. First, lexical selection selects from the
342
grammar those elementary trees whose semantics
subsumes part of the input semantics. Second,
the tree combining phase systematically tries to
combine trees using substitution and adjunction.
Third, the retrieval phase extracts the yields of
the complete derived trees, thereby producing the
generated sentence(s).
There are also differences however. We now
spell these out and indicate how they might im-
pact the relative performance of the two surface
realisers.
Derived vs. derivation trees. While GENI con-
structs derived trees, RTGEN uses the RTG en-
coding of SEMXTAG sketched in the previous
section to construct derivation trees. These are
then unraveled into derived trees at the final re-
trieval stage. As noted by Koller and Striegnitz
(2002), these trees are simpler than TAG elemen-
tary trees, which can favourably impact perfor-
mance.
Interleaving of feature constraint solving and
syntactic analysis. GENI integrates in the tree
combining phase a filtering step in which the ini-
tial search space is pruned by eliminating from
it all combinations of TAG elementary trees that
cover the input semantics but cannot possibly lead
to a valid derived tree. This filtering eliminates
all combinations of trees such that either the cat-
egory of a substitution node cannot be cancelled
out by that of the root node of a different tree, or
a root node fails to have a matching substitution
site. Importantly, filtering ignores feature infor-
mation and tree combining takes place after filter-
ing. RTGEN, on the other hand, directly combines
derivation trees decorated with full feature struc-
ture information.
Handling of intersective modifiers. GENI and
RTGEN differ in their strategies for handling
modification.
Adapting Carroll and Oepen?s (2005) proposal
to TAG, GENI adopts a two-step tree-combining
process such that in the first step, only substitu-
tion applies, while in the second, only adjunc-
tion is used. Although the number of intermediate
structures generated is still 2n for n modifiers, this
strategy has the effect of blocking these 2n struc-
tures from multiplying out with other structures in
the chart.
RTGEN, on the other hand, uses a standard Ear-
ley algorithm that includes sharing and packing.
Sharing allows intermediate structures common
to several derivations to be represented once only
while packing groups together partial derivation
trees with identical semantic coverage and similar
combinatorics (same number and type of substitu-
tion and adjunction requirements), keeping only
one representative of such groups in the chart.
In this way, intermediate structures covering the
same set of intersective modifiers in a different
order are only represented once and the negative
impact of intersective modifiers is lessened.
4.2 Two GENSEM benchmarks
We use GENSEM to construct two benchmarks de-
signed to test the impact of the differences be-
tween the two realisers and, more specifically, to
compare the relative performance of both realisers
(i) on cases involving intersective modifiers and
(ii) on cases of varying overall complexity.
The MODIFIERS benchmark focuses on
intersective modifiers and contains semantic
formulae corresponding to sentences in-
volving an increasing number of modifiers.
Recall that GENSEM calls are of the form
gensem(Cat,Family,[N,NP,V,VP,S],Dth,Sem)
where N,NP,V,VP,S indicates the number of
required adjunctions in N, NP, V, VP and S,
respectively, while Family constrains the subcate-
gorisation type of the root tree in the derivations
produced by GENSEM. To produce formulae
involving the lexical selection of intersective
modifiers, we set the following constraints. Cat is
set to s and Family is set to either n0V (intransitive
verbs) or n0Vn1 (transitive verbs). Furthermore,
N and V P vary from 0 to 4 thereby requiring the
adjunction of 0 to 4 N and/or VP modifiers. All
other adjunction counters are set to null. To avoid
producing formulae with identical derivation trees
but distinct lemmas, we use a restricted lexicon
containing one lemma of each syntactic type,
e.g. one transitive verb, one intransitive verb, etc.
Given these settings, GENSEM produces 1 789
formulae whose adjunction requirements vary
from 1 to 6. For instance, the semantic formula
343
{sleep(b,c),man(c),a(c),blue(c),sleep(i,c),carefully(b)} (A
sleeping blue man sleeps carefully) extracted
from the MODIFIERS benchmark contains two
NP adjunctions and one VP adjunction.
The MODIFIERS benchmark is tailored to fo-
cus on cases involving a varying number of in-
tersective modifiers. To support a comparison of
the realisers on this dimension, it displays little or
no variation w.r.t. other dimensions, such as verb
type and non-modifying adjunctions.
To measure the performance of the two realisers
on cases of varying overall complexity, we con-
struct a second benchmark (COMPLEXITY) dis-
playing such variety. The GENSEM parameters
for the construction of this suite are the follow-
ing. The verb type (Family) is one of 28 possible
verb types9. The number and type of required ad-
junctions vary from 0 to 4 for N adjunctions, 0 to
1 for NP , 0 to 4 for V P and 0 to 1 for S. The re-
sulting benchmark contains 890 semantic formu-
lae covering an extensive set of verb types and of
adjunction requirements.
4.3 Results
Using the two GENSEM-generated benchmarks,
we now compare GENI and RTGEN. We plot the
average number of chart items against both the
number of intersective modifiers present in the in-
put (Figure 3) and the size of the Initial Search
Space (ISS), i.e., the number of combinations of
elementary TAG trees covering the input seman-
tics to be explored after the lexical selection step
(Figure 2). In our case, the ISS gives a more
meaningful idea about the complexity than con-
sidering only the number of input literals. In an
FTAG, the number of elementary trees selected
9The 28 verb types are
En1V,n0BEn1,n0lVN1Pn2,n0V,n0Va1,n0VAN1,n0VAN1Pn2,
n0VDAN1,n0VDAN1Pn2,n0VDN1,n0VDN1Pn2,n0Vn1,
n0VN1,n0Vn1Pn2,n0VN1Pn2,n0Vn2n1,n0Vpl,n0Vpln1,
n0Vpn1,n0VPn1,n0Vs1,REn1VA2,REn1VPn2,Rn0Vn1A2,
Rn0Vn1Pn2,s0V,s0Vn1,s0Vton1. The notational convention
for verb types is from XTAG and reads as follows. Sub-
scripts indicate the thematic role of the verb argument. n
indicates a nominal, Pn a PP and s a sentential argument. pl
is a verbal particle. Upper case letters describe the syntactic
functor type: V is a verb, E an ergative, R a resultative and
BE the copula. Sequences of upper case letters such as
VAN in n0VAN1 indicate a multiword functor with syntactic
categories V, A, and N. For instance, n0Vn1 indicates a verb
taking two nominal arguments (e.g., like) and n0VAN1 a
verb locution such as to cry bloody murder.
0-
10
0
10
0-
10
00
10
00
-
50
00
50
00
-
10
00
0
10
00
0-
10
00
00
10
00
00
-
50
00
00
50
00
00
-
10
00
00
0
m
o
re
th
an
10
00
00
0
102
103
104
105
106
p
p p
p p
p p p
Initial Search Space (ISS) size
u
n
pa
ck
ed
ch
ar
ts
iz
e
RTGEN-all
RTGEN-level0
p RTGEN-selective
GENI
Figure 2: Performance of realisation approaches
on the COMPLEXITY benchmark, average un-
packed chart size as a function of the ISS com-
plexity.
by a given literal may vary considerably depend-
ing on the number and the size of the tree families
selected by this literal. For instance, a literal se-
lecting the n0Vn2n1 class will select many more
trees than a literal selecting the n0V family be-
cause there are many more ways of realising the
three arguments of a ditransitive verb than the sin-
gle subject argument of an intransitive one. Chart
items include all elementary trees selected by the
lexical selection step as well as the intermediate
and final structures produced by the tree combin-
ing phase. In RTGEN, we distinguish between
the number of structures built before unpacking
(packed chart) and the number of structures ob-
tained after unpacking (unpacked chart).
Both realisers are implemetned in different pro-
gramming languages, GENI is implemented in
Haskell whereas RTGEN in Prolog. As for the
time results comparison, preliminary experiments
show that GENI is faster is simple input cases. On
the other hand, in the case of more complex cases,
the point of producing much less intermediate re-
sults pays off compared to the overhead of the
chart/agenda operations.
344
Overall efficiency. The plot in Figure 2 shows
the results obtained by running both realisers on
the COMPLEXITY benchmark. Recall (cf. sec-
tion 4.2) that the COMPLEXITY benchmark con-
tains input with varying verb arity and a varying
number of required adjunctions. Hence it provides
cases of increasing complexity in terms of ISS to
be explored. Furthermore, test cases in the bench-
mark trigger sentence realisation involving certain
TAG families, which have a certain number of
trees. Those trees within a family often have iden-
tical combinatorics but different features. Conse-
quently, the COMPLEXITY benchmark also pro-
vides an appropriate testbed for testing the im-
pact of feature structure information on the two
approaches to tree combination.
The graphs show that as complexity increases,
the performance delta between GENI and RT-
GEN increases. We conjecture that as complex-
ity grows, the filtering used by GENI does not
suffice to reduce the search space to a manage-
able size. Conversely, the overhead introduced by
RTGEN?s all-in-one, tree-combining Earley with
packing strategy seems compensated throughout
by the construction of a derivation rather than a
derived tree and pays off increasingly as complex-
ity increases.
Modifiers. Figure 3 plots the results obtained by
running the realisers on the MODIFIERS bench-
mark. Here again, RTGEN outperforms GENI and
the delta between the two realisers grows with the
number of intersective modifiers to be handled. A
closer look at the data shows that the global con-
straints set by GENSEM on the number of required
adjunctions covers an important range of varia-
tion in the data complexity. For instance, there
are cases where 4 modifiers modify the same NP
(or VP) and cases where the modifiers are dis-
tributed over two NPs. Similarly, literals intro-
duced into the formula by a GENSEM adjunction
requirement vary in terms of the number of auxil-
iary trees whose selection they trigger. The steep
curve in GENI?s plot suggests that although the
delayed adjunction mechanism helps in avoiding
the proliferation of intermediate incomplete mod-
ifiers? structures, the lexical ambiguity of modi-
fiers still poses a problem. In contrast, RTGEN?s
0 1 2 3 4 5 6 7
103
104
p
p
p
p
p
p
number of modifiers
u
n
pa
ck
ed
ch
ar
ts
iz
e
RTGEN-all
RTGEN-level0
p RTGEN-selective
GENI
Figure 3: Performance of realisation approaches
on the MODIFIERS benchmark, average unpacked
chart size as a function of the number of modifiers.
packing uniformly applies to word order varia-
tions and to the cases of lexical ambiguity raised
by intersective modifiers because the items have
the same combinatoric potential and the same se-
mantics.
5 Conclusion
Surface realisers are complex systems that need to
handle diverse input and require complex compu-
tation. Testing raises among other things the issue
of coverage ? how can the potential input space
be covered? ? and of test data creation ? should
this data be hand tailored, created randomly, or
derived from real world text?
In this paper, we presented an approach which
permits automating the creation of test input for
surface realisers whose input is a flat semantic for-
mula. The approach differs from other existing
evaluation schemes in two ways. First, it permits
producing arbitrarily many inputs. Second, it sup-
ports the construction of grammar-controlled, lin-
guistically focused benchmarks.
We are currently working on further extending
GENSEM with more powerful (recursive) control
restrictions on the grammar traversal; on com-
bining GENSEM with tools for detecting grammar
overgeneration; and on producing a benchmark
that could be made available to the community for
testing surface realisers whose input is either a de-
pendency tree or a flat semantic formula.
345
References
Alahverdzhieva, K. 2008. XTAG using XMG. Mas-
ter?s thesis, U. Nancy 2. Erasmus Mundus Master
?Language and Communication Technology?.
Brew, C. 1992. Letting the cat out of the bag: Gen-
eration for shake-and-bake MT. In Proceedings of
COLING ?92, Nantes, France.
Callaway, Charles B. 2003. Evaluating coverage for
large symbolic NLG grammars. In 18th IJCAI,
pages 811?817, Aug.
Carroll, John and Stephan Oepen. 2005. High effi-
ciency realization for a wide-coverage unification
grammar. 2nd IJCNLP.
Carroll, John, A. Copestake, D. Flickinger, and
V. Paznan?ski. 1999. An efficient chart generator
for (semi-)lexicalist grammars. In Proceedings of
EWNLG ?99.
Gardent, Claire and Laura Kallmeyer. 2003. Seman-
tic construction in FTAG. In 10th EACL, Budapest,
Hungary.
Gardent, Claire and Eric Kow. 2007. Spotting over-
generation suspects. In 11th European Workshop
on Natural Language Generation (ENLG).
Kay, Martin. 1996. Chart generation. In Proceedings
of the 34th annual meeting on Association for Com-
putational Linguistics, pages 200?204, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Koller, Alexander and Kristina Striegnitz. 2002. Gen-
eration as dependency parsing. In Proceedings of
the 40th ACL, Philadelphia.
Langkilde-Geary, Irene. 2002. An empirical verifi-
cation of coverage and correctness for a general-
purpose sentence generator. In Proceedings of the
INLG.
Nederhof, M.-J. 1996. Efficient generation of random
sentences. Natural Language Engineering, 2(1):1?
13.
Perez-Beltrachini, Laura. 2009. Using regular
tree grammars to reduce the search space in sur-
face realisation. Master?s thesis, Erasmus Mundus
Master Language and Communication Technology,
Nancy/Bolzano.
Purdom, Paul. 1972. A sentence generator for testing
parsers. BIT, 12(3):366?375.
Schmitz, S. and J. Le Roux. 2008. Feature uni-
fication in TAG derivation trees. In Gardent, C.
and A. Sarkar, editors, Proceedings of the 9th In-
ternational Workshop on Tree Adjoining Grammars
and Related Formalisms (TAG+?08), pages 141?
148, Tu?bingen, Germany.
The XTAG Research Group. 2001. A lexicalised tree
adjoining grammar for english. Technical report,
Institute for Research in Cognitive Science, Univer-
sity of Pennsylvannia.
Vijay-Shanker, K. and AK Joshi. 1988. Feature Struc-
tures Based Tree Adjoining Grammars. Proceed-
ings of the 12th conference on Computational lin-
guistics, 55:v2.
White, Michael. 2004. Reining in CCG chart realiza-
tion. In INLG, pages 182?191.
346
