Proceedings of the SIGDIAL 2013 Conference, pages 142?144,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Interactive Error Resolution Strategies for  
Speech-to-Speech Translation Systems 
 
 
Rohit Kumar, Matthew Roy, Sankaranarayanan Ananthakrishnan,  
Sanjika Hewavitharana, Frederick Choi 
Speech, Language and Multimedia Business Unit 
Raytheon BBN Technologies 
Cambridge, MA, USA 
{rkumar, mroy, sanantha, shewavit, fchoi}@bbn.com 
 
  
 
Abstract1 
In this demonstration, we will showcase 
BBN?s Speech-to-Speech (S2S) transla-
tion system that employs novel interac-
tion strategies to resolve errors through 
user-friendly dialog with the speaker. 
The system performs a series of analysis 
on input utterances to detect out-of-
vocabulary (OOV) named-entities and 
terms, sense ambiguities, homophones, 
idioms and ill-formed inputs. This analy-
sis is used to identify potential errors and 
select an appropriate resolution strategy. 
Our evaluation shows a 34% (absolute) 
improvement in cross-lingual transfer of 
erroneous concepts in our English to Ira-
qi-Arabic S2S system. 
1 Introduction 
Great strides have been made in Speech-to-
Speech (S2S) translation systems that facilitate 
cross-lingual spoken communication (Stallard et. 
al., 2011). However, in order to achieve broad 
domain coverage and unrestricted dialog capabil-
ity, S2S systems need to be transformed from 
passive conduits of information to active partici-
pants in cross-lingual dialogs. These active par-
ticipants must detect key causes of communica-
tion failures and recover from them in an effi-
cient, user-friendly manner. 
                                                 
Disclaimer: This paper is based upon work supported by the 
DARPA BOLT Program. The views expressed are those of 
the authors and do not reflect the official policy or position 
of the Department of Defense or the U.S. Government. 
 
Distribution Statement A (Approved for Public Release, 
Distribution Unlimited) 
Our ongoing work on eyes-free S2S systems is 
focused on detecting three types of errors that 
affect S2S systems. First, out-of-vocabulary 
(OOV) words are misrecognized as phonetically 
similar words that do not convey the intended 
concept. Second, ambiguous words such as hom-
ophones and homographs often lead to recogni-
tion and translation errors. Also, unseen idioms 
produce erroneous literal translations. Third, user 
errors such as mispronunciations and incomplete 
utterances lead to ASR errors. We will demon-
strate our interactive error resolution strategies to 
recover from each of these error types. 
Section 2 presents our system architecture. 
Section 3 describes nine interactive error resolu-
tion strategies that are the focus of this demon-
stration. An evaluation of our English to Iraqi-
Arabic S2S system is summarized in Section 4. 
2 System Architecture 
 
Figure 1 shows the architecture of our two-way 
 
 
Figure 1: BBN S2S System with Error Recovery 
in English to Iraqi-Arabic direction 
142
English to Iraqi-Arabic S2S translation system. 
In the English to Iraqi direction, the initial Eng-
lish ASR hypothesis and its corresponding trans-
lation are processed through a series of analysis 
(e.g. parsing, sense disambiguation) and error 
detection (e.g. ASR/MT confidence, Homo-
phone/Idiom/Named-Entity detection) modules. 
A detailed discussion on the various error detec-
tion modules can be found in Prasad et. al. 
(2012). A novel Inference Bridge data structure 
supports storage of these analyses in an intercon-
nected and retraceable manner. The potential 
erroneous spans are identified and ranked in an 
order of severity using this data structure. 
Based on the top ranked error, one of nine er-
ror resolution strategies (discussed in Section 3), 
is selected and executed. Each strategy is com-
posed of a sequence of steps which include ac-
tions such as TTS output, user input processing, 
translation (unconstrained or constrained) and 
other error type specific operations. This se-
quence is hand-crafted to efficiently recover 
from an error. Following a multi-expert design 
(Turunen and Hakulinen, 2003), each strategy 
represents an error-specific expert. 
3 Error Resolution Strategies 
Figure 2 illustrates the sequence of steps for the 
nine interaction strategies used by our system.  
The OOV Name and ASR Error strategies are 
designed to interactively resolve errors caused by 
OOV words (names and non-names) as well as 
other generic ASR and MT errors. When a span 
of words is identified as an OOV named-entity, 
the user is asked to confirm whether the audio 
segment corresponding to those words is a name. 
Upon user confirmation, the audio segment is 
spliced into the output target language utterance. 
This is based on the principle that audio seg-
ments containing names are understandable 
across languages. 
In the case where a generic erroneous span is 
detected, the user is asked to rephrase the utter-
ance. This strategy is suitable for handling multi-
ple error types including OOVs, mispronuncia-
tions, and generic ASR/MT errors. Additionally, 
the ASR Errors strategy has been designed to 
 
Figure 2. Interaction Strategies for Error Resolution 
143
capture a large fraction of the OOV name false 
negatives (i.e. missed detections) by allowing the 
user to indicate if the identified erroneous span is 
a name. Because of the confusability between the 
errors handled by these two strategies, we have 
found it beneficial to maintain reciprocity be-
tween them to recover from all the errors handled 
by each of these strategies. 
The four Word Sense (WS) disambiguation 
strategies resolve sense ambiguity errors. The 
underlying principle behind these strategies is 
that the sense of an ambiguous word must be 
confirmed by at least two of four possible inde-
pendent sources of evidence. These four sources 
include (a) the translation system (sense lookup 
corresponding to phrase pair associated with the 
ambiguous word), (b) a list of source-language 
contextual keywords that disambiguate a word, 
(c) the sense predicted by a sense-disambiguation 
model and (d) sense specified by the user. Be-
sides the objective to minimize user effort, these 
multiple sources are necessary because not all of 
them may be available for every ambiguous 
word. Case 1: No Mismatch strategy corresponds 
to the case where sources (a) and (c) agree. Case 
2: Filtered strategy corresponds to the case 
where (a) and (b) agree. In both of these cases, 
the system proceeds to present the translation to 
the Arabic speaker without performing any error 
resolution. If these three sources are unable to 
resolve the sense of a word, the user is asked to 
confirm the sense identified by source (a) as il-
lustrated in Case 3: Mismatch strategy. If the 
user rejects that sense, a list of senses is present-
ed to the user (Case 4: Backoff strategy). The 
user-specified sense then drives constrained de-
coding to obtain an accurate translation. 
Albeit simpler, the two homophone resolution 
strategies mimic the word sense disambiguation 
strategies in principle and design. The observed 
homophone variant produced by the ASR must 
be confirmed either by a homophone disambigu-
ation model (Case 1: No Mismatch) or by the 
user (Case 2: Mismatch). The input utterance is 
modified (if needed) by substituting the resolved 
homophone variant in the ASR output which is 
then translated and presented to the Arabic 
speaker. 
Strategies for resolving errors associated with 
idioms and incomplete utterances primarily rely 
on informing the user about these errors and elic-
iting a rephrasal. For idioms, the user is also giv-
en the choice to force a literal translation when 
appropriate. 
Following a mixed-initiative design, at all 
times, the user has the ability to rephrase their 
utterance as well as to force the system to pro-
ceed with the current translation. This allows the 
user to override system false alarms whenever 
suitable. The interface also allows the user to 
repeat the last system message which is helpful 
for comprehension of some of the synthesized 
system prompts for unfamiliar users. 
4 Summary of Evaluation 
Our S2S system equipped with the error resolu-
tion strategies discussed in the previous section 
was evaluated on 103 English utterances (25 
unique utterances repeated by multiple speakers). 
Each utterance was designed to elicit one of the 
error types listed in Section 1. 
The ASR word error rate for these utterances 
was 23%. The error detection components were 
able to identify 59% of these errors and the cor-
responding error resolution strategies were cor-
rectly triggered. 
The erroneous concepts in 13 of the 103 utter-
ances (12.6%) were translated without any error. 
Using the error resolution strategies, an addition-
al 34% of the erroneous concepts were accurate-
ly translated. This increased precision is 
achieved at the cost of user effort. On average, 
the strategies needed 1.4 clarifications turns per 
utterance. 
Besides focusing on improving the error de-
tection and resolution capabilities, we are cur-
rently working on extending these capabilities to 
two-way S2S systems. Specifically, we are de-
signing interactive strategies that engage both 
users in eyes-free cross-lingual communication. 
References 
David Stallard, Rohit Prasad, Prem Natarajan, Fred 
Choi, Shirin Saleem, Ralf Meermeier, Kriste 
Krstovski, Shankar Ananthakrishnan, and Jacob 
Devlin. 2011. The BBN TransTalk Speech-to-
Speech Translation System. Speech and Language 
Technologies, InTech, 31-52 
Rohit Prasad, Rohit Kumar, Sankaranarayanan Anan-
thakrishnan, Wei Chen, Sanjika Hewavitharana, 
Matthew Roy, Frederick Choi, Aaron Challenner, 
Enoch Kan, Arvind Neelakantan, and Premkumar 
Natarajan. 2012.  Active Error Detection and Reso-
lution for Speech-to-Speech Translation. Intl. 
Workshop on Spoken Language Translation 
(IWSLT), Hong Kong 
Markku Turunen, and Jaakko Hakulinen, 2003. Jaspis 
- An Architecture for Supporting Distributed Spo-
ken Dialogues. Proc. of Eurospeech, Geneva, Swit-
zerland 
144
