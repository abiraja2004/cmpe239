Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 58?66,
Portland, Oregon, June 2011. c?2011 Association for Computational Linguistics
Composing and Updating Verb Argument Expectations:
A Distributional Semantic Model
Alessandro Lenci
University of Pisa, Department of Linguistics
via S. Maria, 36
56126 Pisa (Italy)
alessandro.lenci@ling.unipi.it
Abstract
The aim of this paper is to present a com-
putational model of the dynamic composition
and update of verb argument expectations us-
ing Distributional Memory, a state-of-the-art
framework for distributional semantics. The
experimental results conducted on psycholin-
guistic data sets show that the model is able
to successfully predict the changes on the pa-
tient argument thematic fit produced by differ-
ent types of verb agents.
1 Introduction
A number of studies using different experimental
paradigms (priming, self-paced reading, etc.) have
shown that verbs (eat) activate expectations about
nouns occurring as their arguments (cheese)(McRae
et al, 1998), and vice versa (McRae et al, 2005).
Nouns also activate expectations about other nouns
occurring as co-arguments in the same event (key ?
door)(Hare et al, 2009). These behavioral effects
support the hypothesis that in the mental lexicon
verbs and their arguments are arranged into a web
of mutual expectations. Verb argument expectations
encoded in lexical representations are exploited by
subjects to determine the plausibility of a noun as an
argument of a particular verb (thematic fit, or selec-
tional preferences in the linguistic literature), which
has been proved to have important effects on human
sentence processing (McRae et al, 1998).
In a recent work, Bicknell et al (2010) bring evi-
dence suggesting a more complex view of the orga-
nization and on-line use of verb argument expecta-
tions. In fact, the expectations about the likely fillers
of a given verb argument (e.g., the patient role) de-
pend on the way another verb argument (e.g., the
agent role) is filled. For instance, if the agent noun is
journalist, the most likely patient for the verb check
is spelling, while if the agent noun is mechanic, the
most likely patient for the same verb is brakes. As
a consequence, thematic fit judgments are also sen-
sitive to the way other roles of the same verb are
filled. Bicknell et al (2010) show that this fact has
clear consequences for sentence processing, and ar-
gue that subjects dynamically compute and update
verb argument expectations and thematic fit during
on-line sentence comprehension, by integrating var-
ious types of knowledge about events and their ar-
guments.
The aim of this paper is to present a computa-
tional model of the dynamic composition and up-
date of verb argument expectations using Distri-
butional Memory (DM)(Baroni and Lenci, 2010),
a state-of-the-art Distributional Semantic Model
(DSM). DSMs (aka vector space models) repre-
sent word meaning with vectors encoding corpus-
based co-occurence statistics, under the assumption
of the so-called Distributional Hypothesis (Miller
and Charles, 1991; Sahlgren, 2006): Words occur-
ring in similar contexts are also semantically sim-
ilar (Landauer and Dumais, 1997; Pado? and Lap-
ata, 2007; Turney and Pantel, 2010). Thematic fit
judgments have already been successfully modeled
with DSMs (Erk et al, 2010), but to the best of our
knowledge the problem of how thematic fit is dy-
namically updated depending on the way other ar-
guments are filled has not been addressed yet. The
core of our proposal is that Distributional Memory
58
can be used to represent the subject?s expectations
about the most likely words co-occurring in given
syntactic role. We will add to the original Distri-
butional Memory framework a model for verb argu-
ment expectation composition called ECU, Expec-
tation Composition and Update. Specifically, we
will show how the expectations of an agent-verb pair
about their patient noun argument can be composi-
tionally derived from the DM representation of the
verb and the DM representation of its agent. ECU
is evaluated on the data set used in Bicknell et al
(2010), and the experimental results show that it is
able to successfully predict the changes on the pa-
tient thematic fit with a verb, depending on different
agent fillers. More generally, we want to argue that
the ECU model proposed here can represent a gen-
eral and viable approach to address compositionality
in distributional semantics.
After reviewing some related work in section 2,
we present Distributional Memory (section 3) and
its use to model verb-argument composition (section
4). Experiments and evaluation are reported in sec-
tion 5.
2 Background and related work
Elman (2009) argues that the information on pre-
ferred fillers of one verb argument depends on what
the filler is of one of the other arguments. For in-
stance, the most likely patient of cut is wood, when
the agent is lumberjack, but it is meat, when the
agent is butcher. This claim finds an empirical con-
firmation in the experiments reported by Bicknell et
al. (2010), in which subjects are presented with sen-
tence pairs like the following ones:
(1) The journalistAG checked the spellingPA of
his latest report. (congruent condition)
(2) The mechanicAG checked the spellingPA of
his latest report. (incongruent condition)
Each pair contains the same verb and patient argu-
ment, while differing for the agent argument. In the
congruent condition, the patient is a preferred argu-
ment of the verb, given the agent, e.g. spelling is
something which is typically checked by a journal-
ist. In the incongruent condition, the patient is not a
preferred argument of the verb, given its agent, e.g.
spelling is not something that is typically checked by
a mechanic, who rather checks brakes, engines, etc.
Thematic fit judgments used to determine congru-
ent and incongruent agent-verb-patient tuples were
collected in an off-line norming study. Bicknell et
al. (2010) report that self-paced reading times were
shorter at the word directly following the patient for
the congruent than the incongruent items. Similar
results were obtained in an event-related brain po-
tential (ERP) experiment, in which an N400 effect
was observed immediately at the patient noun in the
incongruent condition. In eye-tracking experiments,
Kamide et al (2003) also demonstrated that the the-
matic fit of an object depended on the other verb ar-
gument fillers.
The conclusion drawn by Bicknell et al (2010) is
that verb argument expectations and thematic fit are
not simply stored in the lexicon, but are rather dy-
namically updated during sentence comprehension,
by integrating various types of knowledge. In fact,
if the verb expectations about an argument role de-
pend on the nouns filling its other arguments, the
hypothesis that they are compositionally updated
is highly plausible, since, ?it is difficult to envi-
sion how the potentially unbounded number of con-
texts that might be relevant could be anticipated and
stored in the lexicon? (Elman 2009: 21).
Thematic fit judgments have been successfully
modeled in distributional semantics. Erk et al
(2010) propose the Exemplar-Based Model of Se-
lectional Preferences, in turn based on Erk (2007).
The thematic fit of a noun n as an argument of a
verb v is measured with the similarity in a vector
space between n and a set of noun exemplars oc-
curring in the same argument role of v. A related
approach is adopted by Baroni and Lenci (2010),
the main difference being that the thematic fit of n
is measured by comparing its vector with a ?proto-
type? vector obtaining by averaging over the vectors
of the most typical arguments of v. In both cases,
the distributional measure of thematic fit is shown to
be highly correlated with human plausibility judge-
ments. Their success notwithstanding, these mod-
els fall short of accounting for the dynamical and
context-sensitive nature of thematic fit. In the next
section, we will extend the Baroni and Lenci (2010)
approach with a model for verb-argument composi-
tion, which is able to account for the argument inter-
dependency phenomena shown by the experiments
59
in Bicknell et al(2010).
If verb argument expectations are likely to be dy-
namically computed integrating knowledge of the
verb with information about its fillers, modeling the-
matic fit with DSM requires us to address composi-
tional representations. DSMs have mostly addressed
semantic issues related to the representation of the
content of single words. However, growing efforts
have recently been devoted to the problem of how
to build distributional semantic representations of
complex expressions (e.g., phrases, sentences, etc.)
by composing the distributional representations of
their component lexical items (Kintsch, 2001; Clark
and Pulman, 2007; Widdows, 2008; Mitchell and
Lapata, 2010). Different proposals to address com-
positionality in DSM exist, but the most common
approach is to model semantic composition as vector
composition. Mitchell and Lapata (2010) systemat-
ically explore various vector composition functions
(e.g., vector addition, vector product, and other more
sophisticated variants thereof), which are used to
build distributional vector representations for verb-
noun and adjective-noun phrases. The various mod-
els for vector composition are then evaluated in a
phrase similarity task.
Erk and Pado? (2008) address a partially differ-
ent and yet crucial aspect of compositionality, i.e.,
the fact that when words are composed, they tend
to affect each other?s meanings. The meaning of
run in The horse runs is in fact different from its
meaning in The water runs (Kintsch, 2001). Erk
and Pado? (2008) claim that words are associated
with various types of expectations (typical events for
nouns, and typical arguments for verbs)(McRae et
al., 1998; McRae et al, 2005) that influence each
other when words compose, thereby altering their
meaning. They model this context-sensitive com-
positionality by distinguishing the lemma vector of
a word w1 (i.e. its out-of-context representation),
from its vector in the context of another word w2.
The vector-in-context for w1 is obtained by combin-
ing the lemma vector of w1 with the lemma vectors
of the expectations activated byw2. For instance, the
vector-in-context assigned to run in The horse runs
is obtained by combining the lemma vector of run
with the lemma vectors of the most typical verbs in
which horse appears as a subject (e.g. gallop, trot,
etc.). Like in Mitchell and Lapata (2010), various
functions to build vectors in contexts are tested. Erk
and Pado? (2008) evaluate their model for context-
sensitive vector representation to predict verb simi-
larity in context (e.g. slump in the context of shoul-
der is more similar to slouch than to decline) and to
rank paraphrases.
Our model draws close inspiration from Erk and
Pado? (2008), with which it shares the importance of
verb argument expectations. However, differently
from them, we want to model how the combination
of a verb with an argument affects its expectations
about the likely fillers of its other arguments. While
Erk and Pado? (2008) test their model on a standard
word similarity task (i.e. they measure the similar-
ity between the vector-in-context of a verb with the
vector of another ?landmark? verb), we evaluate our
model for compositionality in distributional seman-
tics in a thematic fit task. Indeed, to the best of our
knowledge this is the first time in which the issues
of thematic fit and compositionality in DSMs are ad-
dressed together.
3 Distributional Memory
Distributional Memory (DM) (Baroni and Lenci,
2010) is a framework for distributional semantics
aiming at generalizing over different existing ty-
pologies of semantic spaces. Distributional Memory
represents corpus-extracted distributional facts as a
weighted tuple structure T , a set of weighted word-
link-word tuples ??w1, l, w2?,??, such that w1 and
w2 belong to W , a set of content words (e.g. nouns,
verbs, etc.), and l belongs to L, a set of syntag-
matic co-occurrence links between words in a text
(e.g. syntactic dependencies, lexicalized patterns,
etc.). For instance, the tuple ??book,obj, read?,??
encodes the piece of distributional information that
book co-occurs with read in the corpus, and obj
specifies the type of syntagmatic link between these
words, i.e. direct object. The score ? is some func-
tion of the co-occurrence frequency of the tuple in
a corpus and is used to characterize its statistical
salience.
Distributional Memory belongs to the family of
so-called structured DSMs, which take into account
the crucial role played by syntactic structures in
shaping the distributional properties of words. To
qualify as context of a target item, a word must be
60
linked to it by some (interesting) lexico-syntactic re-
lation, which is also typically used to distinguish the
type of this co-occurrence (Lin, 1998; Pado? and Lap-
ata, 2007). Differently from other structured DSMs,
the tuple structure T is formally represented as a
3-way geometrical object, namely a third order la-
beled tensor. A tensor is a multi-way array (Turney,
2007; Kolda and Bader, 2009), i.e. a generalization
of vectors (first order tensors) and matrices (second
order tensors). Different semantic spaces are then
generated ?on demand? through tensor matriciza-
tion, projecting the tuple tensor onto 2-way matrices,
whose rows and columns represent semantic spaces
to deal with different semantic tasks.
For instance, the space W1?LW2 is formed by
vectors for words and the dimensions represent the
attributes of these words in terms of lexico-syntactic
relations with lexical collocates, such as ?obj, read?,
or ?use, pen?. Consistently, this space is most suit-
able to address tasks involving the measurement of
the ?attributional similarity? between words (Tur-
ney, 2006), such as synonym detection or modeling
selectional preferences. Instead, the space W1W2?L
contains vectors associated with word pairs, whose
dimensions are links between these pairs. This space
is thus suitable to address tasks involving the mea-
surement of so-called ?relational similarity? (Tur-
ney, 2006), such as analogy detection or relation
classification (cf. Baroni and Lenci 2010 for more
details about the Distributional Memory spaces and
tasks). Crucially, these spaces are now alternative
?views? of the same underlying distributional mem-
ory formalized in the tensor. Many semantic tasks
(such as analogical similarity, selectional prefer-
ences, property extraction, synonym detection, etc.),
which are tackled in the literature with different, of-
ten unrelated semantic spaces, are addressed in DM
with the same distributional tensor, harvested once
and for all from the corpus. This is the reason why
Distributional Memory is claimed to be a general
purpose resource for semantic modeling.
Depending on the selection of the sets W and L
and of the scoring function ?, different DM mod-
els can be generated. The Distributional Memory
instantiation chosen for the experiments reported in
this paper is TypeDM, whose links include lexical-
ized dependency paths and lexico-syntactic shallow
patterns, with a scoring function based on pattern
type frequency.1 We have chosen TypeDM, be-
cause it is the best performing DM model across
the various semantic tasks addressed in Baroni and
Lenci (2010). The TypeDM tensor contains about
130M non-zero tuples automatically extracted from
a corpus of about 2.83 billion tokens, obtained by
concatenating the the Web-derived ukWaC corpus
(about 1,915 billion tokens),2 a mid-2009 dump of
the English Wikipedia (about 820 million tokens),3
and the British National Corpus (about 95 million
tokens).4 The resulting concatenated corpus was
tokenized, POS-tagged and lemmatized with the
TreeTagger5 and dependency-parsed with the Malt-
Parser.6
The TypeDM word set (WTypeDM ) contains
30,693 lemmas (20,410 nouns, 5,026 verbs and
5,257 adjectives). These are the top 20,000 most fre-
quent nouns and top 5,000 most frequent verbs and
adjectives in the corpus, augmented with lemmas in
various standard test sets in distributional semantics,
such as the TOEFL and SAT lists. The TypeDM
link set (LTypeDM ) contains 25,336 direct and in-
verse links formed by (partially lexicalized) syntac-
tic dependencies and patterns. This is a sample of
the links in LTypeDM :
? obj: The journalist is checking his article ?
?article, obj, check?
? verb: The journalist is checking his article ?
?journalist, verb, article?
? sbj tr: The journalist is checking his article
? ?journalist, sbj tr, check?
? preposition: I saw a journalist with a pen
? ?pen, with, journalist?
? such as: ?NOUN such as NOUN? and ?such
NOUN as NOUN?: animals such as cats ?
?animal, such as+ns+ns, cat?
1The TypeDM tensor is publicly available at http://
clic.cimec.unitn.it/dm
2http://wacky.sslmit.unibo.it/
3http://en.wikipedia.org/wiki/Wikipedia:
Database_download
4http://www.natcorp.ox.ac.uk
5http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger/
6http://w3.msi.vxu.se/?nivre/research/
MaltParser.html
61
The first two links above are the most relevant ones
for the purposes of the present paper: obj links a
transitive verb and its direct object, and verb is a
lexically underspecified link between a subject noun
and a complement noun of the same verb.
The scoring function ? is the Local Mutual
Information (LMI) (Evert, 2005) computed on link
type frequency (negative LMI values are raised to 0):
LMI = Oijk log
Oijk
Eijk
(1)
Oijk and Eijk are respectively the observed and ex-
pected frequency of a triple ?wi, lj , wk?.
4 Composing verb argument expectations
In this section, we address the fact that the infor-
mation on preferred fillers of one verb argument de-
pends on the filler of its other arguments by propos-
ing a model for Expectation Composition and Up-
date (ECU), which will then be computationally for-
malized with Distributional Memory.
ECU relies on the hypothesis that nouns and verbs
are linked in a web of mutual expectations. Verbs
are associated with expectations about their likely
arguments, and nouns have expectations about the
events they are involved with and also about other
nouns co-occuring in the same events (cf. section
1). We argue that, when words compose (e.g. a
verb and a noun), their expectations are integrated
and updated. Specifically, we focus here on how the
composition of a verb and its agent argument deter-
mines an update of the verb expectations for its pa-
tient argument. Let EXPA(v) be the expectations
of a verb v about its patient arguments, i.e. a set
of nouns likely to occur as verb patients. For in-
stance, EXPA(check) = mistakes, engines, books,
etc. Let EX(nAG) be the expectations about typi-
cal events performed and typical entities acted upon
by the agent noun. For instance, EX(mechanic) =
mechanics fix cars, mechanics check oil, etc. ECU
is formally defined as follows:
EXPA(?nAG, v?) = f(EX(nAG), EXPA(v)) (2)
f is some function for expectation composition and
update (cf. below). ECU assumes that the result of
semantically composing the verb and its agent ar-
gument is an update of the verb expectations about
its patient argument. EXPA(?nAG, v?) are the up-
dated expectations of v about its patient arguments,
resulting from the composition of its original ex-
pectations with the agent?s expectations. For in-
stance, the result of composing check with the agent
argument mechanic is a new set of expectations
EXPA(?mechanic, check?) formed by objects that
are likely checked by mechanics, such as cars, en-
gines, wheels, etc. These updated expectations are
a function of the typical patients of checking events,
and of the typical patients of events performed by
mechanics.
4.1 Modeling ECU with Distributional
Memory
The tuple structure of the DM tensor is well suited
to represent the web of mutual expectations in which
lexical items are arranged. In fact, given a word w,
the expectations of w, EX(w), can be defined as
the subset of the DM tensor formed by the tuples
??w1, l, w2?,??, such that w = w1 or w = w2. The
tuple score ? determines the statistical salience and
typicality of a particular expectation.
To model ECU with TypeDM, we approximate
the patient semantic role with the syntactic depen-
dency DM link of obj (cf. section 3). The expec-
tations about the typical patient arguments of a verb
v (EXPA(v)) thus correspond to the set of TypeDM
tuples ?ni, obj, v?: e.g, EXPA(check) = ?mistake,
obj, check?, ?engine, obj, check?, etc. We model
EX(nAG) with the set of DM tuples ?nAG, verb,
nj?, which characterize the typical patients (i.e., di-
rect objects) of events performed by the agent noun
nAG: e.g, EX(mechanic) = ?mechanic, verb,
car?, ?mechanic, verb, oil?, ?mechanic, verb,
engine?, etc.
The expectation composition function f of
equation 2 is modeled as a tensor updating function:
f modifies the TypeDM tensor by updating the
scores of the relevant subset of tuples. Following
current compositionality models in distributional
semantics (cf. Mitchell and Lapata 2010), we focus
here on two alternative versions of f :
62
check ?journalist, check? ?mechanic, check?
site article car
page book tyre
website information work
box question price
detail fact vehicle
link report job
list site system
file source bike
record content value
information account problem
Table 1: Original TypeDM expectations for check and
their compositional updates obtained with f = PRODUCT
SUM
For each tuple ??ni, obj, v?, ?i? ? EXPA(v),
??ni, obj, v?, ?u? ? EXPA(?nAG, v?), and
?u =
?
?
?
?i + ?j if ??nAG, verb, nj?, ?j?
? EX(nAG) and ni = nj
?i otherwise
(3)
PRODUCT
For each tuple ??ni, obj, v?, ?i? ? EXPA(v),
??ni, obj, v?, ?u? ? EXPA(?nAG, v?), and
?u =
?
?
?
?i ? ?j if ??nAG, verb, nj?, ?j?
? EX(nAG) and ni = nj
0 otherwise
(4)
The idea underlying both types of tensor updat-
ing functions is that the verb expectations about its
likely patients are modified with the score of the tu-
ples representing objects that are typical patients of
events performed by the agent noun. With SUM,
expectation composition is a linear function of the
score of the tuples in EXPA(v) and in EX(nAG):
EXPA(?nAG, v?) contains all the tuples belonging
to EXPA(v), but their score is added to the score
of the tuples in EX(nAG) sharing the same object
noun. With the PRODUCT function, the updated ex-
pectations only include the tuples inEXPA(v) shar-
ing the same objects with a tuple in EX(nAG). The
score of these tuples is the product of the original
tuple score, while the score of the other tuples in
EXPA(v) is set to 0.
Table 1 reports a sample output of the ap-
plication of ECU to the TypeDM tensor. The
left column contains the objects of the top-
scoring tuples of EXPA(check) in the original
TypeDM tensor (ordered by decreasing values of
?). The central column contains the top-scoring
nouns in EXPA(?journalist, check?), composi-
tionally derived by updating EXPA(check) with
EX(journalist): the nouns are ordered by de-
creasing value of ? modified according to the PROD-
UCT composition function. We can notice that the
updated verb argument expectations include nouns
consistent with what journalists typically check. The
right column instead contains the top-scoring nouns
in EXPA(?mechanic, check?), derived from up-
dating EXPA(check) with EX(mechanic): the
composition function is still the PRODUCT. The dif-
ference with the central column is striking: the top-
scoring nouns in the updated verb argument expec-
tations are now related to what mechanics typically
check.
5 Experiments and evaluation
The ECU model for the compositional update of
verb-argument expectations has been evaluated by
measuring the thematic fit between an agent-verb
pair (?nAG, v?) and a patient noun argument (nPA)
of the same verb. Thematic fit is computed with
the verb expectations in EXPA(?nAG, v?), which in
turned have been obtained by composing EX(nAG)
and EXPA(v) with either of the two functions de-
scribed in section 4. In the following subsections,
we illustrate the data sets used for the experiments,
the procedure to compute the compositional the-
matic fit in TypeDM, and the results of the experi-
ments.
5.1 Data sets
Two data sets of agent-verb-patient triples from
Bicknell et al (2010) have been used to test ECU:
? bicknell.64 - 64 test triples used in the self-
paced reading and ERP experiments in Bicknell
et al (2010);
? bicknell.100 - 100 test triples, a superset of
bicknell.64.
Triples are organized in pairs, each sharing the same
verb, but differing for the agent and patient nouns:
? journalistAG - check - spellingPA
? mechanicAG - check - brakePA
63
Patients in each triple were produced by 47 sub-
jects as the prototypical (congruent) arguments of
the verbs, given a certain agent. The patient noun
in one triple is incongruent for the other triple with
the same verb: e.g., brake is the incongruent patient
for the mechanicAG - check pair. The bicknell.100
dataset contains all the triples produced in the orig-
inal norming study. The bicknell.64 data set is a
subset of the normed triples selected by Bicknell et
al. (2010) after removing test items that were poten-
tially problematic for the behavioral experiments.
5.2 Procedure
The thematic fit of a noun nPA as the patient of
?nAG, v? is measured with the cosine between the
vector of nPA in the TypeDM W1?LW2 space and
the ?prototype? vector in the same space built with
the vectors of the top-k expected objects belong-
ing to EX(nAG, v). This is an extension of the
approach to selectional preferences modeling pre-
sented in Baroni and Lenci (2010) (in turn inspired
to Erk 2007). These are the steps used to com-
pute the compositional thematic fit in the TypeDM
W1?LW2 space:
1. we select a set of k of prototypical patient
nouns nPA for ?nAG, v? (in the reported exper-
iments we set k = 20). The selected nouns
are the ni in the k tuples ??ni, obj, v?, ?u?
? EXPA(?nAG, v?) with the highest score ?.
The patient nouns in the datasets are excluded;
2. the vectors in the W1?LW2 TypeDM space
of the selected nouns are normalized and
summed. The result is a centroid vector rep-
resenting an abstract ?patient prototype vector?
for ?nAG, v?;
3. for each nAG - v - nPA test triple (e.g., journal-
istAG - check - spellingPA), we measure i.) the
cosine between nPA and the ?patient prototype
vector? for the congruent ?nAG, v? pair, (e.g.,
journalistAG - check) and ii.) the cosine be-
tween nPA and the ?patient prototype vector?
for the incongruent ?nAG, v? pair, belonging to
the other triple with the same verb v (e.g., me-
chanicAG - check).
For each test triple, we score a ?hit? if nPA has a
higher thematic fit (i.e., cosine) with the congruent
?nAG, v? pair, than with the incongruent one. For in-
stance, if cosine(?journalist, check?, spelling) >
cosine(?mechanic, check?, spelling), we score a
?hit?, otherwise we score a ?fail?.
5.3 Results
Experiments to model the verb-argument composi-
tional thematic fit have been carried out with the two
ECU functions, SUM and PRODUCT, each tested on
both datasets. Model performance has been evalu-
ated with ?hit? accuracy, i.e. the percentage of ?hits?
scored on each data set. As a baseline, we have sim-
ply adopted the random accuracy. The results of the
ECU models are reported in table 2.
We can notice that when the verb-argument ex-
pectations are compositionally updated with the
PRODUCT function, the model is able to signifi-
cantly outperform the baseline accuracy with both
data sets. Conversely, SUM is never able to go be-
yond the baseline. This is remindful of the results
reported by Erk and Pado? (2008) and Mitchell and
Lapata (2010), in which multiplicative vector com-
position achieves better performance in the (verb in
context or phrase) similarity tasks than (at least sim-
ple) additive functions. In fact, the advantage of the
multiplicative function is that it allows the composi-
tion process to highlight the dimensions shared by
the vectors of the component words, thereby em-
phasizing context effects. Something similar can
be argued to explain the results of the current ex-
periments. With PRODUCT the expectations of
EXPA(?nAG, v?) are a non-linear function of the
expectations about patient nouns shared by v and
nAG. Therefore, the objects that are likely to be
checked by a mechanic depend on the things that are
both typical patients of checking events and typical
patients of actions performed by a mechanic. This
results in a stronger thematic fit in the congruent
condition than in the incongruent one.
We also carried out experiments to investigate
whether the choice of the parameter k (the number
of nouns selected to build the ?prototype patient vec-
tor?) affects the model performance. However, we
obtained no significant difference with respect to the
values reported in table 2.
64
data set ECU function accuracy p-value
bicknell.64 SUM 40.62%
bicknell.64 PRODUCT 84.37% 3.798e-08 ***
bicknell.100 SUM 37.5%
bicknell.100 PRODUCT 73% 4.225e-06 ***
baseline 50%
Table 2: Results of the thematic fit experiments (p-values
computed with a ?2 test).
6 Conclusions and further directions of
research
Psycholinguistic evidence has proved that verb ar-
gument thematic fit is highly context-sensitive. In
fact, subjects? sensitivity to the likelihood of a noun
as a verb argument strongly depends on the nouns
filling other arguments of the same verb. These
data hint at a dynamic process underlying verb ar-
gument expectation and thematic fit computation,
resulting from the compositional integration of the
verb expectations with those activated by its argu-
ments. In this paper, we have presented ECU, a dis-
tributional semantic model for the compositional up-
date of verb argument expectations. ECU has been
applied to Distributional Memory, a state-of-the-art
Distributional Semantic Model, whose core tensor
of corpus-derived tuples is particularly suited to rep-
resent word expectations. ECU has been tested suc-
cesfully in an experiment to measure the thematic
fit between an agent-verb pair (?nAG, v?) and a pa-
tient noun argument (nPA) of the same verb, with
the data set used in the psycholinguistic experiments
reported in Bicknell et al (2010). The good results
we have obtained prove that DSMs can provide in-
teresting computational models of the compositional
update of thematic fit. Of course, other factors be-
sides verb-argument knowledge may also contribute
to the context-sensitive nature of thematic fit. How-
ever, it is worth noticing that one of the hypotheses
advanced by Bicknell et al (2010) to explain their
experimental results is indeed that subjects use their
knowledge of statistical linguistic regularities. This
is exactly the type of knowledge that is represented
in the Distributional Memory tensor structure and is
exploited by ECU.
Starting from the experimental results in Bick-
nell et al (2010) on sentence on-line processing,
in this paper we have addressed the issue of how
the agent of a verb modulates the subjects? expec-
tations about its patients. On the other hand, there is
broad evidence that the meaning of a verb is predom-
inantly modulated by its object. This suggests that
ECU should also be applied to model how the pref-
erences about the agent argument are determined by
the choice of the verb object. We leave this issue for
future research.
Besides being a computational model for thematic
fit, we also claim that the ECU approach has a
more general relevance for the issue of how to ad-
dress compositionality in DSMs. In fact, let us as-
sume that part of the semantic content of a word
consists of expectations about likely co-occurring
words, which in turn can be modeled with subsets
of a distributional tuple tensor. We can therefore
claim that (at least part of) the effect of the semantic
composition of words is to update their expectations
about other co-occurring words, like ECU does. We
have seen here that this hypothesis finds a nice con-
firmation with verb-argument composition. We be-
lieve that an interesting empirical question is to in-
vestigate to what extent this hypothesis can be gen-
eralized to other cases of compositionality.
In the future, we also plan to experiment with
other types of expectation composition functions.
Moreover we will extend the ECU model to tackle
context-sensitive effects in the thematic fit with re-
spect to other types of verb argument relations, be-
sides agent and patient ones. In fact, Matsuki et
al. (submitted) have reported that patient and instru-
ment verb arguments show interdependency effects
similar to the ones between agents and patients that
we have addressed in this paper.
Acknowledgments
I am very grateful to Ken McRae for providing the
data set used in this paper. I also thank Marco Ba-
roni - the co-author of Distributional Memory - and
the two anonymous reviewers for their helpful com-
ments. The usual disclaimers apply.
65
References
Marco Baroni and Alessandro Lenci. 2010. Distribu-
tional Memory : A general framework for corpus-
based semantics. Computational Linguistics, 36(4):
673?721.
Klinton Bicknell, Jeffrey L. Elman, Mary Hare,
Ken McRae, and Marta Kutas. 2010. Effects of event
knowledge in processing verbal arguments. Journal of
Memory and Language, 63(4): 489?505.
Stephen Clark and Stephen Pulman. 2007. Combining
symbolic and distributional models of meaning. In
Proceedings of the AAAI Spring Symposium on Quan-
tum Interaction: 52?55.
Jeffrey L. Elman. 2009. On the meaning of words and
dinosaur bones: Lexical knowledge without a lexicon.
Cognitive Science, 33(4): 547?582.
Katrin Erk. 2007. A simple, similarity-based model for
selectional preferences. In Proceedings of ACL: 216?
223.
Katrin Erk and Sebastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of EMNLP 08: 897?906.
Katrin Erk, Sebastian Pado?, and Ulrike Pado?. 2010. A
flexible, corpus-driven model of regular and inverse
selectional preferences. Computational Linguistics,
36(4): 723?763.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Ph.D. dissertation, Stuttgart University.
Mary Hare, Michael Jones, Caroline Thomson,
Sarah Kelly,and Ken McRae. 2009. Activating
event knowledge. Cognition, 111(2): 151?167.
Yuki Kamide, Gerry T.M. Altmann, and Sarah L. Hay-
wood. 2003. The time-course of prediction in incre-
mental sentence processing: Evidence from anticipa-
tory eye movements. Journal of Memory and Lan-
guage, 49: 133?156.
Walter Kintsch. 2001. Predication. Cognitive Science,
25(2): 173?202.
Tamara Kolda and Brett Bader. 2009. Tensor decomposi-
tions and applications. SIAM Review, 51(3): 455?500.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis.
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2): 1?31.
Dekang Lin 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL: 768?
774.
Kazunaga Matsuki, Tracy Chow, Mary Hare, Jeffrey L.
Elman, Christoph Scheepers, and Ken McRae. sub-
mitted for publication. Event-based plausibility im-
mediately influences on-line language comprehension
Ken McRae, Michael J. Spivey-Knowlton, and
Michael K.Tanenhaus. 1998. Modeling the in-
fluence of thematic fit (and other constraints) in
on-line sentence comprehension Journal of Memory
and Language, 38: 283?312.
Ken McRae, Mary Hare, Jeffrey L. Elman, and Todd Fer-
retti. 2005. A basis for generating expectancies for
verbs from nouns Memory & Cognition, 33(7): 1174?
1184.
GeorgeMiller andWalter Charles. 1991. Contextual cor-
relates of semantic similarity Language and Cognitive
Processes, 6: 1?28.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8): 1388?1429.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2): 161?199.
Magnus Sahlgren. 2006. The Word-Space Model. Ph.D.
dissertation, Stockholm University.
Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3): 379?416.
Peter D. Turney. 2007. Empirical evaluation of four ten-
sor decomposition algorithms. Technical Report ERB-
1152, NRC.
Peter D. Turney, Patrick Pantel. 2010. From frequency to
meaning : Vector space models of semantics. Journal
of Artificial Intelligence Research, 37: 141?188.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Proceedings of the Sec-
ond AAAI Symposium on Quantum Interaction: 1?8.
66
