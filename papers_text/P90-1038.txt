BOTTOM-UP PARSING EXTENDING CONTEXT-FREENESS 
IN  A PROCESS GRAMMAR PROCESSOR 
Mass imo Mar ino 
Department  o f  L inguist ics - Univers i ty o f  Pisa 
Via S. Mar ia 36 1-56100 Pisa - ITALY  
Bitnet: massimom@icnucevm.cnuce.cnr.it 
ABSTRACT 
A new approach to bottom-up arsing that extends 
Augmented Context-Free Grammar to a Process Grammar 
is formally presented. A Process Grammar (PG) defines a 
set of rules suited for bottom-up arsing and conceived as 
processes that are applied by a P G Processor. The matching 
phase is a crucial step for process application, and a 
parsing structure for efficient matching is also presented. 
The PG Processor is composed of a process cheduler that 
allows immediate constituent analysis of structures, and 
behaves in a non-deterministic fashion. On the other side, 
the PG offers means for implementing spec~c parsing 
strategies improving the lack of determinism innate in the 
processor. 
1. INTRODUCTION 
Bottom-up parsing methods are usually preferred 
because of their property of being driven from both the 
input's syntactic/semantic structures and reduced 
constituents structures. Different strategies have been 
realized for handling the structures construction, e.g., 
parallel parsers, backtracking parsers, augmented context- 
free parsers (Aho et al, 1972; Grishman, 1976; Winograd, 
1983). The aim of this paper is to introduce a new approach 
to bottom-up parsing starting from a well known and based 
framework - parallel bottom-up arsing in immediate 
constituent analysis, where all possible parses are 
considered - making use of an Augmented Phrase-S tructure 
Grammar (APSG). In such environment we must perform 
efficient searches inthe graph the parser builds, and limit as 
much as possible the building of structures that will not be 
in the final parse tree. For the efficiency of the search we 
introduce a Parse Graph Structure, based on the def'mition of 
adjacency of the subtrees, that provides an easy method of 
evaluation for deciding at any step whether a matching 
process can be accomplished or not. The control of the 
parsing process is in the hands of an APSG called Process 
Grammar fPG), where grammar rules are conceived as 
processes that are applied whenever proper conditions, 
detected by a process scheduler, exist. This is why the 
parser, called PG Processor, works following a non- 
deterministic parallel strategy, and only the Process 
Grammar has the power of altering and constraining this 
behaviour by means of some Kernel Functions that can 
modify the control structures of the PG Processor, thus 
299 
improving determinism ofthe parsing process, or avoiding 
construction of useless tructures. Some of the concepts 
introduced inthis paper, such as some definitions in Section 
2,  are a development from Grishman (1976) that can be also 
an introductory reading regarding the description of a 
parallel bottom-up parser which is, even if under adifferent 
aspect, the core of the PG Processor. 
2. PARSE GRAPH STRUCTURE 
The Parse Graph Structure (PGS) is built by the parser 
while applying rammar rules. If s = a a a2... a is an input 
string the initial PGS is composed by a set of terminal nodes 
<0,$>, <l,aa>, <2,a2> ..... <n,a >, <n+l,$>, where nodes 
0,n+ 1 represent border markers for the sentence. All the next 
non-terminal nodes are numbered starting from n+2. 
Definition 2.1. A PGS is a triple (Nr,Nr~,T) where N r is the 
set of the terminal nodes numbers {0, 1 .. . . .  n, n+l}; N N is 
the set of the non-terminal nodes numbers {n+2 .... }, and T 
is the set of the subtrees. 
The elements of N N and N T are numbers identifying nodes 
of the PGS whose structure is defined below, and 
throughout the paper we refer to nodes of the PGS by means 
of such nodes number. 
Definition 2.2. If ke Nr~ the node ie N r labeling a i at the 
beginning of the clause covered by k is said to be the left 
corner leaf of k lcl(k). If ke N r then lcl(k)=k. 
Definition 2.3. I fke N s the nodeje N T labeling aj at the end 
of the clause covered by k is said to be the right corner leaf 
of k rcl(k). If ke N T then rcl(k) = k. 
Definition 2.4. Ifk~ N N the node he N r that follows the right 
corner leaf of k rel(k) is said to be the anchor leafofk al(k), 
and al(k) = h = rel(k)+L IfkeNT-{n+l }then al(k) = k+l. 
Definition 2.5. If ke N T the set of the anchored nodes of 
k an(k) is an(k) = {j~ NTUN s I alQ) = k}. 
From this definition it follows that for every ke NT-{0}, 
an(k) contains at the initial time the node number (k-l). 
Definition 2.6. a. If keN T the subtree rooted in k T(k) is 
represented byT(k) = <k,lcl(k),rcl(k),an(k),cat(k)>, where 
kis theroot node; lcl(k)-- rel(k)= k; an(k) = {(k-l)} initially; 
cat(k) = a~, the terminal category of the node. 
b. If ke Nr~ the subtree rooted in k T(k) is represented by 
T(k)=<k,lcl(k),rcl(k),sons(k),cat(k)>, where k is the root 
node; sons(k) = {s I..... sv}, sic NTuN s, i = 1 ..... p, is the set 
of the direct descendants of k; cat(k) = A, a non-terminal 
category assigned to the node. 
From the above definitions the initial PGS for a 
sentence s=a~av..a n is: Nr={0,1 ..... n,n+l}, Ns={}, 
T= { T(0),T(1 ) ..... T(n) ,T(n+ 1 ) }; and: T(0)=<0,0,0, { } $>, 
T(i)=<i,i,i, {i- 1 } ,ai> for i= 1 ..... n, and T(n+ 1)=<n+ 1,
n+l,n+l,{n} ,$>. With this PGS the parser starts its work 
reducing new nodes from the already existing ones. If for 
some k~Nr~, T(k)=<k,lcl(k),rcl(k),{s 1 ..... sp},A>, and 
T(s)=<si,lcl(sl),rcl(s~),{ s n ..... s~t},zi>e T, for i = 1 ..... p0 are 
the direct descendants of k, then k has been reduced from 
s~ .... ,s t by some grammar rule whose reduction rule, as we 
shall see later, has the form (A~---z v ..zp), and the following 
holds: lcl(k) = lcl(st), rcl(s~) = lcl(s2)-l, rcl(s2) = lcl(ss)-1 ..... 
rcl(sr, l) = lcl(sr)- 1, rcl(sp) = rcl(k). From that we can give the 
following definition: 
<12,a12> 
<14,a14> <13,a13> 
<0,$> <l,al> <2,a2> 
{} {0} {I} 
of the match process the matcher must start from the last 
scanned or built node z s, finding afterwards z 2 and z~, 
respectively, sailing in the PGS right-to-left and passing 
through adjacent subtrees. Steps through adjacent subtrees 
are easily accomplished by using the sets of the anchored 
nodes in the terminal nodes. It follows from the above 
def'mitions that if k~ N N then the subtrees adjacent to T(k) 
are given by an(lel(k)), whereas ff k~ N r then the adjacent 
subtrees are given by an(k). The lists of the anchored nodes 
provide an efficient way to represent the relation of 
adjacency between odes. These sets stored only in the 
terminal nodes provide an efficient data structure useful for 
the matcher to accomplish its purpose. Figure 1 shows a 
parse tree at a certain time of a parse, where under each 
I 
T(9) = <9,1,2,{1,2},a9> 
TOO) = <10,2,2,{2},a10> 
T(ll) = <11,2,3,{ 10,3},al 1> 
T(12) = <12,1,3,{9,3},a12> 
T(13) = <13,4,5, {4,5 },a13> 
T(14) -- <14,3,5,{3,4,5 } a14> 
<3#3> <4,a4> <5,a5> <6,a6> <7,a7> <8,$> 
{2,9,10} {3,11,12} {4} {5,13,14} {6} {7} 
Figure 1. A parse tree with the sets of the anchored nodes 
5 
,41 
4 
8 
a7 I 
7 
a6 I 
6 
a5 a14 
13 14 
3 11 12 
2 9 10 1 
la l  a l \ [  
1 1 
Figure 2. 
Definition 2.7. If { s t ..... s.} is a set of nodes in the PGS, then 
their subtrees T(s a) ..... T(~p) are said to be adjacent when 
rcl(si) = lcl(si.~)-1 or, alternatively, al(si) = lcl(sm), for i = 
1 .... ,p-1. 
During a parsing process agreat effort is made in finding a 
set of adjacent subtrees that match a fight-hand side of a 
reduction rule. Let (A~z~ z 2 z 3) be a reduction rule, then the 
parser should start a match process to find all possible sets 
of adjacent subtrees such that heir categories match za z 2 z 3. 
The parser scans the input string left-to-right, so reductions 
grow on the left of the scanner pointer, and for the efficiency 
300 
Adjacency Tree 
terminal node there is the corresponding list of the anchored 
nodes. A useful structure that can be derived from these sets 
is an adjacency tree, recursively defined as follows: 
Definition 2.8. If (Nr,NwT) is a PGS for an input sentence 
s, and Isl = n, then the adjacency tree for the PGS is so built: 
- n+1 is the root of the adjacency tree; 
- for every k~Nr-{0,1}uN ., the sons ofk are the nodes in 
an(Icl(k)) unless an(Icl(k))= {0}. 
Figure 2 shows the adjacency tree obtained from the partial 
parse tree in Figure 1. Any passage from a node k to one of 
its sons h in the adjacency tree represents a passage from a 
3 11 12 2 9 10 
t alt 
2 9 10 1 1 1 
I "1 
1 1 
subtree T(k) to one of its adjacent subtrees T(h) in the PGS. 
Moreover, during a match process this means that a 
constituent of the right-hand side has been consumed, and 
matching the first symbol that-match process is f'mished. 
The adjacency lace also provides further useful information 
for optimizing the search during a match. For every node k, 
if we consider the longest path from k to a leaf, its length is 
an upper bound for the length of the right hand side still to 
consume, and since the sons ofk are the nodes in an(lcl(k)), 
the longest path is always given by the sequence of the 
terminal nodes from the node 1 to the node lcl(k)- 1. Thus its 
length is just lcl(k)-l. 
Property 2.1. If (Nr,Ns,T) is a PGS, (A~zl. . .z v) is a 
reduction rule whose right-hand side has to be matched, and 
T(k)~ T such that cat(k) = z ,  then: 
a. the string z t ... zp is matc'hable iffp < lcl(k); 
b. for i = p ..... 1, zt is partially matchable to a node 
Definition 2.10. If (Nr,Ns,T) is a PGS, an adjacency 
digraph can be represented asfollows: 
a. for any ke N r, k has outgoing arcs directed to the nodes in 
an(k); 
b. for any k? N N, k has one outgoing arc directed to lcl(k). 
In the classic literature the lists of the anchored nodes are 
called adjacency lists, and are used for representing graphs 
(Aho et at., 1974). A graph G=(V,E) can be usually 
represented byIVI adjacency lists. In our representation we
can obtain an optimization representing an adjacency 
digraph by n adjacency lists, if n is the length of the sentence, 
and by INsl simple pointers for accessing the adjacency lists 
from the non-terminal nodes, with respect to n+lNsl 
adjacency lists for a full representation f an adjacency 
digraph composed of arcs as in Det'mition 2.10.a. 
Figure 3 shows how a new non-terminal node is connected 
in an adjacency digraph, and Figure 4 shows the adjacency 
k \[ lcl(k) ~I- - - - k access from k to lcl(k) 
,ql-lcl(k-1) ~ lcl(k),~- ... al(k) =rc l (k)+l .~-  I r ~ k T(k) is adjacent to T(r) 
Figure 3. Adding a non-terminal node k to an adjacency digraph 
04$_ 1 ~4r  "a~ " " " ;~  " "~ _ ~ a4 5.4t.....~..__~ ~7.~.~__8  
".. , / , " . .  j ' - - .  
-Id" '~ l l P "  " " '14~ '13 r
Figure 4. Adjacency Digraph 
he NNuN riff cat(h) = z i and i < Icl(h). 
Property 2. I. along with the adjacency relation provides a 
method for an efficient navigation within the PGS among 
the subtrees. This navigation isperformed by the matcher in 
the PGS as visiting the adjacency tree in a pre-order fashion. 
It is easy to see that a pre-order visit of the adjacency tree 
scans all possible sequences ofthe adjacent subtrees in the 
PGS, but Property 2.1 provides a shortcut for avoiding 
useless passages when matchable conditions do not hold. 
When a match ends the matcher returns one or more sets of 
nodes atisfying the following conditions: 
Definition 2.9. A set RSet = {n I.... ,np} is a match for a string 
zl...zpiff cat(nl) ffi z i, for i = 1,...,p, and T(nl) is adjacent to 
T(ni, l), for i = 1 .... ,p-1. The set RSet is called a reduction 
set. 
The adjacency tree shows the hypothetical search space for 
searching the reduction sets in a PGS, thus it is not a 
representation f what memory is actually required to store 
the useful data for such a search. A more suitable 
representation is an adjacency directed graph defined by 
means of the lists of the anchored nodes in the terminal 
nodes, and by the pointers to the left comer leaf in the non- 
terminal nodes. 
301 
digraph for the parse tree of Figure 1. 
3. PROCESS GRAMMAR 
The Process Grammar isan extension of the Augmented 
Context-Free Grammar such as APSG, oriented to bottom- 
up parsing. Some relevant features make a Process 
Grammar quite different from classical APSG. 
1. The parser is a PG processor that ries to apply the rules 
in a bottom-up fashion. It does not have any knowledge 
about he running rammar but for the necessary structures 
to access its rules. Furthermore, it sees only its internal state, 
the Parse Graph Structure, and works with a non- 
deterministic strategy. 
2. The rules are conceived as processes that the PG 
processor schedules somehow. Any rule defines areduction 
rule that does not represent a rewriting rule, but rather a 
statement for search and construction of new nodes in a 
bottom-up way within the Parse Graph Structure. 
3. The rules are augmented with some sequences of 
operations to be performed as in the classical APSG. In 
general, augmentations such as tests and actions concern 
manipulation flinguistic data at syntactic and/or semantic 
level. In this paper we are not concerned with this aspect (an 
informal description about his is in Marino (1989)), rather 
we examine some aspects concerning parsing strategies by 
means of the augmentations. 
In a Process Grammar the rules can have knowledge of 
the existence of other ules and the purpose for which they 
are defined. They can call some functions that act as filters 
on the control structures of the parser for the scheduling of 
the processes, thus altering the state of the processor and 
forcing alternative applications. This means that any rule 
has the power of changing the state of the processor 
requiring different scheduling, and the processor is a blind 
operator that works following a loose strategy such as the 
non-deterministic one, whereas the grammar can drive the 
processor altering its state. In such a way the lack of 
determinism of the processor can be put in the Process 
Grammar, implementing parsing strategies which are 
transparent tothe processor. 
Definition 3.1. A Process Grammar PG is a 6-tuple 
(VT,Vs,S,R,Vs,F) where: 
. V r is the set of terminal symbols; 
- V N is the set of non-terminal symbols; 
- S? V N is the Root Symbol of PG; 
- R = {r 1 .... ,rt} is the set of the rules. Any rule r i in R is of 
the form r i = <red(ri),st(ri),t(ri),a(Q>, where red(ri) is a 
reduction rule (A~---a), A~ Vr~, ct~ (VruVN)+; st(r) is the 
state of the rule that can be active or inactive; t(Q and a(Q 
are the tests and the actions, respectively; 
- V s is a set of special symbols that can occur in a reduction 
rule and have a special meaning. A special symbol is e a, a 
null category that can occur only in the left-hand side of a 
reduction rule. Therefore, a reduction rule can also have the 
form (e~?---a), and in the following we refer to it as e- 
reduction; 
- F = {fl ..... f \ ]  is a set of functions the rules can call within 
their augmentations. 
Such a definition extends classical APSG in some specific 
ways: first, a Process Grammar is suited for bottom-up 
parsing; second, rules have a state concerning the 
applicability of a rule at a certain time; third, we extend the 
CF structure of the reduction rule allowing null left-hand 
sides by means of e-reductions; fourth, the set F is the 
strategic side that should provide the necessary functions to 
perform operations on the processor structures. As a matter 
of fact, the set F can be further structured giving the PG a 
wider complexity and power. In this paper we cannot reat 
a formal extended definition for F due to space restrictions, 
but a brief outline can be given. The set F can be defined as 
F=Fr~uFt,. In F~ are all those functions devoted to 
operations on the processor structures (Kernel Functions), 
and, in the case of a feature-based system, in Ft, are all the 
functions devoted to the management of feature structures 
(Marino, 1989). In what follows we are also concerned with 
the combined use of e-reductions and the function RA, 
standing for Rule Activation, devoted to the immediate 
scheduling of a rule. RAe Fx~ ' and a call to it means that the 
302 
specified role must be applied, involving the scheduling 
process we describe in Section 4. Before we introduce the 
PG processor we must give a useful definition: 
Definition 32. Let reR be a rule with t(r)=\[f,1;...;f.~\], 
a(r)=\[f l ; . . . ; f  \] be sequences of operations in its 
augmentations, f,~..... f~,ft ..... feF .  Let {n 1 ..... rip) be a 
reduction set for red(r) = (A~z  r..zv), and he Nr~ be the new 
node for A such that T(h) is the new subtree created in the 
PGS, then we define the Process Environment for t(r) and 
a(r), denoted briefly by ProcEnv(r), as: 
ProcEnv(r) = {h,n 1 .... ,n.} 
If red(r) is an e-reduction then ProcEnv(r) = {nl .... ,np}. 
This definition states the operative range for the 
augmentations of any rule is limited to the nodes involved 
by the match of the reduction rule. 
4. PG PROCESSOR 
Process Scheduler. The process scheduler makes 
possible the scheduling of the proper ules to run whenever 
a terminal node is consumed in input or a new non-terminal 
node is added to the PGS by a process. By proper ules we 
mean all the rules satisfying Property 2.1.a. with respect to 
the node being scanned or built. These rules are given by the 
sets def'med in the following definition: 
Definia'on 4.1. Vce VsuV r such that 3 r~ R where red(r) = 
(Ac---ac), AeVNu{e~}, being c the right comer of the 
reduction rule, and lacl _< L, being L the size of the longest 
right-hand side having c as the right comer, the sets P(c,i), 
P,(c,i) for i = 1 ..... L, can be built as follows: 
P(c,i) = {re R I red(r)=(At---cxc), 1 < Itxcl _< i, st(r)=aclive} 
Pe(c,i)= {re R I red(r)=(eac---ac ), 1< lacl < i, st(r)=active} 
Whenever a node he NruNr~ has been scanned or built and 
k=lcl(h), then the process cheduler has to schedule the rules 
in P(cat(h),k)uP,,(cat(h),k). In the following this union is 
also denoted by Yl(cat0a),k). Such a rule scheduling allows 
an efficient realization of the immediate constituent 
analysis approach within a bottom-up arser by means of a 
partitioning of the roles in a Process Grammar. 
The process cheduler sets up aprocess descriptor for each 
rule in l-l(cat0a),k) where the necessary data for applying a
process in the proper environment are supplied. In a Process 
Grammar we can have three main kinds of rules: rules that 
are activated by others by means of the function RA; e- 
reduction roles; and standard rules that do not fall in the 
previous cases. This categorization implies that processes 
have assigned a priority depending on their kind. Thus 
activated rules have the highest priority, e-reduction rules 
have an intermediate priority and standard rules the lowest 
priority. Rules become scheduled processes whenever a
process descriptor for them is created and inserted in a 
priority queue by the process cheduler. The priority queue 
is divided into three stacks, one for each kind of rule, and 
they form one of the structures of the processor state. 
Definition 4.2. A process descriptor is a triple PD=\[r,h,C\] 
where: m R is the rule involved; he NruNsu  {NIL} is either 
the right corner node from which the marcher starts or NIL; 
C is a set of adjacent nodes or the empty set. A process 
descriptor of the form \[r,NiL,\[nl .... ,nc\] is built for an 
activated rule r and pushed in the stack s r A process 
descriptor of the form \[r,h, \[ } \] is built for all the other ules 
and is pushed either in the stack s 2 if r is an e-reduction rule 
or in the stack s 3 if a standard rule. Process descriptors of 
these latter forms are handled by the process cheduler, 
whereas process descriptors for activated rules are only 
created and queued by the function RA. 
State of Computation. The PG processor operates by 
means of an operation Op on some internal structures that 
define the processor state ProcState, and on the parsing 
structures accessible by the process environment ProcEnv. 
The whole state of computation is therefore given by: 
\[Op,ProcState,ProcEnv\] = \[Op,pt,\[s~,svs3\],PD,pn,RSet\] 
where pt? N r is the input pointer to the last terminal node 
scanned; pn~ N~ is the pointer to the last non-terminal node 
added to the PGS. For a sentence s=a r. .a.  the computation 
starts from the initial state \[begin,0,\[NIL,NIL,NIL\], 
NIL,n+I,{}\], and terminates when the state becomes 
\[end,n,\[NIL,NIL,NIL\],NIL,pn,\[ }\]. The aim of this section 
is not to give a complete description of the processor cycle 
in a parsing process, but an analysis of the activation 
mechanism of the processes by means of two main cases of 
rule scheduling and processing. 
Scheduling and Processing of Standard Rules. 
Whenever the state of computation becomes as \[scan, pt, 
\[NIL,NILMIL\]MIL,pn,{ }\] the processor scans the next 
terminal node, performing the following operations: 
sc an: scl if pt = n then Op <--- end 
sc2 else pt*-- pt + 1; 
sc3 schedule 0"I(cat(pt),lcl(pt))); 
sc4 Op <--- activate. 
Step sc4 allows the processor to enter in the state where it 
determines the first non-empty higher priority stack where 
the process descriptor for the next process to be activated 
must be popped off. Let suppose that cat(pt)=zp, and 
l'I(z,lcl(p0)={r } where r is a standard rule such that 
red(~)=(A<--zr..z ~. At this point the state is \[activate, 
pt,\[NILMIL,\[r,pt, \[ } \]\] MIL,pn,\[ } \] and the processor has to 
try reduction for the process in the stack s v thus 
Op<--reduce performing the following statements: 
reduce: rl PD<---pop (%); 
\[reduce,pt,\[NIL,NIL,NIL\],\[r,pt, { } \],pn,{ }\] 
r2 C0--match (red(r), pt); 
C = {nl,...,n vpt} 
r3 PD<-Lir, pt, C\]; 
303 
\[reduce,pt,\[Nfl.,MiL,NIL\],\[r,pt,C\],pn,{ }\] 
r4 V rset~ C: 
r5 RSet ~rset; 
\[reduce,pt,\[NiL,NILMIL\],\[r,pt, { } \],pn~RSet\] 
r6 if t(r) then pn<--pn + 1; 
r7 add subtree(pn ,red (r) ,R S e0; 
r8 a(r); 
r9 schedule (H(cat(pn),lcl(pn)); 
\[reduce,pt,\[NIL,sv%\],\[r,pt, { } \], n,RSet\] 
rl00p<--activate. 
Step r9, where the process scheduler produces process 
descriptors for all the rules in H(AJcl(pn)), implies 
immediate analysis of the new constituent added to the PGS. 
Scheduling and Processing of Rules Activated by ~- 
Reduction Rules. Let consider the case when an ~- 
reduction rule r activates an inactive rule r' such that: 
red(r)f-(eat--zr..zp), a(r)=\[RA (r')\], red(r')=(A~zr..Zh), 
l~,_<.h<p, and st(r')=inactive. When the operation activate 
has checked that an g-reduction rule has to be activated then 
Olx--~-reduce, thus the state of computation becomes: 
\[e.reduce,pt,\[NIL,\[r,m,{}\],NIL\],NIL,pn,{}\], a d the
following statements are performed: 
e-reduce: 0-I PD<---pop (sz); 
\[e-red uce,pt,\[NIL,NIL.NIL\] ,\[r,m, { } \] ,pn, { } \] 
~2 C<---match (red(r), m); 
C = (n I .... ,n I, m} 
0-3 f~b.\[r,m,C\]; 
\[e.red uce,pt,\[NIL,NIL.NIL\] ,\[r,m,C\] ,pn, { } \] 
0-4 V rsemC: 
0"5 RSet.--rset; 
\[e-red uce,pt. \[NIL,NIL,NIL\],\[r,m, { } \],pn,RSet\] 
0-6 if t(r) then a(r)=\[RA (r')\]; 
\[?.reduce,pt,\[\[r',NIL,{n k .... , h}\],NIL,NIL\], 
\[r,m,{}\],pn,RSet\] 
0"70lx--activate. 
In this case, unlike that which the process cheduler does, 
the function RA performs at step 0-6 the scheduling of a 
process descriptor in the stack s, where a subset of 
ProcEnv(r) is passed as the ProcEnv(r'). Therefore, when an 
e-reduction rule r activates another rule r' the step er2 does 
the work also for r', and RA just has to identify the ProcEnv 
of the activated rule inserting it in the process descriptor. 
Afterwards, the operation activate checks the highest 
priority stack s, is not empty, therefore it pops the process 
descriptor \[r',NIL,{ nk ..... n u} \] and OIx--h-reduce that skips 
the match process applying immediately the rule r': 
h-reduce: hrl RSet<--C; 
\[h-reduce,pt,\[NiL,NIL,NlL\],\[r',NIL,{ } \],pn,RSet\] 
hr2 through hr6 as r6 through rl0. 
From the above descriptions it turns out that the 
operation activate plays a central role for deciding what 
operation must run next depending on the state of the three 
stacks. The operation activate just has to check whether 
some process descriptor is in the first non-empty higher 
priority stack, and afterwards to set the proper operation. 
The following statements describe such a work and Figure 
5 depicts graphically the connections among the operations 
defined in this Section. 
activate: al if sI=NIL 
a2 then if %=NIL 
a3 then if s,=NIL 
a4 then Op ~ scan 
a5 else Op <-- reduce 
a6 else Op <--- c-reduce 
a7 else PD ~ pop (%); 
PD = \[r,NIL,C\] 
a8 Op <-- h-reduce. 
s l=s2=s3=NI~ 
Figure 5. Operations Transition Diagram 
5. EXAMPLE 
It is well known that bottom-up parsers have problems 
in managing rules with common right-hand sides like X ---> 
ABCD, X ---> BCD, X ---> CD, X ---> D, since some or all of 
these rules can be fired and build unwanted nodes. A strategy 
called top-down filtering in order to circumvent such a 
problem has been stated, and it is adopted within bottom-up 
parsers (Kay, 1982; Pratt, 1975; Slocum, 1981; Wir6n, 
1987) where it simulates a top-down parser together with the 
bottom-up parser. The PG Processor must face this problem 
as well, and the example we give is a Process Grammar 
subset of rules that tries to resolve it. The kind of solution 
proposed can be put in the family of top-down filters as well, 
taking advantage firstly of using e-reduction rules. 
Unfortunately, the means described so far are still 
insufficient o solve our problem, thus the following 
definitions introduce some functions that extend the Process 
Grammar and the control over the PGS and the PG 
Processor. 
Definition 5.1. Let r be a rule of R with red(r)=(~--z v..z), 
and RSet={n,...np} bea reduction set for red(r). Taken two 
nodes %,nje RSet where n,e N N such that we have cat(n)--z,, 
cat(nj)=zj, and T(n~), T(n) are adjacent, i.e., either j=i+ 1 or 
304 
j=i- 1, then the function Add_Son_Rel of Fx= when called in 
a(r) as Add_Son_Rel (zi~z) has the effect of creating anew 
parent-son relation between %, the parent, and n, the son, 
altering the sets sons(n), and either 1cI(%) or rcl(n) as 
follows: 
a) sons(n) ~- sons(n) u {nj} 
b) lcl(n) ~ lcl(nj) ifj=i-1 
c) rcl(n) 6-- rcl(n) ifj=i+l 
Such a function has the power of making an alteration i the 
structure of a subtree in the PGS extending its coverage to 
one of its adjacent subtrees. 
Definition 5.2. The function RE of Fr~, standing for Rule 
Enable, when called in the augmentations of some rule r as 
RE (r'), where r, r' are in R, sets the state of r' as active, 
masking the original state set in the definition of r'. 
Without entering into greater detail, the function RE can 
have the side effect of scheduling the just enabled rule r' 
whenever the call to RE follows the call Add Son Rel 
(X,Y) for some category Xe V,,,Ye V,wVr, and the right 
corner of red(r') is X. 
Definition 5.3. The function RD of Fx, , standing for Rule 
Disable, when called in the augmentations of ome rule r as 
RD (r'), where r, r' are in R, sets the state ofr '  as inactive, 
masking the original state set in the definition of r'. 
We axe now ready to put the problem as follows: given, 
for instance, the following set P1 of productions: 
PI = {X --> ABCD, X ---> BCD, X --> CD, X ---> D} 
we want o define aset of PG rules having the same coverage 
of the productions in PI with the feature of building in any 
case just one node X in the PGS. 
Such a set of rules is shown in Figure 6 and its aim is to create 
links among the node X and the other constituents just when 
the case occurs and is detected. All the possible cases are 
depicted in Figure 7 in chronological order of building. 
The only active rule is r0 that is fired whenever a D is inserted 
in the PGS, thus a new node X is created by r0 (case (a)). 
Since the next possible case is to have a node C adjacent to 
the node X, the only action of r0 enables the rule rl whose 
work is to find such an adjacency inthe PGS by means of the 
e-reduction rule red(rl)=(e,~ C X'). If such a C exists rl is 
scheduled and applied, thus the actions of rl create a new 
link between X and C (case Co)), and the rule r2 is enabled in 
preparation of the third possible case where a node B is 
adjacent to the node X. The actions of rl disable rl itself 
before ending their work. Because of the side effect of RE 
cited above the rule r2 is always cheduled, and whenever a 
node B exists then it is applied. At this point it is clear how 
the mechanism works and cases (c) and (d) are handled in the 
same way by the rules r2 and r3, respectively. 
As the example.shows, henever the rules rl ?2?3 are 
scheduled their task is realized in two phases. The first phase 
is the match process of the e-reduction rules. At this stage it 
is like when a top-down parser searches lower-level 
constituents for expanding the higher level constituent. If 
this search succeeds the second phase is when the 
red(r0) = (X ~-- D) 
st(r0) = active 
a(r0) = iRE (rl)\] 
red(rl) = (el<--- C X) 
st(rl) = inactive 
a(rl) = \[Add Son_Rel (X,C); RE (r2); RD (rl)\] 
red(a) = B XO 
st(r2) = inactive 
a(r2) = \[Add_Son Rel (X,B); RE (r3); RD (r2)\] 
red(r3) -- (el ?--- A X) 
st(r3) = inactive 
a(r3) = \[Add Son_Rel (X,A); RD (r3)\] 
Figure 6. The Process Grammar of the example 
X 
D 
A 
(a) 
X 
rl/'N  
C D 
AA 
Co) 
X 
B C D 
/x/ A 
(c) 
X 
A B C D 
AAAA 
(d) 
Figure 7. All the possible cases of the example 
appropriate links are created by means of the actions, and the 
advantage of this solution is that the search process 
terminates in a natural way without searching and proposing 
useless relations between constituents. 
We terminate this Section pointing out that this same 
approach can be used in the dual case of this example, with 
a set P2 of productions like: 
P2= {X --~ A, X ---> AB, X ---> ABC, X ---> ABCD} 
The exercise of finding a corresponding setofPG rules is left 
to the reader. 
6. RELATED WORKS 
Some comparisons can be made with related works on 
three main levels: the data structure PGS; the Process 
Grammar; the PG Processor. 
ThePGS can be compared with the chart (Kaplan, 1973; 
Kay, 1982). The PGS embodies much of the information the 
chart has. As a matter of fact, our PGS can be seen as a 
denotational variant of the chart, and it is managed in a 
different way by the PG Processor since in the PGS we 
mainly use classical relations between the nodes of the 
parse-trees: the dominance r lation between a parent and a 
son node, encoded in the non-terminal nodes; the left- 
adjacency relation between subtrees, encoded in the 
terminal nodes. Note that if we add the fight-adjacency 
relation to the PGS we obtain astructure fully comparable to
the chart. 
The Process Grammar can embody many kinds of 
information. Its structure comes from the general structure 
stated for the APSG, being very close to the ATN Grammars 
structure. On the other hand, our approach proposes that 
grammar rules contain directives relative to the control of 
the parsing process. This is a feature not in line with the 
current rend of keeping separate control and linguistic 
restrictions expressed in a declarative way, and it can be 
305 
found in parsing systems making use of grammars based on 
situation-action rules 0Vinograd, 1983); furthermore, our 
way of managing rammar rules, i.e., operations on the 
states, activation and scheduling mechanisms, is very 
similar to that realized in Marcus (1980). 
7. D ISCUSSION AND CONCLUSIONS 
The PG Processor is bottom-up based, and it has to try 
to take advantage from all the available sources of 
information which are just the input sentence and the 
grammar structure. A slrong improvement in the parsing 
process is determined by how the rules of a Process 
Grammar are organized. Take, for instance, a grammar 
where the only active rules are e-reduction rules. Within the 
activation model they merely have to activate inactive rules 
to be needed next, after having determined a proper context 
for them. This can be extended to chains of activations at 
different levels of context in a sentence, thus limiting both 
calls to the matcher and nodes proliferation i the PGS. This 
case can be represented writing (ea~offl3) ~ (A~--T), 
reading itas if the e-reduction in the lhs applies then activate 
the rule with the reduction in the rhs, thus realizing a 
mechanism that works as a context-sensitive reduction of the 
form (otA\[3~---?c#), easily extendable also to the general case 
=, 
This is not the only reason for the presence of the e-reduction 
rules in the Process Grammar. Italso becomes apparent from 
the example that the e-reduction rules are a powerful tool 
that, extending the context-freeness of the reduction rules, 
allow the realization of a wide alternative of techniques, 
especially when its use is combined together with Kernel 
Functions uch as RA getting a powerful mean for the 
control of the parsing process. From that, a parser driven by 
the input - for the main scheduling - and both by the PGS and 
the rules - for more complex phenomena - can be a valid 
framework for solving, as much as possible, classical 
problems of efficiency such as minimal activation of rules, 
and minimal node generation. Our description is 
implementation-independent, it is responsive to 
improvements and extensions, and a first advantage is that it 
can be a valid approach for realizing efficient 
implementations of the PG Processor. 
Extending the Process Grammar. In this paper we 
have described a Process Grammar where rules are 
augmented with simple tests and actions. An extension of 
this structure that we have not described here and that can 
offer further performance to the parsing process is if we 
introduce in the PG some recovery actions that are applied 
whenever the detection of one of the two possible cases of 
process failure happens in either the match process or the 
tests. Consider, for instance, the reduction rule. Its f'mal aim 
is to find a process environment for the rule when scheduled. 
This leads to say that whenever some failure conditions 
happen and a process environment cannot be provided, the 
recovery actions would have to manage just the control of 
what to do next to undertake some recovery task. It is easy 
to add such an extension to the PG, consequently modifying 
properly the reduction operations of the PG processor. 
Other extensions concern the set F~, by adding further 
control and process management functions. Functions uch 
as RE and RD can be defined for changing the state of the 
rules during a parsing process, thus a Process Grammar can 
be partitioned in clusters of rules that can be enabled or 
disabled under proper circumstances detected by qow- 
level'(e-reduction) rules. Finally, there can be also some 
cutting functions that stop local partial parses, or even halt 
the PG processor accepting or rejecting the input, e.g., when 
a fatal condition has been detected making the input 
unparsable, the PG processor might be halted, thus avoiding 
the complete parse of the sentence and even starting a 
recovery process. The reader can refer to Marino (1988) and 
Marino (1989) for an informal description regarding the 
implementation ofsuch extensions. 
Conclusions. We have presented a complete 
framework for efficient bottom-up arsing. Efficiency is 
gained by means of: a structured representation of the 
parsing structure, the Parse Graph Structure, that allows 
efficient matching of the reduction rules; the Process 
Grammar that extends APSG by means of the process-based 
conception of the grammar rules and by the presence of 
Kernel Functions; the PG Processor that implements a non- 
deterministic parser whose behaviour can be altered by the 
Process Grammar increasing the determinism ofthe whole 
system. The mechanism of rule activation that can be 
realized in a Process Grammar is context-sensitive-based, 
but this does not increase computational effort since 
processes involved in the activations receive their process 
environments - which are computed only once - from the 
activating rules. At present we cannot ell which degree of 
determinism can be got, but we infer that the partition of a 
Process Grammar in clusters of rules, and the driving role the 
e-reductions can have are two basic aspects whose 
importance should be highlighted in the future. 
ACKNOWLEDGMENTS 
The author is thankful to Giorgio Satta who made 
helpful comments and corrections on the preliminary draft 
of this paper. 
REFERENCES 
Aho, Alfred, V. and Ullman, Jeffrey, D. (1972). The 
Theory of Parsing, Translation, and Compiling. Volume 1: 
Parsing. Prentice Hall, Englewood Cliffs, NJ. 
Aho, Alfred, V., Hopcroft, John, E. and Ullman, Jeffrey, 
D. (1974). The Design and Analysis of Computer 
Algorithms. Addison-Wesley. 
Grishman, Ralph (1976). A Survey of Syntactic 
Analysis Procedures for Natural Language. American 
Journal of ComputationalLinguistics. Microfiche 47, pp. 2- 
96. 
Kaplan, Ronald, M. (1973). A General Syntactic 
Processor. In Randall Rustin, ed., Natural Language 
Processing, Algodthmics Press, New York, pp. 193-241. 
Kay, Martin (1982). Algorithm Schemata nd Data 
Structures in Syntactic Processing. In Barbara J. Grosz, 
Karen Sparck Jones and Bonnie Lynn Webber, eds., 
Readings in Natural Language Processing, Morgan 
Kaufmann, Los Altos, pp. 35-70. Also CSL-80-12, Xerox 
PARC, Palo Alto, California. 
Marcus, Mitchell, P. (1980). A Theory of Syntactic 
Recognition for NaturaI Language. MIT Press, Cambridge, 
MA. 
Marino, Massimo (1988). A Process-Activation Based 
Parsing Algorithm for the Development of Natural 
Language Grammars. Proceedings of 12th International 
Conference on Computational Linguistics. Budapest, 
Hungary, pp. 390-395. 
Marino, Massimo (1989). A Framework for the 
Development of Natural Language Grammars. Proceedings 
of lnternational Workshop on Parsing Technologies. CMU, 
Pittsburgh, PA, August 28-31 1989, pp. 350-360. 
Pratt, Vaughan, R. (1975). LINGOL - A Progress 
Report. Proceedings of4th IJCAI, Tbilisi, Georgia, USSR, 
pp. 422-428. 
Slocum, Johnathan (1981). A Practical Comparison of 
Parsing Strategies. Proceedings of 19th ACL, Stanford, 
California, pp. 1-6. 
Winograd, Terry (1983). Language as a Cognitive 
Process. Vol. 1: Syntax. Addison-Wesley, Reading, MA. 
Wirtn, Mats (1987). A Comparison of Rule-Invocation 
Strategies in Context-Free Chart Parsing. Proceedings of 
3rd Conference of the European Chapter of the ACL, 
Copenhagen, Denmark, pp. 226-233. 
306 
