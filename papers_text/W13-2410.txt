Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 63?68,
Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational Linguistics
Improving English-Russian sentence alignment through POS tagging and
Damerau-Levenshtein distance
Andrey Kutuzov
National Research University Higher School of Economics
Moscow, Russia
Myasnitskaya str. 20
akutuzov72@gmail.com
Abstract
The present paper introduces approach to
improve English-Russian sentence align-
ment, based on POS-tagging of automat-
ically aligned (by HunAlign) source and
target texts. The initial hypothesis is
tested on a corpus of bitexts. Sequences
of POS tags for each sentence (exactly,
nouns, adjectives, verbs and pronouns)
are processed as ?words? and Damerau-
Levenshtein distance between them is
computed. This distance is then normal-
ized by the length of the target sentence
and is used as a threshold between sup-
posedly mis-aligned and ?good? sentence
pairs. The experimental results show pre-
cision 0.81 and recall 0.8, which allows
the method to be used as additional data
source in parallel corpora alignment. At
the same time, this leaves space for further
improvement.
1 Introduction
Parallel multilingual corpora have long ago be-
come a valuable resource both for academic and
for industrial computational linguistics. They are
employed for solving problems of machine trans-
lation, for research in comparative language stud-
ies and many more.
One of difficult tasks in parallel multilingual
corpora building is alignment of its elements with
each other, that is establishing a set of links be-
tween words and phrases of source and target lan-
guage segments (Tiedemann, 2003). Alignment
can be done on the level of words, sentences,
paragraphs or whole documents in text collection.
Most widely used are word and sentence align-
ment, and the present paper deals with the latter
one.
Word alignment is an essential part of statisti-
cal machine translation workflow. However, usu-
ally it can only be done after sentence alignment
is already present. Accordingly, there have been
extensive research on the ways to improve it.
Basic algorithm of sentence alignment simply
links sentences from source and target text in or-
der of their appearance in the texts. E.g., sentence
number 1 in the source corresponds to sentence
number 1 in the target etc. But this scheme by de-
sign can?t handle one-to-many, many-to-one and
many-to-many links (a sentence translated by two
sentences, two sentences translated by one, etc)
and is sensitive to omissions in source or trans-
lated text.
Mainstream ways of coping with these prob-
lems and increasing alignment quality include
considering sentence length (Gale and Church,
1991) and using bilingual dictionaries (Och and
Ney, 2000) or cognates (Simard et al, 1992) to
estimate the possibility of sentences being linked.
Kedrova and Potemkin (2008) showed that these
ways provide generally good results for Russian
as well.
But often this is not enough. Sentence length
can vary in translation, especially when translation
language is typologically different from the source
one. As for bilingual dictionaries, it is sometimes
problematic to gather and compile a useful set of
them.
Thus, various additional methods were pro-
posed, among them using part-of speech data from
both source and target texts. It is rather com-
monplace in word alignment (Tiedemann, 2003;
Toutanova et al, 2002). Using part-of speech tag-
ging to improve sentence alignment for Chinese-
English parallel corpus is presented in (Chen and
Chen, 1994). In the current paper we propose to
use similar approach in aligning English-Russian
translations.
63
2 Setting up the Experiment
We test the part-of-speech based approach to im-
prove quality of sentence alignment in our par-
allel corpus of learner translations available at
http://rus-ltc.org. Only English to Russian trans-
lations were selected, as of now. The workflow
was as follows.
All source and target texts were automat-
ically aligned with the help of HunAlign
software (Varga et al, 2005) together with
its wrapper LF Aligner by Andra?s Farkas
(http://sourceforge.net/projects/aligner). The
choice of aligner was based on high estimation
by researchers (cf. (Kaalep and Veskis, 2007))
and its open-source code. Sentence splitting was
done with a tool from Europarl v3 Preprocessing
Tools (http://www.statmt.org/europarl) written by
Philipp Koehn and Josh Schroeder. Proper lists of
non-breaking prefixes were used for both Russian
and English.
HunAlign uses both bilingual dictionaries and
Gale-Church sentence-length information. Its re-
sults are quite good, considering the noisiness of
our material. However, about 30 percent of sen-
tences are still mis-aligned. The reasons behind
this are different, but mostly it is sentence split-
ter errors (notwithstanding its preparation for Rus-
sian), omissions or number of sentences changing
during translation. Here is a typical example:
?And these two fuels are superior to ethanol,
Liao says, because they have a higher energy den-
sity, do not attract water, and are noncorrosive?.
||| ???? ??? ???? ??????? ???? ???????????
?????? ?? ????? ?????????.?
0 ||| ??? ?????? ???, ??? ????????
????? ??????? ?????????????? ??????????,
?? ???????? ????, ? ?????? ?????????????.?
The translator transformed one English sen-
tence into two Russian sentences. Consequently,
aligner linked the first Russian sentence to the
source one, and the second sentence is left with-
out its source counterpart (null link). It should be
said that in many cases HunAlign manages to cope
with such problems, but not always, as we can see
in the example above.
The cases of mis-alignment must be human cor-
rected, which is very time-expensive, especially
because there is no way to automatically assess the
quality of alignment. HunAlign?s internal measure
of quality is often not very helpful. For exam-
ple, for the first row of the table above it assigned
rather high quality mark of 0.551299. Trying to
predict alignment correctness with the help of Hun
quality mark only for the whole our data set re-
sulted in precision 0.727 and recall 0.548, which
is much lower than our results presented below.
We hypothesize that source and target sentence
should in most cases correspond in the number and
order of content parts of speech (POS). This data
can be used to trace mis-aligned sentences and per-
haps to find correct equivalents for them. In order
to test this hypothesis, our source and target texts
were POS-tagged using Freeling 3.0 suite of lan-
guage analyzers (Padro? and Stanilovsky, 2012).
Freeling gives comparatively good results in En-
glish and Russian POS-tagging, using Markov tri-
gram scheme trained on large disambiguated cor-
pus.
Freeling tag set for English follows that of
Penn TreeBank, while Russian tag set, ac-
cording to Freeling manual, corresponds to
EAGLES recommendations for morphosyn-
tactic annotation of corpora described on
http://www.ilc.cnr.it/EAGLES96/home.html
(Monachini and Calzolari, 1996). It is not trivial
to project one scheme onto another completely,
except for the main content words ? nouns, verbs
and adjectives. Moreover, these three parts of
speech are the ones used in the paper by Chen
and Chen (1994), mentioned above. So, the
decision was made to take into consideration only
the aforementioned lexical classes, with optional
inclusion of pronouns (in real translations they
often replace nouns and vice versa).
Thus, each sentence was assigned a ?POS wa-
termark?, indicating number and order of content
words in it. Cf. the following sentence:
?Imagine three happy people each win $1 mil-
lion in the lottery.?
and its ?POS watermark?:
VANVNN,
where N is noun, A is adjective and V is verb.
Here is the same analysis for its Russian trans-
lation counterpart:
??????????? ???? ???? ?????????? ?????,
??????? ???????? ? ??????? ?? ????????
????????.?
Corresponding ?POS watermark?:
VPANVNNN,
where N is noun, V is verb, A is adjective and P
is pronoun.
Nouns and verbs are marked identically in Penn
64
and EAGLES schemes. Adjectives in Penn are
marked as JJ, so this mark was corrected to A,
which is also the mark for adjectives in EAGLES.
We considered to be ?pronouns? (P) those words
which are marked as ?E? in EAGLES and ?PRP?
in Penn.
Thus, each content word is represented as
one letter strictly corresponding to one lexi-
cal class. Therefore our ?POS watermark?
can be thought of as a kind of ?word?. The
difference between these ?words? is computed
using Damerau-Levenshtein distance (Damerau,
1964). Basically, it is the number of cor-
rections, deletions, additions and transpositions
needed to transform one character sequence
into another. We employ Python implementa-
tion of this algorithm by Michael Homer (pub-
lished at http://mwh.geek.nz/2009/04/26/python-
damerau-levenshtein-distance).
According to it, the distance between POS wa-
termarks of two sentence above is 2. It means we
need only two operations ? adding one pronoun
and one noun ? to get target POS structure from
source POS structure. At the same time, the dis-
tance between VPVNANNNNNNNNNNVN and
NVNNANANANN is as high as 10, which means
that POS structures of these sentences are quite
different. Indeed, the sentences which generated
these structures are obviously mis-aligned:
?If a solar panel ran its extra energy into a
vat of these bacteria, which could use the en-
ergy to create biofuel, then the biofuel effectively
becomes a way to store solar energy that oth-
erwise would have gone to waste.? ||| ???????
??? ???????????? ??????? ??????, ???
?????????.?
One can suppose that there is correlation
between Damerau-Levenshtein distance and the
quality of alignment: the more is the distance the
more is the possibility that the alignment of these
two sentences has failed in one or the other way.
In the following chapter we present the results of
the preliminary experiment on our parallel texts.
3 The Results
We performed testing of the hypothesis over 170
aligned English-Russian bi-texts containing 3263
sentence pairs. As of genres of original texts, they
included essays, advertisements and informational
passages from mass media. The dataset was hand-
annotated and mis-aligned sentence pairs marked
(663 pairs, 20 percent of total dataset).
Damerau-Levenshtein distances for all sen-
tences were computed and we tried to find opti-
mal distance threshold to cut ?bad? sentence pairs
from ?good? ones.
For this we used Weka software (Hall et
al., 2009) and its Threshold Selector ? a meta-
classifier that selects a mid-point threshold on the
probability output by a classifier (logistic regres-
sion in our case). Optimization was performed
for ?bad? class, and we used both precision and
F-measure for determining the threshold, with dif-
ferent results presented below. The results were
evaluated with 3-fold cross-validation over the en-
tire dataset.
Initially, on the threshold 7 we achieved pre-
cision 0.78, recall 0.77 and F-measure 0.775 for
the whole classifier. F-measure for detecting only
mis-aligned sentences was as low as 0.464.
In order to increase the quality of detection we
tried to change the settings: first, to change the
number of ?features?, i.e., parts of speech consid-
ered. ?Minimalist? approach with only nouns and
adjectives lowered F-measure to 0.742. However,
considering nouns, adjectives and verbs without
pronouns seemed more promising: using the same
distance threshold 7 we got precision 0.787 and
recall 0.78 with F-measure 0.783. F-measure for
detecting mis-aligned sentences also got slightly
higher, up to 0.479. So, general estimate is even
higher than when using pronouns.
Moving further in an effort to improve the al-
gorithm, we found that Damerau-Levenshtein dis-
tance shows some kind of dis-balance when com-
paring short and long ?words?. Short ?words? re-
ceive low distance estimates simply because the
number of characters is small and it?s ?easier? to
transform one into another, even if the ?words? are
rather different. At the same time, long ?words?
tend to receive higher distance estimates because
of higher probability of some variance in them,
even if the ?words? represent legitimate sentence
pairs. Cf. the following pairs:
? distance between PVPVAA and ANAN is es-
timated as 5,
? distance between NNNNVAANNVVN-
NVNNNVV and NNNNVANANPAN-
NANVN is estimated as 7.
Meanwhile, the first sentence pair is in fact mis-
aligned, and the second one is quite legitimate. It
65
is obvious that ?word? length influences results
of distance estimation and it should be somehow
compensated.
Thus, the penalty was assigned to all distances,
depending on the length of original sentences.
Then this ?normalized? distance was used as a
threshold. We tried employing the length of the
source sentence, of target sentence and the aver-
age of both. The length of the target (translated)
sentence gave the best results.
So, the equation is as follows:
DLnorm = DL(sP,tP )LEN(tP ) ,
where DLnorm is ?normalized? distance, DL
is original Damerau-Levenshtein distance, sP is
?POS watermark? for source sentence, tP is ?POS
watermark? for target sentence and LEN is length
in characters.
With nouns, verbs, adjectives and pronouns this
normalization gives considerably better results:
Precision 0.813
Recall 0.802
F-Measure 0.807
After removing pronouns from consideration,
at the optimal threshold of 0.21236, recall gets
slightly higher:
Precision 0.813
Recall 0.803
F-Measure 0.808
Even ?minimalist? nouns-and-adjectives ap-
proach improves after normalization:
Precision: 0.792
Recall: 0.798
F-Measure: 0.795
Overall results are presented in the Table 1.
Methods without target length penalty provide
considerably lower overall performance, thus,
methods with the penalty should be used.
Depending on particular aim, one can vary the
threshold used in classification. In most cases,
mis-aligned pairs are of more interest than ?good
pairs?. If one?s aim is to improve precision of ?bad
pairs? detection, the threshold of 0.8768 will give
0.851 precision for this, at the expense of recall
as low as 0.1. If one wants more balanced out-
put, the already mentioned threshold of 0.21236
is optimal, providing mis-aligned pairs detection
precision of 0.513 and recall of 0.584.
Figure 1 presents distribution of ?good? and
?bad? pairs in our data set in relation to Damerau-
Levenshtein distance (X axis). Correctly aligned
pairs are colored gray and incorrectly aligned ones
Method Precision Recall F-
Measure
Nouns, adjec-
tives, verbs
and pronouns
without length
penalty.
0.78 0.77 0.775
Nouns, adjec-
tives and verbs
without length
penalty.
0.787 0.78 0.783
Nouns and
adjectives
without length
penalty.
0.764 0.728 0.742
Nouns and
adjectives with
target length
penalty.
0.792 0.798 0.795
Nouns, adjec-
tives, verbs and
pronouns with
target length
penalty.
0.813 0.802 0.807
Nouns, ad-
jectives and
verbs with
target length
penalty.
0.813 0.803 0.808
Table 1. Overall performance of pairs classifier
depending on the method.
black. Correlation between alignment correctness
and Levenshtein value can be clearly seen. At the
same time, internal HunAlign quality measure (Y
axis) does not show any stable influence on align-
ment correctness, as we already mentioned above.
4 Discussion and Further Research
The experimental results presented above show
that number and order of POS in source and tar-
get sentences in English-Russian translations are
similar to the degree that makes possible to use
this similarity in order to check alignment cor-
rectness. The method of calculating Damerau-
Levenshtein distance between POS ?watermarks?
of source and target sentences can be applied for
detecting mis-aligned sentence pairs as an addi-
tional factor, influencing the decision to mark the
pair as ?bad?.
66
Figure 1. Levenshtein distance (X axis) and alignment correctness (color)
However, some pairs show anomalies in this
aspect. For example, the pair below is charac-
terized by normalized POS Damerau-Levenshtein
distance of enormous 2.6, however, human asses-
sor marked it as ?good?:
?An opinion poll released by the independent
Levada research group found that only 6 per cent
of Russians polled sympathised with the women
and 51 per cent felt either indifference, irritation
or hostility.? ||| ?? ??? 51 ??????? ??????????
?????????? ? ??? ?????????? ? ????
????????????.?
Translator omitted some information she con-
sidered irrelevant, but the pair itself is aligned cor-
rectly.
On the other hand, cf. two consecutive pairs
below:
?The British Museum? The Louvre?? |||
??????????? ???????
?The Metropolitan?? ||| ????????
Normalized distance for the first pair is 0.3333,
and this correctly classifies it as ?bad?. The sec-
ond target sentence must have belonged to the first
pair and the second pair is obviously bad, but its
distance equals to zero (because both part contain
exactly one noun), so it will be incorrectly classi-
fied as ?good? with any threshold.
Such cases are not detected with the method de-
scribed in this paper.
Our plans include enriching this method with
heuristic rules covering typical translational trans-
formations for particular language pair. For exam-
ple, English construction ?verb + pronominal di-
rect object? is regularly translated to ?pronominal
direct object + verb? in Russian:
?She loves him? ||| ???? ??? ??????.
Also we plan to move from passively marking
mis-aligned pairs and leaving the actual correc-
tion to human to actively searching for possible
equivalent candidates among other sentence pairs,
especially among those with null links. The diffi-
cult part here is designing the method to deal with
?partially correct? alignment, for example, like in
the pair below:
?The magic number that defines this ?com-
fortable standard? varies across individuals and
countries, but in the United States, it seems to
fall somewhere around $75,000.? ||| ??????????
?????, ??????? ???????????? ???????
????????, ??????? ?? ?????? ????????, ?
????? ?? ??????, ? ??????? ?? ?????????.?
In the experiment above we considered such
pairs to be mis-aligned. But ideally, the sec-
ond part of the source sentence should be de-
tached and start ?looking for? appropriate equiv-
alent. Whether this can be done with the help of
POS-tagging (or, perhaps, syntactic parsing), fur-
ther research will show.
The same is true about the possibility to ap-
ply this method to Russian-English translations
or translations between typologically distant lan-
guages.
67
5 Conclusion
In this paper, approach to improve English-
Russian sentence alignment was introduced, based
on part-of-speech tagging of automatically aligned
source and target texts. Sequences of POS-marks
for each sentence (exactly, nouns, adjectives, verbs
and pronouns) are processed as ?words? and
Damerau-Levenshtein distance between them is
computed. This distance is then normalized by
the length of the target sentence and is used as
a threshold between supposedly mis-aligned and
?good? sentence pairs.
The experimental results show precision 0.81
and recall 0.8 for this method. This performance
alone allows the method to be used in parallel cor-
pora alignment, but at the same time leaves space
for further improvement.
Acknowledgments
The study was implemented in the framework of
the Basic Research Program at the National Re-
search University Higher School of Economics
(HSE) in 2013.
References
Kuang-hua Chen and Hsin-Hsi Chen. 1994. A part-of-
speech-based alignment algorithm. In Proceedings
of the 15th conference on Computational linguistics
- Volume 1, COLING ?94, pages 166?171, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
William A. Gale and Kenneth W. Church. 1991. A
program for aligning sentences in bilingual corpora.
In Proceedings of the 29th annual meeting on As-
sociation for Computational Linguistics, ACL ?91,
pages 177?184, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
Heiki-Jaan Kaalep and Kaarel Veskis. 2007. Compar-
ing parallel corpora and evaluating their quality. In
Proceedings of MT Summit XI, pages 275?279.
G.E. Kedrova and S.B. Potemkin. 2008. Alignment
of un-annotated parallel corpora. In Papers from
the annual international conference ?Dialogue?, vol-
ume 7, pages 431?436, Moscow, Russia.
Monica Monachini and Nicoletta Calzolari. 1996.
Eagles synopsis and comparison of morphosyntac-
tic phenomena encoded in lexicons and corpora. a
common proposal and applications to european lan-
guages. Technical report, Paris, France.
Franz Josef Och and Hermann Ney. 2000. A com-
parison of alignment models for statistical machine
translation. In Proceedings of the 18th conference
on Computational linguistics - Volume 2, COLING
?00, pages 1086?1090, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Llu??s Padro? and Evgeny Stanilovsky. 2012. Freel-
ing 3.0: Towards wider multilinguality. In Nico-
letta Calzolari (Conference Chair), Khalid Choukri,
Thierry Declerck, Mehmet Ug?ur Dog?an, Bente
Maegaard, Joseph Mariani, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC?12), Istanbul, Turkey, may. European
Language Resources Association (ELRA).
Michel Simard, George F. Foster, and Pierre Isabelle.
1992. Using cognates to align sentences in bilingual
corpora. In Proceedings of the Fourth International
Conference on Theoretical and Methodological Is-
sues in Machine Translation, pages 67?81.
Jo?rg Tiedemann. 2003. Combining clues for word
alignment. In Proceedings of the tenth conference
on European chapter of the Association for Compu-
tational Linguistics - Volume 1, EACL ?03, pages
339?346, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D.
Manning. 2002. Extensions to hmm-based statisti-
cal word alignment models. In Proceedings of the
ACL-02 conference on Empirical methods in natu-
ral language processing - Volume 10, EMNLP ?02,
pages 87?94, Stroudsburg, PA, USA. Association
for Computational Linguistics.
D. Varga, L. Ne?meth, P. Hala?csy, A. Kornai, V. Tro?n,
and V. Nagy. 2005. Parallel corpora for medium
density languages. In Recent Advances in Natural
Language Processing (RANLP 2005), pages 590?
596.
68
