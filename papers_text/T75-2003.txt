COMPUTATIONAL UNDERSTANDING 
Christopher K. Riesbeck 
I. METHODOLOGICAL POSITION 
The problem of computat ional  
understanding has often been broken into two 
sub-problems: how to syntact ical ly  analyze a 
natural  language sentence and how to 
semant ica l ly  interpret the results of the 
syntact ic analysis. There are many reasons 
for this subdivis ion of the task, involving 
histor ical  inf luences from American 
structural  l inguist ics and the early 
"knowledge-free" approaches to Art i f ic ia l  
Intel l igence. The sub-divis ion has remained 
basic to much work in the area because 
syntact ic analysis seems to be much more 
amenable to computat ional  methods than 
semantic interpretat ion does, and thus more 
workers have been attracted developing 
syntact ic analyzers first. 
It is my bel ief that this subdivis ion 
has hindered rather than helped workers in 
this area. It has led to much wasted effort 
on syntact ic parsers as ends in themselves. 
It raises false issues, such as how much 
semantics should be done by the syntact ic 
analyzer and how much syntactics should be 
done by the semantic interpreter. It leads 
researchers into a l l -or -none choices on 
language processing when they are trying to 
develop complete systems. E i ther  the 
researcher tries to build a syntact ic 
analyzer first, and usually gets no farther, 
or he ignores language processing 
altogether.  
The point to real ize is that these 
problems arise from an overemphasis on the 
syntax/semant ics  dist inction. Certa in ly 
both syntact ic knowledge and semantic 
knowledge are used in the process of 
comprehension. The false problems arise 
when the comprehension process i tself  is 
sect ioned off into weakly communicat ing 
sub-processes, one of which does syntact ic 
analysis  and the other of which does 
semantic. Why should considerat ion of the 
meaning of a sentence have to depend upon 
the successful  syntactic analysis of that 
sentence? This is certainly not a 
restr ict ion that appl ies to people. Why 
should computer programs be more l imited? 
A better model of comprehension 
therefore is one that uses a coherent set of 
processes operat ing upon information of 
di f ferent varieties. When this is done it 
becomes clearer that the real problems of 
computat ional  understanding involves 
quest ions like: what information is 
necessary for understanding a part icular 
text, how does the text cue in this 
information, how is general informat ion 
"tuned" to the current context, how is 
informat ion removed from play, and so on. 
These quest ions must be asked for all the 
dif ferent kinds of information that are 
used. 
Notice that these quest ions are the 
same ones that must be asked about ANY model 
ii 
of memory processes. The reason for this is 
obvious: COMPREHENSION IS A MEMORY PROCESS. 
This simple statement has several impor tant  
impl icat ions about what a comprehension 
model should look like. Comprehension as a 
memory process implies a set of concerns 
very different from those that arose when 
natural  language processing was looked at by 
l inguistics. It implies that the answers 
involve the generat ion of simple mechanisms 
and large data bases. It implies that these 
mechanisms should either be or at least look 
like the mechanisms used for common-sense 
reasoning. It implies that the information 
in the data bases should be organized for 
usefulness -- i.e., so that textual cues 
lead to the RAPID retr ieval  of ALL the 
RELEVANT information -- rather than for 
uni formity -- e.g., syntax in one place, 
semantics in another. 
The next section of this paper is 
concerned with a system of analysis 
mechanisms that I have been developing. 
While the discussion is l imited pr imari ly to 
the problem of computat ional  understanding, 
I hope it wil l  be clear that both the 
mechanisms and the organizat ion of the data 
base given are part of a more general model 
of human memory. 
II. ANALYSIS MECHANISMS 
It has been recognized for some time 
now that understanding even apparent ly  
simple texts can involve the appl icat ion of 
quite general world knowledge, that is, of 
knowledge that would not normal ly be 
considered part of one's knowledge of the 
language in which the text is written. The 
set of informat ion that might be needed for 
understanding a text is therefore 
tremendous. Clearly an understanding system 
cannot be applying all it knows 'to 
everything it reads all the time. It must 
have mechanisms for guessing what 
information is l ikely to be needed in the 
near future. As long as its guesses are 
good, and the understander  updates them in 
the light of new input, understanding can 
proceed at a reasonable rate. 
In other words, the understander must 
be good at PREDICTING what it is l ikely to 
see. Further the data base must be 
organized so that coherent clusters of 
relevant informat ion can be accessed quickly 
with these predict ions. But since no finite 
static data base can have exact ly the  right 
informat ion for every input, the 
understander  must be able to prune and 
modify the information that the data base 
contains so that it appl ies more precisely 
to the s i tuat ion at hand. 
The analyzer  which I developed in my 
thesis \[Riesbeck, 1974\] was based on the 
concept of "expectat ion".  The analyzer 
program consisted of a fairly simple monitor 
program and a lexicon. The lexicon was a 
data base whose contents were organized 
under words and their roots. The 
informat ion in the data base was in the form 
of pairs of predicates and programs, which 
were cal led EXPECTATIONS. 
The analysis  p rocesscons is ted  of the 
monitor reading sentences, one word at a 
time, from left to right. As each word was 
read, the monitor  did two things. It looked 
up the word (or word root if no entry was  
found for the word) in the lexicon, and 
added the associated expectat ions (if any) 
to a master  list of expectat ions.  Then each 
element of this master list was checked. 
Those expectat ions with predicates that 
evaluated to true were "tr iggered" -- i.e., 
their  programs were executed and the 
expectat ions were removed from the master 
list. Those expectat ions that were not 
t r iggered were left on the master list. 
When the end of the sentence was reached, 
the meaning of the sentence was that 
structure (if any) which the tr igger ings of 
the various expectat ions had built. 
A general  idea of the way the analyzer 
worked can be obtained by fo l lowing the flow 
of analys is  of the simple sentence "John 
gave Mary a beating." The chart on the next 
page gives an outl ine of the basic sequence 
of events that takes place in the analyzer  
as the sentence is read, one word at a time, 
from left to right. The column headed "WORD 
READ" indicates where the analyzer is in the 
sentence when something occurs. The column 
headed "EXPECTATIONS WAITING" gives the 
12 
predicate portion for all the act ivated but 
not yet tr iggered expectations. The column 
headed "EXPECTATIONS TRIGGERED" indicates, 
when a number is placed in that column, 
which expectat ion has just been tr iggered at 
that point in the analysis. The column 
headed "ACTIONS TAKEN" indicates what 
effects the tr iggered expectat ions had. 
INPUT refers to whatever has just been read 
or constructed from the input stream. 
Step 0 is the init ial  state of the 
analyzer  before the sentence is begun. The 
analyzer  sets up one expectat ion which 
assumes that the first NP it sees is the 
subject of a verb that wil l  come later. 
In Step I, the first 
word -- "John" -- is read. Because "John" 
is a proper name, it is treated as a noun 
phrase and thus Expectat ion I is tr iggered. 
The program for Expectat ion I chooses "John" 
to be the subject of whatever verb wil l  
fo l low.  Expectat ion I is then removed from 
the set of active expectations. There were 
no expectat ions l isted in the lexical  entry 
for "John". 
In Step 2, "gave" is read. The lexical 
entry for the root form "give" has three 
expectat ions l isted an~ these are added to 
the set of active expectat ions.  None of 
them are tr iggered. 
In Step 3, "Mary" is read. "Mary" is a 
noun phrase referr ing to a human and so 
Expectat ion 2 is tr iggered. The program for 
Expectat ion 2 chooses "Mary" to be the 
recipient of the verb "give". Then 
Expectat ion 2 is removed. There were no 
expectatons in the lexical  entry for "Mary". 
In Step 4, "a" is read. There is one 
expectat ion in the lexicon for "a". This is 
Expectat ion 5 which has a predicate that is 
always true. That means that Expectat ion 5 
is t r iggered immediately.  The program for 
Expectat ion 4 is a complex one. It sets 
aside in a temporary storage area the 
current list of act ive expectat ions.  In its 
place it puts Expectat ion 6, which wil l  be 
tr iggered when something in the input stream 
indicates that the noun phrase begun by "a" 
is complete. 
In Step 5, "beating" is read. There 
are no lexical  entr ies and "beating" is not 
a word that f inishes a noun phrase, so 
nothing happens. 
In Step 6, the end of the sentence is 
seen. This does f inish a noun phrase and so 
Expectat ion 6 is tr iggered. The program for 
Expectat ion 5 builds a noun phrase from the 
words that have been read since the "a" was 
seen. It places this back in the input 
stream and brings back the set of 
expectat ions that Expectat ion 5 had set 
aside. 
In Step 7, the input "a beating,, 
tr iggers Expectat ion 4. The program for 
Expectat ion 4 builds a conceptual  structure 
represent ing the idea of someone hi t t ing 
someone else repeatedly. It uses the 
subject "John" as the actor and the 
I 
I 
I 
! 
I 
I 
I 
i 
I 
I 
I 
I 
I 
1 
I 
1 
I 
II 
I 
recipient "Mary" as the Object being hit. 
The final result therefore is a 
representat ion that says that John hit Mary 
repeatedly. 
The program portions of the 
expectat ions therefore produced the meaning 
of a sentence. These programs were not 
l imited in power. Not only could they 
build, modify and delete syntactic and 
conceptual structures, but they could add, 
modify and delete the list of expectat ions 
as well. This is why the analysis monitor 
was so simple. All the real work was done 
by the program portions of the expectations. 
The predicates were predict ions about 
likely situations that would be encountered 
in the processing of the sentence. Some of 
these predict ions were about what words or 
l word types would be seen. For example, one 
of the expectat ion pairs in the lexical 
entry for "a" contained a predicate that a 
noun would be seen soon. Elsewhere in the 
l lexicon, there were expectat ions whose 
predicates were about the structures that 
other expectat ions had built or would build. 
There were also expectat ions with predicates 
that were true in all situations. In this 
l case the programs were supposed to be 
executed whenever the word referencing them 
in the lexicon was read. 
The predict ive power of the predicates 
arose from the fact that the predicate did 
not look at all the things that an input 
might mean. Rather it asked if the input 
COULD mean some part icular thing. If so the 
expectat ion was triggered. The predicate 
portions of expectat ions were the 
disambiguat ing component of the analyzer 
because they chose only those word meanings 
that the sentential  context had use for. 
To general ize this discr ipt ion of the 
analyzer a bit more, the basic memory 
mechanism used was the expectation, which 
l consisted of a predict ion about a possible 
future situation and instruct ions on what to 
do if that s ituation occurred. The basic 
organizat ion of memory was to have clusters 
i of these expectat ions attached to words and 
word roots. The access to this memory was 
through the words seen in a sentence being 
understood. 
I The thrust of the work of the analyzer 
had been on the development of the 
expectat ion mechanism as a viable analysis 
tool. This meant defining what kinds of 
I expectat ions were needed and how they could 
be easi ly retrieved. One of the major 
weaknesses of the analyzer was the lack of 
any sat isfactory control over the set of 
,. current ly active expectations. There was no 
I real tuning of the set of expectat ions found 
in the lexicon to fit the situation at hand. 
The only interact ion between expectat ions 
occurred when expectat ions were tr iggered 
l and produced concrete structures. The only 
mechanism for removing untr iggered 
expectat ions was the wholesale clearing of 
active memory at the end of a sentence. 
I The extension of the concept of 
expectat ions to make them more control lable 
13 
without destroying their general i ty has been 
the core of the work that I have been doing 
since the thesis. Programming is going on 
right now to incorporate the extensions into 
a second version of the analyzer. 
The first basic extension to the 
predicate-program format of the expectat ions 
was the addit ion of explicit information 
about the purposes of various expectations. 
That is, an expectat ion was made and -- more 
important ly -- kept around because there was 
some need that the tr iggering of this 
expectat ion would fulfill. For example, the 
verb "give"had listed in its lexical entry 
several expectat ions which could fill the 
recipient slot for that verb if triggered. 
There was one which looked for the next noun 
phrase referr ing to a human. This 
expectation, act ivated as soon as "give" was 
seen, would fill the recipient slot in 
sentences like "John gave Mary a book." A 
separate expectation, act ivated at the same 
time, looked for the preposit ion "to" 
fol lowed by  a noun phrase referr ing to 
something that was at least a physical 
object .  This expectat ion if tr iggered would 
fill the recipient of "give" with the object 
of the "to", as in sentences like "John gave 
the book to Mary." 
Both of these expectat ions have the 
same purpose: to fill the recipient case of 
the verb "give". As long as no recipient is 
found there is a reason for keeping both 
expectat ions active. And this implies that 
when the recipient case is f inally filled, 
either by one of the expectat ions set up by 
"give" or by some expectat ion set up by some 
later word, then there is no longer any 
reason for keeping any of these expectat ions 
and they should all be removed. 
If the monitor ing program is to be 
capable of both loading and removing the 
various expectations, it must know what the 
purposes of the expectat ions are. 
Unfortunately,  there are no constraints on 
what sorts of functions can appear as 
predicates and programs in an expectation, 
which makes such a capabi l i ty impossible. 
However it is not necessary for the monitor 
to recognize purposes for ALL expectations. 
It is suff ic ient for it to know about just 
those expectat ions that fill empty 
conceptual  or syntactic slots when they are 
tr iggered. The two expectat ion examples 
given above for f i l l ing the recipient case 
of the verb "give" are of this type. We can 
specify the purposes of such expectat ions by 
s imply specify ing what slot they fil l if 
tr iggered. The monitor can tell with these 
expectat ions when they should be kept and 
when they should be removed. The monitor 
leaves alone actions -- such as those that 
manipulate other expectat ions -- which are 
not l inkable to simple purposes. 
While this was the first important 
extension to the expectat ion format it was 
not the last. Almost immediately it was 
real ized that many expectat ions are 
dependent upon others in the sense that they 
cannot possibly be tr iggered unti l  the other 
ones are. For example, suppose we have an 
expectat ion whose predicate looks at the 
syntactic object slot of the verb "give" and 
whose program builds some conceptual 
structure using this information. Further 
suppose we have another expectat ion active 
at the same time whose predicate looks for a 
noun phrase in the input stream and whose 
program will fill in the syntactic object 
slot for "give" with that noun phrase. Then 
clearly the former expectat ion must wait for 
the latter to be tr iggered first before it 
has a chance of being tr iggered itself. 
This kind of dependency relat ionship 
between expectat ions is not just an 
interest ing observation. Remember that the 
predicate portion of an expectat ion was a 
PREDICTION about what might be seen. This 
means that  the first expectat ion -- the one 
whose predicate looks at the syntactic 
object of "give" when it is f inally 
fi l led -- is not only wait ing for the second 
expectat ion to be tr iggered but in fact is 
making a predict ion about what the second 
expectat ion will produce. This has two 
impl icat ion s ? 
First, if the second expectat ion cannot 
produce a structure that will satisfy the 
predicate of the first expectation, but 
there is an expectat ion that can, then the 
second expectat ion is less preferable to 
this third one, which means that the third 
one would be checked first when new input 
arrives. A dynamic ordering has been 
induced on the set of active expectations. 
Second, structure bui lding expectat ions 
often build from pieces of structures that 
other expectat ions build. If we have a 
predict ion about what an expectat ion should 
produce, we can then make predict ions about 
the sub-structures that the expectat ion 
builds with. These new predict ions can then 
inf luence the expectat ions producing those 
sub-structures,  and so on. 
For example, consider the two 
expectat ions for "give" that were given 
above. Suppose the predicate of first 
expectat ion looks for a syntactic object 
referr ing to an action -- such as "a sock" 
in one interpretat ion of the sentence "John 
gave Mary a sock." Since the second 
expectat ion is the one that fil ls in the 
syntact ic object slot of "give", there is 
now a predict ion that the second expectat ion 
wil l  produce a noun phrase referr ing to an 
action. Since the second expectat ion fi l ls 
the syntact ic object of "give" with a noun 
phrase that it finds in the input stream, 
the monitor  can predict that a noun phrase 
referr ing to an act ion will appear in the 
input stream. The effect of this predict ion 
is that when words are seen in the input, 
the first thing that is looked for is to see 
if they can refer to an action. If so, then 
that sense of the word is taken immediately. 
Thus a word like "sock" is d isambiguated 
immediately as a result of an expectat ion 
or ig inal ly  made about the syntactic object 
of "give". 
To pass the information from one 
expectat ion to the next about what an 
expectat ion would like to see, we need to 
know where the expectat ion is looking. That 
14 
is we need to know what the predicate of the 
expectat ion is applied to. This information 
can be specif ied in the same way that the 
purpose of the expectat ion was: by giving a 
conceptual  or syntactic slot. In this case, 
instead of giving the slot that the 
expectat ion fil ls if tr iggered, we specify 
the slot that the predicate of the 
expectat ion is applied to. Then by knowing 
what slot an expectat ion looks at, we know 
what expectaions this expectat ion depends 
on. It depends on those expectat ions that 
fill this slot -- i.e., that have a "purpose 
slot" equal to the "lock at slot" of the 
expectation. 
Let me summarize this discussion by 
giving the current format for speci fy ing 
expectat ions:  
(NEED FOCUS TEST ACTION SIDE-EFFECTS) 
where 
NEED is the slot the expectat ion fills if 
tr iggered, 
FOCUS is the slot the expectat ion looks at, 
TEST is the predicate portion of the 
expectation, 
ACTION is the structure bui lding portion of 
the expectation, 
S IDE-EFFECTS are those programs that act 
upon other expectat ions and are not -- at 
the moment -- incorporated into the 
network of dependencies and predictions. 
The analysis monitor  is fair ly 
content- independent.  Its job is to take 
input, use it to access clusters of 
expectations, keep active those expectat ions 
that might fill slots that are stil l  empty 
in part ia l ly-bui l t  structures, and keep 
track of the predict ions/preferences that 
are induced by the dependency re lat ionships 
between expectations. The actual knowledge 
about language and the world is stil l  
contained in the expectations, as was true 
in the original  analyzer. 
This encoding of knowledge into small 
pieces of programs that have both procedural  
and declarat ive aspects is of both practical  
and theoret ical  importance. In terms of 
implement ing an AI model, I have found it 
much easier to specify procedural  knowledge 
in small units of "in s i tuat ion X do Y". 
Further it is much easier, as a programmer, 
to extend and modify procedures written in 
this form. It is also easier for a program 
to manipulate knowledge in this way. 
Theoretical ly,  the expectat ion format 
seems to me to be a viable memory 
representat ion for highly procedural  
knowledge. With it we can design expl ic i t ly  
a theory of computat ional  understanding that 
does not have the forced divis ion between 
syntact ic and semantic analysis. Indiv idual  
expectat ions are usual ly concerned with 
syntact ic  or conceptual  structures, but all 
of the expectat ions are maintained in one 
large set. This al lows for those important 
expectat ions that convert information about 
syntact ic structures in semantic information 
and vice-versa. Thus information that 
or ig inal ly  started as an abstract conceptual  
I 
I 
I 
I 
I 
I 
I 
i 
i 
I 
I 
I 
I 
I 
I 
i 
I 
I 
II 
predict ion can be quickly disseminated 
throughout a dependency network of 
expectat ions and lead eventual ly to 
predict ions about things like word senses. 
For example, my thesis describes how 
the interpretat ion of the text "John was mad 
at Mary. He gave her a sock," uses a 
conceptual predict ion that "John wants 
something bad to happen to Mary," which 
follows from the first sentence, to choose 
the appropr iate sense of the word "sock" in 
the second sentence the first time the'word 
is seen. This can be done because the 
general conceptual predict ion in interact ion 
with the expectat ions in the lexical entry 
for "give" led to predict ions about the 
nature of the syntactic object of "give", 
which in turn led to predict ions about the 
words that would be seen in the input 
stream. 
In other words, the analysis 
system -- both the original  one and the new 
version -- as an approach to the 
computat ional  understanding problem, 
exempli f ies the general points made in the 
methodological  portion of this paper. It 
demonstrates the feasibi l i ty of doing 
understanding using very simple mechanisms 
for manipulat ing small but f lexible units of 
knowledge, without forcing the development 
of independent syntactic analyzers or 
semantic interpreters. These simple 
mechansisms al low for a direct attack on 
such problems as what information is 
absolutely necessary for understanding, how 
it is cal led for, and how a workably sized 
set of active information can be maintained. 
REFERENCE 
Riesbeck, C. "Computat ional  Understanding: 
Analysis of Sentences and Context," 
Ph.D. Thesis, Computer Science Dept., 
Stanford University, Stanford, CA. 
1974. 
IS 
STEP WORD READ EXPECTAT IONS EXPECTAT IONS ACT ION TAKEN 
ACT IVE  TR IGGERED 
0 none  I - is INPUT a none  none  
NP?  
I J ohn  I - is INPUT a I choose  " John  to be 
NP?  the  sub jec t  of  the  
verb  to come 
2 gave  2 - does  INPUT re fer  none  none  
to a human?  
3 - does  INPUT re fer  
to a phys ica l  
ob jec t?  
4 - does  INPUT re fer  
to an ac t ion?  
3 Mary  2 - does  INPUT re fer  2 choose  "Mary"  to 
to a human?  be the  rec ip ient  
3 - does  INPUT re fer  o f  "g ive"  
to a phys ica l  
ob jec t?  
4 - does  INPUT re fer  
to an ac t ion?  
4 a 3 - does  INPUT re fer  5 save  the  cur rent  
to a phys ica l  set  of  
ob jec t?  expectat ions  and  
4 - does  INPUT re fer  rep lace  it w i th :  
to an ac t ion?  6 - does  INPUT end 
5 - t rue  a NP?  
5 beat ing  6 - does  INPUT end  none  none  
a NP? 
6 per iod  6 - does  INPUT end 6 set  INPUT to the  
a NP? NP "a beat ing"  and  
reset  the  
expectat ion  set  
7 none  4 set  the  main  
ac t ion  of  the  
in terpretat ion  
to the  ac t ion  
named by INPUT;  
set  the  ac tor  to 
the  sub jec t  ( John)  
and  set  the  ob jec t  
to the  rec ip ient  
(Mary)  
3 - does  INPUT re fer  
to a phys ica l  
ob jec t?  
4 - does  INPUT re fer  
to an ac t ion?  
16 
II 
I 
I 
l 
l 
.I 
l 
i 
1 
D. 
D 
I 
I 
I 
1 
! 
I 
I 
I 
