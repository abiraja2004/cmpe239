Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 153?156,
Prague, June 2007. c?2007 Association for Computational Linguistics
FUH (FernUniversita?t in Hagen):
Metonymy Recognition Using
Different Kinds of Context for a Memory-Based Learner
Johannes Leveling
Intelligent Information and Communication Systems (IICS)
FernUniversita?t in Hagen (University of Hagen)
johannes.leveling@fernuni-hagen.de
Abstract
For the metonymy resolution task at
SemEval-2007, the use of a memory-based
learner to train classifiers for the identifica-
tion of metonymic location names is inves-
tigated. Metonymy is resolved on different
levels of granularity, differentiating between
literal and non-literal readings on the coarse
level; literal, metonymic, and mixed read-
ings on the medium level; and a number of
classes covering regular cases of metonymy
on a fine level. Different kinds of context
are employed to obtain different features:
1) a sequence of n1 synset IDs represent-
ing subordination information for nouns and
for verbs, 2) n2 prepositions, articles, modal,
and main verbs in the same sentence, and 3)
properties of n3 tokens in a context window
to the left and to the right of the location
name.
Different classifiers were trained on the
Mascara data set to determine which values
for the context sizes n1, n2, and n3 yield
the highest accuracy (n1 = 4, n2 = 3,
and n3 = 7, determined with the leave-one-
out method). Results from these classifiers
served as features for a combined classifier.
In the training phase, the combined classifier
achieved a considerably higher precision for
the Mascara data. In the SemEval submis-
sion, an accuracy of 79.8% on the coarse,
79.5% on the medium, and 78.5% on the
fine level is achieved (the baseline accuracy
is 79.4%).
1 Introduction
Metonymy is typically defined as a figure of speech
in which a speaker uses one entity to refer to an-
other that is related to it (Lakoff and Johnson, 1980).
The identification of metonymy becomes important
for NLP tasks such as question answering (Stallard,
1993) or geographic information retrieval (Leveling
and Hartrumpf, 2006).
For regular cases of metonymy for locations and
organizations, Markert and Nissim have proposed
a set of metonymy classes. Annotating a subset of
the BNC (British National Corpus), they extracted a
set of metonymic proper nouns from two categories:
country names (Markert and Nissim, 2002) and or-
ganization names (Nissim and Markert, 2003).
In the metonymy resolution task at SemEval-
2007, the goal was to identify metonymic names in a
subset of the BNC. The task consists of two subtasks
for company and country names, which are further
divided into classification on a coarse level (recog-
nizing literal and non-literal readings), on a medium
level (differentiating non-literal readings into mixed
and metonymic readings), and on a fine level (iden-
tifying classes of regular metonymy, such as a name
referring to the population, place-for-people). The
task is described in more detail by Markert and Nis-
sim (2007).
2 System Description
2.1 Tools and Resources
The following tools and resources are used for the
metonymy classification:
? TiMBL 5.1 (Daelemans et al, 2004), a
memory-based learner for classification is em-
153
ployed for training the classifiers (supervised
learning).1
? Mascara 2.0 ? Metonymy Annotation Scheme
And Robust Analysis (Markert and Nissim,
2003; Nissim and Markert, 2003; Markert
and Nissim, 2002) contains annotated data for
metonymic names from a subset of the the
BNC.
? WordNet 2.0 (Fellbaum, 1998) serves as a lin-
guistic resource for assigning synset IDs and
for looking up subordination information and
frequency of readings.
? The TreeTagger (Schmid, 1994) is utilized for
sentence boundary detection, lemmatization,
and part-of-speech tagging. The English tag-
ger was trained on the PENN treebank and uses
the English morphological database from the
XTAG project (Karp et al, 1992). The param-
eter files were obtained from the web site.2
2.2 Different Kinds of Context
Following the assumption that metonymic location
names can be identified from the context, there are
different kinds of context to consider. At most, the
context comprises a single sentence in this setup.
Three kinds of context were employed to extract fea-
tures for the memory-based learner TiMBL:
? C1: Subordination (hyponymy) information for
nouns and verbs from the left and right context
of the possibly metonymic name.
? C2: The sentence context for modal verbs, main
verbs, prepositions, and articles.
? C3: A context window of tokens left and right
of the location name.
The trial data provided (a subset of the Mascara
data) contained 188 non-literal location names (of
925 samples total). For a supervised learning ap-
proach, this is too few data. Therefore, the full
Mascara data was converted to form training data
consisting of feature values for context C1, C2, and
1Peirsman (2006) also employs TiMBL for metonymy reso-
lution, but trains a single classifier.
2http://www.ims.uni-stuttgart.de/projek-
te/corplex/TreeTagger/
C3. The training data contained 509 metonymic an-
notations (of 2797 samples total). Some cases in
the Mascara corpus are filtered during processing,
including cases annotated as homonyms and cases
whose metonymy class could not be agreed upon.
The test data had a majority baseline of 82.8% accu-
racy for country names.
2.3 Features
The Mascara data was processed to extract the fol-
lowing features (no hand-annotated data from Mas-
cara was employed for feature values, i.e. no gram-
matical roles):
? For C1 (WordNet context): From a context of
n1 verbs and nouns in the same sentence, their
distance to the location name is calculated. A
sequence of eight feature values of WordNet
synset IDs is obtained by iteratively looking up
the most frequent reading for a lemma inWord-
Net and determining its synset ID. Subordina-
tion information between synsets is used to find
a parent synset. This process is repeated until
a top-level parent synset is reached. No actual
word sense disambiguation is employed.
? For C2 (sentence context): Sentence bound-
aries, part-of-speech tags, and lemmatization
are determined from the TreeTagger output.
From a context window of n2 tokens, lemma
and distance are encoded as feature values for
prepositions, articles, modal, and main verbs
? For C3 (word context): From a context of n3
tokens to the left and to the right, the distance
between token and location name, three pre-
fix characters, three suffix characters, part-of-
speech tag, case information (U=upper case,
L=lower case, N=numeric, O=other), and word
length are used as feature values.
Table 1 and Table 2 show results for mem-
ory based learners trained with TiMBL. Perfor-
mance measures were obtained with the leave-one-
out method. The classifiers were trained on fea-
tures for different context sizes (ni ranging from 2
to 7) to determine the setting for which the highest
accuracy is achieved (e.g. 1c, 2c, and 3c). In the
next step, classifiers with a combined context were
154
Table 1: Results for training the classifiers on the
coarse location name classes (2797 instances, 509
non-literal, leave-one-out) for the Mascara data (P =
precision, R = recall, F = F-score).
ID n1,n2,n3 coarse class P R F
1c 4,0,0 literal 0.850 0.893 0.871
1c 4,0,0 non-literal 0.377 0.289 0.327
2c 0,3,0 literal 0.848 0.874 0.860
2c 0,3,0 non-literal 0.342 0.295 0.317
3c 0,0,7 literal 0.880 0.889 0.885
3c 0,0,7 non-literal 0.478 0.455 0.467
4c 4,3,0 literal 0.848 0.892 0.896
4c 4,3,0 non-literal 0.368 0.282 0.320
5c 4,0,7 literal 0.860 0.913 0.885
5c 4,0,7 non-literal 0.459 0.332 0.385
6c 0,3,7 literal 0.875 0.905 0.889
6c 0,3,7 non-literal 0.496 0.420 0.455
7c 4,3,7 literal 0.860 0.918 0.888
7c 4,3,7 non-literal 0.473 0.332 0.390
8c res. of 1c?7c literal 0.852 0.968 0.907
8c res. of 1c?7c non-literal 0.639 0.248 0.357
trained, selecting the setting with the highest accu-
racy for a single context for the combination (e.g.
4c, 5c, 6c, and 7c). As an additional experiment, a
classifier was trained on classification results of the
classifiers described above (combination of 1?7, e.g.
8c). It was expected that the combination of features
from different kinds of context would increase per-
formance, and that the combination of classifier re-
sults would increase performance.
3 Evaluation Results
Table 3 shows results for the official submission.
Compared to results from the training phase on
the Mascara data (tested with the leave-one-out
method), performance is considerably lower. For
this data, the combined classifier achieved a consid-
erably higher precision (63.9% for non-literal read-
ings; 57.3% for the fine class place-for-people and
even 83.3% for the rare class place-for-event).
Performance may be affected by several reasons:
A number of problems were encountered while pro-
cessing the data. The TreeTagger automatically to-
kenizes its input and applies sentence boundary de-
tection. In some cases, the sentence boundary detec-
tion did not work well, returning sentences of more
than 170 words. Furthermore, the tagger output had
to be aligned with the test data again, as multi-word
Table 2: Excerpt from results for training the clas-
sifiers on the fine location name classes (2797 in-
stances, leave-one-out) for the Mascara data.
ID n1,n2,n3 fine class P R F
1f 4,0,0 literal 0.851 0.895 0.873
1f 4,0,0 pl.-for-p. 0.366 0.280 0.318
1f 4,0,0 pl.-for-e. 0.370 0.270 0.312
2f 0,3,0 literal 0.848 0.876 0.862
2f 0,3,0 pl.-for-p. 0.332 0.276 0.301
2f 0,3,0 pl.-for-e. 0.222 0.270 0.244
3f 0,0,7 literal 0.878 0.892 0.885
3f 0,0,7 pl.-for-p. 0.463 0.424 0.442
3f 0,0,7 pl.-for-e. 0.279 0.324 0.300
4f 4,3,0 literal 0.851 0.899 0.875
4f 4,3,0 pl.-for-p. 0.358 0.269 0.307
4f 4,3,0 pl.-for-e. 0.435 0.270 0.333
5f 4,0,7 literal 0.861 0.914 0.887
5f 4,0,7 pl.-for-p. 0.452 0.322 0.377
5f 4,0,7 pl.-for-e. 0.550 0.297 0.386
6f 0,3,7 literal 0.871 0.906 0.888
6f 0,3,7 pl.-for-p. 0.468 0.383 0.422
6f 0,3,7 pl.-for-e. 0.400 0.324 0.358
7f 4,3,7 literal 0.861 0.918 0.889
7f 4,3,7 pl.-for-p. 0.459 0.323 0.378
7f 4,3,7 pl.-for-e. 0.500 0.297 0.373
8f res. of 1f?7f literal 0.854 0.963 0.905
8f res. of 1f?7f pl.-for-p. 0.573 0.262 0.360
8f res. of 1f?7f pl.-for-e. 0.833 0.270 0.408
names (e.g. New York) were split into different to-
kens. In addition, the tag set of the tagger differs
somewhat from the official PENN tag set and in-
cludes additional tags for verbs.
In earlier experiments on metonymy classifica-
tion on a German corpus (Leveling and Hartrumpf,
2006), the data was nearly evenly distributed be-
tween literal and metonymic readings. This seems
to make a classification task easier because there is
no hidden bias in the classifier (i.e. the baseline of
always selecting the literal readings is about 50%).
Features are obtained by shallow NLP methods
only, not making use of a parser or chunker. Thus,
important syntactic or semantic information to de-
cide on metonymy might be missing in the features.
However, semantic features are more difficult to de-
termine, because reliable automatic tools for seman-
tic annotation are still missing. This is also indi-
cated by the fact that the grammatical roles (com-
prising syntactic features) in Mascara data are hand-
annotated.
However, some linguistic phenomena are already
implicitly represented by shallower features from
155
Table 3: Results for the coarse (908 samples: 721
literal, 187 non-literal), medium (721 literal, 167
metonymic, 20 mixed), and fine classification (721
literal, 141 place-for-people, 10 place-for-event, 1
place-for-product, 4 object-for-name, 11 othermet,
20 mixed) of location names.
class P R F
FUH.location.coarse (0.798 accuracy)
literal 0.812 0.971 0.884
non-literal 0.543 0.134 0.214
FUH.location.medium (0.795 accuracy)
literal 0.810 0.970 0.883
metonymic 0.500 0.132 0.208
mixed 0.0 0.0 0.0
FUH.location.fine (0.785 accuracy)
literal 0.808 0.965 0.880
place-for-people 0.386 0.120 0.183
the surface level (given enough training instances).
For instance, active/passive voice may be encoded
by a combination of features for main verb/modal
verbs. If only a small training corpus is available,
overall performance will be higher when utilizing
explicit syntactic or semantic features.
Finally, the data may be too sparse for a super-
vised memory-based learning approach. The iden-
tification of rare classes of metonymy (e.g. place-
for-event) would greatly benefit from a larger corpus
covering these classes.
4 Conclusion
Evaluation results on the training data were very
promising, indicating a boost of precision by com-
bining classification results. In the training phase,
an accuracy of 83.7% was achieved on the coarse
level, compared to the majority baseline accuracy of
81.8%. For the submission for the metonymy res-
olution task at SemEval-2007, accuracy is close to
the majority baseline (79.4%) on the coarse (79.8%),
medium (79.5%), and fine (78.5%) level.
In summary, using different context sizes for dif-
ferent kinds of context and combining results of dif-
ferent classifiers for metonymy resolution increases
performance. The general approach would profit
from combining results of more diverse classifiers,
i.e. classifiers employing features extracted from the
surface, syntactic, and semantic context of a location
name.
Acknowledgments
The research described was in part funded by the
DFG (Deutsche Forschungsgemeinschaft) in the
project IRSAW (Intelligent Information Retrieval on
the Basis of a Semantically Annotated Web).
References
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2004. TiMBL: Tilburg memory
based learner, version 5.1. TR 04-02, ILK.
Christiane Fellbaum, editor. 1998. Wordnet. An Elec-
tronic Lexical Database. MIT Press, Cambridge, Mas-
sachusetts.
Daniel Karp, Yves Schabes, Martin Zaidel, and Dania
Egedi. 1992. A freely available wide coverage mor-
phological analyzer for English. In Proc. of COLING-
92, pages 950?955, Morristown, NJ.
George Lakoff and Mark Johnson. 1980. Metaphors We
Live By. Chicago University Press.
Johannes Leveling and Sven Hartrumpf. 2006. On
metonymy recognition for GIR. In Proc. of GIR-2006,
the 3rd Workshop on Geographical Information Re-
trieval (held at SIGIR 2006), Seattle, Washington.
Katja Markert and Malvina Nissim. 2002. Towards a
corpus for annotated metonymies: The case of location
names. In Proc. of LREC 2002, Las Palmas, Spain.
Katja Markert and Malvina Nissim. 2003. Corpus-based
metonymy analysis. Metaphor and symbol, 18(3).
Katja Markert and Malvina Nissim. 2007. Task 08:
Metonymy resolution at SemEval-07. In Proc. of Sem-
Eval 2007.
Malvina Nissim and Katja Markert. 2003. Syntactic
features and word similarity for supervised metonymy
resolution. In Proc. of ACL-2003, Sapporo, Japan.
Yves Peirsman. 2006. Example-based metonymy recog-
nition for proper nouns. In Proc. of the Student Re-
search Workshop of EACL-2006, pages 71?78, Trento,
Italy.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on NewMethods in Language Processing, Manchester,
UK.
David Stallard. 1993. Two kinds of metonymy. In Proc.
of ACL-93, pages 87?94, Columbus, Ohio.
156
