Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 75?83,
Beijing, August 2010
Adverse?Effect Relations Extraction from  
Massive Clinical Records 
Yasuhide Miura a, Eiji Aramaki b, Tomoko Ohkuma a, Masatsugu Tonoike a,  
Daigo Sugihara a, Hiroshi Masuichi a and Kazuhiko Ohe c 
     a  Fuji Xerox Co., Ltd.
       b Center for Knowledge Structuring, University of Tokyo
         c University of Tokyo Hospital
yasuhide.miura@fujixerox.co.jp, eiji.aramaki@gmail.com, 
{ohkuma.tomoko,masatsugu.tonoike,daigo.sugihara, 
hiroshi.masuichi}@fujixerox.co.jp,  
kohe@hcc.h.u-tokyo.ac.jp 
 
Abstract 
The rapid spread of electronic health 
records raised an interest to large-scale 
information extraction from clinical 
texts. Considering such a background, 
we are developing a method that can 
extract adverse drug event and effect 
(adverse?effect) relations from massive 
clinical records. Adverse?effect rela-
tions share some features with relations 
proposed in previous relation extrac-
tion studies, but they also have unique 
characteristics. Adverse?effect rela-
tions are usually uncertain. Not even 
medical experts can usually determine 
whether a symptom that arises after a 
medication represents an adverse?
effect relation or not. We propose a 
method to extract adverse?effect rela-
tions using a machine-learning tech-
nique with dependency features. We 
performed experiments to extract ad-
verse?effect relations from 2,577 clini-
cal texts, and obtained F1-score of 
37.54 with an optimal parameters and 
F1-score of 34.90 with automatically 
tuned parameters. The results also 
show that dependency features increase 
the extraction F1-score by 3.59.  
1 Introduction  
The widespread use of electronic health rec-
ords (EHR) made clinical texts to be stored as 
computer processable data. EHRs contain im-
portant information about patients? health. 
However, extracting clinical information from 
EHRs is not easy because they are likely to be 
written in a natural language. 
We are working on a task to extract adverse 
drug event and effect relations from clinical 
records. Usually, the association between a 
drug and its adverse?effect relation is investi-
gated using numerous human resources, cost-
ing much time and money. The motivation of 
our task comes from this situation. An example 
of the task is presented in Figure 1. We defined 
an adverse?effect relation as a relation that 
holds between a drug entity and a symptom 
entity. The sentence illustrates the occurrence 
of the adverse?effect hepatic disorder by the 
Singulair medication.  
 
Figure 1. Example of an adverse?effect relation. 
A hepatic disorder found was suspected drug-induced and the Singulair was stopped.
adverse?effect relation
symptom drug
75
A salient characteristic of adverse?effect re-
lations is that they are usually uncertain. The 
sentence in the example states that the hepatic 
disorder is suspected drug-induced, which 
means the hepatic disorder is likely to present 
an adverse?effect relation. Figure 2 presents an 
example in which an adverse?effect relation is 
suspected, but words to indicate the suspicion 
are not stated. The two effects of the drug??the 
recovery of HbA1c and the appearance of the 
edema??are expressed merely as observation 
results in this sentence. The recovery of 
HbA1c is an expected effect of the drug and 
the appearance of the edema probably repre-
sents an adverse?effect case. The uncertain 
nature of adverse?effect relations often engen-
ders the statement of an adverse?effect rela-
tion as an observed fact. A sentence includ-
ing an adverse?effect relation occasionally be-
comes long to list all observations that ap-
peared after administration of a medication. 
Whether an interpretation that expresses an 
adverse?effect relation, such as drug-induced 
or suspected to be an adverse?effect, exists in a 
clinical record or not depends on a person who 
writes it. However, an adverse?effect relation 
is associated with an undesired effect of a 
medication. Its appearance would engender an 
extra action (e.g. stopped in the first example) 
or lead to an extra indication (e.g. but ? ap-
peared in the second example). Proper han-
dling of this extra information is likely to boost 
the extraction accuracy. 
The challenge of this study is to capture re-
lations with various certainties. To establish 
this goal, we used a dependency structure for 
the adverse?effect relation extraction method. 
Adverse?effect statements are assumed to 
share a dependency structure to a certain 
degree. For example, if we obtain the depend-
ency structures as shown in Figure 3, then we 
can easily determine that the structures are 
similar. Of course, obtaining such perfect pars-
ing results is not always possible. A statistical 
syntactic parser is known to perform badly if a 
text to be parsed belongs to a domain which 
differs from a domain on which the parser is 
trained (Gildea, 2001). A statistical parser will 
likely output incomplete results in these texts 
and will likely have a negative effect on rela-
tion extraction methods which depend on it. 
The specified research topic of this study is to 
investigate whether incomplete dependency 
structures are effective and how they behave in 
the extraction of uncertain relations.  
Figure 2. The example of an adverse-effect relation where the suspicion is not stated. 
Figure 3. The example of a similarity within dependency structures. 
ACTOS 30 recovered HbA1c to 6.5% but an edema appeared after the medication.
A suspected drug-induced hepatic disorder found and the Singulair was stopped.
conjunct
nominal subject nominal subject
nominal subject nominal subject
conjunct
was
ACTOS 30 recovered HbA1c to 6.5% but an edema appeared after the medication.
adverse-effect relation
drug symptom
76
2 Related Works 
Various studies have been done to extract se-
mantic information from texts. SemEval-2007 
Task:04 (Girju et al, 2007) is a task to extract 
semantic relations between nominals. The task 
includes ?Cause?Effect? relation extraction, 
which shares some similarity with a task that 
will be presented herein. Saeger et al (2008) 
presented a method to extract potential trou-
bles or obstacles related to the use of a given 
object. This relation can be interpreted as a 
more general relation of the adverse?effect 
relation. The protein?protein interaction (PPI) 
annotation extraction task of BioCreative II 
(Krallinger et al, 2008) is a task to extract PPI 
from PubMed abstracts. BioNLP?09 Shared 
Task on Event Extraction (Kim et al, 2009) is 
a task to extract bio-molecular events (bio-
events) from the GENIA event corpus.  
Similar characteristics to those of the ad-
verse?effect relation are described in previous 
reports in the bio-medical domain. Friedman et 
al. (1994) describes the certainty in findings of 
clinical radiology. Certainty is also known in 
scientific papers of biomedical domains as 
speculation (Light et al, 2004). Vincze et al 
(2008) are producing a freely available corpus 
including annotations of uncertainty along with 
its scope. 
Dependency structure feature which we uti-
lized to extract adverse?effect relations are 
widely used in relation extraction tasks. We 
present previous works which used syntac-
tic/dependency information as a feature of a 
statistical method. Beamer et al (2007), Giuli-
ano et al (2007), and Hendrickx et al (2007) 
all used syntactic information with machine 
learning techniques in SemEval-2007 Task:04 
and achieved good performance. Riedel et al 
(2009) used dependency path features with a 
statistical relational learning method in Bi-
oNLP?09 Shared Task on Event Extraction and 
achieved the best performance in the event en-
richment subtask. Miyao et al (2008) com-
pared syntactic information of various statisti-
cal parsers on PPI. 
3 Corpus  
We produced an annotated corpus of adverse?
effect relations to develop and test an adverse?
effect relation extraction method. This section 
presents a description of details of the corpus. 
3.1 Texts Comprising the Corpus 
We used a discharge summary among various 
documents in a hospital as the source data of 
the task. The discharge summary is a docu-
ment created by a doctor or another medical 
expert at the conclusion of a hospital stay. 
Medications performed during a stay are writ-
ten in discharge summaries. If adverse?effect 
relations were observed during the stay, they 
are likely to be expressed in free text. Texts 
written in discharge summaries tend to be writ-
ten more roughly than texts in newspaper arti-
cles or scientific papers. For example, the 
amounts of medications are often written in a 
name-value list as shown below: 
?When admitted to the hospital, Artist 6 mg1x, 
Diovan 70 mg1x, Norvasac 5 mg1x and BP 
was 145/83, but after dialysis, BP showed a 
decreasing tendency and in 5/14 Norvasac was 
reduced to 2.5 mg1x.? 
3.2 Why Adverse?Effect Relation Extrac-
tion from Discharge Summaries is 
Important 
In many countries, adverse?effects are investi-
gated through multiple phases of clinical trials, 
but unexpected adverse?effects occur in actual 
medications. One reason why this occurs is 
that drugs are often used in combination with 
others in actual medications. Clinical trials 
usually target single drug use. For that reason, 
the combinatory uses of drugs occasionally 
engender unknown effects. This situation natu-
rally motivates automatic adverse?effect rela-
tion extraction from actual patient records.  
  
77
 3.3 Corpus Size 
We collected 3,012 discharge summaries1 writ-
ten in Japanese from all departments of a hos-
pital. To reduce a cost to survey the occurrence 
of adverse?effects in the summaries, we first 
split the summaries into two sets: SET-A, 
which contains keywords related to adverse?
effects and SET-B, which do not contain the 
keywords. The keywords we used were ?stop, 
change, adverse effect?, and they were chosen 
based on a heuristic. The keyword filtering 
resulted to SET-A with 435 summaries and 
SET-B with 2,577 summaries. Regarding SET-
A, we randomly sampled 275 summaries and 
four annotators annotated adverse?effect in-
formation to these summaries to create the ad-
verse?effect relation corpus. For SET-B, the 
four annotators checked the small portion of 
the summaries. Cases of ambiguity were re-
solved through discussion, and even suspicious 
adverse?effect relations were annotated in the 
corpus as positive data. The overview of the 
summary selection is presented in Figure 4.  
                                                 
1 All private information was removed from them. 
The definition of private information was referred 
from the HIPAA guidelines. 
3.4 Quantities of Adverse?Effects in Clin-
ical Texts 
55.6% (=158/275) of the summaries in SET-A 
contained adverse?effects. 11.3% (=6/53) of 
the summaries in SET-B contained adverse?
effects. Since the ratio of SET-A:SET-B is 
14.4:85.6, we estimated that about 17.7%  
(=0.556?0.144+0.113?0.856) of the summar-
ies contain adverse?effects. Even considering 
that a summary may only include suspected 
adverse?effects, we think that discharge sum-
maries are a valuable resource to explore ad-
verse?effects. 
3.5 Annotated Information 
We annotated information of two kinds to the 
corpus: term information and relation infor-
mation. 
(1) Term Annotation  
Term annotation includes two tags: a tag to 
express a drug and a tag to express a drug ef-
fect. Table 1 presents the definition. In the 
corpus, 2,739 drugs and 12,391 effects were 
annotated. 
(2) Relation Annotation  
Adverse?effect relations are annotated as the 
?relation? attribute of the term tags. We repre-
sent the effect of a drug as a relation between a 
drug tag and a symptom tag. Table 2 presents 
Table 2. Annotation examples. 
Figure 4. The overview of the summary 
selection. 
Table 1. Markup scheme. 
The expression of a disease or 
symptom: e.g. endometrial cancer, 
headache. This tag covers not only a 
noun phrase but also a verb phrase 
such as ?<symptom>feels a pain in 
front of the head</symptom>?.
symptom
The expression of an administrated 
drug: e.g. Levofloxacin, Flexeril. 
drug
Definition and Examplestag
<drug relation=?1?>ACTOS(30)</drug> brought 
both <symptom relation=?1?>headache<symptom> 
and <symptom relation=?1?>insomnia</symptom>.
<drug relation=?1?>Ridora</drug> resumed 
because it is associated with an <symptom 
relation=?1?>eczematous rash</symptom>.
* If a drug has two or more adverse-effects, 
symptoms take a same relation ID.
3,012 
discharge 
summaries
435
summaries
w/ keywords
2,577
summaries
w/o keywords
275
summaries
53
summaries
153
summaries
w/ adverse?
effects
122
summaries
w/o adverse?
effects
6
summaries
w/ adverse?
effects
47
summaries
w/o adverse?
effects
YES NO
Contain keywords?
Random samplingRandom sampling
Contain adverse?
effects?
Contain adverse?
effects?
YES YESNO NO
SET-A (annotated corpus) SET-B
78
several examples, wherein ?relation=1? de-
notes the ID of a adverse?effect relation. In the 
corpus, 236 relations were annotated.  
4 Extraction Method 
We present a simple adverse?effect relation 
extraction method. We extract drug?symptom 
pairs from the corpus and discriminate them 
using a machine-learning technique. Features 
based on morphological analysis and depend-
ency analysis are used in discrimination. This 
approach is similar to the PPI extraction ap-
proach of Miyao et al (2008), in which we 
binary classify pairs whether they are in ad-
verse?effect relations or not. A pattern-based 
semi-supervised approach like Saeger et al 
(2008), or more generally Espresso (Pantel and 
Pennacchiotti, 2006), can also be taken, but we 
chose a pair classification approach to avoid 
the effect of seed patterns. To capture a view 
of an adverseness of a drug, a statistic of ad-
verse?effect relations is important. We do not 
want to favor certain patterns and chose a pair 
classification approach to equally treat every 
relation. Extraction steps of our method are as 
presented below. 
STEP 1: Pair Extraction   
All combinations of drug?symptom pairs that 
appear in a same sentence are extracted. Pairs 
<drug relation=?1?>Lasix</drug> for 
<symptom>hyperpiesia</symptom> has 
been suspended due to the appearance of 
a <symptom relation=?1?>headache
</symptom>.
headacheLasixpositive
hyperpiesiaLasixnegative
symptomdruglabel
ID Feature Definition and Examples
1 Character Distance The number of characters between members of a pair.
2 Morpheme Distance The number of morpheme between members of a pair.
3 Pair Order Order in which a drug and a symptom appear in a text; 
?drug?symptom? or ?symptom?drug?.
4 Symptom Type The type of symptom: ?disease name?, ?medical test name?, 
or ?medical test value?. 
5 Morpheme Chain Base?forms of morphemes that appear between a pair.
6 Dependency Chain Base?forms of morphemes included in the minimal 
dependency path of a pair.
7 Case Frame Chain Verb, case frame, and object triples that appear between a 
pair: e.g. ?examine? ??de?(case particle) ? ?inhalation?, 
?begin? ??wo?(case particle) ??medication?.
8 Case Frame 
Dependency Chain
Verb, case frame, and object triples included in the minimal 
dependency path of a pair.
Figure 6. Dependency chain example. 
 
Figure 5. Pair extraction example. 
hyperpiesia no-PP
for no-PP
Lasix wo-PP
headache no-PP 
appear niyori-PP
suspend ta-AUX
Lasix, wo-PP, headache, no-PP, 
appear, niyori-PP, suspend, ta-AUX
minimal path
Table 3. Features used in adverse-effect extraction. 
79
with the same relation ID become positive 
samples; pairs with different relation IDs be-
come negative samples. Figure 5 shows exam-
ples of positive and negative samples.  
STEP 2: Feature Extraction  
Features presented in Table 3 are extracted. 
The text in the corpus is in Japanese. Some 
features assume widely known characteristics 
of Japanese. For example, the dependency fea-
ture allows a phrase to depend on only one 
phrase that appears after a dependent phrase. 
Figure 6 portrays an example of a dependency 
chain feature. In the example, most terms were 
translated into English, excluding postpositions 
(PP) and auxiliaries (AUX), which are ex-
pressed in italic. To reduce the negative effect 
of feature sparsity, features which appeared in 
more than three summaries are used for fea-
tures with respective IDs 5?8. 
STEP 3: Machine Learning  
The support vector machine (SVM) (Vapnik, 
1995) is trained using positive/negative labels 
and features extracted in prior steps. In testing,                                          
an unlabeled pair is given a positive or nega-
tive label with the trained SVM.  
5 Experiment 
We performed two experiments to evaluate the 
extraction method. 
5.1 Experiment 1 
Experiment 1 aimed to observe the effects of 
the presented features. Five combinations of 
the features were evaluated with a five-fold 
cross validation assuming that an optimal pa-
rameter combination was obtained. The exper-
iment conditions are described below: 
A. Data  
7,690 drug?symptom pairs were extracted 
from the corpus.  Manually annotated infor-
mation was used to identify drugs and symp-
toms. Within 7,690 pairs, 149 pairs failed to 
extract the dependency chain feature. We re-
moved these 149 pairs and used the remaining 
7,541 pairs in the experiment. The 7,541 pairs 
consisted of 367 positive samples and 7,174 
negative samples.  
B. Feature Combinations  
We tested the five combinations of features in 
the experiment. Manually annotated infor-
mation was used for the symptom type feature. 
Features related to morphemes are obtained by 
processing sentences with a Japanese mor-
phology analyzer (JUMAN2 ver. 6.0). Features 
related to dependency and case are obtained by 
processing sentences using a Japanese depend-
ency parser (KNP ver. 3.0; Kurohashi and Na-
gao, 1994).  
C. Evaluations  
We evaluated the extraction method with all 
combinations of SVM parameters in certain 
                                                 
2 http://www-lab25.kuee.kyoto-u.ac.jp/nl-
resource/juman-e.html 
E
D
C
B
A
ID
35.45
35.01
34.39
33.30
26.72
Precision
41.05
40.67
43.06
42.43
46.21
Recall
log(c)=1.0, log(g)=-5.0, p=0.10
log(c)=1.0, log(g)=-5.0, p=0.10
log(c)=1.0, log(g)=-5.0, p=0.10
log(c)=1.0, log(g)=-5.0, p=0.10
log(c)=3.0, log(g)=-5.0, p=0.10
Parameters
37.181,2,3,4,5,6,7,8
36.781,2,3,4,5,6,8
37.541,2,3,4,5,6,7
36.641,2,3,4,5,6
33.051,2,3,4,5
F1-scoreFeature 
Combination
Table 4. Best F1-scores and their parameters. 
Figure 7. Precision?recall distribution. 
80
ranges. We used LIBSVM3 ver. 2.89 as an im-
plementation of SVM. The radial basis func-
tion (RBF) was used as the kernel function of 
SVM. The probability estimates option of 
LIBSVM was used to obtain the confidence 
value of discrimination.  
The gamma parameter of the RBF kernel 
was chosen from the range of [2-20, 20]. The C 
parameter of SVM was chosen from the range 
of [2-10, 210]. The SVM was trained and tested 
on 441 combinations of gamma and C. In test-
ing, the probability threshold parameter p be-
tween [0.05, 0.95] was also chosen, and the F1-
scores of all combination of gamma, C, and p 
were calculated with five-fold cross validation. 
The best F1-scores and their parameter values 
for each combination of features (optimal F1-
scores in this setting) are portrayed in Table 4. 
The precision?recall distribution of F1-scores 
with feature combination C is presented in 
Figure 7.  
5.2 Experiment 2 
Experiment 2 aimed to observe the perfor-
mance of our extraction method when SVM 
parameters were automatically tuned. In this 
experiment, we performed two cross valida-
tions: a cross validation to tune SVM parame-
ters and another cross validation to evaluate 
the extraction method. The experiment condi-
tions are described below:  
A. Data 
The same data as Experiment 1 were used. 
B. Feature Combination  
Feature combination C, which performed best 
in Experiment 1, was used.  
C. Evaluation  
                                                 
3 http://www.csie.ntu.edu.tw/~cjlin/libsvm 
Two five-fold cross validations were per-
formed. The first cross validation divided the 
data to 5 sets (A, B, C, D, and E) each consist-
ing of development set and test set with the 
ratio of 4:1.  The second cross validation train 
and test all combination of SVM parameters (C, 
gamma, and p) in certain ranges and decide the 
optimal parameter combination(s) for  the de-
velopment sets of A, B, C, D, and E. The se-
cond cross validation denotes the execution of 
Experiment 1 for each development set.  For 
each optimal parameter combination of A, B, 
C, D, and E, the corresponding development 
set was trained and the trained model was test-
ed on the corresponding test set. The average 
F1-score on five test sets marked 34.90, which 
is 2.64 lower than the F1-score of Experiment 1 
with the same feature combination. 
6 Discussion 
The result of the experiment reveals the effec-
tiveness of the dependency chain feature and 
the case-frame chain feature. This section pre-
sents a description of the effects of several fea-
tures in detail. The section also mentions re-
maining problems in our extraction method.  
6.1 Effects of the Dependency Chain Fea-
ture and Case-frame Features  
A. Dependency Chain Feature  
The dependency chain features improved the 
F1-score by 3.59 (the F1-score difference be-
tween feature combination A and B). This in-
crease was obtained using 260 improved pairs 
and 127 deproved pairs. Improved pairs con-
Figure 8. Relation between the number of 
pairs and the morpheme distance. 
Figure 9. Number of dependency errors 
in the improved pairs sentences. 
25
93
23
sentence
with no error
sentence
with 1?3
errors
sentence
with 4 or
more errors
0
10
20
30
40
50
distance less 
than 40
distance larger than
or equal to 40
fre
qu
en
cy
improved 
deproved
81
tribute to the increase of a F1-score. Deproved 
pairs have the opposite effect. 
We observed that improved pairs tend to 
have longer morpheme distance compared to 
deproved pairs. Figure 8 shows the relation 
between the number of pairs and the mor-
pheme distance of improved pairs and de-
proved pairs. The ratio between the improved 
pairs and the deproved pairs is 11:1 when the 
distance is greater than 40.  In contrast, the 
ratio is 2:1 when the distance is smaller than 
40. This observation suggests that adverse?
effect relations share dependency structures to 
a certain degree.  
We also observed that in improved pairs, 
dependency errors tended to be low. Figure 9 
presents the manually counted number of de-
pendency errors in the 141 sentences in which 
the 260 improved pairs exist: 65.96 % of the 
sentences included 1?3 errors. The result sug-
gests that the dependency structure is effective 
even if it includes small errors.  
B. Case-frame Features  
The effect of the case-frame dependency chain 
feature differed with the effect of the depend-
ency chain feature. The case-frame chain fea-
ture improved the F1-score by 0.90 (the F1-
score difference between feature combination 
B and C), but the case-frame dependency chain 
feature decreased the F1-score by 0.36 (the F1-
score difference between feature combination 
C and E). One reason for the negative effect of 
the case-frame dependency feature might be 
feature sparsity, but no clear evidence of it has 
been found.  
6.2 Remaining Problems 
A. Imbalanced Data  
The adverse?effect relation pairs we used in 
the experiment were not balanced. Low values 
of optimal probability threshold parameter p 
suggest the degree of imbalance. We are con-
sidering introduction of some kind of method-
ology to reduce negative samples or to use a 
machine learning method that can accommo-
date imbalanced data well.  
B. Use of Medical Resources  
The extraction method we propose uses no 
medical resources. Girju et al (2007) indicate 
the effect of WordNet senses in the classifica-
tion of a semantic relation between nominals. 
Krallinger et al (2008) report that top scoring 
teams in the interaction pair subtask used so-
phisticated interactor protein normalization 
strategies. If medical terms in texts can be 
mapped to a medical terminology or ontology, 
it would likely improve the extraction accuracy.  
C. Fully Automated Extraction 
In the experiments, we used the manually 
annotated information to extract pairs and fea-
tures. This setting is, of course, not real if we 
consider a situation to extract adverse?effect 
relations from massive clinical records, but we 
chose it to focus on the relation extraction 
problem. We performed an event recognition 
experiment (Aramaki et al, 2009) and 
achieved F1-score of about 80. We assume that 
drug expressions and symptom expressions to 
be automatically recognized in a similar accu-
racy.  
We are planning to perform a fully automat-
ed adverse?effect relations extraction from a 
larger set of clinical texts to see the perfor-
mance of our method on a raw corpus. The 
extraction F1-score will likely to decrease, but 
we intend to observe the other aspect of the 
extraction, like the overall tendency of extract-
ed relations.  
7 Conclusion 
We presented a method to extract adverse?
effect relations from texts. One important 
characteristic of adverse?effect relations is that 
they are uncertain in most cases. We per-
formed experiments to extract adverse?effect 
relations from 2,577 clinical texts, and ob-
tained F1-score of 37.54 with optimal SVM 
parameters and F1-score of 34.90 with auto-
matically tuned SVM parameters. Results also 
show that dependency features increase the 
extraction F1-score by 3.59. We observed that 
an increased F1-score was obtained using the 
improvement of adverse?effects with long 
morpheme distance, which suggests that ad-
verse?effect relations share dependency struc-
tures to a certain degree. We also observed that 
the increase of the F1-score was obtained with 
dependency structures that include small errors, 
which suggests that the dependency structure 
is effective even if it includes small errors. 
  
82
References 
Aramaki, Eiji, Yasuhide Miura, Masatsugu Tonoike, 
Tomoko Ohkuma, Hiroshi Masuichi, and 
Kazuhiko Ohe. 2009. TEXT2TABLE: Medical 
Text Summarization System Based on Named 
Entity Recognition and Modality Identification. 
In Proceedings of the BioNLP 2009 Workshop, 
pages 185-192. 
Beamer, Brandon, Suma Bhat, Brant Chee, Andrew 
Fister, Alla Rozovskaya, and Roxana Girju. 
2007. UIUC: A Knowledge-rich Approach to 
Identifying Semantic Relations between Nomi-
nals. In Proceedings of Fourth International 
Workshop on Semantic Evaluations, pages 386-
389. 
Friedman, Carol, Philip O. Alderson, John H. M. 
Austin, James J. Cimino, and Stephen B. John-
son. 1994. A General Natural-language Text 
Processor for Clinical Radiology. Journal of the 
American Medical Informatics Association, 1(2), 
pages 161-174. 
Gildea, Daniel. 2001. Corpus Variation and Parser 
Performance. In Proceedings of the 2001 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 1-9. 
Girju, Roxana, Preslav Nakov, Vivi Nastase,  Stan 
Szpakowicz, Peter Turney, and Deniz Yuret.  
2007. SemEval-2007 task 04: Classification of 
Semantic Relations between Nominals. In Pro-
ceedings of Fourth International Workshop on 
Semantic Evaluations, pages 13-18. 
Giuliano, Claudio, Alberto Lavelli, Daniele Pighin, 
and Lorenza Romano. 2007. FBK-IRST: Kernel 
Methods for Semantic Relation Extraction. In 
Proceedings of the 4th International Workshop 
on Semantic Evaluations, pages 141-144.  
Hendrickx , Iris, Roser Morante, Caroline Sporleder, 
and Antal van den Bosch. 2007. ILK: Machine 
learning of semantic relations with shallow fea-
tures and almost no data. In Proceedings of the 
4th International Workshop on Semantic Evalua-
tions, 187-190. 
Kim, Jin-Dong, Tomoko Ohta, Sampo Pyysalo, 
Yoshinobu Kano, and Jun?ichi Tsujii. 2009. 
Overview of  BioNLP?09 Shared Task on Event 
Extraction. In Proceedings of the BioNLP 2009 
Workshop Companion Volume for Shared Task, 
pages 1-9. 
Krallinger, Martin, Florian Leitner, Carlos  
Rodriguez-Penagos, and Alfonso Valencia. 2008.  
Overview of the protein-protein interaction an-
notation extraction task of BioCreative II. Ge-
nome Biology 2008, 9(Suppl 2):S4. 
Kurohashi, Sadao and Makoto Nagao. 1994. KN 
Parser : Japanese Dependency/Case Structure 
Analyzer. In Proceedings of The International 
Workshop on Sharable Natural Language Re-
sources, pages 22-28. Software available at 
http://www-lab25.kuee.kyoto-u.ac.jp/nl-
resource/knp-e.html. 
Light, Marc, Xin Ying Qiu, and Padmini Srinivasan. 
2004. The Language of Bioscience: Facts, Spec-
ulations, and Statements in Between. In Pro-
ceedings of HLT/NAACL 2004 Workshop: Bi-
oLINK 2004, Linking Biological Literature, On-
tologies and Databases, pages 17-24. 
Miyao, Yusuke, Rune S?tre, Kenji Sagae, Takuya 
Matsuzaki, and Jun'ichi Tsujii. 2008. Task-
oriented Evaluation of Syntactic Parsers and 
Their Representations. In Proceedings of the 
46th Annual Meeting of the Association for 
Computational Linguistics: Human Language 
Technologies, pages 46-54. 
Pantel, Patrick and Marco Pennacchiotti. 2006. Es-
presso: Leveraging Generic Patterns for Auto-
matically Harvesting Semantic Relations. In 
Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th An-
nual Meeting of the Association for Computa-
tional Linguistics, pages 113-120. 
Riedel, Sebastian, Hong-Woo Chun, Toshihisa 
Takagi, and Jun'ichi Tsujii. 2009. A Markov 
Logic Approach to Bio-Molecular Event Extrac-
tion. In Proceedings of the BioNLP 2009 Work-
shop Companion Volume for Shared Task, pages 
41-49. 
Saeger, Stijn De, Kentaro Torisawa, and Jun?ichi 
Kazama. 2008. Looking for Trouble. In Proceed-
ings of the 22nd International Conference on 
Computational Linguistics, pages 185-192. 
Vapnik, Vladimir N.. 1995. The Nature of Statisti-
cal Learning Theory. Springer-Verlag New York, 
Inc.. 
Vincze, Veronika, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra, and J?nos Csirik. 2008.  The Bio-
Scope corpus: biomedical texts annotated for un-
certainty, negation and their scopes. BMC Bioin-
formatics 2008, 9(Suppl 11):S9.  
 
83
