CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 1?8
Manchester, August 2008
Semantic Parsing for High-Precision Semantic Role Labelling
Paola Merlo
Linguistics Department
University of Geneva
5 rue de Candolle
1211 Gen`eve 4 Switzerland
merlo@lettres.unige.ch
Gabriele Musillo
Depts of Linguistics and Computer Science
University of Geneva
5 Rue de Candolle
1211 Gen`eve 4 Switzerland
musillo4@etu.unige.ch
Abstract
In this paper, we report experiments that
explore learning of syntactic and seman-
tic representations. First, we extend a
state-of-the-art statistical parser to pro-
duce a richly annotated tree that identi-
fies and labels nodes with semantic role la-
bels as well as syntactic labels. Secondly,
we explore rule-based and learning tech-
niques to extract predicate-argument struc-
tures from this enriched output. The learn-
ing method is competitive with previous
single-system proposals for semantic role
labelling, yields the best reported preci-
sion, and produces a rich output. In com-
bination with other high recall systems it
yields an F-measure of 81%.
1 Introduction
In statistical natural language processing, consid-
erable ingenuity and insight have been devoted to
developing models of syntactic information, such
as statistical parsers and taggers. Successes in
these syntactic tasks have recently paved the way
to applying novel statistical learning techniques
to levels of semantic representation, such as re-
covering the logical form of a sentence for in-
formation extraction and question-answering ap-
plications (Miller et al, 2000; Ge and Mooney,
2005; Zettlemoyer and Collins, 2007; Wong and
Mooney, 2007).
In this paper, we also focus our interest on learn-
ing semantic information. Differently from other
work that has focussed on logical form, however,
we explore the problem of recovering the syn-
tactic structure of the sentence, the propositional
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
argument-structure of its main predicates, and the
substantive labels assigned to the arguments in the
propositional structure, the semantic roles. This
rich output can be useful for information extrac-
tion and question-answering, but also for anaphora
resolution and other tasks for which the structural
information provided by full syntactic parsing is
necessary.
The task of semantic role labelling (SRL), as has
been defined by previous researchers (Gildea and
Jurafsky, 2002), requires collecting all the argu-
ments that together with a verb form a predicate-
argument structure. In most previous work, the
task has been decomposed into the argument iden-
tification and argument labelling subtasks: first the
arguments of each specific verb in the sentence are
identified by classifying constituents in the sen-
tence as arguments or not arguments. The argu-
ments are then labelled in a second step.
We propose to produce the rich syntactic-
semantic output in two steps, which are different
from the argument identification and argument la-
belling subtasks. First, we generate trees that bear
both syntactic and semantic annotation, such as
those in Figure 1. The parse tree, however, does
not explicitly encode information about predicate-
argument structure, because it does not explicitly
associate each semantic role to the verb that gov-
erns it. So, our second step consists in recovering
the predicate-argument structure of each verb by
gleaning this information in an already richly dec-
orated tree.
There are linguistic and computational reasons
to think that we can solve the joint problem of
recovering the constituent structure of a sentence
and its lexical semantics. From a linguistic point
of view, the assumption that syntactic distributions
will be predictive of semantic role assignments is
based on linking theory (Levin, 1986). Linking
theory assumes the existence of a hierarchy of se-
1
mantic roles which are mapped by default on a
hierarchy of grammatical functions and syntactic
positions, and it attempts to predict the mapping
of the underlying semantic component of a predi-
cate?s meaning onto the syntactic structure. For ex-
ample, Agents are always mapped in syntactically
higher positions than Themes. From a computa-
tional point of view, if the internal semantics of a
predicate determines the syntactic expressions of
constituents bearing a semantic role, it is then rea-
sonable to expect that knowledge about semantic
roles in a sentence will be informative of its syn-
tactic structure. It follows rather naturally that se-
mantic and syntactic parsing can be integrated into
a single complex task.
Our proposal also addresses the problem of se-
mantic role labelling from a slightly different per-
spective. We identify and label argument nodes
first, while parsing, and we group them in a
predicate-argument structure in a second step. Our
experiments investigate some of the effects that re-
sult from organising the task of semantic role la-
belling in this way, and the usefulness of some
novel features defined on syntactic trees.
In the remainder of the paper, we first illustrate
the data and the graphical model that formalise the
architecture used and its extension for semantic
parsing. We then report on two kinds of exper-
iments: we first evaluate the architecture on the
joint task of syntactic and semantic parsing and
then evaluate the joint approach on the task of se-
mantic role labelling. We conclude with a discus-
sion which highlights the practical and theoretical
contribution of this work.
2 The Data
Our experiments on joint syntactic and semantic
parsing use data that is produced automatically by
merging the Penn Treebank (PTB) with PropBank
(PRBK) (Marcus et al, 1993; Palmer et al, 2005),
as shown in Figure 1. PropBank encodes proposi-
tional information by adding a layer of argument
structure annotation to the syntactic structures of
the Penn Treebank.
1
Verbal predicates in the Penn
Treebank (PTB) receive a label REL and their ar-
guments are annotated with abstract semantic role
labels, such as A0, A1, or AA for those comple-
ments of the predicative verb that are considered
arguments. Those complements of the verb la-
1
We use PRBK data as they appear in the CONLL 2005
shared task.
S
NP-A0
the authority
VP
VBD-REL
dropped
PP-TMP
IN(TMP)
at
NP
NN
midnight
PP-DIR
TO(DIR)
to
NP
QP
$ 2.80 trillion
Figure 1: A sample syntactic structure with seman-
tic role labels.
belled with a semantic functional label in the orig-
inal PTB receive the composite semantic role label
AM-X , where X stands for labels such as LOC,
TMP or ADV, for locative, temporal and adverbial
modifiers respectively. A tree structure with Prop-
Bank labels is shown in Figure 1. (The bold labels
are not relevant for the moment and they will be
explained later.)
3 The Syntactic and Semantic Parser
Architecture
To achieve the complex task of joint syntactic and
semantic parsing, we extend a current state-of-the-
art statistical parser (Titov and Henderson, 2007)
to learn semantic role annotation as well as syntac-
tic structure. The parser uses a form of left-corner
parsing strategy to map parse trees to sequences of
derivation steps.
We choose this parser because it exhibits the
best performance for a single generative parser,
and does not impose hard independence assump-
tions. It is therefore promising for extensions
to new tasks. Following (Titov and Henderson,
2007), we describe the original parsing architec-
ture and our modifications to it as a Dynamic
Bayesian network. Our description is brief and
limited to the few aspects of interest here. For
more detail, explanations and experiments see
(Titov and Henderson, 2007). A Bayesian network
is a directed acyclic graph that illustrates the statis-
tical dependencies between the random variables
describing a set of events (Jensen, 2001). Dy-
namic networks are Bayesian networks applied to
unboundedly long sequences. They are an appro-
priate model for sequences of derivation steps in
2
dtk
St?1 t
sit
t?cD Dt?1
t?cS S
Dt
Figure 2: The pattern on connectivity and the latent
vectors of variables in an Incremental Bayesian
Network.
parsing (Titov and Henderson, 2007).
Figure 2 illustrates visually the main properties
that are of relevance for us in this parsing architec-
ture. Let T be a parse tree and D
1
, . . . , D
m
be the
sequence of parsing decisions that has led to the
building of this parse tree. Let alo each parsing
decision be composed of smaller parsing decisions
d
1
1
, . . . , d
1
k
, and let al these decisions be indepen-
dent. Then,
P (T ) = P (D
1
, . . . , D
m
)
=
?
t
P (D
t
|D
1
, . . . , D
t?1
)
=
?
t
?
k
P (d
t
k
|h(t, k))
(1)
where h(t, k) denotes the parse history for sub-
decision d
t
k
.
The figure represents a small portion of the ob-
served sequence of decisions that constitute the re-
covery of a parse tree, indicated by the observed
states D
i
. Specifically, it illustrates the pattern of
connectivity for decision d
t
k
. As can be seen the re-
lationship between different probabilistic parsing
decisions are not Markovian, nor do the decisions
influence each other directly. Past decisions can in-
fluence the current decision through state vectors
of independent latent variables, referred to as S
i
.
These state vectors encode the probability distri-
butions of features of the history of parsing steps
(the features are indicated by s
t
i
in Figure 2).
As can be seen from the picture, the pattern
of inter-connectivity allows previous non-adjacent
states to influence future states. Not all states
in the history are relevant, however. The inter-
connectivity is defined dynamically based on the
topological structure and the labels of the tree that
is being developed. This inter-connectivity de-
pends on a notion of structural locality (Hender-
son, 2003; Musillo and Merlo, 2006).
2
2
Specifically, the conditioning states are based on the
In order to extend this model to learn decisions
concerning a joint syntactic-semantic representa-
tion, the semantic information needs to be high-
lighted in the model in several ways. We modify
the network connectivity, and bias the learner.
First, we take advantage of the network?s dy-
namic connectivity to highlight the portion of the
tree that bears semantic information. We augment
the nodes that can influence parsing decisions at
the current state by explicitly adding the vectors
of latent variables related to the most recent child
bearing a semantic role label of either type (REL,
A0 to A5 or AM-X) to the connectivity of the
current decision. These additions yield a model
that is sensitive to regularities in structurally de-
fined sequences of nodes bearing semantic role la-
bels, within and across constituents. These exten-
sions enlarge the locality domain over which de-
pendencies between predicates bearing the REL
label, arguments bearing an A0-A5 label, and ad-
juncts bearing an AM-X role can be specified, and
capture both linear and hierarchical constraints be-
tween predicates, arguments and adjuncts. Enlarg-
ing the locality domain this way ensures for in-
stance that the derivation of the role DIR in Figure
1 is not independent of the derivations of the roles
TMP, REL (the predicate) and A0.
Second, this version of the Bayesian network
tags its sentences internally. Following (Musillo
and Merlo, 2005), we split some part-of-speech
tags into tags marked with semantic role labels.
The semantic role labels attached to a non-terminal
directly projected by a preterminal and belonging
to a few selected categories (DIR, EXT, LOC, MNR,
PRP, CAUS or TMP) are propagated down to the
pre-terminal part-of-speech tag of its head.
3
This
third extension biases the parser to learn the rela-
tionship between lexical items, semantic roles and
the constituents in which they occur. This tech-
nique is illustrated by the bold labels in Figure 1.
We compare this augmented model to a sim-
ple baseline parser, that does not present any of
the task-specific enhancements described above,
stack configuration of the left-corner parser and the derivation
tree built so far. The nodes in the partially built tree and stack
configuration that are selected to determine the relevant states
are the following: top, the node on top of the pushdown stack
before the current derivation move; the left-corner ancestor of
top (that is, the second top-most node on the parser stack);
the leftmost child of top; and the most recent child of top, if
any.
3
Exploratory data analysis indicates that these tags are the
most useful to disambiguate parsing decisions.
3
PTB/PRBK 24
P R F
Baseline 79.6 78.6 79.1
ST 80.5 79.4 79.9
ST+ EC 81.6 80.3 81.0
Table 1: Percentage F-measure (F), recall (R), and
precision (P) of our joint syntactic and semantic
parser on merged development PTB/PRBK data
(section 24). Legend of models: ST=Split Tags;
EC=enhanced connectivity.
other than being able to use the complex syntactic-
semantic labels. Our augmented model has a to-
tal of 613 non-terminals to represent both the PTB
and PropBank labels of constituents, instead of the
33 of the original syntactic parser. The 580 newly
introduced labels consist of a standard PTB label
followed by a set of one or more PropBank seman-
tic role such as PP-AM-TMP or NP-A0-A1. As a
result of lowering the six AM-X semantic role la-
bels, 240 new part-of-speech tags were introduced
to partition the original tag set which consisted
of 45 tags. As already mentioned, argumental la-
bels A0-A5 are specific to a given verb or a given
verb sense, thus their distribution is highly vari-
able. To reduce variability, we add the tag-verb
pairs licensing these argumental labels to the vo-
cabulary of our model. We reach a total of 4970
tag-word pairs. These pairs include, among oth-
ers, all the tag-verb pairs occuring at least 10 times
in the training data. In this very limited form of
lexicalisation, all other words are considered un-
known.
4 Parsing Experiments
Our extended joint syntactic and semantic parser
was trained on sections 2-21 and validated on sec-
tion 24 from the merged PTB/PropBank. To eval-
uate the joint syntactic and semantic parsing task,
we compute the standard Parseval measures of la-
belled recall and precision of constituents, taking
into account not only the original PTB labels, but
also the newly introduced PropBank labels. This
evaluation gives us an indication of how accurately
and exhaustively we can recover this richer set of
syntactic and semantic labels. The results, com-
puted on the development data set from section 24
of the PTB with added PropBank annotation, are
shown in Table 1. As the table indicates, both the
enhancements based on semantic roles yield an im-
provement on the baseline.
This task enables us to compare, albeit indi-
rectly, our integrated method to other methods
where semantic role labels are learnt separately
from syntactic structure. (Musillo and Merlo,
2006) report results of a merging technique where
the output of the semantic role annotation pro-
duced by the best semantic role labellers in the
2005 CONLL shared task is merged with the out-
put of Charniak?s parser. Results range between
between 82.7% and 83.4% F-measure. Our inte-
grated method almost reaches this level of perfor-
mance.
The performance of the parser on the syntactic
labels only (note reported in Table 1) is slightly de-
graded in comparison to the original syntax-only
architecture (Henderson, 2003), which reported
an F-measure of 89.1% since we reach 88.4% F-
measure for the best syntactic-semantic model (last
line of Table 1). This level of performance is still
comparable to other syntactic parsers often used
for extraction of semantic role features (88.2% F-
measure) (Collins, 1999).
These results indicate that the extended parser is
able to recover both syntactic and semantic labels
in a fully connected parse tree. While it is true that
the full fine-grained interpretation of the semantic
label is verb-specific, the PropBank labels (A0,A1,
etc) do respect some general trends. A0 labels are
assigned to the most agentive of the arguments,
while A1 labels tend to be assigned to arguments
bearing a Theme role, and A2, A3, A4 and A5 la-
bels are assigned to indirect object roles, while all
the AM-X labels tend to be assigned to adjuncts.
The fact that the parser learns these labels with-
out explicit annotation of the link between the ar-
guments and the predicate to which they are as-
signed, but based on the smoothed representation
of the derivation of the parse tree and only very
limited lexicalisation, appears to confirm linking
theory, which assumes a correlation between the
syntactic configuration of a sentence and the lexi-
cal semantic labels.
We need to show now that the quality of the
output produced by the joint syntactic and seman-
tic parsing is such that it can be used to perform
other tasks where semantic role information is cru-
cial. The most directly related task is semantic role
labelling (SRL) as defined in the shared task of
CoNLL 2005.
4
5 Extraction of Predicate-Argument
Structures
Although there is reason to think that the good
performance reported in the previous section is
due to implicit learning of the relationship of the
syntactic representation and the semantic role as-
signments, the output produced by the parser does
not explicitly encode the predicate-argument struc-
tures. Collecting these associations is required to
solve the semantic role labelling task as usually de-
fined. We experimented with two methods: a sim-
ple rule-based method and a more complex learn-
ing method.
5.1 The rule-based method
The rule-based extraction method is the natural
second step to solve the complete semantic role
labelling task, after we identify and label seman-
tic roles while parsing. Since in our proposal, we
solve most of the problem in the first step, then we
should be able to collect the predicate-argument
pairs by simple, deterministic rules. The simplic-
ity of the method also provides a useful compari-
son for more complex learning methods, which can
be justified only if they perform better than simple
rule-based predicate-argument extraction.
Our rule-based method automatically compiles
finite-state automatata defining the paths that con-
nect the first node dominating a predicate to its
semantic roles from parse trees enriched with se-
mantic role labels.
4
Such paths can then be used to
traverse parse trees returned by the parsing model
and collect argument structures. More specifically,
a sample of sentences are randomly selected from
the training section of the PTB/PRBK. For each
predicate, then, all the arguments left and right of
the predicate and all the adjuncts left and right
respectively are collected and filtered by simple
global constraints, thereby guaranteeing that only
one type of obligatory argument label (A0 to A5)
is assigned in each proposition.
When evaluated on gold data, this rule-based ex-
traction method reaches 94.9% precision, 96.9%
recall, for an F-measure of 95.9%. These results
provide an upper bound as well as indicating that,
while not perfect, the simple extraction rules reach
a very good level of correctness if the input from
the first step, syntactic and semantic parsing, is
correct. The performance is much lower when
4
It uses VanNoord?s finite-state-toolkit
http://www.let.rug.nl/ vannoord/Fsa/.
parses are not entirely correct, and semantic role
labels are missing, as indicated by the results of
72.9% precision, 66.7% (F-measure 69.7%), ob-
tained when using the best automatic parse tree.
The fact that performance depends on the qual-
ity of the output of the first step, indicates that
the extraction rules are sensitive to errors in the
parse trees, as well as errors in the labelling. This
indicates that a learning method might be more
adapted to recover from these mistakes.
5.2 The SVM learning method
In a different approach to extract predicate argu-
ment structures from the parsing output, the sec-
ond step learns to associate the right verb to each
semantically annotated node (srn) in the tree pro-
duced in the first step. Each individual (verb, srn)
pair in the tree is either a positive example (the srn
is a member of the verb?s argument structure) or a
negative example (the argument either should not
have been labelled as an argument or it is not as-
sociated to the verb). The training examples are
produced by parsing section 2-21 of the merged
PTB/PRBK data with the joint syntactic-semantic
parser and producing the training examples by
comparison with the CONLL 2005 gold proposi-
tions. There are approximately 800?000 training
examples in total. These examples are used by
an SVM classifier (Joachims, 1999).
5
. Once the
predicate-argument structures are built, they are
evaluated with the CONLL 2005 shared task cri-
teria.
5.3 The learning features
The features used for the extraction of the
predicate-argument structure reflect the syntactic
properties that are useful to identify the arguments
of a given verb. We use syntactic and semantic
node label, the path between the verb and the argu-
ment, and the part-of-speech tag of the verb, which
provides useful information about the tense of the
verb. We also use novel features that encode min-
imality conditions and locality constraints (Rizzi,
1990). Minimality is a typical property of natu-
ral languages that is attested in several domains.
In recovering predicate-argument structures, mini-
mality guarantees that the arguments are related to
the closest verb in a predicate domain, which is not
always the verb to which they are connected by the
5
We use a radial basis function kernel, where parameters
c and ? were determined by a grid search on a small subset of
2000 training examples. They are set at c=8 and ? = 0.03125.
5
shortest path. For example, the subject of an em-
bedded clause can be closer to the verb of the main
clause than to the predicate to which it should be
attached. Minimality is encoded as a binary feature
that indicates whether a verbw intervenes between
the verb v and the candidate argument srn. Mini-
mality is defined both in terms of linear precedence
(indicated below as ?) and of dominance within
the same VP group. A VP group is a stack of VPs
covering the same compound verb group, such as
[
V P
should [
V P
have [
V P
[
V
come ]]]]. Formal
definitions are given below:
minimal(v, srn, w) =
df
8
<
:
false if (v ? w ? srn or srn ? w ? v) and
VPG-dominates(v, srn, w)
true otherwise
VPG-dominates(v, srn, w) =
df
8
<
:
true if VP ? path(v, srn) and
VP ? VP-group directly dominating w
false otherwise
In addition to the minimality conditions, which
resolve ambiguity when two predicates compete to
govern an argument, we use locality constraints to
capture distinct local relationships between a verb
and the syntactic position occupied by a candidate
argument. In particular, we distinguish between in-
ternal arguments occupying a position dominated
by a VP node, external arguments occupying a
position dominated by an S node, and extracted
arguments occupying a position dominated by an
SBAR node. To approximate such structural dis-
tinctions, we introduce two binary features indicat-
ing, respectively, whether there is a a node labelled
S or SBAR on the path connecting the verb and the
candidate argument.
6 Results and Discussion
Table 2 illustrates our results on semantic role la-
belling. Notice how much more precise the learn-
ing method is than the rule-based method, when
the minimality constraint is added. The second and
third line indicate that this contribution is mostly
due to the minimality feature. The fifth and sixth
line however illustrate that these features together
are more useful than the widely used feature path.
Recall however, suffers in the learnt method. Over-
all, the learnt method is better than a rule-based
method only if path and either minimality or lo-
cality constraints are added, thus suggesting that
Prec Rec F
Learning all features 87.4 63.6 73.7
Learning all ?min 75.4 66.2 70.5
Learning all ?loc 87.4 63.6 73.6
Rule-based 72.9 66.7 69.7
Learning all ?path 80.6 60.9 69.4
Learning all ?min ?loc 74.3 63.8 68.6
Baseline 57.4 53.9 55.6
Table 2: Results on the development section (24),
rule-based, and learning, (with all features, and
without path, minimality and locality constraints)
compared to a closest verb baseline.
the choice of features is crucial to reach a level
of performance that justifies the added complex-
ity of a learning method. Both methods are much
better than a baseline that attaches each role to
a verb by the shortest path.
6
Notice that both
these approaches are not lexicalised, they apply to
all verbs. Learning experiments where the actual
verbs were used showed a little degradation as well
as a very considerable increase in training times
(precision: 87.0%; recall: 61.0%; F: 71.7%).
7
Some comments are in order to compare prop-
erly our best results ? the learning method with
all features ? to other methods. Most of the best
performing SRL systems are ensemble learners or
rerankers, or they use external sources of infor-
mation such as the PropBank frames files. While
these techniques are effective to improve classifi-
cation accuracy, we might want to compare the sin-
gle systems, thus teasing apart the contribution of
the features and the model from the contribution
of the ensemble technique. Table 3 reports the sin-
gle systems? performance on the test set. These re-
sults seem to indicate that methods like ours, based
on a first step of PropBank parsing, are compara-
ble to other methods when learning regimes are
factored out, contrary to pessimistic conclusions
in previous work (Yi and Palmer, 2005). (Yi and
Palmer, 2005) share the motivation of our work.
They observe that the distributions of semantic la-
6
In case of tie, the following verb is chosen for an A0 label
and the preceding verb is chosen for all the other labels.
7
We should notice that all these models encode the feature
path as syntactic path, because in exploratory data analysis we
found that this feature performed quite a bit better than path
encoded taking into account the semantic roles assigned to the
nodes on the path. Concerning the learning model, we notice
that a simpler, and much faster to train, linear SVM classifier
performs almost as well as the more complex RBF classifier.
It is therefore preferable if speed is important.
6
Model CONLL 23 Comments
P R F
(Surdeanu and Turmo, 2005) 80.3 73.0 76.5 Propbank frames to filter output, boosting
(Liu et al, 2005) 80.5 72.8 76.4 Single system + simple post-processing
(Moschitti et al, 2005) 76.6 75.2 75.9 Specialised kernels for each kind of role
This paper 87.6 65.8 75.1 Single system and model, locality features
(Ozgencil and McCracken, 2005) 74.7 74.2 74.4 Simple system, no external knowledge
(Johansson and Nugues, 2005) 75.5 73.2 74.3 Uses only 3 sections for training
Table 3: Final Semantic Role Labelling results on test section 23 of Propbank as encoded in the CONLL
shared task for those CONLL 2005 participants not using ensemble learning or external resources.
bels could potentially interact with the distribu-
tions of syntactic labels and redefine the bound-
aries of constituents, thus yielding trees that reflect
generalisations over both these sources of infor-
mation. They also attempt to assign SRL while
parsing, by merging only the first two steps of
the standard pipeline architecture, pruning and ar-
gument identification. Their parser outputs a bi-
nary argument-nonargument distinction. The ac-
tual fine-grained labelling is performed, as in other
methods, by an ensemble classifier. Results are
not among the best and Yi and Palmer conclude
that PropBank parsing is too difficult and suffers
from differences between chunk annotation and
tree structure. We think instead that the method is
promising, as shown by the results reported here,
once the different factors that affect performance
are teased apart.
Some qualitative observations on the errors are
useful. On the one hand, as can be noticed in Table
3, our learning method yields the best precision,
but often the worse recall and it has the most ex-
treme difference between these two scores.
8
This
is very likely to be a consequence of the method.
Since the assignment of the semantic role labels
proper is performed during parsing, the number
of nodes that require a semantic role is only 20%
of the total. Therefore the parser develops a bias
against assigning these roles in general, and recall
suffers.
9
On the other hand, precision is very good,
thanks to the rich context in which the roles are as-
signed.
This property of our method suggests that com-
bining our results with those of other existing se-
8
This observation applies also in a comparison to the other
systems that participated in the CONLL shared task.
9
The SVM classifier, on the other hand, exceeds 94% in
accuracy and its F measures are situated around 87?88% de-
pending on the feature sets.
mantic role labellers might be beneficial, since the
errors it performs are quite different. We tested
this hypothesis by combining our outputs, which
are the most precise, with the outputs of the sys-
tem that reported the best recall (Haghighi et al,
2005). The combination, performed on sections
24 and 23, gives priority to our system when it
outputs a non-null label (because of its high pre-
cision) and uses the other system?s label when our
system outputs a null label. This combination pro-
duces a result of 79.0% precision, 80.4% recall,
and 79.7% F-measure for section 24, and 80.5%
precision, 81.4% recall, and 81.0% F-measure for
section 23. We conclude that the combination is in-
deed able to exploit the positive aspects of both ap-
proaches, as the F-measure of the combined result
is better than each individual result. It is also the
best compared to the other systems of the CoNLL
shared task. Comparatively, we find that applying
the same combination technique to the output of
the system by (Haghighi et al, 2005) with the out-
put of the best system in the CoNLL 2005 shared
task (Punyakanok et al, 2005) yields combined
outputs that are not as good as the better of the
two systems (P:76.3%; R:78.6%; F:77.4% for sec-
tion 24; P:78.5%; R:80.0%; F:79.3% for section
23). This result confirms our initial hypothesis,
that combination of systems with different perfor-
mance characteristics yields greater improvement.
Another direct consequence of assigning roles
in a rich context is that in collecting arguments for
a given verb we hardly need to verify global con-
straints. Differently from previous work that had
found that global coherence constraints consider-
ably improved performance (Punyakanok et al,
2005), using global filtering contraints showed no
improvement in our learning model. Thus, these
results confirm the observations that a verb does
7
not assign its semantic roles independently of each
other (Haghighi et al, 2005). Our method too can
be seen as a way of formulating the SRL problem
in a way that is not simply classification of each in-
stance independently. Because identification of ar-
guments and their labelling is done while parsing,
the parsing history, both syntactic and semantic,
is taken into account in identifying and labelling
an argument. Semantic role labelling is integrated
in structured sequence prediction. Further integra-
tion of semantic role labelling in structured prob-
abilistic models related to the one described here
has recently been shown to result in accurate syn-
chronous parsers that derive both syntactic and se-
mantic dependency representations (Henderson et
al., 2008).
7 Conclusion
Overall our experiments indicate that an inte-
grated approach to identification and labelling fol-
lowed by predicate-argument recovery can solve
the problem of semantic role labelling at a level
of performance comparable to other approaches,
as well as yielding a richly decorated syntactic-
semantic parse tree. The high precision of our
method yields very good results in combination
with other high-recall systems. Its shortcomings
indicates that future work lies in improving recall.
Acknowledgments
We thank the Swiss NSF for supporting this research under
grant number 101411-105286/1, James Henderson for shar-
ing the SSN software, and Xavier Carreras for providing the
CoNLL-2005 data. Part of this work was completed while
the second author was visiting MIT/CSAIL, hosted by Prof.
Michael Collins.
References
Collins, Michael John. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University of
Pennsylvania.
Ge, Ruifang and Raymond J. Mooney. 2005. A statistical
semantic parser that integrates syntax and semantics. In
Procs of CONLL-05, Ann Arbor, Michigan.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguistics,
28(3):245?288.
Haghighi, Aria, Kristina Toutanova, and Christopher Man-
ning. 2005. A joint model for semantic role labeling. In
Procs of CoNLL-2005, pages 173?176, Ann Arbor, Michi-
gan.
Henderson, Jamie. 2003. Inducing history representations
for broad-coverage statistical parsing. In Procs of NAACL-
HLT?03, pages 103?110, Edmonton, Canada.
Henderson, James, Paola Merlo, Gabriele Musillo and Ivan
Titov. 2008. A latent variable model of synchronous pars-
ing for syntactic and semantic dependencies. In Procs of
CoNLL?08 Shared Task, Manchester, UK.
Jensen, Finn V. 2001. Bayesian networks and decision
graphs. Springer Verlag.
Joachims, Thorsten. 1999. Making large-scale svm learning
practical. In Schlkopf, B., C. Burges, and A. Smola, edi-
tors, Advances in Kernel Methods - Support Vector Learn-
ing. MIT Press.
Johansson, Richard and Pierre Nugues. 2005. Sparse
bayesian classification of predicate arguments. In Procs
of CoNLL-2005, pages 177?180, Ann Arbor, Michigan.
Levin, Lori. 1986. Operations on lexical form: unaccusative
rules in Germanic languages. Ph.D. thesis, Massachus-
setts Institute of Technology.
Liu, Ting, Wanxiang Che, Sheng Li, Yuxuan Hu, and Huaijun
Liu. 2005. Semantic role labeling system using maximum
entropy classifier. In Procs of CoNLL-2005, pages 189?
192, Ann Arbor, Michigan.
Marcus, Mitch, Beatrice Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of English: the
Penn Treebank. Computational Linguistics, 19:313?330.
Miller, S., H. Fox, L. Ramshaw, and R. Weischedel. 2000. A
novel use of statistical parsing to extract information from
text. In Procs of NAACL 2000.
Moschitti, Alessandro, Ana-Maria Giuglea, Bonaventura
Coppola, and Roberto Basili. 2005. Hierarchical semantic
role labeling. In Procs of CoNLL-2005, pages 201?204,
Ann Arbor, Michigan.
Musillo, Gabriele and Paola Merlo. 2005. Lexical and struc-
tural biases for function parsing. In Procs of IWPT?05,
pages 83?92, Vancouver, British Columbia, October.
Musillo, Gabriele and Paola Merlo. 2006. Accurate semantic
parsing of the proposition bank. In Procs of NAACL?06,
New York, NY.
Ozgencil, Necati Ercan and Nancy McCracken. 2005. Se-
mantic role labeling using libSVM. In Procs of CoNLL-
2005, pages 205?208, Ann Arbor, Michigan, June.
Palmer, Martha, Daniel Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31:71?105.
Punyakanok, Vasin, Peter Koomen, Dan Roth, and Wen tau
Yih. 2005. Generalized inference with multiple seman-
tic role labeling systems. In Procs of CoNLL-2005, Ann
Arbor, MI USA.
Rizzi, Luigi. 1990. Relativized minimality. MIT Press, Cam-
bridge, MA.
Surdeanu, Mihai and Jordi Turmo. 2005. Semantic role
labeling using complete syntactic analysis. In Procs of
CoNLL?05, Ann Arbor, Michigan.
Titov, Ivan and James Henderson. 2007. Constituent parsing
with Incremental Sigmoid Belief Networks. In Procs of
ACL?07, pages 632?639, Prague, Czech Republic.
Wong, Yuk Wah and Raymond Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda cal-
culus. In Procs of ACL?07, pages 960?967, Prague, Czech
Republic.
Yi, Szu-ting and Martha Palmer. 2005. The integration of
semantic parsing and semantic role labelling. In Procs of
CoNLL?05, Ann Arbor, Michigan.
Zettlemoyer, Luke and Michael Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to logical form.
In Procs of EMNLP-CoNLL?07, pages 678?687.
8
