Proceedings of the 5th Workshop on Important Unresolved Matters, pages 73?80,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic composition with (Robust) Minimal Recursion Semantics
Ann Copestake
Computer Laboratory, University of Cambridge
JJ Thomson Avenue, Cambridge, UK
aac@cl.cam.ac.uk
Abstract
We discuss semantic composition in Mini-
mal Recursion Semantics (MRS) and Robust
Minimal Recursion Semantics (RMRS). We
demonstrate that a previously defined for-
mal algebra applies to grammar engineering
across a much greater range of frameworks
than was originally envisaged. We show
how this algebra can be adapted to compo-
sition in grammar frameworks where a lex-
icon is not assumed, and how this underlies
a practical implementation of semantic con-
struction for the RASP system.
1 Introduction
Minimal Recursion Semantics (MRS: Copestake et
al. (2005)) is a flat semantic representation which
factors semantics into elementary predications (EPs)
and allows for underspecification of scope. It has
been widely used, especially for HPSG. Robust
Minimal Recursion Semantics (RMRS: Copestake
(2003)) is a variant of MRS which takes this fac-
torisation further to allow underspecification of re-
lational information as well. While MRS has gen-
erally been used with hand-built HPSG grammars,
RMRS is also suitable for use with shallower ap-
proaches to analysis, including part-of-speech tag-
ging, noun phrase chunking and stochastic parsers
which operate without detailed lexicons. MRSs can
be converted into RMRSs: RMRS output from shal-
lower systems is less fully specified than the out-
put from deeper systems, but in principle fully com-
patible. In our work, the semantics produced by a
deep grammar is taken as normative when devel-
oping semantic representations from shallower pro-
cessing. For English, the target semantic represen-
tations are those produced by the English Resource
Grammar (ERG, Flickinger (2000)). The MRS/RMRS
approach has been adopted as a common framework
for the DELPH-IN initiative (Deep Linguistic Pro-
cessing with HPSG: http://www.delph-in.net).
An algebra for MRS was defined by Copestake et
al. (2001) (henceforth CLF) and forms the starting
point for the work reported here.
The aim of CLF was to formalise the notion of se-
mantic composition within grammars expressed in a
typed feature structure (TFS) logic. Here, we ex-
tend that work to non-lexicalist approaches and also
describe how the formal principles of composition
used in MRS can be adapted to produce a formalism
for RMRS composition. Thus we demonstrate that
the algebra applies to grammar engineering across
a much wider range of frameworks than was origi-
nally envisaged. Besides its theoretical interest, this
result has practical benefits when combining multi-
ple processing systems in that it allows compatible
semantic representations at a phrasal level as well as
at a sentence level.
The next section (?2) describes the most impor-
tant features of MRS, RMRS and the earlier work on
the algebra. We then outline how the algebra can
be used for implementing deep non-TFS approaches
(?3) and explain how it works with RMRS (?4). This
is followed by discussion of the extension to gram-
mars without a detailed lexicon (?5). To briefly illus-
trate the practical applications, section (?6) outlines
how RMRS semantics is constructed from RASP (Ro-
bust accurate domain-independent statistical pars-
ing: Briscoe and Carroll (2002)).
2 MRS, RMRS and the algebra
Details of MRS, RMRS and the algebra are given in
the cited papers, but we will briefly introduce them
here for convenience. Fig. 1 illustrates an MRS from
a deep grammar (based on the ERG output, but sim-
plified for expository purposes), an equivalent RMRS
and a very underspecified RMRS, derived from a
POS tagger.
MRS achieves a flat representation via the use of
labels on EPs, thus factoring out scopal relation-
ships. Scope constraints (HCONS) are shown as qeq
relationships (=q equality modulo quantifiers: the
73
MRS representation:
l0: the q(x0, h01, h02), l1: fat j(x1), l2: cat n(x2), l3: sit v 1(e3, x3), l4: on p(e4, e41, x4),
l5: a q(x5, h51, h52), l6: mat n 1(x6),
h01 =q l1, h51 =q l6
x0 = x1 = x2 = x3, e3 = e41, x4 = x5 = x6, l1 = l2, l3 = l4
RMRS equivalent to the MRS above:
l0: a0: the q(x0), l0: a0: RSTR(h01), l0: a0: BODY(h02), l1: a1: fat j(x1), l2: a2: cat n(x2),
l3: a3: sit v 1(e3), l3: a3: ARG1(x31), l4: a4: on p(e4, e41, x4), l4: a4: ARG1(e41), l4: a4: ARG2(x4),
l5: a5: a q(x5), l5: a5: RSTR(h51), l5: a5: BODY(h52), l6: a6: mat n 1(x6),
h01 =q l1, h51 =q l6
x0 = x1 = x2 = x3, e3 = e41, x4 = x5 = x6, l1 = l2, l3 = l4
Highly underspecified RMRS output:
l0: a0: the q(x0), l1: a1: fat j(x1), l2: a2: cat n(x2), l3: a3: sit v(e3), l4: a4: on p(e4),
l5: a5: a q(x5), l6: a6: mat n(x6)
Figure 1: MRS and RMRS for the fat cat sat on a mat
details are not important to understand this paper).
In MRS, implicit conjunction is indicated by equality
between labels. For instance, the labels on l1: fat(x)
and l2: cat1(x) are equated. In this figure, we show
MRS using explicit equalities (eqs: =) rather than
coindexation of variables since this corresponds to
the formalism used in the algebra.
RMRS uses the same approach to scope but
adopts a variant of a neo-Davidsonian representa-
tion, where arguments (ARGs) are represented as
distinct elements. In the very underspecified RMRS
at the bottom of Fig.1, no relational information is
known so there are no ARGs. Separating out ARGs
from the EPs and allowing them to be omitted per-
mits a straightforward notion of a specificity hierar-
chy in terms of information content. ARGs may also
be underspecified: e.g., ARGn indicates that there
is some argument relationship, but it is unknown
whether it is an ARG1, ARG2 or ARG3. In the
version of RMRS described in this paper, the ARGs
are related to the main EPs via an ?anchor? element.
An EP and its associated ARGs share a unique an-
chor. This version of RMRS uses exactly the same
mechanism for conjunction as does MRS: the anchor
elements are required so that ARGs can still be asso-
ciated with a single EP even if the label of the EP has
been equated with another EP. This is a change from
Copestake (2003): the reasons for this proposal are
discussed in ?4, below. The conjunction informa-
tion is not available from a POS tagger alone and so
is not present in the second RMRS in Fig.1.
The naming convention adopted for the relations
(e.g., sit v) allows them to be constructed without
access to a lexicon. ? v? etc are indications of the
coarse-grained sense distinctions which can be in-
ferred from part-of-speech information. Deep gram-
mars can produce finer-grained sense distinctions,
indicated by ? 1? etc, and there is an implicit hier-
archy such that sit v 1 is taken as being more spe-
cific than sit v. However, in what follows, we will
use simple relation names for readability. MRS and
RMRS both assume an inventory of features on vari-
ables which are used to represent tense etc, but these
will not be discussed in this paper.
2.1 The MRS algebra
In the algebra introduced by CLF, semantic struc-
tures (SEMENTS) for phrases consist of five parts:
1. Hooks: can be thought of as pointers into the
relations list. In a full grammar, hooks consist
of three parts: a label (l), an index (i) and an
external argument (omitted here for simplicity).
2. Slots: structures corresponding to syntac-
tic/semantic unsaturation ? they specify how
the semantics is combined. A slot in one sign is
instantiated by being equated with the hook of
another sign. (CLF use the term ?hole? instead
of ?slot?.) For the TFS grammars considered
in CLF, the slot corresponds to the part of the
TFS accessed via a valence feature. The inven-
tory of slot labels given by CLF is SUBJ, SPR,
SPEC, COMP1, COMP2, COMP3 and MOD.
74
3. rels: The bag of EPs.
4. hcons: qeq constraints (=q).
5. eqs: the variable equivalences which are the re-
sults of equating slots and hooks.
SEMENTs are: [l, i]{slots}[eps][hcons]{eqs}.
Some rules contribute their own semantics (con-
struction semantics: e.g., compound nouns). How-
ever, the MRS approach requires that this can al-
ways be treated as equivalent to having an additional
daughter in the rule. Thus construction semantics
need not be considered separately in the formal al-
gebra, although it does result in some syntactically
binary rules being semantically ternary (and so on).
The principles of composition are:
1. A (syntactically specified) slot in one structure
(the daughter which corresponds to the seman-
tic head) is filled by the hook of the other struc-
ture (by adding equalities).
2. The hook of the phrase is the semantic head?s
hook.
3. The eps of the phrase is equal to appending the
eps of the daughters.
4. The eqs of the phrase is equal to appending the
eqs of the daughters plus any eqs contributed
by the filling of the slot.
5. The slots of the phrase are the unfilled slots of
the daughters (although see below).
6. The hcons of the phrase is equal to appending
the hcons of the daughters.
Formally, the algebra is defined in terms of a se-
ries of binary operations, such as opspec, which
each correspond to the instantiation of a particular
labelled slot.
Fig. 2 illustrates this. The hook of cat instanti-
ates the SPEC slot of a, which is the semantic head
(though not the syntactic head in the ERG). This
leads to the equalities between the variables in the
result. Since the SPEC slot has been filled, it is not
carried up to the phrase. Thus, abstractly at least,
the semantics of the HPSG specifier-head rule cor-
responds to opspec.1
1As usual in MRS, in order to allow scope underspecifica-
tion, the label l4 of the quantifier?s hook is not coindexed with
any EP.
The MRS algebra was designed to abstract away
from the details of the syntax and of the syntax-
semantics interface, so that it can be applied to
grammars with differing feature geometry. The as-
sumption in CLF is simply that the syntax selects
the appropriate op and its arguments for each ap-
plication. i.e., semantic operations are associated
with HPSG constructions so that there is a mapping
from the daughters of the construction to the argu-
ments of the operation. The algebra does not attempt
to completely replicate all aspects of semantic con-
struction: e.g., the way that the features (represent-
ing tense and so on) are instantiated on variables is
not modelled. However, it does constrain semantic
construction compared with the possibilities for TFS
semantic compositional in general. For instance, as
discussed by CLF, it enforces a strong monotonic-
ity constraint. The algebra also contributes to limit-
ing the possibilities for specification of scope. These
properties can be exploited by algorithms that oper-
ate on MRS: e.g., generation, scope resolution.
2.2 The MRS algebra and the syntax-semantics
interface
CLF did not discuss the syntax-semantics interface
in detail, but we do so here for two reasons. Firstly,
it is a preliminary for discussing the use of the al-
gebra in frameworks other than HPSG in the fol-
lowing sections. Secondly, as CLF discuss, the con-
straints that the algebra imposes cannot be fully im-
plemented in a TFS. Thus, for grammar engineering
in TFS frameworks, an additional automatic checker
is needed to determine whether a grammar meets the
algebra?s constraints. This requires specification of
the syntax-semantics interface so that the checker
can extract the slots from the TFSs and determine
the slot operation(s) corresponding to a rule.
Unfortunately, CLF are imprecise about the alge-
bra in several respects. One problem is that they
gloss over the issue of slot propagation in real gram-
mars. CLF state that for an operation opx, the slot
corresponding to opx on the semantic head is instan-
tiated and all other slots appear on the result. For
instance, the definition of opspec states that for all
labels l 6= spec: slotl(opspec(a1, a2)) = slotl(a1)?
slotl(a2). However, this is inadequate for real gram-
mars, if a simple correspondence between the slot
names and the valence paths in the feature structure
75
hook slots rels eqs hcons
cat : [l1, x1] {} [l1 : cat(x1)] {} []
a : [l4, x2] {[l3, x2]spec} [l2 : a(x2, h2, h3)] {} [h2 =q l3]
a cat : [l4, x2] {} [l2 : a(x2, h2, h3), l1 : cat(x1)] {l3 = l1, x2 = x1} [h2 =q l3]
Figure 2: Example of the MRS algebra
is assumed. For instance, the passive rule involves
coindexing a COMP in the original lexical sign with
the SUBJ of the passive (informally, the complement
?becomes? the subject).
There are two ways round this problem. The first
is to keep the algebra unchanged, but to assume that,
for instance, the subject-head grammar rule corre-
sponds to opsubj in the algebra for non-passivized
cases and to opcomp1 for passives of simple tran-
sitives and so on. Though possible formally, this is
not in accord with the spirit of the approach since
selection of the appropriate algebra operation in the
syntax-semantics interface would require non-local
information. Practically, it also precludes the im-
plementation of an algebra checker, since keeping
track of the slot uses would be both complex and
grammar-specific. The alternative is to extend the
algebra to allow for slot renaming. For instance,
opcomp1-subj can be defined so that the COMP1 slot
on the daughter is a SUBJ slot on the mother.
1. For all labels l 6= comp1, l 6= subj:
slotl(opcomp1-subj(a)) = slotl(a)
2. slotsubj(opcomp1-subj(a)) = slotcomp1(a)
This means extending the inventory of operations,
but the choice of operation is then locally deter-
minable from the rule (e.g., the passive rule would
specify opcomp1-subj to be its operation).
Another issue arises in grammars which allow for
optional complements. For instance, one approach
to a verb like eat is to give it a single lexical en-
try which corresponds to both transitive and intran-
sitive uses. The complement is marked as optional
and the corresponding variable in the semantics is
assumed to be discourse bound if there is no syn-
tactic complement in the phrase. Optional comple-
ments can be discharged by a construction. This ap-
proach is (arguably) appropriate for eat because the
intransitive use involves an implicit patient (e.g., I
already ate means I already ate something), in con-
trast to a verb like kick. CLF do not discuss op-
tionality but it can be formalised in the algebra in
terms of a construction-specified sement which has
a hook containing the discourse referent and is oth-
erwise empty. For instance, an optional complement
construction corresponds to opcomp1(a1, a2) where
a1 is the head (and the only daughter appearing in
the TFS for the construction) and a2 is stipulated by
the rule to be [l, d]{}[][]{}, where d is the discourse-
bound referent.
3 The algebra in non-lexicalist grammars
CLF motivate the MRS algebra in terms of formalisa-
tion of the semantics of constraint-based grammars,
such as HPSG, but, as we outline here, it is equally
applicable to non-lexicalist frameworks. With a suit-
able definition of the syntax-semantics interface, the
algebra can be used with non-TFS-based grammars.
Fig. 3 sketches an example of MRS semantics for a
CFG. A syntax-semantic interface component of the
rule (shown in the second line of the figure) specifies
the ops and their daughters: the IOBJ slot of the verb
is instantiated with the first NP?s hook and the OBJ
slot of the result is instantiated with the hook of the
second NP. The idea is extremely similar to the use
of the algebra with TFS but note that with the ad-
dition of this syntax-semantic interface, the algebra
can be used directly to implement semantic compo-
sition for a CFG.
This still relies on the assumption that all slots
are known for every lexical item: semantically the
grammar is lexicalist even though it is not syntacti-
cally. In fact this is analogous to semantic compo-
sition in GPSG (Gazdar et al, 1985) in that conven-
tional lambda calculus also assumes that the seman-
tic properties are known at the lexical level.
4 RMRS composition with deep grammars
The use of the CLF algebra in RMRS composition
with deep lexicalist grammars is reasonably straight-
76
VP -> Vditrans NP1 NP2
opobj(opiobj(Vditrans, NP1), NP2)
MRSs for application of the rule to give a cat a rat.
hook slots rels eqs
give : [l1, e1] {[l1, x12]subj , [l1 : give(e1, x12, x13, x14)] {}
[l1, x13]obj , [l1, x14]iobj}
a cat : [l4, x2] {} [l2 : a(x2, h2, h3), l1 : cat(x1)] {l3 = l1, x2 = x1}
a rat : [l7, x5] {} [l5 : a(x5, h5, h6), l4 : rat(x4)] {l6 = l4, x5 = x4}
iobj : [l1, e1] {[l1, x12]subj , [l1, x13]obj} [l1 : give(e1, x12, x13, x14), {l3 = l1, x2 = x1,
l2 : a(x2, h2, h3), l1 : cat(x1)] l1 = l4, x14 = x2}
obj : [l1, e1] {[l1, x12]subj} [l1 : give(e1, x12, x13, x14), {l3 = l1, x2 = x1,
l2 : a(x2, h2, h3), l1 : cat(x1), l1 = l4, x14 = x2,
l5 : a(x5, h5, h6), l4 : rat(x4)] l1 = l7, x13 = x5}
Figure 3: MRS algebra with a CFG (hcons omitted for clarity)
forward.2 The differences between MRS and RMRS
are that RMRS uses anchors and factors out the
ARGs. Thus for RMRS, we need to redefine the no-
tion of a semantic entity from the MRS algebra to
add anchors. An RMRS EP thus contains:
1. a handle, which is the label of the EP
2. an anchor (a)
3. a relation
4. up to one argument of the relation
Hooks also include anchors: {[l, a, i]} is a hook.
Instead of the rels list only containing EPs, such
as l1:chase(e,x,y), it contains a mixture of EPs
and ARGs, with associated anchors, such as
l1:a1:chase(e), l1:a1:ARG1(x), l1:a1:ARG2(y). But
formally ARGs are EPs according to the definition
above, so this requires no amendment of the alge-
bra. Fig. 4 shows the RMRS version of Fig. 2.
As mentioned above, earlier forms of RMRS used
an explicit representation for conjunction: the in-
group, or in-g. Reasons to avoid explicit binary
conjunction were discussed with respect to MRS by
Copestake et al (2005) and readers are referred to
that paper for an explanation: essentially the prob-
lem is that the syntactic assumptions influence the
semantic representation. e.g., the order of combi-
nation of intersective modifiers affects the semantic
2Current DELPH-IN grammars generally construct MRSs
which may be converted into RMRSs. However, RMRS has
potential advantages, for instance in allowing more extensive
lexical underspecification than is possible with MRS: e.g.,
(Haugereid, 2004).
representation, though it has no effect on denotation.
The binary in-g suffers from this problem.
One alternative would be to use an n-ary conjunc-
tion symbol. However such representations cannot
be constructed compositionally if modification is bi-
nary branching as there is no way of incrementally
adding the conjuncts. Another option we considered
was the use of, possibly redundant, conjunction re-
lations associated with each element which could be
combined to produce a flat conjunction. This leads
to a spurious in-g in the case where there is no mod-
ifier. This looks ugly, but more importantly, does
not allow for incremental specialisation, although
the demonstration of this would take us too far from
the main point of this paper.
We therefore assume a modified version of RMRS
which drops in-g symbols but uses anchors instead.
This means that RMRS and MRS TFS grammars can
be essentially identical apart from lexical types. Fur-
thermore, it turns out that, for composition without
a lexicon, an anchor is needed in the hook regardless
of the treatment of conjunction (see below).
5 RMRS composition without a lexicon
We now discuss the algebra for grammars which
do not have access to subcategorization information
and thus are neither syntactically nor semantically
lexicalist. We concentrate in particular on composi-
tion for the grammar used in the RASP system. RASP
consists of a tokenizer, POS tagger, lemmatizer, tag
sequence grammar and statistical disambiguator. Of
the robust analysers we have looked at, RASP pro-
77
hook slots rels eqs hcons
cat : [l1, a1, x1] {} [l1 : a1 : cat(x1)] {} []
a : [l4, a2, x2] {[l3, a2, x2]spec} [l2 : a2 : a(x2), {} [h2 =q l3]
l2 : a2 : rstr(h2), l2 : a2 : body(h2)]
a cat : [l4, a4, x2] {} [l1 : a1 : cat(x1), l2 : a2 : a(x2), {l3 = l1, [h2 =q l3]
l2 : a2 : rstr(h2), l2 : a2 : body(h2) x2 = x1]
Figure 4: Example of the RMRS algebra.
vides the biggest challenge for the RMRS approach
because it provides quite detailed syntactic analy-
ses which are somewhat dissimilar to the ERG: it
is an intermediate rather than a shallow processor.
The RMRS approach can only be fully successful to
the extent that it abstracts away from the differences
in syntactic analyses assumed by different systems,
so intermediate processors are more difficult to deal
with than shallow ones.
Instead of normal lexical entries, RASP uses the
POS tags for the words in the input. For the exam-
ple in Fig. 1, the output of the POS tagging phase is:
the AT fat JJ cat NN1 sit+ed VVD on II a AT1
mat NN1
The semantics associated with the individual words
in the sentence can be derived from a ?lexicon? of
POS tags, which defines the EPs. Schematically:
AT lexrel q(x) NN1 lexrel n(x)
AT1 lexrel q(x) VVD lexrel v(epast)
JJ lexrel j(x) II lexrel p(e)
Here, ?lexrel? is a special symbol, which is to
be replaced by the individual lemma (with a
leading underscore) ? e.g., lexrel v(epast) yields
l1:a1: sit v(e). Producing the semantics from the
tagger output and this lexicon is a simple matter of
substitution. All EPs are labelled with unique labels
and all variables are different unless repeated in the
same lexical entry.
If the analysis were to stop at POS tagging, the
semantic composition rules would apply trivially.
There are no slots, the hooks are irrelevant and there
are no equalities. The composition principle of ac-
cumulation of elementary predications holds, so the
semantics of the result involves an accumulation of
the rels (see the example at the bottom of Fig. 1).
When using the full RASP parser, although we
cannot expect to obtain all the details available from
deep grammars, we can derive some relational struc-
ture. For instance, given a sentence such as the
cat chased the rat, it should be possible to derive
the ARG1 and ARG2 for chase by associating the
ARG1 with the application of the S/np_vp RASP
rule (i.e., S->NP VP) and the ARG2 with the appli-
cation of the V1/v_np rule. But since there can be
no slot information in the lexical structures (at least
not for open-class words), it is necessary to modify
the lexicalist approach to semantics taken so far.
We assume that both the ARGs and the slots are
specified at a phrasal level rather than lexically. As
mentioned in ?2.1, the MRS algebra allows for rules
to contribute semantics as though they were normal
phrases. The central idea in the application of the
algebra to RASP is to make use of construction se-
mantics in all rules. Fig. 5 illustrates this with the
V1/v_np rule (the NP has been simplified for clar-
ity) assuming the same sort of syntax-semantics in-
terface specification as shown earlier for the CFG.
This is semantically ternary because of the rule se-
mantics. The rule has an ARG2 slot plus a slot R
which is instantiated by the verb?s hook. In effect,
the rule adds a slot to the verb.
It is necessary for the anchor of the argument-
taking structure to be visible at all points where ar-
guments may be attached. For instance, in the ex-
ample above, the anchor of the verb chase has to
be accessible when the ARG1 relation is introduced.
Although generally the anchor will correspond to the
anchor of the semantic head daughter, this is not the
case if there is a scopal modifier (consider a cat did
not chase a rat: the ARG1 must be attached to chase
rather than to not). This is illustrated by not sleep
in Fig. 6. Because not is associated with a unique
tag in RASP, it can be assigned a slot and an ARG1
directly. The anchor of the result is equated with
the label of sleep and thus the subject ARG1 can be
appropriately attached. So the hook would have to
include an anchor even if explicit conjunction were
used instead of equating labels.
78
VP -> V NP
oparg2(opr(rule, V), NP)
chase : [l1, a1, e1] {} [l1 : a1 : chase(e1)] {}
rule : [l2, a2, e2] {[l2, a2, e2]r, [l2 : a2 : ARG2(x2)] {}
[l4, a4, x2]arg2}
(rule V)/r : [l2, a2, e2] {[l4, x2]arg2} [l2 : a1 : ARG2(x2), l1 : a1 : chase(e1)] {l1 = l2, e2 = e1}
it : [l3, a3, x3] {} [l3 : a3 : pron(x3)] {}
chase it : [l2, a2, e2] {} [l2 : a2 : ARG2(x2), l1 : a1 : chase(e1), {l1 = l2, e2 = e1,
l3 : a3 : pron(x3)] l4 = l3, x2 = x3}
Figure 5: RASP-RMRS algebra (hcons omitted)
not : [l1, a2, e2] {[l2, a3, e2]mod} [l1 : a1 : not(e2), l1 : a1 : ARG1(h4)] {} [h4 =q l2]
sleep : [l2, a2, e2] {} [l2 : a2 : sleep(e2)] {} []
not sleep : [l1, a2, e2] {} [l1 : a1 : not(e2), l1 : a1 : ARG1(h4), {} [h4 =q l3]
l2 : a2 : sleep(e2)]
Figure 6: RASP-RMRS illustrating the use of the anchor
6 Experiments with RASP-RMRS
In this section, we outline the practical implementa-
tion of the algebra for RASP-RMRS. The RASP tag
sequence grammar is formally equivalent to a CFG:
it uses phrase structure rules augmented with fea-
tures. As discussed, the algebra requires that ops
are specified for each rule application, and the eas-
iest way of achieving this is to associate semantic
composition rules with each rule name. Composi-
tion operates on the tree output from RASP, e.g.,:
(|T/txt-sc1/----|
(|S/np_vp|
(|NP/det_n1| |Every:1_AT1|
(|N1/n| |cat:2_NN1|))
(|V1/v| |bark+ed:3_VVD|)))
Composition operates bottom-up: the semantic
structures derived from the tags are combined ac-
cording to the semantics associated with the rule.
The implementation corresponds very directly to the
algebra, although the transitive closure of the equali-
ties is computed on the final structure, since nothing
requires that it be available earlier.
The notation used to specify semantics associated
with the rules incorporates some simplifications to
avoid having to explicitly specify the slot and ops.
The specification of equalities between variables and
components of the individual daughters? hooks is a
convenient shorthand for the full algebra.
rule V1/v_np
daughters V NP
semhead V
hook [l,a,e] rels {l:a:ARG2(x)}
eqs {x=NP.index,l=V.label,
a=V.anchor}
If no semantic rule is specified corresponding to
a rule used in a tree, the rels are simply appended.
Semantic composition is thus robust to omissions in
the semantic component of the grammar. In fact, se-
mantic rules can be constructed semi-automatically,
rather than fully manually, although we do not have
space to discuss this in detail here.
There are cases of incompatibility between RASP-
RMRS and ERG-RMRS. For example, the ERG treats
it as expletive in it rains: the lexical entry for rain
specifies an expletive subject (i.e., a semantically
empty it). RASP makes no such distinction, since
it lacks the lexical information and thus the sentence
has extraneous relations for the pronoun and an in-
correct ARG1 for rain. This is an inevitable conse-
quence of the lack of lexical information in RASP.
However, from the perspective of the evaluation of
the revised algebra, the issue is whether there are any
cases where compositional construction of RASP-
RMRSs which match ERG-RMRSs is impossible due
to the restrictions imposed by the algebra. No such
cases have been found.
79
7 Related work
Bos et al (2004) and Bos (2005) derive semantic in-
terpretations from a wide-coverage categorial gram-
mar. There are several differences between this and
RASP-RMRS, but the most important arise from the
differences between CCG and RASP. The CCG parser
relies on having detailed subcategorization infor-
mation (automatically derived from the CCG Bank
which was semi-automatically constructed from the
Penn Treebank), and thus semantic construction can
assume that the arity of the predicate is lexically
available. However, because CCG is purely lexical-
ist, phenomena that we expect to have construction
semantics (e.g., compound nouns, larger numbers)
have to be dealt with in a post-parsing phase rather
than compositionally.
Spreyer and Frank (2005) demonstrate RMRS con-
struction from TIGER dependencies, but do not at-
tempt to match a deep parser output.
8 Conclusion
We have demonstrated that the MRS algebra, orig-
inally intended as a formalisation of some aspects
of semantic composition in constraint-based gram-
mars, can be extended to RMRS and other types of
grammar framework and can be used as the basis of
a full implementation of composition. The algebra
can thus be used much more widely than originally
envisaged and could be exploited by a wide range of
parsers. Useful properties concerning monotonicity
and scope (see Fuchss et al (2004)) are thus guaran-
teed for a range of grammars. Phrasal-level com-
patibility of RMRS (to the extent that this is syn-
tactically possible) is also an important result. The
main practical outcome of this work so far has been
a semantic component for the RASP system which
produces representations compatible with that of the
ERG without compromising RASP speed or robust-
ness. RASP-RMRSs have already been used in sys-
tems for question answering, information extraction,
email response, creative authoring and ontology ex-
traction (e.g., Uszkoreit et al (2004), Watson et al
(2003), Herbelot and Copestake (2006)).
Acknowledgements
This work was funded by EPSRC (EP/C010035/1).
I am very grateful to the anonymous reviewers for
their insightful comments, which sadly I have had
to ignore due to the constraints of time and space. I
hope to address them in a later paper.
References
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran and Julia Hockenmaier 2004. Wide-Coverage
Semantic Representations from a CCG Parser. COL-
ING ?04, Geneva.
Johan Bos. 2005. Towards Wide-Coverage Semantic In-
terpretation. Sixth International Workshop on Compu-
tational Semantics IWCS-6. 42?53.
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. LREC-2002, Las
Palmas, Gran Canaria.
Ann Copestake, Alex Lascarides, and Dan Flickinger.
2001. An Algebra for Semantic Construction in
Constraint-based Grammars. ACL-01, Toulouse.
Ann Copestake. 2003. Report on the design of RMRS.
DeepThought project deliverable.
Ann Copestake, Dan Flickinger, Ivan Sag, and Carl Pol-
lard. 2005. Minimal Recursion Semantics: An in-
troduction. Research in Language and Computation
3(2?3), 281?332.
Dan Flickinger 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering,6:1,15?28.
Ruth Fuchss, Alexander Koller, Joachim Niehren, and
Stefan Thater 2004. Minimal Recursion Semantics as
Dominance Constraints: Translation, Evaluation, and
Analysis. Proceedings of the 42nd ACL, Barcelona.
Gerald Gazdar, Ewan Klein, Geoffrey Pullum and Ivan
Sag 1985. Generalized Phrase Structure Grammar.
Basil Blackwell, Oxford
Petter Haugereid 2004. Linking in Constructions.
HPSG2004, Leuven.
Aurelie Herbelot and Ann Copestake 2006. Acquir-
ing Ontological Relationships from Wikipedia Us-
ing RMRS. ISWC 2006 Workshop on Web Content
Mining with Human Language Technologies, Athens,
Georgia.
Kathrin Spreyer and Anette Frank. 2005 Projecting
RMRS from TIGER Dependencies. HPSG 2005, Lis-
bon. 354?363.
Hans Uszkoreit, Ulrich Callmeier, Andreas Eisele, Ul-
rich Schfer, Melanie Siegel, Jakob Uszkoreit. 2004.
Hybrid Robust Deep and Shallow Semantic Process-
ing for Creativity Support in Document Production.
KONVENS 2004, Vienna, Austria, 209?216.
Rebecca Watson, Judita Preiss and EJ Briscoe. 2003.
The Contribution of Domain-independent Robust
Pronominal Anaphora Resolution to Open-Domain
Question-Answering. Int. Symposium on Reference
Resolution and its Application to Question-Answering
and Summarisation, Venice.
80
