Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 597?603,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Nonparametric Bayesian Inference and Efficient Parsing for
Tree-adjoining Grammars
Elif Yamangil and Stuart M. Shieber
Harvard University
Cambridge, Massachusetts, USA
{elif, shieber}@seas.harvard.edu
Abstract
In the line of research extending statis-
tical parsing to more expressive gram-
mar formalisms, we demonstrate for the
first time the use of tree-adjoining gram-
mars (TAG). We present a Bayesian non-
parametric model for estimating a proba-
bilistic TAG from a parsed corpus, along
with novel block sampling methods and
approximation transformations for TAG
that allow efficient parsing. Our work
shows performance improvements on the
Penn Treebank and finds more compact
yet linguistically rich representations of
the data, but more importantly provides
techniques in grammar transformation and
statistical inference that make practical
the use of these more expressive systems,
thereby enabling further experimentation
along these lines.
1 Introduction
There is a deep tension in statistical modeling of
grammatical structure between providing good ex-
pressivity ? to allow accurate modeling of the
data with sparse grammars ? and low complexity
? making induction of the grammars (say, from
a treebank) and parsing of novel sentences com-
putationally practical. Tree-substitution grammars
(TSG), by expanding the domain of locality of
context-free grammars (CFG), can achieve better
expressivity, and the ability to model more con-
textual dependencies; the payoff would be better
modeling of the data or smaller (sparser) models
or both. For instance, constructions that go across
levels, like the predicate-argument structure of a
verb and its arguments can be modeled by TSGs
(Goodman, 2003).
Recent work that incorporated Dirichlet pro-
cess (DP) nonparametric models into TSGs has
provided an efficient solution to the daunting
model selection problem of segmenting training
data trees into appropriate elementary fragments
to form the grammar (Cohn et al, 2009; Post and
Gildea, 2009). The elementary trees combined in
a TSG are, intuitively, primitives of the language,
yet certain linguistic phenomena (notably various
forms of modification) ?split them up?, preventing
their reuse, leading to less sparse grammars than
might be ideal (Yamangil and Shieber, 2012; Chi-
ang, 2000; Resnik, 1992).
TSGs are a special case of the more flexible
grammar formalism of tree adjoining grammar
(TAG) (Joshi et al, 1975). TAG augments TSG
with an adjunction operator and a set of auxil-
iary trees in addition to the substitution operator
and initial trees of TSG, allowing for ?splicing in?
of syntactic fragments within trees. This func-
tionality allows for better modeling of linguistic
phenomena such as the distinction between modi-
fiers and arguments (Joshi et al, 1975; XTAG Re-
search Group, 2001). Unfortunately, TAG?s ex-
pressivity comes at the cost of greatly increased
complexity. Parsing complexity for unconstrained
TAG scales as O(n6), impractical as compared to
CFG and TSG?s O(n3). In addition, the model
selection problem for TAG is significantly more
complicated than for TSG since one must reason
about many more combinatorial options with two
types of derivation operators. This has led re-
searchers to resort to manual (Doran et al, 1997)
or heuristic techniques. For example, one can con-
sider ?outsourcing? the auxiliary trees (Shieber,
2007), use template rules and a very small num-
ber of grammar categories (Hwa, 1998), or rely
on head-words and force lexicalization in order to
constrain the problem (Xia et al, 2001; Chiang,
597
2000; Carreras et al, 2008). However a solution
has not been put forward by which a model that
maximizes a principled probabilistic objective is
sought after.
Recent work by Cohn and Blunsom (2010) ar-
gued that under highly expressive grammars such
as TSGs where exponentially many derivations
may be hypothesized of the data, local Gibbs sam-
pling is insufficient for effective inference and
global blocked sampling strategies will be nec-
essary. For TAG, this problem is only more se-
vere due to its mild context-sensitivity and even
richer combinatorial nature. Therefore in previ-
ous work, Shindo et al (2011) and Yamangil and
Shieber (2012) used tree-insertion grammar (TIG)
as a kind of expressive compromise between TSG
and TAG, as a substrate on which to build nonpara-
metric inference. However TIG has the constraint
of disallowing wrapping adjunction (coordination
between material that falls to the left and right
of the point of adjunction, such as parentheticals
and quotations) as well as left adjunction along the
spine of a right auxiliary tree and vice versa.
In this work we formulate a blocked sampling
strategy for TAG that is effective and efficient, and
prove its superiority against the local Gibbs sam-
pling approach. We show via nonparametric in-
ference that TAG, which contains TSG as a sub-
set, is a better model for treebank data than TSG
and leads to improved parsing performance. TAG
achieves this by using more compact grammars
than TSG and by providing the ability to make
finer-grained linguistic distinctions. We explain
how our parameter refinement scheme for TAG
allows for cubic-time CFG parsing, which is just
as efficient as TSG parsing. Our presentation as-
sumes familiarity with prior work on block sam-
pling of TSG and TIG (Cohn and Blunsom, 2010;
Shindo et al, 2011; Yamangil and Shieber, 2012).
2 Probabilistic Model
In the basic nonparametric TSG model, there is
an independent DP for every grammar category
(such as c = NP), each of which uses a base dis-
tribution P0 that generates an initial tree by mak-
ing stepwise decisions and concentration parame-
ter ?c that controls the level of sparsity (size) of
the generated grammars: Gc ? DP(?c, P0(? | c))
We extend this model by adding specialized DPs
for auxiliary trees Gauxc ? DP(?auxc , P aux0 (? | c))
Therefore, we have an exchangeable process for
generating auxiliary tree aj given j ? 1 auxiliary
trees previously generated
p(aj | a<j) =
nc,aj + ?auxc P aux0 (aj | c)
j ? 1 + ?auxc
(1)
as for initial trees in TSG (Cohn et al, 2009).
We must define base distributions for initial
trees and auxiliary trees. P0 generates an initial
tree with root label c by sampling rules from a
CFG P? and making a binary decision at every
node generated whether to leave it as a frontier
node or further expand (with probability ?c) (Cohn
et al, 2009). Similarly, our P aux0 generates an aux-
iliary tree with root label c by sampling a CFG rule
from P? , flipping an unbiased coin to decide the di-
rection of the spine (if more than a unique child
was generated), making a binary decision at the
spine whether to leave it as a foot node or further
expand (with probability ?c), and recurring into P0
or P aux0 appropriately for the off-spine and spinal
children respectively.
We glue these two processes together via a set
of adjunction parameters ?c. In any derivation for
every node labeled c that is not a frontier node
or the root or foot node of an auxiliary tree, we
determine the number (perhaps zero) of simulta-
neous adjunctions (Schabes and Shieber, 1994)
by sampling a Geometric(?c) variable; thus k si-
multaneous adjunctions would have probability
(?c)k(1 ? ?c). Since we already provide simul-
taneous adjunction we disallow adjunction at the
root of auxiliary trees.
3 Inference
Given this model, our inference task is to ex-
plore posterior derivations underlying the data.
Since TAG derivations are highly structured ob-
jects, we design a blocked Metropolis-Hastings
sampler that samples derivations per entire parse
trees all at once in a joint fashion (Cohn and Blun-
som, 2010; Shindo et al, 2011; Yamangil and
Shieber, 2012). As in previous work, we use a
Goodman-transformed TAG as our proposal dis-
tribution (Goodman, 2003) that incorporates ad-
ditional CFG rules to account for the possibil-
ity of backing off to the infinite base distribution
P aux0 , and use the parsing algorithm described by
Shieber et al (1995) for computing inside proba-
bilities under this TAG model.
The algorithm is illustrated in Table 1 along
with Figure 1. Inside probabilities are computed
in a bottom-up fashion and a TAG derivation is
sampled top-down (Johnson et al, 2007). The
598
N? N
? N
N
Ni
. . .
?
?N0
? N1
? N2
N3
N4
. . .
?
?
Nj
? Nk
N? ?
Nl
? Nm
N? ?
Figure 1: Example used for illustrating blocked
sampling with TAG. On the left hand side we have
a partial training tree where we highlight the par-
ticular nodes (with node labels 0, 1, 2, 3, 4) that the
sampling algorithm traverses in post-order. On the
right hand side is the TAG grammar fragment that
is used to parse these particular nodes: one initial
tree and two wrapping auxiliary trees where one
adjoins into the spine of the other for full general-
ity of our illustration. Grammar nodes are labeled
with their Goodman indices (letters i, j, k, l,m).
Greek letters ?, ?, ?, ? denote entire subtrees. We
assume that a subtree in an auxiliary tree (e.g., ?)
parses the same subtree in a training tree.
sampler visits every node of the tree in post-order
(O(n) operations, n being the number of nodes),
visits every node below it as a potential foot (an-
other O(n) operations), visits every mid-node in
the path between the original node and the poten-
tial foot (if spine-adjunction is allowed) (O(log n)
operations), and forms the appropriate chart items.
The complexity is O(n2 log n) if spine-adjunction
is allowed, O(n2) otherwise.
4 Parameter Refinement
During inference, adjunction probabilities are
treated simplistically to facilitate convergence.
Only two parameters guide adjunction: ?c, the
probability of adjunction; and p(aj | a<j , c) (see
Equation 1), the probability of the particular aux-
iliary tree being adjoined given that there is an
adjunction. In all of this treatment, c, the con-
text of an adjunction, is the grammar category la-
bel such as S or NP, instead of a unique identi-
fier for the node at which the adjunction occurs as
was originally the case in probabilistic TAG liter-
ature. However it is possible to experiment with
further refinement schemes at parsing time. Once
the sampler converges on a grammar, we can re-
estimate its adjunction probabilities. Using the
O(n6) parsing algorithm (Shieber et al, 1995) we
experimented with various refinements schemes
? ranging from full node identifiers, to Goodman
Chart item Why made? Inside probability
Ni[4] By assumption. ?
Nk[3-4] N?[4] and ? (1 ? ?c) ? pi(?)
Nm[2-3] N?[3] and ? (1 ? ?c) ? pi(?)
Nl[1-3] ? and Nm[2-3] (1 ? ?c) ? pi(?)
?pi(Nm[2-3])
Naux[1-3] Nl[1-3] nc,al/(nc + ?auxc )
?pi(Nl[1-3])
Nk[1-4] Naux[1-3] and Nk[3-4] ?c ? pi(Naux[1-3])
?pi(Nk[3-4])
Nj [0-4] ? and Nk[1-4] (1 ? ?c) ? pi(?)
?pi(Nk[1-4])
Naux[0-4] Nj [0-4] nc,aj /(nc + ?auxc )
?pi(Nj [0-4])
Ni[0] Naux[0-4] and Ni[4] ?c ? pi(Naux[0-4])
?pi(Ni[4])
Table 1: Computation of inside probabilities for
TAG sampling. We create two types of chart
items: (1) per-node, e.g., Ni[?] denoting the
probability of starting at an initial subtree that
has Goodman index i and generating the subtree
rooted at node ?, and (2) per-path, e.g., Nj[?-?]
denoting the probability of starting at an auxiliary
subtree that has Goodman index j and generating
the subtree rooted at ? minus the subtree rooted
at ?. Above, c denotes the context of adjunction,
which is the nonterminal label of the node of ad-
junction (here, N), ?c is the probability of adjunc-
tion, nc,a is the count of the auxiliary tree a, and
nc =
?
a nc,a is total number of adjunctions atcontext c. The function pi(?) retrieves the inside
probability corresponding to an item.
index identifiers of the subtree below the adjunc-
tion (Hwa, 1998), to simple grammar category la-
bels ? and find that using Goodman index identi-
fiers as c is the best performing option.
Interestingly, this particular refinement scheme
also allows for fast cubic-time parsing, which we
achieve by approximating the TAG by a TSG with
little loss of coverage (no loss of coverage under
special conditions which we find that are often sat-
isfied) and negligible increase in grammar size, as
discussed in the next section.
5 Cubic-time parsing
MCMC training results in a list of sufficient statis-
tics of the final derivation that the TAG sampler
converges upon after a number of iterations. Basi-
cally, these are the list of initial and auxiliary trees,
their cumulative counts over the training data, and
their adjunction statistics. An adjunction statistic
is listed as follows. If ? is any elementary tree, and
? is an auxiliary tree that adjoins n times at node ?
of ? that is uniquely reachable at path p, we write
? p? ? (n times). We denote ? alternatively as
?[p].
599
*q
!
p
"
n
m
k
# *
p
"
i
i
i
q
!
i k
# *
m
i
"
i
#
i
i
i
#
j
j
j
q
!
i
j
i
j
!
ij
i
(1) (2) (3)
Figure 2: TAG to TSG transformation algorithm. By removing adjunctions in the correct order we end
up with a larger yet adjunction-free TSG.
Now imagine that we end up with a small gram-
mar that consists of one initial tree ? and two aux-
iliary trees ? and ?, and the following adjunctions
occurring between them
? p? ? (n times)
? p? ? (m times)
? q? ? (k times)
as shown in Figure 2. Assume that ? itself occurs
l > n +m times in total so that there is nonzero
probability of no adjunction anywhere within ?.
Also assume that the node uniquely identified by
?[p] has Goodman index i, which we denote as
i = G(?[p]).
The general idea of this TAG-TSG approxima-
tion is that, for any auxiliary tree that adjoins at a
node ? with Goodman index i, we create an ini-
tial tree out of it where the root and foot nodes of
the auxiliary tree are both replaced by i. Further,
we split the subtree rooted at ? from its parent and
rename the substitution site that is newly created
at ? as i as well. (See Figure 2.) We can sep-
arate the foot subtree from the rest of the initial
tree since it is completely remembered by any ad-
joined auxiliary trees due to the nature of our re-
finement scheme. However this method fails for
adjunctions that occur at spinal nodes of auxiliary
trees that have foot nodes below them since we
would not know in which order to do the initial
tree creation. However when the spine-adjunction
relation is amenable to a topological sort (as is the
case in Figure 2), we can apply the method by go-
ing in this order and doing some extra bookkeep-
ing: updating the list of Goodman indices and re-
directing adjunctions as we go along. When there
is no such topological sort, we can approximate
the TAG by heuristically dropping low-frequency
adjunctions that introduce cycles.1
The algorithm is illustrated in Figure 2. In (1)
we see the original TAG grammar and its adjunc-
tions (n,m, k are adjunction counts). Note that
the adjunction relation has a topological sort of
?, ?, ?. We process auxiliary trees in this order
and iteratively remove their adjunctions by creat-
ing specialized initial tree duplicates. In (2) we
first visit ?, which has adjunctions into ? at the
node denoted ?[p] where p is the unique path from
the root to this node. We retrieve the Goodman in-
dex of this node i = G(?[p]), split the subtree
rooted at this node as a new initial tree ?i, relabel
its root as i, and rename the newly-created sub-
stitution site at ?[p] as i. Since ? has only this
adjunction, we replace it with initial tree version
?i where root/foot labels of ? are replaced with
i, and update all adjunctions into ? as being into
?i. In (3) we visit ? which now has adjunctions
into ? and ?i. For the ?[p] adjunction we create ?i
the same way we created ?i but this time we can-
not remove ? as it still has an adjunction into ?i.
We retrieve the Goodman index of the node of ad-
junction j = G(?i[q]), split the subtree rooted at
this node as new initial tree ?ij , relabel its root
as j, and rename the newly-created substitution
site at ?i[q] as j. Since ? now has only this ad-
junction left, we remove it by also creating initial
tree version ?j where root/foot labels of ? are re-
placed with j. At this point we have an adjunction-
free TSG with elementary trees (and counts)
?(l), ?i(l), ?i(n), ?ij(n), ?i(m), ?j(k) where l is
the count of initial tree ?. These counts, when they
are normalized, lead to the appropriate adjunc-
1We found that, on average, about half of our grammars
have a topological sort of their spine-adjunctions. (On aver-
age fewer than 100 spine adjunctions even exist.) When no
such sort exists, only a few low-frequency adjunctions have
to be removed to eliminate cycles.
600
 0
 5
 10
 15
 20
 25
 30
 35
 40
 0  10  20  30  40  50  60
P
a
r
s
i
n
g
 
t
i
m
e
 
(
s
e
c
o
n
d
s
)
Sentence length (#tokens)
Figure 3: Nonparametric TAG (blue) parsing is ef-
ficient and incurs only a small increase in parsing
time compared to nonparametric TSG (red).
tion probability refinement scheme of ?c ? p(aj |
a<j , c) where c is the Goodman index.
Although this algorithm increases grammar
size, the sparsity of the nonparametric solution
ensures that the increase is almost negligible: on
average the final Goodman-transformed CFG has
173.9K rules for TSG, 189.2K for TAG. Figure 3
demonstrates the comparable Viterbi parsing times
for TSG and TAG.
6 Evaluation
We use the standard Penn treebank methodology
of training on sections 2?21 and testing on section
23. All our data is head-binarized, all hyperpa-
rameters are resampled under appropriate vague
gamma and beta priors. Samplers are run 1000
iterations each; all reported numbers are aver-
ages over 5 runs. For simplicity, parsing results
are based on the maximum probability derivation
(Viterbi algorithm).
In Table 4, we compare TAG inference
schemes and TSG. TAGGibbs operates by locally
adding/removing potential adjunctions, similar to
Cohn et al (2009). TAG? is the O(n2) algorithm
that disallows spine adjunction. We see that TAG?
has the best parsing performance, while TAG pro-
vides the most compact representation.
model F measure # initial trees # auxiliary trees
TSG 84.15 69.5K -
TAGGibbs 82.47 69.9K 1.7K
TAG? 84.87 66.4K 1.5K
TAG 84.82 66.4K 1.4K
Figure 4: EVALB results. Note that the Gibbs
sampler for TAG has poor performance and pro-
vides no grammar compaction due to its lack of
convergence.
label #adj ave. #lex. #left #right #wrap
(spine adj) depth trees trees trees trees
VP 4532 (23) 1.06 45 22 65 0
NP 2891 (46) 1.71 68 94 13 1
NN 2160 (3) 1.08 85 16 110 0
NNP 1478 (2) 1.12 90 19 90 0
NNS 1217 (1) 1.10 43 9 60 0
VBN 1121 (1) 1.05 6 18 0 0
VBD 976 (0) 1.0 16 25 0 0
NP 937 (0) 3.0 1 5 0 0
VB 870 (0) 1.02 14 31 4 0
S 823 (11) 1.48 42 36 35 3
total 23320 (118) 1.25 824 743 683 9
Table 2: Grammar analysis for an estimated TAG,
categorized by label. Only the most common top
10 are shown, binarization variables are denoted
with overline. A total number of 98 wrapping
adjunctions (9 unique wrapping trees) and 118
spine adjunctions occur.
ADJP
?
?
ADJP
ADJP* ?
?
NP
-LRB- NP
NP* -RRB-
S
-LRB-
-LRB-
S
S* -RRB-
-RRB-
S
?
?
S
S* ?
?NP
-LRB-
-LRB-
NP
NP* -RRB-
-RRB-
NNP
,
,
NNP
NNP
NNP* CC
&
NNP
NP
?
?
NP
NP* ?
?
NP
NP
NP :
NP
NP* PP
Figure 5: Example wrapping trees from estimated
TAGs.
7 Conclusion
We described a nonparametric Bayesian inference
scheme for estimating TAG grammars and showed
the power of TAG formalism over TSG for return-
ing rich, generalizable, yet compact representa-
tions of data. The nonparametric inference scheme
presents a principled way of addressing the diffi-
cult model selection problem with TAG. Our sam-
pler has near quadratic-time efficiency, and our
parsing approach remains context-free allowing
for fast cubic-time parsing, so that our overall
parsing framework is highly scalable.2
There are a number of extensions of this
work: Experimenting with automatically in-
duced adjunction refinements as well as in-
corporating substitution refinements can benefit
Bayesian TAG (Shindo et al, 2012; Petrov et al,
2006). We are also planning to investigate TAG
for more context-sensitive languages, and syn-
chronous TAG for machine translation.
2An extensive report of our algorithms and experiments
will be provided in the PhD thesis of the first author (Ya-
mangil, 2013). Our code will be made publicly available at
code.seas.harvard.edu/?elif.
601
References
Xavier Carreras, Michael Collins, and Terry Koo.
2008. TAG, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of the Twelfth Conference on Computational
Natural Language Learning, CoNLL ?08, pages 9?
16, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics, ACL ?00, pages
456?463, Morristown, NJ, USA. Association for
Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2010. Blocked in-
ference in Bayesian tree substitution grammars. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ?10, pages 225?230, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In NAACL ?09: Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 548?556, Morristown, NJ, USA. Association
for Computational Linguistics.
Christine Doran, Beth Hockey, Philip Hopely, Joseph
Rosenzweig, Anoop Sarkar, B. Srinivas, Fei Xia,
Alexis Nasr, and Owen Rambow. 1997. Maintain-
ing the forest and burning out the underbrush in xtag.
In Proceedings of the ENVGRAM Workshop.
Joshua Goodman. 2003. Efficient parsing of DOP
with PCFG-reductions. In Rens Bod, Remko Scha,
and Khalil Sima?an, editors, Data-Oriented Parsing.
CSLI Publications, Stanford, CA.
Rebecca Hwa. 1998. An empirical evaluation of
probabilistic lexicalized tree insertion grammars. In
Proceedings of the 17th international conference on
Computational linguistics - Volume 1, pages 557?
563, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Mark Johnson, Thomas Griffiths, and Sharon Gold-
water. 2007. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 139?146, Rochester, New York, April.
Association for Computational Linguistics.
Aravind K. Joshi, Leon S. Levy, and Masako Taka-
hashi. 1975. Tree adjunct grammars. Journal of
Computer and System Sciences, 10(1):136?163.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433?440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Matt Post and Daniel Gildea. 2009. Bayesian learning
of a tree substitution grammar. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
45?48, Suntec, Singapore, August. Association for
Computational Linguistics.
Philip Resnik. 1992. Probabilistic tree-adjoining
grammar as a framework for statistical natural lan-
guage processing. In Proceedings of the 14th con-
ference on Computational linguistics - Volume 2,
COLING ?92, pages 418?424, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Yves Schabes and Stuart M. Shieber. 1994. An
alternative conception of tree-adjoining derivation.
Computational Linguistics, 20(1):91?124. Also
available as cmp-lg/9404001.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of de-
ductive parsing. J. Log. Program., 24(1&2):3?36.
Stuart M. Shieber. 2007. Probabilistic synchronous
tree-adjoining grammars for machine translation:
The argument from bilingual dictionaries. In Dekai
Wu and David Chiang, editors, Proceedings of the
Workshop on Syntax and Structure in Statistical
Translation, Rochester, New York, 26 April.
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata.
2011. Insertion operator for Bayesian tree substitu-
tion grammars. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: short pa-
pers - Volume 2, HLT ?11, pages 206?211, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined
tree substitution grammars for syntactic parsing. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 440?448, Jeju Island, Korea,
July. Association for Computational Linguistics.
Fei Xia, Chung-hye Han, Martha Palmer, and Aravind
Joshi. 2001. Automatically extracting and compar-
ing lexicalized grammars for different languages. In
Proceedings of the 17th international joint confer-
ence on Artificial intelligence - Volume 2, IJCAI?01,
pages 1321?1326, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
XTAG Research Group. 2001. A lexicalized tree
adjoining grammar for English. Technical Report
IRCS-01-03, IRCS, University of Pennsylvania.
602
Elif Yamangil and Stuart Shieber. 2012. Estimating
compact yet rich tree insertion grammars. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 110?114, Jeju Island, Korea, July.
Association for Computational Linguistics.
Elif Yamangil. 2013. Rich Linguistic Structure from
Large-Scale Web Data. Ph.D. thesis, Harvard Uni-
versity. Forthcoming.
603
