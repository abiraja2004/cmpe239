DIAGNOSIS AS A NOTION OF GRAMMAR 
Mitchel l  Marcus 
Art i f ic ia l  Intel l igence Laboratory 
M.I.T. 
This paper will sketch an approach to 
natural language parsing based on a new 
conception of what makes up a recognit ion 
grammar for syntactic analysis and how such 
a grammar should be structured. This theory 
of syntactic analysis formalizes a notion 
very much like the psychologist 's  notion of 
"perceptual strategies" \[Bever "70\] and 
makes this formal ized notion - which will be 
called the notion of wait -and-see 
diagnostics - a central and integral part of 
a theory of what one knows about the 
structure of language. By recognit ion 
grammar, we mean here what a speaker of a 
language knows about that language that 
allows him to assign grammatical  structure 
to the word strings that make up utterances 
in that language. 
This theory of grammar is based on the 
hypothesis that every language user knows as 
part of his recognit ion grammar a set of 
highly specif ic diagnostics that he uses to 
decide determinist ica l ly  what structure to 
build next at each point in the process of 
parsing an utterance. By determinist ica l i?  
I mean that once grammatical  structure is 
built, it cannot be discarded in the normal 
course of the parsing process, i.e. that no 
"backtracking" can take place unless the 
sentence is consciously perceived as being a 
"garden path". This notion of grammar puts 
knowledge about control l ing the parsing 
process on an equal footing with knowledge 
about its possible outputs. 
To test this theory of grammar, a 
parser has been implemented that provides a 
language for writ ing grammars of this sort, 
and a grammar for Engl ish is current ly being 
written that attempts to capture the 
wait -and-see diagnost ics needed to parse 
Engl ish within the constraints of the  
theory. The control  structure of the parser 
strongly ref lects the assumptions the theory 
makes about the structure of language, and 
the discussion below wil l  use the structure 
of the parser as an example of the 
impl icat ions of this theory for the parsing 
process. The current grammar of Engl ish is 
deep but not yet broad; this has al lowed 
invest igat ion of the sorts of wait -and-see 
diagnost ics needed to handle complex Engl ish 
construct ions without a need to wait unti l  a 
grammar for the entire range of Engl ish 
construct ions could be written. To give 
some idea of the scope of the grammar, the 
parser is capable of handling sentences 
like: 
Do all the boys the l ibrarian gave 
books to want to read them? 
The men John wanted to be bel ieved by shot 
him yesterday. 
It should be ment ioned that certain 
grammatical  phenomena are not handled at all 
by the present grammar, chief among them 
conjunct ion and certain important sorts of 
lexical ambiguity. There is every 
intention, however, of expanding the grammar 
to deal with them. 
Two Paradigms 
To explain exactly what the details of 
this wait -and-see (W&S) paradigm are, it is 
useful to compare this notion with the 
current prevai l ing parsing paradigm, which I 
wil l  call the guess-and-then-backup (G&B) 
paradigm. This paradigm is central to the 
parsers of both Terry' Winograd's SHRDLU 
\[Winograd "72\] and Bill Woods" LUNAR \[Woods 
"72\] systems. 
In a parser based on the G&B paradigm, 
various options are enumerated in the 
parser's grammar for the next possible 
const i tuent at any given point in the parse 
and these options are tested one at a time 
against the input. The parser assumes 
tentat ively that one of these options is 
correct and then proceeds with this option 
until  either the parse is completed or the 
option fails, at which point the parser 
simply backs up and tries the next option 
enumerated in the parser's grammar. This is 
the paradigm of G&B: enumerate all options, 
pick one, and then (if it fails) backup and 
pick another. While attempts have been made 
to make this backup process clever, 
especial ly  in Winograd's  SHRDLU, it seems 
that it is very diff icult,  if not impossible 
in general, to tell from the nature of the 
cul de sac exactly where the parser has gone 
astray. In order to parse a sentence of 
even moderate complexity, there are not one 
but many points at which a G&B parser must 
make guesses about what sort of structure to 
expect next and at all of these points the 
correct hypothesis must be found before the 
parse can be successful ly  completed. 
Furthermore, the parser may proceed 
arb i t rar i ly  far ahead on any of these 
hypotheses before discover ing that the 
hypothesis  was incorret, perhaps 
inval idat ing several other hypotheses 
contingent upon the first. In essence, the 
G&B paradigm considers the grammar of a 
natural  language to be a t ree-structured 
space through which the parser must blindly, 
though perhaps cleverly, search to find a 
correct parse. 
The W&S paradigm rejects the notion of 
backup as a standard control mechanism for 
parsing. At each point in the parsing 
process, a W&S parser wil l  only build 
grammatical  structure it is sure it can use. 
The parser does this by determining, by a 
two part process, which of the hypotheses 
possible at any given point of the parse is 
correct before attempting any of them. The 
parser first recognizes the specif ic  
s i tuat ion it is in, determined both on the 
basis of global expectat ions result ing from 
whatever structure it has parsed and 
absorbed, and from features of lower level 
substructures from a l ittle ahead in the 
input to which internal structure can be 
assigned with certainty but whose function 
is as yet undetermined. Each such s i tuat ion 
can be so defined that it restrains the set 
of possible hypotheses to at most two or 
three. If only one hypothesis is possible, 
a W&S parser wil l  take it as given, 
otherwise it wil l  proceed to the second step 
! 
! 
i 
! 
! 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
i 
I 
I 
! 
I 
of the determinat ion process ,  to do a 
~if ferential  diagnosis to decide between the 
competing hypotheses. For each dif ferent 
situation, a W&S grammar includes a series 
of easi ly computed tests that decides 
between the competing hypotheses. The key 
assumption of  the W&S paradigm, then? i_gs 
that the structure of  natural language 
provides enough and the right information t__~o 
~etermine exactly what too d__oo next at  each 
point of  ~ Parse. There is not suff ic ient 
room here to discuss this assumption; the 
reader is invited to read \[Marcus ~74\], 
which discusses this assumption at length. 
Th___~e Parser Itself 
? To  firm up this talk of "expectations',, 
"situations", and the like, it it useful to 
see how these notions are real ized in the 
exist ing W&S parsing system. Before we can 
do this, it wil l  be necessary to get an 
overview of the structure and operat ion of 
the parser itself. 
A grammar in this system is made up of 
packets of pattern- invoked demons, which 
wil l  be cal led modules. (The notion of 
packet here derives from work by Scott 
Fahlman \[Fahlman "73\].) The parser itself  
consists of two levels, a group level and a 
clause level, and any packet of modules is 
intended to function at one level or the 
o~her. Modules at group level are intended 
to work on a buffer of words and word level 
structures and to eventual ly bu i ld  group 
level structures, such as Noun Grouos (i.e. 
Noun Phrases up to the head noun) and Verb 
GrouPs (i.e. the verb cluster up to the 
main verb), which are then put onto the end 
of a buffer of group level structures not 
yet absorbed by higher level processes. 
Modules at clause level are intended to work 
on these substructures and to assemble them 
into clauses. The group buffer and the word 
buffer can both grow up to some 
predetermined length, on the order of 3, 4, 
or 5 structures. Thus the modules at the 
level above needn't immediately use each 
structure as it comes into the buffer; but 
rather can let a small number of structures 
"pile up" and then examine these structures 
before deciding how to use the first of 
them. In this sense the modules at each 
level have a l imited, sharply constra ined 
look-ahead abil ity; they can wait and see 
what sort of environment surrounds a 
substructure in the buffer below before 
deciding what the higher level funct ion of 
that substructure is. (It should be noted 
that the amount of look-ahead is constra ined 
not only by maximum buffer length but also 
by the restr ict ion that a module may access 
only the two substructures immediately 
fol lowing the one it is current ly trying to 
uti l ize. This constraint  is necessary 
because the substructure about to be 
ut i l ized at any moment may not be the first 
in the buffer, for various reasons.) 
Every module consists of a pattern, a 
pretest procedure, and a body to be executed 
if the pattern matches and the pretest 
succeeds. Each pattern consists of an 
ordered list of sets of features. As 
structures are built up by the parser, they 
are label led with features, where a feature 
is any property of a structure that the 
grammar wants to be visible at a glance to  
any module looking even casual ly at that 
structure. (Structures can also have 
registers attached to them, carrying more 
special ized sorts of information; the 
contents of a register are pr iv i leged in 
that a module can access the contents of a 
register only if it knows the name of that 
register.) A module's  pattern matches if the 
feature sets of the pattern are subsumed by 
the feature sets of consecut ive structures 
in the appropr iate buffer, with the match 
start ing at the effect ive beginning of the 
buffer. 
Very few modules in any W&S grammar ae 
always active, wait ing to be tr iggered when 
their patterns match; a module is active 
only when a packet it is in has been 
activated, i.e. added to the set of 
present ly active packets. Packets are 
act ivated or deact ivated by the parser at 
the specif ic order of individual  modules; 
any module can add or remove packets from 
the set of active packets if it has reason 
to do so. 
A pr ior i ty ordering of modules provides 
sti l l  further control. Every module is 
assigned a numerical  priority, creating a 
part ial  order ing on the active modules. At 
any time, only the h ighest -pr ior i t ied module 
of those whose patterns match wil l  be 
al lowed to run. Thus, a special  purpose 
module can edge out a general purpose module 
both of whose patterns match in a given 
environment,  or a module to handle some 
last-resort  case can lurk low in a pool of 
act ive modules, to serve as default only if 
no h lgher -pr ior i t ied  module responds to a 
situation. 
F irmin~ Up The Notion Of  Si tuat ion 
This, in brief, is the structure of the 
W&S parser; now we can turn to a discussion 
of how this structure ref lects the 
theoret ica l  f ramework discussed above. Let 
us begin by recast ing a statement made 
above: In decid ing what unique course of 
act ion to take at any point in the parse, 
the parser first recognizes a speci f ic  
wel l -def lned s i tuat ion on the basis of a 
combinat ion of global expectat ions and the 
specif ic  features of lower level 
substructures which are as yet unabsorbed. 
It should now become clear that what it 
means to have a global expectat ion is that 
the appropr iate packet is active in the 
parser, and that each module is i tself  the 
special ist  for the s i tuat ion that its 
packet, pattern and pretest define. The 
grammar act ivates and deact ivates packets to 
ref lect its global expectat ions about 
syntact ic structures that may be encountered 
as a result of what it has seen so far. 
(The parser might also act ivate packets on 
the basis of what some higher level process 
in a natural  language understanding system 
tells it to expect by way of d iscourse 
phenomena.)  These packets often ref lect 
rather large scale grammatical  expectat ions;  
for example, the fo l lowing are some packets 
within the exist ing grammar: S imple-sentence 
start, a packet of modules that parse the 
subject, main verb, and init ial modif iers of 
major clauses, determining clause type and 
the like; S imple-sentence after VG, whose 
modules parse objects and clause level 
preposit ional  phrases in top level clauses; 
WH-quest ion after VG, similar to 
S imple-sentence after VG, except that its 
modules are responsible for act ively 
deciding where to "replace" the group 
fronted to form a WH-quest ion and also where 
to replace the NP "pulled out" of a relative 
clause. A dif ferent sort of packet is 
NG-object o__cr To-complement,  a packet of 
modules that includes as subpacket the 
packet To-complement,  whose modules assign 
deep subjects to to-complements with 
"deleted" subjects, e.g. "John wanted to 
buy a bicycle.". Beyond this, NG-object o__rr 
To-complement includes modules which delay 
modules in other active "after VG" packets 
from assigning any NP appear ing after the VG 
as the object of that VG until it can be 
determined ?whether or not it is the subject 
of a fol lowing to-complement - Compare "He 
wanted a free pass to the movie.",  "He 
wanted a friend to see the movie.", "He 
wanted a fr iend to see the movie with.". It 
should be noted that packets of modules, 
i.e. expectat ions in our theoret ical  
framework, roughly correspond to states in 
Woods" Augmented Transit ion Network model 
with some major di f ferences: Not only do 
packets correspond to much larger 
grammatical  chunks than the typical  usage of 
ATN states, but more importantly,  many 
packets may be active at the same time and 
the modules in these packets may strongly 
interact with each other if they so choose. 
The other determinant of s i tuat ion in 
the theoret ical  framework is the general  
features of unabsorbed lower level 
substructures.  It should be obvious that 
this corresponds to the pattern-match 
between a module 's  pattern and the 
substructures in the buffer below it. That 
these lower structures are avai lable at 
clause level is the result of the  
interact ion between the modules at group 
level and those at clause level. Again, for 
a d iscuss ion of why such substructures can 
be built independently,  the reader is 
referred to \[Marcus "74\]. It is an 
important claim of the W&S theory that 
though these patterns are very simple and - 
viewed from the level above - very local, 
they suffice, in conjunct ion with 
expectat ions ref lected by packet 
act ivat ions,  to so restr ict the range of 
possible options open to the parser at any 
point that a determinist ic  diagnosis of 
those options can be made. While a module's  
pretest might ut i l ize very complex tests 
that strongly aid in the restr ict ion of 
situation, in fact pretests seem to be 
needed mainly to compute simple boolean 
functions of features tha are more complex 
than the implic it  logical  conjunct ion 
demanded by the patterns themselves. 
D i f ferent iat ing Between Hypotheses 
Now that it is clear how a s ituat ion is 
defined, i.e. how a given module becomes 
active, we must consider how the correct 
course of act ion can be determined given the 
set of possible a l ternat ives defined by a 
situation, i.e. what a module knows that 
makes it a diagnost ic special ist.  We need 
to invest igate in part icular what sorts of 
tests a module can use to dependably yield a 
di f ferent ial  diagnosis for the s ituat ion it 
is a special ist  for. 
A module has avai lable several 
di f ferent sorts of information for 
diagnosis; it can use the syntact ic 
informat ion it can ferret out direct ly from 
the structures it has built, and it can ask 
quest ions of both a crude but fast semantic 
model and a full world model. To gather 
syntact ic informat ion the parser has a 
faci l i ty for invest igat ing any node of any 
tree structure it knows about; thus it can 
invest igate the features or registers of any 
node it pleases. While the parser i tself  
attempts to build an annotated surface 
structure s imi lar to that built by 
Winograd's  SHRDLU, it converses constant ly  
with a case frame interpreter  that is 
intended to serve as interface between it 
and deep world model l ing. The parser is 
obl iged to inform the case frame interpreter  
as soon as it can of information such as 
what the main verb of a given clause is, 
what its subject is, what its objects and 
preposi t ional  phrases are. (The case frame 
interpreter  speaks the parser's language; it 
knows how to map the grammatical  re lat ions 
the parser deals with into its own case 
roles.) In return, the parser can ask the 
case frame interpreter  for many dif ferent 
sorts of information: whether or not a given 
NP or prepos i t iona l  phrase can fill a given 
grammatical  role, given the informat ion the 
case frame has so far; which of two 
potent ia l  candidates the case frame would 
prefer in a given grammatical  role, and by 
how much; how many slots of various sorts 
are sti l l  open in the case frame; etc. 
Furthermore,  when necessary, the parser can 
ask precompi led f i l l - in - the-b lank quest ions 
of the world model i tself  (the world model  
in the current system being the author); to 
resolve the ambigui ty  of a phrase like "a 
third cup of sugar", for example, the parser 
might ask the world model a precompi led 
quest ion equivalent  to asking whether it 
makes sense in the present discourse context 
to speak of a 3rd (as opposed to I/3rd) cup 
of sugar. 
Regardless of which source or sources 
of in format ion a module uses, it wil l  often 
need to know not merely whether  one 
hypothesis  or another is acceptable,  but 
which of two hypotheses is better. A module 
wil l  often need to ask the case frame 
interpreter,  for example, not merely  whether  
some NP is semant ica l ly  acceptable in a 
given role for a given verb, but rather 
which of two NPs is better in that role and 
by how much. A good example of this is the 
diagnost ic  used by the module which resolves 
the quest ion of which NP serves as direct 
and indirect object in sentences such as 
(i) Who did John give the book? 
(ii) What did John give the elephant? 
(iii) Who did John give the e lephant? 
(where (iii) may be at least quest ionable in 
I 
I 
I 
I 
! 
I 
1 
I 
I 
I 
I 
LI 
I 
1 
1 
I 
I 
I 
I 
the idiolects of some readers). This 
diagnostic is also a good general example of 
the sorts of diagnostic rules that a W&S 
grammar contains. The module containing 
this diagnostic is in packet WH-quest ion 
after VG, active when the under ly ing role of 
the quest ion group has not yet been 
determined. It is tr iggered when a single 
NP follows the main verb of the clause, 
fol lowed by neither another NP nor a 
Preposit ional  Phrase. The module first asks 
the case frame interpreter whether more than 
one slot is feasibly open for NP objects. 
The answer here is yes as there are two 
slots open, so the module knows that it can 
use both the fol lowing NP and the quest ion 
group to fill these slots and it tells the 
case frame interpreter  to commit itself to 
using both slots. The remaining diagnost ic 
is essent ia l ly  this: whi le the module has a 
mild syntact ic preference to use the 
fol lowing NP as indirect object, it wil l  
accept the question group as indirect object 
if the case frame strongly prefers it to the 
fo l low ing  NG as IO, with a feature added to 
the top level clause indicat ing that the 
clause is a wee bit ungrammatical ;  this is 
the path taken for  (i) above. If the case 
frame does prefer the next NP to the 
quest ion group, the module is very happy; 
this is the path for (ii). If the case 
frame mildly prefers the question group to 
the fol lowing NP, balancing the syntact ic 
preference, as in (iii), the sentence is 
perceived as very wierd (although a few 
people have no trouble here), and the module 
wil l  do in desperat ion what many speakers 
seem to do in this case - take the quest ion 
group as IO on semantic grounds whi le 
complaining loudly. 
W&S As A Psvchological  Model 
Though the diagnostic above is far more 
specif ic in both appl icabi l i ty  and in detai l  
than what is normal ly considered a 
perceptual strategy, it fi l ls much the same 
role that perceptual  strategies are assumed 
to play. There are many crucial  
differences, however. Wait -and-see 
diagnost ics are treated as rules of grammar 
in the W&S theory, and the parser appl ies 
them in a consistent, rule- l ike manner. 
Indeed, in the grammar itself  (unlike the 
theoret ical  framework), no d i f ferent iat ion 
is made expl ic i t ly  between grammar code that 
decides what to do and grammar code that 
does it; both are integral parts of the 
grammar. Wait -and-see diagnostics, unl ike 
perceptual  strategies, in general  use 
syntact ic and semantic dist inct ions together 
to diagnose the correct course of action, 
although the information a diagnostic seeks 
is usual ly very specif ic and very focused. 
It is also important to note that a 
diagnostic chooses between a set of very 
specif ic possible options, and is not at all 
a general  rule of thumb. 
But what if one of these W&S 
diagnost ics fai ls? If the parser takes a 
wrong turn because one of the W&S 
diagnost ics in its grammar was misled, then 
the parse cannot be successfu l ly  completed 
and the sentence is a "garden path" with 
respect to that grammar. In these 
situations, however, a W&S parser wil l  not 
"fail", in the sense of throwing up its 
hands and y ie ld ing no parse at all, but 
rather it wil l  yield up whatever structure 
it has constructed at the point of blockage 
and wil l  attempt to build whatever 
grammatical  substructures it can with the 
remainder of the input, much as people seem 
to do when confronted with sentences that 
are garden paths with respect to their 
grammars. A higher level problem solver 
might then use higher level grammatical  
knowledge to try to diagnose the source of 
the garden path, e.g. it might know about 
dangl ing part ic ip les and how to fix them. 
I 
This property of W&S grammars suggests 
a global test of the plausibi l i ty  of the W&S 
theory as a psychological  model as well as a 
local test for the adequacy of any 
diagnostic within a W&S grammar: Not only 
should an ideal W&S grammar be able to parse 
correct ly all sentences that people parse 
correctly, but it should also perceive as 
garden paths exact ly those sentences that 
people perceive as garden paths. At the 
~resent stage of grammar development, it 
should be added, it does not seem to be 
diff icult to build individual W&S 
diagnost ics that behave local ly as people do 
in terms of percept ions of garden paths. To 
this extent it seems reasonable to suggest 
that people may use diagnost ics that are 
s imi lar to the diagnost ics the W&S model 
posits. 
One other property of the implemented 
W&S system is also interest ing in terms of 
the p laus ib i l i ty  of the W&S model as a 
psychological  model. It turns out that the 
length of time required for the parser to 
parse input is d irect ly proport ional  to the 
length of the input; the time per word 
required by the parser to parse any sentence 
seems to vary by no more than 40% over the 
time taken to parse "simple" sentences. 
Construct ions that do take proport ional ly  
longer than simple sentences do so because 
of factors such as the addit ional  
computat ion needed by diagnost ics that 
determine where to insert "deleted" 
structures and the like, but, again, the 
total increase in time per word, averaged 
over the entire sentence is rarely more than 
40% greater than simple sentences. These 
time relat ions are consistent with the range 
of results from psychological  exper iments 
that attempt to measure latencies for 
sentence comprehens ion (such as \[Wanner 
"74\]), a l though no detai led comparison of 
the time behavior of the parser and that 
measured by psychological  test ing has yet 
been undertaken. It is possible that this 
time behavior wil l  change as the W&S grammar 
grows, but this sort of behavior would seem 
to be intr insic to any parser and grammar 
based on the W&S model. 
REFERENCES 
Bever, T.G., The cognit ive bases for 
l inguist ic  structures. In Cognit ion and 
the ~ of Language, J. Hayes, 
editor, John Wiley & Sons, 1970 
Fahlman, S., A Hypothes is -Frame Syste~ FQr 
Problems, Working Paper 57, 
MIT-AI Lab, 1973 
Marcus, M., Wait and See Strategies for 
Parsing Natural Language, Working Paper 
75, MIT-AI Lab, 1974 
Wanner, E., , Kaplan, R. and Shiner, S., 
Garden Paths i_~n Relative Clauses, 
unpublished, Harvard University, 1974 
Winograd, T., Understanding Natural 
Language, Academic Press, 1972 
Woods, W.A., Kaplan, R.M. and Nash-Webber, 
B., The Lunar Sciences Natural Language 
Information System, BBN Report No. 
2378, 1972 
lO  
I 
1 
! 
I 
I 
I 
I 
! 
! 
I 
i 
J 
I 
i 
I 
I 
I 
I 
I 
COMPUTATIONAL UNDERSTANDING 
Christopher K. Riesbeck 
I. METHODOLOGICAL POSITION 
The problem of computat ional  
understanding has often been broken into two 
sub-problems: how to syntact ical ly  analyze a 
natural  language sentence and how to 
semant ica l ly  interpret the results of the 
syntact ic analysis. There are many reasons 
for this subdivis ion of the task, involving 
histor ical  inf luences from American 
structural  l inguist ics and the early 
"knowledge-free" approaches to Art i f ic ia l  
Intel l igence. The sub-divis ion has remained 
basic to much work in the area because 
syntact ic analysis seems to be much more 
amenable to computat ional  methods than 
semantic interpretat ion does, and thus more 
workers have been attracted developing 
syntact ic analyzers first. 
It is my bel ief that this subdivis ion 
has hindered rather than helped workers in 
this area. It has led to much wasted effort 
on syntact ic parsers as ends in themselves. 
It raises false issues, such as how much 
semantics should be done by the syntact ic 
analyzer and how much syntactics should be 
done by the semantic interpreter. It leads 
researchers into a l l -or -none choices on 
language processing when they are trying to 
develop complete systems. E i ther  the 
researcher tries to build a syntact ic 
analyzer first, and usually gets no farther, 
or he ignores language processing 
altogether.  
The point to real ize is that these 
problems arise from an overemphasis on the 
syntax/semant ics  dist inction. Certa in ly 
both syntact ic knowledge and semantic 
knowledge are used in the process of 
comprehension. The false problems arise 
when the comprehension process i tself  is 
sect ioned off into weakly communicat ing 
sub-processes, one of which does syntact ic 
analysis  and the other of which does 
semantic. Why should considerat ion of the 
meaning of a sentence have to depend upon 
the successful  syntactic analysis of that 
sentence? This is certainly not a 
restr ict ion that appl ies to people. Why 
should computer programs be more l imited? 
A better model of comprehension 
therefore is one that uses a coherent set of 
processes operat ing upon information of 
di f ferent varieties. When this is done it 
becomes clearer that the real problems of 
computat ional  understanding involves 
quest ions like: what information is 
necessary for understanding a part icular 
text, how does the text cue in this 
information, how is general informat ion 
"tuned" to the current context, how is 
informat ion removed from play, and so on. 
These quest ions must be asked for all the 
dif ferent kinds of information that are 
used. 
Notice that these quest ions are the 
same ones that must be asked about ANY model 
ii 
of memory processes. The reason for this is 
obvious: COMPREHENSION IS A MEMORY PROCESS. 
This simple statement has several impor tant  
impl icat ions about what a comprehension 
model should look like. Comprehension as a 
memory process implies a set of concerns 
very different from those that arose when 
natural  language processing was looked at by 
l inguistics. It implies that the answers 
involve the generat ion of simple mechanisms 
and large data bases. It implies that these 
mechanisms should either be or at least look 
like the mechanisms used for common-sense 
reasoning. It implies that the information 
in the data bases should be organized for 
usefulness -- i.e., so that textual cues 
lead to the RAPID retr ieval  of ALL the 
RELEVANT information -- rather than for 
uni formity -- e.g., syntax in one place, 
semantics in another. 
The next section of this paper is 
concerned with a system of analysis 
mechanisms that I have been developing. 
While the discussion is l imited pr imari ly to 
the problem of computat ional  understanding, 
I hope it wil l  be clear that both the 
mechanisms and the organizat ion of the data 
base given are part of a more general model 
of human memory. 
