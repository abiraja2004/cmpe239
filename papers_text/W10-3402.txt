Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010), pages 2?11,
Beijing, August 2010
SemanticNet-Perception of Human Pragmatics   
Amitava Das1 and Sivaji Bandyopadhyay2 
Department of Computer Science and Engineering  
Jadavpur University 
amitava.santu@gmail.com1 sivaji_cse_ju@yahoo.com2  
 
Abstract 
SemanticNet is a semantic network of 
lexicons to hold human pragmatic 
knowledge. So far Natural Language 
Processing (NLP) research patronized 
much of manually augmented lexicon 
resources such as WordNet. But the 
small set of semantic relations like 
Hypernym, Holonym, Meronym and 
Synonym etc are very narrow to cap-
ture the wide variations human cogni-
tive knowledge. But no such informa-
tion could be retrieved from available 
lexicon resources. SemanticNet is the 
attempt to capture wide range of con-
text dependent semantic inference 
among various themes which human 
beings perceive in their pragmatic 
knowledge, learned by day to day cog-
nitive interactions with the surrounding 
physical world. SemanticNet holds 
human pragmatics with twenty well es-
tablished semantic relations for every 
pair of lexemes. As every pair of rela-
tions cannot be defined by fixed num-
ber of certain semantic relation labels 
thus additionally contextual semantic 
affinity inference in SemanticNet could 
be calculated by network distance and 
represented as a probabilistic score. 
SemanticNet is being presently devel-
oped for Bengali language. 
1 Historical Motivation 
Semantics (from Greek "??????????" - seman-
tikos) is the study of meaning, usually in lan-
guage. The word "semantics" itself denotes a 
range of ideas, from the popular to the highly 
technical. It is often used in ordinary language 
to denote a problem of understanding that 
comes down to word selection or connotation. 
We studied with various Psycholinguistics ex-
periments to understand how human natural 
intelligence helps to understand general se-
mantic from nature. Our study was to under-
stand the human psychology about semantics 
beyond language. We were haunting for the 
intellectual structure of the psychological and 
neurobiological factors that enable humans to 
acquire, use, comprehend and produce natural 
languages. Let?s come with an example of 
simple conversation about movie between two 
persons. 
Person A: Have you seen the 
movie ?No Man's Land?? How 
is it? 
Person B: Although it is 
good but you should see 
?The Hurt Locker?? 
May be the conversation looks very casual, 
but our intension was to find out the direction 
of the decision logic on the Person B?s brain. 
We start digging to find out the nature of hu-
man intelligent thinking. A prolonged discus-
sion with Person B reveals that the decision 
logic path to recommend a good movie was as 
the Figure 1. The highlighted red paths are the 
shortest semantic affinity distances of the hu-
man brain. 
We call it semantic thinking. Although the 
derivational path of semantic thinking is not 
such easy as we portrait in Figure 1 but we 
keep it easier for understandability. Actually a 
human try to figure out the closest semantic 
affinity node into his pragmatics knowledge by 
natural intelligence. In the previous example 
Person B find out with his intelligence that No 
Man's Land is a war movie and got Oscar 
2
award. Oscar award generally cracked by Hol-
lywood movies and thus Person B start search-
ing his pragmatics network to find out a movie 
fall into war genre, from Hollywood and may 
be got Oscar award. Person B finds out the 
name of a movie The Hurt Locker at nearer 
distance into his pragmatics knowledge net-
work which is an optimized recommendation 
that satisfy all the criteria. Noticeably Person B 
didn?t choice the other paths like Bollywood, 
Foreign movie etc. 
 
Figure 1: Semantic Thinking 
And thus our aim was to develop a computa-
tional lexicon structure for semantics as human 
pragmatics knowledge. We spare long time to 
find out the most robust structure to represent 
pragmatics knowledge properly and it should 
be easy understandable for next level of search 
and usability. 
We look into literature that probably direct 
to the direction of our ideological thinking. We 
found that in the year of 1996 Push Singh and 
Marvin Minsky proposed the field has shat-
tered into subfields populated by researchers 
with different goals and who speak very differ-
ent technical languages. Much has been 
learned, and it is time to start integrating what 
we've learned, but few researchers are widely 
versed enough to do so. They had a proposal 
for how to do so in their ConceptNet work. 
They developed lexicon resources like Con-
ceptNet (Liu and Singh, 2004). ConceptNet- 
ConceptNet is a large-scale semantic network 
(over 1.6 million links) relating a wide variety 
of ordinary objects, events, places, actions, and 
goals by only 20 different link types, mined 
from corpus. 
The present task of developing SemanticNet 
is to capture semantic affinity knowledge of 
human pragmatics as a lexicon database.  We 
extend our vision from the human common 
sense (as in ConceptNet) to human pragmatics 
and have proposed semantic relations for every 
pair of lexemes that cannot be defined by fixed 
number of certain semantic relation labels. 
Contextual semantic affinity inference in Se-
manticNet could be calculated by network dis-
tance and represented as a probabilistic score. 
SemanticNet is being presently developed for 
Bengali language. 
2 Semantic Roles 
The ideological study of semantic roles started 
age old ago since Panini?s karaka theory that 
assigns generic semantic roles to words in a 
natural language sentence. Semantic roles are 
generally domain specific in nature such as 
FROM_DESTINATION,TO_DESTINATION, 
DEPARTURE_TIME etc. Verb-specific se-
mantic roles have also been defined such as 
EATER and EATEN for the verb eat. The 
standard datasets that are used in various Eng-
lish SRL systems are: PropBank (Palmer et al, 
2005), FrameNet (Fillmore et al, 2003) and 
VerbNet (Kipper et al, 2006). These collec-
tions contain manually developed well-trusted 
gold reference annotations of both syntactic 
and predicate-argument structures.  
PropBank defines semantic roles for each 
verb. The various semantic roles identified 
(Dowty, 1991) are Agent, patient or theme etc. 
In addition to verb-specific roles, PropBank 
defines several more general roles that can ap-
ply to any verb (Palmer et al, 2005). 
FrameNet is annotated with verb frame se-
mantics and supported by corpus evidence. 
The frame-to-frame relations defined in Fra-
meNet are Inheritance, Perspective_on, Sub-
frame, Precedes, Inchoative_of, Causative_of 
and Using. Frame development focuses on pa-
raphrasability (or near paraphrasability) of 
words and multi-words.  
VerbNet annotated with thematic roles refer 
to the underlying semantic relationship be-
tween a predicate and its arguments. The se-
mantic tagset of VerbNet consists of tags as 
agent, patient, theme, experiencer, stimulus, 
instrument, location, source, goal, recipient, 
benefactive etc. 
It is evident from the above discussions that 
no adequate semantic role set exists that can be 
defines across various domains. Hence pro-
3
posed SemanticNet does not only rely on fixed 
type of semantics roles as ConceptNet. For 
semantic relations we followed the 20 relations 
defined in ConceptNet. Additionally we pro-
posed semantic relations for every pair of lex-
icons cannot be defined by exact semantic role 
and thus we formulated a probabilistic score 
based technique. Semantic affinity in Seman-
ticNet could be calculated by network distance. 
Details could be found in relevant Section 8. 
3 Corpus 
Present SemanticNet has been developed for 
Bengali language. Resource acquisition is one 
of the most challenging obstacles to work with 
electronically resource constrained languages 
like Bengali. Although Bengali is the sixth1 
popular language in the World, second in India 
and the national language in Bangladesh.  
There was another issue drive us long way 
to find out the proper corpus for the develop-
ment of SemanticNet. As the notion is to cap-
ture and store human pragmatic knowledge so 
the hypothesis was chosen corpus should not 
be biased towards any specific domain know-
ledge as human pragmatic knowledge is not 
constricted to any domain rather it has a wide 
spread range over anything related to universe 
and life on earth. Additionally it must be larger 
in size to cover mostly available general con-
cepts related to any topic. After a detail analy-
sis we decided it is better to choose NEWS 
corpus as various domains knowledge like Pol-
itics, Sports, Entertainment, Social Issues, 
Science, Arts and Culture, Tourism, Adver-
tisement, TV schedule, Tender, Comics and 
Weather etc are could be found only in NEWS 
corpus.  
Statistics NEWS 
Total no. of news documents in the 
corpus 108,305 
Total no. of sentences in the corpus 2,822,737 
Avg no. of sentences in a document 27 
Total no. of wordforms in the corpus 33,836,736 
Avg. no. of wordforms in a document 313 
Total no. of distinct wordforms in the 
corpus 467,858 
Table 1:  Bengali Corpus Statistics 
                                                 
1
 
http://en.wikipedia.org/wiki/List_of_languages_by_
number_of_native_speakers 
Fortunately such corpus development could 
be found in (Ekbal and Bandyopadhyay, 2008) 
for Bengali. We obtained the corpus from the 
authors. The Bengali NEWS corpus consisted 
of consecutive 4 years of NEWS stories with 
various sub domains as reported above. For the 
present task we have used the Bengali NEWS 
corpus, developed from the archive of a lead-
ing Bengali NEWS paper 2  available on the 
Web. The NEWS corpus is quite larger in size 
as reported in Table 1. 
4 Annotation 
From the collected document set 200 docu-
ments have been chosen randomly for the an-
notation task. Three annotators (Mr. X, Mr. Y 
and Mr. Z) participated in the present task.  
Annotators were asked to annotate the theme 
words (topical expressions) which best de-
scribe the topical snapshot of the document. 
The agreement of annotations among three 
annotators has been evaluated. The agreements 
of tag values at theme words level is reported 
in Table 2. 
 
Annotators X vs. Y X Vs. Z Y Vs. Z Avg 
Percentage 82.64% 71.78% 80.47% 78.3% 
All Agree 75.45% 
Table 2: Agreement of annotators at theme 
words level 
5 Theme Identification 
Term Frequency (TF) plays a crucial role to 
identify document relevance in Topic-Based 
Information Retrieval. The motivation behind 
developing Theme detection technique is that 
in many documents relevant words may not 
occur frequently or irrelevant words may occur 
frequently. Moreover for the lexicon affinity 
inference, topic or theme words are the only 
strong clue to start with. The Theme detection 
technique has been proposed to resolve these 
issues to identify discourse level most relevant 
thematic nodes in terms of word or lexicon 
using a standard machine learning technique. 
The machine learning technique used here is 
Conditional Random Field (CRF)3. The theme 
word detection has been defined as a sequence 
                                                 
2
 http://www.anandabazar.com/ 
3
 http://crfpp.sourceforge.net 
4
labeling problem using various useful depend-
ing features. Depending upon the series of in-
put features, each word is tagged as either 
Theme Word (TW) or Other (O). 
5.1 Feature Organization 
The set of features used in the present task 
have been categorized as Lexico-Syntactic, 
Syntactic and Discourse level features. These 
are listed in the Table 3 below and have been 
described in the subsequent subsections. 
 
Types Features 
Lexico-Syntactic 
POS 
Frequency 
Stemming 
Syntactic Chunk Label Dependency Parsing Depth 
Discourse Level 
Title of the Document 
First Paragraph 
Term Distribution 
Collocation 
Table 3: Features 
5.2 Lexico-Syntactic Features 
5.2.1 Part of Speech (POS) 
It has been shown by Das and Bandyopadhyay, 
(2009), that theme bearing words in sentences 
are mainly adjective, adverb, noun and verbs 
as other POS categories like pronoun, preposi-
tion, conjunct, article etc. have no relevance 
towards thematic semantic of any document. 
The detail of the POS tagging system chosen 
for the present task could be found in (Das and 
Bandyopadhyay 2009). 
5.3 Frequency 
Frequency always plays a crucial role in identi-
fying the importance of a word in the docu-
ment or corpus. The system generates four 
separate high frequent word lists after function 
words are removed for four POS categories: 
adjective, adverb, verb and noun. Word fre-
quency values are then effectively used as a 
crucial feature in the Theme Detection tech-
nique. 
5.4 Stemming 
Several words in a sentence that carry thematic 
information may be present in inflected forms. 
Stemming is necessary for such inflected 
words before they can be searched in appropri-
ate lists. Due to non availability of good stem-
mers in Indian languages especially in Bengali, 
a stemmer based on stemming cluster tech-
nique has been used as described in (Das and 
Bandyopadhyay, 2010). This stemmer analyz-
es prefixes and suffixes of all the word forms 
present in a particular document. Words that 
are identified to have the same root form are 
grouped in a finite number of clusters with the 
identified root word as cluster center.  
5.5 Syntactic Features 
5.5.1 Chunk Label 
We found that Chunk level information is very 
much effective to identify lexicon inference 
affinity. As an example: 
 
( 	)/NP (
 
)/NP ()/NP 
()/JJP (?)/SYM 
The movies released by Sa-
tyajit Roy are excellent. 
 
In the above example two lexicons 
?/release? and ?/movie? are collo-
cated in a chunk and they are very much se-
mantically neighboring in human pragmatic 
knowledge. Chunk feature effectively used in 
supervised classifier. Chunk labels are defined 
as B-X (Beginning), I-X (Intermediate) and E-
X (End), where X is the chunk label. In the 
task of identification of Theme expressions, 
chunk label markers play a crucial role. Fur-
ther details of development of chunking sys-
tem could be found in (Das and Bandyopad-
hyay 2009).  
5.5.2 Dependency Parser 
Dependency depth feature is very useful to 
identify Theme expressions. A particular 
Theme word generally occurs within a particu-
lar range of depth in a dependency tree. Theme 
expressions may be a Named Entity (NE: per-
son, organization or location names), a com-
mon noun (Ex: accident, bomb blast, strike etc) 
or words of other POS categories. It has been 
observed that depending upon the nature of 
Theme expressions it can occur within a cer-
tain depth in the dependency tree in the sen-
tences. A statistical dependency parser has 
5
been used for Bengali as described in (Ghosh 
et al, 2009). 
5.6 Discourse Level Features 
5.6.1 Positional Aspect 
Depending upon the position of the thematic 
clue, every document is divided into a number 
of zones. The features considered for each 
document are Title words of the document, the 
first paragraph words and the words from the 
last two sentences. A detailed study was done 
on the Bengali news corpus to identify the 
roles of the positional aspect features of a doc-
ument (first paragraph, last two sentences) in 
the detection of theme words. The importance 
of these positional features has been described 
in the following section.  
5.6.2 Title Words 
It has been observed that the Title words of a 
document always carry some meaningful the-
matic information. The title word feature has 
been used as a binary feature during CRF 
based machine learning. 
5.6.3 First Paragraph Words 
People usually give a brief idea of their beliefs 
and speculations about any related topic or 
theme in the first paragraph of the document 
and subsequently elaborate or support their 
ideas with relevant reasoning or factual infor-
mation. Hence first paragraph words are in-
formative in the detection of Thematic Expres-
sions.  
5.6.4 Words From Last Two Sentences 
It is a general practice of writing style that 
every document concludes with a summary of 
the overall story expressed in the document. 
We found that it is very obvious that every 
document ended with dense theme/topic words 
in the last two sentences. 
5.6.5 Term Distribution Model 
An alternative to the classical TF-IDF weight-
ing mechanism of standard IR has been pro-
posed as a model for the distribution of a word. 
The model characterizes and captures the in-
formativeness of a word by measuring how 
regularly the word is distributed in a document. 
Thus the objective is to estimate  that measures 
the distribution pattern of the k occurrences of 
the word wi in a document d. Zipf's law de-
scribes distribution patterns of words in an en-
tire corpus. In contrast, term distribution mod-
els capture regularities of word occurrence in 
subunits of a corpus (e.g., documents, para-
graphs or chapters of a book). A good under-
standing of the distribution patterns is useful to 
assess the likelihood of occurrences of a theme 
word in some specific positions (e.g., first pa-
ragraph or last two sentences) of a unit of text. 
Most term distribution models try to character-
ize the informativeness of a word identified by 
inverse document frequency (IDF). In the 
present work, the distribution pattern of a word 
within a document formalizes the notion of 
theme inference informativeness. This is based 
on the Poisson distribution. Significant Theme 
words are identified using TF, Positional and 
Distribution factor. The distribution function 
for each theme word in a document is eva-
luated as follows: 
( )1 1
1 1
( ) / ( ) /
n n
d i i i i i
i i
f w S S n TW TW n
? ?
= =
= ? + ?? ?  
where n=number of sentences in a document 
with a particular theme word Si=sentence id of 
the current sentence containing the theme word 
and Si-1=sentence id of the previous sentence 
containing the query term, iTW is the positional 
id of current Theme word and 1iTW ? is the posi-
tional id of the previous Theme word. 
5.6.6 Collocation 
Collocation with other thematic 
words/expressions is undoubtedly an important 
clue for identification of theme sequence pat-
terns in a document. As we used chunk level 
collocation to capture thematic words (as de-
scribed in 5.5.1) and in this section we are in-
troducing collocation feature as inter-chunk 
collocation or discourse level collocation with 
various granularity as sentence level, para-
graph level or discourse level.  
6 Theme Clustering 
Theme clustering algorithms partition a set of 
documents into finite number of topic based 
groups or clusters in terms of theme 
words/expressions. The task of document clus-
tering is to create a reasonable set of clusters 
6
for a given set of documents. A reasonable 
cluster is defined as the one that maximizes the 
within-cluster document similarity and mini-
mizes between-cluster similarities. There are 
two principal motivations for the use of this 
technique in the theme clustering setting: effi-
ciency, and the cluster hypothesis. 
The cluster hypothesis (Jardine and van 
Rijsbergen, 1971) takes this argument a step 
further by asserting that retrieval from a clus-
tered collection will not only be more efficient, 
but will in fact improve retrieval performance 
in terms of recall and precision. The basic no-
tion behind this hypothesis is that by separat-
ing documents according to topic, relevant 
documents will be found together in the same 
cluster, and non-relevant documents will be 
avoided since they will reside in clusters that 
are not used for retrieval. Despite the plausibil-
ity of this hypothesis, there is only mixed ex-
perimental support for it. Results vary consi-
derably based on the clustering algorithm and 
document collection in use (Willett, 1988). We 
employ the clustering hypothesis only to 
measure inter-document level thematic affinity 
inference on semantics. 
Application of the clustering technique to 
the three sample documents results in the fol-
lowing theme-by-document matrix, A, where 
the rows represent various documents and the 
columns represent the themes politics, sport, 
and travel.  
election cricket hotel
A parliament sachin vacation
governor soccer tourist
? ?
? ?
= ? ?
? ?? ?
 
The similarity between vectors is calculated 
by assigning numerical weights to these words 
and then using the cosine similarity measure as 
specified in the following equation.  
, ,
1
, .
N
k j k j i k i j
i
s q d q d w w
? ? ? ?
=
? ?
= = ?? ?? ? ? ---- (1) 
This equation specifies what is known as the 
dot product between vectors.  Now, in general, 
the dot product between two vectors is not par-
ticularly useful as a similarity metric, since it is 
too sensitive to the absolute magnitudes of the 
various dimensions. However, the dot product 
between vectors that have been length norma-
lized has a useful and intuitive interpretation: it 
computes the cosine of the angle between the 
two vectors. When two documents are identic-
al they will receive a cosine of one; when they 
are orthogonal (share no common terms) they 
will receive a cosine of zero. Note that if for 
some reason the vectors are not stored in a 
normalized form, then the normalization can 
be incorporated directly into the similarity 
measure as follows.  
Of course, in situations where the document 
collection is relatively static, it makes sense to 
normalize the document vectors once and store 
them, rather than include the normalization in 
the similarity metric. 
, ,1
2 2
, ,1 1
,
N
i k i ji
k j N N
i k i ki i
w w
s q d
w w
? ?
=
= =
?? ?
=? ?? ? ?
?
? ?  ----(2) 
Calculating the similarity measure and using 
a predefined threshold value, documents are 
classified using standard bottom-up soft clus-
tering k-means technique. The predefined thre-
shold value is experimentally set as 0.5 as 
shown in Table 4. 
 
ID Theme 1 2 3 
1 	
 (administration) 0.63 0.12 0.04 
1  (good-government) 0.58 0.11 0.06 
1  (society) 0.58 0.12 0.03 
1  (law) 0.55 0.14 0.08 
2  (research) 0.11 0.59 0.02 
2  (college) 0.15 0.55 0.01 
2 	 (higher study) 0.12 0.66 0.01 
3  (jehadi) 0.13 0.05 0.58 
3  (mosque) 0.05 0.01 0.86 
3 	  (New Delhi) 0.12 0.04 0.65 
3 	 (Kashmir) 0.03 0.01 0.93 
Table 4: Five cluster centroids (mean j?
?
). 
A set of initial cluster centers is necessary in 
the beginning. Each document is assigned to 
the cluster whose center is closest to the doc-
ument. After all documents have been as-
signed, the center of each cluster is recom-
puted as the centroid or mean ?
?
 (where ?
?
 is 
7
the clustering coefficient) of its members that 
is ( )1/
jj x c
c x?
? ?
?
= ? . The distance function 
is the cosine vector similarity function. 
Table 4 gives an example of theme centroids 
by the K-means clustering. Bold words in 
Theme column are cluster centers. Cluster cen-
ters are assigned by maximum clustering coef-
ficient. For each theme word, the cluster from 
Table 4 is still the dominating cluster. For ex-
ample, ?	
? has a higher membership 
probability in cluster1 than in other clusters. 
But each theme word also has some non-zero 
membership in all other clusters. This is useful 
for assessing the strength of association be-
tween a theme word and a topic. Comparing 
two members of the cluster2, ?	? and 
?	?, it is seen that ?	? is strongly 
associated with cluster2 (p=0.65) but it has 
some affinity with other clusters as well (e.g., 
p =0.12 with the cluster1). This is a good ex-
ample of the utility of soft clustering. These 
non-zero values are still useful for calculating 
vertex weight during Semantic Relational 
Graph generation. 
7 Semantic Relational Graph 
Representation of input text document(s) in the 
form of graph is the key to our design prin-
ciple. The idea is to build a document graph 
G=<V,E> from a given source document 
d D? . At this preprocessing stage, text is 
tokenized, stop words are eliminated, and 
words are stemmed. Thus, the text in each 
document is split into fragments and each 
fragment is represented with a vector of consti-
tuent theme words. These text fragments be-
come the nodes V in the document graph. 
The similarity between two nodes is ex-
pressed as the weight of each edge E of the 
document graph. A weighted edge is added to 
the document graph between two nodes if they 
either correspond to adjacent text fragments in 
the text or are semantically related by theme 
words. The weight of an edge denotes the de-
gree of the semantic inference relationship. 
The weighted edges not only denote document 
level similarity between nodes but also inter 
document level similarity between nodes. Thus 
to build a document graph G, only the edges 
with edge weight greater than some predefined 
threshold value are added to G, which basical-
ly constitute edges E of the graph G. 
The Cosine similarity measure has been 
used here. In cosine similarity, each document 
d is denoted by the vector ( )V d?  derived from 
d, with each component in the vector for each 
Theme words. The cosine similarity between 
two documents (nodes) d1 and d2 is computed 
using their vector representations ( 1)V d
?
and 
( 2)V d
?
as equation (1) and (2) (Described in 
Section 6). Only a slight change has been done 
i.e. the dot product of two vec-
tors ( 1) ( 2)V d V d
? ?
? is defined as
1
( 1) ( 2)
M
i
V d V d
=
? . 
The Euclidean length of d is defined to 
be
2
1
( )
M
ii
dV=
??  where M is the total number of 
documents in the corpus. Theme nodes within 
a cluster are connected by vertex, weight is 
calculated by clustering co-efficient of those 
theme nodes. Additionally inter cluster vertex-
es are there. Cluster centers are interconnected 
with weighted vertex. The weight is calculated 
by cluster distance as measured by cosine simi-
larity measure as discussed earlier. 
To better aid our understanding of the auto-
matically determined category relationships we 
visualized this network using the Fruchterman-
Reingold force directed graph layout algorithm 
(Fruchterman and Reingold, 1991) and the 
NodeXL network analysis tool (Smith et al, 
2009)4. A theme relational model graph drawn 
by NoddeXL is shown in Figure 1. 
8 Semantic Distance Measurement 
Finally generated semantic relational graph is 
the desired SemanticNet that we proposed. 
Generated Bengali SemanticNet consist of al-
most 90K high frequent Bengali lexicons. Only 
four categories of POS (noun, adjective, ad-
verb and verb) considered for present genera-
tion as reported in Section 5.2.1. In the gener-
ated Bengali SemanticNet al the lexicons are 
connected with weighted vertex either directly 
                                                 
4
 Available from 
http://www.codeplex.com/NodeXL 
8
 Figure 1: Semantic Relational Graph by NodeXL 
or indirectly. Semantic lexicon inference could 
be identified by network distance of any two 
nodes by calculating the distance in terms of 
weighted vertex. We computed the relevance 
of semantic lexicon nodes by summing up the 
edge scores of those edges connecting the node 
with other nodes in the same cluster. As cluster 
centers are also interconnected with weighted 
vertex so inter-cluster relations could be also 
calculated in terms of weighted network dis-
tance between two nodes within two separate 
clusters. As an example: 
 
Figure 2: Semantic Affinity Graph 
The lexicon semantic affinity inference from 
Figure 2 could be calculated as follows: 
0
0
0
0
( , )              ----(1) or
                 =  ---(2)
n
kk
d i j
n
m
m kk
cc
c
v
S w w
k
v
l
k
=
=
=
=
=
?
?
?? ?
 
where ( , )d i jS w w =  semantic affinity dis-
tance between two lexicons wi and wj. Equa-
tion (1) and (2) are for intra-cluster and inter-
cluster semantic distance measure respectively. 
k=number of weighted vertex between two 
lexicons wi and wj. vk is the weighted vertex 
between two lexicons. m=number of cluster 
centers between two lexicons. lc is the distance 
between cluster centers between two lexicons. 
For illustration of present technique let take 
an example: 
(Argentina, goal)= 0.5 0.3 0.4
2
+
=  
(Gun, goal)= 0.22 0.5 0.0
1 1
? ?
+ ?? ?? ? =0 
It is evident from the previous example that 
the score based semantic distance can better 
illustrate lexicon affinity between Argentina 
and goal but is no lexicon affinity relation be-
tween gun and goal. 
Instead of giving only certain semantic rela-
tions like WordNet or ConceptNet the present 
relative probabilistic score based lexicon affin-
ity distance based technique can represent best 
acceptable solution for representing the human 
pragmatic knowledge. Not only ideologically 
rather the SemanticNet provide a good solution 
to any type of NLP problem. A detail analysis 
of Information retrieval system using Seman-
ticNet is detailed in evaluation section.  
Although every lexicon pair cannot be la-
beled by exact semantic role but we try to keep 
a few semantic roles to establish a crossroad 
from previous computational lexicon tech-
niques to this new one. These semantic rela-
tions may be treated as a bridge to traverse 
SemanticNet by gathering knowledge from 
other resources WordNet and ConceptNet. 
Approximately 22k (24% of overall Seman-
ticNet) lexicons are tagged with appropriate 
semantic roles by two processes as described 
below. 
9
9 Semantic Role Assignment 
Two types of methods have been taken to as-
sign pair wise lexicon semantic affinity rela-
tions. First one is derived from ConceptNet. In 
the second technique sub-graph is identified 
consisting of a nearest verb and roles are as-
signed accordingly.  
9.1 Semantic Roles from ConceptNet 
A ConceptNet API5 written in Java has been 
used to extract pair wise relation from Con-
ceptNet. A Bengali-English dictionary (ap-
proximately 102119 entries) has been devel-
oped using the Samsad Bengali-English dictio-
nary6 used here for equivalent lookup of Eng-
lish meaning of each Bengali lexicon. Ob-
tained semantic relations from ConceptNet for 
any lexicon English pair are assigned to source 
Bengali pair lexicons. As an example: 
(?Tree?,?Gree?) (?!?,??) 
 
 OftenNear 
 PartOf 
 PropertyOf 
 IsA 
9.2 Verb Sub-Graph Identification 
It is an automatic process using manually 
augmented list of only 220 Bengali verbs. This 
process starts from any arbitrary node of any 
cluster and start finding any nearest verb with-
in the cluster. The system uses the manually 
augmented list of verbs as partly reported in 
Table 5. 
Verb English Gloss Probable Relations 
	 Be IsA 
! Have CapableOf 
" Have CapableOf 
bn_aikaar Made MadeOf 
 Live LocationOf 
Table 5: Semantic Relations 
The semantic relation labels attached with 
every verb in the manually augmented list (as 
reported in Table 5) is then automatically as-
signed between each pair of lexicons.  
                                                 
5
 http://web.media.mit.edu/~hugo/conceptnet/ 
6
 
http://dsal.uchicago.edu/dictionaries/biswas_bengal
i/ 
10 Evaluation 
It is bit difficult to evaluate this type of lexicon 
resources automatically. Manual validation 
may be suggested as a better alternative but we 
prefer for a practical implementation based 
evaluation strategy.  
For evaluation of Bengali SemanticNet it is 
used in Information Retrieval task using cor-
pus from Forum for Information Retrieval 
Evaluation (FIRE) 7  ad-hoc mono-lingual in-
formation retrieval task for Bengali language. 
Two different strategies have been taken. First 
a standard IR technique with TF-IDF, zonal 
indexing and ranking based technique (Ban-
dyopadhyay et al, 2008) has been taken. 
Second technique uses more or less same strat-
egy along with query expansion technique us-
ing SemanticNet (Although the term Seman-
ticNet was not mentioned there) as a resource 
(Bhaskar et al, 2010).  
Only the following evaluation metrics have 
been listed for each run: mean average preci-
sion (MAP), Geometric Mean Average Preci-
sion (GM-AP), (document retrieved relevant 
for the topic) R-Precision (R-Prec), Binary pre-
ferences (Bpref) and Reciprical rank of top 
relevant document (Recip_Rank). The evalua-
tion strategy follows the global standard as 
Text Retrieval Conference (TREC)8 metrics. It 
is clearly evident from the system results as 
reported in Table 6 that SemanticNet is a better 
way to solve lexicon semantic affinity.  
Scores 
Bengali IR using 
IR SemanticNet 
MAP 0.0200 0.4002 
GM_AP 0.0004 0.3185 
R-Prec 0.0415 0.3894 
Bpref 0.0583 0.3424 
Recip_Rank 0.4432 0.6912 
Table 6: Information Retrieval using Seman-
ticNet 
Evaluation result shows effectiveness of de-
veloped SemanticNet in IR. Further analysis 
                                                 
7
 http://www.isical.ac.in/~clia/index.html 
8
 http://trec.nist.gov/ 
10
revealed that general query expansion tech-
nique generally used WordNet synonyms as a 
resource. But in reality ?$	? and ??? 
could not be clustered in one cluster though 
they represent same semantic of ?heart?. First 
one used in general context whereas the second 
one used only in literature. If there is any 
problem to understand Bengali let come with 
an example of English. Conceptually "you" 
and "thy" could be mapped in same cluster as 
they both represent the semantic of 2nd person 
but in reality "thy" simply refers to the 
literature of the great English poet Shakes-
peare. Standard lexicons cannot discriminate 
this type of fine-grained semantic differences. 
11 Conclusion and Future Task 
Experimental result of Information Retrieval 
using SemanticNet proves it is a better solution 
rather than any existing lexicon resources. The 
development strategy employs less human in-
terruption rather a general architecture of 
Theme identification or Theme Clustering 
technique using easily extractable linguistics 
knowledge. The proposed technique could be 
replicated for any new language.  
SemanticNet could be useful any kind of In-
formation Retrieval technique, Information 
Extraction technique, and topic based Summa-
rization and we hope for newly identified NLP 
sub disciplines such as Stylometry or Author-
ship detection and plagiarism detection etc. 
Our future task will be in the direction of 
different experiments of NLP as mentioned 
above to profoundly establish the efficiency of 
SemanticNet. Furthermore we will try to de-
velop SemanticNet for many other languages. 
References 
Bandhyopadhyay S., Das A., Bhaskar P.. English 
Bengali Ad-hoc Monolingual Information Re-
trieval Task Result at FIRE 2008. In Working 
Note of Forum for FIRE-2008. 
Bhaskar P., Das A.,Pakray P.and Bandyopadhyay 
S.(2010). Theme Based English and Bengali Ad-
hoc Monolingual Information Retrieval in FIRE 
2010, In FIRE-2010. 
Das A. and Bandyopadhyay S. (2009). Theme De-
tection an Exploration of Opinion Subjectivity. 
In Proceeding of Affective Computing & Intelli-
gent Interaction (ACII). 
Das A. and Bandyopadhyay S. (2010). Morpholog-
ical Stemming Cluster Identification for Bangla, 
In Knowledge Sharing Event-1: Task 3: Mor-
phological Analyzers and Generators, January, 
2010, Mysore. 
Ekbal A., Bandyopadhyay S (2008). A Web-based 
Bengali News Corpus for Named Entity Recog-
nition. Language Resources and Evaluation 
Journal. pages 173-182, 2008 
Fillmore Charles J., Johnson Christopher R., and 
Petruck Miriam R. L.. 2003. Background to 
FrameNet. International Journal of Lexicogra-
phy, 16:235?250. 
Fruchterman Thomas M. J. and  Reingold Edward 
M.(1991). Graph drawing by force-directed 
placement. Software: Practice and Experience, 
21(11):1129?1164. 
Ghosh A., Das A., Bhaskar P., Bandyopadhyay 
S.(2009). Dependency Parser for Bengali: the JU 
System at ICON 2009. In NLP Tool Contest 
ICON 2009, December 14th-17th, Hyderabad. 
Jardine, N. and van Rijsbergen, C. J. (1971). The 
use of hierarchic clustering in information re-
trieval. Information Storage and Retrieval, 7, 
217-240. 
Kipper Karin, Korhonen Anna, Ryant Neville, and 
Palmer Martha. Extending VerbNet with Novel 
Verb Classes. LREC 2006.  
Liu Hugo and Singh Push (2004). ConceptNet: a 
practical commonsense reasoning toolkit. BT 
Technology Journal, 22(4):211-226. 
Palmer Martha, Gildea Dan, Kingsbury Paul, The 
Proposition Bank: A Corpus Annotated with 
Semantic Roles, Computational Linguistics 
Journal, 31:1, 2005. 
Singh Push and Williams William (2003). LifeNet: 
a propositional model of ordinary human activi-
ty. In the Proc. Of DC-KCAP 2003.  
Singh Push, Barry Barbara, and Liu Hugo (2004). 
Teaching machines about everyday life. BT 
Technology Journal, 22(4):227-240. 
Smith Marc, Ben Shneiderman, Natasa Milic-
Frayling, Eduarda Mendes Rodrigues, Vladimir 
Barash, Cody Dunne, Tony Capone, Adam Per-
er, and Eric Gleave. 2009. Analyzing (social 
media) networks with NodeXL. In C&T ?09: 
Proc. Fourth International Conference on Com-
munities and Technologies, LNCS.  Springer. 
Willerr, P. (1988). Recent trends in hierarchic doc-
ument clustering: A critical review. Information 
Processing and Management, 24(5), 577-597. 
11
