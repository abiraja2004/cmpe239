Antonymy and Conceptual Vectors
Didier Schwab, Mathieu Lafourcade and Violaine Prince
LIRMM
Laboratoire d?informatique, de Robotique
et de Microe?lectronique de Montpellier
MONTPELLIER - FRANCE.
{schwab,lafourca,prince}@lirmm.fr
http://www.lirmm.fr/ ?{schwab, lafourca, prince}
Abstract
For meaning representations in NLP, we focus
our attention on thematic aspects and concep-
tual vectors. The learning strategy of concep-
tual vectors relies on a morphosyntaxic analy-
sis of human usage dictionary definitions linked
to vector propagation. This analysis currently
doesn?t take into account negation phenomena.
This work aims at studying the antonymy as-
pects of negation, in the larger goal of its inte-
gration into the thematic analysis. We present a
model based on the idea of symmetry compat-
ible with conceptual vectors. Then, we define
antonymy functions which allows the construc-
tion of an antonymous vector and the enumer-
ation of its potentially antinomic lexical items.
Finally, we introduce a measure which evaluates
how a given word is an acceptable antonym for
a term.
1 Introduction
Research in meaning representation in NLP is
an important problem still addressed through
several approaches. The NLP team at LIRMM
currently works on thematic and lexical disam-
biguation text analysis (Laf01). Therefore we
built a system, with automated learning capa-
bilities, based on conceptual vectors for mean-
ing representation. Vectors are supposed to en-
code ?ideas? associated to words or to expres-
sions. The conceptual vectors learning system
automatically defines or revises its vectors ac-
cording to the following procedure. It takes, as
an input, definitions in natural language con-
tained in electronic dictionaries for human us-
age. These definitions are then fed to a morpho-
syntactic parser that provides tagging and anal-
ysis trees. Trees are then used as an input
to a procedure that computes vectors using
tree geometry and syntactic functions. Thus,
a kernel of manually indexed terms is necessary
for bootstrapping the analysis. The transver-
sal relationships1, such as synonymy (LP01),
antonymy and hyperonymy, that are more or
less explicitly mentioned in definitions can be
used as a way to globally increase the coher-
ence of vectors. In this paper, we describe a
vectorial function of antonymy. This can help
to improve the learning system by dealing with
negation and antonym tags, as they are often
present in definition texts. The antonymy func-
tion can also help to find an opposite thema to
be used in all generative text applications: op-
posite ideas research, paraphrase (by negation
of the antonym), summary, etc.
2 Conceptual Vectors
We represent thematic aspects of textual seg-
ments (documents, paragraph, syntagms, etc)
by conceptual vectors. Vectors have been used
in information retrieval for long (SM83) and
for meaning representation by the LSI model
(DDL+90) from latent semantic analysis (LSA)
studies in psycholinguistics. In computational
linguistics, (Cha90) proposes a formalism for
the projection of the linguistic notion of se-
mantic field in a vectorial space, from which
our model is inspired. From a set of elemen-
tary concepts, it is possible to build vectors
(conceptual vectors) and to associate them to
lexical items2. The hypothesis3 that considers
a set of concepts as a generator to language
has been long described in (Rog52). Polysemic
words combine different vectors corresponding
1well known as lexical functions (MCP95)
2Lexical items are words or expressions which consti-
tute lexical entries. For instance, ?car ? or ?white ant ? are
lexical items. In the following we will (some what) use
sometimes word or term to speak about a lexical item.
3that we call thesaurus hypothesis.
to different meanings. This vector approach
is based on known mathematical properties, it
is thus possible to undertake well founded for-
mal manipulations attached to reasonable lin-
guistic interpretations. Concepts are defined
from a thesaurus (in our prototype applied to
French, we have chosen (Lar92) where 873 con-
cepts are identified). To be consistent with the
thesaurus hypothesis, we consider that this set
constitutes a generator family for the words and
their meanings. This familly is probably not
free (no proper vectorial base) and as such, any
word would project its meaning on it according
to the following principle. Let be C a finite set
of n concepts, a conceptual vector V is a linear
combinaison of elements ci of C. For a meaning
A, a vector V (A) is the description (in exten-
sion) of activations of all concepts of C. For ex-
ample, the different meanings of ?door ? could be
projected on the following concepts (the CON-
CEPT [intensity] are ordered by decreasing val-
ues): V(?door ?) = (OPENING[0.8], BARRIER[0.7],
LIMIT [0.65], PROXIMITY [0.6], EXTERIOR[0.4], IN-
TERIOR[0.39], . . .
In practice, the larger C is, the finer the mean-
ing descriptions are. In return, the computing
is less easy: for dense vectors4, the enumera-
tion of activated concepts is long and difficult
to evaluate. We prefer to select the themati-
cally closest terms, i.e., the neighbourhood. For
instance, the closest terms ordered by increas-
ing distance to ?door ? are: V(?door ?)=?portal ?,
?portiere?, ?opening?, ?gate?, ?barrier ?,. . .
2.1 Angular Distance
Let us define Sim(A,B) as one of the similar-
ity measures between two vectors A et B, of-
ten used in information retrieval (Mor99). We
can express this function as: Sim(A,B) =
cos(A?, B) = A?B?A???B? with ??? as the scalar
product. We suppose here that vector com-
ponents are positive or null. Then, we define
an angular distance DA between two vectors A
and B as DA(A,B) = arccos(Sim(A,B)). In-
tuitively, this function constitutes an evaluation
of the thematic proximity and measures the an-
gle between the two vectors. We would gener-
ally consider that, for a distance DA(A,B) ? pi4
4Dense vectors are those which have very few null
coordinates. In practice, by construction, all vectors are
dense.
(45 degrees) A and B are thematically close and
share many concepts. For DA(A,B) ? pi4 , the
thematic proximity between A and B would be
considered as loose. Around pi2 , they have no
relation. DA is a real distance function. It ver-
ifies the properties of reflexivity, symmetry and
triangular inequality. We have, for example,
the following angles(values are in radian and de-
grees).
DA(V(?tit ?), V(?tit ?))=0 (0)
DA(V(?tit ?), V(?bird ?))=0.55 (31)
DA(V(?tit ?), V(?sparrow ?))=0.35 (20)
DA(V(?tit ?), V(?train ?))=1.28 (73)
DA(V(?tit ?), V(?insect ?))=0.57 (32)
The first one has a straightforward interpreta-
tion, as a ?tit ? cannot be closer to anything else
than itself. The second and the third are not
very surprising since a ?tit ? is a kind of ?sparrow ?
which is a kind of ?bird ?. A ?tit ? has not much
in common with a ?train?, which explains a large
angle between them. One can wonder why there
is 32 degrees angle between ?tit ? and ?insect ?,
which makes them rather close. If we scruti-
nise the definition of ?tit ? from which its vector
is computed (Insectivourous passerine bird with
colorful feather.) perhaps the interpretation of
these values seems clearer. In effect, the the-
matic is by no way an ontological distance.
2.2 Conceptual Vectors Construction.
The conceptual vector construction is based on
definitions from different sources (dictionaries,
synonym lists, manual indexations, etc). Defini-
tions are parsed and the corresponding concep-
tual vector is computed. This analysis method
shapes, from existing conceptual vectors and
definitions, new vectors. It requires a bootstrap
with a kernel composed of pre-computed vec-
tors. This reduced set of initial vectors is man-
ually indexed for the most frequent or difficult
terms. It constitutes a relevant lexical items
basis on which the learning can start and rely.
One way to build an coherent learning system
is to take care of the semantic relations between
items. Then, after some fine and cyclic compu-
tation, we obtain a relevant conceptual vector
basis. At the moment of writing this article,
our system counts more than 71000 items for
French and more than 288000 vectors, in which
2000 items are concerned by antonymy. These
items are either defined through negative sen-
tences, or because antonyms are directly in the
dictionnary. Example of a negative definition:
?non-existence?: property of what does not exist.
Example of a definition stating antonym: ?love?:
antonyms: ?disgust ?, ?aversion?.
3 Definition and Characterisation of
Antonymy
We propose a definition of antonymy compat-
ible with the vectorial model used. Two lexi-
cal items are in antonymy relation if there is
a symmetry between their semantic components
relatively to an axis. For us, antonym construc-
tion depends on the type of the medium that
supports symmetry. For a term, either we can
have several kinds of antonyms if several possi-
bilities for symmetry exist, or we cannot have
an obvious one if a medium for symmetry is not
to be found. We can distinguish different sorts
of media: (i) a property that shows scalar val-
ues (hot and cold which are symmetrical values
of temperature), (ii) the true-false relevance or
application of a property (e.g. existence/non-
existence) (iii) cultural symmetry or opposition
(e.g. sun/moon).From the point of view of lex-
ical functions, if we compare synonymy and
antonymy, we can say that synonymy is the
research of resemblance with the test of sub-
stitution (x is synonym of y if x may replace
y), antonymy is the research of the symmetry,
that comes down to investigating the existence
and nature of the symmetry medium. We have
identified three types of symmetry by relying
on (Lyo77), (Pal76) and (Mue97). Each sym-
metry type characterises one particular type of
antonymy. In this paper, for the sake of clarity
and precision, we expose only the complemen-
tary antonymy. The same method is used for
the other types of antonymy, only the list of
antonymous concepts are different.
3.1 Complementary Antonymy
The complementary antonyms are couples like
event/unevent, presence/absence.
he is present ? he is not absent
he is absent ? he is not present
he is not absent ? he is present
he is not present ? he is absent
In logical terms, we would have:
?x P (x)? ?Q(x) ?x ?P (x)? Q(x)
?x Q(x)? ?P (x) ?x ?Q(x)? P (x)
This corresponds to the exclusive disjunction
relation. In this frame, the assertion of one
of the terms implies the negation of the other.
Complementary antonymy presents two kinds
of symmetry, (i) a value symmetry in a boolean
system, as in the examples above and (ii) a sym-
metry about the application of a property (black
is the absence of color, so it is ?opposed? to all
other colors or color combinaisons).
4 Antonymy Functions
4.1 Principles and Definitions.
The aim of our work is to create a function
that would improve the learning system by sim-
ulating antonymy. In the following, we will be
mainly interested in antonym generation, which
gives a good satisfaction clue for these functions.
We present a function which, for a given lex-
ical item, gives the n closest antonyms as the
neighbourhood function V provides the n clos-
est items of a vector. In order to know which
particular meaning of the word we want to op-
pose, we have to assess by what context mean-
ing has to be constrained. However, context is
not always sufficient to give a symmetry axis
for antonymy. Let us consider the item ?father ?.
In the ?family? context, it can be opposite to
?mother ? or to ?children? being therefore ambigu-
ous because ?mother ? and ?children? are by no way
similar items. It should be useful, when context
cannot be used as a symmetry axis, to refine
the context with a conceptual vector which is
considered as the referent. In our example, we
should take as referent ?filiation?, and thus the
antonym would be ?children? or the specialised
similar terms (e.g. ?sons? , ?daughters?) ?marriage?
or ?masculine? and thus the antonym would be
?mother ?.
The function AntiLexS returns the n closest
antonyms of the word A in the context defined
by C and in reference to the word R.
AntiLexS(A,C,R, n)
AntiLexR(A,C, n) = AntiLexS(A,C,C, n)
AntiLexB(A,R, n) = AntiLexS(A,R,R, n)
AntiLexA(A, n) = AntiLexS(A,A,A, n)
The partial function AntiLexR has been de-
fined to take care of the fact that in most cases,
context is enough to determine a symmetry axis.
AntiLexB is defined to determine a symmetry
axis rather than a context. In practice, we have
AntiLexB = AntiLexR. The last function is
the absolute antonymy function. For polysemic
words, its usage is delicate because only one
word defines at the same time three things: the
word we oppose, the context and the referent.
This increases the probability to get unsatis-
factory results. However, given usage habits,
we should admit that, practically, this function
will be the most used. It?s sequence process is
presented in picture 1. We note Anti(A,C) the
ITEMS
ANTONYMOUS
CONCEPTUAL VECTOR
CALCULATION
IDENTIFICATION
OF THE CLOSEST 
ITEMS
neighbourhood
CONCEPTUAL VECTORS
strong contextualisation
CALCULATION
X, C, R
X1, X2, ..., Xn
ITEMS
VAnti
VECTOR
ANTONYMOUS
OF THEanti
Vcx, Vcr
VECTORS
CORRESPONDING
OF THE
Figure 1: run of the functions AntiLex
antonymy function at the vector level. Here,
A is the vector we want to oppose and C the
context vector.
Items without antonyms: it is the case
of material objects like car, bottle, boat, etc.
The question that raises is about the continu-
ity the antonymy functions in the vector space.
When symmetry is at stake, then fixed points
or plans are always present. We consider the
case of these objects, and in general, non op-
posable terms, as belonging to the fixed space
of the symmetry. This allows to redirect the
question of antonymy to the opposable proper-
ties of the concerned object. For instance, if we
want to compute the antonym of a ?motorcycle?,
which is a ROAD TRANSPORT, its opposable prop-
erties being NOISY and FAST, we consider its cat-
egory (i.e. ROAD TRANSPORT) as a fixed point,
and we will look for a road transport (SILEN-
CIOUS and SLOW ), something like a ?bicycle? or
an ?electric car ?. With this method, thanks to
the fixed points of symmetry, opposed ?ideas?
or antonyms, not obvious to the reader, could
be discovered.
4.2 Antonym vectors of concept lists
Anti functions are context-dependent and can-
not be free of concepts organisation. They
need to identify for every concept and for ev-
ery kind of antonymy, a vector considered as
the opposite. We had to build a list of triples
?concept, context, vector?. This list is called
antonym vectors of concept list (AVC).
4.2.1 AVC construction.
The Antonym Vectors of Concepts list is manu-
ally built only for the conceptual vectors of the
generating set. For any concept we can have the
antonym vectors such as:
AntiC(EXISTENCE, V ) = V (NON-EXISTENCE)
AntiC(NON-EXISTENCE, V ) = V (EXISTENCE)
AntiC(AGITATION, V ) = V (INERTIA)? V (REST)
AntiC(PLAY, V ) = V (PLAY)
?V
AntiC(ORDER, V (order) ? V (disorder)) =
V (DISORDER)
AntiC(ORDER, V (classification) ? V (order)) =
V (CLASSIFICATION)
As items, concepts can have, according to
the context, a different opposite vector even
if they are not polysemic. For instance, DE-
STRUCTION can have for antonyms PRESERVA-
TION, CONSTRUCTION, REPARATION or PROTEC-
TION. So, we have defined for each concept, one
conceptual vector which allows the selection of
the best antonym according to the situation.
For example, the concept EXISTENCE has the
vector NON-EXISTENCE for antonym for any con-
text. The concept DISORDER has the vector of
ORDER for antonym in a context constituted by
the vectors of ORDER ?DISORDER5 and has CLAS-
SIFICATION in a context constituted by CLASSI-
FICATION and ORDER.
The function AntiC(Ci, Vcontext) returns for
a given concept Ci and the context defined by
Vcontext , the complementary antonym vector in
the list.
4.3 Construction of the antonym
vector: the Anti Function
4.3.1 Definitions
We define the relative antonymy function
AntiR(A,C) which returns the opposite vec-
tor of A in the context C and the absolute
antonymy function AntiA(A) = AntiR(A,A).
The usage of AntiA is delicate because the lexi-
cal item is considered as being its own context.
We will see in 4.4.1 that this may cause real
problems because of sense selection. We should
stress now on the construction of the antonym
vector from two conceptual vectors: Vitem, for
5? is the normalised sum V = A?B | vi = xi+yi?V ?
the item we want to oppose and the other, Vc,
for the context (referent).
4.3.2 Construction of the Antonym
Vector
The method is to focus on the salient notions in
Vitem and Vc. If these notions can be opposed
then the antonym should have the inverse ideas
in the same proportion. That leads us to define
this function as follows:
AntiR(Vitem, Vc) =
?N
i=1 Pi ?AntiC(Ci, Vc)
with Pi = V 1+CV (Vitem)itemi ?max(Vitemi , Vci)
We crafted the definition of the weight P after
several experiments. We noticed that the func-
tion couldn?t be symmetric (we cannot reason-
ably have AntiR(V(?hot ?),V(?temperature?)) =
AntiR(V(?temperature?),V(?hot ?))). That is why
we introduce this power, to stress more on the
ideas present in the vector we want to oppose.
We note also that the more conceptual6 the vec-
tor is, the more important this power should be.
That is why the power is the variation coeffi-
cient7 which is a good clue for ?conceptuality?.
To finish, we introduce this function max be-
cause an idea presents in the item, even if this
idea is not present in the referent, has to be op-
posed in the antonym. For example, if we want
the antonym of ?cold ? in the ?temperature? con-
text, the weight of ?cold ? has to be important
even if it is not present in ?temperature?.
4.4 Lexical Items and Vectors:
Problem and Solutions
The goal of the functions AntiLex is to return
antonym of a lexical item. They are defined
with the Anti function. So, we have to use tools
which allow the passage between lexical items
and vectors. This transition is difficult because
of polysemy, i.e. how to choose the right relation
between an item and a vector. In other words,
how to choose the good meaning of the word.
4.4.1 Transition lexical items ?
Conceptual Vectors
As said before, antonymy is relative to a con-
text. In some cases, this context cannot be suf-
ficient to select a symmetry axis for antonymy.
6In this paragraph, conceptual means: closeness of a
vector to a concept
7The variation coefficient is SD(V )?(V ) with SD as the
standart deviation and ? as the arithmetic mean.
To catch the searched meaning of the item and,
if it is different from the context, to catch the
selection of the meaning of the referent, we use
the strong contextualisation method. It com-
putes, for a given item, a vector. In this vector,
some meanings are favoured against others ac-
cording to the context. Like this, the context
vector is also contextualised.
This contextualisation shows the problem
caused by the absolute antonymy function
Anti?R . In this case, the method will compute
the vector of the word item in the context item.
This is not a problem if item has only one defini-
tion because, in this case, the strong contextu-
alisation has no effect. Otherwise, the returned
conceptual vector will stress on the main idea it
contains which one is not necessary the appro-
priate one.
4.4.2 Transition Conceptual Vectors ?
Lexical Items
This transition is easier. We just have to com-
pute the neighbourhood of the antonym vector
Vant to obtain the items which are in thematic
antonymy with Vitem. With this method, we
have, for instance:
V(AnticR(death, ?death ? & ?life?))=(LIFE 0.4)
(?killer ? 0.449) (?murderer ? 0.467) (?blood sucker ?
0.471) (?strige? 0.471) (?to die? 0.484) (?to live? 0.486)
V(AnticR(life, ?death ? & ?life?))=(?death ? 0.336)
(DEATH 0.357) (?murdered ? 0.367) (?killer ? 0.377)
(C3:AGE OF LIFE 0.481) (?tyrannicide? 0.516) (?to kill ?
0.579) (?dead ? 0.582)
V(AntiCcA(LIFE))=(DEATH 0.034) (?death ? 0.427)
(C3:AGE OF LIFE 0.551) (?killer ? 0.568) (?mudered ?
0.588) (?tyrannicide? 0.699) (C2:HUMAN 0.737) (?to
kill ? 0.748) (?dead ? 0.77)
It is not important to contextualise the con-
cept LIFE because we can consider that, for ev-
ery context, the opposite vector is the same.
In complementary antonymy, the closest item
is DEATH. This result looks satisfactory. We can
see that the distance between the antonymy vec-
tor and DEATH is not null. It is because our
method is not and cannot be an exact method.
The goal of our function is to build the best
(closest) antonymy vector it is possible to have.
The construction of the generative vectors is the
second explanation. Generative vectors are in-
terdependent. Their construction is based on an
ontology. To take care of this fact, we don?t have
boolean vectors, with which, we would have ex-
actly the same vector. The more polysemic the
term is, the farthest the closest item is, as we
can see it in the first two examples.
We cannot consider, even if the potential of
antonymy measure is correct, the closest lexical
item from Vanti as the antonym. We have to
consider morphological features. Simply speak-
ing, if the antonym of a verb is wanted, the re-
sult would be better if a verb is caught.
4.5 Antonymy Evaluation Measure
Besides computing an antonym vector, it seems
relevant to assess wether two lexical items can
be antonyms. To give an answer to this ques-
tion, we have created a measure of antonymy
evaluation. Let A and B be two vectors.
The question is precisely to know if they can
reasonably be antonyms in the context of C.
The antonymy measure MantiEval is the an-
gle between the sum of A and B and the sum
of AnticR(A,C) and AnticR(B,C). Thus, we
have:
MantiEval = DA(A?B,AntiR(A,C)?AntiR(B,C))
A+B
A
B
Anti(A,C)
Anti(B,C)
Anti(A,C)+Anti(B,C)
Figure 2: 2D geometric representation of the antonymy
evaluation measure MantiEval
The antonymy measure is a pseudo-distance.
It verifies the properties of reflexivity, symme-
try and triangular inequality only for the subset
of items which doesn?t accept antonyms. In this
case, notwithstanding the noise level, the mea-
sure is equal to the angular distance. In the
general case, it doesn?t verify reflexivity. The
conceptual vector components are positive and
we have the property: Distanti ? [0, pi2 ]. The
smaller the measure, the more ?antonyms? the
two lexical items are. However, it would be a
mistake to consider that two synonyms would be
at a distance of about pi2 . Two lexical items atpi
2 have not much in common8. We would rather
see here the illustration that two antonyms
share some ideas, specifically those which are
not opposable or those which are opposable with
a strong activation. Only specific activated con-
cepts would participate in the opposition. A
distance of pi2 between two items should rather
be interpreted as these two items do not share
much idea, a kind of anti-synonymy. This re-
sult confirms the fact that antonymy is not the
exact inverse of synonymy but looks more like a
?negative synonymy? where items remains quite
related. To sum up, the antonym of w is not
a word that doesn?t share ideas with w, but a
word that opposes some features of w.
4.5.1 Examples
In the following examples, the context has been
ommited for clarity sake. In these cases, the
context is the sum of the vectors of the two
items.
MantiEval(EXISTENCE,NON-EXISTENCE) = 0.03
MantiEvalC(?existence?, ?non-existence?) = 0.44
MantiEvalC(EXISTENCE, CAR) = 1.45
MantiEvalC(?existence?, ?car ?) = 1.06
MantiEvalC(CAR, CAR) = 0.006
MantiEvalC(?car ?, ?car ?) = 0.407
The above examples confirm what pre-
sented. Concepts EXISTENCE and NON-
EXISTENCE are very strong antonyms in comple-
mentary antonymy. The effects of the polysemy
may explain that the lexical items ?existence? and
?non-existence? are less antonyms than their re-
lated concepts. In complementary antonymy,
CAR is its own antonym. The antonymy mea-
sure between CAR and EXISTENCE is an exam-
ple of our previous remark about vectors shar-
ing few ideas and that around pi/2 this mea-
sure is close to the angular distance (we have
DA(existence, car) = 1.464.). We could con-
sider of using this function to look in a concep-
tual lexicon for the best antonyms. However,
the computation cost (around a minute on a P4
at 1.3 GHz) would be prohibitive.
8This case is mostly theorical, as there is no language
where two lexical items are without any possible relation.
5 Action on learning and method
evaluation
The function is now used in the learning process.
We can use the evaluation measure to show the
increase of coherence between terms:
MantiEvalC new old
?existence?, ?non-existence? 0.33 0.44
?existence?, ?car ? 1.1 1.06
?car ?, ?car ? 0.3 0, 407
There is no change in concepts because they are
not learned. In the opposite, the antonymy eval-
uation measure is better on items. The exemple
shows that ?existence? and ?non-existence? have
been largely modified. Now, the two items are
stronger antonyms than before and the vector
basis is more coherent. Of course, we can test
these results on the 71000 lexical items which
have been modified more or less directly by the
antonymy function. We have run the test on
about 10% of the concerned items and found an
improvement of the angular distance through
MantiEvalC ranking to 0.1 radian.
6 Conclusion
This paper has presented a model of antonymy
using the formalism of conceptual vectors. Our
aim was to be able: (1) to spot antonymy if
it was not given in definition and thus provide
an antonym as a result, (2) to use antonyms
(discovered or given) to control or to ensure the
coherence of an item vector, build by learning,
which could be corrupted. In NLP, antonymy is
a pivotal aspect, its major applications are the-
matic analysis of texts, construction of large lex-
ical databases and word sense disambiguation.
We grounded our research on a computable lin-
guisitic theory being tractable with vectors for
computational sake. This preliminary work on
antonymy has also been conducted under the
spotlight of symmetry, and allowed us to express
antonymy in terms of conceptual vectors. These
functions allow, from a vector and some contex-
tual information, to compute an antonym vec-
tor. Some extensions have also been proposed so
that these functions may be defined and usable
from lexical items. A measure has been identi-
fied to assess the level of antonymy between two
items. The antonym vector construction is nec-
essary for the selection of opposed lexical items
in text generation. It also determines opposite
ideas in some negation cases in analysis.
Many improvements are still possible, the
first of them being revision of the VAC lists.
These lists have been manually constructed by
a reduced group of persons and should widely be
validated and expanded especially by linguists.
We are currently working on possible improve-
ments of results through learning on a corpora.
References
Jacques Chauche?. De?termination se?mantique
en analyse structurelle : une expe?rience base?e
sur une de?finition de distance. TAL Informa-
tion, 1990.
Scott C. Deerwester, Susan T. Dumais,
Thomas K. Landauer, George W. Furnas, and
Richard A. Harshman. Indexing by latent se-
mantic analysis. Journal of the American So-
ciety of Information Science, 41(6):391?407,
1990.
Mathieu Lafourcade. Lexical sorting and lexical
transfer by conceptual vectors. In Proceeding
of the First International Workshop on Mul-
tiMedia Annotation, Tokyo, January 2001.
Larousse. The?saurus Larousse - des ide?es aux
mots, des mots aux ide?es. Larousse, 1992.
Mathieu Lafourcade and Violaine Prince. Syn-
onymies et vecteurs conceptuels. In actes de
TALN?2001, Tours, France, July 2001.
John Lyons. Semantics. Cambridge University
Press, 1977.
Igor Mel?c?uk, Andre? Clas, and Alain Polgue`re.
Introduction a` la lexicologie explicative et
combinatoire. Duculot, 1995.
Emmanuel Morin. Extraction de liens
se?mantiques entre termes a` partir de
corpus techniques. PhD thesis, Universite? de
Nantes, 1999.
Victoria Lynn Muehleisen. Antonymy and se-
mantic range in english. PhD thesis, North-
western university, 1997.
F.R. Palmer. Semantics : a new introduction.
Cambridge University Press, 1976.
P. Roget. Roget?s Thesaurus of English Words
and Phrases. Longman, London, 1852.
Gerard Salton and Michael McGill. Introduc-
tion to Modern Information Retrieval. Mc-
GrawHill, 1983.
