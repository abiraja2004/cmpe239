Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 48?55,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
A Parse-and-Trim Approach with Information Significance               
for Chinese Sentence Compression 
 
 
 
Wei Xu           Ralph Grishman 
Computer Science Department 
New York University 
New York, NY, 10003, USA 
{xuwei,grishman}@cs.nyu.edu 
 
 
 
  
 
Abstract 
In this paper, we propose an event-based ap-
proach for Chinese sentence compression 
without using any training corpus. We en-
hance the linguistically-motivated heuristics 
by exploiting event word significance and 
event information density. This is shown to 
improve the preservation of important infor-
mation and the tolerance of POS and parsing 
errors, which are more common in Chinese 
than English. The heuristics are only required 
to determine possibly removable constituents 
instead of selecting specific constituents for 
removal, and thus are easier to develop and 
port to other languages and domains. The ex-
perimental results show that around 72% of 
our automatic compressions are grammatically 
and semantically correct, preserving around 
69% of the most important information on av-
erage.  
1 Introduction 
The goal of sentence compression is to shorten 
sentences while preserving their grammaticality 
and important information. It has recently at-
tracted much attention because of its wide range 
of applications, especially in summarization 
(Jing, 2000) and headline generation (which can 
be viewed as summarization with very short 
length requirement). Sentence compression can 
improve extractive summarization in coherence 
and amount of information expressed within a 
fixed length. 
An ideal sentence compression will include 
complex paraphrasing operations, such as word 
deletion, substitution, insertion, and reordering. 
In this paper, we focus on the simpler instantia-
tion of sentence simplification, namely word de-
letion, which has been proved a success in the 
literature (Knight and Marcu, 2002; Dorr et al 
2003; Clarke and Lapata, 2006).  
In this paper, we present our technique for 
Chinese sentence compression without the need 
for a sentence/compression parallel corpus. We 
combine linguistically-motivated heuristics and 
word significance scoring together to trim the 
parse tree, and rank candidate compressions ac-
cording to event information density. In contrast 
to probabilistic methods, the heuristics are more 
likely to produce grammatical and fluent com-
pressed sentences. We reduce the difficulty and 
linguistic skills required for composing heuristics 
by only requiring these heuristics to identify pos-
sibly removable constituents instead of selecting 
specific constituents for removal. The word sig-
nificance helps to preserve informative constitu-
ents and overcome some POS and parsing errors. 
In particular, we seek to assess the event infor-
mation during the compression process, accord-
ing to the previous successes in event-based 
summarization (Li et al 2006) and a new event-
oriented 5W summarization task (Parton et al 
2009). 
The next section presents previous approaches 
to sentence compression. In section 3, we de-
scribe our system with three modules, viz. lin-
guistically-motivated heuristics, word signific-
ance scoring and candidate compression selec-
tion. We also develop a heuristics-only approach 
for comparison. In section 4, we evaluate the 
compressions in terms of grammaticality, infor-
48
mativeness and compression rate. Finally, Sec-
tion 5 concludes this paper and discusses direc-
tions of future work. 
2 Previous Work 
Most previous studies relied on a parallel cor-
pus to learn the correspondences between origi-
nal and compressed sentences. Typically sen-
tences are represented by features derived from 
parsing results, and used to learn the transforma-
tion rules or estimate the parameters in the score 
function of a possible compression. A variety of 
models have been developed, including but not 
limited to the noisy-channel model (Knight and 
Marcu, 2002; Galley and McKeown, 2007), the 
decision-tree model (Knight and Marcu, 2002), 
support vector machines (Nguyen et al 2004) 
and large-margin learning (McDonald, 2006; 
Cohn and Lapata 2007).  
Approaches which do not employ parallel cor-
pora are less popular, even though the parallel 
sentence/compression corpora are not as easy to 
obtain as multilingual corpora for machine trans-
lation. Only a few studies have been done requir-
ing no or minimal training corpora (Dorr et al 
2003; Hori and Furui, 2004; Turner and Char-
niak, 2005). The scarcity of parallel corpora also 
constrains the development in languages other 
than English. To the best of our knowledge, no 
study has been done on Chinese sentence com-
pression.  
An algorithm making limited use of training 
corpora was proposed originally by Hori and Fu-
rui (2004) for spoken text in Japanese, and later 
modified by Clarke and Lapata (2006) for Eng-
lish text. Their model searches for the compres-
sion with highest score according to the signific-
ance of each word, the existence of Subject-
Verb-Object structures and the language model 
probability of the resulting word combination. 
The weight factors to balance the three mea-
surements are experimentally optimized by a 
parallel corpus or estimated by experience.  
Turner and Charniak (2005) present semi-
supervised and unsupervised variants of the noi-
sy channel model. They approximate the rules of 
compression from a non-parallel corpus (e.g. the 
Penn Treebank) based on probabilistic context 
free grammar derivation. 
Our approach is most similar to the Hedge 
Trimmer for English headline generation (Dorr et 
al, 2003), in which linguistically-motivated heu-
ristics are used to trim the parse tree. This me-
thod removes low content components in a preset 
order until the desired length requirement is 
reached. It reduces the risk of deleting subordi-
nate clauses and prepositional phrases by delay-
ing these operations until no other rules can be 
applied. This fixed order of applying rules limits 
the flexibility and capability for preserving in-
formative constituents during deletions. It is like-
ly to fail by producing a grammatical but seman-
tically useless compressed sentence. Another 
major drawback is that it requires considerable 
linguistic skill to produce proper rules in a proper 
order.   
3 Algorithms for Sentence Compression 
Our system takes the output of a Chinese Tree-
bank-style syntactic parser (Huang and Harper, 
2009) as input and performs tree trimming opera-
tions to obtain compression. We propose and 
compare two approaches. One uses only linguis-
tically-motivated heuristics to delete words and 
gets the compression result directly. The other 
one uses heuristics to determine which nodes in 
the parse tree are potentially removable. Then all 
removable nodes are deleted one by one accord-
ing to their significance weights to generate a 
series of candidate compressions. Finally, the 
best compression is selected based on sentence 
length and informativeness criteria.  
3.1 Linguistically-motivated Heuristics 
This module aims to identify the nodes in the 
parse tree which may be removed without severe 
loss in grammaticality and information. Based on 
an analysis of the Penn Treebank corpus and 
human-produced compression, we decided that 
the following parse constituents are potential low 
content units.  
 
Set 0 ? basic: 
? Parenthetical elements  
? Adverbs except negative, some temporal 
and degree adverbs 
? Adjectives except when the modified noun 
consists of only one character 
? DNPs (which are formed by various 
phrasal categories plus ??? and appear as 
modifiers of NP in Chinese) 
? DVPs (which are formed by various 
phrasal categories plus ??? in Chinese, 
and appear as modifiers of VP in Chinese) 
? All nodes in noun coordination phrases 
except the first noun 
 
 
49
Set 1 ? fixed: 
? All children of NP nodes except temporal 
nouns and proper nouns and the last noun 
word 
? All simple clauses (IP) except the first one, 
if the sentence consists of more than one 
IP 
? Prepositional phrases except those that 
may contain location or date information, 
according to a hand-made list of preposi-
tions  
 
Set 2 ? flexible: 
? All nodes in verb coordination phrases ex-
cept the first one. 
? Relative clauses 
? Appositive clauses 
? All prepositional phrases 
? All children of NP nodes except the last 
noun word 
? All simple clauses, if the sentence consists 
of more than one IP (at least one clause is 
required to be preserved in later trimming) 
 
Set 0 lists all the fundamental constituents that 
may be removed and is used in both approaches. 
Set 1 and Set 2 are designed to handle more 
complex constituents for the two approaches re-
spectively.  
The heuristics-only approach exploits Set 0 
and Set 1. It can be viewed as the Chinese ver-
sion of Hedge Trimmer (Dorr et al 2003), but 
differs in the following ways: 
1) Chinese has different language construc-
tions and grammar from English. 
2) We eliminate the strict compression 
length constraint in order to yield more 
natural compressions with varying length. 
3) We do not remove time expressions on 
purpose to benefit further applications, 
such as event extraction.  
 
The heuristics-only approach deletes low con-
tent units mechanically while preserving syntac-
tic correctness, as long as parsing is accurate. 
Our preliminary experiments showed that the 
heuristics in Set 0 and Set 1 can generate a com-
paratively satisfying compression, but is sensi-
tive to part-of-speech and parsing errors, e.g. the 
proper noun ??? (Hyundai)? as motor compa-
ny is tagged as an adjective (shown in Figure 1) 
and thus removed since its literal meaning is ??
?(modern)?. Moreover, the rules in Set 1 reduce 
the sentence length in a gross manner, risking 
serious information or grammaticality loss. For 
example, the first clause may not be a complete 
grammatical sentence, and is not always the most 
important clause in the sentence though that is 
usually the case. We also want to point out that 
the heuristics tend to reduce the sentence length 
and preserve the grammar by removing most of 
the modifiers, even though modifiers may con-
tain a lot of important information. 
To address the above problems of heuristics, 
we exploit word significance to measure the im-
portance of each constituent. Set 2 was created to 
work with Set 0 to identify removable low con-
tent units. The heuristics in this approach are 
used only to detect all possible candidates for 
deletion and thus are more general and easier to 
create than Set 1. For instance, we do not need to 
carefully determine which kinds of prepositional 
phrases are safe or dangerous to delete but in-
stead mark all of them as potentially removable.  
The actual word deletion is performed later by 
a compression generation and selection module, 
taking word significance and compression rate 
into consideration. The heuristics in Set 2 are 
able to cover more risky constituents than Set 1, 
e.g. clauses and parallel structures, since the risk 
will be controlled by the later processes. 
 
 ( (IP  
        (NP  
              (*NP (NR ??))                                South Korean  
              (#*ADJP (JJ ??))                            Hyundai 
              (NP     
                       (#*NN ??)                              motor 
                       (NN ??)))                               company 
        (VP (VC ?)                                             is 
              (NP (#*DNP (NP (NR ???))        Volvo 
                                    (DEG ?))                     ?s 
                     (#*ADJP (JJ ??))                     potential 
                     (NP (NN ??))))                        buyer 
        (PU .))) 
 
Figure 1. Parse tree trimming by heuristics 
(#: nodes trimmed out by Set0 & Set1;  
*: nodes labeled as removable by Set0 & Set2.) 
 
Figure 1 shows an example of applying heuris-
tics to the parse tree of the sentence ?????
?????????????? (The South 
Korean Hyundai Motor Company is a potential 
buyer of Volvo.). The heuristics-only approach 
produces ????????? (The South Korean 
company is a buyer.), which is grammatical but 
semantically meaningless. We will see how word 
significance and information density scoring 
produce a better compression in section 3.3. 
50
3.2 Event-based Word Significance 
Based on our observations, a human-compressed 
sentence primarily describes an event or a set of 
relevant events and contains a large proportion of 
named entities, especially in the news article 
domain. Similar to event-based summarization 
(Li et al 2006), we consider only the event 
terms, namely verbs and nouns, with a prefe-
rence for proper nouns. 
The word significance score Ij(wi) indicates 
how important a word wi is to a document j. It is 
a tf-idf weighting scheme with additional weight 
for proper nouns:  
 
?
?
?
?
?
??
?
?
otherwise
nounproperiswifidftf
nouncommonorverbiswifidftf
wI iiij
iiij
ij
,0
,
,
)( ?
 (1) 
where 
wi : a word in the sentence of document j 
tfij :  term frequency of wi in document j 
idfi : inverse document frequency of wi  
? : additional weight for proper noun. 
 
The nodes in the parse tree are then weighted 
by the word significance for leaves or the sum of 
the children?s weights for internal nodes. The 
weighting depends on the word itself regardless 
of its part-of-speech tags in order to overcome 
some part-of-speech errors. 
3.3 Compression Generation and Selection 
In this module, we first apply a greedy algorithm 
to trim the weighted parse tree to obtain a series 
of candidate compressions. Recall that the heu-
ristics Set 0 and 2 have provided the removabili-
ty judgment for each node in the tree. The parse 
tree trimming algorithm is as follows:  
1) remove one node with the lowest weight 
and get a candidate compressed sentence 
2) update  the weights of all ancestors of 
the removed node  
3) repeat until no node is removable  
 
The selection among candidate compressions is a 
tradeoff between sentence length and amount of 
information. Inspired by headlines in news ar-
ticles, most of which contain a large proportion 
of named entities, we create an information den-
sity measurement D(sk) for sentence sk to select 
the best compression: 
                 
)(
)(
)(
k
Pw
i
k sL
wI
sD i
?
??                     (2) 
where 
P : the set of words whose significance scores 
are larger  than ? in (1) 
I(wi) : the significance score of word wi 
L(sk) : the length of sentence in characters  
 
Table 1 shows the effectiveness of information 
density to select a proper compression with a 
balance between length and meaningfulness. Ta-
ble 1 lists all candidate compressions in sequence 
generated from the parse tree in Figure 1. The 
words in bold are considered in information den-
sity. The underlined compression is picked as 
final output as ?????????????
?? (The South Korean Hyundai company is a 
buyer of Volvo.), which makes more sense than 
the one produced by heuristics-only approach as 
????????? (The South Korean company 
is a buyer.). In our approach, ???(Hyundai)? 
tagged as adjective and ?????(Volvo?s)? as 
a modifier to buyer are preserved successfully. 
 
D(s) Sentence  
0.254 ?????????????????. 
The South Korean Hyundai Motor Company 
is a potential buyer of Volvo. 
0.288 ???????????????. 
The South Korean Hyundai Motor Company is 
a buyer of Volvo. 
0.332 ?????????????. 
The South Korean Hyundai Company is a buy-
er of Volvo. 
0.282 ???????????. 
The South Korean company is a buyer of Vol-
vo. 
0.209 ?????????. 
The company is a buyer of Volvo. 
0.0 ?????. 
The company is a buyer. 
 
Table 1. Compression generation and selection 
for the sentence in Figure 1 
 
The compression with highest information 
density is chosen as system output. To achieve a 
better compression rate and avoids overly con-
densed sentences (i.e. very short sentences with 
only a proper noun), we further constrain the 
compression to a limited but varying length 
range [min_length, max_length] according to the 
length of the original sentence: 
 
?
?
? ???
?
?
otherwiselengthoriginal
lengthoriginaliflengthorig
max_length
lengthoriginalmin_length
,_
_,_
},_min{
???
? (3) 
 
51
where 
orig_length : the length of original sentence in 
characters 
?,? : fixed lengths in characters 
 
In contrast to a fixed limitation of length, this 
varying length simulates human behavior in 
creating compression and avoid the overcom-
pression caused by the density selection schema. 
4 Experiments 
4.1 Experiment Setup 
Our experiments were designed to evaluate the 
quality of automatic compression. The evaluation 
corpus is 79 documents from Chinese newswires, 
and the first sentence of each news article is 
compressed.  
The compression of the first sentences in the 
Chinese news articles is a comparatively chal-
lenging task. Unlike English, Chinese often con-
nects two or more self-complete sentences to-
gether without any indicating word or punctua-
tion; this is extremely frequent for the first sen-
tence of news text. The average length of the first 
sentences in the 79 documents is 61.5 characters, 
compared to 46.8 characters for the sentences in 
the body of these news articles.  
We compare the compressions generated by 
four different methods: 
? Human [H]: A native Chinese speaker is 
asked to generate a headline-like compres-
sion (must be a complete sentence, not a 
fragment, and need not preserve original 
SVO structure) based on the first sentence 
of each news article. Only word deletion 
operations are allowed. 
? Heuristics [R]: The heuristics-only ap-
proach mentioned in section 2.1.  
? Heuristics + Word Significance [W]: The 
approach combines heuristics and word 
significance. The parameter ? in (1) is set 
to be 1, which is an upper bound of word?s 
tf-idf value throughout the corpus. 
? Heuristics + Word Significance + Length 
Constraints [L]: Compression is con-
strained to a limited but varying length, as 
mentioned in section 2.3. The length pa-
rameters ? and ? in (3) are set roughly to 
be 10 and 20 characters based on our ex-
perience. 
 
4.2 Human Evaluation 
Sentence compression is commonly evaluated 
by human judgment. Following the literature 
(Knight and Marcu, 2002; Dorr et al 2003; 
Clarke and Lapata, 2006; Cohn and Lapata 
2007), we asked three native Chinese speakers to 
rate the grammaticality of compressions using 
the 1 to 5 scale. We find that all three non-
linguist human judges tend to take semantic cor-
rectness into consideration when scoring gram-
maticality.  
We also asked these three judges to give a list 
of keywords from the original sentence before 
seeing compressions, which they would preserve 
if asked to create a headline based on the sen-
tence. Instead of a subjective score, the informa-
tiveness is evaluated by measuring the keyword 
coverage of the target compression on a percen-
tage scale. The three judges give different num-
bers of keywords varying from 3.33 to 6.51 on 
average over the 79 sentences. 
The compression rate is the ratio of the num-
ber of Chinese characters in a compressed sen-
tence to that in its original sentence. 
The experimental results in Table 2 show that 
our automatically generated compressions pre-
serve grammaticality, with an average score of 
about 4 out of 5, because of the use of linguisti-
cally-motivated heuristics.  
 
 Compres-
sion Rate 
Grammat-
icality 
(1 ~ 5) 
Informa-
tiveness 
(0~100%) 
Human 38.5% 4.962 90.7% 
Heuristics 54.1% 4.114 64.9% 
Heu+Sig 52.8% 3.854 68.8% 
Heu+Sig+L 34.3% 3.664 56.1% 
  
Table 2. Mean rating from human evaluation on 
first sentence compression 
 
Event-based word significance and informa-
tion density increase the amount of important 
information by 6% with similar sentence length, 
but decreases the average grammaticality score 
by 6.5%. This is because the method using word 
significance sacrifices grammaticality to reduce 
the linguistic complexity of the heuristics. None-
theless, this method does improve grammaticali-
ty for 16 of the 79 compressed sentences, typi-
cally for those with POS or parsing errors.  
The compression rates of the two basic auto-
matic approaches are around 53%, while it is 
38.5% for manual compression. This is partially 
because our heuristics only trim the parse tree 
52
but do not transform the structure of it, while a 
human may change the grammatical structure, 
remove more linking words and even abbreviate 
some words. The length constraint boosts the 
compression rate of our combined approach by 
35% with a loss of 18.5% in informativeness and 
5% in grammaticality.  
 
Grammaticali-
ty 
(1 ~ 5) 
Number of 
Sentence 
Compres-
sion Rate 
Informa-
tiveness 
(0~100%) 
Heuristics > 4.5 45 64.1% 75.9% 
Heuristics >= 4 62 54.5% 70.6% 
Heu+Sig  > 4.5 35 59.8% 81.8% 
Heu+Sig  >= 4 57 56.7% 75.8% 
 
Table 3. Compressions with good grammar 
 
We further investigate the performance of our 
automatic system by considering only relatively 
grammatical compressions, as shown in Table 3. 
The compressions which receive an average 
score of more than 4.5 are comparatively reada-
ble.  The combined approach generates 35 such 
compressions among a total of 79 sentences, pre-
serving 81.8% important information on average, 
which is quite satisfying since human-generated 
compression only achieves 90.7%.  
The infomativeness score of human-generated 
compression also demonstrates the difficulty of 
this task. We compare our automatically generat-
ed event words list with the keywords picked by 
human judges. 61.8% of human-selected key-
words are included in the event words list, thus 
considered when calculating information signi-
ficance. This fact demonstrates some success but 
also potential room for improving keyword se-
lection. 
4.3 Some Examples 
We illustrate several representative samples of 
our system output in Table 4. In the first example, 
all three automatic compressions are acceptable, 
though different in preserving important infor-
mation. [W] and [L] concisely contain the WHO, 
WHAT, WHOM information of the event, while 
[R] further preserves the WHY and WHEN in-
formation.  
In the second example, the heuristics-only ap-
proach produced a decent compression by keep-
ing only the first self-complete sub-sentence. The 
weight of word ???(White House)? is some-
what overwhelming and resulted in dense com-
pressions in [W] and [L], which are too short to 
be good. Besides, [W] and [L] in this example 
show that not all the prepositional phrases, noun 
modifiers etc. can be removed in Chinese with-
out affecting grammaticality, though in most 
cases the removals are safe. This is one of the 
main reasons for grammar errors in the compres-
sion results except POS and parsing errors.  
The third example shows how the combined 
approach overcomes POS errors and how length 
constraints avoid overcompression. In [R], ???
?(Nadal)? is deleted because it is mistakenly 
tagged as an adverb modifying the action ?claim 
the victory and progress through?. Since Nadal is 
tagged as proper noun somewhere else in the 
document, its significance makes it survive the 
compression process. [L] produces a perfect 
compression with proper length, information and 
grammar, just as human-made compression. [W] 
selects a very condensed version of compression 
but loses some information.  
 
1.  
[O] ?????????????,???????????
???????????. 
Because both sides were immovable on the drawing of maritime 
borders, a three-day high-level military meeting between North 
and South Korea broke up in discord today. 
[H]??????????????. 
A high-level military meeting between two Koreas broke up in 
discord today. 
[R]??????,??????????????????. 
Because both sides were immovable, a three-day high-level 
meeting between two Koreas broke up in discord today. 
[L]??????????. 
A high-level meeting between two Koreas broke up in discord. 
[W]??????????. 
A high-level meeting between two Koreas broke up in discord. 
2.  
[O]??????????????,??????????
???;??????????????????????
??,??????. 
The White House today called for nuclear inspectors to be sent 
as soon as possible to monitor North Korea?s closure of its nuc-
lear reactors. The White House made this call after US President 
Bush had telephone conversations with South Korean President 
Roh Moo-hyun. 
[H] ????????????????????. 
The White House today called for inspectors to be sent to moni-
tor North Korea?s closure of its nuclear reactors. 
[R]??????????,??????????. 
 The White House today called for inspectors to be sent to moni-
tor North Korea?s closure of its reactors. 
[L]??????????, ???, ????. 
The White House today called for inspectors to be sent. The 
White House is,  made this call. 
[W]???,????. 
The White House is, made this call. 
3.  
[O]??????????,??????,???,????
?????????????. 
Fourth seed Djokovic withdrew from the game, and allowed 
second seed Nadal , who was leading 3-6 , 6-1 , 4-1 , to claim the 
victory and progress through. 
[H]??????????????. 
Djokovic withdrew from the game, and allowed Nadal to claim 
the victory and progress through. 
53
[R]??????,?????,???,?????????
?????. 
Djokovic withdrew from the game, and allowed second seed, 
who was leading 3-6 , 6-1 , 4-1 , to claim the victory and 
progress through. 
 [L]????????????????. 
Djokovic withdrew from the game, and allowed seed Nadal to 
claim the victory and progress through. 
[W]??????. 
Djokovic withdrew from the game. 
4.  
[O]??? 7 ? 31 ????? 30 ???????????
??????????. 
Chinanews.com , July 31 On the 30th Chen Shui-bian questioned 
that members of the judiciary on the island may have tried to get 
involved in elections for leaders in the Taiwan region.  
[H]???????????????????. 
Chen Shui-bian questioned that members of the judiciary may 
get involved in elections for leaders in the Taiwan region. 
[R]??? 7 ? 31 ????? 30 ???????????
????????. 
Chinanews.com , July 31 On the 30th Chen Shui-bian questioned 
that members on the island may have tried to get involved in 
elections for leaders in the Taiwan region.  
[L]??? 30?????????????????. 
On the 30th Chen Shui-bian questioned that members may have 
tried to get involved in elections for leaders in the Taiwan re-
gion. 
[W]??? 30 ?????????????????. 
On the 30th Chen Shui-bian questioned that members may have 
tried to get involved in elections for leaders in the Taiwan re-
gion. 
5.  
[O]??????????????????,?????,
????????????????. 
Patil is India?s first woman presidential candidate, if she is 
elected, she will become India?s first woman president in history. 
[H] ??????????????????. 
Patil is India?s first woman presidential candidate. 
[R]??????????????. 
Patil is the first candidate in the history of India. 
[L]???????,?????????????. 
Patil is the candidate, she will become president of Indian histo-
ry. 
[W]???????. 
Patil is the candidate. 
 
Table 4. Compression examples including human 
and system results, with reference translation 
 (O: Original sentence) 
 
The fourth sample indicates an interesting lin-
guistic phenomenon. The head of the noun 
phrase ???????(members of the judiciary 
on the island)?, ???(members)? cannot stand 
alone making a fluent and valid sentence, though 
all the compressions are grammatically correct. 
Our human assessors also show a preference of 
[R] to [L, W] in grammaticality evaluation, tak-
ing semantic correctness into consideration as 
well. This is probably a reason that our combined 
approach performs worse than heuristic-only ap-
proach in grammaticality. The combined ap-
proach tends to remove risky constituents, but it 
is hard for word significance to control this risk 
properly in every case. This is another of the 
main reasons for bad compression.  
In the fifth sample, all the automatic compres-
sions are grammatically correct preserving well 
the heads of subject and object, but are semanti-
cally incorrect. This case should be hard to han-
dle by any compression approach. 
5 Conclusions and Future Work 
In this paper, we propose a novel approach to 
combine linguistically-motivated heuristics and 
word significance scoring for Chinese sentence 
compression. We take advantage of heuristics to 
preserve grammaticality and not rely on a paral-
lel corpus. We reduce the complexity involved in  
preparing complicated deterministic rules for 
constituent deletion, requiring people only to 
determine potentially removable constituents. 
Therefore, this approach can be easily extended 
to languages or domains for which parallel com-
pression corpora are scarce. The word signific-
ance scoring is used to control the word deletion 
process, pursuing a balance between sentence 
length and information loss. The exploitation of 
event information improves the mechanical rule-
based approach in preserving event-related 
words and overcomes some POS and parsing 
errors.  
The experimental results prove that this com-
bined approach is competitive with a finely-
tuned heuristics-only approach to grammaticality, 
and includes more important information in the 
compressions of the same length.  
In the future, we plan to apply the compres-
sion to Chinese summarization and headline gen-
eration tasks. A careful study on keyword selec-
tion and word weighting may further improve the 
performance of the current system. We also con-
sider incorporating language models to produce 
fluent and natural compression and reduce se-
mantically invalid cases.   
Another important future direction lies in 
creating a parallel compression corpus in Chi-
nese and exploiting statistical and machine learn-
ing techniques. We also expect that an abstrac-
tive approach involving paraphrasing operations 
besides word deletion will create more natural 
compression than an extractive approach.  
 
Acknowledgments 
This work was supported in part by the Defense 
Advanced Research Projects Agency (DARPA) 
under Contract HR0011-06-C-0023.  Any opi-
nions, findings, conclusions, or recommenda-
54
tions expressed in this material are the authors' 
and do not necessarily reflect those of the U.S. 
Government. 
References  
J. Clarke and M. Lapata, 2006. Models for Sentence 
Compression: A Comparison across Domains, 
Training Requirements and Evaluation Measures. 
In Proceedings of the COLING/ACL 2006, Syd-
ney, Australia, pp. 377-384. 
T. Cohn and M. Lapata. 2007. Large Margin Syn-
chronous generation and its application to sentence 
compression. In the Proceedings of the EMNLP/ 
CoNLL 2007, Pragure, Czech Republic, pp. 73-82. 
B. Dorr, D. Zajic and R. Schwartz. 2003. Hedge 
Trimmer: A Parse-and-Trim Approach to Headline 
Generation. In the Proceedings of the 
NAACL/HLT text summarization workshop, Ed-
monton, Canada, pp. 1-8.  
M. Galley and K. McKeown, 2007. Lexicalized Mar-
kov Grammars for Sentence Compression. In the 
Proceedings of NAACL/HLT 2007, Rochester, 
NY, pp. 180-187. 
C. Hori and S. Furui. 2004. Speech Summarization:  
An Approach through Word Extraction and a Me-
thod for Evaluation. IEICE Transactions on Infor-
mation and Systems, E87-D(1): 15-25. 
Z. Huang and M. Harper, 2009. Self-training PCFG 
Grammars with Latent Annotations Across Lan-
guages. In the proceedings of EMNLP 2009, Sin-
gapore.  
H. Jing. 2000. Sentence Reduction for Automatic 
Text Summarization. In Proceedings of the 6th 
ANLP, Seattle, WA, pp. 310-315.  
K. Knight and D. Marcu, 2002. Summarization 
beyond Sentence Extraction: a Probabilistic Ap-
proach to Sentence Compression. Artificial Intelli-
gence, 139(1): 91-107. 
W. Li, W. Xu, M. Wu, C. Yuan and Q. Lu. 2006. Ex-
tractive Summarization using Inter- and Intra- 
Event Relevance. In the Proceedings of COL-
ING/ACL 2006, Sydney, Australia, pp 369-376. 
R. McDonald. 2006. Discriminative Sentence Com-
pression with Soft Syntactic Constraints. In the 
Proceedings of 11th EACL, Trento, Italy, pp. 297-
304. 
M. L. Nguyen, A. Shimazu, S. Horiguchi, T. B. Ho 
and M. Fukushi. 2004. Probabilistic Sentence Re-
duction using Support Vector Machines. In Pro-
ceedings of the 20th COLING, Geneva, Switzer-
land, pp. 743-749.  
K. McKeown, R. Barzilay, S. Blair-Goldensohn, D. 
Evans, V. Hatzivassiloglou, J. Klavans, A. Nenko-
va, B. Schiffman and S. Sigelman. 2002. The Co-
lumbia Multi-Document Summarizer for DUC 
2002. In the Proceedings of the ACL workshop on 
Document Understanding Conference (DUC) 
workshop, Philadelphia, PA, pp. 1-8.  
K. Parton, K. McKeown, R. Coyne, M. Diab, R. 
Grishman, D. Hakkani-T?r, M. Harper, H. Ji, W. 
Ma, A. Meyers, S. Stolbach, A. Sun, G. Tur, W. 
Xu and S. Yaman. 2009. Who, What, When, 
Where, Why? Comparing Multiple Approaches to 
the Cross-Lingual 5W Task. In the Proceedings of 
ACL-IJCNLP, Singapore. 
J. Turner and E. Charniak. 2005. Supervised and Un-
supervised Learning for Sentence Compression. In 
the Proceedings of 43rd ACL, Ann Arbor, MI, pp. 
290-297. 
 
 
55
