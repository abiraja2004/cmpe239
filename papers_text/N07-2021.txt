Proceedings of NAACL HLT 2007, Companion Volume, pages 81?84,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Semi-Supervised Learning for Semantic Parsing
using Support Vector Machines
Rohit J. Kate and Raymond J. Mooney
Department of Computer Sciences
The University of Texas at Austin
1 University Station C0500
Austin, TX 78712-0233, USA
frjkate,mooneyg@cs.utexas.edu
Abstract
We present a method for utilizing unan-
notated sentences to improve a semantic
parser which maps natural language (NL)
sentences into their formal meaning rep-
resentations (MRs). Given NL sentences
annotated with their MRs, the initial su-
pervised semantic parser learns the map-
ping by training Support Vector Machine
(SVM) classifiers for every production in
the MR grammar. Our new method ap-
plies the learned semantic parser to the
unannotated sentences and collects unla-
beled examples which are then used to
retrain the classifiers using a variant of
transductive SVMs. Experimental results
show the improvements obtained over
the purely supervised parser, particularly
when the annotated training set is small.
1 Introduction
Semantic parsing is the task of mapping a natu-
ral language (NL) sentence into a complete, for-
mal meaning representation (MR) which a computer
program can execute to perform some task, like
answering database queries or controlling a robot.
These MRs are expressed in domain-specific unam-
biguous formal meaning representation languages
(MRLs). Given a training corpus of NL sentences
annotated with their correct MRs, the goal of a learn-
ing system for semantic parsing is to induce an ef-
ficient and accurate semantic parser that can map
novel sentences into their correct MRs.
Several learning systems have been developed for
semantic parsing, many of them recently (Zelle and
Mooney, 1996; Zettlemoyer and Collins, 2005; Ge
and Mooney, 2005; Kate and Mooney, 2006). These
systems use supervised learning methods which
only utilize annotated NL sentences. However, it
requires considerable human effort to annotate sen-
tences. In contrast, unannotated NL sentences are
usually easily available. Semi-supervised learning
methods utilize cheaply available unannotated data
during training along with annotated data and of-
ten perform better than purely supervised learning
methods trained on the same amount of annotated
data (Chapelle et al, 2006). In this paper we present,
to our knowledge, the first semi-supervised learning
system for semantic parsing.
We modify KRISP, a supervised learning sys-
tem for semantic parsing presented in (Kate and
Mooney, 2006), to make a semi-supervised system
we call SEMISUP-KRISP. Experiments on a real-
world dataset show the improvements SEMISUP-
KRISP obtains over KRISP by utilizing unannotated
sentences.
2 Background
This section briefly provides background needed for
describing our approach to semi-supervised seman-
tic parsing.
2.1 KRISP: The Supervised Semantic Parsing
Learning System
KRISP (Kernel-based Robust Interpretation for Se-
mantic Parsing) (Kate and Mooney, 2006) is a su-
pervised learning system for semantic parsing which
81
takes NL sentences paired with their MRs as train-
ing data. The productions of the formal MRL
grammar are treated like semantic concepts. For
each of these productions, a Support-Vector Ma-
chine (SVM) (Cristianini and Shawe-Taylor, 2000)
classifier is trained using string similarity as the ker-
nel (Lodhi et al, 2002). Each classifier can then
estimate the probability of any NL substring rep-
resenting the semantic concept for its production.
During semantic parsing, the classifiers are called to
estimate probabilities on different substrings of the
sentence to compositionally build the most probable
meaning representation (MR) of the sentence.
KRISP trains the classifiers used in semantic pars-
ing iteratively. In each iteration, for every produc-
tion  in the MRL grammar, KRISP collects pos-
itive and negative examples. In the first iteration,
the set of positive examples for production  con-
tains all sentences whose corresponding MRs use
the production  in their parse trees. The set of neg-
ative examples includes all of the other training sen-
tences. Using these positive and negative examples,
an SVM classifier is trained for each production 
using a string kernel. In subsequent iterations, the
parser learned from the previous iteration is applied
to the training examples and more refined positive
and negative examples, which are more specific sub-
strings within the sentences, are collected for train-
ing. Iterations are continued until the classifiers con-
verge, analogous to iterations in EM (Dempster et
al., 1977). Experimentally, KRISP compares favor-
ably to other existing semantic parsing systems and
is particularly robust to noisy training data (Kate and
Mooney, 2006).
2.2 Transductive SVMs
SVMs (Cristianini and Shawe-Taylor, 2000) are
state-of-the-art machine learning methods for clas-
sification. Given positive and negative training ex-
amples in some vector space, an SVM finds the
maximum-margin hyperplane which separates them.
Maximizing the margin prevents over-fitting in very
high-dimensional data which is typical in natural
language processing and thus leads to better general-
ization performance on test examples. When the un-
labeled test examples are also available during train-
ing, a transductive framework for learning (Vapnik,
1998) can further improve the performance on the
test examples.
Transductive SVMs were introduced in
(Joachims, 1999). The key idea is to find the
labeling of the test examples that results in the
maximum-margin hyperplane that separates the
positive and negative examples of both the training
and the test data. This is achieved by including
variables in the SVM?s objective function repre-
senting labels of the test examples. Finding the
exact solution to the resulting optimization problem
is intractable, however Joachims (1999) gives an
approximation algorithm for it. One drawback of
his algorithm is that it requires the proportion of
positive and negative examples in the test data be
close to the proportion in the training data, which
may not always hold, particularly when the training
data is small. Chen et al (2003) present another
approximation algorithm which we use in our
system because it does not require this assumption.
More recently, new optimization methods have been
used to scale-up transductive SVMs to large data
sets (Collobert et al, 2006), however we did not
face scaling problems in our current experiments.
Although transductive SVMs were originally de-
signed to improve performance on the test data by
utilizing its availability during training, they can also
be directly used in a semi-supervised setting (Ben-
nett and Demiriz, 1999) where unlabeled data is
available during training that comes from the same
distribution as the test data but is not the actual data
on which the classifier is eventually to be tested.
This framework is more realistic in the context of se-
mantic parsing where sentences must be processed
in real-time and it is not practical to re-train the
parser transductively for every new test sentence. In-
stead of using an alternative semi-supervised SVM
algorithm, we preferred to use a transductive SVM
algorithm (Chen et al, 2003) in a semi-supervised
manner, since it is easily implemented on top of an
existing SVM system.
3 Semi-Supervised Semantic Parsing
We modified the existing supervised system KRISP,
described in section 2.1, to incorporate semi-
supervised learning. Supervised learning in KRISP
involves training SVM classifiers on positive and
negative examples that are substrings of the anno-
82
function TRAIN SEMISUP KRISP(Annotated corpus A = f(s
i
;m
i
)ji = 1::Ng, MRL grammar G,
Unannotated sentences T = ft
i
ji = 1::Mg)
C  fC

j 2 Gg = TRAIN KRISP(A,G) // classifiers obtained by training KRISP
Let
P = fp

= Set of positive examples used in training C

j 2 Gg
N = fn

= Set of negative examples used in training C

j 2 Gg
U = fu

= j 2 Gg // set of unlabeled examples for each production, initially all empty
for i = 1 to M do
fu
i

j 2 Gg =COLLECT CLASSIFIER CALLS(PARSE(t
i
; C))
U = fu

= u

[ u
i

j 2 Gg
for each  2 G do
C

=TRANSDUCTIVE SVM TRAIN(p

; n

; u

) // retrain classifiers utilizing unlabeled examples
return classifiers C = fC

j 2 Gg
Figure 1: SEMISUP-KRISP?s training algorithm
tated sentences. In order to perform semi-supervised
learning, these classifiers need to be given appropri-
ate unlabeled examples. The key question is: Which
substrings of the unannotated sentences should be
given as unlabeled examples to which productions?
classifiers? Giving all substrings of the unannotated
sentences as unlabeled examples to all of the clas-
sifiers would lead to a huge number of unlabeled
examples that would not conform to the underly-
ing distribution of classes each classifier is trying to
separate. SEMISUP-KRISP?s training algorithm, de-
scribed below and shown in Figure 1, addresses this
issue.
The training algorithm first runs KRISP?s exist-
ing training algorithm and obtains SVM classifiers
for every production in the MRL grammar. Sets of
positive and negative examples that were used for
training the classifiers in the last iteration are col-
lected for each production. Next, the learned parser
is applied to the unannotated sentences. During the
parsing of each sentence, whenever a classifier is
called to estimate the probability of a substring rep-
resenting the semantic concept for its production,
that substring is saved as an unlabeled example for
that classifier. These substrings are representative of
the examples that the classifier will actually need to
handle during testing. Note that the MRs obtained
from parsing the unannotated sentences do not play
a role during training since it is unknown whether
or not they are correct. These sets of unlabeled ex-
amples for each production, along with the sets of
positive and negative examples collected earlier, are
then used to retrain the classifiers using transductive
SVMs. The retrained classifiers are finally returned
and used in the final semantic parser.
4 Experiments
We compared the performance of SEMISUP-KRISP
and KRISP in the GEOQUERY domain for semantic
parsing in which the MRL is a functional language
used to query a U.S. geography database (Kate et
al., 2005). This domain has been used in most of
the previous work. The original corpus contains 250
NL queries collected from undergraduate students
and annotated with their correct MRs (Zelle and
Mooney, 1996). Later, 630 additional NL queries
were collected from real users of a web-based inter-
face and annotated (Tang and Mooney, 2001). We
used this data as unannotated sentences in our cur-
rent experiments. We also collected an additional
407 queries from the same interface, making a total
of 1; 037 unannotated sentences.
The systems were evaluated using standard 10-
fold cross validation. All the unannotated sentences
were used for training in each fold. Performance
was measured in terms of precision (the percent-
age of generated MRs that were correct) and recall
(the percentage of all sentences for which correct
MRs were obtained). An output MR is considered
correct if and only if the resulting query retrieves
the same answer as the correct MR when submit-
ted to the database. Since the systems assign confi-
dences to the MRs they generate, the entire range of
the precision-recall trade-off can be obtained for a
system by measuring precision and recall at various
confidence levels. We present learning curves for the
best F-measure (harmonic mean of precision and re-
83
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  20  40  60  80  100  120  140  160  180  200  220  240
Be
st
 F
-m
ea
su
re
No. of annotated training sentences
SEMISUP-KRISP
KRISP
GEOBASE
Figure 2: Learning curves for the best F-measures
on the GEOQUERY corpus.
call) obtained across the precision-recall trade-off as
the amount of annotated training data is increased.
Figure 2 shows the results for both systems.
The results clearly show the improvement
SEMISUP-KRISP obtains over KRISP by utilizing
unannotated sentences, particularly when the num-
ber of annotated sentences is small. We also show
the performance of a hand-built semantic parser
GEOBASE (Borland International, 1988) for com-
parison. From the figure, it can be seen that, on
average, KRISP achieves the same performance as
GEOBASE when it is given 126 annotated examples,
while SEMISUP-KRISP reaches this level given only
94 annotated examples, a 25:4% savings in human-
annotation effort.
5 Conclusions
This paper has presented a semi-supervised ap-
proach to semantic parsing. Our method utilizes
unannotated sentences during training by extracting
unlabeled examples for the SVM classifiers it uses to
perform semantic parsing. These classifiers are then
retrained using transductive SVMs. Experimental
results demonstrated that this exploitation of unla-
beled data significantly improved the accuracy of the
resulting parsers when only limited supervised data
was provided.
Acknowledgments
This research was supported by a Google research
grant. The experiments were run on the Mastodon
cluster provided by NSF grant EIA-0303609.
References
K. Bennett and A. Demiriz. 1999. Semi-supervised support
vector machines. Advances in Neural Information Process-
ing Systems, 11:368?374.
Borland International. 1988. Turbo Prolog 2.0 Reference
Guide. Borland International, Scotts Valley, CA.
O. Chapelle, B. Scho?lkopf, and A. Zien, editors. 2006. Semi-
Supervised Learning. MIT Press, Cambridge, MA.
Y. Chen, G. Wang, and S. Dong. 2003. Learning with progres-
sive transductive support vector machine. Pattern Recogni-
tion Letters, 24:1845?1855.
R. Collobert, F. Sinz, J. Weston, and L. Bottou. 2006. Large
scale transductive SVMs. Journal of Machine Learning Re-
search, 7(Aug):1687?1712.
N. Cristianini and J. Shawe-Taylor. 2000. An Introduction to
Support Vector Machines and Other Kernel-based Learning
Methods. Cambridge University Press.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maxi-
mum likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society B, 39:1?38.
R. Ge and R. J. Mooney. 2005. A statistical semantic parser
that integrates syntax and semantics. In Proc. of CoNLL-05,
pages 9?16, Ann Arbor, MI, July.
T. Joachims. 1999. Transductive inference for text classifica-
tion using support vector machines. In Proc. of ICML-99,
pages 200?209, Bled, Slovenia, June.
R. J. Kate and R. J. Mooney. 2006. Using string-kernels for
learning semantic parsers. In Proc. of COLING/ACL-06,
pages 913?920, Sydney, Australia, July.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learning to
transform natural to formal languages. In Proc. of AAAI-05,
pages 1062?1068, Pittsburgh, PA, July.
H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, and
C. Watkins. 2002. Text classification using string kernels.
Journal of Machine Learning Research, 2:419?444.
L. R. Tang and R. J. Mooney. 2001. Using multiple clause con-
structors in inductive logic programming for semantic pars-
ing. In Proc. of ECML-01, pages 466?477, Freiburg, Ger-
many.
V. N. Vapnik. 1998. Statistical Learning Theory. John Wiley
& Sons.
J. M. Zelle and R. J. Mooney. 1996. Learning to parse database
queries using inductive logic programming. In Proc. of
AAAI-96, pages 1050?1055, Portland, OR, August.
L. S. Zettlemoyer and M. Collins. 2005. Learning to map sen-
tences to logical form: Structured classification with proba-
bilistic categorial grammars. In Proc. of UAI-05, Edinburgh,
Scotland, July.
84
