Proceedings of the ACL 2010 Conference Short Papers, pages 147?150,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Fixed Length Word Suffix for Factored 
Statistical Machine Translation 
 
Narges Sharif Razavian 
School of Computer Science 
Carnegie Mellon Universiy 
Pittsburgh, USA 
nsharifr@cs.cmu.edu 
Stephan Vogel 
School of Computer Science 
Carnegie Mellon Universiy 
Pittsburgh, USA 
stephan.vogel@cs.cmu.edu 
 
 
 
 
Abstract 
Factored Statistical Machine Translation ex-
tends the Phrase Based SMT model by al-
lowing each word to be a vector of factors. 
Experiments have shown effectiveness of 
many factors, including the Part of Speech 
tags in improving the grammaticality of the 
output. However, high quality part of 
speech taggers are not available in open 
domain for many languages. In this paper 
we used fixed length word suffix as a new 
factor in the Factored SMT, and were able 
to achieve significant improvements in three 
set of experiments: large NIST Arabic to 
English system, medium WMT Spanish to 
English system, and small TRANSTAC 
English to Iraqi system. 
1 Introduction 
Statistical Machine Translation(SMT) is current-
ly the state of the art solution to the machine 
translation. Phrase based SMT is also among the 
top performing approaches available as of today. 
This approach is a purely lexical approach, using 
surface forms of the words in the parallel corpus 
to generate the translations and estimate proba-
bilities. It is possible to incorporate syntactical 
information into this framework through differ-
ent ways. Source side syntax based re-ordering 
as preprocessing step, dependency based reorder-
ing models, cohesive decoding features are 
among many available successful attempts for 
the integration of syntax into the translation 
model. Factored translation modeling is another 
way to achieve this goal. These models allow 
each word to be represented as a vector of factors 
rather than a single surface form.  Factors can 
represent richer expression power on each word. 
Any factors such as word stems, gender, part of 
speech, tense, etc. can be easily used in this 
framework.  
   Previous work in factored translation modeling 
have reported consistent improvements from Part 
of Speech(POS) tags, morphology, gender, and 
case factors (Koehn et. a. 2007). In another work, 
Birch et. al. 2007 have achieved improvement 
using Combinational Categorial Grammar (CCG) 
super-tag factors. Creating the factors is done as 
a preprocessing step, and so far, most of the ex-
periments have assumed existence of external 
tools for the creation of these factors (i. e. Part of 
speech taggers, CCG parsers, etc.). Unfortunately 
high quality language processing tools, especial-
ly for the open domain, are not available for most 
languages. 
   While linguistically identifiable representations 
(i.e. POS tags, CCG supertags, etc) have been 
very frequently used as factors in many applica-
tions including MT, simpler representations have 
also been effective in achieving the same result 
in other application areas. Grzymala-Busse and 
Old 1997, DINCER et.al. 2008, were able to use 
fixed length suffixes as features for training a 
POS tagging. In another work Saberi and Perrot 
1999 showed that reversing middle chunks of the 
words while keeping the first and last part intact, 
does not decrease listeners? recognition ability. 
This result is very relevant to Machine Transla-
tion, suggesting that inaccurate context which is 
usually modeled with n-gram language models, 
can still be as effective as accurate surface forms. 
Another research (Rawlinson 1997) confirms this 
finding; this time in textual domain, observing 
that randomization of letters in the middle of 
words has little or no effect on the ability of 
skilled readers to understand the text. These re-
sults suggest that the inexpensive representation-
al factors which do not need unavailable tools 
might also be worth investigating. 
   These results encouraged us to introduce lan-
guage independent simple factors for machine 
translation. In this paper, following the work of 
Grzymala-Busse et. al. we used fixed length suf-
147
fix as word factor, to lower the perplexity of the 
language model, and have the factors roughly 
function as part of speech tags, thus increasing 
the grammaticality of the translation results. We 
were able to obtain consistent, significant im-
provements over our baseline in 3 different expe-
riments, large NIST Arabic to English system, 
medium WMT Spanish to English system, and 
small TRANSTAC English to Iraqi system.  
   The rest of this paper is as follows. Section 2 
briefly reviews the Factored Translation Models. 
In section 3 we will introduce our model, and 
section 4 will contain the experiments and the 
analysis of the results, and finally, we will con-
clude this paper in section 5. 
2 Factored Translation Model  
Statistical Machine Translation uses the log li-
near combination of a number of features, to 
compute the highest probable hypothesis as the 
translation.   
 
e = argmaxe p(e|f) = argmaxe p exp ?i=1
n ?i hi(e,f) 
 
   In phrase based SMT, assuming the source and 
target phrase segmentation as {(fi,ei)}, the most 
important features include: the Language Model 
feature hlm(e,f) = plm(e); the phrase translation 
feature ht(e,f) defined as product of translation 
probabilities, lexical probabilities and phrase pe-
nalty; and the reordering probability, hd(e,f), 
usually defined as ?i=1
n d(starti,endi-1) over the 
source phrase reordering events. 
   Factored Translation Model, recently intro-
duced by (Koehn et. al. 2007), allow words to 
have a vector representation. The model can then 
extend the definition of each of the features from 
a uni-dimensional value to an arbitrary joint and 
conditional combination of features. Phrase 
based SMT is in fact a special case of Factored 
SMT.  
   The factored features are defined as an exten-
sion of phrase translation features. The function 
?(fj,ej), which was defined for a phrase pair be-
fore, can now be extended as a log linear combi-
nation ?f ?f(fjf,ejf).  The model also allows for a 
generation feature, defining the relationship be-
tween final surface form and target factors. Other 
features include additional language model fea-
tures over individual factors, and factored reor-
dering features.  
   Figure 1 shows an example of a possible fac-
tored model.  
 
Figure 1: An example of a Factored Translation and 
Generation Model 
 
   In this particular model, words on both source 
and target side are represented as a vector of four 
factors: surface form, lemma, part of speech 
(POS) and the morphology. The target phrase is 
generated as follows: Source word lemma gene-
rates target word lemma. Source word's Part of 
speech and morphology together generate the 
target word's part of speech and morphology, and 
from its lemma, part of speech and morphology 
the surface form of the target word is finally gen-
erated. This model has been able to result in 
higher translation BLEU score as well as gram-
matical coherency for English to German, Eng-
lish to Spanish, English to Czech, English to 
Chinese, Chinese to English and German to Eng-
lish. 
3 Fixed Length Suffix Factors for Fac-
tored Translation Modeling 
Part of speech tagging, constituent and depen-
dency parsing, combinatory categorical grammar 
super tagging are used extensively in most appli-
cations when syntactic representations are 
needed. However training these tools require 
medium size treebanks and tagged data, which 
for most languages will not be available for a 
while. On the other hand, many simple words 
features, such as their character n-grams, have in 
fact proven to be comparably as effective in 
many applications.  
(Keikha et. al. 2008) did an experiment on text 
classification on noisy data, and compared sever-
al word representations. They compared surface 
form, stemmed words, character n-grams, and 
semantic relationships, and found that for noisy 
and open domain text, character-ngrams outper-
form other representations when used for text 
classification. In another work (Dincer et al
2009) showed that using fixed length word end-
ing outperforms whole word representation for 
training a part of speech tagger for Turkish lan-
guage.  
148
Based on this result, we proposed a suffix fac-
tored model for translation, which is shown in 
Figure 2.  
 
 
 
 
 
 
 
Figure 2: Suffix Factored model: Source word de-
termines factor vectors (target word, target word suf-
fix) and each factor will be associated with its 
language model. 
 
Based on this model, the final probability of 
the translation hypothesis will be the log linear 
combination of phrase probabilities, reordering 
model probabilities, and each of the language 
models? probabilities.  
 
P(e|f) ~  plm-word(eword)* plm-suffix(esuffix) 
 * ?i=1
n  p(eword-j & esuffix-j|fj)  
 * ?i=1
n p(fj | eword-j & esuffix-j) 
 
Where plm-word is the n-gram language model 
probability over the word surface sequence, with 
the language model built from the surface forms. 
Similarly, plm-suffix(esuffix) is the language model 
probability over suffix sequences.  p(eword-j & 
esuffix-j|fj) and p(fj | eword-j & esuffix-j) are translation 
probabilities for each phrase pair i , used in by 
the decoder. This probability is estimated after 
the phrase extraction step which is based on 
grow-diag heuristic at this stage. 
4 Experiments and Results 
We used Moses implementation of the factored 
model for training the feature weights, and SRI 
toolkit for building n-gram language models. The 
baseline for all systems included the moses sys-
tem with lexicalized re-ordering, SRI 5-gram 
language models. 
 
4.1 Small System from Dialog Domain: 
English to Iraqi 
 
This system was TRANSTAC system, which 
was built on about 650K sentence pairs with the 
average sentence length of 5.9 words. After 
choosing length 3 for suffixes, we built a new 
parallel corpus, and SRI 5-gram language models 
for each factor. Vocabulary size for the surface 
form was 110K whereas the word suffixes had 
about 8K distinct words. Table 1 shows the result 
(BLEU Score) of the system compared to the 
baseline. 
 
System Tune on  
Set-
July07 
Test on  
Set-
June08 
Test on  
Set-
Nov08 
Baseline 27.74 21.73 15.62 
Factored 28.83 22.84 16.41 
Improvement 1.09 1.11 0.79 
Table 1: BLEU score, English to Iraqi Transtac sys-
tem, comparing Factored and Baseline systems. 
 
As you can see, this improvement is consistent 
over multiple unseen datasets. Arabic cases and 
numbers show up as the word suffix. Also, verb 
numbers usually appear partly as word suffix and 
in some cases as word prefix. Defining a lan-
guage model over the word endings increases the 
probability of sequences which have this case 
and number agreement, favoring correct agree-
ments over the incorrect ones.  
 
4.2 Medium System on Travel Domain: 
Spanish to English 
 
This system is the WMT08 system, on a corpus 
of 1.2 million sentence pairs with average sen-
tence length 27.9 words. Like the previous expe-
riment, we defined the 3 character suffix of the 
words as the second factor, and built the lan-
guage model and reordering model on the joint 
event of (surface, suffix) pairs. We built 5-gram 
language models for each factor. The system had 
about 97K distinct vocabulary in the surface lan-
guage model, which was reduced to 8K using the 
suffix corpus. Having defined the baseline, the 
system results are as follows.  
 
 
System Tune-
WMT06 
Test set-
WMT08 
Baseline 33.34 32.53 
Factored 33.60 32.84 
Improvement 0.26 0.32 
Table 2: BLEU score, Spanish to English WMT sys-
tem, comparing Factored and Baseline systems. 
 
Here, we see improvement with the suffix fac-
tors compared to the baseline system. Word end-
ings in English language are major indicators of 
word?s part of speech in the sentence. In fact 
 Word Language Model 
Suffix Language Model 
 LM 
  Word 
Word ? 
Suffix ?  
Source Target 
149
most common stemming algorithm, Porter?s 
Stemmer, works by removing word?s suffix. 
Having a language model on these suffixes push-
es the common patterns of these suffixes to the 
top, making the more grammatically coherent 
sentences to achieve a better probability.  
 
4.3 Large NIST 2009 System: Arabic to 
English 
 
We used NIST2009 system as our baseline in 
this experiment. The corpus had about 3.8 Mil-
lion sentence pairs, with average sentence length 
of 33.4 words. The baseline defined the lexica-
lized reordering model. As before we defined 3 
character long word endings, and built 5-gram 
SRI language models for each factor. The result 
of this experiment is shown in table 3.  
 
System Tune 
on  
MT06 
Test on  
Dev07 
News
Wire 
Test on  
Dev07 
Weblog 
Test 
on 
MT08 
Baseline 43.06 48.87 37.84 41.70 
Factored 44.20 50.39 39.93 42.74 
Improve
ment 
1.14 1.52 2.09 1.04 
Table 3: BLEU score, Arabic to English NIST 2009 
system, comparing Factored and Baseline systems. 
 
This result confirms the positive effect of the 
suffix factors even on large systems. As men-
tioned before we believe that this result is due to 
the ability of the suffix to reduce the word into a 
very simple but rough grammatical representa-
tion. Defining language models for this factor 
forces the decoder to prefer sentences with more 
probable suffix sequences, which is believed to 
increase the grammaticality of the result. Future 
error analysis will show us more insight of the 
exact effect of this factor on the outcome. 
5 Conclusion 
In this paper we introduced a simple yet very 
effective factor: fixed length word suffix, to use 
in Factored Translation Models. This simple fac-
tor has been shown to be effective as a rough 
replacement for part of speech. We tested our 
factors in three experiments in a small, English to 
Iraqi system, a medium sized system of Spanish 
to English, and a large system, NIST09 Arabic to 
English. We observed consistent and significant 
improvements over the baseline. This result, ob-
tained from the language independent and inex-
pensive factor, shows promising new 
opportunities for all language pairs.  
References  
Birch, A., Osborne, M., and Koehn, P. CCG supertags 
in factored statistical machine translation. Proceed-
ings of the Second Workshop on Statistical Ma-
chine Translation, pages 9?16, Prague, Czech 
Republic. Association for Computational Linguis-
tics, 2007. 
Dincer T., Karaoglan B. and Kisla T., A Suffix Based 
Part-Of-Speech Tagger For Turkish, Fifth Interna-
tional Conference on Information Technology: 
New Generations, 2008. 
Grzymala-Busse J.W., Old L.J. A machine learning 
experiment to determine part of speech from word-
endings, Lecture Notes in Computer Science, 
Communications Session 6B Learning and Discov-
ery Systems, 1997.  
Keikha M., Sharif Razavian N, Oroumchian F., and 
Seyed Razi H., Document Representation and 
Quality of Text: An Analysis, Chapter 12, Survey 
of Text Mining II, Springer London, 2008. 
Koehn Ph., Hoang H., Factored Translation Models, 
Proceedings of 45th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), 2007. 
Rawlinson G. E., The significance of letter position in 
word recognition, PhD Thesis, Psychology De-
partment, University of Nottingham, Nottingham 
UK, 1976. 
Saberi K and Perrot D R, Cognitive restoration of 
reversed speech, Nature (London) 1999. 
 
150
