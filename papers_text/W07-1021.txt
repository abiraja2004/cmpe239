BioNLP 2007: Biological, translational, and clinical language processing, pages 161?162,
Prague, June 2007. c?2007 Association for Computational Linguistics
Exploring the Use of NLP in the Disclosure of Electronic Patient Records
David Hardcastle
Faculty of Mathematics and Computing
The Open University
d.w.hardcastle@open.ac.uk
Catalina Hallett
Faculty of Mathematics and Computing
The Open University
c.hallett@open.ac.uk
Abstract
This paper describes a preliminary analysis
of issues involved in the production of re-
ports aimed at patients from Electronic Pa-
tient Records. We present a system proto-
type and discuss the problems encountered.
1 Introduction
Allowing patient access to Electronic Patient
Records (EPR) in a comprehensive format is a le-
gal requirement in most European countries. Apart
from this legal aspect, research shows that the provi-
sion of clear information to patients is instrumental
in improving the quality of care (Detmer and Sin-
gleton, 2004). Current work on generating expla-
nations of EPRs to patients suffer from two major
drawbacks. Firstly, existing report generation sys-
tems have taken an intuitive approach to the gener-
ation of explanation: there is no principled way of
selecting the information that requires further expla-
nation. Secondly, most work on medical report gen-
eration systems has concentrated on explaining the
structured part of an EPR; there has been very lit-
tle work on providing automatic explanations of the
narratives (such as letters between health practition-
ers) which represent a considerable part of an EPR.
Attempting to rewrite narratives in a patient-friendly
way is in many ways more difficult than providing
suggestions for natural language generation systems
that take as input data records. In narratives, ambi-
guity can arise from a combination of aspects over
which NLG systems have full control, such as syn-
tax, discourse structure, sentence length, formatting
and readability.
This paper introduces a pilot project that attempts
to address this gap by addressing the following re-
search questions:
1. Given the text-based part of a patient record,
which segments require explanation before being re-
leased to patients?
2. Which types of explanation are appropriate for
various types of segment?
3. Which subparts of a segment require explanation?
The prototype system correctly selects the seg-
ments that require explanation, but we have yet to
solve the problem of accurately identifiying the fea-
tures that contribute to the ?expertness? of a doc-
ument. We discuss the underlying issues in more
detail in section 3 below.
2 Feature identification method
To identify a set of features that differentiate med-
ical expert and lay language, we compared a cor-
pus of expert text with a corpus of lay texts. We
then used the selected features on a corpus of nar-
ratives extracted from a repository of Electronic Pa-
tient Records to attempt to answer the three ques-
tions posed above. First, paragraphs that contain
features characteristic to expert documents are high-
lighted using a corpus of patient information leaflets
as a background reference. Second, we prioritise the
explanations required by decomposing the classifi-
cation data. Finally, we identify within those sec-
tions the features that contribute to the classification
of the section as belonging to the expert register, and
provide suggestions for text simplification.
2.1 Features
The feature identification was performed on two cor-
pora of about 200000 words each: (a) an expert
corpus, containing clinical case studies and med-
ical manuals produced for doctors and (b) a lay
corpus, containing patient testimonials and infor-
mational materials for patients. Both corpora were
161
sourced from a variety of online sources. In com-
paring the corpora we considered a variety of fea-
tures in the following categories: medical content,
syntactic structure, discourse structure, readability
and layout. The features that proved to be best dis-
criminators were the frequency of medical terms,
readability indices, average NP length and the rela-
tive frequency of loan words against English equiva-
lents1. The medical content analysis is based on the
MeSH terminology (Canese, 2003) and consists of
assessing: (a) the frequency of MeSH primary con-
cepts and alternative descriptions, (b) the frequency
of medical terms types and occurences and (c) the
frequency of MeSH terms in various top-level cate-
gories. The readability features consist of two stan-
dard readability indices (FOG and Flesch-Kincaid).
Although some discourse and layout features also
proved to have a high discriminatory power, they
are strongly dependent on the distribution medium
of the analysed materials, hence not suitable for our
analysis of EPR narratives.
2.2 Analysing EPR narratives
We performed our analysis on a corpus of 11000
narratives extracted from a large repository of Elec-
tronic Patient Records, totalling almost 2 million
words. Each segment of each narrative was then as-
sessed on the basis of the features described above,
such as Fog, sentence length, MeSH primary con-
cepts etc. We then smoothed all of the scores for
all segments for each feature forcing the minimum
to 0.0, the maximum to 1.0 and the reference corpus
score for that feature to 0.5. This made it possible to
compare scores with different gradients and scales
against a common baseline in a consistent way.
3 Evaluation and discussion
We evaluated our segment identification method on
a set of 10 narratives containing 27 paragraphs, ex-
tracted from the same repository of EPRs . The seg-
ment identification method proved succesful, with
26/27 (96.3%) segments marked correctly are re-
quiring/not requiring explanation. However, this
only addresses the first of the three questions set
out above, leaving the following research questions
1An in-depth analysis of unfamiliar terms in medical docu-
ments can be found in (Elhadad, 2006)
open to further analysis.
Quantitative vs qualitative analysis
Many of the measures that discriminate expert from
lay texts are based on indicative features; for exam-
ple complex words are indicative of text that is dif-
ficult to read. However, there is no guarantee that
individual words or phrases that are indicative are
also representative - in other words a given complex
word or long sentence will contribute to the readabil-
ity score of the segment, but may not itself be prob-
lematic. Similarly, frequency based measures, such
as a count of medical terminology, discriminate at a
segment level but do not entail that each occurrence
requires attention.
Terminology
We used the MeSH terminology to analyse med-
ical terms in patient records, however (as with prac-
tically all medical terminologies) it contains many
non-expert medical terms. We are currently investi-
gating the possibility of mining a list of expert terms
from MeSH or of making use of medical-lay aligned
ontologies.
Classification
Narratives in the EPR are written in a completely dif-
ferent style from both our training expert corpus and
the reference patient information leaflets corpus. It
is therefore very difficult to use the reference corpus
as a threshold for feature values which can produce
good results on the corpus of narratives, suggest-
ing that a statistical thresholding technique might be
more effective.
Feature dependencies
Most document features are not independent. There-
fore, the rewriting suggestions the system provides
may themselves have an unwanted impact on the
rewritten text, leading to a circular process for the
end-user.
References
Kathi Canese. 2003. New Entrez Database: MeSH.
NLM Technical Bulletin, March-April.
D. Detmer and P. Singleton. 2004. The informed pa-
tient. Technical Report TIP-2, Judge Institute of Man-
agement, University of Cambridge, Cambridge.
Noemi Elhadad. 2006. Comprehending technical texts:
Predicting and defining unfamiliar terms. In Proceed-
ing of AMIA?06, pages 239?243.
162
