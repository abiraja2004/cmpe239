Translation Selection through Source Word Sense Disambiguation
and Target Word Selection
Hyun Ah Lee
yz
and Gil Chang Kim
y
y
Dept. of EECS, Korea Advanced Institute of Science and Technology (KAIST),
373-1 Kusong-Dong Yusong-Gu Taejon 305-701 Republic of Korea
z
Daumsoft Inc., Daechi-Dong 946-12 Kangnam-Gu Seoul 135-280 Republic of Korea
halee@csone.kaist.ac.kr, gckim@cs.kaist.ac.kr
Abstract
A word has many senses, and each sense can
be mapped into many target words. Therefore,
to select the appropriate translation with a cor-
rect sense, the sense of a source word should be
disambiguated before selecting a target word.
Based on this observation, we propose a hybrid
method for translation selection that combines
disambiguation of a source word sense and se-
lection of a target word. Knowledge for transla-
tion selection is extracted from a bilingual dic-
tionary and target language corpora. Dividing
translation selection into the two sub-problems,
we can make knowledge acquisition straightfor-
ward and select more appropriate target words.
1 Introduction
In machine translation, translation selection is
a process that selects an appropriate target lan-
guage word corresponding to a word in a source
language. Like other problems in natural lan-
guage processing, knowledge acquisition is cru-
cial for translation selection. Therefore, many
researchers have endeavored to extract knowl-
edge from existing resources.
As masses of language resources become avail-
able, statistical methods have been attempted
for translation selection and shown practical re-
sults. Some of the approaches have used a bilin-
gual corpus as a knowledge source based on the
idea of Brown et al (1990), but they are not
preferred in general since a bilingual corpus is
hard to come by. Though some latest approach-
es have exploited word co-occurrence that is ex-
tracted from a monolingual corpus in a target
language (Dagan and Itai, 1994; Prescher et al,
2000; Koehn and Knight, 2001), those methods
often fail in selecting appropriate words because
they do not consider sense ambiguity of a target
word.
In a bilingual dictionary, senses of a word are
classied into several sense divisions and also its
translations are grouped by each sense division.
Therefore, when one looks up the translation
of a word in a dictionary, she/he ought to re-
solve the sense of a word in a source language
sentence, and then choose a target word among
translations corresponding to the sense. In this
paper, the fact that a word has many senses
and each sense can be mapped into many tar-
get words (Lee et al, 1999) is referred to as the
`word-to-sense and sense-to-word' relationship,
based on which we propose a hybrid method for
translation selection.
In our method, translation selection is taken
as the combined problem of sense disambigua-
tion of a source language word and selection of
a target language word. To disambiguate the
sense of a source word, we employ both a dic-
tionary based method and a target word co-
occurrence based method. In order to select a
target word, the co-occurrence based method is
also used.
We introduce three measures for translation
selection: sense preference, sense probability
and word probability. In a bilingual dictionary,
example sentences are listed for each sense di-
vision of a source word. The similarity between
those examples and an input sentence can serve
as a measure for sense disambiguation, which we
dene as sense preference. In the bilingual dic-
tionary, target words are also recorded for each
sense division. Since the set of those words can
model each sense, we can calculate the probabil-
ity of the sense by applying the co-occurrence
based method to the set of words. We dene
the estimated probability as sense probability,
which is taken for the other measure of sense
disambiguation. Using co-occurrence, the prob-
ability of selecting a word from the set of trans-
lations can be calculated. We dene it as word
probability, which is a measure for word selec-
tion. Merging sense preference, sense probabili-
ty and word probability, we compute preference
for each translation, and then choose a target
word with the highest translation preference as
a translation.
2 Translation Selection based on
`word-to-sense and sense-to-word'
The `word-to-sense and sense-to-word' relation-
ship means that a word in a source language
could have multiple senses, each of which might
be mapped into various words in a target lan-
guage. As shown in examples below, a Korean
verb `meok-da' has many senses (work, deaf, eat,
etc.). Also `kkae-da' has three senses (break,
hatch, wake), among which the break sense of
`kkae-da' is mapped into multiple words such
as `break', `smash' and `crack'. In that case, if
break

kkae-da
wakehatch
break smash hatch awakewake_up
  (jeobsi-reul kkae-da)
? (X) hatch a dish
? (O) break a dish
? (O) smash a dish
? (O) crack a dish
crack
work

meok-da
eatdeaf
saw bite deaf takeeatcut
?.
dye have
 	 (jeomsim-eul meok-da)
? (X) saw a lunch
? (?) eat a lunch
? (O) have a lunch
? (O) take a lunch
`hatch' is selected as a translation of `kkae-da' in
a sentence `jeobsi-reul kkae-da', the translation
must be wrong because the sense of `kkae-da'
in the sentence is break rather than hatch. In
contrast, any word of the break sense of `kkae-
da' forms a well-translated sentence. Howev-
er, selecting a correct sense does not always
guarantee a correct translation. If the sense of
`meok-da' in `jeomsim-eul meok-da' is correctly
disambiguated as eat but an inappropriate tar-
get word is selected, an improper or unnatural
translation will be produced like `eat a lunch'.
Therefore, in order to get a correct transla-
tion, such a target word must be selected that
has the right sense of a source word and form-
s a proper target language sentence. Previous
approaches on translation selection usually try
to translate a source word directly into a target
word without considering the sense. Thus, they
increase the complexity of the problem and suf-
Target
Language
Corpus
Input
Sentence
Get
Sense
Preference
Get
Word
Probability
Sense Definition &
Example Sentence
Bilingual
Dictionary
Sense & Taget
Word Mapping
Word-level
Translation
Get
Sense
Probability
Combine
spf, sp, wp
WordNet Target WordCooccurrence
Sense
Disambiguation
Word
Selection
Figure 1: Our process of translation selection
fer from the problem of knowledge acquisition
and incorrect selection of translation.
In this paper, we propose a method for
translation selection that reects `word-to-sense
and sense-to-word' relationship. We divide the
problem of translation selection into sense dis-
ambiguation of a source word and selection of
a target word. Figure 1 shows our process of
selecting translation. We introduce three mea-
sures for translation selection: sense preference
(spf) and sense probability (sp) for sense disam-
biguation, and word probability (wp) for word
selection.
Figure 2 shows a part of an English-English-
Korean Dictionary (EssEEK, 1995)
1
. As shown
in the gure, example sentences and denition
sentences are listed for each sense of a source
word. In the gure, example sentences of the
destroy sense of `break' consist of words such as
`cup', `piece', and `thread', and those for the vio-
late sense consist of words such as `promise' and
`contract', which can function as indicators for
the sense of `break'. We calculate sense pref-
erence of a word for each sense by estimating
similarity between words in an input sentence
and words in example and denition sentences.
Each sense of a source word can be mapped
into a set of translations. As shown in the ex-
ample, the break sense of `kkae-da' is mapped
into the set f`break', `smash', `crack'g and some
words in the set can replace each other in a
translated sentence like `break/smash/crack a
1
The English-English-Korean Dictionary used here is
an English-Korean one, where English denitions are
paired with Korean translations.
break breikvbroke, brok "envt1  cause (something) to come to 
pieces by a blow or strain; destroy; crack; smashL 	

	 & ~ a cup   ~ a glass to [into] piece     
~ a thread   ~ one's arm   ~ a stick in two  
  I heard the rope ~.  !" # $ %& The river broke its
bank. '"( 	
)2  hurt; injureL *+, -.
/.0&
~ the skin 12 *+3  put (something) out of order; make useless by 
rough handling, etc. L 345 6.
78-.0& ~ a watch 9:7
8 ~ a line ; < ~ the peace => 344  fail to keep 
or follow; act against (a rule, law, etc) ; violate; disobey?@ "A BC  ~
one's promise DE ~ a contract :D CF05 ?.
Figure 2: A part of an EEK Dictionary
dish'. Therefore, we can expect the behavior
of those words to be alike or meaningfully re-
lated in a corpus. Based on this expectation,
we compute sense probability by applying the
target word co-occurrence based method to all
words in the same sense division.
Word probability represents how likely a tar-
get word is selected from the set of words in the
same sense division. In the example `jeomsim-
eul meok-da', the sense of `meok-da' is eat,
which is mapped into three words f`eat', `have',
`take'g. Among them, `have' forms a natural
phrase while `eat' makes an awkward phrase.
We could judge the naturalness from the fac-
t that `have' and `lunch' co-occurs in a corpus
more frequently than `eat' and `lunch'. In oth-
er words, the level of naturalness or awkward-
ness of a phrase can be captured by word co-
occurrence. Based on this observation, we use
target word co-occurrence to get word probabil-
ity.
3 Calculation of each measure
3.1 Sense Preference
Given a source word s and its k-th sense s
k
,
we calculate sense preference spf(s
k
) using the
equation (1). In the equation, SNT is a set of
all content words except s in an input sentence.
DEF
s
k
is a set of all content words in denition
sentences of s
k
, and EX
s
k
is a set of all content
words in example sentences of s
k
, both of which
are extracted from the dictionary.
spf(s
k
) =
X
w
i
2SNT
max
w
d
2DEF
s
k
sim(w
i
; w
d
) (1)
+
X
w
i
2SNT
max
w
e
2EX
s
k
sim(w
i
; w
e
)
Sense preference is obtained by calculating sim-
ilarity between words in SNT and words in DE-
F and EX. For all words in an input sentence
(w
i
2SNT), we sum up the maximum similarity
between w
i
and all clue words (i.e. w
d
and w
e
).
To get similarity between words (sim(w
i
; w
j
)),
we use WordNet and the metric proposed in
Rigau et al (1997).
Senses in a dictionary are generally ordered
by frequency. To reect distribution of senses
in common text, we use a weight factor (s
k
)
that is inversely proportional to the order of a
sense s
k
in a dictionary. Then, we calculate
normalized sense preference to combine sense
preference and sense probability.
spf
w
(s
k
) = (s
k
) 
spf(s
k
)
P
i
spf(s
i
)
(2)
spf
norm
(s
k
) =
spf
w
(s
k
)
P
i
spf
w
(s
i
)
(3)
3.2 Sense Probability
Sense probability represents how likely target
words with the same sense co-occur with trans-
lations of other words within the input sentence.
Let us suppose that the i-th word in an input
sentence is s
i
and the k-th sense of s
i
is s
k
i
.
Then, sense probability sp(s
k
i
) is computed as
follows:
n(t
k
iq
) =
X
(s
j
;m;c)2(s
i
)
m
X
p=1
f(t
k
iq
; t
jp
; c)
f(t
k
iq
) + f(t
jp
)
(4)
sp(s
k
i
) = p^
sense
(s
k
i
) =
P
q
n(t
k
iq
)
P
x
P
y
n(t
x
iy
)
(5)
In the equation, (s
i
) signies a set of word-
s that co-occur with s
i
on syntactic relations.
In an element (s
j
;m; c) of (s
i
), s
j
is a word
that co-occurs with s
i
in an input sentences, c
is a syntactic relation
2
between s
i
and s
j
, and
m is the number of translations of s
j
. Provided
that the set of translations of a sense s
k
i
is T
k
i
and a member of T
k
i
is t
k
iq
, the frequency of co-
occurring t
k
iq
and t
jp
with a syntactic relation
c is denoted as f(t
k
iq
; t
jp
; c), which is extract-
ed from target language corpora.
3
Therefore,
2
We guess 37 syntactic relations in English sen-
tences based on verb pattern information in the dic-
tionary and the results of the memory based shallow
parser (Daelemans et al, 1999): subj-verb, obj-verb,
modifier-noun, adverb-modifiee, etc.
3
When `jeobsi' has 4 translations (`plate', `dish',
`saucer', `platter'), (kkae-da) has a member (jeob-
n(t
k
iq
) in the equation (4) represents how fre-
quently t
k
iq
co-occurs with translations of s
j
. By
summing up n(t
k
iq
) for all target words in T
k
i
, we
obtain sense probability of s
k
i
.
3.3 Word Probability
Word probability represents how frequently a
target word in a sense division co-occurs in a
corpus with translations of other words within
the input sentence. We denote word probability
as wp(t
k
iq
) that is a probability of selecting t
k
iq
from T
k
i
. Using n(t
k
iq
) in the equation (4), we
calculate word probability as follows:
wp(t
k
iq
) = p^
word
(t
k
iq
) =
n(t
k
iq
)
P
x
n(t
k
ix
)
(6)
3.4 Translation Preference
To select a target word among all translations
of a source word, we compute translation pref-
erence (tpf) by merging sense preference, sense
probability and word probability. We simply
sum up the value of sense preference and sense
probability as a measure of sense disambigua-
tion, and then multiply it with word probability
as follows:
tpf(t
k
iq
) = (? spf
norm
(s
k
i
) + (1 ?)sp(s
k
i
)) (7)
 wp(t
k
iq
)=(T
k
i
)
In the equation, (T
k
i
) is a normalizing factor
for wp(t
k
iq
)
4
, and ? is a weighting factor for sense
preference. We select a target word with the
highest tpf as a translation.
When all of the n(t
k
iq
) in the equation (4) are
0, we use the following equation for smoothing,
which uses only frequency of a target word.
n(t
k
iq
) =  
f(t
k
iq
)
P
p
f(t
k
ip
)
(8)
si, 4, verb-obj) in the example of `jeobsi-reul kkae-
da', and then the following frequencies will be used:
f(break, plate, verb-obj), f(break, dish, verb-obj),...,
f(crack, platter, verb-obj).
4
Because wp(t
k
iq
) is a probability of t
k
iq
within T
k
i
,
wp(t
k
iq
) becomes 1 when T
k
i
has only one element. (T
k
i
)
is used to make the maximum value of wp(t
k
iq
)=(T
k
i
) to
1. Hence, (T
k
i
) = max
j
wp(t
k
ij
). Using (T
k
i
), we can
prevent discounting translation preference of a word, the
sense of which has many corresponding target words.
4 Evaluation
We evaluated our method on English-to-Korean
translation. EssEEK (1995) was used as a bilin-
gual dictionary. We converted this English-
English Korean dictionary into a machine read-
able form, which has about 43,000 entries,
34,000 unique words and 80,000 senses. From
the dictionary, we extracted sense denition-
s, example sentences and translations for each
sense. Co-occurrence of Korean was extract-
ed from Yonsei Corpus, KAIST corpus and Se-
jong Corpus using the method proposed in Yoon
(2001). The number of extracted co-occurrence
is about 600,000.
To exclude any kind of human intervention
during knowledge extraction and evaluation, we
evaluated our method similarly with Koehn and
Knight (2001). English-Korean aligned sen-
tences were collected from novels, textbooks for
high school students and sentences in a Korean-
to-English bilingual dictionary. Among those
sentences, we extracted sentence pairs in which
words in the English sentence satisfy the fol-
lowing condition: if the paired Korean sentence
contains a word that is dened in the dictio-
nary as a translation of the English word. We
randomly chose 945 sentences as an evaluation
set, in which 3,081 words in English sentences
satisfy the condition. Among them, 1,653 are
nouns, 990 are verbs, 322 are adjectives, and
116 are adverbs.
We used Brill's tagger (Brill, 1995) and
Memory-Based Shallow Parser (Daelemans et
al., 1999) to analyze English sentences. To an-
alyze Korean sentences, we used a POS tagger
for Korean (Kim and Kim, 1996).
First, we evaluated the accuracies of sense
preference (spf) and sense probability (sp). If
any target word of the sense with the highest
value is included in an aligned target language
sentence, we regarded it as correct result.
The accuracy of sense preference is shown in
Table 1. In the table, a w/o DEF row shows
the result produced by removing the DEF clue
in the equation (1), and a w/o ORDER row is
the result produced without the weight factor
(s
k
) of the equation (2). As shown in the table,
the result with the weight factor and without a
sense denition sentence is best for all cases. We
will discuss this result in the next section.
The accuracy of sense probability is shown in
Table 1: Accuracy of sense preference (spf)
noun verb adj. adv. all
w/o ORDER w/o DEF 59.23% 42.93% 45.03% 34.48% 52.47%
with DEF 57.65% 40.20% 49.07% 33.62% 51.14%
with ORDER
5
w/o DEF 66.30% 48.48% 59.32% 44.83% 59.94%
with DEF 65.76% 45.56% 56.52% 42.24% 58.41%
Table 2: Accuracy of sense probability (sp)
noun verb adj. adv. all
with all cooc word 52.49% 30.49% 38.94% 59.35% 46.52%
with case cooc word 49.62% 31.33% 40.12% 60.98% 45.41%
Table 3: Accuracy of translation selection
noun verb adj. adv. all
random selection 11.11% 3.92% 7.41% 11.12% 6.77%
1st translation of 1st sense 12.89% 12.12% 7.45% 2.59% 11.86%
most frequent 46.34% 27.58% 31.99% 37.93% 38.68%
spf 53.72% 38.18% 39.75% 44.83% 46.98%
with sp 42.88% 19.28% 26.25% 39.02% 35.04%
all wp 51.66% 31.41% 32.92% 46.55% 43.10%
cooc spwp 51.97% 31.92% 33.54% 50.00% 43.62%
word spfwp 55.23% 40.00% 41.30% 50.00% 50.17%
(spf+sp)wp
6
54.99% 34.55% 36.34% 50.86% 46.42%
with sp 38.90% 20.20% 25.96% 37.40% 33.05%
case wp 48.70% 34.04% 35.40% 45.69% 42.58%
cooc spwp 49.12% 33.74% 36.34% 50.86% 43.01%
word spfwp 53.60% 42.22% 42.86% 50.86% 49.93%
(spf+sp)wp
6
51.18% 36.46% 38.20% 53.45% 45.28%
Table 2. In the equation (4), we proposed to use
syntactic relations to get sense probability. In
the table, the result obtained by using words on
a syntactic relation (with case cooc word) is com-
pared to that obtained by using all words with-
in a sentence (with all cooc word). As shown,
the accuracy for nouns is higher when using all
words in a sentence, whereas the accuracy for
others is higher when considering syntactic re-
lations.
Table 3 shows the result of translation se-
lection.
7
We set three types of baseline - ran-
dom selection, most frequent, 1st translation of
1st sense
8
. We conducted the experiment al-
5
In the case of (1st sense)=1.5, (2nd sense)=1.3,
(3rd sense)=1.15, and (remainder)=1
6
In the case of ?=0.6, which shows the best result in
the experiment.
7
To get spf, we use only the best combination of clues
- with the weight factor and without sense denition sen-
tences.
8
Result of selecting translation that is recorded at
rst place in the rst sense division of a source word.
tering the combination of spf, sp and wp. An
spf row shows the accuracy of selecting the rst
translation of the sense with the highest spf, and
an sp row shows the accuracy of selecting the
rst translation of the sense with the highest
sp. A wp row shows the accuracy of using only
word probability. In other words, it is obtained
through assuming all translations of a source
word to have the same sense. Therefore, the
result in a wp row can be regarded as a result
produced by the method that uses only target
word co-occurrence. An spwp row is the result
obtained without spf
norm
, and an spfwp row
is the result obtained without sp in the equa-
tion (8). An (spf+sp)wp row is the result of
combining all measures.
For the best case, the accuracy for nouns is
55.23%, that for verbs is 42.22% and that for
adjectives is 42.86%. Although we use a simple
method for sense disambiguation, our results of
translation selection are superior to the results
that are produced using only target word co-
occurrence (wp rows).
5 Discussion
We could summarize the advantage of our
method as follows:
 reduction of the problem complexity
 simplifying knowledge acquisition
 selection of more appropriate translation
 use of a mono-bilingual dictionary
 integration of syntactic relations
 guarantee of robust result
The gure below shows the average number of
senses and translations for an English word in
EssEEK (1995). The left half of the graph is
for all English words in the dictionary, and the
right half is for the words that are marked as
frequent or signicant words in the dictionary.
On the average, a frequently appearing English
word has 2.82 senses, each sense has 1.96 Ko-
rean translations, and consequently an English
word has 5.55 Korean translations. Moreover,
in our evaluation set, a word has 6.86 senses and
a word has 14.78 translations. Most approaches
on translation selection have tried to translate
a source word directly into a target word, thus
the complexity of their method grows propor-
tional to the number of translations per word.
In contrast, the complexity of our method is
proportional to only the number of senses per
word or the number of translations per sense.
Therefore, we could reduce the complexity of
translation selection.
When the degree of ambiguity of a source lan-
guage is n and that of a target language is m,
0
1
2
3
4
5
6
7
all noun verb adj adv all noun verb adj adv
all word frequent word
sense per word translation per sense translation per word
Figure 3: The average number of senses and
translations for an English word
the complexity of translation is nearly nm.
Therefore, knowledge acquisition for translation
is more complicated than other problems of nat-
ural language processing. Although the com-
plexity of knowledge acquisition of the meth-
ods based on a target language corpus is on-
ly m, ignorance of a source language results in
selecting inappropriate target words. In con-
trast, by splitting translation selection into two
sub-problems, our method uses only existing re-
sources - a bilingual dictionary and a target lan-
guage monolingual corpus. Therefore, we could
alleviate the complexity of knowledge acquisi-
tion to around n+m.
As shown in the previous section, our method
could select more appropriate translation than
the method based on target word co-occurrence,
although we used coarse knowledge for sense
disambiguation. It is because the co-occurrence
based method does not consider sense ambigu-
ity of a target word. Consider the translation
of the English phrase `have a car'. A word `car'
is translated into a Korean word `cha', which
has many meanings including car and tea. A
word `have' has many senses - posses, eat, drink,
etc., among which the drink sense is mapped in-
to a Korean verb `masi-da'. Because of the tea
sense of `cha', the frequency of co-occuring `cha'
and `masi-da' is dominant in a corpus over that
of all other translations of `car' and `have'. In
that case, the method that uses only word co-
occurrence (wp in our experiment) translated
`have a car' into an incorrect sentence `cha-reul
masi-da' that means `have a tea'. In contrast,
our method produced a correct translation `cha-
reul kaji-da', because we disambiguate the sense
of `have' as posses by employing sense preference
before using co-occurrence.
In the experiment, the combination spfwp
gets higher accuracy than (spf+sp)wp for most
cases. It is due to the low accuracy of sp, which
is also caused by the ambiguity of a target word
as shown in the example of `have a car'. Nev-
ertheless, we do not think it means sense prob-
ability is useless. The accuracies of spwp are
almost better than those of wp, therefore sense
probability can work as a countermeasure when
no knowledge of a source language exists for dis-
ambiguating the sense of a word.
In this paper, we used a mono-bilingual dic-
tionary, which has more information than a
common bilingual dictionary including sense
denitions in a source language and syntactic
patterns for the predicate. While previous re-
search on sense disambiguation that exploits
those kinds of information has shown reliable
results, in our experiment, the use of sense def-
initions lowers the accuracy. It is likely because
we used sense denitions too primitively. There-
fore, we expect that we could increase the ac-
curacy by properly utilizing additional informa-
tion in the dictionary.
Many studies on translation selection have
concerned with translation of only nouns. In
particular, some of them use all co-occurring
words within a sentence, and others use a few re-
stricted types of syntactic relations. Even more,
each syntactic relation is utilized independently.
In this paper, we proposed a statistical model
that integrates various syntactic relations, with
which the accuracies for verbs, adjectives, and
adverbs increase. Nevertheless, since the accu-
racy for nouns is higher when using all words
in a sentence, it seems that syntactic relations
may be dierently used for each case.
The other advantage of our method is ro-
bustness. Since our method disambiguates the
sense of a source language word using reliable
knowledge in a dictionary, we could avoid select-
ing or generating a target word the meaning of
which is denitely improper in given sentences
like `hatch a dish' for 'jeobsi-reul kkae-da' and
`cha-reul masi-da' for `have a car'.
6 Conclusion
In this paper, we proposed a hybrid method for
translation selection. By dividing translation
selection into sense disambiguation of a source
word and selection of a target word, we could
simplify knowledge acquisition and select more
appropriate translation.
Acknowledgement
We are grateful to Sabine Buchholz, Bertjan
Busser at Tilburg University and professor Wal-
ter Daelemans at University of Antwerp for the
aid to use the Memory-Based Shallow Parser
(MBSP).
References
Eric Brill. 1995. Transformation-based error-
driven learning and natural language process-
ing : A case study in part of speech tagging.
Computational Linguistics, 21(4).
Peter F. Brown, John Cocke, Vincent Della
Pietra, Stephen Della Pietra, Frederick Je-
linek, John D. Laerty, Robert L. Mercer,
and Paul S. Roossin. 1990. A statistical ap-
proach to machine translation. Computation-
al Linguistics, 16(2).
Walter Daelemans, Sabine Buchholz, and Jorn
Veenstra. 1999. Memory-based shallow pars-
ing. In Proceedings of the 3rd Internation-
al Workshop on Computational Natural Lan-
guage Learning (CoNLL-'99).
Ido Dagan and Alon Itai. 1994. Word sense dis-
ambiguation using a second language mono-
ligual corpus. Computational Linguistics,
20(4).
EssEEK, 1995. Essence English-English Korean
Dictionary. MinJung SeoRim.
Jae-Hoon Kim and Gil Chang Kim. 1996.
Fuzzy network model for part-of-speech tag-
ging under small training data. Natural Lan-
guage Engineering, 2(2).
Philipp Koehn and Kevin Knight. 2001.
Knowledge sources for word-level translation
models. In Proceedings of Empirical Method-
s in Natural Language Processing conference
(EMNLP-2001).
Hyun Ah Lee, Jong C. Park, and Gil Chang
Kim. 1999. Lexical selection with a target
language monolingual corpus and an mrd. In
Proceedings of the 8th International Confer-
ence on Theoretical and Methodological Issues
in Machine Translation (TMI-'99).
Detlef Prescher, Stefan Riezler, and Mats
Rooth. 2000. Using a probabilistic class-
based lexicon for lexical ambiguity resolu-
tion. In Proceedings of the 18th Internation-
al Conference on Computational Linguistics
(COLING-2000).
German Rigau, Jordi Atserias, and Eneko A-
girre. 1997. Combining unsupervised lexi-
cal knowledge methods for word sense disam-
biguation. In Proceedings of the 35th Annual
Meeting of the Association for Computational
Linguistics (ACL-'97).
Juntae Yoon. 2001. E?cient dependency analy-
sis for korean sentences based on lexical asso-
ciation and multi-layered chunking. Literary
and Linguistic Computing, 16(3).
