Paraphrasing and Aggregating Argumentative Text 
Using Text Structure 
Xiaorong Huang Armin Fiedler 
Fachbereich Informatik, Universit/it des Saarlandes 
Postfach 15 11 50, D-66041 Saarbriicken, Germany 
{huang I a f ied le r}Ocs ,  un i - sb ,  de 
Abst rac t  
We argue in this paper that sophisticated mi- 
croplanning techniques are required even for 
mathematical proofs, in contrast to the belief 
that mathematical texts are only schematic 
and mechanical. We demonstrate why para- 
phrasing and aggregation significantly en- 
hance the flexibility and the coherence of 
the text produced. To this end, we adopted 
the Text Structure of Meteer as our basic 
representation. The type checking mecha- 
nism of Text Structure allows us to achieve 
paraphrasing by building comparable combi- 
nations of linguistic resources. Specified in 
terms of concepts in an uniform ontological 
structure called the Upper Model, our se- 
mantic aggregation rules are more compact 
than similar rules reported in the literature. 
1 In t roduct ion  
Many of the first NLG systems link their 
information structure to the corresponding 
linguistic resources either through predefined 
templates or via careful engineering for a spe- 
cific application. Therefore their expressive 
power is restricted (see \[12\] for an extensive 
discussion). An increasing interest in more 
sophisticated microplanning techniques can 
be clearly observed \[12, 14\], however. In 
this paper, we first motivate the needs for 
paraphrasing and aggregation for the gener- 
ation of argumentative texts, in particular of 
mathematical proofs, and then describe how 
our microplanning operations can be formu- 
lated in terms of Meteer's Text Structure. 
The work reported here is part of a 
fully implemented system called PRO VERB, 
which produces natural language proofs from 
proofs found by' automated reasoning sys- 
tems \[7\]. First experiments with PRO VERB 
resulted in very mechanical texts due to the 
lack of microplanning techniques. According 
to our analysis, there are at least two lin- 
guistic phenomena that call for appropriate 
microplanning techniques. 
First, naturally occurring proofs contain 
paraphrases with respect o both rhetorical 
relations, as well as to logical functions or 
predicates. For instance, the derivation of B 
from A can be verbalized as: 
"Since A, B." or as 
"A leads to B." 
The logic predicate para(C1, C2), also, 
can be verbalized as: 
"Line C1 parallels line C2." or as 
"The parallelism of the lines C1 and 
C2." 
Second, without microplanning 
PROVERB generates text structured ex- 
actly mirroring the information structure of 
the proof and the formulae. This means that 
every step of derivation is translated into a 
separate sentence, and formulae are recur- 
sively verbalized. As an instance of the lat- 
ter, the formula 
Set(F) A Subset(F, G) (1) 
is verbalized as 
21 
"F is a set. F is a subset of G." 
although the following is much more natural: 
"The set F is a subset of G." 
Therefore, we came to the conclusion that 
an intermediate l vel of representation is nec- 
essary that allows flexible combinations of 
linguistic resources. It is worth pointing out 
that these techniques are required although 
the input information chunks are of clause 
size. Another requirement is that this in- 
termediate representation is easy to control, 
since a mathematical text must conform to 
the syntactic rules of its sublanguage. In the 
next section, we first give a brief overview of 
PROVERB. Then we describe the architec- 
ture of our microplanner, and illustrate how 
Meteer's Text Structure can be adopted as 
our central representation. In Sec. 5 and 6 
we describe the handling of paraphrases and 
aggregation rules, two of the major tasks of 
our microplanner. 
2 The Macroplanner of 
P R 0 VERB 
The macroplanner of PROVERB combines 
hierarchical planning \[13\] with local organi- 
zation \[15\] in a uniform planning framework 
\[6\]. The hierarchical planning is realized 
by so-called top-down presentation operators 
that split the task of presenting a particular 
proof into subtasks of presenting subproofs. 
While the overall planning mechanism is sim- 
ilar to the RST-based planning approach, 
the plan operators resemble the schemata in 
schema-based planning. The output of the 
macroplanner is an ordered sequence of proof 
communicative acts (PCAs). 
PCAs are the primitive actions planned 
during macroplanning to achieve commu- 
nicative goals. Like speech acts, PCAs can 
be defined in terms of the communicative 
goals they fulfill as well as in terms of their 
possible verbalizations. Based on an analysis 
of proofs in mathematical textbooks, there 
are mainly two types of goals: 
Conveying derivation step: In terms of 
rhetorical relations, PCAs in this category 
represent a variation of the rhetorical rela- 
tion derive \[8\]. Below we examine the simple 
PCA called Der ive as an example. 
(Der ive  Reasons:  (a 6F ,  F C G) 
Method : de f - subset  
Conclusion: a 6G) 
Depending on the reference choices, the 
following is a possible verbalization: 
"Since a is an element of F and F is a 
subset of G, a is an element of G by the 
definition of subset." 
Updating the global attentional structure: 
These PCAs either convey a partial plan for 
the forthcoming discourse or signal the end 
of a subproof. PCAs of this sort are also 
called meta-comments \[16\]. 
The PCA 
(Beg in -Cases  Goal : Formula 
Assumptions: (A B)) 
produces the verbalization: 
"To prove Formula, let us consider the 
two cases by assuming A and B." 
3 Text Structure in 
P R 0 VERB 
3.1 In t roduct ion  and  Genera l  
S t ructure  
Text Structure is first proposed by Meteer 
\[11, 12\] in order to bridge the generation gap 
between the representation i  the application 
program and the linguistic resources pro- 
vided by the language. By abstracting over 
concrete linguistic resources, Text Structure 
should supply the planner with basic vocab- 
ularies, with which it chooses linguistic re- 
sources. Meteer's text structure is organized 
as a tree, in which each node represents a
constituent of the text. In this form it con- 
tains three types of linguistic information: 
constituency, structural relations among con- 
stituents, and in particular, the semantic at- 
egories the constituents express. 
The main role of the semantic categories 
is to provide vocabularies which specify type 
22 
restrictions for nodes. They define how sep- 
arate Text Structures can be combined, and 
ensure that the planner only builds express- 
ible Text Structures. For instance if tree A 
should be expanded at node n by tree B, the 
resulting type of B must be compatible to 
the type restriction attached to n. Panaget 
\[14\] argues, however, that Meteer's emantic 
categories mix the ideational and the textual 
dimension as argued in the systemic linguis- 
tic theory \[5\]. Here is one of his examples: 
"The ship sank" is an ideational event, 
and it is textually presented from an EVENT- 
PERSPECTIVE. "The sinking of the ship" is 
still an ideational event, but now presented 
from an OBJECT-PERSPECTIVE. 
On account of this, Panaget split the type 
restrictions into two orthogonal dimensions: 
the ideational dimension in terms of the Up- 
per Model \[1\], and the hierarchy of textual 
semantic categories based on an analysis of 
French and of English. In our work, we ba- 
sically follow the approach of Panaget. 
Technically speaking, the Text Structure 
in PROVERB is a tree recursively composed 
of kernel subtrees or composite subtrees: 
An atomic kernel subtree has a head at the 
root and arguments as children, representing 
basically a predicate/argument structure. 
Composite subtrees can be divided into two 
subtypes: the first has a special matrix child 
and zero or more adjunct children and rep- 
resents linguistic hypotaxis, the second has 
two or more coordinated children and stands 
for parataxis. 
3.2 Type  Rest r i c t ions  
Each node is typed both in terms of the 
Upper Model and the hierarchy of textual 
semantic categories. The Upper Model is 
a domain-independent property inheritance 
network of concepts that are hierarchically 
organized according to how they can be lin- 
guistically expressed. Figure 1 shows a frag- 
ment of the Upper Model in PRO VERB. For 
every domain of application, domain-specific 
concepts must be identified and placed as an 
extension of the Upper Model. 
The hierarchy of textual semantic cate- 
gories is also a domain-independent property 
inheritance network. The concepts axe or- 
ganized in a hierarchy based on their tex- 
tual realization. For example, the concept 
clause-modifier-rankingl t is realized as an ad- 
verb, clause-modifier-rankingll as a preposi- 
tional phrase, and clause-modifier-embedded 
as an adverbial clause. Fig. 2 shows a frag- 
ment of the hierarchy of textual semantic 
categories. 
3.3 Mapp ing  APOs  to  UMOs 
The mapping from the content o the linguis- 
tic resources now happens in a two-staged 
way. While Meteer associates the applica- 
tion program objects (APOs) directly with 
so-called resources trees, we map APOs into 
Upper Model objects, which in turn are ex- 
panded to the Text Structures. It is worth 
noting that there is a practical advantage of 
this two-staged process. Instead of having to 
construct resource trees for APOs, the user 
of our system only needs to define a map- 
ping from the APOs to Upper Model objects 
(UMOs). 
When mapping APOs to UMOs, the mi- 
croplanner must choose among available al- 
ternatives. For example, the application 
program object para that stands for the log- 
ical predicate denoting the parallelism rela- 
tion between lines may map in five different 
Upper Model concepts. In the 0-place case, 
para can be mapped into object leading to 
the noun "parallelism," or quality, leading 
to the adjective "parallel." In the binary 
case, the choices are property-ascription that 
may be verbalized as "x and y are parallel," 
quality-relation that allows the verbalization 
as "x is parallel to y", or process-relation, 
that is the formula "x II Y." 
The mapping of Upper Model objects into 
the Text Structure is defined by so-called 
resource trees, i.e. reified instances of text 
structure subtrees. The resource trees of an 
Upper Model concept are assembled in its 
realization class. 
~Concepts of the hierarchy of textual semantic 
categories are noted in sans-serif text. 
23 
concept  
" mod i f ied -concept  
f -  consc ious -be ing  
- ob jec t  ---q . .  
t -  non-concious-Uung 
\ [ -  re la t iona l -p rocesses -  
- p rocess  
t. menta l -p rocesses  
._~- moda l -qua l io '  
qua l i ty  t.  mater ia l -word-qua l i ty  
r "  log ica l  
- a rb i t ra ty -p lace- re la t ion -~ 
, -  sequence  
- genera l i zed-possess ion  
- quant i f i ca t ions  . ,  . 
d i screte -p lace- re la t ion  ~ . . r "  taent t ty  r "  p roper ty -ascr ip t ion  
tn tenstve  - - - '1  . . 
t -  ascript ion -1_ c i rcumstant ia l  c lass -ascr ip t ion  
Figure 1: A Fragment of Upper Model in PROVERB 
- text 
- sentence 
- c lause 
category  - vp 
-np 
modi f ie r -  
c lause-modif ier  
vp-modi f ier  
np-modi f ier  
intensif ier 
_ c lause-modi f ier - rank ing l  c lause-modi f ier - rank ing l l  
c lause-mod i f ie r -embedded 
Figure 2: A Fragment of the Hierarchy of Textual Semantic Categories in PROVERB 
4 Architecture and Control 
The main tasks of our microplanner in- 
clude aggregation to remove redundancies, 
insertion of cue words to increase coher- 
ence, and reference choices, as well as lexical 
choices. Apart from that, the microplanner 
also handles entence scoping and layout. An 
overview of the microplanner's architecture 
is provided in Figure 3. 
Our microplanner takes as input an or- 
dered sequence of PCAs, structured in an 
attentional hierarchy. The first module, 
the derivation reference choice component 
(DRCC), suggests which parts of a PCA are 
to be verbalized. This is done based on the 
hierarchical discourse structure as well as on 
the textual distance. PCAs annotated with 
these decisions annotated are called preverbal 
messages (PMs). 
Starting from a list of PMs as the initial 
Text Structure, the microplanner progres- 
sively maps application program concepts in 
PMs into text structure objects of some tex- 
tual semantic type by referring to Upper 
Model objects as an intermediate level. The 
Text Structure evolves by the expansion of 
leaves top-down and left to right. This pro- 
cess is controlled by the main module of our 
microplanner, the Text Structure Generator 
(TSG), which carries out the following algo- 
rithm: 
? When the current node is an APO with 
more than one son, apply ordering and ag- 
gregation, in order to produce more con- 
cise and more coherent text. The appli- 
cation of an aggregating rule before the 
expansion of a leaf node may trigger the 
insertion of cue words. 
? An APO is mapped into an UMO, which 
is in turn expanded into a Text Structure 
by choosing an appropriate resource tree. 
24 
Natural Deduction Proof 
Macroplanner 
. . . . . . . . . . . . . . . . . . . . . . . . .  R) k: . . . . . . . . . . . . . . . . . . . . .  
gregation rules. 
5 Paraphrasing in PROVERB 
With the help of a concrete example we 
M~c'r~p\]~e\[ . . . . . . . . .  ~ . . . . . . . . . . . . . . . . . . . . . .  illustrate in this section how the Text Struc- 
Text Structure 
Expansion 
Sentence Striping 
Lexical Choice 
Ordering 
Aggregation 
Cue Word 
Insertion 
Layout 
F 
~ \[ Realizatioa Closes 
\] Textual Semantic 
I Categories 
Text Structure 
Transformer ) 
6 
( ) 
Figure 3: Architecture of the Microplanner 
? A fully expanded Text Structure will be 
traversed again: 
- -  to choose the appropriate lexical items. 
- -  to make sentence scoping decisions by 
singling out one candidate textual se- 
mantic category for each constituent. 
This in turn may trigger the execution 
of a cue word rule. For instance, the 
choice of the category sentence for a 
constituent may lead to the insertion of 
the cue word "furthermore" in the next 
sentence. 
- - to  determine the layout parameters, 
which will be realized later as ~TEX- 
commands in the final output text. 
A Text Structure constructed in this way 
is the output of our microplanner, and will 
be transformed into the input formalism of 
TAG-GEN \[10\], our linguistic realizer. 
In the next two sections, we concentrate 
on two major tasks of the Text Structure 
generator: to choose compatible paraphrases 
of application program concepts, and to im- 
prove the textual structure by applying ag- 
ture generator chooses among paraphrases 
and avoids building inexpressible text struc- 
tures via type checking. 
Example  We examine a simple logic for- 
mula derive(para(C1,C2),B). Note that B 
stands for a conclusion which will not be ex- 
amined here. We will also not follow the pro- 
cedure in detail. 
In the current implementation, the rhetor- 
ical relation derive is only connected to one 
Upper Model concept derive, a subconcept 
of cause.relation. The realization class as- 
sociated to the concept, however, contains 
several alternative resource trees leading to 
different patterns of verbalization. We only 
list two variations below: 
? B, since A. 
? Because of A, B. 
The resource tree of the first alternative is 
given in Fig. 4. 
The logic predicate para(C1,  C2) can be 
mapped to one of the following Upper Model 
concepts, where we always include one pos- 
sible verbalization: 
? quality-relation(para, C1, C2) 
(line C1 is parallel to C2) 
? process-relation(para, C1,C2) 
(C1\[\[C2) 
? property-ascription(para, Cl A C2) 
(lines C1 and C2 are parallel) 
Textually, the property-ascription version 
can be realized in two forms, represented by 
the two resource trees in Fig. 5. 
Type checking during the construction of 
the Text Structure must ensure, that the 
realization be compatible along both the 
ideational and the textual dimension. In this 
example, the combination of the tree in Fig. 4 
and the first tree in Fig. 5 is compatible and 
will lead to the verbalization: 
"B, since C1 and C2 are parallel." 
25 
(realization-class (derive :reason R :conclusion C) 
(resource-tree (composite-tree :content nil 
:tsc (sentence clause) 
:matrix (leaf :content C 
: t sc  (clause)) 
:adjunct (composite-tree :content since 
:tsc (clause) 
:matrix (leaf :content R 
: t sc  (c lause) )  ) ) ) 
(.further resource trees . . . )) 
Figure 4: The Realization Class for derive 
<lex be> 
vp 
head 
arguraent  a rgu lnent  
conj(C,, C~) Para 
no no 
As a verb phrase 
? nil 
comproPsite \[ 
matrix adjunct 
Para conj(C,, CC~) 
no modifier 
As a nominal phrase 
Figure 5: Textual Variations in form of Re- 
source Trees 
The second tree in Fig. 5, however, can 
only be combined with another ealization of 
derive, resulting in: 
"Because of the parallelism of line C1 
and line C2, B." 
In our current system we concentrat on 
the mechanism and are therefore still exper- 
imenting with heuristics which control the 
choice of paraphrases. One interesting rule 
is to distinguish between general rhetorical 
relations and domain specific mathematical 
concepts. While the former should be para- 
phrased to increase the flexibility, continuity 
of the latter helps the user to identify tech- 
nical concepts. 
6 Semantic Aggregation 
Rules 
Although the handling of paraphrase gen- 
eration already increases the flexibility in 
the text, the default verbalization strategy 
will still expand the Text Structure by re- 
cursively descending the proof and formula 
structure, and thereby forced to keep these 
structures. To achieve the second verbal- 
ization of equation (1) in the introduction, 
however, we have to combine Set(F) and 
Subset(F, G) to form an embedded structure 
Subset(Set(F), G). Clearly, although still in 
the same format, this is no more an Up- 
per Model object, since Set(F) is an Upper 
Model process, not an object. Actually, this 
documents a textual decision that no mat- 
ter how Subset and Set should be instanti- 
ated, the argument F in Subset(F, G) will be 
replaced by Set(F). This textual operation 
eliminates one of the duplicates of F.  This 
section is devoted to various textual reorgan- 
isations which eliminate such redundancies. 
Following the tradition, we call them aggre- 
gation rules. 
As it will become clear when handling con- 
crete aggregation rules, such rules may nar- 
row the realization choices of APOs by im- 
posing additional type restrictions. Further- 
more, some realization choices block desir- 
able textual reorganisation. On account of 
this we carry out aggregations before con- 
crete resources for the APOs like object and 
class-ascription are chosen. 
APOs, before they are mapped to UMOs, 
can be viewed as variables for UMOs (for 
convenience, we continue to refer to them as 
APOs). In this sense, our rules work with 
such variables at the semantic level of the 
Upper Model, and therefore differ from those 
more syntactic rules reported in the litera- 
ture. For a comparison see Sec. 6.4. 
So far, we have investigated three types of 
aggregation which will be addressed in the 
next two subsections. A categorization of the 
aggregation rules is given in Fig. 6. 
26 
Grouping (5) 
Logical Predicates (1) 
Aggregation(l 1)
Embedding (2) 
PMs (2) Logical Connectives (2) 
Figure 6: Aggregation Rules in PROVERB 
Pattern (4) 
Chaining (3) Others (1) 
6.1 Semant ic  Group ing  
We use semantic grouping to characterize the 
merge of two parallel Text Structure objects 
with the same top-concept by grouping their 
arguments. Two APOs are parallel in the 
sense that they have the same parent node. 
The general form of this type of rules can be 
characterized by the pattern as given below: 
Rule Pat tern  A 
P\[a\] + P\[b\] 
P\[aCb\] 
The syntax of our rules means that a text 
structure of the form above the bar will be 
transformed into one of the form below the 
bar. Viewing Text Structure as a tree, P\[a\] 
and P\[b\] are both sons of +, they are merged 
together by grouping the arguments a and b 
under another operator ~.  In the first rule 
below, + and ~ are identical. 
Rule A.1 (Predicate Grouping)  
P\[a\] + P\[b\] 
P\[a + b\] 
where + can be either a logical A or a logical 
V, and P stands for a logical predicate. The 
following example illustrates the effect of this 
rule. 
Set(F) A Set(G) 
"F is a set. G is a set." 
are aggregated to: 
Set(F A G) 
"F and G are sets." 
The rule covers the predicate grouping rule 
reported in \[3\]. This is also the best place 
to explain why we apply aggregation before 
choosing concrete linguistic resources. If the 
two occurrences of Set are instantiated if- 
ferently, this rule will be blocked. 
Now let us examine another semantic 
grouping rule, where + and ~ are no longer 
identical. 
Rule A.2 ( Impl icat ion w i th  identical 
conclus ion ) 
c) A (P2 c) 
(& v P2) c 
Here +, ~,  and P are instantiated to A, 
V, and ~,  respectively. By instantiating +, 
E\[~ and P in pattern A to different logical 
connectives and derivation relations, we have 
alltogether five rules in this category. The 
correctness of the rules in this category with 
respect o the information conveyed is guar- 
anteed by the semantics of the Upper Model 
concerned. In the case of rule A.2 for in- 
stance, (PiVP2) ~ C is a logical consequence 
of (P1 ~ C) A (P2 ~ C). 
6.2 Semant ic  Embedd ing  
The next category of aggregation rules han- 
dles parallel structures which are not identi- 
cal. In this case, some of them may be con- 
verted to embedded structures, as is done by 
the following rule. 
Rule B.1 (Ob ject  Embedd ing)  
P\[T\] A Q\[T\] 
Q\[P\[T\]\] 
where 
? concepts(f,T) Mconcepts(P) # 0 
27 
? f is the innermost application program 
concept.governing T in Q\[T\], 
? concepts(f, T) denotes the Upper Model 
concepts the argument T of f may take, 
? concepts(P) denotes the Upper Model 
concept P may result in. 
We require also that PIT\] is realized as an 
object T with modifiers. It is this intuitive 
explanation which guarantees the correctness 
of this rule with respect o meaning. 
The following example illustrates this rule, 
in particular, how the decision made here 
narrows the choices of linguistic resources for 
both P and T as an argument of Q. We begin 
with the two APOs in a conjunction below, 
containing a common APO F. 
Set(F) A Subset(F, G) 
"F is a set. F is a subset of G." 
Since F is directly governed by Subset, 
f and Q in our rule above coincide here. 
concepts(Subset, F) = {object), while 
concepts(Set) = (class-ascription, object). 
Therefore, their intersection is {object). 
This not only guarantees the expressibility 
of the new APO, but also restricts the choice 
of linguistic resources for Set, now restricted 
to object. The result as well as its verbaliza- 
tion is given below: 
Subset(Set(F), G) 
"The set F is a subset of G." 
Actually, for mathematical texts we have 
only used two embedding rules, with the 
other being the dual of rule B.1 where P and 
Q change their places. 
6.3 Pat tern -based  
ru les  
Opt imizat ion  
Rules in the third category involve more 
complex changes of the textual structure in 
a way which is neither a grouping nor an em- 
bedding. They could be understood as some 
domain-specific communicative conventions, 
and must be explored in every domain of ap- 
plication. In PRO VERB, currently four such 
rules are integrated. Three of them build a 
sequence of some transitive relations into a 
chain. 
Rule C. 1 below addresses the problem that 
every step of derivation is mapped to a sepa- 
rate sentence in the default verbalization. It 
reflects the familiar phenomenon that when 
several derivation steps form a chain, they 
are verbalized in a more connected way. To 
accommodate he phenomenon of a chain, we 
have also added a slot called next in the do- 
main model concept derive-chain. Now sup- 
pose that we have two consecutive deriva- 
tions with R1,M1,C1 and R2, M2, C2 as its 
premises (called reasons), the rule of infer- 
ence (called method), and the conclusion. 
They form part of a chain if the conclusion 
C1 is used as a premise in the second step, 
namely C1 E R2. In this case, the following 
rule combines them into a chain by putting 
the second derivation into the next slot of the 
chain. At the same time, C1 is removed from 
R2 since it is redundant. 
Ru le  C.1 Der ivat ion  Cha in  2 
derive(R1, M1, C1), derive(R2, IVI2, C2) 
derive-chain(R1, M1, C1, derive( R2 \ C1, M2, C2, )) 
The following example illustrates how this 
rule works. We will only give the verbaliza- 
tion and omit the Text Structure. Given a 
sequence of two derivation steps which can 
be verbalized as: 
"0 C_ o'*, by the definition of transitive 
closure." and 
"Since (x, y) E a and o C o*, (x, y) E c* 
by the definition of subset." 
Rule C.1 will produce a chain which will be 
verbalized as 
"a C 0" by the definition of transitive 
closure, thus establishing (x, y) E 0" by 
the definition of subset, since (x,y) E 
0." 
Note that the rule above is only a simpli- 
fication of a recursive definition, since chain- 
ing is not restricted to two derivation steps. 
2This is a simplified version of the original rule 
defined recursively in \[4\] 
28 
Readers are referred to \[4\]. Although this 
rule inserts the second derive into another 
Text Structure, the resulting structure is now 
a chain, no longer a plain derive. Therefore 
it distinguishes clearly f;om the rules in Sec- 
tion 6.2. 
There are two more chaining rules for 
the logical connectors implication and equiv- 
alence. A further rule removes redundancies 
in some case analyses (see \[4\]). 
6.4 Discuss ion 
While many systems have some. aggregation 
rules implemented \[9, 2\], there are compar- 
atively few detailed discussions in the liter- 
ature. The most structured categorization 
we found is the work of Dalianis and Hovy 
\[3\], where they define aggregation as a way 
of avoiding redundancy. Some of their rules, 
nevertheless, make decisions which we would 
call reference choice. Since this is treated in 
another module, we define our aggregation at 
the semantic level. The following are several 
significant features of our aggregation rules. 
The first difference is that our aggregation 
rules are defined in terms of manipulations 
of the Upper Model. They remove redun- 
dancies by combining the linguistic resources 
of two adjacent APOs, which contain redun- 
dant content. They cover the more syntactic 
rules reported in the literature at a more ab- 
stract level. 
Second, Text Structure provides us 
stronger means to specify textual operations. 
While rules reported in the literature typ- 
ically aggregate clauses, our rules operate 
both above and beneath the level of clause 
constituents. 
Third, while most investigations have con- 
centrated on general purpose microplanning 
operations, we came to the conclusion that 
microplanning needs domain-specific rules 
and patterns as well. 
7 A Running Example 
The following example illustrates the mecha- 
nism of aggregation and its effect on resulting 
text. We start with the following sequence of 
PMs: 
assume(Set(F)) 
assume(Set(G)) 
assume(Subset(F, G) )
assume(element(a, F) ) 
assume(element(b, F) ) 
derive( (element(a, F) A Subset(F, G) ), 
e, element(a, G) ) 
derive( (element(b, F) A Subset(F, G)), 
e, element(b, G) ) 
Without aggregation, the system produces: 
"Let F be a set. Let G be a set. Let F C G. 
Let a E F. Let b E F. Since a E F and 
F C G, a E G. Since b E F and F C G, 
b E G." 
Aggregation of the assume-PMs results in: 
assume(Set(F) A Set(G) A Subset(F, G) 
i element(a, F) i element(b, F)) 
whereas the application of the grouping rule 
for independent derive-PMs provides: 
derive( ( element( a,F) A element(b, F) 
A Subset(F, G)), e, 
(element(a, G) A element(b, G) ) ) 
After that, the predicate grouping rule A.1 
is applied to the arguments of assume, which 
are grouped to: 
( Set( F A G) A Subset(F, G) 
A element(a A B, F A F))) 
Note that F A F is later reduced to F. 
Predicate grouping applies to the arguments 
of derive in a similar way. Finally, the system 
produces the following output: 
"Let F and G be sets, F C G, and a, b E F. 
Then a, b E G." 
8 Conclusion 
We argued in this paper that sophisti- 
cated microplanning techniques are required 
even for mathematical proofs, in contrast 
to the belief that mathematical texts are 
only schematic and mechanical. We demon- 
strated why paraphrasing and aggregation 
will significantly enhance the flexibility and 
the coherence of text produced. In order to 
29 
carry out appropriate textual rearrangement 
we need a representation formalism which al- 
lows flexible but principled manipulation of 
linguistic resources. To this end, we basi- 
cally adopted the Text Structure of Meteer, 
but split her semantic ategories into two di- 
mensions following Panaget. The type check- 
ing mechanism ofText Structure allows us to 
achieve paraphrasing by building comparable 
combinations of linguistic resources. Speci- 
fied in terms of Upper Model concepts, our 
semantic aggregation rules are more abstract 
than similar rules reported in the literature. 
One important feature of our work is the 
integration of microplanning knowledge spe- 
cific to our domain of application. This 
body of knowledge must be refined to further 
improve the quality of the text produced. 
More experience is also required to formulate 
strategies to choose among alternatives. 
References  
\[1\] John Bateman, Bob Kasper, Johanna 
Moore, and Richard Whitney. The pen- 
man upper model. Technical Report ISI 
research report, USC/Information Sci- 
ence institute, 1990. 
\[2\] Robert Dale. Generating Referring Ex- 
pressions. ACL-MIT PressSeries in Nat- 
ural Language Processing. MIT Press, 
1992. 
\[3\] 
\[4\] 
Hercules Dalianis and Eduard Hovy. 
Aggregation in natural anguage gener- 
ation. In Proc. 4th European Workshop 
on Natural Language Generation, 1993. 
Armin Fiedler. Mikroplanungstechniken 
zur PrEsentation mathematischer Be- 
weise. Master's thesis, Fachbereich In- 
formatik, Universit?t des Saartandes, 
1996. 
\[5\] 
\[6\] 
M.A.K. Halliday. Introduction to func- 
tional grammar. Edward Arnold, 1985. 
Xiaorong Huang. Planning argumenta- 
tive texts. In Proc. of 15th International 
Conference on Computational Linguis- 
tics, 1994. 
\[7\] Xiaorong Huang. PROVERB: A sys- 
tem explaining machine-found proofs. 
In Proc. of 16th Annual Conference of 
the Cognitive Science Society, 1994. 
\[8\] Xiaorong Huang. Human Oriented 
Proof Presentation: A Reconstructive 
Approach. Infix, 1996. 
\[9\] Gerard Kempen. Conjunction reduc- 
tion and gapping in clause-level coordi- 
nation: an inheritance-based approach. 
Computational Intelligence, 7(4):357- 
360, 1991. 
\[10\] Anne Kilger and Wolfgang Finkler. In- 
cremental generation for real-time ap- 
plications. Research Report RR-95-11, 
DFKI, Saarbriicken, 1995. 
\[11\] Marie W. Meteer. Bridging the genera- 
tion gap between text planning linguis- 
tic realization. Computational Intelli- 
gence, 7(4), 1991. 
\[12\] Marie W. Meteer. Expressibility and the 
Problem of Efficient Text Planning. Pin- 
ter Publishes, London, 1992. 
\[13\] Johanna Doris Moore and CEcile L. 
Paris. Planning text for advisory dia- 
logues. In Proc. 27th Annual Meeting of 
the Association for Computational Lin- 
guistics, 1989. 
\[14\] FFranck Panaget. Using a textual repre- 
sentational level component in the con- 
text of discourse or dialogue generation. 
In Proc. of 7th International Workshop 
on Natural Language Generation, 1994. 
\[15\] Penelope Sibun. The local organiza- 
tion of text. In Proe. of the fifth in- 
ternational natural language generation 
workshop, 1990. 
\[16\] Ingrid Zukerman. Using meta-com- 
ments to generate fluent text in a techni- 
cal domain. Computational Intelligence, 
7:276-295, 1991. 
30 
