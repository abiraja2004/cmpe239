Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 32?39,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Annotating Dialogue Acts to Construct Dialogue Systems for Consulting
Kiyonori Ohtake Teruhisa Misu Chiori Hori Hideki Kashioka Satoshi Nakamura
MASTAR Project, National Institute of Information and Communications Technology
Hikaridai, Keihanna Science City, JAPAN
kiyonori.ohtake (at) nict.go.jp
Abstract
This paper introduces a new corpus of con-
sulting dialogues, which is designed for
training a dialogue manager that can han-
dle consulting dialogues through sponta-
neous interactions from the tagged dia-
logue corpus. We have collected 130 h
of consulting dialogues in the tourist guid-
ance domain. This paper outlines our tax-
onomy of dialogue act annotation that can
describe two aspects of an utterances: the
communicative function (speech act), and
the semantic content of the utterance. We
provide an overview of the Kyoto tour
guide dialogue corpus and a preliminary
analysis using the dialogue act tags.
1 Introduction
This paper introduces a new dialogue corpus for
consulting in the tourist guidance domain. The
corpus consists of speech, transcripts, speech act
tags, morphological analysis results, dependency
analysis results, and semantic content tags. In this
paper, we describe the current status of a dialogue
corpus that is being developed by our research
group, focusing on two types of tags: speech act
tags and semantic content tags. These speech act
and semantic content tags were designed to ex-
press the dialogue act of each utterance.
Many studies have focused on developing spo-
ken dialogue systems. Their typical task do-
mains included the retrieval of information from
databases or making reservations, such as airline
information e.g., DARPA Communicator (Walker
et al, 2001) and train information e.g., ARISE
(Bouwman et al, 1999) and MASK (Lamel et al,
2002). Most studies assumed a definite and con-
sistent user objective, and the dialogue strategy
was usually designed to minimize the cost of in-
formation access. Other target tasks include tutor-
ing and trouble-shooting dialogues (Boye, 2007).
In such tasks, dialogue scenarios or agendas are
usually described using a (dynamic) tree structure,
and the objective is to satisfy all requirements.
In this paper, we introduce our corpus, which is
being developed as part of a project to construct
consulting dialogue systems, that helps the user in
making a decision. So far, several projects have
been organized to construct speech corpora such
as CSJ (Maekawa et al, 2000) for Japanese. The
size of CSJ is very large, and a great part of the
corpus consists of monologues. Although, CSJ
includes some dialogues, the size of dialogues is
not enough to construct a dialogue system via re-
cent statistical techniques. In addition, relatively
to consulting dialogues, the existing large dialogue
corpora covered very clear tasks in limited do-
mains.
However, consulting is a frequently used and
very natural form of human interaction. We of-
ten consult with a sales clerk while shopping or
with staff at a concierge desk in a hotel. Such dia-
logues usually form part of a series of information
retrieval dialogues that have been investigated in
many previous studies. They also contains various
exchanges, such as clarifications and explanations.
The user may explain his/her preferences vaguely
by listing examples. The server would then sense
the user?s preferences from his/her utterances, pro-
vide some information, and then request a deci-
sion.
It is almost impossible to handcraft a scenario
that can handle such spontaneous consulting dia-
logues; thus, the dialogue strategy should be boot-
strapped from a dialogue corpus. If an extensive
dialogue corpus is available, we can model the
dialogue using machine learning techniques such
as partially observable Markov decision processes
(POMDPs) (Thomson et al, 2008). Hori et al
(2008) have also proposed an efficient approach to
organize a dialogue system using weighted finite-
state transducers (WFSTs); the system obtains the
32
Table 2: Overview of Kyoto tour guide dialogue
corpus
dialogue type F2F WOZ TEL
# of dialogues 114 80 62
# of guides 3 2 2
avg. # of utterance 365.4 165.2 324.5/ dialogue (guide)
avg. # of utterance 301.7 112.9 373.5/ dialogue (tourist)
structure of the transducers and the weight for
each state transitions from an annotated corpus.
Thus, the corpus must be sufficiently rich in in-
formation to describe the consulting dialogue to
construct the statistical dialogue manager via such
techniques.
In addition, a detailed description would be
preferable when developing modules that focus
on spoken language understanding and generation
modules. In this study, we adopt dialogue acts
(DAs) (Bunt, 2000; Shriberg et al, 2004; Banga-
lore et al, 2006; Rodriguez et al, 2007; Levin et
al., 2002) for this information and annotate DAs in
the corpus.
In this paper, we describe the design of the Ky-
oto tour guide dialogue corpus in Section 2. Our
design of the DA annotation is described in Sec-
tion 3. Sections 4 and 5 respectively describe two
types of the tag sets, namely, the speech act tag
and the semantic content tag.
2 Kyoto Tour Guide Dialogue Corpus
We are currently developing a dialogue corpus
based on tourist guidance for Kyoto City as the tar-
get domain. Thus far, we have collected itinerary
planning dialogues in Japanese, in which users
plan a one-day visit to Kyoto City. There are
three types of dialogues in the corpus: face-to-
face (F2F), Wizard of OZ (WOZ), and telephonic
(TEL) dialogues. The corpus consists of 114 face-
to-face dialogues, 80 dialogues using the WOZ
system, and 62 dialogues obtained from telephone
conversations with the interface of the WOZ sys-
tem.
The overview of these three types of dialogues
is shown in Table 2. Each dialogue lasts for almost
30 min. Most of all the dialogues have been man-
ually transcribed. Table 2 also shows the average
number of utterances per a dialogue.
Each face-to-face dialogue involved a profes-
sional tour guide and a tourist. Three guides, one
male and two females, were employed to collect
the dialogues. All three guides were involved in
almost the same number of dialogues. The guides
used maps, guidebooks, and a PC connected to the
internet.
In the WOZ dialogues, two female guides were
employed. Each of them was participated in 40
dialogues. The WOZ system consists of two in-
ternet browsers, speech synthesis program, and
an integration program for the collaborative work.
Collaboration was required because in addition to
the guide, operators were employed to operate the
WOZ system and support the guide. Each of the
guide and operators used own computer connected
each other, and they collaboratively operate the
WOZ system to serve a user (tourist).
In the telephone dialogues, two female guides
who are the same for the WOZ dialogues were
employed. In these dialogues, we used the WOZ
system, but we did not need the speech synthesis
program. The guide and a tourist shared the same
interface in different rooms, and they could talk to
each other through the hands-free headset.
Dialogues to plan a one-day visit consist of sev-
eral conversations for choosing places to visit. The
conversations usually included sequences of re-
quests from the users and provision of information
by the guides as well as consultation in the form of
explanation and evaluation. It should be noted that
in this study, enabling the user to access informa-
tion is not an objective in itself, unlike information
kiosk systems such as those developed in (Lamel
et al, 2002) or (Thomson et al, 2008). The objec-
tive is similar to the problem-solving dialogue of
the study by Ferguson and Allen (1998), in other
words, accessing information is just an aspect of
consulting dialogues.
An example of dialogue via face-to-face com-
munication is shown in Table 1. This dialogue is
a part of a consultation to decide on a sightseeing
spot to visit. The user asks about the location of a
spot, and the guide answers it. Then, the user pro-
vides a follow-up by evaluating the answer. The
task is challenging because there are many utter-
ances that affect the flow of the dialogue during a
consultation. The utterances are listed in the order
of their start times with the utterance ids (UID).
From the column ?Time? in the table, it is easy to
see that there are many overlaps.
33
Table 1: Example dialogue from the Kyoto tour guide dialogue corpus
UID Time (ms) Speaker Transcript Speech act tag** Semantic content tag
56 76669?78819 User
Ato (And,)
WH?Question Where
null
Ohara ga (Ohara is) (activity),location
dono henni (where) (activity),(demonstrative),interr
narimasuka (I?d like to know) (activity),predicate
57 80788?81358 Guide kono (here) State Answer?56 (demonstrative),kosoahendesune (is around) (demonstrative),noun
58 81358?81841 Guide Ohara ha (Ohara) State Inversion location
59 81386?82736 User Chotto (a bit) State Evaluation?57 (transp),(cost),(distance),adverb-phrasehanaresugitemasune (is too far) (transp),(cost),(distance),predicate
60 83116?83316 Guide A (Yeah,) Pause Grabber null
61 83136?85023 User
Kore demo (it)
Y/N?Question
null
ichinichi dewa (in a day) (activity),(planning),duration
doudeshou (Do you think I can do) (activity),(planning),(demonstrative),interr
62 83386?84396 Guide Soudesune (right.) State Acknowledgment?59 null
63 85206?87076 Guide
Ichinichi (One day)
State AffirmativeAnswer?61
(activity),(planning),(entity),day-window
areba (is) (activity),(planning),predicate
jubuN (enough) (consulting),(activity),adverb-phrase
ikemasu (to enjoy it.) (consulting),(activity),action
64 88392?90072 Guide
Oharamo (Ohara is)
State Opinion
(activity),location
sugoku (very) (recommendation),(activity),adverb-phrase
kireidesuyo (a beautiful spot) (recommendation),(activity),predicate
65 89889?90759 User Iidesune (that would be nice.) State Acknowledgment?64 (consulting),(activity),predicateEvaluation?64
* Tags are concatenated using a delimiter ? ? and omitting null values.
The number following the ??? symbol denotes the target utterance of the function.
3 Annotation of Communicative
Function and Semantic Content in DA
We annotate DAs in the corpus in order to de-
scribe a user?s intention and a system?s (or the tour
guide?s) action. Recently, several studies have ad-
dressed multilevel annotation of dialogues (Levin
et al, 2002; Bangalore et al, 2006; Rodriguez et
al., 2007); in our study, we focus on the two as-
pects of a DA indicated by Bunt (2000). One is the
communicative function that corresponds to how
the content should be used in order to update the
context, and the other is a semantic content that
corresponds to what the act is about. We consider
both of them important information to handle the
consulting dialogue. We designed two different
tag sets to annotate DAs in the corpus. The speech
act tag is used to capture the communicative func-
tions of an utterance using domain-independent
multiple function layers. The semantic content tag
is used to describe the semantic contents of an ut-
terance using domain-specific hierarchical seman-
tic classes.
4 Speech Act Tags
In this section, we introduce the speech act (SA)
tag set that describes communicative functions of
utterances. As the base units for tag annotation,
we adopt clauses that are detected by applying
the clause boundary annotation program (Kash-
ioka and Maruyama, 2004) to the transcript of the
dialogue. Thus, in the following discussions, ?ut-
terance? denotes a clause.
4.1 Tag Specifications
There are two major policies in SA annotation.
One is to select exactly one label from the tag set
(e.g., the AMI corpus1). The other is to annotate
with as many labels as required. MRDA (Shriberg
et al, 2004) and DIT++ (Bunt, 2000) are defined
on the basis of the second policy. We believe that
utterances are generally multifunctional and this
multifunctionality is an important aspect for man-
aging consulting dialogues through spontaneous
interactions. Therefore, we have adopted the latter
policy.
By extending the MRDA tag set and DIT++, we
defined our speech act tag set that consists of six
layers to describe six groups of function: Gen-
eral, Response, Check, Constrain, ActionDiscus-
sion, and Others. A list of the tag sets (excluding
the Others layer is shown in Table 3. The General
layer has two sublayers under the labels, Pause
and WH-Question, respectively. The two sublay-
ers are used to elaborate on the two labels, respec-
tively. A tag of the General layer must be labeled
to an utterance, but the other layer?s tags are op-
tional, in other words, layers other than the Gen-
eral layer can take null values when there is no tag
which is appropriate to the utterance. In the practi-
cal annotation, the most appropriate tag is selected
from each layer, without taking into account any
of the other layers.
The descriptions of the layers are as follows:
General: It is used to represent the basic form
1http://corpus.amiproject.org
34
Table 3: List of speech act tags and their occurrence in the experiment
Tag Percentage(%) Tag Percentage(%) Tag Percentage(%) Tag Percentage(%)User Guide User Guide User Guide User Guide
(General) (Response) (ActionDiscussion) (Constrain)
Statement 45.25 44.53 Acknowledgment 19.13 5.45 Opinion 0.52 2.12 Reason 0.64 2.52
Pause 12.99 15.05 Accept 4.68 6.25 Wish 1.23 0.05 Condition 0.61 3.09
Backchannel 26.05 9.09 PartialAccept 0.02 0.10 Request 0.22 0.19 Elaboration 0.28 4.00
Y/N-Question 3.61 2.19 AffirmativeAnswer 0.08 0.20 Suggestion 0.16 1.12 Evaluation 1.35 2.01
WH-Question 1.13 0.40 Reject 0.25 0.11 Commitment 1.15 0.29 (Check)
Open-Question 0.32 0.32 PartialReject 0.04 0.03 RepetitionRequest 0.07 0.03
OR?after-Y/N 0.05 0.02 NegativeAnswer 0.10 0.10 UnderstandingCheck 0.19 0.20
OR-Question 0.05 0.03 Answer 1.16 2.57 DoubleCheck 0.36 0.15
Statement== 9.91 27.79 ApprovalRequest 2.01 1.07
of the unit. Most of the tags in this layer
are used to describe forward-looking func-
tions. The tags are classified into three large
groups: ?Question,? ?Fragment,? and ?State-
ment.? ?Statement==? denotes the continua-
tion of the utterance.
Response: It is used to label responses directed
to a specific previous utterance made by the
addressee.
Check: It is used to label confirmations that are
along a certain expected response.
Constrain: It is used to label utterances that re-
strict or complement the target of the utter-
ance.
ActionDiscussion: It is used to label utterances
that pertain to a future action.
Others: It is used to describe various functions of
the utterance, e.g., Greeting, SelfTalk, Wel-
come, Apology, etc.
In the General layer, there are two sublayers:? (1)
the Pause sublayer that consists of Hold, Grabber,
Holder, and Releaser and (2) the WH sublayer that
labels the WH-Question type.
It should be noted that this taxonomy is in-
tended to be used for training spoken dialogue sys-
tems. Consequently, it contains detailed descrip-
tions to elaborate on the decision-making process.
For example, checks are classified into four cat-
egories because they should be treated in various
ways in a dialogue system. UnderstandingCheck
is often used to describe clarifications; thus, it
should be taken into account when creating a di-
alogue scenario. In contrast, RepetitionRequest,
which is used to request that the missed portions
of the previous utterance be repeated, is not con-
cerned with the overall dialogue flow.
An example of an annotation is shown in Table
1. Since the Response and Constrain layers are not
necessarily directed to the immediately preceding
utterance, the target utterance ID is specified.
4.2 Evaluation
We performed a preliminary annotation of the
speech act tags in the corpus. Thirty dialogues
(900 min, 23,169 utterances) were annotated by
three labellers. When annotating the dialogues, we
took into account textual information, audio infor-
mation, and contextual information The result was
cross-checked by another labeller.
4.2.1 Distributional Statistics
The frequencies of the tags, expressed as a per-
centages, are shown in Table 3. In the General
layer, nearly half of the utterances were Statement.
This bias is acceptable because 66% of the utter-
ances had tag(s) of other layers.
The percentages of tags in the Constrain layer
are relatively higher than those of tags in the other
layers. They are also higher than the percentages
of the corresponding tags of MRDA (Shriberg
et al, 2004) and SWBD-DAMSL(Jurafsky et al,
1997).
These statistics characterize the consulting dia-
logue of sightseeing planning, where explanations
and evaluations play an important role during the
decision process.
4.2.2 Reliability
We investigated the reliability of the annotation.
Another two dialogues (2,087 utterances) were an-
notated by three labelers and the agreement among
them was examined. These results are listed in Ta-
ble 4. The agreement ratio is the average of all the
combinations of the three individual agreements.
In the same way, we also computed the average
Kappa statistic, which is often used to measure the
agreement by considering the chance rate.
A high concordance rate was obtained for the
General layer. When the specific layers and sub-
layers are taken into account, Kappa statistic was
35
Table 4: Agreement among labellers
General layer All layers
Agreement ratio 86.7% 74.2%
Kappa statistic 0.74 0.68
0.68, which is considered a good result for this
type of task. (cf. (Shriberg et al, 2004) etc.)
4.2.3 Analysis of Occurrence Tendency
during Progress of Episode
We then investigated the tendencies of tag occur-
rence through a dialogue to clarify how consult-
ing is conducted in the corpus. We annotated the
boundaries of episodes that determined the spots
to visit in order to carefully investigate the struc-
ture of the decision-making processes. In our cor-
pus, users were asked to write down their itinerary
for a practical one day tour. Thus, the beginning
and ending of an episode can be determined on the
basis of this itinerary.
As a result, we found 192 episodes. We selected
122 episodes that had more than 50 utterances,
and analyzed the tendency of tag occurrence. The
episodes were divided into five segments so that
each segment had an equal number of utterances.
The tendency of tag occurrence is shown in Figure
1. The relative occurrence rate denotes the number
of times the tags appeared in each segment divided
by the total number of occurrences throughout the
dialogues. We found three patterns in the tendency
of occurrence. The tags corresponding to the first
pattern frequently appear in the early part of an
episode; this typically applies to Open-Question,
WH-Question, and Wish. The tags of the sec-
ond pattern frequently appear in the later part, this
typically applies to Evaluation, Commitment, and
Opinion. The tags of the third pattern appear uni-
formly over an episode, e.g., Y/N-Question, Ac-
cept, and Elaboration. These statistics characterize
the dialogue flow of sightseeing planning, where
the guide and the user first clarify the latter?s in-
terests (Open, WH-Questions), list and evaluate
candidates (Evaluation), and then the user makes
a decision (Commitment).
This progression indicates that a session (or di-
alogue phase) management is required within an
episode to manage the consulting dialogue, al-
though the test-set perplexity2 , which was calcu-
2The perplexity was calculated by 10-fold cross validation
of the 30 dialogues.







    
	






	



	



	

	
	

	


		
Figure 1: Progress of episodes vs. occurrence of
speech act tags
lated by a 3-gram language model trained with the
SA tags, was not high (4.25 using the general layer
and 14.75 using all layers).
5 Semantic Content Tags
The semantic content tag set was designed to cap-
ture the contents of an utterance. Some might con-
sider semantic representations by HPSG (Pollard
and Sag, 1994) or LFG (Dalrymple et al, 1994)
for an utterance. Such frameworks require knowl-
edge of grammar and experiences to describe the
meaning of an utterance. In addition, the utter-
ances in a dialogue are often fragmentary, which
makes the description more difficult.
We focused on the predicate-argument structure
that is based on dependency relations. Annotating
dependency relations is more intuitive and is easier
than annotating the syntax structure; moreover, a
dependency parser is more robust for fragmentary
expressions than syntax parsers.
We introduced semantic classes to represent the
semantic contents of an utterance. Semantic class
labels are applied to each unit of the predicate-
argument structure. The task that identifies the
semantic classes is very similar to named entity
recognition, because the classes of the named en-
tities can be equated to the semantic classes that
are used to express semantic content. However,
both nouns and predicates are very important for
capturing the semantic contents of an utterance.
For example, ?10 a.m.? might denote the current
time in the context of planning, or it might signify
the opening time of a sightseeing spot. Thus, we
represent the semantic contents on the basis of the
predicate-argument structure. Each predicate and
argument is assigned a semantic category.
For example, the sentence ?I would like to see
36
I would like to see Kinkakuji temple
would like to see I Kinkakuji temple( )
predicate arguments
automatically analyzed
manually annotated
would like to see I Kinkakuji temple( )
preference.action preference.spot.name
given sentence
predicate argument structure
annotation result
Figure 2: Example of annotation with semantic
content tags
(preference) (reco m m end a t i o n) (d eci s i o n) (co ns u l t i ng )
(s po t ) (a ct i v i t y )(res t a u ra nt ) a ct i o n pred i ca t e
(co s t ) (s ch ed u l e) na m e t y pe
(m o ney )
o b j ectent i t y pred i ca t e
(d i s t a nce)
(v i ew )
a ct i o n
na t u rea rch i t ect u re
??.
Figure 3: A part of the semantic category hierar-
chy
Kinkakuji temple.? is annotated as shown in Fig-
ure 2. In this figure, the semantic content tag
preference.action indicates that the predicate por-
tion expresses the speaker?s preference for the
speaker?s action, while the semantic content tag
preference.spot.name indicates the name of the
spot as the object of the speaker?s preference.
Although we do not define semantic the role
(e.g., object (Kinakuji temple) and subject (I))
of each argument item in this case, we can use
conventional semantic role labeling techniques
(Gildea and Jurafsky, 2002) to estimate them.
Therefore, we do not annotate such semantic role
labels in the corpus.
5.1 Tag Specifications
We defined hierarchical semantic classes to anno-
tate the semantic content tags. There are 33 la-
bels (classes) at the top hierarchical level. The la-
bels are, for example, activity, event, meal, spot,
transportation, cost, consulting, and location, as
shown in Figure 3. There two kinds of labels,
nodes and leaves. A node must have at least one
child, a node or a leaf. A leaf has no children. The
number of kinds for nodes is 47, and the number
of kinds for leaves is 47. The labels of leaves are
very similar to the labels for named entity recog-
nition. For example, there are ?year, date, time,
organizer, name, and so on.? in the labels of the
leaves.
One of the characteristics of the semantic struc-
ture is that the lower level structures are shared by
many upper nodes. Thus, the lower level structure
can be used in any other domains or target tasks.
5.2 Annotation of semantic contents tags
The annotation of semantic contents tags is per-
formed by the following four steps. First, an ut-
terance is analyzed by a morphological analyzer,
ChaSen3. Second, the morphemes are chunked
into dependency unit (bunsetsu). Third, depen-
dency analysis is performed using a Japanese de-
pendency parser, CaboCha4. Finally, we annotate
the semantic content tags for each bunsetsu unit by
using our annotation tool. An example of an an-
notation is shown in Table 1. Each row in column
?Transcript? denotes the divided bunsetsu units.
The annotation tool interface is shown in Figure
4. In the left side of this figure, the dialogue files
and each utterance of the dialogue information are
displayed. The dependency structure of an utter-
ance is displayed in the upper part of the figure.
The morphological analysis results and chunk in-
formation are displayed in the lower part of the
figure.
At present, the annotations of semantic con-
tent tags are being carried out for 10 dialogues.
Approximately 22,000 paths, including paths that
will not be used, exist if the layered structure is
fully expanded. In the 10 dialogues, 1,380 tags (or
paths) are used.
In addition, not only to annotate semantic con-
tent tags, but to correct the morphological analyze
results and dependency analyzed results are being
carried out. If we complete the annotation, we will
also obtain these correctly tagged data of Kyoto
tour guide corpus. These corpora can be used to
develop analyzers such as morphological analyz-
3http://sourceforge.jp/projects/chasen-legacy/
4http://chasen.org/?taku/software/cabocha/
37
Figure 4: Annotation tool interface for annotating semantic content tags
ers and dependency analyzers via machine learn-
ing techniques or to adapt analyzers for this do-
main.
6 Conclusion
In this paper, we have introduced our spoken di-
alogue corpus for developing consulting dialogue
systems. We designed a dialogue act annotation
scheme that describes two aspects of a DA: speech
act and semantic content. The speech act tag set
was designed by extending the MRDA tag set.
The design of the semantic content tag set is al-
most complete. If we complete the annotation, we
will obtain speech act tags and semantic content
tags, as well as manual transcripts, morphologi-
cal analysis results, dependency analysis results,
and dialogue episodes. As a preliminary analysis,
we have evaluated the SA tag set in terms of the
agreement between labellers and investigated the
patterns of tag occurrences.
In the next step, we will construct automatic
taggers for speech act and semantic content tags
by using the annotated corpora and machine learn-
ing techniques. Our future work also includes a
condensation or selection of dialogue acts that di-
rectly affect the dialogue flow in order to construct
a consulting dialogue system using the DA tags as
an input.
References
Srinivas Bangalore, Giuseppe Di Fabbrizio, and
Amanda Stent. 2006. Learning the structure of
task-driven human-human dialogs. In Proceedings
of COLING/ACL, pages 201?208.
Gies Bouwman, Janienke Sturm, and Louis Boves.
1999. Incorporating Confidence Measures in the
Dutch Train Timetable Information System Devel-
oped in the ARISE Project. In Proc. ICASSP.
Johan Boye. 2007. Dialogue Management for Auto-
matic Troubleshooting and Other Problem-solving
Applications. In Proc. of 8th SIGdial Workshop on
Discourse and Dialogue, pages 247?255.
Harry Bunt. 2000. Dialogue pragmatics and context
specification. In Harry Bunt and William Black,
editors, Abduction, Belief and Context in Dialogue,
pages 81?150. John Benjamins.
Mary Dalrymple, Ronald M. Kaplan, John T. Maxwell
III, and Anni e Zaenen, editors. 1994. Formal Is-
sues in Lexical-Functional Grammar. CSLI Publi-
cations.
George Ferguson and James F. Allen. 1998. TRIPS:
An intelligent integrated problem-solving assistant.
In Proc. Fifteenth National Conference on Artificial
Intelligence, pages 567?573.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Chiori Hori, Kiyonori Ohtake, Teruhisa Misu, Hideki
Kashioka, and Satoshi Nakamura. 2008. Dialog
Management using Weighted Finite-state Transduc-
ers. In Proc. Interspeech, pages 211?214.
38
Daniel Jurafsky, Elizabeth Shriberg, and Debra Bi-
asca. 1997. Switchboard SWBD-DAMSL shallow-
discourse-function annotation coders manual, draft
13. Technical report, University of Colorado at
Boulder & SRI International.
Hideki Kashioka and Takehiko Maruyama. 2004. Seg-
mentation of Semantic Unit in Japanese Monologue.
In Proc. ICSLT-O-COCOSDA.
Lori F. Lamel, Samir Bennacef, Jean-Luc Gauvain,
H. Dartigues, and J. N. Temem. 2002. User eval-
uation of the MASK kiosk. Speech Communication,
38(1):131?139.
Lori Levin, Donna Gates, Dorcas Wallace, Kay Peter-
son, Along Lavie, Fabio Pianesi, Emanuele Pianta,
Roldano Cattoni, and Nadia Mana. 2002. Balancing
expressiveness and simplicity in an interlingua for
task based dialogue. In Proceedings of ACL 2002
workshop on Speech-to-speech Translation: Algo-
rithms and Systems.
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-
toshi Isahara. 2000. Spontaneous speech corpus
of Japanese. In Proceedings of the Second Interna-
tional Conference of Language Resources and Eval-
uation (LREC2000), pages 947?952.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. The University of
Chicago Press.
Kepa Joseba Rodriguez, Stefanie Dipper, Michael
Go?tze, Massimo Poesio, Giuseppe Riccardi, Chris-
tian Raymond, and Joanna Rabiega-Wisniewska.
2007. Standoff Coordination for Multi-Tool Anno-
tation in a Dialogue Corpus. In Proc. Linguistic An-
notation Workshop, pages 148?155.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The ICSI Meet-
ing Recorder Dialog Act (MRDA) Corpus. In Proc.
5th SIGdial Workshop on Discourse and Dialogue,
pages 97?100.
Blaise Thomson, Jost Schatzmann, and Steve Young.
2008. Bayesian update of dialogue state for robust
dialogue systems. In Proceedings of ICASSP ?08.
Marilyn A. Walker, Rebecca Passonneau, and Julie E.
Boland. 2001. Quantitative and Qualitative Eval-
uation of DARPA Communicator Spoken Dialogue
Systems. In Proc. of 39th Annual Meeting of the
ACL, pages 515?522.
39
