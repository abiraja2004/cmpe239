Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 159?167,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Animacy Annotation in the Hindi Treebank
Itisree Jena, Riyaz Ahmad Bhat, Sambhav Jain and Dipti Misra Sharma
Language Technologies Research Centre, IIIT-Hyderabad, India
{itisree|riyaz.bhat|sambhav.jain}@research.iiit.ac.in, dipti@iiit.ac.in
Abstract
In this paper, we discuss our efforts to anno-
tate nominals in the Hindi Treebank with the
semantic property of animacy. Although the
treebank already encodes lexical information
at a number of levels such as morph and part
of speech, the addition of animacy informa-
tion seems promising given its relevance to
varied linguistic phenomena. The suggestion
is based on the theoretical and computational
analysis of the property of animacy in the con-
text of anaphora resolution, syntactic parsing,
verb classification and argument differentia-
tion.
1 Introduction
Animacy can either be viewed as a biological prop-
erty or a grammatical category of nouns. In a
strictly biological sense, all living entities are ani-
mate, while all other entities are seen as inanimate.
However, in its linguistic sense, the term is syn-
onymous with a referent?s ability to act or instigate
events volitionally (Kittila? et al, 2011). Although
seemingly different, linguistic animacy can be im-
plied from biological animacy. In linguistics, the
manifestation of animacy and its relevance to lin-
guistic phenomena have been studied quite exten-
sively. Animacy has been shown, cross linguisti-
cally, to control a number of linguistic phenomena.
Case marking, argument realization, topicality or
discourse salience are some phenomena, highly cor-
related with the property of animacy (Aissen, 2003).
In linguistic theory, however, animacy is not seen
as a dichotomous variable, rather a range capturing
finer distinctions of linguistic relevance. Animacy
hierarchy proposed in Silverstein?s influential arti-
cle on ?animacy hierarchy? (Silverstein, 1986) ranks
nominals on a scale of the following gradience: 1st
pers> 2nd pers> 3rd anim> 3rd inanim. Several such
hierarchies of animacy have been proposed follow-
ing (Silverstein, 1986), one basic scale taken from
(Aissen, 2003) makes a three-way distinction as hu-
mans > animates > inanimates. These hierarchies can
be said to be based on the likelihood of a referent of
a nominal to act as an agent in an event (Kittila? et
al., 2011). Thus higher a nominal on these hierar-
chies higher the degree of agency/control it has over
an action. In morphologically rich languages, the
degree of control/agency is expressed by case mark-
ing. Case markers capture the degree of control a
nominal has in a given context (Hopper and Thomp-
son, 1980; Butt, 2006). They rank nominals on the
continuum of control as shown in (1)1. Nominals
marked with Ergative case have highest control and
the ones marked with Locative have lowest.
Erg > Gen > Inst > Dat > Acc > Loc (1)
Of late the systematic correspondences between
animacy and linguistic phenomena have been ex-
plored for various NLP applications. It has been
noted that animacy provides important informa-
tion, to mention a few, for anaphora resolution
(Evans and Orasan, 2000), argument disambiguation
(Dell?Orletta et al, 2005), syntactic parsing (?vre-
lid and Nivre, 2007; Bharati et al, 2008; Ambati et
al., 2009) and verb classification (Merlo and Steven-
1Ergative, Genitive, Instrumental, Dative, Accusative and
Locative in the given order.
159
son, 2001). Despite the fact that animacy could play
an important role in NLP applications, its annota-
tion, however, is not usually featured in a treebank
or any other annotated corpora used for developing
these applications. There are a very few annotation
projects that have included animacy in their anno-
tation manual, following its strong theoretical and
computational implications. One such work, mo-
tivated by the theoretical significance of the prop-
erty of animacy, is (Zaenen et al, 2004). They
make use of a coding scheme drafted for a para-
phrase project (Bresnan et al, 2002) and present
an explicit annotation scheme for animacy in En-
glish. The annotation scheme assumes a three-way
distinction, distinguishing Human, Other animates
and Inanimates. Among the latter two categories
?Other animates? is further sub-categorized into
Organizations and Animals, while the category of
?Inanimates? further distinguishes between con-
crete and non-concrete, and time and place nomi-
nals. As per the annotation scheme, nominals are
annotated according to the animacy of their referents
in a given context. Another annotation work that
includes animacy for nominals is (Teleman, 1974),
however, the distinction made is binary between hu-
man and non-human referents of a nominal in a
given context. In a recent work on animacy annota-
tion, Thuilier et al (2012) have annotated a multi-
source French corpora with animacy and verb se-
mantics, on the lines of (Zaenen et al, 2004). Apart
from the manual annotation for animacy, lexical re-
sources like wordnets are an important source of this
information, if available. These resources usually
cover animacy, though indirectly (Fellbaum, 2010;
Narayan et al, 2002). Although a wordnet is an
easily accessible resource for animacy information,
there are some limitations on its use, as discussed
below:
1. Coverage: Hindi wordnet only treats common
nouns while proper nouns are excluded (except
famous names) see Table 1. The problem is se-
vere where the domain of text includes more
proper than common nouns, which is the case
with the Hindi Treebank as it is annotated on
newspaper articles.
2. Ambiguity: Since words can be ambiguous, the
animacy listed in wordnet can only be used in
presence of a high performance word sense dis-
ambiguation system. As shown in Table 2, only
38.02% of nouns have a single sense as listed in
Hindi Wordnet.
3. Metonymy or Complex Types: Domains like
newspaper articles are filled with metonymic
expressions like courts, institute names, coun-
try names etc, that can refer to a building, a ge-
ographical place or a group of people depend-
ing on the context of use. These words are not
ambiguous per se but show different aspects of
their semantics in different contexts (logically
polysemous). Hindi wordnet treats these types
of nouns as inanimate.
Nominals in HTB Hindi WordNet Coverage
78,136 65,064 83.27%
Table 1: Coverage of Hindi WordNet on HTB Nominals.
HTB Nominals Single Unique Sense
with WN Semantics in Hindi WordNet
65,064 24,741 (38.02%)
Table 2: Nominals in HTB with multiple senses
Given these drawbacks, we have included ani-
macy information manually in the annotation of the
Hindi Treebank, as discussed in this work. In the
rest, we will discuss the annotation of nominal ex-
pressions with animacy and the motivation for the
same, the discussion will follow as: Section 2 gives
a brief overview of the Hindi Treebank with all its
layers. Section 3 motivates the annotation of nom-
inals with animacy, followed by the annotation ef-
forts and issues encountered in Section 4. Section
5 concludes the paper with a discussion on possible
future directions.
2 Description of the Hindi Treebank
In the following, we give an overview of the Hindi
Treebank (HTB), focusing mainly on its dependency
layer. The Hindi-Urdu Treebank (Palmer et al,
2009; Bhatt et al, 2009) is a multi-layered and
multi-representational treebank. It includes three
levels of annotation, namely two syntactic levels and
one lexical-semantic level. One syntactic level is a
dependency layer which follows the CPG (Begum
160
et al, 2008), inspired by the Pa?n. inian grammati-
cal theory of Sanskrit. The other level is annotated
with phrase structure inspired by the Chomskyan ap-
proach to syntax (Chomsky, 1981) and follows a bi-
nary branching representation. The third layer of an-
notation, a purely lexical semantic one, encodes the
semantic relations following the English PropBank
(Palmer et al, 2005).
In the dependency annotation, relations are
mainly verb-centric. The relation that holds between
a verb and its arguments is called a kar.aka relation.
Besides kar.aka relations, dependency relations also
exist between nouns (genitives), between nouns and
their modifiers (adjectival modification, relativiza-
tion), between verbs and their modifiers (adver-
bial modification including subordination). CPG
provides an essentially syntactico-semantic depen-
dency annotation, incorporating kar.aka (e.g., agent,
theme, etc.), non-kar.aka (e.g. possession, purpose)
and other (part of) relations. A complete tag set of
dependency relations based on CPG can be found in
(Bharati et al, 2009), the ones starting with ?k? are
largely Pa?n. inian kar.aka relations, and are assigned
to the arguments of a verb. Figure 1 encodes the de-
pendency structure of (5), the preterminal node is a
part of speech of a lexical item (e.g. NN,VM, PSP).
The lexical items with their part of speech tags are
further grouped into constituents called chunks (e.g.
NP, VGF) as part of the sentence analysis. The de-
pendencies are attached at the chunk level, marked
with ?drel? in the SSF format. k1 is the agent of
an action (KAyA ?eat?), whereas k2 is the object or
patient.
(5) s\@yA n
Sandhya-Erg
sb
apple-Nom
KAyA
eat-Perf
?
?Sandhya ate an apple.?
<Sentence id = ?1?>
Offset Token Tag Feature structure
1 (( NP <fs name=?NP? drel=?k1:VGF?>
1.1 s\@yA NNP<fs af=?s\@yA,n,f,sg,3,o,0,0?>
1.2 n PSP <fs af=?n,psp,,,,,,?>
))
2 (( NP <fs name=?NP2? drel=?k2:VGF?>
2.1 sb NN <fs af=?sb,n,m,sg,3,d,0,0?>
))
3 (( VGF<fs name=?VGF?>
3.1 KAyA VM <fs af=?KA,v,m,sg,any,,yA,yA?>
))
</Sentence>
Figure 1: Annotation of an Example Sentence in SSF.
Despite the fact that the Hindi Treebank already
features a number of layers as discussed above, there
have been different proposals to enrich it further.
Hautli et al (2012) proposed an additional layer to
the treebank, for the deep analysis of the language,
by incorporating the functional structure (or
f-structure) of Lexical Functional Grammar which
encodes traditional syntactic notions such as sub-
ject, object, complement and adjunct. Dakwale et
al. (2012) have also extended the treebank with
anaphoric relations, with a motive to develop a data
driven anaphora resolution system for Hindi. Given
this scenario, our effort is to enrich the treebank
with the animacy annotation. In the following
sections, we will discuss in detail, the annotation of
the animacy property of nominals in the treebank
and the motive for the same.
3 Motivation: In the Context of
Dependency Parsing
Hindi is a morphologically rich language, gram-
matical relations are depicted by its morphology
via case clitics. Hindi has a morphologically
split-ergative case marking system (Mahajan, 1990;
Dixon, 1994). Case marking is dependent on the
aspect of a verb (progressive/perfective), transitivity
(transitive/intransitive) and the type of a nominal
(definite/indefinite, animate/inanimate). Given
this peculiar behavior of case marking in Hindi,
arguments of a verb (e.g. transitive) have a number
of possible configurations with respect to the case
marking as shown in the statistics drawn from
the Hindi Treebank released for MTPIL Hindi
Dependency parsing shared task (Sharma et al,
2012) in Table 3. Almost in 15% of the transitive
clauses, there is no morphological case marker on
any of the arguments of a verb which, in the context
of data driven parsing, means lack of an explicit
cue for machine learning. Although, in other cases
there is a case marker, at least on one argument of a
verb, the ambiguity in case markers (one-to-many
mapping between case markers and grammatical
functions as presented in Table 4) further worsens
the situation (however, see Ambati et al (2010) and
Bhat et al (2012) for the impact of case markers on
parsing Hindi/Urdu). Consider the examples from
161
(6a-e), the instrumental se is extremely ambiguous.
It can mark the instrumental adjuncts as in (6a),
source expressions as in (6b), material as in (6c),
comitatives as in (6d), and causes as in (6e).
K2-Unmarked K2-Marked
K1-Unmarked 1276 741
K1-Marked 5373 966
Table 3: Co-occurrence of Marked and Unmarked verb argu-
ments (core) in HTB.
n/ne ko/ko s/se m\/meN pr/par kA/kaa
(Ergative) (Dative) (Instrumental) (Locative) (Locative) (Genitive)
k1(agent) 7222 575 21 11 3 612
k2(patient) 0 3448 451 8 24 39
k3(instrument) 0 0 347 0 0 1
k4(recipient) 0 1851 351 0 1 4
k4a(experiencer) 0 420 8 0 0 2
k5(source) 0 2 1176 12 1 0
k7(location) 0 1140 308 8707 3116 19
r6(possession) 0 3 1 0 0 2251
Table 4 : Distribution of case markers across case function.
(6a) mohn n
Mohan-Erg
cAbF s
key-Inst
taAlA
lock-Nom
KolA
open
?
?Mohan opened the lock with a key.?
(6b) gFtaA n
Geeta-Erg
Ed?F s
Delhi-Inst
sAmAn
luggage-Nom
m\gvAyA
procure
?
?Geeta procured the luggage from Delhi.?
(6c) m EtakAr n
sculptor-Erg
p(Tr s
stone-Inst
m Eta
idol-Nom
bnAyF
make
?
?The sculptor made an idol out of stone.?
(6d) rAm kF
Ram-Gen
[yAm s
Shyaam-Inst
bAta
talk-Nom
h  I
happen
?
?Ram spoke to Shyaam.?
(6e) bAErf s
rain-Inst
kI Psl\
many crops-Nom
tabAh
destroy
ho gyF\
happen-Perf
?
?Many crops were destroyed due to the rain.?
(7) EcEwyA
bird-Nom
dAnA
grain-Nom
c  g rhF h{
devour-Prog
?
?A bird is devouring grain.?
A conventional parser has no cue for the disam-
biguation of instrumental case marker se in exam-
ples (6a-e) and similarly, in example (7), it?s hard
for the parser to know whether ?bird? or ?grain? is
the agent of the action ?devour?. Traditionally, syn-
tactic parsing has largely been limited to the use
of only a few lexical features. Features like POS-
tags are way too coarser to provide deep informa-
tion valuable for syntactic parsing while on the other
hand lexical items often suffer from lexical ambi-
guity or out of vocabulary problem. So in oder to
assist the parser for better judgments, we need to
complement the morphology somehow. A careful
observation easily states that a simple world knowl-
edge about the nature (e.g. living-nonliving, arti-
fact, place) of the participants is enough to disam-
biguate. For Swedish, ?vrelid and Nivre (2007) and
?vrelid (2009) have shown improvement, with an-
imacy information, in differentiation of core argu-
ments of a verb in dependency parsing. Similarly
for Hindi, Bharati et al (2008) and Ambati et al
(2009) have shown that even when the training data
is small simple animacy information can boost de-
pendency parsing accuracies, particularly handling
the differentiation of core arguments. In Table 5,
we show the distribution of animacy with respect to
case markers and dependency relations in the anno-
tated portion of the Hindi Treebank. The high rate
of co-occurrence between animacy and dependency
relations makes a clear statement about the role an-
imacy can play in parsing. Nominals marked with
dependency relations as k1 ?agent?, k4 ?recipient?,
k4a ?experiencer? are largely annotated as human
while k3 ?instrument? is marked as inanimate,
which confirms our conjecture that with animacy
information a parser can reliably predict linguistic
patterns. Apart from parsing, animacy has been re-
ported to be beneficial for a number of natural lan-
guage applications (Evans and Orasan, 2000; Merlo
and Stevenson, 2001). Following these computa-
tional implications of animacy, we started encoded
this property of nominals explicitly in our treebank.
In the next section, we will present these efforts fol-
162
lowed by the inter-annotator agreement studies.
Human Other-Animates Inanimate
k1
n/ne (Erg) 2321 630 108
ko/ko (Dat/Acc) 172 8 135
s/se (Inst) 6 0 14
m\/me (Loc) 0 0 7
pr/par (Loc) 0 0 1
kA/kaa (Gen) 135 2 99
? (Nom) 1052 5 3072
k2
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 625 200 226
s/se (Inst) 67 0 88
m\/me (Loc) 2 0 6
pr/par (Loc) 5 0 37
kA/kaa (Gen) 15 0 14
? (Nom) 107 61 2998
k3
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 0 0 0
s/se (Inst) 2 0 199
m\/me (Loc) 0 0 0
pr/par (Loc) 0 0 0
kA/kaa (Gen) 0 0 0
? (Nom) 0 0 20
k4
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 597 0 13
s/se (Inst) 53 0 56
m\/me (Loc) 0 0 0
pr/par (Loc) 0 0 0
kA/kaa (Gen) 0 0 0
? (Nom) 7 0 8
k4a
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 132 0 8
s/se (Inst) 4 0 2
m\/me (Loc) 0 0 0
pr/par (Loc) 0 0 0
kA/kaa (Gen) 1 0 0
? (Nom) 56 0 1
k5
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 0 0 0
s/se (Inst) 7 0 460
m\/me (Loc) 0 0 1
pr/par (Loc) 0 0 0
kA/kaa (Gen) 0 0 0
? (Nom) 0 0 2
k7
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 4 0 0
s/se (Inst) 3 0 129
m\/me (Loc) 0 1977 1563
pr/par (Loc) 66 0 1083
kA/kaa (Gen) 0 0 8
? (Nom) 5 0 1775
r6
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 0 0 0
s/se (Inst) 1 0 0
m\/me (Loc) 0 0 0
pr/par (Loc) 0 0 0
kA/kaa (Gen) 156 80 605
? (Nom) 13 3 25
Table 5: Distribution of semantic features with respect
to case markers and dependency relations a.
ak1 ?agent?, k2 ?patient?, k3 ?instrument?, k4 ?recipient?,
k4a ?experiencer?, k5 ?source?, k7 ?location?, r6 ?possession?
4 Animacy Annotation
Following Zaenen et al (2004), we make a three-
way distinction, distinguishing between Human,
Other Animate and In-animate referents of a
nominal in a given context. The animacy of a ref-
erent is decided based on its sentience and/or con-
trol/volitionality in a particular context. Since, pro-
totypically, agents tend to be animate and patients
tend to be inanimate (Comrie, 1989), higher ani-
mates such as humans, dogs etc. are annotated as
such in all contexts since they frequently tend to be
seen in contexts of high control. However, lower
animates such as insects, plants etc. are anno-
tated as ?In-animate? because they are ascribed
less or no control in human languages like inan-
imates (Kittila? et al, 2011). Non-sentient refer-
ents, except intelligent machines and vehicles, are
annotated as ?In-animate? in all contexts. Intel-
ligent machines like robots and vehicles, although,
lack any sentience, they possess an animal like be-
havior which separates them from inanimate nouns
with no animal resemblance, reflected in human lan-
guage as control/volitionality. These nouns unlike
humans and other higher animates are annotated as
per the context they are used in. They are anno-
tated as ?Other animate? only in their agentive
roles. Nominals that vary in sentience in varying
contexts are annotated based on their reference in a
given context as discussed in Subsection 4.2. These
nominals include country names referring to geo-
graphical places, teams playing for the country, gov-
ernments or their inhabitants; and organizations in-
cluding courts, colleges, schools, banks etc. Un-
like Zaenen et al (2004) we don?t further categorize
?Other Animate? and ?In-animate? classes. We
163
don?t distinguish between Organizations and Ani-
mals in ?Other Animate? and Time and Place in
?In-animates?.
The process of animacy annotation in the Hindi
Treebank is straight forward. For every chunk in a
sentence, the animacy of its head word is captured
in an ?attribute-value? pair in SSF format, as
shown in Figure 3. Hitherto, around 6485 sentence,
of the Hindi Treebank, have been annotated with
the animacy information.
<Sentence id = ?1?>
Offset Token Tag Feature structure
1 (( NP <fs name=?NP? drel=?k1:VGF?
semprop=?human?>
1.1 mohn NNP <fs af=?mohn,n,m,sg,3,d,0,0?>
1.2 n PSP <fs af=?n,psp,,,,,,? name=?n?>
))
2 (( NP <fs name=?NP2? drel=?k4:VGF?
semprop=?other-animate?>
2.1 Eb?F NN <fs af=?Eb?F,n,f,sg,3,d,0,0?>
2.2 ko PSP <fs af=?ko,psp,,,,,,? name=?ko?>
))
3 (( NP <fs name=?NP3? drel=?k3:VGF?
semprop=?inanimate?>
3.1 botal NN <fs af=?botal ,n,f,sg,3,d,0,0?>
3.2 s PSP <fs af=?s,psp,,,,,,?>
))
4 (( NP <fs name=?NP4? drel=?k2:VGF?
semprop=?inanimate?>
4.1 d D NN <fs af=?d D,n,m,sg,3,d,0,0?>
))
5 (( VGF <fs name=?VGF?>
5.1 EplAyA VM <fs af=?EplA,v,m,sg,any,,yA,yA?>
))
</Sentence>
Figure 3: Semantic Annotation in SSF.
(8) mohn n
Mohan-Erg
Eb?F ko
cat-Dat
botal s
bottle-Inst
d D
milk-Nom
EplAyA
drink-Perf
?
?Mohan fed milk to the cat with a bottle.?
In the following, we discuss some of the interest-
ing cross linguistic phenomena which added some
challenge to the annotation.
4.1 Personification
Personification is a type of meaning extension
whereby an entity (usually non-human) is given
human qualities. Personified expressions are an-
notated, in our annotation procedure, as Human,
since it is the sense they carry in such contexts.
However, to retain their literal sense, two attributes
are added. One for their context bound sense
(metaphorical) and the other for context free sense
(literal). In example (9), waves is annotated with
literal animacy as In-animante and metaphoric
animacy as Human, as shown in Figure 4 (offset
2).
<Sentence id = ?1?>
Offset Token Tag Feature structure
1 (( NP <fs name=?NP? drel=?k7p:VGF? >
1.1 sAgr NNC <fs af=?sAgr,n,m,sg,3,d,0,0?>
1.2 taV NN <fs af=?taV,n,m,sg,3,d,0,0?>
1.3 pr PSP <fs af=?pr,psp,,,,,,?>
))
2 (( NP <fs name=?NP2? drel=?k1:VGF?
semprop=?inanimate?
metaphoric=?human?>
2.1 lhr\ NN <fs af=?lhr\,n,f,pl,3,d,0,0?>
))
3 (( VGF <fs name=?VGF?>
3.1 nAc VM <fs af=?nAc,v,any,any,any,,0,0?>
3.2 rhF VAUX <fs af=?rhF,v,f,sg,any,,ya,ya?>
3.3 h{\ AUX <sf AF=?h{\,v,any,pl,1,,he,he?>
))
</Sentence>
Figure 4: Semantic Annotation in SSF.
(9) sAgr taV pr
sea coast-Loc
lhr\
waves-Nom
nAc rhF h{\
dance-Prog
?
?Waves are dancing on the sea shore.?
4.2 Complex Types
The Hindi Treebank in largely built on newspa-
per corpus. Logically polysemous expressions
(metonymies) such as government, court,
newspaper etc. are very frequent in news re-
porting. These polysemous nominals can exhibit
contradictory semantics in different contexts. In
example (10a), court refers to a person (judge) or
a group of persons (jury) while in (10b) it is a
building (see Pustejovsky (1996) for the semantics
of complex types). In our annotation procedure,
such expressions are annotated as per the sense or
reference they carry in a given context. So, in case
of (10a) court will be annotated as Human while
in (10b) it will be annotated as In-animante.
(10a) adAlta n
court-Erg
m  kdm kA
case-Gen
P{\slA
decision-Nom
s  nAyA
declare-Perf
?
?The court declared its decision on the case.?
164
(10b) m{\
I-Nom
adAlta m\
court-Loc
h ?
be-Prs
?I am in the court.?
4.3 Inter-Annotator Agreement
We measured the inter-annotator agreement on a
set of 358 nominals (?50 sentences) using Cohen?s
kappa. We had three annotators annotating the same
data set separately. The nominals were annotated
in context i.e., the annotation was carried consider-
ing the role and reference of a nominal in a partic-
ular sentence. The kappa statistics, as presented in
Table 6, show a significant understanding of anno-
tators of the property of animacy. In Table 7, we
report the confusion between the annotators on the
three animacy categories. The confusion is high for
?Inanimate? class. Annotators don?t agree on this
category because of its fuzziness. As discussed ear-
lier, although ?Inanimate? class enlists biologically
inanimate entities, some entities may behave like an-
imates in some contexts. They may be sentient and
have high linguistic control in some contexts. The
difficulty in deciphering the exact nature of the ref-
erence of these nominals, as observed, is the reason
behind the confusion. The confusion is observed for
nouns like organization names, lower animates and
vehicles. Apart from the linguistically and contextu-
ally defined animacy, there was no confusion, as ex-
pected, in the understanding of biological animacy.
Annotators ?
ann1-ann2 0.78
ann1-ann3 0.82
ann2-ann3 0.83
Average ? 0.811
Table 6: Kappa Statistics
Human Other-animate Inanimate
Human 71 0 14
Other-animate 0 9 5
Inanimate 8 10 241
Table 7: Confusion Matrix
5 Conclusion and Future Work
In this work, we have presented our efforts to enrich
the nominals in the Hindi Treebank with animacy
information. The annotation was followed by the
inter-annotator agreement study for evaluating the
confusion over the categories chosen for annotation.
The annotators have a significant understanding of
the property of animacy as shown by the higher val-
ues of Kappa (?). In future, we plan to continue the
animacy annotation for the whole Hindi Treebank.
We also plan to utilize the annotated data to build
a data driven automatic animacy classifier (?vrelid,
2006). From a linguistic perspective, an annotation
of the type, as discussed in this paper, will also be of
great interest for studying information dynamics and
see how semantics interacts with syntax in Hindi.
6 Acknowledgments
The work reported in this paper is supported by the
NSF grant (Award Number: CNS 0751202; CFDA
Number: 47.070). 2
References
Judith Aissen. 2003. Differential object marking:
Iconicity vs. economy. Natural Language & Linguis-
tic Theory, 21(3):435?483.
B.R. Ambati, P. Gade, S. Husain, and GSK Chaitanya.
2009. Effect of minimal semantics on dependency
parsing. In Proceedings of the Student Research Work-
shop.
B.R. Ambati, S. Husain, J. Nivre, and R. Sangal. 2010.
On the role of morphosyntactic features in Hindi de-
pendency parsing. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 94?102. As-
sociation for Computational Linguistics.
R. Begum, S. Husain, A. Dhwaj, D.M. Sharma, L. Bai,
and R. Sangal. 2008. Dependency annotation scheme
for Indian languages. In Proceedings of IJCNLP. Cite-
seer.
Akshar Bharati, Samar Husain, Bharat Ambati, Sambhav
Jain, Dipti Sharma, and Rajeev Sangal. 2008. Two se-
mantic features make all the difference in parsing ac-
curacy. Proceedings of ICON, 8.
2Any opinions, findings, and conclusions or recommenda-
tions expressed in this material are those of the author(s) and do
not necessarily reflect the views of the National Science Foun-
dation.
165
A. Bharati, D.M. Sharma, S. Husain, L. Bai, R. Begum,
and R. Sangal. 2009. AnnCorra: TreeBanks for Indian
Languages Guidelines for Annotating Hindi TreeBank
(version?2.0).
R.A. Bhat, S. Jain, and D.M. Sharma. 2012. Experi-
ments on Dependency Parsing of Urdu. In Proceed-
ings of TLT11 2012 Lisbon Portugal, pages 31?36.
Edic?es Colibri.
R. Bhatt, B. Narasimhan, M. Palmer, O. Rambow, D.M.
Sharma, and F. Xia. 2009. A multi-representational
and multi-layered treebank for hindi/urdu. In Pro-
ceedings of the Third Linguistic Annotation Workshop,
pages 186?189. Association for Computational Lin-
guistics.
Joan Bresnan, Jean Carletta, Richard Crouch, Malvina
Nissim, Mark Steedman, Tom Wasow, and Annie Za-
enen. 2002. Paraphrase analysis for improved genera-
tion, link project.
Miriam Butt. 2006. The dative-ergative connection. Em-
pirical issues in syntax and semantics, 6:69?92.
N. Chomsky. 1981. Lectures on Government and Bind-
ing. Dordrecht: Foris.
Bernard Comrie. 1989. Language universals and lin-
guistic typology: Syntax and morphology. University
of Chicago press.
Praveen Dakwale, Himanshu Sharma, and Dipti M
Sharma. 2012. Anaphora Annotation in Hindi Depen-
dency TreeBank. In Proceedings of the 26th Pacific
Asia Conference on Language, Information, and Com-
putation, pages 391?400, Bali,Indonesia, November.
Faculty of Computer Science, Universitas Indonesia.
Felice Dell?Orletta, Alessandro Lenci, Simonetta Mon-
temagni, and Vito Pirrelli. 2005. Climbing the
path to grammar: A maximum entropy model of sub-
ject/object learning. In Proceedings of the Workshop
on Psychocomputational Models of Human Language
Acquisition, pages 72?81. Association for Computa-
tional Linguistics.
R.M.W. Dixon. 1994. Ergativity. Number 69. Cam-
bridge University Press.
Richard Evans and Constantin Orasan. 2000. Improv-
ing anaphora resolution by identifying animate entities
in texts. In Proceedings of the Discourse Anaphora
and Reference Resolution Conference (DAARC2000),
pages 154?162.
Christiane Fellbaum. 2010. WordNet. Springer.
A. Hautli, S. Sulger, and M. Butt. 2012. Adding an an-
notation layer to the Hindi/Urdu treebank. Linguistic
Issues in Language Technology, 7(1).
Paul J Hopper and Sandra A Thompson. 1980. Tran-
sitivity in grammar and discourse. Language, pages
251?299.
Seppo Kittila?, Katja Va?sti, and Jussi Ylikoski. 2011.
Case, Animacy and Semantic Roles, volume 99. John
Benjamins Publishing.
A.K. Mahajan. 1990. The A/A-bar distinction and move-
ment theory. Ph.D. thesis, Massachusetts Institute of
Technology.
Paola Merlo and Suzanne Stevenson. 2001. Auto-
matic verb classification based on statistical distribu-
tions of argument structure. Computational Linguis-
tics, 27(3):373?408.
Dipak Narayan, Debasri Chakrabarty, Prabhakar Pande,
and Pushpak Bhattacharyya. 2002. An experience in
building the indo wordnet-a wordnet for hindi. In First
International Conference on Global WordNet, Mysore,
India.
Lilja ?vrelid and Joakim Nivre. 2007. When word or-
der and part-of-speech tags are not enough ? Swedish
dependency parsing with rich linguistic features. In
Proceedings of the International Conference on Recent
Advances in Natural Language Processing (RANLP),
pages 447?451.
Lilja ?vrelid. 2006. Towards robust animacy classifica-
tion using morphosyntactic distributional features. In
Proceedings of the Eleventh Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics: Student Research Workshop, pages 47?
54. Association for Computational Linguistics.
Lilja ?vrelid. 2009. Empirical evaluations of animacy
annotation. In Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics (EACL).
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. volume 31, pages 71?106. MIT Press.
M. Palmer, R. Bhatt, B. Narasimhan, O. Rambow, D.M.
Sharma, and F. Xia. 2009. Hindi Syntax: Annotat-
ing Dependency, Lexical Predicate-Argument Struc-
ture, and Phrase Structure. In The 7th International
Conference on Natural Language Processing, pages
14?17.
J. Pustejovsky. 1996. The Semantics of Complex Types.
Lingua.
Dipti Misra Sharma, Prashanth Mannem, Joseph van-
Genabith, Sobha Lalitha Devi, Radhika Mamidi, and
Ranjani Parthasarathi, editors. 2012. Proceedings of
the Workshop on Machine Translation and Parsing in
Indian Languages. The COLING 2012 Organizing
Committee, Mumbai, India, December.
Michael Silverstein. 1986. Hierarchy of features and
ergativity. Features and projections, pages 163?232.
Ulf Teleman. 1974. Manual fo?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
166
Juliette Thuilier, Laurence Danlos, et al 2012. Seman-
tic annotation of French corpora: animacy and verb
semantic classes. In LREC 2012-The eighth interna-
tional conference on Language Resources and Evalu-
ation.
Annie Zaenen, Jean Carletta, Gregory Garretson, Joan
Bresnan, Andrew Koontz-Garboden, Tatiana Nikitina,
M Catherine O?Connor, and Tom Wasow. 2004. Ani-
macy Encoding in English: why and how. In Proceed-
ings of the 2004 ACL Workshop on Discourse Anno-
tation, pages 118?125. Association for Computational
Linguistics.
167
