ON THE COMPLEXITY OF ID /LP  PARSING 1 
G. Edward Barton, Jr. 
MIT Artificial Intelligence Laboratory 
545 Technology Square 
Cambridge, MA 02139 
Modern linguistic theory attributes surface complexity to interacting subsystems of constraints. For 
instance, the ID/LP grammar formalism separates constraints on immediate dominance from those on 
linear order. An ID/LP parsing algorithm by Shieber shows how to use ID and LP constraints directly 
in language processing, without expanding them into an intermediate context-free "object grammar". 
However, Shieber's purported runtime bound underestimates the difficulty of ID/LP parsing. ID/LP 
parsing is actually NP-complete, and the worst-case runtime of Shieber's algorithm is actually expo- 
nential in grammar size. The growth of parser data structures causes the difficulty. Some computa- 
tional and linguistic implications follow; in particular, it is important to note that, desplte its potential 
for combinatorial explosion, Shieber's algorithm remains better than the alternative of parsing an 
expanded object grammar. 
1 INTRODUCTION 
It is common in recent linguistic theories for various 
surface characteristics of a language to be described in 
terms of several different kinds of underlying constraints. 
ID/LP grammars involve immediate-dominance rules and 
linear-order constraints; more broadly, GPSG systems 
can also involve feature relationships and metarules 
(Gazdar et al 1985). The tree adjunetion grammars of 
Kroch and Joshi (1985) separate the statement of local 
constraints from the projection of those constraints to 
larger structures. The GB-framework of Chomsky 
(1981:5) identifies the subtheories of bounding, govern- 
ment, 0-marking, binding, Case, and control. When 
several independent constraints are involved, a system 
that explicitly multiplies out their effects is large, cumber- 
some, and uninformative. 2 If done properly, the disen- 
tanglement of different kinds of constraints can result in 
shorter and more illuminating language descriptions. 
With any such modular framework, two questions 
immediately arise: how can the various constraints be 
put back together in parsing, and what are the computa- 
tional characteristics of the process? One approach is to 
compile a large object grammar that expresses the 
combined effects of the constraints in a more familiar 
format such as an ordinary context-free grammar (CFG). 
The context-free object grammar can then be parsed with 
Earley's (1970) algorithm or any of several other well- 
known procedures with known computational character- 
istics. 
However, in order to apply this method, it is necessary 
to expand out the effects of everything that falls outside 
the strict context-free format: rule schemas, metarules, 
ID rules, LP constraints, feature instantiations, case- 
marking constraints, etc. The standard algorithms oper- 
ate on CFGs, not on extended variants of them. 
Unfortunately, the object grammar may be huge after the 
effects of all nonstandard evices have been expanded 
out. Estimates of the object-grammar size for typical 
systems vary from hundreds or thousands 3 up to trillions 
of rules (Shieber 1983:4). With some formalisms, the 
context-free object-grammar pproach is not even possi- 
ble because the object grammar would be infinite (Shie- 
ber 1985:145). Grammar size matters beyond questions 
of elegance and clumsiness, for it typically affects proc- 
essing complexity. Berwick and Weinberg (1982) argue 
that the effects of grammar size can actually dominate 
complexity for a relevant range of input lengths. 
Given the disadvantages of multiplying out the effects 
of separate systems of constraints, Shieber's (1983) work 
on direct parsing leads in a welcome direction. Shieber 
considers how one might do parsing with ID/LP gram- 
mars, which involve two orthogonal kinds of rules. ID 
rules constrain immediate dominance irrespective of 
constituent order ("a sentence can be composed of V 
with NP and SBAR complements"), while LP rules 
Copyright1985 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided that 
the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To copy 
otherwise, or to republish, requires a fee and/or specific permission. 
0362-613X/85/040205-218503.00 
Computational Linguistics, Volume 11, Number 4, October-December 1985 205 
G. Edward Barton, Jr. On the Complexity of ID/LP Parsing 
constrain linear precedence among the daughters of any 
node ("if V and SBACR are sisters, then V must precede 
SBAR"). Shieber shows how Earley's (1970) algorithm 
for parsing context-free grammars (CFGs) can be 
adapted to use the constraints of ID/LP grammars direct- 
ly, without the combinatorially explosive step of convert- 
ing the ID/LP grammar into standard context-free form. 
Instead of multiplying out all of the possible surface 
interactions among the ID and LP rules, Shieber's algo- 
rithm applies them one step at a time as needed. Surely 
this should work better in a parsing application than 
applying Earley's algorithm to an expanded grammar 
with trillions of rules, since the worst-case time complexi- 
ty of Earley's algorithm is proportional to the square of 
the grammar size! 
Shieber's general approach is on the right track. On 
pain of having a large and cumbersome rule system, the 
parser designer should first look to linguistics to find the 
correct set of constraints on syntactic structure, then 
discover how to apply some form of those constraints in 
parsing without multiplying out all possible surface 
manifestations of their effects. 
Nonetheless, nagging doubts about computational 
complexity remain. Although Shieber (1983:15) claims 
that his algorithm is identical to Earley's in time complex- 
ity, it seems almost too much to hope for that the size of 
an ID/LP grammar should enter into the time complexity 
of ID/LP parsing in exactly the same way that the size of 
a CFG enters into the time complexity of CFG parsing. 
An ID/LP grammar G can enjoy a huge size advantage 
over a context-free grammar G p for the same language; 
for example, if G contains only the rule S -~ ~D abcde, 
the corresponding G r contains 5! = 120 rules. In effect, 
the claim that Shieber's algorithm has the same time 
complexity as Earley's algorithm means that this tremen- 
dously increased brevity of expression comes free (up to 
a constant). The paucity of supporting argument in 
Shieber's article does little to allay these doubts: 
We will not present a rigorous demonstration of time 
complexity, but it should be clear from the close relation 
between the presented algorithm and Earley's that the 
complexity is that of Earley's algorithm. In the worst 
case, where the LP rules always specify a unique ordering 
for the right-hand side of every ID rule, the presented 
algorithm reduces to Earley's algorithm. Since, given the 
grammar, checking the LP rules takes constant ime, the 
time complexity of the presented algorithm is identical to 
Earley's . . . .  That is, it is O( I G 12 n3), where I G I is 
the size of the grammar (number of ID rules) and n is the 
length of the input. (: 14f) 
Many questions remain; for example, why should a situ- 
ation of maximal constraint represent the worst case, as 
Shieber claims? 4
The following sections will investigate the complexity 
of ID/LP parsing in more detail. In brief, the outcome is 
that Shieber's direct-parsing algorithm usually does have 
a time advantage over the use of Earley's algorithm on 
the expanded CFG, but that it blows up in the worst case. 
The claim of O( I G 12 n 3) time complexity is mistaken; in 
fact, the worst-case time complexity of ID/LP parsing 
cannot be bounded by any polynomial in the size of the 
grammar and input, unless ~' = ,A'@. ID/LP parsing is 
NP-complete. 
As it turns out, the complexity of ID/LP parsing has its 
source in the immediate-domination rules rather than the 
linear precedence constraints. Consequently, the prece- 
dence constraints will be neglected. Attention will be 
focused on unordered context-free grammars (UCFGs), 
which are exactly like standard context-free grammars 
except that when a rule is used in a derivation, the 
symbols on its right-hand side are considered to be unor- 
dered and hence may be written in any order. UCFGs 
represent he special case of ID/LP grammars in which 
there are no LP constraints. Shieber's ID/LP algorithm 
can be used to parse UCFGs simply by ignoring all refer- 
ences to LP constraints. 
2 GENERALIZING EARLEY'S ALGORITHM 
Shieber generalizes Earley's algorithm by modifying the 
progress datum that tracks progress through a rule. The 
Earley algorithm uses the position of a dot to track.linear 
advancement through an ordered sequence of constitu- 
ents. The major predicates and operations on such 
dotted rules are these: 
? A dotted rule is initialized with the dot at the left edge, 
as in X --, .ABC . 
? A dotted rule is advanced across a terminal or nonter- 
minal that was predicted and has been located in the 
input by simply moving the dot to the right. For exam- 
ple, X -~ A .BC is advanced across a B by moving the 
dot to obtain X -~ AB.C  . 
? A dotted rule is complete iff the dot is at the right edge. 
For example, X -~ ABC.  is complete. 
? A dotted rule predicts a terminal or nonterminal iff the 
dot is immediately before the terminal or nonterminal. 
For example, X -~ A .BC predicts B. 
UCFG rules differ from CFG rules only in that the right- 
hand sides represent unordered multisets (that is, sets 
with repeated elements allowed). It is thus appropriate to 
use successive accumulation of set elements in place of 
linear advancement through a sequence. In essence, 
Shieber's algorithm replaces the standard operations on 
dotted rules with corresponding operations on what will 
be called dotted UCFG rules. 5 
? A dotted UCFG rule is initialized with the empty multi- 
set before the dot and the entire multiset of right-hand 
elements after the dot, as in X --- { } ? {A, B, C}. 
? A dotted UCFG rule is advanced across a terminal or 
nonterminal that was predicted and has been located in 
the input by simply moving one element from the 
multiset after the dot to the multiset before the dot. 
For example, X -- {d} ? {B, C} is advanced across a B 
by moving the B to obtain X -* {A, B} ? {C}. Similar- 
206 Computational Linguistics, Volume 11, Number 4, October-December 1985 
G. Edward Barton, Jr. On the Complexity of ID/LP Parsing 
ly, X -~ {A } ? {B, C, C} may be advanced across a C to 
obtain X-~ {A, C}" {B, C}. 
? A dotted UCFG rule is complete iff the multiset after 
the dot is empty. For example, X -~ {A, B, C} ? {} is 
complete. 
? A dotted UCFG rule predicts a terminal or nonterminal 
iff the terminal or nonterminal is a member of the 
multiset after the dot. For example, X -~ {A} ? {B, C} 
predicts B and C. 
Given these replacements for operations on dotted rules, 
Shieber's algorithm operates in the same way as Earley's 
algorithm. As usual, each state in the parser's tate sets 
consists of a dotted rule tracking progress through a 
constituent plus the if~terword position defining the 
constituent's left edge (Earley 1970:95, omitting looka- 
head). The left-edge position is also referred to as the 
return pointer because of its role in the complete opera- 
tion of the parser. 
3 THE ADVANTAGES OF SHIEBER'S ALGORITHM 
The first question to ask is whether Shieber's algorithm 
saves anything. Is it faster to use Shieber's algorithm on 
a UCFG than to use Earley's algorithm on the corre- 
sponding expanded CFG? Consider the UCFG G1 that 
has only the single rule S -* abcde. The corresponding 
CFG Grl has 120 rules spelling out all the permutations 
of abcde: S -~ abcde, S -- abced, and so forth. If the 
string abcde is parsed using Shieber's algorithm directly 
on G1, the state sets of the parser emain small. 6 
S O : \[S "-" { } " {a,b,c,d,e}, O\] 
S 1 : \[S ~ {a} ? {b,c,d,e}, O\] 
S z : \[S -*. {a,b} ? {c,d,e}, O\] 
S 3 " \ [S  ~ {a,b,c} ? {d,e}, O\] 
3 4 : \ [S  -~ {a,b,c,d} ? {e}, 0\] 
S 5 : \[S ~ {a,b,c,d,e} ? { }, O\] 
In contrast, consider what happens if the same string is 
parsed using Earley's algorithm on the expanded CFG 
with its 120 rules. As Figure 1 illustrates, the state sets 
of the Earley parser are much larger. In state set S~, the 
Earley parser uses 4! = 24 states to spell out all the 
possible orders in which the remaining symbols {b,c,d,e} 
could appear. Shieber's modified parser does not spell 
them out, but uses the single state \[S ~ {a} ? {b,c,d,e}, O\] 
to summarize them all. Shieber's algorithm should thus be 
faster, since both parsers work by successively processing 
all of the states in the state sets. 
Similar examples show that the Shieber parser can 
enjoy an arbitrarily large advantage over the use of the 
Earley parser on the expanded CFG. Instead of multiply- 
ing out all surface appearances ahead of time to produce 
an expanded CFG, Shieber's algorithm works out the 
possibilities one step at a time, as needed. This can be an 
advantage because not all of the possibilities may arise 
with a particular input. 
(a) 
(b) 
\[S ~ {a} ? {b,c,d,e}, O\] 
\[S --~ a.edcb, O\] \[S ~ a.ecbd, O\] 
\[S -* a.decb, O\] \[S ~ a.cebd, O\] 
\[S --~ a.ecdb, O\] \[S -~ a.ebcd, O\] 
\[S ~ a.cedb, O\] \[S --~ a.becd, O\] 
\[S ~ a.dceb, O\] \[S --~ a.cbed, O\] 
\[S --~ a.cdeb, O\] \[S --,. a.bced, O\] 
\[S --,. a.edbc, O\] \[S --~ a.dcbe, O\] 
\[S -~ a.debc, O\] \[S --,. a.cdbe, O\] 
\[S --~ a.ebdc, O\] \[S -~ a.dbce, O\] 
\[S -,. a.bedc, O\] \[S --~ a.bdce, O\] 
\[S --~ a.dbec, O\] \[S ~ a.cbde, O\] 
\[S ~ a.bdec, O\] \[S ~ a.bcde, O\] 
Figure 1. The use of the Shieber parser on a UCFG can 
enjoy a large advantage over the use of the Earley parser 
on the corresponding expanded CFG. After having proc- 
essed the terminal a while parsing the string abcde as 
discussed in the text, the Shieber parser uses the single 
state shown in (a) to keep track of the same information 
for which the Earley parser uses the 24 states in (b). 
4 COMBINATORIAL EXPLOSION WITH 
SHIEBER'S ALGORITHM 
The answer to the first question is yes, then: it can be 
more efficient to use Shieber's parser than to use the 
Earley parser on an expanded object grammar. The 
second question to ask is whether Shieber's parser always 
enjoys a large advantage. Does the algorithm blow up in 
difficult cases? 
In the presence of lexical ambiguity, Shieber's algo- 
rithm can suffer from combinatorial explosion. Consider 
the following UCFG, Gz, in which x is five-ways ambig- 
uous: 
S -~ ABCDE 
A -~a lx  
B -~blx  
C -~c lx  
D ~dlx  
E -~e lx  
What happens if Shieber's algorithm is used to parse the 
string xxxxa according to this grammar? After the first 
three occurrences of x have been processed, the state set 
of Shieber's parser will reflect the possibility that any 
three of the phrases A, B, C, D, and E might have been 
encountered in the input and any two of them might 
5 remain to be parsed. There will be (q) = 10 states 
reflecting progress through the rule expaffding S, in addi- 
tion to 5 states reflecting phrase completion and 10 states 
reflecting phrase prediction (not shown): 
Computational Linguistics, Volume 11, Number 4, October-December 1985 207 
G. Edward Barton, Jr. On the Complexity of ID/LP Parsing 
S 3" IS- -~ 
\ [ s - .  
\[s-~ 
\[ s -. 
\[s-~ 
\[s-~ 
\[s -, 
\ [ s - .  
\[ s -~ 
\[s-~ 
{A,B,C} {D,E}, O\] 
{A,B,D} {C,E}, O\] 
{A,C,D} {B,E}, O\] 
{B,C,D} {A,E}, O\] 
{A,B,E} {C,D}, O\] 
{A,C,E} {B,D}, O\] 
{B,C,E} {A,D}, O\] 
{A,D,E} {B,C}, O\] 
{B,D,E} {A,C}, O\] 
{C,D,E} {A,B}, O\] 
In cases like this, Shieber's algorithm enumerates all of 
the combinations of k elements taken i at a time, where k 
is the rule length and i is the number of elements already 
processed. Thus it can be combinatorially explosive. 
It is important o note that even in this case Shieber's 
algorithm wins out over parsing the expanded CFG with 
Earley's algorithm. After the same input symbols have 
been processed, the state set of the Earley parser will 
reflect the same possibilities as the state set of the Shie- 
ber parser: any three of the required phrases might have 
been located, while any two of them might remain to be 
parsed. However, the Earley parser has a less concise 
representation to work with. In place of the state involv- 
ing S -~ {A,B,C} ? {D,E}, for instance, there will be 3! ? 
2! = 12 states involving S -, ABC.DE, S -~ BCA.ED, 
and so forth. Instead of a total of 25 states, the Earley 
state set will contain 135 = 12 ? 10 + 15 states. 7
In the above case, although the parser could not be 
sure of the eategorial identities of the phrases parsed, at 
least there was no uncertainty about the number of 
phrases and their extent. We can make matters even 
worse for the parser by introducing uncertainty in those 
areas as well. Let G3 be the result of replacing every x in 
Gz with the empty string e: 
S -~ ABCDE 
A "~ a le  
B --, b le  
C -c le  
D --, d ie  
E - ' -e le  
Then an A, for instance, can be either an a or nothing. 
Before any input has been read, the first state set So in 
Shieber's parser must reflect the possibility that the 
correct parse may include any of the 25 = 32 possible 
subsets of A, B, C, D, and E as empty initial constituents. 
For example, S O must include \[S --- {A,B,C,D,E} ? {}, 0\] 
because the input might turn out to be the null string. 
Similarly, it must include \[S -~ {A,C,E} ? {B,D}, O\] 
because the input might turn out to be bd or db. Counting 
all possible subsets in addition to other states having to 
do with predictions, completions, and the parser's start 
symbol, there are 44 states in S O . (There are 338 states 
in the corresponding state when the expanded CFG Gt3 is 
used.) 
5 THE SOURCE OF THE DIFFICULTY 
Why is Shieber's algorithm potentially exponential in 
grammar size despite its "close relation" to Earley's algo- 
rithm, which has time complexity polynomial in grammar 
size? The answer lies in the size of the state space that 
each parser uses. Relative to grammar size, Shieber's 
algorithm~ involves a much larger bound than Earley's 
algorithm on the number of states in a state set. Since 
the main task of the Earley parser is to perform scan, 
predict, and complete operations on the states in each 
state set (Eariey 1970:97), an explosion in the size of the 
state sets will be fatal to any small runtime bound. 
Given a CFG Go, how many possible dotted rules are 
there? Resulting from each rule X - -  A t . . .  Ak, there are 
k+ 1 possible dotted rules. Then the number of possible 
dotted rules is bounded by I G I, if this notation is taken 
to mean the number of symbols that it takes to write G 
down. An Earley state is a pair \[r,i\], where r is a dotted 
rule and i is an interword position ranging from 0 to the 
length n of the input string. Because of these limits, no 
state set in the Earley parser can contain more than 
O( I Q l'n) (distinct) states. 
The limited size of a state set alows an O( I Go 12 ? n3) 
bound to be placed on the runtime of the Earley parser. 
Informally, the argument (due to Earley) runs as follows. 
The scan operation on a state can be done in constant 
time; the scan operations in a state set thus contribute no 
more than O( \[ Ga \[ " n) computational steps. All of the 
predict operations in a state set taken together can add no 
more states than the number of rules in the grammar, 
bounded by I G al ,  since a nonterminal needs to be 
expanded only once in a state set regardless of how many 
times it is predicted; hence the predict operations need 
not take more than O( I G I " n+ \[ G I ) = o (  I G I ? n) 
steps. Finally, there are the complete operations to be 
considered. A given completion can do no worse than 
advancing every state in the state set indicated by the 
return pointer. Therefore, any bound k on state set size 
leads to a bound of k z on the number of steps it takes to 
do all the completions in a state set. Here k = 
O( \[ G I 'n),  so the complete operations in a state set can 
take at most O( I G I z .  n 2) steps. Overall, then, it takes 
no more than O( I G 12 ? n 2) steps to process one state set 
and no more than O(IG a \[ z ? n 3) steps for the Earley 
parser to process them all. 
In Shieber's parser, though, the state sets can grow 
much larger relative to grammar size. Given a UCFG Gb, 
how many possible dotted UCFG rules are there? Result- 
ing from a rule X -~ A 1 ... A k, there are not k+l  possible 
dotted rules tracking linear advancement, but 2 k possible 
dotted UCFG rules tracking accumulation of set elements. 
In the worst case, the grammar contains only one rule 
and k is on the order of I Gel ;  hence the number of 
possible dotted UCFG rules for the whole grammar is not 
bounded by I Gbl, but by 216b I. (The bound can be 
reached; recall that exponentially many dotted rules are 
created in the processing of G 3 from section 4.) 
208 Computational Linguistics, Volume 11, Number 4, October-December ! 985 
G. Edward Barton, Jr. On the Complexity of ID/LP Parsing 
Informally speaking, the reason why Shieber's parser 
sometimes uffers from combinatorial explosion is that 
there are exponentially more possible ways to progress 
through an unordered rule expansion than an ordered 
one. When disambiguating information is scarce, the 
parser must keep track of all of them. In the more gener- 
al task of parsing ID/LP grammars, the most tractable 
case occurs when constraint from the LP relation is 
strong enough to force a unique ordering for every rule 
expansion. Under such conditions, Shieber's parser 
reduces to Earley's. However, the case of strong 
constraint represents the best case computationally, rather 
than the worst case as Shieber (1983:14) claims. 
6 ID/LP PARSING IS INHERENTLY DIFFICULT 
The worst-case time complexity of Shieber's algorithm is 
exponential in grammar size rather than quadratic as 
Shieber (1983:15) believed. Did Shieber simply choose a 
poor algorithm, or is ID/LP parsing inherently difficult in 
the general case? In fact, the simpler problem of recog- 
n i z ing  sentences according to a UCFG is NP-complete. 8 
Consequently, unless ~ = ,./F~, no algorithm for ID/LP 
parsing can have a runtime bound that is polynomial in 
the size of the grammar and input. 
The proof of NP-completeness involves reducing the 
vertex cover problem (Garey and Johnson 1979:46) to 
the UCFG recognition problem. Through careful 
construction of the grammar and input string, it is possi- 
ble to "trick" the parser into solving a known hard prob- 
lem. The vertex cover problem involves finding a small 
set of vertices in a graph with the property that every 
edge of the graph has at least one endpoint in the set. 
Figure 2 shows a trivial example. 
e$ 
b 
e4 
Figure 2. This graph illustrates a trivial instance of the 
vertex cover problem. The set {c,d} is a vertex cover of 
size 2. 
To construct a grammar that encodes the question of 
whether the graph in Figure 2 has a vertex cover of size 
2, first take the vertex names a, b, c, and d as the alpha- 
bet. Take START as the start symbol. Take H 1 through 
H 4 as special symbols, one per edge; also take U and D 
as special dummy symbols. 
Next, write the rules corresponding to the edges of the 
graph. Edge e a runs from a to c, so include the rules H~ 
-, a and H t -, c. Encode the other edges similarly. 
Rules expanding the dummy symbols are also needed. 
Dummy symbol D will be used to soak up excess input 
symbols, so D -, a through D -, d should be rules. 
Dummy symbol U will also be used to soak up excess 
input symbols, but U will be allowed to match only when 
there are four occurrences in a row of the same symbol 
(one occurrence for each edge). Take U ~ aaaa,  U - ,  
bbbb,  U - ,  cccc, and U -~ dddd as the rules expanding U. 
Now, what does it take for the graph to have a vertex 
cover of size k = 2? One way to get a vertex cover is to 
go through the list of edges and underline one endpoint 
of each edge. If the vertex cover is to be of size 2, the 
underlining must be done in such a way that only two 
distinct vertices are ever touched in the process. Alterna- 
tively, since there are 4 vertices in all, the vertex cover 
will be of size 2 if there are 4-2=2 vertices left untouched 
in the underlining process. This method of finding a 
vertex cover can be translated into a UCFG rule as 
follows: 
START- - , .  H IH2H3H4UUDDDD 
That is, each H-symbol is supposed to match the name of 
one of the endpoints of the corresponding edge, in 
accordance with the rules expanding the H-symbols. 
Each U-symbol is supposed to correspond to a vertex 
that was left untouched by the H-matching, and the 
D-symbols are just there for bookkeeping. Figure 3 lists 
the complete grammar that encodes the vertex-cover 
problem of Figure 2. 
START - ,  HtHzH3H 4UUDDDD 
H I -~ a l c 
H 2 - ,  b lc  
H 3 - , c ld  
H 4 - -b id  
U - ,  aaaa  I bbbb I cccc I dddd 
D - ,  a I b I c I d 
Figure 3. For k = 2, the construction described in the 
text transforms the vertex-cover problem of Figure 2 into 
this UCFG. A parse exists for the string 
aaaabbbbccccdddd iff the graph in the previous figure has 
a vertex cover of size < 2. 
To make all of this work properly, take 
o = aaaabbbbccccdddd 
as the input string to be parsed. (In general, for every 
vertex name x, include in o a contiguous run of occur- 
rences of x, one occurrence for each edge in the graph.) 
The grammar encodes the underlining procedure by 
requiring each H-symbol to match one of its endpoints in 
o. Since the right-hand side of the START rule is unor- 
dered, the grammar allows an H-symbol to match 
anywhere in the input, hence to match any vertex name 
(subject to interference from other rules that have 
already matched). Furthermore, since there is one occur- 
Computational Linguistics, Volume 11, Number 4, October-December 1985 209 
G. Edward Barton, Jr. On the Complexity of ID/LP Parsing 
rence of each vertex name for every edge, all of the 
edges could conceivably be matched up with the same 
vertex; that is, it's impossible to run out of vertex-name 
occurrences. Consequently, the grammar will allow 
either endpoint of an edge to be "underlined". The 
parser will have to figure out which endpoints to choose 
- in other words, which vertex cover to select. However, 
the grammar also requires two occurrences of U to match 
somewhere. U can only match four contiguous identical 
input symbols that have not been matched in any other 
way, and thus if the parser chooses a vertex cover that is 
too large, the U-symbols will not match and the parse 
will fail. The proper number of D-symbols is given by 
the length of the input string, minus the number of edges 
in the graph (to account for the /-/~-matches), minus k 
times the number of edges (to account for the 
U-matches): in this case, 16 - 4 - (2.4) = 4, as illus- 
trated in the START rule. 
The net result of this construction is that in order to 
decide whether o is in the language generated by the 
UCFG, the parser must in effect search for a vertex cover 
of size 2 or less. 9 If a parse exists, an appropriate vertex 
cover can be read off from beneath the H-symbols in the 
parse tree; conversely, if an appropriate vertex cover 
exists, it indicates how to construct a parse. Figure 4 
shows the parse tree that encodes a solution to the 
vertex-cover problem of Figure 2. 
The construction shows that vertex-cover problem is 
reducible to UCFG recognition. Furthermore, the 
construction of the grammar and input string can be 
carried out in polynomial time. Consequently, UCFG 
recognition and the more general task of 1D/LP parsing 
must be computationally difficult. For a more careful 
and detailed treatment of the reduction and its correct- 
ness, see the appendix. 
7 COMPUTATIONAL IMPLICATIONS 
The reduction of Vertex Cover shows that 'the ID /LP  
parsing problem is NP-complete. Unless ~ = ,./V~P, the 
time complexity of ID/LP parsing cannot be bounded by 
any polynomial in the size of the grammar and input, l? 
An immediate conclusion is that complexity analysis must 
be done carefully: despite its similarity to Earley's algo- 
rithm, Shieber's algorithm does not have complexity 
O( I G 12 . n3). For some choices of grammar and input, 
its internal structures undergo exponential growth. Other 
consequences also follow. 
7.1 PARSING THE OBJECT GRAMMAR 
Even in the face of its combinatorially explosive worst- 
case behavior, Shieber's algorithm should not be imme- 
diately cast  aside. Despite the fact that it sometimes 
blows up, it still has an advantage over the alternative of 
parsing the expanded object grammar. One interpreta- 
tion of the NP-completeness result is that the general 
case of ID/LP parsing is inherently difficult; hence it 
should not be surprising that Shieber's algorithm for solv- 
ing that problem can sometimes uffer from combina- 
torial explosion. More significant is the fact that parsing 
with the expanded CFG blows up in cases that should not 
be difficult. There is nothing inherently difficult about 
parsing the language that consists of all permutations of 
the string abcde, but while parsing that language the 
Earley parser can use 24 states or more to encode what 
the Shieber parser encodes in only one (section 3). To 
put the point another way, the significant fact is not that 
the Shieber parser can blow up; it is that the use of an 
expanded CFG blows up unnecessarily. 
START 
U U HI H2 Hs  D 
a aa  a b b bb  e c e c 
114 D D D 
d d d d 
Figure 4. The grammar of Figure 3, which encodes the the vertex-cover problem of Figure 2, generates the string o = 
aaaabbbbccccdddd according to this parse tree. The vertex cover {c,d} can be read off from the parse tree as the set of 
elements dominated by H-symbols. 
210 Computational Linguistics, Volume 11, Number 4, October-December 1985 
G. Edward Barton, J r .  On  the Complexity of ID /LP  Parsing 
7.2 IS PRECOMPILAT ION POSSIBLE?  
The present reduction of Vertex Cover to ID/LP Parsing 
involves constructing a grammar and input string that 
both depend on the problem to be solved. Consequently, 
the reduction does not rule out tlae possibility that 
through clever programming one might concentrate most 
of the computational difficulty of ID/LP parsing into a 
separate precompilation stage, dependent on the grammar 
but independent of the input. According to this optimis- 
tic scenario, the entire procedure of preprocessing the 
grammar and parsing the input string would be as diffi- 
cult as any NP-complete problem, but after precompila- 
tion, the time required for parsing a particular input 
would be bounded by a polynomial in grammar size and 
sentence length. 
Regarding the case immediately at hand, Shieber's 
modified Earley algorithm has no precompilation step. 11 
The complexity result implied by the reduction thus 
applies with full force; any possible precompilation phase 
has yet to be proposed. Moreover, it is by no means 
clear that a clever precompilation step is even possible; it 
depends on exactly how I GI and n enter into the 
complexity function for ID/LP parsing. If n enters as a 
factor multiplying an exponential, precompilation cannot 
help enough to ensure that the parsing phase will run in 
polynomial time. 
For example, suppose some parsing problem is known 
to require 2161 ? n 3 steps for solution. 12 If one is willing to 
spend, say, 10 ? 2161 steps in the precompilation phase, is 
it possible to reduce parsing-phase complexity to some- 
thing like I GI 8 ? n3? The answer is no. Since by 
hypothesis it takes at least 2161 ? n 3 steps to solve the 
problem, there must be at least 2161 ? n 3 - -  10  ? 2161 steps 
left to perform after the precompilation phase. The 
parameter n is necessarily absent from the precompilation 
complexity, hence the term 2161 ? n 3 will eventually domi- 
nate. 
In a related vein, suppose the precompilation step is 
conversion from ID/LP to CFG form and the runtime 
step is the use of the Earley parser on the expanded CFG. 
Although the precompilation step does a potentially 
exponential amount of work in producing G p from G, 
another exponential factor still shows up at runtime 
because I G p I in the complexity bound I G r 1 2 " n3 is 
exponentially arger than the original I G I ? 
7.3 POLYNOMINAL-T IME PARSING 
OF  A F IXED GRAMMAR 
As noted above, both grammar and input in the current 
vertex-cover reduction depend on the vertex-cover prob- 
lem to be solved. The NP-completeness result would be 
strengthened if there were a reduction that used the same 
fixed grammar for all vertex-cover problems, for it would 
then be possible to prove that a precompilation phase 
would be of little avail. However, unless ~ = ,./t'~, it is 
impossible to design such a reduction. Since grammar 
size is not considered to be a parameter of a fixed-gram- 
mar parsing problem, the use of the Earley parser on the 
object grammar constitutes a polynomial-time algorithm 
for solving the fixed-grammar ID/LP parsing problem. 
Although ID/LP parsing for a fixed grammar can 
therefore be done in cubic time, that fact represents little 
more than an accounting trick. The object grammar G p 
corresponding to a practical ID/LP grammar would be 
huge, and if I G' I  2 ? n 3 complexity is too slow, then it 
remains too slow when I G r 1 2 is regarded as a constant. 
The practical irrelevance of polynomial-time parsing 
for a fixed grammar sheds some light on another question 
that is sometimes asked. Can't we have our cake and eat 
it too by using the ID/LP grammar G directly when we 
want to see linguistic generalizations, but parsing the 
object grammar G ~ when we want efficient parsing? 
After all, the Earley algorithm runs in cubic time based 
on the length of the input string, and its dependence on 
grammar size is only I G r 1 2 
Essentially, the answer is that using the object gram- 
mar doesn't help. The reduction shows that it's not 
always easy to process the ID/LP form of the grammar, 
but it is no easier to use the Earley algorithm on the 
expanded form. As the examples that have been 
presented clearly illustrate, both the Shieber parser and 
the Earley parser for a given language can end up with 
state sets that contain large numbers of elements. The 
object grammar does not promote efficient processing; 
the Shieber parser operating on the ID/LP grammar can 
often do better than the Earley parser operating on the 
object grammar, be.cause of its more concise represen- 
tation (section 4). 
The Earley-algorithm grammar-size factor I G r 1 2 
looks smaller than the Shieber factor 2161 until one 
recalls that G ~ can be exponentially larger than G. In 
other words, we can hide the factor 2161 inside I Gr l ,  
but that doesn't make it any smaller. Thus parsing is 
likely to take a great many steps even if we parse the 
object grammar, which might mistakenly be thought to 
be more efficient than direct parsing. If an algorithm 
runs too slowly, it doesn't make it faster if we cover up 
the exponential factor and make it a constant K, and it's 
unlikely that ID/LP parsing can be done quickly in the 
general case. 
7.4 THE POWER OF THE UCFG FORMALISM 
The Vertex Cover reduction also helps pin down the 
computational power of the UCFG formalism. As G 1 and 
Grl in section 3 illustrated, a UCFG (or an ID/LP gram- 
mar) can enjoy considerable brevity of expression 
compared to the equivalent CFG. The NP-completeness 
result illuminates this property in two ways. First, the 
result shows that this brevity of expression is sufficient o 
allow an instance of any problem in ,Tg~ to be stated in a 
UCFG that is only polynomiaUy larger than the original 
problem instance. In contrast, if an attempt is made to 
replicate the current reduction with a CFG rather than 
UCFG, the necessity of spelling out all the orders in 
which the H-, U-, and D-symbols might appear makes 
Computational Linguistics, Volume 11, Number  4,  October -December  1985 211 
G. Edward Barton, Jr. On the Complexity of ID/LP Parsing 
the CFG more than polynomially larger than the problem 
instance. Consequently, the reduction fails to establish 
NP-completeness, which indeed does not hold. Second, 
the result shows that the increased expressive power does 
not come free; while the CFG recognition problem can be 
solved in time O( \[ G lZ'n 3) unless ~ = ,.,F~, the general 
UCFG recognition problem cannot be solved in polyno- 
mial time. 
The details of the reduction show how powerful a 
single UCFG rule can be. If the UCFG formalism is 
extended to permit ordinary CFG rules in addition to 
rules with unordered expansions, the grammar that 
expresses a vertex-cover problem needs only one UCFG 
rule, although that rule may need to be arbitrarily long. t3 
7.5 THE ROLE OF CONSTRAINT 
Finally, the discussion of section 5 illustrates the way in 
which the weakening of constraints can often make a 
problem computationally more difficult. It might erro- 
neously be thought that weak constraints represent he 
best case in computational terms, for "weak" constraints 
sound easy to verify. However, oftentimes the weaken- 
ing of constraint multiplies the number of possibilities 
that must be considered in the course of solving a prob- 
lem. In the case at hand, the removal of constraints on 
the order in which constituents can appear causes the 
dependence of parsing complexity on grammar size to 
grow from \[ G 12 to 21 o l 
8 LINGUISTIC IMPLICATIONS 
The key factors that cause difficulty in ID/LP parsing are 
familiar to linguistic theory. GB-theory and GPSG both 
permit the existence of constituents hat are empty on the 
surface, and thus in principle they both allow the kind of 
pathology illustrated by G 3 in section 4, subject o amel- 
ioration by additional constraints. Similarly, every 
current theory acknowledges lexical ambiguity, a key 
ingredient of the vertex-cover reduction. Though the 
reduction illuminates the power of certain mechanisms 
and formal devices, the direct implications of the 
NP-completeness result for grammatical theory are few. 
The reduction does expose the weakness of attempts 
to link context-free generative power directly to efficient 
parsability. Consider, for instance, Gazdar's (1981 :155) 
claim that the use of a formalism with only context-free 
power can help explain the rapidity of human sentence 
processing: 
Suppose . . .  that the permitted class of generative gram- 
mars constituted asubset of those phrase structure gram- 
mars capable only of generating context-free languages. 
Such a move would have two important metatheoretical 
consequences, one having to do with learnability, the 
other with processability . . . .  We would have the begin- 
nings of an explanation for the obvious, but largely 
ignored, fact that humans process the utterances they 
hear very rapidly. Sentences of a context-free language 
are provably parsable in a time which is proportional to 
the cube of the length of the sentence or less. 
As the arguments and examples in this paper have illus- 
trated, context-free generative power does not guarantee 
efficient parsability. Every ID/LP grammar technically 
generates a context-free language, but the potentially 
large size of the corresponding CFG means that we can't 
count on that fact to give us efficient parsing. Thus it is 
impossible to sustain this particular argument for the 
advantages of such formalisms as (early) GPSG over 
other linguistic theories; instead, GPSG and other modern 
theories eem to be (very roughly) in the same boat with 
respect o complexity. In such a situation, the linguistic 
merits of various theories are more important than 
complexity results. (See Berwick (1982), Berwick and 
Weinberg (1982, 1984), and Ristad (1985) for further 
discussion.) 
The reduction does not rule out the use of formalisms 
that decouple ID and LP constraints; note that Shieber's 
direct parsing algorithm wins out over the use of the 
object grammar. However, if we assume that natural 
languages are efficiently parsable (EP), then computa- 
tional difficulties in parsing a formalism do indicate that 
the formalism itself does not tell the whole story. That is, 
they point out that the range of possible languages has 
been incorrectly characterized: the additional constraints 
that guarantee efficient parsability remain unstated. 
Since the general case of parsing ID/LP grammars is 
computationally difficult, if the linguistically relevant 
ID/LP grammars are to be efficiently parsable, there 
must be additional factors that guarantee a certain 
amount of constraint from some source, t4 (Constraints 
beyond the bare ID/LP formalism are required on linguis- 
tic grounds as well.) Note that the subset principle of 
language acquisition (cf. Berwiek and Weinberg 
1984:233) would lead the language learner to initially 
hypothesize strong order constraints, to be weakened 
only in response to positive evidence. 
However, there are other potential ways to guarantee 
efficient parsability. It might turn out that the principles 
and parameters of the best grammatical theory permit 
languages that are not efficiently parsable in the worst 
case - just as grammatical theory permits sentences that 
are deeply center-embedded (Miller and Chomsky 
1963). 15 In such a situation, difficult languages or 
sentences would not be expected to turn up in general 
use, precisely because they would be difficult to process. 16 
The factors that guarantee fficient parsability would not 
be part of grammatical theory because they would result 
from extragrammatical f ctors, i.e. the resource limita- 
tions of the language-processing mechanisms. This "easy 
way out" is not automatically available, depending as it 
does on a detailed account of processing mechanisms. 
For example, in the Earley parser, the difficulty of pars- 
ing a construction can vary widely with the amount of 
lookahead used (if any). Like any other theory, an 
explanation based on resource limitations must make the 
right predictions about which constructions will be diffi- 
cult to parse. 
212 Computational Linguistics, Volume 11, Number 4, October-December 1985 
G. Edward Barton, Jr. On the Complexity of ID /LP  Parsing 
In the same way, the language-acquisition procedure 
could potentially be the source of some constraints rele- 
vant to efficient parsability. Perhaps not all of the 
languages permitted by the principles and parameters of 
syntactic theory are accessible in the sense that they can 
potentially be constructed by the language-acquisition 
component. It is to be expected that language-acquisi- 
tion mechanisms will be subject to various kinds of limi- 
tations just as all other mental mechanisms are. Again, 
however, concrete conclusions must await a detailed 
proposal. 
9 APPENDIX 
This appendix contains the details of a more careful 
reduction of the vertex-cover problem to the UCFG 
recognition problem. This version of the reduction 
establishes that the difficulty of UCFG recognition is not 
due either to the possibility of empty constituents 
(e-rules) or to the possibility of repeated symbols in rules 
(i.e., to the use of multisets rather than sets). Conse- 
quently, it is somewhat different from and more complex 
than the one sketched in the text. 
9.1 DEFINING UNORDERED CONTEXT-FREE GRAMMARS 
Definition: An unordered CFG (UCFG) is a quadruple (N, 
E, R, S), where 
(a) N is a finite set of nonterminals. 
(b) Y disjoint from N is a finite, nonempty set of terminal 
symbols. 
(c) R is a nonempty set of rules (,4, a), where A E N and 
a E (N U E)*. The rule (,4, a) may be written as A 
0g. 
(d) S e N is the start symbol. 
Convention: The grammar G and its components N, E, R, 
S need not be explicitly mentioned when clear from 
context. 
Convention: Unless otherwise noted, 
(a) A, A', A~, ... denote elements of N; 
(b) a, a', ai ... denote elements of E; 
(c) X, Y, X ~, Y, X, Y, . . . .  denote elements of N U E; 
(d) o, u, u', ui, ... denote elements of E*; 
(e) a,/3, ~,, 4, ~, denote elements of (NU E)*. 
Definition G = (N, E, R, S) is e-free iff for every (,4, a) 
~R, I~1 ~0.  
Definition: G = (N, E, R, S) is branching iff for some (,4, 
a>cR, lal >1.  
Definition: G = (N, E, R, S) is duplicate-free iff for 
every (,4, a) E R, a = Y1 --- Y and for all id" ~ \[1,n\], Y
= Yj iff i=j. 
Definition: G = (N, E, R, S) is simple iff it is e-free, 
duplicate-free, and branching. 
Note. The notion of a simple UCFG is introduced in 
order to help pin down the source of any computational 
difficulties associated with UCFGs. For example, since 
simple UCFGs are restricted to be duplicate-free, a diffi- 
culty that arises with simple UCFGs cannot result from 
the possibility that a symbol may occur more than once 
on the right-hand side of a rule. 
Definition: 4A~-->4a~P (by r) just in case (for some) r = 
G 
(A', Y1 "" Y) ~ R and for some permutation p of \[1,n\], A 
= A'  and a = Yp<l)... Yp<n). If 4 c E*, also write 
4A4,~lm 4aq~. 
G 
Definition: L(G) = {o E E*: S ~*  o} 
Definition: An n-step derivation of q~ from 4 is a sequence 
(4o . . . . .  4,) such that 40, = 4, 4, = ~P, and for all i ~ \[0, 
n- l \] ,  4i ~ 4i+r If it is also true for all i that 4i => lm 4i+1, 
say that the derivation is leftmost. 
9.2 DEFINING THE COMPUTATIONAL PROBLEMS 
Definition: A possible instance of the problem VERTEX 
COVER is a triple (V,E,k), where (V,E) is a finite graph 
with at least one edge and at least two vertices, k c N, 
and k < I V\[ .17 VERTEX COVER itself consists of all 
possible instances (V,E,k) such that for some 1f ___ V, 
I U I < k and for all edges e c E, at least one endpoint 
of e is in l/ .  (Figure 5 gives an example of a VERTEX 
COVER instance.) 
11) 
W $ X 
6y 
e2 
V = {v,w,x ,y ,z} 
E = {e 1, e 2, eye  4, e 5, e 6, eT} 
with the e i as indicated 
k = 3 
Figure 5. The triple (V,E,k) is an instance of VERTEX 
COVER. The set I / = {v,x,z,'pf;} is a vertex cover of size 
k=3. 
Fact: VERTEX COVER is NP-complete. (Garey and 
Johnson 1979:46) 
Definition: A possible instance of the problem SIMPLE 
UCFG RECOGNITION is a pair (G, a), where G is a 
simple UCFG and o c Z*. SIMPLE UCFG 
Computational Linguistics, Volume 11, Number 4, October-December 1985 213 
G. Edward Barton, Jr. On the Complexity of ID/LP Parsing 
RECOGNITION itself consists of all possible instances (G, 
o) such that o ? L(G). 
Notation: Take \[\[. \[\[ to be any reasonable measure of the 
encoded input length for a computational problem; 
continue to use \[. \[ for set cardinality and strihg length. 
It is reasonable to require that if S is a set, k ? H, and 
I s I > k, then II s \[I > II/~ II, that is, the encoding of 
numbers is better than unary. It is also reasonable to 
require that l\[ ( .... x . . . .  ) II _ II x II. 
9.3 THE UCFG RECOGNITION PROBLEM IS IN NP 
Lemma 9.1: Let (if0 . . . . .  q~) be a shortest leftmost deri- 
vation of ~ from ~0 in a branching e-free UCFG. If k > 
IN I+ I  then Iq,~l > I%1.  
Proof. There exists some sequence of rules (,4 o, %) ... 
(Ak_ 1, a~_ 1) such that for all i ? \[0, k - l \ ] ,  ~ => lm q~i+l by 
(,4 i, %). Since G is e-free, I cPi+l \[ > I <Pi\[ always. 
Case 1. For some i, I%1 > 1. Then I~+11 > Iq~il. 
Hence \[ ~kl < \[ ~0 I. 
Case 2. For every i, I%1 = 1. Then there exist u, 3' 
such that for every i ? \[0, k -2 \ ] ,  there is Ari ? N such 
that ~+1 = u A'~ 3'. Suppose the A'~ are all distinct. Then 
IN\ [  > k - l ,  hence IN I+ I  > k, hence IN I+ I  > 
I N I+ I ,  which is impossible. Hence for some i j  ? \[0, 
k -2 \ ] ,  i < j, Ar~ = A~j. Hence ~+~ = q,j+~, since \[1,1\] has 
only one permutation. Then ~0 . . . . .  ff~, q~j+l . . . . .  q~k) is a 
leftmost derivation of ~ from ~0 and has length less than 
k, which is also impossible. 
Then I~1 > I~01. \ [ \ ]  
Corollary 9.2: If G is a branching e-free UCFG and o ? 
L(G), then o has a leftmost derivation of length at most 
1o1 "mwherem= \ [N \ [+2.  
Proof. Let (q'0 . . . .  , q'k) be a shortest leftmost derivation 
of o f romS.  Supposek  > \ [a \ [  "m. Consider the sub- 
derivations 
(~0, . . . ,  ~m)(~ . . . . . .  ~2m) 
(~'( I o i -,).m . . . .  ' '~lo I'm)" ( '~lo I "  . . . . .  '/'~)" 
Each one except the last has m steps and m > \[N\[  + 1. 
Then by lemma, 
I~ lo l .ml  > I~( lo l -1 ) .m\ [  
> " "> Iq'ml > I,~01 = 1. 
Then 1ol _> l+\ [o l ,wh ich is imposs ib le .  Hencek  _< 
I~'1 "m. \[\] 
Lemma 9.3: H = SIMPLE UCFG RECOGNITION is in 
the computational class ,/F~P. 
Proof. Let G = (N, X, R, S) be a simple UCFG and o ? 
Y.*. Consider the following nondeterministic algorithm 
with input (G, o): 
Step 1. Write down ~0 = S. 
Step 2. Perform the following steps for i f rom 0 to \[ a \[ " 
m- l ,  where m = \] NI  +2. 
(a) Express ~i as ur4i7 ~ by finding the leftmost nontermi- 
hal, or loop if impossible. 
(b) Guess a rule (A~, Yt,l -.- Y,,k i) ? R and a permutation 
p~ of \[ 1,k~\], or loop if there is no such rule. 
(c) Write down ~i+1 = u~ Y~, pi(1) " ' "  Y/, Pi(/q) 3`i" 
(d) If t~i+l ---~ O then halt. 
Step 3. Loop. 
It should be apparent hat the algorithm runs in time at 
worst polynomial in l\[ (G, o)II; note that the length of ~ 
increases by at most a constant amount on each iteration. 
Assume (G, o) ? H. Then o has a leftmost derivation of 
length at most 1o1 " m by Corol lary 9.2; hence the  
nondeterministic algorithm will be able to guess it and 
will halt. Conversely, suppose the algorithm halts on 
input (G, o). On the iteration when the algorithm halts, 
the sequence (~0, . . . ,  ff~+l) will constitute a leftmost deri- 
vation of o from S; hence o ? L(G) and ( G, o) ? H. 
Then there is a nondeterministic algorithm that runs in 
polynomial time and accepts exactly H. Hence 
H ? JF@.D 
9.4 THE UCFG RECOGNITION PROBLEM IS NP-COMPLETE 
Lemma 9.4: Let (V,E,k) = (V,{ei} , k) be a possible 
instance of VERTEX COVER. Then it is possible to 
construct, in time polynomial in II VII, UEII, and k, a 
simple UCFG G(V,E,k) and a string o(V,E,k) such that 
(G(V,E,k), o(V,E,k)) ~ SIMPLE UCFG RECOGNITION 
iff (V,E,k) ? VERTEX COVER. 
Proof. Construct G(V,E,k) as follows. Let the set N of 
nonterminals consist of the following symbols not in V: 
START, U, D, 
for i ?  \[1, IE I  1, 
U~ for i e \[1, I V I-k\],  
D, for i ?  \[1, \ [E l  " (k - l ) \ ] .  
II Nil will be at worst polynomial in IIE II, II vii, and k for 
a reasonable length measure. Define the terminal vocab- 
ulary Y to consist of subscripted symbols as follows: 
Z ={a i 'a ?  V,i ?  \[1, \ [E \ [ \ ]} .  
Designate START as the start symbol. Include the 
following as members of the rule set R: 
214 Computational Linguistics, Volume 11, Number 4, October-December 1985 
G. Edward Barton, Jr. On the Complexity of ID /LP  Parsing 
(a) Include the rule 
START -~ 111 ... Hlel Ut ... Ui vl-kDt 
? " DIEI "(k-l)" 
(b) For each ei ? E, include the rules 
{/-/~ -~ a~ : a an endpoint of e~}. 
(c) For each i ?  \[1, I VI -k \ ] ,  include the rule U~ -~ U. 
Also include the rules 
{U~ a a . . .a le  I 
(d) For each i E \[1, 
D. Also include 
:aE  V}. 
I E I ? (k- I) \] ,  include the rule D~ -~ 
the rules 
{D-~a:aE  Y~}. 
Take G(V,E,k) to be (N, E, R, START). (Figure 6 shows 
the results of applying this construction to the VERTEX 
COVER instance of Figure 5.) 
Let h : \[1, I V I \] -~ V be some standard enumeration of 
the elements of V. Construct o(V,E,k) as h(1) t ... 
h(1)lE I ... h( I  VI)1 ... h(J VI) IEI thus o(V,E,k) will 
have length I E I  " I V I. 
It is easy to see that II (G(V,E,k), o(V,E,k))II will 'be at 
worst polynomial in II Ell, II vii, and k for reasonable 
II" II. It will also be possible to construct the grammar and 
string in polynomial time. Finally, note that given the 
definition of a possible instance of VERTEX COVER, the 
grammar will be branching, e-free, and duplicate-free, 
hence simple. 
Now suppose (V,E,k) ? VERTEX COVER. Then there 
exist 1/ ~ V and f : E -~ 1 /such  that I I / I  < k and for 
every e ? E, f(e) is an endpoint of e. E is nonempty by 
hypothesis and I /  must hit every edge, hence \ [ l / \ [  
cannot be zero. Construct a parse tree for o(V,E,k) 
according to G(V,E,k) as follows. 
Step 1. Number the elements of V -  1/ as {x~ : i ? \[1, 
I V -U  I\]}. For each x i where i < I V l -k ,  construct a 
node dominating the substring (xi) t ... (x i) I EI of o(V,E,k) 
and label it U. Then construct a node dominating only 
the U-node and label it U r Note that the available 
symbols U/ are numbered from 1 to IV I-k, so it is 
impossible to run out of U-symbols. Also, J U \[ < k and 
U- - -V ,  hence I v - l / I  = Iv l -  I~ l  > Iv l -k ,  so 
all of the U-symbols will be used. Finally, note that U -~ 
a t ... ale I is a rule for any a ? S and that U~ -- U is  a 
rule for any U r 
Step 2. For each ei E E, construct a node dominating the 
(unique) occurrence of f(e)~ ~ o(V,E,k) and label it H .  
Step 2 cannot conflict with step 1 because f (e)  ? V t, 
hence f(ea) ? V - V'. Different parts of step 2 cannot 
conflict with each other because each one affects a 
symbol with a different subscript. Also note that f (e)  is 
an endpoint of ei and that//~ -- a~ is a rule for any e~ e E 
and a an endpoint of e r 
Step 3. Number all occurrences of terminals in o(V,E,k) 
that were not attached in step 1 or step 2. For the ith 
such occurrence, construct a node dominating the occur- 
rence and label it D. Then construct another node domi- 
START -~ 
111 
I L -~ 
H 7 -~ 
U1 -" 
U4 -~ 
U -~ 
I 
Da 
D 4 --~ 
D 7 
D10 
O13 --~ 
D 
I 
H1 HE H 3 H4 H 5 1t6 H7 U1 Uz D t D 2 D 3 Dn D 5 D 6 D 7 D 8 D 9 Dto Du D12 D13 DIn 
Vl I wl I-I2 -" v~ l y2 1t3 -" w3 I x3 
wn I z4 115 -" Xs I Y5 H6 -" Y6 I z6 
X 7 I Z 7 
u v~ -~ u v~ ~. v 
U 
V 1 V 2 V 3 V 4 V 5"v 6 117 J W 1 W 2 W 3 W 4 W 5 W 6 W 7 I X lx  2X  3 X 4x  5X  6x  7 
Yl 22 Y3 Y4 -)/5 26 27 I Z 1 Z 2 Z 3 Z 4 Z 5 Z 6 Z 7 
D D 2 ~ D D 3 ~ D 
D D 5 --*. D D 6 ~ D 
D D 8 ~ D D 9 ~ D 
D Dl t  ~ D D12 --~ D 
D D14 ~ D 
vl lv21v31v41vslv61vvl wt lw21w31wnlwslw61wTI 
xl Ix2 ix3 Ix4 Ix5 Ix6 Ix7 I Y, lY2 lY3 lY4 lY5 lY6 lYTI 
I z, I zz\[z3 \[zglz 5 \[z 6\[z 7
Figure 6. The construction of Lemma 9.4 produces this grammar when applied to the VERTEX COVER problem of 
Figure 5. The H-symbols ensure that the solution that is found must hit each of the edges, while the U-symbols ensure 
that enough elements of V remain untouched to satisfy the requirement I l / I  _< k. The D-symbols are dummies that 
absorb excess input symbols. A shorter grammar than this will suffice if the grammar is not required to be duplicate- 
free. 
Computational Linguistics, Volume 11, Number 4, October-December 1985 215 
G. Edward Barton, Jr. On the Complexity of 1D/LP Parsing 
nating the D-node and label it D r Note that the stock of 
D-symbols runs from 1 to (k - l )  I E I .  Exactly 
( I VI -k )  ? I E I  symbols of tr(V,E,k) were accounted for 
in step 1. Also, exactly I E I symbols were accounted for 
in step 2. The length of a(V ,E ,k )  is I v I  ? IE I ,  hence 
exactly 
IVI " IE I  - ( IV I -k ) .  IE I  - IE I  
= IV I "  IE I -  IV I "  IE I  +k .  IE I -  IE I  
=(k - l ) "  IE I  
symbols remain at the beginning of step 3. D --- a is a 
rule for any a in Z; Di -~D is a rule for any D r 
Step 4. Finally, construct a node labeled START that 
dominates all of the/at, ~,  and D~ nodes constructed in 
steps 1, 2, and 3. The rule 
START -~ Ha ... Hie I Ua ... U I vl -kDl ... Dlel.<k_t )
is in the grammar. Note also that nodes labeled H t . . . . .  
Hlel were constructed in step 2, nodes labeled U a . . . .  
UI v l-k were constructed in step 1, and nodes labeled D1, 
? .. DI e l.~k-1) were constructed in step 3. Hence the appli- 
cation of the rule is in accord with the grammar. Then 
o(V ,E ,k )  E L (G) .  (Figure 7 illustrates the application of 
this parse-tree construction procedure to the grammar 
and input string derived-from the VERTEX COVER 
example in Figure 5.) 
Conversely, suppose o(V ,E ,k )  e L (G) .  Then the deriva- 
tion of o(V ,E ,k )  from START must begin with the appli- 
cation of the rule 
START -~ H a ... H Ie  I U a .. .  U I vl-kD1 ... DIEI'<k-I) 
and each H must later be expanded as some subscripted 
terminal g(H) .  Define f(ei) tO be g(H)  without the 
START 
HtH2DID2D3D4D5 Ut DoDTHaDaHsDgD,o U2 DttDt2DxsHiD,4HeH; 
D D D D D  DD U D D D D D U 
/A 
'01 '02 '03 104 '05 '?6 07 Wl~'2'W3W4'WSff)6%07 Xl X2 :Z3 :C4 Z5 Z6 .T7 YlY2Y$~/4YSYeY7 Zl ,Z'2 
I 
I 
! 
i 
Z 3 Z- 4 Z 5 Z6 Z7 
Figure 7. This parse tree shows how the grammar shown in Figure 6 can generate the string o(V,E,k) constructed in 
Lemma 9.4 for the VERTEX COVER problem of Figure 5. The corresponding VERTEX COVER solution l/. = {v,x,z} 
and its intersection with the edges can be read off by noticing which terminals the H-symbols dominate. 
216 Computational Linguistics, Volume 11, Number 4, October-December 1985 
G. Edward Barton, Jr. On the Complexity of ID/LP Parsing 
subscript; then by construct ion of the grammar, f (e )  is 
an endpoint of e i for all e i E E. Define 1/ = {f(e) : e i 
E}; then it is apparent hat V' __q Vand that V' contains at 
least one endpoint of e~ for all e i E E. Also, each U~ for i 
E \[1, I V I -k \ ]  must be expanded as U, then as some 
substring (a) ,  . . .  (ai) le I of o(V,E,k) ,  t8 Since the 
substrings dominated by the /7  and U~ must all be 
disjoint, and since there are only I E I subscripted occur- 
rences of any single symbol from V in o(V,E,k) ,  there 
must be I Vl - k distinct elements of V that are not 
dominated in any of their subscripted versions by any H r 
Then IV -  V'I -> \[VI - k. Since in addit ion V_c I J ,  
\] ~ I _< k. Then (V,E,k) in VERTEX COVER. \ [ \ ]  
Theorem 1: SIMPLE UCFG RECOGNITION is NP- 
complete. 
Proof. SIMPLE UCFG RECOGNITION is in the class ,.,4"~ 
by Lemma 9.3, hence a polynomial-t ime reduction of 
VERTEX COVER to SIMPLE UCFG RECOGNITION is 
sufficient. Let (V,E,k) be a possible instance of VERTEX 
COVER. Let G be G(V,E,k)  and o be (V,E,k)  as 
constructed in Lemma 9.4. Note that G is simple. 
The construction of G and o can, by lemma, be carried 
out at time at worst polynomial  in IIEII, II VII, and k. 
Also by lemma (G,o) c SIMPLE UCFG RECOGNITION 
iff (V,E,k) c VERTEX COVER. k is not polynomial  in 
\[I k II under a reasonable encoding scheme. However,  
IE I  > k, hence IIEll > Ilkll; also II(V,E,k}II > IIEII, 
hence II (V,E,k)II _> k, all by propert ies assumed to hold 
of I1" II. Then G and o can in fact be constructed in time 
at worst polynomial  in II (V,E,k} II. 
Hence the VERTEX COVER problem is polynomial-t ime 
reduced to SIMPLE UCFG RECOGNITION. \ [ \ ]  
REFERENCES 
Barton, G. Edward, Jr. 1984 Toward a Principle-Based Parser. A.I. 
Memo No. 788, M.I.T. Artificial Intelligence Laboratory, 
Cambridge, Massachusetts. 
Berwick, Robert C. 1982 Computational Complexity and Lexical- 
Functional Grammar. American Journal of Computational Linguistics 
8(3-4): 97-109. 
Berwick, Robert C. and Weinberg, Amy S. 1982 Parsing Efficiency, 
Computational Complexity, and the Evaluation of Grammatical 
Theories. Linguistic Inquiry 13(2): 165-191. 
Berwick, Robert C. and Weinberg, Amy S. 1984 The Grammatical 
Basis of Linguistic Performance. M.I.T. Press, Cambridge, Massa- 
chusetts. 
Chomsky, Noam A. 1980. Rules and Representations. Columbia 
University Press, New York, New York. 
Chomsky, Noam A. 1981 Lectures on Government and Binding. Foris 
Publications, Dordrecht, Holland. 
Earley, Jay 1970 An Efficient Context-Free Parsing Algorithm. 
Communications of the ACM 13(2): 94-102. 
Garey, Michael R. and Johnson, David S. 1979 Computers and Intract- 
ability. W. H. Freeman and Co., San Francisco, California. 
Gazdar, Gerald 1981 Unbounded Dependencies and Coordinate 
Structure. Linguistic Inquiry 12(2): 155-184. 
Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey K.; and Sag, Ivan 
1985 Generalized Phrase Structure Grammar. Basil Blackwell, 
Oxford, U.K. 
Guerssel, Mohamed; Hale, Kenneth; Laughren, Mary; Levin. Beth; and 
White Eagle, Josie 1985 A Cross-Linguistic Study of Transitivity 
Alternations. Paper presented atthe Parasession on Causatives and 
Agentivity at the Twenty-First Regional Meeting of the Chicago 
Linguistic Society, April 1985. 
Hopcroft, John E. and Ullman, Jeffrey D. 1979 Introduction to Auto- 
mata Theory, Languages, and Computation. Addison-Wesley, Read- 
ing, Massachusetts. 
Kroch, Anthony S. and Joshi, Aravind K. 1985 The Linguistic Rele- 
vance of Tree Adjoining Grammars. Technical Report No. 
MS-CIS-85-16, Department of Computer and Information Science, 
Moore School, University of Pennsylvania, Philadelphia, Pennsylva- 
nia. 
Levin, Beth; and Rappaport, Malka 1985 The Formation of Adjectival 
Passives. Lexicon Project Working Papers #2, M.I.T. Center for 
Cognitive Science, Cambridge, Massachusetts. 
Miller, George A. and Chomsky, Noam A. 1963 Finitary Models of 
Language Users. In: Luce, R. D.; Bush, R. R.; and Galanter, E., 
Eds., Handbook of Mathematical Psychology, vol. II. John Wiley and 
Sons, New York, New York: 419-492. 
Ristad, Eric S. 1985 GPSG-Recognition is NP-Hard. A.I. Memo No. 
837, M.I.T. Artificial Intelligence Laboratory, Cambridge, Massa- 
chusetts. 
Shieber, Stuart M. 1983 Direct Parsing of ID/LP Grammars. Techni- 
cal Report 291R, SRI International, Menlo Park, California. Also 
appears in Linguistics and Philosophy 7(2). 
Shieber, Stuart M. 1985 Using Restriction to Extend Parsing Algo- 
rithms for Complex Feature-Based Formalisms. ACL-85 confer- 
ence proceedings, pp. 145-152. 
NOTES 
1. This report describes research done at the Artificial Intel- 
ligence Laboratory of the Massachusetts Institute of 
Technology. Support for the Laboratory's artificial intelli- 
gence research as been provided in part by the Advanced 
Research Projects Agency of the Department of Defense 
under Office of Naval Research contract 
N00014-80-C-0505. Partial support for the author's 
graduate studies was provided by the Fannie and John 
Hertz Foundation. 
Useful guidance and commentary during the writing of 
this paper have been provided by Bob Berwick, Michael 
Sipser, and Joyee Friedman. The paper was also 
improved in response to interesting remarks from the 
anonymous referees for Computational Linguistics. The 
author gratefully acknowledges the assistance of Blythe 
Heepe in preparing the fitures. 
2. See Barton (1984) for discussion. 
3. This estimate is from an anonymous referee. 
4. See section 5; it is in fact the best case. 
5. Shieber's representation differs in some ways from the 
representation used here, which was developed independ- 
ently by the author. The differences are generally ines- 
sential, but see note 7. 
6. The states related to the auxiliary start symbol and 
endmarker that are added by some versions of the Earley 
parser have been omitted for simplicity. 
7. In contrast o the representation illustrated here, Shieber's 
representation actually suffers to some extent from the 
same problem. Shieber (1983:10) uses an ordered 
sequence instead of a multiset before the dot; consequent- 
ly, in place of the state involving S ~ {A,B,C} ? {D,E}, 
Shieber would have the 3! = 6 states involving S -*- a ? 
{D,E}, where a ranges over the six permutations of ABC. 
8. Recognition is simpler than parsing because a recognizer 
is not required to recover the structure of an input string, 
but only to decide whether the string is in the language 
Computational Linguistics, Volume 1 I, Number 4, October-December 1985 217 
G. Edward Barton, Jr. On the Complexity of ID/LP Parsing 
generated by the grammar: that is, whether or not there 
exists a parse. 
9. If the vertex cover is smaller than expected, the 
D-symbols will soak up the extra contiguous runs that 
could have been matched by more U-symbols. 
10. Even assuming ~ # ,/F~, it does not follow that the time 
complexity must be exponential, though it seems likely to 
be. There are functions such as n ~?g" that fall between 
polynomials and exponentials. See Hopcroft and Ullman 
(1979:341). 
11. Shieber (1983:15 n. 6) mentions a possible precompila- 
tion step, but it is concerned with the LP relation rather 
than the ID rules. 
12. It is not known whether the worst-case complexity of 
ID/LP parsing is exponential, since more generally it is not 
known for sure that ,~ # ,.,?'~. 
13. The complexity of ID/LP parsing drops as maximum rule 
length drops, but so does the succinctness advantage of an 
ID/LP grammar over a standard CFG. 
14. In the GB-framework of Chomsky (1981), for instance, 
the syntactic expression of unordered 0-grids at the .~ 
level is constrained by the principles of Case theory. In a 
related framework, the limited possibilities for projection 
from "lexical-conceptual structure" to syntactic argument 
structure combine with Case-assignment rules to severely 
restrict the possible configurations (Guerssel et al 1985, 
Levin 1985). See also Berwick's (1982) discussion of 
constraints that could be placed on another grammatical 
formalism - lexicai-functional grammar - to avoid a simi- 
lar intractability result. 
15. Indeed, one may not conclude a priori that all the 
sentences of every language permitted by linguistic theory 
are algorithmically parsable at all (Chomsky 1980). This 
is true for a variety of reasons. Imagine, for instance, that 
linguistic theory allowed the strings ruled out by filters to 
be specified by complex enumerators. Then the strings of 
a language would be defined in part by subtracting of f  an 
r.e. set, which could lead to nonrecursiveness because the 
complement of an r.e. set is not always r.e. But even if 
nonreeursive, the set of strings would be perfectly well-de- 
fined. 
16. It is often anecdotally remarked that languages that allow 
relatively free word order tend to make heavy use of 
inflections. A rich inflectional system can supply parsing 
constraints that make up for the lack of ordering 
constraints; thus the situation we do not find is the 
computationally difficult case of weak constraint. 
17. This formulation differs trivially from the one cited by 
Garey and Johnson. 
18. The grammar would allow the substring (a}i ... (a~)lE I to 
appear in any permutation, but in o(V,E,k) it appears only 
in the indicated order. 
218 Computational Linguistics, Volume l l ,  Number 4, October-December 1985 
