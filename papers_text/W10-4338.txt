Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 217?220,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Cooperative User Models in Statistical Dialog Simulators
Meritxell Gonza?lez1,2, Silvia Quarteroni1, Giuseppe Riccardi1, Sebastian Varges1
1 DISI - University of Trento, Povo (Trento), Italy
2 TALP Center - Technical University of Catalonia, Barcelona, Spain
mgonzalez@lsi.upc.edu, name.lastname@disi.unitn.it
Abstract
Statistical user simulation is a promis-
ing methodology to train and evaluate the
performance of (spoken) dialog systems.
We work with a modular architecture for
data-driven simulation where the ?inten-
tional? component of user simulation in-
cludes a User Model representing user-
specific features. We train a dialog sim-
ulator that combines traits of human be-
havior such as cooperativeness and con-
text with domain-related aspects via the
Expectation-Maximization algorithm. We
show that cooperativeness provides a finer
representation of the dialog context which
directly affects task completion rate.
1 Introduction
Data-driven techniques are a promising approach
to the development of robust (spoken) dialog sys-
tems, particularly when training statistical dialog
managers (Varges et al, 2009). User simulators
have been introduced to cope with the scarcity of
real user conversations and optimize a number of
SDS components (Schatzmann et al, 2006).
In this work, we investigate the combination of
aspects of human behavior with contextual aspects
of conversation in a joint yet modular data-driven
simulation model. For this, we integrate conversa-
tional context representation, centered on a Dialog
Act and a Concept Model, with a User Model rep-
resenting persistent individual features. Our aim
is to evaluate different simulation regimes against
real dialogs to identify any impact of user-specific
features on dialog performance.
In this paper, Section 2 presents our simulator
architecture and Section 3 focuses on our model of
cooperativeness. Our experiments are illustrated
Work partly funded by EU project ADAMACH (022593)
and Spanish project OPENMT-2 (TIN2009-14675-C03).
in Section 4 and conclusions are summarized in
Section 5.
2 Simulator Architecture
Data-driven simulation takes place within the rule-
based version of the ADASearch system (Varges
et al, 2009), which uses a taxonomy of 16 dialog
acts and a dozen concepts to deal with three tasks
related to tourism in Trentino (Italy): Lodging En-
quiry, Lodging Reservation and Event Enquiry.
Simulation in our framework occurs at the in-
tention level, where the simulator and the Dia-
log Manager (DM) exchange actions, i.e. or-
dered sequences of dialog acts and a number of
concept-value pairs. In other words, we repre-
sent the DM action as as = {da0, .., dan}, (s
is for ?System?) where daj is short for a dialog
act defined over zero or more concept-value pairs,
daj(c0(v0), .., cm(vm)).
In response to the DM action as, the different
modules that compose the User Simulator gener-
ate an N -best list of simulated actions Au(as) =
{a0u, .., a
N
u }. The probability of each possible ac-
tion being generated after the DM action as is es-
timated based on the conversation context. Such a
context is represented by a User Model, a Dialog
Act Model, a Concept Model and an Error Model
(Quarteroni et al, 2010). The User Model simu-
lates the behavior of an individual user in terms of
goals and other behavioral features such as coop-
erativeness and tendency to hang up. The Dialog
Act Model generates a distribution of M actions
Au = {a0u, .., a
M
u }. Then, one action a?u is chosen
out of Au. In order to vary the simulation behav-
ior, the choice of the user action a?u is a random
sampling according to the distribution of proba-
bilities therein; making the simulation more realis-
tic. Finally, the Concept Model generates concept
values for a?u; and the Error Model simulates the
noisy ASR-SLU channel by ?distorting? a?u.
These models are derived from the ADASearch
217
dataset, containing 74 spoken human-computer
conversations.
2.1 User Model
The User Model represents user-specific fea-
tures, both transient and persistent. The
transient feature we focus on in this work is
the user?s goal in the dialog (UG), represented
as a task name and the list of concepts and
values required to fulfill it: an example of
UG is {Activity(EventEnquiry), Time day(2),
Time month(may), Event type(fair), Loca-
tion name(Povo)}.
Persistent features included in our model so far
are: patience, silence (no input) and cooperative-
ness. Patience pat is defined as the tendency
to abandon the conversation (hang up event), i.e.
pat = P (HangUp|as). Similarly, NoInput prob-
ability noi is used to account for user behavior in
noisy environments: noi = P (NoInput|as). Fi-
nally, cooperativeness coop is a real value repre-
senting the ratio of concepts mentioned in as that
also appear in a?u (see Section 3).
2.2 Dialog Act Model
We define three Dialog Act (DA) Models: Obedi-
ent (OB), Bigram (BI) and Task-based (TB).
In the Obedient model, total patience and coop-
erativeness are assumed of the user, who will al-
ways respond to each query requiring values for a
set of concepts with an answer concerning exactly
such concepts. Formally, the model responds to a
DM action as with a single user action a?u obtained
by consulting a rule table, having probability 1. In
case a request for clarification is issued by the DM,
this model returns a clarifying answer. Any offer
from the DM to continue the conversation will be
either readily met with a new task request or de-
nied at a fixed probability: Au(as) = {(a?u, 1)}.
In the Bigram model, first defined in (Eckert et
al., 1997), a transition matrix records the frequen-
cies of transition from DM actions to user actions,
including hang up and no input/no match. Given
a DM action as, the model responds with a list of
M user actions and their probabilities estimated
according to action distribution in the real data:
Au(as) = {(a0u, P (a
0
u|as)), .., (a
M
u , P (a
M
u |as))}.
The Task-based model, similarly to the ?goal?
model in (Pietquin, 2004), produces an action dis-
tribution containing only the actions observed in
the dataset of dialogs in the context of a spe-
cific task Tk. The TB model divides the dataset
into one partition for each Tk, then creates a
task-specific bigram model, by computing ? k:
Au(as) = {(a0u, P (a
0
u|as, Tk)), .., (a
M
u , P (a
M
u |as, Tk))}.
As the partition of the dataset reduces the number
of observations, the TB model includes a mech-
anism to back off to the simpler bigram and uni-
gram models.
2.3 Concept & Error Model
The Concept Model takes the action a?u selected
by the DA Model and attaches values and sam-
pled interpretation confidences to its concepts. In
this work, we adopt a Concept Model which as-
signs the corresponding User Goal values for the
required concepts, which makes the user simulated
responses consistent with the user goal.
The Error Model is responsible of simulating
the noisy communication channel between user
and system; as we simulate the error at SLU level,
errors consist of incorrect concept values. We ex-
periment with a data-driven model where the pre-
cision Prc obtained by a concept c in the refer-
ence dataset is used to estimate the frequency with
which an error in the true value v? of c will be in-
troduced: P (c(v)|c(v?)) = 1? Prc (Quarteroni et
al., 2010).
3 Modelling Cooperativeness
As in e.g. (Jung et al, 2009), we define coop-
erativeness at the turn level (coopt) as a function
of the number of dialog acts in the DM action as
sharing concepts with the dialog acts in the user
action au; at the dialog level, coop is the average
of turn-level cooperativeness.
We discretize coop into a binary variable reflect-
ing high vs low cooperativeness based on whether
or not the dialog cooperativeness exceeds the me-
dian value of coop found in a reference corpus; in
our ADASearch dataset, the median value found
for coop is 0.28; hence, we annotate dialogs as co-
operative if they exceed such a threshold, and as
uncooperative otherwise. Using a corpus thresh-
old allows domain- and population-driven tuning
of cooperativeness rather than a ?hard? definition
(as in (Jung et al, 2009)).
We then model cooperativeness as two bigram
models, reflecting the high vs low value of coop.
In practice, given a DM action as and the coop
value (? = high/low) we obtain a list of user ac-
tions and their probabilities:
Au(as, ?) = {(a0u, P (a
0
u|as, ?)), .., (a
M
u , P (a
M
u |as, ?))}.
218
3.1 Combining cooperativeness and context
At this point, the distribution Au(as, ?) is lin-
early interpolated with the distribution of actions
Au(as, ?) obtained using the DA model ? (in the
Task-based DA model; ? can have three values,
one for each task as explained in Section 2.2):
Au(as) = ?? ?Au(as, ?) + ?? ?Au(as, ?),
where ?? and ?? are the weights of each fea-
ture/model and ?? + ?? = 1.
For each user action aiu, ?? and ?? are
estimated using the Baum-Welch Expectation-
Maximization algorithm as proposed by (Jelinek
and Mercer, 1980). We use the distributions of ac-
tions obtained from our dataset and we align the
set of actions of the two models. Since we only
have two models, we only need to calculate ex-
pectation for one of the distributions:
P (?|as, a
i
u) =
P (aiu|as, ?)
P (aiu|as, ?) + P (aiu|as, ?)
?Mi=0a
i
u
where M is the number of actions. Then, the
weights ?? and ?? that maximize the data like-
lihood are calculated as follows:
?? =
?M
j=0 P (?|as, a
j
u)
M
;?? = 1? ??.
The resulting combined distribution Au(as) is
obtained by factoring the probabilities of each ac-
tion with the weight estimated for the particular
distribution:
Au(as) = {(a
0
u, ???P (a
0
u|as, ?)), .., (a
M
u , ???P (a
M
u |as, ?)),
(a0u, ?? ? P (a
0
u|as, ?)), .., (a
M
u , ?? ? P (a
M
u |as, ?))}
3.2 Effects of cooperativeness
To assess the effect of the cooperativeness feature
in the final distribution of actions, we set a 5-fold
cross-validation experiment with the ADASearch
dataset where we average the ?? estimated at each
turn of the dialog. We investigated in which con-
text cooperativeness provides more contribution
by comparing the ?? weights attributed by high
vs. low coop models to user action distributions in
response to Dialog Manager actions.
Figure 1 shows the values achieved by ?? for
several DM actions for high vs low coop regimes.
We can see that ?? achieves high values in case
of uncooperative users in response to DM dialog
acts as [ClarificationRequest] and [Info-request].
In contrast, forward-looking actions, such as the
ones including [Offer], seem to discard the con-
tribution of the low coop model, but to favor the
contribution provided by high coop.
!"
!#$"
!#%"
!#&"
'()*+,-*./012345" '-*./012345" '67189/34:3;<5" '=/33<,>?3/5" '>?3/5" '6718/@,>?3/5"
A0+A" *8B"
Figure 1: Estimated ?? weights in response to se-
lected DM actions in case of high/low coop
4 Experiments
We evaluate our simulator models using two meth-
ods: first, ?offline? statistics are used to assess
how realistic the action estimations by DA Models
are with respect to a dataset of real conversations
(Sec. 4.1); then, ?online? statistics (Sec. 4.2) eval-
uate end-to-end simulator performance in terms of
dialog act distributions, error robustness and task
duration and completion rates by comparing real
dialogs with fresh simulated dialogs using action
sampling in the different simulation models.
4.1 ?Offline? statistics
In order to compare simulated and real user ac-
tions, we evaluate dialog act Precision (PDA)
and Recall (RDA) following the methodology in
(Schatzmann et al, 2005).
For each DM action as the simulator picks a
user action a?u from Au(as) and we compare it
with the real user choice a?u. A simulated dialog
act is correct when it appears in the real action
a?u. The measurements were obtained using 5-fold
cross-validation on the ADASearch dataset.
Table 1: Dialog Act Precision and Recall
Simulation (a?u) Most frequent (a
?
u)
DA Model PDA RDA PDA RDA
OB 33.8 33.4 33.9 33.5
BI (+coop) 35.6 (35.7) 35.5 (35.8) 49.3 (47.9) 48.8 (47.4)
TB (+coop) 38.2 (39.7) 38.1 (39.4) 51.1 (50.6) 50.6 (50.2)
Table 1 shows PDA/RDA obtained for the OB,
BI and TB models alone and with cooperative-
ness models (+coop). First, we see that TB is
much better than BI and OB at reproducing real
action selection. This is also visible in both PDA
and RDA obtained by selecting a?u, the most fre-
quent user action from the As generated by each
model. By definition, a?u maximizes the expected
PDA and RDA, providing an upper bound for our
models; however, to reproduce any possible user
behavior, we need to sample au rather than choos-
ing it by frequency. By now inspecting (+coop)
219
values in Table 1, we see that explicit cooperative-
ness models match real dialogs more closely. It
points out that partitioning the reference dataset in
high vs low coop sets allows better data represen-
tation. There is however no improvement in the
a?u case: we explain this by the fact that by ?slic-
ing? the reference dataset, the cooperative model
augments data sparsity, affecting robustness.
4.2 ?Online? statistics
We now discuss online deployment of our sim-
ulation models with different user behaviors and
?fresh? user goals and data. To align with the
ADASearch dataset, we ran 60 simulated dialogs
between the ADASearch DM and each combina-
tion of the Task-based and Bigram models and
high and low values of coop. For each set of simu-
lated dialogs, we measured task duration, defined
as the average number of turns needed to complete
each task, and task completion rate, defined as:
TCR = number of times a task has been completedtotal number of task requests .
Table 2 reports such figures in comparison
to the ones obtained for real dialogs from the
ADASearch dataset. In general, we see that task
duration is closer to real dialogs in the Bigram and
Task-based models when compared to the Obedi-
ent model. Moreover, it can easily be observed
in both BI and TB models that under high-coop
regime (in boldface), the number of turns taken
to complete tasks is lower than under low-coop.
Furthermore, in both TB and BI models, TCR
is higher when cooperativeness is higher, indicat-
ing that cooperative users make dialogs not only
shorter but also more efficient.
Table 2: Task duration and TCR in simulated di-
alogs with different regimes vs real dialogs.
Lodging Enquiry Lodging Reserv Event Enquiry All
Model #turns TCR #turns TCR #turns TCR TCR
OB 9.2?0.0 78.1 9.7?1.4 82.4 8.1?2.9 66.7 76.6
BI+low 15.1?4.1 71.4 14.2?3.9 69.4 9.3?1.8 52.2 66.7
BI+high 12.1?2.5 74.6 12.9?3.1 82.9 7.8?1.8 75.0 77.4
TB+low 13.6?4.1 75.8 13.4?3.7 83.3 8.4?3.3 64.7 77.2
TB+high 11.6?2.8 80.0 12.6?3.6 83.7 6.5?1.9 57.1 78.4
Real dialogs 11.1?3.0 71.4 12.7?4.7 69.6 9.3?4.0 85.0 73.4
5 Conclusion
In this work, we address data-driven dialog sim-
ulation for the training of statistical dialog man-
agers. Our simulator supports a modular combina-
tion of user-specific features with different models
of dialog act and concept-value estimation, in ad-
dition to ASR/SLU error simulation.
We investigate the effect of joining a model of
user intentions (Dialog Act Model) with a model
of individual user traits (User Model). In partic-
ular, we represent the user?s cooperativeness as
a real-valued feature of the User Model and cre-
ate two separate simulator behaviors, reproducing
high and low cooperativeness. We explore the im-
pact of combining our cooperativeness model with
the Dialog Act model in terms of dialog act accu-
racy and task success.
We find that 1) an explicit modelling of user
cooperativeness contributes to an improved accu-
racy of dialog act estimation when compared to
real conversations; 2) simulated dialogs with high
cooperativeness result in higher task completion
rates than low-cooperativeness dialogs. In future
work, we will study yet more fine-grained and re-
alistic User Model features.
References
W. Eckert, E. Levin, and R. Pieraccini. 1997. User
modeling for spoken dialogue system evaluation. In
Proc. IEEE ASRU.
F. Jelinek and R. L. Mercer. 1980. Interpolated estima-
tion of Markov source parameters from sparse data.
In Workshop on Pattern Recognition in Practice.
S. Jung, C. Lee, K. Kim, and G. G. Lee. 2009. Hy-
brid approach to user intention modeling for dialog
simulation. In Proc. ACL-IJCNLP.
O. Pietquin. 2004. A Framework for Unsupervised
Learning of Dialogue Strategies. Ph.D. thesis, Fac-
ulte? Polytechnique de Mons, TCTS Lab (Belgique).
S. Quarteroni, M. Gonza?lez, G. Riccardi, and
S. Varges. 2010. Combining user intention and error
modeling for statistical dialog simulators. In Proc.
INTERSPEECH.
J. Schatzmann, K. Georgila, and S. Young. 2005.
Quantitative evaluation of user simulation tech-
niques for spoken dialogue systems. In Proc. SIG-
DIAL.
J. Schatzmann, K. Weilhammer, M. Stuttle, and
S. Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of di-
alogue management strategies. Knowl. Eng. Rev.,
21(2):97?126.
S. Varges, S. Quarteroni, G. Riccardi, A. V. Ivanov, and
P. Roberti. 2009. Leveraging POMDPs trained with
user simulations and rule-based dialogue manage-
ment in a spoken dialogue system. In Proc. SIG-
DIAL.
220
