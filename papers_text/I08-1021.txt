Modeling Context in Scenario Template Creation
Long Qiu, Min-Yen Kan, Tat-Seng Chua
Department of Computer Science
National University of Singapore
Singapore, 117590
{qiul,kanmy,chuats}@comp.nus.edu.sg
Abstract
We describe a graph-based approach to Sce-
nario Template Creation, which is the task
of creating a representation of multiple re-
lated events, such as reports of different hur-
ricane incidents. We argue that context is
valuable to identify important, semantically
similar text spans from which template slots
could be generalized. To leverage context,
we represent the input as a set of graphs
where predicate-argument tuples are ver-
tices and their contextual relations are edges.
A context-sensitive clustering framework is
then applied to obtain meaningful tuple clus-
ters by examining their intrinsic and extrin-
sic similarities. The clustering framework
uses Expectation Maximization to guide the
clustering process. Experiments show that:
1) our approach generates high quality clus-
ters, and 2) information extracted from the
clusters is adequate to build high coverage
templates.
1 Introduction
Scenario template creation (STC) is the problem of
generating a common semantic representation from
a set of input articles. For example, given multiple
newswire articles on different hurricane incidents,
an STC algorithm creates a template that may in-
clude slots for the storm?s name, current location, di-
rection of travel and magnitude. Slots in such a sce-
nario template are often to be filled by salient entities
in the scenario instance (e.g., ?Hurricane Charley?
or ?the coast area?) but some can also be filled by
prominent clauses, verbs or adjectives that describe
these salient entities. Here, we use the term salient
aspect (SA) to refer to any of such slot fillers that
people would regard as important to describe a par-
ticular scenario. Figure 1 shows such a manually-
built scenario template in which details about im-
portant actions, actors, time and locations are coded
as slots.
STC is an important task that has tangible bene-
fits for many downstream applications. In the Mes-
sage Understanding Conference (MUC), manually-
generated STs were provided to guide Information
Extraction (IE). An ST can also be viewed as reg-
ularizing a set of similar articles as a set of at-
tribute/value tuples, enabling multi-document sum-
marization from filled templates.
Despite these benefits, STC has not received
much attention by the community. We believe this
is because it is considered a difficult task that re-
quires deep NL understanding of the source articles.
A problem in applications requiring semantic simi-
larity is that the same word in different contexts may
have different senses and play different roles. Con-
versely, different words in similar contexts may play
similar roles. This problem makes approaches that
rely on word similarity alone inadequate.
We propose a new approach to STC that incor-
porates the use of contextual information to address
this challenge. Unlike previous approaches that con-
centrate on the intrinsic similarity of candidate slot
fillers, our approach explicitly models contextual ev-
idence. And unlike approaches to word sense disam-
biguation (WSD) and other semantic analyses that
157
use neighboring or syntactically related words as
contextual evidence, we define contexts by semantic
relatedness which extends beyond sentence bound-
aries. Figure 2 illustrates a case in point with two
excerpts from severe storm reports. Here, although
the intrinsic similarity of the main verbs ?hit? and
?land? is low, their contextual similarity is high as
both are followed by clauses sharing similar subjects
(hurricanes) and the same verbs. Our approach en-
codes such contextual information as graphs, map-
ping the STC problem into a general graph overlay
problem that is solvable by a variant of Expectation
Maximization (EM).
Our work also contributes resources for STC re-
search. Until now, few scenario templates have been
publicly available (as part of MUC), rendering any
potential evaluation of automated STC statistically
insignificant. As part of our study, we have com-
piled a set of input articles with annotations that we
are making available to the research community.
Scenario Template: Storm
Storm Name Charley
Storm Action landed
Location Florida?s Gulf coast
Time Friday at 1950GMT
Speed 145 mph
Victim Category 1 13 people
Action died
Victim Category 2 over one million
Action affected
Figure 1: An example scenario template (filled).
2 Related Work
A natural way to automate the process of STC is to
cluster similar text spans in the input article set. SAs
then emerge through clustering; if a cluster of text
spans is large enough, the aspects contained in it will
be considered as SAs. Subsequently, these SAs will
be generalized into one or more slots in the template,
depending on the definition of the text span. As-
suming scenarios are mainly defined by actions, the
focus should be on finding appropriate clusters for
text spans each of which represents an action. Most
of the related work (although they may not directly
address STC) shares this assumption and performs
Charley landed further south on Florida?s
Gulf coast than predicted, ... The hurricane
... has weakened and is moving over South
Carolina.
At least 21 others are missing after the storm
hit on Wednesday. .... But Tokage had
weakened by the time it passed over Japan?s
capital, Tokyo, where it left little damage be-
fore moving out to sea.
Figure 2: Contextual evidence of similarity. Curved
lines indicate similar contexts, providing evidence
that ?land? and ?hit? from two articles are semanti-
cally similar.
action clustering accordingly. While the target ap-
plication varies, most systems that need to group text
spans by similarity measures are verb-centric.
In addition to the verb, many systems expand
their representation by including named entity tags
(Collier, 1998; Yangarber et al, 2000; Sudo et al,
2003; Filatova et al, 2006), as well as restrict-
ing matches (using constraints on subtrees (Sudo et
al., 2003; Filatova et al, 2006), predicate argument
structures (Collier, 1998; Riloff and Schmelzen-
bach, 1998; Yangarber et al, 2000; Harabagiu and
Maiorano, 2002) or semantic roles).
Given these representations, systems then cluster
similar text spans. To our knowledge, all current
systems use a binary notion of similarity, in which
pairs of spans are either similar or not. How they de-
termine similarity is tightly coupled with their text
span representation. One criterion used is pattern
overlap: for example, (Collier, 1998; Harabagiu and
Lacatusu, 2005) judge text spans to be similar if they
have similar verbs and share the same verb argu-
ments. Working with tree structures, Sudo et al and
Filatova et al instead require shared subtrees.
Calculating text span similarity ultimately boils
down to calculating word phrase similarity. Ap-
proaches such as Yangarber?s or Riloff and
Schmelzenbach?s do not employ a thesaurus and
thus are easier to implement, but can suffer from
over- or under-generalization. In certain cases, ei-
ther the same actor is involved in different actions or
different verbs realize the same action. Other sys-
tems (Collier, 1998; Sudo et al, 2003) do employ
158
lexical similarity but threshold it to obtain binary
judgments. Systems then rank clusters by cluster
size and correlation with the relevant article set and
equate top clusters as output scenario slots.
3 Context-Sensitive Clustering (CSC)
Automating STC requires handling a larger degree
of variations than most previous work we have sur-
veyed. Note that the actors involved in actions in a
scenario generally differ from event to event, which
makes most related work on text span similarity cal-
culation unsuitable. Also, action participants are not
limited to named entities, so our approach needs to
process all NPs. As both actions and actors may be
realized using different words, a similarity thesaurus
is necessary. Our approach to STC uses a thesaurus
based on corpus statistics (Lin, 1998) for real-valued
similarity calculation. In contrast to previous ap-
proaches, we do not threshold word similarity re-
sults; we retain their fractional values and incorpo-
rate these values holistically. Finally, as the same
action can be realized in different constructions, the
semantic (not just syntactic) roles of verb arguments
must be considered, lest agent and patient roles be
confused. For these reasons, we use a semantic role
labeler (Pradhan et al, 2004) to provide and delimit
the text spans that contain the semantic arguments
of a predicate. We term the obtained text spans as
predicate argument tuples (tuples) throughout the
paper. The semantic role labeler reportedly achieves
an F 1 measure equal to 68.7% on identification-classification of predicates and core arguments on a
newswire text corpus (LDC, 2002). Within the con-
fines of our study, we find it is able to capture most
of the tuples of interest.
Our approach explicitly captures contextual ev-
idence. We define a tuple?s contexts as other tu-
ples in the same article segment where no topic shift
occurs. This definition refines the n-surrounding
word constraint commonly used in spelling correc-
tion (for example, (Hirst and Budanitsky, 2005)),
Word Sense Disambiguation ((Preiss, 2001), (Lee
and Ng, 2002), for instance), etc. while still en-
sures the relatedness between a tuple and its con-
texts. Specifically, a tuple is contextually related to
other tuples by two quantifiable contextual relations:
argument-similarity and position-similarity. For our
experiments, we use the leads of newswire articles
as they normally summarize the news. We also as-
sume a lead qualifies as a single article segment, thus
making all of its tuples as potential contexts to each
other.
from A2
from A1
weakened(storm)
v21
hit(storm)
v22
moving(storm)
v23
weakened(hurricane)
v11
landed(hurricane)
v12
moving(hurricane)
v13
e21,2
e22,1 e21,3
e23,1
e22,3
e23,2
e11,2
e12,1 e11,3
e13,1
e12,3
e13,2
Figure 3: Being similar contexts, ?weakened? and
?moving? provide contextual evidence that ?land?
and ?hit? are similar.
First, we split the input article leads into sentences
and perform semantic role labeling immediately af-
terwards. Our system could potentially benefit from
additional pre-processing such as co-reference reso-
lution. Currently these pre-processing steps have not
been properly integrated with the rest of the system,
and thus we have not yet measured their impact.
We then transform each lead Ai into a graph Gi =
{V i, Ei}. As shown in Figure 3, vertices V i =
{vij}(j = 1, ..., N) are the N predicate argumenttuples extracted from the ith article, and directed
edges Ei = {eim,n = (vim, vin)} reflect contextualrelations between tuple vim and vin. Edges only con-nect tuples from the same article, i.e., within each
graph Gi. We differentiate between two types of
edges. One is argument-similarity, where the two
tuples have semantically similar arguments. This
models tuple cohesiveness, where the edge weight is
determined by the similarity score of the most sim-
ilar inter-tuple argument pair. The other is position-
similarity, represented as the offset of the ending tu-
ple with respect to the other, measured in sentences.
This edge type is directional to account for simple
causality.
Given this set of graphs, the clustering task is to
find an optimal alignment of all graphs (i.e., super-
imposing the set of article graphs to maximize vertex
overlap, constrained by the edges). We adapt Expec-
tation Maximization (Dempster et al, 1977) to find
159
an optimal clustering. This process assigns tuples to
suitable clusters where they are semantically similar
and share similar contexts with other tuples. Algo-
rithm 1 outlines this alignment process.
Algorithm 1 Graph Alignment(G)
/*G is a set of graph {Gi}*/
T ? all tuples in G
C ? highly cohesive tuples clusters
other? remaining tuples semantically connected with C
C[C.length]? otherrepeat
/*E step*/for each i such that i < C.length dofor each j such that j < C.length doif i == j then
continue;
re-estimate parameters[C[i],C[j]] /*distribution
parameters of edges between two clusters*/
tupleReassigned = false /*reset*/
/*M step*/for each i such that i < T.length do
aBestLikelihood = T [i].likelihood; /*likelihood of
being in its current cluster*/for each tuple tcontxt that contextually related with
T [i] dofor each cluster ccand, any candidate cluster thatcontextually related with tcontxt.cluster do
P (T [i] ? ccand) = comb(Ps, Pc)
likelihood = log(P (T [i] ? ccand))if likelihood > aBestLikelihood then
aBestLikelihood = likelihood
T [i].cluster = ccand
tupleReassigned = trueuntil tupleReassigned == false /*alignment stable*/return
During initialization, tuples whose pairwise simi-
larity higher than a threshold ? are merged to form
highly cohesive seed clusters. To compute a con-
tinuous similarity Sim(ta, tb) of tuples ta and tb,we use the similarity measure described in (Qiu et
al., 2006), which linearly combines similarities be-
tween the semantic roles shared by the two tuples.
Some other tuples are related to these seed clus-
ters by argument-similarity. These related tuples are
temporarily put into a special ?other? cluster. The
cluster membership of these related tuples, together
with those currently in the seed clusters, are to be
further adjusted. The ?other? cluster is so called be-
cause a tuple will end up being assigned to it if it
is not found to be similar to any other tuple. Tuples
that are neither similar to nor contextually related by
argument-similarity to another tuple are termed sin-
gletons and excluded from being clustered.
We then iteratively (re-)estimate clusters of tuples
across the set of article graphs G. In the E-step of the
EM algorithm, all contextual relations between each
pair of clusters are collected as two set of edges.
Here we assume argument-similarity and position-
similarity are independent and thus we differenti-
ate them in the computation. Accordingly, there
are two sets: edgesas and edgesps. For simplicity,we assume independent normal distributions for the
strength of each set (inter-tuple argument similarity
for edgesas and sentence distance for edgesps). Theedge strength distribution parameters for both sets
between each pair of clusters are re-estimated based
on current edges in edgesas and edgesps.In the M-step, we examine each tuple?s fitness for
belonging to its cluster and relocate some tuples to
new clusters to maximize the likelihood given the
latest estimated edge strength distributions. In the
following equations, we denote the proposition that
predicate argument tuple ta belongs to cluster cm as
ta?cm; a typical tuple (the centroid) of the cluster
cm as tcm ; and the cluster of ta as cta . The objectivefunction to maximize is:
Obj(G) =
X
ta?G
log(P (ta?cta)), (1)
where P (ta?cm) = 2Ps(ta?cm) Pc(ta?cm)Ps(ta?cm) + Pc(ta?cm) . (2)
Equation 2 takes the harmonic mean of two factors:
a contextual factor Pc and and a semantic factor Ps:
Pc(ta?cm) = max{P (edges(ta, tb)|
tb:edges(ta,tb)6=null
edges(cm, ctb ))}, (3)
Ps(ta?cm) =
(
simdefault, cm = cother,
Sim(ta, tcm), otherwise. (4)
Here the contextual factor Pc models how likely
ta belongs to cm according to the contextual infor-mation, i.e., the conditional probability of the con-
textual relations between cm and ctb given the con-textual relations between ta and one particular con-text tb, which maximizes this probability. Accord-ing to Bayes? theorem, it is computed as shown in
Equation 3. In practice, we multiply two conditional
probabilities: P (edgeas(ta, tb)|edgesas(cm, ctb))and P (edgeps(ta, tb)|edgesps(cm, ctb)), assumingindependence between edgesas and edgesps.We assume there are still singleton tuples that are
not semantically similar to another tuple and should
belong to the special ?other? cluster. Given that they
160
are dissimilar to each other, we set simdefault toa small nonzero value in Equation 4 to prevent the
?other? cluster from expelling them based on their
low semantic similarity. Tuples? cluster member-
ships are recalculated, and the parameters describ-
ing the contextual relations between clusters are re-
estimated. New EM iterations are performed as long
as one or more tuple relocations occur. Once the
EM halts, clusters of equivalent tuples are formed.
Among these clusters, some correspond to salient
actions that, together with their actors, are all SAs
to be generalized into template slots. Cluster size
is a good indicator of salience, and each large clus-
ter (excluding the ?other? cluster) can be viewed as
containing instances of a salient action.
Formulating the clustering process as a variant of
iterative EM is well-motivated as we consider the
similarity scores as noisy and having missing obser-
vations. Calculating semantic similarity is at best
inaccurate. Thus it is difficult to cluster tuples cor-
rectly based only on their semantic similarity. Also
to check whether a tuple shares contexts with a clus-
ter of tuples, the cluster has to be relatively clean.
An iterative EM as we have proposed naturally im-
prove the cleanness of these tuple clusters gradually
as new similarity information comes to light.
4 Evaluation
For STC, we argue that it is crucial to cluster tuples
with high recall so that an SA?s various surface
forms can be captured and the size of clusters can
serve as a salience indicator. Meanwhile, precision
should not be sacrificed, as more noise will hamper
the downstream generalization process which
outputs template slots. We conduct experiments
designed to answer two relevant research questions:
1) Cluster Quality: Whether using contexts (in
CSC) produces better clustering results than ignor-
ing it (in the K-means baseline); and
2) Template Coverage: Whether slots generalized
from CSC clusters cover human-defined templates.
4.1 Data Set and Baseline
A straightforward evaluation of a STC system would
compare its output against manually-prepared gold
standard templates, such as those found in MUC.
Unfortunately, such scenario templates are severely
limited and do not provide enough instances for a
proper evaluation. To overcome this problem, we
have prepared a balanced news corpus, where we
have manually selected articles covering 15 scenar-
ios. Each scenario is represented by a total of 45 to
50 articles which describe 10 different events.
Our baseline is a standard K-means clusterer. Its
input is identical to that of CSC ? the tuples ex-
tracted from relevant news articles and are not ex-
cluded from being clustered by CSC in the initial-
ization stage (refer to Section 3) ? and employs the
same tuple similarity measure (Qiu et al, 2006). The
differentiating factor between CSC and K-means is
the use of contextual evidence. A standard K-means
clusterer requires a k to be specified. For each sce-
nario, we set its k as the number of clusters gener-
ated by CSC for direct comparison.
We fix the test set for each scenario as ten ran-
domly selected news articles, each reporting a dif-
ferent instance of the scenario; the development set
(which also serves as the training set for determin-
ing the EM initialization threshold ? and simdefaultin Equation 4) is a set of ten articles from the ?Air-
linerCrash? scenario, which are excluded from the
test set. Both systems analyze the first 15 sentences
of each article, and sentences generate 2 to 3 predi-
cate argument tuples on average, resulting in a total
of 10 ? 15 ? (2 to 3) = 300 to 450 tuples for each
scenario.
4.2 Cluster Quality
This experiment compares the clustering results of
CSC and K-means. We use the standard cluster-
ing metrics of purity and inverse purity (Hotho et
al., 2003). The first author manually constructed the
gold standard clusters for each scenario using a GUI
before conducting any experiments. A special clus-
ter, corresponding to the ?other? cluster in the CSC
clusters, was created to hold the singleton tuples for
each scenario. Table 1 shows this under the column
?#Gold Standard Clusters?.
Using the manual clusters as the gold standard, we
obtain the purity (P) and inverse purity (IP) scores
of CSC and K-means on each scenario. In Table 1,
we see that CSC outperforms K-means on 10 of 15
scenarios for both P and IP. For the remaining 5 sce-
narios, where CSC and K-means have comparable
161
P scores, the IP scores of CSC are all significantly
higher than that of K-means. This suggests clus-
ters tend to be split apart more in K-means than in
CSC when they have similar purity. One thing worth
mentioning here is that the ?other? cluster normally
is relatively large for each scenario, and thus may
skew the results. To remove this effect, we excluded
tuples belonging to the CSC ?other? cluster from the
K-means input, generating one fewer cluster. Run-
ning the evaluation again, the resulting P-IP scores
again show that CSC outperforms the baseline K-
means. We only report the results for all tuples in
our paper for simplicity.
#Gold Std. CSC K-meansScenario Clusters P IP P IP
AirlinerCrash 23 .61 .42 .52 .28
Earthquake 18 .60 .44 .53 .30
Election 10 .77 .49 .75 .21
Fire 14 .65 .44 .64 .26
LaunchEvent 12 .77 .37 .73 .22
Layoff 10 .71 .28 .70 .19
LegalCase 8 .75 .37 .75 .18
Nobel 6 .77 .28 .77 .19
Obituary 7 .85 .46 .81 .28
RoadAccident 20 .61 .49 .56 .40
SoccerFinal 5 .88 .39 .88 .15
Storm 14 .61 .31 .61 .22
Tennis 6 .87 .19 .87 .12
TerroristAttack 14 .64 .48 .62 .25
Volcano 16 .68 .38 .66 .17Average 12.2 .72 .39 .69 .23
Table 1: CSC outperforms K-means with respect to
the purity (P) and inverse purity (IP) scores.
A close inspection of the results reveals some
problematic cases. One issue worth mentioning is
that for certain actions both CSC and K-means pro-
duce split clusters. In the CSC case, we traced this
problem back to the thesaurus, where predicates for
one action seem to belong to two or more totally dis-
similar semantic categories. The corresponding tu-
ples are thus assigned to different clusters as their
low semantic similarity forces the tuples to remain
separate, despite the shared contexts trying to join
them. One example is ?blast (off)? and ?lift (off)? in
the ?Launch Event? scenario. The thesaurus shows
the two verbs are dissimilar and the corresponding
tuples end up being in two split clusters. This can
not be solved easily without an improved thesaurus.
We are considering adding a prior to model the op-
timal size for clusters, which may help to compact
such cases.
4.3 Template Coverage
We also assess how well the resulting, CSC-
generated tuple clusters serve in creating good sce-
nario template slots. We start from the top largest
clusters from each scenario, and decompose each
of them into six sets: the predicates, agents, pa-
tients, predicate modiers, agent modiers and pa-
tient modiers. For each of the first three sets for
each cluster, we create a generalized term to repre-
sent it using an extended version of a generaliza-
tion algorithm (Tseng et al, 2006). These terms
are deemed output slots, and are put into the tem-
plate with their agent-predicate-patient relations pre-
served. The size of the template may increase when
more clusters are generalized, as new slots may re-
sult.
We manually compare the slots that are output
from the system with those defined in existing sce-
nario templates in MUC. The results here are only
indicative and not conclusive, as there are only two
MUC7 templates available for comparison: Aviation
Disaster and Launch Event.
Template semantic role general term
action crash
cluster 1 agent aircraft
patient ?
action kill
cluster 2 agent heavier-than-
air-craft
patient people
Figure 4: Automated scenario template of ?Avia-
tionDisaster?.
Figure 4 shows an excerpt of the automatically
generated template ?AviationDisaster? (?Airliner-
Crash? in our corpus) where the semantic roles in
the top two biggest clusters have been generalized.
Their modifiers are quite semantically diverse, as
shown in Table 2. Thus, generalization (probably
after a categorization operation) remains as a chal-
lenging problem.
Nonetheless, the information contained in these
semantic roles and their modifiers covers human-
162
semantic role modifier head samples
agent:aircraft A, U.N., The, Swiss, Canadian-
built, AN, China, CRJ-200, mil-
itary, Iranian, Air, refueling, US,
...
action:crash Siberia, mountain, rain, Tues-
day, flight, Sharjah, flames, Sun-
day, board, Saturday, 225, Rock-
away, approach, United, moun-
tain, hillside
patient:people all, 255, 71
Table 2: Sample automatically detected modifier
heads of different semantic roles.
AviationDisaster LaunchEvent
* AIRCRAFT * VEHICLE
* AIRLINE * VEHICLE TYPE
DEPARTURE POINT * VEHICLE OWNER
DEPARTURE DATE * PAYLOAD
* AIRCRAFT TYPE PAYLOAD TYPE
* CRASH DATE PAYLOAD FUNC
* CRASH SITE * PAYLOAD OWNER
CAUSE INFO PAYLOAD ORIGIN
* VICTIMS NUM * LAUNCH DATE
* LAUNCH SITE
MISSION TYPE
MISSION FUNCTION
MISSION STATUS
Figure 5: MUC-7 template coverage: asterisks
marking all the slots that could be automatically
generated.
defined scenario templates quite well. The two
MUC7 templates are shown as a list of slots in Fig-
ure 5, where horizontal lines delimit slots about dif-
ferent semantic roles, and asterisks mark all the slots
that could be automatically generated by our system
once it has an improved generalizer. We can see
substantial amount of overlap, indicating that a STC
system powered by CSC is able to capture scenarios?
important facts.
5 Conclusion
We have introduced a new context-sensitive ap-
proach to the scenario template creation (STC) prob-
lem. Our method leverages deep NL processing, us-
ing semantic role labeler?s structured semantic tu-
ples as input. Despite the use of deeper semantics,
we believe that intrinsic semantic similarity by itself
is not sufficient for clustering. We have shown this
through examples and argue that an approach that
considers contextual similarity is necessary. A key
aspect of our work is the incorporation of such con-
textual information. Our approach uses a notion of
context that combines two aspects: positional simi-
larity (when two tuples are adjacent in the text), and
argument similarity (when they have similar argu-
ments). The set of relevant articles are represented
as graphs where contextual evidence is encoded.
By mapping our problem into a graphical formal-
ism, we cast the STC clustering problem as one of
multiple graph alignment. Such a graph alignment is
solved by an adaptation of EM, which handles con-
texts and real-valued similarity by treating both as
noisy and potentially unreliable observations.
While scenario template creation (STC) is a dif-
ficult problem, its evaluation is arguably more dif-
ficult due to the dearth of suitable resources. We
have compiled and released a corpus of over 700
newswire articles that describe different instances of
15 scenarios, as a suitable input dataset for further
STC research. Using this dataset, we have evaluated
and analyzed our context-sensitive approach. While
our results are indicative, they show that considering
contextual evidence improves performance.
Acknowledgments
The authors are grateful to Kathleen R. McKeown
and Elena Filatova at Columbia University for their
stimulating discussions and comments over different
stages of the preparation of this paper.
References
Robin Collier. 1998. Automatic Template Creation forInformation Extraction. Ph.D. thesis, University ofSheffield, UK.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the EM al-gorithm. JRSSB, 39:1?38.
Elena Filatova, Vasileios Hatzivassiloglou, and KathleenMcKeown. 2006. Automatic creation of domain tem-plates. In Proceedings of the COLING/ACL ?06.
Sanda M. Harabagiu and V. Finley Lacatusu. 2005.
Topic themes for multi-document summarization. InProceedings of SIGIR ?05.
163
Sanda M. Harabagiu and S. J. Maiorano. 2002. Multi-
document summarization with GISTEXTER. In Pro-ceedings of LREC ?02.
Graeme Hirst and Alexander Budanitsky. 2005. Cor-recting real-word spelling errors by restoring lexical
cohesion. Natural Language Engineering, 11(1).
Andreas Hotho, Steffen Staab, and Gerd Stumme. 2003.WordNet improves text document clustering. In Pro-ceedings of the SIGIR 2003 Semantic Web Workshop.
LDC. 2002. The aquaint corpus of english news text,
catalog no. LDC2002t31.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empiri-cal evaluation of knowledge sources and learning algo-rithms for word sense disambiguation. In Proceedingsof EMNLP ?02.
Dekang Lin. 1998. Automatic retrieval and clustering ofsimilar words. In Proceedings of COLING/ACL ?98.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, JamesMartin, and Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines. In Proceed-ings of HLT/NAACL ?04.
Judita Preiss. 2001. Local versus global context for wsdof nouns. In Proceedings of CLUK4.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2006.Paraphrase recognition via dissimilarity significanceclassification. In Proceedings of EMNLP ?06.
Ellen Riloff and M. Schmelzenbach. 1998. An empiri-
cal approach to conceptual case frame acquisition. InProceedings of WVLC ?98.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.2003. An improved extraction pattern representation
model for automatic IE pattern acquisition. In Pro-ceedings of ACL ?03.
Yuen-Hsien Tseng, Chi-Jen Lin, Hsiu-Han Chen, and Yu-I Lin. 2006. Toward generic title generation for clus-
tered documents. In Proceedings of AIRS ?06.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,and Silja Huttunen. 2000. Unsupervised discovery ofscenario-level patterns for information extraction. InProceedings of ANLP ?00.
164
