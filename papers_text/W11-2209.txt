Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 72?81,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Concept Annotation using Latent Dirichlet Allocation and
Segmental Methods
Nathalie Camelin, Boris Detienne, Ste?phane Huet, Dominique Quadri and Fabrice Lefe`vre
LIA - University of Avignon, BP 91228
84911 Avignon Cedex 09, France
{nathalie.camelin,boris.detienne,stephane.huet,dominique.quadri,fabrice.lefevre}@univ-avignon.fr
Abstract
Training efficient statistical approaches for
natural language understanding generally re-
quires data with segmental semantic annota-
tions. Unfortunately, building such resources
is costly. In this paper, we propose an ap-
proach that produces annotations in an unsu-
pervised way. The first step is an implementa-
tion of latent Dirichlet alocation that produces
a set of topics with probabilities for each topic
to be associated with a word in a sentence.
This knowledge is then used as a bootstrap to
infer a segmentation of a word sentence into
topics using either integer linear optimisation
or stochastic word alignment models (IBM
models) to produce the final semantic anno-
tation. The relation between automatically-
derived topics and task-dependent concepts is
evaluated on a spoken dialogue task with an
available reference annotation.
1 Introduction
Spoken dialogue systems in the field of information
query are basically used to interface a database with
users using speech. When probabilistic models are
used in such systems, good performance can only be
reached at the price of collecting a lot of field data,
which must be transcribed and annotated at the se-
mantic level. It becomes then possible to train effi-
cient models in a supervised manner. However, the
annotation process is costly and as a consequence
represents a real difficulty hindering the widespread
development of these systems. Therefore any means
to avoid it would be profitable as portability to new
tasks, domains or languages would be greatly facili-
tated.
To give a full description of the architecture of a
dialogue system is out of the scope of this paper. In-
stead we limit ourselves to briefly recall that once
a speech recognizer has transcribed the signal it is
common (though avoidable for very simple tasks) to
use a module dedicated to extract the meaning of
the user?s queries. This meaning representation is
then conveyed to an interaction manager that decides
upon the next best action to perform considering the
current user?s input and the dialogue history. One
of the very first steps to build the spoken language
understanding (SLU) module is the identification of
literal concepts in the word sequence hypothesised
by the speech recogniser. An example of a semantic
representation in terms of literal concept is given in
Figure 1. Once the concepts are identified they can
be further composed to form the overall meaning of
the sentence, for instance by means of a tree repre-
sentation based on hierarchical semantic frames.
To address the issue of concept tagging several
techniques are available. Some of these techniques
now classical rely on probabilistic models, that can
be either discriminative or generative. Among these,
the most efficiently studied this last decade are: hid-
den Markov models, finite state transducers, max-
imum entropy Markov models, support vector ma-
chines, dynamic fields (CRF). In (Hahn et al, 2010)
it is shown that CRFs obtain the best performance on
a tourist information retrieval task in French (ME-
DIA (Bonneau-Maynard et al, 2005)), but also in
two other comparable corpora in Italian and Polish.
To be able to apply any such technique, basic con-
72
words concept normalized value
donnez-moi null
le refLink-coRef singular
tarif object payment-amount-room
puisque connectProp imply
je voudrais null
une chambre number-room 1
qui cou?te object payment-amount-room
pas plus de comparative-payment less than
cinquante payment-amount-integer-room 50
euros payment-unit euro
Figure 1: Semantic concept representation for the query ?give me the rate since I?d like a room charged not more than
fifty euros?.
cept units have to be defined by an expert. In the best
case, most of these concepts can be derived straight-
forwardly from the pieces of information lurking in
the database tables (mainly table fields but not ex-
clusively). Some others are general (dialogic units
but also generic entities such as number, dates, etc).
However, to provide an efficient and usable informa-
tion to the reasoning modules (the dialogue manager
in our case) concepts have to be fine-grained enough
and application-dependent (even general concepts
might have to be tailored to peculiar uses). To that
extent it seems out of reach to derive the concept
definitions using a fully automatic procedure. Any-
how the process can be bootstrapped, for instance
by induction of semantic classes such as in (Siu and
Meng, 1999) or (Iosif et al, 2006). Our assumption
here is that the most time-consuming parts of con-
cept inventory and data tagging could be obtained in
an unsupervised way even though a final (but hope-
fully minimal) manual procedure is still required to
tag the classes so as to manually correct automatic
annotation.
Unlike the previous attempts cited above which
developed ad hoc approaches, we investigate here
the use of broad-spectrum knowledge extraction
methods. The notion most related to that of concept
in SLU is the topic, as used in information retrieval
systems. Anyhow for a long time, the topic detec-
tion task was limited to associate a single topic to
a document and thus was not fitted to our require-
ments. The recently proposed LDA technique al-
lows to have a probabilistic representation of a doc-
ument as a mixture of topics. Then multiple topics
can co-occur inside a document and the same topic
can be repeated. From these characteristics it is pos-
sible to consider the application of LDA to unsu-
pervised concept inventory and concept tagging for
SLU. A shortcoming is that LDA does not modelize
at all the sequentiality of the data. To address this is-
sue we propose to conclude the procedure with a fi-
nal step to introduce specific constraints for a correct
segmentation of the data: the assignments of topics
proposed by LDA are modified to be more segmen-
tally coherent.
The paper is organised as follows. Principles
of automatic induction of semantic classes are pre-
sented in Section 2, followed by the presentation of
an induction system based on LDA. The additional
step of segmentation is presented in Section 3 with
two variants: stochastic word alignment (GIZA) and
integer linear programming (ILP). Then evaluations
and results are reported in Section 4 on the French
MEDIA dialogue task.
2 Automatic induction of semantic classes
2.1 Context modeling
The idea of automatic induction of semantic classes
is based on the assumption that concepts often share
the same context (syntactic or lexical). Imple-
mented systems are based on the observation of co-
occurring words according to two different ways.
The observation of consecutive words (bigrams or
trigrams) enables the generation of lexical com-
pounds supposed to follow syntactic rules. The com-
parison of right and left contexts considering pairs
of words enables to cluster words (and word com-
pounds) into semantic classes.
73
In (Siu and Meng, 1999) and (Pargellis et al,
2001), iterative systems are presented. Their im-
plementations differ in the metrics chosen to eval-
uate the similarity during the generation of syntactic
rules and semantic classes, but also in the number
of words taken into account in a word context and
the order of successive steps (which ones to gener-
ate first: syntactic rules or semantic classes?). An
iterative procedure is executed to obtain a sufficient
set of rules in order to automatically extract knowl-
edge from the data.
While there may be still room for improvement in
these techniques we decided instead to investigate
general knowledge extraction approaches in order to
evaluate their potential. For that purpose a global
strategy based on an unsupervised machine learning
technique is adopted in our work to produce seman-
tic classes.
2.2 Implementation of an automatic induction
system based on LDA
Several approaches are available for topic detection
in the context of knowledge extraction and informa-
tion retrieval. They all more or less rely on the pro-
jection of the documents of interest in a semantic
space to extract meaningful information. However,
as the considered spaces (initial document words
and latent semantics) are discrete the performance
of the proposed approaches for the topic extraction
tasks are pretty unstable, and also greatly depend on
the quantity of data available. In this work we were
motivated by the recent development of a very at-
tractive technique with major distinct features such
as the detection of multiple topics in a single docu-
ment. LDA (Blei et al, 2003) is the first principled
description of a Dirichlet-based model of mixtures
of latent variables. LDA will be used in our work
to annotate the dialogue data in terms of topics in
an unsupervised manner. Then the relation between
automatic topics and expected concepts will be ad-
dressed manually.
Basically LDA is a generative probabilistic model
for text documents. LDA follows the assumption
that a set of observations can be explained by latent
variables. More specifically documents are repre-
sented by a mixture of topics (latent variables) and
topics are characterized by distributions over words.
The LDA parameters are {?, ?}. ? represents the
Dirichlet parameters of K latent topic mixtures as
? = [?1, ?2, . . . , ?K ]. ? is a matrix representing a
multinomial distribution in the form of a conditional
probability table ?k,w = P (w|k). Based on this rep-
resentation, LDA can estimate the probability of a
new document d of N words d = [w1, w2, . . . , wN ]
using the following procedure.
A topic mixture vector ? is drawn from the Dirich-
let distribution (with parameter ?). The correspond-
ing topic sequence ? = [k1, k2, . . . , kN ] is generated
for the whole document accordingly to a multino-
mial distribution (with parameter ?). Finally each
word is generated by the word-topic multinomial
distribution (with parameter ?, that is p(wi|ki, ?)).
After this procedure, the joint probability of ?, ? and
d is then:
p(?, ?, d|?, ?) = p(?|?)
N?
i=1
p(ki|?)p(wi|ki, ?)
(1)
To obtain the marginal probability of d, a final in-
tegration over ? and a summation over all possible
topics considering a word is necessary:
p(d|?, ?) =
?
p(?|?)
?
?
N?
i=1
?
ki
p(ki|?)p(wi|ki, ?)
?
?
(2)
The framework is comparable to that of probabilis-
tic latent semantic analysis, but the topic multino-
mial distribution in LDA is assumed to be sampled
from a Dirichlet prior and is not linked to training
documents. This approach is illustrated in Figure 2.
Training of the ? and ? parameters is possible us-
ing a corpus of documents, with a fixed number of
topics to predict. A variational inference procedure
is described in (Blei et al, 2003) which alleviates
the intractability due to the coupling between ? and
? in the summation over the latent topics. Once the
parameters for the Dirichlet and multinomial distri-
butions are available, topic scores can be derived for
any given document or word sequence.
In recent years, several studies have been carried
out in language processing based on LDA. For in-
stance, (Tam and Schultz, 2006) worked on unsuper-
vised language model adaptation; (Celikyilmaz et
al., 2010) ranked candidate passages in a question-
answering system; (Phan et al, 2008) implemented
LDA to classify short and sparse web texts.
74
LATENT DIRICHLET ALLOCATION
?                                    k w?
?
M
N
Figure 1: Graphical model representation of LDA. The boxes are ?plates? representing replicates.
The outer plate represents documents, while the inner plate represents the repeated choice
of topics and words within a document.
where p(zn | ?) is simply ? i for the unique i such that zin = 1. Integrating over ? and summing over
z, we obtain the marginal distribution of a document:
p(w | ?, ?) =
?
p(? | ?)
(
N?
n=1
?
zn
p(zn | ?) p(wn |zn, ?)
)
d ?. (3)
Finally, taking the product of the marginal probabilities of single documents, we obtain the proba-
bility of a corpus:
p(D | ?, ?) = M?
d=1
?
p(? d | ?)
(
Nd?
n=1
?
zdn
p(zdn | ? d)p(wdn |zdn, ?)
)
d ? d .
The LDA model is represented as a probabilistic graphical model in Figure 1. As the figure
makes clear, there are three levels to the LDA representation. The parameters ? and ? are corpus-
level parameters, assumed to be sampled once in the process of generating a corpus. The variables
? d are document-level variables, sampled once per document. Finally, the variables zdn and wdn are
word-level variables and are sampled once for each word in each document.
It is important to distinguish LDA from a simple Dirichlet-multinomial clustering model. A
classical clustering model would involve a two-level model in which a Dirichlet is sampled once
for a corpus, a multinomial clustering variable is selected once for each document in the corpus,
and a set of words are selected for the document conditional on the cluster variable. As with many
clustering models, such a model restricts a document to being associated with a single topic. LDA,
on the other hand, involves three levels, and notably the topic node is sampled repeatedly within the
document. Under this model, documents can be associated with multiple topics.
Structures similar to that shown in Figure 1 are often studied in Bayesian statistical modeling,
where they are referred to as hierarchical models (Gelman et al, 1995), or more precisely as con-
ditionally independent hierarchical models (Kass and Steffey, 1989). Such models are also often
referred to as parametric empirical Bayes models, a term that refers not only to a particular model
structure, but also to the methods used for estimating parameters in the model (Morris, 1983). In-
deed, as we discuss in Section 5, we adopt the empirical Bayes approach to estimating parameters
such as ? and ? in simple implementations of LDA, but we also consider fuller Bayesian approaches
as well.
997
Figure 2: Graphical representation for LDA variables
(from (Blei et al, 2003)). The grey circle is the only ob-
servable variable.
In our work LDA is employed to annotate each
user?s utterance of a dialogue corpus with topic. Ut-
terances longer than one word are included in the
training set as its sequence of words. Once the
model has been trained, inference on data corpus as-
signs the topic with the highest probability to each
word in a document. This probability is computed
from the probability of the topic to appear in the doc-
ument and the probability of the word to be gener-
ated by th opic. As a con equence we obtain a full
topic annotati n of the utterance.
Notice that LDA considers a user utterance as a
bag of words. This implies that each topic is as-
signed to a word without any consideration for its
i m diate co text. An additional segmental process
is required if we want to introduce some context in-
formation in the topic assignment.
3 Segmental annotation
3.1 Benefits of a segmental annotation
The segmental annotation of the data is not a strict
requirement for language understanding. Up to quite
recently, most approaches for literal interpretation
were limited to lexical-concept relations; for in-
stance this is the case of the Phoenix system (Ward,
1991) based on the detection of keywords. However
in an NLP perspective, the segmental approach al-
lows to connect the various levels of sentence analy-
sis (lexical, syntactic and semantic). Even though, in
order to simplify its application, segments are gen-
erally designed specifically for the semantic anno-
tation and do not have any constraint on their rela-
tion with the actual syntactic units (chunks, phrasal
groups, etc). To get relieved of such constraints not
only simplifies the annotation process itself but as
ultimately the interpretation module is to be used in-
side a spoken dialogue system, data will be noisy
and generally bound the performance of the syn-
tactic analysers (due to highly spontaneous and un-
grammatical utterances from the users, combined
with errors from the speech recognizer).
Another interesting property of segmental ap-
proach is to offer a convenient way to dissociate the
detection of a conceptual unit from the extraction of
its associated value. The value corresponds to the
nor alisation of the surface form (see last column
in 1); for instance if the segment ?not more than?
is associated to the concept comparative-payment,
its value is ?less than?. The same value would
be associated to ?not exceeding? or ?inferior to?.
Value extraction requires a link between concepts
and words based on which the normalisation prob-
lem can be addressed by means of regular expres-
sions or concept-dependent language models (even
allowing integrated approaches such as described
in (Lefe`vre, 2007)). In the case of global approaches
(not segmental), value extraction must be dealt with
directly at the level of the conceptual unit tagging,
as in (Mairesse et al, 2009). This additional level is
very complex (as some values may not be enumer-
able, such as numbers and dates) and is only afford-
able when the number of authorised values (for the
enumerable cases) is low.
To refine the LDA output, the topic-to-word align-
ment is discarded and an automatic procedure is
used to derive the best alignment between topics and
words. While the underlying probabilistic models
are pretty comparable, the major interest of this ap-
pro ch is to eparate the tasks of detecting topics and
aligning topics with words. It is then possible to in-
troduce additional constraints (such as locality, num-
ber of segments, limits on repetitions etc) in the lat-
ter task which would otherwise hinder topic detec-
tion. Conversely the alignment is self-coherent and
able to question the associations proposed during
topic detection with respect to its own constraints
only. Two approaches were designed to this pur-
pose: one based on IBM alignment models and an-
other one based on integer linear optimisation.
75
3.2 Alignment with IBM models (GIZA)
Once topic assignments for the documents in the
corpus have been proposed by LDA, a filtering pro-
cess is done to keep only the most relevant topics
of each document. The ?max most probable top-
ics are kept according to the probability p(k|wi, d)
that topic k generated the word wi of the document
d. ?max is a value fixed empirically according to
the expected set of topics in a sentence. Then, the
obtained topic sequences are disconnected from the
words. At this point, the topic and word sequences
can be considered as a translation pair to produce
a word-topic parallel corpus. These data can be
used with classical approaches in machine transla-
tion to align source and target sentences at the word
level. Since these alignment models can align sev-
eral words with a single topic, only the first occur-
rence is kept for consecutive repetitions of the same
topic. These models are expected to correct some er-
rors made by LDA, and to assign in particular words
previously associated with discarded topics to more
likely ones.
In our experiments the statistical word alignment
toolkit GIZA++ (Och and Ney, 2003) is used to
train the so-called IBM models 1-4 as well as the
HMM model. To be able to train the most informa-
tive IBM model 4, the following training pipeline
was considered: 5 iterations of IBM1, 5 iterations
of HMM, 3 iterations of IBM3 and 3 iterations of
IBM4. The IBM4 model obtained at the last iter-
ation is finally used to align words and topics. In
order to improve alignment, IBM models are usu-
ally trained in both directions (words towards con-
cepts and vice versa) and symmetrised by combin-
ing them. For this purpose, we resorted to the default
symmetrization heuristics used by MOSES, a widely
used machine translation system toolkit (Koehn et
al., 2007).
3.3 Alignment with Integer Linear
Programming (ILP)
Another approach to the re-alignment of LDA out-
puts is based on a general optimisation technique.
ILP is a widely used tool for modelling and solv-
ing combinatorial optimisation problems. It broadly
aims at modelling a decision process as a set of equa-
tions or inequations (called constraints) which are
linear with regards to so-called decision variables.
An ILP is also composed of a linear objective func-
tion. Solving an ILP consists in assigning values to
decision variables, such that all constraints are sat-
isfied and the objective function is optimised. We
refer to (Chen et al, 2010) for an overview of appli-
cations and methods of ILP.
We provide two ILP formulations for solving the
topic assignment problem related to a given docu-
ment. They both take as input data an ordered set d
of words wi, i = 1...N , a set of K available topics
and, for each word wi ? d and topic k = 1...K,
the natural logarithm of the probability p(k|wi, d)
that k is assigned to wi in the considered document
d. Model [ILP ] simply finds the highest-probability
assignment of one topic to each word in the doc-
ument, such that at most ?max different topics are
assigned.
[ILP ] : max
N?
i=1
K?
k=1
log(p(k|wi, d)) xik (3)
?K
k=1 xik = 1 i (4)
yk ? xik ? 0 i, k (5)?K
k=1 yk ? ?max (6)
xik ? {0, 1} i, k
yk ? {0, 1} k
In this model, decision variable xik is equal to 1 if
topic k is assigned to word wi, and equal to 0 other-
wise. Constraints (4) ensure that exactly one topic is
assigned to each word. Decision variable yk is equal
to 1 if topic k is used. Constraints (5) force vari-
able yk to take a value of 1 if at least one variable
xik is not null. Moreover, Constraints (6) limit the
total number of topics used. The objective function
(3) merely states that we want to maximize the total
probability of the assignment. Through this model,
our assignment problem is identified as a p-centre
problem (see (ReVelle and Eiselt, 2005) for a survey
on such location problems).
Numerical experiments show that [ILP ] tends to
give sparse assignments: most of the time, adja-
cent words are assigned to different topics even if
the total number of topics is correct. To prevent
this unnatural behaviour, we modified [ILP ] to con-
sider groups of consecutive words instead of isolated
76
words. Model [ILP seg] partitions the document
into segments of consecutive words, and assigns one
topic to each segment, such that at most ?max seg-
ments are created. For the sake of convenience, we
denote by p?(k|wij , d) =
?j
l=i log(p(k|wl, d)) the
logarithm of the probability that topic k is assigned
to all words from i to j in the current document.
[ILP seg] : max
N?
i=1
N?
j=i
K?
k=1
p?(k|wij , d) xijk (7)
i?
j=1
N?
l=i
K?
k=1
xjlk = 1 i (8)
N?
i=1
N?
j=i
K?
k=1
xijk ? ?max (9)
xijk ? {0, 1} i, j, k
In this model, decision variable xijk is equal to 1
if topic k is assigned to all words from i to j, and
0 otherwise. Constraints (8) ensure that each word
belongs to a segment that is assigned a topic. Con-
straints (9) limit the number of segments. Due to
the small size of the instances considered in this pa-
per, both [ILP ] and [ILP seg] are well solved by a
direct application of an ILP solver.
4 Evaluation and results
4.1 MEDIA corpus
The MEDIA corpus is used to evaluate the pro-
posed approach and to compare the various con-
figurations. MEDIA is a French corpus related to
the domain of tourism information and hotel book-
ing (Bonneau-Maynard et al, 2005). 1,257 dia-
logues were recorded from 250 speakers with a wiz-
ard of Oz technique (a human agent mimics an auto-
matic system). This dataset contains 17k user utter-
ances and 123,538 words, for a total of 2,470 distinct
words.
The MEDIA data have been manually transcribed
and semantically annotated. The semantic annota-
tion uses 75 concepts (e.g. location, hotel-state,
time-month. . . ). Each concept is supported by a se-
quence of words, the concept support. The null con-
cept is used to annotate every words segment that
does not support any of the 74 other concepts (and
does not bear any information wrt the task). On aver-
age, a concept support contains 2.1 words, 3.4 con-
cepts are included in a utterance and 32% of the ut-
terances are restrained to a single word (generally
?yes? or ?no?). Table 1 gives the proportions of ut-
terances according to the number of concepts in the
utterance.
# concepts 1 2 3 [4,72]
% utterances 49.4 14.1 7.9 28.6
Table 1: Proportion of user utterances as a function of the
number of concepts in the utterance.
Notice that each utterance contains at least one
concept (the null label being considered as a con-
cept). As shown in Table 2, some concepts are sup-
ported by few segments. For example, 33 concepts
are represented by less than 100 concept supports.
Considering that, we can foresee that finding these
poorly represented concepts will be hard for LDA.
[1,100[ [100,500[ [500,1k[ [1k,9k[ [9k,15k]
33 21 6 14 1 (null)
Table 2: Number of concepts according to their occur-
rence range.
4.2 Evaluation protocol
Unlike previous studies, we chose a fully automatic
way to evaluate the systems. In (Siu and Meng,
1999), a manual process is introduced to reject in-
duced classes or rules that are not relevant to the
task and also to name the semantic classes with the
appropriate label. Thus, they were able to evaluate
their semi-supervised annotation on the ATIS cor-
pus. In (Pargellis et al, 2001), the relevance of the
generated semantic classes was manually evaluated
giving a mark to each induced semantic rule.
To evaluate the unsupervised procedure it is nec-
essary to associate each induced topic with a MEDIA
concept. To that purpose, the reference annotation
is used to align topics with MEDIA concepts at the
word level. A co-occurrence matrix is computed and
each topic is associated with its most co-occurring
concept.
As MEDIA reference concepts are very fine-
grained, we also define a high-level concept hier-
77
archy containing 18 clusters of concepts. For ex-
ample, a high-level concept payment is created from
the 4 concepts payment-meansOfPayment, payment-
currency, payment-total-amount, payment-approx-
amount; a high-level concept location corresponds
to 12 concepts (location-country, location-district,
location-street, . . . ). Thus, two levels of concepts
are considered for the evaluation: high-level and
fine-level.
The evaluation is presented in terms of the classi-
cal F-measure, defined as a combination of precision
and recall measures. Two levels are also considered
to measure topic assignment quality:
? alignment corresponds to a full evaluation
where each word is considered and associated
with one topic;
? generation corresponds to the set of topics gen-
erated for a turn (no order, no word-alignment).
4.3 System descriptions
Four systems are evaluated in our experiments.
[LDA] is the result of the unsupervised learning
of LDA models using GIBBSLDA++ tool1. It as-
signs the most probable topic to each word occur-
rence in a document as described in Section 2.2.
This approach requires prior estimation of the num-
ber of clusters that are expected to be found in the
data. To find an optimal number of clusters, we ad-
justed the number K of topics around the 75 ref-
erence concepts. 2k training iterations were made
using default values for ? and ?.
[GIZA] is the system based on the GIZA++
toolkit2 which re-aligns for each sentence the topic
sequence assigned by [LDA] to word sequence as
described in Section 3.2.
[ILP ] and [ILP seg] systems are the results of
the ILP solver IBM ILOG CPLEX3 applied to the
models described in Section 3.3.
For the three last systems, the value ?max has to
be fixed according to the desired concept annota-
tion. As on average a concept support contains 2.1
words, ?max is defined empirically according to the
number of words: with i = [[2, 4]]: ?max = i with
1http://gibbslda.sourceforge.net/
2http://code.google.com/p/giza-pp/
3http://www-01.ibm.com/software/integration/optimization/cplex-
optimizer/
 56 57 58 59 60 61 62 63 64 65 66 67
 50
 100
 150
 200
Fmeasure
Numb
er of t
opics
GIZA ILP ILP_s
eg LDA
Figure 3: F-measure of the high-level concept generation
as a function of the number of topics.
 44 46 48 50 52 54 56
 50
 100
 150
 200
Fmeasure
Numb
er of t
opics
GIZA ILP ILP_s
eg LDA
Figure 4: F-measure of the high-level concept alignment
as a function of the number of topics.
i = [[5, 10]] words: ?max = i? 2 and for utterances
containing more than 10 words: ?max = i/2.
For the sake of simplicity, single-word utterances
are processed separately with prior knowledge. City
names, months, days or answers (e.g. ?yes?, ?no?,
?yeah?) and numbers are identified in these one-
word utterances.
4.4 Results
Examples of topics generated by [LDA], with K =
100 topics, are shown in Table 3.
Plots comparing the different systems imple-
mented w.r.t. the different evaluation levels in terms
of F-measure are reported in Figures 3, 4, 5 and 6
(high-level vs fine-level, alignment vs generation).
The [LDA] system generates topics which are
78
Topic 0 Topic 13 Topic 18 Topic 35 Topic 33 Topic 43
information time-date sightseeing politeness location answer-yes
words prob. words prob. words prob. words prob. words prob. words prob.
d? 0.28 du 0.16 de 0.30 au 0.31 de 0.30 oui 0.62
plus 0.17 au 0.11 la 0.24 revoir 0.27 Paris 0.12 et 0.02
informations 0.16 quinze 0.08 tour 0.02 madame 0.09 la 0.06 absolument 0.008
autres 0.10 dix-huit 0.07 vue 0.02 merci 0.08 pre`s 0.06 autre 0.008
de?tails 0.03 de?cembre 0.06 Eiffel 0.02 bonne 0.01 proche 0.05 donc 0.007
obtenir 0.03 mars 0.06 sur 0.02 journe?e 0.01 Lyon 0.03 jour 0.005
alors 0.01 dix-sept 0.04 mer 0.01 villes 0.004 aux 0.02 Notre-Dame 0.004
souhaite 0.003 nuits 0.04 sauna 0.01 biento?t 0.003 gare 0.02 d?accord 0.004
Table 3: Examples of topics discovered by LDA (K = 100).
 47 48 49 50 51 52 53 54 55 56 57 58
 50
 100
 150
 200
Fmeasure
Numb
er of t
opics
GIZA ILP ILP_s
eg LDA
Figure 5: F-measure of the fine-level concept generation
as a function of the number of topics.
correctly correlated with the high-level concepts. It
can be observed that the bag of 75 topics reaches
an F-measure of 61.5% (Fig. 3). When not enough
topics are required from [LDA], induced topics are
too wide to fit the fine-grained concept annotation of
MEDIA. On the other hand if too many topics are re-
quired, the performance of bag of high-level topics
stays the same while a substantial decrease of the
F-measure is observed in the alignment evaluation
(Fig. 4). This effect can be explained by the auto-
matic alignment method chosen to transpose topics
into reference concepts. Indeed, the increase of the
number of topics makes them co-occur with many
concepts, which often leads to assign them to the
most frequent concept null in the studied corpus.
From the high-level to fine-level concept evalua-
tions, results globally decrease by 10%. An addi-
tional global loss of 10% is also observed for both
the generation and alignment scorings. In the fine-
 34 36 38 40 42 44 46 48
 50
 100
 150
 200
Fmeasure
Numb
er of t
opics
GIZA ILP ILP_s
eg LDA
Figure 6: F-measure of the fine-level concept alignment
as a function of the number of topics.
level evaluation, a maximum F-measure of 52.2%
is observed for the generation of 75 topics (Fig. 5)
whereas the F-measure decreases to 41.5% in the
alignment evaluation (Fig. 6).
To conclude on the [LDA] system, we can see that
it generates topics having a good correlation with the
high-level concepts, seemingly the best representa-
tion level between topics and concepts. From these
results it seems obvious that an additional step is
needed to obtain a more accurate segmental annota-
tion, which is expected with the following systems.
The [GIZA] system improves the results. It is
very likely that the filtering process helps to dis-
card the irrelevant topics. Therefore, the automatic
alignment between words and the filtered topics in-
duced by [LDA] with IBM models seems more ro-
bust when more topics (a higher value for K) is re-
quired from [LDA], specifically in high-level con-
cept alignment (Fig. 4).
79
Systems based on the ILP technique perform bet-
ter than other systems whatever the evaluation. Con-
sidering [LDA] as the baseline, we can expect sig-
nificant gains of performance. For example, an F-
measure of 66% is observed for the ILP systems
considering the high-level concept generation for 75
topics (Figure 4), where the maximum for [LDA]
was 61.5%, and an F-measure of 55% is observed
(instead of 50.5% for [LDA]) considering the high-
level concept alignment.
No significant difference was finally measured be-
tween both ILP models for the concept generation
evaluations. Even though [ILP seg] seems to ob-
tain slightly better results in the alignment evalua-
tion. This could be expected since [ILP seg] intrin-
sically yields alignments with grouped topics, closer
to the reference alignment used for the evaluation.
It is worth noticing that unlike [LDA] system be-
haviour, the results of [ILP ] are not affected when
more topics are generated by [LDA]. A large num-
ber of topics enables [ILP ] to pick up the best topic
for a given segment among in a longer selection list.
As for [LDA], the same losses are observed be-
tween high-level and fine-level concepts and gener-
ation and alignment paradigms. Nevertheless, an F-
measure of 54.8% is observed at the high-level con-
cept in alignement evaluation (Figure 4) that corre-
sponds to a precision of 56.2% and a recall of 53.5%,
which is not so low considering a fully-automatic
high-level annotation system.
5 Conclusions and perspectives
In this paper an unsupervised approach for con-
cept extraction and segmental annotation has been
proposed and evaluated. Based on two steps
(topic inventory and assignment with LDA, then re-
segmentation with either IBM alignment models or
ILP) the technique has been shown to offer perfor-
mance above 50% for the retrieval of reference con-
cepts. It confirms the applicability of the technique
to practical tasks with an expected gain in data pro-
duction.
Future work will investigate the use of n-grams
to increase LDA accuracy to provide better hypothe-
ses for the following segmentation method. Besides,
other levels of data representation will be examined
(use of lemmas, a priori semantic classes like city
names. . . ) in order to better generalise on the data.
ACKNOWLEDGEMENTS
This work is supported by the ANR funded project
PORT-MEDIA (www.port-media.org) and the LIA
OptimNLP project (www.lia.univ-avignon.fr).
References
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. The Journal of Machine Learning
Research, 3:993?1022.
H. Bonneau-Maynard, S. Rosset, C. Ayache, A. Kuhn,
and D. Mostefa. 2005. Semantic annotation of the
french media dialog corpus. In Proceedings of the 9th
European Conference on Speech Communication and
Technology.
A. Celikyilmaz, D. Hakkani-Tur, and G. Tur. 2010. Lda
based similarity modeling for question answering. In
Proceedings of the NAACL HLT 2010 Workshop on Se-
mantic Search, pages 1?9. Association for Computa-
tional Linguistics.
Der-San Chen, Robert G. Batson, and Yu Dang. 2010.
Applied Integer Programming: Modeling and Solu-
tion. Wiley, January.
Stefan Hahn, Marco Dinarelli, Christian Raymond, Fab-
rice Lefvre, Patrick Lehnen, Renato De Mori, Alessan-
dro Moschitti, Hermann Ney, and Giuseppe Riccardi.
2010. Comparing stochastic approaches to spoken
language understanding in multiple languages. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, PP(99):1.
E. Iosif, A. Tegos, A. Pangos, E. Fosler-Lussier, and
A. Potamianos. 2006. Unsupervised combination of
metrics for semantic class induction. In Proceedings
of the IEEE Spoken Language Technology Workshop,
pages 86?89.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of ACL, Companion Volume, pages 177?180,
Prague, Czech Republic.
F. Lefe`vre. 2007. Dynamic bayesian networks and
discriminative classifiers for multi-stage semantic in-
terpretation. In Proceedings of ICASSP, Honolulu,
Hawai.
F. Mairesse, M. Gas?ic?, F. Jurc???c?ek, S. Keizer, B. Thom-
son, K. Yu, and S. Young. 2009. Spoken language
80
understanding from unaligned data using discrimina-
tive classification models. In Proceedings of ICASSP,
Taipei, Taiwan.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
A. Pargellis, E. Fosler-Lussier, A. Potamianos, and C.H.
Lee. 2001. Metrics for measuring domain indepen-
dence of semantic classes. In Proceedings of the 7th
European Conference on Speech Communication and
Technology.
X.H. Phan, L.M. Nguyen, and S. Horiguchi. 2008.
Learning to classify short and sparse text & web with
hidden topics from large-scale data collections. In
Proceeding of the 17th international conference on
World Wide Web, pages 91?100. ACM.
C. S. ReVelle and H. A. Eiselt. 2005. Location analysis:
A synthesis and survey. European Journal of Opera-
tional Research, 165(1):1?19, August.
K.C. Siu and H.M. Meng. 1999. Semi-automatic acqui-
sition of domain-specific semantic structures. In Pro-
ceedings of the 6th European Conference on Speech
Communication and Technology.
Y.C. Tam and T. Schultz. 2006. Unsupervised language
model adaptation using latent semantic marginals. In
Proceedings of INTERSPEECH, pages 2206?2209.
W Ward. 1991. Understanding Spontaneous Speech.
In Proceedings of ICASSP, pages 365?368, Toronto,
Canada.
81
