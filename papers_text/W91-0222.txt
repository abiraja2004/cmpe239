A Two-Level Knowledge Representation for Machine 
Translation: Lexical Semantics and Tense/Aspect  
Bonnie J. Dorr 
Institute for Advanced Computer Studies 
A. V. Williams Building 
University of Maryland 
College Park, MD $07~ 
bonnie @umiacs. umd. edu 
Abstract  
This paper proposes a two-level model that integrates tense and aspect information, 
based on theories by both Hornstein (in the spirit of Reichenbach) and Allen, with 
lexicai-semantic nformation based on an extended version of $ackendoff's theory 
that includes a verb classification system proposed by Dowty and Vendler. The 
model is intended to be extensible to realms outside of the temporal domain (e.g., 
the spatial domain). The integration of tense and aspect with lexical-semantics 
is especially critical in machine translation because of the lexical selection process 
during generation: there is often a number of lexical connective and tense/aspect 
possibilities that may be produced from a lexical semantic representation, which, 
as defined in the model presented here, is largely underspecified. The use of tense 
and aspect information allows the choice of target-language t rms to be more finely 
tuned and the combination of event structures to be more carefully constrained. 
1 Introduction 
Recently, there has been much discussion in the literature regarding the interaction of 
tense and aspect with lexical-semantics (see, for example, \[Bennett et hi., 1990\], \[Hinrichs, 
1988\], \[Maybury, 1990\], \[Moens and Steedman, 1988\], \[Nakhimovsky, 1988\], \[Passonneau, 
1988\], \[Pustejovsky, 1988, 1989\], and \[Tenny, 1989\]). Among those who have studied the 
problem of tense and/or aspect, many have taken the work of \[Reichenbach, 1947\] as a 
starting point (see, for example, \[Brent, 1988\], \[Moens and Steedman, 1988\], \[Hornstein, 
1990\], and \[Passonneau, 1988\]), while others have built on the work of \[Allen, 1983, 1984\] 
(see, for example, \[Vilain et hi., 1990\], and \[Williams, 1990\]), and still others have based 
their investigation on a combination of Allen and Reichenbach's work (see, for example, 
\[Yip, 1985\]). Among those who have studied lexical-semantic representations, many have 
taken the work of \[Jackendoff, 1983, 1990\] as a starting point (see, for example, \[Brent, 
1988\], \[Dorr, 1989, 1990a, 1990b, 1990c\], \[Levin and Rappaport, 1985\], and \[Siskind, 
1989\]), while others have built on the work of \[Dowty, 1979\] and \[Vendler, 1967\] (see, for 
example, \[Bennett et hi., 1990\], \[Moens and Steedman, 1988\], \[Nakhimovsky, 1988\], and 
\[Passonneau, 1988\]), and still others have followed \[Mourelatos, 1981\] and \[Comrie, 1976\] 
(see, for example, \[Bach, 1986\] and \[Pustejovksy, 1989\]). 
This paper proposes a two-level model that integrates tense and aspect information, 
based on theories by both Hornstein (in the spirit of l:teichenbach) and Allen, with lexical- 
semantic information based on an extended version of Jackendoff's theory that includes a 
verb classification system proposed by Dowty and Vendler. The model is intended to be 
extensible to realms outside of the temporal domain (e.g., the spatial domain). 
250 
(a) 
Lexical- 
Semantic 
Structure 
1 
Tense and 
Aspect 
Structure 
(b) 
Syntactic 
Structure 
John went to the store when Mary arrived 
Juan fue a la t ienda cuando Maria lleg6 
Juan fue a la t ienda al llegar Maria 
Figure 1: Two-Level Knowledge Representation f UNITRAN 
The integration of tense and aspect with lexical-semantics is especially critical in ma- 
chine translation because of the lexical selection process during generation: there is often 
a number of lexical connective and tense/aspect possibilities that may be produced from 
a lexical semantic representation, which, as defined in the model presented here, is largely 
underspecified. The use of tense and aspect information constrains the choice of target- 
language terms, which, in turn, limits the possibilities for the generation of tense and 
aspect; thus, there is a two-way communication channel between the two processes: lexi- 
cal selection and tense/aspect selection. 
The following section defines the dividing line between on-lexical knowledge (i.e., the 
tense and aspect of the surface sentence) and lexical knowledge (i. e., the lexical tokens that 
make up the surface sentence), and then discusses how these two types of knowledge are 
integrated in a two-level knowledge representation model for a machine translation system. 
Section 3 addresses the issue of cross-linguistic applicability of these two knowledge types, 
and section 4 discusses possible xtensions of temporal knowledge to the spatial domain. 
2 Interact ion  of Tense and Aspect  w i th  Lexical  Se- 
mant ics  
The hypothesis proposed by \[Tenny, 1987\] is that the mapping between cognitive structure 
and syntactic structure is governed by aspectual properties. The implication is that 
lexical-semantic knowledge xists at a level that does not include aspectual or temporal 
information (though these two types of knowledge may depend on each other in some 
way). This is the view that is adopted here: it is assumed that lexical semantic knowledge 
consists of such notions as predicate-argument structure, well-formedness conditions on 
predicate-argument structures, and procedures for lexical selection of surface-sentence 
tokens; all other types of knowledge must be represented at some other level. 
The lexical-semantic representation that is adopted as the interlingua for the UNI- 
TRAN machine translation system \[Dorr, 1989, 1990a, 1990b, 1990c\] is an extended ver- 
sion of lezical conceptual structure (henceforth, LCS) (see \[Jackendoff, 1983, 1990\]). This 
representation is the basis for the lexical-semantic level that is included in the knowledge 
representation (KR) component (see figure l(a)). The second level that is included in this 
component is the tense and aspect structure. 
In addition to the KR component, here is also a syntactic representation (SR) compo- 
nent (see figure l(b)) that is used for manipulating the syntactic structure of a sentence. 
Together, the KR and SR operate bidirectionally in order to analyze the source-language 
sentence and synthesize the target-language s ntence. We will omit the discussion of the 
251 
SR component of UNITRAN (see, for example, \[Doff, 1987\]) and will concern ourselves 
only with the KR component for the purposes of this paper. 
The translation example shown here illustrates the fact that the English sentence 
John went to the store when Mary arrived can be translated in two ways in Spanish. 
This example is addressed further throughout this paper. The remainder of this section 
defines the dividing line between on-lexical knowledge (i.e., tense and aspect) and lexical 
knowledge (i.e., properties of predicates and their arguments), and discusses how these 
two types of knowledge might be integrated in a two-level knowledge representation model 
for a machine translation system. 
2.1 Tense  and  Aspect  S t ructure  
The information required for the realization of tense and aspect is considered to be outside 
of the scope of the lexical knowledge. Aspect is taken to have two components, one that 
distinguishes between states and events, 1 and the other that defines the perspective (i.e., 
simple, progressive, and perfective). (See, for example, \[Dowty, 1979\] and \[Vendler, 1967\].) 
Tense, on the other hand, is taken to be the external time relationship between a given 
situation and others. (See, for example, \[Bennett et al, 1990\]). 
In the example of figure 1, the source- and target-language s ntences consist of two 
event structures, each of which is associated with its own tense and aspect structure. In 
the case of go (John went to the store), the event is associated with the Reichenbachian 
Basic Tense Structure (BTS) E,K_S, which indicates that the event is in the past. 2 The 
aspect of this clause is "simple" (as opposed to progressive or perfective). In the case of 
arrive (Mary arrived), the event is associated with the same Reichenbachian temporal 
representation (E,R S) and aspect (simple), since it too is in the simple past tense. As 
for relating these two events, the approach adopted here is based on a neo-Reichenbachian 
framework proposed by \[Hornstein, 1990\] in which the basic tense structures are organized 
into a complex tense structure (CTS) as follows: the first event (i.e., the matrix clause) is 
written over the BTS of the second event (i.e., the adjunct clause) and the S and R points 
are then associated. 3 The entire temporal/aspectual structure for this example would be 
specified as follows: 
1 We will see in section 2.2 that  events are further subdivided into activities, achievements, and 
accomplishments. 
2It is assumed that  the reader is familiar with the Reichenbachian framework, which postulates three 
theoretical entities: S (the moment of speech), R (a reference point), and E (the moment of the event). 
The key idea is that  certain l inear orderings of the three time points get grammaticalized into six basic 
tenses in English. The corresponding Basic Tense Structures are: 
S,R,E present 
E,R._S past 
S_R,E future 
E_S,R present perfect 
E R S past perfect 
S E .R  future perfect 
The S, R, and E points may be separated by a line (in which case, the leftmost point is interpreted 
as temporally earlier than the other) or by a comma (in which case, the points are interpreted as 
contemporaneous). 
3In the general case, the association of the S and R points may force the lq.2 point to be moved so that  
it is aligned with the R1 point. The E2 point is then placed accordingly. 
252 
(1) 
El,  RI_S1 
I I 
Es, R~_Ss 
aspect1 = simple 
aspects = simple 
Both tense and aspect are considered to be non-lexical in that they are determined 
by factors relating not to the lexical-semantic structure or particular lexical tokens of the 
surface sentence, but to the temporal/aspectual features of the context surrounding the 
event coupled with certain linguistically motivated constraints on the tense structure of 
the sentence. In particular, it has been persuasively argued by \[Hornstein, 1990\] that all 
sentences containing a matrix and adjunct clause are subject to a linguistic (syntactic) 
constraint on tense structure regardless of the lexical tokens included in the sentence. For 
example, Hornstein's linguistic Constraint on Derived Tense Structures (CDTS) requires 
that the association of S and R points not involve crossover in a complex tense structure: 
(2) * John went to the store when Mary arrives. 
El, Ri Sa 
S~, Rs, E~ 
aspect1 = simple 
aspects = simple 
Here, the association of R2 and R1 violates the CDTS, thus ruling out the sentence. 
Note that this linguistic constraint is a syntactic restriction on the manipulation of 
tense structures, not on the temporal interpretation of tensed sentences. Thus, the con- 
straint holds regardless of the lexical token that is chosen as the connective between the 
{- } before after 
as  soon  as  
while 
two events. 
(3) * John went to the store Mary arrives. 
? The connecting word that relates the two events must be selected independently of the 
temporal/aspectual structure associated with the sentence since the order of E1 and E2 in 
a given CTS does not necessarily correspond to the order imposed by the interpretation of
the connective. For example, the CTS for John went to the store before Mary had arrived 
is identical to the CTS for John went to the store after Mary had arrived, even though 
El is placed linearly after E2 in both cases: 
(4) 
Ea, Ri_S1 
I I 
E2 Rs_S~ 
aspect1 = simple 
aspects = perfective 
Thus, it is assumed that the knowledge required to determine the temporal/aspectual 
structure associated with a sentence xists at a level that is independent from the lexical- 
semantic knowledge required to select the appropriate lexical items for the surface sen- 
tence. 
The claim that there is a separation of temporal/aspectual knowledge from lexical- 
semantic knowledge is further strengthened if one considers that a given lexical-semantic 
representation may correspond to more than one tense and aspect structure, depending 
253 
on the context of the linguistic utterance. For example, suppose we are given the fact 
that John went to the store before Mary arrived; this fact can be represented as a single 
lexical-semantic structure, but it may be associated with a number of tense and aspect 
structures, each of which corresponds to a different surface-sentence utterance: 
(5) John went to the store before 
Mary arrived. 
(6) John went to the store before 
Mary had arrived. 
(7) John had gone to the store before 
Mary arrived. 
(8) John had gone to the store before 
Mary had arrived. 
El, R1 $1 aspect1 :simple \] 
I I 
E2, R2_$2 aspect2 = simple 
E2 R2 S~ aspect2 = perfective 
E1_Ri_Si aspect1 : perfect \] 
I I 
E2, Ra_Sa aspect2 = simple 
\[ Ei_Ri_S~ aspect1 = perfective \] 
I I 
Ea Ra Sa aspect2 = perfective 
All of these surface realizations are perfectly valid given that they are consistent with our 
temporal knowledge, i.e., that the "going to the store" event occurs before the "arriving" 
event. Thus, the lexical-semantic structure for these two events does not constrain the 
choice of potential tense and aspect structures with which it may be associated. This 
provides further evidence that temporal/aspectual knowledge xists at a level that is 
independent from lexical-semantic knowledge. 
Note that Hornstein's neo-Reichenbachian theory crucially relies on an asymmetry 
between the matrix and adjunct clauses. Thus, there is an important distinction between 
\[Hornstein, 1990\], in which the asymmetrical property is fundamental to the theory, and 
\[Yip, 1985\], in which the asymmetrical property is entirely abandoned. I suggest hat 
Hornstein's intuition is the correct one given that we cannot arbitrarily interchange the 
matrix and adjunct clauses. For example, Yip's theory predicts that we should be able to 
replace "El after E2" with "E2 before El," which is not always the case: 
(9) (i) John will go to the store after Mary has arrived. 
(ii) * Mary has arrived before John will go to the store. 
(10) (i) John will go to the store after Mary arrives. 
(ii) * Mary arrives before John will go to the store. 
Given this asymmetrical property, it would not be possible to randomly select a ma- 
trix/adjunct order and an appropriate t mporal connective for a surface sentence solely on 
the basis of lexical information. What is needed is the temporal relation between the two 
events and the constraints on their combination before it is possible to derive the surface 
structure of the sentence. In addition, the aspectual information must be determined be- 
fore the two events can be combined since this information will be necessary for retrieving 
the tense structure (e.g., simple vs. progressive) and selecting the lexical connective (e.g., 
when vs. while). We will return to the problem of the interaction between tense/aspect 
and lexical semantics in section 2.3, but first we will turn to a brief description of the 
lexical-semantic representation that is used as the interlingua for the machine translation 
system. 
254 
2.2 Lexical-Semantic Structure 
Lexical-semantic structure exists at a level of knowledge representation that is distinct 
from that of tense and aspect in that it encodes information about predicates and their 
arguments, plus the potential realization possibilities in a given language. In terms of the 
representation proposed by \[Jackendoff, 1983, 1990\], the lexical-semantic structures for 
the two events of figure 1 would be the following: 
(11) (i) \[~.,:, GOLo= (\[Th,=s John\], \[Po.l,io. TOLoc (\[Th,oS John\], \[L .... io, Store\])\])\] 
(ii) \[s.,., GOLo? (\[T~,.s Mary\], \[,o.i,o. TOLo? (\[T~,., Mary\], \[~o?.,o. e\])\])\]' 
Although temporal connectives are not discussed in the theory proposed by \[Jackendoff, 
1983, 1990\], it is assumed that these two structures are related by means of a lexical- 
semantic token corresponding to the temporal relation between the two events. 
As it turns out, this Jackendoff-style xical-semantic representation is sufficient for 
the purposes of producing an interlingual representation for machine translation. (See, 
for example, \[Dorr, 1990b, 1990c\].) However, a richer lexical-semantic representation is 
necessary in order to accommodate a framework in which predicates of different empo- 
ral/aspectual categories are readily distinguished. Although the lexical-semantic repre- 
sentation provided by Jackendoff distinguishes between events and states, this distinction 
alone is not sufficient for choosing among similar predicates that occur in different empo- 
ral/aspectual categories. In particular, events can be further subdivided into more specific 
types so that non-atomic events (i.e., events that are allowed to have an extended inter- 
pretation) such as destroy can be distinguished from atomic events (i. e., events that never 
have an extended interpretation) such as obliterate. This distinction is not captured in 
Jackendoff's framework, but it is a crucial distinction given that these two similar words 
cannot be interchangeably used in all temporal/aspectual contexts: 
John destroyed the house { for an hour. } (12) (i) until Jack arrived. 
(ii) * John obliterated the house until Jack arrived. 
Such distinctions are omitted in \[Yip, 1985\], where all events are merged under the single 
heading dynamic, and a constraint on temporal interpretations is applied uniformly across 
all verbs in this category. However, as we have seen above in example (12), it cannot be the 
case that the temporal interpretations of all dynamic verbs adhere to the same constraints 
given that  the temporal/aspectual distributions are not identical. 
A number of lexical-semantic representations have been proposed that more readily 
accommodate mporal/aspectual distinctions. In particular, \[Dowty, 1979\] and \[Vendler, 
1967\] have proposed an aspectually oriented lexical-semantic structure that provides a 
four-way classification system for verbs: states, activities, achievements, and accomplish- 
ments, each of which has a different degree of telicity (i. e., culminated vs. nonculminated), 
and/or atomicity (i.e., point vs. extended). 5 A similar scheme has been suggested by 
4The empty location denoted by e corresponds to an unrealized argument of the predicate arrive. 
5Dowty's version of this classification collapses achievements and accomplishments into a single event 
type called a tranaltlon, which covers both the point and extended versions of the event ype. The 
rationale for this move is that ~11 events have 8ome duration, even in the case of so-called punctual 
events, depending on the granularity of time involved. (See \[Passonneau, 1988\] for an adaptation of this 
scheme as implemented in the PUNDIT system.) For the purposes of this discussion, we will maintain 
the distinction between achievements and accomplishments. 
255 
Dototy; Vendler; Bennett; Mourelatos; Jackendoff Examples 
Passonneau Moens ~ Comrie; Bach; 
\] Steedman Pustejovsky 
State I-d\] State State (BE) be, like, know 
Activity (point) \[+d,-t,+a\] Process Event (GO, STAY) tap, wink 
Activity (extended) 
Achievement 
Accomplishment 
\[+d,-t,-a\] 
\[+d,+t,+a\] 
t+d,+t,-a\] 
Process 
Event 
Event 
Event (GO, STAY) 
Event (GO, STAY) 
Event (GO, STAY) 
swim, run 
obliterate, kill 
destroy, give 
Figure 2: Proposals for Lexical-Semantic Frameworks that Accommodate Tense/Aspect 
\[Bach, 1986\] and \[Pustejovksy, 1989\] (following \[Mourelatos, 1981\] and \[Comrie, 1976\]) in 
which actions are classified into states, processes, and events. 
In light of these observations, the lexicM-semantic structure adopted for UNITRAN 
is an augmented form of Jackendoff's representation i which events are distinguished 
from states (as before), but they are further subdivided into activities, achievements, and 
accomplishments. The subdivision is achieved by means of three features proposed by 
\[Bennett et al, 1990\] following the framework of \[Moens and Steedman, 1988\] (in the 
spirit of \[Dowty, 1979\] and \[Vendler, 1967\]): ~dynamic (i.e., events vs. states, as in the 
Jackendoff ramework), :t:telic (i. e., culminative vents (transitions) vs. nonculminative 
events (activities)), and ~atomic (i.e., point events vs. extended events). This featural 
system is imposed on top of the lexical-semantic framework proposed by Jackendoff. For 
example, the primitive GO would be annotated with the features \[+d,+t,-a\] for the verb 
destroy, but \[+d,+t,+a\] for the verb obliterate, thus providing the appropriate distinction 
for cases such as (12). 
Figure 2 relates the four types of lexical-semantic frameworks outlined above. Note 
that the system of features proposed by \[Bennett et al, 1990\] and \[Moens and Steedman, 
1988\] provide the finest tuning given that five distinct categories of predicates are identi- 
fied by the feature settings. (This system is essentially equivalent to the Dowty/Vendler 
proposal, but features are used to distinguish the categories more precisely.) In the 
next section, we will see how the tense and aspect structure described in section 2.1 
and the lexicM-semantic representation described in this section are combined to provide 
the framework for generating a target-language surface form. 
2 .3  Combin ing  Tense /Aspect  w i th  Lex ica l  Semant ics  
The tense/aspect omponent of UNITRAN operates in tandem with the lexical-semantic 
component in order to provide a surface form that corresponds to the interlingual repre- 
sentation. Tense information is linked with the lexical-semantic representation by means 
of Allen's temporal relations, e These temporal relations are assumed to be determined 
from the context in which the source-language s ntence is uttered or, perhaps, from some 
knowledge source such as a database with temporal information. 
For example, if we determine from a knowledge source that event E1 John went to the 
store and event E2 Mary arrived have both occurred in the past, then the time of the 
sit is assumed that the reader is familiar with Allen's 13 notational relations: > (after), < (before), = 
(equal), m (meets), mi (is met by), o (overlaps), oi (is overlapped by), d (during), di (contains), s (starts), 
si (is started by), f (finishes), and fi (is finished by). 
256 
linguistic utterance S is after the two event times. 7 This means that the only possible 
BTS's (for both E1 and E2) are: E ,RS  (past), E S,R (present perfect), and E R S 
(past perfect). In each of these three cases, the event time E and the speech time S 
are separated by (at least one) line, thus providing a temporal interpretation i which E 
occurs before S. 
Figure 3 illustrates the combination of the two BTS's into nine possible complex tense 
structures. (One component of the aspectual representation, simple vs. perfect, is included 
as well.) The CDTS rules out four of the nine possibilities leaving the following five cases: s 
(13) John went to the store when Mary arrived. 
(14) John went to the store when Mary had arrived. 
(15) John has gone to the store when Mary has arrived. \[as in: Typically, John has gone ...\] 
(16) John had gone to the store when Mary arrived. 
(17) John had gone to the store when Mary had arrived. 
Now that the constraint proposed by Hornstein has pared down the possibilities for the 
tense combinations, we can further constrain the choices of surface sentences by selecting 
the most accurate description of the temporal relation between the two events for the 
connective when. Suppose we have the additional information from the knowledge source 
that the "going to the store" event occurs before the "arriving" event; in terms of Allen's 
notation, this would be specified as E1 < E2. The two sentences that guarantee this 
relation are (16) and (17). We can ensure that only these two realizations are selected 
for the when connective by using the E1 < E2 relation as an index into a table that 
associates the aspectual information of the two events with the when connective. The 
aspectual information in this table includes the featural specifications (i.e., :t:dynamic, 
?relic, :t:atomic) for the two lexical-semantic tokens (i.e., GOLoc in both cases) as well 
as the perspective of the two events (i.e., simple, progressive, or perfective). A portion of 
such a table is shown in figure 4. From this table we see that there are only two possible 
aspectual realizations for the two GOLoc events in the current example: since both events 
are associated with the features \[+d,+t,-a\] nd since there are only five legal tense/aspect 
combinations to choose from, the only admissible aspectual realizations are a perfective 
matrix clause with a simple adjunct clause, or a perfective matrix clause with a perfective 
adjunct clause. Thus, sentences (16) and (17) are selected as legal possibilities for the 
surface sentence. 
Both \[Brent, 1990\] and \[Yip, 1985\] have attempted to compile a table along the lines 
of the one shown in figure 4. However, in the case of Yip, the perfective aspect is omitted, 
thus excluding sentences such as (16) and (17) as possible surface sentence forms. In the 
case of Brent, the table is calculated irrespective of the lexical connective, thus giving 
rise to spurious temporal assignments such as E1 = E2 for the temporal interpretation 
of sentences uch as John went to the store before Mary arrived. Furthermore, only 
punctual events are considered; interval events are entirely ignored in Brent's analysis 
of temporal connectives. Neither Yip nor Brent take telicity or atomicity into account 
in the construction of the connective table. However, as we will see in section 3, these 
features are important for providing a cross-linguistically applicable framework for tense 
and aspect in a machine translation model. 
7We are assuming that the time of the linguistic utterance S refers to the present time. 
SAnalogous results would be obtained if we were to switch the matrix and adjunct clauses for this 
example, although this is not always the case. 
257 
Past / Past : John went to the store when Mary arrived. 
E 1,R1 S 1 aspect 1 =s imple  I I J E 2,R2 S s aspect s : s imp le  
ii. Past / Present Perfect : * John went to the store when Mary has arrived. 
E l ,  R1 S 1 aspect 1 = simple 
E ~_~_~, Ks aspect s : perfective J 
iii. Past / Past Perfect : John went to the store when Mary had arrived. 
E l ,R1  SI aspect I = simple 
I I \] Es_Rs  $2 aspect s = perfective 
iv. Present Perfect / Past : * John has gone to the store when Mary arrived. 
El S 1, R 1 aspect 1 = perfective "~ 
J Es, R~ ~ aspect s simple 
v. Present Perfect / Present Perfect : John has gone to the store when Mary has arrived. 
El S I, R 1 aspect I = perfective 1 
I I \] E2 $2, R 2 aspect 2 = perfective 
vi. Present Perfect / Past Perfect : * John has gone to the store when Mary had arrived. 
E1 Sl, R 1 aspect 1 : perfective \] 
ES KS ~ z aspect s = perfective 
vii. Past Perfect / Past : John had gone to 
E s ,R2  S 2 aspect 2 = simple 
the store when Mary arrived. 
viii. Past Perfect / Present Perfect : * John 
E1 .R i  S1 aspect 1 : perfective 
E2_  2, R2 aspects perfective 
had gone to the store when Mary has arrived. 
1 
ix. Past Perfect / Past Perfect : John had gone to the store when Mary had arrived. 
E1 RI S 1 aspect 1 : perfective \] 
I I 
E2 l:ts S s aspect 2 : perfective 
Figure 3: Nine Possible Tense/Aspect Combinations for Two Events Occurring in the Past 
When Matrix Adj=nct 
Perspective Perspective Relation Type 
E1 = E2 \[::kd,-t,:i=a\] 
E1 < E2 \ [~d, - t ,~a\ ]  
El  > E2 \[::t=d,+t,~a\] 
E1 mi  E2 :t:d,+t,::ka\] 
E1 di E2 ::kd,+t,:ka\] 
E1 fi E2 +d,+t ,~a\ ]  
E1 si E2 -d , - t ,~a\]  
E1 m Es -d,=kt,~a\] 
s imple,  progressive,  perfect ive 
perfect ive 
s imple 
s imple 
s imple,  progressive 
progressive 
s imple 
s imple 
Type 
\[+d,+t,::l=a\] 
\ [+d,+t ,~a\ ]  
\[+d,+t,?a\] 
\[+d,+t,?a\] [ +d,+t,?a\] +d,+t,::ka\] 
+d,+t,::i:a\] 
\[+d,+t,=\[:a\] 
s imple,  progressive,  perfect ive 
s imple,  perfect ive 
s imple 
s imple 
s imple 
s imple 
s imple 
s imple 
Figure 4: Temporal Relations Allowed for the When Connective 
258 
Now that we have looked at the constraints on the choice of temporal/aspectual real- 
izations for the matrix and adjunct clauses with respect o the when temporal connective, 
there is still the question of how the temporal connective is selected in the first place. For 
example, without any additional information, we are unable to determine, unambiguously, 
the relation between the clauses of the sentences shown in figure 1: 
(18) \[s.,.t GOLoc (\[Thins John\], \[po.t,o, TOLoc (\[Vhi.s John\], \[t.o?,tio, Store\])\])\] 1 \[~,.., GOLoc (\[Th,., Mary\], \[po.i.o. TOLoc (\[Wh,os Mary\], \[co..,,?. e\])\])\] \] Potential Temporal Relations : >, =, mi, 
In particular, the connective when may correspond to any of the three relations (i.e., >, 
=, or mi) depending on the intended temporal interpretation of the associated events. 
The mapping between temporal relations and temporal connectives i a problem in 
both directions: on the one hand, connectives that are in a one-to-many relation with 
Allen's temporal relations (e.g., when, which corresponds to >, =, and mi) create a prob- 
lem for the analysis of the source-language s ntence; on the other hand, the temporal 
relations that are in a one-to-many relation with potential temporal connectives (e.g., 
the >, which corresponds to after and when) create a problem for the generation of the 
target-language s ntence. The next section addresses how to control this abundance of 
selection possibilities. 
3 Classification of Connectives: Cross-Linguistic Ap- 
plicability 
The when construction has been studied extensively in the literature. In particular, it has 
been noticed that the aspectual properties of the clauses conjoined by when often change 
the temporal meaning of the entire sentence. (See \[Moens and Steedman, 1988\] and \[Yip, 
1985\] among others.) Allen's temporal logic falls short in this regard: it does not capture 
well-known patterns of tense implications based on aspectual distinctions. 9 
Some examples of when constructions (taken from \[Yip, 1985\]) are shown here: 1? (Note 
that the word when potentially maps to more than one temporal relation if we adopt 
Allen's framework.) 
(19) Simple process with simple event: >, =, mi 
John left when Mary axrived. 
(20) Progressive process with simple event: di, fi 
John was leaving when Mary arrived. 
(21) Simple state with simple event: mi, si, di, m, >, <, = 
John was angry when Mary arrived. 
(22) Progressive process with progressive event: =, f, fi, s, si, d, di, o, oi 
John was leaving when Mary was arriving. 
9An example of a tense implication that is not captured in Allen's approach is that a progressive form 
of a process entails the negation of the perfect form (e.g., John i8 building a house implies John has not 
built the house). See \[Yip, 1985\] for a description of other tense implications that are not captured in 
Allen's approach. 
1?This is a modified version of the enumerated constructions i  \[Yip, 1985\]. Certain changes have been 
made to accommodate differences in interpretative judgments (by native English speakers) for the data 
given in Yip's presentation. 
259 
As it turns out, the temporal relations enumerated in figure 4 for the when connective 
cover the cases shown here, plus many others that are not addressed by Yip. There are 
two important advantages to using the featural scheme of figure 4 over the less specific 
scheme of \[Yip, 1985\]: (1) it provides a more precise specification of states and events; 
and (2) it includes temporal/aspectual information that is important for the realization 
of other types of surface-structure constituents (besides temporal connectives) uch as the 
number of a verbal object. We will return to this second point shortly. 
Although the when construction has been studied extensively, it has not been exam- 
ined in the context of machine translation. In particular, the question of whether the 
tense/aspect theories of Allen, Bennett, et al, Dowty, Reichenbach, Vendler, and others 
can be combined to provide a cross-linguistic account for the selection of tense/aspect 
and temporal connectives such as when (e.g., in an interlingual translation model) has 
not been addressed. 
Machine translation provides an appropriate testbed for trying out such theories. The 
problem of lexical selection during generation of the target language is the most crucial 
issue in this regard. For example, one must choose between the lexical tokens cuando 
and al when generating an equivalent Spanish temporal connective for the following two 
English sentences: 
(23) (i) John went to the store when Mary arrived. 
(ii) John had gone to the store when Mary arrived. 
In the case of (23)(i), there are two possible translations, one that uses the connective 
cuando, and one that uses the connective al: 
(24) (i) Juan rue a la tienda cuando Marfa lleg6. 
(ii) Juan fue a la tienda al llegar Marfa. 
Either one of these sentences i an acceptable translation for (23)(i). However, the same 
is not true of (23)(ii)? 1 
(25) (i) Juan habfa ido a la tienda cuando Marfa lleg6. 
(ii) Juan habfa ido a la tienda al llegar Marfa. 
Sentence (25)(i) is an acceptable translation of (23)(ii), but (25)(ii) does not mean the 
same thing as (23)(ii). This second sentence implies that John has already gone to the 
store and come back, which is not the preferred reading. 
Currently research is under way to enumerate all of the English temporal connectives 
(taken from Webster's on-line dictionary) in order to establish an association between 
these connectives and the aspectual interpretation for the matrix and adjunct events (in 
the manner shown in figure 4). These tables vary from language to language, but the 
procedure for choosing temporal connectives applies cross-linguistically once the tables 
for each language are compiled. For example, the table for the Spanish connective al 
would be similar to the table for the English connective when in figure 4 except that the 
specification for the "<" relation would require the matrix event to have the +relic feature 
(i.e., the matrix action must reach a culmination). Thus, the full type entry under the 
matrix clause for the word al would be \[:kd,+t,:ka\]. This would account for the distinction 
between cuando and al in sentences (25)(i) and (25)(ii) above. 
Space limitations do not permit the enumeration of the other temporal connective 
tables. Some examples of connectives that are currently being compiled into tables are: 
after, as soon as, at the moment hat, before, between, during, since, so long as, until, 
11I am indebted to Jorge Lobo for pointing this out to me. 
260 
while, etc. It is intended that these tables are to be used both for the selection of temporal 
connectives during the generation process (for which the relevant index into the tables 
would be the temporal relation and the lexical-semantic types encoded in the interlingua) 
and for temporal interpretation during the analysis process (for which the relevant index 
into the tables would be the lexical-semantic types and aspectual perspectives associated 
with the source-language sentence). The selection of a temporal connective, then, is simply 
a table look-up procedure based on the type of the events and the temporal interpretation 
that holds between the events. For example, if we had a \[+d,-t,-a\] event E1 (e.g., run) 
and a \[+d,+t,+a\] event E2 (e.g., arrive), and if we knew that the temporal relation 
between E1 and E~ was m, then searching the when table would fail, but searching the 
until table would succeed, thus allowing a sentence such as John ran until Mary arrived 
to be generated. 
As mentioned earlier, the featural system outlined above provides a framework that is 
appropriate not only for the realization of temporal connectives, but also for the realization 
of other types of temporal/aspectual information. For example, the sentence I stabbed 
Mary could he realized in at least two ways in Spanish: 
(26) (i) Juan le dio pufialadas a Marla 
(ii) Juan le dio una pufialada Marla 
Both of these sentences translate literally to "John gave stab wound(s) to Mary." However, 
the first sentence is the repetitive version of the action (i.e., there were multiple stab 
wounds), whereas the second sentence is the non-repetitive version of the action (i. e., there 
was only one stab wound). This distinction is characterized by means of the atomicity 
feature. In (26)(i), the event is associated with the features \[+d,+t,-a\], whereas, in (26)(ii) 
the event is associated with the features \[+d,+t,+a\]. According to \[Bennett et al, 1990\] (in 
the spirit of \[Moens and Steedman, 1988\]), predicates are allowed to undergo an atomicity 
"coercion" in which an inherently non-atomic predicate (such as dio) may become atomic 
under certain conditions. These conditions are language-specific in nature, i.e., they 
depend on the lexical-semantic structure of the predicate in question. Given the featural 
scheme that is imposed on top of the lexical-semantic framework, it is easy to specify 
coercion functions for each language. For example, the atomicity function for the stab 
example would specify that a singular NP verbal object maps a \[+d,-a\] predicate into a 
\[+d,+a\] predicate i.e., a non-atomic event becomes atomic if it is associated with a singular 
NP object. Thus, the notion of feature-based coercion is cross-linguistically applicable, 
providing a useful foundation for a model of interlingual machine translation. 
4 Extension of the Tempora l /Aspectua l  Framework 
to the Spatial Domain 
In addition to investigating the cross-linguistic applicability of the temporal/aspectual 
framework in the context of machine translation, current research is under way to extend 
the representation to the spatial domain. This possibility has also been investigated 
by \[Mukerjee and Joe, 1990\], in which the interval ogic model of \[Allen, 1983\] has been 
extended to be applicable to the spatial domain. For example, the one-dimensional interval 
relation C(++)B specifies that the spatial interval C is, in some sense, "after" the spatial 
interval B. This relation is analogous to Allen's temporal relation C > B, which specifies 
that the temporal interval C occurs after the temporal interval B. Other spatial relations 
261 
that are currently under investigation are: above, before, behind, between, beyond, down, 
following, nezt to, off, on, to, under, within, etc. It is expected that the principles that 
govern the relation of temporal primitives to lexical items will hold for analogous primitives 
in the spatial field; experiments are currently being conducted to test this hypothesis. 
5 Summary 
This paper has examined a two-level knowledge representation model for machine transla- 
tion that integrates the tense and aspect information based on theories by both Hornstein 
(in the spirit of Reichenbach) and Allen with lexical-semantic information based on the- 
ories by Jackendotf in the spirit of Dowty and Vendler. We have examined the question 
of cross-linguistic applicability showing that the integration of tense and aspect with 
lexical-semantics is especially critical in machine translation when there are a number of 
temporal/aspectual possibilities that may be generated from a lexical semantic represen- 
tation. Finally, we have discussed the possibility of extending the temporal notation to 
the spatial domain. 
6 Acknowledgements 
This paper describes research done at the University of Maryland Institute for Advanced 
Computer Studies. Useful guidance and commentary during the research and preparation 
of this document were provided by Gary Coen, Bruce Dawson, Terry Gaasterland, Ken 
Hale, Norbert Hornstein, Jorge Lobo, Paola Merlo, Jeff Siskind, and Amy Weinberg. 
References 
\[Allen, 1983\] James F. Allen. Maintaining knowledge about emporal intervals. Communications o\] the 
A CM, 26(11):832-843, 1983. 
\[Allen, 1984\] James F. Allen. Towards a general theory of action and time. Artificial Intelligence, 
23(2):123-160, 1984. 
\[Bach, 1986\] Emmon Bach. The algebra of events. Linguistics and Philosophy, 9:5-16, 1986. 
\[Bennett e al., 1990\] Winfield S. Bennett, Tanya Herlick, Katherine Hoyt, Joseph Liro, and Ann San- 
tistehan. A computational model of aspect and verb semantics. Machine Translation, 4(4):247-280, 
1990. 
\[Brent, 1988\] Michael R. Brent. Decompositional semantics and argument expression i  natural lan- 
guage. Master's thesis, Massachusetts Institute of Technology, Department of Electrical Engineering 
and Computer Science, Cambridge, MA, 1988. 
\[Brent, 1990\] Michael R. Brent. A simplified theory of tense representations and constraints on their 
composition. In Proceedings of the ~8th Annual Conference of the Association \]or Computational 
Linguistics, University of Pittsburgh, Pittsburgh, PA, 1990. 
\[Comrie, 1976\] Bernard Comrie. Aspect. Cambridge University Press, Cambridge, England, 1976. 
\[Dorr, 1987\] Bonnie J. Dorr. Unitran: A principle-based approach to machine translation. Master's 
thesis, MIT AI Technical Report 1000, Department of Electrical Engineering and Computer Science, 
Cambridge~ MA, 1987. 
\[Dorr, 1989\] Bonnie J. Dorr. Lexical conceptual structure and generation i  machine translation. In MIT 
AI Memo 1160, Proceedings of the Ninth Annual Con\]erence o\] the Cognitive Science Society, Ann 
Arbor, MI, 1989. 
262 
\[Dorr, 1990a\] Bonnie J. Dorr. Solving thematic divergences in machine translation. In Proceedings of the 
~8th Annual Conference of the Association for Computational Linguistics, pages 127-134, University 
of Pittsburgh, Pittsburgh, PA, 1990. 
\[Dorr, 1990b\] Bonnie J. Dorr. A cross-linguistic approach to machine translation. In Proceedings of the 
Third International Conference on Theoretical and Methodological Issues in Machine Translation of 
Natural Languages, pages 13-32, Linguistics Research Center, The University of Texas, Austin, TX, 
1990. 
\[Dorr, 1990c\] Bonnie J. Dorr. Lcxlcal Conceptual Structure and Machine Translation. PhD thesis, Mas- 
sachusetts Institute of Technology, Department ofElectrical Engineering and Computer Science, Cam- 
bridge, MA, 1990. 
\[Dowty, 1979\] David Dowty. Word Meaning and Montague Grammar. Reidel, Dordrecht, Netherlands, 
1979. 
\[Hinrichs, 1988\] Erhard W. Hinrichs. Tense, quantifiers, and contexts. Computational Linguistics, 
14(2):3-14, 1988. 
\[Hornstein, 1990\] Norhert Horustein. As Time Goes By. MIT Press, Cambridge, MA, 1990. 
\[Jackendoff, 198.3\] Ray S. Jackendoff. Semantics and Cognition. MIT Press, Cambridge, MA, 1983. 
\[Jackendoff, 1990\] Ray S. Jackendoff. Semantic Structures. MIT Press, Cambridge, \]VIA, 1990. 
\[Levin and Rappaport, 1985\] Beth Levin and Malka Rappaport. The formation of adjectival passives. 
Lexicon Project Working Papers 2, Massachusetts Institute of Technology, Center for Cognitive Science, 
Cambridge, MA, 1985. 
\[Maybury, 1990\] Mark T. Maybury. Using discourse focus, temporal focus, and spatial focus to plan 
narrative text. In Proceedings of the Fifth International Workshop on Natural Language Generation, 
pages 70-78, Dawson, Pennsylvania, 1990. 
\[Moens and Steedman, 1988\] Marc Moens and Mark Steedman. Temporal ontology and temporal refer- 
ence. Computational Linguistics, 14(2):15-28, 1988. 
\[Mourelatos, 1981\] Alexander Mourelatos. Events, processes and states. In Tense and Aspect, Academic 
Press, New York, NY, 1981. 
\[Mukerjee and Joe, 1990\] Amitabha Mukerjee and Gene Joe. A qualitative model for space. In Proceed- 
ings of the Ninth Annual Conference of the American Association of Artificial Intelligence, Boston, 
MA, 1990. 
\[Nakhimovsky, 1988\] Alexander Nakhimovsky. Aspect, aspectual class, and the temporal structure of 
narrative. Computational Linguistics, 14(2):29-43, 1988. 
\[Passonneau, 1988\] Rebecca J. Passonneau. A computational model of the semantics of tense and aspect. 
Computational Linguistics, 14(2):44-60, 1988. 
\[Pustejovksy, 1988\] James Pustejovksy. The geometry of events, in Lexicon Project Working Papers 24, 
Massachusetts Institute of Technology, Center for Cognitive Science, Cambridge, MA, 1988. 
\[Pustejovksy, 1989\] James Pustejovksy. The semantic representation f lexical knowledge. In Proceedings 
of the First Annual Workshop on Lexical Acquisition, IJCAI-89, Detroit, Michigan, 1989. 
\[Reichenbach, 1947\] H. Reichenbach. Elements of Symbolic Logic. Macmillan, London, 1947. 
\[Siskind, 1989\] Jeffrey Mark Siskind. Decomposition. MIT Computer Science Area Exam Paper, Mas- 
sachusetts Institute of Technology, Cambridge, MA, 1989. 
\[Termy, 1987\] Carol Tenny. Grammaticalizing Aspect and Affectedness. PhD thesis, Massachusetts In- 
stitute of Technology, Department of Electrical Engineering and Computer Science, Cambridge, MA, 
1987. 
\[Tenny, 1989\] Carol Tunny. The aspectual interface hypothesis. Lexicon Project Working Papers 31, 
Massachusetts Institute of Technology, Center for Cognitive Science, Cambridge, MA, 1989. 
\[Vendler, 1967\] Zeno Vendler. Verbs and times. Linguistics in Philosophy, pages 97-121, 1967. 
\[Vilain et al, 1990\] Marc Vilain, Henry Kautz, and Peter van Beek. Constraint propagation algorithms 
for temporalreasoning: A revised report. In Readings in Qualitative Reasoning about Physical Systems, 
Morgan Kaufmann, San Mateo, CA, 1990. 
\[Williams, 1990\] Brian C. Williams. Doing time: Putting qualitative reasoning on firmer ground. In 
Readings in Qualitative Reasoning about Physical Systems, Morgan Kaufmann, San Mateo, CA, 1990. 
\[Yip, 1985\] Kenneth M. Yip. Tense, aspect and the cognitive representation f time. In Proceedings of 
the ~3rd Annual Conference of the Association for Computational Linguistics, pages 18-26, Chicago, 
IL, 1985. 
263 
