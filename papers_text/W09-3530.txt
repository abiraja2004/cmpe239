Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 132?135,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Name Transliteration with Bidirectional Perceptron Edit Models
Dayne Freitag
SRI International
freitag@ai.sri.com
Zhiqiang (John) Wang
SRI International
johnwang@ai.sri.com
Abstract
We report on our efforts as part of the
shared task on the NEWS 2009 Machine
Transliteration Shared Task. We applied
an orthographic perceptron character edit
model that we have used previously for
name transliteration, enhancing it in two
ways: by ranking possible transliterations
according to the sum of their scores ac-
cording to two models, one trained to gen-
erate left-to-right, and one right-to-left;
and by constraining generated strings to
be consistent with character bigrams ob-
served in the respective language?s train-
ing data. Our poor showing in the of-
ficial evaluation was due to a bug in
the script used to produce competition-
compliant output. Subsequent evaluation
shows that our approach yielded compara-
tively strong performance on all alphabetic
language pairs we attempted.
1 Introduction
While transliteration is a much simpler prob-
lem than another linguistic transduction prob-
lem, language translation, it is rarely trivial. At
least three phenomena complicate the automatic
transliteration between two languages using dif-
ferent scripts?differing phoneme sets, lossy or-
thography, and non-alphabetic orthographies (e.g.,
syllabaries).
For most language pairs, these difficulties stand
in the way of a rule-based treatment of the prob-
lem. For this reason, many machine learning ap-
proaches to the problem have been proposed. We
can draw a rough distinction between learning ap-
proaches that attempt to model the phonetics of
a transliteration problem explicitly, and those that
treat the problem as simply one of orthographic
transduction, leaving it to the learning algorithm
to acquire phonetic distinctions directly from or-
thographic features of the training data. For exam-
ple, Knight and Graehl (1998) address the prob-
lem through cascaded finite state transducers, with
explicit representations of the phonetics. Sub-
sequently, Al-Onaizan and Knight (2002) realize
improvements by adding a ?spelling? (i.e., ortho-
graphic) model. There has been an increasing em-
phasis on purely orthographic models, probably
because they require less detailed domain knowl-
edge (e.g., (Lee and Chang, 2003)).
2 Approach
The approach we explored as part of the NEWS
2009 Machine Transliteration Shared Task (Li et
al., 2009) is strictly orthographic. We view the
conversion of a name in one language to its rep-
resentation in another as the product of a series
of single-character edits, and seek to learn a char-
acter edit model that maximizes the score of cor-
rect name pairs. Our approach follows that de-
scribed in Freitag and Khadivi (2007), a ?struc-
tured perceptron? with cheaply computed charac-
ter n-grams as features. Here, we give a brief
description, and present the successful enhance-
ments we tried specifically for the shared task.
2.1 Perceptron Edit Model
Suppose we are given two sequences, sm1 ? ??s
and tn1 ? ??t . We desire a function A(s, t) 7? N
which assigns high scores to correct pairs s, t. If
we stipulate that this score is the sum of the indi-
vidual scores of a series of edits, we can find the
highest-scoring such series through a generaliza-
tion of the standard edit distance:
A(si1, tj1) =
max
?
?
?
?
?
a?,tj(s, i, t, j) + A(si1, tj?11 )
asi,?(s, i, t, j) + A(si?11 , t
j
1)
asi,tj(s, i, t, j) + A(si?11 , t
j?1
1 )
(1)
132
with A(?, ?) = 0. The function asi,tj (s, i, t, j)
represents the score of substituting tj for si; a?,tj
and asi,? represent insertion and deletion, respec-
tively.
In the experiments reported in this paper, we as-
sume that each local function a is defined in terms
of p + q features, {f1, ? ? ? , fp, fp+1, ? ? ? , fp+q},
defined over the source and target alhabets, re-
spectively, and that these features have the func-
tional form ?? ?N 7? R.
In this paper we exclusively use character n-
gram indicator features. The ?order? of a model
is the size of the largest n-grams; for a model of
order 2, features would be the bigrams and uni-
grams immediately adjacent to a given string posi-
tion. Since the shared task is to generate target
strings, only features for preceding n-grams are
used in the target language.
The score of a particular edit is a linear combi-
nation of the corresponding feature values:
a(s, i, t, j) =
p
?
k=1
?k ?fk(s, i)+
p+q
?
k=p+1
?k ?fk(t, j)
(2)
The weights ?k are what we seek to optimize in
order to tune the model for our particular applica-
tion.
We optimize these weights through an exten-
sion of perceptron training for sequence labeling,
due to Collins (2002). Take ? to be a model pa-
rameterization, and let A?(s, t) return an optimal
edit sequence e, with its score v, given input se-
quences s and t under ?. Elements of sequence
e are character pairs ?cs, ct?, with cs ? ?s ? {?}
and ct ? ?t ? {?}, where ? represents the empty
string. Let ?(s, t,e) be a feature vector for a
source string, target string, and corresponding edit
sequence.
Table 1 shows the training algorithm. Starting
with a zero parameter vector, we iterate through
the collection of source sequences. For each se-
quence, we pick two target sequences, one the
?true? transliteration t of the source string s, and
one chosen by searching for a string t? that yields a
maximal score according to the current model A?
(Line 6). If the model fails to assign t a higher
score than t? (Line 9), we apply the perceptron
training update (Line 10).
Note that because generation constructs the tar-
get sequence, the search in Line 6 for a target
string t? that yields maximal A?(s, t?) is not triv-
ial, and does not correspond to a simple recurrence
1: Given training set S = {?s, t?}
2: V ? [], an empty list
3: ?? 0, a weight vector
4: for some number of iterations do
5: for ?s, t? in S do
6: t? ? maxargt?A?(s, t?)
7: ?e, v? ? A?(s, t)
8: ?e?, v?? ? A?(s, t?)
9: if v? ? v then
10: ?? ?+?(s, t,e)??(s, t?,e?)
11: end if
12: Append ? to V
13: end for
14: end for
15: Return the mean ? from V
Table 1: The training algorithm. A? is the affinity
function under model parameters ?, returning edit
sequence e and score v.
relation like Equation 1. Both in training and test-
ing, we use a beam search for target string gener-
ation. In training, this may mean that we find a t?
with lower score than the correct target t. In such
cases (Line 9 returns false), the model has cor-
rectly ordered the two alternative transliterations,
and does not require updating.
2.2 Shared Task Extensions
This approach has been used effectively for prac-
tical transliteration of names from English to Ara-
bic and Mandarin (and vice versa). As part of
the NEWS shared task, we experimented with two
simple extensions, both of which yielded improve-
ments over the baseline described above. These
extensions were used in our official submission for
alphabetic language pairs. We treated English-to-
Chinese somewhat differently, as described below.
Simple character n-gram constraints. The
described approach sometimes violates target
language spelling conventions by interpolating
clearly inappropriate characters into a string that
is otherwise a reasonable transliteration. We take
this behavior as symptomatic of a kind of under-
training in some portion of the problem space,
a possible byproduct of 1-best perceptron train-
ing. One principled solution may be to optimize
against n-best lists (Bellare et al, 2009).
Instead, we address this shortcoming in a
straightforward way?by prohibiting the creation
of n-grams, for some small n, that do not occur
133
in the training data. Under a bigram restriction, if
?ab? is not seen in training, then an operation that
inserts ?b? after ?a? is disallowed. In essence, we
impose a very simple character language model of
the target domain.
Our non-standard English-to-Chinese contribu-
tions, which involved transliterating from English
to pinyin, employed a similar idea. In these exper-
iments, rather than character bigrams, the model
was constrained to produce only legal pinyin se-
quences.
Bidirectional generation. Character n-gram
restrictions yielded modest but universal improve-
ments on development data. Larger improvements
were obtained through an equally simple idea: In-
stead of a single left-to-right model, we trained
two models, one generating left-to-right, the other
right-to-left, each model constrained by n-gram ta-
bles, as described above. At evaluation time, each
of the constituent models was used to generate a
large number of candidate strings (100, typically).
All strings in the union of these two sets were then
assigned a score, which was the unweighted sum
of scores according to the constituent models, and
reranked according to this score. The 10 highest-
scoring were retained for evaluation.
A buggy implementation of this two-model idea
accounts for our poor showing in the official eval-
uation. Because of a trivial error in the script we
used to produce output, right-to-left models were
treated as if they were left-to-right. The resulting
strings and scores were consequently erroneous.
3 Evaluation
We experimented with models of order 2 and
3 (2-gram and 3-gram features) on shared task
data for English to Hindi, Kannada, Russian, and
Tamil (Kumaran and Kellner, 2007). Based on
development accuracy scores, we found models
of order 3 to be consistently better than order 2,
and our submitted results use only order-3 models,
with one exception. English-to-native-Chinese (Li
et al, 2004) was treated as a special case. Using
trigram features in the target language results in
an explosion in the feature space, and a model that
is slow to train and performs poorly. Thus, only
for this language pair, we devised a mixed-order
model, one using trigram features over English
strings, and unigram features over Chinese. Be-
cause of the large target-language branching fac-
tor, the mixed-order native Chinese model remain-
Languages Accuracy Delta
EnHi 0.465 -0.033
EnHi baseline 0.421 -0.077
EnKa 0.396 -0.002
EnKa baseline 0.370 -0.028
EnRu 0.609 -0.004
EnRu baseline 0.588 -0.025
EnTa 0.475 +0.001
EnTa baseline 0.422 -0.052
EnCh standard 0.672 -0.059
EnCh non-standard 1 0.673 -0.236
EnCh non-standard 2 0.5 -0.409
Table 2: Post-contest accuracy on evaluation set,
including delta from highest-scoring contest par-
ticipant.
ing one of the slowest to train.
We trained all models for 20 epochs, evaluating
the 1-best accuracy of intervening models on the
development data. In all cases, we observed that
accuracy increased steadily for some number of it-
erations, after which it plateaued. Consequently,
for all language pairs, we submitted the predic-
tions of the latest model.
Table 2 lists accuracy reported by the official
evaluation script on the contest evaluation data.
All non-Chinese runs in the table are ?standard,?
and are trained exclusively on shared task train-
ing data. Those labeled ?baseline? are left-to-
right models with no character n-gram constraints.
These results were obtained after release of the
evaluation data, but differ from our official sub-
mission in only two ways: First, and most im-
portantly, the bug described previously was cor-
rected. Second, in some cases training runs that
had not completed at evaluation time were allowed
to run to the full 20 epochs, and the resulting mod-
els were used. The exceptions are Hindi and na-
tive Chinese, each of which reflect performance
at approximately 10 epochs. Without exception, a
beam of size 100 was used to generate these re-
sults.
With the exception of ?EnCh standard,? all
results in the table employed the bidirectional
scheme described above. The Chinese non-
standard runs differ from standard only in that
models were trained to perform English-to-pinyin
transliteration, followed by a conversion from
pinyin to native Chinese using tables provided by
134
the Unicode consortium. Non-standard Run 1 re-
tains pinyin tonal diacritics, while Run 2 omits
them. The mapping from pinyin to native Chi-
nese characters introduces indeterminacy, which
we accounted for in a simple fashion: First, in con-
structing a pinyin-to-Chinese conversion table, we
discarded any Chinese characters that were used
in the training data fewer than some small frac-
tion of cases. Then, given a ranked list of pinyin
transliterations, we generated all possible native
Chinese sequences, ranked by the product of ob-
served pinyin-to-Chinese probabilities, according
to training frequencies.
It will be observed that our non-standard
English-to-Chinese results lag considerably be-
hind the best results. We suspect this is due in
part to the fact that no additional training data was
used in these experiments?only a change in repre-
sentation.
4 Discussion and Conclusion
Treating the transliteration problem as one of or-
thographic transduction appears viable, particu-
larly for conversion between alphabetic languages.
An empirical character edit model based on a
structured perceptron and character n-gram fea-
tures, and using a simple training procedure that
discovers appropriate weights for latent character
alignment operations, yields performance that is
generally as good as alternative approaches ex-
plored in the shared task. The key is model com-
bination, particularly the combination of left-to-
right and right-to-left models, respectively.
In contrast to the alphabetic language pairs,
our performance on Chinese falls somewhat short.
Nevertheless, it is possible that simple modifica-
tions of the basic procedure would render it com-
petitive on English-Chinese, as well. In convert-
ing from English to native Chinese, we relied on
a mixed-order model, with order-3 English fea-
tures. It is possible that trigrams are too small
in some cases to identify the appropriate Chinese
character, and that 4-grams, if we can afford them,
will make the difference. There is virtue in the
idea of transliterating to pinyin as an intermediate
step; converting to tonal pinyin yields accuracy at
the same level as English-to-native-Chinese, even
with the indeterminacy it introduces. Future work
includes more principled approaches to resolving
this indeterminacy, and combined pinyin/native
models.
Acknowledgments
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-
0023 (approved for public release, distribution un-
limited). Any opinions, findings and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the view of DARPA.
References
Y. Al-Onaizan and K. Knight. 2002. Machine translit-
eration of names in Arabic text. In Proceedings of
the ACL-02 workshop on computational approaches
to semitic languages.
K. Bellare, K. Crammer, and D. Freitag. 2009. Loss-
sensitive discriminative training of machine translit-
eration models. In Proceedings of the Student
Research Workshop and Doctoral Consortium at
NLT/NAACL 2009.
M. Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of EMNLP-2002.
D. Freitag and S. Khadivi. 2007. A sequence align-
ment model based on the averaged perceptron. In
Proceedings of EMNLP-CoNLL 2007.
K. Knight and J. Graehl. 1998. Machine translitera-
tion. Computational Linguistics, 24(4).
A. Kumaran and T. Kellner. 2007. A generic frame-
work for machine transliteration. In Proceedings of
the 30th SIGIR.
C.-J. Lee and J.S. Chang. 2003. Acquisition
of English-Chinese transliterated word pairs from
parallel-aligned texts using a statistical machine
transliteration model. In Proceedings of the HLT-
NAACL 2003 Workshop on Building and Using Par-
allel Texts.
H. Li, M. Zhang, and J. Su. 2004. A joint source chan-
nel model for machine transliteration. In Proceed-
ings of the 42nd ACL.
H. Li, A. Kumaran, M. Zhang, and V. Pervouch-
ine. 2009. Whitepaper of NEWS 2009 Machine
Transliteration Shared Task. In Proceedings of ACL-
IJCNLP 2009 Named Entities Workshop (NEWS
2009).
135
