Proceedings of the 6th Workshop on Statistical Machine Translation, pages 71?77,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
AMBER: A Modified BLEU, Enhanced Ranking Metric 
 
Boxing Chen and Roland Kuhn 
National Research Council of Canada, Gatineau, Qu?bec, Canada 
First.Last@nrc.gc.ca 
 
  
  
Abstract 
This paper proposes a new automatic ma-
chine translation evaluation metric: 
AMBER, which is based on the metric 
BLEU but incorporates recall, extra penal-
ties, and some text processing variants. 
There is very little linguistic information in 
AMBER. We evaluate its system-level cor-
relation and sentence-level consistency 
scores with human rankings from the 
WMT shared evaluation task; AMBER 
achieves state-of-the-art performance. 
1 Introduction 
Automatic evaluation metrics for machine transla-
tion (MT) quality play a critical role in the devel-
opment of statistical MT systems. Several metrics 
have been proposed in recent years.  Metrics such 
as BLEU (Papineni et al, 2002), NIST (Dodding-
ton, 2002), WER, PER, and TER (Snover et al, 
2006) do not use any linguistic information - they 
only apply surface matching. METEOR (Banerjee 
and Lavie, 2005), METEOR-NEXT (Denkowski 
and Lavie 2010), TER-Plus (Snover et al, 2009), 
MaxSim (Chan and Ng, 2008), and TESLA (Liu et 
al., 2010) exploit some limited linguistic resources, 
such as synonym dictionaries, part-of-speech tag-
ging or paraphrasing tables. More sophisticated 
metrics such as RTE (Pado et al, 2009) and DCU-
LFG (He et al, 2010) use higher level syntactic or 
semantic analysis to score translations. 
Though several of these metrics have shown bet-
ter correlation with human judgment than BLEU, 
BLEU is still the de facto standard evaluation me-
tric. This is probably due to the following facts: 
1. BLEU is language independent (except for 
word segmentation decisions).  
2. BLEU can be computed quickly. This is im-
portant when choosing a metric to tune an 
MT system. 
3. BLEU seems to be the best tuning metric 
from a quality point of view - i.e., models 
trained using BLEU obtain the highest 
scores from humans and even from other 
metrics (Cer et al, 2010). 
When we developed our own metric, we decided 
to make it a modified version of BLEU whose 
rankings of translations would (ideally) correlate 
even more highly with human rankings. Thus, our 
metric is called AMBER: ?A Modified Bleu, En-
hanced Ranking? metric. Some of the AMBER 
variants use an information source with a mild lin-
guistic flavour ? morphological knowledge about 
suffixes, roots and prefixes ? but otherwise, the 
metric is based entirely on surface comparisons.  
2 AMBER 
Like BLEU, AMBER is composed of two parts: a 
score and a penalty. 
 
penaltyscoreAMBER ?=                 (1)  
 
To address weaknesses of BLEU described in 
the literature (Callison-Burch et al, 2006; Lavie 
and Denkowski, 2009), we use more sophisticated 
formulae to compute the score and penalty. 
2.1 Enhancing the score 
First, we enrich the score part with geometric av-
erage of n-gram precisions (AvgP), F-measure de-
rived from the arithmetic averages of precision and 
recall (Fmean), and arithmetic average of F-
measure of precision and recall for each n-gram 
(AvgF). Let us define n-gram precision and recall 
as follows: 
71
)(#
)(#)(
Tngrams
RTngrams
np ?=               (2) 
  )(#
)(#)(
Rngrams
RTngrams
nr
?
=               (3) 
where T = translation, R = reference.  
Then the geometric average of n-gram preci-
sions AvgP, which is also the score part of the 
BLEU metric, is defined as: 
NN
n
npNAvgP
1
1
)()( ???
?
???
?
= ?
=
             (4) 
 
The arithmetic averages for n-gram precision 
and recall are: 
?
=
=
N
n
np
N
NP
1
)(1)(                       (5) 
?
=
=
M
n
nr
M
MR
1
)(1)(                      (6) 
 The F-measure that is derived from P(N) and 
R(M), (Fmean), is given by: 
)()1()(
)()(),,(
MRNP
MRNPMNFmean
??
?
?+
=     (7) 
 
The arithmetic average of F-measure of preci-
sion and recall for each n-gram (AvgF) is given by: 
?
=
?+
=
N
n nrnp
nrnp
N
NAvgF
1 )()1()(
)()(1),(
??
?    (8) 
The score is the weighted average of the three 
values: AvgP, Fmean, and AvgF. 
),()1(
),,(
)()(
21
2
1
???
??
?
NAvgF
MNFmean
NAvgPNscore
???+
?+
?=
  
 (9) 
The free parameters N, M,? , 1?  and 2?  were  
manually tuned on a dev set.  
2.2 Various penalties 
Instead of the original brevity penalty, we experi-
mented with a product of various penalties: 
?
=
=
P
i
w
i
ipenpenalty
1
                  (10) 
where wi is the weight of each penalty peni.  
Strict brevity penalty (SBP): (Chiang et al, 
2008) proposed this penalty. Let ti be the transla-
tion of input sentence i, and let ri be its reference 
(or if there is more than one, the reference whose 
length in words || ir  is closest to length || it ). Set 
???
?
???
?
?= ?
?
i ii
i i
rt
r
SBP |}||,min{|
||
1exp     (11) 
Strict redundancy penalty (SRP): long sen-
tences are preferred by recall. Since we rely on 
both recall and precision to compute the score, it is 
necessary to punish the sentences that are too long.  
???
?
???
?
?= ?
?
i i
i ii
r
rt
SRP ||
|}||,max{|
1exp      (12) 
Character-based strict brevity penalty 
(CSBP) and Character-based strict redundancy 
penalty (CSRP) are defined similarly. The only 
difference with the above two penalties is that 
here, length is measured in characters. 
Chunk penalty (CKP): the same penalty as in 
METEOR: 
?
? ???
?
???
?
??= )(#
#1
wordmatches
chunksCKP       (13) 
? and ?  are free parameters. We do not compute 
the word alignment between the translation and 
reference; therefore, the number of chunks is com-
puted as )(#)(## wordmatchesbigrammatcheschunks ?= . 
For example, in the following two-sentence trans-
lation (references not shown), let ?mi? stand for a 
matched word, ?x? stand for zero, one or more 
unmatched words:  
S1: m1 m2 x m3 m4 m5 x m6  
S2: m7 x m8 m9 x m10 m11 m12 x m13 
If we consider only unigrams and bigrams, there 
are 13 matched words and 6 matched bigrams (m1 
m2, m3 m4, m4 m5, m8 m9, m10 m11, m11 m12), so there 
are 13-6=7 chunks (m1 m2, m3 m4 m5, m6, m7, m8 m9, 
m10 m11 m12, m13).  
Continuity penalty (CTP): if all matched 
words are continuous, then 
segmentRTgramsn
RTngrams
#)()1(#
)(#
???
?
 equals 1.  
Example: 
S3: m1 m2 m3 m4 m5m6  
S4: m7 m8 m9m10 m11 m12 m13 
There are 13 matched unigrams, and 11 matched 
bi-grams; we get 11/(13-2)=1. Therefore, a conti-
nuity penalty is computed as: 
72
 ???
?
???
?
???
?
?
?= ?
=
N
n segmentRTgramsn
RTngrams
N
CTP
2 #)()1(#
)(#
1
1
exp (14) 
 
Short word difference penalty (SWDP): a 
good translation should have roughly the same 
number of stop words as the reference. To make 
AMBER more portable across all Indo-European 
languages, we use short words (those with fewer 
than 4 characters) to approximate the stop words.   
))(#
||
exp(
runigram
baSWDP ??=           (15) 
where a and b are the number of short words in the 
translation and reference respectively. 
Long word difference penalty (LWDP): is de-
fined similarly to SWDP.  
))(#
||
exp(
runigram
dcLWDP ??=           (15) 
where c and d  are the number of long words (those 
longer than 3 characters) in the translation and ref-
erence respectively. 
Normalized Spearman?s correlation penalty 
(NSCP): we adopt this from (Isozaki et al, 2010). 
This penalty evaluates similarity in word order be-
tween the translation and reference. We first de-
termine word correspondences between the 
translation and reference; then, we rank words by 
their position in the sentences. Finally, we compute 
Spearman?s correlation between the ranks of the n 
words common to the translation and reference. 
)1()1(1
2
?+
?=
?
nnn
d
i i?                (16) 
where di indicates the distance between the ranks 
of the i-th element. For example: 
T: Bob reading book likes
 
R: Bob likes reading book
 
 
The rank vector of the reference is [1, 2, 3, 4], 
while the translation rank vector is [1, 3, 4, 2]. The 
Spearman?s correlation score between these two 
vectors is 
)14(4)14(
)42()34()23(01
222
???+
?+?+?+
?
=0.90. 
In order to avoid negative values, we normalized 
the correlation score, obtaining the penalty NSCP: 
2)1( /?NSCP +=                     (17) 
Normalized Kendall?s correlation penalty 
(NKCP):  this is adopted from (Birch and Os-
borne, 2010) and (Isozaki et al, 2010). In the pre-
vious example, where the rank vector of the 
translation is [1, 3, 4, 2], there are 624 =C  pairs of 
integers. There are 4 increasing pairs: (1,3), (1,4), 
(1,2) and (3,4). Kendall?s correlation is defined by:  
1
#
#2 ??=
pairsall
pairsasingincre
?         (18) 
Therefore, Kendall?s correlation for the transla-
tion ?Bob reading book likes? is 16/42 ?? =0.33. 
Again, to avoid negative values, we normalized 
the coefficient score, obtaining the penalty NKCP: 
2)1( /NKCP ?+=                     (19) 
2.3 Term weighting 
The original BLEU metric weights all n-grams 
equally; however, different n-grams have different 
amounts of information. We experimented with 
applying tf-idf to weight each n-gram according to 
its information value. 
2.4 Four matching strategies 
In the original BLEU metric, there is only one 
matching strategy: n-gram matching. In AMBER, 
we provide four matching strategies (the best 
AMBER variant used three of these): 
1. N-gram matching: involved in computing 
precision and recall. 
2. Fixed-gap n-gram: the size of the gap be-
tween words ?word1 [] word2? is fixed; 
involved in computing precision only. 
3. Flexible-gap n-gram:  the size of the gap 
between words ?word1 * word2? is flexi-
ble; involved in computing precision only. 
4. Skip n-gram: as used ROUGE (Lin, 2004); 
involved in computing precision only. 
2.5 Input preprocessing 
The AMBER score can be computed with different 
types of preprocessing. When using more than one 
type, we computed the final score as an average 
over runs, one run per type (our default AMBER 
variant used three of the preprocessing types): 
?
=
=
T
t
tAMBER
T
AMBERFinal
1
)(1_
 
We provide 8 types of possible text input: 
0. Original - true-cased and untokenized. 
73
1. Normalized - tokenized and lower-cased.  
(All variants 2-7 below also tokenized and 
lower-cased.)  
2. ?Stemmed? - each word only keeps its first 
4 letters. 
3. ?Suffixed? - each word only keeps its last 
4 letters. 
4. Split type 1 - each longer-than-4-letter 
word is segmented into two sub-words, 
with one being the first 4 letters and the 
other the last 2 letters. If the word has 5 
letters, the 4th letter appears twice: e.g., 
?gangs? becomes ?gang? + ?gs?. If the 
word has more than 6 letters, the middle 
part is thrown away 
5. Split type 2 - each word is segmented into 
fixed-length (4-letter) sub-word sequences, 
starting from the left.  
6. Split type 3 - each word is segmented into 
prefix, root, and suffix. The list of English 
prefixes, roots, and suffixes used to split 
the word is from the Internet1; it is used to 
split words from all languages. Linguistic 
knowledge is applied here (but not in any 
other aspect of AMBER).  
7. Long words only - small words (those with 
fewer than 4 letters) are removed. 
3 Experiments 
3.1 Experimental data 
We evaluated AMBER on WMT data, using WMT 
2008 all-to-English submissions as the dev set. 
Test sets include WMT 2009 all-to-English, WMT 
2010 all-to-English and 2010 English-to-all sub-
missions. Table 1 summarizes the dev and test set 
statistics. 
Set Dev Test1 Test2 Test3 
Year 2008 2009 2010 2010 
Lang. xx-en xx-en xx-en en-xx 
#system 43 39 53 32 
#sent-pair 7,861 13,912 14,212 13,165 
Table 1: statistics of the dev and test sets. 
                                                           
1http://en.wikipedia.org/wiki/List_of_Greek_and_Latin_roots_
in_English 
3.2 Default settings 
Before evaluation, we manually tuned all free pa-
rameters on the dev set to maximize the system-
level correlation with human judgments and de-
cided on the following default settings for 
AMBER:   
1. The parameters in the formula  
),()1(
),,(
)()(
21
2
1
???
??
?
NAvgF
MNFmean
NAvgPNscore
???+
?+
?=
 
are set as  N=4, M=1, ? =0.9, 1? = 0.3 
and 2? = 0.5.  
2. All penalties are applied; the manually set 
penalty weights are shown in Table 2. 
3. We took the average of runs over input text 
types 1, 4, and 6 (i.e. normalized text, 
split type 1 and split type 3).  
4. In Chunk penalty (CKP), 3=? , and 
? =0.1. 
5. By default, tf-idf is not applied.  
6. We used three matching strategies: n-gram, 
fixed-gap n-gram, and flexible-gap n-
gram; they are equally weighted. 
 
Name of penalty Weight value 
SBP 0.30 
SRP 0.10 
CSBP 0.15 
CSRP 0.05 
SWDP 0.10 
LWDP 0.20 
CKP 1.00 
CTP 0.80 
NSCP 0.50 
NKCP 2.00 
Table 2: Weight of each penalty 
3.3 Evaluation metrics 
We used Spearman?s rank correlation coefficient to 
measure the correlation of AMBER with the hu-
man judgments of translation at the system level. 
The human judgment score we used is based on the 
?Rank? only, i.e., how often the translations of the 
system were rated as better than the translations 
from other systems (Callison-Burch et al, 2008). 
Thus, AMBER and the other metrics were eva-
luated on how well their rankings correlated with 
74
the human ones. For the sentence level, we use 
consistency rate, i.e., how consistent the ranking of 
sentence pairs is with the human judgments. 
3.4 Results 
All test results shown in this section are averaged 
over all three tests described in 3.1. First, we com-
pare AMBER with two of the most widely used 
metrics: original IBM BLEU and METEOR v1.0. 
Table 3 gives the results; it shows both the version 
of AMBER with basic preprocessing, AMBER(1) 
(with tokenization and lowercasing) and the default 
version used as baseline for most of our experi-
ments (AMBER(1,4,6)). Both versions of AMBER 
perform better than BLEU and METEOR on both 
system and sentence levels. 
 
Metric  
 Dev     3 tests average   ? tests 
BLEU_ibm 
(baseline) 
sys 
sent 
0.68            0.72               N/A 
0.37            0.40               N/A 
METEOR 
     v1.0 
sys 
sent 
0.80            0.80              +0.08 
0.58            0.56              +0.17 
AMBER(1) 
(basic preproc.) 
sys 
sent 
0.83            0.83              +0.11 
0.61            0.58              +0.19 
AMBER(1,4,6) 
(default)  
sys 
sent 
0.84            0.86              +0.14 
0.62            0.60              +0.20 
 
 Table 3: Results of AMBER vs BLEU and METEOR 
 
Second, as shown in Table 4, we evaluated the 
impact of different types of preprocessing, and 
some combinations of preprocessing (we do one 
run of evaluation for each type and average the 
results). From this table, we can see that splitting 
words into sub-words improves both system- and 
sentence-level correlation. Recall that input 6 pre-
processing splits words according to a list of Eng-
lish prefixes, roots, and suffixes: AMBER(4,6) is 
the best variant. Although test 3 results, for target 
languages other than English, are not broken out 
separately in this table,  they are as follows: input 1 
yielded 0.8345  system-level correlation and 
0.5848 sentence-level consistency, but input 6 
yielded 0.8766 (+0.04 gain) and 0.5990 (+0.01) 
respectively. Thus, surprisingly, splitting non-
English words up according to English morpholo-
gy helps performance, perhaps because French, 
Spanish, German, and even Czech share some 
word roots with English. However, as indicated by 
the underlined results, if one wishes to avoid the 
use of any linguistic information, AMBER(4) per-
forms almost as well as AMBER(4,6). The default 
setting, AMBER(1,4,6), doesn?t perform quite as 
well as AMBER(4,6) or AMBER(4), but is quite 
reasonable.  
Varying the preprocessing seems to have more 
impact than varying the other parameters we expe-
rimented with.  In Table 5, ?none+tf-idf? means 
we do one run without tf-idf and one run for ?tf-idf 
only?, and then average the scores. Here, applying 
tf-idf seems to benefit performance slightly. 
 
Input  
 Dev     3 tests average     ? tests 
0  
(baseline) 
sys 
sent 
0.84            0.79                 N/A 
0.59            0.58                 N/A 
1 sys 
sent 
0.83            0.83               +0.04 
0.61            0.58               +0.00 
2 sys 
sent 
0.83            0.84               +0.05 
0.61            0.59               +0.01 
3 sys 
sent 
0.83            0.84               +0.05 
0.61            0.58               +0.00 
4 sys 
sent 
0.84            0.87               +0.08 
0.62            0.60               +0.01 
5 sys 
sent 
0.82            0.86               +0.07 
0.61            0.56               +0.01 
6 sys 
sent 
0.83            0.88               +0.09 
0.62            0.60               +0.02 
7 sys 
sent 
0.34            0.56               -0.23 
0.58            0.53               -0.05 
1,4 sys 
sent 
0.84            0.85               +0.07 
0.62            0.60               +0.01 
4,6 sys 
sent 
0.83            0.88               +0.09 
0.62            0.60               +0.02 
1,4,6 sys 
sent 
0.84            0.86               +0.07 
0.62            0.60               +0.02 
 
Table 4: Varying AMBER preprocessing (best  
linguistic = bold, best non-ling. = underline) 
 
tf-idf  
  Dev     3 tests average    ? tests 
none 
(baseline) 
sys 
sent 
0.84             0.86                N/A 
0.62             0.60                N/A 
tf-idf 
only 
sys 
sent 
0.81             0.88              +0.02 
0.62             0.61              +0.01 
none+tf-
idf 
sys 
sent 
0.82             0.87              +0.01 
0.62             0.61              +0.01 
 
Table 5: Effect of tf-idf on AMBER(1,4,6) 
 
Table 6 shows what happens if you disable one 
penalty at a time (leaving the weights of the other 
penalties at their original values). The biggest sys-
tem-level performance degradation occurs when 
LWDP is dropped, so this seems to be the most 
75
useful penalty. On the other hand, dropping CKP, 
CSRP, and SRP may actually improve perfor-
mance. Firm conclusions would require retuning of 
weights each time a penalty is dropped; this is fu-
ture work.  
 
Penalties  
  Dev     3 tests average    ? tests 
All 
(baseline) 
sys 
sent 
0.84            0.86               N/A 
0.62            0.60               N/A 
-SBP sys 
sent 
0.82            0.84               -0.02 
0.62            0.60               -0.00 
-SRP sys 
sent 
0.83            0.88              +0.01 
0.62            0.60              +0.00 
-CSBP sys 
sent 
0.84            0.85               -0.01 
0.62            0.60              +0.00 
-CSRP sys 
sent 
0.83            0.87              +0.01 
0.62            0.60               -0.00 
-SWDP sys 
sent 
0.84            0.86               -0.00 
0.62            0.60              +0.00 
-LWDP sys 
sent 
0.83            0.83               -0.03 
0.62            0.60               -0.00 
-CTP sys 
sent 
0.82            0.84               -0.02 
0.62            0.60               -0.00 
-CKP sys 
sent 
0.83            0.87              +0.01 
0.62            0.60               -0.00 
-NSCP sys 
sent 
0.83            0.86               -0.00 
0.62            0.60              +0.00 
-NKCP sys 
sent 
0.82            0.85               -0.01 
0.62            0.60              +0.00 
 
Table 6: Dropping penalties from AMBER(1,4,6) ? 
biggest drops on test in bold 
 
Matching  
 Dev     3 tests avg     ? tests 
n-gram + fxd-
gap+ flx-gap 
(default) 
sys 
sent 
0.84             0.86         N/A 
0.62             0.60         N/A 
n-gram sys 
sent 
0.84             0.86         -0.00 
0.62             0.60         -0.00 
fxd-gap+ 
 n-gram 
sys 
sent 
0.84             0.86         -0.00 
0.62             0.60         -0.00 
flx-gap+ 
 n-gram 
sys 
sent 
0.83             0.86         -0.00 
0.62             0.60         -0.00 
skip+ 
 n-gram 
sys 
sent 
0.83             0.85         -0.01 
0.62             0.60         -0.00 
All four 
matchings 
sys 
sent 
0.83             0.86         -0.01 
0.62             0.60          0.00 
Table 7: Varying matching strategy for AMBER(1,4,6) 
 
Finally, we evaluated the effect of the matching 
strategy. According to the results shown in Table 
7, our default strategy, which uses three of the four 
types of matching (n-grams, fixed-gap n-grams, 
and flexible-gap n-grams) is close to optimal;  the 
use of skip n-grams (either by itself or in combina-
tion) may hurt performance at both system and 
sentence levels.  
4 Conclusion 
This paper describes AMBER, a new machine 
translation metric that is a modification of the 
widely used BLEU metric. We used more sophisti-
cated formulae to compute the score, we developed 
several new penalties to match the human judg-
ment, we tried different preprocessing types, we 
tried tf-idf, and we tried four n-gram matching 
strategies. The choice of preprocessing type 
seemed to have the biggest impact on performance. 
AMBER(4,6) had the best performance of any va-
riant we tried. However, it has the disadvantage of 
using some light linguistic knowledge about Eng-
lish morphology (which, oddly, seems to be help-
ful for other languages too). A purist may prefer 
AMBER(1,4) or AMBER(4), which use no linguis-
tic information and still match human judgment 
much more closely than either BLEU or 
METEOR. These variants of AMBER share 
BLEU?s virtues: they are language-independent 
and can be computed quickly. 
Of course, AMBER could incorporate more lin-
guistic information: e.g., we could use linguistical-
ly defined stop word lists in the SWDP and LWDP 
penalties, or use synonyms or paraphrasing in the 
n-gram matching.  
AMBER can be thought of as a weighted com-
bination of dozens of computationally cheap fea-
tures based on word surface forms for evaluating 
MT quality. This paper has shown that combining 
such features can be a very effective strategy for 
attaining better correlation with human judgment. 
Here, the weights on the features were manually 
tuned; in future work, we plan to learn weights on 
features automatically. We also plan to redesign 
AMBER so that it becomes a metric that is highly 
suitable for tuning SMT systems. 
References 
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of ACL 
Workshop on Intrinsic & Extrinsic Evaluation Meas-
ures for Machine Translation and/or Summarization. 
76
A. Birch and M. Osborne. 2010. LRscore for evaluating 
lexical and reordering quality in MT. In Proceedings 
of the Joint Fifth Workshop on Statistical Machine 
Translation and MetricsMATR, pages 302?307.  
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz and 
J. Schroeder. 2008. Further Meta-Evaluation of Ma-
chine Translation. In Proceedings of WMT. 
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. 
Re-evaluating the role of BLEU in machine transla-
tion research. In Proceedings of EACL. 
D. Cer, D. Jurafsky and C. Manning. 2010. The Best 
Lexical Metric for Phrase-Based Statistical MT Sys-
tem Optimization. In Proceedings of NAACL. 
Y. S. Chan and H. T. Ng. 2008. MAXSIM: A maximum 
similarity metric for machine translation evaluation. 
In Proceedings of ACL. 
D. Chiang, S. DeNeefe, Y. S. Chan, and H. T. Ng. 2008. 
Decomposability of translation metrics for improved 
evaluation and efficient algorithms. In Proceedings 
of EMNLP, pages 610?619. 
M. Denkowski and A. Lavie. 2010. Meteor-next and the 
meteor paraphrase tables: Improved evaluation sup-
port for five target languages. In Proceedings of the 
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 314?317. 
George Doddington. 2002. Automatic evaluation of 
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of HLT. 
Y. He, J. Du, A. Way, and J. van Genabith. 2010. The 
DCU dependency-based metric in WMT-
MetricsMATR 2010. In Proceedings of the Joint 
Fifth Workshop on Statistical Machine Translation 
and MetricsMATR, pages 324?328.  
H. Isozaki, T. Hirao, K. Duh, K. Sudoh, H. Tsukada. 
2010. Automatic Evaluation of Translation Quality 
for Distant Language Pairs. In Proceedings of 
EMNLP.  
A. Lavie and M. J. Denkowski. 2009. The METEOR 
metric for automatic evaluation of machine transla-
tion. Machine Translation, 23. 
C.-Y. Lin. 2004. ROUGE: a Package for Automatic 
Evaluation of Summaries. In Proceedings of the 
Workshop on Text Summarization Branches Out 
(WAS 2004), Barcelona, Spain.  
C. Liu, D. Dahlmeier, and H. T. Ng. 2010. Tesla: Trans-
lation evaluation of sentences with linear-
programming-based analysis. In Proceedings of the 
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 329?334. 
S. Pado, M. Galley, D. Jurafsky, and C.D. Manning. 
2009. Robust machine translation evaluation with en-
tailment features. In Proceedings of ACL-IJCNLP. 
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. 
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL. 
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. 
Makhoul. 2006. A Study of Translation Edit Rate 
with Targeted Human Annotation. In Proceedings of 
Association for Machine Translation in the Americas. 
M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 
2009. Fluency, Adequacy, or HTER? Exploring Dif-
ferent Human Judgments with a Tunable MT Metric. 
In Proceedings of the Fourth Workshop on Statistical 
Machine Translation, Athens, Greece. 
77
