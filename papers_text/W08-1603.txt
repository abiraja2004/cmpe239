Coling 2008: Proceedings of the workshop on Knowledge and Reasoning for Answering Questions, pages 17?24
Manchester, August 2008
Know-Why Extraction from Textual Data for Supporting What 
Question      
Chaveevan Pechsiri 
Dept. of Information   
Technology,      
DhurakijPundit University,   
Bangkok, Thailand 
itdpu@hotmail.com 
Phunthara Sroison 
Dept. of Information   
Technology,      
DhurakijPundit University,   
Bangkok, Thailand 
phunthara@it.dpu.ac.th 
U. Janviriyasopak 
Eastern Industry Co.ltd. 
Bangkok, Thailand 
uraiwanjan@hotmail.com 
 
 
1Abstract
This research aims to automatically ex-
tract Know-Why from documents on the 
website to contribute knowledge sources 
to support the question-answering sys-
tem, especially What-Question, for dis-
ease treatment.  This paper is concerned 
about extracting Know-Why based on 
multiple EDUs (Elementary Discourse 
Units). There are two problems in ex-
tracting Know-Why: an identification 
problem and an effect boundary determi-
nation problem.  We propose using Na?ve 
Bayes with three verb features, a causa-
tive-verb-phrase concept set, a supporting 
causative verb set, and the effect-verb-
phrase concept set.  The Know-Why ex-
traction results show the success rate of 
85.5% precision and 79.8% recall. 
1 Introduction 
Automatically Know -Why extraction is essential 
for providing the rational knowledge source, to 
the society through question answering system, 
especially in herbal medicines when assisting the 
locals to understand more about herbs. 
According to Jana Trnkova and Wolfgang 
Theilmann (2004) Know-Why is the knowing of 
the reason of why something is the way it is.   
Therefore, Know-Why has to involve the causal 
relation which is ?an irreflexive, transitive and 
asymmetrical? relation that contains the 
properties of ?productivity (effect is ?produced? 
by the cause) and locality (it obeys the markov 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
condition, for model A B  C, if there is no 
B, then A does not cause C)?( Lemeire J. et al 
(2004)).  Wolff P. (2007) stated that the causal 
relation can be decomposed into 2 major 
approaches, the dependency model and the 
physicalist models. The dependency model can 
be represented by using statistical dependency 
model whereas in recent physicalist models are 
based on the concepts of force dynamic models 
consisting of 2 force entities in certain events; 
the agonist and the antagonist (Talmy, 2000).  
Later, the agonist form (Wolff P., 2007) can be 
viewed as the ?effect? and the antagonist as the 
?cause?. According to Talmy (2000), if there is a 
situation where the antagonist is stronger, which 
can be expressed as ?event X happens because of 
event Y?(Y contains the antagonist.), it is a form 
of causation.  Moreover, the causal relation can 
pivot on the distinction between causality and 
causation (Lehmann J. et al 2004) whereas 
causality is ?a law-like relation between cause 
events and effect events? and causation is ?the 
actual causal relation that holds between 
individual events?.  For example: 
?Because a bird sings a song at a window, The 
rock is thrown at the window.? 
Causality: ?An object vibrates. An object 
moves.? 
Causation: ?A bird sings.  The rock is thrown? 
This research focuses only on ?causal relation? to 
provide both ?causality? for extracting Know-
Why from the herbal medicine domain and 
?causation? for answering What-question, since 
what questions contain ambiguities (Girju R. and 
Moldovan D., 2002) for example:  
Know-Why: ?	
 

 

 /A 
basil leaf is used as a medicine releasing gas. 
[The leaf] stops nausea. [The leaf] stops paining 
the abdomen.? (where the [..] symbol means 
ellipsis.) 
17
Know-Why concept:  ?A herb organ is used as 
being a carminative drug. [The organ] is anti 
nausea,  [The organ] is anti stomachache.? 
Question: ?	


/What herb is used 
for stopping nausea?? From this example, ?A 
basil leaf is used as a medicine releasing gas? is 
the causation and the concept is the causality. 
   There are various forms of causal-
relation expression such as in the form of intra-
NP, inter-NP, and inter-sentence (Chang and 
Choi,2004).  According to our research, we 
separated this relation into 2 main forms based 
on the elementary discourse unit (EDU) as 
defined by (Carlson et al, 2003) as a simple 
sentence or clause.  We defined the intra-causal 
EDU as an expression within one simple EDU 
being equivalent to either the intra-NP form or 
the inter-NP form (Chang and Choi,2004).  The 
inter-causal EDU is defined as an expression 
within more than one simple EDU which is 
equivalent to the inter-sentences of Chang and 
Choi (2004).  However, this paper works on only 
the inter-causal EDU extraction because some 
cause-effect relation from the herbal web sites 
are expressed in the form of the EDU containing 
an EDU-like name entity with the causative 
action followed by some effect EDUs. 
Several techniques (Marcu and Echihabi,2002; 
Torisawa  2003; Inui and et al,2004; Pechsiri 
and Kawtrakul, 2007)   have been used to extract 
cause-effect knowledge varying  from  two 
adjacent sentences to multiple sentences.  Our 
work aimed at mining and extracting Know-Why 
from Thai documents of herbal medicines.  Thai 
has several specific characteristics, such as the 
existence of sentence-like name entity, zero 
anaphora or the implicit noun phrase.  All of 
these characteristics are involved in the two main 
problems of Know-Why extraction: the first 
problem is how to identify the interesting 
causality events expressed by an EDU- like name 
entity from documents, and the second one is 
how to identify the effect boundary, where The 
problem of implicit delimiter of the boundary is 
involved.  From all of these problems, we needed 
to develop a framework which combineed 
Language Processing and the machine learning 
technique as Na?ve Bayes to learn features of 
three verb sets, a causative concept verb set, a 
supporting causative verb set, and an effect 
concept verb set, for solving those problems.  
In conclusion, unlike other methods (Marcu 
and Echihabi ,2002; Torisawa  2003; Inui and et 
al.,2004) where the emphasis is based on two 
adjacent sentences, this paper is based on 
multiple  EDU extraction.  Our research was 
separated into 5 sections.  In section 2, related 
work was summarized.  Problems in causality 
mining from Thai documents will be described in 
section 3 and in section 4 our framework for 
causality extraction was explained. In section 5, 
we evaluated and concluded our proposed model. 
2 Related Work 
Several strategies such as those done by Marcu 
and Echihabi ,2002, Torisawa( 2003), Inui and et 
al.(2004), and  Pechsiri and Kawtrakul (2007) 
have been proposed to extract and discover 
knowledge from the textual data. 
Marcu and Echihabi (2002) presented the 
unsupervised approach to recognize the discourse 
relations by using word pair probabilities 
between two adjacent sentences for classifying 
the rhetorical relations, such as Contrast, Cause-
Explanation, Condition, and Elaboration, 
between two adjacent sentences by using Na?ve 
Bayes classifier to the BLIPP corpus (Charniak, 
2000).  They determined the word pairs in the 
cartesian product from the sentence pairs 
connected with or without discourse marker or 
connective marker , i.e. ?because? ?but? ?then?, to 
classify the causal relation from other rhetorical 
relations.  The result showed an accuracy of 75% 
of inter-sentence causality extraction from the 
corpus size of more than a million sentences for 
learning whereas our corpus size is 3000 
sentences for learning.  Therefore, our approach 
is the supervised approach with the statistical 
method because our corpus size is small.    
Inui?s work (Inui and et al,2004) proposed a 
method of extraction and classification of causal 
knowledge. The method of extraction was 
accomplished under two adjacent sentences by 
using explicit connective markers; e.g. ?because? 
?since? ?if..then? ?as the result? etc..  SVM was 
used for the classification process in (Inui and et 
al.,2004).  Four types of causal relations are 
studied, including the following: cause, 
precondition, mean, effect relations.  Inui?s 
work?s precision is high: 90% but the recall is 
low: 30%, because of unresolved anaphora. 
However, in our work, we extract multiple EDUs 
with some implicit discourse markers. 
Torisawa( 2003)? s work in extracting the verb 
phrase pair from the news corpus worked on the 
assumption that if two events share a common 
participant (is specified by a noun) then the two 
events are likely to have a logical relation as 
causal relation. For example ?A man drank 
18
liquor and was intoxicated by the liquor.?(a 
common participant is ?liquor?).  However,  this 
assumption can not be applied in our research 
because most of our causality expression does 
not share a common participant; e. g.  ? 	

 

!"/Ginger is used as being laxative 
medicine. [The ginger] stops constipation. 
Pechsiri and Kawtrakul (2007), proposed 
verb-pair rules learned by two different machine 
learning techniques (NB and SVM) to extract 
causality with multiple EDUs of a causative unit 
and multiple EDUs of an effect unit with the 
problems of the discourse marker ambiguity and 
the implicit discourse marker.  This verb-pair 
rule has been represented by the following equa-
tion (1) (Pechsiri and Kawtrakul, 2007) where Vc 
is the causative verb concept set, Ve is the effect 
verb concept set , C is the Boolean variables of 
causality and non-causality, and a causative verb 
concept (vc , where vcVc) and an effect verb 
concept (ve , where veVe) are referred to Word-
Net (http://wordnet.princeton.edu/) and the pre-
defined plant disease information from Depart-
ment of Agriculture (http://www.doa.go.th/). 
CausalityFunction:  Vc  Ve  C    (1)   
They also proposed using Vc and Ve to solve 
the boundary of the causative unit and using the 
Centering theory along with Ve to solve the 
boundary of the effect unit.  The outcomes of 
their research were the verb-pair rule, Vc, Ve, and 
the multiple EDUs of causality (extracted from 
textual data) was at their highest precision of 
89% and their highest recall of 76%.  The cor-
rectness of the causality-boundary determination 
is 88% on average.  However, our causative unit 
consisted of only one EDU containing an EDU-
like name entity as a cause, and this EDU was 
followed by several effect EDUs. 
In our current work, we aimed at extracting 
the Know-Why in Natural Language description 
instead of visualizing only associations of 
concepts, by applying both language processing 
and learning technique by Na?ve Bayes to 
identify the causality expression. 
3 Problem of Know-Why Extraction 
To extract the cause-effect expressions, there 
are two main problems that must be solved. The 
first problem is how to identify interesting cause-
effect relations from the documents.  The second 
problem is how to determine the effect boundary.      
 
 
There is also the problem of implicit noun 
phrase.  
3.1 Causality Identification 
The problem involves the word level and the sen-
tence level.  For the word level, the medicinal 
name entity may express in the form of a sen-
tence like name entity or an EDU- like name en-
tity which explains the medicinal action as the 
causative action of medicine, and medical char-
acteristic.  The problem of this level is how to 
identify the causative name entity.  For example:  
a)  ?/A basil leaf  	
#is used as #
medicine #releases #gas?    
where ?a medicine releases gas? is an EDU-
like name entity with the causative action, 
?release?.  
b)  ?$%#Nicolson stem 	
&#is used for 
making #medicine #soaks in '
#
liquor?  
where ?a medicine soaks in liquor? is an 
EDU-like name entity with the characteristic 
of medicine being preserved in the alcohol. 
The above examples, a) and b), contain an 
EDU-like name entity which is a cause in a) and 
a non cause in b). 
For the sentence level, the EDU containing an 
EDU-like name entity with the causative action 
may be followed by an effect EDU(s) to form the 
cause-effect or causality relation between the 
EDU like name entity and that following 
EDU(s).  For example: 
Causality 
EDU1 ?(
'/Lemon grass 	
is used as 
#medicine )contracts ,"#a uterus? 
(where ?a medicine contracts a uterus.? is the 
EDU-like name entity with concept of ?the 
medicine causes uterus to contract?.)  
EDU2 ?[The plant ] /discharges %&# 
period.? (=The plant discharges period.)  
 
Non causality 
EDU1 ?/A basil leaf  	
#is used as 
#medicine #releases #gas.? (where ?a 
medicine releases gas? is the causative EDU-
like name entity.)  
19
EDU2 ?[the basil leaf]*#relieves !#ulcer
#in'#stomach.?(= [The basil leaf re-
lives ulcer in a stomach. ) 
Where in this example, EDU 1 is the cause 
and EDU2 is the effect 
3.2 Effect Boundary Determination 
There are two problems of an implicit effect 
boundary cue and the effect EDU containing in-
terrupts. 
 
3.2.1  Implicit Effect Boundary Cue 
Some cause-effect relations from the herbal web 
sites are expressed in the form of the EDU con-
taining an EDU like name entity with the causa-
tive action followed by some effect EDUs with-
out any cue of ending effect boundary, e.g. ?
and?.  For example: 
EDU1 ?A basil leaf 	
is used as #
medicine #releases #gas? (=A basil leaf is 
used as a medicine releasing gas.)  
EDU2 ?[The basil leaf ] 
#stops 
# nau-
seate.? (=The basil leaf stop being nausea.) 
EDU3 ?[And the leaf ]
#stops /pain 
# 
abdomen.? (= [And the leaf] stops paining 
abdomen.) 
Where in this example, EDU 1 is the cause 
and EDU 2 & EDU3 are the effects.  EDU 2 
and EDU3 help us to determine the boundary. 
   3.2.2 Effect EDU Containing Interrupts 
There are some effect EDUs containing inter-
rupts as shown in the following example: 
EDU1 ?'#A red onion 	
is used as #
medicine +$ /be laxztive? (=A red onion is 
used as a laxative medicine.)  
EDU2 ?[And the red onion ] 
#stops 
!"#
being constipation? (= [And the red onion] 
stops being  constipation.) 
EDU3 ?[The red onion ] /discharges , 
/urine.? (= [The red onion] discharges urine.) 
EDU4 ?[The red onion makes a patient] % -
'/be appetite.? (= [The red onion] makes a 
patient] be appetite. ) 
Where the EDU-like name entity in EDU1 is a 
cause with EDU2 and EDU4 as its effects. The 
EDU3 is an interrupt. Although EDU3 is the ef-
fect of red onions, but EDU 3 is not the effect of 
laxatives. 
4 A Framework for Know-Why Extrac-
tion 
 
 
Figure 1.  System Overview 
 
There are three steps in our framework.  First is 
the corpus preparation step followed by causality 
learning, and causality recognition steps (as 
shown in figure 1). 
4.1 Corpus Preparation   
There are two steps of pre-annotation and Cau-
sality annotation. 
 
4.1.1  Pre-annotation 
This step is the preparation of the corpus in the 
form of EDU from the text.  The step involves 
using Thai word segmentation tools to solve a 
boundary of a Thai word and tagging its part of 
speech (Sudprasert and Kawtrakul, 2003).  This 
process includes Name entity (Chanlekha and 
Kawtrakul, 2004), and word-formation recogni-
tion (Pengphom, et al2002) to solve the bound-
ary of Thai Name entity and Noun phrase.  
After the word segmentation is achieved, EDU 
segmentation is dealt with.  According to Charo-
ensuk et al (2005), this process segments plain 
text into units of EDUs by using the rule based 
and the machine learning technique of C4.5 
(Mitchell T.M., 1997).    These generated EDUs 
will be kept as an EDU corpus. This corpus will 
contain 4500 EDUs and will be separated into 2 
parts, one part is 3500 EDUs for causality learn-
ing and the other part of 1000 EDUs for causality 
recognition and extraction. 
4.1.2 Causality Annotation 
Due to the problems in the causality identifica-
tion, verbs from three EDUs (with one EDU as 
an EDU-like name entity) in the EDU corpus are 
used in this process to learn for extracting causal-
ity.  Word ambiguity will be solved through the 
Corpus prepa-
ration
Text 
Word net
Know-Why
Causality learning 
Causality recognition
Causality 
 relation
Causality model
Causality  
identification 
Effect bound-
ary det. 
20
finding of word concepts from Wordnet.  Since 
Thai Wordnet does not exist, we need to translate 
from Thai to English, using Lexitron (the Thai-
English dictionary)( http://lexitron.nectec.or.th/), 
before using Wordnet(http://wordnet.princeton. 
edu/obtain). In this process, we manually anno-
tate the causality EDUs by annotating the EDU 
containing the causative EDU-like name entity as 
the causative EDU.  We annotate a verb phrase 
in the causative EDU-like name entity to be a 
causative-verb-phrase concept (referred to 
Wordnet).  The verb from EDU which contains 
the causative EDU-like name entity is annotated 
with a concept and we call this verb as ?support-
ing causative verb?.   We also annotate the effect-
verb-phrase concept(referred to Wordnet and 
http://www.ars-grin.gov/duke/ethnobot.html) 
from effect EDUs following the EDU containing 
the causative EDU-like name, as shown in Figure 
2)  
4.2 Causality Learning  
The aim of this step was to learn cause-effect 
relation between causative events and effect 
events from annotating an EDU corpus. 
 
 
 
 
 
 
4.2.1 Feature Extraction  
All annotated verb features from the previous 
step are extracted into database table (in Table 1) 
including surface forms of verb features along 
with their concepts used for probability determi-
nation in the next step. 
 
 
 
 
NP1 NP1 Concept Vs Vs Concept VPc
VPc 
concept VPe VPe Concept Class

Naringi 
crenulata
herb 	
 use as

cure 
poison
be-
antipyretic
	


relieve muscle 
pain n

Asiatic 
Pennyworth
herb leaf 	
 use as


apply 
externally
apply 
topically  heal wound y
 !
red onion herb 
 is
"#
excrete be-lexative 		!$
stop being 
constipation y
 !
red onion herb 
 is
"#
excrete be-lexative
%
&'' discharge urine n
 !
red onion herb 
 is
"#
excrete be-lexative

(
  be appetite y
%)
curcumin herb 
 is
*#
)
antiseptic be-antiseptic

+,
 !
cure skin 
disease y
	!
Soianum 
indicum 
Linn
herb -
 make as
)-.


reduce 
blood 
sugar
balance 
blood sugar 
level
	/ stop coughing n


Basil herb leaf 	
 use as
%
release 
gas
be 
carminative 	,'	relieve nausea y


Basil herb leaf 	
 use as
%
release 
gas
be 
carminative 
	
	!
stop paining 
an abdomen y
%!ginger herb 	
 use as
%
release 
gas
be 
carminative 	,'	relieve nausea y
$
bergamot 
leaf
herb leaf 	
 use as
%
release 
gas
be 
carminative 	,'	relieve nausea y
? ? ? ? ? ? ? ? ?
 
Table 1. The extracted features from the annotated corpus 
 <C id=1> 
<EDU type =cause> 
   <NP1 concept=a herb organ>.'A basil leaf</NP1> 
   <VS concept=use#1>	
#is used as</VS> 
<EDU-Like-NE  > 
   <NP2 concept=drug>#medicine</NP2> 
   <CVC concept= be carminative/ eliminate gas from a body>                
     #releases # gas 
   </CVC> 
</EDU-Like-NE> 
</EDU> 
</C> 
<R id=1> 
<EDU type=effect> 
    <EVC concept= stop nausea/ be anti nausea>
#stops 
# nauseate. 
    </EVC> 
</EDU> 
<EDU type=effect> 
     <EVC concept=stop paining an abdomen/ relieve abdominal pain>
# 
         stops /pain 
# abdomen 
     </EVC> 
</EDU> 
</R> 
 
EDU= EDU, EDU-Like-NE= EDU-like name entity tag, 
C=cause tag, R=result or effect tag, VS= supporting verb tag , 
CVC=causative verb concept tag, EVC=effect verb concept tag 
NP1 NP2= noun phrase tag 
 
 
Figure 2.  Causality Annotation Tag 
21
 Vs concept causality 
 non 
causality 
	0
useas 0.27619 0.290323 
Be 0.561905 0.612903 
-0
makeas 0.009524 0.032258 
	0-0
use for making as 0.066667 0.053763 
? ? ? 
VPc concept causality 
non 
causality 
?%+/release-gas? 0.371901 0.192661 
?	+//anti coughing? 0.024793 0.045872 
?/apply? 0.140496 0.009174 
?%/be-bitter? 0.041322 0.009174 
?%+&''/discharge-urine? 0.057851 0.06422 
?%+
' /be expectorant? 0.041322 0.06422 
?1+$/contract 
uterus/oxytocic? 0.041322 0.027523 
?+
 /be antidiabetic? 0.008264 0.027523 
? ? ? 
VPe concept causality 
non 
causality 
?	++	!/stop-
stomachach/relieve abdominal 
pain? 0.035714 0.007813 
?	+,'	/stop-naucea/be anti 
nausea? 0.035714 0.007813 
?	+	!	!
23/stop-
flatulence/relieve indigestion? 0.15 0.007813 
?	+/stop-rash/ be antiurti-
caria? 0.035714 0.023438 
?/%	/reduce-fever? 0.021429 0.039063 
?%+/eliminate-placenta? 0.007143 0.054688 
?
(+ /increase appetite? 0.092857 0.007813 
?%+
 !/release-sweat/be dia-
phoretic? 0.007143 0.070313 
 
Table 2. Show probability of Vs concept, VPc 
concept and VPe concept 
 
 
4.2.2 Probability Determination  
After we had obtained the extracted verb features, 
we then determined the probability of causal and 
non causal from the occurrences of the cartesian 
products of three verb feature concepts , shown 
in Table2, by using Weka  which is a software 
tool for machine learning (http://www.cs. wai-
kato.ac.nz/ml/weka/ ). 
 
4.3 Causality Recognition and Extraction 
The objective of this step was to recognize and 
extract the cause-effect relation from the testing  
EDU corpus. In order to start the causality rec-
ognition process, Na?ve Bayes Classifer shown in 
equation (2) is applied with the feature probabili-
ties in Table 2, where EDUs class is determined  
 
 
by class1 (causality EDUs) and class0 (non  cau-
sality EDUs). 
 
 
Therefore, Causality Recognition can be sepa-
rated into 2 steps: causality identification and 
effect boundary determination. 
 
 
4.3.1 Causality Identification    
This step was to determine the interesting loca-
tions that are cause-effect relations by searching 
any EDU which consists of a verb matching to a 
verb in the supporting causative concept set, Vs, 
and an EDU-like name entity containing a causa-
tive-verb-phrase concept as vpc (where vpcVPc).  
 
4.3.2 Effect Boundary Determination   
The effect EDU and the effect boundary were 
determined at the same time by checking all se-
quence  EDUs right after the EDU containing vpc 
in the EDU-like name entity.  If a verb phrase 
from the sequence of checked EDUs is not in 
VPe, the possible effect boundary is end.       Af-
ter the possible boundary is determined, vs_inEDU1, 
vpc_inEDU1 and vpe_inEDU2..vpe_inEDUn (where n>2) 
will be used to determine the causality class from 
the Na?ve Bayes Classifier equation (2) as shown 
in Figure 3.  The actual effect boundary is deter-
mined from the last class1 in the sequence of 
EDU2.. EDUn. 
 
Furthermore, where the implicit noun phrase 
occurs as the subject of the current EDU, this has 
to be solved in this step by using the heuristic 
rule which is that the noun phrase as a subject of 
the previous EDU will be the subject of the cur-
rent EDU. 
 
     
setconceptPhraseVerbEffectaisVPwhereVPvp
setconceptPhraseVerbCausativeaisVPwhereVPvp
setconceptVerbCausativeSupportingaisVwhereVv
classPclassvpPclassvpPclassvP
vpvpvclassPEDUclass
eee
ccc
sss
ecs
Classclass
ecs
Classclass







)(|||maxarg
,,|maxarg
 
 
 
(2) 
22
 5 Evaluation and Conclusion 
The Thai corpora used to evaluate the proposed 
causality extraction algorithm consist of about 
1,000 EDUs collected from several herbal web 
sites. The evaluation of the causality extraction 
performance of this research methodology is ex-
pressed in terms of the precision and the recall  
as shown below, where R is the causality relation: 
 
 #
#
of samples correctly extracted as RPrecision
of all samples output asbeing R
       (3) 
 #
#
of samples correctly extracted as RRecall
of all samples holding the target relation R
  (4) 
 
   The results of precision and recall are evalu-
ated by three expert judgments with max win 
voting.  The precision of the extracted causality 
85.5% with 79.8% recall.  The correctness of our 
effect boundary determination by these expert 
judgments is 86%.   These research results can be 
increased if we use a larger corpus.  However, 
our methodology will be very beneficial for con-
tribute the causality knowledge for supporting 
What-question with the concept of causal rela-
tion from a web page by inference method of 
backward chaining, for example: 
Extracted causality: ?.'	
 

 


 /A basil leaf is used for a gas released 
medicine. [The leaf] stops nausea. [The leaf] 
stop stomachache.? ?????. 
 
The above extracted causality can be repre-
sented by the following predication. 
 
a) x   be_herb(x) ^ be_herb_medicine(y) ^                                                  
be_carminative (y) ^  use_as(x,y) stop(x, z) ^ 
be_nausea(z)  
 
b) x   be_herb(x) ^ be_herb_medicine(y) ^ 
be_carminative (y) ^use_as(x,y) stop(x, z) ^ 
be_abdominal pain(z)  
Where x  X,{?	
	/basil leaf? ?/ginger? 
?	/black pepper? ?	/bergamot leaf?..}, 
and X is the extracted NP1 set from EDUs con-
taining the causative EDU-like name entities 
and being followed by the effect EDUs , e.g. 
(stop(x, z) ^ be_nausea(z)), (stop(x, z) ^ be_stomach 
ache(z)). 
 
 
Question: ?	
#use #herb #what 
#stop 

#nausea (What kind of herb is used for stop 
nausea?) 
The backward chaining from the above question 
and the extracted causality in a) is shown in the 
following  
stop(x, z) ^ be_nausea(z) be_herb(x) ^ 
be_herb_medicine(y) ^  be_carminative (y) ^ 
use_as(x,y) 
 
where  x  is  ?	
	/basil leaf?, ?/ginger?, 
?	/black pepper?, or ?	/bergamot leaf? 
References 
 
Carlson L., Marcu D., and Okurowski M. E. 2003. 
Building a Discourse-Tagged Corpus in the 
Framework of Rhetorical Structure Theory. In Cur-
rent Directions in Discourse and Dialogue. pp.85-
112.  
Chanlekha H. and Kawtrakul A. 2004. Thai Named 
Entity Extraction by incorporating Maximum En-
tropy Model with Simple Heuristic Information. 
IJCNLP? 2004. 
Chareonsuk J ., Sukvakree T., and Kawtrakul A. 
2005. Elementary Discourse unit Segmentation for 
Assume that each EDU is represented by (np vp) 
L is a list of EDU                        
VPC is a causative-verb-phrase concept set, VPE /VPe is a effect- 
verb-phrase concept set  
VS is a supporting causative verb concept set 
CAUSALITY_EXTRACTION ( L, VC, VE, Vs ) 
 
1 i  1, R  
2 while i  length[L] do 
3 begin_while1 
4 
 CA  , EC   
5 
 if  (vpi  VS)  (vpi_in_NE  VPC) then 
6        begin_if    
 
          CA  CA 	 {i}, i  i + 1    /*CA is causa  
                                                                tive EDU  
7 
          while (vpi  VPE) do 
8            begin_while2 
9 res    
      )(|||maxarg
),(
cPcvpPcvpPcvP ecs
noyesc
 
10            if  res=yes 
11 
            EC  EC 	 {i},    /*EC is effect   
                                                                     EDU 
12               i  i + 1 
13              end_while2 
14            endif   
15 
 if res = yes  CA <>  then 
16 
  R = R 	 { (CA,EC) } 
17 end_while1 
18 
 return R 
Figure3. Show Causality Extraction algorithm for 
the EDU containing the causative EDU-like name 
entity, and followed by multiple effect EDUs . 
 
23
Thai using Discourse Cue and Syntactic Informa-
tion. NCSEC 2005. 
Chang D.S. and Choi K.S. 2004. Causal Relation Ex-
traction Using Cue Phrase and Lexical Pair Prob-
abilities. IJCNLP. pp. 61 ? 70. 
Charniak, E. 2000. A maximum-entropy-inspired 
parser. Proc. of NAACL, pp.132-130.   
Girju R. and Moldovan D. 2002. Mining answers for 
causation questions. AAAI Symposium on Mining 
Answers from Texts and Knowledge Bases. 
Inui T., Inui K., and Matsumoto Y. 2004. Acquiring 
causal knowledge from text using the connective 
markers. Journal of the information processing so-
ciety of Japan 45(3), pp. 919-993. 
Lemeire, J., S. Maes and E. Dirkx. 2004. Causal 
Models for Parallel Performance Analysis. Fourth 
PA3CT-Symposium, Edegem, Belgium, Septem-
ber. 
Marcu D. and Echihabi A. 2002. An Unsupervised 
Approach to Recognizing Discourse Relations. in 
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics Confer-
ence. pp. 368 ? 375. 
Pechsiri C., Kawtrakul A. and Piriyakul R. 2005.  
Mining Causality Knowledge From  Text for Ques-
tion Answering System. IEICE Transactions on In-
formation and Systems, Vol.E90-D, No.10 :1523-
1533. 
Pengphon N., Kawtrakul A., and Suktarachan M. 
2002. Word Formation Approach to Noun Phrase 
Analysis for Thai. SNLP. 
Mitchell T.M. 1997. Machine Learning. The 
McGraw-Hill Companies Inc. and MIT Press, Sin-
gapore. 
Sudprasert S. and Kawtrakul A. 2003. Thai Word 
Segmentation based on Global and Local Unsuper-
vised Learning. NCSEC?2003. 
Talmy, L. 2000. Toward a Cognitive Semantics Con-
cept Structuring Systems ? Vol. 1. The MIT Press. 
Torisawa K. 2003. Automatic Extraction of Common-
sense Inference Rules from Corpora. In Proc. Of 
The 9th Annual Meeting of The Association for 
Natural Language Proceeding. pp. 318-321.  
Trnkova, Jana, Wolfgang Theilmann. 2004. Author-
ing processes for Advanced Learning Strategies. 
Telecooperation Research Group,TU Darmstadt, 
and SAP Research, CEC Karlsruhe. Germany. 
Wolff, P. 2007. Representing Causation. Journal of 
experimental psychology: General 2007 Vol. 136 
No.1 82-111. USA.   
 
24
