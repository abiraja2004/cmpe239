Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 296?305, Prague, June 2007. c?2007 Association for Computational Linguistics
Why doesn?t EM find good HMM POS-taggers?
Mark Johnson
Microsoft Research Brown University
Redmond, WA Providence, RI
t-majoh@microsoft.com Mark Johnson@Brown.edu
Abstract
This paper investigates why the HMMs es-
timated by Expectation-Maximization (EM)
produce such poor results as Part-of-Speech
(POS) taggers. We find that the HMMs es-
timated by EM generally assign a roughly
equal number of word tokens to each hid-
den state, while the empirical distribution
of tokens to POS tags is highly skewed.
This motivates a Bayesian approach using
a sparse prior to bias the estimator toward
such a skewed distribution. We investigate
Gibbs Sampling (GS) and Variational Bayes
(VB) estimators and show that VB con-
verges faster than GS for this task and that
VB significantly improves 1-to-1 tagging ac-
curacy over EM. We also show that EM does
nearly as well as VB when the number of
hidden HMM states is dramatically reduced.
We also point out the high variance in all
of these estimators, and that they require
many more iterations to approach conver-
gence than usually thought.
1 Introduction
It is well known that Expectation-Maximization
(EM) performs poorly in unsupervised induction
of linguistic structure (Carroll and Charniak, 1992;
Merialdo, 1994; Klein, 2005; Smith, 2006). In ret-
rospect one can certainly find reasons to explain this
failure: after all, likelihood does not appear in the
wide variety of linguistic tests proposed for identi-
fying linguistic structure (Fromkin, 2001).
This paper focuses on unsupervised part-of-
speech (POS) tagging, because it is perhaps the sim-
plest linguistic induction task. We suggest that one
reason for the apparent failure of EM for POS tag-
ging is that it tends to assign relatively equal num-
bers of tokens to each hidden state, while the em-
pirical distribution of POS tags is highly skewed,
like many linguistic (and non-linguistic) phenomena
(Mitzenmacher, 2003). We focus on first-order Hid-
den Markov Models (HMMs) in which the hidden
state is interpreted as a POS tag, also known as bitag
models.
In this setting we show that EM performs poorly
when evaluated using a ?1-to-1 accuracy? evalua-
tion, where each POS tag corresponds to at most one
hidden state, but is more competitive when evaluated
using a ?many-to-1 accuracy? evaluation, where sev-
eral hidden states may correspond to the same POS
tag. We explain this by observing that the distribu-
tion of hidden states to words proposed by the EM-
estimated HMMs is relatively uniform, while the
empirical distribution of POS tags is heavily skewed
towards a few high-frequency tags. Based on this,
we propose a Bayesian prior that biases the sys-
tem toward more skewed distributions and show that
this raises the 1-to-1 accuracy significantly. Finally,
we show that a similar increase in accuracy can be
achieved by reducing the number of hidden states in
the models estimated by EM.
There is certainly much useful information that
bitag HMMs models cannot capture. Toutanova et
al. (2003) describe a wide variety of morphologi-
cal and distributional features useful for POS tag-
ging, and Clark (2003) proposes ways of incorporat-
ing some of these in an unsupervised tagging model.
However, bitag models are rich enough to capture
at least some distributional information (i.e., the tag
296
for a word depends on the tags assigned to its neigh-
bours). Moreover, more complex models add addi-
tional complicating factors that interact in ways still
poorly understood; for example, smoothing is gen-
erally regarded as essential for higher-order HMMs,
yet it is not clear how to integrate smoothing into un-
supervised estimation procedures (Goodman, 2001;
Wang and Schuurmans, 2005).
Most previous work exploiting unsupervised
training data for inferring POS tagging models has
focused on semi-supervised methods in the in which
the learner is provided with a lexicon specifying the
possible tags for each word (Merialdo, 1994; Smith
and Eisner, 2005; Goldwater and Griffiths, 2007)
or a small number of ?prototypes? for each POS
(Haghighi and Klein, 2006). In the context of semi-
supervised learning using a tag lexicon, Wang and
Schuurmans (2005) observe discrepencies between
the empirical and estimated tag frequencies similar
to those observed here, and show that constraining
the estimation procedure to preserve the empirical
frequencies improves tagging accuracy. (This ap-
proach cannot be used in an unsupervised setting
since the empirical tag distribution is not available).
However, as Banko and Moore (2004) point out, the
accuracy achieved by these unsupervised methods
depends strongly on the precise nature of the su-
pervised training data (in their case, the ambiguity
of the tag lexicon available to the system), which
makes it more difficult to understand the behaviour
of such systems.
2 Evaluation
All of the experiments described below have the
same basic structure: an estimator is used to infer
a bitag HMM from the unsupervised training cor-
pus (the words of Penn Treebank (PTB) Wall Street
Journal corpus (Marcus et al, 1993)), and then the
resulting model is used to label each word of that
corpus with one of the HMM?s hidden states. This
section describes how we evaluate how well these
sequences of hidden states correspond to the gold-
standard POS tags for the training corpus (here, the
PTB POS tags). The chief difficulty is determining
the correspondence between the hidden states and
the gold-standard POS tags.
Perhaps the most straightforward method of es-
tablishing this correspondence is to deterministically
map each hidden state to the POS tag it co-occurs
most frequently with, and return the proportion of
the resulting POS tags that are the same as the POS
tags of the gold-standard corpus. We call this the
many-to-1 accuracy of the hidden state sequence be-
cause several hidden states may map to the same
POS tag (and some POS tags may not be mapped
to by any hidden states at all).
As Clark (2003) points out, many-to-1 accuracy
has several defects. If a system is permitted to posit
an unbounded number of hidden states (which is not
the case here) then it can achieve a perfect many-to-
1 accuracy by placing every word token into its own
unique state. Cross-validation, i.e., identifying the
many-to-1 mapping and evaluating on different sub-
sets of the data, would answer many of these objec-
tions. Haghighi and Klein (2006) propose constrain-
ing the mapping from hidden states to POS tags so
that at most one hidden state maps to any POS tag.
This mapping is found by greedily assigning hidden
states to POS tags until either the hidden states or
POS tags are exhausted (note that if the number of
hidden states and POS tags differ, some will be unas-
signed). We call the accuracy of the POS sequence
obtained using this map its 1-to-1 accuracy.
Finally, several authors have proposed using
information-theoretic measures of the divergence
between the hidden state and POS tag sequences.
Goldwater and Griffiths (2007) propose using the
Variation of Information (VI) metric described by
Meila? (2003). We regard the assignments of hid-
den states and POS tags to the words of the cor-
pus as two different ways of clustering those words,
and evaluate the conditional entropy of each clus-
tering conditioned on the other. The VI is the sum
of these conditional entropies. Specifically, given a
corpus labeled with hidden states and POS tags, if
p?(y), p?(t) and p?(y, t) are the empirical probabilities
of a hidden state y, a POS tag t, and the cooccurance
of y and t respectively, then the mutual information
I , entropies H and variation of information VI are
defined as follows:
H(Y ) = ?
?
y
p?(y) log p?(y)
H(T ) = ?
?
t
p?(t) log p?(t)
I(Y, T ) =
?
y,t
p?(y, t) log
p?(y, t)
p?(y)p?(t)
H(Y |T ) = H(Y )? I(Y, T )
297
H(T |Y ) = H(T )? I(Y, T )
VI (Y, T ) = H(Y |T ) +H(T |Y )
As Meila? (2003) shows, VI is a metric on the space
of probability distributions whose value reflects the
divergence between the two distributions, and only
takes the value zero when the two distributions are
identical.
3 Maximum Likelihood via
Expectation-Maximization
There are several excellent textbook presentations of
Hidden Markov Models and the Forward-Backward
algorithm for Expectation-Maximization (Jelinek,
1997; Manning and Schu?tze, 1999; Bishop, 2006),
so we do not cover them in detail here. Conceptu-
ally, a Hidden Markov Model generates a sequence
of observations x = (x0, . . . , xn) (here, the words
of the corpus) by first using a Markov model to gen-
erate a sequence of hidden states y = (y0, . . . , yn)
(which will be mapped to POS tags during evalua-
tion as described above) and then generating each
word xi conditioned on its corresponding state yi.
We insert endmarkers at the beginning and ending
of the corpus and between sentence boundaries, and
constrain the estimator to associate endmarkers with
a state that never appears with any other observation
type (this means each sentence can be processed in-
dependently by first-order HMMs; these endmarkers
are ignored during evaluation).
In more detail, the HMM is specified by multi-
nomials ?y and ?y for each hidden state y, where
?y specifies the distribution over states following y
and ?y specifies the distribution over observations x
given state y.
yi | yi?1 = y ? Multi(?y)
xi | yi = y ? Multi(?y)
(1)
We used the Forward-Backward algorithm to per-
form Expectation-Maximization, which is a proce-
dure that iteratively re-estimates the model param-
eters (?, ?), converging on a local maximum of the
likelihood. Specifically, if the parameter estimate at
time ` is (?(`), ?(`)), then the re-estimated parame-
ters at time `+ 1 are:
?(`+1)y?|y = E[ny?,y]/E[ny] (2)
?(`+1)x|y = E[nx,y]/E[ny]
6.95E+06
7.00E+06
7.05E+06
7.10E+06
7.15E+06
0 250 500 750 1000
?
lo
g 
lik
el
ih
oo
d
Iteration
Figure 1: Variation in negative log likelihood with
increasing iterations for 10 EM runs from different
random starting points.
where nx,y is the number of times observation x oc-
curs with state y, ny?,y is the number of times state
y? follows y and ny is the number of occurences of
state y; all expectations are taken with respect to the
model (?(`), ?(`)).
We took care to implement this and the other al-
gorithms used in this paper efficiently, since optimal
performance was often only achieved after several
hundred iterations. It is well-known that EM often
takes a large number of iterations to converge in like-
lihood, and we found this here too, as shown in Fig-
ure 1. As that figure makes clear, likelihood is still
increasing after several hundred iterations.
Perhaps more surprisingly, we often found dra-
matic changes in accuracy in the order of 5% occur-
ing after several hundred iterations, so we ran 1,000
iterations of EM in all of the experiments described
here; each run took approximately 2.5 days compu-
tation on a 3.6GHz Pentium 4. It?s well-known that
accuracy often decreases after the first few EM it-
erations (which we also observed); however in our
experiments we found that performance improves
again after 100 iterations and continues improving
roughly monotonically. Figure 2 shows how 1-to-1
accuracy varies with iteration during 10 runs from
different random starting points. Note that 1-to-1
accuracy at termination ranges from 0.38 to 0.45; a
spread of 0.07.
We obtained a dramatic speedup by working di-
rectly with probabilities and rescaling after each ob-
servation to avoid underflow, rather than working
with log probabilities (thanks to Yoshimasa Tsu-
298
0.35
0.37
0.39
0.41
0.43
0.45
0.47
0 250 500 750 1000
1-
to
-1
 a
ccura
cy
Iteration
Figure 2: Variation in 1-to-1 accuracy with increas-
ing iterations for 10 EM runs from different random
starting points.
ruoka for pointing this out). Since we evaluated
the accuracy of the estimated tags after each iter-
ation, it was important that decoding be done effi-
ciently as well. While most researchers use Viterbi
decoding to find the most likely state sequence, max-
imum marginal decoding (which labels the observa-
tion xi with the state yi that maximizes the marginal
probability P(yi|x, ?, ?)) is faster because it re-uses
the forward and backward tables already constructed
by the Forward-Backward algorithm. Moreover, in
separate experiments we found that the maximum
marginal state sequence almost always scored higher
than the Viterbi state sequence in all of our evalua-
tions, and at modest numbers of iterations (up to 50)
often scored more than 5% better.
We also noticed a wide variance in the perfor-
mance of models due to random initialization (both
? and ? are initially jittered to break symmetry); this
wide variance was observed with all of the estima-
tors investigated in this paper. This means we cannot
compare estimators on the basis of single runs, so we
ran each estimator 10 times from different random
starting points and report both mean and standard
deviation for all scores.
Finally, we also experimented with annealing, in
which the parameters ? and ? are raised to the power
1/T , where T is a ?temperature? parameter that is
slowly lowered toward 1 at each iteration accord-
ing to some ?annealing schedule?. We experimented
with a variety of starting temperatures and annealing
schedules (e.g., linear, exponential, etc), but were
unable to find any that produced models whose like-
0E+0
1E+5
2E+5
Fre
quen
cy
Tag / hidden state (sorted by frequency)
PT B
V B
EM
EM 25
Figure 3: The average number of words labeled with
each hidden state or tag for the EM, VB (with ?x =
?y = 0.1) and EM-25 estimators (EM-25 is the EM
estimator with 25 hidden states).
lihoods were significantly higher (i.e., the models fit
better) than those found without annealing.
The evaluation of the models produced by the
EM and other estimators is presented in Table 1.
It is difficult to compare these with previous work,
but Haghighi and Klein (2006) report that in a
completely unsupervised setting, their MRF model,
which uses a large set of additional features and a
more complex estimation procedure, achieves an av-
erage 1-to-1 accuracy of 41.3%. Because they pro-
vide no information about the variance in this accu-
racy it is difficult to tell whether there is a signifi-
cant difference between their estimator and the EM
estimator, but it is clear that when EM is run long
enough, the performance of even very simple mod-
els like the bitag HMM is better than generally rec-
ognized.
As Table 1 makes clear, the EM estimator pro-
duces models that are extremely competitive in
many-to-1 accuracy and Variation of Information,
but are significantly worse in 1-to-1 accuracy. We
can understand these results by comparing the dis-
tribution of words to hidden states to the distribution
of words to POS tags in the gold-standard evaluation
corpus. As Figure 3 shows, the distribution of words
to POS tags is highly skewed, with just 6 POS tags,
NN, IN, NNP, DT, JJ and NNS, accounting for over
55% of the tokens in the corpus. By contrast, the
EM distribution is much flatter. This also explains
why the many-to-1 accuracy is so much better than
the one-to-one accuracy; presumably several hidden
299
Estimator 1-to-1 Many-to-1 VI H(T |Y ) H(Y |T )
EM (50) 0.40 (0.02) 0.62 (0.01) 4.46 (0.08) 1.75 (0.04) 2.71 (0.06)
VB(0.1, 0.1) (50) 0.47 (0.02) 0.50 (0.02) 4.28 (0.09) 2.39 (0.07) 1.89 (0.06)
VB(0.1, 10?4) (50) 0.46 (0.03) 0.50 (0.02) 4.28 (0.11) 2.39 (0.08) 1.90 (0.07)
VB(10?4, 0.1) (50) 0.42 (0.02) 0.60 (0.01) 4.63 (0.07) 1.86 (0.03) 2.77 (0.05)
VB(10?4, 10?4) (50) 0.42 (0.02) 0.60 (0.01) 4.62 (0.07) 1.85 (0.03) 2.76 (0.06)
GS(0.1, 0.1) (50) 0.37 (0.02) 0.51 (0.01) 5.45 (0.07) 2.35 (0.09) 3.20 (0.03)
GS(0.1, 10?4) (50) 0.38 (0.01) 0.51 (0.01) 5.47 (0.04) 2.26 (0.03) 3.22 (0.01)
GS(10?4, 0.1) (50) 0.36 (0.02) 0.49 (0.01) 5.73 (0.05) 2.41 (0.04) 3.31 (0.03)
GS(10?4, 10?4) (50) 0.37 (0.02) 0.49 (0.01) 5.74 (0.03) 2.42 (0.02) 3.32 (0.02)
EM (40) 0.42 (0.03) 0.60 (0.02) 4.37 (0.14) 1.84 (0.07) 2.55 (0.08)
EM (25) 0.46 (0.03) 0.56 (0.02) 4.23 (0.17) 2.05 (0.09) 2.19 (0.08)
EM (10) 0.41 (0.01) 0.43 (0.01) 4.32 (0.04) 2.74 (0.03) 1.58 (0.05)
Table 1: Evaluation of models produced by the various estimators. The values of the Dirichlet prior param-
eters for ?x and ?y appear in the estimator name for the VB and GS estimators, and the number of hidden
states is given in parentheses. Reported values are means over all runs, followed by standard deviations.
10 runs were performed for each of the EM and VB estimators, while 5 runs were performed for the GS
estimators. Each EM and VB run consisted of 1,000 iterations, while each GS run consisted of 50,000 it-
erations. For the estimators with 10 runs, a 3-standard error 95% confidence interval is approximately the
same as the standard deviation.
states are being mapped onto a single POS tag. This
is also consistent with the fact that the cross-entropy
H(T |Y ) of tags given hidden states is relatively low
(i.e., given a hidden state, the tag is relatively pre-
dictable), while the cross-entropy H(Y |T ) is rela-
tively high.
4 Bayesian estimation via Gibbs Sampling
and Variational Bayes
A Bayesian estimator combines a likelihood term
P(x|?, ?) and a prior P(?, ?) to estimate the poste-
rior probability of a model or hidden state sequence.
We can use a Bayesian prior to bias our estimator
towards models that generate more skewed distri-
butions. Because HMMs (and PCFGs) are prod-
ucts of multinomials, Dirichlet distributions are a
particularly natural choice for the priors since they
are conjugate to multinomials, which simplifies both
the mathematical and computational aspects of the
problem. The precise form of the model we investi-
gated is:
?y | ?y ? Dir(?y)
?y | ?x ? Dir(?x)
yi | yi?1 = y ? Multi(?y)
xi | yi = y ? Multi(?y)
Informally, ?y controls the sparsity of the state-to-
state transition probabilities while ?x controls the
sparsity of the state-to-observation emission proba-
bilities. As ?x approaches zero the prior strongly
prefers models in which each hidden state emits
as few words as possible. This captures the intu-
ition that most word types only belong to one POS,
since the minimum number of non-zero state-to-
observation transitions occurs when each observa-
tion type is emitted from only one state. Similarly,
as ?y approaches zero the state-to-state transitions
become sparser.
There are two main techniques for Bayesian esti-
mation of such models: Markov Chain Monte Carlo
(MCMC) and Variational Bayes (VB). MCMC en-
compasses a broad range of sampling techniques,
including component-wise Gibbs sampling, which
is the MCMC technique we used here (Robert and
Casella, 2004; Bishop, 2006). In general, MCMC
techniques do not produce a single model that char-
acterizes the posterior, but instead produce a stream
of samples from the posterior. The application of
MCMC techniques, including Gibbs sampling, to
HMM inference problems is relatively well-known:
see Besag (2004) for a tutorial introduction and
Goldwater and Griffiths (2007) for an application
of Gibbs sampling to HMM inference for semi-
300
supervised and unsupervised POS tagging.
The Gibbs sampler produces state sequences y
sampled from the posterior distribution:
P(y|x, ?) ?
?
P(x,y|?, ?)P(?|?y)P(?|?x) d? d?
Because Dirichlet priors are conjugate to multino-
mials, it is possible to integrate out the model pa-
rameters ? and ? to yield the conditional distribu-
tion for yi shown in Figure 4. For each observation
xi in turn, we resample its state yi conditioned on
the states y?i of the other observations; eventually
the distribution of state sequences converges to the
desired posterior.
Each iteration of the Gibbs sampler is much faster
than the Forward-Backward algorithm (both take
time linear in the length of the string, but for an
HMM with s hidden states, each iteration of the
Gibbs sampler takes O(s) time while each iteration
of the Forward-Backward algorithm takes O(s2)
time), so we ran 50,000 iterations of all samplers
(which takes roughly the same elapsed time as 1,000
Forward-Backward iterations).
As can be seen from Table 1, the posterior state
sequences we obtained are not particularly good.
Further, when we examined how the posterior like-
lihoods varied with increasing iterations of Gibbs
sampling, it became apparent that the likelihood was
still increasing after 50,000 iterations. Moreover,
when comparing posterior likelihoods from differ-
ent runs with the same prior parameters but differ-
ent random number seeds, none of the likelihoods
crossed, which one would expect if the samplers
had converged and were mixing well (Robert and
Casella, 2004). Just as with EM, we experimented
with a variety of annealing regimes, but were unable
to find any which significantly improved accuracy or
posterior likelihood.
We also experimented with evaluating state se-
quences found using maximum posterior decoding
(i.e., model parameters are estimated from the pos-
terior sample, and used to perform maximum poste-
rior decoding) rather than the samples from the pos-
terior produced by the Gibbs sampler. We found that
the maximum posterior decoding sequences usually
scored higher than the posterior samples, but the
scores converged after the first thousand iterations.
Since the posterior samples are produced as a by-
product of Gibbs sampling while maximum poste-
rior decoding requires an additional time consuming
step that does not have much impact on scores, we
used the posterior samples to produce the results in
Table 1.
In contrast to MCMC, Variational Bayesian in-
ference attempts to find the function Q(y, ?, ?) that
minimizes an upper bound of the negative log likeli-
hood (Jordan et al, 1999):
? log P(x)
= ? log
?
Q(y, ?, ?)
P(x,y, ?, ?)
Q(y, ?, ?)
dy d? d?
? ?
?
Q(y, ?, ?) log
P(x,y, ?, ?)
Q(y, ?, ?)
dy d? d?(3)
The upper bound in (3) is called the Variational Free
Energy. We make a ?mean-field? assumption that
the posterior can be well approximated by a factor-
ized modelQ in which the state sequence y does not
covary with the model parameters ?, ? (this will be
true if, for example, there is sufficient data that the
posterior distribution has a peaked mode):
P(x,y, ?, ?) ? Q(y, ?, ?) = Q1(y)Q2(?, ?)
The calculus of variations is used to minimize the
KL divergence between the desired posterior distri-
bution and the factorized approximation. It turns
out that if the likelihood and conjugate prior be-
long to exponential families then the optimalQ1 and
Q2 do too, and there is an EM-like iterative pro-
cedure that finds locally-optimal model parameters
(Bishop, 2006).
This procedure is especially attractive for HMM
inference, since it involves only a minor modifica-
tion to the M-step of the Forward-Backward algo-
rithm. MacKay (1997) and Beal (2003) describe
Variational Bayesian (VB) inference for HMMs in
detail, and Kurihara and Sato (2006) describe VB
for PCFGs (which only involves a minor modifica-
tion to the M-step of the Inside-Outside algorithm).
Specifically, the E-step for VB inference for HMMs
is the same as in EM, while theM-step is as follows:
??(`+1)y?|y = f(E[ny?,y] + ?y)/f(E[ny] + s?y) (4)
??(`+1)x|y = f(E[nx,y] + ?x)/f(E[ny] +m?x)
f(v) = exp(?(v))
?(v) = (v > 7) ? g(v ? 12) : (?(v + 1)? 1)/v
g(x) ? log(x) + 0.04167x?2 + 0.00729x?4
+0.00384x?6 ? 0.00413x?8 . . . (5)
301
P(yi|x,y?i, ?) ?
(
nxi,yi + ?x
nyi +m?x
) (
nyi,yi?1 + ?y
nyi?1 + s?y
) (
nyi+1,yi + I(yi?1 = yi = yi+1) + ?y
nyi + I(yi?1 = yi)
)
Figure 4: The conditional distribution for state yi used in the Gibbs sampler, which conditions on the states
y?i for all observations except xi. Here m is the number of possible observations (i.e., the size of the
vocabulary), s is the number of hidden states and I(?) is the indicator function (i.e., equal to one if its
argument is true and zero otherwise), nx,y is the number of times observation x occurs with state y, ny?,y is
the number of times state y? follows y, and ny is the number of times state y occurs; these counts are from
(x?i,y?i), i.e., excluding xi and yi.
 0 1
 2
 0  1  2
Figure 5: The scaling function y = f(x) =
exp?(x) (curved line), which is bounded above by
the line y = x and below by the line y = x? 0.5.
where ? is the digamma function (the derivative of
the log gamma function; (5) gives an asymptotic ap-
proximation), and the remaining quantities are just
as in the EM updates (2), i.e., nx,y is the number of
times observation x occurs with state y, ny?,y is the
number of times state y? follows y, ny is the number
of occurences of state y, s is the number of hidden
states and m is the number of observations; all ex-
pectations are taken with respect to the variational
parameters (??(`), ??(`)).
A comparison between (4) and (2) reveals two dif-
ferences between the EM and VB updates. First,
the Dirichlet prior parameters ? are added to the
expected counts. Second, these posterior counts
(which are in fact parameters of the Dirichlet pos-
terior Q2) are passed through the function f(v) =
exp?(v), which is plotted in Figure 5. When v 
0, f(v) ? v ? 0.5, so roughly speaking, VB for
multinomials involves adding ??0.5 to the expected
counts when they are much larger than zero, where
? is the Dirichlet prior parameter. Thus VB can
be viewed as a more principled version of the well-
known ad hoc technique for approximating Bayesian
estimation with EM that involves adding ??1 to the
expected counts. However, in the ad hoc approach
the expected count plus ??1 may be less than zero,
resulting in a value of zero for the corresponding pa-
rameter (Johnson et al, 2007; Goldwater and Grif-
fiths, 2007). VB avoids this problem because f(v) is
always positive when v > 0, even when v is small.
Note that because the counts are passed through f ,
the updated values for ?? and ?? in (4) are in general
not normalized; this is because the variational free
energy is only an upper bound on the negative log
likelihood (Beal, 2003).
We found that in general VB performed much bet-
ter than GS. Computationally it is very similar to
EM, and each iteration takes essentially the same
time as an EM iteration. Again, we experimented
with annealing in the hope of speeding convergence,
but could not find an annealing schedule that signifi-
cantly lowered the variational free energy (the quan-
tity that VB optimizes). While we had hoped that the
Bayesian prior would bias VB toward a common so-
lution, we found the same sensitivity to initial condi-
tions as we found with EM, so just as for EM, we ran
the estimator for 1,000 iterations with 10 different
random initializations for each combination of prior
parameters. Table 1 presents the results of VB runs
with several different values for the Dirichlet prior
parameters. Interestingly, we obtained our best per-
formance on 1-to-1 accuracy when the Dirchlet prior
?x = 0.1, a relatively large number, but best per-
formance on many-to-1 accuracy was achieved with
a much lower value for the Dirichlet prior, namely
?x = 10?4. The Dirichlet prior ?y that controls
302
sparsity of the state-to-state transitions had little ef-
fect on the results. We did not have computational
resources to fully explore other values for the prior
(a set of 10 runs for one set of parameter values takes
25 computer days).
As Figure 3 shows, VB can produce distributions
of hidden states that are peaked in the same way that
POS tags are. In fact, with the priors used here, VB
produces state sequences in which only a subset of
the possible HMM states are in fact assigned to ob-
servations. This shows that rather than fixing the
number of hidden states in advance, the Bayesian
prior can determine the number of states; this idea is
more fully developed in the infinite HMM of Beal et
al. (2002) and Teh et al (2006).
5 Reducing the number of hidden states
EM already performs well in terms of the many-to-1
accuracy, but we wondered if there might be some
way to improve its 1-to-1 accuracy and VI score. In
section 3 we suggested that one reason for its poor
performance in these evaluations is that the distri-
butions of hidden states it finds tend to be fairly
flat, compared to the empirical distribution of POS
tags. As section 4 showed, a suitable Bayesian prior
can bias the estimator towards more peaked distribu-
tions, but we wondered if there might be a simpler
way of achieving the same result.
We experimented with dramatic reductions in the
number of hidden states in the HMMs estimated
by EM. This should force the hidden states to be
more densely populated and improve 1-to-1 accu-
racy, even though this means that there will be no
hidden states that can possibly map onto the less fre-
quent POS tags (i.e., we will get these words wrong).
In effect, we abandon the low-frequency POS tags
in the hope of improving the 1-to-1 accuracy of the
high-frequency tags.
As Table 1 shows, this markedly improves both
the 1-to-1 accuracy and the VI score. A 25-state
HMM estimated by EM performs effectively as well
as the best VB model in terms of both 1-to-1 accu-
racy and VI score, and runs 4 times faster because it
has only half the number of hidden states.
6 Conclusion and future work
This paper studied why EM seems to do so badly in
HMM estimation for unsupervised POS tagging. In
fact, we found that it doesn?t do so badly at all: the
bitag HMM estimated by EM achieves a mean 1-to-
1 tagging accuracy of 40%, which is approximately
the same as the 41.3% reported by (Haghighi and
Klein, 2006) for their sophisticated MRF model.
Then we noted the distribution of words to hidden
states found by EM is relatively uniform, compared
to the distribution of words to POS tags in the eval-
uation corpus. This provides an explanation of why
the many-to-1 accuracy of EM is so high while the
1-to-1 accuracy and VI of EM is comparatively low.
We showed that either by using a suitable Bayesian
prior or by simply reducing the number of hidden
states it is possible to significantly improve both the
1-to-1 accuracy and the VI score, achieving a 1-to-1
tagging accuracy of 46%.
We also showed that EM and other estimators take
much longer to converge than usually thought, and
often require several hundred iterations to achieve
optimal performance. We also found that there is
considerable variance in the performance of all of
these estimators, so in general multiple runs from
different random starting points are necessary in or-
der to evaluate an estimator?s performance.
Finally, there may be more sophisticated ways of
improving the 1-to-1 accuracy and VI score than
the relatively crude methods used here that primar-
ily reduce the number of available states. For ex-
ample, we might obtain better performance by us-
ing EM to infer an HMM with a large number of
states, and then using some kind of distributional
clustering to group similar HMM states; these clus-
ters, rather than the underlying states, would be in-
terpreted as the POS tag labels. Also, the Bayesian
framework permits a wide variety of different priors
besides Dirichlet priors explored here. For example,
it should be possible to encode linguistic knowledge
such markedness preferences in a prior, and there
are other linguistically uninformative priors, such
the ?entropic priors? of Brand (1999), that may be
worth exploring.
Acknowledgements
I would like to thank Microsoft Research for pro-
viding an excellent environment in which to con-
duct this work, and my friends and colleagues at
Microsoft Research, especially Bob Moore, Chris
Quirk and Kristina Toutanova, for their helpful com-
ments on this paper.
303
References
Michele Banko and Robert C. Moore. 2004. Part of
speech tagging in context. In Proceedings, 20th In-
ternational Conference on Computational Linguistics
(Coling 2004), pages 556?561, Geneva, Switzerland.
M.J. Beal, Z. Ghahramani, and C.E. Rasmussen. 2002.
The infinite Hidden Markov Model. In T. Dietterich,
S. Becker, and Z. Ghahramani, editors, Advances in
Neural Information Processing Systems, volume 14,
pages 577?584. The MIT Press.
Matthew J. Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Gatsby
Computational Neuroscience unit, University College
London.
Julian Besag. 2004. An introduction to Markov Chain
Monte Carlo methods. In Mark Johnson, Sanjeev P.
Khudanpur, Mari Ostendorf, and Roni Rosenfeld, ed-
itors, Mathematical Foundations of Speech and Lan-
guage Processing, pages 247?270. Springer, New
York.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
M. Brand. 1999. An entropic estimator for structure dis-
covery. Advances in Neural Information Processing
Systems, 11:723?729.
Glenn Carroll and Eugene Charniak. 1992. Two experi-
ments on learning probabilistic dependency grammars
from corpora. In Proceedings of the AAAI Workshop
on Statistically-Based Natural Language Processing
Techniques, San Jose, CA.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In 10th Conference of the European Chapter of
the Association for Computational Linguistics, pages
59?66. Association for Computational Linguistics.
Victoria Fromkin, editor. 2001. Linguistics: An Intro-
duction to Linguistic Theory. Blackwell, Oxford, UK.
Sharon Goldwater and Tom Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics.
Joshua Goodman. 2001. A bit of progress in language
modeling. Computer Speech and Language, 14:403?
434.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 320?327, New York
City, USA, June. Association for Computational Lin-
guistics.
Frederick Jelinek. 1997. Statistical Methods for Speech
Recognition. The MIT Press, Cambridge, Mas-
sachusetts.
Mark Johnson, Tom Griffiths, and Sharon Goldwater.
2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 139?146,
Rochester, New York. Association for Computational
Linguistics.
Michael I. Jordan, Zoubin Ghahramani, Tommi S.
Jaakkola, and Lawrence K. Sau. 1999. An introduc-
tion to variational methods for graphical models. Ma-
chine Learning, 37(2):183?233.
Dan Klein. 2005. The Unsupervised Learning of Natural
Language Structure. Ph.D. thesis, Stanford Univer-
sity.
Kenichi Kurihara and Taisuke Sato. 2006. Variational
Bayesian grammar induction for natural language. In
8th International Colloquium on Grammatical Infer-
ence.
David J.C. MacKay. 1997. Ensemble learning for hidden
Markov models. Technical report, Cavendish Labora-
tory, Cambridge.
Chris Manning and Hinrich Schu?tze. 1999. Foundations
of Statistical Natural Language Processing. The MIT
Press, Cambridge, Massachusetts.
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Marina Meila?. 2003. Comparing clusterings by the vari-
ation of information. In Bernhard Scho?lkopf and Man-
fred K. Warmuth, editors, COLT 2003: The Sixteenth
Annual Conference on Learning Theory, volume 2777
of Lecture Notes in Computer Science, pages 173?187.
Springer.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20:155?171.
M. Mitzenmacher. 2003. A brief history of generative
models for power law and lognormal distributions. In-
ternet Mathematics, 1(2):226?251.
Christian P. Robert and George Casella. 2004. Monte
Carlo Statistical Methods. Springer.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting of the
304
Association for Computational Linguistics (ACL?05),
pages 354?362, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
Noah A. Smith. 2006. Novel Estimation Methods for
Unsupervised Discovery of Latent Structure in Natu-
ral Language Text. Ph.D. thesis, Johns Hopkins Uni-
versity.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101(476):1566?1581.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 252?
259.
Qin Iris Wang and Dale Schuurmans. 2005. Improved
estimation for unsupervised part-of-speech tagging. In
Proceedings of the 2005 IEEE International Confer-
ence on Natural Language Processing and Knowledge
Engineering (IEEE NLP-KE?2005), pages 219?224,
Wuhan, China.
305
