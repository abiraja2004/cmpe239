Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 71?75,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Supervised Coreference Resolution with SUCRE
Hamidreza Kobdani and Hinrich Schu?tze
Institute for Natural Language Processing
University of Stuttgart, Germany
kobdani@ims.uni-stuttgart.de
Abstract
In this paper we present SUCRE (Kobdani
and Schu?tze, 2010) that is a modular coref-
erence resolution system participating in the
CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNote (Pradhan et
al., 2011). The SUCRE?s modular architecture
provides a clean separation between data stor-
age, feature engineering and machine learning
algorithms.
1 Introduction
Noun phrase coreference resolution is the process
of finding markables (noun phrase) referring to the
same real world entity or concept. In other words,
this process groups the markables of a document
into entities (equivalence classes) so that all mark-
ables in an entity are coreferent. Examples of ap-
plications of coreference resolution are Informa-
tion Extraction, Question Answering and Automatic
Summarization.
Coreference is an equivalence relation between
two markables, i.e., it is reflexive, symmetric and
transitive. The first solution that intuitively comes
to mind is binary classification of markable pairs
(links). Therefore at the heart of most existing ap-
proaches there is a binary classifier that classifies
links to coreferent/disreferent. One can also use the
transitive property of coreference relation to build
the entities; this is done using a clustering method.
Our approach in this paper consist of the above
mentioned steps, namely:
1. Classification of links to coreferent/disreferent.
2. Clustering of links which are classified as
coreferent.
This paper is organized as follows. In Section 2,
we present our feature engineering approach. Sec-
tion 3 presents the system architecture. Data set is
described in Section 4. Sections 5 and 6 present re-
sults and conclusions.
2 Feature Engineering
In recent years there has been substantial work on
the problem of coreference resolution. Most meth-
ods present and report on the benchmark data sets
for English. The feature sets they use are based on
(Soon et al, 2001). These features consist of string-
based features, distance features, span features, part-
of-speech features, grammatical features, and agree-
ment features.
We defined a comprehensive set of features based
on previous coreference resolution systems for En-
glish, e.g. (Bengtson and Roth, 2008). In the com-
mon approach to coreference resolution we have
chosen, features are link features, i.e., features are
defined over a pair of markables. For link feature
definition and extraction, the head words of mark-
ables are usually used, but in some cases the head
word is not a suitable choice. For example, con-
sider these two markables: the book and a book, in
both cases book is the head word but to distinguish
which markable is definite and which indefinite ad-
ditional information about the markables has to be
taken into account. Now consider these two mark-
ables: the university students in Germany and the
university students in France in this case the head
words and the first four words of each markable
are the same but they cannot be coreferent, and this
could be detected only by looking at the entire noun
phrase. Some features require complex preprocess-
71
ing or complex definitions. Consider the two mark-
ables the members of parliament and the members of
the European Union. The semantic class ofmembers
is person in the first case and country in the second.
To cover all such cases, we introduced a feature defi-
nition language (Kobdani et al, 2010). With the fea-
ture definition language we will be able to access all
information that is connected to a markable, includ-
ing the first, last and head words of the two mark-
ables; all other words of the two markables; and the
two markables as atomic elements.
After defining new features (new definition from
scratch or definition by combination of existing fea-
tures), we have to evaluate them. In principle, we
could use any figure of merit to evaluate the useful-
ness of a feature or to compare two similar features,
including Gini coefficient, mutual information, and
correlation coefficient. In our current system, ex-
pected information gain (IG) and information gain
ratio (IGR) are used.
As an example, consider the following two fea-
tures, which can be considered different attempts to
formalize the same linguistic property:
1. The noun phrase has a subject role and is def-
inite (e.g. markable begins with a definite arti-
cle)
2. The noun phrase has a subject role and is not
indefinite (e.g. markable begins with an indefi-
nite article)
The information gain ratios of the above men-
tioned features are equal to 0.0026 for the first and
0.0051 for the second one ? this shows that the sec-
ond one is a better choice. We now define IG and
IGR.
The change in entropy from a prior state to a state
that takes some information is the expected informa-
tion gain (Mitchell, 1997):
IG (f) = H (C) ? Hf (C) (1)
Where f is the feature value, C its corresponding
class, and entropy is defined as follows:
H (C) = ?
?
i
P (Ci) log2P (Ci) (2)
Hf (C) =
?
f
|Cf |
|C| H (Cf ) (3)
If a feature takes a large number of distinct values,
the information gain would not be a good measure
for deciding its relevance. In such cases the infor-
mation gain ratio is used instead. The information
gain ratio for a feature is calculated as follows:
IGR (f) = IG (f)
SInf (C) (4)
SInf(C) = ?
?
i
|Ci|
|C| log2
|Ci|
|C| (5)
Equation (4) can be used as an indicator for which
features are likely to improve classification accu-
racy.
3 System Architecture
The architecture of the system has two main parts:
preprocessing and coreference resolution.
In preprocessing the text corpus is converted to
a relational data model. The main purpose of the
relational model in our system is the use of a fea-
ture definition language (Kobdani et al, 2010). Af-
ter modeling the text corpus, coreference resolution
can be performed.
The main steps of the system are presented as fol-
lows.
3.1 Preliminary text conversion
In this step, tokens are extracted from the corpus. In
the CoNLL-2011 Shared Task this step is as simple
as reading each line of the input data set and extract-
ing its corresponding token.
3.2 Atomic attributes of tokens
Atomic features of the tokens are extracted
in this step. The extracted atomic features
are: part of speech, number, pronoun person
(first, second and third), pronoun type (subjec-
tive,,predeterminer,reflexive,objective and posses-
sive), WordNet semantic class and gender.
We use a rather simple method to extract semantic
class of each token from WordNet. We look at the
synonyms of the token and if one of them is in the
predefined keyword set, we take it as its correspond-
ing semantic class. The example of the keywords
are person, time, abstraction, device, human action,
organization, place and animal.
72
3.3 Markable Detection
In this step all noun phrases from the parse tree are
extracted. After clustering step all markables which
are not included in a chain are deleted from the list
of markables. In other word we will not have any
cluster with less than 2 members.
Figure 1 presents the simple markable detection
method which we used in the SUCRE.
3.4 Atomic attributes of markables
In this step, the atomic attributes of the markables
are extracted. In the data set of the CoNLL-2011
shared task the named entity property of a markable
can be used as its atomic attribute.
3.5 Link Generator
For training, the system generates a positive train-
ing instance for an adjacent coreferent markable pair
(m, n) and negative training instances for the mark-
able m and all markables disreferent with m that oc-
cur before n (Soon et al, 2001). For decoding it
generates all the possible links inside a window of
100 markables.
3.6 Link feature definition and extraction
The output of the link generator, which is the list of
the generated links, is the input to the link feature
extractor for creating train and test data sets. To do
this, the feature definitions are used to extract the
feature values of the links (Kobdani et al, 2011).
3.7 Learning
For learning we implemented a decision tree classi-
fier (Quinlan, 1993). To achieve state-of-the-art per-
formance, in addition to decision tree we also tried
support vector machine and maximum entropy that
did not perform better than decision tree.
3.8 Classification and Clustering
In this part, the links inside one document are clas-
sified then the coreference chains are created. We
use best-first clustering for this purpose. It searches
for the best predicted antecedent from right to left
starting from the end of the document. For the docu-
ments with more than a predefined number of mark-
ables we apply a limit for searching. In this way, in
addition to better efficiency, the results also improve.
Markable Detection PSG A (W1, W2, . . . , Wn)
1. A markable M is presented by a set of
three words:
Begin (Mb), End (Me) and Head (Mh).
2. Let DM be the set of detected markables.
3. Let Ti be the node i in the parse tree with
label Li
(if node is a word then Li is equal to Wi).
4. Start from parse tree root Tr:
Find Markables(Tr,Lr,DM )
Find Markables(T ,L,DM )
1. If L is equal to noun phrase, then extract
the markable M :
(a) Set the begin word of the markable:
Mb = Noun Phrase Begin(T ,L)
(b) Set the end word of the markable:
Me = Noun Phrase End(T ,L)
(c) Set the head word of the markable:
Mh = Noun Phrase Head(T ,L)
(d) Add the markable M to the set of de-
tected markables DM .
2. Repeat for all Ti the daughters of T :
Find Markables(Ti,Li,DM )
Noun Phrase Begin(T ,L)
If T has no daughter then return L;
else set Tb to the first daughter of T and return
Noun Phrase Begin(Tb,Lb).
Noun Phrase End(T ,L)
If T has no daughter then return L;
else set Tb to the last daughter of T and return
Noun Phrase End(Tb,Lb).
Noun Phrase Head(T ,L)
If T has no daughter then return L;
else set Th to the biggest noun phrase daughter
of T and return Noun Phrase Head(Th,Lh).
Figure 1: Markable Detection from Parse Tree (all possi-
ble markables) .
73
Automatic Gold
Rec. Prec. F1 Rec. Prec. F1
MD 60.17 60.92 60.55 62.50 61.62 62.06
MUC 54.30 51.84 53.06 57.44 53.15 55.21
B3 71.39 64.68 67.87 74.07 64.39 68.89
CEAFM 46.36 46.36 46.36 47.07 47.07 47.07
CEAFE 35.38 37.26 35.30 35.19 38.44 36.74
BLANC 65.01 64.93 64.97 66.23 65.16 65.67
Table 1: Results of SUCRE on the development data set
for the automatically detected markables. MD: Markable
Detection.
4 Data Sets
OntoNotes has been used for the CoNLL-2011
shared task. The OntoNotes project 1 is to provide
a large-scale, accurate corpus for general anaphoric
coreference. It aims to cover entities and events (i.e.
it is not limited to noun phrases or a limited set of
entity types) (Pradhan et al, 2007).
For training we used 4674 documents containing
a total of 1909175 tokens, 190700 markables and
50612 chains.
SUCRE participated in the closed track of the
shared task. Experiments have been performed for
the two kind of documents, namely, the automati-
cally preprocessed documents and the gold prepro-
cessed documents. In this paper, we report only the
scores on the development data set using the offi-
cial scorer of the shared task. The automatically
preprocessed part consists of 303 documents con-
taining a total of 136257 tokens, 52189 automati-
cally detected markables, 14291 true markables and
3752 chains. The gold preprocessed part consists of
303 documents containing a total of 136257 tokens,
52262 automatically detected markables, 13789 true
markables and 3752 chains.
5 Results
We report recall, precision, and F1 for MUC (Vi-
lain et al, 1995), B3 (Bagga and Baldwin, 1998),
CEAFM /CEAFE (Luo, 2005) and BLANC (Re-
casens et al, 2010).
Table 1 presents results of our system for the
automatically detected markables. It is apparent
from this table that the application of the gold pre-
processed documents slightly improves the perfor-
mance (MD-F1: +1.51; MUC-F1: +2.15; B3-F1:
1http://www.bbn.com/ontonotes/
Automatic Gold
Rec. Prec. F1 Rec. Prec. F1
MUC 58.63 87.88 70.34 60.48 88.25 71.78
B3 57.91 86.47 69.36 59.21 86.25 70.22
CEAFM 59.81 59.81 59.81 60.91 60.91 60.91
CEAFE 70.49 36.43 48.04 71.09 37.73 49.30
BLANC 69.67 76.27 72.34 70.34 76.01 72.71
Table 2: Results of SUCRE on the development data set
for the true markables (i.e. no singletone is included).
+1.02; CEAFM -F1: +0.71; CEAFE-F1: +1.44;
BLANC-F1: +0.70 ).
Table 2 presents results of our system for the true
markables that were all and only part of coreference
chains. Again the results show that the application
of gold preprocessed documents slightly improves
the performance (MUC-F1: +1.44; B3-F1: +0.86;
CEAFM -F1: +1.1; CEAFE-F1: +1.26; BLANC-F1:
+0.37 ).
Comparing the results of tables 1 and 2, there is a
significant difference between the scores on the au-
tomatically detected markables and the scores on the
true markables (e.g. for the automatically prepro-
cessed documents: MUC-F1: +17.28; CEAFM -F1:
+13.45; CEAFE-F1: +12.74; BLANC-F1: +7.37).
No significant improvement in B3 is seen (auto-
matic: +1.49; gold: +1.33). We suspect that this is
partly due to the very sensitive nature of B3 against
the singleton chains. Because in the implementation
of scorer for the CoNLL-2011 shared task the non-
detected key markables are automatically included
into the response as singletons.
6 Conclusion
In this paper, we have presented our system SUCRE
participated in the CoNLL-2011 shared task. We
took a deeper look at the feature engineering of SU-
CRE. We presented the markable detection method
we applied.
We showed that the application of the gold pre-
processed documents improves the performance. It
has been demonstrated that the availability of the
true markables significantly improves the results.
Also it has been shown that the singletons have a
large impact on the B3 scores.
74
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In LREC Workshop on
Linguistics Coreference ?98, pages 563?566.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
EMNLP ?08, pages 294?303.
Hamidreza Kobdani and Hinrich Schu?tze. 2010. Sucre:
A modular system for coreference resolution. In Se-
mEval ?10, pages 92?95.
Hamidreza Kobdani, Hinrich Schu?tze, Andre Burkovski,
Wiltrud Kessler, and Gunther Heidemann. 2010. Re-
lational feature engineering of natural language pro-
cessing. In CIKM ?10. ACM.
Hamidreza Kobdani, Hinrich Schu?tze, Michael
Schiehlen, and Hans Kamp. 2011. Bootstrap-
ping coreference resolution using word associations.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics, ACL ?11.
Association for Computational Linguistics.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In HLT ?05, pages 25?32.
Tom M. Mitchell. 1997. Machine Learning. McGraw-
Hill, New York.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted Coreference: Identifying Entities and Events
in OntoNotes. In in Proceedings of the IEEE Inter-
national Conference on Semantic Computing (ICSC),
September 17-19.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011), Portland, Oregon,
June.
J. Ross Quinlan. 1993. C4.5: Programs for machine
learning. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena,
M.Anto`nia Mart??, Mariona Taule?, Ve?ronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
SemEval-2010 Task 1: Coreference resolution in
multiple languages. In SemEval ?10, pages 70?75.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to coref-
erence resolution of noun phrases. In CL ?01, pages
521?544.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In MUC6 ?95,
pages 45?52.
75
