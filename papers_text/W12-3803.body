Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 19?27, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
Linking Uncertainty in Physicians? Narratives to Diagnostic Correctness
Wilson McCoy
Department of Interactive
Games and Media
wgm4143@rit.edu
Cecilia Ovesdotter Alm
Department of English
coagla@rit.edu
Cara Calvelli
College of Health
Sciences and Technology
cfcscl@rit.edu
Jeff B. Pelz
Center for
Imaging Science
pelz@cis.rit.edu
Pengcheng Shi
Computing and
Information Sciences
pengcheng.shi@rit.edu
Rochester Institute of Technology
Anne Haake
Computing and
Information Sciences
anne.haake@rit.edu
Abstract
In the medical domain, misdiagnoses and di-
agnostic uncertainty put lives at risk and in-
cur substantial financial costs. Clearly, medi-
cal reasoning and decision-making need to be
better understood. We explore a possible link
between linguistic expression and diagnostic
correctness. We report on an unusual data set
of spoken diagnostic narratives used to com-
putationally model and predict diagnostic cor-
rectness based on automatically extracted and
linguistically motivated features that capture
physicians? uncertainty. A multimodal data
set was collected as dermatologists viewed im-
ages of skin conditions and explained their di-
agnostic process and observations aloud. We
discuss experimentation and analysis in initial
and secondary pilot studies. In both cases,
we experimented with computational model-
ing using features from the acoustic-prosodic
and lexical-structural linguistic modalities.
1 Introduction
Up to 20% of post-mortem diagnoses in the United
States are inconsistent with the diagnosis before
death (Graber, 2005). These misdiagnoses cost both
human lives and estimated millions of dollars every
year. To find where and why misdiagnoses occur, it
is necessary to improve our understanding of doc-
tors? diagnostic reasoning and how it is linked to di-
agnostic uncertainty and correctness. Our contribu-
tion begins to explore the computational modeling
of this phenomenon in diagnostic narratives. From a
cognitive science perspective, we are contributing to
the research on medical reasoning and how it is lin-
guistically expressed. In the long term, this area of
work could be a useful decision-making component
for flagging diagnoses that need further review.
The study used an unusual multimodal data set
collected in a modified Master-Apprentice interac-
tion scenario. It comprises both gaze and linguistic
data. The present study focuses on the linguistic data
which in turn can be conceptualized as consisting of
both acoustic-prosodic and lexical-structural modal-
ities. This data set can further be used to link vision
and language research to understand human cogni-
tion in expert decision-making scenarios.
We report on a study conducted in two phases.
First, an initial pilot study involved a preliminary an-
notation of a small subset of the collected diagnos-
tic narratives and also investigated the prediction of
diagnostic correctness using a set of linguistic fea-
tures from speech recordings and their verbal tran-
scriptions. This provided initial features relevant to
classification, helped us identify annotation issues,
and gave us insight on how to improve the annota-
tion scheme used for annotating ground truth data.
Next, a second pilot study was performed, build-
ing on what was learned in the initial pilot study.
The second pilot study involved a larger data set
with a revised and improved annotation scheme that
considered gradient correctness at different steps of
the diagnostic reasoning process: (1) medical lesion
morphology (e.g. recognizing the lesion type as a
scaly erythematous plaque), (2) differential diagno-
sis (i.e. providing a set of possible final diagnoses),
and (3) final diagnosis (e.g. identifying the disease
condition as psoriasis). We also experiment with
19
classification using an expanded feature set moti-
vated by the initial pilot study and by previously
published research. We report on results that con-
sider different algorithms, feature set modalities, di-
agnostic reasoning steps, and coarse vs. fine grained
classes as explained below in Section 4.3.
2 Previous Work
Much work has been done in the area of medi-
cal decision-making. Pelaccia et al. (2011) have
viewed clinical reasoning through the lens of dual-
process theory. They posit that two systems are at
work in the mind of a clinician: the intuitive system
which quickly produces a response based on expe-
rience and a holistic view of the situation, versus
the analytic system which slowly and logically steps
through the problem with conscious use of knowl-
edge. Croskerry (2009) stated that ?[i]f the presen-
tation is not recognized, or if it is unduly ambiguous
or there is uncertainty, [analytic] processes engage
instead? (p. 1022); for instance, if a clinician is un-
familiar with a disease or unsure of their intuitive
answer. We assume that different reasoning systems
may cause changes in linguistic behaviors. For ex-
ample, when engaging the slower analytic system, it
seems reasonable that frequent pausing could appear
as an indication of, e.g., uncertainty or thoughtful-
ness.
Several studies have explored the task of detect-
ing uncertainty through language. Uncertainty de-
tection necessitates inference of extra-propositional
meaning and is arguably a subjective natural lan-
guage problem, i.e. part of a family of problems
that are increasingly receiving attention in compu-
tational linguistics. These problems involve more
dynamic classification targets and different perfor-
mance expectations (Alm, 2011). Pon-Barry and
Shieber (2009) have shown encouraging results in
finding uncertainty using acoustic-prosodic features
at the word, word?s local context, and whole utter-
ance levels. Henriksson and Velupillai (2010) used
?speculative words? (e.g., could, generally, should,
may, sort of, etc.) as well as ?certainty ampli-
fiers? (e.g., definitely, positively, must, etc.) to deter-
mine uncertainty in text. Velupillai (2010) also ap-
plied the same approach to medical texts and noted
that acoustic-prosodic features should be considered
alongside salient lexical-structural features as indi-
cators of uncertainty. In this work, we draw on the
insight of such previous work, but we also extend
the types of linguistic evidence considered for iden-
tifying possible links to diagnostic correctness.
As another type of linguistic evidence, disfluen-
cies make up potentially important linguistic evi-
dence. Zwarts and Johnson (2011) found that the
occurrence of disfluencies that had been removed
could be predicted to a satisfactory degree. Pakho-
mov (1999) observed that such disfluencies are just
as common in monologues as in dialogues even
though there is no need for the speakers to indicate
that they wish to continue speaking. This finding is
important for the work presented here because our
modified use of the Master-Apprentice scenario re-
sults in a particular dialogic interaction with the lis-
tener remaining silent. Perhaps most importantly,
Clark and Fox Tree (2002) postulated that filled
pauses (e.g., um, uh, er, etc.) play a meaningful
role in speech. For example, they may signal that
the speaker is yet to finish speaking or searching for
a word. There is some controversy about this claim,
however, as explained by Corley and Stewart (2008).
The scholarly controversy about the role of disfluen-
cies indicates that more research is needed to under-
stand the disfluency phenomenon, including how it
relates to extra-propositional meaning.
3 Data Set
The original elicitation experiment included 16
physicians with dermatological expertise. Of these,
12 were attending physicians and 4 were residents
(i.e. dermatologists in training). The observers were
shown a series of 50 images of dermatological con-
ditions. The summary of this collected data is shown
in Table 1, with reference to the pilot studies.
The physicians were instructed to narrate, in En-
glish, their thoughts and observations about each im-
age to a student, who remained silent, as they arrived
at a differential diagnosis or a possible final diagno-
sis. This data elicitation approach is a modified ver-
sion of the Master-Apprentice interaction scenario
(Beyer and Holtzblatt, 1997). This elicitation setup
is shown in Figure 1. It allows us to extract in-
formation about the Master?s (i.e. in this case, the
physician?s) cognitive process by coaxing them to
20
Data parameters Quantity
# of participating doctors 16
# of images for which
narratives were collected 50
# of time-aligned narratives
in the initial pilot study 160
# of time-aligned narratives
in the second pilot study 707
Table 1: This table summarizes the data. Of the collected
narratives, 707 are included in this work; audio is unavail-
able for some narratives.
vocalize their thoughts in rich detail. This teaching-
oriented scenario really is a monologue, yet induces
a feeling of dialogic interaction in the Master.
Figure 1: The Master-Apprentice interaction scenario al-
lows us to extract information about the Master?s (here:
doctor?s) cognitive processes.
The form of narratives collected can be analyzed
in many ways. Figure 2 shows two narratives, re-
cently elicited and similar to the ones in the study?s
data set, that are used here with permission as ex-
amples. In terms of diagnostic reasoning styles, re-
ferring to Pelaccia et al. (2011), we can propose that
observer A may be using the intuitive system and
that observer B may be using the analytical system.
Observer A does not provide a differential diagnosis
and jumps straight to his/her final diagnosis, which
in this case is correct. We can postulate that observer
A looks at the general area of the lesion and uses
previous experience or heuristic knowledge to come
to the correct diagnosis. This presumed use of the
intuitive system could potentially relate to the depth
of previous experience with a disease, for example.
Observer B, on the other hand, might be using the
A. This patient has a pinkish papule with
surrounding hypopigmentation in a field of
other cherry hemagiomas and nevoid type
lesions. The only diagnosis that comes to
mind to me is Sutton?s nevus.
B. I think I?m looking at an abdomen, possibly.
I see a hypopigmented oval-shaped patch in
the center of the image. I see that there
are two brown macules as well. In the center
of the hypopigmented oval patch there
appears to be an area that may be a pink
macule. Differential diagnosis includes
halo nevus, melanoma, post-inflammatory
hypopigmentation. I favor a diagnosis of
maybe post-inflammatory hypopigmentation.
Figure 2: Two narratives collected in a recent elicitation
setup and used here with permission. Narratives A and B
are not part of the studied data set, but exemplify data set
narratives which could not be distributed. Observers A
and B are both looking at an image of a halo or Sutton?s
nevus as seen in Figure 3. Disfluencies are considered in
the experimental work but have been removed for read-
ability in these examples.
Figure 3: The image of a halo or Sutton?s nevus viewed
by the observers and the subject of example narratives.
analytical system. Observer B steps through the di-
agnosis in a methodical process and uses evidence
presented to rationalize the choice of final diagno-
sis. Observer B also provides a differential diagno-
sis unlike observer A. This suggests that observer
B is taking advantage of a process of elimination to
decide on a final diagnosis.
Another way to evaluate these narratives is in
terms of correctness and the related concept of diag-
21
nostic completeness. Whereas these newly elicited
narrative examples have not been annotated by doc-
tors, some observations can still be made. From the
point of view of final diagnosis, observer A is cor-
rect, unlike observer B. Assessment of diagnostic
correctness and completeness can also be made on
intermediate steps in the diagnostic process (e.g. dif-
ferential diagnoses or medical lesion morphological
description). Including such steps in the diagnos-
tic process is considered good practice. Observer A
does not supply a differential diagnosis and instead
skips to the final diagnosis. Observer B provides
the correct answer in the differential diagnosis but
gives the incorrect final diagnosis. Observer B fully
describes the medical lesion morphology presented.
Observer A, however, only describes the pink lesion
and does not discuss the other two brown lesions.
The speech of the diagnostic narratives was
recorded. At the same time, the observers? eye-
movements were tracked; the eye-tracking data
are considered in another report (Li et al., 2010).
We leave the integration of the linguistic and eye-
tracking data for future work.
After the collection of the raw audio data, the
utterances were manually transcribed and time-
aligned at the word level with the speech anal-
ysis tool Praat (Boersma, 2001).1 A sample of
the transcription process output is shown in Fig-
ure 4. Given our experimental context, off-the-shelf
automatic speech recognizers could not transcribe
the narratives to the desired quality and resources
were not available to create our own automatic tran-
1See http://www.fon.hum.uva.nl/praat/.
Figure 4: Transcripts were time-aligned in Praat which
was also used to extract acoustic-prosodic features.
scriber. Manual transcription also preserved disflu-
encies, which we believe convey meaningful infor-
mation. Disfluencies were transcribed to include
filled pauses (e.g. uh, um), false starts (e.g. pur-
reddish purple), repetitions, and click sounds.
This study is strengthened by its involvement of
medical experts. Trained dermatologists were re-
cruited in the original elicitation experiment as well
as the creation and application of both annotation
schemes. This is crucial in a knowledge-rich domain
such as medicine because the annotation scheme
must reflect the domain knowledge. Another study
reports on annotation details (McCoy et al., Forth-
coming 2012).
4 Classification Study
This section discusses the classification work, first
explaining the methodology for the initial pilot study
followed by interpretation of results. Next, the
methodology of the second pilot study is described.
4.1 Generic Model Overview
This work applies computational modeling de-
signed to predict diagnostic correctness in physi-
cians? narratives based on linguistic features from
the acoustic-prosodic and lexical-structural modali-
ties of language, shown in Table 2. Some tests dis-
cussed in 4.2 and 4.3 were performed with these
modalities separated. These features are inspired
by previous work conducted by Szarvas (2008),
Szarvas et al. (2008), Litman et al. (2009), Liscombe
et al. (2005), and Su et al. (2010).
We can formally express the created model in the
following way: Let ni be an instance in a set of nar-
ratives N , let j be a classification method, and let
li be a label in a set of class labels L. We want to
establish a function f(ni, j) : li where li is the label
assigned to the narrative based on linguistic features
from a set F , where F = f1, f2, ...fk, as described
in Table 2. The baseline for each classifier is de-
fined as the majority class ratio. Using scripts in
Praat (Boersma, 2001), Python, and NLTK (Bird et
al., 2009), we automatically extracted features for
each narrative. Each narrative was annotated with
multiple labels relating to its diagnostic correctness.
The labeling schemes used in the initial and second
pilot studies, respectively, are described in subsec-
22
tions 4.2 and 4.3.
4.2 Initial Pilot Study
The initial pilot classification study allowed the op-
portunity to refine the prediction target annotation
scheme, as well as to explore a preliminary set of lin-
guistic features. 160 narratives were assigned labels
Linguistic Feature at the narrative level
Modality
Acoustic- Total duration
prosodic Percent silence
Time silent
# of silences *
Time speaking
# of utterances *
Initial silence length
F0 mean (avg. pitch) ?
F0 min (min. pitch) ?
F0 max (max. pitch) ?
dB mean (avg. intensity) ?
dB max (max. intensity) ?
Lexical- # of words
structural words per minute
# of disfluencies ?
# of certainty amplifiers * ?
# of speculative words * ?
# of stop words * ?
# of content words * ?
# of negations * ?
# of nouns ?
# of verbs ?
# of adjectives ?
# of adverbs ?
Unigram of tokens
Bigram of tokens
Trigram of tokens
Table 2: Features used by their respective modalities.
Features marked with a * were only included in the sec-
ond pilot study. Features marked with ? were included
twice; once as their raw value and again as a z-score nor-
malized to its speaker?s data in the training set. Features
marked with ?were also included twice; once as their raw
count and again as their value divided by the total number
of words in that narrative. Disfluencies were considered
as words towards the total word count, silences were not.
No feature selection was applied.
of correct or incorrect for two steps of the diagnos-
tic process: diagnostic category and final diagno-
sis. These annotations were done by a dermatologist
who did not participate in the elicitation study (co-
author Cara Calvelli). For final diagnosis, 70% were
marked as correct, and for diagnostic category, 80%
were marked as correct. An outcome of the anno-
tation study was learning that the initial annotation
scheme needed to be refined. For example, diagnos-
tic category had a fuzzy interpretation, and correct-
ness and completeness of diagnoses are found along
a gradient in medicine. This led us to pursue an im-
proved annotation scheme with new class labels in
the second pilot study, as well as the adoption of a
gradient scale of correctness.
For the initial pilot study, basic features were ex-
tracted from the diagnostic narratives in two modal-
ities: acoustic-prosodic and lexical-structural (see
Table 2). To understand the fundamental aspects
of the problem, the initial pilot study experimented
with the linguistic modalities separately and to-
gether, using three foundational algorithms, as im-
plemented in NLTK (Naive Bayes, Maximum En-
tropy, Decision Tree), and a maximum vote classi-
fier based on majority consensus of the three basic
classifiers. The majority class baselines were 70%
for diagnosis and 80% for diagnostic category. The
small pilot data set was split into an 80% training set
and a 20% testing set. The following results were
obtained with the maximum vote classifier.
Utilizing only acoustic-prosodic features, the
maximum vote classifier performed 5% above the
baseline when testing final diagnosis and 6% below
it for diagnostic category. F0 min and initial silence
length appeared as important features. This initial si-
lence length could signal that the observers are able
to glean more information from the image, and us-
ing this information, they can make a more accurate
diagnosis.
Utilizing only lexical-structural features, the
model performed near the baseline (+1%) for final
diagnosis and 9% better than the baseline for diag-
nostic category. When combining acoustic-prosodic
and lexical-structural modalities, the majority vote
classifier performed above the baseline by 5% for fi-
nal diagnosis and 9% for diagnostic category. We
are cautious in our interpretation of these findings.
For example, the small size of the data set and the
23
particulars of the data split may have guided the re-
sults, and the concept of diagnostic category turned
out to be fuzzy and problematic. Nevertheless, the
study helped us refine our approach for the second
pilot study and redefine the annotation scheme.
4.3 Second Pilot Study
For the second pilot study, we hoped to gain further
insight into primarily two questions: (1) How accu-
rately do the tested models perform on three steps of
the diagnostic process, and what might influence the
performance? (2) In our study scenario, is a certain
linguistic modality more important for the classifi-
cation problem?
The annotation scheme was revised according to
findings from the initial pilot study. These revisions
were guided by dermatologist and co-author Cara
Calvelli. The initial pilot study scheme only anno-
tated for diagnostic category and final diagnosis. We
realized that diagnostic category was too slippery of
a concept, prone to misunderstanding, to be useful.
Instead, we replaced it with two new and more ex-
plicit parts of the diagnostic process: medical lesion
morphology and differential diagnosis.
For final diagnosis, the class label options of cor-
rect and incorrect could not characterize narratives
in which observers had not provided a final diag-
nosis. Therefore, a third class label of none was
added. New class labels were also created that cor-
responded to the diagnostic steps of medical lesion
morphology and differential diagnosis. Medical le-
sion morphology, which is often descriptively com-
plex, allowed the label options correct, incorrect,
and none, as well as correct but incomplete to deal
with correct but under-described medical morpholo-
gies. Differential diagnosis considered whether or
not the final diagnosis appeared in the differential
and thus involved the labels yes, no, and no differ-
ential given. Table 3 summarizes the refined anno-
tation scheme.
The examples in Figure 2 above can now be ana-
lyzed according to the new annotation scheme. Ob-
server A has a final diagnosis which should be la-
beled as correct but does not give a differential diag-
nosis, so the differential diagnosis label should be no
differential given. Observer A also misses parts of
the morphological description so the assigned med-
ical lesion morphology would likely be correct but
incomplete. Observer B provides what seems to be
a full morphological description as well as lists the
correct final diagnosis in the differential diagnosis,
yet is incorrect regarding final diagnosis. This narra-
tive?s labels for medical lesion morphology and dif-
ferential diagnosis would most likely be correct and
yes respectively. Further refinements may turn out
useful as the data set expands.
Diagnostic step Possible labels Count Ratio
Medical Correct 537 .83
Lesion Incorrect 36 .06
Morphology None Given 40 .06
Incomplete 32 .05
Differential Yes 167 .24
Diagnosis No 101 .14
No Differential 434 .62
Final Correct 428 .62
Diagnosis Incorrect 229 .33
None Given 35 .05
Table 3: Labels for various steps of the diagnostic process
as well as their count and ratios of the total narratives, af-
ter eliminating those with no annotator agreement. These
labels are explained in section 4.3.
Three dermatologists annotated the narratives, as-
signing a label of correctness for each step in the
diagnostic process for a given narrative. Table 3
shows the ratios of labels in the collected annota-
tions. Medical lesion morphology is largely correct
with only smaller ratios being assigned to other cat-
egories. Secondly, a large ratio of narratives were
assigned no differential given but of those that did
provide a differential diagnosis, the correct final di-
agnosis was more likely to be included than not. Re-
garding final diagnosis, a label of correct was most
often assigned and few narratives did not provide
any final diagnosis. These class imbalances, exist-
ing at each level, indicated that the smaller classes
with fewer instances would be quite challenging for
a computational classifier to learn.
Any narrative for which there was not agreement
for at least 2 of the 3 dermatologists in a diagnostic
step was discarded from the set of narratives consid-
ered in that diagnostic step.2
2Because narratives with disagreement were removed, the
total numbers of narratives in the experiment sets differ slightly
on the various step of the diagnostic process.
24
Comparing classification in terms of algorithms,
diagnostic steps, and individual classes
Weka (Witten and Frank, 2005)3 was used with
four classification algorithms, which have a widely
accepted use in computational linguistics.4
Standard performance measures were used to
evaluate the classifiers. Both acoustic-prosodic and
lexical-structural features were used in a leave-one-
out cross-validation scenario, given the small size of
the data set. The results are shown in Table 4. Ac-
curacy is considered in relation to the majority class
baseline in each case. With this in mind, the high
accuracies found when testing medical lesion mor-
phology are caused by a large class imbalance. Dif-
ferential diagnosis? best result is 5% more accurate
than its baseline while final diagnosis and medical
lesion morphology are closer to their baselines.
Final Dx Diff. Dx M. L. M.
Baseline .62 .62 .83
C4.5 .57 .62 .77
SVM .63 .67 .83
Naive Bayes .55 .61 .51
Log Regression .53 .64 .66
Table 4: Accuracy ratios of four algorithms (implemented
in Weka) as well as diagnostic steps? majority class base-
lines. Experiments used algorithms? default parameters
for final diagnosis (3 labels), differential diagnosis (3 la-
bels), and medical lesion morphology (4 labels) using
leave-one-out cross-validation.
In all scenarios, the SVM algorithm reached or
exceeded the majority class baseline. For this rea-
son, other experiments used SVM. The results for
the SVM algorithm when considering precision and
recall for each class label, at each diagnostic step,
are shown in Table 5. Precision is calculated as the
number of true positives for a given class divided by
the number of narratives classified as the given class.
Recall is calculated as the number of true positives
for a given class divided by the number of narra-
tives belonging to the given class. As Table 5 shows,
and as expected, labels representing large propor-
tions were better identified than labels representing
3See http://www.cs.waikato.ac.nz/ml/weka/.
4In this initial experimentation, not all features used were
independent, although this is not ideal for some algorithms.
Dx step Labels Precision Recall
Medical Correct .83 .99
Lesion Incorrect 0 0
Morphology None Given 0 0
Incomplete 0 0
Differential Yes .49 .44
Diagnosis No .26 .10
No Diff. .76 .89
Final Correct .67 .84
Diagnosis Incorrect .32 .47
None Given 0 0
Table 5: Precision and recall of class labels. These were
obtained using the Weka SVM algorithm with default pa-
rameters using leave-one-out cross-validation. These cor-
respond to the experiment for SVM in Table 4.
Final Diagnosis Diff. Diagnosis
Baseline .62 .62
Lex.-struct. .62 .67
Acous.-pros. .65 .62
All .63 .67
Table 6: Accuracy ratios for various modalities. Tests
were performed for final diagnosis and differential diag-
nosis tags with Weka?s SVM algorithm using a leave-
out-out cross-validation method. Lexical-structural and
acoustic-prosodic cases used only features in their respec-
tive set.
intermediate proportions, and classes with few in-
stances did poorly.
Experimentation with types of feature
To test if one linguistic modality was more impor-
tant for classification, experiments were run in each
of three different ways: with only lexical-structural
features, with only acoustic-prosodic features, and
with all features. We considered the final diagnosis
and differential diagnosis scenarios. It was decided
not to run this experiment in terms of medical lesion
morphology because of its extreme class imbalance
with a high baseline of 83%. Medical lesion mor-
phology also differs in being a descriptive step un-
like the other two which are more like conclusions.
Again, a leave-one-out cross-validation method was
used. The results are shown in Table 6.
These results show that, regarding final diagnosis,
considering only acoustic-prosodic features seemed
25
to yield somewhat higher accuracy than when fea-
tures were combined. This might reflect that, con-
ceptually, final diagnosis captures a global end step
in the decision-making process, and we extracted
voice features at a global level (across the narrative).
In the case of differential diagnosis, the lexical-
structural features performed best, matching the ac-
curacy of the combined feature set (5% over the ma-
jority class baseline). Future study could determine
which individual features in these sets were most im-
portant.
Experiments with alternative label groupings for
some diagnostic steps
Another set of experiments examined perfor-
mance for adjusted label combinations. To learn
more about the model, experiments were run in
which selected classes were combined or only cer-
tain classes were considered. The class proportions
thus changed due to the combinations and/or re-
moval of classes. This was done utilizing all fea-
tures, the Weka SVM algorithm, and a leave-one-
out methodology. Only logically relevant tests that
increased class balance are reported here.5
An experiment was run on the differential diagno-
sis step. The no differential given label was ignored
to allow the binary classification of narratives that
included differential diagnoses. The new majority
class baseline for this test was 62% and this classi-
fication performed 1% over its baseline. A similar
experiment was run on the final diagnosis diagnos-
tic step. Class labels of incorrect and none given
were combined to form binary set of class labels
with a 62% baseline. This classification performed
6% over the baseline, i.e., slightly improved perfor-
mance compared to the scenario with three class la-
bels.
5 Conclusion
In these pilot studies, initial insight has been gained
regarding the computational linguistic modeling of
extra-propositional meaning but we acknowledge
that these results need to be confirmed with new
data.
This paper extracted features, which could pos-
sibly relate to uncertainty, at the global level of a
5Other experiments were run but are not reported because
they have no use in future implementations.
narrative to classify correctness of three diagnostic
reasoning steps. These steps are in essence local
phenomena and a better understanding of how un-
certainty is locally expressed in the diagnostic pro-
cess is needed. Also, this work does not consider
parametrization of algorithms or the role of feature
selection. In future work, by considering only the
features that are most important, a better understand-
ing of linguistic expression in relation to diagnostic
correctness could be achieved, and likely result in
better performing models. One possible future adap-
tation would be the utilization of the Unified Medi-
cal Language System to improve the lexical features
used Woods et al. (2006).
Other future work includes integrating eye move-
ment data into prediction models. The gaze modal-
ity informs us as to where the observers were look-
ing when they were verbalizing their diagnostic pro-
cess. We can thus map the narratives to how gaze
was positioned on an image. Behavioral indicators
of doctors? diagnostic reasoning likely extend be-
yond language. By integrating gaze and linguistic
information, much could be learned regarding per-
ceptual and conceptual knowledge.
Through this study, we have moved towards un-
derstanding reasoning in medical narratives, and we
have come one step closer to linking the spoken
words of doctors to their cognitive processes. In a
much more refined, future form, certainty or cor-
rectness detection could become useful to help un-
derstanding medical reasoning or help guide medi-
cal reasoning or detect misdiagnosis.
Acknowledgements
This research supported by NIH 1 R21 LM010039-
01A1, NSF IIS-0941452, RIT GCCIS Seed Fund-
ing, and RIT Research Computing (http://rc.rit.edu).
We would like to thank Lowell A. Goldsmith, M.D.
and the anonymous reviewers for their comments.
References
