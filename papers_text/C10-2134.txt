Coling 2010: Poster Volume, pages 1167?1175,
Beijing, August 2010
Bridging Topic Modeling and Personalized Search
Wei Song Yu Zhang Ting Liu Sheng Li
School of Computer Science
Harbin Institute of Technology
{wsong, yzhang, tliu, lisheng}@ir.hit.edu.cn
Abstract
This work presents a study to bridge topic
modeling and personalized search. A
probabilistic topic model is used to extract
topics from user search history. These
topics can be seen as a roughly summary
of user preferences and further treated as
feedback within the KL-Divergence re-
trieval model to estimate a more accurate
query model. The topics more relevant
to current query contribute more in updat-
ing the query model which helps to dis-
tinguish between relevant and irrelevant
parts and filter out noise in user search
history. We designed task oriented user
study and the results show that: (1) The
extracted topics can be used to cluster
queries according to topics. (2) The pro-
posed approach improves ranking qual-
ity consistently for queries matching user
past interests and is robust for queries not
matching past interests.
1 Introduction
The majority of queries submitted to search en-
gines are short and ambiguous and the users of
search engines often have different search intents
even when they submit the same query (Janse and
Saracevic, 2000)(Silverstein and Moricz, 1999).
The ?one size fits all? approach fails to optimize
each individual?s specific information need. Per-
sonalized search has be viewed as a promising
direction to solve the ?data overload? problem,
and aims to provide different search results ac-
cording to the specific preference of an individ-
ual(Pitkow and Breuel, 2002). Information re-
trieval (IR) communities have developed models
for context sensitive search and related applica-
tions (Shen and Zhai, 2005a)(White and Chen,
2009).
The search context includes a broad range of in-
formation types such as a user?s background, his
personal desktop index, browser history and even
the context information of a group of similar users
(Teevan, 2009). In this paper, we exploit the user
search history of an individual which contains the
past submitted queries, results returned and the
click through information. As described in (Tan
and Zhai, 2006), search history is one of the most
important forms of search context. When dealing
with search history, distinguishing between rele-
vant and irrelevant parts is important. The search
history may contain a lot of noisy information
which can harm the performance of personaliza-
tion (Dou and Wen, 2007). Hence, we need to
sort out relevant and irrelevant parts to optimize
search personalization.
In this paper, we propose a topic model based
approach to study users? preferences. The main
contribution of this work is modeling user search
history with topics for personalized search. Our
approach mainly consists of two steps: topic ex-
traction and relevance feedback. We assume that
a user?s search history is governed by the underly-
ing hidden properties and apply probabilistic La-
tent Semantic Indexing (pLSI) (Hofmann, 1999)
to extract topics from user search history. Each
topic indexes a unigram language model. We
model these extracted topics as feedback in the
KL-Divergence retrieval framework. The task is
to estimate a more accurate query model based
on the evidence from user feedback. We distin-
1167
guish relevant parts from irrelevant parts in search
history by focusing on the relevance between top-
ics and query. The closer a topic is to the cur-
rent query, the more it contributes in updating the
query model, which in turn is used to rerank the
documents in results set.
2 Related Work
2.1 Personalized IR
Personalized search is an active ongoing research
direction. Based on different representations of
user profile, we classify approaches as follows:
Taxonomy based methods: this approach
maps user interests to an existing taxonomy.
ODP1 is widely used for this purpose. For
example, by exploiting the user search history,
(Speretta and Gauch, 2005) modeled user interest
as a weighted concept hierarchy created from the
top 3 level of ODP. (Havelivala, 2002) proposed
the ?topic sensitive pagerank? algorithm by cal-
culating a set of PageRanks for each web page on
the top 16 ODP categories. (Qiu and Cho, 2006)
further improved this approach by building user
models from user click history. In recent stud-
ies, (Xu S. and Yu, 2008) used ODP categories
for exploring folksonomy for personalized search.
(Dou and Wen, 2007) proposed a method that rep-
resent user profile as a weighting vector of 67 pre-
defined topic categories provided by KDD Cup-
2005. Taxonomy based methods rely on a pre-
defined taxonomy and may suffer from the granu-
larity problem.
Content based methods: this category of
methods use traditional text presentation model
such as vector space model and language model
to express user preference. Rich content infor-
mation such as user search history, browser his-
tory and indexes of desktop documents are ex-
plored. The user profiles are built in the forms of
term vectors or term probability distributions. For
example, (Sugiyama and M., 2004) represented
user profiles as vectors of distinct terms and ac-
cumulated past preferences. (Teevan and Horvitz,
2005) constructed a rich user model based on both
search-related information, such as previously is-
sued queries, and other information such as doc-
1Open Directory Project, http://dmoz.org/
uments and emails a user had read and created.
(Shen and Zhai, 2005b) used browsing histories
and query sessions to construct short term indi-
vidual models for personalized search.
Learning to rank methods: (Eugene and Su-
san, 2005) and (Eugene and Zheng, 2006) incor-
porated user feedback into the ranking process in a
learning to rank framework. They leveraged mil-
lions of past user interaction with web search en-
gine to construct implicit feedback features. How-
ever, this approach aims to satisfy majority of
users rather than individuals.
2.2 Probabilistic Topic Models
Probabilistic topic models have become popular
tools for unsupervised analysis of document col-
lection. Topic models are based upon the idea
that documents are mixtures of topics, where
a topic is a probability distribution over words
(Steyvers and Griffiths, 2007). These topics are
interpretable to a certain degree. In fact, one of
the most important applications of topic models
is to find out semantic lexicons from a corpus.
One of the most popular topic models, the prob-
abilistic Latent Semantic Indexing Model (pLSI),
was introduced by Hofmann (Hofmann, 1999)
and quickly gained acceptance in a number of text
modeling applications. In this study, pLSI is used
to discover the underlying topics in user search
history. Though pLSI is argued that it is not a
complete generative model, we used it because it
does not need to generate unseen documents in
our case and the model is much easier to be es-
timated compared with sophisticated models such
as LDA(David M. Blei and Jordan, 2003).
2.3 Model based Relevance Feedback
Our work is also related to language model based
(pseudo) relevance feedback (Zhai and Lafferty,
2001b) and shares the similar idea with (Tan B.
and Zhai, 2007). The differences are: (1) The
feedback source is user search history rather than
top ranked documents for a query. (2) We make
use of user implicit feedback rather than explicit
feedback. (3) The topics in search history could
be extracted offline and updated periodically. Ad-
ditionally, these topics provide an informative pic-
ture of user search history.
1168
Table 1: An illustration of topics extracted from a
user?s search history. Terms with highest proba-
bilities are listed below each topic.
Topic 2 Topic 3 Topic 9 Topic 16
climb movie swim cup
0.032 0.091 0.044 0.027
setup download ticket world
0.022 0.078 0.032 0.022
equipment dvd notice team
0.020 0.061 0.019 0.016
practice watch travel brazil
0.009 0.060 0.016 0.011
player cinema hotel storm
0.006 0.038 0.008 0.007
3 Proposed Approach
3.1 Main Idea
A user?s search history usually covers multiple
topics. It is crucial to distinguish between rele-
vant and irrelevant parts for optimizing personal-
ization. We propose a topic model based method
to achieve that goal. First, we construct a doc-
ument collection revealing user intents according
to the user?s past activities. A probabilistic topic
model is applied on this collection to extract la-
tent topics. Then the extracted topics are used as
feedback. The query model is updated by high-
lighting the topics highly relevant to current query.
Finally, the search results are reranked according
to the relevance to the updated query model. Ta-
ble 1 shows 4 topics extracted from a user?s search
history. Each topic is a unigram language model.
The terms with higher probabilities belonging to
each topic are listed. We can predict that the user
has interests in both movie and football. However,
when the user submits a query about world cup,
the topic 16 is given higher preference for esti-
mating a more accurate query model.
3.2 Topic Extraction from Search History
Individual?s search history consists of all the past
query units. Each query unit includes query text,
returned search results (with title, snippets and
URLs) and click through information. Here, we
concatenated the title and snippet of each search
result to form a document being considered as a
whole. The whole search history can be seen as a
collection of documents. Obviously, many doc-
uments in the collection may fail to satisfy the
user?s information need and are uncertain for dis-
covering the user?s preferences. Therefore, the
first task is to select proper documents in search
history as the preference collection for topic dis-
covery.
3.2.1 Preference Collection
An intuitive solution is to use the documents
that are clicked by the user. The assumption is
that a user clicks on a result only if he is interested
in the document. However, user click is sparse in
real search environments and the documents not
clicked by the user may also be relevant to the
user?s information need. We assumed that the user
had only one search intent for a submitted query.
To enhance this coherence within a query unit, we
created only one super-document for a query unit
as follows: if a query unit had clicked documents,
then we concatenated these document to form a
preferred document. Otherwise, we selected the
top n documents from the search results and con-
catenated them as a preferred document. That is
motivated by the idea of pseudo relevance feed-
back (Lavrenko and Croft, 2001) and used here for
alleviating data sparsity. Pseudo relevance feed-
back is sensitive to the number of feedback docu-
ments. In this work, n is set to 3, because the aver-
age clicks for a query is not more than 3. By this
way, we got a preference collection whose size is
the same as the number of past queries.
3.2.2 Topic Extraction
Given the collection of preferred documents,
we applied pLSI on this collection to extract
underlying topics. We define the collection as
C={d1,d2,. . . ,dM}, where di corresponds to the
ith query unit, and M is the size of the collection.
Each query unit is viewed as a mixture of differ-
ent topics. It is reasonable in reality. For exam-
ple, a news document about ?play basketball with
obama? might be seen as a mixture of topics ?pol-
itics? and ?sports?.
Modeling: The basic idea of pLSI is to treat
the words in each document as being generated
from a mixture model where the component mod-
els are topic word distributions. Let k be the num-
1169
ber of topics which is assumed known and fixed.
?j is the word distribution for topic j. We extract
topics from collection C using a simple proba-
bilistic mixture model as described in (Zhai and
Yu, 2004). A word w within document d can be
viewed as generated from a mixture model:
pd(w) = ?Bp(w|?B)
+(1 ? ?B)
k?
j=1
pid,jp(w|?j)
(1)
where ?B is the background model for all the doc-
uments. The background model is used to draw
common words across all the documents and lead
to more discriminative and informative topic mod-
els, since ?B gives high weights to non-topical
words. ?B is the probability that a term is gen-
erated from the background model which is set to
be a constant. To draw more discriminative topic
models, we set ?B to 0.95. Parameter pid,j indi-
cates the probability that topic j is assigned to the
specific document d, where?kj=1 pid,j=1.Parameter estimation: The parameters we
have to estimate including the background model
?B , {?j} and {pid,j}. ?B is maximum likelihood
estimated (MLE) using all available text in our
data set so that it is a fixed distribution. The other
parameters to be estimated are {?j} and {pid,j}.
The log-likelihood of document d is:
log p(d) =
?
w?V
c(w, d) log[?Bp(w|?B)
+(1 ? ?B)
k?
j=1
pid,jp(w|?j)]
(2)
The log-likelihood of the whole collection C is:
log(C) =
?
d?C
?
w?V
c(w, d) log[?Bp(w|?B)
+(1 ? ?B)
k?
j=1
pid,jp(w|?j)]
(3)
The Expectation-Maximization (EM) algorithm
(Dempster and Rubin, 1977) is used to find a
group of parameters maximizing equation (3).
The updating formulas are:
,
,
1
( ) ( )
,
,
( ) ( )
, ' '
' 1
, ,( 1)
,
, ,
' 1
(
E-Step:
( | )
( )
( | ) (1 ) ( | )
( | )
( )
( | )
M-Step:
( , )(1 ( )) ( )
( , )(1 ( )) ( ')
B B
d w k
B B B d j j
j
m m
d j j
d w k
m m
d j j
j
d w d wm w V
d j k
d w d ww V
j
p w
p z B
p w p w
p w
p z j
p w
c w d p z B p z j
c w d p z B p z j
p
? ?
? ? ? ? ?
? ?
? ?
?
=
=
+ ?
?=
= =
+ ?
= =
? = ==
? = =
?
?
?
??
, ,
1)
, ' , '
'
( , )(1 ( )) ( )
( | )
( ', )(1 ( )) ( )
d w d w
m d C
j
d w d w
d C w V
c w d p z B p z j
w
c w d p z B p z j
?+ ?
? ?
? = =
= ? = =
?
? ?
 
where c(w, d) denotes the number of times w
occurs in d. A hidden variable zd,w is introduced
for the identity of each word. p(zd,w = B) is
the probability that the word w in document d is
generated by the background model. p(zd,w = j)
denotes the probability that the word w in docu-
ment d is generated using topic j given that w is
not generated from the background model. Infor-
mally, the EM algorithm starts with randomly as-
signing values to the parameters to be estimated
and then alternates between E-Step and M-Step
iteratively until it yields a local maximum of the
log likelihood.
Interpretation: As shown in equation (1), a
word can be viewed as a mixture of topics. From
the updating formulas, we can see that the domi-
nant topic of a word depends on both itself and the
context. The word tends to have the same topic
with the document containing it. While the prob-
ability of assigning topic j to document d is es-
timated by aggregating all the fractions of words
generated by topic j in document d. We can ex-
plain it in a more intuitive way with in our applica-
tion. As we know, the queries are usually ambigu-
ous. A classic example is ?apple? which may re-
fer to a kind of fruit, apple Inc, apple electric prod-
ucts, etc. Therefore, it is reasonable to assume
that each word belongs to multiple latent seman-
tic properties. If a returned result contains ?ap-
ple? and other words like ?computer?, ?ipod? ,
etc. The word ?apple? in this result tends to have
the same topic distributions with ?computer? and
1170
?ipod?. If the user clicks the result, we can predict
that the user?s real preference about query ?ap-
ple? is related to electric products having a high
probability. Further, if ?apple? occurs frequently
in many documents related to electric products,
it obtains a higher probability in this topic. As
a result, we not only know user?s interest in elec-
tric products, but also find a preference to ?apple?
brand.
Since a document?s topic depends on the words
it contains, two documents with similar word dis-
tributions have similar topic distributions. In other
words, each topic is like a bridge connecting
queries with similar intents. In summary, the topic
extraction process plays a role in our application
for finding user preference, highlighting discrimi-
native words and connecting queries with similar
intents.
3.3 Topics as Feedback
The topics extracted from search history are con-
sidered as a kind of feedback. Since topic mod-
els actually are extensions of language models,
we use such feedback within the KL-Divergence
retrieval model (Xu and Croft, 1999)(Zhai and
Lafferty, 2001b) that is a principled framework
to model feedback in the language modeling ap-
proach. In this framework, feedback is treated as
updating the query language model based on extra
evidence obtained from the feedback sources. The
information retrieval task is to rank documents ac-
cording to the KL divergence D(?q||?d) between
a query language model ?q and a document lan-
guage model ?d. The KL divergence is defined as:
D(?q||?d) =
?
w?V
p(w|?q) log
p(w|?q)
p(w|?d)
(4)
where V denotes the vocabulary. We estimate
the document model ?d using Dirichlet estimation
(Zhai and Lafferty, 2001a):
p(w|?d) =
c(w, d) + ?p(w|?C)
|d| + ? (5)
where |d| is document length, p(w|?C) is collec-
tion language model which is estimated using the
whole data collection. ? is the Dirichlet prior that
is set to 20 in this work. The updated query model
is defined as:
p(w|?q) = ?pml(w|?q)
+(1 ? ?)
k?
j=1
p(w|?j)p(z = j|q)
(6)
where pml(w|?q) is the MLE query model. {?j}
represents a set of extracted topics each of which
is a unigram language model. ? is used to bal-
ance the two components. z is a hidden variable
over topics. The task is to estimate the multino-
mial topic distribution p(z|q) for query q. Since
pLSI does not properly provide a prior, we esti-
mate p(z = j|q) as:
p(z = j|q) = p(q, z = j)?k
j?=1 p(q, z = j?)
? sim(?q, ?j)?k
j?=1 sim(?q, ?j?)
(7)
Since the query text is usually very short, it is
not easy to make a decision based on query text
alone. Instead, we concatenate all the available
documents in returned result set to form a super-
document. A language model is estimated for it.
We convert both the document language model
and topic models into weighted term vectors and
use cosine similarity as the sim function. p(z|q)
plays an import role here as it determines the con-
tribution of topics. The topics with higher similar-
ity with current query contributes more in updat-
ing query model. This scheme helps to filter out
noisy information in search history.
4 Evaluation and Discussion
4.1 Data Collection
To the best of our knowledge, there is no public
collection with enough content information and
user implicit feedback. We decided to carry out
a data collection. Due to the difficulty to de-
scribe and evaluate user interests implicitly, we
predefined some user interests and implemented
a search system to collect user interactions.
The predefined interests belong to 5 big cate-
gories namely Entertainment, Computer & Inter-
net, Sports, Health and Social life. Each inter-
est is a kind of user preference such as ?movies?
1171
Table 2: An example of predefined user interests
and tasks
category Enterntainment
interest movies
task1 search for a brief introductionof your favorite movie
task2 search for an introduction ofan actor or actress you like
task3 search for movies about?artificial intelligence?
Table 3: Statistics of the data collection
user 1 2 3 4 5
#queries 218 256 177 206 311
#big category 5 5 5 5 5
#interest 25 25 25 25 25
#tasks 100 100 100 100 100
avg.#relevant 4.17 4.22 3.89 4.12 3.24results
avg.#clicked 2.37 2.21 2.71 1.98 2.42results
and ?outdoor sports?. For each interest, we de-
signed several tasks each of which had a goal. Ta-
ble 2 illustrates an example of a predefined user
interest and related tasks. The volunteers were
asked to find out the information need according
to the tasks. Though we defined these interests
and tasks, we did not impose any constraint on
the queries. The volunteers could choose and re-
formulate any query they thought good for find-
ing the desired information. But we did try to in-
crease the possibility that a user might issue am-
biguous queries by designing tasks like ?search
for movies about artificial intelligence? which was
categorized to interest ?movies?, but also related
to computer science.
To collect the user interaction with search en-
gine, we implemented a Lucene based search sys-
tem on Tianwang terabyte corpus(Yan and Peng,
2005). Five volunteers were asked to submit
queries to this system to find information satisfy-
ing the tasks of each interest. The system recorded
users? activities including submitted queries, re-
turned search results (with title, snippet and URL)
and users? click through information. When the
user finished a task, he clicked a button to tell the
system termination of the session containing all
the queries and activities related to this task. After
finishing all the tasks, the volunteers were asked to
judge the top 20 results? relevance (relevant or not
relevant) for each query according to the search
target. Each volunteer submitted 233 queries on
average. Table 3 presents some statistics of this
collection.
4.2 Evaluating Topic Extraction
It is not easy to assess the quality of topics, be-
cause topic extraction is an unsupervised process
and difficult to give a standard answer. Therefore,
we view the topic extraction as a clustering prob-
lem that is to organize queries into clusters. To
group queries into clusters through extracted top-
ics, we use j? = argmax
j
pid,j to assign a query to
the j?th topic. Each topic corresponds to a cluster.
All the queries are divided into k clusters. Based
on the data collection, we setup the golden an-
swers according to the predefined interests. We
view all the queries belonging to a predefined in-
terest(which includes multiple tasks) form a clus-
ter which helps us to build a golden answer with
25 clusters in tatal.
One purpose of making use of topics in search
history is to find more relevant parts and reduce
the noise. We hope that the extracted topics are
coherent. That is, a cluster should contain as many
queries as possible belonging to a single inter-
est. To evaluate coherence, we adopt purity (Zhao
and Karypis, 2001), a commonly used metric for
evaluating clustering. The higher the purity is,
the better the system performs. We compare our
method (denoted as PLSI) against the k-means al-
gorithm(denoted as K-Means) on the preference
collection.
Figure 1 shows the overall purity with differ-
ent number of topics. Our method gained better
performance than k-means algorithm consistently.
It is effective to discover and organize user inter-
ests. Besides, as illustrated in Table 1, our method
is able to give higher probability to discriminative
words of each topic that provides a clear picture
of user search history. This leads to an emergence
of novel approaches for personalized browsing.
1172
0.2
0.3
0.4
0.5
0.6
0.7
0.8
10 20 30 40 50 60 70 80 90 100
Number of topics
Pu
ri
ty
PLSI K-Means
 
Figure 1: Average purity over 5 users gained by
both PLSI and K-Means with different number of
topics(clusters).
4.3 Evaluating Result Reranking
4.3.1 Metric
To quantify the ranking quality, the Dis-
counted Cumulative Gain (DCG) (Jarvelin and
Kekakainen, 2000) is used. DCG is a metric that
gives higher weights to highly ranked documents
and incorporates different relevance levels by giv-
ing them different gain values.
DCG(i) =
{
G(1), if i = 1
DCG(i? 1) + G(i)log(i) , otherwise
In our work, we use G(i) = 1 for the results la-
beled as relevant by a user and G(i) = 0 for the
results that are not relevant. The average normal-
ized DCG (NDCG)over all the test queries is se-
lected to show the performance.
4.3.2 Systems
We evaluated the performance of following sys-
tems:
PLSI: The proposed method. The history model
was a weighted interpolation over topics extracted
from the preference collection described in ses-
sion 3.2.1.
PSEUDO: From each query unit, we selected
top n documents as pseudo feedback. The lan-
guage history model was estimated on all these
documents.
PLSI-PSEUDO: Top n documents from each
query unit were concatenated to form a preferred
document. The history model was constructed
based on topics extracted from these preferred
documents.
HISTORY: The history language model was es-
timated based on all the documents in search his-
tory.
TB: It was based on(Tan and Zhai, 2006)which
built a unit language model for every past query
and the history model was a weighted interpola-
tion of past unit language models.
ORIGINAL: The default search system.
The first 5 systems provided schemes to smooth
the query model. They estimated the query mod-
els by utilizing different types of feedback (im-
plicit feedback or pseudo feedback) and weight-
ing methods (topic modeling or simple language
modeling). The updated query model was an in-
terpolation between MLE query model and his-
tory language model. The interpolation parameter
was set to 0.5, and n was set to 3.
4.3.3 Performance Comparison
To evaluate the performance on a test query, we
focus on two conditions:
1. the test query matches some past interests.
We want to check the ability of systems to
find relevant information from noisy data.
2. the test query does not match any of past in-
terests. We are interested in the robustness of
the systems.
For the first case, the users were asked to se-
lect at most 2 queries they submitted for each
task. These queries were used as test queries.
The other queries were used to simulate the users?
search history. In total we got 400 queries for
testing. Figure 2 demonstrates the performance
of these systems over all test queries. PLSI
outperformed all other systems consistently that
shows topic model based methods help to esti-
mate a more accurate query model and the user
implicit feedback is better evidence. The PLSI-
PSEUDO also performed well that indicates the
top documents is useful for revealing the topic
of queries, even though they do not satisfy user
need on occasion. TB also gained better perfor-
mance than PSEUDO and HISTORY. It indicates
1173
0.49
0.5
0.51
0.52
0.53
0.54
0.55
0.56
10 20 30 40 50 60 70 80 90 100
Number of topics
ND
CG
PLSI PLSI-PSEUDO PSEUDO
HISTORY TB ORIGINAL
0.51
0.514
0.518
0.522
0.526
0.53
10 20 30 40 50 60 70 80 90 100
Number of topics
ND
CG
PLSI PLSI-PSEUDO PSEUDO
HISTORY TB ORIGINAL
 
Figure 2: The overall average performance of sys-
tems, when each test query matches some user
past interests
highlighting relevant parts in search history helps
to improve the retrieval performance, when the
query matches some of user past interests. Com-
pared with default system, both HISTORY and
PSEUDO improved a lot which proves that the
context in search history is reliable feedback.
For the second case, each user was asked to
hold out 5 interests from his collection for test-
ing and the other interests were used as search
history. The users selected queries from the held
out interests as test queries. These queries did
not match each user?s past interests. We got 244
test queries. As figure 3 shows, though systems
still performed better against ORIGINAL, the im-
provements were not significant. PLSI still gained
the best performance. It has better ability to al-
leviate the effect of noise. HISTORY and PLSI
are more robust than PLSI-PSEUDOwhich seems
sensitive to the number of topics in this case.
In both cases, HISTORY gained moderate per-
formance but quite robust. It is still a very strong
baseline, though noisy information is not filtered
out. PLSI performed best in both cases. PLSI-
PSEUDO outperformed PSEUDO when the test
queries matched user past interests and gained
comparable results in second case. It shows that
modeling user search history as a mixture of top-
ics and weighting topics according to relevance
between topics and query help to update a better
query model. However, it is necessary to deter-
mine if a query matches past interests that helps
to optimize personalized search strategies.
0.49
0.5
0.51
0.52
0.53
0.54
0.55
0.56
10 20 30 40 50 60 70 80 90 100
Number of topics
ND
CG
PLSI PLSI-PSEUDO PSEUDO
HISTORY TB ORIGINAL
0.51
0.514
0.518
0.522
0.526
0.53
10 20 30 40 50 60 70 80 90 100
Number of topics
ND
CG
PLSI PLSI-PSEUDO PSEUDO
HISTORY TB ORIGINAL
 
Figure 3: The overall average performance of sys-
tems, when each test query does not match any
user past interest.
5 Conclusion and Future Work
In this paper, we have proposed a topic model
based method for personalized search. This ap-
proach has some advantages: first, it provides a
principled way to combine topic modeling and
personalized search; second, it is able to find user
preferences in an unsupervised way and gives an
informative summary of user search history; third,
it explores the underlying relationship between
different query units via topics that helps to filter
out the noise and improve ranking quality.
In future, we plan to do a large scale study by
leveraging the already built search system or busi-
ness search engines. Also, we will try to add more
information to extend the existing model. Besides,
it is necessary to design methods for determin-
ing whether a submitted query matches the user
past interests that is crucial to apply our algorithm
adaptively and selectively.
Acknowledgements
This research is supported by the National Nat-
ural Science Foundation of China under Grant
No. 60736044, by the National High Technol-
ogy Research and Development Program of China
No. 2008AA01Z144, by Key Laboratory Opening
Funding of MOE-Microsoft Key Laboratory of
Natural Language Processing and Speech, Harbin
Institute of Technology, HIT.KLOF.2009020. We
thank the anonymous reviewers and Fikadu
Gemechu for their useful comments and help.
1174
References
David M. Blei, Andrew Y. Ng and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Dempster, A.P., Laird N.M. and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. Journal of Royal Statist. Soc. B,
39:1?38.
Dou, Z., Su R. and J. Wen. 2007. A large-scale evalu-
ation and analysis of personalized search strategies.
Proc. WWW, pages 581?590.
Eugene, A., Eric B. and D. Susan. 2005. Improving
web search ranking by incorporating user behavior
information. Proc.SIGIR, pages 19?26.
Eugene, A. and Zijian Zheng. 2006. Identifying best
bet web search results by mining past user behavior.
Proc.SIGKDD, pages 902?908.
Havelivala, T.H. 2002. Topic-sensitive pagerank.
Proc. WWW, pages 517?526.
Hofmann, T. 1999. Probabilistic latent semantic in-
dexing. Proc.SIGIR, pages 50?57.
Janse, B.J., Spink A. Bateman J. and T. Saracevic.
2000. Real life, real users, and real needs: a study
and analysis of user queries on the web. Information
Processing and Management, 26(2):207?222.
Jarvelin, K. and J. Kekakainen. 2000. Ir evaluation
methods for retrieving highly relevant documents.
Proc.SIGIR, pages 41?48.
Lavrenko, V. and W. Croft. 2001. Relevance based
language models. Proc.SIGIR, pages 120?127.
Pitkow, J., Schutze H. Cass T. Cooley R. Turnbull D.
Edmonds A. Adar E. and T. Breuel. 2002. Person-
alized search. Commun,ACM, 45(9):50?55.
Qiu, F. and J. Cho. 2006. Automatic identification of
user interest for personalized search. Proc.WWW,
pages 727?736.
Shen, X., Tan B. and C. Zhai. 2005a. Context-
sensitive information retrieval using implicit feed-
back. Proc. SIGIR, pages 43?50.
Shen, X., Tan B. and C. Zhai. 2005b. Implicit user
modeling for personalized search. Proc. CIKM,
pages 824?831.
Silverstein, C., Marais H. Henzinger M. and
M. Moricz. 1999. Analysis of a very large web
search engine query log. SIGIR Forum, 33(1):6?12.
Speretta, M. and S. Gauch. 2005. Personalized search
based on user search histories. Proc. WI?05, pages
622?628.
Steyvers, M. and T. Griffiths. 2007. Probabilistic topic
models. Handbook of Latent Semantic Analysis.
Erlbaum, Hillsdale, NJ.
Sugiyama, K., Hatano K. and Yoshkawa. M. 2004.
Personalized search based on user search histories.
Proc. WWW, pages 675?684.
Tan, B., Shen X. and C. Zhai. 2006. Mining long-
term search history to improve search accuracy.
Proc.SIGKDD, pages 718?723.
Tan B., Atulya Velivelli, Fang H. and C. Zhai. 2007.
Term feedback for information retrieval with lan-
guage models. Proc.SIGIR, pages 263?270.
Teevan, J., Dumais S.T. and E. Horvitz. 2005. Per-
sonalizing search via automated analysis of interests
and activities. Proc.SIGKDD, pages 449?456.
Teevan, J., Morris M.R. Bush S. 2009. Discover-
ing and using groups to improve personalization.
Proc.WSDM, pages 15?24.
White, R.W., Bailey P. and L. Chen. 2009. Pre-
dicting user interest from contextual information.
Proc.SIGIR, pages 363?370.
Xu, Jinxi and W. Croft. 1999. Cluster-based language
models for distributed retrieval. Proc.SIGIR, pages
254?261.
Xu S., Bao, S. Fei B. Su Z. and Y. Yu. 2008. Exploring
folksonomy for personalized search. Proc.SIGIR,
pages 155?162.
Yan, H., Li J. Zhu j. and B. Peng. 2005. Tian-
wang search engine at trec 2005: Terabyte track.
Proc.TREC.
Zhai, C. and J. Lafferty. 2001a. A study of smooth-
ing methods for language models applied to ad hoc
information retrieval. Proc.SIGIR, pages 334?342.
Zhai, Chengxiang and John Lafferty. 2001b. Model-
based feedback in the language modeling approach
to information retrieval. Proc.CIKM, pages 403?
410.
Zhai, C., Velivelli A. and B. Yu. 2004. A cross-
collection mixture model for comparative text min-
ing. Proc.SIGKDD, pages 743?748.
Zhao, Y. and G. Karypis. 2001. Criterion functions
for document clustering: Experiments and analysis.
Technical Report TR #01?40, Department of Com-
puter Science, University of Minnesota, Minneapo-
lis, MN.
1175
