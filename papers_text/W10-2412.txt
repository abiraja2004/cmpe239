Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 76?80,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Mining Transliterations from Wikipedia using Pair HMMs
Peter Nabende
Alfa-Informatica, University of Groningen
The Netherlands
p.nabende@rug.nl
Abstract
This paper describes the use of a pair
Hidden Markov Model (pair HMM) sys-
tem in mining transliteration pairs from
noisy Wikipedia data. A pair HMM vari-
ant that uses nine transition parameters,
and emission parameters associated with
single character mappings between source
and target language alphabets is identified
and used in estimating transliteration sim-
ilarity. The system resulted in a precision
of 78% and recall of 83% when evaluated
on a random selection of English-Russian
Wikipedia topics.
1 Introduction
The transliteration mining task as defined in the
NEWS 2010 White paper (Kumaran et al, 2010)
required identifying single word transliteration
pairs from a set of candidate transliteration pairs.
In the case of Wikipedia data, we have a collection
of corresponding source and target language topics
that can be used for extracting candidate translit-
erations. We apply a pair HMM edit-distance
based method to obtain transliteration similarity
estimates. The similarity estimates for a given set
of source and target language words are then com-
pared with the aim of identifying potential translit-
eration pairs. Generally, the pair HMM method
uses the notion of transforming a source string
to a target string through a series of edit opera-
tions. The three edit operations that we consider
for use in transliteration similarity estimation in-
clude: substitution, insertion, and deletion. These
edit operations are represented as hidden states of
a pair HMM. Depending on the source and target
language alphabets, it is possible to design or use a
specific pair HMM algorithm for estimating paired
character emission parameters in the edit opera-
tion states, and transition parameters for a given
design of transitions between the pair HMM?s
states. Before applying the pair HMM method, we
use external datasets to identify a pair HMM vari-
ant that we consider as suitable for application to
transliteration similarity estimation. We then use
the shared task datasets to train the selected pair
HMM variant, and finally apply an algorithm that
is specific to the trained pair HMM for comput-
ing transliteration similarity estimates. In section
2, we discuss transliteration similarity estimation
with regard to applying the pair HMM method;
section 3 describes the experimental setup and re-
sults; section 4 concludes the paper with pointers
to future work.
2 Transliteration Similarity Estimation
using Pair HMMs
To describe the transliteration similarity estima-
tion process, consider examples of corresponding
English (as source language) and Russian (as tar-
get language) Wikipedia topics as shown in Table
1. Across languages, Wikipedia topics are written
in different ways and all words in a topic could be
important for mining transliterations. One main
step in the transliteration mining task is to identify
a set of words in each topic for consideration as
candidate transliterations. As seen in Table 1, it is
very likely that some words will not be selected as
id English topic Russian topic
1 Johnston Atoll ???????? (?????)
2 Oleksandr ????????, ?????????
Palyanytya ??????????
3 Ministers for ?????????:????????
Foreign Affairs of ??????????? ???
Luxembourg ???????????
... ...
Table 1: Example of corresponding English Rus-
sian Wikipedia topics
76
candidate transliterations depending on the criteria
for selection. For example, if a criterion is such
that we consider only words starting with upper-
case characters for English and Russian datasets,
then the Russian word ??????? in the topic pair 1
in Table 1 will not be used as a candidate translit-
eration and that in turn makes the system loose
the likely pair of ?Atoll, ??????. After extracting
candidate transliterations, the approach we use in
this paper takes each candidate word on the source
language side and determines a transliteration es-
timate with each candidate word on the target lan-
guage side. Consider the example for topic id 1 in
Table 1 where we expect to have ?Johnston? and
?Atoll? as candidate source language translitera-
tions, and ?????????? and ??????? as candidate
target language transliterations. The method used
is expected to compare ?Johnston? against ?????-
????? and ???????, and then compare ?Atoll? to
the Russian candidate transliterations. We expect
the output to be ?Johston, ????????? and ?Atoll,
?????? as the most likely single word transliter-
ations from topic pair 1 after sorting out all the
four transliteration similarity estimates in this par-
ticular case. We employ the pair HMM approach
to estimate transliteration similarity for candidate
source-target language words.
A pair HMM has an emission state or states that
generate two observation sequences instead of one
observation sequence as is the case in standard
HMMs. Pair HMMs originate from work in Bi-
ological sequence analysis (Durbin et al, 1998;
Rivas and Eddy, 2001) from which variants were
created and successfully applied in cognate identi-
fication (Mackay and Kondrak, 2005), Dutch di-
alect comparison (Wieling et al, 2007), translit-
eration identification (Nabende et al, 2010),
and transliteration generation (Nabende, 2009).
As mentioned earlier, we have first, tested two
pair HMM variants on manually verified English-
Russian datasets which we obtain from the previ-
ous shared task on machine transliteration (NEWS
2009) (Kumaran and Kellner, 2007). This pre-
liminary test is aimed at determining the effect of
pair HMM parameter changes on the quality of the
transliteration similarity estimates. For the first
pair HMM variant, no transitions are modeled be-
tween edit states; we only use transtion parame-
ters associated with transiting from a start state to
each of the edit operation states, and from each
of the edit operation states to an end state. The
       
 
I 
M 
End 
1-? D-
? I-? M1-
 
? D - ?
D - 
? D 
1 - ? I
 
-
 
? I - ? I
 
? I  ? D  
? I 
? D 
? M 
? D ? I 
? D ? I 
D 
Figure 1: Pair HMM with nine distinct transi-
tion parameters. Emission parameters are speci-
fied with emitting states and their size is dependent
on the characters used in the source and target lan-
guages
second pair HMM variant uses nine distinct tran-
sition parameters between the pair HMM?s states
as shown in Figure 1. The node M in Figure 1 rep-
resents the substitution state in which emission pa-
rameters encode relationships between each of the
source and target language characters. D denotes
the deletion state where emission parameters spec-
ify relationships between source language charac-
ters and a target language gap. I denotes the inser-
tion state where emission parameters encode rela-
tionships between target language characters and
a source language gap. Starting parameters for the
pair HMM in Figure 1 are assoicated with transit-
ing from the M state to one of the edit operation
states including transiting back to M.
The pair HMM parameters are estimated using
the well-known Baum-Welch Expectation Maxi-
mization (EM) algorithm (Baum et al, 1970).
For each pair HMM variant, the training algorithm
starts with a uniform distribution for substitution,
deletion, insertion, and transition parameters, and
iterates through the data until a local maximum.
A method referred to as stratified ten fold cross
validation (Olson and Delen, 2008) is used to eval-
uate the two pair HMM variants. In each fold,
7056 pairs of English-Russian names from the pre-
vious shared task on machine transliteration (Ku-
77
Pair HMM Model CVA CVMRR
phmm00edtrans
Viterbi 0.788 0.809
Forward 0.927 0.954
phmm09edtrans
Viterbi 0.943 0.952
Forward 0.987 0.991
Table 2: CVA and CVMRR results two pair HMM
variants on a preliminary transliteration identifica-
tio experiment. phmm00edtrans is the pair HMM
variant with no transition parameters between the
edit states while phmm09edtrans is the pair HMM
variant with nine distinct transition parameters.
maran and Kellner, 2007) are used for training and
784 name pairs for testing. The Cross Valida-
tion Accuracy (CVA) and Cross Validation Mean
Reciprocal Rank (CVMRR) results obtained from
applying the Forward and Viterbi algorithms of the
two pair HMM variants on this particular dataset
are shown in Table 2.
The CVA and CVMRR values in Table 2 sug-
gest that it is necessary to model for transition pa-
rameters when using pair HMMs for translitera-
tion similarity estimation. Table 2 also suggests
that it is better to use the Forward algorithm for a
given pair HMM variant. Based on the results in
Table 2, the pair HMM variant illustrated in Figure
1 is chosen for application in estimating transliter-
ation similarity for the mining task.
3 Experimental setup and Results
To simplify the analysis of the source and tar-
get strings, the pair HMM system requires unique
whole number representations for each character
in the source and target language data. This is not
suitable for all the different types of writing sys-
tems. In this paper, we look at only the English
and Russian languages where many characters are
associated with a phonemic alphabet and where
numbered representations are hardly expected to
contribute to errors from loss of information in-
herent in the original orthography. A preliminary
run on Chinese-English1 datasets from the previ-
ous shared task on machine transliteration (NEWS
2009) resulted in an accuracy of 0.213 and MRR
of 0.327 using the pair HMM variant in Figure
1. In the following subsection we discuss some
data preprocessing steps on the English-Russian
1In this case Chinese is the source language while English
is the target language
Wikipedia dataset.
3.1 English and Russian candidate
transliteration extraction
The English-Russian Wikipedia dataset that was
provided for the transliteration mining task is very
noisy meaning that it has various types of other en-
tities in addition to words for each language?s or-
thography. A first step in simplifying the translit-
eration mining process was to remove any unnec-
essary entities.
We observed the overlap of writing systems in
both the English and Russian Wikipedia datasets.
We therefore made sure that there is no topic
where the same writing system is used in both the
English and Russian data. Any strings that contain
characters that are not associated with the writ-
ing systems for English and Russian were also re-
moved.
We also observed the presence of many tempo-
ral and numerical expressions that are not neces-
sary on both the English and Russian Wikipedia
datasets. We applied different sets of rules to re-
move such expressions while leaving any neces-
sary words.
Using knowledge about the initial formatting
of strings in both the English and Russian data,
a set of rules was applied to split most of the
strings based on different characters. For ex-
ample almost all strings in the English side had
the underscore ? ? character as a string separa-
tor. We also removed characters such as: colons,
semi-colons, commas, question marks, exclama-
tion marks, dashes, hyphens, forward and back
slashes, mathematical operator symbols, currency
symbols, etc. Some strings were also split based
on string patterns, for example where different
words are joined into one string and it was easy
to identify that the uppercase character for each
word still remained in the combined string just like
when it is alone. We also removed many abbrevia-
tions and titles in the datasets that were not neces-
sary for analysis during the transliteration mining
process.
After selecting candidate words based on most
of the criteria above, we determine all characters
in our extracted candidate transliteration data and
compare against those in the shared task?s seed
data (Kumaran et al, 2010) with the aim of find-
ing all characters that are missing in the seed data.
Matching transliteration pairs with the the miss-
78
ing characters are then hand picked from the can-
didate words dataset and added to the seed data
before training the pair HMM variant that is se-
lected from the previous section. The process for
identifying missing characters and words that have
them is carried out seperately for each language.
However, a matching word in the other language
is identified to constitute a transliteration pair that
can be added to the seed dataset. For the English-
Russian dataset, we use 142 transliteration pairs in
addition to the 1000 transliteration pairs in the ini-
tial seed data. We hence apply the Baum-Welch
algorithm for the selected pair HMM specification
from section 2 on a total of 1142 transliteration
pairs. The algorithm performed 182 iterations be-
fore converging for this particular dataset.
3.2 Results
To obtain transliteration similarity measures, we
apply the Forward algorithm of the trained pair
HMM from section 3.1 to all the remaining
Wikipedia topics. For each word in an English
topic, the algorithm computes transliteration simi-
larity estimates for all words in the Russian topic.
After observing transliteration similarity estimates
for a subset of candidate transliteration words, we
specify a single threshold value (th) and use it
for identifying potential transliteration pairs. A
threshold value of 1 ? 10?13 was chosen after
observing that many of the pairs that had a sim-
ilarity estimate above this threshold were indeed
transliteration pairs. Therefore, a pair of words
was taken as a potential transliteration pair only
when its transliteration estimate (tr sim) was such
that tr sim > th. This resulted in a total of
299389 potential English-Russian transliteration
pairs. This collection of potential transliteration
pairs has been evaluated using a random set of cor-
responding English and Russian Wikipedia topics
as specified in the NEWS 2010 White paper for
the transliteration mining task (Kumaran et al,
2010). Table 3 shows the precision, recall, and
f-score results2 that were obtained after applying
the Forward algorithm for the pair HMM of Fig-
ure 1.
Despite using the pair HMM method with its
basic probabilistic one-to-one mapping for each
2The numbers in Table 3 were obtained from a post eval-
uation after correcting a number of processing errors in the
pair HMM transliteration mining system. The errors initially
led to relatively lower values associated with the measures in
this Table. The values in this Table are therefore not part of
the initial shared task results
Model precision recall f-score
phmm09edtrans 0.780 0.834 0.806
Table 3: Evaluation results for the Pair HMM of
Figure 1 on a random selection of 1000 corre-
sponding English Russian Wikipedia topics.
of the source target character representations, the
result in Table 3 suggests a promising applica-
tion of pair HMMs in mining transliterations from
Wikipedia.
4 Conclusions and Future Work
We have described the application of Pair HMMs
to mining transliterations from Wikipedia. The
transliteration mining evaluation results suggest
a valuable application of Pair HMMs to mining
transliterations. Currently, the pair HMM system
is considered to be best applicable to languages
whose writing system mostly uses a phonemic al-
phabet. Although an experimental test run was
done for Chinese-English data, a conclusion about
the general applicability of the pair HMM neces-
sitates additional tests using other language pairs
such as Hindi and Tamil which were also part of
the shared task.
As future work, we would like to investigate
the performance of Pair HMMs on additional writ-
ing systems. This may require additional modi-
fications to a pair HMM system to minimize on
input formatting errors for other types of writ-
ing systems. It is also necessary to determine the
transliteration mining performance of pair HMMs
when more tolerant criteria are used on the noisy
Wikipedia data. Currently, the pair HMM is ap-
plied in its most basic form, that is, no complex
modifications have been implemented for example
modeling for context in source and target language
words, and other factors that may affect the quality
of a transliteration similarity estimate; it should be
interesting to investigate perfromance of complex
pair HMM variants in transliteration mining.
Acknowledgments
Research in this paper is funded through a second
NPT (Uganda) Project.
References
A Kumaran, Mitesh Khapra, and Haizhou Li. 2010.
Whitepaper on NEWS 2010 Shared Task on
79
Transliteration Mining.
A Kumaran and Tobias Kellner. 2007. A Generic
Framework for Machine Transliteration. Proceed-
ings of the 30th Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval (SIGIR 2007), pp 721?722, Ams-
terdam, The Netherlands.
Leonard E. Baum, Ted Petrie, George Soules, and Nor-
man Weiss. 1970. A Maximization Technique Oc-
curring in the Statistical Analysis of Probabilistic
Functions of Markov Chains. Annals of Mathemati-
cal Statistics, 41(1):164?171.
David L. Olson and Dursun Delen. 2008. Advanced
Data Mining Techniques. Springer.
Elena Rivas and Sean R. Eddy. 2001. Noncoding RNA
Gene Detection using Comparative Sequence Anal-
ysis. BMC Bioinformatics 2001, 2:8.
Martijn Wieling, Therese Leinonen, and John Ner-
bonne. 2007. Inducing Sound Segment Differences
using Pair Hidden Markov Models. In John Ner-
bonne, Mark Ellison, and Grzegorz Kondrak (eds.)
Computing Historical Phonology: 9th Meeting of
the ACL Special Interest Group for Computational
Morphology and Phonology Workshop, pp 48?56,
Prague, Czech Republic.
Peter Nabende. 2009. Transliteration System using
Pair HMMs with Weighted FSTs. Proceedings of
the Named Entities Workshop, NEWS?09, pp 100?
103, Suntec, Singapore.
Peter Nabende, Jorg Tiedemann, and John Nerbonne.
2010. Pair Hidden Markov Model for Named Entity
Matching. In Tarek Sobh (ed.) Innovations and Ad-
vances in Computer Sciences and Engineering, pp
497?502, Springer, Heidelberg.
Richard Durbin, Sean R. Eddy, Anders Krogh, Graeme
Mitchison. 1998. Biological Sequence Analysis:
Probabilistic Models of Proteins and Nucleic Acids.
Cambridge University Press, Cambridge, UK.
Wesley Mackay and Grzegorz Kondrak. 2005. Com-
puting Word Similarity and Identifying Cognates
with Pair Hidden Markov Models. Proceedings
of the ninth Conference on Computational Natural
Language Learning (CoNLL 2005), pp 40?47, Ann
Arbor, Michigan.
80
