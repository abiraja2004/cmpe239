Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 1?11,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
A New Sentence Compression Dataset and Its Use in an Abstractive
Generate-and-Rank Sentence Compressor
Dimitrios Galanis? and Ion Androutsopoulos?+
?Department of Informatics, Athens University of Economics and Business, Greece
+Digital Curation Unit ? IMIS, Research Center ?Athena?, Greece
Abstract
Sentence compression has attracted much in-
terest in recent years, but most sentence com-
pressors are extractive, i.e., they only delete
words. There is a lack of appropriate datasets
to train and evaluate abstractive sentence com-
pressors, i.e., methods that apart from delet-
ing words can also rephrase expressions. We
present a new dataset that contains candi-
date extractive and abstractive compressions
of source sentences. The candidate compres-
sions are annotated with human judgements
for grammaticality and meaning preservation.
We discuss how the dataset was created, and
how it can be used in generate-and-rank ab-
stractive sentence compressors. We also re-
port experimental results with a novel abstrac-
tive sentence compressor that uses the dataset.
1 Introduction
Sentence compression is the task of producing a
shorter form of a grammatical source (input) sen-
tence, so that the new form will still be grammati-
cal and it will retain the most important information
of the source (Jing, 2000). Sentence compression is
useful in many applications, such as text summariza-
tion (Madnani et al, 2007) and subtitle generation
(Corston-Oliver, 2001). Methods for sentence com-
pression can be divided in two categories: extrac-
tive methods produce compressions by only remov-
ing words, whereas abstractive methods may addi-
tionally rephrase expressions of the source sentence.
Extractive methods are generally simpler and have
dominated the sentence compression literature (Jing,
2000; Knight and Marcu, 2002; McDonald, 2006;
Cohn and Lapata, 2007; Clarke and Lapata, 2008;
Cohn and Lapata, 2009; Nomoto, 2009; Galanis
and Androutsopoulos, 2010; Yamangil and Shieber,
2010). Abstractive methods, however, can in prin-
ciple produce shorter compressions that convey the
same information as longer extractive ones. Further-
more, humans produce mostly abstractive compres-
sions (Cohn and Lapata, 2008); hence, abstractive
compressors may generate more natural outputs.
When evaluating extractive methods, it suffices
to have a single human gold extractive compres-
sion per source sentence, because it has been shown
that measuring the similarity (as F1-measure of de-
pendencies) between the dependency tree of the
gold compression and that of a machine-generated
compression correlates well with human judgements
(Riezler et al, 2003; Clarke and Lapata, 2006a).
With abstractive methods, however, there is a much
wider range of acceptable abstractive compressions
of each source sentence, to the extent that a single
gold compression per source is insufficient. Indeed,
to the best of our knowledge no measure to com-
pare a machine-generated abstractive compression
to a single human gold compression has been shown
to correlate well with human judgements.
One might attempt to provide multiple human
gold abstractive compressions per source sentence
and employ measures from machine translation, for
example BLEU (Papineni et al, 2002), to compare
each machine-generated compression to all the cor-
responding gold ones. However, a large number of
gold compressions would be necessary to capture all
(or at least most) of the acceptable shorter rephras-
1
ings of the source sentences, and it is questionable
if human judges could provide (or even think of) all
the acceptable rephrasings. In machine translation,
n-gram-based evaluation measures like BLEU have
been criticized exactly because they cannot cope
sufficiently well with paraphrases (Callison-Burch
et al, 2006), which play a central role in abstractive
sentence compression (Zhao et al, 2009a).1
Although it is difficult to construct datasets for
end-to-end automatic evaluation of abstractive sen-
tence compression methods, it is possible to con-
struct datasets to evaluate the ranking components
of generate-and-rank abstractive sentence compres-
sors, i.e., compressors that first generate a large set
of candidate abstractive (and possibly also extrac-
tive) compressions of the source and then rank them
to select the best one. In previous work (Galanis and
Androutsopoulos, 2010), we presented a generate-
and-rank extractive sentence compressor, hereafter
called GA-EXTR, which achieved state-of-the art re-
sults. We aim to construct a similar abstractive
generate-and-rank sentence compressor. As part of
this endeavour, we needed a dataset to automatically
test (and train) several alternative ranking compo-
nents. In this paper, we introduce a dataset of this
kind, which we also make publicly available.2
The dataset consists of pairs of source sentences
and candidate extractive or abstractive compres-
sions. The candidate compressions were generated
by first using GA-EXTR and then applying exist-
ing paraphrasing rules (Zhao et al, 2009b) to the
best extractive compressions of GA-EXTR. Each pair
(source and candidate compression) was then scored
by a human judge for grammaticality and meaning
preservation. We discuss how the dataset was con-
structed and how we established upper and lower
performance boundaries for ranking components of
compressors that may use it. We also present the
1Ways to extend n-gram measures to account for para-
phrases have been proposed (Zhou et al, 2006; Kauchak and
Barzilay, 2006; Pado? et al, 2009), but they require accu-
rate paraphrase recognizers (Androutsopoulos and Malakasio-
tis, 2010), which are not yet available; or they assume that
the same paraphrase generation resources (Madnani and Dorr,
2010), for example paraphrasing rules, that some abstractive
sentence compressors (including ours) use always produce ac-
ceptable paraphrases, which is not the case as discussed below.
2The new dataset and GA-EXTR are freely available from
http://nlp.cs.aueb.gr/software.html.
current version of our abstractive sentence compres-
sor, and we discuss how its ranking component was
improved by performing experiments on the dataset.
Section 2 below summarizes prior work on ab-
stractive sentence compression. Section 3 discusses
the dataset we constructed. Section 4 describes our
abstractive sentence compressor. Section 5 presents
our experimental results, and Section 6 concludes.
2 Prior work on abstractive compression
The first abstractive compression method was pro-
posed by Cohn and Lapata (2008). It learns a set of
parse tree transduction rules from a training dataset
of pairs, each pair consisting of a source sentence
and a single human-authored gold abstractive com-
pression. The set of transduction rules is then aug-
mented by applying a pivoting approach to a par-
allel bilingual corpus; we discuss similar pivoting
mechanisms below. To compress a new sentence, a
chart-based decoder and a Structured Support Vec-
tor Machine (Tsochantaridis et al, 2005) are used to
select the best abstractive compression among those
licensed by the rules learnt.
The dataset that Cohn and Lapata (2008) used
to learn transduction rules consists of 570 pairs of
source sentences and abstractive compressions. The
compressions were produced by humans who were
allowed to use any transformation they wished. We
used a sample of 50 pairs from that dataset to con-
firm that humans produce mostly abstractive com-
pressions. Indeed, 42 (84%) of the compressions
were abstractive, and only 7 (14%) were simply ex-
tractive.3 We could not use that dataset, however,
for automatic evaluation purposes, since it only pro-
vides a single human gold abstract compression per
source, which is insufficient as already discussed.
More recently, Zhao et al (2009a) presented a
sentence paraphrasing method that can be config-
ured for different tasks, including a form of sentence
compression. For each source sentence, Zhao et al?s
method uses a decoder to produce the best possible
paraphrase, much as in phrase-based statistical ma-
chine translation (Koehn, 2009), but with phrase ta-
bles corresponding to paraphrasing rules (e.g., ?X
3Cohn and Lapata?s dataset is available from http://
staffwww.dcs.shef.ac.uk/people/T.Cohn/t3/#
Corpus. One pair (2%) of our sample had a ?compression?
that was identical to the input.
2
is the author of Y ? ? ?X wrote Y ?) obtained from
parallel and comparable corpora (Zhao et al, 2008).
The decoder uses a log-linear objective function, the
weights of which are estimated with a minimum er-
ror rate training approach (Och, 2003). The objec-
tive function combines a language model, a para-
phrase model (combining the quality scores of the
paraphrasing rules that turn the source into the can-
didate paraphrase), and a task-specific model; in the
case of sentence compression, the latter model re-
wards shorter candidate paraphrases.
We note that Zhao et al?s method (2009a) is in-
tended to produce paraphrases, even when config-
ured to prefer shorter paraphrases, i.e., the compres-
sions are still intended to convey the same informa-
tion as the source sentences. By contrast, most sen-
tence compression methods (both extractive and ab-
stractive, including ours) are expected to retain only
the most important information of the source sen-
tence, in order to achieve better compression rates.
Hence, Zhao et al?s sentence compression task is not
the same as the task we are concerned with, and the
compressions we aim for are significantly shorter.
3 The new dataset
To construct the new dataset, we used source sen-
tences from the 570 pairs of Cohn and Lapata (Sec-
tion 2). This way a human gold abstractive com-
pression is also available for each source sentence,
though we do not currently use the gold compres-
sions in our experiments. We actually used only 346
of the 570 source sentences of Cohn and Lapata, re-
serving the remaining 224 for further experiments.4
To obtain candidate compressions, we first ap-
plied GA-EXTR to the 346 source sentences, and we
then applied the paraphrasing rules of Zhao et al
(2009b) to the resulting extractive compressions; we
provide more information about GA-EXTR and the
paraphrasing rules below. We decided to apply para-
phrasing rules to extractive compressions, because
we noticed that most of the 42 human abstractive
compressions of the 50 sample pairs from Cohn and
Lapata?s dataset that we initially considered (Sec-
tion 2) could be produced from the corresponding
source sentences by first deleting words and then us-
4The 346 sources are from 19 randomly selected articles
among the 30 that Cohn and Lapata drew source sentences from.
ing shorter paraphrases, as in the following example.
source: Constraints on recruiting are constraints on
safety and have to be removed.
extractive: Constraints on recruiting have to be re-
moved.
abstractive: Recruiting constraints must be removed.
3.1 Extractive candidate compressions
GA-EXTR, which we first applied to the dataset?s
source sentences, generates extractive candidate
compressions by pruning branches of each source?s
dependency tree; a Maximum Entropy classifier is
used to guide the pruning. Subsequently, GA-EXTR
ranks the extractive candidates using a Support Vec-
tor Regression (SVR) model, which assigns a score
F (eij |si) to each candidate extractive compression
eij of a source sentence si by examining features
of si and eij ; consult our previous work (Galanis
and Androutsopoulos, 2010) for details.5 For each
source si, we kept the (at most) kmax = 10 extrac-
tive candidates eij with the highest F (eij |si) scores.
3.2 Abstractive candidate compressions
We then applied Zhao et al?s (2009b) paraphrasing
rules to each one of the extractive compressions eij .
The rules are of the form left ? right, with left and
right being sequences of words and slots; the slots
are part-of-speech tagged and they can be filled in
with words of the corresponding categories. Exam-
ples of rules are shown below.
? get rid of NNS1? remove NNS1
? get into NNP1? enter NNP1
? NNP1 was written by NNP2? NNP2 wrote NNP1
Roughly speaking, the rules were extracted from
a parallel English-Chinese corpus, based on the as-
sumption that two English phrases ?1 and ?2 that
are often aligned to the same Chinese phrase ? are
5We trained GA-EXTR on approximately 1,050 pairs of
source sentences and gold human extractive compressions,
obtained from Edinburgh?s ?written? extractive dataset; see
http://jamesclarke.net/research/resources.
The source sentences of that dataset are from 82 documents.
The 1,050 pairs that we used had source sentences from 52 out
of the 82 documents. We did not use source sentences from
the other 30 documents, because they were used by Cohn and
Lapata (2008) to build their abstractive dataset (Section 2),
from which we drew source sentences for our dataset.
3
si
 '' ,,ei1
 '' ,,
ei2
,, -- ..
? ? ? eik
-- ..ai1.1
 && ,,
ai1.2
,,
? ? ? ai1.mi1 ai2.1 ai2.2 ? ? ? ai2.mi2 ? ? ? aik.1 ? ? ? aik.mik
ai1.1.1

ai1.1.2 ? ? ? ai1.1.mi1.1 ai1.2.1 ? ? ?
? ? ?
Figure 1: Generating candidate extractive (eij) and abstractive (aij...) compressions from a source sentence (si).
likely to be paraphrases and, hence, can be treated
as a paraphrasing rule ?1 ? ?2. This pivoting was
used, for example, by Bannard and Callison-Burch
(2005), and it underlies several other paraphrase
extraction methods (Riezler et al, 2007; Callison-
Burch, 2008; Kok and Brockett, 2010). Zhao et
al. (2009b) provide approximately one million rules,
but we use only approximately half of them, because
we use only rules that can shorten a sentence, and
only in the direction that shortens the sentence.
From each extractive candidate eij , we pro-
duced abstractive candidates aij.1, aij.2, . . . , aij.mij
(Figure 1) by applying a single (each time
different) applicable paraphrasing rule to eij .
From each of the resulting abstractive candidates
aij.l, we produced further abstractive candidates
aij.l.1, aij.l.2, . . . , aij.l.mij.l by applying again a sin-
gle (each time different) rule. We repeated this pro-
cess in a breadth-first manner, allowing up to at most
rulemax = 5 rule applications to an extractive candi-
date eij , i.e., up to depth six in Figure 1, and up to
a total of abstrmax = 50 abstractive candidates per
eij . Zhao et al (2009b) associate each paraphrasing
rule with a score, intended to indicate its quality.6
Whenever multiple paraphrasing rules could be ap-
plied, we applied the rule with the highest score first.
3.3 Human judgement annotations
For each one of the 346 sources si, we placed its
extractive (at most kmax = 10) and abstractive (at
most abstrmax = 50) candidate compressions into
a single pool (extractive and abstractive together),
and we selected from the pool the (at most) 10 can-
didate compressions cij with the highest language
6Each rule is actually associated with three scores. We use
the ?Model 1? score; see Zhao et al (2009b) for details.
model scores, computed using a 3-gram language
model.7 For each cij , we formed a pair ?si, cij?,
where si is a source sentence and cij a candidate
(extractive or abstractive) compression. This led to
3,072 ?si, cij? pairs. Each pair was given to a human
judge, who scored it for grammaticality (how gram-
matical cij was) and meaning preservation (to what
extent cij preserved the most important information
of si). Both scores were provided on a 1?5 scale (1
for rubbish, 5 for perfect). The dataset that we use
in the following sections and that we make publicly
available comprises the 3,072 pairs and their gram-
maticality and meaning preservation scores.
We define the GM score of an ?si, cij? pair to be
the sum of its grammaticality and meaning preser-
vation scores. Table 1 shows the distribution of
GM scores in the 3,072 pairs. Low GM scores (2?
5) are less frequent than higher scores (6?10), but
this is not surprising given that we selected pairs
whose cij had high language model scores, that
we used the kmax extractive compressions of each
si that GA-EXTR considered best, and that we as-
signed higher preference to applying paraphrasing
rules with higher scores. We note, however, that ap-
plying a paraphrasing rule does not necessarily pre-
serve neither grammaticality nor meaning, even if
the rule has a high score. Szpektor et al (2008) point
out that, for example, a rule like ?X acquire Y ??
?X buy Y ? may work well in many contexts, but
not in ?Children acquire language quickly?. Sim-
ilarly, ?X charged Y with? ? ?X accused Y of?
should not be applied to sentences about batteries.
Many (but not all) inappropriate rule applications
7We used SRILM with modified Kneser-Ney smoothing
(Stolcke, 2002). We trained the language model on approxi-
mately 4.5 million sentences from the TIPSTER corpus.
4
Training part Test part
GM extractive abstractive total extractive abstractive total
score candidates candidates candidates candidates candidates candidates
2 13 (1.3%) 10 (1.3%) 23 (1.3%) 19 (1.9%) 2 (0.4%) 21 (1.5%)
3 26 (2.7%) 28 (3.6%) 54 (3.1%) 10 (1.0%) 0 (0%) 10 (0.7%)
4 55 (5.8%) 29 (5.1%) 94 (5.5%) 51 (5.3%) 26 (6.2%) 77 (5.5%)
5 52 (5.5%) 65 (8.5%) 117 (6.9%) 77 (8.0%) 42 (10.0%) 119 (8.6%)
6 102 (10.9%) 74 (9.7%) 176 (10.3%) 125 (13.0%) 83 (19.8%) 208 (15.1%)
7 129 (13.8%) 128 (16.8%) 257 (15.1%) 151 (15.7%) 53 (12.6%) 204 (14.8%)
8 157 (16.8%) 175 (23.0%) 332 (19.5%) 138 (14.3%) 85 (20.3%) 223 (16.1%)
9 177 (18.9%) 132 (17.3%) 309 (18.2%) 183 (19.0%) 84 (20.1%) 267 (19.3%)
10 223 (23.8%) 110 (14.4%) 333 (19.6%) 205 (21.3%) 43 (10.2%) 248 (18.0%)
total 934 (55.1%) 761 (44.9%) 1,695 (100%) 959 (69.6%) 418 (30.4%) 1,377 (100%)
Table 1: Distribution of GM scores (grammaticality plus meaning preservation) in our dataset.
lead to low language model scores, which is partly
why there are more extractive than abstractive can-
didate compressions in the dataset; another reason is
that few or no paraphrasing rules apply to some of
the extractive candidates.
We use 1,695 (from 188 source sentences) of the
3,072 pairs to train different versions of our abstrac-
tive compressor?s ranking component, discussed be-
low, and 1,377 pairs (from 158 sources) as a test set.
3.4 Inter-annotator agreement
Although we used a total of 16 judges (computer sci-
ence graduate students), each one of the 3,072 pairs
was scored by a single judge, because a prelimi-
nary study indicated reasonably high inter-annotator
agreement.8 More specifically, before the dataset
was constructed, we created 161 ?si, cij? pairs (from
22 source sentences) in the same way, and we gave
them to 3 of the 16 judges. Each pair was scored by
all three judges. The average (over pairs of judges)
Pearson correlation of the grammaticality, meaning
preservation, and GM scores, was 0.63, 0.60, and
0.69, respectively.9 We conjecture that the higher
correlation of GM scores, compared to grammati-
cality and meaning preservation, is due to the fact
that when a candidate compression looks bad the
judges sometimes do not agree if they should re-
duce the grammaticality or the meaning preservation
8The judges were fluent, but not native, English speakers.
9The Pearson correlation ranges in [?1,+1] and measures
the linear relationship of two variables. A correlation of +1 in-
dicates perfect positive relationship, while ?1 indicates perfect
negative relationship; a correlation of 0 signals no relationship.
candidate average Pearson
compressions correlation
Extractive 112 0.71
Abstractive 49 0.64
All 161 0.69
Table 2: Inter-annotator agreement on GM scores.
score, but the difference does not show up in the GM
score (the sum). Table 2 shows the average corre-
lation of the GM scores of the three judges on the
161 pairs, and separately for pairs that involved ex-
tractive or abstractive candidate compressions. The
judges agreed more on extractive candidates, since
the paraphrasing stage that is involved in the abstrac-
tive candidates makes the task more subjective.10
3.5 Performance boundaries
When presented with two pairs ?si, cij? and?
si, cij?
?
with the same si and equally long cij and
cij? , an ideal ranking component should prefer the
pair with the highest GM score. More generally, to
consider the possibly different lengths of cij and cij? ,
we first define the compression rate CR(cij |si) of a
candidate compression cij as follows, where |?| is
length in characters; lower values of CR are better.
CR(cij |si) =
|cij |
|si|
The GMC? score of a candidate compression, which
also considers the compression rate by assigning it a
10The correlation that we measured on extractive candidates
(0.71) is very close to the corresponding figure (0.746) that has
been reported by Clarke and Lapata (2006b).
5
Figure 2: Results of three SVR-based ranking components on our dataset, along with performance boundaries obtained
using an oracle and a random baseline. The right diagram shows how the performance of our best SVR-based ranking
component is affected when using only 33% and 63% of the training examples.
weight ?, is then defined as follows.
GMC?(cij |si) = GM(cij |si)? ? ? CR(cij |si)
For a given ?, when presented with ?si, cij? and?
si, cij?
?
, an ideal ranking component should prefer
the pair with the highest GMC? score.
The upper curve of the left diagram of Figure 2
shows the performance of an ideal ranking com-
ponent, an oracle, on the test part of the dataset.
For every source si, the oracle selects the ?si, cij?
pair (among the at most 10 pairs of si) for which
GMC?(cij |si) is maximum; if two pairs have iden-
tical GMC? scores, it prefers the one with the low-
est CR(cij |si). The vertical axis shows the average
GM(cij |si) score of the selected pairs, for all the si
sources, and the horizontal axis shows the average
CR(cij |si). Different points of the curve are obtained
by using different ? values. As the selected candi-
dates get shorter (lower compression rate), the aver-
age GM score decreases, as one would expect.11
11The discontinuity in the oracle?s curve for average com-
The other curves of Figure 2 correspond to al-
ternative ranking components that we tested, dis-
cussed below, which do not consult the judges? GM
scores. For each si, these ranking components at-
tempt to guess the GM scores of the ?si, cij? pairs
that are available for si, and they then rank the pairs
by GMC? using the guessed GM scores. The lower
points of the left diagram were obtained with a base-
line ranking component that assigns a random GM
score to each pair. The oracle and the baseline can
be seen as establishing upper and lower performance
boundaries of ranking components on our dataset.
4 Our abstractive compressor
Our abstractive sentence compressor operates in two
stages. Given a source sentence si, extractive and
pression rates above 0.7, i.e., when long compressions are only
mildly penalized, is caused by the fact that many long candi-
date compressions have high and almost equal GM scores, but
still very different compression rates; hence, a slight modifica-
tion of ? leads the oracle to select candidates with the same GM
scores, but very different compression rates.
6
abstractive candidate compressions are first gener-
ated as in Sections 3.1 and 3.2. In a second stage, a
ranking component is used to select the best candi-
date. Below we discuss the three SVR-based ranking
components that we experimented with.
4.1 Ranking candidates with an SVR
An SVR is very similar to a Support Vector Machine
(Vapnik, 1998; Cristianini and Shawe-Taylor, 2000;
Joachims, 2002), but it is trained on examples of the
form ?xl, y(xl)?, where each xl ? Rn is a vector of n
features, and y(xl) ? R. The SVR learns a function
f : Rn ? R intended to return f(x) values as close
as possible to the correct y(x) values.12 In our case,
each vector xij contains features providing informa-
tion about an ?si, cij? pair of a source sentence si
and a candidate compression cij . For pairs that have
been scored by human judges, the f(xij) returned by
the SVR should ideally be y(xij) = GMC?(cij |si);
once trained, however, the SVR may be presented
with xij vectors of unseen ?si, cij? pairs.
For an unseen source si, our abstractive compres-
sor first generates extractive and abstractive candi-
dates cij , it then forms the vectors xij of all the
pairs ?si, cij?, and it returns the cij for which the
SVR?s f(xij) is maximum. On a test set (like the
test part of our dataset), if the f(xij) values the
SVR returns are very close to the corresponding
y(xij) = GMC?(cij |si) scores, the ranking compo-
nent will tend to select the same cij for each si as the
oracle, i.e., it will achieve optimum performance.
4.2 Base form of our SVR ranking component
The simplest form of our SVR-based ranking compo-
nent, called SVR-BASE, uses vectors xij that include
the following features of ?si, cij?. Hereafter, if cij is
an extractive candidate, then e(cij) = cij ; otherwise
e(cij) is the extractive candidate that cij was derived
from by applying paraphrasing rules.13
? The language model score of si and cij (2 fea-
12We use LIBSVM (http://www.csie.ntu.edu.tw/
?cjlin/libsvm) with an RBF kernel, which permits the
SVR to learn non-linear functions. We also experimented with a
ranking SVM, but the results were slightly inferior.
13All the feature values are normalized in [0, 1]; this also ap-
plies to the GMC? scores when they are used by the SVR. The
e(cij) of each cij and the paraphrasing rules that were applied
to e(cij) to produce cij are also included in the dataset.
tures), computed as in Section 3.3.
? The F (e(cij)|si) score that GA-EXTR returned.
? The compression rate CR(e(cij)|si).
? The number (possibly zero) of paraphrasing
rules that were applied to e(cij) to produce cij .
4.3 Additional PMI-based features
For two words w1, w2, their PMI score is:
PMI(w1, w2) = log
P (w1, w2)
P (w1) ? P (w2)
where P (w1, w2) is the probability of w1, w2 co-
occurring; we require them to co-occur in the same
sentence at a maximum distance of 10 tokens.14
If w1, w2 are completely independent, then their
PMI score is zero. If they always co-occur, their
PMI score is maximum, equal to ? logP (w1) =
? logP (w2).15 We use PMI to assess if the words
of a candidate compression co-occur as frequently
as those of the source sentence; if not, this may indi-
cate an inappropriate application of a paraphrasing
rule (e.g., having replaced ?charged Y with? by ?X
accused Y of? in a sentence about batteries).
More specifically, we define the PMI(?) score of
a sentence ? to be the average PMI(wi, wj) of ev-
ery two content words wi, wj that co-occur in ? at
a maximum distance of 10 tokens; below N is the
number of such pairs.
PMI(?) =
1
N
?
?
i,j
PMI(wi, wj)
In our second SVR-based ranking component, SVR-
PMI, we compute PMI(si), PMI(e), and PMI(cij),
and we include them as three additional features;
otherwise SVR-PMI is identical to SVR-BASE.
14We used texts from TIPSTER and AQUAINT, a total of 953
million tokens, to estimate PMI(w1, w2).
15A problem with PMI is that two frequent and completely de-
pendent words receive lower scores than two other, less frequent
completely dependent words (Manning and Schutze, 2000).
Pecina (2005), however, found PMI to be the best collocation
extraction measure; and Newman et al (2010) found it to be the
best measure of ?topical coherence? for sets of words.
7
4.4 Additional LDA-based features
Our third SVR-based ranking component includes
features from a Latent Dirichlet Allocation (LDA)
model (Blei et al, 2003). Roughly speaking, LDA
models assume that each document d of |d| words
w1, . . . , w|d| is generated by iteratively (for r =
1, . . . , |d|) selecting a topic tr from a document-
specific multinomial distribution P (t|d) overK top-
ics, and then (for each r) selecting a word wr from a
topic-specific multinomial distribution P (w|t) over
the vocabulary.16 The probability, then, of encoun-
tering a word w in a document d is the following.
P (w|d) =
?
t
P (w|t) ? P (t|d) (1)
An LDA model can be trained on a corpus to estimate
the parameters of the distributions it involves; and
given a trained model, there are methods to infer the
topic distribution P (t|d?) of a new document d?.17
In our case, we treat each source sentence as
a new document d?, and we use an LDA model
trained on a generic corpus to infer the topic distri-
bution P (t|d?) of the source sentence.18 We assume
that a good candidate compression should contain
words with high P (w|d?), computed as in Equation
1 with P (t|d) = P (t|d?) and using the P (w|t) that
was learnt during training, because words with high
P (w|d?) are more likely to express (high P (w|t))
prominent topics (high P (t|d?)) of the source.
Consequently, we can assess how good a can-
didate compression is by computing the average
P (w|d?) of its words; we actually compute the
average logP (w|d?). More specifically, for a
given source si and another sentence ?, we define
LDA(?|si) as follows (d? = si), where w1, . . . , w|?|
are now the words of ?, ignoring stop-words.
LDA(?|si) =
1
|?|
?
|?|?
r=1
logP (wr|si)
16The document-specific parameters of the first multinomial
distribution are drawn from a Dirichlet distribution.
17We use MALLET (http://mallet.cs.umass.edu),
with Gibbs sampling (Griffiths and Steyvers, 2004). We
set K = 800, having first experimented with K =
200, 400, 600, 800, 1000.
18We trained the LDA model on approximately 106,000 arti-
cles from the TIPSTER and AQUAINT corpora.
In our third SVR-based ranking component, SVR-
PMI-LDA, the feature vector xij of each ?si, cij?
pair includes LDA(cij |si), LDA(e(cij)|si), and
LDA(si|si) as additional features; otherwise, SVR-
PMI-LDA is identical to SVR-PMI. The third feature
allows the SVR to check how far LDA(cij |si) and
LDA(e(cij)|si) are from LDA(si|si).
5 Experiments
To assess the performance of SVR-BASE, SVR-PMI,
and SVR-PMI-LDA, we trained the three SVR-based
ranking components on the training part of our
dataset, and we evaluated them on the test part. We
repeated the experiments for 81 different ? values to
obtain average GM scores at different average com-
pression rates (Section 3.5). The resulting curves
of the three SVR-based ranking components are in-
cluded in Figure 2 (left diagram). Overall, SVR-
PMI-LDA performed better than SVR-PMI and SVR-
BASE, since it achieved the best average GM scores
throughout the range of average compression rates.
In general, SVR-PMI also performed better than
SVR-BASE, though the average GM score of SVR-
BASE was sometimes higher. All three SVR-based
ranking components performed better than the ran-
dom baseline, but worse than the oracle; hence, there
is scope for further improvements in the ranking
components, which is also why we believe other re-
searchers may wish to experiment with our dataset.
The oracle selected abstractive (as opposed to
simply extractive) candidates for 20 (13%) to 30
(19%, depending on ?) of the 158 source sentences
of the test part; the same applies to the SVR-based
ranking components. Hence, good abstractive can-
didates (or at least better than the corresponding ex-
tractive ones) are present in the dataset. Humans,
however, produce mostly abstractive compressions,
as already discussed; the fact that the oracle (which
uses human judgements) does not select abstrac-
tive candidates more frequently may be an indica-
tion that more or better abstractive candidates are
needed. We plan to investigate alternative methods
to produce more abstractive candidates. For exam-
ple, one could translate each source to multiple pivot
languages and back to the original language by using
multiple commercial machine translation engines in-
stead of, or in addition to applying paraphrasing
8
source generated
Gillette was considered a leading financial analyst on the beverage in-
dustry - one who also had an expert palate for wine tasting.
Gillette was seen as a leading financial analyst on the beverage industry
- one who also had an expert palate.
Nearly 200,000 lawsuits were brought by women who said they suf-
fered injuries ranging from minor inflammation to infertility and in
some cases, death.
Lawsuits were made by women who said they suffered injuries ranging
from inflammation to infertility in some cases, death.
Marcello Mastroianni, the witty, affable and darkly handsome Italian
actor who sprang on international consciousness in Federico Fellini?s
1960 classic ?La Dolce Vita,? died Wednesday at his Paris home.
Marcello Mastroianni died Wednesday at his home.
A pioneer in laparoscopy, he held over 30 patents for medical instru-
ments used in abdominal surgery such as tubal ligations.
He held over 30 patents for the medical tools used in abdominal surgery.
LOS ANGELES - James Arnold Doolittle, a Los Angeles dance im-
presario who brought names such as Joffrey and Baryshnikov to local
dance stages and ensured that a high-profile ?Nutcracker Suite? was
presented here every Christmas, has died.
James Arnold Doolittle, a Los Angeles dance impresario is dead.
After working as a cashier for a British filmmaker in Rome, he joined
an amateur theatrical group at the University of Rome, where he was
taking some classes.
After working as a cashier for a British filmmaker in Rome, he joined
an amateur group at the University of Rome, where he was using some
classes.
He was a 1953 graduate of the Johns Hopkins Medical School and after
completing his residency in gynecology and surgery, traveled to Den-
mark where he joined the staff of the National Cancer Center there.
He was a graduate of the Johns Hopkins Medical School and traveled
to Denmark where he joined a member of the National Cancer Center
there.
Mastroianni, a comic but also suave and romantic leading man in some
120 motion pictures, had suffered from pancreatic cancer.
Mastroianni, a leading man in some 120 motion pictures, had subjected
to cancer.
Table 3: Examples of good (upper five) and bad (lower three) compressions generated by our abstractive compressor.
rules. An approach of this kind has been proposed
for sentence paraphrasing (Zhao et al, 2010).
The right diagram of Figure 2 shows how the per-
formance of SVR-PMI-LDA is affected when using
33% or 63% of the training ?si, ci? pairs. As more
examples are used, the performance improves, sug-
gesting that better results could be obtained by using
more training data. Finally, Table 3 shows examples
of good and bad compressions the abstractive com-
pressor produced with SVR-PMI-LDA.
6 Conclusions and future work
We presented a new dataset that can be used to train
and evaluate the ranking components of generate-
and-rank abstractive sentence compressors. The
dataset contains pairs of source sentences and can-
didate extractive or abstractive compressions. The
candidate compressions were obtained by first ap-
plying a state-of-the-art extractive compressor to the
source sentences, and then applying existing para-
phrasing rules, obtained from parallel corpora. The
dataset?s pairs have been scored by human judges
for grammaticality and meaning preservation. We
discussed how performance boundaries for ranking
components that use the dataset can be established
by using an oracle and a random baseline, and by
considering different compression rates. We also
discussed the current version of an abstractive sen-
tence compressor that we are developing, and how
the dataset was used to train and evaluate three dif-
ferent SVR-based ranking components of the com-
pressor with gradually more elaborate features sets.
The feature set of the best ranking component that
we tested includes language model scores, the con-
fidence and compression rate of the underlying ex-
tractive compressor, the number of paraphrasing
rules that have been applied, word co-occurrence
features, as well as features based on an LDA model.
In future work, we plan to improve our abstractive
sentence compressor, possibly by including more
features in the ranking component. We also plan
to investigate alternative ways to produce candidate
compressions, such as sentence paraphrasing meth-
ods that exploit multiple commercial machine trans-
lation engines to translate the source sentences to
multiple pivot languages and back to the original
language (Zhao et al, 2010). Using methods of this
kind, it may be possible to produce a second, alterna-
tive dataset with more and possibly better abstractive
candidates. We also plan to make the final version of
our abstractive compressor publicly available.
Acknowledgments
This work was partly carried out during INDIGO, an
FP6 IST project funded by the European Union, with
additional funding from the Greek General Secre-
9
tariat of Research and Technology.19
References
I. Androutsopoulos and P. Malakasiotis. 2010. A survey
of paraphrasing and textual entailment methods. Jour-
nal of Artificial Intelligence Research, 38:135?187.
C. Bannard and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Proceedings of
ACL, pages 597?604, Ann Arbor, MI.
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet
allocation. In Journal of Machine Learning Research.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Re-
evaluating the role of BLEU in machine translation
research. In Proceedings of EACL, pages 249?256,
Trento, Italy.
C. Callison-Burch. 2008. Syntactic constraints on para-
phrases extracted from parallel corpora. In Proceed-
ings of EMNLP, pages 196?205, Honolulu, HI.
J. Clarke and M. Lapata. 2006a. Constraint-based
sentence compression: An integer programming ap-
proach. In Proceedings of ACL-COLING.
J. Clarke and M. Lapata. 2006b. Models for sentence
compression: A comparison across domains, training
requirements and evaluation measures. In Proceedings
of ACL-COLING.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear programming
approach. Journal of Artificial Intelligence Research,
1(31):399?429.
T. Cohn and M. Lapata. 2007. Large margin syn-
chronous generation and its application to sentence
compression. In Proceedings of EMNLP-CONLL.
T. Cohn and M. Lapata. 2008. Sentence compression
beyond word deletion. In Proceedings of COLING.
T. Cohn and M. Lapata. 2009. Sentence compression as
tree to tree tranduction. Journal of Artificial Intelli-
gence Research, 34:637?674.
S. Corston-Oliver. 2001. Text compaction for display
on very small screens. In Proceedings of the NAACL
Workshop on Automatic Summarization.
N. Cristianini and J. Shawe-Taylor. 2000. An In-
troduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Univer-
sity Press.
D. Galanis and I. Androutsopoulos. 2010. An extrac-
tive supervised two-stage method for sentence com-
pression. In Proceedings of HLT-NAACL.
T. Griffiths and M. Steyvers. 2004. Finding scientific
topics. In Proceedings of the National Academy of Sci-
ences.
19Consult http://www.ics.forth.gr/indigo/.
H. Jing. 2000. Sentence reduction for automatic text
summarization. In Proceedings of ANLP.
T. Joachims. 2002. Learning to Classify Text Using Sup-
port Vector Machines: Methods, Theory, Algorithms.
Kluwer.
D. Kauchak and R. Barzilay. 2006. Paraphrasing for
automatic evaluation. In Proceedings of the HLT-
NAACL, pages 455?462, New York, NY.
K. Knight and D. Marcu. 2002. Summarization be-
yond sentence extraction: A probalistic approach to
sentence compression. Artificial Intelligence, 139(1).
P. Koehn. 2009. Statistical Machine Translation. Cam-
bridge University Press.
S. Kok and C. Brockett. 2010. Hitting the right para-
phrases in good time. In Proceedings of HLT-NAACL,
pages 145?153, Los Angeles, CA.
N. Madnani and B.J. Dorr. 2010. Generating phrasal and
sentential paraphrases: A survey of data-driven meth-
ods. Computational Linguistics, 36(3):341?387.
N. Madnani, D. Zajic, B. Dorr, N. F. Ayan, and J. Lin.
2007. Multiple alternative sentence compressions
for automatic text summarization. In Proceedings of
DUC.
C.D. Manning and H. Schutze. 2000. Foundations of
Statistical Natural Language Processing. MIT Press.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proceedings of
EACL.
D. Newman, J.H. Lau, K. Grieser, and T. Baldwin. 2010.
Automatic evaluation of topic coherence. In Proceed-
ings of HLT-NAACL.
T. Nomoto. 2009. A comparison of model free versus
model intensive approaches to sentence compression.
In Proceedings of EMNLP.
J. F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proceedings of ACL.
S. Pado?, M. Galley, D. Jurafsky, and C. D. Manning.
2009. Robust machine translation evaluation with en-
tailment features. In Proceedings of ACL-IJCNLP,
pages 297?305, Singapore.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of ACL, pages 311?318,
Philadelphia, PA.
P. Pecina. 2005. An extensive empirical study of colloca-
tion extraction methods. In Proceedings of the Student
Research Workshop of ACL.
S. Riezler, T.H. King, R. Crouch, and A. Zaenen.
2003. Statistical sentence condensation using ambigu-
ity packing and stochastic disambiguation methods for
lexical-functional grammar. In Proceedings of HLT-
NAACL.
10
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and
Y. Liu. 2007. Statistical machine translation for query
expansion in answer retrieval. In Proceedings of ACL,
pages 464?471, Prague, Czech Republic.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proceedings of the International Con-
ference on Spoken Language Processing, pages 901?
904.
I. Szpektor, I. Dagan, R. Bar-Haim, and J. Goldberger.
2008. Contextual preferences. In Proceedings of
ACL-HLT, pages 683?691, Columbus, OH.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2005. Support vector machine learning for indepen-
dent and structured output spaces. Machine Learning
Research, 6:1453?1484.
V. Vapnik. 1998. Statistical Learning Theory. John Wi-
ley.
E. Yamangil and S. M. Shieber. 2010. Bayesian syn-
chronous tree-substitution grammar induction and its
application to sentence compression. In Proceedings
of ACL.
S. Zhao, C. Niu, M. Zhou, T. Liu, and S. Li. 2008. Com-
bining multiple resources to improve SMT-based para-
phrasing model. In Proceedings of ACL-HLT, pages
1021?1029, Columbus, OH.
S. Zhao, X. Lan, T. Liu, and S. Li. 2009a. Application-
driven statistical paraphrase generation. In Proceed-
ings of ACL.
S. Zhao, H. Wang, T. Liu, and S. Li. 2009b. Extract-
ing paraphrase patterns from bilingual parallel cor-
pora. Natural Language Engineering, 15(4):503?526.
S. Zhao, H. Wang, X. Lan, and T. Liu. 2010. Leverag-
ing multiple MT engines for paraphrase generation. In
Proceedings of COLING.
L. Zhou, C.-Y. Lin, and Eduard Hovy. 2006. Re-
evaluating machine translation results with paraphrase
support. In Proceedings of EMNLP, pages 77?84.
11
