Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 881?888,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Prototype-Driven Grammar Induction
Aria Haghighi
Computer Science Division
University of California Berkeley
aria42@cs.berkeley.edu
Dan Klein
Computer Science Division
University of California Berkeley
klein@cs.berkeley.edu
Abstract
We investigate prototype-driven learning for pri-
marily unsupervised grammar induction. Prior
knowledge is specified declaratively, by providing a
few canonical examples of each target phrase type.
This sparse prototype information is then propa-
gated across a corpus using distributional similar-
ity features, which augment an otherwise standard
PCFG model. We show that distributional features
are effective at distinguishing bracket labels, but not
determining bracket locations. To improve the qual-
ity of the induced trees, we combine our PCFG in-
duction with the CCM model of Klein and Manning
(2002), which has complementary stengths: it iden-
tifies brackets but does not label them. Using only
a handful of prototypes, we show substantial im-
provements over naive PCFG induction for English
and Chinese grammar induction.
1 Introduction
There has been a great deal of work on unsuper-
vised grammar induction, with motivations rang-
ing from scientific interest in language acquisi-
tion to engineering interest in parser construc-
tion (Carroll and Charniak, 1992; Clark, 2001).
Recent work has successfully induced unlabeled
grammatical structure, but has not successfully
learned labeled tree structure (Klein and Manning,
2002; Klein and Manning, 2004; Smith and Eis-
ner, 2004) .
In this paper, our goal is to build a system capa-
ble of producing labeled parses in a target gram-
mar with as little total effort as possible. We in-
vestigate a prototype-driven approach to grammar
induction, in which one supplies canonical ex-
amples of each target concept. For example, we
might specify that we are interested in trees which
use the symbol NP and then list several examples
of prototypical NPs (determiner noun, pronouns,
etc., see figure 1 for a sample prototype list). This
prototype information is similar to specifying an
annotation scheme, which even human annotators
must be provided before they can begin the con-
struction of a treebank. In principle, prototype-
driven learning is just a kind of semi-supervised
learning. However, in practice, the information we
provide is on the order of dozens of total seed in-
stances, instead of a handful of fully parsed trees,
and is of a different nature.
The prototype-driven approach has three
strengths. First, since we provide a set of target
symbols, we can evaluate induced trees using
standard labeled parsing metrics, rather than the
far more forgiving unlabeled metrics described in,
for example, Klein and Manning (2004). Second,
knowledge is declaratively specified in an inter-
pretable way (see figure 1). If a user of the system
is unhappy with its systematic behavior, they can
alter it by altering the prototype information (see
section 7.1 for examples). Third, and related to
the first two, one does not confuse the ability of
the system to learn a consistent grammar with its
ability to learn the grammar a user has in mind.
In this paper, we present a series of experiments
in the induction of labeled context-free trees us-
ing a combination of unlabeled data and sparse
prototypes. We first affirm the well-known re-
sult that simple, unconstrained PCFG induction
produces grammars of poor quality as measured
against treebank structures. We then augment a
PCFGwith prototype features, and show that these
features, when propagated to non-prototype se-
quences using distributional similarity, are effec-
tive at learning bracket labels on fixed unlabeled
trees, but are still not enough to learn good tree
structures without bracketing information. Finally,
we intersect the feature-augmented PCFGwith the
CCM model of Klein and Manning (2002), a high-
quality bracketing model. The intersected model
is able to learn trees with higher unlabeled F1 than
those in Klein and Manning (2004). More impor-
881
tantly, its trees are labeled and can be evaluated
according to labeled metrics. Against the English
Penn Treebank, our final trees achieve a labeled F1
of 65.1 on short sentences, a 51.7% error reduction
over naive PCFG induction.
2 Experimental Setup
The majority of our experiments induced tree
structures from the WSJ section of the English
Penn treebank (Marcus et al, 1994), though see
section 7.4 for an experiment on Chinese. To fa-
cilitate comparison with previous work, we ex-
tracted WSJ-10, the 7,422 sentences which con-
tain 10 or fewer words after the removal of punc-
tuation and null elements according to the scheme
detailed in Klein (2005). We learned models on all
or part of this data and compared their predictions
to the manually annotated treebank trees for the
sentences on which the model was trained. As in
previous work, we begin with the part-of-speech
(POS) tag sequences for each sentence rather than
lexical sequences (Carroll and Charniak, 1992;
Klein and Manning, 2002).
Following Klein and Manning (2004), we report
unlabeled bracket precision, recall, and F1. Note
that according to their metric, brackets of size 1
are omitted from the evaluation. Unlike that work,
all of our induction methods produce trees labeled
with symbols which are identified with treebank
categories. Therefore, we also report labeled pre-
cision, recall, and F1, still ignoring brackets of
size 1.1
3 Experiments in PCFG induction
As an initial experiment, we used the inside-
outside algorithm to induce a PCFG in the
straightforward way (Lari and Young, 1990; Man-
ning and Schu?tze, 1999). For all the experiments
in this paper, we considered binary PCFGs over
the nonterminals and terminals occuring in WSJ-
10. The PCFG rules were of the following forms:
? X ? Y Z, for nonterminal types X,Y, and
Z, with Y 6= X or Z 6= X
? X ? t Y , X ? Y t, for each terminal t
? X ? t t?, for terminals t and t?
For a given sentence S, our CFG generates la-
beled trees T over S.2 Each tree consists of binary
1In cases where multiple gold labels exist in the gold trees,
precision and recall were calculated as in Collins (1999).
2Restricting our CFG to a binary branching grammar re-
sults in an upper bound of 88.1% on unlabeled F1.
productions X(i, j) ? ? over constituent spans
(i, j), where ? is a pair of non-terminal and/or
terminal symbols in the grammar. The generative
probability of a tree T for S is:
PCFG(T, S) =
?
X(i,j)???T
P (?|X)
In the inside-outside algorithm, we iteratively
compute posterior expectations over production
occurences at each training span, then use those
expectations to re-estimate production probabili-
ties. This process is guaranteed to converge to a
local extremum of the data likelihood, but initial
production probability estimates greatly influence
the final grammar (Carroll and Charniak, 1992). In
particular, uniform initial estimates are an (unsta-
ble) fixed point. The classic approach is to add a
small amount of random noise to the initial prob-
abilities in order to break the symmetry between
grammar symbols.
We randomly initialized 5 grammars using tree-
bank non-terminals and trained each to conver-
gence on the first 2000 sentences of WSJ-10.
Viterbi parses were extracted for each of these
2000 sentences according to each grammar. Of
course, the parses? symbols have nothing to anchor
them to our intended treebank symbols. That is, an
NP in one of these grammars may correspond to
the target symbol VP, or may not correspond well
to any target symbol. To evaluate these learned
grammars, we must map the models? phrase types
to target phrase types. For each grammar, we fol-
lowed the common approach of greedily mapping
model symbols to target symbols in the way which
maximizes the labeled F1. Note that this can, and
does, result in mapping multiple model symbols
to the most frequent target symbols. This experi-
ment, labeled PCFG? NONE in figure 4, resulted in
an average labeled F1 of 26.3 and an unlabeled F1
of 45.7. The unlabeled F1 is better than randomly
choosing a tree (34.7), but not better than always
choosing a right branching structure (61.7).
Klein and Manning (2002) suggest that the task
of labeling constituents is significantly easier than
identifying them. Perhaps it is too much to ask
a PCFG induction algorithm to perform both of
these tasks simultaneously. Along the lines of
Pereira and Schabes (1992), we reran the inside-
outside algorithm, but this time placed zero mass
on all trees which did not respect the bracketing
of the gold trees. This constraint does not fully
882
Phrase Prototypes Phrase Prototypes
NP DT NN VP VBN IN NN
JJ NNS VBD DT NN
NNP NNP MD VB CD
S PRP VBD DT NN QP CD CD
DT NN VBD IN DT NN RB CD
DT VBZ DT JJ NN DT CD CD
PP IN NN ADJP RB JJ
TO CD CD JJ
IN PRP JJ CC JJ
ADVP RB RB
RB CD
RB CC RB
VP-INF VB NN NP-INF NN POS
Figure 1: English phrase type prototype list man-
ually specified (The entire supervision for our sys-
tem). The second part of the table is additional
prototypes discussed in section 7.1.
eliminate the structural uncertainty since we are
inducing binary trees and the gold trees are flat-
ter than binary in many cases. This approach of
course achieved the upper bound on unlabeled F1,
because of the gold bracket constraints. However,
it only resulted in an average labeled F1 of 52.6
(experiment PCFG ? GOLD in figure 4). While this
labeled score is an improvement over the PCFG ?
NONE experiment, it is still relatively disappoint-
ing.
3.1 Encoding Prior Knowledge with
Prototypes
Clearly, we need to do something more than
adding structural bias (e.g. bracketing informa-
tion) if we are to learn a PCFG in which the sym-
bols have the meaning and behaviour we intend.
How might we encode information about our prior
knowledge or intentions?
Providing labeled trees is clearly an option. This
approach tells the learner how symbols should re-
cursively relate to each other. Another option is to
provide fully linearized yields as prototypes. We
take this approach here, manually creating a list
of POS sequences typical of the 7 most frequent
categories in the Penn Treebank (see figure 1).3
Our grammar is limited to these 7 phrase types
plus an additional type which has no prototypes
and is unconstrained.4 This list grounds each sym-
3A possible objection to this approach is the introduction
of improper reasearcher bias via specifying prototypes. See
section 7.3 for an experiment utilizing an automatically gen-
erated prototype list with comparable results.
4In our experiments we found that adding prototypes for
more categories did not improve performance and took more
bol in terms of an observable portion of the data,
rather than attempting to relate unknown symbols
to other unknown symbols.
Broadly, we would like to learn a grammar
which explains the observed data (EM?s objec-
tive) but also meets our prior expectations or re-
quirements of the target grammar. How might
we use such a list to constrain the learning of
a PCFG with the inside-outside algorithm? We
might require that all occurences of a prototype
sequence, say DT NN, be constituents of the cor-
responding type (NP). However, human-elicited
prototypes are not likely to have the property that,
when they occur, they are (nearly) always con-
stituents. For example, DT NN is a perfectly rea-
sonable example of a noun phrase, but is not a con-
stituent when it is part of a longer DT NN NN con-
stituent. Therefore, when summing over trees with
the inside-outside algorithm, we could require a
weaker property: whenever a prototype sequence
is a constituent it must be given the label specified
in the prototype file.5 This constraint is enough to
break the symmetry between the model labels, and
therefore requires neither random initialization for
training, nor post-hoc mapping of labels for eval-
uation. Adding prototypes in this way and keep-
ing the gold bracket constraint gave 59.9 labeled
F1. The labeled F1 measure is again an improve-
ment over naive PCFG induction, but is perhaps
less than we might expect given that the model has
been given bracketing information and has proto-
types as a form of supervision to direct it.
In response to a prototype, however, we may
wish to conclude something stronger than a con-
straint on that particular POS sequence. We might
hope that sequences which are similar to a proto-
type in some sense are generally given the same
label as that prototype. For example, DT NN is a
noun phrase prototype, the sequence DT JJ NN is
another good candidate for being a noun phrase.
This kind of propagation of constraints requires
that we have a good way of defining and detect-
ing similarity between POS sequences.
3.2 Phrasal Distributional Similarity
A central linguistic argument for constituent types
is substitutability: phrases of the same type appear
time. We note that we still evaluate against all phrase types
regardless of whether or not they are modeled by our gram-
mar.
5Even this property is likely too strong: prototypes may
have multiple possible labels, for example DT NN may also
be a QP in the English treebank.
883
Yield Prototype Skew KL Phrase Type Skew KL
DT JJ NN DT NN 0.10 NP 0.39
IN DT VBG NN IN NN 0.24 PP 0.45
DT NN MD VB DT NNS PRP VBD DT NN 0.54 S 0.58
CC NN IN NN 0.43 PP 0.71
MD NNS PRP VBD DT NN 1.43 NONE -
Figure 2: Yields along with most similar proto-
types and phrase types, guessed according to (3).
in similar contexts and are mutually substitutable
(Harris, 1954; Radford, 1988). For instance, DT
JJ NN and DT NN occur in similar contexts, and
are indeed both common NPs. This idea has been
repeatedly and successfully operationalized using
various kinds of distributional clustering, where
we define a similarity measure between two items
on the basis of their immediate left and right con-
texts (Schu?tze, 1995; Clark, 2000; Klein and Man-
ning, 2002).
As in Clark (2001), we characterize the distribu-
tion of a sequence by the distribution of POS tags
occurring to the left and right of that sequence in
a corpus. Each occurence of a POS sequence ?
falls in a context x ? y, where x and y are the ad-
jacent tags. The distribution over contexts x ? y
for a given ? is called its signature, and is denoted
by ?(?). Note that ?(?) is composed of context
counts from all occurences, constitiuent and dis-
tituent, of ?. Let ?c(?) denote the context dis-
tribution for ? where the context counts are taken
only from constituent occurences of ?. For each
phrase type in our grammar,X , define ?c(X) to be
the context distribution obtained from the counts
of all constituent occurences of type X:
?c(X) = Ep(?|X) ?c(?) (1)
where p(?|X) is the distribution of yield types for
phrase type X . We compare context distributions
using the skewed KL divergence:
DSKL(p, q) = DKL(p??p + (1? ?)q)
where ? controls how much of the source distribu-
tions is mixed in with the target distribution.
A reasonable baseline rule for classifying the
phrase type of a POS yield is to assign it to the
phrase from which it has minimal divergence:
type(?) = argmin
X
DSKL(?c(?), ?c(X)) (2)
However, this rule is not always accurate, and,
moreover, we do not have access to ?c(?) or
?c(X). We chose to approximate ?c(X) us-
ing the prototype yields for X as samples from
p(?|X). Letting proto(X) denote the (few) pro-
totype yields for phrase type X , we define ??(X):
??(X) =
1
|proto(X)|
?
??proto(X)
?(?)
Note ??(X) is an approximation to (1) in sev-
eral ways. We have replaced an expectation over
p(?|X) with a uniform weighting of proto(X),
and we have replaced ?c(?) with ?(?) for each
term in that expectation. Because of this, we will
rely only on high confidence guesses, and allow
yields to be given a NONE type if their divergence
from each ??(X) exceeds a fixed threshold t. This
gives the following alternative to (2):
type(?) = (3)
{
NONE, if minX DSKL(?(?), ??(X)) < t
argminX DSKL(?(?), ??(X)), otherwise
We built a distributional model implementing
the rule in (3) by constructing ?(?) from context
counts in the WSJ portion of the Penn Treebank
as well as the BLIPP corpus. Each ??(X) was ap-
proximated by a uniform mixture of ?(?) for each
of X?s prototypes ? listed in figure 1.
This method of classifying constituents is very
precise if the threshold is chosen conservatively
enough. For instance, using a threshold of t =
0.75 and ? = 0.1, this rule correctly classifies the
majority label of a constituent-type with 83% pre-
cision, and has a recall of 23% over constituent
types. Figure 2 illustrates some sample yields, the
prototype sequence to which it is least divergent,
and the output of rule (3).
We incorporated this distributional information
into our PCFG induction scheme by adding a pro-
totype feature over each span (i, j) indicating the
output of (3) for the yield ? in that span. Asso-
ciated with each sentence S is a feature map F
specifying, for each (i, j), a prototype feature pij .
These features are generated using an augmented
CFG model, CFG+, given by:6
PCFG+(T, F ) =
?
X(i,j)???T
P (pij |X)P (?|X)
=
?
X(i,j)???T
?CFG+(X ? ?, pij)
6Technically, all features in F must be generated for each
assignment to T , which means that there should be terms in
this equation for the prototype features on distituent spans.
However, we fixed the prototype distribution to be uniform
for distituent spans so that the equation is correct up to a con-
stant depending on F .
884
P (S|ROOT) ? ROOT
S
? P (NP VP|S)
P (P = NONE|S)XXXXX
P (NN NNS|NP)
P (P = NP|NP)
ff
NP
 HHH
NNN
payrolls
NN
Factory
VP
? P (VBD PP|VP)
P (P = VP|VP)aaa
!!!
VBD
fell
PP
? P (IN NN|PP)
P (P = PP|PP)!!! aaa
NN
November
IN
in
Figure 3: Illustration of PCFG augmented with
prototype similarity features.
where ?CFG+(X ? ?, pij) is the local factor for
placing X ? ? on a span with prototype feature
pij . An example is given in figure 3.
For our experiments, we fixed P (pij |X) to be:
P (pij |X) =
{
0.60, if pij = X
uniform, otherwise
Modifying the model in this way, and keeping the
gold bracketing information, gave 71.1 labeled F1
(see experiment PROTO ? GOLD in figure 4), a
40.3% error reduction over naive PCFG induction
in the presence of gold bracketing information.
We note that the our labeled F1 is upper-bounded
by 86.0 due to unary chains and more-than-binary
configurations in the treebank that cannot be ob-
tained from our binary grammar.
We conclude that in the presence of gold bracket
information, we can achieve high labeled accu-
racy by using a CFG augmented with distribu-
tional prototype features.
4 Constituent Context Model
So far, we have shown that, given perfect per-
fect bracketing information, distributional proto-
type features allow us to learn tree structures with
fairly accurate labels. However, such bracketing
information is not available in the unsupervised
case.
Perhaps we don?t actually need bracketing con-
straints in the presence of prototypes and distri-
butional similarity features. However this exper-
iment, labeled PROTO ? NONE in figure 4, gave
only 53.1 labeled F1 (61.1 unlabeled), suggesting
that some amount of bracketing constraint is nec-
essary to achieve high performance.
Fortunately, there are unsupervised systems
which can induce unlabeled bracketings with rea-
sonably high accuracy. One such model is
the constituent-context model (CCM) of Klein
and Manning (2002), a generative distributional
model. For a given sentence S, the CCM generates
a bracket matrix, B, which for each span (i, j), in-
dicates whether or not it is a constituent (Bij = c)
or a distituent (Bij = d). In addition, it generates
a feature map F ?, which for each span (i, j) in S
specifies a pair of features, F ?ij = (yij , cij), where
yij is the POS yield of the span, and cij is the con-
text of the span, i.e identity of the conjoined left
and right POS tags:
PCCM (B,F
?) = P (B)
?
(i,j)
P (yij |Bij)P (cij |Bij)
The distribution P (B) only places mass on brack-
etings which correspond to binary trees. We
can efficiently compute PCCM (B,F ?) (up to
a constant) depending on F ? using local fac-
tors ?CCM (yij , cij) which decomposes over con-
stituent spans:7
PCCM (B,F
?) ?
?
(i,j):Bij=c
P (yij |c)P (cij |c)
P (yij |d)P (cij |d)
=
?
(i,j):Bij=c
?CCM (yij , cij)
The CCM by itself yields an unlabeled F1 of 71.9
on WSJ-10, which is reasonably high, but does not
produce labeled trees.
5 Intersecting CCM and PCFG
The CCM and PCFG models provide complemen-
tary views of syntactic structure. The CCM explic-
itly learns the non-recursive contextual and yield
properties of constituents and distituents. The
PCFG model, on the other hand, does not explic-
itly model properties of distituents but instead fo-
cuses on modeling the hierarchical and recursive
properties of natural language syntax. One would
hope that modeling both of these aspects simulta-
neously would improve the overall quality of our
induced grammar.
We therefore combine the CCM with our feature-
augmented PCFG, denoted by PROTO in exper-
iment names. When we run EM on either of
the models alone, at each iteration and for each
training example, we calculate posteriors over that
7Klein (2005) gives a full presentation.
885
model?s latent variables. For CCM, the latent vari-
able is a bracketing matrix B (equivalent to an un-
labeled binary tree), while for the CFG+ the latent
variable is a labeled tree T . While these latent
variables aren?t exactly the same, there is a close
relationship between them. A bracketing matrix
constrains possible labeled trees, and a given la-
beled tree determines a bracketing matrix. One
way to combine these models is to encourage both
models to prefer latent variables which are com-
patible with each other.
Similar to the approach of Klein and Manning
(2004) on a different model pair, we intersect CCM
and CFG+ by multiplying their scores for any la-
beled tree. For each possible labeled tree over a
sentence S, our generative model for a labeled tree
T is given as follows:
P (T, F, F ?) = (4)
PCFG+(T, F )PCCM (B(T ), F
?)
where B(T ) corresponds to the bracketing ma-
trix determined by T . The EM algorithm for the
product model will maximize:
P (S,F, F ?) =
?
T?T (S)
PCCM (B,F
?)PCFG+(T, F )
=
?
B
PCCM (B,F
?)
?
T?T (B,S)
PCFG+(T, F )
where T (S) is the set of labeled trees consistent
with the sentence S and T (B,S) is the set of la-
beled trees consistent with the bracketing matrix
B and the sentence S. Notice that this quantity in-
creases as the CCM and CFG+ models place proba-
bility mass on compatible latent structures, giving
an intuitive justification for the success of this ap-
proach.
We can compute posterior expectations over
(B, T ) in the combined model (4) using a variant
of the inside-outside algorithm. The local factor
for a binary rule r = X ? Y Z, over span (i, j),
with CCM features F ?ij = (yij , cij) and prototype
feature pij , is given by the product of local factors
for the CCM and CFG+ models:
?(r, (i, j)) = ?CCM (yij , cij)?CFG+(r, pij)
From these local factors, the inside-outside al-
gorithm produces expected counts for each binary
rule, r, over each span (i, j) and split point k, de-
noted by P (r, (i, j), k|S, F, F ?). These posteriors
are sufficient to re-estimate all of our model pa-
rameters.
Labeled Unlabeled
Setting Prec. Rec. F1 Prec. Rec. F1
No Brackets
PCFG ? NONE 23.9 29.1 26.3 40.7 52.1 45.7
PROTO ? NONE 51.8 62.9 56.8 59.6 76.2 66.9
Gold Brackets
PCFG ? GOLD 47.0 57.2 51.6 78.8 100.0 88.1
PROTO ? GOLD 64.8 78.7 71.1 78.8 100.0 88.1
CCM Brackets
CCM - - - 64.2 81.6 71.9
PCFG ? CCM 32.3 38.9 35.3 64.1 81.4 71.8
PROTO ? CCM 56.9 68.5 62.2 68.4 86.9 76.5
BEST 59.4 72.1 65.1 69.7 89.1 78.2
UBOUND 78.8 94.7 86.0 78.8 100.0 88.1
Figure 4: English grammar induction results. The
upper bound on labeled recall is due to unary
chains.
6 CCM as a Bracketer
We tested the product model described in sec-
tion 5 on WSJ-10 under the same conditions as
in section 3. Our initial experiment utilizes no
protoype information, random initialization, and
greedy remapping of its labels. This experiment,
PCFG ? CCM in figure 4, gave 35.3 labeled F1,
compared to the 51.6 labeled F1 with gold brack-
eting information (PCFG ? GOLD in figure 4).
Next we added the manually specified proto-
types in figure 1, and constrained the model to give
these yields their labels if chosen as constituents.
This experiment gave 48.9 labeled F1 (73.3 unla-
beled). The error reduction is 21.0% labeled (5.3%
unlabeled) over PCFG ? CCM.
We then experimented with adding distributional
prototype features as discussed in section 3.2 us-
ing a threshold of 0.75 and ? = 0.1. This experi-
ment, PROTO ? CCM in figure 4, gave 62.2 labeled
F1 (76.5 unlabeled). The error reduction is 26.0%
labeled (12.0% unlabeled) over the experiment us-
ing prototypes without the similarity features. The
overall error reduction from PCFG? CCM is 41.6%
(16.7%) in labeled (unlabeled) F1.
7 Error Analysis
The most common type of error by our PROTO ?
CCM system was due to the binary grammar re-
striction. For instance common NPs, such as DT JJ
NN, analyzed as [NP DT [NP JJ NN] ], which pro-
poses additional N constituents compared to the
flatter treebank analysis. This discrepancy greatly,
and perhaps unfairly, damages NP precision (see
figure 6). However, this is error is unavoidable
886
SXXXXXNP
NNP
France
VPXXXXXMD
can
VPhhhhhhh(((((((VB
boast
NPXXXXXNPaaa!!!NPaaa!!!DT
the
NN
lion
POS
?s
NN
share
PPPPPPIN
of
NPHHHJJ
high-priced
NNS
bottles
Shhhhhhhhh(((((((((NNP
France
VPhhhhhhhh((((((((VPXXXXXVPZZMD
can
VB
boast
NPaaa!!!NPll,,DT
the
NN
lion
PPZZPOS
?s
NN
share
PPPPPPIN
of
NPHHHJJ
high-priced
NNS
bottles
Shhhhhhhh((((((((NNP
France
VPhhhhhhhhh(((((((((VPPPPPMD
can
VPXXXXXVB
boast
NPPPPPNPbb""DT
the
NPcc##NN
lion
POS
?s
NN
share
PPPPPPIN
of
NPHHHJJ
high-priced
NNS
bottles
a) b) c)
Figure 5: Examples of corrections from adding VP-INF and NP-POS prototype categories. The tree in (a)
is the Treebank parse, (b) is the parse with PROTO ? CCM model, and c) is the parse with the BEST model
(added prototype categories), which fixes the possesive NP and infinitival VP problems, but not the PP
attachment.
given our grammar restriction.
Figure 5(b) demonstrates three other errors. Pos-
sessive NPs are analyzed as [NP NN [PP POS NN ]
], with the POS element treated as a preposition
and the possessed NP as its complement. While
labeling the POS NN as a PP is clearly incorrect,
placing a constituent over these elements is not
unreasonable and in fact has been proposed by
some linguists (Abney, 1987). Another type of
error also reported by Klein and Manning (2002)
is MD VB groupings in infinitival VPs also some-
times argued by linguists (Halliday, 2004). More
seriously, prepositional phrases are almost always
attached ?high? to the verb for longer NPs.
7.1 Augmenting Prototypes
One of the advantages of the prototype driven ap-
proach, over a fully unsupervised approach, is the
ability to refine or add to the annotation specifica-
tion if we are not happy with the output of our sys-
tem. We demonstrate this flexibility by augment-
ing the prototypes in figure 1 with two new cate-
gories NP-POS and VP-INF, meant to model pos-
sessive noun phrases and infinitival verb phrases,
which tend to have slightly different distributional
properties from normal NPs and VPs. These new
sub-categories are used during training and then
stripped in post-processing. This prototype list
gave 65.1 labeled F1 (78.2 unlabeled). This exper-
iment is labeled BEST in figure 4. Looking at the
CFG-learned rules in figure 7, we see that the basic
structure of the treebank grammar is captured.
7.2 Parsing with only the PCFG
In order to judge how well the PCFG component
of our model did in isolation, we experimented
with training our BEST model with the CCM com-
ponent, but dropping it at test time. This experi-
Label Prec. Rec. F1
S 79.3 80.0 79.7
NP 49.0 74.4 59.1
VP 80.4 73.3 76.7
PP 45.6 78.6 57.8
QP 36.2 78.8 49.6
ADJP 29.4 33.3 31.2
ADVP 25.0 12.2 16.4
Figure 6: Precision, recall, and F1 for individual
phrase types in the BEST model
Rule Probability Rule Probability
S ? NP VP 0.51 VP ? VBZ NP 0.20
S ? PRP VP 0.13 VP ? VBD NP 0.15
S ? NNP VP 0.06 VP ? VBP NP 0.09
S ? NNS VP 0.05 VP ? VB NP 0.08
NP ? DT NN 0.12 ROOT ? S 0.95
NP ? NP PP 0.09 ROOT ? NP 0.05
NP ? NNP NNP 0.09
NP ? JJ NN 0.07
PP ? IN NP 0.37 QP ? CD CD 0.35
PP ? CC NP 0.06 QP ? CD NN 0.30
PP ? TO VP 0.05 QP ? QP PP 0.10
PP ? TO QP 0.04 QP ? QP NNS 0.05
ADJP ? RB VBN 0.37 ADVP ? RB RB 0.25
ADJP ? RB JJ 0.31 ADVP ? ADJP PRP 0.15
ADJP ? RBR JJ 0.09 ADVP ? RB CD 0.10
Figure 7: Top PCFG Rules learned by BEST model
ment gave 65.1 labeled F1 (76.8 unlabeled). This
demonstrates that while our PCFG performance
degrades without the CCM, it can be used on its
own with reasonable accuracy.
7.3 Automatically Generated Prototypes
There are two types of bias which enter into the
creation of prototypes lists. One of them is the
bias to choose examples which reflect the annota-
tion semantics we wish our model to have. The
second is the iterative change of prototypes in or-
der to maximize F1. Whereas the first is appro-
887
priate, indeed the point, the latter is not. In or-
der to guard against the second type of bias, we
experimented with automatically extracted gener-
ated prototype lists which would not be possible
without labeled data. For each phrase type cat-
egory, we extracted the three most common yield
associated with that category that differed in either
first or last POS tag. Repeating our PROTO ? CCM
experiment with this list yielded 60.9 labeled F1
(76.5 unlabeled), comparable to the performance
of our manual prototype list.
7.4 Chinese Grammar Induction
In order to demonstrate that our system is some-
what language independent, we tested our model
on CTB-10, the 2,437 sentences of the Chinese
Treebank (Ircs, 2002) of length at most 10 af-
ter punctuation is stripped. Since the authors
have no expertise in Chinese, we automatically ex-
tracted prototypes in the same way described in
section 7.3. Since we did not have access to a large
auxiliary POS tagged Chinese corpus, our distri-
butional model was built only from the treebank
text, and the distributional similarities are presum-
ably degraded relative to the English. Our PCFG
? CCM experiment gave 18.0 labeled F1 (43.4 un-
labeled). The PROTO ? CCM model gave 39.0 la-
beled F1 (53.2 unlabeled). Presumably with ac-
cess to more POS tagged data, and the expertise of
a Chinese speaker, our system would see increased
performance. It is worth noting that our unlabeled
F1 of 53.2 is the best reported from a primarily
unsupervised system, with the next highest figure
being 46.7 reported by Klein and Manning (2004).
8 Conclusion
We have shown that distributional prototype fea-
tures can allow one to specify a target labeling
scheme in a compact and declarative way. These
features give substantial error reduction in labeled
F1 measure for English and Chinese grammar in-
duction. They also achieve the best reported un-
labeled F1 measure.8 Another positive property
of this approach is that it tries to reconcile the
success of distributional clustering approaches to
grammar induction (Clark, 2001; Klein and Man-
ning, 2002), with the CFG tree models in the su-
pervised literature (Collins, 1999). Most impor-
tantly, this is the first work, to the authors? knowl-
8The next highest results being 77.1 and 46.7 for English
and Chinese respectively from Klein and Manning (2004).
edge, which has learned CFGs in an unsupervised
or semi-supervised setting and can parse natural
language language text with any reasonable accu-
racy.
Acknowledgments We would like to thank the
anonymous reviewers for their comments. This
work is supported by a Microsoft / CITRIS grant
and by an equipment donation from Intel.
References
Stephen P. Abney. 1987. The English Noun Phrase in its
Sentential Aspect. Ph.D. thesis, MIT.
Glenn Carroll and Eugene Charniak. 1992. Two experiments
on learning probabilistic dependency grammars from cor-
pora. Technical Report CS-92-16.
Alexander Clark. 2000. Inducing syntactic categories by con-
text distribution clustering. In CoNLL, pages 91?94, Lis-
bon, Portugal.
Alexander Clark. 2001. The unsupervised induction of
stochastic context-free grammars using distributional clus-
tering. In CoNLL.
Michael Collins. 1999. The Unsupervised learning of Natural
Language Structure. Ph.D. thesis, University of Rochester.
M.A.K Halliday. 2004. An introduction to functional gram-
mar. Edward Arnold, 2nd edition.
Zellig Harris. 1954. Distributional Structure. University of
Chicago Press, Chicago.
Nianwen Xue Ircs. 2002. Building a large-scale annotated
chinese corpus.
Dan Klein and Christopher Manning. 2002. A generative
constituent-context model for improved grammar induc-
tion. In ACL.
Dan Klein and Christopher Manning. 2004. Corpus-based
induction of syntactic structure: Models of dependency and
constituency. In ACL.
Dan Klein. 2005. The unsupervised learning of Natural Lan-
guage Structure. Ph.D. thesis, Stanford University.
Karim Lari and Steve Young. 1990. The estimation of
stochastic context-free grammars using the insideoutside
algorithm. Computer Speech and Language, 2(4):35?56.
Christopher D. Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing. The
MIT Press.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated corpus
of english: The penn treebank. Computational Linguistics,
19(2):313?330.
Fernando C. N. Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed corpora. In
Meeting of the Association for Computational Linguistics,
pages 128?135.
Andrew Radford. 1988. Transformational Grammar. Cam-
bridge University Press, Cambridge.
Hinrich Schu?tze. 1995. Distributional part-of-speech tagging.
In EACL.
Noah A. Smith and Jason Eisner. 2004. Guiding unsuper-
vised grammar induction using contrastive estimation. In
Working notes of the IJCAI workshop on Grammatical In-
ference Applications.
888
