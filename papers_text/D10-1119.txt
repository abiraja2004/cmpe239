Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1223?1233,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Inducing Probabilistic CCG Grammars from Logical Form
with Higher-Order Unification
Tom Kwiatkowski?
t.m.kwiatkowksi@sms.ed.ac.uk
Luke Zettlemoyer?
lsz@cs.washington.edu
Sharon Goldwater?
sgwater@inf.ed.ac.uk
Mark Steedman?
steedman@inf.ed.ac.uk
?School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?Computer Science & Engineering
University of Washington
Seattle, WA 98195
Abstract
This paper addresses the problem of learn-
ing to map sentences to logical form, given
training data consisting of natural language
sentences paired with logical representations
of their meaning. Previous approaches have
been designed for particular natural languages
or specific meaning representations; here we
present a more general method. The approach
induces a probabilistic CCG grammar that
represents the meaning of individual words
and defines how these meanings can be com-
bined to analyze complete sentences. We
use higher-order unification to define a hy-
pothesis space containing all grammars con-
sistent with the training data, and develop
an online learning algorithm that efficiently
searches this space while simultaneously es-
timating the parameters of a log-linear parsing
model. Experiments demonstrate high accu-
racy on benchmark data sets in four languages
with two different meaning representations.
1 Introduction
A key aim in natural language processing is to learn
a mapping from natural language sentences to for-
mal representations of their meaning. Recent work
has addressed this problem by learning semantic
parsers given sentences paired with logical meaning
representations (Thompson & Mooney, 2002; Kate
et al, 2005; Kate & Mooney, 2006; Wong &
Mooney, 2006, 2007; Zettlemoyer & Collins, 2005,
2007; Lu et al, 2008). For example, the training
data might consist of English sentences paired with
lambda-calculus meaning representations:
Sentence: which states border texas
Meaning: ?x.state(x) ? next to(x, tex)
Given pairs like this, the goal is to learn to map new,
unseen, sentences to their corresponding meaning.
Previous approaches to this problem have been
tailored to specific natural languages, specific mean-
ing representations, or both. Here, we develop an
approach that can learn to map any natural language
to a wide variety of logical representations of lin-
guistic meaning. In addition to data like the above,
this approach can also learn from examples such as:
Sentence: hangi eyaletin texas ye siniri vardir
Meaning: answer(state(borders(tex)))
where the sentence is in Turkish and the meaning
representation is a variable-free logical expression
of the type that has been used in recent work (Kate
et al, 2005; Kate & Mooney, 2006; Wong &
Mooney, 2006; Lu et al, 2008).
The reason for generalizing to multiple languages
is obvious. The need to learn over multiple repre-
sentations arises from the fact that there is no stan-
dard representation for logical form for natural lan-
guage. Instead, existing representations are ad hoc,
tailored to the application of interest. For example,
the variable-free representation above was designed
for building natural language interfaces to databases.
Our approach works by inducing a combinatory
categorial grammar (CCG) (Steedman, 1996, 2000).
A CCG grammar consists of a language-specific
lexicon, whose entries pair individual words and
phrases with both syntactic and semantic informa-
tion, and a universal set of combinatory rules that
1223
project that lexicon onto the sentences and meanings
of the language via syntactic derivations. The learn-
ing process starts by postulating, for each sentence
in the training data, a single multi-word lexical item
pairing that sentence with its complete logical form.
These entries are iteratively refined with a restricted
higher-order unification procedure (Huet, 1975) that
defines all possible ways to subdivide them, consis-
tent with the requirement that each training sentence
can still be parsed to yield its labeled meaning.
For the data sets we consider, the space of pos-
sible grammars is too large to explicitly enumerate.
The induced grammar is also typically highly am-
biguous, producing a large number of possible anal-
yses for each sentence. Our approach discriminates
between analyses using a log-linear CCG parsing
model, similar to those used in previous work (Clark
& Curran, 2003, 2007), but differing in that the syn-
tactic parses are treated as a hidden variable during
training, following the approach of Zettlemoyer &
Collins (2005, 2007). We present an algorithm that
incrementally learns the parameters of this model
while simultaneously exploring the space of possi-
ble grammars. The model is used to guide the pro-
cess of grammar refinement during training as well
as providing a metric for selecting the best analysis
for each new sentence.
We evaluate the approach on benchmark datasets
from a natural language interface to a database of
US Geography (Zelle & Mooney, 1996). We show
that accurate models can be learned for multiple
languages with both the variable-free and lambda-
calculus meaning representations introduced above.
We also compare performance to previous methods
(Kate & Mooney, 2006; Wong & Mooney, 2006,
2007; Zettlemoyer & Collins, 2005, 2007; Lu et al,
2008), which are designed with either language- or
representation- specific constraints that limit gener-
alization, as discussed in more detail in Section 6.
Despite being the only approach that is general
enough to run on all of the data sets, our algorithm
achieves similar performance to the others, even out-
performing them in several cases.
2 Overview of the Approach
The goal of our algorithm is to find a function
f : x ? z that maps sentences x to logical ex-
pressions z. We learn this function by inducing a
probabilistic CCG (PCCG) grammar from a train-
ing set {(xi, zi)|i = 1 . . . n} containing example
(sentence, logical-form) pairs such as (?New York
borders Vermont?, next to(ny, vt)). The induced
grammar consists of two components which the al-
gorithm must learn:
? A CCG lexicon, ?, containing lexical items
that define the space of possible parses y for
an input sentence x. Each parse contains both
syntactic and semantic information, and defines
the output logical form z.
? A parameter vector, ?, that defines a distribu-
tion over the possible parses y, conditioned on
the sentence x.
We will present the approach in two parts. The
lexical induction process (Section 4) uses a re-
stricted form of higher order unification along with
the CCG combinatory rules to propose new entries
for ?. The complete learning algorithm (Section 5)
integrates this lexical induction with a parameter es-
timation scheme that learns ?. Before presenting the
details, we first review necessary background.
3 Background
This section provides an introduction to the ways in
which we will use lambda calculus and higher-order
unification to construct meaning representations. It
also reviews the CCG grammar formalism and prob-
abilistic extensions to it, including existing parsing
and parameter estimation techniques.
3.1 Lambda Calculus and Higher-Order
Unification
We assume that sentence meanings are represented
as logical expressions, which we will construct from
the meaning of individual words by using the op-
erations defined in the lambda calculus. We use a
version of the typed lambda calculus (cf. Carpenter
(1997)), in which the basic types include e, for en-
tities; t, for truth values; and i for numbers. There
are also function types of the form ?e, t? that are as-
signed to lambda expressions, such as ?x.state(x),
which take entities and return truth values. We
represent the meaning of words and phrases using
1224
lambda-calculus expressions that can contain con-
stants, quantifiers, logical connectors, and lambda
abstractions.
The advantage of using the lambda calculus
lies in its generality. The meanings of individ-
ual words and phrases can be arbitrary lambda ex-
pressions, while the final meaning for a sentence
can take different forms. It can be a full lambda-
calculus expression, a variable-free expression such
as answer(state(borders(tex))), or any other log-
ical expression that can be built from the primitive
meanings via function application and composition.
The higher-order unification problem (Huet,
1975) involves finding a substitution for the free
variables in a pair of lambda-calculus expressions
that, when applied, makes the expressions equal
each other. This problem is notoriously complex;
in the unrestricted form (Huet, 1973), it is undecid-
able. In this paper, we will guide the grammar in-
duction process using a restricted version of higher-
order unification that is tractable. For a given ex-
pression h, we will need to find expressions for f
and g such that either h = f(g) or h = ?x.f(g(x)).
This limited form of the unification problem will al-
low us to define the ways to split h into subparts
that can be recombined with CCG parsing opera-
tions, which we will define in the next section, to
reconstruct h.
3.2 Combinatory Categorial Grammar
CCG (Steedman, 2000) is a linguistic formalism
that tightly couples syntax and semantics, and
can be used to model a wide range of language
phenomena. For present purposes a CCG grammar
includes a lexicon ? with entries like the following:
New York ` NP : ny
borders ` S\NP/NP : ?x?y.next to(y, x)
Vermont ` NP : vt
where each lexical item w`X : h has words w, a
syntactic categoryX , and a logical form h expressed
as a lambda-calculus expression. For the first exam-
ple, these are ?New York,? NP , and ny. CCG syn-
tactic categories may be atomic (such as S, NP ) or
complex (such as S\NP/NP ).
CCG combines categories using a set of com-
binatory rules. For example, the forward (>) and
backward (<) application rules are:
X/Y : f Y : g ? X : f(g) (>)
Y : g X\Y : f ? X : f(g) (<)
These rules apply to build syntactic and semantic
derivations under the control of the word order infor-
mation encoded in the slash directions of the lexical
entries. For example, given the lexicon above, the
sentence New York borders Vermont can be parsed
to produce:
New York borders Vermont
NP (S\NP )/NP NP
ny ?x?y.next to(y, x) vt
>
(S\NP )
?y.next to(y, vt)
<
S
next to(ny, vt)
where each step in the parse is labeled with the com-
binatory rule (? > or ? <) that was used.
CCG also includes combinatory rules of forward
(> B) and backward (< B) composition:
X/Y : f Y/Z : g ?X/Z : ?x.f(g(x)) (> B)
Y \Z : g X\Y : f ?X\Z : ?x.f(g(x)) (< B)
These rules provide for a relaxed notion of con-
stituency which will be useful during learning as we
reason about possible refinements of the grammar.
We also allow vertical slashes in CCG categories,
which act as wild cards. For example, with this
extension the forward application combinator (>)
could be used to combine the category S/(S|NP )
with any of S\NP , S/NP , or S|NP . Figure 1
shows two parses where the composition combina-
tors and vertical slashes are used. These parses
closely resemble the types of analyses that will be
possible under the grammars we learn in the experi-
ments described in Section 8.
3.3 Probabilistic CCGs
Given a CCG lexicon ?, there will, in general, be
many possible parses for each sentence. We select
the most likely alternative using a log-linear model,
which consists of a feature vector ? and a parame-
ter vector ?. The joint probability of a logical form
z constructed with a parse y, given a sentence x is
1225
hangi eyaletin texas ye siniri vardir
S/NP NP/NP NP NP\NP
?x.answer(x) ?x.state(x) tex ?x.border(x)
<
NP
border(tex)
>
NP
state(border(tex))
>
S
answer(state(border(tex)))
what states border texas
S/(S|NP ) S|NP/(S|NP ) S\NP/NP NP
?f?x.f(x) ?f?x.state(x)?f(x) ?y?x.next to(x, y) tex
>B
S|NP/NP
?y?x.state(x) ? next to(x, y)
>
S|NP
?x.state(x) ? next to(x, tex)
>
S
?x.state(x) ? next to(x, tex)
Figure 1: Two examples of CCG parses with different logical form representations.
defined as:
P (y, z|x; ?,?) =
e???(x,y,z)
?
(y?,z?) e
???(x,y?,z?)
(1)
Section 7 defines the features used in the experi-
ments, which include, for example, lexical features
that indicate when specific lexical items in ? are
used in the parse y. For parsing and parameter es-
timation, we use standard algorithms (Clark & Cur-
ran, 2007), as described below.
The parsing, or inference, problem is to find the
most likely logical form z given a sentence x, as-
suming the parameters ? and lexicon ? are known:
f(x) = arg max
z
p(z|x; ?,?) (2)
where the probability of the logical form is found by
summing over all parses that produce it:
p(z|x; ?,?) =
?
y
p(y, z|x; ?,?) (3)
In this approach the distribution over parse trees y
is modeled as a hidden variable. The sum over
parses in Eq. 3 can be calculated efficiently using
the inside-outside algorithm with a CKY-style pars-
ing algorithm.
To estimate the parameters themselves, we
use stochastic gradient updates (LeCun et al,
1998). Given a set of n sentence-meaning pairs
{(xi, zi) : i = 1...n}, we update the parameters ? it-
eratively, for each example i, by following the local
gradient of the conditional log-likelihood objective
Oi = logP (zi|xi; ?,?). The local gradient of the
individual parameter ?j associated with feature ?j
and training instance (xi, zi) is given by:
?Oi
??j
= Ep(y|xi,zi;?,?)[?j(xi, y, zi)]
?Ep(y,z|xi;?,?)[?j(xi, y, z)]
(4)
As with Eq. 3, all of the expectations in Eq. 4 are
calculated through the use of the inside-outside al-
gorithm on a pruned parse chart. In the experiments,
each chart cell was pruned to the top 200 entries.
4 Splitting Lexical Items
Before presenting a complete learning algorithm, we
first describe how to use higher-order unification to
define a procedure for splitting CCG lexical entries.
This splitting process is used to expand the lexicon
during learning. We seed the lexical induction with
a multi-word lexical item xi`S :zi for each training
example (xi, zi), consisting of the entire sentence xi
and its associated meaning representation zi. For ex-
ample, one initial lexical item might be:
New York borders Vermont `S:next to(ny, vt) (5)
Although these initial, sentential lexical items
can parse the training data, they will not generalize
well to unseen data. To learn effectively, we will
need to split overly specific entries of this type into
pairs of new, smaller, entries that generalize better.
For example, one possible split of the lexical entry
given in (5) would be the pair:
New York borders ` S/NP : ?x.next to(ny, x),
Vermont `NP : vt
where we broke the original logical expression into
two new ones ?x.next to(ny, x) and vt, and paired
them with syntactic categories that allow the new
lexical entries to be recombined to produce the orig-
inal analysis. The next three subsections define the
set of possible splits for any given lexical item. The
process is driven by solving a higher-order unifica-
tion problem that defines all of the ways of splitting
the logical expression into two parts, as described in
Section 4.1. Section 4.2 describes how to construct
1226
syntactic categories that are consistent with the two
new fragments of logical form and which will allow
the new lexical items to recombine. Finally, Sec-
tion 4.3 defines the full set of lexical entry pairs that
can be created by splitting a lexical entry.
As we will see, this splitting process is overly pro-
lific for any single language and will yield many
lexical items that do not generalize well. For
example, there is nothing in our original lexical
entry above that provides evidence that the split
should pair ?Vermont? with the constant vt and not
?x.next to(ny, x). Section 5 describes how we
estimate the parameters of a probabilistic parsing
model and how this parsing model can be used to
guide the selection of items to add to the lexicon.
4.1 Restricted Higher-Order Unification
The set of possible splits for a logical expression
h is defined as the solution to a pair of higher-
order unification problems. We find pairs of logi-
cal expressions (f, g) such that either f(g) = h or
?x.f(g(x)) = h. Solving these problems creates
new expressions f and g that can be recombined ac-
cording to the CCG combinators, as defined in Sec-
tion 3.2, to produce h.
In the unrestricted case, there can be infinitely
many solution pairs (f, g) for a given expression h.
For example, when h = tex and f = ?x.tex, the
expression g can be anything. Although it would be
simple enough to forbid vacuous variables in f and
g, the number of solutions would still be exponen-
tial in the size of h. For example, when h contains a
conjunction, such as h = ?x.city(x)?major(x)?
in(x, tex), any subset of the expressions in the con-
junction can be assigned to f (or g).
To limit the number of possible splits, we enforce
the following restrictions on the possible higher-
order solutions that will be used during learning:
? No Vacuous Variables: Neither g or f can be a
function of the form ?x.e where the expression
e does not contain the variable x. This rules out
functions such as ?x.tex.
? Limited Coordination Extraction: The ex-
pression g cannot contain more than N of the
conjuncts that appear in any coordination in
h. For example, with N = 1 the expression
g = ?x.city(x)?major(x) could not be used
as a solution given the h conjuction above. We
use N = 4 in our experimental evaluation.
? Limited Application: The function f can-
not contain new variables applied to any non-
variable subexpressions from h. For example,
if h = ?x.in(x, tex), the pair f = ?q.q(tex)
and g = ?y?x.in(x, y) is forbidden.
Together, these three restrictions guarantee that
the number of splits is, in the worst case, an N -
degree polynomial of the number of constants in h.
The constraints were designed to increase the effi-
ciency of the splitting algorithm without impacting
performance on the development data.
4.2 Splitting Categories
We define the set of possible splits for a category
X :h with syntax X and logical form h by enumer-
ating the solution pairs (f, g) to the higher-order
unification problems defined above and creating
syntactic categories for the resulting expressions.
For example, given X :h = S\NP :?x.in(x, tex),
f = ?y?x.in(x, y), and g = tex, we would
produce the following two pairs of new categories:
( S\NP/NP :?y?x.in(x, y) , NP :tex )
( NP :tex , S\NP\NP :?y?x.in(x, y) )
which were constructed by first choosing the syntac-
tic category for g, in this caseNP , and then enumer-
ating the possible directions for the new slash in the
category containing f . We consider each of these
two steps in more detail below.
The new syntactic category for g is determined
based on its type, T (g). For example, T (tex) = e
and T (?x.state(x)) = ?e, t?. Then, the function
C(T ) takes an input type T and returns the syntactic
category of T as follows:
C(T ) =
?
?
?
NP if T = e
S if T = t
C(T2)|C(T1) when T = ?T1, T2?
The basic types e and t are assigned syntactic
categories NP and S, and all functional types
are assigned categories recursively. For exam-
ple C(?e, t?) = S|NP and C(?e, ?e, t??) =
S|NP |NP . This definition of CCG categories is
unconventional in that it never assigns atomic cate-
gories to functional types. For example, there is no
1227
distinct syntactic category N for nouns (which have
semantic type ?e, t?). Instead, the more complex cat-
egory S|NP is used.
Now, we are ready to define the set of all category
splits. For a category A = X:h we can define
SC(A) = {FA(A) ? BA(A) ? FC(A) ? BC(A)}
which is a union of sets, each of which includes
splits for a single CCG operator. For example,
FA(X:h) is the set of category pairs
FA(X:h) = {(X/Y :f, Y :g) | h=f(g) ? Y=C(T (g))
}
where each pair can be combined with the forward
application combinator, described in Section 3.2, to
reconstruct X:h.
The remaining three sets are defined similarly,
and are associated with the backward application
and forward and backward composition operators,
respectively:
BA(X:h) = {(Y :g,X\Y :f) | h=f(g) ? Y=C(T (g))
}
FC(X/Y :h) = {(X/W :f,W/Y :g) |
h=?x.f(g(x)) ?W=C(T (g(x)))
}
BC(X\Y :h) = {(W\Y :g,X\W :f) |
h=?x.f(g(x)) ?W=C(T (g(x)))
}
where the composition sets FC and BC only accept
input categories with the appropriate outermost slash
direction, for example FC(X/Y :h).
4.3 Splitting Lexical Items
We can now define the lexical splits that will be used
during learning. For lexical entry w0:n ` A, with
word sequence w0:n = ?w0, . . . , wn? and CCG cat-
egory A, define the set SL of splits to be:
SL(w0:n`A) = {(w0:i`B,wi+1:n`C) |
0 ? i < n ? (B,C) ? SC(A)}
where we enumerate all ways of splitting the words
sequence w0:n and aligning the subsequences with
categories in SC(A), as defined in the last section.
5 Learning Algorithm
The previous section described how a splitting pro-
cedure can be used to break apart overly specific
lexical items into smaller ones that may generalize
better to unseen data. The space of possible lexi-
cal items supported by this splitting procedure is too
large to explicitly enumerate. Instead, we learn the
parameters of a PCCG, which is used both to guide
the splitting process, and also to select the best parse,
given a learned lexicon.
Figure 2 presents the unification-based learning
algorithm, UBL. This algorithm steps through the
data incrementally and performs two steps for each
training example. First, new lexical items are in-
duced for the training instance by splitting and merg-
ing nodes in the best correct parse, given the current
parameters. Next, the parameters of the PCCG are
updated by making a stochastic gradient update on
the marginal likelihood, given the updated lexicon.
Inputs and Initialization The algorithm takes as
input the training set of n (sentence, logical form)
pairs {(xi, zi) : i = 1...n} along with an NP list,
?NP , of proper noun lexical items such as Texas`
NP :tex. The lexicon, ?, is initialized with a single
lexical item xi`S :zi for each of the training pairs
along with the contents of the NP list. It is possible
to run the algorithm without the initial NP list; we
include it to allow direct comparisons with previous
approaches, which also included NP lists. Features
and initial feature weights are described in Section 7.
Step 1: Updating the Lexicon In the lexical up-
date step the algorithm first computes the best cor-
rect parse tree y? for the current training exam-
ple and then uses y? as input to the procedure
NEW-LEX, which determines which (if any) new
lexical items to add to ?. NEW-LEX begins by enu-
merating all pairs (C,wi:j), for i < j, where C is a
category occurring at a node in y? and wi:j are the
(two or more) words it spans. For example, in the
left parse in Figure 1, there would be four pairs: one
with the category C = NP\NP :?x.border(x) and
the phrase wi:j =?ye siniri vardir?, and one for each
non-leaf node in the tree.
For each pair (C,wi:j), NEW-LEX considers in-
troducing a new lexical item wi:j`C, which allows
for the possibility of a parse where the subtree rooted
at C is replaced with this new entry. (If C is a leaf
node, this item will already exist.) NEW-LEX also
considers adding each pair of new lexical items that
is obtained by splitting wi:j`C as described in Sec-
tion 4, thereby considering many different ways of
reanalyzing the node. This process creates a set of
possible new lexicons, where each lexicon expands
1228
? in a different way by adding the items from either
a single split or a single merge of a node in y?.
For each potential new lexicon ??, NEW-LEX
computes the probability p(y?|xi, zi; ??,??) of the
original parse y? under ?? and parameters ?? that are
the same as ? but have weights for the new lexical
items, as described in Section 7. It also finds the
best new parse y? = arg maxy p(y|xi, zi; ??,??).1
Finally, NEW-LEX selects the ?? with the largest
difference in log probability between y? and y?, and
returns the new entries in ??. If y? is the best parse
for every ??, NEW-LEX returns the empty set; the
lexicon will not change.
Step 2: Parameter Updates For each training ex-
ample we update the parameters ? using the stochas-
tic gradient updates given by Eq. 4.
Discussion The alternation between refining the
lexicon and updating the parameters drives the learn-
ing process. The initial model assigns a conditional
likelihood of one to each training example (there
is a single lexical item for each sentence xi, and
it contains the labeled logical form zi). Although
the splitting step often decreases the probability of
the data, the new entries it produces are less spe-
cific and should generalize better. Since we initially
assign positive weights to the parameters for new
lexical items, the overall approach prefers splitting;
trees with many lexical items will initially be much
more likely. However, if the learned lexical items
are used in too many incorrect parses, the stochastic
gradient updates will down weight them to the point
where the lexical induction step can merge or re-split
nodes in the trees that contain them. This allows the
approach to correct the lexicon and, hopefully, im-
prove future performance.
6 Related Work
Previous work has focused on a variety of different
meaning representations. Several approaches have
been designed for the variable-free logical repre-
sentations shown in examples throughout this pa-
per. For example, Kate & Mooney (2006) present a
method (KRISP) that extends an existing SVM learn-
ing algorithm to recover logical representations. The
1This computation can be performed efficiently by incre-
mentally updating the parse chart used to find y?.
Inputs: Training set {(xi, zi) : i = 1 . . . n} where each
example is a sentence xi paired with a logical form
zi. Set of NP lexical items ?NP . Number of iter-
ations T . Learning rate parameter ?0 and cooling
rate parameter c.
Definitions: The function NEW-LEX(y) takes a parse
y and returns a set of new lexical items found by
splitting and merging categories in y, as described
in Section 5. The distributions p(y|x, z; ?,?) and
p(y, z|x; ?,?) are defined by the log-linear model,
as described in Section 3.3.
Initialization:
? Set ? = {xi ` S : zi} for all i = 1 . . . n.
? Set ? = ? ? ?NP
? Initialize ? using coocurrence statistics, as de-
scribed in Section 7.
Algorithm:
For t = 1 . . . T, i = 1 . . . n :
Step 1: (Update Lexicon)
? Let y? = arg maxy p(y|xi, zi; ?,?)
? Set ? = ? ?NEW-LEX(y?) and expand the
parameter vector ? to contain entries for the
new lexical items, as described in Section 7.
Step 2: (Update Parameters)
? Let ? = ?01+c?k where k = i+ t? n.
? Let ? = Ep(y|xi,zi;?,?)[?(xi, y, zi)]
?Ep(y,z|xi;?,?)[?(xi, y, z)]
? Set ? = ? + ??
Output: Lexicon ? and parameters ?.
Figure 2: The UBL learning algorithm.
WASP system (Wong & Mooney, 2006) uses statis-
tical machine translation techniques to learn syn-
chronous context free grammars containing both
words and logic. Lu et al (2008) (Lu08) developed
a generative model that builds a single hybrid tree
of words, syntax and meaning representation. These
algorithms are all language independent but repre-
sentation specific.
Other algorithms have been designed to re-
cover lambda-calculus representations. For exam-
ple, Wong & Mooney (2007) developed a variant
of WASP (?-WASP) specifically designed for this
alternate representation. Zettlemoyer & Collins
(2005, 2007) developed CCG grammar induction
techniques where lexical items are proposed accord-
ing to a set of hand-engineered lexical templates.
1229
Our approach eliminates this need for manual effort.
Another line of work has focused on recover-
ing meaning representations that are not based on
logic. Examples include an early statistical method
for learning to fill slot-value representations (Miller
et al, 1996) and a more recent approach for recover-
ing semantic parse trees (Ge & Mooney, 2006). Ex-
ploring the extent to which these representations are
compatible with the logic-based learning approach
we developed is an important area for future work.
Finally, there is work on using categorial gram-
mars to solve other, related learning problems.
For example, Buszkowski & Penn (1990) describe
a unification-based approach for grammar discov-
ery from bracketed natural language sentences and
Villavicencio (2002) developed an approach for
modeling child language acquisition. Additionally,
Bos et al (2004) consider the challenging problem
of constructing broad-coverage semantic representa-
tions with CCG, but do not learn the lexicon.
7 Experimental Setup
Features We use two types of features in our
model. First, we include a set of lexical features:
For each lexical item L ? ?, we include a feature
?L that fires when L is used. Second, we include se-
mantic features that are functions of the output logi-
cal expression z. Each time a predicate p in z takes
an argument a with type T (a) in position i it trig-
gers two binary indicator features: ?(p,a,i) for the
predicate-argument relation; and ?(p,T (a),i) for the
predicate argument-type relation.
Initialization The weights for the semantic fea-
tures are initialized to zero. The weights for the lex-
ical features are initialized according to coocurrance
statistics estimated with the Giza++ (Och & Ney,
2003) implementation of IBM Model 1. We com-
pute translation scores for (word, constant) pairs that
cooccur in examples in the training data. The initial
weight for each ?L is set to ten times the average
score over the (word, constant) pairs in L, except for
the weights of seed lexical entries in ?NP which are
set to 10 (equivalent to the highest possible coocur-
rence score). We used the learning rate ?0 = 1.0
and cooling rate c = 10?5 in all training scenar-
ios, and ran the algorithm for T = 20 iterations.
These values were selected with cross validation on
the Geo880 development set, described below.
Data and Evaluation We evaluate our system
on the GeoQuery datasets, which contain natural-
language queries of a geographical database paired
with logical representations of each query?s mean-
ing. The full Geo880 dataset contains 880 (English-
sentence, logical-form) pairs, which we split into a
development set of 600 pairs and a test set of 280
pairs, following Zettlemoyer & Collins (2005). The
Geo250 dataset is a subset of Geo880 containing
250 sentences that have been translated into Turk-
ish, Spanish and Japanese as well as the original En-
glish. Due to the small size of this dataset we use
10-fold cross validation for evaluation. We use the
same folds as Wong & Mooney (2006, 2007) and Lu
et al (2008), allowing a direct comparison.
The GeoQuery data is annotated with both
lambda-calculus and variable-free meaning rep-
resentations, which we have seen examples of
throughout the paper. We report results for both rep-
resentations, using the standard measures of Recall
(percentage of test sentences assigned correct log-
ical forms), Precision (percentage of logical forms
returned that are correct) and F1 (the harmonic mean
of Precision and Recall).
Two-Pass Parsing To investigate the trade-off be-
tween precision and recall, we report results with a
two-pass parsing strategy. When the parser fails to
return an analysis for a test sentence due to novel
words or usage, we reparse the sentence and allow
the parser to skip words, with a fixed cost. Skip-
ping words can potentially increase recall, if the ig-
nored word is an unknown function word that does
not contribute semantic content.
8 Results and Discussion
Tables 1, 2, and 3 present the results for all of the ex-
periments. In aggregate, they demonstrate that our
algorithm, UBL, learns accurate models across lan-
guages and for both meaning representations. This
is a new result; no previous system is as general.
We also see the expected tradeoff between preci-
sion and recall that comes from the two-pass parsing
approach, which is labeled UBL-s. With the abil-
ity to skip words, UBL-s achieves the highest recall
of all reported systems for all evaluation conditions.
1230
System
English Spanish
Rec. Pre. F1 Rec. Pre. F1
WASP 70.0 95.4 80.8 72.4 91.2 81.0
Lu08 72.8 91.5 81.1 79.2 95.2 86.5
UBL 78.1 88.2 82.7 76.8 86.8 81.4
UBL-s 80.4 80.8 80.6 79.7 80.6 80.1
System
Japanese Turkish
Rec. Pre. F1 Rec. Pre. F1
WASP 74.4 92.0 82.9 62.4 97.0 75.9
Lu08 76.0 87.6 81.4 66.8 93.8 78.0
UBL 78.5 85.5 81.8 70.4 89.4 78.6
UBL-s 80.5 80.6 80.6 74.2 75.6 74.9
Table 1: Performance across languages on Geo250 with
variable-free meaning representations.
System
English Spanish
Rec. Pre. F1 Rec. Pre. F1
?-WASP 75.6 91.8 82.9 80.0 92.5 85.8
UBL 78.0 93.2 84.7 75.9 93.4 83.6
UBL-s 81.8 83.5 82.6 81.4 83.4 82.4
System
Japanese Turkish
Rec. Pre. F1 Rec. Pre. F1
?-WASP 81.2 90.1 85.8 68.8 90.4 78.1
UBL 78.9 90.9 84.4 67.4 93.4 78.1
UBL-s 83.0 83.2 83.1 71.8 77.8 74.6
Table 2: Performance across languages on Geo250 with
lambda-calculus meaning representations.
However, UBL achieves much higher precision and
better overall F1 scores, which are generally compa-
rable to the best performing systems.
The comparison to the CCG induction techniques
of ZC05 and ZC07 (Table 3) is particularly striking.
These approaches used language-specific templates
to propose new lexical items and also required as in-
put a set of hand-engineered lexical entries to model
phenomena such as quantification and determiners.
However, the use of higher-order unification allows
UBL to achieve comparable performance while au-
tomatically inducing these types of entries.
For a more qualitative evaluation, Table 4 shows a
selection of lexical items learned with high weights
for the lambda-calculus meaning representations.
Nouns such as ?state? or ?estado? are consistently
learned across languages with the category S|NP ,
which stands in for the more conventional N . The
algorithm also learns language-specific construc-
tions such as the Japanese case markers ?no? and
?wa?, which are treated as modifiers that do not add
semantic content. Language-specific word order is
also encoded, using the slash directions of the CCG
System
Variable Free Lambda Calculus
Rec. Pre. F1 Rec. Pre. F1
Cross Validation Results
KRISP 71.7 93.3 81.1 ? ? ?
WASP 74.8 87.2 80.5 ? ? ?
Lu08 81.5 89.3 85.2 ? ? ?
?-WASP ? ? ? 86.6 92.0 89.2
Independent Test Set
ZC05 ? ? ? 79.3 96.3 87.0
ZC07 ? ? ? 86.1 91.6 88.8
UBL 81.4 89.4 85.2 85.0 94.1 89.3
UBL-s 84.3 85.2 84.7 87.9 88.5 88.2
Table 3: Performance on the Geo880 data set, with varied
meaning representations.
categories. For example, ?what? and ?que? take
their arguments to the right in the wh-initial English
and Spanish. However, the Turkish wh-word ?nel-
erdir? and the Japanese question marker ?nan desu
ka? are sentence final, and therefore take their argu-
ments to the left. Learning regularities of this type
allows UBL to generalize well to unseen data.
There is less variation and complexity in the
learned lexical items for the variable-free represen-
tation. The fact that the meaning representation is
deeply nested influences the form of the induced
grammar. For example, recall that the sentence
?what states border texas? would be paired with the
meaning answer(state(borders(tex))). For this
representation, lexical items such as:
what ` S/NP : ?x.answer(x)
states `NP/NP : ?x.state(x)
border `NP/NP : ?x.borders(x)
texas `NP : tex
can be used to construct the desired output. In
practice, UBL often learns entries with only a sin-
gle slash, like those above, varying only in the di-
rection, as required for the language. Even the
more complex items, such as those for quantifiers,
are consistently simpler than those induced from
the lambda-calculus meaning representations. For
example, one of the most complex entries learned
in the experiments for English is the smallest `
NP\NP/(NP |NP ):?f?x.smallest one(f(x)).
There are also differences in the aggregate statis-
tics of the learned lexicons. For example, the aver-
age length of a learned lexical item for the (lambda-
calculus, variable-free) meaning representations is:
1231
(1.21,1.08) for Turkish, (1.34,1.19) for English,
(1.43,1.25) for Spanish and (1.63,1.42) for Japanese.
For both meaning representations the model learns
significantly more multiword lexical items for the
somewhat analytic Japanese than the agglutinative
Turkish. There are also variations in the average
number of learned lexical items in the best parses
during the final pass of training: 192 for Japanese,
206 for Spanish, 188 for English and 295 for Turk-
ish. As compared to the other languages, the mor-
pologically rich Turkish requires significantly more
lexical variation to explain the data.
Finally, there are a number of cases where the
UBL algorithm could be improved in future work.
In cases where there are multiple allowable word or-
ders, the UBL algorithm must learn individual en-
tries for each possibility. For example, the following
two categories are often learned with high weight for
the Japanese word ?chiisai?:
NP/(S|NP )\(NP |NP ):?f?g.argmin(x, g(x), f(x))
NP |(S|NP )/(NP |NP ):?f?g.argmin(x, g(x), f(x))
and are treated as distinct entries in the lexicon. Sim-
ilarly, the approach presented here does not model
morphology, and must repeatedly learn the correct
categories for the Turkish words ?nehri,? ?nehir,?
?nehirler,? and ?nehirlerin?, all of which correspond
to the logical form ?x.river(x).
9 Conclusions and Future Work
This paper has presented a method for inducing
probabilistic CCGs from sentences paired with log-
ical forms. The approach uses higher-order unifi-
cation to define the space of possible grammars in
a language- and representation-independent manner,
paired with an algorithm that learns a probabilistic
parsing model. We evaluated the approach on four
languages with two meaning representations each,
achieving high accuracy across all scenarios.
For future work, we are interested in exploring
the generality of the approach while extending it to
new understanding problems. One potential limi-
tation is in the constraints we introduced to ensure
the tractability of the higher-order unification proce-
dure. These restrictions will not allow the approach
to induce lexical items that would be used with,
among other things, many of the type-raised combi-
nators commonly employed in CCG grammars. We
English
population of ` NP/NP : ?x.population(x)
smallest ` NP/(S|NP ) : ?f.arg min(y, f(y), size(y))
what ` S|NP/(S|NP ) : ?f?x.f(x)
border ` S|NP/NP : ?x?y.next to(y, x)
state ` S|NP : ?x.state(x)
most ` NP/(S|NP )\(S|NP )\(S|NP |NP ) :
?f?g?h?x.argmax(y, g(y), count(z, f(z, y) ? h(z)))
Japanese
no ` NP |NP/(NP |NP ) : ?f?x.f(x)
shuu ` S|NP : ?x.state(x)
nan desu ka ` S\NP\(NP |NP ) : ?f?x.f(x)
wa ` NP |NP\(NP |NP ) : ?f?x.f(x)
ikutsu ` NP |(S|NP )\(S|NP |(S|NP )) :
?f?g.count(x, f(g(x)))
chiiki ` NP\NP :?x.area(x)
Turkish
nedir ` S\NP\(NP |NP ) : ?f?x.f(x)
sehir ` S|NP : ?x.city(x)
nufus yogunlugu ` NP |NP : ?x.density(x)
siniri` S|NP/NP : ?x?y.next to(y, x)
kac tane ` S\NP/(S|NP |NP )\(S|NP ) :
?f?g?x.count(y, f(y) ? g(y, x))
ya siniri ` S|NP\NP : ?x?y.next to(y, x)
Spanish
en ` S|NP/NP : ?x?y.loc(y, x)
que es la ` S/NP/(NP |NP ): ?f?x.f(x)
pequena ` NP\(S|NP )\(NP |NP ) :
?g?f.arg min(y, f(y), g(y))
estado ` S|NP : ?x.state(x)
mas ` S\(S|NP )/(S|NP )\(NP |NP |(S|NP )) :
?f?g?h.argmax(x, h(x), f(g, x))
mayores `S|NP\(S|NP ) :?f?x.f(x) ?major(x)
Table 4: Example learned lexical items for each language
on the Geo250 lambda-calculus data sets.
are also interested in developing similar grammar
induction techniques for context-dependent under-
standing problems, such as the one considered by
Zettlemoyer & Collins (2009). Such an approach
would complement ideas for using high-order unifi-
cation to model a wider range of language phenom-
ena, such as VP ellipsis (Dalrymple et al, 1991).
Acknowledgements
We thank the reviewers for useful feedback. This
work was supported by the EU under IST Cog-
nitive Systems grant IP FP6-2004-IST-4-27657
?Paco-Plus? and ERC Advanced Fellowship 249520
?GRAMPLUS? to Steedman. Kwiatkowski was
supported by an EPRSC studentship. Zettlemoyer
was supported by a US NSF International Research
Fellowship.
1232
References
Bos, J., Clark, S., Steedman, M., Curran, J. R., & Hock-
enmaier, J. (2004). Wide-coverage semantic represen-
tations from a CCG parser. In Proceedings of the In-
ternational Conference on Computational Linguistics.
Buszkowski, W. & Penn, G. (1990). Categorial grammars
determined from linguistic data by unification. Studia
Logica, 49, 431?454.
Carpenter, B. (1997). Type-Logical Semantics. The MIT
Press.
Clark, S. & Curran, J. R. (2003). Log-linear models
for wide-coverage CCG parsing. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Clark, S. & Curran, J. R. (2007). Wide-coverage effi-
cient statistical parsing with CCG and log-linear mod-
els. Computational Linguistics, 33(4), 493?552.
Dalrymple, M., Shieber, S., & Pereira, F. (1991). Ellipsis
and higher-order unification. Linguistics and Philoso-
phy, 14, 399?452.
Ge, R. & Mooney, R. J. (2006). Discriminative rerank-
ing for semantic parsing. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions.
Huet, G. (1975). A unification algorithm for typed ?-
calculus. Theoretical Computer Science, 1, 27?57.
Huet, G. P. (1973). The undecidability of unification in
third order logic. Information and Control, 22(3), 257?
267.
Kate, R. J. & Mooney, R. J. (2006). Using string-kernels
for learning semantic parsers. In Proceedings of the
44th Annual Meeting of the Association for Computa-
tional Linguistics.
Kate, R. J., Wong, Y. W., & Mooney, R. J. (2005). Learn-
ing to transform natural to formal languages. In Pro-
ceedings of the National Conference on Artificial In-
telligence.
LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998).
Gradient-based learning applied to document recogni-
tion. Proceedings of the IEEE, 86(11), 2278?2324.
Lu, W., Ng, H. T., Lee, W. S., & Zettlemoyer, L. S.
(2008). A generative model for parsing natural lan-
guage to meaning representations. In Proceedings of
The Conference on Empirical Methods in Natural Lan-
guage Processing.
Miller, S., Stallard, D., Bobrow, R. J., & Schwartz, R. L.
(1996). A fully statistical approach to natural language
interfaces. In Proc. of the Association for Computa-
tional Linguistics.
Och, F. J. & Ney, H. (2003). A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1), 19?51.
Steedman, M. (1996). Surface Structure and Interpreta-
tion. The MIT Press.
Steedman, M. (2000). The Syntactic Process. The MIT
Press.
Thompson, C. A. & Mooney, R. J. (2002). Acquiring
word-meaning mappings for natural language inter-
faces. Journal of Artificial Intelligence Research, 18.
Villavicencio, A. (2002). The acquisition of a unification-
based generalised categorial grammar. Ph.D. thesis,
University of Cambridge.
Wong, Y. W. & Mooney, R. (2006). Learning for seman-
tic parsing with statistical machine translation. In Pro-
ceedings of the Human Language Technology Confer-
ence of the NAACL.
Wong, Y. W. & Mooney, R. (2007). Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Proceedings of the Association for Com-
putational Linguistics.
Zelle, J. M. & Mooney, R. J. (1996). Learning to parse
database queries using inductive logic programming.
In Proceedings of the National Conference on Artifi-
cial Intelligence.
Zettlemoyer, L. S. & Collins, M. (2005). Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Pro-
ceedings of the Conference on Uncertainty in Artificial
Intelligence.
Zettlemoyer, L. S. & Collins, M. (2007). Online learning
of relaxed CCG grammars for parsing to logical form.
In Proc. of the Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning.
Zettlemoyer, L. S. & Collins, M. (2009). Learning
context-dependent mappings from sentences to logical
form. In Proceedings of The Joint Conference of the
Association for Computational Linguistics and Inter-
national Joint Conference on Natural Language Pro-
cessing.
1233
