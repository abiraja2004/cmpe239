Multi-label Text Categorization with Model Combination
based on F1-score Maximization
Akinori Fujino, Hideki Isozaki, and Jun Suzuki
NTT Communication Science Laboratories
NTT Corporation
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0237
{a.fujino,isozaki,jun}@cslab.kecl.ntt.co.jp
Abstract
Text categorization is a fundamental task in
natural language processing, and is gener-
ally defined as a multi-label categorization
problem, where each text document is as-
signed to one or more categories. We fo-
cus on providing good statistical classifiers
with a generalization ability for multi-label
categorization and present a classifier de-
sign method based on model combination
and F
1
-score maximization. In our formu-
lation, we first design multiple models for
binary classification per category. Then,
we combine these models to maximize the
F
1
-score of a training dataset. Our experi-
mental results confirmed that our proposed
method was useful especially for datasets
where there were many combinations of cat-
egory labels.
1 Introduction
Text categorization is a fundamental task in such
aspects of natural language processing as informa-
tion retrieval, information extraction, and text min-
ing. Since a text document often belongs to multiple
categories in real tasks such as web pages and in-
ternational patent categorization, text categorization
is generally defined as assigning one or more pre-
defined category labels to each data sample. There-
fore, developing better classifiers with a generaliza-
tion ability for such multi-label categorization tasks
is an important issue in the field of machine learning.
A major and conventional machine learning ap-
proach to multi-label categorization is based on bi-
nary classification. With this approach, we assume
the independence of categories and design a binary
classifier for each category that determines whether
or not to assign a category label to data samples.
Statistical classifiers such as the logistic regression
model (LRM), the support vector machine (SVM),
and naive Bayes are employed as binary classi-
fiers (Joachims, 1998).
In text categorization, the F
1
-score is often used
to evaluate classifier performance. Recently, meth-
ods for training binary classifiers to maximize the
F
1
-score have been proposed for SVM (Joachims,
2005) and LRM (Jansche, 2005). It was con-
firmed experimentally that these training methods
were more effective for obtaining binary classifiers
with better F
1
-score performance than the minimum
error rate and maximum likelihood used for train-
ing conventional classifiers, especially when there
was a large imbalance between positive and nega-
tive samples. In multi-label categorization, macro-
and micro-averaged F
1
-scores are often used to eval-
uate classification performance. Therefore, we can
expect to improve multi-label classification perfor-
mance by using binary classifiers trained to maxi-
mize the F
1
-score.
On the other hand, classification frameworks
based on classifier combination have also been stud-
ied in many previous works such as (Wolpert, 1992;
Larkey and Croft, 1996; Ting and Witten, 1999;
Ghahramani and Kim, 2003; Bell et al, 2005;
Fumera and Roli, 2005), to provide better classi-
fier systems. In the classifier combination research
field, it is known that weighted linear combinations
of multiple classifiers often provide better classifica-
tion performance than individual classifiers.
823
We present a classifier design method based on
the combination of multiple binary classifiers to im-
prove multi-label classification performance. In our
framework, we first train multiple binary classifiers
for each category. Then, we combine these bi-
nary classifiers with weights estimated to maximize
micro- or macro-averaged F
1
-scores, which are of-
ten used for evaluating multi-label classifiers. To es-
timate combination weights, we extend the F
1
-score
maximization training algorithm for LRM described
in (Jansche, 2005). Using three real text datasets,
we show experimentally that our classifier design
method is more effective than the conventional bi-
nary classification approaches to multi-label catego-
rization.
Our method is based on a binary classification ap-
proach. However, Kazawa et al (2005) proposed
a method for modeling a map directly from data
samples to the combination of assigned category la-
bels, and confirmed experimentally that the method
outperformed conventional binary classification ap-
proaches. Therefore, we also compare our method
with the direct mapping method experimentally.
2 F
1
-score Maximization Training of LRM
We first review the F
1
-score maximization training
method for linear models using a logistic function
described in (Jansche, 2005). The method was pro-
posed in binary classification settings, where classi-
fiers determine a class label assignment y ? {1, 0}
for a data sample represented by a feature vector x.
Here, y(n) = 1 (= 0) indicates that the class label is
assigned (unassigned) to the nth feature vector x(n).
The discriminative function of a binary classifier
based on a linear model is often defined as
f(x;?) = ?t
1
x + ?
0
, (1)
where ? = (?
0
,?t
1
)t is a model parameter vector,
and ?t
1
x implies the inner product of ?
1
and x. A
binary classifier using f(x;?) outputs a predicted
class label assignment y? for x as y?(n) = 1 (= 0)
when f(x(n);?) ? 0 (< 0).
An LRM is a binary classifier that uses the dis-
criminative function f(x;?). In this model, the
class posterior probability distribution is defined by
using a logistic function:
g(z) = {1 + exp(?z)}?1. (2)
That is, P (y = 1|x;?) = g(f(x;?)) and P (y =
0|x;?) = 1 ? P (y = 1|x;?) = g(?f(x;?)).
The LRM determines that y(n) = 1 (= 0) when
P (y = 1|x(n);?) ? 0.5 (< 0.5), since g(0) = 0.5.
The model parameter vector ? is usually estimated
to maximize the likelihood of P (y|x;?) for training
dataset D = {x(m), y(m)}Mm=1 and the prior proba-
bility density of ?:
JR(?) =
M
?
m=1
log P (y(m)|x(m);?) + log p(?). (3)
In this paper, the classifier design approach that em-
ploys this training method is called LRM-L.
By contrast, in the training method proposed
by (Jansche, 2005), the discriminative function
f(x;w) is estimated to maximize the F
1
-score of
training dataset D. This training method employs an
approximate form of the F
1
-score obtained by using
a logistic function.
The F
1
-score is defined as F
1
= 2(1/PR +
1/RE)?1, where PR and RE represent precision
and recall defined as PR = C/A and RE = C/B,
respectively. Here, C represents the number of data
samples whose true and predicted class label assign-
ments, y(n) and y?(n), respectively, correspond to 1.
A represents the number of data samples for which
y?
(n) = 1. B represents the number of data samples
for which y(n) = 1. C , A, and B are computed
for training dataset D as C =
?M
m=1 y
(m)
y?
(m)
,
A =
?M
m=1 y?
(m)
, and B =
?M
m=1 y
(m)
.
In (Jansche, 2005), y?(m) was approximated by us-
ing the discriminative and logistic functions shown
in Eqs. (1) and (2) as
y?
(m)
? g(?f(x(m);?)), ? > 0, (4)
because lim??? g(?f(x(m);?)) = y?(m). Then, an
approximate distribution of the F
1
-score for training
dataset D was provided as
F?
1
(?) =
2
?M
m=1 g(?f(x;?))y
(m)
?M
m=1 y
(m) +
?M
m=1 g(?f(x;?))
. (5)
The ? estimate for the discriminative function
f(x;?) can be computed to maximize JF (?) =
log F?
1
(?) + log p(?) around the initial ? value by
using a gradient method. In this paper, the classi-
fier design approach that uses this training method
is called LRM-F.
824
3 Proposed Method
We propose a framework for designing a multi-label
classifier based on the combination of multiple mod-
els. In our formulation, multiple models are com-
bined with weights estimated to maximize the F
1
-
scores of the training dataset. In this section, we
show our formulation for model combination and
training methods for combination weights.
3.1 Combination of Multiple Models for
Multi-label Categorization
Multi-label categorization is the task of selecting
multiple category labels from K pre-defined cat-
egory labels for each data sample. Multi-label
classifiers provide a map from a feature vector
x to a category label assignment vector y =
(y
1
, . . . , yk, . . . , yK)
t
, where y(n)k = 1 (= 0) indi-
cates that the kth category label is assigned (unas-
signed) to x(n).
In our formulation, we first design multiple mod-
els for binary classification per category and ob-
tain J ? K discriminative functions, where J is the
number of models. The discriminative function of
the jth model for the kth category is denoted by
fjk(x;?jk), where ?jk represents the model param-
eter vector. Let ? = {?jk}j,k be a model parameter
set. We train model parameter vectors individually
with each model training algorithm and obtain the
estimate ?? = {??jk}jk. Then, we define the dis-
criminative function of our multi-label classifier by
combining multiple models such as
fk(x; ??,w) =
J
?
j=1
wjfjk(x; ??jk) + w0, ?k, (6)
where w = (w
0
, w
1
, . . . , wj , . . . , wJ)
t is a weight
parameter vector and is independent of k. wj pro-
vides the combination weight of the jth model, and
w
0
is the bias factor for adjusting the threshold of
the category label assignment.
We estimate the w value to maximize the
micro-averaged F
1
-score (F?), which is often used
for evaluating multi-label categorization perfor-
mance. The F?-score of training dataset D =
{x(m),y(m)}Mm=1 is calculated as
F? =
2
?M
m=1
?K
k=1 y
(m)
k y?
(m)
k
?M
m=1
?K
k=1 y
(m)
k +
?M
m=1
?K
k=1 y?
(m)
k
, (7)
We provide an approximate form of the F?-score of
the training dataset, F??(??,w), by using the approx-
imation:
y?
(m)
k ? g(?fk(x
(m); ??,w)), ? > 0, (8)
as shown in Eq. (4). In our proposed method, w is
estimated to maximize F??(??,w).
However, training dataset D is also used to es-
timate ?. Using the same training data samples
for both ? and w may lead to a bias estimation of
w. Thus, we used an n-fold cross-validation of the
training data samples to estimate w as in (Wolpert,
1992). Let ??(?m) be the model parameter set esti-
mated by using n ? 1 training data subsets not con-
taining {x(m),y(m)}. Then, using
F?? =
2
?
m,k y
(m)
k g(?fk(x; ??
(?m)
,w))
?
m,k y
(m)
k +
?
m,k g(?fk(x; ??
(?m)
,w))
, (9)
we provide the objective function of w such that
J?(w) = log F?? + log p(w), (10)
where p(w) is a prior probability density of w.
We use a Gaussian prior (Chen and Rosenfeld,
1999) with the form as p(w) ? ?Jj=0 exp{?(wj ?
?j)
2
/2?2j }, where ?j , and ?j are hyperparameters in
the Gaussian prior. We compute an estimate of w to
maximize J?(w) around the initial w value by using
a quasi-Newton method. In this paper, this formula-
tion is called model combination by micro-averaged
F
1
-score maximization (MC-F?).
3.2 Other Training Methods
In multi-label categorization problems, the macro-
averaged F
1
-score (FM ) is also used to evaluate
classifiers. Moreover, the average labeling F
1
-score
(FL) has been used to evaluate the average labeling
performance of classifiers for data samples (Kazawa
et al, 2005). These F
1
-scores are computed for
training dataset D as
FM =
1
K
K
?
k=1
2
?M
m=1 y
(m)
k y?
(m)
k
?M
m=1 y
(m)
k +
?M
m=1 y?
(m)
k
, (11)
FL =
1
M
M
?
m=1
2
?K
k=1 y
(m)
k y?
(m)
k
?K
k=1 y
(m)
k +
?K
k=1 y?
(m)
k
. (12)
Using Eq. (8), we can also obtain the approxi-
mate forms, F?M (??,w) and F?L(??,w), of the FM -
825
and FL-scores, and then present similar objective
functions to that for the F?-score. Therefore, in
the next section, we examine experimentally the per-
formance of classifiers obtained by estimating w to
maximize F?M (??,w) and F?L(??,w). In this paper,
these model combination methods based on FM -
and FL-scores are called MC-FM and MC-FL, re-
spectively.
4 Experiments
4.1 Test Collections
To evaluate our proposed method empirically, we
used three test collections: Reuters-21578 (Reuters),
WIPO-alpha (WIPO), and Japanese Patent (JPAT)
datasets. Reuters and WIPO are English document
datasets and have often been employed for bench-
mark tests of multi-label classifiers.
The Reuters dataset contains news articles from
the Reuters newswire and consists of 135 topic cate-
gories. Following the setup in (Yang and Liu, 1999),
we extracted 7770 and 3019 articles as training and
test samples, respectively. A subset consisting of the
training and test samples contained 90 topic cate-
gories. We removed vocabulary words included ei-
ther in the stoplist or in only one article. There were
16365 vocabulary words in the dataset.
The WIPO dataset consists of patent documents
categorized using the International Patent Classifica-
tion (IPC) taxonomy (Fall et al, 2003). The IPC tax-
onomy has four hierarchical layers: Section, Class,
Subclass, and Group. Using patent documents be-
longing to Section D (textiles; paper), we evalu-
ated classifiers in a task that consisted of selecting
assigned category labels from 160 groups for each
patent document. Following the setting provided in
the dataset, we extracted 1352 and 358 patent docu-
ments as training and test samples, respectively. We
removed vocabulary words in the same way as for
Reuters. There were 45895 vocabulary words in the
dataset.
The JPAT dataset (Iwayama et al, 2007) con-
sists of Japanese patent documents published be-
tween 1993 and 1999 by the Japanese Patent Office.
These documents are categorized using a taxonomy
consisting of Themes and F-terms. The themes are
top-label categories, and the patent documents be-
longing to each theme are categorized by using F-
Reuters WIPO JPAT
Nav 1.17 1.28 10.5
Nmax 15 6 40
K 90 160 268
Nds 10789 1710 2464
NLC 468 378 2430
Nds/NLC 23.1 4.52 1.01
Table 1: Statistical information of three datasets:
Nav and Nmax are the average and maximum num-
ber of assigned category labels per data sample, re-
spectively. K and Nds are the number of category
labels and data samples, respectively. NLC is the
number of category label combinations appearing in
each dataset.
terms. Using patent documents belonging to Theme
5J104, we evaluated classifiers in a task that con-
sisted of selecting assigned category labels from 268
F-terms for each patent document. 1920 patent doc-
uments published between 1993 and 1997 were used
as training samples, and 544 patent documents pub-
lished between 1998 and 1999 were used test sam-
ples. We extracted Japanese nouns, verbs, and adjec-
tives from patent documents by using a morpholog-
ical analyzer named MeCab 1, and removed vocab-
ulary words included in only one patent document.
There were 21135 vocabulary words in the dataset.
Table 1 shows statistical information about the
category label assignment of the data samples for the
three datasets. The average numbers of assigned cat-
egory labels per data sample, Nav , for Reuters and
WIPO were close to 1 and much smaller than that
for JPAT. The number of category label combina-
tions, NLC , included in JPAT was larger than those
for Reuters and WIPO. These statistical information
results show that JPAT is a more complex multi-label
dataset than Reuters or WIPO.
4.2 Experimental Settings
For text categorization tasks, we employed word-
frequency vectors of documents as feature vectors
input into classifiers, using the independent word-
based representation, known as the Bag-of-Words
(BOW) representation. We normalized the L1-
norms of the word-frequency vectors to 1, to miti-
gate the effect of vector size on computation. We
did not employ any word weighting methods such
as inverse document frequency (IDF).
1http://mecab.sourceforge.net/
826
We constructed three multi-label text classifiers
based on our proposed model combination methods,
MC-F?, MC-FM , and MC-FL, where LRM and
SVM (J = 2) were employed as binary classifica-
tion models combined with each method. We trained
the LRM by using LRM-L described in Section 2,
where a Gaussian prior was used as the prior proba-
bility density of the parameter vectors. We provided
the SVM by using SVMlight 2 (SVM-L), where we
employed a linear kernel function and tuned the C
(penalty cost) parameter as a hyperparameter.
To evaluate our proposed method, we examined
the micro- and macro-averaged, and average label-
ing F
1
-scores (F?, FM , and FL), of test samples ob-
tained with the three classifiers based on MC-F?,
MC-FM , and MC-FL. We compared the perfor-
mance of the three classifiers with that of two binary
classification approaches, where LRM-L or SVM-L
was used for binary classification.
We also examined two binary classification ap-
proaches using LRM-F and SVM-F. For LRM-F, we
used a Gaussian prior and provided the initial pa-
rameter vector with a parameter estimate obtained
with LRM-L. SVM-F is a binary classifier design
approach that employs SVMperf 3. For SVM-F, we
used a linear kernel function, set the L (loss parame-
ter) parameter to maximize the F
1
-score, and tuned
the C (penalty cost) parameter as a hyperparameter.
Moreover, we examined the performance of the
Maximal Margin Labeling (MML) method (Kazawa
et al, 2005), which models the map from feature
vectors to category label assignment vectors, be-
cause it was reported that MML provides better per-
formance than binary classification approaches.
We tuned the hyperparameter of SVM-F for JPAT
to provide good performance for test samples, be-
cause the computational cost for training was high.
We tuned the other hyperparameters by using a 10-
fold cross-validation of training samples.
4.3 Results and Discussion
In Table 2, we show the classification performance
obtained for three datasets with our proposed and
other methods described in Section 4.2. We ex-
amined nine evaluation scores: the micro-averaged
F
1
-score (F?), precision (P?), and recall (R?), the
2http://svmlight.joachims.org/
3http://svmlight.joachims.org/svm perf.html
Method F? (P?/R?) FM (PM /RM ) FL (PL/RM )
MC-F? 87.0 (87.4/86.7) 51.3 (60.0/48.4) 90.0 (90.1/92.3)
MC-FM 85.0 (80.8/89.5) 53.9 (54.9/58.4) 89.7 (88.5/94.1)
MC-FL 86.3 (84.3/88.3) 53.4 (59.6/52.6) 90.0 (89.3/93.6)
LRM-L 85.2 (87.3/83.2) 46.1 (55.0/43.1) 86.9 (87.6/88.6)
LRM-F 85.2 (87.2/83.2) 47.4 (58.5/42.7) 87.0 (87.6/88.7)
SVM-L 87.1 (92.9/82.0) 48.9 (58.9/45.8) 88.1 (89.3/88.8)
SVM-F 82.4 (78.9/86.2) 51.4 (49.4/60.1) 87.4 (86.9/91.4)
MML 87.8 (92.6/83.4) 59.3 (62.6/60.0) 91.2 (91.7/93.2)
(a) Reuters
Method F? (P?/R?) FM (PM /RM ) FL (PL/RM )
MC-F? 51.4 (57.3/46.6) 30.4 (35.8/30.3) 46.9 (48.3/51.5)
MC-FM 48.1 (46.1/50.4) 32.2 (33.8/36.0) 46.8 (46.3/56.0)
MC-FL 48.6 (45.8/51.9) 32.5 (33.4/36.5) 47.1 (46.4/56.8)
LRM-L 40.5 (68.0/28.9) 22.1 (33.7/17.9) 32.7 (36.5/32.0)
LRM-F 41.0 (68.6/29.2) 22.3 (34.0/18.1) 33.2 (37.0/32.4)
SVM-L 41.8 (61.9/31.5) 24.4 (34.2/21.0) 35.1 (38.8/35.3)
SVM-F 48.3 (53.8/43.8) 32.3 (37.4/31.8) 45.6 (47.9/49.6)
MML 48.6 (54.9/43.6) 30.8 (36.5/29.7) 49.4 (56.2/48.4)
(b) WIPO
Method F? (P?/R?) FM (PM /RM ) FL (PL/RM )
MC-F? 41.8 (42.6/41.1) 17.5 (21.4/17.4) 40.2 (43.5/44.4)
MC-FM 40.6 (35.8/46.7) 20.2 (20.4/23.1) 39.4 (37.7/50.6)
MC-FL 42.1 (42.3/41.9) 17.6 (21.1/17.8) 40.5 (43.2/45.2)
LRM-L 33.9 (44.4/27.4) 15.8 (20.9/14.0) 32.2 (46.5/29.9)
LRM-F 36.9 (44.6/31.5) 16.9 (22.9/14.7) 35.1 (47.3/34.1)
SVM-L 33.3 (39.6/28.7) 16.3 (20.9/14.6) 31.9 (42.4/31.6)
SVM-F 32.2 (28.6/36.8) 19.7 (15.0/38.4) 31.0 (30.7/40.0)
MML 32.7 (42.1/26.8) 14.7 (19.4/12.9) 32.2 (51.8/30.5)
(c) JPAT
Table 2: Micro- and macro-averaged, and average
labeling F
1
-scores (%) with our proposed and con-
ventional methods.
macro-averaged F
1
-score (FM ), precision (PM ),
and recall (RM ), and the average labeling F1-score
(FL), precision (PL), and recall (RL) of the test sam-
ples. FM and PM were calculated by regarding both
the F
1
-score and precision as zero for the categories
where there were no data samples predicted as posi-
tive samples.
LRM-F and SVM-F outperformed LRM-L and
SVM-L in terms of FM -score for the three datasets,
respectively. The training methods of LRM-F and
SVM-F were useful to improve the FM -scores of
LRM and SVM, as reported in (Jansche, 2005;
Joachims, 2005). The F?- and FL-scores of LRM-F
were similar or better than those of LRM-L. LRM-
F was effective in improving not only the FM -score
but also the F?- and FL-scores obtained with LRM.
Let us evaluate our model combination methods.
827
MC-F? provided better F?-scores than LRM-F and
SVM-F. The FM -scores of MC-FM were similar or
better than those of LRM-F and SVM-F. Moreover,
MC-FL outperformed LRM-F and SVM-F in terms
of FL-scores. The binary classifiers designed by us-
ing LRM-F and SVM-F were trained to maximize
the F
1
-score for each category. On the other hand,
MC-F?, MC-FM , and MC-FL classifiers were con-
structed by combining LRM and SVM with weights
estimated to maximize the F?-, FM -, and FL-scores,
respectively. The experimental results show that our
training methods for combination weights were use-
ful for obtaining better multi-label classifiers.
MC-F?, MC-FM , and MC-FL outperformed
MML as regards the three F
1
-scores for JPAT. How-
ever, MML performed better for Reuters than MC-
F?, MC-FM , and MC-FL, and provided a better FL-
score for WIPO. As shown in Table 1, there were
more category label combinations for JPAT than for
Reuters or WIPO. As a result, there were fewer data
samples for the same category label assignment for
JPAT. Therefore, MML, which learns the map di-
rectly from the feature vectors to the category label
assignment vectors, would have been overfitted to
the training dataset for JPAT. By contrast, our model
combination methods employ binary classifiers for
each category, which mitigates such an overfitting
problem. Our model combination methods will be
useful for complex datasets where there are many
category label combinations.
5 Conclusion
We proposed a multi-label classifier design method
based on model combination. The main idea be-
hind our proposed method is to combine multiple
models with weights estimated to maximize evalua-
tion scores such as the micro- and macro-averaged,
and average labeling F
1
-scores. Using three real
text datasets, we confirmed experimentally that our
proposed method provided similar or better perfor-
mance than conventional binary classification ap-
proaches to multi-label categorization. We also con-
firmed that our proposed method was useful for
datasets where there were many combinations of
category labels. Future work will involve training
our multi-label classifier by using labeled and un-
labeled samples, which are data samples with and
without category label assignment.
References
David A. Bell, J. W. Guan, and Yaxin Bi. 2005. On
combining classifier mass functions for text categorization.
IEEE Transactions on Knowledge and Data Engineering,
17(10):1307?1319.
Stanley F. Chen and Ronald Rosenfeld. 1999. A Gaussian prior
for smoothing maximum entropy models. Technical report,
Carnegie Mellon University.
C. J. Fall, A. To?rcsva?ri, K. Benzineb, and G. Karetka. 2003.
Automated categorization in the international patent classifi-
cation. ACM SIGIR Forum, 37(1):10?25.
Giorgio Fumera and Fabio Roli. 2005. A theoretical and exper-
imental analysis of linear combiners for multiple classifier
systems. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 27(6):942?956.
Zoubin Ghahramani and Hyun-Chul Kim. 2003. Bayesian
classifier combination. Technical report, Gatsby Computa-
tional Neuroscience Unit, University College London.
Makoto Iwayama, Atsushi Fujii, and Noriko Kando. 2007.
Overview of classification subtask at NTCIR-6 patent re-
trieval task. In Proceedings of the 6th NTCIR Workshop
Meeting on Evaluation of Information Access Technologies
(NTCIR-6), pages 366?372.
Martin Jansche. 2005. Maximum expected F-measure train-
ing of logistic regression models. In Proceedings of
Human Language Technology Conference and Conference
on Empirical Methods in Natural Language Processing
(HLT/EMNLP2005), pages 692?699.
Thorsten Joachims. 1998. Text categorization with support
vector machines: Learning with many relevant features. In
Proceedings of the 10th European Conference on Machine
Learning (ECML ?98), pages 137?142.
Thorsten Joachims. 2005. A support vector method for multi-
variate performance measures. In Proceedings of the 22nd
International Conference on Machine Learning (ICML?05),
pages 377?384.
Hideto Kazawa, Tomonori Izumitani, Hirotoshi Taira, and
Eisaku Maeda. 2005. Maximal margin labeling for multi-
topic text categorization. In Advances in Neural Information
Processing Systems 17, pages 649?656. MIT Press, Cam-
bridge, MA.
Leah S. Larkey and W. Bruce Croft. 1996. Combining classi-
fiers in text categorization. In Proceedings of the 19th ACM
International Conference on Research and Development in
Information Retrieval (SIGIR-96), pages 289?297.
Kai Ming Ting and Ian H. Witten. 1999. Issues in stacked
generalization. Journal of Artificial Intelligence Research,
10:271?289.
David H. Wolpert. 1992. Stacked generalization. Newral Net-
works, 5(2):241?259.
Yiming Yang and Xin Liu. 1999. A re-examination of text
categorization methods. In Proceedings of the 22nd ACM
International Conference on Research and Development in
Information Retrieval (SIGIR-99), pages 42?49.
828
