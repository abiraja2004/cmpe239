Annotating Opinions in the World Press
Theresa Wilson
Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260, USA
twilson@cs.pitt.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260, USA
wiebe@cs.pitt.edu
Abstract
In this paper we present a detailed
scheme for annotating expressions of
opinions, beliefs, emotions, sentiment
and speculation (private states) in the
news and other discourse. We explore
inter-annotator agreement for individ-
ual private state expressions, and show
that these low-level annotations are use-
ful for producing higher-level subjec-
tive sentence annotations.
1 Introduction
In this paper we present a detailed scheme for
annotating expressions of opinions, beliefs, emo-
tions, sentiment, speculation and other private
states in newspaper articles. Private state is a
general term that covers mental and emotional
states, which cannot be directly observed or ver-
ified (Quirk et al, 1985). For example, we can
observe evidence of someone else being happy,
but we cannot directly observe their happiness.
In natural language, opinions, emotions and other
private states are expressed using subjective lan-
guage (Banfield, 1982; Wiebe, 1994).
Articles in the news are composed of a mix-
ture of factual and subjective material. Writers
of editorials frequently include facts to support
their arguments, and news reports often mix seg-
ments presenting objective facts with segments
presenting opinions and verbal reactions (van
Dijk, 1988). However, natural language pro-
cessing applications that retrieve or extract infor-
mation from or that summarize or answer ques-
tions about news and other discourse have fo-
cused primarily on factual information and thus
could benefit from knowledge of subjective lan-
guage. Traditional information extraction and in-
formation retrieval systems could learn to concen-
trate on objectively presented factual information.
Question answering systems could identify when
an answer is speculative rather than certain. In
addition, knowledge of how opinions and other
private states are realized in text would directly
support new tasks, such as opinion-oriented in-
formation extraction (Cardie et al, 2003). The
ability to extract opinions when they appear in
documents would benefit multi-document sum-
marization systems seeking to summarize differ-
ent opinions and perspectives, as well as multi-
perspective question-answering systems trying to
answer opinion-based questions.
The annotation scheme we present in this paper
was developed as part of a U.S. government-
sponsored project (ARDA AQUAINT NRRC)1
to investigate multiple perspectives in question
answering (Wiebe et al, 2003). We implemented
the scheme in GATE2, a General Architecture
for Text Engineering (Cunningham et al, 2002).
General instructions for annotating opinions and
specific instructions for downloading and using
GATE to perform the annotations are available at
1This work was performed in support of the Northeast
Regional Research Center (NRRC) which is sponsored by
the Advanced Research and Development Activity in In-
formation Technology (ARDA), a U.S. Government entity
which sponsors and promotes research of import to the In-
telligence Community which includes but is not limited to
the CIA, DIA, NSA, NIMA, and NRO.
2GATE is freely available from the University of
Sheffield at http://gate.ac.uk.
http://www.cs.pitt.edu/ ?wiebe/pubs/ardasummer02.
The annotated data will be available to U.S. gov-
ernment contractors this summer. We are working
to resolve copyright issues to make it available to
the wider research community.
In developing this annotation scheme, we had
two goals. The first was to develop a represen-
tation for opinions and other private states that
was built on work in linguistics and literary the-
ory on subjectivity (please see (Banfield, 1982;
Fludernik, 1993; Wiebe, 1994; Stein and Wright,
1995) for references). The study of subjectiv-
ity in language focuses on how private states are
expressed linguistically in context. Our second
goal was to develop an annotation scheme that
would be useful for corpus-based research on
subjective language and for the development of
applications such as multi-perspective question-
answering systems. The annotation scheme that
resulted is more detailed and comprehensive than
previous ones for subjective language.
Our study of the annotations produced by the
annotation scheme gives two important results.
First, we find that trained annotators can consis-
tently perform detailed opinion annotations with
good agreement (0.81 Kappa). Second, the agree-
ment results are better than in previous sentence-
level annotation studies, suggesting that adding
detail can help the annotators perform more re-
liably.
In the sections that follow, we first review how
opinions and other private states are expressed in
language (section 2) and give a brief overview
of previous work in subjectivity tagging (section
3). We then describe our annotation scheme for
private state expressions (section 4) and give the
results of an annotation study (section 5). We
conclude with a discussion of our findings from
the annotation study and and future work (section
??). In the appendix, we give sample annotations
as well as a snapshot of the annotations in GATE.
2 Expressing Private States in Text
2.1 Private States, Speech Events, and
Expressive Subjective Elements
There are two main ways that private states are
expressed in language. Private states may be ex-
plicitly mentioned, or they may be expressed in-
directly by the types of words and the style of lan-
guage that a speaker or writer uses. An example
of an explicitly-mentioned private state is ?frus-
trated? in sentence (1).
(1) Western countries were left frus-
trated and impotent after Robert
Mugabe formally declared that he
had overwhelmingly won Zimbabwe?s
presidential election.
Although most often verbs, it is interesting to note
that explicit mentions of private states may also
be nouns, such as ?concern? in ?international con-
cern? and ?will? in ?will of the people.? They may
even be adjectives, such as ?fearful? in ?fearful
populace.?
The second way that private states are generally
expressed is indirectly using expressive subjective
elements (Banfield, 1982). For example, the pri-
vate states in sentences (2) and (3) are expressed
entirely by the words and the style of language
that is used.
(2) The time has come, gentlemen, for
Sharon, the assassin, to realize that in-
justice cannot last long.
(3) ?We foresaw electoral fraud but not
daylight robbery,? Tsvangirai said.
In (2), although the writer does not explicitly
say that he hates Sharon, his choice of words
clearly demonstrates a negative attitude. In sen-
tence (3), describing the election as ?daylight rob-
bery? clearly reflects the anger being experienced
by the speaker, Tsvangirai. As used in these sen-
tences, the phrases ?The time has come,? ?gentle-
men,? ?the assassin,? ?injustice cannot last long,?
?fraud,? and ?daylight robbery? are all expressive
subjective elements. Expressive subjective ele-
ments are used by people to express their frus-
tration, anger, wonder, positive sentiment, mirth,
etc., without explicitly stating that they are frus-
trated, angry, etc. Sarcasm and irony often in-
volve expressive subjective elements.
When looking for opinions and other private
states in text, an annotator must consider speech
events as well as explicitly-mentioned private
states. In this work, we use speech event to refer
to any event of speaking or writing. However, the
mere presence of a speech event does not indicate
a private state. Both sentences (3) above and (4)
below contain speech events indicated by ?said.?
As mentioned previously, sentence (3) is opinion-
ated, while in (4) the information is presented as
factual.
(4) Medical Department head Dr
Hamid Saeed said the patient?s blood
had been sent to the Institute for
Virology in Johannesburg for analysis.
For speech terms such as ?said,? ?added,? ?told,?
?announce,? and ?report,? an annotator deter-
mines if there is a private state mainly by looking
inside the scope of the speech term for expressive
subjective elements.
Occasionally, we also find private states that
are expressed by direct physical actions. We call
such actions private state actions. Examples are
booing someone, sighing heavily, shaking ones
fist angrily, waving ones hand dismissively, and
frowning. ?Applauding? in sentence (5) is an ex-
ample of a positive-evaluative private state action.
(5) As the long line of would-be voters
marched in, those near the front of the
queue began to spontaneously applaud
those who were far behind them.
2.2 Nested Sources
An important aspect of a private state or speech
event is its source. The source of a speech event
is the speaker or writer. The source of a private
state is the experiencer of the private state, i.e.,
the person whose opinion or emotion is being ex-
pressed. Obviously, the writer of an article is a
source, because he wrote the sentences compos-
ing the article, but the writer may also write about
other people?s private states and speech events,
leading to multiple sources in a single sentence.
For example, each of the following sentences has
two sources: the writer (because he wrote the sen-
tences), and Sue (because she is the source of a
speech event in (6) and of private states in (7) and
(8), namely thinking and being afraid).
(6) Sue said, ?The election was fair.?
(7) Sue thinks that the election was fair.
(8) Sue is afraid to go outside.
Note, however, that we don?t really know what
Sue says, thinks or feels. All we know is what the
writer tells us. Sentence (6), for example, does
not directly present Sue?s speech event but rather
Sue?s speech event according to the writer. Thus,
we have a natural nesting of sources in a sentence.
The nesting of sources may be quite deep and
complex. For example, consider sentence (9).
(9) The Foreign Ministry said Thursday
that it was ?surprised, to put it mildly?
by the U.S. State Department?s criti-
cism of Russia?s human rights record
and objected in particular to the ?odi-
ous? section on Chechnya.
There are three sources in this sentence: the
writer, the Foreign Ministry, and the U.S. State
Department. The writer is the source of the over-
all sentence. The remaining explicitly mentioned
private states and speech events in (9) have the
following nested sources:
said: (writer, Foreign Ministry)
surprised, to put it mildly:
(writer, Foreign Ministry, Foreign Ministry)
criticism:
(writer, Foreign Ministry, U.S. State Dept.)
objected: (writer, Foreign Ministry)
Expressive subjective elements may also have
nested sources. In sentence (9), ?to put it mildly?
and ?odious? are expressive subjective elements,
both with nested source (writer, Foreign Min-
istry). We might expect that an expressive subjec-
tive element always has the same nested source
as the immediately dominating private state or
speech term. Although this is the case for ?odi-
ous? in (9) (the nested source of ?odious? and
?objected? is the same), it is not the same for ?big-
ger than Jesus? in (10):
(10) ?It is heresy,? said Cao. ?The
?Shouters? claim they are bigger than
Jesus.?
The nested source of the subjectivity expressed
by ?bigger than Jesus? is Cao, while the nested
source of ?claim? is (writer, Cao, Shouters).3
3(10) is an example of a de re rather than de dicto propo-
sitional attitude report (Rapaport, 1986).
3 Previous Work on Subjectivity
Tagging
In previous work (Wiebe et al, 1999), a corpus of
sentences from the Wall Street Journal Treebank
Corpus (Marcus et al, 1993) was manually anno-
tated with subjectivity classifications by multiple
judges. The judges were instructed to classify a
sentence as subjective if it contained any signif-
icant expressions of subjectivity, attributed to ei-
ther the writer or someone mentioned in the text,
and to classify the sentence as objective, other-
wise. The judges rated the certainty of their an-
swers on a scale from 0 to 3.
Agreement in the study was summarized in
terms of Cohen?s Kappa (   ) (Cohen, 1960),
which compares the total probability of agree-
ment to that expected if the taggers? classifica-
tions were statistically independent (i.e., ?chance
agreement?). After two rounds of tagging by
three judges, an average pairwise   value of 0.69
was achieved on a test set. On average, the judges
rated 15% of the sentences as very uncertain (rat-
ing 0). When these sentences are removed, the
average pairwise   value is 0.79. When sentences
with uncertainty judgment 0 or 1 are removed (on
average 30% of the sentences), the average pair-
wise   is 0.88.
4 An Annotation Scheme for Private
States
The annotation scheme described in this section
is more detailed and comprehensive the previ-
ous ones for subjective language. In (Wiebe et
al., 1999), summary subjective/objective judg-
ments were performed at the sentence level. For
this work, annotators are asked to mark within
each sentence the word spans that indicate speech
events or that are expressions of private states.
For every span that an annotator marks, there are
a number of attributes the annotator may set to
characterize the annotation.
The annotation scheme has two main com-
ponents. The first is an annotation type for
explicitly-mentioned private states and speech
events. The second is an annotation type for ex-
pressive subjective elements. Table 1 lists the at-
tributes that may be assigned to these two types
of annotations. In addition, there is an annotation
Explicit private states/speech events
nested-source
onlyfactive: yes, no
overall-strength: low, medium, high, extreme
on-strength: neutral, low, medium, high, extreme
attitude-type: positive, negative, both (exploratory)
attitude-toward (exploratory)
is-implicit
minor
Expressive subjective elements
nested-source
strength: low, medium, high, extreme
attitude-type: positive, negative, other (exploratory)
Table 1: Attributes for the two main annotation
types. For attributes that take on one of a fixed set
of values, the set of possible values are given.
type, agent, that annotators may use to mark the
noun phrase (if one exists) of the source of a pri-
vate state or speech event.
4.1 Explicitly-mentioned Private State and
Speech Event Annotations
An important part of the annotation scheme is
represented by the onlyfactive attribute. This at-
tribute is marked on every private state and speech
event annotation. The onlyfactive attribute is used
to indicate whether the source of the private state
or speech event is indeed expressing an emo-
tion, opinion or other private state. By defini-
tion, any expression that is an explicit private state
(e.g., ?think?, ?believe,? ?hope,? ?want?) or a pri-
vate state mixed with speech (e.g., ?berate,? ?ob-
ject,? ?praise?) is onlyfactive=no. On the other
hand, neutral speech events (e.g., ?said,? ?added,?
?told?) may be either onlyfactive=yes or onlyfac-
tive=no, depending on their contents. For ex-
ample, the annotation for ?said? in sentence (3)
would be marked onlyfactive=no, but the annota-
tion for ?said? in sentence (4) would be marked
onlyfactive=yes (sentences in section 2).
Note that even if onlyfactive=no, the sentence
may express something the nested source believes
is factual. Consider the sentence ?John criti-
cized Mary for smoking.? John expresses a private
state (his negative evaluation of Mary?s smoking).
However, this does not mean that John does not
believe that Mary smokes.
Like the onlyfactive attribute, the nested-source
attribute is included on every private state and
speech event annotation. The nested source (i.e.,
(writer, Foreign Ministry, U.S. State Dept.)) is
typed in by the annotator.
When an annotation is marked onlyfactive=no,
additional attributes are used to characterize the
private state. The overall-strength attribute is
used to indicate the overall strength of the pri-
vate state (considering the explicit private state
or speech event phrase as well as everything in-
side its scope). It?s value may range from low
to extreme. The on-strength attribute is used to
measure the contribution made specifically by the
explicit private state or speech event phrase. For
example, the on-strength of ?said? is typically
neutral, the on-strength of ?criticize? is typically
medium, and the on-strength of ?vehemently de-
nied? is typically high or extreme. (As for all as-
pects of this annotation scheme, the annotators
are asked to make these judgments in context.)
A speech event that is onlyfactive=yes has on-
strength=neutral and no overall-strength. Thus,
there is no need to include the overall-strength
and on-strength attributes for onlyfactive=yes an-
notations.
4.1.1 Implicit Speech Event Annotations
Implicit speech events posed a problem when
we developed the annotation scheme. Implicit
speech events are speech events in the discourse
for which there is no explicit speech event phrase,
and thus no obvious place to attach the anno-
tation. For example, most of the writer?s sen-
tences do not include a phrase such as ?I say.?
Also, direct quotes are not always accompanied
by discourse parentheticals (such as ?, she said?).
Our solution was to add the is-implicit attribute to
the annotation type for private states and speech
events, which may then be used to mark implicit
speech event annotations.
4.1.2 Minor Private States and Speech
Events
Depending on its goals, an application may
need to identify all private state and speech event
expressions in a document, or it may want to find
only those opinions and other private states that
are significant and real in the discourse. By ?sig-
nificant?, we mean that a significant portion of the
contents of the private state or speech event are
given within the sentence where the annotation
is marked. By ?real?, we mean that the private
state or speech event is presented as an existing
event within the domain of discourse, e.g., it is
not hypothetical. We use the term minor for pri-
vate states and speech events that are not signif-
icant or not real. Annotators mark minor private
state and speech event annotations by including
the minor attribute.
The following sentences all contain one or
more minor private states or speech events (high-
lighted in bold).
(11) Such wishful thinking risks mak-
ing the US an accomplice in the de-
struction of human rights. (not signif-
icant)
(12) If the Europeans wish to influence
Israel in the political arena... (in a con-
ditional, so not real)
(13) ?And we are seeking a declara-
tion that the British government de-
mands that Abbasi should not face trial
in a military tribunal with the death
penalty.? (not real, i.e., the declaration
of the demand is just being sought)
(14) The official did not say how many
prisoners were on the flight. (not real
because the saying event did not occur)
(15) No one who has ever studied realist
political science will find this surpris-
ing. (not real since a specific ?surprise?
state is not referred to; note that the
subject noun phrase is attributive rather
than referential (Donnellan, 1966))
4.2 Expressive Subjective Element
Annotations
As with private state/speech event annotations,
the nested-source attribute is included on every
expressive subjective element annotation. In ad-
dition to marking the source of an expression, the
nested-source is also functioning as a link. Within
a sentence, the nested-source chains together all
the pieces that together indicate the overall pri-
vate state of a particular source.
In addition to nested-source, the strength at-
tribute is used to characterize expressive subjec-
tive element annotations. The strength of an ex-
pressive subjective element may range from low
to extreme (see Table 1).
4.3 Exploratory Attributes
We are exploring additional attributes that allow
an annotator to further characterize the type of
attitude being expressed by a private state. An
annotator may use the attitude-type attribute to
mark an onlyfactive=no private state/speech event
annotation or an expressive subjective element
annotation as positive or negative. An attitude-
toward attribute may also be included on private
state/speech event annotations to indicate the par-
ticular target of an evaluation, emotion, etc.
5 Annotation Study
The data in our study consists of English-
language versions of foreign news documents
from FBIS, the U.S. Foreign Broadcast Informa-
tion Service. The data is from a variety of publi-
cations and countries. To date, 252 articles have
been annotated with the scheme described in sec-
tion 4.
To measure agreement on various aspects of the
annotation scheme, three annotators (A, M, and
S) independently annotated 13 documents with a
total of 210 sentences. None of the annotators are
authors of this paper. The articles are from a vari-
ety of topics and were selected so that 1/3 of the
sentences are from news articles reporting on ob-
jective topics (objective articles), 1/3 of the sen-
tences are from news articles reporting on opin-
ionated topics (?hot-topic? articles), and 1/3 of
the sentences are from editorials.
In the instructions to the annotators, we asked
them to rate the annotation difficulty of each arti-
cle on a scale from 1 to 3, with 1 being the eas-
iest and 3 being the most difficult. The annota-
tors were not told which articles were objective
or which articles were editorials, only that they
were being given a variety of different articles to
annotate.
We hypothesized that the editorials would be
the hardest to annotate and that the objective ar-
ticles would be the easiest. The ratings that the
annotators assigned to the articles support this hy-
pothesis. The annotators rated an average of 44%
of the articles in the study as easy (rating 1) and
26% as difficult (rating 3). But, they rated an av-
erage of 73% of the objective articles as easy, and
89% of the editorials as difficult.
It makes intuitive sense that ?hot-topic? articles
would be more difficult to annotate than objective
articles and that editorials would be more difficult
still. Editorials and ?hot-topic? articles contain
many more expressions of private states, requir-
ing an annotator to make more judgments than
they would for objective articles.
5.1 Agreement for Expressive Subjective
Element Annotations
For annotations that involve marking spans of
text, such as expressive subjective element an-
notations, it is not unusual for two annotators to
identify the same expression in the text, but to
differ in how they mark the boundaries.4 For
example, both annotators A and M saw expres-
sive subjectivity in the phrase, ?such a disadvan-
tageous situation.? But, while A marked the entire
phrase as a single expressive subjective element,
M marked the individual words, ?such? and ?dis-
advantageous.? Because the annotators will iden-
tify a different number of annotations, as well as
different (but hopefully strongly overlapping) sets
of expressions, we need an agreement metric that
can measure agreement between sets of objects.
We use the   metric to measure agreement
for expressive subjective elements (and later for
private state/speech event annotations).
  is a directional measure of agreement. Let

and  be the sets of spans annotated by anno-
tators   and 	 . We compute the agreement of 	 to
  as:
 
 	







This measure of agreement corresponds to the no-
tion of precision and recall as used to evaluate, for
example, named entity recognition. The  
 	
metric corresponds to the recall if   is the gold-
standard and 	 the system, and to precision, if they
are reversed.
In the 210 sentences in the annotation study, the
annotators A, M, and S respectively marked 311,
352 and 249 expressive subjective elements. Ta-
ble 2 shows the pairwise agreement for these sets
of annotations. For example, M agrees with 76%
of the expressive subjective elements marked by
4In the coding instructions, we did not attempt to define
rules to try to enforce boundary agreement.
mother of terrorism
if the world has to rid itself from this menace, the perpetrators across the border had to be dealt with firmly
indulging in blood-shed and their lunaticism
ultimately the demon they have reared will eat up their own vitals
Table 3: Extreme strength expressive subjective elements
     	
  
  average
A M 0.76 0.72
A S 0.68 0.81
M S 0.59 0.74
0.72
Table 2: Inter-annotator Agreement: Expressive
subjective elements
A, and A agrees with 72% of the expressive
subjective elements marked by M. The average
agreement in Table 2 is the arithmetic mean of all
six   .
We hypothesized that the stronger the expres-
sion of subjectivity, the more likely the annota-
tors are to agree. To test this hypothesis, we mea-
sure agreement for the expressive subjective ele-
ments rated with a strength of medium or higher
by at least one annotator. This excludes on av-
erage 29% of the expressive subjective elements.
The average pairwise agreement rises to 0.80.
When measuring agreement for the expressive
subjective elements rated high or extreme, this ex-
cludes an average 65% of expressive subjective
elements, and the average pairwise agreement in-
creases to 0.88. Thus, annotators are more likely
to agree when the expression of subjectivity is
strong. Table 3 gives examples of expressive sub-
jective elements that at least one annotator rated
as extreme.
5.2 Agreement for Private State/Speech
Event Annotations
For private state and speech event annotations, we
again use   to measure agreement between the
sets of expressions identified by each annotator.
The three annotators, A, M, and S, respectively
marked 338, 285, and 315 explicit expressions of
private states and speech events. Implicit speech
events for the writer of course are excluded. Table
4 shows the pairwise agreement for these sets of
annotations.
The average pairwise agreement for explicit
private state and speech event expressions is 0.82,
     	    average
A M 0.75 0.91
A S 0.80 0.85
M S 0.86 0.75
0.82
Table 4: Inter-annotator Agreement: Explicitly-
mentioned private states and speech events
which indicates that they are easier to annotate
than expressive subjective elements.
5.3 Agreement for Attributes
In this section, we focus on the annotators? agree-
ment for judgments that reflect whether or not
an opinion, emotion, sentiment, speculation, or
other private state is being expressed. We con-
sider these judgments to be at the core of the an-
notation scheme. Two attributes, onlyfactive and
on-strength, carry information about whether a
private state is being expressed.
For onlyfactive judgments, we measure pair-
wise agreement between annotators for the set
of private state and speech event annotations that
both annotators identified. Because we are now
measuring agreement over the same set of objects
for each annotator, we use Kappa (   ) to capture
how well the annotators agree.
Table 5 shows the contingency table for the on-
lyfactive judgments made by annotators A and M.
The Kappa scores for all annotator pairs are given
in Table 7. For their onlyfactive judgments, i.e.,
whether or not an opinion or other private state
is being expressed, the annotators have an aver-
age pairwise Kappa of 0.81. Under Krippendorf?s
scale (Krippendorf, 1980), this allows for definite
conclusions.
With many judgments that characterize natural
language, one would expect that there are clear
cases as well as borderline cases, which would be
more difficult to judge. The agreement study in-
dicates that this is certainly true for private states.
In terms of our annotations, we define an explicit
private state or speech event to be borderline-
    

 
	
 
  

  fiff

	 flffi fifffi
Table 5: A & M: Agreement for onlyfactive judg-
ments
 
   

 
	
 
  

       

	 flfi fi"!
Table 6: A & M: Agreement for onlyfactive judg-
ments, borderline-onlyfactive cases removed
onlyfactive if 1) at least one annotator marked the
expression onlyfactive=no, and 2) neither anno-
tator characterized an overall-strength as being
greater than low. In Table 6 we give the contin-
gency table for the onlyfactive judgments made
by annotators A and M, excluding borderline-
onlyfactive expressions. Note that removing such
expressions removes agreements as well as dis-
agreements. Borderline-onlyfactive expressions
on average comprise only 10% of the private
state/speech event annotations. When they are
removed, the average pairwise Kappa climbs to
0.89.
In addition to the onlyfactive judgment, us-
ing on-strength we can measure if the annota-
tors agree as to whether an explicit private state
or speech event phrase by itself expresses a pri-
vate state. Specifically, we measure if the an-
notators agree that an expression is neutral, i.e.,
does not indicate a private state. Recall that only-
factive=yes annotations are on-strength=neutral.
Implicit annotations are excluded when measur-
ing on-strength agreement.
The pairwise agreement results for the anno-
tators? on-strength neutral judgments are given
in Table 8. For on-strength neutral judgments,
annotators have an average pairwise Kappa of
All Expressions Borderline Removed
# agree # agree % removed
A & M 0.84 0.91 0.94 0.96 10
A & S 0.84 0.92 0.90 0.95 8
M & S 0.74 0.87 0.84 0.92 12
Table 7: Pairwise Kappa scores and overall per-
cent agreement for onlyfactive judgments
All Expressions Borderline Removed
# agree # agree % removed
A & M 0.81 0.91 0.93 0.97 22
A & S 0.74 0.87 0.92 0.96 17
M & S 0.67 0.83 0.90 0.95 18
Table 8: Pairwise Kappa scores and overall per-
cent agreement for on-strength neutral judgments
0.74. As with the onlyfactive judgments, there
are clearly borderline cases. We define an expres-
sion to be borderline-low if 1) at least one anno-
tator marked the expression onlyfactive=no, and
2) neither annotator characterized an on-strength
as being greater than low. When borderline-low
expressions are removed, the pairwise Kappa in-
creases to 0.92.
5.4 Agreement for Sentences
To compare our results to those of earlier work
that evaluated the agreement of sentence-level
subjectivity annotations (Wiebe et al, 1999), we
define sentence-level classifications in terms of
our lower-level annotations as follows. First, we
exclude explicit private state/speech event expres-
sions that the annotators agree are minor. Then, if
an annotator marked one or more onlyfactive=no
expressions in the sentence, we consider the an-
notator to have judged the sentence to be subjec-
tive. Otherwise, we consider the annotator to have
judged the sentence to be objective.
The pairwise agreement results for these de-
rived sentence-level annotations are given in Ta-
ble 9. The average pairwise Kappa for sentence-
level agreement is 0.77, 8 points higher than the
sentence-level agreement reported in (Wiebe et
al., 1999). Our new results suggest that adding
detail to the annotation task can can help annota-
tors perform more reliably. Note that the agree-
ment is lower than that for onlyfactive judgments
(Table 7) because explicit private-state and speech
event expressions upon which the annotators did
not agree are now included.
As with the onlyfactive and on-strength neutral
judgments, we again test agreement when border-
line cases are removed. We define a sentence to
be borderline if 1) at least one annotator marked
at least one expression onlyfactive=no, and 2)
neither annotator marked an overall-strength at-
tribute as being greater than low. When border-
All Sentences Borderline Removed
# agree # agree % removed
A & M 0.75 0.89 0.87 0.95 11
A & S 0.84 0.94 0.92 0.97 8
M & S 0.72 0.88 0.83 0.93 13
Table 9: Pairwise Kappa scores and overall per-
cent agreement for derived sentence-level judg-
ments
line sentences are removed, the average Kappa in-
creases to 0.89.
6 Conclusions
In this paper, we presented a detailed scheme for
the annotation of opinions and other private states
in the news and other discourse. For the aspects
of this annotation scheme that indicate whether
a private state is expressed, our three annotators
have strong pairwise agreement, as measured by
Cohen?s Kappa.
One interesting area explored in this paper is
the effect of borderline cases on inter-annotator
agreement. We created a number of objec-
tive definitions of borderline cases, based on the
strengths indicated by the annotators, and found
that removing these borderline cases always re-
sults in high agreement values. This shows that
the annotators agree strongly about which are the
clear cases of subjectivity.
We have also shown that lower-level subjectiv-
ity annotations, such as those presented in this pa-
per, may be used to produce higher-level subjec-
tive sentence annotations. In current research, we
are using these higher-level annotations to evalu-
ate subjective sentence classifiers, which we hope
will be useful for enhancing natural language pro-
cessing applications such as information extrac-
tion, summarization, and question answering sys-
tems.
There are characteristics of private state expres-
sions not yet included in our scheme that would
be useful for NLP applications. We believe the
scheme is extendable, and hope that other groups
will build on it.
References
A. Banfield. 1982. Unspeakable Sentences. Rout-
ledge and Kegan Paul, Boston.
C. Cardie, J. Wiebe, T. Wilson, and D. Litman. 2003.
Combining low-level and summary representations
of opinions for multi-perspective question answer-
ing. In Working Notes - New Directions in Question
Answering (AAAI Spring Symposium Series).
J. Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Meas.,
20:37?46.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE: A
framework and graphical development environment
for robust nlp tools and applications. In Proceed-
ings of the 40th Annual Meeting of the Association
for Computational Linguistics.
Keith Donnellan. 1966. Reference and definite de-
scriptions. Philosophical Review, 60:281?304.
M. Fludernik. 1993. The Fictions of Language and
the Languages of Fiction. Routledge, London.
K. Krippendorf. 1980. Content Analysis: An In-
troduction to its Methodology. Sage Publications,
Beverly Hills.
M. Marcus, Santorini, B., and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The penn treebank. Computational Linguis-
tics, 19(2):313?330.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985. A Comprehensive Grammar of the English
Language. Longman, New York.
William Rapaport. 1986. Logical foundations for be-
lief representation. Cognitive Science, 10:371?422.
D. Stein and S. Wright, editors. 1995. Subjectivity
and Subjectivisation. Cambridge University Press,
Cambridge.
T.A. van Dijk. 1988. News as Discourse. Lawrence
Erlbaum, Hillsdale, NJ.
J. Wiebe, R. Bruce, and T. O?Hara. 1999. Develop-
ment and use of a gold standard data set for subjec-
tivity classifications. In Proc. 37th Annual Meeting
of the Assoc. for Computational Linguistics (ACL-
99), pages 246?253, University of Maryland, June.
ACL.
J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis,
B. Fraser, D. Litman, D. Pierce, E. Riloff, T. Wil-
son, D. Day, and M. Maybury. 2003. Recogniz-
ing and organizing opinions expressed in the world
press. In Working Notes - New Directions in Ques-
tion Answering (AAAI Spring Symposium Series).
J. Wiebe. 1994. Tracking point of view in narrative.
Computational Linguistics, 20(2):233?287.
 Figure 1: Example of annotations in GATE
A Sample Annotations
The following is the first sentence from an article
about the 2002 presidential election in Zimbabwe.
The article appeared on March 15, 2002 in the
newspaper, Dawn.
Western countries were left frustrated
and impotent after Robert Mugabe
formally declared that he had over-
whelmingly won Zimbabwe?s presiden-
tial election.
There are three private state/speech event an-
notations and one expressive subjective element
annotation in this sentence. The annotations,
including their attributes, are listed below:
Speech Event: implicit
nested-source = (writer)
onlyfactive = yes
Private State: were left frustrated
nested-source = (writer, Western countries)
onlyfactive = no
overall-strength = medium
on-strength = medium
Speech Event: formally declared:
nested-source = (writer, Mugabe)
onlyfactive = no
overall-strength = medium
on-strength = neutral
Expressive Subjective Element: overwhelm-
ingly:
nested-source = (writer, Mugabe)
strength = medium
Figure 1 shows how these annotations
appear inside the GATE annotation tool. Ad-
ditional annotated examples can be found
with the on-line GATE annotation instruc-
tions, http://www.cs.pitt.edu/mpqa/opinion-
annotations/gate-instructions.
