J. Novak, et al. [1].
http://code.google.com/p/
phonetisaurus
R. Tromble and S. Kumar and F. Och and W. Macherey.
[2]. Lattice Minimum Bayes-Risk Decoding for Sta-
tistical Machine Translation, Proc. EMNLP 2007, pp.
620-629.
G. Blackwood and A. Gispert and W. Byrne. [3]. Effi-
cient path counting transducers for minimum bayes-
risk decoding of statistical machine translation lat-
tices, Proc. ACL 2010, pp. 27-32.
T. Mikolov and M. Karafiat and L. Burget and J. C?er-
nock? and S. Khundanpur. [4]. Recurrent Neural Net-
work based Language Model, Proc. InterSpeech, 2010.
T. Mikolov and S. Kombrink and D. Anoop and L. Burget
and J. C?ernock?. [5]. RNNLM - Recurrent Neural Net-
work Language Modeling Toolkit, ASRU 2011, demo
session.
C. Allauzen and M. Riley and J. Schalkwyk and W. Skut
and M. Mohri. [6]. OpenFST: A General and Effi-
cient Weighted Finite-State Transducer Library, Proc.
CIAA 2007, pp. 11-23.
M. Bisani and H. Ney. [6]. Joint-sequence models for
grapheme-to-phoneme conversion, Speech Communi-
cation 50, 2008, pp. 434-451.
S. Jiampojamarn and G. Kondrak and T. Sherif. [7]. Ap-
plying Many-to-Many Alignments and Hidden Markov
Models to Letter-to-Phoneme Conversion, NAACL
HLT 2007, pp. 372-379.
S. Jiampojamarn and G. Kondrak. [8]. Letter-to-
Phoneme Alignment: an Exploration, Proc. ACL
2010, pp. 780-788.
E. Ristad and P. Yianilos. [9]. Learning String Edit Dis-
tance, IEEE Trans. PRMI 1998, pp. 522-532.
J. Novak and P. Dixon and N. Minematsu and K. Hirose
and C. Hori and H. Kashioka. [10]. Improving WFST-
based G2P Conversion with Alignment Constraints
and RNNLM N-best Rescoring, Interspeech 2012 (Ac-
cepted).
B. Hsu and J. Glass. [11]. Iterative Language Model Es-
timation: Efficient Data Structure & Algorithms, Proc.
Interspeech 2008.