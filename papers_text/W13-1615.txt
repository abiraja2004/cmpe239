Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 108?114,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Sentence-Level Subjectivity Detection Using Neuro-Fuzzy Models 
 
Samir Rustamov Mark A. Clements 
Georgia Institute of Technology Georgia Institute of Technology 
225 North Avenue NW 225 North Avenue NW 
Atlanta, GA 30332, USA Atlanta, GA 30332, USA 
samir.rustamov@gmail.com clements@ece.gatech.edu 
 
 
Abstract 
In this work, we attempt to detect sentence-
level subjectivity by means of two supervised 
machine learning approaches: a Fuzzy Control 
System and Adaptive Neuro-Fuzzy Inference 
System. Even though these methods are popu-
lar in pattern recognition, they have not been 
thoroughly investigated for subjectivity analy-
sis. We present a novel ?Pruned ICF 
Weighting Coefficient,? which improves the 
accuracy for subjectivity detection. Our fea-
ture extraction algorithm calculates a feature 
vector based on the statistical occurrences of 
words in a corpus without any lexical 
knowledge. For this reason, these machine 
learning models can be applied to any lan-
guage; i.e., there is no lexical, grammatical, 
syntactical analysis used in the classification 
process. 
1 Introduction 
There has been a growing interest, in recent years, 
in identifying and extracting subjective infor-
mation from Web documents that contain opinions. 
Opinions are usually subjective expressions that 
describe people's sentiments, appraisals, or feel-
ings. Subjectivity detection seeks to identify 
whether the given text expresses opinions (subjec-
tive) or reports facts (objective) (Lin et al, 2011). 
Automatic subjectivity analysis methods have been 
used in a wide variety of text processing and natu-
ral language  applications. In many natural lan-
guage processing tasks, subjectivity detection has 
been used as a first phase of filtering to generate 
more informative data.  
The goal of our research is to develop learning 
methods to create classifiers that can distinguish 
subjective from objective sentences. In this paper, 
we achieve sentence-level subjectivity classifica-
tion using language independent feature weighting. 
As a test problem, we employed a subjectivity da-
tabase from the "Rotten Tomatoes" movie reviews 
(see http://www.cs.cornell.edu/people/pabo/movie-
review-data). 
We present two supervised machine learning 
approaches in our development of sentence-level 
subjectivity detection: Fuzzy Control System 
(FCS), and Adaptive Neuro-Fuzzy Inference Sys-
tem (ANFIS). Even though these methods are pop-
ular in pattern recognition, they have not been 
thoroughly investigated for subjectivity analysis. 
We present a novel ?Pruned ICF Weighting Coef-
ficient,? which improves the accuracy for subjec-
tivity detection. Our feature extraction algorithm 
calculates a feature vector based on statistical oc-
currences of words in the corpus without any lexi-
cal knowledge. For this reason, the machine 
learning models can be applied to any language; 
i.e., there is no lexical, grammatical, syntactical 
analysis used in the classification process.  
2 Related work 
In recent years, several different supervised and 
unsupervised learning algorithms were investigated 
for defining subjective information in text or 
speech.  
Riloff and Wiebe (2003) presented a bootstrap-
ping method to learn subjectivity classifiers from a 
collection of non-annotated texts.  Wiebe and 
Riloff (2005) used a similar method, but they also 
learned objective expressions apart from subjective 
expressions.  
Pang and Lee (2004) proposed a MinCut based 
algorithm to classify each sentence as being sub-
jective or objective. The goal of this research was 
to remove objective sentences from each review to 
improve document-level sentiment classification 
(82.8% improved to 86.4%).   
108
Grefenstette et al (2004) presented a Web min-
ing method for identifying subjective adjectives.  
Wilson et al (2004) and Kim et al (2005) pre-
sented methods of classifying the strength of opin-
ion being expressed in individual clauses (or 
sentences). 
Riloff et al (2006) defined subsumption rela-
tionships among unigrams, n -grams, and lexico-
syntactic patterns. They found that if a feature is 
subsumed by another, the subsumed feature is not 
needed.  The subsumption hierarchy reduces a fea-
ture set and reduced feature sets can improve clas-
sification performance. 
Raaijmakers et al(2008) investigated the use of 
prosodic features, word n -grams, character n -
grams, and phoneme n -grams for subjectivity 
recognition and polarity classification of dialog 
acts in multiparty conversation. They found that 
for subjectivity recognition, a combination of pro-
sodic, word-level, character-level, and phoneme-
level information yields the best performance and 
for polarity classification, the best performance is 
achieved with a combination of words, characters 
and phonemes.   
Murray and Carenini (2009) proposed to learn 
subjective patterns from both labeled and unla-
beled data using n -gram word sequences with 
varying level of lexical instantiation. They showed 
that learning subjective trigrams with varying in-
stantiation levels from both annotated and raw data 
can improve subjectivity detection and polarity 
labeling for meeting speech and email threads. 
Martineau and Finin (2009) presented Delta 
TFIDF, an intuitive general purpose technique, to 
efficiently weight word scores before 
classification. They compared SVM Difference of 
TFIDFs and SVM Term Count Baseline results for 
subjectivity classification. As a result, they showed 
that SVM based on Delta TFIDF gives high 
accuracy and low variance.  
Barbosa and Feng (2010) classified the subjec-
tivity of tweets (postings on Twitter) based on two 
kind of features: meta-information about the words 
on tweets and characteristics of how tweets are 
written.  
Yulan He (2010) proposed subjLDA for sen-
tence-level subjectivity detection by modifying the 
latent Dirichlet alocation (LDA) model through 
adding an additional layer to model sentence-level 
subjectivity labels. 
Benamara et al (2011) proposed subjectivity 
classification at the segment level for discourse-
based sentiment analysis. They classified each 
segment into four classes, S, OO, O and SN, where 
S segments are segments that contain explicitly 
lexicalized subjective and evaluative expressions, 
OO segments are positive or negative opinion im-
plied in an objective segment, O segments contain 
neither a lexicalized subjective term nor an implied 
opinion, SN segments are subjective, though non-
evaluative, segments that are used to introduce 
opinions.  
Remus (2011) showed that by using readability 
formulae and their combinations as features in 
addition to already well-known subjectivity clues 
leads to significant accuracy improvements in 
sentence-level subjectivity classification. 
Lin et al (2011) presented a hierarchical Bayes-
ian model based on latent Dirichlet alocation, 
called subjLDA, for sentence-level subjectivity 
detection, which automatically identifies whether a 
given sentence expresses opinion or states facts. 
All the aforementioned work focused on English 
data and most of them used an English subjectivity 
dictionary. Recently, there has been some work on 
subjectivity classification of sentences in Japanese 
(Kanayama et al, 2006), Chinese (Zagibalov et al, 
2008; Zhang et al, 2009), Romanian (Banea et al, 
2008; Mihalcea et al, 2007), Urdu (Mukund and 
Srihari, 2010), Arabic (Abdul-Mageed et al, 2011) 
and others based on different machine learning 
algorithms using general and language specific 
features.  
Mihalcea et al, (2007) and Banea et al, (2008) 
investigated methods to automatically generate 
resources for subjectivity analysis for a new target 
language by leveraging the resources and tools 
available for English. Another approach (Banea et 
al., 2010) used a multilingual space with meta clas-
sifiers to build high precision classifiers for subjec-
tivity classification. 
Recently, there has been some work focused on 
finding features that can be applied to any lan-
guage. For example, Mogadala and Varma (2012) 
presented sentence-level subjectivity classification 
using language independent feature weighting and 
performed experiments on 5 different languages 
including English and a South Asian language 
(Hindi).  
109
Rustamov et. al., (2013) applied hybrid Neuro-
Fuzzy and HMMs to document level sentiment 
analysis of movie reviews.  
In the current work, our main goal is to apply 
supervised methods based on language independ-
ent features for classification of subjective and ob-
jective sentences.  
3 Feature Extraction 
Most language independent feature extraction al-
gorithms are based on the presence or occurrence 
statistics within the corpus. We describe such an 
algorithm which is intuitive, computationally effi-
cient, and does not require either additional human 
annotation or lexical knowledge.  
We use a subjectivity dataset 1v.0: 5000 subjec-
tive and 5000 objective processed sentences in 
movie reviews [Pang/Lee ACL 2004].  
As our target does not use lexical knowledge, 
we consider every word as one code word. In our 
algorithm we do not combine verbs in different 
tenses, such as present and past  ("decide" vs "de-
cided") nor nouns as singular or plural ("fact" vs 
"facts"). Instead, we consider them as the different 
code words. 
Below, we describe some of the parameters: 
? N  is the number of classes ( in our problem 
N=2: i.e. subjective and objective classes); 
? M is the number of different words (terms) 
in the corpus; 
? R is the number of observed sequences in 
the training process; 
? ? ?rTrrr roooO ,...,, 21?   are the sentences in the 
training dataset, where 
rT  is the length of r-
th sentence, Rr ,...,2,1? ; 
? 
ji ,?  describes the association between i-th 
term (word) and the j-th class 
? ?NjMi ,...,2,1;,...,1 ?? ;  
? 
jic ,
 is the number of times i-th term oc-
curred in the j-th class; 
? 
?? j jii ct ,
 denotes the occurrence times of 
the i-th term in the corpus; 
? frequency of the i -th term in the j -th class 
i
ji
ji t
cc ,, ?
; 
We present a new weighting coefficient, which 
affects the accuracy of the system, so that instead 
of the number of documents we take the number of 
classes in the well-known IDF (Inverse-Document 
Frequency) formula. Similar to IDF, we call it 
Pruned ICF (Inverse-Class Frequency) 
???
?
???
??
i
i dN
NICF 2log
, 
where i  is a term, idN  is the number of classes 
containing the term i , which qc ji ?, , where  
Nq ?? ?
1 . 
The value of  ?  is found empirically with 
4.1??  being best for the corpus investigated.  
The membership degree of the terms (
ji,? ) for 
appropriate classes can be estimated by experts or 
can be calculated by analytical formulas. Since a 
main goal is to avoid using human annotation or 
lexical knowledge, we calculated the membership 
degree of each term by an analytical formula as 
follows ? ?NjMi ,...,2,1;,...,1 ?? : 
TF:   
?
?
? N
v
vi
ji
ji
c
c
1
,
,
,?
;   (1) 
ICFTF ? :   
?
?
?
?
? N
v
vvi
jji
ji
ICFc
ICFc
1
,
,
,?
;  (2) 
4 Subjectivity detection using Fuzzy Con-
trol System  
We use a statistical approach for estimation of the 
membership function, instead of expert knowledge, 
at the first stage. Then we apply fuzzy operations 
and modify parameters by the back-propagation 
algorithm.  
We now introduce our algorithm ( Rr ,...,2,1? ).  
1. The membership degree of terms (
r ji ,? ) of the 
r -th sentence are calculated from formulas (1)-(2).  
2. Maximum membership degree is found with 
respect to the  classes for every term of the r-th 
sentence  
 
.,...,1
,maxarg
,
,1
,,
Mi
j rviN
r
ji
r
ji
?
?
?
??
?
??
?
   (3) 
3. Means of maxima are calculated for all clas-
ses: 
110
 ? ?
.,...,1
,max:
,
,
1
,
,
Nj
iZ
T
r
vi
N
r
ji
r
j
r
Zk
r
jk
r
j
r
j
?
??
?
??
?
?
??
?
?
?
     (4) 
We use the Center of Gravity Defuzzification 
(CoGD) method for the defuzzification operation.  
Objective and subjective sentences selected ac-
cording to classes are trained by a fuzzy control 
model. The objective function is defined as follows 
(Aida-zade et. al, 2012): 
? ?
NRy
R
r
rN
j
r
j
N
j
j
r
j
d
y
yE
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?? min
2
1
1
2
1
1
?
?
,    (5) 
? ?Nyyyy ,...,, 21? , ? ?Ndr ,...,2,1? desired output. 
The partial derivatives of this function are 
calculated in following form:   
? ?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
? R
r
rN
j
r
j
N
j
j
r
j
N
j
r
j
r
t
t
d
y
y
yE
1
1
1
1
?
?
?
? , N1,2,...,t ? . 
Function (5) is minimized by the conjugate 
gradient method with the defined optimal values of 
*y .  
Rounding of y  shows the index of the classes 
obtained in the result: 
?
?
?
?
?
N
j
j
N
j
jj y
y
1
1
*
?
?
.        (6) 
Acceptance strategy (s): 
? ?
??
? ??????? otherwisereject
iiyifIis sss ,
,, 11, 
where 
si  is the index of the appropriate class, 
? ?NI ,...,2,1? . Here ? ?5.0;01??  is the main quan-
tity, which influences the reliability of the system. 
It is straightforward to check which feature vec-
tor gives the best results for FCS. Table 1. shows 
average accuracy over 10 fold cross validation of 
FCS based on (1)-(2) features in the  non-restricted 
case. Note that these results depend on the classifi-
cation method these results might be different for 
different classifiers. 
 
Features Accuracy (%) 
 TF 89.87 
ICFTF ?  91.3 
Table 1. Results of  FCS based on TF and 
ICFTF ? features. 
 
We also checked FCS based on Delta TFIDF 
features (Martineau and Finin, 2009). As DeltaIDF 
weighting coefficients of both classes are the same, 
application of DeltaIDF weighting does not change 
the accuracy of the FCS. As we see from Table 1., 
the accuracy of the method increases after applica-
tion of Pruned ICF weighting.  
We show results of subjectivity detection by 
FCS with different values of 
1?  based on ICFTF ?  
in Table 2. It can be seen that the rejection per-
centage is 0.01 for 5.01 ?? . In the testing process 
0.01% of the sentences have such words, which 
after pruned ICF weighting, becomes 0 and the 
system rejects such sentences. 
 
 Correct 
(%) 
Rejection 
(%) 
Error 
(%) 
3.01 ??  76.41 20.86 2.73 
4.01 ??  85.11 10.14 4.75 
5.01 ??  91.3 0.01 8.69 
 
Table 2. Average results of 10 folds cross vali-
dation accuracy of FCS based on ICFTF ? feature 
with different value of 
1? . 
5 Subjectivity detection using Adaptive 
Neuro Fuzzy Inference System  
Fig. 1 illustrates the general structure of Adap-
tive Neuro Fuzzy Inference System. In response to 
linguistic statements, the fuzzy interface block 
provides an input vector to a Multilayer Artificial 
Neural Network (MANN) (Fuller, 1995).  
We used statistical estimation of membership 
degree of terms by (2) instead of linguistic state-
ments at the first stage. Then we applied fuzzy op-
erations (3) and (4).  
 
111
 Fig. 1. The structure of ANFIS. 
MANN was applied to the output of the fuzzyfi-
cation operation. The input vector of neural net-
work is taken from the output vector of the 
fuzzyfication operation (fig. 2). Outputs of MANN 
are taken as indexes of classes appropriate to the 
sentences. MANN is trained by the back-
propagation algorithm. 
 
 
 
Fig. 2. The structure of MANN in ANFIS. 
 
We set two boundary conditions for the  ac-
ceptance decision: 
1)
2??ky , 
2) 
3~ ??? pk yy
, 
where y  is the output vector of MANN,  ky and 
py~
 are two successive maximum elements of the 
vector y , i.e. 
iNik yy ??? 1max
, 
iNi yk ??? 1maxarg
, 
iNikkip yy ??????? 1;11 max~
. 
There is shown results of subjectivity detection 
in  movie reviews by ANFIS with different values 
of 
2?  and 3? in Table 3. 
 
 Correct 
(%) 
Rejection 
(%) 
Error 
(%) 
5.0;8.0 32 ????  78.66 18.84 2.5 
5.0;5.0 32 ????  85.77 8.62 5.61 
No restriction 91.66 0.01 8.33 
Table 3. Average results of 10 folds cross valida-
tion accuracy ANFIS based on ICFTF ?  for sub-
jectivity detection in movie reviews. 
 
The accuracy of the ANFIS (91.66%) is higher 
than that of FCS (91.3%) at the cost of additional 
variables being required in the middle layer of the 
neural network.  
 
6 Conclusion  
We have described two different classification sys-
tem structures, FCS, ANFIS, and applied them to 
sentence-level subjectivity detection in a movie 
review data base. We have specifically shown how 
to train and test these methods  for classification of 
sentences as being either objective or subjective. A 
goal of the research was to formulate methods that 
did not depend on linguistic knowledge and there-
fore would be applicable to any language. An im-
portant component of these  methods is the feature 
extraction process. We focused on analysis of  in-
formative features that improve the accuracy of the 
systems with no language-specific constraints. As 
a result,  a novel  "Pruned ICF Weighting Func-
tion" was devised with a parameter specifically 
estimated for the subjectivity data set. 
When comparing the current system with others, 
it is necessary to emphasize that the use of linguis-
tic knowledge does improve accuracy. Since we do 
not use such  knowledge, our results should only 
be compared with other methods having similar 
constraints, such as those which use features based 
on bags of words that are tested on the same data 
set. Examples include studies by  Pang and Lee 
(2004) and Martineau and Finin (2009). Pang and 
Lee report 92% accuracy on sentence-level subjec-
tivity classification using Na?ve Bayes classifiers 
and 90% accuracy using SVMs on the same data 
set. Martineau and Finin (2009) reported 91.26% 
accuracy using SVM Difference of TFIDFs. The 
currently reported results: FCS (91.3%), ANFIS 
(91.7%) are similar. However, our presented meth-
ods have some advantages. Because the function 
(5) is minimized only with respect to 
? ?Nyyyy ,...,, 21?  (in the defined problem N=2), 
FCS is the fastest algorithm among supervised ma-
chine learning methods. At the cost of additional 
variables added within the middle layer of the neu-
ral network, ANFIS is able to improve accuracy a 
112
small amount. It is anticipated that when IF-THEN 
rules and expert knowledge are inserted into 
ANFIS and FCS, accuracy will improve to a level 
commensurate with human judgment.  
References  
Aditya Mogadala. Vasudeva Varma. 2012. Lan-
guage Independent Sentence-Level Subjectivity 
Analysis with Feature Selection. Proceedings of 
the 26th Pacific Asia Conference on Lan-
guage,Information and Computation, pages 
171?180. 
Alina Andreevskaia and Sabine Bergler. 2006. 
Mining wordnet for fuzzy sentiment: Sentiment 
tag extraction from WordNet glosses. In Pro-
ceedings of EACL 2006. 
Bing Liu. Sentiment Analysis and Opinion Mining. 
2012. Synthesis Lectures on Human Language 
Technologies. 
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: Sentiment analysis using subjectivity 
summarization based on minimum cuts. In Pro-
ceedings of the 42nd Annual Meeting on Associ-
ation for Computational Linguistics (ACL), pp. 
271-278.  
Bo Pang and Lillian Lee. 2008. Opinion Mining 
and Sentiment Analysis. Now Publishers Inc. 
Carmen Banea, Rada Mihalcea, and Janyce Wiebe. 
2010. Multilingual subjectivity: are more lan-
guages better. Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics 
(Coling 2010), pp. 28?36. 
Carmen Banea, Rada Mihalcea, Janyce Wiebe and 
Samer Hassan. 2008. Multilingual subjectivity 
analysis using machine translation. Proceedings 
of the Conference on Empirical Methods in Nat-
ural Language Processing, pp. 127?135.  
Chenghua Lin, Yulan He and Richard Everson. 
2011. Sentence Subjectivity Detection with 
Weakly-Supervised Learning. Proceedings of 
the 5th International Joint Conference on Natu-
ral Language Processing, pp. 1153?1161. 
Ellen Riloff and Janyce Wiebe. 2003. Learning 
Extraction Patterns for Subjective Expressions. 
In: Proceedings of the Conference on Empirical 
Methods in Natural Language Processing, pp. 
105?112.  
Ellen Riloff, Siddharth Patwardhan, and Janyce 
Wiebe. Feature subsumption for opinion analy-
sis. 2006. In Proceedings of the Conference on 
Empirical Methods in Natural Language Pro-
cessing (EMNLP-2006). 
Farah Benamara, Baptiste Chardon, Yannick 
Mathieu, and Vladimir Popescu. 2011. Towards 
Context-Based Subjectivity Analysis. In Pro-
ceedings of the 5th International Joint Confer-
ence on Natural Language Processing (IJCNLP-
2011). 
 Gabriel Murray and Giuseppe Carenini. 2009. 
Predicting subjectivity in multimodal conversa-
tions. In Proceedings of the Conference on Em-
pirical Methods in Natural Language 
Processing (EMNLP), pages 1348?1357. 
Gregory Grefenstette, Yan Qu, David A. Evans,  
and James G. Shanahan. 2006. Validating the 
Coverage of Lexical Resources for Affect Anal-
ysis and Automatically Classifying New Words 
along Semantic Axes. In: Proceedings of AAAI 
Spring Symposium on Exploring Attitude and 
Affect in Text: Theories and Applications, pp. 
93?107. 
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. 
Fully automatic lexicon expansion for domain-
oriented sentiment analysis. Proceedings of the 
2006 Conference on Empirical Methods in Nat-
ural Language Processing, pages 355?363. 
Janyce Wiebe and Ellen Riloff. 2005. Creating 
subjective and objective sentence classifiers 
from unannotated texts. Computational Linguis-
tics and Intelligent Text Processing, Springer, 
pp. 486?497.  
Justin Martineau, and Tim Finin. 2009. Delta 
TFIDF: An Improved Feature Space for 
Sentiment Analysis. In Proceedings of the 3rd 
AAAI International Conference on Weblogs and 
Social Media. 
Kamil Aida-zade, Samir Rustamov, Elshan Mustafayev, 
and Nigar Aliyeva, 2012. Human-Computer Dia-
logue Understanding Hybrid System. IEEE  Xplore, 
International Symposium on Innovations in Intelli-
gent Systems and Applications. Trabzon, Turkey, pp. 
1-5. 
Luciano Barbosa and Junlan Feng. 2010. Robust 
sentiment detection on twitter from biased and 
noisy data. In Proceedings of the International 
Conference on Computational Linguistics (CO 
LING-2010). 
Muhammad Abdul-Mageed, Mona T. Diab, and 
Mohammed Korayem. 2011. Subjectivity and 
sentiment analysis of modern standard Arabic, 
In Proceedings of the 49th Annual Meeting of 
113
the Association for Computational Linguistics: 
short papers, pages 587?591. 
Rada Mihalcea, Carmen Banea and Janyce Wiebe. 
2007. Learning multilingual subjective language 
via cross-lingual projections. Proceedings of the 
45th Annual Meeting of the Association of Com-
putational Linguistics, pages 976?983. 
Robert Fuller. Neural Fuzzy Systems, 1995. 
Robert Remus. 2011. Improving Sentence-level 
Subjectivity Classification through Readability 
Measurement. NODALIDA-2011 Conference 
Proceedings, pp. 168?174. 
Samir Rustamov, Elshan Mustafayev, Mark 
Clements. 2013. Sentiment Analysis using 
Neuro-Fuzzy and Hidden Markov Models of 
Text. IEEE Southeastcon 2013, Jacksonvilla, 
Florida,USA. 
Smruthi Mukund and Rohini K. Srihari. 2010. A 
vector space model for subjectivity classifica-
tion in Urdu aided by co-training. In Proceed-
ings of Coling 2010: Poster Volume, pages 860?
868. 
Soo-Min Kim and Eduard Hovy. 2005. Automatic 
Detection of Opinion Bearing Words and Sen-
tences. In: Companion Volume to the Proceed-
ings of the International Joint Conference on 
Natural Language Processing, pp. 61?66. 
Stephan Raaijmakers, Khiet Truong, and Theresa 
Wilson. 2008. Multimodal subjectivity analysis 
of multiparty conversation. In Proceedings of 
the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 466?
474. 
Taras Zagibalov and John Carroll. 2008. Unsuper-
vised classification of sentiment and objectivity 
in Chinese text. In Proceedings of International 
Joint Conference on Natural Language Pro-
cessing (IJCNLP-2008), pp. 304?311. 
Theresa Wilson, Janyce Wiebe, Rebecca Hwa. 
2004. Just How Mad Are You? Finding Strong 
and Weak Opinion Clauses. In: Proceedings of 
the National Conference on Artificial Intelli-
gence, pp. 761?769. 
Yulan He. 2010. Bayesian Models for Sentence-
Level Subjectivity Detection. Technical Report 
KMI-10-02,  June 2010. 
Ziqiong Zhang, Qiang Ye, Rob Law, and Yijun Li. 
2009. Automatic Detection of Subjective Sentences 
Based on Chinese Subjective Patterns. Proceedings  
of 20th International Conference, MCDM-2009, 
pp. 29-36.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
114
