Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 217?221,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
TopicSpam: a Topic-Model-Based Approach for Spam Detection
Jiwei Li , Claire Cardie
School of Computer Science
Cornell University
Ithaca, NY, 14853
jl3226@cornell.edu
cardie@cs.cornell.edu
Sujian Li
Laboratory of Computational Linguistics
Peking University
Bejing, P.R.China, 150001
lisujian@pku.edu.cn
Abstract
Product reviews are now widely used by
individuals and organizations for decision
making (Litvin et al, 2008; Jansen, 2010).
And because of the profits at stake, peo-
ple have been known to try to game the
system by writing fake reviews to promote
target products. As a result, the task of de-
ceptive review detection has been gaining
increasing attention. In this paper, we pro-
pose a generative LDA-based topic mod-
eling approach for fake review detection.
Our model can aptly detect the subtle dif-
ferences between deceptive reviews and
truthful ones and achieves about 95% ac-
curacy on review spam datasets, outper-
forming existing baselines by a large mar-
gin.
1 Introduction
Consumers rely increasingly on user-generated
online reviews to make purchase decisions. Pos-
itive opinions can result in significant financial
gains. This gives rise to deceptive opinion spam
(Ott et al, 2011; Jindal et al, 2008), fake reviews
written to sound authentic and deliberately mis-
lead readers. Previous research has shown that
humans have difficulty distinguishing fake from
truthful reviews, operating for the most part at
chance (Ott et al, 2011). Consider, for example,
the following two hotel reviews. One is truthful
and the other is deceptive1:
1. My husband and I stayed for two nights at the Hilton
Chicago. We were very pleased with the accommoda-
tions and enjoyed the service every minute of it! The
bedrooms are immaculate, and the linens are very soft.
We also appreciated the free wifi, as we could stay in
touch with friends while staying in Chicago. The bath-
room was quite spacious, and I loved the smell of the
shampoo they provided. Their service was amazing,
1The first example is a deceptive review.
and we absolutely loved the beautiful indoor pool. I
would recommend staying here to anyone.
2. We stayed at the Sheraton by Navy Pier the first week-
end of November. The view from both rooms was spec-
tacular (as you can tell from the picture attached). They
also left a plate of cookies and treats in the kids room
upon check-in made us all feel very special. The hotel
is central to both Navy Pier and Michigan Ave. so we
walked, trolleyed, and cabbed all around the area. We
ate the breakfast buffet on both mornings and thought
it was pretty good. The eggs were a little runny. Our
six year old ate free and our two eleven year old were
$14 (instead of the adult $20). The rooms were clean,
the concierge and reception staff were both friendly
and helpful...we will definitely visit this Sheraton again
when we stay in Chicago next time.
Because of the difficulty of recognizing deceptive
opinions, there has been a widespread and growing
interest in developing automatic, usually learning-
based methods to help users identify deceptive re-
views (Ott et al, 2011; Jindal et al, 2008; Jindal
et al, 2010; Li et al, 2011; Lim et al, 2011; Wang
et al, 2011).
The state-of-the-art approach treats the task of
spam detection as a text categorization prob-
lem and was first introduced by Jindal and Liu
(2009) who trained a supervised classifier to dis-
tinguish duplicated reviews (assumed deceptive)
from original ones (assumed truthful). Since then,
many supervised approaches have been proposed
for spam detection. Ott et al (2011) employed
standard word and part-of-speech (POS) n-gram
features for supervised learning and built a gold?
standard opinion dataset of 800 reviews. Lim et
al. (2010) proposed the inclusion of user behavior-
based features and found that behavior abnormali-
ties of reviewers could predict spammers, without
using any textual features. Li et al (2011) care-
fully explored review-related features based on
content and sentiment, training a semi-supervised
classifier for opinion spam detection. However,
the disadvantages of standard supervised learning
methods are obvious. First, they do not gener-
ally provide readers with a clear probabilistic pre-
217
diction of how likely a review is to be deceptive
vs. truthful. Furthermore, identifying features that
provide direct evidence against deceptive reviews
is always a hard problem.
LDA topic models (Blei et al, 2003) have
widely been used for their ability to model latent
topics in document collection. In LDA, each docu-
ment is presented as a mixture distribution of top-
ics and each topic is presented as a mixture distri-
bution of words. Researchers also integrated dif-
ferent levels of information into LDA topic mod-
els to model the specific knowledge that they are
interested in, such as user-specific information
(Rosen-zvi et al, 2006), document-specific infor-
mation (Li et al, 2010) and time-specific infor-
mation (Diao et al, 2012). Ramage et al (2009)
developed a Labeled LDA model to define a one-
to-one correspondence between LDA latent topics
and tags. Chemudugunta et al (2008) illustrated
that by considering background information and
document-specific information, we can largely im-
prove the performance of topic modeling.
In this paper, we propose a Bayesian approach
called TopicSpam for deceptive review detection.
Our approach, which is a variation of Latent
Dirichlet Allocation (LDA) (Blei et al, 2003),
aims to detect the subtle differences between the
topic-word distributions of deceptive reviews vs.
truthful ones. In addition, our model can give
a clear probabilistic prediction on how likely a
review should be treated as deceptive or truth-
ful. Performance is tested on dataset from Ott et
al.(2011) that contains 800 reviews of 20 Chicago
hotels. Our model achieves more than 94% accu-
racy on that dataset.
2 TopicSpam
We are presented with four subsets of ho-
tel reviews, M = {Mi}i=4i=1, representing
deceptive train, truthful train, deceptive test
and truthful test data, respectively. Each re-
view r is comprised of a number of words r =
{wt}t=nrt=1 . Input for the TopicSpam algorithm is
the datasets M ; output is the label (deceptive,
truthful) for each review inM3 andM4. V denotes
vocabulary size.
2.1 Details of TopicSpam
In TopicSpam, each document is modeled as a
bag of words, which are assumed to be gener-
ated from a mixture of latent topics. Each word
is associated with a latent variable that specifies
Figure 1: Graphical Model for TopicSpam
the topic from which it is generated. Words in a
document are assumed to be conditionally inde-
pendent given the hidden topics. A general back-
ground distribution ?B and hotel-specific distri-
butions ?Hj (j = 1, ..., 20) are first introduced
to capture the background information and hotel-
specific information. To capture the difference
between deceptive reviews and truthful reviews,
TopicSpam also learns a deceptive topic distribu-
tion ?D and truthful topic distribution ?T . The
generative model of TopicSpam is shown as fol-
lows:
? For a training review in r1j ? M1, words are
originated from one of the three different top-
ics: ?B , ?Hj and ?D.
? For a training review in r2j ? M2, words are
originated from one of the three different top-
ics: ?B , ?Hj and ?T .
? For a test review in rmj ? Mm,m = 3, 4,
words are originated from one of the four dif-
ferent topics: ?B , ?Hj ?D and ?T .
The generation process of TopicSpam is shown
in Figure 1 and the corresponding graphical
model is illustrated in Figure 2. We use
? = (?G, ?Hi , ?D, ?T ) to represent the asym-
metric priors for topic-word distribution genera-
tion. In our experiments, we set ?G = 0.1,
and ?Hi = ?D = ?T = 0.01. The intu-
ition for the asymmetric priors is that there should
be more words assigned to the background topic.
? = [?B, ?Hi , ?D, ?T ] denotes the priors for
the document-level topic distribution in the LDA
model. We set ?B = 2 and ?T = ?D = ?Hi = 1,
reflecting the intuition that more words in each
document should cover the background topic.
2.2 Inference
We adopt the collapsed Gibbs sampling strategy to
infer the latent parameters in TopicSpam. In Gibbs
218
1. sample ?G ? Dir(?G)
2. sample ?D ? Dir(?D)
3. sample ?T ? Dir(?T )
4. for each hotel j ? [1, N ]: sample ?Hj ? ?H
5. for each review r
if i=1: sample ?r ? Dir(?B, ?Hj , ?D)
if i=2: sample ?r ? Dir(?B, ?Hj , ?T )
if i=3: sample ?r ? Dir(?B, ?Hj , ?D, ?T )
if i=4: sample ?r ? Dir(?B, ?Hj , ?D, ?T )
for each word w in R
sample z ? ?r sample w ? ?z
Figure 2: Generative Model for TopicSpam
sampling, for each word w in review r, we need
to calculate P (zw|w, z?w, ?, ?) in each iteration,
where z?w denotes the topic assignments for all
words except that of the current word zw.
P (zw = m|z?w, i, j, ?, ?)
Nmr + ?m?
m?(Nm
?
r + ??m)
? E
w
m + ?m?V
w? Ewm + V ?m
(1)
where Nmr denotes the number of times that topic
m appears in current review r and Ewm denotes the
number of times that word w is assigned to topic
m. After each sampling iteration, the latent pa-
rameters can be estimated using the following for-
mulas:
?mr =
Nmr + ?m?
m?(Nm
?
r + ?m)
?(w)m =
Ewm + ?m?
w? Ew
?
m + V ?m(2)
2.3 Labeling the Test Data
For each review r in the test data, let NDr denote
the number of words generated from the decep-
tive topic and NTr , the number of words generated
from the truthful topic. The decision for whether a
review is deceptive or truthful is made as follows:
? if NDr > NTr , r is deceptive.
? if NDr < NTr , r is truthful.
? if NDr = NTr , it is hard to decide.
Let P(D) denote the probability that r is deceptive
and P(T) denote the probability that r is truthful.
P (D) = N
D
r
NDr +NTr
P (T ) = N
T
r
NDr +NTr
(3)
3 Experiments
3.1 System Description
Our experiments are conducted on the dataset
from Ott et al(2011), which contains reviews of
the 20 most popular hotels on TripAdvisor in the
Chicago areas. There are 20 truthful and 20 decep-
tive reviews for each of the chosen hotels (800 re-
views total). Deceptive reviews are gathered using
Amazon Mechanical Turk2. In our experiments,
we adopt the same 5-fold cross-validation strat-
egy as in Ott et al, using the same data partitions.
Words are stemmed using PorterStemmer3.
3.2 Baselines
We employ a number of techniques as baselines:
TopicTD: A topic-modeling approach that only
considers two topics: deceptive and truthful.
Words in deceptive train are all generated from
the deceptive topic and words in truthful train
are generated from the truthful topic. Test docu-
ments are presented with a mixture of the decep-
tive and truthful topics.
TopicTDB: A topic-modeling approach that
only considers background, deceptive and truthful
information.
SVM-Unigram: Using SVMlight(Joachims,
1999) to train linear SVM models on unigram fea-
tures.
SVM-Bigram: Using SVMlight(Joachims,
1999) to train linear SVM models on bigram fea-
tures.
SVM-Unigram-Removal1: In SVM-Unigram-
Removal, we first train TopicSpam. Then words
generated from hotel-specific topics are removed.
We use the remaining words as features in SVM-
light.
SVM-Unigram-Removal2: Same as SVM-
Unigram-removal-1 but removing all background
words and hotel-specific words.
Experimental results are shown in Table 14.
As we can see, the accuracy of TopicSpam is
0.948, outperforming TopicTD by 6.4%. This il-
lustrates the effectiveness of modeling background
and hotel-specific information for the opinion
spam detection problem. We also see that Top-
icSpam slightly outperforms TopicTDB, which
2https://www.mturk.com/mturk/.
3http://tartarus.org/martin/PorterStemmer/
4Reviews with NDr = NTr are regarded as incorrectly
classified by TopicSpam.
219
Approach Accuracy T-P T-R T-F D-P D-R D-F
TopicSpam 0.948 0.954 0.942 0.944 0.941 0.952 0.946
TopicTD 0.888 0.901 0.878 0.889 0.875 0.897 0.886
TopicTDB 0.931 0.938 0.926 0.932 0.925 0.937 0.930
SVM-Unigram 0.884 0.899 0.865 0.882 0.870 0.903 0.886
SVM-Bigram 0.896 0.901 0.890 0.896 0.891 0.903 0.897
SVM-Unigram-Removal1 0.895 0.906 0.889 0.898 0.887 0.907 0.898
SVM-Unigram-Removal2 0.822 0.852 0.806 0.829 0.793 0.840 0.817
Table 1: Performance for different approaches based on nested 5-fold cross-validation experiments.
neglects hotel-specific information. By check-
ing the results of Gibbs sampling, we find that
this is because only a small number of words
are generated by the hotel-specific topics. Top-
icTD and SVM-Unigram get comparative accu-
racy rates. This can be explained by the fact
that both models use unigram frequency as fea-
tures for the classifier or topic distribution train-
ing. SVM-Unigram-Removal1 is also slightly
better than SVM-Unigram. In SVM-Unigram-
removal1, hotel-specific words are removed for
classifier training. So the first-step LDA model
can be viewed as a feature selection process for the
SVM, giving rise to better results. We can also see
that the performance of SVM-Unigram-removal2
is worse than other baselines. This can be ex-
plained as follows: for example, word ?my? has
large probability to be generated from the back-
ground topic. However it can also be generated by
deceptive topic occasionaly but can hardly be gen-
erated from the truthful topic. So the removal of
these words results in the loss of useful informa-
tion, and leads to low accuracy rate.
Our topic-modeling approach uses word fre-
quency as features and does not involve any fea-
ture selection process. Here we present the re-
sults of the sample reviews from Section 1. Stop
words are labeled in black, background topics (B)
in blue, hotel specific topics (H) in orange, de-
ceptive topics (D) in red and truthful topic (T) in
green.
1. My husband and I stayed for two nights at the Hilton
Chicago. We were very pleased with the accommoda-
tions and enjoyed the service every minute of it! The
bedrooms are immaculate,and the linens are very soft.
We also appreciated the free wifi, as we could stay in
touch with friends while staying in Chicago. The bath-
room was quite spacious, and I loved the smell of the
shampoo they provided not like most hotel shampoos.
Their service was amazing,and we absolutely loved the
beautiful indoor pool. I would recommend staying here
to anyone.
[B,H,D,T]=[41,6,10,1] p(D)=0.909 P(T)=0.091
2. We stayed at the Sheraton by Navy Pier the first week-
end of November. The view from both rooms was spec-
tacular (as you can tell from the picture attached). They
also left a plate of cookies and treats in the kids room
upon check-in made us all feel very special. The ho-
tel is central to both Navy Pier and Michigan Ave. so
we walked, trolleyed, and cabbed all around the area.
We ate the breakfast buffet both mornings and thought
it was pretty good. The eggs were a little runny. Our
six year old ate free and our two eleven year old were
$14 ( instead of the adult $20) The rooms were clean,
the concierge and reception staff were both friendly
and helpful...we will definitely visit this Sheraton again
when we?re in Chicago next time.
[B,H,D,T]=[80,15,3,18] p(D)=0.143 P(T)=0.857
background deceptive truthful Hilton
hotel hotel room Hilton
stay my ) palmer
we chicago ( millennium
room will but lockwood
! room $ park
Chicago very bathroom lobby
my visit location line
great husband night valet
I city walk shampoo
very experience park dog
Omni Amalfi Sheraton James
Omni Amalfi tower James
pool breakfast Sheraton service
plasma view pool spa
sundeck floor river bar
chocolate bathroom lake upgrade
indoor cocktail navy primehouse
request morning indoor design
pillow wine shower overlook
suitable great kid romantic
area room theater home
Table 2: Top words in different topics from Topic-
Spam
4 Conclusion
In this paper, we propose a novel topic model
for deceptive opinion spam detection. Our model
achieves an accuracy of 94.8%, demonstrating its
effectiveness on the task.
5 Acknowledgements
We thank Myle Ott for his insightful comments and sugges-
tions. This work was supported in part by NSF Grant BCS-
0904822, a DARPA Deft grant, and by a gift from Google.
220
References
David Blei, Andrew Ng and Micheal Jordan. Latent
Dirichlet alocation. 2003. In Journal of Machine
Learning Research.
Carlos Castillo, Debora Donato, Luca Becchetti, Paolo
Boldi, Stefano Leonardi Massimo Santini, and Se-
bastiano Vigna. A reference collection for web
spam. In Proceedings of annual international ACM
SIGIR conference on Research and development in
information retrieval, 2006.
Chaltanya Chemudugunta, Padhraic Smyth and Mark
Steyers. Modeling General and Specific Aspects of
Documents with a Probabilistic Topic Model.. In
Advances in Neural Information Processing Systems
19: Proceedings of the 2006 Conference.
Paul-Alexandru Chirita, Jorg Diederich, and Wolfgang
Nejdl. MailRank: using ranking for spam detection.
In Proceedings of ACM international conference on
Information and knowledge management. 2005.
Harris Drucke, Donghui Wu, and Vladimir Vapnik.
2002. Support vector machines for spam categoriza-
tion. In Neural Networks.
Qiming Diao, Jing Jiang, Feida Zhu and Ee-Peng Lim.
In Proceeding of the 50th Annual Meeting of the As-
sociation for Computational Linguistics. 2012
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical. In Advances in
kernel methods.
Jack Jansen. 2010. Online product research. In Pew In-
ternet and American Life Project Report.
Nitin Jindal, and Bing Liu. Opinion spam and analysis.
2008. In Proceedings of the international conference
on Web search and web data mining
Nitin Jindal, Bing Liu, and Ee-Peng Lim. Finding
Unusual Review Patterns Using Unexpected Rules.
2010. In Proceedings of the 19th ACM international
conference on Information and knowledge manage-
ment
Pranam Kolari, Akshay Java, Tim Finin, Tim Oates and
Anupam Joshi. Detecting Spam Blogs: A Machine
Learning Approach. In Proceedings of Association
for the Advancement of Artificial Intelligence. 2006.
Peng Li, Jing Jiang and Yinglin Wang. 2010. Gener-
ating templates of entity summaries with an entity-
aspect model and pattern mining. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics.
Fangtao Li, Minlie Huang, Yi Yang, and Xiaoyan Zhu.
Learning to identify review Spam. 2011. In Proceed-
ings of the Twenty-Second international joint confer-
ence on Artificial Intelligence.
Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing Liu,
and Hady Wirawan Lauw. Detecting Product Re-
view Spammers Using Rating Behavior. 2010. In
Proceedings of the 19th ACM international confer-
ence on Information and knowledge management.
Stephen Litvina, Ronald Goldsmithb and Bing Pana.
2008. Electronic word-of-mouth in hospitality
and tourism management. Tourism management,
29(3):458468.
Juan Martinez-Romo and Lourdes Araujo. Web Spam
Identification Through Language Model Analysis.
In AIRWeb. 2009.
Arjun Mukherjee, Bing Liu and Natalie Glance. Spot-
ting Fake Reviewer Groups in Consumer Reviews.
In Proceedings of the 18th international conference
on World wide web, 2012.
Alexandros Ntoulas, Marc Najork, Mark Manasse and
Dennis Fetterly. Detecting Spam Web Pages through
Content Analysis. In Proceedings of international
conference on World Wide Web 2006
Myle Ott, Yejin Choi, Claire Cardie and Jeffrey Han-
cock. Finding deceptive opinion spam by any stretch
of the imagination. 2011. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
Bo Pang and Lillian Lee. Opinion mining and senti-
ment analysis. In Found. Trends Inf. Retr.
Daniel Ramage, David Hall, Ramesh Nallapati and
Christopher D. Manning. Labeled LDA: a super-
vised topic model for credit attribution in multi-
labeled corpora. 2009. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing 2009.
Michal Rosen-zvi, Thomas Griffith, Mark Steyvers and
Padhraic Smyth. The author-topic model for authors
and documents. In Proceedings of the 20th confer-
ence on Uncertainty in artificial intelligence.
Guan Wang, Sihong Xie, Bing Liu and Philip Yu. Re-
view Graph based Online Store Review Spammer
Detection. 2011. In Proceedings of 11th Interna-
tional Conference of Data Mining.
Baoning Wu, Vinay Goel and Brian Davison. Topical
TrustRank: using topicality to combat Web spam.
In Proceedings of international conference on World
Wide Web 2006 .
Kyang Yoo and Ulrike Gretzel. 2009. Compari-
son of Deceptive and Truthful Travel Reviews.
InInformation and Communication Technologies in
Tourism 2009.
221
