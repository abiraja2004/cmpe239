Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 71?75,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
English to Indian Languages Machine Transliteration System at 
NEWS 2010 
Amitava Das1, Tanik Saikh2, Tapabrata Mondal3, Asif Ekbal4, Sivaji Bandyopadhyay5 
Department of Computer Science and Engineering1,2,3,5 
Jadavpur University,  
Kolkata-700032, India  
amitava.santu@gmail.com1, tanik4u@gmail.com2, tapabratamon-
dal@gmail.com3, sivaji_cse_ju@yahoo.com5  
Department of Computational Linguistics4 
University of Heidelberg 
Im Neuenheimer Feld 325 
69120 Heidelberg, Germany 
ekbal@cl.uni-heidelberg.de 
 
Abstract 
 
This paper reports about our work in the 
NEWS 2010 Shared Task on Transliteration 
Generation held as part of ACL 2010. One 
standard run and two non-standard runs were 
submitted for English to Hindi and Bengali 
transliteration while one standard and one non-
standard run were submitted for Kannada and 
Tamil. The transliteration systems are based 
on Orthographic rules and Phoneme based 
technology. The system has been trained on 
the NEWS 2010 Shared Task on Translitera-
tion Generation datasets. For the standard run, 
the system demonstrated mean F-Score values 
of 0.818 for Bengali, 0.714 for Hindi, 0.663 
for Kannada and 0.563 for Tamil. The reported 
mean F-Score values of non-standard runs are 
0.845 and 0.875 for Bengali non-standard run-
1 and 2, 0.752 and 0.739 for Hindi non-
standard run-1 and 2, 0.662 for Kannada non-
standard run-1 and 0.760 for Tamil non-
standard run-1. Non-Standard Run-2 for Ben-
gali has achieved the highest score among all 
the submitted runs. Hindi Non-Standard Run-1 
and Run-2 runs are ranked as the 5th and 6th 
among all submitted Runs. 
1 Introduction 
Transliteration is the method of translating one 
source language word into another target lan-
guage by expressing and preserving the original 
pronunciation in their source language. Thus, the 
central problem in transliteration is predicting the 
pronunciation of the original word. Translitera-
tion between two languages that use the same set 
of alphabets is trivial: the word is left as it is. 
However, for languages those use different al-
phabet sets the names must be transliterated or 
rendered in the target language alphabets. Trans-
literation of words is necessary in many applica-
tions, such as machine translation, corpus align-
ment, cross-language Information Retrieval, in-
formation extraction and automatic lexicon ac-
quisition. In the literature, a number of translite-
ration algorithms are available involving English 
(Li et al, 2004; Vigra and Khudanpur, 2003; Go-
to et al, 2003), European languages (Marino et 
al., 2005) and some of the Asian languages, 
namely Chinese (Li et al, 2004; Vigra and Khu-
danpur, 2003), Japanese (Goto et al, 2003; 
Knight and Graehl, 1998), Korean (Jung et al, 
2000) and Arabic (Al-Onaizan and Knight, 
2002a; Al-Onaizan and Knight, 2002c). Recent-
ly, some works have been initiated involving 
Indian languages (Ekbal et al, 2006; Ekbal et al, 
2007; Surana and Singh, 2008). The detailed re-
port of our participation in NEWS 2009 could be 
found in (Das et al, 2009).  
One standard run for Bengali (Bengali 
Standard Run: BSR), Hindi (Hindi Standard 
Run: HSR), Kannada (Kannada Standard Run: 
KSR) and Tamil (Tamil Standard Run: TSR) 
were submitted. Two non-standard runs for Eng-
lish to Hindi (Hindi Non-Standard Run 1 & 2: 
HNSR1 & HNSR2) and Bengali (Bengali Non-
Standard Run 1 & 2: BNSR1 & BNSR1) transli-
teration were submitted. Only one non-standard 
run were submitted for Kannada (Kannada Non-
Standard Run-1: KNSR1) and Tamil (Tamil 
Non-Standard Run-1: TNSR1). 
71
2 Machine Transliteration Systems  
Five different transliteration models have been 
proposed in the present report that can generate 
the transliteration in Indian language from an 
English word. The transliteration models are 
named as Trigram Model (Tri), Joint Source-
Channel Model (JSC), Modified Joint Source-
Channel Model (MJSC), Improved Modified 
Joint Source-Channel Model (IMJSC) and Inter-
national Phonetic Alphabet Based Model (IPA). 
Among all the models the first four are catego-
rized as orthographic model and the last one i.e. 
IPA based model is categorized as phoneme 
based model. 
An English word is divided into Translitera-
tion Units (TUs) with patterns C*V*, where C 
represents a consonant and V represents a vowel. 
The targeted words in Indian languages are di-
vided into TUs with patterns C+M?, where C 
represents a consonant or a vowel or a conjunct 
and M represents the vowel modifier or matra. 
The TUs are the basic lexical units for machine 
transliteration. The system considers the English 
and Indian languages contextual information in 
the form of collocated TUs simultaneously to 
calculate the plausibility of transliteration from 
each English TU to various Indian languages 
candidate TUs and chooses the one with maxi-
mum probability. The system learns the map-
pings automatically from the bilingual NEWS 
2010 training set being guided by linguistic fea-
tures/knowledge. The output of the mapping 
process is a decision-list classifier with collo-
cated TUs in the source language and their 
equivalent TUs in collocation in the target lan-
guage along with the probability of each decision 
obtained from the training set. A Direct example 
base has been maintained that contains the bilin-
gual training examples that do not result in the 
equal number of TUs in both the source and tar-
get sides during alignment. The Direct example 
base is checked first during machine translitera-
tion of the input English word. If no match is 
obtained, the system uses direct orthographic 
mapping by identifying the equivalent TU in In-
dian languages for each English TU in the input 
and then placing the target language TUs in or-
der. The IPA based model has been used for 
English dictionary words. Words which are not 
present in the dictionary are handled by other 
orthographic models as Trigram, JSC, MJSC and 
IMJSC. 
The transliteration models are described below 
in which S and T denotes the source and the tar-
get words respectively: 
3 Orthographic Transliteration models 
The orthographic models work on the idea of 
TUs from both source and target languages. The 
orthographic models used in the present system 
are described below. For transliteration, P(T), 
i.e., the probability of transliteration in the target 
language, is calculated from a English-Indian 
languages bilingual database If, T is not found in 
the dictionary, then a very small value is 
assigned to P(T). These models have been 
desribed in details in Ekbal et al (2007). 
3.1 Trigram 
This is basically the Trigram model where the 
previous and the next source TUs are considered 
as the context.  
( | ) ( , | )1, 11
K
P S T P s t s sk k kk
= < >?
? +
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
3.2  Joint Source-Channel Model (JSC) 
This is essentially the Joint Source-Channel 
model (Hazhiou et al, 2004) where the 
previous TUs with reference to the current TUs 
in both the source (s) and the target sides (t) are 
considered as the context.  
( | ) ( , | , )11
K
P S T P s t s tk kk
= < > < >?
?
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
3.3 Modified Joint Source-Channel Model 
(MJSC) 
In this model, the previous and the next TUs in 
the source and the previous target TU are 
considered as the context. This is the Modified 
Joint Source-Channel model. 
( | ) ( , | , )1, 11
K
P S T P s t s t sk k kk
= < > < >?
? +
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
3.4 Improved Modified Joint Source-
Channel Model (IMJSC) 
In this model, the previous two and the next TUs 
in the source and the previous target TU are 
considered as the context. This is the  Improved 
Modified Joint Source-Channel model. 
72
( | ) ( , | , )1 1, 11
K
P S T P s t s s t sk k k kk
= < > < >? + ? +
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
4 International Phonetic Alphabet 
(IPA) Model 
The NEWS 2010 Shared Task on Transliteration 
Generation challenge addresses general domain 
transliteration problem rather than named entity 
transliteration. Due to large number of dictionary 
words as reported in Table 1 in NEWS 2010 data 
set a phoneme based transliteration algorithm  
has been devised.  
 Train Dev Test 
Bengali 7.77% 5.14% 6.46% 
Hindi 27.82% 15.80% 3.7% 
Kannada 27.60% 14.63% 4.4% 
Tamil 27.87% 17.31% 3.0% 
Table 1: Statistics of Dictionary Words 
The International Phonetic Alphabet (IPA) is a 
system of representing phonetic notations based 
primarily on the Latin alphabet and devised by 
the International Phonetic Association as a 
standardized representation of the sounds of 
spoken language. The machine-readable 
Carnegie Mellon Pronouncing Dictionary 1  has 
been used as an external resource to capture 
source language IPA structure. The dictionary 
contains over 125,000 words and their 
transcriptions with mappings from words to their 
pronunciations in the given phoneme set. The 
current phoneme set contains 39 distinct 
phonemes. As there is no such parallel IPA 
dictionary available for Indian languages, 
English IPA structures have been mapped to TUs 
in Indian languages during training. An example 
of such mapping between phonemes and TUs are 
shown in Table 3, for which the vowels may 
carry lexical stress as reported in Table 2. This 
phone set is based on the ARPAbet2 symbol set 
developed for speech recognition uses.  
Representation Stress level 
0 No 
1 Primary 
2 Secondary 
Table 2: Stress Level on Vowel 
A pre-processing module checks whether a 
targeted source English word is a valid 
dictionary word or not. The dictionary words are 
then handled by phoneme based transliteration 
module. 
                                                 
1
 www.speech.cs.cmu.edu/cgi-bin/cmudict 
2
 http://en.wikipedia.org/wiki/Arpabet 
Phoneme Example Translation TUs 
AA odd AA0-D - 
AH hut HH0-AH-T - 
D dee D-IY1 -?	 
Table 3: Phoneme Map Patterns of English 
Words and TUs 
In the target side we use our TU segregation 
logic to get phoneme wise transliteration pattern. 
We present this problem as a sequence labelling 
problem, because transliteration pattern changes 
depending upon the contextual phonemes in 
source side and TUs in the target side. We use a 
standard machine learning based sequence 
labeller Conditional Random Field (CRF)3 here. 
IPA based model increased the performance 
for Bengali, Hindi and Tamil languages as 
reported in Section 6. The performance has 
decreased for Kannada. 
5 Ranking 
The ranking among the transliterated outputs 
follow the order reported in Table 4: The ranking 
decision is based on the experiments as described 
in (Ekbal et al, 2006) and additionally based on 
the experiments on NEWS 2010 development 
dataset. 
Word Type  Ranking Order 1 2 3 4 5 
Dictionary IPA IMJSC MJSC JSC Tri 
Non-
Dictionary IMJSC MJSC JSC Tri - 
Table 4: Phoneme Patterns of English Words 
In BSR, HSR, KSR and TSR the orthographic 
TU based models such as: IMJSC, MJSC, JSC 
and Tri have been used only trained by NEWS 
2010 dataset. In BNSR1 and HNSR1 all the or-
thographic models have been trained with addi-
tional census dataset as described in Section 6. In 
case of BNSR2, HNSR2, KNSR1 and TNSR1 
the output of the IPA based model has been add-
ed with highest priority. As no census data is 
available for Kannada and Tamil therefore there 
is only one Non-Standard Run was submitted for 
these two languages only with the output of IPA 
based model along with the output of Standard 
Run. 
6 Experimental Results  
We have trained our transliteration models using 
the NEWS 2010 datasets obtained from the 
NEWS 2010 Machine Transliteration Shared 
Task (Li et al, 2010). A brief statistics of the 
                                                 
3
 http://crfpp.sourceforge.net 
73
datasets are presented in Table 5. During train-
ing, we have split multi-words into collections of 
single word transliterations. It was observed that 
the number of tokens in the source and target 
sides mismatched in various multi-words and 
these cases were not considered further. Follow-
ing are some examples:  
Paris Charles de Gaulle  ???? 
???? ??	?
 ? ?????  
Suven Life Scie  ??? ??
	??
 
Delta Air Lines  ???? 
???	
 
In the training set, some multi-words were 
partly translated and not transliterated. Such ex-
amples were dropped from the training set. In the 
following example the English word ?National? 
is being translated in the target as ??????. 
Australian National Univer-
sity  ????? ???? 
?????
? 
      
Set Number of examples Bng Hnd Kn Tm 
Training 11938 9975 7990 7974 
Development 992 1974 1968 1987 
Test 991 1000 1000 1000 
Table 5: Statistics of Dataset 
There is less number of known examples in 
the NEWS 2010 test set from training set. The 
exact figure is reported in the Table 6. 
 Matches with training 
Bengali 14.73% 
Hindi 0.2% 
Kannada 0.0% 
Tamil 0.0% 
Table 6: Statistics of Dataset 
If the outputs of any two transliteration models 
are same for any word then only one output are 
provided for that particular word. Evaluation re-
sults of the final system are shown in Table 7 for 
Bengali, Table 8 for Hindi, Table 9 for Kannada 
and Table 10 for Tamil.  
 
Parameters 
Accuracy 
BSR BNSR1 BNSR2 
Accuracy in top-1 0.232 0.369 0.430 
Mean F-score 0.818 0.845 0.875 
Mean Reciprocal Rank (MRR) 0.325 0.451 0.526 
Mean Average Precision 
(MAP)ref 0.232 0.369 0.430 
Table 7: Results on Bengali Test Set 
 
 
Parameters 
Accuracy 
HSR HNSR1 HNSR2 
Accuracy in top-1 0.150 0.254 0.170 
Mean F-score 0.714 0.752 0.739 
Mean Reciprocal Rank (MRR) 0.308 0.369 0.314 
Mean Average Precision 
(MAP)ref 
0.150 0.254 0.170 
Table 8: Results on Hindi Test Set 
 
Parameters 
Accuracy 
KSR KNSR1 
Accuracy in top-1 0.056 0.055 
Mean F-score 0.663 0.662 
Mean Reciprocal Rank (MRR) 0.112 0.169 
Mean Average Precision (MAP)ref 0.056 0.055 
Table 9: Results on Kannada Test Set 
 
Parameters 
Accuracy 
TSR TNSR1 
Accuracy in top-1 0.013 0.082 
Mean F-score 0.563 0.760 
Mean Reciprocal Rank (MRR) 0.121 0.142 
Mean Average Precision (MAP)ref 0.013 0.082 
Table 10: Results on Tamil Test Set 
 
The additional dataset used for the non-
standard runs is mainly the census data consist-
ing of only Indian person names that have been 
collected from the web 4 . In the BNSR1 and 
HNSR1 we have used an English-Bengali/Hindi 
bilingual census example dataset. English-Hindi 
set consist of 961,890 examples and English-
Bengali set consist of 582984 examples. This 
database contains the frequency of the corres-
ponding English-Bengali/Hindi name pair.  
7 Conclusion  
This paper reports about our works as part of the 
NEWS 2010 Shared Task on Transliteration 
Generation. We have used both the orthographic 
and phoneme based transliteration modules for 
the present task. As our all previous efforts was 
for named entity transliteration. The 
Transliteration Generation challenge addresses 
general domain transliteration problem rather 
than named entity transliteration. To handle 
general transliteration problem we proposed a 
IPA based methodology. 
                                                 
4http://www.eci.gov.in/DevForum/Fullname.asp  
74
References  
A. Das, A. Ekbal, Tapabrata Mondal and S. Bandyo-
padhyay. English to Hindi Machine Transliteration 
at NEWS 2009. In Proceedings of the NEWS 2009, 
In Proceeding of ACL-IJCNLP 2009, August 7th, 
2009, Singapore. 
Al-Onaizan, Y. and Knight, K. 2002a. Named Entity 
Translation: Extended Abstract. In Proceedings of 
the Human Language Technology Conference, 
122? 124. 
Al-Onaizan, Y. and Knight, K. 2002b. Translating 
Named Entities using Monolingual and Bilingual 
Resources. In Proceedings of the 40th Annual 
Meeting of the ACL, 400?408, USA. 
Ekbal, A. Naskar, S. and Bandyopadhyay, S. 2007. 
Named Entity Transliteration. International Journal 
of Computer Processing of Oriental Languages 
(IJCPOL), Volume (20:4), 289-310, World Scien-
tific Publishing Company, Singapore. 
Ekbal, A., Naskar, S. and Bandyopadhyay, S. 2006. A 
Modified Joint Source Channel Model for Transli-
teration. In Proceedings of the COLING-ACL 
2006, 191-198, Australia. 
Goto, I., Kato, N., Uratani, N. and Ehara, T. 2003. 
Transliteration Considering Context Information 
based on the Maximum Entropy Method. In Pro-
ceeding of the MT-Summit IX, 125?132, New Or-
leans, USA.  
Jung, Sung Young , Sung Lim Hong and Eunok Paek. 
2000. An English to Korean Transliteration Model 
of Extended Markov Window. In Proceedings of 
International Conference on Computational Lin-
guistics (COLING 2000), 383-389. 
Knight, K. and Graehl, J. 1998. Machine Translitera-
tion, Computational Linguistics, Volume (24:4), 
599?612. 
Kumaran, A. and Tobias Kellner. 2007. A generic 
framework for machine transliteration. In Proc. of 
the 30th SIGIR. 
Li, Haizhou, A Kumaran, Min Zhang and Vladimir 
Pervouchine. 2010. Whitepaper: NEWS 2010 
Shared Task on Transliteration Generation. In the 
ACL 2010 Named Entities Workshop (NEWS-
2010), Uppsala, Sweden, Association for Computa-
tional Linguistics, July 2010. 
Li, Haizhou, Min Zhang and Su Jian. 2004. A Joint 
Source-Channel Model for Machine Translitera-
tion. In Proceedings of the 42nd Annual Meeting 
of the ACL, 159-166. Spain. 
Marino, J. B., R. Banchs, J. M. Crego, A. de Gispert, 
P. Lambert, J. A. Fonollosa and M. Ruiz. 2005.  
Bilingual n-gram Statistical Machine Translation. 
In Proceedings of the MT-Summit X, 275?282. 
Surana, Harshit, and Singh, Anil Kumar. 2008. A 
More Discerning and Adaptable Multilingual 
Transliteration Mechanism for Indian Languages. 
In Proceedings of the 3rd International Joint Confe-
rence on Natural Language Processing (IJCNLP-
08), 64-71, India. 
Vigra, Paola and Khudanpur, S. 2003. Transliteration 
of Proper Names in Cross-Lingual Information Re-
trieval. In Proceedings of the ACL 2003 Workshop 
on Multilingual and Mixed-Language Named Enti-
ty Recognition, 57?60. 
 
75
