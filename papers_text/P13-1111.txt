Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1127?1136,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Handling Ambiguities of Bilingual Predicate-Argument Structures for 
Statistical Machine Translation 
 
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing Zong 
National Laboratory of Pattern Recognition, Institute of Automation, 
Chinese Academy of Sciences, Beijing, China 
{ffzhai,jjzhang,yzhou,cqzong}@nlpr.ia.ac.cn 
 
 
 
Abstract 
Predicate-argument structure (PAS) has been 
demonstrated to be very effective in improving 
SMT performance. However, since a source-
side PAS might correspond to multiple differ-
ent target-side PASs, there usually exist many 
PAS ambiguities during translation. In this pa-
per, we group PAS ambiguities into two types: 
role ambiguity and gap ambiguity. Then we 
propose two novel methods to handle the two 
PAS ambiguities for SMT accordingly: 1) in-
side context integration; 2) a novel maximum 
entropy PAS disambiguation (MEPD) model. 
In this way, we incorporate rich context in-
formation of PAS for disambiguation. Then 
we integrate the two methods into a PAS-
based translation framework. Experiments 
show that our approach helps to achieve sig-
nificant improvements on translation quality. 
1 Introduction 
Predicate-argument structure (PAS) depicts the 
relationship between a predicate and its associat-
ed arguments, which indicates the skeleton struc-
ture of a sentence on semantic level. Basically, 
PAS agrees much better between two languages 
than syntax structure (Fung et al, 2006; Wu and 
Fung, 2009b). Considering that current syntax-
based translation models are always impaired by 
cross-lingual structure divergence (Eisner, 2003; 
Zhang et al, 2010), PAS is really a better repre-
sentation of a sentence pair to model the bilin-
gual structure mapping. 
However, since a source-side PAS might 
correspond to multiple different target-side PASs, 
there usually exist many PAS ambiguities during 
translation. For example, in Figure 1, (a) and (b) 
carry the same source-side PAS <[A0]1 
[Pred(?)]2 [A1]3> for Chinese predicate ???. 
However, in Figure 1(a), the corresponding 
target-side-like PAS is <[X1] [X2] [X3]>, while in 
Figure 1(b), the counterpart target-side-like PAS1 
is <[X2] [X3] [X1]>. This is because the two 
PASs play different roles in their corresponding 
sentences. Actually, Figure 1(a) is an independ-
ent PAS, while Figure 1(b) is a modifier of the 
noun phrase ??? ? ????. We call this kind 
of PAS ambiguity role ambiguity. 
??  ?  ??? ?? ???
[           A0         ]1 [     A1    ]3[Pred]2
?
being , should  ?two major countries
[           X3            ][X2]
China and Russia
[          X1           ]
? ?
?? ?? ? ???
[ A0 ]1 [          A1         ]3[Pred]2
flood  prevention is the  primary  mission
[           X1          ] [ X2 ] [              X3              ]
??? ? ?? ? ??? ? ? ? ?
[      A0      ]1 [    A1   ]3[Pred]2
the location of the olympic village for athletesis the best
[     X3    ][X2][                    X1                     ]
(a)
(c)
(b)
 
Figure 1. An example of ambiguous PASs. 
Meanwhile, Figure 1 also depicts another kind 
of PAS ambiguity. From Figure 1, we can see 
that (a) and (c) get the same source-side PAS and 
target-side-like PAS. However, they are different 
because in Figure 1(c), there is a gap string ?? 
???? between [A0] and [Pred]. Generally, the 
gap strings are due to the low recall of automatic 
semantic role labeling (SRL) or complex sen-
tence structures. For example, in Figure 1(c), the 
gap string ?? ???? is actually an argument 
?AM-PRP? of the PAS, but the SRL system has 
                                                 
1We use target-side-like PAS to refer to a list of general 
non-terminals in target language order, where a non-
terminal aligns to a source argument. 
1127
ignored it. We call this kind of PAS ambiguity 
gap ambiguity. 
During translation, these PAS ambiguities will 
greatly affect the PAS-based translation models. 
Therefore, in order to incorporate the bilingual 
PAS into machine translation effectively, we 
need to decide which target-side-like PAS should 
be chosen for a specific source-side PAS. We 
call this task PAS disambiguation. 
In this paper, we propose two novel methods 
to incorporate rich context information to handle 
PAS ambiguities. Towards the gap ambiguity, 
we adopt a method called inside context 
integration to extend PAS to IC-PAS. In terms of 
IC-PAS, the gap strings are combined effectively 
to deal with the gap ambiguities. As to the role 
ambiguity, we design a novel maximum entropy 
PAS disambiguation (MEPD) model to combine 
various context features, such as context words 
of PAS. For each ambiguous source-side PAS, 
we build a specific MEPD model to select 
appropriate target-side-like PAS for translation. 
We will detail the two methods in Section 3 and 
4 respectively. 
Finally, we integrate the above two methods 
into a PAS-based translation framework (Zhai et 
al. 2012). Experiments show that the two PAS 
disambiguation methods significantly improve 
the baseline translation system. The main 
contribution of this work can be concluded as 
follows: 
1) We define two kinds of PAS ambiguities: 
role ambiguity and gap ambiguity. To our 
best knowledge, we are the first to handle 
these PAS ambiguities for SMT. 
2) Towards the two different ambiguities, we 
design two specific methods for PAS 
disambiguation: inside context integration 
and the novel MEPD model.  
2 PAS-based Translation Framework 
PAS-based translation framework is to perform 
translation based on PAS transformation (Zhai et 
al., 2012). In the framework, a source-side PAS 
is first converted into target-side-like PASs by 
PAS transformation rules, and then perform 
translation based on the obtained target-side-like 
PASs. 
2.1 PAS Transformation Rules 
PAS transformation rules (PASTR) are used to 
convert a source-side PAS into a target one. 
Formally, a PASTR is a triple <Pred, SP, TP>: 
? Pred means the predicate where the rule is 
extracted. 
? SP denotes the list of source elements in 
source language order. 
? TP refers to the target-side-like PAS, i.e., a 
list of general non-terminals in target 
language order. 
For example, Figure 2 shows the PASTR 
extracted from Figure 1(a). In this PASTR, Pred 
is Chinese verb ???, SP is the source element 
list <[A0]1 [Pred]2 [A1]3>, and TP is the list of 
non-terminals <X1 X2 X3>. The same subscript in 
SP and TP means a one-to-one mapping between 
a source element and a target non-terminal. Here, 
we utilize the source element to refer to the 
predicate or argument of the source-side PAS. 
[X3] [X2] [A0]1 [Pred]2 [A1]3 [X1] 
source-side PAS(?) target-side-like PAS
 
Figure 2. An example PASTR. 
2.2 PAS Decoding 
The PAS decoding process is divided into 3 steps: 
(1) PAS acquisition: perform semantic role 
labeling (SRL) on the input sentences to achieve 
their PASs, i.e., source-side PASs; 
(2) Transformation: use the PASTR to match 
the source-side PAS i.e., the predicate Pred and 
the source element list SP. Then by the matching 
PASTRs, transform source-side PASs to target-
side-like PASs. 
(3) Translation: in this step, the decoder first 
translates each source element respectively, and 
then a CKY-style decoding algorithm is adopted 
to combine the translation of each element and 
get the final translation of the PAS.  
2.3 Sentence Decoding with the PAS-based 
translation framework 
Sometimes, the source sentence cannot be fully 
covered by the PAS, especially when there are 
several predicates. Thus to translate the whole 
sentence, Zhai et al (2012) further designed an 
algorithm to decode the entire sentence.  
In the algorithm, they organized the space of 
translation candidates into a hypergraph. For the 
span covered by PAS (PAS span), a multiple-
branch hyperedge is employed to connect it to 
the PAS?s elements. For the span not covered by 
PAS (non-PAS span), the decoder considers all 
the possible binary segmentations of it and uti-
lizes binary hyperedges to link them. 
1128
During translation, the decoder fills the spans 
with translation candidates in a bottom-up man-
ner. For the PAS span, the PAS-based translation 
framework is adopted. Otherwise, the BTG sys-
tem (Xiong et al, 2006) is used. When the span 
covers the whole sentence, we get the final trans-
lation result. 
 
Obviously, PAS ambiguities are not 
considered in this framework at all. The target-
side-like PAS is selected only according to the 
language model and translation probabilities, 
without considering any context information of 
PAS. Consequently, it would be difficult for the 
decoder to distinguish the source-side PAS from 
different context. This harms the translation 
quality. Thus to overcome this problem, we de-
sign two novel methods to cope with the PAS 
ambiguities: inside-context integration and a 
maximum entropy PAS disambiguation (MEPD) 
model. They will be detailed in the next two sec-
tions. 
3 Inside Context Integration 
In this section, we integrate the inside context of 
the PAS into PASTRs to do PAS disambiguation. 
Basically, a PAS consists of several elements (a 
predicate and several arguments), which are ac-
tually a series of continuous spans. For a specific 
PAS <E1,?, En>, such as the source-side PAS 
<[A0][Pred][A1]> in Figure 2, its controlled range 
is defined as: 
( ) { ( ), [1, ]}irange PAS s E i n= ? ?  
where s(Ei) denotes the span of element Ei. Fur-
ther, we define the closure range of a PAS. It 
refers to the shortest continuous span covered by 
the entire PAS: 
0( ) ( )
_ min , max
nj s E j s E
closure range j j
? ?
? ?= ? ?? ?
 
Here, E0 and En are the leftmost and rightmost 
element of the PAS respectively. The closure 
range is introduced here because adjacent source 
elements in a PAS are usually separated by gap 
strings in the sentence. We call these gap strings 
the inside context (IC) of the PAS, which satisfy: 
_ ( ) ( ( ) ( ) )closure range PAS IC PAS range PAS= ? ?  
The operator ?  takes a list of neighboring spans 
as input2, and returns their combined continuous 
span. As an example, towards the PAS ?<[A0] 
[Pred][A1]>? (the one for Chinese predicate ??
(shi)?) in Figure 3, its controlled range is 
{[3,5],[8,8],[9,11]} and its closure range is [3,11]. 
The IC of the PAS is thus {[6,7]}. 
To consider the PAS?s IC during PAS trans-
formation process, we incorporate its IC into the 
extracted PASTR. For each gap string in IC, we 
abstract it by the sequence of highest node cate-
gories (named as s-tag sequence). The s-tag se-
quence dominates the corresponding syntactic 
tree fragments in the parse tree. For example, in 
Figure 3, the s-tag sequence for span [6,8] is ?PP 
VC?. Thus, the sequence for the IC (span [6,7]) 
in Figure 3 is ?PP?. We combine the s-tag se-
quences with elements of the PAS in order. The 
resulting PAS is called IC-PAS, just like the left 
side of Figure 4(b) shows. 
[           A0           ] [        PP        ]
???3 ???7 ?8 ?10
de wei-zhiao-yun-cun
??5?4 ?6
dui yun-dong-yuan shi
?9 ?11
zui hao de
NN DEC NN
NP
P NN
PP
VC AD VA DEC
CP
VP
IP
??1
VV
biao-shi
VP
,2
PU
?0
PN
ta
?
PU
IP
DNP
[Pred] [      A1     ]  
Figure 3. The illustration of inside context (IC). The 
subscript in each word refers to its position in sen-
tence. 
Differently, Zhai et al (2012) attached the IC 
to its neighboring elements based on parse trees. 
For example, in Figure 3, they would attach the 
gap string ??(dui) ???(yun-dong-yuan)? to the 
PAS?s element ?Pred?, and then the span of 
?Pred? would become [6,8]. Consequently, the 
span [6,8] will be translated as a whole source 
element in the decoder. This results in a bad 
translation because the gap string ??(dui) ???
(yun-dong-yuan)? and predicate ??(shi)? should 
be translated separately, just as Figure 4(a) 
shows. Therefore, we can see that the attachment 
decision in (Zhai et al, 2012) is sometimes un-
reasonable and the IC also cannot be used for 
PAS disambiguation at all. In contrast, our meth-
                                                 
2 Here, two spans are neighboring means that the beginning 
of the latter span is the former span?s subsequent word in 
the sentence. For example, span [3,6] and [7,10] are neigh-
boring spans. 
1129
od of inside context integration is much flexible 
and beneficial for PAS disambiguation. 
(a)
(b)
[X1] [X2] [X4] [A0]1 [PP]2 [Pred]3 [A1]4 [X3] 
source-side PAS(?) target-side-like PAS
??? ??? ? ?
[            A0            ]1 [      A1     ]4[Pred]3
[the location of the olympic village]1 [for athletes]2[is]3 [the best]4
[         PP         ]2
de wei-zhiao-yun-cun
??? ?
dui yun-dong-yuan shi
? ?
zui hao de
 
Figure 4. Example of IC-PASTR. (a) The aligned 
span of each element of the PAS in Figure 3; (b) The 
extracted IC-PASTR from (a). 
Using the IC-PASs, we look for the aligned 
target span for each element of the IC-PAS. We 
demand that every element and its corresponding 
target span must be consistent with word align-
ment. Otherwise, we discard the IC-PAS. After-
wards, we can easily extract a rule for PAS trans-
formation, which we call IC-PASTR. As an ex-
ample, Figure 4(b) is the extracted IC-PASTR 
from Figure 4(a). 
Note that we only apply the source-side PAS 
and word alignment for IC-PASTR extraction. 
By contrast, Zhai et al (2012) utilized the result 
of bilingual SRL (Zhuang and Zong, 2010b). 
Generally, bilingual SRL could give a better 
alignment between bilingual elements. However, 
bilingual SRL usually achieves a really low re-
call on PASs, about 226,968 entries in our train-
ing set while it is 882,702 by using monolingual 
SRL system. Thus to get a high recall for PASs, 
we only utilize word alignment instead of captur-
ing the relation between bilingual elements. In 
addition, to guarantee the accuracy of IC-
PASTRs, we only retain rules with more than 5 
occurrences. 
4 Maximum Entropy PAS Disambigua-
tion (MEPD) Model 
In order to handle the role ambiguities, in this 
section, we concentrate on utilizing a maximum 
entropy model to incorporate the context infor-
mation for PAS disambiguation. Actually, the 
disambiguation problem can be considered as a 
multi-class classification task. That is to say, for 
a source-side PAS, every corresponding target-
side-like PAS can be considered as a label. For 
example, in Figure 1, for the source-side PAS 
?[A0]1[Pred]2[A1]3?, the target-side-like PAS 
?[X1] [X2] [X3]? in Figure 1(a) is thus a label and 
?[X2] [X3] [X1]? in Figure 1(b) is another label of 
this classification problem. 
The maximum entropy model is the classical 
way to handle this problem: 
exp( ( , , ( ), ( )))
( | , ( ), ( ))
exp( ( , , ( ), ( )))
i i i
tp i i i
h sp tp c sp c tp
P tp sp c sp c tp
h sp tp c sp c tp?
?
??
= ?
? ?
 
where sp and tp refer to the source-side PAS (not 
including the predicate) and the target-side-like 
PAS respectively. c(sp) and c(tp) denote the sur-
rounding context of sp and tp. hi is a binary fea-
ture function and ?i is the weight of hi. 
We train a maximum entropy classifier for 
each sp via the off-the-shelf MaxEnt toolkit 3 . 
Note that to avoid sparseness, sp does not in-
clude predicate of the PAS. Practically, the pred-
icate serves as a feature of the MEPD model. As 
an example, for the rule illustrated in Figure 4(b), 
we build a MEPD model for its source element 
list sp <[A0] [PP] [Pred] [A1]>, and integrate the 
predicate ??(shi)? into the MEPD model as a 
feature. 
In detail, we design a list of features for each 
pair <sp, tp> as follows: 
?   Lexical Features. These features include 
the words immediately to the left and right of sp, 
represented as w-1 and w+1. Moreover, the head 
word of each argument also serves as a lexical 
feature, named as hw(Ei). For example, Figure 3 
shows the context of the IC-PASTR in Figure 
4(b), and the extracted lexical features of the in-
stance are: w-1=? , w+1=? , hw([A0]1)=??
(wei-zhi), hw([A1]4)=?(hao). 
?   POS Features. These features are defined 
as the POS tags of the lexical features, p-1, p+1 
and phw(Ei) respectively. Thus, the correspond-
ing POS features of Figure 4 (b) are: p-1=PU, 
p+1=PU, phw([A0]1)=NN, phw([A1]4)=VA. 
?   Predicate Feature. It is the pair of source 
predicate and its corresponding target predicate. 
For example, in Figure 4(b), the source and tar-
get predicate are ??(shi)? and ?is? respectively. 
The predicate feature is thus ?PredF=?(shi)+is?. 
The target predicate is determined by: 
_ ( )
- arg max ( | - )j
j t range PAS
t pred p t s pred
?
=  
where s-pred is the source predicate and t-pred 
is the corresponding target predicate. 
                                                 
3http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.htm
l 
1130
t_range(PAS) refers to the target range covering 
all the words that are reachable from the PAS via 
word alignment.  tj refers to the jth word in 
t_range(PAS). The utilized lexical translation 
probabilities are from the toolkit in Moses 
(Koehn et al, 2007). 
?   Syntax Features. These features include 
st(Ei), i.e., the highest syntax tag for each argu-
ment, and fst(PAS) which is the lowest father 
node of sp in the parse tree. For example, for the 
rule shown in Figure 4(b), syntax features are 
st([A0]1)=NP, st([A1]4)=CP, and fst(PAS)=IP 
respectively.  
Using these features, we can train the MEPD 
model. We set the Gaussian prior to 1.0 and per-
form 100 iterations of the L-BFGS algorithm for 
each MEPD model. At last, we build 160 and 
215 different MEPD classifiers, respectively, for 
the PASTRs and IC-PASTRs. Note that since the 
training procedure of maximum entropy classifi-
er is really fast, it does not take much time to 
train these classifiers. 
5 Integrating into the PAS-based Trans-
lation Framework 
In this section, we integrate our method of PAS 
disambiguation into the PAS-based translation 
framework when translating each test sentence. 
For inside context integration, since the format 
of IC-PASTR is the same to PASTR4, we can 
use the IC-PASTR to substitute PASTR for 
building a PAS-based translation system directly. 
We use ?IC-PASTR? to denote this system. In 
addition, since our method of rule extraction is 
different from (Zhai et al, 2012), we also use 
PASTR to construct a translation system as the 
baseline system, which we call ?PASTR?. 
On the basis of PASTR and IC-PASTR, we 
further integrate our MEPD model into transla-
tion. Specifically, we take the score of the MEPD 
model as another informative feature for the de-
coder to distinguish good target-side-like PASs 
from bad ones. The weights of the MEPD feature 
can be tuned by MERT (Och, 2003) together 
with other translation features, such as language 
model. 
6 Related Work 
The method of PAS disambiguation for SMT is 
relevant to the previous work on context depend-
                                                 
4 The only difference between IC-PASTR and PASTR is 
that there are many syntactic labels in IC-PASTRs.  
ent translation. 
Carpuat and Wu (2007a, 2007b) and Chan et 
al. (2007) have integrated word sense disambig-
uation (WSD) and phrase sense disambiguation 
(PSD) into SMT systems. They combine rich 
context information to do disambiguation for 
words or phrases, and achieve improved transla-
tion performance. 
Differently, He et al (2008), Liu et al (2008) 
and Cui et al (2010) designed maximum entropy 
(ME) classifiers to do better rule section for hier-
archical phrase-based model and tree-to-string 
model respectively. By incorporating the rich 
context information as features, they chose better 
rules for translation and yielded stable improve-
ments on translation quality. 
Our work differs from the above work in the 
following two aspects: 1) in our work, we focus 
on the problem of disambiguates on PAS; 2) we 
define two kinds of PAS ambiguities: role 
ambiguity and gap ambiguity. 3) towards the two 
different ambiguities, we design two specific 
methods for PAS disambiguation: inside context 
integration and the novel MEPD model. 
In addition, Xiong et al (2012) proposed an 
argument reordering model to predict the relative 
position between predicates and arguments. They 
also combine the context information in the 
model. But they only focus on the relation be-
tween the predicate and a specific argument, ra-
ther than the entire PAS. Different from their 
work, we incorporate the context information to 
do PAS disambiguation based on the entire PAS. 
This is very beneficial for global reordering dur-
ing translation (Zhai et al, 2012). 
7 Experiment 
7.1 Experimental Setup  
We perform Chinese-to-English translation to 
demonstrate the effectiveness of our PAS disam-
biguation method. The training data contains 
about 260K sentence pairs5. To get accurate SRL 
results, we ensure that the length of each sen-
tence in the training data is among 10 and 30 
words. We run GIZA++ and then employ the 
grow-diag-final-and (gdfa) strategy to produce 
symmetric word alignments. The development 
set and test set come from the NIST evaluation 
test data (from 2003 to 2005). Similar to the 
training set, we also only retain the sentences 
                                                 
5 It is extracted from the LDC corpus. The LDC category 
number : LDC2000T50, LDC2002E18, LDC2003E07, 
LDC2004T07, LDC2005T06, LDC2002L27, LDC2005T10 
and LDC2005T34. 
1131
whose lengths are among 10 and 30 words. Fi-
nally, the development set includes 595 sentenc-
es from NIST MT03 and the test set contains 
1,786 sentences from NIST MT04 and MT05. 
We train a 5-gram language model with the 
Xinhua portion of English Gigaword corpus and 
target part of the training data. The translation 
quality is evaluated by case-insensitive BLEU-4 
with shortest length penalty. The statistical sig-
nificance test is performed by the re-sampling 
approach (Koehn, 2004). 
We perform SRL on the source part of the 
training set, development set and test set by the 
Chinese SRL system used in (Zhuang and Zong, 
2010b). To relieve the negative effect of SRL 
errors, we get the multiple SRL results by 
providing the SRL system with 3-best parse trees 
of Berkeley parser (Petrov and Klein, 2007), 1-
best parse tree of Bikel parser (Bikel, 2004) and 
Stanford parser (Klein and Manning, 2003). 
Therefore, at last, we can get 5 SRL result for 
each sentence. For the training set, we use these 
SRL results to do rule extraction respectively. 
We combine the obtained rules together to get a 
combined rule set. We discard the rules with 
fewer than 5 appearances. Using this set, we can 
train our MEPD model directly. 
As to translation, we match the 5 SRL results 
with transformation rules respectively, and then 
apply the resulting target-side-like PASs for de-
coding. As we mentioned in section 2.3, we use 
the state-of-the-art BTG system to translate the 
non-PAS spans. 
source-side PAS counts number of classes 
[A0] [Pred(?)] [A1] 245 6 
[A0] [Pred(?)] [A1] 148 6 
[A0] [AM-ADV] [Pred(?)] [A1] 68 20 
[A0] [Pred(??)] [A1] 66 6 
[A0] [Pred(?)] [A1] 42 6 
[A0] [Pred(??)] [A1] 32 4 
[A0] [AM-ADV] [Pred(?)] [A1] 32 19 
[A0] [Pred(??)] [A1] 29 4 
[AM-ADV] [Pred(?)] [A1] 26 6 
[A2] [Pred(?)] [A1] 16 5 
Table 1. The top 10 frequent source-side PASs in the 
dev and test set. 
7.2 Ambiguities in Source-side PASs 
We first give Table 1 to show some examples of 
role ambiguity. In the table, for instance, the se-
cond line denotes that the source-side PAS ?[A0] 
[Pred(?)] [A1]? appears 148 times in the devel-
opment and test set al together, and it corre-
sponds to 6 different target-side-like PASs in the 
training set. 
As we can see from Table 1, all the top 10 
PASs correspond to several different target-side-
like PASs. Moreover, according to our statistics, 
among all PASs appearing in the development 
set and test set, 56.7% of them carry gap strings. 
These statistics demonstrate the importance of 
handling the role ambiguity and gap ambiguity in 
the PAS-based translation framework. Therefore, 
we believe that our PAS disambiguation method 
would be helpful for translation. 
7.3 Translation Result  
We compare the translation result using PASTR, 
IC-PASTR and our MEPD model in this section. 
The final translation results are shown in Table 2. 
As we can see, after employing PAS for transla-
tion, all systems outperform the baseline BTG 
system significantly. This comparison verifies 
the conclusion of (Zhai et al, 2012) and thus also 
demonstrates the effectiveness of PAS. 
MT system Test set 
n-gram precision 
1 2 3 4 
BTG 32.75 74.39 41.91 24.75 14.91 
PASTR 33.24* 75.28 42.62 25.18 15.10 
PASTR+MEPD 33.78* 75.32 43.08 25.75 15.58 
IC-PASTR 33.95*# 75.62 43.36 25.92 15.58 
IC-PASTR+MEPD 34.19*# 75.66 43.40 26.15 15.92 
Table 2. Result of baseline system and the MT sys-
tems using our PAS-based disambiguation method. 
The ?*? and ?#? denote that the result is significantly 
better than BTG and PASTR respectively (p<0.01).  
Specifically, after integrating the inside con-
text information of PAS into transformation, we 
can see that system IC-PASTR significantly out-
performs system PASTR by 0.71 BLEU points. 
Moreover, after we import the MEPD model into 
system PASTR, we get a significant improve-
ment over PASTR (by 0.54 BLEU points). These 
comparisons indicate that both the inside context 
integration and our MEPD model are beneficial 
for the decoder to choose better target-side-like 
PAS for translation. 
On the basis of IC-PASTR, we further add our 
MEPD model into translation and get system IC-
PASTR+MEPD. We can see that this system 
further achieves a remarkable improvement over 
system PASTR (0.95 BLEU points).  
However, from Table 2, we find that system 
IC-PASTR+MEPD only outperforms system IC-
PASTR slightly (0.24 BLEU points). The result 
seems to show that our MEPD model is not such 
1132
useful after using IC-PASTR. We will explore 
the reason in section 7.5. 
7.4 Effectiveness of Inside Context Integra-
tion 
The method of inside context integration is used 
to combine the inside context (gap strings) into 
PAS for translation, i.e., extend the PASTR to 
IC-PASTR. In order to demonstrate the effec-
tiveness of inside context integration, we first 
give Table 3, which illustrates statistics on the 
matching PASs. The statistics are conducted on 
the combination of development set and test set. 
Transformation 
Rules 
Matching PAS 
None Gap PAS Gap PAS Total 
PASTR 1702 1539 3241 
IC-PASTR 1546 832 2378 
Table 3. Statistics on the matching PAS. 
In Table 3, for example, the line for PASTR 
means that if we use PASTR for the combined 
set, 3241 PASs (column ?Total?) can match 
PASTRs in total. Among these matching PASs, 
1539 ones (column ?Gap PAS?) carry gap strings, 
while 1702 ones do not (column ?None Gap 
PAS?). Consequently, since PASTR does not 
consider the inside context during translation, the 
Gap PASs, which account for 47% (1539/3241) 
of all matching PASs, might be handled inappro-
priately in the PAS-based translation framework. 
Therefore, integrating the inside context into 
PASTRs, i.e., using the proposed IC-PASTRs, 
would be helpful for translation. The translation 
result shown in Table 2 also demonstrates this 
conclusion. 
(a) reference
(c) translation result using IC-PASTR
[for economic recovery , especially of investment confidence is]
[  A0  ] [                              PP                               ] [Pred] [      A1      ]
? ? ? ? ???? ?? ?? ? ??? ?? ?? ??
[ a good sign ] [ for economic recovery , especially of investment confidence ]this is
? ? ? ? ??? ?? ?? ? ??? ?? ?? ??  ? 
[a good sign]this
(b) translation result using PASTR
[  A0  ] [                              PP                               ] [Pred] [      A1      ]
? ? ? ? ???? ?? ?? ? ??? ?? ?? ??
[a good sign]this is [for economic recovery and the restoration of investors ' confidence]
[  A0  ] [                            Pred                             ] [      A1      ]
 
Figure 5. Translation examples to verify the effec-
tiveness of inside context.  
From Table 3, we can also find that the num-
ber of matching PASs decreases after using IC-
PASTR. This is because IC-PASTR is more spe-
cific than PASTR. Therefore, for a PAS with 
specific inside context (gap strings), even if the 
matched PASTR is available, the matched IC-
PASTR might not. This indicates that comparing 
with PASTR, IC-PASTR is more capable of dis-
tinguishing different PASs. Therefore, based on 
this advantage, although the number of matching 
PASs decreases, IC-PASTR still improves the 
translation system using PASTR significantly. Of 
course, we believe that it is also possible to inte-
grate the inside context without decreasing the 
number of matching PASs and we plan this as 
our future work. 
We further give a translation example in Fig-
ure 5 to illustrate the effectiveness of our inside 
context integration method. In the example, the  
automatic SRL system ignores the long preposi-
tion phrase ?? ???? ???? ?? ???
?? for the PAS. Thus, the system using PASTRs 
can only attach the long phrase to the predicate 
??? according to the parse tree, and meanwhile, 
make use of a transformation rule as follows: 
[X3] [X2] [A0]1 [Pred]2 [A1]3 [X1] 
source-side PAS(?) target-side-like PAS
 
In this way, the translation result is very bad, just 
as Figure 5(b) shows. The long preposition 
phrases are wrongly positioned in the translation. 
In contrast, after inside context integration, we 
match the inside context during PAS transfor-
mation. As Figure 5(c) shows, the inside context 
helps to selects a right transformation rule as fol-
lows and gets a good translation result finally. 
[X1] [X2] [X4] [A0]1 [PP]2 [Pred]3 [A1]4 [X3] 
source-side PAS(?) target-side-like PAS
 
7.5 Effectiveness of the MEPD Model 
The MEPD model incorporates various context 
features to select better target-side-like PAS for 
translation. On the basis of PASTR and IC-
PASTR, we build 160 and 215 different MEPD 
classifies, respectively, for the frequent source-
side PASs. 
In Table 2, we have found that our MEPD 
model improves system IC-PASTR slightly. We 
conjecture that this phenomenon is due to two 
possible reasons. On one hand, sometimes, many 
PAS ambiguities might be resolved by both in-
side context and the MEPD model. Therefore, 
the improvement would not be such significant 
1133
when we combine these two methods together. 
On the other hand, as Table 3 shows, the number 
of matching PASs decreases after using IC-
PASTR. Since the MEPD model works on PASs, 
its effectiveness would also weaken to some ex-
tent. Future work will explore this phenomenon 
more thoroughly. 
PASTR
Ref
PASTR 
+ MEPD
...  ,  [??]A0    [?]Pred    [? ?? ??]A1  ?
...  [the hague]     [is]      [the last leg]  .
...  ,  [??]    [?]    [? ?? ??]  ?
...  [the hague]   [is]   [his last stop]  .
...  ,  [??]A0    [?]Pred    [? ?? ??]A1  ?
...   [is]    [his last leg of]    [the hague] .
 
Figure 6. Translation examples to demonstrate the 
effectiveness of our MEPD model. 
Now, we give Figure 6 to demonstrate the ef-
fectiveness of our MEPD model. From the Fig-
ure, we can see that the system using PASTRs 
selects an inappropriate transformation rule for 
translation: 
[X1] [X3] [A0]1 [Pred]2 [A1]3 [X2] 
source-side PAS(?) target-side-like PAS
 
This rule wrongly moves the subject ???
(Hague)? to the end of the translation. We do not 
give the translation result of the BTG system 
here because it makes the same mistake. 
Conversely, by considering the context infor-
mation, the PASTR+MEPD system chooses a 
correct rule for translation: 
[X3] [X2] [A0]1 [Pred]2 [A1]3 [X1] 
source-side PAS(?) target-side-like PAS
 
As we can see, the used rule helps to keep the 
SVO structure unchanged, and gets the correct 
translation. 
8 Conclusion and Future Work 
In this paper, we focus on the problem of ambi-
guities for PASs. We first propose two ambigui-
ties: gap ambiguity and role ambiguity. Accord-
ingly, we design two novel methods to do effi-
cient PAS disambiguation: inside-context inte-
gration and a novel MEPD model. For inside 
context integration, we abstract the inside con-
text and combine them into the PASTRs for PAS 
transformation. Towards the MEPD model, we 
design a maximum entropy model for each ambi-
tious source-side PASs. The two methods suc-
cessfully incorporate the rich context information 
into the translation process. Experiments show 
that our PAS disambiguation methods help to 
improve the translation performance significantly.  
In the next step, we will conduct experiments 
on other language pairs to demonstrate the effec-
tiveness of our PAS disambiguation method. In 
addition, we also will try to explore more useful 
and representative features for our MEPD model. 
Acknowledgments 
The research work has been funded by the Hi-
Tech Research and Development Program (?863? 
Program) of China under Grant No. 
2011AA01A207, 2012AA011101, and 
2012AA011102 and also supported by the Key 
Project of Knowledge Innovation Program of 
Chinese Academy of Sciences under Grant 
No.KGZD-EW-501. We thank the anonymous 
reviewers for their valuable comments and sug-
gestions. 
References  
Wilker Aziz, Miguel Rios, and Lucia Specia. (2011). 
Shallow semantic trees for smt. In Proceedings of 
the Sixth Workshop on Statistical Machine Trans-
lation, pages 316?322, Edinburgh, Scotland, July. 
Daniel Bikel. (2004). Intricacies of Collins parsing 
model. Computational Linguistics, 30(4):480-511. 
David Chiang, (2007). Hierarchical phrase-based 
translation. Computational Linguistics, 33 (2):201?
228. 
Marine Carpuat and Dekai Wu. 2007a. How phrase-
sense disambiguation outperforms word sense dis-
ambiguation for statistical machine translation. In 
11th Conference on Theoretical and Methodologi-
cal Issues in Machine Translation, pages 43?52. 
Marine Carpuat and Dekai Wu. 2007b. Improving 
statistical machine translation using word sense 
disambiguation. In Proceedings of EMNLP-CoNLL 
2007, pages 61?72. 
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 
2007. Word sense disambiguation improves statis-
tical machine translation. In Proc. ACL 2007, pag-
es 33?40. 
Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou and 
Tiejun Zhao. A Joint Rule Selection Model for 
Hierarchical Phrase-Based Translation. In Proc. 
of ACL 2010. 
1134
Jason Eisner. (2003). Learning non-isomorphic tree 
mappings for machine translation. In Proc. of ACL 
2003. 
Pascale Fung, Wu Zhaojun, Yang Yongsheng, and 
Dekai Wu. (2006). Automatic learning of chinese 
english semantic structure mapping. In IEEE/ACL 
2006 Workshop on Spoken Language Technology 
(SLT 2006), Aruba, December. 
Pascale Fung, Zhaojun Wu, Yongsheng Yang and 
Dekai Wu. (2007). Learning bilingual semantic 
frames: shallow semantic sarsing vs. semantic sole 
projection. In Proceedings of the 11th Conference 
on Theoretical and Methodological Issues in Ma-
chine Translation, pages 75-84. 
Qin Gao and Stephan Vogel. (2011). Utilizing target-
side semantic role labels to assist hierarchical 
phrase-based machine translation. In Proceedings 
of Fifth Workshop on Syntax, Semantics and Struc-
ture in Statistical Translation, pages 107?115, 
Portland, Oregon, USA, June 2011. Association for 
Computational Linguistics 
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexi-
calized rule selection. In Proc. of Coling 2008, 
pages 321?328. 
Franz Josef Och. (2003). Minimum error rate training 
in statistical machine translation. In Proc. of ACL 
2003, pages 160?167. 
Franz Josef Och and Hermann Ney. (2004). The 
alignment template approach to statistical machine 
translation. Computational Linguistics, 30:417?449. 
Dan Klein and Christopher D. Manning. (2003). Ac-
curate unlexicalized parsing. In Proc. of ACL-2003, 
pages 423-430. 
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 
(2003). Statistical phrase-based translation. In Pro-
ceedings of NAACL 2003, pages 58?54, Edmonton, 
Canada, May-June. 
Philipp Koehn. (2004). Statistical significance tests 
for machine translation evaluation. In Proceedings 
of EMNLP 2004, pages 388?395, Barcelona, Spain, 
July. 
P Koehn, H Hoang, A Birch, C Callison-Burch, M 
Federico, N Bertoldi, B Cowan, W Shen, C Moran 
and R Zens, (2007). Moses: Open source toolkit for 
statistical machine translation. In Proc. of ACL 
2007. pages 177?180, Prague, Czech Republic, 
June. Association for Computational Linguistics. 
Mamoru Komachi and Yuji Matsumoto. (2006). 
Phrase reordering for statistical machine translation 
based on predicate-argument structure. In Proceed-
ings of the International Workshop on Spoken 
Language Translation: Evaluation Campaign on 
Spoken Language Translation, pages 77?82. 
Ding Liu and Daniel Gildea. (2008). Improved tree-
to-string transducer for machine Translation. In 
Proceedings of the Third Workshop on Statistical 
Machine Translation, pages 62?69, Columbus, 
Ohio, USA, June 2008. 
Ding Liu and Daniel Gildea. (2010). Semantic role 
features for machine translation. In Proc. of Coling 
2010, pages 716?724, Beijing, China, August. 
Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin. 
Maximum Entropy based Rule Selection Model for 
Syntax-based Statistical Machine Translation. In 
Proc. of EMNLP 2008.  
Yang Liu, Qun Liu and Shouxun Lin. (2006). Tree-to-
string alignment template for statistical machine 
translation. In Proc. of ACL-COLING 2006. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi and 
Kevin Knight. (2006). SPMT: Statistical machine 
translation with syntactified target language 
phrases. In Proc. of EMNLP 2006, pages 44-52. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. (2002). Bleu: a method for automat-
ic evaluation of machine translation. In Proc. ACL 
2002, pages 311?318, Philadelphia, Pennsylvania, 
USA, July. 
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan 
Klein. (2006). Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the 
21st International Conference on Computational 
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 433?
440, Sydney, Australia, July. Association for Com-
putational Linguistics. 
Andreas Stolcke. (2002). Srilm ? an extensible lan-
guage modelling toolkit. In Proceedings of the 7th 
International Conference on Spoken Language 
Processing, pages 901?904, Denver, Colorado, 
USA, September. 
Dekai Wu and Pascale Fung. (2009a). Can semantic 
role labelling improve smt. In Proceedings of the 
13th Annual Conference of the EAMT, pages 218?
225, Barcelona, May. 
Dekai Wu and Pascale Fung. (2009b). Semantic roles 
for smt: A hybrid two-pass model. In Proc. NAACL 
2009, pages 13?16, Boulder, Colorado, June. 
ShuminWu and Martha Palmer. (2011). Semantic 
mapping using automatic word alignment and se-
mantic role labelling. In Proceedings of Fifth 
Workshop on Syntax, Semantics and Structure in 
Statistical Translation, pages 21?30, Portland, Or-
egon, USA, June 2011. 
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime 
Tsukada, and Masaaki Nagata. (2011). Extracting 
preordering rules from predicate-argument struc-
tures. In Proc. IJCNLP 2011, pages 29?37, Chiang 
Mai, Thailand, November.  
1135
Deyi Xiong, Qun Liu, and Shouxun Lin. (2006). Max-
imum entropy based phrase reordering model for 
statistical machine translation. In Proceedings of 
the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the 
Association for Computational Linguistics, pages 
521?528, Sydney, Australia, July. 
Deyi Xiong, Min Zhang, and Haizhou Li. (2012). 
Modelling the translation of predicate-argument 
structure for smt. In Proc. of ACL 2012, pages 
902?911, Jeju, Republic of Korea, 8-14 July 2012. 
Nianwen Xue. (2008). Labelling chinese predicates 
with semantic roles. Computational Linguistics, 
34(2): 225-255. 
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing 
Zong. Machine Translation by Modeling Predicate- 
Argument Structure Transformation. In Proc. of 
COLING 2012. 
Hui Zhang, Min  Zhang, Haizhou Li and Eng Siong 
Chng. (2010). Non-isomorphic Forest Pair Transla-
tion. In Proceedings of EMNLP 2010, pages 440-
450, Massachusetts, USA, 9-11 October 2010.  
Tao Zhuang, and Chengqing Zong. (2010a). A mini-
mum error weighting combination strategy for chi-
nese semantic role labelling. In Proceedings of 
COLING-2010, pages 1362-1370. 
Tao Zhuang and Chengqing Zong. (2010b). Joint in-
ference for bilingual semantic role labelling. In 
Proceedings of EMNLP 2010, pages 304?314, 
Massachusetts, USA, 9-11 October 2010.  
1136
