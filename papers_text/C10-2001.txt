Coling 2010: Poster Volume, pages 1?8,
Beijing, August 2010
Towards the Adequate Evaluation of Morphosyntactic Taggers
Szymon Acedan?ski
Institute of Computer Science,
Polish Academy of Sciences
Institute of Informatics,
University of Warsaw
accek@mimuw.edu.pl
Adam Przepi?rkowski
Institute of Computer Science,
Polish Academy of Sciences
Institute of Informatics,
University of Warsaw
adamp@ipipan.waw.pl
Abstract
There exists a well-established and almost
unanimously adopted measure of tagger
performance, namely, accuracy. Although
it is perfectly adequate for small tagsets
and typical approaches to disambiguation,
we show that it is deficient when applied
to rich morphological tagsets and propose
various extensions designed to better cor-
relate with the real usefulness of the tag-
ger.
1 Introduction
Part-of-Speech (PoS) tagging is probably the
most common and best researched NLP task, the
first step in many higher level processing solu-
tions such as parsing, but also information re-
trieval, speech recognition and machine transla-
tion. There are also well established evaluation
measures, the foremost of which is accuracy, i.e.,
the percent of words for which the tagger assigns
the correct ? in the sense of some gold standard
? interpretation.
Accuracy works well for the original PoS tag-
ging task, where each word is assumed to have ex-
actly one correct tag, and where the information
carried by a tag is limited roughly to the PoS of
the word and only very little morphosyntactic in-
formation, as in typical tagsets for English. How-
ever, there are two cases where accuracy becomes
less than adequate: the situation where the gold
standard and / or the tagging results contain mul-
tiple tags marked as correct for a single word, and
the use of a rich morphosyntactic (or morphologi-
cal) tagset.
The first possibility is discussed in detail in
(Karwan?ska and Przepi?rkowski, 2009), but the
need for an evaluation measure for taggers which
do not necessarily fully disambiguate PoS was al-
ready noted in (van Halteren, 1999), where the use
of standard information retrieval measures preci-
sion and recall (as well as their harmonic mean,
the F-measure) is proposed. Other natural gen-
eralisations of the accuracy measure, able to deal
with non-unique tags either in the gold standard1
or in the tagging results, are proposed in (Kar-
wan?ska and Przepi?rkowski, 2009).
Standard accuracy is less than adequate also
in case of rich morphosyntactic tagsets, where
the full tag carries information not only about
PoS, but also about case, number, gender, etc.
Such tagsets are common for Slavic languages,
but also for Hungarian, Arabic and other lan-
guages. For example, according to one com-
monly used Polish tagset (Przepi?rkowski and
Wolin?ski, 2003), the form uda has the follow-
ing interpretations: fin:sg:ter:perf (a fi-
nite singular 3rd person perfective form of the
verb UDAC? ?pretend?), subst:pl:nom:n and
1There are cases were it makes sense to manually assign
a number of tags as correct to a given word, as any decision
would be fully arbitrary, regardless of the amount of con-
text and world knowledge available. For example, in some
Slavic languages, incl. Polish, there are verbs which option-
ally subcategorise for an accusative or a genitive comple-
ment, without any variation in meaning, and there are nouns
which are syncretic between these two cases, so for such
?verb + nounacc/gen? sequences it is impossible to fully dis-
ambiguate case; see also (Oliva, 2001).
1
subst:pl:acc:n (nominative or accusative
plural form of the neuter noun UDO ?thigh?).
Now, assuming that the right interpretation in a
given context is subst:pl:acc:n, accuracy
will equally harshly penalise the other nominal in-
terpretation (subst:pl:nom:n), which shares
with the correct interpretation not only PoS, but
also the values of gender and number, and the
completely irrelevant verbal interpretation. A
more accurate tagger evaluation measure should
distinguish these two non-optimal assignments
and treat subst:pl:nom:n as partially correct.
Similarly, the Polish tagset mentioned above
distinguishes between nouns and gerunds, with
some forms actually ambiguous between these
two interpretations. For example, zadanie may be
interpreted as a nominative or accusative form of
the noun ZADANIE ?task?, or a nominative or ac-
cusative form of the gerund derived from the verb
ZADAC? ?assign?. Since gerunds and nouns have
very similar distributions, any error in the assign-
ment of part of speech, noun vs. gerund, will most
probably not matter for a parser of Polish ? it
will still be able to construct the right tree, pro-
vided the case is correctly disambiguated. How-
ever, the ?all-or-nothing? nature of the accuracy
measure regards the tag differing from the correct
one only in part of speech or in case as harshly,
as it would regard an utterly wrong interpretation,
say, as an adverb.
In what follows we propose various evaluation
measures which differentiate between better and
worse incorrect interpretations, cf. ? 2. The im-
plementation of two such measures is described
in ? 3. Finally, ? 4 concludes the paper.
2 Proposed Measures
2.1 Full Interpretations and PoS
The first step towards a better accuracy mea-
sure might consist in calculating two accu-
racy measures: one for full tags, and the
other only for fragments of tags represent-
ing parts of speech. Two taggers wrongly
assigning either fin:sg:ter:perf (T1) or
subst:pl:nom:n (T2) instead of the correct
subst:pl:acc:n would fare equally well with
respect to the tag-level accuracy, but T2 would be
? rightly ? evaluated as better with respect to
the PoS-level accuracy.
The second example given in ? 1 shows, how-
ever, that the problem is more general and that a
tagger which gets the PoS wrong (say, gerund in-
stead of noun) but all the relevant categories (case,
number, gender) right may actually be more use-
ful in practice than the one that gets the PoS right
at the cost of confusing cases (say, accusative in-
stead of nominative).
2.2 Positional Accuracy
A generalisation of the idea of looking separately
at parts of speech is to split tags into their compo-
nents (or positions) and measure the correctness
of the tag by calculating the F-measure. For ex-
ample, if the (perfective, affirmative) gerundial in-
terpretation ger:sg:nom:n:perf:aff is as-
signed instead of the correct nominal interpreta-
tion subst:sg:nom:n, the tags agree on 3 po-
sitions (sg, nom, n), so the precision is 36 , the re-call ? 34 , which gives the F-measure of 0.6. Obvi-ously, the assignment of the correct interpretation
results in F-measure equal 1.0, and the completely
wrong interpretation gives F-measure 0.0. Taking
these values instead of the ?all-or-nothing? 0 or 1,
accuracy is reinterpreted as the average F-measure
over all tag assignments.
Note that while this measure, let us call it po-
sitional accuracy (PA), is more fine-grained than
the standard accuracy, it wrongly treats all com-
ponents of tags as of equal importance and dif-
ficulty. For example, there are many case syn-
cretisms in Polish, but practically no ambiguities
concerning the category of negation (see the value
aff above), so case is inherently much more diffi-
cult than negation, and also much more important
for syntactic parsing, and as such it should carry
more weight when evaluating tagging results.
2.3 Weighted Positional Accuracy
In the current section we make a simplifying as-
sumption that weights of positions are absolute,
rather than conditional, i.e., that the weight of, say,
case does not depend on part of speech, word or
context. Once the weights are attained, weighted
precision and recall may be used as in the follow-
ing example.
2
Assume that PoS, case, number and gender
have the same weight, say 2.0, which is 4 times
larger than that of any other category. Then, in
case ger:sg:nom:n:perf:aff is assigned
instead of the correct subst:sg:nom:n, pre-
cision and recall are given by:
P = 3? 2.04? 2.0 + 2? 0.5 =
2
3 ,
R = 3? 2.04? 2.0 =
3
4 .
This results in a higher F-measure than in case of
non-weighted positional accuracy.
The following subsections propose various
ways in which the importance of particular gram-
matical categories and of the part of speech may
be estimated.
2.3.1 Average Ambiguity
The average number of morphosyntactic inter-
pretations per word is sometimes given as a rough
measure of the difficulty of tagging. For exam-
ple, tagging English texts with the Penn Treebank
tagset is easier than tagging Czech or Polish, as
the average number of possible tags per word is
2.32 in English (Hajic?, 2004, p. 171), while it is
3.65 (Hajic? and Hladk?, 1997, p. 113) and 3.32
(Przepi?rkowski, 2008, p. 44) for common tagsets
for Czech and Polish, respectively.
By analogy, one measure of the difficulty of as-
signing the right value of a given category or part
of speech is the average number of different val-
ues of the category per word.
2.3.2 Importance for Parsing
All measures mentioned so far are intrinsic (in
vitro) evaluation measures, independent ? but
hopefully correlated with ? the usefulness of the
results in particular applications. On the other
hand, extrinsic (in vivo) evaluation estimates the
usefulness of tagging in larger systems, e.g., in
parsers. Full-scale extrinsic evaluation is rarely
used, as it is much more costly and often requires
user evaluation of the end system.
In this and the next subsections we propose
evaluation measures which combine the advan-
tages of both approaches. They are variants of
the weighted positional accuracy (WPA) measure,
where weights correspond to the usefulness of a
given category (or PoS) for a particular task.
Probably the most common task taking advan-
tage of morphosyntactic tagging is syntactic pars-
ing. Here, weights should indicate to what extent
the parser relies on PoS and particular categories
to arrive at the correct parse. Such weights may
be estimated from an automatically parsed corpus
in the following way:
for each category (including PoS) c do
count(c) = 0 {Initialise counts.}
end for
for each sentence s do
for each rule r used in s do
for each terminal symbol (word) t in the
RHS of r do
for each category c referred to by r in t
do
increase count(c)
end for
end for
end for
end for
{Use count(c)?s as weights.}
In prose: whenever a syntactic rule is used, in-
crease counts of all morphosyntactic categories
(incl. PoS) mentioned in the terminal symbols oc-
curring in this rule. These counts may be nor-
malised or used directly as weights.
We assume here that either the parser produces
a single parse for any sentence (assumption realis-
tic only in case of shallow parsers), or that the best
or at least most probable parse may be selected au-
tomatically, as in case of probabilistic grammars,
or that parses are disambiguated manually. In case
only a non-probabilistic deep parser is available,
and parses are not disambiguated manually, the
Expectation-Maximisation method may be used
to select a probable parse (De?bowski, 2009) or all
parses might be taken into account.
Note that, once a parser is available, such
weights may be calculated automatically and used
repeatedly for tagger evaluation, so the cost of us-
ing this measure is not significantly higher than
the cost of intrinsic measures, while at the same
time the correlation of the evaluation results with
the extrinsic application is much higher.
3
2.3.3 Importance for Corpus Search
The final variant (many more are imagin-
able) of WPA that we would like to de-
scribe here concerns another application of tag-
ging, namely, for the annotation of corpora.
Various corpus search engines, including the
IMS Open Corpus Workbench (http://cwb.
sourceforge.net/) and Poliqarp (http://
poliqarp.sourceforge.net/) allow the
user to search for particular parts of speech and
grammatical categories. Obviously, the tagger
should maximise the quality of the disambigua-
tion of those categories which occur frequently
in corpus queries, i.e., the weights should corre-
spond to the frequencies of particular categories
(and PoS) in user queries. Note that the only re-
source needed to calculate weights are the logs of
a corpus search engine.
An experiment involving an implementation of
this measure is described in detail in ? 3.
2.4 Conditional Weighted Positional
Accuracy
The importance and difficulty of a category may
depend on the part of speech. For example, af-
ter case syncretisms, gender ambiguity is one of
the main problems for the current taggers of Pol-
ish. But this problem concerns mainly pronouns
and adjectives, where the systematic gender syn-
cretism is high. On the other hand, nouns do not
inflect for gender, so only some nominal forms
are ambiguous with respect to gender. Moreover,
gerunds, which also bear gender, are uniformly
neuter, so here part of speech alone uniquely de-
termines the value of this category.
A straightforward extension of WPA capitalis-
ing on these observations is what we call con-
ditional weighted positional accuracy (CWPA),
where weights of morphosyntactic categories are
conditioned on PoS.
Note that not all variants of WPA may be easily
generalised to CWPA; although such an extension
is obvious for the average ambiguity (? 2.3.1), it is
less clear for the other two variants. For parsing-
related WPA, we assume that, even if a given rule
does not mention the PoS of a terminal symbol,2
2For example, in unification grammars and constraint-
based grammars a terminal may be identified only by the
that PoS may be read off the parse tree, so the con-
ditional weights may still be calculated. On the
other hand, logs of a corpus search engine are typ-
ically not sufficient to calculate such conditional
weights; e.g., a query for a sequence of 5 genitive
words occurring in logs would have to be rerun
on the corpus again in order to find out parts of
speech of the returned 5-word sequences. For a
large number of queries on a large corpus, this is
a potentially costly operation.
It is also not immediately clear how to gener-
alise precision and recall from WPA to CWPA.
Returning to the example above, where t1 =
ger:sg:nom:n:perf:aff is assigned in-
stead of the correct t2 = subst:sg:nom:n, we
note that the weights of number, case and gender
may now (and should, at least in case of gender!)
be different for the two parts of speech involved.
Hence, precision needs to be calculated with re-
spect to the weights for the automatically assigned
part of speech, and recall ? taking into account
weights for the gold standard part of speech:
P =
?t?1t?2w(t
?
1) +
?
c?C(t1,t2) ?tc1tc2w(c|t?1)
w(t?1) +
?
c?C(t1) w(c|t?1)
,
R =
?t?1t?2w(t
?
2) +
?
c?C(t1,t2) ?tc1tc2w(c|t?2)
w(t?2) +
?
c?C(t2) w(c|t?2)
,
where t? is the PoS of tag t, w(p) is the weight
of the part of speech p, w(c|p) is the conditional
weight of the category c for PoS p, C(t) is the set
of morphosyntactic categories of tag t, C(t1, t2)
is the set of morphosyntactic categories common
to tags t1 and t2, tc is the value of category c in
tag t, and ?ij is the Kronecker delta (equal to 1 if
i = j, and to 0 otherwise). Hence, for the example
above, these formulas may be simplified to:
P =
?
c?{n,c,g}w(c|ger)
w(ger) +?c?{n,c,g,a,neg}w(c|ger)
,
R =
?
c?{n,c,g}w(c|subst)
w(subst) +?c?{n,c,g}w(c|subst)
,
where n, c, g, a and neg stand for number, case,
gender, aspect and negation.
values of some of its categories, as in the following simple
rule, specifying prepositional phrases as a preposition gov-
erning a specific case and a non-empty sequence of words
bearing that case: PPcase=C ? Pcase=C X+case=C.
4
3 Experiment
To evaluate behaviour of the proposed metrics, a
number of experiments were performed using the
manually disambiguated part of the IPI PAN Cor-
pus of Polish (Przepi?rkowski, 2005). This sub-
corpus consists of 880 000 segments. Two tag-
gers of Polish were tested. TaKIPI (Piasecki and
Godlewski, 2006) is a tagger which was used for
automatic disambiguation of the remaining part of
the aforementioned corpus. It is a statistical clas-
sifier based on decision trees combined with some
automatically extracted, hand-crafted rules. This
tagger by default sometimes assigns more than
one tag to a segment, what is consistent with the
golden standard. There is a setting which allows
this behaviour to be switched off. This tagger was
tested with both settings. The other tagger is a
prototype version of this Brill tagger, presented by
Acedan?ski and Go?uchowski in (Acedan?ski and
Go?uchowski, 2009).
For comparison, four metrics were used: stan-
dard metrics for full tags and only parts of speech,
as well as Positional Accuracy and Weighted Posi-
tional Accuracy. For the last measure, the weights
were obtained by analysing logs of user queries of
the Poliqarp corpus search engine. Occurrences
of queries involving particular grammatical cat-
egories were counted and used as weights. Ob-
tained results are presented in Table 1.
Table 1: Occurrences of particular grammatical
categories in query logs of the Poliqarp corpus
search engine.
Category # occurrences
POS 37771
CASE 14055
NUMBER 2074
GENDER 552
ASPECT 222
PERSON 186
DEGREE 81
ACCOMMODABILITY 25
POST-PREP. 8
NEGATION 7
ACCENTABILITY 5
AGGLUTINATION 4
3.1 Scored information retrieval metrics
In ? 2 a number of methods of assigning a score
to a pair of tags were presented. From now on,
let name them scoring functions. One could use
them directly for evaluation, given that both the
tagger and the golden standard always assign a
single interpretation to each segment. This is not
the case for the corpus we use, hence we pro-
pose generalisation of standard information re-
trieval metrics (precision, recall and F-measure)
as well as strong and weak correctness (Kar-
wan?ska and Przepi?rkowski, 2009) to account for
scoring functions.
Denote by Ti and Gi the sets of tags assigned by
the tagger and the golden standard, accordingly,
to the i-th segment of the tagged text. The set of
all tags in the tagset is denoted by T. The scoring
function used is score:T ? T ? [0, 1]. Also, to
save up on notation, we define
score(t, A) := max
t??A
score(t, t?)
Now, given the text has n segments, we take
P =
?n
i=1
?
t?Ti score(t, Gi)?n
i=1 |Ti|
R =
?n
i=1
?
g?Gi score(g, Ti)?n
i=1 |Gi|
F = 2 ? P ?RP +R
WC =
?n
i=1 maxt?Ti score(t, Gi)
n
SC =
?n
i=1 min({score(t, Gi): t ? Ti}
? {score(g, Ti): g ? Gi})
n
Intuitions for scored precision and recall are that
precision specifies the percent of tags assigned by
the tagger which have a high score with some cor-
responding golden tag. Analogously recall esti-
mates the percent of golden tags which have high
scores with some corresponding tag assigned by
the tagger. The definition of recall is slightly dif-
ferent than proposed by Zi??ko et al (Zi??ko et
al., 2007) so that recall is never greater than one.3
3For example if the golden standard specifies a single tag
and the tagger determines two tags which all score 0.6 when
compared with the golden, then if we used equations from
Zi??ko et al, we would get the recall of 1.2.
5
3.2 Evaluation results
Now the taggers were trained on the same data
consisting of 90% segments of the corpus and then
tested on the remaining 10%. Results were 10-
fold cross-validated. They are presented in Ta-
bles 2, 3, 4 and 5.
As expected, the values obtained with PA and
WPA fall between the numbers for standard met-
rics calculated with full tags and only the part of
speech. What is worth observing is that the use
of WPA makes values for scored precision and re-
call much closer together. This can be justified
by the fact that the golden standard relatively fre-
quently contains more than one interpretation for
some tags, which differ only in values of less im-
portant grammatical categories. WPA is resilient
to such situations.
One may argue that such scoring functions may
hide a large number of tagging mistakes occurring
in low-weighted categories. But this is not the
case as the clearly most common tagging errors
reported in both (Piasecki and Godlewski, 2006)
and (Acedan?ski and Go?uchowski, 2009) are for
CASE, GENDER and NUMBER. Also, the moti-
vation for weighting grammatical categories is to
actually ignore errors in not important ones. To
be fair, though, one should make sure that the
weights used for evaluation match the actual ap-
plication domain of the analysed tagger, and if no
specific domain is known, using a number of mea-
sures is recommended.
It should also be noted that for classic infor-
mation retrieval metrics, the result of weak cor-
rectness for TaKIPI is more similar to 92.55% re-
ported by the authors (Piasecki and Godlewski,
2006) than 91.30% shown in (Karwan?ska and
Przepi?rkowski, 2009) despite using the same test
corpus and very similar methodology4 as the sec-
ond paper presents.
4 Conclusion
This paper stems from the observation that the
commonly used measure for tagger evaluation,
i.e., accuracy, does not distinguish between com-
pletely incorrect and partially correct interpreta-
4The only difference was not contracting the grammati-
cal category of ACCOMMODABILITY present for masculine
numerals in the golden standard.
tions, even though the latter may be sufficient for
some applications. We proposed a way of grad-
ing tag assignments, by weighting the importance
of particular categories (case, number, etc.) and
the part of speech. Three variants of the weighted
positional accuracy were presented: one intrin-
sic and two application-oriented, and an extension
of WPA to conditional WPA was discussed. The
variant of WPA related to the needs of the users
of a corpus search engine for the National Corpus
of Polish was implemented and its usefulness was
demonstrated. We plan to implement the parsing-
oriented WPA in the future.
We conclude that tagger evaluation is far from
being a closed chapter and the time has come to
adopt more subtle approaches than sheer accuracy,
approaches able to cope with morphological rich-
ness and oriented towards real applications.
References
Acedan?ski, Szymon and Konrad Go?uchowski. 2009.
A morphosyntactic rule-based Brill tagger for
Polish. In K?opotek, Mieczys?aw A., Adam
Przepi?rkowski, S?awomir T. Wierzchon?, and
Krzysztof Trojanowski, editors, Advances in In-
telligent Information Systems ? Design and Ap-
plications, pages 67?76. Akademicka Oficyna
Wydawnicza EXIT, Warsaw.
De?bowski, ?ukasz. 2009. Valence extraction using
the EM selection and co-occurrence matrices. Lan-
guage Resources and Evaluation, 43:301?327.
Hajic?, Jan and Barbora Hladk?. 1997. Probabilistic
and rule-based tagger of an inflective language - a
comparison. In Proceedings of the 5th Applied Nat-
ural Language Processing Conference, pages 111?
118, Washington, DC. ACL.
Hajic?, Jan. 2004. Disambiguation of Rich Inflection.
Karolinum Press, Prague.
Janus, Daniel and Adam Przepi?rkowski. 2007.
Poliqarp: An open source corpus indexer and search
engine with syntactic extensions. In Proceedings of
the ACL 2007 Demo and Poster Sessions, pages 85?
88, Prague.
Karwan?ska, Danuta and Adam Przepi?rkowski. 2009.
On the evaluation of two Polish taggers. In Goz?dz?-
Roszkowski, Stanis?aw, editor, The proceedings of
Practical Applications in Language and Computers
PALC 2009, Frankfurt am Main. Peter Lang. Forth-
coming.
6
Oliva, Karel. 2001. On retaining ambiguity in dis-
ambiguated corpora: Programmatic reflections on
why?s and how?s. TAL (Traitement Automatique des
Langues), 42(2):487?500.
Piasecki, Maciej and Grzegorz Godlewski. 2006. Ef-
fective Architecture of the Polish Tagger. In Sojka,
Petr, Ivan Kopecek, and Karel Pala, editors, TSD,
volume 4188 of Lecture Notes in Computer Science,
pages 213?220. Springer.
Przepi?rkowski, Adam and Marcin Wolin?ski. 2003.
The unbearable lightness of tagging: A case study in
morphosyntactic tagging of Polish. In Proceedings
of the 4th International Workshop on Linguistically
Interpreted Corpora (LINC-03), EACL 2003, pages
109?116.
Przepi?rkowski, Adam. 2005. The IPI PAN Corpus in
Numbers. In Proceedings of the 2nd Language &
Technology Conference, Poznan?, Poland.
Przepi?rkowski, Adam. 2008. Powierzchniowe
przetwarzanie je?zyka polskiego. Akademicka Ofi-
cyna Wydawnicza EXIT, Warsaw.
van Halteren, Hans. 1999. Performance of taggers.
In van Halteren, Hans, editor, Syntactic Wordclass
Tagging, volume 9 of Text, Speech and Language
Technology, pages 81?94. Kluwer, Dordrecht.
Zi??ko, Bartosz, Suresh Manandhar, and Richard Wil-
son. 2007. Fuzzy Recall and Precision for
Speech Segmentation Evaluation. In Proceedings
of 3rd Language & Technology Conference, Poznan,
Poland, October.
7
Table 2: Evaluation results ? standard information retrieval metrics, full tags
Tagger C (%) WC (%) P (%) R (%) F (%)
TaKIPI ? defaults 87.67% 92.10% 89.93% 84.72% 87.25%
TaKIPI ? one tag per seg. 88.68% 91.06% 90.94% 83.78% 87.21%
Brill 90.01% 92.44% 92.26% 85.00% 88.49%
Table 3: Evaluation results ? standard information retrieval metrics, PoS only
Tagger C (%) WC (%) P (%) R (%) F (%)
TaKIPI ? defaults 95.56% 97.52% 95.71% 97.61% 96.65%
TaKIPI ? one tag per seg. 96.53% 96.54% 96.58% 96.71% 96.65%
Brill 98.17% 98.18% 98.20% 98.26% 98.23%
Table 4: Evaluation results ? scored metrics, Positional Accuracy
Tagger C (%) WC (%) P (%) R (%) F (%)
TaKIPI ? defaults 95.23% 96.58% 95.69% 95.44% 95.57%
TaKIPI ? one tag per seg. 95.69% 96.10% 96.12% 95.00% 95.56%
Brill 97.02% 97.43% 97.42% 96.27% 96.84%
Table 5: Evaluation results ? scored metrics, Weighted PA, Poliqarp weights
Tagger C (%) WC (%) P (%) R (%) F (%)
TaKIPI ? defaults 95.20% 96.62% 95.34% 96.56% 95.95%
TaKIPI ? one tag per seg. 95.88% 95.93% 95.97% 95.94% 95.95%
Brill 97.34% 97.40% 97.41% 97.34% 97.38%
8
