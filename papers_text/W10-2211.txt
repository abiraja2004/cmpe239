Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 87?95,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Morpho Challenge competition 2005-2010: Evaluations and results
Mikko Kurimo, Sami Virpioja, Ville Turunen, Krista Lagus
Adaptive Informatics Research Centre
Aalto University, Espoo, Finland
Firstname.Lastname@tkk.fi
Abstract
Morpho Challenge is an annual evalu-
ation campaign for unsupervised mor-
pheme analysis. In morpheme analysis,
words are segmented into smaller mean-
ingful units. This is an essential part in
processing complex word forms in many
large-scale natural language processing
applications, such as speech recognition,
information retrieval, and machine trans-
lation. The discovery of morphemes is
particularly important for morphologically
rich languages where inflection, deriva-
tion and composition can produce a huge
amount of different word forms. Morpho
Challenge aims at language-independent
unsupervised learning algorithms that can
discover useful morpheme-like units from
raw text material. In this paper we de-
fine the challenge, review proposed algo-
rithms, evaluations and results so far, and
point out the questions that are still open.
1 Introduction
Many large-scale natural language processing
(NLP) applications, such as speech recognition,
information retrieval and machine translation, re-
quire that complex word forms are analyzed into
smaller, meaningful units. The discovery of these
units called morphemes is particularly important
for morphologically rich languages where the in-
flection, derivation and composition makes it im-
possible to even list all the word forms that are
used. Various tools have been developed for mor-
pheme analysis of word forms, but they are mostly
based on language-specific rules that are not eas-
ily ported to other languages. Recently, the per-
formance of tools based on language-independent
unsupervised learning from raw text material has
improved significantly and rivaled the language-
specific tools in many applications.
The unsupervised algorithms proposed so far in
Morpho Challenge typically first generate various
alternative morphemes for each word and then se-
lect the best ones based on relevant criteria. The
statistical letter successor variation (LSV) analy-
sis (Harris, 1955) and its variations are quite com-
monly used as generation methods. LSV is based
on the observation that the segment borders be-
tween the sub-word units often co-occur with the
peaks of variation for the next letter. One popu-
lar selection approach is to minimize a cost func-
tion that balances between the size of the corpus
when coded by the morphemes and the size of
the morpheme codebook needed. Selection cri-
teria that produce results resembling the linguis-
tic morpheme segmentation include, for example,
the Minimum Description Length (MDL) princi-
ple and maximum a posteriori (MAP) probability
optimization (de Marcken, 1996; Creutz and La-
gus, 2005).
The Morpho Challenge competition was
launched in 2005 to encourage the machine
learning people, linguists and specialists in NLP
applications to study this field and come together
to compare their best algorithms against each
other. The organizers selected evaluation tasks,
data and metric and performed all the evaluations.
Thus, participation was made easy for people
who were not specialists in the chosen NLP
applications. Participation was open to everybody
with no charge. The competition became popular
right from the beginning and has gained new
participants every year.
Although not all the authors of relevant mor-
pheme analysis algorithms have yet submitted
their algorithms for this evaluation campaign,
more than 50 algorithms have already been eval-
uated. After the first five years of Morpho Chal-
lenge, a lot has been learned on the various pos-
sible ways to solve the problem and how the dif-
ferent methods work in various NLP tasks. How-
87
ever, there are still open questions such as: how to
find meaning for the obtained unsupervised mor-
phemes, how to disambiguate among the alterna-
tive analyses of one word, and how to use context
in the analysis. Another recently emerged ques-
tion that is the special topic in 2010 competition
is how to utilize small amounts of labeled data
and semi-supervised learning to further improve
the analysis.
2 Definition of the challenge
2.1 Morphemes and their evaluation
Generally, the morphemes are defined as the
smallest meaningful units of language. Rather
than trying to directly specify which units are
meaningful, the Morpho Challenge aims at find-
ing units that would be useful for various practical
NLP applications. The goal is to find automatic
methods that can discover suitable units using un-
supervised learning directly on raw text data. The
methods should also not be restricted to certain
languages or include many language and applica-
tion dependent parameters that needed to be hand
tuned for each task separately. The following three
goals have been defined as the main scientific ob-
jectives for the challenge: (1) To learn of the phe-
nomena underlying word construction in natural
languages. (2) To discover approaches suitable for
a wide range of languages. (3) To advance ma-
chine learning methodology.
The evaluation tasks, metrics and languages
have been designed based on the scientific objec-
tives of the challenge. It can not be directly ver-
ified how well an obtained analysis reflects the
word construction in natural languages, but intu-
itively, the methods that split everything into let-
ters or pre-specified letter n-grams, or leave the
word forms unanalyzed, would not be very in-
teresting solutions. An interesting thing that can
be evaluated, however, is how close the obtained
analysis is to the linguistic gold standard mor-
phemes that can be obtained from CELEX or
various language-dependent rule-based analyzers.
The exact definition of the morphemes, tags, or
features available in the gold standard to be uti-
lized in the comparison should be decided and
fixed for each language separately.
To verify that a proposed algorithm works in
various languages would, ideally, require running
the evaluations on a large number of languages
that would be somehow representative of various
important language families. However, the re-
sources available for both computing and evalu-
ating the analysis in various applications and lan-
guages are limited. The suggested and applicable
compromise is to select morphologically rich lan-
guages where the morpheme analysis is most use-
ful and those languages where interesting state-of-
the-art evaluation tasks are available. By including
German, Turkish, Finnish and Arabic, many inter-
esting aspects of concatenative morphology have
already been covered.
While the comparison against the linguistic
gold standard morphemes is an interesting sub-
goal, the main interest in running the Morpho
Challenge is to find out how useful the proposed
morpheme analyses are for various practical NLP
applications. Naturally, this is best evaluated
by performing evaluations in several state-of-the-
art application tasks. Due to the limitations of
the resources, the applications have been selected
based on the importance of the morpheme analy-
sis for the application, on the availability of open
state-of-the-art evaluation tasks, and on the effort
needed to run the actual evaluations.
2.2 Unsupervised and semi-supervised
learning
Unsupervised learning is the task of learning with-
out labeled data. In the context of morphology dis-
covery, it means learning without knowing where
morpheme borders are, or which morphemes exist
in which words. Unsupervised learning methods
have many attractive features for morphological
modeling, such as language-independence, inde-
pendence of any particular linguistic theory, and
easy portability to a new language.
Semi-supervised learning can be approached
from two research directions, namely unsuper-
vised and supervised learning. In an essentially
unsupervised learning task there may exist some
labeled (classified) data, or some known links be-
tween data items, which might be utilized by the
(typically generative) learning algorithms. Turned
around, an essentially supervised learning task,
such as classification or prediction, may benefit
also from unlabeled data which is typically more
abundantly available.
In morphology modeling one might consider
the former setup to be the case: the learning task
is essentially that of unsupervised modeling, and
morpheme labels can be thought of as known links
88
between various inflected word forms.
Until 2010 the Morpho Challenge has been de-
fined only as an unsupervised learning task. How-
ever, since small samples of morphologically la-
beled data can be provided already for quite many
languages, also the semi-supervised learning task
has become of interest.
Moreover, while there exists a fair amount of
research and now even books on semi-supervised
learning (Zhu, 2005; Abney, 2007; Zhu, 2010),
it has not been as widely studied for structured
classification problems like sequence segmenta-
tion and labeling (cf. e.g. (Jiao et al, 2006)). The
semi-supervised learning challenge introduced for
Morpho Challenge 2010 can thus be viewed as an
opportunity to strengthen research in both mor-
phology modeling as well as in semi-supervised
learning for sequence segmentation and labeling
in general.
3 Review of Morpho Challenge
competitions so far
3.1 Evaluation tasks, metrics, and languages
The evaluation tasks and languages selected for
Morpho Challenge evaluations are shown in Fig-
ure 1. The languages where evaluations have been
prepared are Finnish (FIN), Turkish (TUR), En-
glish (ENG), German (GER), and Arabic (ARA).
First the morphemes are compared to linguis-
tic gold standards in direct morpheme segmen-
tation (2005) and full morpheme analysis (since
2007). The practical NLP application based eval-
uations are automatic speech recognition (ASR),
information retrieval (IR) and statistical machine
translation (SMT). Morphemes obtained by semi-
supervised learning can be evaluated in parallel
with the unsupervised morphemes. For IR, eval-
uation has also been extended for full sentences,
where the morpheme analysis can based on con-
text. The various suggested and tested evaluations
are defined in this section.
year new languages new tasks
2005 FIN, TUR, ENG segmentation, ASR
2007 GER full analysis, IR
2008 ARA context IR
2009 - SMT
2010 - semi-supervised
Table 1: The evolution of the evaluations. The
acronyms are explained in section 3.1.
3.1.1 Comparisons to linguistic gold standard
The first Morpho Challenge in 2005 (Kurimo et
al., 2006) considered unsupervised segmentation
of words into morphemes. The evaluation was
based on comparing the segmentation boundaries
given by the competitor?s algorithm to the bound-
aries obtained from a gold standard analysis.
From 2007 onwards, the task was changed to
full morpheme analysis, that is, the algorithm
should not only locate the surface forms (i.e., word
segments) of the morphemes, but find also which
surface forms are realizations (allomorphs) of the
same underlying morpheme. This generalizes the
task for finding more meaningful units than just
the realizations of morphemes that may be just in-
dividual letters or even empty strings. In applica-
tions this is useful when it is important to identify
which units carry the same meaning even if they
have different realizations in different words.
As an unsupervised algorithm cannot find the
morpheme labels that would equal to the labels in
the gold standard, the evaluation has to be based
on what word forms share the same morphemes.
The evaluation procedure samples a large num-
ber of word pairs, such that both words in the
pair have at least one morpheme in common, from
both the proposed analysis and the gold standard.
The first version of the method was applied in
2007 (Kurimo et al, 2008) and 2008 (Kurimo et
al., 2009a), and minor modifications were done in
2009 (Kurimo et al, 2009b). However, the orga-
nizers have reported the evaluation results of the
2007 and 2008 submissions also with the new ver-
sion, thus allowing a direct comparison between
them. A summary of these results for English,
Finnish, German and Turkish for the best algo-
rithms is presented in Table 2. The evaluations
in 2008 and 2009 were also performed on Arabic,
but these results and not comparable, because the
database and the gold standard was changed be-
tween the years. The exact annual results for all
participants as well as the details of the evaluation
in each year can be reviewed in the annual evalu-
ation reports (Kurimo et al, 2006; Kurimo et al,
2008; Kurimo et al, 2009a; Kurimo et al, 2009b).
Already the linguistic evaluation of Morpho
Challenge 2005 applied some principles that have
been used thereafter: (1) The evaluation is based
on a subset of the word forms given as training
data. This not only makes the evaluation proce-
dure lighter, but also allows changing the set when
89
English Finnish
Method P R F Method P R F
2009 2009
Allomorfessor 68.98 56.82 62.31 Monson PMU 47.89 50.98 49.39
Monson PMU 55.68 62.33 58.82 Monson PMM 51.75 45.42 48.38
Lignos 83.49 45.00 58.48 Spiegler PROMODES C 41.20 48.22 44.44
2008 2008
Monson P+M 69.59 65.57 67.52 Monson P+M 65.21 50.43 56.87
Monson ParaMor 63.32 51.96 57.08 Monson ParaMor 49.97 37.64 42.93
Zeman 1 67.13 46.67 55.06 Monson Morfessor 79.76 24.95 38.02
2007 2007
Monson P+M 70.09 67.38 68.71 Bernhard 2 63.92 44.48 52.45
Bernhard 2 67.42 65.11 66.24 Bernhard 1 78.11 29.39 42.71
Bernhard 1 75.61 57.87 65.56 Bordag 5a 72.45 27.21 39.56
German Turkish
Method P R F Method P R F
2009 2009
Monson PMU 52.53 60.27 56.14 Monson PMM 48.07 60.39 53.53
Monson PMM 51.07 57.79 54.22 Monson PMU 47.25 60.01 52.88
Monson PM 50.81 47.68 49.20 Monson PM 49.54 54.77 52.02
2008 2008
Monson P+M 64.06 61.52 62.76 Monson P+M 66.78 57.97 62.07
Monson Morfessor 70.73 38.82 50.13 Monson ParaMor 57.35 45.75 50.90
Monson ParaMor 56.98 42.10 48.42 Monson Morfessor 77.36 33.47 46.73
2007 2007
Monson P+M 69.96 55.42 61.85 Bordag 5a 81.06 23.51 36.45
Bernhard 2 54.02 60.77 57.20 Bordag 5 81.19 23.44 36.38
Bernhard 1 66.82 42.48 51.94 Zeman 77.48 22.71 35.13
Table 2: The summary of the best three submitted methods for years 2009, 2008 and 2007 using the
linguistic evaluation of Morpho Challenge 2009. The complete results tables by the organizers are avail-
able from http://www.cis.hut.fi/morphochallenge2009/. The three columns numbers
are precision (P), recall (R), and F-measure (F). The best F-measure for each language is in boldface,
and the best result that is not based on a direct combination of two other methods is underlined.
the old one is considered to be ?overlearned?. (2)
The frequency of the word form plays no role in
evaluation; rare and common forms are equally
likely to be selected, and have equal weight to
the score. (3) The evaluation score is balanced F-
measure, the harmonic mean of precision and re-
call. Precision measures how many of the choices
made by the algorithm are matched in gold stan-
dard; recall measures how many of the choices
in the gold standard are matched in the proposed
analysis. (4) If the linguistic gold standard has
several alternative analysis for one word, for full
precision, it is enough that one of the alternatives
is equivalent to the proposed analysis. The same
holds the other way around for recall.
All of the principles can be also criticized. For
example, evaluation based on the full set would
provide more trustworthy estimates, and common
word forms are more significant in any practical
application. However, the third and the fourth
principle have problems that can be considered to
be more serious.
Balanced F-measure favors methods that are
able to get near-to-equal precision and recall. As
many algorithms can be tuned to give either more
or less morphemes per word than in the default
case, this encourages using developments sets to
optimize the respective parameters. The winning
methods in Challenge 2009?Monson?s ParaMor-
Morfessor Union (PMU) and ParaMor-Morfessor
90
Mimic (PMM) (Monson et al, 2009), and Al-
lomorfessor (Virpioja and Kohonen, 2009)?did
this, more or less explicitly.1 Moreover, it can
be argued that the precision would be more im-
portant than recall in many applications, or, more
generally, that the optimal balance between preci-
sion and recall is application dependent. We see
two solutions for this: Either the optimization for
F-measure should be allowed with a public devel-
opment set, which means moving towards semi-
supervised direction, or precision-recall curves
should be compared, which means more complex
evaluations.
The fourth principle causes problems, if the
evaluated algorithms are allowed to have alterna-
tive analyses for each word. If several alternative
analyses are provided, the obtained precision is
about the average over the individual analyses, but
the recall is based on the best of the alternatives.
This property have been exploited in Challenges
2007 and 2008 by combining the results of two
algorithms as alternative analyses. The method,
Monson?s ParaMor+Morfessor (P+M) holds still
the best position measured in F-measures in all
languages. Combining even better-performing
methods in a similar manner would increase the
scores further. To fix this problem, either the eval-
uation metric should require matching number of
alternative analyses to get the full points, or the
symmetry of the precision and recall measures has
to be removed.
Excluding the methods that combine the anal-
yses of two other methods as alternative ones, we
see that the best F-measure (underlined in Table 2)
is held by Monson?s ParaMor-Morfessor Mimic
from 2009 (Monson et al, 2009) in Turkish and
Bernhard?s method 2 from 2007 (Bernhard, 2006)
in all the other three languages. This means that
except for Turkish, there is no improvement in the
results over the three years. Furthermore, both
of the methods are based purely on segmentation,
and so are all the other top methods presented
in Table 2 except for Bordag?s methods (Bordag,
2006) and Allomorfessor (Virpioja and Kohonen,
2009).
3.1.2 Speech recognition
A key factor in the success of large-vocabulary
continuous speech recognition is the system?s abil-
1Allomorfessor was trained with a pruned data to obtain
a higher recall, whereas ParaMor-Morfessor is explicitly op-
timized for F-measure with a separate Hungarian data set.
ity to limit the search space using a statistical lan-
guage model. The language model provides the
probability of different recognition hypothesis by
using a model of the co-occurence of its words
and morphemes. A properly smoothed n-gram is
the most conventional model. The n-gram should
consist of modeling units that are suitable for the
language, typically words or morphemes.
In Morpho Challenge state-of-the-art large-
vocabulary speech recognizers have been built for
evaluations in Finnish and Turkish (Kurimo et al,
2006). The various morpheme analysis algorithms
have been compared by measuring the recogni-
tion accuracy with different language models each
trained and optimized based on units from one of
the algorithms. The best results were quite near
to each other, but Bernhard (Bernhard, 2006) and
Morfessor Categories MAP were at the top for
both languages.
3.1.3 Information retrieval
In the information retrieval task, the algorithms
were tested by using the morpheme segmentations
for text retrieval. To return all relevant documents,
it is important to match the words in the queries to
the words in the documents irrespective of which
word forms are used. Typically, a stemming al-
gorithm or a morphological analyzer is used to re-
duce the inflected forms to their stem or base form.
The problem with these methods is that specific
rules need to be crafted for each language. How-
ever, these approaches were also tested for com-
parison purposes. The IR experiments were car-
ried out by replacing the words in the corpora and
queries by the suggested morpheme segmenta-
tions. Test corpora, queries and relevance assess-
ments were provided by Cross-Language Evalua-
tion Forum (CLEF) (Agirre et al, 2008).
To test the effect of the morpheme segmen-
tation, the number of other variables will have
to be minimized, which poses some challenges.
For example, the term weighting method will af-
fect the results and different morpheme analyz-
ers may perform optimally with different weight-
ing approaches. TFIDF and Okapi BM25 term
weighting methods have been tested. In the 2007
Challenge, it was noted that Okapi BM25 suffers
greatly if the corpus contains a lot of frequent
terms. These terms are often introduced when the
algorithms segment suffixes from stems. To over-
come this problem, a method for automatically
generating stop lists of frequent terms was intro-
91
duced. Any term that occurs more times in the cor-
pus than a certain threshold is added to the stop list
and excluded from indexing. The method is quite
simple, but it treats all morpheme analysis meth-
ods equally as it does not require the algorithm
to tag which morphemes are stems and which are
suffixes. The generated stoplists are also reason-
able sized and the results are robust with respect
to the stop list cutoff parameter. With a stop list,
Okapi BM25 clearly outperformed TFIDF rank-
ing method for all algorithms. However, the prob-
lem of choosing the term weighting approach that
treats all algorithms in an optimal way remains
open.
Another challenge is analyzing the results as it
is hard to achieve statistically significant results
with the limited number of queries (50-60) that
were available. In fact, in each language 11-17 of
the best algorithms belonged to the ?top group?,
that is, had no statistically different result to the
top performer of the language. To improve the
significance of the results, the number of queries
should be increased. This is a known problem in
the field of IR. However, it is important to test the
methods in a real life application and if an algo-
rithm gives good results across languages, there is
evidence that it is doing something useful.
Some conclusions can be drawn from the re-
sults. The language specific reference methods
(Porter stemming for English, two-layer morpho-
logical analysis for Finnish and German) give the
best results, but the best unsupervised algorithms
are almost at par and the differences are not signif-
icant. For German and Finnish, the best unsuper-
vised methods can also beat in a statistically sig-
nificant way the baseline of not doing any segmen-
tation or stemming. The best algorithms that per-
formed well across languages are ParaMor (Mon-
son et al, 2008), Bernhard (Bernhard, 2006), Mor-
fessor Baseline, andMcNamee (McNamee, 2008).
Comparing the results to the linguistic evalua-
tion (section 3.1.1), it seems that methods that per-
form well at the IR task tend to have good preci-
sion in the linguistic task, with exceptions. Thus,
in the IR task it seems important not to overseg-
ment words. One exception is the method (Mc-
Namee, 2008) which simply splits the words into
equal length letter n-grams. The method gives sur-
prisingly good results in the IR task, given the sim-
plicity, but suffers from low precision in the lin-
guistic task.
3.1.4 Machine translation
In phrase-based statistical machine translation
process there are two stages where morpheme
analysis and segmentation of the words into mean-
ingful sub-word units is needed. The first stage
is the alignment of the parallel sentences in the
source and target language for training the transla-
tion model. The second one is training a statistical
language model for the production of fluent sen-
tences in a morphologically rich target language.
In the machine translation tasks used in the
Morpho Challenge, the focus has so far been in
the alignment problem. In the evaluation tasks in-
troduced in 2009 the language-pairs were Finnish-
English and German-English. To obtain state-of-
the-art results, the evaluation consists of minimum
Bayes risk (MBR) combination of two transla-
tion systems trained on the same data, one us-
ing words and the other morphemes as the ba-
sic modeling units (de Gispert et al, 2009). The
various morpheme analysis algorithms are com-
pared by measuring the translation performance
for different two-model combinations where the
word-based model is always the same, but the
morpheme-based model is trained based on units
from each of the algorithms in turns.
Because the machine translation evaluation has
yet been tried only in 2009, it is difficult to draw
conclusions about the results yet. However, the
Morfessor Baseline algorithm seems to be partic-
ularly difficult to beat both in Finnish-German and
German-English task. The differences between
the best results are small, but the ranking in both
tasks was the same: 1. Morfessor Baseline, 2. Al-
lomorfessor, 3. The linguistic gold standard mor-
phemes (Kurimo et al, 2009b).
3.2 Evaluated algorithms
This section attempts to describe very briefly some
of the individual morpheme analysis algorithms
that have been most successful in the evaluations.
Morfessor Baseline (Creutz and Lagus, 2002):
This is a public baseline algorithm based on jointly
minimizing the size of the morph codebook and
the encoded size of the all the word forms using
the minimum description length MDL cost func-
tion. The performance is above average for all
evaluated tasks in most languages.
Allomorfessor (Kohonen et al, 2009; Virpi-
oja and Kohonen, 2009): The development of
this method was based on the observation that the
92
Finnish German English
0.25
0.3
0.35
0.4
0.45
0.5
0.55
 
 
Morfessor baseline
2007 Bernhard
2008 McNamee 4?gram
2008 Monson P+M
2009 Monson PMU
2009 Lignos
2009 Allomorfessor
Figure 1: Mean Average Precision (MAP) values for some of the best algorithms over the years in the IR
task. The upper horizontal line shows the ?goal level? for each language, i.e. the performance of the best
language specific reference method. The lower line shows the baseline reference of doing no stemming
or analysis.
morph level surface forms of one morpheme are
often very similar and the differences occur close
to the morpheme boundary. Thus, the allomor-
phemes could be modeled by simple mutations.
It has been implemented on top of the Morfessor
Baseline using maximum a posteriori (MAP) opti-
mization. This model slightly improves the perfor-
mance in the linguistic evaluation in all languages
(Kurimo et al, 2009b), but in IR and SMT there is
no improvement yet.
Morfessor Categories MAP (Creutz and La-
gus, 2005): In this method hidden Markov models
are used to incorporate morphotactic categories for
theMorfessor Baseline. The structure is optimized
by MAP and yields slight improvements in the lin-
guistic evaluation for most languages, but not for
IR or SMT tasks.
Bernhard (Bernhard, 2006): This has been one
of the best performing algorithms in Finnish, En-
glish and German linguistic evaluation and in IR
(Kurimo et al, 2008). First a list of the most likely
prefixes and suffixes is extracted and alternative
segmentations are generated for the word forms.
Then the best ones are selected based on cost func-
tions that favour most frequent analysis and some
basic morphotactics.
Bordag (Bordag, 2006): This method applies
iterative LSV and clustering of morphs into mor-
phemes. The performance in the linguistic eval-
uation is quite well for Turkish and decent for
Finnish (Kurimo et al, 2008).
ParaMor (Monson et al, 2008): This method
applies an unsupervised model for inflection rules
and suffixation for the stems by building linguisti-
cally motivated paradigms. It has obtained one of
the top performances for all languages when com-
bined with the Morfessor Baseline (Kurimo et al,
2009a). Various combination methods have been
tested: union, weighted probabilistic average and
proposing both the analyses (Monson et al, 2009).
Lignos (Lignos et al, 2009): This method is
based on the observation that the derivation of
the inflected forms can be modeled as transfor-
mations. The best transformations can be found
by optimizing the simplicity and frequency. This
method performs much better in English than in
the other languages (Kurimo et al, 2009b).
Promodes (Spiegler et al, 2009): This method
presents a probabilistic generative model that ap-
plies LSV and combines multiple analysis using a
committee. It seems to generate a large amount
of short morphemes, which is difficult for many
of the practical applications. However, it obtained
the best performance for the linguistic evaluation
in Arabic 2009 (Kurimo et al, 2009b), but did not
survive as well in other languages, and particularly
not in the IR application.
4 Open questions and challenges
Although more than 50 algorithms have already
been tested in the Morpho Challenge evaluations
and many lessons have been learned from the re-
sults and discussions, many challenges are still
open and untouched. In fact, the attempts to solve
the problem have perhaps produced even more
open questions than there were in the beginning.
93
The main new and open challenges are described
in this section.
What is the best analysis algorithm? Some
of the suggested algorithms have produced good
test results and some even in several tasks and lan-
guages, such as Bernhard (Bernhard, 2006), Mon-
son ParaMor+Morfessor (Monson et al, 2008)
and Allomorfessor (Virpioja and Kohonen, 2009).
However, none of the methods perform really well
in all the evaluation tasks and languages and their
mutual performance differences are often rather
small, even though the morphemes and the al-
gorithmic principles are totally different. Thus,
no dominant morpheme analysis algorithm have
been found. Furthermore, reaching the perfor-
mance level that rivals, or even sometimes domi-
nates, the rule-based and language-dependent ref-
erence methods does not mean that the solutions
are sufficient. Often the limited coverage or un-
suitable level of details in the analysis for the task
in the reference methods just indicates that they
are not sufficient either and better solutions are
needed. Another observation which complicates
the finding and determination of the best algorithm
is that in some tasks, such as statistical language
models for speech recognition, very different al-
gorithms can reach the same performance, because
advanced modelling methods can compensate for
unsuitable morpheme analysis.
What is the meaning of the morphemes? In
some of the fundamental applications of mor-
pheme analysis, such as text understanding, mor-
pheme segmentation alone is only part of the solu-
tion. Even more important is to find the meaning
for the obtained morphemes. The extension of the
segmentation of words into smaller units to iden-
tification of the units that correspond to the same
morpheme is a step taken to this direction, but the
question of the meaning of the morpheme is still
open. However, in the unsupervised way of learn-
ing, solutions to this may be so tightly tied to the
applications that much more complex evaluations
would be needed.
How to evaluate the alternative analyses? It
is clear that when a word form is separated from
the sentence context where it was used, the mor-
pheme analysis easily becomes ambiguous. In the
Morpho Challenge evaluations this has been taken
into account by allowing multiple alternative anal-
yses. However, in some evaluations, for exam-
ple, in the measurement of the recall of the gold
standard morphemes, this leads to unwanted re-
sults and may favour methods that always provide
a large number of alternative analysis.
How to improve the analysis using context?
A natural way to disambiguate the analysis in-
volves taking the sentence context into account.
Some of the Morpho Challenge evaluations, for
example, the information retrieval, allow this op-
tion when the source texts and queries are given.
However, this has not been widely tried yet by
the participants, probably because of the increased
computational complexity of the modelling task.
How to effectively apply semi-supervised
learning? In semi-supervised learning, a small set
of labeled data in the form of gold standard anal-
ysis for the word forms are provided. This data
can be used for improving the unsupervised solu-
tions based on unlabeled data in several ways: (1)
The labeled data is used for tuning some learning
parameters, followed by an unsupervised learning
process for the unlabeled data. (2) The labeled
morphemes are used as an ideal starting point
to bootstrap the learning on the unlabeled words
(self-training). (3) Using the EM algorithm for es-
timating a generative model, the unlabeled cases
can be treated as missing data.
The best and most practical way of using the
partly labeled data will be determined in future
when the semi-supervised task has been evaluated
in the future Morpho Challenge evaluations. For
the first time this task will be evaluated in the on-
going Morpho Challenge 2010.
Acknowledgments
We are grateful to the University of Leipzig,
University of Leeds, Computational Linguistics
Group at University of Haifa, Stefan Bordag,
Ebru Arisoy, Nizar Habash, Majdi Sawalha, Eric
Atwell, and Mathias Creutz for making the data
and gold standards in various languages available
to the Challenge. This work was supported by the
Academy of Finland in the project Adaptive In-
formatics, the graduate schools in Language Tech-
nology and Computational Methods of Informa-
tion Technology, in part by the GALE program of
the Defense Advanced Research Projects Agency,
Contract No. HR0011-06-C-0022, and in part by
the IST Programme of the European Community,
under the FP7 project EMIME (213845) and PAS-
CAL Network of Excellence.
94
References
Steven Abney. 2007. Semisupervised Learning
for Computational Linguistics. Chapman and
Hall/CRC.
Eneko Agirre, Giorgio M. Di Nunzio, Nicola Ferro,
Thomas Mandl, and Carol Peters. 2008. CLEF
2008: Ad hoc track overview. In Working Notes for
the CLEF 2008 Workshop.
Delphine Bernhard. 2006. Unsupervised morpholog-
ical segmentation based on segment predictability
and word segments alignment. In Proc. PASCAL
Challenge Workshop on Unsupervised segmentation
of words into morphemes, Venice, Italy. PASCAL
European Network of Excellence.
Stefan Bordag. 2006. Two-step approach to unsuper-
vised morpheme segmentation. In Proc. of the PAS-
CAL Challenge Workshop on Unsupervised segmen-
tation of words into morphemes, Venice, Italy. PAS-
CAL European Network of Excellence.
Mathias Creutz and Krista Lagus. 2002. Unsu-
pervised discovery of morphemes. In Proc. SIG-
PHON/ACL?02, pages 21?30.
Mathias Creutz and Krista Lagus. 2005. Inducing the
morphological lexicon of a natural language from
unannotated text. In Proc. AKRR?05, pages 106?
113.
Adria de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum bayes risk
combination of translation hypothesis from alter-
native morphological decompositions. In Proc.
NAACL?09, pages 73-76.
C. G. de Marcken. 1996. Unsupervised Language Ac-
quisition. Ph.D. thesis, MIT.
Zellig S. Harris. 1955. From phoneme to morpheme.
Language, 31(2):190?222. Reprinted 1970 in Pa-
pers in Structural and Transformational Linguistics,
Reidel Publishing Company, Dordrecht, Holland.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved
sequence segmentation and labeling. In Proc.
ACL?06, pages 209?216.
Oskar Kohonen, Sami Virpioja, and Mikaela Klami.
2009. Allomorfessor: Towards unsupervised mor-
pheme analysis. In Evaluating systems for Mul-
tilingual and MultiModal Information Access, 9th
Workshop of the Cross-Language Evaluation Forum,
CLEF 2008, Revised Selected Papers, Lecture Notes
in Computer Science , Vol. 5706. Springer.
Mikko Kurimo, Mathias Creutz, and Krista Lagus.
2006. Unsupervised segmentation of words into
morphemes - challenge 2005, an introduction and
evaluation report. In Proc. PASCAL Challenge
Workshop on Unsupervised segmentation of words
into morphemes, Venice, Italy. PASCAL European
Network of Excellence.
Mikko Kurimo, Mathias Creutz, and Matti Varjokallio.
2008. Morpho Challenge evaluation using a linguis-
tic Gold Standard. In Advances in Multilingual and
MultiModal Information Retrieval, 8th Workshop of
the Cross-Language Evaluation Forum, CLEF 2007,
Revised Selected Papers, Lecture Notes in Computer
Science , Vol. 5152, pages 864?873. Springer.
Mikko Kurimo, Ville Turunen, and Matti Varjokallio.
2009a. Overview of Morpho Challenge 2008.
In Evaluating systems for Multilingual and Mul-
tiModal Information Access, 9th Workshop of the
Cross-Language Evaluation Forum, CLEF 2008,
Revised Selected Papers, Lecture Notes in Computer
Science , Vol. 5706. Springer.
Mikko Kurimo, Sami Virpioja, Ville T. Turunen,
Graeme W. Blackwood, and William Byrne. 2009b.
Overview and results of Morpho Challenge 2009. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece.
Constantine Lignos, Erwin Chan, Mitchell P. Marcus,
and Charles Yang. 2009. A rule-based unsupervised
morphology learning framework. In Working Notes
for the CLEF 2009 Workshop, Corfu, Greece.
Paul McNamee. 2008. Retrieval experiments at mor-
pho challenge 2008. InWorking Notes for the CLEF
2008 Workshop, Aarhus, Denmark, September.
Christian Monson, Jaime Carbonell, Alon Lavie, and
Lori Levin. 2008. ParaMor: Finding paradigms
across morphology. In Advances in Multilingual
and MultiModal Information Retrieval, 8th Work-
shop of the Cross-Language Evaluation Forum,
CLEF 2007, Revised Selected Papers, Lecture Notes
in Computer Science , Vol. 5152. Springer.
Christian Monson, Kristy Hollingshead, and Brian
Roard. 2009. Probabilistic paraMor. In Working
Notes for the CLEF 2009 Workshop, Corfu, Greece,
September.
Sebastian Spiegler, Bruno Golenia, and Peter Flach.
2009. PROMODES: A probabilistic generative
model for word decomposition. In Working Notes
for the CLEF 2009 Workshop, Corfu, Greece,
September.
Sami Virpioja and Oskar Kohonen. 2009. Unsuper-
vised morpheme discovery with Allomorfessor. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece, September.
Xiaojin Zhu. 2005. Semi-supervised learning litera-
ture survey. Technical Report 1530, Computer Sci-
ences, University of Wisconsin-Madison.
Xiaojin Zhu. 2010. Semi-supervised learning. In En-
cyclopedia of Machine Learning. To appear.
95
