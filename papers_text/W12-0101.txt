Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 1?9,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Semantic Web based Machine Translation
Bettina Harriehausen-Mu?hlbauer
University of Applied Sciences
Darmstadt, Germany
bettina.harriehausen@h-da.de
Timm Heuss
University of Applied Sciences
Darmstadt, Germany
Timm.Heuss@web.de
Abstract
This paper describes the experimental com-
bination of traditional Natural Language
Processing (NLP) technology with the Se-
mantic Web building stack in order to ex-
tend the expert knowledge required for a
Machine Translation (MT) task. Therefore,
we first give a short introduction in the state
of the art of MT and the Semantic Web
and discuss the problem of disambiguation
being one of the common challenges in
MT which can only be solved using world
knowledge during the disambiguation pro-
cess. In the following, we construct a sam-
ple sentence which demonstrates the need
for world knowledge and design a proto-
typical program as a successful solution for
the outlined translation problem. We con-
clude with a critical view on the developed
approach.
1 Introduction
Over the past decades, Machine Translation (MT)
has undergone various changes with regard to
the underlying technology. Starting in the mid-
dle of the last century with rule-based MT, a
first logical step was taken towards the end of
the century, when statistical methods in Natural
Language Processing (NLP) gained overall im-
portance, as the growing number of online avai-
lable texts could be used as a basis for statistical
computations performed on these texts and trans-
lations, which resulted in an enhancement of ex-
isting rules, statistics and thus results. The new
field of Statistical Machine Translation (SMT)
was born and MT systems became increasingly
better as more and more texts and translations
were available. In parallel to the developments in
MT, the Web has significantly grown and gained
importance, especially in the recently defined
field of the Semantic Web. After having accepted
statistical methods as a promising change in MT,
we believe that a next logical step will combine
MT with Semantic Web technology, resulting in
a new focus which can be called Semantic Web
Machine Translation (SWMT).
In this paper, we will develop our ideas step by
step and will demonstrate on a sample sentence
including a lexical ambiguity that our approach
does not involve a costly disambiguation pro-
cess on the basis of parsing online-dictionaries.
Instead, we believe that modern Information
Technology (IT) is aligned and committed to in-
formation and its markup, as the W3C Semantic
Web technology stack1 demonstrates, and that we
can use the contained knowledge in our disam-
biguation process without additional MT rules or
statistics being applied.
2 Development and change of focus in
MT : from the rule-based past to the
web-based future of MT
Traditionally, most MT systems were rule-based
systems built on electronic analysis and ge-
neration grammars as well as a language-pair-
dependent transfer component . These Rule-based
Machine Translation (RBMT) systems always in-
volved a careful and time-consuming develop-
ment of grammatical rules.
More recent development in MT has started to
use the vast amount of texts and knowledge that is
available online for translations based on statistics
1http://semanticweb.org/wiki/Main_Page
(URL last access 2011-12-18).
1
and probabilities, leading to a separate focus in
MT, namely SMT.
With the growing size of texts available in the
web, it is a logical next step to consider using
the available knowledge in these texts to enhance
NLP applications, including MT, leading to a yet
new focus, which we call SWMT.
In this chapter we will develop our idea by
starting with a look at how MT has developed over
the past decades, how it has made use of the ex-
panding Web in recent years and where we see
further potential in using existing knowledge for
MT technology.
2.1 Statistical Machine Translation
The dream of automatically translating docu-
ments from foreign languages into English, or be-
tween any two languages, is one of the oldest pur-
suits of NLP, being a subfield of artificial intel-
ligence research. Traditional MT systems com-
puted translations primarily on the basis of ana-
lysis and generation phrase-structure-rules, which
had to be manually coded in a costly fashion.
One of the leading users of SMT is Google
and Google Translate engineer Anton Andryeyev,
who explains SMT?s essence as follows:
?SMT generates translations based on patterns
found in large amounts of text. [...] Instead of
trying to teach the machine all the rules of a lang-
uage, SMT effectively lets computers discover the
rules for themselves. It works by analysing mil-
lions of documents that have already been trans-
lated by humans [...].
[...] Key to SMT are the translation dictio-
naries, patterns and rules that the program devel-
ops. It does this by creating many different pos-
sible rules based on the previously translated doc-
uments and then ranking them probabilistically.
Google admits this approach to translation in-
evitably depends on the amount of texts available
in particular languages [...].? (Boothroyd 2011)
Therefore, with the change of available re-
sources and the growing number of natural lang-
uage that is available in machine-readable for-
mat as well as the growing number of users in-
putting corrections to machine translations man-
ually, thus allowing a direct and correct match
between source and target texts, we have entered
this subfield of MT which focuses on a statistical
analysis of texts, in which documents are trans-
lated according to a probability distribution p(e|f)
which states that a string e in the target language
is the translation of a string f in the source lang-
uage.
Philipp Koehn, being among the most popu-
lar SMT researchers and developers, also high-
lights today?s quality of SMT and the relevance
of the vast amounts of texts in the web, which
provide the basis for SMT translations, by stating
?Now, armed with vast amounts of example trans-
lations and powerful computers, we can witness
significant progress toward achieving that dream.?
(Koehn et al 2012)
The research field of statistical machine trans-
lation is a rather new field. In his commented bib-
liography2 Koehn includes statistics about the dis-
tribution of publications in the SMT field across
the years 1953 until 2008. It is clearly shown that
only a few publications appeared before the mil-
lennium change and that SMT clearly became an
issue of growing interest in the new millennium,
with a peak in 2006. Scientists working in the
MT field suddenly became aware of the relevance
and potential provided by statistics in machine
translation and computational linguistics in gen-
eral. Still in 2003, Knight & Koehn stated, that
?the currently best performing statistical machine
translation systems are still crawling at the bot-
tom?, (Knight & Koehn 2004, p. 10), implying
that most of the approaches hadn?t gone beyond
simple word to word translations yet and hadn?t
included more advanced stages of NLP, like syn-
tax or even semantics. Among those who made
essential contributions to the field of SMT was
Kevin Knight who stated in 1999 that ?We want
to automatically analyse existing human sentence
translations, with an eye toward building general
translation rules we will use these rules to trans-
late new texts automatically.? (Knight 1999)
The previous statements all point at the vast
knowledge included in the just as vast amounts
of texts available in digital form in the internet,
partly in the form of human sentence translations.
At the same time that MT started clearly mov-
ing into using the Web to search for machine-
readable texts and translations that could be used
in the expanding SMT field, Tim Berners-Lee
(Berners-Lee & Hendler 2001) defined the know-
ledge, that is included in the Web content, to ex-
2http://www.statmt.org/book/bibliograp
hy/ (URL last access 2012-01-30).
2
pand the traditional WWW to become a Semantic
Web
As we are looking at an expanded view of how
to use the Web, and specifically the Semantic
Web, for our approach of MT, we would like to
draw parallels between what has been said so far
about MT and the innovative possibilities that the
Semantic Web provides for MT research.
2.2 W3C Semantic Web
The World Wide Web (WWW) was once designed
to be as simple, as decentralized and as interop-
erable as possible (Berners-Lee 1999, 36f.). The
Web evolved and became a huge success, how-
ever, information was limited to humans. In or-
der to make information available to machines,
an extending and complementary set of technolo-
gies was introduced in the new millennium by
the W3C, the Semantic Web3 (Berners-Lee &
Hendler 2001).
The base technology of the Semantic Web is
the data format Resource Description Framework
(RDF). Aligned to the so called AAA slogan
that ?Anyone can say Anything about Any topic?
(Allemang & Hendler 2008, p. 35), it defines a
structure that is meant to ?be a natural way to
describe the vast majority of the data processed
by machines? (Berners-Lee & Hendler 2001). In
addition to the AAA slogan, a basic construc-
tion paradigms of the Semantic Web is the Open
World Assumption - the fact that there is always
more knowledge than we currently know; new
knowledge can always be added later.
RDF expresses meaning by encoding it in sets
of triples (Berners-Lee & Hendler 2001), com-
posed of subject, predicate and object, which are,
in the N3-notation format4, likewise written down
as triples:
:subject :predicate :object
We see strong connections between MT and the
W3C Semantic Web.
A lot of ideas exist on how to augment
the Resource Description Framework (RDF)
- the base format of the Semantic Web -
with natural language. Since the beginning,
RDF itself provided capacities for a ?human-
readable version of a resource?s name? (Guha
3http://www.w3.org/standards/semantic
web/ (URL last access 2012-01-25).
4http://www.w3.org/DesignIssues/Notati
on3.html (URL last access 2012-01-29).
2004), rdfs:label, with an optional lang-
uage notation following RFC-30665 (Klyne &
Carroll 2004). In addition to that, the Sim-
ple Knowledge Organization System (SKOS)
ontology features a small selection of uni-
code labels for ?creating human-readable rep-
resentations of a knowledge organization sys-
tem?, skos:prefLabel, skos:altLabel
and skos:hiddenLabel - but also remarks
that it ?does not necessarily indicate best practice
for the provision of labels with different language
tags? (Miles 2008).
Some alternatives developed to the approches
above, to address limitations especially of
rdfs:label and to represent natural language
within semantic knowledge in a more sophisti-
cated way, like the SKOS eXtension for Labels
(SKOS-XL)6, Lemon7 and LexInfo8.
And even in the area of wordnets, which
might be considered as a more traditional NLP
domain, W3C Semantic Web technology plays
a role, as approaches were developed to bridge
the gap between natural language representations
within these wordnets and the design principles
of the Semantic Web (Graves & Gutierrez 2005).
The conversion of Princeton WordNet9, for
example, to RDF/OWL is covered by a W3C
Working Draft (van Assem et al 2006) or
the GermaNet wordnet10 equivalent approach
(Kunze & Lu?ngen 2007), adapting the ideas of
the Princeton WordNet conversation.
We decided to give a brief overview of the
state-of-the-art of SMT and the Semantic Web,
as both areas of research are not only very new
developments but they share using information in
the Web for their applications and they both offer
promising enhancements to traditional, rule-based
MT technology. Nevertheless, SMT and Seman-
tic Web Technologies have fundamental differ-
5http://www.ietf.org/rfc/rfc3066.txt
(URL last access 2012-01-25).
6http://www.w3.org/TR/skos-reference/
skos-xl.html (URL last access 2012-01-26).
7http://www.w3.org/International/mult
ilingualweb/madrid/slides/declerck.pdf
(URL last access 2012-01-31).
8http://lexinfo.net/ (URL last access 2012-01-
31).
9http://wordnet.princeton.edu/ (URL last
access 2012-01-26).
10http://www.sfs.uni-tuebingen.de/lsd/
(URL last access 2012-01-26).
3
ences in that SMT, with systems like Moses11, Ba-
bel Fish or Google compute their translations on
a pure probability count of n-grams of different
length in order to find the best translation by pick-
ing the one that gives the highest probability. As
these systems have access to a growing text cor-
pus, which is, as in the case of Moses, directly en-
hanced by collecting manual corrections given by
users after the system has computed an inadequate
translation, they become better with time. But
exactly these statistically based computations are
neither possible nor allowed in the Semantic Web
because of the Open World Assumption.
3 New idea: Enhancing NLP with
Semantic Web technology
With our new approach, we suggest to base MT
on a newly defined set of rules, which differ both
from rules known from earlier MT approaches
but also from any rules that are applied in SMT.
Our rules follow Tim Berners-Lees vision, in that
knowledge, once defined and formalized, is acces-
sible in arbitrary ways. As mentioned earlier, we
believe that modern IT follows the commitment
of information and it?s markup, and the Semantic
Web technology stack is a perfect implementation
of that paradigm.
To demonstrate our approach, we selected a
common and well known issue: The problem in
many areas of NLP is the ambiguity of natural
language on various levels, from word level to
sentence level. In many cases, strings can only
be disambiguated on the basis of world or expert
knowledge. How else would a machine decide on
whether the prepositional phase is modifying the
verb or the preceding noun in ?He eats fish with a
fork.? vs. ?He eats fish with bones.?? Especially
with translations, it is often crucial to understand
the source text correctly, as otherwise ambiguities
may result in incomprehensible target language
translations, as the examples below will demon-
strate.
The state of the art technology of the World
Wide Web to express information, facts and re-
lations for both humans and machines is RDF. So
it is not unlikely that nowadays expert knowledge
is encoded in that format, too.
11Moses is a statistical machine translation system devel-
oped by the Statistical MT Research group of University of
Edinburgh, http://www.statmt.org/moses/.
Taking care of lacking expert knowledge with
Semantic Web technology and thus extending ex-
isting MT technology seems to be a promising
research area. Instead of just combining RBMT
with SMT, we suggest to add the power of the Se-
mantic Web to these existing technologies, as the
previous approaches were not able to extract and
use knowledge from the Web in their translation
algorithms and thus leave ambiguities unsolved.
The previously quoted statements made it clear
that MT can only be enhanced on the basis of
a growing size of text. We claim that the next
logical step is to use this growing size of text
not only statistically, but in a well-defined way
which is offered by Semantic Web technology.
The power of our idea is the combination of a
strong, proven technology with a popular, open,
machine-readable data format.
In order to demonstrate how our approach will
enhance existing MT systems, we chose to use
a variety of MT systems, some rule-based (e.g.
PT12), other statistic-based (e.g. Babel Fish,
Google, and Moses) to compare their context-free
translation results against our approach. We use
those context-free translation results as a starting
point for further processing with Semantic Web
technology. Traditional MT technology should
therefore not be replaced, but enhanced with se-
mantics, to benefit from the advantages provided
by the Web.
In our sample scenario, the required world
knowledge for the sample sentence Pages by
Apple is better than Word by MS.
is modelled as RDF instances. We selected a
simple file-based storage, with the actual trans-
lations being stored as rdfs:labels13 which
are localized as defined in Best Common Practice
4714 (BCP47). To take advantage of the power-
ful Semantic Web tool set, parts of the world
knowledge are not directly defined, but can be
inferenced by Web Ontology Language (OWL)
capacities. The goal is to produce a semantically
good translation for the given sentence.
12Personal Translator 14 distributed by Linguatec.
13http://www.w3.org/TR/rdf-schema/##ch_
label (URL last access 2011-12-19).
14http://www.rfc-editor.org/rfc/bcp/bcp
47.txt (URL last access 2011-12-19).
4
3.1 A sample scenario
The first step is the construction of an expres-
sive sample scenario where world knowledge is
critical for the MT. We looked at the results a
number of different translation tools computed for
our sample sentence: Google Translator15, Bing
Translator16, an online demo of Philipp Koehn?s
Moses17, Linguatec Personal Translator PT 1418
(rule-based) and the reference translation in this
paper, Yahoo! Babel Fish19.
Research concluded with the following sen-
tence, requiring the ?expert knowledge? that a
vendor called Apple produced a product named
Pages and a vendor called MS (very popu-
lar shortform of Microsoft) a product named
Word:
Pages by Apple is better
than Word by MS.
One important measure to stress the transla-
tion service is to use ?indirect? product names
(Pages by Apple and not Apple Pages)
to prevent them from deriving product names
from possible dictionary entries. Another ?trap?
was to abbreviate Microsoft with MS to irritate
possible n-gram-statistics.
The resulting German translations of the sam-
ple sentence were the following:
Google Translator:
Pages von Apple ist besser
als Word MS.
Bing Translator:
Seiten von Apple ist besser
als MS Word.
Babel Fish:
Seiten durch Apple ist besser
als Wort durch Frau.
Moses Machine Translation Demo:
Seiten von Apple ist besser
als Word von MS behandelt.
15http://translate.google.de/ (URL last ac-
cess 2011-12-18).
16http://www.microsofttranslator.com/
(URL last access 2011-12-18).
17http://demo.statmt.org/index.php (URL
last access 2012-01-29).
18http://www.linguatec.net/products/tr/
pt (URL last access 2012-01-29).
19http://de.babelfish.yahoo.com/ (URL last
access 2011-12-18).
Personal Translator PT 14
Paginiert von Apple ist
besser als durch MS
auszudru?cken.
All translations failed, because they did not
take semantic relations into consideration. This is
a systematic issue in MT, demonstrating the ne-
cessity of including world knowledge in the com-
putation of the target translation.
4 More examples
As ambiguities are a common MT problem, there
are various examples where MT can be enhanced
by world knowledge.
Consider, for example, popular persons that
have ambiguous last names - like the politicians
George W. Bush, Helmut Kohl20, Joschka Fis-
cher21 to name a few. MT systems are likely
to translate those names if they are not included
in dedicated expert dictionaries. But thanks to
projects like DBpedia22, we already have the
knowledge available in a Semantic Web accessi-
ble format and could just use it.
Another area that might benefit from a Seman-
tic Web Machine Translation is the internation-
alization of technical documents or handbooks,
which usual deal with several termini technici.
Once modelled in RDF, the required expert know-
ledge is universally present and could aid the
translation process as well.
5 Analysis
World knowledge is the crucial point for the trans-
lation quality of the selected sample sentences.
It becomes obvious that in situations like this,
with missing expert dictionaries, rule sets or lack-
ing statistical tooling like N-grams, the translation
quality is relatively low. And this is not an unre-
alistic scenario: There will always be uncovered
areas in expert dictionaries or missing statistics in
a certain domain.
In the given example, if we are looking at the
Babel Fish translation, the translation engine was
totally mousetrapped as it translated the Apple
product Pages with the obviously context free,
20The proper name Kohl is also the German word for cab-
bage.
21Fischer means fisherman in German.
22http://dbpedia.org/ (URL last access 2012-03-
12).
5
German translation Seiten. Furthermore, it in-
terpreted MS as salutation and Word as the Ger-
man Wort - all mistakes made caused by lexical
ambiguities because of the lack of context know-
ledge.
6 Implementation
In order to prove our idea, we have developed
a prototypical application implementing a Se-
mantic Web enhanced SMT. One principal de-
sign goal was to keep the program simple, but
to apply state-of-the-art Semantic Web technol-
ogy like RDF and the query language SPARQL,
which are both W3C recommendations.
Figure 1: Architectural overview of the involved com-
ponents and exchanged tokens.
And because of the powerful but easy to use
Jena Semantic Web Framework23, a prototype im-
plementation is written in the Java programming
language. The involved MT-components are:
Trivial word dictionary Performs a one-by-one
word translation. Entries are designed to re-
flect the translation results of Babel Fish.
Semantic Core Reads a file based RDF triple
store, executes SPARQL-queries and per-
forms reasoning to inference new know-
ledge. Resulting text phrases may override
certain results derived by dictionary entries.
The following sections give more details about
the concrete implementation of those components
and the overall execution logic.
23http://jena.sourceforge.net/ (URL last
access 2011-12-19).
6.1 Trivial word dictionary
To fake Babel Fishs translation logic, a very
simplified dictionary is defined with the content
aligned at its online pendant. As figure 2 shows,
the context free translation is reproduced with
word-by-word translations.
English German
Apple Apfel
Pages Seiten
Word Wort
better besser
... ...
Figure 2: Simplified dictionary to reproduce Babel
Fishs simple and context free translation results.
6.2 Semantic Core
The much more interesting part is modelling the
world knowledge with Semantic Web technolo-
gies. Thereby, a simple file based RDF store is
used. The notation format is consistently N324,
because of its very good human-readability.
As mentioned in previous sections, world
knowledge about Apple and Microsoft is
crucial in this translation task. So the first state-
ments within the RDF store are about both ven-
dors and the products they produce25:
:apple a :vendor, :trigger;
rdfs:label "Apple";
:produces :numbers , :pages ,
:iphone .
In this case, the instance :apple is defined
to be of the types :vendor and :trigger.
While the former type has no special meaning
in this context, the latter is especially impor-
tant: :trigger-instances mark significant key-
words, indicating that additional world know-
ledge should be loaded when they occur in a
sentence. So in this example occurrence of the
word Apple (rdfs:label of :apple) in the
source text triggers loading and parsing of the
:apple instance and all uses of it within the
store.
Furthermore, some products are defined to be
produced by :apple.
24http://en.wikipedia.org/wiki/Notation
3 (URL last access 2011-12-19).
25For the sake of simplicity, all statements are aligned
in the default namespace http://www.example.org/
##.
6
The property :produces as well as its oppo-
site :producedBy are defined as follows:
:produces rdfs:label "produces"@en-
US, "produziert"@de-DE .
:producedby rdfs:label "by"@en-US, "
von"@de-DE .
Note that both properties have dual-language-
labels. This allows the program express
the world knowledge :apple :produces
:iphone in simple but natural English language
as well as in German.
In the next step, both properties are semanti-
cally connected as owl:inverseOf each other:
:produces owl:inverseOf :producedby
This few statements already allow inferenc-
ing - reasoning about information that is given
implicitly. So it is not only a fact that
:apple :produces :iphone, but also af-
ter OWL-inferencing the fact that :iphone
:producedBy :apple - without having to
state that directly.
Finally, the products get their proper names as-
signed:
:numbers rdfs:label "Numbers" .
:word rdfs:label "Word" .
:windows rdfs:label "Windows" .
:pages rdfs:label "Pages" .
This few lines form the knowledge base which
is, thanks to inferencing, sufficient to solve the
translation task. The following dictionary entries
can directly be read out of the RDF knowledge
base:
Microsoft produces Windows
MS produces Windows
Microsoft produces Word
MS produces Word
Apple produces Pages
Apple produces Numbers
By evaluating the predicates :produces and
inferencing the :producedBy statements, the
knowledge base in addition contains the inverted
entries:
Word by MS
Word by Microsoft
Word produced by MS
Word produced by Microsoft
Windows by MS
Windows by Microsoft
Windows produced by MS
Windows produced by Microsoft
6.3 Wiring it together
As mentioned before, the Semantic World Know-
ledge should enhance traditional MT translations.
Therefore, the program produces technically two
translations of the sentence Pages by Apple
is better than Word by MS.: The first
translation is done by the trivial dictionary, simply
by string-replacing English with German words
according to figure 2. The second translation
first tries to find a better translation by checking
trigger keywords, querying the RDF store for a
knowledge, inferencing relationships and resolv-
ing labels for the right language, before it contin-
ues with the same word-by-word-replacing mech-
anism like in the fist translation.
Figure 3: The two translations produced by the pro-
gram and their technological foundation.
6.4 Program execution
Our prototype simply executes both described
translations and print the result out.
Source sentence:
Pages by Apple is better than
Word by MS.
Semantic Web enhanced translation:
Pages von Apple ist besser als
Word von MS.
7
These simple lines, specially the ?Semantic
Web enhanced translation?, involve a lot of pro-
cessing in the background which is not visible
to the user - except for his waiting time. How-
ever, a semantically correct translation solution
was found.
7 Critical view on the solution
We feel we created something notable here. How-
ever, we stand at the very beginning of our re-
search and have encountered corresponding is-
sues.
Surprisingly, implementation of the program
logic - especially the query mechanism - turned
out to be quite complicated, even for a simple sce-
nario like in this case with a very limited corpus.
As a result, the stepwise refinement of a transla-
tion (trigger word, query of knowledge, inference
relationships and multi-language-label resolution)
consists of a lot SPARQL queries. These queries
require some processing time and power, which
is both already notable in this tiny example. This
finally leads to the conclusion that performance
might be a major withdraw of our approach, at
least for the current implementation.
Another issue was connected to data format:
the translation environment, especially the usage
of RDF triples consisting of subject, predicate and
object, might be regarded to be too much aligned
at the very special and constructed problematic of
only a number of realtime problems. Sentences
have to be somehow split into triples, which is
quite an artificial border - not to say a technical
limitation - of RDF. Real world NLP surely does
not fit into the tripartite simplifications of RDF,
and the question is then how often real world
problems would benefit from this solution.
Another issue is the Open World Assumption,
built into each Semantic Web component: There
is no golden standard truth in the Semantic Web
and therefore we will never be able to find the
?best? translation for a given sentence within
SPARQL-queries or inferencing results. Prob-
ably, our approach does not hold for providing
complete translation solutions, but for giving
very qualified suggestions. Some SMT tools, like
Moses, actually do work with suggestions.
However, some of this issues might be solved
by applying more sophisticated NLP / MT tech-
nology, like n-grams. Besides these issues, the
program works as expected and Semantic Web
technology was successfully used to integrate
world knowledge into a MT process. Thus, the
translation gathered a better quality and it thus can
be stated that the experiment was successful.
8 Related work
The project Monnet has, according to its mission
statement26, a similar idea to combine MT with
Semantic Web technology. However, results are
still pending or not publically accessible at this
point.
We also acknowledge the work by Elita and
Birladeanu (2005), who outlined the combination
of the Semantic Web with Example Based Ma-
chine Translation (EBMT), which is very much
related to our approach. However, there are ma-
jor differences: Elita and Birladeanu (2005) only
applied their technique on certain phrases of off-
ical documents - sequences of words they call
?fields? (Elita & Birladeanu 2005, p. 14). Our
idea is however to aid translation of complete sen-
tences. Another very important difference is the
intensiveness of use of W3C technology. Unlike
Elita and Birladeanu (2005), we heavily use RDF,
SPARQL and - probably the most promising mat-
ter of fact - OWL reasoning and try to follow the
Semantic Web standard tooling very strictly.
9 Outlook
At this point in our research, we have not yet com-
bined existing MT technology, especially SMT,
with SWMT. The combination of approaches has
yet to be explored, but existing MT technologies
and SWMT are certainly not mutually exclusive
and we suspect that a combination of MT ap-
proaches will lead to yet even better results, es-
pecially in cases where the translation quality is
based on world or expert knowledge.
10 Conclusion
In the recent past, MT researchers have already
discussed the combination of RBMT and SMT
(Hutchins 2009, pp. 13-20). We suggest to
add yet another possibility in MT to existing MT
approaches, namely a Semantic Web based MT
(SWMT).
26http://www.monnet-project.eu/Monnet
/Monnet/English?init=true (URL last access
2012-01-26).
8
In this paper we have taken a next logical
step in MT technology by including not only the
vast amounts of texts available in the Web to en-
hance MT quality applying statistical computa-
tions across online texts and translations, but go-
ing one step further by looking at the power of and
knowledge contained in the Semantic Web.
By taking advantage of the knowledge in the
Web of the future, our approach of combining
Semantic Web technology with MT allows this
world knowledge to be made available for ma-
chine translations, thus enhancing challenges in
MT, such as lexical ambiguities. In our discussed
sample sentences, we have shown that a solu-
tion for the disambiguation would traditionally in-
volve a costly disambiguation process or would
be left unsolved. Using our SWMT approach, the
MT quality benefits from world knowledge ex-
tracted from the Semantic Web and by its tech-
nology.
This combination of MT with Semantic Web
technology results in a new focus of MT which
we suggest to be called Semantic Web based MT
(SWMT).
11 Acknowledgements
We like to thank Rike Bacher from Linguatec and
the reviewers of the ESIRMT-HyTra conference
2012 for their valuable hints.
References
Allemang, D. & Hendler, J. (2008), Semantic Web
for the Working Ontologist: Effective Modeling in
RDFS and OWL, Morgan Kaufmann.
URL: http://www.amazon.com/Se
mantic-Web-Working-Ontologist-
Effective/dp/0123735564
Berners-Lee, T. (1999), Weaving the Web : The Origi-
nal Design and Ultimate Destiny of the World Wide
Web by its Inventor, HarperOne.
URL: http://www.amazon.com/We
aving-Web-Original-Ultimate-
Inventor/dp/0062515861
Berners-Lee, T. & Hendler, J. (2001), ?Scientific Amer-
ican: The Semantic Web?, Scientific American,
USA. .
URL: http://scholar.google.com
/scholar?hl=en\&btnG=Search\&
q=intitle:Scientific+American:
+The+Semantic+Web\#3
Boothroyd, D. (2011), ?Statistical machine translation
to enable universal communication??.
URL: http://www.newelectronics.co.
uk/electronics-technology/statist
ical-machine-translation-to-enab
le-universal-communication/33008/
Elita, N. & Birladeanu, A. (2005), ?A first step in inte-
grating an EBMT into the Semantic Web?.
URL: www.mt-archive.info/MTS-2005-
Elita.pdf
Graves, A. & Gutierrez, C. (2005), ?Data representa-
tions for WordNet : A case for RDF?.
URL: http://www.dcc.uchile.cl/?cgu
tierr/papers/wordnet-rdf.pdf
Guha, R. (2004), ?RDF Vocabulary Description Lang-
uage 1.0: RDF Schema?.
URL: http://moodletest.ncnu.ed
u.tw/file.php/9506/references-
2009/RDF\_schema\_1.pdf
Hutchins, J. (2009), ?Multiple Uses of Machine Transla-
tion and Computerised Translation Tools?, Machine
Translation pp. 13?20.
URL: http://www.hutchinsweb.me.uk/
Besancon-2009.pdf
Klyne, G. & Carroll, J. (2004), ?Resource description
framework (RDF): Concepts and abstract syntax?,
Changes 10(February), 1?20.
URL: http://www.mendeley.com/res
earch/w3c-gibt-recommendation-fr-
resource-description-framework-
rdf-frei/
Knight, K. (1999), A statistical MT tutorial workbook,
in ?Prepared for the 1999 JHU Summer Workshop?.
URL: http://www.snlp.de/prescher/t
eaching/2007/StatisticalNLP/bib/
1999jhu.knight.pdf
Knight, K. & Koehn, P. (2004), ?What?s New in Statis-
tical Machine Translation?, Tutorial, HLT/NAACL
pp. 1?89.
URL: http://www.auai.org/uai2003/
Knight-UAI-03.pdf
Koehn, P., Osborne, M., Haddow, B., Auli, M., Buck,
C., Dugast, L., Guillou, L., Hasler, E., Matthews,
D., Williams, P., Wilson, O. & Saint-Amand, H.
(2012), ?Statistical Machine Translation at the Uni-
versity of Edinburgh?.
Kunze, C. & Lu?ngen, H. (2007), ?Repra?sentation und
Verknu?pfung allgemeinsprachlicher und terminolo-
gischer Wortnetze in OWL?, Zeitschrift fu?r Sprach-
wissenschaft .
Mark van Assem, V. U. A., Aldo Gangemi, ISTC-CNR,
R. & Guus Schreiber, V. U. A. (2006), ?RDF/OWL
Representation of WordNet?.
URL: http://www.w3.org/TR/wordnet-
rdf/
Miles, A. (2008), ?SKOS simple knowledge organiza-
tion system reference?, W3C Recommendation .
URL: http://www.mendeley.com/r
esearch/skos-simple-knowledge-
organization-system-reference/
9
