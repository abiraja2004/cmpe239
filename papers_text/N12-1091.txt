2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 731?741,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Exploring Semi-Supervised Coreference Resolution of Medical Concepts
using Semantic and Temporal Features
Preethi Raghavan?, Eric Fosler-Lussier?, and Albert M. Lai?
?Department of Computer Science and Engineering
?Department of Biomedical Informatics
The Ohio State University, Columbus, Ohio, USA
{raghavap, fosler}@cse.ohio-state.edu, albert.lai@osumc.edu
Abstract
We investigate the task of medical concept
coreference resolution in clinical text using
two semi-supervised methods, co-training and
multi-view learning with posterior regulariza-
tion. By extracting semantic and temporal
features of medical concepts found in clinical
text, we create conditionally independent data
views; co-training MaxEnt classifiers on this
data works almost as well as supervised learn-
ing for the task of pairwise coreference resolu-
tion of medical concepts. We also train Max-
Ent models with expectation constraints, using
posterior regularization, and find that poste-
rior regularization performs comparably to or
slightly better than co-training. We describe
the process of semantic and temporal feature
extraction and demonstrate our methods on a
corpus of case reports from the New England
Journal of Medicine and a corpus of patient
narratives obtained from The Ohio State Uni-
versity Wexner Medical Center.
1 Introduction
The clinical community creates and uses a variety
of semi-structured and unstructured electronic tex-
tual documents that include medical reports such
as admission notes, progress notes, pathology re-
ports, radiology reports and hospital discharge sum-
maries. The documents, collectively termed clini-
cal narratives, account for various medical condi-
tions, procedures, diagnoses and assessments in a
patient?s medical history. Researchers have inves-
tigated ways in which clinical text can be automati-
cally processed for enabling access to relevant infor-
mation for physicians and health researchers (Embi
and Payne, 2009). One application is to support pa-
tient recruitment into clinical trials (research studies
that try to answer scientific questions to find bet-
ter ways to prevent, diagnose, or treat a disease)
by matching patient characteristics against eligibil-
ity criteria (Raghavan and Lai, 2010). While there
has been significant efforts to move to structured
data collection, clinical narratives remain a critical
data source for these tasks.
Extracting structured information from unstruc-
tured clinical text using natural language processing
(NLP) is complicated by the distinct clinical report-
ing sub-language characterized by incomplete sen-
tences and domain specific abbreviations (Friedman
et al, 2002). The large number of clinical narra-
tives generated per patient, over the years, along
with redundant information within and across narra-
tives, further adds to the complexity of using infor-
mation structured using NLP. There is a tendency to
copy and edit parts of an old clinical narrative when-
ever a new one is created, thus leading to redundant
information in clinical narratives of a patient. Fur-
thermore, since different types of clinical narratives
are created for different purposes, certain narratives
may summarize information from various other, at
times older, clinical narratives. All of this makes the
task of automatically processing unstructured clin-
ical narratives significantly difficult. However, the
ability to resolve medical concept coreferences helps
deal with redundant information within and across
clinical narratives and thus produce a unique list of
medical concepts in the patient?s clinical history.
We investigate the task of resolving references to
731
the same medical concept in the clinical narratives
of a patient using supervised and semi-supervised
methods. Our main contributions are as follows:
1. Since manual coreference annotation of patient
narratives is a slow and expensive process and pub-
licly available datasets are difficult to acquire, we
study the application of semi-supervised methods,
co-training and using expectation constraints with
posterior regularization, to medical concept coref-
erence resolution (MCCR).
2. We work with the hypothesis that if two medical
concepts have the same meaning and have occurred
at the same time, there is a very high probability that
they corefer. Based on this hypothesis, we explain
extraction of semantic and temporal feature sets that
are effectively used for MCCR.
3. We propose a method to associate medical con-
cepts with time durations centered around admission
and discharge dates of the patient using CRFs.
4. With the help of corpora created from the New
England Journal of Medicine (NEJM) and actual pa-
tient narratives obtained from the medical center, we
demonstrate that the semi-supervised methods per-
form comparably with supervised learning for pair-
wise MCCR using a MaxEnt classifier.
2 Related Work
Free-text reports form a significant portion of the
information content in a patient?s medical record.
There is great need for tools that can structure the
information in clinical text for use in various stud-
ies studies such as clinical trials, quality assess-
ment of healthcare delivery in institutions, and pub-
lic health research. Researchers have been investi-
gating ways in which clinical free-text can be struc-
tured to transform the information content in a clin-
ical narrative into a representation suitable for com-
putational analysis (Ananiadou et al, 2004). Medi-
cal NLP systems like Mayo?s cTakes (Savova et al,
2010), IBM?s MedKAT,1 and MedLEE (Chiang et
al., 2010), have components specifically trained or
designed for the clinical domain, to support tasks
such as named entity recognition. Previous at-
tempts at learning temporal relations between med-
ical events in clinical text include work by Jung et
1https://cabig-kc.nci.nih.gov/Vocab/KC/
index.php/OHNLP
al. (2011) and Zhou et al (2006). Gaizauskas et
al. (2006) learn the temporal relations before, after,
is included between events from a corpus of clinical
text much like the event-event relation tlink learn-
ing in Timebank (Pustejovsky et al, 2003). A com-
prehensive survey of temporal reasoning in medi-
cal data is provided by Zhou and Hripcsak (2007).
Chapman et al (2011) discuss barriers to NLP de-
velopment in the clinical domain.
Coreference resolution is a well-studied prob-
lem in computational linguistics (Ng, 2010; Raghu-
nathan et al, 2010). Supervised machine learn-
ing algorithms have been previously used for noun
phrase coreference resolution with fairly good re-
sults (Soon et al, 2001; Raghunathan et al, 2010).
Recently, the i2b2 challenge2 on coreference reso-
lution examined coreference resolution in clinical
data. The problem addressed in our paper is simi-
lar to the task described in the i2b2 challenge.3 Be-
sides the i2b2 challenge, there has not been signifi-
cant work in MCCR. This may be due to various pri-
vacy concerns and the efforts required to anonymize
and annotate massive amounts of patient narratives.
Zheng et al (2011) review heuristic-based, super-
vised and unsupervised methods for coreference res-
olution in the context of the clinical domain. He
(2007) studied coreference resolution in discharge
summaries, treating coreference resolution as a bi-
nary classification problem and investigated critical
features for coreference resolution for entities that
fall into five medical semantic categories commonly
appearing in discharge summaries. However, we fo-
cus on feature extraction to determine the similarity
between medical concepts, both in terms of meaning
and time of occurrence, for resolving coreferences
within and across all types of clinical narratives.
A disadvantage of supervised machine learning
approaches is the need for an unknown amount of
annotated training data for optimal performance.
Researchers then began to experiment with weakly
supervised machine learning algorithms such as co-
training (Blum and Mitchell, 1998). Muller et al
(2002) investigate the practical applicability of co-
training for the task of building a classifier for coref-
erence resolution and observed that the results were
2https://www.i2b2.org/NLP/Coreference/
3https://www.i2b2.org/NLP/Coreference/assets/
CoreferenceGuidelines.pdf
732
mostly negative for their dataset.
Ganchev et al (2010) propose a posterior regular-
ization framework for weakly supervised learning to
derive a multi-view learning algorithm. Multi-view
methods typically begin by assuming that each view
alone can yield a good predictor. Under this as-
sumption, we can regularize the models from each
view by constraining the amount by which we per-
mit them to disagree on unlabeled instances. In the
proposed approach, they train a model for each view,
and use constraints that the models should agree on
the label distribution.
We investigate the applicability of these two weakly
supervised methods to the task of MCCR using se-
mantic and temporal views. Savova et al (2011) dis-
cuss the creation of a corpus for coreference resolu-
tion in the clinical narrative. We annotate a corpus of
clinical narratives to tag medical concepts, temporal
relations, and coreference information. We use this
corpus as a gold standard to evaluate the proposed
approach to resolving coreferences between medical
concepts in clinical text.
To summarize, we study the problem of intra and
cross-narrative coreference resolution on longitudi-
nal patient data using relatedness between medical
concepts in terms of semantics and time. Further,
we importantly demonstrate that this task gives us
reasonable results even when modeled as a semi-
supervised problem. Creating annotated clinical cor-
pora is tedious, time consuming, and costly, as it
requires experts with medical domain knowledge.
Thus, the ability to train semi-supervised models
with limited labeled data for MCCR would be of
tremendous value.
3 Problem Description
Coreference resolution in clinical text refers to the
problem of identifying all medical concepts that re-
fer to the same medical concept. Medical con-
cepts are medical entities, events or states associ-
ated with the patient?s medical condition and health-
care. These include medical conditions, drugs ad-
ministered, diseases, procedures and lab tests as well
as normal health situations like pregnancy affecting
the patient?s health. The task of MCCR is similar to
noun phrase coreference resolution. However, med-
ical concepts are not restricted to noun phrases. For
instance, the actions cauterize and cauterization are
both considered medical concepts.
To make the task of identifying medical concepts
from clinical text more deterministic, any contigu-
ous group of words that have a direct or close match
in the Unified Medical Language System (UMLS)
Metathesaurus4 is considered a medical concept.
The UMLS includes a large Metathesaurus of con-
cepts and terms from many biomedical vocabular-
ies and a lexicon which contains syntactic, morpho-
logical, and orthographic information for biomedi-
cal and common words in the English language.
Problem Formulation. Consider a corpus of clini-
cal narratives, where multiple clinical narratives are
associated with each patient. If Pi, i ? {1, 2, ..., n}
where n is the number of patients in corpus, then
for each Pi, we have a set of associated clinical nar-
ratives. Each clinical narrative in turn has a set of
medical concepts. Thus, each Pi has a set of associ-
ated medical concepts, M = {M1,M2,M3, ..} that
occur within each clinical narrative as well as across
clinical narratives for that Pi. We study the problem
of MCCR of all medical concepts in M for each Pi.
4 Semantic and Temporal Features
We extract features based on semantic and tempo-
ral relatedness for each pair of medical concepts.
Semantic relatedness measures closeness between
medical concepts in terms of their meaning. This is
quantified by measuring distance between medical
events in the UMLS Metathesaurus graph structure
(Xiang et al, 2011). Temporal relatedness measures
the closeness between medical concepts in terms of
when they occurred. This is achieved by first, learn-
ing to assign every medical concept to a time-bin,
and then using the time-bin as a feature for learn-
ing to resolve coreferences. Extracting semantic and
temporal features helps identify conditionally inde-
pendent views of the data for co-training classifiers.
As previously noted by Nigam and Ghani (2000), it
is hard to identify conditionally independent views
for real-data problems. However, we believe there
are no natural dependencies between the semantic
and temporal feature sets. While semantic features
help identify synonymous medical concepts, that
alone may not guarantee coreference. Medical con-
4https://uts.nlm.nih.gov/home.html
733
Clinical 
Text 
Semantic Feature 
Extraction 
Temporal Feature 
Extraction using 
CRFs 
Co-train 
Posterior 
Regularization 
Coreference 
decisions 
Section 4 Section 5 
Medical Concept 
Coreference Resolution 
(MCCR) 
OR 
Figure 1: MCCR pipeline: Extract semantic and tempo-
ral features from clinical text to train MaxEnt classifiers
for medical concept coreference resolution using 1) Co-
training or 2) Posterior Regularization
cepts that are similar in meaning, but dissimilar in
terms of their time of occurrence, most probably do
not corefer. Similarly, medical concepts that occur
during the same time duration but are dissimilar in
terms of meaning, most probably do not corefer.
Semantic Relatedness. We leverage the UMLS
to derive a semantic relatedness score between med-
ical concepts. The UMLS codifies concepts found
in various medical vocabularies (e.g., ICD5 and
SNOMED-CT6) and includes relationships between
various concepts. The medical concepts and their
relationships are modeled in a graph structure. We
use the k-Neighborhood decentralization method
(kDLS) (Xiang et al, 2011) to index and transi-
tively traverse associated relations between concept
unique identifiers (CUIs) in the UMLS graph. The
UMLS uses semantic relations to mark the avail-
able links between two concepts. Around 2,404,937
CUIs and 15,333,246 links between them are seen in
the full UMLS graph structure. The kDLS method
is shown to outperform both breadth-first and depth-
first search in terms of speed and various other
measures in finding important information, such as
reachability, distance, and a summary of paths, be-
tween two concepts in the UMLS graph structure.
The relation between two concepts Mj (denoted by
x) and Mk (denoted by y) is measured as follows.
R(x, y) =
?
p?D(x,y)
1
?length(p)?1
+
?
q?D(y,x)
1
?length(q)?1
where D(x, y) is the set of paths from x to y and
D(y, x) is the set of paths from y to x obtained us-
5http://www.cdc.gov/nchs/icd.htm
6http://www.ihtsdo.org/snomed-ct/
ing the kDLS method, excluding paths with length
equal to 1. In order to make the measurement be-
tween a medical concepts unbiased against the avail-
able links in the UMLS that directly connect them,
the paths with length being 1 between them are not
counted. Each path?s contribution to the relation
score R(x, y) is determined by its length and ?. ? is
varied between 1 to 50; if ? is set to 1, then all paths
contribute equally to R irrespective of their lengths.
When ? increases, more weight will be placed on
the short paths as opposed to the long paths. Xiang
et al (2011) observe several fold enrichment values
when ? is varied between 5 and 15.
Besides traversing the UMLS graph structure us-
ing the kDLS method to obtain a similarity score
between medical concepts, we also measure similar-
ity between medical concepts by taking into account
the surrounding context. We do so by measuring
the KL-divergence between the sentences to which
the medical concepts belong. In order to avoid the
possibility of an empty set when calculating the in-
tersection of the probability distributions, we use a
smoothing method that makes the probability distri-
butions sum to 1 (Brigitte, 2003).
Another important semantic feature is the type of
relation between the medical concepts. This feature
is calculated by first computing the stemmed word
overlap between the medical concepts and deriving
features based on exact and partial matches between
the word stems of the medical concepts. If there is
no exact or partial match between the concepts, we
query the UMLS to check if the stem of one of the
medical concepts occurs in the UMLS definition or
atoms of the other medical event. An atom is the
smallest unit of naming within the UMLS. A med-
ical concept in UMLS represents a single meaning
and contains all atoms in the UMLS that express that
meaning in any way, whether formal or casual, ver-
bose or abbreviated. All of the atoms within a con-
cept are synonymous.
Besides the described features, we also include
the UMLS semantic category of each medical con-
cept and the WordNet7 similarity score between sen-
tences containing the medical concept.
Temporal Relatedness. Clinical text is fre-
quently characterized by temporal expressions co-
7http://wordnet.princeton.edu/
734
occurring with medical concepts (Zhou and Hripc-
sak, 2007). For instance, two days ago, fever started
4 days before rash, July 10th, 2010 etc. The abil-
ity to associate medical concepts with temporal ex-
pressions helps order medical concepts and deter-
mine potential temporal overlap between them. This
in turn could be a powerful discriminatory feature
in MCCR. Consider the medical concept chest pain
that occurs multiple times in a clinical narrative. If
these mentions of chest pain have occurred at the
same time, there is a possibility that they all refer to
the same instance of the medical concept chest pain.
Instead of relying on implicit temporal references
that may or may be evident from the clinical nar-
rative, we focus on temporal expressions that are
found in most clinical narratives. We do so by lever-
aging structural properties of clinical narratives such
as section information and explicit temporal infor-
mation such as admission and discharge dates, to
learn to assign medical concepts to time periods we
refer to as time-bins.
We now proceed to explain the process of assign-
ing medical concepts to time-bins using CRFs. Clin-
ical narratives are usually formatted with a struc-
tured header with information that includes the pa-
tient admission and discharge date. Clinical narra-
tives are also typically divided into sections. Sec-
tions represent a logical, and at times, temporal
grouping of information in the narrative. Sections
such as ?history of present illness,? ?physical ex-
amination,? ?review of systems,? ?impression,? and
?assessment plan? tend to occur in a certain order
within each clinical narrative. Thus, section tran-
sitions may indicate a temporal pattern for medical
concepts across those sections. For example, ?past
medical history? (before admission), followed by
?findings on admission? (on admission), followed
by ?physical examination? (after admission). Sec-
tions of certain types may also exhibit certain tem-
poral patterns. A ?history of present illness? sec-
tion may start with diseases and diagnoses 30 years
ago and then proceed to talk about them in the con-
text of a medical condition that happened few years
ago and finally describe the patient?s condition on
admission. Given the temporal patterns within sec-
tions and at section transitions, it works well to treat
the list of medical concepts from each clinical nar-
rative as a sequence (considering them in narrative
order) and learning to label them with a correspond-
ing time-bin. We define the following sequence of
time-bins centered around admission and discharge,
{way before admission, before admission, on admis-
sion, after admission, after discharge}.
We model the problem of assigning medical con-
cepts to time-bins as a sequence labeling task using
a CRF where we predict labels from the set {way be-
fore admission, before admission, on admission, af-
ter admission, after discharge} as a sequence Y pre-
dicted from the detected medical concepts X . CRFs
use two types of features in classification, state fea-
tures and transition features. State features con-
sider relating the label y (time-bin) of a single ver-
tex (medical concept) to features corresponding to a
medical concept x, and are given by,
S(x, y, i) =
?
j ?jsj(y, x, i)
Transition features consider the mutual depen-
dence of labels yi?1 and yi (dependence between the
time-bins of the current and previous medical event
in the sequence) and are given by,
T (x, y, i) =
?
k ?ktk(yi?1, yi, x, i)
Above, sj is a state feature function, and ?j is its
associated weight and tk is a transition function, and
?k is its associated weight. In contrast to the state
function, the transition function takes as input the
current label as well as the previous label, in addition
to the data.
Example state features include indicator features
based on verbs patterns in the same sentence as that
of the medical concept, last verb before the medical
concept, and type of clinical narrative. We also in-
clude position of medical event in the narrative as
well as within each section, the temporal expres-
sions and dates co-occurring with the medical con-
cept as features and the difference between these
dates and the admission date on each clinical nar-
rative. Example transition features include section
transitions based on the sections under which the
medical concept occurs, UMLS relatedness score
between the previous and current medical concept,
difference in verb patterns between the previous and
current medical concept, difference in dates (if any)
between the dates co-occurring with the previous
and current medical concept.
In order to enable feature extraction for this learn-
ing task, we use the following heuristic-based al-
735
gorithm to automatically identify sections and asso-
ciate medical concepts with them.
1. Extract lines that are all upper-case, and longer
than a word, from all narratives in corpus. They
mostly correspond to section titles.
2. Derive the stem of each word in the title using a
Porter stemming algorithm8 and sort stemmed
titles by frequency. If two or more words in
the title overlap, they are considered the same.
This gives us a candidate set of section titles.
3. When parsing a clinical narrative, and encoun-
tering a stemmed ngram matching a section ti-
tle from the frequent list, all subsequent sen-
tences are associated with that section until a
new section title is encountered. If an exact
match is not found, we allow partially match-
ing ngrams to be considered as section titles.
Along with the time-bin that are learned using the
process described above, dates and temporal expres-
sions extracted from the annotations in our corpus
are also used as temporal features. The list of fea-
tures extracted for the task of MCCR include the
following:
1. Verb pattern in the sentence in which the med-
ical concept occurs.
2. Last verb before the medical concept in the
same sentence.
3. Type of clinical narrative.
4. Section under which the medical concept is
mentioned.
5. Position of the medical concept.
6. Dates that fall in the same sentence as the med-
ical concept.
7. Difference between admission date and the date
in the same sentence as the clinical narrative.
8. The learned time-bin of each medical concept.
We also derive features based on the overlap-
ping in time-bins for the medical concept pair
and the nature of time-bin (past, present, fu-
ture).
9. Difference in verb patterns in the sentences of
the medical concept pair.
10. Difference in dates between the medical con-
cept pair.
8http://tartarus.org/martin/PorterStemmer/
11. UMLS relatedness score between the medical
concept pair and all the UMLS related and
other features described previously in the se-
mantic relatedness section.
When applying CRFs to the problem of assigning
medical concepts to time-bins, an observation se-
quence is medical concepts in the order in which
they appear in a clinical narrative, and the state se-
quence is the corresponding label sequence of time
bins. Thus, given a sequence of concepts in narrative
order {M1,M2,M3, ..}, we learn a corresponding
label sequence of time-bins {way before admission,
before admission, on admission, after admission, af-
ter discharge}. The learned label sequence is now
used as part of the temporal feature set in co-training
and posterior regularization for MCCR.
5 Weakly Supervised Learning
5.1 Co-training
We co-train two MaxEnt classifiers, one each on the
semantic features fs and temporal features ft of the
data, to classify pairs of medical concepts as core-
fer or no-corefer in a semi-supervised fashion. We
use the co-training algorithm proposed by Blum and
Mitchell (1998).
The assumption here is that each feature set contains
sufficient information to train a model for classifica-
tion of medical concepts. Consider the concept pair,
{renal inflammation, posterior uveitis} that core-
fer. The semantic view for this concept pair may
not strongly indicate coreference. The ?UMLS rela-
tion type? feature indicates that the two concepts are
not similar in meaning. However, both concepts are
mapped to the same time-bin after admission. Thus,
the time-bin along with features extracted based on
explicit temporal expressions co-occurring with the
medical concepts indicate a coreference between the
pair of medical concepts. Similarly, the semantic
view is confident about confident about the corefer-
ence of certain medical concept pairs which do not
occur in the same time-bin. The classifiers trained
on each view complement each other in the learn-
ing process. Thus, we can leverage the predictions
made by each classifier on the unlabeled dataset to
augment the training data of both classifiers.
The co-training algorithm is shown in Table 1. We
set a threshold for an unlabeled sample to be added
736
Function coTrain
Repeat till all unlabeled data is labeled.
1. Train classifier c1 on tf s to obtain model m1
2. Train classifier c2 on tf t to obtain model m2
3. Use m1 to classify a subset of unlabeled data
and update the training data as,
tf s.subset = {usubset1, predicted label}
iff classifier confidence > 1/number of labels
4. Use m2 to classify a subset of unlabeled data
and update the training data as,
tf t.subset = {usubset2, predicted label}
iff classifier confidence > 1/number of labels
5. tf s = tf s + tf t.subset +
{usubset1, predicted label}
6. tf t = tf t + tf s.subset +
{usubset2, predicted label}
Table 1: Co-training algorithm for the binary pairwise
classification task of MCCR (Blum and Mitchell, 1998).
c = classifier, u = unlabeled data.
usubset1, usubset2 = subsets of unlabeled data.
usubset1 and usubset2 are mutually exclusive.
F = {fs, ft} is the features space divided into condition-
ally independent semantic and temporal feature sets.
tf s = {fs,l} training data consisting of semantic features
of a medical concept pair along with class label.
tf t = {ft,l} training data consisting of temporal features
of a medical concept pair along with class label.
into the labeled pool. An unlabeled sample is la-
beled in a particular iteration, if classifier confidence
> 1/number of labels. In the next iteration, ran-
domly pick a subset of unlabeled samples and label
all samples in this subset. This could include sam-
ples that have already been labeled in previous iter-
ations. A label is assigned in a subsequent iteration
if: the sample was previously labeled OR if classi-
fier confidence > threshold. The parameters in this
algorithm are the number of iterations, the pool size
of examples selected from the unlabeled set in each
iteration and the number of labeled examples added
at each iteration to the labeled data pool. Similar to
Blum and Mitchell (1998), we update the pool size
by 2p+ 2n in each iteration, where p is the number
of medical pairs that corefer and n is the number of
medical concept pairs that do not corefer.
5.2 MaxEnt with Posterior Regularization
The next semi-supervised learning method applied
to MCCR is MaxEnt with posterior regularization
using expectation constraints (Ganchev et al, 2010).
This method incorporates prior knowledge directly
on the output variables during learning. The prior
knowledge is expressed as inequalities on the ex-
pected value under the posterior distribution of user-
defined constraint features. Thus, posterior regular-
ization incorporates side-information into unsuper-
vised estimation in the form of constraints on the
model?s posteriors. It is similar to the EM algorithm
during learning, but it solves a problem similar to
Maximum Entropy inside the E-Step to enforce the
constraints.
Posterior regularization is used to derive a multi-
view learning algorithm while specifying constraints
that the models should agree on the label distri-
bution. We train MaxEnt models based on two
views of the data, semantic and temporal. This
method starts by considering the setting of complete
agreement where there is a common desired out-
put for the two models and each of the two views
is sufficiently rich to predict labels accurately. The
search is restricted to model pairs p1, p2 that sat-
isfy p1(y|x) ? p2(y|x), where p1 and p2 each de-
fine a distribution over labels. The product dis-
tribution p1(y1)p2(y2) is considered and constraint
features are defined such that the proposal distri-
bution q(y1, y2) will have the same marginal for
y1 and y2. There is one constraint feature defined
for each label y given by, ?y(y1, y2) = ?(y1 =
y)?(y2 = y), where ?(.) is the 0-1 indicator func-
tion. The constraint set Q = q : Eq[?] = 0 re-
quires that the marginals over the two output vari-
ables are identical q(y1) = q(y2). An agreement
between two models is defined as agree(p1, p2) =
argmin KL(q(y1, y2)||p1(y1)p2(y2)) | Eq [?] = 0.
In the semantic feature set, we convert the follow-
ing feature (described in Section 4) into expectation
constraints. The type of relation between the pair
of medical concepts, is derived from matching the
word stems and querying the UMLS definition and
atoms of the medical concepts. Based on the relation
between the medical concepts (i.e., partial match,
complete match, UMLS definition match, UMLS
atom match, and no match), we indicate the prob-
ability of label distribution coref and no-coref. If
the relation turns out to be no match, there is a high
probability that the medical concepts do not corefer.
In the temporal feature set, we convert the features
based on time-bins of the medical concepts in the
pair into expectation constraints.
737
Class(time-bin) Precision Recall
after discharge 96.05 62.53
before admission 94.02 92.44
on admission 33.25 75.16
way before admission 50.42 66.72
after admission 93.62 99.14
Table 2: Sequence tagging of medical concepts with
time-bins using CRFs.
6 Experimental Setup
6.1 Corpus Annotation
Annotation of clinical text is a time consuming and
costly process. Many annotation efforts have used
physicians to annotate the data. Instead, we use an-
notators that are students or recently graduated stu-
dents from diverse clinical backgrounds with vary-
ing levels of clinical experience. In spite of this di-
versity, the annotation agreement across our team of
annotators is high; all annotators agreed on 89.5% of
the events and our overall inter-annotator Cohen?s
kappa statistic (Conger, 1980) for medical events
was 0.865. The annotators mark medical concepts,
coereference chains and temporal expressions in the
clinical narratives and the NEJM case reports. They
also map each medical concept to a UMLS CUI.
6.2 Feature Extraction
The first step involves extraction of semantic and
temporal features for the annotated medical con-
cepts, as described in Section 4 from both corpora.
The semantic relatedness scores are computed us-
ing the kDLS (Xiang et al, 2011) method to calcu-
late the relationship between concepts in the UMLS
with value of ? set to 7. The type of relation be-
tween medical concepts is derived by matching word
stems in each medical concept using the Lucene9
implementation of the Porter stemming algorithm.
We query the latest release (UMLS 2011AB) of the
UMLS Metathesaurus for finding a match between
medical concept and the UMLS definition or UMLS
atoms. The WordNet similarity score is computed
using Java API for WordNet Searching (JAWS).10
Explicit temporal expressions annotated in the
corpora are included in our temporal feature set.
Medical concepts in the NEJM are mostly de-
scribed temporally relative to the patient?s admis-
9http://lucene.apache.org/
10http://lyle.smu.edu/?tspell/jaws/
Class NEJM Clinical Narratives
Precision Recall Precision Recall
coref 79.24 94.53 74.81 88.33
no-coref 86.71 90.62 83.92 94.86
Table 3: Supervised learning for MCCR.
sion. Temporal expressions like ?2 years before ad-
mission? and ?3 weeks before admission? are com-
mon. Hence, we use a heuristic-based algorithm
to associate medical concepts with explicit tempo-
ral expressions in the NEJM corpus. The algo-
rithm parses case reports and identifies the tempo-
ral expressions anchored to admission. All medi-
cal concepts following such a temporal expression
are anchored to it until a new temporal expression
is encountered. Over 88% of the medical concept-
temporal expression associations done with the al-
gorithm above is accurate when compared against
the NEJM gold standard.
As described in Section 4, we apply sequence tag-
ging using a CRF to assign medical concepts in clin-
ical narratives to time-bins. We use the implementa-
tion of CRF in Mallet,11 trained by Limited-Memory
BFGS for our experiments. We use the Stanford
POS tagger12 to identify verbs and derive verb pat-
terns. The dataset for the task of assigning medi-
cal concepts to time-bins consisted of 1613 medical
concepts. We used a 60-40 train-test split to train a
CRF using a sequence of medical concepts and ob-
served an overall accuracy of 92%. The precision
and recall values for each time-bin class is indicated
in Table 2. The percentage of medical concepts that
fall under ?way before admission? and ?on admis-
sion? are less than 5%, affecting the learning accu-
racy of those classes. When modeled as a multi-
class classification task using MaxEnt, we achieve
around 86% accuracy.
7 MCCR Results and Discussion
We perform the following experiments for pairwise
MCCR: 1) Supervised learning with a MaxEnt clas-
sifier, using the combined semantic and temporal
feature set, 2) Co-training two MaxEnt models, 3)
Training MaxEnt models with using posterior regu-
larization.
11http://mallet.cs.umass.edu/
12http://nlp.stanford.edu/software/tagger.
shtml
738
Class NEJM Clinical Narratives
Co-train Precision Recall Precision Recall
coref 70.32 82.54 69.26 87.31
no-coref 82.54 84.85 71.15 89.44
PR Precision Recall Precision Recall
coref 76.63 90.41 74.81 84.25
no-coref 80.35 89.21 78.93 87.46
Table 4: Co-training and posterior regularization (PR) for
MCCR using semantic and temporal feature sets.
We use the MaxEnt classifier available in Mallet
for 1) and 2) and the the Mallet implementation of
MaxEnt models with posterior regularization for 3).
The NEJM corpus has 722 medical concepts,
12576 candidate pairs of medical concepts includ-
ing 137 pairs that corefer. We include all 12576
pairs in our experiments. The clinical narrative cor-
pus has 1613 medical concepts. The candidate pairs
and coreference chains for each patient is as follows.
Patient 1 has 241001 candidate pairs, 29 corefer-
ence chains. Patient 2 has 149604 candidate pairs,
9 coreference chains. Patient 3 has 6,446,521 can-
didate pairs, 20 coreference chains. From all the
candidate pairs in the clinical narrative corpus, 1025
pairs corefer. We randomly sample the no-coref in-
stances to restrict the corpus size to 1 million candi-
date pairs of medical concepts.
The results for all 3 experiments for both corpora
is shown in Tables 3, 4. We also train-test a super-
vised MaxEnt classifier on a 60-40 split of the en-
tire corpus. This gives us a precision of 74.81% and
88.33% recall (coref) for the binary classification
task of pairwise MCCR in the clinical narratives cor-
pus. In the both the semi-supervised experiments,
we use an initial labeled pool size of 30 where 12
medical concept pairs that corefer (p) and 18 that do
not corefer (n). The growth size is each iteration of
co-training is 2p+2n. At each iteration, confidently
labeled examples are added to the training set from
the previous iteration. The co-training algorithm
is run until all unlabeled instances become labeled.
The parameters in the posterior regularization im-
plementation include the regularization penalty for
each step and the number of iterations. We use the
default values (maxIterations=100, pGaussianPrior-
Variance=0.1, qGaussianPriorVariance=1000) sug-
gested on the Mallet toolkit page (Bellare et al,
2009). Co-training two MaxEnt models based on
independent semantic and temporal views of the
data results in 69.26% precision and 87.31% recall
(coref), whereas training MaxEnt models with ex-
pectation constraints gives us 74.81% precision and
84.25% recall (coref), on the corpus of clinical nar-
ratives.
Posterior regularization does better than co-training
and the performance of both the semi-supervised
methods is comparable to if not as good as the super-
vised classifier trained on a 60-40 split of the corpus.
Thus, our results indicate that the use of semantic
and temporal features is effective for MCCR in clin-
ical text. It is clear from the co-training and poste-
rior regularization results that treating MCCR as a
semi-supervised problem works.
8 Conclusions
We investigated the task of MCCR in clinical text us-
ing supervised and semi-supervised learning meth-
ods. We create annotated corpora of clinical text
with case reports from the NEJM and narratives ob-
tained from The Ohio State University Wexner Med-
ical Center. We work with the hypothesis that de-
termining semantic and temporal similarity between
medical concepts helps resolve coreferences. In
order to test this hypothesis, we describe the pro-
cess of semantic and temporal feature extraction
from clinical text. We demonstrate the effective-
ness of the extracted features in a supervised binary
classification task for MCCR with MaxEnt classi-
fiers (using the combined feature set) as well as us-
ing semi-supervised methods of co-training MaxEnt
classifiers and training MaxEnt models using pos-
terior regularization (using two independent views
of the data - semantic view and temporal view).
Thus, we show that MCRR can be performed using
semi-supervised learning with semantic and tempo-
ral views of the data.
Acknowledgments
The project described was supported by the
National Center for Research Resources,
Grant UL1RR025755, KL2RR025754, and
TL1RR025753, and is now at the National
Center for Advancing Translational Sciences,
Grant 8KL2TR000112-05, 8UL1TR000090-05,
8TL1TR000091-05. The content is solely the re-
sponsibility of the authors and does not necessarily
represent the official views of the NIH.
739
References
Sophia Ananiadou, Carol Freidman, and Jun??chi Tsu-
jii. 2004. Introduction: named entity recognition in
biomedicine. J. of Biomedical Informatics, pages 393?
395.
Kedar Bellare, Gregory Druck, and Andrew McCallum.
2009. Alternating projections for learning with expec-
tation constraints. In Proceedings of the Twenty-Fifth
Conference on Uncertainty in Artificial Intelligence,
UAI ?09, pages 43?50.
Avrim Blum and Tom M. Mitchell. 1998. Combin-
ing labeled and unlabeled data with co-training. In
COLT?98, pages 92?100.
Bigi Brigitte. 2003. Using Kullback-Leibler distance for
text categorization. In Proceedings of the 25th Euro-
pean conference on IR research, ECIR?03, pages 305?
319.
Wendy W Chapman, Prakash M Nadkarni, Lynette
Hirschman, Guergana K Savova Leonard W D?Avolio,
and Ozlem Uzuner. 2011. Overcoming barriers to
NLP for clinical text: the role of shared tasks and the
need for additional creative solutions. In JAMIA.
Jung-Hsien Chiang, Jou-Wei Lin, and Chen-Wei Yang.
2010. Automated evaluation of electronic discharge
notes to assess quality of care for cardiovascular dis-
eases using Medical Language Extraction and Encod-
ing System (MedLEE). JAMIA, pages 245?252.
A.J. Conger. 1980. Integration and generalization of
kappas for multiple raters. In Psychological Bulletin
Vol 88(2), pages 322?328.
Peter J Embi and Philip Payne. 2009. Clinical research
informatics: challenges, opportunities and definition
for an emerging domain. Journal of the American
Medical Informatics Association, 16(3):316?327.
Carol Friedman, Pauline Kra, and Andrey Rzhetsky.
2002. Two biomedical sublanguages: a description
based on the theories of Zellig Harris. Journal of
Biomedical Informatics, 35(4):222?235.
Rob Gaizauskas, Henk Harkema, Mark Hepple, and An-
drea Setzer. 2006. Task-oriented extraction of tem-
poral information: The case of clinical narratives.
In Proceedings of the Thirteenth International Sym-
posium on Temporal Representation and Reasoning,
TIME ?06, pages 188?195.
Kuzman Ganchev, Joo Graa, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine
Learning Research, pages 2001?2049.
Tian Ye He. 2007. Coreference Resolution on Entities
and Events for Hospital Discharge Summaries. EECS,
Cambridge, MA, MIT. M.Eng.
Hyuckchul Jung, James Allen, Nate Blaylock, Will
de Beaumont, Lucian Galescu, and Mary Swift. 2011.
Building timelines from narrative clinical records: ini-
tial results based-on deep natural language under-
standing. In Proceedings of BioNLP 2011 Workshop,
BioNLP ?11, pages 146?154.
Christoph Muller, Stefan Rapp, and Michael Strube.
2002. Applying co-training to reference resolution. In
ACL, pages 352?359.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
ACL, pages 1396?1411.
Kamal Nigam and Rayid Ghani. 2000. Analyzing
the effectiveness and applicability of co-training. In
CIKM?00, pages 86?93.
James Pustejovsky, Jos M. Castao, Robert Ingria, Roser
Sauri, Robert J. Gaizauskas, Andrea Setzer, Graham
Katz, and Dragomir R. Radev. 2003. Timeml: Robust
specification of event and temporal expressions in text.
In New Directions in Question Answering?03, pages
28?34.
Preethi Raghavan and Albert M. Lai. 2010. Leveraging
natural language processing of clinical narratives for
phenotype modeling. In PIKM?10, pages 57?66.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?10, pages 492?
501, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Guergana K. Savova, James J. Masanz, Philip V.
Ogren, Jiaping Zheng, Sunghwan Sohn, Karin Kip-
per Schuler, and Christopher G. Chute. 2010. Mayo
clinical text analysis and knowledge extraction sys-
tem (cTAKES): architecture, component evaluation
and applications. JAMIA, pages 507?513.
Guergana K. Savova, Wendy Webber Chapman, Jiaping
Zheng, and Rebecca S. Crowley. 2011. Anaphoric
relations in the clinical narrative: corpus creation.
JAMIA, 18(4):459?465.
Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.
2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguis-
tics, pages 521?544.
Yang Xiang, Kewei Lu, Stephen L James, Tara B Bor-
lawsky, Kun Huang, and Philip R O Payne. 2011. k-
neighborhood decentralization: A comprehensive so-
lution to index the UMLS for scale knowledge discov-
ery. In Journal of Biomedical Informatics.
Jiaping Zheng, Wendy Webber Chapman, Rebecca S.
Crowley, and Guergana K. Savova. 2011. Coreference
resolution: A review of general methodologies and ap-
plications in the clinical domain. Journal of Biomedi-
cal Informatics, 44(6):1113?1122.
740
Li Zhou and George Hripcsak. 2007. Temporal rea-
soning with medical data - a review with emphasis
on medical natural language processing. Journal of
Biomedical Informatics, pages 183?202.
Li Zhou, Genevieve B. Melton, Simon Parsons, and
George Hripcsak. 2006. A temporal constraint struc-
ture for extracting temporal information from clinical
narrative. Journal of Biomedical Informatics, pages
424?439.
741
