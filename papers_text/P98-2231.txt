Structural Disambiguat ion Based on Reliable 
Est imat ion of Strength of Associat ion 
Haodong Wu Eduardo de Paiva Alves 
Teiji Furugori 
Department  of Computer  Science 
University of E lectro-Communicat ions 
1-5-1, Chofugaoka, Chofu, Tokyo 1828585, JAPAN 
{wu, ea lves ,  fu rugor i  }@phaeton. cs .  uec .  ac.  j p 
Abst rac t  
This paper proposes a new class-based method 
to estimate the strength of association in word 
co-occurrence for the purpose of structural dis- 
ambiguation. To deal with sparseness of data, 
we use a conceptual dictionary as the source 
for acquiring upper classes of the words related 
in the co-occurrence, and then use t-scores to 
determine a pair of classes to be employed for 
calculating the strength of association. We have 
applied our method to determining dependency 
relations in Japanese and prepositional phrase 
attachments in English. The experimental re- 
sults show that the method is sound, effective 
and useful in resolving structural ambiguities. 
1 In t roduct ion  
The strength of association between words pro- 
vides lexical preferences for ambiguity resolu- 
tion. It is usually estimated from statistics on 
word co-occurrences in large corpora (Hindle 
and Rooth, 1993). A problem with this ap- 
proach is how to estimate the probability of 
word co-occurrences that are not observed in 
the training corpus. There are two main ap- 
proaches to estimate the probability: smoothing 
methods (e.g., Church and Gale, 1991; Jelinek 
and Mercer, 1985; Katz, 1987) and class-based 
methods (e.g., Brown et al, 1992; Pereira and 
Tishby, 1992; Resnik, 1992; Yarowsky, 1992). 
Smoothing methods estimate the probabil- 
ity of the unobserved co-occurrences by using 
frequencies of the individual words. For exam- 
pie, when eat and bread do not co-occur, the 
probability of (eat, bread) would be estimated 
by using the frequency of (eat) and (bread). 
A problem with this approach is that it pays 
no attention to the distributional characteris- 
tics of the individual words in question. Using 
this method, the probability of (eat, bread> and 
(eat, cars) would become the same when bread 
and cars have the same frequency. It is unac- 
ceptable from the linguistic point of view. 
Class-based methods, on the other hand, es- 
timate the probabihties by associating a class 
with each word and collecting statistics on word 
class co-occurrences. For instance, instead of 
calculating the probability of (eat, bread) di- 
rectly, these methods associate eat with the 
class \[ingest\] and bread with tile class \[food\] 
and collect statistics on the classes \[ingest\] and 
\[food\]. The accuracy of the estimation depends 
on the choice of classes, however. Some class- 
based methods (e.g., Yarowsky, 1992) associate 
each word with a single class without considcr- 
ing the other words in the co-occurrence. How- 
ever, a word may need to be replaced by differ- 
ent class depending on the co-occurrence. Some 
classes may not have enough occurrences to al- 
low a reliable estimation, while other classes 
may be too general and include too many words 
not relevant o the estimation. An alternative is
to obtain various classes associated in a taxon- 
omy with the words in question and select the 
classes according to a certain criteria. 
There are a number of ways to select the 
classes used in the estimation. Weischedel et al 
(1993) chose the lowest classes in a taxonomy 
1416 
for which the association for the co-occurrence 
can be estimated. This approach may result in 
unreliable estimates, since some of the class co- 
occurrences used may be attr ibuted to chance. 
Resnik (1993) selected all pairs of classes corre- 
sponding to the head of a prepositional phrase 
and weighted them to bias the computation 
of the association in favor of higher-frequency 
co-occurrences which he considered "more reli- 
able." Contrary to this assumption, high fre- 
quency co-occurrences axe unreliable when the 
probabil ity that the co-occurrence may be at- 
tr ibuted to chance is high. 
In this paper we propose a class-based 
method that selects the lowest classes in a tax- 
onomy for which the co-occurrence confidence 
is above a threshold. We subsequently apply 
the method to solving structural ambiguities 
in Japanese dependency structures and English 
prepositional phrase attachments. 
2 Class-based Est imat ion of 
St rength  of Assoc iat ion 
The strength of association (SA) may be 
measured using the frequencies of word co- 
occurrences in large corpora. For instance, 
Church and Hanks (1990) calculated SA in 
terms of mutual information between two words 
wl and w2: 
N * f(wl,w2) I(wl, w2) = log2 (1) f(wl)f(w2) 
here N is the size of the corpus used in the es- 
timation, f(Wl, w2) is the frequency of the co- 
occurrence, f(wl) and f(w2) that of each word. 
When no co-occurrence is observed, SA may 
be estimated using the frequencies of word 
classes that contain the words in question. The 
mutual information in this case is estimated by: 
I(CI, C2) = log2 N * f(Cl, C2) (2) 
f(Cl )f(C2) 
here Cl and C2 are the word classes that respec- 
tively contain Wl and w2, f(C1) and f(C2) the 
numbers of occurrences of all the words included 
in the word classes C1 and C2, and f(C1, C2) is 
the number of co-occurrences of the word classes 
C1 and C2. 
Normally, the estimation using word classes 
needs to select classes, from a taxonomy, for 
which co-occurrences are significant. We use t- 
scores for this purpose 1. 
For a class co-occurrence (C1,C2), the t- 
score may be approximated by: 
~ f(C,,C2) - -~f(Cl)f(C2) (3) 
J/(c,,c2) 
We use the lowest class co-occurrence for 
which the confidence measured with t-scores is 
above a threshold 2. Given a co-occurrence con- 
taining the word w, our method selects a class 
for w in the following way: 
Step 1: Obtain the classes C 1, C 2 .... , C n associ- 
ated with w in a taxonomy. 
Step 2: Set i to 0. 
Step 3: Set i to i q- 1. 
Step 4: Compute t using formula (3). 
Step 5: If t < threshold. 
If i ~ n goto step 3. 
Otherwise xit. 
Step 6: Select the class C i to replace w. 
Let us see what this means with an ex- 
ample. Suppose we try to estimate SA for 
(produce, telephone) 3. See Table 1. Here f (v) ,  
f(n) and f(vn) axe the frequencies for the verb 
produce, classes for the noun telephone, and co- 
occurrences between the verb and the classes for 
telephone, respectively; and t is the t-score 4. 
'The t-score (Church and Mercer, 1993) compares the 
hypothesis that a co-occurrence is significan~ against he 
null hypothesis that the co-occurrence an be attributed 
to chance. 
2The default threshold for t-score is 1.28 which cor- 
responds to a confidence l vel of 90%. t-scores are often 
inflated due to certain violations of assumptions. 
aThe data was obtained from 68,623 verb-noun pairs 
in EDR Corpus (EDR, 1993). 
4In our theory, we are to use each pair of (C i, Ci), 
where i=l,2,...m, j - l ,2, . . . ,n,  to calculate strengths of 
lexical associations. But our experiments show that up- 
per classes of a verb are very unreliable to be used to 
measure the strengths. The reason may be that, unlike 
nouns, the verbs would not have a "neat" hierarchy or 
that the upper classes of a verb become too general as 
they contain too many concepts underneath t em. Be- 
cause of this observation, we use, for the classes of a 
1417 
verb classes for telephone f(v) f(n) f(vn) t-score 
produce concrete thing 671 18926 100 -4.6 
produce inanimate object 671 5593 69 0.83 
produce implement/tool  671 2138 35 1.91 
produce machine 671 664 19 2.86 
produce communication machine 671 83 1 0.25 
produce telephone 671 24 0 - 
Table 1 Estimation of (produce telephone) 
The lowest class co-occurrence (produce, 
communication machine) has a low t-score and 
produces a bad estimation. The most frequent 
co-occurrence (produce, concrete thing) has a 
low t-score also reflecting the fact that it may be 
attr ibuted to chance. The t-scores for (produce, 
machine) and (produce, implement/tooO are 
high and show that these co-occurrences are sig- 
nificant. Among them, our method selects the 
lowest class co-occurrence for which the t-score 
is above the threshold: (produce, machine). 
3 Disambiguation Using 
Class-Based Estimation 
We now apply our method to estimate SA for 
two different types of syntactic constructions 
and use the results in resolving structural am- 
biguities. 
3.1 D isambiguat ion  o f  Dependency  
Re la t ions  in  Japanese  
Identifying the dependency structure of a 
Japanese sentence is a difficult problem since 
the language allows relatively free word or- 
ders. A typical dependency relation in 
Japanese appears in the form of modifier- 
particle-modificand triplets. When a modifier is 
followed by a number of possible modificands, 
verb, the verb itself or, when it does not give us a good 
result, only the lowest class of the verb in calculating the 
strength of association (SA). Thus, for an example, the 
verb eat has a sequence of eat ~ ingest ~ put something 
into body --%... --" event -" concept in the class hierarchy, 
but we use only eat and ingest for the verb eat when 
calculating SA for (eat, apple). 
there arise situations in which syntactic roles 
may be unable to determine the dependency re- 
lation or the modifier-modificand relation. For 
instance, in 
' ~ 0 '(vigorous) may modify either ' q~ 
~'  (middle aged) o r '  t l l~  ' ( health care). 
But which one is the modiflcand o f '  ~ ~ ~ 0 ' ? 
We solve the ambiguity comparing the strength 
of association for the two or more possible de- 
pendency relations. 
Calculation of Strength of Association We cal- 
culate the Strength of Association (SA) score 
for modi f ier  - particle - modi f icand by: 
SA(rn / ;p . . .  m.) = log2 \ /(C., l i .r)/(p..trn.) \] 
(a) 
where Cmfie r stands for the classes that in- 
clude the modifier word, Part is the particle fol- 
lowing the modifier, mc the content word in the 
modificand phrase, and f the frequency. 
Let us see the process of obtaining SA score 
in an example ( ~ - ?)~- ~ ( ) (literally: profes- 
sor - subject.marker - work). To calculate the 
frequencies for the classes associated with ' ~ 
', we obtain from the Co-occurrence Dictionary 
(COD) 5 the number of occurrences for (w- 3 ?- 
SCOD and CD are provided by Japan Electronic Dic- 
tionary Research Institute (EDR, 1993). COD contains 
the frequencies of individual words and of the modifier- 
1418 
< ), where w can be any modifier. We then 
obtain from the Concept Dictionary (CD) 6 the 
closes that include ' $~'  and then sum up all 
the occurrences ofwords included in the classes. 
The relevant portion of CD for ' $~'  in ( ~ 
-$~-~ < ) is shown in Figure 1. The numbers 
in parenthesis here indicate the summed-up fre- 
quencies. 
We then calculate the t-score between ' $~- 
< ' and all the classes that include' ~ '. See 
Table 2. 
Classes for the t- particle- 
modifier ~ score modificand 
A ~ ~ $ # ~  4.57 h?~< 
A~ 5.14 $~< 
~ O ~ A ~  1.74 ~< 
~ ~ A ~  0.74 ~< 
Table 2 t-scores for ( ~ - ~-  ~ < ) 
The t-score for the co-occurrence of the 
modifier and particle-modificand pair, '~}~'  
and '~)~-~ < ', is higher than the threshold 
when ' ~ '  is replaced with \[~J~C~_t~)kr~\].  
Using (4), the strength of ~sociat ion for the co- 
occurrence of ( ~ - ~)~ - ~ < ) is calculated from 
the SA between the c l~s \[~R~lJ'C~_?cgk~\] and 
, ~_~<. '  
When the word in question has more than 
one sense, we estimate SA corresponding to each 
sense and choose the one that results in the 
highest SA score. For instance, we estimate SA 
between ' ~ '  and the various senses of ' ~ < 
', and choose the highest value: in this case the 
one corresponding to the sense 'to be employed.' 
Determination of Most Strongly Associated 
Structure After calculating SA for each possible 
construction, we choose the construction with 
highest SA score as the most probable struc- 
pm-ticle-modificand triplets in a corpus that includes 
220,000 parsed Japanese sentences. 
6 CD provides a hierarchical structure of concepts cor- 
responding to all the words in COD. The number of con- 
cepts in CD is about 400,000. 
ture. See the following example: 
? ? . ~ ? ) ~  ~'C  ~< )kc )~b~:~.  ? ? 
.technic:al progress work people stress |nnovatlon 
Here, the arrows show possible dependency 
relations, the numbers on the arrows the esti- 
mated SA, and the thick arrows the dependency 
with highest mutual  information that means the 
most probable dependency relation. In the ex- 
ample, ' ~d:~ ~' modifies ' j~A.'C ' and ' ~ < 
' modifes ' A '. The estimated mutual  informa- 
tion for ( ~g~#~,  ~A,~C ) is 2.79 and that for 
( ff~ i ,  A ) is 6.13. Thus, we choose ' ~_/,~C ' as 
the modificand for ' ~ $ ?  ' and '  ,k ' as that 
for ' ~ i ' 
In the example shown in Figure 2, our 
method selects the most likely modifier- 
modificand relation. 
Experiment Disambiguation of dependency re- 
lations was done using 75 anlbiguous con- 
structions from Fukumoto (1992). Solving 
the ambiguity in the constructions involves 
choosing among two or more modifier-particle- 
modificand relations. The training data con- 
sists of all 568,000 modifier-particle-modificand 
triplets in COD. 
Evaluation We evaluated the performance of 
our method comparing its results with those of 
other methods using the same test and training 
data. Table 3 shows the various results (suc- 
cess rates). Here, (1) indicates the performance 
obtained using the principle of Closest Attach- 
ment (Kimball, 1973); (2) shows the perfor- 
mance obtained using the lowest observed class 
co-occurrence (Weischedel et al, 1993); (3) is 
the result from the maximum mutual informa- 
tion over all pairs of classes corresponding to 
the words in the co-occurrence (Resnik, 1993; 
Alves, 1996); and (4) shows the performance of 
our method 7. 
7The precision is for the 1.28 default threshold. The 
precision was 81.2% and 84.1% when we set the threshold 
to .84 and .95. In all these cases the coverage was 92.0%. 
1419 
(3) person (3) 
I human or similar (42) I 
AM 
(39) human 
defined by race or origin 
(3) Japanese (2) worker 
(5) person defined by role 
(I) person defined by position 
..?. ....... 
(I) slave (0) professor 
Figure 1 An Extract of CD 
\[~ 9.19 \[ 4.48 
F- ' )  I t 
national investigation based cause prompt study expect 
Figure 2 An example of parsing a Japanese sentence 
method precision 
(1) closest attachment 70.6% 
(2) lowest classes 81.2% 
(3) maximum MI 82.6% 
(4) our method 87.0% 
Table 3 Results for determining dependency 
relations 
Closest attachment (1) has a low perfor- 
mance since it fails to take into consideration 
the identity of the words involved in the deci- 
sion. Selecting the lowest classes (2) often pro- 
duces unreliable stimates and wrong decisions 
due to data sparseness. Selecting the classes 
with highest mutual information (3) results in 
overgeneralization that may lead to incorrect at- 
tachments. Our method avoids both estimating 
from unreliable classes and overgeneralization 
and results in better estimates and a better per- 
formance. 
A qualitative analysis of our results shows 
two causes of errors, however. Some errors oc- 
curred when there were not enough occurrences 
of the particle-modificand pattern to estimate 
any of the strength of association necessary 
for resolving ambiguity. Other errors occurred 
when the decision could not be made without 
surrounding context. 
3.2 P repos i t iona l  Phrase  At tachment  
in Eng l i sh  
Prepositional phrase (PP) attachment is a 
paradigm case of syntactic ambiguity. The most 
probable attachment may be chosen comparing 
the SA between the PP and the various attach- 
ment elements. Here SA is measured by: 
S A( v_attachlv, p, n2) = log2 \ - \ ] - (C~ ~',2 ) ) 
(5) 
SA(n_attachln,,p, n ) -- log, \ 7-(C-~,~-C,--~2 ) \] 
(6) 
where Cw stands for the class that includes 
the word w and f is the frequency in a training 
data containing verb-nounl-preposition-noun2 
constructions. 
Our method selects from a taxonomy the 
classes to be used to calculate the SA score and 
1420 
then chooses the attachment with highest SA 
score as the most probable. 
Experiment We performed a PP attachment 
experiment on the data that consists of all 
the 21,046 semantically annotated verb-noun- 
preposition-noun constructions found in EDR 
English Corpus. We set aside 500 constructions 
for test and used the remaining 20,546 as train- 
ing data. We first performed the experiment 
using various values for the threshold. Table 
4 shows the results. The first line here shows 
the default which corresponds to the most likely 
attachment for each preposition. For instance, 
the preposition of is attached to the noun, re- 
flecting the fact that PP's led by of are mostly 
attached to nouns in the training data. The 
'confidence' values correspond to a binomial dis- 
tribution and are given only as a reference s. 
confidence t coverage precision success 
100% 68.0% 68.0% 
50% .00 82% 82.2% 79.4% 
70% .52 75% 87.3% 83.4% 
80% .84 65% 88.6% 84.2% 
85% .95 57% 89.6% 84.8% 
90% 1.28 50% 91.3% 85.6% 
Table 4 Results for PP attachment with 
various thresholds for t-score 
The precision grows with t-scores, while 
coverage decreases. In order to improve cov- 
erage, when the method cannot find a class 
co-occurrence for which the t-score is above 
the threshold, we recursivcly tried to find a 
co-occurrence using the threshold immediately 
smaller (see Table 4). When the method could 
not find co-occurrences with t-score above the 
smallest hreshold, the default was used. The 
overall success rates are shown in "success" col- 
umn in Table 4. 
SAs another way of reducing the sparse data problem, 
we clustered prepositions using the method escribed in 
"~Vu and Furugori (1996). Prepositions like synonyms 
and antonyms are clustered into groups and replaced by 
a representative preposition (e.g., till and pending are 
replaced by until; amongst, amid and amidst are replaced 
by among.). 
Evaluation We evaluated the performance of 
our method comparing its results with those of 
other methods with the same test and training 
data. The results are given in Table 5. Here, (5) 
shows the performance of two native speakers 
who were just presented quadruples of four head 
words without surrounding contexts. 
Method Success Rate 
(1)closest Attachment 59.6% 
(2)lowest classes 80.2% 
(3)maximum MI 79.0% 
(4)our method 85.6% 
(5)human (head words only) 87.0% 
Table 5 Comparison with other methods 
The lower bound and the upper bound on 
the performance of our method seem to be 
59.6% scored by the simple heuristic of closest 
attachment (1) and 87.0% by human beings (4). 
Obviously, the success rate of closest attach- 
ment (1) is low as it always attaches a word to 
the noun without considering the words in ques- 
tion. The unanticipated low success rate of hu- 
man judges is partly due to the fact that some- 
times constructions were inherently ambiguous 
so that their choices differed from the annota- 
tion in the corpus. 
Our method (4) performed better than the 
lowest classes method (2) and maximum MI 
method (3). It owes mainly to the fact that 
our method makes the estimation from class co- 
occurrences that are more reliable. 
4 Concluding Remarks  
We proposed a class-based method that selects 
classes to be used to estimate the strength of as- 
sociation for word co-occurrences. The classes 
selected by our method can be used to estimate 
various types of strength of association i differ- 
ent applications. The method iffers from other 
class-based methods in that it allows identifica- 
tion of a reliable and specific class for each co- 
occurrence in consideration and can deal with 
date sparseness problem more efficiently. It 
1421 
overcame the shortcomings from other meth- 
ods: overgeneralization and employment of un- 
reliable class co-occurrences. 
We applied our method to two structural 
disambiguation experiments. In both exper- 
iments the performance is significantly better 
than those of others. 
References 
\[1\] Alves, E. 1996. "The Selection of the Most 
Probable Dependency Structure in Japanese 
Using Mutual Information." In Proc. of the 
34th ACL, pages 372-374. 
\[2\] Brown, P., Della Pietra, V. and Mercer, 
R. (1992). "Word Sense Disambiguation Us- 
ing Statistical Methods." Proceedings of the 
30th ACL, pages 264-270. 
\[3\] Church, K., and Mercer, R. 1993. "Introduc- 
tion to the Special Issue on Computational 
Linguistics Using Large Corpora." Compu- 
tational Linguistics, 19 (1): 1-24. 
\[4\] Church, K., and Hanks, P. 1990. "Word As- 
sociation Norms, Mutual Information and 
Lexicography." Computational Linguistics, 
16(1):22-29. 
\[5\] Church, K., and Gale, W. 1991. "A Com- 
parison of the Enhanced Good-Turing and 
Deleted Estimation Methods for Estimat- 
ing Probabilities of English Bigrams." Com- 
puter Speech and Language, 5:19-54. 
\[6\] Fukumoto, F., Sano, H., Saitoh, Y. and 
Fukumoto J. 1992. "A Framework for De- 
pendency Grammar Based on the Word's 
Modifiability Level - Restricted Dependency 
Grammar." Trans. IPS Japan, 33(10):1211- 
1223 (in Japanese). 
\[7\] Hindle, D., and Rooth, M. 1993. "Structural 
Ambiguity and Lexical Relations." Compu- 
tational Linguistics, 19(1):103-120. 
\[8\] Japan Electronic Dictionary Research Insti- 
tute, Ltd. 1993. EDR Electronic Dictionary 
Specifications Guide (in Japanese). 
\[9\] Jelinek, F., and Mercer, R. 1985. "Proba- 
bility Distribution Estimation from Sparse 
Data." IBM Technical Disclosure Bulletin, 
28:2591-2594. 
\[10\] Katz, S. 1987. "Estimation of Probabili- 
ties from Sparse Data for Language Model 
Component of a Speech Recognizer." IEEE 
Transactions on Acoustics, Speech and Sig- 
nal Processing, ASSP-35(3):400-401. 
\[11\] Kimball, J. 1973. "Seven Principles of 
Surface Structure Parsing in Natural Lan- 
guage." Cognition, 2:15-47. 
\[12\] Pereira, F. and Tishby, N. 1992. "Distribu- 
tional Similarity, Phrase Transitions and Hi- 
erarchical Clustering." In Proc. of the 30th 
ACL, pages 183-190. 
\[13\] Resnik, P. 1992. "Wordnet and Distribu- 
tional Analysis: A Class-Based Approach 
to Lexical Discovery." AAAI Workshop on 
Statistically-based Natural Language Pro- 
cessing Techniques, pages 56-64. 
\[14\] Resnik, P. 1993. "Selection and Informa- 
tion: A Class-Based Approach to Lexical 
Relationships." PhD. thesis, University of 
Pennsylvania. 
\[15\] Weischedel, R., Meteer, M., Schwartz, R., 
Ramshaw, L., and Palmucci, J. 1993. "Cop- 
ing with Ambiguity and Unknown Words 
Through Probabilistic Models." Computa- 
tional Linguistics, 19(2):359-382. 
\[16\] Wu, H. and Furugori, T. 1996. "A Hy- 
brid Disambiguation Model for Preposi- 
tional Phrase Attachment." Literary and 
Linguistic Computing. 11(4):187-192. 
\[17\] Yarowsky, D. 1992. "Word Sense Disam- 
biguation using Statistical Models of Roget's 
Categories Trained on Large Corpora." Pro- 
ceedings of COLING-92, pages 454-460. 
1422 
