Coling 2008: Proceedings of the workshop on Cognitive Aspects of the Lexicon (COGALEX 2008), pages 9?17
Manchester, August 2008
Lexical Access Based on Underspecified Input
Michael ZOCK
LIF-CNRS
?
Equipe TALEP
163, Avenue de Luminy
F-13288 Marseille Cedex 9
michael.zock@lif.univ-mrs.fr
Didier SCHWAB
Groupe GETALP
Laboratoire d?Informatique de Grenoble
385 avenue de la Bibliothque - BP 53
F-38041 Grenoble Cedex 9
didier.schwab@imag.fr
Abstract
Words play a major role in language pro-
duction, hence finding them is of vital im-
portance, be it for speaking or writing.
Words are stored in a dictionary, and the
general belief holds, the bigger the bet-
ter. Yet, to be truly useful the resource
should contain not only many entries and a
lot of information concerning each one of
them, but also adequate means to reveal the
stored information. Information access de-
pends crucially on the organization of the
data (words) and on the navigational tools.
It also depends on the grouping, ranking
and indexing of the data, a factor too often
overlooked.
We will present here some preliminary re-
sults, showing how an existing electronic
dictionary could be enhanced to support
language producers to find the word they
are looking for. To this end we have started
to build a corpus-based association ma-
trix, composed of target words and ac-
cess keys (meaning elements, related con-
cepts/words), the two being connected at
their intersection in terms of weight and
type of link, information used subsequently
for grouping, ranking and navigation.
1 Context and problem
When speaking or writing we encounter basi-
cally either of the following two situations: one
where everything works automatically, somehow
like magic, words popping up one after another
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
like spring water, and another where we look de-
liberately and often painstakingly for a specific,
possibly known word. We will be concerned here
with this latter situation: a speaker/ writer using
an electronic dictionary to look for such a word.
Unfortunately, alphabetically organized dictionar-
ies are not well suited for this kind of reverse
lookup where the inputs are meanings (elements of
the word?s definition) or conceptually related ele-
ments (collocations, associations), and the outputs
the target words.
Without any doubt, lexicographers have made
considerable efforts to assist language users, build-
ing huge resources, composed of many words and
lots of information associated with each one of
them. Still, it is not unfair to say most dictionar-
ies have been conceived from the reader?s point of
view. The lexicographers have hardly taken into
account the language producer?s perspective,
1
con-
sidering conceptual input, incomplete as it may be,
as starting point. While readers start with words,
looking generally for their corresponding mean-
ings, speakers or writers usually start with the op-
posite, meanings or concepts,
2
which should be the
entry points of a dictionary, which ideally is neu-
tral in terms of access direction.
3
The problem is that we still don?t know very
well what concepts are, whether they are compo-
sitional and if so, how many primitives there are
(Wilks, 1977; Wierzbicka, 1996; Goddard, 1998).
1
Roget?s thesaurus (Roget, 1852), Miller and Fellbaum?s
WordNet (Fellbaum, 1998) and Longman?s Language Activa-
tor (Summers, 1993), being notable exceptions (For more de-
tails, see next section).
2
Of course, this does not preclude, that we may have to
use words to refer to them in a concept-based query.
3
While we agree with Polgu`ere theoretically when he
pleads for dictionary neutrality with regard to lexical access
(Polgu`ere, 2006), from a practical point of view the situation
is obviously quite different for the speaker and listener, even
if both of them draw on the same resource.
9
Neither do we know how to represent them. Yet,
there are ways around this problem as we will
show. Whether concepts and words are organized
and accessed differently is a question we cannot
answer here. We can agree though on the fact
that getting information concerning words is fairly
unproblematic when reading, at least in the case
of most western languages. Words can gener-
ally be found easily in a dictionary, provided the
user knows the spelling, the alphabet and how to
build lemma starting from an inflected form. Un-
like words, which are organized alphabetically (in
western languages) or by form (stroke counts in
Chinese), concepts are organized topically: they
are clustered into functional groups according to
their role in real world, or our perception of it.
Psychologist have studied the difficulties peo-
ple have when trying to produce or access words
(Aitchinson, 2003). In particular, they have stud-
ied the tip-of-the-tongue phenomenon (Brown and
McNeill, 1996) and the effects an input can have
on the quality of an output (error analysis (Cutler,
1982)) and on the ease of its production: positive
or negative priming effect (activation/inhibition).
Obviously, these findings allow certain conclu-
sions, and they might guide us when developing
tools to help people find the needed word. In par-
ticular, they reveal two facts highly relevant for our
goal:
1. even if people fail to access a given word, they
might know a lot about it: origin, meaning
(word definition, role played in a given sit-
uation), part of speech, number of syllables,
similar sounding words, etc. Yet, despite all
this knowledge, they seem to lack some cru-
cial information to be able to produce the pho-
netic form. The word gets blocked at the very
last moment, even though it has reached the
tip-of-the-tongue. This kind of nuisance is all
the more likely as the target word is rare and
primed by a similar sounding word.
2. unlike words in printed or electronic dictio-
naries, words in our mind may be inexis-
tent as tokens. What we seem to have in
our minds are decomposed, abstract entities
which need to be synthesized over time.
4
Ac-
4
This may be very surprising, yet, this need not be the case
if we consider the fact that speech errors are nearly always
due to competing elements from the same level or an adja-
cent one, unless they are the result of a surrounding concept
which has been activated, or which is about to be translated
cording to Levelt (Levelt, 1996) the genera-
tion of words (synthesis) involves the follow-
ing stages: conceptual preparation, lexical se-
lection, phonological- and phonetic encoding,
articulation. Bear in mind that having per-
formed ?lexical selection? does not imply ac-
cess to the phonetic form (see the experiments
on the tip-of-the-tongue phenomenon).
What can be concluded from these observa-
tions? It seems that underspecified input is suffi-
ciently frequent to be considered as normal. Hence
we should accept it, and make the best out of it by
using whatever information is available (accessi-
ble), no matter how incomplete, since it may still
contribute to find the wanted information, be it by
reducing the search space. Obviously, the more in-
formation we have the better, as this reduces the
number of words among which to choose.
2 Related work and goal
While more dictionaries have been built for the
reader than for the writer, there have been some
onomasiological attempts as early as in the mid-
dle of the 19th century. For example, Roget?s
Thesaurus (Roget, 1852), T?ong?s Chinese and
English instructor (T?ong, 1862), or Boissiere?s
analogical dictionary (Boissi`ere, 1862).
5
Newer
work includes Mel??cuk?s ECD (Mel??cuk et al,
1999), Miller and Fellbaum?s WordNet (Fellbaum,
1998), Richardson and Dolan?s MindNet (Richard-
son et al, 1998), Dong?s HowNet (Dong and
Dong, 2006) and Longman?s Language Activa-
tor (Summers, 1993). There is also the work of
into words. Put differently, we do not store words at all in
our mind, at least not in the layman?s or lexicographer?s sense
who consider word-forms and their meanings as one. If we
are right, than rather continue to consider the human mind as
a word store we could consider it as a word factory. Indeed,
by looking at some of the work done by psychologists who try
to emulate the mental lexicon (for a good survey see (Harley,
2004), pages 359-374) one gets the impression that words are
synthesized rather than located and read out. Taking a look at
all this work, generally connectionist models, one may con-
clude that, rather than having words in our mind we have a
set of more or less abstract features (concepts, syntactic infor-
mation, phonemes), distributed across various layers, which
need to be synthesized over time. To do so we proceed from
abstract meanings to concrete sounds, which at some point
were also just abstract features. By propagating energy rather
than data (as there is no message passing, transformation or
cumulation of information, there is only activation spreading,
that is, changes of energy levels, call it weights, electronic
impulses, or whatever), that we propagate signals, activating
ultimately certain peripheral organs (larynx, tongue, mouth,
lips, hands) in such a way as to produce movements or sounds,
that, not knowing better, we call words.
5
For a more recent proposal see (Robert et al, 1993).
10
(Fontenelle, 1997; Sierra, 2000; Moerdijk, 2008),
various collocation dictionaries (BBI, OECD) and
Bernstein?s Reverse Dictionary.
6
Finally, there is
M. Rundell?s MEDAL, a thesaurus produced with
the help of Kilgariff?s Sketch Engine (Kilgarriff et
al., 2004).
As one can see, a lot of progress has been ac-
complished over the last few years, yet more can be
done, especially with regard to unifying linguistic
and encyclopedic knowledge. Let?s take an exam-
ple to illustrate our point.
Suppose, you were looking for a word express-
ing the following ideas: ?superior dark coffee made
from beans from Arabia?, and that you knew that
the target word was neither espresso nor cappuc-
cino. While none of this would lead you directly
to the intended word, mocha, the information at
hand, i.e. the word?s definition or some of its ele-
ments, could certainly be used. In addition, people
draw on knowledge concerning the role a concept
(or word) plays in language and in real world, i.e.
the associations it evokes. For example, they may
know that they are looking for a noun standing for
a beverage that people take under certain circum-
stances, that the liquid has certain properties, etc.
In sum, people have in their mind an encyclope-
dia: all words, concepts or ideas being highly con-
nected. Hence, any one of them has the potential to
evoke the others. The likelihood for this to happen
depends, of course, on factors such as frequency
(associative strength), distance (direct vs. indirect
access), prominence (saliency), etc.
How is this supposed to work for a dictionary
user? Suppose you were looking for the word
mocha (target word: t
w
), yet the only token com-
ing to your mind were computer (source word:
s
w
). Taking this latter as starting point, the system
would show all the connected words, for example,
Java, Perl, Prolog (programing languages), mouse,
printer (hardware), Mac, PC (type of machines),
etc. querying the user to decide on the direction of
search by choosing one of these words. After all,
s/he knows best which of them comes closest to the
t
w
. Having started from the s
w
?computer?, and
knowing that the t
w
is neither some kind of soft-
ware nor a type of computer, s/he would probably
choose Java, which is not only a programming lan-
guage but also an island. Taking this latter as the
6
There is also at least one electronic incarnation
of a dictionary with reverse access, combining a dic-
tionary (WordNet) and an encyclopedia (Wikipedia)
(http://www.onelook.com/reverse-dictionary.shtml).
new starting point s/he might choose coffee (since
s/he is looking for some kind of beverage, possibly
made from an ingredient produced in Java, coffee),
and finally mocha, a type of beverage made from
these beans. Of course, the word Java might just
as well trigger Kawa which not only rhymes with
the s
w
, but also evokes Kawa Igen, a javanese vol-
cano, or familiar word of coffee in French.
As one can see, this approach allows word ac-
cess via multiple routes: there are many ways lead-
ing to Rome. Also, while the distance covered
in our example is quite unusual, it is possible to
reach the goal quickly. It took us actually very
few moves, four, to find an indirect link, between
two, fairly remotely related terms: computer and
mocha. Of course, cyber-coffee fans might be even
quicker in reaching their goal.
3 The lexical matrix revisited
The main question that we are interested in here
is how, or in what terms, to index the dictionary
in order to allow for quick and intuitive access to
words. Access should be possible on the basis
of meaning (or meaning elements), various kinds
of associations (most prominently ?syntagmatic?
ones) and, more generally speaking, underspeci-
fied input. To this end we have started to build an
association matrix (henceforth AM), akin to, yet
different from G. Miller?s initial proposal of WN
(Miller et al, 1990). He suggested to build a lex-
ical matrix by putting on one axis all the forms,
i.e. words of the language, and on the other, their
corresponding meanings. The latter being defined
in terms of synsets. The corresponding meaning-
form relations are signaled via a boolean (pres-
ence/absence). Hence, looking at the intersection
of meanings and forms, one can see which mean-
ings are expressed by, or converge toward what
forms, or conversely, what form expresses which
meanings. Whether this is the way WN is actually
implemented is not clear to us, though we believe
that it is not. Anyhow, our approach is different,
and we hope the reader will understand in a mo-
ment the reasons why.
We will also put on one axis all the form ele-
ments, i.e. the lemmata or expressions of a given
language (we refer to them as target words, hence-
forth t
w
). On the other axis we will place the trig-
gers or access-words (henceforth a
w
), that is, the
words or concepts capable and likely to evoke the
t
w
. These are typically the kind of words psy-
11
chologists have gathered in their association ex-
periments (Jung and Riklin, 1906; Deese, 1965;
Schvaneveldt, 1989). Note, that instead of putting
a boolean value at the intersection of the t
w
and the
a
w
, we will put weights and the type of link hold-
ing between the co-occurring terms. This gives us
quadruplets. For example, an utterance like ?this
is the key of the door? might yield the a
w
(key),
the t
w
(door), the link type l
t
(part of), and a weight
(let?s say 15).
The fact that we have these two kinds of in-
formation is very important later on, as it allows
the search engine to cluster by type the possible
answers to be given in response to a user query
(word(s) provided as input) and to rank them.
Since the number of hits, i.e. words from which
the user must choose, may be substantial (depend-
ing on the degree of specification of the input), it is
important to group and rank them to ease naviga-
tion, allowing the user to find directly and quickly
the desired word, or at least the word with which
to continue search.
Obviously, different word senses (homographs),
require different entries (bank-money vs bank-
river), but so will synonyms, as every word-form,
synonym or not, is likely to be evoked by a differ-
ent key- or access-word (similarity of sound).
7
Also, we will need a new line for every different
relation between a a
w
and a t
w
. Whether more than
one line is needed in the case of identical links be-
ing expressed by different linguistic resources (the
lock of the door vs. the door?s lock vs. the door
has a lock) remains an open empirical question.
Let us see quickly how our AM is supposed
to work. Imagine you wanted to find the word
for the following concept: hat of a bishop. In
such a case, any of the following concepts or
words might come to your mind: church, Vati-
can, abbot, monk, monastery, ceremony, ribbon,
and of course rhyming words like: brighter, fighter,
lighter, righter, tighter, writer,
8
as, indeed, any of
them could remind us of the t
w
: mitre. Hence, all
of them are possible a
w
.
Once this resource is built, access is quite
straightforward. The user gives as input all the
words coming to his mind when thinking of a given
7
Take, for example, the nouns rubbish and garbage which
can be considered as synonyms. Yet, while the former may
remind you of a rabbit or (horse)-radish, the latter may evoke
the word cabbage.
8
The question, whether rhyming words should be com-
puted is not crucial at this stage.
idea or concept,
9
and the system will display all
connected words. If the user can find the item he
is looking for in this list, search stops, otherwise
it will continue, the user giving other words of the
list, or words evoked by them.
Of course, remains the question of how to build
this resource, in particular, how to populate the
axis devoted to the trigger words, i.e. access-
keys. At present we consider three approaches:
one, where we use the words occurring in word
definitions (see also, (Dutoit and Nugues, 2002;
Bilac et al, 2004)), the other is to mine a well-
balanced corpus, to find co-occurrences within a
given window (Ferret and Zock, 2006), the size
depending a bit on the text type (encyclopedia) or
type of corpus. Still another solution would be
to draw on the association lists produced by psy-
chologists, see for example http://www.usf.edu/, or
http://www.eat.rl.ac.uk.
Of course, the idea of using matrices in linguis-
tics is not new. There are at least two authors who
have proposed its use: M. Gross (Gross, 1984)
used it for coding the syntactic behavior of lex-
ical items, hence the term lexicon-grammar, and
G. Miller, the father of WN (Miller et al, 1990)
suggested it to support lexical access. While the
former work is not relevant for us here, Miller?s
proposal is. What are the differences between his
proposal and ours? There are basically four main
differences:
1. we use, collocations or access-words, i.e a
ws
rather than synsets; Hence, any of the follow-
ing a
ws
(cat, grey, computer device, cheese,
Speedy Gonzales) could point toward the t
w
?mouse?, none of them are part of the mean-
ing, leave alone synonyms.
2. we mark explicitly the weight and the type of
link between the t
w
and the a
w
(isa, part of,
etc.),
10
whereas WN uses only a binary value.
Both the weight and link are necessary infor-
mation for ranking and grouping, i.e. naviga-
tion.
3. our AM is corpus-sensitive (see below),
hence, we can, at least in principle, accommo-
9
The quantifier all shouldn?t be taken too literally. What
we have in mind are ?salient? words available in the speaker?s
mind at a given moment
10
Hence, if several links are possible between the t
w
and
the a
w
, several cells will be used. Think of the many possible
relations between a city and a country, example: Paris and
France (part of, biggest city of, located in, etc.)
12
date the fact that a speaker is changing topics,
adapting the weight of a given word or find a
more adequate a
w
in this new context. Think
of ?piano? in the contexts of a concert or mov-
ing your household. Only the latter would
evoke the notion of weight.
4. relying on a corpus, we can take advantage of
syntagmatic associations (often encyclopedic
knowledge), something which is difficult to
obtain for WN.
4 Keep the set of lexical candidates small
Here and in the next section we describe how the
idea of the AM has been computationally dealt
with. The goal is to reduce the number of hits,
i.e. possible t
ws
(output), as a function of the in-
put, i.e, the number of relevant a
ws
given by the
speaker/writer. To achieve this goal we apply lex-
ical functions to the a
ws
, considering the intersec-
tion of the obtained sets to be the relevant t
ws
.
4.1 Lexical Functions
The usefulness of lexical functions for linguistics
in general and for language production in particu-
lar has been shown by Mel??cuk (Mel??cuk, 1996).
We will use them here, as they seem to fit also our
needs of information extraction or lexical access.
Mel??cuk has coined the term lexical functions to
refer to the fact that two terms are systematically
related. For example, the lexical function Gener
refers to the fact that some term (let?s say ?cat?)
can be replaced by a more general term (let?s say
?animal?).
Lexical functions encode the combinability of
words. While ?big? and ?strong? express the same
idea (intensity, magnitude), they cannot be com-
bined freely with any noun: strong can be as-
sociated with fever, whereas big cannot. Of
course, this kind of combinability between lexical
terms is language specific, because unlike in En-
glish, in French one can say grosse fi`evre or forte
fi`evre, both being correct (Schwab and Lafourcade,
2007). Our AM handles, of course these kind of
functions. Here is a list of some of them:
- paradigmatic associations: hypernymy
(?cat? - ?animal?), hyponymy, synonymy, or
antonymy,. . . ;
- syntagmatic associations: collocations (?fear?
being associated with ?strong? or ?little?);
- morphological relations ie. terms being de-
rived from another part of speech: applying
the change-part-of-speech lexical function
f
cpos
to ?garden? will yield: f
cpos
(?garden?) =
{?to garden?, ?gardener?, . . .}
- sound-related items: homophones, rhymes.
4.2 Assumptions concerning search
The purpose of using lexical functions is to reduce
the number of possible outcomes from which the
user must choose. The list contains either the t
w
or another promising a
w
the user may want use to
continue search. Hence, lexical functions are use-
ful for search provided that:
1. the speaker/writer is able to specify the kind
of relations s/he wants to use. The problem
here lies in the nature and number of the func-
tions, some of them being very well specified,
while others are not.
2. the larger the number of trigger words the
smaller the list of words from which to
choose: the speaker/writer can add or delete
words to broaden or narrow the scope of
his/her query.
These hypotheses are being modeled by using
set properties of lexical functions. The idea is to
apply all functions, or a selection of them, to the
a
ws
and to give the speaker/writer the intersection
as result (see section 5.3.5 for an example)
5 Experiment
We have started with a simple, preliminary exper-
iment. Only one lexical function was used: neigh-
borhood (henceforth f
neig
). Let f
neig
be the func-
tion producing the set of co-occurring terms within
a given window (sentence or a paragraph).
11
The
result produced by the system and returned to the
user is the intersection of the application of f
neig
to the a
ws
. In the next section we explain how this
function is applied to two corpora (Wordnet and
Wikipedia), to show their respective qualities and
shortcomings for this specific task.
5.1 WordNet
5.1.1 Description
WordNet (henceforth WN) is a lexical database
for English developed under the guidance of G.
11
The scope or window size will vary with the text type
(normal text vs. encyclopedia). The optimal size is at this
point still an empirical question.
13
Miller (Miller et al, 1990). One of his goals was
to support lexical access akin to the human mind,
association-based. Knowledge is stored in a net-
work composed of nodes and links (nodes being
words or concepts and the links are the means of
connecting them) and access to knowledge, i.e.
search, takes place by entering the network at some
point and follow the links until one has reached the
goal (unless one has given up before). This kind
of navigation in a huge conceptual/lexical network
can be considered equivalent to spreading activa-
tion taking place in our brain.
Of course, such a network has to be built, and
navigational support must be provided to find the
location where knowledge or words are stored.
This is what Miller and his coworkers did by build-
ing WN. The resource has been built manually, and
it contains at present about 150.000 entries.
The structure of the dictionary is different from
conventional, alphabetical resources. Words are
organized in WN in two ways. Semantically sim-
ilar words, i.e. synonyms, are grouped as clus-
ters. These sets of synonyms, called synsets, are
then linked in various ways, depending on the
kind of relationship they entertain with the ad-
jacent synset. For example, their neighbors can
be more general or specific (hyperonymy vs. hy-
ponymy), they can be part of some reference ob-
ject (meronymy: car-motor), they can be the op-
posite (antonymy: hot-cold), etc. While WN is a
resource it can also be seen as a corpus.
5.1.2 Using WN as a corpus
There are many good reasons to use WN for
learning f
n
. For one, there are many extensions,
and second, the one we are using, eXtended WN
(Mihalcea and Moldovan, 2001) spares us the trou-
ble of having to address issues like: (a) seg-
mentation: we do not need to identify sentence
boundaries ; (b) semantic ambiguity: words being
tagged, we get good precision; (c) lemmatization:
since only verbs, nouns, adjectives and adverbs are
tagged, we need neither a stoplist nor a lemmatizer.
Despite all these qualities, two important prob-
lems remain nevertheless for this kind of corpus:
(a) size: though, all words are tagged, the cor-
pus remains small as it contains only 63.941 dif-
ferent words; (b) in consequence, the corpus lacks
many syntagmatic associations encoding encyclo-
pedic knowledge.
5.2 Using Wikipedia as corpus
Wikipedia is a free, multilingual encyclopedia, ac-
cessible on the Web.
12
For our experiment we have
chosen the English version which of this day (12th
of may 2008) contains 2,369,180 entries.
Wikipedia has exactly the opposite properties of
WN. While it covers well encyclopedic relations, it
is only raw text. Hence problems like text segmen-
tation, lemmatisation and stoplist definition need
to be addressed.
Our experiments with Wikipedia were very rudi-
mentary, given that we considered only 1000 doc-
uments. These latter were obtained in response to
the term ?wine?, by following the links obtained for
about 72.000 words.
5.3 Prototype
5.3.1 Building the resource and using it.
Building the resource requires processing a cor-
pus and building the database. Given a corpus
we apply our neighborhood function to a prede-
termined window (a paragraph in the case of ency-
clopedias).
13
The result, i.e. the co-occurrences,
will be stored in the database, together with their
weight, i.e. number of times two terms appear to-
gether, and the type of link. As mentionned above,
both kinds of information are needed later on for
ranking and navigation.
14
At present, cooccurences are stored as triplets
(t
w
, a
w
, times), where times represents the number
of times the two terms cooccur in the corpus, the
scope of coccurence being here the paragraph.
5.3.2 Processing of the Wikipedia page
For each Wikipedia page, a preprocessor
converts HTML pages into plain text. Next,
a part-of-speech tagger (http://www.ims.uni-
stuttgart.de/projekte/corplex/TreeTagger/) is used
to annotate all the words of the paragraph under
consideration. This allows the filtering of all
irrelevant words, to keep but a bag of words,
that is, the nouns, adjectives, verbs and adverbs
occuring in the paragraph. These words will be
used to fill the triplets of our database.
12
http://www.wikipedia.org
13
The optimal window-size depends probably on the text
type (encyclopedia vs. unformatted text). Yet, in the absence
of clear criteria, we consider the optimal window-size as an
open, empirical question.
14
This latter aspect is not implemented yet, but will be
added in the future, as it is a necessary component for easy
navigation (Zock and Bilac, 2004; Zock, 2006; Zock, 2007).
14
5.3.3 Corpus Building
We start arbitrarily from some page (for our ex-
periment, we have chosen ?wine? as input), apply
the algorithm outlined here above and pick then
randomly a noun within this page to fetch with this
input a new page on Wikipedia. This process is re-
peated until a given sample size is obtained (in our
case 1000 pages). Of course, instead of picking
randomly a noun, we could have decided to pro-
cess all the nouns of a given page, and to add then
incrementally the nouns of the next pages. Yet,
doing this would have led us to privilege a specific
topic (in our case ?wine?) instead of a more general
one.
5.3.4 Usage
We have developed a website in Java as a
servlet. Interactions with humans are simple: peo-
ple can add or delete a word from the current list
(see Input in the figure on top of the next page).
The example presented shows that with very few
words, hence very quickly, we can obtain the de-
sired word.
Given some input, the system provides the user
with a list of words cooccuring with the a
ws
. The
output is an ordered list of words, the order de-
pending on the overall score, i.e. number of cooc-
currences between the a
w
and the t
w
. For exam-
ple, if the a
ws
?wine? and ?harvest? co-occur with
the t
w
?bunch? respectively 5 and 8 times, then
the overall score of cooccurence of ?bunch? is 13:
((wine, harvest), bunch, 13). Hence, all words with
a higher score will precede it, while those with a
lower score will follow it.
5.3.5 Examples and Comparison of the
results of the two corpora
Here below are the examples extracted from the
WN corpus (see figure-1). Our goal was to find
the word ?vintage?. Trigger words are ?wine? and
?harvest?, yielding respectively 488 and 30 hits, i.e.
words. As one can see ?harvest? is a better ac-
cess term than ?wine?. Combining the two will re-
duce the list to 6 items. Please note that the t
w
?vintage? is not among them, eventhough it exists
in WordNet, which illustrates nicely the fact that
storage does not guarantee accessibility (Sinopal-
nikova and Smrz, 2006).
Looking at figure-1 you will see that the results
have improved considerably with Wikipedia. The
same input, ?wine? evokes many more words (1845
as opposed to 488). For ?harvest? we get 983 hits in-
Input WordNet Wikipedia
488 words 1845
words
grape sweet aloholic country
serve france god characteristics
wine small fruit regulation grape
dry bottle appellation system
produce red bottled like
bread hold christian track
. . . . . . . . . . . .
30 words 983 words
month fish produce grain
grape revolutionary autumn farms
calendar festival energy cut
harvest butterfish dollar combine ground
person make balance rain
wine first amount rich
. . . . . . . . . . . .
6 words 45 words
make grape grape vintage
wine fish someone bottle produce
+harvest commemorate person fermentation juice
. . . . . . Beaujolais taste
viticulture France
Bordeaux vineyard
. . . . . .
Figure 1: Comparing two corpora (eXtended
WordNet and Wikipedia) with various inputs
stead of 30 (the intersection containing 62 words).
Combining the two reduces the set to 45 items
among which we will find, of course, the target
word.
We hope that this example is clear enough to
convince the reader that it makes sense to use real
text as corpus to extract from it the kind of in-
formation (associations) people are likely to give
when looking for a word.
6 Conclusion and perspectives
We have addressed in this paper the problem of
word finding for speakers or writers. Conclud-
ing that most dictionaries are not well suited to al-
low for this kind of reverse access based on mean-
ings (or meaning related elements, associations),
we looked at work done by psychologists to get
some inspiration. Next we tried to clarify which
of these findings could help us build the dictionary
of tomorrow, that is, a tool integrating linguistic
and encyclopedic knowledge, allowing navigation
by taking either or as starting point. While lin-
guistic knowledge is more prominent for analysis
(reading), encyclopedic facts are more relevant for
production. We?ve presented then our ideas of how
to build a resource, allowing lexical access based
15
on underspecified, i.e. imperfect input. To achieve
this goal we?ve started building an AM composed
of form elements (the words and expressions of
a given language) and a
ws
. The role of the lat-
ter being to lead to or to evoke the t
w
. In the last
part we?ve described briefly the results obtained by
comparing two resources (WN and Wikipedia) and
various inputs. Given the fact that the project is
still quite young, only preliminary results can be
shown at this point.
Our next steps will be to take a closer look at the
following work: clustering of similar words (Lin,
1998), topic signatures (Lin and Hovy, 2000) and
Kilgariff?s sketch engine (Kilgarriff et al, 2004).
We plan also to add other lexical functions to en-
rich our database with a
ws
. We plan to experiment
with corpora, trying to find out which ones are best
for our purpose
15
and we will certainly experiment
with the window size
16
to see which size is best
for which text type. Finally, we plan to insert in
our AM the relations holding between the a
w
and
the t
w
. As these links are contained in our corpus,
we should be able to identify and type them. The
question is, to what extent this can be done auto-
matically.
Obviously, the success of our resource will de-
pend on the quality of the corpus, the quality of
the a
ws
, weights and links, and the representativ-
ity of all this for a given population. While we do
believe in the justification of our intuitions, more
work is needed to reveal the true potential of the
approach. The ultimate judge being, of course, the
future user.
15
For example, we could consider a resource like Con-
ceptNet of the Open Mind Common-Sense project (Liuh and
Singh, 2004).
16
For example, it would have been interesting to consider
coocurrences beyond the scope of the paragraph, by consider-
ing the logical structure of the Wikipedia document. Anyhow,
our experiment needs to be redone with more data than just
1000 pages, the size chosen here for lack of time. Indeed one
could consider using the entire corpus of Wikipedia or mixed
corpora
References
Aitchinson, Jean. 2003. Words in the Mind: an Intro-
duction to the Mental Lexicon (3d edition). Black-
well, Oxford.
Bilac, S., W. Watanabe, T. Hashimoto, T. Tokunaga,
and H. Tanaka. 2004. Dictionary search based
on the target word description. In Proc. of the
Tenth Annual Meeting of The Association for NLP
(NLP2004), pages 556?559, Tokyo, Japan.
Boissi`ere, P. 1862. Dictionnaire analogique de la
langue franc?aise : r?epertoire complet des mots par
les id?ees et des id?ees par les mots. Larousse et A.
Boyer, Paris.
Brown, R. and D. McNeill. 1996. The tip of the tounge
phenomenon. Journal of Verbal Learning and Ver-
bal Behaviour, 5:325?337.
Cutler, A, editor, 1982. Slips of the Tongue and Lan-
guage Production. Mouton, Amsterdam.
Deese, James. 1965. The structure of associations in
language and thought. Johns Hopkins Press.
Dong, Zhendong and Qiang Dong. 2006. HOWNET
and the computation of meaning. World Scientific,
London.
Dutoit, Dominique and P. Nugues. 2002. A lexical
network and an algorithm to find words from defini-
tions. In van Harmelen, F., editor, ECAI2002, Proc.
of the 15th European Conference on Artificial Intel-
ligence, pages 450?454, Lyon. IOS Press, Amster-
dam.
Fellbaum, Christiane, editor, 1998. WordNet: An Elec-
tronic Lexical Database and some of its Applica-
tions. MIT Press.
Ferret, Olivier and Michael Zock. 2006. Enhancing
electronic dictionaries with an index based on associ-
ations. In ACL ?06: Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the ACL, pages 281?288.
Fontenelle, Thierry. 1997. Using a bilingual dictionary
to create semantic networks. International Journal
of Lexicography, 10(4):275?303.
Goddard, Cliff. 1998. Bad arguments against seman-
tic primitives. Theoretical Linguistics, 24(2-3):129?
156.
16
Gross, Maurice. 1984. Lexicon-grammar and the anal-
ysis of french. In Proc. of the 11th COLING, pages
275?282, Stanford, CA.
Harley, Trevor. 2004. The psychology of language.
Psychology Press, Taylor and Francis, Hove and
New York.
Jung, Carl and F. Riklin. 1906. Experimentelle
Untersuchungen ?uber Assoziationen Gesunder. In
Jung, C. G., editor, Diagnostische Assoziationsstu-
dien, pages 7?145. Barth, Leipzig, Germany.
Kilgarriff, Adam, Pavel Rychl?y, Pavel Smr?z, and David
Tugwell. 2004. The Sketch Engine. In Proceedings
of the Eleventh EURALEX International Congress,
pages 105?116, Lorient, France.
Levelt, Willem. 1996. A theory of lexical access
in speech production. In Proc. of the 16th Con-
ference on Computational Linguistics, Copenhagen,
Denmark.
Lin, Chin-Yew and Eduard H. Hovy. 2000. The auto-
mated acquisition of topic signatures for text summa-
rization. In COLING, pages 495?501. Morgan Kauf-
mann.
Lin, Dekang. 1998. Automatic retrieval and clustering
of similar words. In COLING-ACL, pages 768?774,
Montreal.
Liuh, H. and P. Singh. 2004. ConceptNet: a practi-
cal commonsense reasoning toolkit. BT Technology
Journal.
Mel??cuk, I., N. Arbatchewsky-Jumarie, L. Iordanskaja,
S. Mantha, and A. Polgu`ere. 1999. Dictionnaire
explicatif et combinatoire du franc?ais contemporain
Recherches lexico-s?emantiques IV. Les Presses de
l?Universit?e de Montr?eal, Montr?eal.
Mel??cuk, Igor. 1996. Lexical functions: A tool for
the description of lexical relations in the lexicon. In
Wanner, L., editor, Lexical Functions in Lexicogra-
phy and Natural Language Processing, pages 37?
102. Benjamins, Amsterdam/Philadelphia.
Mihalcea, Rada and Dan Moldovan. 2001. Extended
WordNet: progress report. In NAACL 2001 - Work-
shop on WordNet and Other Lexical Resources, Pitts-
burgh, USA.
Miller, G. A., R. Beckwith, C. Fellbaum, D. Gross, and
K. Miller. 1990. Introduction to WordNet: An on-
line lexical database. International Journal of Lexi-
cography, 3(4), pages 235?244.
Moerdijk, Fons. 2008. Frames and semagrams; Mean-
ing description in the general dutch dictionary. In
Proceedings of the Thirteenth Euralex International
Congress, EURALEX, Barcelona.
Polgu`ere, Alain. 2006. Structural properties of lexi-
cal systems: Monolingual and multilingual perspec-
tives. Sidney. Coling workshop ?Multilingual Lan-
guage Resources and Interoperability?.
Richardson, S., W. Dolan, and L. Vanderwende. 1998.
Mindnet: Acquiring and structuring semantic infor-
mation from text. In ACL-COLING?98, pages 1098?
1102.
Robert, Paul, Alain Rey, and J. Rey-Debove. 1993.
Dictionnaire alphabetique et analogique de la
Langue Franc?aise. Le Robert, Paris.
Roget, P. 1852. Thesaurus of English Words and
Phrases. Longman, London.
Schvaneveldt, R., editor, 1989. Pathfinder Associa-
tive Networks: studies in knowledge organization.
Ablex, Norwood, New Jersey, US.
Schwab, Didier and Mathieu Lafourcade. 2007. Mod-
elling, detection and exploitation of lexical functions
for analysis. ECTI Transactions Journal on Com-
puter and Information Technology, 2(2):97?108.
Sierra, Gerardo. 2000. The onomasiological dictio-
nary: a gap in lexicography. In Proceedings of the
Ninth Euralex International Congress, pages 223?
235, IMS, Universit?at Stuttgart.
Sinopalnikova, Anna and Pavel Smrz. 2006. Knowing
a word vs. accessing a word: Wordnet and word as-
sociation norms as interfaces to electronic dictionar-
ies. In Proceedings of the Third International Word-
Net Conference, pages 265?272, Korea.
Summers, Della. 1993. Language Activator: the
world?s first production dictionary. Longman, Lon-
don.
T?ong, Ting-K?u. 1862. Ying ?u tsap ts??un (The Chinese
and English Instructor). Canton.
Wierzbicka, Anna. 1996. Semantics: Primes and Uni-
versals. Oxford University Press, Oxford.
Wilks, Yorick. 1977. Good and bad arguments about
semantic primitives. Communication and Cognition,
10(3?4):181?221.
Zock, Michael and Slaven Bilac. 2004. Word lookup
on the basis of associations : from an idea to a
roadmap. In Workshop on ?Enhancing and using
electronic dictionaries?, pages 29?35, Geneva. COL-
ING.
Zock, Michael. 2006. Navigational aids, a critical
factor for the success of electronic dictionaries. In
Rapp, Reinhard, P. Sedlmeier, and G. Zunker-Rapp,
editors, Perspectives on Cognition: A Festschrift for
Manfred Wettler, pages 397?414. Pabst Science Pub-
lishers, Lengerich.
Zock, Michael. 2007. If you care to find what you
are looking for, make an index: the case of lexical
access. ECTI, Transaction on Computer and Infor-
mation Technology, 2(2):71?80.
17
