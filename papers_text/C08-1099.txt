Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 785?792
Manchester, August 2008
Toward a Psycholinguistically-Motivated Model of Language Processing
William Schuler
Computer Science and Engineering
University of Minnesota
schuler@cs.umn.edu
Samir AbdelRahman
Department of Computer Science
Cairo University
s.abdelrahman@fci-cu.edu.eg
Tim Miller
Computer Science and Engineering
University of Minnesota
tmill@cs.umn.edu
Lane Schwartz
Computer Science and Engineering
University of Minnesota
lschwar@cs.umn.edu
Abstract
Psycholinguistic studies suggest a model
of human language processing that 1) per-
forms incremental interpretation of spo-
ken utterances or written text, 2) preserves
ambiguity by maintaining competing anal-
yses in parallel, and 3) operates within
a severely constrained short-term memory
store ? possibly constrained to as few
as four distinct elements. This paper de-
scribes a relatively simple model of lan-
guage as a factored statistical time-series
process that meets all three of the above
desiderata; and presents corpus evidence
that this model is sufficient to parse natu-
rally occurring sentences using human-like
bounds on memory.
1 Introduction
Psycholinguistic studies suggest a model of human
language processing with three important proper-
ties. First, eye-tracking studies (Tanenhaus et al,
1995; Brown-Schmidt et al, 2002) suggest that hu-
mans analyze sentences incrementally, assembling
and interpreting referential expressions even while
they are still being pronounced. Second, humans
appear to maintain competing analyses in paral-
lel, with eye gaze showing significant attention to
competitors (referents of words with similar pre-
fixes to the correct word), even relatively long af-
ter the end of the word has been encountered, when
attention to other distractor referents has fallen off
(Dahan and Gaskell, 2007). Preserving ambigu-
ity in a parallel, non-deterministic search like this
may account for human robustness to missing, un-
known, mispronounced, or misspelled words. Fi-
nally, studies of short-term memory capacity sug-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
gest human language processing operates within a
severely constrained short-term memory store ?
possibly restricted to as few as four distinct ele-
ments (Miller, 1956; Cowan, 2001).
The first two observations may be taken to
endorse existing probabilistic beam-search mod-
els which maintain multiple competing analyses,
pruned by contextual preferences and dead ends
(e.g. Roark, 2001). But the last observation on
memory bounds imposes a restriction that until
now has not been evaluated in a corpus study. Can
a simple, useful human-like processing model be
defined using these constraints? This paper de-
scribes a relatively simple model of language as a
factored statistical time-series process that meets
all three of the above desiderata; and presents
corpus evidence that this model is sufficient to
parse naturally occurring sentences using human-
like bounds on memory.
The remainder of this paper is organized as fol-
lows: Section 2 describes some current approaches
to incremental parsing; Section 3 describes a statis-
tical framework for parsing using a bounded stack
of explicit constituents; Section 4 describes an ex-
periment to estimate the level of coverage of the
Penn Treebank corpus that can be achieved with
various stack memory limits, using a set of re-
versible tree transforms, and gives accuracy results
of a bounded-memory model trained on this cor-
pus.
2 Background
Much work on cognitive modeling in psycholin-
guistics is centered on modeling the concepts to
which utterances refer. Coarsely, these concepts
may correspond to activation patterns among neu-
rons in specific regions of the brain. In some the-
ories, a short-term memory store of several unre-
lated concepts may be retained by organizing the
activation of these concepts into compatible pat-
terns, only a few of which can be reliably main-
785
tained (Smolensky and Legendre, 2006). Activa-
tion is then theorized to spread through and among
these groups of concepts in proportion to some
learned probability that the concepts will be rel-
evant (Anderson and Reder, 1999), with the most
active concepts corresponding to the most likely
linguistic analyses. Competition between rival ac-
tivated groups of concepts (corresponding to in-
complete linguistic analyses) has even been linked
to reading delays (Hale, 2003).
This competition among mutually-exclusive
variously-activated short term memory stores of
concepts, essentially a weighted disjunction over
conjunctions of concepts, can be modeled in lan-
guage understanding as simple Viterbi decoding of
a factored HMM-like time-series model (Schuler
et al, in press). In this model, concepts (corre-
sponding to vectors of individuals in a first-order
world model) are introduced and composed (via
set operations like intersection) in each hypothe-
sized short-term memory store, using the elements
of the memory store as a stack. These vectors of
individuals can be considered a special case of vec-
tors of concept elements proposed by Smolensky,
with set intersection a special case of tensor prod-
uct in the composition model. Referents in this
kind of incremental model can be constrained by
? but still distinguished from ? higher-level ref-
erents while they are still being recognized.
It is often assumed that this semantic con-
cept composition proceeds isomorphically with
the composition of syntactic constituents (Frege,
1892). This parallel semantic and syntactic com-
position is considered likely to be performed in
short-term memory because it has many of the
characteristics of short-term memory processes,
including nesting limits (Miller and Chomsky,
1963) and susceptibility to degradation due to in-
terruption. Ericsson and Kintch (1995) propose a
theory of long-term working memory that extends
short-term memory, but only for inter-sentential
references, which do seem to be retained across
interruptions in reading. But while the relation-
ship between competing probability distributions
in such a model and experimental reading times
has been evaluated (e.g. by Hale), the relationship
between the syntactic demands on a short-term
memory store and observations of human short-
term memory limits is still largely untested. Sev-
eral models have been proposed to perform syntac-
tic analysis using a bounded memory store.
For example, Marcus (1980) proposed a deter-
ministic parser with an explicit four-element work-
ing memory store in order to model human parsing
limitations. But this model only stores complete
constituents (whereas the model proposed in this
paper stores incompletely recognized constituents,
in keeping with the Tanenhaus et al findings). As
a result, the Marcus model relies on a suite of spe-
cialized memory operations to compose complete
constituents out of complete constituents, which
are not independently cognitively motivated.
Cascaded finite-state automata, as in FASTUS
(Hobbs et al, 1996), also make use of a bounded
stack, but stack levels in such systems are typically
dedicated to particular syntactic operations: e.g.
a word group level, a phrasal level, and a clausal
level. As a result, some amount of constituent
structure may overflow its dedicated level, and be
sacrificed (for example, prepositional phrase at-
tachment may be left underspecified).
Finite-state equivalent parsers (and thus,
bounded-stack parsers) have asymptotically linear
run time. Other parsers (Sagae and Lavie, 2005)
have achieved linear runtime complexity with
unbounded stacks in incremental parsing by
using a greedy strategy, pursuing locally most
probable shift or reduce operations, conditioned
on multiple surrounding words. But without an
explicit bounded stack it is difficult to connect
these models to concepts in a psycholinguistic
model.
Abney and Johnson (1991) explore left-corner
parsing as a memory model, but again only in
terms of (complete) syntactic constituents. The
approach explored here is similar, but the trans-
form is reversed to allow the recognizer to store
recognized structure rather than structures being
sought, and the transform is somewhat simpli-
fied to allow more structure to be introduced into
syntactic constituents, primarily motivated by a
need to keep track of disconnected semantic con-
cepts rather than syntactic categories. Without this
link to disconnected semantic concepts, the syntax
model would be susceptible to criticism that the
separate memory levels could be simply chunked
together through repeated use (Miller, 1956).
Roark?s (2001) top-down parser generates trees
incrementally in a transformed representation re-
lated to that used in this paper, but requires dis-
tributions to be maintained over entire trees rather
than stack configurations. This increases the beam
786
width necessary to avoid parse failure. Moreover,
although the system is conducting a beam search,
the objects in this beam are growing, so the recog-
nition complexity is not linear, and the connection
to a bounded short-term memory store of uncon-
nected concepts becomes somewhat complicated.
The model described in this paper is arguably
simpler than many of the models described above
in that it has no constituent-specific mechanisms,
yet it is able to recognize the rich syntactic struc-
tures found in the Penn Treebank, and is still
compatible with the psycholinguistic notion of a
bounded short-term memory store of conceptual
referents.
3 Bounded-Memory Parsing with a Time
Series Model
This section describes a basic statistical framework
? a factored time-series model ? for recogniz-
ing hierarchic structures using a bounded store of
memory elements, each with a finite number of
states, at each time step. Unlike simple FSA com-
pilation, this model maintains an explicit represen-
tation of active, incomplete phrase structure con-
stituents on a bounded stack, so it can be readily
extended with additional variables that depend on
syntax (e.g. to track hypothesized entities or rela-
tions). These incomplete constituents are related
to ordinary phrase structure annotations through a
series of bidirectional tree transforms. These trans-
forms:
1. binarize phrase structure trees into linguisti-
cally motivated head-modifier branches (de-
scribed in Section 3.1);
2. transform right-branching sequences to left-
branching sequences (described in Sec-
tion 3.2); and
3. align transformed trees to an array of random
variable values at each depth and time step of
a probabilistic time-series model (described
in Section 3.3).
Following these transforms, a model can be trained
from example trees, then run as a parser on unseen
sentences. The transforms can then be reversed to
evaluate the output of the parser. This representa-
tion will ultimately be used to evaluate the cover-
age of a bounded-memory model on a large corpus
of tree-annotated sentences, and to evaluate the ac-
curacy of a basic (unsmoothed, unlexicalized) im-
plementation of this model in Section 4.
It is important to note that these transformations
are not postulated to be part of the human recog-
nition process. In this model, sentences can be
recognized and interpreted entirely in right-corner
form. The transforms only serve to connect this
process to familiar representations of phrase struc-
ture.
3.1 Binary branching structure
This paper will attempt to draw conclusions about
the syntactic complexity of natural language, in
terms of stack memory requirements in incremen-
tal (left-to-right) recognition. These requirements
will be minimized by recognizing trees in a right-
corner form, which accounts partially recognized
phrases and clauses as incomplete constituents,
lacking one instance of another constituent yet to
come.
In particular, this study will use the trees in the
Penn Treebank Wall Street Journal (WSJ) corpus
(Marcus et al, 1994) as a data set. In order to
obtain a linguistically plausible right-corner trans-
form representation of incomplete constituents, the
corpus is subjected to another, pre-process trans-
form to introduce binary-branching nonterminal
projections, and fold empty categories into non-
terminal symbols in a manner similar to that pro-
posed by Johnson (1998b) and Klein and Manning
(2003). This binarization is done in such a way
as to preserve linguistic intuitions of head projec-
tion, so that the depth requirements of right-corner
transformed trees will be reasonable approxima-
tions to the working memory requirements of a hu-
man reader or listener.
3.2 Right-Corner Transform
Phrase structure trees are recognized in this frame-
work in a right-corner form that can be mapped to
and from ordinary phrase structure via reversible
transform rules, similar to those described by
Johnson (1998a). This transformed grammar con-
strains memory usage in left-to-right traversal to a
bound consistent with the psycholinguistic results
described above.
This right-corner transform is simply the left-
right dual of a left-corner transform (Johnson,
1998a). It transforms all right branching sequences
in a phrase structure tree into left branching se-
quences of symbols of the form A
1
/A
2
, denoting
an incomplete instance of category A
1
lacking an
instance of category A
2
to the right. These incom-
plete constituent categories have the same form
787
a) binarized phrase structure tree:
S
NP
NP
JJ
strong
NN
demand
PP
IN
for
NP
NPpos
NNP
NNP
new
NNP
NNP
york
NNP
city
POS
?s
NNS
JJ
general
NNS
NN
obligation
NNS
bonds
VP
VBN
VBN
propped
PRT
up
NP
DT
the
NN
JJ
municipal
NN
market
b) result of right-corner transform:
S
S/NN
S/NN
S/NP
S/VP
NP
NP/NNS
NP/NNS
NP/NNS
NP/NP
NP/PP
NP
NP/NN
JJ
strong
NN
demand
IN
for
NPpos
NPpos/POS
NNP
NNP/NNP
NNP/NNP
NNP
new
NNP
york
NNP
city
POS
?s
JJ
general
NN
obligation
NNS
bonds
VBN
VBN/PRT
VBN
propped
PRT
up
DT
the
JJ
municipal
NN
market
Figure 1: Trees resulting from a) a binarization of a sample phrase structure tree for the sentence Strong
demand for New York City?s general obligations bonds propped up the municipal market, and b) a right-
corner transform of this binarized tree.
and much of the same meaning as non-constituent
categories in a Combinatorial Categorial Grammar
(Steedman, 2000).
Rewrite rules for the right-corner transform are
shown below, first to flatten out right-branching
structure:1
1The tree transforms presented in this paper will be de-
fined in terms of destructive rewrite rules applied iteratively
to each constituent of a source tree, from leaves to root, and
from left to right among siblings, to derive a target tree. These
rewrites are ordered; when multiple rewrite rules apply to the
same constituent, the later rewrites are applied to the results
of the earlier ones. For example, the rewrite:
A
0
. . . A
1
?
2
?
3
. . .
?
A
0
. . . ?
2
?
3
. . .
could be used to iteratively eliminate all binary-branching
nonterminal nodes in a tree, except the root. In the notation
used in this paper, Roman uppercase letters (A
i
) are variables
matching constituent labels, Roman lowercase letters (a
i
) are
variables matching terminal symbols, Greek lowercase letters
A
1
?
1
A
2
?
2
A
3
a
3
?
A
1
A
1
/A
2
?
1
A
2
/A
3
?
2
A
3
a
3
A
1
?
1
A
2
A
2
/A
3
?
2
. . .
?
A
1
A
1
/A
2
?
1
A
2
/A
3
?
2
. . .
then to replace it with left-branching structure:
(?
i
) are variables matching entire subtree structure, Roman
letters followed by colons, followed by Greek letters (A
i
:?
i
)
are variables matching the label and structure, respectively, of
the same subtree, and ellipses (. . . ) are taken to match zero
or more subtree structures, preserving the order of ellipses in
cases where there are more than one (as in the rewrite shown
above).
788
A1
A
1
/A
2
:?
1
A
2
/A
3
?
2
?
3
. . .
?
A
1
A
1
/A
3
A
1
/A
2
:?
1
?
2
?
3
. . .
Here, the first two rewrite rules are applied iter-
atively (bottom-up on the tree) to flatten all right
branching structure, using incomplete constituents
to record the original nonterminal ordering. The
third rule is then applied to generate left-branching
structure, preserving this ordering. Note that the
last rewrite above leaves a unary branch at the left-
most child of each flattened node. This preserves
the nodes at which the original tree was not right-
branching, so the original tree can be reconstructed
when the right-corner transform concatenates mul-
tiple right-branching sequences into a single left-
branching sequence.
An example of a right-corner transformed tree
is shown in Figure 1(b). An important property of
this transform is that it is reversible. Rewrite rules
for reversing a right-corner transform are simply
the converse of those shown above. The correct-
ness of this can be demonstrated by dividing a
tree into maximal sequences of right branches (that
is, maximal sequences of adjacent right children).
The first two ?flattening? rewrites of the right-
corner transform, applied to any such sequence,
will replace the right-branching nonterminal nodes
with a flat sequence of nodes labeled with slash
categories, which preserves the order of the non-
terminal category symbols in the original nodes.
Reversing this rewrite will therefore generate the
original sequence of nonterminal nodes. The final
rewrite similarly preserves the order of these non-
terminal symbols while grouping them from the
left to the right, so reversing this rewrite will re-
produce the original version of the flattened tree.
3.3 Hierarchic Hidden Markov Models
Right-corner transformed phrase structure trees
can then be mapped to random variable positions
in a Hierarchic Hidden Markov Model (Murphy
and Paskin, 2001), essentially a Hidden Markov
Model (HMM) factored into some fixed number of
stack levels at each time step.
HMMs characterize speech or text as a sequence
of hidden states q
t
(in this case, stacked-up syn-
tactic categories) and observed states o
t
(in this
case, words) at corresponding time steps t. A
most likely sequence of hidden states q?
1..T
can
then be hypothesized given any sequence of ob-
served states o
1..T
, using Bayes? Law (Equation 2)
and Markov independence assumptions (Equa-
tion 3) to define a full P(q
1..T
| o
1..T
) probabil-
ity as the product of a Transition Model (?
A
)
prior probability P(q
1..T
)
def
=
?
t
P
?
A
(q
t
| q
t-1
) and
an Observation Model (?
B
) likelihood probability
P(o
1..T
| q
1..T
)
def
=
?
t
P
?
B
(o
t
| q
t
):
q?
1..T
= argmax
q
1..T
P(q
1..T
| o
1..T
) (1)
= argmax
q
1..T
P(q
1..T
)?P(o
1..T
| q
1..T
) (2)
def
= argmax
q
1..T
T
?
t=1
P
?
A
(q
t
| q
t-1
)?P
?
B
(o
t
| q
t
) (3)
Transition probabilities P
?
A
(q
t
| q
t-1
) over com-
plex hidden states q
t
can be modeled using syn-
chronized levels of stacked-up component HMMs
in a Hierarchic Hidden Markov Model (HHMM)
(Murphy and Paskin, 2001). HHMM transition
probabilities are calculated in two phases: a re-
duce phase (resulting in an intermediate, marginal-
ized state f
t
), in which component HMMs may ter-
minate; and a shift phase (resulting in a modeled
state q
t
), in which unterminated HMMs transition,
and terminated HMMs are re-initialized from their
parent HMMs. Variables over intermediate f
t
and
modeled q
t
states are factored into sequences of
depth-specific variables ? one for each of D levels
in the HMM hierarchy:
f
t
= ?f
1
t
. . . f
D
t
? (4)
q
t
= ?q
1
t
. . . q
D
t
? (5)
Transition probabilities are then calculated as a
product of transition probabilities at each level, us-
ing level-specific reduce ?R and shift ?S models:
P
?
A
(q
t
|q
t-1
) =
?
f
t
P(f
t
|q
t-1
)?P(q
t
|f
t
q
t-1
) (6)
def
=
?
f
1..D
t
D
?
d=1
P
?R(f
d
t
|f
d+1
t
q
d
t-1
q
d-1
t-1
)?
P
?S(q
d
t
|f
d+1
t
f
d
t
q
d
t-1
q
d-1
t
) (7)
with fD+1
t
and q0
t
defined as constants. In Viterbi
decoding, the sums are replaced with argmax oper-
ators. This decoding process preserves ambiguity
by maintaining competing analyses of the entire
memory store. A graphical representation of an
HHMM with three levels is shown in Figure 3.
Shift and reduce probabilities can then be de-
fined in terms of finitely recursive Finite State Au-
tomata (FSAs) with probability distributions over
789
d=1
d=2
d=3
word
t=1 t=2 t=3 t=4 t=5 t=6 t=7 t=8 t=9 t=10 t=11 t=12 t=13 t=14 t=15
strong
dem
and
for
new
york
city ?s
general
obligations
bonds
propped
up the
m
unicipal
m
arket
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
? ? ? ?
NNP/NNP
NNP/NNP
NPpos/POS
? ? ? ?
VBN/PRT
? ? ?
?
NP/NN
NP/PP
NP/NP
NP/NP
NP/NP
NP/NP
NP/NNS
NP/NNS
NP/NNS
S/VP
S/VP
S/NP
S/NN
S/NN
Figure 2: Sample tree from Figure 1 mapped to qd
t
variable positions of an HHMM at each stack depth
d (vertical) and time step t (horizontal). This tree uses only two levels of stack memory. Values for
final-state variables fd
t
are not shown. Note that some nonterminal labels have been omitted; labels for
these nodes can be reconstructed from their children.
transition, recursive expansion, and final-state sta-
tus of states at each hierarchy level. In simple HH-
MMs, each intermediate variable is a boolean vari-
able over final-state status fd
t
? {0,1} and each
modeled state variable is a syntactic, lexical, or
phonetic state qd
t
. The intermediate variable fd
t
is
true or false (equal to 1 or 0 respectively) accord-
ing to ?F-Reduce if there is a transition at the level
immediately below d, and false (equal to 0) with
probability 1 otherwise:2
P
?R(f
d
t
| f
d+1
t
q
d
t-1
q
d-1
t-1
)
def
=
{
if fd+1
t
=0 : [f
d
t
=0]
if fd+1
t
=1 : P
?F-Reduce(f
d
t
| q
d
t-1
, q
d-1
t-1
)
(8)
where fD+1
t
= 1 and q0
t
= ROOT.
Shift probabilities over the modeled variable qd
t
at each level are defined using level-specific tran-
sition ?Q-Trans and expansion ?Q-Expand models:
P
?S(q
d
t
| f
d+1
t
f
d
t
q
d
t-1
q
d-1
t
)
def
=
?
?
?
if fd+1
t
=0, f
d
t
=0 : [q
d
t
= q
d
t-1
]
if fd+1
t
=1, f
d
t
=0 : P
?Q-Trans(q
d
t
| q
d
t-1
q
d-1
t
)
if fd+1
t
=1, f
d
t
=1 : P
?Q-Expand(q
d
t
| q
d-1
t
)
(9)
where fD+1
t
= 1 and q0
t
= ROOT. This model
is conditioned on final-state switching variables at
and immediately below the current FSA level. If
there is no final state immediately below the cur-
rent level (the first case above), it deterministically
copies the current FSA state forward to the next
time step. If there is a final state immediately be-
low the current level (the second case above), it
2Here [?] is an indicator function: [?] = 1 if ? is true, 0
otherwise.
. . .
. . .
. . .
. . .
f
3
t?1
f
2
t?1
f
1
t?1
q
1
t?1
q
2
t?1
q
3
t?1
o
t?1
f
3
t
f
2
t
f
1
t
q
1
t
q
2
t
q
3
t
o
t
Figure 3: Graphical representation of a Hierarchic
Hidden Markov Model. Circles denote random
variables, and edges denote conditional dependen-
cies. Shaded circles are observations.
transitions the FSA state at the current level, ac-
cording to the distribution ?Q-Trans. And if the state
at the current level is final (the third case above),
it re-initializes this state given the state at the level
above, according to the distribution ?Q-Expand. The
overall effect is that higher-level FSAs are allowed
to transition only when lower-level FSAs termi-
nate. An HHMM therefore behaves like a prob-
abilistic implementation of a pushdown automaton
(or shift?reduce parser) with a finite stack, where
the maximum stack depth is equal to the number
of levels in the HHMM hierarchy.
Figure 2 shows the transformed tree from Fig-
ure 1 aligned to HHMM depth levels and time
steps. Because it uses a bounded stack, recognition
in this model is asymptotically linear (Murphy and
Paskin, 2001).
This model recognizes right-corner transformed
trees constrained to a stack depth corresponding to
observed human short term memory limits. This
790
HHMM depth limit sentences coverage
no memory 127 0.32%
1 memory element 3,496 8.78%
2 memory elements 25,909 65.05%
3 memory elements 38,902 97.67%
4 memory elements 39,816 99.96%
5 memory elements 39,832 100.00%
TOTAL 39,832 100.00%
Table 1: Percent coverage of right-corner trans-
formed treebank sections 2?21 with punctuation
omitted, using HHMMs with depth limits D from
zero to five.
is an attractive model of human language process-
ing because the incomplete syntactic constituents
it stores at each stack depth can be directly associ-
ated with (incomplete) semantic referents, e.g. by
adding random variables over environment or dis-
course referents at each depth and time step. If
these referents are calculated incrementally, recog-
nition decisions can be informed by the values of
these variables in an interactive model of language,
following Tanenhaus et al (1995). The corpus re-
sults described in the next section suggest that a
large majority of naturally occurring sentences can
be recognized using only three or four stack mem-
ory elements via this transform.
4 Empirical Results
In order to evaluate the coverage of this bounded-
memory model, Sections 2?21 of the Penn Tree-
bank WSJ corpus were transformed and mapped
to HHMM variables as described in Section 3.3. In
order to counter possible undesirable effects of an
arbitrary branching analysis of punctuation, punc-
tuation was removed. Coverage results on this cor-
pus are shown in Table 1.
Experiments training on transformed trees from
Sections 2?21 of the WSJ Treebank, evaluating
reversed-transformed output sequences from Sec-
tion 22 (development set) and Section 23 (test set),
show an accuracy (F score) of 82.1% and 80.1%
respectively.3 Although they are lower than those
for state-of-the-art parsers, these results suggest
that the bounded-memory parser described here is
doing a reasonably good job of modeling syntac-
tic dependencies, and therefore may have some
3Using unsmoothed relative frequency estimates from the
training set, a depth limit of D = 3, beam with of 2000, and
no lexicalization.
promise as a psycholinguistic model.
Although recognition in this system is linear, it
essentially works top-down, so it has larger run-
time constants than a bottom-up CKY-style parser.
The experimental system described above runs at
a rate of about 1 sentence per second on a 64-
bit 2.6GHz dual core desktop with a beam width
of 2000. In comparison, the Klein and Manning
(2003) CKY-style parser runs at about 5 sentences
per second on the same machine. On sentences
longer than 40 words, the HHMM and CKY-style
parsers are roughly equivalent, parsing at the rate
of .21 sentences per second, versus .24 for the
Klein and Manning CKY.
But since it is linear, the HHMM parser can be
directly integrated with end-of-sentence detection
(e.g. deciding whether ?.? is a sentence delimiter
based on whether the words preceding it can be
reduced as a sentence), or with n-gram language
models (if words are observations, this is simply
an autoregressive HMM topology). The use of
an explicit constituent structure in a time series
model also allows integration with models of dy-
namic phenomena such as semantics and corefer-
ence which may depend on constituency. Finally,
as a linear model, it can be directly applied to
speech recognition (essentially replacing the hid-
den layer of a conventional word-based HMM lan-
guage model).
5 Conclusion
This paper has described a basic incremental pars-
ing model that achieves worst-case linear time
complexity by enforcing fixed limits on a stack
of explicit (albeit incomplete) constituents. Ini-
tial results show a use of only three to four levels
of stack memory within this framework provides
nearly complete coverage of the large Penn Tree-
bank corpus.
Acknowledgments
The authors would like to thank the anonymous
reviewers for their input. This research was
supported by National Science Foundation CA-
REER/PECASE award 0447685. The views ex-
pressed are not necessarily endorsed by the spon-
sors.
791
References
Abney, Steven P. and Mark Johnson. 1991. Memory
requirements and local ambiguities of parsing strate-
gies. J. Psycholinguistic Research, 20(3):233?250.
Anderson, J.R. and L.M. Reder. 1999. The fan effect:
New results and new theories. Journal of Experi-
mental Psychology: General, 128(2):186?197.
Brown-Schmidt, Sarah, Ellen Campana, and
Michael K. Tanenhaus. 2002. Reference res-
olution in the wild: Online circumscription of
referential domains in a natural interactive problem-
solving task. In Proceedings of the 24th Annual
Meeting of the Cognitive Science Society, pages
148?153, Fairfax, VA, August.
Cowan, Nelson. 2001. The magical number 4 in short-
term memory: A reconsideration of mental storage
capacity. Behavioral and Brain Sciences, 24:87?
185.
Dahan, Delphine and M. Gareth Gaskell. 2007. The
temporal dynamics of ambiguity resolution: Evi-
dence from spoken-word recognition. Journal of
Memory and Language, 57(4):483?501.
Ericsson, K. Anders and Walter Kintsch. 1995.
Long-term working memory. Psychological Review,
102:211?245.
Frege, Gottlob. 1892. Uber sinn und bedeutung.
Zeitschrift fur Philosophie und Philosophischekritik,
100:25?50.
Hale, John. 2003. Grammar, Uncertainty and Sen-
tence Processing. Ph.D. thesis, Cognitive Science,
The Johns Hopkins University.
Hobbs, Jerry R., Douglas E. Appelt, John Bear,
David Israel, Megumi Kameyama, Mark Stickel, and
Mabry Tyson. 1996. Fastus: A cascaded finite-state
transducer for extracting information from natural-
language text. In Finite State Devices for Natural
Language Processing, pages 383?406. MIT Press,
Cambridge, MA.
Johnson, Mark. 1998a. Finite state approximation of
constraint-based grammars using left-corner gram-
mar transforms. In Proceedings of COLING/ACL,
pages 619?623.
Johnson, Mark. 1998b. PCFG models of linguistic tree
representation. Computational Linguistics, 24:613?
632.
Klein, Dan and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423?430.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Marcus, Mitch. 1980. A theory of syntactic recognition
for natural language. MIT Press.
Miller, George and Noam Chomsky. 1963. Finitary
models of language users. In Luce, R., R. Bush, and
E. Galanter, editors, Handbook of Mathematical Psy-
chology, volume 2, pages 419?491. John Wiley.
Miller, George A. 1956. The magical number seven,
plus or minus two: Some limits on our capacity
for processing information. Psychological Review,
63:81?97.
Murphy, Kevin P. and Mark A. Paskin. 2001. Lin-
ear time inference in hierarchical HMMs. In Proc.
NIPS, pages 833?840.
Roark, Brian. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Sagae, Kenji and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the Ninth International Workshop on Parsing
Technologies (IWPT?05).
Schuler, William, Stephen Wu, and Lane Schwartz. in
press. A framework for fast incremental interpre-
tation during speech decoding. Computational Lin-
guistics.
Smolensky, Paul and Ge?raldine Legendre. 2006.
The Harmonic Mind: From Neural Computation to
Optimality-Theoretic GrammarVolume I: Cognitive
Architecture. MIT Press.
Steedman, Mark. 2000. The syntactic process. MIT
Press/Bradford Books, Cambridge, MA.
Tanenhaus, Michael K., Michael J. Spivey-Knowlton,
Kathy M. Eberhard, and Julie E. Sedivy. 1995. Inte-
gration of visual and linguistic information in spoken
language comprehension. Science, 268:1632?1634.
792
