First Joint Conference on Lexical and Computational Semantics (*SEM), pages 571?574,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
JU_CSE_NLP: Multi-grade Classification of Semantic Similarity  
Between Text Pairs 
 
    Snehasis Neogi1, Partha Pakray2, Sivaji Bandyopadhyay1            Alexander Gelbukh 
                           1Computer Science & Engineering Department                        Center for Computing Research 
                           Jadavpur University, Kolkata, India                                 National Polytechnic Institute         
                  2Computer Science & Engineering Department                               Mexico City, Mexico 
                           Jadavpur University, Kolkata, India                                 gelbukh@gelbukh.com 
                        Intern at Xerox Research Centre Europe 
                                          Grenoble, France 
                  {snehasis1981,parthapakray}@gmail.com 
            sbandyopadhyay@cse.jdvu.ac.in 
 
Abstract 
This article presents the experiments car-
ried out at Jadavpur University as part of 
the participation in Semantic Textual Si-
milarity (STS) of Task 6 @ Semantic 
Evaluation Exercises (SemEval-2012). 
Task-6 of SemEval- 2012 focused on se-
mantic relations of text pair. Task-6 pro-
vides five different text pair files to 
compare different semantic relations and 
judge these relations through a similarity 
and confidence score. Similarity score is 
one kind of multi way classification in the 
form of grade between 0 to 5. We have 
submitted one run for the STS task. Our 
system has two basic modules - one deals 
with lexical relations and another deals 
with dependency based syntactic relations 
of the text pair. Similarity score given to a 
pair is the average of the scores of the 
above-mentioned modules. The scores 
from each module are identified using rule 
based techniques. The Pearson Correlation 
of our system in the task is 0.3880. 
1 Introduction 
Task-61 [1] of SemEval-2012 deals with seman-
tic similarity of text pairs. The task is to find the 
similarity between the sentences in the text pair 
(s1 and s2) and return a similarity score and an 
optional confidence score. There are five datasets 
                                                          
1 http://www.cs.york.ac.uk/semeval-2012/task6/ 
in the test data and with tab separated text pairs. 
The datasets are as follows: 
 
? MSR-Paraphrase, Microsoft Research Pa-
raphrase Corpus (750 pairs of sentences.) 
? MSR-Video, Microsoft Research Video De-
scription Corpus (750 pairs of sentences.) 
? SMTeuroparl: WMT2008 development data-
set (Europarl section) (459 pairs of sen-
tences.)  
? SMTnews: news conversation sentence pairs 
from WMT.(399 pairs of sentences.) 
? OnWN: pairs of sentences where the first 
comes from Ontonotes and the second from a 
WordNet definition. (750 pairs of sentences.) 
 
Similarity score ranges from 0 to 5 and confi-
dence score from 0 to 100. An s1-s2 pair gets a 
similarity score of 5 if they are completely 
equivalent. Similarity score 4 is allocated for 
mostly equivalent s1-s2 pair. Similarly, score 3 is 
allocated for roughly equivalent pair. Score 2, 1 
and 0 are allocated for non-equivalent details 
sharing, non-equivalent topic sharing and totally 
different pairs respectively. Major challenge of 
this task is to find the similarity score based simi-
larity for the text pair. Generally text entailment 
tasks refer whether sentence pairs are entailed or 
not: binary classification (YES, NO) [2] or multi-
classification (Forward, Backward, bidirectional 
or no entailment) [3][4]. But multi grade classifi-
cation of semantic similarity assigns a score to 
the sentence pair. Our system considers lexical 
and dependency based syntactic measures for 
semantic similarity. Similarity scores are the ba-
sic average of these module scores. A subsequent 
571
section describes the system architecture. Section 
2 describes JU_NLP_CSE system for STS task. 
Section 3 describes evaluation and experimental 
results. Conclusions are drawn in Section 4.  
2 System Architecture  
The system of Semantic textual similarity task 
has two main modules: one is lexical module and 
another one is dependency parsing based syntac-
tic module. Both these module have some pre-
processing tasks such as stop word removal, co-
reference resolution and dependency parsing etc. 
Figure 1 displays the architecture of the system.   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: System Architecture 
2.1 Pre-processing Module 
The system separates the s1-s2 sentence pairs 
contained in the different STS task datasets. 
These separated pairs are then passed through the 
following sub modules: 
i. Stop word Removal: Stop words are removed 
from s1 - s2 sentence pairs. 
ii. Co-reference: Co-reference resolutions are 
carried out on the datasets before passing through 
the TE module. The objective is to increase the 
score of the entailment percentage. A word or 
phrase in the sentence is used to refer to an entity 
introduced earlier or later in the discourse and 
both having same things then they have the same 
referent or co reference. When the reader must 
look back to the previous context, reference is 
called "Anaphoric Reference". When the reader 
must look forward, it is termed "Cataphoric Ref-
erence". To address this problem we used a tool 
called JavaRAP2 (A java based implementation 
of Anaphora Procedure (RAP) - an algorithm by 
Lappin and Leass (1994)). 
iii. Dependency Parsing: Separated s1 ? s2 sen-
tences are parsed using Stanford dependency 
parser3 to produce the dependency relations in 
the texts. These dependency relations are used 
for WordNet based syntactic matching.     
2.2 Lexical Matching Module 
In this module the TE system calculates different 
matching scores such as N ? Gram match, Text 
Similarity, Chunk match, Named Entity match 
and POS match.  
 
i. N-Gram Match module: The N-Gram match 
basically measures the percentage match of the 
unigram, bigram and trigram of hypothesis 
present in the corresponding text. These scores 
are simply combined to get an overall N ? Gram 
matching score for a particular pair.  
 
ii. Chunk Match module: In this sub module 
our system evaluates the key NP-chunks of both 
text (s1) and hypothesis (s2) using NP Chunker 
v1.13 (The University of Sheffield). The hypo-
thesis NP chunks are matched in the text NP 
chunks. System calculates an overall value for 
the chunk matching, i.e., number of text NP 
chunks that match the hypothesis NP chunks. If 
the chunks are not similar in their surface form 
then our system goes for wordnet synonyms 
matching for the words and if they match in 
wordnet synsets information, it will be encoun-
tered as a similar chunk. WordNet [5] is one of 
most important resource for lexical analysis. The 
WordNet 2.0 has been used for WordNet based 
chunk matching. The API for WordNet Search-
ing (JAWS)4 is an API that provides Java appli-
cations with the ability to retrieve data from the 
WordNet synsets. 
iii. Text Similarity Module: System takes into 
consideration several text similarities calculated 
                                                          
2 http://aye.comp.nus.edu.sg/~qiu/NLPTools/JavaRAP.html 
3 http://www.dcs.shef.ac.uk/~mark/phd/software/ 
4 http://lyle.smu.edu/~tspell/jaws/index.html 
 
 
572
over the s1-s2 pair. These text similarity values 
are summed up to produce a total score for a par-
ticular s1-s2 pair. Major Text similarity measures 
that our system considers are: 
 
? Cosine Similarity 
? Lavenstine Distance 
? Euclidean Distance 
? MongeElkan Distance 
? NeedlemanWunch Distance 
? SmithWaterman Distance 
? Block Distance 
? Jaro Similarity 
? MatchingCoefficient Distance 
? Dice Similarity 
? OverlapCoefficient 
? QGrams Distance 
 
iv. Named Entity Matching: It is based on the 
detection and matching of Named Entities in the 
s1-s2 pair. Stanford Named Entity Recognizer5 is 
used to tag the named entities in both s1 and s2. 
System simply maps the number of hypothesis 
(s2) NEs present in the text (s1). A score is allo-
cated for the matching. 
 
NE_match = (Number of common NEs in Text 
and Hypothesis) / (Number of NE in Hypothesis). 
 
v. Part ?of ? Speech (POS) Matching: This 
module basically deals with matching the com-
mon POS tags between s1 and s2 sentences. 
Stanford POS tagger6 is used to tag the part of 
speech in both s1 and s2. System matches the 
verb and noun POS words in the hypothesis that 
match in the text. A score is allocated based on 
the number of POS matching. 
 
POS_match = (Number of common verb and 
noun POS in Text and Hypothesis) / (Total num-
ber of verb and noun POS in hypothesis). 
 
System calculates the sum of the entire sub mod-
ule (modules described in section 2.2) scores and 
forms a single percentage score for the lexical 
matching. This score is then compared with some 
predetermined threshold value to assign a final 
lexical score for each pair. If percentage value is 
                                                          
5 http://nlp.stanford.edu/software/CRF-NER.shtml 
6 http://nlp.stanford.edu/software/tagger.shtml 
above 0.80 then lexical score 5 is allocated. If the 
value is between 0.60 to 0.80 then lexical score 4 
is allocated. Similarly, lexical score 3 is allocated 
for percentage score of 0.40 to 0.60 and so on. 
One lexical score is finally generated for each 
text pair.     
2.3. Syntactic Matching Module: 
TE system considers the preprocessed dependen-
cy parsed text pairs (s1 ? s2) and goes for word 
net based matching technique. After parsing the 
sentences, they have some attributes like subject, 
object, verb, auxiliaries and prepositions tagged 
by the dependency parser tag set. System uses 
these attributes for the matching procedure and 
depending on the nature of matching a score is 
allocated to the s1-s2 pair. Matching procedure is 
basically done through comparison of the follow-
ing features that are present in both the text and 
the hypothesis.    
? Subject ? Subject comparison. 
? Verb ? Verb Comparison. 
? Subject ? Verbs Comparison. 
? Object ? Object Comparison. 
? Cross Subject ? Object Comparison. 
? Object ? Verbs Comparison. 
? Prepositional phrase comparison. 
 
Each of these comparisons produces one match-
ing score for the s1-s2 pair that are finally com-
bined with previously generated lexical score to 
generate the final similarity score by taking sim-
ple average of lexical and syntactic matching 
scores. The basic heuristics are as follows: 
(i) If the feature of the text (s1) directly matches 
the same feature of the hypothesis (s2), matching 
score 5 is allocated for the text pair. 
(ii) If the feature of either text (s1) or hypothesis 
(s2) matches with the wordnet synsets of the cor-
responding text (s1) or hypothesis (s2), matching 
score 4 is allocated.     
(iii) If wordnet synsets of the feature of the text 
(s1) match with one of the synsets of the feature 
of the hypothesis (s2), matching score 3 is given 
to the pair. 
(iv) If wordnet synsets of the feature of either 
text (s1) or hypothesis (s2) match with the syn-
sets of the corresponding text (s1) or hypothesis 
(s2) then matching score 2 is allocated for the 
pair. 
573
(v) Similarly if in both the cases match occurs in 
the second level of wordnet synsets, matching 
score 1is allocated. 
(vi) Matching score 0 is allocated for the pair 
having no match in their features. 
After execution of the module, system generates 
some scores. Lexical module generates one lexi-
cal score and wordnet based syntactic matching 
module generates seven matching scores. At the 
final stage of the system all these scores are 
combined and the mean is evaluated on this 
combined score. This mean gives the similarity 
score for a particular s1-s2 pair of different data-
sets of STS task. Optional confidence score is 
also allocated which is basically the similarity 
score multiplied by 10, i.e., if the similarity score 
is 5.22, the confidence score will be 52.2.     
3. Experiments on Dataset and Result  
We have submitted one run in SemEval-2012 
Task 6. The results for Run on STS Test set are 
shown in Table 1. 
 
task6-JU_CSE_NLP-
Semantic_Syntactic_Approach 
Correlations 
ALL    0.3880 
ALLnrm 0.6706 
Mean 0.4111 
MSRpar  0.3427 
MSRvid 0.3549 
SMT-eur 0.4271 
On-WN 0.5298 
SMT-news 0.4034 
Table 1: Results of Test Set 
ALL: Pearson correlation with the gold standard 
for the five datasets and the corresponding rank 
82. 
ALLnrm: Pearson correlation after the system 
outputs for each dataset are fitted to the gold 
standard using least squares and the correspond-
ing rank 86. 
Mean: Weighted mean across the 5 datasets, 
where the weight depends on the number of pairs 
in the dataset and the corresponding rank 76. 
The subsequent rows show the pearson correla-
tion scores for each of the individual datasets. 
 
4. Conclusion 
Our JU_CSE_NLP system for the STS task 
mainly focus on lexical and syntactic approaches. 
There are some limitations in the lexical match-
ing module that shows a correlation that is not 
higher in the range. In case of simple sentences 
lexical matching is helpful for entailment but for 
complex and compound sentences the lexical 
matching module loses its accuracy. Semantic 
graph matching or conceptual graph implementa-
tion can improve the system. That is not consi-
dered in our present work. Machine learning 
tools can be used to learn the system based on the 
features. It can also improve the correlation. In 
future work our system will include semantic 
graph matching and a machine-learning module.  
Acknowledgments 
The work was done under support of the DST 
India-CONACYT Mexico project ?Answer Vali-
dation through Textual Entailment? funded by 
DST, Government of India.  
References  
