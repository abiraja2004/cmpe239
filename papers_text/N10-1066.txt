Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429?437,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Discriminative Learning over Constrained Latent Representations
Ming-Wei Chang and Dan Goldwasser and Dan Roth and Vivek Srikumar
University of Illinois at Urbana Champaign
Urbana, IL 61801
{mchang,goldwas1,danr,vsrikum2}@uiuc.edu
Abstract
This paper proposes a general learning frame-
work for a class of problems that require learn-
ing over latent intermediate representations.
Many natural language processing (NLP) de-
cision problems are defined over an expressive
intermediate representation that is not explicit
in the input, leaving the algorithm with both
the task of recovering a good intermediate rep-
resentation and learning to classify correctly.
Most current systems separate the learning
problem into two stages by solving the first
step of recovering the intermediate representa-
tion heuristically and using it to learn the final
classifier. This paper develops a novel joint
learning algorithm for both tasks, that uses the
final prediction to guide the selection of the
best intermediate representation. We evalu-
ate our algorithm on three different NLP tasks
? transliteration, paraphrase identification and
textual entailment ? and show that our joint
method significantly improves performance.
1 Introduction
Many NLP tasks can be phrased as decision prob-
lems over complex linguistic structures. Successful
learning depends on correctly encoding these (of-
ten latent) structures as features for the learning sys-
tem. Tasks such as transliteration discovery (Kle-
mentiev and Roth, 2008), recognizing textual en-
tailment (RTE) (Dagan et al, 2006) and paraphrase
identification (Dolan et al, 2004) are a few proto-
typical examples. However, the input to such prob-
lems does not specify the latent structures and the
problem is defined in terms of surface forms only.
Most current solutions transform the raw input into
a meaningful intermediate representation1, and then
encode its structural properties as features for the
learning algorithm.
Consider the RTE task of identifying whether the
meaning of a short text snippet (called the hypoth-
esis) can be inferred from that of another snippet
(called the text). A common solution (MacCartney
et al, 2008; Roth et al, 2009) is to begin by defining
an alignment over the corresponding entities, pred-
icates and their arguments as an intermediate rep-
resentation. A classifier is then trained using fea-
tures extracted from the intermediate representation.
The idea of using a intermediate representation also
occurs frequently in other NLP tasks (Bergsma and
Kondrak, 2007; Qiu et al, 2006).
While the importance of finding a good inter-
mediate representation is clear, emphasis is typi-
cally placed on the later stage of extracting features
over this intermediate representation, thus separat-
ing learning into two stages ? specifying the la-
tent representation, and then extracting features for
learning. The latent representation is obtained by an
inference process using predefined models or well-
designed heuristics. While these approaches often
perform well, they ignore a useful resource when
generating the latent structure ? the labeled data for
the final learning task. As we will show in this pa-
per, this results in degraded performance for the ac-
tual classification task at hand. Several works have
considered this issue (McCallum et al, 2005; Gold-
wasser and Roth, 2008b; Chang et al, 2009; Das
and Smith, 2009); however, they provide solutions
1In this paper, the phrases ?intermediate representation? and
?latent representation? are used interchangeably.
429
that do not easily generalize to new tasks.
In this paper, we propose a unified solution to the
problem of learning to make the classification deci-
sion jointly with determining the intermediate rep-
resentation. Our Learning Constrained Latent Rep-
resentations (LCLR) framework is guided by the in-
tuition that there is no intrinsically good intermedi-
ate representation, but rather that a representation is
good only to the extent to which it improves perfor-
mance on the final classification task. In the rest of
this section we discuss the properties of our frame-
work and highlight its contributions.
Learning over Latent Representations This pa-
per formulates the problem of learning over latent
representations and presents a novel and general so-
lution applicable to a wide range of NLP applica-
tions. We analyze the properties of our learning
solution, thus allowing new research to take advan-
tage of a well understood learning and optimization
framework rather than an ad-hoc solution. We show
the generality of our framework by successfully ap-
plying it to three domains: transliteration, RTE and
paraphrase identification.
Joint Learning Algorithm In contrast to most
existing approaches that employ domain specific
heuristics to construct intermediate representations
to learn the final classifier, our algorithm learns to
construct the optimal intermediate representation to
support the learning problem. Learning to represent
is a difficult structured learning problem however,
unlike other works that use labeled data at the in-
termediate level, our algorithm only uses the binary
supervision supplied for the final learning problem.
Flexible Inference Successful learning depends
on constraining the intermediate representation with
task-specific knowledge. Our framework uses the
declarative Integer Linear Programming (ILP) infer-
ence formulation, which makes it easy to define the
intermediate representation and to inject knowledge
in the form of constraints. While ILP has been ap-
plied to structured output learning, to the best of our
knowledge, this is the first work that makes use of
ILP in formalizing the general problem of learning
intermediate representations.
2 Preliminaries
We introduce notation using the Paraphrase Iden-
tification task as a running example. This is the bi-
nary classification task of identifying whether one
sentence is a paraphrase of another. A paraphrase
pair from the MSR Paraphrase corpus (Quirk et al,
2004) is shown in Figure 1. In order to identify
that the sentences paraphrase each other , we need
to align constituents of these sentences. One possi-
ble alignment is shown in the figure, in which the
dotted edges correspond to the aligned constituents.
An alignment can be specified using binary variables
corresponding to every edge between constituents,
indicating whether the edge is included in the align-
ment. Different activations of these variables induce
the space of intermediate representations.
The notification was first reported Friday by MSNBC.
MSNBC.com first reported the CIA request on Friday.
Figure 1: The dotted lines represent a possible intermediate
representation for the paraphrase identification task. Since dif-
ferent representation choices will impact the binary identifica-
tion decision directly, our approach chooses the representation
that facilitates the binary learning task.
To formalize this setting, let x denote the input
to a decision function, which maps x to {?1, 1}.
We consider problems where this decision depends
on an intermediate representation (for example, the
collection of all dotted edges in Figure 1), which can
be represented by a binary vector h.
In the literature, a common approach is to sepa-
rate the problem into two stages. First, a genera-
tion stage predicts h for each x using a pre-defined
model or a heuristic. This is followed by a learn-
ing stage, in which the classifier is trained using h.
In our example, if the generation stage predicts the
alignment shown, then the learning stage would use
the features computed based on the alignments. For-
mally, the two-stage approach uses a pre-defined in-
ference procedure that finds an intermediate repre-
sentation h?. Using features ?(x,h?) and a learned
weight vector ?, the example is classified as positive
if ?T?(x,h?) ? 0.
However, in the two stage approach, the latent
representation, which is provided to the learning al-
gorithm, is determined before learning starts, and
without any feedback from the final task. It is dic-
tated by the intuition of the developer. This approach
makes two implicit assumptions: first, it assumes
430
the existence of a ?correct? latent representation and,
second, that the model or heuristic used to generate
it is the correct one for the learning problem at hand.
3 Joint Learning with an Intermediate
Representation
In contrast to two-stage approaches, we use the
annotated data for the final classification task to
learn a suitable intermediate representation which,
in turn, helps the final classification.
Choosing a good representation is an optimization
problem that selects which of the elements (features)
of the representation best contribute to success-
ful classification given some legitimacy constraints;
therefore, we (1) set up the optimization framework
that finds legitimate representations (Section 3.1),
and (2) learn an objective function for this optimiza-
tion problem, such that it makes the best final deci-
sion (Section 3.2.)
3.1 Inference
Our goal is to correctly predict the final label
rather than matching a ?gold? intermediate repre-
sentation. In our framework, attempting to learn the
final decision drives both the selection of the inter-
mediate representation and the final predictions.
For each x, let ?(x) be the set of all substructures
of all possible intermediate representations. In Fig-
ure 1, this could be the set of all alignment edges
connecting the constituents of the sentences. Given
a vocabulary of such structures of sizeN , we denote
intermediate representations by h ? {0, 1}N , which
?select? the components of the vocabulary that con-
stitute the intermediate representation. We define
?s(x) to be a feature vector over the substructure
s, which is used to describe the characteristics of s,
and define a weight vector u over these features.
Let C denote the set of feasible intermediate repre-
sentations h, specified by means of linear constraints
over h. While ?(x) might be large, the set of those
elements in h that are active can be constrained by
controlling C. After we have learned a weight vec-
tor u that scores intermediate representations for the
final classification task, we define our decision func-
tion as
fu(x) = max
h?C
uT
?
s??(x)
hs?s(x), (1)
and classify the input as positive if fu(x) ? 0.
In Eq. (1), uT?s(x) is the score associated with
the substructure s, and fu(x) is the score for the en-
tire intermediate representation. Therefore, our de-
cision function fu(x) ? 0 makes use of the interme-
diate representation and its score to classify the in-
put. An input is labeled as positive if its underlying
intermediate structure allows it to cross the decision
threshold. The intermediate representation is cho-
sen to maximize the overall score of the input. This
design is especially beneficial for many phenomena
in NLP, where only positive examples have a mean-
ingful underlying structure. In our paraphrase iden-
tification example, good alignments generally exist
only for positive examples.
One unique feature of our framework is that we
treat Eq. (1) as an Integer Linear Programming
(ILP) instance. A concrete instantiation of this set-
ting to the paraphrase identification problem, along
with the actual ILP formulation is shown in Section
4.
3.2 Learning
We now present an algorithm that learns the
weight vector u. For a loss function ` : R ? R,
the goal of learning is to solve the following opti-
mization problem:
min
u
?
2
?u?2 +
?
i
` (?yifu(xi)) (2)
Here, ? is the regularization parameter. Substituting
Eq. (1) into Eq. (2), we get
min
u
?
2
?u?2+
?
i
`
?
??yi max
h?C
uT
?
s??(x)
hs?s(xi)
?
? (3)
Note that there is a maximization term inside the
global minimization problem, making Eq. (3) a non-
convex problem. The minimization drives u towards
smaller empirical loss while the maximization uses
u to find the best representation for each example.
The algorithm for Learning over Constrained La-
tent Representations (LCLR) is listed in Algorithm
1. In each iteration, first, we find the best feature
representations for all positive examples (lines 3-5).
This step can be solved with an off-the-shelf ILP
solver. Having fixed the representations for the pos-
itive examples, we update the u by solving Eq. (4)
at line 6 in the algorithm. It is important to observe
431
Algorithm 1 LCLR :The algorithm that optimizes (3)
1: initialize: u? u0
2: repeat
3: for all positive examples (xi, yi = 1) do
4: Find h?i ? arg maxh?C
?
s
hsuT?s(xi)
5: end for
6: Update u by solving
min
u
?
2
?u?2 +
?
i:yi=1
`(?uT
?
s
h?i,s?s(xi))
+
?
i:yi=?1
`(max
h?C
uT
?
s
hs?s(xi)) (4)
7: until convergence
8: return u
that for positive examples in Eq. (4), we use the in-
termediate representations h? from line 4.
Algorithm 1 satisfies the following property:
Theorem 1 If the loss function ` is a non-
decreasing function, then the objective function
value of Eq. (3) is guaranteed to decrease in every
iteration of Algorithm 1. Moreover, if the loss func-
tion is also convex, then Eq. (4) in Algorithm 1 is
convex.
Due to the space limitation, we omit the proof.
Theoretically, we can use any loss function that
satisfies the conditions of the theorem. In the exper-
iments in this paper, we use the squared-hinge loss
function: `(?yfu(x)) = max(0, 1? yfu(x))2.
Recall that Eq. (4) is not the traditional SVM or
logistic regression formulation. This is because in-
side the inner loop, the best representation for each
negative example must be found. Therefore, we
need to perform inference for every negative exam-
ple when updating the weight vector solution. In-
stead of solving a difficult non-convex optimization
problem (Eq. (3)), LCLR iteratively solves a series
of easier problems (Eq. (4)). This is especially true
for our loss function because Eq. (4) is convex and
can be solved efficiently.
We use a cutting plane algorithm to solve Eq. (4).
A similar idea has been proposed in (Joachims et al,
2009). The algorithm for solving Eq. (4) is presented
as Algorithm 2. This algorithm uses a ?cache? Hj
to store all intermediate representations for negative
examples that have been seen in previous iterations
Algorithm 2 Cutting plane algorithm to optimize Eq. (4)
1: for each negative example xj , Hj ? ?
2: repeat
3: for each negative example xj do
4: Find h?j ? arg maxh?C
?
s hsu
T?s(xj)
5: Hj ? Hj ? {h?j}
6: end for
7: Solve
min
u
?
2
?u?2 +
?
i:yi=1
`(?uT
?
s
h?i,s?s(xi))
+
?
i:yi=?1
`(max
h?Hj
uT
?
s
hs?s(xi)) (5)
8: until no new element is added to any Hj
9: return u
(lines 3-6) 2. The difference between Eq. (5) in line
7 of Algorithm 2 and Eq. (4) is that in Eq. (5), we do
not search over the entire space of intermediate rep-
resentations. The search space for the minimization
problem Eq. (5) is restricted to the cache Hj . There-
fore, instead of solving the minimization problem
Eq. (4), we can now solve several simpler problems
shown in Eq. (5). The algorithm is guaranteed to
stop (line 8) because the space of intermediate rep-
resentations is finite. Furthermore, in practice, the
algorithm needs to consider only a small subset of
?hard? examples before it converges.
Inspired by (Hsieh et al, 2008), we apply an effi-
cient coordinate descent algorithm for the dual for-
mulation of (5) which is guaranteed to find its global
minimum. Due to space considerations, we do not
present the derivation of dual formulation and the
details of the optimization algorithm.
4 Encoding with ILP: A Paraphrase
Identification Example
In this section, we define the latent representation
for the paraphrase identification task. Unlike the ear-
lier example, where we considered the alignment of
lexical items, we describe a more complex interme-
diate representation by aligning graphs created using
semantic resources.
An input example is represented as two acyclic
2In our implementation, we keep a global cache Hj for each
negative example xj . Therefore, in Algorithm 2, we start with
a non-empty cache improving the speed significantly.
432
graphs, G1 and G2, corresponding to the first
and second input sentences. Each vertex in the
graph contains word information (lemma and part-
of-speech) and the edges denote dependency rela-
tions, generated by the Stanford parser (Klein and
Manning, 2003). The intermediate representation
for this task can now be defined as an alignment be-
tween the graphs, which captures lexical and syntac-
tic correlations between the sentences.
We use V (G) and E(G) to denote the set of ver-
tices and edges in G respectively, and define four
hidden variable types to encode vertex and edge
mappings between G1 and G2.
? The word-mapping variables, denoted by
hv1,v2 , define possible pairings of vertices,
where v1 ? V (G1) and v2 ? V (G2).
? The edge-mapping variables, denoted by
he1,e2 , define possible pairings of the graphs
edges, where e1 ? E(G1) and e2 ? E(G2).
? The word-deletion variables hv1,? (or h?,v2) al-
low for vertices v1 ? V (G1) (or v2 ? V (G2))
to be deleted. This accounts for omission of
words (like function words).
? The edge-deletion variables, he1,? (or h?,e2) al-
low for deletion of edges from G1 (or G2).
Our inference problem is to find the optimal set of
hidden variable activations, restricted according to
the following set of linear constraints
? Each vertex inG1 (orG2) can either be mapped
to a single vertex in G2 (or G1) or marked as
deleted. In terms of the word-mapping and
word-deletion variables, we have
?v1 ? V (G1);hv1,? +
?
v2?V (G2)
hv1,v2 = 1 (6)
?v2 ? V (G2);h?,v2 +
?
v1?V (G1)
hv1,v2 = 1 (7)
? Each edge in G1 (or G2) can either be mapped
to a single edge in G2 (or G1) or marked as
deleted. In terms of the edge-mapping and
edge-deletion variables, we have
?e1 ? E(G1);he1,? +
?
e2?E(G2)
he1,e2 = 1 (8)
?e2 ? E(G2);h?,e2 +
?
e1?E(G1)
he1,e2 = 1 (9)
? The edge mappings can be active if, and only
if, the corresponding node mappings are ac-
tive. Suppose e1 = (v1, v?1) ? E(G1) and
e2 = (v2, v?2) ? E(G2), where v1, v
?
1 ? V (G1)
and v2, v?2 ? V (G2). Then, we have
hv1,v2 + hv?1,v?2 ? he1,e2 ? 1 (10)
hv1,v2 ? he1,e2 ;hv?1,v?2 ? he1,e2 (11)
These constraints define the feasible set for the in-
ference problem specified in Equation (1). This in-
ference problem can be formulated as an ILP prob-
lem with the objective function from Equation (1):
max
h
?
s
hsuT?s(x)
subject to (6)-(11); ?s;hs ? {0, 1} (12)
This example demonstrates the use of integer linear
programming to define intermediate representations
incorporating domain intuition.
5 Experiments
We applied our framework to three different NLP
tasks: transliteration discovery (Klementiev and
Roth, 2008), RTE (Dagan et al, 2006), and para-
phrase identification (Dolan et al, 2004).
Our experiments are designed to answer the fol-
lowing research question: ?Given a binary classifi-
cation problem defined over latent representations,
will the joint LCLR algorithm perform better than a
two-stage approach?? To ensure a fair comparison,
both systems use the same feature functions and def-
inition of intermediate representation. We use the
same ILP formulation in both configurations, with a
single exception ? the objective function parameters:
the two stage approach uses a task-specific heuristic,
while LCLR learns it iteratively.
The ILP formulation results in very strong two
stage systems. For example, in the paraphrase iden-
tification task, even our two stage system is the cur-
rent state-of-the-art performance. In these settings,
the improvement obtained by our joint approach is
non-trivial and can be clearly attributed to the su-
periority of the joint learning algorithm. Interest-
ingly, we find that our more general approach is bet-
ter than specially designed joint approaches (Gold-
wasser and Roth, 2008b; Das and Smith, 2009).
Since the objective function (3) of the joint ap-
proach is not convex, a good initialization is re-
quired. We use the weight vector learned by the two
433
stage approach as the starting point for the joint ap-
proach. The algorithm terminates when the relative
improvement of the objective is smaller than 10?5.
5.1 Transliteration Discovery
Transliteration discovery is the problem of iden-
tifying if a word pair, possibly written using two
different character sets, refers to the same underly-
ing entity. The intermediate representation consists
of all possible character mappings between the two
character sets. Identifying this mapping is not easy,
as most writing systems do not perfectly align pho-
netically and orthographically; rather, this mapping
can be context-dependent and ambiguous.
For an input pair of words (w1, w2), the interme-
diate structure h is a mapping between their charac-
ters, with the latent variable hij indicating if the ith
character in w1 is aligned to the jth character in w2.
The feature vector associated with the variable hij
contains unigram character mapping, bigram char-
acter mapping (by considering surrounding charac-
ters). We adopt the one-to-one mapping and non-
crossing constraint used in (Chang et al, 2009).
We evaluated our system using the English-
Hebrew corpus (Goldwasser and Roth, 2008a),
which consists of 250 positive transliteration pairs
for training, and 300 pairs for testing. As negative
examples for training, we sample 10% from random
pairings of words from the positive data. We report
two evaluation measurements ? (1) the Mean Recip-
rocal Rank (MRR), which is the average of the mul-
tiplicative inverse of the rank of the correct answer,
and (2) the accuracy (Acc), which is the percentage
of the top rank candidates being correct.
We initialized the two stage inference process as
detailed in (Chang et al, 2009) using a Romaniza-
tion table to assign uniform weights to prominent
character mappings. This initialization procedure
resembles the approach used in (Bergsma and Kon-
drak, 2007). An alignment is first built by solving
the constrained optimization problem. Then, a sup-
port vector machine with squared-hinge loss func-
tion is used to train a classifier using features ex-
tracted from the alignment. We refer to this two
stage approach as Alignment+Learning.
The results summarized in Table 1 show the sig-
nificant improvement obtained by the joint approach
(95.4% MRR) compared to the two stage approach
Transliteration System Acc MRR
(Goldwasser and Roth,
2008b)
N/A 89.4
Alignment + Learning 80.0 85.7
LCLR 92.3 95.4
Table 1: Experimental results for transliteration. We compare
a two-stage system: ?Alignment+Learning? with LCLR, our
joint algorithm. Both ?Alignment+Learning? and LCLR use
the same features and the same intermediate representation def-
inition.
(85.7%). Moreover, LCLR outperforms the joint
system introduced in (Goldwasser and Roth, 2008b).
5.2 Textual Entailment
Recognizing Textual Entailment (RTE) is an im-
portant textual inference task of predicting if a given
text snippet, entails the meaning of another (the hy-
pothesis). In many current RTE systems, the entail-
ment decision depends on successfully aligning the
constituents of the text and hypothesis, accounting
for the internal linguistic structure of the input.
The raw input ? the text and hypothesis ? are
represented as directed acyclic graphs, where ver-
tices correspond to words. Directed edges link verbs
to the head words of semantic role labeling argu-
ments produced by (Punyakanok et al, 2008). All
other words are connected by dependency edges.
The intermediate representation is an alignment be-
tween the nodes and edges of the graphs. We used
three hidden variable types from Section 4 ? word-
mapping, word-deletion and edge-mapping, along
with the associated constraints as defined earlier.
Since the text is typically much longer than the hy-
pothesis, we create word-deletion latent variables
(and features) only for the hypothesis.
The second column of Table 2 lists the resources
used to generate features corresponding to each hid-
den variable type. For word-mapping variables, the
features include a WordNet based metric (WNSim),
indicators for the POS tags and negation identifiers.
We used the state-of-the-art coreference resolution
system of (Bengtson and Roth, 2008) to identify the
canonical entities for pronouns and extract features
accordingly. For word deletion, we use only the POS
tags of the corresponding tokens (generated by the
LBJ POS tagger3) to generate features. For edge
3
http://L2R.cs.uiuc.edu/?cogcomp/software.php
434
Hidden RTE Paraphrase
Variable features features
word-mapping WordNet, POS,
Coref, Neg
WordNet, POS,
NE, ED
word-deletion POS POS, NE
edge-mapping NODE-INFO NODE-INFO,
DEP
edge-deletion N/A DEP
Table 2: Summary of latent variables and feature resources for
the entailment and paraphrase identification tasks. See Section
4 for an explanation of the hidden variable types. The linguistic
resources used to generate features are abbreviated as follows ?
POS: Part of speech, Coref: Canonical coreferent entities; NE:
Named Entity, ED: Edit distance, Neg: Negation markers, DEP:
Dependency labels, NODE-INFO: corresponding node align-
ment resources, N/A: Hidden variable not used.
Entailment System Acc
Median of TAC 2009 systems 61.5
Alignment + Learning 65.0
LCLR 66.8
Table 3: Experimental results for recognizing textual entail-
ment. The first row is the median of best performing systems of
all teams that participated in the RTE5 challenge (Bentivogli et
al., 2009). Alignment + Learning is our two-stage system im-
plementation, and LCLR is our joint learning algorithm. Details
about these systems are provided in the text.
mapping variables, we include the features of the
corresponding word mapping variables, scaled by
the word similarity of the words forming the edge.
We evaluated our system using the RTE-5
data (Bentivogli et al, 2009), consisting of 600 sen-
tence pairs for training and testing respectively, in
which positive and negative examples are equally
distributed. In these experiments the joint LCLR al-
gorithm converged after 5 iterations.
For the two stage system, we used WN-
Sim to score alignments during inference. The
word-based scores influence the edge variables
via the constraints. This two-stage system (the
Alignment+Learning system) is significantly better
than the median performance of the RTE-5 submis-
sions. Using LCLR further improves the result by al-
most 2%, a substantial improvement in this domain.
5.3 Paraphrase Identification
Our final task is Paraphrase Identification, dis-
cussed in detail at Section 4. We use all the four
hidden variable types described in that section. The
features used are similar to those described earlier
Paraphrase System Acc
Experiments using (Dolan et al, 2004)
(Qiu et al, 2006) 72.00
(Das and Smith, 2009) 73.86
(Wan et al, 2006) 75.60
Alignment + Learning 76.23
LCLR 76.41
Experiments using Extended data set
Alignment + Learning 72.00
LCLR 72.75
Table 4: Experimental Result For Paraphrasing Identification.
Our joint LCLR approach achieves the best results compared
to several previously published systems, and our own two stage
system implementation (Alignment + Learning). We evaluated
the systems performance across two datasets: (Dolan et al,
2004) dataset and the Extended dataset, see the text for details.
Note that LCLR outperforms (Das and Smith, 2009), which is a
specifically designed joint approach for this task.
for the RTE system and are summarized in Table 2.
We used the MSR paraphrase dataset of (Dolan
et al, 2004) for empirical evaluation. Additionally,
we generated a second corpus (called the Extended
dataset) by sampling 500 sentence pairs from the
MSR dataset for training and using the entire test
collection of the original dataset. In the Extended
dataset, for every sentence pair, we extended the
longer sentence by concatenating it with itself. This
results in a more difficult inference problem because
it allows more mappings between words. Note that
the performance on the original dataset sets the ceil-
ing on the second one.
The results are summarized in Table 4. The first
part of the table compares the LCLR system with
a two stage system (Alignment + Learning) and
three published results that use the MSR dataset.
(We only list single systems in the table4) Inter-
estingly, although still outperformed by our joint
LCLR algorithm, the two stage system is able per-
form significantly better than existing systems for
that dataset (Qiu et al, 2006; Das and Smith, 2009;
Wan et al, 2006). We attribute this improvement,
consistent across both the ILP based systems, to the
intermediate representation we defined.
We hypothesize that the similarity in performance
between the joint LCLR algorithm and the two stage
4Previous work (Das and Smith, 2009) has shown that com-
bining the results of several systems improves performance.
435
(Alignment + Learning) systems is due to the limited
intermediate representation space for input pairs in
this dataset. We evaluated these systems on the more
difficult Extended dataset. Results indeed show that
the margin between the two systems increases as the
inference problem becomes harder.
6 Related Work
Recent NLP research has largely focused on two-
stage approaches. Examples include RTE (Zanzotto
and Moschitti, 2006; MacCartney et al, 2008; Roth
et al, 2009); string matching (Bergsma and Kon-
drak, 2007); transliteration (Klementiev and Roth,
2008); and paraphrase identification (Qiu et al,
2006; Wan et al, 2006).
(MacCartney et al, 2008) considered construct-
ing a latent representation to be an independent task
and used manually labeled alignment data (Brockett,
2007) to tune the inference procedure parameters.
While this method identifies alignments well, it does
not improve entailment decisions. This strengthens
our intuition that the latent representation should be
guided by the final task.
There are several exceptions to the two-stage ap-
proach in the NLP community (Haghighi et al,
2005; McCallum et al, 2005; Goldwasser and Roth,
2008b; Das and Smith, 2009); however, the interme-
diate representation and the inference for construct-
ing it are closely coupled with the application task.
In contrast, LCLR provides a general formulation
that allows the use of expressive constraints, mak-
ing it applicable to many NLP tasks.
Unlike other latent variable SVM frameworks
(Felzenszwalb et al, 2009; Yu and Joachims, 2009)
which often use task-specific inference procedure,
LCLR utilizes the declarative inference framework
that allows using constraints over intermediate rep-
resentation and provides a general platform for a
wide range of NLP tasks.
The optimization procedure in this work and
(Felzenszwalb et al, 2009) are quite different.
We use the coordinate descent and cutting-plane
methods ensuring we have fewer parameters and
the inference procedure can be easily parallelized.
Our procedure also allows different loss functions.
(Cherry and Quirk, 2008) adopts the Latent SVM al-
gorithm to define a language model. Unfortunately,
their implementation is not guaranteed to converge.
In CRF-like models with latent variables (McCal-
lum et al, 2005), the decision function marginal-
izes over the all hidden states when presented with
an input example. Unfortunately, the computational
cost of applying their framework is prohibitive with
constrained latent representations. In contrast, our
framework requires only the best hidden representa-
tion instead of marginalizing over all possible repre-
sentations, thus reducing the computational effort.
7 Conclusion
We consider the problem of learning over an inter-
mediate representation. We assume the existence of
a latent structure in the input, relevant to the learn-
ing problem, but not accessible to the learning algo-
rithm. Many NLP tasks fall into these settings and
each can consider a different hidden input structure.
We propose a unifying thread for the different prob-
lems and present a novel framework for Learning
over Constrained Latent Representations (LCLR).
Our framework can be applied to many different la-
tent representations such as parse trees, orthographic
mapping and tree alignments. Our approach con-
trasts with existing work in which learning is done
over a fixed representation, as we advocate jointly
learning it with the final task.
We successfully apply the proposed framework to
three learning tasks ? Transliteration, Textual En-
tailment and Paraphrase Identification. Our joint
LCLR algorithm achieves superior performance in
all three tasks. We attribute the performance im-
provement to our novel training algorithm and flex-
ible inference procedure, allowing us to encode do-
main knowledge. This presents an interesting line of
future work in which more linguistic intuitions can
be encoded into the learning problem. For these rea-
sons, we believe that our framework provides an im-
portant step forward in understanding the problem
of learning over hidden structured inputs.
Acknowledgment We thank James Clarke and Mark Sam-
mons for their insightful comments. This research was partly sponsored
by the Army Research Laboratory (ARL) (accomplished under Cooper-
ative Agreement Number W911NF-09-2-0053) and by Air Force Re-
search Laboratory (AFRL) under prime contract no. FA8750-09-C-
0181. Any opinions, findings, and conclusion or recommendations ex-
pressed in this material are those of the author(s) and do not necessarily
reflect the view of the ARL or of AFRL.
436
References
E. Bengtson and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP.
L. Bentivogli, I. Dagan, H. T. Dang, D. Giampiccolo, and
B. Magnini. 2009. The fifth PASCAL recognizing
textual entailment challenge. In Proc. of TAC Work-
shop.
S. Bergsma and G. Kondrak. 2007. Alignment-based
discriminative string similarity. In ACL.
C. Brockett. 2007. Aligning the RTE 2006 corpus.
In Technical Report MSR-TR-2007-77, Microsoft Re-
search.
M. Chang, D. Goldwasser, D. Roth, and Y. Tu. 2009.
Unsupervised constraint driven learning for transliter-
ation discovery. In NAACL.
C. Cherry and C. Quirk. 2008. Discriminative, syntactic
language modeling through latent svms. In Proc. of
the Eighth Conference of AMTA.
I. Dagan, O. Glickman, and B. Magnini, editors. 2006.
The PASCAL Recognising Textual Entailment Chal-
lenge.
D. Das and N. A. Smith. 2009. Paraphrase identifica-
tion as probabilistic quasi-synchronous recognition. In
ACL.
W. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Ex-
ploiting massively parallel news sources. In COLING.
P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and
D. Ramanan. 2009. Object detection with discrimina-
tively trained part based models. IEEE Transactions
on Pattern Analysis and Machine Intelligence.
D. Goldwasser and D. Roth. 2008a. Active sample se-
lection for named entity transliteration. In ACL. Short
Paper.
D. Goldwasser and D. Roth. 2008b. Transliteration as
constrained optimization. In EMNLP.
A. Haghighi, A. Ng, and C. Manning. 2005. Robust
textual inference via graph matching. In HLT-EMNLP.
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and
S. Sundararajan. 2008. A dual coordinate descent
method for large-scale linear svm. In ICML.
T. Joachims, T. Finley, and Chun-Nam Yu. 2009.
Cutting-plane training of structural svms. Machine
Learning.
D. Klein and C. D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing. In
NIPS.
A. Klementiev and D. Roth. 2008. Named entity translit-
eration and discovery in multilingual corpora. In
Cyril Goutte, Nicola Cancedda, Marc Dymetman, and
George Foster, editors, Learning Machine Translation.
B. MacCartney, M. Galley, and C. D. Manning. 2008.
A phrase-based alignment model for natural language
inference. In EMNLP.
A. McCallum, K. Bellare, and F. Pereira. 2005. A condi-
tional random field for discriminatively-trained finite-
state string edit distance. In UAI.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics.
L. Qiu, M.-Y. Kan, and T.-S. Chua. 2006. Paraphrase
recognition via dissimilarity significance classifica-
tion. In EMNLP.
C. Quirk, C. Brockett, and W. Dolan. 2004. Monolin-
gual machine translation for paraphrase generation. In
EMNLP.
D. Roth, M. Sammons, and V.G. Vydiswaran. 2009. A
framework for entailed relation recognition. In ACL.
S. Wan, M. Dras, R. Dale, and C. Paris. 2006. Using
dependency-based features to take the p?ara-farceo?ut
of paraphrase. In Proc. of the Australasian Language
Technology Workshop (ALTW).
C. Yu and T. Joachims. 2009. Learning structural svms
with latent variables. In ICML.
F. M. Zanzotto and A. Moschitti. 2006. Automatic learn-
ing of textual entailments with cross-pair similarities.
In ACL.
437
