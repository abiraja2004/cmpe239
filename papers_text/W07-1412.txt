Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 72?77,
Prague, June 2007. c?2007 Association for Computational Linguistics
Shallow Semantics in Fast Textual Entailment Rule Learners
Fabio Massimo Zanzotto
DISP
University of Rome ?Tor Vergata?
Roma, Italy
zanzotto@info.uniroma2.it
Marco Pennacchiotti
Computerlinguistik
Universita?t des Saarlandes,
Saarbru?cken, Germany
pennacchiotti@coli.uni-sb.de
Alessandro Moschitti
DIT
University of Trento
Povo di Trento, Italy
moschitti@dit.unitn.it
Abstract
In this paper, we briefly describe two
enhancements of the cross-pair similarity
model for learning textual entailment rules:
1) the typed anchors and 2) a faster compu-
tation of the similarity. We will report and
comment on the preliminary experiments
and on the submission results.
1 Introduction
Results of the second RTE challenge (Bar Haim et
al., 2006) have suggested that both deep semantic
models and machine learning approaches can suc-
cessfully be applied to solve textual entailment. The
only problem seems to be the size of the knowledge
bases. The two best systems (Tatu et al, 2005; Hickl
et al, 2005), which are significantly above all the
others (more than +10% accuracy), use implicit or
explicit knowledge bases larger than all the other
systems. In (Tatu et al, 2005), a deep semantic
representation is paired with a large amount of gen-
eral and task specific semantic rules (explicit knowl-
edge). In (Hickl et al, 2005), the machine learning
model is trained over a large amounts of examples
(implicit knowledge).
In contrast, Zanzotto&Moschitti (2006) proposed
a machine-learning based approach which reaches a
high accuracy by only using the available RTE data.
The key idea is the cross-pair similarity, i.e. a simi-
larity applied to two text and hypothesis pairs which
considers the relations between the words in the two
texts and between the words in the two hypotheses.
This is obtained by using placeholders to link the re-
lated words. Results in (Bar Haim et al, 2006) are
comparable with the best machine learning system
when this latter is trained only on the RTE exam-
ples.
Given the high potential of the cross-pair similar-
ity model, for the RTE3 challenge, we built on it by
including some features of the two best systems: 1)
we go towards a deeper semantic representation of
learning pairs including shallow semantic informa-
tion in the syntactic trees using typed placeholders;
2) we reduce the computational cost of the cross-pair
similarity computation algorithm to allow the learn-
ing over larger training sets.
The paper is organized as follows: in Sec. 2 we
review the cross-pair similarity model and its limits;
in Sec. 3, we introduce our model for typed anchors;
in Sec. 4 we describe how we limit the computa-
tional cost of the similarity; in Sec. 5 we present the
two submission experiments, and in Sec. 6 we draw
some conclusions.
2 Cross-pair similarity and its limits
2.1 Learning entailment rules with syntactic
cross-pair similarity
The cross-pair similarity model (Zanzotto and
Moschitti, 2006) proposes a similarity measure
aiming at capturing rewrite rules from train-
ing examples, computing a cross-pair similarity
KS((T ?,H ?), (T ??,H ??)). The rationale is that if two
pairs are similar, it is extremely likely that they have
the same entailment value. The key point is the use
of placeholders to mark the relations between the
sentence words. A placeholder co-indexes two sub-
structures in the parse trees of text and hypothesis,
72
indicating that such substructures are related. For
example, the sentence pair, ?All companies file an-
nual reports? implies ?All insurance companies file
annual reports?, is represented as follows:
T1 (S (NP: 1 (DT All) (NNS: 1 compa-
nies)) (VP: 2 (VBP: 2 file) (NP: 3 (JJ: 3
annual) (NNS: 3 reports))))
H1 (S (NP: 1 (DT All) (NNP Fortune)
(CD 50) (NNS: 1 companies)) (VP: 2
(VBP: 2 file) (NP: 3 (JJ: 3 annual)
(NNS: 3 reports))))
(E1)
where the placeholders 1 , 2 , and 3 indicate the rela-
tions between the structures of T and of H .
Placeholders help to determine if two pairs share
the same rewriting rule by looking at the subtrees
that they have in common. For example, suppose
we have to determine if ?In autumn, all leaves fall?
implies ?In autumn, all maple leaves fall?. The re-
lated co-indexed representation is:
T2 (S (PP (IN In) (NP (NN: a automn)))
(, ,) (NP: b (DT all) (NNS: b leaves))
(VP: c (VBP: c fall)))
H2 (S (PP (IN In) (NP: a (NN: a automn)))
(, ,) (NP: b (DT all) (NN maple)
(NNS: a leaves)) (VP: c (VBP: c fall)))
(E2)
E1 and E2 share the following subtrees:
T3 (S (NP: x (DT all) (NNS: x )) (VP: y
(VBP: y )))
H3 (S (NP: x (DT all) (NN) (NNS: x ))
(VP: x (VBP: x )))
(R3)
This is the rewrite rule they have in common. Then,
E2 can be likely classified as a valid entailment, as
it shares the rule with the valid entailment E1.
The cross-pair similarity model uses: (1) a tree
similarity measure KT (?1, ?2) (Collins and Duffy,
2002) that counts the subtrees that ?1 and ?2 have
in common; (2) a substitution function t(?, c) that
changes names of the placeholders in a tree accord-
ing to a set of correspondences between placehold-
ers c. Given C as the collection of all correspon-
dences between the placeholders of (T ?,H ?) and
(T ??,H ??), the cross-pair similarity is computed as:
KS((T ?, H ?), (T ??, H ??)) =
maxc?C(KT (t(T ?, c), t(T ??, c)) + KT (t(H ?, c), t(H ??, c)))
(1)
The cross-pair similarity KS , used in a kernel-based
learning model as the support vector machines, al-
lows the exploitation of implicit true and false en-
tailment rewrite rules described in the examples.
2.2 Limits of the syntactic cross-pair similarity
Learning from examples using cross-pair similarity
is an attractive and effective approach. However,
the cross-pair strategy, as any machine learning ap-
proach, is highly sensitive on how the examples are
represented in the feature space, as this can strongly
bias the performance of the classifier.
Consider for example the following text-
hypothesis pair, which can lead to an incorrect rule,
if misused.
T4 ?For my younger readers, Chapman
killed John Lennon more than twenty
years ago.?
H4 ?John Lennon died more than twenty
years ago.?
(E4)
In the basic cross-pair similarity model, the learnt
rule would be the following:
T5 (S (NP: x ) (VP: y (VBD: y ) (NP: z )
(ADVP: k )))
H5 (S (NP: z ) (VP: y (VBD: y )
(ADVP: k )))
(R5)
where the verbs kill and die are connected by the y
placeholder. This rule is useful to classify examples
like:
T6 ?Cows are vegetarian but, to save
money on mass-production, farmers fed
cows animal extracts.?
H6 ?Cows have eaten animal extracts.?
(E6)
but it will clearly fail when used for:
T7 ?FDA warns migraine medicine makers
that they are illegally selling migraine
medicines without federal approval.?
H7 ?Migraine medicine makers declared
that their medicines have been ap-
proved.?
(E7)
where warn and declare are connected as generically
similar verbs.
The problem of the basic cross-pair similarity
measure is that placeholders do not convey the
semantic knowledge needed in cases such as the
above, where the semantic relation between con-
nected verbs is essential.
2.3 Computational cost of the cross-similarity
measure
Let us go back to the computational cost of KS (eq.
1). It heavily depends on the size of C. We de-
fine p? and p?? as the placeholders of, respectively,
(T ?,H ?) and (T ??,H ??). As C is combinatorial with
respect to |p?| and |p??|, |C| rapidly grows. Assigning
placeholders only to chunks helps controlling their
73
number. For example, in the RTE data the number
of placeholders hardly goes beyond 7, as hypothe-
ses are generally short sentences. But, even in these
cases, the number of KT computations grows. As
the trees t(?, c) are obtained from a single tree ?
(containing placeholder) applying different c ? C,
it is reasonable to think that they will share com-
mon subparts. Then, during the iterations of c ?
C, KT (t(??, c), t(???, c)) will compute the similarity
between subtrees that have already been evaluated.
The reformulation of the cross-pair similarity func-
tion we present takes advantage of this.
3 Adding semantic information to
cross-pair similarity
The examples in the previous section show that
the cross-pairs approach lacks the lexical-semantic
knowledge connecting the words in a placeholder.
In the examples, the missed knowledge is the type
of semantic relation between the main verbs. The
relation that links kill and die is not a generic sim-
ilarity, as a WordNet based similarity measure can
suggest, but a more specific causal relation. The
learnt rewrite rule R5 holds only for verbs in such
relation. In facts, it is correctly applied in example
E6, as feed causes eat, but it gives a wrong sugges-
tion in example E7, since warn and declare are only
related by a generic similarity relation.
We then need to encode this information in the
syntactic trees in order to learn correct rules.
3.1 Defining anchor types
The idea of introducing anchor types should be in
principle very simple and effective. Yet, this may be
not the case: simpler attempts to introduce semantic
information in RTE systems have often failed. To
investigate the validity of our idea, we then need to
focus on a small set of relevant relation types, and to
carefully control ambiguity for each type.
A valuable source of relation types among words
is WordNet. We choose to integrate in our system
three important relation standing at the word level:
part-of, antinomy, and verb entailment. We also de-
fine two more general anchor types: similarity and
the surface matching. The first type links words
which are similar according to some WordNet simi-
larity measure. Specifically, this type is intended to
Rank Relation Type Symbol
1. antinomy ?
2. part-of ?
3. verb entailment ?
4. similarity ?
5. surface matching =
Table 1: Ranked anchor types
capture the semantic relations of synonymy and hy-
peronymy. The second type is activated when words
or lemmas match: then, it captures cases in which
words are semantically equivalent. The complete set
of relation types used in the experiments is given in
Table 1.
3.2 Type anchors in the syntactic tree
To learn more correct rewrite rules by using the an-
chor types defined in the previous section, we need
to add this information to syntactic trees. The best
position would be in the same nodes of the anchors.
Also, to be more effective, this information should
be inserted in as many subtrees as possible. Thus we
define the typed-anchor climbing-up rules. We then
implement in our model the following climbing up
rule:
if two typed anchors climb up to the same
node, give precedence to that with the high-
est ranking in Tab. 1.
This rule can be easily showed to be consistent with
common sense intuitions. For an example like ?John
is a tall boy? that does not entail ?John is a short
boy?, our strategy will produce these trees:
(E8)
T8 H8
S ? 3
NP = 1
NNP = 1
John
VP ? 2
AUX
is
NP ? 3
DT
a
JJ ? 2
tall
NN = 3
boy
S ? 3
NP = 1
NNP = 1
John
VP ? 2
AUX
is
NP ? 3
DT
a
JJ ? 2
short
NN = 3
boy
This representation can be used to derive a correct
rewrite rule, such as:
if two fragments have the same syntactic struc-
ture S(NP1, V P (AUX,NP2)), and there is an
antonym type (?) on the S and NP2 , then the
74
c1 = {(a, 1), (b, 2), (c, 3)} c2 = {(a, 1), (b, 2), (d, 3)}
?1 t(?1, c1) t(?1, c2)
X1 a
A2 a
B3 a
w1
a
C4 b
w2
b
D5 d
D6 c
w3
c
C7 d
w4
d
X1 a:1
A2 a:1
B3 a:1
w1
a:1
C4 b:2
w2
b:2
D5 d
D6 c:3
w3
c:3
C7 d
w4
d
X1 a:1
A2 a:1
B3 a:1
w1
a:1
C4 b:2
w2
b:2
D5 d:3
D6 c
w3
c
C7 d:3
w4
d:3
?2 t(?2, c1) t(?2, c2)
X1 1
A2 1
B3 1
m1
1
C4 2
m2
2
D5
D6 3
m3
3
C7
m4
X1 a:1
A2 a:1
B3 a:1
m1
a:1
C4 b:2
m2
b:2
D5
D6 c:3
m3
c:3
C7
m4
X1 a:1
A2 a:1
B3 a:1
m1
a:1
C4 b:2
m2
b:2
D5
D6 d:3
m3
d:3
C7
m4
Figure 1: Tree pairs with placeholders and t(T, c) transformation
entailment does not hold.
4 Reducing computational cost of the
cross-pair similarity computation
4.1 The original kernel function
In this section, we describe more in detail the simi-
larity function KS (Eq. 1). To simplify, we focus on
the computation of only one KT of the kernel sum.
KS(??,???) = maxc?C KT (t(?
?, c), t(???, c)), (2)
where the (??,???) pair can be either (T ?, T ??) or
(H ?,H ??). We apply this simplification since we
are interested in optimizing the evaluation of the
KT with respect to different sets of correspondences
c ? C.
To better explain KS , we need to analyze the role
of the substitution function t(?, c) and to review the
tree kernel function KT .
The aim of t(?, c) is to coherently replace place-
holders in two trees ?? and ??? so that these two trees
can be compared. The substitution is carried out
according to the set of correspondences c. Let p?
and p?? be placeholders of ?? and ???, respectively,
if p?? ? p? then c is a bijection between a subset
p?? ? p? and p??. For example (Fig. 1), the trees ?1
has p1 ={ a , b , c , d } as placeholder set and ?2 has
p2 ={ 1 , 2 , 3 }. In this case, a possible set of corre-
spondence is c1 = {(a, 1), (b, 2), (c, 3)}. In Fig. 1
the substitution function replaces each placeholder
a of the tree ?1with the new placeholder a:1 by
t(?, c) obtaining the transformed tree t(?1, c1), and
each placeholder 1 of ?2 with a:1 . After these sub-
stitutions, the labels of the two trees can be matched
and the similarity function KT is applicable.
KT (? ?, ? ??), as defined in (Collins and Duffy,
2002), computes the number of common subtrees
between ? ? and ? ??.
4.2 An observation to reduce the
computational cost
The above section has shown that the similarity
function KS firstly applies the transformation t(?, c)
and then computes the tree kernel KT . The overall
process can be optimized by factorizing redundant
KT computations.
Indeed, two trees, t(?, c?) and t(?, c??), obtained
by applying two sets of correspondences c?, c?? ? C,
may partially overlap since c? and c?? can share a non-
empty set of common elements. Let us consider the
subtree set S shared by t(?, c?) and t(?, c??) such
that they contain placeholders in c? ? c?? = c, then
t(?, c) = t(?, c?) = t(?, c??) ?? ? S. Therefore if
we apply a tree kernel function KT to a pair (??,???),
we can find a c such that subtrees of ?? and subtrees
of ??? are invariant with respect to c? and c??. There-
fore, KT (t(??, c), t(???, c)) = KT (t(??, c?), t(???, c?))
= KT (t(??, c??), t(???, c??)). This implies that it is
possible to refine the dynamic programming algo-
rithm used to compute the ? matrices while com-
75
puting the kernel KS(??,???).
To better explain this idea let us consider
Fig. 1 that represents two trees, ?1 and ?2,
and the application of two different transforma-
tions c1 = {(a, 1), (b, 2), (c, 3)} and c2 =
{(a, 1), (b, 2), (d, 3)}. Nodes are generally in the
form Xi z where X is the original node label, z is
the placeholder, and i is used to index nodes of the
tree. Two nodes are equal if they have the same node
label and the same placeholder. The first column of
the figure represents the original trees ?1 and ?2.
The second and third columns contain respectively
the transformed trees t(?, c1) and t(?, c2)
Since the subtree of ?1 starting from A2 a con-
tains only placeholders that are in c, in the trans-
formed trees, t(?1, c1) and t(?1, c2), the subtrees
rooted in A2 a:1 are identical. The same happens
for ?2 with the subtree rooted in A2 1 . In the trans-
formed trees, t(?2, c1) and t(?2, c2), subtrees rooted
in A2 a:1 are identical. The computation of KT
applied to the above subtrees gives an identical re-
sult. Then, this computation can be avoided. If cor-
rectly used in a dynamic programming algorithm,
the above observation can produce an interesting de-
crease in the time computational cost. More de-
tails on the algorithm and the decrease in computa-
tional cost may be found in (Moschitti and Zanzotto,
2007).
5 Experimental Results
5.1 Experimental Setup
We implemented the novel cross-similarity kernel
in the SVM-light-TK (Moschitti, 2006) that en-
codes the basic syntactic kernel KT in SVM-light
(Joachims, 1999).
To assess the validity of the typed anchor model
(tap), we evaluated two sets of systems: the plain
and lexical-boosted systems. The plain systems are:
-tap: our tree-kernel approach using typed place-
holders with climbing in the syntactic tree;
-tree: the cross-similarity model described in Sec.2.
Its comparison with tap indicates the effectiveness
of our approaches;
The lexical-boosted systems are:
-lex: a standard approach based on lexical over-
lap. The classifier uses as the only feature the lexi-
cal overlap similarity score described in (Corley and
Mihalcea, 2005);
-lex+tap: these configurations mix lexical overlap
and our typed anchor approaches;
-lex+tree: the comparison of this configuration with
lex+tap should further support the validity of our in-
tuition on typed anchors;
Preliminary experiments have been performed us-
ing two datasets: RTE2 (the 1600 entailment pairs
from the RTE-2 challenge) and RTE3d (the devel-
opment dataset of this challenge). We randomly
divided this latter in two halves: RTE3d0 and
RTE3d1.
5.2 Investigatory Results Analysis and
Submission Results
Table 2 reports the results of the experiments. The
first column indicates the training set whereas the
second one specifies the used test set. The third and
the forth columns represent the accuracy of basic
models: the original tree model and the enhanced
tap model. The latter three columns report the basic
lex model and the two combined models, lex+tree
and lex+tap. The second and the third rows repre-
sent the accuracy of the models with respect to the
first randomly selected half of RTE3d whilst the
last two rows are related to the second half.
The experimental results show some interesting
facts. In the case of the plain systems (tree and tap),
we have the following observations:
- The use of the typed anchors in the model seems
to be effective. All the tap model results are higher
than the corresponding tree model results. This sug-
gests that the method used to integrate this kind of
information in the syntactic tree is effective.
- The claim that using more training material helps
seems not to be supported by these experiments. The
gap between tree and tap is higher when learn-
ing with RTE2 + RTE3d0 than when learning
with RTE30. This supports the claim. How-
ever, the result is not kept when learning with
RTE2 + RTE3d1 with respect to when learning
with RTE31. This suggests that adding not very
specific information, i.e. derived from corpora dif-
ferent from the target one (RTE3), may not help the
learning of accurate rules.
On the other hand, in the case of the lexical-
boosted systems (lex, lex+tree, and lex+tap), we
see that:
76
Train Test tree tap lex lex+tree lex+tap
RTE3d0 RTE3d1 62.97 64.23 69.02 68.26 69.02
RTE2 +RTE3d0 RTE3d1 62.22 62.47 71.03 71.28 71.79
RTE3d1 RTE3d0 62.03 62.78 70.22 70.22 71.22
RTE2 +RTE3d0 RTE3d0 63.77 64.76 71.46 71.22 72.95
Table 2: Accuracy of the systems on two folds of RTE3 development
- There is an extremely high accuracy result for the
pure lex model. This result is counterintuitive. A
model like lex has been likely used by QA or IE
systems to extract examples for the RTE3d set. If
this is the case we may expect that positive and
negative examples should have similar values for
this lex distance indicator. It is then not clear why
this model results in so high accuracy.
- Given the high results of the lex model, the model
lex+tree does not increase the performances.
- On the contrary, the model lex+tap is always better
(or equal) than the lex model. This suggests that
for this particular set of examples the typed anchors
are necessary to effectively use the rewriting rules
implicitly encoded in the examples.
- When the tap model is used in combination with
the lex model, it seems that the claim ?the more
training examples the better? is valid. The gaps
between lex and lex+tap are higher when the RTE2
is used in combination with the RTE3d related set.
Given this analysis we submitted two systems
both based on the lex+tap model. We did two differ-
ent training: one using RTE3d and the other using
RTE2 +RTE3d. Results are reported in the Table
below:
Train Accuracy
RTE3d 66.75%
RTE2 +RTE3d 65.75%
Such results seem too low to be statistically consis-
tent with our development outcome. This suggests
that there is a clear difference between the content
of RTE3d and the RTE3 test set. Moreover, in
contrast with what expected, the system trained with
only the RTE3d data is more accurate than the oth-
ers. Again, this suggests that the RTE corpora (from
all the challenges) are most probably very different.
6 Conclusions and final remarks
This paper demonstrates that it is possible to ef-
fectively include shallow semantics in syntax-based
learning approaches. Moreover, as it happened in
RTE2, it is not always true that more learning ex-
amples increase the accuracy of RTE systems. This
claim is still under investigation.
References
Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Gi-
ampiccolo, Bernardo Magnini, and Idan Szpektor. 2006.
The II PASCAL RTE challenge. In PASCAL Challenges
Workshop, Venice, Italy.
Michael Collins and Nigel Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In Proceedings of ACL02.
Courtney Corley and Rada Mihalcea. 2005. Measuring the se-
mantic similarity of texts. In Proc. of the ACL Workshop
on Empirical Modeling of Semantic Equivalence and Entail-
ment, pages 13?18, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts,
Bryan Rink, and Ying Shi. 2005. Recognizing textual en-
tailment with LCCs GROUNDHOG system. In Proceedings
of the Second PASCAL Challenges Workshop on Recognis-
ing Textual Entailment, Venice, Italy.
Thorsten Joachims. 1999. Making large-scale svm learning
practical. In B. Schlkopf, C. Burges, and A. Smola, editors,
Advances in Kernel Methods-Support Vector Learning. MIT
Press.
Alessandro Moschitti and Fabio Massimo Zanzotto. 2007.
Fast and effective kernels for relational learning from texts.
In Proceedings of the International Conference of Machine
Learning (ICML), Corvallis, Oregon.
Alessandro Moschitti. 2006. Making tree kernels practical
for natural language learning. In Proceedings of EACL?06,
Trento, Italy.
Marta Tatu, Brandon Iles, John Slavick, Adrian Novischi, and
Dan Moldovan. 2005. COGEX at the second recognizing
textual entailment challenge. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Textual En-
tailment, Venice, Italy.
Fabio Massimo Zanzotto and Alessandro Moschitti. 2006. Au-
tomatic learning of textual entailments with cross-pair sim-
ilarities. In Proceedings of the 21st Coling and 44th ACL,
pages 401?408, Sydney, Australia, July.
77
