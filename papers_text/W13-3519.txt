Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 173?182,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Collapsed Variational Bayesian Inference for PCFGs
Pengyu Wang
Department of Computer Science
University of Oxford
Oxford, OX1 3QD, United Kingdom
Pengyu.Wang@cs.ox.ac.uk
Phil Blunsom
Department of Computer Science
University of Oxford
Oxford, OX1 3QD, United Kingdom
Phil.Blunsom@cs.ox.ac.uk
Abstract
This paper presents a collapsed variational
Bayesian inference algorithm for PCFGs
that has the advantages of two dominant
Bayesian training algorithms for PCFGs,
namely variational Bayesian inference and
Markov chain Monte Carlo. In three kinds
of experiments, we illustrate that our al-
gorithm achieves close performance to the
Hastings sampling algorithm while using
an order of magnitude less training time;
and outperforms the standard variational
Bayesian inference and the EM algorithms
with similar training time.
1 Introduction
Probabilistic context-free grammars (PCFGs) are
commonly used in parsing and grammar induction
systems (Johnson, 1998; Collins, 1999; Klein and
Manning, 2003; Matsuzaki et al, 2005). The tra-
ditional method for estimating the parameters of
PCFGs from terminal strings is the inside-outside
(IO) algorithm (Baker, 1979). As a special in-
stance of the Expectation-Maximization (EM) al-
gorithm (Dempster et al, 1977), based on the prin-
ciple of maximum-likelihood estimation (MLE),
the standard IO algorithm learns relatively uni-
form probability distributions for grammars, while
the true distributions can be highly skewed (John-
son et al, 2007). In order to encourage sparse
grammars and avoid overfitting, recent research
for training PCFGs has drifted away from MLE in
favor of Bayesian inference algorithms that make
either deterministic or stochastic approximations
(Kurihara and Sato, 2006; Johnson et al, 2006;
Johnson et al, 2007).
Variational Bayesian inference (VB) (Kurihara
and Sato, 2006) for PCFGs extends EM and places
no constraints when updating parameters in the M
step. By minimising the divergence between the
true posterior and an approximate one in which
the strong dependencies between the parameters
and latent variables are broken, this determinis-
tic algorithm efficiently converges to an inaccu-
rate and only locally optimal solution like EM.
Alternatively, Johnson et al (2007) proposed two
Markov Chain Monte Carlo algorithms for PCFGs
that can reach the true posterior after convergence.
However, it is often difficult to diagnose a sam-
pler?s convergence, and mixing is notoriously slow
for distributions with tightly coupled hidden vari-
ables such as PCFGs, especially when the data sets
are large. Therefore, there remains a challenge for
more efficient, but also accurate and deterministic
inference algorithms for PCFGs.
In this paper, we present a collapsed variational
Bayesian inference (CVB) algorithm for PCFGs.
It has the same computational complexity as the
standard variational Bayesian inference, but offers
almost the same performance as the stochastic al-
gorithms due to its weak assumptions. The idea of
operating VB in the collapsed space was proposed
by Teh et al (2007) and Sung et al (2008), and it
was successfully applied to ?bag-of-words? mod-
els such as latent Dirichlet alocation (LDA) (Teh
et al, 2007) and mixture of Gaussian (Sung et al,
2008), where the latent variables are conditionally
independent given the parameters. By combining
the CVB idea and the dynamic programming tech-
niques used in structurally dependent models, we
deliver a both efficient and accurate algorithm for
training PCFGs and other structured natural lan-
guage models.
The rest of the paper is structured as follows.
We begin with the Bayesian models of PCFGs,
and relate the existing training algorithms. Sec-
tion 3 introduces collapsed variational Bayesian
inference for ?bag-of-words? models (defined in
Section 3.1). We discuss the difficulty in apply-
ing such inference to structured models, followed
by an approximate CVB algorithm for PCFGs.
173
An alternative approach is also included in brief.
In Section 4, we validate our CVB algorithm in
three simple experiments. They are inferring a
sparse grammar that describes the morphology of
the Sotho language (Johnson et al, 2007), unsu-
pervised dependency parsing (Klein and Manning,
2004) and supervised parsing with latent annota-
tions (Matsuzaki et al, 2005). Section 5 concludes
with future work.
2 Approximate inference for PCFGs
2.1 Definitions
A PCFG is a tuple (T,N, S,R, ?), where T , N ,
R and ? are the finite sets of terminals, non-
terminals, rules and parameters respectively, and
S ? N is the start symbol. We adopt a similar
notation to Johnson et al (2007), and assume that
the context free grammar G = (T,N, S,R) is in
Chomsky normal form and the empty string  /? T .
Hence, each rule r ? R takes either the form
A ? BC or A ? w, where A,B,C ? N and
w ? T . Let ?A?? be the probability of derivation
rule A ? ?, where ? ranges over (N ? N) ? T .
In the Bayesian setting, we place Dirichlet priors
with hyperparameters ?A = {?A??} on each ?A
= {?A??}.
Given a corpus of sentences w = (w1, ..., wn)
and the corresponding hidden parse trees t =
(t1, ..., tn), the joint probability distribution of pa-
rameters and variables is1:
P (w, t, ?|?) =P (?|?)
n?
i=1
PG(wi, ti|?)
=
( ?
A?N
PD(?A|?A)
)?
r?R
?fr(t)r
(1)
PD(?A|?A) =
1
B(?A)
?
r?RA
??r?1r
B(?A) =
?
r?RA ?(?r)
?(
?
r?RA ?r)
where fr(t) is the frequency of product rule r in
all the parse trees t, and RA is the set of rules
with left-hand side A. For a Dirichlet distribution
PD(?A|?A), B(?A) is the normalization constant
that can be written in terms of the gamma function
? (i.e. the generalised factorial function).
1Strictly speaking, for each (w, t) pair, if a hidden tree t
is arbitrary, we need to include two delta functions, namely
?(w = yield(t)) and ?(G ?? t). We assume that both delta
functions are true, otherwise the probability of such pair is 0.
2.2 Variational Bayesian inference
The standard inside-outside algorithm for PCFGs
belongs to the general EM class, which is further
a subclass of VB (Beal, 2003). VB maximises the
negative free energy ?F(Q(t, ?)), a lower bound
of the log marginal likelihood of the observation
logP (w|?). This is equivalent to minimising the
Kullback-Leibler divergence.
logP (w|?) ? ?F(Q(t, ?))
=EQ(t,?)[logP (w, t, ?|?)]? EQ(t,?)[logQ(t, ?)]
Q(t, ?) is an approximate posterior, where the pa-
rameters and hidden variables are assumed to be
independent. Thus, it is factorised:
Q(t, ?) ? Q(t)Q(?) (2)
This strong independence assumption allows for
the separate updates of Q(t) and Q(?) iteratively,
optimising the negative free energy ?F(Q(z, ?)).
For the traditional IO algorithm using maximum
likelihood estimation, Q(?) is further assumed to
be degenerate, i.e. Q(?) = ?(? = ??).
E step: Q(t) ? exp(EQ(?)[logP (w, t, ?)])
M step: ?? = argmax
?
P (w, t, ?)
In the E step, we update Q(t). For each tree t,
Q(t) ? PG(w, t|??)
=
?
r?R
(??r)fr(t) (3)
The distribution over parse tree Q(t) is intractable
to compute as its normalization requires summing
over all possible parse trees producing w. We use
dynamic programming to compute inside and out-
side probabilities recursively with the aim of accu-
mulating the expected counts.
E[fA?BC(t)|w] ?
?
0?i<j<k?|w|
POUT(A, i, k)?
?A?BCPIN(B, i, j)PIN(C, j, k)
E[fA?w(t)|w] ?
?
0?i?|w|
POUT(A, i)?
?A?wi?(wi = w)
where PIN(A, i, k) is the inside probability of ob-
servation wi,k = wi, ..., wk given A is the root of
the subtree, and POUT(A, i, k) is the probability of
A spanning (i, k), together with the rest of w.
174
In the M step, we find the optimal ?? based on
the MLE principle:
??A?? =
E[fA??(t)|w]?
A????RA E[fA???(t)|w]
E[fA??(t)|w] =
n?
i=1
E[fA??(ti)|wi]
VB inference is the generalisation of EM in the
sense that it allows arbitrary parametric forms of
Q(?). Thus, the update equation in the M step is:
Q(?) ? exp(EQ(t)[logP (w, t, ?|?)])
By the conjugacy property, the new Q(?) is still
in Dirichlet distribution form except with updated
hyperparameters as shown by Kurihara and Sato
(2006). Instead, Beal (2003) suggested an equiva-
lent mean parameters ??. Based on implementation
of the EM algorithm, we only need a minor modi-
fication in the M step.
??A?? =
m(E[fA??(t)|w]+?A??)
m(
?
A????RA(E[fA???(t)|w]+?A???))
m(x) = exp(?(x))
where ?(x) = ??(x)?x is the digamma function.
From the joint distribution in (1) proportional
to the true posterior, we notice that the parame-
ters and hidden variables are intimately coupled.
Fluctuations in the parameters can induce changes
in the hidden variables and vice-versa. Hence, the
independence assumption in (2) and Figure 1(d)
seems too strong, leading to inaccurate local max-
imums, although it allows for efficient and deter-
ministic updates in EM and VB. The dependencies
between parameters and hidden variables are kept
intact for the remaining algorithms in this paper.
2.3 Markov Chain Monte Carlo
The standard Gibbs sampler for PCFGs iteratively
samples the parameters ? and all the parse trees
t. Its mixing can be slowed by again the strong
dependencies between the parameters and hidden
variables. Instead of reparsing all the hidden trees
t for each sample of ?, collapsed Gibbs sampling
(CGS) improves upon Gibbs sampling in terms of
convergence speed by integrating out the param-
eters, and sampling directly from P (t|w, ?) in a
component-wise manner. Thus, it also deals with
the dependencies exactly.
By using the conjugacy property, we can easily
compute the marginal distribution of w and t:
P (w, t|?) =
?
?
PG(w, t|?)PD(?|?)d?
=
?
A?N
B(fA(t) + ?A)
B(?A)
(4)
where we define fA(t) to be a vector of rule fre-
quencies in t indexed by A ? ? ? RA. Hence,
the conditional distribution for a parse tree ti given
all others is:
P (ti|wi,w?i, t?i, ?) ? P (wi, ti|w?i, t?i, ?)
=
?
A?N
B(fA(t) + ?A)
B(fA(t?i) + ?A)
(5)
where w?i and t?i denote all other sentences and
trees. It is noticeable that sampling a parse tree
from the above conditional distribution is difficult.
The frequencies fA(t) effectively mean that the
production probabilities are dependent on the cur-
rent parse tree ti. That is rule parameters can be
updated on the fly inside a parse tree, which pro-
hibits efficient dynamic programming tricks.
In order to solve this problem, Johnson et al
(2007) proposed a Hastings sampler that specified
an alternative rule probabilities ?H of a proposal
distribution P (ti|wi, ?H), where
?HA?? =
fA??(t?i) + ?A???
A????RA(fA???(t?i) + ?A???)
The rule probabilities ?H are based on the statistics
collected from all other parse trees, and they are
fixed for the conditional distribution of the current
parse tree. Therefore, by using a variant of inside
algorithm (Goodman, 1998), one can efficiently
sample a parse tree, which will be either accepted
or rejected based on the Metropolis choice.
The MCMC based algorithms do not make any
assumptions at all, and they can converge to the
true posterior, either in joint or collapsed space as
shown in Figure 1(b), 1(c). However, one needs to
have experience about the number of samples to
be collected and the burn-in period. For compu-
tationally intensive tasks such as learning PCFGs
from a large corpus, a sufficiently large number
of samples are required to decrease the sampling
variance. Therefore, MCMC algorithms improves
the performance over EM and VB at the cost of
much more training time.
175
Figure 1: Graphical representations of the PCFG with n = 3 trees (a), and the (approximate) posteriors
for Gibbs sampling (b), collapsed Gibbs sampling (c), variational Bayesian inference (d), and collapsed
variational Bayesian inference (e). We use dashed lines to depict the weak dependencies.
3 Collapsed variational Bayesian
inference
3.1 For bag-of-words models
Leveraging the insight that a sampling algorithm
in collapsed space mixes faster than the standard
one, Teh et al (2007) proposed a similar argument
that a VB inference algorithm in collapsed space
is more effective than the standard one. Following
the success in LDA (Teh et al, 2007), a number of
research results have been accumulated around ap-
plying CVB to a variety of ?bag-of-words? mod-
els (Sung et al, 2008; Sato et al, 2012; Wang and
Blei, 2012).
Formally, we define a model to be independent
and identically distributed (i.i.d.) (or informally
?bag-of-words?) if its hidden variables are condi-
tionally independent given the parameters. LDA,
IBM word alignment model 1 and 2, and various
finite mixture models are typical examples.
For an i.i.d. model, integrating out parameters
induces dependencies that spread over many hid-
den variables, and thus the dependency between
any two variables is very weak. This provides an
ideal setting to apply the mean field method (i.e.
fully factorized VB), as its underlying assumption
is that any variable depends on only the summary
statistics collected from other variables called the
field, and any particular variable?s impact on the
field is very small. Hence, the mean field assump-
tion is better satisfied in collapsed space with very
weak dependencies than in joint space with strong
dependencies. As a result, we expect that VB in
collapsed space can achieve more accurate results
than the standard VB, and the results would be
very close to the true posterior.
Even in collapsed space, CVB remains a deter-
ministic algorithm that updates the posterior dis-
tributions over the hidden variables just like VB
and EM. Therefore, we expect CVB to be compu-
tationally efficient as well.
3.2 For structured NLP models
We notice that the basic condition for applying the
CVB algorithm to a specific model is for the model
to be i.i.d., such that the hidden variables are only
weakly dependent in collapsed space, providing an
ideal condition to operate VB. However, the i.i.d.
condition is certainly not true for structured NLP
models such as hidden Markov models (HMMs)
and PCFGs. Given the shape of a parse tree, a
hidden variable is strongly dependent on its par-
ent, siblings and children, and weakly dependent
on the rest. Even worse, to infer a grammar from
terminal strings, we don?t even have access to the
shape of parse trees, let alne analyzing the depen-
dencies of hidden variables inside trees.
Although the PCFG model is not i.i.d. at the
variable level, we can lift the idea of CVB up to
the tree level. As our research domain is those
large scale applications in language processing, a
common feature of those problems is that there
are usually many sentences, each of which has a
hidden parse tree behind it. Hence, we may con-
sider each sentence together with its parse tree to
be drawn i.i.d. from the same set of parameters.
Therefore, at the tree level, a PCFG can be con-
sidered as an i.i.d. model as shown in Figure 1(a)
and thus, it can be fitted in the CVB framework
as described in Section 3.1. We summarise the as-
176
Q(ti) ?
?
A?N
?
A???RA exp(EQ(t?i)[log(
?fA??(ti)?1
j=0 (fA??(t?i) + ?A?? + j))])
?(PA??? fA??? (ti))?1
j=0 exp(EQ(t?i)[log(
?
A????RA(fA??(t?i) + ?A??? + j))])
Figure 2: The exact mean field update in collapsed space for the parse tree ti.
Q(ti) ?
?
r=A???R
( EQ(t?i)[fA??(t?i)] + ?A???
A???(EQ(t?i)[fA???(t?i)] + ?A???)
)fr(ti)
Figure 3: The approximate mean field update in collapsed space for the parse tree ti.
sumptions made by each algorithm in Figure 1(b-
e) before presenting the CVB algorithm formally.
The CVB algorithm for the PCFG model keeps
the dependencies between the parameters and the
hidden parse trees in an exact fashion:
Q(t, ?) = Q(t)Q(?|t)
We factorise Q(t) by breaking only the weak de-
pendencies between parse trees, while keeping the
inside dependencies intact, as we don?t make fur-
ther assumptions about Q(t) for each t.
Q(t) ?
n?
i=1
Q(ti)
By the above factorisations, we compute the neg-
ative variational free energy ?F(Q(t)Q(?|t)) as
follows:
?F(Q(t)Q(?|t))
=EQ(t)Q(?|t)[logP (w, t, ?|?)? logQ(t)Q(?|t)]
=EQ(t)[EQ(?|t)[log P (w, t, ?|?)Q(?|t) ]? logQ(t)]
Maximizing ?F(Q(t)Q(?|t)) requires to update
Q(?|t) and Q(t) in turn. In particular, Q(?|t) is
set equal to the true posterior P (?|w, t, ?):
?F(Q(t)P (?|w, t))
=EQ(t)[EP (?|w,t,?)[log P (w, t, ?|?)P (?|w, t, ?) ]? logQ(t)]
=EQ(t)[logP (w, t|?)? logQ(t)]
Finally, we update the approximate posterior for
each parse tree t by using the mean field method
in the collapsed space:
Q(ti) ? exp(EQ(t?i)[logP (wi, ti|w?i, t?i, ?)])
(6)
The inner term P (wi, ti|w?i, t?i, ?) in the above
equation is just the unnormalized collapsed Gibbs
sampling in (5). Plugging in (5), and expanding
terms such asB(?A) and ?(x), we obtain an exact
computation of Q(ti) in Figure 2.
The exact computation is both intractable and
expensive. The intractability comes from the sim-
ilar problem as in the collapsed Gibbs sampling
that we are unable to calculate the normalisation
term ?ti Q(ti). Hence, we follow Johnson et al(2007) to approximate it by using only the statis-
tics from other sentences, namely ?H and ignoring
the local contribution.
P (wi, ti|w?i, t?i, ?) ?
?
A???R
(
?HA??
)fA??(ti)
(7)
We discuss the accuracy of (7) in Section 3.3. For
those expensive computations of the expected log
counts in Figure 2, Teh et al (2007) and Sung et
al. (2008) suggested the use of a linear Gaussian
approximation based on the law of large numbers.
EQ(t?i)[log(fA??(t?i) + ?A??)]
? log(EQ(t?i)[fA??(t?i)] + ?A??) (8)
Substituting (7) into (6), and employing the linear
approximation, we derive an approximate CVB al-
gorithm as shown in Figure 3. In addition, its form
is much more simplified and interpretable com-
pared with the exact computation in Figure 2.
The surprising similarity between the approxi-
mate CVB update in Figure 3 and E step update in
(3) indicates that the dynamic programming used
in both EM and VB can take over from now. To
run inside-outside recursion, the EM algorithm
employs the parameters ?? based on maximum
likelihood estimation; the VB algorithm employs
177
the mean parameters ??; and our CVB algorithm
employs the parameters ?CVB computed from the
expected counts of all other sentences.
The implementation can be easily achieved by
modifying code of the EM algorithm. We keep
track of the expected counts at global level, sub-
tract the local mean counts for ti before update,
run the inside-outside recursion using ?CVB, and
finally add the updated distribution back into the
global counts. Therefore, we only need to replace
the parameters with the expected counts, and make
update after each sentence; the core of the inside-
outside implementation remains the same.
Our CVB algorithm bears some similarities to
the online EM algorithm with maximum a pos-
terior (MAP) updates (Neal and Hinton, 1998;
Liang and Klein, 2009), but they differ in several
ways. The online EM algorithm updates each tree
ti based on the statistics of all the trees, optimising
the same objective function p(w|?) as the batch
EM algorithm. MAP estimation searches for the
optimal posterior p(w|?)p(?). On the other hand,
our CVB algorithm optimises the data likelihood
p(w). The smoothing effects for the MAP estima-
tion (?A?? ? 1) prevent the use of sparse priors,
whereas the CVB algorithm (?A??) overcomes
such difficulty by parameter integration.
3.3 Discussion
Breaking the weak dependencies between hidden
variables and employing the linear approximation
have been argued to be accurate (Teh et al, 2007;
Sung et al, 2008; Sato and Nakagawa, 2012), and
they are the standard procedures in applying the
CVB algorithms to i.i.d. models.
In our CVB algorithm for PCFGs, we introduce
an extra approximation in (7), which we argue is
accurate. Theoretically, the inaccuracy only oc-
curs when there are repeated rules in a parse tree as
shown in Figure 2, so the same rule seen later uses
a slightly different probability. Even if the inac-
curacy indeed occurs, in our described scenario of
many sentences, the local contribution from a sin-
gle sentence is small compared with the statistics
from all other sentences. Empirically, we replicate
the experiment of Setho language by Johnson et al
(2007) in Section 4.1, and we find that the sampled
trees based on ?H never get rejected, illustrating an
acceptance rate close to 100%, and meaning that
?H is a very accurate Metropolis proposal. Since
all the assumptions made by the CVB algorithm
Figure 4: A fragment of a tree structure
are reasonable and weak, we expect its results to
be close to true posteriors.
3.4 An alternative approach
We briefly sketch an alternative CVB algorithm at
the variable level for completeness.
For a structured NLP model with its shape to
be fixed such as the PCFG with latent annotations
(PCFG-LA) (Matsuzaki et al, 2005) (See defini-
tion in Section 4.3), we can simply ignore all the
dependencies between the hidden variables in the
collapsed space, despite whether they are strong
(for adjacent nodes) or weak (for others). Al-
though it seems that we have made unreasonable
assumptions, it is not transparent which is worse
comparing with the assumptions in the standard
VB. Following this assumption, we can derive a
CVB algorithm similar to the corresponding local
sampling algorithm that samples one hidden vari-
able at a time. For example, the approximate pos-
terior over the subtype of the node A in the above
tree fragment in Figure 4 is updated follows:
q(A = a)
? E[fB?aC(t
?A)] + ?
E[fB(t?A)] + |RB|? ?
E[fa?DE(t?A)] + ?
E[fa(t?A)] + |Ra|?
where we use A to denote the node position, and
a to denote its hidden subtype. q(A = a) means
the probability of node A being in subtype a. In
addition, we need to take into account the distribu-
tions over its adjacent variables. In our case, A is
strongly dependent on nodesB,C,D,E, and only
weakly dependent on other variables (not shown in
the above tree fragment) via global counts, e.g.:
E[fB?aC(t?A)]
=
?
b
?
c
q(B = b)q(C = c)E[fb?ac(t?A)]
However, it is not obvious how to use this alter-
native approach in general, and the performances
of resulting algorithms remain unclear. Therefore,
we implement only the CVB algorithm at the tree
level in Section 3.2 for our experiments.
178
4 Experiments
We conduct three simple experiments to validate
our CVB algorithm for PCFGs. In Section 4.1, we
illustrate the significantly reduced training time
of our CVB algorithm compared to the related
Hastings algorithm; whereas in later two sections,
we demonstrate the increased performance of our
CVB algorithm compared to the corresponding
VB and EM algorithms.
4.1 Inferring sparse grammars
Firstly, we conduct the same experiment of in-
ferring sparse grammars describing the morphol-
ogy of the Sotho language as in Johnson et al
(2007). We use the same corpus of unsegmented
Sotho verb types from CHILDES (MacWhinney
and Snow, 1985), and define the same initial CFG
productions by allowing each non-terminal to emit
any substrings in the corpus as terminals plus five
predefined morphological rules at the top level.
We randomly withhold 10% of the verb types
from the corpus for testing, and use the rest 90%
for training. Both algorithms are evaluated by
their per word perplexity on the test data set with
prior set to 10?5 as suggested by Johnson et al
(2007). We run 5 times with random starts, and
report the averaged results in Figure 5. The Hast-
ings algorithm2 takes roughly 1,000 iterations to
converge, while our CVB algorithm reaches the
convergence even before 10 iterations, consuming
only a fraction of training time (CVB: 1.5 minutes;
Hastings: 20 minutes). As well as little difference
margin in final perplexities shown in Figure 5, we
also evaluated segmentation quality measured by
the F1 scores, and again the difference is trivial
(CVB: 29.8%, Hastings: 31.3%).
4.2 Dependency model with valence
As a second empirical validation of our CVB in-
ference algorithm, we apply it to unsupervised
grammar induction with the popular Dependency
Model with Valence (DMV) (Klein and Manning,
2004). Although the original maximum likelihood
formulation of this model has long since been sur-
passed by more advanced models, all of the state-
of-the-art approaches to unsupervised dependency
parsing still have DMV at their core (Headden III
et al, 2009; Blunsom and Cohn, 2010; Spitkovsky
et al, 2012). As such we believe demonstrating
2Annealing is not used in order to facilitate the perplexity
calculation in the test set.
0 5 10 15 20
67
89
1011
12
Number of Iterations (CVB)
Test P
erplexi
ty
 
 CVBHastings
0 500 1000 1500 2000Number of Iterations (Hastings)
Figure 5: Perplexities averaged over 5 runs on the
extracted corpus of Sotho verbs.
improved inference on this core model will enable
future improvements to more complex models.
We evaluate a Dirichlet-Multinomial formula-
tion of DMV in the standard fashion by train-
ing on sections 2-21 and testing on section 23 of
the Penn. Wall Street Journal treebank (Marcus
et al, 1993). We initialise our models using the
original harmonic initialiser (Klein and Manning,
2004). Figure 6 displays the directed accuracy re-
sults for DMV model trained with CVB and VB
with Dirichlet ? parameters of either 1 or 0.1, as
well as the previously reported MLE result. In
both cases we see superior results for CVB infer-
ence, providing evidence that CVB may be a bet-
ter choice of inference algorithm for Bayesian for-
mulations of generative grammar induction mod-
els such as DMV.
4.3 PCFG with latent annotations
The vanilla PCFGs estimated by simply taking the
empirical rule frequencies off treebanks are not ac-
curate models to capture the syntactic structures in
most natural languages as demonstrated by Char-
niak (1997) and Klein and Manning (2003). Our
third experiment is to apply the CVB algorithm
to the PCFGs with latent annotations (PCFGs-
LA) (Matsuzaki et al, 2005), where each non-
terminal symbol is augmented with hidden vari-
ables (or subtypes). Given a parsed corpus, train-
ing a PCFG-LA yields a finer grammar with the
automatically induced features represented by the
subtypes. For example, an augmented binary rule
takes the form A[a] ? B[b]C[c], where a, b, c ?
[1, H] are the hidden subtypes, and H denotes the
number of subtypes for each non-terminal.
179
1.0 0.10.45
0.46
0.47
0.48
0.49
Bayesian Priors
F1 Sco
res
 
 EMVBCVB
Figure 6: DMV trained by EM, VB and CVB. F1
scores on section 23, WSJ.
Objective Precision Recall F1 Exact
EM 75.84 72.92 74.35 11.13
VB 76.98 73.32 75.11 11.49
CVB 78.85 76.98 77.90 12.56
Table 1: PCFG-LA (2 subtypes) trained by EM,
VB and CVB. Precision, Recall, F1 scores, Exact
match scores on section 23, WSJ.
We follow the same experiment set-up as DMV,
and report the results on the section 23, using the
best grammar tested on the development set (sec-
tion 22) from 5 random runs for each algorithm.
We adopt Petrov et al (2006)?s methods to process
the data: right binarising and replacing infrequent
words with the generic unknown word marker for
English, and to initialise: adding 1% randomness
to the parameters ?0 to start the EM training. We
calculate the expected counts from (G, ?0) to ini-
tialise our VB and CVB algorithms.
In Table 1, when each non-terminal is split into
2 hidden subtypes, we show that our CVB algo-
rithm outperforms the EM and VB algorithms in
terms of all the evaluation objectives. We also
investigate the hidden state space with higher di-
mensions (4,8,16 subtypes), and find our CVB al-
gorithm retains the advantages over the other two,
whereas the VB algorithm fails to surpass the EM
algorithm as reported in Figure 7.
5 Conclusion and future work
In this paper we have presented a collapsed vari-
ational Bayesian inference algorithm for PCFGs.
We make use of the common scenario where the
data consists of multiple short sentences, such that
1 2 4 8 16
0.65
0.7
0.75
0.8
0.85
0.9
Number of Hidden States
F1 Sco
res
 
 
EMVBCVB
Figure 7: PCFG-LA (2,4,8,16 subtypes) trained by
EM, VB and CVB. F1 scores on section 23, WSJ.
we can ignore the local dependencies induced by
collapsing the parameters. The assumptions in our
CVB algorithm are reasonable for a range of pars-
ing applications and justified in three tasks by the
empirical observations: it produces more accurate
results than standard VB, and close results to sam-
pling with significantly less training time.
While not state-of-the-art, the models we have
demonstrated our CVB algorithm on underlie a
number of high performance grammar induction
and parsing systems (Cohen and Smith, 2009;
Blunsom and Cohn, 2010; Petrov and Klein, 2007;
Liang et al, 2007). Therefore, our work naturally
extends to employing our CVB algorithm in more
advanced models such as hierarchical splitting and
merging system used in Berkeley parser (Petrov
and Klein, 2007), and generalising our CVB al-
gorithm to the non-parametric models such as tree
substitution grammars (Blunsom and Cohn, 2010)
and infinite PCFGs (Liang et al, 2007).
We have also sketched an alternative CVB al-
gorithm which makes a harsher independence as-
sumption for the latent variables but then requires
no approximation of the variational posterior by
performing inference individually for each parse
node. This model breaks some strong dependen-
cies within parse trees, but if we expect the pos-
terior to be highly skewed by using a sparse prior,
the product of constituent marginals may well be a
good approximation. We leave further exploration
of this algorithm for future work.
Acknowledgments
We would like to thank Mark Johnson for the data
used in Section 4.1 and valuable advice.
180
References
James K. Baker. 1979. Trainable grammars for speech
recognition. The Journal of the Acoustical Society
of America, 65(S1):S132.
Matthew Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, The
Gatsby Computational Neuroscience Unit, Univer-
sity College London.
Phil Blunsom and Trevor Cohn. 2010. Unsupervised
induction of tree substitution grammars for depen-
dency parsing. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1204?1213, Cambridge, MA, Oc-
tober. Association for Computational Linguistics.
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
Proceedings of the fourteenth national conference
on artificial intelligence and ninth conference on
Innovative applications of artificial intelligence,
AAAI?97/IAAI?97, pages 598?603. AAAI Press.
Shay B. Cohen and Noah A. Smith. 2009. Shared
logistic normal distributions for soft parameter ty-
ing in unsupervised grammar induction. In NAACL
?09: Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 74?82, Morristown, NJ,
USA. Association for Computational Linguistics.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistics Soci-
ety, Series B, 39(1):1?38.
Joshua T. Goodman. 1998. Parsing inside-out.
Ph.D. thesis, Cambridge, MA, USA. Adviser-Stuart
Shieber.
William P. Headden III, Mark Johnson, and David Mc-
Closky. 2009. Improving unsupervised depen-
dency parsing with richer contexts and smoothing.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 101?109, Boulder, Colorado, June.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2006. Adaptor grammars: A framework for
specifying compositional nonparametric bayesian
models. In NIPS.
Mark Johnson, Thomas Griffiths, and Sharon Gold-
water. 2007. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Proc. of the 7th Inter-
national Conference on Human Language Technol-
ogy Research and 8th Annual Meeting of the NAACL
(HLT-NAACL 2007), pages 139?146, Rochester,
New York, April.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24:613?632.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Dan Klein and Christopher D. Manning. 2004.
Corpus-based induction of syntactic structure: mod-
els of dependency and constituency. In ACL ?04:
Proceedings of the 42nd Annual Meeting on Associ-
ation for Computational Linguistics, page 478.
Kenichi Kurihara and Taisuke Sato. 2006. Variational
bayesian grammar induction for natural language.
In Proceedings of the 8th international conference
on Grammatical Inference: algorithms and appli-
cations, ICGI?06, pages 84?96, Berlin, Heidelberg.
Springer-Verlag.
Percy Liang and Dan Klein. 2009. Online EM for un-
supervised models. In Proceedings HLT/NAACL.
Percy Liang, Slav Petrov, Michael Jordan, and Dan
Klein. 2007. The infinite PCFG using hierarchi-
cal Dirichlet processes. In Proc. of the 2007 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2007), pages 688?697, Prague,
Czech Republic.
Brian MacWhinney and Catherine Snow. 1985. The
child language data exchange system. Child Lan-
guage.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2005. Probabilistic cfg with latent annotations.
In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, ACL ?05,
pages 75?82, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Radford Neal and Geoffrey E. Hinton. 1998. A view of
the em algorithm that justifies incremental, sparse,
and other variants. In Learning in Graphical Mod-
els, pages 355?368. Kluwer Academic Publishers.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
181
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, ACL-44,
pages 433?440, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Issei Sato and Hiroshi Nakagawa. 2012. Rethinking
collapsed variational bayes inference for LDA. In
Proceedings of the 29th International Conference on
Machine Learning.
Issei Sato, Kenichi Kurihara, and Hiroshi Nakagawa.
2012. Practical collapsed variational bayes infer-
ence for hierarchical dirichlet process. In Proceed-
ings of the 18th ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
KDD ?12, pages 105?113, New York, NY, USA.
ACM.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2012. Three dependency-and-boundary
models for grammar induction. In Proceedings of
the 2012 Conference on Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL 2012).
Jaemo Sung, Zoubin Ghahramani, and Sung-Yang
Bang. 2008. Latent-space variational Bayes. IEEE
Trans. Pattern Anal. Mach. Intell., 30(12), Decem-
ber.
Yee Whye Teh, David Newman, and Max Welling.
2007. A collapsed variational Bayesian inference
algorithm for latent Dirichlet alocation. In In Ad-
vances in Neural Information Processing Systems,
volume 19.
Chong Wang and David Blei. 2012. Truncation-free
stochastic variational inference for bayesian non-
parametric models. In Neural Information Process-
ing Systems.
182
