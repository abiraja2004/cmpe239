Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 1?9,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Online Active Learning for Cost Sensitive Domain Adaptation
Min Xiao and Yuhong Guo
Department of Computer and Information Sciences
Temple University
Philadelphia, PA 19122, USA
{minxiao,yuhong}@temple.edu
Abstract
Active learning and domain adaptation are
both important tools for reducing labeling
effort to learn a good supervised model in
a target domain. In this paper, we inves-
tigate the problem of online active learn-
ing within a new active domain adapta-
tion setting: there are insufficient labeled
data in both source and target domains,
but it is cheaper to query labels in the
source domain than in the target domain.
Given a total budget, we develop two cost-
sensitive online active learning methods, a
multi-view uncertainty-based method and
a multi-view disagreement-based method,
to query the most informative instances
from the two domains, aiming to learn a
good prediction model in the target do-
main. Empirical studies on the tasks of
cross-domain sentiment classification of
Amazon product reviews demonstrate the
efficacy of the proposed methods on re-
ducing labeling cost.
1 Introduction
In many application domains, it is difficult or ex-
pensive to obtain labeled data to train supervised
models. It is critical to develop effective learning
methods to reduce labeling effort or cost. Active
learning and domain adaptation are both impor-
tant tools for reducing labeling cost on learning
good supervised prediction models. Active learn-
ing reduces the cost of labeling by selecting the
most informative instances to label, whereas do-
main adaptation obtains auxiliary label informa-
tion by exploiting labeled data in related domains.
Combining the efforts from both areas to further
reduce the labeling cost is an important research
direction to explore.
In this paper, we consider online active learn-
ing with domain adaptations. Online learning has
been widely studied (Borodin and El-Yaniv, 1998)
due to its advantages of low memory requirement
and fast computation speed. Dredze and Crammer
(2008) applied online learning on domain adap-
tation and proposed to combine multiple similar
source domains to perform online learning for the
target domain, which provides a new opportunity
for conducting active learning with domain adap-
tation. Online active learning with domain adap-
tation, to our knowledge, has just gained atten-
tion recently and has been addressed in (Rai et al,
2010; Saha et al, 2011). The active online do-
main adaptation methods developed in (Rai et al,
2010; Saha et al, 2011) leverage information from
the source domain by domain adaptation to intelli-
gently query labels for instances only in the target
domain in an online fashion with a given budget.
They assumed a large amount of labeled data is
readily available in the source domain.
In this work, we however tackle online active
learning with domain adaptation in a different set-
ting, where source domains with a large amount of
free labeled data are not available. Instead we as-
sume there are very few labeled instances in both
the source and target domains and labels in both
domains can be acquired with a cost. Moreover,
we assume the annotation cost for acquiring la-
bels in the source domain is much lower than the
annotation cost in the target domain. This is a
practical setting in many domain adaptation sce-
narios. For example, one aims to learn a good
review classification model for high-end comput-
ers. It may be expensive to acquire labels for such
product reviews. However, but it might be rela-
tively much cheaper (but not free) to acquire la-
bels for reviews on movies or restaurants. In such
an active learning scenario, will a source domain
with lower annotation cost still be helpful for re-
ducing the labeling cost required to learn a good
prediction model in the target domain? Our re-
search result in this paper will answer this ques-
1
Figure 1: The framework of online active learning
with domain adaptation.
tion. Specifically, we address this online active do-
main adaptation problem by extending the online
active learning framework in (Cesa-Bianchi et al,
2006) to consider active label acquirement in both
domains. We first initialize the prediction model
based on the initial labeled data in both the source
and target domains (LS and LT ). Then in each
round of the online learning, we receive one un-
labeled instance from each domain (DS and DT ),
on which we need to decide whether to query la-
bels. Whenever a label is acquired, we update the
prediction model using the newly labeled instance
if necessary. The framework of this online active
learning setting is demonstrated in Figure 1. We
exploit multi-view learning principles to measure
the informativeness of instances and propose two
cost-sensitive online active learning methods, a
multi-view uncertainty-based method and a multi-
view disagreement-based method, to acquire la-
bels for the most informative instances. Our em-
pirical studies on the tasks of cross-domain sen-
timent classification of Amazon product reviews
show the proposed methods can effectively ac-
quire the most informative labels given a budget,
comparing to alternative methods.
2 Related Work
The proposed work in this paper involves re-
search developments in multiple areas, including
online active learning, active domain adaptation
and multi-view active learning. In this section, we
will cover the most related work in the literature.
Online active learning has been widely stud-
ied in the literature, including the perceptron-type
methods in (Cesa-Bianchi et al, 2006; Monteleoni
and Ka?a?ria?inen, 2007; Dasgupta et al, 2009).
Cesa-Bianchi et al (2006) proposed a selective
sampling perceptron-like method (CBGZ), which
serves as a general framework of online active
learning. Monteleoni and Ka?a?ria?inen (2007) em-
pirically studied online active learning algorithms,
including the CBGZ, for optical character recogni-
tion applications. Dasgupta et al (2009) analyzed
the label complexity of the perceptron algorithm
and presented a combination method of a modifi-
cation of the perceptron update with an adaptive
filtering rule. Our proposed online active learn-
ing methods are placed on an extended framework
of (Cesa-Bianchi et al, 2006), by incorporating
domain adaptation and multi-view learning tech-
niques in an effective way.
Active domain adaptation has been studied in
(Chan and Ng, 2007; Rai et al, 2010; Saha et al,
2011; Li et al, 2012). Chan and Ng (2007) pre-
sented an early study on active domain adaptation
and empirically demonstrated that active learn-
ing can be successfully applied on out-of-domain
word sense disambiguation systems. Li et al
(2012) proposed to first induce a shared subspace
across domains and then actively label instances
augmented with the induced latent features. On-
line active domain adaptation, however, has only
been recently studied in (Rai et al, 2010; Saha
et al, 2011). Nevertheless, the active online do-
main adaptation method (AODA) and its vari-
ant method, domain-separator based AODA (DS-
AODA), proposed in these works assume a large
amount of labeled data in the source domain and
conduct online active learning only in the target
domain, which is different from our problem set-
ting in this paper.
Multi-view learning techniques have recently
been employed in domain adaptation (Tur, 2009;
Blitzer et al, 2011; Chen et al, 2011). In par-
ticular, instead of using data with conditional in-
dependent views assumed in standard multi-view
learning (Blum and Mitchell, 1998), Blitzer et al
(2011) and Chen et al (2011) randomly split
original features into two disjoint subsets to pro-
duce two views, and demonstrate the usefulness
of multi-view learning with synthetic two views.
On the other hand, multi-view active learning has
been studied in (Muslea et al, 2000, 2002; Wang
and Zhou, 2008, 2010). These works all suggest
to query labels for contention points (instances
on which different views predict different labels).
Our proposed methods will exploit this multi-view
2
principle and apply it in our multi-view online ac-
tive domain adaptation setting.
In addition, our proposed work is also related
to cost-sensitive active learning. But different
from the traditional cost-sensitive active learn-
ing, which assumes multiple oracles with different
costs exist for the same set of instances (Donmez
and Carbonell, 2008; Arora et al, 2009), we as-
sume two oracles, one for the source domain and
one for the target domain. Overall, the problem we
study in this paper is novel, practical and impor-
tant. Our research will demonstrate a combination
of advances in multiple research areas.
3 Multi-View Online Active Learning
with Domain Adaptation
Our online active learning is an extension of the
online active perceptron learning framework of
(Cesa-Bianchi et al, 2006; Rai et al, 2010) in the
cost-sensitive online active domain adaption set-
ting. We will present two multi-view online ac-
tive methods in this section under the framework
shown in Figure 1.
Assume we have a target domain (DT ) and a
related source domain (DS) with a few labeled in-
stances, LT and LS , in each of them respectively.
The instances in the two domains are drawn from
the same input space but with two different distri-
butions specified by each domain. An initial pre-
diction model (w0) can then be trained with the
current labeled data from both domains. Many
domain adaptation techniques (Sugiyama, 2007;
Blitzer et al, 2011) can be used for training here.
However, for simplicity of demonstrating the ef-
fectiveness of online active learning strategies, we
use vanilla Perceptron to train the initial prediction
model on all labeled instances, as the perceptron
algorithm is widely used in various works (Saha
et al, 2011) and can be combined seamlessly with
the online perceptron updates. It can be viewed as
a simple supervised domain adaptation training.
The very few initial labeled instances are far
from being sufficient to train a good prediction
model in the target domain. Additional labeled
data needs to be acquired to reach a reasonable
prediction model. However it takes time, money,
and effort to acquire labels in all problem domains.
For simplicity of demonstration, we use money to
measure the cost and effort of labeling instances
in each domain. Assume the cost of labeling one
instance in the source domain is cs and the cost
of labeling one instance in the target domain is ct,
where ct > cs. Note the condition ct > cs is
one criterion to be guaranteed when selecting use-
ful source domains. It does not make sense to se-
lect source domains with more expensive labeling
cost. Given a budget B, we need to make wise de-
cisions about which instances to query in the on-
line learning setting. We aim to learn the best pre-
diction model in the target domain with the labels
purchased under the given budget.
Then online active learning will be conducted in
a sequence of rounds. In each round r, we will re-
ceive two randomly sampled unlabeled instances
in parallel, xs,r and xt,r, one from each domain,
xs,r ? DS and xt,r ? DT . Active learning strate-
gies will be used to judge the informativeness of
the two instances in a cost-sensitive manner and
decide whether to query labels for any one of them
to improve the prediction model in the target do-
main. After new labels being acquired, we use the
newly labeled instances to make online perceptron
updates if the true labels are different from the pre-
dicted labels.
In this work, we focus on binary prediction
problems where the labels have binary values, y ?
{+1,?1}. We adopt the online perceptron-style
learning model of (Cesa-Bianchi et al, 2006) for
the online updates of the supervised perceptron
model. Moreover, we extend principles of multi-
view active learning into our online active learn-
ing framework. As we introduced before, syn-
thetic multi-views produced by splitting the orig-
inal feature space into disjoint subsets have been
demonstrated effective in a few previous work
(Blitzer et al, 2011; Chen et al, 2011). We adopt
this idea to generate two views of the instances
in both domains by randomly splitting the com-
mon feature space into two disjoint feature sub-
sets, such that xs,r = {x(1)s,r ,x(2)s,r} and xt,r =
{x(1)t,r ,x
(2)
t,r }. Thus the initial prediction model will
include two predictors (f (1), f (2)) with model pa-
rameters (w(1)0 ,w
(2)
0 ), each trained on one view
of the labeled data using the perceptron algorithm.
Correspondingly, the online updates will be made
on the two predictors.
The critical challenge of this cost-sensitive on-
line active learning problem nevertheless lies in
how to select the most informative instances for
labeling. Based on different measurements of
instance informativeness, we propose two on-
line active learning algorithms: a Multi-view
3
Uncertainty-based instance Selection (MUS) al-
gorithm and a Multi-view Disagreement-based
instance Selection (MDS) algorithm for cost-
sensitive online active domain adaptation, which
we will present below.
3.1 Multi-View Uncertainty-based Instance
Selection Algorithm
We use the initial model (f (1), f (2)), trained on
the two views of the initial labeled data and rep-
resented by the model parameters (w(1)0 ,w
(2)
0 ), as
the starting point of the online active learning.
In each round r of the online active learning,
we receive two instances xs,r = {x(1)s,r ,x(2)s,r} and
xt,r = {x(1)t,r ,x
(2)
t,r }, one for each domain. For the
received instances, we need to make two sequen-
tial decisions:
1. Between the instance (xs,r) from the source
domain and the instance (xt,r) from the tar-
get domain, which one should we select for
further consideration?
2. For the selected instance, do we really need
to query its label?
We answer the first question based on the label-
ing cost ratio, ct/cs, from the two domains and
define the following probability
Pc = e??(ct/cs?1) (1)
where ? is a domain preference weighting param-
eter. Then with a probability Pc we select the tar-
get instance xt,r and with a probability 1 ? Pc we
select the source instance xs,r. Our intuition is that
one should query the less expensive source domain
more frequently. Thus more labeled instances can
be collected within the fix budget. On the other
hand, the more useful and relevant but expensive
instances from the target domain should also be
queried at a certain rate.
For the selected instance x?,r, we then use a
multi-view uncertainty strategy to decide whether
to query its label. We first calculate the prediction
confidence and predicted labels of the selected in-
stance based on the current predictors trained from
each view
mk = |w(k)?x(k)?,r |, y?(k) = sign(w(k)?x(k)?,r ) (2)
where k = 1 or 2, standing for each of the two
views. If the two predictors disagree over the pre-
diction label, i.e., y?(1) 6= y?(2), the selected in-
stance is a contention point and contains useful
Algorithm 1 MUS Algorithm
Input: B, Pc, cs, ct, b,
initial model (w(1)0 ,w
(2)
0 )
Output: prediction model (w(1),w(2))
Initialize: w(1) = w(1)0 , w(2) = w
(2)
0
for each round r = 1, 2, ? ? ? do
Receive two instances xs,r, xt,r
Sample d ? U(0, 1)
if B < ct then d = 1 end if
if d > Pc then x?,r= xs,r, c = cs
else x?,r= xt,r, c = ct
end if
Compute m1,m2, y?(1), y?(2) by Eq.(2)
Compute z1, z2 by Eq.(3)
if z1 = 1 or z2 = 1 or y?(1) 6= y?(2) then
Query label y for x?,r, B = B? c
Update (w(1),w(2)) by Eq (4)
end if
if B < cs then break end if
end for
information for at least one predictor, according
to the principle of multi-view active learning. We
then decide to pay a cost (cs or ct) to query its la-
bel. Otherwise, we make the query decision based
on the two predictors? uncertainty (i.e., the inverse
of the prediction confidence mk) over the selected
instance. Specifically, we sample two numbers,
one for each view, according to
zk = Bernoulli(b/(b + mk)) (3)
where b is a prior hyperparameter, specifying the
tendency of querying labels. In our experiments,
we use b = 0.1. If either z1 = 1 or z2 = 1,
which means that at least one view is uncertain
about the selected instance, we will query for the
label y. The prediction model will be updated us-
ing the new labeled instances when the true labels
are different from the predicted ones; i.e.,
w(k) = w(k) + (yx(k)?,r )I[y 6= y?(k)] (4)
for k = 1, 2, where I[?] is an indicator function.
This multi-view uncertainty-based instance selec-
tion algorithm (MUS) is given in Algorithm 1.
3.2 Multi-View Disagreement-based Instance
Selection Algorithm
MUS is restrained to query at most one instance
at each round of the online active learning. In
this section, we present an alternative multi-view
4
disagreement-based instance selection algorithm
(MDS) within the same framework.
In each round r of the online active learning,
given the two instances xs,r and xt,r we received,
the MDS algorithm evaluates both instances for
potential label acquisition using the multi-view in-
formation provided by the two per-view predic-
tors. Let y?(1)s and y?(2)s denote the predicted la-
bels of instance xs,r produced by the two predic-
tors according to Eq (2). Similarly let y?(1)t and
y?(2)t denote the predicted labels of instance xt,r.
Follow the principle suggested in the multi-view
active learning work (Muslea et al, 2000, 2002;
Wang and Zhou, 2008, 2010) that querying labels
for contention points (instances on which different
views predict different labels) can lead to superior
information gain than querying uncertain points,
we identify the non-redundant contention points
from the two domains for label acquisition.
Specifically, there are three cases: (1) If only
one of the instances is a contention point, we query
its label with probability Pc (Eq (1)) when the in-
stance is from the target domain, and query its la-
bel with probability 1 ? Pc when the instance is
from the source domain. (2) If both instances are
contention points, i.e., y?(1)s 6= y?(2)s and y?(1)t 6= y?
(2)
t ,
but the predicted labels for the two instances are
the same, i.e., y?(k)s = y?(k)t for k = 1, 2, it suggests
the two instances contain similar information with
respect to the prediction model and we only need
to query one of them. We then select the instance
in a cost-sensitive manner stated in the MUS algo-
rithm by querying the target instance with a prob-
ability Pc and querying the source instance with a
probability 1 ? Pc. (3) If both instances are con-
tention points but with different predicted labels, it
suggests the two instances contain complementary
information with respect to the prediction model,
and we thus query labels for both of them.
For any new labeled instance from the target
domain or the source domain, we update the pre-
diction model of each review using Equation (4)
when the acquired true label is different from the
predicted label. The overall MDS algorithm is
given in Algorithm 2.
3.3 Multi-View Prediction
After the training process, we use the two predic-
tors to predict labels of the test instances from
the target domain. Given a test instance xt =
Algorithm 2 MDS Algorithm
Input: B, Pc, cs, ct, b,
initial model (w(1)0 ,w
(2)
0 )
Output: prediction model (w(1),w(2))
Initialize: w(1) = w(1)0 , w(2) = w
(2)
0
for each round r = 1, 2, ? ? ? do
Receive two instances xs,r, xt,r
Compute y?(1)s , y?(2)s , y?(1)t , y?
(2)
t by Eq (2)
Let ds = I[y?(1)s = y?(2)s ], dt = I[y?(1)t = y?
(2)
t ]
Let qs = 0, qt = 0
if B < ct then dt = 0 end if
Sample d ? U(0, 1)
if ds = 1 and dt = 0 then
if d > Pc then qs = 1 end if
else if ds = 0 and dt = 1 then
if d ? Pc then qt = 1 end if
else if ds = 1 and dt = 1
if y?(1)s = y?(1)t then
if d > Pc then qs = 1 else qt = 1 end if
else qs = 1, qt = 1
end if
end if
if qs = 1 then
Query label ys for xs,r, B = B? cs
Update (w(1),w(2)) by Eq (4)
end if
if B < ct then qt = 0 end if
if qt = 1 then
Query label yt for xt,r, B = B? ct
Update (w(1),w(2)) by Eq (4)
end if
if B < cs then break end if
end for
(x(1)t ,x
(2)
t ), we use the predictor that have larger
prediction confidence to determine its label y?.
The prediction confidence of the kth view predic-
tor on xt is defined as the absolute prediction value
|w(k)?x(k)t |. We then select the most confident
predictor for this instance as
k? = argmax
k?{1,2}
|w(k)?x(k)t | (5)
The predicted label is final computed as
y? = sign(w(k?)?x(k
?)
t ) (6)
With this multi-view prediction on the test data,
the multi-view strengths can be exploited in the
testing phase as well.
5
4 Experiments
In this section, we present the empirical evaluation
of the proposed online active learning methods on
the task of sentiment classification comparing to
alternative baseline methods. We first describe the
experimental setup, and then present the results
and discussions.
4.1 Experimental Setup
Dataset For the sentiment classification task, we
use the dataset provided in (Prettenhofer and Stein,
2010). The dataset contains reviews with four
different language versions and in three domains,
Books (B), DVD (D) and Music (M). Each domain
contains 2000 positive reviews and 2000 negative
reviews, with a term-frequency (TF) vector rep-
resentation. We used the English version and con-
structed 6 source-target ordered domain pairs from
the original 3 domains: B2D, D2B, B2M, M2B,
D2M, M2D. For example, for the task of B2D, we
use the Books reviews as the source domain and
the DVD reviews as the target domain. For each
pair of domains, we built a unigram vocabulary
over the combined 4000 source reviews and 4000
target reviews. We further preprocessed the data
by removing features that appear less than twice
in either domain, replacing TF with TFIDF, and
normalizing the attribute values into [0, 1].
Approaches In the experiments, we mainly
compared the proposed MUS and MDS algorithms
with the following three baseline methods. (1)
MTS (Multi-view Target instance Selection): It
is a target-domain variant of the MUS algorithm,
and selects the most uncertain instance received
from the target domain to query according to the
procedure introduced for MUS method. (2) TCS
(Target Contention instance Selection): It is a
target-domain variant of the MDS algorithm, and
uses multi-view predictors to query contention in-
stances received from the target domain. (3) SUS
(Single-view Uncertainty instance Selection): It
selects target vs source instances according to Pc
(see Eq.(1)), and then uses uncertainty measure to
make query decision. This is a single view vari-
ant of the MUS algorithm. In the experiments, we
used ? = 1 for the Pc computation in Eq.(1).
4.2 Classification Accuracy
We first conducted experiments over the 6 do-
main adaptation tasks constructed from the sen-
timent classification data with a fixed cost ratio
ct/cs = 3. We set cs = 1 and ct = 3. Given a bud-
getB = 900, we measure the classification perfor-
mance of the prediction model learned by each on-
line active learning method during the process of
budget being used. We started with 50 labeled in-
stances from the source domain and 10 labeled in-
stances from the target domain. The classification
performance is measured over 1000 test instances
from the target domain. All other instances are
used as inputs in the online process. We repeated
the experiments 10 times using different random
online instance input orders. The average results
are reported in Figure 2.
The results indicate the proposed two algo-
rithms, MUS and MDS, in general greatly out-
perform the other alternative methods. The SUS
method, which is a single-view variant of MUS,
presents very poor performance across all 6 tasks
comparing to the other multi-view based methods,
which demonstrates the efficacy of the multi-view
instance selection mechanism. Among the multi-
view based active learning methods, the MTS
method and TCS method, which only query labels
for more relevant but expensive instances from
the target domain, demonstrated inferior perfor-
mance, comparing to their cost-sensitive counter-
parts, MUS and MDS, respectively. This suggests
that a cheaper source domain is in general helpful
on reducing the labeling cost for learning a good
prediction model in the target domain and our pro-
posed active learning strategies are effective.
4.3 Domain Divergence
To further validate and understand our experimen-
tal results on the sentiment classification data, we
evaluated the domain divergence over the three
pairs of domains we used in the experiments
above. Note, if the domain divergence is very
small, it will be natural that a cheaper source do-
main should help on reducing the labeling cost in
the target domain. If the domain divergence is very
big, the space of exploring a cheaper source do-
main will be squeezed.
The divergence of two domains can be mea-
sured using the A-distance (Ben-David et al,
2006). We adopted the method of (Rai et al, 2010)
to proximate the A-distance. We train a linear
classifier over all 8000 instances, 4000 instances
from each domain, to separate the two domains.
The average per-instance hinge-loss for this sepa-
rator subtracted from 1 was used as the estimate
6
0 200 400 600 800
64
66
68
70
72
74
76
78
B2D
Total budget
A
cc
ur
ac
y
 
 
MTS
TCS
SUS
MUS
MDS
0 200 400 600 800
62
64
66
68
70
72
74
76
B2M
Total budget
A
cc
ur
ac
y
 
 
MTS
TCS
SUS
MUS
MDS
0 200 400 600 800
60
62
64
66
68
70
72
74
76
D2M
Total budget
A
cc
ur
ac
y
 
 
MTS
TCS
SUS
MUS
MDS
(a) (b) (c)
0 200 400 600 800
62
64
66
68
70
72
74
76
D2B
Total budget
A
cc
ur
ac
y
 
 
MTS
TCS
SUS
MUS
MDS
0 200 400 600 800
64
66
68
70
72
74
76
M2B
Total budget
A
cc
ur
ac
y
 
 
MTS
TCS
SUS
MUS
MDS
0 200 400 600 800
64
66
68
70
72
74
76
78
M2D
Total budget
A
cc
ur
ac
y
 
 
MTS
TCS
SUS
MUS
MDS
(d) (e) (f)
Figure 2: Online active learning results over the 6 domain adaptation tasks for sentiment classification,
with a total budget B=900 and a fixed cost ratio ct/cs = 3.
of the proxy A-distance. A score of 1 means per-
fectly separable distributions and 0 means the two
distributions from the two domains are identical.
In general, a higher score means a larger diver-
gence between the two domains.
Table 1: Proxy A-distance over domain pairs.
Domains A-distance
Books vs. DVD 0.7221
Books vs. Music 0.8562
DVD vs. Music 0.7831
The proxy A-distances over the 3 domain pairs
from the sentiment classification dataset are re-
ported in Table 1. It shows that all the 3 pairs
of domains are reasonably far apart. This justi-
fied the effectiveness of the online active domain
adaptation methods we developed and the results
we reported above. It suggests the applicability of
the proposed active learning scheme is not bound
to the existence of highly similar source domains.
Moreover, the A-distance between Books and Mu-
sic is the largest among the three pairs. Thus it
is most challenging to exploit the source domain
in the adaptation tasks, B2M and M2B. This ex-
plains the good performance of the target-domain
method TCS on these two tasks. Nevertheless, the
proposed MUS and MDS maintained consistent
good performance even on these two tasks.
4.4 Robustness to Cost Ratio
We then studied the empirical behavior of the pro-
posed online active domain adaptation algorithms
with different cost ratio values ct/cs.
Given a fixed budget B = 900, we set cs = 1
and run a few sets of experiments on the senti-
ment classification data by setting ct as different
values from {1, 2, 3, 4}, under the same experi-
mental setting described above. In addition to
the five comparison methods used before, we also
added a baseline marker, SCS, which is a source-
domain variant of the MDS algorithm and queries
contention instances from only the source domain.
The final classification performance of the predic-
tion model learned with each approach is recorded
7
1 1.5 2 2.5 3 3.5 4
66
68
70
72
74
76
78
80
82
B2D
ct/cs
A
cc
ur
ac
y
 
 
MTS
TCS
SCS
SUS
MUS
MDS
1 1.5 2 2.5 3 3.5 4
64
66
68
70
72
74
76
78
B2M
ct/cs
A
cc
ur
ac
y
 
 
MTS
TCS
SCS
SUS
MUS
MDS
1 1.5 2 2.5 3 3.5 4
64
66
68
70
72
74
76
78
D2M
ct/cs
A
cc
ur
ac
y
 
 
MTS
TCS
SCS
SUS
MUS
MDS
(a) (b) (c)
1 1.5 2 2.5 3 3.5 4
66
68
70
72
74
76
78
D2B
ct/cs
A
cc
ur
ac
y
 
 
MTS
TCS
SCS
SUS
MUS
MDS
1 1.5 2 2.5 3 3.5 4
62
64
66
68
70
72
74
76
78
M2B
ct/cs
A
cc
ur
ac
y
 
 
MTS
TCS
SCS
SUS
MUS
MDS
1 1.5 2 2.5 3 3.5 465
70
75
80
M2D
ct/cs
A
cc
ur
ac
y
 
 
MTS
TCS
SCS
SUS
MUS
MDS
(d) (e) (f)
Figure 3: Online active learning results over the 6 domain adaptation tasks for sentiment classification,
with different cost ratio values ct/cs = {1, 2, 3, 4}.
after the whole budget being used. The average
results over 10 runs are reported in Figure 3.
We can see that: (1) With the increasing of
the labeling cost in the target domain, the perfor-
mance of all methods except SCS decreases since
the same budget can purchase fewer labeled in-
stances from the target domain. (2) The three cost-
sensitive methods (SUS, MUS, and MDS), which
consider the labeling cost when making query de-
cisions, are less sensitive to the cost ratios than the
MTS and TCS methods, whose performance de-
grades very quickly with the increasing of ct/cs.
(3) It is reasonable that when ct/cs is very big,
the SCS, which simply queries source instances,
produces the best performance. But the proposed
two cost-sensitive active learning methods, MUS
and MDS, are quite robust to the cost ratios across
a reasonable range of ct/cs values, and outper-
form both source-domain only and target-domain
only methods. When ct = cs, the proposed cost-
sensitive methods automatically favor target in-
stances and thus achieve similar performance as
TCS. When ct becomes much larger than cs, the
proposed cost-sensitive methods automatically ad-
just to favor cheaper source instances and maintain
their good performance.
5 Conclusion
In this paper, we investigated the online active do-
main adaptation problem in a novel but practical
setting where we assume labels can be acquired
with a lower cost in the source domain than in the
target domain. We proposed two multi-view on-
line active learning algorithms, MUS and MDS, to
address the proposed problem. The proposed al-
gorithms exploit multi-view active learning learn-
ing principles to measure the informativeness of
instances and select instances in a cost-sensitive
manner. Our empirical studies on the task of cross-
domain sentiment classification demonstrate the
efficacy of the proposed methods. This research
shows that a cheaper source domain can help on
reducing labeling cost for learning a good pre-
diction model in the related target domain, with
proper designed active learning algorithms.
8
References
S. Arora, E. Nyberg, and C. P. Rose?. Estimating
annotation cost for active learning in a multi-
annotator environment. In Proceedings of the
NAACL-HLT 2009 Workshop on Active Learn-
ing for Natural Language Processing, 2009.
S. Ben-David, J. Blitzer, K. Crammer, and
F. Pereira. Analysis of representations for do-
main adaptation. In Advances in Neural Infor-
mation Processing Systems (NIPS), 2006.
J. Blitzer, D. Foster, and S. Kakade. Domain adap-
tation with coupled subspaces. In Proceedings
of the Conference on Artificial Intelligence and
Statistics (AISTATS), 2011.
A. Blum and T. Mitchell. Combining labeled and
unlabeled data with co-training. In Proceedings
of the Conference on Computational Learning
Theory (COLT), 1998.
A. Borodin and R. El-Yaniv. Online computation
and competitive analysis. Cambridge Univer-
sity Press, 1998.
N. Cesa-Bianchi, C. Gentile, and L. Zaniboni.
Worst-case analysis of selective sampling for
linear classification. Journal of Machine Learn-
ing Research (JMLR), 7:1205?1230, 2006.
Y. Chan and H. Ng. Domain adaptation with active
learning for word sense disambiguation. In Pro-
ceedings of the Annual Meeting of the Assoc. of
Computational Linguistics (ACL), 2007.
M. Chen, K. Weinberger, and J. Blitzer. Co-
training for domain adaptation. In Advances in
Neural Information Processing Systems (NIPS),
2011.
S. Dasgupta, A. T. Kalai, and C. Monteleoni.
Analysis of perceptron-based active learning.
Journal of Machine Learning Research (JMLR),
10:281?299, 2009.
P. Donmez and J. G. Carbonell. Proactive learn-
ing: cost-sensitive active learning with multi-
ple imperfect oracles. In Proceedings of the
ACM Conference on Information and knowl-
edge management (CIKM), 2008.
M. Dredze and K. Crammer. Online methods for
multi-domain learning and adaptation. In Pro-
ceedings of the Conf. on Empirical Methods in
Natural Language Processing (EMNLP), 2008.
L. Li, X. Jin, S. Pan, and J. Sun. Multi-domain ac-
tive learning for text classification. In Proceed-
ings of the ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining
(KDD), 2012.
C. Monteleoni and M. Ka?a?ria?inen. Practical on-
line active learning for classification. In Pro-
ceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, Online Learn-
ing for Classification Workshop, 2007.
I. Muslea, S. Minton, and C. Knoblock. Selective
sampling with redundant views. In Proceedings
of the National Conference on Artificial Intelli-
gence (AAAI), 2000.
I. Muslea, S. Minton, and C. A. Knoblock. Ac-
tive + semi-supervised learning = robust multi-
view learning. In Proceedings of the In-
ternational Conference on Machine Learning
(ICML), 2002.
P. Prettenhofer and B. Stein. Cross-language text
classification using structural correspondence
learning. In Proceedings of the Annual Meeting
for the Association of Computational Linguis-
tics (ACL), 2010.
P. Rai, A. Saha, H. Daume? III, and S. Venkata-
subramanian. Domain adaptation meets active
learning. In Proceedings of the North American
Chapter of the Association for Computational
Linguistics (NAACL), 2010.
A. Saha, P. Rai, H. Daume? III, S. Venkata-
subramanian, and S. DuVall. Active super-
vised domain adaptation. In Proceedings of
the European Conference on Machine Learning
(ECML), 2011.
M. Sugiyama. Direct importance estimation with
model selection and its application to covariate
shift adaptation. In Advances in Neural Infor-
mation Processing Systems (NIPS), 2007.
G. Tur. Co-adaptation: Adaptive co-training for
semi-supervised learning. In Proceedings of the
IEEE Inter. Conference on Acoustics, Speech
and Signal Processing (ICASSP), 2009.
W. Wang and Z. Zhou. On multi-view active learn-
ing and the combination with semi-supervised
learning. In Proceedings of the international
conference on Machine learning (ICML), 2008.
W. Wang and Z. Zhou. Multi-view active learn-
ing in the non-realizable case. In Advances in
Neural Information Processing Systems (NIPS),
2010.
9
