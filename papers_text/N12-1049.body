2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 446?455,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Parsing Time: Learning to Interpret Time Expressions
Gabor Angeli
Stanford University
Stanford, CA 94305
angeli@stanford.edu
Christopher D. Manning
Stanford University
Stanford, CA 94305
manning@stanford.edu
Daniel Jurafsky
Stanford University
Stanford, CA 94305
jurafsky@stanford.edu
Abstract
We present a probabilistic approach for learn-
ing to interpret temporal phrases given only a
corpus of utterances and the times they ref-
erence. While most approaches to the task
have used regular expressions and similar lin-
ear pattern interpretation rules, the possibil-
ity of phrasal embedding and modification in
time expressions motivates our use of a com-
positional grammar of time expressions. This
grammar is used to construct a latent parse
which evaluates to the time the phrase would
represent, as a logical parse might evaluate to
a concrete entity. In this way, we can employ
a loosely supervised EM-style bootstrapping
approach to learn these latent parses while
capturing both syntactic uncertainty and prag-
matic ambiguity in a probabilistic framework.
We achieve an accuracy of 72% on an adapted
TempEval-2 task ? comparable to state of the
art systems.
1 Introduction
Temporal resolution is the task of mapping from
a textual phrase describing a potentially complex
time, date, or duration to a normalized (grounded)
temporal representation. For example, possibly
complex phrases such as the week before last are
often more useful in their grounded form ? e.g.,
January 1 - January 7.
The dominant approach to this problem in previ-
ous work has been to use rule-based methods, gen-
erally a combination of regular-expression matching
followed by hand-written interpretation functions.
In general, it is appealing to learn the interpre-
tation of temporal expressions, rather than hand-
building systems. Moreover, complex hierarchical
temporal expressions, such as the Tuesday before
last or the third Wednesday of each month, and am-
biguous expressions, such as last Friday, are diffi-
cult to handle using deterministic rules and would
benefit from a recursive and probabilistic phrase
structure representation. Therefore, we attempt to
learn a temporal interpretation system where tempo-
ral phrases are parsed by a grammar, but this gram-
mar and its semantic interpretation rules are latent,
with only the input phrase and its grounded interpre-
tation given to the learning system.
Employing probabilistic techniques allows us to
capture ambiguity in temporal phrases in two impor-
tant respects. In part, it captures syntactic ambigu-
ity ? e.g., last Friday the 13th bracketing as either
[last Friday] [the 13th], or last [Friday the 13th].
This also includes examples of lexical ambiguity ?
e.g., two meanings of last in last week of November
versus last week. In addition, temporal expressions
often carry a pragmatic ambiguity. For instance, a
speaker may refer to either the next or previous Fri-
day when he utters Friday on a Sunday. Similarly,
next week can refer to either the coming week or the
week thereafter.
Probabilistic systems furthermore allow propaga-
tion of uncertainty to higher-level components ? for
example recognizing that May could have a num-
ber of non-temporal meanings and allowing a sys-
tem with a broader contextual scope to make the fi-
nal judgment. We implement a CRF to detect tem-
poral expressions, and show our model?s ability to
act as a component in such a system.
We describe our temporal representation, fol-
lowed by the learning algorithm; we conclude with
experimental results showing our approach to be
competitive with state of the art systems.
446
2 Related Work
Our approach draws inspiration from a large body of
work on parsing expressions into a logical form. The
latent parse parallels the formal semantics in previ-
ous work, e.g., Montague semantics. Like these rep-
resentations, a parse ? in conjunction with the refer-
ence time ? defines a set of matching entities, in this
case the grounded time. The matching times can be
thought of as analogous to the entities in a logical
model which satisfy a given expression.
Supervised approaches to logical parsing promi-
nently include Zelle and Mooney (1996), Zettle-
moyer and Collins (2005), Kate et al. (2005), Zettle-
moyer and Collins (2007), inter alia. For exam-
ple, Zettlemoyer and Collins (2007) learn a mapping
from textual queries to a logical form. This logical
form importantly contains all the predicates and en-
tities used in their parse. We loosen the supervision
required in these systems by allowing the parse to be
entirely latent; the annotation of the grounded time
neither defines, nor gives any direct cues about the
elements of the parse, since many parses evaluate to
the same grounding. To demonstrate, the grounding
for a week ago could be described by specifying a
month and day, or as a week ago, or as last x ? sub-
stituting today?s day of the week for x. Each of these
correspond to a completely different parse.
Recent work by Clarke et al. (2010) and Liang et
al. (2011) similarly relax supervision to require only
annotated answers rather than full logical forms. For
example, Liang et al. (2011) constructs a latent parse
similar in structure to a dependency grammar, but
representing a logical form. Our proposed lexi-
cal entries and grammar combination rules can be
thought of as paralleling the lexical entries and pred-
icates, and the implicit combination rules respec-
tively in this framework. Rather than querying from
a finite database, however, our system must com-
pare temporal expression within an infinite timeline.
Furthermore, our system is run using neither lexical
cues nor intelligent initialization.
Related work on interpreting temporal expres-
sions has focused on constructing hand-crafted in-
terpretation rules (Mani and Wilson, 2000; Saquete
et al., 2003; Puscasu, 2004; Grover et al., 2010). Of
these, HeidelTime (Stro?tgen and Gertz, 2010) and
SUTime (Chang and Manning, 2012) provide par-
ticularly strong competition.
Recent probabilistic approaches to temporal reso-
lution include UzZaman and Allen (2010), who em-
ploy a parser to produce deep logical forms, in con-
junction with a CRF classifier. In a similar vein,
Kolomiyets and Moens (2010) employ a maximum
entropy classifier to detect the location and temporal
type of expressions; the grounding is then done via
deterministic rules.
3 Representation
We define a compositional representation of time;
a type system is described in Section 3.1 while the
grammar is outlined in Section 3.2 and described in
detail in Sections 3.3 and 3.4.
3.1 Temporal Expression Types
We represent temporal expressions as either a
Range, Sequence, or Duration. We describe these,
the Function type, and the miscellaneous Number
and Nil types below:
Range [and Instant] A period between two dates
(or times). This includes entities such as Today,
1987, or Now. We denote a range by the variable
r. We maintain a consistent interval-based theory of
time (Allen, 1981) and represent instants as intervals
with zero span.
Sequence A sequence of Ranges, not necessarily
occurring at regular intervals. This includes enti-
ties such as Friday, November 27th, or last
Friday. A Sequence is a tuple of three elements
s = (rs,?s, ?s):
1. rs(i): The ith element of a sequence, of type
Range. In the case of the sequence Friday,
rs(0) corresponds to the Friday in the current
week; rs(1) is the Friday in the following week,
etc.
2. ?s: The distance between two elements in the
sequence ? approximated if this distance is not
constant. In the case of Friday, this distance
would be a week.
3. ?s: The containing unit of an element of a se-
quence. For example, ?Friday would be the
Range corresponding to the current week. The
sequence index i ? Z, from rs(i), is defined
447
relative to rs(0) ? the element in the same con-
taining unit as the reference time.
We define the reference time t (Reichenbach,
1947) to be the instant relative to which times are
evaluated. For the TempEval-2 corpus, we approxi-
mate this as the publication time of the article. While
this is conflating Reichenbach?s reference time with
speech time, it is a useful approximation.
To contrast with Ranges, a Sequence can rep-
resent a number of grounded times. Nonetheless,
pragmatically, not all of these are given equal weight
? an utterance of last Friday may mean either of the
previous two Fridays, but is unlikely to ground to
anything else. We represent this ambiguity by defin-
ing a distribution over the elements of the Sequence.
While this could be any distribution, we chose to ap-
proximate it as a Gaussian.
In order to allow sharing parameters between any
sequence, we define the domain in terms of the index
of the sequence rather than of a constant unit of time
(e.g., seconds). To illustrate, the distribution over
April would have a much larger variance than the
distribution over Sunday, were the domains fixed.
The probability of the ith element of a sequence thus
depends on the beginning of the range rs(i), the ref-
erence time t, and the distance between elements of
the sequence ?s. We summarize this in the equation
below, with learned parameters ? and ?:
Pt(i) =
? 0.5
?=?0.5
N?,?
(
rs(i)? t
?s
+ ?
)
(1)
Figure 1 shows an example of such a distribution;
importantly, note that moving the reference time be-
tween two elements dynamically changes the prob-
ability assigned to each.
Duration A period of time. This includes entities
like Week, Month, and 7 days. We denote a du-
ration with the variable d.
We define a special case of the Duration type to
represent approximate durations, identified by their
canonical unit (week, month, etc). These are used
to represent expressions such as a few years or some
days.
Function A function of arity less than or equal to
two representing some general modification to one
-2
11/13
-1
11/20
-0.3
t
?s
1
12/4
2
12/11
Reference time
P (11/20) = ? ?0.5?1.5 f(x)
0
11/27
ff
ff
Figure 1: An illustration of a temporal distribution, e.g.,
Sunday. The reference time is labeled as time t between
Nov 20 and Nov 27; the probability that this sequence
is referring to Nov 20 is the integral of the marked area.
The domain of the graph are the indices of the sequence;
the distribution is overlaid with mean at the (normalized)
reference time t/?s; in our case ?s is a week. Note
that the probability of an index changes depending on the
exact location of the reference time.
of the above types. This captures semantic entities
such as those implied in last x, the third x [of y],
or x days ago. The particular functions and their
application are enumerated in Table 2.
Other Types Two other types bear auxiliary roles
in representing temporal expressions, though they
are not directly temporal concepts. In the grammar,
these appear as preterminals only.
The first of these types is Number ? denoting
a number without any temporal meaning attached.
This comes into play representing expressions such
as 2 weeks. The other is the Nil type ? denoting
terms which are not directly contributing to the se-
mantic meaning of the expression. This is intended
for words such as a or the, which serve as cues with-
out bearing temporal content themselves. The Nil
type is lexicalized with the word it generates.
Omitted Phenomena The representation de-
scribed is a simplification of the complexities of
time. Notably, a body of work has focused on
reasoning about events or states relative to temporal
expressions. Moens and Steedman (1988) describes
temporal expressions relating to changes of state;
Condoravdi (2010) explores NPI licensing in
temporal expressions. Broader context is also not
448
Range
f(Duration) : Range
catRight
next
Duration
Number
Numn?100
2
Duration
Day
days(a)
catRight(t, 2D )
catRight(t,?)
next
2D
Num(2)
2
1D
days(b)
Figure 2: The grammar ? (a) describes the CFG parse of
the temporal types. Words are tagged with their nontermi-
nal entry, above which only the types of the expressions
are maintained; (b) describes the corresponding combi-
nation of the temporal instances. The parse in (b) is de-
terministic given the grammar combination rules in (a).
directly modeled, but rather left to systems in which
the model would be embedded. Furthermore, vague
times (e.g., in the 90?s) represent a notable chunk
of temporal expressions uttered. In contrast, NLP
evaluations have generally not handled such vague
time expressions.
3.2 Grammar Formalism
Our approach builds on the assumption that natural
language descriptions of time are compositional in
nature. Each word attached to a temporal phrase is
usually compositionally modifying the meaning of
the phrase. To demonstrate, we consider the expres-
sion the week before last week. We can construct a
meaning by applying the modifier last to week ? cre-
ating the previous week; and then applying before to
week and last week.
We construct a paradigm for parsing temporal
phrases consisting of a standard PCFG over tempo-
ral types with each parse rule defining a function to
apply to the child nodes, or the word being gener-
ated. At the root of the tree, we recursively apply
the functions in the parse tree to obtain a final tem-
poral value. One can view this formalism as a rule-
to-rule translation (Bach, 1976; Allen, 1995, p. 263),
or a constrained Synchronous PCFG (Yamada and
Knight, 2001).
Our approach contrasts with common approaches,
such as CCG grammars (Steedman, 2000; Bos et
al., 2004; Kwiatkowski et al., 2011), giving us more
flexibility in the composition rules. Figure 2 shows
an example of the grammar.
Formally, we define our temporal grammar
G = (?, S,V,W,R, ?). The alphabet ? and start
symbol S retain their usual interpretations. We de-
fine a set V to be the set of types, as described in
Section 3.1 ? these act as our nonterminals. For each
v ? V we define an (infinite) set Wv corresponding
to the possible instances of type v. Concretely, if
v = Sequence, our set Wv ? W could contain el-
ements corresponding to Friday, last Friday, Nov.
27th, etc. Each node in the tree defines a pair (v, w)
such that w ? Wv, with combination rules defined
over v and function applications performed on w.
A rule R ? R is defined as a pair
R =
(
vi ? vjvk, f : (Wvj ,Wvk)?Wvi
)
. The
first term is our conventional PCFG rule over the
types V . The second term defines the function to
apply to the values returned recursively by the child
nodes. Note that this definition is trivially adapted
for the case of unary rules.
The last term in our grammar formalism denotes
the rule probabilities ?. In line with the usual in-
terpretation, this defines a probability of applying a
particular rule r ? R. Importantly, note that the
distribution over possible groundings of a temporal
expression are not included in the grammar formal-
ism. The learning of these probabilities is detailed
in Section 4.
3.3 Preterminals
We define a set of preterminals, specifying their
eventual type, as well as the temporal instance it pro-
duces when its function is evaluated on the word it
generates (e.g., f(day) = Day). A distinction is
made in our description between entities with con-
tent roles versus entities with a functional role.
The first ? consisting of Ranges, Sequences, and
Durations ? are listed in Table 1. A total of 62 such
preterminals are defined in the implemented system,
corresponding to primitive entities often appearing
in newswire, although this list is easily adaptable to
449
Function Description Signature(s)
shiftLeft Shift a Range or Sequence left by a Duration f : S,D? S; f : R,D? R
shiftRight Shift a Range or Sequence right by a Duration f : S,D? S; f : R,D? R
shrinkBegin Take the first Duration of a Range/Sequence f : S,D? S; f : R,D? R
shrinkEnd Take the last Duration of a Range/Sequence f : S,D? S; f : R,D? R
catLeft Take Duration units after the end of a Range f : R,D? R
catRight Take Duration units before the start of a Range f : R,D? R
moveLeft1 Move the origin of a sequence left by 1 f : S? S
moveRight1 Move the origin of a sequence right by 1 f : S? S
nth x of y Take the nth Sequence in y (Day of Week, etc) f : Number? S
approximate Make a Duration approximate f : D? D
Table 2: The functional preterminals of the grammar; R, S, and D denote Ranges Sequences and Durations respec-
tively. The name, a brief description, and the type signature of the function (as used in parsing) are given. Described
in more detail in Section 3.4, the functions are most easily interpreted as operations on either an interval or sequence.
Type Instances
Range Past, Future, Yesterday,
Tomorrow, Today, Reference,
Year(n), Century(n)
Sequence Friday, January, . . .
DayOfMonth, DayOfWeek, . . .
EveryDay, EveryWeek, . . .
Duration Second, Minute, Hour,
Day, Week, Month, Quarter,
Year, Decade, Century
Table 1: The content-bearing preterminals of the gram-
mar, arranged by their types. Note that the Sequence
type contains more elements than enumerated here; how-
ever, only a few of each characteristic type are shown here
for brevity.
fit other domains. It should be noted that the expres-
sions, represented in Typewriter, have no a pri-
ori association with words, denoted by italics; this
correspondence must be learned. Furthermore, enti-
ties which are subject to interpretation ? for example
Quarter or Season ? are given a concrete inter-
pretation. The nth quarter is defined by evenly split-
ting a year into four; the seasons are defined in the
same way but with winter beginning in December.
The functional entities are described in Table 2,
and correspond to the Function type. The majority
of these mirror generic operations on intervals on a
timeline, or manipulations of a sequence. Notably,
like intervals, times can be moved (3 weeks ago) or
their size changed (the first two days of the month),
or a new interval can be started from one of the end-
points (the last 2 days). Additionally, a sequence can
be modified by shifting its origin (last Friday), or
taking the nth element of the sequence within some
bound (fourth Sunday in November).
The lexical entry for the Nil type is tagged with the
word it generates, producing entries such as Nil(a),
Nil(November), etc. The lexical entry for the Num-
ber type is parameterized by the order of magnitude
and ordinality of the number; e.g., 27th becomes
Number(101,ordinal).
3.4 Combination Rules
As mentioned earlier, our grammar defines both
combination rules over types (in V) as well as a
method for combining temporal instances (in Wv ?
W). This method is either a function application of
one of the functions in Table 2, a function which is
implicit in the text (intersection and multiplication),
or an identity operation (for Nils). These cases are
detailed below:
? Function application, e.g., last week. We apply
(or partially apply) a function to an argument
on either the left or the right: f(x, y)x or x
f(x, y). Furthermore, for functions of arity 2
taking a Range as an argument, we define a rule
treating it as a unary function with the reference
time taking the place of the second argument.
? Intersecting two ranges or sequences, e.g.,
450
Input (w,t) ( Last Friday the 13 th , May 16 2011 )
LatentparseR
moveLeft1( FRI ) ? 13th
moveLeft1( FRI )
moveLeft1(?)
last
FRI
friday
13th
Nilthe
the
13th
13th
Output ?? May 13 2011
Figure 3: An overview of the system architecture. Note
that the parse is latent ? that is, it is not annotated in the
training data.
November 27th. The intersect function treats
both arguments as intervals, and will return an
interval (Range or Sequence) corresponding to
the overlap between the two.1
? Multiplying a Number with a Duration, e.g., 5
weeks.
? Combining a non-Nil and Nil element with no
change to the temporal expression, e.g., a week.
The lexicalization of the Nil type allows the
algorithm to take hints from these supporting
words.
We proceed to describe learning the parameters of
this grammar.
4 Learning
We present a system architecture, described in Fig-
ure 3. We detail the inference procedure in Sec-
tion 4.1 and training in Section 4.2.
4.1 Inference
To provide a list of candidate expressions with their
associated probabilities, we employ a k-best CKY
parser. Specifically, we implement Algorithm 3 de-
scribed in Huang and Chiang (2005), providing an
O(Gn3k log k) algorithm with respect to the gram-
mar size G, phrase length n, and beam size k. We
set the beam size to 2000.
1In the case of complex sequences (e.g., Friday the 13th) an
A? search is performed to find overlapping ranges in the two
sequences; the origin rs(0) is updated to refer to the closest
such match to the reference time.
Revisiting the notion of pragmatic ambiguity, in
a sense the most semantically complete output of
the system would be a distribution ? an utterance of
Friday would give a distribution over Fridays rather
than a best guess of its grounding. However, it is of-
ten advantageous to ground to a concrete expression
with a corresponding probability. The CKY k-best
beam and the temporal distribution ? capturing syn-
tactic and pragmatic ambiguity ? can be combined to
provide a Viterbi decoding, as well as its associated
probability.
We define the probability of a syntactic parse
y making use of rules R ? R as P (y) =
P (w1, . . . wn;R) =
?
i?j,k?R P (j, k | i). As de-
scribed in Section 3.1, we define the probability of
a grounding relative to reference time t and a par-
ticular syntactic interpretation Pt(i|y). The prod-
uct of these two terms provides the probability of
a grounded temporal interpretation; we can obtain a
Viterbi decoding by maximizing this joint probabil-
ity:
Pt(i, y) = P (y)? Pt(i|y) (2)
This provides us with a framework for obtaining
grounded times from a temporal phrase ? in line with
the annotations provided during training time.
4.2 Training
We present an EM-style bootstrapping approach to
training the parameters of our grammar jointly with
the parameters of our Gaussian temporal distribu-
tion.
Our TimEM algorithm for learning the parame-
ters for the grammar (?), jointly with the temporal
distribution (? and ?) is given in Algorithm 1. The
inputs to the algorithm are the initial parameters ?,
?, and ?, and a set of training instances D. Further-
more, the algorithm makes use of a Dirichlet prior ?
on the grammar parameters ?, as well as a Gaussian
prior N on the mean of the temporal distribution ?.
The algorithm outputs the final parameters ??, ??
and ??.
Each training instance is a tuple consisting of
the words in the temporal phrase w, the annotated
grounded time ??, and the reference time of the ut-
terance t. The input phrase is tokenized according
to Penn Treebank guidelines, except we additionally
451
Algorithm 1: TimEM
Input: Initial parameters ?, ?, ?; data
D = {(w, ??, t)}; Dirichlet prior ?,
Gaussian prior N
Output: Optimal parameters ??, ??, ??
while not converged do1
(M??, M??,?) := E-Step (D,?,?,?)2
(?, ?, ?) := M-Step (M??, M??,?)3
end4
return (?s, ?, ?)5
begin E-Step(D,?,?,?)6
M?? = []; M??,? = []7
for (w, ??, t) ? D do8
m?? = []; m??,? = []9
for y ? k-bestCKY(w, ?) do10
if p = P?,?(?? | y, t) > 0 then11
m?? += (y, p); m??,? += (i, p)12
end13
end14
M? += normalize(m??)15
M??,? += normalize(m??,?)16
end17
return M?18
end19
begin M-Step(M??,M??,?)20
?? := bayesianPosterior(M??, ?)21
?? := mlePosterior(M??,?)22
?? := bayesianPosterior(M??,?, ??, N )23
return (??, ??, ??)24
end25
split on the characters ?-? and ?/,? which often de-
limit a boundary between temporal entities. Beyond
this preprocessing, no language-specific information
about the meanings of the words are introduced, in-
cluding syntactic parses, POS tags, etc.
The algorithm operates similarly to the EM algo-
rithms used for grammar induction (Klein and Man-
ning, 2004; Carroll and Charniak, 1992). How-
ever, unlike grammar induction, we are allowed a
certain amount of supervision by requiring that the
predicted temporal expression match the annotation.
Our expected statistics are therefore more accurately
our normalized expected counts of valid parses.
Note that in conventional grammar induction, the
expected sufficient statistics can be gathered analyt-
ically from reading off the chart scores of a parse.
This does not work in our case for two reasons. In
part, we would like to incorporate the probability
of the temporal grounding in our feedback probabil-
ity. Additionally, we are only using parses which are
valid candidates ? that is, the parses which ground to
the correct time ?? ? which we cannot establish until
the entire expression is parsed. The expected statis-
tics are thus computed non-analytically via a beam
on both the possible parses (line 10) and the pos-
sible temporal groundings of a given interpretation
(line 11).
The particular EM updates are the standard up-
dates for multinomial and Gaussian distributions
given fully observed data. In the multinomial case,
our (unnormalized) parameter updates, with Dirich-
let prior ?, are:
??mn|l = ?+
?
(y,p)?M??
?
vjk|i?y
1
(
vjk|i = vmn|l
)
p (3)
In the Gaussian case, the parameter update for ?
is the maximum likelihood update; while the update
for ? incorporates a Bayesian prior N (?0, ?0):
?? =
?
?
?
?
1
?
(i,p)?M??,?
p
?
(i,p)?M??,?
(i? ??)2 ? p (4)
?? =
??2?0 + ?20
?
(i,p)?M??,? i ? p
??2 + ?20
?
(i,p)?M??,? p
(5)
As the parameters improve, the parser more effi-
ciently prunes incorrect parses and the beam incor-
porates valid parses for longer and longer phrases.
For instance, in the first iteration the model must
learn the meaning of both words in last Friday; once
the parser learns the meaning of one of them ? e.g.,
Friday appears elsewhere in the corpus ? subsequent
iterations focus on proposing candidate meanings
for last. In this way, a progressively larger percent-
age of the data is available to be learned from at each
iteration.
5 Evaluation
We evaluate our model against current state-of-the
art systems for temporal resolution on the English
452
Train Test
System Type Value Type Value
GUTime 0.72 0.46 0.80 0.42
SUTime 0.85 0.69 0.94 0.71
HeidelTime 0.80 0.67 0.85 0.71
OurSystem 0.90 0.72 0.88 0.72
Table 3: TempEval-2 Attribute scores for our system and
three previous systems. The scores are calculated us-
ing gold extents, forcing a guessed interpretation for each
parse.
portion of the TempEval-2 Task A dataset (Verhagen
et al., 2010).
5.1 Dataset
The TempEval-2 dataset is relatively small, contain-
ing 162 documents and 1052 temporal phrases in the
training set and an additional 20 documents and 156
phrases in the evaluation set. Each temporal phrase
was annotated as a TIMEX32 tag around an adver-
bial or prepositional phrase
5.2 Results
In the TempEval-2 A Task, system performance is
evaluated on detection and resolution of expressions.
Since we perform only the second of these, we eval-
uate our system assuming gold detection.
Similarly, the original TempEval-2 scoring
scheme gave a precision and recall for detection,
and an accuracy for only the temporal expressions
attempted. Since our system is able to produce a
guess for every expression, we produce a precision-
recall curve on which competing systems are plotted
(see Figure 4). Note that the downward slope of the
curve indicates that the probabilities returned by the
system are indicative of its confidence ? the prob-
ability of a parse correlates with the probability of
that parse being correct.
Additionally, and perhaps more accurately, we
compare to previous system scores when con-
strained to make a prediction on every example; if
no guess is made, the output is considered incorrect.
This in general yields lower results, as the system
is not allowed to abstain on expressions it does not
2See http://www.timeml.org for details on the
TimeML format and TIMEX3 tag.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
Valu
e ac
cura
cy
Extent recall
HeidelTime1
HeidelTime2
SUTime
OurSystem
Figure 4: A precision-recall curve for our system, com-
pared to prior work. The data points are obtained by set-
ting a threshold minimum probability at which to guess
a time creating different extent recall values. The curve
falls below HeidelTime1 and SUTime in part from lack
of context, and in part since our system was not trained
to optimize this curve.
recognize. Results are summarized in Table 3.
We compare to three previous rule-based sys-
tems. GUTime (Mani and Wilson, 2000) presents an
older but widely used baseline.3 More recently, SU-
Time (Chang and Manning, 2012) provides a much
stronger comparison. We also compare to Heidel-
Time (Stro?tgen and Gertz, 2010), which represents
the state-of-the-art system at the TempEval-2 task.
5.3 Detection
One of the advantages of our model is that it can pro-
vide candidate groundings for any expression. We
explore this ability by building a detection model to
find candidate temporal expressions, which we then
ground. The detection model is implemented as a
Conditional Random Field (Lafferty et al., 2001),
with features over the morphology and context. Par-
ticularly, we define the following features:
? The word and lemma within 2 of the current
word.
? The word shape4 and part of speech of the cur-
rent word.
3Due to discrepancies in output formats, the output of
GUTime was heuristically patched and manually checked to
conform to the expected format.
4Word shape is calculated by mapping each character to one
of uppercase, lowercase, number, or punctuation. The first four
characters are mapped verbatim; subsequent sequences of sim-
ilar characters are collapsed.
453
Extent Attribute
System P R F1 Typ Val
GUTime 0.89 0.79 0.84 0.95 0.68
SUTime 0.88 0.96 0.92 0.96 0.82
HeidelTime1 0.90 0.82 0.86 0.96 0.85
HeidelTime2 0.82 0.91 0.86 0.92 0.77
OurSystem 0.89 0.84 0.86 0.91 0.72
Table 4: TempEval-2 Extent scores for our system and
three previous systems. Note that the attribute scores are
now relatively low compared to previous work; unlike
rule-based approaches, our model can guess a temporal
interpretation for any phrase, meaning that a good pro-
portion of the phrases not detected would have been in-
terpreted correctly.
? Whether the current word is a number, along
with its ordinality and order of magnitude
? Prefixes and suffixes up to length 5, along with
their word shape.
We summarize our results in Table 4, noting that
the performance indicates that the CRF and interpre-
tation model find somewhat different phrases hard to
detect and interpret respectively. Many errors made
in detection are attributable to the small size of the
training corpus (63,000 tokens).
5.4 Discussion
Our system performs well above the GUTime base-
line and is competitive with both of the more recent
systems. In part, this is from more sophisticated
modeling of syntactic ambiguity: e.g., the past few
weeks has a clause the past ? which, alone, should
be parsed as PAST ? yet the system correctly dis-
prefers incorporating this interpretation and returns
the approximate duration 1 week. Furthermore,
we often capture cases of pragmatic ambiguity ? for
example, empirically, August tends to refers to the
previous August when mentioned in February.
Compared to rule-based systems, we attribute
most errors the system makes to either data spar-
sity or missing lexical primitives. For example ?
illustrating sparsity ? we have trouble recognizing
Nov. as corresponding to November (e.g., Nov. 13),
since the publication time of the articles happen to
often be near November and we prefer tagging the
word as Nil (analogous to the 13th). Missing lexi-
cal primitives, in turn, include tags for 1990s, or half
(in minute and a half ); as well as missing functions,
such as or (in weeks or months).
Remaining errors can be attributed to causes such
as providing the wrong Viterbi grounding to the
evaluation script (e.g., last rather than this Friday),
differences in annotation (e.g., 24 hours is marked
wrong against a day), or missing context (e.g., the
publication time is not the true reference time),
among others.
6 Conclusion
We present a new approach to resolving temporal ex-
pressions, based on synchronous parsing of a fixed
grammar with learned parameters and a composi-
tional representation of time. The system allows
for output which captures uncertainty both with re-
spect to the syntactic structure of the phrase and the
pragmatic ambiguity of temporal utterances. We
also note that the approach is theoretically better
adapted for phrases more complex than those found
in TempEval-2.
Furthermore, the system makes very few
language-specific assumptions, and the algorithm
could be adapted to domains beyond temporal
resolution. We hope to improve detection and
explore system performance on multilingual and
complex datasets in future work.
Acknowledgements The authors would like to thank Valentin
Spitkovsky, David McClosky, and Angel Chang for valuable
discussion and insights. We gratefully acknowledge the support
of the Defense Advanced Research Projects Agency (DARPA)
Machine Reading Program under Air Force Research Labora-
tory (AFRL) prime contract no. FA8750-09-C-0181. Any opin-
ions, findings, and conclusions or recommendations expressed
in this material are those of the authors and do not necessarily
reflect the view of DARPA, AFRL, or the US government.
References
