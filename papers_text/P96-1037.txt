Mechanisms for Mixed-Initiative Human-Computer  Collaborative 
Discourse 
Curry I. Guinn 
Department ofComputer Science 
Duke University 
Box 90129 
Durham, NC 27708 
cig~cs, duke. edu 
Abstract 
In this paper, we examine mechanisms for 
automatic dialogue initiative setting. We 
show how to incorporate initiative chang- 
ing in a task-oriented human-computer di-
alogue system, and we evaluate the ef- 
fects of initiative both analytically and via 
computer-computer dialogue simulation. 
1 Motivation: Initiative in 
Task-Oriented Dialogues 
Efficient human-computer dialogue requires imme- 
diate utterance-by-utterance a commodation to the 
needs of the interaction. The participants must con- 
tinuously focus all concerns on the goals of the task 
and avoid extraneous paths. This means that the di- 
alogue in i t ia t ive  should always pass immediately 
to the participant who is best able to handle the 
current task. An agent is said to have dialogue ini- 
tiative over a mutual goal when that agent controls 
how that goal will be solved by the collaborators. In 
the implemented voice dialogue system "The Circuit 
Fix-it Shop" (Smith et al, 1992; Smith and Hipp, 
1995), the dialogue fragment given in Figure 1 occurs 
between a human user and a computer collaborator. 
Utterances 2 and 4 indicate that the computer is di- 
recting the search for the missing wire in the faulty 
circuit. However, in Utterance 5 the user takes the 
initiative and suggests a different wire than has been 
proposed by the computer. In this paper we will 
present a theory explaining how initiative changes 
between participants and how computational gents 
can evaluate who should be in control of solving a 
goal. 
2 Automating Dialogue Initiative 
Center for Digital Systems Engineering 
Research Triangle Institute 
Box 12194 
Research Triangle Park, NC 27709 
cig~rt?, org 
1 U The LED is displaying an alter- 
nately flashing one and the 
top corner of a seven. 
2 C There is supposed to be a wire 
between connector nine nine 
and connector one zero zero. 
3 U It is there. 
4 C There is supposed to be a wire 
between connector nine eight and 
connector one zero two. 
5 U There is no wire on connector 
one zero four.  \[emphasis mine\] 
6 C There is supposed to be a wire 
between connector one zero four 
and 
connector one zero two. 
Table 1: A Dialogue Fragment in the Circuit Fix-it 
Shoppe Domain 
is in control and the other agent is passive (the 
master-slave assumption). For instance, in the LAD- 
DER system (Hendrix et al, 1978) the user has the 
initiative while the computer is passive. In contrast, 
the VODIS system (Young and Proctor, 1989) has 
the computer taking the initiative while the user is 
passive. Some dialogue systems (like GUS (Bobrow 
et al, 1977)) allow for a more mixed-initiative inter- 
action; however, the places where the user can take 
the initiative are limited and defined a priori. The 
dialogue model of Smith (1995) allows for either the 
computer or the user to assume degrees of initiative; 
however, Smith presents no algorithm for the com- 
puter to change initiative during a dialogue. Our 
model of mixed-initiative dialogue allows either par- 
ticipant to be in control of the dialogue at any point 
in time. 
Like Smith (1995) we believe that the level of ini- 
Many implemented ialogue systems are question- tiative in the dialogue should mirror the level of ini- 
answer systems with fixed initiative where one agent tiative in the task (which is a corollary to Grosz's 
278 
(1978) the structure of a dialog mirrors the structure 
o\] the underlying task). Unlike previous research in 
dialogue initiative, however, we attach an initiative 
level to each goal  in the task tree. Thus an agent 
may have initiative over one goal but not another. 
As goals get pushed and popped from the problem- 
solving stack, initiative changes accordingly. Thus 
many initiative changes are done implicitly based 
on which goal is being solved. 
2.1 The  Set t ing  o f  In i t ia t ive  Levels  
In our model of dialogue, initiative levels for each 
goal are defined during the interaction based on 1) 
explicit and implicit initiative-changing utterances 
and 2) competency evaluation. 
Expl ic i t  and Imp l i c i t  In i t ia t ive -Chang ing  Ut -  
te rances  Several researchers (Whittaker and Sten- 
ton, 1988; Walker and Whittaker, 1990) have noted 
that dialogue control can be exchanged through 
overt cues in the discourse. Our model concentrates 
on two specific dialogue cues: questions and answers. 
When an agent AI asks another agent A2 to satisfy 
a goal G, agent A2 gains initiative over goal G and 
all subgoals of G until agent A2 passes control of 
one of those subgoals back to agent A1. A similar 
initiative-setting mechanism is fired if agent A1 an- 
nounces that it cannot satisfy goal G. When a goal 
has been answered (satisfied) the problem-solving 
stack is popped. The initiative will now belong to 
whomever the initiative is for the goal on top of the 
stackJ. In the following human-computer dialogue 
gathered by Smith and Hipp(Smith and Hipp, 1995), 
U: How do I fix this circuit? 
C: What is the LED displaying? 
U: Nothing. 
C: What is the switch at? 
U: Where is the switch? 
C: In the lower left corner. 
U: The switch is down. 
C: Put the switch up. 
all initiative changes can be accounted for by ex- 
plicit initiative-changing utterances or by popping 
of the problem-solving stack due to goal resolution 
as illustrated in Figure 1. 
Competency  Eva luat ion  for In i t ia t ive  Set-  
t ing How does an agent decide whether to ask its 
collaborator for help? An obvious approach is to ask 
for help when the agent is unable to satisfy a goal on 
its own. This approach is the basic mechanism for 
several dialogue systems (Young et al, 1989; Smith 
iSince each participant is carrying out initiative eval- 
uation independently, there may be conflicts on who 
should be in control. Numerous researchers have stud- 
ied how negotiation may be used to resolve these con- 
flicts (Guinn, 1994; Guinn, 1993a; Lambert and Car- 
berry, 1992; McRoy, 1993; Sidner, 1993) 
and Hipp, 1995; Guinn, 1994). An additional ap- 
proach is to ask the collaborator for help if it is be- 
lieved that the collaborator has a better chance of 
solving the goal (or solving it more efficiently). Such 
an evaluation requires knowledge of the collaborat- 
ing agent's capabilities as well as an understanding 
of the agent's own capabilities. 
Our methodology for evaluating competency in- 
volves a probabilistic examination of the search 
space of the problem domain. In the process of solv- 
ing a goal, there may be many branches that can be 
taken in an attempt o prove a goal. Rather than 
selecting a branch at random, intelligent behavior 
involves evaluating (by some criteria) each possible 
branch that may lead toward the solution of a goal 
to determine which branch is more likely to lead to a 
solution. In this evaluation, certain important fac- 
tors  are examined to weight various branches. For 
example, during a medical exam, a patient may com- 
plain of dizziness, nausea, fever, headache, and itchy 
feet. The doctor may know of thousands of possible 
diseases, conditions, allergies, etc. To narrow the 
search, the doctor will try to find a pathology that 
accounts for these symptoms. There may be some 
diseases that account for all 5 symptoms, others that 
might account for 4 out of the 5 symptoms, and so 
on. In this manner, the practitioner sorts and prunes 
his list of possible pathologies. Competency evalu- 
ation will be based on how likely an agent's branch 
will be successful (based on a weighted factor analy- 
sis) and how likely the collaborator's branch will be 
successful (based on a weighted factor analysis and a 
probabilistic model of the collaborator's knowledge). 
In Section 3 we will sketch out how this calcula- 
tion is made, present several mode selection schemes 
based on this factor analysis, and show the results of 
analytical evaluation of these schemes. In Section 4 
we will present he methodology and results of using 
these schemes in a simulated ialogue environment. 
3 Mathematical Analysis of 
Efficiency 
Our model of best-first search assumes that for each 
goal there exists a set of n factors, f l , . - . ,  f~, which 
are used to guide the search through the problem- 
solving space. Associated with each factor are two 
weights, wi, which is the percentage of times a suc- 
cessful branch will have that factor and xi which is 
the percentage of all branches that satisfy fi. If an 
agent, a, knows q~',..., qn a percentage of the knowl- 
edge concerning factors f l , . . . ,  f~, respectively, and 
assuming independence of factors, using Bayes' rule 
an agent can calculate the success likelihood of each 
279 
U: How do I fix I 
this circuit? ~/  
goal(fix_circuit). 
Initiative: Computer 
Problem-Solving Slack 
ITHINKING\] 
> 
observe(switch). 
hdtiutive: Computer 
debug(led.oft). 
bfftiative: Computer 
goal(fix_circuit). 
lnititaive: Computer 
Problem-Solving Stack 
C: What is the switch at? 
\[THINKING\] 
<:  
observe(switch). 
Inith~tive: User 
debug(led,off). 
Initiutive: Computer 
goal(fixcircuit). 
Initiutive: Computer 
Problem-Solvlng Stack 
raise(switch). 
Initiative: User 
debug(led.off). 
Inithttive: Computer 
goal(fix_circuit). 
bdtiative: Computer 
Problem-Solving Stack 
U: Where is the switch? 
::> 
C: Put the switch up. 
observe(led). 
Initiative: Computer 
goal(fix_circuit). 
Initiative: Computer 
Problem-Solving Stack 
debug(led,off). 
Initiative: Computer 
goal(fix_circuiO. 
Initiative: Computer 
Problem-Solving Stack 
locate(switch). 
Initiative: Computer 
observe(switch). 
bdtiutive: User 
debug(led,offL 
Initiative: Computer 
goal(fix_circuit). 
hdtiative: Computer 
Problem-Solving Stack 
raise(switch). 
Initiative: Computer 
debug(led,off). 
Initiative: Computer 
goal(fix_circuit). 
hlitiative: Computer 
Problem-Solving Stack 
C: What is the 
LED displaying? 
\[THINKING\] 
C: In the lower left comer. 
\[POPI 
\[THINKING\] 
observe(led). 
Initiative: User 
goal(fix_circuit). 
Initiative: Computer 
Problem-Soiling Stack 
U: Nothing. / 
IPOP\] i 
I goal(fix circuit). I 
Initiutive: Computer 
Problem-Solving Stack 
observe(switch). 
lnitiutive: U.~er 
debug(led,oil). 
Initiative: Computer 
goal(fix_circuit). 
blitiative: Computer 
Problem-Solving Stack 
I 
U: The switch is down I 
I 
24-- 
I debug(ll~d,off). 
I goal(fixcircuit). 
Initiative." Computer 
Problem-Solving Stack 
Figure h Tracking Initiative via Explicit Initiative-Changing Utterances and Problem-Solving Stack Manip- 
ulation 
280 
possible branch of a goal G that it knows: 
p(b) = 1 - f I  1 - F(i)wi (1/k) (1) 
i=-I Xi 
where b is a branch out of a list of k branches and 
F(i) = 1 if the agent knows branch b satisfies factor 
f /and  F(i) = x i (1 -qa)  otherwise. \[Note: x i (1 -qa)  
is the probability that the branch satisfies factor fi 
but the agent does not know this fact.\] We define 
the sorted list of branches for a goal G that an agent 
knows, \[b~,... , b~\], where for each be~, p(b~) is the 
likelihood that branch b~ will result in success where 
p(b~) >= p(b~), Vi < j.  
3.1 Eff ic iency Ana lys is  of  D ia logue  
In i t iat ive 
For efficient initiative-setting, it is also necessary 
to establish the likelihood of success for one's col- 
laborator's lSt-ranked branch, 2nd-ranked branch, 
and so on. This calculation is difficult because the 
agent does not have direct access to its collabora- 
tor's knowledge. Again, we will rely on a proba- 
bilistic analysis. Assume that the agent does not 
know exactly what is in the collaborator's knowledge 
but does know the degree to which the collaborator 
knows about the factors related to a goal. Thus, in 
the medical domain, the agent may know that the 
collaborator knows more about diseases that account 
for dizziness and nausea, less about diseases that 
cause fever and headache, and nothing about dis- 
eases that cause itchy feet. For computational pur- 
poses these degrees of knowledge for each factor can 
be quantified: the agent, a, may know percentage q~ 
of the knowledge about diseases that cause dizzi- 
ness while the collaborator, c, knows percentage qC 
of the knowledge about these diseases. Suppose the 
agent has 1) a user model that states that the col- 
laborator knows percentages q{, q~,..., q~, about fac- 
tors f l , f2 , . . . , fm respectively and 2) a model of 
the domain which states the approximate number 
of branches, N'. Assuming independence, the ex- 
pected number of branches which satisfy all n factors 
is ExpAUN = N" l-Ii=l Xi" Given that a branch sat- 
isfies all n factors, the likelihood that the collabora- 
tor will know that branch is rZin_l qC. Therefore, the 
expected number of branches for which the collabo- 
rator knows all n factors is ExpAl lN I~i~=1 qg. The 
probability that one of these branches is a success- 
producing branch is 1 - \ [L~I  1 -w i  ~ (from Equa- 
tion 1). By computing similar probabilities for each 
combination of factors, the agent can compute the 
likelihood that the collaborator's first branch will be 
a successful branch, and so on. A more detailed he- 
count of this evaluation is given by Guinn (1993b; 
1994). 
We have investigated four initiative-setting 
schemes using this analysis. These schemes 
do not necessarily correspond to any observable 
human-human or human-computer  dialogue behav- 
ior. Rather, they provide a means  for exploring pro- 
posed dialogue initiative schemes. 
Random In Random mode,  one agent is given ini- 
tiative at random in the event of a conflict. This 
scheme provides a baseline for initiative setting 
algorithms. Hopefully, a proposed algorithm 
will do better than Random.  
SingleSelection In SingleSelection mode,  the more  
knowledgeable agent  (defined by which agent 
has the greater total percentage of knowledge) 
is given initiative. The  initiative is set through- 
out the dialogue. Once  a leader is chosen, the 
participants act in a master-slave fashion. 
Cont inuous  In Cont inuous mode,  the more  knowl- 
edgeable agent (defined by which agent's first- 
ranked branch is more  likely to succeed) is ini- 
tially given initiative. If that branch fails, this 
agent's second-ranked branch is compared to 
the other agent's first-ranked branch with the 
winner gaining initiative. In general if Agent 1 
is working on its ith-ranked branch and Agent 2 
is working on its jth-ranked branch, we compare 
A1 A1 p (h i )  to 
Orac le  In Oracle mode, an all-knowing mediator 
selects the agent that has the correct branch 
ranked highest in its list of branches. This 
scheme is an upper bound on the effectiveness of
initiative setting schemes. No initiative setting 
algorithm can do better. 
As knowledge is varied between participants we 
see some significant differences between the various 
strategies. Figure 2 summarizes this analysis. The x 
and y axis represent the amount of knowledge that 
each agent is given 2, and the z axis represents the 
percentage of branches explored from a single goal. 
SingleSelection and Continuous modes perform sig- 
nificantly better than Random mode. On aver- 
age Continuous mode results in 40% less branches 
searched per goal than Random. Continuous mode 
2This distribution is normalized to insure that all the 
knowledge is distributed between each agent. Agent 1 
will have ql + (1 ql ) (1-  2 - q ) ql+q2 percent of the knowl- 
edge while Agent 2 will have q2 + (1 - ql)(1 - q2) q~ 
ql  "~-q2 
percent of the knowledge. If ql + q2 = O, then set 
ql -= q2 -= 0.5. 
281 
E q.., 
1. Rando~ o .  ~::,  $ingleSdcctioa x 
m Co~tiw~o~ 
xxxxxxxx:  
X:.*MXX::XX: 
XMXXXXXX:  X 
x::x:::,:::x::: I~ - ,  C1 ~ ~xxxx x:  ~u I~ 
0 
X-axis: q i 
Z-axis: E~ect.e4pezceat~g? of q~ o.7~ 
bzaaches explozed ~. 
Figure 2: An Analytical Comparison of Dialogue Initiative-Setting Schemes 
performs between 15-20% better than SingleSelec- 
tion. The large gap between Oracle and Continuous 
is due to the fact that Continuous initiative selection 
is only using limited probabilistic information about 
the knowledge of each agent. 
4 Computer Simulations 
The dialogue model outlined in this paper has 
been implemented, and computer-computer dia- 
logues have been carried out to evaluate the model 
and judge the effectiveness of various dialogue initia- 
tive schemes. In a methodology similar to that used 
by Power (1979), Carletta (1992) and Walker (1993), 
knowledge is distributed by a random process be- 
tween agents, and the resulting interaction between 
these collaborating agents is observed. This method- 
ology allows investigators to test different aspects of 
a dialogue theory. Details of this experimental strat- 
egy are given by Guinn (1995). 
4.1 The  Usage of  Computer-Computer 
Dialogues 
The use of computer-computer simulations to study 
and build human-computer dialogue systems is 
controversial. Since we are building computa- 
tional models of dialogue, it is perfectly reason- 
able to explore these computational models through 
computer-computer simulations. The difficulty 
lies in what these simulations ay about human- 
computer or computer-computer dialogues. This 
author argues that computer-computer simulations 
are one layer in the multi-layer process of build- 
ing human-computer dialogue systems. Computer- 
computer simulations allow us to evaluate our com- 
putational models and explore issues that can not 
be resolved analytically. These simulations can help 
us prune out some mechanisms and suggest mech- 
anisms that may work well in a human-computer 
system. For instance, if the dialogues that result 
in a computer-computer conversation are incoherent 
to a human observer, this suggests that the dialogue 
mechanisms employed may be inappropriate for a 
human-computer system. On the other hand, if the 
resulting dialogues are coherent and exhibit features 
that are desired in a human-computer system, this 
suggests that these mechanisms may work well in a 
human-computer system. The final test, of course, 
must be in the implementation f a human-computer 
dialogue system. The scarcity of such systems ug- 
gests that it is an extremely expensive process to 
build a functional human-computer dialogue system, 
and computer-computer simulations can assist in re- 
ducing these costs. 
4.2 Simulation Results 
A murder mystery domain was created with 22 
suspects. An annotated example dialogue from a 
computer-computer collaboration in this domain is 
presented in Figure 3. Agents were given partial 
information through a random process. By using 
either Random mode or Continuous mode, we can 
evaluate the effect of those mechanisms in this ex- 
perimental environment. Table 2 presents results 
computed from 5008 collaborations where the agents 
282 
Speaker Utterance 
Watson  
Ho lmes  
Watson  
Ho lmes  
Watson  
Ho lmes  
Watson  
Hohnes 
Holmes 
Watson 
Holmes 
Watson 
Holmes 
Watson 
Holmes 
Watson 
Holmes 
Watson 
Watson 
Holmes 
Watson 
Holmes 
Watson 
Who is the murderer of Lord Dunsmore? ~ 
Is it the case that Suspect16 is the murderer of Lord Dunsmore? 
Is it the case that Suspectl6 had a motive to murder Lord Dunsmore? 
Suspectl6 had a motive to murder Lord Dunsmore. 
Is it the case that Suspectl6 had access to the poison? 
I am unable to determine whether Suspectl6 had access to the poison. 
I am unable to determine whether Suspectl6 is the murderer of Lord 
Dunsmore. 
I am unable to determine whether Suspectl6 is the murderer of Lord 
Dunsmore. 
Who is the murderer of Lord Dunsmore? b 
Is it the case that Suspectl0 is the murderer of Lord Dunsmore? 
Is it the case that Suspectl0 had a motive to murder Lord Dunsmore? 
Suspectl0 had a motive to murder Lord Dunsmore. 
Is it the case that Suspectl0 had access to the poison? 
Suspectl0 had access to the poison. 
Is it the case that Suspectl0 had an opportunity to administer the 
poison? 
I am unable to determine whether Suspectl0 had an opportunity to 
administer the poison. 
I am unable to determine whether Suspectl0 is the murderer of Lord 
Dunsmore. 
I am unable to determine whether Suspectl0 is the murderer of Lord 
Dunsmore. 
Is it the case that Suspect9 is the murderer of Lord Dunsmore? 
Is it the case that Suspect7 is the murderer of Lord Dunsmore? c 
I have proven that Suspect9 has a motive to murder Lord Dunsmore and 
Suspect9 had access to the poison, d
I have proven that Suspect7 had access to the poison, Suspect7 had 
an opportunity to administer the poison, and Suspect7 has a criminal 
disposition. ~ 
Suspect7 is the murderer of Lord Dunsmore. f 
awatson gives control of the investigation over to Holmes. Each part ic ipant uses the Cont inuous Mode a lgor i thm to determine who 
should be in control. 
bHolmes is giving up control of direct ing the invest igat ion here. 
CHolmes is challenging Watson's investigative choice. 
dwatson negot iates for his choice. 
eHolmes negotiates for his choice. 
fWatson now has enough information to prove that  Suspect7 is the murderer. 
Figure 3: A Sample Dialogue 
283 
had to communicate o solve the task. 
Random Continuous 
Times (secs) 82.398 44.528 
of Utterances 39.921 26.650 
~uspects Examined 6.188 3.412 
Table 2: Data on 5008 Non-trivial Dialogues from 
the Murder Mystery Domain 
5 Extension to Human-Computer 
Dialogues 
Currently, two spoken-dialogue human-computer 
systems are being developed using the underlying 
algorithms described in this paper. The Duke Pro- 
gramming Tutor instructs introductory computer 
science students how to write simple Pascal pro- 
grams by providing multiple modes of input and 
output (voice/text/graphics) (Bierman et al, 1996). 
The Advanced Maintenance Assistant and Trainer 
(AMAT) currently being developed by Research Tri- 
angle Institute for the U.S. Army allows a mainte- 
nance trainee to converse with a computer assistant 
in the diagnosis and repair of a virtual MIA1 tank. 
While still in prototype development, preliminary 
results suggest hat the algorithms that were suc- 
cessful for efficient computer-computer collabora- 
tion are capable of participating in coherent human- 
machine interaction. Extensive testing remains to be 
done to determine the actual gains in efficiency due 
to various mechanisms. 
One tenet of our theory is that proper initiative 
setting requires an effective user model. There are 
several mechanisms we are exploring in acquiring the 
kind of user model information necessary for the pre- 
viously described ialogue mode algorithms. Stereo- 
types (Rich, 1979; Chin, 1989) are a valuable tool 
in domains where user classification is possible and 
relevant. For instance, in the domain of military 
equipment maintenance, users can be easily classi- 
fied by rank, years of experience, quipment famil- 
iarity and so on. An additional source of user model 
information can be dynamically obtained in envi- 
ronments where the user interacts for an extended 
period of time. A tutoring/training system has the 
advantage of knowing exactly what lessons a stu- 
dent has taken and how well the student did on in- 
dividual essons and questions. Dynamically mod- 
ifying the user model based on on-going problem 
solving is difficult. One mechanism that may prove 
particularly effective is negotiating problem-solving 
strategies (Guinn, 1994). The quality of a collabora- 
tor's negotiation reflects the quality of its underlying 
knowledge. There is a tradeoff in that negotiation 
is expensive, both in terms of time and computa- 
tional complexity. Thus, a synthesis of user model- 
ing techniques will probably be required for effective 
and efficient collaboration. 
6 Acknowledgements 
Work on this project has been supported by grants 
from the National Science Foundation (NSF-IRI-92- 
21842 ), the Office of Naval Research (N00014-94-1- 
0938), and ACT II funding from STRICOM for the 
Combat Service Support Battlelab. 
References 
A. Bierman, C. Guinn, M. Fulkerson, G. Keim, 
Z. Liang, D Melamed, and K Rajagopalan. 1996. 
Goal-Oriented multimedia dialogue with variable 
initiative. In submitted for publication. 
D.G. Bobrow, R.M. Kaplan, M. Kay, D.A. Norman, 
H. Thompson, and T. Winograd. 1977. GUS, a 
frame driven dialog system. Artificial Intelligence, 
8:155-173. 
J. Carletta. 1992. Planning to fail, not failing to 
plan: Risk-taking and recovery in task-oriented 
dialogue. In Proceedings of the l~th Interna- 
tional Conference on Computational Linguistics 
(COLING-92), pages 896-900, Nantes, France. 
D.N. Chin. 1989. KNOME: Modeling what the user 
knows in UC. In A. Kobsa and W. Wahlster, ed- 
itors, User Models in Dialog Systems, pages 74- 
107. Springer-Verlag, New York. 
B. J. Grosz. 1978. Discourse analysis. In D. Walker, 
editor, Understanding Spoken Language, chap- 
ter IX, pages 235-268. Elsevier, North-Holland, 
New York, NY. 
C.I. Guinn. 1993a. Conflict resolution in collabora- 
tive discourse. In Computational Models of Con- 
flict Management in Cooperative Problem Solving, 
Workshop Proceedings from the 13th International 
Joint Conference on Artificial Intelligence, Cham- 
bery, France, August. 
Curry I. Guinn. 1993b. A computational 
model of dialogue initiative in collaborative dis- 
course. Human-Computer Collaboration: Recon- 
ciling Theory, Synthesizing Practice, Papers from 
the 1993 Fall Symposium Series, Technical Report 
FS-93-05. 
Curry I. Guinn. 1994. Meta-Dialogue Behaviors: 
Improving the EJficiency of Human-Machine Di- 
alogue -- A Computational Model of Variable Ini- 
tiative and Negotiation in Collaborative Problem- 
Solving. Ph.D. thesis, Duke University. 
284 
Curry I. Guinn. 1995. The role of computer- 
computer dialogues in human-computer dialogue 
system development. AAAI Spring Symposium 
on Empirical Methods in Discourse Interpretation 
and Generation, Technical Report SS-95-06. 
G.G. Hendrix, E.D. Sacerdoti, D. Sagalowicz, and 
J. Slocum. 1978. Developing a natural anguage 
interface to complex data. ACM Transactions on 
Database Systems, pages 105-147, June. 
L. Lambert and S. Carberry. 1992. Modeling ne- 
gotiation subdialogues. Proceedings o\] the 30th 
Annual Meeting o\] the Association for Computa- 
tional Linguistics, pages 193-200. 
S. McRoy. 1993. Misunderstanding and the ne- 
gotiation of meaning. Human-Computer Collab- 
oration: Reconciling Theory, Synthesizing Prac- 
tice, Papers from the 1993 Fall Symposium Series, 
AAAI Technical Report FS-93-05, September. 
R. Power. 1979. The organization of purposeful 
dialogues. Linguistics, 17. 
E. Rich. 1979. User modeling via stereotypes. Cog- 
nitive Science, 3:329-354. 
C. L. Sidner. 1993. The role of negotiation in 
collaborative activity. Human-Computer Collab- 
oration: Reconciling Theory, Synthesizing Prac- 
tice, Papers from the 1993 Fall Symposium Series, 
AAAI Technical Report FS-93-05, September. 
R.W. Smith and D.R. Hipp. 1995. Spoken Natural 
Language Dialog Systems: A Practical Approach. 
Oxford University Press, New York. 
R.W. Smith, D.R. Hipp, and A.W Biermann. 1992. 
A dialog control algorithm and its performance. 
In Proceedings o\] the 3rd Conference on Applied 
Natural Language Processing. 
M. Walker and S Whittaker. 1990. Mixed ini- 
tiative in dialogue: An investigation into dis- 
course segmentation. In Proceedings of the 28th 
Annual Meeting of the Association for Computa- 
tional Linguistics, pages 70-78. 
M. A. Walker. 1993. Informational Redundancy and 
Resource Bounds in Dialogue. Ph.D. thesis, Uni- 
versity of Pennsylvania. 
S. Whittaker and P. Stenton. 1988. Cues and con- 
trol in expert-client dialogues. In Proceedings of 
the 26th Annual Meeting of the Association/or 
Computational Linguistics, pages 123-130. 
S.J. Young and C.E. Proctor. 1989. The design and 
implementation of dialogue control in voice oper- 
ated database inquiry systems. Computer Speech 
and Language, 3:329-353. 
S.R. Young, A.G. Hauptmann, W.H. Ward, E.T. 
Smith, and P. Werner. 1989. High level knowl- 
edge sources in usable speech recognition systems. 
Communications o\] the ACM, pages 183-194, Au- 
gust. 
285 
