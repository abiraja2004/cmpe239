r O,, Plmming Argumcnl;  l;iv0 l- ,xts 
Xiaorong Hua.ng 
Fachbereich Inforlnatik, Uniw:rsiti\[t, des S+ul,rlandes 
6(i0d\]. Saarbr i icken,  (l(~rlltTi,liy, etnail: huang~3cs.uni-sb.de 
Abst rac t  
This paper presents PI?OVF, I?,I"f a text planner for 
argumentative texts. I~I~OVI'\]\]~II~ nain feature is 
that it combines global hierarchical planning and llti- 
phmned organization of text with respect to local de- 
rivation relations in a complementary way. The former 
splits the task of presenting a particular pronf into 
subtasks of llresenting sul)proot~. "\['lie lati, er silnli\[al;cs 
\]iow the next intermediate onchision to Im l)resenl,ed 
is chosell under the guida.nce o\[ the local \['ocils. 
1. In t roduct ion  
This pallor presents a, text planner for l,h,' w,rlmliz~t- 
tion of natural deduction (ND) st, yle proofs \[Gm,:Uq. 
Several similar attempi;s can be lbulld in previous 
work. I)eveloped before the era of NL genl!ral.))li, 
the system EXPOUND of D. (Thesl.er \[Che76\] call I>e 
characterized as an exatnl+le of direct translatio'a: Al- 
though a sophisticated linearizatioii is applied on the 
input ND proofs, the steps are then I:ranslated loc- 
ally in a template driven way. ND i>rool:s were tested 
as input to an early version of the MUMI~,I,I'; sys- 
tern of D. McDonald \[McD83\], the Irllain aim however, 
was to show the fl~asibility of the architecture. A 
more receitt attempt can be foilnd in 'l'lI\[Nl(li;ll. 
\[1'\]P93\], whMI implements sew~ral interesting but isol- 
ated proof presentation strategies, witliout giving ;l 
comprehensive underlying model. 
Our computational model can therelbre I>e viewed 
;is the first serious attempt at a comprehensive conipu- 
tational model that produces adeqilate argillneill, al,iw~ 
te?l,s froln N\]) si,yle proofs. The inaill nilll is I,+) sh+:,v 
how existing text planning techlliques Call \[)t~ adapted 
for this particular apl)iication, q'o test its feasibility, 
this computational model is imldelilenl,ed lit a sysl,enl 
called PROVERH. 
Most current NL text pialillers assiiltle thai, \[ali- 
guage generation is planned I>eh~vior ~tlid there- 
fore adopt a hiera.rchical platiliilig aplll'oach \[Iiov88, 
Moo89, Da192, Rei91\]. Nonetheless there! is psycllolo- 
gical ,~vidence that language has an ullplaluled, si~oti-. 
taneous aspect ms well \[Och79\]. Based on this ol~sei'va- 
l, ion, researchers have exploited organizing text with 
respect to some local relations. Sibun \[Sib90\] itnl+> 
merited a system generating descripticms for oltiects 
with a strong domain sl.ructure, such as houses, chips 
alld families. Once a discourse is st.;i,l'l,~;(I, local strllc- 
lures suggest the next objects awtilalfle. \[nstead of 
planning globally, short-range sl;rategies ~tre cnlpl~tyed 
to ol'gallize ~t short seglrl<!lll, or' text. l?roni a collll)ltl, a- 
ti~mnl point of view, a hierarchical planner elaborates 
recursively on the initial commmiicatiw~ goal mitil the 
Ihtal sultgoals can be achieved by ++l>plyivg a prlmitiw'~ 
operator. A text generator based on the local organ- 
ization, in contrast, repeatedly chooses a part of the 
renmiNing t.ask and carries it out. 
The macroplanner of \]'IgOVER.B combines hier- 
arcDical planTdng with local orgaMzation in a uniform 
planning framework. ' the hierarchical planning is real- 
ized by so-called top-dowu presentation operators that 
split the task of presenting a particular proof into 
subtasks of presenting subproofs. While the over- 
all planning mechanism follows the RS'l.'-lmsed plan- 
ning approach \[Moo89, Reigl\], the Idaiming operators 
lilt)re cl<+sely Feselllllle the schellHl+ta ill schema-based 
phtnning \[McK85, Iqtr88\]. l \]ottom-up resentation 
Oilel'O, tOl'S ~tl'(~ (ll~vised to Sil/Itll;t|,e \[,lie tLltpl,~tllll~d RS- 
i>ect;, where the next intermediate cotichlsion to lm 
presenLed is chosen under the gttidance of the local 
fc>cus tnechanisnt in a lllore SpOlttarleous way. }'Jil:ce 
top-dmvn operal,ors enibody explicit coinnnnticative 
liOrlilS, they are ai',wiys given a higher priority. Only 
wheil lie l;op-dOWtl pr<?selil;atioit operator is apl>licabh;, 
will a bot,toiil-ttp present,alien operator be chosen. 
This distinction betweeii plalllled alld illiplaltrled 
presentation loads to a w;ry natural seglnenl;ation of 
the discourse int.o an allciilional hierarchy, since, fol.. 
h>willg t.he theory of (.Irosz and Sidner \[CS86\], there is 
+l Oll+P-to-olle COl'l'esl)Oltdellce betweeli  the ititelit, iolial 
hierarrhy and the al;tentional hierarchy. '\['his atl;en- 
t,iemal hierarchy iv used to itlltke, r@Jz'encc choices for 
inference ilietliods and for previously presented inter- 
nimiiat,e conclusions, 'l'he inference choices itl'l~ the 
tllailt COliCerll of the iliieroitllmner of PR, O Vl'.'l~II(see 
\[llua{)dlq). 
2. (hmtext  o f  Our  Research  
Tim text planner discussed hi this paper is the mac- 
roplanner of I'ROVI'H~,IJ, which translates machine- 
f<mml iwool's in sew~rM steps into u~ttural lallgl.lage. 
PI:OVEI~II adopts a rcconslr'uclive approach: Once 
a l~roof in a m~u:hhie oriented forlnalistn is genel'~d;cd 
in the proof dewdopnwnt envh'onlnent fI--MKRP, a 
new proof' l, hat, m<+re resetnbles those found in mai;h- 
enmt,ical tex.tl)ooks is reconstructed \[lhla94a\]. The re- 
construcl,<~d proo\[" is ~t proof lT"ce, where proof nodes 
ave derived from their children by applying an infer- 
c'nce tnel, hod (also called a justilical,ion). Most of the 
steps are justified by Lhe application of a definition 
329 
_ sgr(U, 1,'),, , U ~ ~ ~(U, 1,,, *),, 
u(ul, 1,,, *), Ul E U Du, ul C (;' _~z~zl)snl)gr\[r,  ' 1  " mo ; ~ _r,, - -T -~UUl , ,  E  Ds, 'gr(F' *) D,  
_i!1: ,,, * 1,, - -g l :  I" - - ' "   )2 -,rsol 
Figure 1: An Example lnlmt Proof 
or a theorem, the rest are justified by inference rules 
of the natural deduction (ND) ealcuhls, such as the 
"Case" rule. Figure 1 is an examph', of a segment of 
a possible input proof, where some nodes are labeled 
for convenience. 
The justifications "Du", "Dsubgr", "Ds", "Dg", and 
"Tsol" stand for the definitions of unit element, of 
subgroup, of subset, of group, and the theorem about 
solution, respectively. 
The input proof tree is also augmented with an 
ordered list of nodes, being roots of subproofs planned 
in this order. The proof in Figure 1 is associated with 
the list: (\[2\], \[a\], \[4\], \[l\]). 
3. The  F ramework  o f  the  Macrop lanner  
Tl,e macroplanner of PROVERB elaborates on com- 
municat ive  goals, selects and orders pieces of inforrl~a- 
tion to fullill these goals. The output is an ordered sc'- 
quenee of proof communicat ive act iuteu~ions (PCAs). 
PCAs can be viewed as speech acts in our domain of 
application. 
P lann ing  Pramework  
PROVERB combines the two above mentioned 
presentation modes by encoding communicat.ion 
knowledge for both top-down planning and bottom-up 
presentation in form of operators in a uniform plan- 
ning framework. Since top-down presentation operat- 
ors embody e?plieit communicatiw~ norms, they are 
given a higher priority. A botl.om-up presentation is
chosen only when no top-down presentation operal,or 
applies. The overall planning framework is realized 
by the fimction present .  Taking as inpul, a subproof, 
Present  repeatedly executes a hasic planning cycle 
unl,il the inlmt subproof is eouw!yed. F, ach cycle car- 
ries out Olle presentation operal, or, where Present al- 
ways tries first to choose and apply a top-down op- 
erator, if impossible, a Imttom.-up opc:rator will he 
chosen. ~l~he function Present  is first called wil.}l t,he 
entire proof as the presentation task. The execution of 
a top-down presentation operator may generate sub- 
tasks by calling it recursively. The discourse produced 
by each call to Present  tbrms an attentioual unit 
(compare the subsection below). 
The  D iscourse  Mode l  and the A. t tent iomd Hier-  
archy 
The discourse carried out so far is recorded in a dis- 
course model. Rather than recording |he semantic oh- 
jeets and their properties, our discourse model consists 
basically of the part of the input proof tree which has 
already been conveyed. The discourse model is also 
segmented into an allenl ional hierarchy, where, sub- 
proofs posted by a top-down presentation operators 
as subtasks constitute attentional units. The. follow- 
ing are some notions useful for the formulation of the 
prese,\]tation perators: 
? Task is the subproof in the input proof whose 
presentation is the current task. 
? Local focus is the intermediate conclusion lmst 
presented, while the semantic objects involved in 
t;he local tbcus are called the focal centers. 
Proof  Comlnunieat ivc,  Acts  
P(.,'As are the primil;ive actions plammd during the 
macroplanning to achiew. ? communical;ive goals. Like 
speech acts, PCAs can be defined in terms of the com- 
mmlicative goals they fulfill as well as tlu-qr possible 
verbalizations. Based on an analysis of proofs in math- 
enuttical textlmoks, each PCA has as goal a combin- 
ation o17 the lbllowing sllhgoals: 
1. CoIweying a st.ep of the deriwttion. '.Phe simplest 
\]'CA is the operator Derive. hlstantiated as be- 
(Der ive  Reasons: l ag  ,5'I, EI C -- $2) 
Intermediate-Results  : nil 
Derived-Formula:  a G $2 
Method: def-subset) 
depending on the reference choices, a possible 
verbalization is given as following: 
"lb;cause a is an eh'.ment of 51 and ,%. 
is a subset of S,.,, according to the detin- 
ition of subset, a is an elelne/Lt of S:!." 
2. I.Jpdates o\[' I.he glob:d attentional structure. 
These I~CAs som,'t.imes also convey ~L partial plan 
for tim further l)resentation. IBlfects of this group 
of I'CAs include: creal, ing new attentional units, 
setting up partially premises and the goal of a 
new unit, closing t.he current unit, or l'ealloeal;ing 
the attention of the reader from one attentional 
unit to another. The PCA 
(Begin-Cases Goal : l,'ormula 
Assumptions: (A I~)) 
creates two atteD.tional units with A and II as the 
assumptions, and Formula as the goal by produ- 
cing the verbalization: 
"To prow" Formula, let us consider the 
two cases by assuming A and B." 
Thirteen PCAs are currently employed in PRO- 
VEI?t3. See \[Ilua94b\] for more details. 
330 
St ructure  of the  P lamf ing  OI)erators  
Although top-down and bottom-up presentation 
activities are of a eoneel)tually dift~rent nature, the 
corresponding communication knowledge is uniformly 
encoded as presentat ion  oper'ators i|l a planning frame- 
work, similar to the plan operators in other generation 
systems \[Hov88, Moo89, Da192, ILeigl\]. In general, 
presentation operators map an original presentation 
task into a seqnenee of subtasks and finally into a se- 
quence of PCAs. All of thenr haw~ the following four 
slots: 
? P ro@ a proof schema, which characterizes the. 
syntactical structure of a proof segment for wllich 
this operator is designed. It plays 1;t1(.' role of the. 
goal slot in the traditional l)lanning franrework. 
? Appl icabi l i ty  Condi t ion:  a pre(\[icate.. 
? Acts: a procedure which essentially carries out 
it seqtlellce of preselfl;atioli acts. They are either 
primitive PCAs, or are recursive calls to the pro- 
cedure Present  for subproofs. 
? Features: a list of features which helps to select 
the best of a set of aI)l)licable operators. 
4. Top-Down P lann ing  
' I 'h is  sect ion  e laborM;es  oil the co ln ln t ln icat ive  norms 
concerning how a proof to he presented can Im split 
into sitbproofs, as well ~us how the hierarchically- 
structured subprooN can lie maplied onto some lineear 
order for presentation. In contrast with operators em- 
ployed in RST-b~se(l plmuters that split goals accord- 
ing to the rhetorical structures, our operators encode 
standard schemata for presenting proofs, which (:oil- 
lain subgoals. The top-down presentation operators 
are roughly divided into two cate.gories: 
? schemata-based operators encoding complex 
schemata for the presentation of proofs of a sl)e- 
cilie pattern (twelve o1' tlwm are currently i,ltcg- 
rated in P IgOVERI I ) ,  
? general operators embodying general pr,~senta- 
lion norms, concerning splitting proofs and or- 
dering subgoals. 
F t- \[,' (1 i- (7 
i 
~-rv -o  ~ ",~,,,~:r Lcasl.: 
Figure 2: A Schmmt Involving Cases 
Let us first look at an operator devised tbr proof seg- 
ments containing eases. 'l'he. eorreslmnding schenra of 
such a proof tree is shown in Figure 2. Under two 
circumstances a writer lnay recognize that 11(; is con- 
fronted with a proof segment containing cases. First, 
when the snbproof that has the structure of l"igure. 2 is 
the current presentation task, tested by (task ?L1) 1. 
Second, when the disjunction I,' V G has just been 
presented in the bottom-up mode, tested by (local- 
\['octls "?L4). Under both circumstances, a teammate- 
alien norm motiwttes the writer to First present the 
part leading to 1,' V G (in the second case this subgoal 
has ah'eady been aehiew3d), and then to proceed with 
the two cases. It enforces also that certain PCAs be 
used to mediat.e between 1)arts of l)roofs. This proced- 
ure is exactly captured by the presentation operator 
below. 
Case- Impl ic i t  
? Proof: as given in lqgure 2 
? Applicability Condition: ((task ?LI) V 
(local-l~,,',s '?1;4)) A (,,oi,-conveyed (?L., 7l~-,)) 
? Acts: 
1. if ?L4 has not been conveyed, then l)resenl; 
'7174 (subgoal 1) 
2. a PCA with the verbalization: "First, let us 
conside.r the first east., by assuming F."  
3. preselfl; ?L2 (subgoal 2) 
4. a PCA wit, h the vm'balization: "Next, we 
consider the se.cond case by assuming (;." 
5, presel,t '?La (subgoal 3) 
(i. mark "71)1 as conw.ye(l 
? lL, atures: (top-down compulsory implicit) 
q'he f l~atm'e  values can be divided into two groups: 
those characterizing the style, of the 1;ext this oper- 
ator produces, and those concerning other planning 
aspects. "Implicit" is a stylistie feature value, indic- 
ating that the splitting of the p,'oof into the three sub- 
goals is not made explicit. In its explicit dnal Case- 
Expl ic i t  a PCA is added to the beginning of the Acts 
slot., wlfich l)ro(hiee.s tim verbalization: 
"To prow~ Q, let us first prove F V G, and 
consider the two eases sel)arately." 
The feature, wdue "COmlmlsory" indicates thai. if the 
applicallility condition is satisfied, and the style of the 
Ol)(~r;tl,{:,r (:Oll\['orlns to the ghd)al style the texl. planner 
is (:olrlruitted to, this operator should be chosen. Two 
weaker vahms also retlect the speci\[icii,y of plan oper- 
ators: "speci\[ic" and "general". 
(h,neral l)resental.ion operators perform a simple 
task according to some general text organization prin- 
ciples. They either 
? enforce a linearization on subprool~ to be presen- 
ted, or 
? split the task of the presentation of a proof with 
ordered snhproofs into sul)t.asks. 
t Labels stand fro" the ?m'respondhlg nodes 
331 
The first ordering operator operationalizes a gen- 
eral ordering strategy called minimal oad principle. 
This principle predicates that a writer usually presents 
shorter branches beibre longer ones. The argument of 
Levelt is rather simple: When one branch is chosen to 
be described first, the writer has to have the choice 
node flagged in his memory for return. If he follows 
the shorter branch first., the duratiml of the load will 
be shorter. The eonerete operator is omitted. 
Note that, the subproofs being ordered are sub- 
proofs conceptually planned while the correspm,ding 
proof is constructed. There are two other ordering 
operators based on general ordering principles: the 
local focus principle and the proof time order principle 
\[IIua94b\]. 
The invocation of an ordering operator is always 
followed by the invocation of a splitting operator, 
which actually posts subgoals by calling the function 
Present  with the ordered goals subsequently. 
5. Bot tom-up  Presentat ion  
The bottom-up resentation process simulates the un- 
planned part of proof presentation. Instead of split- 
ting presentation goals into subgoals according to 
standard schernata, it follows the local derivation re- 
lation to find a next proof node or subproof to be 
presented, in this sense, it is similar to the local organ- 
ization techniques used in \[Sib90\]. When no top-down 
presentation operator applies, I~ROVI'2RB chooses a 
bottom-up operator. 
The  Local  Focus 
The node to be presented next is suggested by the 
mechanism of local focus. Although logically any proof 
node having the local focus as a child could be choserl 
for the next step, usually the one with the greatest se- 
mantic overlapping with the focol cenier's is preferred. 
As mentioned above, focal centers are senmntic ob- 
jects mentioned in the proof node which is the local 
focus. This is based on the observation that if one 
has proved a property about some semantic obje.cts, 
one tends to continue to talk about these particular 
objects before turning to new ohjects. Let ns examine 
the situation when the proof below is awn.iting I~,'ese,,L- 
ation. 
j~\] : or,, ,  b)' \[a\] .7-0(., b) A :e(I,, .) 
Assume that node \[1\] is the local focus, the set; 
{a, b} are the focal centers, \[3\] is a previously presen- 
ted node and node \[5\] is the current task. \[2\] is chosen 
as the next node to be presented, since it, does not 
(re)introduce any new semantic object and it.s overlap 
with the focal centers ({a, b}) is larger than those of 
\[4\] ({~}). 
The Bot tom-Up Presentat io l l  Operators  
Under different circumstances the deriwH, ion of the 
next-node is also presented in different ways, The 
corresponding presentation knowledge is encoded as 
bottom-np presentation operators. The one most fre- 
quent.ly used presents one. step of derivation: 
Derlve.-Bot tom-Up 
? Proof: ?Nodel~...l?Node,~?M 
? Noden+ l 
? Applicability Condition: ?Noden+ 1 is suggested 
by the focus mechar, ism as the next node, and 
?No&a,..., ?Node,, are conveyed. 
? Acts: a PCA that conveys the fact that ?Node,+1 
is derived from the premises '?Nodq,..., ?Noden 
by applying ?M. 
? Features: (bottom-up general explicit detailed) 
If the conclusion ?Node,+l, the premises and the 
method ?M are instantiated to a G S1, (a G ,92, 
S~ G S.,), def s'ubse.t respectively, the following w.~rbal- 
ization can be produced: 
"Since a is an element of S~, and $1 is a 
subset of S.,, a is an element of oe2 according 
to the definition of subset." 
A lrivial suhproof may be presented as a single de- 
riwttion by ornitting the intermediate nodes. 'this nezl 
s.ubproof is also suggested by the local focus. This is 
sinmlated by a bottom-up operator called S impl i fy-  
Bot tom-Up.  Currently seven bottom-up operators 
are it,l.egrated in PROVERB. 
6. Verba l i za t ion  o f  PCAs  
Macroplanning produces a sequence of PCAs. Our 
mieroplanner is restricted to the treatment of the re f- 
eremite choices for the inference methods and for the 
previously presented intermediate conclusions. While 
the former depelMs on static salience relating l,o the 
domain kuowledge, the latter is similar to subsequent 
refi.'rences, and is therelbre sensitive to the context, 
in particular to it:s segmentation i to attenl,ional hier- 
archy. Dne to space restrictions, we only show the fol- 
lowing piece of a prcverbal message as an example, he- 
i,lg a PCA enriched with reflq'ence dmices for reasons 
aml nn!l.hod by the microplanner \[IluaDdh, IIua94b\]. 
(Derive Reasons: (((ELE a U) explicit) 
((SUBSET U F) omit)) 
Conclusion: (ELE a F) 
Method: (Dof-Subsot omit)) 
Our surface generator TAG-GI'~N \[Ki194\] produces 
the ul, terance: 
"Since a is an element of U, a is an element 
of F." 
Notice, only the l'\[~ason labeled as "explicit" is verb- 
alized. 
Finally, to demonstrate the type of proofs currently 
generated by PI~OVER.B, below is the complete out- 
l)ut \['or a proof constructed by f2-MKIIP: 
Thc, orem: Let /'~ be a grou I) and U a subgroup of F, 
if I and Iv are unit elements of F and U respectively, 
then 1 = 1?:. 
332 
Proof :  
Let F be a group, U be a subgroup of /,', 1 he a 
unit element of F and lu  be a unit element of U. 
According to the definition of unit eleme,lt, Iu rE U. 
Therefore there is an X,  X C U. Now suppose that 
~tl is such an X. According to the definition of ilnit. \[Che711\] 
element, ut * 1u = ut. Since U is a subgrmtp of t:', 
U C F. "lqterefore 1u E F. Similarly ul G F, since \[\[);,19'-'\] 
ul G U. Since F is a group, F is asemigroui). Because 
u l*  lcr = ul, 1u is asolut ion of tile equation " l  *X = \[EP93\] 
ul. Since 1 is a unit element of /" ,  vx * 1 = ul. Since 
1 is a unit element of F, 1 C F. Because tq ~ \[', 1 
is a solution of the equation ul * X = ul. Since F is 
a group, lry = 1 by the uniqueness of solution. This \[Gen35\] 
conclusion is independent of the choice of the element 
ut. \[CSsq 
7. Conc lus ion  and  lh l tu re  "Work  
\[ll~,vss\] 
This paper puts forward an architecture that comlfines 
several estahlished NL generation techniques adapted 
for a particular application, namely the presentation 
of ND style proofs. We hope that this architecture is
also of general interest beyond this particular applic- 
ation. 
The most important feature of this model is that 
hierarchical planning and unplanned spontaneous 
presentation are integrated in a nnitbrm framework. 
~Ibp-down hierarchical planning views language gmmr- 
ation ,'~s planned behavior. Bmsed on explicit colnn!u- 
nicative knowledge ncoded as schemata, hierarchical 
planning splits a presentation task into sul~tasks. Al- 
though our overall presentation mechanism has much 
in common with that of H.ST-Imsed text planm.,rs, the \[I,:i\]9,1\] 
top-down planning operators contain mostly complex 
presentation schemata, like those in schema-based 
plamfing. Since schemata-based planning covers only \[Mct)83\] 
proofs of some particular structure, it is complenlen- 
ted by a mechanism called hottom-up presentation. 
Bottom-up presentation aims at simulating the un- 
planned part of proof presentation, where, a proof node \[McI(.85\] 
or a subproof awaiting present.ation is chosen as the 
next to he presented via the local derivation relations. \[Moo89\] 
Since more than one such node is often available, the 
local focus mechanism is employed to single out. the 
candidate having the strongest semantic links with the \[Och7!J\] 
focal centers. The distinction between l~lanned and 
unplanned behavior enables a very natural segment- \[ParS8\] 
ation of the discourse into an attentional hierarchy. 
This provide an appropriate basis for a discourse the- 
ory which handles reference choices \[Iluagdb}. \[l'~ei91\] 
Compared with proofs found in mathematical  text- 
books, the output of PROVERB is still to ,  tedious 
and inflexible. The tediousness i largely ascril)ed to \[Sibg0\] 
the lack of plan level knowledge of the input proofs, 
which distinguishes crucial steps from unimportant 
details. Therefore, sophisticated plan recognition 
techniques are necessary. The inflexibility of text cur- 
rently produced is partly inherited from the schemata- 
based approach, for which a fine-grained plamfing in 
terms of single PCAs might he a remedy. It is also 
partly due to the fixed lexicon choice, which we are 
currently reimplcnmntiug. 
References  
\[Ilua94a\] 
\[ltuag,ll,\] 
\[IIua94 b\] 
l). Chester. The translation o\[ formal proofs 
into English. Artificial Intelligence, 1976. 
R. Dale. Generating Re/erring I'Txpressio,Js. 
MVI" Press, 1092. 
A. Edgar and P..\]. Pelletier. Natural angnage 
explat)llttiott of lla.Lllr;tl de.dllctiolt proofs, in 
Pr'oe. of the first Conf. of the t'aeqic Assoc. 
for Comp. Linguistics, 1993. 
G. Qentzen. Untersuchungen (iber das logische 
Schliegen 1. Math. Zeitsehrift, 1935. 
B. J. Grosz and C. L. Sidaer. Attention, inten- 
tions, and the structure of discourse. Compu- 
tational Liug.istics, 1986. 
E. It. \[Iovy. (/enerating Natural Language un- 
der Progm,tie Coustrints. L;twrence Erlbaum 
Associates, \[IillsdMe, 1988. 
X. IIuang. Reconstructing proofs at the asser- 
tion h-'vel. In Proc. o\] l~th CADE, 199,1, forth- 
eOlllitlg. 
X. IIuitng. Pl~tnning Reference Choices for Ar- 
gumentatiw: ~D:xts. In Proe. of the 7th Inter- 
national Wo,'kshol~ on Natural Language Gen- 
e.ratiott, 199,1, forthcoming. 
X. IIuang. A Reconstructive Approach to Hu- 
man Oriented Proof P~vsentation. PlID thesis, 
(lniversitiit des Satarlatndes, (-lermally, 1994, 
forthcoming. 
A. Kilger. Using U'I'AQs for increment:d 
;tnd parallel generatitm. Computationallntelli- 
\[lence, forthcoming, 11994. 
1). l). McDonald. Natural language gen- 
eration its ;t COmlmt,'ttionM prn\[~leln. In 
Brady/llcrwick: Computational Models of Dis- 
course. MI'\]." Press, 1983. 
\[(. R. MeI(ec~wn. Te:L't (~eneratlon, Cambridge 
University Press, 1985. 
J. I). Moore. A Reactive Approach to l:2xphm- 
ation i.  Expert arm Adoice-Gioin9 Systems. 
PhD thesis, Univ. of California, 1989. 
E. Ochs, I'lamled ;tml ulq)lanned discourse. 
Synt.x and ?'em~mlies, 1979. 
C. P;tris. Taih~ring object descriptions to zt 
user's lewd of experti:;e. Compulutional Lin- 
guisl.:s, 1988. 
N. l{eithinger. Eine parallch~ Architektur zur 
inkrententeller Dialogbeitriige. Pill) thesis, Uni- 
versitiit ies Saarlrtndes, 1991. 
P. Sibun. The loc;tl org,'tnization of text. In 
I(.R. McKeown eta1, editors, Proc. of the 5th 
International Workshop on Natural Language 
(?ener.tion, 1990. 
.~3 
