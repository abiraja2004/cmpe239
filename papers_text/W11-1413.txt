Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 105?110,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Generating Example Contexts to Illustrate a Target Word Sense 
 
 
Jack Mostow Weisi Duan 
Carnegie Mellon University Carnegie Mellon University 
RI-NSH 4103, 5000 Forbes Avenue Language Technologies Institute 
Pittsburgh, PA 15213-3890, USA Pittsburgh, PA 15213-3890, USA 
mostow@cs.cmu.edu wduan@cs.cmu.edu 
 
  
Abstract 
Learning a vocabulary word requires seeing it 
in multiple informative contexts.  We describe 
a system to generate such contexts for a given 
word sense.  Rather than attempt to do word 
sense disambiguation on example contexts al-
ready generated or selected from a corpus, we 
compile information about the word sense into 
the context generation process.  To evaluate the 
sense-appropriateness of the generated contexts 
compared to WordNet examples, three human 
judges chose which word sense(s) fit each ex-
ample, blind to its source and intended sense.  
On average, one judge rated the generated ex-
amples as sense-appropriate, compared to two 
judges for the WordNet examples.  Although 
the system?s precision was only half of Word-
Net?s, its recall was actually higher than 
WordNet?s, thanks to covering many senses for 
which WordNet lacks examples. 
 
1 Introduction 
Learning word meaning from example contexts is 
an important aspect of vocabulary learning. Con-
texts give clues to semantics but also convey many 
other lexical aspects, such as parts of speech, mor-
phology, and pragmatics, which help enrich a per-
son?s word knowledge base (Jenkins 1984; Nagy et 
al. 1985; Schatz 1986; Herman et al 1987; Nagy 
et al 1987; Schwanenflugel et al 1997; Kuhn and 
Stahl 1998; Fukkink et al 2001). Accordingly, one 
key issue in vocabulary instruction is how to find 
or create good example contexts to help children 
learn a particular sense of a word. Hand-vetting 
automatically generated contexts can be easier than 
hand-crafting them from scratch (Mitkov et al 
2006; Liu et al 2009). 
This paper describes what we believe is the first 
system to generate example contexts for a given 
target sense of a polysemous word.  Liu et al 
(2009) characterized good contexts for helping 
children learn vocabulary and generated them for a 
target part of speech, but not a given word sense.  
Pino and Eskenazi  (2009) addressed the polysemy 
issue, but in a system for selecting contexts rather 
than for generating them.  Generation can supply 
more contexts for a given purpose, e.g. teaching 
children, than WordNet or a fixed corpus contains. 
Section 2 describes a method to generate sense-
targeted contexts. Section 3 compares them to 
WordNet examples.  Section 4 concludes. 
2 Approach 
An obvious way to generate sense-targeted con-
texts is to generate contexts containing the target 
word, and use Word Sense Disambiguation (WSD) 
to select the ones that use the target word sense.  
However, without taking the target word sense into 
account, the generation process may not output any 
contexts that use it. Instead, we model word senses 
as topics and incorporate their sense indicators into 
the generation process ? words that imply a unique 
word sense when they co-occur with a target word.   
For example, retreat can mean ?a place of pri-
vacy; a place affording peace and quiet.?  Indica-
tors for this sense, in decreasing order of Pr(word | 
topic for target sense), include retreat, yoga, place, 
retreats, day, home, center, church, spiritual, life, 
city, time, lake, year, room, prayer, years, school, 
dog, park, beautiful, area, and stay.  Generated 
contexts include ?retreat in this bustling city?. 
Another sense of retreat (as defined in Word-
Net) is ?(military) a signal to begin a withdrawal 
105
from a dangerous position,? for which indicators 
include states, war, united, american, military, 
flag, president, world, bush, state, Israel, Iraq, in-
ternational, national, policy, forces, foreign, na-
tion, administration, power, security, iran, force, 
and Russia.  Generated contexts include ?military 
leaders believe that retreat?. 
We decompose our approach into two phases, 
summarized in Figure 1.  Section 2.1 describes the 
Sense Indicator Extraction phase, which obtains 
indicators for each WordNet synset of the target 
word.  Section 2.2 describes the Context Genera-
tion phase, which generates contexts that contain 
the target word and indicators for the target sense. 
 
Figure 1: overall work flow diagram 
2.1 Sense Indicator Extraction 
Kulkarni and Pedersen (2005) and Duan and Yates 
(2010) performed Sense Indicator Extraction, but 
the indicators they extracted are not sense targeted.  
Content words in the definition and examples for 
each sense are often good indicators for that sense, 
but we found that on their own they did poorly. 
One reason is that such indicators sometimes co-
occur with a different sense.  But the main reason 
is that there are so few of them that the word sense 
often appears without any of them.  Thus we need 
more (and if possible better) sense indicators. 
 To obtain sense-targeted indicators for a target 
word, we first assemble a corpus by issuing a 
Google query for each synset of the target word.  
The query lists the target word and all content 
words in the synset?s WordNet definition and ex-
amples, and specifies a limit of 200 hits.  The re-
sulting corpus contains a few hundred documents. 
To extract sense indicators from the corpus for a 
word, we adapt Latent Dirichlet Allocation (LDA) 
(Blei et al 2003).  LDA takes as input a corpus of 
documents and an integer k, and outputs k latent 
topics, each represented as a probability distribu-
tion over the corpus vocabulary.  For k, we use the 
number of word senses.  To bias LDA to learn top-
ics corresponding to the word senses, we use the 
content words in their WordNet definitions and 
examples as seed words. 
After learning these topics and filtering out stop 
words, we pick the 30 highest-probability words 
for each topic as indicators for the corresponding 
word sense, filtering out any words that also indi-
cate other senses. We create a corpus for each tar-
get word and run LDA on it. 
Having outlined the extraction process, we now 
explain in more detail how we learn the topics; the 
mathematically faint-hearted may skip to Section 
2.2.  Formally, given corpus   with  documents, 
let   be the number of topics, and let    and    be 
the parameters of the document and topic distribu-
tions respectively.  LDA assumes this generative 
process for each document    for a corpus  : 
1. Choose            where           
2. Choose            where           
3. For each word      in    where   
       ,   is the number of words in    
(a) Choose a topic                       
(b) Choose a topic                     
where        
In classical LDA, all   ?s are the same. We al-
low them to be different in order to use the seed 
words as high confidence indicators of target 
senses to bias the hyper-parameters of their docu-
ment distributions. 
For inference, we use Gibbs Sampling (Steyvers 
and Griffiths 2006) with transition probability  
                            
              
               
 
              
                  
 
Here        denotes the topic assignments to all 
other words in the corpus except     ;            
is the number of times word   is assigned to topic 
  in the whole corpus;          is the number of 
words assigned to topic   in the entire corpus; 
106
          is the count of tokens assigned to topic 
  in document   ; and    and       are the hyper-
parameters on     and      respectively in the two 
Dirichlet distributions. 
For each document    that contains seed words 
of some synset, we bias    toward the topic   for 
that synset by making      larger; specifically, we 
set each      to 10 times the average value of   .  
This bias causes more words     in    to be as-
signed to topic   because the words of    are likely 
to be relevant to  . These assignments then influ-
ence the topic distribution of   so as to make      
likelier to be assigned to   in any document     , 
and thus shift the document distribution in      
towards  . By this time we are back to the start of 
the loop where the document distribution of      
is biased to  .  Thus this procedure can discover 
more sense indicators for each sense. 
Our method is a variant of Labeled LDA (L-
LDA) (Ramage 2009), which allows only labels 
for each document as topics.  In contrast, our va-
riant allows all topics for each document, because 
it may use more than one sense of the target word.  
Allowing other senses provides additional flexibili-
ty to discover appropriate sense indicators. 
The LDA method we use to obtain sense indica-
tors fits naturally into the framework of bootstrap-
ping WSD (Yarowsky 1995; Mihalcea 2002; 
Martinez et al 2008; Duan and Yates 2010), in 
which seeds are given for each target word, and the 
goal is to disambiguate the target word by boot-
strapping good sense indicators that can identify 
the sense.  In contrast to WSD, our goal is to gen-
erate contexts for each sense of the target word.  
2.2 Context Generation 
To generate sense-targeted contexts, we extend the 
VEGEMATIC context generation system (Liu et 
al. 2009). VEGEMATIC generates contexts for a 
given target word using the Google N-gram cor-
pus.  Starting with a 5-gram that contains the target 
word, VEGEMATIC extends it by concatenating 
additional 5-grams that overlap by 4 words on the 
left or right. 
To satisfy various constraints on good contexts 
for learning the meaning of a word, VEGEMATIC 
uses various heuristic filters.  For example, to gen-
erate contexts likely to be informative about the 
word meaning, VEGEMATIC prefers 5-grams that 
contain words related to the target word, i.e., that 
occur more often in its presence.  However, this 
criterion is not specific to a particular target sense. 
To make VEGEMATIC sense-targeted, we 
modify this heuristic to prefer 5-grams that contain 
sense indicators.  We assign the generated contexts 
to the senses whose sense indicators they contain. 
We discard contexts that contain sense indicators 
for more than one sense. 
3 Experiments and Evaluation 
To evaluate our method, we picked 8 target words 
from a list of polysemous vocabulary words used 
in many domains and hence important for children 
to learn (Beck et al 2002).  Four of them are 
nouns:  advantage (with 3 synsets), content (7), 
force (10), and retreat (7). Four are verbs:  dash 
(6), decline (7), direct (13), and reduce (20).  Some 
of these words can have other parts of speech, but 
we exclude those senses, leaving 73 senses in total. 
We use their definitions from WordNet because 
it is a widely used, comprehensive sense inventory. 
Some alternative sense inventories might be un-
suitable. For instance, children?s dictionaries may 
lack WordNet?s rare senses or hypernym relations. 
We generated contexts for these 73 word senses 
as described in Section 2, typically 3 examples for 
each word sense.  To reduce the evaluation burden 
on our human judges, we chose just one context for 
each word sense, and for words with more than 10 
senses we chose a random sample of them.  To 
avoid unconscious bias, we chose random contexts 
rather than the best ones, which a human would 
likelier pick if vetting the generated contexts by 
hand.  For comparison, we also evaluated WordNet 
examples (23 in total) where available. 
We gave three native English-speaking college-
educated judges the examples to evaluate indepen-
dently, blind to their intended sense.  They filled in 
a table for each target word.  The left column listed 
the examples (both generated and WordNet) in 
random order, one per row.  The top row gave the 
WordNet definition of each synset, one per col-
umn.  Judges were told:  For each example, put a 
1 in the column for the sense that best fits how 
the example uses the target word.  If more than 
one sense fits, rank them 1, 2, etc.  Use the last 
two columns only to say that none of the senses 
fit, or you can't tell, and why.  (Only 10 such cas-
es arose.) 
We measured inter-rater reliability at two levels. 
107
At the fine-grained level, we measured how well 
the judges agreed on which one sense fit the exam-
ple best.  The value of Fleiss? Kappa (Shrout and 
Fleiss 1979) was 42%, considered moderate.  At 
the coarse-grained level, we measured how well 
judges agreed on which sense(s) fit at all.  Here 
Fleiss? Kappa was 48%, also considered moderate. 
We evaluated the examples on three criteria. 
Yield is the percentage of intended senses for 
which we generate at least one example ? whether 
it fits or not.  For the 73 synsets, this percentage is 
92%.  Moreover, we typically generate 3 examples 
for a word sense.  In comparison, only 34% of the 
synsets have even a single example in WordNet. 
 (Fine-grained) precision is the percentage of 
examples that the intended sense fits best accord-
ing to the judges. Human judges often disagree, so 
we prorate this percentage by the percentage of 
judges who chose the intended sense as the best fit.  
The result is algebraically equivalent to computing 
precision separately according to each judge, and 
then averaging the results.  Precision for generated 
examples was 36% for those 23 synsets and 27% 
for all 67 synsets with generated examples.  Al-
though we expected WordNet to be a gold stan-
dard, its precision for the 23 synsets having 
examples was 52% ? far less than 100%. 
This low precision suggests that the WordNet 
contexts to illustrate different senses were often 
not informative enough for the judges to distin-
guish them from all the other senses.  For example, 
the WordNet example reduce one?s standard of 
living is attached to the sense ?lessen and make 
more modest.?  However, this sense is hard to dis-
tinguish from ?lower in grade or rank or force 
somebody into an undignified situation.? In fact, 
two judges did not choose the first sense, and one 
of them chose the second sense as the best fit.   
Coarse-grained precision is similar, but based on 
how often the intended sense fits the example at 
all, whether or not it fits best.  Coarse-grained pre-
cision was 67% for the 23 WordNet examples, 
40% for the examples generated for those 23 syn-
sets, and 33% for all 67 generated examples. 
Coarse-grained precision is important because 
fine-grained semantic distinctions do not matter in 
illustrating a core sense of a word.  The problem of 
how to cluster fine-grained senses into coarse 
senses is hard, especially if consensus is required 
(Navigli et al 2007). Rather than attempt to identi-
fy a single definitive partition of a target word?s 
synsets into coarse senses, we implicitly define a 
coarse sense as the subset of synsets rated by a 
judge as fitting a given example.  Thus the cluster-
ing into coarse senses is not only judge-specific but 
example-specific:   different, possibly overlapping 
sets of synsets may fit different examples. 
Recall is the percentage of synsets that fit their 
generated examples. Algebraically it is the product 
of precision and yield.  Fine-grained recall was 
25% for the generated examples, compared to only 
18% for the WordNet examples. Coarse-grained 
recall was 30% for the generated examples, com-
pared to 23% for the WordNet examples. 
Figure 2 shows how yield, inter-rater agreement, 
and coarse and fine precision for the 8 target words 
vary with their number of synsets.  With so few 
words, this analysis is suggestive, not conclusive. 
We plot all four metrics on the same [0,1] scale to 
save space, but only the last two metrics have di-
rectly comparable values,  However, it is still mea-
ningful to compare how they vary.  Precision and 
inter-rater reliability generally appear to decrease 
with the number of senses.  As polysemy increases, 
the judges have more ways to disagree with each 
other and with our program.  Yield is mostly high, 
but might be lower for words with many senses, 
due to deficient document corpora for rare senses.   
  
Figure 2: Effects of increasing polysemy 
Table 1 compares the generated and WordNet 
examples on various measures.  It compares preci-
sion on the same 23 senses that have WordNet ex-
amples.  It compares recall on all 73 senses.  It 
compares Kappa on the 23 WordNet examples and 
the sample of generated examples the judges rated.   
Number of target word synsets 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0 5 10 15 20
Yield
Fleiss' 
Kappa
Precision 
(Coarse)
Precision 
(Fine)
108
 
Generated WordNet 
Yield 92% 34% 
Senses with examples 67 23 
Avg. words in context 5.91 7.87 
Precision 
(same 23) 
    Fine 36% 52% 
Coarse 40% 67% 
Recall 
Fine  25% 18% 
Coarse 30% 23% 
Fleiss? 
Kappa 
Fine 0.43 0.39 
Coarse 0.48 0.49 
Table 1: Generated examples vs. WordNet 
Errors occur when 1) the corpus is missing a 
word sense; 2) LDA fails to find good sense indi-
cators; or 3) Context Generation fails to generate a 
sense-appropriate context. 
Our method succeeds when (1) the target sense 
occurs in the corpus, (2) LDA finds good indica-
tors for it, and (3) Context Generation uses them to 
construct a sense-appropriate context.  For exam-
ple, the first sense of advantage is ?the quality of 
having a superior or more favorable position,? for 
which we obtain the sense indicators support, work, 
time, order, life, knowledge, mind, media, human, 
market, experience, nature, make, social, informa-
tion, child, individual, cost, people, power, good, 
land, strategy, and company, and generate (among 
others) the context ?knowledge gave him an ad-
vantage?. 
Errors occur when any of these 3 steps fails.  
Step 1 fails for the sense ?reduce in scope while 
retaining essential elements? of reduce because it 
is so general that no good example exists in the 
corpus for it.  Step 2 fails for the sense of force in 
?the force of his eloquence easily persuaded them? 
because its sense indicators are men, made, great, 
page, man, time, general, day, found, side, called, 
and house.  None of these words are precise 
enough to convey the sense.  Step 3 fails for the 
sense of advantage as ?(tennis) first point scored 
after deuce,? with sense indicators point, game, 
player, tennis, set, score, points, ball, court, ser-
vice, serve, called, win, side, players, play, team, 
games, match, wins, won, net, deuce, line, oppo-
nent, and turn.  This list looks suitably tennis-
related.  However, the generated context ?the 
player has an advantage? fits the first sense of 
advantage; here the indicator player for the tennis 
sense is misleading. 
4 Contributions and Limitations 
This paper presents what we believe is the first 
system for generating sense-appropriate contexts to 
illustrate different word senses even if they have 
the same part of speech.  We define the problem of 
generating sense-targeted contexts for vocabulary 
learning, factor it into Sense Indicator Extraction 
and Context Generation, and compare the resulting 
contexts to WordNet in yield, precision, and recall 
according to human judges who decided, given 
definitions of all senses, which one(s) fit each con-
text, without knowing its source or intended sense.  
This test is much more stringent than just deciding 
whether a given word sense fits a given context. 
There are other possible baselines to compare 
against, such as Google snippets. However, Google 
snippets fare poorly on criteria for teaching child-
ren vocabulary (Liu et al under revision).  Another 
shortcoming of this alternative is the inefficiency 
of retrieving all contexts containing the target word 
and filtering out the unsuitable ones.  Instead, we 
compile constraints on suitability into a generator 
that constructs only contexts that satisfy them.  
Moreover, in contrast to retrieve-and-filter, our 
constructive method (concatenation of overlapping 
Google 5-grams) can generate novel contexts. 
There is ample room for future improvement. 
We specify word senses as WordNet synsets rather 
than as coarser-grain dictionary word senses more 
natural for educators.  Our methods for target word 
document corpus construction, Sense Indicator 
Extraction, and Context Generation are all fallible.  
On average, 1 of 3 human judges rated the result-
ing contexts as sense-appropriate, half as many as 
for WordNet examples.  However, thanks to high 
yield, their recall surpassed the percentage of syn-
sets with WordNet examples.  The ultimate crite-
rion for evaluating them will be their value in 
tutorial interventions to help students learn vocabu-
lary. 
Acknowledgments 
This work was supported by the Institute of Educa-
tion Sciences, U.S. Department of Education, 
through Grant R305A080157 to Carnegie Mellon 
University. The opinions expressed are those of the 
authors and do not necessarily represent the views 
of the Institute or the U.S. Department of Educa-
tion.  We thank the reviewers and our judges. 
109
References  
Isabel L.  Beck, Margaret G. Mckeown and Linda 
Kucan. 2002. Bringing Words to Life:  Robust 
Vocabulary Instruction. NY, Guilford. 
David Blei, Andrew Ng and Michael Jordan. 2003. 
Latent Dirichlet alocation. Journal of Machine 
Learning Research 3: 993?1022. 
Weisi Duan and Alexander Yates. 2010. Extracting 
Glosses to Disambiguate Word Senses. Human 
Language Technologies: The 2010 Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics, Los 
Angeles. 
Ruben G. Fukkink, Henk Blok and Kees De Glopper. 
2001. Deriving word meaning from written context: 
A multicomponential skill. Language Learning 51(3): 
477-496. 
Patricia A. Herman, Richard C. Anderson, P. David 
Pearson and William E. Nagy. 1987. Incidental 
acquisition of word meaning from expositions with 
varied text features. Reading Research Quarterly 
22(3): 263-284. 
Joseph R. Jenkins, Marcy  Stein and Katherine Wysocki. 
1984. Learning vocabulary through reading. 
American Educational Research Journal 21: 767-787. 
Melanie R. Kuhn and Steven A. Stahl. 1998. Teaching 
children to learn word meaning from context: A 
synthesis and some questions. Journal of Literacy 
Research 30(1): 119-138. 
Anagha Kulkarni and Ted Pedersen. 2005. Name 
discrimination and email clustering using 
unsupervised clustering and labeling of similar 
contexts. Proceedings of the Second Indian 
International Conference on Artificial Intelligence, 
Pune, India. 
Liu Liu, Jack Mostow and Greg Aist. 2009. Automated 
Generation of Example Contexts for Helping 
Children Learn Vocabulary. Second ISCA Workshop 
on Speech and Language Technology in Education 
(SLaTE), Wroxall Abbey Estate, Warwickshire, 
England. 
Liu Liu, Jack Mostow and Gregory S. Aist. under 
revision. Generating Example Contexts to Help 
Children Learn Word Meaning. Journal of Natural 
Language Engineering. 
David Martinez, Oier Lopez de Lacalle and Eneko 
Agirre. 2008. On the use of automatically acquired 
examples for all-nouns word sense disambiguation. 
Journal of Artificial Intelligence Research 33: 79--
107. 
Rada Mihalcea. 2002. Bootstrapping large sense tagged 
corpora. Proceedings of the 3rd International 
Conference on Languages Resources and Evaluations 
LREC 2002, Las Palmas, Spain. 
R. Uslan Mitkov, Le An Ha and Nikiforos Karamanis. 
2006. A computer-aided environment for generating 
multiple choice test items. Natural Language 
Engineering 12(2): 177-194. 
William E. Nagy, Richard C. Anderson and Patricia A. 
Herman. 1987. Learning Word Meanings from 
Context during Normal Reading. American 
Educational Research Journal 24(2): 237-270. 
William E. Nagy, Patricia A. Herman and Richard C. 
Anderson. 1985. Learning words from context. 
Reading Research Quarterly 20(2): 233-253. 
Roberto Navigli, Kenneth C. Litkowski and Orin 
Hargraves. 2007. Semeval-2007 task 07: Coarse-
grained English all-words task. Proceedings of the 
4th International Workshop on Semantic Evaluations, 
Association for Computational Linguistics: 30-35. 
Juan Pino and Maxine Eskenazi. 2009. An Application 
of Latent Semantic Analysis to Word Sense 
Discrimination for Words with Related and Unrelated 
Meanings. The 4th Workshop on Innovative Use of 
NLP for Building Educational Applications, 
NAACL-HLT 2009 Workshops, Boulder, CO, USA. 
Daniel Ramage, David Hall, Ramesh Nallapati, and 
Christopher D. Manning. 2009. Labeled LDA: A 
supervised topic model for credit attribution in multi-
labeled corpora. Proceedings of the 2009 Conference 
on Empirical Methods in Natural Language 
Processing, Association for Computational 
Linguistics. 
Elinore K. Schatz and R. Scott Baldwin. 1986. Context 
clues are unreliable predictors of word meanings. 
Reading Research Quarterly 21: 439-453. 
Paula J. Schwanenflugel, Steven A. Stahl and Elisabeth 
L. Mcfalls. 1997. Partial Word Knowledge and 
Vocabulary Growth during Reading Comprehension. 
Journal of Literacy Research 29(4): 531-553. 
Patrick E. Shrout and Joseph L. Fleiss. 1979. Intraclass 
correlations: Uses in assessing rater reliability. 
Psychological Bulletin 86(2): 420-428. 
Mark Steyvers and Tom Griffiths. 2006. Probabilistic 
topic models. Latent Semantic Analysis: A Road to 
Meaning. T. Landauer, D. McNamara, S. Dennis and 
W. Kintsch. Hillsdale, NJ, Laurence Erlbaum. 
David Yarowsky. 1995. Unsupervised WSD rivaling 
supervised methods. Proceedings of the 33rd Annual 
Meeting of the Association for Computational 
Linguistics, Massachusetts Institute of Technology, 
Cambridge, MA. 
  
 
 
110
