Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 842?850,
Beijing, August 2010
Co-STAR: A Co-training Style Algorithm for Hyponymy Relation
Acquisition from Structured and Unstructured Text
Jong-Hoon Oh, Ichiro Yamada, Kentaro Torisawa, and Stijn De Saeger
Language Infrastructure Group, MASTAR Project,
National Institute of Information and Communications Technology (NICT)
{rovellia,iyamada,torisawa,stijn}@nict.go.jp
Abstract
This paper proposes a co-training style
algorithm called Co-STAR that acquires
hyponymy relations simultaneously from
structured and unstructured text. In Co-
STAR, two independent processes for hy-
ponymy relation acquisition ? one han-
dling structured text and the other han-
dling unstructured text ? collaborate by re-
peatedly exchanging the knowledge they
acquired about hyponymy relations. Un-
like conventional co-training, the two pro-
cesses in Co-STAR are applied to dif-
ferent source texts and training data.
We show the effectiveness of this al-
gorithm through experiments on large-
scale hyponymy-relation acquisition from
Japanese Wikipedia and Web texts. We
also show that Co-STAR is robust against
noisy training data.
1 Introduction
Acquiring semantic knowledge, especially se-
mantic relations between lexical terms, is re-
garded as a crucial step in developing high-level
natural language applications. This paper pro-
poses Co-STAR (a Co-training STyle Algorithm
for hyponymy Relation acquisition from struc-
tured and unstructured text). Similar to co-
training (Blum and Mitchell, 1998), two hy-
ponymy relation extractors in Co-STAR, one for
structured and the other for unstructured text, it-
eratively collaborate to boost each other?s perfor-
mance.
Many algorithms have been developed to auto-
matically acquire semantic relations from struc-
tured and unstructured text. Because term pairs
are encoded in structured and unstructured text in
different styles, different kinds of evidence have
been used for semantic relation acquisition:
Evidence from unstructured text: lexico-
syntactic patterns and distributional similar-
ity (Ando et al, 2004; Hearst, 1992; Pantel
et al, 2009; Snow et al, 2006; De Saeger et
al., 2009; Van Durme and Pasca, 2008);
Evidence from structured text: topic hierarchy,
layout structure of documents, and HTML
tags (Oh et al, 2009; Ravi and Pasca, 2008;
Sumida and Torisawa, 2008; Shinzato and
Torisawa, 2004).
Recently, researchers have used both structured
and unstructured text for semantic-relation acqui-
sition, with the aim of exploiting such different
kinds of evidence at the same time. They ei-
ther tried to improve semantic relation acquisition
by putting the different evidence together into a
single classifier (Pennacchiotti and Pantel, 2009)
or to improve the coverage of semantic relations
by combining and ranking the semantic relations
obtained from two source texts (Talukdar et al,
2008).
In this paper we propose an algorithm called
Co-STAR. The main contributions of this work
can be summarized as follows.
? Co-STAR is a semi-supervised learning
method composed of two parallel and iter-
ative processes over structured and unstruc-
tured text. It was inspired by bilingual co-
training, which is a framework for hyponymy
relation acquisition from source texts in two
languages (Oh et al, 2009). Like bilingual
co-training, two processes in Co-STAR op-
erate independently on structured text and
unstructured text. These two processes are
trained in a supervised manner with their
initial training data and then each of them
tries to enlarge the existing training data of
the other by iteratively exchanging what they
842
have learned (more precisely, by transfer-
ring reliable classification results on com-
mon instances to one another) (see Section
4 for comparison Co-STAR and bilingual
co-training). Unlike the ensemble semantic
framework (Pennacchiotti and Pantel, 2009),
Co-STAR does not have a single ?master?
classifier or ranker to integrate the differ-
ent evidence found in structured and unstruc-
tured text. We experimentally show that, at
least in our setting, Co-STAR works better
than a single ?master? classifier.
? Common relation instances found in both
structured and unstructured text act as a
communication channel between the two ac-
quisition processes. Each process in Co-
STAR classifies common relation instances
and then transfers its high-confidence classi-
fication results to training data of the other
process (as shown in Fig. 1), in order to im-
prove classification results of the other pro-
cess. Moreover, the efficiency of this ex-
change can be boosted by increasing the
?bandwidth? of this channel. For this pur-
pose each separate acquisition process auto-
matically generates a set of relation instances
that are likely to be negative. In our experi-
ments, we show that the above idea proved
highly effective.
? Finally, the acquisition algorithm we propose
is robust against noisy training data. We
show this by training one classifier in Co-
STAR with manually labeled data and train-
ing the other with automatically generated
but noisy training data. We found that Co-
STAR performs well in this setting. This is-
sue is discussed in Section 6.
This paper is organized as follows. Sections 2
and 3 precisely describe our algorithm. Section 4
describes related work. Sections 5 and 6 describe
our experiments and present their results. Conclu-
sions are drawn in Section 7.
2 Co-STAR
Co-STAR consists of two processes that simul-
taneously but independently extract and classify
Structured	 ?Texts	 Unstructured	 ?Texts	
Itera?on	
Training	 ?Data	 ?for	 ?Structured	 ?Texts	
Classifier	 Classifier	Training	 Training	
Enlarged	 ?	 ?Training	 ?Data	 ?for	 ?Structured	 ?Text	
Enlarged	 ?	 ?Training	 ?Data	 ?for	 ?Unstructured	 ?Texts	
Training	 ?Data	 ?For	 ?Unstructured	 ?Texts	
Classifier	Classifier	
Further	 ?Enlarged	 ?Training	 ?Data	 ?for	 ?Structured	 ?Texts	
Further	 ?Enlarged	 ?Training	 ?Data	 ?for	 ?Unstructured	 ?Texts	
Training	
Training	 Training	
Training	
?..	 ?..	Common	 ?instances	Transferring	 ?reliable	 ?classifica?on	 ?results	 ?of	 ?classifiers	
Transferring	 ?reliable	 ?classifica?on	 ?results	 ?of	 ?classifiers	
Figure 1: Concept of Co-STAR.
hyponymy relation instances from structured and
unstructured text. The core of Co-STAR is the
collaboration between the two processes, which
continually exchange and compare their acquired
knowledge on hyponymy relations. This collabo-
ration is made possible through common instances
shared by both processes. These common in-
stances are classified separately by each process,
but high-confidence classification results by one
process can be transferred as new training data to
the other.
2.1 Common Instances
Let S and U represent a source (i.e. corpus)
of structured and unstructured text, respectively.
In this paper, we use the hierarchical layout of
Wikipedia articles and the Wikipedia category
system as structured text S (see Section 3.1), and
a corpus of ordinary Web pages as unstructured
text U . Let XS and XU denote a set of hyponymy
relation candidates extracted from S and U , re-
spectively. XS is extracted from the hierarchi-
cal layout of Wikipedia articles (Oh et al, 2009)
and XU is extracted by lexico-syntactic patterns
for hyponymy relations (i.e., hyponym such as hy-
ponymy) (Ando et al, 2004) (see Section 3 for a
detailed explanation)
We define two types of common instances,
called ?genuine? common instances (G) and ?vir-
tual? common instances (V ). The set of common
instances is denoted by Y = G ? V . Genuine
common instances are hyponymy relation candi-
dates found in both S and U (G = XS ?XU ). On
843
the other hand, term pairs are obtained as virtual
common instances when:
? 1) they are extracted as hyponymy relation
candidates in either S or U and;
? 2) they do not seem to be a hyponymy rela-
tion in the other text
The first condition corresponds to XS ? XU .
Term pairs satisfying the second condition are de-
fined as RS and RU , where RS ? XS = ? and
RU ?XU = ?.
RS contains term pairs that are found in the
Wikipedia category system but neither term ap-
pears as ancestor of the other1. For example, (nu-
trition,protein) and (viruses,viral disease), respec-
tively, hold a category-article relation, where nu-
trition is not ancestor of viruses and vice versa in
the Wikipedia category system. Here, term pairs,
such as (nutrition, viruses) and (viral disease, nu-
trition), can be ones in RS .
RU is a set of term pairs extracted from U
when:
? they are not hyponymy relation candidates in
XU and;
? they regularly co-occur in the same sentence
as arguments of the same verb (e.g., A cause
B or A is made by B);
As a result, term pairs in RU are thought as hold-
ing some other semantic relations (e.g., A and B
in ?A cause B? may hold a cause/effect relation)
than hyponymy relation. Finally, virtual common
instances are defined as:
? V = (XS ?XU ) ? (RS ?RU )
The virtual common instances, from the view-
point of either S or U , are unlikely to hold a hy-
ponymy relation even if they are extracted as hy-
ponymy relation candidates in the other text. Thus
many virtual common instances would be a nega-
tive example for hyponymy relation acquisition.
On the other hand, genuine common instances
(hyponymy relation candidates found in both S
1A term pair often holds a hyponymy relation if one term
in the term pair is a parent of the other in the Wikipedia cat-
egory system (Suchanek et al, 2007).
and U ) are more likely to hold a hyponymy re-
lation than virtual common instances.
In summary, genuine and virtual common in-
stances can be used as different ground for collab-
oration as well as broader collaboration channel
between the two processes than genuine common
instances used alone.
2.2 Algorithm
We assume that classifier c assigns class label
cl ? {yes, no} (?yes? (hyponymy relation) or
?no? (not a hyponymy relation)) to instances in
x ? X with confidence value r ? R+, a non-
negative real number. We denote the classifica-
tion result by classifier c as c(x) = (x, cl, r). We
used support vector machines (SVMs) in our ex-
periments and the absolute value of the distance
between a sample and the hyperplane determined
by the SVMs as confidence value r.
1: Input: Common instances (Y = G ? V ) and
the initial training data (L0S and L0U )
2: Output: Two classifiers (cnS and cnU )
3: i = 0
4: repeat
5: ciS := LEARN(LiS)
6: ciU := LEARN(LiU )
7: CRiS := {ciS(y)|y ? Y , y /? LiS ? LiU}
8: CRiU := {ciU (y)|y ? Y , y /? LiS ? LiU}
9: for each (y, clS , rS) ? TopN(CRiS) and
(y, clU , rU ) ? CRiU do
10: if (rS > ? and rU < ?)
or (rS > ? and clS = clU ) then
11: L(i+1)U := L
(i+1)
U ? {(y, clS)}
12: end if
13: end for
14: for each (y, clU , rU ) ? TopN(CRiU ) and
(y, clS , rS) ? CRiS do
15: if (rU > ? and rS < ?)
or (rU > ? and clS = clU ) then
16: L(i+1)S := L
(i+1)
S ? {(y, clU )}
17: end if
18: end for
19: i = i+ 1
20: until stop condition is met
Figure 2: Co-STAR algorithm
844
The Co-STAR algorithm is given in Fig. 2. The
algorithm is interpreted as an iterative procedure
1) to train classifiers (ciU , ciS) with the existing
training data (LiS and LiU ) and 2) to select new
training instances from the common instances to
be added to existing training data. These are re-
peated until stop condition is met.
In the initial stage, two classifiers c0S and c0U
are trained with manually prepared labeled in-
stances (or training data) L0S and L0U , respec-
tively. The learning procedure is denoted by
c = LEARN(L) in lines 5?6, where c is a re-
sulting classifier. Then ciS and ciU are applied
to classify common instances in Y (lines 7?8).
We denote CRiS as a set of the classification re-
sults of ciS for common instances, which are not
included in the current training data LiS ? LiU .
Lines 9?13 describe a way of selecting instances
in CRiS to be added to the existing training data
in U . During the selection, ciS acts as a teacher
and ciU as a student. TopN(CRiS) is a set of
ciS(y) = (y, clS , rS), whose rS is the top-N high-
est in CRiS . (In our experiments, N = 900.) The
teacher instructs his student the class label of y if
the teacher can decide the class label of y with a
certain level of confidence (rS > ?) and the stu-
dent satisfies one of the following two conditions:
? the student agrees with the teacher on class
label of y (clS = clU ) or
? the student?s confidence in classifying y is
low (rU < ?)
rU < ? enables the teacher to instruct his student
in spite of their disagreement over a class label.
If one of the two conditions is satisfied, (y, clS)
is added to existing labeled instances L(i+1)U . The
roles are reversed in lines 14?18, so that ciU be-
comes the teacher and ciS the student.
The iteration stops if the change in the differ-
ence between the two classifiers is stable enough.
The stability is estimated by d(ciS , ciU ) in Eq. (1),
where ?i represents the change in the average
difference between the confidence values of the
two classifiers in classifying common instances.
We terminate the iteration if d(ciS , ciU ) is smaller
than 0.001 in three consecutive rounds (Wang and
Zhou, 2007).
d(ciS , ciU ) = |?i ? ?(i?1)|/|?(i?1)| (1)
3 Hyponymy Relation Acquisition
In this section we explain how each process ex-
tracts hyponymy relations from its respective text
source either Wikipedia or Web pages. Each pro-
cess extracts hyponymy relation candidates (de-
noted by (hyper,hypo) in this section). Because
there are many non-hyponymy relations in these
candidates2, we classify hyponymy relation can-
didates into correct hyponymy relation or not. We
used SVMs (Vapnik, 1995) for the classification
in this paper.
3.1 Acquisition from Wikipedia
(a) Layout structure
Range
Siberian tiger
Bengal tiger
Subspecies
Taxonomy
Tiger
Malayan tiger
(b) Tree structure
Figure 3: Example borrowed from Oh et al
(2009): Layout and tree structures of Wikipedia
article TIGER
We follow the method in Oh et al (2009) for
acquiring hyponymy relations from the Japanese
Wikipedia. Every article is transformed into a tree
structure as shown in Fig. 3, based on the items in
its hierarchical layout including title, (sub)section
headings, and list items. Candidate relations are
extracted from this tree structure by regarding a
node as a hypernym candidate and all of its subor-
dinate nodes as potential hyponyms of the hyper-
nym candidate (e.g., (TIGER, TAXONOMY) and
(TIGER, SIBERIAN TIGER) from Fig. 3). We ob-
tained 1.9?107 Japanese hyponymy relation can-
didates from Wikipedia.
2Only 25?30% of candidates was true hyponymy relation
in our experiments.
845
Type Description
Feature from Wikipedia Lexical Morphemes and POS of hyper and hypo; hyper and hypo themselves
(?WikiFeature?) Structure Distance between hyper and hypo in a tree structure;
Lexical patterns for article or section names, where listed items often appear;
Frequently used section headings in Wikipedia (e.g., ?Reference?);
Layout item type (e.g., section or list); Tree node type (e.g., root or leaf);
Parent and children nodes of hyper and hypo
Infobox Attribute type and its value obtained from Wikipedia infoboxes
Feature from Web texts Lexical Morphemes and POS of hyper and hypo; hyper and hypo themselves
(?WebFeature?) Pattern Lexico-syntactic patterns applied to hyper and hypo;
PMI score between pattern and hyponymy relation candidate (hyper,hypo)
Collocation PMI score between hyper and hypo
Noun Class Noun classes relevant to hyper and hypo
Table 1: Feature sets (WikiFeature and WebFeature): hyper and hypo represent hypernym and hyponym
parts of hyponymy relation candidates, respectively.
As features for classification we used lex-
ical, structure, and infobox information from
Wikipedia (WikiFeature), as shown in Table 1.
Because they are the same feature sets as those
used in Oh et al (2009), here we just give a brief
overview of the feature sets. Lexical features3
are used to recognize the lexical evidence for
hyponymy relations encoded in hyper and hypo.
For example, the common head morpheme tiger
in (TIGER, BENGAL TIGER) can be used as the
lexical evidence. Such information is provided
along with the words/morphemes and the parts of
speech of hyper and hypo, which can be multi-
word/morpheme nouns.
Structure features provide evidence found in
layout or tree structures for hyponymy relations.
For example, hyponymy relations (TIGER, BEN-
GAL TIGER) and (TIGER,MALAYAN TIGER) can
be obtained from tree structure ?(root node, chil-
dren nodes of Subspecies)? in Fig 3.
3.2 Acquisition from Web Texts
As the target for hyponymy relation acquisition
from the Web, we used 5 ? 107 pages from
the TSUBAKI corpus (Shinzato et al, 2008),
a 108 page Japanese Web corpus that was de-
pendency parsed with KNP (Kurohashi-Nagao
Parser) (Kurohashi and Kawahara, 2005). Hy-
ponymy relation candidates are extracted from the
corpus based on the lexico-syntactic patterns such
as ?hypo nado hyper (hyper such as hypo)? and
?hypo to iu hyper (hyper called hypo)? (Ando
3MeCab (http://mecab.sourceforge.net/)
was used to provide the lexical features.
et al, 2004). We extracted 6 ? 106 Japanese
hyponymy relation candidates from the Japanese
Web texts. Features (WebFeature) used for classi-
fication are summarized in Table 1. Similar to the
hyponymy relation acquisition from Wikipedia,
lexical features are used to recognize the lexical
evidence for hyponymy relations.
Lexico-syntactic patterns for hyponymy rela-
tion show different coverage and accuracy in hy-
ponymy relation acquisition (Ando et al, 2004).
Further if multiple lexico-syntactic patterns sup-
port acquisition of hyponymy relation candidates,
these candidates are more likely to be actual hy-
ponymy relations. The pattern feature of hy-
ponymy relation candidates is used for these ev-
idence.
We use PMI (point-wise mutual information)
of hyponymy relation candidate (hyper, hypo) as
a collocation feature (Pantel and Ravichandran,
2004), where we assume that hyper and hypo in
candidates would frequently co-occur in the same
sentence if they hold a hyponymy relation.
Semantic noun classes have been regarded as
useful information in semantic relation acquisi-
tion (De Saeger et al, 2009). EM-based clus-
tering (Kazama and Torisawa, 2008) is used for
obtaining 500 semantic noun classes4 from 5 ?
105 nouns (including single-word and multi-word
ones) and their 4? 108 dependency relations with
5 ? 105 verbs and other nouns in our target Web
4Because EM clustering provides a probability distri-
bution over noun class nc, we obtain discrete classes of
each noun n with a probability threshold p(nc|n) ?
0.2 (De Saeger et al, 2009).
846
Co-training Bilingual co-training Co-STAR
(Blum and Mitchell, 1998) (Oh et al, 2009) (Proposed method)
Instance space Same Different Almost different
Feature space Split by human decision Split by languages Split by source texts
Common instances Genuine-common Genuine-common Genuine-common and
(or All unlabeled) instances instances (Translatable) virtual-common instances
Table 2: Differences among co-training, bilingual co-training, and Co-STAR
corpus. For example, noun class C311 includes
biological or chemical substances such as tatou
(polysaccharide) and yuukikagoubutsu (organic
compounds). Noun classes (i.e., C311) relevant to
hyper and hypo, respectively, are used as a noun
class feature.
4 Related Work
There are two frameworks, which are most rele-
vant to our work ? bilingual co-training and en-
semble semantics.
The main difference between bilingual co-
training and Co-STAR lies in an instance space.
In bilingual co-training, instances are in different
spaces divided by languages while, in Co-STAR,
many instances are in different spaces divided by
their source texts. Table 2 shows differences be-
tween co-training, bilingual co-training and Co-
STAR.
Ensemble semantics is a relation acquisition
framework, where semantic relation candidates
are extracted from multiple sources and a single
ranker ranks or classifies the candidates in the fi-
nal step (Pennacchiotti and Pantel, 2009). In en-
semble semantics, one ranker is in charge of rank-
ing all candidates extracted from multiple sources;
while one classifier classifies candidates extracted
from one source in Co-STAR.
5 Experiments
We used the July version of Japanese Wikipedia
(jawiki-20090701) as structured text. We ran-
domly selected 24,000 hyponymy relation candi-
dates from those identified in Wikipedia and man-
ually checked them. 20,000 of these samples were
used as training data for our initial classifier, the
rest was equally divided into development and test
data for Wikipedia. They are called ?WikiSet.?
As unstructured text, we used 5 ? 107 Japanese
Web pages in the TSUBAKI corpus (Shinzato et
al., 2008). Here, we manually checked 9,500
hyponymy relation candidates selected randomly
from Web texts. 7,500 of these were used as train-
ing data. The rest was split into development and
test data. We named this data ?WebSet?.
In both classifiers, the development data was
used to select the optimal parameters, and the test
data was used to evaluate our system. We used
TinySVM (TinySVM, 2002) with a polynomial
kernel of degree 2 as a classifier. ? (the threshold
value indicating high confidence), ? (the thresh-
old value indicating low confidence), and TopN
(the maximum number of training instances to be
added to the existing training data in each iter-
ation) were selected through experiments on the
development set. The combination of ? = 1,
? = 0.3, and TopN=900 showed the best perfor-
mance and was used in the following experiments.
Evaluation was done by precision (P ), recall (R),
and F-measure (F ).
5.1 Results
We compare six systems. Three of these, B1?B3,
show the effect of different feature sets (?Wik-
iFeature? and ?WebFeature? in Table 1) and dif-
ferent training data. We trained two separate clas-
sifiers in B1 and B2, while we integrated feature
sets and training data for training a single classi-
fier in B3. The classifiers in these three systems
are trained with manually prepared training data
(?WikiSet? and ?WebSet?). For the purpose of our
experiment, we consider B3 as the closest possible
approximation of the ensemble semantics frame-
work (Pennacchiotti and Pantel, 2009).
? B1 consists of two completely independent
classifiers. Both S and U classifiers are
trained and tested on their own feature and
data sets (respectively ?WikiSet + WikiFea-
ture? and ?WebSet + WebFeature?).
847
? B2 is the same as B1, except that both clas-
sifiers are trained with all available training
data ? WikiSet and WebSet are combined
(27,500 training instances in total). However,
each classifier only uses its own feature set
(WikiFeature or WebFeature)5.
? B3 adds a master classifier to B1. This third
classifier is trained on the complete 27,500
training instances (same as B2) using all
available features from Table 1, including
each instance?s SVM scores obtained from
the two B1 classifiers6. The verdict of the
master classifier is considered to be the final
classification result.
The other three systems, BICO, Co-B, and Co-
STAR (our proposed method), are for compari-
son between bilingual co-training (Oh et al, 2009)
(BICO) and variants of Co-STAR (Co-B and Co-
STAR). Especially, we prepared Co-B and Co-
STAR to show the effect of different configura-
tions of common instances on the Co-STAR al-
gorithm. We use both B1 and B2 as the initial
classifiers of Co-B and Co-STAR. We notate Co-
B and Co-STAR without ??? when B1 is used as
their initial classifier and those with ??? when B2
is used.
? BICO implements the bilingual co-training
algorithm of (Oh et al, 2009), in which
two processes collaboratively acquire hy-
ponymy relations in two different languages.
For BICO, we prepared 20,000 English and
20,000 Japanese training samples (Japanese
ones are the same as training data in the
WikiSet) by hand.
? Co-B is a variant of Co-STAR that uses only
the genuine-common instances as common
instances (67,000 instances)7, to demonstrate
5Note that training instances from WebSet (or WikiSet)
can have WikiFeature (or WebFeature) if they also appear
in Wikipedia (or Web corpus). But they can always have
lexical feature, the common feature set between WikiFeature
and WebFeature.
6SVM scores are assigned to the instances in training data
in a 10-fold cross validation manner.
7Co-B can be considered as conventional co-
training (Blum and Mitchell, 1998) in the sense that
two classifiers collaborate through actual common instances.
the effectiveness of the virtual common in-
stances.
? Co-STAR is our proposed method, which
uses both genuine-common and virtual-
common instances (643,000 instances in to-
tal).
WebSet WikiSet
P R F P R F
B1 84.3 65.2 73.5 87.8 74.7 80.7
B2 83.4 69.6 75.9 87.4 79.5 83.2
B3 82.2 72.0 76.8 86.1 77.7 81.7
BICO N/A N/A N/A 84.5 81.8 83.1
Co-B 86.2 63.5 73.2 89.7 74.1 81.2
Co-B? 85.5 69.9 77.0 89.6 76.5 82.5
Co-STAR 85.9 76.0 80.6 88.0 81.8 84.8
Co-STAR? 83.3 80.7 82.0 87.6 81.8 84.6
Table 3: Comparison of different systems
Table 3 summarizes the result. Features for
common instances in Co-B and Co-STAR are pre-
pared in the same way as training data in B2, so
that both classifiers can classify the common in-
stances with their trained feature sets.
Comparison between B1?B3 shows that B2 and
B3 outperform B1 in F-measure. More train-
ing data used in B2?B3 (27,500 instances for
both WebSet and WikiSet) results in higher per-
formance than that of B1 (7,500 and 20,000 in-
stances used separately). We think that the lexical
features, assigned regardless of source text to in-
stances in B2?B3, are mainly responsible for the
performance gain over B1, as they are the least
domain-dependent type of features. B2?B3 are
composed of different number of classifiers, each
of which is trained with different feature sets and
training instances. Despite this difference, B2 and
B3 showed similar performance in F-measure.
Co-STAR outperformed the algorithm similar
to the ensemble semantics framework (B3), al-
though we admit that a more extensive com-
parison is desirable. Further Co-STAR outper-
formed BICO. While the manual cost for build-
ing the initial training data used in Co-STAR
and BICO is hard to quantify, Co-STAR achieves
better performance with fewer training data in
total (27,500 instances) than BICO (40,000 in-
stances). The difference in performance between
Co-B and Co-STAR shows the effectiveness of
848
the automatically generated virtual-common in-
stances. From these comparison, we can see that
virtual-common instances coupled with genuine-
common instances can be leveraged to enable
more effective collaboration between the two clas-
sifiers in Co-STAR.
As a result, our proposed method outperforms
the others in F-measure by 1.4?8.5%. We ob-
tained 4.3 ? 105 hyponymy relations from Web
texts and 4.6? 106 ones from Wikipedia8.
6 Co-STAR with Automatically
Generated Training Data
For Co-STAR, we need two sets of manually pre-
pared training data, one for structured text and the
other for unstructured text. As in any other su-
pervised system, the cost of preparing the training
data is an important issue. We therefore investi-
gated whether Co-STAR can be trained for a lower
cost by generating more of its training data auto-
matically.
We automatically built training data for Web
texts by using definition sentences9 and category
names in the Wikipedia articles, while we stuck to
manually prepared training data for Wikipedia. To
obtain hypernyms from Wikipedia article names,
we used definition-specific lexico-syntactic pat-
terns such as ?hyponym is hypernym? and ?hy-
ponym is a type of hypernym? (Kazama and Tori-
sawa, 2007; Sumida and Torisawa, 2008). Then,
we extracted hyponymy relations consisting of
pairs of Wikipedia category names and their mem-
ber articles when the Wikipedia category name
and the hypernym obtained from the definition
of the Wikipedia article shared the same head
word. Next, we selected a subset of the extracted
hyponymy relations that are also hyponymy re-
lation candidates in Web texts, as positive in-
stances for hyponymy relation acquisition from
Web text. We obtained around 15,000 positive in-
stances in this way. Negative instances were cho-
sen from virtual-common instances, which also
originated from the Wikipedia category system
and hyponymy relation candidates in Web texts
8We obtained them with 90% precision by setting the
SVM score threshold to 0.23 for Web texts and 0.1 for
Wikipedia.
9The first sentences of Wikipedia articles.
(around 293,000 instances).
The automatically built training data was noisy
and its size was much bigger than manually pre-
pared training data in WebSet. Thus 7,500 in-
stances as training data (the same number of man-
ually built training data in WebSet) were ran-
domly chosen from the positive and negative in-
stances with a positive:negative ratio of 1:410.
WebSet WikiSet
P R F P R F
B1 81.0 47.6 60.0 87.8 74.7 80.7
B2 80.0 55.4 65.5 87.1 79.5 83.1
B3 82.0 33.7 47.8 87.1 75.6 81.0
Co-STAR 82.2 60.8 69.9 87.3 80.7 83.8
Co-STAR? 79.2 69.6 74.1 87.0 81.8 84.4
Table 4: Results with automatically generated
training data
With the automatically built training data for
Web texts and manually prepared training data for
Wikipedia, we evaluated B1?B3 and Co-STAR,
which are the same systems in Table 3. The results
in Table 4 are encouraging. Co-STAR was robust
even when faced with noisy training data. Further
Co-STAR showed better performance than B1?
B3, although its performance in Table 4 dropped a
bit compared to Table 3. This result shows that we
can reduce the cost of manually preparing training
data for Co-STAR with only small loss of the per-
formance.
7 Conclusion
This paper proposed Co-STAR, an algorithm for
hyponymy relation acquisition from structured
and unstructured text. In Co-STAR, two indepen-
dent processes of hyponymy relation acquisition
from structured texts and unstructured texts, col-
laborate in an iterative manner through common
instances. To improve this collaboration, we in-
troduced virtual-common instances.
Through a series of experiments, we showed
that Co-STAR outperforms baseline systems and
virtual-common instances can be leveraged to
achieve better performance. We also showed that
Co-STAR is robust against noisy training data,
which requires less human effort to prepare it.
10We select the ratio by testing different ratio from 1:2 to
1:5 with our development data in WebSet and B1.
849
References
Ando, Maya, Satoshi Sekine, and Shun Ishiza. 2004.
Automatic extraction of hyponyms from Japanese
newspaper using lexico-syntactic patterns. In Proc.
of LREC ?04.
Blum, Avrim and Tom Mitchell. 1998. Combin-
ing labeled and unlabeled data with co-training. In
COLT? 98: Proceedings of the eleventh annual con-
ference on Computational learning theory, pages
92?100.
De Saeger, Stijn, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large
scale relation acquisition using class dependent pat-
terns. In Proc. of ICDM 2009, pages 764?769.
Hearst, Marti A. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics,
pages 539?545.
Kazama, Jun?ichi and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proc. of Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing, pages 698?707.
Kazama, Jun?ichi and Kentaro Torisawa. 2008. In-
ducing gazetteers for named entity recognition by
large-scale clustering of dependency relations. In
Proceedings of ACL-08: HLT, pages 407?415.
Kurohashi, Sadao and Daisuke Kawahara. 2005. KNP
(Kurohashi-Nagao Parser) 2.0 users manual.
Oh, Jong-Hoon, Kiyotaka Uchimoto, and Kentaro
Torisawa. 2009. Bilingual co-training for mono-
lingual hyponymy-relation acquisition. In Proc. of
ACL-09: IJCNLP, pages 432?440.
Pantel, Patrick and Deepak Ravichandran. 2004. Au-
tomatically labeling semantic classes. In Proc. of
HLT-NAACL ?04, pages 321?328.
Pantel, Patrick, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP ?09, pages 938?947.
Pennacchiotti, Marco and Patrick Pantel. 2009. En-
tity extraction via ensemble semantics. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 238?247.
Ravi, Sujith and Marius Pasca. 2008. Using structured
text for large-scale attribute extraction. In CIKM-
08, pages 1183?1192.
Shinzato, Keiji and Kentaro Torisawa. 2004. Ex-
tracting hyponyms of prespecified hypernyms from
itemizations and headings in web documents. In
Proceedings of COLING ?04, pages 938?944.
Shinzato, Keiji, Tomohide Shibata, Daisuke Kawa-
hara, Chikara Hashimoto, and Sadao Kurohashi.
2008. Tsubaki: An open search engine infrastruc-
ture for developing new information access. In Pro-
ceedings of IJCNLP ?08, pages 189?196.
Snow, Rion, Daniel Jurafsky, and Andrew Y. Ng.
2006. Semantic taxonomy induction from heteroge-
nous evidence. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 801?808.
Suchanek, Fabian M., Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In Proc. of WWW ?07, pages 697?706.
Sumida, Asuka and Kentaro Torisawa. 2008. Hack-
ing Wikipedia for hyponymy relation acquisition.
In Proc. of the Third International Joint Conference
on Natural Language Processing (IJCNLP), pages
883?888, January.
Talukdar, Partha Pratim, Joseph Reisinger, Marius
Pasca, Deepak Ravichandran, Rahul Bhagat, and
Fernando Pereira. 2008. Weakly-supervised acqui-
sition of labeled class instances using graph random
walks. In Proc. of EMNLP08, pages 582?590.
TinySVM. 2002. http://chasen.org/?taku/
software/TinySVM.
Van Durme, Benjamin and Marius Pasca. 2008. Find-
ing cars, goddesses and enzymes: Parametrizable
acquisition of labeled instances for open-domain in-
formation extraction. In Proc. of AAAI08, pages
1243?1248.
Vapnik, Vladimir N. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Wang, Wei and Zhi-Hua Zhou. 2007. Analyzing co-
training style algorithms. In ECML ?07: Proceed-
ings of the 18th European conference on Machine
Learning, pages 454?465.
850
