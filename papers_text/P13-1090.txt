Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 914?923,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Semi-Supervised Semantic Tagging of Conversational Understanding
using Markov Topic Regression
Asli Celikyilmaz
Microsoft
Mountain View, CA, USA
asli@ieee.org
Dilek Hakkani-Tur, Gokhan Tur
Microsoft Research
Mountain View, CA, USA
dilek@ieee.org
gokhan.tur@ieee.org
Ruhi Sarikaya
Microsoft
Redmond, WA, USA
rusarika@microsoft.com
Abstract
Finding concepts in natural language ut-
terances is a challenging task, especially
given the scarcity of labeled data for learn-
ing semantic ambiguity. Furthermore,
data mismatch issues, which arise when
the expected test (target) data does not
exactly match the training data, aggra-
vate this scarcity problem. To deal with
these issues, we describe an efficient semi-
supervised learning (SSL) approach which
has two components: (i) Markov Topic
Regression is a new probabilistic model
to cluster words into semantic tags (con-
cepts). It can efficiently handle seman-
tic ambiguity by extending standard topic
models with two new features. First, it en-
codes word n-gram features from labeled
source and unlabeled target data. Sec-
ond, by going beyond a bag-of-words ap-
proach, it takes into account the inherent
sequential nature of utterances to learn se-
mantic classes based on context. (ii) Ret-
rospective Learner is a new learning tech-
nique that adapts to the unlabeled target
data. Our new SSL approach improves
semantic tagging performance by 3% ab-
solute over the baseline models, and also
compares favorably on semi-supervised
syntactic tagging.
1 Introduction
Semantic tagging is used in natural language un-
derstanding (NLU) to recognize words of seman-
tic importance in an utterance, such as entities.
Typically, a semantic tagging model require large
amount of domain specific data to achieve good
performance (Tur and DeMori, 2011). This re-
quires a tedious and time intensive data collection
and labeling process. In the absence of large la-
beled training data, the tagging model can behave
poorly on test data (target domain). This is usually
caused by data mismatch issues and lack of cover-
age that arise when the target data does not match
the training data.
To deal with these issues, we present a new
semi-supervised learning (SSL) approach, which
mainly has two components. It initially starts with
training supervised Conditional Random Fields
(CRF) (Lafferty et al, 2001) on the source train-
ing data which has been semantically tagged. Us-
ing the trained model, it decodes unlabeled dataset
from the target domain. With the data mismatch
issues in mind, to correct errors that the supervised
model make on the target data, the SSL model
leverages the additional information by way of a
new clustering method. Our first contribution is a
new probabilistic topic model, Markov Topic Re-
gression (MTR), which uses rich features to cap-
ture the degree of association between words and
semantic tags. First, it encodes the n-gram context
features from the labeled source data and the unla-
beled target data as prior information to learn se-
mantic classes based on context. Thus, each latent
semantic class corresponds to one of the seman-
tic tags found in labeled data. MTR is not invari-
ant to reshuffling of words due to its Markovian
property; hence, word-topic assignments are also
affected by the topics of the surrounding words.
Because of these properties, MTR is less sensitive
to the errors caused by the semantic ambiguities.
Our SSL uses MTR to smooth the semantic tag pos-
teriors on the unlabeled target data (decoded using
the CRF model) and later obtains the best tag se-
quences. Using the labeled source and automati-
914
cally labeled target data, it re-trains a new CRF-
model.
Although our iterative SSL learning model can
deal with the training and test data mismatch, it
neglects the performance effects caused by adapt-
ing the source domain to the target domain. In
fact, most SSL methods used for adaptation, e.g.,
(Zhu, 2005), (Daume?-III, 2010), (Subramanya et
al., 2010), etc., do not emphasize this issue. With
this in mind, we introduce a new iterative training
algorithm, Retrospective Learning, as our second
contribution. While retrospective learning itera-
tively trains CRF models with the automatically
annotated target data (explained above), it keeps
track of the errors of the previous iterations so as
to carry the properties of both the source and target
domains.
In short, through a series of experiments we
show how MTR clustering provides additional in-
formation to SSL on the target domain utter-
ances, and greatly impacts semantic tagging per-
formance. Specifically, we analyze MTR?s perfor-
mance on two different types of semantic tags:
named-entities and descriptive tags as shown in
Table 1. Our experiments show that it is much
harder to detect descriptive tags compared to
named-entities.
Our SSL approach uses probabilistic clustering
method tailored for tagging natural language utter-
ances. To the best of our knowledge, our work is
the first to explore the unlabeled data to iteratively
adapt the semantic tagging models for target do-
mains, preserving information from the previous
iterations. With the hope of spurring related work
in domains such as entity detection, syntactic tag-
ging, etc., we extend the earlier work on SSL part-
of-speech (POS) tagging and show in the experi-
ments that our approach is not only useful for se-
mantic tagging but also syntactic tagging.
The remainder of this paper is divided as fol-
lows: ?2 gives background on SSL and semantic
clustering methods, ?3 describes our new cluster-
ing approach, ?4 presents the new iterative learn-
ing, ?5 presents our experimental results and ?6
concludes our paper.
2 Related Work and Motivation
(I) Semi-Supervised Tagging. Supervised meth-
ods for semantic tagging in NLU require a large
number of in-domain human-labeled utterances
and gazetteers (movie, actor names, etc.), increas-
? Are there any [comedies] with [Ryan Gosling]?
? How about [oscar winning] movies by
[James Cameron]?
? Find [Woody Allen] movies similar to [Manhattan].
[Named Entities]
director: James Cameron, Woody Allen,...
actor: Ryan Gosling, Woody Allen,...
title: Manhattan, Midnight in Paris,...
[Descriptive Tags]
restriction: similar, suitable, free,rate,...
description: oscar winning, new release, gardening,...
genre: spooky, comedies, feel good, romance,...
Table 1: Samples of semantically tagged utter-
ances from movie domain, named-entities and de-
scriptive tags.
ing the need for significant manual labor (Tur and
DeMori, 2011). Recent work on similar tasks
overcome these challenges using SSL methods as
follows:
? (Wang et al, 2009; Li et al, 2009; Li,
2010; Liu et al, 2011) investigate web query
tagging using semi-supervised sequence models.
They extract semantic lexicons from unlabeled
web queries, to use as features. Our work dif-
fers from these, in that, rather than just detecting
named-entities, our utterances include descriptive
tags (see Table 1).
? Typically the source domain has different dis-
tribution than the target domain, due to topic shifts
in time, newly introduced features (e.g., until re-
cently online articles did not include facebook
?like? feature.), etc. Adapting the source domain
using unlabeled data is the key to achieving good
performance across domains. Recent adaptation
methods for SSL use: expectation minimization
(Daume?-III, 2010) graph-based learning (Chapelle
et al, 2006; Zhu, 2005), etc. In (Subramanya et
al., 2010) an efficient iterative SSL method is de-
scribed for syntactic tagging, using graph-based
learning to smooth POS tag posteriors. However,
(Reisinger and Mooney, 2011) argues that vector
space models, such as graph-learning, may fail to
capture the richness of word meaning, as simi-
larity is not a globally consistent metric. Rather
than graph-learning, we present a new SSL using
a probabilistic model, MTR, to cluster words based
on co-occurrence statistics.
?Most iterative SSL methods, do not keep track
of the errors made, nor consider the divergence
from the original model. (Lavoie et al, 2011) ar-
gues that iterative learning models should mitigate
new errors made by the model at each iteration by
915
keeping the history of the prior predictions. This
ensures that a penalty is paid for diverging from
the previous model?s predictions, which will be
traded off against the benefit of reducing classi-
fication loss. We present a retrospective SSL for
CRF, in that, the iterative learner keeps track of the
errors of the previous iterations so as to carry the
properties of both the source and target domains.
(II) Semantic Clustering. A common prop-
erty of several context-based word clustering tech-
niques, e.g., Brown clustering (Brown et al,
1992), Clustering by Committee (Pantel, 2003),
etc., is that they mainly cluster based on local con-
text such as nearby words. Standard topic models,
such as Latent Dirichlet Allocation (LDA) (Blei
et al, 2003), use a bag-of-words approach, which
disregards word order and clusters words together
that appear in a similar global context. Such mod-
els have been effective in discovering lexicons in
many NLP tasks, e.g., named-entity recognition
(Guo et al, 2009), word-sense disambiguation
(Boyd-Graber et al, 2007; Li et al, 2010), syntac-
tic/semantic parsing (Griffiths et al, 2005; Singh
et al, 2010), speaker identification (Nyugen et al,
2012), etc. Recent topic models consider word
sequence information in documents (Griffiths et
al., 2005; Moon et al, 2010). The Hidden Topic
Markov Model (HTMM) by (Gruber et al, 2005),
for instance, models sentences in documents as
Markov chains, assuming all words in a sentence
have the same topic. While MTR has a similar
Markovian property, we encode features on words
to allow each word in an utterance to sample from
any of the given semantic tags, as in ?what are
[scary]genre movies by [Hitchcock]director??.
In LDA, common words tend to dominate all
topics causing related words to end up in differ-
ent topics. In (Petterson et al, 2010), the vector-
based features of words are used as prior informa-
tion in LDA so that the words that are synonyms
end up in same topic. Thus, we build a seman-
tically rich topic model, MTR, using word context
features as side information. Using a smoothing
prior for each word-topic pair (instead of a con-
stant ? smoother), MTR assures that the words are
distributed over topics based on how similar they
are. (e.g., ?scary? and ?spooky?, which have sim-
ilar context features, go into the same semantic
tag, ?genre?). Thus, to best of our knowledge,
MTR is the first topic model to incorporate word
features while considering the sequence of words.
3 Markov Topic Regression - MTR
3.1 Model and Abstractions
LDA assumes that the latent topics of documents
are sampled independently from one of K topics.
MTR breaks down this independence assumption
by allowing Markov relations between the hidden
tags to capture the relations between consecutive
words (as sketched in Figure 1 and Algorithm 1).
(I) Semantic Tags (si): Each word wi of a
given utterance with Nj words, uj={wi}Nji=1?U ,
j=1,..|U |, from a set of utterances U , is associated
with a latent semantic tag (state) variable si?S,
where S is the set of semantic tags. We assume a
fixed K topics corresponding to semantic tags of
labeled data. In a similar way to HTMM (Gruber
et al, 2005) described for documents, MTR sam-
ples each si from a Markov chain that is specific
to its utterance uj . Each state si generates a word,
wi, based on the word-state co-occurrences. MTR
allows for sampling of consecutive words from
different tag clusters. The initial probabilities of
the latent states are sampled from a Dirichlet dis-
tribution over state variables, ?j , with ? hyper-
parameter for each uj .
(II) Tag Transition Indicator (?v): Given ut-
terance uj , the decision to sample a wi from a
new topic is determined by an indicator variable,
cj,i, that is sampled from a Binomial(?v=wi) dis-
tribution with a Beta conjugate prior. (There are v
binomials for each vocabulary term.) cj,i=1 sug-
gests that a new state be sampled from K possible
tags for the word wi in uj , and cj,i=0 suggests that
the state si of wi should be the same as the previ-
ous word?s latent state si?1. The first position of
the sequence is sampled from a new state, hence
cj,i=1=1.
(III) Tag Transition Base Measure (?): Prior
probability of a word given a tag should increase
the chances of sampling words from the correct se-
mantic tag. MTR constrains the generation of a tag
si given the previous tag si?1 and the current wi
based on cj,i by using a vocabulary specific Beta
prior, ?v?Beta(?v) 1, on each word in vocabulary
wv=1,..V . We inject the prior information on se-
mantic tags to define values of the base measure
?v using external knowledge from two sources:
(a) Entity Priors (?S): Prior probability on
named-entities and descriptive tags denoted as
1For each beta distribution we use symmetric
Beta(?v)=Beta(?=?v ,?=?v).
916
latent 
semantic tag
distribution over 
semantic tags
s
1
...
w
1
...
!
j
"
 c
2
 c
3
#
$
kv
%
kv
x
v
&
k
s
2
s
3
w
2
w
3
w
n
'
V
K
|U|
V
indicator for 
sampling 
semantic tags
vocabulary 
features
as prior
information
semantic tag 
dependent 
smoothing coefficient
semantic tag 
indicator 
parameter
prior on 
per-word 
state 
transitions
$
k
 ! Dir(%
kv
|x;&
k
)
!
k
 = exp(f(x;&
k
))
semantic tag 
distribution 
over tags
smoother for 
tag-word
pair
 c
N
j
s
N
j
Figure 1: The graph representation of the Markov
Topic Regression (MTR). To demonstrate hidden
state Markov Chain, the generation of each word
is explicitly shown (inside of the plate).
?S=p(si|si?1,wi=v,wi?1). We use web sources
(wiki pages on movies and urls such as imdb.com)
and labeled training data to extract entity lists that
correspond to the semantic tags of our domains.
We keep the frequency of each n-gram to convert
into (empirical) prior probability distribution.
(b) Language Model Prior (?W ): Probabilities
on word transitions denoted as ?W=p(wi=v|wi?1).
We built a language model using SRILM (Stol-
cke, 2002) on the domain specific sources such as
top wiki pages and blogs on online movie reviews,
etc., to obtain the probabilities of domain-specific
n-grams, up to 3-grams. The observed priors, ?S
and ?W , are used for calculating the base measure
? for each vocabulary wv as:
?si|si?1v =
{
?si|si?1,wi=vS , if ?si|si?1,wi=vS exists,
?wi=v,wi?1W , otherwise (1)
In Eq.(1), we assume that the prior on the se-
mantic tags, ?S , is more indicative of the deci-
sion for sampling a wi from a new tag compared
to language model posteriors on word sequences,
?W . Here we represent the base-measure (hyper-
parameter) of the semantic tag indicator variable,
which is not to be confused with a probability
measure 2
We update the indicator parameter via mean cri-
teria, ?v=wi=
?K
i,j=1?
si|sj
v=wi /(K2). If no prior on
2The base-measure used in Eq.(1) does not relate to a
back-off model in LM sense. Here, instead of using a
constant value for the hyper-parameters, we use probability
scores that we obtain from LM.
Algorithm 1 Markov Topic Regression
1: for each semantic tag topic sk, k ? 1, ...,K do
2: ? draw a topic mixture ?k ? Dir(?k|?k,x),
3: ? let ?k=exp(f(x;?k)); x={xv}Vlv=1, ?k? RVl4: for each word wv in vocabulary v ? 1, ..., V do
5: ? draw a tag indicator mixture ?v ? Beta(?),
6: for each utterance j ? 1, ..., |U | do
7: ?draw transition distribution ?sj ? Dir(?)
8: over states si and set cj1=1.
9: ?for words wi in uj , i? 1, ..., Nj do
10:  if i >1, toss a coin cj,i ? Binomial(?wi).
11:  If cj,i=1, draw si?Multi(?si,si?1j )?12: otherwise si=si?1.
13:  Sample wi?Multi(?si ).
? Markov assumption over utterance words is used (See Eq.(4)).
a specific word exists, a default value is used for
base measure, ?v=0.01.
(IV) Topic-Word Distribution Priors (?k):
Different from (Mimno et al, 2008), which uses
asymmetric hyper-parameters on document-topic
distributions, in MTR, we learn the asymmetric
hyper-parameters of the semantic tag-word distri-
butions. We use blocked Gibbs sampling, in which
the topic assignments sk and hyper-parameters
{?k}Kk=1 are alternately sampled at each Gibbs
sampling lag period g given all other variables. We
impose the prior knowledge on naturally related
words, such that if two words ?funny? and ?hilar-
ious? indicate the same given ?genre? class, then
their latent tag distributions should also be simi-
lar. We enforce this on smoothing parameter ?k,v,
e.g., ?k,?funny???k,?hilarious? for a given tag k as
follows:
At each g lag period of the Gibbs sampling, K
log-linear models with parameters, ?(g)k ?RM , is
trained to predict ?(g)kv ??k, for each wv of a tag
sk:
?(g)k = exp(f(xl;?
(g)
k )) (2)
where the log-linear function f is:
n(g)kv = f(xlv;?
(g)
k ) =
?
m
?(g)k,mxlv,m (3)
Here x?RV?M is the input matrix x, wherein
rows xv?RM represents M -dimensional scalar
vector of explanatory features on vocabulary
words. We use the word-tag posterior probabili-
ties obtained from a CRF sequence model trained
on labeled utterances as features. The x={xl,xu}
has labeled (l) and unlabeled (u) parts. The labeled
part contains Vl size vocabulary of which we know
the semantic tags, xl={(xl1,s1),...,(xlVl ,sVl)}. Atthe start of the Gibbs sampling, we designate the
917
K latent topics to the K semantic tags of our la-
beled data. Therefore, we assign labeled words to
their designated topics. This way we use observed
scalar counts of each labeled word v associated
with its semantic tag k, n(g)kv , as the output labelof its input vector, xlv; an indication of likelihood
of words getting sampled from the correspond-
ing semantic label sk. Since the impact of the
asymmetric prior is equivalent to adding pseudo-
counts to the sufficient statistics of the semantic
tag to which the word belongs, we predict the
pseudo-counts ?(g)kv using the scalar counts of the
labeled data, n(g)kv , based on the log-linear model
in Eq. (2). At g=0, we use ?(0)kv =28, if xv?X l; oth-
erwise ?(0)kv =2?2, commonly used values for largeand small ?. Note that larger ?-values indicate
correlation between the word and the topic.
3.2 Collapsed Sampler
The goal of MTR is to infer the degree of relation-
ship between a word v and each semantic tag k,
?kv. To perform inference we need two compo-
nents:
? a sampler which can draw from condi-
tional PMTR(sji=k|sji?1, s\ji, ?, ?i, ?ji), when
cj,i=1, where sji and sji?1 are the semantic
tags of the current wi=v of vocabulary v and
previous word wi?1 in utterance uj , and s\ji
are the semantic tag topics of all words except
for wi; and,
? an estimation procedure for (?kv, ?k) (see
?3.1).
We integrate out the multinomial and binomial pa-
rameters of the model: utterance-tag distributions
?j , binomial state transition indicator distribution
per each word ?v, and ?k for tag-word distribu-
tions. We use collapsed Gibbs sampling to re-
duce random components and model the posterior
distribution by obtaining samples (sji, cj,i) drawn
from this distribution. Under the Markov assump-
tion, for each word wi=v in a given utterance uj ,
if cj,i=1, we sample a new tag si=k given the
remaining tags and hyper-parameters ?k, ?, and
?si|si?1wi=v . Using the following parameters; n(si)ji ,
which is the number of words assigned to a seman-
tic class si=k excluding case i, and n(si?1)si is the
number of transitions from class si?1 to si, where
indicator I(si?1, si)=1 if slot si=si?1, the update
equation is formulated as follows:
p(sji = k|w, s?ji, ?, ?si|si?1wi ,?k) ?
n(si)ji + ?kwi
n(k)(.) +
?
v ?kv
? (n(si?1)si + ?)?
(n(si)si+1 + I(si?1, si) + I(si+1, si) + ?)
n(si)(.) + I(si?1, k) +K?
(4)
4 Semi-Supervised Semantic Labeling
4.1 Semi Supervised Learning (SSL) with
CRF
In (Subramanya et al, 2010), a new SSL method
is described for adapting syntactic POS tagging of
sentences in newswire articles along with search
queries to a target domain of natural language
(NL) questions. They decode unlabeled queries
from target domain (t) using a CRF model trained
on the POS-labeled newswire data (source do-
main (o)). The unlabeled POS tag posteriors are
then smoothed using a graph-based learning algo-
rithm. On graph, the similarities are defined over
sequences by constructing the graph over types,
word 3-grams, where types capture the local con-
text of words. Since CRF tagger only uses lo-
cal features of the input to score tag pairs, they
try to capture all the context with the graph with
additional context features on types. Later, using
viterbi decoding, they select the 1-best POS tag
sequence, s?j for each utterance uj . Graph-based
SSL defines a new CRF objective function:
?(t)n+1 =argmin
??RK{
??
j=1:l
log p(sj |uj ; ?(t)n ) + ???(t)n ?2
}
?
{
?
?l+u
j=l log pn(s?j |uj ; ?
(t)
n )
}
(5)
The first bracket in Eq.(5) is the loss on the la-
beled data and L2 regularization on parameters,
?(t)n , from nth iteration, same as standard CRF.
The last term is the loss on unlabeled data from
target domain with a hyper-parameter ? . They use
a small value for ? to enable the new model to be
as close as possible to the initial model trained on
source data.
4.2 Retrospective Semi-Supervised CRF
We describe a Retrospective SSL (R-SSL) train-
ing with CRF (Algorithm 2), using MTR as a
918
smoothing model, instead of a graph-based model,
as follows:
I. DECODING and SMOOTHING. The poste-
rior probability of a tag sji=k given a word wji
in unlabeled utterance uj from target domain (t)
p?n(j, i)=p?n(sji=k|wji; ?(t)n ), is decoded using the
n-th iteration CRF model. MTR uses the decoded
probabilities as semantic tag prior features on vo-
cabulary items. We generate a word-tag matrix of
posteriors, x?(0, 1)V?K , where K is the number
of semantic tags and V is the vocabulary size from
n-th iteration. Each row is aK dimensional vector
of tag posterior probabilities xv={xv1,. . . xvK} on
the vocabulary term, wv. The labeled rows xl of
the vocabulary matrix, x={xl,xu}, contain only
{0,1} values, indicating the word?s observed se-
mantic tags in the labeled data. Since a labeled
term wv can have different tags (e.g., ?clint east-
wood? may be tagged as actor-name and director-
name in the training data), ?Kk xvk?1 holds. The
x is used as the input matrix of the kth log-linear
model (corresponding to kth semantic tag (topic))
to infer the ? hyper-parameter of MTR in Eq. (2).
MTR generates smoothed conditional probabilities
?kv for each vocabulary term v given semantic tag
k.
II. INTERPOLATION. For each word wji=v
in unlabeled utterance uj , we interpolate tag
marginals from CRF and MTR for each semantic
tag sji = k:
q?n(sji|wij ; ?(t)n ) = pi
CRF posterior? ?? ?
p?n(sji|wij ; ?(t)n )
+(1? pi)
MTR????
?kv (6)
III. VITERBI. Using viterbi decoding over
the tag marginals, q?n(sji|wij ; ?(t)n ), and transition
probabilities obtained from the CRF model of n-th
iteration, we get p?n(s?j |uj ; ?(t)n ), the 1-best decode
s?j of each unlabeled utterance uj?Uun .
IV. RETROSPECTIVE SSL (R-SSL). After
we decode the unlabeled data, we re-train a new
CRF model at each iteration. Each iteration makes
predictions on the semantic tags of unlabeled data
with varying posterior probabilities. Motivated by
(Lavoie et al, 2011), we want the loss function to
have a dependency on the prior model predictions.
Thus, R-SSL encodes the history of the prior pre-
Algorithm 2 Retrospective Semi-Supervised CRF
Input: Labeled U l, and unlabeled Uu data.
Process: ?(o)n =crf-train(Ul) at n=0, n=n+1 ?.
While not converged
p?=posterior-decode(Uun ,?(o)n )
?=smooth-posteriors(p?) using MTR,
q?=interpolate-posteriors(p?,?),
Uun=viterbi-decode(q?)
?(t)n+1=crf-retrospective(U l, Uun ,. . . ,Uu1 ,?(t)n )
? (n):iteration, (t):target, (o):source domains.
dictions, as follows:
?(t)n+1 =argmin
??RK{
??
j=1:l
log p(sj |uj ; ?(t)n ) + ???(t)n ?2
}
{
??
j=1:(l+u)
max{0, p???n }
}
(7)
where, p???n =1 ? log hn(uj)p?n(s?j |uj ; ?(t)n ). The
first two terms are same as standard CRF. The
last term ensures that the predictions of the cur-
rent model have the same sign as the predic-
tions of the previous models (using labeled and
unlabeled data), denoted by a maximum margin
hinge weight, hn(uj)= 1n?1
?n?1
1 p?n(s?j |uj ; ?
(t)
n ).
It should also be noted that with MTR, the R-SSL
learns the word-tag relations by using features that
describe the words in context, eliminating the need
for additional type representation of graph-based
model. MTR provides a separate probability dis-
tribution ?j over tags for each utterance j, implic-
itly allowing for the same word v in separate utter-
ances to differ in tag posteriors ?kv.
5 Experiments
5.1 Datasets and Tagsets
5.1.1 Semantic Tagging Datasets
We focus here on audiovisual media in the movie
domain. The user is expected to interact by voice
with a system than can perform a variety of tasks
such as browsing, searching, querying informa-
tion, etc. To build initial NLU models for such
a dialog system, we used crowd-sourcing to col-
lect and annotate utterances, which we consider
our source domain. Given movie domain-specific
tasks, we asked the crowd about how they would
919
interact with the media system as if they were talk-
ing to a person.
Our data from target domain is internally col-
lected from real-use scenarios of our spoken dia-
log system. The transcribed text forms of these ut-
terances are obtained from speech recognition en-
gine. Although the crowd-sourced data is similar
to target domain, in terms of pre-defined user in-
tentions, the target domain contains more descrip-
tive vocabulary, which is almost twice as large as
the source domain. This causes data-mismatch is-
sues and hence provides a perfect test-bed for a
domain adaptation task. In total, our corpus has
a 40K semantically tagged utterances from each
source and target domains. There are around 15
named-entity and 10 descriptive tags. We sep-
arated 5K utterances to test the performance of
the semantic tagging models. The most frequent
entities are: movie-director (?James Cameron?),
movie-title (?Die Hard?), etc.; whereas top de-
scriptive tags are: genre (?feel good?), description
(?black and white?, ?pg 13?), review-rate (?epic?,
?not for me?), theater-location (?near me?,?city
center?), etc.
Unlabeled utterances similar to the movie do-
main are pulled from a month old web query logs
and extracted over 2 million search queries from
well-known sites, e.g., IMDB, Netflix, etc. We
filtered queries that are similar to our target set
that start with wh-phrases (?what?, ?who?, etc.) as
well as imperatives ?show?, ?list?, etc. In addition,
we extracted web n-grams and entity lists (see ?3)
from movie related web sites, and online blogs and
reviews. We collected around 300K movie review
and blog entries on the entities observed in our
data. We extract prior distributions for entities and
n-grams to calculate entity list ? and word-tag ?
priors (see ?3.1).
5.1.2 Syntactic Tagging Datasets
We use the Wall Street Journal (WSJ) section of
the Penn Treebank as our labeled source data. Fol-
lowing previous research, we train on sections 00-
18, comprised of 38,219 POS-tagged sentences.
To evaluate the domain adaptation (DA) approach
and to compare with results reported by (Subra-
manya et al, 2010), we use the first and second
half of QuestionBank (Judge et al, 2006) as our
development and test sets (target). The Question-
Bank contains 4000 POS-tagged questions, how-
ever it is difficult to tag with WSJ-trained tag-
gers because the word order is different than WSJ
and contains a test-set vocabulary that is twice
as large as the one in the development set. As
for unlabeled data we crawled the web and col-
lected around 100,000 questions that are similar
in style and length to the ones in QuestionBank,
e.g. ?wh? questions. There are 36 different tag
sets in the Penn dataset which includes tag la-
bels for verbs, nouns, adjectives, adverbs, modal,
determiners, prepositions, etc. More information
about the Penn Tree-bank tag set can be found here
(Marcus et al, 1993).
5.2 Models
We evaluated several baseline models on two
tasks:
5.2.1 Semantic Clustering
Since MTR provides a mixture of properties
adapted from earlier models, we present perfor-
mance benchmarks on tag clustering using: (i)
LDA; (ii) Hidden Markov Topic Model HMTM
(Gruber et al, 2005); and, (iii) w-LDA (Petterson
et al, 2010) that uses word features as priors in
LDA. When a uniform ? hyper-parameter is used
with no external information on the state transi-
tions in MTR, it reduces to a HMTM model. Sim-
ilarly, if no Markov properties are used (bag-of-
words), MTR reduces to w-LDA. Each topic model
uses Gibbs sampling for inference and parameter
learning. We sample models for 1000 iterations,
with a 500-iteration burn-in and a sampling lag of
10. For testing we iterated the Gibbs sampler us-
ing the trained model for 10 iterations on the test-
ing data.
5.2.2 SSL for Semantic/Syntactic Tagging
We evaluated three different baselines against our
SSL models:
? CRF: a standard supervised sequence tagging.
? Self-CRF: a wrapper method for SSL using
self-training. First a supervised learning algorithm
is used to build a CRF model based on the labeled
data. A CRF model is used to decode the unla-
beled data to generate more labeled examples for
re-training.
? SSL-Graph: A SSL model presented in (Sub-
ramanya et al, 2010) that uses graph-based learn-
ing as posterior tag smoother for CRF model using
Eq.(5).
In addition to the three baseline, we evaluated
three variations of our SSL method:
? SSL-MTR: Our first version of SSL uses MTR to
920
LDA w-LDA HMTM MTR
0.6
0.7
0.8
0.9
82%
77%
84%
82%
79%78%
74% ? Descriptive Tags Named-Entities
 All Tags
F-M
eas
ure
Figure 2: F-measure for semantic clustering per-
formance. Performance differences for three dif-
ferent baseline models and our MTR approach by
different semantic tags.
smooth the semantic tag posteriors of a unlabeled
data decoded by the CRF model using Eq.(5).
? R-SSL-Graph: Our second version uses
graph-learning to smooth the tag posteriors and re-
train a new CRF model using retrospective SSL in
Eq.(7).
? R-SSL-MTR: Our full model uses MTR as a
Bayesian smoothing model, and retrospective SSL
in Eq.(7) for iterative CRF training.
For all the CRF models, we use lexical fea-
tures consisting of unigrams in a five-word win-
dow around the current word. To include contex-
tual information, we add binary features for all
possible tags. We inject dictionary constraints to
all CRF models, such as features indicating label
prior information. For each model we use sev-
eral named entity features, e.g., movie-title, actor-
name, etc., non-named entity (descriptive) fea-
tures, e.g., movie-description, movie-genre, and
domain independent dictionaries, e.g, time, loca-
tion, etc. For graph-based learning, we imple-
mented the algorithm presented in (Subramanya
et al, 2010) and used the same hyper-parameters
and features. For the rest of the hyper-parameters,
we used: ?=0.01 for MTR, pi=0.5 for interpolation
mixing. These parameters were chosen based on
the performance of the development set. All CRF
objective functions were optimized using Stochas-
tic Gradient Descent.
5.3 Results and Discussions
5.3.1 Experiment 1: Clustering Semantic
Tags.
Here, we want to demonstrate the performance
of MTR model for capturing relationships between
words and semantic tags against baseline topic
models: LDA, HMTM, w-LDA. We take the se-
mantically labeled utterances from the movie tar-
get domain and use the first half for training and
the rest for performance testing. We use all the
collected unlabeled web queries from the movie
domain. For fair comparison, each benchmark
topic model is provided with prior information on
word-semantic tag distributions based on the la-
beled training data, hence, each K latent topic is
assigned to one of K semantic tags at the begin-
ning of Gibbs sampling.
We evaluate the performance separately on de-
scriptive tags, named-entities, and all tags to-
gether. The performance of the four topic models
are reported in Figure 2. LDA shows the worst per-
formance, even though some supervision is pro-
vided by way of labeled semantic tags. Although
w-LDA improves semantic clustering performance
over LDA, the fact that it does not have Markov
properties makes it fall short behind MTR. As for
the effect of word features in MTR, we see a 3%
absolute performance gain over the second best
performing HMTM baseline on named-entity tags,
a 1% absolute gain on descriptive tags and a 2%
absolute overall gain. As expected, we see a drop
in F-measure on all models on descriptive tags.
5.3.2 Experiment 2: Domain Adaptation
Task.
We compare the performance of our SSL model
to that of state-of-the-art models on semantic and
syntactic tagging. Each SSL model is built us-
ing labeled training data from the source do-
main and unlabeled training data from target do-
main. In Table 2 we show the results on Movie
and QuestionBank target test datasets. The re-
sults of SSL-Graph on QuestionBank is taken
from (Subramanya et al, 2010). The self-
training model, Self-CRF adds 3% improve-
ment over supervised CRF models on movie do-
main, but does not improve syntactic tagging. Be-
cause it is always inherently biased towards the
source domain, self-training tends to reinforce
the knowledge that the supervised model already
has. SSL-Graph works much better for both
syntactic and semantic tagging compared to CRF
and Self-CRF models. Our Bayesian MTR ef-
ficiently extracts information from the unlabeled
data for the target domain. Combined with retro-
spective training, R-SSL-MTR demonstrates no-
ticeable improvements, ?2% on descriptive tags,
and 1% absolute gains in overall semantic tag-
921
ging performance over SSL-Graph. On syntac-
tic tagging, the two retrospective learning models
is comparable, close to 1% improvement over the
SSL-Graph and SSL-MTR.
Movie Domain QBank
Model Desc. NE All POS
CRF 75.05 75.84 75.84 83.80
Self-CRF 78.96 79.53 79.19 84.00
SSL-Graph 80.27 81.35 81.23 86.80
SSL-MTR 79.87 79.31 79.19 86.30
R-SSL-Graph 80.58 81.95 81.52 87.12
R-SSL-MTR 82.76 82.27 82.24 87.34
Table 2: Domain Adaptation performance
in F-measure on Semantic Tagging on
Movie Target domain and POS tagging on
QBank:QuestionBank. Best performing models
are bolded.
5.3.3 Experiment 3: Analysis of Semantic
Disambiguation.
Here we focus on the accuracy of our models in
tagging semantically ambiguous words. We inves-
tigate words that have more than one observed se-
mantic tag in training data, such as ?are there any
[war]genre movies available.?, ?remove all movies
about [war]description.?). Our corpus contained
30,000 unique vocabulary, 55% of which are con-
tained in one or more semantic categories. Only
6.5% of those are tagged as multiple categories
(polysemous), which are the sources of semantic
ambiguity. Table-3 shows the precision of two best
models for most confused words.
We compare our two best SSL models with dif-
ferent smoothing regularizes: R-SSL-MTR (MTR)
and R-SSL-Graph (GRAPH). We use preci-
sion and recall criterion on semantically confused
words.
In Table 3 we show two most frequent descrip-
tive tags; genre and description, and commonly
misclassified words by the two models. Results
indicate that the R-SSL-MTR, performs better
than the R-SSL-Graph, in activating the correct
meaning of a word. The results indicate that incor-
porating context information with MTR is an effec-
tive option for identifying semantic ambiguity.
6 Conclusions
We have presented a novel semi supervised learn-
ing approach using a probabilistic clustering
genre description
Vocab. GRAPH MTR GRAPH MTR
war 50% 100% 75% 88%
popular 90% 89% 80% 100%
kids 78% 86% ? 100%
crime 49% 80% 86% 67%
zombie 67% 89% 67% 86%
Table 3: Classification performance in F-measure
for semantically ambiguous words on the most fre-
quently confused descriptive tags in the movie do-
main.
method to semantically tag spoken language ut-
terances. Our results show that encoding priors
on words and context information contributes sig-
nificantly to the performance of semantic cluster-
ing. We have also described an efficient iterative
learning model that can handle data inconsisten-
cies that leads to performance increases in seman-
tic and syntactic tagging.
As a future work, we will investigate using ses-
sion data, namely the entire dialog between the
human and the computer. Rather than using sin-
gle turn utterances, we hope to utilize the con-
text information, e.g., information from previous
turns for improving the performance of the seman-
tic tagging of the current turns.
References
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research.
J. Boyd-Graber, D. Blei, and X. Zhu. 2007. A
topic model for word sense disambiguation. Proc.
EMNLP.
P.F. Brown, V.J.D. Pietra, P.V. deSouza, and J.C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479.
O. Chapelle, B. Schlkopf, and Alexander Zien. 2006.
Semi-supervised learning. MIT Press.
H. Daume?-III. 2010. Frustratingly easy semi-
supervised domain adaptation. Proc. Workshop on
Domain Adaptation for Natural Language Process-
ing at ACL.
T.L Griffiths, M. Steyvers, D.M. Blei, and J.M. Tenen-
baum. 2005. Integrating topics and syntax. Proc. of
NIPS.
A. Gruber, M. Rosen-Zvi, and Y. Weiss. 2005. Hidden
topic markov models. Proc. of ICML.
H. Guo, H. Zhu, Z. Guo, X. Zhang, X. Wu, and Z. Su.
2009. Domain adaptation with latent semantic asso-
ciation for named entity recognition. Proc. NAACL.
922
J. Judge, A. Cahill, and J.Van Genabith. 2006.
Question-bank: Creating corpus of parse-annotated
questions. Proc. Int. Conf. Computational Linguis-
tics and ACL.
A. Lavoie, M.E. Otey, N. Ratliff, and D. Sculley. 2011.
History dependent domain adaptation. Proc. NIPS
Workshop on Domain Adaptation.
X. Li, Y.-Y. Wang, and A. Acero. 2009. Extracting
structured information from user queries with semi-
supervised conditional random fields. Proc. of SI-
GIR.
L. Li, B. Roth, and C. Sporleder. 2010. Topic mod-
els for word sense disambiguation and token-based
idiom detection. Proc. ACL.
X. Li. 2010. Understanding semantic structure of noun
phrase queries. Proc. ACL.
J Liu, X. Li, A. Acero, and Ye-Yi Wang. 2011. Lex-
icon modeling for query understanding. Proc. of
ICASSP.
M. P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguis-
tics, 27:1?30.
D. Mimno, W. Li, and A. McCallum. 2008.
Topic models conditioned on arbitrary features with
dirichlet-multinomial regression. Proc. UAI.
T. Moon, K. Erk, and J. Baldridge. 2010. Crouch-
ing dirichlet, hidden markov model: Unsupervised
pos tagging with context local tag generation. Proc.
ACL.
V.-A. Nyugen, J. Boyd-Graber, and P. Resnik. 2012.
Sits: A hierarchical nonparametric model using
speaker identity for topic segmentation in multiparty
conversations. Proc. ACL.
P. Pantel. 2003. Clustering by committee. Ph.D. The-
sis, University of Alberta, Edmonton, Alta., Canada.
J. Petterson, A. Smola, T. Caetano, W. Buntine, and
S. Narayanamurthy. 2010. Word features for latent
dirichlet alocation. In Proc. NIPS.
J. Reisinger and R. Mooney. 2011. Cross-cutting mod-
els of lexical semantics. In Proc. of EMNLP.
S. Singh, D. Hillard, and C. Leggetter. 2010.
Minimally-supervised extraction of entities from
text advertisements. Proc. NAACL-HLT.
A. Stolcke. 2002. An extensible language modeling
toolkit. Proc. Interspeech.
A. Subramanya, S. Petrov, and F. Pereira. 2010. Effi-
cient graph-based semi-supervised learning of struc-
tured tagging models. In Proc. EMNLP.
G. Tur and R. DeMori. 2011. Spoken language under-
standing: Systems for extracting semantic informa-
tion from speech. Wiley Press.
Y.-Y. Wang, R. Hoffman, X. Li, and J. Syzmanski.
2009. Semi-supervised learning of semantic classes
for query understanding from the web and for the
web. In The 18th ACM Conference on Information
and Knowledge Management.
X. Zhu. 2005. Semi-supervised learning litera-
ture survey. Technical Report 1530, University of
Wisconsin-Madison.
923
