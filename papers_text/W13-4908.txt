Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 71?77,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Exploiting the Contribution of Morphological Information to Parsing: the
BASQUE TEAM system in the SPRML?2013 Shared Task
Iakes Goenaga, Nerea Ezeiza
IXA NLP Group
Faculty of Computer Science
Univ. of the Basque Country UPV/EHU
iakesg@gmail.com, n.ezeiza@ehu.es
Koldo Gojenola
IXA NLP Group
Technical School of Engineering, Bilbao
Univ. of the Basque Country UPV/EHU
koldo.gojenola@ehu.es
Abstract
This paper presents a dependency parsing
system, presented as BASQUE TEAM at
the SPMRL?2013 Shared Task, based on
the analysis of each morphological feature
of the languages. Once the specific rel-
evance of each morphological feature is
calculated, this system uses the most sig-
nificant of them to create a series of ana-
lyzers using two freely available and state
of the art dependency parsers, MaltParser
and Mate. Finally, the system will com-
bine previously achieved parses using a
voting approach.
1 Introduction
Morphologically rich languages present new chal-
lenges, as the use of state of the art parsers for
more configurational and non-inflected languages
like English does not reach similar performance
levels in languages like Basque, Greek or Turk-
ish (Nivre et al, 2007). Using morphological in-
formation as features in parsing has been a com-
monly used method for parsing MRLs (Tsarfaty et
al., 2010). In some cases the effect of this infor-
mation is positive but in others it does not help or
causes a negative effect.
In most of the work on dependency parsing, the
specific relevance of each morphological feature
in the final result is unknown. The authors include
all the morphological features1 in their systems
with the aim of taking advantage of the diversity
of the used information. This approach commonly
produces very good results but they are not always
the best ones (see table 2).
On the other hand, some authors have made ex-
periments to specify which is the real impact of
1That is, they treat all the morphological features in the
same way in the feature specification, and let the learning
algorithms decide the weight assigned to each one.
the morphological features. Ambati et al (2010)
explore ways of integrating local morphosyntactic
features into Hindi dependency parsing. They ex-
periment with different sets of features on a graph-
based and a transition-based dependency parser.
They show that using some morphological fea-
tures (root, case, and suffix) outperforms a base-
line using POS as the only feature, with both gold
and predicted settings .
Bengoetxea and Gojenola (2010) make use of
MaltParser?s feature configuration file to take ad-
vantage of morphological features in parsing with
gold data. Their experiments show that case and
subordination type considerably increase parsing
accuracy.
Marton et al (2013) also explore which mor-
phological features could be useful in dependency
parsing of Arabic. They observe the effect of fea-
tures by adding them one at a time separately and
comparing the outcomes. Experiments showed
that when gold morphology is provided, case
markers help the most, whereas when the mor-
phology is automatically predicted the outcome
is the opposite: using case harms the results the
most. When features are combined in a greedy
heuristic, using definiteness, person, number, and
gender information improves accuracy.
Similarly, Seeker and Kuhn (2013) also deter-
mine that the use of case is specially relevant for
parsing, demonstrating that morpho-syntactic con-
straints can delimit the search space of a statistical
dependency parser to outperform state-of-the-art
baselines for Czech, German and Hungarian.
Following this line of research, our first step
will be to determine which is the concrete value of
each feature on dependency parsing, adding one of
the morphological features at a time starting with
an empty FEATS column.
C?etinog?lu and Kuhn (2013) have shown that
some parsers tend to improve the results when
swapping or replacing POS by some of the mor-
71
phological features. They have made use of the
METU-Sabanc Turkish Treebank (Oflazer et al,
2003) for training and the ITU validation set
(Eryigit, 2007) for testing. In their work, it is ob-
served that moving CASE to the POS field helps
with a 0.3% LAS absolute increase in the gold
pipeline settings and using CASE instead of nom-
inal POS improves the labelled accuracy by 0.3%
absolute for the training set.
These experiments suggest that in some way
the parser is not making an optimal use of all the
available morpho-syntactic information, and that
the parser algorithm (or the feature specification
for the learning phase) is geared towards POS and
CPOS, giving a lower status to other types of in-
formation. Although this strategy is good in gen-
eral, it seems that, at least for some languages, spe-
cific features (e.g. CASE) are crucial in obtaining
a high parsing performance. Taking these ideas
into consideration, we will work on three different
approaches:
? We will experiment the effect of using only
the best three morphological features in the
FEATS column (see table 1), compared to
working with the full set of morpho-syntactic
features. This can have the effect of speed-
ing the learning and parsing processes, as the
number of features can be smaller. On the
other hand, the elimination of non-relevant
features can also help to improve the parser?s
results, because some features can even be
detrimental for parsing.
? Following C?etinog?lu and Kuhn (2013), once
our system resolves which feature is the most
significant, it will be used to replace the POS
and CPOS fields one by one and we will test
the effect of these variants on the parsers. Fi-
nally, we will also try right-to-left versions
of those 3 variants (baseline, and replacing
POS and CPOS) completing a set of 6 differ-
ent parsers.
? Finally, we will experiment the combination
of the different or parsers with a voting ap-
proach (Hall et al, 2010) using the Malt-
Blender tool2.
All of the experiments will be performed on
automatically predicted POS and morphosyntactic
data, taking the tags given in the Shared Task data,
2http://w3.msi.vxu.se/users/jni/blend/
that is, we will not made use of any specifically
trained morphological tagger.
In the rest of this paper we will first present
the resources we have used to carry out our ex-
periments in section 2, followed by a study of the
contribution of the morphological information to
parsing in section 3 and the effect of this infor-
mation on the individual parsers in subsection 4.1.
The final results of the best parser combinations
are showed in subsection 4.2 and the main conclu-
sions of the work in section 5.
2 Resources
This section will describe the main resources that
have been used in the experiments. Subsection
2.1 will describe the languages we have used in
our experiments, subsection 2.2 will explain the
parsers we use, while subsection 2.3 will present
briefly the MaltBlender tool.
2.1 Selected Languages
Although the SPMRL?2013 Shared Task (Seddah
et al, 2013) offers the opportunity to parse nine
morphologically rich languages, to carry out our
experiments we have selected five of them, due in
part to time constraints, but also taking into ac-
count the relevance of the morpho-syntactic infor-
mation (FEATS column, see table 1) . The selected
five languages are: Basque (Aduriz et al, 2003),
French (Abeille? et al, 2003), German (Seeker and
Kuhn, 2012), Hungarian (Vincze et al, 2010) and
Swedish (Nivre et al, 2006).
2.2 Parsers
We have made use of MaltParser (Nivre et al,
2007b) and Mate (Bohnet and Nivre, 2012), two
state of the art dependency parsers3 representing
the dominant approaches in data-driven depen-
dency parsing, and that have been successfully
applied to typologically different languages and
treebanks.
MaltParser is a representative of local, greedy,
transition-based dependency parsing models,
where the parser obtains deterministically a
dependency tree in a single pass over the input
using two data structures: a stack of partially
analyzed items and the remaining input sequence.
To determine the best action at each step, the
3Due to time constraints, we did not have enough time to
experiment with other options such as the MST parser or the
EasyFirst parser.
72
parser uses history-based feature models and dis-
criminative machine learning. The specification
of the learning configuration can include any
kind of information (such as word-form, lemma,
category, subcategory or morphological features).
We will use one of its latest versions (MaltParser
version 1.7).
To fine-tune Maltparser we have used MaltOp-
timizer (Ballesteros and Nivre, 2012a; Ballesteros
and Nivre, 2012b). This tool is an interactive sys-
tem that first performs an analysis of the training
set in order to select a suitable starting point for
optimization and then guides the user through the
optimization of parsing algorithm, feature model,
and learning algorithm. Empirical evaluation on
data from the CoNLL 2006 and 2007 shared tasks
on dependency parsing shows that MaltOptimizer
consistently improves over the baseline of default
settings and sometimes even surpasses the result
of manual optimization.
The Mate parser (Bohnet and Nivre, 2012) is a
development of the algorithms described in (Car-
reras, 2007; Johansson and Nugues, 2008). It basi-
cally adopts the second order maximum spanning
tree dependency parsing algorithm. In particular,
this parser exploits a hash kernel, a new parallel
parsing and feature extraction algorithm that im-
proves accuracy as well as parsing speed (Bohnet,
2010).
2.3 Parser Combinations
The MaltBlender tool makes a two-stage optimiza-
tion of the result of several parser outcomes, based
on the work of Sagae and Lavie (2006), and it was
used for the first time for the ten languages in the
multilingual track of the CoNLL 2007 shared task
on dependency parsing(Hall et al, 2010). The first
stage consists in tuning several single-parser sys-
tems. The second stage consists in building an
ensemble system that will combine the different
parsers. When this system was evaluated on the
official test sets at the CoNLL 2007 shared task,
the ensemble system significantly outperformed
the single-parser system and achieved the highest
average labelled attachment score of all participat-
ing systems.
3 Contribution of Morphological
Information to Parsing
We examined the effect of each type of morpho-
logical information, contained in the FEATS col-
umn, to investigate their overall contribution to
parsing. This will help us to determine which are
the most relevant features for parsing. To carry out
this task we have used the Mate parser, due to lack
of time for testing, and also taking into consid-
eration that it gives better results than MaltParser
for all the languages?s baselines. Firstly, we will
obtain the baseline for each language parsing the
files with an empty FEATS column. This baseline
will help us to determine the contribution of each
morphological feature to parsing. Next, we trained
the parsers using one feature at a time obtaining as
many results as features for each language. Table
1 shows the effect of each information on the Mate
parser.
In this table we can observe that Basque is one
of the most sensitive languages regarding the influ-
ence of its features. Using case (KAS) as a unique
feature improves the labelled attachment score
over using an empty FEATS column by almost
5.7%. The next two better features are number
(NUM) and type of subordinate sentence (ERL).
They help with a 1.1% and 0.6% increase, respec-
tively. The rest of the features do not contribute
much in isolation, with a maximum of 0.2%. On
the other hand, including all the features results in
an improvement of 6.5%.
If we analyze the results for French we see that,
in contrast to Basque, the influence of the features
on the parser is minimum. The most significant
feature is gender (g), which helps with a 0.1% in-
crease. With respect to the improvement using the
other features, although they do not provide big in-
creases all of them contribute positively. In clos-
ing, including all the features we obtain a 84.6%
labelled attachment score with a 0.4% improve-
ment over not using any features.
As with French, the German morphological fea-
tures provide small increases. The most two sig-
nificant features are case and gender, which obtain
increases of 0.2%, 0.13%, respectively. It is inter-
esting to observe how including all the features we
obtain worse results than using only the case, al-
though the difference is not significant. That could
occur due to the weak influence of its features in
the final result and the negative influence of some
of them.
Hungarian is the language which offers more
features, 14 altogether. This language, in line with
Basque, tends to vary significantly its labelled at-
tachment score depending on the used morpholog-
73
Basque French German Hungarian Swedish
all feats 83.0 all feats 84.6 all feats 91.0 all feats 82.8 all feats 76.7
no feats 76.5 no feats 84.2 no feats 90.9 no feats 75.3 no feats 76.9
KAS 82.2 g 84.3 case 91.0 Cas 80.9 verbform 77.0
NUM 77.7 n 84.3 gender 91.0 PerP 76.3 definiteness 76.8
ERL 77.1 p 84.3 number 90.9 NumP 76.3 degree 76.8
DADUDIO 76.8 c 84.2 person 90.9 SubPOS 75.9 case 76.8
NORK 76.7 m 84.2 tense 90.9 Def 75.7 number 76.3
MDN 76.6 s 84.2 degree 90.8 Num 75.7 perfectform 76.3
NOR 76.6 t 84.2 mood 90.8 PerP 75.7 abbrv 76.3
ASP 76.4 Mood 75.5 mood 76.2
NORI 76.2 NumPd 75.4 pronounform 76.1
ADM 76.5 Coord 75.3 gender 76.0
Form 75.3
Tense 75.3
Type 75.3
Deg 75.0
Table 1: The effect of each feature sorted by language (MATE parser)
ical feature. If we focus on the three most signif-
icant features, the case (Cas) helps with a 5.6%
increase, person of possessor (PerP) with a 1%,
while number of possessor helps with a 0.9%. The
grammatical subcategory within the main part of
speech (SubPOS) improves the baseline in a 0.6%
and the number and person in a 0.4%. The remain-
ing features do not contribute very appreciatively
even obtaining negative results. Including all the
features we obtain a labelled attachment score of
82.83%. That means the real contribution of all
the features is 7.5%, this improvement being the
most important among all the used languages.
In common with French and German, the
Swedish morphological features do not seem to
help the parsers to achieve significant improve-
ments in terms of LAS. However, we can observe
some interesting phenomena. While in the other
languages the case is one of the best features, in
Swedish is does not help, achieving a negative re-
sult. In general, excluding the verb form (verb-
form), all the features obtain negative results with
respect to not using any feature. In this scenario
it is not surprising to verify that including all the
features does not help the Mate parser. Having
said this, the best three features are the verb form
(verbform), definiteness (definiteness) and degree
(degree).
4 Testing the Effect of Different
Morphosyntactic features on parsers
We examined the effect of the most significant
morphological features, examined in the previous
step, to investigate their overall contribution to
parsing. For this task, we created three variants for
each parser, apart from the baseline using all the
morphosyntactic features. We obtain these vari-
ants by: i) using the most 3 relevant features in
the FEATS column (see table 1 in previous sec-
tion), ii) moving the most relevant feature for each
language to the POS column and iii) moving the
most relevant feature to the CPOS column. Next,
we have tested parser combinations including all
the baselines and their variants in subsection 4.2.
4.1 Individual Parsers
Table 2 shows the effect of each information on
both parsers, Maltparser and Mate parser. If we
analyze the results on Basque, the difference be-
tween the two parsers is noticeable, as Mate ob-
tains on average a 3 point improvement with re-
spect to MaltParser. A similar difference occurs
on all the used languages. The best LAS in Basque
is acquired using the 3 best features in the FEATS
column with the Mate parser (83.4%). On a com-
parison with the LAS obtained by the Mate base-
line (All-Feats), that means a 0.4 improvement.
Regarding Maltparser?s results for Basque, we get
the best LAS (81.0%) moving the best feature
(case) to POS in its right-to-left version, increas-
ing the LAS baseline (All-Feats) by 1.0. We no-
tice that Maltparser and Mate tend to improve their
baseline scores using some of the presented vari-
ants.
On the other hand, the best score for French
is obtained using the baseline (All-Feats and
74
Basque French German Hungarian Swedish
Baselines
All ? FeatsMalt 80.0 79.9 87.6 77.3 73.4
All ? FeatsMate 83.0 84.6 91.0 82.3 76.7
Left2right
3? bestMalt 79.9 79.9 87.6 75.9 73.4
CPOS ? bestMalt 80.3 79.7 87.5 76.6 72.9
POS ? bestMalt 78.7 78.7 86.6 77.2 72.8
3? bestMate 83.4 84.3 90.8 82.4 76.6
CPOS ? bestMate 82.7 84.3 91.0 82.7 76.8
POS ? bestMate 82.2 83.4 90.5 82.5 76.5
Right2left
3? bestMalt 80.1 78.9 86.9 75.3 69.3
CPOS ? bestMalt 80.0 79.0 86.7 76.6 69.3
POS ? bestMalt 81.0 77.8 85.4 74.9 70.2
3? bestMate 83.3 84.3 90.9 82.1 76.5
CPOS ? bestMate 83.1 84.6 91.0 82.6 77.0
POS ? bestMate 81.6 83.5 90.6 82.4 76.4
Table 2: Testing the effect of features on MaltParser and Mate
the Mate parser, 84,6%). Contrary to Basque,
in French, although some of the used variants
achieve similar scores with respect to their base-
lines (All-Feats), they do not give noticeable in-
creases. The unique variant that equals its base-
line (79,9%) is 3? bestMalt using the left-to-right
version and the three best features (gender, num-
ber and person) in the FEATS column using Malt-
parser.
With respect to German, the only variant that
equals the baseline is CPOS ? bestMate with
91.0% LAS. . If we focus on Maltparser?s (Mal-
tOptimizer) scores, we get the best result among
the variants with 3 ? bestMalt (87.6%) using the
left-to-right version. The variants do not improve
Maltparser?s baseline.
Although some of the Hungarian variant scores
are very similar to their baselines, they give some
improvements over the baseline. The best two re-
sults on the Mate parser are 82.7% and 82.6%. We
obtain the first score moving the best feature (case)
to CPOS in its left-to-right version, and the second
one using the same configuration in its right-to-left
version. The best two scores on Maltparser with-
out taking the baseline into account are 77.2% and
76.6%, obtained when moving the best feature to
POS and moving the best feature to CPOS in its
right-to-left version, respectively.
The best two results for Swedish on the Mate
parser are 77.0% and 76.8%. We get the first re-
sult moving the best feature (verbform) to CPOS
in its right-to-left version and the second one in its
standard version. These two results are the only
variants that improve the baseline (76.7% LAS)
with a 0.30 and 0.17 increase, respectively. On the
other hand, if we focus on Maltparser, the variants
do not improve the baseline (73.4% LAS) where
the best two results are 73.4% and 72.9% LAS.
For the best result we use the three best features
(verbform, definiteness and degree) in the FEATS
column, while for the second one the best feature
(verbform) has been moved to CPOS.
Despite that only the Basque and Swedish vari-
ants haven been able to significantly improve their
baselines, in the next subsection we present a com-
bination system expecting to take advantage on the
variety of the parsed files (Surdeanu and Manning,
2010).
4.2 Parser Combinations
Although in several cases the use of specific mor-
phosyntactic information does not give noticeable
increases, we also tested the effect on parser com-
binations. Table 3 presents the result of combin-
ing the extended parsers with the baselines (us-
ing all the features) obtained in individual parsers.
The table shows that the Basque language has
achieved the biggest increase. Parser combination
in Basque helps with an improvement of 3.2 with
respect to the Mate baseline. Contrary to Basque,
French is the language that has obtained the small-
est increases in parser combination if we compare
it with the Mate (highest) parser baseline. The
combined system improves the Mate parser base-
75
Basque French German Hungarian Swedish
MaltParser baseline 80.0 79.9 87.6 77.3 73.4
Mate parser baseline 83.0 84.6 91.0 82.8 76.7
Parser combination 86.2 85.1 91.8 84.1 78.1
Table 3: Results of parser combinations
line by 0.5. Parser combination in German gives a
0.8 increase with respect to the best single parser
(Mate, 91.0). Our system achieves a 1.3 increase
for Hungarian with respect to the Mate parser?s
baseline. Finally, if we focus on Swedish, the
parser combination helps with a 1.4 increase with
respect to the Mate parser.
After examining the parsers involved in parser
combinations we noticed that there are always sev-
eral variants included in the best parser combina-
tions, although the only variant that appears in all
the best parser combinations is CPOS?bestMate
in its left-to-right version. Taking into account
that the most relevant feature for Basque, German
and Hungarian is the case, it would be interest-
ing to use the CPOS?caseMate variant for other
languages. Finally, the presented results suggest
that the introduced variants contribute positively
on parsing and they help to improve the scores ob-
tained by the base parsers.
5 Conclusion and Future Work
We have presented a combined system that was
designed after analyzing the relevance of the mor-
phological features in order to take advantage on
the effect of those features on some parsers. In
general the improvements have been noticeable,
specially for Basque. We can point out some in-
teresting avenues for research:
? Use of new parsing algorithms for testing
the effect of different morphological fea-
tures. The results of this work show that the
used techniques are specially useful for lan-
guages where the FEATS column, contain-
ing morpho-syntactic information, gives the
biggest increments with respect to not us-
ing the features, like Basque and Hungar-
ian. We expect that similar improvements
could be obtained for languages like Turkish
or Czech, which share many characteristics
with Basque and Hungarian.
? Experimenting different models for parser
combinations using new parsers. Several of
the parser variants we have used give only
slight modifications over the base algorithms,
even though when combined they give sig-
nificant increases. Widening the spectrum of
parsers and adding new algorithms can imply
an important boost in parser combination.
? Application to the rest of the languages of the
SPMRL 2013 Shared Task: Korean, Hebrew,
Arabic and Polish.
Acknowledgements
This research was supported by the Department of
Industry of the Basque Government (IT344-10, S
PE11UN114), the University of the Basque Coun-
try (GIU09/19) and the Spanish Ministry of Sci-
ence and Innovation (MICINN, TIN2010-20218).
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.
2003. Building a treebank for french. In Anne
Abeille?, editor, Treebanks. Kluwer, Dordrecht.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. D??az de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency tree-
bank. pages 201?204.
Bharat Ram Ambati, Samar Husain, Sambhav Jain,
Dipti Misra Sharma, and Rajeev Sangal. 2010. Two
methods to incorporate local morphosyntactic fea-
tures in hindi dependency parsing. In Proceedings of
the NAACL HLT 2010 First Workshop on Statistical
Parsing of Morphologically-Rich Languages, pages
22?30.
Miguel Ballesteros and Joakim Nivre. 2012a. Maltop-
timizer: A system for maltparser optimization. In
LREC, pages 2757?2763.
Miguel Ballesteros and Joakim Nivre. 2012b. Mal-
toptimizer: an optimization tool for maltparser. In
Proceedings of the Demonstrations at the 13th Con-
ference of the European Chaptr of the Association
for Computational Linguistics, pages 58?62.
Kepa Bengoetxea and Koldo Gojenola. 2010. Appli-
cation of different techniques to dependency pars-
ing of basque. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 31?39.
76
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1455?1465.
Bernd Bohnet. 2010. Very high accuracy and fast de-
pendency parsing is not a contradiction. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, pages 89?97.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, Prague, Czech Republic, June.
O?zlem C?etinog?lu and Jonas Kuhn. 2013. Towards
joint morphological analysis and dependency pars-
ing of turkish. In Proceedings of the Second In-
ternational Conference on Dependency Linguistics
(DepLing 2013), pages 23?32, Prague, Czech Re-
public, August. Charles University in Prague, Mat-
fyzpress, Prague, Czech Republic.
Gu?lsen Eryigit. 2007. Itu validation set for metu-
sabanc? turkish treebank. URL: http://www3. itu.
edu. tr/ gulsenc/papers/validationset. pdf.
Johan Hall, Jens Nilsson, and Joakim Nivre. 2010.
Single malt or blended? a study in multilingual
parser optimization. In Trends in Parsing Technol-
ogy, pages 19?33. Springer.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic-semantic analysis with
propbank and nombank. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning, pages 183?187.
Yuval Marton, Nizar Habash, and Owen Rambow.
2013. Dependency parsing of modern standard ara-
bic with lexical and inflectional features. Computa-
tional Linguistics, 39(1):161?194.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase struc-
ture and dependency annotation. In Proceedings of
LREC, pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task
on dependency parsing. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL
2007, Prague, Czech Republic, June.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007b. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Kemal Oflazer, Bilge Say, Dilek Zeynep Hakkani-
Tu?r, and Go?khan Tu?r. 2003. Building a turkish
treebank. Building and Exploiting Syntactically-
annotated Corpora.
Kenji Sagae and Alon Lavie. 2006. Parser com-
bination by reparsing. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL.
Djame? Seddah, Reut Tsarfaty, Sandra Ku?bler, Marie
Candito, Jinho Choi, Richa?rd Farkas, Jennifer Fos-
ter, Iakes Goenaga, Koldo Gojenola, Yoav Goldberg,
Spence Green, Nizar Habash, Marco Kuhlmann,
Wolfgang Maier, Joakim Nivre, Adam Przepi-
orkowski, Ryan Roth, Wolfgang Seeker, Yannick
Versley, Veronika Vincze, Marcin Wolin?ski, Alina
Wro?blewska, and Eric Villemonte de la Cle?rgerie.
2013. Overview of the spmrl 2013 shared task: A
cross-framework evaluation of parsing morpholog-
ically rich languages. In Proceedings of the 4th
Workshop on Statistical Parsing of Morphologically
Rich Languages: Shared Task, Seattle, WA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making El-
lipses Explicit in Dependency Conversion for a Ger-
man Treebank. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation, pages 3132?3139, Istanbul, Turkey. Euro-
pean Language Resources Association (ELRA).
Wolfgang Seeker and Jonas Kuhn. 2013. Morphologi-
cal and syntactic case in statistical dependency pars-
ing. Computational Linguistics, 39(1):23?55.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: Cheap
and good? In Proceedings of the North Ameri-
can Chapter of the Association for Computational
Linguistics Conference (NAACL-2010), Los Ange-
les, CA, June.
Reut Tsarfaty, Djam Seddah, Yoav Goldberg, San-
dra Ku?bler, Marie Candito, Jennifer Foster, Yan-
nick Versley, Ines Rehbein, and Lamia Tounsi.
2010. Statistical parsing of morphologically rich
languages (spmrl) what, how and whither. In In Pro-
ceedings of the NAACL HLT 2010 First Workshop
on Statistical Parsing of Morphologically-Rich Lan-
guages.
Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgy
Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010. Hun-
garian dependency treebank. In LREC.
77
