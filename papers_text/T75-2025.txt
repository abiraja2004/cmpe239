IMPROVING METHODOLOGY* 
in 
Natural Language Processing 
Wil l iam C. Mann 
USC Information Sciences Institute 
Marina Del Rey, Cal i fornia 
SCOPE 
This is a posit ion paper on 
understanding and improving the curreht 
styles and methods of scientif ic work in the 
appl icat ion of computers to texts composed 
of elements from human languages, such as 
stories, dialogues and sentences. It deals 
on ly  with kinds of research in which 
acoustic issues are secondary or absent. It 
is written speci f ical ly  to precede 
discussion at the Workshop on Technical  
Issues in Natural Language Processing. 
There are various or ientat ions toward 
value that tend  to get assumed rather than 
discussed at this point. They need not 
conflict, but some select iv i ty is necessary. 
Very roughly, there is an or ientat ion toward 
understanding and scientif ic knowledge, and 
there is an or ientat ion toward appl icat ion 
and practical  use. Many people regard 
understanding as a near ly-necessary 
prerequis ite to practical  accomplishment.  
That's the view in this paper, so we 
therefore concentrate on scientif ic values 
without denying the others. 
There is a great diversity of 
acbivit ies that are carried out by 
recognizable methods, for which serious 
questions of methodology could be raised. 
There are tool -bui ld ing and laboratory setup 
activit ies. We do not build l inear 
accelerators or observatories,  but we put 
large efforts into tools anyway. There are 
speculat ive and exploratory act ivit ies that 
inf luence the course of later, more formal 
work. Choice of phenomena to study is an 
absolutely crucial  one of these activit ies. 
There are administrat ive act iv it ies for 
which methods are important. Staff ing and 
seeking funds are also vital. All of these 
ant ic ipate and support the creation of 
specif ic results and are vital to success. 
The act iv it ies that produce the 
knowledge that keeps the work going are of a 
different kind. IT IS THESE CONSUMMATORY 
ACTIVITIES THAT I FOCUS ON HERE, TO THE 
EXCLUSION OF ALL THE OTHERS. 
CONSEQUENCES OF METHODOLOGY CHOICE 
We are current ly at a crucial stage in 
the development of methodology, since we 
have a s ignif icant history of experience, 
but a great deal of remaining f lexibi l i ty. 
For better or for worse, the methodological  
choices made in the next few years by our 
present leaders are l ikely to be with us for 
a very long time. The formal 
result -produclng style that we adopt is 
*(C) Copyright 1975, Wi l l iam C. Mann 
126 
part icular ly crucial for two reasons - 
first, because it ends up being the least 
f lexible set of precedents, perhaps with the 
exception of basic presupposit ions, and 
second, because it produces a strong final 
f i l ter ing effect on the results. The 
adoption of a stat ist ical  hypothesis 
evaluat ion framework leads to different 
kinds of results. Likewise, our formal 
approach will produce its own kind of 
results and inherent l imitations. So, we 
must pay careful attent ion to our current 
style. 
My general att i tude is that current 
methods can be very s igni f icant ly improved, 
and that doing so will have a very high 
payoff with benefits far beyond the 
improvements to present and contemplated 
efforts. The methods current ly in use are 
under-examined and poorly understood, and 
tradit ions are still weak enough to al low 
changes. There are attract ive a l ternat ives 
to many common practices. 
PRESENT ADVANTAGES 
Of the great diversity of approaches to 
language, the process approach represented 
at the workshop is uniquely capable. The 
two key methodological  problems in the study 
of language over the last 2,500 years or so 
have been the problem of rigor and the 
problem of complexity. The problem of r igor 
in the use of natural  language led to formal 
logics and to Godel. The problem of 
complexity has led to various strong 
reduct ions on the general phenomena, with 
tools Such as the Osgood Semantic 
Dif ferential ,  or paired-associate tests. 
Sequent ia l -order  phenomena and individual  
use of language tend to get badly obscured. 
Process theory approaches the problem 
of r igor with methods by which process 
speci f icat ions are made very explicit. It 
approaches the problem of complexity with 
computers, that can hold and make use of 
very large numbers of processes at once. 
The compatabi l i ty  and effective coverage of 
large col lect ions of hypotheses can now 
actual ly  be tested. 
? These are exciting, reor ient ing 
advantages that make me prefer the process 
approach to any other, to hold high hopes 
for its success, and to want it to be built 
on good foundations. 
WHAT MAKES A DIFFERENCE? 
What do we want out of our methodology? 
Three character ist ics  of a methodology are 
part icular ly  important: 
re l iabi l i ty  
ef f ic iency 
integrat ive power 
Rel iabi l i ty  encompasses all of those 
things that make experiments t rustworthy at 
face value, including repeatabi l i ty,  c lar i ty  
I 
l 
I 
I 
! 
I 
I 
I 
I 
I 
i 
I 
I 
i 
i 
l 
I 
I 
! 
,I 
! 
1 
I 
I 
I 
of def init ion and freedom from various kinds 
of c ircumstantial  effects that might be 
responsible for success. Eff ic iency 
addresses the effort required to achieve 
part icular results. (You don't plan to do 
basic genetics studies on elephants; you may 
prefer fruit- f l ies as subjects.) It deals 
not only with the costs of performing the 
work, but with support costs as well. 
Integrative power involves the scope of the 
theories, what diversity of phenomena they 
cover, what subtheories they coordinate, 
what kinds of investigations they 
facil itate. 
In order to discuss current practices 
we need some representat ive example. The 
one here is del iberately simple and not 
identi f ied with a particular development 
effort. However it is composed of elements 
that seem to be widely used. 
EXAMPLE OF A NATURAL LANGUAGE PROJECT 
Step I: Select a phenomenon: 
CONTRADICTION 
Step 2: Select an input form: 
ENGLISH SENTENCES 
Step 3: Select an output form: 
ENGLISH SENTENCES THAT CONTRADICT 
THE INPUT SENTENCES 
Step 4: Design and draft a 
program in the local language: 
MEGALISP 
Step 5: Debug on examples of 
opportunity, selected to exercise 
the code. 
Step 6: Publish: 
"CONTRADICTION IN NATURAL LANGUAGE" 
by Leader and Worker. 
SOME STRENGTHS IN CURRENT PRACTICE 
We should hold on to the dist inct ive 
strengths of our methods in any changes we 
plan. These strengths are general ly direct 
classic consequences of the use of computers 
to hold models: 
Complexity of data and theory is 
easy to accommodate. 
Time sequences and dependencies are 
preserved. 
A diversity of hypotheses can be 
applied and tested for consistency 
in each experiment. 
All of these have to do with 
integrat ive power, and on this dimension we 
are, at least potential ly, in very good 
shape. 
SOME WEAKNESSES 
We have some serious problems. Here 
are some recurring problems with the FORM of 
the work: 
127 
I. Single experiments often take 
years to execute. 
2. The act ivity is often treated 
as programming and program 
documentat ion rather than science. 
The consequences are general ly that 
the data are poorly identif ied and 
poorly chosen, the status of the 
programs as theory is not clear, 
the business of making clear 
theoretical  claims is neglected, 
and the relevance of the act ivity 
to exist ing theories that are not 
programs is never established. The 
remainder of science is thus cut 
off, and left wondering whether we 
are into science at all. 
3. The attempt to perform a 
general transaction, such as 
Sentence:Contradict ion,  strongly 
limits the complexity of the input 
that gets actual ly addressed, with 
the result that signif icant 
phenomena are missed. The effects 
of prior context, speakers" goals, 
tacit mutual knowledge of speaker 
and hearer are often attenuated by 
the attempt to be general. 
4. The unit of production is a 
system. Whole systems are 
diff icult to disseminate and 
diff icult to judge as scienti f ic 
hypotheses, and are not general ly 
understood or appreciated by 
non-programming scientists. 
5. Coping with ad-hocness is a 
problem: The system runs the 
examples, but what else it will do 
is unclear, or, the degree of 
tuning to the examples is unclear, 
or, the representat iveness of the 
examples is unclear, or, the 
r ightness of the answers is only 
establ ished intuit ively. 
We have problems with the CONTENT of 
the work. There are many problems, which 
may be a healthy condition, but I want to 
attend to just one that seems to be 
otherwise. 
In the common notion, a natural 
language is a scheme of communicat ion that 
people use. The fact that a language is 
used to communicate has strong consequences. 
For example, as languages change, their 
adequacy for communicat ion must be 
maintained. 
The communicat ion properties of 
language are being ignored in a wide variety 
of approaches, including processing 
approaches. Often, it is outside of the 
paradigmatic scope of the studies. 
Communicat ion deals with changing 
correspondences between the knowledge of one 
individual  or system and the knowledge of 
another. It is more than relat ions between 
str ings and strings, or relat ions between 
str ings and generators of strings (syntax). 
It is more than relat ions between str ings 
and a world or a data base (semantics). 
Communicat ion involves two active 
processors, and an adequate theory of 
language will specify some consequences of 
that fact. By restr ict ing the view to a 
single processor (or less), I suspect that 
we are cutting ourselves off from the 
organizing principles that produce the 
regular it ies that we are trying to study. 
Some of the changes of style that I 
would suggest are implicit in ~he 
ident i f icat ions of the problems cited above: 
Design clear data col lect ion 
methods. 
State theoretical  claims that 
are distinct from the programs. 
(The claims may still contain 
algorithms, of course.) 
Decommit from attempts to be 
general, except where an empir ical  
demonstrat ion of general i ty is 
included in the work. 
Shift from focus on systems to 
focus on algorithms. 
Do something to drast ical ly  
shorten the period required to do 
single experiments. 
Beyond these suggestions, the special  
advantages of case analysis should be 
considered. 
CASE ANALYSIS AS THE BASIS FOR AN ALTERNATE 
PROCESSING METHODOLOGY 
Case analysis as a basic scient i f ic  
act iv i ty  is an attract ive alternat ive to the 
current methodology sketched above. How 
would it work? 
STEPS IN A CASE-ANALYSIS-BASED DEVELOPMENT 
IN 
NATURAL LANGUAGE PROCESSING 
Step 1: DATA ACQUISITION. Examples 
of real-world use of natural 
language are col lected. Some are  
selected for detai led attention. 
Step 2: PHENOMENON 
IDENTIFICATION: The data are 
annotated and scored for part icular 
phenomena of interest. Data can be 
scored for several phenomena at 
once. Scoring is performed by 
people who understand the language 
and the c i rcumstances of the data 
occurrence, and who are given 
expl icit  instruct ions on what to 
look for and how to annotate it. 
The result of this step is a 
Commentary on the data. 
Examples: 
a. Identi fy requests and 
judge whether they are fulf i l led in 
running dialogue. 
128 
b. Identi fy repeated 
references to an object, action or 
idea in a document. 
Step 3: CASE MODELING: 
Custom-bui ld for this data, a new 
one-shot program that will take the 
data as input, and make entries 
into a simulated Hearer's Memory. 
The program is the Model, and its 
"output" is its trace. 
Step 4: MODEL EVALUATION: 
Compare the Commentary with the 
execution trace of the model. For 
each signif icant event identi f ied 
in the Commentary, decide whether 
there was a correctly corresponding 
event in the model 's execution. 
With suitable select ions of phenomena 
for study, it is not hard to decide whether 
the program performed appropriately.  
However, a serious problem remains: a 
program for a single case can be entirely ad 
hoc. This is an advantage, in that it is 
certain beforehand that the program will run 
successful ly,  independent of the complexi ty 
of the phenomena. But the program may or 
may not have any long-term signif icance. 
The program is composed of cooperat ing 
processes. Each process can be considered 
to be an over-speci f ied hypothesis,  
over-speci f ied because details such as the 
programming language are inessential  to the 
corresponding functional claims about 
language. 
VERIF ICATION STEP: In order to meet 
the ad-hocness problem, these 
hypotheses must be veri f ied by 
repeated appl icat ion to a divers i ty 
of cases. The experiment steps 
cited above must be repeated, and 
their results compared. 
Inessent ia l  details (such as 
programming language and machine) 
may be changed, if desired, but the 
propert ies of the algor i thms which 
form the basis for the theoret ical  
claims of the work must be held 
constant. 
The veri f ied results are those 
algor i thms that continue to work correctly, 
when their act ions are judged against the 
Commentary, in model after model. These 
algor i thms are the valuable ones both for 
practical  appl icat ion and for sc ient i f ic  
knowledge. 
ADVANTAGES OF CASE ANALYSIS 
METHODOLOGY 
Since the data acquis i t ion step is 
first rather than nearly last, stronger 
claims can be made for the abi l i ty to model 
real-world phenomena. Having the data in 
hand is a strong guide to implementat ion.  
Because phenomena ident i f icat ion is 
explicit, and proceeds from expl ic i t  
instructions, the result ing theory has a 
! 
{ 
\ ! 
{ 
! 
! 
! 
! 
{ 
! 
! 
! 
I 
! 
I 
! 
e 
S 
a 
clear operat ional  interpretat ion, since it 
subst itutes powerful hindsight for 
less-powerful  antic ipation. 
There is better control on complexity 
and effort, since no claims are made for the 
general i ty of the whole systems that are 
built. The amount of data modeled can be 
controlled, and a diversity of data sources 
can be accommodated. There is strong 
control over the involvement of 
wor ld-knowledge in models, since most of the 
part iculars can be ant ic ipated by looking at 
the data. 
The method can also be control led by 
choices about whether several phenomena will  
be modeled in a s ing le  model or several 
smaller models. The smaller models are 
simpler, but the single model exhibits the 
compatabi l i ty  of the parts and the 
consistency of the set of hypotheses. 
This approach typical ly runs in a more 
data -dr iven ,  phenomena-responsive manner 
than a general system bui lding approach. It 
avoids the s ituat ion in which system design 
is based on inadequate stereotypes of what 
might happen at the input. Programming can 
be more goal -d irected as well, since the 
phenomena of interest have already been 
ident i f ied in the Commentary. 
The problems of ad-hocness are treated 
explicit ly, rather than being left to the 
suspicions of the journal readers. This 
faci l i tates representat ions of the degree 
and kinds of tests that the theories have 
had. (I suspect that for some current 
systems, many readers believe that they wil l  
only run the explanatory examples in the 
papers). 
Finally, because of the close control  
and 20-20 hindsight of case analysis, more 
complex phenomena can be accommodated. In 
particular, communicat ion between two 
non- ident ical  human processors can be 
modeled. 
AN ACTIVE EXAMPLE OF CASE MODELING 
METHODOLOGY 
The Dialogue Process Model ing work at 
ISI is an active attempt to apply the ideas 
above with some embel l ishments,  to real 
natural language processing problems. All 
of the recommendat ions are being used in 
ident i f iable ways. This work wil l  be 
described in discussion at the conference as 
time permits. 
129 
! 
