Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 564?572,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Variational Inference for Adaptor Grammars
Shay B. Cohen
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
scohen@cs.cmu.edu
David M. Blei
Computer Science Department
Princeton University
Princeton, NJ 08540, USA
blei@cs.princeton.edu
Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
Adaptor grammars extend probabilistic
context-free grammars to define prior dis-
tributions over trees with ?rich get richer?
dynamics. Inference for adaptor grammars
seeks to find parse trees for raw text. This
paper describes a variational inference al-
gorithm for adaptor grammars, providing
an alternative to Markov chain Monte Carlo
methods. To derive this method, we develop
a stick-breaking representation of adaptor
grammars, a representation that enables us
to define adaptor grammars with recursion.
We report experimental results on a word
segmentation task, showing that variational
inference performs comparably to MCMC.
Further, we show a significant speed-up when
parallelizing the algorithm. Finally, we report
promising results for a new application for
adaptor grammars, dependency grammar
induction.
1 Introduction
Recent research in unsupervised learning for NLP
focuses on Bayesian methods for probabilistic gram-
mars (Goldwater and Griffiths, 2007; Toutanova and
Johnson, 2007; Johnson et al, 2007). Such meth-
ods have been made more flexible with nonparamet-
ric Bayesian (NP Bayes) methods, such as Dirichlet
process mixture models (Antoniak, 1974; Pitman,
2002). One line of research uses NP Bayes meth-
ods on whole tree structures, in the form of adaptor
grammars (Johnson et al, 2006; Johnson, 2008b;
Johnson, 2008a; Johnson and Goldwater, 2009), in
order to identify recurrent subtree patterns.
Adaptor grammars provide a flexible distribu-
tion over parse trees that has more structure than
a traditional context-free grammar. Adaptor gram-
mars are used via posterior inference, the compu-
tational problem of determining the posterior distri-
bution of parse trees given a set of observed sen-
tences. Current posterior inference algorithms for
adaptor grammars are based on MCMC sampling
methods (Robert and Casella, 2005). MCMC meth-
ods are theoretically guaranteed to converge to the
true posterior, but come at great expense: they are
notoriously slow to converge, especially with com-
plex hidden structures such as syntactic trees. John-
son (2008b) comments on this, and suggests the use
of variational inference as a possible remedy.
Variational inference provides a deterministic al-
ternative to sampling. It was introduced for Dirich-
let process mixtures by Blei and Jordan (2005) and
applied to infinite grammars by Liang et al (2007).
With NP Bayes models, variational methods are
based on the stick-breaking representation (Sethu-
raman, 1994). Devising a stick-breaking represen-
tation is a central challenge to using variational in-
ference in this setting.
The rest of this paper is organized as follows. In
?2 we describe a stick-breaking representation of
adaptor grammars, which enables variational infer-
ence (?3) and a well-defined incorporation of recur-
sion into adaptor grammars. In ?4 we give an em-
pirical comparison of the algorithm to MCMC in-
ference and describe a novel application of adaptor
grammars to unsupervised dependency parsing.
2 Adaptor Grammars
We review adaptor grammars and develop a stick-
breaking representation of the tree distribution.
2.1 Definition of Adaptor Grammars
Adaptor grammars capture syntactic regularities in
sentences by placing a nonparametric prior over the
distribution of syntactic trees that underlie them.
The model exhibits ?rich get richer? dynamics: once
a tree is generated, it is more likely to reappear.
Adaptor grammars were developed by Johnson et
al. (2006). An adaptor grammar is a tuple A =
?G,M,a, b,??, which contains: (i) a context-free
grammar G = ?W,N,R, S? where W is the set of
564
terminals, N is the set of nonterminals, R is a set of
production rules, and S ? N is the start symbol?we
denote byRA the subset ofR with left-hand sideA;
(ii) a set of adapted nonterminals, M ? N; and (iii)
parameters a, b and ?, which are described below.
An adaptor grammar assumes the following gen-
erative process of trees. First, the multinomial dis-
tributions ? for a PCFG based on G are drawn
from Dirichlet distributions. Specifically, multino-
mial ?A ? Dir(?A) where? is collection of Dirich-
let parameters, indexed by A ? N.
Trees are then generated top-down starting with
S. Any non-adapted nonterminal A ? N \ M is
expanded by drawing a rule from RA. There are
two ways to expand A ?M:
1. With probability (nz ? bA)/(nA + aA) we ex-
pand A to subtree z (a tree rooted at A with a
yield in W?), where nz is the number of times
the tree z was previously generated and nA is the
total number of subtrees (tokens) previously gen-
erated root being A. We denote by a the concen-
tration parameters and b the discount parameters,
both indexed by A ? M. We have aA ? 0 and
bA ? [0, 1].
2. With probability (aA + kAbA)/(nA + aA), A is
expanded as in a PCFG by a draw from ?A over
RA, where kA is the number of subtrees (types)
previously generated with root A.
For the expansion of adapted nonterminals, this
process can be explained using the Chinese restau-
rant process (CRP) metaphor: a ?customer? (cor-
responding to a partially generated tree) enters a
?restaurant? (corresponding to a nonterminal) and
selects a ?table? (corresponding to a subtree) to at-
tach to the partially generated tree. If she is the first
customer at the table, the PCFG ?G,?? produces the
new table?s associated ?dish? (a subtree).1
When adaptor grammars are defined using the
CRP, the PCFG G has to be non-recursive with re-
1We note that our construction deviates from the strict def-
inition of adaptor grammars (Johnson et al, 2006): (i) in our
construction, we assume (as prior work does in practice) that
the adaptors in A = ?G,M,a, b,?? follow the Pitman-Yor
(PY) process (Pitman and Yor, 1997), though in general other
stochastic processes might be used; and (ii) we place a sym-
metric Dirichlet over the parameters of the PCFG, ?, whereas
Johnson et al used a fixed PCFG for the definition (though they
experimented with a Dirichlet prior).
spect to the adapted nonterminals. More precisely,
for A ? N, denote by Reachable(G, A) all the non-
terminals that can be reached from A using a partial
derivation from G. Then we restrict G such that
for all A ? M, we have A /? Reachable(G, A).
Without this restriction, we might end up in a sit-
uation where the generative process is ill-defined:
in the CRP terminology, a customer could enter a
restaurant and select a table whose dish is still in
the process of being selected.2 In the more general
form of adaptor grammars with arbitrary adaptors,
the problem amounts to mutually dependent defini-
tions of distributions which rely on the others to be
defined. We return to this problem in ?3.1.
Inference The inference problem is to compute
the posterior distribution of parse trees given ob-
served sentences x = ?x1, . . . , xn?. Typically, in-
ference with adaptor grammars is done with Gibbs
sampling. Johnson et al (2006) use an embedded
Metropolis-Hastings sampler (Robert and Casella,
2005) inside a Gibbs sampler. The proposal distribu-
tion is a PCFG, resembling a tree substitution gram-
mar (TSG; Joshi, 2003). The sampler of Johnson et
al. is based on the representation of the PY process
as a distribution over partitions of integers. This rep-
resentation is not amenable to variational inference.
2.2 Stick-Breaking Representation
To develop a variational inference algorithm for
adaptor grammars, we require an alternative repre-
sentation of the model in ?2.1. The CRP-based def-
inition implicitly marginalizes out a random distri-
bution over trees. For variational inference, we con-
struct that distribution.
We first review the Dirichlet process and its stick-
breaking representation. The Dirichlet process de-
fines a distribution over distributions. Samples from
the Dirichlet process tend to deviate from a base
distribution depending on a concentration parame-
ter. Let G ? DP(G0, a) be a distribution sampled
from the Dirichlet process with base distribution G0
2Consider the simple grammar with rules { S ? S S, S ? a
}. Assume that a customer enters the restaurant for S. She sits
at a table, and selects a dish, a subtree, which starts with the rule
S ? S S. Perhaps the first child S is expanded by S ? a. For
the second child S, it is possible to re-enter the ?S restaurant?
and choose the first table, where the ?dish? subtree is still being
generated.
565
and concentration parameter a. The distribution G
is discrete, which means it puts positive mass on a
countable number of atoms drawn from G0. Re-
peated draws from G exhibit the ?clustering prop-
erty,? which means that they will be assigned to the
same value with positive probability. Thus, they ex-
hibit a partition structure. Marginalizing out G, the
distribution of that partition structure is given by a
CRP with parameter a (Pitman, 2002).
The stick-breaking process gives a constructive
definition of G (Sethuraman, 1994). With the stick-
breaking process (for the PY process), we first sam-
ple ?stick lengths? pi ? GEM(a, b) (in the case of
Dirichlet process, we have b = 0). The GEM par-
titions the interval [0, 1] into countably many seg-
ments. First, draw vi ? Beta(1 ? b, a + ib) for
i ? {1, . . .}. Then, define pii , vi
?i?1
j=1(1 ? vj).
In addition, we also sample infinitely many ?atoms?
independently zi ? G0. Define G as:
G(z) =
??
i=1 pii?(zi, z) (1)
where ?(zi, z) is 1 if zi = z and 0 otherwise. This
random variable is drawn from a Pitman-Yor pro-
cess. Notice the discreteness of G is laid bare in the
stick-breaking construction.
With the stick-breaking representation in hand,
we turn to a constructive definition of the distri-
bution over trees given by an adaptor grammar.
Let A1, . . . , AK be an enumeration of the nonter-
minals in M which satisfies: i ? j ? Aj /?
Reachable(G, Ai). (That this exists follows from
the assumption about the lack of recursiveness of
adapted nonterminals.) Let Yield(z) be the yield of
a tree derivation z. The process that generates ob-
served sentences x = ?x1, . . . , xn? from the adaptor
grammarA = ?G,M,a, b,?? is as follows:
1. For each A ? N, draw ?A ? Dir(?A).
2. For A from A1 to AK , define GA as follows:
(a) Draw piA | aA, bA ? GEM(aA, bA).
(b) For i ? {1, . . .}, grow a tree zA,i as follows:
i. Draw A? B1 . . . Bn fromRA.
ii. zA,i = A
HHH

B1 ? ? ? Bn
iii. While Yield(zA,i) has nonterminals:
A. Choose an unexpanded nonterminal B
from yield of zA,i.
B. If B ? M, expand B according to GB
(defined on previous iterations of step 2).
C. If B ? N \M, expand B with a rule from
RB according to Mult(?B).
(c) For i ? {1, . . .}, define GA(zA,i) = piA,i
3. For i ? {1, . . . , n} draw zi as follows:
(a) If S ?M, draw zi | GS ? GS .
(b) If S /? M, draw zi as in 2(b) (omitted for
space).
4. Set xi = Yield(zi) for i ? {1, . . . , n}.
Here, there are four collections of hidden variables:
the PCFG multinomials ? = {?A | A ? N}, the
stick length proportions v = {vA | A ? M} where
vA = ?vA,1, vA,2, . . .?, the adapted nonterminals?
subtrees zA = {zA,i | A ? M; i ? {1, . . .}} and
the derivations z1:n = z1, . . . , zn. The symbol z
refers to the collection of {zA | A ? M}, and z1:n
refers to the derivations of the data x.
Note that the distribution in 2(c) is defined with
the GEM distribution, as mentioned earlier. It is a
sample from the Pitman-Yor process (or the Dirich-
let process), which is later used in 3(a) to sample
trees for an adapted non-terminal.
3 Variational Inference
Variational inference is a deterministic alternative
to MCMC, which casts posterior inference as an
optimization problem (Jordan et al, 1999; Wain-
wright and Jordan, 2008). The optimized function
is a bound on the marginal likelihood of the obser-
vations, which is expressed in terms of a so-called
?variational distribution? over the hidden variables.
When the bound is tightened, that distribution is
close to the posterior of interest. Variational meth-
ods tend to converge faster than MCMC, and can be
more easily parallelized over multiple processors in
a framework such as MapReduce (Dean and Ghe-
mawat, 2004).
The variational bound on the likelihood of the
data is:
log p(x | a,?) ? H(q) +
?
A?M
Eq[log p(vA | aA)]
+
?
A?M
Eq[log p(?A | ?A)]
+
?
A?M
Eq[log p(zA | v,?)] + Eq[log p(z | vA)]
566
Expectations are taken with respect to the variational
distribution q(v,?, z) and H(q) is its entropy.
Before tightening the bound, we define the func-
tional form of the variational distribution. We use
the mean-field distribution in which all of the hid-
den variables are independent and governed by in-
dividual variational parameters. (Note that in the
true posterior, the hidden variables are highly cou-
pled.) To account for the infinite collection of ran-
dom variables, for which we cannot define a varia-
tional distribution, we use the truncated stick distri-
bution (Blei and Jordan, 2005). Hence, we assume
that, for all A ? M, there is some value NA such
that q(vA,NA = 1) = 1. The assigned probability to
parse trees in the stick will be 0 for i > NA, so we
can ignore zA,i for i > NA. This leads to a factor-
ized variational distribution:
q(v,?, z) = (2)
?
A?M
(
q(?A)
NA?
i=1
q(vA,i)? q(zA,i)
)
?
n?
i=1
q(zi)
It is natural to define the variational distributions
over ? and v to be Dirichlet distributions with pa-
rameters ?A and Beta distributions with parameters
?A,i, respectively. The two distributions over trees,
q(zA,i) and q(zi), are more problematic. For ex-
ample, with q(zi | ?), we need to take into ac-
count different subtrees that could be generated by
the model and use them with the proper probabilities
in the variational distribution q(zi | ?). We follow
and extend the idea from Johnson et al (2006) and
use grammatons for these distributions. Gramma-
tons are ?mini-grammars,? inspired by the grammar
G.
For two strings in s, t ? W?, we use ?t ? s?
to mean that t is a substring of s. In that case, a
grammaton is defined as follows:
Definition 1. LetA = ?G,M,a, b,?? be an adap-
tor grammar with G = ?W,N,R, S?. Let s be a fi-
nite string over the alphabet ofG andA ? N. Let U
be the set of nonterminals U , Reachable(G, A) ?
(N \M). The grammaton G(A, s) is the context-
free grammar with the start symbol A and the rules
RA?
(
?
B?U
RB
)
?
?
A?B1...Bn?RA
?
i?{i|Bi?M}
{Bi ?
t | t ? s}.
Using a grammaton, we define the distributions
q(zA,i | ?A) and q(zi | ?). This requires a pre-
processing step (described in detail in ?3.3) that de-
fines, for each A ? M, a list of strings sA =
?sA,1, . . . , sA,NA?. Then, for q(zA,i | ?A) we use
the grammaton G(A, sA,i) and for q(zi | ?) we
use the grammaton G(A, xi) where xi is the ith
observed sentence. We parametrize the grammaton
with weights ?A (or ?) for each rule in the gramma-
ton. This makes the variational distributions over the
trees for strings s (and trees for x) globally normal-
ized weighted grammars. Choosing such distribu-
tions is motivated by their ability to make the varia-
tional bound tight (similar to Cohen et al, 2008, and
Cohen and Smith, 2009). In practice we do not have
to use rewrite rules for all strings t ? s in the gram-
maton. It suffices to add rewrite rules only for the
strings t = sA,i that have some grammaton attached
to them,G(A, sA,i).
The variational distribution above yields a vari-
ational inference algorithm for approximating the
posterior by estimating ?A,i, ?A, ?A and ? it-
eratively, given a fixed set of hyperparameters
a, b and ?. Let r be a PCFG rule. Let
f?(r, sB,k) = Eq(zk|?B,k)[f(r; zk)], where f(r; zk)
counts the number of times that rule r is applied in
the derivation zk. Let A ? ? denote a rule from
G. The quantity f?(r, sB,k) is computed using the
inside-outside (IO) algorithm. Fig. 1 gives the vari-
ational inference updates.
Variational EM We use variational EM to fit the
hyperparameters. Variational EM is an EM algo-
rithm where the E step is replaced by variational in-
ference (Fig. 1). The M-step optimizes the hyperpa-
rameters (a, b and ?) with respect to expected suffi-
cient statistics under the variational distribution. We
use Newton-Raphson for each (Boyd and Vanden-
berghe, 2004); Fig. 2 gives the objectives.
3.1 Note about Recursive Grammars
With recursive grammars, the stick-breaking pro-
cess representation gives probability mass to events
which are ill-defined. In step 2(iii)(c) of the stick-
breaking representation, we assign nonzero proba-
bility to an event in which we choose to expand the
current tree using a subtree with the same index that
we are currently still expanding (see footnote 2). In
567
short, with recursive grammars, we can get ?loops?
inside the trees.
We would still like to use recursion in the cases
which are not ill-defined. In the case of recur-
sive grammars, there is no problem with the stick-
breaking representation and the order by which we
enumerate the nonterminals. This is true because the
stick-breaking process separates allocating the prob-
abilities for each index in the stick and allocating the
atoms for each index in the stick.
Our variational distributions give probability 0 to
any event which is ill-defined in the sense men-
tioned above. Optimizing the variational bound in
this case is equivalent to optimizing the same vari-
ational bound with a model p? that (i) starts with p,
(ii) assigns probability 0 to ill-defined events, and
(iii) renormalizes:
Proposition 2. Let p(x, z) be a probability distri-
bution, where z ? Z, and let S ? Z. Let Q = {q |
q(z) = 0, ?z ? S}, a set of distributions. Then:
argmax
q?Q
Eq[log p(x, z)] = argmax
q
Eq[log p?(x, z)]
where p?(x, z) is a probability distribution defined
as p?(x, z) = p(x, z)/
?
z?S p(x, z) for z ? S and
0 otherwise.
For this reason, our variational approximation al-
lows the use of recursive grammars. The use of re-
cursive grammars with MCMC methods is problem-
atic, since it has no corresponding probabilistic in-
terpretation, enabled by zeroing events that are ill-
defined in the variational distribution. There is no
underlying model such as p?, and thus the inference
algorithm is invalid.
3.2 Time Complexity
The algorithm in Johnson et al (2006) works by
sampling from a PCFG containing rewrite rules that
rewrite to a whole tree fragment. This requires
a procedure that uses the inside-outside algorithm.
Despite the grammar being bigger (because of the
rewrite rules to a string), the asymptotic complexity
of the IO algorithm stays O(|N|2|xi|3 + |N|3|xi|2)
where |xi| is the length of the ith sentence.3
3This analysis is true for CNF grammars augmented with
rules rewriting to a whole string, like those used in our study.
?1A,i = 1? bA +
?
B?M
?NB
k=1 f?(A? sA,i, sB,k)
?2A,i = aA + ibA
+
?i?1
j=1
?
B?M
?NB
k=1 f?(A? sA,j , sB,k)
?A,A?? =
?
B?M
?NB
k=1 f?(A? ?, sB,k)
?A,A?sA,i = ?(?
1
A,i)??(?
1
A,i + ?
2
A,i)
+
?i?1
j=1
(
?(?2A,i)??(?
1
A,i + ?
2
A,i)
)
?A,A?? = ?(?A,A??)??
(?
? ?A,A??
)
Figure 1: Updates for variational inference with adaptor
grammars. ? is the digamma function.
Our algorithm requires running the IO algorithm
for each yield in the variational distribution, for each
nonterminal, and for each sentence. However, IO
runs with much smaller grammars coming from the
grammatons. The cost of running the IO algorithm
on the yields in the sticks for A ? M can be taken
into account parsing a string that appears in the cor-
pus with the full grammars. This leads to an asymp-
totic complexity of O(|N|2|xi|3 + |N|3|xi|2) for the
ith sentence in the corpus each iteration.
Asymptotically, both sampling and variational
EM behave the same. However, there are different
constants that hide in these asymptotic runtimes: the
number of iterations that the algorithm takes to con-
verge (for which variational EM generally has an ad-
vantage over sampling) and the number of additional
rewrite rules that rewrite to a string representing a
tree (for which MCMC has a relative advantage, be-
cause it does not use a fixed set of strings; instead,
the size of the grammars it uses grow as sampling
proceeds). In ?4, we see that variational EM and
sampling methods are similar in the time it takes to
complete because of a trade-off between these two
constants. Simple parallelization, however, which
is possible only with variational inference, provides
significant speed-ups.4
3.3 Heuristics for Variational Inference
For the variational approximation from ?3, we need
to decide on a set of strings, sA,i (for A ? M and
i ? {1, . . . , NA}) to define the grammatons in the
4Newman et al (2009) show how to parallelize sampling al-
gorithms, but in general, parallelizing these algorithms is more
complicated than parallelizing variational algorithms and re-
quires further approximation.
568
max?A log ?(|RA|?A)? |RA| log ?(?A) + (?A ? 1)
(?
A???RA
?(?A??)??
(?
A???RA
?A??
))
maxaA
?NA
i=1 aA
(
?(?2A,i)??(?
1
A,i + ?
2
A,i)
)
+ log ?(aA + 1 + ibA)? log ?(ibA + aA)
maxbA
?NA
i=1 ibA
(
?(?2A,i)??(?
1
A,i + ?
2
A,i)
)
+ log ?(aA + 1 + ibA)? log ?(1? bA)? log ?(ibA + aA)
Figure 2: Variational M-step updates. ? is the gamma function.
nonparametric stick. Any set of strings will give
a valid approximation, but to make the variational
approximation as accurate as possible, we require
that: (i) the strings in the set must be likely to be
generated using the adaptor grammar as constituents
headed by the relevant nonterminal, and (ii) strings
that are more likely to be generated should be asso-
ciated with a lower index in the stick. The reason for
the second requirement is the exponential decay of
coefficients as the index increases.
We show that a simple heuristic leads to an order
over the strings generated by the adaptor grammars
that yields an accurate variational estimation. We
begin with a weighted context-free grammar Gheur
that has the same rules as in G, only the weight for
all of its rules is 1. We then compute the quantity:
c(A, s) =
1
n
(
n?
i=1
EGheur [fi(z;A, s)]
)
? ? log |s|
(3)
where fi(z;A, s) is a function computing the count
of constituents headed by A with yield s in the tree
z for the sentence xi. This quantity can be com-
puted by using the IO algorithm onGheur. The term
? log |s| is subtracted to avoid preference for shorter
constituents, similar to Mochihashi et al (2009).
While computing c(A, s) using the IO algorithm,
we sort the set of all substrings of s according to
their expected counts (aggregated over all strings s).
Then, we use the top NA strings in the sorted list for
the grammatons of A.5
3.4 Decoding
The variational inference algorithm gives a distribu-
tions over parameters and hidden structures (through
the grammatons). We experiment with two com-
monly used decoding methods: Viterbi decoding
5The requirement to select NA in advance is strict. We ex-
perimented with dynamic expansions of the stick, in the spirit
of Kurihara et al (2006) and Wang and Blei (2009), but we did
not achieve better performance and it had an adverse effect on
runtime. For completeness, we give these results in ?4.
and minimum Bayes risk decoding (MBR; Good-
man, 1996).
To parse a string with Viterbi (or MBR) decoding,
we find the tree with highest score for the gramma-
ton which is attached to that string. For all rules
which rewrite to strings in the resulting tree, we
again perform Viterbi (or MBR) decoding recur-
sively using other grammatons.
4 Experiments
We describe experiments with variational inference
for adaptor grammars for word segmentation and de-
pendency grammar induction.
4.1 Word Segmentation
We follow the experimental setting of Johnson and
Goldwater (2009), who present state-of-the-art re-
sults for inference with adaptor grammars using
Gibbs sampling on a segmentation problem. We
use the standard Brent corpus (Brent and Cartwright,
1996), which includes 9,790 unsegmented phone-
mic representations of utterances of child-directed
speech from the Bernstein-Ratner (1987) corpus.
Johnson and Goldwater (2009) test three gram-
mars for this segmentation task. The first grammar
is a character unigram grammar (GUnigram). The
second grammar is a grammar that takes into con-
sideration collocations (GColloc) which includes the
rules { Sentence? Colloc, Sentence? Colloc Sen-
tence, Colloc ? Word+, Word ? Char+ }. The
third grammar incorporates more prior knowledge
about the syllabic structure of English (GSyllable).
GUnigram and GSyllable can be found in Johnson
and Goldwater (2009). Once an utterance is parsed,
Word constituents denote segments.
The value of ? (penalty term for string length) had
little effect on our results and was fixed at ? = ?0.2.
When NA (number of strings used in the variational
distributions) is fixed, we use NA = 15,000. We re-
port results using Viterbi and MBR decoding. John-
son and Goldwater (2009) experimented with two
569
this paper J&G 2009
grammar model Vit. MBR SA MM
GU
ni
gr
am
Dir 0.49 0.84 0.57 0.54
PY 0.49 0.84 0.81 0.75
PY+inc 0.42 0.59 - -
GC
ol
lo
c Dir 0.40 0.86 0.75 0.72
PY 0.40 0.86 0.83 0.86
PY+inc 0.43 0.60 - -
G S
yl
la
bl
e Dir 0.77 0.83 0.84 0.84
PY 0.77 0.83 0.89 0.88
PY+inc 0.75 0.76 - -
Table 1: F1 performance for word segmentation on the
Brent corpus. Dir. stands for Dirichlet Process adaptor
(b = 0), PY stands for Pitman-Yor adaptor (b optimized),
and PY+inc. stands for Pitman-Yor with iteratively in-
creasing NA for A ? M (see footnote 5). J&G 2009 are
the results adapted from Johnson and Goldwater (2009);
SA is sample average decoding, and MM is maximum
marginal decoding.
Truncated stick length
F1 sc
ore
65
70
75
80
l
l
l
l
l l l
l l l l l l
l l
l
l
l
l
l
l l
l l
l l l l l l
2000 4000 6000 8000 10000 12000 14000
Figure 3: F1 performance of GUnigram as influenced by
the length of the stick, NWord.
decoding methods, sample average (SA) and maxi-
mal marginal decoding (MM), which are closely re-
lated to Viterbi and MBR, respectively. With MM,
we marginalize the tree structure, rather than the
word segmentation induced, similar to MBR decod-
ing. With SA, we compute the probability of a whole
tree, by averaging its count in the samples, an ap-
proximation to finding the tree with highest proba-
bility, like Viterbi.
Table 1 gives the results for our experiments. No-
tice that the results for the Pitman-Yor process and
the Dirichlet process are similar. When inspecting
the learned parameters, we noticed that the discount
parameters (b) learned by the variational inference
algorithm for the Pitman-Yor process are very close
to 0. In this case, the Pitman-Yor process is reduced
to the Dirichlet process.
Similar to Johnson and Goldwater?s comparisons,
we see superior performance when using minimum
Bayes risk over Viterbi decoding. Further notice that
the variational inference algorithm obtains signifi-
cantly superior performance for simpler grammars
than Johnson et al, while performance using the syl-
lable grammar is lower. The results also suggest that
it is better to decide ahead on the set of strings avail-
able in the sticks, instead of working gradually and
increase the size of the sticks as described in foot-
note 5. We believe that the reason is that the varia-
tional inference algorithm settles in a trajectory that
uses fewer strings, then fails to exploit the strings
that are added to the stick later. Given that select-
ing NA in advance is advantageous, we may inquire
if choosing NA to be too large can lead to degraded
performance, because of fragmention of the gram-
mar. Fig. 3 suggests it is not the case, and per-
formance stays steady after NA reaches a certain
value.
One of the advantages of variational approxima-
tion over sampling methods is the ability to run
for fewer iterations. For example, with GUnigram
convergence typically takes 40 iterations with vari-
ational inference, while Johnson and Goldwater
(2009) ran their sampler for 2,000 iterations, for
which 1,000 were for burning in. The inside-outside
algorithm dominates the iteration?s runtime, both
for sampling and variational EM. Each iteration
with sampling, however, takes less time, despite the
asymptotic analysis in ?3.2, because of different im-
plementations and the different number of rules that
rewrite to a string. We now give a comparison of
clock time for GUnigram for variational inference
and sampling as described in Johnson and Goldwa-
ter (2009).6 Replicating the experiment in Johnson
and Goldwater (first row in Table 1) took 2 hours
and 11 minutes. With the variational approximation,
we had the following: (i) the preprocessing (?3.3)
step took 114 seconds; (ii) each iteration took ap-
proximately 204 seconds, with convergence after 40
iterations, leading to 8,160 seconds of pure varia-
6We used the code and data available at http://www.
cog.brown.edu/?mj/Software.htm. The machine
used for this comparison is a 64-bit machine with 2.6GHz CPU,
4MB of cache memory and 8GB of RAM.
570
tional EM processing; (iii) parsing took another 952
seconds. The total time is 2 hours and 34 minutes.
At first glance it seems that variational inference
is slower than MCMC sampling. However, note that
the cost of the grammar preprocessing step is amor-
tized over all experiments with the specific gram-
mar, and the E-step with variational inference can be
parallelized, while sampling requires an update of a
global set of parameters after each tree update. We
ran our algorithm on a cluster of 20 1.86GHz CPUs
and achieved a significant speed-up: preprocessing
took 34 seconds, each variational EM iteration took
43 seconds and parsing took 208 seconds. The total
time was 47 minutes, which is 2.8 times faster than
sampling.
4.2 Dependency Grammar Induction
We conclude our experiments with preliminary re-
sults for unsupervised syntax learning. This is a new
application of adaptor grammars, which have so far
been used in segmentation (Johnson and Goldwater,
2009) and named entity recognition (Elsner et al,
2009).
The grammar we use is the dependency model
with valence (DMV Klein and Manning, 2004) rep-
resented as a probabilistic context-free grammar,
GDMV (Smith, 2006). We note that GDMV is re-
cursive; this is not a problem (?3.1).
We used part-of-speech sequences from the Wall
Street Journal Penn Treebank (Marcus et al, 1993),
stripped of words and punctuation. We follow stan-
dard parsing conventions and train on sections 2?
21 and test on section 23 (while using sentences of
length 10 or less). Because of the unsupervised na-
ture of the problem, we report results on the training
set, in addition to the test set.
The nonterminals that we adapted correspond to
nonterminals that define noun constituents. We then
use the preprocessing step defined in ?3.3 with a uni-
form grammar and take the top 3,000 strings for each
nonterminal of a noun constituent.
The results are in Table 4.2. We report attach-
ment accuracy, the fraction of parent-child relation-
ships that the algorithm classified correctly. Notice
that the results are not very different for Viterbi and
MBR decoding, unlike the case with word segmen-
tation. It seems like the DMV grammar, applied
to this task, is more robust to changes in decod-
model Vit. MBR
tr
ai
n
non-Bayesian 48.2 48.3
Dirichlet prior 48.3 48.6
Adaptor grammar 54.0 ?53.7
te
st
non-Bayesian 45.8 46.1
Dirichlet prior 45.9 46.1
Adaptor grammar 48.3 50.2
Table 2: Attachment accuracy for different models for
dependency grammar induction. Bold marks best overall
accuracy per evaluation set, and ? marks figures that are
not significantly worse (binomial sign test, p < 0.05).
ing mechanism. Adaptor grammars improve perfor-
mance over classic EM and variational EM with a
Dirichlet prior significantly.
We note that adaptor grammars are not limited to
a selection of a Dirichlet distribution as a prior for
the grammar rules. Our variational inference algo-
rithm, for example, can be extended to use the lo-
gistic normal prior instead of the Dirichlet, shown
successful by Cohen and Smith (2009).7
5 Conclusion
We described a variational inference algorithm for
adaptor grammars based on a stick-breaking process
representation, which solves a problem with adaptor
grammars and recursive PCFGs. We tested it for a
segmentation task, and showed results which are ei-
ther comparable or an imporvement of state of the
art. We showed that significant speed-ups can be
obtained using parallelization of the algorithm. We
also tested the algorithm on a novel task for adap-
tor grammars, dependency grammar induction. We
showed that an improvement can be obtained using
adaptor grammars over non-Bayesian and paramet-
ric baselines.
Acknowledgments
The authors would like to thank the anonymous review-
ers, Jordan Boyd-Graber, Reza Haffari, Mark Johnson,
and Chong Wang for their useful feedback and com-
ments. This work was supported by the following grants:
ONR 175-6343 and NSF CAREER 0745520 to Blei; NSF
IIS-0836431 and IIS-0915187 to Smith.
7The performance of Cohen and Smith (2009), like the per-
formance of Headden et al (2009), is greater than what we re-
port, but those developments are orthogonal to the contributions
of this paper.
571
References
C. Antoniak. 1974. Mixtures of Dirichlet processes with
applications to Bayesian nonparametric problems. The
Annals of Statistics, 2(6):1152?1174.
N. Bernstein-Ratner. 1987. The phonology of parent
child speech. Children?s Language, 6.
D. Blei and M. Jordan. 2005. Variational inference for
Dirichlet process mixtures. Journal of Bayesian Anal-
ysis, 1(1):121?144.
S. Boyd and L. Vandenberghe. 2004. Convex Optimiza-
tion. Cambridge Press.
M. Brent and T. Cartwright. 1996. Distributional reg-
ularity and phonotactic constraints are useful for seg-
mentation. Cognition, 6:93?125.
S. B. Cohen and N. A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in unsu-
pervised grammar induction. In Proc. of NAACL-HLT.
S. B. Cohen, K. Gimpel, and N. A. Smith. 2008. Logistic
normal priors for unsupervised probabilistic grammar
induction. In NIPS.
J. Dean and S. Ghemawat. 2004. MapReduce: Sim-
plified data processing on large clusters. In Proc. of
OSDI.
M. Elsner, E. Charniak, and M. Johnson. 2009. Struc-
tured generative models for unsupervised named-
entity clustering. In Proc. of NAACL-HLT.
S. Goldwater and T. L. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Proc. of ACL.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proc. of ACL.
W. P. Headden, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with
richer contexts and smoothing. In Proc. of NAACL-
HLT.
M. Johnson and S. Goldwater. 2009. Improving nonpa-
rameteric Bayesian inference experiments on unsuper-
vised word segmentation with adaptor grammars. In
Proc. of NAACL-HLT.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2006.
Adaptor grammars: A framework for specifying com-
positional nonparameteric Bayesian models. In NIPS.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proc. of NAACL.
M. Johnson. 2008a. Unsupervised word segmentation
for Sesotho using adaptor grammars. In Proceedings
of the Tenth Meeting of ACL Special Interest Group on
Computational Morphology and Phonology.
M. Johnson. 2008b. Using adaptor grammars to identify
synergies in the unsupervised acquisition of linguistic
structure. In Proc. of ACL.
M. I. Jordan, Z. Ghahramani, T. S. Jaakola, and L. K.
Saul. 1999. An introduction to variational methods
for graphical models. Machine Learning, 37(2):183?
233.
A. Joshi. 2003. Tree adjoining grammars. In R. Mitkov,
editor, The Oxford Handbook of Computational Lin-
guistics, pages 483?501. Oxford University Press.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In Proc. of ACL.
K. Kurihara, M. Welling, and N. A. Vlassis. 2006. Ac-
celerated variational Dirichlet process mixtures. In
NIPS.
P. Liang, S. Petrov, M. Jordan, and D. Klein. 2007. The
infinite PCFG using hierarchical Dirichlet processes.
In Proc. of EMNLP.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19:313?330.
D. Mochihashi, T. Yamada, and N. Ueda. 2009.
Bayesian unsupervised word segmentation with nested
Pitman-Yor language modeling. In Proc. of ACL.
D. Newman, A. Asuncion, P. Smyth, and M. Welling.
2009. Distributed algorithms for topic models. Jour-
nal of Machine Learning Research, 10:1801?1828.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. Annals of Probability, 25(2):855?900.
J. Pitman. 2002. Combinatorial Stochastic Processes.
Lecture Notes for St. Flour Summer School. Springer-
Verlag, New York, NY.
C. P. Robert and G. Casella. 2005. Monte Carlo Statisti-
cal Methods. Springer.
J. Sethuraman. 1994. A constructive definition of Dirich-
let priors. Statistica Sinica, 4:639?650.
N. A. Smith. 2006. Novel Estimation Methods for Unsu-
pervised Discovery of Latent Structure in Natural Lan-
guage Text. Ph.D. thesis, Johns Hopkins University.
K. Toutanova and M. Johnson. 2007. A Bayesian LDA-
based model for semi-supervised part-of-speech tag-
ging. In Proc. of NIPS.
M. J. Wainwright and M. I. Jordan. 2008. Graphi-
cal models, exponential families, and variational infer-
ence. Foundations and Trends in Machine Learning,
1:1?305.
C. Wang and D. M. Blei. 2009. Variational inference for
the nested Chinese restaurant process. In NIPS.
572
