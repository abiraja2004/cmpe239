The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 295?301,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
 
 
Helping Our Own: NTHU NLPLAB System Description 
  
Jian-Cheng Wu+, Joseph Z. Chang*, Yi-Chun Chen+, Shih-Ting Huang+, Mei-Hua Chen*, 
Jason S. Chang+ 
  * Institute of Information Systems and Applications, NTHU, HsinChu, Taiwan, R.O.C. 30013 
+ Department of Computer Science, NTHU, HsinChu, Taiwan, R.O.C. 30013 
{wujc86, bizkit.tw, pieyaaa, koromiko1104, chen.meihua, 
jason.jschang}@gmail.com 
  
  
Abstract 
Grammatical error correction has been an active 
research area in the field of Natural Language 
Processing. In this paper, we integrated four 
distinct learning-based modules to correct 
determiner and preposition errors in leaners? 
writing. Each module focuses on a particular 
type of error. Our modules were tested in 
well-formed data and learners? writing. The 
results show that our system achieves high 
recall while preserves satisfactory precision. 
1. Introduction 
Researchers have demonstrated that prepositions 
and determiners are the two most frequent error 
types for language learners (Leacock et al 2010). 
According to Swan and Smith (2001), preposition 
errors might result from L1 interference. Chen and 
Lin (2011) also reveal that prepositions are the 
most perplexing problem for Chinese-speaking 
EFL learners mainly because there are no clear 
preposition counterparts in Chinese for learners to 
refer to. On the other hand, Swan and Smith (2001) 
predict that the possibility of determiner errors 
depends on learners? native language. The 
Cambridge Learners Corpus illustrates that 
learners of Chinese, Japanese, Korean, and Russian 
might have a poor command of determiners.  
In view of the fact that a large number of 
grammatical errors appear in non-native speakers? 
writing, more and more research has been directed 
towards the automated detection and correction of 
such errors to help improve the quality of that 
writing (Dale and Kilgarriff, 2010). In recent years, 
preposition error detection and correction has 
especially been an area of increasingly active 
research (Leacock et al 2010). The HOO 2012 
shared task also focuses on error detection and 
correction in the use of prepositions and 
determiners (Dale et al, 2012).  
Many studies have been done at correcting 
errors using hybrid modules: implementing distinct 
modules to correct errors of different types. In 
other word, instead of using a general module to 
correct any kind of errors, using different modules 
to deal with different error types seems to be more 
effective and promising. In this paper, we propose 
four distinct modules to deal with four kinds of 
determiner and preposition errors (inserting 
missing determiner, replacing erroneous 
determiner, inserting missing preposition, and 
replacing erroneous prepositions). Four 
learning-based approaches are used to detect and 
correct the errors of prepositions and determiners.   
In this paper, we describe our methods in the 
next section. Section 3 reports the evaluation 
results. Then we conclude this paper in Section 4.  
2. System Description 
2.1 Overview 
In this sub-section, we give a general view of our 
system. Figure 1 shows the architecture of the 
integrated error detection and error correction 
system. The input of the system is a sentence in a 
learner?s writing. First, the data is pre-processed 
using the GeniaTagger tool (Tsuruoka et al, 2005), 
which provides the base forms, part-of-speech tags, 
chunk tags and named entity tags. The tag result of 
295
  
the sample sentence ?This virus affects the defense 
system.? is shown in Table 1. The determiner error 
detection module then directly inserts the missing 
determiners and deletes the unnecessary 
determiners. Meanwhile, the error determiners are 
replaced with predicted answers by the determiner 
error correction module. After finishing the 
determiner error correction, the preposition error 
detection and correction module detects and 
corrects the preposition errors of the modified 
input sentence.  
In the following subsections, we first introduce 
the training and testing of the determiner error 
detection and correction modules (Section 3.2). 
Then in section 3.3 we focus on the training and 
testing of the preposition error detection and 
correction modules. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. System Architecture (Run-Time) 
 
 
Word Base form POS Chunk NE 
This This DT B-NP O 
virus virus NN I-NP O 
affects affect VBZ B-VP O 
the the DT B-NP O 
defence defence NN I-NP O 
system system NN I-NP O 
. . . O O 
Table 1. The tag result of sample sentence. 
2.2 Determiners 
In this section, we investigate the performance of 
two maximum entropy classifiers (Ratnaparkhi, 
1997), one for determining whether a noun phrase 
has a determiner or not and the other for selecting 
the appropriate determiner if one is needed.  
 From the British National Corpus (BNC), we 
extract 22,552,979 noun phrases (NPs). For 
determining which features are useful for this task, 
all NPs are divided into two sets, 20 million cases 
as a training set and the others as a validation set.  
For the classifier (named the DetClassifier 
hereafter) trained for predicting whether a NP has a 
determiner or not, the label set contains two labels: 
?Zero? and ?DET.? On the other hand, for the 
classifier (named the SelClassifier hereafter) which 
predicts appropriate determiners, the label set 
contains 9 labels: the, a, an, my, your, our, one, 
this, their. (In the training data, there are 7,249,218 
cases with those labels.) 
Both of the classifiers use contextual and 
syntactic information as features to predict the 
labels. The features include single features such as 
the headword of the NP, the part of speech (PoS) 
of the headword, the words and  PoSs in the 
chunks before or after the NP (pre-NP, post-NP), 
and all words and PoSs in the NP (excluding the 
determiner if there was one), etc. We also combine 
the single features to form more specific features 
for better performance. 
At run time, the given data are also tagged and 
all features for each NP in the data are extracted 
for classification. For testing, all determiners at the 
beginning of the NPs are ignored if they exist. At 
first, the DetClassifier is used to determine 
whether a NP needs a determiner or not. If the 
classifier predicts that the NP should not have a 
determiner but it does, there is an ?UD? 
(Unnecessary determiner) type mistake. In contrast, 
Preposition Error 
Choice 
Determiner Error 
Detection 
Determiner 
Choice 
Preposition Error 
Detection 
Input 
sentence 
Tagger & Parser 
Determiner 
Preposition 
Output 
296
  
if the classifier predicts that the NP should have a 
determiner but it does not, there is a ?MD? type 
mistake. For both ?MD? (Missing determiner) and 
?RD? (Replace determiner) mistake types, we 
would use the SelClassifier to predict which 
determiner is more appropriate for the given NP.  
2.3 Prepositions 
2.3.1 Preposition Error Detection 
In solving other problems in natural language 
processing, supervised training methods suffers 
from the difficulty of acquiring manually labeled 
data. This may not be the case with grammatical 
language error correction. Although high quality 
error learner?s corpora are not currently available 
to the public to provide negative cases, any 
ordinary corpus can used as positive cases at 
training time. 
In our method, we use an ordinary corpus to 
train a Conditional Random Field (CRF) tagger to 
identify the presence of a targeted lexical category. 
The input of the tagger is a sentence with all words 
in the targeting lexical category removed. The 
tagger will tag every word with a positive or 
negative tag, predicting the presence of a word in 
the targeted lexical category. In this paper, we 
choose the top 13 most frequent prepositions: of, to, 
in, for, on, with, as, at, by, from, about, like, since. 
Conditional Random Field 
The sequence labeling is the task of assigning 
labels from a finite set of categories sequentially to 
a set of observation sequences. This problem is 
encountered not only in the field of computational 
linguistics, but also many others, including 
bioinformatics, speech recognition, and pattern 
recognition. 
Traditionally sequence labeling problems are 
solved using the Hidden Markov Model (HMM). 
HMM is a directed graph model in which every 
outcome is conditioned on the corresponding 
observation node and only the previous outcomes. 
Conditional Random Field (CRF) is considered 
the state-of-the-art sequence labeling algorithm. 
One of the major differences of CRF is that it is 
modeled as a undirected graph. CRF also obeys the 
Markov property, with respect to the undirected 
graph, every outcome is conditioned on its 
neighboring outcomes and potentially the entire 
observation sequence. 
 
 
Figure 2. Simplified view of HMM and CRF 
 
Supervised Training 
Obtaining labeled training data is relatively easy 
for this task, that is, it requires no human labeler. 
For this task, we will use this method to target the 
lexical category preposition. To produce training 
data, we simply use an ordinary English corpus 
and use the presence of prepositions as the 
outcome, and remove all prepositions. For example, 
the sentence  
 
?Miss Hardbroom ?s eyes bored into Mildred 
like    a    laser-beam    the    moment    
they    came into view .? 
 
will produce  
 
?Miss _Hardbroom _?s _eyes _bored +Mildred 
_like _a _laser-beam _the _moment _they 
_came  +view .?  
 
where the underscores indicate no preposition 
presence and the plus signs indicate otherwise. 
Combined with additional features described in 
following sections, we use the CRF model to train 
a preposition presence detection tagger. Features 
additional to the words in the sentence are their 
corresponding lemmas, part-of-speech tags, upper 
or lower case, and word suffix. 
At runtime, we first remove all prepositional 
words in the user input sentence, generate 
additional features, and use the trained tagger to 
predict the presence of prepositions in the altered 
sentence. By comparing the tagged result with the 
original sentence, the system can output insertion 
and/or deletion of preposition suggestions. 
The process of generating features is identical to 
producing the training set. To generate 
297
  
part-of-speech tag features at runtime, one simple 
approach is to use an ordinary POS tagger to 
generate POS tags to the tokens in the altered 
sentences, i.e. English sentences without any 
prepositions. A more sophisticated approach is to 
train a specialized POS tagger to tag English 
sentences with their prepositions removed. A 
state-of-the-art part-of-speech tagger can achieve 
around 95% precision. In our implementation, we 
find that using an ordinary POS tagger to tag 
altered sentences yield near 94% precision, 
whereas a specialized POS tagger performed 
around 1% higher precision. 
We used a small portion of the British National 
Corpus (BNC) to train and evaluate our tagger (1M 
and 10M tokens, i.e. words and punctuation marks). 
The British National Corpus contains over 100 
million words of both written (90%) and spoken 
(10%) British English. The written part of the BNC 
is sampled from a wide variety of sources, 
including newspapers, journals, academic books, 
fictions, letter, school and university essays. A 
separate portion of the BNC is selected to evaluate 
the performance of the taggers. The test set 
contains 322,997 tokens (31,916 sentences). 
 
2.3.2 Preposition Error Correction 
Recently, the problem of preposition error 
correction has been viewed as a word sense 
disambiguation problem and all prepositions are 
considered as candidates of the intended senses. In 
previous studies, well-formed corpora and learner 
corpora are both used in training the classifiers. 
However, due to the limited size of learner corpora, 
it is difficult to use the learner corpora to train a 
classifier. A more feasible approach is to use a 
large well-formed corpus to train a model in 
choosing prepositions. Similar to the determiner 
error correction, we choose the maximum entropy 
model as our classifier to choose appropriate 
prepositions underlying certain contexts. In order 
to cover a large variety of genres in learners? 
writing, we use a balanced well-formed corpus, the 
BNC, to train a maximum entropy model.  
Our context features include four feature 
categories which are introduced as follows.  
? Word feature (f1): Word features include a 
window of five content words to the left and 
right with their positions. 
? Head feature (f2): We select two head words 
in the left and right of prepositions with their 
relative orders as head features. For example, 
in Table 2, we select the first head word, face, 
with its relative order, Rh1, as one of the 
head features of preposition, to. More 
specifically, ?Rh1=face? denotes first head 
word, face, right of the preposition, to. 
? Head combine feature (f3): Combine any 
two head features described above to get six 
features. For example, L1R2 denotes two 
head words surrounding the preposition. 
? Phrase combine feature (f4): Combine the 
head words of noun phrase and verb phrase 
where the preposition is between the phrases. 
For example, V_N feature denotes the head 
words of verb phrase and noun phrase where 
the preposition is followed by noun phrase 
and is preceded by verb phrase. 
   
 
Word Feature 
(f1) 
Lw1=leaving, Rw1=face,  
Rw2= chronic, Rw3= condition 
Head Feature 
(f2) 
Lh1=them, Lh2=leaving, 
Rh1=face, Rh2=condition 
Head Combine 
Feature (f3) 
L1L2= them_leaving,  
L1R1= them_face,  
L1R2= them_condition, ? 
Phrase Combine 
Feature (f4) 
N_N= them_condition,  
V_N= leaving_condition,  
N_V= them_face,  
V_V= leaving_face 
Table 2. Features example for leaving them to face this 
chronic condition 
At run time, we extract the features of each 
preposition in learners? writings and ask the model 
to predict the preposition. The preposition error 
detection model described in section 2.3.1 first 
removes all prepositions from test sentences and 
then marks the ?presence? and ?absence? labels in 
every blank of a sentence. For each blank labeled 
?presence?, the correction model predicts the 
preposition which best fits the blank underlying the 
contexts. The correction model does not predict 
when the blanks are labeled ?absence?. Although 
some blanks labeled ?absence? may still 
correspond to prepositions, we decide to reduce 
some recall score to ensure the accuracy of the 
results. 
298
  
3. Experimental Results 
In this section, we present the experimental results 
of the determiner and preposition modules 
respectively.   
3.1 Determiners 
Table 3 shows the performance of the 
DetClassifier of individual feature and Table 4 
shows the performance of the SelClassifier. We 
also wonder how the size of training data 
influences the performance of the models. Table 5 
and 6 show the precision of modes of different 
sizes of training data with the best feature ?whole 
words in NP and last word of pre-NP.? Because the 
performance converges while using more than 5 
million training cases, we use only 1 million 
training cases to investigate the performance of 
using multiple features.  When using all features, 
the precision increases from 84.8% to 85.8% for 
DetClassifier, and from 39.8% to 56.0% for 
SelClassifier. 
We also implement another data-driven model 
for determiner selection (including zero) by using 
the 5gram of Web 1T corpus. The basic concept of 
the model is to use the frequency of determiners 
which fit the context of the given test data to 
choose the determiner candidates. If the frequency 
of the determiner using in the given NP is lower 
than other candidate determiners, we would use the 
most frequent one as the suggestion. However, 
according to our observation during testing, we 
find that the model tends to cause false alarms. To 
reduce the probability of false alarm, we set a high 
threshold for the ratio f1/f2 where f1 is the frequency 
of the used determiner and f2 is the frequency of 
the most frequent determiner. The suggestion is 
accepted only when the ratio exceeds the threshold.  
The major limitation of the proposed method is 
that some errors are ignored due to parsing errors. 
For example, the given data ?the them? should be 
considered as one NP with the ?UD? type error. 
However, the parser would give the chunk result 
?the [B-NP] them [B-NP]? and the error would not 
be recognized. It might need some rules to handle 
these exceptions. Another weakness of the 
proposed methods is that the less frequently used 
determiners are usually considered as errors and 
suggested to be replaced with more frequently used 
ones. For example, possessives such as ?my? 
and ?your?, are usually replaced with ?the.? We 
need to integrate more informative features to 
improve performance. 
 
Features Precision 
head/PoS 79.1% 
word/PoS of pre-NP 70.0% 
word/PoS of all words in NP 85.9% 
PoS of all words in NP 77.8% 
word/PoS of post-NP 71.8% 
whole words in NP 87.2% 
last word/PoS of pre-NP and head/PoS 92.3% 
whole words in NP and last word of 
pre-NP 
96.8% 
Table 3. Precision of features used in the DetClassifier 
 
Features Precision 
head/PoS 55.2% 
word/PoS of pre-NP 49.5% 
word/PoS of all words in NP 53.9% 
PoS of all words in NP 45.3% 
word/PoS of post-NP 46.1% 
whole words in NP 60.4% 
last word/PoS of pre-NP and head/PoS 65.3% 
whole words in NP and last word of 
pre-NP 
70.8% 
Table 4. Precision of features used in the SelClassifier 
 
Size Precision 
1,000,000 84.8% 
5,000,000 96.8% 
10,000,000 96.8% 
15,000,000 96.8% 
20,000,000 96.8% 
Table 5. Precision of different training size for the 
DetClassifier 
 
Size Precision 
1,000,000 39.8% 
3,000,000 43.2% 
5,000,000 44.5% 
7,000,000 61.6% 
7,249,218 70.8% 
Table 6. Precision of different training size for the  
 SelClassifier 
 
 
 
299
  
3.2 Prepositions 
Two sets of evaluation were carried out for 
detection. First, we use a randomly-selected 
portion of the BNC containing 1 million tokens to 
train our tokenizer targeting the 34 highest 
frequency prepositions. Second, we use a larger 
training corpus containing 10 million tokens, also 
randomly selected from the BNC, and target a 
smaller set of the 13 highest frequency 
prepositions, due to the fact that these 13 
prepositions can cover over 90% of the preposition 
errors found in the development set. 
We evaluate the trained taggers using two 
different metrics. First we evaluate the overall 
tagging precision, which is defined as 
 
Poverall   =  # of correctly tagged words  / # of 
all words  
Ppresence =  # correctly tagged PRESENCE / #  
all words labeled with PRESENCE 
 
Since most answer tags are Non-presence, 
Poverall is not informative, we therefore focus on 
Ppresense, and further evaluate the recall of presence, 
defined as: 
 
Rpresence = # correctly tagged PRESENCE  / # 
word should be tagged with PRESENCE  
 
We then evaluate on Precision and Recall of the 
PRESENCE tag using different probabilities to 
threshold the CRF tagging results. Then we show 
the result of two evaluation sets. On the left is the 
tagger train with 1 million tokens, targeting 34 
prepositions. On the right is the tagger trained with 
10 million tokens, targeting 13 prepositions. Only 
the latter tagger is used for producing the 
submitted runs. 
We used the development data released as part 
of HOO 2012 Shared Task as the gold standard for 
the evaluation of our preposition correction module. 
In order to observe the effect of different feature 
sets in training, we first extracted the MT and RT 
instances marked by the gold standard and then ask 
the correction module to correct these prepositions 
directly. Table 7 shows the precision of the models 
trained on different feature sets. The definition of 
precision is the same as the definition in the HOO 
2012 Shared Task. The results shows that the 
model trained using four feature sets achieved 
higher precision.   
Features Precision 
MT RT MT+RT 
f1 43.62% 39.15% 40.48% 
f1+f2 52.58% 43.47% 46.18% 
f1+f2+f3 55.20% 46.77% 49.27% 
f1+f2+f3+f4 55.11% 47% 49.41% 
Table 7. The feature selection and accuracy of the 
preposition correction module. 
 
In addition to the evaluation on the effect of 
different feature sets, we also conducted an 
evaluation done on the development data of HOO 
2012 Shared Task to observe the performance of 
the correction model when combined with the 
detection model. The correction model corrected 
three different types of preposition errors, MT, RT 
and MT+RT simultaneously (Table 8). 
 
 
  MT RT MT+RT 
Precision 1.16% 3.80% 4.96% 
Recall 29.86% 41.14% 37.79% 
  
Table 8. Precision and recall scores of the correction 
modules when combined with the detection module.  
 
Note that when we only corrected the 
preposition errors marked MT by preposition error 
detection module, the precision and recall are both 
lower than that of RT. The amount of false alarm 
instances of detection module in MT seems to be 
too high, thus in this paper, we won?t correct the 
instance marked MT to insure the higher precision 
of overall preposition correction. 
 
4. Conclusion 
In this paper, we integrate four learning-based 
methods in determiner and preposition error 
detection and correction. The integrated system 
simply parses and tags the test sentences and then 
corrects determiners and prepositions step by step. 
The training of our system relies on well-formed 
corpora and thus seems to be easier to 
re-implement it. The large well-formed corpus 
might also insure higher recall.  
In the future, we plan to integrate the system in 
a more flexible way. The detection modules could 
300
  
pass probabilities to the correction modules. The 
correction modules thus could decide whether to 
correct the instances or not. In addition, we plan to 
reduce the false alarm rate of the detection module. 
Besides, a more considerable evaluation would be 
conducted in the near future. 
Acknowledgements 
We would acknowledge the funding support 
from the Project (NSC 100-2627-E-007-001) 
and the help of the participants. Thanks also go to 
the comments of anonymous reviewers on this 
paper. 
References  
Mei-Hua Chen and Maosung Lin, 2011. Factors and 
Analyses of Common Miscollocations of College 
Students in Taiwan. Studies in English Language and 
Literature, 28, pp. 57-72. 
Martin Chodorow, Joel R. Tetreault, and Na-Rae Han. 
2007. Detection of grammatical errors involving 
prepositions. In Proceedings of the Fourth 
ACL-SIGSEM Workshop on Prepositions, pp.  
25-30. 
Robert Dale and Adam Kilgarriff. 2010. Helping Our 
Own: Text massaging for computational linguistics 
as a new shared task. In Proceedings of the 6th 
International Natural Language Generation 
Conference, pp. 261?266. 
Robert Dale, Ilya Anisimoff and George Narroway 
(2012) HOO 2012: A Report on the Preposition and 
Determiner Error Correction Shared Task. In 
Proceedings of the Seventh Workshop on Innovative 
Use of NLP for Building Educational Applications. 
Rachele De Felice and Stephen G. Pulman. 2007. 
Automatically acquiring models of preposition use. 
In Proceedings of the Fourth ACL-SIGSEM 
Workshop on Prepositions, pp. 45-50. 
Claudia Leacock, Martin Chodorow, Michael Gamon, 
and Joel Tetreault. 2010. Automated Grammatical 
Error Detection for Language Learners. Synthesis 
Lectures on Human Language Technologies. Morgan 
and Claypool. 
Adwait Ratnaparkhi. 1997. A linear observed time 
statistical parser based on maximum entropy models. 
In Proceedings of the Second Conference on 
Empirical Methods in Natural Language Processing, 
Brown University, Providence, Rhode Island. 
Michael Swan and Bernard Smith, editors. Learner 
English: A teacher?s guide to interference and other 
problems. Cambridge University Press, 2 edition, 
2001. DOI: 10.1017/CBO9780511667121 19, 23, 91 
Tsuruoka Y, Tateishi Y, Kim JD, Ohta T, McNaught J, 
Ananiadou S, Tsujii J. 2005. Developing a robust 
part-of-speech tagger for biomedical text. In 
Advances in Informatics, 10th Panhellenic 
Conference on Informatics; 11-13 November 2005 
Volos, Greece. Springer; pp. 382-392. 
 
301
