Proceedings of the 2012 Student Research Workshop, pages 13?18,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Active Learning with Transfer Learning 
 
 
Chunyong Luo, Yangsheng Ji, Xinyu Dai, Jiajun Chen 
State Key Laboratory for Novel Software Technology, 
Department of Computer Science and Technology, 
Nanjing University, Nanjing, 210046, China 
{luocy,jiys,daixy,chenjj}@nlp.nju.edu.cn 
 
 
 
 
 
 
Abstract 
In sentiment classification, unlabeled user 
reviews are often free to collect for new 
products, while sentiment labels are rare. In this 
case, active learning is often applied to build a 
high-quality classifier with as small amount of 
labeled instances as possible.  However, when 
the labeled instances are insufficient, the 
performance of active learning is limited. In 
this paper, we aim at enhancing active learning 
by employing the labeled reviews from a 
different but related (source) domain. We 
propose a framework Active Vector Rotation 
(AVR), which adaptively utilizes the source 
domain data in the active learning procedure. 
Thus, AVR gets benefits from source domain 
when it is helpful, and avoids the negative 
affects when it is harmful. Extensive 
experiments on toy data and review texts show 
our success, compared with other state-of-the-
art active learning approaches, as well as 
approaches with domain adaptation. 
1 Introduction 
To get a good generalization in traditional 
supervised learning, we need sufficient labeled 
instances in training, which are drawn from the 
same distribution as testing instances. When there 
are plenty of unlabeled instances but labels are 
insufficient and expensive to obtain, active 
learning (Settles, 2009) selects a small set of 
critical instances from target domain to be labeled, 
but costs are incurred for each label. On the other 
hand, transfer learning (Ji et al, 2011), also known 
as domain adaptation (Blitzer et al, 2006), aims at 
leveraging instances from other related source 
domains to construct high-quality models in the 
target domain. For example, we may employ 
labeled user reviews of similar products, to predict 
sentiment labels of new product reviews. When the 
distributions of source and target domain are 
similar, transfer learning would work well. But 
significant distribution divergence might cause 
negative transfer (Rosenstein et al, 2005). 
To further reduce the labeling cost and avoid 
negative transfer, we propose a framework, namely 
Active Vector Rotation (AVR), which takes 
advantage of both active learning and transfer 
learning techniques. Basically, AVR makes 
model?s parameter vector ? actively rotate towards 
its optimal direction with as few labeled instances 
in target domain as possible. Specifically, AVR 
first applies certain unsupervised learning 
techniques to make source and target domain?s 
distributions more ?similar?, and then leverages 
source domain information to query the most 
informative instances of target domain. Most 
importantly, it carefully reweights instances to 
mitigate the risk of negative transfer. AVR is 
general enough to incorporate various active 
learning and transfer learning modules, as well as 
varied basic learners such as LR and SVM.  
2 Related Work 
Shi et al (2008) proposed an approach AcTraK, 
using labeled source and target domain instances to 
build a so-called ?transfer classifier? to help label 
actively selected target domain instances. AcTraK 
initially requires labeled target domain instances, 
13
and relies too much on the transfer classifier. Thus 
it might be degenerated by negative transfer. 
An ALDA framework was proposed in (Saha et 
al., 2011). ALDA employs source domain 
classifier ????? to help label actively selected target domain instances. When conditional distributions 
???|?? are a bit different (Chattopadhyay et al, 
2011) or marginal distributions ????  are 
significantly different between source and target 
domain, ALDA would perform poorly. ALDA 
doesn?t discuss the negative transfer problem and 
gets hurts when it happens, while AVR actively 
avoids it by its projection and reweighting strategy. 
Liao et al (2005) proposed a method M-Logit, 
utilizing auxiliary data to help train LR. They also 
proposed actively sampling target domain 
instances using Fisher Information Matrix 
(Fedorov, 1972; Mackay, 1992). Besides, instance 
weighting was used to mitigate distribution 
difference between source and target domain in 
(Huang et al, 2006; Jiang and Zhai, 2007; 
Sugiyama et al, 2008). These can work as a 
module in our framework. 
3 AVR: Active Vector Rotation  
Without loss of generalization, we will constrain 
the discussion of AVR to binary classification 
tasks. But in fact, AVR can also be applied to 
multi-class classification and regression. 
Given training set ??? ? ????, ???|? ? 1,? ,??, ?? ? ?? , ?? ? ??1,?1? , traditional supervised learning tries to optimize (Fan et al, 2008; Lin et 
al., 2008): 
min??? ||?|| ? ? ? ???; ??, ??????? ,        (1) where the penalty parameter ? ? 0 , controls the 
importance ratio between loss function ???; ??, ??? and regularization parameter ||?||. Loss function?s 
definition is diverse for different basic learners, e.g. 
LR uses log?1 ? ????????? , while L2-SVM uses  
max??1 ? ??????, 0??. In the paper, we have the following assumptions: 
1) Target domain ???? ? ????? , ????|? ?
1,? , ?????, ??? ? ??? , ??? ? ??1,?1?, ???? 
is the size of ????; 
2) Source domain ???? ? ?????, ????|? ?1, ? , ?????, ??? ? ??? , ??? ? ??1,?1?, ???? is the size of ????; 3) ????? ? ?????; 
4) ???? and ???? are large enough; 
5) Testing set ????? and ???? are i.i.d.. 
Under maximum labeling budget ??, our goal is to employ source and target domain instances to 
maximize model accuracy: 
max? ????????? ? ? ?????????????????,????????? ,  (2) where the hypothesis is: 
????? ? ??1, ?
?? ? 0
?1, ??? ? 0.              (3) 
So, we design the machine learning framework, 
Active Vector Rotation, to optimize ?: 
min??? ||?|| ? ? c????; ??, ??????? ,         (4) where the weight variables c? ? 0 ,  control the importance of each instance in training. Larger c?  means more necessity of ?  to fit ???, ??? . Intuitively, ?  of ???  should try harder to fit the instances from ????  than the instances from ???? , 
so that the corresponding c?  of instances from ???? 
should be larger. The algorithm of AVR is 
described in Table 1, which is discussed in detail in 
the following subsections. 
Input: ????, ????, ?????, ??; Output: ?, ????????? 
1. Project ??  and ??  to a common latent semantic 
space, where ???, ??? ? ??. 
2. Actively select the least source domain instances, 
which can characterize source domain classifier 
???? , into training set ??? ? ??????, ?????|? ?
1,? , ????? ?. 3. Initialize ? using ???. 4. For ?? ? ????? ? 1 ? ????? ? ?? 1) Actively select the most informative 
instance ?????, ????? from ????. 
2) Insert the new labeled instance into 
training set, ??? ? ??? ? ?????, ?????. 
3) Update c? for ? ? 1: ??. 
4) Retrain ? using ??? and (4). end 
5. Compute ????????? . 
Table 1: AVR algorithm 
3.1 Projection of Source and Target Domain 
?? and ??  might be in different vector spaces. To 
employ ????  in the training of ???? ?s optimal ? , 
we?d better project ??  and ??  into a common n-
dimensional latent semantic space, where the 
distributions of the projected ???, ??? ? ?? would 
be more similar. Varied projection approaches 
could be employed in different tasks. For example, 
Hardoon et al (2004) used CCA to project text and 
14
image to a latent semantic space, where image 
could be retrieved by text. Blitzer et al (2007) and 
Ji et al (2011) utilized SCL and VMVPCA 
respectively in sentiment classification. Huang et 
al. (2006) applied RKHS and KMM in breast 
cancer prediction. 
Regarding the case where ??  and ??  are in the 
same vector space but certain approach is applied 
to make their distributions more similar, we also 
consider it as a kind of projection of ???? and ????. 
3.2 Initialization of Training set 
To reduce training cost and risk of negative 
transfer, AVR actively selects a relatively small set 
of instances from ????  into ??? . Transfer learning mainly leverages ???? ?s separating hyperplane information, i.e. ???? , while only a small set of critical instances from ????  can characterize the statistics of ???? . AVR initializes ???  by these critical instances. Different tasks may employ 
different selection strategy. E.g. in our experiments, 
the text classification task employs uncertainty 
sampling (Settles, 2009), while sentiment 
classification task selects the least ?????  instances which can accurately characterize ?????, such that: 
min????????? ? ????? ????
????????? .                 (5) 
3.3 Query Strategy in Target Domain 
After initialization of ???, AVR uses certain basic learner, such as LR and SVM, to get ? ? ?????? . As the labeling budget ??  is limited, we need iteratively query the most informative instance and 
add the new labeled instance into ??? to retrain ?. AVR revises the query strategy of traditional 
active learning. After a few new labeled instances 
added to ??? , the retrained ?  would be different from ?????? and closer to the optimum. Traditional active learning queries the instance in ???? w.r.t. ?, 
e.g. uncertainty sampling queries the instance 
closest to separating hyperplane, such that: 
min?????????? ??
??????.                    (6) 
However, AVR queries the most informative 
instance from which are identically classified by ?  
and ?????? , e.g. for uncertainty sampling, AVR queries the instance such that: 
min?????????,??????????????? ???????? ??
?????? .       (7) 
The instance queried by AVR makes ? more 
quickly approach to its optimum, as to some extent, 
part of the statistics of the instances which are 
differently classified by ? and ????? , can be characterized by the new queried instances. But 
when ? is very close to the optimum, AVR will 
query by traditional active learning strategy. 
3.4 Reweighting ?? 
Appropriate reweighting can help accelerate ? 
rotating to the optimum and avoid negative transfer. 
Intuitively, the instances from ????  and the 
instances which have similar distribution with ???? 
should be given higher weight. Varied reweighting 
strategy, e.g. TrAdaBoost (Dai et al, 2007), could 
be applied in AVR framework. In our experiments, 
AVR employs a simple but efficient reweighting 
strategy, without iteration: 
?? ? ??
1, ? ? ????? , ???????????? ???? ? 0
??0, ? ? ????? , ???????????? ???? ? 0??
?, ?????????.
    (8) 
4 Experiments 
We perform AVR on a set of toy data and two real 
world datasets, 20 Newsgroups Dataset 1  and 
Multi-Domain Sentiment Dataset 2 , comparing it 
with several baseline methods. In this paper, we 
use model accuracy ????????? under fixed labeling budget ?? as the evaluation. We used LR and L2-SVM as basic learner respectively, but due to 
space limit, we only report the results of LR. 
4.1 Toy Data 
We generate four bivariate Gaussian distributions 
as the positive and negative instances of ???? and ???? respectively as illustrated in Figure 1.  
 Figure 1: Distribution of toy data and AVR process 
                                                          
1 http://people.csail.mit.edu/jrennie/20Newsgroups/. 
2 http://www.cs.jhu.edu/~mdredze/datasets/sentiment/. 
15
As shown in Figure 1, ????  and ????  randomly 
sample 1000 instances respectively, then ????? randomly samples 200 instances from ????. Circle 
and diamond, big plus and cross, small plus and 
cross, represent positive and negative instances of 
????, ???? and ????? respectively. 
To this toy data, AVR?s configuration is: 
1) ??? ? ??, ??? ? ??. 
2) AVR uses uncertainty sampling to select the 
least 5 instances which can characterize 
?????, to initialize ??? and ?????. In Figure 1, the 5 instances are marked by big filled 
circles or diamonds, the dash line draws the 
separating hyperplane ?????? ? ? 0. 3) Then AVR queries instances as described in 
Section 3.3, the first 10 queried instances are 
marked by large numerals, with the first 3 
are queried w.r.t. (7). The small numerals 
mark the first 3 instances which would be 
queried w.r.t. (6). 
4) AVR reweights ?? by (8), where ? ? 4. The black filled circles mark the instances whose 
corresponding c? ? 0. The solid line draws the current hyperplane ??? ? 0. 
Baseline methods are briefly described in Table 
2. Details about AcTraK and ALDA can be found 
in (Shi et al, 2008) and (Saha et al, 2011) 
respectively. 
Method Note 
Random 
 
Active 
Randomly sample instances from ????, 
without use of ???? Uncertainty sampling, without use of ???? AcTraK 
 
 
 
O-ALDA 
Initiated by one positive and one negative 
instances from ????, followed by uncertainty 
sampling from ???? 
Stream-based sampling, without instance 
reweighting 
B-ALDA 
 
 
Source-A 
Pool-based sampling, without instance 
reweighting 
Initialize ??? by ????, following uncertainty sampling without instance reweighting 
AVR-U 
 
 
AVR-W?
Uncertainty sampling with instance 
reweighting 
Give all instances from ???? the same weight, regardless prediction difference 
between ??and??????.?Table 2: Brief description of baseline methods 
 
The first 4 methods referring randomness are run 
1000 times to average results as shown in Table 3. 
Method Target Domain Labeling Budget ??  1 2 3 4 5 6 7 8 9 10 
Random 
Active 
50.05
49.90
69.35
75.65
79.88
90.41
86.04
95.92
90.26 
96.30 
93.01 
97.23 
94.41 
97.41 
95.30 
97.59 
96.03
97.64
96.41
97.72
AcTraK 
O-ALDA
93.15
77 
95.23
77 
96.10
77.01
96.69
77.07
97.03 
77.15 
97.30 
77.24 
97.53 
77.33 
97.68 
77.37 
97.78
77.42
97.82
77.48
B-ALDA
Source-A
AVR-U 
AVR-W 
77 
77 
80.50
80.50
77 
77 
95 
94 
77 
77 
85 
94.50
77 
77 
96 
97 
77 
77 
98.50 
98.50 
77 
77 
96 
97 
77 
77 
98 
98.50 
77.50 
77.50 
98 
97.50 
77.50
77.50
97 
98.50
77.50
77.50
96.50
97 
AVR 80.50 94 94.50 97 98.50 97 98.50 97.50 98.50 98.50
Table 3: Performance of different methods on toy 
data, where AcTraK unfairly uses two more labels. 
4.2 20 Newsgroups Dataset 
20 Newsgroups Dataset is commonly used in 
machine learning and NLP tasks. It contains about 
20000 newsgroup documents which are 
categorized into 6 top categories and 20 
subcategories. We split it into 6 pair of ????  and ???? , with each pair includes only two top 
categories documents, such as ?comp? and ?rec?, 
but ????  and ????  are drawn from different 
subcategories, e.g. ????  has ?comp.graphics? and ?comp.graphics?, but ????  has ?comp.windows.x? 
and ?sci.autos?. The task is to leverage ????  to distinguish the top categories of documents in ????. 
Our settings of 20 Newsgroups Dataset is identical 
with Dai et al (2007), details can be found there. 
On this dataset, AVR?s configuration is similar 
with that on toy data, with ?????  varies from 500 to 800 on different pairs. 
Due to space limit, we only report results on the 
pair of ?comp vs. rec? in Figure 2, with all 
methods are averaged over 30 runs. The results on 
other pairs are similar. Since AVR-U and AVR-W 
are variants of AVR, with similar performance, we 
only report the results of AVR.  
 Figure 2: AVR outperforms others on the ?comp 
vs. rec? pair. 
16
4.3 Multi-Domain Sentiment Dataset 
The sentiment dataset consists of user reviews 
about several products (Book, DVD, Electronic, 
Kitchen) from Amazon.com, the task is to classify 
a review?s sentiment label as positive or negative. 
We have 12 pairs with each pair has two products 
as ????  and ????  respectively. On this dataset, 
AVR employs VMVPCA (Ji et al, 2011) to project 
????  and ???? , and initializes ???  with ????? ?
1000  instances from ????  w.r.t. (5), while the other configuration is the same as that described in 
Section 4.1. To be comparable, the baseline 
methods which leverage ????  are preprocessed by VMVPCA. We also add another baseline method 
Source-A? here, which is identical with Source-A, 
except that it is not projected by VMVPCA. Given 
space limit, we only report the results on the pair 
?DVD?Kitchen?, with other pairs have similar 
performance. 
 
 Figure 3: AVR does better than previous work on 
the ?DVD?Kitchen? dataset for all budget sizes. 
4.4 Discussion 
From inspection of experimental results, we get the 
following remarks. 
Why to combine active learning and transfer 
learning? 
? Active learning such as uncertainty sampling 
can significantly reduce the labeling cost. But 
when ? is far from the optimum, uncertainty 
sampling may oversample instances near a 
direction. For example, in Figure 2, Active 
method is worse than Random method when 
??? ? 50. 
? ????  could help ????  in learning accurate ? , 
e.g. in Figure 2, when ??? ? 200, Source-A method with the help of ????  outperforms Random and Active methods which never use 
????. But inappropriate use of ???? may cause negative transfer, e.g. in Figure 2, when 
??? ? 200 , Source-A, ALDA and AcTraK methods, which overuse ???? , underperform Active method. 
? Thus, we realize that appropriate combination 
of transfer learning and active learning could 
advance and complement each other. 
Especially when ????  has scarce labels, ???? 
could help avoid oversample instances near a 
direction. But with the increase of labels in 
???? , ????  should decrease its weight in 
training to avoid negative transfer. 
Does each component of AVR work? 
? Appropriate Projection of ???? and ???? could 
mitigate distribution divergence, e.g. in our 
sentiment classification task, Source-A and 
AVR which applied VMVPCA significantly 
and consistently outperforms Source-A?. 
? Initialize ???  by a small set of critical instances from ????  can significantly reduce training cost without loss of accuracy. E.g. in 
our experiments, when ??? ? 1 , AVR has better or comparable performance w.r.t. 
Source-A which initializes ??? by whole ????. More importantly, AVR trims initial ???  size from 1000 to 5 in toy data, from 4000 to 500 
in Newsgroups dataset, and from 2000 to 
1000 in Sentiment dataset. 
? The query strategy of AVR described in 
Section 3.3 advances traditional active 
learning, which is supported by the 
performance of AVR over AVR-U. 
? Appropriately reweighting instances from 
????  and ????  could result in accurate ?  and 
avoid negative transfer meanwhile. For 
example, in our experiments, the reweighting 
strategy of (8) makes AVR outperform all 
baseline methods, while some of which suffer 
from negative transfer. 
How about AcTraK?s performance? 
? AcTraK works well on our toy data, just 
because it unfairly uses too much more labels 
of ???? , even though, it underperforms AVR 
when ? ??? ? 3 . Besides, AcTraK performs poorly on high dimensional data like text in 
our experiments. 
17
5 Conclusion and Future Work 
Our proposed machine learning framework AVR 
actively and carefully leverages information of 
source domain to query the most informative 
instances in target domain, as well as to train the 
best possible model of target domain. The four 
essential components of AVR, which establish its 
efficacy and help it avoid negative transfer, are 
validated in experiments.  
In the future, we are planning to apply AVR in 
more tasks with appropriate specification of 
projection, query and reweighting strategy. 
Especially for sentiment classification, we will 
combine prior domain knowledge, such as domain 
sentiment lexicon, with AVR framework to further 
reduce labeling cost. 
Acknowledgements 
This work is supported by the National 
Fundamental Research Program of China 
(2010CB327903) and the Doctoral Fund of 
Ministry of Education of China (20110091110003). 
We also thank Shujian Huang, Ning Xi, Yinggong 
Zhao, and anonymous reviewers for their greatly 
helpful comments. 
References 
John Biltzer, Ryan Mcdonald, Fernando Pereira. 2006. 
Domain adaptation with structural correspondence 
learning. In Proc.EMNLP, pp.120-128. 
John Biltzer, Mark Dredze, Fernando Pereira. 2007. 
Biographies, bollywood, boom-boxes and blenders: 
domain adaptation for sentiment classification. In 
Proc. ACL, pp.432-439. 
Rita Chattopadhyay, Jieping Ye, Sethuraman 
Panchanathan, Wei Fan, Ian Davidson. 2011. Multi-
source domain adaptation and its application to early 
detection of fatigue. In Proc. KDD, pp.717-725. 
Wenyuan Dai, Qiang Yang, Gui-Rong Xue, Yong Yu. 
2007. Boosting for transfer learning. In Proc. ICML, 
pp.93-200. 
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, Chih-Jen Lin. 2008. Liblinear: a library 
for large linear classification. JMLR, 9:1871-1874. 
Valerii? Vadimovich Fedorov. 1972. Theory of optimal 
experiments. Academic Press. 
David R. Hardoon, Sandor Szedmak, John Shaew-
Taylor.  2004. Canonical correlation analysis: An 
overview with application to learning methods. 
Neural Computation, 16(12): 2639-2664. 
Jiayuan Huang, Alexander J. Smola, Arthur Gretton, 
Karsten M. Borgwardt, Bernhard Scho? lkpf. 2006. 
Correcting sample selection bias by unlabeled data. 
In Proc. NIPS, pp.601-608. 
Yangsheng Ji, Jiajun Chen, Gang Niu, Lin Shang, 
Xinyu Dai. 2011. Transfer learning via multi-view 
principal component analysis. JCST, 26(1):81-98. 
Jing Jiang, ChengXiang Zhai. 2007. Instance weighting 
for domain adaptation in NLP. In proc. ACL, pp.264-
271. 
Xuejun Liao, Ya Xue, Lawrence Cain. 2005. Logistic 
regression with an auxiliary data source. In Proc. 
ICML,  pp.505-512. 
Chih-Jen Lin, Ruby C. Weng, S. Sathiya Keerthi. 2008. 
Trust region newton method for large-scale logistic 
regression. JMLR, 9:627-650. 
David J. C. Mackay. 1992. Information-based objective 
functions for active data selection. Neural 
Computation, 5:590-604. 
Michael T. Rosenstein, Zvika Marx, Leslie Pack 
Kaelbling, Thomas G. Dietterich. 2005. To transfer 
or not to transfer. In Proc. NIPS, December 9-10. 
Avishek Saha, Piyush Rai, Hal Daum e?  III, Suresh 
Venkatasubramanian, Scott L. DuVall. 2011. Active 
supervised domain adaptation. In Proc. ECML-
PKDD, pp.97-112. 
Burr Settles. 2009. Active learning Literature Survey. In 
Computer Sciences Technology Report 1648, 
University of Wisconsin-Madison. 
Xiaoxiao Shi, Wei Fan, Jiangtao Ren. 2008. Actively 
transfer domain knowledge. In Proc. ECML-PKDD 
pp.342-357. 
Masashi Sugiyama, Shinichi Nakajima, Hisashi 
Kashima, Paul von B??nau, Motoaki Kawanabe. 2008. 
Direct importance estimation with model selection 
and its application to covariate shift adaptation. 
NIPS, pp.1433-1440. 
18
