Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 503?511,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Hidden Markov Tree Model for Word Alignment
Shuhei Kondo Kevin Duh Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5, Takayama, Ikoma, Nara, 630-0192, Japan
{shuhei-k,kevinduh,matsu}@is.naist.jp
Abstract
We propose a novel unsupervised word
alignment model based on the Hidden
Markov Tree (HMT) model. Our model
assumes that the alignment variables have
a tree structure which is isomorphic to the
target dependency tree and models the dis-
tortion probability based on the source de-
pendency tree, thereby incorporating the
syntactic structure from both sides of the
parallel sentences. In English-Japanese
word alignment experiments, our model
outperformed an IBM Model 4 baseline
by over 3 points alignment error rate.
While our model was sensitive to poste-
rior thresholds, it also showed a perfor-
mance comparable to that of HMM align-
ment models.
1 Introduction
Automatic word alignment is the first step in the
pipeline of statistical machine translation. Trans-
lation models are usually extracted from word-
aligned bilingual corpora, and lexical translation
probabilities based on word alignment models are
also used for translation.
The most widely used models are the IBM
Model 4 (Brown et al, 1993) and Hidden Markov
Models (HMM) (Vogel et al, 1996). These mod-
els assume that alignments are largely monotonic,
possibly with a few jumps. While such assump-
tion might be adequate for alignment between sim-
ilar languages, it does not necessarily hold be-
tween a pair of distant languages like English and
Japanese.
Recently, several models have focused on in-
corporating syntactic structures into word align-
ment. As an extension to the HMM alignment,
Lopez and Resnik (2005) present a distortion
model conditioned on the source-side dependency
tree, and DeNero and Klein (2007) propose a
distortion model based on the path through the
source-side phrase-structure tree. Some super-
vised models receive syntax trees as their input
and use them to generate features and to guide the
search (Riesa and Marcu, 2010; Riesa et al, 2011),
and other models learn a joint model for pars-
ing and word alignment from word-aligned par-
allel trees (Burkett et al, 2010). In the context of
phrase-to-phrase alignment, Nakazawa and Kuro-
hashi (2011) propose a Bayesian subtree align-
ment model trained with parallel sampling. None
of these models, however, can incorporate syntac-
tic structures from both sides of the language pair
and can be trained computationally efficiently in
an unsupervised manner at the same time.
The Hidden Markov Tree (HMT) model
(Crouse et al, 1998) is one such model that sat-
isfies the above-mentioned properties. The HMT
model assumes a tree structure of the hidden vari-
ables, which fits well with the notion of word-to-
word dependency, and it can be trained from un-
labeled data via the EM algorithm with the same
order of time complexity as HMMs.
In this paper, we propose a novel word align-
ment model based on the HMT model and show
that it naturally enables unsupervised training
based on both source and target dependency trees
in a tractable manner. We also compare our HMT
word alignment model with the IBM Model 4 and
the HMM alignment models in terms of the stan-
dard alignment error rates on a publicly available
English-Japanese dataset.
2 IBM Model 1 and HMM Alignment
We briefly review the IBM Model 1 (Brown et
al., 1993) and the Hidden Markov Model (HMM)
word alignment (Vogel et al, 1996) in this section.
Both are probabilistic generative models that fac-
503
tor as
p(f |e) =
?
a
p(a, f |e)
p(a, f |e) =
J?
j=1
pd(aj |aj )pt(fj |eaj )
where e = {e1, ..., eI} is an English (source) sen-
tence and f = {f1, ..., fJ} is a foreign (target)
sentence. a = {a1, ..., aJ} is an alignment vec-
tor such that aj = i indicates the j-th target word
aligns to the i-th source word and aj = 0 means
the j-th target word is null-aligned. j is the index
of the last non null-aligned target word before the
index j.
In both models, pt(fj |eaj ) is the lexical transla-
tion probability and can be defined as conditional
probability distributions. As for the distortion
probability pd(aj |aj ), pd(aj = 0|aj = i?) = p0
where p0 is NULL probability in both models.
pd(aj = i|aj = i?) is uniform in the Model 1
and proportional to the relative count c(i ? i?) in
the HMM for i 6= 0. DeNero and Klein (2007)
proposed a syntax-sensitive distortion model for
the HMM alignment, in which the distortion prob-
ability depends on the path from the i-th word to
the i?-th word on the source-side phrase-structure
tree, instead of the linear distance between the two
words.
These models can be trained efficiently using
the EM algorithm. In practice, models in two di-
rections (source to target and target to source) are
trained and then symmetrized by taking their in-
tersection, union or using other heuristics. Liang
et al (2006) proposed a joint objective of align-
ment models in both directions and the probability
of agreement between them, and an EM-like algo-
rithm for training.
They also proposed posterior thresholding for
decoding and symmetrization, which take
a = {(i, j) : p(aj = i|f , e) > ?}
with a threshold ? . DeNero and Klein (2007) sum-
marized some criteria for posterior thresholding,
which are
? Soft-Union
?
pf (aj = i|f , e) ? pr(ai = j|f , e)
? Soft-Intersection
pf (aj = i|f , e) + pr(ai = j|f , e)
2
? Hard-Union
max(pf (aj = i|f , e), pr(ai = j|f , e))
? Hard-Intersection
min(pf (aj = i|f , e), pr(ai = j|f , e))
where pf (aj = i|f , e) is the alignment probabil-
ity under the source-to-target model and pr(ai =
j|f , e) is the one under the target-to-source model.
They also propose a posterior decoding heuris-
tic called competitive thresholding. Given a j ? i
matrix of combined weights c and a threshold ? , it
choose a link (j, i) only if its weight cji ? ? and it
is connected to the link with the maximum weight
both in row j and column i.
3 Hidden Markov Tree Model
The Hidden Markov Tree (HMT) model was first
introduced by Crouse et al (1998). Though it has
been applied successfully to various applications
such as image segmentation (Choi and Baraniuk,
2001), denoising (Portilla et al, 2003) and biol-
ogy (Durand et al, 2005), it is largely unnoticed
in the field of natural language processing. To
the best of our knowledge, the only exception is
Z?abokrtsky` and Popel (2009) who used a variant
of the Viterbi algorithm for HMTs in the transfer
phase of a deep-syntax based machine translation
system.
An HMT model consists of an observed random
tree X = {x1, ..., xN} and a hidden random tree
S = {s1, ..., sN}, which is isomorphic to the ob-
served tree.
The parameters of the model are
? P (s1 = j), the initial hidden state prior
? P (st = j|s?(t) = i), transition probabilities
? P (xt = h|st = j), emission probabilities,
where ?() is a function that maps the index of a
hidden node to the index of its parent node. These
parameters can be trained via the EM algorithm.
The ?upward-downward? algorithm proposed
in Crouse et al (1998), an HMT analogue of the
forward-backward algorithm for HMMs, can be
used in the E-step. However, it is based on the de-
composition of joint probabilities and suffers from
numerical underflow problems.
Durand et al (2004) proposed a smoothed vari-
ant of the upward-downward algorithm, which is
504
based on the decomposition of smoothed probabil-
ities and immune to underflow. In the next section,
we will explain this variant in the context of word
alignment.
4 Hidden Markov Tree Word Alignment
We present a novel word alignment model based
on the HMT model. Given a target sentence f =
{f1, ..., fJ}with a dependency treeF and a source
sentence e = {e1, ..., eI} with a dependency tree
E, an HMT word alignment model factors as
p(f |e) =
?
a
p(a, f |e)
p(a, f |e) =
J?
j=1
pd(aj |aj )pt(fj |eaj ).
While these equations appear identical to the ones
for the HMM alignment, they are different in that
1) e, f and a are not chain-structured but tree-
structured, and 2) j is the index of the non null-
aligned lowest ancestor of the j-th target word1,
rather than that of the last non null-aligned word
preceding the j-th word as in the HMM alignment.
Note that A, the tree composed of alignment vari-
ables a = {a1, ..., aJ}, is isomorphic to the target
dependency tree F.
Figure 1 shows an example of a target depen-
dency tree with an alignment tree, and a source
dependency tree. Note that English is the target
(or foreign) language and Japanese is the source
(or English) language here. We introduce the fol-
lowing notations following Durand et al (2004),
slightly modified to better match the context of
word alignment.
? ?(j) denotes the index of the head of the j-th
target word.
? c(j) denotes the set of indices of the depen-
dents of the j-th target word.
? Fj = f j denotes the target dependency sub-
tree rooted at the j-th word.
As for the parameters of the model, the initial
hidden state prior described in Section 3 can be
defined by assuming an artificial ROOT node for
both dependency trees, forcing the target ROOT
node to be aligned only to the source ROOT
1This dependence on aj can be implemented as a first-
order HMT, analogously to the case of the HMM alignment
(Och and Ney, 2003).
node and prohibiting other target nodes from be-
ing aligned to the source ROOT node. The lexi-
cal translation probability pt(fj |eaj ), which corre-
sponds to the emission probability, can be defined
as conditional probability distributions just like in
the IBM Model 1 and the HMM alignment.
The distortion probability pd(aj = i|aj = i?),
which corresponds to the transition probability,
depends on the distance between the i-th source
word and the i?-th source word on the source de-
pendency tree E, which we denote d(i, i?) here-
after. We model the dependence of pd(aj =
i|aj = i?) on d(i, i?) with the counts c(d(i, i?)).
In our model, d(i, i?) is represented by a pair
of non-negative distances (up, down), where up
is the distance between the i-th word and the
lowest common ancestor (lca) of the two words,
down is the one between the i?-th word and the
lca. For example in Figure 1b, d(0, 2) = (0, 4),
d(2, 5) = (2, 2) and d(4, 7) = (3, 0). In practice,
we clip the distance by a fixed window size w and
store c(d(i, i?)) in a two-dimensional (w + 1 ) ?
(w + 1 ) matrix. When w = 3, for example, the
distance d(0, 2) = (0, 3) after clipping.
We can use the smoothed variant of upward-
downward algorithm (Durand et al, 2004) for the
E-step of the EM algorithm. We briefly explain
the smoothed upward-downward algorithm in the
context of tree-to-tree word alignment below. For
the detailed derivation, see Durand et al (2004).
In the smoothed upward-downward algorithm,
we first compute the state marginal probabilities
p(aj = i)
=
?
i?
p(a?(j) = i?)pd(aj = i|a?(j) = i?)
for each target node and each state, where
pd(aj = i|a?(j) = i?) = p0
if the j-th word is null-aligned, and
pd(aj = i|a?(j) = i?)
= (1? p0) ?
c(d(i?, i))?
i?? 6=0 c(d(i?, i??))
if the j-th word is aligned. Note that we must ar-
tificially normalize pd(aj = i|a?(j) = i?), because
unlike in the case of the linear distance, multiple
words can have the same distance from the j-th
word on a dependency tree.
505
a0 a1 a2 a3 a4 a5
? ? ? ? ? ?
?ROOT?f0 Thatf1 wasf2 Mountf3 Kuramaf4 .f5
(a) Target sentence with its dependency/alignment tree. Target words {f0, ..., f5} are emitted from
alignment variables {a0, ..., a5}. Ideally, a0 = 0, a1 = 1, a2 = 7, a3 = 5, a4 = 4 and a5 = 9.
?ROOT?e0 ??e1 ?e2 ?e3 ??e4 ?e5 ?e6 ??e7 ?e8 ?e9
that mountain Kurama mountain be .
(b) Source sentence with its dependency tree. None of the target words are aligned to e2, e3, e6 and e8.
Figure 1: An example of sentence pair under the Hidden Markov Tree word alignment model. If we
ignore the source words to which no target words are aligned, the dependency structures look similar to
each other.
In the next phase, the upward recursion, we
compute p(aj = i|Fj = f j) in a bottom-up man-
ner. First, we initialize the upward recursion for
each leaf by
?j(i) = p(aj = i|Fj = fj)
= pt(fj |ei)p(aj = i)Nj
,
where
Nj = p(Fj = fj) =
?
i
pt(fj |ei)p(aj = i).
Then, we proceed from the leaf to the root with the
following recursion,
?j(i) = p(aj = i|Fj = f j)
=
{?j??c(j) ?j,j?(i)}pt(fj |ei)p(aj = i)
Nj
,
where
Nj =
p(Fj = f j)?
j??c(j) p(Fj? = f j?)
=
?
i
{
?
j??c(j)
?j,j?(i)}pt(fj |ei)p(aj = i)
and
??(j),j(i) =
p(Fj = f j |a?(j) = i)
p(Fj = f j)
=
?
i?
?j(i?)pd(aj = i?|a?(j) = i)
p(aj = i?)
.
After the upward recursion is completed, we
compute p(aj = i|F0 = f0) in the downward
recursion. It is initialized at the root node by
?0(i) = p(a0 = i|F0 = f0).
Then we proceed in a top-down manner, comput-
ing
?j(i) = p(aj = i|F0 = f0)
= ?j(i)p(aj = i)
?
?
i?
pd(aj = i|a?(j) = i?)??(j)(i?)
??(j),j(i?)
.
for each node and each state.
The conditional probabilities
p(aj = i, a?(j) = i?|F0 = f0)
=
?j(i)pd(aj = i|a?(j) = i?)??(j)(i?)
p(aj = i)??(j),j(i?)
,
506
which is used for the estimation of distortion prob-
abilities, can be extracted during the downward re-
cursion.
In the M-step, the lexical translation model can
be updated with
pt(f |e) =
c(f, e)
c(e) ,
just like the IBM Models and HMM alignments,
where c(f, e) and c(e) are the count of the word
pair (f, e) and the source word e. However, the
update for the distortion model is a bit compli-
cated, because the matrix that stores c(d(i, i?))
does not represent a probability distribution. To
approximate the maximum likelihood estimation,
we divide the counts c(d(i, i?)) calculated during
the E-step by the number of distortions that have
the distance d(i, i?) in the training data. Then we
normalize the matrix by
c(d(i, i?)) = c(d(i, i
?))?w
i=0
?w
i?=0 c(d(i, i?))
.
Given initial parameters for the lexical trans-
lation model and the distortion counts, an HMT
aligner collects the expected counts c(f, e), c(e)
and c(d(i, i?)) with the upward-downward algo-
rithm in the E-step and re-estimate the parameters
in the M-Step. Dependency trees for the sentence
pairs in the training data remain unchanged during
the training procedure.
5 Experiment
We evaluate the performance of our HMT align-
ment model in terms of the standard alignment er-
ror rate2 (AER) on a publicly available English-
Japanese dataset, and compare it with the IBM
Model 4 (Brown et al, 1993) and HMM alignment
with distance-based (HMM) and syntax-based (S-
HMM) distortion models (Vogel et al, 1996;
Liang et al, 2006; DeNero and Klein, 2007).
We use the data from the Kyoto Free Transla-
tion Task (KFTT) version 1.3 (Neubig, 2011). Ta-
ble 1 shows the corpus statistics. Note that these
numbers are slightly different from the ones ob-
served under the dataset?s default training proce-
dure because of the difference in the preprocessing
scheme, which is explained below.
2Given sure alignments S and possible alignments P , the
alignment error rate of alignments A is 1 ? |A?S|+|A?P ||A|+|S|
(Och and Ney, 2003).
The tuning set of the KFTT has manual align-
ments. As the KFTT doesn?t distinguish between
sure and possible alignments, F-measure equals
1?AER on this dataset.
5.1 Preprocessing
We tokenize the English side of the data using the
Stanford Tokenizer3 and parse it with the Berkeley
Parser4 (Petrov et al, 2006). We use the phrase-
structure trees for the Berkeley Aligner?s syntactic
distortion model, and convert them to dependency
trees for our dependency-based distortion model5.
As the Berkeley Parser couldn?t parse 7 (out of
about 330K) sentences in the training data, we re-
moved those lines from both sides of the data. All
the sentences in the other sets were parsed suc-
cessfully.
For the Japanese side of the data, we first con-
catenate the function words in the tokenized sen-
tences using a script6 published by the author
of the dataset. Then we re-segment and POS-
tag them using MeCab7 version 0.996 and parse
them using CaboCha8 version 0.66 (Kudo and
Matsumoto, 2002), both with UniDic. Finally,
we modify the CoNLL-format output of CaboCha
where some kind of symbols such as punctuation
marks and parentheses have dependent words. We
chose this procedure for a reasonable compromise
between the dataset?s default tokenization and the
dependency parser we use.
As we cannot use the default gold alignment due
to the difference in preprocessing, we use a script9
published by the author of the dataset to modify
the gold alignment so that it better matches the
new tokenization.
5.2 Training
We initialize our models in two directions with
jointly trained IBM Model 1 parameters (5 itera-
tions) and train them independently for 5 iterations
3http://nlp.stanford.edu/software/
4We use the model trained on the WSJ portion of
Ontonotes (Hovy et al, 2006) with the default setting.
5We use Stanford?s tool (de Marneffe et al, 2006)
with options -conllx -basic -makeCopulaHead
-keepPunct for conversion.
6https://github.com/neubig/
util-scripts/blob/master/
combine-predicate.pl
7http://code.google.com/p/mecab/
8http://code.google.com/p/cabocha/
9https://github.com/neubig/
util-scripts/blob/master/
adjust-alignments.pl
507
Sentences English Tokens Japanese Tokens
Train 329,974 5,912,543 5,893,334
Dev 1,166 24,354 26,068
Tune 1,235 30,839 33,180
Test 1,160 26,730 27,693
Table 1: Corpus statistics of the KFTT.
Precision Recall AER
HMT (Proposed) 71.77 55.23 37.58
IBM Model 4 60.58 57.71 40.89
HMM 69.59 56.15 37.85
S-HMM 71.60 56.14 37.07
Table 2: Alignment error rates (AER) based on
each model?s peak performance.
with window size w = 4 for the distortion model.
The entire training procedure takes around 4 hours
on a 3.3 GHz Xeon CPU.
We train the IBM Model 4 using GIZA++ (Och
and Ney, 2003) with the training script of the
Moses toolkit (Koehn et al, 2007).
The HMM and S-HMM alignment models are
initialized with jointly trained IBM Model 1 pa-
rameters (5 iterations) and trained independently
for 5 iterations using the Berkeley Aligner. We
find that though initialization with jointly trained
IBM Model 1 parameters is effective, joint train-
ing of HMM alignment models harms the perfor-
mance on this dataset (results not shown).
5.3 Result
We use posterior thresholding for the HMT and
HMM alignment models, and the grow-diag-final-
and heuristic for the IBM Model 4.
Table 2 and Figure 2 show the result. As
the Soft-Union criterion performed best, we don?t
show the results based on other criteria. On the
other hand, as the peak performance of the HMT
model is better with competitive thresholding and
those of HMM models are better without it, we
compare Precision/Recall curves and AER curves
both between the same strategy and the best per-
forming strategy for each model.
As shown in Table 2, the peak performance of
the HMT alignment model is better than that of
the IBM Model 4 by over 3 point AER, and it was
somewhere between the HMM and the S-HMM.
Taking into account that our distortion model is
simpler than that of S-HMM, these results seem
natural, and it would be reasonable to expect that
replacing our distortion model with more sophisti-
cated one might improve the performance.
When we look at Precision/Recall curves and
AER curves in Figures 2a and 2d, the HMT model
is performing slightly better in the range of 50 to
60 % precision and 0.15 to 0.35 posterior thresh-
old with the Soft-Union strategy. Results in Fig-
ures 2b and 2e show that the HMT model performs
better around the range around 60 to 70 precision
and it corresponds to 0.2 to 0.4 posterior thresh-
old with the competitive thresholding heuristic. In
addition, results on both strategies show that per-
formance curve of the HMT model is more peaked
than those of HMM alignment models.
We suspect that a part of the reason behind such
behavior can be attributed to the fact that the HMT
model?s distortion model is more uniform than that
of HMM models. For example, in our model, all
sibling nodes have the same distortion probability
from their parent node. This is in contrast with the
situation in HMM models, where nodes within a
fixed distance have different distortion probabili-
ties. With more uniform distortion probabilities,
many links for a target word may have a consider-
able amount of posterior probability. If that is true,
too many links will be above the threshold when it
is set low, and too few links can exceed the thresh-
old when it is set high. More sophisticated distor-
tion model may help mitigate such sensitivity to
the posterior threshold.
6 Related Works
Lopez and Resnik (2005) consider an HMM
model with distortions based on the distance in
dependency trees, which is quite similar to our
model?s distance. DeNero and Klein (2007) pro-
pose another HMM model with syntax-based dis-
tortions based on the path through constituency
trees, which improves translation rule extraction
for tree-to-string transducers. Both models as-
508
(a) Precision/Recall Curve with Soft-Union. (b) Precision/Recall Curve with Soft-Union + Competi-
tive Thresholding.
(c) Precision/Recall Curve with the Best Strategy. (d) Alignment Error Rate with Soft-Union.
(e) Precision/Recall Curve with Soft-Union + Competi-
tive Thresholding.
(f) Alignment Error Rate with with the Best Strategy.
Figure 2: Precision/Recall Curve and Alignment Error Rate with Different Models and Strategies.
509
sume a chain structure for hidden variables (align-
ment) as opposed to a tree structure as in our
model, and condition distortions on the syntactic
structure only in one direction.
Nakazawa and Kurohashi (2011) propose
a dependency-based phrase-to-phrase alignment
model with a sophisticated generative story, which
leads to an increase in computational complexity
and requires parallel sampling for training.
Several supervised, discriminative models use
syntax structures to generate features and to guide
the search (Burkett et al, 2010; Riesa and Marcu,
2010; Riesa et al, 2011). Such efforts are orthog-
onal to ours in the sense that discriminative align-
ment models generally use statistics obtained by
unsupervised, generative models as features and
can benefit from their improvement. It would be
interesting to incorporate statistics of the HMT
word alignment model into such discriminative
models.
Z?abokrtsky` and Popel (2009) use HMT mod-
els for the transfer phase in a tree-based MT sys-
tem. While our model assumes that the tree struc-
ture of alignment variables is isomorphic to tar-
get side?s dependency tree, they assume that the
deep-syntactic tree of the target side is isomorphic
to that of the source side. The parameters of the
HMT model is given and not learned by the model
itself.
7 Conclusion
We have proposed a novel word alignment model
based on the Hidden Markov Tree (HMT) model,
which can incorporate the syntactic structures of
both sides of the language into unsupervised word
alignment in a tractable manner. Experiments on
an English-Japanese dataset show that our model
performs better than the IBM Model 4 and com-
parably to the HMM alignment models in terms
of alignment error rates. It is also shown that the
HMT model with a simple tree-based distortion
is sensitive to posterior thresholds, perhaps due to
the flat distortion probabilities.
As the next step, we plan to improve the dis-
tortion component of our HMT alignment model.
Something similar to the syntax-sensitive distor-
tion model of DeNero and Klein (2007) might be
a good candidate.
It is also important to see the effect of our
model on the downstream translation. Apply-
ing our model to recently proposed models that
directly incorporate dependency structures, such
as string-to-dependency (Shen et al, 2008) and
dependency-to-string (Xie et al, 2011) models,
would be especially interesting.
Last but not least, though the dependency struc-
tures don?t pose a hard restriction on the align-
ment in our model, it is highly likely that parse
errors have negative effects on the alignment ac-
curacy. One way to estimate the effect of parse
errors on the accuracy is to parse the input sen-
tences with inferior models, for example trained
on a limited amount of training data. Moreover,
preserving some ambiguities using k-best trees or
shared forests might help mitigate the effect of 1-
best parse errors.
Acknowledgments
We thank anonymous reviewers for insightful sug-
gestions and comments.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational linguistics,
19(2):263?311.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint Parsing and Alignment with Weakly Synchro-
nized Grammars. In Proceedings of NAACL HLT
2010, pages 127?135.
Hyeokho Choi and Richard G. Baraniuk. 2001. Mul-
tiscale Image Segmentation Using Wavelet-Domain
Hidden Markov Models. IEEE Transactions on Im-
age Processing, 10(9):1309?1321.
Matthew S. Crouse, Robert D. Nowak, and Richard G.
Baraniuk. 1998. Wavelet-Based Statistical Signal
Processing Using Hidden Markov Models. IEEE
Transactions on Signal Processing, 46(4):886?902.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of LREC?06, pages 449?454.
John DeNero and Dan Klein. 2007. Tailoring Word
Alignments to Syntactic Machine Translation. In
Proceedings of ACL 2007, pages 17?24.
Jean-Baptiste Durand, Paulo Gonc?alve`s, and Yann
Gue?don. 2004. Computational Methods for Hid-
den Markov Tree Models-An Application to Wavelet
Trees. IEEE Transactions on Signal Processing,
52(9):2551?2560.
510
J.-B. Durand, Y. Gue?don, Y. Caraglio, and E. Costes.
2005. Analysis of the plant architecture via tree-
structured statistical models: the hidden Markov tree
models. New Phytologist, 166(3):813?825.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% Solution. In Proceedings of
HLT-NAACL 2006, Short Papers, pages 57?60.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of ACL 2007, Demonstration Ses-
sion, pages 177?180.
Taku Kudo and Yuji Matsumoto. 2002. Japanese De-
pendency Analysis using Cascaded Chunking. In
Proceedings of CoNLL-2002, pages 63?69.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by Agreement. In Proceedings of HLT-NAACL
2006, pages 104?111.
Adam Lopez and Philip Resnik. 2005. Im-
proved HMM Alignment Models for Languages
with Scarce Resources. In Proceedings of the ACL
Workshop on Building and Using Parallel Texts,
pages 83?86.
Toshiaki Nakazawa and Sadao Kurohashi. 2011.
Bayesian Subtree Alignment Model based on De-
pendency Trees. In Proceedings of IJCNLP 2011,
pages 794?802.
Graham Neubig. 2011. The Kyoto Free Translation
Task. http://www.phontron.com/kftt.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational linguistics, 29(1):19?51.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and
Interpretable Tree Annotation. In Proceedings of
COLING/ACL 2006, pages 433?440.
Javier Portilla, Vasily Strela, Martin J. Wainwright,
and Eero P. Simoncelli. 2003. Image Denoising
Using Scale Mixtures of Gaussians in the Wavelet
Domain. IEEE Transactions on Image Processing,
12(11):1338?1351.
Jason Riesa and Daniel Marcu. 2010. Hierarchical
Search for Word Alignment. In Proceedings of ACL
2010, pages 157?166.
Jason Riesa, Ann Irvine, and Daniel Marcu. 2011.
Feature-Rich Language-Independent Syntax-Based
Alignment for Statistical Machine Translation. In
Proceedings of EMNLP 2011, pages 497?507.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, pages 577?585.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-Based Word Alignment in Statisti-
cal Translation. In Proceedings of COLING 1996,
pages 836?841.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A
Novel Dependency-to-String Model for Statistical
Machine Translation. In Proceedings of EMNLP
2011, pages 216?226.
Zdene?k Z?abokrtsky` and Martin Popel. 2009. Hidden
Markov Tree Model in Dependency-based Machine
Translation. In Proceedings of ACL-IJCNLP 2009,
Short Papers, pages 145?148.
511
