Coping With Ambiguity in a Large-Scale Machine Translation System 
Kathryn  L. Baker ,  A lexander  M.  F ranz ,  Pamela  W. Jo rdan ,  
Teruko  Mi tamura ,  E r i c  H. Nyberg ,  3 rd  
Center  for Mach ine  Trans lat ion  
Carneg ie  Mel lon  Un ivers i ty  
P i t tsburgh,  PA 15213 
Topical Paper: machine translation, parsing 
Abstract 
In an interlingual knowledge-based machine trans- 
lation system, ambiguity arises when the source 1.qn- 
guage analyzer produces more than one interlingua 
expression for a source sentence. This can have a 
negative impact on translation quality, since a tar- 
get sentence may be produced from an unintended 
meaning. In this paper we describe the ,nethods 
nsed in the KANT machine translation system to 
reduce or eliminate ambiguity in a large-scale ap- 
plication domain. We also test these methods on a 
large corpus of test sentences, in order to illustrate 
how the different disambiguation methods redtuce 
the average number of parses per sentence, 
1 In t roduct ion  
The KANT system \[Mitamura etal., 1991\] is a system for 
Knowledge-basexl, Accurate Natural-language Translation. 
The system is used in focused technical domains for multi- 
lingual translation of controlled source language documents. 
KANT is an interlingua-based system: the sonrce language 
analyzer produces an interlingua expression for each source 
sentence, and this interlingua is processed to produce the 
corresponding target sentence. The problen3 el' ambiguity 
arises when the system produces more that~ ()tie interlingua 
representation for a single input sentence. If the goal is to 
automate translation and produce output hat does not require 
post-editing, then the presence of ambiguity has a negative 
impact on translation quality, since a target sentence may he 
produced from an unintended meaning. When it is possible 
to limit tile interpretations of a sentence to just those that are 
coherent in the translation domain, then the accuracy of the 
MT system is enhanced. 
Ambiguity can occnr at different levels of processing in 
source analysis. In this paper, we describe how we cope 
with ambiguity in the KANT controlled lexicon, grammar, 
and semantic domain model, and how these :ire designed to 
reduce or eliminate ambiguity in a given translation domain. 
2 Const ra in ing  the  Source  Text  
The KANT domain lexicon and grammar are a constrained 
subset of the general source language lexicon and gra,nmar. 
The strategy of constraining the source text has three main 
I 
I 
l-:igurc 1: The KANT System 
goals. First, it encourages clear and direct writing, which 
is beneficial to both the reader of tile source text and to the 
translation process. Second, it facilitates consistent writing 
among tile many authors who use the system and across all 
document types. And third, the selection of unambiguous 
words :111(I constructions tobe used during authoring reduces 
the necessity for ambiguity resolution during the auto,natic 
stages of processing. It is important to reduce the processing 
overhead associaled wilh amhiguity resolution in order tokeep 
tile system fast enough for on-line use. 
2.1 The l)omain Lexicml 
The domain lexicon is built using corpt, s analysis. Lists 
of terms, arranged by part of speech, are automatically ex- 
tracted from the corpus \[Mitamura etal., 1993\]. "File lexicon 
consists of closed-class general words, open-class general 
words, idioms, and nomenclature phrases. Closed-class gen- 
eral words (e.g. the, with. should) are taken from general 
English. Open-class general words (e.g. drain, run, hot) are 
limited in the lexicon to one sense per part of speech with 
some exceptions ~. Idioms (e.g. on and off) and nomencl> 
tnre phrases (e.g. summing valve) are domain-specilic and 
are limited to those phrases identilied in the domain corpus. 
Phrases, too, are delined with a single sense. Special vncab- 
t Far example, in the heavy-equipment lexicon, there are a few 
hundred terms out of 60,000 which have more than one sense per 
part of speech. 
90 
ulary items, including symbols, abbreviations, and the like, 
,are restricted in use and are chosen for the lexicon in collab- 
oration with domain experts. Senses for prepositions, which 
are highly ambiguous and context-dependent, are determined 
(luring processing using the semantic domain model (of. Sec- 
tion 4). 
Nominal compounds in the domain may be several words 
long. Because of the potential ambiguity associated wit h com- 
positional parsing of nominal compounds, non-productive 
nominal compounds are listed explicitly ill tile lexicon as 
idioms or nomenclature phrases. 
2.2 Controlled Grammar 
Some constructions in the general source l,'nlgtmge that arc in- 
herently ambiguous are excluded from the restricted grammar, 
since they may l~td to multiple analyses during processing: 
? Conjunction of VPs, ADJs, or ADVs e.g. *Extend and 
retract the cylinder. 
? Pronominal reference, .g. *Start the engine and keel) it 
running. 
? Ellipsis, e.g. reduced relative clauses: *the tools !~t  
for the procedure 
? Long-distance dependencies, snch as interrogatives and 
object-gap relative clauses, e.g. The parts which the 
service representative ordered. 
? Nominal compounding which is not explicitly coded in 
the phrasal lexicon. 
On the other h,'md, tim grammar inchules the following con- 
structions: 
? Active, passive and imperative sentences, e.g. Start the 
engine. 
? Conjunction of NPs, PPs or Ss. Sentences may be con- 
joined using coordinate or subordinate con jr, notions, e.g. 
If you are on the last parameter, ~zen lhe program pro- 
ceeds to the lop. 
? Subject-gap relative clauses, e.g. The service represen- 
tative can determine the parts which are faulty. 
Tile recommendations i  tile controlled grammar include 
guidelines for authoring, such as how to rewrile a text from 
general English into the domain language. Authors are ad- 
'vised, for example, to choose the most concise terms available 
in the lexicon and to rewrite long, conjoined sentences into 
short, simple ones. The recommendations are useful both 
for rewriting old text and creating new text (set l:igure 2 for 
examples). 
Example 1: Rewrite Anaphnric Use (1t' Numerals 
Problematic Text: 
Suggested Rewrite: 
Loosen tile smaller (me first. 
Loosen the smaller bolt ftrst. 
Example 2: Use Concise Vocabulary 
Problematic Text: The parts must be 
put ba_ck toget h!'L. 
Suggested Rewrite: The parts must be (easse~tbl#d. 
Figure 2: Grammar  Recommendatiml Examl)les 
2.3 SGML Text Markup 
q'he grammar makes use of Standard Generalized Markup 
Language (SGML) text markl,p tags. The set of markup tags 
for our applicatiou were developed in conjunction with do- 
,nain experts. A set of domain-specific tags is used not only 
to demarcate tile text but also to identify tile content of poten- 
tially ambiguous expressions, and to help during vocabl,lary 
checking. For example, at the lexical level, number tags 
identify numerals as diagram callouts, part munbers, product 
model numbers, or parts of measurement exl)ressions. At 
the syntactic level, rules for tag combinations restrict how 
phrases rnay be constructed, aswith tagged part rmmbers an(l 
part names (see Figure 3 for an example). 
'\['tie <p~l/"t;no> 4S152-1  </parLno> <parLr~amo> 
Hose  A~{sornt ) \ ]y  </parLname> <Cd\ ] IouL> l 
</ca  l lout :> el  t.he <pa l t r lo> 5'\['65-'\]q 
< / \[)El #'L NO> < f)/l \[ \[ lID, Ill(}-" \[~i~ El k (. ~ COl"tLr o \[ G lTOklp  
<~/~)~I/~LEIglmQZ > IIILI~;L nOW k)() connOcLed  Lo  Lho 
<parLno> 4K2986 </par t :no> <partzname> 
Arl(:!lOi TOO </D~lrt.r/,lm~?:, . 
t.'igure 3: Sample S(;ML Text Mark-Up 
3 Granun 'w Des ign  I ssues  
The parser in KANT is based on the "Universal Parser" 
\[Tonfita nd Carbonell, 19871. "File gramnmr consists of 
context-free rules that define tile input's constitt,ent s ruc- 
ture (c-structure) and these rules are annotated with con- 
straint equations that define the input's functional structure 
(f-structure). 'l'omita's parser compiles the gratnnmr into an 
l.R-table, and the constraint equations into Lisp code. Al- 
though this compilation results in f.'lst run-time parsing, the 
need to minimize ambiguity still exists. 
One source of ambiguity is the attachment site for a prepo- 
sitional phrase, llowever, many of the PP attachments are 
encoded irectly.in the gramma," because tile syntactic on- 
text indicates an unanfl)iguous attaclunent site. For example: 
+ A partitive where the PP attaches to the noun: a gallon 
of antifreeze. 
) ) )  ? A pre-sentential I P where tile I \[ attaches to the sentence: 
For this test, ensttre thor a signal line is connected from 
lhe pump outpul to the pump compensator. 
? A PI' attaches to the verb be when there is no predicale 
adjective: The trm:'k is in the shop. 
? A ditransitive verb where the PP attaches to the verb: 
Give your suggestions to tile dealer. 
,, A stand-alone PP inside ;m SGML tag such as QUAL- 
II"IER where tile PP attaches to tile MDLDESC tag 
contents: Inspect <mdldesc> all track-type trac- 
tors <qualifier> with hydraulic heads </qualifier> 
</mdldesc>. 
3.1 l'assive vs. Cnpt,l'lr with Participial? 
There are many adjectives in English that have tile same form 
. as ,'m -ed participle, l:or example: 
7"tie radius is poorly formed. (adjective) 
The calibration mode is enabled by moving the 
rocker switch. (participle) 
i 
"R} distinguish the qdjectival from the participial form we 
have added two heuristics to tile constraint rules of the gram- 
mar. The litst is to use verb class mapping information, If the 
91 
verb is classified as being more active than stative, then tile 
passive reading is preferred. So, for example, an intransitive 
verb would indicate an adjectival reading: 
The display is faded. (adjective) 
The second heuristic uses the notion of "quasi-agents". 
There are several prepositions that can introduce "quasi- 
agents" \[Quirk et al, 197211, such as: about, at, over, to, 
with. If the domain model indicates that the -ed verb is a 
possible attachment site for a prepositional phrase occurring 
in the sentence, then the passive reading is preferred. 
These two heuristics are incorporated into tile constraints 
of rules involving predicate adjectives. If the -ed feral is 
classified as active, or if there is at PP in the sentence that 
can attach to the -ed verb form, tfien tile adjectival reading 
is ruled out. In the constraints of rules for the passive, tim 
passive reading is ruled out if the -ed form is classified as 
smtive. 
3.2 Adverb or Adjective? 
For tile most part, eacfi word in the system is limited to one 
meaning per part of speeclt. So while we have nearly elimi- 
nated one source of lexical ambiguity, there is still the l)roblem 
of ambiguity between the various parts of speech for a par- 
ticuhlr word. While ambiguity between, lbr example, a noun 
and a verb is usually resolved by the syntactic context, parts of 
speech that participate in similar contexts are still a problem. 
For example, the content of the SGML tag, POSITION, can 
be an adjective or adverb phrase and "as \[ <adj >l<ad v>\] as" 
can contain either an adjective or an adverb. This means that 
an input such as "as fast as" would have two analyses. We 
Imve found witll our domain that tile COtTeCt hing to do is 
to prefer the adverb reading. We put this preference directly 
into the constraints of rules involvingadjectives forwhich the 
same context allows an adverb. If the word is also an ad- 
verb then tim adjective rule will fail. This allows tile adverb 
reading to be preferred. 
4 Semantic Donmin Model 
We have implemented a practical method for integrating se- 
mantic rules intoan LR parser. The resulting system combines 
the merits of a semantic domain nlodel with the generality and 
wide coverage of syntactic parsing, and is fast and efficient 
enough to remain practical. 
4.1 Interleaved vs. Sentence-final Constraints 
Some previous knowledge-l)ased natural hmglmge analysis 
systems have constructed tile semantic represent'ilion for the 
sentence in tandem with syntactic parsing, lit this schenle 
semantic onstr;fints from tile domain model filter out se- 
mantically ill-lormed representations and kill tile associated 
pro'sing path. Examples include AIISITY tHirst, 19861 and 
KBMT-89 \[Goodman anti Nireuburg, 1991\]. Other inevious 
systems have delayed semantic interpretation a d al)plicalion 
of semantic well-formedness constraints until after tile syn- 
tactic parse. 
Both of these schemes entail performance problems. The 
solution to this probleln lies ill importing the right type and 
right amount of semantic information into syntactic lmrsing. 
Iu KANT, the relevant knowledge sonrces are reorganized into 
data structures that arc optimized for ambiguity resolution 
during parsing. 
4.2 Example of Attachment Ambiguity 
Tile knowledge-based disambiguation scheme covers Prepo- 
sitional Phrase attachment, Noun-Notre conlponnding, and 
Adjective-Noun attachment. The remainder of this section 
discusses examples involving PP-attacl/m~nt. The syntactic 
grammar contains two rules that allow these attachments: 
VP ,---- VP PP 
NP ,--- NP PP 
Consider tim sentence Measure the voltage with the voltmeter. 
Syntactically, the PP with the voltmeter can modify either tile 
verb measure, or tile noun voltage. 
4.3 Slructure and Content of tile I)omain Model 
We use knowledge abol,t the domain to resolve ambiguities 
like PP-attachment. Tile domain model cent;fins all of the 
semantic oncepts in the domain. Leaf concepts, such as 
*O-VOLTMETER, correspond closely to linguistic expres- 
sions. The concepls are. arranged in an inheritance hierarchy, 
and other concepts, uch as *O-MEASURING-DEVICE, rep- 
resent abstract concepts. The domain model is implemented 
as a hierarchy of concepts. Constraints on possible attributes 
of concepls, along with semantic onstraints on the fillers, are 
inherited through this hierarchy. Figure 4 shows an example. 
? 
( "a- Pl A 61~JSl I('-,~C'll O~ "N) 
(INglRtlMFNr ~'}- ME& ~ URDdE brr. \[&~'l(l:') &B.A 
Figure 4: Excerp! t'rnm l)unutiu Model 
4.4 Using Semantics in tile Syntax 
In order tO keel) parsing traclable, the domain model is con- 
suited at the earliest possible stage during parsing. Every 
grannnar ule that involves an attachment decision that is 
subject o knowledge-based disamhigv'ltion calls a function 
that consults the domain model, and allows the gramn/ar rule 
to succeed only if the attachmcut is sclnantically licensed. 
The grammar formalism allows procedural calls to be made 
directly fron/tim gramnmr ules. The function that performs 
or deuies attachment based on the domaiu model is called 
sere-attach. 
The inpttts to the sem-at t  ach  function are the functiomll 
structures (f-strttcturcs) lk)r the potential attachment site, tile 
structure to be attached, and the type of attachment (e.g., PP 
= t:'repositional Phrase). sere -a t tach  consults inlbrm,'ltion 
from the domain model to decide whether the attachment is 
semantically licensed. This process is described in the next 
subsection. 
4.5 Steps in Sem,'mtle Disaml)iguation I 
There are three main steps in selnanlic disambiguation of
possib\]e syntactic attachments: (1) mapping from syntax to 
92 
semantic concepts using tile lexical mappiug rules; (2) check- 
ing inform,'ttkm from the domain model; and (3) determining 
semantic roles using tile semantic interpretation rules. 
?7   v :iiii 
Figure 5: Lexical Mapping Rules 
Lexical Mapping Rules. The first step is to real I from 
syntactic structures to semantic oncepts. The lexical map- 
ping rnles associate syntactic lexicon entries with concepls 
from the Domain Model (Figure 5). 
Domain Model. "File second step consists in looking up 
the appropriate concepts in the Domain Model (Figure 4). 
Semantic Interpretation Rules. The third step consists 
of consulting the semantic interpretation rules to determine 
whether the concepts from tile sentence can lo,'m approl~ri- 
ate modilication relationships. Semantic interpretation rules 
describe the mapping from the syntactic representation to the 
frmne-based semantic representation, An interpretation rub 
consisls of a syntactic path (an index into tile f-structure), a 
semantic Imth (an index into tile senmntic frame), and an op. 
tional syntactic onstraint on the mapping rule. For exmnple, 
below is mt interpretation rule for the INSTRUMENT role: 
( : :~yn-pat .h  IPP OBJ) 
: sem~path  ZN.qTRUNENT 
: sys-consttraint  
((pp ((root (*OR* "wlth .... by")) })) ) 
Eflicient Run-time Use. In order to make this process as 
efticient as possible, aml to minimize delays during parsing, 
the knowledge described in this section is reorganized off- 
line I)cfore parsing. The result of this reorganization are 
data strtletnres known as S&*;'lalllic" restrictors. The SelllLIIlliL' 
restrictors have three main properties: 
1. They are indexed by head concept, and provide a list of 
all approl)riate modiiiers. 
2. All inheritance in the Domain Model is performed off- 
line, so that the restrictors contain all necessary informa.. 
tion. 
3. The semantic restrictors are stored in a Slmce-efficient 
structure+shared manne, +.
S Author  l ) i sambiguat io t !  
Once KANT has analyzed a source sentence and all l)OS- 
s ine disambiguations h;tve been performed, there may still 
be more than one interlingua representation for tim sentence. 
This occurs when the sentence is truly ambiguous, i.e., it hns 
more than one acceptable domain interpretation. I  this case, 
KANT makes use of disambiguation by the author - -  tile 
ambiguity is described to the author and the author is then 
pmml)ted to select the desired interpretation. The choice is 
"remembered" byplacing extra in fomuttion into tile input text 
at the point of alnbiguity. There are two types of ambiguity 
cnrrently addressed by author disambit~uation: 
? Lexical Ambiguity. When more than one interlingua is
produced because a certain word or phrase ean be in  
terpreted in more than one way (iv. as two different 
concepts), then the author is prompted to select he de- 
sired meaning. 
* Structural kmbigt.tity. When more than one attachment 
site is possible for a phrase like a prel)o~ilional phrase, 
the different attachments are glossed for tile :luther, who 
is then prompted to select ile desired inteq)retation. 
Since author disambiguation is utilized only when the sen- 
tence cannot I)c disambiguated by other nteans, it will not 
occur very frequently once tile system is complete. On tile 
other hand, having such a mech,'mism available during system 
development is very helpful, since it helps to point out where 
there is residu-d ambiguity left to be addressed by knowledge 
St} tlrce ieli neltlent. 
6 Test ing  l ) i saml f igua l ion  Methods  
When disambiguation methods are int,oduced, the number 
of parses per sentence can be reduced dramatically. If we 
use a general lexicon and grammar to parse texts lro\[n ~.1 spe- 
cialized dolnain corpus (rather than a general corpus),  then 
more lmrses will be assigned than those thai are desired in 
the dOlnain. Figure 6 illustrates how the successive introduc- 
tion of disambiguation ntethods reduces the set of l)ossiblc 
parses to just those desired in tile domain. The smallest set of 
interpretations is that remaining after tile controlled lexicon, 
gla\[nllrar, seln\[illlic restrictions, and author disambigtmtion 
have \[)cell applied; in practice this sel ,,viii contain just one 
interpretation, since the author will select only the intended 
interpretation. 
# Inlerprelalion~ 
Ouin~l General  
I N Inle llWUt aliot~ U ~+ing Dotu,~irl h l le lpte lu l lo l l l  
\[}i~ambi~ualiolL 
Followia~ A~Jlllo? 
l:igure 6: Reducing the Set of Possible lnterl)retatinns 
Wc have experimented with tile KANT analyzer in order to 
determine the effects of the different disambiguation strate- 
gies mentioned above. We used a test suite containing 891 
sentences which is used fo, regression testing during system 
development. The sentences in the test suite range in lenglh 
fronl t word to over 25 words. 
General lexicon entries were derived automatically from the 
online version of Wcbster's 7th dictionary. Webster's includes 
55,000 reels that are in at least one open class category (verh, 
ram,, adjective, adverb). One diclionary entry was created 
for each sense of one of these dalegories, This resulted in 
117,000 lexicon entries. The constrained lexicon consists of 
10,000 words and 50,000 phrases talk)red to the application 
9.3 
domain. For the results listed below, the "general lexicon" 
consists of the constrained lexicon plus the general entries 
from Webster's. 
The constrained grammar has been tailored to the restricted 
source language for the domain (of. Section 2). In ,'tddition, it 
includes a number of constraint annotations and parse prefer- 
ences that limit the number of ambiguous parses (cf. Section 
3). A general grammar was derived from the constrained 
grammar by removing most restrictions and constraints on 
specific rules, leaving only the most general constraints such 
as subject-verb agreement. 
When noun-noun compounding is allowed, sequences of 
nouns may form NPs even if they ~u'e not listed as nomencla- 
ture phrases in the lexicon. Each such sequence isonly parsed 
one way; the parser does not build different smmtures for the 
sequence of nouns, but just reads them into a list. 
In order to reduce the exponential complexity of some of 
the longer sentences, all test results were produced using the 
"shared packed forest" method of ambiguity packing for am- 
biguity internal to a sentence \[Tomita, 1986\]. The results 
for "parses per sentence" is simply the average for all the 
sentences. 
Test LEX GRA N-N DM P 
1 GEN GEN YES NO 27.0 
2 GEN GEN NO NO 10.2 
3 GEN CON YES NO 8.4 
4 CON GEN YES NO 1.7 
5 CON GEN NO NO 1.6 
6 CON CON NO YES 1.5 
LEX: Lexicon GEN: General 
GILA: Grammar CON: Constrained 
N-N: Noun-Noun Compounding 
DM: Semantic Restriction with l)omain Model 
Figure 7: Testing Disambiguation Methods (12/17/93) 
The results of this testing ~u'e shown in Figure 7. Test 1 is 
the baseline result for parsing with a general lexicon, general 
grammar, noun-noun compounding and no semantic restric- 
tions. As expected, the average number of parses per sentence 
is quite high (27.0). Limiting noun-noun compounding ('lest 
2) cuts this number by more than hal f, yielding 1(/.2 parses per 
sentence. Note that a similar effect is achieved if we run the 
test with a controlled grammar and noun-noun compounding 
(Test 3, 8.4 parses per sentence). 
Constraining the lexicon seems to achieve the largest re- 
duction in the average number of parses per sentence (Tests 4, 
5, 6), with elimination of noun-notre compounding yielding 
only slight improvements when the lexicon has already been 
restricted. As expected, the best results are achieved when the 
system is run with constrained lexicon and grammar, no noun- 
noun compounding, and semantic restriction with a domain 
model (Test 6). 
We expect that tile primary reason wily tile addition of 
semantic restrictions from a domain model does not have a 
greater impact is due to tile incomplete natttre of the domain 
model we used in the experiment. The domain model used in 
the experiment captures the domain relationships associated 
with prepositional phrase attachment to VP and object N1; lint 
there are several areas of the domain model still under devel- 
opment. When complete, these will further educe ambiguity 
by placing additional limitations on the following: 
? The semantic lassification of words inside particular 
SGML tags; 
,, Attachment of prepositional phrases to suhject NP; 
? Attachment of inlinitive clauses; 
,, Attachment of relative clauses. 
This testing has proved extremely useful ill prioritizing the 
level of effort expended oil different disambiguation methods 
dnring system development. As is often the case, theoreti- 
cally interesting or difticult issues (such as noun-noun com- 
ponnding) are reduced in effect when other domain-related 
restrictions are put in place (such as a controlled lexicon). 
On the other hand, this type of testing can also identify Ihe 
areas of the system (such as the semantic domain model) 
which are not reducing ambiguity as much as expected. In 
our ongoing work, we will complete the domain model for 
the KANT he,'tvy-eqtfipment application in those areas men- 
tioned above; in the process, we expect o rednce the average 
number of parses per sentence in the most constrained ease. 
7 Ac l (nowledgements  
We would like to thank Jaime Carbonell, Radha Rao, and 
Todd Kaufinann, 'rod all of ottr colleagttes on the KANT 
project, inchtding James Altucher, Nicholas Brownlow, Mil- 
dred Galarza, Sue Hohn, Kathi lannamico, Kevin 1(eck, Mar- 
ion Kee, Sarah Law, John Leavitt, Daniela Lonsdale, Deryle 
Lonsdale, Jeanne Mier, Venkatesh Narayan, Amalio Nieto, 
and Will Walker, our sponsors at Caterpillar Inc., and onr 
colleagues at Carnegie Group. 
References  
\[Chevalier tal., 1978\] Chevalier, M., Danscre:m, J., and 
Poulin, G. (1978). TatmHnetco: description dn systOne. 
Technical report, Groupe (le recherches pour la traduction 
automatique, Universit6 de MontrS,ql. 
\[Goodman and Nirenburg, 19911 Goodman, K. and Niren- 
burg, S. (1991). The KBMT Project: A Case Study in 
Knowledge-Based Machine Translation. Morgan Kauf- 
ulan\[1, S:.ln Mated, CA. 
\[Hirst, 1986\] Hirst, G. (1986). Semantic Inlerpretation and 
tile Resolution of Ambiguity. Cambridge University Press, 
Cambridge. 
\[Mitamura etal., 1991\] Mitamura, T., Nybcrg, E., and Car- 
bonell, J. (1991). An efficient interlingua translation sys- 
tem for multi-lingtmldocument prodttction. InProceedings 
of Machine Translation Summit I11, Washington, DC. 
\[Mitamnra etal., 19931 Mitamura, T., Nyberg, E., anti Car- 
bonell, J. (1993). Automated corl)uS analysis ,'rod the acqui- 
sition of large, multi-lingual knowledge hases for MT. In 
5th International Cotlference on 771eoretical and Method- 
ological Issues in Machine Translation, Kyoto, Japan. 
\[Quirk et al, 1972\] Quirk, R., Grccnbaum, S.. Leech, G., and 
Svartvik, J. (1972). A Grammar of Contemporary English. 
l_ongman Group UK Limited, Essex Engl,'md. 
\[Suzuki, 19921 Suzuki, M. (1992). A method of utilizing 
domain and langnage-sl)ecitic conslraints i l l  dialog trans- 
lation. In Coling-92. 
\[Tonfita, 1986 \] Tom ira, M. (1986). Efficient Parsing for Nat- 
ural Language. Kluwer Academic Publishers, Boston, 
MA. 
\[qk)mita nd Carbonell, 19871 "ll)mita, M. and Carbonell, J. 
(1987). "i'he Universal Parser architecurc Rlr Km)wledge- 
based Machine Translation. Technical Report CMU-CMT- 
87-101, Center for Machine Translation, Carnegie Mellon 
University. 
94 
