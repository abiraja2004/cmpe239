Coling 2010: Poster Volume, pages 605?613,
Beijing, August 2010
Best Topic Word Selection for Topic Labelling
Jey Han Lau,?? David Newman,?? Sarvnaz Karimi? and Timothy Baldwin??
? NICTA Victoria Research Laboratory
? Dept of Computer Science and Software Engineering, University of Melbourne
? Dept of Computer Science, University of California
jhlau@csse.unimelb.edu.au, newman@uci.edu, skarimi@unimelb.edu.au, tb@ldwin.net
Abstract
This paper presents the novel task of best
topic word selection, that is the selection
of the topic word that is the best label for
a given topic, as a means of enhancing the
interpretation and visualisation of topic
models. We propose a number of features
intended to capture the best topic word,
and show that, in combination as inputs to
a reranking model, we are able to consis-
tently achieve results above the baseline of
simply selecting the highest-ranked topic
word. This is the case both when training
in-domain over other labelled topics for
that topic model, and cross-domain, us-
ing only labellings from independent topic
models learned over document collections
from different domains and genres.
1 Introduction
In the short time since its inception, topic mod-
elling (Blei et al, 2003) has become a main-
stream technique for tasks as diverse as multi-
document summarisation (Haghighi and Vander-
wende, 2009), word sense discrimination (Brody
and Lapata, 2009), sentiment analysis (Titov and
McDonald, 2008) and information retrieval (Wei
and Croft, 2006). For many of these tasks, the
multinomial topics learned by the topic model can
be interpreted natively as probabilities, or mapped
onto a pre-defined discrete class set. However,
for tasks where the learned topics are provided
to humans as a first-order output, e.g. for use in
document collection analysis/navigation, it can be
difficult for the end-user to interpret the rich sta-
tistical information encoded in the topics. This
research is concerned with making topics more
readily human interpretable, by selecting a single
term with which to label the topic.
Although topics are formally a multinomial dis-
tribution over terms, with every term having finite
probability in every topic, topics are usually dis-
played by printing the top-10 terms (i.e. the 10
most probable terms) in the topic. These top-10
terms typically account for about 30% of the topic
mass for reasonable setting of number of topics,
and usually provide sufficient information to de-
termine the subject area and interpretation of a
topic, and distinguish one topic from another.
Our research task can be illustrated via the top-
10 terms in the following topic, learned from a
book collection. Terms wi are presented in de-
scending order of P (wi|tj) for the topic tj :
trout fish fly fishing water angler stream rod
flies salmon
Clearly the topic relates to fishing, and indeed,
the fourth term fishing is an excellent label for the
topic. The task is thus termed best word or most
representative word selection, as we are selecting
the label from the closed set of the top-N topic
words in that topic.
Naturally, not all topics are equally coherent,
however, and the lower the topic coherence, the
more difficult the label selection task becomes.
For example:
oct sept nov aug dec july sun lite adv globe
appears to conflate months with newspaper
names, and no one of these topic words is able to
capture the topic accurately. As such, our method-
ology presupposes an automatic means of rating
topics for coherence. Fortunately, recent research
by Newman et al (2010) has shown that this is
achievable at levels approaching human perfor-
mance, meaning that this is not an unreasonable
assumption.
Labelling topics has applications across a di-
verse range of tasks. Our original interest in the
605
problem stems from work in document collection
visualisation/navigation, and the realisation that
presenting users with topics natively (e.g. as rep-
resented by the top-N terms) is ineffective, and
would be significantly enhanced if we could au-
tomatically predict succinct labels for each topic.
Another application area where labelling has been
shown to enhance the utility of topic models is se-
lectional preference learning via topic modelling
(Ritter et al, to appear). Here, topic labelling via
taxonomic classes (e.g. WordNet synsets) can lead
to better topic generalisation, in addition to better
human readability.
This paper is based around the assumption that
an appropriate label for a topic can be found
among the high-ranking (high probability) terms
in that topic. We assess the suitability of each term
by way of comparison with other high-ranking
terms in that same topic, using simple pointwise
mutual information and conditional probabilities.
We first experiment with a simple ranking method
based on the component scores, and then move
on to using those scores, along with features from
WordNet and from the original topic model, in a
ranking support vector regression (SVR) frame-
work. Our experiments demonstrate that we are
able to perform the task significantly better than
the baseline of selecting the topic word of high-
est marginal probability, including when training
the ranking model on labelled topics from other
document collections.
2 Related Work
Predictably, there has been significant work on in-
terpreting topics in the context of topic modelling.
Topic are conventionally interpreted via the top-
N words in each topic (Blei et al, 2003; Grif-
fiths and Steyvers, 2004), or alternatively by post-
hoc manual labelling of each topic based on do-
main knowledge and subjective interpretation of
each topic (Wang and McCallum, 2006; Mei et
al., 2006).
Mei et al (2007) proposed various approaches
for automatically suggesting phrasal labels for
topics, based on first extracting phrases from the
document collection, and subsequently ranking
the phrases based on KL divergence with a given
topic.
Magatti et al (2009) proposed a method for la-
belling topics induced by hierarchical topic mod-
elling, based on ontological alignment with the
Google Directory (gDir) hierarchy, and optionally
expanding topics based on a thesaurus or Word-
Net. Preliminary experiments suggest the method
has promise, but the method crucially relies on
both a hierarchical topic model and a pre-existing
ontology, so has limited applicability.
Over the general task of labelling a learned se-
mantic class, Pantel and Ravichandran (2004) pro-
posed the use of lexico-semantic patterns involv-
ing each member of that class to learn a (usu-
ally hypernym) label. The proposed method was
shown to perform well over the semantically ho-
mogeneous, fine-grained clusters learned by CBC
(Pantel and Lin, 2002), but for the coarse-grained,
heterogeneous topics learned by topic modelling,
it is questionable whether it would work as well.
The first works to report on human scoring of
topics were Chang et al (2009) and Newman et
al. (2010). The first study used a novel but syn-
thetic intruder detection task where humans eval-
uate both topics (that had an intruder word), and
assignment of topics to documents (that had an in-
truder topic). The second study had humans di-
rectly score topics learned by a topic model. This
latter work introduced the pointwise mutual infor-
mation (PMI) score to model human scoring. Fol-
lowing this work, we use PMI as features in the
ranking SVR model.
3 Methodology
Our task is to predict which words annotators
tend to select as most representative or best words
when presented with a list of ten words. Since
annotators are not generally unanimous in their
choice of best word, we formulate this as a rank-
ing task, and treat the top-1, 2 and 3 system-
ranked items as the best words, and compare that
to the top-1, 2 and 3 words chosen most frequently
by annotators. In this section, we describe the fea-
tures that may be useful for this ranking task. We
start with features motivated by word association.
An obvious idea is that the most representative
word should be readily evoked by other words
in the topic. For example, given a list of words
?space, earth, moon, nasa, mission?, which is a
606
Space Exploration topic, space could arguably be
the most representative word. This is because
it is natural to think about the word space after
seeing the words earth, moon and nasa individ-
ually. A good candidate for best word could be
the word that has high average conditional proba-
bility given each of the other words. To calculate
conditional probability, we use word counts from
the entire collection of English Wikipedia articles.
Conditional probability is defined as:
P (wi|wj) =
P (wi, wj)
P (wj)
,
where i 6= j and P (wi, wj) is the probability of
observing both wi and wj in the same sliding win-
dow, and P (wi) is the overall probability of word
wi in the corpus. In the above example, evoked by
means that space would fill the slot of wi. The av-
erage conditional probability for word wi is given
by:
avg-CP1(wi) = 19
?
j
P (wi|wj),
for j = 1 . . . 10, j 6= i (this range of indices ap-
plies to all following average quantities).
In other cases, we have the flip situation, where
the most representative word may evoke (rather
than be evoked by) other words in the list of ten
words. Imagine a NASCAR Racing topic, which
has a list of words ?race, car, nascar, driver, rac-
ing?. Given the word nascar, words from the list
such as race, car, racing and driver might come
to mind because nascar is heavily associated with
these words. Therefore, a good candidate, wi,
might also correlate with high P (wj |wi). As be-
fore, the average conditional probability (here de-
noted with CP2) for word wi is given by:
avg-CP2(wi) = 19
?
j
P (wj |wi).
Another approach to measuring word associa-
tion is by calculating pointwise mutual informa-
tion (PMI) between word pairs. Unlike condi-
tional probability, PMI is symmetric and thus the
order of words in a pair does not matter. We
calculate PMI using word counts from English
Wikipedia as follows:
PMI(wi, wj) = log P (wi, wj)P (wi)P (wj) .
The average PMI for word wi is given by:
avg-PMI(wi) = 19
?
j
PMI(wi, wj).
The topic model produces an ordered list of
words for each topic, and the ordering is given by
the marginal probability of each word given that
topic, P (wi|tj). The ranking of words based on
these probabilities indicates the importance of a
word in a topic, and it is also a feature that we use
for predicting the most representative word.
We also observe that sometimes the most repre-
sentative words are generalized concepts of other
words. As such, hypernym relations could be an-
other feature that may be relevant to predicting the
best word. To this end, we use WordNet to find
hypernym relations between pairs of words in a
topic and obtain a set of boolean-valued relation-
ships for each topic word.
Our last feature is the distributional similar-
ity scores of Pantel et al (2009), as trained over
Wikipedia.1 This takes the form of representing
the distributional similarity between each pairing
of terms sim(wi|wj); if wi is not in the top-200
most similar terms for a given wj , we assume it to
have a similarity of 0.
While the above features can be used alone
to get a ranking on the ten topic words, we can
also use various combinations of features in a
reranking model such as support vector regres-
sion (SVMrank: Joachims (2006)). Applying the
features described above ? conditional probabil-
ities, PMI, WordNet hypernym relations, the topic
model word rank, and Pantel?s distributional simi-
larity score ? as features for SVMrank, a ranking
of words is produced and candidates for the most
representative word are selected by choosing the
top-ranked words.
607
NEWS stock market investor fund trading investment firm exchange ...
police gun officer crime shooting death killed street victim ...
food restaurant chef recipe cooking meat meal kitchen eat...
patient doctor medical cancer hospital heart blood surgery ...
BOOKS loom cloth thread warp weaving machine wool cotton yarn ...
god worship religion sacred ancient image temple sun earth ...
crop land wheat corn cattle acre grain farmer manure plough ...
sentence verb noun adjective grammar speech pronoun ...
Figure 1: Selected topics from the two collections
(each line is one topic, with fewer than ten topic
words displayed because of limited space)
4 Datasets
We used two collections of text documents from
different genres for our experiments. The first col-
lection (NEWS) was created by selecting 55,000
news articles from the LDC Gigaword corpus.
The second collection (BOOKS) was 12,000 En-
glish language books selected from the Inter-
net Archive American Libraries collection. The
NEWS and BOOKS collections provide a diverse
range of content for topic modeling. In the first
case ? news articles from the past decade written
by journalists ? each article usually attempts to
clearly and concisely convey information to the
reader, and hence the learned topics tend to be
fairly interpretable. For BOOKS (with publication
dates spanning more than a century), the writing
style often uses lengthy and descriptive prose, so
one sees a different style to the learned topics.
The input to the topic model is a bag-of-words
representation of the collection of text documents,
where word counts are preserved, but word order
is lost. After performing fairly standard tokeniza-
tion and limited lemmatisation, and creating a vo-
cabulary of terms that occurred at least ten times,
each corpus was converted into its bag-of-words
representation. We learned topic models for the
two collections, choosing a setting of T = 200
topics for NEWS and T = 400 topics for BOOKS.
After computing the PMI-score for each topic (ac-
cording to Newman et al (2010)), we selected 60
topics with high PMI-score, and 60 topics with
low PMI-score, from both corpora, resulting in a
total of 240 topics for human evaluation.
The 240 topics selected for human scoring were
1Accessed from http://demo.patrickpantel.
com/Content/LexSem/thesaurus.htm.
Features Description
PMI Pointwise mutual information
CP1 Conditional probability P (wi|?)
CP2 Conditional probability P (?|wi)
TM Rank Original topic model word rank
Hypernym WordNet hypernym relationships
PDS Pantel distributional similarity score
Table 1: Description of feature sets
each evaluated by between 10 and 20 users. For
the two topic models, we used the conventional
approach of displaying each topic with its top-10
terms. In a typical survey, a user was asked to
evaluate anywhere from 60 to 120 topics. The in-
structions asked the user to perform the following
tasks, for each topic in the survey: (a) score the
topic for ?usefulness? or ?coherence? on a scale
of 1 to 3; and (b) select the single best word that
exemplifies the topic (when score=3).
From both NEWS and BOOKS, the 40 topics
with the highest average human scores had rela-
tively complete data for the ?best word? selection
task (i.e. every time a user gave a topics score=3,
they also selected a ?best word?). The remain-
der of this paper is concerned with the 40 NEWS
topics and 40 BOOKS topics where we had ?best
word? data from the annotators. Sample topics
from these two sets are given in Figure 1.
To measure presentational bias (i.e. the extent
to which annotators tend to choose a word seen
earlier rather than later, particularly when armed
with the knowledge that words are presented in or-
der of probability), we reissued a survey using the
40 NEWS topics to ten additional annotators, but
this time the top-10 topic words were presented
in random order. Again, these ten new annotators
were asked to select the best word.
5 Experiments
We used average PMI and conditional probabili-
ties, CP1 and CP2, to rank the ten words in each
topic. Candidates for the best words were selected
by choosing the top-1, 2 and 3 ranked words.
We used the following weighted scoring func-
tion for evaluation:
Best-N score =
?N
i=1 n(wrevi)?N
i=1 n(wi)
608
Features Best-1 Best-2 Best-3
Baseline 0.35 0.50 0.59
PMI 0.25 0.38 0.49
CP1 0.30 0.42 0.51
CP2 0.15 0.27 0.45
Upper bound 0.48 ? ?
Table 2: Best-1,2,3 scores for ranking with single
feature sets (PMI and both conditional probabili-
ties) for NEWS
Features Best-1 Best-2 Best-3
Baseline 0.38 0.48 0.60
PMI 0.25 0.38 0.49
CP1 0.30 0.38 0.47
CP2 0.15 0.30 0.49
Upper bound 0.64 ? ?
Table 3: Best-1,2,3 scores for ranking with single
feature sets (PMI and both conditional probabili-
ties) for BOOKS
where wrevi is the ith term ranked by the system
and wi is the ith most popular term selected by
annotators; revi gives the index of the word wi
in the annotator?s list; and n(w) is the number of
votes given by annotators for word w.
The baseline is obtained using the original word
rank produced by the topic model based on topic
word probabilities P (wi|tj). An upperbound is
calculated by evaluating the decision of an annota-
tor against others for each topic. This upperbound
signifies the maximum accuracy for human anno-
tators on average; since the annotators were asked
to pick a single best word in the survey, only the
Best-1 upperbound can be obtained.
The Best-1/2/3 results are summarized in Ta-
ble 2 for NEWS and Table 3 for BOOKS. These
Best-N scores are computed just using the single
feature of PMI, CP1 and CP2 (each in turn) to rank
the words in each topic. None of these features
alone produces a result that exceeds baseline per-
formance.
To make better use of all the features described
in Section 3, namely the PMI score, conditional
probabilities (both directions), topic model word
rank, WordNet Hypernym relationships and Pan-
tel?s distributional similarity score, we build a
ranking classifier using SVMrank and evaluating
Feature Set Best-1 Best-2 Best-3
Baseline 0.35 0.50 0.59
All Features 0.43 0.56 0.62
?PMI 0.45 (+0.02) 0.52 (?0.04) 0.62 (?0.00)
?CP1 0.35 (?0.08) 0.49 (?0.07) 0.57 (?0.05)
?CP2 0.40 (?0.03) 0.50 (?0.06) 0.61 (?0.01)
?TM Rank 0.40 (?0.03) 0.52 (?0.04) 0.57 (?0.05)
?Hypernym 0.43 (?0.00) 0.57 (+0.01) 0.62 (?0.00)
?PDS 0.43 (?0.00) 0.53 (?0.03) 0.62 (?0.00)
Upper bound 0.48 ? ?
Table 4: SVR-based best topic word results for
NEWS for all six feature types, and feature abla-
tion over each (numbers in brackets show the rel-
ative change over the full feature set)
Feature Set Best-1 Best-2 Best-3
Baseline 0.38 0.48 0.60
All Features 0.40 0.51 0.62
?PMI 0.38 (?0.02) 0.51 (?0.00) 0.63 (+0.01)
?CP1 0.33 (?0.07) 0.47 (?0.04) 0.56 (?0.06)
?CP2 0.40 (?0.00) 0.50 (?0.01) 0.64 (+0.02)
?TM Rank 0.35 (?0.05) 0.49 (?0.02) 0.63 (+0.01)
?Hypernym 0.40 (?0.00) 0.50 (?0.01) 0.61 (?0.01)
?PDS 0.45 (+0.05) 0.48 (?0.03) 0.67 (+0.05)
Upper bound 0.64 ? ?
Table 5: SVR-based best topic word results for
BOOKS for all six feature types, and feature abla-
tion over each (numbers in brackets show the rel-
ative change over the full feature set)
using 10-fold cross validation. Our first approach
is to use the entire set of features to train the clas-
sifier. Following this, we also measure the effect
of each feature by ablating (removing) one fea-
ture at a time. The drop in Best-N score indicates
which features are the strongest predictors of the
best words (a larger drop in score indicates that
feature is more important). The results for Best-1,
Best-2 and Best-3 scores are summarized in Ta-
ble 4 for NEWS, and Table 5 for BOOKS (averaged
across the 10 iterations of cross validation).
We then produced a condensed set of features,
consisting of the conditional probabilities, the
original topic model word rank and the WordNet
hypernym relationships. This ?best? set of fea-
tures is used to make predictions of best words.
Results are improved in most cases, and are sum-
marized in Table 6 for both NEWS and BOOKS.
609
Dataset Best-1 Best-2 Best-3
NEWS
Baseline 0.35 0.50 0.59
Best Feat. Set 0.45 0.50 0.65
Upper bound 0.48 ? ?
BOOKS
Baseline 0.38 0.48 0.60
Best Feat. Set 0.48 0.56 0.66
Upper bound 0.64 ? ?
Table 6: Results with the best feature set com-
pared to the baseline
Dataset Best-1 Best-2 Best-3
NEWS baseline 0.35 0.50 0.59
BOOKS ? NEWS 0.38 0.56 0.62
NEWS upper bound 0.48 ? ?
BOOKS baseline 0.38 0.48 0.60
NEWS ? BOOKS 0.48 0.56 0.65
BOOKS upper bound 0.64 ? ?
Table 7: Results for cross-domain learning
We also tested whether the SVM classifier
could be trained using data from one domain, and
run on data from another domain. Using our two
datasets as these different domains, we trained a
model using BOOKS data and made predictions
for NEWS, and then we trained a model using
NEWS data and made predictions for BOOKS.
The results, shown in Table 7, indicate that
we are still able to outperform the baseline, even
when the ranking classifier is trained on a differ-
ent domain. In fact, when we trained a model
using NEWS, we saw almost no drop in perfor-
mance for predicting best words for BOOKS, and
improvement is seen for Best-2 score from NEWS.
This implies that the SVM classifier generalizes
well across domains and suggests the possibility
of having a fixed training model to predict best
words for any data.
In these experiments, topic words are presented
in the original order that the topic model produces,
i.e. in descending order of probability of a word
under a topic P (wi|tj). We noticed that the first
words of the topics are frequently selected as the
best words by annotators, and suspected that this
was introducing a bias towards the first word. As
our baseline scores are derived from this topic
word ordering, such a bias could give rise to an
artificially high baseline.
To investigate this effect, we ran a second anno-
Word Order Best-1 Best-2 Best-3
Original 0.35 0.50 0.59
Randomized 0.23 0.33 0.46
Table 8: Reduction of baseline scores for NEWS
when words are presented in random order to an-
notators.
2 4 6 8 10
0.
0
0.
1
0.
2
0.
3
0.
4
Rank
Fr
ac
tio
n 
of
 h
um
an
 s
el
ec
te
d 
be
st
 w
or
d
ordered
random
Figure 2: Bias for humans selecting the best word,
when the topic words are presented in their origi-
nal ordering (ordered) or randomised (random)
tation exercise over the same set of topics (but dif-
ferent annotators), to obtain a new set of best word
annotations for NEWS, with the topic words pre-
sented in random order. In Figure 2, we plot the
cumulative proportion of words selected as best
word by the annotators across the topics, in the
case of the random topic word order, mapping the
topic words back onto their original ranks in the
topic model. A slight drop can be observed in the
proportion of first- and second-ranked topic words
being selected when we randomise the topic word
order. When we recalculate the baseline accuracy
for NEWS on the basis of the new set of annota-
tions, we observe an appreciable drop in the scores
(see Table 8).
6 Discussion
From the experiments in Section 5, perhaps the
first thing to observe is: (a) the high performance
of the baseline, and (b) the relatively low (Best-
1) upper bound accuracy for the task. The first is
perhaps unsurprising, given that it represents the
610
topic model?s own interpretation of the word(s)
which are most representative of that topic. In this
sense, we have set our sights high in attempting to
better the baseline. The upper bound accuracy is
a reflection of both the inter-annotator agreement,
and the best that we can meaningfully expect to
do for the task. That is, any result higher than this
would paradoxically suggest that we are able to do
better at a task than humans, where we are evalu-
ating ourselves relative to the labellings of those
humans. The upper bound for NEWS was slightly
less than 0.5, indicating that humans agree on the
best topic word only 50% of the time. To better
understand what is happening here, consider the
following topic from Figure 1:
health drug patient medical doctor hospital
care cancer treatment disease
This is clearly a coherent topic, but at least two
topic words suggest themselves as labels: health
and medical. By way of having between 10 and 20
annotators (uniquely) label a given topic, and in-
terpreting the multiple labellings probabilistically,
we are side-stepping the inter-annotator agree-
ment issue, but ultimately, for the Best-1 evalu-
ation, we are forced to select one term only, and
consider any alternative to be wrong. Because an-
notators selected only one best topic word, we un-
fortunately have no way of performing Best-2 or
Best-3 upper bound evaluation and deal with top-
ics such as this, but would expect the numbers to
rise appreciably.
Looking at the original feature rankings in Ta-
bles 2 and 3, no clear picture emerges as to which
of the three methods (PMI, CP1 and CP2) was
most successful, but there were certainly clear dif-
ferences in the relative numbers for each, point-
ing to possible complementarity in the scoring.
This expectation was born out in the results for
the reranking model in Tables 4 and 5, where the
combined feature set surpassed the baseline in all
cases, and feature ablation tended to lead to a drop
in results, with the single most effective feature set
being CP1 (P (wi|?)), followed by CP2 (P (?|wi))
and topic model rank. The lexical semantic fea-
tures of WordNet hypernymy and PDS (Pantel?s
distributional similarity) were the worst perform-
ers, often having no or negative impact on the re-
sults.
Comparing the best results for the SVR-based
reranking model and the upper bound Best-1
score, we approach the upper bound performance
for NEWS, but are still quite a way off with
BOOKS when training in-domain. This is encour-
aging, but a slightly artificial result in terms of the
broader applicability of this research, as what it
means in practical terms is that if we can access
multi-annotator best word labelling for the ma-
jority of topics in a given topic model, we can
use those annotations to predict the best word for
the remainder of the topics with reasonably suc-
cess. When we look to the cross-domain results,
however, we see that we almost perfectly replicate
the best-achieved Best-1, Best-2 and Best-3 in-
domain results for BOOKS by training on NEWS
(making no use of the annotations for BOOKS).
Applying the annotations for BOOKS to NEWS is
less successful in terms of Best-1 accuracy, but we
actually achieve higher Best-2, and largely mir-
ror the Best-3 results as compared to the best of
the in-domain results in Table 6. This leads to
the much more compelling conclusion that we can
take annotations from an independent topic model
(based on a completely unrelated document col-
lection), and apply them to successfully model the
best topic word for a new topic model, without
requiring any additional annotation. As we now
have two sets of topics multiply-annotated for best
words, this result suggests that we can perform the
best topic word selection task with high success
over novel topic models.
We carried out manual analysis of topics where
the model did particularly poorly, to get a sense
for how and where our model is being led astray.
One such example is the topic:
race car nascar driver racing cup winston team
gordon season
where the following topic words were selected by
our annotators: nascar (8 people), race (2 peo-
ple), and racing (2 people). First, we observe the
split between race and racing, where more judi-
cious lemmatisation/stemming would make both
the annotation easier and the evaluation cleaner.
The SVR model tends to select more common,
general terms, so in this case chose race as the
best word, and ranked nascar third. This is one
611
instance were nascar evokes all of the other words
effectively, but not conversely (racing is asso-
ciated with many events/sports beyond nascar,
e.g.).
Another topic where our model had difficulty
was:
window nave aisle transept chapel tower arch
pointed arches roof
where our best model selected nave, while the hu-
man annotators selected chapel (6 people), arch
(2 people), nave, roof , tower and transept (1 per-
son each). Clearly, our annotators struggled to
come up with a best word here, despite the topic
again being coherent. This is an obvious candi-
date for labelling with a hypernym/holonym of
the topic words (e.g. church or church architec-
ture), and points to the limitations of best word la-
belling ? there are certainly many topics where
best word labelling works, as our upper bound
analysis demonstrated, but there are equally many
topics where the most natural label is not found
in the top-ranked topic words. While this points
to slight naivety in the current task set up ? we
are forcing annotators to label words with topic
words, where we know that this is sub-optimal
for a significant number of topics ? we contend
that our numbers suggest that: (a) consistent best
topic word labelling is possible at least 50% of
the time; and (b) we have developed a method
which is highly adept at labelling these topics. As
a way forward, we intend to relax the constraint
on the topic label needing to be based on a topic
word, and explore the possibility of predicting
which topics are best labelled with topic words,
and which require independent labels. For topics
which can be labelled with topic words, we can
use the methodology developed here, and for top-
ics where this is predicted to be sub-optimal, we
intend to build on the work of Mei et al (2007),
Pantel and Ravichandran (2004) and others in se-
lecting phrasal/hypernym labels for topics. We are
also interested in applying the methodology pro-
posed herein to the closely-related task of intruder
word, or worst topic word, detection, as proposed
by Chang et al (2009).
Finally, looking to the question of the impact of
the presentation order of the topic words on best
word selection, it would appear that our baseline
is possibly an over-estimate (based on Table 8).
Having said that, the flipside of the bias is that it
leads to more consistency in the annotations, and
tends to help in tie-breaking of examples such as
race and racing from above, for example. In sup-
port of this claim, the upper bound Best-1 accu-
racy of the randomised annotations, relative to the
original gold-standard is 0.44, slightly below the
original upper bound for NEWS. More work is
needed to determine the real impact of this bias
on the overall task setup and evaluation.
7 Conclusion
This paper has presented the novel task of best
topic word selection, that is the selection of the
topic word that is the best label for a given topic.
We proposed a number of features intended to
capture the best topic word, and demonstrated
that, while they were relatively unsuccessful in
isolation, in combination as inputs to a rerank-
ing model, we were able to consistently achieve
results above the baseline of simply selecting the
highest-ranked topic word, both when training in-
domain over other labelled topics for that topic
model, and cross-domain, using only labellings
from independent topic models learned over docu-
ment collections from different domains and gen-
res.
Acknowledgements
NICTA is funded by the Australian government as repre-
sented by Department of Broadband, Communication and
Digital Economy, and the Australian Research Council
through the ICT centre of Excellence programme. DN has
also been supported by a grant from the Institute of Museum
and Library Services, and a Google Research Award.
References
Blei, D.M., A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
Brody, S. and M. Lapata. 2009. Bayesian word sense
induction. In Proceedings of the 12th Conference
of the EACL (EACL 2009), pages 103?111, Athens,
Greece.
Chang, J., J. Boyd-Graber, S. Gerrish, C. Wang, and
D. Blei. 2009. Reading tea leaves: How humans
interpret topic models. In Proceedings of the 23rd
612
Annual Conference on Neural Information Process-
ing Systems (NIPS 2009), pages 288?296, Vancou-
ver, Canada.
Griffiths, T. and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy
of Sciences, 101:5228?5235.
Haghighi, A. and L. Vanderwende. 2009. Explor-
ing content models for multi-document summariza-
tion. In Proceedings of the North American Chapter
of the Association for Computational Linguistics ?
Human Language Technologies 2009 (NAACL HLT
2009), pages 362?370, Boulder, USA.
Joachims, T. 2006. Training linear SVMs in lin-
ear time. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD),
pages 217?226, Philadelphia, USA.
Magatti, D., S. Calegari, D. Ciucci, and F. Stella. 2009.
Automatic labeling of topics. In Proceedings of the
International Conference on Intelligent Systems De-
sign and Applications, pages 1227?1232, Pisa, Italy.
Mei, Q., C. Liu, H. Su, and C. Zhai. 2006. A prob-
abilistic approach to spatiotemporal theme pattern
mining on weblogs. In Proceedings of the 15th
International World Wide Web Conference (WWW
2006), pages 533?542.
Mei, Q., X. Shen, and C. Zhai. 2007. Automatic la-
beling of multinomial topic models. In Proceedings
of the 13th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD
2007), pages 490?499, San Jose, USA.
Newman, D., J.H. Lau, K. Grieser, and T. Baldwin.
2010. Automatic evaluation of topic coherence.
In Proceedings of Human Language Technologies:
The 11th Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL HLT 2010), pages 100?108, Los
Angeles, USA.
Pantel, P. and D. Lin. 2002. Discovering word
senses from text. In Proceedings of the 8th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 613?619, Ed-
monton, Canada.
Pantel, P. and D. Ravichandran. 2004. Automati-
cally labeling semantic classes. In Proceedings of
the 4th International Conference on Human Lan-
guage Technology Research and 5th Annual Meet-
ing of the NAACL (HLT-NAACL 2004), pages 321?
328, Boston, USA.
Pantel, P., E. Crestan, A. Borkovsky, A-M. Popescu,
and V. Vyas. 2009. Web-scale distributional sim-
ilarity and entity set expansion. In Proceedings of
the 2009 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2009), pages
938?947, Singapore.
Ritter, A, Mausam, and O Etzioni. to appear. A la-
tent Dirichlet alocation method for selectional pref-
erences. In Proceedings of the 48th Annual Meeting
of the ACL (ACL 2010), Uppsala, Sweden.
Titov, I. and R. McDonald. 2008. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of the 17th International World Wide Web
Conference (WWW 2008), pages 111?120, Beijing,
China.
Wang, X. and A. McCallum. 2006. Topics over time:
A non-Markov continuous-time model of topical
trends. In Proceedings of the 12th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining (KDD 2006), pages 424?433,
Philadelphia, USA.
Wei, S. and W.B. Croft. 2006. LDA-based document
models for ad-hoc retrieval. In Proceedings of 29th
International ACM-SIGIR Conference on Research
and Development in Information Retrieval (SIGIR
2006), pages 178?185, Seattle, USA.
613
