Proceedings of NAACL HLT 2009: Short Papers, pages 57?60,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Spherical Discriminant Analysis in Semi-supervised Speaker Clustering?
Hao Tang
Dept. of ECE
University of Illinois
Urbana, IL 61801, USA
haotang2@ifp.uiuc.edu
Stephen M. Chu
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598, USA
schu@us.ibm.com
Thomas S. Huang
Dept. of ECE
University of Illinois
Urbana, IL 61801, USA
huang@ifp.uiuc.edu
Abstract
Semi-supervised speaker clustering refers to
the use of our prior knowledge of speakers
in general to assist the unsupervised speaker
clustering process. In the form of an in-
dependent training set, the prior knowledge
helps us learn a speaker-discriminative fea-
ture transformation, a universal speaker prior
model, and a discriminative speaker subspace,
or equivalently a speaker-discriminative dis-
tance metric. The directional scattering pat-
terns of Gaussian mixture model mean su-
pervectors motivate us to perform discrimi-
nant analysis on the unit hypersphere rather
than in the Euclidean space, which leads to
a novel dimensionality reduction technique
called spherical discriminant analysis (SDA).
Our experiment results show that in the
SDA subspace, speaker clustering yields su-
perior performance than that in other reduced-
dimensional subspaces (e.g., PCA and LDA).
1 Introduction
Speaker clustering is a critical part of speaker di-
arization (a.k.a. speaker segmentation and cluster-
ing) (Barras et al, 2006; Tranter and Reynolds,
2006; Wooters and Huijbregts, 2007; Han et al,
2008). Unlike speaker recognition, where we have
the training data of a set of known speakers and thus
recognition can be done supervised, speaker cluster-
ing is usually performed in a completely unsuper-
vised manner. The output of speaker clustering is the
internal labels relative to a dataset rather than real
?This work was funded in part by DARPA contract HR0011-
06-2-0001.
speaker identities. An interesting question is: Can
we do semi-supervised speaker clustering? That is,
can we make use of any available information that
can be helpful to speaker clustering?
Our answer to this question is positive. Here,
semi-supervision refers to the use of our prior
knowledge of speakers in general to assist the un-
supervised speaker clustering process. In the form
of an independent training set, the prior knowledge
helps us learn a speaker-discriminative feature trans-
formation, a universal speaker prior model, and a
discriminative speaker subspace, or equivalently a
speaker-discriminative distance metric.
2 Semi-supervised Speaker Clustering
A general pipeline of speaker clustering consists
of four essential elements, namely feature extrac-
tion, utterance representation, distance metric, and
clustering. We incorporate our prior knowledge
of speakers into the various stages of this pipeline
through an independent training set.
2.1 Feature Extraction
The most popular speech features are spectrum-
based acoustic features such as mel-frequency cep-
stral coefficients (MFCCs) and perceptual linear pre-
dictive (PLP) coefficients. In order to account for
the dynamics of spectrum changes over time, the
basic acoustic features are often supplemented by
their first and second derivatives. We pursue a dif-
ferent avenue in which we augment the basic acous-
tic features of every frame with those of the neigh-
boring frames. Specifically, the acoustic features
of the current frame and those of the KL frames
57
to the left and KR frames to the right are con-
catenated to form a high-dimensional feature vec-
tor. In the context-expanded feature vector space, we
learn a speaker-discriminative feature transforma-
tion by linear discriminant analysis (LDA) based on
the known speaker labels of the independent training
set. The resulting low-dimensional feature subspace
is expected to provide optimal speaker separability.
2.2 Utterance Representation
Deviating from the mainstream ?bag of acoustic fea-
tures? representation where the extracted acoustic
features are represented by a statistical model such
as a Gaussian mixture model (GMM), we adopt the
GMM mean supervector representation which has
emerged in the speaker recognition area (Campbell
et al, 2006). Such representation is obtained by
maximum a posteriori (MAP) adapting a universal
background model (UBM), which has been finely
trained with all the data in the training set, to a
particular utterance. The component means of the
adapted GMM are stacked to form a column vector
conventionally called a GMM mean supervector. In
this way, we are allowed to represent an utterance
as a point in a high-dimensional space where tra-
ditional distance metrics and clustering techniques
can be naturally applied. The UBM, which can be
deemed as a universal speaker prior model inferred
from the independent training set, imposes generic
speaker constraints to the GMM mean supervector
space.
2.3 Distance Metric
In the GMM mean supervector space, a naturally
arising distance metric is the Euclidean distance
metric. However, it is observed that the supervec-
tors show strong directional scattering patterns. The
directions of the data points seem to be more indica-
tive than their magnitudes. This observation moti-
vates us to favor the cosine distance metric over the
Euclidean distance metric for speaker clustering.
Although the cosine distance metric can be used
in the GMM mean supervector space, it is optimal
only if the data points are uniformly spread in all di-
rections in the entire space. In a high-dimensional
space, most often the data lies in or near a low-
dimensional manifold or subspace. It is advanta-
geous to learn an optimal distance metric from the
data directly.
The general cosine distance between two data
points x and y can be defined and manipulated as
follows.
d(x,y) = 1? xTAy?
xTAx
?
yTAy
(1)
= 1? (A1/2x)T (A1/2y)?
(A1/2x)T (A1/2x)
?
(A1/2y)T (A1/2y)
= 1? (WTx)T (WTy)?
(WTx)T (WTx)
?
(WTy)T (WTy)
The general cosine distance can be casted as the
cosine distance between two transformed data points
W Tx and W Ty where W T = A1/2. In this sense,
learning an optimal distance metric is equivalent to
learning an optimal linear subspace of the original
high-dimensional space.
3 Spherical Discriminant Analysis
Most existing linear subspace learning techniques
(e.g. PCA and LDA) are based on the Euclidean
distance metric. In the GMM mean supervector
space, we seek to perform discriminant analysis in
the cosine distance metric space. We coin the phrase
?spherical discriminant analysis? to denote discrim-
inant analysis on the unit hypersphere. We define
a projection from a d-dimensional hypersphere to a
d?-dimensional hypersphere where d? < d
y = W
Tx
?W Tx? (2)
We note that such a projection is nonlinear. How-
ever, under two mild conditions, this projection can
be linearized. One is that the objective function for
learning the projection only involves the cosine dis-
tance. The other is that only the cosine distance is
used in the projected space. In this case, the norm of
the projected vector y has no impact on the objective
function and distance computation in the projected
space. Thus, the denominator term of Equation 2
can be safely dropped, leading to a linear projection.
3.1 Formulation
The goal of SDA is to seek a linear transformation
W such that the average within-class cosine similar-
ity of the projected data set is maximized while the
58
average between-class cosine similarity of the pro-
jected data set is minimized. Assuming that there are
c classes, the average within-class cosine similarity
can be written in terms of the unknown projection
matrix W and the original data points x
SW = 1c
c?
i=1
Si (3)
Si = 1|Di||Di|
?
yj ,yk?Di
yTj yk?
yTj yj
?
yTk yk
= 1
|Di||Di|
?
xj ,xk?Di
xTj WWTxk?
xTj WWTxj
?
xTk WWTxk
where |Di| denotes the number of data points in the
ith class. Similarly, the average between-class co-
sine similarity can be written in terms of W and x
SB = 1c(c? 1)
c?
m=1
c?
n=1
Smn (m 6= n) (4)
Smn = 1|Dm||Dn|
?
yj?Dm
yk?Dn
yTj yk?
yTj yj
?
yTk yk
= 1
|Dm||Dn|
?
xj?Dm
xk?Dn
xTj WWTxk?
xTj WWTxj
?
xTk WWTxk
where |Dm| and |Dn| denote the number of data
points in the mth and nth classes, respectively.
The SDA criterion is to maximize SW while min-
imizing SB
W = argmax
W
(SW ? SB) (5)
Our SDA formulation is similar to the work of Ma
et al (2007). However, we solve it efficiently in a
general dimensionality reduction framework known
as graph embedding (Yan et al, 2007).
3.2 Graph Embedding Solution
In graph embedding, a weighted graph with vertex
set X and similarity matrix S is used to characterize
certain statistical or geometrical properties of a data
set. A vertex in X represents a data point and an
entry sij in S represents the similarity between the
data points xi and xj . For a specific dimensional-
ity reduction algorithm, there may exist two graphs.
The intrinsic graph {X,S(i)} characterizes the data
properties that the algorithm aims to preserve and
the penalty graph {X,S(p)} characterizes the data
properties that the algorithm aims to avoid. The goal
of graph embedding is to represent each vertex in X
as a low dimensional vector that preserves the simi-
larities in S. The objective function is
W=argminW
?
i 6=j ?f(xi,W )?f(xj ,W )?2(s
(i)
ij ?s
(p)
ij ) (6)
where f(x,W ) is a general projection with param-
eters W . If we take the projection to be of the form
in Equation 2, the objective function becomes
W=argminW
?
i6=j
????
WT xi
?WT xi?
? W
T xj
?WT xj?
????
2
(s(i)ij ?s
(p)
ij ) (7)
It is shown that the solution to the graph embed-
ding problem of Equation 7 may be obtained by
a steepest descent algorithm (Fu et al, 2008). If
we expand the L2 norm terms of Equation 7, it is
straightforward to show that Equation 7 is equiva-
lent to Equation 5 provided that the graph weights
are set to proper values, as follows.
s(i)jk ?
1
c|Di||Di|
if xj ,xk ? Di, i = 1, ..., c
s(p)jk ?
1
c(c? 1)|Dm||Dn| if xj ? Dm,xk ? Dn
m,n = 1, ..., c,m 6= n (8)
That is, by assigning appropriate values to the
weights of the intrinsic and penalty graphs, the SDA
optimization problem in Equation 5 can be solved
within the elegant graph embedding framework.
4 Experiments
Our speaker clustering experiments are based on a
test set of 630 speakers and 19024 utterances se-
lected from the GALE database (Chu et al, 2008),
which contains about 1900 hours of broadcasting
news speech data collected from various TV pro-
grams. An independent training set of 498 speak-
ers and 18327 utterances is also selected from the
GALE database. In either data set, there are an aver-
age of 30-40 utterances per speaker and the average
duration of the utterances is about 3-4 seconds. Note
that there are no overlapping speakers in the two data
59
sets ? speakers in the test set are not present in the
independent training set.
The acoustic features are 13 basic PLP features
with cepstrum mean subtraction. In computing the
LDA feature transformation using the independent
training set, KL and KR are both set to 4, and the di-
mensionality of the low-dimensional feature space is
set to 40. The entire independent training set is used
to train a UBM via the EM algorithm, and a GMM
mean supervector is obtained for every utterance in
the test set via MAP adaptation. The trained UBM
has 64 mixture components. Thus, the dimension of
the GMM mean supervectors is 2560.
We employ the hierarchical agglomerative clus-
tering technique with the ?ward? linkage method.
Our experiments are carried out as follows. In each
experiment, we perform 4 cases, each of which is as-
sociated with a specific number of test speakers, i.e.,
5, 10, 20, and 50, respectively. In each case, the
corresponding number of speakers are drawn ran-
domly from the test set, and all the utterances from
the selected speakers are used for clustering. For
each case, 100 trials are run, each of which involves
a random draw of the test speakers, and the average
of the clustering accuracies across the 100 trials is
recorded.
First, we perform speaker clustering in the orig-
inal GMM mean supervector space using the Eu-
clidean distance metric and the cosine distance met-
ric, respectively. The results indicate that the cosine
distance metric consistently outperforms the Eu-
clidean distance metric. Next, we perform speaker
clustering in the reduced-dimensional subspaces us-
ing the eigenvoice (PCA) and fishervoice (LDA)
approaches, respectively. The results show that
the fishervoice approach significantly outperforms
the eigenvoice approach in all cases. Finally, we
perform speaker clustering in the SDA subspace.
The results demonstrate that in the SDA subspace,
speaker clustering yields superior performance than
that in other reduced-dimensional subspaces (e.g.,
PCA and LDA). Table 1 presents these results.
5 Conclusion
This paper proposes semi-supervised speaker clus-
tering in which we learn a speaker-discriminative
feature transformation, a universal speaker prior
Metric Subspace 5 10 20 50
Euc
Orig 85.0 82.6 78.1 69.4
PCA 85.5 82.9 79.3 69.9
LDA 94.0 90.8 86.6 79.6
Cos Orig 90.7 86.5 82.2 77.7SDA 98.0 94.7 90.0 85.9
Table 1: Average speaker clustering accuracies (unit:%).
model, and a speaker-discriminative distance metric
through an independent training set. Motivated by
the directional scattering patterns of the GMM mean
supervectors, we peroform discriminant analysis on
the unit hypersphere rather than in the Euclidean
space, leading to a novel dimensionality reduction
technique ?SDA?. Our experiment results indicate
that in the SDA subspace, speaker clustering yields
superior performance than that in other reduced-
dimensional subspaces (e.g., PCA and LDA).
References
C. Barras, X. Zhu, S. Meignier, and J. Gauvain. 2006.
Multistage speaker diarization of broadcast news.
IEEE Trans. ASLP, 14(5):1505?1512.
W. Campbell, D. Sturim, D. Reynolds. 2006. Support
vector machines using GMM supervectors for speaker
verification. Signal Processing Letters 13(5):308-311.
S. Chu, H. Kuo, L. Mangu, Y. Liu, Y. Qin, and Q. Shi.
2008. Recent advances in the IBM GALE mandarin
transcription system. Proc. ICASSP.
Y. Fu, S. Yan and T. Huang. 2008. Correlation Met-
ric for Generalized Feature Extraction. IEEE Trans.
PAMI 30(12):2229?2235.
K. Han, S. Kim, and S. Narayanan. 2008. Strategies to
Improve the Robustness of Agglomerative Hierarchi-
cal Clustering under Data Source Variation for Speaker
Diarization. IEEE Trans. SALP 16(8):1590?1601.
Y. Ma, S. Lao, E. Takikawa, and M. Kawade. 2007. Dis-
criminant Analysis in Correlation Similarity Measure
Space. Proc. ICML (227):577?584.
S. Tranter and D. Reynolds. 2006. An Overview of
Automatic Speaker Diarization Systems. IEEE Trans.
ASLP, 14(5):1557?1565.
C. Wooters and M. Huijbregts. 2007. The ICSI RT07s
Speaker Diarization System. LNCS.
S. Yan, D. Xu, B. Zhang, H. Zhang, Q. Yang, S. Lin.
2007. Graph Embedding and Extensions: A Gen-
eral Framework for Dimensionality Reduction. IEEE
Trans. PAMI 29(1):40?51.
60
