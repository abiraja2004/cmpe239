Knowledge Acqulsit ion= 
Classif ication of Terms in a thesaurus from~a Corpus 
STA Jean-David 
EDF, Direction des Etudes et des Recherches 
1, av. de G~n4ral de Gaulle 
92140 Clamart - France 
(33) 1 47.65.49.58 E-mail : jd.sta@der.edfgdf.fr  
Abstract 
! 
Faced with growing volume and accessibi l i ty of electronic textual 
information, information retrieval, and, in general, automatic 
documentat ion require updated terminological resources that are ever 
more voluminous. A current problem is the automated construction of 
these resources (e.g., terminologies, thesauri, glossaries, etd~ ~) from 
a corpus. Various linguistic and statistical methods to handle this 
problem are coming to light. One problem that has been less studied is 
that of updating these resources, in particular, of classifying a term 
extracted from a corpus in a subject field, discipl ine or branch of an 
exist ing thesaurus. This is the first step in posit ioning a term 
extracted from a corpus in the structure of a thesaurus (generic 
relations, synonymy relations ..). This is an important problem in 
certain disciplines in which knowledge, and, in particular, vocabulary 
is not very stable over time, especial ly because of neologisms. 
This experiment compares different models for representing a term 
from a corpus for its automatic classif ication in the subject fields 
of a thesaurus. The classif ication method used is linear 
discr iminatory analysis, based on a learning sample. The models 
evaluated here are: a term/document model where each term is a vector 
in document vector space, two term/term models where each term is a 
vector in term space, and the coordinates are either the co- 
occurrence, or the mutual information between terms. The most 
effective model is the one based on mutual information between terms, 
which typifies the fact that two terms often appear together in the 
corpus, but rarely apar t . .  
I. Introduction 
In documentation, terminologies, thesauri and other 
terminological lists are reference systems which can be used for 
manual or automatic indexing. Indexing consists of recognising the 
terms in a text that belong to a reference system; this is called 
control led indexing. The quality of the result of the indexing process 
depends in large part on the quality of the terminology (completeness, 
consistence ..). Thus, applications downstream from the indexing depend 
on these terminological resources. 
101 
,I 
The most thoroughly studied application is the information 
retr ieval (IR). Here, the term provides a means for accessing 
information through its standardising effect on the query and on the 
text to be found. The term can also be a variable that is used in 
statist ical c lassif icat ion or clustering processes of documents (\[BLO 
92\] and \[STA 95a\]), or in selective dissemination of information, in 
which it is used to bring together a document to be disseminated and 
its target \[STA 93\]. 
Textual information is becoming more and more accessible in 
electronic form. This accessibi l i ty is certainly one of the 
prerequis i tes for the massive use of natural language process ing (NLP) 
techniques. These techniques applied on part icular domains, often use 
terminological resources that supplement the lexical resources. The 
lexical resources (general language dictionaries) are fairly stable, 
whereas terminologies evolve dynamical ly with the fields they 
describe. In particular, the discipl ines of information processing 
(computers, etc.) and biology or genetics are characterised today by 
an extraordinary terminological activity. 
Unfortunately, the abundance of electronic corpora and the 
relat ive matur i ty  of natural language processing techniques have 
induced a shortage of updated terminological data. The various efforts 
in automatic acquis i t ion of terminologies from a corpus stem from this 
observation, and try to answer the following question: "How can 
candidate terms be extracted from a corpus?" 
Another inKoortant question is how to posit ion a term in an 
exist ing thesaurus. That question can itself be subdivided into 
several questions that concern the role of the standard relationships 
in a thesaurus: synonymy, hyperonymy, etc. The question studied in 
this experiment concerns the posit ioning or classif icat ion of a term 
in a subject f ield or semantic field of a thesaurus. This is the first 
step in a precise posit ioning using the standard relat ionships of a 
thesaurus. This prob lem is very diff icult for a human being to resolve 
when he is not an expert in the field to which the term belongs and 
one can hope that an automated classif ication process would be of 
great help. 
To classify a term in a subject field can be considered similar 
to word sense disambiguation (WSD) which consists in classifying a 
word in a conceptual class (one of its senses). The difference is 
that, in a corpus, a term is general ly monosemous and a word is 
polysemous. Word sense disambiguation uses a single context (generally 
a window of a few words around the word to be disambiguated) as input 
to predict  its sense among a few possible senses (generally less than 
ten). Term subject field discr imination uses a representat ion of the 
term calculated on the whole corpus in order to classify it into about 
330 subject fields in this experiment. 
The experiment described here was used to evaluate different 
methods for c lassi fying terms from a corpus in the subject fields of a 
thesaurus. After a brief description of the corpus and the thesaurus, 
automatic indexing and terminology extraction are described. 
Linguist ic and statistical techniques are used to extract a candidate 
term from a corpus or to recognise a term in a document. This 
preparatory processing allows the document to be represented as a set 
102 
of terms (candidate terms and key words A c lass i f i cat ion method is 
then implemented to c lass i fy  a subset of 1,000 terms in the 49 themes 
and 330 semantic fields that make up the thesaurus. The 1,000 terms 
thus c lass i f ied comprise the test sample that is used to evaluate 
three models  for represent ing terms. 
IX. Data Preparation 
IX.i. Descript ion of the Corpus 
The corpus studied is a set of I0,000 scient i f ic  and technical  
doc~unents in French (4,150,000 words). Each document consists of one 
or two pages of text. This corpus descr ibes research carr ied out by 
the research d iv is ion of EDF, the French e lectr ic i ty  company. Many 
d iverse subjects are dealt with: nuclear  energy, thermal energy, home 
automation, sociology, art i f ic ia l  intel l igence, etc. Each document 
descr ibes the object ives and stages of a research pro ject  on a 
par t icu lar  subject. These documents are used to p lan EDF research 
activity. 
Thus, the vocabulary  used is either very technical, w i th  subject 
f ie ld terms and candidate terms, or very general, w i th  sty l ist ic  
expressions, etc. 
| 
Obj ectif  : .... ,IObj ecfif 
Const ruct ion  de thesaurus~Etatd 'avancement  ~- -~ genera/ 
. . . .  , _ ~ ~'-qphase d~ndus~/a/isafion expressions ~an a avancemen~ : - 2~-~ 
La phase d' industr ia l isat ion /... ~ Iconstrucfion de the~z , . . . " \[ . . . . . . . . . . . . . . . .  auru_  
L indexat lon automat lque . . . . . . . .  
-imqxexauon automauoue 
k 
terms 
A document with terms and general expressions 
II.2. Descript ion of the Thesaurus 
The EDF thesaurus consists of 20,000 terms ( including 6,000 
synonyms) that cover a wide var ie ty  of fields (statistics, nuclear  
power  plants, informat ion retrieval, etc.). This reference system was 
created manual ly  from corporate documents, and was va l idated w i th  the 
help of many experts. Currently, updates are handled by a group of 
documental is ts  who regular ly  examine and insert new terms. One of the 
sources of new terms is the corpora. A l inguist ic  and stat ist ical  
extractor  proposes candidate terms for va l idat ion  by the 
documental ists .  A f ter  val idation, the documental ists  must pos i t ion  the 
se lected terms in the thesaurus. It's a di f f icult  exerc ise because of 
the wide var ie ty  of fields. 
103 
The thesaurus is composed of 330 semantic (or subject) fields 
included in 49 themes such as mathematics, sociology, etc. 
' i  
th6ode des erreurs \[
I analyse discriminante I \]statistique \[ , s .ttmation I 
l analyse statistique 
\[:modUle statistique lineaire ! 
l analyse de la variance ~ Generic Relationship 
i'statistique n'on param6trique \] 
See Also 
Relationship 
Extract from the "statistics" sm-~t ic  f ield from the EDF thesaurus 
This example gives an overview of the various relations between 
terms. Each term belongs to a single semantic field. Each term is 
l inked to other terms through a generic relat ion (arrow) or a 
neighbourhood relation (line). Other relations (e.g., synonym, 
translated by, etc.) exist, but are not shown in this example. 
II?3. Document Indexing 
As a first step, the set of documents in the corpus is indexed. 
This consists of producing two types of indexes: candidate terms, and 
descriptors. The candidate terms are expressions that may become 
terms, and are submitted to an expert for validation. Descriptors are 
terms from the EDF thesaurus that are automatical ly recognised in the 
documents. 
II.3.1. Terminological  Fi l ter ing 
In this experiment, terminological f i l tering is used for each 
document to produce terms that do not belong to the thesaurus, but 
which nonetheless might be useful to describe the documents. Moreover, 
these expressions are candidate terms that are submitted to experts or 
documental ists for validation. 
L inguist ic and statistical terminological f i ltering are used. The 
method chosen for this experiment combines an initial l inguist ic 
extraction with statistical f i ltering \[STA 95b\]. 
104 
Linguistic Extraction 
General ly,  it appears that the syntact ical  s t ructure  of a term in 
F rench  language is the noun phrase. For example, in the EDF thesaurus, 
the syntact ic  structures of terms are d is t r ibuted as fol lows: 
syntact ic  structure 
Noun Ad~ect ive  
Noun Prepos i t ion  Noun 
Noun 
Proper  noun 
Noun Prepos i t ion  Art ic le  
Noun 
Noun Prepos i t ion  Noun 
Ad ject ive  
Noun Part ic ipe 
Noun Noun 
example 
~rosion f luvia le 
analyse de contenu 
d~centra l i sat ion 
Chinon 
assurance de la 
qual i t4 
unit4 de bande 
magn4t ique 
puissance absorb4e 
acc~s m4moire  
% 
2S .i 
24.4 
18.1 
6.8 
3.2 
Distriknation of the syntact ic structures of terms 
2.8 
2.2 
2.1 
Thus, term extract ion is in i t ia l ly  syntact ical .  It cons ists  of 
app ly ing  seven recurs ive syntact ic patterns to the corpus \[OGO 94\]. 
NP <- ADJECTIVE NP 
NP<-  NPADJECT IVE  
NP <- NP ~ NP 
NP <- NP de NP 
NP <- NP en NP 
NP <- NP pour  NP 
NP <- NPNP 
The seven syntact ic  patterns for terminology extract ion  
Statistical Filtering 
L inguist ic  extraction, however, is not enough. In fact, many 
express ions  wi th  a noun phrase structure are not terms. This includes 
genera l  expressions,  styl ist ic effects, etc. Stat ist ica l  methods  can 
thus be used, in a second step, to d iscr iminate terms from non- 
termino log ica l  expressions.  Three indicators are used  here: 
Frequency:  This is based on the fact that the more of ten an 
express ion  is found in the corpus, the more l ike ly  it is to be a 
term. This statement must be kept in proportion, however. Indeed, it 
seems that a small  number of words (usually, very  general  uniterms) 
are very  frequent, but are not terms. 
- Var iance:  This is based on the idea that the more the occurrences in 
a document  of an expression are scattered, the more l ike ly  it is to 
be a term. This is the most ef fect ive indicator. Its d rawback  is 
that it also h ighl ights  large noun phrases in wh ich  the terms are 
included. 
Local  dens i ty  \[STA 95b\]: This is based on the idea that the c loser  
together  the documents  are that contain the expression, the more 
l i ke ly  it is to be a term. The local densi ty  of an express ion  is the 
105 
mean of the cosines between documents which contain the given 
expression. A document is a vector in the Document Vector  Space 
where a dimension is a term. This indicator highlights a certain 
number of terms that are not transverse to the corpus, but rather 
concentrated in documents that are close to each other. Nonetheless, 
this is not a very effective indicator for terms that are transverse 
to  the corpus. For example, terms from computer science, which are 
found in a lot of documents, are not highl ighted by this indicator. 
Results of the Tez~inological Extraction 
During this experiment, the terminological extraction u l t imately 
produced 3,000 new terms that did not belong to the thesaurus. These 
new' terms are used in the various representat ion models descr ibed 
below. The initial l inguistic extracting produced about 50,000 
expressions. 
II.3.2. Controlled Indexing 
A supplementary way of characterising a document's contents is by 
recognising control led terms in the document that belong to a 
thesaurus. To do this, an NLP technique is used \[BLO 92\]. Each 
sentence is processed on three levels: morphologically, syntactically, 
and semantically. These steps use a grammar and a general language 
dictionary. 
The method consists of breaking down the text fragment being 
processed by a series of successive transformations that may be 
syntactical (nominalisation, de-coordination, etc.), semantic (e.g., 
nuclear and atomic), or pragmatic (the thesaurus" synonym 
relationships are scanned to transform a synonym by its main form). At 
the end of these transformations, the decomposed text is compared to 
the list of documented terms of the thesaurus in order to supply the 
descriptors. 
Results of the Controlled Iz~exing 
Control led indexing of the corpus supplied 4,000 terms (of 20,000 
in the thesaurus). Each document was indexed by 20 to 30 terms. These 
documented terms, like the candidate terms, are used in the 
representat ion models described below. The quality of the indexing 
process is estimated at 70 percents (number of right terms div ided by 
number of terms). The wrong terms are essential ly due to problems of 
polysemy. Indeed some terms (generally uniterms) have mult ip le senses 
(for example "BASE") and produce a great part of the noise. 
106 
III. Term Subject Field Discrimination 
XXI.l. Word Sense D isa~higuat ionand Term Subject Field Discr imination 
The d iscr iminat ion  of word  senses is a wel l  known prob lem in 
computat iona l  l inguist ics \[YNG 55\]. The prob lem of WSD is to bu i ld  
ind icators  descr ib ing  the di f ferent senses of a word. Given a context 
of a word, these indicators are used to predict  its sense. Face to the 
d i f f i cu l ty  of manua l ly  bu i ld ing these indicators, researchers  have 
turned to resources such as machine- readable  d ict ionar ies  \[VER 90\] and 
corpora  \[YAR 92\]. 
WSD and term subject f ie ld d iscr iminat ion f rom corpora can be 
cons idered  s imi lar  in the way that they are both  a prob lem of 
c lass i f i ca t ion  into a class (a sense for a word  and a subject  f ie ld 
for a term). Nevertheless,  the prob lem is s tat i s t ica l ly  di f ferent.  In 
one case, a word  is represented by a few var iables (its context) and 
is c lass i f ied  into one class chosen among a few classes. In the other 
case, a term is represented by hundred of var iables (one of the models  
descr ibed  in chapter  IV) and is c lass i f ied  into a class chosen among 
hundred  of classes. 
ZII.2. L inear Discr~.mlnatozyAnalysis 
The prob lem of d iscr iminat ion can be descr ibed as follows: A 
random var iab le  X is d is t r ibuted in a p -d imens iona l  space, x 
represents  the observed values of var iab le  X. The prob lem is to 
determine the d is t r ibut ion of X among q d ist r ibut ions (the classes), 
based  on the observed values x. The method implemented here is l inear 
d i sc r iminatory  analysis. 
Us ing a sample that has a l ready been classif ied, d iscr iminatory  
analys is  can construct  c lass i f icat ion functions wh ich  take into 
account  the var iab les  that descr ibe the elements to be classi f ied.  
Each e lement x to be c lass i f ied is descr ibed by a b inary  vector  x= 
(xl, x2 . . . . .  xi . . . . .  xp) where xi=l or xi=0. 
xi=l  means the var iab le  xi descr ibes the term x. 
x i=0 means the vara ib le  xi does not descr ibe the term x. 
The probab i l i ty  that an element x is in a class c is written: 
P( C=c I X=x ) where C is a random var iab le  and X is a random vector. 
Us ing Bayes formula, it may be deduced that: 
P( C=c \[ X=x ) = ( P( C = c ) P( X = x I C = c ) ) / P( X = x ) 
There are three probabi l i t ies  to estimate: 
- Est imate  P( C = c) 
107 
P( C = c ) is est imated by nc / n where: 
nc is the number  of elements of the class c 
n is the number  of elements in the sample 
- Es t imate  P( X = x) 
This est imate  is s impl i f ied by normal is ing  the probabi l i t ies  to I. 
- Es t imate  P( X = x I C = c ) 
For this estimate, we assume that the random var iab les  Xl, X2 . . . .  Xm 
are independent  for a given class c. This leads to: 
P( X=x I C=c)  =HP(  xi =x i  I C =c  ) and 
P( Xi = 1 I C = c ) is est imated by nc, i  / nc 
P( Xi = 0 I C = c ) is est imated by 1 - nc, i  / nc 
where  nc, i  is the number of elements in the sample which are in class 
c, and for which  xi =i, and nc is the number  of e lements of the sample 
in c lass c. 
Once all the probabi l i t ies  are est imated, the c lass i f i ca t ion  
funct ion  for an element x consists of choosing the class that has the 
h ighest  probabi l i ty .  This funct ion min imises  the r i sk  of 
c lass i f i ca t ion  error  \[RAO 65\]. 
IV. Description of the Experiment 
The purpose  of this exper iment is to determine the best  way to 
c lass i fy  candidate terms from a corpus in semant ic  fields. The general  
p r inc ip le  is, firstly, to represent  the candidate terms to be 
c lassi f ied,  then, to c lass i fy  them, and finally, to eva luate  the 
qua l i ty  of the c lassi f icat ion.  The c lass i f i ca t ion  method is based  on 
learn ing process, which requires a set of p rev ious ly -c lass i f ied  terms 
(the learn ing sample manual ly  c lassi f ied).  The  eva luat ion  also 
requi res  a test sample, a set of p rev ious ly -c lass i f ied  terms which  
have to be automat ica l ly  classif ied. The eva luat ion  then cons ists  of 
compar ing  the results  of the c lass i f i cat ion  process  to the prev ious  
manual  c lass i f i ca t ion  of the test sample. 
IV.I. Learning and Test Sample 
The thesaurus terms found in the corpus were separated into two 
sets: a subset of about 3,000 terms which composed the learn ing 
sample, and a subset of 1,000 terms which  composed the test sample. 
Al l  these terms had already been manua l ly  c lass i f ied  by theme and 
semant ic  f ie ld in the thesaurus. 
108 
Rate of We l l  C lass i f ied  Terms 
The eva luat ion  cr i ter ia  is the rate of wel l  c lass i f ied  terms 
ca lcu la ted  among the 1,000 terms of the test sample. 
Rate of wel l  c lass i f ied terms = number  of wel l  c lass i f ied  terms 
d iv ided  by the number  of c lass i f ied  terms. 
ZV.2 .  TezmRepresentat ion  Models  
The representat ion  of the terms to be c lass i f ied is the main 
parameter  that determines the qual i ty of the classi f icat ion.  Indeed, 
this exper iment  showed that, for a s ingle representat ion model,  there 
is no s ign i f icant  d i f ference between the results of the var ious 
c lass i f i ca t ion  methods. By example, the nearest neighbours method 
(KNN) \[DAS 90\] was tested without  any s igni f icant difference. The only 
parameter  that t ru ly  inf luences the result  is the way of represent ing  
the terms to be classif ied. Three models  were evaluated. The first is 
based  on a term/document approach, and the two others by a term/term 
approach. 
IV2 .  i .  The TezmlDocument  Mode l  
The term/document  model  uses the transposi t ion of the standard 
document / te rm matrix.  Each l ine represents a term, and each column a 
document.  At the intersect ion of a term and a document, there is 0 if 
the term is not in the document in question, and 1 if it is present.  
The s tandard  document / term matr ix  showed its worth  in the Salton 
vector  model  \[SAL 88\]. It can therefore be hoped that the documents 
that conta in  a term provide a good representat ion of this term for its 
c lass i f i ca t ion  in a field. 
ZV.2 .2 .  The Tezm/TezmMode ls  
The term/term model uses a matr ix  where each line represents  a 
term to be classif ied, and each column represents a thesaurus term 
recogn ised  in the corpus, or a candidate term extracted f rom the 
corpus. At the intersect ion of a line and a column, two indicators 
have been studied. 
Co-occurrences  matr ix:  The indicator  is the co-occurrence between two 
terms. Co-occurrence ref lects the fact that two terms are found 
together  in documents.  
Mutual  in format ion  matrix: The indicator is the mutual  in format ion 
between two terms. Mutual informat ion (\[CHU 89\] and \[FEA 61\]) ref lects 
the fact that two terms are often found together in documents, but 
rare ly  alone. MI(x,y) is the mutual  information between terms x and y, 
and is written:  
109 
MI(x,y) = log2(P(xy)  /P(x) P(y )) 
where P(x,y) the probabil ity of observing x and y together and P(x) 
the probabi l i ty  of observing x, P(y) the probabi l i ty  of observing y. 
In the two cases, the matrix has to be transformed into a binary 
matrix. The solution is to choice a threshold under which the value is 
put to 0 and above which the value is put to i. Lots of values had 
been tested. The best classif ication for the co-occurrence matr ix  is 
obtained for a threshold of three. The best c lass i f icat ion for the 
mutual  information matrix is obtained for a threshold of 0.05. 
Resu l ts  
The main results concern three term representat ion models and two 
classif ications: the first in 49 themes, and the second in 330 
semantic fields. The criterion chosen for the evaluation is the well  
c lassi f ied rate. 
Method 
=Term Document model 
Term term model with co-occurrence 
Term term model  wi th  tmatual 
in format ion  
Themes 
classi f icat ion 
42.9 
31 .5  
89.8  
Semantic 
fields 
classif icat i  
on 
27.3 
19.8 
65.2 
Rate of Wel l  C lass i f ied  Terms 
There is a significant difference between the term/term model 
wi th  mutual information and the other two models. The good rates (89.8 
and 65.2) can be improved if the system proposes more than one class. 
In the case of 3 proposed classes (sorted by descending 
probabil it ies),  the probabil ity that the r ight class is in these 
classes is est imated respectively by 97.1 and 91.2 for the themes and 
the semantic fields. 
Discuss ion  
Without a doubt, the term/term model with mutual information has 
the best performance. Nonetheless, these good results must be 
qualified. 
A detai led examination of the results shows that there is a wide 
dispersion of the rate of well c lassif ied terms depending on the f ield 
(the 49 themes or the 320 semantic fields). The explanation is that 
the documents in the corpus are essential ly thematic. Thus, the 
vocabulary for certain fields in the thesaurus is essential ly 
concentrated in a few documents. Classi f icat ion based on mutual 
information is then efficient. On the other hand, certain fields are 
transverse (e.g., computer science, etc.), and are found in many 
documents that have few points in co~ton (and l itt le common technical 
vocabulary). Terms in these fields are diff icult to classify. 
110 
Another problem with the method is connected to the 
representat iveness of the learning sample. Commonly, for a given 
field, a certain number of terms are available (for example 20,000 
terms in the EDF thesaurus). It is more rare for all these terms to be 
found in the corpus under study (4,000 terms found in this 
experiment). Thus, if a class (a theme or semantic field) is not well 
represented in the corpus, the method is unable to classify candidate 
terms in this class because the learning sample for this class is not 
enough. 
Through this experiment, an automatic classif ication of 300 
candidate terms in 330 semantic fields was proposed to the group that 
val idates new thesaurus terms. This classif ication was used by the 
documental ists to update the EDF thesaurus. Each term was proposed in 
three semantic fields (among 330) sorted from the highest probabi l i ty 
(to be the right semantic field of the term) to the lowest. 
111 
References 
\[BLO 92\] Blosseville M.J., Hebrail G., Monteil M.G., Penot N., 
"Automatic Document Classification: Natural Language Processing, 
Statistical Data Analysis, and Expert System Techniques used together 
", ACM-SIGIR'92 proceedings, 51-58,1992. 
\[CHU 89\] Church K., "Word Association Norms, Mutual information, and 
Lexicography ", ACL 27 proceedings, Vancouver, 76-83, 1989. 
\[DAS 90\] Dasarathy B.V., "Nearest Neighbor (NN) Norms: NN Pattern 
Classification Techniques", IEEE Computer Society Press, 1990. 
\[FEA 61\] Fano R., "Transmission of Information", MIT Press, Cambridge, 
Massachusetts, 1961. 
lOGO 94\] Ogonowski A., Herviou M.L., 
extracting and structuring knowledge 
proceedings, 1049-1053, 1994. 
Monteil M.G., "Tools for 
from text", Coling'94 
\[RAO 65\] Rao C.R., "Linear Statistical Inference and its applications 
", 2nd edition, Wiley, 1965. 
\[SAL 88\] Salton G., ~ Automatic Text Processing : the Transformation, 
Analysis, and Retrieval of Information by Computer ~, Addison-Wesley, 
1988. 
\[STA 93\] Sta J.D., "Information filtering : a tool for communication 
between researches", INTERCHI'93 proceedings, Amsterdam, 177-178, 
1993. 
\[STA 95a\] Sta J.D., "Document expansion applied to classification : 
weighting of additional terms", ACM-SIGIR'95 proceedings, Seattle, 
177-178, 1995. 
\[STA 95b\] Sta J.D., "Comportement statistique des termes et 
acquisition terminologique & partir de corpus", T.A.L., Vol. 36, Num. 
1-2, 119-132, 1995. 
\[VER 90\] Veronis J., Ide N., "Word Sens Disambiguation with Very 
Large Neural Networks Etracted from Machine Radable Dictionnaries" 
COLING'90 proceedings, 389-394, 1990. 
\[YAR 92\] Yarowsky D., "Word-Sense Disambiguation Using Statistical 
Models of Roger's Categories Trained on Large Corpora", COLING'92 
proceedings, 454-460, 1992. 
\[YNG 55\] Yngve V., "Syntax and the Problem of Multiple Meaning" in 
Machine Translation of Languages, Will iam Lock and Donald Booth eds., 
Wiley, New York, 1955. 
112 
