Coling 2010: Poster Volume, pages 1140?1148,
Beijing, August 2010
A Method for Automatically Generating a Mediatory
Summary to Verify Credibility of Information on the Web
Hideyuki Shibuki and Takahiro Nagai and Masahiro Nakano
Rintaro Miyazaki and Madoka Ishioroshi and Tatsunori Mori
Graduate School of Environment and Information Sciences,
Yokohama National University
{shib, nagadon, nakano, rintaro, ishioroshi, mori}@forest.eis.ynu.ac.jp
Abstract
In this paper, we propose a method for
mediatory summarization, which is a
novel technique for facilitating users?
assessments of the credibility of infor-
mation on the Web. A mediatory sum-
mary is generated by extracting a pas-
sage from Web documents; this sum-
mary is generated on the basis of its
relevance to a given query, fairness,
and density of keywords, which are fea-
tures of the summaries constructed to
determine the credibility of informa-
tion on the Web. We demonstrate
the effectiveness of the generated me-
diatory summary in comparison with
the summaries of Web documents pro-
duced by Web search engines.
1 Introduction
Many pages on the Web contain incorrect or
unverifiable information. Therefore, there is a
growing demand for technologies that can en-
able us to obtain reliable information. How-
ever, it would be almost impossible to auto-
matically ascertain the accuracy of informa-
tion presented on the Web. Hence, the second-
best approach is the development of a sup-
porting method for judging the credibility of
information on the Web.
Presently, when we wish to judge the credi-
bility of information on the Web, we often read
some relevant Web documents retrieved via
Web search engines. However, Web search en-
gines do not provide any suggestions in cases
where the content of some documents conflicts
with the content of other documents. Further-
more, the retrieved documents are too many
to read and may not be ranked according to
the credibility of the information they provide.
In other words, information retrieval is not
sufficient to support users? assessments of the
credibility of information, and therefore, ad-
ditional techniques are required for the same.
Several previous researches have been con-
ducted for developing such techniques. Juffin-
ger et al (2009) ranked blogs in terms of
their concurrence with well-verified informa-
tion from sources such as a news corpus.
Miyazaki et al (2009) devised a method for
extracting the description of the information
sender, namely, the person or organization
providing texts in Web pages. Ohshima et
al. (2009) proposed a method for reranking
Web pages according to users? regionality,
which depends on two factors: uniformity and
proximity. While the abovementioned stud-
ies mainly facilitate users? assessments of the
credibility of individual Web pages, the fol-
lowing studies deal with the issues of cred-
ibility of information in multiple documents
on the Web. Murakami et al (2009) pro-
posed a method to analyze semantic rela-
tionships such as agreement, conflict, or ev-
idence between texts on the Web. Kawahara
et al (2009) reported a method for present-
ing overviews of evaluative information such
as positive/negative opinions.
Although the above techniques facilitate
users? assessments of credibility of information
on the Web, there is still room for methods
that support users? judgment. For example,
when the truth of a statement1 ?Diesel en-
gines are harmful to the environment? is to
1In this paper, a statement is defined as text such
as an opinion, evaluation, or objective fact.
1140
	
	
				
		
		
			
		

					
	
					
	
  !		

	
		"	
	!	#		$	!	
	
			
				
		


	

	 $ 		$
	

	!#$ !		
%
		
	!	"#		


	


#

%
	
	
	
	
			
	

	






	
 	

		


 	!
"
 
#
! 
!

$
	




Figure 1: An example of the mediatory summary.
be verified, the following two contradictory
groups of Web documents are obtained: one
stating that ?Diesel engines are harmful to
the environment? and the other stating that
?Diesel engines are not harmful to the envi-
ronment.? How does one resolve this conflict?
Are the contents of one group that contains
a less reasonable description wrong? On the
other hand, if contents of both these groups
are correct, then why do they appear to be
contradictory to each other? A display of
only the overview of statements in Web doc-
uments and the relationships between these
statements does not always provide sufficient
information to answer these questions.
In order to direct users to a reasonable in-
terpretation written by one author, Kaneko et
al. (2009) proposed the notion of a mediatory
summary for pseudo conflicts, which are rela-
tionships between statements that appear to
contradict each other at first glance but can
coexist under a certain situation. However,
Kaneko et aldid not describe any algorithms for
automatically generating the mediatory sum-
mary. Therefore, in this paper, we propose a
method for automatically generating the me-
diatory summary and demonstrate the effec-
tiveness of generated mediatory summaries.
The rest of this paper is organized as fol-
lows. In Section 2, we describe the concept
of a mediatory summary and the features re-
quired for automatically generating it. In Sec-
tion 3, we describe an algorithm for generation
of the mediatory summary. In Section 4, we
present experimental results to demonstrate
the effectiveness of the generated mediatory
summary. Section 5 provides the conclusion.
2 Mediatory Summary
The generation of the mediatory summary is
a type of informative summarization based on
passage extraction. Figure 1 shows an exam-
ple of the ideal mediatory summary for the
query ?Are diesel engines harmful to the en-
vironment?? The text in boxes with thick
lines is extracted from Web documents, and
the italicized text is generated through tem-
plates. The user is shown both positive and
negative responses to the query and appropri-
ately guided on how to interpret them. One
of the most difficult issues in the automated
generation of the mediatory summary is the
extraction of the most suitable passage that is
used for interpretation.2
2Owing to space limitations, we have omitted the
discussion on generation through templetes.
1141
Nakano et al (2010) constructed a text
summarization corpus for determining the
credibility of information on the Web. Their
corpus contains six query statements, the Web
document collections retrieved for each query,
and 24 summaries made by four persons per
query. We analyzed these summaries from the
viewpoint of mediatory summary generation
and observed that mediatory summaries usu-
ally display the following three features.
The first feature is the high relevance to a
given query. We can approximately determine
whether the text displays this feature by ex-
amining whether or not it contains content
words in the query such as ?diesel,? as in Fig-
ure 1. The second feature is fairness, or in
other words, evenly describing both positive
and negative opinions. We can approximately
determine whether the text displays this fea-
ture by examining whether or not it contains
words for both opinions and having different,
typically opposite meanings such as ?lot? and
?less,? as in Figure 1. It should be noted that
words with opposite meanings are not lim-
ited to antonyms. In the case of the query
?Are diesel engines harmful to the environ-
ment?,? ?carbon dioxide? and ?smog-forming
pollutants? should be also regarded as words
with opposite meanings. The third feature is
the high density of words of the above two
types in a text. In addition to appropriateness
of a text with high density as a summary, such
a text is likely to be a text contrasting both
sides of contents.
3 Proposed Method
3.1 Outline
We propose a method for generating media-
tory summaries by extracting passages that
display the features described in the previous
section. First, we define the content words
related to the topic of a query as topic key-
words.3 Next, we define the content words cor-
3Although topic words are not restricted to words
in the given query, we use only the words in the given
query as topic words in this paper. Inclusion of words
other than those in the given query is a topic for future
research.
	



	

	

	

	



	

	
	



	








	





 	


Figure 2: Outline of the proposed method.
responding to the positive and negative opin-
ions as positive keywords and negative key-
words, respectively. Although topic keywords
appear in the given query, positive/negative
keywords hardly appear in the given query.
Therefore, positive/negative keywords have to
be extracted from other text.
Figure 2 shows the outline of the proposed
method. First, in order to find contents op-
posed to a given query, inverse queries are
generated. We define an inverse query as a
query generated by replacing a word in the
given query with its antonym. Next, the given
query and the inverse queries are used to re-
tain three sets of Web documents, which are
likely to contain more positive keywords, neg-
ative keywords, and neither. The topic, posi-
tive, and negative keywords are then extracted
from these sets. Finally, the extracted key-
words are used to extract passages from the
three sets of Web documents; these passages
are ranked in order of the score described in
Section 3.5 as a mediatory summary.
1142
3.2 Inverse Query Generation
Inverse queries are generated by simply re-
placing a word in the given query with an
antonym of the word using a dictionary for
antonyms. For example, if the query is ?Is
safety of LASIK operation high?,? and if
?risk? and ?low? are input into the dictionary
as antonyms of ?safety? and ?high,? respec-
tively, then the two generated inverse queries
are ?Is risk of LASIK operation high?? and
?Is safety of LASIK operation low?? Positive
keywords are words in the given query that
are to be replaced with their opposite words
in the inverse queries. On the other hand,
the newly introduced opposite words in the
inverse queries are regarded as negative key-
words. In the above example, ?safety? and
?high? are positive keywords, whereas ?risk?
and ?low? are negative ones. If there is no re-
placeable word, the inverse query is not gen-
erated.
3.3 Web Document Retrieval
Using a given query and the corresponding
inverse queries, Web documents are retrieved
via TSUBAKI (Shinzato et al, 2008); TSUB-
AKI is an open-search engine with aninfras-
tructure based on deep Japanese natural lan-
guage processing, and it can accept a natural-
language sentence as a query. The number of
documents retrieved per query is tentatively
set to 100 in our experiment described in Sec-
tion 4.
The retrieved documents are classified into
the following three document sets: one con-
taining documents retrieved by the given
query but not retrieved by the inverse queries,
one containing documents retrieved by the in-
verse queries but not retrieved by the given
query, and one containing documents re-
trieved by both the given query and the in-
verse queries. These three document sets are
termed Dquery, Dinverse, and Dboth, respec-
tively. Words appearing frequently in Dquery
are likely to be positive keywords, and those
appearing frequently in Dinverse, negative key-
words.
TSUBAKI has an optional function for au-
Table 1: An example of extracted keywords.
rank rank rank
word tf POS NEG polarity
LASIK 1 1 1 other
operation 2 2 2 other
eyesight 3 3 3 other
examination 19 13 60 positive
glasses 47 58 75 other
blindness 61 5,045 20 negative
effect 66 72 111 positive
complications 77 206 58 negative
tomatically expanding a submitted query by
including the negative form of the main verb
in the query. For example, if ?Is safety of
LASIK operation high?? is the submitted
query, then the expanded query is ?Is safety of
LASIK operation not high?? The documents
retrieved for the expanded query derived from
the given query are regarded as documents
retrieved for inverse queries. Similarly, the
documents retrieved for the expanded query
derived from the inverse queries are regarded
as documents retrieved for the given query.
Hence, even if there is no inverse query, pos-
sibly conflicting documents can be retrieved.
3.4 Keyword Extraction
Positive and negative keywords are extracted
from the retrieved Web documents as follows.
First, the positive score scPOS(w) and nega-
tive score scNEG(w) of a word w are calculated
by Equations (1) and (2), respectively:
scPOS(w) =
df(w,Dquery) ? tf (w)
df(w,Dinverse) + 1
(1)
scNEG(w) =
df(w,Dinverse) ? tf (w)
df(w,Dquery) + 1
(2)
where tf (w) is the frequency of w in all re-
trieved documents, and df(w,D) is the num-
ber of documents containing w in D. The
scPOS(w) is higher and scNEG(w) is lower if
the word w appears more frequently in the
documents retrieved by the given query and
less frequently in the documents retrieved by
the inverse queries. The frequency tf (w) is
1143
used to express these scores in order to con-
sider the global importance of w in the entire
retrieved document set.
Because words such as ?LASIK? in the
query ?Is safety of LASIK operation high??
have high scores in terms of both scPOS(w)
and scNEG(w), such words should not be ex-
tracted as positive or negative keywords. Be-
cause the number of documents in Dquery
is different from that in Dinverse, scPOS(w)
cannot be directly compared with scNEG(w).
Hence, scPOS(w) and scNEG(w) are nor-
malized, and rankPOS(w) and rankNEG(w)
are compared. The ranking functions
rankPOS(w) and rankNEG(w) are defined as
the nth place ranks when all words are ranked
in the descending order of scPOS(w) and
scNEG(w), respectively. We consider the top
Crank words on the tf () ranking as candi-
dates for possible keywords. If rankPOS(w)?
rankNEG(w) or rankNEG(w)?rankPOS(w) is
greater than Cdif , then we regard w as a pos-
itive or negative keyword, respectively. Ta-
ble 1 shows an example of the extracted key-
words when the given query is ?Is safety of
LASIK operation high?? In this paper, Crank
and Cdif are tentatively set to 100 and 20, re-
spectively, in a preliminary experiment using
several queries except the ones described in
Section 4.4 Finally, the positive/negative key-
words, mentioned in Section 3.2, are respec-
tively added to the positive/negative keyword
sets described above.
The topic keywords are the words in the
given query excluding the positive/negative
keywords.
3.5 Passage Extraction
Passages suitable for a mediatory summary
are extracted through the following four
stages.
For all sentences in the document sets de-
scribed in Section 3.3, the first stage involves
the recognition of sentences that are useless
for mediatory summary. We regard insuffi-
4The parameters used in this paper are set tenta-
tively. Determining the optimal parameters is a topic
for future research.
cient or incomplete sentences as useless sen-
tences. We simply consider a sentence to
be sufficient when the sentence has, at least,
one verb phrase and one noun phrase and
when the sentence contains more than two
verb/noun phrases. We simply consider a sen-
tence to be insufficient if it is not a sufficient
sentence. When the expression ?...,? which
indicates an omission, appears at the end of
a sentence, then we consider the sentence in-
complete. This recognition of sentences plays
an important role in the calculation of scores
in subsequent stages.
The second stage involves the calculation
of the score of each sentence. When KW is
defined as a set of all the topic, positive, and
negative keywords, the basic score scBAS(s) of
sentence s is calculated by Equation (3).
scBAS(s) =
?
w?KW appear(w, s)
|KW | (3)
where appear(w, s) is a function whose value
is 1 if w appears in s and 0 otherwise. If s
contains many different keywords, scBAS(s)
acquires a high value. If s is recognized as
a useless sentence, then scBAS(s) is multi-
plied by Cuseless as a penalty. In this pa-
per, Cuseless is tentatively set to 0.5 in the
case of ungrammatical sentences and 0 in the
case that the end of the sentences is omitted.
When the score of a sentence is calculated,
the fairness described in Section 2 is deter-
mined in the following manner. If s contains
positive/negative keywords besides topic key-
words, the scBAS(s) is multiplied by Cbasic.
The negative form of positive/negative key-
words is considered, and Cbasic is tentatively
set to two if either a positive or negative ex-
pression appears in s and to three if both pos-
itive and negative expressions appear in s.
The third stage involves the application of
a smoothing method to raw scores of a sen-
tence in order to suppress over-fragmentation
of passages. As given in Equation (4), the
smoothed score scSMO(si) for the ith sentence
si in a document is calculated using the Hann
1144
function, whose window length is L.
scSMO(si) =
L
2?
j=?L2
(scBAS(si+j) ? hf(j)) (4)
hf(k) = 0.5 + 0.5 cos 2? kL (5)
The value of L is tentatively set as 5 in
this study. Insufficient sentences may con-
vey useful information to readers when they
are embedded in an appropriate context. On
the other hand, incomplete sentences do not.
Therefore, scSMO(s) of such omitted sen-
tences is set as 0 even after smoothing. If all
types of keywords appear in the Hann win-
dow, then scSMO(s) is multiplied by Csmooth
because a passage containing s is likely to be
a part of a mediatory summary. The value of
Csmooth is tentatively set as 2 in this study.
The fourth stage involves the extraction and
ranking of passages. Every series of sentences
with scSMO(s) greater than 1N of the maxi-
mum score in the document is extracted as
a passage. N is tentatively set as 3. The
score scPAS(p) of passage p is the highest score
scSMO(s) of sentence s in the passage. If all
types of keywords appear in p, the scPAS(p)
is multiplied by Cpassage, as in the third stage.
Note that the length of the extracted passages
is not set to L sentences and that passages
that contain sentences multiplied by Csmooth
are not always multiplied by Cpassage. Cpassage
is tentatively set as 3 in this study. Be-
cause of summarization, the passages whose
lengths are nearer to the ideal length Clength
are ranked higher. The final score scFIN(p) is
calculated by Equation (6).
scFIN(p) = exp (scPAS(p) ? ? ? er(p)) (6)
er(p) = |Clength ? nc(p)| (7)
where nc(p) is the number of characters in p,
and ? is a coefficient. Clength and ? are ten-
tatively set to 300 and 0.02, respectively.
4 Experiment
4.1 Conditions
Because the proposed method is the first
method for automated generation of media-
tory summary, there is no existing method to
directly compare our proposed method with.
Therefore, we compare the proposed method
with the following three methods. The first
method, KWtf , uses frequent words instead of
the three types of keywords described in Sec-
tion 3.4. The second method, LinTSU , uses an
existing summarization module for summariz-
ing the top documents in order of the score in
which TSUBAKI retrieves them. The third
method, LinscF IN , uses the same existing
summarization module for summarizing docu-
ments containing the top passages in order of
the score scFIN () described in Section 3.5. We
employ Lingua::JA::Summarize::Extract5 as a
summarization module, which extracts sen-
tences containing more characteristic words;
this extraction is based on the word frequency
and word bigram frequency in a given docu-
ment.
KWtf is compared with the proposed
method in order to investigate the effective-
ness of keywords in terms of polarity. One of
the functions of the proposed method is the
classification of the top Crank words on the
tf () ranking into positive keywords, negative
keywords, and others in order to determine
the fairness described in Section 2. KWtf is
simply based on frequent words and does not
classify them. In other words, all of the top
Crank words on the tf () ranking are used as
keywords without polarity. It should be noted
that no rewards can be obtained using Cbasic,
Csmooth, and Cpassage in the passage extrac-
tion described in Section 3.5, although penal-
ties can be obtained using Cuseless and Clength.
LinTSU is compared with the proposed
method in order to clarify the difference be-
tween the summarization by our method and
that by a method used for general purposes.
In other words, we investigate whether or not
the extraction of sentences containing more
characteristic words is sufficient for generat-
ing mediatory summaries.
LinscF IN is compared with the proposed
method in order to investigate the appropri-
5http://search.cpan.org/?yappo/Lingua-JA-
Summarize-Extract-0.02/
1145
Table 2: Average precision of appropriateness of summaries generated by each query.
Top 3 Top 5 Top 10 Top 3 Top 5 Top 10
Are diesel engines harmful to the environment? Is safety of LASIK operation high?
Proposed 100.0% 60.0% 36.7% Proposed 33.3% 20.0% 20.0%
KWtf 0.0% 0.0% 0.0% KWtf 0.0% 0.0% 0.0%
LinTSU 66.7% 40.0% 26.7% LinTSU 33.3% 20.0% 30.0%
LinscFIN 0.0% 13.3% 16.7% LinscFIN 0.0% 0.0% 0.0%
Are whales endangered species? Does asbestos have toxics?
Proposed 55.6% 33.3% 30.0% Proposed 33.3% 40.0% 56.7%
KWtf 0.0% 0.0% 0.0% KWtf 33.3% 20.0% 30.0%
LinTSU 11.1% 20.0% 13.3% LinTSU 33.3% 20.0% 30.0%
LinscFIN 22.2% 13.3% 10.0% LinscFIN 0.0% 20.0% 30.0%
Is catch and release a better way of fishing? Does carbon dioxide cause global warming?
Proposed 33.3% 26.7% 13.3% Proposed 0.0% 0.0% 6.7%
KWtf 0.0% 0.0% 6.7% KWtf 0.0% 0.0% 0.0%
LinTSU 66.7% 46.7% 26.7% LinTSU 11.1% 6.7% 3.3%
LinscFIN 33.3% 26.7% 13.3% LinscFIN 0.0% 0.0% 13.3%
ateness of the extracted passages. Another
function of the proposed method is the rerank-
ing of passages regardless of the order of doc-
uments containing the passages during docu-
ment retrieval. Therefore, a set of documents
summarized by the proposed method may be
different from the set of documents summa-
rized by LinTSU . Therefore, it is observed that
LinscF IN handles the same documents as the
proposed method.
Because the mediatory summary is a novel
concept, methods for evaluating it have not
been developed yet. Although ROUGE (Lin
and Hovy, 2003) is one of the most popular
methods for evaluation of summaries, it may
not be appropriate for the evaluation of the
mediatory summary because the scoring based
on N-gram in this method cannot be used to
consider the fairness described in Section 2.
Therefore, we evaluate the methods through
the binary judgment of three human asses-
sors, If the top summaries produced by each
method are deemed to be appropriate by the
three human assessors, we will be able to fa-
cilitate users? assessments of the contradictory
opinions that are relevant to the given query.
Because we consider that filling a passage
with all the information necessary to facili-
tate users? assessments is more important than
shortening the passage under conditions for
generation of mediatory summary, we imposed
no limitation on the length of passages but the
resultant penalty is obtained using Equations
(6) and (7). The average length of all sum-
maries generated by the proposed method and
KWtf was 288.4 characters, and none of the
summaries exceeded 500 characters. There-
fore, we allowed LinTSU and LinscF IN to gen-
erate summaries as long as 500 characters,
which is about 200 characters longer than
summaries generated by the proposed method
and KWtf . We instructed the assessors to not
judge the appropriateness of the summaries on
the basis of their length.
For the experiment, we prepared the follow-
ing six queries: ?Are diesel engines harmful
to the environment?,? ?Is safety of LASIK
operation high?,? ?Are whales endangered
species?,? ?Does asbestos have toxics?,? ?Is
catch and release a better way of fishing?,?
and ?Does carbon dioxide cause global warm-
ing?? We used the Japanese morphological
analyzer, MeCab.6
6http://mecab.sourceforge.net/ (in Japanese)
1146
Table 3: Average precision of assessors in
terms of appropriateness of overall summaries.
Top 3 Top 5 Top 10
Proposed 42.6% 30.0% 27.2%
KWtf 5.6% 3.3% 6.1%
LinTSU 37.0% 25.6% 21.7%
LinscFIN 9.3% 12.2% 13.9%
4.2 Result and Consideration
The kappa values between each pair of asses-
sors? judgments on the appropriateness of the
summaries were 0.79, 0.77, and 0.76, respec-
tively; these values indicate a high level of
agreement among assessors? judgments.
Table 2 shows the average precision7 of the
assessors in terms of appropriateness of sum-
maries on the basis of responses to each query,
and Table 3 shows the overall precision. The
columns in Tables 2 and 3 show the precision
of appropriateness for the top 3, top 5, and
top 10 summaries produced by each method.
It should be noted that there are only a few
passages suitable for mediatory summary in
all of the retrieved documents, and therefore
a method for placing such suitable passages at
a higher rank is more effective. We confirmed
that the proposed method provided the best
overall results among all the compared meth-
ods.
The difference between the proposed
method and KWtf shows that classification
of frequent words into positive keywords, neg-
ative ones, and others, in other words, the fair-
ness described in Section 2 contributed to gen-
eration of appropriate mediatory summaries.
The difference between LinTSU and
LinscF IN indicates that the order of the
score scFIN () described in Section 3.5 was
different from that of the score of TSUB-
AKI. Lingua::JA::Summarize::Extract could
not extract the appropriate passages from
7We use precision, which represents #correct out-
puts/#total outputs, as an evaluation measure because
it is difficult to calculate the recall of the mediatory
summaries that are dynamically generated from Web
documents and because users tend to read just a few
of the summaries generated by the system.
document sets on the basis of scFIN(), even
though the document sets contained the same
appropriate passages that were extracted by
the proposed method. Therefore, summa-
rization for a general purpose is insufficient
for generation of mediatory summaries, and
the proposed method can provide more
appropriate mediatory summaries.
However, the results of the queries ?Is catch
and release a better way of fishing?? and
?Does carbon dioxide cause global warming??
in Table 2 show that the proposed method
could not extract all the appropriate passages
that were extracted by LinTSU . We aim to im-
prove the proposed method in future research.
5 Conclusion
We proposed a method for automated genera-
tion of mediatory summaries in order to facil-
itate users? assessment of the credibility of in-
formation on the Web. A mediatory summary
is generated by extracting a passage from Web
documents on the basis of their relevance to
a given query, fairness, and density of key-
words, which are features of the summaries
constructed to determine the credibility of in-
formation on the Web. We demonstrated the
effectiveness of the generated mediatory sum-
mary in comparison with the summaries of
Web documents produced by Web search en-
gines.
Acknowledgement
This research is partially supported by the Na-
tional Institute of Information and Communi-
cations Technology, Japan.
References
Juffinger, Andreas, Michael Granitzer, and Elisa-
beth Lex. 2009. Blog credibility ranking by ex-
ploiting verified content. In Proceedings of the
Second Workshop on Information Credibility on
the Web (WICOW 2009), pages 51?57.
Kaneko, Koichi, Hideyuki Shibuki, Masahiro
Nakano, Rintaro Miyazaki, Madoka Ishioroshi,
and Tatsunori Mori. 2009. Mediatory summary
1147
generation: Summary-passage extraction for in-
formation credibility on the web. In Proceed-
ings of the 23rd Pacific Asia Conference on Lan-
guage, Information and Computation (PACLIC
23), pages 240?249.
Kawahara, Daisuke, Tetsuji Nakagawa, Takuya
Kawada, Kentaro Inui, and Sadao Kurohashi.
2009. Summarizing evaluative information on
the web for information credibility analysis. In
Proceedings of the 3rd International Univer-
sal Communication Symposium (IUCS 2009),
pages 187?192.
Lin, Chin-Yew and Eduard Hovy. 2003. Au-
tomatic evaluation of summaries using n-gram
co-occurrence statistics. In Proceedings of the
Human Language Technology Conference 2003
(HLT-NAACL-2003), pages 71?78.
Miyazaki, Rintaro, Ryo Momose, Hideyuki
Shibuki, and Tatsunori Mori. 2009. Using
web page layout for extraction of sender names.
In Proceedings of the 3rd International Univer-
sal Communication Symposium (IUCS 2009),
pages 181?186.
Murakami, Koji, Eric Nichols, Suguru Matsuyoshi,
Asuka Sumida, Shouko Masuda, Kentaro Inui,
and Yuji Matsumoto. 2009. Statement map:
Assisting information credibility analysis by vi-
sualizing arguments. In Proceedings of the Sec-
ond Workshop on Information Credibility on the
Web (WICOW 2009), pages 43?50.
Nakano, Masahiro, Hideyuki Shibuki, Rintaro
Miyazaki, Madoka Ishioroshi, Koichi Kaneko,
and Tatsunori Mori. 2010. Construction of text
summarization corpus for the credibility of in-
formation from the web. In Proceedings of the
7th Language Resources and Evaluation Confer-
ence (LREC 2010), pages 3125?3131.
Ohshima, Hiroaki, Satoshi Oyama, Hiroyuki
Kondo, and Katsumi Tanaka. 2009. Web infor-
mation credibility analysis by geographical so-
cial support. In Proceedings of the 3rd Inter-
national Universal Communication Symposium
(IUCS 2009), pages 193?196.
Shinzato, Keiji, Tomohide Shibata, Daisuke Kawa-
hara, Chikara Hashimoto, and Sadao Kuro-
hashi. 2008. Tsubaki: An open search en-
gine inflastructure for developing new informa-
tion access methodology. In Proceedings of the
Third International Joint Conference on Natu-
ral Language Processing (IJCNLP 2008), pages
189?196.
1148
