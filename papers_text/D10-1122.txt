Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1256?1265,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Hashing-based Approaches to Spelling Correction of Personal Names
Raghavendra Udupa
Microsoft Research India
Bangalore, India
raghavu@microsoft.com
Shaishav Kumar
Microsoft Research India
Bangalore, India
v-shaisk@microsoft.com
Abstract
We propose two hashing-based solutions to
the problem of fast and effective personal
names spelling correction in People Search
applications. The key idea behind our meth-
ods is to learn hash functions that map similar
names to similar (and compact) binary code-
words. The two methods differ in the data
they use for learning the hash functions - the
first method uses a set of names in a given lan-
guage/script whereas the second uses a set of
bilingual names. We show that both methods
give excellent retrieval performance in com-
parison to several baselines on two lists of
misspelled personal names. More over, the
method that uses bilingual data for learning
hash functions gives the best performance.
1 Introduction
Over the last few years, People Search has emerged
as an important search service. Unlike general Web
Search and Enterprise Search where users search for
information on a wide range of topics including peo-
ple, products, news, events, etc., People Search is
about people. Hence, personal names are used pre-
dominantly as queries in People Search. As in gen-
eral Web Search, a good percentage of queries in
People Search is misspelled. Naturally, spelling cor-
rection of misspelled personal names plays a very
important role in not only reducing the time and ef-
fort needed by users to find people they are search-
ing for but also in ensuring good user experience.
Spelling errors in personal names are of a differ-
ent nature compared to those in general text. Long
before People Search became widely popular, re-
searchers working on the problem of personal name
matching had recognized the human tendency to be
inexact in recollecting names from the memory and
specifying them. A study of personal names in
hospital databases found that only 39% of the er-
rors in the names were single typographical errors
(Friedman and Sideli, 1992)1. Further, multiple and
long distance typographical errors (Gregzorz Kon-
drak for Grzegorz Kondrak), phonetic errors (as in
Erik Bryl for Eric Brill), cognitive errors (as in Sil-
via Cucerzan for Silviu Cucerzan) and word substi-
tutions (as in Rob Moore for Bob Moore) are ob-
served relatively more frequently in personal names
compared to general text.
In addition to within-the-word errors, People
Search queries are plagued by errors that are not
usually seen in general text. The study by Fried-
man and Sideli discovered that 36% of the errors
were due to addition or deletion of a word (as in
Ricardo Baeza for Ricardo Baeza-Yates) (Friedman
and Sideli, 1992). Although word addition and dele-
tion generally do not come under the purview of
spelling correction, in People Search they are im-
portant and need to be addressed.
Standard approaches to general purpose spelling
correction are not well-suited for correcting mis-
spelled personal names. As pointed out by
(Cucerzan and Brill, 2004), these approaches ei-
ther try to correct individual words (and will fail to
correct Him Clijsters to Kim Clijsters) or employ
features based on relatively wide context windows
1In contrast, 80% of misspelled words in general text are due
to single typographical errors as found by (Damerau, 1964).
1256
which are not available for queries in Web Search
and People Search. Spelling correction techniques
meant for general purpose web-queries require large
volumes of training data in the form of query logs
for learning the error models (Cucerzan and Brill,
2004), (Ahmad and Kondrak, 2005). However,
query logs are not available in some applications
(e.g. Email address book search). Further, un-
like general purpose web-queries where word order
often matters, in People Search word order is lax
(e.g. I might search for either Kristina Toutanova or
Toutanova Kristina). Therefore, spelling correction
techniques that rely crucially on bigram and higher
order language models will fail on queries with a dif-
ferent word order than what is observed in the query
log.
Unlike general purpose Web Search where it is
not reasonable to assume the availability of a high-
coverage trusted lexicon, People Search typically
employs large authoritative name directories. For
instance, if one is searching for a friend on Face-
book, the correct spelling of the friend?s name exists
in the Facebook people directory2 (assuming that the
friend is a registered user of Facebook at the time of
the search). Similarly, if one is searching for a con-
tact in Enterprise address book, the correct spelling
of the contact is part of the address book. In fact,
even in Web Search, broad-coverage name directo-
ries are available in the form of Wikipedia, IMDB,
etc. The availability of large authoritative name di-
rectories that serve as the source of trusted spellings
of names throws open the possibility of correcting
misspelled personal names with the help of name
matching techniques (Pfeifer et al, 1996), (Chris-
ten, 2006), (Navarro et al, 2003). However, the best
of the name matching techniques can at best work
with a few thousand names to give acceptable re-
sponse time and accuracy. They do not scale up to
the needs of People Search applications where the
directories can have millions of names.
In this work, we develop hashing-based name
similarity search techniques and employ them for
spelling correction of personal names. The motiva-
tion for using hashing as a building block of spelling
correction is the following: given a query, we want
to return the global best match in the name directory
2http://www.facebook.com/directory/people/
that exceeds a similarity threshold. As matching the
query with the names in the directory is a time con-
suming task especially for large name directories,
we solve the search problem in two stages:
? NAME BUCKETING: For each token of the
query, we do an approximate nearest neighbor
search of the name tokens of the directory and
produce a list of candidates, i.e., tokens that are
approximate matches of the query token. Using
the list of candidate tokens, we extract the list
of candidate names which contain at least one
approximately matching token.
? NAME MATCHING: We do a rigorous match-
ing of the query with candidate names.
Clearly, our success in finding the right name sug-
gestion for the query in the NAME MATCHING
stage depends crucially on our success in getting
the right name suggestion in the list of candidates
produced by the NAME BUCKETING stage search.
Therefore, we need a name similarity search tech-
nique that can ensure very high recall without pro-
ducing too many candidates. Hashing is best suited
for this task of fast and approximate name match-
ing. We hash the query tokens as well as directory
tokens into d bit binary codes. With binary codes,
finding approximate matches for a query token is as
easy as finding all the database tokens that are at a
Hamming distance of r or less from the query token
in the binary code representation (Shakhnarovich et
al., 2008), (Weiss et al, 2008). When the binary
codes are compact, this search can be done in a frac-
tion of a second on directories containing millions
of names on a simple processor.
Our contributions are:
? We develop a novel data-driven technique for
learning hash functions for mapping similar
names to similar binary codes using a set of
names in a given language/script (i.e. monolin-
gual data). We formulate the problem of learn-
ing hash functions as an optmization problem
whose relaxation can be solved as a generalized
Eigenvalue problem. (Section 2.1).
? We show that hash functions can also be learnt
using bilingual data in the form of name equiv-
alents in two languages. We formulate the
1257
problem of learning hash functions as an opt-
mization problem whose relaxation can be
solved using Canonical Correlation Analysis.
(Section 2.2)
? We develop new similarity measures for match-
ing names (Section 3.1).
? We evaluate the two methods systematically
and compare our performance against multiple
baselines. (Section 5).
2 Learning Hash Functions
In this section, we develop two techniques for learn-
ing hash functions using names as training data. In
the first approach, we use monolingual data consist-
ing of names in a language whereas in the second we
use bilingual name pairs. In both techniques, the key
idea is the same: we learn hash functions that map
similar names in the training data to similar code-
words.
2.1 M-HASH: Learning with Monolingual
Names Data
Let (s, s?) be a pair of names and w (s, s?) be their
similarity3. We are given a set of name pairs T =
{(s, s?)} as the training data. Let ? (s) ? Rd1 be the
feature representation of s. We want to learn a hash
function f that maps each name to a d bit codeword:
f : s 7? {?1, 1}d. We also want the Hamming dis-
tance of the codeword of s to the codeword of s? be
small when w (s, s?) is large. Further, we want each
bit of the codewords to be either 1 or ?1 with equal
probablity and the successive bits of the codewords
to be uncorrelated. Thus we arrive at the following
optimization problem4:
minimize :
?
(s,s?)?T
w
(
s, s?
) ?
?f (s)? f
(
s?
)?
?2
s.t. :
?
s:(s,s?)?T
f (s) = 0
?
s:(s,s?)?T
f (s) f (s)T = ?2Id
f (s) , f
(
s?
)
? {?1, 1}d
3We used 1? length normalized Edit Distance between s
and s? as w (s, s?).
4Note that the Hamming distance of a codeword y to another
codeword y? is 14 ?y ? y
??2.
where Id is an identity matrix of size d? d.
Note that the second constraint helps us avoid the
trap of mapping all names to the same codeword and
thereby making the Hamming error zero while satis-
fying the first and last constraints.
It can be shown that the above minimization prob-
lem is NP-hard even for 1-bit codewords (Weiss et
al., 2008). Further, the optimal solution gives code-
words only for the names in the training data. As we
want f to be defined for all s, we address the out-of-
sample extension problem by relaxing f as follows5:
fR (s) = A
T? (s) =
(
aT1 ? (s) , . . . , a
T
d ? (s)
)T
(1)
where A = [a1, . . . , ad] ? Rd1?d is a rank d matrix
(d ? d1).
After the linear relaxation (Equation 1), the first
constraint simply means that the data be centered,
i.e., have zero mean. We center ? by subtracting the
mean of ? from every ? (s) ? ? to get ??.
Subsequent to the above relaxation, we get the
following optimization problem:
minimize : Tr AT ??L??TA (2)
s.t. : (3)
AT ????TA = ?2Id (4)
whereL is the graph Laplacian for the similarity ma-
trix W defined by the pairwise similarities w (s, s?).
The minimization problem can be transformed
into a generalized Eigenvalue problem and solved
efficiently using either Cholesky factorization or QZ
algorithm (Golub and Van Loan, 1996):
??L??TA = ????TA? (5)
where ? is a d? d diagonal matrix.
OnceA has been estimated from the training data,
the codeword of a name s can be produced by bina-
rizing each coordinate of fR (s):
f (s) =
(
sgn
(
aT1 ? (s)
)
, . . . , sgn
(
aTd ? (s)
))T
(6)
where sgn(u) = 1 if u > 0 and?1 otherwise for all
u ? R.
5In contrast to our approach, Spectral Hashing, a well-
known hashing technique, makes the unrealistic assumption
that the training data is sampled from a multidimensional uni-
form distribution to address the out-of-sample extension prob-
lem (Weiss et al, 2008).
1258
In the reminder of this work, we call the system
that uses the hash function learnt from monolingual
data as M-HASH.
2.2 B-HASH: Learning with Bilingual Names
Data
Let (s, t) be a pair of name s and its transliteration
equivalent t in a different language/script. We are
given the set T = {(s, t)} as the training data. Let
? (s) ? Rd1 (and resp. ? (t) ? Rd2) be the feature
representation of s (and resp. t). We want to learn
a pair of hash functions f, g that map names to d bit
codewords: f : s 7? {?1, 1}d, g : t 7? {?1, 1}d.
We also want the Hamming distance of the code-
word of a name to the codeword of its transliteration
be small. As in Section 2.1, we want each bit of the
codewords to be either 1 or?1 with equal probablity
and the successive bits of the codewords to be uncor-
related. Thus we arrive at the following optimization
problem:
minimize :
?
(s,t)?T
?f (s)? g (t)?2
s.t. :
?
s:(s,t)?T
f (s) = 0
?
t:(s,t)?T
g (t) = 0
?
s:(s,t)?T
f (s) f (s)T = ?2Id
?
t:(s,t)?S
g (t) g (t)T = ?2Id
f (s) , g (t) ? {?1, 1}d
where Id is an identity matrix of size d? d.
As we want f (and resp. g) to be defined for all s
(and resp. t), we relax f (and resp. g) as follows:
fR (s) = A
T? (s) (7)
gR (t) = B
T? (s) (8)
where A = [a1, . . . , ad] ? Rd1?d and B =
[b1, . . . , bd] ? Rd2?d are rank d matrices.
As before, we center ? and ? to get ?? and ?? re-
spectively. Thus, we get the following optimization
problem:
minimize : Tr H
(
A,B; ??, ??
)
(9)
s.t. : (10)
AT ????TA = ?2Id (11)
BT ????TB = ?2Id (12)
where H
(
A,B; ??, ??
)
=
(
AT ???BT ??
)(
AT ???BT ??
)T
.
The minimization problem can be solved as a gen-
eralized Eigenvalue problem:
????TB = ????TA? (13)
????TA = ????TB? (14)
where ? is a d ? d diagonal matrix. Further, Equa-
tions 13 and 14 find the canonical coefficients of ??
and ?? (Hardoon et al, 2004).
As with monolingual learning, we get the code-
word of s by binarizing the coordinates of fR (s)6:
f (s) =
(
sgn
(
aT1 ? (s)
)
, . . . , sgn
(
aTd ? (s)
))T
(15)
In the reminder of this work, we call the system
that uses the hash function learnt from bilingual data
as B-HASH.
3 Similarity Score
In this section, we develop new techniques for com-
puting the similarity of names at token level as well
as a whole. We will use these techniques in the
NAME MATCHING stage of our algorithm (Sec-
tion 4.2.1).
3.1 Token-level Similarity
We use a logistic function over multiple distance
measures to compute the similarity between name
tokens s and s?:
K
(
s, s?
)
=
1
1 + e?
?
i ?idi(s,s
?)
. (16)
While a variety of distance measures can be
employed in Equation 16, two obvious choices
6As a biproduct of bilingual learning, we can hash names in
the second language using g:
g (t) =
(
sgn
(
bT1 ? (t)
)
, . . . , sgn
(
bTd ? (t)
))T
1259
are the normalized Damerau-Levenshtein edit dis-
tance between s and s? and the Hamming dis-
tance between the codewords of s and s? (=
?f (s)? f (s?)?). In our experiments, we found that
the continuous relaxation ?fR (s)? fR (s?)? was
better than ?f (s)? f (s?)? and hence we used it
with Damerau-Levenshtein edit distance. We esti-
mated ?1 and ?2 using a small held out set.
3.2 Multi-token Name Similarity
Let Q = s1s2 . . . sI and D = s?1s
?
2 . . . s
?
J be two
multi-token names. To compute the similarity be-
tween Q and D, we first form a weighted bipartite
graph with a node for each si and a node for each s?j
and set edge weight toK
(
si, s?j
)
. We then compute
the weight (?max) of the maximum weighted match-
ing7 in this graph. The similarity between Q and D
is then computed as
K (Q,D) =
?max
|I ? J + 1|
. (17)
4 Spelling Correction using Hashing
In this section, we describe our algorithm for
spelling correction using hashing as a building
block.
4.1 Indexing the Name Directory
Given a name directory, we break each name into its
constituent tokens and form a set of distinct name to-
kens. Using the name tokens and the original names,
we build an inverted index which, for each name to-
ken, lists all the names that have the token as a con-
stituent. Further, we hash each name token into a d
bit codeword as described in Equation 6 (and resp.
Equation 15) when using the hash function learnt on
monolingual data (and resp. bilingual data) and store
in a hash table.
4.2 Querying the Name Directory
Querying is done in two stages:
NAME BUCKETING and NAME MATCHING.
7In practice, a maximal matching computed using a greedy
approach suffices since many of the edges in the bipartite graph
have low weight.
4.2.1 Name Bucketing
Given a query Q = s1s2 . . . sI , we hash each si
into a codeword yi and retrieve all codewords in the
hash table that are at a Hamming distance of r or
less from yi. We rank the name tokens thus retrieved
using the token level similarity score of Section 3.1
and retain only the top 100. Using the top tokens, we
get al names which contain any of the name tokens
as a constituent to form the pool of candidates C for
the NAME MATCHING stage.
4.2.2 Name Matching
First we find the best match for a query Q in the
set of candidates C as follows:
D? = argmax
D?C
K (Q,D) . (18)
Next we suggest D? as the correction for Q if
K (Q,D?) exceeds a certain empirically determined
threshold.
5 Experiments and Results
We now discuss the experiments we conducted to
study the retrieval performance of the two hashing-
based approaches developed in the previous sec-
tions. Apart from evaluating the systems on test sets
using different name directories, we were interested
in comparing our systems with several baselines, un-
derstanding the effect of some of the choices we
made (e.g. training data size, conjugate language)
and comparative analysis of retrieval performance
on queries of different complexity.
5.1 Experimental Setup
We tested the proposed hashing-based spelling cor-
rection algorithms on two test sets:
? DUMBTIONARY: 1231 misspellings of var-
ious names from Dumbtionary8 and a name
directory consisting of about 550, 000 names
gleaned from the English Wikipedia. Each of
the misspellings had a correct spelling in the
name directory.
? INTRANET: 200 misspellings of employees
taken from the search logs of the intranet
of a large organization and a name directory
8http://www.dumbtionary.com
1260
consisting of about 150, 000 employee names.
Each of the misspellings had a correct spelling
in the name directory.
Table 1 shows the average edit distance of a mis-
spelling from the correct name. Compared to
DUMBTIONARY, the misspellings in INTRANET
are more severe as the relatively high edit distance
indicates. Thus, INTRANET represents very hard
cases for spelling correction.
Test Set Average Std. Dev.
DUMBTIONARY 1.39 0.76
INTRANET 2.33 1.60
Table 1: Edit distance of a misspelling from the correct
name.
5.1.1 Training
For M-HASH, we used 30,000 single token
names in English (sampled from the list of names
in the Internet Movie Database9) as training data
and for B-HASH we used 14,941 parallel single to-
ken names in English-Hindi 10. Each name was
represented as a feature vector over character bi-
grams. Thus, the name token Klein has the bigrams
{?k, kl, le, ei, in, n?} as the features.
We learnt the hash functions from the training
data by solving the generalized Eigenvalue problems
of Sections 2.1 and 2.2. For both M-HASH and B-
HASH we used the top 32 Eigenvectors to form the
hash function resulting in a 32 bit representation for
every name token11.
5.1.2 Performance Metric
We measured the performance of all the systems
using Precision@1, the fraction of names for which
a correct spelling was suggested at Rank 1.
5.1.3 Baselines
The baselines are two popular search engines
(S1 and S2), Double Metaphone (DM), a widely
9http://www.imdb.com
10We obtained the names from the organizers
of NEWS2009 workshop (http://www.acl-ijcnlp-
2009.org/workshops/NEWS2009/pages/sharedtask.html).
11We experimented with codewords of various lengths and
found that the 32 bit representation gave the best tradeoff be-
tween retrieval accuracy and speed.
used phonetic search algorithm (Philips, 2000) and
BM25, a very popular Information Retrieval algo-
rithm (Manning et al, 2008). To use BM25 algo-
rithm for spelling correction, we represented each
name as a bag of bigrams and set the parameters K
and b to 2 and 0.75 respectively.
5.2 Results
5.2.1 DUMBTIONARY
Table 2 compares the results of the hashing-based
systems with the baselines on DUMBTIONARY. As
the misspellings in DUMBTIONARY are relatively
easier to correct, all the systems give reasonably
good retrieval results. Nevertheless, the results of
M-HASH and B-HASH are substantially better than
the baselines. M-HASH reduced the error over the
best baseline (S1) by 13.04% whereas B-HASH re-
duced by 46.17% (Table 6).
M-HASH B-HASH S1 S2 DM BM25
87.93 92.53 86.12 79.33 78.95 84.70
Table 2: Precision@1 of the various systems on DUMB-
TIONARY.
To get a deeper understanding of the retrieval per-
formance of the various systems, we studied queries
of varying complexity of misspelling. Table 3 com-
pares the results of our systems with S1 for queries
that are at various edit distances from the correct
names. We observe that M-HASH and B-HASH are
better than S1 in dealing with relatively less severe
misspellings. More interestingly, B-HASH is con-
sistently and significantly better than S1 even when
the misspellings are severe.
Distance M-HASH B-HASH S1
1 96.18 96.55 89.59
2 81.79 87.42 75.76
3 44.07 67.80 59.65
4 21.05 31.58 29.42
5 0.00 37.50 0.00
Table 3: Precision@1 for queries at various edit distances
on DUMBTIONARY.
5.2.2 INTRANET
For INTRANET, search engines could not be used
as baselines and therefore we compare our systems
1261
with Double Metaphone and BM25 in Table 4. We
observe that both M-HASH and B-HASH give sign-
ficantly better retrieval results than the baselines. M-
HASH reduced the error by 36.20% over Double
Metaphone whereas B-HASH reduced it by 51.73%.
Relative to BM25, M-HASH reduced the error by
31.87% whereas B-HASH reduced it by 48.44%.
M-HASH B-HASH DM BM25
70.65 77.79 54.00 56.92
Table 4: Precision@1 of the various systems on IN-
TRANET.
Table 5 shows the results of our systems for
queries that are at various edit distances from the
correct names. We observe that the retrieval results
for each category of queries are consistent with the
results on DUMBTIONARY. As before, B-HASH
gives signficantly better results than M-HASH.
Distance M-HASH B-HASH
1 82.76 87.93
2 57.14 72.86
3 34.29 65.71
4 38.46 53.85
5 6.67 26.67
Table 5: Precision@1 for queries at various edit distances
on INTRANET.
Test Set M-HASH B-HASH
DUMBTIONARY 13.04 46.17
INTRANET 36.20 51.73
Table 6: Percentage error reduction over the best base-
line.
5.2.3 Effect of Training Data Size
As both M-HASH and B-HASH are data driven
systems, the effect of training data size on retrieval
performance is important to study. Table 7 com-
pares the results for systems trained with various
amounts of training data on DUMBTIONARY. B-
HASH trained with just 1000 name pairs gives
95.5% of the performance of B-HASH trained with
15000 name pairs. Similarly, M-HASH trained with
1000 names gives 98.5% of the performance of
M-HASH trained with 30000 name pairs. This is
probably because the spelling mistakes in DUMB-
TIONARY are relatively easy to correct.
Table 8 shows the results on INTRANET. We see
that increase in the size of training data brings sub-
stantial returns for B-HASH. In contrast, M-HASH
gives the best results at 5000 and does not seem to
benefit from additional training data.
Size M-HASH B-HASH
1000 86.60 88.34
5000 87.36 91.13
10000 86.96 92.53
15000 87.19 92.20
30000 87.93 -
Table 7: Precision@1 on DUMBTIONARY as a function
of training data size.
Size M-HASH B-HASH
1000 66.04 66.03
5000 70.65 72.67
10000 68.09 75.26
15000 68.60 77.79
30000 65.40 -
Table 8: Precision@1 on INTRANET as a function of
training data size.
5.2.4 Effect of Conjugate Language
In Sections 5.2.1 and 5.2.2, we saw that bilingual
data gives substantially better results than monolin-
gual data. In the experiments with bilingual data,
we used English-Hindi data for training B-HASH.
A natural question to ask is what happens when we
use someother language, say Hebrew or Russian or
Tamil, instead of Hindi. In other words, does the
retrieval performance, on an average, vary substan-
tially with the conjugate language?
Table 9 compares the results on DUMB-
TIONARY when B-HASH was trained using
English-Hindi, English-Hebrew, English-Russian,
and English-Tamil data. We see that the retrieval
results are good despite the differences in the script
and language. Clearly, the source language (English
in our experiments) benefits from being paired with
any target language. However, some languages seem
1262
to give substantially better results than others when
used as the conjugate language. For instance, Hindi
as a conjugate for English seems to be better than
Tamil. At the time of writing this paper, we do not
know the reason for this behavior. We believe that a
combination of factors including feature representa-
tion, training data, and language-specific confusion
matrix need to be studied in greater depth to say any-
thing conclusively about conjugate languages.
Conjugate DUMBTIONARY INTRANET
Hindi 92.53 77.79
Hebrew 91.30 71.68
Russian 89.42 64.94
Tamil 90.48 69.12
Table 9: Precision@1 of B-HASH for various conjugate
languages.
5.2.5 Error Analysis
We looked at cases where either M-HASH or
B-HASH (or both) failed to suggest the correct
spelling. It turns out that in the DUMBTIONARY
test set, for 81 misspelled names, both M-HASH and
B-HASH failed to suggest the correct name at rank
1. Similarly, in the case of INTRANET test set, both
M-HASH and B-HASH failed to suggest the correct
name at rank 1 for 47 queries. This suggests that
queries that are difficult for one system are also in
general difficult for the other system. However, B-
HASH was able to suggest correct names for some
of the queries where M-HASH failed. In fact, in the
INTRANET test set, whenever B-HASH failed, M-
HASH also failed. And interestingly, in the DUMB-
TIONARY test set, the average edit distance of the
query and the correct name for the cases where M-
HASH failed to get the correct name in top 10 while
B-HASH got it at rank 1 was 2.96. This could be be-
cause M-HASH attempts to map names with smaller
edit distances to similar codewords.
Table 10 shows some interesting cases we found
during error analysis. For the first query, M-HASH
suggested the correct name whereas B-HASH did
not. For the second query, both M-HASH and B-
HASH suggested the correct name. And for the third
query, B-HASH suggested the correct name whereas
M-HASH did not.
Query M-HASH B-HASH
John Tiler John Tyler John Tilley
Ddear Dragba Didear Drogba Didear Drogba
James Pol James Poe James Polk
Table 10: Error Analysis.
5.3 Query Response Time
The average query response time is a measure of
the speed of a system and is an important factor
in real deployments of a Spelling Correction sys-
tem. Ideally, one would like the average query re-
sponse time to be as small as possible. However, in
practice, average query response time is not only a
function of the algorithm?s computational complex-
ity but also the computational infrastructure support-
ing the system. In our expriments, we used a sin-
gle threaded implementation of M-HASH and B-
HASH on an Intel Xeon processor (2.86 GHz). Ta-
ble 11 shows the average query response time. We
note that M-HASH is substantially slower than B-
HASH. This is because the number of collisions
in the NAME BUCKETING stage is higher for M-
HASH.
We would like to point out that both
NAME BUCKETING and NAME MATCHING
stages can be multi-threaded on a multi-core ma-
chine and the query response time can be decreased
by an order easily. Further, the memory footprint
of the system is very small and the codewords
require 4.1 MB for the employees name directory
(150,000 names) and 13.8 MB for the Wikipedia
name directory (550,000 names).
Test Set MHASH BHASH
DUMBTIONARY 190 87
INTRANET 148 75
Table 11: Average response time in milliseconds (single
threaded system running on 2.86 GHz Intel Xeon Proces-
sor).
6 Related Work
Spelling correction of written text is a well stud-
ied problem (Kukich, 1996), (Jurafsky and Mar-
tin, 2008). The first approach to spelling correc-
1263
tion made use of a lexicon to correct out-of-lexicon
terms by finding the closest in-lexicon word (Dam-
erau, 1964). The similarity between a misspelled
word and an in-lexicon word was measured using
Edit Distance (Jurafsky and Martin, 2008). The next
class of approaches applied the noisy channel model
to correct single word spelling errors (Kernighan et
al., 1990), (Brill and Moore, 2000). A major flaw of
single word spelling correction algorithms is they do
not make use of the context of the word in correcting
the errors. The next stream of approaches explored
ways of exploiting the word?s context (Golding and
Roth, 1996), (Cucerzan and Brill, 2004). Recently,
several works have leveraged the Web for improved
spelling correction (Chen et al, 2007),(Islam and
Inkpen, 2009), (Whitelaw et al, 2009). Spelling cor-
rection algorithms targeted for web-search queries
have been developed making use of query logs and
click-thru data (Cucerzan and Brill, 2004), (Ah-
mad and Kondrak, 2005), (Sun et al, 2010). None
of these approaches focus exclusively on correcting
name misspellings.
Name matching techniques have been studied in
the context of database record deduplication, text
mining, and information retrieval (Christen, 2006),
(Pfeifer et al, 1996). Most techniques use one or
more measures of phonetic similarity and/or string
similarity. The popular phonetic similarity-based
techniques are Soundex, Phonix, and Metaphone
(Pfeifer et al, 1996). Some of the string similarity-
based techniques employ Damerau-Levenshtein edit
distance, Jaro distance or Winkler distance (Chris-
ten, 2006). Data driven approaches for learning edit
distance have also been proposed (Ristad and Yiani-
los, 1996). Most of these techniques either give poor
retrieval performance on large name directories or
do not scale.
Hashing techniques for similarity search is also a
well studied problem (Shakhnarovich et al, 2008).
Locality Sensitive Hashing (LSH) is a theoretically
grounded data-oblivious approach for using random
projections to define the hash functions for data ob-
jects with a single view (Charikar, 2002), (Andoni
and Indyk, 2006). Although LSH guarantees that
asymptotically the Hamming distance between the
codewords approaches the Euclidean distance be-
tween the data objects, it is known to produce long
codewords making it practically inefficient. Re-
cently data-aware approaches that employ Machine
Learning techniques to learn hash functions have
been proposed and shown to be a lot more effective
than LSH on both synthetic and real data. Semantic
Hashing employs Restricted Boltzmann Machine to
produce more compact codes than LSH (Salakhutdi-
nov and Hinton, 2009). Spectral Hashing formalizes
the requirements for a good code and relates them to
the problem of balanced graph partitioning which is
known to be NP hard (Weiss et al, 2008). To give
a practical algorithm for hashing, Spectral Hashing
assumes that the data are sampled from a multidi-
mensional uniform distribution and solves a relaxed
partitioning problem.
7 Conclusions
We developed two hashing-based techniques for
spelling correction of person names in People
Search applications.To the best of our knowledge,
these are the first techniques that focus exclusively
on correcting spelling mistakes in person names.
Our approach has several advantages over other
spelling correction techniques. Firstly, we do not
suggest incorrect suggestions for valid queries un-
like (Cucerzan and Brill, 2004). Further, as we sug-
gest spellings from only authoritative name direc-
tories, the suggestions are always well formed and
coherent. Secondly, we do not require query logs
and other resources that are not easily available un-
like (Cucerzan and Brill, 2004), (Ahmad and Kon-
drak, 2005). Neither do we require pairs of mis-
spelled names and their correct spellings for learn-
ing the error model unlike (Brill and Moore, 2000)
or large-coverage general purpose lexicon for unlike
(Cucerzan and Brill, 2004) or pronunciation dictio-
naries unlike (Toutanova and Moore, 2002). Thirdly,
we correct the query as a whole unlike (Ahmad and
Kondrak, 2005) and can handle word order changes
unlike (Cucerzan and Brill, 2004). Fourthly, we
do not iteratively process misspelled name unlike
(Cucerzan and Brill, 2004). Fifthly, we handle large
name directories efficiently unlike the spectrum of
name matching techniques discussed in (Pfeifer et
al., 1996). Finally, our training data requirement is
relatively small.
As future work, we would like to explore the pos-
sibility of learning hash functions using 1) bilingual
1264
and monolingual data together and 2) multiple con-
jugate languages.
References
Farooq Ahmad and Grzegorz Kondrak. 2005. Learn-
ing a spelling error model from search query logs. In
HLT ?05: Proceedings of the conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing, pages 955?962, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Alexandr Andoni and Piotr Indyk. 2006. Near-optimal
hashing algorithms for approximate nearest neighbor
in high dimensions. In FOCS, pages 459?468.
Rahul Bhagat and Eduard H. Hovy. 2007. Phonetic mod-
els for generating spelling variants. In IJCAI, pages
1570?1575.
Mikhail Bilenko, Raymond J. Mooney, William W. Co-
hen, Pradeep D. Ravikumar, and Stephen E. Fienberg.
2003. Adaptive name matching in information inte-
gration. IEEE Intelligent Systems, 18(5):16?23.
E. Brill and R. Moore. 2000. An improved error model
for noisy channel spelling correction. In Proceedings
of ACL ?00, pages 286?293.
Moses Charikar. 2002. Similarity estimation techniques
from rounding algorithms. In STOC, pages 380?388.
Qing Chen, Mu Li, and Ming Zhou. 2007. Improving
query spelling correction using web search results. In
EMNLP-CoNLL, pages 181?189.
P. Christen. 2006. A comparison of personal name
matching: techniques and practical issues. Techni-
cal Report TR-CS-06-02, Dept. of Computer Science,
ANU, Canberra.
William W. Cohen, Pradeep D. Ravikumar, and
Stephen E. Fienberg. 2003. A comparison of string
distance metrics for name-matching tasks. In IIWeb,
pages 73?78.
S Cucerzan and E. Brill. 2004. Spelling correction as an
iterative process that exploits the collective knowledge
of web users. In Proceedings of EMNLP ?04, pages
293?300.
F.J. Damerau. 1964. A technique for computer detection
and correction of spelling errors. Communications of
ACM, 7(3):171?176.
C. Friedman and R. Sideli. 1992. Tolerating spelling
errors during patient validation. Computers and
Biomedical Research, 25:486?509.
Andrew R. Golding and Dan Roth. 1996. Applying win-
now to context-sensitive spelling correction. CoRR,
cmp-lg/9607024.
Gene H. Golub and Charles F. Van Loan. 1996. Matrix
Computations. Johns Hopkins University Press, Balti-
more, MD, 3rd edition.
David R. Hardoon, Sa?ndor Szedma?k, and John Shawe-
Taylor. 2004. Canonical correlation analysis: An
overview with application to learning methods. Neu-
ral Computation, 16(12):2639?2664.
Aminul Islam and Diana Inkpen. 2009. Real-word
spelling correction using google web 1tn-gram data
set. In CIKM, pages 1689?1692.
D. Jurafsky and J.H. Martin. 2008. Speech and Lan-
guage Processing. Prentice-Hall.
Mark D. Kernighan, Kenneth W. Church, and William A.
Gale. 1990. A spelling correction program based on a
noisy channel model. In COLING, pages 205?210.
K. Kukich. 1996. Techniques for automatically correct-
ing words in a text. Computing Surveys, 24(4):377?
439.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
G. Navarro, R. Baeza-Yates, and J. Azevedo-Arcoverde.
2003. Matchsimile: a flexible approximate matching
tool for searching proper names. Journal of the Amer-
ican Society for Information Science and Technology,
54(1):3?15.
U. Pfeifer, T. Poersch, and N. Fuhr. 1996. Retrieval ef-
fectiveness of proper name search methods. Informa-
tion Processing and Management, 32(6):667?679.
L. Philips. 2000. The double metaphone search algo-
rithm. C/C++ Users Journal.
Eric Sven Ristad and Peter N. Yianilos. 1996. Learning
string edit distance. CoRR, cmp-lg/9610005.
Ruslan Salakhutdinov and Geoffrey E. Hinton. 2009. Se-
mantic hashing. Int. J. Approx. Reasoning, 50(7):969?
978.
Gregory Shakhnarovich, Trevor Darrell, and Piotr In-
dyk. 2008. Nearest-neighbor methods in learning
and vision. IEEE Transactions on Neural Networks,
19(2):377?377.
Xu Sun, Jianfeng Gao, Daniel Micol, and Chris Quirk.
2010. Learning phrase-based spelling error models
from clickthrough data. In Proceedings of ACL 2010.
K. Toutanova and R. Moore. 2002. Pronounciation mod-
eling for improved spelling correction. In Proceedings
of ACL ?02, pages 141?151.
Yair Weiss, Antonio B. Torralba, and Robert Fergus.
2008. Spectral hashing. In NIPS, pages 1753?1760.
Casey Whitelaw, Ben Hutchinson, Grace Chung, and Ged
Ellis. 2009. Using the web for language independent
spellchecking and autocorrection. In EMNLP, pages
890?899.
1265
