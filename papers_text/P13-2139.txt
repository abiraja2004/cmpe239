Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 797?803,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Recognizing Identical Events with Graph Kernels
Goran Glavas? and Jan S?najder
University of Zagreb
Faculty of Electrical Engineering and Computing
Text Analysis and Knowledge Engineering Lab
Unska 3, 10000 Zagreb, Croatia
{goran.glavas,jan.snajder}@fer.hr
Abstract
Identifying news stories that discuss the
same real-world events is important for
news tracking and retrieval. Most exist-
ing approaches rely on the traditional vec-
tor space model. We propose an approach
for recognizing identical real-world events
based on a structured, event-oriented doc-
ument representation. We structure docu-
ments as graphs of event mentions and use
graph kernels to measure the similarity be-
tween document pairs. Our experiments
indicate that the proposed graph-based ap-
proach can outperform the traditional vec-
tor space model, and is especially suitable
for distinguishing between topically simi-
lar, yet non-identical events.
1 Introduction
News stories typically describe real-world events.
Topic detection and tracking (TDT) aims to de-
tect stories that discuss identical or directly related
events, and track these stories as they evolve over
time (Allan, 2002). Being able to identify the sto-
ries that describe the same real-world event is es-
sential for TDT, and event-based information re-
trieval in general.
In TDT, an event is defined as something hap-
pening in a certain place at a certain time (Yang
et al, 1999), while a topic is defined as a set of
news stories related by some seminal real-world
event (Allan, 2002). To identify news stories on
the same topic, most TDT approaches rely on tra-
ditional vector space models (Salton et al, 1975),
as more sophisticated natural language processing
techniques have not yet proven to be useful for
this task. On the other hand, significant advances
in sentence-level event extraction have been made
over the last decade, in particular as the result of
standardization efforts such as TimeML (Puste-
jovsky et al, 2003a) and TimeBank (Pustejovsky
et al, 2003b), as well as dedicated evaluation tasks
(ACE, 2005; Verhagen et al, 2007; Verhagen et
al., 2010). However, these two lines of research
have largely remained isolated from one another.
In this paper we bridge this gap and address
the task of recognizing stories discussing identical
events by considering structured representations
from sentence-level events. More concretely, we
structure news stories into event graphs built from
individual event mentions extracted from text. To
measure event-based similarity of news stories, we
compare their event graphs using graph kernels
(Borgwardt, 2007). We conduct preliminary ex-
periments on two event-oriented tasks and show
that the proposed approach can outperform tradi-
tional vector space model in recognizing identical
real-world events. Moreover, we demonstrate that
our approach is especially suitable for distinguish-
ing between topically similar, yet non-identical
real-world events.
2 Related Work
The traditional vector space model (VSM) (Salton
et al, 1975) computes the cosine between bag-of-
words representations of documents. The VSM is
at the core of most approaches that identify same-
topic news stories (Hatzivassiloglou et al, 2000;
Brants et al, 2003; Kumaran and Allan, 2005;
Atkinson and Van der Goot, 2009). However, it
has been observed that some word classes (e.g.,
named entities, noun phrases, collocations) have
more significance than the others. Among them,
named entities have been considered as particu-
larly important, as they often identify the partici-
pants of an event. In view of this, Hatzivassiloglou
et al (2000) restrict the set of words to be used
for document representation to words constituting
noun phrases and named entities. Makkonen et
797
al. (2004) divide document terms into four seman-
tic categories (locations, temporal expressions,
proper names, and general terms) and construct
separate vector for each of them. Kumaran and
Allan (2004) represent news stories with three dif-
ferent vectors, modeling all words, named-entity
words, and all non-named-entity words occurring
in documents. When available, recognition of
identical events can rely on meta-information as-
sociated with news stories, such as document cre-
ation time (DCT). Atkinson and Van der Goot
(2009) combine DCT with VSM, assuming that
temporally distant news stories are unlikely to de-
scribe the same event.
In research on event extraction, the task of rec-
ognizing identical events is known as event coref-
erence resolution (Bejan and Harabagiu, 2010;
Lee et al, 2012). There, however, the aim is to
identify sentence-level event mentions referring to
the same real-world events, and not stories that
discuss identical events.
3 Kernels on Event Graphs
To identify the news describing the same real-
world event, we (1) structure event-oriented in-
formation from text into event graphs and (2) use
graph kernels to measure the similarity between a
pair of event graphs.
3.1 Event graphs
An event graph is a vertex- and edge-labeled
mixed graph in which vertices represent individ-
ual event mentions and edges represent temporal
relations between event mentions. We adopt a
generic representation of event mentions, as pro-
posed by Glavas? and S?najder (2013): each men-
tion consists of an anchor (a word that conveys
the core meaning) and four types of arguments
(agent, target, time, location). Furthermore, we
consider four types of temporal relations between
event mentions: before, after, overlap, and equal
(Allen, 1983). As relations overlap and equal are
symmetric, whereas before and after are not, an
event graph may contain both directed and undi-
rected edges.
Formally, an event graph G is represented as a
tuple G = (V,E,A,m, r), where V is the set of
vertices, E is the set of undirected edges, A is the
set of directed edges (arcs), m : V ? M is a
bijection mapping the vertices to event mentions,
and r : E ? R is the edge-labeling function, as-
signing temporal relations to edges (cf. Fig. 1).
The construction of an event graph from a news
story involves the extraction of event mentions
(anchors and arguments) and the extraction of
temporal relations between mentions. We use a
supervised model (with 80% F1 extraction perfor-
mance) based on a rich set of features similar to
those proposed by Bethard (2008) to extract event
anchors. We then employ a robust, rule-based ap-
proach proposed by Glavas? and S?najder (2013) to
extract generic event arguments. Finally, we em-
ploy a supervised model (60% micro-averaged F1
classification performance) with a rich set of fea-
tures, similar to those proposed by Bethard (2008),
to extract temporal relations between event men-
tions. A detailed description of the graph con-
struction steps is outside the scope of this paper.
To compute event graph kernels (cf. Section
3.2), we need to determine whether two event
mentions co-refer. To resolve cross-document
event coreference, we use the model proposed
by Glavas? and S?najder (2013). The model de-
termines coreference by comparing factual event
anchors and arguments of four coarse-grained se-
mantic types (agent, target, location, and time),
and achieves an F-score of 67% (79% precision
and 57% recall) on the cross-document mention
pairs from the EventCorefBank dataset (Bejan and
Harabagiu, 2008). In what follows, cf (m1,m2)
denotes whether event mentions m1 and m2 co-
refer (equals 1 if mentions co-refer, 0 otherwise).
3.2 Graph kernels
Graph kernels are fast polynomial alternatives
to traditional graph comparison techniques (e.g.,
subgraph isomorphism), which provide an expres-
sive measure of similarity between graphs (Borg-
wardt, 2007). We employ two different graph ker-
nels: product graph kernel and weighted decom-
position kernel. We chose these kernels because
their general forms have intuitive interpretations
for event matching. These particular kernels have
shown to perform well on a number of tasks from
chemoinformatics (Mahe? et al, 2005; Menchetti
et al, 2005).
Product graph kernel. A product graph kernel
(PGK) counts the common walks between two in-
put graphs (Ga?rtner et al, 2003). The graph prod-
uct of two labeled graphs, G and G? , denoted
GP = G?G?, is a graph with the vertex set
VP =
{
(v, v?) | v ? VG, v? ? VG? , ?(v, v?)
}
798
where ?(v, v?) is a predicate that holds when
vertices v and v? are identically labeled (Ham-
mack et al, 2011). Given event graphs G =
(V,E,A,m, r) and G? = (V ?, E?, A?,m?, r?), we
consider the vertices to be identically labeled if
the corresponding event mentions co-refer, i.e.,
?(v, v?) .= cf (m(v),m?(v?)). The edge set of the
graph product depends on the type of the product.
We experiment with two different products: ten-
sor product and conormal product. In the tensor
product, an edge is introduced iff the correspond-
ing edges exist in both input graphs and the labels
of those edges match (i.e., both edges represent the
same temporal relation). In the conormal product,
an edge is introduced iff the corresponding edge
exists in at least one input graph. Thus, a conor-
mal product may compensate for omitted temporal
relations in the input graphs.
Let AP be the adjacency matrix of the graph
productGP built from input graphsG andG?. The
product graph kernel that counts common walks in
G and G? can be computed efficiently as:
KPG(G,G?) =
|VP |?
i,j=1
[(I ? ?AP )?1]ij (1)
when ? < 1/t , where t is the maximum degree of
a vertex in the graph product GP . In our experi-
ments, we set ? to 1/(t+ 1) .
Weighted decomposition kernel. A weighted
decomposition kernel (WDK) compares small
graph parts, called selectors, being matched ac-
cording to an equality predicate. The importance
of the match is weighted by the similarity of the
contexts in which the matched selectors occur.
For a description of a general form of WDK, see
Menchetti et al (2005).
Let S(G) be the set of all pairs (s, z), where s is
the selector (subgraph of interest) and z is the con-
text of s. We decompose event graphs into individ-
ual vertices, i.e., we define selectors to be the indi-
vidual vertices. In this case, similarly as above, the
equality predicate ?(v, v?) for two vertices v ? G
and v? ? G? holds if and only if the correspond-
ing event mentions m(v) and m?(v?) co-refer. Us-
ing selectors that consist of more than one vertex
would require a more complex and perhaps a less
intuitive definition of the equality predicate ?. The
selector context Zv of vertex v is a subgraph of G
that contains v and all its immediate neighbors. In
other words, we consider as context all event men-
tions that are in a direct temporal relation with the
selected mention. WDK between event graphs G
and G? is computed as:
KWD(G,G?) =
?
v?VG,v??VG?
cf (m(v),m?(v?)) ?(Zv, Z ?v?)
(2)
where ?(Zv, Z ?v?) is the context kernel measuring
the similarity between the context Zv of selector
v ? G and the context Z ?v? of selector v? ? G?.
We compute the context kernel ? as the number of
coreferent mention pairs found between the con-
texts, normalized by the context size:
?(Zv, Z ?v?) =
?
w?VZv ,w??VZ?v?
cf(m(w),m?(w?))
max(|VZv |, |VZ?v? |)
The intuition behind this is that a pair of corefer-
ent mentions m(v) and m?(v?) should contribute
to the overall event similarity according to the
number of pairs of coreferent mentions,m(w) and
m?(w?), that are in temporal relation with v and v?,
respectively.
Graph kernels example. As an example, con-
sider the following two story snippets describing
the same sets of real-world events:
Story 1: A Cezanne masterpiece worth at least $131
million that was the yanked from the wall of a Zurich
art gallery in 2008 has been recovered, Serbian po-
lice said today. Four arrests were made overnight
in connection with the theft, which was one of the
biggest art heists in recent history.
Story 2: Serbian police have recovered a painting
by French impressionist Paul Cezanne worth an esti-
mated 100 million euros (131.7 million U.S. dollars),
media reported on Thursday. The painting ?A boy in
a red vest? was stolen in 2008 from a Zurich museum
by masked perpetrators. Four members of an interna-
tional crime ring were arrested Wednesday.
The corresponding event graphs G and G? are
shown in Fig. 1a and 1b, respectively, while their
product is shown in Fig. 1c. There are three pairs
of coreferent event mentions between G and G?:
(yanked, stolen), (recovered, recovered), and (ar-
rests, arrested). Accordingly, the product graph
P has three nodes. The dashed edge between ver-
tices (yanked, stolen) and (arrests, arrested) exists
only in the conormal product graph. By substi-
tuting into (1) the adjacency matrix and maximum
vertex degree of tensor product graph P , we obtain
799
(a) Event graph G (Story 1) (b) Event graph G? (Story 2) (c) Product graph P
Figure 1: Example event graphs and their product
the tensor PGK score as:
KPG =
3?
i,j=1
?
?
(
I ? 13
(
0 0 1
0 0 1
1 1 0
))?1?
?
i,j
? 5.6
Similarly, for the conormal product graph P we
obtain the conormal PGK score of KPG = 9. By
substitutingG andG? into (2), we obtain the WDK
score as:
KWD =
?
(v,v?)?VP
?(Zv, Z ?v?) =
2
3 +
3
4 +
2
4 ? 1.9
where VP contains pairs of coreferent event men-
tions: (yanked, stolen), (recovered, recovered),
and (arrests, arrested).
4 Experiments
We conducted two preliminary experiments to in-
vestigate whether kernels on event graphs can be
used to recognize identical events.
4.1 Task 1: Recognizing identical events
Dataset. In the first experiment, we classify
pairs of news stories as either describing identical
real-world events or not. For this we need a collec-
tion of stories in which pairs of stories on identi-
cal events have been annotated as such. TDT cor-
pora (Wayne, 2000) is not directly usable because
it has no such annotations. We therefore decided
to build a small annotated dataset.1 To this end,
we use the news clusters of the EMM NewsBrief
service (Steinberger et al, 2009). EMM clusters
news stories from different sources using a docu-
ment similarity score. We acquired 10 randomly
chosen news clusters, manually inspected each of
them, and retained in each cluster only the doc-
uments that describe the same real-world events.
Additionally, we ensured that no documents from
1Datasets for both experiments are available at:
http://takelab.fer.hr/evkernels
Model P R F
Tensor PGK 89.7 82.3 85.8
Conormal PGK 89.3 77.8 83.2
WDK 88.6 73.7 80.5
SVM Graph 91.1 87.6 89.3
SVM Graph + VSM 93.8 96.2 95.0
VSM baseline 90.9 82.9 86.7
Table 1: Results for recognition of identical events
different clusters discuss the same event. To ob-
tain the gold standard dataset, we build all pairs
of documents. The final dataset consists of 64
documents in 10 clusters, with 195 news pairs
from the same clusters (positive pairs) and 1821
news pairs from different clusters (negative pairs).
We divide the dataset into a train and a test set
(7:3 split ratio). Note that, although our dataset
has ground-truth annotations, it is incomplete in
the sense that some pairs of documents describ-
ing the same events, which were not recognized
as such by the EMM, are not included. Further-
more, because EMM similarity score uses VSM
cosine similarity as one of the features, VSM co-
sine similarity constitutes a competitive baseline
on this dataset.
Results. For each graph kernel and the VSM
baseline, we determine the optimal threshold on
the train set and evaluate the classification per-
formance on the test set. The results are given
in Table 1. The precision is consistently higher
than recall for all kernels and the baseline. High
precision is expected, as clusters represent topi-
cally dissimilar events. PGK models (both ten-
sor and conormal) outperform the WDK model,
indicating that common walks correlate better to
event-based document similarity than common
subgraphs. Individually, none of the graph kernels
outperforms the baseline. To investigate whether
the two kernels complement each other, we fed the
800
Original
?Taliban militants have attacked a prison in north-west
Pakistan, freeing at least 380 prisoners. . . ?
Event-preserving paraphrase
?Taliban militants in northwest Pakistan attacked the
prison, liberated at least 380 prisoners . . . ?
Event-shifting paraphrase
?Taliban militants have been arrested in north-west Pak-
istan. At least 380 militants have been arrested. . . ?
Table 2: Event paraphrasing example
individual kernel scores to an SVM model (with
RBF kernel), along with additional graph-based
features such as the number of nodes and the num-
ber of edges (SVM graph model). Finally, we com-
bined the graph-based features with the VSM co-
sine similarity (SVM graph + VSM model). SVM
graph model significantly (at p < 0.05, student?s
2-tailed t-test) outperforms the individual kernel
models and the baseline. The combined model
(SVM graph + VSM) significantly (at p < 0.01)
outperforms the baseline and all kernel models.
4.2 Task 2: Event-based similarity ranking
Dataset. In the second experiment we focus
on the task of distinguishing between news sto-
ries that describe topically very similar, yet dis-
tinct events. For this purpose, we use a small
set of event paraphrases, constructed as fol-
lows. We manually selected 10 news stories from
EMM NewsBrief and altered each of them to
obtain two meaning-preserving (event-preserving)
and two meaning-changing (event-shifting) para-
phrases. To obtain the meaning-preserving para-
phrases, we use Google translate and round-trip
translation via two pairs of arbitrarily chosen lan-
guages (Danish/Finnish and Croatian/Hungarian).
Annotators manually corrected lexical and syn-
tactic errors introduced by the round-trip transla-
tion. To obtain meaning-changing paraphrases, we
asked human annotators to alter each story so that
it topically resembles the original, but describes a
different real-world event. The extent of the al-
teration was left to the annotators, i.e., no specific
transformations were proposed. Paraphrase exam-
ples are given in Table 2. The final dataset consists
of 60 news pairs: 30 positive and 30 negative.
Results. For each method we ranked the pairs
based on the assigned similarity scores. An ideal
method would rank all positive pairs above all neg-
ative pairs. We evaluated the performance using
Model R-prec. Avg. prec.
Tensor PGK 86.7 96.8
Conormal PGK 93.3 97.5
WDK 86.7 95.7
VSM baseline 80.0 77.1
Table 3: Results for event-based similarity ranking
two different rank evaluation metrics: R-precision
(precision at rank 30, as there are 30 positive pairs)
and average precision. The performance of graph
kernel models and the VSM baseline is given in
Table 3. We tested the significance of differences
using stratified shuffling (Yeh, 2000). When con-
sidering average precision, all kernel models sig-
nificantly (at p < 0.01) outperform the baseline.
However, when considering R-precision, only the
conormal PGK model significantly (at p < 0.05)
outperforms the baseline. There is no statistical
significance in performance differences between
the considered kernel methods. Inspection of the
rankings reveals that graph kernels assign very low
scores to negative pairs, i.e., they distinguish well
between textual representations of topically simi-
lar, but different real-world events.
5 Conclusion
We proposed a novel approach for recognizing
identical events that relies on structured, graph-
based representations of events described in a
document. We use graph kernels as an expres-
sive framework for modeling the similarity be-
tween structured events. Preliminary results on
two event-similarity tasks are encouraging, indi-
cating that our approach can outperform tradi-
tional vector-space model, and is suitable for dis-
tinguishing between topically very similar events.
Further improvements could be obtained by in-
creasing the accuracy of event coreference resolu-
tion, which has a direct influence on graph kernels.
The research opens up many interesting direc-
tions for further research. Besides a systematic
evaluation on larger datasets, we intend to inves-
tigate the applications in event tracking and event-
oriented information retrieval.
Acknowledgments
This work has been supported by the Ministry of
Science, Education and Sports, Republic of Croa-
tia under the Grant 036-1300646-1986. We thank
the reviewers for their constructive comments.
801
References
ACE. 2005. Evaluation of the detection and recogni-
tion of ACE: Entities, values, temporal expressions,
relations, and events.
James Allan. 2002. Topic Detection and Tracking:
Event-based Information Organization, volume 12.
Kluwer Academic Pub.
James Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Martin Atkinson and Erik Van der Goot. 2009. Near
real time information mining in multilingual news.
In Proceedings of the 18th International Conference
on World Wide Web, pages 1153?1154. ACM.
Cosmin Adrian Bejan and Sanda Harabagiu. 2008. A
linguistic resource for discovering event structures
and resolving event coreference. In Proceedings of
the 6th International Conference on Language Re-
sources and Evaluation (LREC 2008).
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.
Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 1412?1422. Association for Com-
putational Linguistics.
Steven Bethard. 2008. Finding Event, Temporal and
Causal Structure in Text: A Machine Learning Ap-
proach. Ph.D. thesis, University of Colorado at
Boulder.
Karsten Michael Borgwardt. 2007. Graph Ker-
nels. Ph.D. thesis, Ludwig-Maximilians-Universita?t
Mu?nchen.
Thorsten Brants, Francine Chen, and Ayman Farahat.
2003. A system for new event detection. In Pro-
ceedings of the 26th Annual International ACM SI-
GIR Conference on Research and Development in
Information Retrieval, pages 330?337. ACM.
Thomas Ga?rtner, Peter Flach, and Stefan Wrobel.
2003. On graph kernels: Hardness results and ef-
ficient alternatives. In Learning Theory and Kernel
Machines, pages 129?143. Springer.
Goran Glavas? and Jan S?najder. 2013. Exploring coref-
erence uncertainty of generically extracted event
mentions. In Proceedings of 14th International
Conference on Intelligent Text Processing and Com-
putational Linguistics, pages 408?422. Springer.
Richard Hammack, Wilfried Imrich, and Sandi
Klavz?ar. 2011. Handbook of Product Graphs. Dis-
crete Mathematics and Its Applications. CRC Press.
Vasileios Hatzivassiloglou, Luis Gravano, and Anki-
needu Maganti. 2000. An investigation of linguistic
features and clustering algorithms for topical doc-
ument clustering. In Proceedings of the 23rd An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
pages 224?231. ACM.
Giridhar Kumaran and James Allan. 2004. Text clas-
sification and named entities for new event detec-
tion. In Proceedings of the 27th Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 297?304.
ACM.
Giridhar Kumaran and James Allan. 2005. Using
names and topics for new event detection. In Pro-
ceedings of the Conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 121?128. Association for
Computational Linguistics.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
489?500. Association for Computational Linguis-
tics.
Pierre Mahe?, Nobuhisa Ueda, Tatsuya Akutsu, Jean-
Luc Perret, and Jean-Philippe Vert. 2005. Graph
kernels for molecular structure-activity relationship
analysis with support vector machines. Journal
of Chemical Information and Modeling, 45(4):939?
951.
Juha Makkonen, Helena Ahonen-Myka, and Marko
Salmenkivi. 2004. Simple semantics in topic detec-
tion and tracking. Information Retrieval, 7(3):347?
368.
Sauro Menchetti, Fabrizio Costa, and Paolo Frasconi.
2005. Weighted decomposition kernels. In Pro-
ceedings of the 22nd International Conference on
Machine Learning, pages 585?592. ACM.
James Pustejovsky, Jose? Castano, Robert Ingria, Roser
Sauri, Robert Gaizauskas, Andrea Setzer, Graham
Katz, and Dragomir Radev. 2003a. Timeml: Robust
specification of event and temporal expressions in
text. New Directions in Question Answering, 3:28?
34.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al 2003b. The TimeBank corpus. In Cor-
pus Linguistics, volume 2003, page 40.
Gerard Salton, Anita Wong, and Chung-Shu Yang.
1975. A vector space model for automatic indexing.
Communications of the ACM, 18(11):613?620.
802
Ralf Steinberger, Bruno Pouliquen, and Erik Van
Der Goot. 2009. An introduction to the euro-
pean media monitor family of applications. In Pro-
ceedings of the Information Access in a Multilin-
gual World-Proceedings of the SIGIR 2009 Work-
shop, pages 1?8.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 Task 15: TempEval tempo-
ral relation identification. In Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 75?80. Association for Computational Lin-
guistics.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 Task 13:
TempEval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 57?
62. Association for Computational Linguistics.
Charles Wayne. 2000. Multilingual topic detection
and tracking: Successful research enabled by cor-
pora and evaluation. In Proceedings of the Second
International Conference on Language Resources
and Evaluation Conference (LREC 2000), volume
2000, pages 1487?1494.
Yiming Yang, Jaime G Carbonell, Ralf D Brown,
Thomas Pierce, Brian T Archibald, and Xin Liu.
1999. Learning approaches for detecting and track-
ing news events. Intelligent Systems and their Ap-
plications, IEEE, 14(4):32?43.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Pro-
ceedings of the 18th Conference on Computational
linguistics, pages 947?953. Association for Compu-
tational Linguistics.
803
