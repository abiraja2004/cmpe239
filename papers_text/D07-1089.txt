Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 847?857, Prague, June 2007. c?2007 Association for Computational Linguistics
Multiple Alignment of Citation Sentences with
Conditional Random Fields and Posterior Decoding
Ariel S. Schwartz?
EECS, Computer Science Division
UC Berkeley
Berkeley, CA 94720-1776
sariel@cs.berkeley.edu
Anna Divoli, Marti A. Hearst
School of Information
UC Berkeley
Berkeley, CA 94720-4600
{hearst,divoli}@ischool.berkeley.edu
Abstract
In scientific literature, sentences that cite re-
lated work can be a valuable resource for
applications such as summarization, syn-
onym identification, and entity extraction.
In order to determine which equivalent en-
tities are discussed in the various citation
sentences, we propose aligning the words
within these sentences according to semantic
similarity. This problem is partly analogous
to the problem of multiple sequence align-
ment in the biosciences, and is also closely
related to the word alignment problem in sta-
tistical machine translation. In this paper
we address the problem of multiple citation
concept alignment by combining and mod-
ifying the CRF based pairwise word align-
ment system of Blunsom & Cohn (2006)
and a posterior decoding based multiple se-
quence alignment algorithm of Schwartz &
Pachter (2007). We evaluate the algorithm
on hand-labeled data, achieving results that
improve on a baseline.
1 Introduction
The scientific literature of biomedicine, genomics,
and other biosciences is a rich, complex, and con-
tinually growing resource. With appropriate infor-
mation extraction and retrieval tools, bioscience re-
searchers can use the contents of the literature to
further their research goals. With online full text
?Current address: Department of Bioengineering, Univer-
sity of California, San Diego, La Jolla, CA 92093-0412. Email:
sariel@ucsd.edu.
of journal articles finally becoming the norm, new
forms of citation analysis become possible.
Nearly every statement in biology articles is
backed up by at least one citation, and, conversely,
it is quite common for papers in the bioscience do-
main to be cited by 30?100 other papers. The cited
facts are typically stated in a more concise way in the
citing papers than in the original papers. Since the
same facts are repeatedly stated in different ways in
different papers, statistical models can be trained on
existing citation sentences to identify similar facts in
unseen text. Citation sentences also have the poten-
tial to be useful for text summarization and database
curation. Figure 1 shows an example of three differ-
ent citation sentences to the same target paper.
Most citation analysis work focuses on the cita-
tion network structure, to determine which papers
are most central, or uses co-citation analysis to de-
termine which papers are similar to one another in
content (White, 2004; Liu, 1993; Garfield, 1955;
Lipetz, 1965; Giles et al, 1998). In this paper we
focus instead on analyzing the sentences that sur-
round the citations to related work, which we termed
citances in Nakov et al (2004). In that paper we
note that one subproblem for using citances for au-
tomated analysis is to identify the different concepts
mentioned; a given paper may be cited for more than
one fact or relation.
Citances often state similar information using
varying words and phrases. In order to build con-
cise summaries, those entities and relations that are
expressed in different ways should be matched up,
or aligned, so that subsequent processing steps will
know what the core concepts are. In this paper we
847
Exam
ple o
f una
ligne
d cit
ance
s
?In r
espo
nse t
o ge
noto
xics
tress
, Ch
k1 a
nd C
hk2 
phos
phor
ylate
Cdc
25A
 on N
-term
inal 
sites
 and
 targ
et it 
rapid
ly fo
r ubi
quiti
n-de
pend
ent d
egra
datio
n (M
ailan
det 
al, 2
000,
 200
2; 
Mol
inari
et al
, 200
0 ; Fa
lcke
t al, 
2001
; Shi
muta
et al
, 200
2; B
usin
oet 
al, 2
003)
, wh
ich i
s 
thou
ght t
o be
 cen
tral t
o the
 S an
d G2
 cell
 cyc
le ch
eckp
oints
 (Ba
rtek
and 
Luk
as, 2
003;
 
Don
zelli
and 
Drae
tta, 2
003 )
.?
?Giv
en th
at C
hk1 
prom
otes
 Cdc
25A
 turn
over
 in r
espo
nse t
o DN
Ada
mag
e in 
vivo
 
(Fal
cke
t al. 
2001
; Sor
ense
n et 
al. 2
003)
 and
that 
Chk
1 is 
requ
ired 
for C
dc25
A 
ubiq
uitin
ation
by S
CF?-T
RCP
 in v
itro,
 we 
expl
ored
 the 
role 
of C
dc25
A 
phos
phor
ylati
onin
the u
biqu
itina
tion
proc
ess.?
?Sin
ce ac
tivat
ed p
hosp
hory
lated
Chk
2-T68
is in
volv
ed in
 pho
spho
rylat
iona
nd 
degr
adat
ion o
f Cd
c25A
 (Fal
cke
t al.,
 200
1 , Fa
lcke
t al.,
2002
; Ba
rtek
and 
Luk
as, 
2003
), we
 also
 exa
mine
d the
 leve
ls of
Cdc
25A
 in 2
fTG
H an
d U3
A ce
lls e
xpos
ed to
 
?-IR.?
Figure 1: Example of three unaligned citances.
Alig
nme
nt af
ter n
orm
aliza
tion
resp
onse
geno
toxi
cstr
essC
hk1
 Chk
2ph
osph
oryl
ateC
dc25
AN
 term
inal
 site
s tar
get 
rapi
dly u
biqu
itin
depe
nden
tdeg
rada
tion
thou
ght 
cent
ral S
 G2 
cell 
cycl
e ch
eckp
oint
s
Give
n Ch
k1p
rom
otes
 Cdc
25A
turn
over
resp
onse
DNA
 dam
age
vivo
 Chk
1re
quir
ed
Cdc
25A
ubiq
uitin
atio
nSC
F be
ta T
RCP
 vitr
o ex
plor
edr
ole C
dc25
Aph
osph
oryl
atio
n
ubiq
uitin
atio
npr
oces
s
activ
ated
 pho
spho
ryla
tedC
hk2
T68
invo
lved
 pho
spho
ryla
tion
degr
adat
ionC
dc25
A
exam
ined
leve
ls C
dc25
A2f
TGH
 U3A
 cell
s exp
osed
 gam
ma I
R
Figure 2: Example of three normalized aligned ci-
tances. Homologous entities are colored the same.
Unaligned entities are black.
build on the work of Nakov et al (2004) by tackling
the entity normalization step.
The citance alignment problem is partially anal-
ogous to the problem of multiple alignment of bi-
ological sequences (Durbin et al, 1998). In both
cases the goal is to align homologous entities that
are derived from the same ancestral entity. While in
biology homology is well-defined in the molecular
level, in the citances case it is defined in the seman-
tic level, which is much more subjective. Given a
group of citances that cite the same target paper, we
loosely define semantic homology as a symmetric,
transitive, and reflexive relation between two enti-
ties (words or phrases) in the same or different ci-
tance that have similar semantics in the context of
the cited paper.
Figure 1 shows an example of three citances that
cite the same target paper (Falck et al, 2001). A
multiple alignment of the entities in the same ci-
tances (after removal of stopwords) is shown in Fig-
ure 2. Homologous entities are colored the same.
This small example illustrates some of the main
challenges of multiple citance alignment (MCA).
While orthographic similarity can help to identify
semantic homology (e.g., phosphorylate and phos-
phorylation), it can also be misleading (e.g., cell cy-
cle and U3A cells). In addition, semantic homology
might not include any orthographic clues (e.g., geno-
toxic stress and DNA damage).
Unlike global multiple sequence alignment
(MSA) in genomics, where each character can be
aligned to at most one character in every other se-
quence, in multiple citance alignment, each word
can be aligned to any number of words in other sen-
tences. Another major difference between the two
problems is the fact that while the sequential order-
ing of characters must be maintained in multiple se-
quence alignment, this is not the case for multiple
citance alignment.
MCA is also related to the problem of word align-
ment in statistical machine translation (SMT) (Och
and Ney, 2003). However, unlike SMT alignment,
MCA aligns multiple citances in the same language
rather than a pair of sentences in different languages.
In this paper we present an MCA algorithm that
is based on an extension to the posterior decoding
algorithm for MSA called AMAP (Schwartz et al,
2006; Schwartz and Pachter, 2007), with an under-
lying pairwise alignment model based on the CRF
SMT alignment of Blunsom & Cohn (2006).
2 Multiple citance alignments
Let G , {C1, C2, . . . , CK} be a group of K ci-
tances that cite the same target paper, where the ith
citance is a sequence of words Ci , Ci1C
i
2 ? ? ?C
i
ni ,
and ci , {ci1, c
i
2, . . . , c
i
ni} is the set of word indices
of Ci. A pairwise citance alignment of Ci and Cj
is an equivalence (symmetric, reflexive, and transi-
tive) relation ?ij on the set ci ? cj . The expres-
sion cik ?ij c
j
l means that according to the pairwise
alignment?ij word k in citance Ci and word l in ci-
tance Cj are aligned. A multiple citance alignment
(MCA) is an equivalence relation ?,
(?
ij ?ij
)+
on the set
?
i c
i, which is the transitive closure of
the union of all pairwise alignments of citance pairs
in G. Taking the transitive closure and not only
the union of all pairwise alignments ensures that the
MCA is an equivalence relation.
An MCA ? defines a partition of the set of all
word indices c ,
?
ik {c
i
k}, which is of size n ,
848
|c| =
?
i n
i. Therefore, the number of distinct
MCAs of G is the number of partitions of a set of
size n. This number is called the nth Bell number
(Rota, 1964)
Bn ,
1
e
??
k=0
kn
k!
. (1)
Asymptotically,Bn grows faster than an exponential
but slower than a factorial. For example B100 ?
10116. Obviously, enumerating all possible MCAs
is impractical even for small problems.
3 Probabilistic model for MCA
Unlike biological sequences, for which pair-HMMs
are a natural choice for modeling evolutionary pro-
cesses between two sequences, there is no simple
generative model that can be used for modeling
pairwise citance alignment. Most of the work on
pairwise alignment of sentences at the word level
has been done in the statistical machine translation
(SMT) community.
Och & Ney (2003) present an overview and com-
parison of the most common models used for SMT
word alignments. Out of the models they describe,
the HMM models are the most expressive mod-
els that can compute posterior probabilities using
the forward-backward algorithm. However, unlike
sequence alignments, there are no ordering con-
straints in word alignments, and the alignments are
many-to-many as opposed to one-to-one. Therefore,
the SMT HMM models cannot be based on pair-
HMMs, which generate two sentences simultane-
ously. Rather, they are directional models that model
the probability of generating a target sentence given
a source sentence. In other words they only model
many-to-one alignments, recovering the many-to-
many alignments in a preprocessing step. Therefore,
SMT HMMs can only compute the posterior proba-
bilities P (cik ; c
j
l |C
i, Cj) and P (cjl ; c
i
k|C
i, Cj),
where the relation ; represents the (directional)
event that a source word is translated into a target
word. Nevertheless, recently such posterior proba-
bilities have been used in SMT word alignment sys-
tem as an alternative to Viterbi decoding, and helped
to improve the performance of such systems (Ma-
tusov et al, 2004; Liang et al, 2006).
Generative models like HMMs have several lim-
itations. First, they require relatively large train-
ing data, which is difficult to attain in case of SMT
word alignment, and even more so in the case of
MCA. Second, generative models explicitly model
the inter-dependence of different features, which re-
duces the ability to incorporate multiple arbitrary
features into the model. Since orthographic similar-
ity is not a strong enough indication for semantic ho-
mology in MCA, we would like to be able to incor-
porate multiple inter-dependent features into a single
model, including orthographic, contextual, ontolog-
ical, and lexical features.
Recently, several authors have described dis-
criminative SMT alignment models (Moore, 2005;
Lacoste-Julien et al, 2006; Blunsom and Cohn,
2006). However, to the best of our knowledge only
the model of Blunsom & Cohn (2006), which is
based on a Conditional Random Field (CRF) (Laf-
ferty et al, 2001), can compute word indices pairs?
directional posterior probabilities, like those com-
puted by the HMM models. Therefore, we decided
to adopt the CRF-based model to the MCA problem.
3.1 Conditional random fields for word
alignment
The model of Blunsom & Cohn (2006) is based on
a linear chain CRF, which can be viewed as the
undirected version of an HMM. The CRF models
a many-to-one pairwise alignment, in which every
source word can get algned to zero or one target
words, but every word in the target sentence can be
the target of multiple source words. CRFs define
a conditional distribution over a latent labeling se-
quence given observation sequence(s). In the case
of CRF for word alignment, the observed sequences
are the source and target sentences (citances), and
the latent labeling sequence is the mapping of source
words to target word-indices. Given a source citance
Ci of length ni, and a target citance Cj of length nj ,
the many-to-one alignment of Ci to Cj is the rela-
tion ;. Since this is a many-to-one alignment, ;
can be represented by a vector a of length ni. The
CRF models the probability of the alignment a con-
ditioned on Ci and Cj as follows:
P?(a|C
i, Cj) =
exp
(?
t
?
k ?kfk(t, at?1, at, C
i, Cj)
)
Z?(Ci, Cj)
, (2)
849
where f , {fk} are the model?s features, ? , {?k}
are the features? weights, and Z?(Ci, Cj) is the par-
tition (normalization) function which is defined as:
Z?(C
i, Cj) ,
?
a
exp
(
?
t
?
k
?kfk(t, at?1, at, C
i, Cj)
)
.
(3)
Parameters are estimated from fully observed data
(manually aligned citances) using a maximum a pos-
teriori estimate. The parameter estimation proce-
dure is described in more details in the original pa-
per. Blunsom & Cohn (2006) use Viterbi decoding
to find an alignment of two sentences given a trained
CRF model, a? , argmaxa P?(a|C
i, Cj). How-
ever, the posterior probabilities of the labels at each
position can be calculated as well using the forward-
backward algorithm:
P?(c
i
l ; c
j
k|C
i, Cj) = P?(al = c
j
k|C
i, Cj) =
?l(c
j
k|C
i, Cj)?l(c
j
k|C
i, Cj)
Z?(Ci, Cj)
(4)
where ?l and ?l are the forward and backward vec-
tors that are computed with the forward-backward
algorithm (Lafferty et al, 2001).
3.2 The posterior decoding algorithm for MCA
Ultimately, the success of an MCA algorithm should
be judged by its effect on the success of the citance
analysis systems that use MCAs as their input. How-
ever, measuring this effect directly is difficult, since
high-level tasks such as summarization are difficult
to evaluate objectively. More to the point, it is dif-
ficult to quantify the contribution of the MCA ac-
curacy to the accuracy of the high-level system that
uses it. A more practical alternative is to measure the
accuracy of MCAs directly using a meaningful ac-
curacy measure, under the simplifying assumption
that there is a strong correlation between the mea-
sured MCA accuracy and the performance of the
high-level application.
We argue that a useful utility function should be
correlated (or even identical) to the accuracy mea-
sure used to evaluate the performance of an algo-
rithm. In addition, the utility function should be
easily decomposable, to enable direct optimization
using posterior-decoding. Although any accuracy
measure that is acceptable as a single performance
measure can be used to guide the design of the util-
ity function, metric-based accuracy measures have
several noticeable advantages. First, a metric for-
malizes the intuitive notion of distance. Hence, an
accuracy measure which is based on a metric fol-
lows the intuition that reducing the distance to the
correct answer should increase the accuracy of the
predicted answer. Therefore, defining a metric space
for the objects of a given problem leads to a nat-
ural definition of accuracy. Another advantage of
using a metric-based accuracy measure is the abil-
ity to provide bounds in the search space using the
triangle inequality. For example, while searching for
the answer with the optimal (metric-based) expected
utility, a step of length x can only change the ex-
pected utility as well as the actual utility by at most
?x units. Examples of more complex bounds us-
ing metric loss functions are described in (Schlu?ter
et al, 2005) and (Domingos, 2000).
Schwartz et al (2006) define the alignment met-
ric accuracy (AMA), which is a utility function for
one-to-one MSA. Intuitively, AMA measures the
fraction of characters that are aligned correctly ac-
cording to the reference alignment, either to another
character or to a gap (null). We extend the definition
of AMA to the case of many-to-many MCA.
A good utility function for MCA should give par-
tial credit to word positions that align to some of the
correct word positions while penalizing for aligning
to wrong word positions. To help define such a util-
ity function we define the following. Let mij?(c
j
l ) ,
{cik ? c
i|cik ? c
j
l } be the set of all word positions
in citance Ci that align to word position l in citance
Cj according to MCA ?. We can then define the
following utility function for the MCA ?p of the ci-
tance group G given a reference MCA ?r:
UAMA(?
r,?p) ,
?
ijl|i6=j Uset agreement
(
mij?r(c
j
l ),m
ij
?p(c
j
l )
)
n(K ? 1)
, (5)
where n is the number of word indices in G, K ,
|G| is the number of citances in the group, and
Uset agreement is any utility function for agreement
between sets that assigns values in the range [0, 1].
850
Uset agreement can be viewed as a ?score? assigned
to each word position based on the agreement be-
tween the two alignments with regards to the other
word positions that align to it. Using a 0?1 loss as
the set agreement score is equivalent to the original
AMA. Other utility functions, such as Dice, Jaccard
and Hamming can be used as Uset agreement. How-
ever, only metric-based utility functions will result
in a metric-based UAMA utility function. It is easy
to see that 1 ? UAMA satisfies all the requirements
of a metric, i.e., it is non-negative, equals zero if
and only if ?r=?p, symmetric, and obeys the trian-
gle inequality, since if the triangle inequality holds
for 1 ? Uset agreement, it must hold for a sum of
1 ? Uset agreement values. (We refer the reader to
Schwartz (2007) for a longer discussion of the prop-
erties of the different utility functions.) We define
the AMA for MCA by setting the Uset agreement to
be the Braun-Blanquet coefficient (Braun-Blanquet,
1932), which is defined as:
UBraun?Blanquet
(
mij?r(c
j
l ),m
ij
?p(c
j
l )
)
,
?
???
???
1 if mij?r(c
j
l ) = ?
and mij?p(c
j
l ) = ?
|mij?r (c
j
l )?m
ij
?p (c
j
l )|
max{|mij?r (c
j
l )|,|m
ij
?p (c
j
l )|}
otherwise
.
(6)
Caillez & Kuntz (1996) show that the Braun-
Blanquet coefficient is based on a metric.
As with the MSA case, a family of utility func-
tions can be defined to enable control of the re-
call/precision trade-off. Unlike MSA, in the case of
MCA two free parameters are needed, in order to
have better control of the trade-off using posterior-
decoding. In addition to a gap-factor that controls
the threshold at which unaligned words start to get
aligned, a match-factor is added to enable control of
the number of word-positions each word aligns to.
The result is the following utility function:
U?,?(?
r,?p) ,
1
n(K ? 1)
?
ijl|i6=j
(
?|m
ij
?p (c
j
l )|
|mij?r(c
j
l ) ?m
ij
?p(c
j
l )|
max
{
|mij?r(c
j
l )|, |m
ij
?p(c
j
l )|, 1
}+
?1{mij?r(c
j
l ) = m
ij
?p(c
j
l ) = ?}
)
, (7)
where ? ? [0,?) is a gap-factor, and ? ? (0,?) is
a match factor. The neutral value for both parame-
ters is 1. Increasing ? results in increased utility to
sparser MCAs, while reducing ? increases the util-
ity of denser alignments. However, in the case of
MCA, the gap-factor only affects the first aligned
word position, but it cannot affect the number of
word positions each word is aligned to. The match-
factor adds this functionality by rewarding MCAs
that align words to multiple word positions when
? > 1, and penalizing such MCAs when ? < 1.
Given a group of K citances G and a trained
CRF model, the goal of the MCA algorithm is to
find the MCA ??, argmax?p E?tU?,?(?
t,?p)
that maximizes the expected utility. Since search-
ing the space of possible MCAs exhaustively is in-
feasible, we resort to a simple heuristic for predict-
ing an MCA. Instead of searching for a global opti-
mum, the predicted MCA is defined as the equiva-
lence (symmetric transitive) closure of the union of
multiple local optima. For each target word posi-
tion cjl and every source citance C
i the combina-
tion of source word positions ci? that maximize the
expected set-agreement score of cjl is added to the
predicted MCA. Formally, let P(ci) be the power-
set of ci, then we define the predicted MCA as
?p,
(
;p ?(;p)?1
)+
, where;p is defined as:
;p,
?
ijl|i6=j
{cjl } ? argmax
ci??P(ci)
Emij
?t
(cjl )
?
??|c
i
?|
|mij?t(c
j
l ) ? c
i
?|
max
{
|mij?t(c
j
l )|, |c
i
?|, 1
}+
?1{mij?t(c
j
l ) = c
i
? = ?}
)
. (8)
The value of ;p can be computed from the CRF
directional posterior probabilities as follows:
;p=
?
ijl|i6=j
{cjl } ? argmax
ci??P(ci)
?
ci??P(ci)
P
(
mij?t(c
j
l ) = c
i
?
)
(
?|c
i
?|
|ci? ? c
i
?|
max {|ci?|, |ci?|, 1}
+ ?1{ci? = c
i
? = ?}
)
,
(9)
851
and using an independence assumption we get:
;p?
?
ijl|i6=j
{cjl } ? argmax
ci??P(ci)
?
ci??P(ci)
?
?
?
cik
(
P?(c
i
k ; c
j
l |C
i, Cj)1{cik ? c
i
?}+
(1? P?(c
i
k ; c
j
l |C
i, Cj))1{cik /? c
i
?}
)
)
(
?|c
i
?|
|ci? ? c
i
?|
max {|ci?|, |ci?|, 1}
+ ?1{ci? = c
i
? = ?}
)
.
(10)
Note that although the directional posterior prob-
abilities are used to generate the predicted MCA,
the result is a many-to-many alignment, since the
union is done over all pairs of sequences in both
directions. The calculation in Equation (10) can
be computationally intensive in practice, as it re-
quires |P(ci)|2 = 22n
i
operations for each word
position cjl and citance C
i. This can be over-
come by restricting the combinations of source word
positions (ci? and c
i
?) to include only the the top
MAX SOURCES source words with a minimum
posterior probability of MIN PROB to align to cjl
(P?(cik ; c
j
l |C
i, Cj) ? MIN PROB). In our im-
plementation we set MAX SOURCES to 8 and
MIN PROB to 0.01. Additionally, the probabilities
of each combination ci? can be calculated only once,
since it is independent of ci?. This reduces the to-
tal computational complexity of calculating ?p to
O
(
216(K2 ?K)maxni
{
ni
})
.
4 Data sets
Since citance alignment is a new task, we had to
create our own evaluation and training sets. We re-
stricted the domain of the target papers to molec-
ular interactions, a domain which is actively re-
searched in the biosciences text mining community
(Hirschman et al, 2002). The biologist in our group
annotated citances to 6 target papers. The training
set consisted of 40 citances to 4 different target pa-
pers (10 citances each; we wanted to have variety in
the training set). The development set consisted of
51 citances to the fifth target paper, and the test set
contained 45 citances to the sixth target paper.
For each target paper we downloaded the full text
of those papers citing it that were available in HTML
format. The link structure of the cited references in
the HTML documents allowed us to automatically
extract citances to a given target paper. We defined a
citance to be the full sentence that contains a citation
to the target paper. Each citance was then tokenized,
and normalized by removing all stopwords from a
predefined list.
One goal of the annotation was to cover as much
of the content of the citances as possible. Another
goal was consistency; our biologist manually fol-
lowed a small number of rules to determine seman-
tic similarity. Distinct semantic units (words or
phrases) were identified and given an annotation ID.
Within each group of citances, words or phrases that
share semantic similarity were annotated with the
same ID.
Using the manually annotated citance groups,
pairwise word alignments were generated for every
source-target pair of citances from every group. That
resulted in a training, development, and test sets
of 180, 1275, and 990 pairwise alignments respec-
tively.
Alignments that were used for development and
testing were generated as many-to-many alignments.
However, many-to-many alignments are not suit-
able for the training the many-to-one CRF align-
ment model. When a given source word cik aligns
to multiple words in the target citance, the CRF
model chooses only one target word as a true pos-
itive, while incorrectly treating the other true posi-
tive target words as true negatives. To alleviate this
problem, in such cases we replaced all true-positive
target words other than the first with ?*?, thus forcing
them to act as real true negatives for the purpose of
training. This adjustment does not solve the inher-
ent limitation of the CRF?s many-to-one modeling of
a many-to-many alignment, but it prevents learning
incorrect weights for good features.
5 Feature engineering
The CRF alignment model can combine multiple
overlapping features. We evaluated the effectiveness
of different features by training models on the train-
ing set and evaluating their performance on the de-
velopment set. We considered variations of features
852
that were part of the original system of Blunsom &
Cohn (2006), and also designed new features that are
specific to the problem of MCA.
Orthographic features
We used the following orthographic features from
the original system of Blunsom & Cohn (2006) (be-
low all features are either Boolean indicator func-
tions (b) or real valued (r)):
(b) exact string similarity of source-target words;
(b) every possible source-target pair of length 3 prefixes;
(b) exact string match of length 3 prefixes;
(b) exact string match of length 3 suffixes;
(r) absolute difference in word lengths;
(b) both words are shorter than 4 characters.
In addition, the following orthographic features
were added: indicator that both words include capi-
tal letters, and normalized edit-similarity of the two
words (1?
edit distance(cik,c
j
l )
max{|cik|,|c
j
l |}
).
Markov features
We used the following Markov features from the
original system:
(r) absolute jump width (abs(at?at?1?1), which measures
the distance between the target words of adjacent source
words;
(r) positive jump width (max{at ? at?1 ? 1, 0});
(r) negative jump width (max{at?1 + 1? at, 0});
(b) transition from null aligned source-word to non-null
aligned source-word;
(b) transition from non-null aligned source-word to null
aligned source-word;
(b) transition from null aligned source-word to null aligned
source-word.
In addition we added the following Markov fea-
tures in order to model the tendency of certain words
to be part of longer phrases:
(b) source-word aligns to the same target-word as the previ-
ous source-word;
(b) source-word aligns to the same target-word as the next
source-word;
(b) transition from non-null aligned source-word to non-null
aligned source-word.
Sentence position: We included the relative sen-
tence position feature from the original system,
which is defined as abs( at|cj | ?
t
ci ). Although it was
not expected to be relevant for MCA, since the ci-
tances are not expected to align along the diagonal,
this feature slightly improved the performance of the
development set.
Null: An indicator function for leaving a source-
word unaligned was retained from the original sys-
tem. This is an essential feature since without it the
CRF tends to over-align words, and produces mean-
ingless posterior probabilities.
Ontological features: Orthographic and posi-
tional features alone do not cover all cases of se-
mantic homology. We therefore included features
that are based on domain specific ontologies.
Using an automated script we mapped specific
words and phrases in every citance to MeSH1 terms,
Gene identifiers from Entrez Gene,2 UniProt,3 and
OMIM.4 We then added features indicating when
the source and target words are annotated with the
same MeSH term or the same gene identifier. We
tried numerous features that compare MeSH terms
based on their distance in the ontology, and other
features that indicate whether a word is part of a
longer term. However, none of these feature were
selected for the final system.
In addition to biological ontologies we added a
feature for semantic word similarity between the
source and target words, based on the Lin (1998)
WordNet similarity measure.
6 Results
We modified the CRF alignment system of Blun-
som & Cohn (2006) to support MCA by incorpo-
rating the posterior decoding algorithm from Sec-
tion 3.2 into the existing system. The CRF model
was trained using the features that were selected us-
ing the development set, on a dataset that included
the training and development MCAs. All the perfor-
mance results in this section are reported on the test
set, which includes 990 pairs of citances (45?44/2),
with a total of 34188 words (8547 ? 44). On aver-
age, 20% of the source words are aligned to at least
one other target word in a given reference pairwise
alignment. Since the union of all the pairwise align-
ments results in only a single test MCA, it is hard
to make strong arguments about the performance
1http://www.nlm.nih.gov/mesh/
2http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=gene
3http://www.pir.uniprot.org/
4http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=OMIM
853
00.10.20.30.40.50.60.70.80.91 0.
3
0.4
0.5
0.6
0.7
0.8
Reca
ll
Precision
Viterb
i_Inte
rsect
ion
Viterb
i_Uni
on
Poste
rior d
ecod
ing
Figure 3: Recall/Precision curve of pairwise ci-
tance alignments comparing Viterbi to posterior
decoding.
of the system in general. Therefore, we concen-
trate our discussion on general trends, and do not
claim that the specific performance numbers we re-
port here are statistically significant. As a point of
comparison, the SMT community has been evalu-
ating performance of word-alignment systems on an
even smaller dataset of 447 pairs of non-overlapping
sentences (Mihalcea and Pedersen, 2003).
We first analyze the performance of the system
on pairwise citance alignments. Instead of tak-
ing the equivalence closure of ;p we take only
the symmetric closure. The result is 990 many-
to-many pairwise alignments. In order to evalu-
ate the effectiveness of the posterior-decoding al-
gorithm, we generate the Viterbi alignments using
the same CRF model. The Viterbi many-to-many
pairwise alignments are then generated by combin-
ing equivalent pairs of many-to-one alignments us-
ing three different standard symmetrization methods
for word-alignment?union, intersection, and the re-
fined method of Och & Ney (2003).
Figure 3 shows the recall/precision trade-off of
the pairwise posterior-decoding and Viterbi align-
ments. The curve for the posterior-decoding align-
ments was produced by varying the gap and match
factors. For the Viterbi alignments, only three re-
sults could be generated (one for each symmetriza-
tion method). However, since the refined method
produced a very similar result to the union, only
the union is displayed in the figure. The impor-
tant observation is that while posterior-decoding en-
ables refined control over the recall/precision trade-
off, the Viterbi decoding generates only three align-
ments, which cover only a small fraction of the curve
at its high precision range. The union of Viterbi
alignments achieves 0.531 recall at 0.913 preci-
sion, which is similar result to the 0.540 recall at
0.909 precision achieved using posterior-decoding
with gap-factor and match-factor set to 1. However,
unlike Viterbi, posterior-decoding produces align-
ments with much higher recall levels, by increas-
ing the match-factor and decreasing the gap-factor.
For example setting the gap-factor to 0.1 and match-
factor to 1.2 results in alignments with 0.636 recall
at 0.517 precision, and setting them to 0.05 and 1.5
results in 0.742 recall at 0.198 precision. Generally,
the gap and match factor affect the accuracy of the
alignments as expected. In particular, the alignments
with the best AMA (0.889) and the best F1-measure
(0.678) are generated when the gap match factor are
set to their natural values (1,1), which theoretically
should maximize the expected AMA.
The performance of the pairwise alignments
validates the underlying probabilistic model,
showing it behaves as theoretically expected.
However, the union of all pairwise alignments
is not a valid MCA. To evaluate the MCA
posterior decoding algorithm, we compared it
to baseline MCAs. The baseline MCAs are
constructed by using only the normalized-edit-
distance
edit distance(cik,c
j
l )
max{|cik|,|c
j
l |}
, and defining cik ;
?
cjl if and only if normalized edit distance(c
i
k, c
j
l ) ?
?, where ? is a distance threshold. The fi-
nal baseline MCA is constructed by taking the
equivalence closure of all pairwise alignments,
??,
(
;? ?(;?)?1)
)+
. The ? parameter can
be used to control the recall/precision trade-off,
since increasing it adds more position-pairs to the
alignment, thus increasing recall, while decreasing
it increases precision.
Figures 4 compares the performance of the CRF
posterior-decoding MCAs with the baseline MCAs.
The different MCAs were produced by varying the
gap and match factors in the case of the posterior-
decoding, and ? for the baseline MCAs. The CRF
curve clearly dominates the baseline curve. How-
ever, they do overlap in range between 0.52 and
0.55 recall (0.84 and 0.90 precision). This is prob-
854
00.10.20.30.40.50.60.70.80.91 0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Reca
ll
Precision
CRF
Base
line
Figure 4: Recall/Precision curve of MCAs
comparing CRF with posterior decoding to
normalized-edit-distance baseline.
ably a range in which for this particular MCA the
orthographic similarity is the most dominant fea-
ture. While the baseline curve drops sharply after
that range, the posterior-decoding curve keeps im-
proving recall up to 0.636 at 0.748 precision, be-
fore a major drop in precision. The additional recall
is due to the ability of the CRF model to incorpo-
rate multiple overlapping features. In particular, the
domain-specific features are important for aligning
words and phrases that have little or no orthographic
similarity. At the other end of the overlap range, the
posterior-decoding achieves better precision than the
baseline for the same recall levels. For example, the
posterior decoding gets 0.381 recall at 0.982 preci-
sion compared with 0.346 at 0.937 for the baseline.
Unlike the pairwise alignment case, the neutral
settings of the gap and match factors did not result in
the best AMA score. This is due to the equivalence
closure heuristic that results in MCAs that are too
dense, since a single link between two equivalence
classes causes them to merge. The best AMA score
(0.886) is obtained by reducing the gap-factor to 0.5
and match-factor to 0.45, in order to compensate for
the effect of the equivalence closure heuristic. For
comparison, the best F1-measure (0.690) is achieved
by setting the gap and match factors to 0.75.
An error analysis on the latter MCA shows that
out of 1400 unique errors, 1194 (85.3%) are false
negatives (FN) and 206 (14.7%) false positives (FP).
Most errors (more than 600) are due to misalign-
ment of subtypes (e.g., cdc, cdc6, cdc25A), oppo-
sites (e.g., phosphorylated and unphosphorylated)
and complex entities (e.g., cell cycle v.s. cell line).
In addition, a large portion of FN errors are due to
not aligning entities that belong to just four equiva-
lence classes (e.g., 97 FN errors caused by terms in
the class of motif, site and domain). Other types of
errors include not aligning plural and singular forms
of the same entities, aligning only part of multi-
word entities, and incorrectly aligning orthographi-
cally similar entities that belong to different classes.
7 Conclusions
We have shown how to derive a posterior-decoding
algorithm that aims at maximizing the expected util-
ity for the MCA problem, as a substitute for the
sequence-annealing algorithm for MSA. Adding a
gap and match factor to the utility function allows
control over the recall/precision trade-off when us-
ing posterior-decoding. Another advantage of opti-
mizing the expected utility with posterior-decoding
methods is the decoupling from the probabilistic
model that generated the posterior probabilities.
This allows the use of CRFs instead of HMMs with
a similar posterior decoding algorithm.
Our experiments were limited by the size of the
labeled data. However, the results support the the-
oretical predictions, and demonstrate the advantage
of posterior-decoding over Viterbi decoding.
Since citances are still a relatively unexplored re-
source, it is still unclear whether the formulation we
presented here for citance alignment is the most use-
ful for applications that use citances for compara-
tive analysis of bioscience text. Unlike biological
sequence alignment, citance alignments are much
more subjective, as they depend on a loose defini-
tion of semantic homology between entities. Even
the definition of the basic entities can vary, since in
many cases noun-compounds and other multi-word
entities seem to be a more natural choice for basic el-
ements of semantic homology and alignment. How-
ever, automatic segmentation and entity recognition
are still difficult tasks in the bioscience text domain
and so new methods are worth investigating.
Acknowledgements: We thank Phil Blunsom and
Trevor Cohn for sharing their CRF-based word
alignment code. This research was supported in part
by NSF DBI 0317510.
855
References
Phil Blunsom and Trevor Cohn. 2006. Discrimina-
tive word alignment with conditional random fields.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 65?72, Sydney, Australia, July. Association for
Computational Linguistics.
Josias Braun-Blanquet. 1932. Plant sociology: the study
of plant communities. McGraw-Hill, New York.
Francis Caillez and Pascale Kuntz. 1996. A contribution
to the study of the metric and euclidean structures of
dissimilarities. Psychometrika, 61(2):241?253.
Pedros Domingos. 2000. A unified bias-variance de-
composition and its applications. In Proceedings
of the Seventeenth International Conference on Ma-
chine Learning, pages 231?238, Stanford, CA. Mor-
gan Kaufmann.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological sequence analy-
sis. Probablistic models of proteins and nucleic acids.
Cambridge University Press.
Jacob Falck, Niels Mailand, Randi G. Syljuasen, Jiri
Bartek, and Jiri Lukas. 2001. The ATM-Chk2-
Cdc25A checkpoint pathway guards against radiore-
sistant DNA synthesis. Nature, 410(6830):842?847.
Eugene Garfield. 1955. Citation indexes for science: A
new dimension in documentation through association
of ideas. Science, 122(3159):108?111.
C. Lee Giles, Kurt D. Bollacker, and Steve Lawrence.
1998. Citeseer: an automatic citation indexing sys-
tem. In Proceedings of the third ACM conference on
Digital libraries, pages 89?98. ACM Press.
Lynette Hirschman, Jong C. Park, Junichi Tsujii, Lim-
soon Wong, and Cathy H. Wu. 2002. Accomplish-
ments and challenges in literature data mining for bi-
ology. Bioinformatics, 18(12):1553?1561.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael I. Jordan. 2006. Word alignment via
quadratic assignment. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 112?119, New York City, USA,
June. Association for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289. Morgan Kaufmann, San Francisco, CA.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 104?111, New York City, USA,
June. Association for Computational Linguistics.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proc. 15th International Conf. on Ma-
chine Learning, pages 296?304. Morgan Kaufmann,
San Francisco, CA.
Ben-Ami Lipetz. 1965. Improvements of the selectiv-
ity of citation indexes to science literature through in-
clusion of citation relationship indicators. American
Documentation, 16:81?90.
Mengxiong Liu. 1993. Progress in documentation. the
complexities of citation practice: A review of citation
studies. Journal of Documentation, 49(4):370?408.
Evgeny Matusov, Richard Zens, and Hermann Ney.
2004. Symmetric word alignments for statistical ma-
chine translation. In COLING ?04: Proceedings of the
20th international conference on Computational Lin-
guistics, page 219, Morristown, NJ, USA. Association
for Computational Linguistics.
Rada Mihalcea and Ted Pedersen. 2003. An evaluation
exercise for word alignment. In Rada Mihalcea and
Ted Pedersen, editors, HLT-NAACL 2003 Workshop:
Building and Using Parallel Texts: Data Driven Ma-
chine Translation and Beyond, pages 1?10, Edmonton,
Alberta, Canada, May 31. Association for Computa-
tional Linguistics.
Robert C. Moore. 2005. A discriminative framework for
bilingual word alignment. In HLT/EMNLP, pages 81?
88.
Preslav I. Nakov, Ariel S. Schwartz, and Marti A. Hearst.
2004. Citances: Citation sentences for semantic anal-
ysis of bioscience text. In SIGIR?04 Workshop on
Search and Discovery in Bioinformatics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Gian-Carlo Rota. 1964. The number of partitions of a
set. The American Mathematical Monthly, 71(5):498?
504, may.
Ralf Schlu?ter, Thomas Scharrenbach, Volker Steinbiss,
and Hermann Ney. 2005. Bayes risk minimization
using metric loss functions. In Proceedings of the
European Conference on Speech Communication and
Technology, Interspeech, pages 1449?1452, Portugal,
September.
Ariel S. Schwartz and Lior Pachter. 2007. Multiple
alignment by sequence annealing. Bioinformatics,
23(2):e24?29.
856
Ariel S. Schwartz, Eugene W. Myers, and Lior Pachter.
2006. Alignment metric accuracy. arXiv:q-
bio.QM/0510052.
Ariel S. Schwartz. 2007. Posterior Decoding Meth-
ods for Optimization and Accuracy Control of Multiple
Alignments. Ph.D. thesis, EECS Department, Univer-
sity of California, Berkeley.
Howard D. White. 2004. Citation analysis and discourse
analysis revisited. Applied Linguistics, 25(1):89?116.
857
